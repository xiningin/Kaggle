{"cell_type":{"3f5eb819":"code","45b70d3b":"code","e81028dd":"code","4d1ce76d":"code","cb368f0d":"code","007fb6eb":"code","0230fb24":"code","e3afdb4d":"code","6b0568f0":"code","a1311885":"code","c4e0e52b":"code","8258211b":"code","443facd2":"code","40b26761":"code","4013d2ca":"code","2a214a76":"code","f9d41b47":"code","40db53fb":"code","fb192f39":"code","90b9be99":"code","50352f59":"code","df268d28":"code","04415f77":"code","9264b69a":"code","4c811b46":"code","344745c6":"code","0d6eb25e":"code","074a1bbf":"code","371131a1":"code","7ea173a6":"code","6f51c564":"code","ecd5bf1d":"code","17f29160":"code","7c03e5e8":"code","522d7248":"code","ae5d5e0d":"code","a934cef9":"code","db598934":"code","2c276d30":"code","222d23cc":"code","af24a6d2":"code","0390ac2c":"code","a8c4f81b":"code","82eeb3c8":"code","32ac2517":"code","046f2d68":"code","c133ad69":"code","52306455":"code","32cc2e0d":"code","9cad6943":"code","54e62dfe":"code","edc52e35":"code","feae7e4b":"code","4ca8c2da":"code","b0b574e8":"code","7aea52d5":"code","3e91b4b5":"code","b3c76a38":"code","78c576b5":"code","f5f1d0a9":"code","4b8616c9":"code","c084c447":"code","e076432d":"code","4c1f8bdf":"code","ab15a6e7":"code","7091b76c":"code","7abc35fa":"code","fc1953d1":"code","8ef0eeb0":"code","a821aa03":"code","1a4225ce":"code","af87b9df":"code","041f764b":"code","96d4a506":"code","573025b2":"code","30636b86":"code","e812cf8b":"code","985ed662":"code","6736f3c4":"code","f2d1be60":"code","39440fc9":"code","681c6339":"code","b1da527a":"code","c26a9024":"code","66a9150e":"code","cc3d1889":"code","20049aa7":"code","d79c1bd4":"code","72373974":"code","8ac25513":"code","ccf856f5":"code","b0d9dc83":"code","b8767c83":"code","abd873c2":"code","51d49e78":"code","88bc01d7":"code","cbc47045":"code","0a1467fa":"code","6aa97d39":"code","11444014":"code","feef8cf3":"code","fa2b90a7":"code","9de37acf":"code","ffa02161":"code","3c40b2a0":"code","aad14e44":"code","7eeb7cda":"code","3d994dad":"code","706cd834":"code","35dc8940":"code","4816d34b":"code","a86aa0c3":"code","75f7faf7":"code","d7532ac9":"code","b7d6e55b":"code","eb4606e0":"code","f8830731":"code","465e9fc9":"code","5bc9b723":"code","f0fcf416":"code","e9346f9a":"code","2100ffbd":"code","aa130eaa":"code","81b52214":"code","985e8493":"code","3d189b29":"code","710887b3":"code","33d8cbfd":"code","e498cff6":"code","4df76da6":"code","e0fb74f0":"code","98d7fb73":"code","53f68788":"code","2a632f8d":"code","fefa24e5":"code","a558c9f6":"code","ff5c260b":"code","dd88aa62":"code","006e6028":"code","37c12c59":"code","76972e3e":"code","a5741732":"code","38a186a2":"code","38df692b":"code","b1f00e79":"code","f936691a":"code","1eddad28":"code","22391ec2":"code","2ab79887":"code","b383f258":"markdown","2ce453e1":"markdown","3398aec0":"markdown","8e9f8471":"markdown","aed3ce9a":"markdown","6617345a":"markdown","269e0694":"markdown","db744958":"markdown","4f998be5":"markdown","233353a5":"markdown","c6f3f779":"markdown","badebb1c":"markdown","a6804604":"markdown","dc0a382e":"markdown","cb952214":"markdown","c24fc341":"markdown","f6e96892":"markdown","ac731c8d":"markdown","a20a75ee":"markdown","ea8de025":"markdown","96746613":"markdown","4e599f03":"markdown","6842ab4a":"markdown","f341b38f":"markdown","b2fc8fa4":"markdown","5081197f":"markdown","f0580eeb":"markdown","b1a5a034":"markdown","c9bbe84d":"markdown","2d3eec1c":"markdown","96864068":"markdown","c0314b60":"markdown","c1d00f24":"markdown","b7787ff7":"markdown","340f0813":"markdown","7bf368a0":"markdown","817a449e":"markdown","08d953ac":"markdown","806203c9":"markdown","fb891b6a":"markdown","fc25594f":"markdown","27ae3907":"markdown","7669fa72":"markdown","9b3d4f17":"markdown","41d6054b":"markdown","fae529f6":"markdown","7d9c962f":"markdown","97006571":"markdown","99677fe3":"markdown","28b60508":"markdown","f83b08d2":"markdown","8978c17e":"markdown","d87723d5":"markdown","9eed6fe5":"markdown","ce7097ca":"markdown","cd03f645":"markdown","217d9c20":"markdown","b5beb2d8":"markdown","528980b6":"markdown","01e1847f":"markdown","2dfe1018":"markdown","b05741ac":"markdown","da5ef432":"markdown","241a3181":"markdown"},"source":{"3f5eb819":"%reset -f","45b70d3b":"import warnings\nwarnings.filterwarnings(\"ignore\")","e81028dd":"# 1.0 Importing Basic Libraries.\n# 1.1 Load pandas, numpy, matplotlib, jason, Encoder, os & time\nimport numpy as np                     # linear algebra\nimport pandas as pd                    # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport json \nfrom sklearn.preprocessing import LabelEncoder,OrdinalEncoder\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nimport time\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","4d1ce76d":"# 1.2 Image manipulation\nfrom skimage.io import imshow, imsave","cb368f0d":"# 1.3 Libraries for Scaling\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import scale","007fb6eb":"# 1.4 Libraries for splitting.\nfrom sklearn.model_selection import train_test_split","0230fb24":"# 1.4.1 Return stratified folds. The folds are made by preserving the percentage of samples for each class.\nfrom sklearn.model_selection import StratifiedKFold\n# Libraries for AOC(Area Under theCurve) & ROC (Receiver Operating Characteristic Curve)\nfrom sklearn.metrics import auc, roc_curve\n# Libraries for Modelling\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as gbm\nfrom sklearn.tree import  DecisionTreeClassifier as dt\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.ensemble import RandomForestRegressor","e3afdb4d":"# 1.4.2 Visualization Plotly\n#import plotly.plotly as py\n#import plotly.graph_objs as go\nimport seaborn as sns","6b0568f0":"# 1.5 ML - we will classify using lightgbm\nimport lightgbm as lgb","a1311885":"# 1.6 Bayes Optimization -- One method\nfrom bayes_opt import BayesianOptimization","c4e0e52b":"# 1.7 Bayes optimization--IInd method\n# SKOPT is a parameter-optimisation framewor\nfrom skopt import BayesSearchCV","8258211b":"# Metrics\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score","443facd2":"#os.chdir(\"..\/input\")\n#os.listdir()\ndf_train = pd.read_csv(\"..\/input\/train.csv\") \ndf_test =  pd.read_csv(\"..\/input\/test.csv\")\ndf_sample_submission =  pd.read_csv(\"..\/input\/sample_submission.csv\")","40b26761":"print (\"Glimpse \/ sample of Train Dataset: \")\ndf_train.head()","4013d2ca":"print (\"Summary of Train Dataset: \",df_train.describe())\ndf_train.head(5)","2a214a76":"target = df_train['Target']\ntarget.value_counts()","f9d41b47":"print (\"Glimpse \/ Sample of Test Dataset: \")\ndf_test.head()","40db53fb":"print (\"Summary of Test Dataset: \")\ndf_test.describe()","fb192f39":"print (\"Glimpse of Sample Submission Dataset: \")\ndf_sample_submission.head()","90b9be99":"print (\"Summary of Sample Submission Dataset: \")\ndf_sample_submission.describe()","50352f59":"# 3.1 Target\nimport seaborn as sns\nsns.countplot(\"Target\", data=df_train)","df268d28":"sns.countplot(x=\"r4t3\",hue=\"Target\",data=df_train)","04415f77":"sns.countplot(x=\"v18q\",hue=\"Target\",data=df_train)","9264b69a":"sns.countplot(x=\"tamhog\",hue=\"Target\",data=df_train)","4c811b46":"sns.countplot(x=\"hhsize\",hue=\"Target\",data=df_train)","344745c6":"sns.countplot(x=\"noelec\",hue=\"Target\",data=df_train)","0d6eb25e":"from pandas.plotting import scatter_matrix","074a1bbf":"scatter_matrix(df_train.select_dtypes('float'), alpha=0.2, figsize=(26, 20), diagonal='kde')\nplt.show()","371131a1":"def missingdata(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/ data.isnull().count()*100).sort_values(ascending = False)\n    ms=pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n    ms = ms[ms[\"Percent\"] > 0]\n    f,ax = plt.subplots(figsize=(8,6))\n    plt.xticks(rotation='90')\n    fig = sns.barplot(ms.index, ms[\"Percent\"], color=\"green\", alpha=0.8)\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15)\n    return ms","7ea173a6":"missingdata(df_train)","6f51c564":"df_train.head(10)","ecd5bf1d":"df_train.dtypes.value_counts() ","17f29160":"missingdata(df_test)","7c03e5e8":"df_test.shape","522d7248":"df_test.dtypes.value_counts() ","ae5d5e0d":"df_train.drop(columns = ['rez_esc','v18q1','v2a1','meaneduc', 'SQBmeaned'], inplace = True)","a934cef9":"df_test.drop(columns = ['rez_esc','v18q1','v2a1','meaneduc', 'SQBmeaned'], inplace = True)","db598934":"naData = df_train.isnull().sum().values \/ df_train.shape[0] *100 \ndf_na = pd.DataFrame(naData, index=df_train.columns, columns=['Count']) \ndf_na = df_na.sort_values(by=['Count'], ascending=False)\nmissing_count = df_na[df_na['Count']>0].shape[0]\nprint('We got', missing_count, 'rows which have missing value in train set.') \ndf_na.head(10) \n###* We will get 5 rows which comprises of missing values in the train dataset","2c276d30":"naData = df_test.isnull().sum().values \/ df_test.shape[0] *100 \ndf_na = pd.DataFrame(naData, index=df_test.columns, columns=['Count']) \ndf_na = df_na.sort_values(by=['Count'], ascending=False)\nmissing_count = df_na[df_na['Count']>0].shape[0]\nprint('We got', missing_count, 'rows which have missing value in test set.') \ndf_na.head(10)","222d23cc":"# 2.3 Function to examine any dataset\n#     ExamineData.__doc__  => Gives help\ndef ExamineData(x):\n    \"\"\"Prints various data charteristics, given x\n    \"\"\"\n    print(\"Data shape:\", x.shape)\n    print(\"\\nColumns:\", x.columns)\n    print(\"\\nData types\\n\", x.dtypes)\n    print(\"\\nDescribe data\\n\", x.describe())\n    print(\"\\nData\\n\", x.head(2))\n    print (\"\\nSize of data:\", np.sum(x.memory_usage()))    # Get size of dataframes\n    print(\"\\nAre there any NULLS\\n\", np.sum(x.isnull()))","af24a6d2":"# 2.3.2 Let us understand test data\nExamineData(df_train)","0390ac2c":"# 2.3.2 Let us understand test data\nExamineData(df_test)","a8c4f81b":"df_train.loc[(df_train['tipovivi1'] == 1), 'v2a1'] = 0\ndf_test.loc[(df_test['tipovivi1'] == 1), 'v2a1'] = 0\ndf_train.loc[((df_train['age'] > 19) | (df_train['age'] < 7))] = 0\ndf_test.loc[((df_test['age'] > 19) | (df_test['age'] < 7))] = 0","82eeb3c8":"print(\"The Train dataset has {0} rows and {1} columns\".format(df_train.shape[0], df_train.shape[1]))\nprint(\"The Test dataset has {0} rows and {1} columns\".format(df_test.shape[0], df_test.shape[1]))","32ac2517":"# Remove Squared Variables to avoid confusion.\ndf_test = df_test[[x for x in df_test if not x.startswith('SQB')]]\ndf_train = df_train[[x for x in df_train if not x.startswith('SQB')]]\ndf_train = df_train.drop(columns = ['agesq'])\ndf_test = df_test.drop(columns = ['agesq'])\ndf_test.shape, df_train.shape","046f2d68":"print(\"The Train dataset has {0} rows and {1} columns\".format(df_train.shape[0], df_train.shape[1]))\nprint(\"The Test dataset has {0} rows and {1} columns\".format(df_test.shape[0], df_test.shape[1]))","c133ad69":"#Getting list of Columns which can act as Features\n## list of features to be used\nfeatures = [c for c in df_train.columns if c not in ['Id', 'Target']]","52306455":"import random\n#Making Seed Locked so that We can avoid different results Occuring when we run The file\nrandom.seed (45)","32cc2e0d":"#Handling data by Label encoding\ndef label_encoding(col):\n    le = LabelEncoder()\n    le.fit(list(df_train[col].values) + list(df_test[col].values))\n    df_train[col] = le.transform(df_train[col].astype(str))\n    df_test[col] = le.transform(df_test[col].astype(str))\n\nnum_cols = df_train._get_numeric_data().columns\ncat_cols = list(set(features) - set(num_cols))\nfor col in cat_cols:\n    label_encoding(col)\n    \ndf_train.shape,df_test.shape","9cad6943":"print(\"The Train dataset has {0} rows and {1} columns\".format(df_train.shape[0], df_train.shape[1]))\nprint(\"The Test dataset has {0} rows and {1} columns\".format(df_test.shape[0], df_test.shape[1]))","54e62dfe":"# Difference between people living in house and household size\ndf_train['hhsize-diff'] = df_train['tamviv'] - df_train['hhsize']\ndf_test['hhsize-diff'] = df_test['tamviv'] - df_test['hhsize']\nelec_tr = []\nelec_ts = []\n# Assign values in Train data for electricity type\nfor i, row in df_train.iterrows():\n    if row['noelec'] == 1:\n        elec_tr.append(0)\n    elif row['coopele'] == 1:\n        elec_tr.append(1)\n    elif row['public'] == 1:\n        elec_tr.append(2)\n    elif row['planpri'] == 1:\n        elec_tr.append(3)\n    else:\n        elec_tr.append(np.nan)\n\n#Assign Values in df_test data for electricity type\nfor i, row in df_test.iterrows():\n    if row['noelec'] == 1:\n        elec_ts.append(0)\n    elif row['coopele'] == 1:\n        elec_ts.append(1)\n    elif row['public'] == 1:\n        elec_ts.append(2)\n    elif row['planpri'] == 1:\n        elec_ts.append(3)\n    else:\n        elec_ts.append(np.nan)\n        \n# Record the new variable and missing flag\ndf_test['elec'] = elec_ts\ndf_test['elec-missing'] = df_test['elec'].isnull()\ndf_train['elec'] = elec_tr\ndf_train['elec-missing'] = df_train['elec'].isnull()\n\n# Wall ordinal variable\ndf_train['walls'] = np.argmax(np.array(df_train[['epared1', 'epared2', 'epared3']]),\n                           axis = 1)\ndf_test['walls'] = np.argmax(np.array(df_test[['epared1', 'epared2', 'epared3']]),\n                           axis = 1)\n# Roof ordinal variable\ndf_train['roof'] = np.argmax(np.array(df_train[['etecho1', 'etecho2', 'etecho3']]),\n                           axis = 1)\ndf_test['roof'] = np.argmax(np.array(df_test[['etecho1', 'etecho2', 'etecho3']]),\n                           axis = 1)\n# Floor ordinal variable\ndf_train['floor'] = np.argmax(np.array(df_train[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)\ndf_test['floor'] = np.argmax(np.array(df_test[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)\n# Create new feature\ndf_train['walls+roof+floor'] = df_train['walls'] + df_train['roof'] + df_train['floor']\ndf_test['walls+roof+floor'] = df_test['walls'] + df_test['roof'] + df_test['floor']\n# No toilet, no electricity, no floor, no water service, no ceiling\ndf_test['warning'] = 1 * (df_test['sanitario1'] + \n                         (df_test['elec'] == 0) + \n                         df_test['pisonotiene'] + \n                         df_test['abastaguano'] + \n                         (df_test['cielorazo'] == 0))\ndf_train['warning'] = 1 * (df_train['sanitario1'] + \n                         (df_train['elec'] == 0) + \n                         df_train['pisonotiene'] + \n                         df_train['abastaguano'] + \n                         (df_train['cielorazo'] == 0))\n# Owns a refrigerator, computer, tablet, and television\ndf_train['bonus'] = 1 * (df_train['refrig'] + \n                      df_train['computer'] + \n                      df_train['television'])\ndf_test['bonus'] = 1 * (df_test['refrig'] + \n                      df_test['computer'] + \n                      df_test['television'])\n# Per capita features\ndf_test['phones-per-capita'] = df_test['qmobilephone'] \/ df_test['tamviv']\ndf_test['rooms-per-capita'] = df_test['rooms'] \/ df_test['tamviv']\ndf_test['rent-per-capita'] = df_test['v2a1'] \/ df_test['tamviv']\n\ndf_train['phones-per-capita'] = df_train['qmobilephone'] \/ df_train['tamviv']\ndf_train['rooms-per-capita'] = df_train['rooms'] \/ df_train['tamviv']\ndf_train['rent-per-capita'] = df_train['v2a1'] \/ df_train['tamviv']\n\n# Create one feature from the `instlevel` columns\ndf_train['inst'] = np.argmax(np.array(df_train[[c for c in df_train if c.startswith('instl')]]), axis = 1)\ndf_test['inst'] = np.argmax(np.array(df_test[[c for c in df_test if c.startswith('instl')]]), axis = 1)\n\ndf_train['escolari\/age'] = df_train['escolari'] \/ df_train['age']\ndf_train['inst\/age'] = df_train['inst'] \/ df_train['age']\ndf_test['escolari\/age'] = df_test['escolari'] \/ df_test['age']\ndf_test['inst\/age'] = df_test['inst'] \/ df_test['age']\n\nprint('Train Data shape: ', df_train.shape,'Test Data shape: ',df_test.shape)","edc52e35":"print(\"The Train dataset has {0} rows and {1} columns\".format(df_train.shape[0], df_train.shape[1]))\nprint(\"The Test dataset has {0} rows and {1} columns\".format(df_test.shape[0], df_test.shape[1]))","feae7e4b":"#Finding Object type columns\ndf_train.select_dtypes('object').head()","4ca8c2da":"# Dropping the Only object type column \"ID\" so that data can be precessed for further Modelling\ndf_train =df_train.drop(columns = ['Id'])","b0b574e8":"print(\"The Train dataset has {0} rows and {1} columns\".format(df_train.shape[0], df_train.shape[1]))\nprint(\"The Test dataset has {0} rows and {1} columns\".format(df_test.shape[0], df_test.shape[1]))","7aea52d5":"df_train.head(10)","3e91b4b5":"# Defining the transformation function using columnTransformer, OneHotEncoder and StandardScaler\ndef transform(categorical_columns,numerical_columns,df):\n    #  (taskName, objectToPerformTask, columns-upon-which-to-perform)\n    # One hot encode categorical columns\n    cat = ('categorical', ohe() , categorical_columns  )\n    # Scale numerical columns\n    num = ('numeric', StandardScaler(), numerical_columns)\n    # Instantiate columnTransformer object to perform task\n    # It transforms X separately by each transformer and then concatenates results.\n    col_trans = ct([cat, num])\n    # Learn data\n    col_trans.fit(df)\n    # Now transform df\n    df_transAndScaled = col_trans.transform(df)\n    # Return transformed data and also transformation object\n    return df_transAndScaled, col_trans","b3c76a38":"df_train.info()","78c576b5":"df_test.info()","f5f1d0a9":"# Cleaninig Data\ndf_train.replace(0, np.nan)\ndf_test.replace(0,np.nan)\n#fillna() to replace missing values with the mean value for each column,\ndf_train.fillna(df_train.mean(), inplace=True);\nprint(df_train.isnull().sum());\n\ndf_train.shape","4b8616c9":"df_test.fillna(df_test.mean(), inplace=True);\nprint(df_test.isnull().sum());\ndf_test","c084c447":"df_train.drop(['idhogar',\"dependency\",\"edjefe\",\"edjefa\"], inplace = True, axis =1)\ndf_test.drop(['idhogar',\"dependency\",\"edjefe\",\"edjefa\"], inplace = True, axis =1)","e076432d":"from sklearn.decomposition import PCA","4c1f8bdf":"# Need to run the standerdised data set to perform PCA\npca = PCA().fit(df_train)","ab15a6e7":"def pca_summary(pca, standardized_data, out=True):\n    names = [\"PC\"+str(i) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n    a = list(np.std(pca.transform(standardized_data), axis=0))\n    b = list(pca.explained_variance_ratio_)\n    c = [np.sum(pca.explained_variance_ratio_[:i]) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n    columns = pd.MultiIndex.from_tuples([(\"sdev\", \"Standard deviation\"), (\"varprop\", \"Proportion of Variance\"), (\"cumprop\", \"Cumulative Proportion\")])\n    summary = pd.DataFrame(list(zip(a, b, c)), index=names, columns=columns)\n    \n    if out:\n        print(\"Importance of components:\")\n        display(summary)\n    return summary","7091b76c":"# Use the standerdised data set to get PCA_Summary\nsummary = pca_summary(pca, df_train)","7abc35fa":"# Summary of SD is taken from the above\nsummary.sdev","fc1953d1":"np.sum(summary.sdev**2)","8ef0eeb0":"summary.sdev**2","a821aa03":"X = df_train.values","1a4225ce":"X","af87b9df":"X = scale(X)","041f764b":"pca = PCA(n_components =141)","96d4a506":"pca.fit(X)","573025b2":"var= pca.explained_variance_ratio_","30636b86":"var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)","e812cf8b":"plt.plot(var1)","985ed662":"# Here we decide how many principle components are being considered. \n# Standard thumb rule is is consider all those principle components whoes SD > 1 or at 80 as beyond that we have flat pr no variation.\npca = PCA(n_components=80)","6736f3c4":"X = pca.fit_transform(X)","f2d1be60":"X.shape","39440fc9":"df_train.head()","681c6339":"#seprating target and predictors \ny=df_train[\"Target\"]\ny.unique()","b1da527a":"df_train.drop(['Target'], inplace = True, axis =1)","c26a9024":"df_train.shape #(9557, 140)","66a9150e":"#X=df_train","cc3d1889":"df_test.shape #(23856, 141)","20049aa7":"#Scaling the data \nscale = StandardScaler()\nX = scale.fit_transform(X)","d79c1bd4":"#Final set of features for modelling \nX.shape #(9557, 80)","72373974":"y.shape #(9557)","8ac25513":"df_test.shape #(23856, 141)","ccf856f5":"#Splitting the data into test and trainn\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.3, stratify = y)","b0d9dc83":"print(\"The X_train dataset has {0} rows and {1} columns\".format(X_train.shape[0], X_train.shape[1]))\nprint(\"The X_test dataset has {0} rows and {1} columns\".format(X_test.shape[0], X_test.shape[1]))\nprint(\"The y_train dataset has {0} rows\".format(y_train.shape[0]))\nprint(\"The y_test dataset has {0} rows\".format(y_test.shape[0]))","b8767c83":"modelrf = rf()","abd873c2":"modelrf = modelrf.fit(X_train, y_train)\nstart = time.time()\nend = time.time()\n(end-start)\/60","51d49e78":"classes = modelrf.predict(X_test)\nclasses","88bc01d7":"(classes == y_test).sum()\/y_test.size  #91","cbc47045":"f1 = f1_score(y_test, classes, average='macro')\nf1 #56","0a1467fa":" y_pred = modelrf.predict(X_test)\n accuracy_score(y_test, y_pred) #91","6aa97d39":"f  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\nf","11444014":"bayes_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    rf(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (80, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)\n# Start op","feef8cf3":"bayes_tuner.fit(X_train, y_train)","fa2b90a7":"best_params = pd.Series(bayes_tuner.best_params_)\nbest_params","9de37acf":"TunedRF=rf(criterion=\"gini\",\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","ffa02161":"start = time.time()\nTunedRF = TunedRF.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","3c40b2a0":"rf_predict=TunedRF.predict(X_test)\nrf_predict","aad14e44":"#  What accuracy is available on test-data\n(rf_predict == y_test).sum()\/y_test.size ","7eeb7cda":"bayes_tuner.best_score_","3d994dad":"#  And what all sets of parameters were tried?\nbayes_tuner.cv_results_['params']","706cd834":"rf = rf(random_state = 42)\nfrom pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","35dc8940":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","4816d34b":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","a86aa0c3":"rf_random.best_params_","75f7faf7":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy\nbase_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_model.fit(X_train, y_train)\nbase_accuracy = evaluate(base_model, X_test, y_test)","d7532ac9":"best_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test, y_test)","b7d6e55b":"print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) \/ base_accuracy))","eb4606e0":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","f8830731":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_\n{'bootstrap': True,\n 'max_depth': 80,\n 'max_features': 3,\n 'min_samples_leaf': 5,\n 'min_samples_split': 12,\n 'n_estimators': 100}\nbest_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, y_test)","465e9fc9":"print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) \/ base_accuracy))","5bc9b723":"modelgbm=gbm()","f0fcf416":"start = time.time()\nmodelgbm = modelgbm.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","e9346f9a":"classes = modelgbm.predict(X_test)\nclasses","2100ffbd":"unique_elements, counts_elements = np.unique(classes, return_counts=True)\nprint(np.asarray((unique_elements, counts_elements)))","aa130eaa":"(classes == y_test).sum()\/y_test.size #93\t","81b52214":"f1 = f1_score(y_test, classes, average='macro')\t\nf1 # 58%","985e8493":"bayes_tuner = BayesSearchCV(\n        gbm(\n            ),\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this \t\n        'max_depth': (4, 100),                # integer valued parameter\t\n        'max_features' : (10,64),             # integer-valued parameter\t\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\t\n    },\n    n_iter=32,            # How many points to sample\t\n    cv = 3                # Number of cross-validation folds\t\n)","3d189b29":"bayes_tuner.fit(X_train, y_train)","710887b3":"best_params = pd.Series(bayes_tuner.best_params_)\t\nbest_params","33d8cbfd":"TunedGBM=rf(criterion=\"entropy\",\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=250)","e498cff6":"start = time.time()\nTunedGBM = TunedGBM.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","4df76da6":"modeletf = ExtraTreesClassifier()\nstart = time.time()\nmodeletf = modeletf.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","e0fb74f0":"classes = modeletf.predict(X_test)\nclasses\t","98d7fb73":"(classes == y_test).sum()\/y_test.size","53f68788":"bayes_cv_tuner = BayesSearchCV(\n    ExtraTreesClassifier( ),\n    {   'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n)","2a632f8d":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","fefa24e5":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","a558c9f6":"modeletc=ExtraTreesClassifier(criterion=\"entropy\",\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=100)","ff5c260b":"start = time.time()\nmodeletc = modeletc.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","dd88aa62":"bayes_cv_tuner.best_score_","006e6028":"# Accuracy available on test-data\nbayes_cv_tuner.score(X_test, y_test)","37c12c59":"bayes_cv_tuner.cv_results_['params']","76972e3e":"modellgb = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)\n","a5741732":"start = time.time()\nmodellgb = modellgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","38a186a2":"classes = modellgb.predict(X_test)\nclasses\n(classes == y_test).sum()\/y_test.size ","38df692b":"modelneigh = KNeighborsClassifier(n_neighbors=4)","b1f00e79":"start = time.time()\nmodelneigh = modelneigh.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","f936691a":"classes = modelneigh.predict(X_test)\nclasses\n(classes == y_test).sum()\/y_test.size","1eddad28":"modelXGB = XGBClassifier()\nmodelXGB.fit(X_train, y_train)","22391ec2":"y_pred = modelXGB.predict(X_test)\t\npredictions = [round(value) for value in y_pred]","2ab79887":"accuracy = accuracy_score(y_test, predictions)\t\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","b383f258":"Created on Tue Apr 9, 2019\n\n**Project Team on Kaggle for Machine Learning:**\n * Jatin Solanki\n * Hemant Pandey\n * Dinesh Bulusu\n * Bheemeswara Sarma Kalluri","2ce453e1":"** 2.2 Check the data**","3398aec0":"**Steps to Follow**:\n1. State the question and determine required data\n2. Acquire the data in an accessible format\n3. Identify and correct missing data points\/anomalies as required\n4. Prepare the data for the machine learning model\n5. Establish a baseline model that you aim to exceed\n6. Train the model on the training data\n7. Make predictions on the test data\n8. Compare predictions to the known test set targets and calculate performance metrics\n9. If performance is not satisfactory, adjust the model, acquire more data, or try a different modeling technique\n10. Interpret model and report results visually and numerically.","8e9f8471":"### finding columns with Null Values","aed3ce9a":"### Model 2 - Gradient Boosting Classifier","6617345a":"<font size=\"4\">Costa Rica Poverty Exploration based on the data set provided on [Kaggle](www.kaggle.com)<\/font>","269e0694":"PCA_Summary function provides summary of PCA Analysis","db744958":"## Hyper Parameter Finetuning - Baysiean Optimization","4f998be5":"We achieved an **improvement in accuracy of 0.86%**. \n\nDepending on the application, this could be a significant benefit. \n\nWe can further improve our results by using grid search to focus on the most promising hyperparameters ranges found in the random search.","233353a5":"#### Random Forest has provided an accuracy of 92.95%","c6f3f779":"Now check for missing values in df_test data set","badebb1c":"XGBoost, short for \u201cExtreme Gradient Boosting\u201d, was introduced by Chen in 2014. Since its introduction, XGBoost has become one of the most popular machine learning algorithm. \t","a6804604":"After all the work of data preparation before modelling, creating and training the model is simple using Scikit-learn.\n\nWe import the random forest regression model from skicit-learn, instantiate the model, and fit (scikit-learn\u2019s name for training) the model on the training data. (Again setting the random state for reproducible results).","dc0a382e":"** 2.1 Load the data**","cb952214":"## Part II - Modelling","c24fc341":"These are the core data fields as described in the data description of df_train:\n1. Id - a unique identifier for each row.\n2. Target - the target is an ordinal variable indicating groups of income levels. \n            1 = extreme poverty 2 = moderate poverty 3 = vulnerable households 4 = non vulnerable households\n3. idhogar - this is a unique identifier for each household. This can be used to create household-wide features, etc. All rows in a given household will have a matching value for this identifier.\n4. parentesco1 - indicates if this person is the head of the household.","f6e96892":"**Accuracy on Test Data with Random Forest is : 94.94%**","ac731c8d":"Hyperparameters in Random Forest","a20a75ee":"In order to quantify the usefulness of all the variables in the entire random forest, we can look at the relative importances of the variables. The importances returned in Skicit-learn represent how much including a particular variable improves the prediction. The actual calculation of the importance is beyond the scope of this post, but we can use the numbers to make relative comparisons between variables.","ea8de025":"### Model 3: Extra Tree Classifier\t","96746613":"### Results of Bayesian Optimization\t","4e599f03":"**1. Import Required libraries**\n\n* **Numpy** - The famous numerical analysis library. It provides support from computing the median of data distribution to processing multidimensional arrays.\n* **Pandas** - Used for processing CSV files. This will also process tables and see statistics.\n* **Matplotlib** - Used for visualizations of data in pandas dataframes. Image is better than 100 words.\n* **Seaborn** - Another visualization tool tjhat is more focused on statistical visualizations such as histograms,pie charts, curves etc.\n* **SciKit-Learn** - This is the final boss of Machine Learning with Python. All things we need from algorithms to improvements.","6842ab4a":"Imporovements that can be done to Basic Gradient Boosting:\n1. Tree Constraints\n2. Shrinkage\n3. Random Sampling\n4. Penalized Learning","f341b38f":"This gives us the standard deviation of each component, and the proportion of variance explained by each component. \n\nThe standard deviation of the components is stored in a named row called sdev of the output variable made by the pca_summary function and stored in the summary variable:","b2fc8fa4":"### Model 1 - Random Forest","5081197f":"#### Feature Engineering","f0580eeb":"**Tips:**\n*     On Kaggle to hide the code: \n    * In the kernel editor, there is an option in the top right of every code cell - to hide input or output.. \n*     Use Markdown for formatting on Kaggle\n    * URL: https:\/\/www.markdowntutorial.com\/","b1a5a034":"Most of ther steps untill data cleanup and splitting has been completed before we began modelling and hence will focus now more on the remaining steps.","c9bbe84d":"**Objectives:**\n1. Import Required libraries    \n1. Standard Scaling (numeric data \/ Data Preparation)\n1. Visualization\n1. PCA (to reduce number of columns)\n1. Pipeline\n1. Use PCA to reduce number of columns\n1. Apply following modeling techniques:\n    * RandomForest (Use cross-validation and Bayes Optimization for values of Hyper Parameters)\n    * XGBoost (Use cross-validation and Bayes Optimization for values of Hyper Parameters)\n    *      (generally the Best Model observed on Kaggle)\n    * LightGBM (Use cross-validation and Bayes Optimization for values of Hyper Parameters)\n1. Summary Observations","2d3eec1c":"#### XGBoost Has given an accuracy of 92.89%\t","96864068":"#### KNeighborsClassifie has given an accuracy of 90.55%\t","c0314b60":"### Model 4 - Light GBM","c1d00f24":"#### Gradient Boosting Classifier has given an accuracy of 92.29%","b7787ff7":"# Sample code from hyperopt\t\nfrom hyperopt import fmin, tpe, hp\nbest = fmin(\n    fn=lambda x: x,\n    space=hp.uniform('x', 0, 1),\n    algo=tpe.suggest,\n    max_evals=100)\nprint(best)","340f0813":"This will try out 1 x 4 x 2 x 3 x 3 x 4 = 288 combinations of settings. \n\nWe can fit the model, display the best hyperparameters, and evaluate performance:","7bf368a0":"Finding and handling column names with Object Dtypes","817a449e":"Our model has now been trained to learn the relationships between the features and the targets. \n\nThe next step is figuring out how good the model is! To do this we make predictions on the test features (the model is never allowed to see the test answers). \n\nWe then compare the predictions to the known answers. \n\nWhen performing regression, we need to make sure to use the **absolute error** because we expect some of our answers to be low and some to be high. We are interested in how far away our average prediction is from the actual value so we take the absolute value.","08d953ac":"## Cumilative Model Summary for Costa Rica Project","806203c9":"### Model 6 XGBoost\t","fb891b6a":"This is a supervised, regression machine learning and it is supervised because we have both the features and the target that we want to predict. As part of this section providing random forest both the features and targets and it must learn how to map the data to a prediction. ","fc25594f":"Using PCA we have reduced the principle components from 144 to 80 as there is no impact or variation for these 64 components.","27ae3907":"Dropped that are not required. the columns that are not required..","7669fa72":"#### Extra Tree Classifier its accuracy to 95.01% after finetuning with Bayesian optimization\t\n#### Random Forest improved its accuracy to 94.45% after finetuning with Bayesian optimization","9b3d4f17":"We see that of all the 6 Models, **Extra Tree Classifier tops with an Accuracy of 94.28%**\n1. Extra Tree Classifier has given an accuracy of 94.28%\n2. Light GBM has given an accuracy of 94.14%\n3. Random Forest has provided an accuracy of 93.79%\n4. Gradient Boosting Classifier has given an accuracy of 93.27%\n5. XGBoost has given an accuracy of 92.89%\n6. KNeighborsClassifie has given an accuracy of 90.55%","41d6054b":"### Principal Component Analysis (PCA) \/ Karhunen-Loeve Transform (KLT)","fae529f6":"#### Grid Search with Cross Validation (CV)","7d9c962f":"The total variance explained by the components is the sum of the variances of the components:","97006571":"### Split the data","99677fe3":"Generic Column Transform Function","28b60508":"Load data from Train, Test & Sample Submission CSV files.","f83b08d2":"Using Bayesian optimization for parameter tuning allows us to obtain the best parameters for a given model, e.g., Random Forest. \t\nThis also allows us to perform optimal model selection. Typically, a machine learning engineer or data scientist will perform some form of manual parameter tuning (grid search or random search) for a few models\u200a\u2014\u200alike decision tree, XGBoost, and k nearest neighbors\u200aetc. \u2014\u200athen compare the accuracy scores and select the best one for use as this has the possibility of comparing sub-optimal models. Maybe we have found the optimal parameters for the decision tree, but missed the optimal parameters for Random Forest. This means the model comparison was flawed. K nearest neighbors may beat Random Forest every time if the Random Forest parameters are poorly tuned. \t\nBayesian optimization allows us to find the best parameters of all the models considered, and therefore compare the best models. \t\nThis results in better model selection, because we are comparing the best k nearest neighbors to the best decision tree, etc. Only in this way can you do model selection with high confidence, assured that the actual best model is selected and used.","8978c17e":"#### Extra Tree Classifier has given an accuracy of 94.28%","d87723d5":"Gradient Boosting involves 3 elements:\n1. A loss function to be optimised.\n2. A weak learner to make predictions.\n3. An additive model to add weak learners to minimise the loss function.","9eed6fe5":"#### PCA Conclusion","ce7097ca":"There are two common methods of parameter tuning: grid search and random search. \t\nEach have their pros and cons. Grid search is slow but effective at searching the whole search space, while random search is fast, but could miss important points in the search space. \t\nLuckily, a third option exists: Bayesian optimization. \t","cd03f645":"### Model 5 KNeighborsClassifier\t","217d9c20":"### Results of Random Forest Classifier","b5beb2d8":"### Visualization","528980b6":"PCA Principle Component Analysis is used mainly for reducing the dimensionality where we have lot of features and we are undecessive on which features \/ components to consider in our analysis without impacting our end result.\n\n1. Can be used to mitigate theproblem caused by dimensionality.\n2. The above can be used to compress the data with verey little inormation lost.\n3. Understanding the structure of the data with hundreds od dimensions can be difficult.\n\nPCA is also known as **Karhunen-Loeve Transform (KLT)**, technique used for finding patterns in high dimensional data.  \n\nPCA reduces a set of possibly correlated high dimensional variables to a lower dimensional set of linearly uncorrelated synthetic variables called **Principle Components**.\n\nPCA can be used to find a set of vectors that span a subsoace that minimizes the sum of the squared errors of the projected data that would retain the greatest proportion of the original datasets's variance.\n\nAlso PCA is very useful when the variance in a data set is distributed unevenly across the dimensions.","01e1847f":"**References:**\n*     Scaling the data using SciKit: \n    *         https:\/\/www.analyticsvidhya.com\/blog\/2016\/07\/practical-guide-data-preprocessing-python-scikit-learn\/\n*     Splitting to Train & Test using SciKit:\n    *         https:\/\/medium.com\/@contactsunny\/how-to-split-your-dataset-to-train-and-test-datasets-using-scikit-learn-e7cf6eb5e0d\n*     Cross Validation:\n    *         https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\n*     K Nearest Neighbors\n    *         https:\/\/www.analyticsvidhya.com\/blog\/2018\/03\/introduction-k-neighbours-algorithm-clustering\/\n*     How to get started with Machine Learning\n     *        http:\/\/www.freecodecamp.org\n*     Feature Engineering\n     *     https:\/\/www.kaggle.com\/willkoehrsen\/a-complete-introduction-and-walkthrough\n*     Value Error\n     *     https:\/\/datascience.stackexchange.com\/questions\/11928\/valueerror-input-contains-nan-infinity-or-a-value-too-large-for-dtypefloat32\n*     Random Forest: Hyperpameter Tuning\n     *     https:\/\/towardsdatascience.com\/random-forest-in-python-24d0893d51c0\n     *     https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74","2dfe1018":"Another option to check for missing values in both test and train data sets","b05741ac":"#### Light GBM has given an accuracy of 94.14%\t","da5ef432":"### 2. Data Loading and Processing for understanding","241a3181":"Handling the missing values in test and train data"}}