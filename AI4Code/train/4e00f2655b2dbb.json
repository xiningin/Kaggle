{"cell_type":{"e3e867b5":"code","91770c01":"code","51882910":"code","7b0f9c01":"code","e3f31088":"code","b7558014":"code","4108e9f2":"code","ecaba233":"code","a6f4f7f0":"code","f498f537":"code","96733317":"code","f7e2b2d5":"code","98fcdd36":"code","5a2a2e01":"code","166d3deb":"code","c89bca9a":"code","60500060":"code","218f21fe":"code","996ca223":"code","6c88172c":"code","b50e3911":"code","343c8383":"markdown","fd8d8bb0":"markdown","062f13aa":"markdown","a888cc10":"markdown","5c8442da":"markdown","d2f2ef90":"markdown","30ff6eac":"markdown","eea1bc90":"markdown","20c893cc":"markdown"},"source":{"e3e867b5":"import sys\nsys.path.append('\/kaggle\/input\/efficientnet-keras-dataset\/efficientnet_kaggle')\n! pip install -e \/kaggle\/input\/efficientnet-keras-dataset\/efficientnet_kaggle","91770c01":"import cv2, os, gc, sys\nfrom sklearn.metrics import roc_auc_score\nimport albumentations as albu\nimport matplotlib.pyplot as plt\nimport pandas as pd, numpy as np\nimport efficientnet.tfkeras as efn\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf, math\nimport tensorflow.keras.backend as K\nprint('TF version',tf.__version__)\n\nTRAIN_MODEL = False\nFOLD_0_ONLY = True\nMODEL_PATH = '\/kaggle\/input\/setieb4768model\/'\n# IF ONLY INTERESTED IN GRAD CAM, SET BELOW TO FALSE\nPREDICT_OOF = True\nPREDICT_TEST = True","51882910":"# LIST GPUS TO BE USED\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n\n# EXPERIMENT VERSION NUMBER\nVER = 1003","7b0f9c01":"# USE MULTIPLE GPUS\nif os.environ[\"CUDA_VISIBLE_DEVICES\"].count(',') == 0:\n    strategy = tf.distribute.get_strategy()\n    print('single strategy')\nelse:\n    strategy = tf.distribute.MirroredStrategy()\n    print('multiple strategy')","e3f31088":"# USE MIXED PRECISION\n# UNFORTUNATELY FOR THIS MODEL, MIXED PRECISION HURTS MODEL PERFORMANCE\n#tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n#print('Mixed precision enabled')","b7558014":"train = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\nprint('Train shape is', train.shape )\ntrain.head()","4108e9f2":"test = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nprint('Test shape is', test.shape )\ntest.head()","ecaba233":"SIZE = 768\nBASE = '..\/input\/seti-breakthrough-listen\/train\/'\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, batch_size=32, shuffle=False, augment=False, visualize=False, size=SIZE, path=BASE,\n                 flipH=False, flipV=False, mixup_prob=0, mixup_alpha=3, mixup_max=True): \n\n        self.df = df.reset_index(drop=True)\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.augment = augment\n        self.mixup_prob = mixup_prob\n        self.mixup_alpha = mixup_alpha\n        self.mixup_max = mixup_max\n        self.visualize = visualize\n        self.size = size\n        self.path = path\n        self.flipH = flipH\n        self.flipV = flipV\n        self.on_epoch_end()\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = int( np.ceil( len(self.df) \/ self.batch_size ) )\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X, y = self.__data_generation(indexes)\n        \n        if self.augment: X = self.__augment_batch(X)                       \n        if self.flipH: X = X[:,::-1,:,:]\n        if self.flipV: X = X[:,:,::-1,:]\n            \n        return X,y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange( len(self.df ) )\n        if self.shuffle: np.random.shuffle(self.indexes)\n            \n    def _get_image(self,row):\n        data = np.load(self.path+row.id[0]+'\/'+row.id+'.npy').astype('float32') \n        X = np.zeros((273*3,256),dtype='float32')\n        \n        for k in range(3):\n            if self.visualize:\n                md = np.median(data[2*k,].flatten())\n                q75, q25 = np.percentile(data[2*k,].flatten(), [75 ,25])\n                iqr = q75 - q25\n                tmp = np.clip(data[2*k,],md-2*iqr,md+2*iqr)\n                tmp -= md-2*iqr\n                tmp \/= 4*iqr\n            else: \n                tmp = data[2*k,]       \n            X[273*k:273*(k+1),] = tmp\n            \n        X = cv2.resize(X,(self.size,self.size))\n                               \n        return X,float(row.target)\n        \n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        \n        X = np.zeros((len(indexes),self.size,self.size,1),dtype='float32')\n        y = np.zeros((len(indexes)),dtype='float32')\n        \n        df = self.df.loc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            X[i,:,:,0],y[i] = self._get_image(row)\n                                \n        # MIXUP WITHIN BATCH\n        y2 = y.copy(); X2 = X.copy()\n        for i in range(len(indexes)):\n            if np.random.uniform(0,1) < self.mixup_prob:\n                rw = np.random.randint(0,len(indexes),2)\n                img,tar = X2[rw[0],], y2[rw[0]]  \n                img2,tar2 = X2[rw[1],], y2[rw[1]]\n                w = np.random.beta(self.mixup_alpha,self.mixup_alpha)\n                X[i,] = w * img2 + (1-w) * img\n                if self.mixup_max:\n                    y[i] = np.max([tar,tar2])\n                else:\n                    y[i] = w * tar2 + (1-w) * tar\n                    \n        return X,y\n \n    def __random_transform(self, img):\n        composition = albu.Compose([\n            albu.HorizontalFlip(p=0.5),\n            albu.VerticalFlip(p=0.5),\n            #albu.ShiftScaleRotate(rotate_limit=0,scale_limit=0.125,shift_limit=0.0625,p=0.25), \n            #albu.ColorJitter(brightness=0.3, contrast=0.3, saturation=0, hue=0, p=0.25),\n        ])\n        return composition(image=img)\n            \n    def __augment_batch(self, img_batch):\n        for i in range(img_batch.shape[0]):\n            tmp = self.__random_transform(img_batch[i, ])\n            img_batch[i, ] = tmp['image']\n        return img_batch","a6f4f7f0":"# DISPLAY EXAMPLES OF DATALOADER\ncols = 4\ntrain_gen = DataGenerator(train, augment=True, shuffle=True, batch_size=4, visualize=True, mixup_prob=1.0)\n\nfor i,b in enumerate(train_gen):\n    plt.figure(figsize=(20,10))\n    for k in range(cols):\n        plt.subplot(1,cols,k+1)\n        plt.imshow( b[0][k] ) \n        t = b[1][k] \n        plt.title('target = %i'%t,size=16)\n    plt.show()\n    if i>=3: break","f498f537":"def build_model():\n\n    inp = tf.keras.layers.Input(shape=(None,None,1))\n    #x = tf.keras.layers.Concatenate(axis=-1)([inp,inp,inp])\n    x = tf.keras.layers.Conv2D(3,3,strides=1,padding='same')(inp)\n    base = efn.EfficientNetB4(weights='imagenet',include_top=False, input_shape=None)\n    x = base(x)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.15)(x)\n    x = tf.keras.layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n        \n    model = tf.keras.Model(inputs=inp, outputs=x)\n    \n    opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n    met = tf.keras.metrics.AUC()\n    loss = tf.keras.losses.BinaryCrossentropy()\n    \n    model.compile(loss=loss, optimizer=opt, metrics=met) \n        \n    return model","96733317":"def build_cam_model():\n\n    inp = tf.keras.layers.Input(shape=(None,None,1))\n    x = tf.keras.layers.Conv2D(3,3,strides=1,padding='same')(inp)\n    base = efn.EfficientNetB4(weights='imagenet',include_top=False, input_shape=None)\n    x0 = base(x)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x0)\n    x = tf.keras.layers.Dropout(0.15)(x)\n    x = tf.keras.layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n        \n    model = tf.keras.Model(inputs=inp, outputs=[x,x0])\n        \n    return model","f7e2b2d5":"LR_START = 5e-5\nLR_MAX = 5e-4\nLR_MIN = 5e-7\nLR_RAMPUP_EPOCHS = 3\nLR_SUSTAIN_EPOCHS = 0\nEPOCHS = 40\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index \/ decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN\n    return lr\n\nrng = [i for i in range(EPOCHS)]\nlr_y = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, lr_y, '-o')\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n      format(lr_y[0], max(lr_y), lr_y[-1]))\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","98fcdd36":"FOLDS = 5\nBATCH = 32\nVAL_BATCH = 32 #make this larger offline\n\nskf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\ntrain['fold'] = -1\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\nfor fold,(idx_t, idx_v) in enumerate(skf.split(train)):\n    if (not TRAIN_MODEL)&(not PREDICT_OOF)&(not PREDICT_TEST): break\n        \n    K.clear_session()\n    print('#'*25)\n    print('### FOLD',fold+1)\n    print('### train size',len(idx_t),'valid size',len(idx_v))\n    print('#'*25)\n    \n    train_gen = DataGenerator(train.iloc[idx_t], shuffle=True, augment=True, batch_size=BATCH, mixup_prob=1.0)\n    valid_gen = DataGenerator(train.iloc[idx_v], batch_size=VAL_BATCH) \n    test_gen = DataGenerator(test, batch_size=VAL_BATCH, path='..\/input\/seti-breakthrough-listen\/test\/')\n    \n    sv = tf.keras.callbacks.ModelCheckpoint(\n        'model_fold%i_v%i.h5'%(fold,VER), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch'\n    )\n    \n    with strategy.scope():\n        model = build_model()  \n    if TRAIN_MODEL:\n        model.fit(train_gen, epochs=EPOCHS, validation_data=valid_gen, verbose=1, callbacks=[sv,lr_callback]\n             ,use_multiprocessing=True, workers=4)\n               \n    if PREDICT_OOF | PREDICT_TEST:\n        print('Loading model to predict oof and preds...')\n        model.load_weights(MODEL_PATH+'model_fold%i_v%i.h5'%(fold,VER))\n    \n    if PREDICT_OOF:\n        print('Predicting oof with TTAx4...')\n        oof[idx_v] += model.predict(valid_gen,verbose=1).flatten()\/4.\n        valid_gen = DataGenerator(train.iloc[idx_v], batch_size=VAL_BATCH, flipH=True) \n        oof[idx_v] += model.predict(valid_gen,verbose=1).flatten()\/4.\n        valid_gen = DataGenerator(train.iloc[idx_v], batch_size=VAL_BATCH, flipV=True) \n        oof[idx_v] += model.predict(valid_gen,verbose=1).flatten()\/4.\n        valid_gen = DataGenerator(train.iloc[idx_v], batch_size=VAL_BATCH, flipH=True, flipV=True) \n        oof[idx_v] += model.predict(valid_gen,verbose=1).flatten()\/4.\n    \n        auc = roc_auc_score(train.target.values[idx_v],oof[idx_v])\n        print(f'Fold {fold+1} AUC =',auc)\n        print('wrote OOF to disk')\n        print('#'*25)\n    \n        # SAVE EACH OOF IN CASE WE STOP TRAINING EARLY\n        train.loc[idx_v,'fold'] = fold\n        train['oof'] = oof\n        train.to_csv(f'oof_v{VER}_f{fold}.csv',index=False)  \n    \n        # LOG FOLD OOF AUC SCORE\n        f = open(f'log_v{VER}.txt','a')\n        f.write(f'Fold {fold+1} AUC = {auc}\\n')\n        f.close()\n        \n    if PREDICT_TEST:    \n        print('Predicting test with TTAx4...')\n        preds += model.predict(test_gen,verbose=1).flatten()\/FOLDS\/4\n        test_gen = DataGenerator(test, batch_size=VAL_BATCH, path='..\/input\/seti-breakthrough-listen\/test\/',flipH=True)\n        preds += model.predict(test_gen,verbose=1).flatten()\/FOLDS\/4\n        test_gen = DataGenerator(test, batch_size=VAL_BATCH, path='..\/input\/seti-breakthrough-listen\/test\/',flipV=True)\n        preds += model.predict(test_gen,verbose=1).flatten()\/FOLDS\/4\n        test_gen = DataGenerator(test, batch_size=VAL_BATCH, path='..\/input\/seti-breakthrough-listen\/test\/',flipH=True,flipV=True)\n        preds += model.predict(test_gen,verbose=1).flatten()\/FOLDS\/4\n    \n        # SAVE EACH TEST IN CASE WE STOP TRAINING EARLY\n        test['target'] = preds*5\/(fold+1)\n        test.to_csv(f'submission_v{VER}_f{fold}.csv',index=False)\n        print('wrote submission to disk')\n        \n    del model, train_gen, valid_gen, test_gen, sv\n    _ = gc.collect()\n    \n    if FOLD_0_ONLY: break","5a2a2e01":"# LOAD WEIGHTS INTO GRAD CAM MODEL\nwith strategy.scope():\n    model = build_cam_model()    \nmodel.load_weights(MODEL_PATH+'model_fold%i_v%i.h5'%(fold,VER))\nlayer_weights = model.layers[-1].get_weights()[0][:,0]","166d3deb":"# HELPER FUNCTION\ndef mask2contour(mask, width=5):\n    w = mask.shape[1]\n    h = mask.shape[0]\n    mask2 = np.concatenate([mask[:,width:],np.zeros((h,width))],axis=1)\n    mask2 = np.logical_xor(mask,mask2)\n    mask3 = np.concatenate([mask[width:,:],np.zeros((width,w))],axis=0)\n    mask3 = np.logical_xor(mask,mask3)\n    return np.logical_or(mask2,mask3) \n\nclahe = cv2.createCLAHE(clipLimit=16.0, tileGridSize=(8,8))","c89bca9a":"# GET OOF WITH TARGET EQUAL 1\nPORTION = 512\ntmp = train.iloc[idx_v[:PORTION]]\ntmp = tmp.reset_index(drop=True)\nIDX = tmp.loc[tmp.target==1].index.values\nlen(IDX)","60500060":"# PREDICT OOF SAMPLES FOR GRAD CAM\nvalid_gen = DataGenerator(train.iloc[idx_v[IDX]], batch_size=VAL_BATCH)\np,x = model.predict(valid_gen,verbose=1)\nprint(x.shape)","218f21fe":"SKIP = 0\nSHOW = 32\n\nfor i,k in enumerate(IDX[SKIP:SKIP+SHOW]):\n    \n    plt.figure(figsize=(20,5))\n    \n    # PLOT GRAD CAM\n    img = np.sum(x[i+SKIP,] * layer_weights,axis=-1)\n    img = cv2.resize(img,(320,320))\n    plt.subplot(1,4,4)\n    plt.imshow(img)\n    \n    # GET GRAD CAM CONTOUR\n    cut = np.percentile(img.flatten(), [90])[0]\n    cntr = img.copy()\n    cntr[cntr>=cut] = 100\n    cntr[cntr<cut] = 0\n    cntr = mask2contour(cntr)\n\n    # PLOT ORIGINAL ON CADENCE\n    name = train.iloc[idx_v[k],0]\n    tar = train.iloc[idx_v[k],1]\n    img0 = np.load(BASE+name[0]+'\/'+name+'.npy').astype('float32')\n    img = np.vstack(img0[::2])\n    img = cv2.resize(img,(320,320))\n    plt.subplot(1,4,1)\n    plt.imshow(img)\n    plt.title(f'Train ID = {name}',size=14)\n        \n    # PLOT ON CADENCE WITH IMPROVED VISIBILITY FILTER\n    plt.subplot(1,4,2)\n    img = img[1:,1:] - img[:-1,:-1] #emboss\n    img -= np.min(img)\n    img \/= np.max(img)\n    img = (img*255).astype('uint8')\n    img = cv2.GaussianBlur(img,(5,5),0)\n    img = clahe.apply(img)\n    mx = np.max(img)\n    if p[i+SKIP,0]>0.5: \n        cntr = cntr[1:,1:]\n        img[cntr>0] = mx\n    plt.imshow(img)\n    plt.title(f'True = {tar}',size=14)\n    \n    # PLOT OFF CADENCE WITH IMPROVED VISIBILITY\n    img = np.vstack(img0[1::2])\n    img = cv2.resize(img,(320,320))\n    plt.subplot(1,4,3)  \n    img = img[1:,1:] - img[:-1,:-1] #emboss\n    img -= np.min(img)\n    img \/= np.max(img)\n    img = (img*255).astype('uint8')\n    img = cv2.GaussianBlur(img,(5,5),0)\n    img = clahe.apply(img)\n    plt.imshow(img)\n    plt.title(f'Pred = {p[i+SKIP,0]:.3}',size=14)\n    \n    plt.show()","996ca223":"# PREDICT OOF SAMPLES FOR GRAD CAM\nPORTION = 256\ntest_gen = DataGenerator(test.iloc[:PORTION], batch_size=VAL_BATCH, path='..\/input\/seti-breakthrough-listen\/test\/')\np,x = model.predict(test_gen,verbose=1)\nprint(x.shape)","6c88172c":"# FIND PREDICTIONS WITH TARGET EQUAL 1\nIDX = np.where(p>0.75)[0]\nlen(IDX)","b50e3911":"SKIP = 0\nSHOW = 32\nBASE2 = '..\/input\/seti-breakthrough-listen\/test\/'\n\nfor i,k in enumerate(IDX[SKIP:SKIP+SHOW]):\n    \n    plt.figure(figsize=(20,5))\n    \n    # PLOT GRAD CAM\n    img = np.sum(x[k,] * layer_weights,axis=-1)\n    img = cv2.resize(img,(320,320))\n    plt.subplot(1,4,4)\n    plt.imshow(img)\n    \n    # GET GRAD CAM CONTOUR\n    cut = np.percentile(img.flatten(), [90])[0]\n    cntr = img.copy()\n    cntr[cntr>=cut] = 100\n    cntr[cntr<cut] = 0\n    cntr = mask2contour(cntr)\n\n    # PLOT ORIGINAL ON CADENCE\n    name = test.iloc[k,0]\n    img0 = np.load(BASE2+name[0]+'\/'+name+'.npy').astype('float32')\n    img = np.vstack(img0[::2])\n    img = cv2.resize(img,(320,320))\n    plt.subplot(1,4,1)\n    plt.imshow(img)\n    plt.title(f'Test ID = {name}',size=14)\n        \n    # PLOT ON CADENCE WITH IMPROVED VISIBILITY FILTER\n    plt.subplot(1,4,2)\n    img = img[1:,1:] - img[:-1,:-1] #emboss\n    img -= np.min(img)\n    img \/= np.max(img)\n    img = (img*255).astype('uint8')\n    img = cv2.GaussianBlur(img,(5,5),0)\n    img = clahe.apply(img)\n    mx = np.max(img)\n    if p[k,0]>0.5: \n        cntr = cntr[1:,1:]\n        img[cntr>0] = mx\n    plt.imshow(img)\n    #plt.title(f'True = {tar}',size=14)\n    \n    # PLOT OFF CADENCE WITH IMPROVED VISIBILITY\n    img = np.vstack(img0[1::2])\n    img = cv2.resize(img,(320,320))\n    plt.subplot(1,4,3)  \n    img = img[1:,1:] - img[:-1,:-1] #emboss\n    img -= np.min(img)\n    img \/= np.max(img)\n    img = (img*255).astype('uint8')\n    img = cv2.GaussianBlur(img,(5,5),0)\n    img = clahe.apply(img)\n    plt.imshow(img)\n    plt.title(f'Pred = {p[k,0]:.3}',size=14)\n    \n    plt.show()","343c8383":"# Learning Schedule","fd8d8bb0":"# Load Train and Test","062f13aa":"# Simple Silver Medal SETI Model with Grad Cam - LB 0.780+\nThis notebook demonstrates a simple silver medal model for Kaggle's SETI comp. We train 1 fold of EfficientNet-B4 with image size 768x768. It only uses Hflip, Vflip, and Mixup augmentation. It uses a cosine train schedule with warmup and 40 epochs. Using full precision, this takes 24 hours to train on 4xV100 Nvidia GPU. (Using mixed precision trains twice as fast but unfortunately hurts accuracy for this model).\n\nThis notebook also demonstrates Grad Cam to show us what image features the model is using to predict targets.","a888cc10":"# Models\nIn the model below we can use `tf.keras.layers.Concatenate` instead of `tf.keras.layers.Conv2D` and we can remove `tf.keras.layers.Dropout(0.15)` and achieve the same model performance. So they are not important. The most important thing is mixup augmentation, large image size, and large backbone.","5c8442da":"# Grad Cam OOF Preds","d2f2ef90":"# Grad Cam Test Preds","30ff6eac":"# Data Loader","eea1bc90":"# Train Model","20c893cc":"# Display Examples\nWhen displaying example, we use the flag `visualize=True`. We do not use this during training, but when displaying images, this flag makes the colors easier for us to see with our human eye."}}