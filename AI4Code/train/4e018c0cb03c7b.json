{"cell_type":{"33f648ca":"code","cf83a2ee":"code","378a0da2":"code","ae5b67c1":"code","152ccadf":"code","4872fac7":"code","bcc0a49c":"code","f036de31":"code","83211d19":"code","5d02127a":"code","1201df93":"code","38c5ca20":"code","b9603fec":"code","2c55453f":"code","fd3200f2":"code","090765cf":"code","2281ea62":"code","adb5566c":"code","febbbe21":"code","d5170e18":"code","404763de":"code","e484769c":"code","ed6fb687":"code","8825525c":"code","f8cfbe74":"code","1eacf851":"code","3a5f1ffa":"code","d397b6d9":"markdown","aea6fa5a":"markdown","cedd472f":"markdown","34505786":"markdown","b2f2abd5":"markdown","281fbaaa":"markdown","59bfc8f2":"markdown","e32409c6":"markdown","31dc3973":"markdown","e828ae9c":"markdown","e2052879":"markdown","2b76ed7f":"markdown","b3fa2a7b":"markdown","32a8126c":"markdown","d39406e4":"markdown","3582749b":"markdown","5a228f51":"markdown","0ee1c462":"markdown","408e4a16":"markdown","808a6e83":"markdown","16bd71c2":"markdown","29d2bb3b":"markdown"},"source":{"33f648ca":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf","cf83a2ee":"size = 3200  #data points count\nepochs_ = 20 #Epochs count for training\nverbose_ = 1  #set as 1 to see progress bar or as 0 for silent ","378a0da2":"# creating features using random values\nrawdf = pd.DataFrame({'feat1': np.random.randint(2000,8001,size)\/100, \n                   'feat2': np.random.randint(0,3001,size)\/100})\nrawdf.describe()","ae5b67c1":"# Defining three categories as 0, 1 & 2\nrawdf['ctg'] = 1\nrawdf.loc[ (rawdf.feat1>=60) & (rawdf.feat2>=20)  ,'ctg'] = 0\nrawdf.loc[(rawdf.feat1>=40) & (rawdf.feat1<60) & (rawdf.feat2>=10) & (rawdf.feat2<20) ,'ctg'] = 0\nrawdf.loc[(rawdf.feat1<40)  & (rawdf.feat2<10) ,'ctg'] = 0\nrawdf.loc[(rawdf.feat1>=60) & (rawdf.feat2<10)  ,'ctg'] = 2\nrawdf.loc[(rawdf.feat1>=40) & (rawdf.feat1<60)  & (rawdf.feat2>=20)  ,'ctg'] = 2\nrawdf.loc[(rawdf.feat1<40)  & (rawdf.feat2>=10) & (rawdf.feat2<20) ,'ctg'] = 2\nrawdf.groupby(['ctg']).size()\/size","152ccadf":"#Visualize category class with respect to each feature value before adding the noise\nsns.pairplot(rawdf, hue='ctg', height=4);","4872fac7":"# Adding \u00b12% mean feature value\nrawdf['feat1'] = rawdf['feat1'] + np.random.rand(size) * 24 - 12\nrawdf['feat2'] = rawdf['feat2'] + np.random.rand(size) * 12 - 6","bcc0a49c":"#Visualize categories and corresponding feature values After adding the noise\nsns.pairplot(rawdf, hue='ctg', height=4);","f036de31":"df = rawdf.copy()\nfeat_arr = df[['feat1','feat2']].to_numpy()\nlabels_arr = df[['ctg']].to_numpy().astype(int).reshape(-1)\n# Split data as 80% for Training & 20% for Validation\nsplit = int(size*0.8)\ntrain_feat, valid_feat = feat_arr[:split], feat_arr[split:]\ntrain_labels, valid_labels = labels_arr[:split], labels_arr[split:]\n#Check Shapes\ntrain_feat.shape , train_labels.shape","83211d19":"# Creating Summary Dict to store results\nmodels = ['no_normalization', 'normalized','f1_buck_f2_num','f1_dual_f2_buck','f1f2_buckets','f1f2_dual']\nresults = {'acc': '', 'val_acc': '', 'loss':'', 'val_loss':''}\nsummary_dict = {k: results.copy() for k in models}","5d02127a":"# helper function to plot results after each model training & also to store the results\ndef store_results(model_name, hist_obj):\n    'to store results in main dict & to demo run results'\n    summary_dict[model_name]['acc'] = hist_obj.history['acc']\n    summary_dict[model_name]['val_acc'] = hist_obj.history['val_acc']\n    summary_dict[model_name]['loss'] = hist_obj.history['loss']\n    summary_dict[model_name]['val_loss'] = hist_obj.history['val_loss']\n    fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n    x_range = list(range(epochs_))\n\n    ax[0].plot(x_range, hist_obj.history['loss'] , color='red', label='Training Loss') #\n    ax[0].plot(x_range, hist_obj.history['val_loss'] , color='maroon', label='Validation Loss')\n    ax[0].legend(loc='best')\n    ax[0].set_title('Loss Curves')\n\n    ax[1].plot(x_range, hist_obj.history['acc'] , color='lime', label='Training Acc') #\n    ax[1].plot(x_range, hist_obj.history['val_acc'] , color='green', label='Validation Acc')\n    ax[1].legend(loc='best')\n    ax[1].set_title('Accuracy Curves')\n\n    fig.suptitle(model_name + ' Model')\n\n    plt.show()","1201df93":"tf.keras.backend.clear_session()\nmodel1 = tf.keras.Sequential( )\nmodel1.add(tf.keras.layers.InputLayer(input_shape =2, ))\nmodel1.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel1.add(tf.keras.layers.Dense(8, activation='relu'))\nmodel1.add(tf.keras.layers.Dense(3, activation='softmax'))\nmodel1.compile(optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'] )\nhist_1 = model1.fit(x= train_feat, y = train_labels, validation_data = (valid_feat, valid_labels), epochs=epochs_, verbose=verbose_)\nstore_results('no_normalization', hist_1)","38c5ca20":"normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\nnormalizer.adapt(train_feat)\n# Show Mean & Variance for each feature\npd.DataFrame({'Feature': df.columns[:2], 'Mean': normalizer.mean.numpy(), 'Variance': normalizer.variance.numpy()})","b9603fec":"tf.keras.backend.clear_session()\nmodel2 = tf.keras.Sequential( )\nmodel2.add(tf.keras.layers.InputLayer(input_shape =2, ))\nmodel2.add(normalizer)\nmodel2.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel2.add(tf.keras.layers.Dense(8, activation='relu'))\nmodel2.add(tf.keras.layers.Dense(3, activation='softmax'))\nmodel2.compile(optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'] )\nhist_2 = model2.fit(x= train_feat, y = train_labels, validation_data = (valid_feat, valid_labels), epochs=epochs_, verbose=verbose_)\nstore_results('normalized', hist_2)","2c55453f":"# slice a df with features\ndemo_df = df.loc[:4,['feat1', 'feat2']]\n# Converting sliced df to a dict\ndemo_dict = dict(demo_df)\n# defining feat1 as a numeric column \nf1_numeric_col = tf.feature_column.numeric_column('feat1')\n# defining bucketized column & its boundaries\nf1_buckets = tf.feature_column.bucketized_column(f1_numeric_col, boundaries=[40,60])\n# creating feature layer that will convert bucketizeed column into tensor\nfeature_layer = tf.keras.layers.DenseFeatures([f1_buckets])","fd3200f2":"# Initial Feature Input\ndemo_df","090765cf":"feature_layer(demo_dict)","2281ea62":"train_dict = dict(df.loc[:split-1,['feat1', 'feat2']])\nvalid_dict = dict(df.loc[split:,['feat1', 'feat2']])","adb5566c":"# We need to start by defining nature of each column\nf1_numeric_col = tf.feature_column.numeric_column('feat1')\nf2_numeric_col = tf.feature_column.numeric_column('feat2')\n# Creating Bucketized Columns from Numeric Columns\n# Boundaries were defined to split the range into 3 buckets\nf1_buckets = tf.feature_column.bucketized_column(f1_numeric_col, boundaries=[40,60])\nf2_buckets = tf.feature_column.bucketized_column(f2_numeric_col, boundaries=[10,20])\n","febbbe21":"feature_layer_3 = tf.keras.layers.DenseFeatures([f1_buckets,f2_numeric_col])\n\ntf.keras.backend.clear_session()\nmodel3 = tf.keras.Sequential([\n    feature_layer_3,\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(3, activation='softmax') ])\nmodel3.compile(optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'])\nhist_3 = model3.fit(x=train_dict, y=train_labels, validation_data=(valid_dict,valid_labels), epochs=epochs_, verbose=verbose_)\nstore_results('f1_buck_f2_num', hist_3)","d5170e18":"feature_layer_3 = tf.keras.layers.DenseFeatures([f1_buckets,f2_numeric_col])\n\ntf.keras.backend.clear_session()\nmodel3 = tf.keras.Sequential([\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(3, activation='softmax') ])\nmodel3.compile(optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'])\nhist_3 = model3.fit(x=feature_layer_3(train_dict), y=train_labels, validation_data=(feature_layer_3(valid_dict),valid_labels), epochs=epochs_, verbose=verbose_)\nstore_results('f1_buck_f2_num', hist_3)","404763de":"feature_layer_4 = tf.keras.layers.DenseFeatures([f1_buckets,f1_numeric_col,f2_numeric_col])\n\ntf.keras.backend.clear_session()\nmodel4 = tf.keras.Sequential([\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(3, activation='softmax') ])\nmodel4.compile(optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'])\nhist_4 = model4.fit(x=feature_layer_4(train_dict), y=train_labels, validation_data=(feature_layer_4(valid_dict),valid_labels), epochs=epochs_, verbose=verbose_)\nstore_results('f1_dual_f2_buck', hist_4)","e484769c":"feature_layer_5 = tf.keras.layers.DenseFeatures([f1_buckets,f2_buckets])\n\ntf.keras.backend.clear_session()\nmodel5 = tf.keras.Sequential([\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(3, activation='softmax') ])\nmodel5.compile(optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'])\nhist_5 = model5.fit(x=feature_layer_5(train_dict), y=train_labels, validation_data=(feature_layer_5(valid_dict),valid_labels), epochs=epochs_, verbose=verbose_)\nstore_results('f1f2_buckets', hist_5)","ed6fb687":"feature_layer_6 = tf.keras.layers.DenseFeatures([f1_buckets,f2_buckets])\n\ntf.keras.backend.clear_session()\n\nbuckets_input_path = tf.keras.layers.Input(shape=(6,))\nbuck_node = tf.keras.layers.Dense(16, activation='relu')(buckets_input_path)\n\nnum_input_path = tf.keras.layers.Input(shape=(2,))\nnum_node = normalizer(num_input_path)\nnum_node = tf.keras.layers.Dense(16, activation='relu')(num_node)\n\nconc_ = tf.keras.layers.concatenate([buck_node, num_node])\njoined_ =  tf.keras.layers.Dense(8, activation='relu')(conc_)\nout_ =  tf.keras.layers.Dense(3, activation='softmax')(joined_)\n\nmodel6 = tf.keras.models.Model(inputs=[ buckets_input_path, num_input_path], outputs=[out_])\nmodel6.compile(optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'])\nhist_6 = model6.fit( x=(feature_layer_6(train_dict), train_feat) , y=train_labels,\n                    validation_data= (( feature_layer_6(valid_dict),valid_feat),valid_labels),\n                    epochs=epochs_, verbose=verbose_)\nstore_results('f1f2_dual', hist_6)","8825525c":"tf.keras.utils.plot_model(model6, show_shapes=True, rankdir=\"LR\")","f8cfbe74":"model6.summary()","1eacf851":"summary_dict.keys()","3a5f1ffa":"fig, ax = plt.subplots(2, 2, figsize=(20, 10))\nx_range = list(range(epochs_))\nfor model_name in summary_dict.keys():\n    \n    ax[0,0].set_title('Traing Loss')\n    ax[0,0].plot(x_range, summary_dict[model_name]['loss'] , label=model_name)\n    ax[0,0].legend(loc='best')\n    ax[0,0].set_ylim([0.6,1.6]) # to enlarge view\n\n    ax[0,1].set_title('Traing Accuracy')\n    ax[0,1].plot(x_range, summary_dict[model_name]['acc'] , label=model_name)\n    ax[0,1].legend(loc='best')\n\n    ax[1,0].set_title('Validation Loss')\n    ax[1,0].plot(x_range, summary_dict[model_name]['val_loss'] , label=model_name)\n    ax[1,0].legend(loc='best')\n    ax[1,0].set_ylim([0.6,1.6])\n\n    ax[1,1].set_title('Validation Accuracy')\n    ax[1,1].plot(x_range, summary_dict[model_name]['val_acc'] , label=model_name)\n    ax[1,1].legend(loc='best')\nplt.show()","d397b6d9":"# Models\nSix models will be created as described above. Results will be stored in `summary_dict` so we could make a comparison between all of them at the end.\na helper function will be defined to store the results for each trainging in mentioned dict.","aea6fa5a":"## Understanding Bucketization\n\nBucketing is defined [here](https:\/\/developers.google.com\/machine-learning\/glossary#bucketing) as: Converting a (usually continuous) feature into multiple binary features called buckets or bins, typically based on value range. You could explore more about Bucketing in [Google ML course](https:\/\/developers.google.com\/machine-learning\/data-prep\/transform\/bucketing).\n\nIn our example for 1st feature that is ranging from 20 to 80, we need to convert it into 3 bins (3 buckets). Assuming that we will use 3 buckets, then bucket ranges will be:\n1. 1st bucket: from 20 to 40\n2. 2nd bucket: from 40 to 60\n3. 3rd bucket: from 60 to 80\n\nBased on above, all values of 1st feature that are falling within the range of 20 to 40 will have the same input to the model. i.e. no difference between 21 and 39 values. Model should receive one hot keys instead of numeric values. A value of 45.6 should be passsed to the model as `[0,1,0]` because 45.6 is falling into 2nd bucket, while 33.3 should be passed as `[1,0,0]` as it is in 1st bucket and so on.","cedd472f":"## 1_ No Normalization","34505786":"### Bucketization Demo","b2f2abd5":"### Splitting for Train & Valid","281fbaaa":"## 5_ Bucket Both Features\nIn this model, both features will be passed as buckets","59bfc8f2":"## 6_ Dual Both Features\nYou may noticed that numeric data was not normalized before passing them to some previous models while doing bucketizing.\n* 3rd Model: Feature 2 was passed as numeric and not normalized\n* 4th Model: Feature 1 was passed without normalization.\n\nIn this model, we will use Functional API model with two input branches:\n1. Bucketized Features\n2. Numeric data with Normalization layer","e32409c6":"## Synthetic Data Creation\n* feat1: will range from 20 to 80\n* feat2: will range from 0 to 30.\n* Three Categorical labels will be created using some conditionas manually where each category is presented as almost 33.3% in the data\n* After defining the categories using some equations, some noise was added to the features ","31dc3973":"### Warning Message\n\n\n---\n\n\nWARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'feat1': <tf.Tensor 'ExpandDims:0' shape=(32, 1) dtype=float64>, 'feat2': <tf.Tensor 'ExpandDims_1:0' shape=(32, 1) dtype=float64>}\nConsider rewriting this model with the Functional API\n\n---\n\n\nYou noticed above warning message that could be fixd by either:\n1. Passing the input in tensor form (Model will be made again below)\n2. Using Functional API. It will be made in last model in this notebook.","e828ae9c":"## 2_ With Normalization\nWe need to normalize input data using [Normalization](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/experimental\/preprocessing\/Normalization) layer by passing training data only and then include it inside the model. Normalization layer must be adapted ahead of the training process by calling `.adapt`. You could also view mean and variance as below. As we have two features, then we should expect to get 2 means and 2 variances.","e2052879":"### Hints about Bucketization\n* To realize the value added from bucketization, watch one minute in [this video starting from here](https:\/\/youtu.be\/d12ra3b_M-0?t=93).\n* Nature of the feature may enforce you to use specific buckets count and boundaries like Age. Buckets should represent some description like child, youth, adult, senior as in [this definition](https:\/\/www.statcan.gc.ca\/eng\/concepts\/definitions\/previous\/age1a).\n* In this notebook, Features will bucketized into 3 buckts for demo purposes.\n* You must try different buckets count to see the impact.\n* if your final goal is to do classification into n categories, think to bucketize features into nk buckets where k=1,2,3, ... i.e. if we are building a classification model for 4 categories, it is recommended to try bucketizing the features into 4 or 8 or 12 buckets. Take care about overfitting.\n* Boundaries could be equally spaced or logspaced based on the feature nature and data distribution. Check [this page](https:\/\/developers.google.com\/machine-learning\/data-prep\/transform\/bucketing) where it demos some examples for quantile bucketing. \n","2b76ed7f":"# Comparison","b3fa2a7b":"Final Feature Input to the Model for `feat1` only","32a8126c":"TensorFlow is converting a numeric feature to buckets using `bucketized_column` [method](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/bucketized_column?hl=en) where following actions are involved in the process.\n\n| Step | Action | Hint |\n| --- | --- | --- |\n| Input Data | to pass it as dict not array | we need to have a key for each feature to create a feature column |\n| [numeric_column(key)](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/numeric_column?hl=en) | to define desired feature as a numeric feature | This function needs feature key. This justifies passing data as dict in previous step |\n| [bucketized_column(source_column, boundaries)](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column\/bucketized_column?hl=en) | to return Bucketized Column | this column will be passed to the model so we need to convert it back to tensor |\n| [layers.DenseFeatures(feature_columns)](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/DenseFeatures?hl=en) | to produces a dense Tensor for feature_columns | this layer should be part of the model |\n","d39406e4":"# Comments\n* It is very important to do numerical normalization as shown in 2nd model `normalized`\n* Bucketization is significantly enhancing model accuracy.\n* Further research must be made when deciding bucketing method (Equal spaced as in this notebook or log spaced). This area was outside the scope of this notebook.\n","3582749b":"# Features Bucketization\n\nThis notebook aims to demo features bucketization using TensorFlow. Synthetic data will be used with 2 features named as `feat1` and `feat2` while labels will be categorical with three classes as `[0,1,2]`.\nSix Models will be created and results will be compared at the end.\n\n| # | Model Name | Characteristics |\n| :- | :- | --- |\n| 1 | `no_normalization` | Using numeric feature values as input without doing normalization  |\n| 2 | `normalized` | Applying data normalization (Scaling) and no bucketization yet |\n| 3 | `f1_buck_f2_num` | Bucketizing 1st feature `feat1` while keeping `feat2` as numeric without normalization |\n| 4 | `f1_dual_f2_buck` | pass `feat1` as buckets and numeric (w\/o Norm) while passing `feat2` as buckets |\n| 5 | `f1f2_buckets` | passing both features as buckets and no numeric input |\n| 6 | `f1f2_dual` | Passing both features as buckets and also numeric with normalization using Fuctional API |\n\n","5a228f51":"## 4_ Bucket & Numeric \nIn this model, we will input feature 1 in two forms (Buckets & Numeric) while we will pass feature 2 as buckets. Model will be called `f1_dual_f2_buck`","0ee1c462":"## 3_ Bucketizing 1 Feature only\nIn 3rd model, we will bucketize feature 1 only while keeping feature 2 as numeric. As illustarted above, we neeed to convert our df to dict.","408e4a16":"### Converting df to dict\nAs illustarted above, we need to convert our df to dict so there are keys for each feature.","808a6e83":"Adding some white noise to feature values (\u00b1 2% )","16bd71c2":"### Redo 3rd model \nto bypass warning message, we need to passing training data input in form of tensors by utlizing `DenseFeatures` layer. As it produces a dense Tensor based on given feature column, it should be removed from the model. i.e. to be existed once","29d2bb3b":"To visualize the previous model, we could use plot_model & summary"}}