{"cell_type":{"2aa0f587":"code","69f181bd":"code","6256c2a7":"code","b583ac85":"code","b65dfb6f":"code","d6c7b3e9":"code","ba7105ae":"code","ac43ce1d":"code","18b56e06":"code","02f747fb":"code","11eeaae5":"code","3a9679aa":"code","b6908f2d":"code","66f1bfe1":"code","4b08d0e0":"code","47179c71":"code","91181511":"markdown","d205fb9b":"markdown","7f696bf1":"markdown","14391c56":"markdown","f16b00f7":"markdown","36b0eaeb":"markdown","d073eaaa":"markdown","cc5b5c96":"markdown","a444bade":"markdown","f25eb1ce":"markdown"},"source":{"2aa0f587":"# Notebook parameters\n\nBATCH_SIZE = 64\nVALID_BATCH_SIZE = 100\nTEST_BATCH_SIZE = 100\nEPOCHS = 5\nNUM_CLASSES = 10\nSEED = 42\nEARLY_STOPPING = 25\nOUTPUT_DIR = '\/kaggle\/working\/'\nMODEL_NAME = 'efficientnet-b0'","69f181bd":"!pip install efficientnet-pytorch","6256c2a7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom sklearn.metrics import accuracy_score\nfrom PIL import Image, ImageOps, ImageEnhance\nfrom efficientnet_pytorch import EfficientNet","b583ac85":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\nprint('Shape of the training data: ', train.shape)\nprint('Shape of the test data: ', test.shape)","b65dfb6f":"sample_df = train.groupby('label').apply(lambda x: x.sample(n=1)).reset_index(drop = True)\nsample_df.drop(columns=['label'], inplace=True)","d6c7b3e9":"nrows = 2\nncols = 5\nfig, axs = plt.subplots(nrows=nrows, ncols=ncols, gridspec_kw={'wspace': 0.01, 'hspace': 0.05},\n                       squeeze=True, figsize=(10,12))\n\nind_y = 0\nind_x = 0\nfor i, row in sample_df.iterrows():\n    if ind_y > ncols - 1:\n        ind_y = 0\n        ind_x += 1\n    sample_digit = sample_df.values[i, :].reshape((28, 28))\n    axs[ind_x, ind_y].axis('off')\n    axs[ind_x, ind_y].imshow(sample_digit, cmap='gray')\n    axs[ind_x, ind_y].set_title(\"Digit {}:\".format(i))\n    ind_y += 1\n\nplt.show()","ba7105ae":"from sklearn.model_selection import train_test_split\n\n# Perform train, validation split\ntrain_df, valid_df = train_test_split(train, test_size = 0.2, random_state=SEED,stratify=train['label'])","ac43ce1d":"import cv2\n\n# Define custom data loader, \n# code adapted from https:\/\/www.kaggle.com\/juiyangchang\/cnn-with-pytorch-0-995-accuracy\n\nn_pixels = len(train_df.columns) - 1\n\n\nclass MNIST_Dataset(Dataset):\n    \"\"\"MNIST data set\"\"\"\n    \n    def __init__(self, df\n                ):\n        \n        if len(df.columns) == n_pixels:\n            # test data\n            self.X = df.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n            self.y = None\n            \n            self.X3 = np.full((self.X.shape[0], 3, 28, 28), 0.0)\n\n            for i, s in enumerate(self.X):\n                self.X3[i] = np.moveaxis(cv2.cvtColor(s, cv2.COLOR_GRAY2RGB), -1, 0)\n                \n        else:\n            # training\/validation data\n            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n            self.y = torch.from_numpy(df.iloc[:,0].values)\n            \n            self.X3 = np.full((self.X.shape[0], 3, 28, 28), 0.0)\n\n            for i, s in enumerate(self.X):\n                self.X3[i] = np.moveaxis(cv2.cvtColor(s, cv2.COLOR_GRAY2RGB), -1, 0)\n    \n    def __len__(self):\n        return len(self.X3)\n\n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.X3[idx] , self.y[idx]\n        else:\n            return self.X3[idx]","18b56e06":"train_dataset = MNIST_Dataset(train_df)\nvalid_dataset = MNIST_Dataset(valid_df)\ntest_dataset = MNIST_Dataset(test)\n\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=BATCH_SIZE,\n                                           shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n                                           batch_size=VALID_BATCH_SIZE, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                           batch_size=TEST_BATCH_SIZE, shuffle=False)","02f747fb":"## Load in pretrained effnet model and remove its head, replacing it with fully connected layer \n## that gives 10 outputs \ndef get_model(model_name='efficientnet-b0'):\n    model = EfficientNet.from_pretrained(model_name)\n    del model._fc\n    # # # use the same head as the baseline notebook.\n    model._fc = nn.Linear(1280, NUM_CLASSES)\n    return model","11eeaae5":"import random\nimport os\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore","3a9679aa":"set_seed(SEED)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noutput_dir = OUTPUT_DIR\n\n\nmodel = get_model(MODEL_NAME)\nmodel = model.to(device)\n\n    \n# # # get optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# # # get scheduler\nscheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n# # # get loss\nloss_func = nn.CrossEntropyLoss()\n\nif torch.cuda.is_available():\n    model = model.cuda()\n    loss_func = loss_func.cuda()\n\nbest_val_accuracy = 0\nmin_val_loss = np.inf\nbest_epoch = 0\nbatches = 0\nepochs_no_improve = 0\nn_epochs_stop = EARLY_STOPPING\nfor epoch in range(EPOCHS):\n    running_loss = 0.0\n    targets = torch.empty(size=(BATCH_SIZE, )).to(device) \n    outputs = torch.empty(size=(BATCH_SIZE, )).to(device)\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        batches += 1\n        data, target = Variable(data), Variable(target)\n        if torch.cuda.is_available():\n            data = data.type(torch.FloatTensor).cuda()\n            target = target.cuda()\n        targets = torch.cat((targets, target), 0)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = loss_func(output, target)\n        output = torch.argmax(torch.softmax(output, dim=1), dim=1)\n        outputs = torch.cat((outputs, output), 0)\n        running_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n    print('train\/loss on EPOCH {}: {}'.format(epoch, running_loss\/batches))\n    train_acc = accuracy_score(targets.cpu().detach().numpy().astype(int), \n                              outputs.cpu().detach().numpy().astype(int))\n    print('train\/accuracy: {} for epoch {}'.format(train_acc, epoch))\n\n    model.eval()\n    # Validation loop\n    running_loss = 0.0\n    batches = 0\n    targets = torch.empty(size=(BATCH_SIZE, )).to(device) \n    outputs = torch.empty(size=(BATCH_SIZE, )).to(device) \n    for batch_idx, (data, target) in enumerate(valid_loader):\n        batches += 1\n        data, target = Variable(data, volatile=True), Variable(target)\n        if torch.cuda.is_available():\n            data = data.type(torch.FloatTensor).cuda()\n            target = target.cuda()\n        with torch.no_grad():\n            targets = torch.cat((targets, target), 0)\n            output = model(data)\n            loss = loss_func(output, target)\n            output = torch.argmax(torch.softmax(output, dim=1), dim=1)\n            outputs = torch.cat((outputs, output), 0)\n            running_loss += loss.item()\n\n    val_loss = running_loss\/batches\n    print('val\/loss: {}'.format(val_loss))\n    val_acc = accuracy_score(targets.cpu().detach().numpy().astype(int), \n                      outputs.cpu().detach().numpy().astype(int))\n    print('val\/accuracy: {} for epoch {}'.format(val_acc, epoch))\n\n\n\n    # Model Checkpoint for best validation f1\n    if val_acc > best_val_accuracy:\n        best_val_accuracy = val_acc\n        min_val_loss = val_loss\n        print('Best val\/acc: {} for epoch {}, saving model---->'.format(val_acc, epoch))\n        torch.save(model.state_dict(), \"{}\/snapshot_epoch_{}.pth\".format(output_dir, epoch))\n        best_epoch = epoch\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n    if epochs_no_improve == n_epochs_stop:\n        print('Early stopping!')\n        break","b6908f2d":"def prediction(model, data_loader):\n    model.eval()\n    test_pred = torch.LongTensor()\n    \n    for i, data in enumerate(data_loader):\n        data = Variable(data, volatile=True)\n        if torch.cuda.is_available():\n            data = data.type(torch.FloatTensor).cuda()\n            \n        output = model(data)\n        pred = output.cpu().data.max(1, keepdim=True)[1]\n        test_pred = torch.cat((test_pred, pred), dim=0)\n        \n    return test_pred","66f1bfe1":"model.load_state_dict(torch.load(\"snapshot_epoch_{}.pth\".format(best_epoch)))\ntest_pred = prediction(model, test_loader)\nsubmission = pd.DataFrame(np.c_[np.arange(1, len(test_dataset)+1)[:,None], test_pred.numpy()], \n                          columns=['ImageId', 'Label'])\n","4b08d0e0":"submission.head()","47179c71":"submission.to_csv('submission.csv', index=False)","91181511":"It's as simple as that! Just 6 lines of code to import the weights and architecture of pretrained efficientNet and replace its head.","d205fb9b":"<ins>Great, but how does Transfer Learning help in training a CNN? <\/ins>\n\n* Generally, the rate of convergence of a CNN is faster when transfer learning is applied as compared to without.","7f696bf1":"## Transfer Learning using EfficientNet\n---\n\nYou may have seen solutions of winning solutions in kaggle using EfficientNet. B**ut what is EfficientNet?**\n\nEfficientNet is deep learning architecture designed by Google to tackle the problem of scaling Neural Networks (deciding how to best increase model size and increase accuracy). Given that there is a tradeoff between efficiency and accuracy in scaling CNNs, the idea by Google is to provide better accuracy and improve the efficiency of the models by reducing the parameters and FLOPS (Floating Point Operations Per Second) manifold. There is an excellent article on Medium that explains in detail, what this architecture does: https:\/\/towardsdatascience.com\/efficientnet-scaling-of-convolutional-neural-networks-done-right-3fde32aef8ff","14391c56":"In this notebook, we will perform transfer learning using EfficientNetB0 on MNIST. I will explain the architecture behind EfficientNetB0 as well as the concept behind transfer learning","f16b00f7":"## Prediction on Test Set","36b0eaeb":"Now, let's do some training with our efficientNet architecture","d073eaaa":"## Read in MNIST data:","cc5b5c96":"We can take the pretrained weights and the architecture provided in EfficientNet, remove the \"head\" of the CNN network, and add our own custom head to form a new architecture. Then train it for some epochs for our model to learn specific to the task we set it to do ","a444bade":"## Concept Behind Transfer Learning\n---\n\nFirst let's understand the concept behind transfer learning.\n\nAn analogy is the way humans learn. For example, we learn and gain knowledge in a certain field like math and statistics. We are able to take what we have learnt and apply to other fields.\n\nIn deep learning, there is a similar concept known as transfer learning. Transfer learning is the idea of overcoming the isolated learning paradigm and utilizing knowledge acquired for one task to solve related ones. \n\nFor Convolutional Neural Networks, transfer learning can be done by taking weights learnt by a model architecture on a specific task (for eg, Image classification of a predefined set of images) and using these weights as a starting point, with a modified architecture, letting the new model learn for more epochs.\n\n![image.png](attachment:image.png)\n\n(Image from: https:\/\/researchgate.net\/figure\/Illustration-of-transfer-learning-concept-where-the-rst-layers-in-network-A-and-network-B_fig2_316748306)\n","f25eb1ce":"## Import necessary libraries"}}