{"cell_type":{"071cd74e":"code","df765e73":"code","f4696e7e":"code","c4869c38":"code","0e733334":"code","c040baae":"code","f5f6a730":"code","2849c076":"code","d323cbe0":"code","4455816e":"code","e7656171":"code","247af5ae":"code","86812d31":"code","df8485b9":"code","2f2bb4fb":"code","1fc5647f":"markdown","7d0a4699":"markdown","9c21093b":"markdown","1c1bf7ac":"markdown","ba8de0cd":"markdown","530718b3":"markdown","9e153ff4":"markdown","89a4e746":"markdown","18540fdb":"markdown","5f7ab1b5":"markdown","e995c0bf":"markdown","251869db":"markdown","6ca958e9":"markdown","cb48f3d0":"markdown","095af083":"markdown","b4570645":"markdown","1143d4a0":"markdown","21e2ac3a":"markdown","6515789d":"markdown","e05bdd57":"markdown","c18e2ef7":"markdown","2bdb203b":"markdown","7d5f3ccf":"markdown"},"source":{"071cd74e":"# Importing Required Libraries\nimport numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics","df765e73":"data = pd.read_csv(\"..\/input\/train.csv\")\ntrain,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])","f4696e7e":"train.head()","c4869c38":"# Dropping Features\ntrain = train.drop(['Name'], axis=1)\ntest = test.drop(['Name'], axis=1)\n\ntrain = train.drop(['Ticket'], axis=1)\ntest = test.drop(['Ticket'], axis=1)\n\ntrain = train.drop(['Cabin'], axis=1)\ntest = test.drop(['Cabin'], axis=1)\n\ntrain = train.drop(['PassengerId'], axis=1)\ntest = test.drop(['PassengerId'], axis=1)\n\n# Convert categorical variables into dummy\/indicator variables\ntrain_processed = pd.get_dummies(train)\ntest_processed = pd.get_dummies(test)\n\n# Filling Null Values\ntrain_processed = train_processed.fillna(train_processed.mean())\ntest_processed = test_processed.fillna(test_processed.mean())\n\n# Create X_train,Y_train,X_test\nX_train = train_processed.drop(['Survived'], axis=1)\nY_train = train_processed['Survived']\n\nX_test  = test_processed.drop(['Survived'], axis=1)\nY_test  = test_processed['Survived']\n\n# Display\nprint(\"Processed DataFrame for Training : Survived is the Target, other columns are features.\")\ndisplay(train_processed.head())","0e733334":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nrandom_forest_preds = random_forest.predict(X_test)\nprint('The accuracy of the Random Forests model is :\\t',metrics.accuracy_score(random_forest_preds,Y_test))","c040baae":"import lime\nimport lime.lime_tabular","f5f6a730":"predict_fn_rf = lambda x: random_forest.predict_proba(x).astype(float)\nX = X_train.values\nexplainer = lime.lime_tabular.LimeTabularExplainer(X,feature_names = X_train.columns,class_names=['Will Die','Will Survive'],kernel_width=5)","2849c076":"test.loc[[421]]","d323cbe0":"choosen_instance = X_test.loc[[421]].values[0]\nexp = explainer.explain_instance(choosen_instance, predict_fn_rf,num_features=10)\nexp.show_in_notebook(show_all=False)","4455816e":"test.loc[[310]]","e7656171":"choosen_instance = X_test.loc[[310]].values[0]\nexp = explainer.explain_instance(choosen_instance, predict_fn_rf,num_features=10)\nexp.show_in_notebook(show_all=False)","247af5ae":"test.loc[[736]]","86812d31":"choosen_instance = X_test.loc[[736]].values[0]\nexp = explainer.explain_instance(choosen_instance, predict_fn_rf,num_features=10)\nexp.show_in_notebook(show_all=False)","df8485b9":"test.loc[[788]]","2f2bb4fb":"choosen_instance = X_test.loc[[788]].values[0]\nexp = explainer.explain_instance(choosen_instance, predict_fn_rf,num_features=10)\nexp.show_in_notebook(show_all=False)","1fc5647f":"Choosen instance refers to an **Unlucky(Not Survived) female passenger of age 48 travelling in passenger class 3, embarked from S**. Let's see what and how our model predicts her survival.","7d0a4699":"---\n### Explaining Instance 2[^](#2_3_2)<a id=\"2_3_2\" ><\/a><br>\n","9c21093b":"Choosen instance refers to an **Survived male passenger of age 1 travelling in passenger class 3, embarked from S**. Let's see what and how our model predicts his survival.","1c1bf7ac":"---\n### Explaining Instance 3[^](#2_3_3)<a id=\"2_3_3\" ><\/a><br>","ba8de0cd":"###### Interpretation :\n\nModel predicted Will Die (Not survived). Biggest effect is person being a male; This has decreased his chances of survival significantly. Next, passenger class 3 also decreases his chances of survival while being 21 increases his chances of survival.","530718b3":"#### Explaining Instance 1[^](#2_3_1)<a id=\"2_3_1\" ><\/a><br>","9e153ff4":"## Interpreting the Model With Shapely Values[^](#2)<a id=\"2\" ><\/a><br>\n\n### 1. Import LIME package[^](#2_1)<a id=\"2_1\" ><\/a><br>","89a4e746":"---\n## Credits[^](#3)<a id=\"3\" ><\/a><br>\n* https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/building-trust-in-machine-learning-models\/\n* https:\/\/christophm.github.io\/interpretable-ml-book\/intro.html\n* https:\/\/blog.dominodatalab.com\/shap-lime-python-libraries-part-2-using-shap-lime\/","18540fdb":"###### Interpretation :\nModel predicted Will Survive. Although passenger class 3 and being a male passenger has decresed his chances of survival. Biggest effect has come from his age being 1 years old; This has increased his chances of survival significantly.","5f7ab1b5":"### 3. Use the explainer to explain predictions[^](#2_3)<a id=\"2_3\" ><\/a><br>","e995c0bf":"---\n# Thank you!\n## If you like the notebook and think that it helped you..PLEASE UPVOTE. It will keep me motivated :) :)","251869db":"# Explaining Random Forest Model With LIME\n\n### Hello Kaggler!, <span style=\"color:PURPLE\">Objective of this short kernal is to<\/span> <span style=\"color:red\">Interprete a Random Forest Model with LIME method.<\/span>\n\nTo make this very easy to grasp I have used infamouse Titanic data set to train the ML model.\n\n### Additionally, after reading this Kernel I hope that you would \n* Get an understanding How to use LIME library for a random forest classifier.\n* Get an understanding on how the model makes predictions using LIME method.\n\n\nThe intent here is not to build the best possible model but rather the focus is on the aspect of interpretability.","6ca958e9":"## Model Training[^](#0)<a id=\"1\" ><\/a><br>\n\nLet's train the RF model and get the accuracy measure.","cb48f3d0":"# Using LIME Package for understanding a Random Forest Classifier","095af083":"# Content\n1. [Dataset](#-1)\n1. [Data Preparation](#0)\n1. [Model Training](#1)\n1.  [Interpreting the Model With Shapely Values](#2)\n    1. [Import LIME package](#2_1)\n    1. [Create the Explainer](#2_2)\n    1. [Use the explainer to explain predictions](#2_3)\n        1. [Explaining Instance 1](#2_3_1)\n        1. [Explaining Instance 2](#2_3_2)\n        1. [Explaining Instance 3](#2_3_3)\n        1. [Explaining Instance 4](#2_3_4)\n1. [Credits](#3)","b4570645":"Choosen instance refers to an **Survived female passenger of age 24 travelling in passenger class 1, embarked from C**. Let's see what and how our model predicts her survival.","1143d4a0":"###### Interpretation :\nModel predicted 1 (Fully confident that passenger survives). Biggest effect is person being a female; This has increased her chances of survival significantly. Next, passenger class 1 and Fare>31 has also increases her chances of survival.","21e2ac3a":"---\n### Explaining Instance 4[^](#2_3_4)<a id=\"2_3_4\" ><\/a><br>","6515789d":"Choosen instance refers to an Unlucky (not survived) **Male passenger of age 21 travelling in passenger class 3, embarked from Q**. Let's see what and how our model predicts his survival.","e05bdd57":"### 2. Create the Explainer[^](#2_2)<a id=\"2_2\" ><\/a><br>","c18e2ef7":"###### Interpretation :\nModel predicted Will Die. Biggest effect is person being a female; This has increased her chances of survival significantly. Fare value of 34.38 has also played a part incresing her chances. However, beign a passenger in class 3 and her age (48) has significantly decreased her chances of survival.","2bdb203b":"## Data Preparation [^](#0)<a id=\"0\" ><\/a><br>\nlet's get the datasets ready to put into training. Since our goal is not to make a better classiefier for the problem, let's train a simple model.\n\nFollowing steps are carried out in the following code block.\n1. Dropping unneeded Features\n    * Lets only use Pclass, Sex, Age, SibSp, Parch and Embarked features.\n2. Convert categorical variables into dummy\/indicator variables.\n    * Pclass, Sex and Embarked features needs to be converted.\n3. Filling Null Values\n4. Create X_train, Y_train, X_test, Y_test datasets","7d5f3ccf":"## Dataset[^](#-1)<a id=\"-1\" ><\/a><br>\n\nFor the demonstration I have used infamouse [**Titanic dataset**](https:\/\/www.kaggle.com\/c\/titanic) to train the Random Forest Classifier. Objective of this problem to predict the survival for passengers in Titanic."}}