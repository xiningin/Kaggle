{"cell_type":{"1d5d0079":"code","636e4b0a":"code","dddc8203":"code","488544bf":"code","0d4bccbf":"code","ff3315eb":"code","5b72829d":"code","c9be5f32":"code","67a213e6":"code","5911fbee":"code","91dd4e03":"code","ad298cff":"code","3c852532":"code","ee0bb38b":"code","21c3a459":"code","40cb2112":"code","5f67af7b":"code","b6a5548c":"code","4d7bd237":"code","9eea6341":"code","b3baa5ad":"code","8112bc03":"code","34fe51e2":"code","a0625593":"code","cb7e159f":"code","a9cbe720":"code","10ac4028":"code","c7a7df4d":"code","3a208e88":"code","64ec5e1e":"code","afa6cee0":"markdown","4ad1c835":"markdown","5b879696":"markdown","d0c28fed":"markdown","12df3559":"markdown","50b8abf0":"markdown","914ea700":"markdown","9013bbf3":"markdown","59ca725a":"markdown","743ec417":"markdown","4ecbe614":"markdown","8c6f9555":"markdown","8f4efcf4":"markdown","962ce0d7":"markdown","261e5a14":"markdown","3da3244b":"markdown","67c86369":"markdown","59968888":"markdown","761a7fca":"markdown","da2f2aac":"markdown","65028204":"markdown","2161d05a":"markdown"},"source":{"1d5d0079":"! pip install pycountry_convert","636e4b0a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom os import path, makedirs\nfrom matplotlib import pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nfrom pycountry_convert import country_alpha2_to_continent_code, country_name_to_country_alpha2\nfrom geopy.geocoders import Nominatim\nimport bqplot as bq\n\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.io as pio\nimport plotly.express as px\npio.templates.default = \"plotly_white\"\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 5000)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dddc8203":"geolocator = Nominatim(user_agent='nom123')\ndef geolocate(country):\n    \n    try:\n        # Geolocate the center of the country\n        loc = geolocator.geocode(country)\n        # And return latitude and longitude\n        return (loc.latitude, loc.longitude)\n    except:\n        # Return missing value\n        return np.nan\n    \ndef get_continent(col):\n    \"\"\" Country to Continent\"\"\"\n    try:\n        cn_continent = country_alpha2_to_continent_code(col)\n    except:\n        cn_continent = 'Unknown' \n    return cn_continent\n\ndef get_country(col):\n    \"\"\" Country code\"\"\"\n    try:\n        cn_a2_code =  country_name_to_country_alpha2(col)\n    except:\n        cn_a2_code = 'Unknown' \n    return cn_a2_code","488544bf":"import re\ndf = pd.read_csv('\/kaggle\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv', low_memory=False)\n\nquestions_list = {k: v  for k, v in df.iloc[0].to_dict().items()}\ncolumn_remap = {}\nmultiple_questions = []\nsingle_questions = []\nfor k, v in df.iloc[0].to_dict().items():\n    if 'Part' in k:\n        multiple_questions.append(k.split('_')[0])\n        column_remap[k] = re.sub('[^A-Za-z0-9]+_', '',f'{\"_\".join(k.split(\"_\")[:-2])}_{v.split(\"-\")[-1].strip()}')\n    else:\n        column_remap[k] = k\n        single_questions.append(k)\n\nmultiple_questions = np.unique(multiple_questions)\ncolumn_remap['Time from Start to Finish (seconds)'] = 'Duration'\ndf.rename(columns = column_remap, inplace=True)\ndf = df[1:]\n\ndf['Duration'] = df['Duration'].astype('int')\ndf['Duration'] = np.floor(df['Duration']\/60)\ndf['Country'] = df['Q3'].apply(get_country)\ndf['Continent'] = df['Country'].apply(get_continent)\n#df['Centroid'] = df['Country'].apply(geolocate)\n#df['Lat'] = df['Centroid'].apply(lambda x: x[0])\n#df['Lon'] = df['Centroid'].apply(lambda x: x[1])\n\neducation_remap = {'Bachelor\u2019s degree': 'BSc', 'Master\u2019s degree': 'MSc', 'Doctoral degree':'PhD',\n       'I prefer not to answer':'unknown',\n       'Some college\/university study without earning a bachelor\u2019s degree':'No-Degree',\n       'No formal education past high school': 'High School', 'Professional doctorate': 'P-PhD'}\n\ndf['Q4'].replace(education_remap, inplace=True)\ndf['UserId'] = [f'user{i}' for i in range(df.shape[0])]\nfor col in multiple_questions:\n    columns = list(df.columns[df.columns.str.contains(col)])\n    df[columns] = df[columns].notnull().astype(int)\n    \nprint('Number of Users', df.shape[0])\nprint('Number of countries', df['Q3'].nunique())\n","0d4bccbf":"#questions_list","ff3315eb":"question_dict = {}\nfor k, v in questions_list.items():\n    if 'Part' in k:\n        k = '_'.join(k.split('_')[:-2])\n    if k in question_dict:\n        continue\n    else:\n        question_dict[k] = v.split('-')[0]\n#question_dict","5b72829d":"pd.DataFrame.from_dict(question_dict, orient='index').T","c9be5f32":"pd.DataFrame.from_dict({i:round(df['Duration'].quantile(i),2) for i in np.arange(0, 1, .02)}, \n                       orient='index').iplot(kind='scatter', theme='white', yTitle='Duration', xTitle='Quantile', logy=True)\ndf[df['Duration']<60]['Duration'].iplot(kind='hist', bins=100, theme='white')","67a213e6":"for col in ['Continent', 'Q1', 'Q2', 'Q4', 'Q5', 'Q6', 'Q15']:\n    df_ = df.groupby(col)[['UserId']].count().sort_values(by='UserId')\/df.shape[0]\n    fig = px.bar(df_.T,orientation='h', labels={'value':'', 'index':''},\n                 text=[[round(x[0],2) for x in df_.values]], title=question_dict[col] if col in question_dict else col)\n    fig.update_layout(legend=dict( yanchor=\"bottom\",y=-0.3,xanchor=\"left\",x=0.01,orientation=\"h\"))\n    fig.show()","5911fbee":"#px.bar(df.groupby('Q3')['UserId'].count().sort_values(), orientation='h')","91dd4e03":"#px.choropleth(df.groupby('Q3')['UserId'].count().reset_index(), locations='Q3', \n              #locationmode='country names', color='UserId')","ad298cff":"df_gender = (df.groupby('Q2')['UserId'].count()\/df.shape[0]).reset_index()\n\nfig = px.pie((df.groupby('Q2')['UserId'].count()\/df.shape[0]).reset_index(), values='UserId', \n       names='Q2',title='Gender Distribution',  height=300)\nfig.show()\n\ndf_continent_gender = df.groupby(['Continent', 'Q2'])['UserId'].count().reset_index()\ndf_continent_gender['total'] = df_continent_gender.groupby(['Continent'])['UserId'].transform('sum')\ndf_continent_gender['UserId'] = df_continent_gender['UserId']\/df_continent_gender['total']\nfig = px.bar(df_continent_gender.sort_values('Continent'), y='Continent', x='UserId', color='Q2', \n        title='Continent vs Gender Distribution')\nfig.show()\n\n#fig = make_subplots(rows=3, cols=3,specs=[[{}, {}, {}], [{},{}, {}], [{},{},{}]])\n#fig.add_trace(,row=1, col=1)\n#fig.show()\n","3c852532":"#df_country_gender =  df.groupby(['Q3', 'Q2'])['UserId'].count().reset_index()\n#df_country_gender['total'] = df_country_gender.groupby(['Q3'])['UserId'].transform('sum')\n#df_country_gender['UserId'] = df_country_gender['UserId'] \/df_country_gender['total']\n#\n#px.bar(df_country_gender[df_country_gender['Q2'].isin(['Man', 'Woman'])].sort_values('Q3'), y='Q3', x='UserId', \n#       facet_col='Q2', facet_col_wrap=2, height=500)\n","ee0bb38b":"#for i, group in df_country_gender.groupby('Q2'):\n #   fig = px.choropleth(group, locations='Q3', locationmode='country names', color='UserId', title=i)\n #   fig.show()","21c3a459":"def plot_gender_conti(df , col, p_analysis_col=['Continent', 'Q2']):\n    fig = px.bar((df.groupby(col)['UserId'].count()\/df.shape[0]).reset_index().sort_values('UserId'), x='UserId', \n           y=col,title=question_dict[col],  height=300)\n    fig.show()\n    \n    # distribution across continents conditioned on col\n    for col_ in p_analysis_col:\n        df_ = df.groupby([col_, col])['UserId'].count().reset_index()\n        df_['total'] = df_.groupby([col])['UserId'].transform('sum')\n        df_['UserId'] = df_['UserId']\/df_['total']\n        fig = px.bar(df_, y=col, x='UserId', color=col_, barmode='relative', \n                     color_discrete_sequence=px.colors.qualitative.Dark24)\n        fig.show()\n    \n    # Distribution of users conditioned on a continent\n    df_continent = df.groupby(['Continent', col])['UserId'].count().reset_index()\n    df_continent['total'] = df_continent.groupby(['Continent'])['UserId'].transform('sum')\n    df_continent['UserId'] = df_continent['UserId']\/df_continent['total']\n\n    fig, axes = plt.subplots(1,2, figsize=(26, 6))    \n    sns.heatmap(df_continent.pivot(columns=col, values='UserId', index='Continent').fillna(0), ax=axes[0], \n                annot=True, fmt='.2f')\n    \n    # Distribution of users conditioned on gender\n    df_gender = df.groupby(['Q2', col])['UserId'].count().reset_index()\n    df_gender['total'] = df_gender.groupby(['Q2'])['UserId'].transform('sum')\n    df_gender['UserId'] = round(df_gender['UserId']\/df_gender['total'],2)\n    sns.heatmap(df_gender.pivot(columns=col, values='UserId', index='Q2').fillna(0), ax=axes[1], annot=True, fmt='.2f')\n    \n    #df_ = df.groupby(['Q2', col])['UserId'].count().reset_index()\n    #df_['total'] = df_.groupby([col])['UserId'].transform('sum')\n    #df_['UserId'] = df_['UserId']\/df_['total']\n    #fig = px.bar(df_, y=col, x='UserId', color='Q2', barmode='relative', orientation='h')\n    #fig.show()\n    #fig.show()\n    \n    \n    # distribtion of users conditioned on gender and education\n    df_ = df.groupby(['Continent','Q2', col])['UserId'].count().reset_index()\n    df_['total'] = df_.groupby([col, 'Q2'])['UserId'].transform('sum')\n    df_['UserId'] = df_['UserId']\/df_['total']\n    fig = px.bar(df_, x=col, y='UserId', color='Continent', facet_col='Q2', facet_col_wrap=4, barmode='relative')\n    fig.show()\n    \n    ","40cb2112":"plot_gender_conti(df, 'Q4', ['Continent', 'Q5'])","5f67af7b":"plot_gender_conti(df, 'Q1', ['Continent', 'Q2'])","b6a5548c":"plot_gender_conti(df, 'Q5', ['Continent', 'Q4', 'Q6'])","4d7bd237":"plot_gender_conti(df, 'Q6', ['Continent','Q2'])","9eea6341":"plot_gender_conti(df, 'Q15', ['Continent','Q2', 'Q5', 'Q1'])","b3baa5ad":"plot_gender_conti(df, 'Q25', ['Continent','Q2','Q1','Q4','Q5', 'Q15', 'Q20'])","8112bc03":"plot_gender_conti(df, 'Q23', ['Continent', 'Q5','Q6','Q20' ,'Q25'])","34fe51e2":"plot_gender_conti(df, 'Q20', ['Continent', 'Q2','Q5','Q6', 'Q15', 'Q25'])","a0625593":"plot_gender_conti(df, 'Q21')","cb7e159f":"def plot_gender_conti_expr_field(data , col):\n    selected_columns = list(data.columns[data.columns.str.contains(col)])\n    meta_columns =  ['Continent', 'Q2', 'Q4' , 'Q5', 'Q1', 'Q6',  'Q15' ]\n    agg_dict = {i:'sum' for i in selected_columns}\n    agg_dict.update({'UserId':'count'})\n    df = data[selected_columns + meta_columns + ['UserId']]\n    \n    # distribution of the user for the column\n   \n    fig = px.bar((df[selected_columns].sum()\/df.shape[0]).sort_values().T, title='',  height=300, \n                 orientation='h',labels={'value': 'user ratio'})\n    fig.show()\n    \n    fig, axes = plt.subplots(3,2, figsize=(20,18), sharey=False, sharex=False)\n    for ax, col in zip(np.array(axes).reshape(-1), meta_columns):\n        df_ = df.groupby(col).agg(agg_dict)\n        df_[selected_columns] = df_[selected_columns].div(df_['UserId'], axis=0)\n        sns.heatmap(df_[selected_columns], ax=ax,fmt='.2f', annot=True,)\n    \n    #df_ = df.groupby('Q2').agg(agg_dict)\n    #df_[selected_columns] = df_[selected_columns].div(df_['UserId'], axis=0)\n    #sns.heatmap(df_[selected_columns], ax=ax[1],fmt='.2f', annot=True)\n    \n    ","a9cbe720":"plot_gender_conti_expr_field(df, 'Q7')\n","10ac4028":"plot_gender_conti_expr_field(df, 'Q9')","c7a7df4d":"plot_gender_conti_expr_field(df, 'Q16')","3a208e88":"def plot_sankey_diagram(data, meta_columns , columns):\n    loop_columns = ['Continent'] + list(columns)\n    list_df = []\n    \n    df_users = data.groupby(meta_columns[0])['UserId'].count().reset_index().rename(\n        columns={meta_columns[0]:'target', 'UserId':'value'})\n    df_users['source'] = 'Users'\n    list_df.append(df_users)\n\n    for i, j in zip(meta_columns[0:-1], meta_columns[1:]):\n        df_ = df.groupby([i,j])['UserId'].count().reset_index().rename(\n        columns={j:'target', 'UserId':'value', i:'source'})\n        list_df.append(df_)\n    \n    selected_columns = list(df.columns[df.columns.str.contains(columns[0])])\n    df_ = df[selected_columns + meta_columns + ['UserId']]\n    df_ = df_.groupby(meta_columns[-1])[selected_columns].sum()\n    df_ = df_.stack().reset_index().rename(columns={meta_columns[-1]:'source', 'level_1':'target', 0:'value'})\n    list_df.append(df_)\n    \n    for i, j in zip(columns[:-1], columns[1:]):\n        print(i, j)\n        selected_columns = list(df.columns[df.columns.str.contains(i)])\n        df_1 = df[selected_columns]\n        \n        selected_columns = list(df.columns[df.columns.str.contains(j)])\n        df_2 = df[selected_columns]\n        \n        df_ = pd.DataFrame(np.dot(df_1.values.T, df_2.values), index=df_1.columns, \n                           columns=df_2.columns).stack().reset_index().rename(\n            columns={'level_0':'source', 'level_1':'target', 0:'value'})\n        \n        list_df.append(df_)\n    \n    df_ = pd.concat(list_df)\n    all_nodes = list(df_['source'].unique()) + list(df_['target'].unique())\n    source_idx = [all_nodes.index(x) for x in df_['source'].values]\n    target_idx = [all_nodes.index(x) for x in df_['target'].values]\n    weights = df_['value'].values\n    #return df_\n\n    fig = go.Figure(data=go.Sankey( node={'label': all_nodes},link={'source':source_idx, 'target':target_idx, \n                                                                    'value':weights}))\n    fig.show()\n    ","64ec5e1e":"plot_sankey_diagram(df, ['Continent', 'Q25'], ['Q7'])","afa6cee0":"## Education ","4ad1c835":"- EU and NA has more experienced users, confirms early adoption of ML \n- Asia has the most users who havnt used ML, could be emphasis on software tech and consultancy industry overpowering R&D \n- Looking only at women users with >10 yr exp, EU and NA has more experineced users when compared to Asia. Asia's % increases with decreasing expereince , showing increasing popularity of ML among women\n- ML expereince is mainly restricted to research, student and data science fields\n- ML methods are more popular among younger people","5b879696":"- Popularity of ML is growing but is still in nascent stage\n- ","d0c28fed":"# Analysis of Questions where only one option can be selected","12df3559":"The Challenge is to tell a story rather than focus on any results. Here we try to tell a story on based on Continents, Gender and Age.\n\nFew interesting ideas that we had but havent been implemented yet \n- Graph signal processing and identification of subgroups and hidden relationships\n- Subspace representation of data using GNN and then use it as a recomender system","50b8abf0":"# General Properties","914ea700":"## Role","9013bbf3":"## Coding Experience","59ca725a":"- Suprisingly EU has lower Women participation, when compared to NA, Asia and Africa\n- SA has the least women participation\n- SA have the least users who didnt wished to disclose their gender or non-binary gender, shows a relactance to accept third gender","743ec417":"- Asia has more Novice coders when compared to EU and NA\n- Women participation increases with decreasing coding experience, showing impact of policies to include women in STEM ","4ecbe614":"## Age","8c6f9555":"- 90p of the people finished the survey within an hour, on an average <10mins was taken by majority of the people\n","8f4efcf4":"## ML Experience ","962ce0d7":"- Asia and Africa has more younger users than others\n- % of Asia and Africa decreases with the age, showing a younger population and a growing economy\n- Women participation is increasing in younger demographics, showing influence of policies and changing times\n","261e5a14":"- In military and security application NA leads the way, \n- Asia leads the way in e-commerce applications and manufacturing industry\n- Asia leads the way in adaptation of ML in education (fierce competation in asia and emergence of e-education companies might the reason)","3da3244b":"# Multiple Options","67c86369":"- Majority of the users have a degree(92%)\n- Asia leads in all metrics, this can be attibuted to larger population of countries in asia. Number would make sense only when considered relative to their overall population\n- Majority of the users in Europe have masters\n- Majority of the users in Asia has masters or bachelors\n- Asia and Africa lags in number of PhD and Masters users\n- Looking at users with highschool degrees, NA has the least users, where as EU has more users. This can be attribted to the programs like real schule or study with work in germany ","59968888":"# Introduction","761a7fca":"- 13% of compensations are < 1000, which could be due to starter jobs in Asia and student jobs which usually pay around 1000\n- $> 200k users from NA start to dominate, could be due to salary limits in EU and lower salary in Asia and other countries\n- Except for NA and Africa , no other continent has women users earning >500k, which highlights gender pay descrimination E\n- EU dominates in pay scale 30-80k, which is the ball park region of majority of the salaries in EU\n","da2f2aac":"The challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A \u201cstory\u201d could be defined any number of ways, and that\u2019s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about! ","65028204":"- Students and Data scientist are the major users of the platform\n- A lot of students are from Asia, but when it comes to professional roles, Europe and NA takes a lead, showing early adoption of tech in these area.\n- Majority of the research engineers have a PhD\n- MSc has majorty contribution in many roles\n- Non coders are majorly grouped in management , product owner and business analyst roles","2161d05a":"- Asia accounts for more users but this can be attributed to large user pool, comparison is only meaningful when we compare relative numbers\n- Younger demographic(~60%) is using kaggle more than the older\n- Men: Women :: 1:4\n- Majority of the users have a bachelors or masters or doctrate\n- Students are major users, which fits with previously observed demographic and also shows that kaggle is a playground for new users and also a platform where people come to learn\n- 35% dont use ML which could indicate them using the platform as a learning and playground setup, which is also reflected in their coding experience"}}