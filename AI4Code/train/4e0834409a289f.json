{"cell_type":{"edee0b55":"code","2e35c94c":"code","c8838c50":"code","8473636e":"code","8c52e7fc":"code","9a1bcfb5":"code","6e9ebb70":"code","7609e497":"code","1c675892":"code","bf76ed3f":"code","49dc6627":"code","648ea1d6":"code","813dee1e":"code","01b1f4bb":"code","a6d63a44":"markdown","491c7e52":"markdown","5e58a2e6":"markdown","70893d8d":"markdown"},"source":{"edee0b55":"import random\n\nSEED = 32\nrandom.seed(SEED)\n\nimport numpy as np \nimport pandas as pd\nimport spacy\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import  f1_score\n\nfrom torch import nn\nimport torch\nfrom torchtext import data\nfrom torch.nn  import functional as F\nimport torch.optim as  optim \nif torch.cuda.is_available():  \n  dev = \"cuda:0\" \n  print(\"gpu up\")\nelse:  \n  dev = \"cpu\"  \ndevice = torch.device(dev)","2e35c94c":"\"\"\"\n\nthose are the libraries I use for processing text\n\n\"\"\"\n\nimport nltk\nnltk.download(\"punkt\")\n\nimport re\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.lang.en import English\nnlp = English()\n\ntokenizer = Tokenizer(nlp.vocab)\n\nfrom nltk import word_tokenize,sent_tokenize\nfrom nltk.stem  import PorterStemmer\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nnltk.download('stopwords')\nstops = stopwords.words(\"english\")\n\n\ndef removepunc(my_str): # function to remove punctuation\n    punctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n    no_punct = \"\"\n    for char in my_str:\n        if char not in punctuations:\n            no_punct = no_punct + char\n    return no_punct\n\ndef hasNumbers(inputString):\n    return bool(re.search(r'\\d', inputString))\nsnowstem = SnowballStemmer(\"english\")\nportstem = PorterStemmer()\n","c8838c50":"traindata = pd.read_csv(\"\/kaggle\/input\/hate-speech-detection\/toxic_train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/hate-speech-detection\/toxic_test.csv\")\ntraindata.drop(\"Unnamed: 0\",axis=1,inplace=True)\ntest.drop(\"Unnamed: 0\",axis=1,inplace=True)","8473636e":"\"\"\"\nthis function is the tokenizer we are using, it does basic processing also  like ,\nLowercase the text\nremoving punctuation, stop words and numbers,\nit also removes extra spaces and unwanted characters (I use regex for that)\n\n\nbefore using the tokenizer I was testing it on the train dataframe manually  \n\"\"\"\n\ndef myTokenizer(x):\n return  [snowstem.stem(word.text)for word in \n          tokenizer(removepunc(re.sub(r\"\\s+\\s+\",\" \",re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"\\r+\\n+]\",\" \",x.lower()))).strip()) \n          if (word.text not in stops and not hasNumbers(word.text)) ]\n","8c52e7fc":"\"\"\"\nhere I'm using the torchtext fields and dataset classes they can ease the work to get\nthe dataset ready for the pytorch model\n\nthe class DataFrameDataset is the easiest way I found to turn a dataframe into a torchtext dataset\n\nthis cell will take sometime to finish\n\"\"\"\n\nTEXT = data.Field(tokenize=myTokenizer,batch_first=True,fix_length=140)\nLABEL = data.LabelField(dtype=torch.float ,batch_first=True)\n\n\nclass DataFrameDataset(data.Dataset):\n\n    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n        fields = [('comment_text', text_field), ('toxic', label_field)]\n        examples = []\n        for i, row in df.iterrows():\n            label = row.toxic \n            text = row.comment_text\n            examples.append(data.Example.fromlist([text, label], fields))\n\n        super().__init__(examples, fields, **kwargs)\n  \n\ntorchdataset = DataFrameDataset(traindata, TEXT,LABEL)\ntorchtest = DataFrameDataset(test, TEXT,LABEL)","9a1bcfb5":"train_data, valid_data = torchdataset.split(split_ratio=0.8, random_state = random.seed(SEED))","6e9ebb70":"\"\"\"\nthis cell build the vocab which means it get all the used words and if also ignores any word \nthat only appeared less than 3 times\n\"\"\"\nTEXT.build_vocab(train_data,min_freq=3)  \nLABEL.build_vocab(train_data)\n","7609e497":"#No. of unique tokens in text\nprint(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n\n#No. of unique tokens in label\nprint(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n\n#Commonly used words\nprint(TEXT.vocab.freqs.most_common(10))  \n","1c675892":"#set batch size\nBATCH_SIZE = 128\n\n\"\"\"\nwe are using batches for validation and test set because of memory usage we can't pass the whole set at once \n\"\"\"\n\n\ntrain_iterator,valid_iterator,test_iterator= data.BucketIterator.splits(\n    (train_data,valid_data,torchtest), \n    batch_size = BATCH_SIZE,\n    device = device,\n    sort =False,\nshuffle=False)\n","bf76ed3f":"\n\"\"\"\none major point here is that I encoded the embeddings in a different way \nI made an embedding layer for the position then I concatenated position embeddings with the word embeddings \njust thought it could be a usefull way to encode the positions \n\nhad to reshape the output of the transformer layer to get the prediction\n\"\"\"\nclass TextTransformer(nn.Module):\n  def __init__(self):\n    super(TextTransformer,self).__init__()\n    self.wordEmbeddings = nn.Embedding(len(TEXT.vocab),140)\n    self.positionEmbeddings = nn.Embedding(140,20)\n    self.transformerLayer = nn.TransformerEncoderLayer(160,8) \n    self.linear1 = nn.Linear(160,  64)\n    self.linear2 = nn.Linear(64,  1)\n    self.linear3 = nn.Linear(140,  16)\n    self.linear4 = nn.Linear(16,  1)\n  def forward(self,x):\n    positions = (torch.arange(0,140).reshape(1,140) + torch.zeros(x.shape[0],140)).to(device) \n    # broadcasting the tensor of positions \n    sentence = torch.cat((self.wordEmbeddings(x.long()),self.positionEmbeddings(positions.long())),axis=2)\n    attended = self.transformerLayer(sentence)\n    linear1 = F.relu(self.linear1(attended))\n    linear2 = F.relu(self.linear2(linear1))\n    linear2 = linear2.view(-1,140) # reshaping the layer as the transformer outputs a 2d tensor (or 3d considering the batch size)\n    linear3 = F.relu(self.linear3(linear2))\n    out = torch.sigmoid(self.linear4(linear3))\n    return out\n\nmyTransformer = TextTransformer()\nmyTransformer.to(device)\n\n    \n","49dc6627":"def calculateMetrics(ypred,ytrue):\n  acc  = accuracy_score(ytrue,ypred)\n  f1  = f1_score(ytrue,ypred)\n  f1_average  = f1_score(ytrue,ypred,average=\"macro\")\n  return \" f1 score: \"+str(round(f1,3))+\" f1 average: \"+str(round(f1_average,3))+\" accuracy: \"+str(round(acc,3))\n  ","648ea1d6":"\"\"\"\nusing adagrad because it assign bigger updates to less frequently updated weights \n(like words that are not used many times)\n\n\"\"\"\n\noptimizer = optim.Adagrad(myTransformer.parameters(),lr = 0.001)\n\nfor i in range(20):\n  trainpreds = torch.tensor([])\n  traintrues = torch.tensor([])\n  for  batch in train_iterator:\n    X = batch.comment_text\n    y = batch.toxic\n    myTransformer.zero_grad()\n    pred = myTransformer(X).squeeze()\n    trainpreds = torch.cat((trainpreds,pred.cpu().detach()))\n    traintrues = torch.cat((traintrues,y.cpu().detach()))\n    err = F.binary_cross_entropy(pred,y)\n    err.backward()\n    optimizer.step()\n  err = F.binary_cross_entropy(trainpreds,traintrues)\n  print(\"train BCE loss: \",err.item(),calculateMetrics(torch.round(trainpreds).numpy(),traintrues.numpy()))\n \n\n  valpreds = torch.tensor([])\n  valtrues = torch.tensor([])\n  for batch in valid_iterator:\n    X = batch.comment_text\n    y = batch.toxic\n    valtrues = torch.cat((valtrues,y.cpu().detach()))\n    pred = myTransformer(X).squeeze().cpu().detach()\n    # print(valtrues.shape)\n    valpreds = torch.cat((valpreds,pred))\n  err = F.binary_cross_entropy(valpreds,valtrues)\n  print(\"validation BCE loss: \",err.item(),calculateMetrics(torch.round(valpreds).numpy(),valtrues.numpy()))\n  ","813dee1e":"\"\"\"\nnow getting the results on the test set\n\"\"\"\n\ntestpreds = torch.tensor([])\ntesttrues = torch.tensor([])\nfor batch in test_iterator:\n    X = batch.comment_text\n    y = batch.toxic\n    testtrues = torch.cat((testtrues,y.cpu().detach()))\n    pred = myTransformer(X).squeeze().cpu().detach()\n    # print(valtrues.shape)\n    testpreds = torch.cat((testpreds,pred))\nerr = F.binary_cross_entropy(testpreds,testtrues)\nprint(\"test BCE loss: \",err.item(),calculateMetrics(torch.round(testpreds).numpy(),testtrues.numpy()))\n  ","01b1f4bb":"test[\"predicted\"] = torch.round(testpreds).numpy()\n\n\n\"\"\"\nthis shows that the model understands the language well \n\n\"\"\"\n\ntest[test.predicted==1].iloc[32:37]\n","a6d63a44":"as this will be a classification task, we will just need the encoder part of the transformer, so I used the transformer encoder layer from pytorch","491c7e52":"I will be using  torchtext it's really good for preparing the data, it has a good documentation also \nthose are links about an example using torchtext and a tutorial\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/first-text-classification-in-pytorch\/\n\nhttp:\/\/mlexplained.com\/2018\/02\/08\/a-comprehensive-tutorial-to-torchtext\/","5e58a2e6":"The goal of this kernel is to train a simple transformer from only pytorch to classify hate speech in comments\nto understand how the transformer works, I would recommend this video it walks through the Attention paper and explains it well\n\nhttps:\/\/www.youtube.com\/watch?v=U0s0f995w14&t=2522s","70893d8d":"so the final scores on validation are  \n\nvalidation BCE loss:  0.137 f1 score: 0.706 f1 average: 0.84 accuracy: 0.952"}}