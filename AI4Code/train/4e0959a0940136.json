{"cell_type":{"ca3c96f0":"code","173c56e9":"code","a0e3eb62":"code","b2badb0a":"code","8026d139":"code","d3a7ae25":"code","70ea1caa":"code","d920a8a1":"code","5a99f2c3":"code","7f257b29":"code","4620408b":"code","3e4de8a8":"code","2851806f":"code","f90eeb84":"code","8eb57ac5":"code","31cc519f":"code","4c1ddf76":"code","fb768470":"code","ddadb888":"code","262e03c0":"code","dbb25b21":"code","13707a3f":"code","3f366fee":"code","bada0c5e":"code","940f1537":"code","ef9a66e6":"code","5ee77279":"code","242e68c8":"code","eff1585c":"code","a6ece8bb":"code","8043e2f0":"code","e3903712":"code","e7dbd69b":"code","547ec9b4":"code","635b7270":"code","f0a6a71d":"code","f66919af":"code","43fed48e":"code","5c733abb":"code","0bb7a210":"code","0d61ed57":"code","d71e7016":"code","331bee3f":"code","249268b2":"code","4123f646":"code","4d6d4b0c":"code","294d0585":"code","5ecfd528":"code","65242f5c":"code","7128289c":"code","55ad5ab0":"code","9356fefa":"code","f3336ef4":"code","e548fc32":"code","0ac933dd":"code","8e3915e2":"code","43b0ccfd":"code","7e550128":"code","ee5384f8":"code","0f44d9ff":"code","9e8ed544":"code","93ef2b06":"code","70e94e36":"code","703cc699":"code","2589e56a":"code","93c92195":"code","59da3f44":"code","925c6301":"code","59293ebb":"code","d2cb5bbd":"code","df7462ee":"code","a26d6a79":"code","9b1146f5":"code","84faac4b":"code","e1fe3273":"code","98938488":"code","c27cf639":"code","83689c4e":"code","856bef33":"markdown","c7ef54fe":"markdown","c97a1633":"markdown","2bb79305":"markdown","87da9f60":"markdown","d691d409":"markdown","e653b13d":"markdown","0e47eb57":"markdown","a65f2b0f":"markdown","654b244c":"markdown","a1b6b5fa":"markdown","045e27f6":"markdown","0c324276":"markdown","f03d95e8":"markdown","2b1738d6":"markdown","1c91380e":"markdown","0199c487":"markdown","3547268b":"markdown","4eea8c6b":"markdown","c1fca34d":"markdown","1f263d49":"markdown","93e3ae8d":"markdown","f0215910":"markdown","bfc24f7e":"markdown","b8b39746":"markdown","f4f67b03":"markdown","e09e6e90":"markdown","d5b86fe6":"markdown","93c74c0a":"markdown","907086a1":"markdown","aa40d08e":"markdown","515b2658":"markdown","1d88524b":"markdown","5d3b497a":"markdown","f8f08467":"markdown","866a95e8":"markdown","91569853":"markdown","0b8d648a":"markdown","6146a4d1":"markdown","e9efdf2f":"markdown","77729bef":"markdown","976f3e84":"markdown","46465208":"markdown","15ba33d9":"markdown","81563d53":"markdown","3847c8ba":"markdown","aa3d68f7":"markdown","e32122b5":"markdown","bc6a5a51":"markdown","5ac4c377":"markdown","3a0e916a":"markdown","ff834f3c":"markdown","eda6e132":"markdown","b738d165":"markdown","ee045eda":"markdown","5b6fcea1":"markdown"},"source":{"ca3c96f0":"import numpy as np \nimport pandas as pd \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport pandas\nimport seaborn as sns \nfrom sklearn import preprocessing\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import RegexpTokenizer\nfrom collections import Counter \nimport re\nimport string\nimport matplotlib.cm as cm\nfrom matplotlib import rcParams\nfrom prettytable import PrettyTable\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression","173c56e9":"cols = ['class','tweet_text']\npositive = pd.read_csv('..\/input\/arabic-twitter-sentiment-analysis\/Arabic_tweets_positive_20190413.tsv',sep='\\t', error_bad_lines = False ,header=None, names=cols)","a0e3eb62":"positive.head(10)","b2badb0a":"print(len(positive))","8026d139":"positive['tweet_text'].head(10)","d3a7ae25":"positive['class'].head(10)","70ea1caa":"positive[positive.isnull().any(axis=1)].head()","d920a8a1":"np.sum(positive.isnull().any(axis=1))","5a99f2c3":"positive.isnull().any(axis=0)","7f257b29":"positive.info()","4620408b":"for letter in '#.][!XR':\n    positive['tweet_text'] = positive['tweet_text'].astype(str).str.replace(letter,'')","3e4de8a8":"positive.head(10)","2851806f":"arabic_punctuations = '''`\u00f7\u00d7\u061b<>_()*&^%][\u0640\u060c\/:\"\u061f.,'{}~\u00a6+|!\u201d\u2026\u201c\u2013\u0640'''\nenglish_punctuations = string.punctuation\npunctuations_list = arabic_punctuations + english_punctuations\n\ndef remove_punctuations(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)","f90eeb84":"def normalize_arabic(text):\n    text = re.sub(\"[\u0625\u0623\u0622\u0627]\", \"\u0627\", text)\n    text = re.sub(\"\u0649\", \"\u064a\", text)\n    text = re.sub(\"\u0629\", \"\u0647\", text)\n    text = re.sub(\"\u06af\", \"\u0643\", text)\n    return text","8eb57ac5":"def remove_repeating_char(text):\n    return re.sub(r'(.)\\1+', r'\\1', text)","31cc519f":"def processPost(tweet): \n\n    #Replace @username with empty string\n    tweet = re.sub('@[^\\s]+', ' ', tweet)\n    \n    #Convert www.* or https?:\/\/* to \" \"\n    tweet = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))',' ',tweet)\n    \n    #Replace #word with word\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n\n    # remove punctuations\n    tweet= remove_punctuations(tweet)\n    \n    # normalize the tweet\n     #tweet= normalize_arabic(tweet)\n    \n    # remove repeated letters\n    tweet=remove_repeating_char(tweet)\n    \n    return tweet","4c1ddf76":"positive[\"tweet_text\"] = positive['tweet_text'].apply(lambda x: processPost(x)) # apply used to call the method processpost","fb768470":"tokenizer = RegexpTokenizer(r'\\w+')\npositive[\"tweet_text\"] = positive[\"tweet_text\"].apply(tokenizer.tokenize)","ddadb888":"positive[\"tweet_text\"].head(10)","262e03c0":"stopwords_list = stopwords.words('arabic')","dbb25b21":"stopwords_list","13707a3f":"print(len(stopwords_list))","3f366fee":"print(type(stopwords_list))","bada0c5e":"listToStr = ' '.join([str(elem) for elem in stopwords_list]) ","940f1537":"listToStr","ef9a66e6":"positive[\"tweet_text\"]=positive[\"tweet_text\"].apply(lambda x: [item for item in x if item not in stopwords_list])","5ee77279":"all_words = [word for tokens in positive[\"tweet_text\"] for word in tokens]\nsentence_lengths = [len(tokens) for tokens in positive[\"tweet_text\"]]\n\nVOCAB = sorted(list(set(all_words)))\n\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\nprint(\"Max sentence length is %s\" % max(sentence_lengths))","242e68c8":"counter = Counter(all_words)","eff1585c":"counter.most_common(35)","a6ece8bb":"counted_words = Counter(all_words)\n\nwords = []\ncounts = []\nfor letter, count in counted_words.most_common(10):\n    words.append(letter)\n    counts.append(count)","8043e2f0":"colors = cm.rainbow(np.linspace(0, 1, 20))\nrcParams['figure.figsize'] = 20, 10\n\nplt.title('Top words in positive')\nplt.xlabel('Count')\nplt.ylabel('Words')\nplt.barh(words, counts, color=colors)","e3903712":"cols = ['class','tweet_text']\nnegative = pd.read_csv('..\/input\/arabic-twitter-sentiment-analysis\/Arabic_tweets_negative_20190413.tsv',sep='\\t', error_bad_lines = False ,header=None, names=cols)","e7dbd69b":"negative.head(10)","547ec9b4":"print(len(negative))","635b7270":"negative['tweet_text'].head(10)","f0a6a71d":"negative['class'].head(10)","f66919af":"negative[negative.isnull().any(axis=1)].head()","43fed48e":"np.sum(negative.isnull().any(axis=1))","5c733abb":"negative.isnull().any(axis=0)","0bb7a210":"negative.info()","0d61ed57":"for letter in '#.][!XR':\n    negative['tweet_text'] = negative['tweet_text'].astype(str).str.replace(letter,'')","d71e7016":"negative.head(11)","331bee3f":"negative[\"tweet_text\"] = negative['tweet_text'].apply(lambda x: processPost(x)) # apply used to call the method processpost","249268b2":"tokenizer = RegexpTokenizer(r'\\w+')\nnegative[\"tweet_text\"] = negative[\"tweet_text\"].apply(tokenizer.tokenize)","4123f646":"negative[\"tweet_text\"].head(11)","4d6d4b0c":"negative[\"tweet_text\"]=negative[\"tweet_text\"].apply(lambda x: [item for item in x if item not in stopwords_list])","294d0585":"all_words = [word for tokens in negative[\"tweet_text\"] for word in tokens]\nsentence_lengths = [len(tokens) for tokens in negative[\"tweet_text\"]]\n\nVOCAB = sorted(list(set(all_words)))\n\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\nprint(\"Max sentence length is %s\" % max(sentence_lengths))","5ecfd528":"counter = Counter(all_words)","65242f5c":"counter.most_common(35)","7128289c":"counted_words = Counter(all_words)\n\nwords = []\ncounts = []\nfor letter, count in counted_words.most_common(10):\n    words.append(letter)\n    counts.append(count)","55ad5ab0":"colors = cm.rainbow(np.linspace(0, 1, 10))\nrcParams['figure.figsize'] = 20, 10\n\nplt.title('Top words in negative')\nplt.xlabel('Count')\nplt.ylabel('Words')\nplt.barh(words, counts, color=colors)","9356fefa":"final_data = pd.concat([positive, negative], axis=0)","f3336ef4":"final_data.head(15)","e548fc32":"print(len(final_data))","0ac933dd":"y=final_data['class']\ny.value_counts()","8e3915e2":"sns.countplot(data= final_data, x = \"class\")\nplt.show()","43b0ccfd":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    ngram_range=(1, 1),\n    max_features =10000)\n\nunigramdataGet= word_vectorizer.fit_transform(final_data['tweet_text'].astype('str'))\nunigramdataGet = unigramdataGet.toarray()\n\nvocab = word_vectorizer.get_feature_names()\nunigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\nunigramdata_features[unigramdata_features>0] = 1\n\nunigramdata_features.head()","7e550128":"pro= preprocessing.LabelEncoder()\nencpro=pro.fit_transform(final_data['class'])\nfinal_data['class'] = encpro","ee5384f8":"y=final_data['class']\nX=unigramdata_features","0f44d9ff":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=333)","9e8ed544":"nb=GaussianNB()\nnb= nb.fit(X_train , y_train)\nnb","93ef2b06":"y_pred = nb.predict(X_test)\nnb_1=nb.score(X_test, y_test)\nprint('Accuracy= {:.3f}'.format(nb.score(X_test, y_test)))","70e94e36":"print('Precision',round(f1_score(y_test, y_pred),2),'%')","703cc699":"rf_f1=round(f1_score(y_test, y_pred),2)\nprint('F1',round(f1_score(y_test, y_pred),2),'%')","2589e56a":"LR= LogisticRegression(penalty = 'l2', C = 1)\nLR= LR.fit(X_train , y_train)\nLR","93c92195":"y_pred = LR.predict(X_test)\nlr_1=LR.score(X_test, y_test)\nprint('Accuracy= {:.3f}'.format(LR.score(X_test, y_test)))","59da3f44":"print('Precision',round(f1_score(y_test, y_pred),2),'%')","925c6301":"rf_f1=round(f1_score(y_test, y_pred),2)\nprint('F1',round(f1_score(y_test, y_pred),2),'%')","59293ebb":"from sklearn.svm import SVC\nSVCModel = SVC(kernel= 'poly',max_iter=100,C=1.0,gamma='auto')# it can be also linear,poly,sigmoid,precomputed\nSVCModel.fit(X_train, y_train)","d2cb5bbd":"y_pred = SVCModel.predict(X_test)\nsvm =SVCModel.score(X_test, y_test)\nprint('Accuracy= {:.3f}'.format(SVCModel.score(X_test, y_test)))","df7462ee":"from sklearn.neural_network import MLPClassifier\nMLPClassifierModel = MLPClassifier(activation='relu', # can be also identity , logistic , relu\n                                   solver='lbfgs',  # can be also sgd , adam\n                                   learning_rate='constant', # can be also invscaling , adaptive\n                                   early_stopping= False,max_iter=200,\n                                   alpha=0.0001 ,hidden_layer_sizes=(100, 3),random_state=99)\nMLPClassifierModel.fit(X_train, y_train)","a26d6a79":"y_pred = MLPClassifierModel.predict(X_test)\nnn =MLPClassifierModel.score(X_test, y_test)\nprint('Accuracy= {:.3f}'.format(MLPClassifierModel.score(X_test, y_test)))","9b1146f5":"from sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier(n_estimators=100)\nclf.fit(X_train,y_train)","84faac4b":"y_pred = clf.predict(X_test)\nRF =clf.score(X_test, y_test)\nprint('Accuracy= {:.3f}'.format(clf.score(X_test, y_test)))","e1fe3273":"tv=['\u0647\u0630\u0627 \u0627\u0645\u0631 \u0645\u062d\u0632\u0646']\n","98938488":"x = word_vectorizer.transform(tv)\n","c27cf639":"pred=LR.predict(x)\npred=pro.inverse_transform(pred)\nprediction=pd.DataFrame(pred, columns=['Prediction']) \nprint (prediction)","83689c4e":"result=pd.DataFrame()\nresult['Text']=tv\nresult['Prediction']=prediction\nresult","856bef33":"# Stop words ","c7ef54fe":"# <img src=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--4KDpwn0C--\/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000\/https:\/\/dev-to-uploads.s3.amazonaws.com\/i\/t72k5kh3v868bjdgbnww.png\">","c97a1633":"# Checking null values count","2bb79305":"# Arabic_tweets_positive dataset ","87da9f60":"#### F1","d691d409":"# Getting tweet text","e653b13d":"#### F1","0e47eb57":"<div class=\"alert alert-block alert-info\">  \n<h1><strong>Introduction<\/strong><\/h1>\n    <p>In this kernel, we are going to analyse the Arabic langauge based tweets for the sentiment analysis. There are two types of sentiments which are positive and negative. The data contains the positive tweet text and negative tweet text for the binary classification. There are 18743 records\/tweets are labeled as positive and 29051 are negative tweets.<\/p>\n    <br>\n        <hr>\n      <b>Problem description: <\/b> \n    <hr>\n<ul>\n    <li>To build a robust system to classify the sentiments in arabic text based tweets using machine learning.<\/li>\n<\/ul>\n\n\n<hr>\n<b>Evolution measures: <\/b> \n<hr>\n<ul>\n<p> After training the model, we will apply the evaluation measures to check that how the model is getting predictions. We will use the following evaluation measures to evaluate the performance of the model:<\/p>\n    <li>Accuracy<\/li>\n    <li>Precision<\/li>\n     <li>Recall<\/li>\n     <li>F1 Measure<\/li>\n<\/ul>\n<hr>\n<b>Technical Approach<\/b>\n<hr>\n<p>We are using python language in the implementations and Jupyter Notebook that support the machine learning and data science projects. After training on the model, we will evaluate the model to check the performance of trained model and will select the final model for classification of arabic tweet sentiments<\/p>\n \n<hr>\n<b>Source of Data: <\/b> \n<hr> \n <a href=\"hhttps:\/\/www.kaggle.com\/imranzaman5202\/arabic-twitter-sentiment-analysis\">https:\/\/www.kaggle.com\/imranzaman5202\/arabic-twitter-sentiment-analysis<\/a>\n   \n<\/div>","a65f2b0f":"# normalize_arabic","654b244c":"<div class=\"alert alert-block alert-info\">  \n    <h1><strong>If you like my work, please upvote ^ \ud83d\udc4d my kernel and dataset as well so that i will be motivated to share more content to Kaggle community. Thanks \ud83d\ude0d<\/strong><\/h1>\n    <i><\/i>\n<\/div>","a1b6b5fa":"# LogisticRegression Algorithm","045e27f6":"# applying processPost function for preprocessing","0c324276":"# Arabic_tweets_negative dataset ","f03d95e8":"# --------Trainig and Testing with Machine Learning Algorithms ----------------","2b1738d6":"#### Precision","1c91380e":"# Removing stop words","0199c487":"# Count of positve tweets","3547268b":"# Preprocess data","4eea8c6b":"# Tweet text information","c1fca34d":"# Dataset analysis","1f263d49":"# Showing prediction with tweet text","93e3ae8d":"# Combining the positive and negative classes and tweets text","f0215910":"<div class=\"alert alert-block alert-success\">  \n<h1><center><strong>Conclusion \ud83d\udcdd<\/strong><\/center><\/h1>\n    <p>\n<li>We used this dataset  <a href=\"hhttps:\/\/www.kaggle.com\/imranzaman5202\/arabic-twitter-sentiment-analysis\">https:\/\/www.kaggle.com\/imranzaman5202\/arabic-twitter-sentiment-analysis<\/a> and explored the data with different ways.<\/li>\n        <li>We prepared the data and extract the features.<\/li>\n          <li>We trained model based on different machine learning models <\/li>\n        <li>We evaluated the model with accuracy, precision, recall and F1 measure<\/li>\n        <li>We selected the best model and perform classification on arabic tweet sentiments<\/li>\n        <\/p>\n<\/div>","bfc24f7e":"# Naive Bayes Algorithm","b8b39746":"# Features Extraction from tweets text with TFIDF unigram","f4f67b03":"# top 25 words in negative","e09e6e90":"# Dataset analysis","d5b86fe6":"# Getting target class","93c74c0a":"# cleaning tweet text","907086a1":"# encoding class as 1 for Postive class and 0 for negative class","aa40d08e":"# Spliting Dataset into 70% Training and 30% Testing","515b2658":"#### Accuracy","1d88524b":"# Getting tweet text","5d3b497a":"# Getting target class","f8f08467":"# top 10 words in positive","866a95e8":"# cleaning tweet text","91569853":"# Prediction on sample text","0b8d648a":"# Getting Tokenize the tweet text","6146a4d1":"#### Precision","e9efdf2f":"# processPost for applying all functions","77729bef":"# showing top 5 records","976f3e84":"#### Accuracy","46465208":"# Count of each target class","15ba33d9":"# Tweet text information","81563d53":"# By getting features and Class","3847c8ba":"# python libraries","aa3d68f7":"# Checking null values count","e32122b5":"# applying processPost function for preprocessing","bc6a5a51":"# total count of final data","5ac4c377":"# showing top 5 records","3a0e916a":"# Count of negative tweets","ff834f3c":"# Checking null values","eda6e132":"# Checking null values","b738d165":"# Removing stop words","ee045eda":"# Getting Tokenize the tweet text","5b6fcea1":"# remove_repeating_char"}}