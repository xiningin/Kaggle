{"cell_type":{"40bed669":"code","05ffde77":"code","2aa3383e":"code","d10c7d0b":"code","929a2e4a":"code","df92d64f":"code","a1401fe8":"code","27ac07f2":"code","5f504e63":"code","287a59b8":"code","b8e8ab6f":"code","d7ac6064":"code","6727f08e":"code","997b6b24":"code","2471fb41":"markdown","af1007e7":"markdown","bb13ac73":"markdown","6feb8a45":"markdown","bd060b0f":"markdown","13f23ec7":"markdown","f96f1bf6":"markdown","0a72d05a":"markdown","4135c1a7":"markdown"},"source":{"40bed669":"import pandas  as pd\nimport numpy   as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as plticker\nfrom sklearn.metrics import mean_squared_log_error\npd.options.mode.chained_assignment = None ","05ffde77":"#=========================================================================\n# read in the data\n#=========================================================================\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n#=========================================================================\n# also, read in the 'solution' data \n#=========================================================================\nsolution   = pd.read_csv('..\/input\/house-prices-advanced-regression-solution-file\/solution.csv')\ny_true     = solution[\"SalePrice\"]\n\n#=========================================================================\n# select only one feature: \"OverallQual\"\n#=========================================================================\nfeatures = ['OverallQual']\n\nX_train = train_data[features]\ny_train = train_data[\"SalePrice\"]\nX_test  = test_data[features]","2aa3383e":"fig, ax = plt.subplots(figsize=(15, 5))\nplt.scatter(X_train,y_train)\nloc = plticker.MultipleLocator(base=1.0)\nax.xaxis.set_major_locator(loc)\nax.set_title (\"House Prices data\", fontsize=18)\nax.set_xlabel (\"OverallQual\", fontsize=18)\nax.set_ylabel (\"SalePrice\", fontsize=18);","d10c7d0b":"# The model:\nmean_price = y_train.mean()\nprint(\"The mean price is\", mean_price)\nX_test.loc[:,'y_pred'] = mean_price\n# calculate the score\nRMSLE = np.sqrt( mean_squared_log_error(y_true, X_test['y_pred']) )\nprint(\"The score is %.5f\" % RMSLE )","929a2e4a":"#=========================================================================\n# compare the model to the data\nfig, ax = plt.subplots(figsize=(15, 5))\n# plot the training data\nplt.scatter(X_train,y_train)\n# now plot the results of our model\nplt.plot(X_test['OverallQual'],X_test['y_pred'],color='orange',linewidth=3)\nax.set_title (\"House Prices data\", fontsize=18)\nax.set_xlabel (\"OverallQual\", fontsize=18)\nax.set_ylabel (\"SalePrice\", fontsize=18);","df92d64f":"# The model:\nfit = (np.polyfit(X_train['OverallQual'], y_train, 1 ))\nc = fit[1]\nm = fit[0]\nX_test.loc[:,'y_pred'] = (m*X_test['OverallQual'] + c)\n# set any negative prices to be zero:\nX_test.loc[X_test.y_pred < 0, 'y_pred'] = 0\n# calculate the score\nRMSLE = np.sqrt( mean_squared_log_error(y_true, X_test['y_pred']) )\nprint(\"The score is %.5f\" % RMSLE )","a1401fe8":"#=========================================================================\n# compare the model to the data\nfig, ax = plt.subplots(figsize=(15, 5))\n# plot the training data\nplt.scatter(X_train,y_train)\n# now plot the results of our model\nx = np.linspace(0,10,100)\ny = m*x + c\nplt.plot(x,y,color='orange',linewidth=3)\nax.set_title (\"House Prices data\", fontsize=18)\nax.set_xlabel (\"OverallQual\", fontsize=18)\nax.set_ylabel (\"SalePrice\", fontsize=18)\nax.set_ylim(ymin=0);","27ac07f2":"# The model:\nfit = (np.polyfit(X_train['OverallQual'], y_train, 2 ))\nc = fit[2]\nb = fit[1]\na = fit[0]\nX_test.loc[:,'y_pred'] = (a*X_test['OverallQual']**2 +b*X_test['OverallQual'] + c)\n# calculate the score\nRMSLE = np.sqrt( mean_squared_log_error(y_true, X_test['y_pred']) )\nprint(\"The score is %.5f\" % RMSLE )","5f504e63":"#===========================================================================\n# compare the model to the data\nfig, ax = plt.subplots(figsize=(15, 5))\n# plot the training data\nplt.scatter(X_train,y_train)\n# now plot the results of our model\ny = a*x**2 + b*x + c\nplt.plot(x,y,color='orange',linewidth=3)\nax.set_title (\"House Prices data\", fontsize=18)\nax.set_xlabel (\"OverallQual\", fontsize=18)\nax.set_ylabel (\"SalePrice\", fontsize=18);","287a59b8":"# The model:\nfit = (np.polyfit(X_train['OverallQual'], np.log(y_train), 1))\nA = np.exp(fit[1])\nB = fit[0]\nX_test.loc[:,'y_pred'] = (A*np.exp(B*X_test['OverallQual']))\n# calculate the score\nRMSLE = np.sqrt( mean_squared_log_error(y_true, X_test['y_pred']) )\nprint(\"The score is %.5f\" % RMSLE )","b8e8ab6f":"#===========================================================================\n# compare the model to the data\nfig, ax = plt.subplots(figsize=(15, 5))\n# plot the training data\nplt.scatter(X_train,y_train)\n# now plot the results of our model\ny = A*np.exp(B*x)\nplt.plot(x,y,color='orange',linewidth=3)\nax.set_title (\"House Prices data\", fontsize=18)\nax.set_xlabel (\"OverallQual\", fontsize=18)\nax.set_ylabel (\"SalePrice\", fontsize=18);","d7ac6064":"train_data[\"OverallQual_median\"] = train_data.groupby(\"OverallQual\")[\"SalePrice\"].transform(np.median)\npredictions_dict = dict(zip(train_data[\"OverallQual\"].values,train_data[\"OverallQual_median\"]))\nX_test[\"SalePrice\"] = X_test[\"OverallQual\"].map(predictions_dict)\n# calculate the score\nRMSLE = np.sqrt( mean_squared_log_error(y_true, X_test[\"SalePrice\"] ) )\nprint(\"The score is %.5f\" % RMSLE )","6727f08e":"fig, ax = plt.subplots(figsize=(15, 5))\n# plot the training data\nplt.scatter(X_train,y_train)\n# now plot the results of our model\nplt.scatter(train_data[[\"OverallQual\"]],train_data[[\"OverallQual_median\"]], color='orange', s=80)\nax.set_title (\"House Prices data\", fontsize=18)\nax.set_xlabel (\"OverallQual\", fontsize=18)\nax.set_ylabel (\"SalePrice\", fontsize=18);","997b6b24":"#===========================================================================\n# write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({\"Id\":test_data.Id, \"SalePrice\":X_test['y_pred']})\noutput.to_csv('submission.csv', index=False)","2471fb41":"## House Prices: Using only the feature 'OverallQual'\nBoth [recursive feature elimination](https:\/\/www.kaggle.com\/carlmcbrideellis\/recursive-feature-elimination-rfe-example), [permutation importance](https:\/\/www.kaggle.com\/carlmcbrideellis\/house-prices-permutation-importance-example), and the new [Boruta-SHAP package](https:\/\/www.kaggle.com\/carlmcbrideellis\/feature-selection-using-borutashap) indicate that by far the most significant numerical feature of the [House Prices data](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data) is `OverallQual`. This feature ranges from 1 to 10 where 1 corresponds to *Very Poor* and 10 is *Very Excellent*. This is perhaps not so surprising; from the name *overall quality* one can imagine that it originates from the considered opinion of an expert in the matter, for example that of a [real estate broker](https:\/\/en.wikipedia.org\/wiki\/Real_estate_broker) or the like, *i.e.* someone who has [domain knowledge](https:\/\/en.wikipedia.org\/wiki\/Domain_knowledge). If that is the case, then this feature is more than just pure data, but is really a model in its-self. With this in mind I thought it would be interesting to make some extremely simple models of this feature, and this feature alone, just to see how well it can do on the leaderboard. \n\nThis notebook consists of the following sections:\n* [Visualisation](#Visualisation) of `OverallQual`\n* [Model #1:](#model1) The mean value: $\\bar y$\n* [Model #2:](#model2) A straight line fit: $y = mx + c$\n* [Model #3:](#model3) A univariate quadratic function: $y = ax^2 + bx +c$\n* [Model #4:](#model4) An exponential fit: $y = A \\exp(Bx)$\n* [Model #5:](#model5) Target Encoding: median\n* [Summary](#summary)\n\nAll fitting will be performed using the [NumPy](https:\/\/numpy.org\/) [polyfit](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.polyfit.html) routine.\nAs we will be using various models we shall work 'offline' rather than submit to the public leaderboard.\n(To see how to do this look at the notebook: [\"House Prices: How to work offline\"](https:\/\/www.kaggle.com\/carlmcbrideellis\/house-prices-how-to-work-offline)).","af1007e7":"## 1. Visualisation <a class=\"anchor\" id=\"Visualisation\"><\/a>\nFirst let us take a look at what we are dealing with by making a scatter plot of the `SalePrice` with respect to the `OverallQual`:","bb13ac73":"# Summary <a class=\"anchor\" id=\"summary\"><\/a>\n\n| Model | score | \n| :--- | --- |\n| #1: mean value only | 0.42578 | \n| #2:  straight line fit| 1.00433 | \n| #3: univariate quadratic function| 0.24416 | \n| #4: exponential fit| 0.22898 | \n| #5: median target encoding | 0.22735 | \n\n\nFrom this we can see that the 'best' model is the [median target encoding](#model5), with a leaderboard score of 0.22735. Just for fun let us submit this model to the competition and see how we place:","6feb8a45":"## Model #1: Mean value: $\\bar y$ <a class=\"anchor\" id=\"model1\"><\/a>","bd060b0f":"This score, though not worthy of any medals, would actually place in the top 85% of the [Public Leaderboard](http:\/\/https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/leaderboard).\n\n\nIt is worth commenting as to why the straight line fit is so bad, worse in fact than the [House Prices baseline score (0.42577)](https:\/\/www.kaggle.com\/carlmcbrideellis\/create-a-house-prices-baseline-score-0-42577). This is because my model crosses the $x$-axis at `OverallQual`=2.117. It is important to know that the score is calculated using the root of the [mean squared logarithmic error regression loss](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_squared_log_error.html), i.e. by using [the following equation](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#mean-squared-log-error): <br>\n\n$$ {\\mathrm {RMSLE}}\\,(y, \\hat y) = \\sqrt{ \\frac{1}{n_{   \\mathrm{samples}    }}  \\sum_{i=0}^{n_{    \\mathrm{samples} }-1} \\left( \\ln (1+y_i) - \\ln (1+ \\hat y_i) \\right)^2 }  $$\n\nwhere $\\hat y_i$ is the predicted value of the $i$-th sample, and $y_i$ is the corresponding true value. When the price falls below 0 the natural logarithm is undefined. To avoid this I set the `SalePrice` for houses having a `OverallQual` $\\leq 2$ to be zero (not very realistic, I know), which leads to a significant penalty in the score.\n\n# References\n* [Dean De Cock \"Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project\", Journal of Statistics Education, Volume 19, Number 3 (2011)](http:\/\/jse.amstat.org\/v19n3\/decock.pdf) + [Supplementary Materials](http:\/\/jse.amstat.org\/v19n3\/decock\/DataDocumentation.txt)","13f23ec7":"## Model #4: Exponential fit: $y = A \\exp(Bx)$ <a class=\"anchor\" id=\"model4\"><\/a>","f96f1bf6":"## Model #5: Target Encoding <a class=\"anchor\" id=\"model5\"><\/a>\nMedian value of `SalePrice` for each value of `OverallQual` <a class=\"anchor\" id=\"model5\"><\/a>","0a72d05a":"## Model #2: Straight line fit: $y = mx + c$ <a class=\"anchor\" id=\"model2\"><\/a>","4135c1a7":"## Model #3: A univariate quadratic function: $y = ax^2 + bx +c$ <a class=\"anchor\" id=\"model3\"><\/a>"}}