{"cell_type":{"ce9fc933":"code","839916d1":"code","b6ba9ac6":"code","bf42fe3c":"code","225c5f05":"code","18705ada":"code","d981dc43":"code","8fef9833":"code","707bfee1":"code","9c1c252a":"code","8fc18d36":"code","5bb104d8":"code","bf49d740":"code","300fe151":"code","17b091e3":"code","06eaf85f":"code","3d9ce00d":"code","d983bee0":"code","14432494":"code","89442269":"code","51d0cd05":"code","aa4db4f6":"markdown","84a5affa":"markdown","2b7d2a58":"markdown","55395db4":"markdown","249081be":"markdown","db008bd2":"markdown","1b408aa0":"markdown","273db937":"markdown","0b6f839c":"markdown","6dff42e2":"markdown","e5ee80ab":"markdown","7d105e02":"markdown","752ee1ae":"markdown","68b35ed8":"markdown","222f8c48":"markdown"},"source":{"ce9fc933":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, uniform\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# data_list=[]\n# i=0\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(dirname, filename)\n#         i=i+1\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","839916d1":"file_addr='\/kaggle\/input\/2020-digix-advertisement-ctr-prediction\/train_data\/train_data.csv'\nnum_lines = sum(1 for l in open(file_addr))\nnum_lines\nskip = sorted(random.sample(range(1,num_lines+1),num_lines-500000))\norig_file=pd.read_csv(file_addr , sep='|', skiprows=skip)\npd.set_option('max_columns', None)\nedited_file=orig_file.copy()\nedited_file","b6ba9ac6":"edited_file.info()","bf42fe3c":"edited_file.describe()","225c5f05":"corr_matrix=edited_file.corr()","18705ada":"plt.figure(figsize=(30, 30))\nmask = np.zeros_like(corr_matrix)\nmask[np.triu_indices_from(mask)] = True\ncorr_heatmap=sns.heatmap(edited_file.corr(), cmap=\"Blues\", annot=True, mask=mask)","d981dc43":"edited_file.boxplot(column='app_score')","8fef9833":"edited_file.boxplot(column='his_app_size')","707bfee1":"edited_file.hist(figsize=(20,15), bins=50)                           ","9c1c252a":"edited_file['label'].value_counts()[1] \/ 500000","8fc18d36":"edited_file.isnull().sum()","5bb104d8":"communication_onlinerate=edited_file['communication_onlinerate'].value_counts()[edited_file['communication_onlinerate'].value_counts()>50]\ncommunication_onlinerate=pd.DataFrame(communication_onlinerate.index)\n","bf49d740":"cat_encoder=OneHotEncoder(sparse=False, handle_unknown='ignore')\ncat_encoder.fit(communication_onlinerate.values.reshape(-1,1))\ncommunication_onlinerate_encoded=pd.DataFrame( cat_encoder.transform(edited_file['communication_onlinerate'].values.reshape(-1,1)) )\nfeatures=list(edited_file.columns)\nfeatures=features+cat_encoder.categories_[0].tolist()\nfeatures.remove('communication_onlinerate')\nengineered_data=edited_file.join(communication_onlinerate_encoded)\nengineered_data=engineered_data.drop(columns=['communication_onlinerate'])\n\n\n\nsplit=StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\nfor train_index, test_index in split.split(engineered_data, engineered_data['adv_prim_id']):\n    strat_train_set=engineered_data.loc[train_index]\n    strat_test_set=engineered_data.loc[test_index]\n\ntrain_labels=strat_train_set['label']\ntest_labels=strat_test_set['label']\nstrat_train_set=strat_train_set.drop(columns=['label'])\nstrat_test_set=strat_test_set.drop(columns=['label'])\n\nscaler=StandardScaler()\nstrat_train_set=pd.DataFrame(scaler.fit_transform(strat_train_set))\nstrat_test_set=pd.DataFrame(scaler.fit_transform(strat_test_set))\n","300fe151":"del cat_encoder\ndel communication_onlinerate_encoded\ndel scaler","17b091e3":"dtc = DecisionTreeClassifier(random_state=0)\ndtc.fit(strat_train_set, train_labels)\nroc_auc_score(test_labels, dtc.predict_proba(strat_test_set)[:, 1])","06eaf85f":"dtc = DecisionTreeClassifier(random_state=0)\nscores=cross_val_score(dtc,strat_train_set ,train_labels , cv=10, scoring='roc_auc')\nscores","3d9ce00d":"forest=RandomForestClassifier(n_estimators=30, random_state=0)\nforest.fit(strat_train_set, train_labels)\nroc_auc_score(test_labels, forest.predict_proba(strat_test_set)[:, 1])","d983bee0":"params= {'n_estimators':randint(low=1, high=200), 'max_features':randint(low=1, high=10), }\nforest=RandomForestClassifier(random_state=0)\nrnd_search=RandomizedSearchCV(forest, param_distributions=params, n_iter=10, cv=5, scoring='roc_auc', random_state=0)\nrnd_search.fit(strat_train_set, train_labels)","14432494":"cvres=rnd_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print (mean_score, params)","89442269":"xg_class= xgb.XGBClassifier(random_state=0, use_label_encoder=False)\nparams=  {'n_estimators': randint(50, 400),\n              'learning_rate': uniform(0.01, 0.59),\n              'subsample': uniform(0.3, 0.6),\n              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n              'colsample_bytree': uniform(0.5, 0.4),\n              'min_child_weight': [1, 2, 3, 4] }\nrnd_search=RandomizedSearchCV(xg_class, param_distributions = params, cv = 5,n_iter = 10, scoring = 'roc_auc', error_score = 0, verbose = 3, n_jobs = -1) \nrnd_search.fit(strat_train_set, train_labels, eval_metric='mlogloss')","51d0cd05":"cvres=rnd_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print (mean_score, params)","aa4db4f6":"## Algorithm Selection\n#### AUC metric used to evaluate the models\n##### AUC of 1 means it has a good measure of separability, AUC of 0.5 means the model has no class separation capacity.\n\n### Decision Trees\n","84a5affa":"## Feature Engineering\n#### Its time to deal with the variable 'communication_onlinerate', scale down the other variables and split the data into train and test sets.\n#### In order to make sure the split of data sets is done in a way such that the train set variables are distributed as close as possible to the test set, I will use 'StratifiedShuffleSplit'.\n#### The focus of the split will be around the variable adv_prim_id since its well distributed compared to other variables that have the same relation strength to 'label'.","2b7d2a58":"#### The values above are the best parameters to use for XGBClassifier which will provide a model with the highest AUC among other XGBClassifier with different parameters that I have randomized, overall XGBClassifier is a prefered model over RandomForestClassifier when the hyperparameters are well tuned\n","55395db4":"#### The values above are the best parameters to use for RandomForestClassifier which will provide a model with the highest AUC among other RandomForestClassifier with different parameters that I have randomized\n\n## XGboost","249081be":"#### The app_score values are mostly just '2', and in some rare occasions its '1', such values are considered outliers.\n#### I will examine another variable - his_app_size, which according to the data description seem to be more well distributed.","db008bd2":"#### Majority of the variables seem to be encoded, I will deal with the variable 'communication_onlinerate' later on.","1b408aa0":"#### The variable I will try to predict - 'label' =1 , is a rare occurrence and no other variable has a strong relation to it according to the heatmap. therefore any prediction model will have a very limited potential. ","273db937":"#### It will be easier to understand the correlation matrix by visualizing it using a heatmap","0b6f839c":"## Data Cleaning\n#### I will check the data and search for nulls","6dff42e2":"#### Thanks to the heatmap, I can already infer about some key relations between the variables, such relations could prove valueable to stakeholders.\n#### App tag of an ad task (tags) has a strong positive relation to App rating score (app_score) - it might suggest with which ad tasks are used by the more popular apps. or that such ad tasks were used to increase the app popluarity so far.\n#### App rating score (app_score) has an even stronger positive relation to display form of an ad material (inter_typ_cd) - it might suggest that some ad forms are better than others to increase popularity of an app.\n#### Its difficult to infer more accurate insights regarding App rating score since according to the description of the data - most of the values are the max value of 2, so the data is greatly biased. A boxplot will visualize that:\n","e5ee80ab":"#### App storage size (his_app_size) has some very strong positive relations to app score, app release time and App level-1 category of an ad task.\n#### It could mean many things, for example that a more recently released apps are bigger in storage size.\n#### Its unlikely to infer much beyond relations since the data is already encoded.\n#### Histograms provide more understanding about the different variables and how skewed they are.","7d105e02":"#### No nulls, its a good thing. Variables containing null values would be needed to be imputed or the rows containing them would need to be dropped.","752ee1ae":"#### Its no surprise that the random forest classifier is better than the decision tree classifier, but did I use the best parameters? maybe the algorithm can perform better.\n#### Hyperparameter tuning","68b35ed8":"## Overview of the data","222f8c48":"# CTR Data Exploration\n\n #### The data in this notebook is taken from https:\/\/www.kaggle.com\/louischen7\/2020-digix-advertisement-ctr-prediction.\n #### This notebook serves as a demonstration of my skills and way of thinking as an analyst, the contents will be revised as I continue to learn and improve.\n #### For this analysis, I will start by exploring and visualizing the data to unlock key insights.\n #### The second part will focus on prediction (Machine Learning).\n\n## Summary for CTR\n#### ClickThrough Rate (CTR) is used to gauge the performance of keywords, ads and free listings.\n#### Meaning, CTR can act as the KPI (Key Performance Indicator) of a business that relies on  online advertising.\n#### With that in mind, I will go ahead and explore the data. \n"}}