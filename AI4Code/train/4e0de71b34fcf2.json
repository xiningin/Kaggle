{"cell_type":{"f5c2fd76":"code","d350d237":"code","9aeb085e":"code","8de0853e":"code","57e74efa":"code","9246afca":"code","a95434bc":"code","d16256fd":"code","7254f807":"code","a7e1cf7b":"code","727ff629":"code","cd0d27b3":"code","81016a07":"code","b9415e5c":"code","233ed090":"code","d0420a80":"code","a0f1c42f":"code","bb99742b":"code","67c3e337":"code","6866e9bf":"code","9930b93f":"code","d350fdde":"code","562b6fc1":"code","1b40bfa0":"code","80a3f3c6":"code","d8d85bb2":"code","9cdb9751":"code","a31273fe":"code","0f5e4ec5":"code","913d180a":"code","82d51a25":"code","af9cee8f":"code","711a0b38":"code","7213a750":"code","765e45da":"code","d5a8a958":"code","37a50d92":"code","43a09dcf":"code","b38768db":"code","3950d9bb":"code","e8fabe15":"code","18f2a699":"code","a11f63b2":"code","15833b80":"code","6f12b86c":"code","45d7bcc4":"code","12231897":"code","e11ce74d":"code","f73e4670":"code","54bd07fa":"code","5b74bdf3":"code","696b51a8":"code","18588be3":"code","499bc9b3":"code","cbac27d5":"code","5e9499c5":"code","8c3c74ba":"code","43261b4c":"markdown","4e5255f8":"markdown","69e9cd2a":"markdown","774a673f":"markdown","71963ebe":"markdown","54a17fd2":"markdown","3374c9cf":"markdown","df9f6cd1":"markdown","ebf6e94d":"markdown","cbfdf113":"markdown","11efd122":"markdown","810060c7":"markdown","fbdcab66":"markdown","9fd4f376":"markdown","b60a85e7":"markdown","a30f65d1":"markdown","267bc2c0":"markdown","2a1eb03c":"markdown","c4a362eb":"markdown","c1771415":"markdown","a9f89c79":"markdown","5233b2f8":"markdown","0642a95b":"markdown","f6c95641":"markdown","3790988f":"markdown","5042c740":"markdown","1d287686":"markdown"},"source":{"f5c2fd76":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport warnings\nimport os \nwarnings.filterwarnings(\"ignore\")\nimport datetime\n","d350d237":"data=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\n","9aeb085e":"data.head()      #displaying the head of dataset they gives the 1st to 5 rows of the data","8de0853e":"data.describe()      #description of dataset ","57e74efa":"data.info()","9246afca":"data.shape       #569 rows and 33 columns","a95434bc":"data.columns     #displaying the columns of dataset","d16256fd":"data.value_counts","7254f807":"data.dtypes","a7e1cf7b":"data.isnull().sum()","727ff629":"data.drop('Unnamed: 32', axis = 1, inplace = True)\n","cd0d27b3":"data","81016a07":"data.corr()","b9415e5c":"plt.figure(figsize=(18,9))\nsns.heatmap(data.corr(),annot = True, cmap =\"Accent_r\")\n\n\n\n","233ed090":"sns.barplot(x=\"id\", y=\"diagnosis\",data=data[160:190])\nplt.title(\"Id vs Diagnosis\",fontsize=15)\nplt.xlabel(\"Id\")\nplt.ylabel(\"Diagonis\")\nplt.show()\nplt.style.use(\"ggplot\")\n","d0420a80":"sns.barplot(x=\"radius_mean\", y=\"texture_mean\", data=data[170:180])\nplt.title(\"Radius Mean vs Texture Mean\",fontsize=15)\nplt.xlabel(\"Radius Mean\")\nplt.ylabel(\"Texture Mean\")\nplt.show()\nplt.style.use(\"ggplot\")\n","a0f1c42f":" \nmean_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nsns.pairplot(data[mean_col],hue = 'diagnosis', palette='Accent')\n","bb99742b":"sns.violinplot(x=\"smoothness_mean\",y=\"perimeter_mean\",data=data)","67c3e337":"plt.figure(figsize=(14,7))\nsns.lineplot(x = \"concavity_mean\",y = \"concave points_mean\",data = data[0:400], color='green')\nplt.title(\"Concavity Mean vs Concave Mean\")\nplt.xlabel(\"Concavity Mean\")\nplt.ylabel(\"Concave Points\")\nplt.show()\n\n","6866e9bf":"worst_col = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\n\nsns.pairplot(data[worst_col],hue = 'diagnosis', palette=\"CMRmap\")","9930b93f":"# Getting Features\n\nx = data.drop(columns = 'diagnosis')\n\n# Getting Predicting Value\ny = data['diagnosis']\n","d350fdde":"\n#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n","562b6fc1":"print(len(x_train))\n","1b40bfa0":"print(len(x_test))","80a3f3c6":"print(len(y_train))","d8d85bb2":"print(len(y_test))","9cdb9751":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)                         \n","a31273fe":"y_pred=reg.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",reg.score(x_train,y_train)*100)\n\n\n","0f5e4ec5":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n\n\n\n\n","913d180a":"print(accuracy_score(y_test,y_pred)*100)","82d51a25":"from sklearn.model_selection import GridSearchCV\nparam = {\n         'penalty':['l1','l2'],\n         'C':[0.001, 0.01, 0.1, 1, 10, 20,100, 1000]\n}\nlr= LogisticRegression(penalty='l1')\ncv=GridSearchCV(reg,param,cv=5,n_jobs=-1)\ncv.fit(x_train,y_train)\ncv.predict(x_test)\n","af9cee8f":"print(\"Best CV score\", cv.best_score_*100)","711a0b38":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123)\n\ndtree.fit(x_train,y_train)\n\n#y_pred = dtree.predict(x_test)\n","7213a750":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",dtree.score(x_train,y_train)*100)\n\n","765e45da":"print(accuracy_score(y_test,y_pred)*100)","d5a8a958":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)\n\n","37a50d92":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",rfc.score(x_train,y_train)*100)\n","43a09dcf":"print(accuracy_score(y_test,y_pred)*100)","b38768db":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train,y_train)\n","3950d9bb":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",knn.score(x_train,y_train)*100)\nprint(knn.score(x_test,y_test))\n","e8fabe15":"print(accuracy_score(y_test,y_pred)*100)\n","18f2a699":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\n","a11f63b2":"y_pred=svc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",svc.score(x_train,y_train)*100)\nprint(svc.score(x_test,y_test))\n","15833b80":"print(\"Training Score: \",svc.score(x_train,y_train)*100)","6f12b86c":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nadb.fit(x_train,y_train)\n\n\n\n\n\n","45d7bcc4":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",adb.score(x_train,y_train)*100)","12231897":"print(accuracy_score(y_test,y_pred)*100)","e11ce74d":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\n","f73e4670":"y_pred=gbc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gbc.score(x_train,y_train)*100)\nprint(gbc.score(x_test,y_test))\n","54bd07fa":"print(accuracy_score(y_test,y_pred)*100)","5b74bdf3":"from xgboost import XGBClassifier\n\nxgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxgb.fit(x_train, y_train)\n","696b51a8":"y_pred=xgb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",xgb.score(x_train,y_train)*100)\nprint(xgb.score(x_test,y_test))\n","18588be3":"print(\"Training Score: \",xgb.score(x_train,y_train)*100)","499bc9b3":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","cbac27d5":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)","5e9499c5":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(\"Training Score: \",gnb.score(x_train,y_train)*100)\nprint(gnb.score(x_test,y_test))\n","8c3c74ba":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","43261b4c":"**So we get a accuracy score of 98.24 % using AdaBoostClassifier**","4e5255f8":"**So we get a accuracy score of 96.49 % using Random Forest Classifier**","69e9cd2a":"**Ada Boost Classifier got the highest accuracy**","774a673f":"# 6. AdaBoostClassifier","71963ebe":"**So we get a accuracy score of 63.7 % using SVC**","54a17fd2":"# VISUALIZING THE DATA","3374c9cf":"**So we get a accuracy score of 97.80 % using  XGBClassifier**","df9f6cd1":"# 4. KNeighborsClassifier\n\n","ebf6e94d":"# MODELS","cbfdf113":"# LOADING THE DATASET","11efd122":"**So we get a accuracy score of 70.17 % using KNeighborsClassifier**","810060c7":"**So we get a accuracy score of 58.7 % using logistic regression**","fbdcab66":"# If you liked this notebook, please UPVOTE it.","9fd4f376":"# IMPORTING THE LIBRARIES","b60a85e7":"**So we get a accuracy score of 63.29 % using Naive Bayes**","a30f65d1":"# 9. Naive Bayes","267bc2c0":"#  7. Gradient Boosting Classifier","2a1eb03c":"**So now we conclude the accuracy of different models:**\n\n**1. AdaBoost Classifier = 98.24 %**\n\n**2. XGB Classifier= 97.84 %**\n\n**3. Random Forest Classifier =96.57 %**\n\n**4. Gradient Boosting Classifier= 95.66%**\n\n**5. Decision Tree Classifier= 94.78 %**\n\n**6. K Neighbours Classifier= 70.18 %**\n\n**7. SVC = 63.80 %**\n\n**8. Naiye Bayes= 63.30 %**\n\n**9. Logistic Regression = 58.82%**\n","c4a362eb":"**So we have to drop the Unnamed: 32 coulumn which contains NaN values**","c1771415":"# 8. XGBClassifier","a9f89c79":"# 1. Logistic Regression","5233b2f8":"# TRAINING AND TESTING DATA","0642a95b":"# 3. Random Forest Classifier","f6c95641":"**So we get a accuracy score of 94.73 % using Decision Tree Classifier**","3790988f":"# 2. DECISION TREE CLASSIFIER","5042c740":"**So we get a accuracy score of 95.61 % using GradientBoostingClassifier**","1d287686":"# 5. SVC"}}