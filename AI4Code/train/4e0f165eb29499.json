{"cell_type":{"0598fe18":"code","db4a1f66":"code","353a903d":"code","14028fea":"code","1a7d8027":"code","8c4fb4a0":"code","e58a47db":"code","2ee13ff3":"code","14fedf63":"code","de74b680":"code","4cd3caa8":"code","968e3e8f":"code","90ea30c8":"code","5f55ae48":"code","327511cd":"code","609397af":"code","30f6b17f":"code","9b9b971f":"code","d4578d4c":"code","d7fc8262":"code","a2f1804c":"code","b36b140c":"code","c25abcf6":"code","18473e4b":"code","76ff8e2b":"markdown","38608ac8":"markdown","ce4acfa5":"markdown","a8f286c9":"markdown","250172ca":"markdown","d5113664":"markdown","c27225bd":"markdown"},"source":{"0598fe18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db4a1f66":"df_train = pd.read_csv(\"\/kaggle\/input\/toxicvaleur\/comment_train.csv\",encoding=\"UTF8\",sep=\",\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/toxicvaleur\/comment_test.csv\",encoding=\"UTF8\",sep=\",\")","353a903d":"import pickle","14028fea":"def get_embedding(embedding_path):\n    embedding_path = embedding_path\n    # Load embedding indexes\n    with open(embedding_path, 'rb') as f:\n        embedding_indexes = pickle.load(f)\n    # Return\n    return embedding_indexes","1a7d8027":"embedding_indexes = get_embedding(\"\/kaggle\/input\/toxicvaleur\/cc.en.300.pkl\")","8c4fb4a0":"from keras.preprocessing.text import Tokenizer","e58a47db":"x_train = df_train.comment_text.values\nx_test = df_test.comment_text.values\ny_train = df_train.toxic.values\n","2ee13ff3":"tokenizer = Tokenizer(num_words=10000000)\ntokenizer.fit_on_texts(x_train)","14fedf63":"word_index = {e: i for e, i in tokenizer.word_index.items() if i <= tokenizer.num_words}\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\ni = 0\nfor word in list(word_index.keys()):\n    embedding_vector = embedding_indexes.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        i = i + 1\nprint(f\"Taille de la matrice d'embedding (i.e. nombre de match sur input) : {len(embedding_matrix)}\")","de74b680":"from keras.preprocessing.sequence import pad_sequences","4cd3caa8":"max_seq_len = 120","968e3e8f":"sequences = tokenizer.texts_to_sequences(x_train)\nx_train_keras = pad_sequences(sequences, maxlen=max_seq_len, padding='post', truncating='pre')","90ea30c8":"sequences = tokenizer.texts_to_sequences(x_test)\nx_test_keras = pad_sequences(sequences, maxlen=max_seq_len, padding='post', truncating='pre')","5f55ae48":"from keras.layers import (ELU, LSTM, AveragePooling1D,\n                                            BatchNormalization, Bidirectional,\n                                            Conv1D, Dense, Dropout, Embedding,\n                                            Flatten, GlobalAveragePooling1D,\n                                            GlobalMaxPooling1D, Input,\n                                            LeakyReLU, MaxPooling1D, ReLU,\n                                            SpatialDropout1D, add, concatenate)\nfrom keras.models import Model, Sequential\nfrom keras.models import load_model as load_model_keras\nfrom keras.optimizers import Adam\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nfrom tensorflow.compat.v1.keras.layers import CuDNNGRU","327511cd":"from keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight(shape=(input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight(shape=(input_shape[1],),\n                                     initializer='zero',\n                                     name = '{}_b'.format(self.name),\n                                     regularizer = self.b_regularizer,\n                                     constraint = self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","609397af":"def get_model():\n\n    # Get input dim\n    input_dim = embedding_matrix.shape[0]\n\n    # Get model\n    num_classes = 1\n    # Process\n    LSTM_UNITS = 50\n    DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n    words = Input(shape=(max_seq_len,))\n    x = Embedding(input_dim, 300, weights=[embedding_matrix], trainable=False)(words)\n    x = BatchNormalization(momentum=0.9)(x)\n    x = SpatialDropout1D(0.5)(x)    \n\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x) \n    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x) \n    x = BatchNormalization(momentum=0.9)(x)\n    \n    att = Attention(max_seq_len)(x)\n    avg_pool1 = GlobalAveragePooling1D()(x)\n    max_pool1 = GlobalMaxPooling1D()(x)\n\n    x = concatenate([att, avg_pool1, max_pool1])\n    # Last layer\n    activation = 'sigmoid' \n    x = BatchNormalization(momentum=0.9)(x)\n    out = Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform')(x)\n\n    # Compile model\n    model = Model(inputs=words, outputs=[out])\n    metrics = ['acc']\n    model.compile(optimizer=Adam(lr=0.01,decay=0.001), loss='binary_crossentropy', metrics=metrics)\n    model.summary()\n\n\n    # Return\n    return model\n","30f6b17f":"from keras.callbacks import (CSVLogger, EarlyStopping,TerminateOnNaN)\n\ncallbacks = [EarlyStopping(monitor='val_loss', patience=12, mode='auto', restore_best_weights=True)]\ncallbacks.append(CSVLogger(filename='logger.csv', separator=';', append=False))\ncallbacks.append(TerminateOnNaN())\n","9b9b971f":"NUM_MODELS=4","d4578d4c":"predictions_model2 = np.zeros((x_test_keras.shape[0],1))\nfor model_idx in range(NUM_MODELS):\n    print(\"MODEL n\u00b0\"+ str(model_idx))\n    model = get_model()\n\n    history = model.fit(x_train_keras, y_train,\n                        epochs=999,\n                        batch_size=128,\n                        verbose=1,\n                        validation_split=0.2,\n                        callbacks = callbacks)\n    predictions_model2=predictions_model2+(model.predict(x_test_keras, batch_size=64))\n    \npredictions_2=predictions_model2\/(NUM_MODELS)","d7fc8262":"res_test = predictions_2","a2f1804c":"res_test","b36b140c":"submission = df_test\nsubmission[\"toxic\"] = np.round(res_test).astype(int)\nsubmission = submission[[\"id\",\"toxic\"]]","c25abcf6":"print(submission)","18473e4b":"submission.to_csv(\"submission.csv\",encoding=\"utf8\",sep=',',index=False)","76ff8e2b":"### Sauvegarde du fichier au format demand\u00e9","38608ac8":"### build sequences","ce4acfa5":"### build model","a8f286c9":"### Create embedding matrix","250172ca":"### obtention des pr\u00e9dictions sur le test","d5113664":"### Load data","c27225bd":"### embedding"}}