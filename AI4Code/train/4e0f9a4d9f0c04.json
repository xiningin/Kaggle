{"cell_type":{"7866d0cd":"code","c1a15c39":"code","a6aad0e0":"code","0d70de20":"code","09183b7e":"code","99769e80":"code","8c480703":"code","df01db1c":"code","70139aa0":"code","85895636":"code","f12b20db":"code","d0262c3d":"code","a0ed7c0f":"code","29b58954":"code","4eb057dd":"code","dfa657cc":"code","00af3179":"code","ed6a242f":"code","53ca657f":"code","c2920afc":"code","9619322b":"code","fdf3bb84":"code","fbae3475":"code","717010ad":"code","64cd5513":"code","a72f8924":"code","cfd18c27":"markdown","68697cdc":"markdown"},"source":{"7866d0cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1a15c39":"import numpy as np\nimport pandas as pd\nimport seaborn as sn\nfrom scipy import stats\nimport missingno as msno\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","a6aad0e0":"train_df=pd.read_csv('..\/input\/bike-sharing-demand\/train.csv')\ntest_df=pd.read_csv('..\/input\/bike-sharing-demand\/test.csv')","0d70de20":"train_df.head()","09183b7e":"train_df.info()","99769e80":"train_df.describe()","8c480703":"train_df.isnull().sum() ","df01db1c":"train_df.duplicated().sum()\n","70139aa0":"train_df.drop_duplicates(inplace = True)\n","85895636":"test_df.duplicated().sum()\n","f12b20db":"train_df.groupby('season')['count'].hist(alpha = 0.4)\nplt.legend( ['spring','summer','fall','winter'])","d0262c3d":"train_df.groupby('season').count()","a0ed7c0f":"plt.figure(figsize=(16,8))\nsns.histplot(train_df['count'],kde = True)\nplt.show()","29b58954":"sns.catplot(x='weather',data=train_df,kind='count',height=5,aspect=1.5)\n","4eb057dd":"cor_mat= train_df[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","dfa657cc":"train_df[\"hour\"] = [t.hour for t in pd.DatetimeIndex(train_df.datetime)]\ntrain_df[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(train_df.datetime)]\ntrain_df[\"month\"] = [t.month for t in pd.DatetimeIndex(train_df.datetime)]\ntrain_df['year'] = [t.year for t in pd.DatetimeIndex(train_df.datetime)]\n\ntrain_df['year'] = train_df['year'].map({2011:0, 2012:1})\ntrain_df.head()","00af3179":"test_df[\"hour\"] = [t.hour for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df[\"month\"] = [t.month for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df['year'] = [t.year for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df['year'] = test_df['year'].map({2011:0, 2012:1})\ntest_df.head()","ed6a242f":"train_df.drop(['datetime','casual','registered','atemp'],axis=1,inplace=True)\n","53ca657f":"date_time = test_df.datetime\ntest_df.drop(['datetime','atemp'],axis=1,inplace=True)\n","c2920afc":"train_df.head()","9619322b":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.svm import SVR\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor","fdf3bb84":"train_df['count']=np.log(train_df['count']+1)\nY_train = train_df['count']\nX_train = train_df.drop(['count'], axis=1)\n# X_train['Date']=X_train['Date'].map(dt.datetime.toordinal)\nx_train, x_test, y_train, y_test = train_test_split(X_train , Y_train,  train_size=0.8, test_size=0.2, random_state =0)","fbae3475":"import xgboost as xgb\n\nxg_reg = xgb.XGBRegressor( colsample_bytree = 0.8, learning_rate = 0.149,min_child_weight =25, max_depth = 3,  n_estimators = 450)\nxg_reg.fit(x_train,y_train)\npred=xg_reg.predict(x_test)\nsc=np.sqrt(mean_squared_error(y_train , xg_reg.predict(x_train)))\nscore=np.sqrt(mean_squared_error(y_test, pred))\nprint(\"The score is:\",score)\nprint(\"The score is:\",sc)\n","717010ad":"final_y_test = xg_reg.predict(test_df) \nfinal_y_test = np.round(np.exp(final_y_test)-1)","64cd5513":"output = pd.DataFrame({'datetime': date_time,\n                       'count': final_y_test})\noutput.to_csv('submission.csv', index=False)","a72f8924":"output","cfd18c27":"### Model","68697cdc":"## EDA & Feature Engineering"}}