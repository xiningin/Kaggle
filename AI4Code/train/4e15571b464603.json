{"cell_type":{"94f46734":"code","654e45c3":"code","74667a7a":"code","915d4b0a":"code","d6886f5a":"code","3b96fc65":"code","9dc0506f":"code","f45250dc":"code","6d5d1582":"code","883ffb82":"code","4c5f8a25":"code","1f6c3ff1":"code","c23f38dd":"code","86681cf5":"code","ec1abd1e":"code","dc6680d5":"code","1fa22f6c":"code","1330d436":"code","88a9fb74":"code","7c53f9c6":"code","62089f32":"code","abd7eb41":"code","16e9708b":"code","67574702":"code","aa02d5db":"code","82402a39":"code","db41f813":"code","780ebae1":"markdown","4fad4f11":"markdown","914718ba":"markdown","ea6b062c":"markdown","060a3d1b":"markdown","bd810a7b":"markdown","0ec9d704":"markdown","ea5c3f97":"markdown","f71709bd":"markdown","8ddb0631":"markdown","4c79eded":"markdown","d1ed63fc":"markdown","96fb98e2":"markdown"},"source":{"94f46734":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport sys\nfrom colorama import Fore, Style\nfrom tqdm.notebook import tqdm\n\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\n\nfrom IPython import display\n\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nwarnings.simplefilter(\"ignore\")\npd.set_option('display.max_columns', None)\nplt.style.use(\"classic\")","654e45c3":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.metrics import log_loss\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","74667a7a":"def cout(string: str, color: str) -> str:\n    \"\"\"\n    Prints a string in the required color\n    \"\"\"\n    print(color+string+Style.RESET_ALL)\n    \ndef statistics(dataframe, column):\n    cout(f\"The Average value in {column} is: {dataframe[column].mean():.2f}\", Fore.RED)\n    cout(f\"The Maximum value in {column} is: {dataframe[column].max()}\", Fore.BLUE)\n    cout(f\"The Minimum value in {column} is: {dataframe[column].min()}\", Fore.YELLOW)\n    cout(f\"The 25th Quantile of {column} is: {dataframe[column].quantile(0.25)}\", Fore.GREEN)\n    cout(f\"The 50th Quantile of {column} is: {dataframe[column].quantile(0.50)}\", Fore.CYAN)\n    cout(f\"The 75th Quantile of {column} is: {dataframe[column].quantile(0.75)}\", Fore.MAGENTA)","915d4b0a":"train_feats = pd.read_csv(\"..\/input\/lish-moa\/train_features.csv\")\ntrain_targets = pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\")\ntest_feats = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\nsub = pd.read_csv(\"..\/input\/lish-moa\/sample_submission.csv\")\n\ndata = pd.concat([train_feats, test_feats])","d6886f5a":"cout(\"Combined Train and Test Features\", Fore.BLUE)\ndata.head()","3b96fc65":"cout(\"Train Targets\", Fore.RED)\ntrain_targets.head()","9dc0506f":"data[['cp_type']].describe()","f45250dc":"df = data.groupby(['cp_type'])['sig_id'].count().reset_index()\ndf.columns = ['type', 'count']\n\nfig = px.bar(\n    df,\n    x='type',\n    y='count',\n    color = 'type',\n    width=650,\n    height=500,\n    title=\"Type of Treatment\",\n    labels={'type':\"Type\", 'count':\"Count\"},\n)\n\nfig.show()","6d5d1582":"fig = px.pie(\n    df,\n    names='type',\n    values='count',\n    hole=0.3,\n    color_discrete_sequence=px.colors.sequential.Cividis,\n    title=\"Type of Treatment - Pie Chart\",\n)\n\nfig.show()","883ffb82":"statistics(data, \"cp_time\")","4c5f8a25":"df = data.groupby(['cp_time'])['sig_id'].count().reset_index()\ndf.columns = ['time', 'count']\n\nfig = px.bar(\n    df,\n    x='time',\n    y='count',\n    color = 'count',\n    width=650,\n    height=500,\n    title=\"Time of Treatment\",\n    labels={'time':\"Time\", 'count':\"Count\"},\n)\n\nfig.show()","1f6c3ff1":"fig = px.pie(\n    df,\n    names='time',\n    values='count',\n    hole=0.3,\n    color_discrete_sequence=px.colors.sequential.Mint_r,\n    title=\"Time of Treatment - Pie Chart\",\n)\n\nfig.show()","c23f38dd":"data[['cp_dose']].describe()","86681cf5":"df = data.groupby(['cp_dose'])['sig_id'].count().reset_index()\ndf.columns = ['dose', 'count']\n\nfig = px.pie(\n    df,\n    names='dose',\n    values='count',\n    hole=0.3,\n    color_discrete_sequence=px.colors.sequential.PuRd_r,\n    title=\"Type of Dose Administered\",\n)\n\nfig.show()","ec1abd1e":"df = data.groupby(['cp_type', 'cp_time', 'cp_dose'])['sig_id'].count().reset_index()\ndf.columns = ['Type', 'Time', 'Dose', 'Count']\n\nfig = px.sunburst(\n    df, \n    path=[\n        'Type',\n        'Time',\n        'Dose' \n    ], \n    values='Count', \n    title='Sunburst chart for Type, Time and Dose',\n    width=600,\n    height=600,\n    color_discrete_sequence=px.colors.sequential.Sunset_r\n)\nfig.show()","dc6680d5":"train_targets.describe()","1fa22f6c":"vals = train_targets.sum()[1:].sort_values().tolist()[:10]\nnames = list(dict(train_targets.sum()[1:].sort_values()).keys())[:10]","1330d436":"# Plot the sparsity of target matrix\nplt.figure(figsize=(16, 9))\nsns.barplot(names, vals)\nplt.title(\"Top-10 Features with Most Sparse Target Matrix\")\nplt.xlabel(\"Feature Name\")\nplt.ylabel(\"Count of '1' in the feature\")\nplt.xticks(rotation=90)\nplt.show()","88a9fb74":"correlation_matrix = pd.DataFrame()\n\ncolumns = [i for i in train_feats.columns if i.startswith('g-')] + [i for i in train_feats.columns if i.startswith('c-')]\n\nfor t_col in tqdm(train_targets.columns):\n    corr_list = list()\n    if t_col == 'sig_id':\n        continue\n    for col in columns:\n        res = train_feats[col].corr(train_targets[t_col])\n        corr_list.append(res)\n    correlation_matrix[t_col] = corr_list","7c53f9c6":"correlation_matrix['train_features'] = columns\ncorrelation_matrix = correlation_matrix.set_index('train_features')\ncorrelation_matrix","62089f32":"# Encode categorical features in both training and testing sets\n\n# DOSE\ntrain_feats['cp_dose'] = train_feats['cp_dose'].map({'D1':0, 'D2':1})\ntest_feats['cp_dose'] = test_feats['cp_dose'].map({'D1':0, 'D2':1})\n\n# CP_TYPE\ntrain_feats['cp_type'] = train_feats['cp_type'].map({'trt_cp':0, 'ctl_vehicle':1})\ntest_feats['cp_type'] = test_feats['cp_type'].map({'trt_cp':0, 'ctl_vehicle':1})","abd7eb41":"# Remove column sig_id\ntrain_feats = train_feats.drop(['sig_id'], axis=1)\ntest_feats = test_feats.drop(['sig_id'], axis=1)\ntrain_targets = train_targets.drop(['sig_id'], axis=1)","16e9708b":"important_feats = [  0,   1,   2,   3,   5,   6,   8,   9,  10,  11,  12,  14,  15,\n        16,  18,  19,  20,  21,  23,  24,  25,  27,  28,  29,  30,  31,\n        32,  33,  34,  35,  36,  37,  39,  40,  41,  42,  44,  45,  46,\n        48,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n        63,  64,  65,  66,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n        78,  79,  80,  81,  82,  83,  84,  86,  87,  88,  89,  90,  92,\n        93,  94,  95,  96,  97,  99, 100, 101, 103, 104, 105, 106, 107,\n       108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n       121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134,\n       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n       149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164,\n       165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 180,\n       181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n       197, 198, 199, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213,\n       214, 215, 218, 219, 220, 221, 222, 224, 225, 227, 228, 229, 230,\n       231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245,\n       246, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260,\n       261, 263, 265, 266, 268, 270, 271, 272, 273, 275, 276, 277, 279,\n       282, 283, 286, 287, 288, 289, 290, 294, 295, 296, 297, 299, 300,\n       301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 315,\n       316, 317, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330, 331,\n       332, 333, 334, 335, 338, 339, 340, 341, 343, 344, 345, 346, 347,\n       349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 368, 369, 370, 371, 372, 374, 375, 376, 377,\n       378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n       392, 393, 394, 395, 397, 398, 399, 400, 401, 403, 405, 406, 407,\n       408, 410, 411, 412, 413, 414, 415, 417, 418, 419, 420, 421, 422,\n       423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n       436, 437, 438, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n       452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n       466, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482,\n       483, 485, 486, 487, 488, 489, 491, 492, 494, 495, 496, 500, 501,\n       502, 503, 505, 506, 507, 509, 510, 511, 512, 513, 514, 516, 517,\n       518, 519, 521, 523, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n       534, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547,\n       549, 550, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563,\n       564, 565, 566, 567, 569, 570, 571, 572, 573, 574, 575, 577, 580,\n       581, 582, 583, 586, 587, 590, 591, 592, 593, 595, 596, 597, 598,\n       599, 600, 601, 602, 603, 605, 607, 608, 609, 611, 612, 613, 614,\n       615, 616, 617, 619, 622, 623, 625, 627, 630, 631, 632, 633, 634,\n       635, 637, 638, 639, 642, 643, 644, 645, 646, 647, 649, 650, 651,\n       652, 654, 655, 658, 659, 660, 661, 662, 663, 664, 666, 667, 668,\n       669, 670, 672, 674, 675, 676, 677, 678, 680, 681, 682, 684, 685,\n       686, 687, 688, 689, 691, 692, 694, 695, 696, 697, 699, 700, 701,\n       702, 703, 704, 705, 707, 708, 709, 711, 712, 713, 714, 715, 716,\n       717, 723, 725, 727, 728, 729, 730, 731, 732, 734, 736, 737, 738,\n       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n       752, 753, 754, 755, 756, 758, 759, 760, 761, 762, 763, 764, 765,\n       766, 767, 769, 770, 771, 772, 774, 775, 780, 781, 782, 783, 784,\n       785, 787, 788, 790, 793, 795, 797, 799, 800, 801, 805, 808, 809,\n       811, 812, 813, 816, 819, 820, 821, 822, 823, 825, 826, 827, 829,\n       831, 832, 833, 834, 835, 837, 838, 839, 840, 841, 842, 844, 845,\n       846, 847, 848, 850, 851, 852, 854, 855, 856, 858, 860, 861, 862,\n       864, 867, 868, 870, 871, 873, 874]","67574702":"class MOAData(Dataset):\n    def __init__(self, feature_dataframe, target_dataframe=None, is_pred=False, important_features=important_feats):\n        self.fd = feature_dataframe\n        self.td = target_dataframe\n        self.is_pred = is_pred\n        self.important_features = important_feats\n    \n    def __getitem__(self, idx):\n        item = self.fd.values[:, self.important_features][idx]\n        \n        if self.is_pred:\n            return item, None\n        \n        else:\n            target = self.td.astype(float).values[idx]\n            return (item, target)\n        \n    def __len__(self):\n        return len(self.fd)","aa02d5db":"class Network(nn.Module):\n    def __init__(self, nb_feats):\n        super(Network, self).__init__()\n        \n        self.nb_feats = nb_feats\n        \n        self.bn1 = nn.BatchNorm1d(num_features=self.nb_feats)\n        self.fc1 = nn.utils.weight_norm(nn.Linear(in_features=nb_feats, out_features=512))\n        self.bn2 = nn.BatchNorm1d(num_features=512)\n        self.drp1 = nn.Dropout(0.2)\n        self.fc2 = nn.utils.weight_norm(nn.Linear(in_features=512, out_features=256))\n        self.bn3 = nn.BatchNorm1d(num_features=256)\n        self.drp2 = nn.Dropout(0.3)\n        self.out = nn.utils.weight_norm(nn.Linear(in_features=256, out_features=206))\n        \n    def forward(self, x):\n        x = self.bn1(x)\n        \n        x = F.relu(self.fc1(x))\n        x = self.bn2(x)\n        x = self.drp1(x)\n        \n        x = F.relu(self.fc2(x))\n        x = self.bn3(x)\n        x = self.drp1(x)\n        \n        output = F.sigmoid(self.out(x))\n        \n        return output","82402a39":"model = Network(nb_feats=len(important_feats))\nmodel = model.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=1e-4)\nloss_fn = nn.BCELoss()","db41f813":"# Training loop\nnb_epochs = 5\nsave_path = \"model_sav.pth\"\n\nresult = train_targets.copy()\n\nsub.loc[:, train_targets.columns] = 0\nresult.loc[:, train_targets.columns] = 0\n\nfor n, (train_split, val_split) in enumerate(MultilabelStratifiedKFold(n_splits=10, shuffle=True).split(train_targets, train_targets)):\n#     cout(f\"{'='*20} Fold: {n} {'='*20}\", Fore.YELLOW)\n    \n    # Init a dataset object for every fold\n    train_set = MOAData(\n        feature_dataframe=train_feats.iloc[train_split].reset_index(drop=True),\n        target_dataframe=train_targets.iloc[train_split].reset_index(drop=True),\n    )\n    \n    valid_set = MOAData(\n        feature_dataframe=train_feats.iloc[val_split].reset_index(drop=True),\n        target_dataframe=train_targets.iloc[val_split].reset_index(drop=True),\n    )\n    \n    # Connect them to dataloaders\n    train = DataLoader(train_set, batch_size=32, num_workers=0)\n    # valid = DataLoader(valid_set, batch_size=, shuffle=True, num_workers=2)\n    \n    # Run training epochs\n    for epoch in range(nb_epochs):\n        epoch_loss = 0\n        model.train()\n        \n        for i, (x, y) in enumerate(train):\n            x = torch.tensor(x, dtype=torch.float32).to(device)\n            y = torch.tensor(y, dtype=torch.float32).to(device)\n            \n            # Train steps\n            optim.zero_grad()\n            z = model(x)\n            loss = loss_fn(z, y)\n            loss.backward()\n            optim.step()\n            \n            # Calculate the loss\n            # epoch_loss += loss.item()\n            print(f\"Fold: {n} | Epochs: {epoch}\/{nb_epochs} | Batch: {i} | Loss: {loss.item():.4f}\")\n            display.clear_output(wait=True)","780ebae1":"<h1 style=\"color:aqua\">What, Why and How?<\/h1>\n\n![](https:\/\/images.pexels.com\/photos\/139398\/thermometer-headache-pain-pills-139398.jpeg?auto=compress&cs=tinysrgb&dpr=3&h=750&w=1260)\n\nIn the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. \nToday, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. \nIn this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.\n\nOne approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs.\n\n<h1 style=\"color:blue\">My Work<\/h1>\nThe notebook you are looking at now is still under work. I am making many adjustements and changes to the code. I may completely change the Neural Network code from PyTorch to Tensorflow since it's having some problems with GPU Utilization.\n\nOtherwise, I have used basic Neural Network with Weight Normalization and Batch Normalization to further refine the training process.\n\n<h4 style=\"color:red\">If you like my work, please leave an upvote, it takes 10+ hours of work-everyday to make a single notebook like this, and even one upvote goes a long way.<\/h4>","4fad4f11":"## Feature Correlation\nLet's look at the correlation of **Gene Expression (g-)** and **Cell Viability (c-)** data features with the target features.","914718ba":"# EDA\nLet's start with some EDA on the data.","ea6b062c":"<h3 style=\"color:magenta\">UNDER WORK<\/h3>\n\nThe notebook is still under work and I am making changes to the training procedure every commit.","060a3d1b":"### Mapping Categorical Features\nLet's first map the categorical features in both train and test datasets)","bd810a7b":"## Type of Treatement (`cp_type`)\nThis feature can take 2 possible values:\n* `trt_cp`: Treatment Compound\n* `ctl_vehicle`: Control Vehicle (Control Peturbations) - These have no Mechanism of Actions (MoA)","0ec9d704":"<p style=\"color:green\">We can see that there are 2 features (<code>atp-sensitive_potassium_channel_antagonist<\/code>, <code>erbb2_inhibitor<\/code>) that have only <strong>1<\/strong> number of 1-target, others are 0-target.\nIf we set this one to 0, it won't punish our model that much (since we are only incorrectly classifying 1 sample).<\/p>","ea5c3f97":"# Data Preprocessing and Modelling\nLet's now preprocess the data and make it ready for our model.","f71709bd":"## Type of Dose (`cp_dose`)\nLet's look at the type of dose administered (`D1` or `D2`)","8ddb0631":"## Time of Treatment (`cp_time`)\nThis feature has 3 possible values:\n* `24` (in hours)\n* `48` (in hours)\n* `72` (in hours)","4c79eded":"### Removing Id Column\nAlso, remove the `sig_id` column from training matrix and testing matrix","d1ed63fc":"## Train Targets\nLet's look at how sparse the train targets dataframe is.","96fb98e2":"## Sunburst Chart for Type, Time and Dose"}}