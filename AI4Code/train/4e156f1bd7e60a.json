{"cell_type":{"bea45b11":"code","f6bef9b7":"code","77d254d8":"code","ac5f5dae":"code","dfcc8f94":"code","00ba5110":"code","b3ab7760":"code","c69fa65b":"code","297c9d74":"code","758aa622":"code","5c64c61a":"code","20f58c09":"code","801b6804":"code","02e1398d":"code","5438e5e2":"code","96e3799a":"code","0c5869a6":"code","54cf4583":"code","06e53e6d":"code","0c57f2be":"code","3a9947ad":"code","b52d0cc3":"markdown"},"source":{"bea45b11":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","f6bef9b7":"#Load dataset and save it in a variable\ncancer = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","77d254d8":"#View first 5 rows of the dataset\ncancer.head()","ac5f5dae":"#Count number of rows and columns in the dataset\ncancer.shape","dfcc8f94":"#Count the number of empty values in each column\ncancer.isnull().sum()","00ba5110":"#Drop the column with all missing values\ncancer = cancer.dropna(axis = 1)","b3ab7760":"#Get the new count of the number of rows and columns\ncancer.shape","c69fa65b":"#Get a count of the number of Malignant (M) or Benign (B) cells\ncancer['diagnosis'].value_counts()","297c9d74":"#Visualize the count\nsns.countplot(cancer['diagnosis'], label = 'count', palette = \"husl\")","758aa622":"#Look at the data types to see which columns need to be encoded\ncancer.dtypes","5c64c61a":"#Encode categorical data values\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ncancer.iloc[:, 1] = le.fit_transform(cancer.iloc[:,1].values)","20f58c09":"#Create a pair plot\nsns.pairplot(cancer.iloc[:,1:5], hue='diagnosis')","801b6804":"#Print first 5 rows of the data\ncancer.head()","02e1398d":"#Get the coreltion of the columns\ncancer.iloc[:,1:12].corr()","5438e5e2":"#Visualize the corelation\nplt.figure(figsize=(12, 9))\nsns.heatmap(cancer.iloc[:,1:12].corr(), annot = True, fmt = '.0%')","96e3799a":"#Split the data set into independent(X) and dependent (Y) data sets\nX = cancer.iloc[:, 2:31].values\ny = cancer.iloc[:,1].values","0c5869a6":"#Split the data set into 75% training and 25% testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","54cf4583":"#Scale the data (Feature Scaling)\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","06e53e6d":"#Create a function for models\ndef models(X_train, y_train):\n    \n    #Logistic Regression\n    from sklearn.linear_model import LogisticRegression\n    log = LogisticRegression(random_state = 0)\n    log.fit(X_train, y_train)\n    \n    #Decision Tree\n    from sklearn.tree import DecisionTreeClassifier\n    tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n    tree.fit(X_train, y_train)\n    \n    #Random Forest Classifier\n    from sklearn.ensemble import RandomForestClassifier\n    forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n    forest.fit(X_train, y_train)\n    \n    #Print the models accuracy on the training data\n    print('Logistic Regression Training Accuracy:', log.score(X_train, y_train))\n    print('Decision Tree Classifier Training Accuracy:', tree.score(X_train, y_train))\n    print('Random Forest Classifier Training Accuracy:', forest.score(X_train, y_train))\n    \n    return log, tree, forest\n    ","0c57f2be":"#Getting all of the models\nmodel = models(X_train, y_train)","3a9947ad":"#test model accuracy on test data on confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfor i in range (len(model)):\n    print('Model ', i)\n    cm = confusion_matrix(y_test, model[i].predict(X_test))\n\n    TP = cm[0][0]\n    TN = cm[1][1]\n    FN = cm[1][0]\n    FP = cm[0][1]\n\n    print(cm)\n    print('Testing Accuracy = ', (TP + TN) \/ (TP + TN + FN + FP))\n    print()","b52d0cc3":"<h1>Attribute Information:<\/h1>\n<p><br>1) ID number\n<br>2) Diagnosis (M = malignant, B = benign)\n<br>3) Real-valued features are computed for each cell nucleus:\n    \n<br>a) radius (mean of distances from center to points on the perimeter)\n<br>b) texture (standard deviation of gray-scale values)\n<br>c) perimeter\n<br>d) area\n<br>e) smoothness (local variation in radius lengths)\n<br>f) compactness (perimeter^2 \/ area - 1.0)\n<br>g). concavity (severity of concave portions of the contour)\n<br>h). concave points (number of concave portions of the contour)\n<br>i). symmetry\n<br>j). fractal dimension(\"coastline approximation\" - 1)<\/p>\n"}}