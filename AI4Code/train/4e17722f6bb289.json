{"cell_type":{"70272b0e":"code","1f288140":"code","813fed45":"code","b2f0d1ce":"code","b2f0af59":"code","4485d1c5":"code","eb3da2cc":"code","c455dad3":"code","33f6fa90":"code","2caa373b":"code","4c9839a3":"code","ed9eb81d":"code","ceb37ee7":"code","8511be71":"code","65d20daf":"code","7d42ce77":"code","0e4c9d54":"code","c5ca4f4f":"code","e0eb08cb":"markdown","eab7ef0b":"markdown","feed3c55":"markdown","8a4be63b":"markdown","ac9cf133":"markdown","9abe0623":"markdown","4c1d1907":"markdown","67cb15dc":"markdown","cb50befd":"markdown","6eaa7348":"markdown","fdc09af8":"markdown","00148b18":"markdown","f7554206":"markdown","01a6051b":"markdown","e62d9d30":"markdown","a690dcff":"markdown","00fc9f26":"markdown","9ed47ace":"markdown","38832756":"markdown","ca93904b":"markdown","84464e34":"markdown","50507b5f":"markdown","48f865a1":"markdown","48e05147":"markdown"},"source":{"70272b0e":"# Some constants for dataset size and stroke paddings\nSTROKE_COUNT = 196\nTRAIN_SAMPLES = 50\nVALID_SAMPLES = 50\nTEST_SAMPLES = 50\nPOOL_SAMPLES = 250","1f288140":"%matplotlib inline\nimport os\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import shuffle\nimport pandas as pd\nfrom keras.metrics import top_k_categorical_accuracy\ndef top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout\nfrom keras import backend as K\nfrom glob import glob\nimport gc\ngc.enable()\nfrom IPython.display import clear_output\n\ndef plot_history(history):\n    # Plot training & validation loss values\n    plt.plot(history['loss'])\n    plt.plot(history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\ndef plot_learn_process(process):\n    plt.plot(process[\"Percentage\"], process[\"Accuracy\"])\n    plt.title('Accuracy\/Percentage')\n    plt.xlabel('percentage of data used')\n    plt.ylabel('accuracy on test data')\n    plt.show()\n    \ndef get_available_gpus():\n    from tensorflow.python.client import device_lib\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\nif len(get_available_gpus())>0:\n    from keras.layers import CuDNNLSTM as LSTM # this one is about 3x faster on GPU instances  \nbase_dir = os.path.join('..', 'input', 'quickdraw-doodle-recognition')\n# base_dir = os.path.join('..', 'input')\ntest_path = os.path.join(base_dir, 'test_simplified.csv')\ninline_process_template = \"..\/input\/learning-process-inline\/{}\"","813fed45":"from ast import literal_eval\nALL_TRAIN_PATHS = glob(os.path.join(base_dir, 'train_simplified', '*.csv'))\nCOL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n\ndef _stack_it(raw_strokes):\n    \"\"\"preprocess the string and make \n    a standard Nx3 stroke vector\"\"\"\n    stroke_vec = literal_eval(raw_strokes) # string->list\n    # unwrap the list\n    in_strokes = [(xi,yi,i)  \n     for i,(x,y) in enumerate(stroke_vec) \n     for xi,yi in zip(x,y)]\n    c_strokes = np.stack(in_strokes)\n    # replace stroke id with 1 for continue, 2 for new\n    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n    c_strokes[:,2] += 1 # since 0 is no stroke\n    # pad the strokes with zeros\n    return pad_sequences(c_strokes.swapaxes(0, 1), \n                         maxlen=STROKE_COUNT, \n                         padding='post').swapaxes(0, 1)\ndef read_batch(samples=5, \n               start_row=0,\n               max_rows = 1000):\n    \"\"\"\n    load and process the csv files\n    this function is horribly inefficient but simple\n    \"\"\"\n    out_df_list = []\n    for c_path in ALL_TRAIN_PATHS:\n        c_df = pd.read_csv(c_path, nrows=max_rows, skiprows=start_row)\n        c_df.columns=COL_NAMES\n        out_df_list += [c_df.sample(samples)[['drawing', 'word']]]\n    full_df = pd.concat(out_df_list)\n    full_df['drawing'] = full_df['drawing'].\\\n        map(_stack_it)\n    \n    return full_df\n\ndef get_Xy(in_df):\n    X = np.stack(in_df['drawing'], 0)\n    y = to_categorical(word_encoder.transform(in_df['word'].values))\n    return X, y","b2f0d1ce":"train_args = dict(samples=TRAIN_SAMPLES, \n                  start_row=0, \n                  max_rows=TRAIN_SAMPLES)\npool_args = dict(samples=POOL_SAMPLES, \n                 start_row=train_args['max_rows'], \n                 max_rows=POOL_SAMPLES)\nvalid_args = dict(samples=VALID_SAMPLES, \n                  start_row=train_args['max_rows']+pool_args['max_rows'], \n                  max_rows=VALID_SAMPLES)\ntest_args = dict(samples=TEST_SAMPLES, \n                 start_row=train_args['max_rows']+valid_args['max_rows']+pool_args['max_rows'], \n                 max_rows=TEST_SAMPLES)\n\ntrain_df = read_batch(**train_args)\nvalid_df = read_batch(**valid_args)\ntest_df = read_batch(**test_args)\npool_df = read_batch(**pool_args)\nword_encoder = LabelEncoder()\nword_encoder.fit(train_df['word'])\nprint('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))","b2f0af59":"train_data = get_Xy(train_df)\npool_data = get_Xy(pool_df)\nval_data = get_Xy(valid_df)\ntest_data = get_Xy(test_df)","4485d1c5":"stroke_read_model = Sequential()\nstroke_read_model.add(BatchNormalization(input_shape = (None, 3)))\n# filter count and length are taken from the script https:\/\/github.com\/tensorflow\/models\/blob\/master\/tutorials\/rnn\/quickdraw\/train_model.py\nstroke_read_model.add(Conv1D(48, (5,)))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(Conv1D(64, (5,)))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(Conv1D(96, (3,)))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(LSTM(128, return_sequences = True))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(LSTM(128, return_sequences = False))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(Dense(512))\nstroke_read_model.add(Dropout(0.3))\nstroke_read_model.add(Dense(len(word_encoder.classes_), activation = 'softmax'))\nstroke_read_model.compile(optimizer = 'adam', \n                          loss = 'categorical_crossentropy', \n                          metrics = ['categorical_accuracy', top_3_accuracy])\nclear_output()","eb3da2cc":"class active_learner:\n    def __init__(self, model, train, pool, val, test):\n        self.model = model\n        self.train = train\n        self.pool = pool\n        self.val = val\n        self.test = test\n        self.weight_path=\"..\/input\/base-weight\/{}_weights.best.hdf5\".format('stroke_lstm_model')\n#         self.weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_model')\n        checkpoint = ModelCheckpoint(self.weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n        reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, \n                                           verbose=1, mode='auto', cooldown=5, min_lr=0.0001)\n        early = EarlyStopping(monitor=\"val_loss\", \n                              mode=\"min\", \n                              patience=3) # probably needs to be more patient, but kaggle time is limited\n        self.callbacks_list = [checkpoint, reduceLROnPlat, early]\n        self.__load_base()\n    \n    def __load_base(self, batch_size=4096):\n        model = self.model\n        weight_path = self.weight_path\n        callbacks_list = self.callbacks_list\n        train_x, train_y = self.train\n        val_x, val_y = self.val\n        \n        try:\n            model.load_weights(weight_path)\n        except:\n            history = model.fit(train_x, train_y,\n                                validation_data = (val_x, val_y), \n                                batch_size = batch_size,\n                                epochs = 500,\n                                verbose = 0,\n                                callbacks = callbacks_list[:-1])\n            clear_output()\n            plot_history(history.history)\n            \n    def learn(self, query_fn, prop=0.70, batch_size=4096, epoch_per_al_cycle=20, extra_label_per_epoch=200):\n        self.__load_base()\n        model = self.model\n        train_x, train_y = self.train\n        pool_x, pool_y = self.pool\n        val_x, val_y = self.val\n        test_x, test_y = self.test\n        \n        tot_size = len(train_x) + len(pool_x)\n        cur_size = len(train_x)\n        \n        lstm_results = model.evaluate(test_x, test_y, batch_size = batch_size)\n        print\n        learn_process = {\"Percentage\":[], \"Accuracy\":[]}\n        learn_process[\"Percentage\"].append(cur_size\/tot_size)\n        learn_process[\"Accuracy\"].append(100*lstm_results[1])\n        \n        while cur_size\/tot_size < prop:\n            rank = query_fn(model, pool_x)\n            selected_x, selected_y = pool_x[rank[:extra_label_per_epoch]], pool_y[rank[:extra_label_per_epoch]]\n            pool_x, pool_y = pool_x[rank[extra_label_per_epoch:]], pool_y[rank[extra_label_per_epoch:]]\n            train_x, train_y = np.vstack((train_x, selected_x)), np.vstack((train_y, selected_y))\n            \n            model.fit(train_x, train_y, \n                      validation_data = (val_x, val_y), \n                      batch_size = batch_size, \n                      epochs=epoch_per_al_cycle, \n                      callbacks = self.callbacks_list[1: ],\n                      verbose=0)\n            cur_size = len(train_x)\n#             print(cur_size\/tot_size)\n            lstm_results = model.evaluate(test_x, test_y, batch_size = batch_size, verbose=0)\n            print(\"Test data accuracy is: \", 100*lstm_results[1])\n            learn_process[\"Percentage\"].append(cur_size\/tot_size)\n            learn_process[\"Accuracy\"].append(100*lstm_results[1])\n        \n        return learn_process\n    \n    def evaluate(self):\n        model = self.model\n        test_x, test_y = self.test\n        lstm_results = stroke_read_model.evaluate(test_x, test_y, batch_size = 4096)\n        print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))","c455dad3":"learner = active_learner(stroke_read_model, train_data, pool_data, val_data, test_data)","33f6fa90":"learner.evaluate()","2caa373b":"def random_selection(model, pool_x):\n    rank = shuffle(np.arange(pool_x.shape[0]))\n    return rank\n\ntry:\n    with open(inline_process_template.format(\"learn_process_random\"), 'rb') as f:\n        process = pickle.load(f)\nexcept:\n    # learn function returns accuracy with respect to the percentage of data labeled\n    process = learner.learn(random_selection)\n    \n# plot_learn_process will visualize the active learning process\nplot_learn_process(process)","4c9839a3":"def confidence_selection(model, pool_x):\n    scores = model.predict(pool_x)\n    scores = np.max(scores, axis=1)\n    rank = np.argsort(scores)\n    return rank\n\ntry:\n    with open(inline_process_template.format(\"learn_process_confidence\"), 'rb') as f:\n        process = pickle.load(f)\nexcept:\n    # learn function returns accuracy with respect to the percentage of data labeled\n    process = learner.learn(confidence_selection)\n    \n# plot_learn_process will visualize the active learning process\nplot_learn_process(process)","ed9eb81d":"def margin_selection(model, pool_x):\n    scores = model.predict(pool_x)\n    scores_max = np.max(scores, axis=1)\n    max_idx = np.argmax(scores, axis=1)\n    scores[(np.arange(len(max_idx)), max_idx)] = 0\n    scores_secmax = np.max(scores, axis=1)\n    scores = scores_max-scores_secmax\n    rank = np.argsort(scores)\n    return rank\n\ntry:\n    with open(inline_process_template.format(\"learn_process_margin\"), 'rb') as f:\n        process = pickle.load(f)\nexcept:\n    # learn function returns accuracy with respect to the percentage of data labeled\n    process = learner.learn(margin_selection)\n    \n# plot_learn_process will visualize the active learning process\nplot_learn_process(process)","ceb37ee7":"def entropy_selection(model, pool_x):\n    scores = model.predict(pool_x)\n    log_scores = -scores*np.log2(scores)\n    scores = np.sum(scores, axis=1)\n    rank = np.argsort(-scores)\n    return rank\n\ntry:\n    with open(inline_process_template.format(\"learn_process_entropy\"), 'rb') as f:\n        process = pickle.load(f)\nexcept:\n    # learn function returns accuracy with respect to the percentage of data labeled\n    process = learner.learn(entropy_selection)\n    \n# plot_learn_process will visualize the active learning process\nplot_learn_process(process)","8511be71":"NUM_EGL_SELECTION = 100\ndef EGL_selection(model, pool_x, extra_label_per_epoch=NUM_EGL_SELECTION):\n    with tf.Session() as sess:\n#         print(\"before: \", pool_x.shape)\n        sess.run(tf.global_variables_initializer())\n        outputTensor = model.output\n        listOfVariableTensors = model.trainable_weights\n        gradients = K.gradients(outputTensor, listOfVariableTensors)\n        shuffled = shuffle(np.arange(len(pool_x)))\n        rnd_select = shuffled[:extra_label_per_epoch*5]\n        shuffled = shuffled[extra_label_per_epoch*5:]\n        scores = [-np.sum(list(map(\n                     lambda x: np.linalg.norm(x), \n                     sess.run(gradients, feed_dict={model.input:(input_data.reshape(1,-1,3))})\n                  ))) for input_data in list(pool_x[rnd_select])]\n        selected = rnd_select[np.argsort(scores)]\n        unselected = np.setdiff1d(rnd_select, selected)\n#         print(\"after: \", np.hstack((selected, unselected, shuffled)).shape)\n        return np.hstack((selected, unselected, shuffled))\n\ntry:\n    with open(inline_process_template.format(\"learn_process_egl\"), 'rb') as f:\n        process = pickle.load(f)\nexcept:\n    # learn function returns accuracy with respect to the percentage of data labeled\n    process = learner.learn(EGL_selection, extra_label_per_epoch=NUM_EGL_SELECTION)\n    \n# plot_learn_process will visualize the active learning process\nplot_learn_process(process)","65d20daf":"# Pairwise Potential Loopy Belief Propagation\nclass LBP:\n    def __init__(self, model, data, num_classes=340, neighbour=10, iteration=2):\n        self.model = model\n        self.num_classes = num_classes\n        self.iteration = iteration\n        num_data = len(data)\n        factors = set(zip([-1]*num_data, range(num_data)))\n        for x in range(num_data):\n            lst = self.nearest_k_neighbours(data, x, neighbour)\n            for y in range(len(lst)):\n                factors.add((min(x, lst[y]), max(x, lst[y])))\n        factors = list(factors)\n        self.factors = factors\n        \n        node_to_factors = [[] for _ in range(num_data)]\n        order = []\n        for x in range(len(factors)):\n            if factors[x][0] != -1:\n                order = order + [(1, factors[x][0], x), (0, x, factors[x][0])]\n                node_to_factors[factors[x][0]].append(x)\n            order = order + [(1, factors[x][1], x), (0, x, factors[x][1])]\n            node_to_factors[factors[x][1]].append(x)\n        self.order = order\n        self.node_to_factors = node_to_factors\n        \n        self.node_po = list(model.predict(np.array(data)))\n        \n        num_factors = len(factors)\n        self.msg_node_factor = [[np.ones(num_classes) for j in range(len(node_to_factors[i]))] for i in range(num_data)]\n        self.msg_factor_node = [[np.ones(num_classes) for j in range(2)] for i in range(num_factors)]\n#         print(\"number of factors: \", num_factors)\n\n    def nearest_k_neighbours(self, data, x, k):\n        lst = []\n        for y in range(len(data)):\n            if x==y:\n                continue\n            d = np.linalg.norm(data[x]-data[y])\n            lst.append((d, y))\n            for i in range(len(lst)-1, 0, -1):\n                if lst[i][0] < lst[i-1][0]:\n                    lst[i-1], lst[i] = lst[i], lst[i-1]\n                else:\n                    break\n            if len(lst)>k:\n                lst.pop()\n        lst = [l[1] for l in lst]\n        return lst\n    \n    def pretty_print(self, e):\n        msg_node_factor = self.msg_node_factor\n        msg_factor_node = self.msg_factor_node\n        node_to_factors = self.node_to_factors\n        factors = self.factors\n        \n        if e[0]==0:\n            print(\"==========Factor to Node==========\")\n            print(\"From \", factors[e[1]], \" to \", e[2])\n            if(factors[e[1]][0]==e[2]):\n                print(msg_factor_node[e[1]][0][:10])\n                print(\"Sum is: \", np.sum(msg_factor_node[e[1]][0]))\n            else:\n                print(msg_factor_node[e[1]][1][:10])\n                print(\"Sum is: \", np.sum(msg_factor_node[e[1]][1]))\n            print(\"==================================\")\n        else:\n            print(\"==========Node to Factor==========\")\n            print(\"From \", e[1], \" to \", factors[e[2]])\n            print(msg_node_factor[e[1]][node_to_factors[e[1]].index(e[2])][:10])\n            print(\"Sum is: \", np.sum(msg_node_factor[e[1]][node_to_factors[e[1]].index(e[2])]))\n            print(\"==================================\")\n            \n    def converge(self):\n        order = self.order\n        factors = self.factors\n        node_po = self.node_po\n        node_to_factors = self.node_to_factors\n        msg_node_factor = self.msg_node_factor\n        msg_factor_node = self.msg_factor_node\n        num_classes = self.num_classes\n        \n        edge_po_matrix = 5*np.ones((num_classes, num_classes))\n        edge_po_matrix[range(num_classes), range(num_classes)] = 10\n        \n        for i in range(self.iteration):\n#             print(\"Iteration \", i, \":------------------------\")\n            order = shuffle(order)\n            for e in order:\n                # factor to node\n                if e[0] == 0:\n                    # for factor with id e[1], (a, b), if a is the target node\n                    if factors[e[1]][0] == e[2]:\n                        msg_factor_node[e[1]][0] = np.dot(edge_po_matrix, msg_node_factor[factors[e[1]][1]][node_to_factors[factors[e[1]][1]].index(e[1])])\n                    elif factors[e[1]][0]==-1:\n                        msg_factor_node[e[1]][1] = node_po[factors[e[1]][1]]\n                    # for factor with id e[1], (a, b), if b is the target node\n                    else:\n                        msg_factor_node[e[1]][1] = np.dot(edge_po_matrix, msg_node_factor[factors[e[1]][0]][node_to_factors[factors[e[1]][0]].index(e[1])])\n                    msg_factor_node[e[1]][1] = self.normalize_msg(msg_factor_node[e[1]][1])\n                    msg_factor_node[e[1]][0] = self.normalize_msg(msg_factor_node[e[1]][0])\n                # node to factor\n                else:\n                    new_msg = np.ones(num_classes)\n                    for j in node_to_factors[e[1]]:\n                        if j==e[2]:\n                            continue\n                        if e[1] == factors[j][0]:\n                            new_msg = new_msg * msg_factor_node[j][0]\n                        else:\n                            new_msg = new_msg * msg_factor_node[j][1]\n                        new_msg = self.normalize_msg(new_msg)\n                    msg_node_factor[e[1]][node_to_factors[e[1]].index(e[2])] = new_msg\n#                 self.pretty_print(e)\n    \n    def normalize_msg(self, msg):\n        s = np.sum(msg)\n        if s==0:\n            return msg\n        else:\n            return msg\/s\n    \n    def get_belief(self):\n        factors = self.factors\n        msg_node_factor = self.msg_node_factor\n        msg_factor_node = self.msg_factor_node\n        node_to_factors = self.node_to_factors\n        node_po = self.node_po\n        belief = {}\n        for i in range(len(factors)):\n            fa = factors[i]\n            if fa[0]==-1:\n                belief[fa] = msg_node_factor[fa[1]][node_to_factors[fa[1]].index(i)]*node_po[fa[1]]\n            else:\n                belief[fa] = np.outer(msg_node_factor[fa[0]][node_to_factors[fa[0]].index(i)], msg_node_factor[fa[1]][node_to_factors[fa[1]].index(i)])\n                belief[fa] = self.normalize_msg(belief[fa])\n        return belief","7d42ce77":"import heapq\n\nclass submodular_opt:\n    def __init__(self, be, batch_pool_size):\n        M = np.zeros((batch_pool_size, batch_pool_size))\n        h = np.zeros(batch_pool_size)\n        for k, v in be.items():\n            if k[0]==-1:\n                v[v==0] = 0.01\n                h[k[1]] = np.sum(-v*np.log2(v))\n            else:\n                out = np.outer(be[(-1, k[0])], be[(-1, k[1])])\n                out[out==0] = 0.01\n                tmp_M = v*np.log2(v\/out)\n                tmp_M[np.isnan(tmp_M)] = 0\n                tmp_M[tmp_M==-np.inf] = 0\n                tmp_M[tmp_M==np.inf] = 0\n                M[k[0], k[1]] = np.sum(tmp_M)\n                M[k[1], k[0]] = np.sum(tmp_M)\n        self.M = M\n        self.h = h\n        self.size = batch_pool_size\n        self.ones = np.ones(batch_pool_size)\n    \n    def obj_func(self, x):\n        M = self.M\n        h = self.h\n        ones = self.ones\n        return 0.5*x.dot(-M).dot(x) + x.dot(M.dot(ones)-h)\n    \n    def make_x(self, lst):\n        size = self.size\n        zeros = np.zeros(size)\n        zeros[lst] = 1\n        return zeros\n    \n    def lazy_greedy(self, k):\n        size = self.size\n        selections = []\n        pq = [[self.obj_func(self.make_x([i])), i, 0] for i in range(size)]\n        heapq.heapify(pq)\n        cur_obj = 0\n        for i in range(k):\n#             print(pq)\n            while True:\n                elem = heapq.heappop(pq)\n                if len(selections) == elem[2]:\n                    selections.append(elem[1])\n                    cur_obj = cur_obj + elem[0]\n                    break\n                else:\n                    elem[0] = self.obj_func(self.make_x(selections + [elem[1]])) - cur_obj\n                    elem[2] = len(selections)\n                    heapq.heappush(pq, elem)\n        return selections","0e4c9d54":"NUM_LOOPY_SFM_SELECTION = 100\ndef loopy_sfm_selection(model, pool_x, extra_label_per_epoch=NUM_LOOPY_SFM_SELECTION):\n#     print(pool_x.shape)\n    shuffled = shuffle(np.arange(len(pool_x)))\n    rnd_select = shuffled[:extra_label_per_epoch*5]\n    shuffled = shuffled[extra_label_per_epoch*5:]\n    pool_x = list(pool_x[rnd_select])\n    \n    lbp = LBP(model, pool_x, iteration=1)\n    lbp.converge()\n    be = lbp.get_belief()\n#     print([np.argmax(be[(-1, i)]) for i in range(extra_label_per_epoch)])\n    SFM = submodular_opt(be, extra_label_per_epoch*5)\n    selected = rnd_select[SFM.lazy_greedy(extra_label_per_epoch)]\n#     print(selected)\n    unselected = np.setdiff1d(rnd_select, selected)\n#     print(np.hstack((selected, unselected, shuffled)).shape)\n    return np.hstack((selected, unselected, shuffled))\n\ntry:\n    with open(inline_process_template.format(\"learn_process_submodule\"), 'rb') as f:\n        process = pickle.load(f)\nexcept:\n    # learn function returns accuracy with respect to the percentage of data labeled\n    process = learner.learn(EGL_selection, extra_label_per_epoch=NUM_LOOPY_SFM_SELECTION)\n    \n# plot_learn_process will visualize the active learning process\nplot_learn_process(process)","c5ca4f4f":"import re\ns = \"learn_process_(.+?)_\"\np = re.compile(s)\nprocess_template = \"..\/input\/learning-process\/{}\"\ncolors = ['green', 'blue', 'red', 'yellow', 'magenta', 'cyan']\n\ndef read_learning_process(filename):    \n    with open(process_template.format(filename), 'rb') as f:\n        content = pickle.load(f)\n    return content\n\ndef plot_all_learn_process(processes, title):\n    contents = [read_learning_process(process) for process in processes]\n    for i in range(len(contents)):\n        plt.plot(contents[i][\"Percentage\"], contents[i][\"Accuracy\"], colors[i])\n    plt.title('Accuracy\/Percentage'+title)\n    plt.xlabel('percentage of data used')\n    plt.ylabel('accuracy on test data')\n    plt.legend([p.match(process).group(1) for process in processes], loc='upper left')\n    plt.show()\n\n# train, eval, test, pool\nprocess_50_100_100_200 = [\"learn_process_random_50_100_100_200\", \"learn_process_confidence_50_100_100_200\", \"learn_process_margin_50_100_100_200\", \"learn_process_entropy_50_100_100_200\", \"learn_process_egl_50_100_100_200\",\"learn_process_submodule_50_100_100_200\"]\nprocess_50_100_500_950 = [\"learn_process_random_50_100_500_950\", \"learn_process_confidence_50_100_500_950\", \"learn_process_margin_50_100_500_950\", \"learn_process_entropy_50_100_500_950\"]\nprocess_50_100_500_400 = [\"learn_process_random_50_100_500_400\", \"learn_process_confidence_50_100_500_400\", \"learn_process_margin_50_100_500_400\", \"learn_process_entropy_50_100_500_400\", \"learn_process_egl_50_100_500_400\"]\nprocess_100_100_100_400 = [\"learn_process_random_100_100_100_400\", \"learn_process_confidence_100_100_100_400\", \"learn_process_margin_100_100_100_400\", \"learn_process_entropy_100_100_100_400\", \"learn_process_egl_100_100_100_400\"]\nprocess_50_100_300_400 = [\"learn_process_random_50_100_300_400\", \"learn_process_confidence_50_100_300_400\", \"learn_process_margin_50_100_300_400\", \"learn_process_entropy_50_100_300_400\", \"learn_process_egl_50_100_300_400\"]\n\ntitle_str = \" - train: {}, validate: {}, test: {}, pool: {}\"\n\nplot_all_learn_process(process_50_100_100_200, title_str.format(50, 100, 100, 200))\nplot_all_learn_process(process_50_100_500_950, title_str.format(50, 100, 500, 950))\nplot_all_learn_process(process_50_100_500_400, title_str.format(50, 100, 500, 400))\nplot_all_learn_process(process_100_100_100_400, title_str.format(100, 100, 100, 400))\nplot_all_learn_process(process_50_100_300_400, title_str.format(50, 100, 300, 400))","e0eb08cb":"### Limitation\nDue to the limit in computation power and memory, we modified each round of active learning such that if x data need to be selected for this round, we select this x data from a randomly shuffled subpool with 5x data instead of the whole pool.","eab7ef0b":"### There're different scenarios of active learning; We only experiment with pool-based here.\nGenerally, pool-based active learning algorithm follows this simple pattern:\n```\nGiven: Labeled set L, unlabeled pool U, query strategy Q, query batch size B\n\nrepeat\n    train the model on dataset L\n    for 1 to B\n        Pick the most informative label l according to strategy Q\n        add l to set L\n        remove l from U\nuntil some stopping criterion;\n```","feed3c55":"### Create a Loopy Belief Propagation Class\nThis class takes in a pool of data and connects each data to the 10 nearest other data points in Euclidean distance; then runs loopy belief propagation. Since we only have pairwise edge potential here, each factor in the factor graph is just a pair of adjacent nodes. For simplicty I also added factors with only 1 node for easier calculation.","8a4be63b":"### Optimization\nIn the paper, it uses Fujishige-Wolfe Min Norm Point algorithm to solve the function minimization. Because the algorithm is complex and I did not find an existing implementation of this algorithm, so I turned to the lazy greedy algorithm from the previous section hoping for a good approximation.","ac9cf133":"### Load the dataset and preprocess the input","9abe0623":"## 4. Submodularity and Graph Belief\nThe previous section talked about submodularity on a theoretical level and I find coming up with an objective function with the property of submodularity and monotonicity is quite hard.\n\nThis paper: http:\/\/openaccess.thecvf.com\/content_cvpr_2017\/papers\/Paul_Non-Uniform_Subset_Selection_CVPR_2017_paper.pdf proposes an approach of constructing a graph with data as nodes to represent the relation between each pair of unlabeled data and then uses the node entropy and mutual information between each pair to construct the objective function. It is proved in the paper that the objective function is submodular and we can use submodular function minimization algorithm to efficiently solve the optimization problem.\n","4c1d1907":"## 2. Expected Gradient Length\nGradient quantifies how much we update the trainable weights in the model. This approach chooses the unlabeled data that will produce the greatest gradient when feed into the current model, so we choose the ones that impart the most to our model.","67cb15dc":"### Let's get started","cb50befd":"### Class active_learner is the main object handling all the logic\nIt loads the pretrained weight and also controls things like how many rounds of selections we do for active learning, how many unlabeled data we select during each round, etc.","6eaa7348":"A **submodular function** is a set function satisfying a natural diminishing returns property. We call a set\nfunction F defined over a ground set V submodular iff for all $A \\subseteq B \\subseteq V$ and $v \\in V \\backslash B$\n$$\nF(A + v) \u2212 F(A) \\ge F(B + v) \u2212 F(B)\n$$\nBasically the same thing as marginal returns in Economics.\n\nAnother simple property for the set function is that the function is monotone non-decreasing if for all $A \\subseteq B \\subseteq V$, $F(A) \\leq F(B)$.\n\n### Why do we need diminishing returns and monotone non-decreasing property?\nImagine you have 5 sensors and you want to properly place the sensors inside the office so that no matter which corner of the office could be on fire, on average the sensors should be able to detect the fire quickly. Another example is you have a social network relation graph of your customers, there's some costs with doing campaign for your products to one of the customers. But the characteristic of social network is that these customers may propagate information about your product to other people. The goal can be something like to using as few costs as possible while achieving campaign outcome of some measure $X$. \n\nThe first problem is known as submodular maximization problem and the second problem is called submodular set cover problem.\n\nIt turns out both problems are NP-Hard. However if the objective function (maximizing the fire sensoring and minimizing the campaign cost in the examples above) is submodular and monotone non-decreasing, a very simple greedy algorithm can be used to approximate the optimal value. The strtegy is just for current Set $S$ with objective value of $F(S)$, we pick the element $e$ that generates the largest benefit $F(S \\cup e) - F(S)$. The result get from the greedy strategy is no worse than $1-\\frac{1}{e}$ times the optimal value. (A theorem proved by Nemhauser et al '78)\n\n### Connection to active learning\nThe connection can be made with active learning because it is essentially a set cover problem where the goal is to minimize the labeled data put into the training data with the constraint of achieving some accuracy.\n\nThe only difference is that active learning is an adaptive version of submodular set cover problem in the sense that after one data is labeled, this newly labeled data should be included into the set when considering the data to be labeled next. \n\nDescibed in the paper:\n\nhttp:\/\/arxiv.org\/pdf\/1003.3967.pdf\n\nhttp:\/\/icml.cc\/Conferences\/2010\/papers\/436.pdf\n\nThe greedy algorithm is proved to have similar approximation as in the regular submodular set cover problem but with a lower bound(can perform worse when compared to optimal value).\n\nThe papers only give theoretical ideas of connecting submodular optimization with active learning but without hints to pick an proper objective submodular function. The tricky problem in this QuickDraw recognition setting, and probably in other deep neural network settings as well, is to find an objective function that measure the robustness and performance of the model with the diminishing return and monotone properties. Another thing to watch out is that when choosing unlabled data and evaluating the objective function, the actual labels of the data are unknown. \n\n### Some examples\nThere's a project: http:\/\/github.com\/jmschrei\/apricot that applies the submodularity on the data selection and very easy to use. The idea is to find the similarity between each pair of data and greedily select the data that are mostly different from others. Again the submodularity allows this approach to approximate relatively well while doing the job in polynomial time. However, the complexity finding the simiarity is quadratic to the size of unlabeled data. In the pool-based setting, the unlabeled pool is too big for this.","fdc09af8":"### Now it's obvious that the core part is the query strategy and the following strategies are introduced\n\n* uncertainty sampling\n    * least confidence\n    * margin\n    * entropy   \n* expected gradient length\n* submodular function optimization\n* submodularity and graph belief","00148b18":"### Least Confidence\nSince the LSTM model gives probabilities of the unlabeled data belongs to each category. Least confidence selects labels that the current model has least greatest probility in determining how the unlabeled data to be categorized.\n$$\nx^*_{LC} = \\text{argmax}_x 1 \u2212 P_\\theta(y^*|x)\n$$\nwhere\n$$\ny* = \\text{argmax}_y P_\\theta(y|x)\n$$","f7554206":"We can evaluate our current model and see how well it does on the test dataset.","01a6051b":"### Here are class labels for the Quick Draw Game","e62d9d30":"## Set up the LSTM model\nThe quick draw recognizing sequence model is used to experiment with the active learning techniques.\n\n[Quick, Draw](https:\/\/quickdraw.withgoogle.com\/) is a game made by Google AI to gather drawing dataset from the public for machine leaning research and the collected datasets are made public\n\nThe network structure used here is the same one used in tensorflow sequence [tutorial](https:\/\/www.tensorflow.org\/tutorials\/sequences\/recurrent_quickdraw)\n\n![quick draw model image](https:\/\/www.tensorflow.org\/images\/quickdraw_model.png)","a690dcff":"## 1. Uncertainty Sampling","00fc9f26":"### Load libraries and some utility plot functions","9ed47ace":"#### The learner object definied above provides a learn function that takes a query strategy as input and then do  the active learning cycle with the provided query strategy function\n\nLet's start with random selection as an example see how learner does the active learning.\nRandom selection just randomly select unlabeled data from the pool.","38832756":"### Least Margin\nSimilar to the least confidence approach, we pick the unlabeled data that produces the least difference between the greatest and the second greatest probabilities.\n$$\nx^*_{LM} = \\text{argmin}_x P_\\theta(y_1|x) \u2212 P_\\theta(y_2|x)\n$$\nwhere $y_1$ and $y_2$ are the most and the second most likely label","ca93904b":"### Greatest Entropy\nEntropy quantifies the uncertainty that the model holds on an unlabeled data. This approach simply selects the labels that our current model are most uncertain about.","84464e34":"## 3. Submodular Function Optimization","50507b5f":"### Some Experiment results with larger pool example\n\nDue to applying the submodular optimization and expected gradient length approaches require long computation time. When the pool size gets larger, they become intractable and hence are not displayed on the plots below.\n\nThe following plots represent the learning processes of different active learning approaches mentioned above. The plot title shows how many training data, validation data, test data and pool data are used for each class label(there're 340 classes in total).\n\nIt seems that in the case of LSTM model for the Quick Draw game, only margin and confidence approaches are doing better most time; the other three sometimes perform worse than the random selection approach.","48f865a1":"# Active Learning Experiments\n\n### What is active learning\nActive learning is a machine learning setting where the learning algorithm decides what data to be labeled.\n### Why active learning\nThe key idea behind active learning is that a machine learning algorithm can\nachieve greater accuracy with fewer training labels if it is allowed to choose the\ndata from which it learns. An active learner may pose queries, usually in the form\nof unlabeled data instances to be labeled by an oracle (e.g., a human annotator).\nActive learning is well-motivated in many modern machine learning problems,\nwhere unlabeled data may be abundant or easily obtained, but labels are difficult,\ntime-consuming, or expensive to obtain.","48e05147":"### Limitation\nSame as in the expected gradient length approach, we still struggle with the memory and computation, so we use the same strategy here. Notice that by doing this, our active learning setting is very similar to the stream-based scenario."}}