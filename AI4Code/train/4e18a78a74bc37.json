{"cell_type":{"0dc3d7fa":"code","4163b716":"code","32d019cd":"code","e000875e":"code","2d8c8b71":"code","c7bdd7f5":"code","c2ffebf5":"code","e32f7bf7":"code","94e75411":"code","5c960d69":"code","e8b607bd":"code","a8747bf2":"code","3a223a2b":"code","3df80879":"code","9af5bdc5":"code","f51b96ab":"code","da039f5e":"code","663e0525":"code","69621ba6":"code","9a79c39e":"code","a9aa340a":"code","d2b33fa3":"code","c5da80a9":"code","a4762669":"code","432764a6":"code","4883327d":"code","bca4bbda":"code","b5ba9e1a":"code","9526be7d":"code","8c781265":"code","1ffa4bde":"code","90f51ae3":"code","ad9c61fa":"code","cb67e2b6":"code","5a1c7bdd":"code","94d97ad2":"code","a6eb5b4d":"code","6b081102":"code","df1c840d":"code","d7708f0b":"code","ffd0107e":"code","63488d9a":"code","8b33dab5":"code","07ab53f0":"code","bd7a376a":"code","e906e329":"code","dd205c53":"code","815679b7":"code","13d8080f":"code","2035dccf":"code","167c3c8e":"code","efcd6403":"code","500801ca":"code","3efa9d5e":"code","bc5105be":"markdown","4ce67b7f":"markdown","fb9cc823":"markdown","96f1aa5f":"markdown","9db2a921":"markdown","cf055e27":"markdown"},"source":{"0dc3d7fa":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport json\nfrom time import time\nimport pickle\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nfrom keras.models import Model, load_model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Input, Dense, Dropout, Embedding, LSTM\nfrom keras.layers.merge import add","4163b716":"# Read Text Captions\n\ndef readTextFile(path):\n    with open(path) as f:\n        captions = f.read()\n    return captions","32d019cd":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e000875e":"captions  = readTextFile(\"\/kaggle\/input\/flicker8k-dataset\/Flickr8k_text\/Flickr8k.token.txt\")\ncaptions = captions.split('\\n')[:-1]","2d8c8b71":"import cv2\nprint(len(captions))\nfirst,second  = captions[0].split('\\t')\nprint(first.split(\".\")[0])\nprint(second)\nprint(type(captions))\nimg=cv2.imread(\"\/kaggle\/input\/flicker8k-dataset\/flickr8k_dataset\/Flicker8k_Dataset\/1000268201_693b08cb0e.jpg\")\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.title(second)\nplt.imshow(img)\nprint(captions[0])","c7bdd7f5":"# Dictionary to Map each Image with the list of captions it has\ndescriptions = {}\n\nfor x in captions:\n    first,second = x.split('\\t')\n    img_name = first.split(\".\")[0]\n    \n    #if the image id is already present or not\n    if descriptions.get(img_name) is None:\n        descriptions[img_name] = []\n    \n    descriptions[img_name].append(second)\n    \n\ndescriptions[\"1000268201_693b08cb0e\"]\nlen(descriptions)","c2ffebf5":"#Data Cleaning\n\ndef clean_text(sentence):\n    sentence = sentence.lower()\n    sentence = re.sub(\"[^a-z]+\",\" \",sentence)\n    sentence = sentence.split()\n    \n    sentence  = [s for s in sentence if len(s)>1]\n    sentence = \" \".join(sentence)\n    return sentence","e32f7bf7":"clean_text(\"A cat is sitting over the house # 64\")","94e75411":"# Clean all Captions\nfor key,caption_list in descriptions.items():\n    for i in range(len(caption_list)):\n        caption_list[i] = clean_text(caption_list[i])","5c960d69":"descriptions[\"1000268201_693b08cb0e\"]","e8b607bd":"# Write the data to text file\nwith open(\"descriptions_1.txt\",\"w\") as f:\n    f.write(str(descriptions))","a8747bf2":"descriptions = None\nwith open(\"descriptions_1.txt\",'r') as f:\n    descriptions= f.read()\n    \njson_acceptable_string = descriptions.replace(\"'\",\"\\\"\")\ndescriptions = json.loads(json_acceptable_string)","3a223a2b":"# Vocab\n\nvocab = set()\nfor key in descriptions.keys():\n    [vocab.update(sentence.split()) for sentence in descriptions[key]]\n    \nprint(\"Vocab Size : %d\"% len(vocab))","3df80879":"# Total No of words across all the sentences\ntotal_words = []\n\nfor key in descriptions.keys():\n    [total_words.append(i) for des in descriptions[key] for i in des.split()]\n    \nprint(\"Total Words %d\"%len(total_words))\n","9af5bdc5":"# Filter Words from the Vocab according to certain threshold frequncy\nimport collections\n\ncounter = collections.Counter(total_words)\nfreq_cnt = dict(counter)\nprint(len(freq_cnt.keys()))","f51b96ab":"# Sort this dictionary according to the freq count\nsorted_freq_cnt = sorted(freq_cnt.items(),reverse=True,key=lambda x:x[1])\n\n# Filter\nthreshold = 10\nsorted_freq_cnt  = [x for x in sorted_freq_cnt if x[1]>threshold]\ntotal_words = [x[0] for x in sorted_freq_cnt]\n\nprint(len(total_words))","da039f5e":"train_file_data = readTextFile(\"\/kaggle\/input\/flicker8k-dataset\/Flickr8k_text\/Flickr_8k.trainImages.txt\")\ntest_file_data = readTextFile(\"\/kaggle\/input\/flicker8k-dataset\/Flickr8k_text\/Flickr_8k.testImages.txt\")\n\ntrain = [row.split(\".\")[0] for row in train_file_data.split(\"\\n\")[:-1]]\ntest = [row.split(\".\")[0] for row in test_file_data.split(\"\\n\")[:-1]]\n\ntrain[0:3]","663e0525":"# Prepare Description for the Training Data\n# Tweak - Add <s> and <e> token to our training data\ntrain_descriptions = {}\n\nfor img_id in train:\n    train_descriptions[img_id] = []\n    for cap in descriptions[img_id]:\n        cap_to_append = \"startseq \"  + cap + \" endseq\"\n        train_descriptions[img_id].append(cap_to_append)","69621ba6":"train_descriptions[\"1000268201_693b08cb0e\"]","9a79c39e":"model = ResNet50(weights=\"imagenet\",input_shape=(224,224,3))\nmodel.summary()","a9aa340a":"#Since we only need image features\nmodel_new = Model(model.input,model.layers[-2].output)","d2b33fa3":"def preprocess_img(img):\n    img = image.load_img(img,target_size=(224,224))\n    img = image.img_to_array(img)\n    img = np.expand_dims(img,axis=0)\n    # Normalisation\n    img = preprocess_input(img)\n    return img","c5da80a9":"def encode_image(img):\n    img = preprocess_img(img)\n    feature_vector = model_new.predict(img)\n    \n    feature_vector = feature_vector.reshape((-1,))\n    #print(feature_vector.shape)\n    return feature_vector","a4762669":"# Testing image encodings\nencode_image(\"\/kaggle\/input\/flicker8k-dataset\/Flickr8k_Dataset\/Flicker8k_Dataset\/\"+\"1000268201_693b08cb0e.jpg\")","432764a6":"start = time()\nencoding_train = {}\n#image_id -->feature_vector extracted from Resnet Image\n\nfor ix,img_id in enumerate(train):\n    img_path = \"\/kaggle\/input\/flicker8k-dataset\/Flickr8k_Dataset\/Flicker8k_Dataset\"+\"\/\"+img_id+\".jpg\"\n    encoding_train[img_id] = encode_image(img_path)\n    \n    if ix%100==0:\n        print(\"Encoding in Progress Time step %d \"%ix)\n        \nend_t = time()\nprint(\"Total Time Taken :\",end_t-start)","4883327d":"# Store everything to the disk \nwith open(\"\/kaggle\/working\/encoded_train_features.pkl\",\"wb\") as f:\n    pickle.dump(encoding_train,f)","bca4bbda":"start = time()\nencoding_test = {}\n#image_id -->feature_vector extracted from Resnet Image\n\nfor ix,img_id in enumerate(test):\n    img_path = \"\/kaggle\/input\/flicker8k-dataset\/Flickr8k_Dataset\/Flicker8k_Dataset\"+\"\/\"+img_id+\".jpg\"\n    encoding_test[img_id] = encode_image(img_path)\n    \n    if ix%100==0:\n        print(\"Test Encoding in Progress Time step %d \"%ix)\n        \nend_t = time()\nprint(\"Total Time Taken(test) :\",end_t-start)","b5ba9e1a":"with open(\"\/kaggle\/working\/encoded_test_features.pkl\",\"wb\") as f:\n    pickle.dump(encoding_test,f)","9526be7d":"# Vocab\nlen(total_words)","8c781265":"word_to_idx = {}\nidx_to_word = {}\n\nfor i,word in enumerate(total_words):\n    word_to_idx[word] = i+1\n    idx_to_word[i+1] = word","1ffa4bde":"#word_to_idx[\"dog\"]\n#idx_to_word[1]\nprint(len(idx_to_word))","90f51ae3":"# Two special words\nidx_to_word[1846] = 'startseq'\nword_to_idx['startseq'] = 1846\n\nidx_to_word[1847] = 'endseq'\nword_to_idx['endseq'] = 1847\n\nvocab_size = len(word_to_idx) + 1\nprint(\"Vocab Size\",vocab_size)","ad9c61fa":"max_len = 0 \nfor key in train_descriptions.keys():\n    for cap in train_descriptions[key]:\n        max_len = max(max_len,len(cap.split()))\n        \nprint(max_len)","cb67e2b6":"def data_generator(train_descriptions,encoding_train,word_to_idx,max_len,batch_size):\n    X1,X2, y = [],[],[]\n    \n    n =0\n    while True:\n        for key,desc_list in train_descriptions.items():\n            n += 1\n            \n            photo = encoding_train[key+\".jpg\"]\n            for desc in desc_list:\n                \n                seq = [word_to_idx[word] for word in desc.split() if word in word_to_idx]\n                for i in range(1,len(seq)):\n                    xi = seq[0:i]\n                    yi = seq[i]\n                    \n                    #0 denote padding word\n                    xi = pad_sequences([xi],maxlen=max_len,value=0,padding='post')[0]\n                    yi = to_categorcial([yi],num_classes=vocab_size)[0]\n                    \n                    X1.append(photo)\n                    X2.append(xi)\n                    y.append(yi)\n                    \n                if n==batch_size:\n                    yield [[np.array(X1),np.array(X2)],np.array(y)]\n                    X1,X2,y = [],[],[]\n                    n = 0","5a1c7bdd":"f = open(\"\/kaggle\/input\/glove6b50dtxt\/glove.6B.50d.txt\",encoding='utf8')","94d97ad2":"embedding_index = {}\n\nfor line in f:\n    values = line.split()\n    \n    word = values[0]\n    word_embedding = np.array(values[1:],dtype='float')\n    embedding_index[word] = word_embedding\n\nf.close()","a6eb5b4d":"embedding_index['apple']","6b081102":"def get_embedding_matrix():\n    emb_dim = 50\n    matrix = np.zeros((vocab_size,emb_dim))\n    for word,idx in word_to_idx.items():\n        embedding_vector = embedding_index.get(word)\n        \n        if embedding_vector is not None:\n            matrix[idx] = embedding_vector\n            \n    return matrix\n        ","df1c840d":"embedding_matrix = get_embedding_matrix()\nembedding_matrix.shape","d7708f0b":"input_img_features = Input(shape=(2048,))\ninp_img1 = Dropout(0.3)(input_img_features)\ninp_img2 = Dense(256,activation='relu')(inp_img1)","ffd0107e":"# Captions as Input\ninput_captions = Input(shape=(max_len,))\ninp_cap1 = Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_captions)\ninp_cap2 = Dropout(0.3)(inp_cap1)\ninp_cap3 = LSTM(256)(inp_cap2)","63488d9a":"decoder1 = add([inp_img2,inp_cap3])\ndecoder2 = Dense(256,activation='relu')(decoder1)\noutputs = Dense(vocab_size,activation='softmax')(decoder2)\n\n# Combined Model\nmodel = Model(inputs=[input_img_features,input_captions],outputs=outputs)","8b33dab5":"model.summary()","07ab53f0":"# Important Thing - Embedding Layer\nmodel.layers[2].set_weights([embedding_matrix])\nmodel.layers[2].trainable = False","bd7a376a":"model.compile(loss='categorical_crossentropy',optimizer=\"adam\")","e906e329":"epochs = 20\nbatch_size = 3\nsteps = len(train_descriptions)","dd205c53":"def train():\n    \n    for i in range(epochs):\n        generator = data_generator(train_descriptions,encoding_train,word_to_idx,max_len,batch_size)\n        model.fit_generator(generator,epochs=1,steps_per_epoch=steps,verbose=1)\n        model.save('\/kaggle\/working\/model_'+str(i)+'.h5')","815679b7":"train()","13d8080f":"\nmodel = load_model('\/kaggle\/input\/img-cap\/model_9.h5')","2035dccf":"def predict_caption(photo):\n    \n    in_text = \"startseq\"\n    for i in range(max_len):\n        sequence = [word_to_idx[w] for w in in_text.split() if w in word_to_idx]\n        sequence = pad_sequences([sequence],maxlen=max_len,padding='post')\n        \n        ypred = model.predict([photo,sequence])\n        ypred = ypred.argmax() #WOrd with max prob always - Greedy Sampling\n        word = idx_to_word[ypred]\n        in_text += (' ' + word)\n        \n        if word == \"endseq\":\n            break\n    \n    final_caption = in_text.split()[1:-1]\n    final_caption = ' '.join(final_caption)\n    return final_caption","167c3c8e":"# Pick Some Random Images and See Results\nplt.style.use(\"seaborn\")\nfor i in range(5):\n    idx = np.random.randint(0,1000)\n    all_img_names = list(encoding_test.keys())\n    img_name = all_img_names[idx]\n    photo_2048 = encoding_test[img_name].reshape((1,2048))\n    i = plt.imread(\"\/kaggle\/input\/flicker8k-dataset\/Flickr8k_Dataset\/Flicker8k_Dataset\/\"+img_name+\".jpg\")\n    \n    caption = predict_caption(photo_2048)\n    #print(caption)\n    \n    plt.title(caption)\n    plt.imshow(i)\n    plt.axis(\"off\")\n    plt.show()","efcd6403":"\ntemp=encode_image(\"\/kaggle\/input\/img-cap\/download.jpeg\")\npict=plt.imread(\"\/kaggle\/input\/img-cap\/download.jpeg\")\ntemp2=temp.reshape((1,2048))\ncaption=predict_caption(temp2)\nplt.title(caption)\nplt.imshow(pict)\nplt.axis(\"off\")","500801ca":"\ntemp=encode_image(\"\/kaggle\/input\/img-cap\/hd7.jpg\")\npict=plt.imread(\"\/kaggle\/input\/img-cap\/hd7.jpg\")\ntemp2=temp.reshape((1,2048))\ncaption=predict_caption(temp2)\nplt.title(caption)\nplt.imshow(pict)\nplt.axis(\"off\")","3efa9d5e":"##Evaluation\nfrom nltk.translate.bleu_score import sentence_bleu\n\n\ntemp=encode_image(\"\/kaggle\/input\/img-cap\/download.jpeg\")\npict=plt.imread(\"\/kaggle\/input\/img-cap\/download.jpeg\")\ntemp2=temp.reshape((1,2048))\ncaption=predict_caption(temp2)\nplt.title(caption)\nplt.imshow(pict)\nplt.axis(\"off\")\n\nreference = [['two','men','are','running','over','mountain']]\ncandidate = [i for i in caption.split()]\nsc=sentence_bleu(reference, candidate)\nprint(sc)\n","bc5105be":"# **Train\/Test data**","4ce67b7f":"# **Vocabulary building**","fb9cc823":"# Image Feature extraction","96f1aa5f":"# Model","9db2a921":"# Pre processing captions","cf055e27":"# Data loader"}}