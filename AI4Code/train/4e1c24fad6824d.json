{"cell_type":{"44ce4c6a":"code","21d9a5fd":"code","d2642679":"code","3187df22":"code","c77aa7af":"code","1615dc84":"code","77942b82":"code","cf3bd927":"code","16d7365e":"code","86406278":"code","22fa3174":"code","e1b17e59":"code","241eeb4e":"code","590f57d8":"code","2d6d0ad3":"code","402c9ff3":"code","aaf023da":"code","23b171bf":"code","f3d39b4b":"code","719e89fa":"code","684bf937":"code","5e2257b8":"code","b97a4c10":"code","b81a4e4e":"code","75bf531d":"code","a5adb985":"code","e59d2e58":"code","4fb1042f":"code","640ee423":"code","b6607077":"code","43a7b1a3":"code","55e1fe19":"code","4aa84fc2":"code","956bb26e":"code","14f625f7":"code","371ecdd1":"code","1747e2e2":"code","3ff9b9e0":"code","332f3cee":"code","427cb1a4":"code","087650d3":"code","b2076b19":"code","131d4ea3":"code","0f8b26bf":"code","5c96652f":"code","643b87d0":"code","50c073be":"code","d6391ef0":"code","943b5f03":"code","5898233c":"code","9663fcc9":"code","45b5248e":"code","28f1ea4a":"code","ca28e41a":"code","437881dd":"code","8951a2f5":"code","47ab46f8":"code","8b657ee1":"code","e00724ef":"code","3b2111a3":"code","694cbc03":"code","a15c2b1e":"code","89b21ff1":"code","b1c70bc8":"code","17030bcd":"code","a532d9f9":"code","098fa717":"code","78af41a4":"code","7bfc8cac":"code","8cb45ffe":"code","3dbdcc0a":"code","f4b7981f":"code","677033a7":"code","b47ffc48":"code","9c94a2aa":"code","1ba90f5a":"code","f4d093d9":"code","2ba96666":"code","dd8d01dc":"code","9aeddf72":"code","0b870c06":"code","8ac266c6":"code","cefc1d65":"code","1dda69bf":"code","cf3b6a2b":"code","dd09b9c1":"code","b6728a72":"code","09542d71":"code","c228bffa":"code","1497f756":"code","355aa836":"code","f91e98e8":"code","ec432d68":"code","e1b3062c":"code","bb67a210":"code","c38612a2":"code","cbc0332c":"code","cf055814":"code","82e03f9b":"code","c5f6686f":"code","83502512":"code","dd81ae2b":"code","6d4fd42a":"code","9676f530":"code","ef4988de":"code","83e1eb1f":"code","78e9cb14":"code","54ed34eb":"code","56ca4b1c":"code","0992f880":"code","d53d6381":"code","ebb88839":"code","4bb2aa6c":"code","ca15ba6f":"code","eb13a8ff":"code","f3c231e5":"code","3a44a164":"code","d607d8e8":"code","26fc40ae":"code","f3689dd4":"code","176e4266":"code","7c3fec15":"code","e3003580":"code","ac9bd339":"code","b0e1e79a":"code","5fc1f94e":"code","375d3606":"code","f46cda92":"code","e295b6e5":"code","0a7ee2d8":"code","9c921df9":"code","89c414bd":"code","06d10bff":"code","f32bb648":"code","8054de8e":"code","7a72d1db":"code","3478b26f":"code","cf4d656e":"code","bdbd20db":"code","dbea539b":"code","45a54eff":"code","e95a57df":"code","4549a11b":"code","ab42b245":"code","af0ee737":"code","91e342ca":"code","b5da08f5":"code","7ae88168":"code","f305bcd4":"code","e53240f0":"code","e7bffdb8":"code","f4a43404":"code","05d46c73":"code","7e27fad8":"code","ad9ead55":"code","1ab4d733":"code","71dde714":"code","bca83ee9":"code","9151c145":"code","b140bee4":"code","6f87c369":"code","62c09dc2":"code","ab67a14b":"markdown","d881d35f":"markdown","d05e72c2":"markdown","fa64f17e":"markdown","6f0bb976":"markdown","afba3bf0":"markdown","321e473b":"markdown","e9a77343":"markdown","860fe724":"markdown","1c3232a8":"markdown","ef097969":"markdown","eb3998cf":"markdown","7d87d9d2":"markdown","bef2e098":"markdown","c9004bf8":"markdown","fc846368":"markdown","764d51ed":"markdown","df831bda":"markdown","ea737da4":"markdown","f1e562fe":"markdown","123984a2":"markdown","7a9d4c70":"markdown","bc9b43c7":"markdown","7afb4eb0":"markdown","6652e4bf":"markdown","f42dd251":"markdown","45f1eeab":"markdown","3690507b":"markdown","691cf4f3":"markdown","8777980d":"markdown","6b07a568":"markdown","d1321cf6":"markdown","a5e5bed1":"markdown","e7449ffb":"markdown","aa6fe189":"markdown","4333759d":"markdown","9360cc8f":"markdown","05ca4c07":"markdown","f3d3ffc6":"markdown","7bea1f0b":"markdown","55b6c788":"markdown","554c3707":"markdown","5ab75482":"markdown","d14a991e":"markdown","534e5185":"markdown","0af4baa0":"markdown","e50ef842":"markdown","a8190599":"markdown","e49bada0":"markdown","4b20b565":"markdown","0aeaa31c":"markdown","50e28280":"markdown","7891013f":"markdown","1f15323a":"markdown","58fe0a3e":"markdown","972d2f46":"markdown","acc95b71":"markdown","ac070589":"markdown","91b1a71a":"markdown","cf178238":"markdown","a89213d6":"markdown","8213bbc7":"markdown","53882797":"markdown","634cdb51":"markdown","84fb0c37":"markdown","9ed7028e":"markdown","0b07b330":"markdown","624cf1dc":"markdown","1aaa7ce6":"markdown","bf56600e":"markdown","e1b60183":"markdown","27ce9843":"markdown","384b83c5":"markdown","c48cf26c":"markdown","a5a5e762":"markdown","aae38bb4":"markdown","c55d41a2":"markdown","d11a7df9":"markdown","84ebd343":"markdown","f6c66e6f":"markdown","aa9820c2":"markdown","441595ec":"markdown","993dcc31":"markdown","9da2360e":"markdown","81908c09":"markdown","21921576":"markdown","cc8acdaf":"markdown","703c2363":"markdown","6e820a7f":"markdown","f9f2d4f2":"markdown","108b6891":"markdown","9ebc1f87":"markdown","bf5073e6":"markdown","37fc0044":"markdown","d29864c7":"markdown","7e3e0814":"markdown","e0ad2f28":"markdown","ef3dfc9d":"markdown","2f6cb959":"markdown","32350bff":"markdown","0075cdda":"markdown","1d6ab381":"markdown","144146ed":"markdown","dd854ff9":"markdown","5aa86e9e":"markdown","583ea030":"markdown","204eec89":"markdown","d974e4b9":"markdown","87b8168e":"markdown","ff7c2262":"markdown","2d7a0963":"markdown","41f04595":"markdown","e251747e":"markdown","489d69f5":"markdown","4e201bb3":"markdown","b6249e68":"markdown","eb2129bc":"markdown","80b8cc5e":"markdown","f4a2317a":"markdown","83c3f6a9":"markdown","a7eed139":"markdown","bf67cea2":"markdown","cc17a340":"markdown","f70e05c9":"markdown","f146538f":"markdown","b6d8397b":"markdown","da85c0be":"markdown","439b8a68":"markdown","6e09b763":"markdown","c17cbd46":"markdown","f04471bc":"markdown","b4bb0332":"markdown","8484d66b":"markdown","b30846e3":"markdown","b4cf0b59":"markdown","369bcac3":"markdown","0f95da79":"markdown","f1b61c07":"markdown","cae6b1b9":"markdown","7c9875d5":"markdown","4a997ea4":"markdown","9650dc74":"markdown","a6efa3fd":"markdown","5d1a271f":"markdown","1edbf715":"markdown","b586a162":"markdown","094f1ccc":"markdown","699d2913":"markdown","821d5c9d":"markdown","d4b0478d":"markdown","91098df8":"markdown","85e7f26a":"markdown","5cd41294":"markdown","558e210e":"markdown","e3b8b4c4":"markdown","171699f1":"markdown","03640496":"markdown","abfed0aa":"markdown","b95c8de2":"markdown","d9cbcff0":"markdown","cd08b0a6":"markdown","7099bf2f":"markdown","94980a15":"markdown","cc7372de":"markdown","4cc218e9":"markdown","af7e6380":"markdown","b8fc8125":"markdown","b83bceb4":"markdown","f900580b":"markdown","278fcddf":"markdown","5c26acff":"markdown","e917a15f":"markdown","38948395":"markdown","f7ff969e":"markdown","ea9288b1":"markdown","c36dccd9":"markdown","5314048b":"markdown","d40bcf76":"markdown","c2a0ccf3":"markdown","9c4f41f1":"markdown","16fdccda":"markdown","b9853b27":"markdown","86bfa1ee":"markdown","2f590ebd":"markdown","9c694d7c":"markdown","835a335f":"markdown","e5665899":"markdown","56ed44cf":"markdown","78f873ce":"markdown","626f8127":"markdown","37a1f863":"markdown","5038b5ad":"markdown","b631ea9a":"markdown","049287fa":"markdown","41e632c2":"markdown","a29e8aa7":"markdown","de520d8b":"markdown","9cc066bf":"markdown","15779aa0":"markdown","481956e5":"markdown","97192cf7":"markdown","2691da99":"markdown","6c6fc916":"markdown"},"source":{"44ce4c6a":"### [Environment setting]\nfrom __future__ import print_function\n\n## 0. Basics\nimport re\nimport math\nimport copy\nimport random\nimport datetime, time\nfrom operator import add\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import YouTubeVideo\n# skipping cells with a long runtime, if True\nfast = False\n# runtime checker\nstart_log = datetime.now()\n\n## 1. Data Science\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom plotnine import *\nfrom glob import glob\nimport missingno as msno\nimport folium\n# import plotly.offline as offline\n# import plotly.graph_objs as go\n\n## 2. Statistics\nimport statsmodels.api as sm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nfrom scipy import stats\n\n## 3. Forecasting\nfrom pandas.plotting import autocorrelation_plot as acplot\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom fbprophet import Prophet\nfrom fbprophet.diagnostics import cross_validation\nfrom fbprophet.diagnostics import performance_metrics\nfrom fbprophet.plot import plot_cross_validation_metric\n\n## 4. Machine Learning\nimport sklearn\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, r2_score\n\n## 5. Plots & Fonts\nplt.rcParams['figure.autolayout'] = True\nplt.rcParams['figure.figsize'] = (13, 7)\nplt.rcParams['axes.unicode_minus'] = False\nplt.rcParams.update({'font.size': 13})\ncolor_list = ['#8DD3C7', '#FEFFB3', '#BFBBD9'\n              , '#FA8174', '#81B1D2', '#FDB462'\n              , '#B3DE69', '#BC82BD', '#CCEBC4']","21d9a5fd":"### [Functions]\n\ndef get_data(path, transpose=False):\n    \"\"\"\n    FUNCTION\n        to get and check data from a file path\n    @ PARAMS\n        path(string) = file path for data\n        transpose(bool) = transpose a wide data to show vertically \/ default=False\n    > RETURN\n        df_raw = dataframe\n    \"\"\"\n    df_raw = pd.read_csv(path)\n    print('[Sample data]')\n    if transpose:\n        display(df_raw.head(3).append(df_raw.tail(3)).T)\n    else:\n        display(df_raw.head(3).append(df_raw.tail(3)))\n    return df_raw\n\ndef data_range(data, column, describe=None):\n    \"\"\"\n    FUNCTION\n        to check date range of time series or scarce chronological data\n    @ PARAMS\n        data(dataframe) = data to check\n        column(string) = column name for date\n        describe(string) = additional description on date column\n                           e.g. starting_date\n    > RETURN\n        None\n    \"\"\"\n    data_col = pd.to_datetime(data[column]).dt.date\n    date_range = (max(data_col) - min(data_col)).days + 1\n    if describe==None:\n        print(f'Date range: {date_range} days')\n    else:\n        print(f'Date range ({describe}): {date_range} days')\n    print(f'# {min(data_col)} to {max(data_col)}')\n    return None\n\n\n## [Refactor point] handle multiple subplots in 1 figure\ndef plot_groupby(data, groupby, column, title, ylabel=None, axis=None, tick_num=11):\n    \"\"\"\n    FUCNTION\n        to plot data after grouping by a column\n    @ PARAMS\n        data = dataframe to plot\n        groupby(string) = column name for grouping\n        column(string) = column name for y axis\n        title(string) = plot title\n        axis = specific axis to plot on\n    > RETURN\n        None\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(13, 7))\n    plt.title(f'{title}', fontsize=17)\n    ax.set_xlabel('Date', size=13)\n    if ylabel == None:\n        ax.set_ylabel('Number of cases', size=13)\n    else:\n        ax.set_ylabel(ylabel, size=13)\n    group_list = data.groupby(groupby)\n    for group in group_list:\n        if axis == None:\n            plt.plot(group[1].date.values\n                     , group[1][column].values\n                     , label=group[0])\n        else:\n            axis.plot(group[1].date.values\n                     , group[1][column].values\n                     , label=group[0])\n        \"\"\" Code Performance\n        0. Without .values\n            - 558 ms \u00b1 23.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n            - 537 ms \u00b1 27.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n            - 587 ms \u00b1 34.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n        1. With .values \n            - 508 ms \u00b1 34.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n            - 515 ms \u00b1 19.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n            - 540 ms \u00b1 19 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n        \"\"\"\n    tick_num = tick_num\n    ax.set_xticks(ax.get_xticks()[::int(len(age_raw.date.unique())\/tick_num)+1])\n    ax.legend()\n    plt.show()\n\n \ndef plot_lines(data, column_list, column_max, title):\n    \"\"\"\n    FUCNTION\n        to show many plots with combinations of lines with consistent colors and legend\n        useful for plotting lines with different scales at once and then separately\n    @ PARAMS\n        data = dataframe to plot\n        column_list = columns to have numeric values, each can be the initial column to plot up to column_max\n        column_max = the last column to plot from the initial column in column_list for len(column_list)\n        title(string) = plot title\n    > RETURN\n        None\n    \"\"\"\n    for i in column_list:\n        fig, ax = plt.subplots(figsize=(13, 7))\n        plt.title(f'{title}', fontsize=17)\n        color_group = color_list[:-4][-(column_max-i):]\n        for test_each, color_each in zip(data.columns[i:column_max], color_group):\n            plt.plot(data.date, data[test_each]\n                     , label=test_each, color=color_each\n                    )\n            label=data[test_each]\n        ax.set_xticks(ax.get_xticks()[::int(len(data.date)\/8)])\n        plt.xlabel('Date', size=13)\n        plt.ylabel('Number of cases', size=13)\n        ax.legend(loc='upper left')\n        plt.show()\n\n\ndef split_fit_pred(X, y, model, test_size=0.3, random_state=13, classification=False):\n    \"\"\"\n    Function\n        to split data into training\/test sets, fit a model, and predict with them in one-shot manner\n    @params \n        X = independent variables\n        y = dependent variable\n        model = model to train\n        test_size = size of test set \/ 0<s<1 \/ default=0.3\n        random_state = random seed for stable testing results \/ default=13\n        classification = if classification, split X with stratifying y labels \/ default=False(linear)\n    >return\n        model = a fitted model\n        prediction = predictions by the model\n        train_score = a score by the predictions in training\n        test_score = a score by the predictions in test\n    \"\"\"\n    # Split\n    if classification == True:\n        X_train, X_test, y_train, y_test = train_test_split(X, y\n                                                            , stratify = y\n                                                            , test_size = test_size\n                                                            , random_state = random_state)\n    else:\n        X_train, X_test, y_train, y_test = train_test_split(X, y\n                                                            , test_size = test_size\n                                                            , random_state = random_state)\n    # Fit & validate\n    model.fit(X_train, y_train)\n    prediction = model.predict(X_test);\n    train_score, test_score = model.score(X_train, y_train), model.score(X_test, y_test)\n    return model, prediction, train_score, test_score\n\n\ndef test_with_range(X, y, model, size_range, random_state=13):\n    \"\"\"\n    Function\n        to test a model with various test set sizes\n    @params\n        X = independent variables\n        y = dependent variable\n        model = model to train\n        size_range = the range of test sets size to experiment with \/ 0<s<1\n        random_state = random seed for stable testing results \/ default=13\n    @return\n        train_list = list of training scores by speicifc value in the given range\n        test_list = list of test scores by speicifc value in the given range\n        best_test_size = the best size with the best score in test_list\n    \"\"\"\n    plt.title(f'Score by Test Set Size (random seed={random_state})', size=17)\n    train_list, test_list = list(), list()\n    for s in size_range:\n        model, prediction, train_score, test_score = split_fit_pred(X, y, model, random_state=random_state, test_size=s)\n        train_list.append(train_score); test_list.append(test_score)\n    # train_list, test_list\n    plt.plot(size_range, train_list)\n    plt.plot(size_range, test_list)\n    plt.legend(['train_score', 'test_score'])\n    plt.xlabel('Test set size')\n    plt.ylabel('Score')\n    plt.show()\n    best_test_size = size_range[np.nanargmax(test_list)]\n    print('Best train score: {} (test set size: {})'\n          .format(round(np.nanmax(train_list), 2), round(size_range[np.nanargmax(train_list)], 2)))\n    print('Best test score: {} (test set size: {})'\n          .format(round(np.nanmax(test_list), 2), round(best_test_size, 2)))\n    return train_list, test_list, best_test_size\n\n\n## [Refactor point] need to be merged into test_with_range() as both share a lot except for data type\ndef test_with_range_ts(X, y, model, size_range, variable):\n    \"\"\"\n    Function\n        to test a model with various sizes on time series data\n    @params \n        X = independent variables\n        y = dependent variable\n        model = model to train\n        size_range = the range of test sets size to experiment with \/ 0<s<1\n        random_state = random seed for stable testing results \/ default=13\n    >return\n        train_list = list of training scores by speicifc value in the given range\n        test_list = list of test scores by speicifc value in the given range\n        best_test_size = the best size with the best score in test_list\n    \"\"\"\n    X = np.array(X).reshape(-1, 1)\n    train_list, test_list, pred_list = list(), list(), list()\n    for i in size_range:\n        X_train, X_test, y_train, y_test = X[:-i, :], X[-i:, :], y[:-i], y[-i:]\n        model.fit(X_train, y_train)\n        train_pred = list(model.predict(X_train))\n        test_pred = list(model.predict(X_test))\n        train_score = model.score(X_train, y_train)\n        test_score = model.score(X_test, y_test)\n        train_list.append(train_score)\n        test_list.append(test_score)\n        pred_list.append(train_pred + test_pred)\n    plt.figure(figsize=(13, 7))\n    plt.title(f'Score by Test Set Size (%s)'%variable, size=17)\n    plt.plot(size_range\/len(X), train_list)\n    plt.plot(size_range\/len(X), test_list)\n    plt.legend(['Training score', 'Test score'])\n    plt.xlabel('Test set size')\n    plt.ylabel('Score')\n    plt.show()\n    best_score = np.nanmax(test_list)\n    best_size = size_range[np.nanargmax(test_list)]\n    best_pred = pred_list[np.nanargmax(test_list)]\n    print(f'Best training score: %.4f (test set size=%.2f)'%(np.nanmax(train_list)\n                                                          , size_range[np.nanargmax(train_list)]\/len(X)))\n    print(f'Best test score: %.4f (test set size=%.2f)'%(np.nanmax(test_list)\n                                                       , best_size\/len(X)))\n    return train_list, test_list, best_pred, best_score, best_size\n\n\ndef get_recent_to_max(data, group='Country_Region', min_cases=1, recent_window=3, max_num=1):\n    \"\"\"\n    FUNCTION \n        to convert raw data with cases by subregion into with that by country\n        to add a column measuring which phase of spreading a country is in\n            recent_to_max = n latest observation (3 latest, in case the last is 0) \n                            divided by\n                            max daily case of a country\n    @ PARAMS \n        data = data to convert\n        group = criterion for grouping target values\n        min_caes = minimum number of cases in a country to be listed \/default=1\n        recent_window = how many observations to apply in recent_to_max calculation \/ default=3\n        max_num = number of max values to be averaged (1st max, 2nd max, ..., Nth max) \/ default=1\n    > RETURN \n        rtm_df = DataFrame with columns of total_cases and recent_to_max (indexed by country)\n        data_group = DataFrame grouped by group column\n    \"\"\"\n    ## 1. Get source data by country \/ set dictionary to save total cases and recent_to_max\n    data_group = data.groupby(group)\n    total_dict, rtm_dict = dict(), dict()\n    for i in data_group:\n        if i[1].TargetValue.sum() > min_cases:\n            total_dict[i[0]] = i[1].TargetValue.sum()\n    ## 2-1. Handle 0 latest cases (for being robust to the exceptional 0 day)\n            if i[1].TargetValue.max() in i[1].TargetValue[-recent_window:].values:\n                rtm_dict[i[0]] = 1\n            else:\n    ## 2-2. Average N max values (for being robust to the errorneous max value)\n                if max_num > 1:\n                    temp = copy.deepcopy(i[1].TargetValue)\n                    max_list = list()\n                    for j in range(max_num):\n                        max_list.append(max(temp))\n                        temp.pop(np.argmax(temp))\n                    max_val = np.mean(max_list)\n                else:\n                    max_val = i[1].TargetValue.max()\n                rtm_dict[i[0]] = i[1].TargetValue[-recent_window:].mean() \/ max_val\n    ## 3. Get the result dataframe\n    rtm_df = pd.DataFrame(np.array([list(total_dict.values()), list(rtm_dict.values())]).T\n                          , index = total_dict.keys()\n                          , columns = ['total_cases', 'recent_to_max'])\n    rtm_df = rtm_df.sort_values('total_cases', ascending=False)\n    return rtm_df, data_group\n\n\ndef plot_phase(data_grouped, rtm_df, no_of_country=3, trim=False, scale=False, reverse=False):\n    \"\"\"\n    FUNCTION \n        to plot the daily confirmed cases for each country\n        to check in which phase (early, middle, late) of spreading it is\n    @ PARAMS \n        group = list of time series data grouped by a column\n        rtm_df = dataframe with total_cases and recent_to_max columns per country\n        no_of_country = number of countries to plot\n        trim = starts from the day with 1st case (than the given first)\n        scale = scale target values to the max in a given period\n        reverse = reverse the given order of index\n    > RETURN \n        None\n    \"\"\"\n    ## 1. Set list of countries to plot\n    if reverse:\n        country_list = rtm_df.index[-no_of_country:][::-1]\n    else:\n        country_list = rtm_df.index[0:no_of_country]\n    ## 2. Plot daily cases for each country\n    for i, j in zip(country_list, np.arange(1, len(country_list)+1)):\n        group = data_grouped.get_group(i)\n        if trim:\n            # starts from the day with 1st case\n            first_case_idx = int(group[group.TargetValue > 0].head(1).index.values)\n        else:\n            first_case_idx = 0\n        group = group.loc[first_case_idx:, :] # loc than iloc\n        group = group.groupby('Date').sum()\n        group = group.loc[ :, ['TargetValue'] ]\n        fig, ax = plt.subplots(figsize=(13, 7))\n        plt.title('Daily Confirmed Cases - {0}. {1} (total cases={2}, recent to max={3})'\n                  .format(j, i\n                          , int(rtm_df.loc[i, \"total_cases\"])\n                          , round(rtm_df.loc[i, \"recent_to_max\"], 4))\n                  , size=17)\n        if scale:\n            group.TargetValue = group.TargetValue \/ max(group.TargetValue) * 100\n        ax.plot(group.index\n                , group.TargetValue)\n        ax.set_xticks(ax.get_xticks()[::int(len(group.index)\/8)])\n        ax.set_xlabel('Confirmed date')\n        ax.set_ylabel('Number of cases', size=13)\n    return None\n    \n    \ndef arima_grid(truth, p_anchor, d_anchor, q_anchor, window):\n    \"\"\"\n    FUNCTION\n        to grid search for ARIMA model parameters (p, d, q)\n    @ PARAMS\n        truth = dataframe saving Date and TargetValue columns\n        p_anchor, d_anchor, q_anchor = mid point for list of possible parameters\n        window = range of extending anchor values (-\/+ window applied to each anchor)\n    > RETURN\n        param_list = list of parameter sets\n        mape_list = list of MAPE per each parameter sets\n    \"\"\"\n    p_range = np.arange(max(0, p_anchor-window), p_anchor+window+1)\n    d_range = np.arange(max(0, d_anchor-window), d_anchor+window+1)\n    q_range = np.arange(max(0, q_anchor-window), q_anchor+window+1)\n    param_list, mape_list = list(), list()\n    for p in p_range:\n        for d in d_range:\n            for q in q_range:\n                arima = ARIMA(truth.TargetValue\n                              , dates=truth.Date\n                              , order=(p, d, q)\n                              , freq=\"D\").fit()\n                arima_pred = arima.predict()\n                _, _, _, mape = diff_metrics(truth.TargetValue[:-1], arima_pred[1:])\n                param_list.append(f'{p}, {d}, {q}')\n                mape_list.append(mape)\n    plt.plot(param_list, mape_list)\n    plt.title('MAPE by parameter set (p, d, q)', size=17)\n    plt.xlabel('Parameter set', size=13)\n    plt.ylabel('MAPE (Mean Absolute Percentage Error)', size=13)\n    plt.xticks(rotation=31)\n    plt.show()\n    return param_list, mape_list\n\n\ndef do_prophet(data, date_column, y_column\n               , benchmark=None, test_size=1\n               , is_bench=False, bm_name='best model at present'\n               , season_mode='additive', custom_period=False, period=None, num_curve=5\n               , simple=True\n               ):\n    \"\"\"\n    Function \n        to apply Prophet with default parameters\n    @Params \n        data = data before being formatted in Prophet manner\n        date_column = column name for time steps\n        y_column = column name for independent variable\n        benchmark = benchmark prediction to plot\n        test_size = size for forecasting \/ 1<s<len(data) \/ default=1\n        is_bench = if is_bench, plot benchmark prediction too \/ default=True\n        bm_name = name (or description) for benchmark method\n        sesson_mode = the mode for calculating seasonality \/ default='additive', optional='multiplicative'\n        custom_period = set and use custom period\n        period = custom period \/ format='n days(or other time unit)'\n        num_curve = how many curves are allowed \/ default=5 \/ the more, the more flexible, prone to overfitting, and time need to fit\n        simple = if simple, don't print out intermediate details (only plot the last prediction plot) \/ default=True\n    >return \n        pp = the fitted Prophet model (useful for cross validation)\n        predict = the predictions by the model (useful for visualization)\n    \"\"\"\n    ## 1. Prepare a dataframe\n    forecast_df = data[ [date_column, y_column] ]\n    if not simple:\n        print('[Raw Data - sample]')\n        display(forecast_df.head(3).append(forecast_df.tail(3)))\n    forecast_df = data.rename(columns = {date_column: 'ds' # ds: Date Stamp\n                                         , y_column: 'y'}) # y: values to forecast\n    # forecast_df.ds = pd.to_datetime(forecast_df.ds) # string to datetime\n    ## 2. Split\n    forecast_train, forecast_test = forecast_df.iloc[:-test_size, :], forecast_df.iloc[-test_size:, :]\n    if not simple:\n        print(f'Training-set size: %d\\nTest-set size: %d\\n'%(forecast_train.shape[0], forecast_test.shape[0]))\n    ## 3. Fit\n    if custom_period:\n        pp = Prophet(\\\n                     seasonality_mode=season_mode\n                     , yearly_seasonality=False\n                     , weekly_seasonality=False\n                     , daily_seasonality=False\n                    ).add_seasonality(name='custom'\n                                      , period=period\n                                      , fourier_order=num_curve)\n    else:\n        pp = Prophet()\n    pp.fit(forecast_train)\n    if not simple:\n        print('# Prophet model trained\\n')\n    ## 4. Predict\n    frame = pp.make_future_dataframe(periods=test_size)\n    predict = pp.predict(frame)\n    if not simple:\n        print('[Forecasting result - sample]')\n        display(predict.tail(3).T)\n        print('# yhat = predicted point value')\n    ## 5. Plot predictions\n    fig, ax = plt.subplots(figsize=(13, 7))\n    plt.title('Confirmed Cases Prediction (Prophet)', size=17)\n    plt.plot(forecast_df.ds, forecast_df.y\n             , color='#33322B', ls=':', lw=3)\n    plt.plot(forecast_df.ds, predict.yhat)\n    if is_bench:\n        plt.plot(forecast_df.ds, benchmark)\n    ax.axvline(forecast_df.ds.values[len(forecast_df)-test_size], ls=':', color='crimson')\n    ax.set_xticks(ax.get_xticks()[::int(len(forecast_df.ds)\/8)])\n    if is_bench:\n        ax.legend(['Truth'\n                   , 'Prediction (Prophet)'\n                   , f'Benchmark (%s)'%bm_name\n                   , 'Test starts']\n                  , fontsize=11\n                  , loc='upper left'\n                 )\n    else:\n        ax.legend(['Truth'\n                   , 'Prediction (Prophet)'\n                   , 'Test starts']\n                  , fontsize=11\n                  , loc='upper left'\n                 )\n    plt.xlabel('Confirmed date', size=13)\n    plt.ylabel('Number of cases', size=13)\n    plt.show()\n    return pp, predict\n\n\n## [Refactor point] need to embrace multi-steps forecasting\ndef result_df(date, truth, pred):\n    \"\"\"\n    FUNCTION \n        to make a dataframe for test results for one-step forecasting\n    @ PARAMS \n        date(string) = column name for date\n        truth(numeric) = truth value\n        pred(numeric) = predicted value\n    > RETURN \n        result = result dataframe with 'truth, prediction, difference(truth-prediction)' as columns\n    \"\"\"\n    col = ['truth', 'prediction', 'difference']\n    result = pd.DataFrame(columns=[col], index=[date])\n    result.loc[date, 'truth'] = truth\n    result.loc[date, 'prediction'] = pred\n    result.loc[date, 'difference'] = abs(truth-pred)\n    return result\n\n\ndef diff_metrics(truth, pred):\n    \"\"\"\n    FUNCTION\n        to calculate difference (predictions-truth) and the performance metrics\n    @ PARAMS\n        truth(sequences of numeric values) = truth values\n        pred(sequences of numeric values) = predicted values\n    > RETURN\n        diff = pred - truth\n        rmse = Root Mean Sqaure Error\n        mae = Mean Absolute Error\n        mape = Mean Absolute Percentage Error\n    \"\"\"\n    diff = np.array(pred) - np.array(truth)\n    rmse = np.sqrt(np.mean(diff**2))\n    mae = np.mean(np.abs(diff))\n    mape = np.mean(np.abs(diff) \/ (np.array(truth) + 1))\n    return diff, rmse, mae, mape\n\n\ndef perform_report(truth, pred):\n    \"\"\"\n    FUNCTION\n        to get performance report as dataframes (dependent on diff_metrics())\n    @ PARAMS\n        truth(sequences of numeric values) = truth values\n        pred(sequences of numeric values) = predicted values\n    > RETURN\n        result = dataframe with columns of 'truth, prediction, difference'\n        metrics = dataframe with columns of 'rmse, mae, mape'\n    \"\"\"\n    diff, rmse, mae, mape = diff_metrics(truth, pred)\n    result = pd.DataFrame(\n        {'truth': truth\n         , 'prediction': pred\n         , 'difference': diff}\n    )\n    metrics = pd.DataFrame(\n        {'rmse': rmse\n         , 'mae': mae\n         , 'mape': mape}\n        , index=['score']\n    )\n    display(result)\n    display(metrics)\n    return result, metrics\n\n\ndef set_style():\n    \"\"\"\n    FUNCTION\n        to set color theme with a custom dark, plus additional style settings like figsize\n    @ PARAMS\n        None\n    > RETURN\n        None\n    \"\"\"\n    plt.style.use('seaborn')\n    plt.figure(figsize=(13, 1.3))\n    plt.title('Plot Style Set - seaborn')\n    plt.plot(np.linspace(np.pi, np.pi**np.pi))\n    plt.show()\n    plt.style.use('dark_background')\n    plt.rcParams['axes.grid.axis'] = 'y'\n    plt.rcParams['grid.linestyle'] = ':'\n    plt.rcParams['grid.color'] = '#33322B'\n    plt.rcParams['figure.autolayout'] = True\n    plt.rcParams['figure.figsize'] = (13, 7)\n    plt.rcParams['axes.unicode_minus'] = False\n    plt.rcParams.update({'font.size': 13})\n    plt.figure(figsize=(13, 1.3))\n    plt.title('Plot Style Set - dark, again')\n    plt.plot(np.linspace(np.pi, np.pi**np.pi))\n    plt.show()\n    \nset_style()","d2642679":"## Get data\nimport os\nlast_update = '2020-06-30'\nprint(f'[Dataset list]\\n# updated on %s'%last_update)\nfile_paths_raw = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/coronavirusdataset'):\n    for filename in filenames:\n        file_paths_raw.append(os.path.join(dirname, filename))\nfile_names = ['TimeAge.csv', 'TimeProvince.csv', 'Region.csv', 'TimeGender.csv'\n              , 'Time.csv', 'Case.csv', 'Weather.csv', 'PatientInfo.csv'\n              , 'PatientRoute.csv', 'SeoulFloating.csv', 'SearchTrend.csv', 'Policy.csv']\nfile_paths = [ j for i in file_names for j in file_paths_raw if i in j ]\nfile_paths","3187df22":"## 1. Get \/ Check data\nage_raw = get_data(file_paths[0])\ndata_range(age_raw, 'date')\nage_list = age_raw.age.unique()\nprint('Age groups:', age_list)\nprint('# 80s == 80s and older')\n\n## 2. Plot cases by age\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title(f'Confirmed Cases by Age ({last_update})', fontsize=17)\nsns.barplot(age_list, age_raw.confirmed[-9:])\nax.set_xlabel('age', size=13)\nax.set_ylabel('number of cases', size=13)\nplt.show()","c77aa7af":"## 1. Get population distribution table\npop_order = pd.DataFrame()\npop_order['age'] = age_list\npop_order['population'] = (4055740, 4732100, 6971785, 7203550, 8291728, 8587047, 6472987, 3591533, 1874109)\npop_order['proportion'] = round(pop_order['population']\/sum(pop_order['population']) * 100, 2)\npop_order = pop_order.sort_values('population', ascending=False)\npop_order.set_index(np.arange(1, 10), inplace=True)\ndisplay(pop_order)\n\n## 2. Plot population distribution\n    # code source for donut chart: https:\/\/python-graph-gallery.com\/162-change-background-of-donut-plot\/\ncolor_pie = [color_list[5], color_list[4], color_list[3]\n             , color_list[2], color_list[6], color_list[1]\n             , color_list[0], color_list[7], color_list[8]]\nfig, ax = plt.subplots(figsize=(11, 11))\nplt.title('Population Distribution (2020)', fontsize=17)\npop_circle = plt.Circle((0,0), 0.79, color='black')\nplt.pie(pop_order.proportion\n        , labels = pop_order.age\n        , autopct = '%.2f%%'\n        , colors = color_pie\n        , startangle=90\n        , counterclock=False)\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()","1615dc84":"## 1. Get a new table with population \/ proportion by age\nconfirmed_by_population = pop_order.sort_values('age')\nconfirmed_by_population['confirmed'] = list(age_raw[-9:].confirmed)\n\n## 2. Get confirmed ratio regarding population\nconfirmed_by_population['confirmed_rate'] = confirmed_by_population['confirmed']\/confirmed_by_population['population'] * 100;\ndisplay(confirmed_by_population)\n\n## 3. Plot confirmed rate by age\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Population-adjusted Confirmed Rate by Age', fontsize=17)\nax.bar(age_list, confirmed_by_population.confirmed_rate[-9:], color=color_list)\nax.set_xlabel('Age', size=13)\nax.set_ylabel('Confirmed rate (%)', size=13)\nplt.show()","77942b82":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(23, 7))\n\n## 1. Absolute numbers\naxes[0].set_title('Confirmed Cases by Age', fontsize=15)\naxes[0].bar(age_list, confirmed_by_population.confirmed, color=color_list)\n\n## 2. Confirmed rate\naxes[1].set_title('Population-adjusted Confirmed Rate', fontsize=15)\naxes[1].bar(age_list, confirmed_by_population.confirmed_rate, color=color_list)\n\nplt.show()","cf3bd927":"## Plot time series of confirmed cases\nplot_groupby(age_raw, 'age', 'confirmed', 'Confirmed Cases by Age (cumulative)')","16d7365e":"## 1. Plot time series of deceased cases\nplot_groupby(age_raw, 'age', 'deceased', 'Deceased Cases by Age (cumulative)')\n\n## 2. Check absolute numbers\nage_deceased = age_raw.tail(9)[['age', 'deceased']]\nage_deceased.set_index(np.arange(0, len(age_raw.age.unique())), inplace=True)\nprint('[Latest deceased cases]')\ndisplay(age_deceased.T)","86406278":"## 1. Add a column for deceased rate \nage_raw['deceased_rate'] = age_raw.deceased\/age_raw.confirmed * 100.0\n\n## 2. Plot time series of deceased rate\nplot_groupby(age_raw, 'age', 'deceased_rate', 'Deceased Rate by Age (cumulative)', 'Deceased Rate')","22fa3174":"## Three graphs side by side\nif not fast:\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(23, 7))\n    sub_list = [age_raw.confirmed, age_raw.deceased, age_raw.deceased_rate]\n    title_list = ['Confirmed Cases', 'Deceased Cases', 'Deceased Rate']\n    for sub, i, title in zip(sub_list, range(len(sub_list)), title_list):\n        confirmed_set = sub.groupby(age_raw.age)\n        for confirmed_each, age_each in zip(confirmed_set, age_list):\n            axes[i].plot(age_raw.date.unique(), confirmed_each[1], label=age_each)\n            axes[i].set_title(title, size=17)\n        axes[i].set_xticks(axes[i].get_xticks()[::int(len(age_raw.date.unique())\/5)])\n        axes[i].legend(fontsize=13)","e1b17e59":"# 1071 rows in total: cases in each province on a certain date\nregion_raw = get_data(file_paths[1])\ndata_range(region_raw, 'date')","241eeb4e":"# There are 17 provinces (we call it region from now on)\n    # as there are cities in the columns too\nprint('Number of regions:', len(region_raw.province.unique()))\nprint('Number of logs per region:', len(region_raw[region_raw.province=='Jeju-do']))\nprint('regions * logs:', len(region_raw.province.unique()) * len(region_raw[region_raw.province=='Jeju-do']))\nprint('Number of rows:', len(region_raw))","590f57d8":"region_raw.describe().iloc[1:, 1:]","2d6d0ad3":"# Get the latest distribution of cumulative confirmed cases\nloc_latest = region_raw[region_raw.date==region_raw.date.iloc[-1]]\ndel loc_latest['date']\ndel loc_latest['time']\nloc_latest = loc_latest.iloc[:, :2]\nloc_latest['proportion'] = round(loc_latest.confirmed \/ sum(loc_latest.confirmed) * 100, 2)\nloc_latest = loc_latest.sort_values('proportion', ascending=False)\nloc_latest.set_index(np.arange(1, len(loc_latest)+1), inplace=True)\nloc_latest_all = loc_latest.copy()\nloc_latest_all","402c9ff3":"# Put other than the 5 regions with most cases into others\nloc_latest.loc['18',:] = loc_latest.iloc[6:, :].sum()\nloc_latest.loc['18','province'] = 'Others'\nloc_latest = loc_latest[loc_latest.proportion > loc_latest.iloc[5, 2]]\nloc_latest\n\n# Distribution Graph\nfig, ax = plt.subplots(figsize=(11, 11))\nplt.title(f'Confirmed Cases Distribution by Region ({last_update})', fontsize=17)\npop_circle=plt.Circle((0,0), 0.79, color='black')\nplt.pie(loc_latest.proportion\n        , labels=loc_latest.province\n        , autopct='%.2f%%'\n#         , explode=(0.03, 0, 0, 0, 0, 0, 0)\n        , startangle=90\n        , counterclock=False\n       )\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()","aaf023da":"# Total\t\t\ntotal_list = region_raw.groupby('date').sum().confirmed\n\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Cumulative Confirmed Cases (total)', fontsize=17)\nax.set_xlabel('Date', size=13)\nax.set_ylabel('Number of cases', size=13)\nplt.plot(region_raw.date.unique()\n         , region_raw.groupby('date').sum().confirmed)\nax.set_xticks(ax.get_xticks()[::int(len(region_raw.date.unique())\/8)])\nplt.show()","23b171bf":"pd.DataFrame(total_list[total_list<=104].tail(3)).T","f3d39b4b":"pd.DataFrame(total_list[total_list>=7900].head(3)).T","719e89fa":"if not fast:\n    print('[Confirmed Cases in each region (most to least)]')\n    # Region list by order of confirmed cases (based on latest data)\n    region_list = region_raw[region_raw.date==region_raw.date.iloc[-1]]\\\n                            .sort_values('confirmed', ascending = False)\\\n                            .province\n\n    # Get a graph for each region\n    for region_name in region_list:\n        fig, ax = plt.subplots(figsize=(13, 7))\n        plt.title(f'Confirmed Cases ({region_name})', fontsize=17)\n        ax.set_xlabel('Date', size=13)\n        ax.set_ylabel('Number of cases', size=13)\n        region_each = region_raw[region_raw.province==region_name]\n        plt.plot(region_each.date.unique(), region_each.confirmed)\n        ax.set_xticks(ax.get_xticks()[::int(len(region_each.date.unique())\/8)])\n        plt.show()","684bf937":"## 1. Without 2 outliers\nwo_outliers = region_raw[(region_raw.province!='Daegu') \n                           & (region_raw.province!='Gyeongsangbuk-do')\n                          ].groupby('date').sum().confirmed\n## 2. Plot cases\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Confirmed Cases (without Daegu, Gyeongsangbuk-do)', fontsize=17)\nax.set_xlabel('Date', size=13)\nax.set_ylabel('Number of cases', size=13)\nplt.plot(region_raw.date.unique(), wo_outliers)\nax.set_xticks(ax.get_xticks()[::int(len(region_raw.date.unique())\/8)])\nplt.show()","5e2257b8":"## 1. The regions with 2 most cases\noutliers = region_raw[(region_raw.province=='Daegu')\\\n                        | (region_raw.province=='Gyeongsangbuk-do')\n                       ].groupby('date').sum().confirmed\n## 2. Compare by plot\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Confirmed cases (total \/ outliers \/ without outliers)', fontsize=17)\nplt.plot(region_raw.date.unique(), total_list, color=color_list[2])\nplt.plot(region_raw.date.unique(), outliers, color=color_list[1])\nplt.plot(region_raw.date.unique(), wo_outliers, color=color_list[0])\nax.set_xticks(ax.get_xticks()[::int(len(region_raw.date.unique())\/8)])\nplt.xlabel('Date')\nplt.ylabel('Number of cases')\nplt.legend(['Total', 'Outliers (Daegu, Gyeongsangbuk-do)', 'Without outliers'])\n\nplt.show()","b97a4c10":"loc_meta_raw = pd.read_csv(file_paths[2])\nprint('[Sample data]')\nloc_meta_raw.tail(3).T","b81a4e4e":"print('Number of unique regions:', len(loc_meta_raw.province.unique()))\nprint(loc_meta_raw.province.unique())","75bf531d":"loc_meta_raw[loc_meta_raw.province=='Korea'].T","a5adb985":"elderly_pop_df = pd.DataFrame(loc_meta_raw[loc_meta_raw.province!='Korea']\n                          .groupby('province').mean()\n                          .elderly_population_ratio\n                          .sort_values(ascending=False)\n                         )\nelderly_pop_df","e59d2e58":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Elderly population proportion by region', fontsize=17)\nplt.xticks(rotation=31)\nplt.bar(elderly_pop_df.index, elderly_pop_df.elderly_population_ratio, color=color_list[1])\nplt.xlabel('region')\nplt.ylabel('Proportion to total population (%)')\nplt.show()","4fb1042f":"elderly_pop_ordered = elderly_pop_df.loc[loc_latest_all.province.values, :].elderly_population_ratio\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Elderly population proportion (order by confirmed cases - most to least)', fontsize=17)\nplt.xticks(rotation=43)\nplt.xlabel('Region')\nplt.ylabel('Proportion to total population (%)')\nplt.bar(elderly_pop_ordered.index, elderly_pop_ordered, color=color_list[1])\nplt.show()","640ee423":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Confirmed Cases by Region (without 2 outliers - most to least)', fontsize=17)\nplt.xticks(rotation=37)\nplt.bar(loc_latest_all[2:].province\n        , loc_latest_all[2:].confirmed\n        , color=color_list[1])\nplt.xlabel('region')\nplt.ylabel('Number of cases')\nplt.show()","b6607077":"pop_dense = pd.DataFrame()\npop_dense['region'] = loc_latest_all.province\npop_dense['population'] = np.divide(\n    [2450, 2674, 13031, 9705, 2180, 3400\n     , 3356, 304, 2939, 1154, 1619, 1521\n     , 1518, 1493, 1820, 1790, 653]\n    , 1000 # thousand to million\n)\n\n# density = number of people \/ km\u00b2\npop_dense['density'] = [2773, 141, 16034, 1279, 265, 4416\n                        , 318, 653, 2764, 1088, 219, 90\n                        , 2813, 2980, 226, 145, 353]\npop_dense","43a7b1a3":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Population (order by confirmed cases - most to least)', fontsize=17)\nplt.bar(pop_dense.region, pop_dense.population, color=color_list[1])\nplt.xticks(rotation=43)\nplt.xlabel('Region')\nplt.ylabel('Number of people (millions)', size=13)\nplt.show()","55e1fe19":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Population Density (order by confirmed cases)', fontsize=17)\nplt.bar(pop_dense.region, pop_dense.density, color=color_list[1])\nplt.ylabel('Number of people (per km\u00b2)', size=13)\nplt.xlabel('Region')\nplt.xticks(rotation=43)\nplt.show()","4aa84fc2":"sex_raw = get_data(file_paths[3])\ndata_range(sex_raw, 'date')","956bb26e":"fig, ax = plt.subplots(figsize=(11, 11))\nplt.title(f'Confirmed Cases Distribution by Sex ({last_update})', fontsize=17)\npop_circle=plt.Circle((0,0), 0.79, color='black')\nplt.pie(sex_raw.confirmed[-2:]\n        , labels=['male', 'female']\n        , autopct='%.2f%%'\n        , startangle=90\n        , counterclock=False)\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()","14f625f7":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Confirmed Cases by Sex (cumulative)', fontsize=17)\nsex_confirmed = (sex_raw[sex_raw.sex=='male'].confirmed, sex_raw[sex_raw.sex=='female'].confirmed)\nfor sex_each, sex_label in zip(sex_confirmed, ['male', 'female']):\n    plt.plot(sex_raw.date.unique(), sex_each, label=sex_label)\nax.set_xticks(ax.get_xticks()[::int(len(sex_raw.date.unique())\/8)])\nplt.xlabel('Date')\nplt.ylabel('Number of cases')\nax.legend()\nplt.show()","371ecdd1":"fig, ax = plt.subplots(figsize=(11, 11))\nplt.title(f'Deceased Cases Distribution by Sex ({last_update})', fontsize=17)\npop_circle=plt.Circle((0,0), 0.79, color='black')\nplt.pie(sex_raw.deceased[-2:]\n        , labels=['male', 'female']\n        , autopct='%.2f%%'\n        , startangle=90\n        , counterclock=False)\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()","1747e2e2":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Deceased cases by sex (cumulative)', fontsize=17)\nsex_deceased = (sex_raw[sex_raw.sex=='male'].deceased, sex_raw[sex_raw.sex=='female'].deceased)\n\nfor sex_each, sex_label in zip(sex_deceased, ['male', 'female']):\n    plt.plot(sex_raw.date.unique(), sex_each, label=sex_label)\nax.set_xticks(ax.get_xticks()[::int(len(sex_raw.date.unique())\/8)])\nplt.xlabel('Date')\nplt.ylabel('Number of cases')\nax.legend()\nplt.show()","3ff9b9e0":"fig, ax = plt.subplots(figsize=(11, 11))\nplt.title('Population Sex Balance (2020-02)', fontsize=17)\npop_circle=plt.Circle((0,0), 0.79, color='black')\nplt.pie([25984136, 25860491]\n        , labels=['male', 'female']\n        , autopct='%.2f%%'\n        , startangle=90\n        , counterclock=False)\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()","332f3cee":"pop_meta = pop_dense.copy()\npop_meta['sex_bal'] = [97.7, 101.4, 101.2, 95.1, 104.0, 96.3, 101.3, 99.4, 100.5\n                          , 105.7, 102.7, 101.2, 99.8, 98.0, 98.9, 100.9, 101.1]\npop_meta","427cb1a4":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Population sex Balance by Region (order by confirmed cases)', fontsize=17)\nplt.xticks(rotation=43)\nplt.bar(pop_meta.region, pop_meta.sex_bal, color=color_list[3])\nplt.xlabel('Region')\nplt.ylabel('Sex balance')\nplt.show()","087650d3":"test_raw = get_data(file_paths[4])\ndata_range(test_raw, 'date')","b2076b19":"plot_lines(test_raw, [2, 4, 5, 6], 7, 'Cumulative Cases')","131d4ea3":"print('# Columns for daily new cases added')\nfor col in test_raw.columns[2:7]:\n    if col in [2, 4]:\n        new_dict = {0: 1}\n    else:\n        new_dict = {0: 0}\n    new_dict.update({ i : test_raw[col][i] - test_raw[col][i-1] for i in range(1, len(test_raw)) })\n    test_raw[f'new_{col}'] = new_dict.values()\n\nplot_lines(test_raw, [7, 9, 10, 11], 12, 'Daily Cases')","0f8b26bf":"## 1. Add rate columns\ntotal_pop = sum(pop_meta.population)*1000000\ntest_raw['test_rate'] = test_raw.test\/total_pop * 100\ntest_raw['confirmed_rate'] = test_raw.confirmed\/test_raw.test * 100 # to total tests\ntest_raw['deceased_rate'] = test_raw.deceased\/(test_raw.released+test_raw.deceased) * 100 # to released or deceased cases\nprint('# Columns for rates added')\n\n## 2. Plot rates by day\nstart_day = np.nanargmin(test_raw.deceased_rate)\npeak_day = np.argmax(test_raw.new_confirmed)\nfor i, j in zip(np.arange(len(test_raw.columns)-3, len(test_raw.columns))\n                , ['Test Rate', 'Confirmed Rate', 'Deceased Rate']):\n    fig, ax = plt.subplots(figsize=(13, 7))\n    plt.plot(test_raw.date[start_day:]\n             , test_raw[test_raw.columns[i]][start_day:]\n             , color=color_list[(i-(len(test_raw.columns)-3))*2])\n    plt.title(j, size=17)\n    ax.axvline(test_raw.date[peak_day]\n               , ls=':', color='crimson')\n    ax.set_xlabel('Date', size=13)\n    ax.set_ylabel('Rate (%)', size=13)\n    ax.set_xticks(ax.get_xticks()[::int(len(test_raw.date[start_day:])\/8)])\n    ax.legend([test_raw.columns[i], 'peak of new_confirmed']\n              , loc='upper left')\n    plt.show()","5c96652f":"# Recent checker\ntest_raw.loc[len(test_raw)-1, ['date', 'test_rate', 'confirmed_rate', 'deceased_rate']]","643b87d0":"path_raw = get_data(file_paths[5])\nprint(f'Number of unique paths: {len(path_raw.infection_case.unique())}\\n')\nprint('[6 paths with the most confirmed cases]')\npath = path_raw.sort_values('confirmed', ascending=False)\ndisplay(path.head(6))","50c073be":"# Put paths other than those with the 6 most confirmed cases into others\npath.loc[len(path), :] = path[path.confirmed<path.confirmed.iloc[5]].sum()\npath.loc[len(path)-1, 'case_id'] = 0\npath.loc[len(path)-1, 'province'] = 'various regions'\npath.loc[len(path)-1, 'city'] = 'various cities'\npath.loc[len(path)-1, 'group'] = 'various groups'\npath.loc[len(path)-1, 'infection_case'] = 'others'\npath.loc[len(path)-1, 'latitude'] = '-'\npath.loc[len(path)-1, 'longitude'] = '-'\npath_with_others = path[path.confirmed >= path.confirmed.iloc[5]]\n\n# Plot distribution\nfig, ax = plt.subplots(figsize=(11, 11))\nplt.title(f'Path of Transmission ({last_update})', fontsize=17)\npop_circle=plt.Circle((0,0), 0.79, color='black')\nplt.pie(path_with_others.confirmed, autopct='%.2f%%'\n        , labels=path_with_others.infection_case + ' (' + path_with_others.province + ')'\n#         , explode=(0.03, 0, 0, 0, 0, 0, 0)\n        , startangle=90\n        , counterclock=False)\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()","d6391ef0":"path_group = path_raw.groupby('group').sum().sort_values('confirmed', ascending=False)\npath_group.index = ['group', 'individual']\nprint('[Type of transmission]')\npath_group","943b5f03":"fig, ax = plt.subplots(figsize=(11, 11))\nplt.title(f'Type of Transmission ({last_update})', fontsize=17)\npop_circle=plt.Circle((0,0), 0.79, color='black')\nplt.pie(path_group.confirmed\n        , autopct='%.2f%%'\n        , labels=path_group.index\n        , startangle=90\n        , counterclock=False)\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()","5898233c":"weather_raw = get_data(file_paths[6], transpose=True)\ndata_range(weather_raw, 'date')","9663fcc9":"# 1. Null check\nprint('Number of na values')\nprint(weather_raw.isna().sum())\nweather_raw = weather_raw.dropna()\nprint('\\n# na values dropped\\n')\nprint('Number of na values')\nprint(weather_raw.isna().sum())\nprint('\\n\\n')\n\n# 2. Unique regions\nprint('Number of regions in region data:', len(region_raw.province.unique()))\nprint(sorted(region_raw.province.unique()))\nprint()\nprint('Number of regions in weather data:', len(weather_raw.province.unique()))\nprint(sorted(weather_raw.province.unique()))\nweather_raw.province = weather_raw.province.replace('Chunghceongbuk-do', 'Chungcheongbuk-do')\nprint('\\n# Chunghceongbuk-do replaced with Chungcheongbuk-do\\n')\nprint('Number of regions in weather data:', len(weather_raw.province.unique()))","45b5248e":"print('[Basic statistics - total]')\ndisplay(weather_raw.loc[:,'avg_temp':].describe().T)\n\nprint('[Average weather by region - sample]')\nweather_stat = weather_raw.loc[:, 'province':].groupby('province').mean()\ndisplay(weather_stat.tail(3).T)","28f1ea4a":"# 1. Create a dataframe with selective average features\nweather_avg = pd.DataFrame(\n    [weather_stat.index\n     , weather_stat['avg_temp']\n     , weather_stat['precipitation']\n     , weather_stat['max_wind_speed']\n     , weather_stat['avg_relative_humidity']]\n    ).T\nweather_avg.columns = ['region', 'temperature', 'precipitation'\n                       , 'max_wind_speed', 'relative_humidity']\n\n# 2. Order by confirmed cases\nsorter = list(pop_meta.region[pop_meta.region != 'Sejong'].values)\nweather_avg.region = weather_avg.region.astype('category')\nweather_avg.region.cat.set_categories(sorter, inplace=True)\nweather_avg = weather_avg.sort_values(['region'])\nweather_avg.index = range(len(weather_raw.province.unique()))\n\n# 3. Plot each feature\ntitle_list = ['Average temperature', 'Average Maximun Wind Speed', 'Average relative humidity']\nfor col, title in zip(weather_avg.columns[[1, 3, 4]], title_list):\n    plt.figure(figsize=(13, 7))\n    plt.title(f'{title} (since 2016-01-01)', fontsize=17)\n    plt.xticks(rotation=41)\n    plt.bar(weather_avg.region, weather_avg[col], color=color_list[2])\n    plt.xlabel('Region')\n    plt.ylabel('Average value')\n    plt.show()","ca28e41a":"## 1. Create a dataframe\nweather_covid = weather_raw[weather_raw.date >= '2020-01-20']\nweather_cov_stat = weather_covid.loc[:, 'province':].groupby('province').mean()\nweather_cov_avg = pd.DataFrame(\n    [weather_cov_stat.index\n     , weather_cov_stat['avg_temp']\n     , weather_cov_stat['precipitation']\n     , weather_cov_stat['max_wind_speed']\n     , weather_cov_stat['avg_relative_humidity']]\n    ).T\n\n## 2. Order by confirmed cases\nweather_cov_avg.columns = ['region', 'temperature', 'precipitation'\n                           , 'max_wind_speed', 'relative_humidity']\nweather_cov_avg.region = weather_cov_avg.region.astype('category')\nweather_cov_avg.region.cat.set_categories(sorter, inplace=True)\nweather_cov_avg = weather_cov_avg.sort_values(['region'])\n\n## 3. Plot values\ntitle_list = ['Average temperature', 'Average Maximun Wind Speed', 'Average relative humidity']\nfor col, title in zip(weather_cov_avg.columns[[1, 3, 4]], title_list):    \n    plt.title(f'{title} (since 2020-01-20)', fontsize=17)\n    plt.xticks(rotation=41)\n    plt.bar(weather_avg.region, weather_cov_avg[col], color=color_list[2])\n    plt.xlabel('region')\n    plt.ylabel('Average value')\n    plt.show()","437881dd":"patient_raw = get_data(file_paths[7], transpose=True)\ndata_range(patient_raw, 'confirmed_date', describe='confirmed date')","8951a2f5":"confirmed_total = test_raw.confirmed.iloc[-1]\nprint(f'Number of confirmed cases in test_raw data ({last_update}):', confirmed_total)","47ab46f8":"# Ratio checker\nlen(patient_raw), confirmed_total, len(patient_raw)\/confirmed_total","8b657ee1":"print('Unique sex values:', patient_raw.sex.unique())\nprint('Number of nan values:', patient_raw.sex.isna().sum())","e00724ef":"sex_dis = patient_raw[['patient_id', 'sex']].groupby('sex', as_index=False).count()\nsex_dis.columns = ['sex', 'confirmed']\n\nfig, ax = plt.subplots(figsize=(11, 11))\nplt.title('Sex Distribution of Confirmed Cases (in patient data)', fontsize=17)\npop_circle=plt.Circle((0,0), 0.79, color='black')\nplt.pie(sex_dis.confirmed\n        , labels=[f'female ({sex_dis.confirmed[0]})', f'male ({sex_dis.confirmed[1]})']\n        , autopct='%.2f%%'\n        , startangle=90\n        , counterclock=False)\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()","3b2111a3":"# get proportion in patient data\nsex_dis['proportion'] = round(sex_dis.iloc[:,-1]\n                                 \/sum(sex_dis.confirmed) * 100, 2).values\n# get proportion in sex data\nsex_raw_dis = sex_raw.iloc[-2:, [2,3]]\nsex_raw_dis['proportion'] = round(sex_raw_dis.iloc[:,-1]\n                                     \/sum(sex_raw_dis.confirmed) * 100, 2).values\n# show difference\nprint('[Sex distribution in patient data]')\ndisplay(sex_dis.sort_values('confirmed', ascending=False))\nprint('[Sex distribution in sex data]')\ndisplay(sex_raw_dis.sort_values('confirmed', ascending=False))","694cbc03":"## 1. Cleanse data\n# 1) Drop non numeric values\npatient_raw.contact_number = patient_raw.contact_number.replace('-', None)\npatient_contact = patient_raw[ ~patient_raw.contact_number.isna() ]\n# 2) Convert numeric but str type values into int type \npatient_contact.contact_number = list(map(int, patient_contact.contact_number))\n# 3) Drop unreasonably large values\npatient_contact = patient_contact[ patient_contact.contact_number < 10000 ]\n\n## 2. Check statistics\nprint('[Basic statistics]')\ndisplay(patient_contact.contact_number.describe())\n\n## 3. Plot distribution\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Patient Distribution by Number of Contacts Before Confirmed', fontsize=17)\nsns.swarmplot(patient_contact.contact_number, color = color_list[5])\nax.set_xlabel('Number of contacts', size=13)\nax.set_ylabel('Patients', size=13)\nplt.show()","a15c2b1e":"print('[Outliers with more than or equal to 200 contacts]')\npatient_outliers = patient_contact[patient_contact.contact_number >= 200].sort_values('contact_number', ascending=False)\nprint('Number of outliers:', len(patient_outliers))\npatient_outliers[ ['contact_number', 'sex', 'age', 'country', 'province', 'infection_case', 'confirmed_date', 'state'] ]","89b21ff1":"patient_inliers = patient_contact[ ~patient_contact.patient_id.isin(patient_outliers.patient_id) ]\nprint('[Inliers with less than 200 contacts - sample]')\npatient_inliers = patient_inliers[~patient_contact.contact_number.isna()] # without null contacts\ndisplay(patient_inliers.sort_values('contact_number', ascending=False)\\\n[ ['contact_number', 'sex', 'age', 'country', 'province', 'infection_case', 'confirmed_date', 'state'] ].tail(3))\nprint('Number of inliers:', len(patient_inliers))","b1c70bc8":"print('[Average contacts by date - sample]')\npatient_time_contact = patient_inliers[['contact_number', 'confirmed_date']].groupby('confirmed_date').mean()\npatient_time_contact.head(3).append(patient_time_contact.tail(3))","17030bcd":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Average contacts of a patient (by confirmed date)', fontsize=17)\nplt.bar(patient_time_contact.index\n        , patient_time_contact.contact_number\n        , color = color_list[5])\nax.set_xlabel('Confirmed date', size=13)\nax.set_ylabel('Number of contacts', size=13)\nax.set_xticks(ax.get_xticks()[::int(len(patient_time_contact.index)\/8)])\nplt.show()","a532d9f9":"seoul_raw = get_data(file_paths[8])\ndata_range(seoul_raw, 'date')","098fa717":"xtick_num = 7\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Floating Population by Date', size=17)\nplt.bar(seoul_raw.date.unique()\n       , seoul_raw.groupby(['date', 'hour']).sum()\\\n         .groupby('date').mean().apply(lambda x: x\/1000000)\\\n         .fp_num.values\n       , color=color_list[7])\nax.set_xlabel('Date', size=13)\nax.set_ylabel('Floating population (millions)', size=13)\nplt.xticks(rotation=0, size=9)\nax.set_xticks(ax.get_xticks()[::int(len(seoul_raw.date.unique())\/xtick_num)])\nplt.show()","78af41a4":"## 1. Get hourly floating populations by date\nspike_idx = np.argmax(seoul_raw.groupby(['date', 'hour']).sum().groupby('date').mean().fp_num)\ndip_idx = np.argmin(seoul_raw.groupby(['date', 'hour']).sum().groupby('date').mean().fp_num)\nspike = seoul_raw[ seoul_raw.date == spike_idx ].groupby('hour')\\\n                                                .sum().apply(lambda x: x\/1000000)\\\n                                                .fp_num\nnormal = seoul_raw[ seoul_raw.date == '2020-02-16' ].groupby('hour')\\\n                                                    .sum().apply(lambda x: x\/1000000)\\\n                                                    .fp_num\ndip = seoul_raw[ seoul_raw.date == dip_idx ].groupby('hour')\\\n                                            .sum().apply(lambda x: x\/1000000)\\\n                                            .fp_num\n## 2. Compare three days\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Floating Population by Time (outliers VS. normal)', size=17)\nplt.plot(spike, color=color_list[7])\nplt.plot(normal, color=color_list[0])\nplt.plot(dip, color=color_list[2])\nax.set_xlabel('Hour of day', size=13)\nax.set_ylabel('Floating population (millions)', size=13)\nplt.legend([f'spike ({spike_idx})'\n            , 'normal (2020-02-16)'\n            , f'dip ({dip_idx})'\n           ], fontsize=11, loc='center right')\nplt.show()","7bfc8cac":"print('[Before havling \\'fp_num\\']')\ndisplay(seoul_raw[(seoul_raw.date==spike_idx) & (seoul_raw.hour!=11) ].tail(1))\nseoul_raw.loc[ (seoul_raw.date==spike_idx) & (seoul_raw.hour!=11), 'fp_num' ] \/= 2\nprint('[After havling \\'fp_num\\']')\ndisplay(seoul_raw[(seoul_raw.date==spike_idx) & (seoul_raw.hour!=11) ].tail(1))","8cb45ffe":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Floating Population by Time', size=17)\nplt.plot(seoul_raw[ seoul_raw.date == '2020-02-23' ].groupby('hour')\\\n                                                    .sum().apply(lambda x: x\/1000000)\\\n                                                    .fp_num\n         , color=color_list[7])\nplt.plot(normal, color=color_list[0])\nax.set_ylabel('Floating population (millions)', size=13)\nax.set_xlabel('Hour of day', size=13)\nplt.legend(['spike (2020-02-23)', 'normal (2020-02-16)'], fontsize=11, loc='lower right')\nplt.show()","3dbdcc0a":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Floating Population by Date', size=17)\nplt.bar(seoul_raw.date.unique()\n       , seoul_raw.groupby(['date', 'hour']).sum()\\\n         .groupby('date').mean().apply(lambda x: x\/1000000)\\\n         .fp_num.values\n       , color=color_list[7])\nax.set_xlabel('Date', size=13)\nax.set_ylabel('Floating population (millions)', size=13)\nplt.xticks(rotation=0, size=9)\nax.set_xticks(ax.get_xticks()[::int(len(seoul_raw.date.unique())\/xtick_num)])\nplt.show()","f4b7981f":"# get 2 separate tables by sex\nseoul_female, seoul_male = seoul_raw[seoul_raw.sex == 'female'], seoul_raw[seoul_raw.sex == 'male']\n# pie chart\nfig, ax = plt.subplots(figsize=(11, 11))\nplt.title('Floating Population by sex (total)', size=17)\npop_circle=plt.Circle((0,0), 0.79, color='black')\nplt.pie([seoul_female.fp_num.sum()\n         , seoul_male.fp_num.sum()]\n        , labels=['female', 'male']\n        , autopct='%.2f%%'\n        , startangle=90\n        , counterclock=False)\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()","677033a7":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Floating Population by Sex (overlapped)', size=17)\nfor each, i in zip([seoul_female, seoul_male], range(2)):\n    plt.bar(each.date.unique()\n            , each.groupby(['date', 'hour']).sum()\\\n                  .groupby('date').mean()\n                  .apply(lambda x: x\/1000000)\\\n                  .fp_num.values\n            , color=color_list[i])\nax.set_xlabel('Date', size=13)\nax.set_ylabel('Floating population (millions)', size=13)\nplt.xticks(rotation=0, size=9)\nplt.legend(['female', 'male'], loc='lower right')\nax.set_xticks(ax.get_xticks()[::int(len(each.date.unique())\/xtick_num)])\nplt.show()","b47ffc48":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Floating Population by Sex (average per day)', size=17)\nplt.plot(seoul_female.groupby(['date', 'hour']).sum()\\\n         .groupby('hour').mean().apply(lambda x: x\/1000000)\\\n         .fp_num\n         , color=color_list[0])\nplt.plot(seoul_male.groupby(['date', 'hour']).sum()\\\n         .groupby('hour').mean().apply(lambda x: x\/1000000)\\\n         .fp_num\n         , color=color_list[1])\nax.set_ylabel('Floating population (millions)', size=13)\nax.set_xlabel('Hour of day', size=13)\nplt.legend(['female', 'male'], loc='lower right')\nplt.show()","9c94a2aa":"float_age_order = list(seoul_raw.groupby('birth_year').sum().sort_values('fp_num', ascending=False).index)\nfig, ax = plt.subplots(figsize=(11, 11))\nplt.title('Floating Population by Age', size=17)\npop_circle=plt.Circle((0,0), 0.79, color='black')\nplt.pie(seoul_raw.groupby('birth_year').sum().sort_values('fp_num', ascending=False).fp_num\n        , labels=float_age_order\n        , autopct='%.2f%%'\n        , startangle=90\n        , counterclock=False)\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()","1ba90f5a":"seoul_pop_dis = {'20s': 682490 + 841476 + 1539441, '30s': 1572046, '40s': 1608068\n                 , '50s': 1608155, '60s': 1192140, '70s': 681810 + 255068 + 39036 + 6197}\nfig, ax = plt.subplots(figsize=(11, 11))\nplt.title('Resident Population by Age', size=17)\npop_circle=plt.Circle((0,0), 0.79, color='black')\nplt.pie(seoul_pop_dis.values()\n        , labels=seoul_pop_dis.keys()\n        , autopct='%.2f%%'\n        , startangle=90\n        , counterclock=False)\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()","f4d093d9":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Floating Population by Age (overlapped)', size=17)\nfor age, i in zip(float_age_order, range(len(float_age_order))):\n    plt.bar(seoul_raw.date.unique()\n            , seoul_raw[seoul_raw.birth_year==age].groupby(['date', 'hour']).sum()\\\n            .groupby('date').mean().apply(lambda x: x\/1000000)\\\n            .fp_num.values\n            , color=color_list[i])\nax.set_xlabel('Date', size=13)\nax.set_ylabel('Floating population (millions)', size=13)\nplt.xticks(rotation=0, size=9)\nplt.legend(float_age_order\n           , loc='lower right'\n          )\nax.set_xticks(ax.get_xticks()[::int(len(seoul_raw.date.unique())\/xtick_num)])\nplt.show()","2ba96666":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Floating Population by Age', size=17)\nfor age, i in zip(float_age_order, np.arange(len(float_age_order))):\n    plt.plot(seoul_raw[seoul_raw.birth_year==age].groupby(['date', 'hour']).sum()\\\n             .groupby('hour').mean().apply(lambda x: x\/1000000)\\\n             .fp_num\n             , color=color_list[i])\nax.set_ylabel('Floating population (millions)', size=13)\nax.set_xlabel('Hour of day', size=13)\nplt.legend(float_age_order, loc='lower right')\nplt.show()","dd8d01dc":"float_area_order = list(seoul_raw.groupby('city').sum().sort_values('fp_num', ascending=False).index)\n\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Floating Population by Subregion (total)', size=17)\nsns.barplot(float_area_order\n           , seoul_raw.groupby('city').sum().sort_values('fp_num', ascending=False).fp_num\/(seoul_raw.fp_num.sum())*100)\nax.set_xlabel('Subregion', size=13)\nax.set_ylabel('Floating population proportion (%)', size=13)\nplt.xticks(rotation=37, size=9)\nplt.show()","9aeddf72":"## 1. Get confrimed cases by subregion and order of floating population\nseoul_confirmed = [76, 49, 50, 44, 31    # Gangnam, Songpa, Gangseo, Seocho, Nowon\n                   , 60, 39, 43, 27, 29  # Gwanak, Yeongdeungpo, Guro, Mapo, Seongbuk\n                   , 35, 26, 35, 40, 19  # Eunpyeong, Gangdong, Yangcheon, Dongjag, Jungnang\n                   , 34, 14, 41, 30, 19    # Dongdaemun, Gwangjin, Seongdong, Seodaemun, Dobong\n                   , 16, 16, 9, 19, 38]   # Gangbuk, Geumcheon, Jung, Jongno, Yongsan\nfloat_area_order = list(seoul_raw.groupby('city').sum().sort_values('fp_num', ascending=False).index)\n\n## 2. Plot distribution\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Confirmed Cases by Subregion (order by floating population)', size=17)\nsns.barplot(float_area_order\n           , seoul_confirmed)\nax.set_xlabel('Subregion', size=13)\nax.set_ylabel('Number of confirmed cases (2020-06-03)', size=13)\nplt.xticks(rotation=37, size=9)\nplt.show()","0b870c06":"search_raw = get_data(file_paths[9])\ndata_range(search_raw, 'date')","8ac266c6":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Search Trends Related to Respiratory Diseases (whole period)', size=17)\nax.set_xlabel('Date', size=13)\nax.set_ylabel('Relative interests in time range (%)', size=13)\nfor column in search_raw.columns[1:]:\n    plt.plot(search_raw.date, search_raw[column])\nax.set_xticks(ax.get_xticks()[::int(len(search_raw.date.unique())\/8)])\nax.legend()\nplt.show()","cefc1d65":"fig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Search Trends Related to Respiratory Diseases (since 1st case worldwide)', size=17)\nax.set_xlabel('Date', size=13)\nax.set_ylabel('Relative interests in time range (%)', size=13)\nfor column in search_raw.columns[1:]:\n    plt.plot(search_raw.date[search_raw.date >= '2019-11-17']\n             , search_raw[search_raw.date >= '2019-11-17'][column])\nax.set_xticks(ax.get_xticks()[::int(len(search_raw.date[search_raw.date >= '2019-11-17'])\/8)])\nax.legend(loc='upper left')\nplt.show()","1dda69bf":"## 1. Search trends after 1st case in S.Korea\nfig, ax1 = plt.subplots(figsize=(13, 7))\nplt.title('Search Trends Related to Respiratory Diseases (since 1st case in S.Korea)', size=17)\nax1.set_xlabel('Date', size=13)\nax1.set_ylabel('Relative interests in time range (%)', size=13)\nfor column in search_raw.columns[1:]:\n    ax1.plot(search_raw[search_raw.date >= '2020-01-20'].date\n             , search_raw[search_raw.date >= '2020-01-20'][column])\nax1.set_xticks(ax1.get_xticks()[::int(len(search_raw[search_raw.date >= '2020-01-20'].date)\/8)])\nax1.legend(loc='upper left')\n## 2. Daily new cases\nax2 = ax1.twinx()\nax2.grid(False)\nax2.set_ylabel(\"Number of cases\", size=13)\nax2.plot(test_raw.date[:-1]\n         , test_raw.new_confirmed[:-1]\n         , label='new confirmed', color='dimgray', ls=':', lw=3)\nax2.set_xticks(ax2.get_xticks()[::int(len(test_raw.date[:-1])\/8)])\nax2.legend(loc='upper right')\n\nplt.show()","cf3b6a2b":"policy_raw = get_data(file_paths[10])\ndata_range(policy_raw, 'start_date', describe='start date')","dd09b9c1":"print(f'[Null check]\\n{policy_raw.isna().sum()}')","b6728a72":"print('[Redundancy check]')\nprint('Are values in \\'policy_id\\' column just index + 1?')\nprint(f'# {(policy_raw.index + 1 == policy_raw.policy_id).all()}')\nprint('Unique countries in \\'country\\' column:')\nprint(f'# {policy_raw.country.unique()[0]}')","09542d71":"## Check redundancy conditions before ruling out\n    # index + 1 == policy_id ?\n    # country.unique() == Korea ?\nif ( sum(policy_raw.index+1==policy_raw.policy_id) == len(policy_raw) \n   ) and ( len(policy_raw.country.unique()) == 1 ):\n    policy_compact = policy_raw.iloc[:, 2:]\nprint('[Sample data - compact version]')\ndisplay(policy_compact.tail(3))\nprint('[Basic numbers]')\ndisplay(policy_compact.describe())","c228bffa":"## 1. Policy type distribution\nfig, ax = plt.subplots(figsize=(11, 11))\nplt.title(f'Policy Type Distribution ({last_update})', fontsize=17)\npop_circle = plt.Circle((0,0), 0.79, color='black')\nplt.pie(policy_compact.type.value_counts()\n        , labels = policy_compact.type.value_counts().index\n        , autopct = '%.2f%%'\n        , startangle = 90\n        , counterclock = False)\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()\n\n## 2. Policy title distribution (title: specific policies in a type)\n# 1) Put others than top 7 into others\npolicy_titles = pd.DataFrame(policy_compact.gov_policy.value_counts().items(), columns=['title', 'counts'])\npolicy_titles.loc[len(policy_titles), 'title'] = 'Others'\npolicy_titles.loc[len(policy_titles)-1, 'counts'] = policy_titles.iloc[6:, 1].sum()\npolicy_titles = pd.concat([policy_titles[:7], policy_titles[-1:]])\n\n# 2) Plot distribution\nfig, ax = plt.subplots(figsize=(11, 11))\nplt.title(f'Policy Title Distribution ({last_update})', fontsize=17)\npop_circle = plt.Circle((0,0), 0.79, color='black')\nplt.pie(policy_titles['counts']\n        , labels = policy_titles['title']\n        , autopct = '%.2f%%'\n        , startangle = 90\n        , counterclock = False)\np=plt.gcf()\np.gca().add_artist(pop_circle)\nplt.show()","1497f756":"## 1. Policy starting day distribution\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title(f'Policy Starting Day Distribution ({last_update})', size=17)\nsns.swarmplot(policy_compact.start_date)\nplt.xlabel('The day of policy implementation', size=13)\nplt.ylabel('Policies', size=13)\nax.set_xticks(ax.get_xticks()[::int(len(policy_compact.start_date.unique())\/9)])\nplt.show()\n\n## 2. Policy starting month distribution\npolicy_by_day = pd.DataFrame(policy_raw.groupby('start_date').count().gov_policy)\npolicy_by_day['month'] = [ int(policy_by_day.index[i][6:7]) for i in range(len(policy_by_day)) ]\npolicy_by_month = policy_by_day.groupby('month').sum()\nplt.title(f'Policy Starting Month Distribution ({last_update})', size=17)\nplt.bar(policy_by_month.index\n        , policy_by_month.gov_policy\n        , width=0.31)\nplt.xlabel('Month', size=13)\nplt.ylabel('Number of policies started', size=13)\nplt.yticks(np.arange(0, max(policy_by_month.gov_policy)+1, 3))\nplt.show()","355aa836":"# Proportion check\nprint(policy_by_month.gov_policy[3]\n      , sum(policy_by_month.gov_policy)\n      , policy_by_month.gov_policy[3]\/sum(policy_by_month.gov_policy)*100)","f91e98e8":"policy_alerts = policy_compact[ policy_compact.type == 'Alert' ]\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Infectious Disease Alert Level', size=17)\nplt.plot(test_raw.date.unique(), test_raw.new_confirmed\n         , color='dimgray'\n         , lw=3)\nax.set_xticks(ax.get_xticks()[::int(len(test_raw.date.unique())\/8)])\nfor day, color in zip(policy_alerts.start_date.values[1:], ['yellow', 'orange', 'red']):\n    ax.axvline(day, ls=':', color=color, lw=3)\nax.legend(['new confirmed', 'alert level 2', 'alert level 3', 'alert level 4'], fontsize=11)\nplt.ylabel('Number of cases')\nplt.show()","ec432d68":"## 1. Get the first implemented health policies by type and title\nfirst_policy = { i[1].start_date.values[0]: i[0].lower() \n                for i in policy_raw[policy_raw.type=='Health'].groupby(['gov_policy']) }\nfirst_policy = dict(sorted(first_policy.items()))\n\n## 2. Plot starting dates\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Policy on Public Health', size=17)\nplt.plot(test_raw.date.unique(), test_raw.new_confirmed\n         , color='dimgray', lw=3)\nax.set_xticks(ax.get_xticks()[::int(len(test_raw.date.unique())\/8)])\nfor date, color in zip(first_policy.keys(), color_list[:4]):\n    ax.axvline(date, ls=':', color=color, lw=3)\nax.legend(['new confirmed'] + list(first_policy.values())\n          , fontsize=11, loc='upper right')\nplt.ylabel('Number of cases')\nplt.show()","e1b3062c":"## 1. Get the oldest and recent policies on immigration\npolicy_immi = policy_raw[ policy_raw.type == 'Immigration' ].head(1).append(\n              policy_raw[ policy_raw.type == 'Immigration' ].tail(3))\npolicy_list = list( map(add # 1) add policy titles + country names\n                        , map( lambda x: x.lower() # 2) lower case policy titles\n                              , policy_immi.gov_policy.values)\n                        , map( lambda x: ' (' + x[5:].replace(' the', '') +')' # 3) simplify country names\n                              , policy_immi.detail.values)\n                       ) \n                  )\n\n## 2. Plot starting dates\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Policy on Immigration', size=17)\nplt.plot(test_raw.date.unique(), test_raw.new_confirmed\n         , color='dimgray'\n         , lw=3)\nax.set_xticks(ax.get_xticks()[::int(len(test_raw.date.unique())\/8)])\nfor day, color in zip(policy_immi.start_date.unique(), color_list[:4]):\n    ax.axvline(day, ls=':', color=color, lw=3)\nax.legend(['new confirmed'] + policy_list, fontsize=11\n           , loc='upper left')\nplt.ylabel('Number of cases')\nplt.show()","bb67a210":"## Various education policies started on the sames days\n    # thus we better handle this empirically than systematically\n    # cause it's not generalizable to other use cases\npolicy_edu = policy_compact[ policy_compact.type == 'Education' ]\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Policy on Education', size=17)\nplt.plot(test_raw.date.unique(), test_raw.new_confirmed\n         , color='dimgray'\n         , lw=3)\nax.set_xticks(ax.get_xticks()[::int(len(test_raw.date.unique())\/8)])\nfor day, color in zip(policy_edu.start_date.unique(), color_list[:4]):\n    ax.axvline(day, ls=':', color=color, lw=3)\nax.axvline('2020-05-20', ls=':', color=color_list[4], lw=3)\nax.legend(['new confirmed', 'school closure & opening delay'\n           , 'online class open (high&middle 3)'\n           , 'online class open (high&middle 1,2 \/ elementary 4~6)'\n           , 'online class open (elementary 1~3)'\n           , 'physical class open (high 3)'\n          ], fontsize=11\n           , loc='upper left')\nplt.ylabel('Number of cases')\nplt.show()","c38612a2":"print('[Datasets we\\'ve been through]')\n[ i[:-4] for i in file_names ]","cbc0332c":"## 1. Get data on global confirmed cases and fatalities (which are more recent than test_raw)\ntest_global = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-5\/train.csv')\n## 2. Pick relevant rows and columns\ndaily_korea_raw = [0, 0, 0]\ndaily_korea_raw = test_global[ (test_global.Country_Region=='Korea, South') \n                              & (test_global.Target=='ConfirmedCases') \n                              & (test_global.Date>='2020-01-20') \n                             ].loc[ :, ['Date', 'TargetValue'] ]","cf055814":"## 1. Get data on search trends\nsearch_korea_raw = pd.read_csv('\/kaggle\/input\/search\/search_southkorea.csv')\n## 2. Cleanse nulls and insignificant values\nsearch_korea_raw = search_korea_raw.dropna()\nsearch_korea_raw = search_korea_raw.replace('<1', 0) # <1 means less than 1 significance in raw data\n## 3. Convert object to float\nsearch_korea_raw.iloc[:, 1:-2] = search_korea_raw.iloc[:, 1:-2].astype(float)\n## 4. Make a new column by merging 5 sub-columns for COVID-19 search on google\nsearch_korea_raw['google(all)'] = search_korea_raw.iloc[:, 1:-1].sum(axis=1)\nsearch_korea_raw['google(all)'] = search_korea_raw['google(all)']\/max(search_korea_raw['google(all)']) * 100 # normalizing\n\n## 5. Match lengths between search & case datasets\nif len(search_korea_raw) < len(daily_korea_raw):\n    daily_korea = daily_korea_raw[:len(search_korea_raw)]\n    search_korea = search_korea_raw[3:]\nelse:\n    daily_korea = daily_korea_raw\n    search_korea = search_korea_raw[3:3+len(daily_korea)]\ndaily_korea.index = range(len(daily_korea))\nsearch_korea.index = range(len(search_korea))\nprint('[Sample data]')\ndisplay(search_korea.head(3).append(search_korea.tail(3)))\n# 6. Get X, y\nshorter_len = min(len(search_korea), len(daily_korea))\nX_google = search_korea['google(all)'][:shorter_len]\nX_naver = search_korea['naver(all)'][:shorter_len]\ny = daily_korea.TargetValue.values[:shorter_len]\ndate_range = daily_korea.Date[:shorter_len]\n# X_google, X_naver, y = np.log(X_google+1), np.log(X_naver+1), np.log(y+1) # log version","82e03f9b":"fig, ax1 = plt.subplots(figsize=(13, 7))\nplt.title('Search Trends on COVID-19 (google \/ naver)', size=17)\n\nax1.plot(date_range, X_google)\nax1.plot(date_range, X_naver)\nax1.set_xticks(ax1.get_xticks()[::int(len(date_range)\/11)])\nax1.set_ylabel('Relative interests in time range (%)', size=13)\nax1.legend(loc='upper left')\n\nax2 = ax1.twinx()\nax2.grid(False)\nax2.set_ylabel(\"Number of cases\", size=13)\nax2.plot(date_range, y\n         , label='new confirmed'\n         , color='dimgray'\n         , ls=':'\n         , lw=3)\nax2.set_xticks(ax2.get_xticks()[::int(len(date_range)\/11)])\nax2.legend(loc='upper right')\n\nplt.show()","c5f6686f":"## 1. Linear relationship\n# 1) Get time-gaps between search trends and confirmed cases empirically \ngap_google = np.argmax(y) - np.argmax(X_google[:50])\ngap_naver = np.argmax(y) - np.argmax(X_naver)\n# 2) Plot regressions on scatter points\nplt.figure(figsize=(7, 7))\nplt.title('Linear Relationships - Search Trends & New Confirmed Cases', size=17)\nplt.grid(False)\nsns.regplot(X_google[gap_google:len(X_google)]\n           , y[:len(X_google)-gap_google])\nsns.regplot(X_naver[gap_naver:len(X_naver)]\n           , y[:len(X_naver)-gap_naver])\nplt.xlabel('Search trends', size=13)\nplt.ylabel('New confirmed', size=13)\nplt.legend(['Linear(Google)', 'Linear(Naver)'\n           ], fontsize=11)\nplt.show()\n\n## 2. Residuals - Google\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 7))\naxes[0].set_title('Residuals after Excluding Linearity (google)', size=17)\nsns.residplot(X_google[gap_google:len(X_google)]\n              , y[:len(X_google)-gap_google]\n              , ax=axes[0])\naxes[0].grid(False)\naxes[0].set_xlabel('Search trends', size=13)\naxes[0].set_ylabel('New confirmed (residuals)', size=13)\n# plt.show()\n\n## 3. Residuals - Naver\nplt.title('Residuals after Excluding Linearity (naver)', size=17)\nsns.residplot(X_naver[gap_naver:len(X_naver)]\n              , y[:len(X_naver)-gap_naver]\n              , color=color_list[1]\n              , ax=axes[1])\naxes[1].grid(False)\naxes[1].set_xlabel('Search trends', size=13)\naxes[1].set_ylabel('New confirmed (residuals)', size=13)\nplt.show()","83502512":"size_range = np.arange(55, len(y))\n_, _, pred_g, best_score_g, best_size_g = test_with_range_ts(X_google, y, LinearRegression(), size_range, 'Google');","dd81ae2b":"size_range = np.arange(50, len(y))\n_, _, pred_n, best_score_n, best_size_n = test_with_range_ts(X_naver, y, LinearRegression(), size_range, 'Naver trends');","6d4fd42a":"test_start_g, test_start_n = len(search_korea.Day)-best_size_g, len(search_korea.Day)-best_size_n\n\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.plot(search_korea.Day, y, color='dimgray'\n         , ls=':'\n         , lw=3)\nplt.plot(search_korea.Day, pred_g)\nax.axvline(search_korea.Day[test_start_g], ls=':', color=color_list[0])\nplt.plot(search_korea.Day, pred_n)\nax.axvline(search_korea.Day[test_start_n], ls=':', color=color_list[1])\nax.set_xticks(ax.get_xticks()[::int(len(search_korea.Day)\/11)])\nplt.title('Confirmed Cases Prediction by Search Trends (Google \/ Naver)', size=17)\nax.legend(['Truth (new confirmed)'\n           , 'Prediction (Google)'\n           , 'Test starts (Google)'\n           , 'Prediction (Naver)'\n           , 'Test starts (Naver)'\n           ], fontsize=11)\nplt.xlabel('Confirmed date')\nplt.ylabel('Number of cases')\nplt.show()","9676f530":"print(f'[R2 score in testing]\\nGoogle: %.4f\\nNaver: %.4f'\n      %(r2_score(y[test_start_g:], pred_g[test_start_g:])\n        , r2_score(y[test_start_n:], pred_n[test_start_n:])))","ef4988de":"print('[Sample data] Google + Naver')\nX_all = search_korea.iloc[:, -2:]\nX_all.head(3).append(X_all.tail(3))","83e1eb1f":"## 1. Split X into train \/ test sets\nbest_size = int((best_size_g + best_size_n)\/2)\ntest_start = len(search_korea.Day)-best_size\nX_train, y_train, X_test, y_test = X_all.iloc[:test_start, :], y[:test_start], X_all.iloc[test_start:, :], y[test_start:]\n\n## 2. Fit \/ Predict\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ntrain_pred = lr.predict(X_train)\ntest_pred = lr.predict(X_test)\npred_all = np.concatenate([train_pred, test_pred])\n\n## 3. Plot predictions \/ truths\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Confirmed Cases Prediction by Search Trends (Naver only \/ Google + Naver)', size=17)\nplt.plot(search_korea.Day, y, color='dimgray'\n         , ls=':'\n         , lw=3)\nplt.plot(search_korea.Day, pred_n)\nplt.plot(search_korea.Day, pred_all)\nax.axvline(search_korea.Day[test_start], ls=':', color='crimson')\nax.set_xticks(ax.get_xticks()[::int(len(search_korea.Day)\/11)])\nax.legend(['Truth'\n           , 'Prediction (Naver)'\n           , 'Prediction (Naver+Google)'\n           , 'Test starts'\n          ], fontsize=11)\nplt.xlabel('Confirmed date')\nplt.ylabel('Number of cases')\nplt.show()\n\n## 4. Check scores\nprint(f'[R2 score in testing]\\nNaver: %.4f\\nGoogle + Naver: %.4f'\n      %(r2_score(y[test_start:], pred_n[test_start:])\n        , r2_score(y[test_start:], pred_all[test_start:])))","78e9cb14":"## 1. 0-flooring negative predictions\nprint('# All negative predictions converted to 0')\npred_all[pred_all < 0] = 0\nprint('Minium value in prediction:', min(pred_all))\n\n## 2. Plot predictions\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Confirmed Cases Prediction by Search Trends (Naver only \/ Google + Naver(0-floored))', size=17)\nplt.plot(search_korea.Day, y, color='dimgray'\n         , ls=':'\n         , lw=3)\nplt.plot(search_korea.Day, pred_n)\nplt.plot(search_korea.Day, pred_all)\nax.axvline(search_korea.Day[test_start], ls=':', color='crimson')\nax.set_xticks(ax.get_xticks()[::int(len(search_korea.Day)\/11)])\nax.legend(['Truth'\n           , 'Prediction (Naver)'\n           , 'Prediction (all, 0-floored)'\n           , 'Test starts'\n          ], fontsize=11)\nplt.xlabel('Confirmed date')\nplt.ylabel('Number of cases')\nplt.show()\n\n## 3. Check scores\nprint(f'[R2 score in testing]\\nNaver: %.4f\\nGoogle + Naver(0-floored): %.4f'\n      %(r2_score(y[test_start:], pred_n[test_start:])\n        , r2_score(y[test_start:], pred_all[test_start:])))","54ed34eb":"## 0. Reset starting point\nbest_size = best_size_n\ntest_start = len(search_korea.Day)-best_size\n\n## 1. Naive\npred_naive = [0] # dynamic all the time\n# pred_naive = list(y[:best_size_n]) + list(np.ones(len(y) - best_size_n) * y[best_size_n-1]) # static with the last observation before testing\n# pred_naive = list(y[:best_size_n]) + [ y[i-1] for i in np.arange(best_size_n, len(y)) ] # dynamic in testing\n[ pred_naive.append(y[i-1]) for i in np.arange(1, len(y)) ]\n\n## 2. Moving average of (up to) 7 latest observations\navg_window = 7\npred_avg_former = [ y[0:i].mean() for i in np.arange(1, avg_window) ]\npred_avg_latter = [ y[i-avg_window:i].mean() for i in np.arange(avg_window, len(y)+1) ]\npred_avg = pred_avg_former + pred_avg_latter\n\n## 3. Plot predictions\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('New Confirmed Cases Prediction (naive \/ average)', size=17)\nplt.plot(search_korea.Day, y, color='dimgray'\n         , ls=':'\n         , lw=3)\nplt.plot(search_korea.Day, pred_naive)\nplt.plot(search_korea.Day, pred_avg)\nax.axvline(search_korea.Day[test_start], ls=':', color='crimson')\nax.set_xticks(ax.get_xticks()[::int(len(search_korea.Day)\/11)])\nax.legend(['Truth'\n           , 'Prediction (naive)'\n           , 'Prediction (average)'\n           , 'Test starts'\n          ], fontsize=11)\nplt.xlabel('Confirmed date')\nplt.ylabel('Number of cases')\nplt.show()\n\n## 4. Check scores\nprint(f'[R2 score in testing]\\nnaive: %.4f\\naverage: %.4f'\n      %(\n          r2_score(y[-best_size_n:], pred_naive[-best_size_n:])\n        , r2_score(y[-best_size_n:], pred_avg[-best_size_n:])\n       ))","56ca4b1c":"## 1. Ensemble models\npred_fusion = (np.array(pred_naive) + np.array(pred_n))\/2\npred_fusion_avg = (np.array(pred_avg) + np.array(pred_n))\/2\npred_fusion_all = (np.array(pred_avg) + np.array(pred_naive) + np.array(pred_n))\/3\n\n## 2. Compare all predictions\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('New Confirmed Cases Prediction (naive \/ average \/ Naver \/ ensembles )', size=17)\nplt.plot(search_korea.Day, y, color='dimgray'\n         , ls=':'\n         , lw=3)\nplt.plot(search_korea.Day, pred_naive)\nplt.plot(search_korea.Day, pred_avg)\nplt.plot(search_korea.Day, pred_n)\nplt.plot(search_korea.Day, pred_fusion)\nplt.plot(search_korea.Day, pred_fusion_avg)\nplt.plot(search_korea.Day, pred_fusion_all)\nax.axvline(search_korea.Day[test_start], ls=':', color='crimson')\nax.set_xticks(ax.get_xticks()[::int(len(search_korea.Day)\/11)])\nax.legend(['Truth'\n           , 'Prediction (naive)'\n           , 'Prediction (average)'\n           , 'Prediction (LR(Naver))'\n           , 'Prediction (LR(Naver)+naive)'\n           , 'Prediction (LR(Naver)+avg)'\n           , 'Prediction (LR(Naver)+naive+avg)'\n           , 'Test starts'\n          ], fontsize=11)\nplt.xlabel('Confirmed date')\nplt.ylabel('Number of cases')\nplt.show()\n\n## 3. Check scores\nprint(f'[R2 score in testing]\\\n        \\nNaive: {round( r2_score(y[-best_size_n:], pred_naive[-best_size_n:]), 4 )}\\\n        \\nAverage: {round( r2_score(y[-best_size_n:], pred_avg[-best_size_n:]), 4 )}\\\n        \\nLR(Naver): {round( r2_score(y[-best_size_n:], pred_n[-best_size_n:]), 4 )}\\\n        \\nLR(Naver) + Naive: {round( r2_score(y[-best_size_n:], pred_fusion[-best_size_n:]), 4 )}\\\n        \\nLR(Naver) + Average: {round( r2_score(y[-best_size_n:], pred_fusion_avg[-best_size_n:]), 4 )}\\\n        \\nLR(Naver) + Naive + Average: {round( r2_score(y[-best_size_n:], pred_fusion_all[-best_size_n:]), 4 )}')\n\n## 4. Save the best prediction as a benchmark\nbench_korea = pred_fusion_all","0992f880":"## 1. Get data\nsearch_germany = pd.read_csv('\/kaggle\/input\/search\/search_germany.csv')\n## 2. Cleanse nulls and insignificant values\nsearch_germany = search_germany.dropna()\nsearch_germany = search_germany.replace('<1', 0) # <1 means less than 1 significance in raw data\n## 3. Convert object to float\nsearch_germany.iloc[:, 1:-2] = search_germany.iloc[:, 1:-2].astype(float)\n## 4. Get a new column by merging 5 sub-columns for COVID-19 search on Google\nsearch_germany['google(all)'] = search_germany.iloc[:, 1:-1].sum(axis=1)\nsearch_germany['google(all)'] = search_germany['google(all)']\/max(search_germany['google(all)']) * 100 # normalizing\n\nsearch_all = search_germany[:len(test_raw)] # match lengths with test_raw\n\nprint('[Search Trends in Germany - sample]')\nsearch_germany.head(3).append(search_germany.tail(3))","d53d6381":"## 1. Pick relevant rows and columns from test_global\ndaily_germany_raw = test_global[ (test_global.Country_Region=='Germany') \n                                & (test_global.Target=='ConfirmedCases') \n                                & (test_global.Date>='2020-01-27') \n                               ].loc[ :, ['Date', 'TargetValue'] ]\nprint('[Daily confirmed cases in Germany - sample]')\nif len(search_germany) < len(daily_germany_raw):\n    daily_germany = daily_germany_raw[:len(search_germany)]\nelse:\n    daily_germany = daily_germany_raw\n    search_germany = search_germany[:len(daily_germany)]\n\n## 2. Check result\ndisplay(daily_germany.head(3).append(daily_germany.tail(3)))\ndata_range(daily_germany, 'Date')\n\n## 3. Get X, y\nX_germany, y_germany = search_germany['google(all)'], daily_germany.TargetValue.values\n# X_germany, y_germany = np.log(X_germany+1), np.log(y_germany+1) # log version","ebb88839":"fig, ax1 = plt.subplots(figsize=(13, 7))\nplt.title('Search Trends (Germany)', size=17)\nax1.plot(daily_germany.Date\n         , X_germany)\n#          , np.log(search_germany.iloc[:,-1])+1) # log version\nax1.set_xticks(ax1.get_xticks()[::int(len(daily_germany.Date)\/8)])\nax1.legend(['Google search trends']\n           , loc='upper left')\nax1.set_xlabel('Confirmed date')\nax1.set_ylabel('Relative interests in time range (%)', size=13)\n\nax2 = ax1.twinx()\nax2.grid(False)\nax2.plot(daily_germany.Date\n         , y_germany\n#          , np.log(daily_germany.TargetValue+1) # log version\n         , color='dimgray'\n         , ls=':'\n         , lw=3)         \nax2.set_xticks(ax2.get_xticks()[::int(len(daily_germany.Date)\/8)])\nax2.legend(['Daily confirmed cases']\n           , loc='upper right')\nax2.set_ylabel('Number of cases', size=13)\nplt.show()","4bb2aa6c":"## 1. Find the best size\nsize_range = np.arange(10, len(y_germany))\n_, _, pred_g, best_score_g, best_size_g = test_with_range_ts(X_germany, y_germany, LinearRegression(), size_range, 'Google');\n\n## 2. Split \/ fit \/ predict\n# 1) Split\nX_train, y_train, X_test, y_test = np.array(X_germany[:-best_size_g]), y_germany[:-best_size_g], np.array(X_germany[-best_size_g:]), y_germany[-best_size_g:]\nX_train, X_test = X_train.reshape(-1, 1), X_test.reshape(-1, 1)\n# 2) Fit \/ Predict\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ntrain_pred = lr.predict(X_train)\ntest_pred = lr.predict(X_test)\npred_all = np.concatenate([train_pred, test_pred])\n# 3) Plot predictions\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Daily Confirmed Cases Prediction (Germany)', size=17)\nplt.plot(daily_germany.Date, y_germany, color='dimgray'\n         , ls=':'\n         , lw=3)\nplt.plot(daily_germany.Date, pred_all)\nax.axvline(daily_germany.Date.iloc[-best_size_g], ls=':', color='crimson')\nax.set_xticks(ax.get_xticks()[::int(len(daily_germany.Date)\/8)])\nax.legend(['Truth'\n           , 'Prediction (LR(Google))'\n           , 'Test starts'\n          ], fontsize=11)\nplt.xlabel('Confirmed date')\nplt.ylabel('Number of cases')\nplt.show()\n\n## 3. Check score\nprint(f'[R2 score in testing]\\nLR(Google): %.4f'\n      %(\n          r2_score(y_germany[-best_size_g:], pred_all[-best_size_g:])\n       ))","ca15ba6f":"## 1. Dynamic naive\npred_naive = [0]\n[ pred_naive.append(y_germany[i-1]) for i in np.arange(1, len(y_germany)) ]\n\n## 2. Moving average of (up to) 7 latest observations\npred_avg_former = [ y_germany[0:i].mean() for i in np.arange(1, avg_window) ]\npred_avg_latter = [ y_germany[i-avg_window:i].mean() for i in np.arange(avg_window, len(y_germany)+1) ]\npred_avg = pred_avg_former + pred_avg_latter\n\n## 3. Plot predictions\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Daily Confirmed Cases Prediction (Germany)', size=17)\nplt.plot(daily_germany.Date, y_germany, color='dimgray'\n         , ls=':'\n         , lw=3)\nplt.plot(daily_germany.Date, pred_naive)\nplt.plot(daily_germany.Date, pred_avg)\nax.axvline(daily_germany.Date.iloc[-best_size_g], ls=':', color='crimson')\nax.set_xticks(ax.get_xticks()[::int(len(daily_germany.Date)\/8)])\nax.legend(['Truth'\n           , 'Prediction (naive)'\n           , 'Prediction (average)'\n           , 'Test starts'\n          ], fontsize=11)\nplt.xlabel('Confirmed date')\nplt.ylabel('Number of cases')\nplt.show()\n\n## 5. Check score\nprint(f'[R2 score in testing]\\nNaive: %.4f\\nAverage: %.4f'\n      %(\n          r2_score(y_germany[-best_size_g:], pred_naive[-best_size_g:])\n          , r2_score(y_germany[-best_size_g:], pred_avg[-best_size_g:])\n       ))","eb13a8ff":"## 1. Ensemble models\npred_fusion_germany = (np.array(pred_naive) + np.array(pred_all))\/2\npred_fusion_germany_avg = (np.array(pred_avg) + np.array(pred_all))\/2\npred_fusion_germany_all = (np.array(pred_avg) + np.array(pred_naive) + np.array(pred_all))\/3\n\n## 2. Plot predictions with all models\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title('Daily Confirmed Cases Prediction (Germany)', size=17)\nplt.plot(daily_germany.Date, y_germany, color='dimgray'\n         , ls=':'\n         , lw=3)\nplt.plot(daily_germany.Date, pred_naive)\nplt.plot(daily_germany.Date, pred_avg)\nplt.plot(daily_germany.Date, pred_all)\nplt.plot(daily_germany.Date, pred_fusion_germany)\nplt.plot(daily_germany.Date, pred_fusion_germany_avg)\nplt.plot(daily_germany.Date, pred_fusion_germany_all)\nax.axvline(daily_germany.Date.iloc[-best_size_g], ls=':', color='crimson')\nax.set_xticks(ax.get_xticks()[::int(len(daily_germany.Date)\/8)])\nax.legend(['Truth'\n           , 'Prediction (naive)'\n           , 'Prediction (average)'\n           , 'Prediction (LR(Google))'\n           , 'Prediction (LR(Google)+naive)'\n           , 'Prediction (LR(Google)+avg)'\n           , 'Prediction (LR(Google)+naive+avg)'\n           , 'Test starts'\n          ], fontsize=11)\nplt.xlabel('Confirmed date')\nplt.ylabel('Number of cases')\nplt.show()\n\n## 2. Check scores\nprint(f'[R2 score in testing]\\\n        \\nNaive: {round( r2_score(y_germany[-best_size_g:], pred_naive[-best_size_g:]), 4 )}\\\n        \\nAverage: {round( r2_score(y_germany[-best_size_g:], pred_avg[-best_size_g:]), 4 )}\\\n        \\nLR(Google): {round( r2_score(y_germany[-best_size_g:], pred_all[-best_size_g:]), 4 )}\\\n        \\nLR(Google) + Naive: {round( r2_score(y_germany[-best_size_g:], pred_fusion_germany[-best_size_g:]), 4 )}\\\n        \\nLR(Google) + Average: {round( r2_score(y_germany[-best_size_g:], pred_fusion_germany_avg[-best_size_g:]), 4 )}\\\n        \\nLR(Google) + Naive + Average: {round( r2_score(y_germany[-best_size_g:], pred_fusion_germany_all[-best_size_g:]), 4 )}')\n\n## 3. Save the best prediction as a benchmark\nbench_germany = pred_fusion_germany_all","f3c231e5":"acplot(daily_korea.TargetValue)\nplt.title('Autocorrelation of daily confrimed cases (S.Korea)', size=17)\nplt.xlabel('Lag size', size=13)\nplt.ylabel('Autocorrelation', size=13)\nplt.show()","3a44a164":"## 1. Fit the model with 10 lags for AR(autoregression) and 1 window for I(difference)\np, d, q = 10, 1, 0\ndate_list = daily_korea.Date.apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\narima = ARIMA(daily_korea.TargetValue\n              , dates= date_list\n              , order=(p, d, q)\n              , freq=\"D\").fit()\nprint(f'# ARIMA model fitted\\n[Parameters]\\np(AR part): {p}, d(I part): {d}, q(MA part): {q}')\n\n## 2. Print the result report\n# print(arima.summary()) \n\n## 3. Check residual errors\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.plot(daily_korea.Date, arima.resid)\nplt.title('Residual Errors of ARIMA on Daily Confrimed Cases (S.Korea)', size=17)\nplt.xlabel('Date', size=13)\nplt.ylabel('Residual errors', size=13)\nax.set_xticks(ax.get_xticks()[::int(len(daily_korea.Date)\/8)])\nplt.show()","d607d8e8":"## 1. Check distribution of residual errors\narima.resid.plot(kind='kde'\n                 , grid=False)\nplt.title('Residual Errors Distribution', size=17)\nplt.xlabel('Residual Errors', size=13)\nplt.ylabel('Density', size=13)\nplt.show()\n## 2. Check statistics\nprint('[Basic statistics]')\nprint(arima.resid.describe())","26fc40ae":"## 1. Overlap predictions(+1 step to the last observation) onto the truth\nfig, ax = plt.subplots(figsize=(13, 7))\n\nplt.plot(daily_korea.Date\n         , daily_korea.TargetValue\n         , color='#33322B', ls=':' , lw=3)\nplt.plot(daily_korea.Date[:-1]\n        , arima.predict()[1:])\nplt.title('ARIMA (one-step forecasting for every date)', size=17)\nplt.xlabel('Date', size=13)\nplt.ylabel('Number of cases', size=13)\nax.set_xticks(ax.get_xticks()[::int(len(daily_korea.Date)\/8)])\nplt.legend(['Truth', 'Prediction'], loc='upper left')\nplt.show()\n\n## 2. Check scores\ndiff, rmse, mae, mape = diff_metrics(daily_korea.TargetValue[:-1], arima.predict()[1:])\nscores = pd.DataFrame(\n    {'rmse': rmse\n     , 'mae': mae\n     , 'mape': mape}\n    , index=['score']\n)\ndisplay(scores)\nprint('- RMSE: Root Mean Sqaure Error\\\n    \\n- MAE: Mean Absolute Error\\\n    \\n- MAPE: Mean Absolute Percentage Error\\\n      ')","f3689dd4":"p, d, q = 10, 1, 0\nparam_list, mape_list = arima_grid(daily_korea, p, d, q, 1);\nprint(f'Minium MAPE: {round(min(mape_list), 4)} by {param_list[np.argmin(mape_list)]} (p, d, q)')","176e4266":"## 1. Apply best parameter set\narima = ARIMA(daily_korea.TargetValue\n              , dates=date_list\n              , order=(11, 2, 0)\n              , freq=\"D\").fit()\narima_pred = arima.predict()\n\n## 2. Overlap predictions(+1 step to the last observation) onto the truth\nfig, ax = plt.subplots(figsize=(13, 7))\n\nplt.plot(daily_korea.Date\n         , daily_korea.TargetValue\n         , color='#33322B', ls=':' , lw=3)\nplt.plot(daily_korea.Date[:-1]\n        , arima.predict()[1:])\nplt.title('ARIMA (with parametrs found by gird search=11, 2, 0)', size=17)\nplt.xlabel('Date', size=13)\nplt.ylabel('Number of cases', size=13)\nax.set_xticks(ax.get_xticks()[::int(len(daily_korea.Date)\/8)])\nplt.legend(['Truth', 'Prediction'], loc='upper left')\nplt.show()\n\n## 3. Check scores\ndiff_grid, rmse_grid, mae_grid, mape_grid = diff_metrics(daily_korea.TargetValue[:-1], arima.predict()[1:])\nscores = pd.DataFrame(\n    {'rmse': rmse_grid\n     , 'mae': mae_grid\n     , 'mape': mape_grid}\n    , index=['score']\n)\nscores","7c3fec15":"# Performance gain checker\nround((mape - mape_grid) \/ mape, 4)*100, round(mape, 4), round(mape_grid, 4)","e3003580":"## 1. Set test set(step) size as the difference between daily_korea and its raw version\ntest_size = 13\n\n## 2. Copy date for updating the rolled predictions by ARIMA as the new truth\ndaily_korea_pred = copy.deepcopy(daily_korea[:-test_size])\ndaily_korea_pred.Date = daily_korea_pred.Date.apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\n\n## 3. Roll ARIMA test_size times to utilize the previous predictions as pseudo truths\nfor i in range(test_size):\n    arima = ARIMA(daily_korea_pred.TargetValue\n                  , dates=daily_korea_pred.Date\n                  , order=(11, 2, 0)\n                  , freq=\"D\").fit()\n    arima_pred = arima.predict()\n    daily_korea_pred.loc[len(daily_korea_pred)] = (daily_korea_pred.Date.values[-1] + timedelta(1)\n                                                   , arima_pred.values[-1])\n\n## 4. Compare predictions with truths (from daily_korea_raw)\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.plot(daily_korea_raw.Date\n         , daily_korea_raw.TargetValue\n         , color='#33322B', ls=':' , lw=3)\nplt.plot(daily_korea_raw.Date[-test_size:], \n         daily_korea_pred[-test_size:].TargetValue)\nplt.title(f'ARIMA (small-step forecasting, step size = {test_size})', size=17)\nplt.xlabel('Date', size=13)\nplt.ylabel('Number of cases', size=13)\nax.set_xticks(ax.get_xticks()[::int(len(daily_korea.Date)\/8)])\nax.axvline(daily_korea_raw.Date.values[-test_size], ls=':', color='crimson')\nplt.legend(['Truth', 'Prediction', 'Test starts'], loc='upper left')\nplt.show()\nprint('# All predictions before test are same as truth')\n## 5. Check scores\ndiff_small, rmse_small, mae_small, mape_small = diff_metrics(daily_korea_raw.TargetValue[-test_size:]\n                                                             , daily_korea_pred.TargetValue[-test_size:])\nscores = pd.DataFrame(\n    {'rmse': rmse_small\n     , 'mae': mae_small\n     , 'mape': mape_small}\n    , index=['score']\n)\ndisplay(scores)","ac9bd339":"# Performance gain checker\nprint(round((mape_grid - mape_small) \/ mape_grid, 4)*100, round(mape_grid, 4), round(mape_small, 4))\n# Length checker\nlen(daily_korea_raw.Date)","b0e1e79a":"pp_daily, predict_daily = do_prophet(daily_korea[:shorter_len], 'Date', 'TargetValue' # dataframe, columns for date and y\n                                     , test_size=best_size_n # set forecasting horizon\n                                     , is_bench=True # plot a given model's predictions too\n                                     , benchmark=bench_korea, bm_name='LR(Naver) + average'\n                                     , simple=False # skip printing intermetidate outputs\n                                    )","5fc1f94e":"print('[Plot by Prophet built-in function]')\nplt.style.use('seaborn')\npp_daily.plot(predict_daily)\nplt.show()\nprint('# dakrer blue line = point forecasting (yhat)\\\n        \\n# lighter blue area = 80% uncertainty interval (yhat_lower ~ yhat_upper)')\n\nprint('\\n[Detailed plots by Prophet built-in function]')\nplt.style.use('seaborn')\npp_daily.plot_components(predict_daily) # detailed plots (trend, seasonality)\nplt.show()\nprint('# trend = global trend\\\n        \\n# weekly = weekly seasonality')","375d3606":"## Reset plot style\nset_style()","f46cda92":"pp_daily, predict_daily = do_prophet(daily_korea[:shorter_len], 'Date', 'TargetValue'\n                                     , test_size=best_size_n\n                                     , is_bench=True\n                                     , benchmark=bench_korea, bm_name='LR(Naver) + average'\n                                     , simple=True\n                                     , custom_period=True, period=len(daily_korea)-best_size_n\n                                    )","e295b6e5":"pp_1step, predict_1step = do_prophet(daily_korea[:shorter_len], 'Date', 'TargetValue' # dataframe, columns for date and y\n                                     , test_size=1 # set forecasting horizon\n                                     , is_bench=True # plot a given model's predictions too\n                                     , benchmark=bench_korea, bm_name='LR(Naver) + average'\n                                     , simple=True # skip printing intermetidate outputs\n                                     , custom_period=True, period=shorter_len-1, num_curve=5\n                                    )","0a7ee2d8":"date = daily_korea.Date.values[shorter_len]\ntruth = daily_korea.iloc[shorter_len, -1]\npred = predict_1step.yhat.values[-1]\ndiff = pred - truth\nresult_1step = result_df(date, truth, pred)\nprint('[Prediction - Prophet]')\ndisplay(result_1step)\nresult_bench = result_df(date, truth, bench_korea[-1])\nprint('[Prediction - benchmark]')\ndisplay(result_bench)","9c921df9":"pp_cumulative, predict_cumulative = do_prophet(test_raw, 'date', 'confirmed'\n                                               , test_size=1 , is_bench=False\n                                               , custom_period=True, period=shorter_len-1\n                                              )","89c414bd":"date = test_raw.date.values[-1]\ntruth = test_raw.loc[len(test_raw)-1, 'confirmed']\npred = predict_cumulative.yhat.values[-1]\ndiff = pred - truth\n\nresult_1step = result_df(date, truth, pred)\nresult_1step","06d10bff":"print('[Mean absolute difference - for all dates]')\nprint(f'Cumulative cases: {round(abs((test_raw.confirmed - predict_cumulative.yhat)).mean(), 4)}')\nprint(f'Daily cases: {round(abs(daily_korea[:shorter_len].TargetValue - predict_1step.yhat).mean(), 4)}')","f32bb648":"## 1. Set parameters\npp_list = [pp_1step, pp_cumulative]\nhorizon = 5\ninitial = len(predict_1step) - horizon - int(horizon\/2)\n## 2-1. Corss validate for the model on daily cases\npp_cv = cross_validation(pp_1step\n                         , horizon= f'{horizon} days'\n                         , initial=f'{initial} days')\nprint('[Performance Metrics - daily cases]')\npp_pm = performance_metrics(pp_cv)\ndisplay(pp_pm)\nprint('- MSE: Mean Sqaure Error\\\n    \\n- RMSE: Root Mean Sqaure Error\\\n    \\n- MAE: Mean Absolute Error\\\n    \\n- MAPE: Mean Absolute Percentage Error\\\n    \\n- MdAPE: Median Absolute Percentage Error\\\n      ')\nplt.style.use('seaborn')\nprint('\\n[Score(MAPE) by length of the forecasting horizon - daily cases]')\nplot_cross_validation_metric(pp_cv\n                             , metric='mape')\nplt.show()\n\n## 2-2. Corss validate for the model on cumulative cases\ninitial = len(predict_cumulative) - horizon - int(horizon\/2)\npp_cv_cumul = cross_validation(pp_cumulative\n                               , horizon= f'{horizon} days'\n                               , initial=f'{initial} days')\nprint('[Performance Metrics - cumulative cases]')\npp_pm_cumul = performance_metrics(pp_cv_cumul)\ndisplay(pp_pm_cumul)\nprint('\\n[Score(MAPE) by length of the forecasting horizon - cumulative cases]')\nplot_cross_validation_metric(pp_cv_cumul\n                             , metric='mape')\nplt.show()","8054de8e":"set_style()","7a72d1db":"print('[Sample data]')\ndisplay(test_global.head(3).append(test_global.tail(3)).T)","3478b26f":"row_by_country = pd.DataFrame({'number_of_rows': test_global.Country_Region.value_counts().values\n                               , 'number_of_(sub)regions': map(int, test_global.Country_Region.value_counts().values\/min(test_global.Country_Region.value_counts().values))}\n                              , index=test_global.Country_Region.value_counts().index)\nrow_by_country.head(3).append(row_by_country.tail(3))","cf4d656e":"## 1. Get only rows with confirmed cases (excluding fatalities)\ntest_global_confirmed = test_global[ test_global.Target == 'ConfirmedCases' ]\nprint(f'# Only rows with confirmed cases selected (excluding fatalities)')\n## 2. Check null\nprint(f'Number of nulls: {test_global_confirmed.TargetValue.isna().sum()}')\n## 3. Replace negative values with the previous value (empirically checked there's no successive negative values)\nneg_rows = test_global_confirmed[ test_global_confirmed.TargetValue < 0 ]\nprint(f'Number of negative values: {len(neg_rows)}')\nfor i in neg_rows.index:\n    test_global_confirmed.loc[i, 'TargetValue'] = test_global_confirmed.loc[i-2, 'TargetValue']\nprint(f'# {len(neg_rows)} negative values are replaced with the previous ones')\nprint(f'Number of negative values after replacing: {len(test_global_confirmed[ test_global_confirmed.TargetValue < 0 ])}')","bdbd20db":"print('[Sample data]')\nrecent_window = 3\ntest_global_all, data_grouped = get_recent_to_max(test_global_confirmed\n                                                  , recent_window = recent_window\n                                                  , max_num = 3)\ndisplay(test_global_all.head(3).append(test_global_all.tail(3)))\nprint('[Basic statistics]')\ntest_global_all.describe()","dbea539b":"min_cases = 3000\ntest_global_recent = test_global_all[test_global_all.total_cases >= min_cases].sort_values('recent_to_max')\nprint(f'# countries with less than {min_cases} total cases excluded')\nprint(f'Number of countries left: {len(test_global_recent)} (from {len(test_global_all)})' )","45a54eff":"plot_phase(data_grouped, test_global_recent)","e95a57df":"plot_phase(data_grouped, test_global_recent, reverse=True)","4549a11b":"if not fast:\n    plot_phase(data_grouped, test_global_recent, no_of_country=len(test_global_recent))","ab42b245":"cluster_full = test_global_recent[ test_global_recent .recent_to_max < 0.1 ]\ncluster_full = cluster_full.sort_values('recent_to_max')\nprint('[First\/last countries in the cluster with 1 full cycle]')\ndisplay(cluster_full.head(1).append(cluster_full.tail(1)))\nprint('Number of countries included: ', len(cluster_full))","af0ee737":"## 1. Make a new dataframe \/ fill it with daily confirmed cases (scaled to max of each country)\nfull_df_raw = pd.DataFrame()\nfor i in cluster_full.index:\n    temp = test_global_confirmed[test_global_confirmed.Country_Region == i].groupby('Date').sum()\n    full_df_raw = pd.concat([full_df_raw, temp.TargetValue\/max(temp.TargetValue)*100])\nfull_df = full_df_raw.rename(columns={0: 'confirmed_scaled'})\n\n## 2. Set artificial historical dates to convert the similarity in space into the seasonality in time\ndate_range = np.arange(\\\n                       # first date\n                       datetime.strptime(full_df.index[-1], '%Y-%m-%d').date() - timedelta(days=len(full_df)-1)\n                       # last date\n                       , datetime.strptime(full_df.index[-1], '%Y-%m-%d').date() + timedelta(days=1)\n                      )\nfull_df.index = range(len(full_df))\nfull_df['date'] = date_range\nfull_df = full_df[['date', 'confirmed_scaled']]\nprint('[Sample data - vectorized \/ scaled]')\ndisplay(full_df.head(3).append(full_df.tail(3)))\n# artificial season size = numbers of days in a country in test_global\ndays_of_country = len(data_grouped.get_group('Germany'))\nprint(f'# vectorized: convert a country\\'s {days_of_country} days into a part of the whole time series')\nprint('# scaled: ratio of daily confirmed cases to the max of each country')","91e342ca":"# 3. Basic statistics\nprint('[Besic statistics]')\ndisplay(full_df.describe())\n\n# 4. Plot cases\nplt.plot(full_df.date\n         , full_df.confirmed_scaled.values)\nplt.title(f'Vectorized confirmed cases ({len(cluster_full)} countries with <0.1 recent_to_max ratio)', size=17)\nplt.xlabel(f'Time range (1 country = {days_of_country} days)', size=13)\nplt.ylabel('Confirmed cases ratio (%)', size=13)\nplt.show()","b5da08f5":"pp_full, predict_full = do_prophet(full_df, 'date', 'confirmed_scaled'\n                                   , test_size=days_of_country\n                                   , custom_period=True, period=days_of_country\n                                   , simple=True);","7ae88168":"## 1. Scale predictions for min \/ max max to be 0 \/ 100\npredict_floor = predict_full.yhat[-days_of_country:] - min(predict_full.yhat[-days_of_country:])\npredict_scale = predict_floor \/ max(predict_floor) * 100\n\n## 2. Amplify predictions to amplify the height variances\npredict_ampl = predict_full.yhat[-days_of_country:]**2\npredict_ampl = predict_ampl \/ max(predict_ampl) * 100\n\n## 3. Plot 3 versions of predictions\ndate_range = data_grouped.get_group('Germany').Date\nfig, ax = plt.subplots(figsize=(13, 7))\nax.plot(data_grouped.get_group('Germany').Date\n         , full_df.confirmed_scaled.values[-days_of_country:]\n         , color='#33322B'\n         , ls=':' , lw=3)\nax.plot(date_range, predict_full.yhat[-days_of_country:])\nax.plot(date_range, predict_scale)\nax.plot(date_range, predict_ampl)\nax.set_xticks(ax.get_xticks()[::int(len(date_range)\/8)])\nplt.title(f'Prediction by Prophet (test on {cluster_full.index[-1]})', size=17)\nplt.xlabel('Date', size=13)\nax.set_ylabel('Daily cases ratio to max (%)', size=13)\nplt.legend(['Truth'\n            , 'Prediction '\n            , 'Prediction (scaeld)'\n            , 'Prediction (amplified)'])\nplt.show()","f305bcd4":"## 1. Fuse amplified and scaled version\npeak = np.argmax(predict_ampl.values)\npredict_fusion = pd.DataFrame(np.concatenate([ predict_ampl.values[:peak], predict_scale.values[peak:] ])\n                              , index = date_range, columns=['TargetValue'])\n\n## 2. Compare with the truth\nfig, ax = plt.subplots(figsize=(13, 7))\nax.plot(data_grouped.get_group('Germany').Date\n         , full_df.confirmed_scaled.values[-days_of_country:]\n         , color='#33322B'\n         , ls=':' , lw=3)\nax.plot(predict_fusion, color=color_list[3])\nax.set_xticks(ax.get_xticks()[::int(len(date_range)\/8)])\nplt.title(f'Prediction by Prophet (test on {cluster_full.index[-1]})', size=17)\nplt.xlabel('Date', size=13)\nax.set_ylabel('Daily cases ratio to max (%)', size=13)\nplt.legend(['Truth'\n            , 'Prediction (amplified + scaled)'])\nplt.show()","e53240f0":"pp_full_1, predict_full_1 = do_prophet(full_df, 'date', 'confirmed_scaled'\n                                       , custom_period=True, period=days_of_country);","e7bffdb8":"scale_1step = predict_full_1.yhat[-1:] \/ max(predict_full_1.yhat) * 100\nampl_1step = (predict_full_1.yhat[-1:]**2 \/ max(predict_full_1.yhat)**2) * 100\npeak = np.argmax(ampl_1step.values)\nfuse_1step = np.concatenate([ ampl_1step.values[:peak], scale_1step.values[peak:] ])\npd.DataFrame([full_df.confirmed_scaled[-1:].values\n              , predict_full_1.yhat[-1:]\n              , scale_1step\n              , ampl_1step\n              , fuse_1step\n             ]\n            , index = ['Truth', 'Prophet', 'Prophet (scaled)', 'Prophet (amplified)', 'Prophet (fusion)']\n            , columns = ['One-step forecasting (value)'])","f4a43404":"print('[Plot by Prophet built-in function]')\nprint(f'# Model with test set size = {days_of_country}')\nplt.style.use('seaborn')\npp_full.plot(predict_full)\nplt.show()\n\nprint('[Detailed plots by Prophet built-in function]')\npp_full.plot_components(predict_full)\nplt.show()\n\nif not fast:\n    cv_full = cross_validation(pp_full, horizon=f'{days_of_country} days')\n    pm_full = performance_metrics(cv_full)\n    print(f'[Score(RMSE) with forecasting horizon {days_of_country} days]')\n    plot_cross_validation_metric(cv_full, metric='rmse')\n    plt.show()","05d46c73":"print('[Plot by Prophet built-in function]')\nprint('# Model with test set size = 1')\n# plt.style.use('seaborn')\npp_full_1.plot(predict_full_1)\nplt.show()\n\nprint('[Detailed plots by Prophet built-in function]')\npp_full_1.plot_components(predict_full_1)\nplt.show()\n\n# if not fast:\n#     cv_full_1 = cross_validation(pp_full_1, horizon=f'{days_of_country} days')\n#     pm_full_1 = performance_metrics(cv_full_1)\n#     print(f'[Score(RMSE) with forecasting horizon {days_of_country} days]')\n#     plot_cross_validation_metric(cv_full_1, metric='rmse')\n#     plt.show()","7e27fad8":"print('[Mean confirmed_scaled values]')\nprint(f'# Last country ({cluster_full.index[-1]}): {round(float(full_df[-days_of_country:-1].mean().values),2)}%')\nprint(f'# Second to last country ({cluster_full.index[-2]}): {round(float(full_df[-days_of_country*2:-days_of_country].mean().values),2)}%')\nprint(f'# All previous countries: {round(float(full_df[:-days_of_country].mean().values),2)}%')","ad9ead55":"## Reset plot style\nset_style()","1ab4d733":"rtm_mid = pd.DataFrame(test_global_recent.iloc[int(len(test_global_recent)\/2), :]).T\nmid_name = rtm_mid.index.values[0]\nrtm_mid_confirmed = test_global_confirmed[ test_global_confirmed.Country_Region.values == rtm_mid.index.values ].groupby('Date').sum().TargetValue\nmid_scale = rtm_mid_confirmed \/ max(rtm_mid_confirmed) * 100\n\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title(f'Prophet for Countries in Mid-phase ({mid_name})', size=17)\nplt.plot(mid_scale\n         , color='#33322B', ls=':', lw=3)\n# ax.plot(predict_fusion.index, predict_scale, color=color_list[3])\nax.plot(predict_fusion.index, predict_fusion, color=color_list[3])\nax.set_xticks(ax.get_xticks()[::int(len(predict_fusion.index)\/8)])\nax.set_xlabel('Confirmed date')\nax.set_ylabel('Daily cases ratio to max (%)', size=13)\nplt.legend([f'Truth'\n            , f'Prediction (by {len(cluster_full)} countries in a late phase - amplified \/ scaled)']\n           , fontsize=11)\nplt.show()","71dde714":"peak_truth, peak_pred = np.argmax(mid_scale), np.argmax(predict_fusion.TargetValue)\nprint(f'[Peak date - {rtm_mid.index.values[0]}]\\nPrediction: {peak_pred}\\\n                   \\nTruth: {peak_truth}')","bca83ee9":"## 1. Get the difference of two peaks\npeak_diff = (datetime.strptime(peak_truth, '%Y-%m-%d').date()\n             - datetime.strptime(peak_pred, '%Y-%m-%d').date()).days\n\n## 2. Move the peak of prediction to that of truth\ndef add_days(day):\n    return (datetime.strptime(day, '%Y-%m-%d').date() + timedelta(days=peak_diff)).strftime('%Y-%m-%d')\nindex_matched = list(map(add_days, predict_fusion.index))\n\n## 3. Check visually\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title(f'Forecasting for Countries in Mid-phase ({mid_name})', size=17)\nplt.plot(mid_scale, color='#33322B', ls=':', lw=3)\nax.plot(index_matched, predict_scale, color=color_list[3])\nax.set_xticks(ax.get_xticks()[::int(len(index_matched)\/8)])\nax.set_xlabel('Confirmed date')\nax.set_ylabel('Daily cases ratio to max (%)', size=13)\n## 4. Scoring starts from the peak day of the truth\nax.axvline(peak_truth, ls=':', color='crimson')\nax.legend([f'Truth ({rtm_mid.index.values[0]})'\n            , f'Prediction (by {len(cluster_full)} countries in a late phase - amplified & scaled)'\n            , f'Test starts ({peak_truth})']\n          , fontsize=11\n          , loc='upper left')\nplt.show()","9151c145":"## 1. Get data for validation after the last update of the current global data\nvalid_pred = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv')\nvalid_mid = valid_pred[valid_pred['Country\/Region'] == mid_name]\n\n## 2. Filter confirmed cases only\nvalid_confirmed = valid_mid.groupby('ObservationDate').sum().iloc[:, 1]\nvalid_confirmed\n\n## 3. Get daily confirmed cases\nvalid_daily_origin = valid_confirmed[1:].values - valid_confirmed[:-1].values\nvalid_daily = valid_daily_origin \/ max(rtm_mid_confirmed) * 100\n\n## 4. Get only data after the last update\nvalid_date = pd.to_datetime(valid_confirmed.index)\nadded_len = len(valid_date[np.argmax(valid_date == mid_scale.index[-1]):]) - 1\nvalid_added = pd.DataFrame(valid_daily[-added_len:]\n                           , index = valid_date[-added_len:].strftime('%Y-%m-%d'))\n\n## 5. Append to the truth data\nmid_scale_added = mid_scale.append(valid_added)\n\n## 6. Check visually\nfig, ax = plt.subplots(figsize=(13, 7))\nplt.title(f'Forecasting for Countries in Mid-phase ({mid_name})', size=17)\nplt.plot(mid_scale_added, color='#33322B', ls=':', lw=3)\nax.plot(index_matched, predict_scale, color=color_list[3])\nax.set_xticks(ax.get_xticks()[::int(len(index_matched)\/8)])\nax.set_xlabel('Confirmed date')\nax.set_ylabel('Daily cases ratio to max (%)', size=13)\nax.axvline(peak_truth, ls=':', color='crimson')\nax.legend([f'Truth ({rtm_mid.index.values[0]})'\n            , f'Prediction (by {len(cluster_full)} countries in a late phase - amplified & scaled)'\n            , f'Test starts ({peak_truth})']\n          , fontsize=11\n          , loc='upper left')\nplt.show()","b140bee4":"print('Latest date on truth: ', max(valid_pred.ObservationDate))\nprint('Max daily on truth: ', max(valid_daily_origin))\nprint('Max daily on pred: ', max(rtm_mid_confirmed))","6f87c369":"finish_log = datetime.now()","62c09dc2":"if fast:\n    version = 'fast'\nversion = 'full'\nprint(f'# Total runtime ({version} version): {(finish_log - start_log)}')","ab67a14b":"<a id='fc_arima'><\/a>\n> # [[FC]](#table) 2. ARIMA\nARIMA - A statistical model for time series forecasting (AutoRegressive + Integrated + Moving Average)\n    1. AR (autoregressive): checks and uses the dependent relationship between the truth and the lagged value of it\n    2. I (integrated): subtracts previous values from the truth to make the time series stationary\n        - stationary means the values are random without trend, seasonal or cyclic patterns\n    3. MA (moving average): uses the moving average of residual errors for forecasting future values\n        - similar to moving average of truth values above\n        - but of residual errors than the values themselves\n- reference\n    1. [Forecasting: Principles and Practice - Chapter 8. ARIMA models](https:\/\/otexts.com\/fpp2\/arima.html)\n    2. [How to Create an ARIMA Model for Time Series Forecasting in Python](https:\/\/machinelearningmastery.com\/arima-for-time-series-forecasting-with-python\/)","d881d35f":"Three countries with the highest recent_to_max ratio","d05e72c2":"Performance Analysis - daily confirmed cases prediction (Germany)\n    1. Naive VS. average\n        1) Naive wins when there's little difference between days\n        2) Averaging is more robust for the short-term shifts\n            - the robustness depends on the window size of averaging (7 applied now)\n        3) This shows in the case of ensembling with other models too\n    2. Linear regression on Google search trends works better in test than training (same as S.Korea)\n        1) Only the scale is alterable while the shape is stable\n        2) The degree of change is up to the choice of testset size\n            - which we decided by empirical runnings with various test set sizes\n            - this could make the model overfitted only to the test set we have now\n        3) We need a complementary model which is insensitive to test set sizes\n    3. Average \/ naive forecasting complement the regression model as they are running dynamically\n        1) Linear regression could harness all the observations in the training set\n        2) Average forecasting is more dynamic than linear regression as it learns from only the recent truths\n        3) Naive catches more dynamic change than average (1 observation VS. 7 observations)\n        4) When combined all three actually seem working harmoniously, yielding the best performance","fa64f17e":"<a id='eda_search'><\/a>\n> # [[EDA]](#table) 9. Search","6f0bb976":"Snap Analysis - population sex balance by region\n    1. Sex seems balanced as a whole and in each region\n    2. It shows no correlation with the number of confirmed cases\n    3. There are no outlier region\n        - by outlier rule: less than Q1-1.5*IQR or bigger than Q3+1.5*IQR\n- Q1: 25% quartile | Q3: 75% quartile | IQR: InterQuartile Range (Q3 - Q1)","afba3bf0":"Data size\n    1. The informations on 40% patients are logged in patient data (5165 among 12800 confirmed patients)\n    2. Which means 60% patients are confirmed but under-documented","321e473b":"Solution for plot mismatching\n    1. Most countries have the different starting \/ peak day of infections\n    2. We ignored this for applying an artificial seasonality to the new time series data above\n    3. The peak of a country can be the anchor for overlapping two different countries' infetion trends\n        - by matching x-axis(date range) of forecasting to that of the truth","e9a77343":"At first glance\n    1. The most surprising factor is the age distribution of patients\n    2. Because there are much more young patients (especially 20s) than the elders \n        - it could be counterintuitive to some media-consumers' prespectives\n    3. So let's face with TimeAge data first","860fe724":"- corrected","1c3232a8":"Snap Analysis - elderly proportion and confirmed cases\n    1. No strong correlation between elderly population proportion and the number of confirmed cases\n    2. Gyeongsangbuk-do is right next to Daegu geographically (check the map below)\n        - Daegue: the pink area\n        - Gyeongsangbuk-do: the upper-right area (almost including Daegu)\n        - thus its high infection rate seems to come from the location than olderness\n![Daegu_SK.png](attachment:Daegu_SK.png)\n    3. What about population (density) than elderly proportion?\n        - Without the outliers (Daegu, Gyeongsangbuk-do)\n          the two most infected regions, Gyeonggi-do and Seoul, are the most populated regions\n- reference: [wikipedia - Map of South Korea with Daegu highlighted](https:\/\/simple.wikipedia.org\/wiki\/Daegu#\/media\/File:Daegu_SK.png) ","ef097969":"Result Analysis - Prophet on one-step forecasting (cumulative cases)\n    1. The model fitted the truth better than daily cases in a visual manner\n    2. But the numeric score is worse than that of daily cases\n        - Differences from truth: 105.5786 to 40.858 (cumulative to daily)\n        - This might come from the bigger absolute numbers of cumulative cases\n    3. We better focuse on improving the model with daily cases\n        - because the two differeces direct the same figure related to the truth\n        - which is the number of over\/under-estimated confirmed cases on a specific day\nWhat about experimenting with various test set sizes (forecasting horizons) more systematically?","eb3998cf":"    1. We learned by EDA there seems to be a positive correlation between search trends on COVID-19 and the number of daily confirmed cases\n    2. we have analysed the search trends in Naver above (market share: 17.42%)\n    3. Let's delve into search trends with additional data from Google (market share: 76.52%)\n- reference - [Search engine market share](https:\/\/gs.statcounter.com\/search-engine-market-share\/all\/south-korea) (2020-05)","7d87d9d2":"Snap Analysis\n    1. The peaks of search trends are prior to that of daily cases with about 2 weeks of interval\n    2. This showed in the graph for S.Korea too with a shorter interval (1 week)\n    3. Not only the global shape but also the small spikes have the similarity to the truth\n        - e.g. around 2020-02-29, 2020-03-21, 2020-04-05\nLet's apply linear regression and naive\/average forecasting","bef2e098":"Snap Analysis - confirmed cases and floating population\n    1. Positive correlation in general\n        - the more people are floating around, the more infections could be\n    2. Most cases in Seoul are from abroad and group infections (like workplaces and clubs)\n        - which could have something to do with the floating population in those areas\n        - regarding the difference in wealth and the concentration of workplaces in downtown\n    3. But the absolute number of cases in each subregion is too small to draw a strong conlcusion from","c9004bf8":"Preprocessing","fc846368":"Policy on immigration","764d51ed":"- The pattern seems same as that of date","df831bda":"Prophet with a custom season size (training set size)","ea737da4":"Compare with sex data","f1e562fe":"Data description\n    1. More than 4 years of search trends on 4 topics related to respiratory diseases (cold \/ flu \/ pneumonia \/ coronavirus)\n    2. 100% means the highest interest in the time range while the others are relative to this\n    3. Data provider: Naver (one of the influential search engines in S.Korea, market share=17.42%)\n- reference: [Search engine market share](https:\/\/gs.statcounter.com\/search-engine-market-share\/all\/south-korea) (as of 2020-05)","123984a2":"Snap Analysis - sex and confirmed cases\n    1. There have been more female patients for all the documented time\n    2. The gap has been growing up to 2020-04-06 and stabilized after then\n    3. More males got infected daily from starting of May","7a9d4c70":"Sex balance index\n    1. sex_bal = 100: totally balanced\n    2. sex_bal < 100: more female\n    3. sex_bal > 100: more male","bc9b43c7":"- 'Sejong' (3rd to last in region data) is absent in weather data","7afb4eb0":"Result Analysis - predictions by Prophet (in test phase)\n    1. With a proper measure, we can get back the original shape of the plot\n        1) scaling: to the max of a country\n        2) amplifying: to the 2nd power then scaling to the max of a country\n    2. But none of both matches just right to the shape of truth\n    3. Why don't we fuse scaling and amplifying?\n        1) Assumptions\n            - the increasing and decreasing phases of daily cases have different growth factors\n                - increasing exponentially \/ decreasing linearly\n            - Prophet caught this to some extent\n                - see the slight difference between the slopes of two phases in green line above\n        2) Problems\n            - we lose some linearity in the decreasing phase while amplifying (because of powering)\n        3) Solutions\n            - apply amplifying up to the peak of the predictions\n            - apply scaling after then\n            - fuse both to get 1 set of predictions","6652e4bf":"Snap Analysis - search trends after the outbreak of COVID-19 worldwide\n    1. No distinct change had occured for about 1 and half months\n    2. Increase of searching pneumonia was prior to that of coronavirus(COVID-19)\n        - because it's called Wuhan pneumonia at first in S.Korea\n            - Wuhan is the assumed place where COVID-19 pandemic started\n    3. After then coronavirus kept dominating the search trends\n        - on 2020-01-23 (3 days after the 1st case of S.Korea) it reached 100% interests in this time range","f42dd251":"Linear Relationships \/ residuals after excluding linearity","45f1eeab":"Representativeness of patient data (sex)","3690507b":"Number of contacts (by a patient before being confirmed)\n    # 0 = no contact with other patients\n    # 1 = contacted (and possibly infected) 1 person which turned out to be confirmed too\n    # and so forth","691cf4f3":"Let's dissect floating populations by feature","8777980d":"Explosion","6b07a568":"Let's check the actual predictions","d1321cf6":"Snap Analysis - search trends after the outbreak of COVID-19 in S.Korea\n    1. After passing the first peak, searching on COVID-19 had been decreasing for about 1 month\n    2. But around 2020-02-21 the mass confirmations in Daegu occured\n        - the search on coronavirus leaped again just before then (2020-02-19)\n            - as the public already heard massive infections might be there\n    3. The search trends shows an impressive similarity to the shape of daily confirmed cases after then\n        - there might be an underlying correlation between them","a5e5bed1":"Cumulative Cases","e7449ffb":"<a id='fc_lessons'><\/a>\n# [[Lessons from forecasting]](#table)\n1. [Linear Regression \/ Naive \/ Average](#fc_basics)\n    - Linear regression(LR) model on search trends could match a general trend of infections\n    - Naive forecasting (by 1 last observation) learns the latest change on the fly\n    - Moving average forecasting (by 7 latest observations) works better than LR and naive\n    - The ensembled model of the three gives the best score\n2. [ARIMA](#fc_arima)\n    - ARIMA requires basic understandings on auto regression, deferencing and moving average to set 3 parameters\n    - A grid search can yeild a better performance than the model with empirically picked parameters\n    - A Rolling-based model learning every prediction as the latest observation tends to underestimate the truth\n3. [Prophet - basics](#fc_prophet_basics)\n    - The data with 1 infection cycle of increase and decrease are not enough to feed Prophet\n        - which is mainly for catching repetitive cyclic and\/or seasonal patterns with a global trend\n    - S.Korea and some countries have only 1 cycle as they are now fighting against the 1st wave of COVID-19\n    - Many others have less than 1 cycle as their curves are not flattening yet (meaning they are in the earlier phase)\n4. [Prophet - space to time](#fc_prophet_fit)\n    - We clustered the countries by recent_to_max equation\n        - which is the ratio of the latest daily cases to the max daily cases in a country\n    - Then used the data of the countries with <0.1 ratio (<10% to max) to make 1 long time series\n    - Prophet learned the general cycle of spreading with a reasonable accuracy\n        - but the scale of predictions is about half of the truth\n        - we scaled and amplified them to match the top with 100% and bottom with 0% daily confirmed cases ratio\n5. [Prophet - time to space](#fc_prophet_apply)\n    - These predictions could give insights to the countries in the earlier phase\n        - on how long does it take to quit the 1 cycle of spreading\n            - to prepare necessary resources\n            - to design a longterm exit plan\n            - to make an effort to shorten the expected time to quit\n    - For the country with median of recent_to_max\n        - the predictions match with the truth in terms of exponential increasing to the peak\n        - the spreading pace from first case to the peak also is aligned with the truth\n        - regarding this, the expected time to approach the late phase is about 1.5 months","aa6fe189":"Snap Analysis\n    1. The 1st flattening in the total curve was mainly due to the decrease in the outliers (from 2020-03-08 to 2020-04-06)\n    2. The number of cases in the other areas kept linearly increasing up to 2020-04-06\n    3. This curve began flattening after then while those of the outliers were flattened more\n        - non outliers' curve moved upward from 2020-05-09 again which pushed the total curve upward too ","4333759d":"### Books\n1. [Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. OTexts.com\/fpp2. Accessed on 2020-04-25](https:\/\/otexts.com\/fpp2\/)","9360cc8f":"Population Density","05ca4c07":"- Too many nulls in end_date (37 out of 61)\n    - We will use start_date as reference dates","f3d3ffc6":"Seoul - age","7bea1f0b":"Snap Analysis - residual errors of ARIMA on daily confirmed cases    \n    1. The errors show a random spread around the 0 horizon\n    2. There seems some left informations the model can use but not extracted yet\n    3. Especially around 2020-02-29 (near to the peak of the truth) could have them","55b6c788":"Snap Analysis - outlier or error\n    1. 2020-02-23\/2020-03-11 was just another Sunday\/Wednesday (officially and empirically)\n    2. In case of the spike day, especially one hour (11 o'clock) shows the similar number to those of the normal days\n        - there could have been double counting at the other hours on that day\n        - we better halve the other hours to match the level\n    3. Whereas the dip day has a stable floating population around the clock\n        - we let this part as it is cause we can't assume further only with this similarity","554c3707":"<a id='eda_age'><\/a>\n> # [[EDA]](#table) 1. Age","5ab75482":"R2 score on linear regression (applied above and below)\n    1. Equation\n        R2 = 1 - ( ((truth - prediction)**2).sum() \/ ((truth - truth.mean())**2).sum() )\n    2. Measuring\n        How similar the predictions' varaiance from truth is to that of the truth itself\n    3. Interpretation\n        1 = best fit\n        0 = irrelevant predictions\n        <0 = worse than random","d14a991e":"Snap Analysis - policy on immigration\n    1. The first policy on immigration procedure was for the visitors from China\n    2. The same special procedure was applied to all countries after a month\n        - for this month the number of applied countries kept increasing\n    3. 14 day self quarantine has been mandatory for all incomers since 2020-03-30\n        - with additional diagnostic tests for those from the U.S.","534e5185":"<a id='resources'><\/a>\n# [[Resources]](#table)","0af4baa0":"3 features on average weather by region (order by confirmed cases)","e50ef842":"### Notebooks\n1. [Covid-19 spread and containment](https:\/\/www.kaggle.com\/lucabasa\/covid-19-spread-and-containment)\n2. [Time series Basics : Exploring traditional TS](https:\/\/www.kaggle.com\/jagangupta\/time-series-basics-exploring-traditional-ts)","a8190599":"Snap Analysis - policy distribution (by type \/ title)\n    1. Most frequently implemented policies are on immigration\n        - because the immigration policies are speicifc to each country\n    2. A tie policy type is for educations\n        - especially on the delay of school opening and its alternatives (online classes)\n    3. 3rd are about public heath\n        - the authorization for diagnostic kit, social distancing campaigns, and mask distributions","e49bada0":"Elderly population proportion by region (without the row for Korea)","4b20b565":"Snap Analysis - different phase by country\n    1. The larger the ratio, the earlier the phase of spreading\n    2. The countries with <0.2 ratio seem in the later phase\n    3. With 0.1 ratio, we can assume that a country is at the end of 1 full cycle or has passed it\nLet's experiment with countries with <0.1 ratio","0aeaa31c":"Train \/ test with Prophet","50e28280":"[[Exercise 1.8]](https:\/\/otexts.com\/fpp2\/intro-exercises.html) First> Possible Predictor Variables\n    1. Case: COVID-19\n        1) Predict the number of daily confirmed cases (deceased, recovered too if possible)\n\t2. Forecasting Variables\n\t\t1) Confirmed cases history (more than 100 days of observations in S.Korea)\n\t3. Explanatroy Variables\n        1) In-house data\n            a. numbers of other case types\n                - tested, deceased, recovered\n            b. search trends on COVID-19\n            c. floating population (Seoul)\n            d. number of incoming visitors from abroad\n            e. weather\n            f. policy implemented on COVID-19\n            g. infection rate to total population\n            h. reproduction rate by a patient \n            i. incubation period\n        2) Data from outside\n            a. confirmed cases in other countries\n            b. other cases in other countries\n            c. policies of other countries\n            d. oil price\n            e. airline usage\n            f. travel trend\n            g. infection rate \/ reproduction rate \/ incubation period in other countries (actual data)\n            h. research results or predictions on g.\n            i. historical data on other coronavirus-related diseases (e.g. SARS, MERS)\n            j. historical data on other infectious disease (e.g. flu, smallpox, ebola)","7891013f":"Result Analysis - ARIMA (with parametrs found by gird search=11, 2, 0)\n    1. MAPE improved 12.61% (from 0.6253 to 0.5464)\n    2. Visaul fitting, RMSE, and MAE also show better results\n    3. But this could lead overfitting\nLet's try multi-step forecasting with ARIMA","1f15323a":"About the datasets\n    1. TimeAge - time series data on the patients by age\n    2. TimeProvince - time series data by administrative region\n    3. Region - metadata on each region\n    4. TimeGender - time series data on the patients by sex\n    5. Time - time series data on the tests and the results\n    6. Case - data on infection cases (path of transmission)\n    7. Weather - time series data on the weather in each region\n    8. PatientInfo - sparse data on the profiles of some confirmed patients\n    9. SeoulFloating - time series data on the floating population in Seoul\n    10. SearchTrend - time series data on the search keywords related to COVID-19 and other respiratory diseases\n    11. Policy - sparse data on the policies implemented by S.Korean government on COVID-19\n    -. PatientRoute - saprse data on the patients' activities before being confirmed\n        # Discarded from the original dataset as of 2020-06-30\n\n- more detailed description by the data uploader - [[DS4C] What is this dataset](https:\/\/www.kaggle.com\/kimjihoo\/ds4c-what-is-this-dataset-detailed-description)","58fe0a3e":"Population Distribution in South Korea (as of 2020)\n- reference: [KOSIS - Korean Statistical Information Service](http:\/\/kosis.kr\/visual\/populationKorea\/experienceYard\/populationPyramid.do?mb=N&menuId=M_3_2)","972d2f46":"###### Click \"output\" to see the plots for all regions","acc95b71":"###### Click \"output\" to see all plots for all countries (lowest to highest ratio)","ac070589":"<a id='updates'><\/a>\n# [[Major Updates]](#table)\n    1. Version 1 to 42: EDA (on 10 datasets excluding policy) updated\n    2. Version 43 to 49: EDA (on policy) \/ Forecasting (LR, naive, averaging) updated\n    3. Version 50 to 88: Forecasting (Prophet - basics, space to time, time to space) updated\n    4. Version 89 to 105: Forecasting (ARIMA) updated\n    5. Version 106: Original datasets on EDA updated\n        1) Activity dataset is discarded\n        2) A column for underlying disease of a patient is discarded in the patient dataset\n        3) A typo in a name of regions appears again in the weather dataset\n        4) Most datasets are updated with the values up to the late of June 2020 (of May for Seoul floating) ","91b1a71a":"Enviroment setting \/ Get data","cf178238":"Numbers to check","a89213d6":"<a id='fc_prophet_basics'><\/a>\n> # [[FC]](#table) 3.1 Prophet - basics","8213bbc7":"Confirmed cases distribution by region","53882797":"Basic numbers (total)","634cdb51":"Infection phase by country - order by confirmed cases","84fb0c37":"Snap Analysis - deceased cases by time\n    1. Clearly different than confirmed, 80s and 70s are at the highest places\n    2. 20s have 0 deceased cases even though there are around 3500 infections among them\n    3. As of 2020-06-30 all ages show flattening curves, even though infections keep occuring","9ed7028e":"Snap Analysis - population (density) and confirmed cases\n    1. Population shows positive correlation in general \n        - the more populations, the more infections (without 2 outliers)\n    2. Population density seems to have no correlation with confirmed cases\n        - what's clearer is the trend that urban areas have distinctively more cases than rural\n    3. Seoul is the outlier among these cities\n        - most crowded as the capital of S.Korea \n        - but has a relatively low infection rate\n        - which shows preventing and mitigating infections are possible even in a megacity","0b07b330":"Country selection\n    1. The country with the median of recent_to_max could be the first candidate\n    2. Cause it might have passed the peak but not finished climbing down the hill yet\n    3. Thus it better know how long does it take to \"exit\" the first wave\n        - to get resources ready\n        - to set a long-term exit plan\n        - to facilitate efforts to shorten the expected time to quit","624cf1dc":"Result Analysis\n    1. The fluctuations of Google are applied reversely\n    2. By this effect, the model gives negative predictions\n    3. Negative values are not applicable to daily new confirmed cases\nLet's 0-floor the predictions","1aaa7ce6":"Data Description\n    1. Daily search trends since the day with the first confirmed case in S.Korea\n    2. Google offers the trend of an individual word, so we use 5 columns for 5 search queries related to COVID-19\n        - we merge them into one column (relative significances among words applied)\n            - 'corona' + 'corona virus' + 'covid' + 'covid19' + 'corona(Kor)' = 'google(all)'\n    3. Naver gives the cumulative result on up to 20 words for 1 search topic\n        - Now we gather a new search trends data from Naver for the same date range as Google's\n            - to account the relative interests only after the first case (not after 2016-01-01 as in search_raw)\n- reference \n    1. [Google trends](https:\/\/trends.google.com\/trends\/explore?date=2020-01-20%202020-05-25&geo=KR&q=corona,coronavirus,covid19,covid,%EC%BD%94%EB%A1%9C%EB%82%98)   (2020-05-25)\n    2. [Naver DataLab](https:\/\/datalab.naver.com\/keyword\/trendResult.naver?hashKey=N_77f94e17adc9a88a8b4a09da941777ab)   (2020-05-25)","bf56600e":"Snap Analysis - floating population by sex and date\n    1. There were more female than male for the documented days\n    2. This doesn't mean the actual proportion of all Seoul floaters must be same\n        - regarding the data provider's limited market share (46.24%)\n    3. Nonetheless by considering the sex balance of Seoul (95.1 male to 100 female),\n       the pattern of more female than male would persist in daily floating populations too","e1b60183":"Compare two graphs","27ce9843":"Snap Analysis - representativeness of patient data (sex)\n    1. The distribution is similar (more female than male)\n    2. The differences between sexs are different (about 10%p in pateint data VS. 14%p in sex data)\n    3. This might come from the missing patients logs in patient data\nWe better recognize this limit while examining other columns which only exist in patient data\n(infection_order, state)","384b83c5":"Snap Analysis - cumulative cases\n    1. Test \/ negative cases have a linear increasing\n        - and show steeper slopes than the other cases since 2020-05-09\n    2. Confirmed cases start flattening on 2020-04-06\n        - but moved upward again since 2020-05-09\n    3. Released \/ deceased cases increase in a stable manner","c48cf26c":"Snap Analysis - test rate \/ confirmed rate \/ deceased rate (as of 2020-06-30)\n    1. Test rate to total population: 2.47%\n        - keeps linearly increasing \n    2. Confirmed rate to the tests: 1.00%\n        - has decreased since 1 week after the peak of daily cases\n    3. Deceased rate to the cases with result (released or deceased): 2.39%\n        - exponentialy decreased for 3 weeks after the peak of daily cases\n        - and flattened after then up to now","a5a5e762":"Second> Describe the five steps of forecasting in the context of this project\n    1. Problem definition\n        1) Predict the number of daily confirmed cases\n        2) Find a general cycle of spreading the virus in a country\n    2. Information gathering\n        1) Gathered\n            - cases, search trends, floating population (Seoul), weather, policies\n        2) Need to gather (S.Korea)\n            - incoming visitors from abroad\n        3) Need to gather (other countries)\n            - cases, policies, search trends\n        4) Need to gather (all)\n            - infection \/ reproduction rate, incubation period, ariline usage, travel trend, oil price\n            - data on other endemics by coronavirus, data on other infectious diseases\n    3. Preliminary (exploratory) analysis\n        1) Possible features found by EDA\n            - strong: search trends (APPLIED)\n                - turned out to be useful for linear regression\n            - mild: floating population\n            - weak: weather (temperature, max_wind_speed)\n        2) Possible strategies for selecting features\n            - feature importances (with decision trees)\n            - correlation matrix\n            - dimensionality reduction (with PCA)\n    4. Choosing and fitting models\n        1) Linear regression (APPLIED)\n            - more feature engineering needed\n            - how to predict future confirmed cases only with past search trends\n                - apply forecasting method on predictions by a linear regression to stretch them out\n        2) Models from the textbook \"Forecasting: Principles and Practice\"\n            - ARIMA: AutoRegressive Integrated Moving Average (APPLIED)\n                - turned out not to work well with a simple parameter selection\n        3) Facebook Prophet (APPLIED)\n            - catches the global shape and seasonality with a proper season size\n            - can learn more general shape with more data of repeating cycle (than with 1 single country) \n        4) Neural network (especially RNN, LSTM)\n    5. Using and evaluating forecasting models\n        1) Train \/ test on the data of S.Korea (APPLIED)\n        2) Train \/ test on the data of similar groups as S.Korea in terms of spreading phase (APPLIED - Germany)\n        3) Train \/ test on all countries (partly APPLIED with those with <0.1 recent_to_max ratio)\n        4) Choose (or ensemble) the best models\n        5) Refine (with (hyper)parameter tuning)\n        6) Deploy (with keeping debugging)","aae38bb4":"Deceased cases","c55d41a2":"Result Analysis\n    1. Average works better than naive\n    2. Cause it is less sensitive to the latest change than naive\n    3. Linear regression model on Naver search trends shows a comparable score to average's\n        - we can get a better score by ensembling both predictions 'LR(Naver) + average'\n        - 'LR(Naver) + naive' also shows a performance gain but not as big as that with average\n        - When we combine all of the 3, it gives a little bit better score than LR + average set\nCould we validate this result with the data of other countries?","d11a7df9":"Result Analysis - Prohet with default parameters (test_size=best_size_n)\n    1. The model catches the early trend of increasing while missing the late trend of decreasing\n        - even though the downhill is also included in the training set, Prophet didn't catch it\n    2. It shows the ability to grasp a weekly seasonality by itself (we didn't notice this above)\n    3. But the seasonality isn't able to turn down the global curve after the peak\n        - this auto-caught weekly seasonality might be the cause of the mismatching in trends\nWhat about using a custom season size?","84ebd343":"<a id='fc_prophet_apply'><\/a>\n> # [[FC]](#table) 3.3 Prophet - time to space\n    - Apply our time series forecasting to the countries in the mid \/ early phase of spreading","f6c66e6f":"Population","aa9820c2":"Snap Analysis - resident population by age (compared to floating population)\n    1. The order and proportions are simliar to that of floating population\n        - except that 20s became the most populated group\n        - by our assumption 20s means <=29 in floating population\n    2. 20's floating populations might refer to 20s only\n        - while 70s means >=70s since the porportions match\n    3. When it comes to 0s and 10s, they seem to be included in 30s ~ 50s\n        - because most of their caregivers are expected to be 30s ~ 50s\n        - and they make contracts with a mobile company on behalf of their children","441595ec":"Snap Analysis - confirmed cases by region\n    1. 'Daegu' + 'Gyeongsangbuk-do' have about 70% of all confirmed cases\n    2. There was a big religious event in Daegu right before the mass infections\n        - which is said to be the origin of most cases in those regions\n    3. Other than them, the capital metropolitan area has the most cases (Seoul, Gyunggi-do, Incheon)","993dcc31":"Snap Analysis - Google search trends on COVID-19\n    1. Naver trends have a lot more similar pattern to daily confirmed cases than Google's\n    2. Google trends show some random fluctuation for the first 3 weeks in April\n        1) This mainly came from the query of corona in Korean\n        2) It seems unreasonable that there were more interests 1 month after the peak of confirmed cases than at the peak\n        3) There might be systematic errors or causes to raise the frequency of queries for this period\n    3. By its similarity, Naver trends are expected to be more useful for forecasting the confirmed cases\nLet's experiment with a linear regression model first","9da2360e":"Snap Analysis - policy on education\n    1. All schools were closed right after the peak of daily new cases (including children daycare center)\n    2. When there were 39 daily cases (yellow line)\n       the online classes for 3rd graders in high\/middle schools opened\n       - and those for the other graders followed\n    3. Part of physical schools opened since 2020-05-20 (blue line)","81908c09":"Naive \/ average forecasting with daily confirmed cases","21921576":"Path distribution","cc8acdaf":"Search trends","703c2363":"Seoul - sex","6e820a7f":"Snap Analysis - autocorrelation of daily confirmed cases (S.Korea)\n    1. The values show significant correlations with the truth up to 10 lag\n        - which means the lagged (previous) values of the truth could be useful\n    2. We choose 10 lag as the parameter of p(AR part) in ARIMA model\n    3. Lag 38 reaches the significance threshold with 99% confidence\n        - but not with 95%, thus we won't use it","f9f2d4f2":"Ensemble and compare models","108b6891":"Snap Analysis - floating population by age and date\n    1. On 2020-02-02 the number of 60s and 70s dropped about 10% (0.1 millions)\n        - and kept being similar till the last day of logging\n    2. Other age groups showed the same pattern as usaul for that period\n    3. This could be explained by the fact that most workers in Seoul are in their 20s~50s","9ebc1f87":"Confirmed cases by subregion (as of 2020-06-03)\n- reference - [Seoul COVID-19](http:\/\/www.seoul.go.kr\/coronaV\/coronaStatus.do)","bf5073e6":"What about using more sophisticated forecasting methods? (on the data of S.Korea)","37fc0044":"Rates (test to total population, confirmed to test, deceased to resulted)","d29864c7":"Infection phase by country - order by recent_to_max (lowest to highest)","7e3e0814":"###### Click \"output\" to compare three graphs side by side","e0ad2f28":"<a id='eda_test'><\/a>\n> # [[EDA]](#table) 4. Test","ef3dfc9d":"Snap Analysis - linear relationships \/ residuals\n    1. Naver trends show a stronger positive linearity than Google trends as expected by the similarity of shapes\n        - but this result is not definitive since we applied the time gap between the peaks of two graphs for overlapping them better\n    2. Both residual plots seem not random (thus not stationary) yet\n        - which means a simple linearity can't explain the relationship of search trends and confirmed cases solely\n    3. We better run the actual model to check if the search trends have any (limited) potential to predict infections","2f6cb959":"- We will use only the columns other than above 2 (policy_id, country)\n    - While keeping end_date as additional references in case","32350bff":"Snap Analysis - type of transmission\n    1. In accordance with above (path of transmission), about 70% transmissions occured in group\n    2. This figure includes other group events like transmissions at workplaces and clubs\n    3. The first focus of prevention must be on groups","0075cdda":"Linear regression with Google search trends","1d6ab381":"Selection criterion\n    1. We need a criterion to choose which countries to learn from before making a time series\n    2. We defined it as recent_to_max ratio for grasping the infection phase in which a country is\n    3. Equation: recent_daily_confirmed.mean() \/ max_daily_confirmed = recent_to_max (of a country)\n        1) e.g. ((35+38+49)\/3) \/ 851 = 0.0478 (S.Korea)\n        2) recent_to_max = 1, if max_daily_confirmed is one of the values in recent_daily_confirmed\n    4. Reasoning behind recent_window (number of days to be considered as \"recent\")\n        1) default = 3\n            - To catch the latest trend of increasing\n        2) only 1 recent window is too sensitive to daily up and down\n        3) longer than 7 window is good at catching the countries having passed 1 cycle already\n            - but terrible in differntiating the countries with high ratios (in the early phase)","144146ed":"Population Sex Balance - total (as of 2020-02)\n- reference: [KOSIS - Korean Statistical Information Service](http:\/\/kosis.kr\/statHtml\/statHtml.do?orgId=101&tblId=DT_1YL20701&vw_cd=MT_GTITLE01&list_id=101&seqNo=&lang_mode=ko&language=kor&obj_var_id=&itm_id=&conn_path=MT_GTITLE01)","dd854ff9":"### Videos\n1. [Simulating an epidemic](https:\/\/www.youtube.com\/watch?v=gxAaO2rsdIs)\n2. [Exponential growth and epidemics](https:\/\/www.youtube.com\/watch?v=Kas0tIxDvrg)","5aa86e9e":"Cross validation with Prophet built-in function\n    1. Prophet offers an automated model evaluation function\n    2. The main parameter is the upper limit of forecasting horizon (N)\n    3. Prophet runs with 1 to N horizon to get a score on each horizon (n)","583ea030":"Data structure - test_global (sample)","204eec89":"[Germany] a country with reasonably larger values of variables than S.Korea\n    1. Population - Germany: 83,149,300 \/ S.Korea: 51,780,579\n    2. Confirmed rate - Germany: 2,198 \/ S.Korea: 226 (per 1 million people)\n    3. Google market share - Germany: 92.86% \/ S.Korea: 76.52%\n- reference:\n[German population (2019-09-30)](https:\/\/www.destatis.de\/DE\/Themen\/Gesellschaft-Umwelt\/Bevoelkerung\/Bevoelkerungsstand\/Tabellen\/zensus-geschlecht-staatsangehoerigkeit-2019.html) \/ \n[Confirmed rate (2020-06-02)](https:\/\/www.worldometers.info\/coronavirus\/) \/ \n[German market share (2020-05)](https:\/\/gs.statcounter.com\/search-engine-market-share\/all\/germany)","d974e4b9":"### Articles\n1. [The Korean clusters - How coronavirus cases exploded in South Korean churches and hospitals](https:\/\/graphics.reuters.com\/CHINA-HEALTH-SOUTHKOREA-CLUSTERS\/0100B5G33SB\/index.html)","87b8168e":"Result Analysis - validation by Prophet (test set size = 1)\n    1. Seasonality is almost same\n    2. The extent of changing trend at the last part is also similar to the model with a larger test set\n        1) This might be because of the similar truth values in the last country to the previous values\n    3. But this stability might be broken with significantly different values in another country\n        1) this model could be sensitive to one additional season\n        2) or it learns new data fast and applies them to forecasting\nNow we have predictions by a Prophet model which learned from the countries in the late phase of spreading","ff7c2262":"Early phase","2d7a0963":"Prophet on one-step forecasting (daily cases)\n    - Training set: all data except for 1 last observation\n    - Test set: the last observation","41f04595":"Snap Anaylsis - alert level\n    1. Level 3 (orange) were applied proactively when there were 4 cumulative confirmed cases\n    2. The other levels were implemented following the criteria in the guideline\n    3. As of 2020-06-08 the alert level keeps at 4 (red)\n       and the level of social distancing moved to the highest again (after lowering down once)","e251747e":"<a id='eda_seoul'><\/a>\n> # [[EDA]](#table) 8. Seoul","489d69f5":"Snap Analysis - weather and confirmed cases (since 2020-01-20)\n    1. Temperature : weak negative correlation in general (the colder, the more cases)\n    2. Maximum wind speed : weaker positive correlation (the more wind, the more cases)\n        - except for the last two regions\n    3. Humidity : little or no correlation\nAs a whole, there seems no significant correlation between the weather and the number of cases","4e201bb3":"Preprocessing","b6249e68":"Let's start with default settings","eb2129bc":"Snap Analysis - daily new cases\n    1. Test \/ negative \/ confirmed cases have been decreasing since the peak with 813 confirmed cases\n    2. Test and negative cases peaked again with increase of new confirmed cases in the 2nd week of May\n        - resulting in the matching level of daily testing in the 1st week of June\n    3. Released \/ deceased cases kept increasing up to 1 month after the peak then began decreasing\n        - there was a jump in the released cases at the end of June","80b8cc5e":"Snap Analysis - weather and confirmed cases (since 2016-01-01)\n    1. There seems no strong correlations\n    2. S.Korea shows a uniform weather in general\n        - there are some outstanding cases like maximun wind speed in Jeollanam-do\n    3. We are now seeing the general picture of weather for more than 4 years\n        - what about only the period after COVID-19 outbreak in S.Korea? (2020-01-20)","f4a2317a":"Snap Analysis - path of transmission\n    1. 'Shincheonji Church' (in Daegu + Gyeongsangbuk-do) has about 45% of all infections logged in this dataset\n        - this religious group is said to be with hundreds of thousands members nationwide\n    2. There could be indirect infections by this event in these areas as the category of 'contatct with patients' or 'etc'\n    3. Summing up, up to 60% of infections are suspected to occur by this single event\n        - showing how important the effort of preventing and detecting this type of mass gatherings is","83c3f6a9":"### Websites\n1. [COVID-19 Coronavirus Pandemic by worldometer](https:\/\/www.worldometers.info\/coronavirus\/)\n2. [COVID-19 Community Mobility Reports by Google](https:\/\/www.google.com\/covid19\/mobility\/)\n3. [COVID-19 Forecasting by EpidemicForecasting.org](http:\/\/epidemicforecasting.org\/models?)","a7eed139":"Basic statistics - Total","bf67cea2":"###### - End of EDA","cc17a340":"Seoul - subregion","f70e05c9":"Result Analysis - predictions by Prophet (fused version = scaled + amplified)\n    1. The overall shape matches with the truth better than individual scalers\n    2. But the location and height of a hill is still mismatching with the truth\n    3. This mismatching is one of the core questions of our effort\n        1) How to apply what we learned from the countries which have passed 1 cycle of infections\n           for those who are in the middle or early phase\n        2) We are going to use the peaks of two shapes (check below in [FC] 4. time to space)\nLet's experiment with a larger training set first","f146538f":"Snap Analysis - policy on public health\n    1. The first authorization for emergency use of diagnositc kit was implemented\n       when there were 16 confirmed cases by 607 tests in sum\n    2. The first drive-through diagnostic center was launched at local level\n       when there were 1,261 cumulative confirmed cases by 53,553 tests (with 284 daily confirmed cases)\n    3. The first mask distribution by government was on the next day of opening drive-through centers\n       and it has been implemented up to now","b6d8397b":"#### Confirmed cases by time","da85c0be":"Confirmed cases regarding population distribution by age","439b8a68":"Snap Analysis - policy distribution (by starting day \/ starting month)\n    1. The first policy started on 2020-01-03 \n        - by setting infectious disease alert level 1 (blue)\n        - which was 17 days before the first case in S.Korea occured\n    2. In general the frequency and numbers of policies are evenly distributed\n        - 0 to 7 policies were applied per day\n    3. About 34% (21\/61) policies began in 2020-03","6e09b763":"Snap Analysis - sex and deceased cases\n    1. There have been more deceased males almost all the time\n    2. The gap is narrower than that of confirmed cases\n    3. Some underlying factors could cause male's higher fatality (e.g. smoking)\nIs there any difference of sex in population itself?","c17cbd46":"Result Analysis\n    1. The longer the horizon, the less accurate the model in general\n        - not for daily cases which shows better scores with longer horizons\n    2. It's the innate limitation of our model which learned from utmost 1 cycle of infections\n        1) It assumes the season must be repeated with the start of forecasting (like ARIMA)\n            - which means it is not just the limit of our model but also the basic assumption of major forecasting methods\n        2) But this assumption can be wrong when there's no more waves of infections\n            - the preformance would be poor too, even if there are more cycles\n              but they aren't of the similar size and pattern to the first one\n        3) In short, our dataset isn't proper for forcasting our own cases in the future\n    3. But it could be useful for those who are in the early or middle of the first wave to learn what's coming\n        1) S.Korea has only 1 cycle (most other countries do as well)\n        2) It must be not enough to generalize to others\n        3) We need more data\n            - not from the future of one country\n            - but from the present of many countries\nLet's convert the similarity in space to the seasonality in time\n- by merging the cases of numerous countries into 1 long artificial time series","f04471bc":"Three countries with the lowest recent_to_max ratio","b4bb0332":"Result Analysis - score by test set size (Naver trends)\n    1. Training score: similar pattern to that of Google trends\n        1) Best score 1 comes with test set size 0.98 (same as Google)\n    2. Test score: far better than Google trends\n        1) This is expected by the similarity of shapes\n        2) The best test set size is about 70% of the data\nLet's check the actual predictions with each model's best size","8484d66b":"Time series forecasting - naive \/ average\n    1. naive: the 1 latest observed truth is the next prediction\n    2. average: the average of 7 latest observations is the next prediction","b30846e3":"<a id='fc'><\/a>\n# [[FC: ForeCasting]](#table)\n    1. How can machine (learning) help us deepen the understandings from EDA?\n        1) While our human eyes used to focus more on what happened already,\n           machine may look beyond to the future objectively and systematically\n    2. How much worthy or useful is it to predict the number of infections or deceased victims?\n        1) It could help decision makers select more efficient and timely measures to tackle the new cases\n        2) It might be helpful for individuals to know how much we need to be serious on preventing efforts\n    3. What could be fundamental or neccessary too for anyone in the storm of pandemic?\n        1) A general forecasting model not only for S.Korea but for all countries\n            - especially for the ones showing the early pattern of spreading\n        2) Ways of allocating necessary medical resources nationwide or even worldwide (e.g. ventilator)\n            - from the places with the decreasing trend to those with the increasing one","b4cf0b59":"Confirmed cases","369bcac3":"Time series of cumulative cases by age","0f95da79":"Compare all models","f1b61c07":"Population \/ Population Density in S.Korea (as of 2018)\n- reference - [National Index Portal](http:\/\/index.go.kr\/potal\/main\/EachDtlPageDetail.do?idx_cd=1007)","cae6b1b9":"Visual check","7c9875d5":"<a id='fc_prophet_fit'><\/a>\n> # [[FC]](#table) 3.2 Prophet - space to time","4a997ea4":"Result Analysis - validation by Prophet (test set size = days_of_country)\n    1. There are some changes in the global trend in the latter part\n    2. Seasonal up and down is caught well (but skewed toward right)\n        - the case logs of all countries start from 2020-01-23 in test_global data\n            - and we utilized this uniformity to make an artificial seasonality\n    3. RMSE(Root Mean Sqaure Error) fluctuates around 20 as the horizon of forecasting changes","9650dc74":"Result Analysis - ARIMA (small-step forecasting, step size = 13)\n    1. MAPE gets 41.12% performance gain (from 0.5464 to 0.3217)\n        - this is not only from the actual performance change\n        - but also from the shorter length of testing period (13) this time\n        - as we applied one step forcasting for all days (140) above\n    2. The actual predictions by ARIMA tend to underestimate the daily confirmed cases\n    3. Which shows the basic limitation and assumption of forecasting\n        - the global trend the model learned must be same in the future\n        - this could be overcome partly by feeding ARIMA with the lastest truth all the time (like naive forecasting)\nWe better experiment the nature of forecasting with another sophisticated method first","a6efa3fd":"Policy on education","5d1a271f":"Snap Analysis - confirmed cases by time\n    1. The gap between 20s and the other age groups kept wider up to 2020-03-08\n    2. After then the slope of all groups are getting smoother\n        - But that of 80s began steeper again around 2020-03-18\n        - 20s' cases increased more than others (starting 2020-03-23 and 2020-05-07)\n    3. As of 2020-06-30 most groups show upward curves (except for 0s, 10s, 80s) ","1edbf715":"Result Analysis - Prophet on artificial time series data (test set size = 1)\n    1. The global trend and cyclic shape seem almost same as that with a smaller training set\n    2. Scaling tends to increase the small values while amplifying does to decrease more\n        - Prophet without scaling or amplifying works well on 1 step forecasting\n    3. But the difference among those are not big\n        - the model was trained on the countries at the end of 1 cycle\n        - when the absolute number of infections is small\n        - thus the effect of scaling\/amplifying is small too\nLet's delve deeper with Prophet's validation tool for both models","b586a162":"<a id='eda_sex'><\/a>\n> # [[EDA]](#table) 3. Sex","094f1ccc":"Snap Analysis - outliers (7 out of 678 contacts logs)\n    1. SEX - female: 3 | male: 4\n    2. AGE - 10s: 1 | 20s: 1 | 30s: 1 | 40s: 2 | 50s: 1 | 60s: 1\n    3. NATIONALITY - Korean: 5 | Chinese: 2\n    4. URBANITY - all living in the urban area (major cities)\n    5. CONFIRMED DATE - all in 2020-02\n    6. STATE - isolated: 1 | released: 6 (as of 2020-06-30)\n        - it is suspicious that the patient in her 60s is still isolated after 5 months of the confirmed date\n        - according to a journal in S.Korea she has been released after 67 days of hospitalization\n        - meaning there could be other errors in this dataset\n- reference: [Patients 31 has been released from hospitalizatiion (in Korean)](https:\/\/www.yna.co.kr\/view\/AKR20200426038700053)","699d2913":"Snap Analysis - infection phase by country\n    1. The more cases, the higher recent_to_max ratio broadly\n        - countries with 3 most cases VS. countries with 3 least cases\n    2. But this doesn't fit for all individual cases\n        - US shows a lower ratio than Brazil, although it has more cases\n    3. About 50% countries show less than 0.2 ratio of recent_to_max\n        - they are assumed to be in the phase of slowing down or containing infections\nLet's see if there's a similarity in the curves among the countries with the similar ratio","821d5c9d":"Snap Analysis - elderly popultaion proportion\n    1. The more rural a region is, the more elderly are there\n        - up to 7th, Gangwon-do, the regions are major rural areas in S.Korea\n    2. No speicific region shows an exceptionally high or low composition of the elderly\n    3. The epicentre of outbreak, Daegu, is at the 9th place on the list \n        - and not that older than the other cities rightside","d4b0478d":"Data structure - a new time seires\n    1. There are 187 countries in test_global with different numbers of rows by subregion\n        - 1 to 3198 subregions (1 subregion means it has no subregion than the country itself)\n    2. We will group them country-wide\n    3. And use 'Date, TargetValue' columns to make a time series","91098df8":"<a id='eda_policy'><\/a>\n> # [[EDA]](#table) 10. Policy","85e7f26a":"Linear Regression","5cd41294":"# [Introduction]\n1. First I really appreciate this initiative, datasets, and all the contributors of them\n2. For a resident in one of the epicentres of COVID-19, this data must be heartbreaking but valuable sources to learn from\n3. We Kagglers as data scientists may have a sort of duty to confront this crisis more seriously and collaboratively than ever\n4. It's better to confess that I have not enough knowledge nor skills on data science to tackle this professionally\n5. But I decided to dive as soon as possible after I found this initiative. Because it's a matter and war of timing\n6. It'd be an honor if this effort helped any better understand the disease, spread of it, and its impact on us","558e210e":"<a id='directions'><\/a>\n# [[Developing Directions]](#table)\n    1. Make recent_to_max more robust to the countries with recent fluctuations\n        1) Is 1000 minimum cumulative cases enough to filter out the countries with unstable daily cases?\n            - no > increased to 2000 (APPLIED) > increased to 3000 (2020-06-12)\n            - with a bigger threshold, the number of countries with <0.1 ratio would be too small to make a long time series\n        2) Isn't 1 recent window too vulnerable to daily fluctuations?\n            - yes > changed to 3 (APPLIED)\n            - a longer than 7 window works bad at differentiating the countries with high ratios\n        3) What about avg_max than max?\n            a. (max + second_max + third_max) \/ 3 (APPLIED)\n            b. (1day_before_max + max + 1day_after_max) \/ 3\n                - discarded because there are countries with fluctuations around the peak\n    2. How can we use our model for the countries which haven't reached the peak of infections yet?\n    3. Get more features for LR \/ Consider non-linear regressions too","e3b8b4c4":"Failure analysis - how our model turned out to be terribly wrong after 1 month\n    1. We missed or ignored the probability that the peak hadn't yet arrvied in the mid-phased countries\n    2. Our recent_to_max ratio could lead a wrong conclusion on the phase a country is in\n        - especially for the countries like Columbia with an unreliably fluctuating daily report, it is more plausible\n    3. Our assumption that the countries in the similar phase may have a similar extent of tackling effort on COVID-19 might be wrong\n        - we didn't include the factors on the level of general and medical development in each country\nHow to solve or metigate these problems?","171699f1":"<a id='eda_insights'><\/a>\n# [[Insights from EDA]](#table)\n1. [Age](#eda_age)\n    - 20s are the most infected age group (27.80% to total)\n    - Older than 70 are the most deceased age group (48.08% to total)\n    - Countries need to separate the two groups while preventing spreads as a whole\n2. [Region](#eda_location)\n    - The more populated, the more cases in general (excluding 2 outliers)\n    - What's riskier are community-level gatherings like religious events\n    - Countries better try to prevent them and detect the particular event if it occured\n3. [Sex](#eda_sex)\n    - Any significant difference between sexes was not found\n    - Females seem more prone to be infected (58.91% VS. 41.09%)\n    - Males seem more prone to be deceased (51.92% VS. 48.08%)\n4. [Test](#eda_test)\n    - The number of tests and negative\/released cases have kept increasing linearly\n    - Confirmed\/deceased cases show more dynamic slope changes than above three while increasing\n    - Daily confirmed cases have the second spike around 2020-05-11 (a probable sign of the 2nd wave)\n5. [Path](#eda_path)\n    - Up to 60% cases seemed to be transmitted by 1 specific gathering (Sinchonji Church in Daegu)\n    - The following paths are also from groups like workplaces\n    - The imported cases from abroad keep increasing\n6. [Weather](#eda_weather)\n    - Weather-related features show no clear correlation with infections\n    - There is a weak negative correlation between temperature and infections (colder & more infections)\n    - There is a weaker positive correlation between maximun wind speed and infections (more wind & more infections)\n7. [Patient](#eda_patient)\n    - Half of the logged patients have <=4 contacts with (or possible infections to) other patients before confirmed\n    - There are outliers with more than 1,000 contacts though\n    - The average number of contacts by a patient per day has decreased as time went by\n8. [Seoul](#eda_seoul) (the capital of S.Korea)\n    - Floating population shows a positive correlation with the number of infections (more floating & more infections)\n    - The activities of older than 60 decreased after the outbreak of COVID-19\n    - The absolute number of cases in each subregion is too small (<100) to draw a conclusion from\n9. [Search](#eda_search)\n    - Searching on pneumonia was prior to that on COVID-19\n    - Increase of COVID-19 searches has been ahead of the actual increase of confirmed cases\n    - The searching itself and the number of daily new cases have decreased (with a small spike around 2020-05-11)\n10. [Policy](#eda_policy)\n    - Level 3 (2nd highest) caution on infectious diseases was implemented proactively\n    - Policies on immigration have kept updated for more countries while the borders are open\n    - Educational measures are yet shifting 'complete closure' > 'online classes' > 'physical classes (some graders)'\n11. [Activity](#eda_activity) (discarded from the original dataset as of 2020-06-30)\n    - Half of the documented patients have <=4 activity logs for 2 weeks before confirmed\n    - The most visited places are hospitals followed by stores, public transportations and restaurants\n    - Countries should apply a lot stricter preventing measures on these places","03640496":"###### - Work in Progress","abfed0aa":"Without 2 outlier regions having 70% of all cases","b95c8de2":"Result Analysis - predictions by search trends\n    1. Google trends: the recent fluctuations in search trends make the predictions irrelevant\n    2. Naver trends: it shows better predictions in test than training\n    3. Naver's accuracy heavily relies on the similar shape to the confirmed cases\n        - it could be worse if the matching was broken at any point\n        - we need a more robust model\nLet's use both trends (even if there may be some correlation between them, do at least try)","d9cbcff0":"Snap Analysis - average contacts by time\n    1. Negative correlation in general (excluding the days with an exceptional spike)\n        - the more recently a person was confirmed, the less contacts she or he had before the diagnosis\n    2. It might be because of the stricter social distancing and growing public awareness as time went by\n        - meaning the outbreak of outliers is not just the fault of individuals but the failure of the guidance too\n        - in other more hopeful words, it CAN be prevented by the right measures at the right time\n    3. This trends has been shifted since the early of May\n        - probably reflecting the loosen awareness and efforts on preventing","cd08b0a6":"<a id='eda_weather'><\/a>\n> # [[EDA]](#table) 6. Weather","7099bf2f":"Snap Analysis - floating population by age and hour\n    1. 60s and 70s look insensitive to hour as expected above\n    2. Whereas 20s, 30s, and 40s have the workers' hill pattern from 6 to 20 (8 p.m.)\n    3. 50s seem mixed (passersby + workers)","94980a15":"Prophet (or fbprophet)\n    1. A semi-automated forecasting library open-sourced by Facebook in 2017\n    2. Focusing to automate\n        1) forecasting\n        2) evaluating the results\n    3. While freeing human analysts for\n        1) designing the model with a simpler parameter tuning (than non-automated alternatives)\n        2) getting into the loop for improvement, only if necessary after inspecting the results\n- reference: [Prophet github page](https:\/\/facebook.github.io\/prophet\/)","cc7372de":"### Metadata on region","4cc218e9":"Probable Reasons - for the exceptionally many confirmed cases in 20s\n    1. Social Factors\n        - 20s tend to have more social activities than the others\n           - e.g. universities, datings, and hanging out\n        - When it comes to 10s, all schools have been on vacation and the beginning of new semesters delayed\n            - unlike these schools, universities have not just classrooms but also research institutes which keep running\n    2. Romantic Factors\n        - 20s are the peak of romantic seasons biologically\n    3. Psychological Factors\n        - We heard from media that COVID-19 is critically dangerous to the elders\n        - COVID-19 might seem \"just\" another type of flu or cold for some youngers\n        - This could make them take containing efforts (like quarantine and social distancing) less seriously\nWhat about the relative size of confirmed cases to the population of each group? (than absolute numbers like above)","af7e6380":"<a id='eda_patient'><\/a>\n> # [[EDA]](#table) 7. Patient","b8fc8125":"Result Analysis - ARIMA (one-step forecasting for every date)\n    1. ARIMA model gives one-step prediction for every date\n    2. We apply -1 step to all predictions for overlapping them with the truth\n    3. The model fits the truth to some extent visually and numerically\nCan't it do better? Let's grid search better parameters (p, d, q)","b83bceb4":"Compare with\/without 2 outliers and total","f900580b":"Alert level on infectious disease\n    1. Level 1 (blue)\n        0) When\n            - There is a new epidemic outside of S.Korea\n            - An epidemic with unknown causes occurs or the known epidemic reoccurs in S.Korea\n        1) Do\n            - Set and run a taskforce for each type of epidemics in KCDC (Korea Centers for Disease Control)\n            - Monitor the situtation and keep ready for worse scenarios\n            - Disinfect the affected area and run the infrastructures for disinfecting (if necessary)\n    2. Level 2 (yellow)\n        0) When\n            - The epidemic from abroad comes into S.Korea\n            - An epidemic with unknown causes or the known epidemic spreads in S.Korea to a limited extent\n        1) Do\n            - Set and run the central taskforce in KCDC\n            - Start cooperative work with the related organizations\n            - Disinfect the affected area and run the infrastructures for disinfecting\n            - Intensify monitoring\n    3. Level 3 (orange)\n        0) When\n            - The epidemic from abroad spreads in S.Korea with to a limited extent\n            - An epidemic with unknown causes or the known epidemic spreads in S.Korea community-wide\n        1) Do\n            - Keep running the central taskforce\n            - Set and run another taskforce for handling losses from the epidemic in MOHW (Ministry Of Health and Welfare)\n            - Hold a general governmental conference by the prime minister if necessary\n            - Check whether it's required to run the general assisting taskforce in MOIS (Ministry Of the Interior and Safety)\n            - Intensify cooperative work with the related organizations\n            - Intensify disinfecting and monitoring more\n    4. Level 4 (red)\n        0) When\n            - The epidemic from abroad spread in S.Korea community-wide or nation-wide\n            - An epidemic with unknown causes or the known epidemic spreads in S.Korea nation-wide\n        1) Do\n            - Allocate all the governmental offices to reactive efforts\n            - Set and run the central taskforce for natural disasters if necessary\n- reference : [Korea Centers for Disease Control](http:\/\/www.cdc.go.kr\/contents.es?mid=a20301020300)","278fcddf":"### Postscript\n    1. Thanks for investing your time and energy to read through this long report\n    2. Please don't hesitate, if you have any thoughts or insights to improve it\n    3. Stay safe and strong wherever whenever you are","5c26acff":"Result Analysis    \n    1. Most predictions in testing turn into 0, with improving the score\n    2. But this doesn't solve the problem of poor prediction itself\n    3. In conclusion, Naver-only model works best on the data up to now\n        - but it's not an actual forecasting yet\n        - as the model relies on the given search trends in future days\n        - which can't be accessible beforehand in practice (like the daily cases)\nWe better learn about and apply time series forecasting to use the case value itself as a training set","e917a15f":"Snap Analysis - floating population by date\n    1. Around 17 million people are expected to be active in Seoul daily\n        - the data provider, SKT, has 46.24% mobile service market share\n        - 8 millions * (100 \/ 46.24) = 17.30 millions\n    2. 2020-01-23 to 2020-01-28 showed lower numbers as it was the new year festival season\n        - many people visited their hometown outside of Seoul\n    3. What happened on 2020-02-23(spike) and 2020-03-11(dip)?","38948395":"Result Analysis - Prophet on one-step forecasting (daily cases)\n    1. Prophet works better than our best model (linear regression on Naver search + average + naive forecasting)\n        - Differences from truth: 7.3247 to 20.1383 (Prophet to benchmark)\n        - but this is a matter of timing than a stable performance\n    2. The shape is a lot smoother than the benchmark and truth (including some negative values)\n    3. What about using cumulative confirmed cases than daily new cases as target values?\n        - they show a clear logistic growth curve which seems more stable than daily up and down","f7ff969e":"Daily new cases","ea9288b1":"Data Description - snapshot on active population in Seoul (the capital of S.Korea)\n    1. 1,084,800 rows in sum\n    2. They show each day's floating population (7200 rows per day)\n    3. These are divided by coulumns of 'hour > birth_year > sex > city(subregion, in fact)'\n        - all 'province(city, in fact)' are Seoul","c36dccd9":"A true validation","5314048b":"<a id='eda'><\/a>\n# [[EDA: Exploratory Data Analysis]](#table)","d40bcf76":"Average weather - since 2020-01-20","c2a0ccf3":"Train \/ test with the mean of two best sizes","9c4f41f1":"Snap analysis - applying a time series forecasting to countries in the mid phase of spreading\n    1. The truth shows exponential increasing which matches the prediction\n        - but there are a lot more fluctuations around the peak in the truth \n    2. The time from the first case to the peak (pace of spreading) is similar too\n    3. Our model tells it would take about 1.5 months to get into the ending phase of 1st wave\nHow accurate this prediction would be on the truth for more than 1 month?","16fdccda":"Result Analysis - score by test set size (Google)\n    1. Training score: no clear tendency of improving with more training data\n        1) Which is counter-intuitivie at first (as we do expect more data drive better learning)\n        2) This is related to the shape of Google trends (a rough bell shape with recent fluctuations)\n        3) Only the early parts of it are similar to the truth\n            - the middle and latter parts are significantly different\n            - thus when the training set contains only the part before diverging, it gives the best score\n    2. Test score: too low to get a meaningful insight from\n        1) The model learns almost nothing with any size\n        2) The test score gets extremely worse when the test set size is less than 70%\nLet's check with Naver trends","b9853b27":"Result Analysis - Prophet on an artificial time series data (test set size = days_of_country)\n    1. The scale of predictions is about half to the truth\n    2. This might come from averaging all values to get a general trend (mean of all values are 14.90%)\n    3. What we better focus on now is the overrapped pattern of two graphs\n        - And they seem matched in general\nLet's zoom in the test phase","86bfa1ee":"Policy on public health","2f590ebd":"Snap Analysis - floating population by age\n    1. Age range starts\/ends with 20s\/70s unlike age_raw data (which starts\/ends with 0s\/80s)\n    2. It seems that 20s means <=29 and 70s does >=70 in seoul_raw\n    3. Let's validate with the resident population","9c694d7c":"Resident population by age (as of 2019-09)\n- applying 20s = 0s + 10s + 20s \/ 70s = 70s + 80s + 90s + older than 100\n- reference: [Seoul open data](https:\/\/data.seoul.go.kr\/dataList\/10718\/S\/2\/datasetView.do#none)","835a335f":"<a id='eda_region'><\/a>\n> # [[EDA]](#table) 2. Region","e5665899":"Snap Analysis - deceased rate (confirmed case-adjusted)\n    1. 80s show more than 2 times higher deceased rate than 70s\n    2. The deceased rates of 80s and 70s behgan decreasing at the end of May (more cases, less deceased)\n    3. The other groups have shown stable rates up to now","56ed44cf":"<a id='fc_basics'><\/a>\n> # [[FC]](#table) 1. LR \/ Naive \/ Average\n- LR: Linear Regression","78f873ce":"Result Analysis - Prophet with a custom season size (training set size)\n    1. The model learns the global shape of the training part\n    2. It seems merely to copy and paste the same shape for forecasting\n        - with weakening values by judging the trend is decreasing\n    3. This is a wrong assumption regarding that training set hasn't finished 1 cycle yet\nWe need at least 1 full cycle for training","626f8127":"Snap Analysis - confirmed cases by time (total)\n    1. Lower than 100 cases were for the first 1 month (from the 1st case on 2020-01-20 to 104th on 2020-02-20)\n    2. A steep linear growth happened from then for about 3 weeks (with passing 8000 cases on 2020-03-14)\n    3. The curve has been flattening up to 2020-05-08 (then began to go upward again)","37a1f863":"Cleanse and preprocess data","5038b5ad":"<a id='eda_path'><\/a>\n> # [[EDA]](#table) 5. Path","b631ea9a":"Snap Analysis - number of contacts by a patient\n    1. The size of logs is not enough to represent all patients (678 contacts logs among 4004 patients logs)\n    2. 50% have less than or equal to 4 | 75% have less than or equal to 14 contacts history\n    3. There are outliers with more than 200 contacts\n        - applying a stricter empirical method than statistical outlier rules [Q3 + 1.5*IQR]\n        - the outliers among these outliers have more than 1,000 contacts","049287fa":"- reference: [Google trends](https:\/\/trends.google.com\/trends\/explore?date=2020-01-20%202020-06-05&geo=DE&q=corona,coronavirus,covid19,covid) (2020-06-05)","41e632c2":"Policy distribution (by type \/ title \/ starting day \/ starting month)","a29e8aa7":"- As expected, there were only the spikes of seasonal flu and cold before COVID-19 outbreak","de520d8b":"### Papers\n1. [Impact of non-pharmaceutical interventions (NPIs) to reduce COVID-19 mortality and healthcare demand<br \/>by Neil M Ferguson et al<br \/>on 16 Mar 2020](https:\/\/www.imperial.ac.uk\/media\/imperial-college\/medicine\/sph\/ide\/gida-fellowships\/Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf)\n2. [Transmission of corona virus disease 2019 during the incubation period may lead to a quarantine loophole<br \/>by Wei Xia et al<br \/>on 08 Mar 2020](https:\/\/www.medrxiv.org\/content\/10.1101\/2020.03.06.20031955v1)\n3. [SARS-CoV-2 Infection among Travelers Returning from Wuhan, China<br \/>by Oon-Tek Ng et al<br \/>on 9 Apr 2020](https:\/\/www.nejm.org\/doi\/full\/10.1056\/NEJMc2003100)\n4. [Transmission of 2019-nCoV Infection from an Asymptomatic Contact in Germany<br \/>by Camilla Rothe et al<br \/>on 5 Mar 2020](https:\/\/www.nejm.org\/doi\/full\/10.1056\/NEJMc2001468)\n5. [Transmission characteristics of the COVID-19 outbreak in China: a study driven by data<br \/>by Meili Li et al<br \/>on 1 March 2020](https:\/\/www.medrxiv.org\/content\/10.1101\/2020.02.26.20028431v1.full.pdf)\n6. [The Future of the COVID-19 Pandemic: Lessons Learned from Pandemic Influenza<br \/>by Moore, Lipsitch, Barry, Osterholm - COVID-19: The CIDRAP Viewpoint<br \/>on 30 Apr 2020](https:\/\/www.cidrap.umn.edu\/sites\/default\/files\/public\/downloads\/cidrap-covid19-viewpoint-part1_0.pdf)\n7. [Forecasting at Scale<br \/>by Sean J. Taylor and Benjamin Letham<br \/>on 27 Sep 2017](https:\/\/peerj.com\/preprints\/3190.pdf)\n8. [Another look at measures of forecast accuracy<br \/>by Rob J Hyndman and Anne B Koehler<br \/>on 2 Nov 2005](https:\/\/robjhyndman.com\/papers\/mase.pdf)\n9. [Handbook of COVID-19 Prevention and Treatment<br \/>by the First Affiliated Hospital, Zhejiang University School of Medicine<br \/>on 18 Mar 2020](https:\/\/brandon-lighting.com\/wp-content\/uploads\/2020\/03\/Handbook_of_COVID_19_Prevention_en_Mobile.pdf)\n10. [Forecasting the impact 1 of the first wave of the COVID-19 pandemic on hospital demand and deaths for the USA and European Economic Area countries<br \/>by IHME COVID-19 health service utilization forecasting team<br \/>on 26 Apr 2020](https:\/\/www.medrxiv.org\/content\/10.1101\/2020.04.21.20074732v1)\n11. [Substantial undocumented infection facilitates the rapid dissemination of novel coronavirus (SARS-CoV2)<br \/>by Ruiyun Li et al<br \/>on 16 Mar 2020](https:\/\/science.sciencemag.org\/content\/sci\/early\/2020\/03\/13\/science.abb3221.full.pdf)\n12. [Nowcasting and forecasting the potential domestic and international spread of the 2019-nCoV outbreak originating in Wuhan, China: a modelling study<br \/>by Joseph T Wu, Kathy Leung, Gabriel M Leung<br \/>on 31 Jan 2020](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0140673620302609)\n13. [Guidelines for Accurate and Transparent Health Estimates Reporting: the GATHER statement<br \/>by Gretchen A Stevens et al<br \/>on 28 Jun 2016](http:\/\/gather-statement.org)\n14. [The GATHER Statement: Explanation and Elaboration<br \/>by The GATHER Working Group<br \/>on Jun 2016](http:\/\/retina-amd.org\/wp-content\/uploads\/2017\/11\/GATHER-EE-June-2016.pdf)\n15. [A scandal of invisibility: making everyone count by counting everyone<br \/>by Philip W Setel et al on behalf of the Monitoring of Vital Events (MoVE) writing group<br \/>on 29 Oct 2007](https:\/\/www.who.int\/healthinfo\/statistics\/WhoCounts1.pdf)\n16. [Ensemble modeling or selecting the best model: Many could be better than one<br \/>by S. V. Barai and Yoram Reich<br \/>in 1998](http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download;jsessionid=508E9C583A1B49B4C0937558FA9B57F6?doi=10.1.1.29.6244&rep=rep1&type=pdf)\n17. [Learning representations by back-propagating errors<br \/>by David E. Rumelhart, Geoffrey E. Hinton and Ronald J. Williams<br \/>on 09 Oct 1986](http:\/\/www.cs.toronto.edu\/~hinton\/absps\/naturebp.pdf)\n18. [Long Short-term Memory <br \/>by Sepp Hochreiter and J\u00fcrgen Schmidhuber <br \/>in Dec 1997](https:\/\/www.researchgate.net\/publication\/13853244_Long_Short-term_Memory)","9cc066bf":"Prophet on one-step forecasting (cumulative cases)","15779aa0":"Halving the (suspected) error values in the spike day","481956e5":"- reference: [COVID19 Global Forecasting (Week 5) challenge](https:\/\/www.kaggle.com\/c\/covid19-global-forecasting-week-5\/data)","97192cf7":"- Residual errors seem normally distributed, with 0 mean","2691da99":"Snap Analysis - population based confirmed rate\n    1. The older, the more prone to get infected in general\n    2. 50s's big portion shrinks because they are the largest age group\n    3. 20s are the outlier as they are midsize but the most infected group","6c6fc916":"<a id='table'><\/a>\n# [Table of Contents]\n1. [Insights from EDA](#eda_insights)\n2. [Lessons from forecasting](#fc_lessons)\n3. [EDA: Exploratory Data Analysis](#eda)\n    - [Age](#eda_age)\n    - [Region](#eda_region)\n    - [Sex](#eda_sex)\n    - [Test](#eda_test)\n    - [Path](#eda_path)\n    - [Weather](#eda_weather)\n    - [Patient](#eda_patient)\n    - [Seoul](#eda_seoul)\n    - [Search](#eda_search)\n    - [Policy](#eda_policy)\n4. [FC: Forecasting](#fc)\n    - [LR \/ Naive \/ Average](#fc_basics)\n    - [ARIMA](#fc_arima)\n    - Prophet\n        - [Basics](#fc_prophet_basics)\n        - [Space to time](#fc_prophet_fit)\n        - [Time to space](#fc_prophet_apply)\n5. [Resources](#resources)\n6. [Major updates](#updates)\n7. [Developing directions](#directions)"}}