{"cell_type":{"43a41e59":"code","898757e2":"code","b4d6781e":"code","4d6414ee":"code","b49fe20d":"code","c586a2eb":"code","5c983107":"code","7a0e0b65":"code","19ddb803":"code","b1d98f85":"code","25daa9c5":"code","3926aac6":"code","5e9097ac":"code","33da6ed7":"code","38edc906":"code","c9c91ae4":"code","9de4785d":"code","fd3cb093":"code","07926cea":"code","da704433":"code","fad1a086":"code","f91884b3":"code","4ff7a193":"code","dffa21da":"code","895b4ce7":"code","34cc01aa":"code","82dc70ea":"code","3382bd5c":"code","d96f8aa4":"code","75a24abc":"code","47bd61d8":"code","54db915e":"code","39012f7a":"code","47aa80fc":"code","0ecbf900":"code","2c584193":"code","72f9e0c1":"code","17e54bdd":"code","3df0d7a3":"code","ebbaa1f1":"code","f9040e1e":"code","12201920":"code","17a8e8cf":"code","d96865a6":"code","ff9f9351":"code","407da0f3":"code","9620150c":"code","dcbfba63":"code","97e2d8f4":"code","21872174":"code","569b4cee":"code","24b929d0":"code","5437b975":"code","126d66d0":"code","904a33b8":"code","47e78ec6":"code","cd40bfc8":"code","b5d84eaf":"code","f3d10563":"code","1bd1bde3":"code","6f37b8c7":"code","535fe0b1":"code","48a50ef5":"code","67b1ab69":"code","2e2efc68":"code","ccff843d":"code","abd7ec36":"code","947715d8":"code","1f60e373":"code","0ba874f2":"markdown","2d309ced":"markdown","aa217b91":"markdown","3c0cdae9":"markdown","5f5e1d87":"markdown","4de0bfbe":"markdown","3d5e4061":"markdown","c6316d54":"markdown","59660642":"markdown","1122d171":"markdown","1ff32032":"markdown","40330b21":"markdown","3f5fddc5":"markdown","2440e714":"markdown","0c451f25":"markdown","c8feb4a3":"markdown","3cac1677":"markdown","241a053a":"markdown","87dab0f1":"markdown","2ec497b5":"markdown","7116138b":"markdown","fa43d465":"markdown","0f4dc6ae":"markdown","f11abc20":"markdown","7593856c":"markdown","5abbe994":"markdown","f1e5c4e0":"markdown","da1a2e8d":"markdown","0f8165b4":"markdown","c2867e73":"markdown","4b7a8c05":"markdown","9afb4038":"markdown","1fcb5346":"markdown","463009c2":"markdown","41b15b8a":"markdown","65d03f88":"markdown","ce53cb6f":"markdown","8b7063e0":"markdown","7ecc708b":"markdown","049538ec":"markdown","41fa8434":"markdown","0a1d9f93":"markdown","ad902019":"markdown","f1728b7d":"markdown","b3757170":"markdown","50ce6353":"markdown","9bc5166f":"markdown"},"source":{"43a41e59":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\n#!pip install chart_studio\n#!pip install textstat\n\nimport numpy as np \nimport pandas as pd \n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n#import textstat\nimport random\n\n\n\n# Visualisation libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.express as px\n#import chart_studio.plotly as py\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\n\n\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n\n\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nfrom tqdm import tqdm\n\nimport os\n\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","898757e2":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","b4d6781e":"print(\"There are {} percentage of test data proportion compared to train data\".format(round(test.shape[0]\/train.shape[0]*100,2)))","4d6414ee":"# First few rows of the training dataset\ntrain.head()","b49fe20d":"# First few rows of the testing dataset\ntest.head()","c586a2eb":"train.info()","5c983107":"train.isnull().sum()","7a0e0b65":"train.dropna(inplace = True)","19ddb803":"test.info()","b1d98f85":"test.isnull().sum()","25daa9c5":"print('Positive tweet example :', train[train['sentiment'] == 'positive']['text'].values[1])\nprint()\nprint('Neutral tweet example :', train[train['sentiment'] == 'neutral']['text'].values[1])\nprint()\nprint('Negative tweet example :', train[train['sentiment'] == 'negative']['text'].values[1])","3926aac6":"train['sentiment'].value_counts()","5e9097ac":"train['sentiment'].value_counts(normalize = True)","33da6ed7":"test['sentiment'].value_counts(normalize = True)","38edc906":"def jaccard(str1,str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)\/ (len(a) + len(b) - len(c)))","c9c91ae4":"results_jaccard=[]\n\nfor ind,row in train.iterrows():\n    sentence1 = row.text\n    sentence2 = row.selected_text\n\n    jaccard_score = jaccard(sentence1,sentence2)\n    results_jaccard.append([sentence1,sentence2,jaccard_score])","9de4785d":"jaccard = pd.DataFrame(results_jaccard, columns = [\"text\", \"selected_text\", \"jaccard_score\"])","fd3cb093":"jaccard.head()","07926cea":"train = train.merge(jaccard, how = 'outer')\ntrain.head()","da704433":"train['Num_Word_ST'] = train['selected_text'].apply(lambda x: len(str(x).split()))","fad1a086":"train['Num_Word_Text'] = train['text'].apply(lambda x: len(str(x).split()))","f91884b3":"train['diff_in_word'] = train['Num_Word_Text'] - train['Num_Word_ST']","4ff7a193":"train.head()","dffa21da":"plt.figure(figsize = (8,6))\nplt.hist(train['Num_Word_ST'], bins = 30)\nplt.title('Distribution of Number Of words for selected_text');","895b4ce7":"plt.figure(figsize = (8,6))\nplt.hist(train['Num_Word_Text'], bins = 30)\nplt.title('Distribution of Number Of words for text');","34cc01aa":"plt.figure(figsize = (8,6))\nplt.hist(train['diff_in_word'], bins = 30)\nplt.title('Distribution of Number Of words for diff_in_word');","82dc70ea":"plt.figure(figsize = (8,6))\np = sns.kdeplot(train['Num_Word_ST'], shade = True, color = \"r\")\np = sns.kdeplot(train['Num_Word_Text'], shade = True, color = 'g')\nplt.title('Kernel Distribution of Number Of words');","3382bd5c":"plt.figure(figsize = (10,8))\nsns.kdeplot(train['diff_in_word'], shade = True, color = 'b', legend = False)\nplt.title('Kernel Distribution of diff_in_word');","d96f8aa4":"plt.figure(figsize = (10,8))\nsns.kdeplot(train[train['sentiment'] == 'positive']['diff_in_word'], shade = True, color = 'b', legend = False )\nplt.title('Kernel Distribution of diff_in_word for positive sentiment');","75a24abc":"plt.figure(figsize = (10,8))\nsns.kdeplot(train[train['sentiment'] == 'negative']['diff_in_word'], shade = True, color = 'r', legend = False )\nplt.title('Kernel Distribution of diff_in_word for negative sentiment');","47bd61d8":"plt.figure(figsize = (10,8))\nplt.hist(train[train['sentiment'] == 'neutral']['diff_in_word'], bins = 20)\nplt.title('Distribution of diff_in_word for neutral sentiment');","54db915e":"plt.figure(figsize = (10,8))\nsns.kdeplot(train[train['sentiment'] == 'positive']['jaccard_score'], shade = True, color = 'r', legend = False).set_title('Distribution of jaccard_score for positive sentiment');","39012f7a":"plt.figure(figsize = (10,8))\nsns.kdeplot(train[train['sentiment'] == 'negative']['jaccard_score'], shade = True, color = 'b', legend = False).set_title('Distribution of jaccard_score for negative sentiment');","47aa80fc":"plt.figure(figsize = (10,8))\nplt.hist(train[train['sentiment'] == 'neutral']['jaccard_score'], bins = 20)\nplt.title('Distribution of jaccard_score for neutral sentiment');","0ecbf900":"j = train[train['Num_Word_Text'] < 3]","2c584193":"train.head()","72f9e0c1":"j.groupby('sentiment').mean()['jaccard_score']","17e54bdd":"j[['text', 'selected_text']].head(15)","3df0d7a3":"stop_words = stopwords.words(\"english\")\nstop_words.extend(['im', 'u'])\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\ndef clean_text(text):\n    text = text.lower() #make text lowercase and fill na\n    text = re.sub('\\[.*?\\]', '', text) \n    text = re.sub('\\\\n', '',str(text))\n    text = re.sub(\"\\[\\[User.*\",'',str(text))\n    text = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(text))\n    text = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text) #remove hyperlinks\n    text = re.sub(r'\\:(.*?)\\:', '', text) #remove emoticones\n    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '', str(text)) #remove email\n    text = re.sub(r'(?<=@)\\w+', '', text) #remove @\n    text = re.sub(r'[0-9]+', '', text) #remove numbers\n    text = re.sub(\"[^A-Za-z0-9 ]\", '', text) #remove non alphanumeric like ['@', '#', '.', '(', ')']\n    text = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\\/:;=#@?\\[\\\\\\]^_`{|}~]*', '', text) #remove punctuations from sentences\n    text = re.sub('<.*?>+', '', str(text))\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', str(text))\n    text = re.sub('\\w*\\d\\w*', '', str(text))\n    text = tokenizer.tokenize(text)\n    text = [word for word in text if not word in stop_words]\n    final_text = ' '.join( [w for w in text if len(w)>1] ) #remove word with one letter\n    return final_text","ebbaa1f1":"train['text_clean'] = train['text'].apply(lambda x: clean_text(x))\ntrain['selected_text_clean'] = train['selected_text'].apply(lambda x : clean_text(x))","f9040e1e":"train[['text', 'text_clean']].head()","12201920":"train[['selected_text', 'selected_text_clean']].head()","17a8e8cf":"train['list'] = train['selected_text_clean'].apply(lambda x: str(x).split())\n\ntop = Counter([item for sublist in train['list'] for item in sublist])\n\nmostcommon = pd.DataFrame(top.most_common(20))\nmostcommon.columns = ['Common Word', 'Count']\nmostcommon.head(20)","d96865a6":"train['list'] = train['text_clean'].apply(lambda x: str(x).split())\n\n\ndef top(corpus, n = None):\n    top = Counter([item for sublist in corpus['list'] for item in sublist])\n    mostcommon = pd.DataFrame(top.most_common(20))\n    mostcommon.columns = ['Common Word', 'Count']\n    return mostcommon.head(20)\n\ntop(train)","ff9f9351":"positive = train[train['sentiment'] == 'positive']\nnegative = train[train['sentiment'] == 'negative']\nneutral = train[train['sentiment'] == 'neutral']","407da0f3":"print('The most common word in positive tweets')\ntop(positive)","9620150c":"print('The most common word in negative tweets')\ntop(negative)","dcbfba63":"print('The most common word in neutral tweets')\ntop(neutral)","97e2d8f4":"raw_text = [word for word_list in train['list'] for word in word_list]","21872174":"def words_unique(sentiment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'Neutral');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train[train.sentiment != sentiment]['list']:\n        for word in item:\n            allother .append(word)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train[train.sentiment == sentiment]['list']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","569b4cee":"Unique_Positive= words_unique('positive', 20, raw_text)\nprint(\"The top 20 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","24b929d0":"Unique_Negative= words_unique('negative', 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Reds')","5437b975":"Unique_Neutral= words_unique('neutral', 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Oranges')","126d66d0":"wc = WordCloud(stopwords = stop_words)\nplt.figure(figsize = (18,12))\nwc.generate(str(positive['text_clean']))\nplt.imshow(wc)\nplt.title('WordCloud of positive tweets');","904a33b8":"wc = WordCloud(stopwords = stop_words)\nplt.figure(figsize = (18,12))\nwc.generate(str(neutral['text_clean']))\nplt.imshow(wc)\nplt.title('WordCloud of neutral tweets');","47e78ec6":"wc = WordCloud(stopwords = stop_words)\nplt.figure(figsize = (18,12))\nwc.generate(str(negative['text_clean']))\nplt.imshow(wc)\nplt.title('WordCloud of negative tweets');","cd40bfc8":"def get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","b5d84eaf":"positive_bigram = get_top_n_gram(positive['text_clean'], (2,2), 20)\nnegative_bigram = get_top_n_gram(negative['text_clean'], (2,2), 20)\nneutral_bigram = get_top_n_gram(neutral['text_clean'], (2,2), 20)","f3d10563":"def process(corpus):\n    corpus = pd.DataFrame(corpus, columns= ['Text', 'count']).sort_values('count', ascending = True)\n    return corpus","1bd1bde3":"positive_bigram = process(positive_bigram)\nnegative_bigram = process(negative_bigram)\nneutral_bigram = process(neutral_bigram)","6f37b8c7":"plt.figure(figsize = (10,8))\nplt.barh(positive_bigram['Text'], positive_bigram['count'])\nplt.title('Top 20 Bigrams in positive text');","535fe0b1":"plt.figure(figsize = (10,8))\nplt.barh(negative_bigram['Text'], negative_bigram['count'])\nplt.title('Top 20 Bigrams in negative text');","48a50ef5":"plt.figure(figsize = (10,8))\nplt.barh(neutral_bigram['Text'], neutral_bigram['count'])\nplt.title('Top 20 Bigrams in neutral text');","67b1ab69":"positive_trigram = get_top_n_gram(positive['text_clean'], (3,3), 20)\nnegative_trigram = get_top_n_gram(negative['text_clean'], (3,3), 20)\nneutral_trigram = get_top_n_gram(neutral['text_clean'], (3,3), 20)","2e2efc68":"positive_trigram = process(positive_trigram)\nnegative_trigram = process(negative_trigram)\nneutral_trigram = process(neutral_trigram)","ccff843d":"plt.figure(figsize = (10,8))\nplt.barh(positive_trigram['Text'], positive_trigram['count'])\nplt.title('Top 20 Trigrams in positive text');","abd7ec36":"plt.figure(figsize = (10,8))\nplt.barh(negative_trigram['Text'], negative_trigram['count'])\nplt.title('Top 20 Trigrams in negative text');","947715d8":"plt.figure(figsize = (10,8))\nplt.barh(neutral_trigram['Text'], neutral_trigram['count'])\nplt.title('Top 20 Trigrams in neutral text');","1f60e373":"%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/XaQ0CBlQ4cY\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>","0ba874f2":"<a id=\"6\"><\/a> <br>\n## Conclusion Of EDA\n\nWe can see from the jaccard score plot that there is peak for negative and positive plot around score of 0,1 and 1 .That means there is a cluster of tweets where :\n- there is a high similarity between text and selected texts\n- there is a weak similarity between text and selected texti\n\nIf we can find those clusters then we can predict text for selected texts for those tweets irrespective of segment\nLet's see if we can find those clusters,one interesting idea would be to check tweets which have number of words less than 3 in text, because there the text might be completely used as text","2d309ced":"- \"Day\" was always a part among all sentiment tweets and thus confirmed the event.Well,\"Happy Mothers day\".All these tweets were collected around May 2nd Week.\n- Positive bigrams and trigrams words are more biased towards mothers day.\n- Negative ngrams are displaying negative emotional words.\n- Neutral shows common words.Nothing much inference from this sentiment.","aa217b91":"Let's see the distribution of the jaccard score accros each sentiment :","3c0cdae9":"**Length of words in text and selected** ","5f5e1d87":"**Objective**\n\nSentiment analysis is a common use case of NLP where the idea is to classify the tweet as positive, negative or neutral depending upon the text in the tweet. This problem goes a way ahead and expects us to also determine the words in the tweet which decide the polarity of the tweet.\n\n1. [Import packages](#1)\n1. [Load data](#2)\n1. [Data Exploration](#3)\n    - Analysis of the Sentiment Column\n    - Distribution of the Sentiment Column\n    - What do we currently Know About our Data\n1. [Generating Meta-Features](#4)\n    - Jaccard Similarity Scores between text and Selected_text\n    - Number of words in selected text and main text\n    - Length of words in text and selected\n    - Difference In Number Of words of Selected_text and Text\n1. [Exploration on the meta-features](#5)\n1. [Conclusion Of EDA](#6)\n1. [Text Data Preprocessing](#7)\n1. [Most common words](#8)\n    - Most Common words in our Target-Selected Text\n    - Most Common words in Text\n    - Most common word for each sentiments\n    - Unique word for each sentiments\n1. [It's Time For WordClouds](#10)\n    - Dimensionality Reduction with t-SNE (Doc2Vec)\n    - Dimensionality Reduction with PCA (Doc2Vec)\n1. [Ngram exploration](#11)\n    - Distribution of bigram\n    - Distribution of trigram\n1. [Extracting the part of the tweet that reflects the sentiment : Resources](#12)\n    - Question-Answering\n    - Method with Pytorch and BERT","4de0bfbe":"It would be a good idea to see the most common word for each sentiments :","3d5e4061":"I was not able to plot kde of jaccard_scores of neutral tweets for the same reason,thus I will plot a distribution plot","c6316d54":"Let's see the result :","59660642":"It'll be better if we could get a relative percentage instead of the count. It is very simple with value_counts and can be achieved with a minor modification in the above code. ","1122d171":"The columns denote the following:\n\n- The textID of a tweet\n- The text of a tweet\n- The selected text which determines the polarity of the tweet\n- sentiment of the tweet\n\nThe test dataset doesn't have the selected text column which needs to be identified.","1ff32032":"**Number of words in selected text and main text**","40330b21":"<a id=\"4\"><\/a> <br>\n##  Generating Meta-Features","3f5fddc5":"- Positive produces words like good,happy,thanks which is quite expected\n- Negative produces words like miss,sad,sorry,bad which indicates negative emotions\n- Neutral produces words like get,going,work which is sort of common words\n- We could see \"day\" comes in all three sentiments.(These tweets might be indicating a event day.Hopefully we may come to know after competition)","2440e714":"<a id=\"11\"><\/a> <br>\n## Ngram exploration\nTo analyse more in depth the text column we will be extracting the N-Gram features.\n\nN-grams are used to describe the number of words used as observation points, e.g., unigram means singly-worded, bigram means 2-worded phrase, and trigram means 3-worded phrase. Here is a nice way to understand this:\n\n![](https:\/\/i.stack.imgur.com\/8ARA1.png)\n\nSource: https:\/\/stackoverflow.com\/questions\/18193253\/what-exactly-is-an-n-gram\n\nWe have already see the unigram above so let's go straight to bigram.\n\n**Distribution of bigram**","0c451f25":"Before we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Let's create a function which will perform the following tasks on the text columns:\n\n- Make text lowercase,\n- removes hyperlinks,\n- remove punctuation\n- removes numbers\n- tokenizes\n- removes stopwords","c8feb4a3":"<a id=\"12\"><\/a> <br>\n## Extracting the part of the tweet that reflects the sentiment : Resources\n\nThere are a number of ways in which the problem can be approached and a lot of good solutions have been provided as notebooks which I would like to point out :\n\n### A) Question-Answering \n\nHere's some solution with in mind the extractive question answering problem :\n\n- Here is a solution by [Cheongwoong Kang](https:\/\/www.kaggle.com\/cheongwoongkang) provided in his notebook titled: [**DistilBERT-QA Starter (+ cross-validation)**](https:\/\/www.kaggle.com\/cheongwoongkang\/distilbert-qa-starter-cross-validation)\n\n- Here is another solution by  [Jonathan Besomi](https:\/\/www.kaggle.com\/jonathanbesomi) provided in his notebook titled: [**Question-Answering Starter pack**](https:\/\/www.kaggle.com\/jonathanbesomi\/question-answering-starter-pack\/output)\n\n\n\n### B) Method with Pytorch and BERT\n\n[This](https:\/\/www.kaggle.com\/abhishek\/bert-base-uncased-using-pytorch) is kernel by *Abhishek Thakur* which his based on his  videos on youtube. If you want to understand more, be sure to check them out.","3cac1677":"**distribution of trigram**","241a053a":"Let's see the result :","87dab0f1":"**Difference In Number Of words of Selected_text and Text**","2ec497b5":"By Looking at the Unique Words of each sentiment,we now have much more clarity about the data,these unique words are very strong determiners of Sentiment of tweets","7116138b":"We can see some interesting trends here:\n\n- Positive and negative tweets have high kurtosis (right skewed) and thus values are concentrated in two regions narrow and high density\n- Neutral tweets have a low kurtosis value and their is bump in density near values of 1\n\nFor those who don't know :\n\nKurtosis is the measure of how peaked a distribution is and how much spread it is around that peak\nSkewness measures how much a curve deviates from a normal distribution","fa43d465":"It's time to clean our corpus \n\n<a id=\"7\"><\/a> <br>\n## Text Data Preprocessing","0f4dc6ae":"**Jaccard Similarity Scores between text and Selected_text**","f11abc20":"That's kind of right skewed too !","7593856c":"This is a better representation. About 40 percent of the tweets are neutral followed by positive and negative tweets.\n\nIt would be interesting to see if the distribution is also same in the test set.","5abbe994":"<a id=\"3\"><\/a> <br>\n## Data Exploration","f1e5c4e0":"**What do we currently Know About our Data:**\n\nBefore starting let's look at some things that we already know about the data and will help us in gaining more new insights:\n\n- We Know that selected_text is a subset of text\n- We know that selected_text contains only one segment of text,i.e,It does not jump between two sentences.For Eg:- If text is 'Spent the entire morning in a meeting w\/ a vendor, and my boss was not happy w\/ them. Lots of fun. I had other plans for my morning' The selected text can be 'my boss was not happy w\/ them. Lots of fun' or 'Lots of fun' but cannot be 'Morning,vendor and my boss,\n- Thanks to this discussion:https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/138520 We know that neutral tweets have a jaccard similarity of 97 percent between text and selected_text","da1a2e8d":"**Unique word for each sentiments**","0f8165b4":"<a id=\"5\"><\/a> <br>\n## Exploration on the meta-features","c2867e73":"Just one NULL value in text and selected_text\/","4b7a8c05":"I think it will be more useful to see this for diff_in_word :","9afb4038":"The Jaccard score are high, so we can see that there is similarity between text (<= 2) and selected text so our intuition seems right. Let's have a closer look :","1fcb5346":"**Most Common words in Text**","463009c2":"They looklike the same. I was not able to plot kde plot for neutral tweets because most of the values for difference in number of words were zero. \nThus I will plot a distribution plot","41b15b8a":"I saw some notebook used them for their models :\n - Number of words in selected text and main text \n - Length of words in text and selected \n - Difference In Number Of words of Selected_text and Text\n - Jaccard Similarity Scores between text and Selected_text","65d03f88":"So we can see the Most common words in Selected text and Text are almost the same,which was obvious\n\n**Most common word for each sentiments**","ce53cb6f":"<a id=\"10\"><\/a> <br>\n## It's Time For WordClouds\n\nWe will be building wordclouds in the following order:\n\n- WordCloud of Neutral Tweets\n- WordCloud of Positive Tweets\n- WordCloud of Negative Tweets","8b7063e0":"<a id=\"2\"><\/a> <br>\n## Load Data","7ecc708b":"<a id=\"8\"><\/a> <br>\n## Most common words\n\n**Most Common words in our Target-Selected Text**","049538ec":"It's seemes clear that most of the times , text is used as selected text.\nWe can improve this by preprocessing the text which have word length less than 3. We will remember this information and use it in our model building.","41fa8434":"**Analysis of the Sentiment Column**\n\nNow, let's analyse and see how the Sentiment column looks like. I have only used the training dataset but the process will remain the same if we wish to to do it for the test dataset as well.","0a1d9f93":"<a id=\"1\"><\/a> <br>\n## Import packages","ad902019":"Obviously most of the case are between 0 and 3, because selected text is an abstrac of text so..","f1728b7d":"The number of words plot is really interesting ,the tweets having number of words greater than 25 are very less and thus the number of words distribution plot is right skewed","b3757170":"So We have 27486 tweets in the train set and 3535 tweets in the test set","50ce6353":"Let's see diff_in_word but by sentiment :","9bc5166f":"**Distribution of the Sentiment Column**"}}