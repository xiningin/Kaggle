{"cell_type":{"ec75592b":"code","f3be9696":"code","391124d5":"code","dac24008":"code","4215df24":"code","deaf01eb":"code","993e2f36":"code","249bdc11":"code","3ba76277":"code","7ab66f8a":"code","ebfb27f7":"code","f1e315c6":"code","f3119bb2":"code","1b9a1ad1":"markdown","8f8e77df":"markdown","8d7373b8":"markdown","166e5c94":"markdown","6956ea74":"markdown","d035a253":"markdown","f2c316c3":"markdown","b3c291a8":"markdown","cb607d69":"markdown","cf13936f":"markdown","b865804e":"markdown","df7362a4":"markdown","0af72ee8":"markdown","5e7b2511":"markdown","dbc19c8e":"markdown","4c6c4dbc":"markdown"},"source":{"ec75592b":"\"\"\"\nCopyright 2020 Stanislav Dereka\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\"\"\"","f3be9696":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pylab\npylab.rcParams['figure.figsize'] = (20, 5)\nfrom sklearn.linear_model import LinearRegression\nfrom itertools import combinations\nfrom seaborn import distplot\nimport seaborn as sns\nsns.set()\n\nbatches_train = [\n    slice(100000*0, 100000*5),\n    slice(100000*5, 100000*10),\n    slice(100000*10, 100000*15),\n    slice(100000*15, 100000*20),\n    slice(100000*20, 100000*25),\n    slice(100000*25, 100000*30),\n    slice(100000*30, 100000*35),\n    slice(100000*35, 100000*40),\n    slice(100000*40, 100000*45),\n    slice(100000*45, 100000*50),\n]\n\ngroups_train = [0, 0, 1, 2, 3, 4, 1, 2, 4, 3]\n\nbatches_test = [\n    slice(100000*0, 100000*1),\n    slice(100000*1, 100000*2),\n    slice(100000*2, 100000*3),\n    slice(100000*3, 100000*4),\n    slice(100000*4, 100000*5),\n    slice(100000*5, 100000*6),\n    slice(100000*6, 100000*7),\n    slice(100000*7, 100000*8),\n    slice(100000*8, 100000*9),\n    slice(100000*9, 100000*10),\n    slice(100000*10, 100000*15),\n    slice(100000*15, 100000*20),\n]\n\ngroups_test = [0, 2, 4, 5, 1, 3, 4, 3, 5, 2, 5, 5]\n\ndef label_batches_and_groups(data, batches, groups):\n    data.loc[:, 'batch'] = np.empty(len(data), dtype=np.int)\n    data.loc[:, 'group'] = np.empty(len(data), dtype=np.int)\n\n    for i in range(len(batches)):\n        b = batches[i]\n        g = groups[i]\n        data.loc[b, 'batch'] = i\n        data.loc[b, 'group'] = g\n\ntest = pd.read_csv(\"..\/input\/data-without-drift\/test_clean.csv\")\ntrain = pd.read_csv(\"..\/input\/data-without-drift\/train_clean.csv\")\nlabel_batches_and_groups(train, batches_train, groups_train)\nlabel_batches_and_groups(test, batches_test, groups_test)\n\nres = 1000\nplt.figure(figsize=(20, 5))\nplt.plot(train.time.values[::res], train.signal.values[::res])\nplt.plot(train.time.values[::res], train.open_channels.values[::res])\nfor b in batches_train:\n    t = train.time.values[b]\n    plt.plot([t[0],t[0]],[-10,8],'g')\n    plt.text(t[0]+4,6,str(batches_train.index(b)),size=13)\nplt.show()\n\nres = 1\nplt.plot(test.time.values[::res], test.signal.values[::res])\nfor b in batches_test:\n    t = test.time.values[b]\n    plt.plot([t[0],t[0]],[-10,8],'g')\n    plt.text(t[0]+4,6,str(batches_test.index(b)),size=13)\nplt.show()","391124d5":"corrupted = slice(3_640_000, 3_840_000)\nhealthy = slice(1_500_000, 1_700_000)\n\ncleaned = train.drop(train[corrupted].index)\nsignal = cleaned[cleaned.group != 3].signal.values\nchannels = cleaned[cleaned.group != 3].open_channels.values\n\n\n# https:\/\/www.kaggle.com\/kakoimasataka\/remove-pick-up-electric-noise\nc = 6\nlabel = np.arange(len(signal))\n\nchannel_list = np.arange(c)\nn_list = np.empty(c)\nmean_list = np.empty(c)\nstd_list = np.empty(c)\nstderr_list = np.empty(c)\n\nfor i in range(c):\n    x = label[channels == i]\n    y = signal[channels == i]\n    n_list[i] = np.size(y)\n    mean_list[i] = np.mean(y)\n    std_list[i] = np.std(y)\n\nstderr_list = std_list \/ np.sqrt(n_list)\nplt.show()\n\nw = 1 \/ stderr_list\nchannel_list = channel_list.reshape(-1, 1)\nlinreg_m = LinearRegression()\nlinreg_m.fit(channel_list, mean_list, sample_weight=w)\n\nmean_predict = linreg_m.predict(np.arange(0, 11).reshape(-1, 1))\n\nx = np.linspace(-0.5, 10, 5)\ny = linreg_m.predict(x.reshape(-1, 1))\nplt.figure(figsize = (10, 10))\nplt.plot(x, y, label=\"regression\")\nplt.plot(channel_list, mean_list, \".\", markersize=8, label=\"original\")\nplt.legend()\nplt.show()\n\nprint(\"mean:\", mean_predict)","dac24008":"noise = train[train.batch == 3].signal.values - mean_predict[train[train.batch == 3].open_channels.values]\n\n# https:\/\/www.kaggle.com\/kakoimasataka\/remove-pick-up-electric-noise\nfs=10000.\nfig, ax = plt.subplots(nrows=1, ncols=1)\nfig.subplots_adjust(hspace = .5)\n\nfft = np.fft.fft(noise)\npsd = np.abs(fft) ** 2\nfftfreq = np.fft.fftfreq(len(psd),1\/fs)\n\ni = abs(fftfreq) < 200\nax.grid()\nax.plot(fftfreq[i], 20*np.log10(psd[i]), linewidth=.5)\nax.set_xlabel('Frequency (Hz)') \nax.set_ylabel('PSD (dB)')\nplt.show()","4215df24":"healthy_noise = train[healthy].signal.values - mean_predict[train[healthy].open_channels.values]\nfixed = mean_predict[train[corrupted].open_channels.values] + healthy_noise\ntrain.loc[train[corrupted].index, 'signal'] = fixed","deaf01eb":"def compose(data_1, data_2, means, noise_factor):\n    ch_1 = data_1.open_channels.values\n    ch_2 = data_2.open_channels.values\n    comp_label = ch_1 + ch_2\n\n    noise_1 = data_1.signal.values - means[ch_1]\n    noise_2 = data_2.signal.values - means[ch_2]\n    noise = (noise_1 + noise_2) \/ noise_factor\n\n    comp = means[comp_label] + noise\n    return comp, comp_label\n\n\ndef combinatorial_synthesis(data, n, flip, **params):\n    assert len(data) % n == 0\n    l_s = len(data) \/\/ n\n    comb = combinations(list(range(n)), 2)\n    for i, j in comb:\n        sig, ch = compose(data[i*l_s:(i+1)*l_s], data[j*l_s:(j+1)*l_s], **params)\n        yield sig, ch\n        if flip:\n            sig, ch = compose(data[i * l_s:(i + 1) * l_s], data[j * l_s:(j + 1) * l_s][::-1], **params)\n            yield sig, ch\n\n\ndef append_dataset(data, signal, channels, group):\n    t_0 = data.time.values[-1]\n    b = data.batch.values[-1]\n    tau = 0.0001\n    time = np.arange(t_0 + tau, t_0 + tau * (len(signal) + 1), tau)\n    new = pd.DataFrame()\n    new['time'] = time\n    new['signal'] = signal\n    new['open_channels'] = channels\n    new['batch'] = b + 1\n    new['group'] = group\n    return pd.concat([data, new], ignore_index=True, axis=0)","993e2f36":"cs1 = combinatorial_synthesis(train[train.group == 0], 4, flip=False, means=mean_predict, noise_factor=2 ** 0.5)\nfor sig, ch in cs1:\n    train = append_dataset(train, sig, ch, 5)\n\nplt.plot(test[test.group == 5].signal.values[:10000])\nplt.title('Group 5 original')\nplt.show()\n\nplt.plot(train[train.group == 5].signal.values[-10000:])\nplt.title('Group 5 synthetic')\nplt.show()\n    \ndistplot(train[train.group == 5].signal.values, label='Group 5 synthetic')\ndistplot(test[test.group == 5].signal.values, label='Group 5 original')\nplt.legend()\nplt.show()","249bdc11":"cs2 = combinatorial_synthesis(train[train.group == 4], 10, flip=False, means=mean_predict, noise_factor=1.0)\nfor sig, ch in cs2:\n    train = append_dataset(train, sig, ch, 3)\n\nnew = train.batch >= len(batches_train)\nmean_new = train[new & (train.group == 3)].signal.mean()\n\nto_be_fixed = (train.batch == 4) | (train.batch == 9)\n\nfor b in [4, 9]:\n    train.loc[train.batch == b, 'signal'] = train[train.batch == b].signal.values - train[\n        train.batch == b].signal.values.mean() + mean_new\n\nfor b in [5, 7]:\n    test.loc[test.batch == b, 'signal'] = test[test.batch == b].signal.values - test[\n        test.batch == b].signal.values.mean() + mean_new\n\nplt.plot(train[train.group == 3].signal.values[:10000])\nplt.title('Group 3 original')\nplt.show()\n\nplt.plot(train[train.group == 3].signal.values[-10000:])\nplt.title('Group 3 synthetic')\nplt.show()\n\ndistplot(train[10_000_000:].signal.values, label='Group 3 original')\ndistplot(test[test.group == 3].signal.values, label='Group 3 synthetic')\nplt.legend()\nplt.show()","3ba76277":"noise = train[train.batch == 28].signal.values - mean_predict[train[train.batch == 28].open_channels.values]\n\n# https:\/\/www.kaggle.com\/kakoimasataka\/remove-pick-up-electric-noise\nfs=10000.\nfig, ax = plt.subplots(nrows=1, ncols=1)\nfig.subplots_adjust(hspace = .5)\n\nfft = np.fft.fft(noise)\npsd = np.abs(fft) ** 2\nfftfreq = np.fft.fftfreq(len(psd),1\/fs)\n\ni = abs(fftfreq) < 200\nax.grid()\nax.plot(fftfreq[i], 20*np.log10(psd[i]), linewidth=.5)\nax.set_xlabel('Frequency (Hz)') \nax.set_ylabel('PSD (dB)')\nplt.show()","7ab66f8a":"train.signal.plot()\nplt.title('Train')\nplt.show()\n\ntest.signal.plot()\nplt.title('Test')\nplt.show()\n\ntrain.to_csv(\"train_synthetic.csv\", index=False, float_format='%.4f')\ntest.to_csv(\"test_synthetic.csv\", index=False, float_format='%.4f')","ebfb27f7":"def rescale_noise(data, means, scale_factor):\n    sig = data.signal.values\n    ch = data.open_channels.values\n    noise = sig - means[ch]\n    noise *= scale_factor\n    sig_ = noise + means[ch]\n    return sig_\n\n\ndef reduce_channels(data, res, means):\n    residual = res.open_channels.values\n    reduced_sig = data.signal.values - (means - means[0])[residual]\n    return reduced_sig, residual","f1e315c6":"reduced_sig, residual = reduce_channels(train[train.batch == 35], train[train.group == 4][500_000:600_000], mean_predict)\nsig = rescale_noise(train[train.group == 4], mean_predict, 2 ** 0.5)","f3119bb2":"distplot(sig, label='group 4 signal with rescaled noise')\ndistplot(reduced_sig, label='group 3 reduced signal')\nplt.legend()\nplt.show()","1b9a1ad1":"# Inverting synthesis. Sealing opened channels. Channel reduction\n\nAnd the last spell. There is an operation inverse to composition. I call it reduction. It corresponds to a case when we have imaginary fictive \"channels\", acting in the opposite direction. I found it in the last two days of the competition. After studing the question of code sharing (it is not recommended to share in the last week) I decided not to share.","8f8e77df":"## Simulating unknown test group\nLet us try to make a signal, which is close in its properties to unknown signal (group 5) from the test data. This signal seems to be a superposition of several low open probability channels (group 0).","8d7373b8":"# Creating new data. Synthesis\n\nEvery single ion channel can be desribed as hidden Markov process. It is a \"brick\" of ion switching data. The signal from many channels acting together is a superposition of such bricks. Having understood the property of superposition we can produce more data using only existing training dataset. All one need to do is just sum up the labels from different segments and do the same trick with the noise.","166e5c94":"# Loading data and labeling batches","6956ea74":"# Conclusion\nI hope you enjoyed this kernel. I have no idea if some of these methods are of use in electrophysiology. But they could definitely help researchers who work with synthetic data as it reveals the way of how such kind of data can be leaked.\n\n**P.S. I am not a native English speaker, so if you found a mistake, please, let me know and I will correct it immediately.**","d035a253":"# Augmented dataset\n\nThe output files of this kernel are two .csv with augmented dataset.","f2c316c3":"## More group 3\nBy the analogous reasoning group 3 is a superposition of two segments of group 4. It is the most complicated data in the competition so it is extremely important to augment it properly. Let us create more group 3.","b3c291a8":"The data has been successfully produced.","cb607d69":"# Healing batch 7\nWith extracted noise, corrupted noise from 7th batch can be easily replaced by healthy noise from any of group 2 segments.","cf13936f":"In this kernel I explain my preprocessing strategy for this competition. It allowed me to diversify the initial dataset without using any external data. There is much more data than it seems at first glance. **Note: it is not my full solution, just a preprocessing. The post with all the details is [here](https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/153991#862688). In one week I will disclose my repo on GitHub with full code, so everybody can reproduce the results.**","b865804e":"Indeed, now the distributions look very similar, but classes 3 and 4 are still not represented. It is better then nothing, though.","df7362a4":"## But what happened with the noise?\nI want to make sure that Fourier image of the noise has not changed after synthesis.","0af72ee8":"Indeed, the nature of the noise rest untouched when using superposition synthesis. In this way, synthetic data is similar to the original data. It is a powerful augmentation, which can drastically diversify dataset, what is essential for NN-based models I used in the competition.","5e7b2511":"# Extracting and analysing noise\n\nWith precalculated means we are able to extract noise from every segment of data. Let us analyse it with Fourier transform.","dbc19c8e":"Noise has a 50 Hz component, which takes origin (probably) from electronic devices used for measurements in the laboratory. Noise is the only natural part of the data, so it should be treated with caution. I decided not to remove 50 Hz component as it didn't affect NN-based models CV score. But we need to consider the nature of the noise when doing any data augmentation.","4c6c4dbc":"# Calculating mean signal values for each number of opened channels\n\nHere i don't use corrupted data in train batch 7. Predicted mean values of channels 6-10 will be of use in the section dedicated to data synthesis."}}