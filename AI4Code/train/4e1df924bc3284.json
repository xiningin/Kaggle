{"cell_type":{"f8d234f9":"code","53f2283f":"code","f8bd5f20":"code","1c049342":"code","b4b5b10c":"code","6591ac81":"code","dac561e9":"code","d5270814":"code","3e30d647":"code","f2ddaf64":"code","cdf653f3":"code","e3c88d1a":"code","7aba7181":"code","0b6e8a52":"code","c66ab8f3":"code","e68a6667":"code","3449cbd9":"code","aab46d6b":"code","595e836e":"code","cc52c850":"code","49ee0e02":"code","77efda01":"code","7bfa4c1e":"code","cb1b82b2":"code","50d3cc93":"code","dce40c53":"code","5685c467":"markdown","dbc1b2e2":"markdown","b18d2c0c":"markdown","72f64389":"markdown","d21f90b8":"markdown"},"source":{"f8d234f9":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.preprocessing import QuantileTransformer\nimport tensorflow_addons as tfa\nfrom sklearn.metrics import log_loss\n\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","53f2283f":"X_train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ny_train = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\nX_test = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","f8bd5f20":"# RankGauss\n# \u6570\u5024\u5909\u6570\u3092\u9806\u4f4d\u306b\u5909\u63db\u3057\u305f\u3042\u3068\u9806\u4f4d\u3092\u4fdd\u3063\u305f\u307e\u307e\u534a\u3070\u7121\u7406\u3084\u308a\u6b63\u898f\u5206\u5e03\u306b\u306a\u308b\u3088\u3046\u306b\u5909\u63db\u3059\u308b\u624b\u6cd5\ndef rank_gauss(df):\n    for col in df.columns:\n        transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\n        vec_len = len(df[col].values)\n        raw_vec = df[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n\n        # \u5909\u63db\u5f8c\u306e\u30c7\u30fc\u30bf\u3067\u5404\u5217\u3092\u7f6e\u63db\n        df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n\n    return df\n","1c049342":"def preprocess(df):\n    df = df.copy()\n    # \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092\u6570\u5024\u5909\u63db\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    \n    df = rank_gauss(df)\n    return df","b4b5b10c":"# cp_type == 0 \u306e\u307f\u5229\u7528\n# \u3042\u308f\u305b\u3066y\u3082\u540c\u3058\u3088\u3046\u306b\u843d\u3068\u3059\ny_train = y_train.loc[X_train['cp_type']=='trt_cp'].reset_index(drop=True)\nX_train = X_train.loc[X_train['cp_type']=='trt_cp'].reset_index(drop=True)\n\ntrain = preprocess(X_train)\ntest = preprocess(X_test)\ndel y_train['sig_id']","6591ac81":"def create_model(num_columns):\n    model = keras.models.Sequential([\n        keras.layers.Input(num_columns),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(rate=0.2),\n        tfa.layers.WeightNormalization(keras.layers.Dense(2048, activation='relu')),\n        \n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(rate=0.2),\n        tfa.layers.WeightNormalization(keras.layers.Dense(1024, activation='relu')),\n        \n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(rate=0.5),\n        tfa.layers.WeightNormalization(keras.layers.Dense(512, activation='relu')),                            \n        \n        # == final layer == \n         keras.layers.BatchNormalization(),\n         keras.layers.Dropout(rate=0.5),\n         tfa.layers.WeightNormalization(keras.layers.Dense(206, activation='sigmoid'))\n    ])\n    model.compile(\n        optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(),sync_period=10),\n        loss='binary_crossentropy'\n    )\n\n    return model","dac561e9":"top_feats = [i for i in range(train.shape[1])]\n# len(train.columns) \u3068\u540c\u3058","d5270814":"# \u73fe\u5728\u306e\u30a8\u30dd\u30c3\u30af\u3092\u5f15\u6570\u3068\u3057\u3066\u5b66\u7fd2\u7387\u3092\u8fd4\u3059\u95a2\u6570\u3092\u5b9a\u7fa9\n# \u305d\u308c\u307e\u3067\u306e\u5b66\u7fd2\u6563\u308b\u306b0.1 ** (epoch\/20)\u3092\u304b\u3051\u3066\u3044\u3066\u5b66\u7fd2\u7387\u306f\u6307\u6570\u95a2\u6570\u7684\u306a\u6e1b\u8870\u3092\u3059\u308b\n# keras.optimizer.schedules \u3092\u4f7f\u3046\u65b9\u6cd5\u3082\u3042\u308b\n\ndef exponential_delay_fn(epoch):\n    return 0.01 * 0.1 ** (epoch\/20)","3e30d647":"N_STARTS = 7\nK_FOLD = 7\nBATCH_NUM = 128\nEPOCH_NUM = 35\ntf.random.set_seed(1)\nss.loc[:, y_train.columns] = 0\nres = y_train.copy()\nres.loc[:, y_train.columns] = 0\n\nhistorys = dict()\n\n# N_STARS * KFOLD\u3000\u56de\u5b66\u7fd2\u3059\u308b\n# MultilabelStratifielsKFold \u306f\u30e9\u30f3\u30c0\u30e0\u30b7\u30e3\u30c3\u30d5\u30eb\u3092\u3057\u3066\u304f\u308c\u308bgroup-k-fold \u306e\u3088\u3046\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(\n        MultilabelStratifiedKFold(n_splits=K_FOLD,\n                                  random_state=seed, shuffle=True).split(train, train)):\n        print(f\"--{train.values[tr].shape}--{train.values[te].shape}--\")\n        print(f\"Seed: {seed}, Fold: {n}\")\n        \n        # \u30e2\u30c7\u30eb\u4f5c\u6210\n        model = create_model(len(top_feats))\n        \n        # === \u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u95a2\u6570\u306e\u8a2d\u5b9a ===\n        checkpoint_path = f'repeated:{seed}_Fold:{n}.hdf5'\n        \n        # \u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u95a2\u6570\u3092\u5f15\u6570\u3068\u3057\u3066LearningRateScheduler \u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3092\u4f5c\u308a\u305d\u306e\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3092fit()\u30e1\u30bd\u30c3\u30c9\u306b\u6e21\u3059\n        # model.fit(...., callback=[exponential_delay_fn(epoch)])\u306e\u3088\u3046\u306b\u4f7f\u3046\n\n        lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_delay_fn)\n\n        # \u30d9\u30b9\u30c8\u306a\u30e2\u30c7\u30eb\u3092checkpoint_path \u306b\u4fdd\u5b58\u3057\u3066\u304a\u3044\u3066\u304f\u308c\u308b\u8a2d\u5b9a\n        checkpoint_cb = keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True)\n        # early_stopping \u306e\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u95a2\u6570\n        # val_loss \u3092\u76e3\u8996\n        # mode\u306f\u4e0a\u66f8\u304d\u3059\u308b\u3068\u304d\u306e\u8a2d\u5b9a\u3001\u57fa\u672c\u7684\u306bauto\u306b\u3057\u3066\u304a\u3051\u3070OK\n        early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n        \n        # ===========================\n        \n        history = model.fit(train.values[tr][:, top_feats],\n                  y_train.values[tr],\n                  validation_data=(train.values[te][:, top_feats], y_train.values[te]),\n                  epochs=EPOCH_NUM, \n                  batch_size=BATCH_NUM, \n                  callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler], \n                  verbose=2 #\u3000\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306b1\u884c\u306e\u30ed\u30b0\u3092\u51fa\u529b\n                 )\n        \n        historys[f'history_{seed+1}'] = history\n        \n        model.load_weights(checkpoint_path)\n        test_predict = model.predict(test.values[:, top_feats])\n        \n        # \u81ea\u5df1\u8a55\u4fa1\u7528\u306bvalidation\u30e2\u30c7\u30eb\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u3082\u4fdd\u5b58\u3057\u3066\u304a\u304f\n        val_predict = model.predict(train.values[te][:, top_feats]) \n        \n        ss.loc[:, y_train.columns] += test_predict\n        res.loc[te, y_train.columns] += val_predict\n        print('')\n\n# \u6700\u7d42\u7684\u306b\u8db3\u3057\u5408\u308f\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u8a66\u884c\u56de\u6570\u5206\u306e\u5e73\u5747\u3092\u53d6\u308b\nss.loc[:, y_train.columns] \/= ((n+1) * N_STARTS)\n# val \u306b\u3064\u3044\u3066\u306fN_STARTS\u5206\nres.loc[:, y_train.columns] \/= N_STARTS\n        \n","f2ddaf64":"# Show Model loss in plots\n#\u3000\u8a13\u7df4\u306e\u53ef\u8996\u5316\n# https:\/\/keras.io\/ja\/visualization\/\n\nfor k,v in historys.items():\n    loss = []\n    val_loss = []\n    loss.append(v.history['loss'][:40])\n    val_loss.append(v.history['val_loss'][:40])\n    \nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15, 6))\nplt.plot(np.mean(loss, axis=0))\nplt.plot(np.mean(val_loss, axis=0))\nplt.yscale('log')\nplt.yticks(ticks=[1,1e-1,1e-2])\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])\nplt.show()","cdf653f3":"def metric(y_true, y_pred):\n    metrics = []\n    for _target in y_train.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float)))\n    return np.mean(metrics)","e3c88d1a":"# OOF (Out of Fold)\u3068\u306f\u3001k-Fold \u306a\u3069\u3067\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\u305f\u969b\u306b\u5b66\u7fd2\u306b\u4f7f\u308f\u306a\u304b\u3063\u305f\u30c7\u30fc\u30bf\u3092\u6307\u3059\n# OOF \u306b\u5bfe\u3057\u3066log_loss \u306e\u5e73\u5747\u3092\u51fa\u529b\nprint(f'OOF Metric :{metric(y_train, res)}')","7aba7181":"# \u4e88\u6e2c\u6642\u306b\u306f\u7701\u3044\u3066\u3044\u305fcp_type=1 \u306e\u30ab\u30e9\u30e0\u306f\u5f37\u5236\u7684\u306b0\u3092\u4ee3\u5165\u3059\u308b\n# \u3067\u3082\u3082\u3068\u3082\u3068\u5168\u90e80\u3063\u307d\u3044\nss.loc[test['cp_type']==1, y_train.columns] = 0\nss.to_csv('submission1.csv', index=False)","0b6e8a52":"del historys","c66ab8f3":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multioutput import MultiOutputClassifier\nimport warnings\nwarnings.filterwarnings('ignore')","e68a6667":"X_train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ny_train = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\nX_test = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\n# cp_type == 0 \u306e\u307f\u5229\u7528\n# \u3042\u308f\u305b\u3066y\u3082\u540c\u3058\u3088\u3046\u306b\u843d\u3068\u3059\ny_train = y_train.loc[X_train['cp_type']=='trt_cp'].reset_index(drop=True)\nX_train = X_train.loc[X_train['cp_type']=='trt_cp'].reset_index(drop=True)\n\ntrain = preprocess(X_train)\ntest = preprocess(X_test)\ndel y_train['sig_id']\n\n# drop id col\nX = train.to_numpy()\nX_test = test.to_numpy()\ny = y_train.to_numpy()","3449cbd9":"\nclassifier = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n\nclf = Pipeline([('encode', CountEncoder(cols=[0, 2])),\n                ('classify', classifier)\n               ])\n\nparams = {'classify__estimator__colsample_bytree': 0.6522,\n          'classify__estimator__gamma': 3.6975,\n          'classify__estimator__learning_rate': 0.0503,\n          'classify__estimator__max_delta_step': 2.0706,\n          'classify__estimator__max_depth': 10,\n          'classify__estimator__min_child_weight': 31.5800,\n          'classify__estimator__n_estimators': 166,\n          'classify__estimator__subsample': 0.8639\n         }\n\n_ = clf.set_params(**params)\n","aab46d6b":"N = 7\noof_preds = np.zeros(y.shape)\ntest_preds = np.zeros((test.shape[0], y.shape[1]))\noof_losses = []\nkf = KFold(n_splits=N)\nfor fn, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print('Starting fold: ', fn)\n    X_train, X_val = X[trn_idx], X[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n\n    \n    clf.fit(X_train, y_train)\n    val_preds = clf.predict_proba(X_val) # list of preds per class\n    val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n    oof_preds[val_idx] = val_preds\n    \n    loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n    oof_losses.append(loss)\n    preds = clf.predict_proba(X_test)\n    preds = np.array(preds)[:,:,1].T # take the positive class\n    test_preds += preds \/ N\n    \nprint(oof_losses)\nprint('Mean OOF loss across folds', np.mean(oof_losses))\nprint('STD OOF loss across folds', np.std(oof_losses))","595e836e":"# set control train preds to 0\ncontrol_mask = train['cp_type']=='ctl_vehicle'\noof_preds[control_mask] = 0\n\nprint('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))","cc52c850":"sub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","49ee0e02":"# create the submission file\nsub.iloc[:,1:] = test_preds\nsub.to_csv('submission2.csv', index=False)","77efda01":"stack = (oof_preds + res)\/2","7bfa4c1e":"stack.to_csv('submission.csv', index=False)","cb1b82b2":"# stacked_test_pred = np.columns_stack(test_preds, ss)\n# meta_model_pred = meta_model.predict(stacked_test_pred)","50d3cc93":"# meta_model_pred.to_csv('submission_stacked,csv', index=False)","dce40c53":"# assert(len(meta_model_pred) == len(ss))","5685c467":"# Ensumble","dbc1b2e2":"\u30ab\u30c6\u30b4\u30ea\u95a2\u6570ctl_vehicle\u306f\u5e38\u306b\u76ee\u7684\u5909\u65700\u3068\u306a\u308b\u3053\u3068\u304c\u77e5\u3089\u308c\u3066\u3044\u308b\u306e\u3067\u5b66\u7fd2\u306b\u4e0d\u8981\nReason: [notebook](https:\/\/www.kaggle.com\/demetrypascal\/t-test-pca-rfe-logistic-regression)","b18d2c0c":"# XGBOOST\n","72f64389":"control_mask = train['cp_type']=='ctl_vehicle'\noof_preds[control_mask] = 0\n\nprint('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))","d21f90b8":"# \u6982\u8981\n\nKeras\u3092\u4f7f\u3046\u306e\u306f\u521d\u3081\u3066\u3067\u81ea\u5206\u7528\u306e\u30e1\u30e2\u3068\u3057\u3066\u516c\u958b\u3057\u307e\u3059\n\n\n1. \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306e\u524a\u9664\n\u30ab\u30c6\u30b4\u30ea\u95a2\u6570ctl_vehicle\u306f\u5e38\u306b\u76ee\u7684\u5909\u65700\u3068\u306a\u308b\u3053\u3068\u304c\u77e5\u3089\u308c\u3066\u3044\u308b\u306e\u3067\u5b66\u7fd2\u306b\u4e0d\u8981 Reason: notebook\n\n2. Keras \u3092\u4f7f\u3063\u305f\u5358\u7d14\u306a\u6df1\u5c64\u5b66\u7fd2\n\u30fb\u3000BatchNormalization \u3068DropOut\u5c64\n\u30fb\u3000\u30a2\u30fc\u30ea\u30fc\u30b9\u30c8\u30c3\u30d4\u30f3\u30b0\n\u30fb\u3000\u5b66\u7fd2\u7387\u306e\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\n\u30fb\u300032\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\nhttps:\/\/arxiv.org\/abs\/1804.07612\n\u2191\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306f32\u304f\u3089\u3044\u304c\u3044\u3044\u3088\u3063\u3066\u8ad6\u6587\n\u2192\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3092\u304a\u304a\u304d\u304f\u3057\u305f\u65b9\u304c\u826f\u3044\u7d50\u679c\u304c\u51fa\u308b\n\n\n3. KN-FOLD\nAdd Data \u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u52a0\u3048\u3001MultilabelStratifiedKFold \u306e\u30a4\u30f3\u30dd\u30fc\u30c8\n\n4. RankGauss\nRankGauss \u3067\u524d\u51e6\u7406\n\n\n### \u53c2\u8003\u8cc7\u6599\n- https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2\n"}}