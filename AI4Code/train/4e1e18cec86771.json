{"cell_type":{"8b3580d9":"code","2e6d36ae":"code","5b46c375":"code","074b745e":"code","f7103979":"code","dcf72949":"code","aeec0919":"code","87f540a5":"code","74850d99":"code","08a0d608":"code","2a4753d5":"code","0ef2c563":"code","5eefa394":"code","7ef62f6c":"code","cf282945":"code","94355a11":"code","8e80596d":"code","70aaba74":"code","1efa73ee":"code","56d36793":"code","0a8215a6":"code","5d3c4b91":"code","faceda5d":"code","06b4b9f7":"code","08e97fee":"code","52d6dd75":"code","6d909223":"markdown","64a434d1":"markdown","658291e6":"markdown","afe37300":"markdown","e745dbd5":"markdown","132ca7d9":"markdown","bb7d46ca":"markdown","3fe89a48":"markdown","bccf4f13":"markdown","dbf80322":"markdown","cae20a2e":"markdown","2b81d21f":"markdown","7771e7f4":"markdown","a7052f82":"markdown","5a69e732":"markdown","587861d2":"markdown"},"source":{"8b3580d9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2e6d36ae":"# for plotting:\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# for preprocessing\/ feature engineering:\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# getting some algos from sci-kit learn's libraries:\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n# for building a fantastic neural network!:\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.losses import binary_crossentropy\nfrom keras.metrics import Accuracy\n","5b46c375":"data = pd.read_csv(\"..\/input\/churn-modelling\/Churn_Modelling.csv\", index_col='CustomerId')","074b745e":"data.head()","f7103979":"y = data['Exited']\ndata.drop(['Exited', 'RowNumber', 'Surname'], axis=1, inplace=True)","dcf72949":"plt.figure(figsize=(6,6))\nsns.countplot(data['Gender'])","aeec0919":"plt.figure(figsize=(6,6))\nsns.countplot(data['Geography'], hue=data['Gender'])","87f540a5":"xtrain, xtest, ytrain, ytest = train_test_split(data, y, train_size=0.9, test_size=0.1)","74850d99":"plt.figure(figsize=(10,10))\nsns.scatterplot(data=xtrain, y='Age', x='CreditScore', hue=y)","08a0d608":"cat_cols = ['Geography', 'Gender']","2a4753d5":"enctrain = xtrain.copy()\nenctest = xtest.copy() \n\nlab = LabelEncoder()\nfor i in cat_cols:\n    enctrain[i] = lab.fit_transform(enctrain[i])\n    enctest[i] = lab.transform(enctest[i])","0ef2c563":"enctrain.head()","5eefa394":"plt.figure(figsize=(10,10))\nsns.heatmap(enctrain.corr())","7ef62f6c":"xtrain.shape","cf282945":"Model = Sequential([\n    Dense(128, input_shape=(10,), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid')\n])","94355a11":"rms = Adam(lr=0.1)\n\nModel.compile(\n    optimizer=rms, \n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)","8e80596d":"hist = Model.fit(\n    enctrain,\n    ytrain,\n    epochs=50,\n    validation_data=(enctest,ytest),\n)","70aaba74":"tremod = DecisionTreeClassifier()\ntremod.fit(enctrain, ytrain)\npreds = tremod.predict(enctest)\naccuracy_score(preds, ytest)","1efa73ee":"logmodel = LogisticRegression()\nlogmodel.fit(enctrain, ytrain)\nlogpreds = logmodel.predict(enctest)\naccuracy_score(logpreds, ytest)\n","56d36793":"svm = SVC()\nsvm.fit(enctrain, ytrain)\nsvmpreds = svm.predict(enctest)\naccuracy_score(svmpreds, ytest)","0a8215a6":"std = StandardScaler()\n\nscaledtrain = std.fit_transform(enctrain)\n\nscaledtest = std.transform(enctest)","5d3c4b91":"svm.fit(scaledtrain, ytrain)\nscaledsvmpreds = svm.predict(scaledtest)\naccuracy_score(scaledsvmpreds, ytest)","faceda5d":"logmodel.fit(scaledtrain, ytrain)\nscaledlogpreds = logmodel.predict(scaledtest)\naccuracy_score(scaledlogpreds, ytest)","06b4b9f7":"ScaledModel = Sequential([\n    Dense(128, input_shape=(10,), activation='relu'),\n    Dense(128, activation='relu'),\n    Dense(128, activation='relu'),\n    Dense(128, activation='relu'),\n    Dense(1, activation='sigmoid')\n])","08e97fee":"rms = RMSprop(lr=0.004)\n\nScaledModel.compile(\n    optimizer=rms, \n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)","52d6dd75":"\nhistscaled = ScaledModel.fit(\n    scaledtrain,\n    ytrain,\n    epochs=220,\n    validation_data=(scaledtest,ytest)\n)","6d909223":"Lets try running the Support Vector Classifier on scaled data:\n","64a434d1":"Below is a list of columns on which I would be defining the categorical columns on which the LabelEncoder will work on:","658291e6":"Reading the data:","afe37300":"Lets see the distribution of Males and Females in our data:","e745dbd5":"Lets look at the distribution of the customers across the countries:","132ca7d9":"Lets see how the data is distributed as per customer's salary and the number of items they bought:","bb7d46ca":"The data in here seems quite messed up, this might be due to low correlation between the two features I have mentioned.","3fe89a48":"Encoding the categorical feautures:","bccf4f13":"As we can see that all of these algorthms are not exactly working well on this dataset which such mediocre accuracy. Lets scale the data:\n\n","dbf80322":"The model now shows an accuracy of 91% on the training data with 85% accuracy on data that it has not seen before.","cae20a2e":"Its time to split the data into training and test sets!","2b81d21f":"Importing the necessary libraries:","7771e7f4":"Okay, so lets check the correlation of the columns or features:","a7052f82":"Dropping the 'Exited' column since it contained the labels, also we drop the 'Surname' column because it might mess up the preprocessing since it has way too many unique values and that won't help anyway. 'RowNumber' column just felt useless to me.","5a69e732":"Alrighty, so all columns have an almost low correlation if not negative. This tells me that there is no need to drop any columns or features. Lets build a neural network now, shall we?\n","587861d2":"Lets build another neural network but change its composition a little bit, tinker with it and..."}}