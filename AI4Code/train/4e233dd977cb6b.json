{"cell_type":{"2e36e574":"code","e3775043":"code","986ab2a2":"code","c572d192":"code","3640e083":"code","96cdcd9a":"code","16e6320d":"code","9cb8268b":"code","3a0d82d7":"code","ca22802e":"code","aff28cac":"code","76e593ea":"code","417dbcb2":"code","f23da133":"code","ae347662":"code","7b902de0":"code","cdd14a26":"code","02fbcf00":"code","2065425a":"code","eae98930":"code","609371fc":"code","ae01e149":"code","218ef897":"code","29f2cc9f":"code","f4fa20f0":"code","e4780b32":"code","90b1b1ca":"code","6c694ea9":"code","d4be9398":"code","7c8af2af":"code","14e8795f":"code","e377375c":"code","94dffcc4":"code","bfb83e9f":"code","d0f34e90":"code","85502725":"code","f491900a":"code","b2712034":"code","de876cd5":"code","61da55c0":"code","75ae93dd":"code","02a701d9":"code","36a03281":"code","dadaf63b":"code","05fcf779":"code","4635f61d":"code","be579ff1":"code","f62026b9":"code","fbed5c42":"code","7ff5b68a":"code","e0bcdfef":"code","61400e26":"code","0e67d9d0":"code","29240a83":"code","23fe72e0":"markdown","f4973ca5":"markdown","57053413":"markdown","e27efc61":"markdown","6f71b589":"markdown","e0c32aea":"markdown","c1619423":"markdown","8af19d85":"markdown","af86a616":"markdown","e29b12ec":"markdown","2f261d3c":"markdown","49269db0":"markdown","79e98845":"markdown","b3de97cb":"markdown","707f7c62":"markdown","f5b085ed":"markdown","84de573f":"markdown","5966acbd":"markdown","d0052d48":"markdown","7668484c":"markdown","e63503d4":"markdown","6cd8cb2a":"markdown","b240e3b7":"markdown","33bfb9bc":"markdown","cda54d3d":"markdown","90f49164":"markdown","6af86aa4":"markdown","421713af":"markdown","d84c104a":"markdown","60a9cce3":"markdown","4874300a":"markdown","6e2b7c8e":"markdown","ec08af9e":"markdown","9eb64865":"markdown","8144fd9a":"markdown","d21698de":"markdown","71e3c525":"markdown","4a1f0b12":"markdown"},"source":{"2e36e574":"# Data Processing\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve, train_test_split\n\n# Data Visualizing\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\n# Math\nimport math\nfrom scipy.stats import norm\nfrom scipy import stats\n\n# Data Modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC, SVC, NuSVC\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint\n\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Data Validation\nfrom sklearn import metrics\n\n# Warning Removal\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","e3775043":"df = pd.read_csv('..\/input\/train.csv')","986ab2a2":"df.head(5)","c572d192":"df.describe(include=\"all\")","3640e083":"df.info(verbose=True)","96cdcd9a":"df.corr()['Survived'].sort_values(ascending = False)","16e6320d":"df.drop(['PassengerId', 'Ticket', 'Cabin'], axis=1, inplace=True)","9cb8268b":"df.isnull().sum()","3a0d82d7":"zero_count = (df.isnull()).sum() # (df1 == 0).sum()\nzero_count_df = pd.DataFrame(zero_count)\nzero_count_df.drop('Survived', axis=0, inplace=True)\nzero_count_df.columns = ['count_0']\n\n# https:\/\/stackoverflow.com\/questions\/31859285\/rotate-tick-labels-for-seaborn-barplot\/60530167#60530167\nsns.set(style='whitegrid')\nplt.figure(figsize=(13,8))\nsns.barplot(x=zero_count_df.index, y=zero_count_df['count_0'])\nplt.xticks(rotation=70)","ca22802e":"### Selecting categorical data for univariate analysis\ncats = ['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n\ndef plotFrequency(cats):\n    #\"A plot for visualize categorical data, showing both absolute and relative frequencies\"\n    fig, axes = plt.subplots(2, 3, figsize=(20, 15))\n    axes = axes.flatten()\n\n    for ax, cat in zip(axes, cats):\n        total = float(len(df[cat]))\n        sns.countplot(df[cat], palette='plasma', ax=ax)\n\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x() + p.get_width() \/ 2.,\n                    height + 10,\n                    '{:1.2f}%'.format((height \/ total) * 100),\n                    ha=\"center\")\n\n        plt.ylabel('Count', fontsize=15, weight='bold')","aff28cac":"plotFrequency(cats)","76e593ea":"def plotSurvival(cats):\n    #\"A plot for visualize categorical data, showing both absolute and relative frequencies\"\n    fig, axes = plt.subplots(2, 3, figsize=(25, 20))\n    axes = axes.flatten()\n\n    for ax, cat in zip(axes, cats):\n        sns.countplot(df[cat], palette='plasma',hue=df['Survived'], ax=ax)\n        \n        ax.legend(title='Survived?',loc='upper right',labels=['No', 'Yes'])\n        \n        plt.ylabel('Count', fontsize=15, weight='bold')","417dbcb2":"plotSurvival(cats)","f23da133":"def plot_3chart(feature):\n    fig = plt.figure(constrained_layout=True, figsize=(12, 8))\n    # creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.distplot(df[feature], hist=True, kde=True, fit=norm, color='#e74c3c', ax=ax1)\n    ax1.legend(labels=['Normal', 'Actual'])\n    \n    # customizing the QQ_plot.\n    ax2 = fig.add_subplot(grid[1, :2])\n    # Set the title.\n    ax2.set_title('Probability Plot')\n    # Plotting the QQ_Plot.\n    stats.probplot(df[feature].fillna(np.mean(df.loc[:, feature])), plot=ax2)\n    #ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n    \n     # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(df[feature], orient='v', color='#e74c3c', ax=ax3)\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.suptitle(f'{feature}', fontsize=24)","ae347662":"plot_3chart('Age')\nplot_3chart('Fare')","7b902de0":"sns.scatterplot(x=df['Age'], y=df['Fare'], hue=df['Pclass'], palette='coolwarm')","cdd14a26":"sns.scatterplot(x=df['Age'], y=df['Fare'], hue=df['Survived'], palette='deep')","02fbcf00":"sns.scatterplot(x=df['Age'], y=df['Pclass'], hue=df['Survived'], palette='deep')","2065425a":"def Age_mean_accordingto_target():\n    print(df.groupby(['Pclass'])['Age'].mean())\n    print()\n    print(df.groupby(['Sex'])['Age'].mean())\n    print()\n    print(df.groupby(['Pclass', 'Sex'])['Age'].mean())\n    print()\n    print(df.groupby(['Survived'])['Age'].mean())\n    print()\n    print(df.groupby(['Pclass', 'Survived'])['Age'].mean())\n    print()\n    print(df.groupby(['Pclass', 'Sex', 'Survived'])['Age'].mean())","eae98930":"Age_mean_accordingto_target()","609371fc":"# Fill Age's null value with median according to Pclass, Sex, Survived\ndf['Age'] = df.groupby(['Pclass', 'Sex', 'Survived'])['Age'].apply(lambda x: x.fillna(x.median()))","ae01e149":"# Check if Age has any Null value\ndf[df['Age'].isnull()]['Age']","218ef897":"bins = [0, 2, 18, 35, 65, np.inf]\nnames = ['00-02', '02-18', '18-35', '35-65', '65plus']\ndf['AgeBin'] = pd.cut(df['Age'], bins, labels=names)\n\ndf['AgeBin'] = LabelEncoder().fit_transform(df['AgeBin'])\ndf[['AgeBin', 'Survived']].groupby(['AgeBin'], as_index=False).mean().sort_values(by='AgeBin', ascending=True)","29f2cc9f":"df.drop(['Age'], axis=1, inplace=True)","f4fa20f0":"df.corr()['Survived'].sort_values(ascending = False)","e4780b32":"df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\ndf['Embarked'] = df['Embarked'].map( {'S': 0, 'Q': 1, 'C': 2})","90b1b1ca":"df['FamilySize'] = df['SibSp'] + df['Parch'] + 1","6c694ea9":"df[\"Alone\"] = df['FamilySize'].apply(lambda x: 1 if x==1 else 0)","d4be9398":"df['SpecialFamilySize'] = df['FamilySize'].apply(lambda x: 1 if (x==2)or(x==3)or(x==4)  else 0)","7c8af2af":"g = sns.FacetGrid(df, col=\"Survived\")\ng.map(plt.hist, \"Fare\");","14e8795f":"bins = [-1, 7.91, 14.454, 31, 99, 250, np.inf]\nnames = [0, 1, 2, 3, 4, 5]\n\ndf['FareBin'] = pd.cut(df['Fare'], bins, labels=names).astype('int')","e377375c":"df.corr()['Survived'].sort_values(ascending = False)","94dffcc4":"df['Sex'] = df['Sex'].map({'male':0, 'female':1})","bfb83e9f":"df.corr()['Survived'].sort_values(ascending = False)","d0f34e90":"df['Title'] = df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())","85502725":"plt.figure(figsize=(13,8))\nsns.countplot(df['Title'])\nplt.xticks(rotation=70)","f491900a":"df.groupby('Title')['Survived'].mean().sort_values(ascending = False)","b2712034":"df['Title'] = df['Title'].replace(['Mr','Rev','Jonkheer','Don','Capt'], 0)\ndf['Title'] = df['Title'].replace(['Master','Major','Col','Don','Dr'], 1)\ndf['Title'] = df['Title'].replace(['the Countess','Mlle','Lady','Ms','Sir','Mme','Mrs','Miss'], 2)","de876cd5":"df.groupby('Title')['Survived'].mean().sort_values(ascending = False)","61da55c0":"# df['Pclass'] = df['Pclass'].astype('str')\n# df['AgeBin'] = df['AgeBin'].astype('str')\n# df['FamilySize'] = df['FamilySize'].astype('str')","75ae93dd":"df.drop(['SibSp','Parch','FamilySize','Fare','Name'],axis=1, inplace=True)","02a701d9":"df","36a03281":"df = pd.get_dummies(df, drop_first=True)\ndf","dadaf63b":"feature_data = df.drop('Survived', axis=1)\ntarget_data = df['Survived']","05fcf779":"cv = StratifiedKFold(10, shuffle=True, random_state=0)\n\ndef model_check(X, y, estimators, cv):\n    model_table = pd.DataFrame()\n    \n    row_index = 0\n    for est in estimators:\n\n        MLA_name = est.__class__.__name__\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n        #    model_table.loc[row_index, 'MLA Parameters'] = str(est.get_params())\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='accuracy',\n                                    return_train_score=True,\n                                   )\n\n        model_table.loc[row_index, 'Train Accuracy Mean'] = cv_results['train_score'].mean()\n        model_table.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n        model_table.sort_values(by=['Test Accuracy Mean'],\n                            ascending=False,\n                            inplace=True)\n\n    return model_table","4635f61d":"logreg = LogisticRegression(n_jobs=-1, solver='newton-cg')\n\nknn = KNeighborsClassifier(n_neighbors=13)\n\ngnb = GaussianNB()\n\nlinearSVC = LinearSVC()\n\nRbfSVC = SVC()\n\ndt = DecisionTreeClassifier(max_depth=10)\n\nrf = RandomForestClassifier(random_state=0,n_jobs=-1,verbose=0)\n\nadab = AdaBoostClassifier(random_state=0)\n\ngb = GradientBoostingClassifier(random_state=0)\n\nxgb = XGBClassifier(random_state=0)\n\nlgbm = LGBMClassifier(random_state=0)","be579ff1":"estimators = [logreg,knn,gnb,linearSVC,RbfSVC,dt,rf,gb,xgb,lgbm]","f62026b9":"raw_models = model_check(feature_data, target_data, estimators, cv)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","fbed5c42":"def plot_learning_curve(estimators,\n                        X,\n                        y,\n                        ylim=None,\n                        cv=None,\n                        n_jobs=None,\n                        train_sizes=np.linspace(.1, 1.0, 5)):\n\n    fig, axes = plt.subplots(math.ceil(len(estimators) \/ 2),\n                             2,\n                             figsize=(25, 50))\n    axes = axes.flatten()\n\n    for ax, estimator in zip(axes, estimators):\n\n        ax.set_title(f'{estimator.__class__.__name__} Learning Curve')\n        if ylim is not None:\n            ax.set_ylim(*ylim)\n        ax.set_xlabel('Training examples')\n        ax.set_ylabel('Score')\n\n        train_sizes, train_scores, test_scores, fit_times, _ = \\\n            learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                           train_sizes=train_sizes,\n                           return_times=True)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n\n        # Plot learning curve\n\n        ax.fill_between(train_sizes,\n                        train_scores_mean - train_scores_std,\n                        train_scores_mean + train_scores_std,\n                        alpha=0.1,\n                        color='r')\n        ax.fill_between(train_sizes,\n                        test_scores_mean - test_scores_std,\n                        test_scores_mean + test_scores_std,\n                        alpha=0.1,\n                        color='g')\n        ax.plot(train_sizes,\n                train_scores_mean,\n                'o-',\n                color='r',\n                label='Training score')\n        ax.plot(train_sizes,\n                test_scores_mean,\n                'o-',\n                color='g',\n                label='Cross-validation score')\n        ax.legend(loc='best')\n        ax.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.show()","7ff5b68a":"plot_learning_curve(estimators,\n                    feature_data, \n                    target_data,\n                    ylim=None,\n                    cv=cv,\n                    n_jobs=-1,\n                    train_sizes=np.linspace(.1, 1.0, 10))","e0bcdfef":"dt = DecisionTreeClassifier(max_depth=10).fit(feature_data, target_data)\nrf = RandomForestClassifier(random_state=0,n_jobs=-1,verbose=0).fit(feature_data, target_data)\nxgb = XGBClassifier(random_state=0).fit(feature_data, target_data)\nadab = AdaBoostClassifier(random_state=0).fit(feature_data, target_data)\nlgbm = LGBMClassifier(random_state=0).fit(feature_data, target_data)","61400e26":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"DecisionTree\", dt), (\"RF\", rf), (\"XGB\", xgb), (\"AdaBoost\", adab)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=feature_data.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","0e67d9d0":"votingC = VotingClassifier(estimators=[(\"DecisionTree\", dt), (\"RF\", rf), (\"XGB\", xgb), \n                                       (\"AdaBoost\", adab), ('LightGBM', lgbm)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(feature_data, target_data)","29240a83":"print('Final_Score: {:.2f}'.format(votingC.score(feature_data, target_data)))","23fe72e0":"#### Age filler\n- Check age average according to Pclass, Sex, Pclass & Sex, Survived\n- Fill Nan with age average, leading to the best accuracy. This step is a trial and error process.","f4973ca5":"# Titanic Classification Prediction\n### Vu Duong\n#### May 18, 2020","57053413":"Using common sense to discard those 3 features: PassengerId, Ticket, Cabin as for having no relationship with survival opportunities and as for simplifying the problem.","e27efc61":"#### Numerical features\n- Age: passenger's age\n- Fare: money amount spent on the trip","6f71b589":"# LIBRARY","e0c32aea":"#### Drop Features","c1619423":"#### Title Extractor\n- As Sex and Title have close relationship. Thus we may derive an useful feature.\n- Grouping titles into 3 groups, one for only male, one for only female, and one the for rest.\n- Seeing this feature is even higher predictabe than Sex feature. ","8af19d85":"#### Sex\n- High correlation with Survived, at 0.54 .\n- As having seen above, female group has higher survival opportunities than its countrerpart.","af86a616":"##### Observation\n- Significant missing values lie in Age feature\n- Embarked feature contains only 2 Null values","e29b12ec":"### Ensemble Model\n- Using Voting Classifier to combine best 5 models above\n- Soft argument is taken into account for the sake of the probability of each vote.","2f261d3c":"# FEATURE ENGINEERING","49269db0":"### Train-Test Split","79e98845":"### Important Features\n- 4 tree-based model indicates different top features based on its math\/algorithms behind.\n- However they all share some common important features such as Title, Pclass, Sex, Alone","b3de97cb":"#### Encoding feature","707f7c62":"#### Plot Missing Value","f5b085ed":"### Learning Curve\n- Learning curves are good indicators for showing if our models are overfitting or underfitting. \n- It's also showing if we need more data or not but it's not the case here since we can't increase our numbers.","84de573f":"##### Observation\n- Age distribution looks quite similar to normal curve, a little right skewed with high number of children.\n- Fare distribution is right skewed with those spending too much on tickets. Thus scaling or categorizing into bins to get better result.","5966acbd":"##### Plot distribution, QQ, box graph","d0052d48":"#### Drop Features","7668484c":"#### Family Size\n- Combining sibling number and parents gives the new feature, FamilySize. Yet FamilySize does not perform better than SibSp and Parch.\n- Deriving a new feature from FamilySize, called Alone. This feature helps models\n- Examing FamilySize indicates people having FamilySize of 2,3,4 got better chance of survival. Thus creating a new feature, called SpecialFamilySize","e63503d4":"#### Observation\n- Fare and Pclass have significant positive and negative correlation with Survived","6cd8cb2a":"#### Categorical features\n- Survived, Pclass, Sex, SibSp, Parch, Embarked","b240e3b7":"### Choosing the best Models\n- Using GridSearch for tuning hyperparameters","33bfb9bc":"This notebook is inspired by great work on Titanic competition:\n- https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\n- https:\/\/www.kaggle.com\/datafan07\/my-journey-to-top-3-eda-with-several-approaches\n- https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions ","cda54d3d":"### SCALING\n- standardization, normalize\n- Having feature scaling has model worse.","90f49164":"#### Embarked filler","6af86aa4":"### DATA IMPUTATION\n- mean\n- median\n- mode\n- correlation with other columns","421713af":"#### Changing Data Types\n- Chaning data types from int to string for one-hot encoding, rather than label encoding.\n- However model get worse.\n- Categorical features such as Pclass, AgeBin, FamilySize are ordinal categories. Thus model gives better result. ","d84c104a":"#### Age Grouper\n- Categorizing age into bins to avoid outliers\n- Correlation between AgeBin and Survived is greater than that of Age and Survived.\n- Thus keep the new feature and remove Age to prevent multicollinearity problem.","60a9cce3":"### Models\n- A list of models for training and predicting","4874300a":"# MODELING","6e2b7c8e":"# CREDIT","ec08af9e":"### VISUALIZATION\n- dist: 1 var\n- count:\n- scatter: 2, 3 variable\n- box: 1 var to look for outliers","9eb64865":"### EXLORATORY\n- read_csv\n- head\n- describe\n- info\n- correlation","8144fd9a":"### Model Evaluation","d21698de":"#### Fare grouper\n- Similar to Age grouper.\n- Performing FareBin to get rid of outlier and get higher corrleation with Survived feature","71e3c525":"### Cross-Validate Models\n- Shuffle and fold 10 times across training dataset.","4a1f0b12":"##### Observation\n- Most passengers died, around 62%\n- Having the highest portion of passengers in Pclass 3 with highest causulty rate, as oppose to those in Pclass 1.\n- Male number accounts for two third of passengers, taking highest casualty ratio as well.\n- Sibsp and Parch are in context of family. Thus May having family or without family affect survival rate.\n- Most people departed from Southhamton. For those from Southampton have less chance of survival"}}