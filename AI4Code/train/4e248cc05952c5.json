{"cell_type":{"2fb70bde":"code","6b7ddb1c":"code","bf8d3421":"code","0f203fa5":"code","122fe0af":"code","0c81c992":"code","158be4fb":"code","8b258642":"code","c0389343":"code","61338a0d":"code","e045286a":"code","e4e74891":"code","07b62816":"code","d6b2fdab":"code","f2408c48":"code","fe751b44":"code","c9940efd":"code","00bbfc49":"code","7a8b165e":"code","86bc715f":"code","278bd813":"code","393045d4":"code","247260ad":"code","a93468c2":"code","e675e12f":"code","2a89c647":"code","1470df84":"code","1d580328":"code","d3bed5fe":"code","30cd570e":"code","be66dc91":"code","1728f823":"code","c3a3f8cd":"code","9017f499":"code","50a0b9e8":"code","dfc1bdb3":"code","a0b94460":"code","94734980":"code","b7d29e89":"code","8b72e812":"code","bbfa6f8b":"code","fadacb9b":"code","ba5945e0":"code","1362398e":"code","32fe73e8":"code","8b4704e9":"code","3a3b5271":"code","e9208195":"code","6b583827":"code","34616c21":"markdown","2dbfaa09":"markdown","798517ea":"markdown","7cf182b8":"markdown","ce287fff":"markdown","55c467ff":"markdown","e1866841":"markdown","0a8d5b45":"markdown","21c950cf":"markdown","ad5664fb":"markdown","de0b2e58":"markdown","ea59a692":"markdown","f70a5a08":"markdown","5545038a":"markdown","487b3450":"markdown","adbf43be":"markdown","38f4b707":"markdown","b6306b42":"markdown","c3d4cb9c":"markdown","0334353f":"markdown","391e698f":"markdown","2168b20b":"markdown","b81ecf58":"markdown","d201e438":"markdown","2eb5e09c":"markdown","1fe12134":"markdown","d876d8f0":"markdown","047a5a8a":"markdown","52fe5f1c":"markdown"},"source":{"2fb70bde":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6b7ddb1c":"data = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\nprint(data.shape)\ndata.head()","bf8d3421":"data.info()","0f203fa5":"data.describe()","122fe0af":"data.columns","0c81c992":"import seaborn as sns\nimport matplotlib.pyplot as plt","158be4fb":"sns.pairplot(data)","8b258642":"plt.figure(figsize= (16,10))\nsns.heatmap(data.corr(),cmap = 'Dark2',annot = True,linewidths=1.0,linecolor='black')","c0389343":"data.corr()['quality'].sort_values(ascending=False)","61338a0d":"np.abs(data.corr()['quality']).sort_values(ascending=False)","e045286a":"sns.countplot(data['quality'])","e4e74891":"plt.hist((data['fixed acidity']))","07b62816":"sns.boxplot('quality','fixed acidity',data=data) #you can try sin","d6b2fdab":"sns.regplot(x=\"fixed acidity\", y=\"quality\", data=data)","f2408c48":"data['volatile acidity'].hist(bins = 30)","fe751b44":"sns.boxplot('quality','volatile acidity',data=data)","c9940efd":"data[\"volatile acidity\"].describe()","00bbfc49":"sns.regplot(x=\"volatile acidity\", y=\"quality\", data=data)","7a8b165e":"plt.hist(data['citric acid'])","86bc715f":"sns.boxplot('quality','citric acid',data=data)","278bd813":"data['citric acid'].describe()","393045d4":"sns.regplot(x=\"citric acid\", y=\"quality\", data=data)","247260ad":"plt.hist(data['alcohol'])","a93468c2":"sns.boxplot('quality','alcohol',data=data)","e675e12f":"sns.regplot(x=\"alcohol\", y=\"quality\", data=data)","2a89c647":"g = sns.FacetGrid(data, col='quality')\ng.map(plt.hist, 'alcohol', bins=20)","1470df84":"plt.hist(data['sulphates'])","1d580328":"sns.boxplot('quality','sulphates',data=data)","d3bed5fe":"sns.regplot(x=\"sulphates\", y=\"quality\", data=data)","30cd570e":"g = sns.FacetGrid(data, col='quality')\ng.map(plt.hist, 'sulphates', bins=20)","be66dc91":"from sklearn.linear_model import LogisticRegression   \nfrom sklearn.model_selection import KFold \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.naive_bayes import GaussianNB","1728f823":"def classification_model(model, data, predictors, outcome):  \n    #Fit the model:  \n    model.fit(data[predictors],data[outcome])    \n    #Make predictions on training set:  \n    predictions = model.predict(data[predictors])    \n    #Print accuracy  \n    accuracy = metrics.accuracy_score(predictions,data[outcome])  \n    print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n    #Perform k-fold cross-validation with 5 folds  \n    kf = KFold(5,shuffle=True)  \n    error = []  \n    for train, test in kf.split(data):\n        # Filter training data    \n        train_predictors = (data[predictors].iloc[train,:])        \n        # The target we're using to train the algorithm.    \n        train_target = data[outcome].iloc[train]        \n        # Training the algorithm using the predictors and target.    \n        model.fit(train_predictors, train_target)\n        #Record error from each cross-validation run    \n        error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n     \n    print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error))) \n    # %s is placeholder for data from format, next % is used to conert it into percentage\n    #.3% is no. of decimals\n    return model","c3a3f8cd":"def classification_model2(model, x_train,p,y_train ):#, outcome):  \n    #Fit the model:  \n    model.fit(x_train[p],y_train)    \n    #Make predictions on training set:  \n    predictions = model.predict(x_train[p])    \n    #Print accuracy  \n    accuracy = metrics.accuracy_score(predictions,y_train)  \n    print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n    #Perform k-fold cross-validation with 5 folds  \n    kf = KFold(5,shuffle=True)  \n    error = []  \n    for train, test in kf.split(x_train):\n        # Filter training data    \n        train_predictors = (x_train[p].iloc[train,:])        \n        # The target we're using to train the algorithm.    \n        train_target = y_train.iloc[train]        \n        # Training the algorithm using the predictors and target.    \n        model.fit(train_predictors, train_target)\n        #Record error from each cross-validation run    \n        error.append(model.score(x_train[p].iloc[test,:], y_train.iloc[test]))\n     \n    print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error))) \n    # %s is placeholder for data from format, next % is used to conert it into percentage\n    #.3% is no. of decimals\n    return model","9017f499":"from sklearn.model_selection import train_test_split\nX = (data.iloc[:,0:11])\ny = (data['quality'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)","50a0b9e8":"s=time.time()\noutput = 'quality'\npredict = ['alcohol','volatile acidity','sulphates','citric acid','residual sugar','pH']\nlr = LogisticRegression(max_iter=10000,fit_intercept=False, C=10000)\nprint(\"Logistic Regression(1)\")\nlr = classification_model(lr,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))","dfc1bdb3":"s=time.time()\noutput = 'quality'\npredict = ['alcohol','volatile acidity','sulphates','citric acid','residual sugar','pH']\nlr2 = LogisticRegression(max_iter=10000,fit_intercept=False, C=10000)\nX_train[predict]\nprint(\"Logistic Regression with Train Test Split\")\nlr = classification_model2(lr2,X_train,predict,y_train)\npredictions = lr2.predict(X_test[predict])    \n    #Print accuracy  \naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on test data = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","a0b94460":"s=time.time()\noutput = 'quality'\npredict = ['alcohol','volatile acidity']#,'citric acid','sulphates']#,'sulphates']#volatile acidity, sulphates\nknn = KNeighborsClassifier(weights='distance', n_neighbors=200)\nprint(\"K-Nearest Neighbors(1)\")\nknn = classification_model(knn,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))\n# 68.6 , 54.8","94734980":"s=time.time()\noutput = 'quality'\npredict = ['alcohol','volatile acidity']#,'sulphates','citric acid','residual sugar','pH']\nknn2 = KNeighborsClassifier(weights='distance', n_neighbors=150)\nprint(\"KNN with Train Test Split\")\nknn2 = classification_model2(knn2,X_train,predict,y_train)\npredictions = knn2.predict(X_test[predict])    \n    #Print accuracy  \naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on test data = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","b7d29e89":"s=time.time()\noutput = 'quality'\npredict = ['volatile acidity','sulphates', 'alcohol']#,'citric acid'] #['alcohol',\ndtree = DecisionTreeClassifier(random_state=40,max_depth=20,max_leaf_nodes=100)#,max_features='sqrt')#random_state=40,max_depth=20,max_features='sqrt',max_leaf_nodes=700)\nprint(\"DecisionTree\")\ndtree = classification_model(dtree,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))\n#99,54","8b72e812":"s=time.time()\noutput = 'quality'\npredict = ['alcohol','volatile acidity','sulphates']#'citric acid','residual sugar','pH']\ndtree2 = DecisionTreeClassifier(random_state=40)#,max_features='sqrt')\nprint(\"Decision Tree 2.\")\ndtree2 = classification_model2(dtree2,X_train,predict,y_train)\npredictions = dtree2.predict(X_test[predict])    \n    #Print accuracy  \naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on test data = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","bbfa6f8b":"s=time.time()\noutput = 'quality'\npredict = ['volatile acidity','alcohol','sulphates'] #['alcohol',\nrf = RandomForestClassifier(n_estimators=10000,max_depth=5,bootstrap=False)\nprint(\"Random Forest\")\ndtree = classification_model(dtree,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))","fadacb9b":"s=time.time()\noutput = 'quality'\npredict = ['alcohol','sulphates','citric acid', 'volatile acidity']\nnb= GaussianNB()\nprint(\"Naive Bayes\")\nnb = classification_model(nb,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))","ba5945e0":"s=time.time()\nestimators = [('lr',lr),('knn',knn),('tree',dtree)]#,('rf',rf)] #,('support',svc)('nb',nb)('tree',dtree),\nsoft_vote = VotingClassifier(estimators=estimators , voting= 'soft')\nprint(\"soft voting \")\nsoft_vote=classification_model(soft_vote,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))","1362398e":"s=time.time()\npredict=[\"alcohol\",\"sulphates\", \"volatile acidity\"]\nestimators = [('lr',lr),('tree',dtree),('knn',knn)]#,('knn2',knn2)]#,('rf',rf)] #,('support',svc)('nb',nb)('tree',dtree),\nsoft_vote2 = VotingClassifier(estimators=estimators , voting= 'soft')\nprint(\"Soft Vote using Train Test Split\")\nsoft_vote2 = classification_model2(soft_vote2,X_train,predict,y_train)\npredictions = soft_vote2.predict(X_test[predict])    \n    #Print accuracy  \naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on test data = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","32fe73e8":"s=time.time()\nprint(\"Hard Voting\")\nhard_vote = VotingClassifier(estimators=estimators , voting= 'hard')\nhard_vote = classification_model(hard_vote,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))","8b4704e9":"s=time.time()\nestimators = [('lr',lr),('tree',dtree),('knn',knn)]#,('tree',dtree2)]#,('rf',rf)] #,('support',svc)('nb',nb)('tree',dtree),\nhard_vote2 = VotingClassifier(estimators=estimators , voting= 'hard')\nprint(\"Hard Voting using Train Test Split\")\nhard_vote2 = classification_model2(hard_vote2,X_train,predict,y_train)\npredictions = hard_vote2.predict(X_test[predict])    \n    #Print accuracy  \naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on test data = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","3a3b5271":"s=time.time()\nprint(\"Stacking using KNN as Meta\")\nmeta = KNeighborsClassifier(weights='distance', n_neighbors=100)\nstack = StackingClassifier(estimators = [('lr2',lr2),('knn',knn2),('tree',dtree2),('hard2',hard_vote2)])#('rf',rf)],final_estimator=meta) #('hard',hard_vote),\nstack = classification_model2(stack,X_train,predict,y_train) #('hard',hard_vote),\np = stack.predict(X_test[predict])\naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on test data = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","e9208195":"s=time.time()\nmeta = LogisticRegression()#max_iter=10000,fit_intercept=False, C=10000)\nstack = StackingClassifier(estimators = [('knn',knn),('tree',dtree),('soft',soft_vote),('rf',rf)],final_estimator=meta) #('hard',hard_vote),\nstack = classification_model(stack,data,predict,output)#('hard',hard_vote),,\nprint(\"Time = {}\".format(time.time()-s))","6b583827":"s=time.time()\nprint(\"Using Logistic Regression as meta\")\nmeta = LogisticRegression(max_iter = 1000)#max_iter=10000,fit_intercept=False, C=10000)\nstack = StackingClassifier(estimators = [('knn2',knn2),('tree',dtree),('soft2',soft_vote2),('hard2',hard_vote2)],final_estimator=meta) #('hard',hard_vote),\nstack = classification_model2(stack,X_train,predict,y_train) #('hard',hard_vote),\np = stack.predict(X_test[predict])\naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on Test Set = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","34616c21":"First we will import the data and look at the first few rows and some information about the data.","2dbfaa09":"The mean and the median are quite close. So,there aren't really outliers which could affect the predictions. There is, again, an upward trend but the trend isn't very clear","798517ea":"# EXPLORATORY DATA ANALYSIS","7cf182b8":"First we will import all the necessary libraries and functions","ce287fff":"We will look at the sulphates column. ","55c467ff":"If we look at the heatmap of the correlations closely, we observe :-\n1. alcohol has the highest correlation with quality.\n2. volatile acidity has the highest negative correlation with quality.\n3. sulphates, citric acid are the next highly correlated columns with quality.","e1866841":"Now, we will look at the fixed acidity column. ","0a8d5b45":"First, we will look at the column we have to predict, which is quality.","21c950cf":"The following code is a classification model creator, which returns a trained model and its accuracy and cross validation score.","ad5664fb":"Next up, we will create a Decision Tree Classifier","de0b2e58":"The mean and the median are quite close. The boxplot shows the presence of some outliers, but the the statistical summary might allow those outliers to be present . Here, the trend between quality and volatile acidity is ,much more pronounced. And, this fact is backed up by the number -0.390558 , which is the correlation between the columns. They are negatively correlated but there is some relation between them. To be precise, as the volatile acidity increases, quality seems to decrease.   ","ea59a692":"Looking at the correlations, we can say that all the colummns with higher correlation. So, we can do more Exploratory Data Analysis, but it would not be of much importance. So, next we will move forward to modelling.","f70a5a08":"First, we will train Logistic Regression models.","5545038a":"# MODEL CREATION","487b3450":"Plotting Every Variable against each other","adbf43be":"Again, there is a slight trend for sulphates and quality","38f4b707":"Now we will look at volatile acidity","b6306b42":"Looking at the above regression plot, we can infer that there is a clear trend,as the alcohol increases, the quality increases.","c3d4cb9c":"classification_model2 is a function, which is a slightly tweaked version of the above classification_model function. It basically trains on a splitted dataset, and then tests on test set.","0334353f":"So, first we will try to gain some insights from the data. \nHere are few of the insights I gained from this dataset - \n1. There are 1599 data points across 12 different columns.\n2. There are no missing values. \n3. The column we have to predict, 'quality' has minimum value 3 and maximum value 8. \nWe will find out more information about the data with Exploratory Data Analysis","391e698f":"From the histogram, the boxplot and the difference between the mean and median, we can say that this data is slightly skewed. \nNow, looking at the above regression plot, we can infer that there is a slight trend, which isn't very clear but, as the fixed acidity increases, the quality slightly increases.","2168b20b":"In all the models we have trained and tested, we have seen that many of the models tend to overfit the dataset, hence they have  high accuracy but their cross validation score is low. \n\nWhen I tried to reduce this difference, the accuracy decreased. The test set accuracy didn't reach 0.7 . \n\nCan anyone suggest any other way to increase the cross validation score and the accuracy on the test set?","b81ecf58":"# COMMENTS","d201e438":"Exploratory Data Analysis on this dataset becomes a bit difficult because all columns contain continuous data except our target column, which is quality.","2eb5e09c":"As said before, the quality column consists of classes from 3 to 8. Most of the wine is of quality 5 and 6. ","1fe12134":"The following is a K-Nearest Neighbors Model","d876d8f0":"Now, we will look at the alcohol column. It has the highest correlation","047a5a8a":"We will also train a random forest model","52fe5f1c":"Now, we will try some ensemble learning models."}}