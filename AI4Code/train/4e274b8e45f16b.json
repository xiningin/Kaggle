{"cell_type":{"37ba155b":"code","ffae5b0f":"code","482432d8":"code","3e6ab90b":"code","6c10c4b4":"code","81e9cba7":"code","1115c8d6":"code","c84e2c74":"code","28ea0a0f":"code","a17e641e":"code","833d1e8d":"code","e160c02f":"code","bbab5232":"code","b1e4d45c":"code","6651a42f":"code","c242744a":"code","15929b41":"code","4e9d72d5":"code","a4801042":"code","7b6597cc":"code","89b1284a":"code","b3a46f3d":"code","9f9136f6":"code","8969dc2a":"code","bd18b112":"code","683d5772":"code","42e14620":"code","c3a7a3f0":"code","ad2b6ec8":"code","c1777e2f":"code","5bede176":"code","672b2aed":"code","0f0bc59d":"code","728e291a":"code","c10c6779":"code","ae78e9ea":"code","fa372d22":"code","9359bc3d":"code","00aaf730":"code","5592c269":"code","ab360200":"code","c5953556":"code","39067b23":"code","d2052a22":"code","6f893b0c":"code","e2baae0c":"code","40886a97":"code","5d62c680":"code","0b43ffd9":"code","c9969875":"code","b0f02cee":"code","43e192e1":"code","fbfdfdb9":"code","8ba6dc3b":"code","3dc0c7b1":"code","c36e861a":"code","23f065bf":"code","8146916d":"markdown","66dee801":"markdown","003f7729":"markdown","ee595d24":"markdown","9b6e0332":"markdown","abd8ea36":"markdown","c01f3569":"markdown","c8eb1241":"markdown","ca249530":"markdown","f9d5672a":"markdown","caa41473":"markdown","b46a2c5b":"markdown","6734f089":"markdown","9e366a0c":"markdown","9ff469e7":"markdown","9a5388bd":"markdown","55d428b9":"markdown","eaff13ae":"markdown","05f2a716":"markdown","a2c7c7ac":"markdown","a86811f2":"markdown","f8f93df4":"markdown","abd1b539":"markdown","9fee527b":"markdown","0e82cc5d":"markdown","da2f54ff":"markdown","1b654a31":"markdown","55af2582":"markdown","23455024":"markdown","78ca5a14":"markdown"},"source":{"37ba155b":"import pandas as pd\nimport numpy as np\nimport math\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns\nfrom sklearn.linear_model import (LinearRegression, Ridge, Lasso,LogisticRegression)\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ffae5b0f":"df_train = pd.read_csv('..\/input\/restaurant-revenue-prediction\/train.csv.zip')\ndf_test = pd.read_csv('..\/input\/restaurant-revenue-prediction\/test.csv.zip')\ndf_train.head()","482432d8":"df_test.head()","3e6ab90b":"df_train.columns","6c10c4b4":"df_train.isnull().sum().max()","81e9cba7":"print(df_train['revenue'].describe())\nsns.distplot(a=df_train['revenue'], kde=True).set(xlabel='revenue', ylabel='P(revenue)')","1115c8d6":"df_train[df_train['revenue'] > 10000000 ]","c84e2c74":"# Elimination des valeurs extr\u00eames\ndf_train = df_train[df_train['revenue'] < 10000000 ]\ndf_train.reset_index(drop=True).head()","28ea0a0f":"# Cat\u00e9gorisation des caract\u00e9ristique entre valeurs num\u00e9riques et valeurs cat\u00e9goriques\nnumerical_features = df_train.select_dtypes([np.number]).columns.tolist()\ncategorical_features = df_train.select_dtypes(exclude = [np.number,np.datetime64]).columns.tolist()\nprint(categorical_features)\nprint(numerical_features)","a17e641e":"from datetime import date, datetime\n\ndef calculate_age(born):\n        born = datetime.strptime(born, \"%m\/%d\/%Y\").date()\n        today = date.today()\n        return today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n\ndf_train['Age'] = df_train['Open Date'].apply(calculate_age)\ndf_test['Age'] = df_test['Open Date'].apply(calculate_age)\n\n# Drop 'Open Date' column from Dataframes\ndf_train = df_train.drop('Open Date', axis=1)\ndf_test = df_test.drop('Open Date', axis=1)\n\n# Drop 'Id' column from Dataframes\n\n\ndf_train.head()","833d1e8d":"result = df_train.groupby([\"Age\"])[\"revenue\"].aggregate(np.median).reset_index()\n\nnorm = plt.Normalize(df_train[\"revenue\"].values.min(), df_train[\"revenue\"].values.max())\ncolors = plt.cm.Reds(norm(df_train[\"revenue\"])) \n\nplt.figure(figsize=(12,8))\nsns.barplot(x=\"Age\", y=\"revenue\", data=result, palette=colors)\nplt.ylabel('revenue', fontsize=12)\nplt.xlabel('Age', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()","e160c02f":"categorical_features = df_train.select_dtypes(exclude = [np.number,np.datetime64]).columns.tolist()\nfig, ax = plt.subplots(3, 1, figsize=(40, 30))\nfor variable, subplot in zip(categorical_features, ax.flatten()):\n    df_2 = df_train[[variable,'revenue']].groupby(variable).revenue.sum().reset_index()\n    df_2.columns = [variable,'total_revenue']\n    sns.barplot(x=variable, y='total_revenue', data=df_2 , ax=subplot)\n    subplot.set_xlabel(variable,fontsize=20)\n    subplot.set_ylabel('Total Revenue',fontsize=20)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(45)\n        label.set_size(20)\n    for label in subplot.get_yticklabels():\n        label.set_size(20)\nfig.tight_layout()","bbab5232":"n = len(df_train[numerical_features].columns)\nw = 3\nh = (n - 1) \/\/ w + 1\nfig, axes = plt.subplots(h, w, figsize=(w * 6, h * 3))\nfor i, (name, col) in enumerate(df_train[numerical_features].items()):\n    r, c = i \/\/ w, i % w\n    ax = axes[r, c]\n    col.hist(ax=ax)\n    ax2 = col.plot.kde(ax=ax, secondary_y=True, title=name)\n    ax2.set_ylim(0)\n\nfig.tight_layout()","b1e4d45c":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(df_train['Age'])\ndf_train['Age']=le.fit_transform(df_train['Age'])\nle.fit(df_train['City'])\ndf_train['City']=le.fit_transform(df_train['City'])\nle.fit(df_train['City Group'])\ndf_train['City Group']=le.fit_transform(df_train['City Group'])\nle.fit(df_train['Type'])\ndf_train['Type']=le.fit_transform(df_train['Type'])\n\ndf_train.dtypes==object","6651a42f":"df_train.info()","c242744a":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(df_test['Age'])\ndf_test['Age']=le.fit_transform(df_test['Age'])\nle.fit(df_test['City'])\ndf_test['City']=le.fit_transform(df_test['City'])\nle.fit(df_train['City Group'])\ndf_test['City Group']=le.fit_transform(df_test['City Group'])\nle.fit(df_test['Type'])\ndf_test['Type']=le.fit_transform(df_test['Type']) \ndf_test.info()","15929b41":"df_train.head()","4e9d72d5":"df_test.head()","a4801042":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans","7b6597cc":"X = StandardScaler().fit_transform(df_train)\nkmeans = KMeans(n_clusters=5)\nmodel = kmeans.fit(X)\nprint(\"model\\n\", model)","89b1284a":"centers = model.cluster_centers_\ncenters","b3a46f3d":"from sklearn.cluster import DBSCAN\ndbscan=DBSCAN(eps=3, min_samples=20)\ndbscan.fit(X)","9f9136f6":"from sklearn.cluster import DBSCAN\ndbscan=DBSCAN(eps=3, min_samples=20)\ndbscan.fit(X)","8969dc2a":"labels=dbscan.labels_\n#les valeurs extr\u00eames dont labels == -1\ndf_train[labels==-1]","bd18b112":"df_train[labels==-1].shape","683d5772":"# La cible\ny= df_train['revenue']","42e14620":"from sklearn.neighbors import NearestNeighbors\nfrom matplotlib import pyplot as plt\n\nneighbors = NearestNeighbors(n_neighbors=5)\nneighbors_fit = neighbors.fit(df_train)\ndistances, indices = neighbors_fit.kneighbors(df_train)\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)","c3a7a3f0":"# Import TSNE\nfrom sklearn.manifold import TSNE\nmodel = TSNE(learning_rate=10)\ntsne_features = model.fit_transform(X)\nxs = tsne_features[:,0]\nys = tsne_features[:,1]\nplt.scatter(xs,ys, c=y)\nplt.show()\nplt.clf()","ad2b6ec8":"fig, ax = plt.subplots(13, 3, figsize=(30, 35))\nfor variable, subplot in zip(numerical_features, ax.flatten()):\n    sns.regplot(x=df_train[variable], y=df_train['revenue'], ax=subplot)\n    subplot.set_xlabel(variable,fontsize=20)\n    subplot.set_ylabel('Revenue',fontsize=20)\nfig.tight_layout()","c1777e2f":"fig = plt.figure(figsize=(20,16))\ntarget_corr = df_train[df_train.columns[1:]].corr()['revenue']\norder_corr = target_corr.sort_values()\ny = pd.DataFrame(order_corr).index[:-1]\nx = pd.DataFrame(order_corr).revenue[:-1]\nsns.barplot(x, y, orient='h')\nplt.show()","5bede176":"corr_positive= target_corr[target_corr>0]\ncorr_negative= target_corr[target_corr<0]\ncorr_positive\n","672b2aed":"plt.figure(figsize=(45,25))\nmask = np.triu(np.ones_like(df_train.corr(), dtype=np.bool))\nsns.heatmap(df_train.corr(),annot=True, mask=mask)\nsns.set(font_scale=1.4)","0f0bc59d":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\ndf_train['revenue'] = np.log1p(df_train['revenue'])\nX, y = df_train.drop('revenue', axis=1), df_train['revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=150)\n\nparams_ridge = {\n    'alpha' : [.01, .1, .5, .7, .9, .95, .99, 1, 5, 10, 20],\n    'fit_intercept' : [True, False],\n    'normalize' : [True,False],\n    'solver' : ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n}\n\nridge_model = Ridge()\nridge_regressor = GridSearchCV(ridge_model, params_ridge, scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1)\nridge_regressor.fit(X_train, y_train)\nprint(f'Optimal alpha: {ridge_regressor.best_params_[\"alpha\"]:.2f}')\nprint(f'Optimal fit_intercept: {ridge_regressor.best_params_[\"fit_intercept\"]}')\nprint(f'Optimal normalize: {ridge_regressor.best_params_[\"normalize\"]}')\nprint(f'Optimal solver: {ridge_regressor.best_params_[\"solver\"]}')\nprint(f'Best score: {ridge_regressor.best_score_}')","728e291a":"ridge_model = Ridge(alpha=ridge_regressor.best_params_[\"alpha\"], fit_intercept=ridge_regressor.best_params_[\"fit_intercept\"], \n                    normalize=ridge_regressor.best_params_[\"normalize\"], solver=ridge_regressor.best_params_[\"solver\"])\nridge_model.fit(X_train, y_train)\ny_train_pred = ridge_model.predict(X_train)\ny_pred = ridge_model.predict(X_test)\nprint('Train r2 score: ', r2_score(y_train_pred, y_train))\nprint('Test r2 score: ', r2_score(y_test, y_pred))\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'Train RMSE: {train_rmse:.4f}')\nprint(f'Test RMSE: {test_rmse:.4f}')","c10c6779":"# Ridge Model Feature Importance\nridge_feature_coef = pd.Series(index = X_train.columns, data = np.abs(ridge_model.coef_))\nridge_feature_coef.sort_values().plot(kind = 'bar', figsize = (13,5));","ae78e9ea":"params_lasso = {\n    'alpha' : [.01, .1, .5, .7, .9, .95, .99, 1, 5, 10, 20],\n    'fit_intercept' : [True, False],\n    'normalize' : [True,False],\n}\n\nlasso_model = Lasso()\nlasso_regressor = GridSearchCV(lasso_model, params_lasso, scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1)\nlasso_regressor.fit(X_train, y_train)\nprint(f'Optimal alpha: {lasso_regressor.best_params_[\"alpha\"]:.2f}')\nprint(f'Optimal fit_intercept: {lasso_regressor.best_params_[\"fit_intercept\"]}')\nprint(f'Optimal normalize: {lasso_regressor.best_params_[\"normalize\"]}')\nprint(f'Best score: {lasso_regressor.best_score_}')","fa372d22":"lasso_model = Lasso(alpha=lasso_regressor.best_params_[\"alpha\"], fit_intercept=lasso_regressor.best_params_[\"fit_intercept\"], \n                    normalize=lasso_regressor.best_params_[\"normalize\"])\nlasso_model.fit(X_train, y_train)\ny_train_pred = lasso_model.predict(X_train)\ny_pred = lasso_model.predict(X_test)\nprint('Train r2 score: ', r2_score(y_train_pred, y_train))\nprint('Test r2 score: ', r2_score(y_test, y_pred))\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'Train RMSE: {train_rmse:.4f}')\nprint(f'Test RMSE: {test_rmse:.4f}')","9359bc3d":"# Lasso Model Feature Importance\nlasso_feature_coef = pd.Series(index = X_train.columns, data = np.abs(lasso_model.coef_))\nlasso_feature_coef.sort_values().plot(kind = 'bar', figsize = (13,5));","00aaf730":"from sklearn.neighbors import KNeighborsRegressor\n\nparams_knn = {\n    'n_neighbors' : [3, 5, 7, 9, 11],\n}\n\nknn_model = KNeighborsRegressor()\nknn_regressor = GridSearchCV(knn_model, params_knn, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\nknn_regressor.fit(X_train, y_train)\nprint(f'Optimal neighbors: {knn_regressor.best_params_[\"n_neighbors\"]}')\nprint(f'Best score: {knn_regressor.best_score_}')","5592c269":"knn_model = KNeighborsRegressor(n_neighbors=knn_regressor.best_params_[\"n_neighbors\"])\nknn_model.fit(X_train, y_train)\ny_train_pred = knn_model.predict(X_train)\ny_pred = knn_model.predict(X_test)\nprint('Train r2 score: ', r2_score(y_train_pred, y_train))\nprint('Test r2 score: ', r2_score(y_test, y_pred))\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'Train RMSE: {train_rmse:.4f}')\nprint(f'Test RMSE: {test_rmse:.4f}')","ab360200":"y = df_train['revenue']\ndf_train=df_train.drop('revenue', axis=1)\ndf_train","c5953556":"print(\"Shapes: Train set \", df_train.shape ,\", Test \",df_test.shape)\ndf_full = pd.concat([df_train,df_test])\nprint(\"Full dataset shapes: \", df_full.shape)","39067b23":"print('There are {} cities which restaurant location have been collected.'.format(len(df_full['City'].unique())))","d2052a22":"p_name = ['P'+str(i) for i in range(1,38)]\ndf_full","6f893b0c":"from sklearn.decomposition import PCA\npca = PCA().fit(df_full[p_name])\nplt.figure(figsize=(7,5))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of Components')\nplt.ylabel('Explained variance')\nplt.yticks(np.arange(0.1,1.1,0.05))\nplt.xticks(np.arange(0,41,2))\nplt.grid(True)","e2baae0c":"pca_list = ['pca'+str(i) for i in range(1,30,1)]\ndf_full[pca_list] = PCA(n_components=29).fit_transform(df_full[p_name])\ndf_full.drop(p_name,axis=1,inplace=True)\n","40886a97":"df=pd.get_dummies(df_full, dtype=float)","5d62c680":"# Get number of train sets\nnumTrain=df_train.shape[0]\n\ntrain = df[:numTrain]\ntest = df[numTrain:]","0b43ffd9":"sns.distplot(a=y, kde=True).set(xlabel='revenue', ylabel='P(revenue)')\n","c9969875":"from sklearn.model_selection import train_test_split\n\n# Split the data into train and test set\nX_train, X_test, y_train, y_test =  train_test_split(df_train,y,test_size=0.33,random_state=50)\nprint(\"Shapes: \", X_train.shape, X_test.shape, y_train.shape, y_test.shape)","b0f02cee":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom xgboost import XGBRegressor\nbest_estimators=[]\ny","43e192e1":"from sklearn.ensemble import VotingRegressor\nr1 = LinearRegression()\nrandomforest = RandomForestRegressor()\n\nvotingReg = VotingRegressor([('lr', r1), ('rf', randomforest)])\nvotingReg.fit(X_train, y_train)\ny_pred = votingReg.predict(X_test)\nRMSE_vo = math.sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\nbest_estimators.append([\"RMSE_voting\",RMSE_vo])\nRMSE_vo","fbfdfdb9":"randomforest = RandomForestRegressor()\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_test)\nRMSE_RF = math.sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\nprint(RMSE_RF)\nbest_estimators.append([\"RMSE_RandomForest\",RMSE_RF])","8ba6dc3b":"## parameters\nparams = {\n    \"n_estimators\": [10, 30, 50, 100],\n    \"learning_rate\": [.01, 0.1, 0.5, 0.9, 0.95, 1],\n    \"random_state\" : [42]\n}\n\n## AdaBoost Regressor\nAdaBoostR =   AdaBoostRegressor()\nAdaBoostR_grid = GridSearchCV(AdaBoostR, params, scoring='r2', cv=5, n_jobs=-1)\nAdaBoostR_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(AdaBoostR_grid.best_params_))\nprint(\"Best score: {}\".format(AdaBoostR_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"AdaBoostR\",AdaBoostR_grid.best_score_])","3dc0c7b1":"import xgboost\n\nxgb_reg = xgboost.XGBRegressor()\nxgb_reg.fit(X_train, y_train)\ny_pred = xgb_reg.predict(X_test)\nRMSE_XG=math.sqrt(mean_squared_error(y_test, y_pred))\nbest_estimators.append([\"RMSE_XGBoost\",RMSE_XG])\nRMSE_XG","c36e861a":"best_estimators","23f065bf":"randomforest = RandomForestRegressor()\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_test)\n# store the result\nsubmission_df=pd.DataFrame(\n{'Id':X_test.index,\n'Prediction':y_pred}\n)\nsubmission_df\nsubmission_df.to_csv('Submission.csv',index=False)\n","8146916d":"Comparaison de la performance des algorithmes","66dee801":"2- Visualisation de la matrice de corr\u00e9lation des caract\u00e9ristiques","003f7729":"5- XGBoost","ee595d24":"Selon les graphiques de r\u00e9gression lin\u00e9aire et la carte thermique des scores de corr\u00e9lation, ces attributs num\u00e9riques ont une relation lin\u00e9aire tr\u00e8s faible avec la variable cible \u00ab\u00a0Revenus\u00a0\u00bb. Le score de corr\u00e9lation le plus \u00e9lev\u00e9 est \u00ab\u00a0Age\u00a0\u00bb qui est de 0,2, tandis que les autres ont un score de corr\u00e9lation proche de 0. Cependant, il existe des groupes d'attributs qui ont une forte corr\u00e9lation les uns avec les autres.","9b6e0332":"2-b les caract\u00e9ristiques les plus corr\u00e9l\u00e9es avec la cible","abd8ea36":"**Affichage de la moyenne d'importance de chaque caract\u00e9ristique en utilisant les facteurs obtenus par les methodes d'emballage**","c01f3569":"3-a **Affichage de la moyenne d'importance de chaque caract\u00e9ristique en utilisant les facteurs obtenus par les methodes embarqu\u00e9es**\n","c8eb1241":"4-a Est-ce que la date d'ouverture d'un restaurant affecte t-elle la pr\u00e9dictioin final d'apr\u00e9s la visualisation","ca249530":"3- Visualisation des 5 Clusters en utilisant K-means","f9d5672a":"4- AdaBoost","caa41473":"5- Visualisation en utilisant t-SNE (T-Distributed Stochastic Neighbouring Entities)","b46a2c5b":"4-a Utilisation du KNN pour sp\u00e9cifier eps","6734f089":"1-a la d\u00e9t\u00e9ction des valeurs manquantes","9e366a0c":"# 1.2- Features Engineering","9ff469e7":"2-a la ville comportant le plus grand nombre de restaurants (ISTANBUL)\n\n2-c le type des restaurant le plus pr\u00e9sent dans le dataset \n\nle type de villes des restaurant ayant plus de revenue (Big Cities)\n\n5- le type de restaurant g\u00e9n\u00e9rant plus de revenus ave justification (FC : Food Court)","9a5388bd":"# 1.3 Apprentissage du mod\u00e9le et r\u00e9glage des hyper-param\u00e9tres","55d428b9":"# Restaurant revenue prediction\n*****\nChamps de donn\u00e9es\n**Id** : Id du restaurant.\n \n**Open Date** : date d'ouverture d'un restaurant\n\n**City** : Ville o\u00f9 se trouve le restaurant. Notez qu'il y a unicode dans les noms.\n\n**City Group** : Type de ville. Grandes villes, ou Autre.\n\n**Type** : Type de restaurant. FC\u00a0: Food Court, IL\u00a0: Inline, DT\u00a0: Drive Thru, MB\u00a0: Mobile\n\n**P1, P2 - P37** : Il existe trois cat\u00e9gories de ces donn\u00e9es obscurcies. Les donn\u00e9es d\u00e9mographiques sont recueillies aupr\u00e8s de fournisseurs tiers dot\u00e9s de syst\u00e8mes SIG. Ceux-ci incluent la population dans une zone donn\u00e9e, la r\u00e9partition par \u00e2ge et par sexe, les \u00e9chelles de d\u00e9veloppement. Les donn\u00e9es immobili\u00e8res concernent principalement le m2 de l'emplacement, la fa\u00e7ade avant de l'emplacement, la disponibilit\u00e9 des parkings. Les donn\u00e9es commerciales incluent principalement l'existence de points d'int\u00e9r\u00eat, notamment des \u00e9coles, des banques, d'autres op\u00e9rateurs de QSR.\n\n**Revenue**\u00a0: la colonne des revenus indique un revenu (transform\u00e9) du restaurant au cours d'une ann\u00e9e donn\u00e9e et est la cible d'une analyse pr\u00e9dictive. Veuillez noter que les valeurs sont transform\u00e9es afin qu'elles ne correspondent pas \u00e0 des valeurs r\u00e9elles en dollars.","eaff13ae":"4-b Visualtion de la d\u00e9marche","05f2a716":"1- Visualisation de la corr\u00e9lation des carct\u00e9ristiques avec la cible","a2c7c7ac":"La valeur id\u00e9ale de sera \u00e9gale \u00e0 la valeur de la distance au \u00ab creux du coude \u00bb, ou au point de courbure maximale. Ce point repr\u00e9sente le point d'optimisation o\u00f9 les rendements d\u00e9croissants ne valent plus le co\u00fbt suppl\u00e9mentaire. Ce concept de rendements d\u00e9croissants s'applique ici car si l'augmentation du nombre de clusters am\u00e9liore toujours l'ajustement du mod\u00e8le, cela augmente \u00e9galement le risque de surajustement.\nDans ce cas la valeur d'EPSILON : Eps Est : 18","a86811f2":"3- Random forest","f8f93df4":"On remarque d'apr\u00e9s les r\u00e9sultat de la visualisation que  l'age du restaurant le revenue sont corr\u00e9l\u00e9e tant que l'age est inf\u00e9rieur \u00e0 11 ans, mais pour les ages plus de 11 ans il ne sont plus corr\u00e9l\u00e9s","abd1b539":"2- Voting","9fee527b":"# 1.1-Analyse exploratoire et visualisation","0e82cc5d":"1- La regression logistique","da2f54ff":"L'histogramme montre que la cible disponible est une distribution asym\u00e9trique \u00e0 droite.\nIl y a des valeurs aberrantes qui ont des revenus > 10.000.000 affecteront les r\u00e9sultats. Ces valeurs aberrantes doivent \u00eatre supprim\u00e9es de l'ensemble de donn\u00e9es.","1b654a31":"3-b Visualisation du r\u00e9sultat","55af2582":"1-b la transformation des donn\u00e9es (donn\u00e9es discr\u00e9tes,... )","23455024":"pas de valeurs nulles","78ca5a14":"Transformation de la valeur Open Date \u00e0 Age une valeur enti\u00e9re"}}