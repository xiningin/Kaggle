{"cell_type":{"fc9a5553":"code","17ac59b1":"code","66a761d4":"code","914dc765":"code","f8ce0013":"code","6906988c":"code","8df95679":"code","0163a353":"code","f1b882dc":"code","c3edf8c4":"code","f2aea56c":"code","3295de2b":"code","61a1b4db":"code","89fad96e":"code","680163e1":"code","7bdbd710":"code","b4df3b36":"code","cf179b05":"code","56d777b3":"code","2f624bb4":"code","26b572ac":"code","61e31645":"code","9e0dec17":"code","e8a7dcb9":"code","401b7136":"code","e829972a":"code","54776d87":"code","2545c7d6":"code","0358064f":"code","c87be1d9":"code","e4d4ca4b":"code","eadb2aeb":"code","121287d9":"code","d00a0a58":"code","29a8a9b1":"code","2e131433":"code","bc2d4ed0":"code","d69cdbad":"code","e5aef55c":"markdown","96832892":"markdown"},"source":{"fc9a5553":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Create a list with the filepaths for training and testing\ntrain_dir = Path('..\/input\/fruit-and-vegetable-image-recognition\/train')\ntrain_filepaths = list(train_dir.glob(r'**\/*.jpg'))\n\ntest_dir = Path('..\/input\/fruit-and-vegetable-image-recognition\/test')\ntest_filepaths = list(test_dir.glob(r'**\/*.jpg'))\n\nval_dir = Path('..\/input\/fruit-and-vegetable-image-recognition\/validation')\nval_filepaths = list(test_dir.glob(r'**\/*.jpg'))\n\ndef proc_img(filepath):\n    \"\"\" Create a DataFrame with the filepath and the labels of the pictures\n    \"\"\"\n\n    labels = [str(filepath[i]).split(\"\/\")[-2] \\\n              for i in range(len(filepath))]\n\n    filepath = pd.Series(filepath, name='Filepath').astype(str)\n    labels = pd.Series(labels, name='Label')\n\n    # Concatenate filepaths and labels\n    df = pd.concat([filepath, labels], axis=1)\n\n    # Shuffle the DataFrame and reset index\n    df = df.sample(frac=1).reset_index(drop = True)\n    \n    return df\n\ntrain_df = proc_img(train_filepaths)\ntest_df = proc_img(test_filepaths)\nval_df = proc_img(val_filepaths)\n","17ac59b1":"print('-- Training set --\\n')\nprint(f'Number of pictures: {train_df.shape[0]}\\n')\nprint(f'Number of different labels: {len(train_df.Label.unique())}\\n')\nprint(f'Labels: {train_df.Label.unique()}')","66a761d4":"train_df.head(5)","914dc765":"df_unique = train_df.copy().drop_duplicates(subset=[\"Label\"]).reset_index()\n\n# Display some pictures of the dataset\nfig, axes = plt.subplots(nrows=6, ncols=6, figsize=(8, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(df_unique.Filepath[i]))\n    ax.set_title(df_unique.Label[i], fontsize = 12)\nplt.tight_layout(pad=0.5)\nplt.show()","f8ce0013":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)\n\ntrain_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=0,\n    rotation_range=30,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=val_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=0,\n    rotation_range=30,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","6906988c":"pretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\npretrained_model.trainable = False\n","8df95679":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(36, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    batch_size = 32,\n    epochs=5,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=2,\n            restore_best_weights=True\n        )\n    ]\n)","0163a353":"pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\nplt.title(\"Accuracy\")\nplt.show()","f1b882dc":"pd.DataFrame(history.history)[['loss','val_loss']].plot()\nplt.title(\"Loss\")\nplt.show()","c3edf8c4":"pred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\ny_test = [labels[k] for k in test_images.classes]","f2aea56c":"from sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test, pred)\nprint(f'Accuracy on the test set: {100*acc:.2f}%')","3295de2b":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (15,10))\nsns.heatmap(cf_matrix, \n            annot=True, \n            xticklabels = sorted(set(y_test)), \n            yticklabels = sorted(set(y_test)),\n            )\nplt.title('Normalized Confusion Matrix')\nplt.show()","61a1b4db":"fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","89fad96e":"import matplotlib.cm as cm\n\ndef get_img_array(img_path, size):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n    array = tf.keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size \"size\"\n    array = np.expand_dims(array, axis=0)\n    return array\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = tf.keras.preprocessing.image.load_img(img_path)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n#     display(Image(cam_path))\n    \n    return cam_path\n    \npreprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\ndecode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n\nlast_conv_layer_name = \"Conv_1\"\nimg_size = (224,224)\n\n# Remove last layer's softmax\nmodel.layers[-1].ativation = None","680163e1":"fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    img_path = test_df.Filepath.iloc[i]\n    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n    cam_path = save_and_display_gradcam(img_path, heatmap)\n    ax.imshow(plt.imread(cam_path))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","7bdbd710":"import pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport transformers #huggingface transformers library\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","b4df3b36":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","cf179b05":"df = pd.read_json('\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json', lines = True)\ndf.head()","56d777b3":"df.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)\nprint(f\"The dataset contains { df.category.nunique() } unique categories\")","2f624bb4":"encoder = LabelEncoder()\ndf['categoryEncoded'] = encoder.fit_transform(df['category'])\n\ndf['headline'] = df['headline'].apply(lambda headline: str(headline).lower())\ndf['short_description'] = df['short_description'].apply(lambda descr: str(descr).lower())\n\ndf['descr_len'] = df['short_description'].apply(lambda x: len(str(x).split()))\ndf['headline_len'] = df['headline'].apply(lambda x: len(str(x).split()))\n\ndf.describe()","26b572ac":"sns.distplot(df['descr_len'])\nplt.title('Description Number of Words')\nplt.show()","61e31645":"df['short_description'] = df['headline'] + df['short_description']","9e0dec17":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","e8a7dcb9":"tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')","401b7136":"X_train,X_test ,y_train,y_test = train_test_split(df['short_description'], df['categoryEncoded'], random_state = 2020, test_size = 0.3)","e829972a":"Xtrain_encoded = regular_encode(X_train.astype('str'), tokenizer, maxlen=80)\nytrain_encoded = tf.keras.utils.to_categorical(y_train, num_classes=40,dtype = 'int32')\nXtest_encoded = regular_encode(X_test.astype('str'), tokenizer, maxlen=80)\nytest_encoded = tf.keras.utils.to_categorical(y_test, num_classes=40,dtype = 'int32')","54776d87":"def build_model(transformer, loss='categorical_crossentropy', max_len=512):\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    #adding dropout layer\n    x = tf.keras.layers.Dropout(0.3)(cls_token)\n    #using a dense layer of 40 neurons as the number of unique categories is 40. \n    out = tf.keras.layers.Dense(40, activation='softmax')(x)\n    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n    #using categorical crossentropy as the loss as it is a multi-class classification problem\n    model.compile(tf.keras.optimizers.Adam(lr=3e-5), loss=loss, metrics=['accuracy'])\n    return model","2545c7d6":"with strategy.scope():\n    transformer_layer = transformers.TFAutoModel.from_pretrained('bert-large-uncased')\n    model = build_model(transformer_layer, max_len=80)\nmodel.summary()","0358064f":"BATCH_SIZE = 32*strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE \ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((Xtrain_encoded, ytrain_encoded))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(Xtest_encoded)\n    .batch(BATCH_SIZE)\n)","c87be1d9":"n_steps = Xtrain_encoded.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    epochs=10\n)","e4d4ca4b":"preds = model.predict(test_dataset,verbose = 1)\n#converting the one hot vector output to a linear numpy array.\npred_classes = np.argmax(preds, axis = 1)","eadb2aeb":"encoded_classes = encoder.classes_\n#mapping the encoded output to actual categories\npredicted_category = [encoded_classes[x] for x in pred_classes]\ntrue_category = [encoded_classes[x] for x in y_test]","121287d9":"result_df = pd.DataFrame({'description':X_test,'true_category':true_category, 'predicted_category':predicted_category})\nresult_df.head()","d00a0a58":"print(f\"Accuracy is {sklearn.metrics.accuracy_score(result_df['true_category'], result_df['predicted_category'])}\")","29a8a9b1":"print(f\"Accuracy is {sklearn.metrics.accuracy_score(result_df['true_category'], result_df['predicted_category'])}\")","2e131433":"result_df[result_df['true_category']!=result_df['predicted_category']]","bc2d4ed0":"confusion_mat = confusion_matrix(y_true = true_category, y_pred = predicted_category, labels=list(encoded_classes))","d69cdbad":"df_cm = pd.DataFrame(confusion_mat, index = list(encoded_classes),columns = list(encoded_classes))\nplt.rcParams['figure.figsize'] = (20,20)\nsns.heatmap(df_cm)","e5aef55c":"Transfer Lerning for Text data","96832892":"This notebook shows transfer learning with an image dataset and a text dataset.\n\nThe transfer learning for image data set is forked from https:\/\/www.kaggle.com\/databeru\/fruit-and-vegetable-classification wherease the one for text is forked from https:\/\/www.kaggle.com\/foolofatook\/news-classification-using-bert.\n\nI'll be making changes to both of these models pretty soon"}}