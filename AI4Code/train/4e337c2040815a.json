{"cell_type":{"23b4e3a6":"code","393ac017":"code","06ae1abe":"code","522eb1bf":"code","27bb13e0":"code","af97b558":"code","86c45d44":"code","c014380b":"code","6fa0a075":"code","d962117e":"code","045a1ca1":"code","521be205":"code","bb81fb17":"code","5730e4ba":"code","a448e60d":"code","d295df49":"code","4bea528f":"code","581242ac":"markdown","80ce3287":"markdown","27ba402f":"markdown","c5dcd7b7":"markdown","e5050dcb":"markdown","b421be81":"markdown","1e14285a":"markdown","79934702":"markdown","267ab2a6":"markdown","04ab9d11":"markdown","6d6a9728":"markdown","ddd5db57":"markdown"},"source":{"23b4e3a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # matplotlib\nimport plotly.graph_objects as go\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","393ac017":"# read the file that I uploaded in Kaggle Data - the first try with the SEA data\ndataset = pd.read_csv(\"..\/input\/abdata\/reviews_SEA.csv\")\ndf = pd.DataFrame(dataset)\n\n# print to check the data\ndf.tail()","06ae1abe":"# The dataset file is too large. I will delete listing_id, id, and review_id columns, and will only bring the rows with '\/20'\n\n# Remove columns\ndf = df.drop(columns=['listing_id', 'id', 'reviewer_id'])\n\n# Extract year and months from 'date'\n# Extract the rows only with the recent entries in Apr 2020, because I was worried if it wouldn't work for NY data due to the file size.\n# But somehow these codes don't work.\n# df['year'] = pd.DatetimeIndex(df['date']).year\n# df['month'] = pd.DatetimeIndex(df['date']).month\n# df = df[df['year'] == '2020']\n\n# Second thought: just directly sort the data by date - this actually takes huge amount of time...\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values(by = 'date')","522eb1bf":"# drop the blank rows\ndf = df.dropna(how='any')","27bb13e0":"# get the last 100 & 75 datasets\n\n# to train the model\nsmaller = df.tail(300)\n\n# to analyze the data\nsmaller1 = df.tail(75)\nsmaller1","af97b558":"# save the new file\n\nsmaller.to_csv('Reviews_SEA_300.csv')\nsmaller1.to_csv('Reviews_SEA_75.csv')","86c45d44":"# read the file\ndataset2 = pd.read_csv(\"..\/input\/abdata\/reviews_NY_rev.csv\")\ndf2 = pd.DataFrame(dataset2)\n\n# Drop the unnecessary columns\ndf2 = df2.drop(columns=['listing_id', 'id', 'reviewer_id'])\n\n# drop the rows with blank entries\ndf2 = df2.dropna(how='any')\n\n# Sort the data by date - this actually takes huge amount of time...\ndf2['date'] = pd.to_datetime(df2['date'])\ndf2 = df2.sort_values(by = 'date')","c014380b":"# get the last 75 datasets\n\nsmaller2 = df2.tail(75)\nsmaller2","6fa0a075":"# save the new file\n\nsmaller2.to_csv('Reviews_NY_75.csv')","d962117e":"# read the processed files (analysis dataset) that I uploaded on the kaggle account\ndataset3 = pd.read_csv(\"..\/input\/abdata\/processed_batch_SEA_All.csv\")\ndf3 = pd.DataFrame(dataset3)\n\ndataset4 = pd.read_csv(\"..\/input\/abdata\/processed_batch_NY_All.csv\")\ndf4 = pd.DataFrame(dataset4)\n\n# check\ndf3","045a1ca1":"# dataset manipulation\n\n# Rename some columns\ndf3.rename(columns={\"comments\": \"Comments\", \"Classification\": \"Sentiment\", \"Classification.1\": \"Aspect\"}, inplace=True)\ndf4.rename(columns={\"comments\": \"Comments\", \"Classification\": \"Sentiment\", \"Classification.1\": \"Aspect\"}, inplace=True)\n\n# split the Aspect column into multiple columns \/ leave comments, sentiment, aspects only\ndf3 = pd.concat([df3['Comments'], df3['Sentiment'], df3['Aspect'].str.split(':', expand=True)], axis=1)\ndf4 = pd.concat([df4['Comments'], df4['Sentiment'], df4['Aspect'].str.split(':', expand=True)], axis=1)\n\ndf3","521be205":"# unpivot the aspects using melt to have them in one column\n\ndf3 = pd.melt(df3, id_vars =['Comments', 'Sentiment'], var_name='Order(no meaning)', value_name='Aspects')\ndf4 = pd.melt(df4, id_vars =['Comments', 'Sentiment'], var_name='Order(no meaning)', value_name='Aspects')\n\ndf3","bb81fb17":"# Clean the rows with no aspect data\n\n# drop column 'Order(no meaning)'\ndf3 = df3.drop(columns=['Order(no meaning)'])\ndf4 = df4.drop(columns=['Order(no meaning)'])\n\n# drop rows with 'None'\ndf4 = df4.dropna(how='any',axis=0) \ndf3 = df3.dropna(how='any',axis=0) \n\n# try\ndf3","5730e4ba":"# Use plotly to visualize - seattle\n\nx1 = df3.loc[df3['Sentiment'] == 'Positive', 'Aspects']\nx2 = df3.loc[df3['Sentiment'] == 'Negative', 'Aspects']\n\ntrace1 = go.Histogram(x=x1, name='Positive',opacity=0.75)\ntrace2 = go.Histogram(x=x2, name = 'Negative',opacity=0.75)\ndata = [trace1, trace2]\n\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(\n    xaxis={'categoryorder':'total descending'},\n    title='Distribution of Aspects Based on Positive\/Negative Sentiments - Seattle',\n    barmode='overlay'\n)\n\nfig.show()","a448e60d":"# Use plotly to visualize - New York\n\nx3 = df4.loc[df4['Sentiment'] == 'Positive', 'Aspects']\nx4 = df4.loc[df4['Sentiment'] == 'Negative', 'Aspects']\n\ntrace3 = go.Histogram(x=x3, name='Positive',opacity=0.75)\ntrace4 = go.Histogram(x=x4, name = 'Negative',opacity=0.75)\ndata2 = [trace3, trace4]\n\nfig2 = go.Figure(data=data2, layout=layout)\nfig2.update_layout(\n    xaxis={'categoryorder':'total descending'},\n    title='Distribution of Aspects Based on Positive\/Negative Sentiments - New York',\n    barmode='overlay'\n)\n\nfig2.show()","d295df49":"# Draw pie charts to check the portion of the aspects\n\n# count the aspects \naspect = df3['Aspects'].value_counts()\n\n# plotting it on the pie chart\nlabel_aspect = aspect.index\nsize_aspect = aspect.values\n\ncolors = ['Oxygen', 'Hydrogen', 'mediumturquoise', 'gold', 'crimson']\n\naspect_piechart = go.Pie(labels = label_aspect,\n                         values = size_aspect,\n                         marker = dict(colors = colors),\n                         name = 'Aspects of Airbnb stay in Seattle', hole = 0.3)\n\ndataset = [aspect_piechart]\n\nfig3 = go.Figure(data = dataset,layout = layout)\nfig3.update_layout(title_text='Aspects of Airbnb stay in Seattle', title_x=0.5)\n\nfig3.show()","4bea528f":"# count the aspects \naspect2 = df4['Aspects'].value_counts()\n\n# plotting it on the pie chart\nlabel_aspect2 = aspect2.index\nsize_aspect2 = aspect2.values\n\ncolors = ['Oxygen', 'Hydrogen', 'mediumturquoise', 'gold', 'crimson']\n\naspect_piechart2 = go.Pie(labels = label_aspect2,\n                         values = size_aspect2,\n                         marker = dict(colors = colors),\n                         name = 'Aspects of Airbnb stay in Seattle', hole = 0.3)\n\ndataset2 = [aspect_piechart2]\n\nfig4 = go.Figure(data = dataset2,layout = layout)\nfig4.update_layout(title_text='Aspects of Airbnb stay in New York', title_x=0.5)\n\nfig4.show()","581242ac":"# 3. Process\n\n## 1) Data preparation\n\nFirst, I will curate only the most recent 1,000 records from each data file of the three cities.","80ce3287":"## 2) Creating a new aspect model on MonkeyLearn\n\nI started using MonkeyLearn utilizing the dataset I created.\n\n\u2022 Create a model -> Choose Classifier -> Choose Topic Classification\n\n![image.png](attachment:image.png)\n","27ba402f":"# [Project 2 - Aspect-based Sentiment Analysis on Airbnb Reviews]\n\n\n# 1. Overview\n\nFor my second mini project, I wanted to work on text datasets to try ML techniques. After searching for data to bring a meaningful analysis, I picked the airbnb review data to conduct aspect-based setimant analysis. Airbnb already provides star ratings in 6 aspects, so I wanted to utilize this categorization to conduct aspect-based sentiment analysis to find out the general interests and satisfactions of the travelers in the cities of Seattle and New York.\n\n![image.png](attachment:image.png)\n(Airbnb review example - star ratings)\n\n\n### Analysis goals:\n\n1) Data visualization for each aspect's sentiment analysis (Seattle and NY)\n\n2) Data visualization for comparison in the interest of the aspects between the two cities\n\n3) Data visualization for comparison in the satisfaction(good sentiment) of the aspects between the two cities\n","c5dcd7b7":"# 6. Findings from the figures:\n\n\n### 1. There are almost no negative reviews.\nIn seattle data, there are only 3 negative responses, and in New York data, there are absolutely no negative reviews, according to the data. This sentiment analysis might be not very accurate because I used a demo model on MonkeyLearn. However, this result is fairly trustworthy, as I already expected somewhat similar result to this when I was training the model. I couldn't really find negative reviews while training more than 300 reviews. This is very surprising that most of the airbnb units are very pleasant and not many visitors have complaints. \n\n### 2. Airbnb's categorization (tags) needs improvement.\nAirbnb uses 6 categories to rate: accuracy, check-in, cleanliness, communication, location, and value. However, in the written reviews, people barely wrote about accuracy, check-in, and value. They mostly care about comfort of the place and communication with hosts. Also, I newly created decorations & view tag, which fair amount of reviews mentioned. Airbnb's current categorization cannot describe how beautiful the places are, but it seems an important feature to the visitors.\n\n### 3. No big difference in the portion of review tags for the two cities\nThe ranks of the number of tags mentioned in the reviews are almost the same in the two cities. It can be explained in two ways: first, Airbnb locally manages well to maintain the quality of the places and provides similar quality experiences to visitors, and second, the two cities are alike in their environments. I might be able to find something different if I process the new data of totally different areas such as Istanbul and Porto. To prove this, I will need to run the model with more data further.","e5050dcb":"I downloaded the output file and checked it works for MonkeyLearn. So I moved further to do the same job for the NY file.","b421be81":"## 5) Data visualization for analysis\n\nFirst, I will manipulate the dataset for data visualization.","1e14285a":"Second, Data visualization - I use plotly to depict bar charts and pie charts.","79934702":"## 3) Training a new aspect model on MonkeyLearn\n\n\u2022 I trained the model for a couple of hours. After a couple of tagging practices, I changed from 6 tags to 8 tags to improve the classification for better accuracy. I stopped after completing 300 texts to tag as it seemed it's quite well-trained at this point.\n\n\u2022 Tags: Accuracy \/ Check-in \/ Cleanliness \/ Comfort & Amenities \/ Communication & Friendliness \/ Decorations & View \/ Value\n\n\u2022 Most reviews fit in these tags, except very short ones simply describing such as \"Great place\" and \"Nice spot.\"\n \n![image.png](attachment:image.png)","267ab2a6":"# 2. Method \/ Tools\n\n### The scope of the data:\n\u2022 The dataset URL: http:\/\/insideairbnb.com\/get-the-data.html\n\n\u2022 The dataset I use: reviews.csv.gz (Seattle and New York)\n\n\u2022 Due to the limited capacity on MonkeyLearn which allows only 300 queries and 1 new model for a free account, I decided to utilize the latest 75 review data records to run the models 4 times. At first, I wanted to explore three different cities including LA, but due to the limitation, I had to narrow my scope to compare the two cities only.\n\n### Tool that I use:\n\u2022 Kaggle: I utilize this notebook further for data preparation and data visualization \n\n\u2022 MonkeyLearn (https:\/\/monkeylearn.com\/): I conduct aspect-based sentiment analysis on MonkeyLearn.\n\n### Method & project plan:\n\n1. Since the data files are extremely large, I will first reshape and prepare to train the model and conduct analysis. I am using this kaggle notebook to manipulate the data files.\n\n2. Create a new aspect model on MonkeyLearn: I wanted to create a new sentiment analysis model as well, but due to the limitation on the account, I only created the aspect model and decided to use demo sentiment model on MonkeyLearn.\n(Reference: https:\/\/monkeylearn.com\/blog\/aspect-based-sentiment-analysis\/)\n\n3. Train the new model\n\n4. Run the models on MonkeyLearn\n\n5. Visualize the data in the Kaggle notebook","04ab9d11":"\u2022 I imported data file and defined data tags. At first, I used the same tags that Airbnb currently uses.\n\n![image.png](attachment:image.png)","6d6a9728":"# 7. Conclusion & Reflection\n\nWhen I initially planned this project, I wanted to focus more on the sentiment analysis, and aspect analysis was an addition to the sentiment analysis to bring more rich and meaningful conclusion. However, Airbnb's reviews are excessively positive, and sentiment analysis could not give an effective and valuable insight. Another problem I had during this project was that the free acount of MonkeyLearn only allows one customized model, which made me use a demo model to conduct sentiment analysis.\n \n\nFor the future research on this project, there are three things that need to be prepared:\n\n1) **New sentiment analysis model**: I would like to create a new model and train it myself to improve accuracy.\n\n2) **Richer dataset with more texts processed**: To generalize the final result, more texts are needed to be analyzed.\n\n3) **Dataset from different cities in another region globally**: as mentioned before, it is likely that this will help to show different results on the graph.\n\n\nEven though the use of MonkeyLearn this time was very limited, I enjoyed it a lot and thought this is a very powerful tool that enables me to do ML easily, especially on the text data. As a former PM, I only used numerical data to analyze and forcast sales revenue and market share, but the experience with MonkeyLearn taught me that there are numerous ways to conduct many different types of analysis using ML techniques.","ddd5db57":"## 4) Running the models to get processed datasets from MonkeyLearn\n\nA. First, I batched the dataset files of Seattle and NY I created on this notebook to the demo sentiment model on MonkeyLearn. (Again, I couldn't build another model myself due to the account limitation...)\n\nB. After that, I batched the processed files to the aspect model I created and uploaded in Kaggle."}}