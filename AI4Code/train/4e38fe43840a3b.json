{"cell_type":{"2e855366":"code","f2924938":"code","59c3944d":"code","2d9f4cd0":"code","77525c09":"code","86ad7052":"code","5a670296":"code","5676a12e":"code","4ff39336":"code","88991e48":"code","5e2f4e72":"code","b269a9cb":"code","e10249a8":"code","de40489d":"code","6d7fed5f":"code","c5050db2":"code","fa50a0f9":"code","4866d09a":"code","e07f4ded":"code","3ebd4893":"code","5b25df12":"code","f4ce2edc":"code","3d172953":"code","e61c0d1d":"code","f581fe9e":"code","823b2494":"code","bce2cf15":"code","ae9ababf":"code","3cffcbc3":"code","3c22945e":"code","732d35f4":"code","f382913b":"code","e9c422f3":"code","f535d14d":"code","ce191812":"code","1727fe36":"code","73fd465d":"code","e109529d":"code","ce3476a1":"code","1c45b907":"markdown","623f4422":"markdown","71cb187c":"markdown","e469bbcb":"markdown","a364c07c":"markdown","3dcce0ff":"markdown","5a800b94":"markdown","4d8a27fd":"markdown","d66f2c0d":"markdown","bfb601ec":"markdown","5ba42027":"markdown","46ca0375":"markdown","a495c32f":"markdown","caf535af":"markdown","5381d93c":"markdown","cbd270af":"markdown","a0c78d6d":"markdown","77a92599":"markdown","d34796a2":"markdown","e1bd9ee6":"markdown","d35bbfb2":"markdown","c1dd25a9":"markdown","1e857b19":"markdown","da53c398":"markdown","9ec520fc":"markdown","ca745632":"markdown","c26caee9":"markdown","44599fbd":"markdown","2f326e6b":"markdown","ed89433d":"markdown","955be913":"markdown","38f24587":"markdown","fd489e19":"markdown","8b7052be":"markdown","5a63f43c":"markdown","327a561a":"markdown","505de658":"markdown","bc79eebc":"markdown"},"source":{"2e855366":"import pandas as pd\nfrom pandas.plotting import scatter_matrix\nfrom pandas import to_datetime\nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import absolute\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport plotly.express as px\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nfrom scipy.stats import norm, skew\nfrom scipy import stats\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, StratifiedKFold, RepeatedKFold, KFold, GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom xgboost import XGBRegressor, plot_importance \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom cycler import cycler\nimport matplotlib as mpl\nfrom yellowbrick.regressor import PredictionError, ResidualsPlot\nfrom yellowbrick.model_selection import learning_curve, ValidationCurve, FeatureImportances, CVScores\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\nimport datetime\nimport operator\nimport random\nimport math\nimport time\n# to improve matplotlib graphs\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('retina')\npio.renderers.default='notebook'\nimport warnings\nwarnings.simplefilter(\"ignore\")\nwarnings.filterwarnings('ignore')\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\nmpl.rcParams['figure.dpi'] = 300","f2924938":"#Importing the csv file\ndf = pd.read_csv(\"..\/input\/covid19-socioeconomic-and-health-disparities\/data.csv\")\n","59c3944d":"# Print the shape of dataframe\nprint(\"Dimension of this datasets (rows, columns) is: \", df.shape)\nprint()","2d9f4cd0":"print(\"The first few rows: \")\ndf.head()","77525c09":"print(df.info(verbose=True))","86ad7052":"df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")","5a670296":"# Brief statistical description of the data\ndf.describe().T","5676a12e":"#https:\/\/towardsdatascience.com\/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b\n# Count the missing values.\nmiss_values = df.columns[df.isnull().any()]\nprint(f\"Missing values:\\n{df[miss_values].isnull().sum()}\")\nnull_values = df.columns[df.isna().any()]\nprint(f\"Null values:\\n{df[null_values].isna().sum()}\")\ndf_missing = df","4ff39336":"# Dropping columns from Yougov source as they are mostly empty\ndf.drop([col for col in df.columns if \"weekly\" in col], axis=1, inplace=True)\ndf.drop([col for col in df.columns if \"yougov\" in col], axis=1, inplace=True)\ndf.drop([col for col in df.columns if \"ox_m1_wildcard\" in col], axis=1, inplace=True)","88991e48":"# pandas drop columns using list of column names\ndf.drop([\"iso_code\", \"jhu_confirmed\", \"jhu_deaths\", \"owid_new_tests_smoothed\", \"owid_new_tests_per_thousand\", \"owid_new_tests_smoothed_per_thousand\", \n        \"owid_tests_per_case\", \"owid_positive_rate\", \"owid_tests_units\", \"ox_confirmed_cases\", \"owid_total_tests\", \"owid_new_tests\", \"owid_total_tests_per_thousand\",\n        \"ox_confirmed_deaths\", \"marioli_ci_65_u\", \"marioli_ci_65_l\", \"marioli_ci_95_u\", \"marioli_ci_95_l\", \"sdsn_effective_reproduction_rate_smoothed\",\n        \"sdsn_positive_test_rate_smoothed\", \"sdsn_new_cases_per_million_smoothed\", \"sdsn_new_deaths_per_million_smoothed\", \"owid_total_cases_per_million\",\n        \"owid_total_deaths_per_million\", \"ox_c1_flag\", \"ox_c2_flag\", \"ox_c3_flag\", \"ox_c4_flag\", \"ox_c5_flag\", \"ox_c6_flag\", \"ox_c7_flag\", \"ox_e1_flag\", \"ox_h1_flag\",\n        \"owid_handwashing_facilities\", \"sdsn_overall_transmission\", \"google_mobility_change_grocery_and_pharmacy\", \"google_mobility_change_parks\",\n        \"google_mobility_change_transit_stations\", \"google_mobility_change_retail_and_recreation\", \"google_mobility_change_residential\",\n        \"google_mobility_change_workplaces\", \"marioli_effective_reproduction_rate\",\"ox_stringency_index_for_display\", \"ox_stringency_legacy_index_for_display\",\n        \"ox_government_response_index_for_display\", \"ox_containment_health_index_for_display\", \"ox_economic_support_index_for_display\", \"ox_stringency_legacy_index\",\n        \"owid_stringency_index\",\"owid_aged_70_older\"], axis=1, inplace=True)","5e2f4e72":"df = df.loc[df[\"owid_population\"] > 1000000]","b269a9cb":"top_n_country_names = df.groupby(\"country\").max()[\"owid_total_deaths\"].nlargest(5).keys()\ndf = df.loc[df['country'].isin(top_n_country_names)]","e10249a8":"#Find Start and finish Date\nstart_date = df.groupby('country').min()['date'].min()\nend_date = df.groupby('country').max()['date'].max()\ndate_range = pd.date_range(start_date, end_date, freq='D')\nprint (\"Start Date : \", start_date)\nprint (\"End Date : \", end_date)","de40489d":"df = df[~(df['date'] < '2020-04-01')]\nstart_date = df.groupby('country').min()['date'].min()\ndate_range = pd.date_range(start_date, end_date, freq='D')\nprint (\"Start Date : \", start_date)","6d7fed5f":"# Recovered cases = daily - deaths - recovered\ndf['owid_new_recovered_per_million'] = df['owid_new_cases_per_million'] - df['owid_new_deaths_per_million']\n# Recovered cases = daily - deaths - recovered\ndf['owid_new_recovered'] = df['owid_new_cases'] - df['owid_new_deaths']","c5050db2":"## 2. Forward Fill --------------------------\ndf = df.fillna(method='ffill').fillna(method='bfill')","fa50a0f9":"fig, axes = plt.subplots(2, 1, figsize=(22, 15));\nfig.suptitle('Visualisation of dataset for missing values (in yellow) before and after data imputation', fontsize=32, weight=\"bold\");\nplt.subplot(2,1,1);\nsns.heatmap(df_missing.isnull(),xticklabels=False,cbar=False,cmap='summer');\nplt.title('Before imputation: missing values (yellow)', size=30, weight=\"bold\");\nplt.subplot(2,1,2);\nsns.heatmap(df.isnull(),xticklabels=True,cbar=False,cmap='summer');\nplt.title('After imputation: no missing values', size=30, weight=\"bold\");\nplt.subplots_adjust(top=0.92);\nplt.subplots_adjust(wspace=0, hspace=0.1)","4866d09a":"sns.set_style('white');\nfig, axes = plt.subplots(2, 4, figsize=(24, 10));\nfig.suptitle('COVID-19 mortality, survival, recovery, health indices and governments restrictions', fontsize=28, weight=\"bold\");\nplt.subplot(2,4,1);\nplt.gca().set_title('COVID-19 mortality\/million', fontsize=22, weight=\"bold\");\nst = sns.stripplot(x = 'country', y = 'owid_new_deaths_per_million', data = df, jitter=0.25, split=True, linewidth=0.5, palette = \"husl\");\nbox = sns.boxplot(palette=['#BBBBBB','#DDDDDD'], linewidth=1, x = 'country', y = 'owid_new_deaths_per_million', data = df,showfliers=False);\nbox.set(xlabel=None);\nbox.set(ylabel=\"Daily mortality\/million\");\nplt.subplot(2,4,2);\nplt.gca().set_title('COVID-19 recovery\/million', fontsize=22, weight=\"bold\");\nst = sns.stripplot(x = 'country', y = 'owid_new_recovered_per_million', data = df, jitter=0.25, split=True, linewidth=0.5, palette = \"husl\");\nbox = sns.boxplot(palette=['#BBBBBB','#DDDDDD'], linewidth=1, x = 'country', y = 'owid_new_recovered_per_million', data = df,showfliers=False);\nbox.set(xlabel=None);\nbox.set(ylabel=\"Daily recovery\/million\");\nplt.subplot(2,4,3);\nplt.gca().set_title('Health containment policy', fontsize=22, weight=\"bold\");\nst = sns.stripplot(x = 'country', y = 'ox_containment_health_index', data = df, jitter=0.25, split=True, linewidth=0.5, palette = \"husl\");\nbox = sns.boxplot(palette=['#BBBBBB','#DDDDDD'], linewidth=1, x = 'country', y = 'ox_containment_health_index', data = df,showfliers=False);\nbox.set(xlabel=None);\nbox.set(ylabel=\"Containment health index\");\nplt.subplot(2,4,4);\nplt.gca().set_title('Governments stringency policy', fontsize=22, weight=\"bold\");\nst = sns.stripplot(x = 'country', y = 'ox_stringency_index', data = df, jitter=0.25, split=True, linewidth=0.5, palette = \"husl\");\nbox = sns.boxplot(palette=['#BBBBBB','#DDDDDD'], linewidth=1, x = 'country', y = 'ox_stringency_index', data = df,showfliers=False);\nbox.set(xlabel=None);\nbox.set(ylabel=\"Government policy stringency\");\nplt.subplot(2,4,5);\nplt.gca().set_title('Extreme poverty', fontsize=22, weight=\"bold\");\nbar = sns.barplot(x = 'country', y = 'owid_extreme_poverty', data = df, palette = \"husl\");\nbar.set(xlabel=None);\nbar.set(ylabel=\"Extreme povery\");\nplt.subplot(2,4,6);\nplt.gca().set_title('Life expectancy', fontsize=22, weight=\"bold\");\nbar = sns.barplot(x = 'country', y = 'owid_life_expectancy', data = df, linewidth=0.5, palette = \"husl\");\nbar.set(xlabel=None);\nbar.set(ylabel=\"Average life expectancy\");\nplt.subplot(2,4,7);\nplt.gca().set_title('Age 65 or over\/million', fontsize=22, weight=\"bold\");\nbar = sns.barplot(x = 'country', y = 'owid_aged_65_older', data = df, linewidth=0.5, palette = \"husl\");\nbar.set(xlabel=None);\nbar.set(ylabel=\"Age >= 65 per million\");\nplt.subplot(2,4,8);\nplt.gca().set_title('Hospital beds\/thousand', fontsize=22, weight=\"bold\");\nbar = sns.barplot(x = 'country', y = 'owid_hospital_beds_per_thousand', data = df, linewidth=0.5, palette = \"husl\");\nbar.set(xlabel=None);\nbar.set(ylabel=\"Number of hospital beds\/thousand\");\n","e07f4ded":"#https:\/\/www.kaggle.com\/therealcyberlord\/coronavirus-covid-19-visualization-prediction\/notebook\ndf1 = df[(df.iloc[:,2:-1] >= 0).all(1)]\nfig, axes = plt.subplots(2, 1, figsize=(24, 10))\nfig.suptitle('Daily COVID-19 survival and mortality (per million) in top 5 affected countries', fontsize=30, weight=\"bold\");\nplt.subplot(2,1,1)\nplt.bar(df1.date, df1.owid_new_recovered_per_million, label=\"Recovered\", color='teal')\nplt.ylabel('Recovered (per million)', size=22, weight=\"bold\");\nplt.xticks(visible=False)\nplt.yticks(size=20)\nplt.legend(loc='upper right', shadow=True, fontsize='xx-large')\nplt.subplot(2,1,2)\nplt.bar(df1.date, df1.owid_new_deaths_per_million, label=\"Mortality\", color='lightcoral')\nplt.xlabel('Days Since 1\/04\/2020', size=30, weight=\"bold\");\nplt.ylabel('Mortality (per million)', size=22, weight=\"bold\");\nplt.legend(loc='upper right', shadow=True, fontsize='xx-large')\nplt.xticks(size=24)\nplt.yticks(size=24)\nplt.subplots_adjust(wspace=0, hspace=0.02)\nplt.subplots_adjust(top=0.93);","3ebd4893":"#dropping columns not needed for analysis\ndf.drop([\"jhu_recovered\", \"owid_total_cases\", \"owid_new_cases\", \"owid_total_deaths\", \"owid_new_deaths\", \"owid_population\", \"owid_new_recovered\", \"owid_new_cases_per_million\",\n        \"owid_new_recovered_per_million\"], axis=1, inplace=True)","5b25df12":"#https:\/\/simply-python.com\/2019\/08\/21\/useful-seaborn-plots-for-data-exploration\/\n\n#numeric_features= df.select_dtypes(exclude=[\"object\",\"datetime\"])\n#numeric_features = numeric_features.stack().reset_index().rename(columns = {\"level_1\":\"Variable\",0:\"Value\"})\n#g = sns.FacetGrid(data =numeric_features, col=\"Variable\",  col_wrap=6, sharex=False, sharey=False)\n#g = g.map(sns.distplot, \"Value\", color ='blue')\n#plt.subplots_adjust(top=0.93)\n#plt.suptitle(\"Histograms of various variables to test distribution of target (first) and predictors (remainder)\", fontsize=28, weight=\"bold\");","f4ce2edc":"# https:\/\/www.kaggle.com\/duonghoanvu1\/momo-secret-finding\n#### Some variables are skewed and as linear models like normally distributed data , we will transform SalePrice and make it more normally distributed.\npal = sns.color_palette('Paired')\nfig, axes = plt.subplots(3, 2, figsize=(20, 15))\nfig.suptitle('COVID-19 daily mortality before and after log transformation', fontsize=32, weight=\"bold\");\nplt.subplots_adjust(top=0.93)\nax = plt.subplot(2,2,1)\nsns.distplot(df['owid_new_deaths_per_million'] , fit=norm, color = \"dodgerblue\");\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df['owid_new_deaths_per_million']);\n# Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best');\nplt.ylabel('Frequency')\nplt.title('Mortality\/million before transformation', fontsize=18, weight=\"bold\");\n# Get also the QQ-plot\nax = plt.subplot(2,2,2)\nres = stats.probplot(df['owid_new_deaths_per_million'], plot=plt);\nplt.title('Probability plot mortality\/million before transformation', fontsize=18, weight=\"bold\");\n\nax = plt.subplot(2,2,3)\ndf['owid_new_deaths_per_million_transf'] = np.log1p(df['owid_new_deaths_per_million']);\nsns.distplot(df['owid_new_deaths_per_million_transf'], fit=norm, color = \"dodgerblue\");\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df['owid_new_deaths_per_million_transf'])\n# Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],  loc='best')\nplt.ylabel('Frequency')\nplt.title('Mortality\/million after transformation', fontsize=18, weight=\"bold\");\nax = plt.subplot(2,2,4)\n# Get also the QQ-plot\nres = stats.probplot(df['owid_new_deaths_per_million_transf'], plot=plt);\nplt.title('Probability plot mortality\/million after transformation', fontsize=18, weight=\"bold\");\nplt.subplots_adjust(wspace=0.08, hspace=0.14)","3d172953":"# dropping the skewed target variable as this is no longer needed. For analysis transformed variable is used.\ndf = df.drop(['owid_new_deaths_per_million'], axis = 1)","e61c0d1d":"# Computing correlation matrix to describe correlation of variables\ncorrmat = df.corr() \nk = 50 \ncols = corrmat.nlargest(k, 'owid_new_deaths_per_million_transf')['owid_new_deaths_per_million_transf'].index \ncm = np.corrcoef(df[cols].values.T)\nf, ax = plt.subplots(figsize =(16, 12)) \nsns.heatmap(cm, ax = ax, cmap = \"coolwarm\", \n            linewidths = 0.1, yticklabels = cols.values,  \n                              xticklabels = cols.values)\nplt.show()","f581fe9e":"#Correlation with output target variable: graph\ncor_target = corrmat[\"owid_new_deaths_per_million_transf\"].sort_values(ascending=False)\n#Selecting highly correlated features\nplt.figure(figsize=(20,8))\ncor_target.drop(\"owid_new_deaths_per_million_transf\").plot.bar(color=\"darkcyan\")","823b2494":"#Correlation with output variable: numbers\ncor_target = corrmat[\"owid_new_deaths_per_million_transf\"].sort_values(ascending=False)\n#Selecting highly correlated features\ncor_target","bce2cf15":"X = df.drop(['country','date','owid_new_deaths_per_million_transf'], axis = 1)\ny = df.owid_new_deaths_per_million_transf","ae9ababf":"X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = 0.2, random_state = 42)","3cffcbc3":"vif = pd.DataFrame()\nvif[\"variables\"] = X.columns\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif","3c22945e":"#https:\/\/boostedml.com\/2018\/08\/testing-linear-regression-assumptions-the-kaggle-housing-price-dataset.html\nfrom scipy import stats\nfrom statsmodels.regression.linear_model import OLS \nimport statsmodels as sm\ndef abline(slope, intercept):\n     #Plot a line from slope and intercept, borrowed from https:\/\/stackoverflow.com\/questions\/7941226\/how-to-add-line-based-on-slope-and-intercept-in-matplotlib\"\"\"\n     axes = plt.gca()\n     x_vals = np.array(axes.get_xlim())\n     y_vals = intercept + slope * x_vals\n     plt.plot(x_vals, y_vals, '--')\n #fit an OLS model to data\n\nX_train_np = np.array(X_train)\ny_np = np.array(y_train)\n\nmodel = OLS(y_np,sm.tools.add_constant(X_train_np))\nresults = model.fit()\n#predict y values for training data\ny_hat = model.predict(results.params)\n#plot predicted vs actual\nplt.plot(y_hat,y_np,'o')\nplt.xlabel('Predicted')#,color='white')\nplt.ylabel('Actual')#,color='white')\nplt.title('Predicted vs. Actual: Visual Linearity Test')#,color='white')\nplt.tick_params(axis='x', colors='white')\nplt.tick_params(axis='y', colors='white')\nabline(1,0)\nplt.show()","732d35f4":"# create an array of alpha values\n#https:\/\/harvard-iacs.github.io\/2018-CS109A\/labs\/lab-5\/solutions\/\n\nalphas = np.logspace(-4, 0, 50)\nsplitter = KFold(10, random_state=42, shuffle=True)\n\n# select the best alpha with RidgeCV\nfrom sklearn.linear_model import RidgeCV\nridge_CV = RidgeCV(alphas=alphas, normalize=True, scoring='neg_mean_squared_error', cv=splitter)\nridge_CV.fit(X_train, y_train)\n\nbest_alpha = ridge_CV.alpha_;\nprint(\"Best model searched:\\nalpha = {}\\nintercept = {}\\nbetas = {}, \".format(best_alpha, ridge_CV.intercept_, ridge_CV.coef_))\nprint()\n\ntuned_ridge = Ridge(alpha=best_alpha, normalize=True,)\ntuned_ridge.fit(X_train, y_train)\npred_y = tuned_ridge.predict(X_train)\nridge_score = metrics.r2_score(y_train, pred_y)\nridge_EV=metrics.explained_variance_score(y_train, pred_y) #Explained variance\nridge_MAE=metrics.mean_absolute_error(y_train, pred_y) #Mean absolute error\nridge_mse = metrics.mean_squared_error(y_train, pred_y)\nridge_RMSE= np.sqrt(metrics.mean_squared_error(y_train, pred_y))\n\n\n#ypredict_ridge_best = est.predict(test_set)\ntuned_ridge.coef_\n\n# calculate R^2 value, MAE, MSE, RMSE\nprint(\"Performance of tuned ridge regression on entire training dataset: \\n R2:{:.3f}, EV: {:.3f}, MAE: {:.3f}, MSE:{:.3f}, RMSE:{:.3f}\"\\\n      .format(ridge_score, ridge_EV, ridge_MAE, ridge_score, ridge_mse, ridge_RMSE))\nprint()\n\ny_pred = tuned_ridge.predict(X_test)\nridge_score = metrics.r2_score(y_test, y_pred)\nridge_EV=metrics.explained_variance_score(y_test, y_pred) #Explained variance\nridge_MAE=metrics.mean_absolute_error(y_test, y_pred) #Mean absolute error\nridge_mse = metrics.mean_squared_error(y_test, y_pred)\nridge_RMSE= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n\nprint(\"Performance of ridge regression on testing dataset: \\n R2:{:.3f}, EV: {:.3f}, MAE: {:.3f}, MSE:{:.3f}, RMSE:{:.3f}\"\\\n      .format(ridge_score, ridge_EV, ridge_MAE, ridge_mse, ridge_RMSE))\nprint()\nprint()","f382913b":"from yellowbrick.style import set_palette\nset_palette('yellowbrick')\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(25, 12))\nviz = CVScores(tuned_ridge, cv=10, scoring='r2', ax=ax1)\nviz.fit(X_train, y_train)\nviz.finalize()\nviz = ValidationCurve(Ridge(), param_name=\"alpha\", param_range=alphas, cv=10, scoring=\"r2\", ax=ax2);\nviz.fit(X_train, y_train)\nviz.finalize()\nviz = ResidualsPlot(tuned_ridge, hist=False, qqplot=True, ax=ax3)\nviz.fit(X_train, y_train)\nviz.score(X_test, y_test);\nviz.finalize()\nviz = PredictionError(tuned_ridge, ax=ax4)\nviz.fit(X_train, y_train) \nviz.score(X_test, y_test) \nviz.finalize()\nf.suptitle('Performance of ridge regression on train and test datasets', fontsize=32, weight=\"bold\");\nplt.subplots_adjust(top=0.91)","e9c422f3":"fig, axes = plt.subplots(1, 2, figsize=(22, 6))\nax = plt.subplot(1,2,1)\nx_ax = range(len(X_train))\nplt.scatter(x_ax, y_train, s=15, color=\"dodgerblue\", label=\"Train original\")\nplt.scatter(x_ax, pred_y, s=15, color=\"m\", label=\"Train predicted\")\n#plt.plot(pred_y, 'm--', label=r\"$\\lambda =  {{{0:1.6f}}}$\".format(best_alpha),alpha=0.4)\nplt.ylabel('Log daily COVID-19 mortality', size=20)\nplt.xlabel('Days since 01\/April\/2020', size=20)\nplt.title('Ridge regression prediction on training dataset', size=20)\nplt.xticks(size=20)\nplt.yticks(size=20)\nplt.legend()\nplt.text(3, 3, 'Train R2 = 0.676',\n         {'color': 'black', 'fontsize': 20, \n          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)});\n\nax = plt.subplot(1,2,2)\nx_ax = range(len(X_test))\nplt.scatter(x_ax, y_test, s=15, color=\"dodgerblue\", label=\"Test original\")\nplt.scatter(x_ax, y_pred, s=15, color=\"m\", label=\"Test predicted\")\n#plt.plot(y_pred, 'm--', label=r\"$\\lambda =  {{{0:1.6f}}}$\".format(best_alpha),alpha=0.4)\n\nplt.ylabel('Log daily COVID-19 mortality', size=20)\nplt.xlabel('Days since 01\/April\/2020', size=20)\nax.set_title('Ridge regression prediction on testing dataset', size=20)\nplt.xticks(size=20)\nplt.yticks(size=20)\nplt.legend()\nfig.suptitle('Comparison of ridge regression COVID-19 mortality prediction to original data', fontsize=28, weight=\"bold\");\nplt.subplots_adjust(top=0.84)\nplt.text(2.8, 2.7, 'Test R2 = 0.700',\n         {'color': 'black', 'fontsize': 20, \n          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)});","f535d14d":"# Feature importance by Ridge Regression \nfeatures = X.keys();\n#mpl.rcParams['axes.prop_cycle'] = cycler('color', ['dodgerblue']);\nfig = plt.gcf();\nfig.set_size_inches(20,13);\nax = plt.subplot(211);\nlabels = features;\nviz = FeatureImportances(tuned_ridge, ax=ax, labels=labels, relative=False);\nax.spines['right'].set_visible(False);\nax.spines['top'].set_visible(False);\nax.grid(False);\nfig.suptitle('Predictor importance computed by the ridge regression', fontsize=28, weight=\"bold\");\nplt.subplots_adjust(top=0.84);\n# Fit and display\nviz.fit(X, y);\nviz.poof();\n","ce191812":"#https:\/\/www.mikulskibartosz.name\/xgboost-hyperparameter-tuning-in-python-using-grid-search\/\n\ngbm_param_grid = {\n    'colsample_bytree': [0.3, 0.7],\n    'n_estimators': range(60, 220, 40),\n    'max_depth': range (2, 10, 1),\n    'learning_rate': [0.1, 0.01, 0.05] };\n\n# Instantiate the regressor: gbm\ngbm = XGBRegressor();\n\n# Perform grid search: grid_mse\ngrid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n                        scoring='neg_mean_squared_error', cv=10, verbose=False);\n\ngrid_mse.fit(X_train, y_train);\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", grid_mse.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n\ngrid_mse.best_estimator_ # The best_estimator_ field contains the best model trained by GridSearch.","1727fe36":"xgb_model1 = grid_mse.best_estimator_\nxgb_model1.fit(X_train, y_train, verbose=False)\ny_train_pred1 = xgb_model1.predict(X_train)\ny_test_pred1 = xgb_model1.predict(X_test)\n\nprint('Train r2 score: ', r2_score(y_train, y_train_pred1))\nprint('Test r2 score: ', r2_score(y_test, y_test_pred1))\ntrain_mse1 = mean_squared_error(y_train, y_train_pred1)\ntest_mse1 = mean_squared_error(y_test, y_test_pred1)\ntrain_rmse1 = np.sqrt(train_mse1)\ntest_rmse1 = np.sqrt(test_mse1)\nprint('Train RMSE: %.4f' % train_rmse1)\nprint('Test RMSE: %.4f' % test_rmse1)\nprint()\nprint()","73fd465d":"f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(25, 12))\nviz = CVScores(xgb_model1, cv=10, scoring='r2', ax=ax1)\nviz.fit(X_train, y_train)\nviz.finalize()\nviz = ValidationCurve(xgb_model1, param_name=\"max_depth\", param_range=np.arange(1, 11), cv=10, scoring=\"r2\", ax=ax2);\nviz.fit(X_train, y_train)\nviz.finalize()\nviz = ResidualsPlot(xgb_model1, hist=False, qqplot=True, ax=ax3)\nviz.fit(X_train, y_train)\nviz.score(X_test, y_test);\nviz.finalize()\nviz = PredictionError(xgb_model1, ax=ax4)\nviz.fit(X_train, y_train) \nviz.score(X_test, y_test) \nviz.finalize()\nf.suptitle('Performance of ridge regression on train and test datasets', fontsize=28, weight=\"bold\");\nplt.subplots_adjust(top=0.91)","e109529d":"fig, axes = plt.subplots(1, 2, figsize=(22, 6))\nax = plt.subplot(1,2,1)\nx_ax = range(len(X_train))\nplt.scatter(x_ax, y_train, s=15, color=\"dodgerblue\", label=\"Train original\")\nplt.scatter(x_ax, y_train_pred1, s=15, color=\"m\", label=\"Train predicted\")\n#plt.plot(pred_y, 'm--', label=r\"$\\lambda =  {{{0:1.6f}}}$\".format(best_alpha),alpha=0.4)\nplt.ylabel('Log daily COVID-19 mortality', size=20)\nplt.xlabel('Days since 01\/April\/2020', size=20)\nplt.title('XGBoost prediction on training dataset', size=20)\nplt.xticks(size=20)\nplt.yticks(size=20)\nplt.legend()\n#plt.text(3.1, 3.1, 'Train R2 = 0.846')\nplt.text(3, 3, 'Train R2 = 0.846',\n         {'color': 'black', 'fontsize': 20, \n          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)});\n\nax = plt.subplot(1,2,2)\nx_ax = range(len(X_test))\nplt.scatter(x_ax, y_test, s=15, color=\"dodgerblue\", label=\"Test original\")\nplt.scatter(x_ax, y_test_pred1, s=15, color=\"m\", label=\"Test predicted\")\n#plt.plot(y_pred, 'm--', label=r\"$\\lambda =  {{{0:1.6f}}}$\".format(best_alpha),alpha=0.4)\n\nplt.ylabel('Log daily COVID-19 mortality', size=20)\nplt.xlabel('Days since 01\/April\/2020', size=20)\nax.set_title('XGBoost prediction on testing dataset', size=20)\nplt.xticks(size=20)\nplt.yticks(size=20)\nplt.legend()\nfig.suptitle('Comparison of XGBoost COVID-19 mortality prediction to original data', fontsize=28, weight=\"bold\");\nplt.subplots_adjust(top=0.84)\nplt.text(2.8, 2.7, 'Test R2 = 0.825',\n         {'color': 'black', 'fontsize': 20, \n          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)});","ce3476a1":"# Feature importance by XGBoost \nfeatures = X.keys()\n#mpl.rcParams['axes.prop_cycle'] = cycler('color', ['purple'])\nfig = plt.gcf()\nfig.set_size_inches(20,13)\nax = plt.subplot(211)\nlabels = features\nviz = FeatureImportances(xgb_model1, ax=ax, labels=labels, relative=True)\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.grid(False)\nfig.suptitle('Predictor importance computed by the XGBoost', fontsize=28, weight=\"bold\");\nplt.subplots_adjust(top=0.90);\n# Fit and display\nviz.fit(X, y)\nviz.poof()","1c45b907":"#### 9.3 Performance of XGBoost on train and test datasets","623f4422":"### 6. Data distribution, transformation and correlation matrix","71cb187c":"### 4. Data imputation to process missing values","e469bbcb":"#### 8.7 Performance of ridge regression on train and test datasets","a364c07c":"#### 4.1 Count the missing values","3dcce0ff":"#### 6.1 Distribution of data","5a800b94":"\n#### 3.2 Concise summary of dataframe\nHere, df.info() method is used to print summary and data types.","4d8a27fd":"\n### 9. XGBoost","d66f2c0d":"\n\n#### 3.3 Descriptive statistics","bfb601ec":"#### 8.8 Computation of predictor importance by ridge regression","5ba42027":"#### 3.1 Data dimension and head to obtain some information on the structure","46ca0375":"#### 4.3 Selecting countires with population more than 1,000,000","a495c32f":"#### 9.2 Visualisation of cross-validation scores, validation curve, residual plot and prediction error","caf535af":"### 8. Ridge regression","5381d93c":"#### 6.3 Correlation matrix to test correlation of predictors to target variable","cbd270af":"#### 4.6 Engineering a new feature: COVID-19 daily recovery per million","a0c78d6d":"#### 6.2 Transformation of skewed target variable","77a92599":"#### 4.8 Visualisation of data before and after imputation to remove missing values","d34796a2":"#### 8.1 Defining predictors (X) and target outcome (y)","e1bd9ee6":"#### 8.4 Testing linearity","d35bbfb2":"\n\n#### 4.4 Limiting analysis to top 5 most affected countries","c1dd25a9":"### 3. Exploratory Data Analysis (EDA)\nIn this step basic information about the data structure is obtained. ","1e857b19":"#### 8.6 Visualisation of cross-validation scores, validation curve, residual plot and prediction error","da53c398":"#### 4.5 Confining period of analysis to between 01\/April and 30\/October\/2020","9ec520fc":"This is generated by describe() function to summarise the central tendency and distribution of the dataframe columns excluding missing values.\nCount provides information on missing values.\nMean provides mean of variable.\nStd provides standard deviation of that variables. etc.","ca745632":"#### 9.1 Grid-search for optimal parameters for XGBoost","c26caee9":"### 2. Importing the dataset","44599fbd":"#### 8.2 Splitting dataset into train and test; 80 and 20% respectively","2f326e6b":"#### 9.4 Computation of predictor importance by ridge regression","ed89433d":"The plot above suggests that the target variable (first histogram) suffers from assymetry and therefore skewed. ","955be913":"\nVariable \"date\" has \"object\" as data type. Whilst, time aspect of this dataset only used for visualisation and not part of analysis and model building,\ntype needs to be changed appropriately. This is achieved using to_datetime","38f24587":"#### 8.3 Testing for multicollinearity","fd489e19":"### 5. Data visualisation","8b7052be":"# The COVID pandmic: socioeconomic and health disparities\n\n#### Javaheri, B. The COVID-19 Pandemic: Socioeconomic and Health Disparities. Preprints 2020, 2020120599 (doi: 10.20944\/preprints202012.0599.v1\n\n### This notebook contains all the steps taken to process and analyse the COVID-19 data. These are:\n\n#### 1. Loading required libraries\n#### 2. Importing the dataset\n#### 3. Exploratory data analysis\n#### 4. Data imputation to process missing values\n#### 5. Data visualisation\n#### 6. Data distribution, transformation and correlation matrix\n#### 7. Data imputation to process missing values\n#### 8. Ridge regression\n#### 9. XGBoost \n\n","5a63f43c":"\n\n#### 8.5 Cross-validation for ridge regression\n","327a561a":"#### 4.2 Dropping empty columns","505de658":"\n\n### 1. Loading the required libraries","bc79eebc":"#### 4.7 Forward and backward fill"}}