{"cell_type":{"364ff94e":"code","fb8ea422":"code","983fd2d0":"code","4fb8df5d":"code","6a457eca":"code","9ccae5fc":"code","6dbeee9f":"code","f66dfb5c":"code","2b846f71":"code","4933e5ee":"code","aa8fee22":"markdown","879ce8e0":"markdown","27b7d53d":"markdown","e8d83d98":"markdown","493b4f0d":"markdown"},"source":{"364ff94e":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport utilfn as fn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\n\nwinpath = \"..\/input\/the-winton-stock-market-challenge\/\"\npcpath  = \"..\/input\/wintonpca\/\"\nprint(os.listdir(winpath))\n","fb8ea422":"fn.test()","983fd2d0":"# Rows excluded from MacroPCA, drop from train\/test data\ndrp_rows = \"1     40    393   522   840   844   1573  1611  1853  2102  3050  3458 \\\n            3885  4330  4701  5171  5331  5881  6709  6720  6856  7305  7528  8572 \\\n            8710  9511  9796  10654 10661 11213 12608 12806 13206 13273 13334 14148 \\\n            14161 14255 14510 14626 15914 15973 16329 16549 16811 17120 17782 19173 \\\n            19630 19807 19955 21771 22198 22207 22225 22243 23210 23625 23699 23851 \\\n            24041 24429 24525 24568 24907 24941 25264 25441 25536 25756 26945 27892 \\\n            28074 28110 28321 28720 30545 31963 32118 32764 32828 32978 33188 33227 \\\n            33258 33268 33469 33609 33613 33766 33769 33982 34372 34776 35095 35112 \\\n            35403 35419 35668 36034 36506 36539 36869 37251 37809 37899 38010 38520 \\\n            39301 39560 39748 3985\"\n\ndrp_rows = list(map(lambda x: int(x) - 1, list(filter(lambda x: x.isdigit(), drp_rows.split(' ')))))\ndrp_rows","4fb8df5d":"# Read\ndfw = pd.read_csv(winpath + 'train.csv')\npc = pd.read_csv(pcpath + 'winton_pca_final.csv')\npcns = pd.read_csv(pcpath + 'winton_pca_finalNS.csv')\n\n# Prep unmodified features\ndfw = dfw.drop(index=drp_rows)\nx_std = dfw.iloc[:,1:26]\nx_std.fillna(x_std.mean(), inplace=True)\n\n# create array containing y and evaluation weights (if use of weights is desired)\ny = np.array(dfw['Ret_PlusOne']).reshape((-1,1))\nw = np.array(dfw['Weight_Daily']).reshape((-1,1))\nyw = np.concatenate([y,w], axis=1)\n\n\ndfw.head()","6a457eca":"# Train\/Test Split (use for RFR)\nx_tr, x_tt, y_tr, y_tt = train_test_split(x_std, yw, test_size=0.33,\n                                          random_state=22391) # Features with no PCA\n\nxPC_tr, xPC_tt, yPC_tr, yPC_tt = train_test_split(pc, yw, test_size=0.33,\n                                                  random_state=22391) # Scaled PCA\n\nxNS_tr, xNS_tt, yNS_tr, yNS_tt = train_test_split(pcns, yw, test_size=0.33,\n                                                  random_state=22391) # Not Scaled PCA","9ccae5fc":"# Standardize and Train\/Test Split (use for SVR)\n\nstd1, std2, std3, = StandardScaler(), StandardScaler(), StandardScaler()\nscales = [std1,std2,std3]\nframes = [x_std, pc, pcns]\n\nfor i,k in enumerate(scales):\n    frames[i] = k.fit_transform(frames[i])\n\nx_trs, x_tts, y_trs, y_tts = train_test_split(x_std, yw, test_size=0.33,\n                                          random_state=22391) # Features with no PCA\n\nxPC_trs, xPC_tts, yPC_trs, yPC_tts = train_test_split(pc, yw, test_size=0.33,\n                                                  random_state=22391) # Scaled PCA\n\nxNS_trs, xNS_tts, yNS_trs, yNS_tts = train_test_split(pcns, yw, test_size=0.33,\n                                                  random_state=22391) # Not Scaled PCA","6dbeee9f":"# Fit\nstd_rf = RandomForestRegressor(max_depth=10, random_state=323, criterion='mse',\n                               n_estimators=5, verbose=1, bootstrap=True)\n\npc_rf = RandomForestRegressor(max_depth=10, random_state=323, criterion='mse',\n                               n_estimators=5, bootstrap=True)\n\nns_rf = RandomForestRegressor(max_depth=10, random_state=323, criterion='mse',\n                               n_estimators=5, bootstrap=True)\n\nstd_rf.fit(x_tr, y_tr[:,0])\npc_rf.fit(xPC_tr, yPC_tr[:,0])\nns_rf.fit(xNS_tr, yNS_tr[:,0])","f66dfb5c":"# Scores\nstd_tr_score, std_tt_score = std_rf.score(x_tr,y_tr[:,0]), std_rf.score(x_tt,y_tt[:,0])\n\npc_tr_score, pc_tt_score = pc_rf.score(xPC_tr,yPC_tr[:,0]), pc_rf.score(xPC_tt,yPC_tt[:,0])\n\nns_tr_score, ns_tt_score = ns_rf.score(xNS_tr,yNS_tr[:,0]), ns_rf.score(xNS_tt,yNS_tt[:,0])\n\nprint(\"No PCA Train Score: {:.3f}  Test Score: {:.3f}\".format(std_tr_score,std_tt_score),'\\n\\\nScaled PCA Train Score: {:.3f}  Test Score: {:.3f}'.format(pc_tr_score,pc_tt_score),'\\n\\\nNo Scale PCA Train Score: {:.3f}  Test Score: {:.3f}'.format(ns_tr_score,ns_tt_score))","2b846f71":"# Fit\n'''std_sv = SVR(kernel='poly', gamma='auto')\npc_sv = SVR(kernel='poly', gamma='auto')\nns_sv = SVR(kernel='poly', gamma='auto')\n\nstd_sv.fit(x_trs, y_trs[:,0])\npc_sv.fit(xPC_trs, yPC_trs[:,0])\nns_sv.fit(xNS_trs, yNS_trs[:,0])'''","4933e5ee":"\"\"\"std_tr_score, std_tt_score = std_sv.score(x_tr,y_tr[:,0]), std_sv.score(x_tt,y_tt[:,0])\n\npc_tr_score, pc_tt_score = pc_sv.score(xPC_tr,yPC_tr[:,0]), pc_sv.score(xPC_tt,yPC_tt[:,0])\n\nns_tr_score, ns_tt_score = ns_sv.score(xNS_tr,yNS_tr[:,0]), ns_sv.score(xNS_tt,yNS_tt[:,0])\n\nprint(\"No PCA Train Score: {:.3f}  Test Score: {:.3f}\".format(std_tr_score,std_tt_score),'\\n\\\nScaled PCA Train Score: {:.3f}  Test Score: {:.3f}'.format(pc_tr_score,pc_tt_score),'\\n\\\nNo Scale PCA Train Score: {:.3f}  Test Score: {:.3f}'.format(ns_tr_score,ns_tt_score))\"\"\"","aa8fee22":"# Support Vector Regression","879ce8e0":"# Motivation\n\n*In Progress*\n\nIn [this previous kernel](https:\/\/www.kaggle.com\/rlagrois\/macropca-with-unknown-features) I ran MacroPCA on the 25 unknown given features.  Since what the features and specific rows represent are unknown, interpreting and evaluating the PCA is a little more tricky than normal.  I showed that there were a number of potentially problematic high-leverage rows though they were outnumbered by \"good\" highleverage points.  At this point it's unclear if this PCA will provide any value in making predictions.  \n\nIn order to try to answer this question I will run a regression on one of the total return target columns using only the PCA data and compare that to doing the same with the original data.  These checks will be done using a Random Forest Regressor and Suport Vector Regressor; these were chosen as I plan to use either extensions or similar methods for the full multi-output problem.  They also benefit from being easy and quick to run and train.","27b7d53d":"# Read and Prep","e8d83d98":"Somewhat dissapointingly there's a rather large gap in the performance between the unmodified features (that is except for replacing NA with mean) and the MacroPCA scores using RFR.  The scaled PCA version does much better than the Not Scaled though still quite a bit worse than the raw features. \n\nOverfitting is also clearly and issue, specially for the PCA models.  Even though the scaled PCA does seem to have some predictive value it overfits the worst.  The unmodified features overfit as well but not to the same degree. Another issue that presented itself was how slowly the models train when using Mean Absolute Error as the objective function.  Frankly I doubt it would have made much, if any, of a difference but the actual evaluation will be done using WMAE as the metric.  It is also possible that comparing these models using the WMAE would have some effect on the relative performance as opposed the R-square above.  Again, however, I'm struck as this being not especially likely.  Furthermore, there is still the issue of overfitting which should lead one to believe that the specifc metric won't ultimately matter here.\n\nThe overfitting problem and relative predictive ability seems to stay consistent through changes to target variable and hyperparameters.","493b4f0d":"# Random Forest Models"}}