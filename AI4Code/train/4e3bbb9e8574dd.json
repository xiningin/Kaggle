{"cell_type":{"1ee39773":"code","a10c6f76":"code","9545d6a5":"code","1b09986c":"code","6f9ce5d3":"code","57dfffb9":"code","7154824e":"code","6d07268a":"code","fe5d8541":"code","c557b6b2":"code","4013aed2":"code","6974934b":"code","12f88bdd":"code","dbdf6bfa":"code","8ae414f8":"code","094d0fd1":"code","da7fe633":"code","b5639200":"code","66fdda92":"code","0c4b4f20":"code","b1842e14":"code","88a4cc0e":"code","8e4958b9":"code","7ff70417":"code","df7e1c62":"code","8b5025a5":"code","6a1cde93":"code","46db2aea":"code","340bbf2b":"markdown","742188cd":"markdown","32d547b7":"markdown","34775909":"markdown","02d5e334":"markdown","9348e873":"markdown","4ed8ef27":"markdown","4760cfbd":"markdown","9c0a8abd":"markdown","ad4b8875":"markdown","c1336639":"markdown","6f937969":"markdown","75f2c31b":"markdown","957e0bc2":"markdown","f9cf639d":"markdown"},"source":{"1ee39773":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv('..\/input\/used-car-dataset-ford-and-mercedes\/audi.csv')\n","a10c6f76":"data","9545d6a5":"data.isna().sum()","1b09986c":"data.plot(kind=\"scatter\",x=\"mileage\",y=\"price\")","6f9ce5d3":"data.plot(kind=\"scatter\",x=\"tax\",y=\"price\")","57dfffb9":"data.plot(kind=\"scatter\",x=\"mpg\",y=\"price\")","7154824e":"data.plot(kind=\"scatter\",x=\"year\",y=\"price\")","6d07268a":"data.plot(kind=\"scatter\",x=\"engineSize\",y=\"price\")","fe5d8541":"data.plot(kind=\"scatter\",x=\"model\",y=\"price\")","c557b6b2":"data.groupby(\"price\").mean()","4013aed2":"def corr(dataframe,target_variable):\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(10,10))\n    correlation_matrix = dataframe.corr().round(2)\n    sns.heatmap(data=correlation_matrix, annot=True)\n    \n    correlation = data.corr()[target_variable].abs().sort_values(ascending = False)\n    return correlation\n\n\ncorr(data,\"price\")","6974934b":"data2=data\ndata2=data2.drop([\"year\"],axis=1)\ndata2=data2.drop([\"mileage\"],axis=1)\ndata2=data2.drop([\"tax\"],axis=1)\ndata2=data2.drop([\"mpg\"],axis=1)\ndata2=data2.drop([\"engineSize\"],axis=1)\ndata2=data2.drop([\"price\"],axis=1)\n\nfrom sklearn import preprocessing \ndata2=data2.apply(preprocessing.LabelEncoder().fit_transform)\n\ndata=data.drop([\"transmission\"],axis=1)\ndata=data.drop([\"fuelType\"],axis=1)\ndata=data.drop([\"model\"],axis=1)\n\n\ndata=pd.concat([data,data2],axis=1)","12f88bdd":"corr(data,\"price\")","dbdf6bfa":"def feature_importance(dataframe,target_variable):\n    from sklearn.ensemble import RandomForestRegressor\n    import seaborn as sns\n    v=12\n    w=6\n    considered_features=[]\n    for x in dataframe.columns.values.tolist():\n        if x==target_variable:\n            pass\n        else:\n            considered_features.append(x)\n    col_to_consider=[]\n    for x in considered_features:\n        col_to_consider.append(x)\n    X= dataframe[col_to_consider]\n    Y= dataframe[target_variable]\n    random_forest= RandomForestRegressor(n_estimators=100, max_depth=5, min_samples_leaf=20, min_samples_split=40).fit(X,Y)\n    feature_importances_=random_forest.feature_importances_\n    feature_importances=pd.DataFrame({'Feature_name':col_to_consider, 'Feature_importance':feature_importances_})\n    if len(considered_features)>15:\n        v=32\n        w=24\n    elif len(considered_features)>30:\n        v=64\n        w=48\n    else:\n        pass\n    fig, ax=plt.subplots(1, figsize=(v,w))\n    sns.barplot(x='Feature_name', y='Feature_importance', data=feature_importances, ax=ax)\n    plt.xticks(fontsize=10, rotation=90);\n    plt.yticks(fontsize=12);\n    plt.xlabel('Feature name',fontsize=v)\n    plt.ylabel('Feature importance',fontsize=v)\n    ","8ae414f8":"feature_importance(data,\"price\")","094d0fd1":"chosen_col=[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\",\"model\"]\ndef VIF(dataframe,chosen_col):\n    from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n    from statsmodels.tools.tools import add_constant\n    X=dataframe[chosen_col]\n    X=add_constant(X)\n    vif_data=pd.DataFrame()\n    vif_data[\"feature\"]=X.columns\n    vif_data[\"VIF\"]=[VIF(X.values, i) for i in range(len(X.columns))]\n    return vif_data","da7fe633":"VIF(data,chosen_col)","b5639200":"def Outliers(dataframe,chosen_cols):\n    import plotly.graph_objects as go\n    from plotly.subplots import make_subplots\n    numeric_col2=[]\n    for x in cols:\n        numeric_col2.append(x)\n\n    fig=make_subplots(rows=1, cols=len(cols))\n\n    for i,col in enumerate(numeric_col2):\n        fig.add_trace(go.Box(y=dataframe[col].values, name=dataframe[col].name), row=1, col=i+1)\n\n    return fig.show()","66fdda92":"cols=[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\",\"model\"]\nOutliers(data,cols)","0c4b4f20":"data=data[~(data[\"year\"]<2000)]\ndata=data[~(data[\"mileage\"]>=168017)]\ndata=data[~(data[\"tax\"]>=515)]\ndata=data[~(data[\"tax\"]<45)]\ndata=data[~(data[\"mpg\"]>=117.7)]\ndata=data[~(data[\"engineSize\"]>=4.1)]\ndata=data[~(data[\"model\"]>=22)]","b1842e14":"def linear_model(dataframe,feature_cols,target_variable):\n    from sklearn.linear_model import LinearRegression\n    X=dataframe[feature_cols]\n    Y=dataframe[target_variable]\n    lm= LinearRegression()\n    lm.fit(X,Y)\n    dataframe[\"Predict\"]=lm.predict(X)\n    dataframe[\"RSE\"]= (dataframe[target_variable] - dataframe[\"Predict\"])**2\n    SSD= sum(dataframe[\"RSE\"])\n    RSE= np.sqrt(SSD\/(len(dataframe)-len(feature_cols) -1 ))\n    target_mean= np.mean(dataframe[target_variable])\n    error= RSE\/target_mean\n    print(f\"The intercept is {lm.intercept_}\\n\")\n    print(f\"The coefs are: {list(zip(feature_cols,lm.coef_))}\\n\")\n    print(f\"The R2 is: {lm.score(X,Y)}\\n\")\n    print(f\"The SSD is: {SSD}\\n\")\n    print(f\"The RSE is: {RSE}\\n\")\n    print(f\"The {target_variable} mean is: {target_mean}\\n\")\n    print(f\"The error is: {error}\\n\")\n    if len(feature_cols)==1:\n        %matplotlib inline\n        plt.plot(X,Y,\"go\")\n        plt.plot(X,lm.predict(X),color=\"red\")\n        for x in feature_cols:\n            plt.xlabel(x)\n            plt.title(f\"{target_variable} vs {x}\")\n        plt.ylabel(target_variable)\n    else:\n        pass\n","88a4cc0e":"feature_cols=[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\",\"model\"]\nlinear_model(data,feature_cols,\"price\")","8e4958b9":"def cuadratic_model(dataframe,feature_cols,target_variable):\n    from sklearn.linear_model import LinearRegression\n    X=dataframe[feature_cols]**2\n    Y=dataframe[target_variable]\n    lm= LinearRegression()\n    lm.fit(X,Y)\n    dataframe[\"Cuadratic_Predict\"]=lm.predict(X)\n    dataframe[\"Cuadratic_RSE\"]= (dataframe[target_variable] - dataframe[\"Cuadratic_Predict\"])**2\n    SSD= sum(dataframe[\"Cuadratic_RSE\"])\n    RSE= np.sqrt(SSD\/(len(dataframe)-len(feature_cols) -1 ))\n    target_mean= np.mean(dataframe[target_variable])\n    error= RSE\/target_mean\n    print(f\"The intercept is {lm.intercept_}\\n\")\n    print(f\"The coefs are: {list(zip(feature_cols,lm.coef_))}\\n\")\n    print(f\"The R2 is: {lm.score(X,Y)}\\n\")\n    print(f\"The SSD is: {SSD}\\n\")\n    print(f\"The RSE is: {RSE}\\n\")\n    print(f\"The {target_variable} mean is: {target_mean}\\n\")\n    print(f\"The error is: {error}\\n\")\n    if len(feature_cols)==1:\n        %matplotlib inline\n        plt.plot(X,Y,\"go\")\n        plt.plot(X,lm.predict(X),color=\"red\")\n        for x in feature_cols:\n            plt.xlabel(x)\n            plt.title(f\"{target_variable} vs {x}\")\n        plt.ylabel(target_variable)\n    else:\n        pass","7ff70417":"cuadratic_model(data,feature_cols,\"price\")","df7e1c62":"def polynomial_model(dataframe,feature_cols,target_variable,degree_number):\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn import linear_model\n    poly= PolynomialFeatures(degree=degree_number)\n    X= poly.fit_transform(dataframe[feature_cols])\n    Y= dataframe[target_variable]\n    lm= linear_model.LinearRegression()\n    lm.fit(X,Y)\n    dataframe[\"Cuadratic_Linear_Predict\"]=lm.predict(X)\n    dataframe[\"Cuadratic_Linear_RSE\"]= (dataframe[target_variable] - dataframe[\"Cuadratic_Linear_Predict\"])**2\n    SSD= sum(dataframe[\"Cuadratic_Linear_RSE\"])\n    RSE= np.sqrt(SSD\/(len(dataframe)-len(feature_cols) -1 ))\n    target_mean= np.mean(dataframe[target_variable])\n    error= RSE\/target_mean\n    print(f\"The intercept is {lm.intercept_}\\n\")\n    print(f\"The coefs are: {lm.coef_}\\n\")\n    print(f\"The R2 is: {lm.score(X,Y)}\\n\")\n    print(f\"The SSD is: {SSD}\\n\")\n    print(f\"The RSE is: {RSE}\\n\")\n    print(f\"The {target_variable} mean is: {target_mean}\\n\")\n    print(f\"The error is: {error}\\n\")\n   ","8b5025a5":"polynomial_model(data,feature_cols,\"price\",3)","6a1cde93":"def degree_increase(dataframe,feature_cols,target_variable,Max_Degree_Number):\n    from sklearn.linear_model import LinearRegression\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn import linear_model\n    for d in range(2,Max_Degree_Number+1):\n        poly= PolynomialFeatures(degree=d)\n        X= poly.fit_transform(dataframe[feature_cols])\n        Y= dataframe[target_variable]\n        lm= linear_model.LinearRegression()\n        lm.fit(X,Y)\n        dataframe[\"Cuadratic_Linear_Predict\"]=lm.predict(X)\n        dataframe[\"Cuadratic_Linear_RSE\"]= (dataframe[target_variable] - dataframe[\"Cuadratic_Linear_Predict\"])**2\n        SSD= sum(dataframe[\"Cuadratic_Linear_RSE\"])\n        RSE= np.sqrt(SSD\/(len(dataframe)-len(feature_cols) -1 ))\n        target_mean= np.mean(dataframe[target_variable])\n        error= RSE\/target_mean\n        print(f\"For the regression of degree {d}:\\nThe R2 is {lm.score(X,Y)}\")\n        print(f\"The intercept is: {lm.intercept_}\")\n        print(f\"The coefs are: {lm.coef_}\")\n        print(f\"The error is {error}\")\n        print(f\"The SSD is {SSD}\")\n        print(f\"The RSE is {RSE}\")\n        print()\n        ","46db2aea":"degree_increase(data,feature_cols,\"price\",6)","340bbf2b":"**We will set a maximum degree of 6**","742188cd":"# Now we will try a polynomial model at the same time, using different degrees to see whom  suits better","32d547b7":"# Having encoded the object variables we have now a better Correlation Matrix","34775909":"# CORRELATION MATRIX","02d5e334":"**NOT THE BEST WAY TO ENCODE LABELS I KNOW**","9348e873":"**We can see a small increase and decrease of R2 and the error**","4ed8ef27":"* We see that 3 is the best degree. \n* This is my first Kaggle Notebook and i am super new to ML. So i would love some feedback. \n* Thx for Watching","4760cfbd":"# Feature Importance from RandomForestRegressor (i recomend it , is very useful)","9c0a8abd":"# We got an R2 of 92% and an error of 12%. Which is good. But now, we will use a func that increases the degrees to see wich one is better","ad4b8875":"# The relation between price and all the variables is not linear so we will try a cuadratic model","c1336639":"# VIF","6f937969":"# Linear Prediction","75f2c31b":"#  As the correlation of Transmission and FuelType with the target variable (price) is too low, we wont use them","957e0bc2":"# Looking for outliers","f9cf639d":"# LOOKING AT LINEAR RELATIONS"}}