{"cell_type":{"c81177ad":"code","9e2110ca":"code","c5d01873":"code","329fe77d":"code","e9b5bf4c":"code","63122357":"code","ac088dc6":"code","a998f572":"code","614c527d":"code","8729e6bf":"code","b3c27182":"code","c27ad9e7":"code","644eb99c":"code","f3bb7f90":"code","a672048b":"code","218a121d":"code","064e3bb4":"code","039f4a3c":"code","f6e6be42":"code","169febc2":"code","ca3ffd93":"code","9654cb6c":"code","aa3679da":"code","675826fe":"code","cf5c9e6a":"code","182c5b1e":"code","6b9c96ed":"code","bc980270":"code","22eb9217":"code","0cf89e80":"code","fc403bba":"code","21b5a7c6":"code","437d901e":"markdown","790803a0":"markdown","2a200ad7":"markdown","727993c2":"markdown","13aee1a0":"markdown","5b3ad80f":"markdown","a5b4cc07":"markdown","7c300511":"markdown","632c3a8c":"markdown","e4edb95e":"markdown","7035930f":"markdown","30b73042":"markdown","40f035b8":"markdown","15d7b934":"markdown","07596a58":"markdown","0bd7b4b4":"markdown","4eae4d2b":"markdown","4a22556e":"markdown","2281a8d2":"markdown","99a10b55":"markdown","a0a95427":"markdown","1bfd0a07":"markdown","d11bb695":"markdown","1f5d9bd1":"markdown","91db82d3":"markdown","c22a2487":"markdown","d0df1105":"markdown","0f3ffd31":"markdown","3d6ea06e":"markdown","33323a0c":"markdown","8f5cc7e5":"markdown","0e1f1b9e":"markdown","f8d45bff":"markdown","3a49464c":"markdown","bfaade58":"markdown","84c3287e":"markdown","c7bef67c":"markdown","d2eaab5f":"markdown","6c795d6d":"markdown","6aba10a7":"markdown"},"source":{"c81177ad":"from IPython.core.display import display, HTML\nimport IPython.display\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import *\nlistings = pd.read_csv('..\/input\/listings_detail.csv')\nlistings_raw = pd.read_csv('..\/input\/listings_detail.csv')","9e2110ca":"listings.shape\nlistings.columns","c5d01873":"# output of the mean avaibility over 30 days of all the listings by neighourhood. It can be computed for avaibility_60, avaibility_90 in the same fashion.\nle = LabelEncoder()\nneigh_encoded = le.fit_transform(listings_raw['neighbourhood'].astype(str))\nneigh_encoded  = pd.DataFrame(neigh_encoded)\nneigh_encoded.columns = ['neigh_encoded']\nlistings_raw_enc = pd.merge(listings_raw, neigh_encoded, left_index=True, right_index=True)\nfor i in range(0,13,1): # I don't include index 13, these are NaNs encoded values\n    avaib_mean = [None]*15\n    avaibility = [None]*15\n    avab = [None]*15\n    avaibility[i] = listings_raw_enc[listings_raw_enc.neigh_encoded==i]\n    avaib_mean[i] = avaibility[i].availability_30\n    avab[i] = np.around(np.nanmean(avaib_mean[i]), 0)\n    number = i\n    labels_integers = {l: i for i, l in enumerate(le.classes_)}\n    lab = list(labels_integers)\n    print(avab[i],\"days over the next 30 days are available for rent at\", lab[i])\n    \n","329fe77d":"import numpy as np","e9b5bf4c":"a = listings_raw.availability_30.describe()# 75% of properties are rented for a minimum of 21 days in the next 30 >>> 70% occupancy\nprint(a.index[6],\"of properties are  rented for a maximum of\", 30-a[6],\"days in the next 30. That makes an (optimistic) assumption of\", np.round((30-a[6])\/0.3, 1),\"% occupancy\")","63122357":"a = listings_raw.availability_60.describe() # 58.3%\nprint(a.index[6],\"of properties are  rented for a maximum of\", 60-a[6],\"days in the next 60. That makes an (optimistic) assumption of\", np.round((60-a[6])\/0.6, 1),\"% occupancy\")","ac088dc6":"a =listings_raw.availability_90.describe() # 48.9%\nprint(a.index[6],\"of properties are  rented for a maximum of\", 90-a[6],\"days in the next 90. That makes an (optimistic) assumption of\", np.round((90-a[6])\/0.9, 1),\"% occupancy\")","a998f572":"a = listings_raw.availability_365.describe()  # 51%\nprint(a.index[6],\"of properties are  rented for a maximum of\", 365-a[6],\"days in the next 365. That makes an (optimistic) assumption of\", np.round((365-a[6])\/3.65, 1),\"%\")","614c527d":"listings_raw.reviews_per_month.describe()\nrev_excell = listings_raw[listings_raw.reviews_per_month==15]\nprint(\"There are\", rev_excell.shape[0], \"host(s) with at least 15 reviews per month\")","8729e6bf":"\nprint(\"The studio is  rented for a maximum of\", 30-rev_excell.availability_30.mean(),\"days in the next 30. That makes an (optimistic) assumption of\", np.round((30-rev_excell.availability_30.mean())\/0.3, 1),\"% occupancy rate\")\nprint(\"The studio is  rented for a maximum of\", 60-rev_excell.availability_60.mean(),\"days in the next 60. That makes an (optimistic) assumption of\", np.round((60-rev_excell.availability_60.mean())\/0.6, 1),\"% occupancy rate\")\nprint(\"The studio is  rented for a maximum of\", 90-rev_excell.availability_90.mean(),\"days in the next 60. That makes an (optimistic) assumption of\", np.round((90-rev_excell.availability_90.mean())\/0.9, 1),\"% occupancy rate\")\nprint(\"The studio is  rented for a maximum of\", 365-rev_excell.availability_365.mean(),\"days in the next 365. That makes an (optimistic) assumption of\", np.round((365-rev_excell.availability_365.mean())\/3.65, 1),\"% occupancy rate\")","b3c27182":"rev_excell.listing_url","c27ad9e7":"%matplotlib inline\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler","644eb99c":"listings.columns\nlistings.price.head()\nprint(listings.shape)","f3bb7f90":"def isnull(X):\n    listings_na = pd.isnull(X)\n    listings_na = listings_na*1\n    listings_na = (listings_na==1).sum()\/listings_na.shape[0]*100\n    listings_na = pd.DataFrame(listings_na.T.sort_index(ascending=True))\n    listings_na = listings_na[listings_na>0.0001] # je garde uniquement les colonnes dont le % de NaNs (non-attribu\u00e9es) est d\u00e9fini \n    listings_na = listings_na.dropna()\n    return listings_na\n\nisnull(listings)","a672048b":"import missingno as msno\n\nmissingdata_df = listings_raw.columns[listings_raw.isnull().any()].tolist()\nmsno.heatmap(listings_raw[missingdata_df], figsize=(20,20))","218a121d":"listings.reviews_per_month  = listings.reviews_per_month.fillna(0)","064e3bb4":"listings.price.head()","039f4a3c":"a = []\nfor i in listings.price:\n    if pd.isnull(i):\n        a.append(i)\n    else:\n        b = str(i)[1:]\n        b = b.replace(',','')\n        b = float(b)\n        c = b*0.88 \n        a.append(c)\n        \nlistings.price = a\n\na = []\nfor i in listings.weekly_price:\n    if pd.isnull(i):\n        a.append(i)\n    else:\n        b = str(i)[1:] #removes the first character '$'\n        b = b.replace(',','') # replaces ',' value as 135,00 to 'empty' in order to be able to convert it then \n        b = float(b)\n        c = b*0.88\n        a.append(b)\nlistings.weekly_price = a\n\na = []\n\nfor i in listings.monthly_price:\n    if pd.isnull(i):\n        a.append(i)\n    else:\n        b = str(i)[1:]\n        b = b.replace(',','')\n        b = float(b)\n        a.append(b)\n        c = b*0.88\nlistings.monthly_price = a\n\na = []\nfor i in listings.cleaning_fee:\n    if pd.isnull(i):\n        a.append(i)\n    else:\n        b = str(i)[1:]\n        b = b.replace(',','')\n        b = float(b)\n        c = b*0.88\n        a.append(b)\n        \nlistings.cleaning_fee = a\n\na = []\nfor i in listings.extra_people:\n    if pd.isnull(i):\n        a.append(i)\n    else:\n        b = str(i)[1:]\n        b = b.replace(',','')\n        b = float(b)\n        c = b*0.88\n        a.append(b)\n\nlistings.extra_people = a\n\na = []\nfor i in listings.security_deposit:\n    if pd.isnull(i):\n        a.append(i)\n    else:\n        b = str(i)[1:]\n        b = b.replace(',','')\n        b = float(b)\n        c = b*0.88\n        a.append(b)\nlistings.security_deposit = a","f6e6be42":"categorical_cols = ['room_type', 'host_since', 'property_type','cancellation_policy'] \n\nle = LabelEncoder()\n\nlistings_encoded = listings[categorical_cols].apply(lambda col: le.fit_transform(col))\nneigh_encoded = le.fit_transform(listings['neighbourhood'].astype(str))\nneigh_encoded  = pd.DataFrame(neigh_encoded)\nneigh_encoded.columns = ['neighbourhood']\n\nlistings_num = listings.select_dtypes(include = ['float64', 'int64']) \n\nlistings = pd.merge(listings_encoded, listings_num, left_index=True, right_index=True) \nlistings = pd.merge(listings, neigh_encoded, left_index=True, right_index=True)\n\n\nlistings  = listings.drop(columns = ['longitude', 'latitude','id', 'host_id'])\nli = listings.index[listings.price>800].tolist()\nlistings = listings.drop(index = li)\nlistings = listings.reset_index(drop=True)\nlistings = listings.fillna(listings.mean())","169febc2":"\n# I temporarily enter cleaning_fee and security_deposit by their respective mean\nlistings = listings.fillna(listings.mean())","ca3ffd93":"listings_for_knn_imputation = listings\nlistings = listings.dropna(axis=0, subset = ['review_scores_location'])\nlistings = listings.dropna(axis=0, subset = ['review_scores_accuracy'])\nlistings = listings.dropna(axis=0, subset = ['review_scores_checkin'])\nlistings = listings.dropna(axis=0, subset = ['review_scores_communication'])\nlistings = listings.dropna(axis=0, subset = ['review_scores_value'])\nlistings = listings.dropna(axis=0, subset = ['review_scores_cleanliness'])\nlistings = listings.dropna(axis=0, subset = ['review_scores_rating'])\nlistings = listings.reset_index()\n#del listings['index', 'thumbnail_url', 'xl_picture_url', 'host_acceptance_rate', 'medium_url', 'scrape_id', 'square_feet']\n#More than 95% of the surfaces in m2 are missing. After verification, some hosts mention the area of the property in the descriptions (str variables): these could be determined.","9654cb6c":"listings = listings.dropna(axis=0, subset = ['bathrooms'])\nlistings = listings.dropna(axis=0, subset = ['bedrooms'])\nlistings = listings.dropna(axis=0, subset = ['beds'])\nlistings = listings[listings['beds'] != 0] \nlistings = listings[listings['bedrooms'] != 0]\nlistings = listings[listings['price'] != 0]\ndel listings['thumbnail_url'] \ndel listings['xl_picture_url']\ndel listings['host_acceptance_rate']\ndel listings['square_feet'] #  more than 95% missing\ndel listings['medium_url']\ndel listings['scrape_id']\nlistings = listings.drop(index = 5843) #outlier removal\nlistings = listings.reset_index(drop=True)\n","aa3679da":"%matplotlib inline\nimport os\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, skew\nfrom sklearn.feature_selection import f_regression\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression as Lin_Reg\nfrom sklearn import metrics\nimport xgboost as xgb\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass","675826fe":"listings = listings.drop(index = [64, 743, 5842])\nlistings = listings.reset_index(drop=True)\n\nlistings = listings.fillna(listings.mean())\nY = listings.price\nprint(Y.shape)\ndel(listings['weekly_price']) # removal of the leaked variable 'price': It would not normally be possessed in a real situation\ndel(listings['monthly_price']) # removal of the leaked variable 'price': It would not normally be possessed in a real situation","cf5c9e6a":"columns = listings.columns.tolist()\n\np_values = f_regression(listings, Y)[1]\np_valuesdf = pd.DataFrame(p_values, index = listings.columns)\n\np_valuesdf.sort_values(by = 0, ascending=True)","182c5b1e":"import seaborn as sns\nsns.kdeplot(listings['price'] , clip= (0.0, 800))\n\nfig = plt.figure()\nres = stats.probplot(listings['price'], plot=plt)\nplt.show()","6b9c96ed":"sns.kdeplot(np.log(listings['price']) , clip= (0.0, 800))\n\nfig = plt.figure()\nres = stats.probplot(np.log(listings['price']), plot=plt)\nplt.show()","bc980270":"class model:\n\n    def __init__(self, model):\n        self.model = model\n        self.x_train = None\n        self.y_train = None\n        self.x_test = None\n        self.y_test = None\n        self.y_pred_train = None\n        self.y_pred_test = None\n        self.train_score = None\n        self.test_score = None\n        self.train_score_log = None\n        self.test_score_log = None\n        self.train_score_mae = None\n        self.test_score_mae = None\n        self.train_score_mae1 = None\n        self.test_score_mae1 = None\n        self.train_score_m_ae_unlog = None \n        self.test_score_m_ae_unlog = None\n        self.train_score_mae_unlog = None\n        self.test_score_mae_unlog = None\n\n    def data_split(self, x, y, test_size):\n        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(x, y, test_size=test_size)\n\n    def score_reg(self):\n        return self.train_score, self.test_score\n        \n    def score_mean_abs_err(self):\n        self.train_score_mae = metrics.mean_absolute_error(self.y_pred_train, self.y_train)\n        self.test_score_mae = metrics.mean_absolute_error(self.y_test, self.y_pred_test)\n        return self.train_score_mae, self.test_score_mae\n\n    def score_median_abs_err(self):\n      self.train_score_mae1 = metrics.median_absolute_error(self.y_pred_train, self.y_train)  \n      self.test_score_mae1 = metrics.median_absolute_error(self.y_test, self.y_pred_test)\n      return self.train_score_mae1, self.test_score_mae1\n\n    def score_log(self):\n        self.train_score_log = metrics.r2_score(np.exp(self.y_train), np.exp(self.y_pred_train))\n        self.test_score_log = metrics.r2_score(np.exp(self.y_test), np.exp(self.y_pred_test))\n        return self.train_score_log, self.test_score_log\n\n    def score_mean_log(self):\n       self.train_score_m_ae_unlog = metrics.mean_absolute_error(np.exp(self.y_pred_train), np.exp(self.y_train))\n       self.test_score_m_ae_unlog = metrics.mean_absolute_error(np.exp(self.y_test), np.exp(self.y_pred_test))\n       return self.train_score_m_ae_unlog, self.test_score_m_ae_unlog\n\n    def score_median_log(self):\n        self.train_score_mae_unlog = metrics.median_absolute_error(np.exp(self.y_pred_train), np.exp(self.y_train))\n        self.test_score_mae_unlog = metrics.median_absolute_error(np.exp(self.y_test), np.exp(self.y_pred_test))\n        return self.train_score_mae_unlog, self.test_score_mae_unlog\n\n    def data_frame_convert(self):\n        df_train = pd.DataFrame({'y_pred': self.y_pred_train, 'y_real': self.y_train})\n        df_test = pd.DataFrame({'y_pred_test': self.y_pred_test, 'y_real_test': self.y_test})\n        return self.train_score, self.test_score, df_train, df_test\n\n    def data_frame_convert_log(self):\n        df_train = pd.DataFrame({'y_pred': np.exp(self.y_pred_train), 'y_real': np.exp(self.y_train)})\n        df_test = pd.DataFrame({'y_pred_test': np.exp(self.y_pred_test), 'y_real_test': np.exp(self.y_test)})\n        return self.train_score_log, self.test_score_log, df_train, df_test\n\n    def fit_model(self, x, y, test_size):\n        self.data_split(x, y, test_size)\n        self.model = self.model.fit(self.x_train, self.y_train)\n        self.train_score = self.model.score(self.x_train, self.y_train)\n        self.test_score = self.model.score(self.x_test, self.y_test)\n        self.y_pred_train = self.model.predict(self.x_train)\n        self.y_pred_test = self.model.predict(self.x_test)\n\ndef model_iterations(n, x, y, model_arg, log_bool=False):\n    training_scores = [None]*n\n    testing_scores = [None]*n\n    training_scores_mean_ae = [None]*n\n    testing_scores_mean_ae = [None]*n\n    training_scores_mae = [None]*n\n    testing_scores_mae = [None]*n\n\n\n    for i in range(n):\n        new_model = model(model_arg)\n        new_model.fit_model(x, y, 0.7)\n        training_scores[i], testing_scores[i] = new_model.score_reg() if not log_bool else new_model.score_log()\n        training_scores_mean_ae[i], testing_scores_mean_ae[i]  = new_model.score_mean_abs_err() if not log_bool else new_model.score_mean_log()\n        training_scores_mae[i], testing_scores_mae[i] = new_model.score_median_abs_err() if not log_bool else new_model.score_median_log()\n\n    print('-Best R2 training', np.max(training_scores))\n    print('-Best R2 testing', np.max(testing_scores))\n    print('-Avg R2 training', np.mean(training_scores))\n    print ('-Avg R2 testing',np.mean(testing_scores))\n    print ('Training mean score (_mean absolute error)', np.mean(training_scores_mean_ae))\n    print ('Testing mean score (_mean absolute error)', np.mean(testing_scores_mean_ae))\n\n    print ('Training best score (_mean absolute error)', np.min(training_scores_mean_ae))\n    print ('Training best score (median absolute error)', np.min(training_scores_mae))\n\n    print ('Testing best score (_mean absolute error)', np.min(testing_scores_mean_ae))\n    print ('Testing best score (median absolute error)', np.min(testing_scores_mae))\n    print ('std -ecarts moyens des perfs testing', np.std(testing_scores_mae))\n    print ('std des perfs training', np.std(training_scores_mae))\n\n    return new_model","22eb9217":"def plot_residual(ax1, ax2, ax3, y_pred, y_real, line_label, title):\n    ax1.scatter(y_pred,\n                y_real,\n                color='blue',\n                alpha=0.6,\n                label=line_label)\n    ax1.set_xlabel('Predicted Y')\n    ax1.set_ylabel('Real Y')\n    ax1.legend(loc='best')\n    ax1.set_title(title)\n\n    ax2.scatter(y_pred,\n                y_real - y_pred,\n                color='green',\n                marker='x',\n                alpha=0.6,\n                label='Residual')\n    ax2.set_xlabel('Y Pr\u00e9dit')\n    ax2.set_ylabel('Residual')\n\n    ax2.axhline(y=0, color='black', linewidth=2.0, alpha=0.7, label='y=0')\n\n    ax2.legend(loc='best')\n    ax2.set_title('Residual Graph')\n\n    ax3.hist(y_real - y_pred, bins=30, color='green', alpha=0.7)\n    ax3.set_title('Histogram of residual values')\n\n    return ax1, ax2, ax3\n\ndef plots(model):\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    data_vals = model.data_frame_convert()\n    plot_residual(axes[0][0], axes[0][1], axes[0][2], data_vals[2]['y_pred'], data_vals[2]['y_real'], 'model: {}'.format(data_vals[0]), 'Scatter Plot: Y_Predit vs. Y')\n    plot_residual(axes[1][0], axes[1][1], axes[1][2], data_vals[3]['y_pred_test'], data_vals[3]['y_real_test'], 'model: {}'.format(data_vals[1]), 'Residual Plot for Test Data')\nplt.show()\n","0cf89e80":"del(listings['price'])\nimport xgboost as xgb\ngpu_params = {'tree_method':'gpu_hist',\n              'predictor':'gpu_predictor',\n              'gamma': 1,  \n              'learning_rate': 0.01,\n              'max_depth': 3,\n              'n_estimators': 10000,                                                           \n              'random_state': 98,\n               }\n\nmod_gpu = xgb.XGBRegressor(**gpu_params)\n\nXgb_model = model_iterations(1, listings, Y, mod_gpu, log_bool=False)\n","fc403bba":"listings['price'] = Y\nlistings_prices_inf200 = listings[listings.price<200]\nY_log_prices_inf200 = np.log(listings_prices_inf200.price)\ndel(listings_prices_inf200['price'])\nXgb_model2 = model_iterations(1, listings_prices_inf200, Y_log_prices_inf200, mod_gpu, log_bool=True)","21b5a7c6":"plots(Xgb_model2)","437d901e":"I did a basic grid search before with 4 parameters (4 ^ 4: 256) that lasted more than 2h30.. JH By the way, is it possible to grid-search with GPU ? I use the best parameters obtained:\n","790803a0":"## <a id='4.3'>4.3. Quartile-Quartile plot of target variable<\/a>\n\n\n\nThe Quartile-Quartile graph is a comparison between a distribution (set of values) and another distribution (a variable, a law of probability).\n\nHere, we are interested to know if the data are distributed in a normal way (of parameters mu (average) and sigma (standard deviation)).\n\nIf the data follows the normal distribution, the points will be on the diagonal line.\nWe note that the quantiles of the distribution (values that divide the data into intervals containing the same number of data). are roughly equal to the quartiles of the normal distribution.\n\nThe values are abnormally high at the end of distribution to speak perfectly of normal distribution. There is dissymmetry in the right side (high rental prices). By keeping these values, the risk is to destabilize the future linear model.\n\n","2a200ad7":"### <a id='2.5'>2.5 Return on investment of the property<\/a> \n\n\nWe take into account the price per square meter of the apartment in M\u00e9rignac (2.531 \u20ac) and the configuration of this luxury studio, I will now compute the **return on investment (on a yearly basis)** of this property.\n\nI take **200 nights** (** 55% ** occupancy rate applied over 1 year (10 months)), *a daily rental at the cost of* **69 \u20ac per day** which allows... \n\nA *rental return* of **16%** for this studio. **Important caution** : I took high expectations property value to match the good. This is a HUGE rate realistically computed. I didn't take into account legal fees because, while in New York people are punished for renting, we apply a fine policy in France; this risk is also to add in this situation. Then it is is one way to make money on Airnb... Small optimized space with classy and sober authentic decoration, wood, cozy, high luminosity and notably talent to conjugate everything.","727993c2":"## <a id='2.2'>2.2 Main motivations underlying demand<\/a> \n","13aee1a0":"## <a id='3.5'>3.5 Detection and removal of weird variables<\/a>","5b3ad80f":"**Legend** : *Mean availability (in days) of the properties by Neigbourhood*\n\n<img src=\"https:\/\/raw.githubusercontent.com\/JeremieDec\/home\/master\/pics\/Bordeaux%20Post\/Disponibilite%20moyenne%20des%20Biens%20par%20Quartier%20(moy%2030j).png\" width=\"1200px\">\n\n<img src=\"https:\/\/raw.githubusercontent.com\/JeremieDec\/home\/master\/pics\/Bordeaux%20Post\/Disponibilite%20moyenne%20des%20Biens%20par%20Quartier%20(moy%2060j).png\" width=\"1200px\">\n\n<img src=\"https:\/\/raw.githubusercontent.com\/JeremieDec\/home\/master\/pics\/Bordeaux%20Post\/Disponibilite%20moyenne%20des%20Biens%20par%20Quartier%20(moy%2090j).png\" width=\"1200px\">\n","a5b4cc07":"Note that the lowest average availability is in the 'Capucins-Victoire' district ... Thus, the highest occupancy rate is Capucins-Victoire followed by 'H\u00f4tel de Ville'(City Hall district).\n\nH\u00f4tel de Ville (City Hall district) is the second area with the highest daily rate booking. Would the rental prices of properties in the Capucins-Victoire district be underestimated by host ? This question could be the subject of a future study.","7c300511":"- <a href='#1'>1. Key facts of the study<\/a>  \n- <a href='#2'>2. Exploration and questionnings<\/a> \n    - <a href='#2.1'>2.1 Trends in demand for properties by neighbourhood - at 90, 60, 30 days<\/a> \n    - <a href='#2.2'>2.2 Main motivations underlying demand<\/a> \n    - <a href='#2.3'>2.3 Pay its expenses thanks to Airbnb, in what proportion ?<\/a> \n    - <a href='#2.4'>2.4 The best customer experience- who's the most solicited host ?<\/a> \n        - <a href='#2.5'>2.5. Return on investment of the property<\/a>         \n- <a href='#3'>3. Data cleaning<\/a> \n    - <a href='#3.1'>3.1 Loading libraries & data<\/a> \n    - <a href='#3.2'>3.2 The nullity matrix<\/a> \n    - <a href='#3.3'>3.3 Maintained variables<\/a> \n    - <a href='#3.4'>3.4 Transformed variables<\/a> \n    - <a href='#2.5'>3.5 Detection and removal of weird variables<\/a>\n    - <a href='#2.6'>3.6 Feature engineering<\/a>\n    \n- <a href='#4'>4. Prediction<\/a>\n    - <a href='#4.1'>4.1. Loading libraries<\/a> \n    - <a href='#4.2'>4.2. Which variables are best suited to linearly predict rental price ?<\/a> \n    - <a href='#4.3'>4.3. Quartile-Quartile plot of target variable<\/a> \n    - <a href='#4.4'>4.4. Pipeline class definition for models and perfs measures<\/a> \n    - <a href='#4.5'>4.5. Xgboost- Linear regression<\/a> \n        - <a href='#4.6'>4.6. Price \u2208 (Dataset)\n        - <a href='#4.7'>4.7. log(Price \u2208 0,200)","632c3a8c":"**As you can see**, the rental rate is quite higher than the average property ! Nevertheless the 86% occupancy is probably overestimated as because *the host lists as available the property only up to the end of this year.*","e4edb95e":"# <a id='1'>I. Key facts of the study<\/a> ","7035930f":"# <a id='4'>4. Prediction<\/a>\n\nOnce hosts publish their ads, the price stays fixed throughout time. However, demand varies seasonally and even on *weekly scheldule* (special events). Estimating the price of goods can be a new informative feature for Airbnb hosts. At the gate of fixing the price of their property, host could be suggested a price fork in order to rationalize more the value of its good and opt for a variable price for e.g. It helps to better manage the occupancy rate, profitability and offer attractive opportunities for customers at certain times. \n**It would take part inside a competitive strategy of Airbnb in market acquisition, where the goal is to increase renting rate and take markets parts of its competitors or simply increase demand.**\n\n*The goal is to predict the rental price of goods below \u20ac 200 (~ 227 USD) a night with a median error of \u20ac 10.50.  Which means: 50% of the goods will have to be predicted with an error lower than 10.50 \u20ac on the test data.*","30b73042":"## <a id='4.2'>4.2. Which variables are best suited to linearly predict rental price ?<\/a>\n\n**P-values** \nThe p-value is the probability that two variables are dependent if the correlation coefficient between these two variables is equal to zero (null hypothesis confirmed). If the p-value is smaller than 0.05 (5%) then we can say that the variable is statistically significant.\n\nThanks to the function f_regression, we obtain the F-scores (index [0]) converted into p_values (index [1]) between 0 and 1.\n\nThe beds and cleaning_fee variables have the most ability to predict the price.","40f035b8":"Out []: \ntrain score 0.8906280063056778   #R2\ntest score 0.6984784216019209    #R2\ntrain score (mean absolute error) 9.018808187769666\ntest score (mean absolute error) 14.690016550400438\ntrain score (median absolute error) 6.665931701660156\ntest score (median absolute error) 10.828716278076172","15d7b934":"```\nfeature_importance_dispo30\n\nneighbourhood                                 0.037372\nprice                                         0.037857\nminimum_minimum_nights                        0.038779\nhost_listings_count                           0.045887\nnumber_of_reviews_ltm                         0.049543\nroom_type                                     0.115095\n```","07596a58":"# <a id='3'>3. Data cleaning<\/a>\n## <a id='3.1'>3.1 Loading libraries & data<\/a>","0bd7b4b4":"\nAs we can note,'reviews per month' is not defined while 'first_review' is not defined. If there are no 'first review', it makes common sense that there will be no calculation of 'reviews per month'. The set of these non-defined values are therefore equal to 0.","4eae4d2b":"Thanks you for reading up to here; if you found it useful, please grant a small upvote to this kernel. Thanks you :)","4a22556e":"## <a id='3.3'>3.3 Maintained variables<\/a>\n\nWe keep all the numeric data (47 columns) as well as the categorical variables 'room_type', 'host_since', 'property_type' and 'cancellation_policy'.","2281a8d2":"## <a id='3.4'>3.4 Transformed variables<\/a>\n\nI'm using regression trees in the prediction part. Indeed, splitting the variable into several binary classes (OneHot) may significantly affect the model performance . So, \nI will now encode 'interesting' categorical variables (type of room, property, neighborhood... ) in floats using the LabelEncoder.","99a10b55":"## <a id='3.2'>3.2 The nullity matrix<\/a>","a0a95427":"We temporarily get rid of the different unassigned notes (about 22% of the data). This is an important part of missing data which I remove, its treatment will be reserved at the time of the prediction to test different imputation techniques.","1bfd0a07":"# <a id='2'>II. Exploration and questionnings<\/a> \n\n\n## <a id='2.1'>1.1 Trends in demand for properties by neighbourhood at 90, 60, 30 days<\/a> \n \n","d11bb695":"The nullity matrix gives a quick idea of the distribution of missing data in the dataset.\n\nThis heatmap computes the nullity relation between the different variables thanks to a coefficient of correlation R (-1 \u2264 R \u2264 1).\n\n- When the first variable and the second variable have the **corresponding missing values**, the null correlation is positive perfect **(R = 1)**.\n\n- A perfect negative null correlation (**R = -1**) means that **one of the variables is missing and the second is present**.\n\n- If the null correlation is very close to zero (-0.05 <R <0.05), no value is displayed.\n","1f5d9bd1":"## <a id='2.4'>2.4 The best customer experience: who's the most solicited host ?<\/a> \n\n*The hosts rating ranges from 0 to 15 per month, with more than 3\/4 of ads that have at least a little post-lease attention*. \nHowever, we notice (index 'max') below that there are hosts rated up to 15 times during the month! There is one host, over +9500 who fulfills this exceptional condition:\n\n","91db82d3":"Thanks to Wale Akinfaderin for this tool idea https:\/\/medium.com\/ibm-data-science-experience\/missing-data-conundrum-exploration-and-imputation-techniques-9f40abe0fd87 ","c22a2487":"I first define a function that allows to check the unassigned values for all the variables (in percentages)","d0df1105":"## <a id='4.4'>4.4. Pipeline class definition for models and perfs measures<\/a>\n\nI use a class with the main functions of evaluation models. To evaluate the performance of the algorithm, we calculate on n iterations the average score at each convergence in order to be able to make comparisons. We take 70% of the properties in training, the rest for the validation.\n\nTo evaluate the model, I use the R2 score which is used to measure the precision given by the linear regression model and the mean absolute error (on the train and test set).","0f3ffd31":"## <a id='4.1'>4.1. Loading libraries<\/a>","3d6ea06e":"## <a id='4.7'>4.7 log(Price \u2208 0,200)<\/a>\n","33323a0c":"![Sequence chart](https:\/\/raw.githubusercontent.com\/JeremieDec\/home\/master\/pics\/Bordeaux%20Post\/SeqChart.png)\n\n#### created on lucidchart.com","8f5cc7e5":"*Here we obtain the most important variables that determine the demand for goods. They were outputed by xgboost feature_importance after fitting the values. Thus, these variables represent (in %) how much importance the algorithm (Xgboost regression) accorded  to each variable for splitting.*  \n\nThe type of housing comes first with an importance of 11.5% (0.115), the number of ratings comes next with 5% importance then the number of properties managed by host (4.5%). ","0e1f1b9e":"The dataset was collected by an independent scrapper on InsideAirbnb.\nI use the biggest dataset provided named 'listings_details'.","f8d45bff":"## <a id='4.5'>4.5. Xgboost- Linear regression<\/a>\n## <a id='4.6'>4.6 Price \u2208 (Dataset)<\/a>\n","3a49464c":"<img src=\"https:\/\/raw.githubusercontent.com\/JeremieDec\/home\/master\/pics\/Bordeaux%20Post\/Place_de_la_Bourse_Bordeaux_couv_en.jpg\" width=\"100%\">\n\n## Preface\n\n*Where does the name 'Airbnb' come from? : Founder Brian Chesky and his roommates rented an inflatable mattress in their living room when Brian had the idea to set up the service. Since its launch in 2008, and despite increased competition from a few projects financed by the same fund (Ycombinator), Airbnb has seen in large numbers...*\n\n\nFor this study, we will explore the data collected from Airbnb Bordeaux (*France*) in mid-June 2019. \nThe country became **Airbnb's first market after the United States**. The hexagon has more than *300,000 homes to disposal* and Paris (ahead of London, Rome and Barcelona) ranks as first European cities with 65,852 homes to rent. \n \n\n**For this exploration, I use the whole data (and define a case study) to answer these questions**:\n\n- What is the proportion of licensed goods by neighborhood ?\n\n- Which key variables determine the rental performance of the properties? (with the 'rental performance' corresponding to the actual demand for the properties)\n\n- What is the most popular property in Bordeaux ? I am are particularly interested in a property that is very successful and for which I calculate the amount of Return on Investment (ROI).\n\n- In what proportion can one finance the expenses of his anuity ? \n\n- How are the licences (tax autority declaration) distributed accross the city ?\n\nFinally, I cleaned up the data in order to prepare it to *predict the daily rental price*. I explain the **interest of such indicator** for users and hosts. I finally use and evaluate one algorithm to predict it.\n","bfaade58":"## <a id='2.3'>2.3 Pay its expenses thanks to Airbnb, in what proportion ?<\/a> \n\n*Imagine that you take a collocation with a friend. Unfortunately, the best friend metric is going to a local minima after two month. \nYou realize that he screws up the apartment but he doesn't aggree much with the affirmation, you're overestimating. After much discussion, he takes the step through the door sill, you stay friends, but not too close to avoid underfitting. Now, how will you rentabilize the cost that you've to bear over the next few months ? Indeed, Airbnb may be a handy solution, we'll find how...*\n\nI will try to answer in which proportions it is possible to pay the rent + charges for a two bedroom apartment when *listing one of the rooms on Airbnb* (after rent, utilities (water, gas, electricity) and Internet).\n\nIn order to make correct assumptions, *I take the several data as basis* :\n\n- **24.5 %** is the average share of expenditures (in renting proportion) *to cover the housing cost* (insurance, gas, electricity, internet).\n- The property has a *surface* of **65 m2 (700 sqm)**\n- The average *rental cost* of the city is **14\u20ac \/ m2 (10.76sqm)**. \n- **30 %** is the average occupancy rate for this type of accommodation in Bordeaux *(method below)*\n\n*These numbers lead to :*\n\n- **1130 \u20ac** as the *average monthly rent cost* for a 2 bedrooms accommodation **all charges included** (910 \u20ac (rent) + 220 \u20ac (charges))\n\n*It results in* :\n\n\u00a0- **4068 \u20ac** as the *annual amount of expenses yearly covered by the annuity on Airbnb*\n\n*To answer the question* : **36 %** is the total renting amount one can finance by renting one bedroom in Bordeaux\n \n**Method used to calculate the occupancy rate**\n\nIt is very complex to obtain accurate information on the occupation rate metric of Airbnb.\nPlatforms such as Airdna and others indicate very high rates **(up to 80%) over the next 30 days, only !!**. The calculus often done is based on the average of \"busy\" days (not available) that *they consider by default \"Rented\", over the next 30 days*. For this matter, I use a different and more cautious method:\n\n- Example: 75% of the property is available for a maximum of 18 out of 30 days, which means that it is suspected to be rented for a maximum of 12 days. It is an optimistic asumption because a part of these 12 days (non available) could simply be intentional.\nThis accounts for a max occupancy rate of 40.00% (12\/30).\n\nLet's see the  calculation of max expected availability over the next 30, 60, 90, 365 days of all properties :","84c3287e":"The closer the date is, the higher the ratio. It makes common sense that the closer the date, the highest is the likelihood that the property is rented. I take an average of the four rates which leads to 57 % in overall renting time for properties on the platform.\n\nHowever, the data was collected mi-July : which explains a much higher demand at this period. One should take into account that some people will rent only a few months in a year and-or will profit from their property at a specific time once a year. Some hosts (Airbnb forums) also mention that 1-3 days can happen to fall between bookings. To obtain a more accurate rate, it would be needed to scrape constantly (daily) Airbnb and follow the trend of avaibility at overall. Indeed, in order to find a more realistic ratio for the purpose of this study, I apply a average 50% penalty out of 57%, which will be rounded to 30%. .","c7bef67c":"\n**73 \u20ac** is the average price of a rental for two people, located in the heart of downtown Bordeaux (example of the apartment in second cover photo)\n\n*the type of accommodation, the number of reviews, the amount of rentals per host, the minimum number of nights required to rent, the price and the neighbourhood* (**in order of importance**) are the critical variables that determine the rental performance of the goods (the demand by property).\n\nThe *tax declaration* to the  authority * averages **14%**. We note that the number of licenses is decreasing as we move away from the center.\n\n**16%** is the rental return on investment for the most popular property (counted in terms of reviews)\n\n**36%**, this is the average realistic percentage of the expenses financed (rent + charges) by renting a room in Bordeaux on Airbnb.\n\n50% of goods are correctly predicted with an error lower than 9.53 euros (absolute median) by using Xgboost regressor, for an average error of 14.55 euros .","d2eaab5f":"*I define the plotting function which takes imputs from class model*","6c795d6d":"\n<img src=\"https:\/\/raw.githubusercontent.com\/JeremieDec\/home\/master\/pics\/Bordeaux%20Post\/Selection_006.png\" width=\"100%\">\n\n<img src=\"https:\/\/raw.githubusercontent.com\/JeremieDec\/home\/master\/pics\/Bordeaux%20Post\/ubuntuShutter.png\" width=\"100%\">","6aba10a7":"- The apartment is located in the outskirts of Bordeaux (M\u00e9rignac), although the *surface is reduced* (20 m\u00b2 = 215 sqm.), the decoration and layout makes it a place of first choice. It looks clearly like a high level-profesionnal decoration.\n\n**The numbers** : - the *rate of reviews is 15 per month* -> *15 different guests* -> *a minimum of 15 nights*.\n\n- The 3 next  days soon are not checked (date of this study: mid-July 2019). Also, 6 nights are available next month and 15 next month. Thus, the **occupancy rate** of the property is  certainly **above average**."}}