{"cell_type":{"ee118a7e":"code","49d8108f":"code","2b0c68de":"code","833d6ef0":"code","3ab7d0d9":"code","f29f2dda":"code","f4a21c24":"code","5916ef92":"code","d0344c98":"code","b738d649":"code","d4cfb996":"code","44cbb3d4":"code","ed8a0873":"code","2eafad08":"code","28dfb81e":"code","ef8201c9":"code","168187ef":"code","172a2330":"code","bbd40cf9":"code","b5147835":"code","ee4f06ff":"code","043b47b5":"code","e5900df8":"code","e6345dd8":"markdown","d46719c3":"markdown","ee9e5cf8":"markdown","ce0d274e":"markdown","b3cfa4a0":"markdown","381b9e8f":"markdown","21045e64":"markdown"},"source":{"ee118a7e":"import json\nimport re\nimport pandas as pd\nfrom itertools import combinations\nimport networkx as nx\nimport graphviz\nimport pylab\nfrom networkx.drawing.nx_agraph import graphviz_layout\nimport matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\nfrom collections import Counter","49d8108f":"f = open('\/kaggle\/input\/start-trek-scripts\/all_scripts_raw.json')\njson_file = json.load(f)\nf.close()","2b0c68de":"def get_characters_in_each_scene(series, episode):\n    '''\n    Takes in a string of the series and the episode number.\n    splits the episodes into scenes\n    gets list of characters per scene\n    tries to clean the list of characters to be mo\n    '''\n    \n    ## is it controversial to say that Computer isn't a character?\n    remove_brackets = \"[\\(\\{\\[].*?[\\)\\}\\]]\"\n    non_character_list = [\"BOTH\", \"ALL\", 'ROBOT', \"MAN\", \"ENSIGN\", \"WOMAN\", \"SECURITY\", \n                         \"VOICE\", \"CREWMAN\", 'AMBASSADOR', \"GUARD\", \"PRISONER\", \"CREWWOMAN\", \"ALIEN\",\n                         \"COMPUTER\",'ENGINEER', \"GIRL\", \"BOY\",'LIEUTENANT', 'SOLDIER','OFFICER','GENERAL',\n                         'CAPTAIN', \"HELM\"]\n    \n    ## list of characters with bad naming convention: key is the garbage name, value is the name we want.\n    swap_names = {'ANNIKA':\"SEVEN\",\n                 'FUTURE SEVEN':\"SEVEN\",\n                 'SEVEN OF NINE':\"SEVEN\",\n                 'HOLO-EMH':\"EMH\",\n                 \"B'ELANNA\":\"TORRES\",\n                 }\n                                 \n    lines_in_ep = json_file[series][episode]\n    lines_in_ep = [x.strip() for x in lines_in_ep.split(\"\\n\") if len(x) >2]\n    \n    # Split list into lists by the open close brackets, which usually sets the scene \"[Bridge]\"\n    # code via https:\/\/www.geeksforgeeks.org\/python-split-list-into-lists-by-particular-value\/\n    size = len(lines_in_ep)\n    idx_list = [idx + 1 for idx, val in\n                enumerate(lines_in_ep) if len(val) >0 and val[0] == \"[\" and val[-1] == \"]\"]\n\n    res = [lines_in_ep[i: j] for i, j in\n            zip([0] + idx_list, idx_list + \n            ([size] if idx_list[-1] != size else []))]\n    \n    #split on the : so that it's just the list of all the people who speak\n    # and get rid of the lines where the speaker isn't listed (if a line is more than one \\n, continues to next line)\n    # todo, maybe weight interactions by how many interactions people have had\n    people_per_scene = [[line.split(\":\")[0] for line in scene if \":\" in line] \n                        for scene in res]\n\n    ## get rid of the things in brackets or parenthesis, since there's SPOCK and SPOCK[OC]\n    people_per_scene = [[re.sub(remove_brackets, \"\", person).strip() for person in scene] \n                        for scene in people_per_scene]\n    \n    ## keep only the characters that are all uppercase\n    people_per_scene = [[person for person in scene if person.isupper()] \n                        for scene in people_per_scene]\n    \n    ## get rid of filler non-character names (man, woman, ensign, etc)\n    people_per_scene = [[person for person in scene if person not in non_character_list] \n                        for scene in people_per_scene]    \n    \n    ## swap out the names from the swaplist if we see the random names we don't want\n    people_per_scene = [[swap_names.get(person,person) for person in scene] \n                        for scene in people_per_scene]    \n\n    # make a list of sets of the people in each group\n    people_per_scene = [set(group) for group in people_per_scene if len(group)>1]\n    \n    # the * unpacks the list to pass into the set.union method\n    characters = set().union(*people_per_scene)\n    \n    return (people_per_scene, characters)\n\ndef get_list_all_scenes(series, min_episode_threshold = 2):\n    \n    # list of all the scenes we've seen\n    ## dict of each chracter we've seen and how many eps they've appeared in\n    all_scenes = list()\n    character_appearance_count = dict()\n    \n    for ep in json_file[series].keys():\n        #print(ep)\n        people_per_scene, characterset = get_characters_in_each_scene(series, ep)    \n\n        all_scenes.extend(people_per_scene)\n\n        ## keep track of how many eps a character appears in\n        for dude in characterset:\n            if dude in character_appearance_count.keys():\n                character_appearance_count[dude] += 1\n            else:\n                character_appearance_count[dude] = 1\n\n    ## filter characters to only ones who appeared in enough eps- here, >1, could be fine-tuned.\n    repeat_characters_dict = {key:value for (key,value) in character_appearance_count.items() if value >= min_episode_threshold}\n    repeat_characters = set(repeat_characters_dict.keys())\n    \n    # only keep characters in the scene list if they're a repeat char\n    scenes_with_popular_characters = [{s for s in scene if s in repeat_characters} \n                                      for scene in all_scenes]\n\n    # remove scenes that now only have 1 character\n    scenes_with_popular_characters = [scene for scene in scenes_with_popular_characters if len(scene)>1]\n    \n    return scenes_with_popular_characters\n\ndef make_adj_matrix_from_scene_list(scene_list):\n    \n    '''\n    takes in a scene list, a list of sets with the characters for the scene in each set\n    makes an adjacency matrix dataframe with all the characters on the rows and columns\n    and the value of location a,b being the # of scenes that a and b share.\n    '''\n    \n    cols = list(set().union(*scene_list))\n    df = pd.DataFrame(columns = cols)\n    df[\"Characters\"] = list(cols)\n    df.set_index('Characters', inplace=True)\n    df.fillna(0, inplace=True) ## if they're nans the +=1 doesn't work\n    \n    ## iterate through each set in the scene list, and add the interaction between all the people on the df\n    for scene in scene_list:\n\n        ## get all the combos of chars per set and increase the paring by 1 every time they see it\n        for i,j in list(combinations(scene, 2)):\n            df.loc[i][j]  += 1        \n            df.loc[j][i]  += 1 \n            \n    return df","833d6ef0":"print(\"Options in the next block:\\n\" + \"\\n\".join(json_file.keys()))","3ab7d0d9":"scene_list = get_list_all_scenes(\"VOY\", 3)","f29f2dda":"## create a list of strings of the alphabetical characters in each scene\n## \"ARCHER,TRIP,TUCKER\"\n## Counter (below) didn't work with a list.\nsorted_scene_list = [\",\".join(sorted(x)) for x in scene_list]","f4a21c24":"## make a counter object of all the scene partners, look at the top 10\ncounted_list = Counter(sorted_scene_list)\ncounted_list.most_common(n = 10)","5916ef92":"scene_list2 = get_list_all_scenes(\"VOY\", 1)\nsorted_scene_list2 = [\",\".join(sorted(x)) for x in scene_list2]\ncounted_list2 = Counter(sorted_scene_list2)\ncounted_list2.most_common(n = 10)","d0344c98":"## create df where rows, cols are all characters in at least 3 eps\n## the value if i,j is # of scenes with both character i and j.\ndf = make_adj_matrix_from_scene_list(scene_list)","b738d649":"# transform df from count of scenes between chars to just 1 if there's any scenes.\ngraph_df = df.applymap(lambda x: 1 if x>0 else 0)","d4cfb996":"G = nx.from_pandas_adjacency(graph_df)\nG.name = \"Graph from pandas adjacency matrix\"\nprint(nx.info(G))","44cbb3d4":"nx.draw(G,node_size=16, cmap=pylab.cm.Blues,)\npylab.show()","ed8a0873":"# create edge dict where the keys are each character\n# the values are a set the 3 characters that key had the most scenes with.\n\nedge_dict = dict()\nfor i, row in df.iterrows():\n    edge_dict[i] = set(row.sort_values(ascending = False).index[:3])","2eafad08":"graph = nx.from_dict_of_lists(edge_dict)\nnx.draw(graph,\n        node_size=16, \n        cmap=pylab.cm.Blues,\n        with_labels=True)\npylab.show()","28dfb81e":"## find who has the most scene partners\nmost_scene_partners_series = graph_df.apply(lambda x: x.sum(), axis = 1).sort_values(ascending=False)\nmost_scene_partners = most_scene_partners_series.head(20).index\n\n## who's in the most scenes\nmost_scene_series = df.apply(lambda x: x.sum(), axis = 1).sort_values(ascending=False)\nmost_scene = most_scene_series.head(20).index","ef8201c9":"venn2(subsets=[set(most_scene_partners), set(most_scene)], \n    set_labels=(\"Most Scene Partners\", \"Most Scenes\")\n     )\nplt.show()","168187ef":"most_important_chars = list(set(most_scene).intersection(most_scene_partners))","172a2330":"important_df = df[most_important_chars]\nimportant_df = important_df.loc[most_important_chars]","bbd40cf9":"# create edge dict where the keys are each character\n# the values are a set the 3 characters that key had the most scenes with.\n\nimportant_edge_dict = dict()\nfor i, row in important_df.iterrows():\n    important_edge_dict[i] = set(row.sort_values(ascending = False).index[:3])","b5147835":"important_graph = nx.from_dict_of_lists(important_edge_dict)\nnx.draw(important_graph,\n        node_size=16, \n        cmap=pylab.cm.Blues,\n        with_labels=True)\npylab.show()","ee4f06ff":"not_important_df = df.drop(most_important_chars, axis = 0)\nnot_important_df = not_important_df.drop(most_important_chars, axis = 1)","043b47b5":"not_important_edge_dict = dict()\nfor i, row in not_important_df.iterrows():\n    not_important_edge_dict[i] = set(row.sort_values(ascending = False).index[:3])","e5900df8":"not_important_graph = nx.from_dict_of_lists(not_important_edge_dict)\nnx.draw(not_important_graph,\n        node_size=16, \n        cmap=pylab.cm.Blues,\n        with_labels=True)\npylab.show()","e6345dd8":"# Let's make the graph smaller and more readable","d46719c3":"## Graph each person to just their top 3 scene partners\n\nCause that could use some, well, cleaning.","ee9e5cf8":"## What if we only use the the \"most important\" characters?\n\n...well, what does that even mean? What about looking at who's got the most scenes (seen in original df) and who has the most scene partners (graph_df)\n\nMethodology point, this would still discount any scenes that someone had with only characters that were in under 3 episodes since those were filtered out when we created df. Is this smart? Idk. It's what we're doing.","ce0d274e":"# What's the most common scene group?","b3cfa4a0":"## What if we got rid of \"important\" characters?\n\nIs this good methodology? _Who knows._ ","381b9e8f":"# Make a graph connecting all the characters to all their scene partners\nit's chaos","21045e64":"Quick thought- for the graphs below I wanted to have a threshold of how many episodes a character has to be in before they're in this scene list, so that the number of nodes didn't get totally out of control\n\nbuuuuut for a question like this, it's probably a bad idea since we're dropping characters from the scenes but leaving the remainders - so if character A and B were the only ones in the scene, there's a good chance there were some of those scenes were there was a one-off character also in the scene and ahve been stripped out by the min_episode_threshold parameter in get_list_all_scenes\n\nso.. let's go set that threshold back to 1 and see if the results change"}}