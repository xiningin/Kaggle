{"cell_type":{"d60d00a4":"code","16fe2198":"code","f14556e2":"code","b21f84cb":"code","d1a96deb":"code","95c37381":"code","791e37bf":"code","f54e0dc4":"code","4eef153b":"code","6ee0dca1":"code","ab1ac63d":"code","f90ed82f":"code","f45040eb":"code","04dcfd09":"code","7213d747":"code","97a8769f":"code","fb5e0b92":"code","4c4a8ff8":"code","a4ec038a":"code","bee9a4bd":"code","06137559":"code","25434723":"code","c99340cb":"code","e6805a78":"code","e095d347":"code","9af6b093":"code","da3894b0":"code","545c5916":"code","940cdabf":"code","5451c769":"code","f8c6fe16":"code","ef07b1db":"code","9dd91371":"code","d157e72b":"code","af4eb227":"code","3439aa59":"code","83182042":"code","9b442400":"code","4f4ccd77":"code","3ac77945":"code","300bbed1":"code","3e9a958c":"code","578c1041":"code","8a61a1e5":"code","7b511dca":"code","f0112407":"code","4b4845e0":"code","abc492b8":"code","8e64885e":"code","1efc462c":"code","b96b1ee0":"code","5dc439db":"code","211b386c":"code","8b0b2524":"code","414a3f53":"code","ebeedcad":"code","054faa48":"code","6aef4233":"code","a7241e68":"code","496b1912":"code","ccb7432d":"code","1bf8291d":"code","d67f3cfa":"code","c2e5e0e1":"code","d7ca3b2a":"code","272a32ff":"code","8f1e99c5":"code","0179af1e":"code","32b53675":"code","19987ea4":"code","12c3dda6":"code","0ba07278":"code","051297eb":"code","3ea9f12d":"code","d65ced14":"code","0ff9251d":"markdown","6d0dc8c9":"markdown","c5ccffe8":"markdown","26a8ccda":"markdown"},"source":{"d60d00a4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nsns.set()","16fe2198":"import os\nprint(os.listdir(\"..\/input\"))","f14556e2":"train_data = pd.read_excel(r'..\/input\/Data_Train.xlsx')","b21f84cb":"pd.set_option('display.max_columns', None)\n","d1a96deb":"train_data.head()","95c37381":"train_data.info()","791e37bf":"train_data[\"Duration\"].value_counts()","f54e0dc4":"train_data.shape","4eef153b":"train_data.dropna(inplace = True)","6ee0dca1":"train_data.isnull().sum()","ab1ac63d":"train_data[\"Journey_day\"] = pd.to_datetime(train_data.Date_of_Journey, format = \"%d\/%m\/%Y\").dt.day","f90ed82f":"train_data[\"Journey_month\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month","f45040eb":"train_data.head()","04dcfd09":"train_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)","7213d747":"# Extracting Hours\ntrain_data[\"Dep_hour\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Dep_min\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.minute","97a8769f":"train_data.drop([\"Dep_Time\"], axis = 1, inplace = True)","fb5e0b92":"train_data.head()","4c4a8ff8":"# Extracting Hours\ntrain_data[\"Arrival_hour\"] = pd.to_datetime(train_data.Arrival_Time).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Arrival_min\"] = pd.to_datetime(train_data.Arrival_Time).dt.minute","a4ec038a":"# Now we can drop Arrival_Time as it is of no use\ntrain_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)","bee9a4bd":"train_data.head()","06137559":"# Assigning and converting Duration column into list\nduration = list(train_data[\"Duration\"])\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration","25434723":"# Adding duration_hours and duration_mins list to train_data dataframe\n\ntrain_data[\"Duration_hours\"] = duration_hours\ntrain_data[\"Duration_mins\"] = duration_mins","c99340cb":"train_data.head()","e6805a78":"train_data.drop([\"Duration\"], axis = 1, inplace = True)","e095d347":"train_data.head()","9af6b093":"train_data[\"Airline\"].value_counts()","da3894b0":"# Airline vs Price\nsns.catplot(y = \"Price\", x = \"Airline\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 6, aspect = 3)\nplt.show()","545c5916":"Airline = train_data[[\"Airline\"]]\nAirline = pd.get_dummies(Airline, drop_first = True)\nAirline.head()\n","940cdabf":"train_data[\"Source\"].value_counts()","5451c769":"# Source vs Price\n\nsns.catplot(y = \"Price\", x = \"Source\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 4, aspect = 3)\nplt.show()","f8c6fe16":"Source = train_data[[\"Source\"]]\nSource = pd.get_dummies(Source, drop_first = True)\nSource.head()","ef07b1db":"train_data[\"Destination\"].value_counts()","9dd91371":"\nDestination = train_data[[\"Destination\"]]\n\nDestination = pd.get_dummies(Destination, drop_first = True)\n\nDestination.head()","d157e72b":"train_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)","af4eb227":"train_data[\"Total_Stops\"].value_counts()","3439aa59":"# As this is case of Ordinal Categorical type we perform LabelEncoder\n# Here Values are assigned with corresponding keys\n\ntrain_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)","83182042":"train_data.head()","9b442400":"# Concatenate dataframe --> train_data + Airline + Source + Destination\n\ndata_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)","4f4ccd77":"data_train.head()","3ac77945":"data_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","300bbed1":"test_data = pd.read_excel(r\"..\/input\/Test_set.xlsx\")","3e9a958c":"# Preprocessing\n\nprint(\"Test data Info\")\nprint(\"-\"*75)\nprint(test_data.info())\n\nprint()\nprint()\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest_data.dropna(inplace = True)\nprint(test_data.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest_data[\"Journey_day\"] = pd.to_datetime(test_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\ntest_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\ntest_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\ntest_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\ntest_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest_data[\"Duration_hours\"] = duration_hours\ntest_data[\"Duration_mins\"] = duration_mins\ntest_data.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test_data[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test_data[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test_data[\"Source\"].value_counts())\nSource = pd.get_dummies(test_data[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test_data[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test_data + Airline + Source + Destination\ndata_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n\ndata_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", data_test.shape)","578c1041":"data_test.head()","8a61a1e5":"data_train.shape","7b511dca":"data_train.columns","f0112407":"X = data_train.loc[:, ['Total_Stops', 'Journey_day', 'Journey_month', 'Dep_hour',\n       'Dep_min', 'Arrival_hour', 'Arrival_min', 'Duration_hours',\n       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n       'Airline_Jet Airways', 'Airline_Jet Airways Business',\n       'Airline_Multiple carriers',\n       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\n       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',\n       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\n       'Destination_Kolkata', 'Destination_New Delhi']]\nX.head()","4b4845e0":"y = data_train.iloc[:, 1]\ny.head()","abc492b8":"# Finds correlation between Independent and dependent attributes\n\nplt.figure(figsize = (18,18))\nsns.heatmap(train_data.corr(), annot = True, cmap = \"RdYlGn\")\n\nplt.show()","8e64885e":"# Important feature using ExtraTreesRegressor\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X, y)","1efc462c":"print(selection.feature_importances_)","b96b1ee0":"#plot graph of feature importances for better visualization\n\nplt.figure(figsize = (12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","5dc439db":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","211b386c":"from sklearn.ensemble import RandomForestRegressor\nreg_rf = RandomForestRegressor()\nreg_rf.fit(X_train, y_train)","8b0b2524":"y_pred = reg_rf.predict(X_test)","414a3f53":"reg_rf.score(X_train, y_train)","ebeedcad":"reg_rf.score(X_test, y_test)","054faa48":"sns.distplot(y_test-y_pred)\nplt.show()","6aef4233":"plt.scatter(y_test, y_pred, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","a7241e68":"from sklearn import metrics","496b1912":"print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","ccb7432d":"metrics.r2_score(y_test, y_pred)","1bf8291d":"from sklearn.model_selection import RandomizedSearchCV","d67f3cfa":"#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","c2e5e0e1":"# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","d7ca3b2a":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","272a32ff":"rf_random.fit(X_train,y_train)","8f1e99c5":"rf_random.best_params_","0179af1e":"prediction = rf_random.predict(X_test)","32b53675":"plt.figure(figsize = (8,8))\nsns.distplot(y_test-prediction)\nplt.show()","19987ea4":"plt.figure(figsize = (8,8))\nplt.scatter(y_test, prediction, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","12c3dda6":"print('MAE:', metrics.mean_absolute_error(y_test, prediction))\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))","0ba07278":"import pickle\n# open a file, where you ant to store the data\nfile = open('flight_rf_new.pkl', 'wb')\n\n# dump information to that file\npickle.dump(rf_random, file)","051297eb":"model = open('flight_rf_new.pkl','rb')\nforest = pickle.load(model)","3ea9f12d":"y_prediction = forest.predict(X_test)","d65ced14":"metrics.r2_score(y_test, y_prediction)","0ff9251d":"Hyperparameter Tuning\nChoose following method for hyperparameter tuning\nRandomizedSearchCV --> Fast\nGridSearchCV\nAssign hyperparameters in form of dictionery\nFit the model\nCheck best paramters and best score","6d0dc8c9":"Fitting model using Random Forest\nSplit dataset into train and test set in order to prediction w.r.t X_test\nIf needed do scaling of data\nScaling is not done in Random forest\nImport model\nFit the data\nPredict w.r.t X_test\nIn regression check RSME Score\nPlot graph\n","c5ccffe8":"Feature Selection\nFinding out the best feature which will contribute and have good relation with target variable. Following are some of the feature selection methods,\n\nheatmap\nfeature_importance_\nSelectKBest\n","26a8ccda":"Test Data"}}