{"cell_type":{"c831ba9a":"code","f6dc5251":"code","c10cd298":"code","feead153":"code","efb008d3":"code","5b64eb53":"code","9c04e170":"code","fcc545fa":"code","6169f88d":"code","1a5beede":"code","ac453c86":"code","20f9c504":"code","ee168e91":"code","ad89dce4":"code","92083b0d":"code","ff52760e":"code","d5bb290e":"code","9b74c3d6":"code","4f4ec688":"code","8bfb002a":"code","f868cc15":"code","f893e287":"code","4d81f52c":"code","09cc092f":"code","ee452a0d":"code","875f9615":"code","24ef5614":"code","ccd42166":"code","29622b8a":"code","afb23471":"code","083fb6bc":"code","2d0f6541":"code","d59dda35":"code","2f93fd08":"code","4ae3a17b":"code","8320ff8c":"code","78119af7":"code","fa1f076a":"code","1f103565":"code","71163876":"code","aaac1204":"code","8e3cf6b6":"code","c1579ddc":"code","412526e9":"code","7911f724":"code","0b0a6a0c":"markdown","c5cd865d":"markdown","f489f578":"markdown","24499d33":"markdown","3cead7ef":"markdown","00f2e9c9":"markdown","a9e133b3":"markdown","f488dbec":"markdown","92a96f48":"markdown","7ff935cc":"markdown","6db78991":"markdown","3b6250aa":"markdown","cd2ded2d":"markdown","55451e0c":"markdown","4d6cc99d":"markdown","1d508af1":"markdown","0b141185":"markdown","13868720":"markdown","89cea100":"markdown","87780bf0":"markdown","5cd94eca":"markdown","be346a75":"markdown","79501176":"markdown","1317f0f9":"markdown","e1afa4a6":"markdown","2f1dc723":"markdown","75ad669a":"markdown","475928a7":"markdown","baa7bd69":"markdown","ab421242":"markdown","d5707a2d":"markdown"},"source":{"c831ba9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure","f6dc5251":"\"\"\"# Installing and importing gmplot \n!pip install gmplot\n    \nimport gmplot\n\n## Installing and importing geopy \n!pip install geopy\n\nimport geopy\n  \nprint('Please insert your Google Maps Api Key')\napi_key = input()\ngmap = gmplot.GoogleMapPlotter(41.9109, 12.4818, 6, apikey=api_key)\n\n# Retrieving the lat and long \nfrom geopy.geocoders import Nominatim\n\nlocations = ['Auser, Italy', 'Doganella, Italy', 'Luco, Italy', 'Petrignano, Italy', 'Bilancino lake, Italy',\n            'Amiata, Italy', 'Lupa, Italy', 'Arno River, Italy', 'Madonna di Canneto, Italy']\n\ngeolocator = Nominatim(user_agent=\"localizer\")\n\nlatitudes = []\nlongitudes = []\nlabels = []\nfor loc in locations:\n    latitudes.append(geolocator.geocode(loc).latitude)\n    longitudes.append(geolocator.geocode(loc).longitude)\n    labels.append(loc.split(',')[0])\nlabels\ngmap.scatter(latitudes, longitudes, color='#3B0B39', size=40, marker=True, title = labels)\n\n# Draw the map:\ngmap.draw('.\/map.html')\"\"\"","c10cd298":"# Reading and basic exploration of the data\nauser = pd.read_csv('..\/input\/acea-water-prediction\/Aquifer_Auser.csv')\ndoganella = pd.read_csv('..\/input\/acea-water-prediction\/Aquifer_Doganella.csv')\nluco = pd.read_csv('..\/input\/acea-water-prediction\/Aquifer_Luco.csv')\npetrignano = pd.read_csv('..\/input\/acea-water-prediction\/Aquifer_Petrignano.csv')","feead153":"df = auser","efb008d3":"# Printing the dataframe without NaNs\ndf.dropna().head(10)","5b64eb53":"# Retrieving the dataframe columns\ncolumns = df.columns.tolist()\n\nprint('Number of variables: ' + str(len(columns)))\nprint('Variables type:')\nprint(df.dtypes)\n\n# Descriptive statistics summary\ndf.describe()","9c04e170":"# Histogram to understand the data distribution of some relevant features\n# Number of columns\nncols = 5\n# Number of rows\nimport math\nnrows = math.ceil((len(columns) -1) \/ 5)\nfig, axs = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 20))\ngrid = []\nfor j in range(nrows):\n    for h in range(ncols):\n        grid.append([j,h])\n[sns.distplot(df[columns[i+1]], ax=axs[grid[i][0], grid[i][1]]) for i in range(0,len(columns)-1)]\nprint('### AUSER ###')","fcc545fa":"x_plot = pd.to_datetime(df['Date'], format = '%d\/%m\/%Y')\n\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) - 1, figsize= (22, 8 * len(columns)))\n\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(x_plot, df[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 25)\n    \n    axs[var-1].grid()","6169f88d":"total = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# We are going to consider just the feaures that have at least 1 missing value\nmissing_data = missing_data[missing_data['Percent'] > 0]\nmissing_data","1a5beede":"# Plotting the percentage of the missing data.\nfigure(figsize= (22, 8))\nsns.barplot(x = missing_data.index, y = missing_data['Percent'], palette=\"rocket\")\n\nax = plt.gca()\nax.axhline(0, color=\"k\", clip_on=False)\nax.set_ylabel(\"Percent\", fontsize = 25)\nax.set_xlabel(\"Variable\", fontsize = 25)\nplt.xticks(fontsize=18, rotation=90)\nplt.yticks(fontsize=18)","ac453c86":"df['Date'] = pd.to_datetime(df['Date'], format = '%d\/%m\/%Y')\n\ndf['year'] = df['Date'].dt.year\n\nnan_by_year = pd.DataFrame(index = df['year'].unique())\n\nfor col in missing_data.index.tolist():\n    count = auser[col].isnull().groupby(df['year']).sum().astype(int).reset_index(name='count')\n    nan_by_year[col] = count['count'].values\nnan_by_year","20f9c504":"figure(figsize= (22, 12))\nmissing_percent = nan_by_year\/365\nsns.heatmap(missing_percent, annot = True,  linewidths=.5, cmap = 'rocket_r')\n\nax = plt.gca()\nax.set_ylabel(\"Year\", fontsize = 25)\nax.set_xlabel(\"Variable\", fontsize = 25)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18, rotation=0)\n\nax.set_title(\"% of Missing values per year\", fontsize = 35, pad = 25)\n\nplt.savefig('Auser_missing.png', bbox_inches='tight')","ee168e91":"df = doganella","ad89dce4":"# Printing the dataframe without NaNs\ndf.dropna().head(10)","92083b0d":"# Retrieving the dataframe columns\ncolumns = df.columns.tolist()\n\nprint('Number of variables: ' + str(len(columns)))\nprint('Variables type:')\nprint(df.dtypes)\n\n# Descriptive statistics summary\ndf.describe()","ff52760e":"# Histogram to understand the data distribution of some relevant features\n# Number of columns\nncols = 5\n# Number of rows\nimport math\nnrows = math.ceil((len(columns) -1) \/ 5)\nfig, axs = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 20))\ngrid = []\nfor j in range(nrows):\n    for h in range(ncols):\n        grid.append([j,h])\n[sns.distplot(df[columns[i+1]], ax=axs[grid[i][0], grid[i][1]]) for i in range(0,len(columns)-1)]\nprint('### DOGANELLA ###')","d5bb290e":"x_plot = pd.to_datetime(df['Date'], format = '%d\/%m\/%Y')\n\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) - 1, figsize= (22, 8 * len(columns)))\n\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(x_plot, df[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 25)\n    \n    axs[var-1].grid()","9b74c3d6":"total = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# We are going to consider just the feaures that have at least 1 missing value\nmissing_data = missing_data[missing_data['Percent'] > 0]\nmissing_data","4f4ec688":"# Plotting the percentage of the missing data.\nfigure(figsize= (22, 8))\nsns.barplot(x = missing_data.index, y = missing_data['Percent'], palette=\"rocket\")\n\nax = plt.gca()\nax.axhline(0, color=\"k\", clip_on=False)\nax.set_ylabel(\"Percent\", fontsize = 25)\nax.set_xlabel(\"Variable\", fontsize = 25)\nplt.xticks(fontsize=18, rotation=90)\nplt.yticks(fontsize=18)","8bfb002a":"df['Date'] = pd.to_datetime(df['Date'], format = '%d\/%m\/%Y')\n\ndf['year'] = df['Date'].dt.year\n\nnan_by_year = pd.DataFrame(index = df['year'].unique())\n\nfor col in missing_data.index.tolist():\n    count = df[col].isnull().groupby(df['year']).sum().astype(int).reset_index(name='count')\n    nan_by_year[col] = count['count'].values\nnan_by_year","f868cc15":"figure(figsize= (22, 12))\nmissing_percent = nan_by_year\/365\nsns.heatmap(missing_percent, annot = True,  linewidths=.5, cmap = 'rocket_r')\n\nax = plt.gca()\nax.set_ylabel(\"Year\", fontsize = 25)\nax.set_xlabel(\"Variable\", fontsize = 25)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18, rotation=0)\n\nax.set_title(\"% of Missing values per year\", fontsize = 35, pad = 25)\nplt.savefig('Doganella_missing.png', bbox_inches='tight')","f893e287":"df = luco","4d81f52c":"# Printing the dataframe without NaNs\ndf.dropna().head(10)","09cc092f":"# Retrieving the dataframe columns\ncolumns = df.columns.tolist()\n\nprint('Number of variables: ' + str(len(columns)))\nprint('Variables type:')\nprint(df.dtypes)\n\n# Descriptive statistics summary\ndf.describe()","ee452a0d":"# Histogram to understand the data distribution of some relevant features\n# Number of columns\nncols = 5\n# Number of rows\nimport math\nnrows = math.ceil((len(columns) -1) \/ 5)\nfig, axs = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 20))\ngrid = []\nfor j in range(nrows):\n    for h in range(ncols):\n        grid.append([j,h])\n[sns.distplot(df[columns[i+1]], ax=axs[grid[i][0], grid[i][1]], kde_kws = {'bw' : 1}) for i in range(0,len(columns)-1)]\nprint('### LUCO ###')","875f9615":"x_plot = pd.to_datetime(df['Date'], format = '%d\/%m\/%Y')\n\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) - 1, figsize= (22, 8 * len(columns)))\n\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(x_plot, df[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 25)\n    \n    axs[var-1].grid()","24ef5614":"total = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# We are going to consider just the feaures that have at least 1 missing value\nmissing_data = missing_data[missing_data['Percent'] > 0]\nmissing_data","ccd42166":"# Plotting the percentage of the missing data.\nfigure(figsize= (22, 8))\nsns.barplot(x = missing_data.index, y = missing_data['Percent'], palette=\"rocket\")\n\nax = plt.gca()\nax.axhline(0, color=\"k\", clip_on=False)\nax.set_ylabel(\"Percent\", fontsize = 25)\nax.set_xlabel(\"Variable\", fontsize = 25)\nplt.xticks(fontsize=18, rotation=90)\nplt.yticks(fontsize=18)","29622b8a":"df['Date'] = pd.to_datetime(df['Date'], format = '%d\/%m\/%Y')\n\ndf['year'] = df['Date'].dt.year\n\nnan_by_year = pd.DataFrame(index = df['year'].unique())\n\nfor col in missing_data.index.tolist():\n    count = df[col].isnull().groupby(df['year']).sum().astype(int).reset_index(name='count')\n    nan_by_year[col] = count['count'].values\nnan_by_year","afb23471":"figure(figsize= (22, 12))\nmissing_percent = nan_by_year\/365\nsns.heatmap(missing_percent, annot = True,  linewidths=.5, cmap = 'rocket_r')\n\nax = plt.gca()\nax.set_ylabel(\"Year\", fontsize = 25)\nax.set_xlabel(\"Variable\", fontsize = 25)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18, rotation=0)\n\nax.set_title(\"% of Missing values per year\", fontsize = 35, pad = 25)\n\nplt.savefig('Luco_missing.png', bbox_inches='tight')","083fb6bc":"df = petrignano","2d0f6541":"# Printing the dataframe without NaNs\ndf.dropna().head(10)","d59dda35":"# Retrieving the dataframe columns\ncolumns = df.columns.tolist()\n\nprint('Number of variables: ' + str(len(columns)))\nprint('Variables type:')\nprint(df.dtypes)\n\n# Descriptive statistics summary\ndf.describe()","2f93fd08":"# Histogram to understand the data distribution of some relevant features\n# Number of columns\nncols = 5\n# Number of rows\nimport math\nnrows = math.ceil((len(columns) -1) \/ 5)\nfig, axs = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 20))\ngrid = []\nfor j in range(nrows):\n    for h in range(ncols):\n        grid.append([j,h])\n[sns.distplot(df[columns[i+1]], ax=axs[grid[i][0], grid[i][1]]) for i in range(0,len(columns)-1)]\nprint('### PETRIGNANO ###')","4ae3a17b":"x_plot = pd.to_datetime(df['Date'], format = '%d\/%m\/%Y')\n\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) - 1, figsize= (22, 8 * len(columns)))\n\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(x_plot, df[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 25)\n    \n    axs[var-1].grid()","8320ff8c":"total = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# We are going to consider just the feaures that have at least 1 missing value\nmissing_data = missing_data[missing_data['Percent'] > 0]\nmissing_data","78119af7":"# Plotting the percentage of the missing data.\nfigure(figsize= (22, 8))\nsns.barplot(x = missing_data.index, y = missing_data['Percent'], palette=\"rocket\")\n\nax = plt.gca()\nax.axhline(0, color=\"k\", clip_on=False)\nax.set_ylabel(\"Percent\", fontsize = 25)\nax.set_xlabel(\"Variable\", fontsize = 25)\nplt.xticks(fontsize=18, rotation=90)\nplt.yticks(fontsize=18)","fa1f076a":"df['Date'] = pd.to_datetime(df['Date'], format = '%d\/%m\/%Y')\n\ndf['year'] = df['Date'].dt.year\n\nnan_by_year = pd.DataFrame(index = df['year'].unique())\n\nfor col in missing_data.index.tolist():\n    count = df[col].isnull().groupby(df['year']).sum().astype(int).reset_index(name='count')\n    nan_by_year[col] = count['count'].values\nnan_by_year\n\nfigure(figsize= (22, 12))\nmissing_percent = nan_by_year\/365\nsns.heatmap(missing_percent, annot = True,  linewidths=.5, cmap = 'rocket_r')\n\nax = plt.gca()\nax.set_ylabel(\"Year\", fontsize = 25)\nax.set_xlabel(\"Variable\", fontsize = 25)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18, rotation=0)\n\nax.set_title(\"% of Missing values per year\", fontsize = 35, pad = 25)\n\nplt.savefig('Petrignano_missing.png', bbox_inches='tight')","1f103565":"data_frames = { 'auser' : auser, 'doganella' :doganella,\n               'luco' :luco, 'petrignano' :petrignano }\n\nfor key in data_frames.keys():\n    \n    df = data_frames[key].dropna()\n\n    columns = df.columns.tolist()\n\n    ncols = 5\n    nrows = math.ceil((len(columns) -1) \/ 5)\n    fig, axs = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 30))\n    \n    fig.suptitle(key.capitalize(), fontsize = 35, va = 'center')\n\n    grid = []\n\n    for j in range(nrows):\n        for h in range(ncols):\n            grid.append([j,h])\n    [df.boxplot(column = columns[i+1], ax=axs[grid[i][0], grid[i][1]]) for i in range(0,len(columns)-1)]\n    ","71163876":"init = True\n\nfor key in data_frames.keys():\n    \n    df = data_frames[key]\n    \n    for column in df.columns:\n        \n        if df[column].name != 'Date':\n            mean = np.mean(df[column]) \n            std = np.std(df[column])\n            \n            z = abs((df[column]-mean)\/std)\n            \n            if init == True:\n                z_scores = pd.DataFrame(z.sort_values(ascending = False)[0:25].values ,columns = [[key], [z.name]], index = ['Top ' + str(i) for i in range(1,26)])\n                init = False\n            else:\n                z_scores[key,z.name] = z.sort_values(ascending = False)[0:25].values\n                \n        \nz_scores","aaac1204":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 150)","8e3cf6b6":"z_scores['auser']","c1579ddc":"z_scores['doganella']","412526e9":"z_scores['luco']","7911f724":"z_scores['petrignano']","0b0a6a0c":"# **Acea**\nThe Acea Group is one of the leading Italian multiutility operators. Listed on the Italian Stock Exchange since 1999, the company manages and develops water and electricity networks and environmental services. Acea is the foremost Italian operator in the water services sector supplying 9 million inhabitants in Lazio, Tuscany, Umbria, Molise, Campania.\n\n# Introduction\nIn this competition we will focus only on the water sector to help Acea Group preserve precious waterbodies. As it is easy to imagine, a water supply company struggles with the need to forecast the water level in a waterbody (water spring, lake, river, or aquifer) to handle daily consumption. During fall and winter waterbodies are refilled, but during spring and summer they start to drain. To help preserve the health of these waterbodies it is important to predict the most efficient water availability, in terms of level and water flow for each day of the year.\n\n## Data\nThe reality is that each waterbody has such unique characteristics that their attributes are not linked to each other. This analytics competition uses datasets that are completely independent from each other. However, it is critical to understand total availability in order to preserve water across the country.\n\nEach dataset represents a different kind of waterbody. As each waterbody is different from the other, the related features are also different. So, if for instance we consider a water spring we notice that its features are different from those of a lake. These variances are expected based upon the unique behavior and characteristics of each waterbody. The Acea Group deals with four different type of waterbodies: water springs, lakes, rivers and aquifers.\n\n## Challenge\nCan you build a story to predict the amount of water in each unique waterbody? The challenge is to determine how features influence the water availability of each presented waterbody. To be more straightforward, gaining a better understanding of volumes, they will be able to ensure water availability for each time interval of the year.\n\nThe time interval is defined as day\/month depending on the available measures for each waterbody. Models should capture volumes for each waterbody(for instance, for a model working on a monthly interval a forecast over the month is expected).\n\nThe desired outcome is a notebook that can generate four mathematical models, one for each category of waterbody (acquifers, water springs, river, lake) that might be applicable to each single waterbody.\n\n![](https:\/\/storage.googleapis.com\/newagent-ahdsra.appspot.com\/inbox_6195295_cca952eecc1e49c54317daf97ca2cca7_Acea-Input.png)\n\n# Evaluation\nCan you build a model to predict the amount of water in each waterbody to help preserve this natural resource?\nThis is an Analytics competition where your task is to create a Notebook that best addresses the Evaluation criteria below. Submissions should be shared directly with host and will be judged by the Acea Group based on how well they address:\n\n### Methodology\/Completeness (min 0 points, max 5 points)\n* Are the statistical models appropriate given the data?\n* Did the author develop one or more machine learning models?\n* Did the author provide a way of assessing the performance and accuracy of their solution?\n* What is the Mean Absolute Error (MAE) of the models?\n* What is the Root Mean Square Error (RMSE) of the models?\n\n### Presentation (min 0 points, max 5 points)\n* Does the notebook have a compelling and coherent narrative?\n* Does the notebook contain data visualizations that help to communicate the author\u2019s main points?\n* Did the author include a thorough discussion on the intersection between features and their prediction? For example between rainfall and amount\/level of water.\n* Was there discussion of automated insight generation, demonstrating what factors to take into account?\n* Is the code documented in a way that makes it easy to understand and reproduce?\n* Were all external sources of data made public and cited appropriately?\n\n### Application (min 0 points, max 5 points)\n* Is the provided model useful\/able to forecast water availability in terms of level or water flow in a time interval of the year?\n* Is the provided methodology applicable also on new datasets belong to another waterbody?","c5cd865d":"### Doganella","f489f578":"The result can be seen [here](https:\/\/storage.googleapis.com\/newagent-ahdsra.appspot.com\/map.html).","24499d33":"### Doganella\n\nDescription: The wells field Doganella is fed by two underground aquifers not fed by rivers or lakes but fed by meteoric infiltration. The upper aquifer is a water table with a thickness of about 30m. The lower aquifer is a semi-confined artesian aquifer with a thickness of 50m and is located inside lavas and tufa products. These aquifers are accessed through wells called Well 1, ..., Well 9. Approximately 80% of the drainage volumes come from the artesian aquifer. The aquifer levels are influenced by the following parameters: rainfall, humidity, subsoil, temperatures and drainage volumes.","3cead7ef":"#### Missing data","00f2e9c9":"# **Methodology**\n\nEvery waterbody category, i.e. **aquifers, water springs, lakes, rivers**, will be considered as stand alone. In this way it'll be possible to optimize every phase for every category in order to obtain the best result. In particular the different phases that will be developed are:\n1. **Data exploration and cleaning;**\n2. **Feature extraction;**\n3. **Feature selection;**\n4. **Model development;**\n5. **Results evaluation;**\n6. **Deployment.**","a9e133b3":"## Missing data\n\nWe'll use different approaches for each variable.\n\nFirst of all we are going to consider just the periods where the target variables have an accettable amount of missing data.\nThe options we'll consider are:\n\n**Interpolation:** \n- Linear;\n- Polynomial;\n- Akima;\n\n**Stochastic Methods:**\n- Regression Methods;\n- AutoRegressive Methods;","f488dbec":"### Luco\n\nDescription: The Luco wells field is fed by an underground aquifer. This aquifer not fed by rivers or lakes but by meteoric infiltration at the extremes of the impermeable sedimentary layers. Such aquifer is accessed through wells called Well 1, Well 3 and Well 4 and is influenced by the following parameters: rainfall, depth to groundwater, temperature and drainage volumes.","92a96f48":"### Auser","7ff935cc":"### Z-Score\nWe're going to use the z-score to detect the outliers but we'll analyze the results before doing anything.\n\nZ score is an important concept in statistics. Z score is also called standard score. This score helps to understand if a data value is greater or smaller than mean and how far away it is from the mean. More specifically, Z score tells how many standard deviations away a data point is from the mean.\n\n**Z score = (x -mean) \/ std. deviation**\n\nLet's create a table where we'll list the highest absolute value of the z-score for each variable in order to understand better a way to remove just the outliers.","6db78991":"#### Missing data","3b6250aa":"#### Missing data","cd2ded2d":"## Checkpoint\nLet's try to wrap up what we've seen so far in order to understand how to go ahead.\n\nThe most important point to analyze seems to be the missing data. Indeed we have seen as for almost all the variables missing data is a serious problem, considering that when more than 15% of the data is missing we should delete the corresponding variable and pretend it never existed. The situation is even worse when the variable affected is the target variable; for this reason we must be careful on choosing the best approach to clean the data.\n\nFor every aquifier we're going to list here the target variables and how's the relative missing data situation (Considering that unitl 2005\/06 all variables are unusable):\n### Auser\n* **Depth_to_Groundwater_SAL:** From 2010 til 2020 it seems we have an acceptable percentage of missing data except for year 2016;\n* **Depth_to_Groundwater_COS:** Data is not looking good like the previous one, years 2011\/13\/14 are rich of missing data;\n* **Depth_to_Groundwater_LT2:** It seems very siilar to \"Depth_to_Groundwater_SAL\" but in years 2011\/12 we have high rates of missing data.\n![Auser](.\/Auser_missing.png)\n\n### Doganella\nIn general we are going to consider years from 2013 on and moreover 2017 seems to be critical for most of the target variables except for \"Pozzo\" 2 and 3.\n\n* **Depth_to_Groundwater_Pozzo_1\/2:** Don't look very bad;\n* **Depth_to_Groundwater_Pozzo_3:** 2019\/20 missing data rates a bit too high;\n* **Depth_to_Groundwater_Pozzo_4:** Same as \"Pozzo 3\" but with 2016 very high;\n* **Depth_to_Groundwater_Pozzo_5:** 2015\/16 missing data rates too high;\n* **Depth_to_Groundwater_Pozzo_6:** Looks good apart from 2016;\n* **Depth_to_Groundwater_Pozzo_7:** 2014\/16 missing data rates too high;\n* **Depth_to_Groundwater_Pozzo_8:** 2019\/20 missing data rates a bit too high;\n* **Depth_to_Groundwater_Pozzo_9:** 2014\/16 missing data rates too high;\n![Doganella](.\/Doganella_missing.png)\n\n### Luco\n\n* **Depth_to_Groundwater_Podere_Casetta:** Very complicated situation I'd say! Target data are present mainly from 2008 to 2015 but for this period we also have a lot of missing data in the other variables. Let's see that we'll be able to do here!\n\n![Luco](.\/Luco_missing.png)\n\n### Petrignano\n\n**I Like it!!**\n![Petrignano](.\/Petrignano_missing.png)\n","55451e0c":"### Auser","4d6cc99d":"### Petrignano\n\nDescription: The wells field of the alluvial plain between Ospedalicchio di Bastia Umbra and Petrignano is fed by three underground aquifers separated by low permeability septa. The aquifer can be considered a water table groundwater and is also fed by the Chiascio river. The groundwater levels are influenced by the following parameters: rainfall, depth to groundwater, temperatures and drainage volumes, level of the Chiascio river.","1d508af1":"## Aquifers","0b141185":"# Modules import","13868720":"# 2. Data Cleaning\n\n## Outliers\n\nBefore diving into missing data we are going to deal with outliers and we'll set them as NaN in such a way that during the data cleaning process they will be considered as well.\n\nFor simplicity we'll remove the outliers in batch, considering all the quifiers together.\n\nLet's visualize the data with a plot box.","89cea100":"# 1. Exploratory Data Analysis\n\nIn this section we are going to explore the data, one category at the time, in order to understand the variables and their ditribution. After this we will analyze the missing data and we'll find a way to fill and clean them for every category in order to have a cleansing methodology that is replicable also with other datasets belonging with that given category.","87780bf0":"![](https:\/\/storage.googleapis.com\/newagent-ahdsra.appspot.com\/header.png)","5cd94eca":"### Auser\n\nDescription: This waterbody consists of two subsystems, called NORTH and SOUTH, where the former partly influences the behavior of the latter. Indeed, the north subsystem is a water table (or unconfined) aquifer while the south subsystem is an artesian (or confined) groundwater.\n\nThe levels of the NORTH sector are represented by the values of the SAL, PAG, CoS and DIEC wells, while the levels of the SOUTH sector by the LT2 well.","be346a75":"### Luco","79501176":"# 0. Geographic overview of the sites.","1317f0f9":"### Petrignano","e1afa4a6":"#### Missing data\n\nThe questions we want to answer by analyzing missing data are:\n* How prevalent is the missing data?\n* Is missing data random or does it have a pattern?","2f1dc723":"# Some literature review\n\n> ### *It's always a good practise to start from a solid base*\n\n\n## [Prediction of Water Level using Monthly Lagged Data in Lake Urmia, Iran.](https:\/\/link.springer.com\/article\/10.1007\/s11269-016-1463-y#citeas)<br><sup>*Babak Vaheddoost, Hafzullah Aksoy, Hirad Abghari*<\/sup>\nIn this interesting work the authors are using parametric and nonparametric models for predicting monthly water level fluctuations in Lake Urmia. Eleven previous water levels in the form of monthly lagged data are used as the known independent variables of the model while lake water level at the twelfth month is considered as the unknown dependent variable to be predicted. Parametric models used in the modelling are multi-linear regression (MLR), additive and multiplicative non-linear regression (ANLR and MNLR) and decision tree (DT) while feed forward back propagation neural network (FFBP-NN), generalized regression neural network (GR-NN) and radial basis function neural network (RBF-NN) are used to represent the non-parametric approach.\n","75ad669a":"**Is missing data random or does it have a pattern?**\n\nIt is possible that the missing data are relative to the same period.","475928a7":"With some variables it seems to be easy to detect which are the outliers. Take for example **Depth_to_Groundwater_CoS** in the Auser aquifier, it's clear how there're some 0 values that must be removed.\n![Depth_to_Groundwater_CoS - Auser](https:\/\/storage.googleapis.com\/newagent-ahdsra.appspot.com\/Auser_outlier.png)\nThe situation looks very different for example for the rainfall variables. As stated in the dataset description **Rainfall_X** indicates the quantity of rain falling, expressed in millimeters (mm), in the area X. It could happen that in some isolated cases the quantity of rain fallen is above tha usual one. Therefore we should be careful to consider the best way to treat these data.\n![Depth_to_Groundwater_CoS - Auser](https:\/\/storage.googleapis.com\/newagent-ahdsra.appspot.com\/auser_rainfall.png)\n","baa7bd69":"![](https:\/\/storage.googleapis.com\/newagent-ahdsra.appspot.com\/Immagine%202020-12-16%20160936.png)","ab421242":"**How prevalent is the missing data?**","d5707a2d":"Let's analyze them one by one"}}