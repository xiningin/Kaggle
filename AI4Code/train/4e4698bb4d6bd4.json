{"cell_type":{"95345054":"code","b46fa1ac":"code","eb3b8cc9":"code","7f632f77":"code","68b9b43b":"code","7473e03a":"code","bfb28354":"code","713fcc75":"code","61deee14":"code","4c0babb2":"code","7be4f194":"code","d5dbc905":"code","2e209246":"code","4732bcdc":"code","8634320a":"code","9e6a3f58":"code","9cdfe1df":"code","9329dffc":"code","fff4d26f":"code","2b7a334d":"code","3abc8c70":"code","1eb58330":"code","eea00393":"code","19287320":"code","7bd26e5a":"code","79cc7eb5":"code","b505fca6":"code","4c847ffb":"code","5ee26654":"code","369f120f":"code","0e6919cf":"code","01b7f4ff":"code","f2165133":"code","340f807b":"code","da448066":"code","fa2abf16":"code","ede6fdf2":"code","5c1e574e":"code","f9ea57aa":"code","7e632425":"code","23410f33":"code","bd02df40":"code","1d7ce3c5":"code","4f3d81b5":"code","e3ea586c":"code","a63a2273":"code","e137769b":"code","ffb975ed":"code","ba10e52e":"code","8c8d5e74":"code","010c4c27":"markdown","1aa69bb1":"markdown","2bcff2a2":"markdown","36bdfdaa":"markdown","51722e34":"markdown","6b2a2267":"markdown","bf756cd8":"markdown","567810c3":"markdown","b6a5ff74":"markdown","6d898579":"markdown","6b5b0a68":"markdown","12486986":"markdown","cd63d71d":"markdown"},"source":{"95345054":"# Most features are created by applying min, max, mean, sum and var functions to grouped tables. \n# Little feature selection is done and overfitting might be a problem since many features are related.\n# The following key ideas were used:\n# - Divide or subtract important features to get rates (like annuity and income)\n# - In Bureau Data: create specific features for Active credits and Closed credits\n# - In Previous Applications: create specific features for Approved and Refused applications\n# - Modularity: one function for each table (except bureau_balance and application_test)\n# - One-hot encoding for categorical features\n# All tables are joined with the application DF using the SK_ID_CURR key (except bureau_balance).\n# You can use LightGBM with KFold or Stratified KFold.\n\n# Update 16\/06\/2018:\n# - Added Payment Rate feature\n# - Removed index from features\n# - Use standard KFold CV (not stratified)\n\nimport numpy as np\nimport pandas as pd\nimport gc\nimport time\nfrom contextlib import contextmanager\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom warnings import filterwarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b46fa1ac":"cd ..\/input\/home-credit-default-risk\/","eb3b8cc9":"# Training data\napp_train = pd.read_csv('application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","7f632f77":"def new_features(df):\n    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\n    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] \/ df['AMT_CREDIT']\n    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] \/ df['CNT_FAM_MEMBERS']\n    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] \/ df['AMT_INCOME_TOTAL']\n    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] \/ df['AMT_CREDIT']","68b9b43b":"# Testing data features\napp_test = pd.read_csv('application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","7473e03a":"new_features(app_train)\nnew_features(app_test)","bfb28354":"app_train.head()","713fcc75":"print(app_train['TARGET'].value_counts())\nsns.countplot(app_train['TARGET'])","61deee14":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","4c0babb2":"# Missing value statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(10)","7be4f194":"# Number of each type of column\napp_train.dtypes.value_counts()","d5dbc905":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","2e209246":"from sklearn.preprocessing import LabelEncoder\n# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","4732bcdc":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","8634320a":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","9e6a3f58":"(app_train['DAYS_BIRTH'] \/ -365).describe() #Burada ka\u00e7 y\u0131ld\u0131r ya\u015fad\u0131\u011f\u0131 belirtiliyor.    (-)'ye b\u00f6lme nedeni ","9cdfe1df":"app_train['DAYS_EMPLOYED'].describe()          #  Burada 1000 y\u0131ld\u0131r \u00e7al\u0131\u015fan birileri g\u00f6z\u00fck\u00fcyor. (Ayk\u0131r\u0131 de\u011fer)","9329dffc":"app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram', histtype= 'stepfilled', bins= 30);\nplt.xlabel('Days Employment');              # Histogramda g\u00f6relim \n#histtype{'bar', 'barstacked', 'step', 'stepfilled'}","fff4d26f":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d abnormal days of employment' % len(anom))","2b7a334d":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napp_train['DAYS_EMPLOYED'] = abs(app_train['DAYS_EMPLOYED'])\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n\n\n\nplt.xlabel('Days Employment');\n\nprint(app_train['DAYS_EMPLOYED'].nunique())","3abc8c70":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","1eb58330":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","eea00393":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","19287320":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nsns.distplot(app_train['DAYS_BIRTH'] \/ 365, bins = 100, hist= True, kde=True)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","7bd26e5a":"plt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] ==0, 'DAYS_BIRTH'] \/ 365 , label = \"target\" == 0)\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] \/ 365, label = 'target' == 1)\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages'); \n\n#Red  => target 0\n#Blue => target 1","79cc7eb5":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] \/ 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","b505fca6":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","4c847ffb":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.pie(100 * age_groups['TARGET'], labels = age_groups.index.astype(str),radius = 1 )\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","5ee26654":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","369f120f":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","0e6919cf":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)\n    ","01b7f4ff":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                height = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, height = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot)# ,cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","f2165133":"# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","340f807b":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","da448066":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","fa2abf16":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","ede6fdf2":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)\n","5c1e574e":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] \/ app_train_domain['DAYS_BIRTH']","f9ea57aa":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] \/ app_test_domain['DAYS_BIRTH']","7e632425":"plt.figure(figsize = (12, 20))\n# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source\n    plt.subplot(4, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","23410f33":"from sklearn.preprocessing import MinMaxScaler \nfrom sklearn.impute import SimpleImputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = SimpleImputer()\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","bd02df40":"model = LGBMClassifier(objective='binary', n_estimators = 75)\nmodel.fit(train, train_labels)\ny_pred = model.predict_proba(test)\n# Submission dataframe\nsubmit = pd.read_csv(\"sample_submission.csv\")\nsubmit['TARGET'] = y_pred[0:, 1]\n\nsubmit.sort_values(by=\"SK_ID_CURR\")\nsubmit.to_csv('\/kaggle\/working\/lgbm75est.csv', index = False)","1d7ce3c5":"params = {\"learning_rate\" : [0.06, 0.075, 0.095,  0.1, 0.15],\n          \"n_estimators\" : [65, 70, 75, 80]}\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(estimator=model, param_grid = params, scoring='accuracy', n_jobs=-1, verbose=2, cv=cv)\ngrid_search.fit(train,train_labels)\npredictions = grid_search.predict_proba(test)[:, 1]\nsubmit = pd.read_csv(\"sample_submission.csv\")\nsubmit['TARGET'] = predictions\n\nsubmit.sort_values(by=\"SK_ID_CURR\")\nsubmit.to_csv('\/kaggle\/working\/newbien.csv', index = False)","4f3d81b5":"params = {\"learning_rate\" : [0.1]}\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(estimator=model, param_grid = params, scoring='accuracy', n_jobs=-1, verbose=2, cv=cv)\ngrid_search.fit(train,train_labels)\npredictions = grid_search.predict_proba(test)[:, 1]\nsubmit = pd.read_csv(\"sample_submission.csv\")\nsubmit['TARGET'] = predictions\n\nsubmit.sort_values(by=\"SK_ID_CURR\")\nsubmit.to_csv('\/kaggle\/working\/newbien.csv', index = False)","e3ea586c":"best_of_grid = grid_search.best_params_\nbest_of_grid","a63a2273":"lgbm = LGBMClassifier(learning_rate= 0.1,\n                      n_estimators= 75,\n                      verbose =1)\nlgbm.fit(train,train_labels)\n\npredictions = lgbm.predict_proba(test)[:, 1]\n\n# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('\/kaggle\/working\/newbien.csv', index = False)","e137769b":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","ffb975ed":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]\n\n# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('\/kaggle\/working\/random_forest_baseline.csv', index = False)","ba10e52e":"feature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})","8c8d5e74":"plot_feature_importances(feature_importances)","010c4c27":"Bizim target de\u011ferimizi en fazla etkileyen \u00f6zelliklerden polynomial feature adl\u0131 i\u015flemden en yararl\u0131 feature'lar\u0131 \u00e7\u0131kar\u0131yoruz. degree k\u0131sm\u0131n\u0131n artmas\u0131 bizim computation'\u0131 artt\u0131r\u0131r. \u00c7\u00fcnk\u00fc polynomialfeature i\u015flemi ger\u00e7ekle\u015firken exponential olarak artar katlana katlana.","1aa69bb1":"### Continue for EDA","2bcff2a2":"Verilerimizi \u00f6nce imputer ile doldurduk ard\u0131ndan scale ile say\u0131sal de\u011ferlere \u00e7evirdik.","36bdfdaa":"EXT_Sourcelar target ile ters orant\u0131l\u0131 olsa da days_birth ile do\u011fru orant\u0131l\u0131. Buradan bir bakal\u0131m EXT ile Target aras\u0131ndaki ili\u015fkiye","51722e34":"Ya\u015f ge\u00e7tik\u00e7e \u00f6dememe oran\u0131 azal\u0131yor. Yani gen\u00e7ler kredi bor\u00e7lar\u0131n\u0131 \u00f6demiyor.","6b2a2267":"Burada bir encoding i\u015flemi yapmam\u0131z gerekiyor. Fakat \u015f\u00f6yle bir mesele var. Encoding i\u015fleminde 2 tane s\u0131n\u0131f varsa Label Encoding, e\u011fer 2den fazla s\u0131n\u0131f\u0131m\u0131z varsa One-Hot encoding uygulamam\u0131z gerekiyor. Bunun nedeni Label encodingin \u00e7al\u0131\u015fma mant\u0131\u011f\u0131nda s\u0131ral\u0131 yani a\u011f\u0131rl\u0131klarla ili\u015fkilendiriyor. Bunu tabikide biz istemiyoruz.","bf756cd8":"Bu \u00f6zelliklerin Modeli e\u011fitirken i\u015fimize yaray\u0131p(iyi y\u00f6nde etkileyece\u011fini) yaramad\u0131\u011f\u0131n\u0131 tam olarak bilmiyoruz. Bunu \u00f6\u011frenmenin tek yolu denemektir. ","567810c3":"### Feature Engineering","b6a5ff74":"One hot encoding uygulad\u0131\u011f\u0131m\u0131z zaman test veri setinde olmayan e\u011fitim verisetinde olan kolonlar i\u00e7in hizalama(align) i\u015flemi yapmam\u0131z laz\u0131m. Bunun sebebi one hot encoding.","6d898579":"### Column Type","6b5b0a68":"get_feature_names methodu kullanarak inputlardan olu\u015fan farkl\u0131 farkl\u0131 \u00f6zellikler olu\u015ftu.","12486986":"## Exploratory Data Analysis","cd63d71d":"## Missing Values"}}