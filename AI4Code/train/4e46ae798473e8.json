{"cell_type":{"6434bbd1":"code","8f95b7f9":"markdown","7eb1d531":"markdown","ec186c69":"markdown"},"source":{"6434bbd1":"from scipy import spatial\n\nvector_1 = [3, 45, 7, 2]\nvector_2 = [2, 54, 13, 15]\n\ncosine_simlarity = 1 - spatial.distance.cosine(vector_1, vector_2)\nprint(cosine_simlarity)\n# 0.97228425171235\n\n# *********************************************************\n\n# Alternate-2 with numpy\n# This is probably the fastest approach\n\nfrom numpy import dot\nfrom numpy.linalg import norm\n\ncosine_similarity_2 = dot(vector_1, vector_2) \/ (norm(vector_1) * norm(vector_2))\nprint(cosine_similarity_2)\n# 0.9722842517123499\n\n# *********************************************************\n\n# Alternate-3 with scikit-learn\n# You can use cosine_similarity function form sklearn.metrics.pairwise docs\n# http:\/\/scikit-learn.org\/stable\/modules\/metrics.html\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsimilarity = cosine_similarity([[1, 0, -1]], [[-1,-1, 0]])\nprint(similarity)\n# array([[-0.5]])","8f95b7f9":"---\n\n## What is a Hyperplane?\n\nIn mathematics, a hyperplane H is a linear subspace of a vector space V such that the basis of H has cardinality one less than the cardinality of the basis for V.  In other words, if V is an n-dimensional vector space than H is an (n-1)-dimensional subspace.  Examples of hyperplanes in 2 dimensions are any straight line through the origin. In 3 dimensions, any plane containing the origin.  In higher dimensions, it is useful to think of a hyperplane as member of an affine family of (n-1)-dimensional subspaces (affine spaces look and behavior very similar to linear spaces but they are not required to contain the origin), such that the entire space is partitioned into these affine subspaces. This family will be stacked along the unique vector (up to sign) that is perpendicular to the original hyperplane.\n\n#### Mathematical Representation of Random Hyperplane Hash (RHH) algorithm  from [this](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-540-87481-2_17) paper\n\n![img](https:\/\/i.imgur.com\/Rn61p5q.png)\n\nSome more mathematical explanation of hyperplane-vs-plane from [math.stackexchange](https:\/\/math.stackexchange.com\/a\/1905956\/517433)\n\nIn general, a hyperplane in $\\Bbb R^n$ is an $(n-1)$-dimensional subspace of $\\Bbb R^n$. So, in the case of $\\mathbb R^4$, you may think of a hyperplane as a rotated version of our three-dimensional space $\\mathbb R^3$. In $\\Bbb R^3$, a hyperplane is a two-dimensional plane, and in $\\Bbb R^2$, a hyperplane is a one-dimensional line.\n\nOn the difference between a plane and a hyperplane, a plane and a hyperplane are the same thing in $\\Bbb R^3$. We use the term *hyperplane* to speak of $(\\dim V - 1)$-dimensional subspaces of a $(\\dim V)$-dimensional vector space $V$. In the case $n = 3$, geometric planes may be thought of as the classic span of $2 = 3-1$ vectors in $\\Bbb R^3$. (The number $1$ is often referred to as the \"codimension\" of the plane.)\n\nIn $\\Bbb R^n$, an example hyperplane is defined by the equation\n$$\na_1x_1 + \\dotsb + a_nx_n = 0,\n$$\nwhere $(a_1,\\dots,a_n)$ is not the zero vector. More explicitly, the hyperplane in the discussion is the kernel of the linear map $\\Bbb R^n\\to\\Bbb R$ defined by\n$$\n(x_1,\\dots,x_n)\\mapsto a_1x_1 + \\dotsb + a_nx_n.\n$$\nSince $(a_1,\\dots,a_n)$ is not the zero vector, the image of this linear map is a $1$-dimensional subspace of $\\Bbb R$ (i.e. *is equal to* $\\mathbb R$), so the rank-nullity theorem tells us that the kernel of this linear map is an $(n-1)$-dimensional subspace of $\\Bbb R^n$, i.e., a hyperplane in $\\mathbb R^n$. In the case $n = 3$, this is usually how we define a plane orthogonal to the vector $(a_1,a_2,a_3)$, as the set of vectors $(x_1,x_2,x_3)$ satisfying\n$$\na_1x_1 + a_2x_2 + a_3x_3 = 0.\n$$\nIn the sense that geometric planes and kernels of nonzero linear functions $\\Bbb R^3\\to\\Bbb R$ coincide, hyperplanes in $n$-space are the appropriate generalization of geometric planes in $3$-space to arbitrary dimensions.\n\n---\n\n## What is a normal?\n\nA surface normal from a surface at P, is a vector perpendicular to the tangent plane to that surface at P. If you know the tangent T and bi-tangent B of the surface at P (which defines the plane tangent to the surface at P) then we can compute the surface normal at P using a simple cross product between T and B:\n\n#### N=T\u00d7B\n\n![img](https:\/\/i.imgur.com\/RTFoNhh.png)\n\n#### The tangent (T) and bi-tangent (B) are lying in the plane tangent at P. Taking the cross product between T and B gives the surface normal N. Note that T, B and N are orthogonal to each other and form a Cartesian coordinate system.\n\n## Calculus definition of Unit normal vector of a surface\n\n![img](https:\/\/i.imgur.com\/kmHk7XX.png)\n\n[Source](https:\/\/www.khanacademy.org\/math\/multivariable-calculus\/integrating-multivariable-functions\/flux-in-3d-articles\/a\/unit-normal-vector-of-a-surface)\n\n\n\n## Equation connecting Plane to its Normal or Equation of a Plane being explained by the Normal to the Plane\n\n![img](https:\/\/i.imgur.com\/Dwwt25I.png)\n\nNow we will go through an example to understand the Equation above. From high school geometry, you may recall that a plane is determined by three (noncollinear) points.\n\nLet\u2019s find an equation of the plane that contains the points P 0 (1, 2, 0), P 1 (3, 1, 2), and P 2(0, 1, 1).\n\nThere are two ways to solve this problem. The first approach is algebraic and rather uninspired. From the aforementioned remarks, any plane must have an equation of the form Ax + By + C z = D for suitable constants A, B, C, and D. Thus, we need only to substitute the coordinates of P 0, P 1 , and P 2 into this equation and solve for A, B, C, and D. We have that\n\n\u2022 substitution of P 0 gives A + 2B = D;\n\u2022 substitution of P 1 gives 3A + B + 2C = D; and\n\u2022 substitution of P 2 gives B + C = D.\n\nHence, we must solve a system of three equations in four unknowns:\n\n![img](https:\/\/i.imgur.com\/ICsIOzq.png)\n\nAfter solving we will get\n\n### `x \u2212 4y \u2212 3z = \u22127.`\n\n### And now the 2-nd way to solve..\n\n![img](https:\/\/i.imgur.com\/H731pzJ.png)\n\nIf we take P0 (1, 2, 0) to be the particular point in equation (1), we find that the equation we desire is\n\n### (i \u2212 4j \u2212 3k) \u00b7 ((x \u2212 1)i + (y \u2212 2)j + zk) = 0\n\nor\n\n### (x \u2212 1) \u2212 4(y \u2212 2) \u2212 3z = 0.\n\nWhich is the same equation as the one given by the first method.\n\n---\n\n## Basic Idea of Location-Sensitive-Hashing\n\nLSH refers to a family of functions (known as LSH families) to hash data points into buckets so that data points near each other are located in the same buckets with high probability, while data points far from each other are likely to be in different buckets. This makes it easier to identify observations with various degrees of similarity.\n\nHashing data points can be done in many different ways and depend on the given distance metric. Considering the angular distance as metric, the vector space is divided by randomly\ngenerated hyperplanes. These planes cut the space into subspaces, where each subspace represents a bucket. Now, any point within that subspace is distributed to the corresponding\nbucket (or leaf).\n\nLocality Sensitive Hashing (LSH) is a computationally efficient approach for finding nearest neighbors in large datasets. The main idea in LSH is to avoid having to compare every pair of data samples in a large dataset in order to find the nearest similar neighbors for the different data samples. With LSH, one can expect a data sample and its closest similar neighbors to be hashed into the same bucket with a high probability. By treating the data samples placed in the same bucket as candidates for similarity checking, we significantly reduce the computational burden associated with similarity detection in large datasets.\n\n\n#### Mathematical Representation of Locality Sensitive Hashing from [this](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-540-87481-2_17) paper\n\n![img](https:\/\/i.imgur.com\/UVi0FHe.png)\n\nSimply put, a locality sensitive hash function is designed in such a way that if\ntwo vectors are close in the intended distance measure, the probability that they hash to the same value is high; if they are far in the intended distance measure, the probability that they hash to the same value is low.\n\n---\n\n## Actual Implementation steps for LSH\n\n![img](https:\/\/i.imgur.com\/9aY9y6p.png)\n[Source](https:\/\/www.appliedaicourse.com\/)\n\nThe above picture, summarizes the generation of Hyperplanes, note the points below about this hyperplane generation math.\n\n- A. How to generate the random plane `\u03c0` ? As this `\u03c0` hyperplane is passing through origin its equation is `W^T * x = 0`  And if Equation of a plane is `W^T * x = 0` that means **W is nothing but the normal to the plane** by definition of Normal of a Vector.\n\n- B. Random hyperplane is generated using `w^Tx = 0` where w is \"normal\" to the plane passing through origin, since w is normal to the plane we have taken values from N(0,1) in order to initialize its value.\n\nAnd now the next steps\n\n1. First make **`m`** hyperplanes to split into regions and create slices such that cluster of points lie in a particular slice and be called their neighbourhood.typically `m = int(log(number_of_data_points))`\n\n   #### We consider those hyper-planes vectors, that slice out the regions, as unit vectors. As only the direction of the vector is of importance here.\n\n2. Next for each point create a vector by `W1(transpose).point`. if it is greater than 0 , it lies on the same side of that hyperplane else other side. Based on that create a vector of m size. For eg the vector can be [1,-1,-1] denoting point x lies on same side of normal to hyperplane 1, opposite side to normal of hyperplane 2 and 3. Now this vector serves as key to the hash table and all the points with the same key or vector representation will go in the same bucket as they have similar vector representation denoting they lie in the neighbourhood of each other.\n\n    ##### Random hyperplane is generated using w^Tx = 0 where w is \"normal\" to the plane passing through origin, since w is normal to the plane we have taken values from N(0,1) in order to initialize its value.\n\n    Typically many will choose `int(log(number_of_data_points))` hyperplane, but you can opt for any number of hyperplanes to get better results.\n\n3. Now it may happen that two points which are very close fall on different slice due to placing of hyperplane and hence not considered as nearest neighbour. To resolve this problem, create **`l`** hash tables (l is typically small). In other words repeat step 2 above **`l`** times thus creating **`l`** hash tables and **`m`** random hyper-planes **`l`** times. So when a query point comes compute the hash function each of the **`l`** times and get its neighbours from each of bucket.\n\n4. Union them and find the nearest neighbours from the list. So basically in each **`l`** iterations create **`m`** hyperplanes and hence region splitting will be different thus vector representation or hash function of the same query point will be different in each of the representations. Thus the hash table will be different as points which lied on the same region in previous iteration might lie in a different region in this iteration and vice versa due to different placement of hyperplanes.\n\n    #### Time complexity is `O(m*d*l)` for each query point. And for creating the hash table `O(m*d*l*n)` which is one time only\n\n    where m is the number of planes. And usually, m will be `int(log(number_of_data_points))`, so time complexity is `O(log n *d)`.\n\n5. Dont think that LSH is trying to break the space in regions such that each of the neighbors is in the same region\/cell like in KD-Tree. Unlike KD-Tree, in LSH, we are not trying to break the space into regions. Each hyperplane is independent of others as each of them is chosen randomly. Each hyperplane separates points into two bins: those on each side of the hyperplane. Now as the number of hyperplanes increases, the chance that two points that are close (angularly close as we are using cosine similarity) have the same hash value, h(x), increases as there would be more planes that place the close-by points in the same side.\n\nSo explaining the whole above steps together, LSH is technique used to create a hash table for identifying the nearest neighbors faster as compared to the other methods. In the other methods, we consider all the points and then carefully shortlist the points which leads to high time complexity. Since KNN is a lazy algorithm, this method retains the complete dataset and builds a model over it. This in turn creates the space complexity.\n\nTo avoid the above and obtain the nearest neighbors we developed the LSH, where we develop a hashing table basing on the locality ( Nearest neighbors) of the points ( not the query point). This hash table helps us in reducing n different points to a set of points(union) basing on hyperplanes.\n\nThese hyper planes are planes which slice through the points and identify the nature of the points i.e., basing on for all i = 1 to m (# of hyper planes)\n\nWiXj > 0 is considered as +1 and WiXj < 0 is considered as -1. Basing on this we create a hash table by finding the union of every point with respect to every plane that slices these points We are using Unions here because, if we use intersections then there are chances of loosing some NN.\n\nThis hash table is stored as the trained data for the new query point that is given as the input.\nWhen the new query point is given then we identify the same WiXq for all i =1 to m and identify the nature of the point. Using the output of WXq key we search through the hash table we created with the data and identify the nearest neighbors of the Xq.\n\nAccuracy of the model increases with more # of hyperplanes.\n\n---\n\n## Time Complexity Improvement\n\nGiven **`m`** is the number of hyperplanes, and usually, **`m`** will be `int(log(number_of_data_points))`, so time complexity is `O(log n *d)`.\n\nIf you use the traditional method, it will be `O(n*d)`. There is a significant difference in both these approaches.\n\n---\n\n## How increasing m increases the accuracy ?\n\nThe hyperplane generation does not take any information about the dataset into account. Especially when the dataset contains dense clusters of points, it can happen that a generated\nhyperplane separates those points into different subspaces and does not consider them as near neighbor to each other. Hence, the information. And this problem is largely solved by creating many hyperplanes.\n\nAs we have more planes, angular distance gets closer to actual distance. In that case if points are pretty much close then only they both get same value else a plane out of pool of planes passes between two points(if distance is reasonable) and assign them two different values. so as m increases points condition to get same value for near points get stricter and stricter so only points which are very close get same value. so accuracy increases\n\n---\n\n## How exactly the union of Hashed-Bucket work\n\nWe take all the buckets in which the query point lies and take all the points in those buckets. This is what we are calling union. After taking those points we choose k nearest neighbours.\n\nLet us consider we have 3 sets of hash tables h1, h2, h3;\n\n1. h1(x_q) ===> {x4, x7, x9}\n2. h2(x_q) ===> {x7, x9}\n3. h3(x_q) ===> {x5, x67, x8, x9}\n\nNow we consider uniun of all the 3 sets above the points. And calculate the Nearest neighbours.\n\n---\n\n## More explanatory points on the above steps described\n\n1. we are not computing the distance here but the sign(W^TX) right.Now since -1 <= cos(theta) <=1 so if w^TX >=0 we have just placed +1 if it is negative then it means points are on the other side right <=0 so we have placed the sign -1.\n\n2) here W^TX gives the projection of X on normal to the plane(W^T). If it comes out to be positive then X got projected toward positive side(in direction of normal to the plane) and if negative then in direction opposite to the normal of the plane.\n\n---\n\n### References\n\n- [MinHash Tutorial with Python Code](http:\/\/mccormickml.com\/2015\/06\/12\/minhash-tutorial-with-python-code\/)\n- [Building a Recommendation Engine with Locality-Sensitive Hashing (LSH) in Python](https:\/\/www.learndatasci.com\/tutorials\/building-recommendation-engine-locality-sensitive-hashing-lsh-python\/)\n\n","7eb1d531":"Locality-sensitive hashing (LSH) is an important tool for managing high-dimensional noisy or uncertain data, for example in connection with data cleaning (similarity join) and noise-robust search\n(similarity search).\n\n- LSH is a procedure that takes as input a set of documents\/images\/objects and outputs a kind of Hash Table.\n- The indexes of this table contain the documents such that documents that are on the same index are considered **similar** and those on different indexes are \"**dissimilar**\".\n- Where **similar** depends on the metric system and also on a threshold similarity **s** which acts like a global parameter of LSH.\n- It is up to you to define what the adequate threshold **s** is for your problem.\n\n[![enter image description here][1]][1]\n\nIt is important to underline that different similarity measures have different implementations of LSH.\n\n\n  [1]: https:\/\/i.stack.imgur.com\/GOBbB.png\n\n## Use case of LSH\n\nA classical application of similarity search is in recommender systems: Suppose you have shown interest in a particular item, for example a news article x. The semantic meaning of a piece of text can be represented as a high-dimensional feature vector, for example computed using latent semantic indexing. In order to recommend other news articles we might search the set P of article feature vectors for articles that are \u201cclose\u201d to x.\n\nIn this case, for a large textual dataset containing millions of words, the problem is there may be far too many pairs of items to calculate the similarity of each pair. And also, we will have sparse amounts of overlapping data for all items.\nSo in this case, LSH can be used for compressing the rows into \u201csignatures\u201d, or sequences of integers, which will let us compare published-papers or news-articles without having to compare the entire sets of words. Reducing the computational intensity of the model.\n\nFirst I shall go over the building block concepts of Location Sensitive Hashing which are cosine similarity, Normal of a Vector and the concept of Hyperplanes\n\nAnd note, Locality Sensitive Hashing (LSH) is actually a family of algorithm, different distance metric will correspond to a different method. Here we will be focusing on cosine distance.\nHowever, many LSH is calculated based on many other distances including, most famously the Euclidean distance between two vectors or the Hamming Distance\n\n## What is cosine distance and cosine similarity\n\nLet's consider the following dataset:\n\n![img](https:\/\/i.imgur.com\/lXNQQmJ.png)\n\n![img](https:\/\/i.imgur.com\/8nBSrtc.png)\n\n### Some other different form of expressing Cosine Similarity mathematically\n\n![img](https:\/\/i.imgur.com\/nqJrNje.png)\n\ncosine similarity between two points p and q is their dot product divided by their L2-norms:\n\n![img](https:\/\/i.imgur.com\/OBQwzjw.png)\n\n![img](https:\/\/i.imgur.com\/yqBQxQe.png)\n\n![img](https:\/\/i.imgur.com\/Qo8HuuY.png)\n\n![img](https:\/\/i.imgur.com\/QyYa36E.png)\n\nThe Cosine Distance (also called angular distance) is one of the most popular distances for LSH and is used as main metric in this thesis. It is defined for spaces with a dimension, such that points can be considered as vectors as shown in Figure 3. Rather than to distinguish vectors by their length, the Cosine Distance is defined as an angle between two vectors. On any dimension, the angle between two vectors always ranges from 0\u00b0 to 180\u00b0, which is then used to define the distance. Unlike the Euclidean distance, the cosine distance is scale-insensitive.\n\nThe cosine distance, is useful when we need a distance proportional to the angle between two vectors. If the direction is the same, the distance is null, while it is maximum when the angle is equal to 180\u00b0 (meaning opposite directions). This distance can be employed when the clustering must not consider the L2 norm of each point. For example, a dataset could contain bi-dimensional points with different scales and we need to group them into clusters corresponding to circular sectors. Alternatively, we could be interested in their position according to the four quadrants because we have assigned a specific meaning (invariant to the distance between a point and the origin) to each of them.\n\nThe beauty of this distance is its sheer speed at calculating distances between sparse vectors. For instance if we had 1,000 attributes collected about houses and 300 of these were mutually exclusive (meaning that one house had them but the others don\u2019t), then we would only need to include 700 dimensions in the calculation. Visually this measures the inner product space between two vectors and presents us with cosine as a measure.\n\n### The cosine distance is 1 minus the cosine similarity of two vectors:\n\n## Quick Calculation of Cosine Similarity with Scikit-learn","ec186c69":"### Also, just for fun and to see the actual calculation, implementation of Cosine Similarity from scratch with plain-vanilla Python\n\n"}}