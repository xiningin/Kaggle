{"cell_type":{"da5b495d":"code","c4cd7f03":"code","b58cf33b":"code","6e67c267":"code","8ed83641":"code","d1d33696":"code","a44c3888":"code","7fb1f1b2":"code","125d24c6":"code","9538cf8e":"code","4594633c":"code","12531caa":"code","33bdd064":"code","8159f5d3":"code","7ef6ede0":"code","6f043045":"code","c88e8b0f":"code","e9a2b59a":"code","5e2e1137":"code","0ac45250":"code","c21f9507":"code","f6e8bc16":"code","6b5d1854":"code","b7afa214":"code","f2ebd712":"code","1d8da891":"code","cb845bee":"code","ee7a9215":"code","a9b42b7e":"code","9e30747b":"code","8a771710":"code","b4817d4e":"code","fbc112d0":"code","c8062ce6":"code","d4bb4ef4":"code","011b555e":"code","0ca2fda9":"code","fead1973":"markdown","337c6ca9":"markdown","28e50793":"markdown","48b2e765":"markdown","e2e9f4a1":"markdown","d100c89d":"markdown","e1363a4e":"markdown","ec6ae274":"markdown","be295305":"markdown","2ccdd63b":"markdown","736ff68a":"markdown","5a996643":"markdown","d4ac6a24":"markdown","582dfae2":"markdown","465d750b":"markdown","94878c25":"markdown","b615446d":"markdown","00514959":"markdown","6a35622b":"markdown","6a3f7572":"markdown","f0d03214":"markdown","ea22bcce":"markdown","00a843d9":"markdown","d138f320":"markdown","e03ea3fe":"markdown","17ef5db3":"markdown","9e50b9e1":"markdown","b087c225":"markdown","f58fa7f8":"markdown","643560dc":"markdown","32242ab3":"markdown","b173182b":"markdown","8e905688":"markdown","ade6750e":"markdown","e3893779":"markdown","fe5c7a9e":"markdown","de709a19":"markdown","2f28fbe4":"markdown","be0b23ab":"markdown","266c7597":"markdown","513579e9":"markdown","84527081":"markdown","418b3dc6":"markdown","baec9ecc":"markdown","377d6c7e":"markdown"},"source":{"da5b495d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize as wt \nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport gensim\nfrom gensim.models import Word2Vec\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c4cd7f03":"dataset=pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\ndataset=dataset[['target','excerpt']]\ndataset.head()","b58cf33b":"data = []\n\nfor i in range(dataset.shape[0]):\n\n    sms = dataset.iloc[i, 1]\n\n    # remove non alphabatic characters\n    sms = re.sub('[^A-Za-z]', ' ', sms)\n\n    # make words lowercase, because Go and go will be considered as two words\n    sms = sms.lower()\n\n    # tokenising\n    tokenized_sms = wt(sms)\n\n    # remove stop words and stemming\n \n    sms_processed = []\n    for word in tokenized_sms:\n        if word not in set(stopwords.words('english')):\n            sms_processed.append(stemmer.stem(word))\n\n    sms_text = \" \".join(sms_processed)\n    data.append(sms_text)","6e67c267":"# creating the feature matrix \nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(max_features=1000) #Bag of words\nX = vectorizer.fit_transform(data).toarray()\ny = dataset.iloc[:, 0]\n\n# split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","8ed83641":"regr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = regr.predict(X_test)\n\n# RMSE volue on test set\nscores = cross_val_score(regr, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","d1d33696":"coef_table = pd.DataFrame(list(vectorizer.get_feature_names())).copy()\ncoef_table.insert(len(coef_table.columns),\"Coefs\",regr.coef_.transpose())\ncoef_table.nlargest(25,'Coefs')","a44c3888":"print(f' RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}')\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_pred, ax=ax, label='Prediction')\nax.legend()\nplt.show()","7fb1f1b2":"# creating the feature matrix \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=1000) #TFIDF\n\nX = vectorizer.fit_transform(data).toarray()\ny = dataset.iloc[:, 0]\n\n# split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","125d24c6":"regr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = regr.predict(X_test)\n\n# RMSE volue on test set\nscores = cross_val_score(regr, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","9538cf8e":"print(f' RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}')\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_pred, ax=ax, label='Prediction')\nax.legend()\nplt.show()","4594633c":"word2vecModel = gensim.models.KeyedVectors.load_word2vec_format(\"\/kaggle\/input\/google-pretrain-model\/GoogleNews-vectors-negative300.bin.gz\", binary=True)","12531caa":"def avg_feature_vector(sentence, model, num_features):\n    words = sentence.replace('\\n',\" \").replace(',',' ').replace('.',\" \").split()\n    feature_vec = np.zeros((num_features,),dtype=\"float32\")\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, model[word])\n        except KeyError as error:\n            feature_vec \n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec","33bdd064":"word2vec_train = np.zeros((len(dataset.index),300),dtype=\"float32\")\n\nfor i in range(len(dataset.index)):\n    word2vec_train[i] = avg_feature_vector(dataset[\"excerpt\"][i],word2vecModel, 300)","8159f5d3":"# creating the feature matrix \n\nX = word2vec_train\ny = dataset.iloc[:, 0]\n\n# split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","7ef6ede0":"regr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = regr.predict(X_test)\n\n# RMSE volue on test set\nscores = cross_val_score(regr, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","6f043045":"print(f' RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}')\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_pred, ax=ax, label='Prediction')\nax.legend()\nplt.show()","c88e8b0f":"from sklearn.tree import DecisionTreeRegressor \n  \n# create a regressor object\nTree = DecisionTreeRegressor(random_state = 42) \n  \n# Train the model using the training sets\nTree.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_predT = Tree.predict(X_test)\n\nscores = cross_val_score(Tree, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","e9a2b59a":"print(f' RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}')\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_predT, ax=ax, label='Prediction')\nax.legend()\nplt.show()","5e2e1137":"from sklearn.ensemble import RandomForestRegressor \n  \n# create a regressor object\nForest = RandomForestRegressor(random_state = 42) \n  \n# Train the model using the training sets\nForest.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_predF = Forest.predict(X_test)\n\nscores = cross_val_score(Forest, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","0ac45250":"print(f' RMSE: {mean_squared_error(y_test, y_predF, squared=False):.4f}')\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_predF, ax=ax, label='Prediction')\nax.legend()\nplt.show()","c21f9507":"from sklearn.ensemble import GradientBoostingRegressor \n  \n# create a regressor object\nBoost = GradientBoostingRegressor(random_state = 42) \n  \n# Train the model using the training sets\nBoost.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_predB = Forest.predict(X_test)\n\nscores = cross_val_score(Boost, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","f6e8bc16":"#Error Distribution\nprint(f' RMSE: {mean_squared_error(y_test, y_predB, squared=False):.4f}')\n\nsns.scatterplot(\n    x=y_test, y= y_predB,\n    palette=sns.color_palette(\"hls\", 10),\n    legend=\"full\")","6b5d1854":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_predB, ax=ax, label='Prediction')\nax.legend()\nplt.show()","b7afa214":"from sklearn.svm import SVR\n  \n# create a regressor object\nSupport = SVR(kernel = 'rbf') \n  \n# Train the model using the training sets\nSupport.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_predS = Support.predict(X_test)\n\nscores = cross_val_score(Support, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","f2ebd712":"print(f' RMSE: {mean_squared_error(y_test, y_predS, squared=False):.4f}')\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_predS, ax=ax, label='Prediction')\nax.legend()\nplt.show()","1d8da891":"from sklearn.model_selection import GridSearchCV\n  \n#defining parameter range\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']} \n  \ngrid = GridSearchCV(SVR(), param_grid, refit = True, verbose = 0)\n  \n#fitting the model for grid search\ngrid.fit(X, y)","cb845bee":"#print best parameter after tuning\nprint(grid.best_params_)\n","ee7a9215":"#KFold \u3000n_splits=5\nfrom sklearn.model_selection import KFold\ny_train_num=dataset.target.to_numpy()\n\nfold = KFold(n_splits=5, shuffle=True, random_state=42)\ncv=list(fold.split(word2vec_train, y_train_num))","a9b42b7e":"rmses = []\nfor tr_idx, val_idx in cv: \n    x_tr, x_va = word2vec_train[tr_idx], word2vec_train[val_idx]\n    y_tr, y_va = y_train_num[tr_idx], y_train_num[val_idx]\n        \n    # Training\n    model = SVR(kernel = 'rbf',gamma=1,C=1) \n    model.fit(x_tr, y_tr)    \n    y_pred = model.predict(x_va)\n    rmse =  np.sqrt(mean_squared_error(y_va, y_pred))\n    rmses.append(rmse)\n    \n    \n    fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n    sns.distplot(y_va, ax=ax, label='Label')\n    sns.distplot(y_pred, ax=ax, label='Prediction')\n    ax.legend()\n    plt.show()\n        \nprint(\"\\n\", \"Mean Fold RMSE:\", np.mean(rmses))    ","9e30747b":"import tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import optimizers, losses, metrics, Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom transformers import TFAutoModelForSequenceClassification, TFAutoModel, AutoTokenizer\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom sklearn.metrics import mean_squared_error\n\n# Utility functions\ndef custom_standardization(text):\n    text = text.lower() # if encoder is uncased\n    text = text.strip()\n    return text\n\n\ndef sample_target(features, target):\n    mean, stddev = target\n    sampled_target = tf.random.normal([], mean=tf.cast(mean, dtype=tf.float32), \n                                      stddev=tf.cast(stddev, dtype=tf.float32), dtype=tf.float32)\n    \n    return (features, sampled_target)\n    \n\ndef get_dataset(pandas_df, tokenizer, labeled=True, ordered=False, repeated=False, \n                is_sampled=False, batch_size=32, seq_len=128):\n    \"\"\"\n        Return a Tensorflow dataset ready for training or inference.\n    \"\"\"\n    text = [custom_standardization(text) for text in pandas_df['excerpt']]\n    \n    # Tokenize inputs\n    tokenized_inputs = tokenizer(text, max_length=seq_len, truncation=True, \n                                 padding='max_length', return_tensors='tf')\n    \n    if labeled:\n        dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']}, \n                                                      (pandas_df['target'], pandas_df['standard_error'])))\n        if is_sampled:\n            dataset = dataset.map(sample_target, num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']})\n        \n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return dataset\n\n\ndef plot_metrics(history):\n    metric_list = list(history.keys())\n    size = len(metric_list)\/\/2\n    fig, axes = plt.subplots(size, 1, sharex='col', figsize=(20, size * 5))\n    axes = axes.flatten()\n    \n    for index in range(len(metric_list)\/\/2):\n        metric_name = metric_list[index]\n        val_metric_name = metric_list[index+size]\n        axes[index].plot(history[metric_name], label='Train %s' % metric_name)\n        axes[index].plot(history[val_metric_name], label='Validation %s' % metric_name)\n        axes[index].legend(loc='best', fontsize=16)\n        axes[index].set_title(metric_name)\n\n    plt.xlabel('Epochs', fontsize=16)\n    sns.despine()\n    plt.show()","8a771710":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync","b4817d4e":"#Ml parameter\nBATCH_SIZE = 16\nLEARNING_RATE = 1e-5\nEPOCHS = 5\nSEQ_LEN = 256 \n#Cv numbers\nN_FOLDS = 5\nBASE_MODEL = \"\/kaggle\/input\/huggingface-roberta\/roberta-base\" ","fbc112d0":"def model_fn(encoder, seq_len=256):\n    input_ids = L.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n    input_attention_mask = L.Input(shape=(seq_len,), dtype=tf.int32, name='attention_mask')\n    \n    outputs = encoder({'input_ids': input_ids, \n                       'attention_mask': input_attention_mask})\n    \n    model = Model(inputs=[input_ids, input_attention_mask], outputs=outputs)\n\n    optimizer = optimizers.Adam(lr=LEARNING_RATE)\n    model.compile(optimizer=optimizer, \n                  loss=losses.MeanSquaredError(), \n                  metrics=[metrics.RootMeanSquaredError()])\n    \n    return model\n\n\nwith strategy.scope():\n    encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n    model = model_fn(encoder, SEQ_LEN)\n    \nmodel.summary()","c8062ce6":"tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\nskf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\noof_pred = []; oof_labels = []; history_list = []; test_pred = []\ntrain=pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\ntrain.drop(['url_legal', 'license'], axis=1, inplace=True)\ntest.drop(['url_legal', 'license'], axis=1, inplace=True)\n\nfor fold,(idxT, idxV) in enumerate(skf.split(train)):\n    if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n    print(f'\\nFOLD: {fold+1}')\n    print(f'TRAIN: {len(idxT)} VALID: {len(idxV)}')\n\n    # Model\n    K.clear_session()\n    with strategy.scope():\n        encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1,hidden_dropout_prob=0.1)\n        model = model_fn(encoder, SEQ_LEN)\n        \n    model_path = f'model_{fold}.h5'\n    es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', \n                       patience=2, restore_best_weights=True, verbose=1)\n    checkpoint = ModelCheckpoint(model_path, monitor='val_root_mean_squared_error', mode='min', \n                                 save_best_only=True, save_weights_only=True)\n\n    # Train\n    history = model.fit(x=get_dataset(train.loc[idxT], tokenizer, repeated=True, is_sampled=True, \n                                      batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                        validation_data=get_dataset(train.loc[idxV], tokenizer, ordered=True, \n                                                    batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                        steps_per_epoch=100, \n                        callbacks=[es, checkpoint], \n                        epochs=EPOCHS,  \n                        verbose=2).history\n      \n    history_list.append(history)\n    # Save last model weights\n    model.load_weights(model_path)\n    \n    # Results\n    print(f\"#### FOLD {fold+1} OOF RMSE = {np.min(history['val_root_mean_squared_error']):.4f}\")\n\n    # OOF predictions\n    valid_ds = get_dataset(train.loc[idxV], tokenizer, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    oof_labels.append([target[0].numpy() for sample, target in iter(valid_ds.unbatch())])\n    x_oof = valid_ds.map(lambda sample, target: sample)\n    oof_pred.append(model.predict(x_oof)['logits'])\n\n    # Test predictions\n    test_ds = get_dataset(test, tokenizer, labeled=False, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    x_test = test_ds.map(lambda sample: sample)\n    test_pred.append(model.predict(x_test)['logits'])","d4bb4ef4":"y_true = np.concatenate(oof_labels)\ny_preds = np.concatenate(oof_pred)\n\n\nfor fold, history in enumerate(history_list):\n    print(f\"FOLD {fold+1} RMSE: {np.min(history['val_root_mean_squared_error']):.4f}\")\n    \nprint(f'OOF RMSE: {mean_squared_error(y_true, y_preds, squared=False):.4f}')","011b555e":"for fold, history in enumerate(history_list):\n    print(f'\\nFOLD: {fold+1}')\n    plot_metrics(history)","0ca2fda9":"submission = test[['id']]\nsubmission['target'] = np.mean(test_pred, axis=0)\nsubmission.to_csv('.\/submission.csv', index=False)\ndisplay(submission.head(10))","fead1973":"## Models :\n1. Bag of words \n2. TF-IDF \n3. Word2Vec\n3. Decision tree & ensamble\n4. Support Vector machine\n5. Transformer","337c6ca9":"#### Model","28e50793":"## 1. Bag of words & linear regression\n","48b2e765":"Training and evaluation of a regression tree.","e2e9f4a1":"#### Training performance","d100c89d":"The first step in our process is cleaning the data and making them machine-readable. \nTo this purpose, we perform:\n\n* Stemming \n* Stop Words Removing\n* Tokenization\n* Lowercase Standardization","e1363a4e":"Loading data from the challenge:","ec6ae274":"Training and evaluation of a regression forest.","be295305":"## 5. Support Vector machine","2ccdd63b":"The first model is a simple linear regression trained on the bag of words representation.\nFirst we transform the data into vectors, then we proceed with the classical train\/test split.","736ff68a":"We proceed with the classical train\/test split.","5a996643":"### Regression Forest\n","d4ac6a24":"Training and evaluation of a support vector machine.","582dfae2":"## 3. Word2Vec\n","465d750b":"#### Import libraries from transformes and define useful function for the training phase.","94878c25":"Then we compute the vectorization on the training data.","b615446d":"#### Learning strategy","00514959":"From the linear regression we can extract the coefficient that represents the feature importance of each word.","6a35622b":"After training the algorithm on our current dataset we push further with new models based on transfer learning.\nIn this case we will use the \"RoBERTa-base\" model, which is a smaller implementation of  [Roberta](https:\/\/arxiv.org\/abs\/1907.11692).","6a3f7572":"After having decided that Word2Vec is the best technique to pre-process the data we start going further with the linear regression.\nIn this section we will evaluate the tree-based models.","f0d03214":"Most importantly, we build a matrix that allows the subsequent models to be implemented. In fact, machine learning models receive as an input a matrix that should represent the underlying data in vectors.\n\nIt's worth noticing that we don't limit our approach to one single vectorization. That is, we use three different approaches as elements of the matrix in order to allow model comparison and to reduce the risk of having biased models.\nSpecifically, we use:\n    \n1. COUNT\n2. TF\/IDF\n3. WORD2VEC\n\nNotice that word2vec doesn't need stemming or other pre-processing techniques.\n\nWe will now compare the different vectorization approaches using a simple linear regression.\n","ea22bcce":"The second model is a linear regression trained on the TF-IDF representation.\nWe follow the same approach as the previous model","00a843d9":"Error distribution:","d138f320":"SVM with best parameter found by the grid search and to further optimize the model we train it with cross validation with five fold.","e03ea3fe":"### Boosted Tree\n","17ef5db3":"SVM with best parameter found by the grid search and to further optimize the model we train it with cross validation with five fold.","9e50b9e1":"#### Submission","b087c225":"The support vector machine seems to be the model that perform better in this task.\n\nSo we are going to optimize the hyperparameter using the grid search technique.","f58fa7f8":"### Regression Tree\n","643560dc":"#### Hyperparameters","32242ab3":"The first step, obviously, is to import all the necessary packages to implement the NLP prediction process. \nSpecifically, to carry out the different machine learning tasks we import:\n\n* NLTK (data preprocessing)\n* scikitlearn (models implementation) \n* re (Regex)\n* gensim(word2vec)\n","b173182b":"Training and evaluation of a boosted tree.","8e905688":"#### Training","ade6750e":"Then we initialized the linear regression, trained it and evaluated it on the test set.","e3893779":"\n\n__This notebook is using commonlitreadibilityprize__ data to showcase the different possible approaches to solve an NLP regression problem.\nIn real word application this would be just the last part of the long data science pipeline.\nIn this notebook we will only focus on the most common modelling techniques, going from the most basic to the most complex one","fe5c7a9e":"## Pre Processing","de709a19":"Finally we compute the last linear regression on the word2vec data.","2f28fbe4":"## 4. Decision tree & ensamble\n","be0b23ab":"## 2. TF-IDF\n","266c7597":"## 5. Transfomer\n","513579e9":"The final linear regression presented is based on the word2vec representation.\n\nFirst of all, we load the pretrained word embedding model.","84527081":"It's clear that the word2vec representation of the data is superior compared to the Bag of words and the TF-IDF. Therefore, that would be the representation of choice for the next models.\n","418b3dc6":"# NLP from bag of words to transformer","baec9ecc":"# Conclusion\n\nThe performance of the transformer is superior to the Support vector machine so we are going to push that model to the challenge submission.\nObviously there is room for further improvement by fine tuning the transformer model with techniques such as:\n\n1. Testing different hyperparameters \n2. Gradient Clipping\n3. Differential Learning Rate\n4. Scaling up the the full Roberta model\n5. Ensamble learning on multiples istances of the model","377d6c7e":"Then we create a function to compute the vector that will represent each excerpt."}}