{"cell_type":{"8e54619a":"code","b4c19d3b":"code","77fbc5cf":"code","702ee482":"code","975e7578":"code","c8885063":"code","6bb4850a":"code","dedf5054":"code","687038fd":"code","d46031de":"code","55182a74":"code","9c1b2ac5":"code","7b58872c":"code","1c441047":"code","d53e6c5a":"code","a54b4b34":"code","93fcbf63":"code","eec720d4":"code","5e34ee7d":"code","550443ec":"code","6884c027":"code","40934809":"code","fe4365e1":"code","74924d98":"markdown","4251aa70":"markdown","638a1713":"markdown","7063c609":"markdown","a52d9c90":"markdown","052c7db7":"markdown","f8929707":"markdown","940d66a9":"markdown","347566a2":"markdown","4656c933":"markdown"},"source":{"8e54619a":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D,GlobalMaxPooling1D\nfrom sklearn.model_selection import train_test_split\nprint(tf.__version__)","b4c19d3b":"train_df = pd.read_csv('..\/input\/cleaned-toxic-comments\/train_preprocessed.csv')\ntrain_df.sample(10, random_state=1)","77fbc5cf":"x = train_df['comment_text'].values\nprint(x)","702ee482":"# View few toxic comments\ntrain_df.loc[train_df['toxic']==1].sample(10, random_state=10)","975e7578":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = train_df['comment_text'].loc[train_df['toxic']==1].values\nwordcloud = WordCloud(\n    width = 640,\n    height = 640,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (12, 8),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","c8885063":"y = train_df['toxic'].values\nprint(y)","6bb4850a":"# Plot frequency of toxic comments\ntrain_df['toxic'].plot(kind='hist', title='Distribution of Toxic Comments');","dedf5054":"train_df['toxic'].value_counts()","687038fd":"max_features = 20000\nmax_text_length = 400","d46031de":"x_tokenizer = tf.keras.preprocessing.text.Tokenizer(max_features)\nprint(x_tokenizer)","55182a74":"x_tokenizer = tf.keras.preprocessing.text.Tokenizer(max_features)\nx_tokenizer.fit_on_texts(list(x))\nx_tokenized = x_tokenizer.texts_to_sequences(x) #list of lists(containing numbers), so basically a list of sequences, not a numpy array\n#pad_sequences:transform a list of num_samples sequences (lists of scalars) into a 2D Numpy array of shape \nx_train_val = sequence.pad_sequences(x_tokenized, maxlen=max_text_length)","9c1b2ac5":"embedding_dims = 100\nembeddings_index = dict()\nf = open('..\/input\/glove6b\/glove.6B.100d.txt')\nfor line in f:\n  values = line.split()\n  word = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()\n\nembedding_matrix = np.zeros((max_features, embedding_dims))\nfor word, index in x_tokenizer.word_index.items():\n  if index > max_features -1:\n    break\n  else:\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n      embedding_matrix[index] = embedding_vector","7b58872c":"print('Build model...')\nmodel = Sequential()\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\n#load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\n#(we don't want to update them during training).\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n                    trainable=False))\nmodel.add(Dropout(0.2))","1c441047":"filters = 250\nkernel_size = 3\nhidden_dims = 250","d53e6c5a":"# we add a Convolution1D, which will learn filters\n# word group filters of size filter_length:\nmodel.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu'))\nmodel.add(MaxPooling1D())\nmodel.add(Conv1D(filters,\n                 5,\n                 padding='valid',\n                 activation='relu'))\n# we use max pooling:\nmodel.add(GlobalMaxPooling1D())\n# We add a vanilla hidden layer:\nmodel.add(Dense(hidden_dims, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# We project onto 6 output layers, and squash it with a sigmoid:\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","a54b4b34":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","93fcbf63":"x_train, x_val, y_train, y_val = train_test_split(x_train_val, y, test_size=0.15, random_state=1)","eec720d4":"batch_size = 32\nepochs = 3","5e34ee7d":"model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(x_val, y_val))","550443ec":"model.evaluate(x_val, y_val, batch_size=128)","6884c027":"word_index = x_tokenizer.word_index\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])","40934809":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape)","fe4365e1":"import io\n\nout_v = io.open('vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('meta.tsv', 'w', encoding='utf-8')\n\nfor word_num in range(max_features):\n  word = reverse_word_index[word_num+1]\n  embeddings = weights[word_num]\n  out_m.write(word + \"\\n\")\n  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\nout_v.close()\nout_m.close()","74924d98":"## Task 5: Create Embedding Layer","4251aa70":"### Task 6: Build the Model","638a1713":"## Task 1: Import Packages and Functions","7063c609":"## Task 3: Data Prep \u2014 Tokenize and Pad Text Data","a52d9c90":"## (Optional) Task 8: Visualize Embeddings","052c7db7":"## Task 6: Train Model","f8929707":"## Task 2: Load and Explore Data","940d66a9":"## Task 7: Evaluate Model","347566a2":"<h2 align=center> Toxic Comments Classification using 1D CNN with Keras<\/h2>","4656c933":"## Task 4: Prepare Embedding Matrix with Pre-trained GloVe Embeddings"}}