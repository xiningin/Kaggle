{"cell_type":{"284497e7":"code","d2d8e2a1":"code","7d7cb105":"code","4250179a":"code","acb7c7be":"code","66ec170f":"code","21b37a57":"code","63a9a709":"code","94e0e66b":"markdown","1f4959e8":"markdown","ef1eeb76":"markdown","784813e4":"markdown","6254fff3":"markdown","503044c9":"markdown","c5426d2d":"markdown","cebc8c98":"markdown","f620e47d":"markdown","e2d54005":"markdown"},"source":{"284497e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2d8e2a1":"from __future__ import print_function\nfrom keras.layers import Dense, Activation\nfrom keras.layers.recurrent import SimpleRNN\nfrom keras.models import Sequential","7d7cb105":"fin = open(\"..\/input\/alice_in_wonderland.txt\", 'rb')\nlines = []\nfor line in fin:\n    line = line.strip().lower()\n    line = line.decode(\"ascii\", \"ignore\")\n    if len(line) == 0:\n        continue\n    lines.append(line)\nfin.close()\ntext = \" \".join(lines)","4250179a":"# set of characters that occur in the text\nchars = set([c for c in text])\n# Total items in our vocabulary\nnb_chars = len(chars)\n# lookup tables to deal with indexes of characters rather than the characters themselves.\nchar2index = dict((c, i) for i, c in enumerate(chars))\nindex2char = dict((i, c) for i, c in enumerate(chars))","acb7c7be":"SEQLEN = 10\nSTEP = 1\ninput_chars = []\nlabel_chars = []\nfor i in range(0, len(text) - SEQLEN, STEP):\n    input_chars.append(text[i:i + SEQLEN])\n    label_chars.append(text[i + SEQLEN])","66ec170f":"X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\ny = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\nfor i, input_char in enumerate(input_chars):\n    for j, ch in enumerate(input_char):\n        X[i, j, char2index[ch]] = 1\n    y[i, char2index[label_chars[i]]] = 1","21b37a57":"HIDDEN_SIZE = 128\nBATCH_SIZE = 128\nNUM_ITERATIONS = 25\nNUM_EPOCHS_PER_ITERATION = 1\nNUM_PREDS_PER_EPOCH = 100\n\nmodel = Sequential()\nmodel.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False, \n                    input_shape=(SEQLEN, nb_chars), \n                    unroll=True))\nmodel.add(Dense(nb_chars))\nmodel.add(Activation(\"softmax\"))\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")","63a9a709":"for iteration in range(NUM_ITERATIONS):\n    print(\"=\" * 50)\n    print(\"Iteration #: %d\" % (iteration))\n    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n    test_idx = np.random.randint(len(input_chars))\n    test_chars = input_chars[test_idx]\n    print(\"\\nGenerating from seed: %s\" % (test_chars))\n    print(test_chars, end=\"\")\n    for i in range(NUM_PREDS_PER_EPOCH):\n        Xtest = np.zeros((1, SEQLEN, nb_chars))\n        for i, ch in enumerate(test_chars):\n            Xtest[0, i, char2index[ch]] = 1\n        pred = model.predict(Xtest, verbose=0)[0]\n        ypred = index2char[np.argmax(pred)]\n        print(ypred, end=\"\")\n        # move forward with test_chars + ypred\n        test_chars = test_chars[1:] + ypred\nprint()","94e0e66b":"## 4. Model Training and Prediction\n* Our training approach is a little different from what we have seen so far. So far our approach has been to train a model for a fixed number of epochs, then evaluate it against a portion of held-out test data. Since we don't have any labeled data here, we train the model for an epoch(`NUM_EPOCHS_PER_ITERATION=1`) then test it. We continue training like this for **25** (`NUM_ITERATIONS=25`) iterations, stopping once we see intelligible output. So effectively, we are training for NUM_ITERATIONS epochs and testing the model after each epoch.\n\n* Our test consists of generating a character from the model given a random input, then dropping the first character from the input and appending the predicted character from our previous run, and generating another character from the model. We continue this 100 times (`NUM_PREDS_PER_EPOCH=100`) and generate and print the resulting string. The string gives us an indication of the quality of the model.\n\nLet's review the code.","1f4959e8":"## 3. Model Building.\n\n* We define the RNN's output dimension to have a size of **128**. This is a hyper-parameter that needs to be determined by experimentation. In general, if we choose too small a size, then the model does not have sufficient capacity for generating good text, and you will see long runs of repeating characters or runs of repeating word groups. On the other hand, if the value chosen is too large, the model has too many parameters and needs a lot more data to train effectively.\n* We want to return a single character as output, not a sequence of characters, so `return_sequences=False`.\n* In addition, we set `unroll=True` because it improves performance on the TensorFlow backend.\n* The RNN is connected to a dense (fully connected) layer. The dense layer has (nb_char) units, which emits scores for each of the characters in the vocabulary. The activation on the dense layer is a softmax, which normalizes the scores to probabilities. The character with the highest probability is chosen as the prediction.\n* We compile the model with the categorical cross-entropy loss function, a good loss function for categorical outputs, and the RMSprop optimizer.\n\nNow let's review the code of model building.\n","ef1eeb76":"The next step is to create the input and label texts. We do this by stepping through the text by a numberof characters given by the `STEP` variable and then extracting a span of text whose size is determined by the `SEQLEN` variable. The next character after the span is our label character.","784813e4":"## 1. Introduction.\n\nRNNs have been used extensively by the **natural language processing (NLP)** community for various applications. One such application is building language models. A language model allows us to predict the probability of a word in a text given the previous words. Language models are important for various higher level tasks such as machine translation, spelling correction, and so on.\n\nA side effect of the ability to predict the next word given previous words is a generative model that allows us to generate text by ***sampling from the output probabilities***. In language modeling, our input is typically a sequence of words and the output is a sequence of predicted words. The training data used is existing unlabeled text, where we set the label **y(t)** at time **t** to be the input **x(t+1)** at time **t+1**.\n\nWe will train a character based language model on the text of ***Alice in Wonderland*** to predict the next character given **10** previous characters. We have chosen to build a character-based model here because it has a smaller vocabulary and trains quicker. The idea is the same as using a **word-based language model**, except we use characters instead of words. We will then use the trained model to generate some text in the same style.\n\nLet's get started.","6254fff3":"The next step is to vectorize these input and label texts. Each row of the input to the RNN\ncorresponds to one of the input texts shown previously. There are `SEQLEN` characters in this input, and since our vocabulary size is given by `nb_chars`, we represent each input character as a **one-hot encoded vector** of size (`nb_chars`). Thus each input row is a tensor of size (`SEQLEN` and `nb_chars`). \n\nOur output label is a single character, so similar to the way we represent each character of our input, it is represented as a **one-hot vector of size** (`nb_chars`). Thus, the shape of each label is `nb_chars`.","503044c9":"## 2. Import data and preprocessing.\n\nWe read our input text from the text of ***Alice in Wonderland*** on the Project Gutenberg website (https:\/\/www.gutenberg.org\/files\/11\/11.txt). The file contains line breaks and non-ASCII characters, so we do some preliminary cleanup and write out the contents into a variable called text.","c5426d2d":"**Hope that you find this notebook helpful. More to come.**\n\n**Please upvote this, to keep me motivate for doing better.**\n\n**Thanks.**\n","cebc8c98":"As you can see, by the end of the 25th epoch, it has learned to spell reasonably well, although it has trouble expressing coherent thoughts. The amazing thing about this model is that it is character-based and has no knowledge of words, yet it learns to spell words that look like they might have come from the original text.","f620e47d":"# Generating text for NLP using SimpleRNN with keras\n\n### Table of interest:\n1. Introduction.\n2. Import data and preprocessing.\n3. Model Building.\n4. Model Training and Prediction.\n5. Conclusion.\n\nThis exemple was simply taken in the book **Deep Learning with Keras**, *by Antonio Gulli and Sujit Pal, 2017.*\n","e2d54005":"## 5. Conclusion.\n\nGenerating the next character or next word of text is not the only thing you can do with this sort of model. This kind of model has been successfully used to make **stock predictions** (for more information refer to the article: ***Financial Market Time Series Prediction with Recurrent Neural Networks, by A. Bernal, S. Fok, and R. Pidaparthi, 2012***) and generate classical music (for more information refer to the article: ***DeepBach: A Steerable Model for Bach Chorales Generation, by G. Hadjeres and F. Pachet, arXiv:1612.01010, 2016***), to name a few interesting applications. **Andrej Karpathy** covers a few other fun examples, such as generating fake Wikipedia pages, algebraic geometry proofs, and Linux source code in his blog post at: ***The Unreasonable Effectiveness of Recurrent Neural Networks at http:\/\/karpathy.github.io\/2015\/05\/21\/rnn-effectiveness\/.***\n\n### References:\n* **Deep Learning with Keras**, *by Antonio Gulli and Sujit Pal, 2017.*"}}