{"cell_type":{"51b23f25":"code","17a3a8c1":"code","38a5e1f1":"code","34f788b0":"code","4deec829":"code","c83f3d79":"code","ff67dbec":"code","264a8691":"code","1e0bc241":"code","32e1668b":"markdown","bcf47211":"markdown","17e0ca47":"markdown","f42eb30d":"markdown"},"source":{"51b23f25":"from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input \nfrom keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, concatenate\nfrom keras.layers import Activation, Dense, Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model\nfrom keras.layers.core import RepeatVector\nfrom PIL import Image\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img, ImageDataGenerator\nfrom skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\nfrom skimage.transform import resize\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom skimage.io import imsave","17a3a8c1":"images_rgb_path = '..\/input\/intel-image-classification\/seg_pred\/seg_pred'\n\nX = []\nfor filename in os.listdir(images_rgb_path):\n    img = Image.open(images_rgb_path+'\/'+filename)\n    img = img.resize((150,150))\n    X.append(np.array(img))\n    #X.append(img_to_array(load_img(images_rgb_path+'\/'+filename)))\nprint(len(X))","38a5e1f1":"Xtrain = np.array(X)\nXtrain = Xtrain \/ 255.0\nprint(Xtrain.shape)","34f788b0":"inception = InceptionResNetV2(include_top=True, weights=\"imagenet\")","4deec829":"\nembed_input = Input(shape=(1000,))\n\n#Encoder\nencoder_input = Input(shape=(150, 150, 1,))\nencoder_output = Conv2D(32, (3,3), activation='relu', strides=2)(encoder_input)\nencoder_output = Conv2D(64, (3,3), activation='relu', strides=2)(encoder_output)\nencoder_output = Conv2D(128, (3,3), activation='relu')(encoder_output)\nencoder_output = Conv2D(128, (3,3), activation='relu')(encoder_output)\n\n#Fusion\nfusion_output = RepeatVector(32 * 32)(embed_input) \nfusion_output = Reshape((32, 32, 1000))(fusion_output)\nfusion_output = concatenate([encoder_output, fusion_output], axis=3) \nfusion_output = Conv2D(32, (1, 1), activation='relu', padding='same')(fusion_output) \n\n#Decoder\ndecoder_output = Conv2D(64, (3,3), activation='relu')(fusion_output)\ndecoder_output = UpSampling2D((5, 5))(decoder_output)\ndecoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\ndecoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\ndecoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n\nmodel = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)\nmodel.summary()","c83f3d79":"generator = ImageDataGenerator().flow(Xtrain,batch_size=10)","ff67dbec":"def train_gen():\n    a=1\n    \n    for batch in generator :\n        g = []\n        for i in batch : \n            g.append(resize(i,(299,299,3)))\n        embed = inception.predict(preprocess_input(np.array(g)))\n        #embed = inception.predict(preprocess_input(batch))\n        lab_batch = rgb2lab(batch)\n        X_batch = lab_batch[:,:,:,0]\n        X_batch = X_batch.reshape(X_batch.shape+(1,))\n        Y_batch = lab_batch[:,:,:,1:] \/ 128.0\n        if(a):\n            a=0\n            print('batch length',len(batch))\n            print('X_batch.shape',X_batch.shape)\n            print('Y_batch.shape',Y_batch.shape)\n            print('embed.shape',embed.shape)\n        yield ([X_batch, embed], Y_batch)\n\n#Train model      \nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nlr_reduction = ReduceLROnPlateau(monitor='loss', patience=1, verbose=1, factor=0.1, min_lr=0.00001)\ncheckpoint = ModelCheckpoint(\"colorizeAdam.h5\", monitor='accuracy',\n                             save_weights_only=True, mode='max', save_best_only=True, verbose=1)\nmodel.fit(train_gen(), epochs=100, steps_per_epoch=len(generator)\/\/10, callbacks=[lr_reduction])","264a8691":"\ncolor_me = []\nimgT = Image.open('..\/input\/intel-image-classification\/seg_test\/seg_test\/buildings\/20083.jpg')\nimgT = img.resize((150,150))\ncolor_me.append(np.array(imgT))\nplt.imshow(color_me[0])","1e0bc241":"print(color_me)\n\ncolor_me = np.array(color_me)\ncolor_me_embed = inception.predict_generator(preprocess_input(color_me))\ncolor_me = rgb2lab(1.0\/255*color_me)[:,:,:,0]\ncolor_me = color_me.reshape(color_me.shape+(1,))\n\n\n# Test model\noutput = model.predict([color_me, color_me_embed])\noutput = output * 128\n\n# Output colorizations\nfor i in range(len(output)):\n    cur = np.zeros((150, 150, 3))\n    cur[:,:,0] = color_me[i][:,:,0]\n    cur[:,:,1:] = output[i]\n    print(cur)\n    plt.imshow(lab2rgb(cur))\n    #imsave(str(i)+\".jpg\", lab2rgb(cur))","32e1668b":"https:\/\/arxiv.org\/pdf\/2008.10774.pdf\nImage Colorization: A Survey and Dataset","bcf47211":"66.6% acc","17e0ca47":"loading the images and the inception model","f42eb30d":"This Data contains around 25k images of size 150x150 distributed under 6 categories."}}