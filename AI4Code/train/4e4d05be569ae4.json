{"cell_type":{"abf5660e":"code","ce1b78f1":"code","4fde76f7":"code","e1972948":"code","44400e76":"code","5cfc7eb8":"code","20c03eee":"code","4492d49a":"code","5647d7fa":"code","089558e8":"code","272dd6b7":"code","f5f13ea7":"code","015ba63a":"code","c213a475":"code","64950016":"code","cc0e8528":"code","7c14658c":"code","5b71c05e":"code","499d0534":"code","d1019d19":"code","63cdf239":"code","f539e14c":"code","a50e61ba":"code","4fcfa7a6":"code","4c1a55ee":"code","6a566292":"code","ab8bcb5f":"code","3dad80f5":"code","320435df":"code","266eaa47":"code","343862b3":"code","9681385b":"code","952ddb38":"code","adec8187":"code","53745349":"code","2c89db62":"code","f927d205":"code","137f948d":"code","874e57b0":"code","c48f21d3":"code","0b947ff8":"code","e00ad405":"code","4709ca93":"code","4ab4f9bc":"code","08978bd9":"code","6b357c79":"code","277772cf":"code","994a4594":"code","c9d0dda5":"code","9215f1ab":"code","3368058e":"code","4b272325":"code","e4b439a1":"code","25f4808b":"code","f366e2d9":"code","dadc3639":"code","c767a0f1":"code","02647464":"code","4e48c66b":"code","d1a1c33f":"code","ee57653d":"code","6d552057":"code","9e1ba554":"code","4e209e11":"code","29a905bb":"code","fff60aa0":"code","e5a17e0a":"code","fe2310a8":"code","4e855dd7":"code","e1dccb5d":"code","93717e54":"markdown"},"source":{"abf5660e":"# Print me to document when run:\nimport datetime\nprint(\"Start Run: \" + str(datetime.datetime.now()) )","ce1b78f1":"# Imports:\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n# import numpy as np\n# import pandas as pd\n# import bq_helper\n# import csv\nprint('done')","4fde76f7":"# Code created by Kaggle when initializing notebook.\n# KEPT AS IS:\n# ---------------------------------------------------\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/\"))\n\n# Any results you write to the current directory are saved as output.","e1972948":"# Load data using BigQueryHelper:\n# I'm using some existing code provided by: \n# https:\/\/www.kaggle.com\/juliaelliott\/ga-bigquery-starter-kernel\/notebook\n# Also found a great article\/ help from Megan Risdal:\n# https:\/\/medium.com\/google-cloud\/learning-to-analyze-huge-bigquery-datasets-using-python-on-kaggle-2c6c6153f542\n\nimport bq_helper\n# use the BQHelper library to pull datasets\/tables from BigQuery\n\nprint(\"Loading Training Data Set...\")\nga_bq_train = bq_helper.BigQueryHelper(active_project= \"kaggle-public-datasets\", dataset_name = \"ga_train_set\")\nprint(\"Loading Test Data Set... \")\nga_bq_test = bq_helper.BigQueryHelper(active_project= \"kaggle-public-datasets\", dataset_name = \"ga_test_set\")\nprint(\"Data Loaded: done\")","44400e76":"# What is ga_bq_test? \n# For my reference: some of the BigQueryHelper methods\n# -------------------------------------------------------\n# type(ga_bq_test) # bq_helper.BigQueryHelper\n# ga_bq_test.BYTES_PER_GB # 1073741824\n# ga_bq_test.tables # {}\n# ga_bq_test.client # <google.cloud.bigquery.client.Client at 0x7f1bf9a3ea90>\n# ga_bq_test.dataset # Dataset(DatasetReference('kaggle-public-datasets', 'ga_test_set'))\n# ga_bq_test.dataset_name # 'ga_test_set'\n# ga_bq_test.head # expects the table name head('table_name')\n# ga_bq_test.project_name # 'kaggle-public-datasets'\n# ga_bq_test.total_gb_used_net_cache # 0\n# ga_bq_test.list_tables() # a list of all the tables\n# ga_bq_test.table_schema(table_name) # schema of a specific table","5cfc7eb8":"# First look at what we have to work with:\nprint( \"Number of Tables in Train Data Set: \" + str(len(ga_bq_train.list_tables())) )\nprint( \"Number of Tables in Test Data Set: \" + str(len(ga_bq_test.list_tables())) )\nga_bq_test_list_tables = ga_bq_test.list_tables()\nga_bq_test_list_tables[0:3]\n#\n# Number of Tables in Train Data Set: 366\n# Number of Tables in Test Data Set: 272\n# ['ga_sessions_20170802',\n#  'ga_sessions_20170803',\n# ...","20c03eee":"# OUTPUT a csv file of the list of tables\nimport csv\n\n# TRAIN : ga_bq_train\nprint('saving... ga_bq_train_list_tables ...')\nlist_of_tables = ga_bq_train.list_tables()\ncsvfile = \"ga_bq_train_list_tables.csv\"\n#Assuming res is a flat list\nwith open(csvfile, \"w\") as output:\n    writer = csv.writer(output, lineterminator='\\n')\n    for table in list_of_tables:\n        writer.writerow([table])    \nprint('done')\n\n# TEST : ga_bq_test\nprint('saving... ga_bq_test_list_tables ...')\nlist_of_tables = ga_bq_test.list_tables()\ncsvfile = \"ga_bq_test_list_tables.csv\"\n#Assuming res is a flat list\nwith open(csvfile, \"w\") as output:\n    writer = csv.writer(output, lineterminator='\\n')\n    for table in list_of_tables:\n        writer.writerow([table])    \nprint('done')","4492d49a":"# TABLE AND SCHEMA INSPECTION VIA GOOGLE SHEETS\n# I used the above csv file to inpsect data in Google Sheets :\n# https:\/\/docs.google.com\/spreadsheets\/d\/1XBRL5cmJwqGe2KENLITTL_4DY-DOeAr4-JzlUYoMdYw\/edit?usp=sharing\n#\n# NOTES:\n# -- The main field to test against will be: totals.transactionRevenue\n# -- A number of fields will NOT be needed\n# -- The 'time' seems to NOT include timezone","5647d7fa":"# TEST table schema:\n# Take a look at the TEST set schema for one of the tables\nprint(ga_bq_test.table_schema('ga_sessions_20170802').shape)\nga_bq_test_table_schema_ga_sessions_20170802 = ga_bq_test.table_schema('ga_sessions_20170802')\nga_bq_test_table_schema_ga_sessions_20170802.head(5)\n# There are 185 'columns'","089558e8":"# TRAIN table schema:\n# Take a look at the TRAIN set schema for one of the tables\nprint(ga_bq_train.table_schema('ga_sessions_20160801').shape)\nga_bq_train_table_schema_ga_sessions_20160801 = ga_bq_train.table_schema('ga_sessions_20160801')\nga_bq_train_table_schema_ga_sessions_20160801.head(5)\n# There are 185 'columns'","272dd6b7":"# Save the schema outputs to csv files:\n# a great quick reference: https:\/\/www.kaggle.com\/szamil\/where-is-my-output-file\nprint('saving csv file...')\nga_bq_test_table_schema_ga_sessions_20170802.to_csv('ga_bq_test_table_schema_ga_sessions_20170802.csv', index = False)\nga_bq_train_table_schema_ga_sessions_20160801.to_csv('ga_bq_train_table_schema_ga_sessions_20160801.csv', index = False)\nprint('done')","f5f13ea7":"# Just checking the way to reference specific column name:\nreference_schema = ga_bq_test.table_schema('ga_sessions_20170802')\nprint( reference_schema.name[184] )\nreference_schema.name[184]","015ba63a":"# I want to make sure all of the tables have the same schema:\n# for an item in list I want to look up the schema and compare it to a reference schema\n# I'm just picking a field at random, and comparing the dimension size\n#\n# ----------- commentted out no need to re-run each commit -----------\n# ----------- all tables do appear to have same schema -----------\n'''\n# CHECK TEST SET:\n# let's use the first table within the test set of data as our reference:\nreference_schema = ga_bq_test.table_schema('ga_sessions_20170802')\nreference_shape = reference_schema.shape\nprint(reference_shape)\n#\nprint('checking test data schema')\nga_bq_test_list_tables = ga_bq_test.list_tables()\nprint(len(ga_bq_test_list_tables))\n# keep a count of the table we are on for easy tracking (old school coding)\nnnn = 0\nfor table in ga_bq_test_list_tables:\n    nnn = nnn + 1\n    # grab the schema\n    table_to_check_schema = ga_bq_test.table_schema(table)\n    table_to_check_shape = table_to_check_schema.shape\n    # ok, I'm going to cheat:\n    # I'm going to check if the shapes are the same\n    # and I'll spot check one random column near the end.\n    if (reference_shape == table_to_check_shape) and (table_to_check_schema.name[150] == reference_schema.name[150]):\n        print(\"table: \" + str(nnn) + \" : \" + table + \" check ok\" )\n    else:\n        print(\"table: \" + str(nnn) + \" : \" + table + \" check NOT OK\")\n        break\nprint('TEST set done')\nprint(' ')\n\n# CHECK TRAIN SET:\n# let's use the first table within the train set of data as our reference:\nreference_schema = ga_bq_train.table_schema('ga_sessions_20160801')\nreference_shape = reference_schema.shape\nprint(reference_shape)\n#\nprint('checking train data schema')\nga_bq_train_list_tables = ga_bq_train.list_tables()\nprint(len(ga_bq_train_list_tables))\nnnn = 0\nfor table in ga_bq_train_list_tables:\n    nnn = nnn + 1\n    # grab the schema\n    table_to_check_schema = ga_bq_train.table_schema(table)\n    table_to_check_shape = table_to_check_schema.shape\n    # ok, I'm going to cheat:\n    # I'm going to check if the shapes are the same\n    # and I'll spot check one random column near the end.\n    if (reference_shape == table_to_check_shape) and (table_to_check_schema.name[150] == reference_schema.name[150]):\n        print(\"table: \" + str(nnn) + \" : \" + table + \" check ok\" )\n    else:\n        print(\"table: \" + str(nnn) + \" : \" + table + \" check NOT OK\")\n        #print(table_to_check_shape)\n        #print(table_to_check_schema.name[150])\n        #print(reference_schema.name[150])\n        break\nprint('TRAIN set done')\n'''\n# ----------- end of commented section -----------\n#\n# Results:\n# OK, \n# I commented this out because it takes some time, and I only needed to do it once to convince myself\n# \n# The Test data and Train data sets are the same except:\n# The Train set has one additional totals.transactionRevenue column\n#\nprint('done')","c213a475":"# DATA FIELDS AS FROM THE CONTEST DESCRIPTION\n# https:\/\/www.kaggle.com\/c\/ga-customer-revenue-prediction\/data\n#\n# Data Fields:\n# fullVisitorId- A unique identifier for each user of the Google Merchandise Store.\n# channelGrouping - The channel via which the user came to the Store.\n# date - The date on which the user visited the Store.\n# device - The specifications for the device used to access the Store.\n# geoNetwork - This section contains information about the geography of the user.\n# sessionId - A unique identifier for this visit to the store.\n# socialEngagementType - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n# totals - This section contains aggregate values across the session.\n# trafficSource - This section contains information about the Traffic Source from which the session originated.\n# visitId - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.\n# visitNumber - The session number for this user. If this is the first session, then this is set to 1.\n# visitStartTime - The timestamp (expressed as POSIX time).","64950016":"# A query estimate for a single table within the training dataset:\n# ga_sessions_20160801\nqueryy = \\\n    \"\"\"\n    SELECT  *\n    FROM `kaggle-public-datasets.ga_train_set.ga_sessions_20160801` \n    \"\"\"\n\nprint(\"size = \" + str( ga_bq_train.estimate_query_size(queryy) * 1000 ) + \" MB\" )","cc0e8528":"# A query for a single table within the training dataset\n# ga_sessions_20160801\nqueryy = \\\n    \"\"\"\n    SELECT  *\n    FROM `kaggle-public-datasets.ga_train_set.ga_sessions_20160801`\n    \"\"\"\n\nprint('a_small_query: start...')\nga_train_set_ga_sessions_20170803 = ga_bq_train.query_to_pandas_safe(queryy)\nprint('a_small_query: done.')\n\nga_train_set_ga_sessions_20170803.describe()","7c14658c":"ga_train_set_ga_sessions_20170803.info()","5b71c05e":"ga_train_set_ga_sessions_20170803.shape","499d0534":"print('saving csv file...')\nga_train_set_ga_sessions_20170803.to_csv('ga_train_set_ga_sessions_20170803.csv', index = False)\nprint('done')","d1019d19":"# SPECIFIC TABLE INSPECTION VIA GOOGLE SHEET\n# I used the above csv file to inpsect a single table within Google Sheets :\n# https:\/\/docs.google.com\/spreadsheets\/d\/1-kn-O6H4p_jNbfF0cN2i5o-FG1AN2leBTdXAzFDDdeQ\/edit?usp=sharing","63cdf239":"# A query estimate for a ALL tables within the training dataset:\nqueryy = \\\n    \"\"\"\n    SELECT  *\n    FROM `kaggle-public-datasets.ga_train_set.ga_sessions_*` \n    \"\"\"\n\nprint(\"size = \" + str( ga_bq_train.estimate_query_size(queryy) * 1000 ) + \" MB\" )\n# size = 737.8220958635211 MB","f539e14c":"# query the BQ train set tables to summarize total transaction revenue per user, where fullVisitorId is unique per user.\nqueryy = \\\n    \"\"\"\n    SELECT  fullVisitorId, coalesce(SUM( totals.transactionRevenue ),0) AS total_transactionrevenue_per_user\n    FROM `kaggle-public-datasets.ga_train_set.ga_sessions_*` \n    GROUP BY fullVisitorId\n    ORDER BY total_transactionrevenue_per_user DESC\n    \"\"\"\n\nprint(\"size = \" + str( ga_bq_train.estimate_query_size(queryy) * 1000 ) + \" MB\" )","a50e61ba":"# ok, let's do the query and save it:\nprint('starting queryy...')\ntotal_revenue_per_unique_fullVisitorID = ga_bq_train.query_to_pandas_safe(queryy)\nprint('done')\n# This took some time to run","4fcfa7a6":"type(total_revenue_per_unique_fullVisitorID)","4c1a55ee":"#Take a look at some basic analytics of the transaction revenue pulled from our query\ntotal_revenue_per_unique_fullVisitorID.describe()","6a566292":"total_revenue_per_unique_fullVisitorID.head(5)","ab8bcb5f":"# Save output.\nprint('saving csv file...')\ntotal_revenue_per_unique_fullVisitorID.to_csv('total_revenue_per_unique_fullVisitorID.csv', index = False)\nprint('done')","3dad80f5":"#How many of the users spent any money?\nlen(total_revenue_per_unique_fullVisitorID[total_revenue_per_unique_fullVisitorID.total_transactionrevenue_per_user > 0])\n# 9996","320435df":"total_revenue_per_unique_fullVisitorID.shape\n# (714167, 2)","266eaa47":"#Let's take a look at the average transaction revenue for each user who spent money.\n\nqueryy= \\\n\"\"\"\nSELECT\n( (SUM(total_transactionrevenue_per_visitorid) \/ SUM(total_visits_per_visitorid))\/1000000 ) AS avg_revenue_by_user_per_visit_as_dollars\nFROM (\n    SELECT\n    fullVisitorId,\n    SUM( totals.visits ) AS total_visits_per_visitorid,\n    SUM( totals.transactionRevenue ) AS total_transactionrevenue_per_visitorid\n    FROM\n    `kaggle-public-datasets.ga_train_set.ga_sessions_*`\n    WHERE\n    totals.visits > 0\n    AND totals.transactionRevenue IS NOT NULL\n    GROUP BY\n    fullVisitorId );\n\"\"\"\n\navg_transrev = ga_bq_train.query_to_pandas_safe(queryy, max_gb_scanned=10)\nprint('done')\navg_transrev.head(5)\n# $133.75","343862b3":"# another query to pull a few specific datapoints per user.\nprint('starting query...')\nqueryy = \\\n\"\"\"\nSELECT  fullVisitorId, \n    SUM(totals.transactionRevenue) AS total_transactionrevenue_per_user, \n    SUM(totals.pageviews) AS total_pagesviews_per_user,\n    SUM(totals.visits ) AS total_visits_per_user,\n    SUM(totals.timeOnSite ) AS total_timeonsite_per_user\nFROM `kaggle-public-datasets.ga_train_set.ga_sessions_*` \nGROUP BY fullVisitorId\nORDER BY total_transactionrevenue_per_user DESC\n\"\"\"\nall_train_summary2 = ga_bq_train.query_to_pandas_safe(queryy)\nprint('done')\nall_train_summary2.head()","9681385b":"#Now let's drop the data from that query into a dataframe.\n#This is the start of how you might pull features that you've queried out of BQ to begin modelling.\nfull_df = ga_bq_train.query_to_pandas(queryy)\nfull_df.describe()","952ddb38":"# END OF REFERENCE FROM \n# https:\/\/www.kaggle.com\/juliaelliott\/ga-bigquery-starter-kernel\/notebook\n# JULIA ELLIOTT\n# Thanks Julia!","adec8187":"# ok... what's next...","53745349":"# This, is all of the users\n# fullVisitorID = 4984366501121503466\nqueryy = \\\n    \"\"\"\n    SELECT  \n    fullVisitorId, \n    sessionId,\n    date,\n    visitStartTime,\n    coalesce(totals.transactionRevenue,0) AS totals_transactionRevenue\n    FROM `kaggle-public-datasets.ga_train_set.ga_sessions_*` \n    ORDER BY visitStartTime\n    \"\"\"\nprint(\"size = \" + str( ga_bq_train.estimate_query_size(queryy) * 1000 ) + \" MB\" )","2c89db62":"print('starting query...')\nallUserQuery_event_date_revenue = ga_bq_train.query_to_pandas_safe(queryy)\nprint('done')","f927d205":"allUserQuery_event_date_revenue.head(5)","137f948d":"allUserQuery_event_date_revenue.describe()","874e57b0":"# Save output.\nprint('saving csv file...')\nallUserQuery_event_date_revenue.to_csv('allUserQuery_event_date_revenue.csv', index = False)\nprint('done')","c48f21d3":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(15, 4), dpi=80, facecolor='w', edgecolor='k')\nx = allUserQuery_event_date_revenue.visitStartTime\ny = allUserQuery_event_date_revenue.totals_transactionRevenue\nplt.plot(x, y, 'o', color='black');","0b947ff8":"# This, is all of the users\n# fullVisitorID = 4984366501121503466\nqueryy = \\\n    \"\"\"\n    SELECT  \n    fullVisitorId, \n    sessionId,\n    date,\n    visitStartTime,\n    coalesce(totals.transactionRevenue,0) AS totals_transactionRevenue\n    FROM `kaggle-public-datasets.ga_train_set.ga_sessions_*` \n    WHERE fullVisitorId = '4984366501121503466'\n    ORDER BY visitStartTime\n    \"\"\"\nprint(\"size = \" + str( ga_bq_train.estimate_query_size(queryy) * 1000 ) + \" MB\" )","e00ad405":"print('start query')\nuserData_4984366501121503466 = ga_bq_train.query_to_pandas_safe(queryy)\nprint('done')","4709ca93":"userData_4984366501121503466.head(5)","4ab4f9bc":"# Let's save it:\nprint('saving csv file...')\nuserData_4984366501121503466.to_csv('userData_4984366501121503466.csv', index = False)\nprint('done')","08978bd9":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(15, 4), dpi=80, facecolor='w', edgecolor='k')\nx = userData_4984366501121503466.visitStartTime\ny = userData_4984366501121503466.totals_transactionRevenue\nmarkerline, stemlines, baseline = plt.stem(x, y, '-.')\n# setting property of baseline with color red and linewidth 2\nplt.setp(baseline, color='r', linewidth=2)\nplt.show()\n#\n# Interesting points about this specific user:\n# Most visits result in revenue\n# Is there a trend, or re-occuring pattern?\n# will need to lay seasons or events along time path.\n","6b357c79":"# just checking how to sort by time using the pandas sort_values() method:\nallUserQuery_event_date_revenue.sort_values(by=['visitStartTime'])\nallUserQuery_event_date_revenue.head(5)","277772cf":"# I want the SUM for each individual user\nqueryy = \\\n    \"\"\"\n    SELECT  \n    fullVisitorId, \n    MIN(date) AS minimum_date,\n    MIN(visitStartTime) AS minimum_vistStartTime,\n    coalesce(SUM( totals.transactionRevenue ),0) AS total_transactionrevenue_per_user\n    FROM `kaggle-public-datasets.ga_train_set.ga_sessions_*` \n    GROUP BY fullVisitorId\n    ORDER BY total_transactionrevenue_per_user DESC\n    \"\"\"\nprint(\"size = \" + str( ga_bq_train.estimate_query_size(queryy) * 1000 ) + \" MB\" )","994a4594":"print('starting query...')\ntrain_sum_for_each_user = ga_bq_train.query_to_pandas_safe(queryy)\nprint('done')","c9d0dda5":"train_sum_for_each_user.head(5)","9215f1ab":"# Double checking the data type, (Want to make sure we are working with numbers, not text)\nprint('minimum_date:')\nprint(train_sum_for_each_user['minimum_date'].describe())\nprint('-----')\nprint('minimum_vistStartTime:')\nprint(train_sum_for_each_user['minimum_vistStartTime'].describe())\nprint('-----')\nprint('total_transactionrevenue_per_user:')\nprint(train_sum_for_each_user['total_transactionrevenue_per_user'].describe())\n# minimum_date is an object (probably text)\n# minimum_visitStartTime is float64\n# total_transactionrevenue_per_user is float64","3368058e":"# In this silly model,\n# I want to calculate the average spend per day,\n# for the date range, I want to use the date of the first purchase in the data, divided by the last day of the data.\n#\n# We are subtracting midnight on 8\/1\/2017 plus 1.5 days (to make sure we don't get any negitive time, found by trial and error)\n# 8\/1\/2017 is 1501545600 in POSIX time (the time in seconds from 1\/1\/1970)\n#\n# This pandas dataframe will be a time delta in seconds:\ntrain_user_time_delta = (1501545600+(1.5*24*60*60)) - train_sum_for_each_user['minimum_vistStartTime'] \ntrain_user_time_delta.head(5)","4b272325":"train_user_time_delta.describe()","e4b439a1":"# Now calculating the revenue per day in given units of 1\/10^6 dollars per day.\ntrain_user_revenue_per_day = train_sum_for_each_user['total_transactionrevenue_per_user'] \/ (train_user_time_delta \/ (60 * 60 * 24) )\ntrain_user_revenue_per_day.head(5)","25f4808b":"train_user_revenue_per_day.describe()","f366e2d9":"# add results to dataframe as new column:\ntrain_sum_for_each_user['revenuePerDay'] = train_user_revenue_per_day\ntrain_sum_for_each_user.head(5)","dadc3639":"# checking some stats:\nprint(\"count of NaN: \" + str( train_sum_for_each_user['revenuePerDay'].isna().sum()) )\nprint(\"count of Null: \" + str( train_sum_for_each_user['revenuePerDay'].isnull().sum()) )\nprint(\"count of not NaN: \" + str( train_sum_for_each_user['revenuePerDay'].count()) )\nprint(\"rows and columns of data: \" + str(train_sum_for_each_user.shape) )\n#\n# train_sum_for_each_user[train_sum_for_each_user['revenuePerDay'] > 0].count()\nprint(sum(train_sum_for_each_user['revenuePerDay'] > 0)) # 9996 with revenue\nprint(sum(train_sum_for_each_user['revenuePerDay'] == 0)) # 704,171 no revenue\nprint(sum(train_sum_for_each_user['revenuePerDay'] < 0)) # just being complete","c767a0f1":"# Great!\n# now I just need to grab the 'test' data, and apply the silly formula\n# time detla of test set * revenue per day for each user = answer!","02647464":"# let's query for a list of all the unique test users:\n# I want the SUM for each individual user\nqueryy = \\\n    \"\"\"\n    SELECT  \n    fullVisitorId\n    FROM `kaggle-public-datasets.ga_test_set.ga_sessions_*` \n    GROUP BY fullVisitorId\n    ORDER BY fullVisitorId\n    \"\"\"\nprint(\"size = \" + str( ga_bq_test.estimate_query_size(queryy) * 1000 ) + \" MB\" )","4e48c66b":"print('starting query...')\ntest_only_unique_user_list = ga_bq_test.query_to_pandas_safe(queryy)\nprint('done.')","d1a1c33f":"test_only_unique_user_list.head(5)","ee57653d":"# Ok... let's create a dataframe where we have the unique list of test users\n# I'll be doing a left-join so I can keep all Test users, but match up any Train users:\ndf_a = test_only_unique_user_list\ndf_b = train_sum_for_each_user\ndf_new = pd.merge(df_a, df_b, on='fullVisitorId', how='left')\ndf_new.head(5)","6d552057":"df_new.describe()","9e1ba554":"# Im going to do an outer and inner join just to see how things add up:\ndf_outer = pd.merge(df_a, df_b, on='fullVisitorId', how='outer')\ndf_inner = pd.merge(df_a, df_b, on='fullVisitorId', how='inner')\nprint('done')","4e209e11":"print('train set unique count: ' + str(df_b.shape)) # (714167, 5)\nprint('test set unique count: ' + str(df_a.shape)) # (617242, 1)\nprint('train and test total combined: ' + str(df_outer.shape)) # (1323730, 5)\nprint('train within test: ' + str(df_inner.shape)) # (7679, 5)\nprint('count of how many users in test set have postive revenue: ' + str(sum(df_new['revenuePerDay'] > 0)) ) #536\n# NOTES: \n# Yes it all adds up: 714,167 + 617,242 - 7,679 = 1,323,730 (Joins work as they should)\n# We only have 7,679 matching user IDs between test and train.\n# we had 9,996 within Train that had greater than zero revenue (calculated previously)\n# in the new Test set caculated with this silly model: we have 536 users with positive revenue\n# (assuming that our population would be a similar percentage, we are wayyy under)","29a905bb":"print(\"count of NaN: \" + str( df_new['revenuePerDay'].isna().sum()) )\nprint(\"count of not NaN: \" + str( df_new['revenuePerDay'].count()) )\nprint(\"rows and columns of data: \" + str(df_new.shape) )\nprint(\"percent of data with NaN: \" + str(609563\/617242*100) + \"%\")\nprint(\"check that sums add up: \" + str(609563+7679) + \" ... yes, they add up\")\n# ok, this model sucks\n# 98.8% of users in the test set do NOT have data in the train set","fff60aa0":"df_new.revenuePerDay.head(5)","e5a17e0a":"# getting rid of NaN's to make math work\ndf_new.revenuePerDay.fillna(0,inplace=True)\ndf_new.revenuePerDay.head(5)","fe2310a8":"# I need to simply multiply the revenuePerday by the number of days in the test set for each user\noutput = np.log(df_new.revenuePerDay * 271+1)\noutput.head(5)","4e855dd7":"# finally, let's put it all together:\nvisit_id = df_new.fullVisitorId\nout_PredictedLogRevenue = pd.concat([visit_id, output], axis=1)\nout_PredictedLogRevenue.rename(columns={'revenuePerDay':'PredictedLogRevenue'}, inplace=True)\nout_PredictedLogRevenue.head(5)","e1dccb5d":"# Let's save it:\nprint('saving csv file...')\nout_PredictedLogRevenue.to_csv('submission_SteveBlack_v001_date_2018_1003_1137.csv', index = False)\nprint('done')","93717e54":"**Google Analytics Customer Revenue Prediction**\n\n* [http:\/\/www.kaggle.com\/c\/ga-customer-revenue-prediction](http:\/\/www.kaggle.com\/c\/ga-customer-revenue-prediction)\n\nCreated by Steve Black\nStarted on Sept.25.2018\n\nThis is a first basic attempt:\n* Connecting to the data using BigQueryHelper\n* Using Python 3 Notebook within Kaggle Kernal\n* Exploring how to access and query the data\n* Making notes on the data\n* Creating a silly model \n\nThe model:\n* For each user with revenue in the train set,\n* Calculate their total spend for each individual user\n* Divide by the total number of days (first transaction for each user to global end of train data time)\n* For any of the users that exist in the final Test set that are also in the Train set ( a small overlap)\n* Multiply the global duration (271 days) of the Test data set by the indivdual dollars per day calculated from the Train data set.\n\nYes, This is a bad model:\n* There are many users within the Test set that are not in the Train set. \n* These unique to Test users all get $0.\n* I used the whole Train set, and thus did not predict any sort of error or confidence with the Train set to compare any next models to.\n* However, it was a quick test of formatting the submisssion without getting deep on a real model.\n\nThis notebook spends most of it's time exploring the data using querys via BigQueryHelper. I've created two google sheets to make notes\n\nA list of the tables within Train and Test, along with their schemas and some notes I made:\n* https:\/\/docs.google.com\/spreadsheets\/d\/1-kn-O6H4p_jNbfF0cN2i5o-FG1AN2leBTdXAzFDDdeQ\/edit?usp=sharing\n\nA single specific table from the Train set:\n* https:\/\/docs.google.com\/spreadsheets\/d\/1-kn-O6H4p_jNbfF0cN2i5o-FG1AN2leBTdXAzFDDdeQ\/edit?usp=sharing\n\nI found this google support link that lists out the details of the schema as well:\n* https:\/\/support.google.com\/analytics\/answer\/3437719?hl=en\n\n----\nI submitted my results and my RMS score was 1.8181\nNot bad, but it put near the bottom of the list, so it is a bad model"}}