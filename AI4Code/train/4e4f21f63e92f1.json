{"cell_type":{"dce43037":"code","baef122b":"code","8bd32a11":"code","1b6c0a1f":"code","a430ffde":"code","7c80a4d8":"code","b46139f0":"code","2cee6908":"code","540d6fca":"code","3bbe3aef":"code","c175b26d":"code","75e66f95":"code","e34134e3":"code","fdb7ca9f":"code","15758688":"code","e1b2569b":"code","4a5af4f4":"code","929b6b66":"code","8ed5d7e2":"code","7a4f5070":"code","56e5a11a":"markdown","ac3ff1f3":"markdown","6f9b5730":"markdown","e549e2fc":"markdown","633ae14e":"markdown","c981d87f":"markdown"},"source":{"dce43037":"import sys\nimport pandas\nimport numpy\nimport sklearn\nimport keras","baef122b":"import pandas as pd\nimport numpy as np\n\n#load dataset\ndata=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","8bd32a11":"data.describe()","1b6c0a1f":"#preprocess data to replace zero values to nan and drop\n\ncolumns=['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\nfor col in columns:\n    data[col].replace(0,np.NaN,inplace=True)\n\ndata.describe()","a430ffde":"#drop rows with missing values\ndata.dropna(inplace=True)\ndata.describe()","7c80a4d8":"#converting to numpy array\ndf=data.values\nprint(df.shape)","b46139f0":"X=df[:,:-1]\ny=df[:,-1].astype(int)","2cee6908":"print(X.shape)\nprint(y.shape)\n","540d6fca":"#Normalising data\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler().fit(X)","3bbe3aef":"X_sc=sc.transform(X)\ndf=pd.DataFrame(X_sc)\ndf.describe()","c175b26d":"#import algorithms\nfrom sklearn import model_selection\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score","75e66f95":"#define scoring method\nscoring='accuracy'\n\n#define models to train\n\nnames=[\"K Nearest Neighbors\", \"Gaussian Process\", \"Decision Tree\", \"Random Forest\", \"Neural Network\",\"AdaBoost\", \"Naive Bayes\"\n      ,\"SVM Linear\",\"SVM RBF\", \"SVM Sigmoid\"]\n\nclassifiers=[KNeighborsClassifier(n_neighbors=3),GaussianProcessClassifier(1.0*RBF(1.0)), DecisionTreeClassifier(max_depth=5)\n    ,RandomForestClassifier(max_depth=5, n_estimators=50,max_features=1), MLPClassifier(alpha=1),AdaBoostClassifier(),GaussianNB(),\n            SVC(kernel='linear'),SVC(kernel='rbf'),SVC(kernel='sigmoid') ]\n\nmodels=zip(names,classifiers)\n\nresults=[]\nnames=[]\n\nfor name,model in models:\n    kfold=model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results=model_selection.cross_val_score(model,X_sc,y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg=\"{0} : {1}({2})\".format(name, cv_results.mean(), cv_results.std())\n    print(msg)","e34134e3":"from sklearn.model_selection import GridSearchCV, KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.wrappers.scikit_learn import KerasClassifier","fdb7ca9f":"def create_model():\n    model=Sequential()\n    model.add(Dense(8, input_dim=8, kernel_initializer='normal',activation='relu'))\n    model.add(Dense(4,input_dim=8, kernel_initializer='normal',activation='relu'))\n    model.add(Dense(1,input_dim=8,activation='sigmoid'))\n    \n    #compile the model\n    adam=Adam(lr=0.01)\n    model.compile(loss='binary_crossentropy',optimizer=adam,metrics=['accuracy'])\n    return model\n\nmodel=create_model()\nprint(model.summary())","15758688":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam\n\n# Define a random seed\nseed = 6\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(8, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(4, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # compile the model\n    adam = Adam(lr = 0.01)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, verbose = 1)\n\n# define the grid search parameters\nbatch_size = [10, 20, 40]\nepochs = [10, 50, 100]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(batch_size=batch_size, epochs=epochs)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed,n_splits=3), verbose = 10)\ngrid_results = grid.fit(X_sc, y)\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","e1b2569b":"# Do a grid search for learning rate and dropout rate\n# import necessary packages \nfrom keras.layers import Dropout\n\n# Define a random seed\nseed = 6\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model(learn_rate, dropout_rate):\n    # create model\n    model = Sequential()\n    model.add(Dense(8, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(4, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # compile the model\n    adam = Adam(lr = learn_rate)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, epochs = 100, batch_size = 20, verbose = 0)\n\n# define the grid search parameters\nlearn_rate = [0.001, 0.01, 0.1]\ndropout_rate = [0.0, 0.1, 0.2]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(learn_rate=learn_rate, dropout_rate=dropout_rate)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed, n_splits=3), verbose = 10)\ngrid_results = grid.fit(X_sc, y)\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","4a5af4f4":"# Do a grid search to optimize kernel initialization and activation functions\n# Start defining the model \ndef create_model(activation,init):\n    # create model\n    model = Sequential()\n    model.add(Dense(8, input_dim = 8, kernel_initializer=init, activation=activation))\n    \n    model.add(Dense(4, input_dim = 8, kernel_initializer=init, activation=activation))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    \n    # compile the model\n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, epochs = 100, batch_size = 20, verbose = 0)\n\n# define the grid search parameters\nactivation = ['softmax', 'relu', 'tanh', 'linear']\ninit = ['uniform', 'normal', 'zero']\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(activation=activation,init=init)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed, n_splits=3), verbose = 10)\ngrid_results = grid.fit(X_sc, y)\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","929b6b66":"# Do a grid search to find the optimal number of neurons in each hidden layer\n# Start defining the model \ndef create_model(neuron1, neuron2):\n    # create model\n    model = Sequential()\n    model.add(Dense(neuron1, input_dim = 8, kernel_initializer='uniform', activation='linear'))\n    \n    model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer='uniform', activation='linear'))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    \n    # compile the model\n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, epochs = 100, batch_size = 20, verbose = 0)\n\n# define the grid search parameters\nneuron1 = [4, 8, 16]\nneuron2 = [2, 4, 8]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(neuron1=neuron1, neuron2=neuron2)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed, n_splits=3),refit=True, verbose = 10)\ngrid_results = grid.fit(X_sc, y)\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","8ed5d7e2":"# generate predictions with optimal hyperparameters\ny_pred = grid.predict(X_sc)\n\nprint(y_pred.shape) #(392L, 1L)","7a4f5070":"# Generate a classification report\nfrom sklearn.metrics import classification_report, accuracy_score\n\nprint(accuracy_score(Y, y_pred))\nprint(classification_report(Y, y_pred))","56e5a11a":"Best: 0.783163272148, using {'learn_rate': 0.001, 'dropout_rate': 0.0}","ac3ff1f3":"Best: 0.7781757712364197, using {'batch_size': 20, 'epochs': 100}","6f9b5730":"Best: 0.793367353173, using {'activation': 'linear', 'init': 'uniform'}","e549e2fc":"0.7806122448979592\n             precision    recall  f1-score   support\n\n          0       0.81      0.89      0.84       262\n          1       0.71      0.57      0.63       130\n\navg \/ total       0.77      0.78      0.77       392","633ae14e":"K Nearest Neighbors : 0.7577564102564103(0.07983507922637882)\nGaussian Process : 0.7803205128205128(0.09586540068924587)\nDecision Tree : 0.755448717948718(0.07293353566494007)\nRandom Forest : 0.7803846153846153(0.09916449121642132)\nNeural Network : 0.7803205128205127(0.08495756859479006)\nAdaBoost : 0.7625(0.07488060296802303)\nNaive Bayes : 0.7752564102564102(0.0673090353956351)\nSVM Linear : 0.7803846153846153(0.09154609247802299)\nSVM RBF : 0.7678205128205129(0.09068045782106383)\nSVM Sigmoid : 0.727051282051282(0.07940069737056646)\n\nLet's go ahead with neural network and optimise it","c981d87f":"Best: 0.790816335198, using {'neuron1': 16, 'neuron2': 2}"}}