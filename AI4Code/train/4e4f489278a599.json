{"cell_type":{"1e5f6b24":"code","bb783c89":"code","1244b962":"code","649e0766":"code","5bdd73e2":"code","15fcd4a6":"code","0f79c989":"code","7740c396":"code","23f2c92e":"code","17dc6831":"code","100ae15f":"code","70c54fff":"code","7463585a":"code","20790742":"code","ed595619":"code","05296e71":"code","46fb268b":"code","c0c50296":"code","627fabe1":"code","af50486c":"code","0a38e71d":"code","2449ceae":"code","2c74e127":"code","39760aa9":"code","545c540a":"code","ecf00c20":"code","292e0be2":"code","c38e6f67":"code","619dc643":"code","8801bd65":"markdown","408623ca":"markdown","00d5caed":"markdown","3ca40d29":"markdown","dc7d7639":"markdown","242a39f3":"markdown","e3134f98":"markdown","4be2b1c2":"markdown"},"source":{"1e5f6b24":"# import relevant packages\nimport os\nimport numpy as np\nimport pandas as pd\nimport re\nimport datetime\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nimport seaborn as sns","bb783c89":"# Import data\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1244b962":"train_df=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\nprint(train_df.head(5))\nprint(\"\\n\")\nprint(test_df.head(5))","649e0766":"# Inspect data (type, missing values, ...)\ntrain_df.info()","5bdd73e2":"# TRAINING DATA\n\n# Rename class\ntrain_df[\"Pclass\"].replace(1, \"Upper\", inplace=True)\ntrain_df[\"Pclass\"].replace(2, \"Middle\", inplace=True)\ntrain_df[\"Pclass\"].replace(3, \"Lower\", inplace=True)\n\n# Replace missing age with median\ntrain_df[\"Age\"].fillna(np.nanmedian(train_df[\"Age\"]), inplace=True)\n\n# Create Age Bands\nbins = [0,10,20,30,40,50,60,70,80,100]\ntrain_df[\"Age_bin\"] = pd.cut(train_df['Age'], bins)\n\n# With family \ntrain_df[\"with_family\"] = (train_df[\"SibSp\"] + train_df[\"Parch\"])>0\n\n# Replace NA for embarked with \"S\"\ntrain_df[\"Embarked\"].fillna(\"S\", inplace=True)\n\n# Replace NA for Cabin with \"Unknown\"\ntrain_df[\"Cabin\"].fillna(\"Unknown\", inplace=True)\n\n# Extract Name Title\ntrain_df[\"Title\"] = train_df.Name.apply(lambda x: x.split(\",\")[1].split(\".\")[0].strip())\ntitle_list = [\"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Dr\", \"Rev\", \"Col\"]\ntrain_df.loc[~train_df[\"Title\"].isin(title_list), \"Title\"] = \"NA\"\n\n# Extract deck from Cabin\nfor i in range(0, len(train_df)):\n    train_df.at[i, \"Deck\"] = \" \".join(re.findall(\"[a-zA-Z]+\", train_df.at[i, \"Cabin\"]))\n\ntrain_df[\"Deck\"].replace(\"B B\", \"B\", inplace=True)\ntrain_df[\"Deck\"].replace(\"B B B\", \"B\", inplace=True)\ntrain_df[\"Deck\"].replace(\"B B B B\", \"B\", inplace=True)\ntrain_df[\"Deck\"].replace(\"C C\", \"C\", inplace=True)\ntrain_df[\"Deck\"].replace(\"D D\", \"D\", inplace=True)\ntrain_df[\"Deck\"].replace(\"C C C\", \"C\", inplace=True)\ntrain_df[\"Deck\"].replace(\"F G\", \"F\", inplace=True)\ntrain_df[\"Deck\"].replace(\"F E\", \"E\", inplace=True)\ntrain_df[\"Deck\"].replace(\"T\", \"Unknown\", inplace=True)","15fcd4a6":"# TEST DATA\n\n# Rename class\ntest_df[\"Pclass\"].replace(1, \"Upper\", inplace=True)\ntest_df[\"Pclass\"].replace(2, \"Middle\", inplace=True)\ntest_df[\"Pclass\"].replace(3, \"Lower\", inplace=True)\n\n# Replace missing age with median\ntest_df[\"Age\"].fillna(np.nanmedian(test_df[\"Age\"]), inplace=True)\n\n# Create Age Bands\nbins = [0,10,20,30,40,50,60,70,80,100]\ntest_df[\"Age_bin\"] = pd.cut(test_df['Age'], bins)\n\n# Replace missing fare with median\ntest_df[\"Fare\"].fillna(np.nanmedian(test_df[\"Fare\"]), inplace=True)\n\n# Replace NA for embarked with \"S\"\ntest_df[\"Embarked\"].fillna(\"S\", inplace=True)\n\n# Replace NA for Cabin with \"Unknown\"\ntest_df[\"Cabin\"].fillna(\"Unknown\", inplace=True)\n\n# With family\ntest_df[\"with_family\"] = (test_df[\"SibSp\"] + test_df[\"Parch\"])>0\n\n# Extract Name Title\ntest_df[\"Title\"] = test_df.Name.apply(lambda x: x.split(\",\")[1].split(\".\")[0].strip())\ntitle_list = [\"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Dr\", \"Rev\", \"Col\"]\ntest_df.loc[~test_df[\"Title\"].isin(title_list), \"Title\"] = \"NA\"\n\n# Extract deck from Cabin\nfor i in range(0, len(test_df)):\n    test_df.at[i, \"Deck\"] = \" \".join(re.findall(\"[a-zA-Z]+\", test_df.at[i, \"Cabin\"]))\n\ntest_df[\"Deck\"].replace(\"B B\", \"B\", inplace=True)\ntest_df[\"Deck\"].replace(\"B B B\", \"B\", inplace=True)\ntest_df[\"Deck\"].replace(\"B B B B\", \"B\", inplace=True)\ntest_df[\"Deck\"].replace(\"C C\", \"C\", inplace=True)\ntest_df[\"Deck\"].replace(\"E E\", \"E\", inplace=True)\ntest_df[\"Deck\"].replace(\"D D\", \"D\", inplace=True)\ntest_df[\"Deck\"].replace(\"C C C\", \"C\", inplace=True)\ntest_df[\"Deck\"].replace(\"F G\", \"F\", inplace=True)\ntest_df[\"Deck\"].replace(\"F E\", \"E\", inplace=True)\ntest_df[\"Deck\"].replace(\"T\", \"Unknown\", inplace=True)","0f79c989":"# detect attributes with missing values\n# -> no missing values left\nprint(train_df.info())\nprint(\"\\n\")\nprint(test_df.info())","7740c396":"# Pre-processing: Standardization and One-Hot Encoder\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ntrain_df_pre = train_df.drop(columns=[\"Name\", \"Ticket\", \"Cabin\"])\ntest_df_pre = test_df.drop(columns=[ \"Name\", \"Ticket\", \"Cabin\"])\n\nnum_attribs = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\ncat_attribs = [\"Pclass\", \"Embarked\", \"Deck\", \"Sex\", \"Title\", \"with_family\", \"Age_bin\"]\n\ncol_transformer = ColumnTransformer([\n    (\"num\", StandardScaler(), num_attribs),\n    (\"cat\", OneHotEncoder(), cat_attribs),\n    ],\n    remainder=\"passthrough\")\n\n# Fit transform TRAIN\ntrain_array_transformed = col_transformer.fit_transform(train_df_pre)\n\n# Convert numpy.ndarray to pd.DataFrame\ntrain_df_transformed = pd.DataFrame(data=train_array_transformed)\n#train_df_transformed = pd.DataFrame(data=train_array_transformed.toarray())\n\n# Rename columns\ncolumn_names = num_attribs + list(col_transformer.named_transformers_['cat'].get_feature_names()) + [\"PassengerId\"] + [\"Survived\"]\ntrain_df_transformed.columns = column_names\n\n\n# Fit transform TEST\ntest_array_transformed = col_transformer.fit_transform(test_df_pre)\n\n# Convert numpy.ndarray to pd.DataFrame\ntest_df_transformed = pd.DataFrame(data=test_array_transformed)\n\n# Rename columns\ncolumn_names = num_attribs + list(col_transformer.named_transformers_['cat'].get_feature_names()) + [\"PassengerId\"]\ntest_df_transformed.columns = column_names","23f2c92e":"# check columns\nprint(train_df_transformed.columns)\nprint(\"\\n\")\nprint(test_df_transformed.columns)","17dc6831":"# summary statistics\ntrain_df[[\"Survived\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]].describe()","100ae15f":"# Survived\nplt.figure(figsize=(5,5))\ncount_survived = train_df[\"Survived\"].value_counts()\ncount_survived .plot.bar()\nplt.title(\"Number of Survivors\", size=16)","70c54fff":"# Number per class\nplt.figure(figsize=(5,5))\ncount_class = train_df[\"Pclass\"].value_counts()\ncount_class.plot.bar()\nplt.title(\"Number per Class\", size=16)","7463585a":"# Number per Title\nplt.figure(figsize=(5,5))\ncount_title = train_df[\"Title\"].value_counts()\ncount_title.plot.bar()\nplt.title(\"Number per Title\", size=16)","20790742":"# Number of siblings or spouses\nplt.figure(figsize=(5,5))\ncount_sibsp = train_df[\"SibSp\"].value_counts()\ncount_sibsp.plot.bar()\nplt.title(\"Number of Siblings or Spouses\", size=16)","ed595619":"# Number per port\nplt.figure(figsize=(5,5))\ncount_port = train_df[\"Embarked\"].value_counts()\ncount_port.plot.bar()\nplt.title(\"Number per Port\", size=16)","05296e71":"# Number per deck\nplt.figure(figsize=(5,5))\ncount_deck = train_df[\"Deck\"].value_counts()\ncount_deck.plot.bar()\nplt.title(\"Number per Deck\", size=16)","46fb268b":"# Number per with_family\nplt.figure(figsize=(5,5))\ncount_with_family = train_df[\"with_family\"].value_counts()\ncount_with_family.plot.bar()\nplt.title(\"Number per with_family\", size=16)","c0c50296":"# correlation between all numeric attributes\nplt.figure(figsize=(8,8))\nmatrix = np.triu(train_df.drop(columns=[\"PassengerId\"]).corr())\nsns.heatmap(train_df.drop(columns=[\"PassengerId\"]).corr(), annot=True, fmt=\".1g\", \n            vmin=-1, vmax=1, center=0, \n            cmap=\"icefire\", square=True, mask=matrix)\nplt.title(\"Heatmap: Correlation of numeric features\", size=16)","627fabe1":"# Survival countplots all in one\nfig, axs = plt.subplots(2, 3, figsize=(20,10))\nsns.countplot(data=train_df, y=\"Survived\", hue=\"Pclass\", ax=axs.flatten()[0])\naxs.flatten()[0].legend(title=\"Pclass\", loc=4)\nsns.countplot(data=train_df, y=\"Survived\", hue=\"Sex\", ax=axs.flatten()[1])\naxs.flatten()[1].legend(title=\"Sex\", loc=4)\naxs.flatten()[1].set_ylabel('') \nsns.countplot(data=train_df, y=\"Survived\", hue=\"Deck\", ax=axs.flatten()[2])\naxs.flatten()[2].set_ylabel('') \naxs.flatten()[2].legend(title=\"Deck\", loc=4)\nsns.countplot(data=train_df, y=\"Survived\", hue=\"Parch\", ax=axs.flatten()[3])\naxs.flatten()[3].legend(title=\"Parch\", loc=4)\nsns.countplot(data=train_df, y=\"Survived\", hue=\"SibSp\", ax=axs.flatten()[4])\naxs.flatten()[4].set_ylabel('') \naxs.flatten()[4].legend(title=\"SibSp\", loc=4)\nsns.countplot(data=train_df, y=\"Survived\", hue=\"Embarked\", ax=axs.flatten()[5])\naxs.flatten()[5].set_ylabel('') \naxs.flatten()[5].legend(title=\"Embarked\", loc=4)\nfig.suptitle(\"Characteristics of Survivors vs Non-Survivors\", size=22)","af50486c":"# Age Histogram Test\n# doesnt work on Kaggle currently sns.histplot(data=test_df, x=\"Age\", binwidth=10, kde=True)\nsns.distplot(test_df[\"Age\"]) # alternative","0a38e71d":"# check features before training\n\nprint(train_df_transformed.columns)\nprint(\"\\n\")\nprint(test_df_transformed.columns)","2449ceae":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nX_train = train_df_transformed.drop(columns=[\"PassengerId\",\"Survived\"])\ny_train = train_df_transformed[\"Survived\"]\n\nX_test = test_df_transformed.drop(columns=[\"PassengerId\"])\n\n#  Logistic regression\nlr = LogisticRegression(max_iter=2000)\n\ncv_lr = cross_val_score(lr, X_train, y_train, cv=3)\nprint(cv_lr)\nprint(\"mean accuracy: \" + str(cv_lr.mean()))\n\nlr.fit(X_train, y_train)\n\nlr.score(X_train, y_train) # 0.82-0.83\n\ny_test = lr.predict(X_test) # 169\n\n# check number of survivors\nprint(\"Number of survivors predicted: \" + str(sum(y_test)))","2c74e127":"# Feature importance\nfeature_importance_lr = pd.DataFrame()\nfeature_importance_lr[\"Features\"] = X_train.columns\nfeature_importance_lr[\"lr_coeff\"] = np.transpose(lr.coef_)\nfeature_importance_lr[\"coef_abs\"] = np.transpose(np.abs(lr.coef_))\n\nplt.figure().set_size_inches(20, 6)\nfg3 = sns.barplot(x='Features', y='lr_coeff',data=feature_importance_lr, palette=\"Blues_d\")\nfg3.set_xticklabels(rotation=90, labels=feature_importance_lr.Features)\nplt.title(\"Logistic Regression Model: Regression Coefs\")","39760aa9":"# Tuned model using GridSearchCV\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nX_train = train_df_transformed.drop(columns=[\"PassengerId\",\"Survived\"])\ny_train = train_df_transformed[\"Survived\"]\n\nX_test = test_df_transformed.drop(columns=[\"PassengerId\"])\n\n#  Logistic regression\nlr = LogisticRegression()\n\n# Find optimal parameter settings using GridSearch\nparam_grid = {\"max_iter\" : [500,1000,2000],\n              \"penalty\" : [\"l1\", \"l2\"],\n              \"C\" : np.logspace(-4, 4, 20),\n              \"solver\" : [\"liblinear\"]}\n    \nlr_clf = GridSearchCV(lr, param_grid=param_grid, cv=3, verbose=True, n_jobs=-1)\n\nbest_lr_clf = lr_clf.fit(X_train, y_train)\nprint(\"Best Logistic Regression Score: \" + str(best_lr_clf.best_score_))\nprint(\"Best Parameter:  \" + str(best_lr_clf.best_params_))\n\n# Prediction\ny_test = lr_clf.predict(X_test)\n\n# check number of survivors\nprint(\"Number of survivors predicted: \" + str(sum(y_test))) # 166","545c540a":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nX_train = train_df_transformed.drop(columns=[\"PassengerId\",\"Survived\"])\ny_train = train_df_transformed[\"Survived\"]\n\nX_test = test_df_transformed.drop(columns=[\"PassengerId\"])\n\n# Random forrest classifier\nrnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n\ncv_rnd_clf = cross_val_score(rnd_clf, X_train, y_train, cv=5)\nprint(cv_rnd_clf)\nprint(\"mean accuracy: \" + str(cv_rnd_clf.mean()))\n\nrnd_clf.fit(X_train, y_train)\n\nrnd_clf.score(X_train, y_train) # 0.8092\n\ny_test = rnd_clf.predict(X_test)\n\n# check number of survivors\nprint(\"Number of survivors predicted: \" + str(sum(y_test))) # 157","ecf00c20":"# Feature Importance\nplt.figure().set_size_inches(15, 10)\nfeat_importances = pd.Series(rnd_clf.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","292e0be2":"# Random Forest tuned\n\n''''''\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nX_train = train_df_transformed.drop(columns=[\"PassengerId\",\"Survived\"])\ny_train = train_df_transformed[\"Survived\"]\n\nX_test = test_df_transformed.drop(columns=[\"PassengerId\"])\n\n# Random forrest classifier\nrf = RandomForestClassifier()\n\n# Find optimal parameter settings using Randomized Search\nparam_grid =  {'n_estimators': [200,300,400,600,800,1000,1500], \n               'bootstrap': [True,False],\n               'max_depth': [3,5,10,20,50,75,100,None],\n               'max_features': ['auto'],\n               'min_samples_leaf': [1,2,4,6,10],\n               'min_samples_split': [2,5,10,20]}\n\nrnd_clf = RandomizedSearchCV(rf, param_distributions=param_grid, n_iter=500, cv=3, verbose=True, n_jobs=-1)\n\nbest_rnd_clf = rnd_clf.fit(X_train, y_train)\nprint(\"Best Random Forest Score: \" + str(best_rnd_clf .best_score_))\nprint(\"Best Parameter:  \" + str(best_rnd_clf .best_params_))\n\nbest_rnd_clf.score(X_train, y_train) # 0.8361\n\n# Prediction\ny_test = best_rnd_clf.predict(X_test)\n\n# check number of survivors\nprint(\"Number of survivors predicted: \" + str(sum(y_test))) # 152","c38e6f67":"# Feature Importance\nplt.figure().set_size_inches(15, 10)\nbest_rf = best_rnd_clf.best_estimator_.fit(X_train, y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","619dc643":"# Export for submit (tuned model)\n\nexport_df = pd.DataFrame()\nexport_df[\"PassengerId\"] = test_df_transformed[\"PassengerId\"].astype(int)\nexport_df[\"Survived\"] = y_test.astype(int)\nnow = datetime.datetime.now()\nname_add = \"date_\"+str(now.year)+\"-\"+str(now.month)+\"-\"+str(now.day)+\"_time_\"+str(now.hour)+\"-\"+str(now.minute)\nexport_df.to_csv(f\"random_forest_tuned_{name_add}.csv\", index=False)","8801bd65":"## 3.2 Random Forest Classifier","408623ca":"# 2. Exploratory Data Analysis","00d5caed":"## 3.1 Logistic Regression","3ca40d29":"Performance of the **random forest classifier** in training and after submission to Kaggle\n\n| Model | mean accuracy training | Kaggle score\n|------|------|------|\n|   Random Forest (simple)  | 0.8092| 0.xxxx |\n|   Random Forest (tuned)  | 0.8361| 0.79425 |\n\nPerformance will be evaluated further later as well as compared to other models using more than just accuracy.","dc7d7639":"# 0. Intro\n\nThis notebook is a work-in-progress and will be continuously extended. Please keep that in mind. Anyways, I hope it helps you already!\n\nThe code right now brought me into the **Top 6%**, mainly using feature engineering and a simple hyperparameter-tuned random forest classifier.\nI'll add more text to the plots soon, too. Please be patient!\n\nMore about this project on GitHub:\nhttps:\/\/github.com\/JonasSchroeder\/kaggle_titanic\n\nUpcoming: Support Vector Machine Classifier and Boosted Random Forest Models. After that I might dip my toe into XGBoost water, let's see. Furthermore, I should work more on Precision-Recall and other visuals to further optimize my models. \n\nPlease check out my Medium post on TowardsDataScience if you're interested in my process and what I've learned from my first Kaggle \"competition\". You can find it here: https:\/\/towardsdatascience.com\/impressions-from-a-kaggle-noob-dd923e8024bf\n\nBest,\nJonas","242a39f3":"# 1. Data Transformation & Pre-processing","e3134f98":"Performance of the **logistic regression** in training and after submission to Kaggle\n\n| Model | mean accuracy training | Kaggle score\n|------|------|------|\n|   Logistic Regression (simple)  | 0.8215| 0.77272 |\n|   Logistic Regression (tuned)  | 0.8294| 0.77033 |\n\n\nPerformance will be evaluated further later as well as compared to other models using more than just accuracy.","4be2b1c2":"# 3. Model Training"}}