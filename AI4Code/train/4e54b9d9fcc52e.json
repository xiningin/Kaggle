{"cell_type":{"caf8b615":"code","7c4beaff":"code","5a01c8e0":"code","7a20888b":"code","f2337163":"code","a6e9e074":"code","ba750f52":"code","5f39dad9":"code","2f4b3bef":"code","341bbac0":"code","146036cd":"code","e8484f5f":"code","e0d72446":"code","d8d96c6d":"code","e9987df0":"code","ee575599":"code","53d03770":"code","764b4a18":"code","4e0175f3":"code","804c9046":"code","602fa7db":"code","9b8a3c72":"code","7a365c8d":"code","d51db538":"code","5f91ba01":"code","afb79925":"code","9feda7eb":"code","81d31bbb":"code","581a15ef":"code","a12d41e9":"code","fb8b6f3a":"code","649ad9a7":"code","745974a5":"code","6a76805b":"code","ae52fbb3":"code","a02e14ac":"code","64390371":"code","27d9d8c5":"code","415dd435":"markdown","bd6346bb":"markdown","fd0edbee":"markdown","f5237179":"markdown","c3f6d3e4":"markdown","b7bfced4":"markdown","1f46d749":"markdown","a067be7a":"markdown","bbaac7cf":"markdown","bb9c40d2":"markdown","9bfe97aa":"markdown","13d73f60":"markdown","7ad25d17":"markdown"},"source":{"caf8b615":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set(style='darkgrid')\nimport matplotlib.pyplot as plt","7c4beaff":"df = pd.read_csv('..\/input\/loanpredictionusingmachinelearningandpython\/train_u6lujuX_CVtuZ9i.csv')","5a01c8e0":"df.head()","7a20888b":"df['Gender'] = df['Gender'].fillna( df['Gender'].dropna().mode().values[0] )\ndf['Married'] = df['Married'].fillna( df['Married'].dropna().mode().values[0] )\ndf['Dependents'] = df['Dependents'].fillna( df['Dependents'].dropna().mode().values[0] )\ndf['Self_Employed'] = df['Self_Employed'].fillna( df['Self_Employed'].dropna().mode().values[0] )\ndf['LoanAmount'] = df['LoanAmount'].fillna( df['LoanAmount'].dropna().mean() )\ndf['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna( df['Loan_Amount_Term'].dropna().mode().values[0] )\ndf['Credit_History'] = df['Credit_History'].fillna( df['Credit_History'].dropna().mode().values[0] )\ndf['Dependents'] = df['Dependents'].str.rstrip('+')\ndf['Gender'] = df['Gender'].map({'Female':0,'Male':1}).astype(np.int)\ndf['Married'] = df['Married'].map({'No':0, 'Yes':1}).astype(np.int)\ndf['Education'] = df['Education'].map({'Not Graduate':0, 'Graduate':1}).astype(np.int)\ndf['Self_Employed'] = df['Self_Employed'].map({'No':0, 'Yes':1}).astype(np.int)\ndf['Loan_Status'] = df['Loan_Status'].map({'N':0, 'Y':1}).astype(np.int)\ndf['Dependents'] = df['Dependents'].astype(np.int)","f2337163":"df.dtypes","a6e9e074":"X,y  = df.iloc[:, 1:-1], df.iloc[:, -1]","ba750f52":"X= pd.get_dummies(X)","5f39dad9":"dtrain= pd.get_dummies(df)\nimport xgboost as xgb\nfrom sklearn import cross_validation, metrics\nfrom sklearn.grid_search import GridSearchCV \ntrain = dtrain\ntarget = 'Loan_Status'\nIDcol = 'Loan_ID'","2f4b3bef":"def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'],\n                          nfold=cv_folds,metrics='auc', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain['Loan_Status'],eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Loan_Status'].values, dtrain_predictions))\n    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Loan_Status'], dtrain_predprob))\n                    \n    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')","341bbac0":"#Choose all predictors except target & IDcols\npredictors = [x for x in train.columns if x not in [target, IDcol]]\nxgb2 = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8,\n                     colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\nmodelfit(xgb2, train, predictors)","146036cd":"param_test1 = { 'max_depth':list(range(3,10,2)), 'min_child_weight':list(range(1,6,2))}\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n                                                  min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n                                                  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n                        param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch1.fit(train[predictors],train[target])\ngsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_","e8484f5f":"param_test2 = { 'max_depth':[8,9,10], 'min_child_weight':[4,5,6] }\ngsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch2.fit(train[predictors],train[target])\ngsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_","e0d72446":"param_test2b = {\n 'min_child_weight':[6,8,10,12]\n}\ngsearch2b = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=8,\n min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test2b, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch2b.fit(train[predictors],train[target])\ngsearch2b.grid_scores_, gsearch2b.best_params_, gsearch2b.best_score_","d8d96c6d":"param_test2c = {\n 'min_child_weight':[6,7]\n}\ngsearch2c = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=8,\n min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test2b, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch2c.fit(train[predictors],train[target])\ngsearch2c.grid_scores_, gsearch2c.best_params_, gsearch2c.best_score_","e9987df0":"param_test3 = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\ngsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=8,\n min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch3.fit(train[predictors],train[target])\ngsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_","ee575599":"predictors = [x for x in train.columns if x not in [target, IDcol]]\nxgb2 = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=8, min_child_weight=6, gamma=0.1, subsample=0.9,\n                     colsample_bytree=0.95,reg_alpha=2, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\nmodelfit(xgb2, train, predictors)","53d03770":"param_test4 = {\n 'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]\n}\ngsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=8,\n min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch4.fit(train[predictors],train[target])\ngsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_","764b4a18":"param_test4a = {\n 'subsample':[i\/10.0 for i in range(9,12)],\n 'colsample_bytree':[i\/10.0 for i in range(9,12)]\n}\ngsearch4a = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=8,\n min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch4.fit(train[predictors],train[target])\ngsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_","4e0175f3":"param_test5 = {\n 'subsample':[i\/100.0 for i in range(95,105,5)],\n 'colsample_bytree':[i\/100.0 for i in range(95,105,5)]\n}\ngsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=8,\n min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch5.fit(train[predictors],train[target])\ngsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_","804c9046":"param_test6 = {\n 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\ngsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=8,\n min_child_weight=6, gamma=0.1, subsample=0.95, colsample_bytree=0.95,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test6, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch6.fit(train[predictors],train[target])\ngsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_","602fa7db":"param_test6a = {\n 'reg_alpha':[1,2,3,4,5]\n}\ngsearch6a = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=8,\n min_child_weight=6, gamma=0.1, subsample=0.95, colsample_bytree=0.95,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test6a, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch6a.fit(train[predictors],train[target])\ngsearch6a.grid_scores_, gsearch6a.best_params_, gsearch6a.best_score_","9b8a3c72":"\nxgb3 = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=8, min_child_weight=6, gamma=0.1, subsample=0.95,\n                     colsample_bytree=0.95, reg_alpha=2, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\nmodelfit(xgb3, train, predictors)","7a365c8d":"from sklearn.preprocessing import StandardScaler\nslc= StandardScaler()\nX_train_std = slc.fit_transform(X)","d51db538":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators =400, criterion='entropy', oob_score=True, random_state=1,n_jobs=-1)","5f91ba01":"from xgboost.sklearn import XGBClassifier\n#xgb1 = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=8, min_child_weight=6, gamma=0.1, subsample=0.95,\n                     #colsample_bytree=0.95, reg_alpha=2, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\nxgb1=XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=8, min_child_weight=6, gamma=0.1, subsample=0.9,\n                     colsample_bytree=0.95,reg_alpha=2, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)","afb79925":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(criterion='entropy',max_depth=1)\nada = AdaBoostClassifier(base_estimator=tree, n_estimators=500, learning_rate=0.1, random_state=0)","9feda7eb":"from sklearn.ensemble import VotingClassifier\n\neclf = VotingClassifier(estimators=[('forest', forest), ('xgb', xgb1), ('adaboost', ada)], voting='hard')","81d31bbb":"eclf.fit(X_train_std, y)","581a15ef":"dtest = pd.read_csv('test_Y3wMUE5_7gLdaTN.csv')","a12d41e9":"dtest['Gender'] = dtest['Gender'].map({'Female':0,'Male':1})\ndtest['Married'] = dtest['Married'].map({'No':0, 'Yes':1}).astype(np.int)\ndtest['Education'] = dtest['Education'].map({'Not Graduate':0, 'Graduate':1}).astype(np.int)\ndtest['Self_Employed'] = dtest['Self_Employed'].map({'No':0, 'Yes':1})\ndtest['Dependents'] = dtest['Dependents'].str.rstrip('+')\ndtest['Gender'] = dtest['Gender'].fillna( dtest['Gender'].dropna().mode().values[0]).astype(np.int)\ndtest['Dependents'] = dtest['Dependents'].fillna( dtest['Dependents'].dropna().mode().values[0]).astype(np.int)\ndtest['Self_Employed'] = dtest['Self_Employed'].fillna( dtest['Self_Employed'].dropna().mode().values[0])\ndtest['LoanAmount'] = dtest['LoanAmount'].fillna( dtest['LoanAmount'].dropna().mode().values[0])\ndtest['Loan_Amount_Term'] = dtest['Loan_Amount_Term'].fillna( dtest['Loan_Amount_Term'].dropna().mode().values[0])\ndtest['Credit_History'] = dtest['Credit_History'].fillna( dtest['Credit_History'].dropna().mode().values[0] )","fb8b6f3a":"\nX_test = dtest.iloc[:,1:]","649ad9a7":"X_test= pd.get_dummies(X_test)","745974a5":"X_test_std = slc.transform(X_test)","6a76805b":"\ny_test_pred = eclf.predict(X_test_std)","ae52fbb3":"dtest['Loan_Status'] = y_test_pred\ndf_final = dtest.drop(['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area'], axis=1)","a02e14ac":"df_final['Loan_Status'] = df_final['Loan_Status'].map({0:'N', 1:'Y'})","64390371":"df_final.to_csv('my_submission7.csv', index=False)","27d9d8c5":"df_final.head()","415dd435":"# I defined a function which will help us to create xgboost models and perform cross validation","bd6346bb":"# Similar approach is done on the competition given test data.","fd0edbee":"# Voting Classifier\nI used VotingClassifier which will fit clones of those original estimators.","f5237179":"Well some of the columns has the missing values. So I filled them mode of the respective columns.\nFurther there were several categorize labels, which were actually 'object' type.\nI converted 'object' dtype to 'int' type manually using map()","c3f6d3e4":"# Tuning reg_alpha","b7bfced4":"# Applying machine learning models\n Using RandomForestClassifier with the parameters","1f46d749":"Features on larger scales can unduly influence the model. We want features on a similar scale. Scikit's preprocessing provides us with StandardScaler package to scale our data.","a067be7a":"# Tuning Gamma","bbaac7cf":" Predicting on the unseen dataset(test) using the voting classifier model","bb9c40d2":"Using the above function of different XGBoost models and with grid search-cross validation, I found the effective value of each of the parameters","9bfe97aa":"# Tuning subsample and colsample_bytree","13d73f60":"# Parameter Tuning For XGBoost model\nParameter tuning like learning rate, some tree specific parameters like max_depth, min_child_weight, gamma, subsample, colsample_bytree.\nKeeping all the parameters initially fixed, with the use of grid search :\nTuning max_depth and min_child_weight","7ad25d17":"Dividing the given train dataset in the form of features and target variables and further making use of get_dummies from pandas library"}}