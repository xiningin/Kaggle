{"cell_type":{"8b868bb2":"code","52fc2c22":"code","7ae9ed1f":"code","4ca3ede1":"code","b793780d":"code","aecbbff5":"code","f809471d":"code","d286bf7c":"code","bf54caa7":"code","28e9a760":"code","a4a6d4ce":"code","032b9365":"code","c68df175":"code","87bc0ee1":"code","42c3c777":"code","c3a1d01c":"code","b51a176c":"code","b2e0a92a":"code","2fade88e":"code","6102bb9a":"code","965c829b":"code","8b8a5487":"code","fceb106a":"code","af08fa8b":"code","0b7f7979":"code","bb293bae":"code","da5ddea4":"code","ffd71a45":"code","6a54bd8b":"code","e53bca50":"code","7a40e925":"code","b7ffb065":"code","bab91338":"code","a1f80f46":"code","960731cf":"code","b32f2389":"code","6c353ea9":"code","319878a0":"code","01347a0f":"code","09baed16":"code","60812dc5":"code","8e87f271":"code","1ec425ad":"code","6834edae":"code","ad6bd0ef":"code","303ad005":"code","8ed3fcc8":"code","60fc66c3":"code","6bfb9615":"code","dbf43142":"code","f55e5679":"code","d3b4dd83":"code","60b4fcf8":"code","db6ea4c1":"code","9502a00f":"code","6136626c":"code","83d18bb2":"code","da8fceae":"code","d3b759ff":"code","606eb0fe":"code","504fb6bd":"code","0e098310":"code","b7b1ea78":"code","9f9af125":"code","2c06e140":"code","eedef12b":"code","a62613af":"code","08b51d6f":"code","55aa8d09":"code","2938683f":"code","5ceae8c2":"code","34f01b88":"markdown","d6070931":"markdown","6d1ed188":"markdown","c4a9f674":"markdown"},"source":{"8b868bb2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","52fc2c22":"!pip install pydotplus","7ae9ed1f":"store = pd.read_csv(\"\/kaggle\/input\/rossmann-store-sales\/store.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/rossmann-store-sales\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/rossmann-store-sales\/test.csv\",parse_dates=[3])","4ca3ede1":"train.shape","b793780d":"train.head()","aecbbff5":"test.shape","f809471d":"test.head()","d286bf7c":"store.head()","bf54caa7":"train.info()","28e9a760":"train.describe()","a4a6d4ce":"train.describe()[['Sales','Customers']].loc['min']","032b9365":"train.describe()[['Sales','Customers']].loc['max']","c68df175":"#  no. of stores\ntrain.Store.nunique()","87bc0ee1":"train['Store'].value_counts().head(50).plot.bar()","42c3c777":"train.DayOfWeek.value_counts()","c3a1d01c":"train.Open.value_counts()","b51a176c":"train.isna().sum()","b2e0a92a":"test.isnull().sum()","2fade88e":"#date wise line plot for sales\ntrain['Date'] = pd.to_datetime(train['Date'],format = '%Y-%m-%d')\nstore_id = train.Store.unique()[0]\nprint(store_id)\nstore_rows = train[train['Store'] == store_id]\nprint(store_rows.shape)\nstore_rows.resample('1D',on = 'Date')['Sales'].sum().plot.line(figsize = (18,8))","6102bb9a":"#missing values on days\nstore_rows[store_rows['Sales']==0]","965c829b":"# checking the same for test data\ntest['Date'] = pd.to_datetime(test['Date'],format = '%Y-%m-%d')\nstore_test_rows = test[test['Store'] == store_id]\nprint(store_test_rows.shape)\nstore_test_rows['Date'].min(), store_test_rows['Date'].max()","8b8a5487":"store_rows['Sales'].plot.hist(figsize = (14,8))","fceb106a":"## Store data\nstore.isnull().sum()","af08fa8b":"store.head()","0b7f7979":"store[store['Store']==store_id].T # here store id was 1","bb293bae":"# checking the non null values in store data to make sure what we can fill in the missing values\nstore[~store['Promo2SinceYear'].isna()].iloc[0]","da5ddea4":"#method 1\nstore['Promo2SinceWeek'].fillna(0,inplace = True)\nstore['Promo2SinceYear'].fillna(store['Promo2SinceYear'].mode()[0],inplace = True)\nstore['PromoInterval'].fillna(store['PromoInterval'].mode()[0],inplace = True)","ffd71a45":"store['CompetitionOpenSinceMonth'].fillna(store['CompetitionOpenSinceMonth'].mode()[0],inplace = True)\nstore['CompetitionDistance'].fillna(store['CompetitionDistance'].max(),inplace = True)\nstore['CompetitionOpenSinceYear'].fillna(store['CompetitionOpenSinceYear'].mode()[0],inplace = True)\nstore.isnull().sum()","6a54bd8b":"# merge the data train and store\ndata_merged = train.merge(store,on = 'Store',how = 'left')\nprint(train.shape)\nprint(data_merged.shape)\nprint(data_merged.isnull().sum().sum()) # cross check if there are any missing values","e53bca50":"## Encoding\n# 3 categorical columns, 1 date column, rest are numerical\ndata_merged.dtypes","7a40e925":"data_merged['day'] = data_merged['Date'].dt.day\ndata_merged['month'] = data_merged['Date'].dt.month\ndata_merged['year'] = data_merged['Date'].dt.year\n#data_merged['weekday'] = data_merged['Date'].dt.strftime(%a)  This is already in data","b7ffb065":"# stateHoliday, StoreType, Assortment, PromoInterval\ndata_merged['StateHoliday'].unique()","bab91338":"data_merged['StateHoliday'] = data_merged['StateHoliday'].map({'a':1,'b':2,'c':3,'0':0,0:0})\ndata_merged['StateHoliday'] = data_merged['StateHoliday'].astype(int)","a1f80f46":"pd.set_option('display.max_columns',None)\ndata_merged.head()","960731cf":"data_merged['Assortment'] = data_merged['Assortment'].map({'a':1,'b':2,'c':3})\ndata_merged['Assortment'] = data_merged['Assortment'].astype(int)","b32f2389":"data_merged['StoreType'] = data_merged['StoreType'].map({'a':1,'b':2,'c':3,'d':4})\ndata_merged['StoreType'] = data_merged['StoreType'].astype(int)","6c353ea9":"map_promo = {'Jan,Apr,Jul,Oct':1,'Feb,May,Aug,Nov':2,'Mar,Jun,Sept,Dec':3}\ndata_merged['PromoInterval'] = data_merged['PromoInterval'].map(map_promo)","319878a0":"from sklearn.model_selection import train_test_split","01347a0f":"features = data_merged.columns.drop(['Sales','Date'])\nX = data_merged[features]\ny = np.log(data_merged['Sales']+1)\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n","09baed16":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error","60812dc5":"model_dt  = DecisionTreeRegressor(max_depth = 20, random_state = 42).fit(X_train,y_train)\ny_pred = model_dt.predict(X_test)","8e87f271":"r2_score(y_test,y_pred)","1ec425ad":"mean_squared_error(y_test,y_pred)","6834edae":"np.sqrt(mean_squared_error(y_test,y_pred))","ad6bd0ef":"def draw_tree(model, columns):\n    import pydotplus\n    from sklearn.externals.six import StringIO\n    from IPython.display import Image\n    import os\n    from sklearn import tree\n    \n    graphviz_path = 'C:\\Program Files (x86)\\Graphviz2.38\/bin\/'\n    os.environ[\"PATH\"] += os.pathsep + graphviz_path\n\n    dot_data = StringIO()\n    tree.export_graphviz(model,\n                         out_file=dot_data,\n                         feature_names=columns)\n    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n    return Image(graph.create_png())","303ad005":"# draw_tree(model_dt,X.columns)y","8ed3fcc8":"def ToWeight(y):\n    w = np.zeros(y.shape, dtype=float)\n    ind = y != 0\n    w[ind] = 1.\/(y[ind]**2)\n    return w\n\ndef rmspe(y, yhat):\n    w = ToWeight(y)\n    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n    return rmspe","60fc66c3":"y_inv = np.exp(y_test)-1\ny_pred_inv = np.exp(y_pred)-1\nnp.sqrt(mean_squared_error(y_inv,y_pred_inv))","6bfb9615":"rmspe(y_inv,y_pred_inv)","dbf43142":"test.head()","f55e5679":"# import matplotlib.pyplot as plt\n# plt.figure(figsize = (18,8))\n# plt.bar(X,model_dt.feature_importances_)","d3b4dd83":"train_avg_cust = train.groupby(['Store'])[['Customers']].mean().reset_index().astype(int)\ntest_1 = test.merge(train_avg_cust,on = 'Store',how = 'left')\ntest.shape,test_1.shape","60b4fcf8":"test_1.head()","db6ea4c1":"test_merged = test_1.merge(store,on = 'Store',how = 'inner')\ntest_merged['Open'] = test_merged['Open'].fillna(1)\ntest_merged['Date'] = pd.to_datetime(test_merged['Date'],format = '%Y-%m-%d')\ntest_merged['day'] = test_merged['Date'].dt.day\ntest_merged['month'] = test_merged['Date'].dt.month\ntest_merged['year'] = test_merged['Date'].dt.year\ntest_merged['StateHoliday'] = test_merged['StateHoliday'].map({'0':0,'a':1})\ntest_merged['StateHoliday'] = test_merged['StateHoliday'].astype(int)\ntest_merged['Assortment'] = test_merged['Assortment'].map({'a':1,'b':2,'c':3})\ntest_merged['Assortment'] = test_merged['Assortment'].astype(int)\ntest_merged['StoreType'] = test_merged['StoreType'].map({'a':1,'b':2,'c':3,'d':4})\ntest_merged['StoreType'] = test_merged['StoreType'].astype(int)\nmap_promo = {'Jan,Apr,Jul,Oct':1,'Feb,May,Aug,Nov':2,'Mar,Jun,Sept,Dec':3}\ntest_merged['PromoInterval'] = test_merged['PromoInterval'].map(map_promo)\n\n","9502a00f":"test_pred = model_dt.predict(test_merged[features])\ntest_pred_inv = np.exp(test_pred)-1\n","6136626c":"'''\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n'''","83d18bb2":"# rf = RandomForestRegressor(n_jobs = -1)\n# param_grid = { \n#         \"n_estimators\"      : [10,50,100],\n#         \"max_features\"      : [\"auto\", \"sqrt\", \"log2\"],\n#         \"min_samples_split\" : [2,4],\n#         \"bootstrap\": [True, False],\n#         \"max_depth\" : [5,10,20]\n#         }\n\n# grid = GridSearchCV(estimator = rf,param_grid = param_grid, cv=3)\n\n# grid.fit(X_train, y_train)\n\n# grid.best_score_ , grid.best_params_\n","da8fceae":"## Hyperparameter Tuning\n\n'''\ndef get_rmspe_score(input_values,y_actual):\n    y_predicted = model.predict(input_values)\n    y_actual = np.exp(y_actual)-1\n    y_predicted = np.exp(y_predicted)-1\n    score = rmspe(y_actual,y_predicted)\n\n\nparams = {'max_depth': list(range(5,40))}\nbase_model = DecisionTreeRegressor()\ncv_model = GridSearchCV(base_model,param_grid = params,cv = 5,return_train_score=True,scoring = get_rmspe_score).fit(X_train,y_train)\n\npd.DataFrame(cv_model.cv_results)\n'''","d3b759ff":"# cv_model.best_params_","606eb0fe":"# df_cv_results = pd.DataFrame(cv_model.cv_results_)","504fb6bd":"# df_cv_results","0e098310":"# df_cv_results[df_cv_results['param_max_depth']==11].T","b7b1ea78":"# import matplotlib.pyplot as plt\n# df_cv_results = pd.DataFrame(cv_model.cv_results_).sort_values(by='mean_test_score',ascending=False)\n# df_cv_results.set_index('param_max_depth')['mean_test_score'].plot.line()\n# df_cv_results.set_index('param_max_depth')['mean_train_score'].plot.line()\n# plt.show()","9f9af125":"# rf = RandomForestRegressor()\n# rf.fit(X_train,y_train)\n# y_pred = rf.predict(X_test)\n# y_inv = np.exp(y_test)-1\n# y_pred_inv = np.exp(y_pred)-1\n# np.sqrt(mean_squared_error(y_inv,y_pred_inv))\n# test_pred = rf.predict(test_merged[features])\n# test_pred_inv = np.exp(test_pred)-1\n","2c06e140":"features = data_merged.columns.drop(['Sales','Customers','Date'])\nX = data_merged[features]\ny = np.log(data_merged['Sales']+1)\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n","eedef12b":"model_dt  = DecisionTreeRegressor(max_depth = 12, random_state = 1).fit(X_train,y_train)\ny_pred = model_dt.predict(X_test)","a62613af":"y_inv = np.exp(y_test)-1\ny_pred_inv = np.exp(y_pred)-1\nnp.sqrt(mean_squared_error(y_inv,y_pred_inv))","08b51d6f":"rmspe(y_inv,y_pred_inv)","55aa8d09":"test_pred = model_dt.predict(test_merged[features])\ntest_pred_inv = np.exp(test_pred)-1","2938683f":"submission_predicted = pd.DataFrame({'Id' : test['Id'],'Sales':test_pred_inv })\nsubmission_predicted.head()","5ceae8c2":"submission_predicted.to_csv('submission.csv',index = False)","34f01b88":"## taking the test data","d6070931":"### Missing values treatment","6d1ed188":"### Train and Validate split just to check the accuracy we can get after final model we will make we will use test data","c4a9f674":"## Model Building Decision Trees"}}