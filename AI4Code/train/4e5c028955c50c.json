{"cell_type":{"8bd5a9e9":"code","50e7f173":"code","6495c8b8":"code","855e7277":"code","43092cff":"code","5971a838":"code","6894b4df":"code","348fe490":"code","de898915":"code","bbf2bc8a":"code","f188d702":"code","e101a9ed":"code","14a5d39e":"code","3adf8e6d":"code","ee8f40ae":"code","8a42dfd7":"code","965d2f67":"code","d7f70146":"code","ce4e381d":"code","e5be8933":"markdown","3372f278":"markdown","21a22c2f":"markdown","5d17a4f3":"markdown","8eff40a2":"markdown","6727d095":"markdown","5e729473":"markdown"},"source":{"8bd5a9e9":"# Importando os m\u00f3dulos das bibliotecas de Data Science\n\nimport sys\nimport IPython \nfrom IPython import display\n\nimport numpy as np\nimport pandas as pd      \n\nimport sklearn as sk\nimport scipy as sp\n\nimport matplotlib as plt   \nimport seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n\n# Faz com que os relat\u00f3rios (plots) sejam apresentados em uma c\u00e9lula e n\u00e3o em uma nova janela\n%matplotlib inline       ","50e7f173":"# Checando as vers\u00f5es para acompanhamento de atualiza\u00e7\u00f5es\n\nprint(\"Python version: {}\". format(sys.version))\nprint(\"NumPy version: {}\". format(np.__version__))\nprint(\"pandas version: {}\". format(pd.__version__))\nprint(\"matplotlib version: {}\". format(plt.__version__))\nprint(\"SciPy version: {}\". format(sp.__version__)) \nprint(\"scikit-learn version: {}\". format(sk.__version__))\nprint(\"Seaborn version: {}\". format(sns.__version__)) \nprint(\"IPython version: {}\". format(IPython.__version__)) ","6495c8b8":"# Prepara os dados - importando os datasets\n\ndsTrain = pd.read_csv('..\/input\/dataset_treino.csv', \n                      names=['id', 'num_gestacoes', 'glicose', 'pressao_sanguinea', 'grossura_pele', 'insulina', 'bmi', 'indice_historico', 'idade', 'classe'], \n                      sep=',', header=0)\n\ndsTest  = pd.read_csv('..\/input\/dataset_teste.csv',  \n                      names=['id', 'num_gestacoes', 'glicose', 'pressao_sanguinea', 'grossura_pele', 'insulina', 'bmi', 'indice_historico', 'idade'], \n                      sep=',', header=0)\n\n# Verifica a importa\u00e7\u00e3o dos dados de treino\nprint(dsTrain.count())\nprint('\\n')\n# Verifica a importa\u00e7\u00e3o dos dados de teste\nprint(dsTest.count())","855e7277":"# Verifica valores nulos\n\ndsTrain.isnull().sum()","43092cff":"dsTrain.head()","5971a838":"dsTrain.info()","6894b4df":"# Checando a vari\u00e1vel Preditora\nsns.countplot(x='classe',data=dsTrain)","348fe490":"# Checando as vari\u00e1veis Independentes\ncolumns = ['num_gestacoes', 'glicose', 'pressao_sanguinea', 'grossura_pele', 'insulina', 'bmi', 'indice_historico', 'idade']\nsns.pairplot(data=dsTrain[columns])","de898915":"# Checando as var\u00edaveis - Sumarizado Estat\u00edstico\ndsTrain.describe()","bbf2bc8a":"# Criando a fun\u00e7\u00e3o para Correla\u00e7\u00e3o\nimport matplotlib.pyplot as plt   \n\ndef fnCcorrelation_heatmap(data):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        data.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )","f188d702":"# An\u00e1lise de Correla\u00e7\u00e3o de Pearson das vari\u00e1veis\n\ncolumns = ['num_gestacoes', 'glicose', 'pressao_sanguinea', 'grossura_pele', 'insulina', 'bmi', 'indice_historico', 'idade']\nfnCcorrelation_heatmap(dsTrain[columns])\ndsTrain[columns].corr().apply(lambda x: x.sort_values(ascending=False).values)","e101a9ed":"# Criando c\u00f3pias dos datasets para manipula\u00e7\u00e3o e manter os datasets originais\n\nmTrain = dsTrain.copy()\nmTest = dsTest.copy()","14a5d39e":"# Defini\u00e7\u00e3o das classes de vari\u00e1veis aplicaveis aos modelos\ncolumns = ['num_gestacoes', 'glicose', 'pressao_sanguinea', 'grossura_pele', 'insulina', 'bmi', 'indice_historico', 'idade']\n\n# X_train - define os dados independentes de treino\nX_train = mTrain[columns]\n\n# y_train - define a vari\u00e1vel preditora\ny_train = mTrain['classe']\n\n# X_test - define os dados independentes de teste\nX_test = mTest[columns]\n\n# y_test - define a vari\u00e1vel preditora\ny_test = mTrain['id']","3adf8e6d":"# Standardization, or mean removal and variance scaling\u00b6\nfrom sklearn import preprocessing\n\nX_scaled = preprocessing.scale(X_train)\nX_scaled","ee8f40ae":"# Verifica\u00e7\u00e3o das vari\u00e1veis independentes e sua classifica\u00e7\u00e3o\n\nfrom sklearn.model_selection import KFold, StratifiedShuffleSplit, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifierCV\nfrom xgboost import XGBClassifier\n\n#clf = XGBClassifier(n_estimators=50, max_features='sqrt')\n#predictions = clf.fit(X_train, y_train)\n\nclf = DecisionTreeClassifier()\nclassifier = clf.fit(X_train, y_train)\n\n#clf = BernoulliNB()\n#classifier = clf.fit(X_train, y_train)\n\nfeatures = pd.DataFrame()\nfeatures['feature'] = X_train.columns\nfeatures['importance'] = classifier.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(25, 25))","8a42dfd7":"# Fun\u00e7\u00e3o para classifica\u00e7\u00e3o\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Metrics\nfrom sklearn.metrics import accuracy_score, log_loss\n\n#Splits\nfrom sklearn.model_selection import KFold, ShuffleSplit, StratifiedShuffleSplit, StratifiedKFold\n\n# Compara\u00e7\u00e3o dos classificadores\nclassifiers = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\ndef fnClassifier(splits, target, features):\n    cols = [\"Classifier\", \"Accuracy\"]\n    acc_dict = {}\n    log = pd.DataFrame(columns=cols)\n\n    X = features.values\n    y = target.values\n\n    #_split = KFold(n_splits=splits, random_state=42, shuffle=True)\n    #_split = model_selection.ShuffleSplit(n_splits = splits, test_size = .3, train_size = .6, random_state = 0 )\n    _split = StratifiedShuffleSplit(n_splits= splits, test_size=0.1, random_state=0)\n\n    for train_index, test_index in _split.split(X, y):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        for clf in classifiers:\n            name = clf.__class__.__name__\n            clf.fit(X_train, y_train)\n            predictions = clf.predict(X_test)\n            acc = accuracy_score(y_test, predictions)\n            if name in acc_dict:\n                acc_dict[name] += acc\n            else:\n                acc_dict[name] = acc\n                \n    for clf in acc_dict:\n        acc_dict[clf] = acc_dict[clf] \/ 10.0\n        log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=cols)\n        log = log.append(log_entry)\n    \n    # Plot Classifier Accuracy\n    sns.set(style=\"darkgrid\")\n    sns.barplot(x='Accuracy', y='Classifier', data=log)\n    \n    return log.groupby(['Classifier', 'Accuracy']).count().sort_values(by=['Accuracy'], ascending=False)","965d2f67":"# Classifier(splits, target, features)\n\nclassifier = fnClassifier(12, y_train, X_train)\nprint(classifier.iloc[0])\nclassifier","d7f70146":"# Cria a matriz de Confusao: Confusion Matrix\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import datasets, linear_model, tree\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n\nprint('Confusion Matrix')\n#lm = tree.DecisionTreeClassifier()\n#lm = naive_bayes.BernoulliNB()\n#lm = ensemble.RandomForestClassifier()\nlm = discriminant_analysis.LinearDiscriminantAnalysis()\n\nmodel = lm.fit(X_scaled, y_train)\npredictions = lm.predict(X_test)\n\nmatrix = cross_val_predict(lm, X_scaled, y_train, cv=10)\npd.DataFrame(confusion_matrix(y_train, matrix), columns=['True', 'False'], index=['True', 'False'])","ce4e381d":"## output(output, test, name)\n\ndf_output = pd.DataFrame()\ndf_output['id'] = mTest['id'].astype(int)\ndf_output['classe'] = pd.DataFrame(predictions)\n\nfilename = '..\/output\/submission.csv'\ndf_output[['id', 'classe']].to_csv(filename, sep=',', encoding='utf-8', index=False)  \nprint('ok - arquivo gerado: ' + filename)","e5be8933":"# Data Science Academy - Previs\u00e3o de Ocorr\u00eancias de Do\u00ean\u00e7as - 1\u00ba Semana\n## Ricardo Galiardi - rgaliardi@gmail.com","3372f278":"## Competi\u00e7\u00e3o DSA de Machine Learning\n### Competi\u00e7\u00e3o DSA de Machine Learning -  Edi\u00e7\u00e3o Janeiro\/2019","21a22c2f":"## Descri\u00e7\u00e3o\n    O conjunto de dados \u00e9 do Instituto Nacional de Diabetes e Doen\u00e7as Digestivas e Renais \n    (National Institute of Diabetes and Digestive and Kidney Diseases). O objetivo \u00e9 prever \n    com base em medidas de diagn\u00f3stico, se um paciente tem diabetes. V\u00e1rias restri\u00e7\u00f5es foram \n    colocadas na sele\u00e7\u00e3o dessas inst\u00e2ncias de um banco de dados maior. Em particular, todos \n    os pacientes aqui s\u00e3o do sexo feminino com pelo menos 21 anos de idade.","5d17a4f3":"## Gerando o Resultado","8eff40a2":"## Analisando e Definindo os Modelos","6727d095":"## Classifica\u00e7\u00e3o por Compara\u00e7\u00e3o","5e729473":"## Analisando os dados "}}