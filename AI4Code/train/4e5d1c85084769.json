{"cell_type":{"d8b3b485":"code","927d8e59":"code","ae7c70f6":"code","d2eed7ff":"code","be9a7464":"code","a2466cb8":"code","365eedc6":"code","0013fcd6":"code","f42130fb":"code","db92a35d":"code","88e8d453":"code","37ba3909":"code","78d6b031":"code","4b67fe78":"code","7dcb1deb":"code","e9551d74":"code","8e774f94":"code","38a50b6c":"code","ca1e1246":"code","d4f94962":"code","f56de1be":"code","a8679312":"code","5efb3d20":"code","3f16aee8":"code","88f0f4e8":"code","d12f0782":"code","6de0ad7d":"code","bb86276d":"code","a1ed4d69":"code","aba78b10":"code","7d43095e":"code","6637536f":"code","66ef9bec":"code","39622552":"code","d8d4e7b2":"code","5e93a354":"code","c9b6e79d":"code","d43278ac":"code","8f7c1394":"code","12c68326":"code","2fbf9858":"code","edc1d092":"code","0203bf57":"code","64505b29":"code","0f73fdaf":"markdown","6306f1a6":"markdown","351e4bee":"markdown","6244336d":"markdown","0ae8d068":"markdown","aaeda26e":"markdown","d3d15d67":"markdown","c0ff952e":"markdown","6d931584":"markdown","0a846518":"markdown","742544bc":"markdown","17b93c4d":"markdown","2249c189":"markdown","38b1999b":"markdown","079a8257":"markdown","9da981a9":"markdown"},"source":{"d8b3b485":"import pandas as pd\nimport numpy as np\nimport re\nimport os\nfrom IPython.display import HTML\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text \nfrom sklearn.decomposition import PCA\n\nfrom tensorflow.python.keras.models import Sequential, load_model\nfrom tensorflow.python.keras.layers import Dense, Dropout\nfrom tensorflow.python.keras import optimizers\n\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import words\nfrom nltk.corpus import wordnet \nallEnglishWords = words.words() + [w for w in wordnet.words()]\nallEnglishWords = np.unique([x.lower() for x in allEnglishWords])\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')","927d8e59":"path = \"\/kaggle\/input\/aclimdb\/aclImdb\/\"\npositiveFiles = [x for x in os.listdir(path+\"train\/pos\/\") if x.endswith(\".txt\")]\nnegativeFiles = [x for x in os.listdir(path+\"train\/neg\/\") if x.endswith(\".txt\")]\ntestFiles = [x for x in os.listdir(path+\"test\/\") if x.endswith(\".txt\")]","ae7c70f6":"positiveReviews, negativeReviews, testReviews = [], [], []\nfor pfile in positiveFiles:\n    with open(path+\"train\/pos\/\"+pfile, encoding=\"latin1\") as f:\n        positiveReviews.append(f.read())\nfor nfile in negativeFiles:\n    with open(path+\"train\/neg\/\"+nfile, encoding=\"latin1\") as f:\n        negativeReviews.append(f.read())\nfor tfile in testFiles:\n    with open(path+\"test\/\"+tfile, encoding=\"latin1\") as f:\n        testReviews.append(f.read())","d2eed7ff":"reviews = pd.concat([\n    pd.DataFrame({\"review\":positiveReviews, \"label\":1, \"file\":positiveFiles}),\n    pd.DataFrame({\"review\":negativeReviews, \"label\":0, \"file\":negativeFiles}),\n    pd.DataFrame({\"review\":testReviews, \"label\":-1, \"file\":testFiles})\n], ignore_index=True).sample(frac=1, random_state=1)\nreviews.head()","be9a7464":"reviews = reviews[[\"review\", \"label\", \"file\"]].sample(frac=1, random_state=1)\ntrain = reviews[reviews.label!=-1].sample(frac=0.6, random_state=1)\nvalid = reviews[reviews.label!=-1].drop(train.index)\ntest = reviews[reviews.label==-1]","a2466cb8":"print(train.shape)\nprint(valid.shape)\nprint(test.shape)","365eedc6":"HTML(train.review.iloc[0])","0013fcd6":"class Preprocessor(object):\n    ''' Preprocess data for NLP tasks. '''\n\n    def __init__(self, alpha=True, lower=True, stemmer=True, english=False):\n        self.alpha = alpha\n        self.lower = lower\n        self.stemmer = stemmer\n        self.english = english\n        \n        self.uniqueWords = None\n        self.uniqueStems = None\n        \n    def fit(self, texts):\n        texts = self._doAlways(texts)\n\n        allwords = pd.DataFrame({\"word\": np.concatenate(texts.apply(lambda x: x.split()).values)})\n        self.uniqueWords = allwords.groupby([\"word\"]).size().rename(\"count\").reset_index()\n        self.uniqueWords = self.uniqueWords[self.uniqueWords[\"count\"]>1]\n        if self.stemmer:\n            self.uniqueWords[\"stem\"] = self.uniqueWords.word.apply(lambda x: PorterStemmer().stem(x)).values\n            self.uniqueWords.sort_values([\"stem\", \"count\"], inplace=True, ascending=False)\n            self.uniqueStems = self.uniqueWords.groupby(\"stem\").first()\n        \n        #if self.english: self.words[\"english\"] = np.in1d(self.words[\"mode\"], allEnglishWords)\n        print(\"Fitted.\")\n            \n    def transform(self, texts):\n        texts = self._doAlways(texts)\n        if self.stemmer:\n            allwords = np.concatenate(texts.apply(lambda x: x.split()).values)\n            uniqueWords = pd.DataFrame(index=np.unique(allwords))\n            uniqueWords[\"stem\"] = pd.Series(uniqueWords.index).apply(lambda x: PorterStemmer().stem(x)).values\n            uniqueWords[\"mode\"] = uniqueWords.stem.apply(lambda x: self.uniqueStems.loc[x, \"word\"] if x in self.uniqueStems.index else \"\")\n            texts = texts.apply(lambda x: \" \".join([uniqueWords.loc[y, \"mode\"] for y in x.split()]))\n        #if self.english: texts = self.words.apply(lambda x: \" \".join([y for y in x.split() if self.words.loc[y,\"english\"]]))\n        print(\"Transformed.\")\n        return(texts)\n\n    def fit_transform(self, texts):\n        texts = self._doAlways(texts)\n        self.fit(texts)\n        texts = self.transform(texts)\n        return(texts)\n    \n    def _doAlways(self, texts):\n        # Remove parts between <>'s\n        texts = texts.apply(lambda x: re.sub('<.*?>', ' ', x))\n        # Keep letters and digits only.\n        if self.alpha: texts = texts.apply(lambda x: re.sub('[^a-zA-Z0-9 ]+', ' ', x))\n        # Set everything to lower case\n        if self.lower: texts = texts.apply(lambda x: x.lower())\n        return texts  ","f42130fb":"train.head()","db92a35d":"preprocess = Preprocessor(alpha=True, lower=True, stemmer=True)","88e8d453":"%%time\ntrainX = preprocess.fit_transform(train.review)\nvalidX = preprocess.transform(valid.review)","37ba3909":"trainX.head()","78d6b031":"print(preprocess.uniqueWords.shape)\npreprocess.uniqueWords[preprocess.uniqueWords.word.str.contains(\"disappoint\")]","4b67fe78":"print(preprocess.uniqueStems.shape)\npreprocess.uniqueStems[preprocess.uniqueStems.word.str.contains(\"disappoint\")]","7dcb1deb":"stop_words = text.ENGLISH_STOP_WORDS.union([\"thats\",\"weve\",\"dont\",\"lets\",\"youre\",\"im\",\"thi\",\"ha\",\n    \"wa\",\"st\",\"ask\",\"want\",\"like\",\"thank\",\"know\",\"susan\",\"ryan\",\"say\",\"got\",\"ought\",\"ive\",\"theyre\"])\ntfidf = TfidfVectorizer(min_df=2, max_features=10000, stop_words=stop_words) #, ngram_range=(1,3)","e9551d74":"%%time\ntrainX = tfidf.fit_transform(trainX).toarray()\nvalidX = tfidf.transform(validX).toarray()","8e774f94":"print(trainX.shape)\nprint(validX.shape)","38a50b6c":"trainY = train.label\nvalidY = valid.label","ca1e1246":"print(trainX.shape, trainY.shape)\nprint(validX.shape, validY.shape)","d4f94962":"from scipy.stats.stats import pearsonr","f56de1be":"getCorrelation = np.vectorize(lambda x: pearsonr(trainX[:,x], trainY)[0])\ncorrelations = getCorrelation(np.arange(trainX.shape[1]))\nprint(correlations)","a8679312":"allIndeces = np.argsort(-correlations)\nbestIndeces = allIndeces[np.concatenate([np.arange(1000), np.arange(-1000, 0)])]","5efb3d20":"vocabulary = np.array(tfidf.get_feature_names())\nprint(vocabulary[bestIndeces][:10])\nprint(vocabulary[bestIndeces][-10:])","3f16aee8":"trainX = trainX[:,bestIndeces]\nvalidX = validX[:,bestIndeces]","88f0f4e8":"print(trainX.shape, trainY.shape)\nprint(validX.shape, validY.shape)","d12f0782":"DROPOUT = 0.5\nACTIVATION = \"tanh\"\n\nmodel = Sequential([    \n    Dense(int(trainX.shape[1]\/2), activation=ACTIVATION, input_dim=trainX.shape[1]),\n    Dropout(DROPOUT),\n    Dense(int(trainX.shape[1]\/2), activation=ACTIVATION, input_dim=trainX.shape[1]),\n    Dropout(DROPOUT),\n    Dense(int(trainX.shape[1]\/4), activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(100, activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(20, activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(5, activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(1, activation='sigmoid'),\n])","6de0ad7d":"model.compile(optimizer=optimizers.Adam(0.00005), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","bb86276d":"EPOCHS = 30\nBATCHSIZE = 1500","a1ed4d69":"model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCHSIZE, validation_data=(validX, validY))","aba78b10":"x = np.arange(EPOCHS)\nhistory = model.history.history\n\ndata = [\n    go.Scatter(x=x, y=history[\"acc\"], name=\"Train Accuracy\", marker=dict(size=5), yaxis='y2'),\n    go.Scatter(x=x, y=history[\"val_acc\"], name=\"Valid Accuracy\", marker=dict(size=5), yaxis='y2'),\n    go.Scatter(x=x, y=history[\"loss\"], name=\"Train Loss\", marker=dict(size=5)),\n    go.Scatter(x=x, y=history[\"val_loss\"], name=\"Valid Loss\", marker=dict(size=5))\n]\nlayout = go.Layout(\n    title=\"Model Training Evolution\", font=dict(family='Palatino'), xaxis=dict(title='Epoch', dtick=1),\n    yaxis1=dict(title=\"Loss\", domain=[0, 0.45]), yaxis2=dict(title=\"Accuracy\", domain=[0.55, 1]),\n)\npy.iplot(go.Figure(data=data, layout=layout), show_link=False)","7d43095e":"train[\"probability\"] = model.predict(trainX)\ntrain[\"prediction\"] = train.probability-0.5>0\ntrain[\"truth\"] = train.label==1\ntrain.tail()","6637536f":"print(model.evaluate(trainX, trainY))\nprint((train.truth==train.prediction).mean())","66ef9bec":"valid[\"probability\"] = model.predict(validX)\nvalid[\"prediction\"] = valid.probability-0.5>0\nvalid[\"truth\"] = valid.label==1\nvalid.tail()","39622552":"print(model.evaluate(validX, validY))\nprint((valid.truth==valid.prediction).mean())","d8d4e7b2":"trainCross = train.groupby([\"prediction\", \"truth\"]).size().unstack()\ntrainCross","5e93a354":"validCross = valid.groupby([\"prediction\", \"truth\"]).size().unstack()\nvalidCross","c9b6e79d":"truepositives = valid[(valid.truth==True)&(valid.truth==valid.prediction)]\nprint(len(truepositives), \"true positives.\")\ntruepositives.sort_values(\"probability\", ascending=False).head(3)","d43278ac":"truenegatives = valid[(valid.truth==False)&(valid.truth==valid.prediction)]\nprint(len(truenegatives), \"true negatives.\")\ntruenegatives.sort_values(\"probability\", ascending=True).head(3)","8f7c1394":"falsepositives = valid[(valid.truth==True)&(valid.truth!=valid.prediction)]\nprint(len(falsepositives), \"false positives.\")\nfalsepositives.sort_values(\"probability\", ascending=True).head(3)","12c68326":"falsenegatives = valid[(valid.truth==False)&(valid.truth!=valid.prediction)]\nprint(len(falsenegatives), \"false negatives.\")\nfalsenegatives.sort_values(\"probability\", ascending=False).head(3)","2fbf9858":"HTML(valid.loc[22148].review)","edc1d092":"unseen = pd.Series(\"this movie very good\")","0203bf57":"unseen = preprocess.transform(unseen)       # Text preprocessing\nunseen = tfidf.transform(unseen).toarray()  # Feature engineering\nunseen = unseen[:,bestIndeces]              # Feature selection\nprobability = model.predict(unseen)[0,0]  # Network feedforward","64505b29":"print(probability)\nprint(\"Positive!\") if probability > 0.5 else print(\"Negative!\")","0f73fdaf":"This is the review that got predicted as positive most certainly - while being labeled as negative. However, we can easily recognize it as a poorly labeled sample.","6306f1a6":"### Custom Reviews\nTo use this model, we would store the model, along with the preprocessing vectorizers, and run the unseen texts through following pipeline.","351e4bee":"---\n\n## Model Application","6244336d":"With everything centralized in 1 dataframe, we now perform train, validation and test set splits.","0ae8d068":"---\n\n## Data Import\nFirst, we need to import the data.","aaeda26e":"---\n\n## Feature Selection\nNext, we take the 10k dimensional tfidf's as input, and keep the 2000 dimensions that correlate the most with our sentiment target. The corresponding words - see below - make sense.","d3d15d67":"In this notebook Sentiment Analysis is performed on movie reviews.\n\n---","c0ff952e":"<h1>Content<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Import\" data-toc-modified-id=\"Data-Import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Data Import<\/a><\/span><\/li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span><\/li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Feature Engineering<\/a><\/span><\/li><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Feature Selection<\/a><\/span><\/li><li><span><a href=\"#Model-Architecture\" data-toc-modified-id=\"Model-Architecture-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Model Architecture<\/a><\/span><\/li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Model Training<\/a><\/span><\/li><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Model Evaluation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy-&amp;-Loss\" data-toc-modified-id=\"Accuracy-&amp;-Loss-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Accuracy &amp; Loss<\/a><\/span><\/li><li><span><a href=\"#Error-Analysis\" data-toc-modified-id=\"Error-Analysis-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>Error Analysis<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Application\" data-toc-modified-id=\"Model-Application-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Model Application<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Test-Predictions\" data-toc-modified-id=\"Test-Predictions-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>Test Predictions<\/a><\/span><\/li><li><span><a href=\"#Custom-Reviews\" data-toc-modified-id=\"Custom-Reviews-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Custom Reviews<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","6d931584":"---\n\n## Model Evaluation","0a846518":"---\n\n## Data Preprocessing\nThe next step is data preprocessing. The following class behaves like your typical SKLearn vectorizer.\n\nIt can perform the following operations.\n* Discard non alpha-numeric characters\n* Set everything to lower case\n* Stems all words using PorterStemmer, and change the stems back to the most occurring existent word.\n* Discard non-Egnlish words (not by default).","742544bc":"### Accuracy & Loss\nLet's first centralize the probabilities and predictions with the original train and validation dataframes. Then we can print out the respective accuracies and losses.","17b93c4d":"---\n\n## Feature Engineering\nNext, we take the preprocessed texts as input and calculate their TF-IDF's ([info](http:\/\/www.tfidf.com)). We retain 10000 features per text.","2249c189":"---\n\n## Model Architecture\nWe choose a very simple dense network with 6 layers, performing binary classification.","38b1999b":"### Error Analysis\nError analysis gives us great insight in the way the model is making its errors. Often, it shows data quality issues.","079a8257":"# Sentiment Analysis on Movie Reviews","9da981a9":"---\n\n## Model Training\nLet's go."}}