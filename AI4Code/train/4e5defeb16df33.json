{"cell_type":{"2b5edafb":"code","1e0d0edd":"code","fbbf4890":"code","981c5cbb":"code","715fb8e3":"code","150a29df":"code","60317977":"code","7bb159d9":"code","c11f1de4":"code","8e836341":"code","b351bc70":"code","abb73242":"code","f0d6ffc7":"code","27546a89":"code","fd5a301e":"code","ab38ae47":"code","c6d6f1a0":"code","363dd0de":"code","0030d6fe":"code","fc261f0a":"code","89732c35":"code","7147be44":"code","a8fad5b2":"code","1b406471":"code","0d4079fa":"code","b29ddd9c":"code","ed7d647b":"code","d57c2a26":"code","9f931886":"code","7322a2e0":"code","4f73f18b":"code","da834517":"code","f546b9d3":"code","da72dc33":"code","a945f99a":"code","523e5040":"code","94250d33":"code","ded15e6b":"code","e7e06916":"code","5c321209":"code","714c3bc5":"code","094c2c0b":"code","a71a7156":"code","c4d7422d":"code","db04d1f3":"code","9d68f6b2":"code","48ddde00":"code","cf95a7da":"code","a4803f80":"code","7ddc0ed3":"code","05217ca2":"code","e0653978":"code","fdbd21d0":"code","30716050":"code","bb91d3a9":"code","aa4c78b9":"code","17d8072e":"code","2df5280b":"code","7aafabb1":"code","f30d80c0":"code","f67b286f":"code","e7288a0b":"code","007a9d97":"code","17a3484a":"code","e844651a":"code","987157f9":"code","b708f4ac":"code","02a7c529":"code","84c1e804":"code","2547548b":"code","250e74d5":"code","efdf610f":"code","404228f3":"code","88801454":"code","a7205f98":"code","40054efa":"code","20aeab0e":"code","6ff98194":"code","6d072f54":"code","56499cf1":"code","687ee909":"code","c8379d3e":"code","cd5f35db":"code","67408de8":"code","5db69257":"code","66f31209":"code","705cddfb":"code","12135a3e":"code","18411604":"code","cdbd3bb6":"code","c3ce16bf":"code","b75fb266":"code","f9e02824":"code","189b4e02":"code","58417e81":"code","0e2916dc":"code","b6254df2":"code","9140dd44":"code","8767ac0a":"code","dad5453f":"code","0ec4bda4":"code","1e571d3c":"code","c7cde40e":"code","5e543579":"code","d088ff84":"code","4ff9b046":"markdown","8b0dcd97":"markdown"},"source":{"2b5edafb":"'''\n\nContext\nComputer Network Traffic Data - A ~500K CSV with summary of some real network traffic data from the past. The dataset has ~21K rows and covers 10 local workstation IPs over a three month period. Half of these local IPs were compromised at some point during this period and became members of various botnets.\n\nContent\nEach row consists of four columns:\n\ndate: yyyy-mm-dd (from 2006-07-01 through 2006-09-30)\nl_ipn: local IP (coded as an integer from 0-9)\nr_asn: remote ASN (an integer which identifies the remote ISP)\nf: flows (count of connnections for that day)\nReports of \"odd\" activity or suspicions about a machine's behavior triggered investigations on the following days (although the machine might have been compromised earlier)\n\nDate : IP 08-24 : 1 09-04 : 5 09-18 : 4 09-26 : 3 6\n\n\n'''","1e0d0edd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fbbf4890":"import pandas as pd\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import zscore","981c5cbb":"df = pd.read_csv('..\/input\/cs448b_ipasn.csv')","715fb8e3":"df.head()","150a29df":"dfOrig = df.copy()\n#df = dfOrig.copy()","60317977":"df.describe()","7bb159d9":"df.info()","c11f1de4":"df.columns","8e836341":"#vivewing given examples ","b351bc70":"df[(df.l_ipn == 1) & (df.date == '2006-08-24')]\n","abb73242":"df[(df.l_ipn == 5) & (df.date == '2006-09-04')]\n\n ","f0d6ffc7":"df[(df.l_ipn == 4) & (df.date == '2006-09-18')]","27546a89":"df[(df.l_ipn == 3) & (df.date == '2006-09-26')]","fd5a301e":"#removing f == 1\n#df = df[df.f > 1]","ab38ae47":"#len(df)\/len(dfOrig)","c6d6f1a0":"df.l_ipn.value_counts()","363dd0de":"# 0 is he most active user, 3 is the least","0030d6fe":"for ip in set(df.l_ipn):\n    fNormed = df.loc[(df.l_ipn == ip),'f']\n    plt.boxplot(fNormed,len(fNormed) * [0],\".\")\n    plt.title('IP:' + str(ip))\n    plt.show()","fc261f0a":"for ip in set(df.l_ipn):\n    df[df.l_ipn == ip].f.hist(bins = 100)\n    plt.autoscale(enable=True, axis='both', tight=None)\n    plt.title(('IP: %d') % ip)\n    plt.show()","89732c35":"# instead use log scale since anomaly detection (have skewness of large values for \"normal activity\")\n\nfor ip in set(df.l_ipn):\n    df[df.l_ipn == ip].f.hist(log=True,bins =200)\n    plt.title(('IP: %d') % ip)\n    plt.show()","7147be44":"#normalize flows per IP since different ratios and scales ","a8fad5b2":"#sort by IP address\ndf.sort_values(inplace=True, by=['l_ipn'])","1b406471":"from sklearn.preprocessing import robust_scale\n# Scale features using statistics that are robust to outliers.\n# using robust_scale instead of RobustScaler since want to Standardize a dataset along any axis","0d4079fa":"from sklearn.preprocessing import StandardScaler","b29ddd9c":"#accessing columns\n\n#sample[sample['l_ipn'] == 0].fNorm = 2 # not good. returns a copy of sample.l_ipn\n\n#use iloc or loc instead \n# .loc[criterion,selection]\n#use df.iloc[1, df.columns.get_loc('s')] = 'B'    or use df.loc[df.index[1], 's'] = 'B'\n\n#sample[sample.iloc[:,sample.columns.get_loc('l_ipn')] == 0]\n#sample[sample.loc[:,'l_ipn'] == 0]","ed7d647b":"#normalize traffic for each IP \n\n#scaler = robust_scale()\nscaler = StandardScaler()","d57c2a26":"for ip in set(df.l_ipn):\n    df.loc[(df.l_ipn == ip),'fNorm'] = scaler.fit_transform(df.loc[(df.l_ipn == ip),'f'].values.reshape(-1, 1)) # reshaped since it's scaling a single feature only \n    df.loc[(df.l_ipn == ip),'fMean'] = scaler.mean_\n    df.loc[(df.l_ipn == ip),'fVar'] = scaler.var_","9f931886":"for ip in set(df.l_ipn):\n    fNormed = df.loc[(df.l_ipn == ip),'fNorm']\n    plt.plot(fNormed,len(fNormed) * [0],\".\")\n    fMean = df.loc[(df.l_ipn == ip),'fMean'].iloc[0]# only need the first value as they are all the same in this column for this ip\n    plt.plot(fMean,0,'ro')\n    plt.title('IP:' + str(ip))\n    plt.show()","7322a2e0":"# it is clear that there are anomalies in the amount of traffic flow ","4f73f18b":"# todo: trying a scaler for skewed data","da834517":"#analyzing ASN","f546b9d3":"#is asn unique per user?","da72dc33":"listOfAsnsPerUser = [[]] * len(df.l_ipn)","a945f99a":"numAsnsPerIp = 0\nfor ip in set(df.l_ipn):\n    numAsnsPerIp += len(set(df.loc[(df.l_ipn == ip),'r_asn']))","523e5040":"numAsnsPerIp","94250d33":"len(set(df.loc[:,'r_asn']))","ded15e6b":"#number of  unique of asns per ip != number of unqiue asns for total dataset\n#therefore, asns are not unique per IP","e7e06916":"#using asns as categorical variable ","5c321209":"dfDummy = df.copy()","714c3bc5":"dfDummy = pd.get_dummies(df,columns=['r_asn'],drop_first=True)","094c2c0b":"dfDummy.head()","a71a7156":"#takes too long \n# dfDummy.drop(labels =['date','fNorm','fMean','fVar','l_ipn'],axis=1).corr() ","c4d7422d":"dfCorrAsnFlow = dfDummy.drop(labels =['date','fNorm','fMean','fVar','l_ipn'],axis=1)","db04d1f3":"# todo: look for anomalies in users using suddently a different ASN and have a high traffic flow ","9d68f6b2":"# time sampling","48ddde00":"df.head()","cf95a7da":"df.date = pd.to_datetime(df.date,errors='coerce')","a4803f80":"len(df.date) == len(df.date.dropna()) ","7ddc0ed3":"# all dates were valid ","05217ca2":"df.info()","e0653978":"#df.date.hist(bins = 100)","fdbd21d0":"#fig = plt.figure(figsize = (15,20))\n#ax = fig.gca()\n#df.date.hist(bins = 50, ax = ax)","30716050":"df = df.sort_values('date', ascending=True)\nplt.figure(figsize=(15,20))\nplt.plot(df['date'], df['f'])\nplt.xticks(rotation='vertical')","bb91d3a9":"# now per ip ","aa4c78b9":"for ip in set(df.l_ipn):\n    plt.figure(figsize=(10,15))\n    plt.xticks(rotation='vertical')\n    plt.title(('IP: %d') % ip)\n    plt.plot(df[df.l_ipn == ip]['date'], df[df.l_ipn == ip]['f'])\n    plt.show()","17d8072e":"#using IP = 4 as poc ","2df5280b":"dataset = df.copy()","7aafabb1":"dataset = pd.get_dummies(dataset,columns=['r_asn'],drop_first=True)","f30d80c0":"dataset.head()","f67b286f":"dataset = dataset.drop(labels =['date','fNorm','fMean','fVar'],axis=1)","e7288a0b":"dataset.head()","007a9d97":"#using IP == 4 as POC","17a3484a":"dataset = dataset[dataset.l_ipn == 4].drop(['l_ipn'],axis=1)","e844651a":"from sklearn.preprocessing import StandardScaler","987157f9":"scaler = StandardScaler()","b708f4ac":"dataset.loc[:,'f'] = scaler.fit_transform(dataset.f.values.reshape(-1, 1))","02a7c529":"dataset","84c1e804":"from sklearn.cluster import KMeans","2547548b":"kmeans = KMeans(n_clusters=2, random_state=0).fit(dataset)","250e74d5":"kmeans.labels_","efdf610f":"from sklearn.decomposition import PCA","404228f3":"pca = PCA(n_components=2)","88801454":"centroids = kmeans.cluster_centers_","a7205f98":"centroids","40054efa":"centroids2d = pd.DataFrame(pca.fit_transform(centroids))","20aeab0e":"centroids2d","6ff98194":"xPca = centroids2d.loc[:,0]\nyPca = centroids2d.loc[:,1]","6d072f54":"xPca","56499cf1":"yPca","687ee909":"plt.scatter(xPca,yPca)","c8379d3e":"# will create a single dataset without any anomalies for IP = 4 ","cd5f35db":"plt.figure(figsize=(10,15))\nplt.xticks(rotation='vertical')\nplt.yscale('log')\nplt.title(('IP: %d') % 4)\nplt.plot(range(len(dataset)), dataset['f'])\nplt.show()","67408de8":"len(dataset[(dataset.f < 10**-.5)])","5db69257":"len(dataset)","66f31209":"#seems as those two days were anomalious","705cddfb":"negativeClass = dataset[(dataset.f >= 10**-.5)]","12135a3e":"negativeClass","18411604":"positiveClass = dataset.head(2)","cdbd3bb6":"positiveClass","c3ce16bf":"#removing test classes from dataset\ndataset = dataset.drop(dataset.index[[0,1]])","b75fb266":"len(dataset)","f9e02824":"dataset = dataset[(dataset.f < 10**-.5)]","189b4e02":"len(dataset)","58417e81":"kmeans.predict(positiveClass)","0e2916dc":"kmeans.predict(negativeClass)","b6254df2":"posRes = kmeans.transform(positiveClass)","9140dd44":"negRes = kmeans.transform(negativeClass)","8767ac0a":"negRes[0]","dad5453f":"from numpy import linalg","0ec4bda4":"centroids","1e571d3c":"dist = numpy.linalg.norm(a-b)","c7cde40e":"# todo checkout auto encoders","5e543579":"#todo checkout one class SVM","d088ff84":"#create classifier and split data into train test and validation ","4ff9b046":"# classification models","8b0dcd97":"#EDA - Data Analysis"}}