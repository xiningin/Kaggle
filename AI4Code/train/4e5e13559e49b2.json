{"cell_type":{"985b02ee":"code","9570757e":"code","6e20cf4e":"code","4109bffc":"code","d3dd9705":"code","f2d93028":"code","f98d1e19":"code","5fba9b3c":"code","c8245daf":"code","10bbcc97":"code","9afab021":"code","f1833c6d":"code","fa3c2bea":"code","62bf9a56":"code","4fea52f8":"code","765c14c9":"code","e726a675":"code","81b9d18d":"code","df545892":"code","ae99d5bf":"code","a4172101":"code","2bf8982e":"code","26d144a9":"code","0b5e5bac":"code","c33a5263":"code","ea5a97fa":"code","6946d848":"code","5686fc69":"code","82ad951f":"code","86423946":"code","a8f2370b":"code","1e3d4323":"code","2340ca19":"code","6e1f8dc9":"markdown","c9527006":"markdown","a8367526":"markdown","c51bede1":"markdown","0defcaaa":"markdown","ec1202c0":"markdown","e9a3af19":"markdown","f09c9960":"markdown","6d39a003":"markdown","e2235ccc":"markdown","bc947121":"markdown","7bc4e1e9":"markdown","89c51d79":"markdown","6d743b24":"markdown","95d5e64b":"markdown","21e91f5d":"markdown","4cec691a":"markdown","d1c9e25b":"markdown","b938c061":"markdown","ffdcccfc":"markdown","3236c666":"markdown","081eff75":"markdown","3f20bfe2":"markdown","43c91e75":"markdown","e32115d6":"markdown","d8e1d147":"markdown","11b03c50":"markdown","65f14cad":"markdown","cce9edf3":"markdown","54804640":"markdown","9863c2fb":"markdown","c2c60402":"markdown","2070b68f":"markdown","a6c7843e":"markdown","ca2e3420":"markdown","09baa725":"markdown","1ef2676c":"markdown","27ed2b39":"markdown","167dacba":"markdown","fb910a2c":"markdown","a9fcbbdb":"markdown","9468e084":"markdown","b5840e3f":"markdown","b9536580":"markdown","08070837":"markdown","49dec03a":"markdown","932c3a07":"markdown","f0dfda3a":"markdown","4ed63280":"markdown","f0b06e5a":"markdown","962e5943":"markdown","39b2275d":"markdown","cb30825c":"markdown","becab523":"markdown","d0f11b64":"markdown","6736a0cb":"markdown","b53d1fef":"markdown","6063f3f5":"markdown","9bc9e895":"markdown"},"source":{"985b02ee":"!pip install --pre torch==1.7.0.dev20200701+cu101 torchvision==0.8.0.dev20200701+cu101 -f https:\/\/download.pytorch.org\/whl\/nightly\/cu101\/torch_nightly.html","9570757e":"pip install pytorch-lightning==0.8.5 efficientnet_pytorch wandb --upgrade","6e20cf4e":"import os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid","4109bffc":"data_dir = '..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data'\n\nmean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\ntrain_transforms = transforms.Compose([\n            transforms.Resize((400, 400)),\n            transforms.RandomCrop(350),\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std, inplace=True)\n        ])\ntrain = ImageFolder(os.path.join(data_dir,'train'), train_transforms)","d3dd9705":"batch_size = 10\ntrain_dl = DataLoader(train, batch_size, shuffle=True, num_workers=4, pin_memory=True)","f2d93028":"def imshow(inp):\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    \n    \ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(14, 14))\n        ax.set_xticks([]); ax.set_yticks([])\n        imshow(make_grid(images[:10], nrow=5))\n        break","f98d1e19":"show_batch(train_dl)","5fba9b3c":"import albumentations as albu\nfrom albumentations.pytorch import ToTensor\n\n\ndef pre_transforms():\n    return [albu.Resize(400, 400, p=1),\n           albu.PadIfNeeded(min_height=400, min_width=400, always_apply=True, border_mode=0)]\n\n\ndef hard_transforms():\n    result = [\n        albu.RandomCrop(352, 352, always_apply=True),\n        albu.HorizontalFlip(p=0.5),\n        albu.OneOf([\n            albu.RandomBrightnessContrast(\n              brightness_limit=0.4, contrast_limit=0.4, p=1),\n            albu.CLAHE(p=1),\n            albu.HueSaturationValue(p=1)\n        ],\n            p=0.9,\n        ),\n        albu.OneOf([\n                albu.IAASharpen(p=1),\n                albu.Blur(blur_limit=3, p=1),\n                albu.MotionBlur(blur_limit=3, p=1),\n            ],\n            p=0.9,\n        ),\n        albu.IAAAdditiveGaussianNoise(p=0.2)\n    ]\n\n    return result\n  \ndef post_transforms():\n    # we use ImageNet image normalization\n    # and convert it to torch.Tensor\n    return [albu.Normalize(), ToTensor()]\n  \ndef compose(transforms_to_compose):\n    # combine all augmentations into single pipeline\n    result = albu.Compose([\n      item for sublist in transforms_to_compose for item in sublist\n    ])\n    return result","c8245daf":"from typing import List\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage.io import imread as gif_imread\nfrom catalyst import utils\nfrom pathlib import Path\n\nROOT = Path(\"..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/\")\ntrain_image_path = ROOT \/ \"train\/Audi S6 Sedan 2011\/\"\nALL_IMAGES = sorted(train_image_path.glob(\"*.jpg\"))\nlen(ALL_IMAGES)\n\n\ndef show_examples(name: str, image: np.ndarray, image2: np.ndarray):\n    plt.figure(figsize=(12, 16))\n    plt.subplot(1, 2, 1)\n    plt.imshow(image)\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(image2)\n\ndef show(index: int, images: List[Path], transforms=None) -> None:\n    image_path = images[index]\n    image_path2 = images[index+4]\n    name = image_path.name\n\n    image = utils.imread(image_path)\n    image2 = utils.imread(image_path2)\n    temp = transforms(image=image)\n    temp2 = transforms(image=image2)\n    image = temp[\"image\"]\n    image2 = temp2['image']\n\n    show_examples(name, image, image2)\n\ndef show_random(images: List[Path], transforms=None) -> None:\n    length = len(images)\n    index = random.randint(0, length - 5)\n    show(index, images, transforms)\n    \nfrom torch.utils.data import Dataset\n\n\nclass ClassificationDataset(Dataset):\n    def __init__(\n        self,\n        images: List[Path],\n        transforms=None\n    ) -> None:\n        self.images = images\n        self.transforms = transforms\n\n    def __len__(self) -> int:\n        return len(self.images)\n\n    def __getitem__(self, idx: int) -> dict:\n        image_path = self.images[idx]\n        image_path2 = self.images[idx+4]\n        image = utils.imread(image_path)\n        image2 = utils.imread(image_path2)\n        \n        result = {\"image\": image, 'image2':image2}\n        result = self.transforms(**result)\n        \n        result[\"filename\"] = image_path.name\n\n        return result\n    \nshow_transforms = compose([pre_transforms(), hard_transforms()])","10bbcc97":"show_random(ALL_IMAGES, transforms=show_transforms)","9afab021":"import os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.metrics.functional.classification import accuracy\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateLogger\nfrom pytorch_lightning.loggers import WandbLogger\nfrom torch.optim import Adam\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import ImageFolder\nfrom efficientnet_pytorch import EfficientNet\n\nimport wandb\n\nfrom collections import OrderedDict\nfrom typing import Optional\nfrom torch.nn import Module","f1833c6d":"BN_TYPES = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n\n\ndef _make_trainable(module: Module) -> None:\n    \"\"\"Unfreezes a given module.\n    Args:\n        module: The module to unfreeze\n    \"\"\"\n    for param in module.parameters():\n        param.requires_grad = True\n    module.train()\n\n\ndef _recursive_freeze(module: Module,\n                      train_bn: bool = True) -> None:\n    \"\"\"Freezes the layers of a given module.\n    Args:\n        module: The module to freeze\n        train_bn: If True, leave the BatchNorm layers in training mode\n    \"\"\"\n    children = list(module.children())\n    if not children:\n        if not (isinstance(module, BN_TYPES) and train_bn):\n            for param in module.parameters():\n                param.requires_grad = False\n            module.eval()\n        else:\n            # Make the BN layers trainable\n            _make_trainable(module)\n    else:\n        for child in children:\n            _recursive_freeze(module=child, train_bn=train_bn)\n\n\ndef freeze(module: Module,\n           n: Optional[int] = None,\n           train_bn: bool = True) -> None:\n    \"\"\"Freezes the layers up to index n (if n is not None).\n    Args:\n        module: The module to freeze (at least partially)\n        n: Max depth at which we stop freezing the layers. If None, all\n            the layers of the given module will be frozen.\n        train_bn: If True, leave the BatchNorm layers in training mode\n    \"\"\"\n    children = list(module.children())\n    n_max = len(children) if n is None else int(n)\n\n    for child in children[:n_max]:\n        _recursive_freeze(module=child, train_bn=train_bn)\n\n    for child in children[n_max:]:\n        _make_trainable(module=child)","fa3c2bea":"class ResNet50(LightningModule):\n\n    def __init__(self, \n                train_bn: bool = True,\n                batch_size: int = 70,\n                lr: float = 1e-3,\n                num_workers: int = 4,\n                hidden_1: int = 1024,\n                hidden_2: int = 512,\n                epoch_freeze: int = 8,\n                total_steps: int = 15,\n                pct_start: float = 0.2,\n                anneal_strategy: str = 'cos',\n                **kwargs):\n        super().__init__()\n        self.train_bn = train_bn\n        self.batch_size = batch_size\n        self.lr = lr\n        self.num_workers = num_workers\n        self.hidden_1 = hidden_1\n        self.hidden_2 = hidden_2\n        self.epoch_freeze = epoch_freeze\n        self.total_steps = total_steps\n        self.pct_start = pct_start\n        self.anneal_strategy = anneal_strategy\n        self.save_hyperparameters()\n\n        self.__build_model()\n        \n    def __build_model(self):\n        num_target_classes = 196\n        backbone = models.resnet50(pretrained=True)\n    \n        _layers = list(backbone.children())[:-1]\n        self.feature_extractor = nn.Sequential(*_layers)\n\n        _fc_layers = [nn.Linear(2048, self.hidden_1),\n                     nn.Linear(self.hidden_1, self.hidden_2),\n                     nn.Linear(self.hidden_2, num_target_classes)]\n        self.fc = nn.Sequential(*_fc_layers)\n\n    def forward(self, x):\n\n        x = self.feature_extractor(x)\n        x = x.squeeze(-1).squeeze(-1)\n\n        x = self.fc(x)\n        return x\n    \n    def train(self, mode=True):\n        super().train(mode=mode)\n\n        epoch = self.current_epoch\n        if epoch < self.epoch_freeze and mode:\n            freeze(module=self.feature_extractor,\n                   train_bn=self.train_bn) \n            \n    def training_step(self, batch, batch_idx):\n\n        x, y = batch\n        y_logits = self.forward(x)\n\n        train_loss = F.cross_entropy(y_logits, y)\n        acc = accuracy(y_logits, y)\n\n        tqdm_dict = {'train_loss': train_loss}\n        output = OrderedDict({'loss': train_loss,\n                               'train_acc': acc,\n                              'log': tqdm_dict,\n                             'progress_bar': tqdm_dict})\n\n        return output\n\n\n    def training_epoch_end(self, outputs):\n\n        train_loss_mean = torch.stack([output['loss']\n                                       for output in outputs]).mean()\n        avg_acc = torch.stack([x['train_acc'] for x in outputs]).mean()\n        \n        tensorboard_logs = {'train_loss': train_loss_mean, 'train_acc': avg_acc}\n        return {'train_loss': train_loss_mean,  'log': tensorboard_logs}\n\n\n    def validation_step(self, batch, batch_idx):\n\n        x, y = batch\n        y_logits = self.forward(x)\n\n        val_loss = F.cross_entropy(y_logits, y)\n        acc = accuracy(y_logits, y)\n\n        return {'val_loss': val_loss, 'val_acc': acc}\n\n    def validation_epoch_end(self, outputs):\n        \n        val_loss_mean = torch.stack([output['val_loss']\n                                     for output in outputs]).mean()\n        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n        \n        tensorboard_logs = {'val_loss': val_loss_mean, 'val_acc': avg_acc}\n        return {'val_loss': val_loss_mean,  'log': tensorboard_logs}\n        \n\n    def configure_optimizers(self):\n        if self.current_epoch < self.epoch_freeze:\n        \n            optimizer = optim.Adam(filter(lambda p: p.requires_grad,\n                                      self.parameters()),\n                                   lr=self.lr)\n            return optimizer\n        \n        else:\n            \n            optimizer = optim.Adam(filter(lambda p: p.requires_grad,\n                                      self.parameters()),\n                                   lr=self.lr)\n            \n            scheduler = OneCycleLR(optimizer,\n                            max_lr=self.lr,\n                            total_steps=self.total_steps,\n                            pct_start=self.pct_start, anneal_strategy=self.anneal_strategy)\n\n        return [optimizer], [scheduler]\n\n    def setup(self, stage: str):\n        data_dir = '..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data'\n\n        mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n        train_transforms = transforms.Compose([\n            transforms.Resize((400, 400)),\n            transforms.RandomCrop(350),\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std, inplace=True)\n        ])\n        train = ImageFolder(data_dir+'\/train', train_transforms)\n\n        # transform val\n        val_transforms = transforms.Compose([\n            transforms.Resize((400, 400)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std, inplace=True)\n        ])\n        val = ImageFolder(data_dir+'\/test', val_transforms)\n        valid, _ = random_split(val, [len(val), 0])\n\n        # assign to use in dataloaders\n        self.train_dataset = train\n        self.val_dataset = valid\n\n    def train_dataloader(self):\n        return DataLoader(dataset=self.train_dataset,\n                            batch_size=self.batch_size,\n                            num_workers=self.num_workers,\n                            shuffle=True,\n                            pin_memory=True)\n\n    def val_dataloader(self):\n        return DataLoader(dataset=self.val_dataset,\n                            batch_size=self.batch_size,\n                            num_workers=self.num_workers,\n                            shuffle=False,\n                            pin_memory=True)","62bf9a56":"def main():\n\n    seed_everything(42)\n    model = ResNet50()\n\n    wandb.login(key=os.environ.get('WANDB_API_KEY'))\n    \n    wandb_logger = WandbLogger(name='Name', project=\"Project\")\n    \n    checkpoint_cb = ModelCheckpoint(filepath = '.\/cars-{epoch:02d}-{val_acc:.4f}',monitor='val_acc', mode='max')\n    early = EarlyStopping(patience=5, monitor='val_acc', mode='max')\n\n    trainer = Trainer(\n        gpus=1,\n        logger=wandb_logger,\n        max_epochs=15,\n        deterministic=True,\n        precision=16,\n        checkpoint_callback=checkpoint_cb,\n        early_stop_callback=early,\n        callbacks=[LearningRateLogger()],\n    )\n\n    trainer.fit(model)\n    \n    wandb.save(checkpoint_cb.best_model_path)\n\nif __name__ == '__main__':\n    main()","4fea52f8":"class EffNet(LightningModule):\n\n    def __init__(self, \n                num_target_classes = 196,\n                backbone: str = 'efficientnet-b5',\n                batch_size: int = 20,\n                lr: float = 5e-4,\n                wd: float = 0,\n                num_workers: int = 4,\n                factor: float = 0.5,\n                **kwargs):\n        super().__init__()\n        self.num_target_classes = num_target_classes\n        self.backbone= backbone\n        self.batch_size = batch_size\n        self.lr = lr\n        self.wd = wd\n        self.num_workers = num_workers\n        self.factor = factor\n        self.save_hyperparameters()\n\n        self.__build_model()\n        \n    def __build_model(self):\n        self.net = EfficientNet.from_pretrained(self.backbone)\n        in_features = self.net._fc.in_features\n\n        _fc_layers = [nn.Linear(in_features, self.num_target_classes)]\n        self.net._fc = nn.Sequential(*_fc_layers)\n\n    def forward(self, x):\n\n        return self.net.forward(x)\n    \n    def validation_step(self, batch, batch_idx):\n\n        x, y = batch\n        y_logits = self.forward(x)\n\n        val_loss = F.cross_entropy(y_logits, y)\n        acc, acc2 = self.__accuracy(y_logits, y, topk=(1,2))\n\n        return OrderedDict({'val_loss': val_loss, 'val_acc': acc,\n                            'top2_acc': acc2})\n\n    def validation_epoch_end(self, outputs):\n        \"\"\"Compute and log validation loss and accuracy at the epoch level.\"\"\"\n\n        val_loss_mean = torch.stack([output['val_loss']\n                                     for output in outputs]).mean()\n        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n        avg_acc2 = torch.stack([x['top2_acc'] for x in outputs]).mean()\n        \n        tensorboard_logs = {'val_loss': val_loss_mean, 'val_acc': avg_acc,\n                            'top2_acc': avg_acc2}\n        return {'val_loss': val_loss_mean,  'log': tensorboard_logs}\n\n    @classmethod\n    def __accuracy(cls, output, target, topk=(1,)):\n        \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n        with torch.no_grad():\n            maxk = max(topk)\n            batch_size = target.size(0)\n\n            _, pred = output.topk(maxk, 1, True, True)\n            pred = pred.t()\n            correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n            res = []\n            for k in topk:\n                correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n                res.append(correct_k.mul_(1.0 \/ batch_size))\n            return res\n\n    def configure_optimizers(self):\n\n        optimizer = Adam(self.parameters(),\n            lr=self.lr, weight_decay=self.wd)\n        lr_scheduler = {'scheduler': ReduceLROnPlateau(optimizer, factor=self.factor, \n            patience=2, mode='max')\n            ,'name': 'learning_rate',\n            'monitor': 'val_acc'}\n        return [optimizer], [lr_scheduler]","765c14c9":"class EffNetMultiStep(LightningModule):\n\n    def __init__(self, \n                num_target_classes = 196,\n                backbone: str = 'efficientnet-b5',\n                hidden: int = 1024,\n                dropout: float = 0.2, \n                train_bn: bool = True,\n                milestones: tuple = (4, 6),\n                batch_size: int = 20,\n                lr: float = 1e-3,\n                lr_scheduler_gamma: float = 3e-1,\n                wd: float = 0,\n                factor: float = 0.7,\n                num_workers: int = 4,\n                **kwargs):\n        super().__init__()\n        self.num_target_classes = num_target_classes\n        self.backbone= backbone\n        self.hidden = hidden\n        self.dropout = dropout\n        self.batch_size = batch_size\n        self.train_bn = train_bn\n        self.milestones = milestones\n        self.lr = lr\n        self.lr_scheduler_gamma = lr_scheduler_gamma\n        self.wd = wd\n        self.num_workers = num_workers\n        self.factor = factor\n        self.save_hyperparameters()\n\n        self.__build_model()\n        \n    def __build_model(self):\n        self.net = EfficientNet.from_pretrained(self.backbone)\n        \n        _layers = list(self.net.children())[:1]\n        self.feature_extractor = nn.Sequential(*_layers)\n        freeze(module=self.feature_extractor, train_bn=self.train_bn)\n\n        in_features = self.net._fc.in_features\n        _fc_layers = [nn.Linear(in_features, self.hidden),\n                    nn.ReLU(),\n                    nn.Dropout(self.dropout),\n                    nn.Linear(self.hidden, self.num_target_classes)]\n        self.net._fc = nn.Sequential(*_fc_layers)\n\n    def forward(self, x):\n        return self.net.forward(x)\n    \n    def train(self, mode=True):\n        super().train(mode=mode)\n\n        epoch = self.current_epoch\n        if epoch < self.milestones[0] and mode:\n            # feature extractor is frozen (except for BatchNorm layers)\n            freeze(module=self.feature_extractor,\n                   train_bn=self.train_bn)\n\n        elif self.milestones[0] <= epoch < self.milestones[1] and mode:\n            # Unfreeze last two layers of the feature extractor\n            freeze(module=self.feature_extractor,\n                   n=-2,\n                   train_bn=self.train_bn)\n\n    def configure_optimizers(self):\n        \n        if self.current_epoch <= self.milestones[1]:\n\n            optimizer = Adam(filter(lambda p: p.requires_grad,\n                                      self.parameters()),\n                               lr=self.lr)\n\n            lr_scheduler = {'scheduler': MultiStepLR(optimizer,\n                                milestones=self.milestones,\n                                gamma=self.lr_scheduler_gamma),\n                            'name': 'learning_rate' }\n\n            return [optimizer], [lr_scheduler]\n\n        else:\n            optimizer = Adam(self.parameters(),\n                lr=self.lr, weight_decay=self.wd)\n            lr_scheduler = {'scheduler': ReduceLROnPlateau(optimizer, factor=self.factor, \n            patience=2, mode='max'),'name': 'learning_rate',\n            'monitor': 'val_acc'}\n            return [optimizer], [lr_scheduler]","e726a675":"    @staticmethod\n    def add_model_specific_args(parent_parser):\n        parser = ArgumentParser(parents=[parent_parser])\n        parser.add_argument('--num_target_classes',\n                            default=196,\n                            type=int,\n                            metavar='NUM',\n                            help='Number of target classes')\n        parser.add_argument('--backbone',\n                            default='efficientnet-b5',\n                            type=str,\n                            metavar='BK',\n                            help='Name of the feature extractor')\n        parser.add_argument('--batch-size',\n                            default=16,\n                            type=int,\n                            metavar='B',\n                            help='batch size',\n                            dest='batch_size')\n        parser.add_argument('--lr',\n                            '--learning-rate',\n                            default=1e-3,\n                            type=float,\n                            metavar='LR',\n                            help='initial learning rate',\n                            dest='lr')\n        parser.add_argument('--weight_decay',\n                            default=0,\n                            type=float,\n                            metavar='WD',\n                            help='L2 Penalty',\n                            dest='weight_decay')\n        parser.add_argument('--num-workers',\n                            default=4,\n                            type=int,\n                            metavar='W',\n                            help='number of CPU workers',\n                            dest='num_workers')\n        parser.add_argument('--factor',\n                            default=0.5,\n                            type=float,\n                            metavar='FAC',\n                            help='Factor by which learning rate will be reduced',\n                            dest='factor')\n\n        return parser\n\n\ndef main(args: Namespace):\n\n    seed_everything(42)\n    model = EffNet(**vars(args))\n    \n    wandb_logger = WandbLogger(name='EffN', project=\"Cars\")\n\n    checkpoint_cb = ModelCheckpoint(filepath = '.\/cars-{epoch:02d}-{val_acc:.4f}',monitor='val_acc', mode='max')\n    early = EarlyStopping(patience=args.patience, monitor='val_acc', mode='max')\n\n    trainer = Trainer(\n        gpus=args.gpus,\n        logger=wandb_logger,\n        max_epochs=args.nb_epochs,\n        deterministic=True,\n        precision=16 if args.use_16bit else 32,\n        checkpoint_callback=checkpoint_cb,\n        early_stop_callback=early,\n        callbacks=[LearningRateLogger()],\n    )\n\n    trainer.fit(model)\n    \n    wandb.save(checkpoint_cb.best_model_path)\n\ndef get_args() -> Namespace:\n    parent_parser = ArgumentParser(add_help=False)\n    parent_parser.add_argument('--gpus', type=int, default=1,\n                               help='how many gpus')\n    parent_parser.add_argument('--use-16bit', \n                               dest='use_16bit', \n                               action='store_true',\n                               help='if true uses 16 bit precision')\n    parent_parser.add_argument('--epochs',\n                            default=15,\n                            type=int,\n                            metavar='N',\n                            help='total number of epochs',\n                            dest='nb_epochs')\n    parent_parser.add_argument('--patience',\n                            default=3,\n                            type=int,\n                            metavar='ES',\n                            help='early stopping',\n                            dest='patience')\n \n    parser = EffNet.add_model_specific_args(parent_parser)\n    return parser.parse_args()\n\nif __name__ == '__main__':\n    main(get_args())","81b9d18d":"pip install streamlit captum","df545892":"import numpy as np\nimport joblib\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport torchvision\n\nfrom captum.attr import IntegratedGradients, DeepLift, GradientShap, Occlusion\nfrom captum.attr import visualization as viz","ae99d5bf":"model = EffNet.load_from_checkpoint('.\/cars-epoch=09-val_acc=0.9375.ckpt')\nmodel.eval()\nclasses = joblib.load('..\/input\/classes\/classes.pkl')","a4172101":"def open_transform_image(path):\n    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n\n    # transform val\n    img_transforms = transforms.Compose([\n            transforms.Resize((400, 400)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std, inplace=True)\n        ])\n    img = Image.open(path)\n    image = img_transforms(img)\n    \n    return image\n\ndef interpretation_transform(path):\n    img_transforms = transforms.Compose([\n            transforms.Resize((400, 400)),\n            transforms.ToTensor()\n        ])\n    img = Image.open(path)\n    image = img_transforms(img)\n    \n    return image\n\ndef predict_label(img, model=model):\n    # Convert to a batch of 1\n    xb = img.unsqueeze(0)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    output = F.softmax(yb, dim=1)\n    prediction_score, pred = torch.topk(output, 1)\n\n    return pred.squeeze()","2bf8982e":"test_image = open_transform_image('..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/test\/Dodge Challenger SRT8 2011\/01673.jpg')\ntransformed_img = interpretation_transform('..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/test\/Dodge Challenger SRT8 2011\/01673.jpg')\npred_ix = predict_label(test_image)\ninterpretation_image = test_image.unsqueeze(0)\nprint(classes[pred_ix])","26d144a9":"integrated_gradients = IntegratedGradients(model)\nattributions_ig = integrated_gradients.attribute(interpretation_image, target=pred_ix, n_steps=5)","0b5e5bac":"default_cmap = LinearSegmentedColormap.from_list('custom blue', \n                                                 [(0, '#ffffff'),\n                                                  (0.25, '#000000'),\n                                                  (1, '#000000')], N=256)\n\n_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)),\n                             np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                             methods=[\"original_image\", \"heat_map\"],\n                             cmap=default_cmap,\n                             show_colorbar=True,\n                             signs=[\"all\", \"positive\"],\n                             outlier_perc=1)","c33a5263":"gradient_shap = GradientShap(model)\n\n# Defining baseline distribution of images\nrand_img_dist = torch.cat([interpretation_image * 0, interpretation_image * 1])\n\nattributions_gs = gradient_shap.attribute(interpretation_image,\n                                          baselines=rand_img_dist,\n                                          target=pred_ix)\n\n_ = viz.visualize_image_attr_multiple(np.transpose(attributions_gs.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      [\"original_image\", \"heat_map\"],\n                                      [\"all\", \"absolute_value\"],\n                                      cmap=default_cmap,\n                                      show_colorbar=True)","ea5a97fa":"dl = DeepLift(model)\nattributions_dl = dl.attribute(interpretation_image,\n                                baselines=interpretation_image*0,\n                                target=pred_ix)\n_ = viz.visualize_image_attr_multiple(np.transpose(attributions_dl.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      [\"original_image\", \"heat_map\"],\n                                      [\"all\", \"all\"],\n                                      show_colorbar=True,\n                                      outlier_perc=2,\n                                     )","6946d848":"occlusion = Occlusion(model)\n\nattributions_occ = occlusion.attribute(interpretation_image,\n                                       strides = (3, 8, 8),\n                                       target=pred_ix,\n                                       sliding_window_shapes=(3,15, 15),\n                                       baselines=0)","5686fc69":"_ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      [\"original_image\", \"heat_map\"],\n                                      [\"all\", \"positive\"],\n                                      show_colorbar=True,\n                                      outlier_perc=2,\n                                     )\n_ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      [\"original_image\", \"heat_map\"],\n                                      [\"all\", \"all\"],\n                                      show_colorbar=True,\n                                      outlier_perc=2,\n                                     )","82ad951f":"import streamlit as st\n\n@st.cache\ndef classes():\n    cl = joblib.load('classes.pkl')\n    return cl\n\ndef open_transform_image(path):\n    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n\n    img_transforms = transforms.Compose([\n            transforms.Resize((400, 400)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std, inplace=True)\n        ])\n    img = Image.open(path)\n    image = img_transforms(img)\n    \n    return image\n\ndef predict_logits(img, model):\n    xb = img.unsqueeze(0)\n    yb = model(xb)\n    output = F.softmax(yb, dim=1)\n    return output\n\ndef interpretation_transform(path):\n    img_transforms = transforms.Compose([\n            transforms.Resize((400, 400)),\n            transforms.ToTensor()\n        ])\n    img = Image.open(path)\n    image = img_transforms(img)\n    \n    return image\n\ndef interpretation_show(attributions):\n    return np.transpose(attributions.squeeze().cpu().detach().numpy(), (1,2,0))\n\n@st.cache\ndef main(path):\n    model = st_model.EffNet.load_from_checkpoint('.\/cars-epoch=09-val_acc=0.9375.ckpt')\n    model.eval()\n    def predict(path, model):\n        image = open_transform_image(path)\n        output = predict_logits(image, model)\n        _, pred_idx = torch.topk(output, 1)\n        return pred_idx[0]\n    pred_label_idx = predict(path, model)\n    return model, pred_label_idx\n\n@st.cache\ndef interpretation_deeplift(model, input_img, pred_ix):\n    dl = DeepLift(model)\n    attributions_dl = dl.attribute(input_img,\n                                          baselines=input_img*0,\n                                          target=pred_ix)\n\n    return attributions_dl\n\n@st.cache\ndef interpretation_occlusion(model, input_img, pred_ix):\n    occlusion = Occlusion(model)\n\n    attributions_occ = occlusion.attribute(input_img,\n                                       strides = (3, 8, 8),\n                                       target=pred_ix,\n                                       sliding_window_shapes=(3,15, 15),\n                                       baselines=0)\n    return attributions_occ\n\n@st.cache\ndef interpretation_gradient_shap(model, input_img, pred_ix):\n    gradient_shap = GradientShap(model)\n\n    # Defining baseline distribution of images\n    rand_img_dist = torch.cat([input_img * 0, input_img * 1])\n\n    attributions_gs = gradient_shap.attribute(input_img,\n                                          baselines=rand_img_dist,\n                                          target=pred_ix)\n    return attributions_gs\n\ndefault_cmap = LinearSegmentedColormap.from_list('custom blue', \n                                                 [(0, '#ffffff'),\n                                                  (0.25, '#000000'),\n                                                  (1, '#000000')], N=256)\n\nst.title('Car Model Classification')\n\nst.sidebar.header(\"User Input Image\")\n\nimg = st.sidebar.file_uploader(label='Upload your JPG file', type=['jpg'])\nif img:\n    image = Image.open(img)\n    st.image(image)\n\n    model, pred_ix = main(img)\n    input_img = open_transform_image(img).unsqueeze(0)\n    transformed_img = interpretation_transform(img)\n    labels = [classes()[pr] for pr in pred_ix]\n    result = (f'**{labels[0]}**')\n    \n    st.sidebar.header(\"Model Interpretation Algorithm\")\n\n    captum = st.sidebar.radio(\n        label = 'Select Algorithm',\n        options=[\"Prediction\", \"GradientShap\", \"DeepLift\", \"Occlusion\"]\n    )\n\n    if captum == 'Occlusion':\n        st.info('It may take up to 20 minutes to run Occlusion')\n        attributions = interpretation_occlusion(model, input_img, pred_ix)\n        _ = viz.visualize_image_attr_multiple(interpretation_show(attributions),\n                                      interpretation_show(transformed_img),\n                                      [\"original_image\", \"heat_map\"],\n                                      [\"all\", \"positive\"],\n                                      show_colorbar=True,\n                                      outlier_perc=2,\n                                     )\n        st.pyplot()\n        _ = viz.visualize_image_attr_multiple(interpretation_show(attributions),\n                                      interpretation_show(transformed_img),\n                                      [\"original_image\", \"heat_map\"],\n                                      [\"all\", \"all\"],\n                                      show_colorbar=True,\n                                      outlier_perc=2,\n                                     )\n        st.pyplot()\n\n\n    if captum == 'GradientShap':\n        attributions_gs = interpretation_gradient_shap(model, input_img, pred_ix)\n        _ = viz.visualize_image_attr_multiple(interpretation_show(attributions_gs),\n                                      interpretation_show(transformed_img),\n                                      [\"original_image\", \"heat_map\"],\n                                      [\"all\", \"absolute_value\"],\n                                      cmap=default_cmap,\n                                      show_colorbar=True)\n        st.pyplot()\n    \n    if captum == \"DeepLift\":\n        attributions_gb = interpretation_deeplift(model, input_img, pred_ix)\n        _ = viz.visualize_image_attr_multiple(interpretation_show(attributions_gb),\n                                      interpretation_show(transformed_img),\n                                      [\"original_image\", \"heat_map\"],\n                                      [\"all\", \"all\"],\n                                      show_colorbar=True,\n                                      outlier_perc=2,\n                                     )\n        st.pyplot()\n    \n    if captum == \"Prediction\":\n        st.sidebar.markdown(result)","86423946":"FROM ubuntu:18.04\n\nEXPOSE 8501\n# Keeps Python from generating .pyc files in the container\nENV PYTHONDONTWRITEBYTECODE 1\n\n# Turns off buffering for easier container logging\nENV PYTHONUNBUFFERED 1\n\nRUN apt-get update && apt-get install -y python3 python3-pip sudo\n\nCOPY . \/app \n\nRUN cd \/app\/ && pip3 install -r requirements.txt \n# make app directiry\nWORKDIR \/app \n\n# cmd to launch app when container is run\nCMD streamlit run streamlit.py\n\n# streamlit-specific commands for config\nENV LC_ALL=C.UTF-8\nENV LANG=C.UTF-8\nRUN mkdir -p \/root\/.streamlit\nRUN bash -c 'echo -e \"\\\n[general]\\n\\\nemail = \\\"\\\"\\n\\\n\" > \/root\/.streamlit\/credentials.toml'\n\nRUN bash -c 'echo -e \"\\\n[server]\\n\\\nenableCORS = false\\n\\\n\" > \/root\/.streamlit\/config.toml'\n","a8f2370b":"docker image build -t streamlit:v1 .","1e3d4323":"docker container run -p 8501:8501 -d streamlit:v1","2340ca19":"docker image push my_repo\/streamlit:v1","6e1f8dc9":"This time, we only change fully connected layers and use ReduceLROnPlateau.\n\nWe also going to keep track on top2 accuracy.","c9527006":"![](https:\/\/i.ibb.co\/PcyHCBk\/Eff-Net-Start.png)","a8367526":"After several tries with different `dropout` and `lr_scheduler_gamma`\n\n**Top1 ~ 90.5-91%**\n\n**Top2 ~ 96-96.4%**","c51bede1":"Occlusion-based attribution: We can estimate which areas of the image are critical for the classifier's decision by occluding them and quantifying how the decision changes.\n\nWe run a sliding window of size 15x15 with a stride of 8 along both image dimensions. At each location, we occlude the image with a baseline value of 0 which correspondes to a gray patch.\n\n*Note: this computation might take up to 20 minutes*","0defcaaa":"### Update\n\nWe can add mode advanced augmentations with [Albumentations](https:\/\/albumentations.ai\/) like Blur, Motion Blur, GaussianNoise and others.","ec1202c0":"### IntegratedGradients","e9a3af19":"We\u2019ll be managing the Stanford car dataset, which contains 196 classes with 8144 and 8041 images for train and test, respectively. Classes are balanced with ~41 images in train and test for each.","f09c9960":"## Inspecting and Preparing Data","6d39a003":"### DeepLift","e2235ccc":"### Occlusion","bc947121":"Here we set seed to reproduce our work, initialize model, then set callbacks to monitor validation accuracy, fit model with trainer, and save the best one.","7bc4e1e9":"We reached **92.28%** accuracy on 8 epoch.","89c51d79":"We divide arguments in two sections:\n* Model specific args\n* Trainer args\n\nFirst, in LightningModule, define the arguments specific to that module. Then in `get_args` we add the Trainer args.","6d743b24":"We need to update torch version to use native Automatic Mixed Precision.","95d5e64b":"## Baseline Model - ResNet50","21e91f5d":"Deeplift assigns attributions to each input pixel by looking at the differences of output and its reference in terms of the differences of the input from the reference.","4cec691a":"We define our helper functions, functions for attributions(`interpretation_name`) and predictions(`main`). Then we mark them with @st.cache decorator to store results in a local cache.\n\nWe add a sidebar radio button widget with four options: \"Prediction\", \"GradientShap\", \"DeepLift\", \"Occlusion\". \nAfter image is uploaded, `main` executed.","d1c9e25b":"![First10](https:\/\/i.ibb.co\/1dgZSsg\/Res-Net50-first-10.png)","b938c061":"### First Method","ffdcccfc":"### Build Image and Run Container","3236c666":"Streamlit is an open-source Python library that makes it easy to build beautiful custom web-apps for machine learning and data science. \n\nYou can run your script with `streamlit run [filename]`","081eff75":"Our objectives for this notebook will be:\n   1. **Fine-tune ResNet50 and EfficientNet(b5) with different training stages**\n   1. **Write script with CLI**\n   1. **Gain an insight of best model prediction with Gradient-based and Occlusion-based algorithms**\n   1. **Create Web App using Streamlit**\n   1. **Create Dockerfile and deploy our web app on Azure App Service**","3f20bfe2":"### Training","43c91e75":"## CLI","e32115d6":"### GradientShap","d8e1d147":"# Model Interpretation and Web App","11b03c50":"Push an image to a Docker Hub","65f14cad":"## EfficientNet","cce9edf3":"# Overview\n\n![](https:\/\/pythonawesome.com\/content\/images\/2019\/12\/Car-Recognition.jpg)","54804640":"![](https:\/\/s7.gifyu.com\/images\/Car-St-1.gif)","9863c2fb":"# Model Creation","c2c60402":"![](https:\/\/s7.gifyu.com\/images\/Car-Azure.gif)","2070b68f":"![](https:\/\/i.ibb.co\/1fCcgRP\/Multi-Step.png)","a6c7843e":"Utility functions to freeze layers","ca2e3420":"Captum (\u201ccomprehension\u201d in Latin) is an open source, extensible library for model interpretability built on PyTorch.\n\nCaptum provides state-of-the-art algorithms to provide researchers and developers with an easy way to understand which features are contributing to a model\u2019s output.","09baa725":"Then we need to install Pytorch-Lightning to create a model with well-structured code, EfficientNet to download pre-trained model and Wandb to log metrics and save checkpoints.","1ef2676c":"![Main](https:\/\/i.ibb.co\/HFLzTPK\/ResNet50.jpg)","27ed2b39":"### Second Method","167dacba":"![](https:\/\/i.ibb.co\/yNmfwbX\/Best.png)","fb910a2c":"First, we'll use classifier with two hidden layers, set the number of epochs to freeze convolutional block expect BatchNorm, then we unfreeze all layers and use One Cycle Learning Rate Policy, which involves starting with a low learning rate, gradually increasing it batch-by-batch to a high learning rate for about 20% of epochs, then gradually decreasing it to a very low value for the remaining epochs. \nWhen learning rate is higher, the learning rate works as regularisation method and keep network from overfitting. This helps the network to avoid steep areas of loss and land better flatter minima.\n\nWe pass our image transformation in setup function. Then we are making split with train_dataloader and val_dataloader","a9fcbbdb":"Integrated Gradients computes the integral of the gradients of the output prediction for the class index `pred_ix` with respect to the input image pixels along the path from the black image to our input image.\n\n*Note: to compute integral, we need at least 5GB RAM available*","9468e084":"Hello everyone!\n\n\nIf you liked this notebook, don't forget to upvote, thanks.","b5840e3f":"The Dockerfile for streamplit app looks like this:\n\n*Note: We can use less image space*","b9536580":"## System setup","08070837":"### Azure App Service","49dec03a":"### Load model and define helper functions","932c3a07":"![Model Size vs. Accuracy Comparison](https:\/\/test.neurohive.io\/wp-content\/uploads\/2019\/06\/Screenshot-from-2019-06-02-14-09-40-570x417.png)","f0dfda3a":"# Docker and Azure App Service","4ed63280":"After we continued with the small learning rate around 1.5e-5.\n\n**Top1 ~ 93.7-93.75%**\n\n**Top2 ~ 97.8-97.85%**","f0b06e5a":"After the second stage with OneCycleLR we reach **89.14%**","962e5943":"Here we train model in three stages. From epoch 0 to 3, the feature extractor is frozen except for the BatchNorm layers. From epoch 4 to 5, the last two\nlayer groups of the pre-trained network are unfrozen. Finally, from epoch 6 all the remaining layer groups are unfrozen. We use MultiStepLR to train with different learning rates on each stage.\nWe also add one hidden layer with dropout.","39b2275d":"First, lets create some transforms for our data and load the train\/test data+labels from the folders.","cb30825c":"## Captum","becab523":"* Go to [Azure portal](https:\/\/azure.microsoft.com\/en-us\/features\/azure-portal\/)\n* Search for App Services\n* Click the \u201cAdd\u201d button\n* Fill out all the necessary fields\n* For Publish -> select \u201cDocker Container\u201d\n* On the \u201cDocker\u201d tab -> under Image Source -> select \u201cDocker Hub\u201d -> select your registry \n* Select \u201cReview + Create\u201d\n\nAnd the app is running on https:\/\/carclassifier.azurewebsites.net\n\n*Note: It may take around 10 minutes to load*\n\n*By the time this notebook is published, the app will be removed*","d0f11b64":"The image augmentations that I use are defined as follow:\n* Random Crop\n* Random Horizontal Flip\n* Color Jitter\n* Channel-wise data normalization\n\nSince the transformation will be applied randomly and dynamically each time a particular image is loaded, the model sees slightly different images in each epoch of training, which allows it generalize better.","6736a0cb":"## Streamlit","b53d1fef":"GradientShap is a linear explanation model which uses a distribution of reference samples (in this case two images) to explain predictions of the model. It computes the expectation of gradients for an input which was chosen randomly between the input and a baseline. The baseline is also chosen randomly from given baseline distribution.","6063f3f5":"Let\u2019s visualize a few training images so as to understand the data augmentations.\n","9bc9e895":"Unfortunately, there is no tensorboard on Kaggle.\n\nFrom the graphs above, we can see that after 10 epochs with freezed layers, our best accuracy was **73.12%**"}}