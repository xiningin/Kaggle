{"cell_type":{"c14243c5":"code","73b01880":"code","feebe052":"code","bc73a814":"code","d0e8e9b0":"code","b769fe2a":"code","ce5335bb":"code","d8e341b2":"code","3daebed0":"code","56662b2f":"code","dd788f31":"code","d52a24b6":"code","d27271aa":"code","90a24446":"code","d235908b":"code","727a0ed2":"code","8a7694ac":"code","26bda715":"code","b3512501":"code","dec48178":"code","9eac49eb":"code","d91060c6":"code","6aac1a1e":"code","3c95a885":"code","05d0e442":"code","2510523e":"code","11001bd6":"code","94c8c366":"code","3ea0e66e":"code","c972d977":"code","f864fd1a":"code","74b8f350":"code","79b39465":"code","daf305f9":"code","94b9695a":"code","44a2a78d":"code","76ddb3e2":"markdown","e447c342":"markdown","201419b3":"markdown","6d2a52f4":"markdown","1a2eb7f2":"markdown","9cfc0982":"markdown","736dc5db":"markdown","06a321a9":"markdown","4aa3c69e":"markdown","459c8552":"markdown","673a9f87":"markdown","99a9d1a2":"markdown","0adf7199":"markdown","9208cbaf":"markdown","a3012db7":"markdown","540773a2":"markdown","a7c743de":"markdown","df8df543":"markdown","bfb8022c":"markdown","6a9e7ced":"markdown","35fa8c4a":"markdown"},"source":{"c14243c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","73b01880":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport datetime\n\nimport plotly.express as px\nimport plotly.graph_objs as go\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","feebe052":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv')","bc73a814":"train.head()","d0e8e9b0":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n    return summary\n\n\n## Function to compute outliers for normally distributed column\ndef CalcOutliers(df_num): \n\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"Total percentual of Outliers: \", round((len(outliers_total) \/ len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n    \n    return","b769fe2a":"resumetable(train)","ce5335bb":"resumetable(test)","d8e341b2":"t1 = resumetable(train)\nt2 = resumetable(test)\n\nfeature_cols = [f for f in train.columns if f.startswith('feature')]\nt1_ = t1.loc[t1['Name'].isin(feature_cols), ['Name','Uniques']].rename({'Uniques':'Train_Uniques'}, axis=1)\nt2_ = t2.loc[t2['Name'].isin(feature_cols), ['Uniques']].rename({'Uniques':'Test_Uniques'}, axis=1)\nt_concat = pd.concat([t1_,t2_], axis=1)\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=t_concat['Name'], y=t_concat['Train_Uniques'],\n                marker_color='crimson',\n                name='Train Unique Count'))\nfig.add_trace(go.Bar(x=t_concat['Name'], y=t_concat['Test_Uniques'],\n                marker_color='lightslategrey',\n                name='Test Unique Count'\n                ))\n\nfig.update_layout(title='Unique Count acorss Dataset')\nfig.show()","3daebed0":"t_concat['Difference'] = t_concat['Train_Uniques']- t_concat['Test_Uniques']\nt_concat[t_concat['Difference']!=0].sort_values(by='Difference')\\\n            .plot(x='Name', y='Difference', kind='bar')\n\nplt.title('Count Difference between Train and Test Dataset', fontsize='16')\nplt.xlabel('Feature Names')\nplt.ylabel('Count')\nplt.tight_layout()","56662b2f":"f15 = np.setdiff1d(train['feature_15'].values, test['feature_15'].values)\nf46 = np.setdiff1d(train['feature_46'].values, test['feature_46'].values)\nf73 = np.setdiff1d(train['feature_73'].values, test['feature_73'].values)\nf28 = np.setdiff1d(train['feature_28'].values, test['feature_28'].values)\nf59 = np.setdiff1d(train['feature_59'].values, test['feature_59'].values)\nf60 = np.setdiff1d(train['feature_60'].values, test['feature_60'].values)\n\nprint(f\"feature_15 : There are {len(f15)} different values in Train dataset and these values are : {f15}\")\nprint(f\"feature_46 : There are {len(f46)} different values in Train dataset and these values are : {f46}\")\nprint(f\"feature_73 : There are {len(f73)} different values in Train dataset and these values are : {f73}\")\nprint(f\"feature_28 : There are {len(f28)} different values in Train dataset and these values are : {f28}\")\nprint(f\"feature_59 : There are {len(f59)} different values in Train dataset and these values are : {f59}\")\nprint(f\"feature_60 : There are {len(f60)} different values in Train dataset and these values are : {f60}\")","dd788f31":"arr_train = [train[col].values for col in feature_cols]\narr_test = [test[col].values for col in feature_cols]\n\ntrain_array = np.sort(np.concatenate(arr_train, axis=0))\ntest_array = np.sort(np.concatenate(arr_test, axis=0))\n\nunique1, counts1 = np.unique(train_array, return_counts=True)\nunique2, counts2 = np.unique(test_array, return_counts=True)\n\ndf_ = pd.DataFrame([unique1, counts1, unique2, counts2]).T\\\n                            .rename({0:\"Train Unique Value\", 1:\"Train Unique Count\", 2:\"Test Unique Value\", 3:\"Test Unique Count\"}, axis=1)","d52a24b6":"df_graph = df_.head(20)\nfig = go.Figure()\n# Create and style traces\nfig.add_trace(go.Scatter(x=df_graph['Train Unique Value'], y=df_graph['Train Unique Count'], name = 'Train Unique Count',\n                         line=dict(color='royalblue', width=4, dash='dashdot')))\nfig.add_trace(go.Scatter(x=df_graph['Train Unique Value'], y=df_graph['Test Unique Count'], name = 'Test Unique Count',\n                         line=dict(color='firebrick', width=4,\n                              dash='dash')))\n\nfig.update_layout(title='Top 10 Unique Count of Train and Test Dataset Values',\n                   xaxis_title='Unique Values in Dataset',\n                   yaxis_title='Unique Count')\n\nfig.show()","d27271aa":"train.drop('id', axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='PuRd')\\\n                            .background_gradient(subset=['max'], cmap='Greens')","90a24446":"test.drop('id', axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='PuRd')\\\n                            .background_gradient(subset=['max'], cmap='Greens')","d235908b":"numeric_features = train.select_dtypes('int64').columns.to_list()\nnumeric_features.remove('id')","727a0ed2":"plt.subplots(25,3, figsize=(25,60))\nplt.suptitle(\"Distribution of Numeric Features in Train Dataset\", fontsize=20, y=1)\n\nfor e, feature in enumerate(numeric_features,1):\n    plt.subplot(25,3,e)\n    sns.histplot(np.log1p(train[feature]), kde=True)\n        \n    plt.xlabel('Values', size=12, labelpad=15)\n    plt.ylabel('Count', size=12, labelpad=15)    \n    plt.title(f\"{feature.capitalize()}\", size=16)\n\nplt.tight_layout(h_pad=2)\nplt.show()","8a7694ac":"target_count = (train['target'].value_counts(normalize=True)*100).round(1).to_frame()\n\nplt.subplots(1,1, figsize=(15,7))\nplt.suptitle('Target Variable\/Classes Distribution', size=22)\n\nplt.subplot(1,1,1)\ng = sns.countplot(x='target', data=train, order=target_count.index)\ngt = g.twinx()\ngt = sns.pointplot(x= target_count.index, y= 'target', data=target_count, color='black')\ngt.set_ylabel(\"% of Class Distribution\", fontsize=16)\n\ng.set_xlabel('Classes', size=20, labelpad=15)\ng.set_ylabel('Count', size=20, labelpad=15)    \n\nsizes=[]\ntotal = len(train)\n\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format((height\/total)*100),\n            ha=\"left\", fontsize=14)\n\nplt.tight_layout()\nplt.show()","26bda715":"# Find correlations among feature columns \ncorr = train[feature_cols].corr()\n\n# Heatmap of correlations\nfig, ax = plt.subplots(figsize=(35,15))\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr,\n         square=True, center=0, linewidth=0.2,\n         cmap=sns.diverging_palette(240, 10, as_cmap=True),\n         mask=mask, ax=ax) \n\nax.set_title('Feature Correlation', loc='left', fontweight='bold')\nplt.show()","b3512501":"train_agg = train.groupby('target')[[col for col in train.columns if(col.startswith(\"feature\"))]].mean()\ntrain_agg","dec48178":"train_idmax = train_agg.idxmax(axis=1).to_frame().reset_index().rename({0:'Feature'}, axis=1)\ntrain_max = train_agg.max(axis=1).to_frame().reset_index(drop=True).rename({0:'Mean Value'}, axis=1)\n\ntrain_combined = pd.concat([train_idmax, train_max], axis=1)\ntrain_combined","9eac49eb":"import plotly.express as px\n\nfig = px.bar(train_combined, x='target', y='Mean Value',\n             hover_data=['Feature', 'Mean Value'], color='Feature',\n             labels={'pop':'Mean Value'}, height=400, title=\"Top Feature affecting the Target Variable\/Classes based on Mean\")\nfig.show()","d91060c6":"train_model = train.copy()\n\nfrom sklearn import preprocessing\n  \nlabel_encoder = preprocessing.LabelEncoder()\ntrain_model['target']= label_encoder.fit_transform(train_model['target'])\n\ny=train_model['target']","6aac1a1e":"X = train_model[feature_cols]\n\nX = StandardScaler().fit_transform(X)\nX","3c95a885":"from sklearn.decomposition import PCA\n\npca =  PCA(n_components= 30)\npca.fit(X)\npca_samples = pca.transform(X)","05d0e442":"ps = pd.DataFrame(pca_samples)\nps.head()","2510523e":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\n\nprint(f\"Train and Test Shape: \\n {[x.shape for x in [X_train, X_test, y_train, y_test]]}\")","11001bd6":"lr = LogisticRegression(penalty='l2',random_state=42)\n\nlr.fit(X_train, y_train)\n\n# Predict y_pred\ny_pred = lr.predict_proba(X_test)\nlog_loss_score_lr = log_loss(y_test,y_pred)\nprint(f\"Logistic Regression : {log_loss_score_lr}\")","94c8c366":"# Hyperparameter grid\nparam_grid = {\n    \"criterion\" : [\"gini\", \"entropy\"], \n    \"min_samples_leaf\" : [1, 5, 10], \n    \"min_samples_split\" : [2, 4, 10, 12, 16], \n    \"n_estimators\": [50, 100, 150, 200],\n    'max_depth': list(np.linspace(5, 30).astype(int)),\n    'max_leaf_nodes': list(np.linspace(10, 50, 500).astype(int)),\n    'bootstrap': [True, False]\n}\n\n# Estimator for use in random search\nrf = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)","3ea0e66e":"# Create the random search model\nrs = RandomizedSearchCV(rf, param_grid, n_jobs = -1, \n                        cv = 3, verbose = 1, random_state=1)","c972d977":"rs.fit(X_train, y_train)\n\n# Predict y_pred\ny_pred = rs.predict_proba(X_test)\nlog_loss_score_rs = log_loss(y_test,y_pred)\nprint(f\"RandomForestClassifier : {log_loss_score_rs}\")","f864fd1a":"test_model = test.drop('id', axis=1)\n\nX_test = test_model[feature_cols]\n\nX_test = StandardScaler().fit_transform(X_test)\nX_test","74b8f350":"y_pred_test_dataset = rs.predict_proba(X_test)\ny_pred_test_dataset","79b39465":"test_pred=pd.DataFrame(y_pred_test_dataset)\ntest_pred","daf305f9":"test_pred.columns = label_encoder.inverse_transform(test_pred.columns)\ntest_pred","94b9695a":"final_submiss = pd.concat([test.id, test_pred], axis=1)\nfinal_submiss","44a2a78d":"final_submiss.to_csv(\"result.csv\",index=False)","76ddb3e2":"## Model Development","e447c342":"#### Train Dataset","201419b3":"## Predict Test Dataset","6d2a52f4":"### PCA","1a2eb7f2":"## Unique Values Counts","9cfc0982":"### Desriptive Analysis","736dc5db":"# Loading Libraries","06a321a9":"Key Observation : \n- There are 6 features where there is a difference in unique value counts.  ","4aa3c69e":"### Numeric Features Distribution","459c8552":"# Sneak Peak into Dataset","673a9f87":"Key Observation : \n- There are few features where there is a difference in the unique values across Test and Train Dataset. Let's try to take a closer look at those features :","99a9d1a2":"### Final Submission","0adf7199":"### Bi-Variate Analysis","9208cbaf":"### Most Common Unique Values ","a3012db7":"### Target Variable Distribution","540773a2":"## Data Pre-processing","a7c743de":"Let's deep dive in feature_19 and feature_54 variable.","df8df543":"### Please Upvote if you like it !","bfb8022c":"## Handy Functions","6a9e7ced":"### Model 2 : Random Forest","35fa8c4a":"### Model 1 : Logistic Regression"}}