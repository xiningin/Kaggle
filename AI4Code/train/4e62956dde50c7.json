{"cell_type":{"991f58cb":"code","4f25396a":"code","42b81155":"code","0ca2caee":"code","a111109e":"code","bf231cd8":"code","38be176f":"code","fe98d5cb":"code","00d434cb":"code","90bb3cf5":"code","a8d0da22":"code","341fbb57":"code","f81f83b2":"code","81b6f0e8":"code","73da039f":"code","93a59fbc":"code","b75fc506":"code","b4300b68":"code","872182c8":"code","a1d9b52d":"code","22b6424f":"code","4d142220":"code","0ff14a6f":"code","c8a75b1c":"code","ef79f159":"markdown","8135c9da":"markdown","66ff8e2f":"markdown","a9c8765c":"markdown","656467c6":"markdown","08b5bb01":"markdown","b49ed371":"markdown","fcf0289f":"markdown","3cd3a2a4":"markdown","8b74419c":"markdown","dbfe36d5":"markdown","3f367c99":"markdown"},"source":{"991f58cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f25396a":"os.chdir('\/kaggle\/input\/predicting-energy-rating-from-raw-data')\ntrain_data = pd.read_csv('train_rating_eu.csv')\ntest_data = pd.read_csv('test_rating_eu.csv')\n\ntrain_data = train_data.drop(['building_id', 'site_id', 'Unnamed: 0'], axis=1)\ntest_data = test_data.drop(['building_id', 'site_id', 'Unnamed: 0'], axis=1)","42b81155":"import requests\n\nURL1 = 'https:\/\/platform.carbonculture.net\/communities\/ucl\/30\/apps\/assets\/list\/place\/'\nURL2 = 'https:\/\/platform.carbonculture.net\/places\/119-torrington-place\/1155\/'\nURL3 = 'https:\/\/platform.carbonculture.net\/about\/'\npage1 = requests.get(URL1)\npage2 = requests.get(URL2)\npage3 = requests.get(URL3)","0ca2caee":"f = open('\/kaggle\/working\/output1', 'wb')\nf.write(page1.content)\nf = open('\/kaggle\/working\/output2', 'wb')\nf.write(page2.content)\nf = open('\/kaggle\/working\/output3', 'wb')\nf.write(page3.content)","a111109e":"from bs4 import BeautifulSoup\n\nsoup1 = BeautifulSoup(page1.content, 'html.parser')\nsoup2 = BeautifulSoup(page2.content, 'html.parser')\nsoup3 = BeautifulSoup(page3.content, 'html.parser')","bf231cd8":"places_elems = soup3.find_all('a', href=True)\nplaces_elems","38be176f":"i = 0\nplaces = []\n\nfor place in places_elems:\n    if i > 10 and i < 23:\n        if place.text: \n            places.append(place['href'])\n    i = i + 1\n    \nplaces","fe98d5cb":"urls=[]\nbase = 'https:\/\/platform.carbonculture.net'\nend = 'apps\/assets\/list\/place\/'\n\nfor p in places:\n    urls.append((base + p + end))\n    \nurls","00d434cb":"soups=[]\n\nfor u in urls:\n    p = requests.get(u)\n    soups.append(BeautifulSoup(p.content, 'html.parser'))","90bb3cf5":"titles = []\ntitles_whole = []\n\nfor soup in soups:\n    url_elems = soup.find_all(href=True)\n    for elem in url_elems:\n        if elem.text: \n            if elem['href'].find('places') == 1:\n                titles.append(elem['href'])\n                \nfor t in titles:\n    titles_whole.append(base + t)","a8d0da22":"years = []\nfloors = []\nheating = []\noccupants = []\ni = 0\n\nfor w in titles_whole:\n    test = requests.get(w)\n    soup_test = BeautifulSoup(test.content, 'html.parser')\n    test_elems = soup_test.find_all('li', class_='assets-meta__list-item')\n    if len(test_elems) == 0:\n        print(w)\n        years.append('-')\n        floors.append(-1)\n        heating.append('-')\n        occupants.append(-1)\n    for elem in test_elems:\n        a = str(elem.find('span'))[6:-7]\n        if i == 0:\n            years.append(a)\n        elif i == 1:\n            floors.append(a)\n        elif i == 3:\n            heating.append(a)\n        elif i == 4:\n            occupants.append(a)\n            i = -1\n        i = i + 1","341fbb57":"i = 0\nj = 0\ntitle = []\nconsumption = []\nfloor_area = []\nconsumption_area = []\nrating = []\n\nfor soup in soups:\n    url = titles_whole[i]\n    page = requests.get(url)\n    soup_title = BeautifulSoup(page.content, 'html.parser')\n    table_elems = soup.find_all('td')\n    for elem in table_elems:\n        if i != 5 and i != 6 and i != 7:\n            elem = str(elem)\n            elem = elem[4:-5]\n            if i == 0:\n                elem = elem[-11:-8]\n                if elem != 'N\/A':\n                    elem = elem[2:]\n                rating.append(elem)\n            if i == 1:\n                title.append(elem)\n            elif i == 2:\n                elem = elem[:-9]\n                if len(elem) == 1:\n                    elem = '-1'\n                elem = elem.replace(',', '')\n                consumption.append(elem)\n            elif i == 3:\n                elem = elem[:-9]\n                if len(elem) == 1:\n                    elem = '-1'\n                elem = elem.replace(',', '')\n                floor_area.append(elem)\n            elif i == 4:\n                elem = elem[:-9]\n                if len(elem) == 1:\n                    elem = '-1'\n                elem = elem.replace(',', '')\n                consumption_area.append(elem)\n            j = j + 1\n        if i == 7:\n            i = -1\n        i = i + 1\n","f81f83b2":"i = 0\nnull = occupants[320]\n\nfor o in occupants:\n    if str(o) == null:\n        occupants[i] = -1\n    i = i + 1\n\ni = 0\nnull = floors[33]\nfor f in floors:\n    if str(f) == null:\n        print('here!')\n        floors[i] = -1\n    elif len(str(f)) > 3:\n        seq_type= type(f)\n        f = seq_type().join(filter(seq_type.isdigit, f))\n        floors[i] = f\n    i = i + 1","81b6f0e8":"consumption = list(map(float, consumption))\nconsumption_area = list(map(float, consumption_area))\nfloor_area = list(map(float, floor_area))\noccupants = list(map(float, occupants))\nfloors = list(map(float, floors))","73da039f":"columns = ['sqm', 'building', 'energy consumption', 'energy consumption per area', 'year built', 'floors', 'no. occupants', 'main heating type', 'rating2']\ndf = pd.DataFrame(columns=columns)","93a59fbc":"df['sqm'] = floor_area\ndf['building'] = title\ndf['energy consumption'] = consumption\ndf['energy consumption per area'] = consumption_area\ndf['year built'] = years\ndf['floors'] = floors\ndf['no. occupants'] = occupants\ndf['main heating type'] = heating\ndf['rating2'] = rating","b75fc506":"df['sqm'] = df['sqm'].replace(-1, np.nan)\ndf['energy consumption'] = df['energy consumption'].replace(-1, np.nan)\ndf['energy consumption per area'] = df['energy consumption per area'].replace(-1, np.nan)\ndf['rating2'] = df['rating2'].replace('N\/A', np.nan)\ndf['floors'] = df['floors'].replace(-1, np.nan)\ndf['no. occupants'] = df['no. occupants'].replace(-1, np.nan)\ndf['year built'] = df['year built'].replace('-', np.nan)\ndf['main heating type'] = df['main heating type'].replace('-', np.nan)\ndf","b4300b68":"df.info()","872182c8":"train1 = df[df['rating2'].notna()]\ntrain1.info()","a1d9b52d":"test1 = df[df['rating2'].isnull()]\ntest1 = test1.drop('rating2', axis=1)\ntest1.info()","22b6424f":"merge1 = pd.merge(left=train_data, right=train1, how='outer', left_on='sqm', right_on='sqm')\nmerge1.info()","4d142220":"merge2 = pd.merge(left=test_data, right=test1, how='outer', left_on='sqm', right_on='sqm')\nmerge2.info()","0ff14a6f":"sqm1 = train_data['sqm']\nsqm2 = train1['sqm']\nyear1 = train_data['yearbuilt']\nyear2 = train1['year built']\n\ntrain_intersection = list(set(sqm1) & set(sqm2))\nyear_intersection = list(set(year1) & set(year2))\n\n\nprint(\"There are \", len(train_intersection), \" sqm matches!\")\nprint(\"There are \", len(year_intersection), \" year matches!\")","c8a75b1c":"sqm1 = test_data['sqm']\nsqm2 = test1['sqm']\nyear1 = test_data['yearbuilt']\nyear2 = test1['year built']\n\ntrain_intersection = list(set(sqm1) & set(sqm2))\nyear_intersection = list(set(year1) & set(year2))\n\n\nprint(\"There are \", len(train_intersection), \" sqm matches!\")\nprint(\"There are \", len(year_intersection), \" year matches!\")","ef79f159":"Here I read in the existing train and test datasets created from performing EDA on the metadata dataset","8135c9da":"Here I am merging our new train and test dataframes obtained from the above web scraping with our original train and test dataframes from the metadata dataset.","66ff8e2f":"The following gathers data from the above created list of urls. The urls used correspond to the specific buildings and were obtained above by looping through the 12 general location urls and creating a new list containing each individual building url. There are multiple buildings reported for each location. Specifically, it scrapes data pertaining the year built, number of floors, number of occupants, and main heating type","a9c8765c":"The following creates a list of urls that correspond to the 12 different locations reported in this website. Each url contains information about all of the buildings reported for each individual location.","656467c6":"From the above scraped data, we can make our new dataframe:","08b5bb01":"The following block of code changes the 'floors' data into a usable format: getting rid of any unneccessary text and converting any missing values into -1 so that the list can be converted to a float later on.","b49ed371":"The below analysis of our merge yields the following question: if this dataset supposedly already includes data from this website, why do only 35 of the sqm's match up for the train dataset and 12 for the test dataset?\n\n","fcf0289f":"Converting all of the numeric data into float lists:","3cd3a2a4":"From this dataframe, we can split it into a train and test by segmenting the values with and without a rating, respectively.","8b74419c":"# Web Scraping\nIn this notebook, I used the carbonculture website in order to scrape building data for all 435 locations reported on this website. This data can be integrated into the Building Data Urban Genome 2 Project as a form of feature engineering in order to improve the accuracy of different models. My goal in this was to improve the performance of a classification model used to predict the energy rating on a scale of A - G for European buildings.","dbfe36d5":"These are the main pages that I will be gathering information from. URL3 is the home page, URL1 is a list of buildings at one of the 12 locations included in this website, and URL2 is a webpage for a specific location","3f367c99":"The following gathers data from the table on the general location page (which lists each building reported at that location). From this table I scraped the name of the building, annual energy consumption, annual energy consumption per area, rating, and usable floor area (sqm). The loop runs 12 times (12 locations) and the nested loop runs through each individual building in the table (however many buildings are reported at that location)"}}