{"cell_type":{"b004a0cd":"code","653da40b":"code","993037cf":"code","b1ee79b9":"code","a257564c":"code","f9470cce":"code","978ae5ac":"code","6d3d23cf":"code","a0a2f749":"code","891f9b3f":"code","dd28a4dc":"code","026357c3":"code","b38b7ead":"markdown","55f60278":"markdown","e2c7596d":"markdown","5dbdd0b3":"markdown","65c79f41":"markdown","9800cdcb":"markdown","7d6df3c7":"markdown","e2014365":"markdown","9075618b":"markdown","6fca4071":"markdown","b23f32c7":"markdown","3442b97f":"markdown"},"source":{"b004a0cd":"import numpy as np\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sys\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\nimport sklearn","653da40b":"data = pd.read_csv('..\/input\/creditcard.csv')","993037cf":"print(data.columns)","b1ee79b9":"data.shape","a257564c":"# random_state helps assure that you always get the same output when you split the data\n# this helps create reproducible results and it does not actually matter what the number is\n# frac is percentage of the data that will be returned\ndata = data.sample(frac = 0.2, random_state = 1)\nprint(data.shape)","f9470cce":"# plot the histogram of each parameter\ndata.hist(figsize = (20, 20))\nplt.show()","978ae5ac":"# determine the number of fraud cases\nfraud = data[data['Class'] == 1]\nvalid = data[data['Class'] == 0]\n\noutlier_fraction = len(fraud) \/ float(len(valid))\nprint(outlier_fraction)\n\nprint('Fraud Cases: {}'.format(len(fraud)))\nprint('Valid Cases: {}'.format(len(valid)))","6d3d23cf":"# correlation matrix\ncorrmat = data.corr()\nfig = plt.figure(figsize = (12, 9))\n\nsns.heatmap(corrmat, vmax = .8, square = True)\nplt.show()","a0a2f749":"# get the columns from the dataframe\ncolumns = data.columns.tolist()\n\n# filter the columns to remove the data we do not want\ncolumns = [c for c in columns if c not in ['Class']]\n\n# store the variable we will be predicting on which is class\ntarget = 'Class'\n\n# X includes everything except our class column\nX = data[columns]\n# Y includes all the class labels for each sample\n# this is also one-dimensional\nY = data[target]\n\n# print the shapes of X and Y\nprint(X.shape)\nprint(Y.shape)","891f9b3f":"from sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor","dd28a4dc":"# define a random state\nstate = 1\n\n# define the outlier detection methods\nclassifiers = {\n    # contamination is the number of outliers we think there are\n    'Isolation Forest': IsolationForest(max_samples = len(X),\n                                       contamination = outlier_fraction,\n                                       random_state = state),\n    # number of neighbors to consider, the higher the percentage of outliers the higher you want to make this number\n    'Local Outlier Factor': LocalOutlierFactor(\n    n_neighbors = 20,\n    contamination = outlier_fraction)\n}","026357c3":"n_outliers = len(fraud)\n\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    \n    # fit the data and tag outliers\n    if clf_name == 'Local Outlier Factor':\n        y_pred = clf.fit_predict(X)\n        scores_pred = clf.negative_outlier_factor_\n    else:\n        clf.fit(X)\n        scores_pred = clf.decision_function(X)\n        y_pred = clf.predict(X)\n        \n    # reshape the prediction values to 0 for valid and 1 for fraud\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n\n    # calculate the number of errors\n    n_errors = (y_pred != Y).sum()\n    \n    # classification matrix\n    print('{}: {}'.format(clf_name, n_errors))\n    print(accuracy_score(Y, y_pred))\n    print(classification_report(Y, y_pred))","b38b7ead":"## Imports ","55f60278":"## Exploring the Dataset","e2c7596d":"## Data Importing","5dbdd0b3":"You can see most of the V's are clustered around 0 with some or no outliers. Notice we have very few fraudulent cases over valid cases in our class histogram.","65c79f41":"## Applying Algorithms","9800cdcb":"You can see a lot of the values are close to 0 . Most of them are fairly unrelated. The lighter squares signify a stronger correlation. ","7d6df3c7":"## Fit the Model","e2014365":"Our Isolation Forest method (which is Random Forest based) was able to produce a better result. Looking at the f1-score 26% (or approx. 30%) of the time we are going to detect the fraudulent transactions.","9075618b":"Looking at precision for fraudulent cases (1) lets us know the percentage of cases that are getting correctly labeled. 'Precision' accounts for false-positives. 'Recall' accounts for false-negatives. Low numbers could mean that we are constantly calling clients asking them if they actually made the transaction which could be annoying.\n\nGoal: To get better percentages.","6fca4071":"## Organizing the Data","b23f32c7":"#  <font color=green>Credit Card Fraud Detection Project<\/font>","3442b97f":"Hello!\n\nI am really excited about machine-learning and decided to take on this project as the first of many to get more comfortable the models used. \n\nThis project covers credit card fraud and is meant to look at a dataset of transactions and predict whether it is fraudulent or not. I learned alot of this from \nEduonix Learning Solutions. "}}