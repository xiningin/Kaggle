{"cell_type":{"8b269805":"code","ea86a62c":"code","9438d6b7":"code","c303c90f":"code","4c6881cf":"code","d98ffae1":"code","3625169b":"code","b5d8ce27":"code","eee7f7e4":"code","ce37af81":"code","c4ad361f":"code","b7cf8a20":"code","96d19143":"code","33b6926c":"code","978b91e3":"code","d242e4e2":"code","a85ca581":"code","1bcf9011":"code","d33f43de":"code","14bffd28":"code","1eae7cde":"code","fdec5362":"code","1e121798":"code","8169de20":"code","8597ac60":"code","0cf44b4b":"code","3300e2db":"code","6de1d9ca":"code","0d91eac7":"code","0a49e7e4":"code","cee75bfe":"code","04e50b4b":"code","7a8eb033":"code","503c63ab":"code","b23f457e":"code","7e4e86fa":"code","dac78850":"code","29d29d84":"code","fb07b9f4":"code","bef28660":"code","e8c3f6de":"code","7a499b83":"code","4b710e8f":"code","be5442b9":"code","03ef7d60":"code","b2882b8d":"code","c4d99a5b":"code","2c3b4ee5":"code","fbfddb77":"code","80bf63b4":"code","4d22c0ed":"code","176d24f8":"code","d8d94c09":"code","f5d7318c":"code","e6abbefd":"code","6736e055":"code","7a03103c":"code","333f4768":"markdown","c0e3603c":"markdown","6dacaf34":"markdown","3853ed1e":"markdown","5a19ae9c":"markdown","1a5f9d87":"markdown","4c5c0201":"markdown","90737b17":"markdown","a84c6a70":"markdown","4b034306":"markdown","c0196082":"markdown"},"source":{"8b269805":"import numpy as np\nimport pandas as pd","ea86a62c":"df = pd.read_csv(\"..\/input\/stock-headlines\/Stock Headlines.csv\", encoding = 'ISO-8859-1')","9438d6b7":"df.columns","c303c90f":"df.shape","4c6881cf":"df.head(3)","d98ffae1":"# Importing essential libraries for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3625169b":"# Visualizing the count of 'Label' column from the dataset\nplt.figure(figsize=(8,8))\nsns.countplot(x='Label', data=df)\nplt.xlabel('Stock Sentiments (0-Down\/Same, 1-Up)')\nplt.ylabel('Count')\nplt.show()","b5d8ce27":"print(df.shape)","eee7f7e4":"# Finding any NaN values\ndf.isna().any()","ce37af81":"# Dropping NaN values\ndf.dropna(inplace=True)\nprint(df.shape)","c4ad361f":"df_copy = df.copy()","b7cf8a20":"df_copy.reset_index(inplace=True)","96d19143":"# Splitting the dataset into train an test set\ntrain = df_copy[df_copy['Date'] < '20150101']\ntest = df_copy[df_copy['Date'] > '20141231']\nprint('Train size: {}, Test size: {}'.format(train.shape, test.shape))","33b6926c":"train.columns","978b91e3":"# Splitting the dataset\ny_train = train['Label']\ntrain = train.iloc[:, 3:28]\ny_test = test['Label']\ntest = test.iloc[:, 3:28]","d242e4e2":"# Importing essential libraries for performing Natural Language Processing on given dataset\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer","a85ca581":"# Removing punctuation and special character from the text\ntrain.replace(to_replace='[^a-zA-Z]', value=' ', regex=True, inplace=True)\ntest.replace(to_replace='[^a-zA-Z]', value=' ', regex=True, inplace=True)","1bcf9011":"# Renaming columns\nnew_columns = [str(i) for i in range(0,25)]\ntrain.columns = new_columns\ntest.columns = new_columns","d33f43de":"# Converting the entire text to lower case\nfor i in new_columns:\n  train[i] = train[i].str.lower()\n  test[i] = test[i].str.lower()","14bffd28":"# Joining all the columns\ntrain_headlines = []\ntest_headlines = []\n\nfor row in range(0, train.shape[0]):\n  train_headlines.append(' '.join(str(x) for x in train.iloc[row, 0:25]))\n\nfor row in range(0, test.shape[0]):\n  test_headlines.append(' '.join(str(x) for x in test.iloc[row, 0:25]))","1eae7cde":"train_headlines[0]","fdec5362":"test_headlines[0]","1e121798":"# Creating corpus of train dataset\nps = PorterStemmer()\ntrain_corpus = []\n\nfor i in range(0, len(train_headlines)):\n  \n  # Tokenizing the news-title by words\n  words = train_headlines[i].split()\n\n  # Removing the stopwords\n  words = [word for word in words if word not in set(stopwords.words('english'))]\n\n  # Stemming the words\n  words = [ps.stem(word) for word in words]\n\n  # Joining the stemmed words\n  headline = ' '.join(words)\n\n  # Building a corpus of news-title\n  train_corpus.append(headline)","8169de20":"# Creating corpus of test dataset\ntest_corpus = []\n\nfor i in range(0, len(test_headlines)):\n  \n  # Tokenizing the news-title by words\n  words = test_headlines[i].split()\n\n  # Removing the stopwords\n  words = [word for word in words if word not in set(stopwords.words('english'))]\n\n  # Stemming the words\n  words = [ps.stem(word) for word in words]\n\n  # Joining the stemmed words\n  headline = ' '.join(words)\n\n  # Building a corpus of news-title\n  test_corpus.append(headline)","8597ac60":"train_corpus[0:10]","0cf44b4b":"test_corpus[0:10]","3300e2db":"down_words = []\nfor i in list(y_train[y_train==0].index):\n  down_words.append(train_corpus[i])\n\nup_words = []\nfor i in list(y_train[y_train==1].index):\n  up_words.append(train_corpus[i])","6de1d9ca":"# Creating wordcloud for down_words\nfrom wordcloud import WordCloud\nwordcloud1 = WordCloud(background_color='white', width=3000, height=2500).generate(down_words[1])\nplt.figure(figsize=(8,8))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.title(\"Words which indicate a fall in DJIA \")\nplt.show()","0d91eac7":"# Creating wordcloud for up_words\nwordcloud2 = WordCloud(background_color='white', width=3000, height=2500).generate(up_words[5])\nplt.figure(figsize=(8,8))\nplt.imshow(wordcloud2)\nplt.axis('off')\nplt.title(\"Words which indicate a rise in DJIA \")\nplt.show()","0a49e7e4":"# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=10000, ngram_range=(2,2))\nX_train = cv.fit_transform(train_corpus).toarray()","cee75bfe":"X_test = cv.transform(test_corpus).toarray()","04e50b4b":"from sklearn.linear_model import LogisticRegression\nlr_classifier = LogisticRegression()\nlr_classifier.fit(X_train, y_train)","7a8eb033":"lr_y_pred = lr_classifier.predict(X_test)","503c63ab":"# Accuracy, Precision and Recall\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nscore1 = accuracy_score(y_test, lr_y_pred)\nscore2 = precision_score(y_test, lr_y_pred)\nscore3 = recall_score(y_test, lr_y_pred)\nprint(\"---- Scores ----\")\nprint(\"Accuracy score is: {}%\".format(round(score1*100,2)))\nprint(\"Precision score is: {}\".format(round(score2,2)))\nprint(\"Recall score is: {}\".format(round(score3,2)))","b23f457e":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nlr_cm = confusion_matrix(y_test, lr_y_pred)","7e4e86fa":"lr_cm","dac78850":"# Plotting the confusion matrix\nplt.figure(figsize=(10,7))\nsns.heatmap(data=lr_cm, annot=True, cmap=\"Blues\", xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\nplt.xlabel('Predicted values')\nplt.ylabel('Actual values')\nplt.title('Confusion Matrix for Logistic Regression Algorithm')\nplt.show()","29d29d84":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators=100, criterion='entropy')\nrf_classifier.fit(X_train, y_train)","fb07b9f4":"rf_y_pred = rf_classifier.predict(X_test)","bef28660":"# Accuracy, Precision and Recall\nscore1 = accuracy_score(y_test, rf_y_pred)\nscore2 = precision_score(y_test, rf_y_pred)\nscore3 = recall_score(y_test, rf_y_pred)\nprint(\"---- Scores ----\")\nprint(\"Accuracy score is: {}%\".format(round(score1*100,2)))\nprint(\"Precision score is: {}\".format(round(score2,2)))\nprint(\"Recall score is: {}\".format(round(score3,2)))","e8c3f6de":"# Making the Confusion Matrix\nrf_cm = confusion_matrix(y_test, rf_y_pred)","7a499b83":"rf_cm","4b710e8f":"# Plotting the confusion matrix\nplt.figure(figsize=(10,7))\nsns.heatmap(data=rf_cm, annot=True, cmap=\"Blues\", xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\nplt.xlabel('Predicted values')\nplt.ylabel('Actual values')\nplt.title('Confusion Matrix for Random Forest Algorithm')\nplt.show()","be5442b9":"from sklearn.naive_bayes import MultinomialNB\nnb_classifier = MultinomialNB()\nnb_classifier.fit(X_train, y_train)","03ef7d60":"# Predicting the Test set results\nnb_y_pred = nb_classifier.predict(X_test)","b2882b8d":"# Accuracy, Precision and Recall\nscore1 = accuracy_score(y_test, nb_y_pred)\nscore2 = precision_score(y_test, nb_y_pred)\nscore3 = recall_score(y_test, nb_y_pred)\nprint(\"---- Scores ----\")\nprint(\"Accuracy score is: {}%\".format(round(score1*100,2)))\nprint(\"Precision score is: {}\".format(round(score2,2)))\nprint(\"Recall score is: {}\".format(round(score3,2)))","c4d99a5b":"# Making the Confusion Matrix\nnb_cm = confusion_matrix(y_test, nb_y_pred)","2c3b4ee5":"nb_cm","fbfddb77":"# Plotting the confusion matrix\nplt.figure(figsize=(10,7))\nsns.heatmap(data=nb_cm, annot=True, cmap=\"Blues\", xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\nplt.xlabel('Predicted values')\nplt.ylabel('Actual values')\nplt.title('Confusion Matrix for Multinomial Naive Bayes Algorithm')\nplt.show()","80bf63b4":"import re\n\ndef stock_prediction(sample_news):\n  sample_news = re.sub(pattern='[^a-zA-Z]',repl=' ', string=sample_news)\n  sample_news = sample_news.lower()\n  sample_news_words = sample_news.split()\n  sample_news_words = [word for word in sample_news_words if not word in set(stopwords.words('english'))]\n  ps = PorterStemmer()\n  final_news = [ps.stem(word) for word in sample_news_words]\n  final_news = ' '.join(final_news)\n\n  temp = cv.transform([final_news]).toarray()\n  return lr_classifier.predict(temp)","4d22c0ed":"# For generating random integer\nfrom random import randint","176d24f8":"sample_test = df_copy[df_copy['Date'] > '20141231']","d8d94c09":"sample_test.reset_index(inplace=True)\nsample_test = sample_test['Top1']","f5d7318c":"# Predicting values\nrow = randint(0,sample_test.shape[0]-1)\nsample_news = sample_test[row]\n\nprint('News: {}'.format(sample_news))\nif stock_prediction(sample_news):\n  print('Prediction: The stock price will remain the same or will go down.')\nelse:\n  print('Prediction: The stock price will go up!')","e6abbefd":"# Predicting values\nrow = randint(0,sample_test.shape[0]-1)\nsample_news = sample_test[row]\n\nprint('News: {}'.format(sample_news))\nif stock_prediction(sample_news):\n  print('Prediction: The stock price will remain the same or will go down.')\nelse:\n  print('Prediction: The stock price will go up!')","6736e055":"# Predicting values\nrow = randint(0,sample_test.shape[0]-1)\nsample_news = sample_test[row]\n\nprint('News: {}'.format(sample_news))\nif stock_prediction(sample_news):\n  print('Prediction: The stock price will remain the same or will go down.')\nelse:\n  print('Prediction: The stock price will go up!')","7a03103c":"# Predicting values\nrow = randint(0,sample_test.shape[0]-1)\nsample_news = sample_test[row]\n\nprint('News: {}'.format(sample_news))\nif stock_prediction(sample_news):\n  print('Prediction: The stock price will remain the same or will go down.')\nelse:\n  print('Prediction: The stock price will go up!')","333f4768":"# **Model Building**","c0e3603c":"# **Predictions**","6dacaf34":"![1*oPXEHY-EvtAu_bmdqtzWHA.jpeg](https:\/\/miro.medium.com\/max\/11520\/1*oPXEHY-EvtAu_bmdqtzWHA.jpeg)","3853ed1e":"*Note: Here 'Label' is a binary attribute which consists 0 - Stock \nprice goes down or stays the same, 1 - Stock price goes up.*","5a19ae9c":"## *Logistic Regression*","1a5f9d87":"## *Random Forest Classifier*","4c5c0201":"# **Exploring the dataset**","90737b17":"## *Multinomial Naive Bayes*","a84c6a70":"\n<h1 style=\"background-color:#FD0E35\n;font-family:newtimeroman;font-size:225%;text-align:center;border-radius: 15px 50px;\"> \ud83d\udcc8 Sentiment Analysis - Dow Jones (DJIA) Stock using News Headlines \ud83d\udcc8<\/h1><a id=0><\/a>\n\n","4b034306":"# **Data Cleaning and Preprocessing**","c0196082":"# **Loading the data**"}}