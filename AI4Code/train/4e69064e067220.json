{"cell_type":{"74a87df0":"code","9672c6f9":"code","9eba2283":"code","885ea49b":"code","53860b8b":"code","897283ac":"code","277221ee":"code","11a41d42":"code","6860d629":"code","2a1dd430":"code","3c1a08f0":"code","0f5b265e":"code","9c6a0092":"code","1e60414b":"code","20789fae":"code","4142cfbe":"code","5abfa019":"code","b0369c38":"code","1b421dd8":"code","76e8fe36":"code","a107dd18":"code","30112ef6":"code","c0d2566a":"code","bc0410ae":"code","eb8386f0":"code","7547c857":"code","f2a9d85c":"code","2f4a2370":"code","1f81060e":"code","6a49721e":"code","c8f39948":"code","37ddbc58":"code","ee858688":"code","e8546c73":"code","5bd7e221":"code","1fae8440":"code","79c24419":"code","51a813fc":"code","d1f468f5":"code","ca2bde81":"code","0a1f8b6a":"code","5d2f78f5":"code","78ec1e5a":"markdown","916d4253":"markdown","da5fba84":"markdown","29894c39":"markdown","c093c5af":"markdown","61a617c0":"markdown","7610c74f":"markdown","aaee0c1b":"markdown","7a2129df":"markdown","063487f8":"markdown","b248c6d1":"markdown","d07852eb":"markdown","08f1dc47":"markdown","db837e4c":"markdown","d34b4f73":"markdown","7b023d6b":"markdown","a24e7953":"markdown","5a444c8b":"markdown","d0229536":"markdown","8a0c61ba":"markdown","076f5e5a":"markdown","3a4e14b7":"markdown","eaf46648":"markdown","9167640c":"markdown","99d8765d":"markdown","9844637e":"markdown","1d2c4bbe":"markdown","31a633ff":"markdown","10580110":"markdown","41d50689":"markdown","4d91df68":"markdown","48729f14":"markdown","b21268c0":"markdown","c281bcae":"markdown","3e2cb2a4":"markdown"},"source":{"74a87df0":"# Gerkli K\u00fct\u00fcphaneler ve Ayarlar\n\nimport numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom sklearn.metrics import mean_absolute_error\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.api as sm\nimport itertools\n\nimport warnings\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","9672c6f9":"train = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/train.csv', parse_dates=['date'])\ntest = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/test.csv', parse_dates=['date'])\ndf = pd.concat([train, test], sort=False) # veri \u00f6ni\u015fleme i\u00e7in test ve train bir araya getirdim\ndf.head()","9eba2283":"print(\"Train setinin boyutu:\",train.shape)\nprint(\"Test setinin boyutu:\",test.shape)","885ea49b":"df.shape","53860b8b":"df.quantile([0, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 1]).T # Ayk\u0131r\u0131l\u0131k g\u00f6z\u00fckm\u00fcyor.","897283ac":"df[\"date\"].min()","277221ee":"df[\"date\"].max()","11a41d42":"# Sat\u0131\u015f da\u011f\u0131l\u0131m\u0131 nas\u0131l?\ndf[\"sales\"].describe([0.10, 0.30, 0.50, 0.70, 0.80, 0.90, 0.95, 0.99])","6860d629":"# Ka\u00e7 store var?\ndf[\"store\"].nunique()","2a1dd430":"# Ka\u00e7 \u00fcr\u00fcn var?\ndf[\"item\"].nunique() ","3c1a08f0":"# Her store'da e\u015fit say\u0131da m\u0131 e\u015fsiz \u00fcr\u00fcn var?\ndf.groupby([\"store\"])[\"item\"].nunique()","0f5b265e":"# Ma\u011faza-\u00fcr\u00fcn k\u0131r\u0131l\u0131m\u0131nda sat\u0131\u015f istatistikleri\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","9c6a0092":"# hangi ayda sat\u0131\u015f yap\u0131lm\u0131\u015f\ndf['month'] = df.date.dt.month\n# ay\u0131n hangi g\u00fcn\u00fcnde sat\u0131\u015f yap\u0131lm\u0131\u015f\ndf['day_of_month'] = df.date.dt.day\n# y\u0131l\u0131n hangi g\u00fcn\u00fcnde sat\u0131\u015f yap\u0131lm\u0131\u015f\ndf['day_of_year'] = df.date.dt.dayofyear \n# y\u0131l\u0131n hangi haftas\u0131nda sat\u0131\u015f yap\u0131lm\u0131\u015f\ndf['week_of_year'] = df.date.dt.weekofyear\n# haftan\u0131n hangi g\u00fcn\u00fcnde sat\u0131\u015f yap\u0131lm\u0131\u015f\ndf['day_of_week'] = df.date.dt.dayofweek\n# hangi y\u0131lda sat\u0131\u015f yap\u0131lm\u0131\u015f\ndf['year'] = df.date.dt.year\n# haftasonu mu de\u011fil mi\ndf[\"is_wknd\"] = df.date.dt.weekday \/\/ 4\n# ay\u0131n ba\u015flang\u0131c\u0131 m\u0131\ndf['is_month_start'] = df.date.dt.is_month_start.astype(int)\n# ay\u0131n biti\u015fi mi\ndf['is_month_end'] = df.date.dt.is_month_end.astype(int) ","1e60414b":"df.head()","20789fae":"# store-item-month k\u0131r\u0131l\u0131m\u0131nda sat\u0131\u015f istatistikleri\ndf.groupby([\"store\", \"item\", \"month\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","4142cfbe":"# Bu veri seti gibi k\u00fc\u00e7\u00fck veri k\u00fcmeleri i\u00e7in, overfittingi \u00f6nlemek amac\u0131yla de\u011ferlere rastgele g\u00fcr\u00fclt\u00fc eklenebilir.\n# Ben burada normal olarak 1 standart sapma ve 0 ortalama ile da\u011f\u0131lan Gauss rastgele g\u00fcr\u00fclt\u00fcs\u00fcn\u00fc ekleyece\u011fim.\n\ndef random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))","5abfa019":"df.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)\ndf.head(10)","b0369c38":"def lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe\n\ndf = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])","1b421dd8":"def roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(\n            dataframe)\n    return dataframe\n\n\ndf = roll_mean_features(df, [365, 546, 730])\n","76e8fe36":"def ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\n\nalphas = [0.99, 0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n\ndf = ewm_features(df, alphas, lags)","a107dd18":"df.tail()","30112ef6":"# De\u011fi\u015fken t\u00fcretme i\u015flerim bitti \u015fimdi ML haz\u0131rl\u0131klar\u0131na ba\u015fl\u0131yorum.\ndf = pd.get_dummies(df, columns=['day_of_week', 'month'])","c0d2566a":"df['sales'] = np.log1p(df[\"sales\"].values)","bc0410ae":"# 2017'nin ba\u015f\u0131na kadar (2016'n\u0131n sonuna kadar) train seti.\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\n\n# 2017'nin ilk 3'ay\u0131 validasyon seti.\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :]\n\n# ba\u011f\u0131ms\u0131z de\u011fi\u015fkenler\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]","eb8386f0":"# train seti i\u00e7in ba\u011f\u0131ml\u0131 de\u011fi\u015fkenin se\u00e7ilmesi\nY_train = train['sales']\n\n# train seti i\u00e7in ba\u011f\u0131ms\u0131z de\u011fi\u015fkenin se\u00e7ilmesi\nX_train = train[cols]\n\n# validasyon seti i\u00e7in ba\u011f\u0131ml\u0131 de\u011fi\u015fkenin se\u00e7ilmesi\nY_val = val['sales']\n\n# validasyon seti i\u00e7in ba\u011f\u0131ms\u0131z de\u011fi\u015fkenin se\u00e7ilmesi\nX_val = val[cols] \n\n# kontrol\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape","7547c857":"# Custom Cost Function\n\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num \/ denom)) \/ n\n    return smape_val\n\n\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False\n","f2a9d85c":"# LightGBM parameters\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 10000, \n              'early_stopping_rounds': 200,\n              'nthread': -1}","2f4a2370":"lgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape, # haty\u0131 g\u00f6zlemliyoruz\n                  verbose_eval=100)\n\ny_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n\n# validasyon hatas\u0131n\u0131n y\u00fczdesi\nsmape(np.expm1(y_pred_val), np.expm1(Y_val))","1f81060e":"# test ve train ba\u011f\u0131ml\u0131\/ba\u011f\u0131ms\u0131z de\u011fi\u015fkenlerinin belirlenmesi\n\ntrain = df.loc[~df.sales.isna()]\nY_train = train['sales']\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]","6a49721e":"lgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}\n\n# LightGBM dataset\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\ntest_preds = model.predict(X_test, num_iteration=model.best_iteration)","c8f39948":"forecast = pd.DataFrame({\"date\":test[\"date\"],\n                        \"store\":test[\"store\"],\n                        \"item\":test[\"item\"],\n                        \"sales\":test_preds\n                        })\n\nforecast[(forecast.store == 1) & (forecast.item == 1)].set_index(\"date\").sales.plot(color = \"green\",\n                                                                                    figsize = (20,9),\n                                                                                    legend=True, label = \"Store 1 Item 1 Forecast\");\n","37ddbc58":"train[(train.store == 1) & (train.item == 17)].set_index(\"date\").sales.plot(figsize = (20,9),legend=True, label = \"Store 1 Item 17 Sales\")\nforecast[(forecast.store == 1) & (forecast.item == 17)].set_index(\"date\").sales.plot(legend=True, label = \"Store 1 Item 17 Forecast\");","ee858688":"data = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/train.csv', parse_dates=['date'])\ndata.head()\ndata.shape\n\n# veri setini haftal\u0131\u011fa indirgeme\ndata.set_index(\"date\",inplace=True)\ndf= data.resample(\"W\").mean()\ndf.reset_index(inplace=True)\ndf.head()\ndf.shape\n\ndf.index.freq = \"W\"\ndf.head()","e8546c73":"df.shape","5bd7e221":"# hangi ayda sat\u0131\u015f yap\u0131lm\u0131\u015f\ndf['month'] = df.date.dt.month\n# ay\u0131n hangi g\u00fcn\u00fcnde sat\u0131\u015f yap\u0131lm\u0131\u015f\ndf['day_of_month'] = df.date.dt.day\n# y\u0131l\u0131n hangi g\u00fcn\u00fcnde sat\u0131\u015f yap\u0131lm\u0131\u015f\ndf['day_of_year'] = df.date.dt.dayofyear\n# y\u0131l\u0131n hangi haftas\u0131nda sat\u0131\u015f yap\u0131lm\u0131\u015f\ndf['week_of_year'] = df.date.dt.weekofyear\n# haftan\u0131n hangi g\u00fcn\u00fcnde sat\u0131\u015f yap\u0131lm\u0131\u015f\ndf['day_of_week'] = df.date.dt.dayofweek\n# hangi y\u0131lda sat\u0131\u015f yap\u0131lm\u0131\u015f\ndf['year'] = df.date.dt.year\n# haftasonu mu de\u011fil mi\ndf[\"is_wknd\"] = df.date.dt.weekday \/\/ 4\n# ay\u0131n ba\u015flang\u0131c\u0131 m\u0131\ndf['is_month_start'] = df.date.dt.is_month_start.astype(int)\n# ay\u0131n biti\u015fi mi\ndf['is_month_end'] = df.date.dt.is_month_end.astype(int)\n\n# Lag\/Shifted Features (Gecikmeler)\ndef lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe\n\ndf = lag_features(df, [31, 61, 91, 98, 105, 112])\n\n\n# Rolling Mean Features (Hareketli Ortalamalar)\ndef roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(\n            dataframe)\n    return dataframe\n\n\ndf = roll_mean_features(df, [31, 61, 91, 98, 105, 112])\n\n\n# Exponentially Weighted Mean Features (\u00dcssel A\u011f\u0131rl\u0131kl\u0131 Ortalama Featurs\ndef ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\n\nalphas = [0.99, 0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [10, 20, 30, 40, 50]\n\ndf = ewm_features(df, alphas, lags)\n\ndf.tail()\n","1fae8440":"# One-Hot Encoding\ndf = pd.get_dummies(df, columns=['day_of_week', 'month'])\n\n# Converting sales to log(1+sales)\ndf['sales'] = np.log1p(df[\"sales\"].values)\n\n# train-test seti\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\ntest = df.loc[(df[\"date\"] >= \"2017-01-01\"), :]\n\n# ba\u011f\u0131ml\u0131 ve ba\u011f\u0131ms\u0131z de\u011fi\u015fkenler\ncols = [col for col in train.columns if col not in ['date', \"sales\", \"year\"]]\nX_train = train[cols]\nY_train = train['sales']\nX_test = test[cols]\nY_test = test[\"sales\"]\n","79c24419":"# LightGBM parameters\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 10000, \n              'early_stopping_rounds': 200, \n              'nthread': -1}\n\nlgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_test, label=Y_test, reference=lgbtrain, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape, \n                  verbose_eval=100)\n\ny_pred_test = model.predict(X_test, num_iteration=model.best_iteration)\n\n# test hatas\u0131n\u0131n y\u00fczdesi\nsmape(np.expm1(y_pred_test), np.expm1(Y_test))\n","51a813fc":"def plot_lgb_importances(model, plot=False, num=10):\n\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n\n\nplot_lgb_importances(model, num=30)\nplot_lgb_importances(model, num=30, plot=True)\n\nlgb.plot_importance(model, max_num_features=20, figsize=(10, 10), importance_type=\"gain\")\nplt.show()\n","d1f468f5":"# Final Model\n\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}\n\n# LightGBM dataset\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\ntest_preds = model.predict(X_test, num_iteration=model.best_iteration)\n","ca2bde81":"# 1 y\u0131ll\u0131k ger\u00e7ek ve tahmin edilen de\u011ferler\nforecast = pd.DataFrame({\"date\":test[\"date\"],\n                        \"store\":test[\"store\"],\n                        \"item\":test[\"item\"],\n                        \"sales\":test_preds\n                        })\n\ndf.set_index(\"date\").sales.plot(figsize = (20,9),legend=True, label = \"Actual\")\nforecast.set_index(\"date\").sales.plot(legend=True, label = \"Predict\")\nplt.show()","0a1f8b6a":"df.index.freq = \"W\"\ndff = pd.DataFrame({\"date\":df[\"date\"], \"sales\":df[\"sales\"]})\ndff.index = dff[\"date\"]\ndff.drop(\"date\",axis=1, inplace=True)\ndff.head()\n\n# Dura\u011fanl\u0131k Testi (Dickey-Fuller Testi)\ndef is_stationary(y):\n\n    p_value = sm.tsa.stattools.adfuller(y)[1]\n    if p_value < 0.05:\n        print(F\"Result: Stationary (H0: non-stationary, p-value: {round(p_value, 3)})\")\n    else:\n        print(F\"Result: Non-Stationary (H0: non-stationary, p-value: {round(p_value, 3)})\")\n\n# Zaman Serisi Bile\u015fenleri ve Dura\u011fanl\u0131k Testi\ndef ts_decompose(y, model=\"additive\", stationary=False):\n    result = seasonal_decompose(y, model=model)\n    fig, axes = plt.subplots(4, 1, sharex=True, sharey=False)\n    fig.set_figheight(10)\n    fig.set_figwidth(15)\n\n    axes[0].set_title(\"Decomposition for \" + model + \" model\")\n    axes[0].plot(y, 'k', label='Original ' + model)\n    axes[0].legend(loc='upper left')\n\n    axes[1].plot(result.trend, label='Trend')\n    axes[1].legend(loc='upper left')\n\n    axes[2].plot(result.seasonal, 'g', label='Seasonality & Mean: ' + str(round(result.seasonal.mean(), 4)))\n    axes[2].legend(loc='upper left')\n\n    axes[3].plot(result.resid, 'r', label='Residuals & Mean: ' + str(round(result.resid.mean(), 4)))\n    axes[3].legend(loc='upper left')\n    plt.show(block=True)\n\n    if stationary:\n        is_stationary(y)\n\nts_decompose(dff, model=\"additive\")","5d2f78f5":"train_dff = dff.loc[(dff.index < \"2017-01-01\"), :]\ntest_dff = dff.loc[(dff.index >= \"2017-01-01\"), :]\n\ndff.index.freq = \"W\"\n\ndef ses_optimizer(train, alphas, step=48):\n    best_alpha, best_mae = None, float(\"inf\")\n    for alpha in alphas:\n        ses_model = SimpleExpSmoothing(train).fit(smoothing_level=alpha)\n        y_pred = ses_model.forecast(step)\n        mae = mean_absolute_error(test_dff, y_pred)\n        if mae < best_mae:\n            best_alpha, best_mae = alpha, mae\n        print(\"alpha:\", round(alpha, 2), \"mae:\", round(mae, 4))\n    print(\"best_alpha:\", round(best_alpha, 2), \"best_mae:\", round(best_mae, 4))\n    return best_alpha, best_mae\n\nalphas = np.arange(0.01, 1, 0.10)\nbest_alpha, best_mae = ses_optimizer(train_dff, alphas, step=53)\n\nses_model = SimpleExpSmoothing(train_dff).fit(smoothing_level=best_alpha)\ny_pred = ses_model.forecast(53)\n\ndef plot_prediction(y_pred, label):\n    train_dff[\"sales\"].plot(legend=True, label=\"TRAIN\")\n    test_dff[\"sales\"].plot(legend=True, label=\"TEST\")\n    y_pred.plot(legend=True, label=\"PREDICTION\")\n    plt.title(\"Train, Test and Predicted Test Using \"+label)\n    plt.show()\n\nplot_prediction(y_pred, \"Single Exponential Smoothing\")\n","78ec1e5a":"### 1. ma\u011fazan\u0131n 1.\u00fcr\u00fcnleri i\u00e7in yap\u0131lan tahminler","916d4253":"## 5. TIME SERIES PART\n\n* Bu b\u00f6l\u00fcmde \u00f6ncelikle train veri seti haftal\u0131\u011fa indirgenecektir.\n* Daha sonra haftal\u0131\u011fa indirgenmi\u015f veri seti kullan\u0131larak s\u0131ras\u0131yla :\n* LightGBM Model \n* Single Exponential Smoothing\n* Double Exponential Smoothing\n* Triple Exponential Smoothing\n* ARIMA\n* SARIMA ile 2017 y\u0131l\u0131na ait sat\u0131\u015f talep tahmin modelleri olu\u015fturulacakt\u0131r.\n* Ger\u00e7ek de\u011ferler ile tahmin edilen de\u011ferler kar\u015f\u0131la\u015ft\u0131r\u0131lacakt\u0131r.\n","da5fba84":"![5.png](attachment:7316eb0f-2ae9-4cda-9977-9312b97644ee.png)","29894c39":"![4.png](attachment:c95d1e30-cc34-4dea-ae5d-8e165b5c59be.png)","c093c5af":"### Converting sales to log(1+sales)","61a617c0":"### One-Hot Encoding","7610c74f":"## 4. MODEL (3 Ayl\u0131k Talep Tahmini i\u00e7in)","aaee0c1b":"## 3. Feature Engineering","7a2129df":"### SARIMA","063487f8":"### Exponentially Weighted Mean Features (\u00dcssel A\u011f\u0131rl\u0131kl\u0131 Ortalama Featurelar\u0131)","b248c6d1":"### LightGBM Model","d07852eb":"#### De\u011fi\u015fkenler\n* date \u2013 Sat\u0131\u015f verilerinin tarihi Tatil efekti veya ma\u011faza kapan\u0131\u015f\u0131 yoktur.\n* store \u2013 Ma\u011faza ID\u2019si Her bir ma\u011faza i\u00e7in e\u015fsiz numara.\n* item \u2013 \u00dcr\u00fcn ID\u2019si Her bir \u00fcr\u00fcn i\u00e7in e\u015fsiz numara.\n* sales \u2013 Sat\u0131lan \u00fcr\u00fcn say\u0131lar\u0131, Belirli bir tarihte belirli bir ma\u011fazadan sat\u0131lan \u00fcr\u00fcnlerin say\u0131s\u0131","08f1dc47":"### Final Model","db837e4c":"### Time Series Analysis","d34b4f73":"![image.png](attachment:4a6a78a0-80e9-4774-a01d-fb6b3f319bc4.png)","7b023d6b":"### Feature Importance","a24e7953":"### Random Noise","5a444c8b":"### Lag\/Shifted Features (Gecikmeler)","d0229536":"### Double Exponential Smoothing","8a0c61ba":"### 1.Ma\u011fazan\u0131n 17. \u00dcr\u00fcn\u00fc \u0130\u00e7in Tahmin Edilen De\u011ferler","076f5e5a":"### Feature Engineering","3a4e14b7":"#### Veri Setine Genel Bak\u0131\u015f\n* Bir ma\u011faza zincirinin 5 y\u0131ll\u0131k verilerinde, 10 farkl\u0131 ma\u011faza ve 50 farkl\u0131 \u00fcr\u00fcn\u00fcn bilgileri yer almaktad\u0131r.\n* Veri seti 01-01-2013 ile 31-12-2017 aras\u0131ndaki d\u00f6nemi kapsamaktad\u0131r.","eaf46648":"### Single Exponential Smoothing (SES)","9167640c":"### ARIMA","99d8765d":"## 2. Load Dataset and EDA","9844637e":"## Content\n1. [Introduction](#section-intro)\n2. [Load Dataset and EDA](#section-ts)\n3. [Feature Engineering](#section-pro)\n    * [Random Noise](#section-one)\n    * [Lag\/Shifted Features ](#section-two)\n    * [Rolling Mean Features ](#section-three)    \n    * [Exponentially Weighted Mean Features ](#section-four)\n4. [MODEL](#section-ten)\n    * [One-Hot Encoding](#section-eleven)\n    * [Converting sales to log(1+sales)](#section-eleven)\n    * [LightGBM Model](#section-twelve) \n    * [Custom Cost Function](#section-thirteen)\n    * [Final Model](#section-fourteen)\n    * [1.Ma\u011fazan\u0131n 1. \u00dcr\u00fcn\u00fc i\u00e7in Yap\u0131lan Tahminler](#section-fifteen)\n    * [1.Ma\u011fazan\u0131n 17. \u00dcr\u00fcn\u00fc i\u00e7in Tahmin Edilen De\u011ferler](#section-fifteen)\n5. [TIME SERIES PART](#section-ten)\n    * [Load Dataset and EDA](#section-eleven)\n    * [Feature Engineering](#section-eleven)\n    * [LightGBM Model](#section-eleven)\n    * [Feature Importance](#section-eleven)\n    * [Time Series Analysis](#section-eleven)\n    * [Single Exponential Smoothing ](#section-eleven)\n    * [Double Exponential Smoothing](#section-eleven)\n    * [Triple Exponential Smoothing ](#section-eleven)\n    * [ARIMA](#section-eleven)\n    * [SARIMA](#section-eleven)\n    ","1d2c4bbe":"![2.png](attachment:fd0e6c8d-cd68-4350-845b-ddfd3b0e8461.png)","31a633ff":"![3.png](attachment:851e0d8b-7153-4914-9ecb-36082966361d.png)","10580110":"### LightGBM Model","41d50689":"### Rolling Mean Features (Hareketli Ortalamalar)","4d91df68":"### Triple Exponential Smoothing ","48729f14":"## Demand Forecasting with LightGBM, SES, DES, TES, ARIMA and SARIMA","b21268c0":"### Load Dataset and EDA","c281bcae":"## 1. Introduction","3e2cb2a4":"#### \u0130\u015f Problemi\n* Bir ma\u011faza zincirinin 10 farkl\u0131 ma\u011fazas\u0131 ve 50 farkl\u0131 \u00fcr\u00fcn\u00fc i\u00e7in 3 ayl\u0131k bir talep tahmini modeli olu\u015fturulmak istenmektedir.\n* Daha sonra ise veri seti haftal\u0131\u011fa indirgenip 2017 y\u0131l\u0131 i\u00e7in bir talep tahmin modeli olu\u015fturulmak istenmektedir."}}