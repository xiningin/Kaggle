{"cell_type":{"b7344782":"code","73f26a79":"code","e806fc48":"code","b475ad8d":"code","5056f3d5":"code","375d033c":"code","ca65a658":"code","29e80103":"code","1f7017a6":"code","acc89ea5":"code","52e4f447":"code","f862072a":"code","3426aa31":"code","3ce8ae0d":"code","9a8a566c":"code","b3e4f57c":"code","379dc65c":"code","b9ead20f":"code","99701aa5":"code","72cf1087":"code","b7d2ff2f":"code","27781411":"code","7f85e339":"code","24cbc580":"code","2de522ac":"code","b303f4b2":"code","594122da":"code","a7c0648b":"code","ff0d7c35":"code","e0de684c":"code","605daf6f":"code","6652b4ed":"code","8c9ccfc3":"code","50e4a4b8":"code","7e757037":"code","00e1a587":"code","6dc43e0f":"code","db845ddd":"code","1841f657":"code","e3268985":"code","abba12f2":"code","b6eadcbf":"code","d0787dbe":"code","34069e17":"code","1611b547":"code","4e22fac2":"code","562140a5":"code","534538c4":"markdown","0f255d2d":"markdown","cabda14f":"markdown","59d120cc":"markdown","7294d4c1":"markdown","42c77fff":"markdown","a618df9c":"markdown"},"source":{"b7344782":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import zscore\nfrom sklearn.linear_model import Lasso\nfrom sklearn.cluster import KMeans\nimport copy\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","73f26a79":"conc_Data = pd.read_csv('concrete.csv')","e806fc48":"conc_Data.shape","b475ad8d":"conc_Data.info()","5056f3d5":"conc_Data.head()","375d033c":"conc_Data.describe().T","ca65a658":"# zero values present for ash ,slag and superplastic\n# Looking at description of data we can see there are outliers present in data","29e80103":"# DIST plot to check gaissians and get idea how attributes are distributed\n\nfig, axes = plt.subplots(nrows=3, ncols=3,figsize=(20,20))\nfor i, column in enumerate(conc_Data.columns): sns.distplot(conc_Data[column],ax=axes[i\/\/3,i%3],kde=True)","1f7017a6":"# Check outliers presence using box plot\nfig, axes = plt.subplots(nrows=3, ncols=3,figsize=(20,20))\n\nfor i, column in enumerate(conc_Data.columns): \n    sns.boxplot(conc_Data[column],ax=axes[i\/\/3,i%3])","acc89ea5":"# outliers present for water,superplastic,slag and fineagg age","52e4f447":"\nsns.pairplot(conc_Data)","f862072a":"# Get idea how independent attributes are releated with target variable usin corelation matrix\ncorrelation_matrix = conc_Data.corr() \n\nfig, ax = plt.subplots(figsize=(10,10)) \n\nsns.heatmap(correlation_matrix, annot=True, ax=ax)  \n\nplt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns) \n\nplt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns) ","3426aa31":"# looking at coorelation matrix we cant drop any column directly,there seems to be some relation ,\n# We ned to apply model and see which features are important\n","3ce8ae0d":"\nfor i in conc_Data.columns:\n    q1, q2, q3 = conc_Data[i].quantile([0.25,0.5,0.75])\n    IQR = q3 - q1\n    a = conc_Data[i] > q3 + 1.5*IQR\n    b = conc_Data[i] < q1 - 1.5*IQR\n    conc_Data[i] = np.where(a | b, q2, conc_Data[i]) ","9a8a566c":"# scaling data to apply clusetering ,lassso\nconc_Data_Z = conc_Data.apply(zscore) ","b3e4f57c":"# a. Identify opportunities (if any) to create a composite feature, drop a\n# feature etc.\n# Applying lasso regression to get idea of which attributes can be convert to approx zero coefficients \n","379dc65c":"X_scaled = conc_Data_Z.drop('strength',axis = 1)\ny_scaled = conc_Data_Z['strength']","b9ead20f":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.30, random_state=1)","99701aa5":"lasso = Lasso(alpha=0.1)\nlasso.fit(X_train,y_train)\nprint (\"Lasso model:\", (lasso.coef_))","72cf1087":"# lasso result: as number of attributes are less lasso model tried to make coeffiecient zero for some feature,\n# but these can not be dropped directly as number of attributes are less \n# it has made zero value for coeffiecient for ash,coarseagg,fineagg","b7d2ff2f":"# make composit of independent attributes,and apply model with more number of attributes\n# with more degree\npolynomial_results = pd.DataFrame(columns={'Degree','Training data Accuracy','Testing data Accuracy'})\ndef get_polynomial_results(deg):\n    polynomial = PolynomialFeatures(degree = deg, interaction_only=True)\n    X_scaled_P = polynomial.fit_transform(X_scaled)\n    X_train_P, X_test_P, y_train, y_test = train_test_split(X_scaled_P, y_scaled, test_size=0.30, random_state=1)\n    \n    \n    lasso = Lasso(alpha=0.1)\n    lasso.fit(X_train_P,y_train)\n\n#     regression_model = LinearRegression()\n#     regression_model.fit(X_train_P, y_train)\n    \n    return {'Degree':deg,'Training data Accuracy':lasso.score(X_train_P, y_train),'Testing data Accuracy':lasso.score(X_test_P, y_test)}\n","27781411":"for deg in np.arange(1,6):\n    accuracy = get_polynomial_results(deg)\n    polynomial_results = polynomial_results.append(accuracy,ignore_index=True)\npolynomial_results","7f85e339":"# Explore for gaussians\n","24cbc580":"cluster_range = range(1,15)\ncluster_errors = []\nfor num_clusters in cluster_range:\n    clusters = KMeans(num_clusters, n_init = 5)\n    clusters.fit(conc_Data_Z)\n    labels = clusters.labels_\n    centroids = clusters.cluster_centers_\n    cluster_errors.append(clusters.inertia_)\n\nclusters_df = pd.DataFrame({\"num_clusters\": cluster_range, \"cluster_errors\": cluster_errors})\nclusters_df[0:15]","2de522ac":"from matplotlib import cm\n\nplt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","b303f4b2":"# crating 6 clusters\nkmeans = KMeans(n_clusters=6, n_init = 5, random_state=12345)\nkmeans.fit(conc_Data_Z)","594122da":"labels = kmeans.labels_\ncounts = np.bincount(labels[labels>=0])\nprint(counts)","a7c0648b":"cluster_labels = pd.DataFrame(kmeans.labels_ , columns = list(['labels']))\ncluster_labels['labels'] = cluster_labels['labels'].astype('category')\nconc_Data_labeled = conc_Data.join(cluster_labels)\n\nconc_Data_labeled.boxplot(by = 'labels',  layout=(3,3), figsize=(30, 20))","ff0d7c35":"# feature importance\ndTree = DecisionTreeRegressor(random_state=12)\ndTree.fit(X_train,y_train)\nprint(dTree.score(X_train,y_train))\nprint(dTree.score(X_test,y_test))","e0de684c":"print (pd.DataFrame(dTree.feature_importances_, columns = [\"Imp\"], index = X_train.columns))","605daf6f":"conc_Data_Z_modified =  conc_Data_Z.drop(['coarseagg','ash','superplastic'],axis=1)","6652b4ed":"# with less features\nX_mod_scaled = conc_Data_Z_modified.drop('strength',axis = 1)\ny_mod_scaled = conc_Data_Z_modified['strength']\n\nX_mod_train, X_mod_test, y_train, y_test = train_test_split(X_mod_scaled, y_mod_scaled, test_size=0.30, random_state=1)\n\ndTree = DecisionTreeRegressor(random_state=12)\ndTree.fit(X_mod_train,y_train)\nprint(dTree.score(X_mod_train,y_train))\nprint(dTree.score(X_mod_test,y_test))","8c9ccfc3":"# Still we are getting 79% of accuracy after dropping some attributes","50e4a4b8":"# 4. Tuning","7e757037":"rf = RandomForestRegressor(random_state = 1)\n","00e1a587":"rf.get_params","6dc43e0f":"n_estimators = np.arange(10,100,20)\nmax_depth = np.arange(3,9,3)\nmax_features = ['auto', 'log2']\nmin_samples_leaf = np.arange(1,5)\nbootstrap = [True, False]\n# np.arange(5,50,5)","db845ddd":"random_grid =  {'n_estimators' : n_estimators,\n               'max_depth': max_depth,\n               'max_features' : max_features,\n                'min_samples_leaf':min_samples_leaf,\n               'bootstrap' : bootstrap}","1841f657":"rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n                              n_iter = 5, scoring='neg_mean_absolute_error', \n                              cv = 3, verbose=2, random_state=42, n_jobs=-1,\n                              return_train_score=True)","e3268985":"rf_random.fit(X_mod_train, y_train);","abba12f2":"rf_random.best_params_","b6eadcbf":"best_random = rf_random.best_estimator_","d0787dbe":"print(best_random.score(X_mod_train , y_train))\nprint(best_random.score(X_mod_test , y_test))","34069e17":"# Performance of our model has been improved by applying random search CV and also it is giving good accracy oon both training and testing data\n","1611b547":"# Applying k fold validation ","4e22fac2":"num_folds = 10\nseed = 7\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel = best_random\nresults = cross_val_score(model, X_mod_scaled, y_mod_scaled, cv=kfold)","562140a5":"print(results)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100.0, results.std()*100.0))","534538c4":"Suitable algorithms for project:\n    1.Linear Regression\n    2.DecisionTree regression\n    3.RandomForest Regressor\n    4.GradientBoosting Regressor\n    5.Bagging Regressor","0f255d2d":"Outliers treament","cabda14f":"#### 1. a.Exploratory data quality report\n#### a. Univariante analysis","59d120cc":"No distinct clusters are there as for all attributes cluster quartiles are ovrlapping\nWhich means there will be no gain to apply model into varipus clusters ","7294d4c1":"Findings for Important features :\n1. Looks like slag,ash and super plastic are not adding much value as independent variables\n2. We can compare same with result of lasso model where also model converted coefficients to zero\n3. Among all attributes cement,water and age are important features in data","42c77fff":"#### Summary:\n\n1. With confidance interval of 95 % we can say that with using random forest classifiers we can get acc 82.51% with std deviation 2.59% \n2. In Provided data there were zero values present but we could not drop these data directy or replace by mean or median\n3. By exploring clusters we didnt find any clear groups for which we can apply models separately for better results\n4. Applying random grid search CV and K Fold improved the proformance of results without making model underfit or overfit","a618df9c":"##### bi variante analysis\n"}}