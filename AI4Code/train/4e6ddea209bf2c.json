{"cell_type":{"011a0e3d":"code","7367b044":"code","85b2c2ec":"code","2ddf580b":"code","1cd3b91d":"code","165ddfd9":"code","c49b8569":"code","2099fed2":"code","c4f68a6e":"code","7c54a716":"code","e8437b2c":"code","7d94e33a":"code","cd53a969":"code","d6fb061c":"code","8d9bf7b1":"code","2b1fce75":"code","f31027fa":"code","6c4d77ea":"code","f9bec7bc":"code","cc6e0084":"code","39bf5e40":"code","b2ec5b8f":"code","92788003":"code","993f7122":"code","142df2c6":"code","17df5f69":"code","b5e43e51":"code","9900c1a9":"code","2342e157":"code","0587bf7f":"code","daac698d":"code","6aa874af":"code","925ce37a":"code","4cf1a07a":"code","b135ab4b":"code","8850b4fe":"code","c647164c":"code","36a95b1f":"code","48950af4":"code","17ce4d90":"code","d79a65e1":"code","59dda298":"code","f05bf0ba":"code","31644bf6":"code","78b3b688":"code","a3fab144":"code","56f89400":"code","2570f94e":"code","3b91cc9c":"code","d30f353e":"code","d68a366c":"code","9b928d72":"code","efc0d90d":"code","a87f7331":"code","f400701a":"code","65a1c830":"code","992b73f9":"code","bd96cf17":"code","9e5ceebd":"code","c98c6b72":"code","f4d5e474":"code","c56c8676":"code","236365e6":"code","6af4f6fb":"code","1efc1261":"code","cccfbca1":"code","73a45525":"code","01806a38":"code","693cf308":"code","f4437543":"code","25ab5d01":"code","c52db54a":"code","a2381b85":"code","a92af8ce":"code","24826e42":"code","ae1bc5af":"code","ca5d9831":"code","874a1702":"code","d0825f5b":"code","44ab5267":"code","62e21578":"code","96f535bc":"code","92fe39ea":"code","bfe95c17":"code","1dabea29":"code","fbda3319":"code","e5be04ac":"code","ea02a94b":"code","4c9c1b2a":"code","cbd13714":"code","086da942":"code","7da43640":"code","23fc7cb1":"code","3c1dde0f":"code","e645f5cc":"code","3787c4f5":"code","8d4aea12":"code","ac0b8add":"code","26a16ae5":"code","c205f43e":"code","26d4d312":"code","7ff4fa47":"code","d9a60ffe":"code","be573130":"code","e597a5eb":"code","40c47218":"code","e4f19b83":"code","2b104dc7":"code","7da05c64":"code","1b92e08f":"code","f0d059c3":"code","7f59773b":"code","32ecff68":"code","679fe21d":"code","b1a15cfe":"code","6862f1b0":"code","96a07499":"code","404e26a8":"code","722f31f2":"code","bf287746":"code","63aef39c":"code","a58a6a4c":"code","a293b6f0":"code","db443861":"code","15ba6090":"code","3e69e1a8":"code","492da2b4":"code","6c91c70e":"code","7054028a":"code","03ab7c0a":"code","142037b5":"code","e014b442":"code","b0e4898b":"code","584936a1":"code","ac58454e":"code","e4828975":"code","b07e7f01":"code","acb5d55f":"code","507423b1":"code","f3cdf442":"code","ff61458f":"code","61f80a77":"code","06ad0e05":"code","3545768f":"code","dd891c98":"code","38ca6362":"code","74b876d6":"code","7509e608":"code","7acee8a3":"code","ab3d0942":"code","a5978968":"code","fed65666":"code","ac332aad":"code","45b10e06":"markdown","81e250da":"markdown","fb3aba56":"markdown","edf6667e":"markdown","1c9fa472":"markdown","b2dd4fa7":"markdown","6efd3c83":"markdown","21887370":"markdown","c4f9608a":"markdown","20121613":"markdown","e1ae7568":"markdown","4506bf3c":"markdown","f69b5543":"markdown","223d0958":"markdown","3f649eb9":"markdown","c43fcc7c":"markdown","f5750e0c":"markdown","4dbf4ef6":"markdown","1f6ab2ce":"markdown","52bdcd49":"markdown","4331c49c":"markdown","8ce1bc47":"markdown","9b50045f":"markdown","d47bf27b":"markdown","91d03579":"markdown","702b1f5d":"markdown","5f42c6a7":"markdown","b994b308":"markdown","f4f6ce01":"markdown","82bd5854":"markdown","29d939df":"markdown","7e41fa41":"markdown","f560f77b":"markdown","ba08dcd5":"markdown","69d559c4":"markdown","44c6c305":"markdown","83094ff3":"markdown","9b49c9d0":"markdown","3cedec9f":"markdown","913d4ebc":"markdown","362de57b":"markdown","3ae03e13":"markdown","b18af792":"markdown","2ec36703":"markdown","9b2b51ce":"markdown"},"source":{"011a0e3d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7367b044":"import warnings\nwarnings.filterwarnings('ignore')","85b2c2ec":"train_data_raw = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data_raw = pd.read_csv('..\/input\/titanic\/test.csv')","2ddf580b":"train_data_raw.sample(5)","1cd3b91d":"columns_all = train_data_raw.columns\ncolumns_all","165ddfd9":"train_data_raw.info()","c49b8569":"train_data_raw.hist(figsize=(9,9))\nplt.tight_layout()","2099fed2":"train_data_raw['Survived'].value_counts().apply(lambda x:f'{x} ({x*100\/len(train_data_raw):0.2f}%)')","c4f68a6e":"from sklearn.model_selection import train_test_split","7c54a716":"train_set,test_set = train_test_split(train_data_raw,test_size=0.2,random_state=21)","e8437b2c":"train_original = train_set.copy()\ntrain_set.reset_index(drop=True,inplace=True)","7d94e33a":"train_set.sample(5)","cd53a969":"train_set.describe()","d6fb061c":"# No. of unique elements in each column\ntrain_set.apply(lambda x: x.nunique())","8d9bf7b1":"num_cols = train_set.select_dtypes('number').columns.drop(['PassengerId','Survived','Pclass']).to_numpy()\ncat_cols = list(train_set.select_dtypes('object').columns.drop(['Name']))\ncat_cols.extend(['Pclass'])\nprint(\"Numerical Columns : \",num_cols)\nprint(\"Categorical Columns : \",cat_cols)","2b1fce75":"n_def_num_cols = len(num_cols)\nfig,ax = plt.subplots(round(n_def_num_cols\/2),2,figsize=(10,n_def_num_cols*2))\nfor i,col in enumerate(num_cols):\n  sns.violinplot(x='Survived',y=col,data=train_set,ax=ax.ravel()[i],orient='v',cut=0)\nfig.tight_layout()","f31027fa":"fig,ax = plt.subplots(round(n_def_num_cols\/2),2,figsize=(16,n_def_num_cols*2))\nfor i,col in enumerate(num_cols):\n  sns.histplot(x=col,data=train_set,hue='Survived',multiple='dodge',ax=ax.ravel()[i],bins=20,lw=1)\nfig.tight_layout()","6c4d77ea":"sns.pairplot(hue='Survived',data=train_set,corner=True)\nplt.tight_layout()","f9bec7bc":"train_corr = train_set.corr()","cc6e0084":"plt.subplots(figsize=(8,7))\nsns.heatmap(train_corr,vmax=1,vmin=-1,annot=True,cmap=sns.color_palette(\"icefire\", as_cmap=True))\nplt.tight_layout()","39bf5e40":"print(\"Correlation of Features with 'Survived' \\n\")\ntrain_corr.loc[:,'Survived'].sort_values(ascending=False).drop('Survived')","b2ec5b8f":"print(\"Correlation within Features  \\n\")\nfor i,y in enumerate(train_corr.index):\n    for j,x in enumerate(train_corr.columns.drop('Survived')):\n        if(j<i):\n            continue\n        if ((train_corr.loc[x,y] >0.4) or (train_corr.loc[x,y] <-0.4)) and x!=y:\n            print(f'{x} - {y}  : {train_corr.loc[x,y]}')","92788003":"for i,col in enumerate(['Pclass','Sex','Embarked','Survived']):\n  j=0\n  fig,ax = plt.subplots(1,3,figsize=(16,4),)\n  for col1 in ['Pclass','Sex','Embarked','Survived']:\n    if col1!=col:    \n      sns.countplot(x=col,data=train_set,hue=col1,ax=ax[j])\n      j=j+1\n  \n  fig.suptitle(col,size=16)\n  fig.tight_layout()","993f7122":"from scipy.stats import chi2_contingency","142df2c6":"alpha = 0.05\nfor col in cat_cols:\n  cross_table = pd.crosstab(train_set[col],train_set['Survived'])\n  chi2_stat,p_value, dof, exp = chi2_contingency(cross_table)\n  if p_value <= alpha:\n    print(f\"{col}-Survived \\np-value : \",p_value)\n    print(\"Dependent (reject H0)\",'\\n')\n  else:\n    print(f\"{col}-Survived \\np-value : \",p_value)\n    print(\"Independent (fail to reject H0)\",'\\n')","17df5f69":"train_set.dtypes","b5e43e51":"# Modifying DataType\n#\ntrain_set.loc[:,cat_cols] = train_set[cat_cols].astype('category',errors='ignore')\ntrain_set.loc[:,'PassengerId'] = train_set[['PassengerId']].astype('object',errors='ignore')","9900c1a9":"def missing_count(data,cols=None):\n  print(\"Number of Instances : \",len(data))\n  print(\"Number of Missing Values in :\")\n  df = pd.DataFrame(data)\n  if cols==None:\n    cols=df.columns\n  for x in cols:\n    count  = df[x].isna().sum()\n    if count >=1:\n      print(f' - {x} : {count}({count*100\/len(df):0.2f}%)')\n\n","2342e157":"missing_count(train_set)","0587bf7f":"train_set.dropna(subset=['Embarked'],inplace=True)\ntrain_set.reset_index(drop=True,inplace=True)","daac698d":"pclass_avg_age = train_set.groupby(['Pclass'])['Age'].median()\npclass_avg_age","6aa874af":"pd.Series(train_set.columns)","925ce37a":"train_set.Age = train_set.apply((lambda x: pclass_avg_age[x[8]] if np.isnan(x[0]) else x[0]),axis=1)","4cf1a07a":"missing_count(train_set)","b135ab4b":"print(\"No. of Entries available : \",train_set.Cabin.notna().sum(),'\\n')\ntrain_set.Cabin.unique()","8850b4fe":"# Checking if all the values in Cabin starts with an alphabet\npd.Series([str(x)[0].isalpha() if x!=np.nan else False for x in train_set.Cabin.unique()]).sum()","c647164c":"# Checking if multiple people have the same cabin\/s\ntrain_set.Cabin.value_counts()","36a95b1f":"shared_cabins = train_set.Cabin.value_counts()[train_set.Cabin.value_counts()>1].index\nshared_cabins","48950af4":"cabins=[]\nfor x in train_set.Cabin.value_counts().index:\n  if ' ' in x:\n    cabins.extend(x.split(' '))\n  else:\n    cabins.append(x)\nprint(cabins)","17ce4d90":"cabin_cat = []\ncabin_cat.extend([x[0] for x in cabins])\npd.Series(cabin_cat).value_counts()","d79a65e1":"for cabin_x in set(cabin_cat):\n  train_set[f'Cabin_{cabin_x}']=[int(cabin_x in str(x)) for x in train_set.Cabin]","59dda298":"# Categories of Cabins with more than 1 passenger.\n\nfor cabin_ in shared_cabins:\n  train_set[f'Cabin_shared_{cabin_}']=[int(x==cabin_) for x in train_set.Cabin]","f05bf0ba":"passengers_in_cabin = train_set.Cabin.value_counts()[train_set.Cabin.value_counts()>1]\npassengers_in_cabin","31644bf6":"for n in passengers_in_cabin.unique():\n  train_set[f'{n}_Passenger_Cabin'] =  0\nfor index,x in enumerate(train_set.Cabin):\n  if x in passengers_in_cabin.index:\n    n = passengers_in_cabin[x]\n    train_set.loc[index,f'{n}_Passenger_Cabin'] =  1","78b3b688":"train_set.columns","a3fab144":"def clean_data(X):\n\n    # Modifying DataType\n    X.loc[:,cat_cols] = X[cat_cols].astype('category',errors='ignore')\n    X.loc[:,'PassengerId'] = X[['PassengerId']].astype('object',errors='ignore')\n    X.loc[:,num_cols] = X[num_cols].apply(lambda x: pd.to_numeric(x,errors='coerce'),axis=1)\n    \n    #Dropping Missing values in Embarked\n    X.dropna(subset=['Embarked'],inplace=True)\n\n    # Imputing Missing values in Age\n    X.Age = X.apply((lambda x: pclass_avg_age[x[8]] if np.isnan(x[0]) else x[0]),axis=1)\n\n    X = X.reset_index(drop=True)\n\n    if 'Survived' in X:\n      y = X.Survived\n      X = X.drop(['Survived'],axis=1)\n\n      return X,y\n    else:\n      return X\n","56f89400":"train_set.Ticket.nunique()","2570f94e":"train_set.Ticket.head(25)","3b91cc9c":"# Checking if the initial text in String are random\/unique or if it has any significance\npd.Series([str(x).split(' ')[0] if ' ' in str(x) else x for x in train_set.Ticket]).value_counts()","d30f353e":"ticket_codes=[]\nfor x in train_set.Ticket.value_counts().index:\n  if ' ' in x:\n    ticket_codes.append(x.split(' ')[0])\nprint(ticket_codes)","d68a366c":"ticket_codes = [x.replace('.','') for x in ticket_codes]\nticket_pattern_uniq = pd.Series(ticket_codes).unique()\npd.Series(ticket_codes).value_counts()","9b928d72":"for x in ticket_pattern_uniq:\n  train_set['Ticket_'+x] = [int(x == str(y).split(' ')[0].replace('.','')) for y in train_set.Ticket]","efc0d90d":"shared_tickets = train_set.Ticket.value_counts()[train_set.Ticket.value_counts()>1]\nshared_tickets","a87f7331":"for n in shared_tickets.unique():\n  train_set[f'{n}_Passenger_Ticket'] =  0\nfor index,x in enumerate(train_set.Ticket):\n  if x in shared_tickets.index:\n    n = shared_tickets[x]\n    train_set.loc[index,f'{n}_Passenger_Ticket'] =  1","f400701a":"train_set.sample(5)","65a1c830":"train_set['PassengerId']","992b73f9":"train_set['Name']","bd96cf17":"train_set['Title'] = train_set['Name'].apply(lambda x: x.split(', ')[1].split('.')[0])\nprint(train_set['Title'].unique())\ntrain_set['Title'].nunique()","9e5ceebd":"train_set['Title'] = train_set['Title'].replace(['Mlle'],['Ms'])\nprint(train_set['Title'].unique())\ntrain_set['Title'].nunique()","c98c6b72":"pd.crosstab(train_set['Survived'],train_set['Title'])","f4d5e474":"fig,ax = plt.subplots(figsize=(12,5))\nsns.countplot(hue='Survived',data=train_set,x='Title',)\nplt.tight_layout()","c56c8676":"train_set['FamilySize'] = train_set.SibSp +\ttrain_set.Parch\ntrain_set['FamilySize'].unique()","236365e6":"train_set.drop(['Cabin','Ticket','PassengerId','Name'],axis=1,inplace=True)","6af4f6fb":"cat_cols_updated = list(cat_cols)\ncat_cols_updated.append('Title')\nnum_cols_updated = list(num_cols)\nnum_cols_updated.append('FamilySize')","1efc1261":"for i,col in enumerate(['Title','FamilySize']):\n  j=0\n  fig,ax = plt.subplots(1,4,figsize=(24,4),)\n  for col1 in ['Pclass','Sex','Embarked','Survived']:\n      sns.countplot(x=col,data=train_set,hue=col1,ax=ax[j])\n      j=j+1\n  \n  fig.suptitle(col,size=16)\n  fig.tight_layout()","cccfbca1":"from sklearn.base import TransformerMixin,BaseEstimator","73a45525":"class FeatureEngineering(TransformerMixin,BaseEstimator):\n\n  def __init__(self):\n    self\n  \n  def fit(self,X,y=None):\n    return self\n\n  def transform(self,X,y=None):\n\n    X = X.reset_index(drop=True)\n\n    # Creating Feature 'Title'\n    X['Title'] = X['Name'].apply(lambda x: x.split(', ')[1].split('.')[0])\n    X['Title'] = X['Title'].replace(['Mlle'],['Ms'])\n\n    # Creating Feature 'FamilySize'\n    X['FamilySize'] = X.SibSp +\tX.Parch\n\n\n    #cabins=[]\n    #cabin_cat = []\n    #for x in X.Cabin.value_counts().index:\n    #  if ' ' in x:\n    #    cabins.extend(x.split(' '))\n    #  else:\n    #    cabins.append(x)\n    #cabin_cat.extend([x[0] for x in cabins])\n    for cabin_x in set(cabin_cat):\n      X[f'Cabin_{cabin_x}']=[int(cabin_x in str(x)) for x in X.Cabin]\n\n    #shared_cabins = X.Cabin.value_counts()[X.Cabin.value_counts()>1].index\n    for cabin_ in shared_cabins:\n      X[f'Cabin_shared_{cabin_}']=[int(x==cabin_) for x in X.Cabin]\n\n    #passengers_in_cabin = X.Cabin.value_counts()[X.Cabin.value_counts()>1]\n    for n in passengers_in_cabin.unique():\n      X[f'{n}_Passenger_Cabin'] =  0\n    for index,x in enumerate(X.Cabin):\n      if x in passengers_in_cabin.index:\n        n = passengers_in_cabin[x]\n        X.loc[index,f'{n}_Passenger_Cabin'] =  1\n\n\n    #ticket_codes=[]\n    #for x in X.Ticket.value_counts().index:\n    #  if ' ' in x:\n    #    ticket_codes.append(x.split(' ')[0])\n    #ticket_codes = [x.replace('.','') for x in ticket_codes]\n    #ticket_pattern_uniq = pd.Series(ticket_codes).unique()\n    for x in ticket_pattern_uniq:\n      X['Ticket_'+x] = [int(x == str(y).split(' ')[0].replace('.','')) for y in X.Ticket]\n\n    for ticket_ in shared_tickets.index:\n      X[f'Ticket_shared_{ticket_}']=[int(x==ticket_) for x in X.Ticket]\n\n    #shared_tickets = X.Ticket.value_counts()[X.Ticket.value_counts()>1]\n    for n in shared_tickets.unique():\n      X[f'{n}_Passenger_Ticket'] =  0\n    for index,x in enumerate(X.Ticket):\n      if x in shared_tickets.index:\n        n = shared_tickets[x]\n        X.loc[index,f'{n}_Passenger_Ticket'] =  1\n\n    X = X.drop(['PassengerId','Name','Ticket', 'Cabin'],axis=1)\n\n    return X\n","01806a38":"# A custom transformer to view the data inbetween the various stages of the pipeline\nclass TransformationSubStage(TransformerMixin,BaseEstimator):\n\n  def __init__(self):\n    self\n    self.transformed_X = None\n    self.transformed_y = None\n  \n  def fit(self,X,y=None):\n    return self\n\n  def transform(self,X,y=None):\n    self.transformed_X = X\n    self.transformed_y = y\n    return X","693cf308":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA","f4437543":"from sklearn.preprocessing import StandardScaler,OneHotEncoder","25ab5d01":"sub_pipe1 = Pipeline([\n                      ('imputer',SimpleImputer(strategy='most_frequent')),\n                      (('ohe',OneHotEncoder(handle_unknown='ignore')))\n])","c52db54a":"coltransformer = ColumnTransformer([\n                                    ('num_impute',SimpleImputer(strategy='median'),['Age', 'SibSp', 'Parch', 'FamilySize']),\n                                    ('num_impute2',SimpleImputer(strategy='mean'),['Fare']),\n                                    ('cat_impute',sub_pipe1,['Sex', 'Embarked', 'Pclass', 'Title'])\n],remainder='passthrough')","a2381b85":"pipe = Pipeline([\n                 ('feat_engg',FeatureEngineering()),\n                 ('substage_feat_engg',TransformationSubStage()),\n                 ('coltransformer',coltransformer),\n                 ('substage_coltransformer',TransformationSubStage()),\n                 ('num',StandardScaler()),\n])","a92af8ce":"X_train,y_train = clean_data(train_original)\nX_train = pipe.fit_transform(X_train)","24826e42":"X_train.shape","ae1bc5af":"X_test,y_test = clean_data(test_set)\nX_test = pipe.transform(X_test)","ca5d9831":"from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","874a1702":"from sklearn.metrics import accuracy_score","d0825f5b":"from sklearn.model_selection import cross_val_score,GridSearchCV,RandomizedSearchCV","44ab5267":"models = {}","62e21578":"logreg_gridSearch = LogisticRegressionCV(solver='saga',penalty='elasticnet',Cs=[0.1,0.2,0.5,1,10,15,20,25,50,100,100],l1_ratios=[0,0.35,0.5,0.65,1],n_jobs=-1,cv=3,random_state=0)\nlogreg_gridSearch.fit(X_train,y_train)","96f535bc":"logreg = LogisticRegression(solver='saga',penalty='elasticnet',C=logreg_gridSearch.C_[0],l1_ratio=logreg_gridSearch.l1_ratio_[0],n_jobs=-1,random_state=0)\nlogreg.fit(X_train,y_train)","92fe39ea":"accuracy = accuracy_score(y_test,logreg.predict(X_test))\naccuracy","bfe95c17":"models['Logistic Regression'] = accuracy","1dabea29":"params ={'C':[0.01,0.1,1,2,5,10,20,50,100,1000],\n         'penalty':['l1','l2']}\nlin_svc = GridSearchCV(LinearSVC(random_state=0),params)","fbda3319":"lin_svc.fit(X_train,y_train)\nlin_svc.best_params_","e5be04ac":"lin_svc = lin_svc.best_estimator_","ea02a94b":"accuracy = accuracy_score(y_test,lin_svc.predict(X_test))\naccuracy","4c9c1b2a":"models['Linear SVC'] = accuracy","cbd13714":"params ={'C':[0.01,0.1,1,2,5,10,20,50,100,1000],\n         'kernel':['rbf','sigmoid']}\nsvc = GridSearchCV(SVC(random_state =0,probability=True),params)","086da942":"svc.fit(X_train,y_train)","7da43640":"svc.best_params_","23fc7cb1":"svc = svc.best_estimator_","3c1dde0f":"accuracy = accuracy_score(y_test,svc.predict(X_test))\naccuracy","e645f5cc":"models['SVC'] = accuracy","3787c4f5":"dt_clf = DecisionTreeClassifier(random_state =0)\ndt_clf.fit(X_train,y_train)","8d4aea12":"accuracy = accuracy_score(y_test,dt_clf.predict(X_test))\naccuracy","ac0b8add":"models['Decision Tree'] = accuracy","26a16ae5":"!pip install -q optuna","c205f43e":"import optuna","26d4d312":"def objective(trial):\n\n  max_features=trial.suggest_float('max_features',0.3,1,step=0.05)\n  max_samples=trial.suggest_float('max_samples',0.3,0.95,step=0.05)\n  min_samples_split=trial.suggest_float('min_samples_split',0.01,0.11,step=0.01)\n  class_weight=trial.suggest_categorical('class_weight',['balanced', 'balanced_subsample',None])\n\n  clf = RandomForestClassifier(max_features=max_features, max_samples=max_samples, min_samples_split=min_samples_split,class_weight=class_weight,random_state =0)\n\n  return cross_val_score(clf,X_train,y_train,cv=3,n_jobs=-1,scoring='accuracy').mean()","7ff4fa47":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective,n_trials=50)","d9a60ffe":"best_trial = study.best_trial\nprint(\"Accuracy : \",best_trial.value)\nbest_trial.params","be573130":"rf_clf = RandomForestClassifier(**best_trial.params,random_state =0)\nrf_clf.fit(X_train,y_train)","e597a5eb":"accuracy = accuracy_score(y_test,rf_clf.predict(X_test))\naccuracy","40c47218":"models['Random Forest'] = accuracy","e4f19b83":"params = {'n_neighbors' : [2,3,4,5,6,7,8,9,10]}\nknn_clf = GridSearchCV(KNeighborsClassifier(), params)","2b104dc7":"knn_clf.fit(X_train,y_train)","7da05c64":"knn_clf.best_params_","1b92e08f":"accuracy = accuracy_score(y_test,knn_clf.predict(X_test))\naccuracy","f0d059c3":"models['K-Nearest Neighbor'] = accuracy","7f59773b":"params = {'var_smoothing': np.logspace(0,-9, num=100)}\nnb_clf = GridSearchCV(GaussianNB(), params)","32ecff68":"nb_clf.fit(X_train,y_train)","679fe21d":"nb_clf.best_params_","b1a15cfe":"accuracy = accuracy_score(y_test,nb_clf.predict(X_test))\naccuracy","6862f1b0":"models['Gaussian Naive Bayes'] = accuracy","96a07499":"import xgboost as xgb","404e26a8":"cv = cross_val_score(xgb.XGBClassifier(),X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","722f31f2":"tst = xgb.XGBClassifier().fit(X_train,y_train)\naccuracy_score(y_test,tst.predict(X_test))","bf287746":"n_iter = []\ndef objective_xgb(trial):\n\n  params = {\n      'learning_rate' : trial.suggest_loguniform('learning_rate',1e-8,0.5),\n      'max_depth' : trial.suggest_int('max_depth',8,33),\n      'subsample' : trial.suggest_float('subsample',0.5,1),\n      'colsample_bynode' : trial.suggest_float('colsample_bynode',0.5,1),\n      'lambda' : trial.suggest_loguniform(\"lambda\", 1e-8, 1.0),\n      'alpha': trial.suggest_loguniform(\"alpha\", 1e-8, 1.0),\n      'gamma' : trial.suggest_loguniform(\"gamma\", 1e-8, 1.0),\n                \n      'objective':'binary:logistic','random_state':0\n  }\n  \n  dtrain = xgb.DMatrix(X_train,y_train)\n\n\n  cv = xgb.cv(params, dtrain, num_boost_round=1000, metrics='auc', early_stopping_rounds=50)\n  n_iter.append(len(cv))\n\n  return cv.mean()['test-auc-mean']","63aef39c":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective_xgb,n_trials=750)","a58a6a4c":"best_trial = study.best_trial\nprint(\"Accuracy : \",best_trial.value)\nbest_trial.params","a293b6f0":"n_iter[best_trial.number]","db443861":"xgb_clf = xgb.XGBClassifier(**best_trial.params,n_estimators=n_iter[best_trial.number],random_state =0)\nxgb_clf","15ba6090":"xgb_clf.fit(X_train,y_train)","3e69e1a8":"accuracy = accuracy_score(y_test,xgb_clf.predict(X_test))\naccuracy","492da2b4":"models['XGBoost'] = accuracy","6c91c70e":"models","7054028a":"adaboost_base = AdaBoostClassifier(random_state=0)\nadaboost_base.fit(X_train,y_train)\naccuracy = accuracy_score(y_test,adaboost_base.predict(X_test))\naccuracy","03ab7c0a":"def objective_adaboost(trial):\n\n  params = {\n      'n_estimators':trial.suggest_int('n_estimators',2,200),\n      'learning_rate' : trial.suggest_loguniform('learning_rate',1e-6,0.5)\n  }\n\n  clf = AdaBoostClassifier(**params,random_state=0)\n\n  cv_score = cross_val_score(clf,X_train, y_train , scoring='accuracy', cv=3, n_jobs=-1,)\n\n  return cv_score.mean()","142037b5":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective_adaboost,100)","e014b442":"best_trial = study.best_trial\nbest_trial.params","b0e4898b":"adaboost_clf = AdaBoostClassifier(**best_trial.params,random_state =0).fit(X_train,y_train)\naccuracy = accuracy_score(y_test,adaboost_clf.predict(X_test))\naccuracy\n# 0.8379888268156425","584936a1":"models['AdaBoost Classifier'] = accuracy\nmodels","ac58454e":"votting_clf = VotingClassifier([('Linear SVC',lin_svc),('Logistic Regression',logreg),('SVC',svc),('Random Forest',rf_clf),('K-Nearest Neighbor',knn_clf)],n_jobs=-1)\nvotting_clf.fit(X_train,y_train)","e4828975":"accuracy = accuracy_score(y_test,votting_clf.predict(X_test))\naccuracy","b07e7f01":"votting_clf2 = VotingClassifier([('Logistic Regression',logreg),('SVC',svc),('Random Forest',rf_clf),('K-Nearest Neighbor',knn_clf)],voting='soft',n_jobs=-1)\nvotting_clf2.fit(X_train,y_train)","acb5d55f":"accuracy = accuracy_score(y_test,votting_clf2.predict(X_test))\naccuracy","507423b1":"votting_clf3 = VotingClassifier([('SVC',svc),('AdaBoost',adaboost_clf),('XGBoost',xgb_clf),],voting='soft',n_jobs=-1)\nvotting_clf3.fit(X_train,y_train)","f3cdf442":"accuracy = accuracy_score(y_test,votting_clf3.predict(X_test))\naccuracy","ff61458f":"test_data_raw.shape","61f80a77":"test_data_raw.describe()","06ad0e05":"X_test_data = clean_data(test_data_raw)\nX_test_data = pipe.transform(X_test_data)","3545768f":"vot_clf1_result = votting_clf.predict(X_test_data).astype(int)\nvot_clf2_result = votting_clf2.predict(X_test_data).astype(int)\nvot_clf3_result = votting_clf3.predict(X_test_data).astype(int)","dd891c98":"svc_result = svc.predict(X_test_data).astype(int)\nsub_data = {'PassengerId': test_data_raw.PassengerId, 'Survived': svc_result}\nsubmission = pd.DataFrame(data=sub_data)\nsubmission.to_csv('submission_svc.csv', index =False)","38ca6362":"sub_data = {'PassengerId': test_data_raw.PassengerId, 'Survived': vot_clf1_result}\nsubmission = pd.DataFrame(data=sub_data)\nsubmission.to_csv('submission_vot_clf1.csv', index =False)","74b876d6":"sub_data = {'PassengerId': test_data_raw.PassengerId, 'Survived': vot_clf2_result}\nsubmission = pd.DataFrame(data=sub_data)\nsubmission.to_csv('submission_vot_clf2.csv', index =False)","7509e608":"sub_data = {'PassengerId': test_data_raw.PassengerId, 'Survived': vot_clf3_result}\nsubmission = pd.DataFrame(data=sub_data)\nsubmission.to_csv('submission_vot_clf3.csv', index =False)","7acee8a3":"logreg_result = logreg.predict(X_test_data).astype(int)\nsub_data = {'PassengerId': test_data_raw.PassengerId, 'Survived': logreg_result}\nsubmission = pd.DataFrame(data=sub_data)\nsubmission.to_csv('submission_logreg.csv', index =False)","ab3d0942":"lin_SVC_result = lin_svc.predict(X_test_data).astype(int)\nsub_data = {'PassengerId': test_data_raw.PassengerId, 'Survived': lin_SVC_result}\nsubmission = pd.DataFrame(data=sub_data)\nsubmission.to_csv('submission_linSVC.csv', index =False)","a5978968":"xgboost_result = xgb_clf.predict(X_test_data).astype(int)\nsub_data = {'PassengerId': test_data_raw.PassengerId, 'Survived': xgboost_result}\nsubmission = pd.DataFrame(data=sub_data)\nsubmission.to_csv('submission_xgboost.csv', index =False)","fed65666":"adaboost_result = adaboost_clf.predict(X_test_data).astype(int)\nsub_data = {'PassengerId': test_data_raw.PassengerId, 'Survived': adaboost_result}\nsubmission = pd.DataFrame(data=sub_data)\nsubmission.to_csv('submission_adaboost_result.csv', index =False)","ac332aad":"#!kaggle competitions submit -c titanic -f submission_vot_clf2.csv -m \"VotingClassifier_2\"","45b10e06":"- PassengerId column contains unique integer values only, no useful information can be extracted from them. Dropping is ideal.","81e250da":"## Feature Engineering","fb3aba56":"We could also try to group Cabins by the number of passengers in it and also by Cabins with more than 1 passenger as passengers in groups may have higher chance of survival.","edf6667e":"The tickets seems much more random at first glance apart from the fact that they are mostly numerical or numericals preceeded by some text. Individuals travelling together will have the same ticket code.","1c9fa472":"Only 'Cabin' had no relation with 'Survived' column. This could also be due to the unavailability of over 75% of the data for 'Cabin'","b2dd4fa7":"### Voting Classifier (Ensemble)","6efd3c83":"We could also add a feature of Family Size","21887370":"### Feature - 'PassengerId'","c4f9608a":"- All passengers have a title in their name and to be specific there are 14 titles. \n- Mlle is French for Ms, so we shall replace this.","20121613":"### SVC","e1ae7568":"We have now extracted information from the features 'Cabin','Ticket' and 'Name' and now we shall drop these columns along with 'PassengerId'.\n","4506bf3c":"### Feature - 'SibSp' & 'Parch'","f69b5543":"# Importing Data","223d0958":"### Decision Tree Classifier","3f649eb9":"As we can see, all of 119 unique elements starts with an alphabet. We could group the Cabin codes using this initial alphabet character.","c43fcc7c":"# ML Modeling","f5750e0c":"- The Ticket class has the highest correlation with the target column 'Survived'\n- The ticket fare and the ticket class are correlated which makes much sense.\n- Also the number of siblings\/spouses aboard is correlated with the number of parents\/children aboard.","4dbf4ef6":"# Exploratory Data Analysis","1f6ab2ce":"## Building a Pipeline","52bdcd49":"### K-Nearest Neighbor Classifier","4331c49c":"We will create a Custom Transformer to extract\/create new features","8ce1bc47":"We shall create a copy of train_set so as to not loose the original training set during feature engineering.","9b50045f":"### Linear SVC","d47bf27b":"We shall split our training data to train-test set before proceeding further to avoid any data leakage into test set.","91d03579":"### Feature - 'Cabin'","702b1f5d":"'Cabin' has over 77% of its values missing, dropping the column is the ideal choice. But before droping, we shall try to extract any information if possible.","5f42c6a7":"- Embarked has 2 values missing, we could remove the entry\/instance since its only 2.\n- Age has 140 values missing, which constitutes about 20% of the whole data. We could impute these missing values.\n- Cabin has more than 75% of missing values. Ideally we should drop this feature or find some way to extract any available information if possible.","b994b308":"### AdaBoost Classifier (Ensemble)","f4f6ce01":"### Feature - 'Ticket'","82bd5854":"### Feature - 'Age'","29d939df":"We could follow a similar approach to 'Ticket' as in 'Cabin'. We shall try to extract any useful information possible from Ticket column.","7e41fa41":"### Random Forest Classifier (Ensemble)","f560f77b":"# Kaggle Submission","ba08dcd5":"### Logistic Regression","69d559c4":"### Feature - 'Embarked '","44c6c305":"We shall create a method to do the above done cleaning tasks","83094ff3":"As we can see, some of the insights that can be drawn are\n- The survival chances for females were much higher than males.\n- The chances of survival  were higher for Individuals with TicketClass('Pclass')-1. Passengers with Class-1 Ticket has survived more than any other class.\n- Passengers who embarked from port Cherbourg has a higher survival ratio.\n- Most passengers with 1st class tickets survived and the survival rate was much higher than any other ticket class. It could also be noted that there were no 1st class passengers from Queenstown.\n- Passengers embarked from Cherbourg has higher survival ratio.\n\n","9b49c9d0":"- The classification of survival isnt linearly seperable with any of the feature.\n- There arent any distinct correlation within various features.","3cedec9f":"- We can see that, the survival rate is higher for the kids and elderly. The survial rate for inividuals between 20-30 is very low.\n- We could also observe that the chances of survival is increasing with the price paid for the ticket.","913d4ebc":"- For passenger 'Name', all seems to have a 'Title'.","362de57b":"# Data Preparation","3ae03e13":"We shall impute the missing values in 'Age'. We had noticed that 'Pclass' had the highest correlation with 'Age', so instead of taking the median of the whole training set, we shall impute with class-wise(ticket) median age.","b18af792":"- There are 891 records and there are missing values in some of the columns.","2ec36703":"Initial Inferences :\n\n- The dataset is mildly imbalanced.\n- The columns 'PassengerId' & 'Name' are unique identifiers.\n- 'Survived' is the target column that we have to predict.\n- The columns 'Pclass', 'Sex' and 'Embarked' are categorical columns and the rest are numerical.\n- The column 'SibSp' should ideally be integer value.\n","9b2b51ce":"### Gaussian Naive Bayes Classifier"}}