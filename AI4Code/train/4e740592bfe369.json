{"cell_type":{"ae2d5ff2":"code","ed64be39":"code","0d90b2c8":"code","26a8e684":"code","766d685a":"code","7293954b":"code","f6d9d780":"code","992e00be":"code","a3f9fa5d":"code","5987a5ab":"code","eb70ed21":"code","c07a145e":"code","98910cfd":"code","f8adf947":"code","d0a63bb0":"code","143e2134":"code","fd13b231":"code","ebe35914":"code","9f8a45a9":"code","14c3059e":"code","11907efc":"code","4d8e7d37":"code","49a1260f":"code","7b804892":"code","b46e91dd":"code","6a41fcdb":"code","5da10003":"code","dceaaab5":"code","b7a3d84a":"code","6041eeb6":"code","51c801d3":"code","a98a545c":"code","8f944dcc":"code","a81d43b0":"code","174f729c":"code","e7980586":"markdown","95d50df2":"markdown","0564c4ec":"markdown","b36db536":"markdown","c592a77d":"markdown","7986959d":"markdown","c79be2f4":"markdown","469afef7":"markdown","29daf8c8":"markdown","73c3a445":"markdown","7a1ef415":"markdown","e80120c0":"markdown","e267935f":"markdown","3ab14c06":"markdown","4d24b8b7":"markdown","b306463e":"markdown","3ecfff8c":"markdown","3daaa7fe":"markdown","ceee18bb":"markdown","02e4deb1":"markdown","4ce0786e":"markdown","24eb4eef":"markdown","0337bacb":"markdown","322ea094":"markdown","94b5b3db":"markdown","b7aae7b1":"markdown","f1e9650b":"markdown","4926d385":"markdown","e1bb2069":"markdown","0bf0950e":"markdown","38e063c5":"markdown","bcf122c4":"markdown"},"source":{"ae2d5ff2":"import pandas as pd # dataframes\nimport numpy as np # math\nfrom tqdm.auto import tqdm # tells you how much longer you need to wait for for-loops to end\ntqdm.pandas()\nfrom scipy import sparse # deals with sparse matricies\nfrom scipy.sparse import coo_matrix\nimport torch # AKA pytorch\nimport torch.nn as nn #AKA pytorch neuroal net\nimport torch.nn.functional as F # more pytorch\nimport optuna # finds best parameters\nfrom sklearn.model_selection import StratifiedKFold # used to cross\n\n# libraries used to clean the text\nfrom bs4 import BeautifulSoup # used to decode html\nimport re # regular expression library\n\n# text embedding libraries\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom transformers import AutoTokenizer, AutoModel\n\n# modeling libraries\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n#garbage collection\nimport gc\n\n# memory variables\nbatch_size=100","ed64be39":"import os # operating system library\n\n# print out paths to input files\nprintInputFilePaths=False\nif printInputFilePaths:\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))","0d90b2c8":"# set seed for randomness\nrseed=201","26a8e684":"# for testing the code\ntesting=False\nif testing:\n    train = pd.read_csv('..\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/train.csv',nrows=1000)","766d685a":"if not testing:\n    train = pd.DataFrame()\n\n    train1 = pd.read_csv('..\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/train.csv')\n    train = train.append(train1, ignore_index=True)\n    del train1\n\n    train2 = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')\n    train = train.append(train2, ignore_index=True)\n    del train2\n\n    # train3 = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv')\n    # train3['severe_toxic'] = train3['severe_toxicity']\n    # train3['identity_hate'] = train3['identity_attack']\n    # train3 = train3[['id','comment_text','toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n    # train = train.append(train3, ignore_index=True)\n    # del train3\n\n    train.drop_duplicates(subset=['id'], inplace=True)","7293954b":"FEATURE_WTS = {\n    'severe_toxic': 1.5, 'identity_hate': 1.5, 'threat': 1.5, \n    'insult': 0.64, 'toxic': 0.32, 'obscene': 0.16, \n}\n\ntrain['toxicity'] = list(train['severe_toxic'] * FEATURE_WTS['severe_toxic']+\n                             train['toxic'] * FEATURE_WTS['toxic']+\n                             train['obscene'] * FEATURE_WTS['obscene']+ \n                             train['threat'] * FEATURE_WTS['threat']+\n                             train['insult'] * FEATURE_WTS['insult']+\n                             train['identity_hate']* FEATURE_WTS['identity_hate'])\n\ntrain = train.drop(columns=['id','toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])","f6d9d780":"train.sample(10)","992e00be":"print (\"The training dataset has %i rows.\" % len(train))\nprint (\"The first training dataset has %i toxic comments.\" % ((train['toxicity'] > 0).sum()))\nprint (\"The first training dataset has %i non-toxic comments.\" % ((train['toxicity'] == 0).sum()))","a3f9fa5d":"# undersample to the number of toxic comments (undersample_n)\nundersample_n = (train['toxicity'] > 0).sum()\n\n# perform undersample\ntrain_undersample = train.loc[train['toxicity'] == 0,:].sample(\n    n=undersample_n, random_state=rseed)\n\n# generate new training dataframe given undersampled commets\ntrain = pd.concat([train.loc[train['toxicity'] > 0,:], train_undersample])","5987a5ab":"print (\"The training dataset has %i rows.\" % len(train))\nprint (\"The first training dataset has %i toxic comments.\" % ((train['toxicity'] > 0).sum()))\nprint (\"The first training dataset has %i non-toxic comments.\" % ((train['toxicity'] == 0).sum()))","eb70ed21":"# function to clean raw text\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","c07a145e":"train['comment_text'] = train['comment_text'].progress_apply(text_cleaning)","98910cfd":"# Load the tokenizer associated to the model\ntokenizer = AutoTokenizer.from_pretrained(\n    '..\/input\/sentence-transformer-pretrained-models\/pre_trained_models\/paraphrase-MiniLM-L6-v2\/0_Transformer',\n    model_max_length=512)\n# Load model \nmodel = AutoModel.from_pretrained(\n    '..\/input\/sentence-transformer-pretrained-models\/pre_trained_models\/paraphrase-MiniLM-L6-v2\/0_Transformer')\n\n# convert `model` to a CUDA optimized model\n# AKA it makes better use of GPUs\ndevice = torch.device(\"cpu\")\nmodel.to(device)\n","f8adf947":"def bertmodel(comment_series, tokenizer, model):\n    # STEP 1: Tokenization\n    # * Either pads or truncates the sentences to equal word lengths\n    # * Converts the words to numbers\n    # * Returns lists of embeddings\n    encoded_input = tokenizer(list(comment_series),\n                              padding=True,\n                              truncation=True,\n                              return_tensors='pt').to(device)\n    \n    # STEP 2: run BERT model\n    with torch.no_grad():\n        token_embeddings = model(**encoded_input)['last_hidden_state']\n        \n    # STEP 3: MEAN POOLING\n    input_mask_expanded = encoded_input['attention_mask'].unsqueeze(-1).expand(token_embeddings.size())\n\n    # save memory\n    del encoded_input\n\n    sentence_embeddings = (torch.sum(token_embeddings * input_mask_expanded, 1) \/ torch.clamp(input_mask_expanded.sum(1), min=1e-9))\n\n    # save memory\n    del token_embeddings\n    del input_mask_expanded\n\n    # STEP 4: L_p normalize embeddings\n    sentence_embedding=F.normalize(sentence_embeddings, p=2, dim=1).tolist()\n    \n    return(sentence_embedding)","d0a63bb0":"x2 = []\nfor i in tqdm(list(range(0,len(train),batch_size))):\n    \n    end_i=i+batch_size\n    if end_i > len(train):\n        end_i=len(train)\n    train_sub=train[i:end_i]\n    if i>0:\n        x2.extend(\n            (\n             bertmodel(train_sub[\"comment_text\"], tokenizer, model)))\n    else:\n        x2 = bertmodel(train_sub[\"comment_text\"], tokenizer, model)","143e2134":"x2 = sparse.csr_matrix(np.array(x2))","fd13b231":"vec = TfidfVectorizer(min_df= 10,\n                      max_df=0.05,\n                      analyzer = 'char_wb',\n                      ngram_range = (3,5),\n                      max_features = 1000)\nvec.fit(train['comment_text'])","ebe35914":"x1 = vec.transform(train['comment_text'])\ngc.collect()","9f8a45a9":"gc.collect()\n\nX=None\nwith torch.no_grad():\n    for i in tqdm(list(range(0,x1.shape[0],batch_size))):\n\n        end_i = i + batch_size\n        if end_i > x1.shape[0]:\n            end_i=x1.shape[0]\n        x1_sub = x1[i:end_i]\n        x2_sub = x2[i:end_i]\n        if i == 0:\n            X = sparse.hstack(([x1_sub,x2_sub]))\n        else:\n            X = sparse.vstack([X, sparse.hstack([x1_sub,x2_sub])])\n\n# save memory\ndel x1\ndel x2","14c3059e":"def sentencesFeatureMatrix(comment_series, vec, tokenizer, model):\n    # clean comments\n    comment_series = comment_series.progress_apply(text_cleaning)\n    \n    # TFIDF matrix\n    x1 = vec.transform(comment_series)\n    \n    # BERT Tokenizer\n    x2 = []\n    batch_size=100\n    for i in tqdm(list(range(0,len(comment_series),batch_size))):\n\n        end_i=i+batch_size\n        if end_i > len(comment_series):\n            end_i=len(comment_series)\n        train_sub=comment_series[i:end_i]\n        if i>0:\n            x2.extend(\n                (bertmodel(train_sub, tokenizer, model)))\n        else:\n            x2 = bertmodel(train_sub, tokenizer, model)\n    x2 = sparse.csr_matrix(np.array(x2))\n    # Stack the Embeddings\n    X = np.hstack(([x1,x2]))\n\n    del x1\n    del x2\n    \n    return(X)","11907efc":"# Set the target values (Y)\nY = list(train['toxicity'])\n\n#Training classifier to predict toxicity\nxgb_model= xgb.XGBRegressor(max_depth=7,\n    n_estimators=100, learning_rate=0.44,\n    subsample = 0.25,tree_method = \"hist\").fit(\n    X, Y)","4d8e7d37":"val_df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nval_df.sample(n=5)","49a1260f":"lessToxic_embeddings_list = sentencesFeatureMatrix(\n    val_df['less_toxic'],vec, tokenizer, model)\ngc.collect()\n\n# predict the less_toxic toxicity values\nval_df['less_toxic_score'] = xgb_model.predict(lessToxic_embeddings_list)\n\nmoreToxic_embeddings_list = sentencesFeatureMatrix(\n    val_df['more_toxic'],vec, tokenizer, model)\ngc.collect()\n\n# predict the more_toxic toxicity values\nval_df['more_toxic_score'] = xgb_model.predict(moreToxic_embeddings_list)\n\nmodel_acc = (val_df['less_toxic_score'] < val_df['more_toxic_score']).mean()","7b804892":"print(model_acc)","b46e91dd":"# import submission text data\nto_score = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\n\nto_score_embeddings_list = sentencesFeatureMatrix(to_score['text'], vec, tokenizer, model)\n\ngc.collect()\n\n# predict the toxicity values\nto_score['score'] = xgb_model.predict(to_score_embeddings_list)\n\n# lets see some results\nto_score.sample(10)","6a41fcdb":"results = pd.DataFrame()\nresults['comment_id'] = to_score['comment_id']\nresults['score'] = to_score['score']\nresults.to_csv('submission.csv', index=False)","5da10003":"if not 'val_df' in globals():\n    val_df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\n# get training data from validation set\nless_toxic_score_df=pd.DataFrame()\nless_toxic_score_df[\"comment_text\"] = val_df[\"less_toxic\"].copy()\nless_toxic_score_df[\"toxicity\"] = 0\n\nmore_toxic_score_df=pd.DataFrame()\nmore_toxic_score_df[\"comment_text\"] = val_df[\"more_toxic\"].copy()\nmore_toxic_score_df[\"toxicity\"] = 1\n\ntoxic_score_df = pd.concat([less_toxic_score_df, more_toxic_score_df], ignore_index=True)\n\nval_score_df = toxic_score_df.groupby('comment_text')['toxicity'].mean().reset_index()\nval_score_df['tox_bin'] = (val_score_df['toxicity'] > 0).astype(int)","dceaaab5":"# function to normalize values between 0 and 1\ndef normFrac(nonNorm_series):\n    return (nonNorm_series-nonNorm_series.min())\/(nonNorm_series.max()-nonNorm_series.min())","b7a3d84a":"# get a dataframe where each comment has a predicted\n# and true score, we split the comments in half\n# evenly and then predict which score is more toxic\ndef marginRankLossF(df, predScoreCol, trueScoreCol):\n    total_rows=len(df)\n    if total_rows % 2 == 1:\n        total_rows = total_rows - 1\n    half_rows = total_rows \/\/ 2\n    input1_df = df.loc[range(0,half_rows),:].copy().reset_index()\n    input2_df = df.loc[range(half_rows,total_rows),:].copy().reset_index()\n    input1 = torch.tensor(input1_df[predScoreCol].values, requires_grad=False)\n    input2 = torch.tensor(input2_df[predScoreCol].values, requires_grad=False)\n    target_series = (input1_df[trueScoreCol] > input2_df[trueScoreCol]).astype(int) \n    target_series = target_series.replace(0, -1)\n    target=torch.tensor(target_series)\n    loss = nn.MarginRankingLoss()\n    output = loss(input1, input2, target)\n    return(output.numpy())\n                        ","6041eeb6":"val_score_df.head()","51c801d3":"del tokenizer\ndel model\ndel vec\ndel xgb_model\ndel to_score\ndel results","a98a545c":"def objective(trial):\n    \n    if not 'train' in locals():\n        \n        # import training data\n        train = pd.DataFrame()\n\n        train1 = pd.read_csv('..\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/train.csv')\n        train = train.append(train1, ignore_index=True)\n        del train1\n\n        train2 = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')\n        train = train.append(train2, ignore_index=True)\n        del train2\n\n        train.drop_duplicates(subset=['id'], inplace=True)\n    \n    # optimize params\n    # target subcategories\n    obscene_w1 = trial.suggest_float('obscene_w1', 0, .5)\n    toxic_w1 = trial.suggest_float('toxic_w1', 0, .5)\n    threat_w1 = trial.suggest_float('threat_w1', .5, 1)\n    insult_w1 = trial.suggest_float('insult_w1', 0, .5)\n    severe_toxic_w1 = trial.suggest_float('severe_toxic_w1', .5, 1)\n    identity_hate_w1 = trial.suggest_float('identity_hate_w1', .5, 1)\n    # bert params\n    min_df_param = trial.suggest_int('min_df_param', 5, 20, 5)\n    max_bert_feat = trial.suggest_int('max_bert_feat', 100, 900, 200)\n    # xgboost params\n    max_depth_param = trial.suggest_int('max_depth_param', 5, 20)\n    nest_param = trial.suggest_int('nest_param', 50, 350,50)\n    learningRate_param = trial.suggest_float('learningRate_param', 0, .5)\n        \n    # get target values for training data\n    toxicity_weights = {'obscene': obscene_w1, 'toxic': toxic_w1, 'threat': threat_w1, \n                    'insult': insult_w1, 'severe_toxic': severe_toxic_w1, 'identity_hate': identity_hate_w1}\n    for cat in toxicity_weights:\n        train[cat] = train[cat] * train[cat]\n    train['toxicity'] = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1)\n    train = train.drop(columns=['id','toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n        \n    error_results=[]\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=rseed)\n    for train3_index, val_df2_index in skf.split(val_score_df['comment_text'], val_score_df['tox_bin']):\n        \n        train_df = pd.concat([train,val_score_df.loc[train3_index,:]])\n        val_df2 = val_score_df.loc[val_df2_index,:]\n\n        \n\n        # undersample to the number of toxic comments (undersample_n)\n        undersample_n = (train_df['toxicity'] > 0).sum()\n\n        # perform undersample\n        train_undersample = train_df.loc[train_df['toxicity'] == 0,:].sample(\n            n=undersample_n, random_state=rseed)\n\n        # generate new training dataframe given undersampled commets\n        train_df = pd.concat([train_df.loc[train_df['toxicity'] > 0,:], train_undersample])\n        \n        del train_undersample\n        \n        # clean training data\n        train_df['comment_text'] = train_df['comment_text'].progress_apply(text_cleaning)\n\n        # Load the tokenizer associated to the BERT model\n        tokenizer = AutoTokenizer.from_pretrained(\n            '..\/input\/sentence-transformer-pretrained-models\/pre_trained_models\/paraphrase-MiniLM-L6-v2\/0_Transformer',\n            model_max_length=512)\n        # Load model associated to the BERT model\n        model = AutoModel.from_pretrained(\n            '..\/input\/sentence-transformer-pretrained-models\/pre_trained_models\/paraphrase-MiniLM-L6-v2\/0_Transformer')\n\n        # convert `model` to a CUDA optimized model\n        # AKA it makes better use of GPUs\n        model.to(device)\n\n        # fit TFIDF sparse matrix\n        vec = TfidfVectorizer(min_df= 20,\n                          max_df = min_df_param,\n                          analyzer = 'char_wb',\n                          ngram_range = (3,5),\n                          max_features = max_bert_feat)\n        vec.fit(train_df['comment_text'])\n\n        # get embedding matrix for training data\n        X = sentencesFeatureMatrix(train_df['comment_text'], vec, tokenizer, model)\n\n\n        \n\n        #Training classifier to predict toxicity\n        xgb_model= xgb.XGBRegressor(\n            max_depth=max_depth_param,\n            n_estimators=nest_param,\n            learning_rate=learningRate_param,\n            subsample = 0.25,tree_method = \"gpu_hist\").fit(\n            X, list(train_df['toxicity']))\n    \n        # run validation\n        to_score_embeddings_list = sentencesFeatureMatrix(\n            val_df2['comment_text'],vec, tokenizer, model)\n\n        gc.collect()\n\n        # predict the toxicity values\n        val_df2['pred_tox'] = xgb_model.predict(to_score_embeddings_list)\n        # normalize predictions and targets\n        val_df2['toxicity']=normFrac(val_df2['toxicity'])\n        val_df2['pred_tox']=normFrac(val_df2['pred_tox'])\n        error_results.append(marginRankLossF(val_df2,'pred_tox','toxicity'))\n        \n        del train_df\n        del val_df2\n        del undersample_n\n        del X\n        del tokenizer\n        del model\n        del vec\n        del to_score_embeddings_list\n        del xgb_model\n    print(error_results)\n    return (sum(error_results) \/ len(error_results))\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=2)\ngc.collect()\nbest_param_dict = study.best_params\nbest_param_dict","8f944dcc":"# import training data\n        \n# import training data\ntrain = pd.DataFrame()\n\ntrain1 = pd.read_csv('..\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ntrain = train.append(train1, ignore_index=True)\ndel train1\n\ntrain2 = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')\ntrain = train.append(train2, ignore_index=True)\ndel train2\n\ntrain.drop_duplicates(subset=['id'], inplace=True)\n        \n# handle target values\ntrain_df = train.copy()\ntoxicity_weights = {'obscene': study.best_params['obscene_w1'],\n                    'toxic': study.best_params['toxic_w1'],\n                    'threat': study.best_params['threat_w1'], \n                    'insult': study.best_params['insult_w1'],\n                    'severe_toxic': study.best_params['severe_toxic_w1'],\n                    'identity_hate': study.best_params['identity_hate_w1']}\nfor cat in toxicity_weights:\n    train_df[cat] = train_df[cat] * toxicity_weights[cat]\n        \ntrain_df['toxicity'] = train_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1)\ntrain_df = train_df.drop(columns=['id','toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n\nval_score_df.loc[:,[\"comment_text\",\"toxicity\"]]\n\ntrain_df = pd.concat([train_df,val_score_df])\n\n# undersample to the number of toxic comments (undersample_n)\nundersample_n = (train_df['toxicity'] > 0).sum()\n\n# perform undersample\ntrain_undersample = train_df.loc[train_df['toxicity'] == 0,:].sample(\n    n=undersample_n, random_state=rseed)\n\n# generate new training dataframe given undersampled commets\ntrain_df = pd.concat([train_df.loc[train_df['toxicity'] > 0,:], train_undersample])\n\n# clean training data\ntrain_df['comment_text'] = train_df['comment_text'].progress_apply(text_cleaning)\n\n# Load the tokenizer associated to the BERT model\ntokenizer = AutoTokenizer.from_pretrained(\n    '..\/input\/sentence-transformer-pretrained-models\/pre_trained_models\/paraphrase-MiniLM-L6-v2\/0_Transformer',\n    model_max_length=512)\n# Load model associated to the BERT model\nmodel = AutoModel.from_pretrained(\n    '..\/input\/sentence-transformer-pretrained-models\/pre_trained_models\/paraphrase-MiniLM-L6-v2\/0_Transformer')\n\n# convert `model` to a CUDA optimized model\n# AKA it makes better use of GPUs\ndevice = torch.device(\"cpu\")\nmodel.to(device)\n\n# fit TFIDF sparse matrix\nvec = TfidfVectorizer(min_df= study.best_params['min_df_param'],\n                  max_df=0.05,\n                  analyzer = 'char_wb',\n                  ngram_range = (3,5),\n                  max_features = study.best_params['max_bert_feat'])\nvec.fit(train_df['comment_text'])\n\n# get embedding matrix for training data\nX = sentencesFeatureMatrix(train_df['comment_text'], vec, tokenizer, model)\n\n\n#Training classifier to predict toxicity\nxgb_model= xgb.XGBRegressor(\n    max_depth=study.best_params['max_depth_param'],\n    n_estimators=study.best_params['nest_param'], \n    learning_rate=study.best_params['learningRate_param'],\n    subsample = 0.25,tree_method = \"hist\").fit(\n    X, list(train_df['toxicity']))","a81d43b0":"# import submission text data\nto_score = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\n\nto_score_embeddings_list = sentencesFeatureMatrix(to_score['text'], vec, tokenizer, model)\n\ngc.collect()\n\n# predict the toxicity values\nto_score['score'] = xgb_model.predict(to_score_embeddings_list)\n\n# lets see some results\nto_score.sample(10)","174f729c":"results = pd.DataFrame()\nresults['comment_id'] = to_score['comment_id']\nresults['score'] = to_score['score']\nresults.to_csv('submission.csv', index=False)","e7980586":"To optimize the parameters we can use the validation dataset for training by transforming validation data to give each row a comment and a score.","95d50df2":"## Run the model on submission data","0564c4ec":"## Import Training Data\nFirst let's import the training data into pandas data frames","b36db536":"## Fit XGBoost Model","c592a77d":"Next, we want to apply a [pretrained sentence transformer](https:\/\/www.sbert.net\/docs\/pretrained_models.html) model to the text.\n\nThe goal of sentence embedding models is to project similar sentences closer together or alternatively further apart if the sentences are completely different. Specifically the model chosen here offers pretty good quality and is relatively fast compared to the model that is supposed to be the best. \n\nTo run the pretrained model requires two steps. In the first step you split the sentences into lists of numbers where each number represents a word in the sentence (tokenization). The numbers come from a vocabulary generated when you run the command \"AutoTokenizer.from_pretrained().\" The second step is to run the results of the first step into a model to project the sentence.\n\n","7986959d":"clean the validation text data","c79be2f4":"## Transform comments using a pretrained BERT model","469afef7":"Now format the submission","29daf8c8":"We also need to set a loss function to compare comments","73c3a445":"## Stack the features","7a1ef415":"## Validation of the Model","e80120c0":"convert list of embeddings into a sparse matrix","e267935f":"Now that we trained the model let's transform the submission text and run the model","3ab14c06":"### Imbalanced dataset\nBelow we run code to see if the comments have a relatively equal amount of toxic and non-toxic comments.","4d24b8b7":"To run the BERT model on the comments we need to run the comments in batches. These are the steps:\n\n1. Tokenize each of the comments using the pretrained BERT model. The output contain matricies of size:  NUMBER_OF_COMMENTS X model_max_length\n\n2. Run the BERT model to project the sentences. The `model` will return `last_hidden_state` and `pooler_output`. The `last_hidden_state` is a matrix containing the sequence of hidden-states at the output of the last layer of the model. It is of size: NUMBER_OF_COMMENTS X model_max_length X NUM_HIDDEN_STATES. The `pooler_output` is a the last layer hidden-state of the first token of the sequence after  further processing through the layers used for the auxiliary pretraining task. It is a matrix of size: NUMBER_OF_COMMENTS X NUM_HIDDEN_STATES.\n\n3. Perform mean pooling (multiply the results of the model by the attention mask and then average the columns). Note that an attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them\n\n4. L_p normalize the embeddings","b306463e":"In parallel to modeling the comments using a pretrained BERT model, we generate a sparse matrix based on the words in each comment we fit the text data into vectors using term frequency\u2013inverse document frequency (TFIDF) library. This library will create a model which converts a list of comments into a sparse matrix format where columns are each of the \"relevant words\" from all of the comments, and rows are each of the comments. The values will be 0 if the word is not in the respective comment. Otherwise, the value will be equal to  the importance of the word in the document ([the TFIDF value](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf)).\n\nFirst we fit the model using the training comments.","3ecfff8c":"Now I will reset the weights with the optimized values and generate a new submission.","3daaa7fe":"This code originated from Josh Ludan at https:\/\/www.kaggle.com\/jmrludan\/jigsaw-competition. I am simply organizing and annotating his work.","ceee18bb":"Then we transform the comments into a sparse matrix (NUMBER_OF_COMMENTS X VOCAB_SIZE) with the TFIDF model","02e4deb1":"# Model without optimization","4ce0786e":"## Custom Fuction for Embedding","24eb4eef":"import validation data","0337bacb":"Now that we have the two ways of transforming the text we can combine them as two sets of features for training the model. We stack these features horizontally into a pytorch tensor called `X`","322ea094":"Cool done with those shenanigans. Time to fit a model to use transformed text matrix and return a toxicity values.","94b5b3db":"# Import libraries","b7aae7b1":"# Optimize parameters","f1e9650b":"## Transform comments into a sparse word frequency matrix","4926d385":"Let's put all the embedding steps into a single function so that we don't have to do all those steps again...","e1bb2069":"## Handle Target Values","0bf0950e":"## Clean Training Data\nNow that we have the training data. We can clean the comments.","38e063c5":"The dataset is very unbalanced. Below is code to undersample the majority class.","bcf122c4":"Now that we are done transforming the text, let's work on the target values. Below we set weights to the different target columns to generate a final single target we want."}}