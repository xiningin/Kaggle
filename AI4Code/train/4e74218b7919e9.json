{"cell_type":{"edc30268":"code","5ee2c51b":"code","311cfe05":"code","b894cf2e":"code","d638b453":"code","83ef2279":"code","fe5d8c74":"code","0cabf2ab":"code","90dcdfed":"code","ef314c4b":"code","ee7711fc":"code","69eeaa00":"code","02200371":"code","0603adbd":"code","40fd6c4e":"code","65279315":"code","a321e0ae":"code","be4831c9":"code","9a97cf1b":"code","281d7ad5":"code","4c0a9025":"code","5c0a0c70":"code","4562f538":"code","c429b24c":"code","8f7892b2":"code","51e2dff5":"code","0ee44047":"code","679d3882":"code","7a5d2902":"code","e8f31735":"code","5cc46386":"code","c9859481":"code","62fe967b":"code","d0376a7d":"code","a4dbd9f9":"code","6ec54363":"code","0689c303":"code","94033c1d":"code","380b4ac7":"code","3cdcff81":"markdown","11a92d61":"markdown","7d819c69":"markdown","c02325b0":"markdown","2e150dec":"markdown","7a8f0a28":"markdown","b56186f8":"markdown","49035d0b":"markdown","5ffd99a3":"markdown","939abfca":"markdown","ffd9201c":"markdown","7a75f7c0":"markdown","30529630":"markdown","32be3f8e":"markdown","97170a0b":"markdown","3f067944":"markdown","f1e0e0e1":"markdown","d5b57213":"markdown","d131d6ac":"markdown","4f96d070":"markdown","7258fe17":"markdown","14a7f6f2":"markdown","fd5b20c1":"markdown","e4f8532b":"markdown","5f9ed75b":"markdown","09a32ac4":"markdown","b400dee2":"markdown","d93a26d9":"markdown"},"source":{"edc30268":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns  # visualization tool\n# keras imports for the dataset and building our neural network\nfrom keras.datasets import mnist\nfrom keras.preprocessing import image\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential, load_model\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.utils import np_utils\n\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\n\nseed = 42\nnp.random.seed(seed)\n\nfrom collections import Counter\nimport itertools\nimport os\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint('Input Folder Detail :' )\nprint('----------------------' )\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","5ee2c51b":"# Reading the data frames\n#Training Data\ntrain_raw = pd.read_csv(\"..\/input\/mnist_train.csv\") \n# data info\nprint('')\nprint('Train Data Info :' )\nprint('------------------' )\ntrain_raw.info()\nprint('')","311cfe05":"print('data_train shape:', train_raw.shape)\nprint('Number of images in train  :', train_raw.shape[0])\ntrain_raw.head() \n#The Head Table = 5 rows \u00d7 785 columns \n#The Row = 784 features, 1 label","b894cf2e":"z_train = Counter(train_raw['label'])\nprint(z_train)","d638b453":"train_raw.describe()","83ef2279":"# Countplot for labels\nsns.countplot(train_raw['label'])\nprint(\"Dataset is pretty balanced!\")","fe5d8c74":"# Reading the data frames\n#Test Data\ntest_raw = pd.read_csv(\"..\/input\/mnist_test.csv\") \n# data info\nprint('')\nprint('Test Data Info :' )\nprint('------------------' )\ntest_raw.info()\nprint('')","0cabf2ab":"print('data_test shape:', test_raw.shape)\nprint('Number of images in data_test  :', test_raw.shape[0])\ntest_raw.head()\n# 5 rows \u00d7 785 columns\n# 784 features, 1 label","90dcdfed":"train_raw.isnull().any().describe()","ef314c4b":"test_raw.isnull().any().describe()","ee7711fc":"# Assign features and corresponding labels\n#Converting the Pandas frame to numpy \n\n#Train data\nX_train = train_raw.iloc[:,1:].values.astype('float32') # all pixel values in train\ny_train_label = train_raw.iloc[:,0].values.astype('int32') # only train labels i.e targets digits\n\n#Test data\nX_test = test_raw.iloc[:,1:].values.astype('float32') # or np.array(test.iloc[:,1:],np.float32)  # all pixel values in test\ny_test_label = test_raw.iloc[:,0].values.astype('int32') # only test labels i.e targets digits\n\n#X_train = train_raw.iloc[:, 1:785]\n#train_label = train_raw.iloc[:, 0]\n#X_test = test_raw.iloc[:, 1:785]\n#X_test = test_raw.iloc[:, 1:785]\n\n#label values == ['0','1','2','3','4','5','6','7','8','9']\nlabel = np.array(train_raw.iloc[:,0],np.str) #or label =  train_raw.iloc[:,0].values.astype('str')\n\nprint('X_train:', X_train)\nprint('X_train_shape:', X_train.shape)\nprint('X_test.shape:',X_test.shape)\nprint('X_test:', X_test)\nprint('y_train_label:', y_train_label)\nprint('y_train_label.shape:',y_train_label.shape)\nprint('y_test_label:', y_test_label)\nprint('y_test_label.shape:',y_test_label.shape)\nprint('Label:',label)","69eeaa00":"def visualize_input(img, ax):\n    ax.imshow(img, cmap='gray')\n    width, height = img.shape\n    thresh = img.max()\/2.5\n    for x in range(width):\n        for y in range(height):\n            ax.annotate(str(round(img[x][y],2)), xy=(y,x),\n                        horizontalalignment='center',\n                        verticalalignment='center',\n                        color='white' if img[x][y]<thresh else 'black')\n\nfig = plt.figure(figsize = (12,12)) \nax = fig.add_subplot(111)\n#\nvisualize_input(X_test[10].reshape(28,28), ax)","02200371":"#Drawing Train Data\nfig = plt.figure(figsize=(20,20))\nfor i in range(14):\n    ax = fig.add_subplot(7,7,i+1)\n    ax.imshow(np.reshape(X_train[i],(28,28)),cmap='gray')\n    plt.tight_layout()\n    ax.set_title(\"Digit: {}\".format(label[i]))\n    #ax.set_title(str(label[i]))\n#fig    ","0603adbd":"# Making sure that the values are float so that we can get decimal points after division\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\nX_train = X_train.astype('float32')\nX_test  = X_test.astype('float32')","40fd6c4e":"# Normalizing the RGB codes by dividing it to the max RGB value(255).\n#print((min(X_train[1]), max(X_train[1])))\nX_train = X_train \/ 255\nX_test  = X_test \/ 255\n\nprint('X_train matrix shape:', X_train.shape) #_train shape: (42000, 28, 28, 1)\nprint('Number of images in X_train :', X_train.shape[0]) #Number of images in X_train : 42000\nprint('X_test matrix shape:', X_test.shape) #X_test shape: (28000, 28, 28, 1)\nprint('Number of images in X_test  :', X_test.shape[0])#Number of images in X_test  : 28000","65279315":"# one-hot encoding using keras' numpy-related utilities\nfrom keras.utils import np_utils\nnum_classes = 10\n\nprint(\"Shape before one-hot encoding Y_train: \", y_train_label.shape)\nprint(\"Shape before one-hot encoding Y_test: \", y_test_label.shape)\nY_train = np_utils.to_categorical(y_train_label, num_classes)\nY_test = np_utils.to_categorical(y_test_label, num_classes)\nprint(\"Shape after one-hot encoding Y_train: \", Y_train.shape)\nprint(\"Shape after one-hot encoding Y_test: \", Y_test.shape)\n\n","a321e0ae":"plt.title(Y_train[5])\nplt.plot(Y_train[5])\nplt.xticks(range(10));","be4831c9":"#import numpy as np # linear algebra\n# fix random seed for reproducibility\n#seed = 42\n#np.random.seed(seed)\n#tf.set_random_seed(42) # for tensorflow","9a97cf1b":"#python simple code\ndef sigmoid(z):\n    output = 1 \/ (1+np.exp(-z))\n    return output\n\ndef relu(z):\n    return np.maximum(z, 0)\n\ndef tanh(z):\n    return (np.exp(z) - np.exp(-z)) \/ (np.exp(z) + np.exp(-z))\n    \n#plot all activation function\nimport matplotlib.pylab as plt\ndef plot_function(function, title=\"sigmoid\"):\n    x = np.arange(-7, 7, 0.01)\n    y = function(x)\n    \n    plt.plot(x, y)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(title)\n    plt.show()\n    \nplot_function(sigmoid, \"sigmoid\")    \nplot_function(tanh, \"tanh\")\nplot_function(relu, \"relu\")","281d7ad5":"from keras.models import  Sequential\nfrom keras.layers.core import  Lambda , Dense, Flatten, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import BatchNormalization, Convolution2D , MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint\nimport os","4c0a9025":"def create_model(input_length, hidden_lenght1, hidden_lenght2, activation_func , \n                 dropout_val,lr_val,num_epochs, batch_size):\n\n    # training the model and saving metrics in history\n    model = Sequential()\n\n    model.add(Dense(hidden_lenght1, input_dim=input_length, activation=activation_func))\n    model.add(Dropout(dropout_val))\n\n    model.add(Dense(hidden_lenght2, activation=activation_func))\n    model.add(Dropout(dropout_val))\n    model.add(Dense(10, activation='softmax'))\n\n    lr = lr_val#.001\n    adam0 = Adam(lr=lr)\n\n    # Modeli derleyip ve daha iyi bir sonu\u00e7 elde edildi\u011finde a\u011f\u0131rl\u0131klar\u0131 kaydedelim\n    model.compile(loss='categorical_crossentropy', optimizer=adam0, metrics=['accuracy'])\n    filepath = 'tmp_weights.best.hdf5'\n    checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n    callbacks_list = [checkpoint]\n\n    # training the model and saving metrics in history\n    history_model = model.fit(X_train, Y_train, callbacks=callbacks_list,\n                              epochs=num_epochs, batch_size=batch_size, verbose=2,\n                                validation_data = (X_test, Y_test))  # verbose 2 or 0 \n    return model, history_model\n\n","5c0a0c70":"def saving_the_model(model, model_name = 'model_history.h5'):  \n    model.save(model_name)\n    print('Saved trained model at %s ' % model_name)\n    ","4562f538":"def draw_model(training):\n    plt.figure()\n    plt.subplot(2,1,1)\n    plt.plot(training.history['acc'])\n    plt.plot(training.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='lower right')\n\n    plt.subplot(2,1,2)\n    plt.plot(training.history['loss'])\n    plt.plot(training.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper right')\n\n    plt.tight_layout()\n    plt.show()    \n  ","c429b24c":"# evaluate test accuracy\ndef scoring_the_model(model_name=\"model_history.h5\"):\n    # load the model and create predictions on the test set\n    mnist_model = load_model(model_name)\n    #1- genel bak\u0131\u015f\n    loss_and_metrics = mnist_model.evaluate(X_test, Y_test, verbose=2)\n    print(\"Test Loss\", loss_and_metrics[0])\n    print(\"Test Accuracy\", loss_and_metrics[1])\n\n    #2- test sonras\u0131\n    predicted_classes = mnist_model.predict_classes(X_test)\n    # see which we predicted correctly and which not\n    correct_indices = np.nonzero(predicted_classes == y_test_label)[0]\n    incorrect_indices = np.nonzero(predicted_classes != y_test_label)[0]\n    print()\n    print(len(correct_indices),\" classified correctly\")\n    print(len(incorrect_indices),\" classified incorrectly\")\n    \ndef draw_9_correct_9_incorrect_predictions(model_name=\"model_history.h5\"):  \n    # load the model and create predictions on the test set\n    mnist_model = load_model(model_name) \n    predicted_classes = mnist_model.predict_classes(X_test)\n    correct_indices = np.nonzero(predicted_classes == y_test_label)[0]\n    incorrect_indices = np.nonzero(predicted_classes != y_test_label)[0]\n    # adapt figure size to accomodate 18 subplots\n    plt.rcParams['figure.figsize'] = (7,14)\n\n    plt.figure()\n   # print(\"plot 9 correct predictions\")\n    # plot 9 correct predictions\n    for i, correct in enumerate(correct_indices[:9]):\n        plt.subplot(6,3,i+1)\n        plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n        plt.title(\n          \"Predicted: {}, Truth: {}\".format(predicted_classes[correct],\n                                            y_test_label[correct]))\n        plt.xticks([])\n        plt.yticks([])\n\n    #print(\"plot 9 incorrect predictions\")\n    # plot 9 incorrect predictions\n    for i, incorrect in enumerate(incorrect_indices[:9]):\n        plt.subplot(6,3,i+10)\n        plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n        plt.title(\n          \"Predicted {}, Truth: {}\".format(predicted_classes[incorrect],\n                                           y_test_label[incorrect]))\n        plt.xticks([])\n        plt.yticks([])\n\n","8f7892b2":"#parameters\nnum_epochs = 20\nlengh_features = 784 #lengh_features = input layer = 28 pixel  * 28 pixel == 784 node\n\n# Create Model1 # 784-128-128-10 Dropout 0.2,Learning Rate==0.001,Activation Function =relu\n# hidden layer start 128 and 2 hidden layer\n# activation relu for popular and fast\nmodel1, history_model1 = create_model(input_length=lengh_features,hidden_lenght1=128,hidden_lenght2=128,\n                                    activation_func='relu',dropout_val=0.20,\n                                    lr_val=0.001,num_epochs=20, batch_size=128\n                                    )\n# saving the model\nsaving_the_model(model1,'model1_history.h5')\n#model scoring\nscoring_the_model('model1_history.h5')\n","51e2dff5":"#Model Drawing to visualize the behavior\ndraw_model(history_model1) ","0ee44047":"#draw_9 correct_9_incorrect_predictions\ndraw_9_correct_9_incorrect_predictions('model1_history.h5')","679d3882":"# Create Model2 # 784-[128-128]-10 CHANGE DROPOUT 0.1,Learning Rate==0.001,Activation Function =relu\n# Change Dropout from 0.20 to 0.10\nmodel2, history_model2 = create_model(input_length=lengh_features,hidden_lenght1=128,hidden_lenght2=128,\n                                    activation_func='relu',dropout_val=0.10,\n                                    lr_val=0.001,num_epochs=20, batch_size=128)\n# saving the model\nsaving_the_model(model2,'model2_history.h5')\n#model scoring\nscoring_the_model('model2_history.h5')                                  ","7a5d2902":"#Model Drawing to visualize the behavior\ndraw_model(history_model2) ","e8f31735":"#draw_9 correct_9_incorrect_predictions\ndraw_9_correct_9_incorrect_predictions('model2_history.h5')","5cc46386":"#Change Hidden Layer1 Node from 128 to 256\n#Change Hidden Layer1 Node from 128 to 256\n\n# Create Model3 # 784-[256-256]-10 Dropout 0.2,Learning Rate==0.001,Activation Function =relu\nmodel3, history_model3 = create_model(input_length=lengh_features,hidden_lenght1=256,hidden_lenght2=256,\n                                    activation_func='relu',dropout_val=0.20,\n                                    lr_val=0.001,num_epochs=20, batch_size=128)\n # saving the model\nsaving_the_model(model3,'model3_history.h5')\n#model scoring\nscoring_the_model('model3_history.h5')                                         ","c9859481":"#Model Drawing to visualize the behavior\ndraw_model(history_model3) ","62fe967b":"#draw_9 correct_9_incorrect_predictions\ndraw_9_correct_9_incorrect_predictions('model3_history.h5')","d0376a7d":"#DROPOUT Change from 0.20 to 0.25\n\n#Create Model4 # 784-[256-256]-10 DROPOUT Change from 0.20 to 0.25,Learning Rate==0.001,Activation Function =relu\nmodel4, history_model4 = create_model(input_length=lengh_features,hidden_lenght1=256,hidden_lenght2=256,\n                                    activation_func='relu',dropout_val=0.25,\n                                    lr_val=0.001,num_epochs=20, batch_size=128)\n # saving the model\nsaving_the_model(model4,'model4_history.h5')\n#model scoring\nscoring_the_model('model4_history.h5')","a4dbd9f9":"#Model Drawing to visualize the behavior\ndraw_model(history_model4) ","6ec54363":"#draw_9 correct_9_incorrect_predictions\ndraw_9_correct_9_incorrect_predictions('model4_history.h5')","0689c303":"#Change Hidden Layer1 Node from 256 to 512\n#Change Hidden Layer2 Node from 256 to 512\n\n# Create Model5 # 784-[512-512]-10 Dropout 0.25,Learning Rate==0.001,Activation Function =relu\nmodel5, history_model5 = create_model(input_length=lengh_features,hidden_lenght1=256,hidden_lenght2=256,\n                                    activation_func='relu',dropout_val=0.25,\n                                    lr_val=0.001,num_epochs=20, batch_size=128)\n                                      \n# saving the model\nsaving_the_model(model5,'model5_history.h5')\n#model scoring\nscoring_the_model('model5_history.h5')","94033c1d":"#Model Drawing to visualize the behavior\ndraw_model(history_model5) ","380b4ac7":"#draw_9 correct_9_incorrect_predictions\ndraw_9_correct_9_incorrect_predictions('model5_history.h5')","3cdcff81":"<a id=\"7-11\"><\/a>\n# Compare Model-3 to Model-2\n## Result :  256 Hidden node better than 128 hidden node","11a92d61":"<a id=\"2\"><\/a> <br>\n# Overview The Data Set\n* We will use \"MNIST in CSV\".\n* In this data there are 60000  digits images. 0 to 59999 index\n* As you know digits are from 0 to 9. ","7d819c69":"<a id=\"7-5\"><\/a>\n# Model Drawing to visualize the behavior ","c02325b0":"<a id=\"5\"><\/a> <br>\n# Data Visualization\n## Drawing Sample of the Testing Data","2e150dec":"<a id=\"7-7\"><\/a>\n# Model - 1  \n## 784-[128-128]-10 Dropout 0.2,Learning Rate==0.001,Activation Function =relu","7a8f0a28":"<font color='red'>\n<br>Content:\n* [Introduction](#1)\n* [Aim](#1-1)\n* [Overview the Data Set](#2)   \n* [Import  Required Pyton Libraries](#3 )\n* [ Loading The  Dataset ](#4)\n    * [Loading Train Raw  Dataset and Detail](#4-1)\n    * [Loading Test Raw  Dataset and Detail](#4-2)\n    * [Check data for any null values](#4-3)\n* [Data Visualization](#5)\n     * [ Drawing Sample of the Testing Data](#5)\n     * [Drawing  Train Data With Label](#5-1)\n* [Data Preparation](#6)\n     * [ Data Normalization](#6-1)\n     * [One Hot encoding of labels](#6-2)    \n* [Initialization the random number generator](#7-1)\n* [Designing  Multilayer Perceptron-MLP Architecture](#7)\n     * [Activation function](#7-2)     \n     * [Create model](#7-3)\n     * [Save model](#7-4)  \n     * [Model Drawing to visualize the behavior](#7-5)\n     * [Model Score](#7-6)  \n     * [Model-1](#7-7)\n     * [Model-2](#7-8)\n     * [Model-3](#7-10)\n     * [Compare Model-3 to Model-2](#7-11)\n     * [Model-4](#7-12) \n     * [Model-5](#7-14)     \n    ","b56186f8":"<a id=\"7-6\"><\/a>\n# Model Score","49035d0b":"<a id=\"7\"><\/a> <br>\n# Designing  Multilayer Perceptron-MLP Architecture\n* Dropout parameter\n* Denses\n* Activation function","5ffd99a3":"<a id=\"4-2\"><\/a> <br>\n# Loading The  Dataset \n## Test Database and Detail","939abfca":"<a id=\"7-12\"><\/a>\n# Create Model - 4 \n## 784-[256-256]-10 DROPOUT Change from 0.20 to 0.25,Learning Rate==0.001,Activation Function =relu\n## DROPOUT Change from 0.20 to 0.25","ffd9201c":"<a id=\"5-1\"><\/a> \n# Data Visualization\n## Drawing  Train Data With Label\nLets look at 14 images from data set with their labels.","7a75f7c0":"<a id=\"6\"><\/a> <br>\n# Data Preparation\n* The dataset is clean no need for cleaning\n\n","30529630":"<a id=\"6-1\"><\/a> <br>\n# Data Normalization\n## We have to normalize our data, in the case of the MNIST dataset, this simply means dividing by 255, a more formal explanations is that \n\n### $$X = \\frac{X - min(x)}{max(x) - min(x)}$$\n### $$X = \\frac{X - 0}{255 - 0}$$$$X = \\frac{X }{255 }$$\n* Normalizing the data as each pixel is from 0 : 255 => Black => white in gray scale\n* Normalizing the RGB codes by dividing it to the max RGB value(255).\n* The data will be from 0 to 1","32be3f8e":"<a id=\"7-1\"><\/a>\n## Initialization the random number generator\nNext, we can initialize the random number generator to ensure that we always get the same results \nwhen executing this code. This will help if we are debugging.","97170a0b":"<a id=\"7-3\"><\/a>\n# Create model ","3f067944":"<a id=\"7-4\"><\/a>\n# Save model ","f1e0e0e1":"<a id=\"3\"><\/a> \n# Import required Pyton libraries\n","d5b57213":"<a id=\"7-8\"><\/a>\n# Model - 2 \n## 784-[128-128]-10 CHANGE DROPOUT 0.1,Learning Rate==0.001,Activation Function =relu\n## Change Dropout from 0.20 to 0.10","d131d6ac":"\n<a id=\"7-14\"><\/a>\n# Create Model - 5\n## 784-[512-512]-10 Dropout 0.25,Learning Rate==0.001,Activation Function =relu\n## Change Hidden Layer-1  Node from 256 to 512\n## Change Hidden Layer-2 Node from 256 to 512","4f96d070":"Plot second label.","7258fe17":"There is no null values in the dataset.","14a7f6f2":"<a id=\"4\"><\/a> <br>\n# Loading The  Dataset \n<a id=\"4-1\"><\/a> \n## Train Raw Database and Detail","fd5b20c1":"<a id=\"1\"><\/a> <br>\n# INTRODUCTION\n* **Deep learning:** One of the machine learning technique that learns features directly from data. \n* **Why deep learning:** When the amounth of data is increased, machine learning techniques are insufficient in terms of performance and deep learning gives better performance like accuracy.\n<a href=\"http:\/\/ibb.co\/m2bxcc\"><img src=\"http:\/\/preview.ibb.co\/d3CEOH\/1.png\" alt=\"1\" border=\"0\"><\/a>\n* **What is amounth of big:** It is hard to answer but intuitively 1 million sample is enough to say \"big amounth of data\"\n* **Usage fields of deep learning:** Speech recognition, image classification, natural language procession (nlp) or recommendation systems\n* **What is difference of deep learning from machine learning:** \n    * Machine learning covers deep learning. \n    * Features are given machine learning manually.\n    * On the other hand, deep learning learns features directly from data.\n<a href=\"http:\/\/ibb.co\/f8Epqx\"><img src=\"http:\/\/preview.ibb.co\/hgpNAx\/2.png\" alt=\"2\" border=\"0\"><\/a>\n\n","e4f8532b":"<a id=\"1-1\"><\/a> <br>\n# AIM\n Multi classification  on Hand writing images database. We will use deep learning model. This model is MLP-Multilayer Perceptron.","5f9ed75b":"<a id=\"7-2\"><\/a>\n# Activation function\n## The sigmoid function is defined as  \n # $$sigmoid(z) = \\frac{1}{1 + e ^ {-z}}$$\n## The relu function is much simpler, as it is the maximum between the input and zero\n## The tanh function is defined as  \n#   $$tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$","09a32ac4":"<a id=\"4-3\"><\/a> <br>\n# Check data for any null values\n* Check data for any null values\n* Is the dataset  clean ?","b400dee2":"<a id=\"7-10\"><\/a>\n# Model - 3 \n## 784-[256-256]-10 Dropout 0.2,Learning Rate==0.001,Activation Function =relu\n## Change Hidden Layer1 Node from 128 to 256\n## Change Hidden Layer1 Node from 128 to 256","d93a26d9":"<a id=\"6-2\"><\/a> <br>\n# One Hot Encoding of Labels\nA one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension.\n\nExample,\n*  0 would be [1,0,0,0,0,0,0,0,0,0].\n*  1 would be [0,1,0,0,0,0,0,0,0,0].\n*  2 would be [0,0,1,0,0,0,0,0,0,0].\n* ............\n*  9 would be [0,0,0,0,0,0,0,0,0,1]."}}