{"cell_type":{"79798771":"code","f85503c5":"code","7eea465e":"code","205a356e":"code","89e6cb03":"code","4d0e47cd":"code","45d32c25":"code","70d0472e":"code","308500ab":"code","9c811710":"code","e6862777":"code","63e9d747":"code","ecae11e3":"code","9b0c6602":"code","8c3538e5":"code","2665b61d":"code","664877b9":"code","a1de1eb3":"code","78c99122":"code","7cee5f91":"code","c0fde76c":"code","1c3272dd":"code","832142fe":"code","62d39835":"code","9efaeeaa":"code","7afe4e19":"code","e81523be":"code","26d399c9":"code","08b562e0":"code","bcbed28c":"code","cb35fe8e":"code","607f1187":"code","f5676fa7":"code","9fe4e348":"code","261c734d":"code","07f0458a":"code","c71f6742":"code","dbaf3a2c":"code","f2865a02":"code","5ec653a9":"code","91de953e":"code","2e8a9709":"code","934044d1":"code","901835e8":"code","7c9611d6":"code","82475026":"code","e8b7c5e3":"code","2a674569":"markdown","9d74a448":"markdown","3c3aef36":"markdown","e7e7b83e":"markdown","0e7ee00c":"markdown","4033153b":"markdown","6d6fd904":"markdown","71c580a7":"markdown","1c976ea3":"markdown","78673353":"markdown","c4ad2acc":"markdown"},"source":{"79798771":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f85503c5":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","7eea465e":"# Reading the data\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","205a356e":"train.head()","89e6cb03":"test.head()","4d0e47cd":"print(f\"The train dataset has {train.shape[0]} rows and {train.shape[1]} columns\")\nprint(f\"The train dataset has {test.shape[0]} rows and {test.shape[1]} columns\")","45d32c25":"train.info()","70d0472e":"test.info()","308500ab":"train.describe()","9c811710":"test.describe()","e6862777":"# Printing the columns that present missing values\nfor column in train.columns:\n    if train[column].isnull().sum() > 0:\n        missing_values = train[column].isnull().sum()\n        print(f\"{column} ---> {missing_values}\")","63e9d747":"# Printing the columns that present missing values (test dataframe)\nfor column in test.columns:\n    if test[column].isnull().sum() > 0:\n        missing_values = test[column].isnull().sum()\n        print(f\"{column} ---> {missing_values}\")","ecae11e3":"# Let's check for missing data from train dataset\ntotal_train_miss = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/ len(train) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total_train_miss,percent],axis=1,keys=['Total Miss','Percent'], join='outer')\nmissing_data.head(30)","9b0c6602":"# Let's check for missing data from test dataset.\ntotal_test_miss = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()\/len(test) * 100).sort_values(ascending=False)\nmissing_test_data = pd.concat([total_test_miss,percent],axis=1,keys=['Total Miss','Percent'])\nmissing_test_data.head(40)","8c3538e5":"# Plot the missing values for train dataset\nplt.figure(figsize=(12, 7))\ntrain_miss = train.isnull().sum().sort_values(ascending=True)\ntrain_miss[train_miss > 0].plot.bar()","2665b61d":"# Plot the missing values for test dataset\nplt.figure(figsize=(12, 7))\ntest_miss = test.isnull().sum().sort_values(ascending=True)\ntest_miss[test_miss > 0].plot.bar()","664877b9":"# Correlation beetwen variables and SalePrice (Correlation matrix)\ntrain_corr = train.corr()\nprint(train_corr['SalePrice'].sort_values(ascending=True))","a1de1eb3":"# Visualization by using an heatmap\nplt.figure(figsize=(30, 25))\nsns.heatmap(train_corr, annot=True, cmap=\"RdYlGn\")","78c99122":"# Most correlated features\nfor feature in train_corr.columns:\n    if train_corr[feature].SalePrice > 0.5:\n        print(feature)","7cee5f91":"train = train.drop((missing_data[missing_data['Total Miss']>=8]).index,1)","c0fde76c":"# Let's also fill 'Electrical' field.\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\n# Double check that there no data is missing anymore\ntrain.isnull().sum().max()","1c3272dd":"# Deleting features with missing values in the test data as done for the train data.\ntest = test.drop((missing_test_data[missing_test_data['Total Miss']>4]).index,1)","832142fe":"# filling with mode the 15 features left in the test dataset\ntest['MSZoning'] = test['MSZoning'].fillna(test['MSZoning'].mode()[0])\n#no_miss_test['MSZoning'].isnull().sum()\ntest['BsmtHalfBath'] = test['BsmtHalfBath'].fillna(test['BsmtHalfBath'].mode()[0])\n#no_miss_test['BsmtHalfBath'].isnull().sum()\ntest['Utilities'] = test['Utilities'].fillna(test['Utilities'].mode()[0])\n#no_miss_test['Utilities'].isnull().sum()\ntest['Functional'] = test['Functional'].fillna(test['Functional'].mode()[0])\n#no_miss_test['Functional'].isnull().sum()\ntest['BsmtFullBath'] = test['BsmtFullBath'].fillna(test['BsmtFullBath'].mode()[0])\n#no_miss_test['BsmtFullBath'].isnull().sum()\ntest['BsmtFinSF1'] = test['BsmtFinSF1'].fillna(test['BsmtFinSF1'].mode()[0])\n#no_miss_test['BsmtFinSF1'].isnull().sum()\ntest['BsmtFinSF2'] = test['BsmtFinSF2'].fillna(test['BsmtFinSF2'].mode()[0])\n#no_miss_test['BsmtFinSF2'].isnull().sum()\ntest['BsmtUnfSF'] = test['BsmtUnfSF'].fillna(test['BsmtUnfSF'].mode()[0])\n#no_miss_test['BsmtUnfSF'].isnull().sum()\ntest['KitchenQual'] = test['KitchenQual'].fillna(test['KitchenQual'].mode()[0])\n#no_miss_test['KitchenQual'].isnull().sum()\ntest['TotalBsmtSF'] = test['TotalBsmtSF'].fillna(test['TotalBsmtSF'].mode()[0])\n#no_miss_test['TotalBsmtSF'].isnull().sum()\ntest['Exterior2nd'] = test['Exterior2nd'].fillna(test['Exterior2nd'].mode()[0])\n#no_miss_test['Exterior2nd'].isnull().sum()\ntest['GarageCars'] = test['GarageCars'].fillna(test['GarageCars'].mode()[0])\n#no_miss_test['GarageCars'].isnull().sum()\ntest['Exterior1st'] = test['Exterior1st'].fillna(test['Exterior1st'].mode()[0])\n#no_miss_test['Exterior1st'].isnull().sum()\ntest['GarageArea'] = test['GarageArea'].fillna(test['GarageArea'].mode()[0])\n#no_miss_test['GarageArea'].isnull().sum()\ntest['SaleType'] = test['SaleType'].fillna(test['SaleType'].mode()[0])\n#no_miss_test['SaleType'].isnull().sum()","62d39835":"test.isnull().sum().max()","9efaeeaa":"# Numerical Variables\nnumeric_var = [num for num in train.columns if train[num].dtypes == \"int64\" or train[num].dtypes == \"float64\"]\n# Categorical Variables\ncategorical_var = [cat for cat in train.columns if train[cat].dtypes == \"object\"]\nprint(f\"The train dataframe includes {len(numeric_var)} numerical variables\")\nprint(f\"The train dataframe includes {len(categorical_var)} categorical variables\")","7afe4e19":"from scipy.stats import norm\nplt.figure(figsize=(15, 7))\nsns.set_theme(style='darkgrid')\nsns.distplot(train.SalePrice, kde=True, fit=norm)\nplt.xlabel('Price in $')","e81523be":"no_miss_categorical = [cat for cat in train.columns if train[cat].dtype == 'object']\nno_miss_numerical = [num for num in train.columns if train[num].dtype != 'object']\n#Separating the dataset in categorical and numerical features\ntrain_num = train[no_miss_numerical]\ntrain_cat = train[no_miss_categorical]","26d399c9":"train_num.shape, train_cat.shape","08b562e0":"# using get_dummies here it is used for data manipulation. It converts categorical data into dummy or indicator variables\ntrain_cat = pd.get_dummies(train_cat)\ntrain_cat.shape","bcbed28c":"full_train = pd.concat([train_cat,train_num],axis=1)\nprint(f'The full_train dataset contains {full_train.shape[0]} rows and {full_train.shape[1]} columns')","cb35fe8e":"# Same procedure on test dataset\nno_miss_categorical = [cat for cat in test.columns if test[cat].dtype == 'object']\nno_miss_numerical = [num for num in test.columns if test[num].dtype != 'object']\n#Separating the dataset in categorical and numerical features\ntest_num = test[no_miss_numerical]\ntest_cat = test[no_miss_categorical]","607f1187":"test_num.shape,test_cat.shape","f5676fa7":"test_cat = pd.get_dummies(test_cat)\ntest_cat.head()","9fe4e348":"full_test = pd.concat([test_cat,test_num],axis=1)\nprint(f'The full_test dataset contains {full_test.shape[0]} rows and {full_test.shape[1]} columns')","261c734d":"# Let's see the skewness of the features\nfrom scipy.stats import skew\ntrain_skew = train_num.apply(lambda x: skew(x)).sort_values(ascending=False)\ntest_skew = test_num.apply(lambda x: skew(x)).sort_values(ascending=False)","07f0458a":"train_skew","c71f6742":"test_skew","dbaf3a2c":"plt.figure(figsize=(12, 7))\nsns.boxplot(data=full_train.SalePrice)\nplt.ylabel('SalePrice') ","f2865a02":"# Removing outliers\nmean = np.mean(full_train['SalePrice'])\nsigma = np.std(full_train['SalePrice'])\nlower_range = mean - (3 * sigma) \nupper_range = mean + (3 * sigma)\nprint(f\"Data between {lower_range} and {upper_range} should be considered good data\")","5ec653a9":"# Outliers\noutliers = [i for i in full_train.SalePrice if i < lower_range or i > upper_range]\nprint(f\"Outliers: {outliers}\")\nprint(f\"{len(outliers)} outliers have been found!\")\nnew_train = full_train[(full_train['SalePrice'] > lower_range) & (full_train['SalePrice'] < upper_range)]\nnew_train.shape","91de953e":"from sklearn.model_selection import train_test_split\ncols = [col for col in full_train.columns if col not in full_test.columns]\ncols.remove('SalePrice')\nfull_train = full_train.drop(cols,axis=1)","2e8a9709":"# assigning the required data to the respective variables  \nX = full_train.drop(['SalePrice'],axis=1)\ny = full_train['SalePrice']\n\n# checking shapes\nfull_test.shape,full_train.shape","934044d1":"# XGBoost\nfrom xgboost import XGBRegressor\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n# XGBoost model\nxgb_model = XGBRegressor(n_estimators=1000, learning_rate = 0.05, n_jobs=4)\n# Fit the model\nxgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train)], early_stopping_rounds=10, verbose=False)\n# Making prediction\nprediction = xgb_model.predict(X_test)","901835e8":"# MAE and MAPE\nfrom sklearn.metrics import mean_absolute_error\n\ndef mape(y_test, prediction):\n    y_test, prediction = np.array(y_test), np.array(prediction)\n    return np.mean(np.abs((y_test - prediction)\/y_test))*100\n        \nprint(f\"Mean Absolute Error: {mean_absolute_error(y_test, prediction)}\")\nprint(f\"Mean Absolute Percentage Error: {mape(y_test, prediction)}\")","7c9611d6":"# Linear Regression plot\nplt.figure(figsize=(12, 7))\nplt.title('Linear Regression Plot')\nsns.regplot(x=y_test, y=prediction, color='g')","82475026":"test_predictions = xgb_model.predict(full_test)","e8b7c5e3":"# Submitting the data\nsub = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nmy_sub = pd.DataFrame({'Id':sub['Id'],'SalePrice':test_predictions})\nmy_sub.to_csv(\"final_sub.csv\", index=False)","2a674569":"### Skewness\nAt this point we have two datasets we can work on. Let's visualize the skewness.","9d74a448":"### Exploratory Data Analysis","3c3aef36":"### Building the model","e7e7b83e":"[Thanks to Viviane Andrade for sharing your knowlwdge](https:\/\/www.kaggle.com\/vivianandrade\/house-price-evaluation-xgboost-with-features)","0e7ee00c":"From the correlation matrix above we have 11 features more correlated to SalePrice than others:\n> - OverllQual\n> - YearBuilt\n> - YearRemodAdd\n> - TotalBsmtSF\n> - 1stFlrSF\n> - GrLivArea\n> - FullBath\n> - TotRmsAbvGrd\n> - GarageCars\n> - GarageArea\n> - SalePrice","4033153b":"## House Prices - Advanced Regression Techniques\nThe following project was developed in this way:\n>- importing the libraries\n>- EDA (Exploratory Data Analysis)\n>- Missing Values\n>- Skewness\n>- Outliers\n>- Building the model","6d6fd904":"### Missing Values\nNow, we need to know how many missing values our dataframes handle","71c580a7":"We are going to separate the two datasets using the categorical variables and numerical variables","1c976ea3":"We are going to drop some data from train dataset and test dataset. If the percentage of missing data is above 15% then it will be dropped","78673353":"### Numerical and Categorical Variables","c4ad2acc":"### Finding Outliers\nOutliers are extreme values that deviate from other observations on data , they may indicate a variability in a measurement, experimental errors or a novelty. In other words, an outlier is an observation that diverges from an overall pattern on a sample."}}