{"cell_type":{"5feba989":"code","696b732e":"code","8e7b0a02":"code","6a01c339":"code","7f1cb3df":"code","09a437b4":"code","765eebf6":"code","447d0afc":"code","a4cfebf3":"code","39110c6a":"code","1c75b173":"code","5bd0a553":"code","accefa0d":"code","00b448c2":"code","3cbff8c1":"code","8b982f58":"code","539a3c29":"code","5f17e750":"code","50a265fa":"code","017013f2":"code","3c7a5816":"code","120b47f3":"markdown","6f7f41ab":"markdown","1e4afe84":"markdown","6e1619f5":"markdown","76f3be11":"markdown","40338a76":"markdown","f9b49a19":"markdown","2ebe0278":"markdown","feb0dbe2":"markdown","fd3bb74b":"markdown","7f95542c":"markdown","52fe2827":"markdown","1fb06f28":"markdown","7395e548":"markdown","6e7f9740":"markdown","a1e0bbf0":"markdown","f3a32540":"markdown","54dbac37":"markdown","5d97cdd4":"markdown","89be5da9":"markdown","cae850ee":"markdown","5e04c640":"markdown","dde7106c":"markdown","26cd3927":"markdown","5b66d58c":"markdown","5e82c632":"markdown","d054155a":"markdown","8a5fa43b":"markdown","d616528c":"markdown"},"source":{"5feba989":"!pip install -U git+https:\/\/github.com\/albumentations-team\/albumentations > \/dev\/null 2>&1","696b732e":"import os\nimport cv2\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport matplotlib.pyplot as plt","8e7b0a02":"images_path = '..\/input\/hubmap-256x256\/train'\nmasks_path = '..\/input\/hubmap-256x256\/masks'","6a01c339":"class HuBMAPDataset(Dataset):\n    def __init__(self, ids, transforms=None, preprocessing=None):\n        self.ids = ids\n        self.transforms = transforms\n        self.preprocessing = preprocessing\n    def __getitem__(self, idx):\n        name = self.ids[idx]\n        img = cv2.imread(f\"{images_path}\/{name}\")\n        mask = cv2.imread(f\"{masks_path}\/{name}\")[:,:,0:1]\n        if self.transforms:\n            augmented = self.transforms(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n        '''\n        # Now, we are use new feature in albumentations\n        if self.preprocessing:\n            preprocessed = self.preprocessing(image=img, mask=mask)\n            img = preprocessed['image']\n            mask = preprocessed['mask']\n        '''\n        return img, mask\n\n    def __len__(self):\n        return len(self.ids)","7f1cb3df":"def get_train_augmentation(size=1024):\n    return A.Compose([\n        A.HorizontalFlip(),\n        A.VerticalFlip(),\n        A.ShiftScaleRotate(),\n        A.CoarseDropout(max_holes=8, max_height=20, max_width=20, mask_fill_value=0, always_apply=True), # For visualization, set always_apply=True.\n        A.Resize(size,size, always_apply=True),\n        A.Normalize(\n            mean=(0.485, 0.456, 0.406),\n            std=(0.229, 0.224, 0.225)\n        ),\n        ## Check transpose_mask=True\n        ToTensorV2(transpose_mask=True)\n    ])\n\ndef get_valid_augmentation(size=1024):\n    return A.Compose([\n        A.Resize(size,size, always_apply=True),\n        A.Normalize(\n            mean=(0.485, 0.456, 0.406),\n            std=(0.229, 0.224, 0.225)\n        ),\n        ## Check transpose_mask=True\n        ToTensorV2(transpose_mask=True)\n    ])","09a437b4":"data = os.listdir(images_path)\ntrain_lsit = list(set([row.split(\"_\")[0] for row in data]))\ntrain_idx = [row for row in data if row.split(\"_\")[0] in train_lsit[:-2]]\nvalid_idx = [row for row in data if row.split(\"_\")[0] not in train_lsit[:-2]]\nlen(train_idx),len(valid_idx)","765eebf6":"train_aug = get_train_augmentation(size=1024)\ntrain_dataset = HuBMAPDataset(\n    ids=train_idx,\n    transforms=train_aug)","447d0afc":"# I pick train_dataset[11] for mask\nimg, mask = train_dataset[11]\nimg = img.permute(1,2,0).detach().numpy()\nmask = mask.permute(1,2,0).detach().numpy()\n\nfig, ax = plt.subplots(figsize=(16, 8),  nrows=1, ncols=2)\nax[0].imshow(img)\nax[1].imshow(mask[:,:,0])","a4cfebf3":"# I pick train_dataset[11] for mask\nimg, mask = train_dataset[11]\nimg = img.permute(1,2,0).detach().numpy()\nmask = mask.permute(1,2,0).detach().numpy()\n\nfig, ax = plt.subplots(figsize=(16, 8),  nrows=1, ncols=2)\nax[0].imshow(img)\nax[1].imshow(mask[:,:,0])","39110c6a":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=16,\n    collate_fn=collate_fn\n)","1c75b173":"n_rows=4\nn_cols=4\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn)\n\nimages, masks = next(iter(train_loader))\n\n# plot some augmentations!\nfig, ax = plt.subplots(figsize=(20, 20),  nrows=n_rows, ncols=n_cols)\nfor i in range (n_rows*n_cols):     \n    image = images[i].permute(1,2,0).detach().numpy()\n    mask = masks[i].permute(1,2,0).detach().numpy()\n\n    ax[i \/\/ n_rows][i % n_cols].imshow(image)\n    ax[i \/\/ n_rows][i % n_cols].imshow(mask[:,:,0], cmap=\"hot\", alpha=0.7)","5bd0a553":"dataset_index = 11","accefa0d":"valid_aug = get_valid_augmentation(size=1024)\ntrain_dataset = HuBMAPDataset(\n    ids=train_idx,\n    transforms=valid_aug)\n\nimg, mask = train_dataset[dataset_index]\n\nimg = img.permute(1,2,0).detach().numpy()\nmask = mask.permute(1,2,0).detach().numpy()\n\nfig, ax = plt.subplots(figsize=(16, 8),  nrows=1, ncols=2)\nax[0].imshow(img)\nax[1].imshow(mask[:,:,0])","00b448c2":"transformed_mask = A.HorizontalFlip(always_apply=True)(image=mask)\n\nfig, ax = plt.subplots(figsize=(16, 8),  nrows=1, ncols=2)\nax[0].imshow(mask[:,:,0])\nax[1].imshow(transformed_mask['image'][:,:,0])","3cbff8c1":"transformed_mask = A.VerticalFlip(always_apply=True)(image=mask)\n\nfig, ax = plt.subplots(figsize=(16, 8),  nrows=1, ncols=2)\nax[0].imshow(mask[:,:,0])\nax[1].imshow(transformed_mask['image'][:,:,0])","8b982f58":"transformed_mask = A.RandomRotate90(always_apply=True)(image=mask)\n\nfig, ax = plt.subplots(figsize=(16, 8),  nrows=1, ncols=2)\nax[0].imshow(mask[:,:,0])\nax[1].imshow(transformed_mask['image'][:,:,0])","539a3c29":"transformed_mask = A.ShiftScaleRotate(shift_limit=0.25, \n                                      scale_limit=0.25, \n                                      rotate_limit=15, \n                                      border_mode=cv2.BORDER_REFLECT,\n                                      always_apply=True)(image=mask)\n\nfig, ax = plt.subplots(figsize=(16, 8),  nrows=1, ncols=2)\nax[0].imshow(mask[:,:,0])\nax[1].imshow(transformed_mask['image'][:,:,0])","5f17e750":"transformed_mask = A.OpticalDistortion(always_apply=True, \n                                       distort_limit=0.85, \n                                       shift_limit=0.85, \n                                       mask_value=0)(image=mask)\n\nfig, ax = plt.subplots(figsize=(16, 8),  nrows=1, ncols=2)\nax[0].imshow(mask[:,:,0])\nax[1].imshow(transformed_mask['image'][:,:,0])","50a265fa":"transformed_mask = A.GridDistortion(always_apply=True, \n                                    distort_limit=0.85, \n                                    mask_value=0)(image=mask)\n\nfig, ax = plt.subplots(figsize=(16, 8),  nrows=1, ncols=2)\nax[0].imshow(mask[:,:,0])\nax[1].imshow(transformed_mask['image'][:,:,0])","017013f2":"transformed_mask = A.ElasticTransform(alpha=120, \n                                      sigma=120 * 0.05, \n                                      alpha_affine=120 * 0.03, \n                                      always_apply=True,\n                                      mask_value=0)(image=mask)\n\nfig, ax = plt.subplots(figsize=(16, 8),  nrows=1, ncols=2)\nax[0].imshow(mask[:,:,0])\nax[1].imshow(transformed_mask['image'][:,:,0])","3c7a5816":"transformed_mask = A.CoarseDropout(max_holes=8, \n                                   max_height=50, \n                                   max_width=50, \n                                   mask_fill_value=0, \n                                   always_apply=True)(image=mask)\n\nfig, ax = plt.subplots(figsize=(16, 8),  nrows=1, ncols=2)\nax[0].imshow(mask[:,:,0])\nax[1].imshow(transformed_mask['image'][:,:,0])","120b47f3":"## VerticalFlip","6f7f41ab":"## Base Image and mask\n\nI used `train_dataset[11]` for visualization.","1e4afe84":"## GridDistortion","6e1619f5":"## First\nFocus on that image and compare second one.\n\nWe can see also `CoarseDropout` results between image and mask.","76f3be11":"# Installation","40338a76":"# Multiple View","f9b49a19":"Use `transpose_mask`=`True`.","2ebe0278":"# Let's start more Albumentation with masks","feb0dbe2":"https:\/\/www.kaggle.com\/iafoss\/hubmap-pytorch-fast-ai-starter","fd3bb74b":"## RandomRotate90","7f95542c":"<img src='https:\/\/albumentations.ai\/docs\/images\/logo.png' width='160'>\n\n\n<h1><center>[Tutorial] Albumentations with masks<\/center><h1>\n    \n# <a id='1'>Let's start augmentation with masks\ud83d\udd25 <\/a>","52fe2827":"- Version `13` : Add `CoarseDropout with Masks` and more examples\n- Version `8` : Add basic guideline and simple examples","1fb06f28":"# Albumentations with masks","7395e548":"## ShiftScaleRotate","6e7f9740":"## If this kernel is useful, <font color='orange'>please upvote<\/font>!","a1e0bbf0":"## New feature - releases 0.5.1 \n\n`A.ToTensorV2` now supports an additional argument transpose_mask (False by default).\n\nIf the argument is set to True and an input mask has 3 dimensions, A.ToTensorV2 will transpose dimensions of a mask tensor in addition to transposing dimensions of an image tensor.\n\n","f3a32540":"# What is Albumentations\n\n`Albumentations` is a Python library for fast and flexible image augmentations. \n\nAlbumentations efficiently implements a rich variety of image transform operations that are optimized for performance, and does so while providing a concise, yet powerful image augmentation interface for different computer vision tasks, including object classification, segmentation, and detection.\n\n<img src='https:\/\/camo.githubusercontent.com\/3bb6e4bb500d96ad7bb4e4047af22a63ddf3242a894adf55ebffd3e184e4d113\/68747470733a2f2f686162726173746f726167652e6f72672f776562742f62642f6e652f72762f62646e6572763563746b75646d73617a6e687734637273646669772e6a706567' width='640'>\n","54dbac37":"# Applying augmentation with mask","5d97cdd4":"- Parameters such as `distort_limit` are exaggerated for visualization.\n- Be careful of use.","89be5da9":"https:\/\/www.kaggle.com\/orkatz2\/hubmap-res34unet-baseline-train","cae850ee":"## HorizontalFlip","5e04c640":"## OpticalDistortion","dde7106c":"## CoarseDropout","26cd3927":"## If this kernel is useful, <font color='orange'>please upvote<\/font>!\n- See you next time!","5b66d58c":"- Parameters such as `distort_limit`, `shift_limit` are exaggerated for visualization.\n- Be careful of use.","5e82c632":"## ElasticTransform","d054155a":"Different augmentations **are applied to** `image` and `mask`!","8a5fa43b":"# Dataset","d616528c":"## Second"}}