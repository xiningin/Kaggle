{"cell_type":{"03e0a991":"code","1cfd53ff":"code","3d69c58b":"code","6243ade9":"code","90b197df":"code","fcf2e617":"code","7f8d104e":"code","e1ba96f4":"code","6bb4390f":"code","631af8ff":"code","b85e0765":"code","dcb2257c":"code","e0533100":"code","5bc6f054":"code","c40bf14b":"code","b086aedb":"code","cd70d643":"code","d547ef66":"code","972681a5":"code","c9cc5078":"code","a481bbcd":"code","7a88e4e8":"code","7f40b6ce":"code","d5509f5d":"code","1ba8531a":"code","f641ce9a":"code","d991bcfc":"code","a6acbc87":"code","fbdc407e":"code","9ea189ee":"code","e8d5b83e":"code","a2850348":"code","56c5fb59":"code","d8210313":"code","b2459189":"code","732cfcdc":"markdown","f4272c70":"markdown","2e3fc6cb":"markdown","9ebcfe23":"markdown","de54a9f2":"markdown","d844df9f":"markdown","fb42299d":"markdown","8f089c51":"markdown"},"source":{"03e0a991":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1cfd53ff":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport nltk\n\nimport re\nimport string\nfrom sklearn import model_selection, feature_extraction\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nimport seaborn as sns ","3d69c58b":"from nltk.corpus import stopwords\n","6243ade9":"root = Path.cwd().parent \/ 'input' \/ 'nlp-getting-started'","90b197df":"df_submission = pd.read_csv(root\/'sample_submission.csv')\ndf_submission.head()","fcf2e617":"df_train = pd.read_csv(root\/'train.csv')\ndf_train.head()","7f8d104e":"df_test = pd.read_csv(root\/'test.csv')\ndf_test.head()","e1ba96f4":"print('There are {} rows and {} columns in train'.format(df_train.shape[0],df_train.shape[1]))\nprint('There are {} rows and {} columns in train'.format(df_test.shape[0],df_test.shape[1]))","6bb4390f":"x = df_train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","631af8ff":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\nword = df_train[df_train['target'] == 1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')\nax1.set_title('disaster')\n\nword = df_train[df_train['target'] == 0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\n\nfig.suptitle('Average word length in each tweet')","b85e0765":"def change_contraction_verb(text):\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    \n    # specific\n    text = re.sub(r\"won\\'t\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    return text\n\ndf_train['text'] = df_train['text'].apply(lambda x : change_contraction_verb(x))\ndf_test['text'] = df_test['text'].apply(lambda x : change_contraction_verb(x))\n\ndf_train['text'].head()","dcb2257c":"def wordopt(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub(\"\\\\W\",\" \",text) # remove special chars\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    return text","e0533100":"df_train['text'] = df_train['text'].apply(lambda x : wordopt(x))\ndf_test['text'] = df_test['text'].apply(lambda x : wordopt(x))","5bc6f054":"df_test['text'].head()","c40bf14b":"def deEmojify(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)","b086aedb":"df_train['text'] = df_train['text'].apply(lambda x :deEmojify(x))\ndf_train['text'].head()","cd70d643":"df_test['text'] = df_test['text'].apply(lambda x :deEmojify(x))\ndf_test['text'].head()","d547ef66":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","972681a5":"df_train['text'] = df_train['text'].apply(lambda x :remove_URL(x))\ndf_train['text'].head()","c9cc5078":"df_test['text'] = df_test['text'].apply(lambda x :remove_URL(x))\ndf_test['text'].head()","a481bbcd":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","7a88e4e8":"df_test['text'] = df_test['text'].apply(lambda x :remove_html(x))\ndf_test['text'].head()","7f40b6ce":"df_train['text'] = df_train['text'].apply(lambda x :remove_html(x))\ndf_train['text'].head()","d5509f5d":"# A disaster tweet\ndisaster_tweets = df_train[df_train['target']==1]['text']\ndisaster_tweets.values[1]","1ba8531a":"#not a disaster tweet\nnon_disaster_tweets = df_train[df_train['target']==0]['text']\nnon_disaster_tweets.values[1]","f641ce9a":"from wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","d991bcfc":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nstopwords = stopwords.words('english')","a6acbc87":"count_vectorizer = TfidfVectorizer(token_pattern=r'\\w{1,}', ngram_range=(1, 2), stop_words = stopwords)","fbdc407e":"count_vectorizer = feature_extraction.text.TfidfVectorizer()\ncount_vectorizer","9ea189ee":"train_vectors = count_vectorizer.fit_transform(df_train[\"text\"])\ntest_vectors = count_vectorizer.transform(df_test[\"text\"])","e8d5b83e":"clf = LogisticRegression(C=1.0)","a2850348":"clf.fit(train_vectors, df_train[\"target\"])","56c5fb59":"scores = model_selection.cross_val_score(clf, train_vectors, df_train[\"target\"], cv=5, scoring=\"f1\")\nscores","d8210313":"df_submission[\"target\"] = clf.predict(test_vectors)\ndf_submission.head()","b2459189":"df_submission.to_csv(\"df_submission.csv\", index=False)","732cfcdc":"# Function \n\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a \u0e41\u0e17\u0e19\u0e17\u0e35\u0e48\u0e04\u0e33\u0e43\u0e19\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e17\u0e35\u0e48\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a \u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e25\u0e14\u0e23\u0e39\u0e1b\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e33\u0e40\u0e15\u0e47\u0e21","f4272c70":"IDF = \u0e40\u0e1b\u0e47\u0e19\u0e04\u0e48\u0e32\u0e17\u0e35\u0e48\u0e04\u0e33\u0e19\u0e27\u0e13\u0e08\u0e32\u0e01 Document \u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14 \u0e08\u0e36\u0e07\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e40\u0e1b\u0e47\u0e19 Generalize \u0e21\u0e32\u0e01\u0e01\u0e27\u0e48\u0e32\u0e41\u0e25\u0e30\u0e43\u0e0a\u0e49\u0e1a\u0e48\u0e07\u0e1a\u0e2d\u0e01\u0e04\u0e27\u0e32\u0e21\u0e40\u0e1b\u0e47\u0e19 General \u0e44\u0e14\u0e49\u0e43\u0e19\u0e23\u0e30\u0e14\u0e31\u0e1a\u0e2b\u0e19\u0e36\u0e48\u0e07\n>IDF(t)  = log [ (1 + n) \/ (1 + DF(t)) ] + 1\n\n>t \u0e04\u0e37\u0e2d term \u0e2b\u0e23\u0e37\u0e2d\u0e04\u0e33 1 \u0e04\u0e33\n\n>n \u0e04\u0e37\u0e2d\u0e08\u0e33\u0e19\u0e27\u0e19 Document \u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14\u0e17\u0e35\u0e48\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\n\n>DF(t) \u0e04\u0e37\u0e2d\u0e08\u0e33\u0e19\u0e27\u0e19 Document \u0e17\u0e35\u0e48\u0e1e\u0e1a\u0e02\u0e2d\u0e07\u0e04\u0e33 t","2e3fc6cb":"\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e15\u0e31\u0e14 URL \u0e2d\u0e2d\u0e01\u0e08\u0e32\u0e01 text","9ebcfe23":"\u0e43\u0e0a\u0e49 TfidfVectorizer : TF - IDF TF = Term Frequency : \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e14\u0e39\u0e27\u0e48\u0e32\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e33\u0e19\u0e31\u0e49\u0e19\u0e1b\u0e23\u0e32\u0e01\u0e0e\u0e1a\u0e48\u0e2d\u0e22\u0e41\u0e04\u0e48\u0e44\u0e2b\u0e19\u0e43\u0e19 document\n>TF(t , d) is the number of occurrences of  \u201ct\u201d in document \u201cd\u201d\n\n>t \u0e04\u0e37\u0e2d term \u0e2b\u0e23\u0e37\u0e2d \u0e04\u0e33\n\n>d \u0e04\u0e37\u0e2d Document \u0e19\u0e31\u0e49\u0e19\u0e46","de54a9f2":"\u0e43\u0e0a\u0e49\u0e15\u0e31\u0e14 \u0e2d\u0e34\u0e42\u0e21\u0e08\u0e34\u0e41\u0e25\u0e30\u0e2a\u0e31\u0e0d\u0e25\u0e31\u0e01\u0e29\u0e13\u0e4c\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e2d\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21","d844df9f":"\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e15\u0e31\u0e14\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e2d\u0e2d\u0e01","fb42299d":"# Data Prepare Process\nSTOPWORDS : \u0e04\u0e33\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e2a\u0e19\u0e43\u0e08\u0e43\u0e19 Text","8f089c51":"\u0e2b\u0e32\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e1c\u0e34\u0e14\u0e1e\u0e25\u0e32\u0e14\u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 cross_val_score"}}