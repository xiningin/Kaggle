{"cell_type":{"2d2f2941":"code","9c0436a0":"code","9244947a":"code","692178fa":"code","0af8cb65":"code","0ac382a4":"code","11f51f0a":"code","07ad861a":"code","af60c8a4":"code","65d87d9a":"code","c19f2d9d":"code","f8676777":"code","9ecfd90c":"code","fa0bb96f":"code","a9b0894f":"code","57c70222":"code","1af8a6ee":"code","7b3ff84a":"code","575a166d":"code","b7db7020":"code","97ec772a":"code","809c3b0e":"code","3e5efb92":"code","f93cb399":"code","d431b579":"code","7f35a7de":"code","5d1d8d7c":"markdown","baf126cc":"markdown","f9b37b96":"markdown","bb712ee0":"markdown","06771e53":"markdown","ced361b5":"markdown","ee726422":"markdown","68fc512d":"markdown","fe6e05e9":"markdown","30ca98e7":"markdown","4201cd25":"markdown","71ee0b5f":"markdown","da30bada":"markdown"},"source":{"2d2f2941":"#Importing the necessary packages\n\nimport collections\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\n\nimport scipy\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, plot_roc_curve, accuracy_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","9c0436a0":"#Reading the datasets\ndata = pd.read_csv(\"\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")\ndata.head()","9244947a":"data.describe()","692178fa":"data.isnull().sum()","0af8cb65":"corr = data.corr()\ncorr.style.background_gradient(cmap='coolwarm')","0ac382a4":"#Removing the serial number column as it adds no correlation to any columns\ndata = data.drop(columns = [\"Serial No.\"])\n\n#The column \"Chance of Admit\" has a trailing space which is removed\ndata = data.rename(columns={\"Chance of Admit \": \"Chance of Admit\"})\n\ndata.head()","11f51f0a":"plt.hist(data[\"Chance of Admit\"])\nplt.xlabel(\"Chance of Admit\")\nplt.ylabel(\"Count\")\nplt.show()","07ad861a":"sns.pairplot(data)","af60c8a4":"sns.kdeplot(data[\"Chance of Admit\"], data[\"GRE Score\"], cmap=\"Blues\", shade=True, shade_lowest=False)","65d87d9a":"sns.kdeplot(data[\"Chance of Admit\"], data[\"University Rating\"], cmap=\"Blues\", shade=True, shade_lowest=False)","c19f2d9d":"sns.kdeplot(data[\"GRE Score\"], data[\"University Rating\"], cmap=\"Blues\", shade=True, shade_lowest=False)","f8676777":"sns.scatterplot(data[\"GRE Score\"], data[\"University Rating\"])","9ecfd90c":"collections.Counter([i-i%0.1+0.1 for i in data[\"Chance of Admit\"]])","fa0bb96f":"data['Label'] = np.where(data[\"Chance of Admit\"] <= 0.72, 0, 1)\nprint(data['Label'].value_counts())\ndata.sample(10)","a9b0894f":"#Checking feature importance with DTree classifier\n# define the model\nmodel = DecisionTreeClassifier()\n\nx = data.drop(columns = ['Chance of Admit', 'Label'])\ny = data['Label']\n\n# fit the model\nmodel.fit(x, y)\n\n# get importance\nimportance = model.feature_importances_\n\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nsmallest(7).plot(kind='barh')","57c70222":"x_train, x_test, y_train, y_test = x[:400], x[400:], y[:400], y[400:]\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","1af8a6ee":"def plot_roc(false_positive_rate, true_positive_rate, roc_auc):\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],linestyle='--')\n    plt.axis('tight')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","7b3ff84a":"parameters = [\n    {\n        'penalty' : ['l1', 'l2', 'elasticnet'],\n        'C' : [0.1, 0.4, 0.5],\n        'random_state' : [0]\n    }\n]\n\ngscv = GridSearchCV(LogisticRegression(),parameters,scoring='accuracy')\ngscv.fit(x_train, y_train)\n\nprint('Best parameters set:')\nprint(gscv.best_params_)\nprint()\n\nprint(\"*\"*50)\nprint(\"Train classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(x_train), y_train))\nprint(confusion_matrix(gscv.predict(x_train), y_train))\n\nprint()\nprint(\"*\"*50)\nprint(\"Test classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(x_test), y_test))\nprint(confusion_matrix(gscv.predict(x_test), y_test))\n\n#Crossvalidation:\ncvs = cross_val_score(estimator = LogisticRegression(), \n                      X = x_train, y = y_train, cv = 12)\n\nprint()\nprint(\"*\"*50)\nprint(cvs.mean())\nprint(cvs.std())","575a166d":"lr = LogisticRegression(C= 0.1, penalty= 'l2', random_state= 0)\nlr.fit(x_train,y_train)\n\ny_pred = lr.predict(x_test)\ny_proba=lr.predict_proba(x_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nplot_roc(false_positive_rate, true_positive_rate, roc_auc)\n\nprint('Accurancy Score :',accuracy_score(y_test, y_pred))\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)","b7db7020":"parameters = [\n    {\n        'criterion' : ['gini', 'entropy'],\n        'max_depth' : [3, 4, 5],\n        'min_samples_split' : [10, 20, 5],\n        'random_state': [0],\n        \n    }\n]\n\ngscv = GridSearchCV(DecisionTreeClassifier(),parameters,scoring='accuracy')\ngscv.fit(x_train, y_train)\n\nprint('Best parameters set:')\nprint(gscv.best_params_)\nprint()\n\nprint(\"*\"*50)\nprint(\"Train classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(x_train), y_train))\nprint(confusion_matrix(gscv.predict(x_train), y_train))\n\nprint()\nprint(\"*\"*50)\nprint(\"Test classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(x_test), y_test))\nprint(confusion_matrix(gscv.predict(x_test), y_test))\n\n#Crossvalidation:\ncvs = cross_val_score(estimator = DecisionTreeClassifier(), \n                      X = x_train, y = y_train, cv = 12)\n\nprint()\nprint(\"*\"*50)\nprint(cvs.mean())\nprint(cvs.std())","97ec772a":"dt = DecisionTreeClassifier(criterion= 'gini', max_depth= 3, min_samples_split= 10, \n                            random_state= 0)\ndt.fit(x_train,y_train)\n\ny_pred = dt.predict(x_test)\ny_proba=dt.predict_proba(x_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nplot_roc(false_positive_rate, true_positive_rate, roc_auc)\n\nprint('Accurancy Score :',accuracy_score(y_test, y_pred))\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)","809c3b0e":"parameters = [\n    {\n        'n_estimators': np.arange(10, 40, 5),\n        'criterion' : ['gini', 'entropy'],\n        'max_depth' : [3, 4, 5],\n        'min_samples_split' : [10, 20, 5],\n        'random_state': [0],\n        \n    }\n]\n\ngscv = GridSearchCV(RandomForestClassifier(),parameters,scoring='accuracy')\ngscv.fit(x_train, y_train)\n\nprint('Best parameters set:')\nprint(gscv.best_params_)\nprint()\n\nprint(\"*\"*50)\nprint(\"Train classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(x_train), y_train))\nprint(confusion_matrix(gscv.predict(x_train), y_train))\n\nprint()\nprint(\"*\"*50)\nprint(\"Test classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(x_test), y_test))\nprint(confusion_matrix(gscv.predict(x_test), y_test))\n\n#Crossvalidation:\ncvs = cross_val_score(estimator = RandomForestClassifier(), \n                      X = x_train, y = y_train, cv = 12)\n\nprint()\nprint(\"*\"*50)\nprint(cvs.mean())\nprint(cvs.std())","3e5efb92":"rf = RandomForestClassifier(criterion= 'gini', max_depth= 5, \n                            min_samples_split= 10, n_estimators= 15, \n                            random_state= 0)\nrf.fit(x_train,y_train)\n\ny_pred = rf.predict(x_test)\ny_proba=rf.predict_proba(x_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nplot_roc(false_positive_rate, true_positive_rate, roc_auc)\n\nprint('Accurancy Score :',accuracy_score(y_test, y_pred))\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)","f93cb399":"parameters = [\n    {\n        'learning_rate': [0.01, 0.02, 0.002],\n        'n_estimators' : np.arange(10, 100, 5),\n        'max_depth' : [3, 4, 5],\n        'min_samples_split' : [10, 20, 5],\n        'random_state': [0],\n        \n    }\n]\n\ngscv = GridSearchCV(GradientBoostingClassifier(),parameters,scoring='accuracy')\ngscv.fit(x_train, y_train)\n\nprint('Best parameters set:')\nprint(gscv.best_params_)\nprint()\n\nprint(\"*\"*50)\nprint(\"Train classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(x_train), y_train))\nprint(confusion_matrix(gscv.predict(x_train), y_train))\n\nprint()\nprint(\"*\"*50)\nprint(\"Test classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(x_test), y_test))\nprint(confusion_matrix(gscv.predict(x_test), y_test))\n\n#Crossvalidation:\ncvs = cross_val_score(estimator = GradientBoostingClassifier(), \n                      X = x_train, y = y_train, cv = 12)\n\nprint()\nprint(\"*\"*50)\nprint(cvs.mean())\nprint(cvs.std())","d431b579":"gbm = GradientBoostingClassifier(learning_rate= 0.02, max_depth= 3, \n                                 min_samples_split= 10, n_estimators= 80, \n                                 random_state= 0)\ngbm.fit(x_train,y_train)\n\ny_pred = gbm.predict(x_test)\ny_proba = gbm.predict_proba(x_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nplot_roc(false_positive_rate, true_positive_rate, roc_auc)\n\nprint('Accurancy Score :',accuracy_score(y_test, y_pred))\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)","7f35a7de":"#for submission using the random forest\ny_proba=rf.predict(x_test)\nnp.sqrt(mean_squared_error(y_proba, y_test))","5d1d8d7c":"# Model 1: Logistic regression\n","baf126cc":"Good that there are no missing values in the dataset. It makes our data pre-processing very much easier.\n\nNow let's check the correlation between the variables.","f9b37b96":"# References:\n\nI refered to lot of other kernels and notebooks as well as lot of stack overflow for the coding doubts, here are the prominent ones that I refered to. Thanks to all the contributers!\n\n1. https:\/\/stackoverflow.com\/questions\/15697350\/binning-frequency-distribution-in-python\n2. https:\/\/machinelearningmastery.com\/calculate-feature-importance-with-python\/\n3. https:\/\/www.kaggle.com\/kralmachine\/analyzing-the-graduate-admission-eda-ml\n","bb712ee0":"# Importing and exploring the datasets\n\nFor importing the packages, I just like to put them in alphabetical order of the package, so that it is easy to manage and review if needed","06771e53":"# Defining the class labels for classification\n\nFor ease of working with the classifier, it will be nice to have a 50\/50 split on the data. \n\nFor class balance, let us assume that the bottom 50% of the observations fall in class 0 (no or less chance of admit), and the top 50% of the observations fall in class 1.\n\nBinning the *Chance of Admit* variable and seeing where the 50% lies","ced361b5":"# Modeling\nSplitting the dataset into train and test and seeing the size","ee726422":"We now have 252 observations in class 0 and 248 observations in class 1, which is good enough balance that we are expecting\n\n# Checking variable importance\nLet us now check what variables are important for out labels. For checking variable importance, we will use a basic decision tree classifier and then check what is the variable importance within the classifier\n\n","68fc512d":"# Conclusion:\nIn this kernel I have learnt and demonstrated how a simple two class binary classification is performed with this dataset.\n\nPlease upvote the kernel if you like it, and to motivate me!\n\nHopefully, this is first of many of my kernels on Kaggle!","fe6e05e9":"# Exploratory data analysis\nThe main EDA that I performed on this dataset is to see how the variables are distributed, to check if the variables are distributed normally. For that the pair plot is used to check the histogram of the variables as well as for the scatter plot to see how the variables are corelated to each other.","30ca98e7":"# Introduction:\nThis is my first Kernel on Kaggle, I wanted to build foundation with a simple classification model.\n\nI choose this dataset because it is clean and simple, with less number of variables and observations, an ideal dataset for me to work on.\n\nI have structured the notebook into the following tasks:\n1. Importing and exploring the dataset\n2. EDA on the dataset\n3. Defining classification labels\n4. Modelling\n5. Conclusion\n6. References","4201cd25":"# Model 4: Gradient boost classifier","71ee0b5f":"# Model 3: Random forest","da30bada":"# Model 2: Decision tree"}}