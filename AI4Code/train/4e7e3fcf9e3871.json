{"cell_type":{"8f4b00e8":"code","f1ff8d79":"code","97fe543a":"code","d3a96b15":"code","912238db":"code","4a3f5ca8":"code","ad849b90":"code","eb4ec4ff":"code","34e83753":"code","aaf902c2":"code","c894f3eb":"code","ca80124c":"code","45446887":"code","ba226f48":"code","d2268eaa":"code","7761a49b":"code","d97ef9f5":"code","ce4c679a":"code","95b31901":"code","61eaa670":"code","ee9b0dc9":"code","4640b4d4":"code","85697852":"code","26338d59":"code","3d8ca560":"code","210ef1f6":"code","62dcf3d3":"code","be909f45":"code","c994ca58":"code","a63e751b":"code","5a185a35":"code","8951942d":"code","5411ae9c":"code","a96972b6":"code","e0f978f1":"code","f9d5ae4c":"markdown","86931e38":"markdown","2ad57431":"markdown","72cc5a89":"markdown","df6fb40d":"markdown","803a6dd4":"markdown","7e2974cb":"markdown","c7b47d2d":"markdown","a3f42a34":"markdown","f012d419":"markdown","b94a989e":"markdown","641d3f09":"markdown","deb829e3":"markdown","f0748b49":"markdown","18531f6f":"markdown","822c4a72":"markdown","c963f67f":"markdown","e8695a2c":"markdown","ee5094a9":"markdown","f80c3fd4":"markdown","fa6f749f":"markdown","73d33995":"markdown","02cb58db":"markdown","8506467c":"markdown","bc36b570":"markdown","e16f13b1":"markdown","a9623e8b":"markdown","e33111fc":"markdown","a55a8127":"markdown"},"source":{"8f4b00e8":"#Note the code in this Notebook requires tensorflow version 2.1.0rc0\n#!pip install grpcio==1.27.2 #This version of grpcio is reqd for tensorflow 2.1\n#!pip install tensorflow==2.1.0rc0\n#tf.__version__\n#!pip install tensorflow-hub","f1ff8d79":"# Common imports\nimport pandas as pd\nimport numpy as np\nimport time\nimport os\nimport warnings\n\n#Time\/CPU Profiling\noverall_start_time= time.time()\n\n# Load the TensorBoard notebook extension.\n%load_ext tensorboard\n\n# Clear any logs from previous runs\n!rm -rf .\/logs\/ \n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n#Disabling Warnings\nwarnings.filterwarnings('ignore')\n\n# To plot figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","97fe543a":"#Verifying pathname of dataset before loading - for Kaggle\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename));\n        print(os.listdir(\"..\/input\"))","d3a96b15":"# Load Datasets\ndef loadDataset(file_name):\n    df = pd.read_csv(file_name,engine = 'python')\n    return df\nstart_time= time.time()\ndf_train = loadDataset(\"\/kaggle\/input\/dataset-of-malicious-and-benign-webpages\/Webpages_Classification_train_data.csv\/Webpages_Classification_train_data.csv\")\ndf_test = loadDataset(\"\/kaggle\/input\/dataset-of-malicious-and-benign-webpages\/Webpages_Classification_test_data.csv\/Webpages_Classification_test_data.csv\")\n#Ensuring correct sequence of columns \ndf_train = df_train[['url','content','label']]\ndf_test = df_test[['url','content','label']]\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","912238db":"start_time= time.time()\ndf_test['content'] = df_test['content'].str.lower()\ndf_test.drop(columns=['url'],inplace=True)\ndf_test.rename(columns={'content':'text'},inplace=True)\ndf_train['content'] = df_train['content'].str.lower()\ndf_train.drop(columns=['url'],inplace=True)\ndf_train.rename(columns={'content':'text'},inplace=True)\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))\n#Looking for NaN, if any\n#print(df_train.isnull().sum())\n#print(df_test.isnull().sum())","4a3f5ca8":"#df_test\n#df_train","ad849b90":"#converting 'label' to numerical value (0-Malicious,1-Benign)\nstart_time= time.time()\ndf_test['label'].replace(to_replace =\"good\", value =1, inplace=True)\ndf_train['label'].replace(to_replace =\"good\", value =1, inplace=True)\ndf_test['label'].replace(to_replace =\"bad\", value =0, inplace=True)\ndf_train['label'].replace(to_replace =\"bad\", value =0, inplace=True)\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","eb4ec4ff":"# No of Classes in Label\ndf_train['label'].unique()","34e83753":"# Class Distribution of Labels\ndf_train.groupby('label').size()","aaf902c2":"# Analysis of Postives and Negatives in the Dataset\nneg, pos = np.bincount(df_train['label'])\ntotal = neg + pos\nprint ('Total of Samples: %s'% total)\nprint('Positive: {} ({:.2f}% of total)'.format(pos, 100 * pos \/ total))\nprint('Negative: {} ({:.2f}% of total)'.format(neg, 100 * neg \/ total))","c894f3eb":"# Representation of Labels in the Stack form\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\n# create dummy variable then group by that\n# set the legend to false because we'll fix it later\ndf_train.assign(dummy = 1).groupby(['dummy','label']).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()).to_frame().unstack().plot(kind='bar',\n    stacked=True,legend=False, color={'red','green'})\n# or it'll show up as 'dummy' \nplt.xlabel('good\/bad Websites')\n# disable ticks in the x axis\nplt.xticks([])\n# fix the legend or it'll include the dummy variable\ncurrent_handles, _ = plt.gca().get_legend_handles_labels()\nreversed_handles = reversed(current_handles)\ncorrect_labels = reversed(['bad','good'])\nplt.legend(reversed_handles,correct_labels)\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n#plt.savefig(\"\/img\/C4.1\/Fig1.png\")\nplt.show()","ca80124c":"#Segregating Validation Set away from the Training Set\ntrain= df_train.iloc[:1000000,]\nval= df_train.iloc[1000001:,]\ntest= df_test.iloc[:,]\n\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')","45446887":"#Converting the dataframes into X, y numpy arrays \nX_train = train['text'].to_numpy()\ny_train = train['label'].astype(int).to_numpy()\nX_val = val['text'].to_numpy()\ny_val = val['label'].astype(int).to_numpy()\nX_test = test['text'].to_numpy()\ny_test = test['label'].astype(int).to_numpy()","ba226f48":"# Using Transfer Learning from Tensorflow hub- Universal Text Encoder\nimport tensorflow_hub as hub\nfrom tensorflow import keras \n\nstart_time= time.time()\n# Use the saved ecoder from Stage I\nencoder = keras.models.load_model(\"\/kaggle\/input\/savedmodel\/PretrainedTFModel\/1\")\n#encoder = hub.load(\"\/kaggle\/input\/savedmodel\/PretrainedTFModel\/1\")\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))\nencoder(['Hello World']) #For Testing the Encoder","d2268eaa":"# Scaling by total\/2 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\nweight_for_0 = (1 \/ neg)*(total)\/2.0 \nweight_for_1 = (1 \/ pos)*(total)\/2.0\nclass_weight = {0: weight_for_0, 1: weight_for_1}\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","7761a49b":"#Using Initial Bias to overcome Class Imbalance\ninitial_bias = np.log([pos\/neg])\ninitial_bias","d97ef9f5":"#Making a Tensorflow Model\nfrom tensorflow import keras\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nMETRICS = [\n      keras.metrics.TruePositives(name='tp'),\n      keras.metrics.FalsePositives(name='fp'),\n      keras.metrics.TrueNegatives(name='tn'),\n      keras.metrics.FalseNegatives(name='fn'), \n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc')]\ndef make_model(metrics = METRICS, output_bias=None):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n    model = keras.Sequential([\n        #First layer is of Universal Text Encoder Deep Averaging Network (DAN) using Transfer Learning\n        hub.KerasLayer(encoder, input_shape=[],dtype=tf.string,trainable=True),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dense(64, activation='relu'),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(32, activation='relu'),\n        keras.layers.Dense(16, activation='relu'),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(1, activation='sigmoid',\n        bias_initializer=output_bias),\n    ])\n    model.compile(optimizer=keras.optimizers.Adam(0.001),loss='binary_crossentropy',metrics=metrics)\n    return model","ce4c679a":"#Initialize the Model\nmodel_zero_bias = make_model()\nmodel_initial_bias = make_model(output_bias = initial_bias)\nmodel_zero_bias.summary() # print model summary with zeor bias\nmodel_initial_bias.summary() # print model summary with initial bias","95b31901":"#Fitting the Model with Class Weights\nfrom datetime import datetime\n\n#Defining Early Stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy', \n    verbose=1,\n    patience=70,\n    mode='max',\n    restore_best_weights=True)\n\n# Define the Keras TensorBoard callback.\nlogdir=\"logs\/fit\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)","61eaa670":"# Fitting the Model without Inital Bias, but with Class weights\nstart_time= time.time()\nzero_bias_history = model_zero_bias.fit(X_train,y_train,batch_size=2048, epochs=100,validation_data=(X_val, y_val),\n          callbacks=[early_stopping,tensorboard_callback],\n          class_weight=class_weight)\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","ee9b0dc9":"#Accuracy over Test Dataset\nresults = model_zero_bias.evaluate(X_test,y_test)\nprint(\"Loss: {0}, Accuracy: {1}\".format(results[0], results[5]))","4640b4d4":"# Fitting the Model without Inital Bias, but with Class weights\nstart_time= time.time()\ninitial_bias_history = model_initial_bias.fit(X_train,y_train,batch_size=2048, epochs=100,validation_data=(X_val, y_val),\n          callbacks=[early_stopping,tensorboard_callback])\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","85697852":"#Accuracy over Test Dataset\nresults = model_initial_bias.evaluate(X_test,y_test)\nprint(\"Loss: {0}, Accuracy: {1}\".format(results[0], results[5]))","26338d59":"#Confusion Matrix\nr=results\ncon_mat_norm= [[r[3]\/(r[3]+r[4]),r[2]\/(r[2]+r[3])],[r[4]\/(r[4]+r[1]),r[1]\/(r[1]+r[4])]]\ncon_mat_df = pd.DataFrame(con_mat_norm,index = ['Malicious','Benign'], columns = ['Malicious','Benign'])\ncon_mat_df","3d8ca560":"# Plotting the Confusion Matrix Using Matploit & Seaborn\nimport seaborn as sns\n\nstart_time= time.time()\nfigure = plt.figure(figsize=(6,6))\nsns.heatmap(con_mat_df,annot=True,cmap=plt.cm.binary,fmt='g',linewidths=0.50,linecolor='black')\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n#plt.savefig(\"\/img\/C4.1\/Fig2:ConfusionMatrix_B&W.png\")\nplt.show()\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","210ef1f6":"mpl.rcParams['figure.figsize'] = (12, 10)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\ndef plot_loss(history, label, n):\n    # Use a log scale to show the wide range of values.\n    plt.semilogy(history.epoch,  history.history['loss'],\n               color=colors[n], label='Train '+label)\n    plt.semilogy(history.epoch,  history.history['val_loss'],\n          color=colors[n], label='Val '+label,\n          linestyle=\"--\")\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()","62dcf3d3":"plot_loss(zero_bias_history, \"Zero Bias\", 0)\nplot_loss(initial_bias_history, \"Initial Bias\", 1)\n#plt.savefig(\"\/img\/C4.1\/Fig3:Bias Plot.png\")","be909f45":"def plot_metrics(history):\n    metrics =  ['loss', 'auc', 'precision', 'recall']\n    for n, metric in enumerate(metrics):\n        name = metric.replace(\"_\",\" \").capitalize()\n        plt.subplot(2,2,n+1)\n        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n        plt.plot(history.epoch, history.history['val_'+metric],\n             color=colors[0], linestyle=\"--\", label='Val')\n        plt.xlabel('Epoch')\n        plt.ylabel(name)\n        if metric == 'loss':\n            plt.ylim([0, plt.ylim()[1]])\n        elif metric == 'auc':\n            plt.ylim([0.8,1])\n        else:\n            plt.ylim([0,1])\n\n        plt.legend()","c994ca58":"plot_metrics(initial_bias_history)\n#plt.savefig(\"\/img\/C4.1\/Fig4:Loss, AuC, Precision & Recall.png\")","a63e751b":"import sklearn\nfrom sklearn import metrics\n\ndef plot_roc(name, labels, predictions, **kwargs):\n    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n\n    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n    plt.xlabel('False Positive Rate [%]')\n    plt.ylabel('True Positive Rate [%]')\n    plt.xlim([-0.5,20])\n    plt.ylim([80,100.5])\n    plt.grid(True)\n    ax = plt.gca()\n    ax.set_aspect('equal')","5a185a35":"train_predictions_baseline = model_initial_bias.predict(X_train, batch_size=2048)\ntest_predictions_baseline = model_initial_bias.predict(X_test, batch_size=2048)","8951942d":"plot_roc(\"Train\", y_train, train_predictions_baseline, color=colors[0])\nplot_roc(\"Test\", y_test, test_predictions_baseline, color=colors[0], linestyle='--')\nplt.legend(loc='lower right')\n#plt.savefig(\"\/img\/C4.1\/Fig5:ROC Curve.png\")","5411ae9c":"tensorboard --logdir logs  --port=8050","a96972b6":"# Total Runtime of this Notebook\nprint(\"***Total Time taken --- %s mins ---***\" % ((time.time() - overall_start_time)\/60))","e0f978f1":"#Clearing Additional load of variables: Creating More RAM Space\nimport gc\n\n#del df_train_good\n#del df_train_bad\n#del df_trial\n#gc.collect()","f9d5ae4c":"### Making and Initializing the TensorFlow Model","86931e38":"## <font color='blue'> Making the Tensor Flow Deep Learning Model <\/font>","2ad57431":"### Display Tensorboard Graphs","72cc5a89":"# <font style=\"color:red;\"> <center>Deep Learning Hybrid Classification Model for Web Content <br> (Balanced Using Weights, Initialized with Output Bias) <\/center><\/font>","df6fb40d":"### Fitting and Training with Initial Bias","803a6dd4":"#### Converting Label Value to 0,1","7e2974cb":"The correct bias to set can be derived from:\n\n$$ p_0 = pos\/(pos + neg) = 1\/(1+e^{-b_0}) $$\n$$ b_0 = -log_e(1\/p_0 - 1) $$\n$$ b_0 = log_e(pos\/neg)$$","c7b47d2d":"### Setting the Initial Bias","a3f42a34":"## <font color=blue> Showing the Influence of Initial Bias <\/font>","f012d419":"## <font color='blue'>Basic Initialisation<\/font>","b94a989e":" ## <font color='blue'>Preprocessing the Dataset <\/font>","641d3f09":"#### Confusion Matrix: Initial Bias Model","deb829e3":"### Defining Early Stopping and Keras Tensorboard","f0748b49":"### Using Transfer Learning - making use of Text Embedding Model from Tensorflow Hub","18531f6f":"### Evaluation of Model & Results: With Zero Bias","822c4a72":"## <font color=blue> Plotting of Metrics: Loss, AUC, Precision & Recall <\/font>","c963f67f":"### Evaluation of Model & Results: With Initial Bias","e8695a2c":"This loss is about 50 times less than a naive initialisation","ee5094a9":"## <font color='blue'> Loading Dataset <\/font>","f80c3fd4":"With this initialization the initial loss should be approximately:\n\n$$-p_0log(p_0)-(1-p_0)log(1-p_0) = 0.01317$$","fa6f749f":"### Miscellaneous Maintenance Code: Run this for Selected Variables to Clear RAM Space\n(Note: Run this selectively only if you are Running Short of Memory)","73d33995":"## <font color=blue> Run Time Profiling Statistics of this Notebook <\/font>","02cb58db":"## <font color =blue> Training the Model <\/font>","8506467c":"#### Cleaning the Dataset","bc36b570":"### Making Class Specific Weights (for handling the Imbalance)","e16f13b1":"### Fitting and Training without Initial Bias","a9623e8b":"#### ROC Plot","e33111fc":"## <font color='blue'>Analysis of Class Imbalance <\/font>","a55a8127":"## <font color='blue'> Earmarking Validation, Train & Test Sets <\/font>"}}