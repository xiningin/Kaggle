{"cell_type":{"920d834b":"code","7f8f65e1":"code","d23ff4a0":"code","5aa2a933":"code","c6bf4a5a":"code","59be4874":"code","3e425c44":"code","c0a8ff1d":"code","e1dc19b1":"code","1ddf37de":"code","a222772d":"code","b8de7630":"code","730c6545":"code","596de8e4":"code","15194917":"code","077554e4":"code","8719d90f":"code","48feb038":"code","2b844b2d":"code","05719968":"code","d86e8b5e":"code","8c210b8b":"code","06d1e0b5":"code","401f13b2":"code","43244584":"code","911b0649":"code","1aee261a":"code","ae7b216c":"code","51764fe5":"code","865a72ff":"code","cecd6770":"code","66613204":"code","fec5c5fb":"code","942590ad":"code","56eda672":"code","3315335c":"code","ffa519a2":"code","c05aeabd":"code","bf5cc464":"code","98f888b3":"code","22145fb1":"code","a5662d59":"code","c97500ed":"code","1cd50ebc":"markdown","714fc911":"markdown","688e545d":"markdown","9310363b":"markdown","071cb6d1":"markdown","6fbada1f":"markdown","a8e22acb":"markdown","991675ad":"markdown","ab4e613e":"markdown","92a10698":"markdown","22f0995e":"markdown","906f2b7f":"markdown","5fbb7661":"markdown","8fd8e8e6":"markdown","83aa1918":"markdown","04dcab47":"markdown","ac125371":"markdown","8b1e87b7":"markdown","f99624a2":"markdown","08f18dc0":"markdown","2e980f49":"markdown","ac280c86":"markdown","894ca541":"markdown","54ea4c06":"markdown","ff6efd1f":"markdown","a3c587fa":"markdown","a11ff1fb":"markdown","bc737199":"markdown","c56a69d6":"markdown","4ebd7dd6":"markdown","7c5b1f9a":"markdown","2c2626b0":"markdown","89e0a200":"markdown","8ba8fc92":"markdown","3640376e":"markdown","cab6b6e1":"markdown","203592ed":"markdown","886b589b":"markdown","608e3671":"markdown","4e8362c5":"markdown","22145673":"markdown","29ca3d6e":"markdown","8a64db94":"markdown","d1c3b7e6":"markdown","b677b435":"markdown","bef14cb3":"markdown","8aebdb52":"markdown","ec0603b5":"markdown","25b88b4e":"markdown","0795bfe2":"markdown","048a1473":"markdown","783fe23d":"markdown","44087a27":"markdown","0f5ebbc2":"markdown","1768f203":"markdown","628adc0d":"markdown","58de5ed1":"markdown","7dedd5be":"markdown","92dc149e":"markdown"},"source":{"920d834b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f8f65e1":"import pandas as pd \nimport numpy as np \nfrom time import time \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.patches as mpatches\n\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.simplefilter(action='ignore',category=FutureWarning)\nimport plotly.express as px","d23ff4a0":"train_data = pd.read_csv('\/kaggle\/input\/udacity-mlcharity-competition\/census.csv')\ndata = train_data.copy()\ndata.head()","5aa2a933":"data.dtypes","c6bf4a5a":"#checking for missing values\ndata.isnull().sum()","59be4874":"#creating features for EDA\ndata['income_above_50k'] = data['income'].apply(lambda x: 1 if x == '>50K' else 0)\ndata['income_below_50k'] = data['income'].apply(lambda x: 1 if x != '>50K' else 0)\ndata.head()","3e425c44":"#categorical variables \ncat = [cat for cat in data.columns if data[cat].dtype=='object']\n\ndef cat_features(df,col):\n    print(col + \" features has \" + str(df[col].nunique()) + \" categories.\")\n    \nfor col in cat:\n    cat_features(data,col)","c0a8ff1d":"#distribution of the target class\ndf = data[['income_above_50k','income_below_50k']].sum().to_frame()\ndf.reset_index(inplace=True)\ndf.columns = ['target_class','count']\nfig=px.bar(df,x='target_class',y='count', title='Distribution of Target Class', height = 500, width = 450)\nfig.show()","e1dc19b1":"#workclass\ndf = data.groupby(by='workclass')['income_above_50k','income_below_50k'].sum()\ndf.reset_index(inplace=True)\nfig = px.bar(df.sort_values(by='income_below_50k'), y='workclass',x=['income_below_50k','income_above_50k'], title=\"Workclass Distribution\",height=380)\nfig.update_layout(xaxis_title=\"count\")\nfig.show()","1ddf37de":"#education_level\ndf = data.groupby(by='education_level')['income_above_50k','income_below_50k'].sum()\ndf.reset_index(inplace=True)\nfig = px.bar(df.sort_values(by='income_below_50k'), y='education_level',x=['income_below_50k','income_above_50k'], title=\"Education_level Distribution\")\nfig.update_layout(xaxis_title=\"count\")\nfig.show()","a222772d":"#marital status\ndf = data.groupby(by='marital-status')['income_above_50k','income_below_50k'].sum()\ndf.reset_index(inplace=True)\nfig = px.bar(df.sort_values(by='income_below_50k'), y='marital-status',x=['income_below_50k','income_above_50k'], title=\"marital-status Distribution\", height=380)\nfig.update_layout(xaxis_title=\"count\")\nfig.show()","b8de7630":"#occupation\ndf = data.groupby(by='occupation')['income_above_50k','income_below_50k'].sum()\ndf.reset_index(inplace=True)\nfig = px.bar(df.sort_values(by='income_below_50k'), y='occupation',x=['income_below_50k','income_above_50k'], title=\"occupation Distribution\")\nfig.update_layout(xaxis_title=\"count\")\nfig.show()","730c6545":"#race \ndf = data.groupby(by='race')['income_above_50k','income_below_50k'].sum()\ndf.reset_index(inplace=True)\nfig = px.bar(df.sort_values(by='income_below_50k'), y='race',x=['income_below_50k','income_above_50k'], title=\"race Distribution\", height=380)\nfig.update_layout(xaxis_title=\"count\")\nfig.show()","596de8e4":"#sex \ndf = data.groupby(by='sex')['income_above_50k','income_below_50k'].sum()\ndf.reset_index(inplace=True)\nfig = px.bar(df.sort_values(by='income_below_50k'), x='sex',y=['income_below_50k','income_above_50k'], title=\"sex Distribution\", height=500, width=500)\nfig.update_layout(yaxis_title=\"count\")\nfig.show()","15194917":"#dropping features income_above_50k and income_below_50k\ndata.drop(['income_above_50k'], axis = 1,inplace =True)\ndata.drop(['income_below_50k'], axis = 1,inplace =True)","077554e4":"plt.figure(figsize=(15,12))\n\nplt.subplot(3,3,1)\nplt.hist(data['age'])\nplt.title('Distribution of Age')\n\nplt.subplot(3,3,2)\nplt.hist(data['education-num'])\nplt.title('Distribution of Education Years')\n\nplt.subplot(3,3,3)\nplt.hist(data['capital-gain'])\nplt.xlim([0, 22000])\nplt.ylim([0, 50000])\nplt.title('Distribution of Capital Gains')\n\nplt.subplot(3,3,4)\nplt.hist(data['capital-loss'], bins = 5)\nplt.xlim([0, 4000])\nplt.title('Distribution of Capital Losses')\n\nplt.subplot(3,3,5)\nplt.hist(data['hours-per-week'])\nplt.title('Distribution of Working hours per Week')\n","8719d90f":"#Relation between education years and education Level\nfig = px.box(data.sort_values(by='education-num', ascending = False), x=\"education_level\", y=\"education-num\",width=1000, height=400)\nfig.show()","48feb038":"#education level and number of education years represent the same quantity. \n#Dropping education level, number of education years represents education level when label encoded. \ndata.drop(['education_level'], axis = 1,inplace =True)","2b844b2d":"#encoding income attribute to 1 for income above 50k. Storing target series in a separate variable\ndata['income'] = data['income'].apply(lambda x: 1 if x == '>50K' else 0 )\ntarget = data['income']\ndata.drop(['income'],axis=1,inplace=True)","05719968":"#prcoessing numberical features \n#transforming skewed continous features - capital gain and capital losses \n\ndata['capital-gain']=data['capital-gain'].apply(lambda x: np.log(x+1))\ndata['capital-loss']=data['capital-loss'].apply(lambda x: np.log(x+1))\n\n#scaling numerical features so that all features are treated equally by the model\nscale_features = ['age','education-num','capital-gain','capital-loss','hours-per-week']\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata[scale_features] = scaler.fit_transform(data[scale_features])\n\n#one hot encoding fot categorical features\nencoded_features = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\ndata = pd.get_dummies(data,columns = encoded_features ,prefix = encoded_features, drop_first=True)\ndata.head()","d86e8b5e":"# #function for feature processing for a given data set\n# def process_df(df, scale_features,encoded_features):\n#     df.drop(['education_level'], axis =1, inplace=True)\n#     df['capital-gain']=df['capital-gain'].apply(lambda x: np.log(x+1))\n#     df['capital-loss']=df['capital-loss'].apply(lambda x: np.log(x+1))\n#     df[scale_features] = scaler.transform(df[scale_features])\n#     df = pd.get_dummies(df,columns = encoded_features ,prefix = encoded_features, drop_first=True)\n#     return df     ","8c210b8b":"#baseline model and evaluation of base line model\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\n\ndummyClassifier = DummyClassifier(strategy=\"most_frequent\")\ndummyClassifier.fit(data,target)\npred = dummyClassifier.predict(data)\nprint(\"Accuracy Score: \",accuracy_score(target,pred))\nprint(\"F1 Score: \",f1_score(target,pred))\nprint(\"ROC-AUC Score: \", roc_auc_score(target,pred))","06d1e0b5":"from sklearn.model_selection import train_test_split\n\nx_train,x_val,y_train,y_val = train_test_split(data,target,test_size=0.2,random_state=42)\nprint(\"Training set has {} samples\".format(x_train.shape[0]))\nprint(\"Testing set has {} samples\".format(x_val.shape[0]))","401f13b2":"from sklearn.metrics import roc_auc_score\n\ndef evaluate_model(model,x_train,x_test,y_train,y_test):\n    results = {}\n    \n    start = time()\n    model.fit(x_train,y_train)\n    end = time()\n    \n    results['train_time'] = end - start \n    \n    start = time()\n    test_pred = model.predict(x_test)\n    train_pred = model.predict(x_train[:300])\n    end = time()\n    \n    results['pred_time'] = end - start \n    \n    results['roc_train'] = roc_auc_score(y_train[:300],train_pred)\n    \n    results['roc_test'] = roc_auc_score(y_test,test_pred)\n    \n    return results","43244584":"def compare_model(results):\n    \n    fig, axes = plt.subplots(2,2,figsize=(18,10))\n    \n    colour = ['rosybrown','lightcoral','indianred','brown','maroon']\n    \n    for i,model in enumerate(results.keys()):\n        for j, metric in enumerate(['train_time','pred_time','roc_train','roc_test']):\n            \n            axes[j\/\/2,j%2].bar(i,results[model][metric],color = colour[i])\n            \n    #setting y labels\n    axes[0,0].set_ylabel(\"Time in sec\")\n    axes[0,1].set_ylabel(\"Time in sec\")\n    axes[1,0].set_ylabel(\"ROC-AUC score\")\n    axes[1,1].set_ylabel(\"ROC-AUC score\")\n    \n    #setting titles \n    axes[0,0].set_title(\"Model Training\")\n    axes[0,1].set_title(\"Model Prediction\")\n    axes[1,0].set_title(\"ROC-AUC for Training Set\")\n    axes[1,1].set_title(\"ROC-AUC for Test Set\")\n    \n    axes[1,0].set_ylim((0,1))\n    axes[1,1].set_ylim((0,1))\n    \n    patches = []\n    for i, model in enumerate(results.keys()):\n        patches.append(mpatches.Patch(color = colour[i], label = model))\n    axes[0, 0].legend(handles = patches)\n    \n    \n    plt.suptitle(\"Performance Metrics for Supervised Learning Models\", fontsize = 16, y = 1.10)\n    plt.tight_layout()\n    plt.show()","911b0649":"#importing models for machine learning \n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\nrandom_state = 1\nn_estimators = 100 \n\nclf1 = GaussianNB()\nclf2 = DecisionTreeClassifier()\nclf3 = RandomForestClassifier(random_state = random_state, n_estimators = n_estimators)\nclf4 = BaggingClassifier(random_state = random_state, n_estimators = n_estimators)\nclf5 = GradientBoostingClassifier(random_state = random_state, n_estimators = n_estimators)\n\nresults = {}\nfor clf in [clf1,clf2,clf3,clf4,clf5]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = evaluate_model(clf,x_train,x_val,y_train,y_val)\n\ncompare_model(results)","1aee261a":"#hyperparameter tuning for gradient boosting classifier \n\n#finding the number of estimators\nfrom sklearn.metrics import make_scorer \nfrom sklearn.model_selection import GridSearchCV\n\nclf = GradientBoostingClassifier(random_state = random_state)\n\nparameters = {\n    'n_estimators' : range(20,101,20),\n    'learning_rate' : [0.2],\n    'min_samples_split': [500],\n    'min_samples_leaf' : [50],\n    'max_depth': [8],\n    'subsample': [0.8] \n}\n\nscorer = make_scorer(roc_auc_score)\n\ngrid_cv = GridSearchCV(clf,param_grid=parameters,scoring=scorer,verbose = 1, n_jobs = -1)\ngrid_fit = grid_cv.fit(x_train,y_train)\n\nbest_clf = grid_fit.best_estimator_\n\npredictions = clf.fit(x_train,y_train).predict(x_val)\nbest_pred_val = best_clf.predict(x_val)\nbest_pred_train = best_clf.predict(x_train[:300])\n\nprint(\"For unoptimized model ----------\")\nprint(\"The ROC-AUC score on validation data is {}\".format(roc_auc_score(y_val,predictions)))\nprint(\"\")\nprint(\"\")\n\n\nprint(\"For optimized model ----------\")\nprint(\"The ROC-AUC score on validation data is {}\".format(roc_auc_score(y_val,best_pred_val)))\nprint(\"The ROC-AUC score on training data is {}\".format(roc_auc_score(y_train[:300],best_pred_train)))\nprint(\"\")\nprint(\"-------------\")\nprint(\"The parameters for optimized model are : \", grid_cv.best_params_)","ae7b216c":"#finding the min_sample_split and depth\n\nclf = GradientBoostingClassifier(random_state = random_state)\n\nparameters = {\n    'n_estimators' : [80],\n    'learning_rate' : [0.2],\n    'min_samples_split': range(100,601,100),\n    'min_samples_leaf' : [50],\n    'max_depth': range(2,12,2),\n    'subsample': [0.8] \n}\n\nscorer = make_scorer(roc_auc_score)\n\ngrid_cv = GridSearchCV(clf,param_grid=parameters,scoring=scorer,verbose = 1, n_jobs = -1)\ngrid_fit = grid_cv.fit(x_train,y_train)\n\nbest_clf = grid_fit.best_estimator_\n\nbest_pred_val = best_clf.predict(x_val)\nbest_pred_train = best_clf.predict(x_train[:300])\n\nprint(\"For optimized model ----------\")\nprint(\"The ROC-AUC score on validation data is {}\".format(roc_auc_score(y_val,best_pred_val)))\nprint(\"The ROC-AUC score on training data is {}\".format(roc_auc_score(y_train[:300],best_pred_train)))\nprint(\"\")\nprint(\"-------------\")\nprint(\"The parameters for optimized model are : \", grid_cv.best_params_)","51764fe5":"#finding the min_samples_leaf\n\nclf = GradientBoostingClassifier(random_state = random_state)\n\nparameters = {\n    'n_estimators' : [80],\n    'learning_rate' : [0.2],\n    'min_samples_split': [100],\n    'min_samples_leaf' : range(20,81,10),\n    'max_depth': [6],\n    'subsample': [0.8] \n}\n\nscorer = make_scorer(roc_auc_score)\n\ngrid_cv = GridSearchCV(clf,param_grid=parameters,scoring=scorer,verbose = 1, n_jobs = -1)\ngrid_fit = grid_cv.fit(x_train,y_train)\n\nbest_clf = grid_fit.best_estimator_\n\nbest_pred_val = best_clf.predict(x_val)\nbest_pred_train = best_clf.predict(x_train[:300])\n\nprint(\"For optimized model ----------\")\nprint(\"The ROC-AUC score on validation data is {}\".format(roc_auc_score(y_val,best_pred_val)))\nprint(\"The ROC-AUC score on training data is {}\".format(roc_auc_score(y_train[:300],best_pred_train)))\nprint(\"\")\nprint(\"-------------\")\nprint(\"The parameters for optimized model are : \", grid_cv.best_params_)","865a72ff":"#finding the subsample\n\nclf = GradientBoostingClassifier(random_state = random_state)\n\nparameters = {\n    'n_estimators' : [80],\n    'learning_rate' : [0.2],\n    'min_samples_split': [100],\n    'min_samples_leaf' : [50],\n    'max_depth': [6],\n    'subsample': [0.7,0.75,0.8,0.85,0.9] \n}\n\nscorer = make_scorer(roc_auc_score)\n\ngrid_cv = GridSearchCV(clf,param_grid=parameters,scoring=scorer,verbose = 1, n_jobs = -1)\ngrid_fit = grid_cv.fit(x_train,y_train)\n\nbest_clf = grid_fit.best_estimator_\n\nbest_pred_val = best_clf.predict(x_val)\nbest_pred_train = best_clf.predict(x_train[:300])\n\nprint(\"For optimized model ----------\")\nprint(\"The ROC-AUC score on validation data is {}\".format(roc_auc_score(y_val,best_pred_val)))\nprint(\"The ROC-AUC score on training data is {}\".format(roc_auc_score(y_train[:300],best_pred_train)))\nprint(\"\")\nprint(\"-------------\")\nprint(\"The parameters for optimized model are : \", grid_cv.best_params_)","cecd6770":"#finding the learning rate \n\nclf = GradientBoostingClassifier(random_state = random_state)\n\nparameters = {\n    'n_estimators' : range(80,400,40),\n    'learning_rate' : [0.05,0.1,0.2],\n    'min_samples_split': [100],\n    'min_samples_leaf' : [50],\n    'max_depth': [6],\n    'subsample': [0.8] \n}\n\nscorer = make_scorer(roc_auc_score)\n\ngrid_cv = GridSearchCV(clf,param_grid=parameters,scoring=scorer,verbose = 1, n_jobs = -1)\ngrid_fit = grid_cv.fit(x_train,y_train)\n\nbest_clf = grid_fit.best_estimator_\n\nbest_pred_val = best_clf.predict(x_val)\nbest_pred_train = best_clf.predict(x_train[:300])\n\nprint(\"For optimized model ----------\")\nprint(\"The ROC-AUC score on validation data is {}\".format(roc_auc_score(y_val,best_pred_val)))\nprint(\"The ROC-AUC score on training data is {}\".format(roc_auc_score(y_train[:300],best_pred_train)))\nprint(\"\")\nprint(\"-------------\")\nprint(\"The parameters for optimized model are : \", grid_cv.best_params_)","66613204":"#fitting gradient boosting classifiers with optimum parameters \nfinal_gbc_clf = GradientBoostingClassifier(random_state=random_state, learning_rate = 0.1 , n_estimators = 280 , max_depth = 6, min_samples_leaf = 50, \n                                           min_samples_split = 100, subsample = 0.8 )  \nfinal_gbc_clf.fit(x_train,y_train)\nfinal_pred_gbc_clf_val = final_gbc_clf.predict(x_val)\nfinal_pred_gbc_clf_train = final_gbc_clf.predict(x_train[:300])\n\nprint(\"The AUC-ROC score for optimized Gradient Boosting Classifier on the Validation Dataset is \",roc_auc_score(y_val,final_pred_gbc_clf_val))\nprint(\"The AUC-ROC score for optimized Gradient Boosting Classifier on the Training Dataset is \",roc_auc_score(y_train[:300],final_pred_gbc_clf_train))","fec5c5fb":"#ROC-AUC score using probabilities \nprob_val_gbc = final_gbc_clf.predict_proba(x_val)[:,1]\nprob_train_gbc = final_gbc_clf.predict_proba(x_train[:300])[:,1]\n\nprint(\"The AUC-ROC score for optimized Gradient Boosting Classifier on the Validation Dataset using probability \",roc_auc_score(y_val,prob_val_gbc))\nprint(\"The AUC-ROC score for optimized Gradient Boosting Classifier on the Training Dataset using probability \",roc_auc_score(y_train[:300],prob_train_gbc))","942590ad":"#plotting roc-auc curve and finding optimum threshold for classification\n\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_val, prob_val_gbc)\n\nplot_roc_curve(fpr,tpr)\nprint(\"The ROC-AUC Score is \",roc_auc_score(y_val,prob_val_gbc))","56eda672":"optimal_idx = np.argmax(tpr-fpr)\noptimal_threshold = thresholds[optimal_idx]\nprint(\"Threshold value is:\", optimal_threshold)","3315335c":"y_pred = prob_val_gbc.copy()\nfor i in range(len(y_pred)):\n    if (y_pred[i] > optimal_threshold):\n        y_pred[i] = 1\n    else: \n        y_pred[i] = 0 \ny_pred","ffa519a2":"print(\"The ROC-AUC Score achieved by selecting the optimum threshold for classification is\" ,roc_auc_score(y_val,y_pred))","c05aeabd":"#procesing test data\nscale_features = ['age','education-num','capital-gain','capital-loss','hours-per-week']\nencoded_features = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n\ntest_data = pd.read_csv('\/kaggle\/input\/udacity-mlcharity-competition\/test_census.csv')\ntest_data.drop(['Unnamed: 0'],axis=1,inplace = True)\ntest_data.head()","bf5cc464":"test_data.isnull().sum()","98f888b3":"#filling missing values\nfor col in ['age','education-num','hours-per-week']:\n    test_data[col] = test_data[col].fillna(train_data[col].mean())\n    \nfor col in ['capital-gain','capital-loss']:\n    test_data[col] = test_data[col].fillna(train_data[col].median())\n    \nfor col in ['workclass','education_level','marital-status','occupation','relationship','race','sex','native-country']:\n    test_data[col]= test_data[col].fillna(train_data.groupby([col])[col].count().sort_values(ascending=False).index[0])\n    \ntest_data.isnull().sum()","22145fb1":"test_data.drop(['education_level'],axis=1,inplace = True)\ntest_data['capital-gain']=test_data['capital-gain'].apply(lambda x: np.log(x+1))\ntest_data['capital-loss']=test_data['capital-loss'].apply(lambda x: np.log(x+1))\ntest_data[scale_features] = scaler.transform(test_data[scale_features])\ntest_data = pd.get_dummies(test_data,columns = encoded_features ,prefix = encoded_features, drop_first=True)\ntest_data.head()","a5662d59":"#predicting on test dataset \npred_test = final_gbc_clf.predict(test_data)\nprob_test = final_gbc_clf.predict_proba(test_data)[:,1]\n\n\nsubmission_pred = prob_test.copy()\nfor i in range(len(submission_pred)):\n    if (submission_pred[i] > optimal_threshold):\n        submission_pred[i] = int(1)\n    else: \n        submission_pred[i] = int(0) \nsubmission_pred","c97500ed":"submission_df = pd.DataFrame(data=submission_pred)\nsubmission_df.reset_index(inplace=True)\nsubmission_df.columns = ['id','income']\nsubmission_df.to_csv(\"submission.csv\", index=False)","1cd50ebc":"Looking at the distribution of education-level, it can be concluded that most wealthy people hold a college or higher level of degree.","714fc911":"# Feature Transformation","688e545d":"****","9310363b":"### Description of Attributes used in the Dataset\n\n- age: continuous.\n- workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n- education-num: continuous.\n- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n- sex: Female, Male.\n- capital-gain: continuous.\n- capital-loss: continuous.\n- hours-per-week: continuous.\n- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.","071cb6d1":"### Final Optimized Model","6fbada1f":"# Sending out invitations for CharityML Event","a8e22acb":"The optimized gradient boosting classifier is used for prediction of probabilites for datapoints to belong to class with value 1. The optimum threshold value obtained earlier is used for classification to correct labels.","991675ad":"# Processing of Test Data","ab4e613e":"The submission dataset is prepared.","92a10698":"*****","22f0995e":"******","906f2b7f":"# Hyperparameter Tuning","5fbb7661":"Looking at the performance of the different supervised learning models it is evident that Gradient Boosting Classifier is the best performing alogrithm given its high ROC-AUC score on the test data and comparitively lesser model training and prediction time. Gradient Boosting Classifier is further tuned to improve performance over test dataset.","8fd8e8e6":"Keeping the n_estimators fixed at 80, the model is tuned for min_samples_split and max_depth.","83aa1918":"The gradient boosting classifier is trained on the training data with the optimum hyperparameters. The final ROC-AUC score achieved on the test data after prediction is 0.8022.","04dcab47":"# Data Exploration","ac125371":"In the test dataset, we observe missing values for the above attributes. The missing values are imputed using median for skewed features like captial_loss and capital_gain. For continous attributes, the mean is used for replacing null points and categorical features are filled using the most frequent occuring value for the feature.","8b1e87b7":"The males constitute a greater proportion of wealthy people.\nSo it can be summarized that a wealthy person may have these characteristics - he is a white male college graduate with a job in the private sector and marries to a civilian spouse.","f99624a2":"# Prediction for Test Data & Submission","08f18dc0":"### Applying Machine Learning Models","2e980f49":"A generic function evaluate_model is written to evaluate the performance of a given model in terms of training time, prediction time, ROC-AUC score of training and testing set. The results of these models can be compared visually using the compare_model method.","ac280c86":"On optimizing the number of estimators to 80 in the classifier, the ROC-AUC score has increased from 0.78 to 0.802.","894ca541":"The dataset is divided into training and validation sets using train-test split. The size of the testing sample is 20% of the entire dataset.","54ea4c06":"# Introduction","ff6efd1f":"### Exploring Numerical Features ","a3c587fa":"On predicting the outcome using predict_proba() method which gives the probability of the data point to belong to class label with value 1, we get a higher ROC-AUC score of 0.93 on the test data. \nThe predict() method considers a default threshold of 0.5 for classification. So datapoints with probability > 0.5 are predicted to belong to class with label 1. \nClearly this dataset is imbalanced, so the optimum threshold value needs to be calculated by plotting the ROC-AUC curves.","a11ff1fb":"******","bc737199":"Similar, transformation and prcessing is applied to the test dataset as the train dataset.","c56a69d6":"### Exploring Categorical Features","4ebd7dd6":"The dataset is imported using Pandas and stored in a dataframe. The datatype of each column is checked.","7c5b1f9a":"Capital Losses and Capital Gains have a very skewed distribution. \nWe perform log normalisation on these skewed features. The numerical features are also scaled using Min-Max scaler. \nFurther the categorical variables are encoded using one-hot encoding to convert them into numerical features.","2c2626b0":"Before applying any machine learning model to the dataset, we use a dummy classifier to establish a baseline accuracy and ROC-AUC score. The dummy classifier predicts the majority class label for every datapoint. On applying any machine learning alogorithm, the classification should improve, so should the accuracy and ROC-AUC score.","89e0a200":"The plot above shows the distribution of wealthy people in different occupations.","8ba8fc92":"After finding the optimum hyperparamters, the learning rate is tuned. The learning rate is inversely proportional to n_estimators. On decreasing the learning rate, the n_estimators also has to tuned for higher values. ","3640376e":"The current ROC-AUC score is 0.5, indicating that the dummy classifier is unable to distinguish between positive and negative groups.","cab6b6e1":"******","203592ed":"Most people work in the Private sector. The private sector also indicates a high presence of wealthy people.","886b589b":"Firstly, the categorical features are explored to draw characteristics of wealthy people.","608e3671":"Thus, the model is finally optimised. ","4e8362c5":"# Modeling ","22145673":"Education level attribute is dropped as discussed. The target variable - income is spearate from the rest of the dataset.","29ca3d6e":"For the purpose of classification with 80 attributes, 5 models were used for prediction - Naive Bayes, Decision Tree Classifier, Graident Boosting Classifier, Random Forest Classifier and Bagging Classifier.\n\nThis is decided keeping in mind the size of the dataset. Ensemble methods (Random Forest, Gradient Boosting, Bagging Classifier) tend to perform better in case of non-linear data. These models reduce variance and run efficiently on large datasets. Time for training can be large while fitting multiple estimators. \nNaive Bayes performs well with categorical data. The correlation between features however can affect its performance. \n\nI have not considered other classification algorithms such as K-Nearest Neighbours (slow in predicting for a dataset with large number of attributes) and logistic regression (Absence of linear correlation between features).\n\nn_estimators in tree based algorithms is considered 100 for now","8a64db94":"The optimum threshold value is found to be 0.2285.","d1c3b7e6":"Since every education level corresponds to the number of education year, these features are essentially the same. Education level can be dropped. education_num represents education_level when label encoded.","b677b435":"The training data is checked for missing values. No imputation is required since no data is missing. Exploratory Data Analysis is carried out to understand the attributes in the dataset. \nTwo features are created for this purpose, income_above_50k and income_below_50k. The value 1 for these features indicates their presence.","bef14cb3":"In this project, I have applied supervised learning techniques on data collected for the U.S. census to help CharityML (a fictitious charity organization) identify people most likely to donate to their cause. The dataset provides information regarding the education level, occupation, sex, marital status, native origin and working conditions of indivduals to determine if their income is above or below 50,000USD. Folks earning greater than 50,000USD are more likely to donate to CharityML. Sending out select invitations to these people minimizes costs and efforts of the organisation.\n\nI have evaluated the data against Naive Bayes, Random Forest Classifier, Bagging Classifier, Decision Tree and Gradient Boosting Classifier. The evaluation metric used for measuring performance is ROC-AUC score. The best performing model is optimised further by tuning the hyperparameters. The ROC-AUC curve is plotted to calculate the optimum threshold value for classification. The test data is processed before making predictions.\n\n\n\n### The analysis of the data contains the following steps - \n1. Data Exploration\n2. Feature Transformation\n3. Modeling\n4. Hyperparameter Tuning\n5. Threshold Selection for Classification\n5. Processing of Test Data\n6. Prediction for Test Data & Submission","8aebdb52":"# Threshold Selection for Classification","ec0603b5":"Most wealthy people have a civilian spouse.","25b88b4e":"To understand the distribution of continuous numerical attributes, we plot histograms for each of them.","0795bfe2":"I have tuned for the following parameters in Gradient Boosting Classifier using Grid Search Cross Validation:\n1. n_estimators \n2. learning_rate\n3. min_sample_split\n4. min_sample_leaf\n5. max_depth \n6. subsample\n\nFirstly, all parameters except the n_estimators is kept constant for the Gradient Boosting Classifier.","048a1473":"*****","783fe23d":"It is evident from the dataset that there is a unequal distribution of the target class. About 25% of population have income above $50,000. ","44087a27":"The optimal max_depth is 6 and min_sample_split is 100. The ROC-AUC is similar. The min_samples_leaf is further tuned.","0f5ebbc2":"Data points with probabilities > optimum threshold will be considered to belong to the class with value 1.","1768f203":"### Train-Test Split","628adc0d":"*****","58de5ed1":"<div>\n<img src=\"https:\/\/images.unsplash.com\/photo-1591522810850-58128c5fb089?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1770&q=80\" width=\"900\" style=\"margin-left:auto; margin-right:auto\"\/>\n<\/div>","7dedd5be":"The ROC-AUC curve is plotted by finding the true positive rate (TPR) and false positive rate (FPR) for various threshold values for classification. The threshold value which has the maximum value of (TPR - FPR) indicates better ability to classify points to their correct labels.","92dc149e":"The model is then tuned for the optimum subsample."}}