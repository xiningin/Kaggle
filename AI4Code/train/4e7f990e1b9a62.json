{"cell_type":{"996c860e":"code","24fe6628":"code","e272e1b6":"code","b416f2f3":"code","9ea3aa69":"code","d8d71e4b":"code","773e47f8":"code","e6f3d205":"code","7a2cb34f":"code","e7c1f12f":"code","2db39b44":"code","58b852c7":"markdown","6b5f9139":"markdown","2c5e8b77":"markdown","986e9672":"markdown","af99c1d1":"markdown","4731dd74":"markdown","2273b089":"markdown","adc6d1a8":"markdown","5ab1b8ff":"markdown","15dee075":"markdown","9fc30a9e":"markdown"},"source":{"996c860e":"import numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense \nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\npyo.init_notebook_mode(connected=True)\n\nimport os\nimport cv2\nfrom sklearn.model_selection import train_test_split\n\nfrom random import randint\n","24fe6628":"train_dir = '..\/input\/rock-paper-scissor\/rps\/rps'\ntest_dir = '..\/input\/rock-paper-scissor\/rps-test-set\/rps-test-set'\nclasses = os.listdir(train_dir)\nclasses","e272e1b6":"X_train_val = []\ny_train_val = []\nfor i, cls in enumerate(os.listdir(train_dir)):\n    for img in os.listdir(os.path.join(train_dir, cls)):\n        image = cv2.imread(os.path.join(train_dir,cls,img),cv2.IMREAD_GRAYSCALE)\n        X_train_val.append(image)\n        y_train_val.append(i)\nX_test = []\ny_test = []\nfor i, cls in enumerate(os.listdir(test_dir)):\n    for img in os.listdir(os.path.join(test_dir, cls)):\n        image = cv2.imread(os.path.join(test_dir,cls,img),cv2.IMREAD_GRAYSCALE)\n        X_test.append(image)\n        y_test.append(i)\nX_test = np.array(X_test)\nX_train_val = np.array(X_train_val)\ny_test = np.array(y_test)\ny_train_val = np.array(y_train_val)\nX_train_val = X_train_val.reshape(*(X_train_val.shape), 1)\nX_test = X_test.reshape(*(X_test.shape), 1)\n\nprint(f'There are {X_train_val.shape[0]} samples in training set')\nprint(f'There are {X_test.shape[0]} samples in test set')","b416f2f3":"fig, axes = plt.subplots(4,4,figsize=(16,16))\nfor i in range(16):\n    idx = randint(0,X_train_val.shape[0]-1)\n    img = X_train_val[idx]\n    label = y_train_val[idx]\n    axes[i\/\/4][i%4].imshow(img,cmap='gray')\n    axes[i\/\/4][i%4].title.set_text(classes[label])","9ea3aa69":"X_train, X_val, y_train,y_val = train_test_split(X_train_val,y_train_val,stratify=y_train_val,test_size = 0.15)","d8d71e4b":"train_datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1,rescale=1.0\/255)\ntrain_generator = train_datagen.flow(X_train,y_train, shuffle=True)\nX_val = X_val\/255.0\nX_test=X_test\/255.0","773e47f8":"model = Sequential([\n    Conv2D(32, 3, strides = 2, input_shape=(300,300,1),activation='relu'),\n    MaxPooling2D(2),\n    Conv2D(64, 3,strides=2,activation='relu'),\n    MaxPooling2D(2),\n    Conv2D(64, 3,activation='relu'),\n    MaxPooling2D(2),\n    Conv2D(64, 3,activation='relu'),\n    MaxPooling2D(2),\n    Flatten(),\n    Dropout(0.5),\n    Dense(256, activation='relu'),\n    Dropout(0.5),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(3,activation='softmax')\n])\nmodel.summary()","e6f3d205":"earlystop = EarlyStopping(patience=8, restore_best_weights=True,min_delta=1e-3)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=100, validation_data=(X_val,y_val), callbacks=[earlystop])\n","7a2cb34f":"metrics = pd.DataFrame(history.history)\npx.line(metrics[['loss','val_loss']])","e7c1f12f":"loss, accuracy = model.evaluate(X_val,y_val)\nprint(f\"Model could classify {int(100*accuracy)}% of images in test set correctly \")","2db39b44":"fig, axes = plt.subplots(4,4,figsize=(16,16))\nfor i in range(16):\n    idx = randint(0,X_test.shape[0]-1)\n    img = X_test[idx]\n    label = classes[np.argmax(model.predict(np.array([img])))]\n    axes[i\/\/4][i%4].imshow(img,cmap='gray')\n    axes[i\/\/4][i%4].title.set_text(label)","58b852c7":"# Preprocessing","6b5f9139":"Let's look at some of the images from training set.","2c5e8b77":"# Modelling","986e9672":"That's awesome let's confirm that 100% accuracy is real","af99c1d1":"That's it for this notebook. If you have suggestions, please write them in the comments.","4731dd74":"I will use 15% of training set as validation set and only manipulation I will do is dividing each value by 255.0. For training set a little augmentation will be enough for preventing overfitting.","2273b089":"Even though dataset is small (only 220 mbs) I loaded it to memory as it is easy to work now, just be careful not to load memory too much.","adc6d1a8":"# Imports","5ab1b8ff":"It is obvious that we have 3 classes: paper, rock and scissors.","15dee075":"# Reading data","9fc30a9e":"I am using a simple Sequential model consisting of Convolution and Dense layers. For preventing overfitting I am using Dropout between Dense layers"}}