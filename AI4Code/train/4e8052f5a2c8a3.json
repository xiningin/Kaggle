{"cell_type":{"3e4c6e66":"code","57f21356":"code","2139cca1":"code","3c542725":"code","19eeeb25":"code","ef5e06f0":"code","1b9c6a6a":"code","9c94e8f8":"code","e3e131f1":"code","30a480b0":"code","afe1cb99":"code","cd18af5f":"code","436058b7":"code","564fbe21":"code","699e63fb":"code","d3fc549a":"code","40d32d28":"code","0361045a":"code","0dd72765":"code","d0a3e9de":"code","c078fbf8":"code","2cd6a5b4":"code","4643497f":"code","3d0a07aa":"code","4527bf7b":"code","7b6c17b8":"code","4aa27bd7":"code","7575d32a":"code","371051b0":"code","33f0119a":"code","4a9bfdc9":"code","b00c915e":"code","98af76d0":"code","e781472f":"code","39ecffad":"code","6fd6bdd9":"code","9f15c47d":"code","f02721fb":"code","fcba39fc":"markdown","ff36fec7":"markdown","f756aa55":"markdown","a2517d63":"markdown","3605f8c4":"markdown","014daca0":"markdown","7bd68899":"markdown","6b2d9ad0":"markdown","d33eddf5":"markdown","984e3485":"markdown","b1a5c227":"markdown","2e440047":"markdown","be9c5c14":"markdown","664817ee":"markdown","406924aa":"markdown","7adabf75":"markdown","81bbec43":"markdown","bc9e2100":"markdown","0fdbe564":"markdown","d82381e3":"markdown","6dba9cc7":"markdown","aa69f7a2":"markdown","b12cf415":"markdown","0936fb43":"markdown","407e02fe":"markdown","693962cc":"markdown","a66f6409":"markdown","4346f18e":"markdown","dc966fa0":"markdown","1ae874d1":"markdown","7216914d":"markdown","deb99bff":"markdown","8f039698":"markdown","2749f889":"markdown","bc9a3378":"markdown","9ba1611c":"markdown","07ec6b3c":"markdown","42a80f0a":"markdown","0b565b9f":"markdown","387011c6":"markdown"},"source":{"3e4c6e66":"import numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error","57f21356":"# distribution according to Benford's law\nbenford = np.array([30.1, 17.6,12.5,9.7,7.9,6.7,5.8,5.1,4.6])","2139cca1":"def Benford(w):\n    \n    # if a list is given, turn it into an numpy array\n    if type(w) is not np.ndarray: w = w.to_numpy()\n        \n    # extract the first non zero digit\n    for j in range(len(w)):\n        # if the number is zero, skip\n        if w[j]==0: continue\n        u = str(w[j])\n        # until there are zeros, skip to the next digit\n        while (u[0]=='0') or (u[0]=='.'): # skip also '.' in case the number is 0.04274 for example\n            u = u[1:]\n        \n        w[j]=int(u[0])\n    \n    # now w is an array with integers from 0. to 9.\n    \n    # count the frequency of each digit as percentage \n    count = pd.Series(w).value_counts(normalize=True).sort_index()\n    \n    # it might be that a digit is missing. put zero instead.\n    perc = pd.DataFrame(count)\n    perc.index= [int(x) for x in perc.index]\n    zeros = pd.DataFrame({'zero': np.zeros(10)},np.arange(0,10))\n    join_df = perc.join(zeros,how='outer').fillna(0)\n    \n    perc = list((join_df[0]+join_df['zero'])*100)[1:10] # EXCLUDE ZERO!\n      \n    # calculate root mean squared error\n    RMSE = mean_squared_error(benford,perc, squared= False)\/(benford.max()-benford.min())\n \n    return perc, RMSE","3c542725":"def Benford_print(w, name= 'Input'):\n    perc, RMSE = Benford(w)\n    \n    fig = plt.figure()\n    ax = fig.gca()\n    ax.set_xticks(np.arange(1,10))\n    plt.scatter(np.arange(1,10), perc, label = 'Naive')\n    plt.scatter(np.arange(1,10), benford, color = 'r', label = 'Benford')\n    plt.title('Comparing '+name+' and Benford distributions')\n    plt.ylabel('Percentage')\n    plt.xlabel('Digit')\n    ax.legend([name,'Benford'])\n    ax.grid(linestyle='--', linewidth=0.5)\n    plt.show()\n    \n    print('RMSE: ', RMSE,'\\n\\n')","19eeeb25":"plt.bar(np.arange(1,10), benford, tick_label = np.arange(1,10))\nplt.title('Benford distribution')\nplt.ylabel('Frequency (%)')\nplt.xlabel('Digit')\nplt.show()","ef5e06f0":"w = np.random.rand(5674)\nBenford_print(w, 'Random')","1b9c6a6a":"for k in range(100):\n    length = np.random.randint(1,10000)\n    perc, RMSE = Benford(np.random.rand(length))\n    if k ==0: \n        RMSE_ = RMSE\n        perc_ = np.array([perc])\n    else:\n        RMSE_ = np.append(RMSE_,RMSE)\n        perc_ = np.append(perc_,np.array([perc]), axis =0)\n    \nprint('Average RMSE:', RMSE_.mean())\nprint('Average percentages:', perc_.mean(axis=0))","9c94e8f8":"from scipy.stats import expon\ndata_expon = expon.rvs(scale=1,loc=0,size=1000)","e3e131f1":"Benford_print(data_expon, 'Exponential')","30a480b0":"# again, let's find the average RMSE\n\nfor k in range(1000):\n    length = np.random.randint(1,1000)\n    perc, RMSE = Benford(expon.rvs(scale=1,loc=0,size=length))\n    if k ==0: \n        RMSE_ex = RMSE\n        perc_ex = np.array([perc])\n    else:\n        RMSE_ex = np.append(RMSE_ex,RMSE)\n        perc_ex = np.append(perc_ex,np.array([perc]), axis =0)\n    \nprint('Average RMSE:', RMSE_ex.mean())\nprint('Average percentages:', perc_ex.mean(axis=0))","afe1cb99":"df = pd.read_csv('\/kaggle\/input\/banksim1\/bs140513_032310.csv')\n\namount = df['amount']\nBenford_print(amount, 'Fraud')","cd18af5f":"df_1 = pd.read_csv('\/kaggle\/input\/musicnet-dataset\/musicnet_metadata.csv')\n\nsec = df_1['seconds']\nBenford_print(sec)","436058b7":"df_body = pd.read_csv('\/kaggle\/input\/body-measurements-dataset\/Body Measurements _ original_CSV.csv')\n\ndf_body.info()","564fbe21":"print('Average RMSE of each column: \\n')\nfor col in df_body.columns[1:]:\n    print(col,': ', Benford(df_body[col])[1], '\\n')","699e63fb":"Benford_print(df_body['Age'], 'Age')","d3fc549a":"dfYT = pd.read_csv('\/kaggle\/input\/youtube-new\/USvideos.csv')\n\nattr = ['views', 'likes', \n        'dislikes', 'comment_count']\n\ndfYT_copy= dfYT.copy()\nfor col in attr:\n    Benford_print(dfYT_copy[col],col)","40d32d28":"dfYT.head(2)","0361045a":"dfYT.drop(['description'], axis=1, inplace= True)","0dd72765":"print('Length of the DataFrame: ',len(dfYT))\nprint('Unique video_id: ',len(dfYT.video_id.unique()))","d0a3e9de":"dfYT[dfYT.duplicated()==True]['trending_date'].unique()","c078fbf8":"print(len(dfYT[dfYT['video_id']=='j4KvrAUjn6c']))\nprint(len(dfYT[dfYT['video_id']=='j4KvrAUjn6c']['trending_date'].unique()))","2cd6a5b4":"duplicates = dfYT[dfYT.duplicated()==True].index\n\ndfYT.drop(labels = duplicates, axis = 0 , inplace = True)\n\nduplic_df = dfYT[dfYT.duplicated(subset = ['video_id'], keep = False)==True]\n\nfor i in duplic_df['video_id'].unique():\n    if len(duplic_df[duplic_df['video_id']==i]['trending_date'].unique())!= len(duplic_df[duplic_df['video_id']==i]):\n        print(i)","4643497f":"dfYT[dfYT['video_id']== 'YI3tsmFsrOg']   # the indices 34763 and 34912 are the ones\ndfYT.loc[34763]==dfYT.loc[34912]         # the only difference is in the number of likes: 197843 vs 197847\n# it doesn't change much!\n\ndfYT[dfYT['video_id']== 'vY_5EvoL1Is']   # the indices 34771 and 34920 are the ones\ndfYT.loc[34771]==dfYT.loc[34920]         # same here, only likes and dislikes are different, but not too far! \n \ndfYT.drop(labels = [34763, 34771], axis = 0 , inplace = True)","3d0a07aa":"dfYT.columns","4527bf7b":"cols = ['video_id', 'trending_date', 'title', 'channel_title', 'category_id', 'publish_time']\n\nfor c in cols:\n    print (c, ' has ', len(dfYT[c].unique()) , ' unique values')","7b6c17b8":"dfYT['publish_time'] = pd.to_datetime(dfYT['publish_time'])\ndfYT['publish_day'] = dfYT['publish_time']\nfor i in dfYT.index:\n    dfYT['publish_day'][i] = dfYT['publish_time'][i].replace(hour=0, minute=0, second = 0)\n\nprint('\\n Oldest video: ', min(dfYT['publish_day']).date())\nprint('Most recent video: ', max(dfYT['publish_day']).date())","4aa27bd7":"# dfYT['trending_date'] = pd.to_datetime(dfYT['trending_date'])\n# it looks the dates have a strange format... convert it to timedate accordingly\ndfYT['trending_date'] = pd.to_datetime(dfYT['trending_date'], format = '%y.%d.%m')\n# don't consider timezone of the published time, as I don't know the one of the trending date\ndfYT['publish_time'] = dfYT['publish_time'].apply(lambda x: x.replace(tzinfo=None))\n# calculate the delta\ndiff = dfYT['trending_date']-dfYT['publish_time']\n\nprint ('Minimum time before being trendy: ', diff.min().days, ' days')\nprint ('Maximum time before being trendy: ', diff.max().days, ' days \\n')\nprint ('Average time: ', diff.mean().days, ' days')\nprint ('Median time: ', diff.median().days, ' days')\n\ndfYT['trending_time'] = diff","7575d32a":"print('There are on average ', dfYT['trending_date'].value_counts().mean(), ' trending videos on YouTube per day')\nprint('The median is ', dfYT['trending_date'].value_counts().median())\nprint('Standard deviation: ', dfYT['trending_date'].value_counts().std())","371051b0":"dfYT_18 = dfYT[dfYT['publish_day']>pd.to_datetime('2018-01-01', utc = True)].copy()\nlen(dfYT_18)\/len(dfYT)","33f0119a":"dfYT_copy= dfYT.copy()\nfor col in attr:\n    Benford_print(dfYT_copy[col],col)","4a9bfdc9":"ind_new = list(dfYT.video_id.unique())   \ncols_new_num = {'total_'+att: list(dfYT.groupby('video_id')[att].sum()) for att in ['views', 'likes', 'dislikes', 'comment_count']}\n\ntot_dfYT = pd.DataFrame(data = cols_new_num, columns = ['total_views', 'total_likes', 'total_dislikes', 'total_comment_count'] , index = ind_new)\ntot_dfYT.head()","b00c915e":"dfYT_copy= tot_dfYT.copy()\nfor col in tot_dfYT.columns:\n    Benford_print(dfYT_copy[col],col)","98af76d0":"dfYT.channel_title.value_counts()","e781472f":"video_id_channels = dfYT[['video_id', 'channel_title']]\nd = video_id_channels[video_id_channels.duplicated()==True].index\nvideo_id_channels= video_id_channels.drop(labels = d, axis = 0)\n\n# there are too many videos.. there might be typos in the channel_title column.\n# I'll be brutal and erase all duplicates:\nd_more = video_id_channels[video_id_channels['video_id'].duplicated()==True]\nvideo_id_channels = video_id_channels.drop(labels = d_more.index, axis = 0 )\n\nvideo_id_channels = video_id_channels.set_index('video_id')\ntot_dfYT ['channel_title'] = video_id_channels['channel_title']","39ecffad":"top_5 = tot_dfYT.channel_title.value_counts()[:5].index\n\n# create a dictionary with {'channel title': mean squared error, ....}\nRMSE_dict = {}\n\nfor channel in top_5:\n    df_ch = tot_dfYT[tot_dfYT['channel_title']==channel]\n    c, rmse = Benford(df_ch['total_views'])\n    RMSE_dict[channel] = rmse\n    print(channel, rmse)\n    \nc, rmse = Benford(tot_dfYT['total_views'])\nprint('\\nTotal RMSE: ', rmse)","6fd6bdd9":"import json\n\ncategory_id_filepath = '\/kaggle\/input\/youtube-new\/US_category_id.json'\ncategory_id_json = open(category_id_filepath)\n\nraw_category_data = json.load(category_id_json)\n\ncategory_data = {}\n\nfor item in raw_category_data['items']:\n    category_data[int(item['id'])] = item['snippet']['title']\n\n# Mapping category data into the dataframe\ndfYT['category'] = dfYT['category_id'].apply(lambda x: category_data.get(x, 'N\/A'))","9f15c47d":"sns.histplot(data = dfYT, y = 'category')\nplt.title('Number of videos per category (per day)')\nplt.show()","f02721fb":"# create a dictionary with {'category': mean squared error, ....}\nRMSE_dict_cat = {}\n\nfor cat in dfYT.category.unique():\n    df_cat = dfYT[dfYT['category']==cat]\n    c, rmse = Benford(df_cat['views'])\n    RMSE_dict_cat[cat] = rmse\n    \nval_count = dfYT.category.value_counts().values\n\nrmse_cat = dict(sorted(RMSE_dict_cat.items(), key=lambda item: item[1]))\nrmse_cat_col = [rmse_cat[i] for i in dfYT.category.value_counts().index]\n\ncategory_df =pd.DataFrame({'rmse_cat': rmse_cat_col, 'count': val_count}, index = dfYT.category.value_counts().index)\ncategory_df","fcba39fc":"In the previous plot we can see the distributions of the first digits, according to Benford's law. Each bar represents a digit and the height of the bar is the percentage of numbers that start with that digit.","ff36fec7":"YouTube trending videos are on average 200 per day and do not deviate much from the mean. It must be some YouTube rule.","f756aa55":"The following function takes a vector w as input and returns \n- *perc*: an array of length 9 with the percentage of the appeareance of the first digits in w \n- *RMSE*: the root mean squared error, measuring how far *perc* is from Benford's distribution.","a2517d63":"Only some rows in the 'description' column have NaN values. I don't find interesting this column anyway, I'll drop it.","3605f8c4":"First, I'll create a function that plots the distribution of the digits of a given array compared to the distribution of Benford's law:","014daca0":"The next function plots the percentages of a given vector compared to Benford's distribution.","7bd68899":"Let's generate a number of random vectors and see the average RMSE and percentages. We expect that the digits of a uniform distribution occur about 11.11% of the time.","6b2d9ad0":"I confirm the hypothesis that **we're looking at the statistics PER TRENDING DAY**. There must have been a mistake for two videos, but we fixed it.\n\nAnyway, the answer to the question 'can I use video_id as index?' is **no**.","d33eddf5":"## Back to Benfordness\n\nNow that we've cleaned the dataframe, let's see if the Benfordness has changed.","984e3485":"Most of the videos (73% of the total) are published in 2018... it must have been the golden age of the website.","b1a5c227":"## Benfordness per channel:","2e440047":"The channel themselves don't have a very low RMSE (but yet not too high: less then 0.15 on average). The point here, which is the point of Benford's law, is that it holds for **large collections of data from the real world**. When we look at a specific object (in this case one channel) we loose Benfordness. \n\nTo conclude our analysis, let us study the categories.","be9c5c14":"The most common category is \"Entertainment\", followed by \"Music\". Unfortunately in last position we find \"Nonprofits & Activism\"... Sadly reflects the world we're living in...","664817ee":"We can actually see that the lower digits appear more often than the others, but the RMSE tell us that the distributions don't agree.\n\n- Consider now the [MusicNet dataset](http:\/\/www.kaggle.com\/imsparsh\/musicnet-dataset?select=musicnet_metadata.csv), a collection of 330 freely-licensed classical music recordings\nand its column measuring the duration of the recording in seconds.","406924aa":"To know which category corresponds to a given number, let's extract the category names using JSON.","7adabf75":"#### There is a column named \"video_id\": can I use it as index of the dataframe? ","81bbec43":"WOW!! I don't need to explain the results... it fits perfectly :) ","bc9e2100":"As we have seen before, this attributes follow Benford's law.\n\nNow that we know that these numbers are views\/likes\/etc per day, **let's see the behaviour of the total numbers per video**. To do so, we create a new dataframe with the total numbers of views, likes, dislikes and comments adding up the ones per day.","0fdbe564":"Mean value and median are quite different.. **Most of the videos are viral in very short time!**","d82381e3":"No surprises here: a random array behaves as expected.\n#### The shape of the Benford distribution reminds me the exponential one. Let's compare them.","6dba9cc7":"Indeed, we can see that the average RMSE of 1000 exponential sample taken at random and Benford's is quite low. But the two distributions do not coincide.","aa69f7a2":"#### More preprocessing:","b12cf415":"I'm curious about how old the videos are and how long they remained in the trends.\nAlso, how many days after the publication does a video go in the trends?","0936fb43":"There are 2207 different channels. I'd like to check the **Benfordness of the 5 channels that have an higher number of published videos**. To do so, I need the tot_dfYT dataframe.","407e02fe":"- Next, I'd like to look at some measures. For example, take the \n[Body Measurements Dataset](http:\/\/www.kaggle.com\/saurabhshahane\/body-measurements-dataset) from Kaggle.","693962cc":"- Let's turn to other statistics, the [Trending YouTube Video Statistics](http:\/\/www.kaggle.com\/datasnaek\/youtube-new?select=USvideos.csv).\n Here we can consider different columns, such as 'views', 'likes', 'dislikes', 'comment_count'","a66f6409":"# Test on real data\n\n#### Now it's time to take some real dataset and see whether Benford's law holds\n\n- First, we take the [Fraud Detection on Bank Payments dataset](http:\/\/www.kaggle.com\/turkayavci\/fraud-detection-on-bank-payments\/data) from Kaggle\nand consider the column with the amount of the purchase.","4346f18e":"Only 'Age' seems following Benford's distribution:\n","dc966fa0":"There are **6351 unique video_id** over a total of 40949 videos. Why?\nLet's have a closer look at the duplicates.","1ae874d1":"Here unfortunately the result is even worse... ","7216914d":"#### Let's try it on a random array","deb99bff":"Even the sums follow the law!! :)","8f039698":"There are only two videos that share the same trending_date. Let's see why.","2749f889":"# Benfordness\n\n[Benford's law](http:\/\/en.wikipedia.org\/wiki\/Benford%27s_law) is an observation about the frequency distribution of leading digits in many real-life sets of numerical data. \n\nThe law states that in many naturally occurring collections of numbers, **the leading digit is likely to be small**. In sets that obey the law, the number 1 appears as the leading significant digit about 30% of the time, while 9 appears as the leading significant digit less than 5%. If the digits were distributed uniformly, they would each occur about 11.1% of the time. Benford's law also makes predictions about the distribution of second digits, third digits, digit combinations, and so on.\n\nIn this notebook we upload different datasets to see if the rule applies or not. The first striking result comes with  Youtube statistics... continue reading to find out more! ","bc9a3378":"# Youtube statistics\n\n#### After the surprising result on the \"Benfordness\" of the Youtube statistics, let's study in more detail the YouTube dataframe.","9ba1611c":"All the duplicates have tranding_date equal to 18.15.05. There must have been a mistake on this day (yes, the day format is confusing.. we'll have a look at this later) \n\nOn the other hand, have all the videos with same video_id a different trending_date? ","07ec6b3c":"For example, there are 30 rows with video_id = j4KvrAUjn6c. Except the 18.15.05 which has a duplicate, all the other trending days are different. \n\nAt this point I assume \nthe rows represent the statistics (views\/likes\/etc) PER DAY.\nLet's drop the duplicates and confirm that all the unique video_id's have different trending_date.","42a80f0a":"According to the fact that with a larger amount of data it's easier to find Benfordness, note that the two most attractive categories have the lowest rmse, and thus follow better Benford's law.","0b565b9f":"# Conclusions\n\nBenford's law looked so crazy at a first glance that I could not believe it... \nMy mathematical sense had to investigate more to confirm it holds true!\n\nThe main point is that if Benford's law is true, then the significant digits of real-world data are statistically dependent. How could it be possible??\nUnfortunately, I've no answer for the 'how' question, but\nI had really fun writing this notebook. The main conclusion of this analysis are the following.\n\n1.  Now I do believe in Benfordness and I hope you do, too.\n\n2. More data, the better: the law tends to be more accurate when we have more values and when they are distributed across multiple orders or magnitude. That's why when we focussed on videos of a given channel or of a certain category the error increased.\n\n3. Scale-invariance: Benforness do not depend on the unit of the measurement. Indeed, the choice of taking videos per day or the total number of videos don't affect Benfordness.\n\nI hope this notebook stimulated your curiosity and that you'll look for Benfordness in the data you're analysing ;) \nPlease leave a comment with your results!","387011c6":"## Benfordness per category"}}