{"cell_type":{"8808d073":"code","22500791":"code","d3654626":"code","d64757a2":"code","4fc6aff3":"code","f3cb19f2":"code","56790705":"code","4966cee3":"code","04e86fc5":"code","5074947d":"code","8e0c73e2":"code","e78ec620":"code","53c91b70":"code","c42a6634":"code","b6f14713":"code","b9b2eb5c":"code","b4d9c69e":"code","54247626":"code","586cc02e":"code","303e8387":"code","4966dd0a":"code","72c9e1d6":"code","248b21ef":"code","00d128f3":"code","2933730d":"code","d9988649":"code","e1960f89":"markdown","04c7742d":"markdown","217e08b2":"markdown","0f85f4fa":"markdown","cade595b":"markdown","dc178dae":"markdown","605976db":"markdown","e1a2ee70":"markdown","40a285c8":"markdown","4c7275cc":"markdown"},"source":{"8808d073":"import tensorflow as tf\nprint(tf.version)","22500791":"# Library\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow.keras.models as models\nimport tensorflow.keras.layers as layers\nimport IPython\nimport sklearn\nimport seaborn as sns\nfrom sklearn.utils import shuffle\n\n%load_ext tensorboard","d3654626":"#datasets chainsaw and crackling fire\nPATH_ESC = \"..\/input\/environmental-sound-classification-50\/audio\/audio\/16000\/\"\nCSV_ESC = \"..\/input\/environmental-sound-classification-50\/esc50.csv\"\n\n#datasets gun shot\nCSV_URBAN = \"..\/input\/urbansound8k\/UrbanSound8K.csv\" \nPATH_URBAN = \"..\/input\/urbansound8k\/fold\" ","d64757a2":"#read csv\ndf_chainsaw = pd.read_csv(CSV_ESC)\ndf_gunshot = pd.read_csv(CSV_URBAN)\n\nchainsaw = df_chainsaw.loc[df_chainsaw['category'].isin(['chainsaw', 'crackling_fire'])]\ngunshot = df_gunshot[df_gunshot['class'] == 'gun_shot']\nchainsaw = chainsaw.drop(['esc10', 'src_file', 'take'], axis=1)\ngunshot = gunshot.drop(['fsID', 'start', 'end', 'classID', 'salience'], axis=1)\n\ngunshot = gunshot.rename(columns={'class': 'category', 'slice_file_name': 'filename'})","4fc6aff3":"#combined chainsaw and gunshot datasets\ncombined_datasets = pd.concat([chainsaw, gunshot])\n\nclasses = combined_datasets['category'].unique()\nclass_dict = {i:x for x,i in enumerate(classes)}\ncombined_datasets['target'] = combined_datasets['category'].map(class_dict)\n\nsample_df = combined_datasets.drop_duplicates(subset=['target'])\nsample_df","f3cb19f2":"# Class Conf will save the settings we are going to use in this notebook\nclass conf:\n    sr = 16000\n    duration = 3\n    hop_length = 340*duration\n    fmin = 20\n    fmax = sr \/\/ 2\n    n_mels = 128\n    n_fft = n_mels * 20\n    samples = sr * duration\n    epochs = 30\n\ndef read_audio(conf, pathname, trim_long_data):\n    y, sr = librosa.load(pathname, sr=conf.sr)\n    # trim silence\n    if 0 < len(y): # workaround: 0 length causes error\n        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n    # make it unified length to conf.samples\n    if len(y) > conf.samples: # long enough\n        if trim_long_data:\n            y = y[0:0+conf.samples]\n    else: # pad blank\n        padding = conf.samples - len(y)    # add padding at both ends\n        offset = padding \/\/ 2\n        y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')\n    return y\n\ndef audio_to_melspectrogram(conf, audio):\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=conf.sr,\n                                                 n_mels=conf.n_mels,\n                                                 hop_length=conf.hop_length,\n                                                 n_fft=conf.n_fft,\n                                                 fmin=conf.fmin,\n                                                 fmax=conf.fmax)\n    spectrogram = librosa.power_to_db(spectrogram)\n    return spectrogram\n\ndef show_melspectrogram(conf, mels, title='Log-frequency power spectrogram'):\n    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n                             sr=conf.sr, hop_length=conf.hop_length,\n                            fmin=conf.fmin, fmax=conf.fmax)\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(title)\n    plt.show()","56790705":"# Visualization of Soundwave\nfig, ax = plt.subplots(3, figsize = (8, 6))\nfig.suptitle('Sound Waves', fontsize=16)\ncolor = ['#A300F9', '#4300FF', '#009DFF']\ni=0\nfor index,row in sample_df.iterrows(): \n    if row['category'] == \"gun_shot\":\n        PATH = PATH_URBAN + str(row[1]) + '\/' + row[0]\n    else:\n        PATH = PATH_ESC + row[0]\n    signal , rate = librosa.load(PATH, sr=conf.sr)\n    librosa.display.waveplot(y = signal, sr = rate, color = color[i], ax=ax[i])\n    ax[i].set_ylabel(classes[row[2]], fontsize=13)\n    i +=1","4966cee3":"# Visualization of Soundwave\nfig, ax = plt.subplots(3, figsize = (8, 6))\nfig.suptitle('Mel Spectogram', fontsize=16)\ncolor = ['#A300F9', '#4300FF', '#009DFF']\ni=0\nfor index,row in sample_df.iterrows(): \n    if row['category'] == \"gun_shot\":\n        PATH = PATH_URBAN + str(row[1]) + '\/' + row[0]\n    else:\n        PATH = PATH_ESC + row[0]\n    signal , rate = librosa.load(PATH, sr=conf.sr)\n    mel_spec = audio_to_melspectrogram(conf, signal)\n    librosa.display.specshow(mel_spec, sr = conf.sr, hop_length = conf.hop_length, x_axis = 'time', \n                         fmin=conf.fmin, fmax=conf.fmax, y_axis = 'mel', ax=ax[i])\n    ax[i].set_ylabel(classes[row[2]], fontsize=13)\n    i +=1","04e86fc5":"# Visualization of MFCC Plot\nfig, ax = plt.subplots(3, figsize = (8, 6))\nfig.suptitle('MFCC', fontsize=16)\ncolor = ['#A300F9', '#4300FF', '#009DFF']\ni=0\nfor index,row in sample_df.iterrows(): \n    if row['category'] == \"gun_shot\":\n        PATH = PATH_URBAN + str(row[1]) + '\/' + row[0]\n    else:\n        PATH = PATH_ESC + row[0]\n    signal , rate = librosa.load(PATH, sr=conf.sr)\n    mfcc = librosa.feature.mfcc(signal , rate , n_mfcc=13, dct_type=3)\n    librosa.display.specshow(mfcc, sr = conf.sr, hop_length = conf.hop_length, x_axis = 'time', \n                          y_axis = 'mel', ax=ax[i])\n    ax[i].set_ylabel(classes[row[2]], fontsize=13)\n    i +=1","5074947d":"for index,row in sample_df.iterrows(): \n    if row['category'] == \"gun_shot\":\n        PATH = PATH_URBAN + str(row[1]) + '\/' + row[0]\n    else:\n        PATH = PATH_ESC + row[0]\n    signal , rate = librosa.load(PATH, sr=conf.sr)\n    print(len(signal)\/rate)\n    \ncombined_datasets.category.value_counts()","8e0c73e2":"!git clone https:\/\/github.com\/nicorenaldo\/audioset-processing.git\n%cd audioset-processing\/\n!pip install -r requirements.txt","e78ec620":"!python3 process.py download -c \"chainsaw\" -s STRICT --limit 20\n!python3 process.py download -c \"fire\" -s STRICT --limit 20","53c91b70":"%cd ..\/\nchainsaw_dir = \".\/audioset-processing\/output\/chainsaw\/\"\nfire_dir = \".\/audioset-processing\/output\/fire\/\"\nchainsaw_file = os.listdir(chainsaw_dir)\nfire_file = os.listdir(fire_dir)\n\naudioset_chainsaw = pd.DataFrame({\"filename\":chainsaw_file, \"target\":0, \"category\":\"chainsaw\"})\naudioset_fire = pd.DataFrame({\"filename\":fire_file, \"target\":1, \"category\":\"crackling_fire\"})\naudioset = pd.concat([audioset_chainsaw, audioset_fire])\naudioset.tail()","c42a6634":"X = []\ny = []\n\n# Example Output of data\n# Pandas(Index=24, filename='1-116765-A-41.wav', fold=1, target=0, category='chainsaw')\nfor data in combined_datasets.itertuples():\n    if data[4]==\"gun_shot\":\n        PATH = PATH_URBAN + str(data[2]) + '\/' + data[1]\n        signal , rate = librosa.load(PATH, sr=conf.sr)\n        if(len(signal)\/16000 <= 2.0):\n            blank = np.zeros((rate*2)-len(signal))\n            sig_ = np.append(signal,blank)\n        else:\n            sig_ = signal[0 : int(rate*2)]\n        mel_spec = audio_to_melspectrogram(conf, sig_)\n        X.append(mel_spec)\n        y.append(data[3])\n    else:\n        PATH1 = PATH_ESC + data[1]\n        signal , rate = librosa.load(PATH1, sr=conf.sr)\n        #Creating four 2 second clip from each audio file, to create more samples\n        for i in range(4):\n            sig_ = signal[i : int(i+rate*2)]\n            mel_spec = audio_to_melspectrogram(conf, sig_)\n            X.append(mel_spec)\n            y.append(data[3])\n\n# Example Output\n# Pandas(Index=0, filename='-DVM0BK_h5A_30.wav', target=0, category='chainsaw')\nfor data in audioset.itertuples():\n    if data[3] == \"chainsaw\":\n        PATH = chainsaw_dir + str(data[1])\n    else:\n        PATH = fire_dir + str(data[1])\n    signal , rate = librosa.load(PATH, sr=conf.sr)\n    for i in range(9):\n        sig_ = signal[i : int(i+rate*2)]\n        mel_spec = audio_to_melspectrogram(conf, sig_)\n        X.append(mel_spec)\n        y.append(data[2])    \n\n# convert list to numpy array\nX = np.array(X)\ny = np.array(y)\n\n#one-hot encoding the target\ny_hot = tf.keras.utils.to_categorical(y , num_classes=len(classes))\n\n# our tensorflow model takes input as (no_of_sample , height , width , channel).\n# here X has dimension (no_of_sample , height , width).\n# So, the below code will reshape it to (no_of_sample , height , width , 1).\nX_reshaped = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n                                  \nx_train , x_val , y_train , y_val = train_test_split(X_reshaped , y_hot ,test_size=0.2, random_state=42)","b6f14713":"INPUTSHAPE = (128, 32, 1)\ndef create_model():\n    created_model =  models.Sequential([\n        layers.Conv2D(64 , (3,3),activation = 'relu',padding='same', input_shape = INPUTSHAPE),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, (3,3), activation='relu',padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2,2), strides=(2,2)),\n        layers.Dropout(0.2),\n\n        layers.Conv2D(128, (3,3), activation='relu',padding='same'),                      \n        layers.BatchNormalization(),\n        layers.Conv2D(128, (3,3), activation='relu',padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2,2), strides=(2,2)),\n        layers.Dropout(0.2),\n\n        layers.Conv2D(256, (3,3), activation='relu',padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(256, (3,3), activation='relu',padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2,2), strides=(2,2)),    \n        layers.Dropout(0.2),\n\n        layers.GlobalAveragePooling2D(),\n\n        layers.Dense(256 , activation = 'relu'),\n        layers.Dense(256 , activation = 'relu'),\n        layers.Dense(len(classes) , activation = 'softmax')\n    ])\n\n    created_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['acc'])\n    return created_model","b9b2eb5c":"# Our model summary\nmodel = create_model()\nprint(model.summary())","b4d9c69e":"%mkdir \"cpkt\"\n%mkdir \"logs\"\nLOGDIR = \"logs\"\nCPKT = \"cpkt\/\"\n\n#this callback is used to prevent overfitting.\ncallback_1 = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto',\n    baseline=None, restore_best_weights=False\n)\n\n#this checkpoint saves the best weights of model at every epoch\ncallback_2 = tf.keras.callbacks.ModelCheckpoint(\n    CPKT, monitor='val_loss', verbose=0, save_best_only=True,\n    save_weights_only=True, mode='auto', save_freq='epoch', options=None\n)\n\n#this is for tensorboard\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOGDIR)","54247626":"model = create_model()\nhistory = model.fit(x_train,y_train ,\n        validation_data=(x_val,y_val),\n        epochs=conf.epochs,\n        callbacks = [callback_1], verbose=1)\n\neval_score = model.evaluate(x_val, y_val)\nprint(\"Val Score: \",eval_score )","586cc02e":"color = ['black', 'red', 'green', 'blue', 'purple']\nplt.figure(figsize=(15,5))\nplt.title('Accuracies vs Epochs')\n\nlabel_name_train = 'Train Accuracy'\nlabel_name_val = 'Val Accuracy'\nplt.plot(history.history['acc'], label=label_name_train)\nplt.plot(history.history['val_acc'], label=label_name_train)\n\nplt.legend()\nplt.show()","303e8387":"color = ['black', 'red', 'green', 'blue', 'purple']\nplt.figure(figsize=(15,5))\nplt.title('Accuracies vs Epochs')\n\nlabel_name_train = 'Train Accuracy'\nlabel_name_val = 'Val Accuracy'\nplt.plot(history.history['loss'], label=label_name_train)\nplt.plot(history.history['val_loss'], label=label_name_train)\n\nplt.legend()\nplt.show()","4966dd0a":"# Creating a confusion matrix to see the error occured\ny_pred = model.predict(x_val)\nconfusion_matrix = sklearn.metrics.confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))\n\nax = plt.subplot()\nsns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\", ax=ax);\n\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(classes, rotation='vertical'); ax.yaxis.set_ticklabels(classes, rotation='horizontal');","72c9e1d6":"# The audio we are using is this one\nimport IPython.display as ipd\nsig , sr = librosa.load('..\/input\/chainsaw-testing\/chainsaw-01.wav', sr=conf.sr)\nipd.display(ipd.Audio(sig, rate=sr))\nlibrosa.display.waveplot(y = sig, sr = sr)","248b21ef":"def split_audio(audio_data, w, h, threshold_level, tolerence=10):\n    split_map = []\n    start = 0\n    data = np.abs(audio_data)\n    threshold = threshold_level*np.mean(data[:25000])\n    inside_sound = False\n    near = 0\n    for i in range(0,len(data)-w, h):\n        win_mean = np.mean(data[i:i+w])\n        if(win_mean>threshold and not(inside_sound)):\n            inside_sound = True\n            start = i\n        if(win_mean<=threshold and inside_sound and near>tolerence):\n            inside_sound = False\n            near = 0\n            split_map.append([start, i])\n        if(inside_sound and win_mean<=threshold):\n            near += 1\n    return split_map","00d128f3":"# To identify the sounds in the audio, we are going to cut the soundwave into several parts\n# The clip will be clipped to it's highlight (noisiest) with certain interval\n\nsound_clips = split_audio(sig, 10000, 2500, 15, 10)\nduration = len(sig)\ni = 1\n\nfor intvl in sound_clips:\n    clip, index = librosa.effects.trim(sig[intvl[0]:intvl[1]],       \n                                       top_db=20, frame_length=512, hop_length=64)\n    mel_spec = audio_to_melspectrogram(conf, clip)\n    testing = np.array(mel_spec)\n    testing = testing.reshape(1, testing.shape[0], testing.shape[1], 1)\n    pred = model.predict(testing)\n    \n    blank = np.zeros(intvl[0]-0)\n    blank2 = np.zeros(duration-intvl[1])\n    temp = np.append(blank,clip)\n    temp = np.append(temp,blank2)\n    librosa.display.waveplot(y = temp, sr = sr, )\n    \n    print(\"Clip Number :\", i)\n    print(\"Interval from : \", intvl[0]\/16000, \" to \",intvl[1]\/16000, \"seconds\")\n    i += 1\n    if(pred.max() > 0.8):\n        print(\"Results : \", classes[np.argmax(pred)], \"\\n\")\n    else:\n        print(\"Results : Unknown\")\n        print(\"Confidence Level : \", pred)\n        print(\"Highest Confidence Level : \", classes[np.argmax(pred)], \" of \", np.max(pred)*100, \"%\\n\")\n        ipd.display(ipd.Audio(clip, rate=sr))","2933730d":"# Showing the Mel Spectogram that is passed to the model\n\nfig, ax = plt.subplots(5, figsize = (15, 10))\nfig.suptitle('Mel Spectogram', fontsize=16)\ni=0\nfor intvl in sound_clips:\n    clip, index = librosa.effects.trim(sig[intvl[0]:intvl[1]],       \n                                       top_db=20, frame_length=512, hop_length=64)\n    mel_spec = audio_to_melspectrogram(conf, clip)\n    librosa.display.specshow(mel_spec, sr = conf.sr, hop_length = conf.hop_length, x_axis = 'time', \n                         fmin=conf.fmin, fmax=conf.fmax, y_axis = 'mel', ax=ax[i])\n    i +=1","d9988649":"# Uncomment code below to save the model to folder \"\/kaggle\/working\/export\"\n# The exported model will be on SavedModel Tensorflow format, which is the default for Tensorflow 2.% model\n\nmodel.save(\"jagawana_v2\")","e1960f89":"# <center> Data Distribution <\/center>\n### Distribution Problem\nAlthough the Gun Shot data is plenty, totaling to 374 clips. But the clips itself is only 2~3 seconds long. Meanwhile the chainsaw and fire sounds totaling only to 40 clips each, up to 5 seconds of audio clip.\n\nTo feed our data to our ML model, we need to normalize the data to the same length, this time I am going to only use 2 seconds of audio clip. For the Gun Shot data, we are going to trim the clips from 2.6 seconds to 2 seconds. Meanwhile for Chainsaw and Fire, we are going to use windowing and shifting it by 1 second, so a 5 second clip will resulted in 4 clips of 2 clips (From 0-2s, 1-3s, 2-4s, 3-5s).\n\nUsing this method, we are totaling our data from Gun Shot, Chainsaw, and Fire to 374, 160, 160 number of clips each.\n\nTo add more balanced dataset, I am going to download more data from [Google Audioset](https:\/\/research.google.com\/audioset\/dataset\/chainsaw.html) using scripts from [here](https:\/\/github.com\/nicorenaldo\/audioset-processing), which I modified from [here](https:\/\/github.com\/aoifemcdonagh\/audioset-processing)\n\nThe script will download a 10 second audio from youtube links. Using a 2-seconds windowing, we could get 9 clip from each file. I'm setting goals to use 340 clips from each category, so that means I would need to download 20 audio data from Google Audioset for chainsaw and fire categories.","04c7742d":"# <center> Acquire Training and Testing Data <\/center>","217e08b2":"# <center> Preparing Training Data <\/center>","0f85f4fa":"# Jagawana - Forest Logging Detection\n\n### \n<div class=\"alert alert-block alert-success\"> \ud83d\udccc This notebook is created for a capstone project, we are creating a Forest Logging Detection System to identify Chainsaws and Gunshots from forest ambiance sounds.<\/div>\n\n### Workflow Problem Definition\nForests are huge and the terrain is hard to pass through, on the other side, forest ranger usually comprises of only several people. Often, rangers are patrolling the forest area for 1\u20132 weeks in a month, which means there are many opportunities for illegal loggers to get in and out without any patrol. This gap hole could be prevented by incorporating technology for the ranger and forests.\n\nJagawana is a Wide Sensor Network System deployed in the forests to prevent Ilegal Logging. By using sensors to pick up voices in the forests, we could monitor what happened in the forest in real-time. We deployed a Machine Learning Model to process the sounds taken by the sensor, then the model will identify the sounds into various categories, such as chainsaws, trucks, gunshot, and burning sounds.\n   \n### Workflow Goals\nOur Machine Learning Model main goals is to **Classify Forests Ambience Sounds** taken by the sensors. Our priority is to identify chainsaw sounds and alert users from Android App. Though identifying other sounds is as important too. Being able to identify other sounds may enable us to map out fauna habitats, and for further research data.\n\n### Workflow Stages :\nThis notebook workflow goes through seven stages.\n1. Acquire training and testing data.\n2. Wrangle, prepare, cleanse the data.\n3. Analyze, identify patterns, and explore the data.\n4. Model, predict and solve the problem.\n5. Visualize, report, and present the problem solving steps and final solution.\n6. Exporting Models\n\n### Resources and References\n* We use ESC-50, Urbansound8k, and Google's Audioset dataset. \n* We use VGG-16 Models as our baseline and slightly adjust it for audio classification.\n* Papers papers papers\n\n<div class=\"alert alert-block alert-warning\"> \ud83d\udccc This project is still on development, feel free to comment or contact me through link in profile.<\/div>","cade595b":"# <center> Modifying VGG16 Model <\/center>","dc178dae":"# <center> Saving Model <\/center>","605976db":"# <center> Evaluation <\/center>","e1a2ee70":"#### There are some audio files I inserted to my datasets, let's see how the model identify sounds in the audio.","40a285c8":"# <center> Visualization <\/center>","4c7275cc":"# <center> Training<\/center>"}}