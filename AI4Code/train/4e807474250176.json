{"cell_type":{"d6fe1ed2":"code","8f0293a7":"code","9ba982e5":"code","89ce25a6":"code","95fcadac":"code","dc0b353d":"code","047e7fbf":"code","55e0fa54":"code","8bf489a3":"code","1b12d4eb":"code","948e31ec":"code","23f27140":"code","7d6dc800":"code","803505b6":"code","430d1504":"code","ebb98446":"code","39928204":"code","06a24f0f":"code","bc251f2f":"code","6bbf619e":"code","09fc0fe1":"code","c19a1895":"code","4505a9d2":"code","d0bf1549":"code","419b3269":"code","2fdcd349":"code","6b964302":"code","a3287d67":"code","cbdf5e29":"code","4d04db40":"code","86cba76f":"code","9bc1f2b4":"code","cc9e8ef3":"code","19abddfb":"code","caa177fd":"code","ee68ce1e":"code","18661874":"code","986d587a":"code","8b6fdd71":"code","2cfa73a3":"code","5f74e9bb":"code","e83dfdf5":"code","007aa0e1":"code","9067e2e6":"code","7c732fd6":"code","6aae0c4f":"code","6877cec4":"code","12ab0ebf":"code","7bc16c5e":"code","f34974f2":"markdown"},"source":{"d6fe1ed2":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom glob import glob","8f0293a7":"images_path = '..\/input\/flickr8k-sau\/Flickr_Data\/Images\/'\nimages= glob(images_path+'*.jpg')\nlen(images)","9ba982e5":"images[:5]","89ce25a6":"import matplotlib.pyplot as plt\nfor i in range(5):\n    plt.figure()\n    img = cv2.imread(images[i])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)","95fcadac":"from tensorflow.keras.applications import ResNet50\nincept_model = ResNet50(include_top=True)","dc0b353d":"incept_model.summary()","047e7fbf":"from tensorflow.keras.models import Model\nlast = incept_model.layers[-2].output\nmodele = Model(inputs = incept_model.input, outputs= last)\nmodele.summary()","55e0fa54":"images_features = {}\ncount = 0\nfor i in images:\n    img= cv2.imread(i)\n    img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img= cv2.resize(img, (224,224))\n    \n    img = img.reshape(1, 224, 224, 3)\n    pred = modele.predict(img).reshape(2048, )\n    \n    img_name = i.split('\/')[-1]\n    \n    images_features[img_name] = pred\n    count += 1\n    \n    if count>1499:\n        break\n        \n    elif count % 50 == 0:\n        print(count)\n        ","8bf489a3":"caption_path ='..\/input\/flickr8k-sau\/Flickr_Data\/Flickr_TextData\/Flickr8k.token.txt'\n","1b12d4eb":"captions = open (caption_path, 'rb').read().decode('utf-8').split('\\n')","948e31ec":"captions","23f27140":"captions_dict = {}\nfor i in captions:\n    try:\n        img_name = i.split('\\t')[0][:-2]\n        caption = i.split('\\t')[1]\n        if img_name in images_features:\n            if img_name not in captions_dict:\n                captions_dict[img_name] = [caption]\n            else:\n                captions_dict[img_name].append(caption)\n    except:\n        pass","7d6dc800":"captions_dict","803505b6":"def preprocessed(txt):\n    modified = txt.lower()\n    modified = 'startofseq ' + modified + ' endofseq'\n    return modified","430d1504":"for k,v in captions_dict.items():\n    for vv in v:\n        captions_dict[k][v.index(vv)] = preprocessed(vv)\n    ","ebb98446":"captions_dict","39928204":"count_words = {}\ncount =1\nfor k,vv in captions_dict.items():\n    for v in vv:\n        for word in v.split():\n            if word not in count_words:\n                count_words[word] = count\n                count += 1\n            ","06a24f0f":"len(count_words)","bc251f2f":"for k,vv in captions_dict.items():\n    for v in vv:\n        encoded = []\n        for word in v.split():\n            \n                encoded.append(count_words[word])\n                \n        captions_dict[k][vv.index(v)] = encoded\n                    \n        ","6bbf619e":"captions_dict","09fc0fe1":"from tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","c19a1895":"MAX_LEN = 0\nfor k,vv in captions_dict.items():\n    for v in vv:\n        if len(v) > MAX_LEN:\n            MAX_LEN = len(v)\n            print(v)","4505a9d2":"MAX_LEN\n\n","d0bf1549":"VOCAB_SIZE = len(count_words)\ndef generator(photo, caption):\n    n_samples = 0\n    X= []\n    y_in= []\n    y_out = []\n    \n    for k, vv in caption.items():\n        for v in vv:\n            for i in range (1, len(v)):\n                X.append(photo[k])\n            \n                in_seq = [v[:i]]\n                out_seq = v[i]\n            \n                in_seq = pad_sequences(in_seq, maxlen = MAX_LEN, padding='post', truncating= 'post')[0]\n                out_seq = to_categorical([out_seq], num_classes= VOCAB_SIZE+1)[0]\n            \n                y_in.append(in_seq)\n                y_out.append(out_seq)\n        \n    return X, y_in, y_out\n            \n    \n\n    ","419b3269":"X, y_in, y_out = generator(images_features, captions_dict)\n","2fdcd349":"len(X), len(y_in), len(y_out)","6b964302":"X= np.array(X)\ny_in= np.array(y_in, dtype= 'float64')\ny_out= np.array(y_out, dtype= 'float64')","a3287d67":"X.shape","cbdf5e29":"y_in.shape","4d04db40":"y_out.shape","86cba76f":"len(X), len(y_in), len(y_out)","9bc1f2b4":"X = np.array(X)\ny_in = np.array(y_in, dtype='float64')\ny_out = np.array(y_out, dtype='float64')","cc9e8ef3":"X.shape, y_in.shape, y_out.shape","19abddfb":"X[1510]","caa177fd":"y_in[2]","ee68ce1e":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\nfrom tensorflow.keras.models import Sequential, Model","18661874":"embedding_size = 128\nmax_len = MAX_LEN\nvocab_size = len(count_words)+1\n\nimage_model = Sequential()\n\nimage_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\nimage_model.add(RepeatVector(max_len))\n\nimage_model.summary()\n\nlanguage_model = Sequential()\n\nlanguage_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nlanguage_model.add(LSTM(256, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(embedding_size)))\n\nlanguage_model.summary()\n\nconca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(128, return_sequences=True)(conca)\nx = LSTM(512, return_sequences=False)(x)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\nmodel = Model(inputs=[image_model.input, language_model.input], outputs = out)\n\n# model.load_weights(\"..\/input\/model_weights.h5\")\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\nmodel.summary()","986d587a":"model.fit([X, y_in], y_out, batch_size=512, epochs=200)","8b6fdd71":"inv_dict = {v:k for k, v in count_words.items()}","2cfa73a3":"model.save(\"model.h5\")","5f74e9bb":"model.save_weights('my_model_weights.h5')","e83dfdf5":"np.save('vocab.npy', count_words)","007aa0e1":"def getImage(x):\n    test_img_path = images[x]\n    \n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n    test_img = cv2.resize(test_img, (224,224))\n\n    test_img = np.reshape(test_img, (1,224,224,3))\n    \n    return test_img\n    ","9067e2e6":"test_feature = modele.predict(getImage(2500)).reshape(1,2048)","7c732fd6":"test_feature","6aae0c4f":"test_img_path = images[2500]\ntest_img = cv2.imread(test_img_path)\ntest_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\nplt.imshow(test_img)","6877cec4":"for i in range(5):\n    \n    no = np.random.randint(1500,7000,(1,1))[0,0]\n    test_feature = modele.predict(getImage(no)).reshape(1,2048)\n    \n    test_img_path = images[no]\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n\n    ","12ab0ebf":"count_words\n","7bc16c5e":"for i in range(5):\n    \n    no = np.random.randint(1500,7000,(1,1))[0,0]\n    test_feature = modele.predict(getImage(no)).reshape(1,2048)\n    \n    test_img_path = images[no]\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n    text_inp =['startofseq']\n    count = 0\n    caption = ''\n    while count < 25:\n        count += 1\n\n        encoded = []\n        for i in text_inp:\n            encoded.append(count_words[i])\n\n        encoded = [encoded]\n        encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n\n\n        prediction = np.argmax(model.predict([test_feature, encoded]))\n\n        sampled_word = inv_dict[prediction]\n\n        caption = caption + ' ' + sampled_word\n            \n        if sampled_word == 'endofseq':\n            break\n\n        text_inp.append(sampled_word)\n        \n    plt.figure()\n    plt.imshow(test_img)\n    plt.xlabel(caption)","f34974f2":"from keras.applications import"}}