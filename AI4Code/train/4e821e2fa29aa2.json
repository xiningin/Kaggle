{"cell_type":{"7b9a5131":"code","9d4acc5b":"code","19598c17":"code","d22213dc":"code","72142122":"code","dfa45b93":"code","a5ec1bdc":"code","37616b67":"code","6c26b07a":"code","fd8a9af5":"code","1cee8f29":"code","b380faf1":"code","3116a838":"code","e6cdb3e4":"code","b531116e":"code","496c3eff":"code","bc7656a0":"code","c8bfbb44":"code","ad5d852d":"code","fa6c3d74":"code","90aae79f":"code","98b6a443":"markdown","f9d877fc":"markdown","075d19a7":"markdown","c5f0c661":"markdown","60c1a89f":"markdown","469715d0":"markdown","e9694733":"markdown","fe634c88":"markdown","79177a0f":"markdown","9b632e0e":"markdown","f0177b37":"markdown","8c1b155a":"markdown","9d441269":"markdown","9a9780ae":"markdown","d907cb23":"markdown"},"source":{"7b9a5131":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#For confusion matrixes\nfrom sklearn.metrics import confusion_matrix\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9d4acc5b":"# Read our data from dataset.\ndata = pd.read_csv(\"..\/input\/voice.csv\")","19598c17":"#Let's looking at top 5 datas.\ndata.head()","d22213dc":"#Let's looking at last 10 datas.\ndata.tail(10)","72142122":"data.describe()","dfa45b93":"data.corr()","a5ec1bdc":"# Firstly, we must check our data. If we have NaN values, we should drop them.\ndata.info()\n#As we can see easily, we have no NaN values.","37616b67":"data.label = [1 if each == \"female\" else 0 for each in data.label]\n#We assign 1 to female, 0 to male.","6c26b07a":"data.info()","fd8a9af5":"#We should have x and y values for test-train datas.\ny = data.label.values\nx_data = data.drop([\"label\"],axis=1)","1cee8f29":"#Normalization\nx = (x_data - np.min(x_data)) \/ (np.max(x_data)).values","b380faf1":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 42)\n#test_size=0.2 means %20 test datas, %80 train datas\nmethod_names = []\nmethod_scores = []\n#These are for barplot in conclusion","3116a838":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train) #Fitting\nprint(\"Logistic Regression Classification Test Accuracy {}\".format(log_reg.score(x_test,y_test)))\nmethod_names.append(\"Logistic Reg.\")\nmethod_scores.append(log_reg.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = log_reg.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","e6cdb3e4":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\nprint(\"Score for Number of Neighbors = 3: {}\".format(knn.score(x_test,y_test)))\nmethod_names.append(\"KNN\")\nmethod_scores.append(knn.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = knn.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","b531116e":"score_list=[]\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n\nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"score\")","496c3eff":"knn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(x_train,y_train)\nprint(\"Score for Number of Neighbors = 2: {}\".format(knn.score(x_test,y_test)))\n\n#Confusion Matrix\ny_pred = knn.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","bc7656a0":"from sklearn.svm import SVC\nsvm = SVC(random_state=42)\nsvm.fit(x_train,y_train)\nprint(\"SVM Classification Score is: {}\".format(svm.score(x_test,y_test)))\nmethod_names.append(\"SVM\")\nmethod_scores.append(svm.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = svm.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","c8bfbb44":"from sklearn.naive_bayes import GaussianNB\nnaive_bayes = GaussianNB()\nnaive_bayes.fit(x_test,y_test)\nprint(\"Naive Bayes Classification Score: {}\".format(naive_bayes.score(x_test,y_test)))\nmethod_names.append(\"Naive Bayes\")\nmethod_scores.append(naive_bayes.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = naive_bayes.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","ad5d852d":"from sklearn.tree import DecisionTreeClassifier\ndec_tree = DecisionTreeClassifier()\ndec_tree.fit(x_train,y_train)\nprint(\"Decision Tree Classification Score: \",dec_tree.score(x_test,y_test))\nmethod_names.append(\"Decision Tree\")\nmethod_scores.append(dec_tree.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = dec_tree.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","fa6c3d74":"from sklearn.ensemble import RandomForestClassifier\nrand_forest = RandomForestClassifier(n_estimators=100, random_state=42)\nrand_forest.fit(x_train,y_train)\nprint(\"Random Forest Classification Score: \",rand_forest.score(x_test,y_test))\nmethod_names.append(\"Random Forest\")\nmethod_scores.append(rand_forest.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = rand_forest.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","90aae79f":"plt.figure(figsize=(15,10))\nplt.ylim([0.85,1])\nplt.bar(method_names,method_scores,width=0.5)\nplt.xlabel('Method Name')\nplt.ylabel('Method Score')","98b6a443":" **INTRODUCTION**\n \n We'll learn, practise and compare 6 classification models in this project. So, you'll see in this kernel:\n\n* EDA (Exploratory Data Analysis)\n* What is Confusion Matrix?\n* Test-Train Datas Split\n* Logistic Regression Classification\n* KNN Classification\n* Support Vector Machine (SVM) Classification\n* Naive Bayes Classification\n* Desicion Tree Classification\n* Random Forest Classification\n* Compare all of these Classification Models\n* Conclusion","f9d877fc":"**DECISION TREE CLASSIFICATION**","075d19a7":"**Our 'label' feature has 2 valuable: male and female. These are string but we need integers for classification. Therefore, we must convert them from object to integer.**","c5f0c661":"**As you can see; our label features converted integer!**","60c1a89f":"**SUPPORT VECTOR MACHINE (SVM)**","469715d0":"**RANDOM FOREST CLASSIFICATION**","e9694733":"**After assign x and y value; we should train and test datas split.**","fe634c88":"**As we can see; the best value of n_neighbor is 2. Let's find score when n_neighbors=2**","79177a0f":"**And now time to classification our data!**\n\n**We start with:**\n\n**LOGISTIC REGRESSION CLASSIFICATION**","9b632e0e":"**Let's check it!**","f0177b37":"**CONCLUSION**\n\nWe completed seven different classification on this data and we see; Random Forest Classification is the best way to make classification on this dataset. Of course not everytime but for this practice Random Forest gave us the best classifications!\n\nLet's see differences between our methods scores!","8c1b155a":"**KNN (K-Nearest Neighbour) CLASSIFICATION**","9d441269":"**Confusion Matrix**\n\nBefore start the classifications, we should know one thing: Confusion Matrix!\nFor example; we have 100 data point (dogs and cats) and we make a prediction. Our prediction score is 0.8 so we predict well %80. Confusion matrix gives us; \n* How many true values we predict true  (TP = True Positive)\n* How many true values we predict false  (FP = False Positive)\n* How many false values we predict false  (TN = True Negative)\n* How many false values we predict true (FN = False Negative\n","9a9780ae":"**n_neighbors is an optional parameter. I wrote it 3 but you can write anything. Let's learn the best value of n_neighbors parameter.**","d907cb23":"**NAIVE BAYES CLASSIFICATION**"}}