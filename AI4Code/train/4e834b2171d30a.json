{"cell_type":{"0f7569f5":"code","efa31757":"code","b64bec0f":"code","c619cf19":"code","32f74d06":"code","032ac925":"code","34de9a6b":"code","40f42f2a":"code","b439fade":"code","1f70ca99":"code","636acc8b":"code","24f4a7e3":"code","9a4464d0":"code","c1594181":"code","d787cac9":"code","4c42ac7e":"code","1576f632":"code","444509e8":"code","8579e090":"code","f23e8519":"code","262bd48a":"code","fca8f8b5":"code","af184408":"code","823de12e":"code","1faa4b5f":"code","3e746848":"code","e90a0a6c":"code","ff6aa258":"code","5b58f989":"code","af09133c":"code","48fb30be":"markdown","49dc4fcb":"markdown","1638c622":"markdown","fefafdc7":"markdown","5780bc8a":"markdown","93ebb248":"markdown","9c201cb4":"markdown","44848b4b":"markdown","9a0ff5b4":"markdown","ddecbf64":"markdown","f4c8495b":"markdown","4064ec5e":"markdown","132f4a94":"markdown","e7f363c2":"markdown","20d6bb72":"markdown","88fc35de":"markdown","1fdd7546":"markdown","75c45802":"markdown","c84403e1":"markdown","a9694a1b":"markdown","1a2e7d7d":"markdown","7d0b1e46":"markdown","999af44e":"markdown"},"source":{"0f7569f5":"# %%capture\n# !pip install forgi\n# !yes Y |conda install -c bioconda viennarna","efa31757":"import json,os, math\n\nimport subprocess\n# from forgi.graph import bulge_graph\n# import forgi.visual.mplotlib as fvm\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow.keras.backend as K\nimport plotly.express as px\nimport tensorflow.keras.layers as L\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow_addons as tfa\n\nfrom itertools import combinations_with_replacement\nfrom sklearn.model_selection import train_test_split, KFold,  StratifiedKFold,GroupKFold\nfrom keras.utils import plot_model\nfrom colorama import Fore, Back, Style","b64bec0f":"###### USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD\nSEED = 53\n\n###### NUMBER OF FOLDS. USE 3, 5, 7,... \nn_folds=5\n\n###### TRAIN DEBUG\ndebug=True\n\n###### APPLY WINDOW FEATURES\nWindow_features = True\n\n###### Number of Feature Given to Model\n# cat_feature = 3  ## ( Categorical Features Only)\n# num_features = 1  ## ( Numerical Features Only)\n\n###### Model Configuration ######\n\nmodel_name=\"GG\"                ## MODEL NAME (Files will save according to this )\nepochs=100              ## NUMBER OF EPOCHS MODEL TRAIN IN EACH FOLD. USE 3, 5, 7,... \nBATCH_SIZE = 32                 ## NUMBER OF BATCH_SIZE USE 16, 32, 64, 128,...\nn_layers = 2                  ## Number of Layers Present in model # ex. 3 Layer of GRU Model\nlayers = [\"GRU\",\"GRU\"]   ## Stacking sequence of GRU and LSTM (list of length == n_layers)\nhidden_dim = [128, 128]    ## Hidden Dimension in Model (Default : [128,128]) (list of length == n_layers)\ndropout = [0.5, 0.5]       ## 1.0 means no dropout, and 0.0 means no outputs from the layer.\nsp_dropout = 0.2                ## SpatialDropout1D (Fraction of the input units to drop) [https:\/\/stackoverflow.com\/a\/55244985]\nembed_dim = 250                  ## Output Dimention of Embedding Layer (Default : 75)\nnum_hidden_units = 8      ## Number of GRU units after num_input layer\n\n\n###### LR Schedular ######\n\nCosine_Schedule = True         ## cosine_schedule Rate\nRampup_decy_lr = False           ## Rampup decy lr Schedule","c619cf19":"def seed_everything(seed=1234):   \n    np.random.seed(seed)   \n    tf.random.set_seed(seed)   \n    os.environ['PYTHONHASHSEED'] = str(seed)   \n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \nseed_everything(SEED)","32f74d06":"target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\nwindow_columns = ['sequence','structure','predicted_loop_type']\n\ncategorical_features = ['sequence', 'structure', 'predicted_loop_type',]\n#                        'predicted_loop_index']\n\ncat_feature = len(categorical_features)\nif Window_features:\n    cat_feature += len(window_columns)\n\nnumerical_features = ['BPPS_Max','BPPS_nb', 'BPPS_sum',\n                      'positional_entropy',\n                      'stems', 'interior_loops', 'multiloops',#'hairpin loops', 'fiveprimes', 'threeprimes', \n                      'A_percent', 'G_percent','C_percent', 'U_percent', \n                      'U-G', 'C-G', 'U-A', 'G-C', 'A-U', 'G-U', \n#                       'E', 'S', 'H', 'B', 'X', 'I', 'M', \n                      'pair_map', 'pair_distance', ]\n    \n\nnum_features = len(numerical_features)  ## ( Numerical Features Only)\n\nfeature_cols = categorical_features + numerical_features\npred_col_names = [\"pred_\"+c_name for c_name in target_cols]\n\ntarget_eval_col = ['reactivity','deg_Mg_pH10','deg_Mg_50C']\npred_eval_col = [\"pred_\"+c_name for c_name in target_eval_col]","032ac925":"data_dir = '\/kaggle\/input\/stanford-covid-vaccine\/'\nfearure_data_path = '..\/input\/openvaccine\/'\n\n# train  = pd.read_csv(fearure_data_path+'train.csv')\n# test = pd.read_csv(fearure_data_path+'test.csv')\n\ntrain  = pd.read_json(fearure_data_path+'train.json')\ntest = pd.read_json(fearure_data_path+'test.json')\n\n# train_j = pd.read_json(data_dir + 'train.json', lines=True)\n# test_j = pd.read_json(data_dir + 'test.json', lines=True)\nsample_sub = pd.read_csv(data_dir + 'sample_submission.csv')","34de9a6b":"train[target_cols] = train[target_cols].applymap(lambda x: x[1:-1].split(\", \"))","40f42f2a":"train = train[train['SN_filter'] == 1]\n# train = train[train['signal_to_noise'] >= 0.5]","b439fade":"def pair_feature(row):\n    arr = list(row)\n    its = [iter(['_']+arr[:]) ,iter(arr[1:]+['_'])]\n    list_touple = list(zip(*its))\n    return list(map(\"\".join,list_touple))","1f70ca99":"def preprocess_categorical_inputs(df, cols=categorical_features,Window_features=Window_features):\n    \n    if Window_features:\n        for c in window_columns:\n            df[\"pair_\"+c] = df[c].apply(pair_feature)\n            cols.append(\"pair_\"+c)\n    cols = list(set(cols))\n    \n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )","636acc8b":"def preprocess_numerical_inputs(df, cols=numerical_features):\n    \n    return np.transpose(\n        np.array(\n            df[cols].values.tolist()\n        ),\n        (0, 2, 1)\n    )","24f4a7e3":"# We will use this dictionary to map each character to an integer\n# so that it can be used as an input in keras\n# ().ACGUBEHIMSXshftim0123456789[]{}'_,\ntoken_list = list(\"().ACGUESHBXIM\")\nif Window_features:\n    comb = combinations_with_replacement(list('_().ACGUESHBXIM'*2), 2) \n    token_list += list(set(list(map(\"\".join,comb))))\n\ntoken2int = {x:i for i, x in enumerate(list(set(token_list)))}\nprint(\"token_list Size :\",len(token_list))\n    \ntrain_inputs_all_cat = preprocess_categorical_inputs(train,cols=categorical_features)\ntrain_inputs_all_num = preprocess_numerical_inputs(train,cols=numerical_features)\ntrain_labels_all = np.array(train[target_cols].values.tolist(),dtype =np.float32).transpose((0, 2, 1))\n\nprint(\"Train categorical Features Shape : \",train_inputs_all_cat.shape)\nprint(\"Train numerical Features Shape : \",train_inputs_all_num.shape)\nprint(\"Train labels Shape : \",train_labels_all.shape)","9a4464d0":"# train_inputs_all_cat = train_inputs_all_cat[:,:68,:]\n# train_inputs_all_num = train_inputs_all_num[:,:68,:]\n# train_labels_all = train_labels_all[:,:68,:]\n\n# print(\"Train categorical Features Shape : \",train_inputs_all_cat.shape)\n# print(\"Train numerical Features Shape : \",train_inputs_all_num.shape)\n# print(\"Train labels Shape : \",train_labels_all.shape)","c1594181":"public_df = test.query(\"seq_length == 107\")\nprivate_df = test.query(\"seq_length == 130\")\nprint(\"public_df : \",public_df.shape)\nprint(\"private_df : \",private_df.shape)\n\npublic_inputs_cat = preprocess_categorical_inputs(public_df)\nprivate_inputs_cat = preprocess_categorical_inputs(private_df)\n\npublic_inputs_num = preprocess_numerical_inputs(public_df,cols=numerical_features)\nprivate_inputs_num = preprocess_numerical_inputs(private_df,cols=numerical_features)\n\nprint(\"Public categorical Features Shape : \",public_inputs_cat.shape)\nprint(\"Public numerical Features Shape : \",public_inputs_num.shape)\n\nprint(\"Private categorical Features Shape : \",private_inputs_cat.shape)\nprint(\"Private numerical Features Shape : \",private_inputs_num.shape)","d787cac9":"### Custom Loss Function for ['reactivity','deg_Mg_pH10','deg_Mg_50C'] target Columns\n\n# def rmse(y_actual, y_pred):\n#     mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n#     return K.sqrt(mse)\n\n# def MCRMSE(y_actual, y_pred, num_scored=3):\n#     score = 0\n#     for i in range(num_scored):\n#         score += rmse(y_actual[:,:, i], y_pred[:,:, i]) \/ num_scored\n#     return score\n\ndef MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true[:,:,:3] - y_pred[:,:,:3]), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)","4c42ac7e":"def get_lr_callback(batch_size=8):\n    lr_start   = 0.00001\n    lr_max     = 0.004\n    lr_min     = 0.00005\n    lr_ramp_ep = 45\n    lr_sus_ep  = 2\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","1576f632":"def get_cosine_schedule_with_warmup(lr,num_warmup_steps, num_training_steps, num_cycles=3.5):\n    \"\"\"\n    Modified version of the get_cosine_schedule_with_warmup from huggingface.\n    (https:\/\/huggingface.co\/transformers\/_modules\/transformers\/optimization.html#get_cosine_schedule_with_warmup)\n\n    Create a schedule with a learning rate that decreases following the\n    values of the cosine function between 0 and `pi * cycles` after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"\n\n    def lrfn(epoch):\n        if epoch < num_warmup_steps:\n            return (float(epoch) \/ float(max(1, num_warmup_steps))) * lr\n        progress = float(epoch - num_warmup_steps) \/ float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)","444509e8":"def lstm_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.LSTM(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer = 'orthogonal'))","8579e090":"def gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(\n        L.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal')\n    )","f23e8519":"# def build_model(embed_size, \n#                 seq_len = 107, \n#                 pred_len = 68, \n#                 dropout = dropout, \n#                 sp_dropout = sp_dropout, \n#                 num_features = num_features,\n#                 num_hidden_units = num_hidden_units,\n#                 embed_dim = embed_dim,\n#                 layers = layers, \n#                 hidden_dim = hidden_dim, \n#                 n_layers = n_layers,\n#                 cat_feature = cat_feature):\n    \n#     inputs = L.Input(shape=(seq_len, cat_feature),name='category_input')\n#     embed = L.Embedding(input_dim=embed_size, output_dim=embed_dim)(inputs)\n#     reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n#     reshaped_conv = tf.keras.layers.Conv1D(filters=512, kernel_size=3,strides=1, padding='same', activation='elu')(reshaped)\n    \n#     numerical_input = L.Input(shape=(seq_len, num_features), name='numeric_input')\n#     n_Dense_1 = L.Dense(64)(numerical_input)\n#     n_Dense_2 = L.Dense(128)(n_Dense_1)\n#     numerical_conv = tf.keras.layers.Conv1D(filters=256, kernel_size=4,strides=1, padding='same', activation='elu')(n_Dense_2)\n    \n#     hidden = L.concatenate([reshaped_conv, numerical_conv])\n#     hidden = L.SpatialDropout1D(sp_dropout)(hidden)\n    \n#     for x in range(n_layers):\n#         if layers[x] == \"GRU\":\n#             hidden = gru_layer(hidden_dim[x], dropout[x])(hidden)\n#         else:\n#             hidden = lstm_layer(hidden_dim[x], dropout[x])(hidden)\n            \n#     # Since we are only making predictions on the first part of each sequence, \n#     # we have to truncate it\n#     truncated = hidden[:, :pred_len]\n    \n#     out = L.Dense(5)(truncated)\n    \n#     model = tf.keras.Model(inputs=[inputs] + [numerical_input], outputs=out)\n    \n#     adam = tf.optimizers.Adam()\n#     radam = tfa.optimizers.RectifiedAdam()\n#     lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n#     ranger = tfa.optimizers.Lookahead(radam, sync_period=6)\n    \n#     model.compile(optimizer=radam, loss=MCRMSE)\n    \n#     return model","262bd48a":"def build_model(embed_size, \n                seq_len = 107, \n                pred_len = 68, \n                dropout = dropout, \n                sp_dropout = sp_dropout, \n                num_features = num_features,\n                num_hidden_units = num_hidden_units,\n                embed_dim = embed_dim,\n                layers = layers, \n                hidden_dim = hidden_dim, \n                n_layers = n_layers,\n                cat_feature = cat_feature):\n    \n    inputs = L.Input(shape=(seq_len, cat_feature),name='category_input')\n    embed = L.Embedding(input_dim=embed_size, output_dim=embed_dim)(inputs)\n    reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n    reshaped = L.SpatialDropout1D(sp_dropout)(reshaped)\n    reshaped_conv = tf.keras.layers.Conv1D(filters=512, kernel_size=3,strides=1, padding='same', activation='elu')(reshaped)\n    \n    numerical_input = L.Input(shape=(seq_len, num_features), name='numeric_input')\n#     n_Dense_1 = L.Dense(64)(numerical_input)\n#     n_Dense_2 = L.Dense(128)(n_Dense_1)\n#     numerical_conv = tf.keras.layers.Conv1D(filters=256, kernel_size=4,strides=1, padding='same', activation='elu')(n_Dense_2)\n    \n    hidden = L.concatenate([reshaped_conv, numerical_input])\n    hidden_1 = tf.keras.layers.Conv1D(filters=256, kernel_size=4,strides=1, padding='same', activation='elu')(hidden)\n    hidden = gru_layer(128, 0.5)(hidden_1)\n    hidden = L.concatenate([hidden, hidden_1])\n#     hidden = L.SpatialDropout1D(sp_dropout)(hidden)\n    \n    for x in range(n_layers):\n        if layers[x] == \"GRU\":\n            hidden = gru_layer(hidden_dim[x], dropout[x])(hidden)\n        else:\n            hidden = lstm_layer(hidden_dim[x], dropout[x])(hidden)\n        \n        hidden = L.concatenate([hidden, hidden_1])\n            \n    # Since we are only making predictions on the first part of each sequence, \n    # we have to truncate it\n    truncated = hidden[:, :pred_len]\n    \n    out = L.Dense(5)(truncated)\n    \n    model = tf.keras.Model(inputs=[inputs] + [numerical_input], outputs=out)\n    \n    adam = tf.optimizers.Adam()\n    radam = tfa.optimizers.RectifiedAdam()\n    lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n    ranger = tfa.optimizers.Lookahead(radam, sync_period=6)\n    \n    model.compile(optimizer=radam, loss=MCRMSE)\n    \n    return model","fca8f8b5":"model = build_model(embed_size=len(token_list))\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","af184408":"def get_stratify_group(row):\n    snf = row['SN_filter']\n    snr = row['signal_to_noise']\n    cnt = row['cnt']\n    id_ = row['id']\n    structure = row['structure']\n    if snf == 0:\n        if snr<0:\n            snr_c = 0\n        elif 0<= snr < 2:\n            snr_c = 1\n        elif 2<= snr < 4:\n            snr_c = 2\n        elif 4<= snr < 5.5:\n            snr_c = 3\n        elif 5.5<= snr < 10:\n            snr_c = 4\n        elif snr >= 10:\n            snr_c = 5\n            \n    else: # snf == 1\n        if snr<0:\n            snr_c = 6\n        elif 0<= snr < 1:\n            snr_c = 7\n        elif 1<= snr < 2:\n            snr_c = 8\n        elif 2<= snr < 3:\n            snr_c = 9\n        elif 3<= snr < 4:\n            snr_c = 10\n        elif 4<= snr < 5:\n            snr_c = 11\n        elif 5<= snr < 6:\n            snr_c = 12\n        elif 6<= snr < 7:\n            snr_c = 13\n        elif 7<= snr < 8:\n            snr_c = 14\n        elif 8<= snr < 9:\n            snr_c = 15\n        elif 9<= snr < 10:\n            snr_c = 15\n        elif snr >= 10:\n            snr_c = 16\n        \n    return '{}_{}'.format(id_,snr_c)\n\ntrain['stratify_group'] = train.apply(get_stratify_group, axis=1)\ntrain['stratify_group'] = train['stratify_group'].astype('category').cat.codes\n\nskf = StratifiedKFold(n_folds, shuffle=True, random_state=SEED)\ngkf = GroupKFold(n_splits=n_folds)\nfig, ax = plt.subplots(n_folds,3,figsize=(20,5*n_folds))\n\nfor Fold, (train_index, val_index) in enumerate(gkf.split(train_inputs_all_cat, groups=train['stratify_group'])):\n    print(Fore.YELLOW);print('#'*45);print(\"###  Fold : \", str(Fold+1));print('#'*45);print(Style.RESET_ALL)\n    \n    train_data = train.iloc[train_index]\n    val_data = train.iloc[val_index]\n    print(\"Augmented data Present in Val Data : \",len(val_data[val_data['cnt'] != 1]))\n    print(\"Augmented data Present in Train Data : \",len(train_data[train_data['cnt'] != 1]))\n    val_data = val_data[val_data['cnt'] == 1]\n    print(\"Data Lekage : \",len(val_data[val_data['id'].isin(train_data['id'])]))\n#     print(train_data['stratify_group'].unique(),val_data['stratify_group'].unique())\n    \n    print(\"number of Train Data points : \",len(train_data))\n    print(\"number of val_data Data points : \",len(val_data))\n    print(\"number of unique Structure in Train data : \", len(train_data.structure.unique()))\n    print(\"number of unique Structure in val data : \",len(val_data.structure.unique()), val_data.structure.value_counts()[:5].values)\n    \n    print(\"Train SN_Filter == 1 : \", len(train_data[train_data['SN_filter']==1]))\n    print(\"val_data SN_Filter == 1 : \", len(val_data[val_data['SN_filter']==1]))\n    print(\"Train SN_Filter == 0 : \", len(train_data[train_data['SN_filter']==0]))\n    print(\"val_data SN_Filter == 0 : \", len(val_data[val_data['SN_filter']==0]))\n    \n    print(\"Unique ID :\",len(train_data.id.unique()))\n    sns.kdeplot(train[train['SN_filter']==0]['signal_to_noise'],ax=ax[Fold][0],color=\"Red\",label='Train All')\n    sns.kdeplot(train_data[train_data['SN_filter']==0]['signal_to_noise'],ax=ax[Fold][0],color=\"Blue\",label='Train')\n    sns.kdeplot(val_data[val_data['SN_filter']==0]['signal_to_noise'],ax=ax[Fold][0],color=\"Green\",label='Validation')            \n    ax[Fold][0].set_title(f'Fold : {Fold+1} Signal\/Noise & SN_filter == 0')\n    \n    sns.kdeplot(train[train['SN_filter']==1]['signal_to_noise'],ax=ax[Fold][1],color=\"Red\",label='Train All')\n    sns.kdeplot(train_data[train_data['SN_filter']==1]['signal_to_noise'],ax=ax[Fold][1],color=\"Blue\",label='Train')\n    sns.kdeplot(val_data[val_data['SN_filter']==1]['signal_to_noise'],ax=ax[Fold][1],color=\"Green\",label='Validation')            \n    ax[Fold][1].set_title(f'Fold : {Fold+1} Signal\/Noise & SN_filter == 1')\n    \n    sns.kdeplot(train['signal_to_noise'],ax=ax[Fold][2],color=\"Red\",label='Train All')\n    sns.kdeplot(train_data['signal_to_noise'],ax=ax[Fold][2],color=\"Blue\",label='Train')\n    sns.kdeplot(val_data['signal_to_noise'],ax=ax[Fold][2],color=\"Green\",label='Validation')            \n    ax[Fold][2].set_title(f'Fold : {Fold+1} Signal\/Noise')\n            \nplt.show()\n","823de12e":"submission = pd.DataFrame(index=sample_sub.index, columns=target_cols).fillna(0) # test dataframe with 0 values\n\nval_losses = []\nhistorys = []\noof_preds_all = []\nstacking_pred_all = []\n\nkf = KFold(n_folds, shuffle=True, random_state=SEED)\nskf = StratifiedKFold(n_folds, shuffle=True, random_state=SEED)\ngkf = GroupKFold(n_splits=n_folds)\n\nfor Fold, (train_index, val_index) in enumerate(gkf.split(train_inputs_all_cat, groups=train['stratify_group'])):\n    print(Fore.YELLOW);print('#'*45);print(\"###  Fold : \", str(Fold+1));print('#'*45);print(Style.RESET_ALL)\n    print(f\"|| Batch_size: {BATCH_SIZE} \\n|| n_layers: {n_layers} \\n|| embed_dim: {embed_dim}\")\n    print(f\"|| cat_feature: {cat_feature} \\n|| num_features: {num_features}\")\n    print(f\"|| layers : {layers} \\n|| hidden_dim: {hidden_dim} \\n|| dropout: {dropout} \\n|| sp_dropout: {sp_dropout}\")\n    \n    \n    train_data = train.iloc[train_index]\n    val_data = train.iloc[val_index]\n    \n    print(\"|| number Augmented data Present in Val Data : \",len(val_data[val_data['cnt'] != 1]))\n    print(\"|| number Augmented data Present in Train Data : \",len(train_data[train_data['cnt'] != 1]))\n    print(\"|| Data Lekage : \",len(val_data[val_data['id'].isin(train_data['id'])]))\n    \n    val_data = val_data[val_data['cnt'] == 1]\n    model_train = build_model(embed_size=len(token_list))\n    model_short = build_model(embed_size=len(token_list),seq_len=107, pred_len=107)\n    model_long = build_model(embed_size=len(token_list),seq_len=130, pred_len=130)\n\n    train_inputs_cat = preprocess_categorical_inputs(train_data,cols=categorical_features)\n    train_inputs_num = preprocess_numerical_inputs(train_data,cols=numerical_features)\n    train_labels = np.array(train_data[target_cols].values.tolist(),dtype =np.float32).transpose((0, 2, 1))\n    \n    val_inputs_cat = preprocess_categorical_inputs(val_data,cols=categorical_features)\n    val_inputs_num = preprocess_numerical_inputs(val_data,cols=numerical_features)\n    val_labels = np.array(val_data[target_cols].values.tolist(),dtype =np.float32).transpose((0, 2, 1))\n    \n#     train_inputs_cat, train_labels = train_inputs_all_cat[train_index], train_labels_all[train_index]\n#     val_inputs_cat, val_labels = train_inputs_all_cat[val_index], train_labels_all[val_index]\n#     train_inputs_num, val_inputs_num = train_inputs_all_num[train_index],train_inputs_all_num[val_index]\n    \n    # csv_logger\n    csv_logger = tf.keras.callbacks.CSVLogger(f'Fold_{Fold}_log.csv', separator=',', append=False)\n    \n    # SAVE BEST MODEL EACH FOLD\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'{model_name}_Fold_{Fold}.h5', \n                                                    monitor='val_loss', \n                                                    verbose=0, \n                                                    mode='min', \n                                                    save_freq='epoch')\n    \n    if Cosine_Schedule:\n        #cosine Callback\n        lr_schedule= get_cosine_schedule_with_warmup(lr=0.001, num_warmup_steps=20, num_training_steps=epochs)\n    elif Rampup_decy_lr : \n        # Rampup decy lr \n        lr_schedule = get_lr_callback(BATCH_SIZE)\n    else:\n        lr_schedule = tf.keras.callbacks.ReduceLROnPlateau()\n        \n    history = model_train.fit(\n        {'numeric_input': train_inputs_num,\n           'category_input': train_inputs_cat} , train_labels, \n        validation_data=({'numeric_input': val_inputs_num,\n                          'category_input': val_inputs_cat}\n                         ,val_labels),\n        batch_size=BATCH_SIZE,\n        epochs=epochs, \n        callbacks=[lr_schedule, checkpoint, csv_logger,lr_schedule],\n        verbose=1 if debug else 0\n    )\n    \n    print(\"Min Validation Loss : \", min(history.history['val_loss']))\n    print(\"Min Validation Epoch : \",np.argmin( history.history['val_loss'] )+1)\n    val_losses.append(min(history.history['val_loss']))\n    historys.append(history)\n\n    model_short.load_weights(f'{model_name}_Fold_{Fold}.h5')\n    model_long.load_weights(f'{model_name}_Fold_{Fold}.h5')\n    \n    public_preds = model_short.predict({'numeric_input': public_inputs_num,\n                                        'category_input': public_inputs_cat})\n    \n    private_preds = model_long.predict({'numeric_input': private_inputs_num,\n                                        'category_input': private_inputs_cat})\n    \n    oof_preds = model_train.predict({'numeric_input': val_inputs_num,\n                                        'category_input': val_inputs_cat})\n    \n    stacking_pred = model_short.predict({'numeric_input': val_inputs_num,\n                                        'category_input': val_inputs_cat})\n    \n    preds_model = []\n    for df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n        for i, uid in enumerate(df.id):\n            single_pred = preds[i]\n\n            single_df = pd.DataFrame(single_pred, columns=target_cols)\n            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n            \n            preds_model.append(single_df)\n    \n    preds_model_df = pd.concat(preds_model)\n    preds_model_df = preds_model_df.groupby(['id_seqpos'],as_index=True).mean()\n    submission[target_cols] += preds_model_df[target_cols].values \/ n_folds\n    \n    for df, preds in [(val_data, oof_preds)]:\n        for i, uid in enumerate(df.id):\n            single_pred = preds[i]\n            single_label = val_labels[i]\n            single_label_df = pd.DataFrame(single_label, columns=target_cols)\n            single_label_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_label_df.shape[0])]\n            single_label_df['id'] = [f'{uid}' for x in range(single_label_df.shape[0])]\n            single_label_df['s_id'] = [x for x in range(single_label_df.shape[0])]\n            single_df = pd.DataFrame(single_pred, columns=pred_col_names)\n            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n            \n            single_df = pd.merge(single_label_df,single_df, on=\"id_seqpos\", how=\"left\")\n            \n            oof_preds_all.append(single_df)\n    \n    for df, preds in [(val_data, stacking_pred)]:\n        for i, uid in enumerate(df.id):\n            single_pred = preds[i]\n#             single_label = val_labels[i]\n#             single_label_df = pd.DataFrame(single_label, columns=target_cols)\n#             single_label_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_label_df.shape[0])]\n#             single_label_df['id'] = [f'{uid}' for x in range(single_label_df.shape[0])]\n#             single_label_df['s_id'] = [x for x in range(single_label_df.shape[0])]\n            single_df = pd.DataFrame(single_pred, columns=pred_col_names)\n            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n            single_df['id'] = [uid for x in range(single_df.shape[0])]\n            stacking_pred_all.append(single_df)\n        \n    # PLOT TRAINING\n    history_data = pd.read_csv(f'Fold_{Fold}_log.csv')\n    EPOCHS = len(history_data['epoch'])\n    history = pd.DataFrame({'history':history_data.to_dict('list')})\n    fig = plt.figure(figsize=(15,5))\n    plt.plot(np.arange(EPOCHS),history.history['lr'],'-',label='Learning Rate',color='#ff7f0e')\n    x = np.argmax( history.history['lr'] ); y = np.max( history.history['lr'] )\n    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,f'Max Learning Rate : {y}' ,size=12)\n    plt.ylabel('Learning Rate',size=14); plt.xlabel('Epoch',size=14)\n    plt.legend(loc=1)\n    plt2 = plt.gca().twinx()\n    plt2.plot(np.arange(EPOCHS),history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n    plt2.plot(np.arange(EPOCHS),history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n    x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n    ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n    plt.ylabel('Loss',size=14)\n    fig.text(s=f\"Model Name : {model_name}\" , x=0.5, y=1.08, fontsize=18, ha='center', va='center',color=\"green\")\n    fig.text(s=f\"|| Fold : {Fold+1} | Batch_size: {BATCH_SIZE} | num_features: {num_features} | cat_feature: {cat_feature} |n_layers: {n_layers} | embed_dim: {embed_dim} ||\", x=0.5, y=1.0, fontsize=15, ha='center', va='center',color=\"red\")\n    fig.text(s=f\"|| layers : {layers} | hidden_dim: {hidden_dim} | dropout: {dropout} | sp_dropout: {sp_dropout} ||\", x=0.5, y=0.92, fontsize=15, ha='center', va='center',color=\"blue\")\n    plt.legend(loc=3)\n    plt.savefig(f'Fold_{Fold+1}.png', bbox_inches='tight')\n    plt.show()\n    \nsubmission[\"id_seqpos\"] = preds_model_df.index\nsubmission = pd.merge(sample_sub[\"id_seqpos\"], submission, on=\"id_seqpos\", how=\"left\")\nOOF = pd.concat(oof_preds_all)\nstacking_df = pd.concat(stacking_pred_all)","1faa4b5f":"OOF = OOF.groupby(['id_seqpos','id','s_id'],as_index=False).mean()\nOOF = OOF.sort_values(['id','s_id'],ascending=[True, True])\nOOF_score = MCRMSE(np.expand_dims(OOF[target_eval_col].values, axis=0), np.expand_dims(OOF[pred_eval_col].values, axis=0)).numpy()[0]\nprint(\"Overall OOF Score :\",OOF_score)\nOOF.to_csv('OOf.csv',index=True)\nOOF.head()","3e746848":"OOF_filter_1 = pd.merge(train[['SN_filter','id']],OOF,on='id')\nOOF_filter_1 = OOF_filter_1.groupby(['id_seqpos','id'],as_index=False).mean()\nOOF_filter_1 = OOF_filter_1.sort_values(['id','s_id'],ascending=[True, True])\nOOF_filter_1 = OOF_filter_1[OOF_filter_1['SN_filter'] == 1]\nOOF_filter_1_score = MCRMSE(np.expand_dims(OOF_filter_1[target_eval_col].values, axis=0), np.expand_dims(OOF_filter_1[pred_eval_col].values, axis=0)).numpy()[0]\nprint(\"OOF_public Score :\",OOF_filter_1_score)\nOOF_filter_1.to_csv('OOF_filter_1.csv',index=False)\nOOF_filter_1.head()","e90a0a6c":"OOF_filter_0 = pd.merge(train[['SN_filter','id']],OOF,on='id')\nOOF_filter_0 = OOF_filter_0.sort_values(['id','s_id'],ascending=[True, True])\nOOF_filter_0 = OOF_filter_0[OOF_filter_0['SN_filter'] == 0]\nOOF_filter_0_score = MCRMSE(np.expand_dims(OOF_filter_0[target_eval_col].values, axis=0), np.expand_dims(OOF_filter_0[pred_eval_col].values, axis=0)).numpy()[0]\nprint(\"OOF_public Score :\",OOF_filter_0_score)\nOOF_filter_0.to_csv('OOF_filter_0.csv',index=False)\nOOF_filter_0.head()","ff6aa258":"stacking_df.to_csv('stacking.csv', index=False)\nstacking_df.head()","5b58f989":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","af09133c":"print(\"|No|OOF_score|OOF_filter_1 |OOF_filter_0|LB|n_folds|Window_features|cat_feature|num_features |epochs|BATCH_SIZE|\")\nprint(\"|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\")\nprint(f\"|-|{OOF_score}|{OOF_filter_1_score}|{OOF_filter_0_score}|-|{n_folds}|{Window_features}|{cat_feature}|{num_features}|{epochs}|{BATCH_SIZE}|\")","48fb30be":"|No|n_folds|Window_features|cat_feature|num_features |epochs|BATCH_SIZE|n_layers|layers |hidden_dim|dropout |sp_dropout |embed_dim |num_hidden_units |OOF_score|OOF_filter_1 |OOF_filter_0|LB|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|||||||||||||||||||","49dc4fcb":"### OOF  SN_filter == 1","1638c622":"## loss Function","fefafdc7":"## Reduce Train Data","5780bc8a":"## Calculate OOF Score :","93ebb248":"## Used Columns\n","9c201cb4":"### OOF  SN_filter == 0","44848b4b":"## Model Building","9a0ff5b4":"## Learning Rate Schedulars\n### Rampup decy lr Schedule","ddecbf64":"## Configuration","f4c8495b":"### Cosine schedule with warmup","4064ec5e":"This notebook shows you how to build a model for predicting degradation at various locations along RNA sequence. \n* We will first pre-process and tokenize the sequence, secondary structure and loop type. \n* Then, we will use all the information to train a model on degradations recorded by the researchers from OpenVaccine. \n* Finally, we run our model on the public test set (shorter sequences) and the private test set (longer sequences), and submit the predictions.\n","132f4a94":"## Load and preprocess data","e7f363c2":"## Save Stacking Data","20d6bb72":"The OOF (out of fold) predictions are saved to disk. If you wish to ensemble multiple models, use the OOF to determine what are the best weights to blend your models with. Choose weights that maximize OOF CV score when used to blend OOF. Then use those same weights to blend your test predictions.","88fc35de":"## Different Layers","1fdd7546":"#### Public and private sets have different sequence lengths, so we will preprocess them separately and load models of different tensor shapes.","75c45802":"## stratify_group Based on structure and SN_Filter","c84403e1":"## Build and train model\n\nWe will train a bi-directional GRU model. It has three layer and has dropout. To learn more about RNNs, LSTM and GRU, please see [this blog post](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/).","a9694a1b":"## Add Augmentation Data","1a2e7d7d":"### Overall OOF Score","7d0b1e46":"## Set Seed","999af44e":"## Submit To Kaggle"}}