{"cell_type":{"440fd22f":"code","49d3e84d":"code","e4c9fb05":"code","1902ffee":"code","65a90c67":"code","4fb2a297":"code","6fb1c84b":"code","5ee673c2":"code","70ba2da2":"code","0b3d7a92":"code","2d0db63f":"code","0736814f":"code","58a0335b":"code","3fa4700f":"code","5de27ebf":"code","713168a0":"code","b399b871":"code","9229e1f7":"code","afe8a3d2":"code","db3b7bee":"code","2da9c0be":"markdown","ddc19276":"markdown","aa8ac719":"markdown","3f72c72a":"markdown","ab844d60":"markdown","e7e4b7f9":"markdown","6dbc71c6":"markdown","aff02dea":"markdown","aab75f24":"markdown","816d23fc":"markdown","fb492224":"markdown","a3faca7a":"markdown","f9ce35d3":"markdown","df570194":"markdown","05000300":"markdown","125414e7":"markdown"},"source":{"440fd22f":"# Basic Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n\nfrom glob import glob\nfrom PIL import Image\n\n%matplotlib inline\n\nnp.random.seed(2)","49d3e84d":"# Machine Learning\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_curve, roc_curve\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn import tree, linear_model, ensemble\n\n#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore')","e4c9fb05":"# Setting Default Seaborn Style\nsns.set(style='white', context='notebook', palette='deep')","1902ffee":"train = pd.read_csv(\"..\/input\/chestxraypneumoniacsv\/train.csv\").iloc[:, 1:]\ntest = pd.read_csv(\"..\/input\/chestxraypneumoniacsv\/test.csv\").iloc[:, 1:]\nval = pd.read_csv(\"..\/input\/chestxraypneumoniacsv\/val.csv\").iloc[:, 1:]","65a90c67":"df = pd.concat([train, test])\ndf","4fb2a297":"df.iloc[:, :-1] = df.iloc[:, :-1] \/ 255.0\nval.iloc[:, :-1] = val.iloc[:, :-1] \/ 255.0","6fb1c84b":"le = LabelEncoder()\n\ndf.iloc[:, -1] = le.fit_transform(df.iloc[:, -1])\nval.iloc[:, -1] = le.transform(val.iloc[:, -1])","5ee673c2":"df.reset_index(drop=True, inplace=True)\nval.reset_index(drop=True, inplace=True)","70ba2da2":"X_df = df.iloc[:, :-1]\ny_df = df.iloc[:, -1]\n\nX_val = val.iloc[:, :-1]\ny_val = val.iloc[:, -1]","0b3d7a92":"pca_columns = []\nfor i in range(50):\n    pca_columns.append(f\"PCA{i+1}\")\n\npca = PCA(50)\n\nX_df = pd.DataFrame(pca.fit_transform(X_df), columns=pca_columns)\nX_val = pd.DataFrame(pca.transform(X_val), columns=pca_columns)","2d0db63f":"df_pca = pd.concat([X_df, y_df], axis=1)\nval_pca = pd.concat([X_val, y_val], axis=1)","0736814f":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df_pca, df_pca.iloc[:, -1]):\n    train = df_pca.loc[train_index]\n    test = df_pca.loc[test_index]","58a0335b":"X_train = train.iloc[:, :-1]\ny_train = train.iloc[:, -1].values\n\nX_test = test.iloc[:, :-1]\ny_test = test.iloc[:, -1].values\n\nX_val = val.iloc[:, :-1]\ny_val = val.iloc[:, -1].values","3fa4700f":"MLA_compare = pd.DataFrame()\n\nrow_index = 0\n\ndef MLA_testing(MLA, X_train, X_test, y_train, y_test):  \n    global row_index\n    \n    # Training The Model\n    MLA.fit(X_train, y_train)\n\n    # KFold Accuracies on Training Data\n    kfold_accuracy = cross_val_score(estimator = MLA, X = X_train, y = y_train, cv = 10, n_jobs=-1)\n    print(\"K-Fold Accuracies:\\n\", kfold_accuracy, \"\\n\")\n    \n    # Prediction on Testing Data\n    y_pred = cross_val_predict(estimator = MLA, X = X_test, y = y_test, cv = 10, n_jobs=-1)\n    \n    # Accuracy for y_test and y_pred\n    classifier_accuracy_score = accuracy_score(y_test, y_pred)\n    print(\"Accuracy Score:\\n\", classifier_accuracy_score, \"\\n\")\n    \n    # Confusion Matrix\n    conf_mtx = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix:\\n\", conf_mtx, \"\\n\")\n    \n    # Classification Report\n    class_rep = classification_report(y_test, y_pred)\n    print(\"Classification Report:\\n\", class_rep, \"\\n\")\n    \n    # Precision - Recall Curve\n    yhat = MLA.predict_proba(X_test)\n    no_skill = len(df_pca[\"784\"][df_pca[\"784\"]==1]) \/ len(df_pca[\"784\"])\n    precision, recall, _ = precision_recall_curve(y_test, yhat[:, 1])\n    \n    plt.figure(dpi=100, figsize=(15, 6))\n    plt.subplot(121)\n    sns.lineplot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n    sns.lineplot(recall, precision, marker='.', label=MLA.__class__.__name__)\n    plt.title(\"Recall vs Precision Curve\")\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.legend()\n    \n    # ROC Curve\n    plt.subplot(122)\n    sns.lineplot([0, 1], [0, 1], linestyle='--', label='No Skill')\n    fpr, tpr, _ = roc_curve(y_test, yhat[:, 1])\n    sns.lineplot(fpr, tpr, marker='.', label=MLA.__class__.__name__)\n    plt.title(\"ROC Curve\")\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()\n    plt.show()\n\n    # Saving Data in Dataframe\n    MLA_name = MLA.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'Accuracy Score'] = classifier_accuracy_score*100\n    MLA_compare.loc[row_index, 'K-Fold Accuracy'] = kfold_accuracy.mean()*100\n\n    print(MLA_name, \"Done\")\n    \n    row_index+=1","5de27ebf":"rf_clf = ensemble.RandomForestClassifier()\n\nMLA_testing(rf_clf, X_train, X_test, y_train, y_test)","713168a0":"gb_clf = ensemble.GradientBoostingClassifier()\n\nMLA_testing(gb_clf, X_train, X_test, y_train, y_test)","b399b871":"lr_clf = linear_model.LogisticRegression()\n\nMLA_testing(lr_clf, X_train, X_test, y_train, y_test)","9229e1f7":"sgf_clf = linear_model.SGDClassifier(loss=\"log\")\n\nMLA_testing(sgf_clf, X_train, X_test, y_train, y_test)","afe8a3d2":"dt_clf = tree.DecisionTreeClassifier()\n\nMLA_testing(dt_clf, X_train, X_test, y_train, y_test)","db3b7bee":"MLA_compare = MLA_compare.sort_values(by=\"K-Fold Accuracy\", ascending=False).reset_index(drop=True)\nMLA_compare","2da9c0be":"## GradientBoostingClassifier","ddc19276":"# Classification Model - ML","aa8ac719":"## PCA","3f72c72a":"## SGDClassifier","ab844d60":"# Data Preprocessing","e7e4b7f9":"## Importing the Data","6dbc71c6":"## DecisionTreeClassifier","aff02dea":"## Label encoding","aab75f24":"## Before Proceeding i Request you to add the Dataset at the link Below for the Code Below to Function\nhttps:\/\/www.kaggle.com\/datarohitingole\/chestxraypneumoniacsv\n### What i did is used PIL and Numpy to scale all the Images to 28x28 size and store them as Numeric data in CSV Files.","816d23fc":"## Feature Scaling","fb492224":"# Preparing Data For Model","a3faca7a":"# Comparing Algorithm's Performance","f9ce35d3":"## LogisticRegression","df570194":"## RandomForestClassifier","05000300":"## Stratified Train Test Split","125414e7":"# Importing Libraries"}}