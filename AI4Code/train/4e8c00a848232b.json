{"cell_type":{"08306da5":"code","ae41538b":"code","12add36c":"code","dd2de13d":"code","d9b7bd46":"code","2373f715":"code","0b7ac25e":"code","8aa0775b":"code","25cb78e5":"code","8bfccb3a":"code","2b89fac0":"code","1777f063":"code","60149616":"code","16a19670":"code","581a258a":"code","eb8ea7f7":"code","cf5f5a21":"code","467d9230":"code","a4814d60":"code","b95e78d8":"code","f813996e":"code","8a90f677":"code","332ccddd":"code","f147d97e":"code","5826ff12":"code","09c70a56":"code","0678adf6":"code","6cdcb669":"code","b52aa9ce":"code","cc475eb8":"code","b198a96b":"code","fb826a7e":"code","703d116d":"code","ee019a7d":"code","fbe55751":"code","a120707c":"code","1b241506":"code","dfae8f36":"code","1108fcf9":"code","093103de":"code","5e034884":"code","1afaa053":"code","548592aa":"code","399d0637":"markdown","241f06ef":"markdown","b84c68c8":"markdown","c720715a":"markdown","3eb4a50f":"markdown","a0074544":"markdown","bcee523f":"markdown","a440c467":"markdown","e467d16a":"markdown","713780a0":"markdown","6dd4db22":"markdown","5b3fa9b1":"markdown","87d06b99":"markdown","bf51e7da":"markdown","f4dc9de8":"markdown","db72c709":"markdown","9e4ef51f":"markdown","e2afafad":"markdown","75c3c961":"markdown","70c6c63b":"markdown","cc0d6d2a":"markdown","d4375784":"markdown","891efb70":"markdown"},"source":{"08306da5":"import numpy as np\nimport pandas as pd","ae41538b":"loan_df = pd.read_csv('..\/input\/loan-data\/loan_data.csv')\nvariable_explain_df = pd.read_csv('..\/input\/loan-data-variableexp\/variable_explanation.csv',index_col=0)","12add36c":"loan_df.head()","dd2de13d":"variable_explain_df","d9b7bd46":"loan_df.info()","2373f715":"loan_df.describe()","0b7ac25e":"loan_df.isnull().value_counts()","8aa0775b":"loan_df.columns = [c.replace(\".\", \"_\") for c in loan_df.columns]\r\nloan_df.head()","25cb78e5":"variable_explain_df.head()","8bfccb3a":"variables = pd.DataFrame(columns=['Variable','Number of unique values','Values'])\r\n\r\nfor i, value in enumerate(loan_df.columns):\r\n    variables.loc[i] = [value, loan_df[value].nunique(), loan_df[value].unique().tolist()]\r\n\r\nvariables =  variables.set_index('Variable').join(variable_explain_df)\r\nvariables","2b89fac0":"import matplotlib.pyplot as plt\r\nimport seaborn as sns","1777f063":"#Find fico scores and purpose\r\nsns.histplot(x='fico',data= loan_df, kde=True,bins = 20)\r\nplt.xlabel('Fico scores')\r\nplt.ylabel('# of Customers')\r\nplt.title('Fico Scores of Customers')\r\nplt.show()","60149616":"fig=plt.gcf()\r\nfig.set_size_inches(15,8)\r\nsns.countplot(x='purpose',data=loan_df)\r\nplt.xlabel(\"Loan Purpose\")\r\nplt.xticks(rotation=45)\r\nplt.show()","16a19670":"counts_pur_fico = loan_df.groupby(['fico','purpose'],as_index=False)['int_rate'].count().reset_index()\r\ncounts_pur_fico.rename(columns = {'int_rate':'Counts'}, inplace = True)\r\ncounts_pur_fico.drop(columns='index',inplace=True)","581a258a":"counts_pur_fico['rank'] = counts_pur_fico['Counts'].rank(method='dense',ascending=False)\r\ncounts_pur_fico = counts_pur_fico[counts_pur_fico['rank'] <=5]","eb8ea7f7":"counts_pur_fico","cf5f5a21":"not_paid = loan_df['not_fully_paid'].value_counts().reset_index()\r\nprint(not_paid)","467d9230":"sns.countplot(x='not_fully_paid',data=loan_df)\r\nplt.ylabel('Counts')\r\nplt.xlabel('Paid\/Not-Paid')\r\nplt.title('Comparing Paid\/Not-Paid Customers')\r\nplt.show()","a4814d60":"cor_mat= loan_df.corr()\r\nfig=plt.gcf()\r\nfig.set_size_inches(12,12)\r\nsns.heatmap(data=cor_mat, square=True, annot=True, cbar=True,cmap=sns.diverging_palette(220, 20, as_cmap=True))\r\nplt.show()","b95e78d8":"from sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler,MinMaxScaler","f813996e":"loan_df.shape","8a90f677":"X = loan_df.iloc[:,:-1]\r\ny = loan_df.iloc[:,-1]\r\nX","332ccddd":"X_train, X_test,y_train,y_test = train_test_split(X,y,random_state=42,test_size=0.2)","f147d97e":"categorical_var = ['purpose']\r\n\r\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\r\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[categorical_var]))\r\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[categorical_var]))\r\n\r\n# One-hot encoding removed index; put it back\r\nOH_cols_train.index = X_train.index\r\nOH_cols_test.index = X_test.index\r\n\r\n# Remove categorical columns (will replace with one-hot encoding)\r\nnum_X_train = X_train.drop(categorical_var, axis=1)\r\nnum_X_test = X_test.drop(categorical_var, axis=1)\r\n\r\n# Add one-hot encoded columns to numerical features\r\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\r\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)","5826ff12":"OH_X_train.head()","09c70a56":"print(\"Shape of training Data - {}\".format(OH_X_train.shape))\r\nprint(\"Shape of test Data - {}\".format(OH_X_test.shape))","0678adf6":"minmax = MinMaxScaler()\r\nscaled_X_train = minmax.fit_transform(OH_X_train)\r\nscaled_X_test = minmax.transform(OH_X_test)","6cdcb669":"print(\"Shape of final X_train - {}\".format(scaled_X_train.shape))","b52aa9ce":"from sklearn.linear_model import LogisticRegression\r\n\r\nlogreg = LogisticRegression(verbose=True,n_jobs=-1)\r\nlogreg.fit(scaled_X_train,y_train)","cc475eb8":"# Import confusion_matrix\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.metrics import accuracy_score\r\n\r\n# Use logreg to predict instances from the test set and store it\r\ny_pred = logreg.predict(scaled_X_test)\r\n\r\n# Get the accuracy score of logreg model and print it\r\nprint(\"Accuracy of logistic regression classifier: \", logreg.score(scaled_X_test,y_test))\r\n\r\n# Print the confusion matrix of the logreg model\r\nprint(confusion_matrix(y_test,y_pred))","b198a96b":"from sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\r\nfrom sklearn.model_selection import cross_val_score\r\nfrom sklearn.model_selection import RepeatedStratifiedKFold\r\nfrom sklearn.feature_selection import RFE\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.linear_model import Perceptron\r\nfrom sklearn.model_selection import GridSearchCV","fb826a7e":"def get_models_rfe():\r\n    models = dict()\r\n    for i in range(2,19):\r\n        rfe = RFE(estimator=GradientBoostingClassifier(),n_features_to_select=i)\r\n        model = GradientBoostingClassifier()\r\n        models[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\r\n    return models","703d116d":"# evaluate a give model using cross-validation\r\ndef evaluate_model(model, X, y):\r\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\r\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)    \r\n    return scores","ee019a7d":"models_rfe = get_models_rfe()\r\nresults, names = list(),list()\r\nfor name, model in models_rfe.items():\r\n    scores = evaluate_model(model,scaled_X_train,y_train)\r\n    results.append(scores)\r\n    names.append(name)\r\n    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))","fbe55751":"# plot model performance for comparison\r\nplt.boxplot(results, labels=names, showmeans=True)\r\nplt.show()","a120707c":"# define RFE\r\nrfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\r\n# fit RFE\r\nrfe.fit(scaled_X_train, y_train)\r\n# summarize all features\r\nfor i in range(scaled_X_train.shape[1]):\r\n    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))","1b241506":"# get a list of models to evaluate\r\ndef get_models():\r\n    models = dict()\r\n    # lr\r\n    rfe = RFE(estimator=LogisticRegression(), n_features_to_select=5)\r\n    model = LogisticRegression()\r\n    models['lr'] = Pipeline(steps=[('s',rfe),('m',model)])\r\n    # perceptron\r\n    rfe = RFE(estimator=Perceptron(), n_features_to_select=5)\r\n    model = Perceptron()\r\n    models['per'] = Pipeline(steps=[('s',rfe),('m',model)])\r\n    # cart\r\n    rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\r\n    model = DecisionTreeClassifier()\r\n    models['cart'] = Pipeline(steps=[('s',rfe),('m',model)])\r\n    # rf\r\n    rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=5)\r\n    model = RandomForestClassifier()\r\n    models['rf'] = Pipeline(steps=[('s',rfe),('m',model)])\r\n    # gbm\r\n    rfe = RFE(estimator=GradientBoostingClassifier())\r\n    model = GradientBoostingClassifier()\r\n    models['gbm'] = Pipeline(steps=[('s',rfe),('m',model)])\r\n    return models\r\n","dfae8f36":"# get the models to evaluate\r\nmodels = get_models()\r\n# evaluate the models and store results\r\nresults, names = list(), list()\r\nfor name, model in models.items():\r\n    scores = evaluate_model(model, scaled_X_train, y_train)\r\n    results.append(scores)\r\n    names.append(name)\r\n    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))","1108fcf9":"tol = [0.01, 0.001,0.0001]\r\nmax_iter = [100,150,200]\r\nsolvers = ['newton-cg', 'lbfgs']\r\npenalty = ['l2']\r\nc_values = [100, 10, 1.0, 0.1, 0.01]\r\n\r\n# Create a dictionary where tol and max_iter are keys and the lists of their values are corresponding values\r\nparam_grid = dict(tol=tol,max_iter=max_iter,solver=solvers,penalty=penalty,C=c_values)\r\n\r\n# Instantiate GridSearchCV with the required parameters\r\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5,verbose=False,n_jobs=-1)\r\n\r\nscaled_X_train.shape","093103de":"scaled_X_train = scaled_X_train[:,[1,2,3,6,7]]\r\nscaled_X_test = scaled_X_test[:,[1,2,3,6,7]]\r\n\r\nscaled_X_train.shape","5e034884":"# Fit grid_model to the data\r\ngrid_model_result = grid_model.fit(scaled_X_train, y_train)\r\n\r\n# Summarize results\r\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\r\nprint(\"Best: %f using %s\" % (best_score, best_params))","1afaa053":"logreg = LogisticRegression(C=0.1,max_iter=100,penalty='l2',solver='newton-cg',tol=0.01,random_state=42)\r\nlogreg.fit(scaled_X_train,y_train)","548592aa":"# Use logreg to predict instances from the test set and store it\r\ny_pred = logreg.predict(scaled_X_test)\r\n\r\n# Get the accuracy score of logreg model and print it\r\nprint(\"Accuracy of logistic regression classifier: \", logreg.score(scaled_X_test,y_test))\r\n\r\n# Print the confusion matrix of the logreg model\r\nprint(confusion_matrix(y_test,y_pred))","399d0637":"### Finding Majority of Customer Population Fico Scores\nMajority of the customers lie between 670 & 710 fico scores","241f06ef":"Currently our model is giving a 84.13% accuracy. But surely we can do better using HyperParameter Tuning for the model. ","b84c68c8":"# Loan Data","c720715a":"## Context\nThis dataset ([source](https:\/\/www.kaggle.com\/itssuru\/loan-data)) consists of data from almost 10,000 borrowers that took loans - with some paid back and others still in progress. It was extracted from lendingclub.com which is an organization that connects borrowers with investors. We've included a few suggested questions at the end of this template to help you get started.","3eb4a50f":"Convert the categorical variables using OneHotEncoder. Since models work better and fast with numerical data","a0074544":"Majority of the customers lie between 670 & 710 fico scores","bcee523f":"#### Plot top 5 reasons purpose and Fico","a440c467":"### Correlation matrix with paying back the loan","e467d16a":"### Finding null value counts","713780a0":"### Getting to know the loan file data","6dd4db22":"### Number of customer that have paid vs not Paid","5b3fa9b1":"### Find the prime reason for getting the loan\n\nFrom the below graph it is evident that the debit consoliation is the main reason and it is mainly with customers having Fico score less than 710","87d06b99":"### Finding the best performing model\n<p>We have defined the grid of hyperparameter values and converted them into a single dictionary format which <code>GridSearchCV()<\/code> expects as one of its parameters. Now, we will begin the grid search to see which values perform best.<\/p>\n<p>We will instantiate <code>GridSearchCV()<\/code> with our earlier <code>logreg<\/code> model with all the data we have. Instead of passing train and test sets separately, we will supply <code>X<\/code> (scaled version) and <code>y<\/code>. We will also instruct <code>GridSearchCV()<\/code> to perform a <a href=\"https:\/\/www.dataschool.io\/machine-learning-with-scikit-learn\/\">cross-validation<\/a> of five folds.<\/p>\n<p>We'll end the notebook by storing the best-achieved score and the respective best parameters.<\/p>","bf51e7da":"## EDA with Visuals","f4dc9de8":"## Working with Features & Model ","db72c709":"## Understanding your variables","9e4ef51f":"Building a baseline model in order to iterate and improve over time. From the above analysis we have noticed that 1533 customers have yet not paid fully.","e2afafad":"## Loading Data","75c3c961":"## Summary\r\n- Build a model that can predict the probability a user will be able to pay back their loan within a certain period. 84%\r\n- We see that majority of the people take loans for debt consolidation and the fico scores of them is less than 710\r\n- Using RFE we can say that only 5 columns are required to give the optimum output.\r\n","70c6c63b":"No null values detected\n\nReplacing '.' with '_' in the column names","cc0d6d2a":"### HyperParameter Tuning ","d4375784":"The main reason of borrowing money is debt consolidation and it is majorly in the customers having Fico score less than 710","891efb70":"### Final Prediction using the tuned parameters\r\n\r\nThe best parameters we found for log reg. "}}