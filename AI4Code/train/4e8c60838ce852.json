{"cell_type":{"391c2414":"code","35de684b":"code","a954f73c":"code","617b4bb8":"code","aad8264c":"code","31a02ace":"code","6e0e713a":"code","89c15ef9":"code","fd4cfed8":"code","1546f17a":"code","685ffea1":"code","2b0ccda9":"code","92024b5e":"code","ff29d8ce":"code","ea407e54":"code","a62f22db":"code","1d614f14":"code","b22e014e":"code","cbefddd4":"code","62428335":"code","ebbba6a3":"code","6405207f":"code","466f0510":"code","e14d2abe":"code","72b8312a":"code","29286163":"code","e472fffb":"code","32e0d773":"code","23220baf":"code","dd0ac8bf":"code","63272fdf":"code","58799461":"code","4337d0d6":"code","16e9f787":"code","ade60b68":"code","2fdaaea1":"code","aaad5b7c":"code","0acda5ef":"code","c9b91cf4":"code","1a870351":"code","5c371503":"code","20688699":"code","ce747cbf":"code","3b44deea":"code","e6dbdba0":"code","5c84cb9f":"code","b5961ef7":"code","a5e4fb32":"code","64117a03":"code","4b6995d4":"code","c5260b32":"code","f7b23e58":"code","8974420f":"code","6bf653da":"markdown","14314752":"markdown","4682e3c2":"markdown","42f40f61":"markdown","4698b543":"markdown","5645ce5a":"markdown","75562b1e":"markdown","10b16ded":"markdown","d7b494fd":"markdown","46391f43":"markdown","4dd76d49":"markdown","1ad46c9d":"markdown","064dc5d4":"markdown","3db1ba0e":"markdown","586bd20f":"markdown"},"source":{"391c2414":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35de684b":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest =pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","a954f73c":"train.head()","617b4bb8":"train.describe(include='all')","aad8264c":"train.info()","31a02ace":"test.head()","6e0e713a":"test.describe(include='all')","89c15ef9":"test.info()","fd4cfed8":"test.info()","1546f17a":"import matplotlib.pyplot as plt\n%matplotlib inline\ntrain.hist(figsize=(20,30))","685ffea1":"import seaborn as sns\nsns.boxplot(x = \"YrSold\", y= \"SalePrice\",data=train)","2b0ccda9":"sns.distplot(train['SalePrice'],norm_hist=True);\n","92024b5e":"#sns.pairplot(data)","ff29d8ce":"import seaborn as sns\nsns.boxplot(x = \"MSZoning\", y= \"SalePrice\",data=train)","ea407e54":"sns.distplot(train['SalePrice'])\n","a62f22db":"sns.distplot(train['YearBuilt'])","1d614f14":"sns.distplot(train['LotArea'],color=\"blue\", label=\"LotArea\")\n","b22e014e":"corr =train.corr()","cbefddd4":"corr","62428335":"corr[corr['SalePrice']>0.3].index","ebbba6a3":"train = train[['LotFrontage', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n       'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n       'FullBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars',\n       'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'SalePrice']]\ntest=test[['LotFrontage', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n       'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n       'FullBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars',\n       'GarageArea', 'WoodDeckSF', 'OpenPorchSF']]","6405207f":"print(train.info())\nprint(\"#\"*100)\nprint(test.info())","466f0510":"threshold = 0.7\n#Dropping columns with missing value rate higher than threshold\ntrain = train[train.columns[train.isnull().mean() < threshold]]\n#Dropping rows with missing value rate higher than threshold\ntrain = train.loc[train.isnull().mean(axis=1) < threshold]\n\n","e14d2abe":"#Filling missing values with medians of the columns\ntrain = train.fillna(train.median())","72b8312a":"threshold = 0.7\n#Dropping columns with missing value rate higher than threshold\ntest = test[test.columns[test.isnull().mean() < threshold]]\n#Dropping rows with missing value rate higher than threshold\ntest = test.loc[test.isnull().mean(axis=1) < threshold]\n\n","29286163":"#Filling missing values with medians of the columns\ntest = test.fillna(test.median())","e472fffb":"train.info()","32e0d773":"test.describe()","23220baf":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train.drop('SalePrice', axis=1), train['SalePrice'], test_size=0.3, random_state=101)","dd0ac8bf":"y_train= y_train.values.reshape(-1,1)\ny_test= y_test.values.reshape(-1,1)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.fit_transform(X_test)\ny_train = sc_X.fit_transform(y_train)\ny_test = sc_y.fit_transform(y_test)","63272fdf":"print(X_train)","58799461":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators = 100, random_state = 0)\nrfr.fit(X_train, y_train)","4337d0d6":"rfr_pred= rfr.predict(test)\nrfr_pred = rfr_pred.reshape(-1,1)","16e9f787":"rfr_pred.shape","ade60b68":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error","2fdaaea1":"print('MAE:', mean_absolute_error(y_test, rfr_pred))\nprint('MSE:', mean_squared_error(y_test, rfr_pred))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, rfr_pred)))","aaad5b7c":"plt.figure(figsize=(16,8))\nplt.plot(y_test,label ='Test')\nplt.plot(rfr_pred, label = 'predict')\nplt.show()","0acda5ef":"a = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","c9b91cf4":"a","1a870351":"test_id = a['Id']\na = pd.DataFrame(test_id, columns=['Id'])","5c371503":"test = sc_X.fit_transform(test)","20688699":"test.shape","ce747cbf":"a","3b44deea":"test.shape","e6dbdba0":"X_test.shape","5c84cb9f":"rfr_pred= rfr.predict(test)\nrfr_pred = rfr_pred.reshape(-1,1)","b5961ef7":"rfr_pred.shape","a5e4fb32":"test_prediction_rfr =sc_y.inverse_transform(rfr_pred)","64117a03":"test_prediction_rfr = pd.DataFrame(test_prediction_rfr, columns=['SalePrice'])","4b6995d4":"test_prediction_rfr","c5260b32":"result = pd.concat([a,test_prediction_rfr], axis=1)","f7b23e58":"result","8974420f":"result.to_csv('submission.csv',index=False)","6bf653da":"from sklearn import svm\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nimport pandas as pd\n \n    # model = RandomForestRegressor(n_estimators=150, max_features='sqrt', n_jobs=-1)  # Random Forest\nmodels = [LinearRegression(),\n              RandomForestRegressor(n_estimators=100, max_features='sqrt'),\n              KNeighborsRegressor(n_neighbors=6),\n              SVR(kernel='linear'),\n              LogisticRegression()\n              ]\n \nTestModels = pd.DataFrame()\ntmp = {}\n \nfor model in models:\n        # get model name\n        m = str(model)\n        tmp['Model'] = m[:m.index('(')]\n        # fit model on training dataset\n        model.fit(X_train, y_train['SalePrice'])\n        # predict prices for test dataset and calculate r^2\n        tmp['R2_SalePrice'] = r2_score(y_test['SalePrice'], model.predict(X_test))\n        # write obtained data\nTestModels = TestModels.append([tmp])\n \nTestModels.set_index('Model', inplace=True)\n \nfig, axes = plt.subplots(ncols=1, figsize=(10, 4))\nTestModels.R2_Price.plot(ax=axes, kind='bar', title='R2_SalePrice')\nplt.show()","14314752":"# Visualising the data","4682e3c2":"# Data Cleaning for test Data","42f40f61":"## Let's remove the redundant columns from the dataset and keep those which are important","4698b543":"# Test Data","5645ce5a":"# Let's start this with exploring the dataset","75562b1e":"# Plot the predicted and actual","10b16ded":"error_rate = np.array(mean_squared_error(y_test, rfr_pred))","d7b494fd":"# Choose a regression algorithm","46391f43":"# Using Random Forest to build the model","4dd76d49":"## Let's drop the Column & Rows with missing data more than 70%","1ad46c9d":"Data Scaling","064dc5d4":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","3db1ba0e":"# *You can get the feature importance of each feature of your dataset by using the feature importance property of the model.Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.Feature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset.*","586bd20f":"# Let's get down to Feature Engineering Now"}}