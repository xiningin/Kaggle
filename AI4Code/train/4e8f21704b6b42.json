{"cell_type":{"8280075d":"code","048c6d0c":"code","e591c902":"code","a091d8a9":"code","15af0af9":"code","8b137f96":"code","2b040a2a":"code","210c4a21":"code","53c72d30":"code","bebaa522":"code","8652a252":"code","949754d3":"code","285355f8":"code","91e27cf7":"code","da7b96ff":"code","e3cc28ad":"code","9124b1ba":"code","d195bcce":"code","1e8fc4f3":"code","546a2ac6":"code","e7a74f64":"code","f5c58269":"code","4cff4347":"code","773549f8":"code","90c863a3":"code","8ff4092b":"code","3178b3c3":"code","9a526bdd":"code","570cf90a":"markdown","9b2b0a06":"markdown","bdaca54b":"markdown","71a5b84e":"markdown","39558dcc":"markdown","b35eaebc":"markdown","d881e31e":"markdown","9c5748dc":"markdown","1bc06a08":"markdown","dc85325c":"markdown","555a75b3":"markdown","0fc1857a":"markdown"},"source":{"8280075d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\n\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.options.mode.chained_assignment = None\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","048c6d0c":"df = pd.read_csv(\"..\/input\/insurance\/insurance.csv\")","e591c902":"df.head()","a091d8a9":"df.describe()","15af0af9":"sns.set_theme(style=\"darkgrid\")\nax= plt.figure(figsize=(20,6))\nsns.histplot(df['age'], bins=100,color='red')\nplt.title(\"Age Distribution\")","8b137f96":"ax= plt.figure(figsize=(20,6))\nsns.histplot(df['bmi'], bins=100,color='blue')\nplt.title(\"BMI Distribution\")","2b040a2a":"ax= plt.figure(figsize=(6,8))\nax = sns.countplot(x=\"smoker\", data=df)\nplt.title(\"Smoker Count\")","210c4a21":"plt.figure(figsize=(12,8))\nsns.violinplot(x='smoker',y=\"charges\", hue='sex', data=df, split='True', palette='rainbow')\nplt.title(\"Charges for smokers\/nonsmokers\")","53c72d30":"plt.figure(figsize=(12,8))\nsns.lmplot(x='age', y='charges', hue='smoker', data=df, height=15)\nplt.title(\"Charges for smokers\/nonsmokers by age\")","bebaa522":"plt.figure(figsize=(12,8))\nsns.lmplot(x='bmi', y='charges', hue='smoker', data=df, height=15)\nplt.title(\"Charges for smokers\/nonsmokers by BMI\")","8652a252":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# We specify random seed so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\ndf_train, df_test = train_test_split(df, train_size = 0.7, test_size = 0.3, random_state = 100)\n\nscaler = MinMaxScaler()\nnum_vars = ['age', 'bmi', 'children']\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\ndf_test[num_vars] = scaler.fit_transform(df_test[num_vars])\n\n# binarize the binary variables\ndf_train['smoker'] = df_train.smoker.eq('yes').mul(1)\ndf_train['sex'] = df_train.sex.eq('female').mul(1)\ndf_test['smoker'] = df_test.smoker.eq('yes').mul(1)\ndf_test['sex'] = df_test.sex.eq('female').mul(1)\n\ndf_train = pd.get_dummies(df_train)\ndf_test = pd.get_dummies(df_test)\ndf = pd.get_dummies(df)","949754d3":"# looking at correlations between variables\nplt.figure(figsize=(12,10))\ncor = df_train.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.title(\"Correlations of each variable with one another\")\nplt.show()","285355f8":"# use backward elimination with p value < 0.05 to find the variabels that will help predict model\n#Adding constant column of ones, mandatory for sm.OLS model\ndf_1 = sm.add_constant(df_train)\ny = df_train['charges']\ndf = df_train.drop(columns='charges')\n#Fitting sm.OLS model\nmodel = sm.OLS(y,df_1).fit()\nmodel.pvalues","91e27cf7":"cols = list(df.columns)\npmax = 1\nwhile (len(cols)>0):\n    p = []\n    df_1 = df[cols]\n    df_1 = sm.add_constant(df_1)\n    model = sm.OLS(y,df_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","da7b96ff":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\n# separating the data out\n\nfeatured_df = df.loc[:, selected_features_BE]\nX_test = df_test[selected_features_BE].values\nY_test = df_test['charges'].values.reshape(-1, 1)\nX = featured_df.iloc[:, :].values\nY = y.iloc[:].values.reshape(-1, 1)","e3cc28ad":"def getOptimalNearestNeighbour(X, Y, X_test, Y_test):\n    n_neighbors = []\n    error = []\n    for i in range(30):\n        if i == 0:\n            continue\n        neigh = KNeighborsRegressor(n_neighbors=i)\n        neigh.fit(X, Y)\n        predictions_neigh = neigh.predict(X_test)\n        mean_square_error = mean_squared_error(predictions_neigh, Y_test, squared=False)\n        n_neighbors.append(i)\n        error.append(mean_square_error)\n    return pd.DataFrame(list(zip(n_neighbors, error)), columns=['n_neighbors', 'error'])","9124b1ba":"def getOptimalMaxDepth(X, Y, X_test, Y_test):\n    max_depth = []\n    error = []\n    for i in range(30):\n        if i == 0:\n            continue\n        forest = RandomForestRegressor(max_depth=i, random_state=0)\n        forest.fit(X, Y)\n        prediction = forest.predict(X_test)\n        mean_square_error = mean_squared_error(prediction, Y_test, squared=False)\n        max_depth.append(i)\n        error.append(mean_square_error)\n    return pd.DataFrame(list(zip(max_depth, error)), columns=['max depth', 'error'])","d195bcce":"maxdepth_df = getOptimalMaxDepth(X, Y, X_test, Y_test)\nideal_max_depth = maxdepth_df.idxmin()['error'] + 1\nsns.lineplot(x='max depth', y='error', data=maxdepth_df)\nplt.title('Mean Non-squared Error by nth Neighbor')","1e8fc4f3":"neighbor_df = getOptimalNearestNeighbour(X, Y, X_test, Y_test)\nsns.lineplot(x='n_neighbors', y='error', data=neighbor_df)\nplt.title('Mean Non-squared Error by nth Neighbor')","546a2ac6":"# building neighbor model based on 10 neighbors\nneigh = KNeighborsRegressor(n_neighbors=10)\nneigh.fit(X, Y)\npredictions_neigh = neigh.predict(X_test)\nboth_arrays_neigh = np.append(Y_test, predictions_neigh, axis=1)\npre_act_neigh = pd.DataFrame(both_arrays_neigh, columns=['Actual', 'Predicted'])\n\nlinear_regressor = LinearRegression()\nlinear_regressor.fit(X, Y)\npredictions_linear = linear_regressor.predict(X_test)\nboth_arrays_linear = np.append(Y_test, predictions_linear, axis=1)\npre_act_linear = pd.DataFrame(both_arrays_linear, columns=[\"Actual\", \"Predicted\"])\n\nforest_regressor = RandomForestRegressor(max_depth=ideal_max_depth, random_state=0)\nforest_regressor.fit(X, Y)\npredictions_forest = forest_regressor.predict(X_test)\npredictions_forest = predictions_forest.reshape((402, 1))\nboth_arrays_forest = np.append(Y_test, predictions_forest, axis=1)\npre_act_forest = pd.DataFrame(both_arrays_forest, columns=[\"Actual\", \"Predicted\"])","e7a74f64":"pre_act_forest.head()","f5c58269":"coef = []\nfor number in linear_regressor.coef_:\n    for i in number:\n        coef.append(i)\n        \nprint('COEFFICIENTS OF EACH VARIABLE IN LINEAR REGRESSOR:')\nprint(list(zip(selected_features_BE, coef)))","4cff4347":"sns.lmplot(x='Actual', y='Predicted', data=pre_act_linear, height=10, scatter_kws={'color': 'green'})\nplt.title(\"Actual vs Predicted Charges using Linear Regression\")","773549f8":"mean_squareroot_error_linear = mean_squared_error(predictions_linear, Y_test, squared=False)\nprint(mean_squareroot_error_linear)","90c863a3":"sns.lmplot(x='Actual', y='Predicted', data=pre_act_neigh, height=10, scatter_kws={'color': 'orange'})\nplt.title(\"Actual vs Predicted Charges using k-nearest-neighbor\")","8ff4092b":"mean_squareroot_error_neigh = mean_squared_error(predictions_neigh, Y_test, squared=False)\nprint(mean_squareroot_error_neigh)","3178b3c3":"sns.lmplot(x='Actual', y='Predicted', data=pre_act_forest, height=10, scatter_kws={'color': 'black'})\nplt.title(\"Actual vs Predicted Charges using random forest regressor\")","9a526bdd":"mean_squareroot_error_forest = mean_squared_error(predictions_forest, Y_test, squared=False)\nprint(mean_squareroot_error_forest)","570cf90a":"# Regression model","9b2b0a06":"# Plotting our models (ideally want to see x=y slope for all datapoints)","bdaca54b":"# **From the heatmap we can see the variables that correlate with \"charges\" the most is:**\n- **Smoker (yes\/no)**\n- **Age**\n- **BMI**","71a5b84e":"Mean_nonsquared_regression for nearest neighbor model","39558dcc":"# Manipulating\/preprocessing for model","b35eaebc":"# Correlation analysis","d881e31e":"# Coefficients","9c5748dc":"# Exploring the data","1bc06a08":"# Feature Selection","dc85325c":"# Iteratively remove highest p-value of variables","555a75b3":"Mean_nonsquared_regression for linear model","0fc1857a":"For the k-nearest-neighbor model, I want to find the optimal n_neighbor that will minimize mean squared error"}}