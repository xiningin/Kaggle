{"cell_type":{"4ad70adb":"code","33e9c97b":"code","911e3dd2":"code","2c61b215":"code","2614e32b":"code","42076343":"code","b229a2fd":"code","cfab2b0f":"code","2cc6cbf4":"code","eb9bc5d1":"code","c3580ddc":"code","205070ff":"code","6e593ea1":"code","04799c07":"code","43e95ff1":"code","43af8a50":"code","9644abf5":"code","7a5aa55e":"code","9f7d9ff8":"code","f5123bf3":"code","9ba868bc":"markdown","6c59004b":"markdown","0ae9a63b":"markdown","c8907a94":"markdown","16273ef3":"markdown","103530fb":"markdown","26cfeef3":"markdown","77e70b42":"markdown","37e1e309":"markdown","b2283411":"markdown","f3aac39e":"markdown","4ffefd73":"markdown","cad8c14f":"markdown","1a987602":"markdown","2bfe43fc":"markdown"},"source":{"4ad70adb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","33e9c97b":"full_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nfull_df.describe(include = 'all')","911e3dd2":"import seaborn as sns\ncorr = full_df.corr()\nprint(len(corr))\nf, ax = plt.subplots(figsize=(12, 10))\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(corr, annot=False, mask = mask, cmap=cmap)","2c61b215":"from sklearn.model_selection import train_test_split\n\n# 'SalePrice' is the target variable, the price of the house\n# Other columns related to the SalePrice should be removed for data leakage\n# Id should be removed as it's only labelling, it carries no importance on price\n\ny_data = full_df['SalePrice']\nX_data = full_df.drop(['Id'], axis=1)","2614e32b":"# One-Hot encoding of all the class variables\n\nvariables = list(X_data.columns)\nnumerical_variables = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1','BsmtFinSF2',\n                       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', 'ndFlrSF', 'LowQualFinSF','GrLivArea', 'BsmtFullBath',\n                       'BsmtHalf', 'Bath', 'FullBath', 'HalfBath','Bedroom', 'Kitchen', 'TotRmsAbvGrd', 'Fireplaces',\n                       'GarageYrBlt', 'GarageCars','GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch',\n                       '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'BedroomAbvGr']\nclass_variables = list(set(variables) - set(numerical_variables))\n\nX_data = pd.get_dummies(X_data,columns = class_variables)\n\n# Remove the features from debugging\nX_data.drop(['GarageYrBlt', 'GarageArea', 'GarageCars'], axis = 1, inplace = True)\n\n# Cast type, and fill some NaN values found in debugging\nX_data = X_data.astype(np.float64)\nX_data.fillna({'LotFrontage':0, 'MasVnrArea':0}, inplace=True)\n\n# Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=173)\nprint(full_df.shape, X_train.shape, X_test.shape, y_train.shape, y_test.shape)","42076343":"from sklearn.ensemble import RandomForestRegressor\n\n# Firstly, we'll use default parameters except 'bootsrap'.\n# We'll set it to 'False' so that each tree is built from the\n# whole dataset.\n\nrfr_feature = RandomForestRegressor(n_estimators = 100,\n                           max_depth = None,\n                           min_samples_split = 2,\n                           min_samples_leaf = 1,\n                           max_features = \"auto\",\n                           max_leaf_nodes = None,\n                           bootstrap = False,\n                           n_jobs = -1)\n\nrfr_feature.fit(X_train, y_train)","b229a2fd":"# Check feature Importances with a plot\nfeature_names = list(X_data.columns)\nweights = rfr_feature.feature_importances_\n\n# The above were once plotted without any of the adjustments below, just to get a feel for the data.\n\nimportant_weights = []\nimportant_feature_names = []\nindex = 0\nfor item in weights:\n    if item >= 0.005:  # the minimum value for importance we want to consider\n        important_weights.append(item)\n        important_feature_names.append(feature_names[index])\n    index += 1\n\nplt.figure(figsize=(6, 4))\nplt.bar(important_feature_names, important_weights)\nplt.xticks(rotation=45)\nplt.show()","cfab2b0f":"important_feature_names","2cc6cbf4":"# selection of features as chosen from feature_importances_ investigation\n\nX_data2 = full_df[['LotArea','YearRemodAdd','MasVnrArea',\n                   'TotalBsmtSF','1stFlrSF', 'GrLivArea', 'FullBath',\n                   'WoodDeckSF','OpenPorchSF','ExterQual','GarageQual',\n                   'LandSlope','BedroomAbvGr','KitchenQual']]\n\nX_data2 = pd.get_dummies(X_data2,columns = ['ExterQual','GarageQual','LandSlope','KitchenQual'])\n\n\n# Cast type, and fill some NaN values found in debugging\nX_data2 = X_data2.astype(np.float64)\nX_data2.fillna({'MasVnrArea':0}, inplace=True)\n\n# Splitting the data\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_data2, y_data, random_state=173)\nprint(full_df.shape, X_train2.shape, X_test2.shape, y_train2.shape, y_test2.shape)","eb9bc5d1":"from sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import GridSearchCV\n\nrfr = RandomForestRegressor()\n\nrfr.fit(X_train2, y_train2)\n\nrfr_predictions = rfr.predict(X_test2)\n\nrfr_score = mean_squared_log_error(rfr_predictions, y_test2)\n\nprint(rfr_score)\n\n# Now a tuned model\n\nrfr_params = {'n_estimators':[100, 200, 400],\n             }\n\ngrid_rfr = GridSearchCV(rfr, param_grid = rfr_params,\n                       scoring = 'neg_mean_squared_error')\n\ngrid_rfr.fit(X_train2, y_train2)\n\ngrid_rfr_predictions = grid_rfr.predict(X_test2)\n\ngrid_rfr_score = mean_squared_log_error(grid_rfr_predictions, y_test2)\n\nprint(grid_rfr_score, '\\n', grid_rfr.best_params_)\n\n# {'n_estimators': 200}","c3580ddc":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n\nlasr = Lasso()\n\nlasr.fit(X_train2, y_train2)\n\nlasr_predictions = lasr.predict(X_test2)\n\nlasr_score = mean_squared_log_error(lasr_predictions, y_test2)\n\nprint(lasr_score)\n\n# Now a tuned model\n\nlas_params = {'alpha':[0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10],\n               'fit_intercept':[True], 'normalize':[True],\n               'max_iter':[100000], 'tol':[0.00001],'selection':['random']}\n\ngrid_las = GridSearchCV(lasr, param_grid = las_params,\n                       scoring = 'neg_mean_squared_error')\n\ngrid_las.fit(X_train2, y_train2)\n\ngrid_las_predictions = grid_las.predict(X_test2)\n\ngrid_las_score = mean_squared_log_error(grid_las_predictions, y_test2)\n\nprint(grid_las_score, '\\n', grid_las.best_params_)\n\n# {'alpha': 10, 'fit_intercept': True, 'max_iter': 100000,\n# 'normalize': True, 'selection': 'random', 'tol': 1e-05}","205070ff":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr = GradientBoostingRegressor()\n\ngbr.fit(X_train2, y_train2)\n\ngbr_predictions = gbr.predict(X_test2)\n\ngbr_score = mean_squared_log_error(gbr_predictions, y_test2)\n\nprint(gbr_score)\n\ngbr_params = {'n_estimators':[10, 100, 200],\n              'learning_rate':[0.01, 0.03, 0.1, 0.3, 1.0, 3],\n             'criterion':['mse']}\n\ngrid_gbr = GridSearchCV(gbr, param_grid = gbr_params,\n                       scoring = 'neg_mean_squared_error')\n\ngrid_gbr.fit(X_train2, y_train2)\n\ngrid_gbr_predictions = grid_gbr.predict(X_test2)\n\ngrid_gbr_score = mean_squared_log_error(grid_gbr_predictions, y_test2)\n\nprint(grid_gbr_score, '\\n', grid_gbr.best_params_)\n\n# {'criterion': 'mse', 'learning_rate': 0.1, 'n_estimators': 200}","6e593ea1":"X_data3 = full_df[['LotArea','YearRemodAdd','MasVnrArea','TotalBsmtSF','1stFlrSF',\n                        'GrLivArea','FullBath','WoodDeckSF','OpenPorchSF',\n                        'ExterQual','GarageQual','LandSlope',\n                        'BedroomAbvGr','KitchenQual']]\n\n# combining SF features into one\nX_data3['SF'] = X_data3['LotArea'] + X_data3['TotalBsmtSF'] + X_data3['1stFlrSF'] + X_data3['GrLivArea']+ X_data3['WoodDeckSF'] + X_data3['OpenPorchSF']\n\nX_data3.drop(['LotArea','TotalBsmtSF','1stFlrSF',\n              'GrLivArea','WoodDeckSF','OpenPorchSF'], axis = 1, inplace=True)\n\nX_data3 = pd.get_dummies(X_data3,columns = ['ExterQual','GarageQual','LandSlope','KitchenQual'])\n\n\n# Cast type, and fill some NaN values found in debugging\nX_data3 = X_data3.astype(np.float64)\nX_data3.fillna({'MasVnrArea':0}, inplace=True)\n\n# Splitting the data\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X_data3, y_data, random_state=173)\nprint(full_df.shape, X_train3.shape, X_test3.shape, y_train3.shape, y_test3.shape)","04799c07":"rfr2 = RandomForestRegressor()\n\nrfr2.fit(X_train3, y_train3)\n\nrfr2_predictions = rfr2.predict(X_test3)\n\nrfr2_score = mean_squared_log_error(rfr2_predictions, y_test3)\n\nprint(rfr2_score)\n\n# Now a tuned model\n\nrfr2_params = {'n_estimators':[100, 200, 400],\n             }\n\ngrid_rfr2 = GridSearchCV(rfr2, param_grid = rfr2_params,\n                       scoring = 'neg_mean_squared_error')\n\ngrid_rfr2.fit(X_train3, y_train3)\n\ngrid_rfr2_predictions = grid_rfr2.predict(X_test3)\n\ngrid_rfr2_score = mean_squared_log_error(grid_rfr2_predictions, y_test3)\n\nprint(grid_rfr2_score, '\\n', grid_rfr2.best_params_)\n\n# {'n_estimators': 100}","43e95ff1":"lasr2 = Lasso()\n\nlasr2.fit(X_train3, y_train3)\n\nlasr2_predictions = lasr2.predict(X_test3)\n\nlasr2_score = mean_squared_log_error(lasr2_predictions, y_test3)\n\nprint(lasr2_score)\n\n# Now a tuned model\n\nlas2_params = {'alpha':[0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10],\n               'fit_intercept':[True], 'normalize':[True],\n               'max_iter':[100000], 'tol':[0.00001],'selection':['random']}\n\ngrid_las2 = GridSearchCV(lasr2, param_grid = las2_params,\n                       scoring = 'neg_mean_squared_error')\n\ngrid_las2.fit(X_train3, y_train3)\n\ngrid_las2_predictions = grid_las2.predict(X_test3)\n\ngrid_las2_score = mean_squared_log_error(grid_las2_predictions, y_test3)\n\nprint(grid_las_score, '\\n', grid_las.best_params_)\n\n# {'alpha': 10}","43af8a50":"gbr2 = GradientBoostingRegressor()\n\ngbr2.fit(X_train3, y_train3)\n\ngbr2_predictions = gbr2.predict(X_test3)\n\ngbr2_score = mean_squared_log_error(gbr2_predictions, y_test3)\n\nprint(gbr2_score)\n\ngbr2_params = {'n_estimators':[10, 100, 200],\n              'learning_rate':[0.01, 0.03, 0.1, 0.3, 1.0, 3],\n             'criterion':['mse']}\n\ngrid_gbr2 = GridSearchCV(gbr2, param_grid = gbr2_params,\n                       scoring = 'neg_mean_squared_error')\n\ngrid_gbr2.fit(X_train3, y_train3)\n\ngrid_gbr2_predictions = grid_gbr2.predict(X_test3)\n\ngrid_gbr2_score = mean_squared_log_error(grid_gbr2_predictions, y_test3)\n\nprint(grid_gbr2_score, '\\n', grid_gbr2.best_params_)\n\n# {'criterion': 'mse', 'learning_rate': 0.03, 'n_estimators': 200}","9644abf5":"X_data4 = full_df[['LotArea','YearRemodAdd','MasVnrArea','TotalBsmtSF','1stFlrSF',\n                        'GrLivArea','FullBath','WoodDeckSF','OpenPorchSF',\n                        'ExterQual','GarageQual','LandSlope',\n                        'BedroomAbvGr','KitchenQual']]\n\nX_data4 = pd.get_dummies(X_data4,columns = ['ExterQual','GarageQual','LandSlope','KitchenQual'])\n\n# Cast type, and fill some NaN values found in debugging\nX_data4 = X_data4.astype(np.float64)\nX_data4.fillna({'MasVnrArea':0}, inplace=True)\n\n# Splitting the data\nX_train4, X_test4, y_train4, y_test4 = train_test_split(X_data4, y_data, random_state=173)\nprint(full_df.shape, X_train4.shape, X_test4.shape, y_train4.shape, y_test4.shape)","7a5aa55e":"gbr_final = GradientBoostingRegressor()\n\n\ngbr_final_params = {'n_estimators':[200,  300, 400],\n                    'learning_rate':[0.03, 0.1, 0.3, 0.5],\n                   'min_samples_split':[2], 'min_samples_leaf':[2],\n                    'criterion':['friedman_mse']}\n\ngrid_gbr_final = GridSearchCV(gbr_final, param_grid = gbr_final_params,\n                       scoring = 'neg_mean_squared_error')\n\ngrid_gbr_final.fit(X_train4, y_train4)\n\ngrid_gbr_final_predictions = grid_gbr_final.predict(X_test4)\n\ngrid_gbr_final_score = mean_squared_log_error(grid_gbr_final_predictions, y_test4)\n\nprint(grid_gbr_final_score, '\\n', grid_gbr_final.best_params_)\n\n# {'criterion': 'friedman_mse', 'learning_rate': 0.03,\n# 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 400}\n## X_train4: 0.0281712\n\n#{'criterion': 'friedman_mse', 'learning_rate': 0.1,\n# 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 300}\n## X_train4: 0.0268932","9f7d9ff8":"gbr_final4 = GradientBoostingRegressor(criterion = 'friedman_mse', learning_rate = 0.03,\n                                     min_samples_leaf = 2, min_samples_split = 2,\n                                     n_estimators = 300)\n\ngbr_final4.fit(X_train4, y_train4)\ngbr_final4_predictions = gbr_final4.predict(X_test4)\ngbr_final4_score = mean_squared_log_error(gbr_final4_predictions, y_test4)\nprint(\"4\", gbr_final4_score)\n      \n    \ngbr_final3 = GradientBoostingRegressor(criterion = 'friedman_mse', learning_rate = 0.03,\n                                     min_samples_leaf = 2, min_samples_split = 2,\n                                     n_estimators = 300)\n\ngbr_final3.fit(X_train3, y_train3)\ngbr_final3_predictions = gbr_final3.predict(X_test3)\ngbr_final3_score = mean_squared_log_error(gbr_final3_predictions, y_test3)\nprint(\"3\", gbr_final3_score)\n\n\ngbr_final2 = GradientBoostingRegressor(criterion = 'friedman_mse', learning_rate = 0.03,\n                                     min_samples_leaf = 2, min_samples_split = 2,\n                                     n_estimators = 300)\n\ngbr_final2.fit(X_train2, y_train2)\ngbr_final2_predictions = gbr_final2.predict(X_test2)\ngbr_final2_score = mean_squared_log_error(gbr_final2_predictions, y_test2)\nprint(\"2\", gbr_final2_score)\n\n\ngbr_final = GradientBoostingRegressor(criterion = 'friedman_mse', learning_rate = 0.03,\n                                     min_samples_leaf = 2, min_samples_split = 2,\n                                     n_estimators = 300)\n\ngbr_final.fit(X_train, y_train)\ngbr_final_predictions = gbr_final.predict(X_test)\ngbr_final_score = mean_squared_log_error(gbr_final_predictions, y_test)\nprint(\"1\", gbr_final_score)","f5123bf3":"test_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest_id = test_df['Id']\ntest_df.drop('Id', axis=1, inplace=True)\n\n########################################\n\ntest_4 = test_df[['LotArea','YearRemodAdd','MasVnrArea','TotalBsmtSF','1stFlrSF',\n                        'GrLivArea','FullBath','WoodDeckSF','OpenPorchSF',\n                        'ExterQual','GarageQual','LandSlope',\n                        'BedroomAbvGr','KitchenQual']]\n\ntest_4 = pd.get_dummies(test_4,columns = ['ExterQual','GarageQual','LandSlope','KitchenQual'])\n\n#Found in debugging, no observation of 'Ex' in GarageQual in test_df\ntest_4['GarageQual_Ex'] = 0\n\n# Cast type, and fill some NaN values found in debugging\ntest_4 = test_4.astype(np.float64)\n\n# Found in debugging, reordering columns to match training\ntest_4 = test_4.reindex(columns= list(X_train4.columns))\n\n# replace all infinity values with nan\ntest_4.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# replace nan values with 0\ntest_4.fillna(0, inplace=True)\n\n\n########################################\n\n\ntest_3 = test_df[['LotArea','YearRemodAdd','MasVnrArea','TotalBsmtSF','1stFlrSF',\n                  'GrLivArea','FullBath','WoodDeckSF','OpenPorchSF',\n                  'ExterQual','GarageQual','LandSlope','KitchenQual']]\n\n# combining SF features into one\ntest_3['SF'] = test_3['LotArea'] + test_3['TotalBsmtSF'] + test_3['1stFlrSF'] + test_3['GrLivArea']+ test_3['WoodDeckSF'] + test_3['OpenPorchSF']\n\ntest_3.drop(['LotArea','TotalBsmtSF','1stFlrSF',\n              'GrLivArea','WoodDeckSF','OpenPorchSF'], axis = 1, inplace=True)\n\ntest_3 = pd.get_dummies(test_3,columns = ['ExterQual','GarageQual','LandSlope','KitchenQual'])\n\n#Found in debugging, no observation of 'Ex' in GarageQual in test_df\ntest_3['GarageQual_Ex'] = 0\n\n# Cast type, and fill some NaN values found in debugging\ntest_3 = test_3.astype(np.float64)\n\n# Found in debugging, reordering columns to match training\ntest_3 = test_3.reindex(columns= list(X_train3.columns))\n\n# replace all infinity values with nan\ntest_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# replace nan values with 0\ntest_3.fillna(0, inplace=True)\n\n\n########################################\n\n\ntest_2 = test_df[['LotArea','YearRemodAdd','MasVnrArea',\n                   'TotalBsmtSF','1stFlrSF', 'GrLivArea', 'FullBath',\n                   'WoodDeckSF','OpenPorchSF','ExterQual','GarageQual',\n                   'LandSlope','BedroomAbvGr','KitchenQual']]\n\ntest_2 = pd.get_dummies(test_2,columns = ['ExterQual','GarageQual','LandSlope','KitchenQual'])\n\n#Found in debugging, no observation of 'Ex' in GarageQual in test_df\ntest_2['GarageQual_Ex'] = 0\n\n# Cast type, and fill some NaN values found in debugging\ntest_2 = test_2.astype(np.float64)\n\n# Found in debugging, reordering columns to match training\ntest_2 = test_2.reindex(columns= list(X_train2.columns))\n\n# replace all infinity values with nan\ntest_2.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# replace nan values with 0\ntest_2.fillna(0, inplace=True)\n\n\n########################################\n\n\ntest_1 = test_df.copy()\n\nvariables = list(test_df.columns)\nnumerical_variables = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1','BsmtFinSF2',\n                       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', 'ndFlrSF', 'LowQualFinSF','GrLivArea', 'BsmtFullBath',\n                       'BsmtHalf', 'Bath', 'FullBath', 'HalfBath','Bedroom', 'Kitchen', 'TotRmsAbvGrd', 'Fireplaces',\n                       'GarageYrBlt', 'GarageCars','GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch',\n                       '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'BedroomAbvGr']\nclass_variables = list(set(variables) - set(numerical_variables))\n\ntest_1 = pd.get_dummies(test_1,columns = class_variables)\n\n#Found in debugging, no observation of 'Ex' in GarageQual in test_df\ntest_1['GarageQual_Ex'] = 0\n\n# Remove the features from debugging\ntest_1.drop(['GarageYrBlt', 'GarageArea', 'GarageCars'], axis = 1, inplace = True)\n\n# Cast type, and fill some NaN values found in debugging\ntest_1 = test_1.astype(np.float64)\n\n# Found in debugging, reordering columns to match training\ntest_1 = test_1.reindex(columns= list(X_train.columns))\n\n# replace all infinity values with nan\ntest_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# replace nan values with 0\ntest_1.fillna(0, inplace=True)\n\n\n########################################\n\nsub4 = gbr_final4.predict(test_4)\nsub3 = gbr_final3.predict(test_3)\nsub2 = gbr_final2.predict(test_2)\nsub1 = gbr_final.predict(test_1)\n\n########################################\n\nresult4 = pd.DataFrame({'Id':test_id, 'SalePrice': sub4})\nresult4.to_csv('.\/my_submission_04.csv', index=False)\n\nresult3 = pd.DataFrame({'Id':test_id, 'SalePrice': sub3})\nresult3.to_csv('.\/my_submission_03.csv', index=False)\n\nresult2 = pd.DataFrame({'Id':test_id, 'SalePrice': sub2})\nresult2.to_csv('.\/my_submission_02.csv', index=False)\n\nresult1 = pd.DataFrame({'Id':test_id, 'SalePrice': sub1})\nresult1.to_csv('.\/my_submission_01.csv', index=False)","9ba868bc":"# Prepare and split data for training\n\nBecause we have a lot of variables which have various string entries. We are going to one-hot encode each one. To save us time we are going to split the full_df into y_data and X_data first, as there are some columns that we don't need to do this with.","6c59004b":"# Challenge Description\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","0ae9a63b":"### TL:DR 2 Estimators, score, tuned score\n\n- RandomForestRegressor: 0.0457961, 0.0456705\n\n- Lasso: 0.0494255, 0.0334479\n\n- GradientBoostingRegressor: 0.0424119, 0.0419498\n\n#### Our scores are worse.","c8907a94":"# Model Choice\n\nWe'll choose the GradientBoostingRegressor as it has performed best in those tests with various feature choices.\n\nSo we'll run through one more time, but a much larger and finer gridsearchcv to optimise. We'll check our score, save the parameters, and then submit.","16273ef3":"## Our Model","103530fb":"## Debugging and version control-ish\n\nRan full dataframe into model, error: can't turn \"RL\" into float.\n-  check types, get dummy variables\n\nRan new df with dummy variables into model, error: 'contains Nan, inf, or value too large of 'float32''\n-  check for nan, inf values in dataframe, returned zero\n-  Same error. Change all quantities by a magnitude of 10, 100\n-  Same error. Investigated further. Found NaN values in X_data columns 0, 4, 17.\n-  Fixed: X_data.fillna({'LotFrontage':0, 'MasVnrArea':0, 'GarageYrBlt':0}, inplace=True)\n\nRan model and found feature_importances_\n- found that \"GarageYrBlt\" had an importance weight of over 0.35 which seems very strange\n- Propose it's because fill.na() was used to replace NaN with 0.0. \n- We'll set it instead to the minimum (oldest) before we simply remove it\n- Still weighed a lot, so used mean() instead of min()\n- Still weighed a lot, so removed the feature \"GarageYrBlt\"\n- Found that 'GarageCars' had an importance weight of over 0.35 which seems very strange\n- I'm going to make an assumption that if we remove 'GarageCars', the next large influence will be 'GarageArea'\n- I was correct.\n- Removing these should not pose much of an issue as they all fall under whether there is a garage or not,which is already a condition with a feature for it.\n\nCreated a new rfr model using the subset of features from above\n- It matches the same feature importance as before\n\nCreated rfr, lasso, gbr models\n- Lowest score 0.024 on test data... even with gridsearchCV\n\nEpiphany moment -> Can we combine all SF variables for total SF of house?\nIs this another way to reduce variables down even further?\n- Ran with reduced variables, lowest score of 0.033\n\nOverall, GradientBoostingRegressor appears best\n- Set up Larger gridsearchCV for gradientboostingregressor, gbr_final\n- runtime error: fixed\n\nSet up test_df for submission predictions\n- test_df has 'MoSold', 'YrSold' and 'SaleCondition' included. Decided to run notebook again with these included to check for any adjustments.\n- These three variables did not impact 'feature_importances_', no major adjustments needed\n\nFocusing on model_4 (gbr4, test_4)\n- Error: \"Input contains NaN, infinity or a value too large for dtype('float32')\"\n- Found one missing value in test_4 'TotalBsmtSF', \u2192 fillna(0)\n- Error: \"Number of features of the model must match the input. Model n_features is 33 and input n_features is 31\" \n- Found that 'BedroomAbvGr' has been included as categorical instead of numerical, leading to more\/less columns following the pd.get_dummies()\n- Error: Number of features of the model must match the input. Model n_features is 26 and input n_features is 25\n- Found 'GarageQual_Ex' is missing from test_4. (No observation of 'Ex' in test data)\n- added column manually, test_#['GarageQual_Ex'] = 0\n- !!!!! not a solution, SKLearn doesn't store column labels, order needs to be the same\n- Error: 'contains Nan, inf, or value too large of 'float32'\n- Fixed with: 'df.replace([np.inf, -np.inf], np.nan, inplace=True)' and 'df.fillna(0, inplace=True)'\n\nSuccess","26cfeef3":"# Choosing Main Features\n\nLooking at 'important_feature_names', we will want to prune and engineer our most important features. Of the features in this list a few stand out.\n\n- 'YearBuilt' and 'YearRemodAdd'.\nAccording to the documentation, \"YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)\" .**So** this feature includes 'YearBuilt'. So we'll condense by just dropping YearBuilt.\n\n- 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF'\nThe last of this triplet is the sum of the previous two. So it contains both. So we'll drop these in favour of 'TotalBsmtSF'\n\n- Categorical\nThe categorical appears to lean heavily on only one class. Eg. the kitchen only matters if it's \"excellent\". Regardless if this interpretation is correct, we'll just include these in their entirety with not change.","77e70b42":"# Choosing a Model\n\nConsidering we have now brought down the number of features from 81 to 730 to 36, we should now consider which model to choose to best solve our problem.\n\nWe already have the RandomForestRegressor loaded so we will use this first and se how it performs. But we'll look at Lasso immediately after, and then some.\n\n### TL:DR Estimators, score (.6sf), tuned score\n\n- RandomForestRegressor: 0.0250458, 0.0247159\n\n- Lasso: 0.0305654, 0.0298520\n\n- GradientBoostingRegressor: 0.0254922, 0.0241351","37e1e309":"This plot is disgusting. However we can indeed see some important features. So we'll filter our list by those which have an importance of 'just enough to matter', and see what comes out second time round.","b2283411":"# Final results\n\nWe have, through our various attempts at feature engineering.\n\n#### Data: Score\n\n- X_data4: 0.0290195\n- X_data3: 0.0412940\n- X_data2: 0.0290195\n- X_data: 0.0176113  <- original with one-hot encoding.\n\nWe'll submit each of these models and see which is best","f3aac39e":"# More feature engineering\n\nSo after more thought, I realised that a lot of the features that weighed importance have the 'SF' tag. Square footage. *Obviously*, the value of a house depends on its size! So, let's combine them all together to *further* reduce the number of features.\n\nWe'll see how the three models compare this time around.","4ffefd73":"## See any strange values?\n\n**I certainly did**, check 'Debugging' section for some\n\nAt Version 0_2, we have the following in order\n\n- ExterQual_TA: Exterior Quality of the house is 'TA' - typical\n- KitchenQual_Ex: Kitchen Quality of teh house is 'Ex' - Excellent\n- GrLivArea: Above ground living area, in square feet\n- 1stFlrSF: First Floor square footage\n- FullBath: Full bathrooms above grade\n- *TotalBsmtSF*: Basement Unfinished square footage\n\nTotalBsmtSF depends on whether a property has a basement, and then the condition, and ties close with unfinished square footage and also total square footage. So there appears to be good room for feature engineering here\n","cad8c14f":"## Read the description\n\nThat is \"data_description.txt\". It gives us a full breakdown of each variable and at first glance it seems that feature engineering will be \"imaginative\". Almost every variable can feasibly contribute in the overall price, but then clearly some will weigh more than others. (and we will have to ignore the ridiculous pricing that housing undergoes regardless of the actual house itself)\n\nTo help, I'm sure we can use \"feature_importances_\" property of the RandomForestRegressor model from SKLearn to do some feature weighing. That is: we'll retroactively choose our features by looking at the data first and foremost.\n\nFirst though we can get a look into how the features correlate with eachother and against the target variable 'SalePrice', using Seaborn ","1a987602":"# Submission\n\nSubmission 'result1' scored best at 0.14616.","2bfe43fc":"# The Data\n\nWe will load our data, \"train.csv\" into a dataframe and inspect it, as always. There are 79 variables so there should be something of interest to us instead of simply throwing everything into a model"}}