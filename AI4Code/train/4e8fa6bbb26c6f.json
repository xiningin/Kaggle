{"cell_type":{"c68ae2b4":"code","3f881d6c":"code","3c12e8f6":"code","a03e76c4":"code","6aed9553":"code","37d9db6b":"code","1a6c8941":"code","dd4c4453":"code","3cedbf2e":"code","e330ec6d":"code","9000a421":"code","41adbb59":"code","fa794332":"code","7a526604":"code","5175fe6d":"code","d48e1ecf":"code","60630b9d":"code","1cf31533":"code","a750c07a":"code","37f877f7":"code","eaf9787b":"code","d499e989":"code","c993aa6a":"code","e5e1e323":"code","9b715c94":"code","7bd52fd0":"code","8bbc3da0":"code","deec5598":"code","e0c098ef":"code","7372a4a2":"code","694e7fdf":"code","4b2b0a0d":"code","8afa0a68":"code","5523c358":"code","66c8cc8c":"code","22417eb0":"code","baf1633a":"code","7b9ddc2f":"code","af411e37":"code","0585e790":"code","668f16ab":"code","76ff45f7":"markdown"},"source":{"c68ae2b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3f881d6c":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import median_absolute_error, mean_squared_error\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom nltk.tokenize import RegexpTokenizer\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc, mean_absolute_error\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom xgboost import XGBRegressor","3c12e8f6":"test = pd.read_csv('\/kaggle\/input\/ammi-ghana-bootcamp-kaggle-competition\/test.csv')\n# Convert the data to a Pandas data frame\ndata = pd.read_csv('\/kaggle\/input\/ammi-ghana-bootcamp-kaggle-competition\/train.csv')\n\n# Shuffle the data\ndata = data.sample(frac=1)\n\n# Print the first 5 rows\ndata.head()","a03e76c4":"import category_encoders as ce \nimport re\n\ndef combine_train_test_set(train, test):\n    train['train'] = 1\n    test['train'] = 0\n    train['test'] = 0\n    test['test'] = 1\n    return pd.concat([train, test[train.columns.tolist()]])\n\ndef add_text_features(df, col):\n    df = df.copy()\n    # Count number of \\n\n    df[\"ant_slash_n\"] = df[col].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n    # Get length in words and characters\n    df[col + \"_word_len\"] = df[col].apply(lambda x: len(x.split()))\n    df[col+\"_char_len\"] = df[col].apply(lambda x: len(x))\n    # Get the new length in words and characters\n    df[col+\"_word_len\"] = df[col].apply(lambda x: len(x.split()))\n    df[col+\"_char_len\"] = df[col].apply(lambda x: len(x))\n    # Number of different characters used in a comment\n    # Using the f word only will reduce the number of letters required in the comment\n    df[\"clean_chars\"] = df[col].apply(lambda x: len(set(x)))\n    df[\"clean_chars_ratio\"] = df[col].apply(lambda x: len(set(x))) \/ df[col].apply(\n        lambda x: 1 + min(99, len(x)))\n    return df\n    \ndef split_train_test_set(combine):\n    test = combine[combine['test'] == 1].drop(columns=['test', 'train'])\n    train = combine[combine['train'] == 1].drop(columns=['test', 'train'])\n    return train, test\n\ndef make_lower_case(text):\n    return text.lower()\n\ndef remove_punctuation(text):\n    tokenizer = RegexpTokenizer(r'\\w+')\n    text = tokenizer.tokenize(text)\n    text = \" \".join(text)\n    return text\n\ndef clean_text_data(data, cols):\n    for col in cols:\n        data[col].fillna('#na', inplace=True)\n        data[col] = data[col].str.replace(r'\\d+','')\n        data[col] = data.description.apply(func=remove_punctuation)\n        data[col] = data.description.apply(func=make_lower_case)\n    \ndef remove_null_rows(data, cols):\n    for c in cols:\n        data = data[pd.notnull(data[c])]\n    return data\n\ndef plot_len(data, col, bins=50, title='Token per sentence', ylabel='# samples', xlabel='Len (number of token)'):\n    plt.hist(data[col].apply(len).values, bins=50)\n    plt.title(title)\n    plt.xlabel(ylabel)\n    plt.ylabel(xlabel)\n    plt.show()\n    \n\ndef transform_text(data, col, n_components=5):\n    #Train tfidf and svd\n    tf = TfidfVectorizer(analyzer='word', min_df=10, ngram_range=(1, 2), stop_words='english')\n    svd = TruncatedSVD(n_components=n_components)\n\n    #Fit tfidf and svd, and transform training data\n    tfidf_matrix = tf.fit_transform(data[col])\n    lsa_features = pd.DataFrame(svd.fit_transform(tfidf_matrix))\n\n    #Creat meaningful column names\n    collist = map(str, range(0, 5))\n    collist = [\"latent_\" + col + '_' + s for s in collist]\n    lsa_features.columns = collist\n    lsa_features = lsa_features.set_index(data.index)\n    return lsa_features\n\ndef map_to_pd_categorical(data, cols, fill_na='#nan'):\n    for col in cols:\n        data[col] = pd.Categorical(data[col].fillna(fill_na))\n    return data\n\ndef cat_to_int(data, cols, fill_na='#nan'):\n    for col in cols:\n        if data[col].dtype.name == 'object':\n            data[col] = pd.Categorical(data[col].fillna(fill_na))\n        elif data[col].dtype.name == 'category': \n            data[col] = data[col].cat.codes\n        else:\n            print('Not categorical or object : ' + col + ' ', data[col].dtype)\n    return data\n        \ndef rmse_error(predictions, y_test):\n    return np.sqrt(mean_squared_error(predictions, y_test))\n\ndef save_submissions(submissions, test, name='submissions.csv'):\n    if 'id' in test.columns:\n        test.set_index('id', inplace=True)\n    submissions = pd.DataFrame(submissions, columns=['price'], index=test.index)\n    submissions.to_csv(name)\n    return submissions\n\ndef limit_categorical_by_count(data, cols=[], thres=500, fill=np.nan):\n    for c in cols:\n        value_counts = data[c].value_counts()\n        to_remove = value_counts[value_counts <= thres].index\n        if to_remove.size > 0:\n            data.replace(to_remove, fill, inplace=True)\n        data = data[pd.notnull(data[c])]\n    return data\n\n\ndef print_str_info(X, features):\n    for feat in features:\n        print('Unique {} : {}'.format(feat, X[feat].nunique()))\n        print('Data with No {} values : {}'.format(feat, X[feat].isna().sum()))\n        print('Max length : ', X[feat].apply(lambda x: len(str(x).split())).max())\n\ndef train_model(model, X_train, y_train, X_test, y_test):\n    model.fit( X_train, y_train)\n    predictions = model.predict(X_test)\n    print(\"RMSE on test data: \"+ str(rmse_error(predictions, y_test)))\n    return model\n\ndef target_encoding(data, cols, fit_data, y=None,y_label=None, fill_na='#nan'):\n    # Target with default parameters\n    ce_target = ce.TargetEncoder(cols = cols)\n    y = fit_data[y_label] if y is None  else y\n    print(y.shape, fit_data.shape)\n    ce_target.fit(fit_data, y)\n    # Must pass the series for y in v1.2.8\n    return ce_target.transform(data)\n    \ndef target_encoding_smoothing(data, cols, fit_data, y=None,y_label=None, fill_na='#nan'):\n    # Target with smoothing higher\n    ce_target = ce.TargetEncoder(cols = cols, smoothing = 10)\n    y = fit_data[y_label] if y is None  else y\n    print(y.shape, fit_data.shape)\n    ce_target.fit(fit_data, y)\n    # Must pass the series for y in v1.2.8\n    return ce_target.transform(data)\n\ndef add_length(data, col, fill_na='#nan'):\n    # Target with default parameters\n    data[col].fillna(value='#nan', inplace=True)\n    data[col+'_len'] = data[col].apply(lambda s: len(s.split()))\n    \n# Let turn our string to tokens\ndef text_to_seq(data, col, max_features = 6000):\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(data[col])\n    list_tokenized = tokenizer.texts_to_sequences(data['title_desc'])\n    return pad_sequences(list_tokenized, maxlen=maxlen), tokenizer\n\ndef count_regexp_occ(regexp=\"\", text=None):\n    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n    return len(re.findall(regexp, text))","6aed9553":"data = remove_null_rows(data, ['country', 'price'])","37d9db6b":"combine = combine_train_test_set(data, test)\ncombine.head()","1a6c8941":"# Do some preprocessing to limit the # of wine varities in the dataset\n# Clean it from null values\ncombine = limit_categorical_by_count(combine, cols=['variety'], fill='nan')\ncombine.head()","dd4c4453":"def plot_keras():\n    # Plot training & validation accuracy values\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n# plot_keras()","3cedbf2e":"combine.head()","e330ec6d":"categorical_features = ['country','designation', 'province','region_1','region_2', 'taster_name','taster_twitter_handle', 'variety','winery']\nencoded = target_encoding_smoothing(combine,categorical_features, combine[combine['train'] == 1],y_label='price')\nadd_length(encoded, 'description')\ndata, test = split_train_test_set(encoded)\ndata.head()","9000a421":"X = data.drop(columns=['title', 'description', 'id', 'price'])\ny = data['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","41adbb59":"#PROCESS TEXT: RAW\nprint(\"Text to seq process...\")\nprint(\"   Fitting tokenizer...\")\nfrom keras.preprocessing.text import Tokenizer\nencoded.title=encoded.title.astype(str)\nencoded.description=encoded.description.astype(str)\nclean_text_data(encoded[['title', 'description']], cols=['title', 'description'])\nraw_text = np.hstack([encoded.title.str.lower(), encoded.description.str.lower()])\nprint(raw_text[0])\n\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(raw_text)\nprint(\"   Transforming text to seq...\")\nencoded[\"seq_title\"] = tok_raw.texts_to_sequences(encoded.title.str.lower())\nencoded[\"seq_description\"] = tok_raw.texts_to_sequences(encoded.description.str.lower())","fa794332":"encoded.head()","7a526604":"encoded.seq_title.apply(lambda x: len(x)).hist()","5175fe6d":"encoded.seq_description.apply(lambda x: len(x)).hist()","d48e1ecf":"#EMBEDDINGS MAX VALUE\n#Base on the histograms, we select the next lengths\nMAX_TITLE_SEQ = 13\nMAX_DESC_SEQ = 70\nMAX_TEXT = np.max(encoded.seq_description.max())+1","60630b9d":"# #SCALE target variable\n# from sklearn.preprocessing import MinMaxScaler\n# y_train = np.log(encoded[encoded['train'] == 1].price+1)\n# target_scaler = MinMaxScaler(feature_range=(-1, 1))\n# y_train = target_scaler.fit_transform(y_train.values.reshape(-1,1))\n# pd.DataFrame(y_train).hist()","1cf31533":"#EXTRACT DEVELOPTMENT TEST\ntrain, test = split_train_test_set(encoded.drop(columns=['description', 'title']))\ntrain.head()","a750c07a":"#KERAS DATA DEFINITION\nfrom keras.preprocessing.sequence import pad_sequences\nX = train.drop(columns=['id', 'price'])\ny = train['price']\ndef get_seq_data(dataset):\n    return {\n    'title':pad_sequences(dataset.seq_title, maxlen=MAX_TITLE_SEQ),\n    'description': pad_sequences(dataset.seq_description, maxlen=MAX_DESC_SEQ)\n#     ,'num_vars': np.array(get_num_data(dataset))\n}\n\ndef get_num_data(data):\n    return data[categorical_features+['points','description_len']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, train_size=0.9)\n\nX_train = get_seq_data(X_train)\nX_test = get_seq_data(X_test)","37f877f7":"#KERAS MODEL DEFINITION\nfrom keras.layers import Input, Dropout, Dense,Bidirectional, LSTM, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization, GlobalMaxPool1D, Conv1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import backend as K\n\ndef get_callbacks(filepath, patience=2):\n    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [es, msave]\n\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n\ndef get_model():\n    #params\n    dr_r = 0.05\n    dr_d = 0.05\n    \n    #Inputs\n    title = Input(shape=[X_train[\"title\"].shape[1]], name=\"title\")\n    description = Input(shape=[X_train[\"description\"].shape[1]], name=\"description\")\n#     num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n    \n    #Embeddings layers\n    emb_size = 55\n    emb_title = Embedding(MAX_TEXT, 32)(title)\n    emb_item_desc = Embedding(MAX_TEXT, 60)(description)\n    \n    lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(emb_item_desc)\n    lstm_layer = Dropout(0.4)(lstm_layer)\n    lstm_layer = LSTM(32)(lstm_layer)\n    lstm_layer_2 = Bidirectional(LSTM(32, return_sequences=True))(emb_title)\n    lstm_layer_2 = Dropout(0.4)(lstm_layer_2)\n    lstm_layer_2 = LSTM(32)(lstm_layer_2)\n    \n    #main layer\n    main_l = concatenate([ lstm_layer , lstm_layer_2 ])\n    \n    main_l = Dropout(dr_r) (Dense(512,activation='relu') (main_l))\n    main_l = Dropout(dr_r) (Dense(256,activation='relu') (main_l))\n    main_l = Dropout(dr_d) (Dense(16,activation='relu') (main_l))\n    \n    #output\n    output = Dense(1, activation=\"linear\") (main_l)\n    \n    #model\n    model = Model([description, title], output)\n    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\",'acc', root_mean_squared_error])\n    \n    return model\n\n","eaf9787b":"model = get_model()\nmodel.summary()","d499e989":"#FITTING THE MODEL\nBATCH_SIZE = 512\nepochs = 10\n\nmodel = get_model()\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=BATCH_SIZE , validation_data=[X_test, y_test], verbose=1)\n","c993aa6a":"predictions = model.predict(get_seq_data(X))\nprint(\"RMSE on test data: \"+ str(rmse_error(predictions, y)))\nplot_keras()","e5e1e323":"dec_encode = model.predict(get_seq_data(encoded))","9b715c94":"encoded['description_encoded'] = pd.DataFrame(dec_encode, columns=['description_encoded'], index=encoded.index)['description_encoded']","7bd52fd0":"encoded.head()","8bbc3da0":"import lightgbm as lgb","deec5598":"encoded_data = encoded.drop(columns=['seq_title','seq_description'])\ndata, test = split_train_test_set(encoded_data)\ndata.head()","e0c098ef":"test_set = test.drop(columns=['title', 'description', 'price'])\ntest_set = test_set.fillna(-1)\ntest_set.head()","7372a4a2":"X = data.drop(columns=['title', 'description', 'price', 'description_len'])\ny = data['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","694e7fdf":"# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\nlgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train, free_raw_data=False)","4b2b0a0d":"evals_result = {}  # to record eval results for plotting\n\n# specify your configurations as a dict\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'num_leaves': 35,\n    'max_depth':7,\n    'metric': 'rmse',\n    'verbose': 0,\n    'bagging_fraction': 0.8\n}\n\ngbm_deeplearning = lgb.train(params,\n                lgb_train,\n                num_boost_round=3000,\n                valid_sets=[lgb_train, lgb_test],\n                feature_name=X_train.columns.tolist(),\n#                 categorical_feature=categorical_features,\n                evals_result=evals_result,\n                verbose_eval=10)","8afa0a68":"predictions_gbm_deeplearning = gbm_deeplearning.predict(X_test)\npredictions_gbm_deep_test = pd.DataFrame(predictions_gbm_deeplearning, columns=['price'], index=y_test.index)\nprint(rmse_error(predictions_gbm_deeplearning, y_test))","5523c358":"predictions_gbm_deeplearning = gbm_deeplearning.predict(test_set.drop(columns=['description_len']))\npredictions_gbm_deeplearning = pd.DataFrame(predictions_gbm_deeplearning, columns=['price'], index=test_set.index)\npredictions_gbm_deeplearning.to_csv('predictions_gbm_deeplearning_13.csv')","66c8cc8c":"X = data.drop(columns=['title', 'description', 'price'])\ny = data['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","22417eb0":"\nxgb_deep_learning = XGBRegressor(max_depth=5, n_estimators=1000, min_child_weight=5)\ntrain_model(xgb_deep_learning,  X_train, y_train, X_test, y_test)","baf1633a":"predictions_xgb_deep_learning = xgb_deep_learning.predict(X_test)\npredictions_xgb_deep_test = pd.DataFrame(predictions_xgb_deep_learning, columns=['price'], index=y_test.index)\nprint(rmse_error(predictions_xgb_deep_learning, y_test))","7b9ddc2f":"predictions_xgb_deep_learning = xgb_deep_learning.predict(test_set)\npredictions_xgb_deep_learning = pd.DataFrame(predictions_xgb_deep_learning, columns=['price'], index=test_set.index)\npredictions_xgb_deep_learning.to_csv('predictions_xgb_deep_learning_11.csv')","af411e37":"predictions_xgb_deep_learning.to_csv('predictions_xgb_deep_learning_11.csv')","0585e790":"avg = (predictions_xgb_deep_learning + predictions_gbm_deeplearning)\/2\navg = pd.DataFrame(avg, columns=['price'], index=test_set.index)\navg.to_csv('hybrid_deep_learning_11.csv')","668f16ab":"avg = (predictions_xgb_deep_test + predictions_gbm_deep_test)\/2\nprint(rmse_error(avg, y_test))","76ff45f7":"Smoothing with target encoding"}}