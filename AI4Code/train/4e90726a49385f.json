{"cell_type":{"ad3099f1":"code","f5bd333c":"code","a5d6da2b":"code","2809413e":"code","6cc9e418":"code","4a385823":"code","5fb420e4":"code","71dd32fb":"code","bb7e9132":"code","168d6391":"code","6cb53213":"code","a2652e91":"code","672dcfbc":"code","5c23394c":"code","0abcffbe":"code","1ef1e31a":"code","0c347c27":"code","23621a8e":"code","17f3a39c":"code","e448b385":"code","d63ace29":"code","ddf934fd":"code","3036a59c":"code","d73f6b6e":"code","5d8dbc05":"code","675b46a9":"code","baf67f67":"code","89f46146":"markdown","3593be37":"markdown","3c8d5f07":"markdown","23325f1c":"markdown","97e48480":"markdown","12c91337":"markdown","1533c2e8":"markdown","646fa099":"markdown","0007e4d0":"markdown","7f8c7d24":"markdown","a884ee09":"markdown","d3dc9b90":"markdown","d4ea9911":"markdown","b0bf7a64":"markdown","e2d1023c":"markdown"},"source":{"ad3099f1":"#install Apache Spark\n!pip install pyspark --quiet","f5bd333c":"#Generic Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Apache Spark Libraries\nimport pyspark\nfrom pyspark.sql import SparkSession\n\n#Apache Spark ML CLassifier Libraries\nfrom pyspark.ml.classification import DecisionTreeClassifier,RandomForestClassifier,NaiveBayes\n\n#Apache Spark Evaluation Library\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n#Apache Spark Features libraries\nfrom pyspark.ml.feature import StandardScaler,StringIndexer, VectorAssembler, VectorIndexer, OneHotEncoder\n\n#Apache Spark Pipelin Library\nfrom pyspark.ml import Pipeline\n\n# Apache Spark `DenseVector`\nfrom pyspark.ml.linalg import DenseVector\n\n#Data Split Libraries\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\n\n#Tabulating Data\nfrom tabulate import tabulate\n\n#Garbage\nimport gc","a5d6da2b":"#Building Spark Session\nspark = (SparkSession.builder\n                  .appName('Apache Spark Beginner Tutorial')\n                  .config(\"spark.executor.memory\", \"1G\")\n                  .config(\"spark.executor.cores\",\"4\")\n                  .getOrCreate())","2809413e":"spark.sparkContext.setLogLevel('INFO')","6cc9e418":"spark.version","4a385823":"url = '..\/input\/iris-dataset\/iris.csv'\n\ndata = spark.read.format(\"csv\") \\\n       .option(\"header\", \"true\") \\\n       .option(\"inferSchema\",\"true\")\\\n       .load(url) \n\ndata.cache() #for faster re-use","5fb420e4":"#Total records \ndata.count()","71dd32fb":"#Data Type\ndata.printSchema()","bb7e9132":"#Display records\ndata.show(5)","168d6391":"#Records per Species\ndata.groupBy('species').count().show()","6cb53213":"#Dataset Summary Stats\ndata.describe().show()","a2652e91":"#String Indexing the Species column\nSIndexer = StringIndexer(inputCol='species', outputCol='species_indx')\ndata = SIndexer.fit(data).transform(data)\n\n#Inspect the dataset\ndata.show(5)\n","672dcfbc":"#creating a seperate dataframe with re-ordered columns\ndf = data.select(\"species_indx\",\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n\n#Inspect the dataframe\ndf.show(5)","5c23394c":"# Define the `input_data` as Dense Vector\ninput_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))","0abcffbe":"# Creating a new Indexed Dataframe\ndf_indx = spark.createDataFrame(input_data, [\"label\", \"features\"])","1ef1e31a":"#view the indexed dataframe\ndf_indx.show(5)","0c347c27":"#Initialize Standard Scaler\nstdScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n\n#Fit the Standard Scaler to the indexed Dataframe\nscaler = stdScaler.fit(df_indx)\n\n#Transform the dataframe\ndf_scaled =scaler.transform(df_indx)","23621a8e":"#Viewing the Scaled Data\ndf_scaled.show(5)","17f3a39c":"#Dropping the Features column\ndf_scaled = df_scaled.drop(\"features\")","e448b385":"train_data, test_data = df_scaled.randomSplit([0.9, 0.1], seed = 12345)","d63ace29":"#Inspect Training Data\ntrain_data.show(5)","ddf934fd":"model = ['Decision Tree','Random Forest','Naive Bayes']\nmodel_results = []","3036a59c":"# -- Decision Tree Classifier --\n\ndtc = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features_scaled\")          #instantiate the model\ndtc_model = dtc.fit(train_data)                                                        #train the model\ndtc_pred = dtc_model.transform(test_data)                                              #model predictions\n\n#Evaluate the Model\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\ndtc_acc = evaluator.evaluate(dtc_pred)\n#print(\"Decision Tree Classifier Accuracy =\", '{:.2%}'.format(dtc_acc))\nmodel_results.extend([[model[0],'{:.2%}'.format(dtc_acc)]])                               #appending to list\n    ","d73f6b6e":"# -- Random Forest Classifier --\n\nrfc = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features_scaled\", numTrees=10)          #instantiate the model\nrfc_model = rfc.fit(train_data)                                                                     #train the model\nrfc_pred = rfc_model.transform(test_data)                                                           #model predictions\n\n#Evaluate the Model\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\nrfc_acc = evaluator.evaluate(rfc_pred)\n#print(\"Random Forest Classifier Accuracy =\", '{:.2%}'.format(rfc_acc))\nmodel_results.extend([[model[1],'{:.2%}'.format(rfc_acc)]])                                            #appending to list","5d8dbc05":"# -- Naive Bayes Classifier --\n\nnbc = NaiveBayes(smoothing=1.0,modelType=\"multinomial\", labelCol=\"label\",featuresCol=\"features_scaled\")    #instantiate the model\nnbc_model = nbc.fit(train_data)                                                                          #train the model\nnbc_pred = nbc_model.transform(test_data)                                                                #model predictions\n\n#Evaluate the Model\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\nnbc_acc = evaluator.evaluate(nbc_pred)\n#print(\"Naive Bayes Accuracy =\", '{:.2%}'.format(nbc_acc))\nmodel_results.extend([[model[2],'{:.2%}'.format(nbc_acc)]])                                            #appending to list","675b46a9":"#freeing memory\ngc.collect()","baf67f67":"print (tabulate(model_results, headers=[\"Classifier Models\", \"Accuracy\"]))","89f46146":"## Feature Engineering","3593be37":"## Data Exploration & Preparation","3c8d5f07":"## Build Spark Session","23325f1c":"**Note:** Observe that the species column which is our label (aka Target) is now at beginning of the dataframe","97e48480":"**Note:** Observe the definition of the Dense Vector. So,when we create a new indexed dataframe(below) the machine understands that the first column is a Label (Target) and the remaining columns are Features.","12c91337":"## Importing Libraries","1533c2e8":"The Spark model needs two columns: \u201clabel\u201d and \u201cfeatures\u201d and we are not going to do much feature engineering because we want to focus on the mechanics of training the model in Spark. \n\nSo, creating a seperate dataframe with re-ordered columns, then defining an input data using Dense Vector. A Dense Vector is a local vector that is backed by a double array that represents its entry values. In other words, it's used to store arrays of values for use in PySpark.\n","646fa099":"## Data Split\n\nJust like always, before building a model we shall split our scaled dataset into training & test sets. \nTraining Dataset = 90%\nTest Dataset = 10%","0007e4d0":"## Build, Train & Evaluate Model\n\nIn this step we will create multiple models, train them on our scaled dataset and then compare their accuracy.","7f8c7d24":"# Apache Spark - Beginner Tutorial\n\nFirstly, Thank you for checking this tutorial notebook. By now you must have read a lot about Apache Spark and its ML Library.So, I won't bore you with the introduction to Apache-Spark or even the library details. We shall move straight to the interesting stuff i.e. coding.\n\nIn this tuorial notebook we will try to compare some of the Apache Spark's Classification Algorithms in an easy way to make predictions. And since this is a beginner's tutorial, we will use the Iris Flower Dataset aka the beginner's dataset in machine learning.\n\n**A quick summary:**\n\n* Import Libraries\n* Build Spark Session\n* Data Load\n* Data Exploration & Preparation\n* Feature Engineering\n* Data Scaling\n* Data Split\n* Build, Train & Evaluate Model\n","a884ee09":"## Data Load","d3dc9b90":"Inorder for our model to make predictions the Species aka Label column should be a numerical value (models don't like string!). To achieve this we shall use String Indexing on the Species columns","d4ea9911":"![](https:\/\/www.appreciationatwork.com\/wp-content\/uploads\/2018\/01\/thank-you.jpg)\n\nI hope this tutorial was helpful.","b0bf7a64":"## Data Scaling\n\nThis is also known as Feature Scaling. It is a method of normalizing the features of the data. Scaling can make a difference between a weak machine learning model and a better one. \n\nIn this tutorial we will use a Standard Scaler to scale our feature data. Apache Spark has a Standard Scaler library to do the job.","e2d1023c":"Tabulating the results."}}