{"cell_type":{"de5b4de4":"code","cf0bd634":"code","6de65352":"code","97ce781a":"code","47ed5709":"code","4c131243":"code","075f2abc":"code","2af615ca":"code","8759d2cf":"code","438b4639":"code","64132040":"code","50ea2e92":"code","25e1a8a8":"code","c19b6104":"code","47aa11c0":"code","b1cdb926":"code","2dde65d6":"code","3fbbb99e":"code","93ae8ec4":"code","02e45f7e":"code","d6abb28f":"markdown","a445f5eb":"markdown"},"source":{"de5b4de4":"import sys\nimport numpy as np\nimport keras\nfrom keras.utils import Sequence\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nimport cv2","cf0bd634":"BATCH_SIZE = 128\nSEED = 777\nSHAPE = (192, 192, 4)\nDIR = '..\/input'\nVAL_RATIO = 0.1 # 10 % as validation\nTHRESHOLD = 0.05 # due to different cost of True Positive vs False Positive, this is the probability threshold to predict the class as 'yes'\n\nia.seed(SEED)","6de65352":"def getTrainDataset():\n    \n    path_to_train = DIR + '\/train\/'\n    data = pd.read_csv(DIR + '\/train.csv')\n\n    paths = []\n    labels = []\n    \n    for name, lbl in zip(data['Id'], data['Target'].str.split(' ')):\n        y = np.zeros(28)\n        for key in lbl:\n            y[int(key)] = 1\n        paths.append(os.path.join(path_to_train, name))\n        labels.append(y)\n\n    return np.array(paths), np.array(labels)\n\ndef getTestDataset():\n    \n    path_to_test = DIR + '\/test\/'\n    data = pd.read_csv(DIR + '\/sample_submission.csv')\n\n    paths = []\n    labels = []\n    \n    for name in data['Id']:\n        y = np.ones(28)\n        paths.append(os.path.join(path_to_test, name))\n        labels.append(y)\n\n    return np.array(paths), np.array(labels)\n","97ce781a":"# credits: https:\/\/github.com\/keras-team\/keras\/blob\/master\/keras\/utils\/data_utils.py#L302\n# credits: https:\/\/stanford.edu\/~shervine\/blog\/keras-how-to-generate-data-on-the-fly\n\nclass ProteinDataGenerator(keras.utils.Sequence):\n            \n    def __init__(self, paths, labels, batch_size, shape, shuffle = False, use_cache = False, augment = False):\n        self.paths, self.labels = paths, labels\n        self.batch_size = batch_size\n        self.shape = shape\n        self.shuffle = shuffle\n        self.use_cache = use_cache\n        self.augment = augment\n        if use_cache == True:\n            self.cache = np.zeros((paths.shape[0], shape[0], shape[1], shape[2]), dtype=np.float16)\n            self.is_cached = np.zeros((paths.shape[0]))\n        self.on_epoch_end()\n    \n    def __len__(self):\n        return int(np.ceil(len(self.paths) \/ float(self.batch_size)))\n    \n    def __getitem__(self, idx):\n        indexes = self.indexes[idx * self.batch_size : (idx+1) * self.batch_size]\n\n        paths = self.paths[indexes]\n        X = np.zeros((paths.shape[0], self.shape[0], self.shape[1], self.shape[2]))\n        # Generate data\n        if self.use_cache == True:\n            X = self.cache[indexes]\n            for i, path in enumerate(paths[np.where(self.is_cached[indexes] == 0)]):\n                image = self.__load_image(path)\n                self.is_cached[indexes[i]] = 1\n                self.cache[indexes[i]] = image\n                X[i] = image\n        else:\n            for i, path in enumerate(paths):\n                X[i] = self.__load_image(path)\n\n        y = self.labels[indexes]\n                \n        if self.augment == True:\n            seq = iaa.Sequential([\n                iaa.OneOf([\n                    iaa.Fliplr(0.5), # horizontal flips\n                    iaa.Crop(percent=(0, 0.1)), # random crops\n                    # Small gaussian blur with random sigma between 0 and 0.5.\n                    # But we only blur about 50% of all images.\n                    iaa.Sometimes(0.5,\n                        iaa.GaussianBlur(sigma=(0, 0.5))\n                    ),\n                    # Strengthen or weaken the contrast in each image.\n                    iaa.ContrastNormalization((0.75, 1.5)),\n                    # Add gaussian noise.\n                    # For 50% of all images, we sample the noise once per pixel.\n                    # For the other 50% of all images, we sample the noise per pixel AND\n                    # channel. This can change the color (not only brightness) of the\n                    # pixels.\n                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n                    # Make some images brighter and some darker.\n                    # In 20% of all cases, we sample the multiplier once per channel,\n                    # which can end up changing the color of the images.\n                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n                    # Apply affine transformations to each image.\n                    # Scale\/zoom them, translate\/move them, rotate them and shear them.\n                    iaa.Affine(\n                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n                        rotate=(-180, 180),\n                        shear=(-8, 8)\n                    )\n                ])], random_order=True)\n\n            X = np.concatenate((X, seq.augment_images(X), seq.augment_images(X), seq.augment_images(X)), 0)\n            y = np.concatenate((y, y, y, y), 0)\n        \n        return X, y\n    \n    def on_epoch_end(self):\n        \n        # Updates indexes after each epoch\n        self.indexes = np.arange(len(self.paths))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __iter__(self):\n        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n        for item in (self[i] for i in range(len(self))):\n            yield item\n            \n    def __load_image(self, path):\n        R = Image.open(path + '_red.png')\n        G = Image.open(path + '_green.png')\n        B = Image.open(path + '_blue.png')\n        Y = Image.open(path + '_yellow.png')\n\n        im = np.stack((\n            np.array(R), \n            np.array(G), \n            np.array(B),\n            np.array(Y)), -1)\n        \n        im = cv2.resize(im, (SHAPE[0], SHAPE[1]))\n        im = np.divide(im, 255)\n        return im","47ed5709":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model, Model\nfrom keras.layers import Activation, Dropout, Flatten, Dense, Input, Conv2D, MaxPooling2D, BatchNormalization, Concatenate, ReLU, LeakyReLU\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom keras import metrics\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\nimport keras\nimport tensorflow as tf\n\nfrom tensorflow import set_random_seed\nset_random_seed(SEED)","4c131243":"# credits: https:\/\/www.kaggle.com\/guglielmocamporese\/macro-f1-score-keras\n\ndef f1(y_true, y_pred):\n    #y_pred = K.round(y_pred)\n    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\ndef f1_loss(y_true, y_pred):\n    \n    #y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1-K.mean(f1)","075f2abc":"# some basic useless model\ndef create_model(input_shape):\n    \n    dropRate = 0.25\n    \n    init = Input(input_shape)\n    x = BatchNormalization(axis=-1)(init)\n    x = Conv2D(8, (3, 3))(x)\n    x = ReLU()(x)\n    x = BatchNormalization(axis=-1)(x)\n    x = Conv2D(8, (3, 3))(x)\n    x = ReLU()(x)\n    x = BatchNormalization(axis=-1)(x)\n    x = Conv2D(16, (3, 3))(x)\n    x = ReLU()(x)\n    x = BatchNormalization(axis=-1)(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Dropout(dropRate)(x)\n    c1 = Conv2D(16, (3, 3), padding='same')(x)\n    c1 = ReLU()(c1)\n    c2 = Conv2D(16, (5, 5), padding='same')(x)\n    c2 = ReLU()(c2)\n    c3 = Conv2D(16, (7, 7), padding='same')(x)\n    c3 = ReLU()(c3)\n    c4 = Conv2D(16, (1, 1), padding='same')(x)\n    c4 = ReLU()(c4)\n    x = Concatenate()([c1, c2, c3, c4])\n    x = BatchNormalization(axis=-1)(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Dropout(dropRate)(x)\n    x = Conv2D(32, (3, 3))(x)\n    x = ReLU()(x)\n    x = BatchNormalization(axis=-1)(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Dropout(dropRate)(x)\n    x = Conv2D(64, (3, 3))(x)\n    x = ReLU()(x)\n    x = BatchNormalization(axis=-1)(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Dropout(dropRate)(x)\n    x = Conv2D(128, (3, 3))(x)\n    x = ReLU()(x)\n    x = BatchNormalization(axis=-1)(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Dropout(dropRate)(x)\n    #x = Conv2D(256, (1, 1), activation='relu')(x)\n    #x = BatchNormalization(axis=-1)(x)\n    #x = MaxPooling2D(pool_size=(2, 2))(x)\n    #x = Dropout(0.25)(x)\n    x = Flatten()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(28)(x)\n    x = ReLU()(x)\n    x = BatchNormalization(axis=-1)(x)\n    x = Dropout(0.1)(x)\n    x = Dense(28)(x)\n    x = Activation('sigmoid')(x)\n    \n    model = Model(init, x)\n    \n    return model","2af615ca":"model = create_model(SHAPE)\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(1e-03),\n    metrics=['acc',f1])\n\nmodel.summary()","8759d2cf":"paths, labels = getTrainDataset()\n\n# divide to \nkeys = np.arange(paths.shape[0], dtype=np.int)  \nnp.random.seed(SEED)\nnp.random.shuffle(keys)\nlastTrainIndex = int((1-VAL_RATIO) * paths.shape[0])\n\npathsTrain = paths[0:lastTrainIndex]\nlabelsTrain = labels[0:lastTrainIndex]\npathsVal = paths[lastTrainIndex:]\nlabelsVal = labels[lastTrainIndex:]\n\nprint(paths.shape, labels.shape)\nprint(pathsTrain.shape, labelsTrain.shape, pathsVal.shape, labelsVal.shape)\n\ntg = ProteinDataGenerator(pathsTrain, labelsTrain, BATCH_SIZE, SHAPE, use_cache=True, augment = False, shuffle = False)\nvg = ProteinDataGenerator(pathsVal, labelsVal, BATCH_SIZE, SHAPE, use_cache=True, shuffle = False)\n\n# https:\/\/keras.io\/callbacks\/#modelcheckpoint\ncheckpoint = ModelCheckpoint('.\/base.model', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min', period=1)\nreduceLROnPlato = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='min')","438b4639":"epochs = 100\n\nuse_multiprocessing = False # DO NOT COMBINE MULTIPROCESSING WITH CACHE! \nworkers = 1 # DO NOT COMBINE MULTIPROCESSING WITH CACHE! \n\nhist = model.fit_generator(\n    tg,\n    steps_per_epoch=len(tg),\n    validation_data=vg,\n    validation_steps=8,\n    epochs=epochs,\n    use_multiprocessing=use_multiprocessing,\n    workers=workers,\n    verbose=1,\n    callbacks=[checkpoint])","64132040":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('loss')\nax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\nax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\nax[1].set_title('acc')\nax[1].plot(hist.epoch, hist.history[\"f1\"], label=\"Train F1\")\nax[1].plot(hist.epoch, hist.history[\"val_f1\"], label=\"Validation F1\")\nax[0].legend()\nax[1].legend()","50ea2e92":"bestModel = load_model('.\/base.model', custom_objects={'f1': f1}) #, 'f1_loss': f1_loss})","25e1a8a8":"fullValGen = vg","c19b6104":"lastFullValPred = np.empty((0, 28))\nlastFullValLabels = np.empty((0, 28))\nfor i in tqdm(range(len(fullValGen))): \n    im, lbl = fullValGen[i]\n    scores = bestModel.predict(im)\n    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\nprint(lastFullValPred.shape, lastFullValLabels.shape)","47aa11c0":"from sklearn.metrics import f1_score as off1\nrng = np.arange(0, 1, 0.001)\nf1s = np.zeros((rng.shape[0], 28))\nfor j,t in enumerate(tqdm(rng)):\n    for i in range(28):\n        p = np.array(lastFullValPred[:,i]>t, dtype=np.int8)\n        scoref1 = off1(lastFullValLabels[:,i], p, average='binary')\n        f1s[j,i] = scoref1","b1cdb926":"print('Individual F1-scores for each class:')\nprint(np.max(f1s, axis=0))\nprint('Macro F1-score CV =', np.mean(np.max(f1s, axis=0)))","2dde65d6":"plt.plot(rng, f1s)\nT = np.empty(28)\nfor i in range(28):\n    T[i] = rng[np.where(f1s[:,i] == np.max(f1s[:,i]))[0][0]]\nprint('Probability threshold maximizing CV F1-score for each class:')\nprint(T)","3fbbb99e":"pathsTest, labelsTest = getTestDataset()\n\ntestg = ProteinDataGenerator(pathsTest, labelsTest, BATCH_SIZE, SHAPE)\nsubmit = pd.read_csv(DIR + '\/sample_submission.csv')\nP = np.zeros((pathsTest.shape[0], 28))\nfor i in tqdm(range(len(testg))):\n    images, labels = testg[i]\n    score = bestModel.predict(images)\n    P[i*BATCH_SIZE:i*BATCH_SIZE+score.shape[0]] = score","93ae8ec4":"PP = np.array(P)","02e45f7e":"prediction = []\n\nfor row in tqdm(range(submit.shape[0])):\n    \n    str_label = ''\n    \n    for col in range(PP.shape[1]):\n        if(PP[row, col] < T[col]):\n            str_label += ''\n        else:\n            str_label += str(col) + ' '\n    prediction.append(str_label.strip())\n    \nsubmit['Predicted'] = np.array(prediction)\nsubmit.to_csv('4channels_cnn_from_scratch.csv', index=False)","d6abb28f":"# Using in Keras\nLet's try to test the multi_processing.","a445f5eb":"# Full validation\nPerform validation on full validation dataset. Choose appropriate prediction threshold maximalizing the validation F1-score."}}