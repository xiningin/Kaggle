{"cell_type":{"8a01d57b":"code","f18239bd":"code","79ecaaa1":"code","61955545":"code","b5584f0d":"code","1406e254":"code","0e2401f1":"code","da61c55c":"code","1e418047":"code","644e7398":"code","77b8703c":"code","f1a2f491":"code","c43f4559":"code","dff342f5":"code","2c42634d":"code","92198d4e":"code","e0f2ef4e":"code","e78376c9":"code","97f68d7b":"code","e701b11d":"code","87a5bcb5":"code","b5fcb542":"code","121563b9":"code","8fef74b6":"code","635e609b":"code","eb230cb0":"code","8ced2754":"code","e2c12606":"markdown","ee9b0ccb":"markdown","ea46ee4c":"markdown","fcc1de06":"markdown","289d6831":"markdown","0b124583":"markdown","4e8d64c8":"markdown","5f63fd61":"markdown","6aa85395":"markdown","77c3eba0":"markdown","a457bac5":"markdown","793c3ede":"markdown","9ceba3c4":"markdown","e3d6b5a1":"markdown","37bbab6c":"markdown","8ac4e692":"markdown","43d7adf6":"markdown","74a47863":"markdown","b05e41b3":"markdown","b8a7ecac":"markdown","064f8f70":"markdown","08f1d82b":"markdown","4b21ea09":"markdown","eab0c73d":"markdown","9c767580":"markdown","c18cb4dc":"markdown","c1aab24e":"markdown","53a9defd":"markdown"},"source":{"8a01d57b":"#Importing necessary packages in Python \n%matplotlib inline \nimport matplotlib.pyplot as plt \n\nimport numpy as np ; np.random.seed(sum(map(ord, \"aesthetics\")))\nimport pandas as pd\n\nfrom sklearn.datasets import make_classification \nfrom sklearn.learning_curve import learning_curve \n#from sklearn.cross_validation import train_test_split \n#from sklearn.grid_search import GridSearchCV\n#from sklearn.cross_validation import ShuffleSplit\nfrom sklearn.metrics import classification_report,confusion_matrix, roc_curve, roc_auc_score, auc, accuracy_score\nfrom sklearn.model_selection import ShuffleSplit,train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize, StandardScaler, MinMaxScaler\n\nimport seaborn \nseaborn.set_context('notebook') \nseaborn.set_style(style='darkgrid')\n\nfrom pprint import pprint \n \n","f18239bd":"# Function for evaluation reports\ndef get_eval1(clf, X,y):\n    # Cross Validation to test and anticipate overfitting problem\n    scores1 = cross_val_score(clf, X, y, cv=2, scoring='accuracy')\n    scores2 = cross_val_score(clf, X, y, cv=2, scoring='precision')\n    scores3 = cross_val_score(clf, X, y, cv=2, scoring='recall')\n    scores4 = cross_val_score(clf, X, y, cv=2, scoring='roc_auc')\n    \n    # The mean score and standard deviation of the score estimate\n    print(\"Cross Validation Accuracy: %0.2f (+\/- %0.2f)\" % (scores1.mean(), scores1.std()))\n    print(\"Cross Validation Precision: %0.2f (+\/- %0.2f)\" % (scores2.mean(), scores2.std()))\n    print(\"Cross Validation Recall: %0.2f (+\/- %0.2f)\" % (scores3.mean(), scores3.std()))\n    print(\"Cross Validation roc_auc: %0.2f (+\/- %0.2f)\" % (scores4.mean(), scores4.std()))\n    \n    return \n\ndef get_eval2(clf, X_train, y_train,X_test, y_test):\n    # Cross Validation to test and anticipate overfitting problem\n    scores1 = cross_val_score(clf, X_test, y_test, cv=2, scoring='accuracy')\n    scores2 = cross_val_score(clf, X_test, y_test, cv=2, scoring='precision')\n    scores3 = cross_val_score(clf, X_test, y_test, cv=2, scoring='recall')\n    scores4 = cross_val_score(clf, X_test, y_test, cv=2, scoring='roc_auc')\n    \n    # The mean score and standard deviation of the score estimate\n    print(\"Cross Validation Accuracy: %0.2f (+\/- %0.2f)\" % (scores1.mean(), scores1.std()))\n    print(\"Cross Validation Precision: %0.2f (+\/- %0.2f)\" % (scores2.mean(), scores2.std()))\n    print(\"Cross Validation Recall: %0.2f (+\/- %0.2f)\" % (scores3.mean(), scores3.std()))\n    print(\"Cross Validation roc_auc: %0.2f (+\/- %0.2f)\" % (scores4.mean(), scores4.std()))\n    \n    return  \n  \n# Function to get roc curve\ndef get_roc (y_test,y_pred):\n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    fpr, tpr, _ = roc_curve(y_test, y_pred)\n    roc_auc = auc(fpr, tpr)\n    #Plot of a ROC curve\n    plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"upper left\")\n    plt.show()\n    return\n","79ecaaa1":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n#print('XGBoost v',xgb.__version__)\n\n# fit, train and cross validate Decision Tree with training and test data \ndef xgbclf(params, X_train, y_train,X_test, y_test):\n  \n    eval_set=[(X_train, y_train), (X_test, y_test)]\n    \n    model = XGBClassifier(**params).\\\n      fit(X_train, y_train, eval_set=eval_set, \\\n                  eval_metric='auc', early_stopping_rounds = 100, verbose=100)\n        \n    #print(model.best_ntree_limit)\n\n    model.set_params(**{'n_estimators': model.best_ntree_limit})\n    model.fit(X_train, y_train)\n    #print(model,'\\n')\n    \n    # Predict target variables y for test data\n    y_pred = model.predict(X_test, ntree_limit=model.best_ntree_limit) #model.best_iteration\n    #print(y_pred)\n   \n    # Get Cross Validation and Confusion matrix\n    #get_eval(model, X_train, y_train)\n    #get_eval2(model, X_train, y_train,X_test, y_test)\n    \n    # Create and print confusion matrix    \n    abclf_cm = confusion_matrix(y_test,y_pred)\n    print(abclf_cm)\n    \n    #y_pred = model.predict(X_test)\n    print (classification_report(y_test,y_pred) )\n    print ('\\n')\n    print (\"Model Final Generalization Accuracy: %.6f\" %accuracy_score(y_test,y_pred) )\n    \n    # Predict probabilities target variables y for test data\n    y_pred_proba = model.predict_proba(X_test, ntree_limit=model.best_ntree_limit)[:,1] #model.best_iteration\n    get_roc (y_test,y_pred_proba)\n    return model\n\ndef plot_featureImportance(model, keys):\n  importances = model.feature_importances_\n\n  importance_frame = pd.DataFrame({'Importance': list(importances), 'Feature': list(keys)})\n  importance_frame.sort_values(by = 'Importance', inplace = True)\n  importance_frame.tail(10).plot(kind = 'barh', x = 'Feature', figsize = (8,8), color = 'orange')","61955545":"file = '..\/input\/germancreditdata\/german.data'\nurl = \"http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/statlog\/german\/german.data\"\n\nnames = ['existingchecking', 'duration', 'credithistory', 'purpose', 'creditamount', \n         'savings', 'employmentsince', 'installmentrate', 'statussex', 'otherdebtors', \n         'residencesince', 'property', 'age', 'otherinstallmentplans', 'housing', \n         'existingcredits', 'job', 'peopleliable', 'telephone', 'foreignworker', 'classification']\n\ndata = pd.read_csv(file,names = names, delimiter=' ')\nprint(data.shape)\nprint (data.columns)\ndata.head(10)","b5584f0d":"# Binarize the y output for easier use of e.g. ROC curves -> 0 = 'bad' credit; 1 = 'good' credit\ndata.classification.replace([1,2], [1,0], inplace=True)\n# Print number of 'good' credits (should be 700) and 'bad credits (should be 300)\ndata.classification.value_counts()","1406e254":"#numerical variables labels\nnumvars = ['creditamount', 'duration', 'installmentrate', 'residencesince', 'age', \n           'existingcredits', 'peopleliable', 'classification']\n\n# Standardization\nnumdata_std = pd.DataFrame(StandardScaler().fit_transform(data[numvars].drop(['classification'], axis=1)))","0e2401f1":"from collections import defaultdict\n\n#categorical variables labels\ncatvars = ['existingchecking', 'credithistory', 'purpose', 'savings', 'employmentsince',\n           'statussex', 'otherdebtors', 'property', 'otherinstallmentplans', 'housing', 'job', \n           'telephone', 'foreignworker']\n\nd = defaultdict(LabelEncoder)\n\n# Encoding the variable\nlecatdata = data[catvars].apply(lambda x: d[x.name].fit_transform(x))\n\n# print transformations\nfor x in range(len(catvars)):\n    print(catvars[x],\": \", data[catvars[x]].unique())\n    print(catvars[x],\": \", lecatdata[catvars[x]].unique())\n\n#One hot encoding, create dummy variables for every category of every categorical variable\ndummyvars = pd.get_dummies(data[catvars])","da61c55c":"data_clean = pd.concat([data[numvars], dummyvars], axis = 1)\n\nprint(data_clean.shape)","1e418047":"# Unscaled, unnormalized data\nX_clean = data_clean.drop('classification', axis=1)\ny_clean = data_clean['classification']\nX_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_clean,y_clean,test_size=0.2, random_state=1)","644e7398":"X_train_clean.keys()","77b8703c":"params={}\nxgbclf(params, X_train_clean, y_train_clean, X_test_clean, y_test_clean)","f1a2f491":"params={}\n\nparams1={\n    'n_estimators':3000,\n    'objective': 'binary:logistic',\n    'learning_rate': 0.05,\n    'gamma':0.1,\n    'subsample':0.8,\n    'colsample_bytree':0.3,\n    'min_child_weight':3,\n    'max_depth':3,\n    #'seed':1024,\n    'n_jobs' : -1\n}\n\nparams2={\n    'n_estimators':3000,\n    'objective': 'binary:logistic',\n    'learning_rate': 0.005,\n    #'gamma':0.01,\n    'subsample':0.555,\n    'colsample_bytree':0.7,\n    'min_child_weight':3,\n    'max_depth':8,\n    #'seed':1024,\n    'n_jobs' : -1\n}\n\nxgbclf(params2, X_train_clean, y_train_clean, X_test_clean, y_test_clean)","c43f4559":"\nfrom imblearn.over_sampling import SMOTE\n\n# Oversampling\n# http:\/\/contrib.scikit-learn.org\/imbalanced-learn\/auto_examples\/combine\/plot_smote_enn.html#sphx-glr-auto-examples-combine-plot-smote-enn-py\n\n# Apply SMOTE\nsm = SMOTE(ratio='auto')\nX_train_clean_res, y_train_clean_res = sm.fit_sample(X_train_clean, y_train_clean)\n\n# Print number of 'good' credits and 'bad credits, should be fairly balanced now\nprint(\"Before\/After clean\")\nunique, counts = np.unique(y_train_clean, return_counts=True)\nprint(dict(zip(unique, counts)))\nunique, counts = np.unique(y_train_clean_res, return_counts=True)\nprint(dict(zip(unique, counts)))","dff342f5":"#Great, before we do anything else, let's split the data into train\/test.\nX_train_clean_res = pd.DataFrame(X_train_clean_res, columns=X_train_clean.keys())\n#y_train_clean_res = pd.DataFrame(y_train_clean_res)","2c42634d":"print(np.shape(X_train_clean_res))\nprint(np.shape(y_train_clean_res))\nprint(np.shape(X_test_clean)) \nprint(np.shape(y_test_clean))","92198d4e":"#BASE MODEL\nparams={}\nxgbclf(params,X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","e0f2ef4e":"params = {}\n\nparams1={\n    'n_estimators':3000,\n    'objective': 'binary:logistic',\n    'learning_rate': 0.05,\n    'gamma':0.1,\n    'subsample':0.8,\n    'colsample_bytree':0.3,\n    'min_child_weight':3,\n    'max_depth':3,\n    #'seed':1024,\n    'n_jobs' : -1\n}\n\nparams2={\n    'n_estimators':3000,\n    'objective': 'binary:logistic',\n    'learning_rate': 0.005,\n    #'gamma':0.01,\n    'subsample':0.555,\n    'colsample_bytree':0.7,\n    'min_child_weight':3,\n    'max_depth':8,\n    #'seed':1024,\n    'n_jobs' : -1\n}\n\n#xgbclf(params, X_train, y_train,X_test,y_test)\nmodel = xgbclf(params2,X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)\nmodel\n#plot_featureImportance(model, X_train_clean_res.keys())","e78376c9":"#model = xgbclf(params1,X_train_clean_res[importance_col], y_train_clean_res,X_test_clean[importance_col], y_test_clean)\n\nimportances = model.feature_importances_\nimportance_frame = pd.DataFrame({'Importance': list(importances), 'Feature': list(X_train_clean_res.keys())})\nimportance_frame.sort_values(by = 'Importance', inplace = True, ascending=False)\nimportance_col = importance_frame.Feature.head(10).values","97f68d7b":"params = {}\n\nparams1={\n    'n_estimators':3000,\n    'objective': 'binary:logistic',\n    'learning_rate': 0.01,\n    #'gamma':0.1,\n    #'subsample':0.8,\n    #'colsample_bytree':0.3,\n    #'min_child_weight':3,\n    'max_depth':3,\n    #'seed':1024,\n    'n_jobs' : -1\n}\n\nxgbclf(params,X_train_clean_res[importance_col], y_train_clean_res,X_test_clean[importance_col], y_test_clean)","e701b11d":"from sklearn.grid_search import GridSearchCV\n\nprint('XGBoost with grid search')\n# play with these params\nparams={\n    'learning_rate': [0.01, 0.02],\n    'max_depth': [3], # 5 is good but takes too long in kaggle env\n    #'subsample': [0.6], #[0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n    #'colsample_bytree': [0.5], #[0.5,0.6,0.7,0.8],\n    'n_estimators': [50, 100, 200, 300, 400, 500]\n    #'reg_alpha': [0.03] #[0.01, 0.02, 0.03, 0.04]\n}\n\n\nxgb_clf = xgb.XGBClassifier()\n\nrs = GridSearchCV(xgb_clf,\n                  params,\n                  cv=2,\n                  scoring=\"roc_auc\",\n                  n_jobs=1,\n                  verbose=False)\nrs.fit(X_train_clean_res[importance_col], y_train_clean_res)\nbest_est = rs.best_estimator_\nprint(best_est)\nprint(rs.best_score_)\n\n# Roc AUC with test data\nprint(rs.score(X_test_clean[importance_col],y_test_clean))\n\n# Roc AUC with all train data\n#y_pred_proba = best_est.predict_proba(X_test_clean[importance_col])[:,1]\n#print(\"Roc AUC: \", roc_auc_score(y_test_clean, y_pred_proba))\n\n#xgbclf(params1,X_train_clean_res[importance_col], y_train_clean_res,X_test_clean[importance_col], y_test_clean)","87a5bcb5":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\nimport lightgbm as lgb\n\n# fit, train and cross validate Decision Tree with training and test data \ndef lgbclf(X_train, y_train,X_test, y_test):\n\n    model = lgb.LGBMClassifier().fit(X_train, y_train)\n    print(model,'\\n')\n\n    # Predict target variables y for test data\n    y_pred = model.predict_proba(X_test)[:,1]\n\n    # Get Cross Validation and Confusion matrix\n    #get_eval(model, X_train, y_train,y_test,y_pred)\n    #get_eval2(model, X_train, y_train,X_test, y_test,y_pred)\n    get_roc (y_test,y_pred)\n    return\n\n# Logistic Regression\n#lgbclf(X_train, y_train,X_test,y_test)\nlgbclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","b5fcb542":"from sklearn.linear_model import LogisticRegression\n\n# fit, train and cross validate Decision Tree with training and test data \ndef logregclf(X_train, y_train,X_test, y_test):\n    print(\"LogisticRegression\")\n    model = LogisticRegression().fit(X_train, y_train)\n    print(model,'\\n')\n\n    # Predict target variables y for test data\n    y_pred = model.predict_proba(X_test)[:,1]\n\n    # Get Cross Validation and Confusion matrix\n    #get_eval(model, X_train, y_train,y_test,y_pred)\n    #get_eval2(model, X_train, y_train,X_test, y_test,y_pred)\n    get_roc (y_test,y_pred)\n    return\n\n# Logistic Regression\n#logregclf(X_train, y_train,X_test,y_test)\nlogregclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","121563b9":"from sklearn.ensemble import RandomForestClassifier \n\n# fit, train and cross validate Decision Tree with training and test data \ndef randomforestclf(X_train, y_train,X_test, y_test):\n    print(\"RandomForestClassifier\")\n    randomforest = RandomForestClassifier().fit(X_train, y_train)\n    print(randomforest,'\\n')\n    \n    # Predict target variables y for test data\n    y_pred = randomforest.predict_proba(X_test)[:,1]\n\n    # Get Cross Validation and Confusion matrix\n    #get_eval(randomforest, X_train, y_train,y_test,y_pred)\n    get_roc (y_test,y_pred)\n    return\n\n# Random Forest\n# Choose clean data, as tree is robust\nrandomforestclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","8fef74b6":"from sklearn.ensemble import ExtraTreesClassifier\n\n# fit, train and cross validate Decision Tree with training and test data \ndef extratreesclf(X_train, y_train,X_test, y_test):\n    print(\"ExtraTreesClassifier\")\n    extratrees = ExtraTreesClassifier().fit(X_train, y_train)\n    print(extratrees,'\\n')\n    \n    # Predict target variables y for test data\n    y_pred = extratrees.predict_proba(X_test)[:,1]\n\n    # Get Cross Validation and Confusion matrix\n    #get_eval(extratrees, X_train, y_train,y_test,y_pred)\n    \n    get_roc (y_test,y_pred)\n    return\n \n# Extra Trees\n# Choose clean data, as tree is robust\nextratreesclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","635e609b":"from sklearn.tree import DecisionTreeClassifier \n# fit, train and cross validate Decision Tree with training and test data \ndef dectreeclf(X_train, y_train,X_test, y_test):\n    print(\"DecisionTreeClassifier\")\n    dec_tree = DecisionTreeClassifier(min_samples_split=10,min_samples_leaf=5).fit(X_train, y_train)\n    print(dec_tree,'\\n')\n    \n    # Predict target variables y for test data\n    y_pred = dec_tree.predict_proba(X_test)[:,1]\n\n    \n    # Get Cross Validation and Confusion matrix\n    #get_eval(dec_tree, X_train, y_train,y_test,y_pred)\n    get_roc (y_test,y_pred)\n    return\n\n# Decisiontree\ndectreeclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","eb230cb0":"from sklearn.ensemble import GradientBoostingClassifier\n\n# fit, train and cross validate GradientBoostingClassifier with training and test data \ndef gradientboostingclf(X_train, y_train, X_test, y_test):  \n    print(\"GradientBoostingClassifier\")\n    gbclf = GradientBoostingClassifier().fit(X_train, y_train)\n    print(gbclf,'\\n')\n    \n    # Predict target variables y for test data\n    y_pred = gbclf.predict_proba(X_test)[:,1]\n\n    # Get Cross Validation and Confusion matrix\n    #get_eval(gbclf, X_train, y_train,y_test,y_pred)\n    get_roc (y_test,y_pred)\n    return\n  \n# GradientBoostingClassifier\n# Choose clean data, as tree is robust\ngradientboostingclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","8ced2754":"from sklearn.ensemble import AdaBoostClassifier\n\n# fit, train and cross validate GradientBoostingClassifier with training and test data \ndef adaboostclf(X_train, y_train, X_test, y_test):  \n    print(\"AdaBoostClassifier\")\n    abclf = AdaBoostClassifier().fit(X_train, y_train)\n    print(abclf,'\\n')\n    \n    # Predict target variables y for test data\n    y_pred = abclf.predict_proba(X_test)[:,1]\n\n    # Get Cross Validation and Confusion matrix\n    #get_eval(abclf, X_train, y_train,y_test,y_pred)\n    get_roc (y_test,y_pred)\n    return\n\n# AdaBoostClassifier\n# Choose clean data, as tree is robust\nadaboostclf(X_train_clean_res, y_train_clean_res,X_test_clean, y_test_clean)","e2c12606":"### Lighgbm (ROC_AUC:0.73)","ee9b0ccb":"### Import Dataset\n\nOK let's get started. We'll download the data from the UCI website.","ea46ee4c":"# 4.  Feature Selection\n- XGBoost3 (Base Model:ROC_AUC:0.73)\n- GridSearchCV (ROC_AUC:0.70)","fcc1de06":"### Import Library","289d6831":"# Table of Content\n\n**1. [Introduction](#Introduction)** <br>\n    - Import Library\n    - Evaluation Function\n    - XGBoost Model\n**2. [Preprocess](#Preprocess)** <br>\n    - Importing Dataset\n    - StandardScaler\n    - Encoding Categorical Feature\n    - Concate Transformed Dataset\n    - Split Training Dataset\n    - XGBoost  1a: Unbalance Dataset (Base Model: ROC_AUC:0.74)\n    - XGBoost  1b: Unbalance Dataset (ROC_AUC:0.79)\n**3. [Balanced Dataset](#Balanced Dataset)** <br>    \n    - XGBoost 2a: Balanced (Base Model: ROC_AUC:0.77)\n    - **XGBoost 2b: Balanced (ROC_AUC:0.80)**\n**4. [Others](#Others)** <br>  \n    - Lighgbm (ROC_AUC:0.73)\n    - LogisticRegression (ROC_AUC:0.77)\n    - RandomForestClassifier (ROC_AUC:0.69)\n    - ExtraTreesClassifier (ROC_AUC:0.74)\n    - DecisionTreeClassifier (ROC_AUC:0.64)\n    - GradientBoostingClassifier (ROC_AUC:0.76)\n    - AdaBoostClassifier (ROC_AUC:0.72)","0b124583":"The German Credit data set is a publically available data set downloaded from the UCI Machine Learning Repository. The data contains data on 20 variables and the classification whether an applicant is considered a Good or a Bad credit risk for 1000 loan applicants.\n\n### [Data Source](https:\/\/archive.ics.uci.edu\/ml\/datasets\/statlog+(german+credit+data))\n- Professor Dr. Hans Hofmann  \n- Institut f\"ur Statistik und \"Okonometrie  \n- Universit\"at Hamburg  \n- FB Wirtschaftswissenschaften  \n- Von-Melle-Park 5    \n- 2000 Hamburg 13\n\n### Benchmark\n![Credit Risk Classification: Faster Machine Learning with Intel Optimized Packages](https:\/\/i.imgur.com\/nL1l7WI.png)\n\naccording to [1] the best model is Random Forest with balanced feature selection data. it's has Accuracy 82%, Precision 84%, Recall 82% and F1-Score 81%. \n\n<br>\n\n\nThe goal of this kernel is to beat The benchmark with  :\n- Convert dataset to Machine Learning friendly (Feature Engginering)\n- Develop XGBoost model to predict whether a loan is a good or bad risk.\n- Find the Best parameter for XGBoost Model (Hyperparameter Tunning)\n- Beat the Benchmark","4e8d64c8":"### XGBoost 2a: Balanced (Base Model: ROC_AUC:0.77)","5f63fd61":"<a id=\"Preprocess\"><\/a> <br>\n# **2. Preprocess** \n- Importing Dataset\n- StandardScaler\n- Encoding Categorical Feature\n- Concate Transformed Dataset\n- Split Training Dataset\n- XGBoost  1a: Unbalance Dataset (Base Model: ROC_AUC:0.74)\n- XGBoost  1b: Unbalance Dataset (ROC_AUC:0.79)","6aa85395":"<a id=\"Balanced Dataset\"><\/a> <br>\n# **3. Balanced Dataset** \n- XGBoost 2a: Balanced (Base Model: ROC_AUC:0.77)\n- XGBoost 2b: Balanced (ROC_AUC:0.80)","77c3eba0":"### LogisticRegression (ROC_AUC:0.77)","a457bac5":"### Concate Transformed Dataset\nappend the dummy variable of the initial numerical variables numvars# append ","793c3ede":"### Evaluation Function\n","9ceba3c4":"### XGBoost  1b: Unbalance Dataset (ROC_AUC:0.79)","e3d6b5a1":"<a id=\"Introduction\"><\/a> <br>\n# **1. Introduction:** \n- Import Library\n- Evaluation Function\n- XGBoost Model","37bbab6c":"### GridSearchCV (ROC_AUC:0.70)","8ac4e692":"### Split Training Dataset","43d7adf6":"### ExtraTreesClassifier (ROC_AUC:0.74)","74a47863":"### XGBoost 2b: Balanced (ROC_AUC:0.80)","b05e41b3":"<a id=\"Others\"><\/a> <br>\n# 5. Others\n- Lighgbm (ROC_AUC:0.73)\n- LogisticRegression (ROC_AUC:0.77)\n- RandomForestClassifier (ROC_AUC:0.69)\n- ExtraTreesClassifier (ROC_AUC:0.74)\n- DecisionTreeClassifier (ROC_AUC:0.64)\n- GradientBoostingClassifier (ROC_AUC:0.76)\n- AdaBoostClassifier (ROC_AUC:0.72)","b8a7ecac":"### RandomForestClassifier (ROC_AUC:0.69)","064f8f70":"#### XGBoost Model","08f1d82b":"### AdaBoostClassifier (ROC_AUC:0.75)","4b21ea09":"### DecisionTreeClassifier (ROC_AUC:0.64)","eab0c73d":"### XGBoost  1a: Unbalance Dataset (Base Model: ROC_AUC:0.74)","9c767580":"### Encoding Categorical Feature\n\nLabelencoding to transform categorical to numerical, Enables better Visualization than one hot encoding","c18cb4dc":"### StandardScaler","c1aab24e":"### XGBoost3 (Base Model:ROC_AUC:0.73)","53a9defd":"### GradientBoostingClassifier (ROC_AUC:0.76)"}}