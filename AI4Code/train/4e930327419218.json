{"cell_type":{"1ad2cae8":"code","cedae02c":"code","5268e644":"code","e8ca2d0e":"code","64d31cda":"code","faee07f3":"code","0c571a93":"code","13ffc719":"code","feb02ab5":"code","3613b51e":"code","df2f60f6":"code","64bd792d":"code","e37b4ab8":"code","1b5809e0":"code","7a64d30a":"code","d371d967":"code","e9a9df69":"code","1f79119b":"code","ec782f54":"markdown","a96cc58b":"markdown","b7e43ecf":"markdown","8ef2647c":"markdown","d99378ee":"markdown","8bc22411":"markdown","b918dbd2":"markdown","ce3d7433":"markdown","3d6751f1":"markdown","db2df960":"markdown","17b608b0":"markdown","8881d23e":"markdown","3fc1dcd6":"markdown","35ce70cc":"markdown","0b2bdb17":"markdown"},"source":{"1ad2cae8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error, mean_squared_error, precision_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\n#from sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n#from sklearn.naive_bayes import GaussianNB\n#from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n#from xgboost import XGBRegressor\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npath_to_file = '..\/input\/particle-identification-from-detector-responses\/pid-5M.csv'\ndf = pd.read_csv(path_to_file)","cedae02c":"#classifiers = [\n#    KNeighborsClassifier(3),\n#    SVC(kernel=\"linear\", C=0.025),\n#    SVC(gamma=2, C=1),\n#    GaussianProcessClassifier(1.0 * RBF(1.0)),\n#    DecisionTreeClassifier(max_depth=5),\n#    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n#    MLPClassifier(alpha=1, max_iter=1000),\n#    AdaBoostClassifier(),\n#    GaussianNB(),\n#    QuadraticDiscriminantAnalysis()]\n","5268e644":"df.head(5)","e8ca2d0e":"#for the moment reduce the size -> speed up \ndf = df[0:100000]\n#df = df[0:20000]\n\ndf_pi = df[df.id == abs(211)]\ndf_ka = df[df.id == abs(321)]\ndf_pr = df[df.id == abs(2212)]\n\n\ndf.head(10)","64d31cda":"\nfig, ax1 = plt.subplots(figsize=(15,10))\nsns.scatterplot(x=df_pi.p, y=df_pi.beta , data=df, ax = ax1, label = \"Pions\");\nsns.scatterplot(x=df_ka.p, y=df_ka.beta , data=df, ax = ax1, label = \"Kaons\");\nsns.scatterplot(x=df_pr.p, y=df_pr.beta , data=df, ax = ax1, label = \"Protons\");\nax1.set_xlabel(r'Particle momentum $p$',fontsize=22)\nax1.set_ylabel(r'Relativistic velocity $\\beta$', fontsize=22)\nax1.tick_params(axis='both', which='major', labelsize=15)\n\nplt.legend(loc=\"lower right\", fontsize=20)\n\nplt.show()\n","faee07f3":"X_full = df\nX = X_full.drop(columns=['id'],axis = 1)\ny = X_full.id\n# training and validation sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)","0c571a93":"def get_acc(y,pred):\n    corr = 0\n    for i in range(len(y)):\n        if abs(y.iloc[i] - pred[i]) < 0.1*y.iloc[i]:\n            corr+=1\n    return corr\/len(y)","13ffc719":"param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"n_estimators\": [10, 100]\n             }\n\n\n# run grid search\n\n\nDTC = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"balanced\",max_depth = None)\nmodel_ADC = AdaBoostClassifier(base_estimator = DTC)\ngrid_ADC  = GridSearchCV(model_ADC, param_grid=param_grid, scoring = 'roc_auc')\n\n\nmodel_ADC.fit(X_train,y_train) \npred_ADC  = model_ADC.predict(X_test)\n\nacc_ADC   = accuracy_score(y_test,pred_ADC)\nmse_ADC   = mean_squared_error(y_test, pred_ADC)\npre_ADC   = precision_score(y_test, pred_ADC, average='macro')\n\n\n\nprint(acc_ADC)\nprint(mse_ADC)\nprint(pre_ADC)\n\n\n","feb02ab5":"# Create the parameter grid based on the results of random search \n#param_grid_XGB = {\n#    'max_depth': [3,5,9],\n#    'max_features': [3,4,5],\n#    'min_samples_leaf': [2, 3,4],\n#    'min_samples_split': [6, 8, 10],\n#    'n_estimators': [75, 100, 200]\n#}\n\n#model_XGB = XGBClassifier()\n#grid_search_XGB = GridSearchCV(estimator = model_XGB, param_grid = param_grid_XGB, cv = 3, n_jobs = -1, verbose = 2)\n#grid_search_XGB.fit(X_train, y_train)\n#best_grid = grid_search_RF.best_estimator_","3613b51e":"    \nmodel_XGB    = XGBClassifier()\nmodel_XGB.fit(X_train, y_train)\npred_XGB     = model_XGB.predict(X_test)\n\ndata_dmatrix = xgb.DMatrix(data=X,label=y)\nparams       = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.3,'learning_rate': 0.05,'max_depth': 5, 'alpha': 10}\nXGB_cla      = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\nacc_XGB      = accuracy_score(y_test,pred_XGB)\n\nmae_XGB      = mean_absolute_error(y_test, pred_XGB)\nmse_XGB      = mean_squared_error(y_test, pred_XGB)\npre_XGB      = precision_score(y_test, pred_XGB, average='macro')\n\n\n\nprint(acc_XGB)\nprint(mse_XGB)\nprint(pre_XGB)\n\n\nfig, ax = plt.subplots(figsize=(20, 30))\n#xgb.plot_importance(XGB_cla, ax=ax)\nxgb.plot_tree(XGB_cla, ax = ax)\n","df2f60f6":"model_KNN  = KNeighborsClassifier(n_neighbors=2, algorithm='ball_tree')\nmodel_KNN.fit(X_train, y_train)\nmodel_KNN.predict(X_test)\npred_KNN  = model_KNN.predict(X_test)\nacc_KNN   = accuracy_score(y_test, pred_KNN)\n\n\nmae_KNN   = mean_absolute_error(y_test, pred_KNN)\nmse_KNN   = mean_squared_error(y_test, pred_KNN)\npre_KNN   = precision_score(y_test, pred_KNN, average='macro')\n\n\nprint(acc_KNN)\nprint(mse_KNN)\nprint(pre_KNN)\n","64bd792d":"\n\nmodel_MLP = MLPClassifier(alpha=1, max_iter=100)\nmodel_MLP.fit(X_train, y_train)\nmodel_MLP.predict(X_test)\npred_MLP = model_MLP.predict(X_test)\nacc_MLP  = accuracy_score(y_test, pred_MLP)\nmae_MLP  = mean_absolute_error(y_test, pred_MLP)\nmse_MLP  = mean_squared_error(y_test, pred_MLP)\npre_MLP0 = precision_score(y_test, pred_MLP, average='macro', zero_division = 0)\npre_MLP1 = precision_score(y_test, pred_MLP, average='macro', zero_division = 1)\n\n\n\nprint(acc_MLP)\nprint(mse_MLP)\nprint(pre_MLP0)\nprint(pre_MLP1)\n","e37b4ab8":"model_SVC = SVC(kernel=\"linear\", C=0.025)\nmodel_SVC.fit(X_train, y_train)\nmodel_SVC.predict(X_test)\npred_SVC = model_KNN.predict(X_test)\nacc_SVC  = accuracy_score(y_test, pred_SVC)\nmse_SVC  = mean_squared_error(y_test, pred_SVC)\npre_SVC  = precision_score(y_test, pred_SVC, average='macro')\n\n\nprint(acc_SVC)\nprint(mse_SVC)\nprint(pre_SVC)","1b5809e0":"#GaussianProcessClassifier\n\n#super slow -> comment out\n#model_GPC = GaussianProcessClassifier(1.0 * RBF(1.0))\n#model_GPC.fit(X_train, y_train)\n#model_GPC.predict(X_test)\n#pred_GPC = model_GPC.predict(X_test)\n#acc_GPC  = accuracy_score(y_test,pred_GPC)\n#print(acc_GPC)\n","7a64d30a":"model_DT = DecisionTreeClassifier(max_depth=5)\nmodel_DT.fit(X_train, y_train)\nmodel_DT.predict(X_test)\npred_DT = model_DT.predict(X_test)\nacc_DT  = accuracy_score(y_test, pred_DT)\nmse_DT  = mean_squared_error(y_test, pred_DT)\npre_DT  = precision_score(y_test, pred_DT, average='macro')\n\n\nprint(mse_DT)\nprint(acc_DT)\nprint(pre_DT)\n","d371d967":"\n#Grid search for hyperparameter optimisation\n#commented out otherwise it would take too long\n\n# Create the parameter grid based on the results of random search \n#param_grid_RF = {\n#    'bootstrap': [True],\n#    'max_depth': [80, 100, 120],\n#    'max_depth': [3, 5, 7, 10, 20],\n#    'max_features': [3,4],\n#    'min_samples_leaf': [2, 3, 4],\n#    'min_samples_split': [6, 8, 10],\n#    'n_estimators': [100]\n#}\n\n#model_RF = RandomForestClassifier(random_state=0)\n#grid_search_RF = GridSearchCV(estimator = model_RF, param_grid = param_grid_RF, cv = 3, n_jobs = -1, verbose = 2)\n#grid_search_RF.fit(X_train, y_train)\n#best_grid = grid_search_RF.best_estimator_\n#best_grid","e9a9df69":"\n\n\nmodel_RF = Pipeline([\n        ('classifier', RandomForestClassifier(n_estimators =100, min_samples_leaf=2, min_samples_split=8, max_depth=10, max_features=3, random_state=0))\n])\n\nmodel_RF.fit(X_train, y_train)\npred_RF = model_RF.predict(X_test)\nacc_RF  = accuracy_score(y_test, pred_RF)\nmse_RF  = mean_squared_error(y_test, pred_RF)\npre_RF  = precision_score(y_test, pred_RF, average='macro')\n\n\n\n# feature importance\ntree_feature_importances = (\n    model_RF.named_steps['classifier'].feature_importances_)\nsorted_idx = tree_feature_importances.argsort()\n\nfig_size = plt.rcParams[\"figure.figsize\"]\nfig_size[0] = 10\nfig_size[1] = 8\nplt.rcParams[\"figure.figsize\"] = fig_size\ny_ticks = np.arange(0, len(X.columns))\nfig, ax = plt.subplots()\nax.barh(y_ticks, tree_feature_importances[sorted_idx])\nax.set_yticklabels(X.columns[sorted_idx])\nax.set_yticks(y_ticks)\nax.set_title(\"Random Forest Feature Importances\")\nplt.show()\n\n\nprint(acc_RF)\nprint(mse_RF)\nprint(pre_RF)","1f79119b":"print(\"AdaBoost            : Accuracy: %.2f%%, Precision: %.2f%%, mean sq. error : %.2f\" % (acc_ADC * 100.0, pre_ADC * 100.0, mse_ADC))\nprint(\"Decision Tree       : Accuracy: %.2f%%, Precision: %.2f%%, mean sq. error : %.2f\" % (acc_DT * 100.0, pre_DT * 100.0, mse_DT))\nprint(\"XGBoost             : Accuracy: %.2f%%, Precision: %.2f%%, mean sq. error : %.2f\" % (acc_XGB * 100.0, pre_XGB * 100.0, mse_XGB))\nprint(\"KNearNeigh          : Accuracy: %.2f%%, Precision: %.2f%%, mean sq. error : %.2f\" % (acc_KNN * 100.0, pre_KNN * 100.0, mse_KNN))\nprint(\"MLP                 : Accuracy: %.2f%%, Precision: %.2f%%, mean sq. error : %.2f\" % (acc_MLP * 100.0, pre_MLP1 * 100.0, mse_MLP))\nprint(\"Random forest       : Accuracy: %.2f%%, Precision: %.2f%%, mean sq. error : %.2f\" % (acc_RF * 100.0, pre_RF * 100.0, mse_RF))\nprint(\"SVC                 : Accuracy: %.2f%%, Precision: %.2f%%, mean sq. error : %.2f\" % (acc_SVC * 100.0, pre_SVC * 100.0, mse_SVC))\n\n\nlabels = ['', 'AdaBoost','Dec. Tree','XGBoost','k near. neighb.', 'Mult. Lay. Perc.', 'Random For.', 'SVC']\n\n\nfigRes, axRes = plt.subplots(3, sharex=True, figsize=(15,10))\naxRes[0].set_ylabel('Accuracy', fontsize=22)\naxRes[0].tick_params(axis='both', which='major', labelsize=15)\naxRes[0].plot([acc_ADC, acc_DT, acc_XGB, acc_KNN, acc_MLP, acc_RF, acc_SVC])\naxRes[1].set_ylabel('Precision', fontsize=22)\naxRes[1].tick_params(axis='both', which='major', labelsize=15)\naxRes[1].plot([pre_ADC, pre_DT, pre_XGB, pre_KNN, pre_MLP1, pre_RF, pre_SVC])\naxRes[2].set_xlabel(r'ML algorithm',fontsize=22)\naxRes[2].set_ylabel(r'Mean sq. error', fontsize=22)\naxRes[2].tick_params(axis='both', which='major', labelsize=15)\naxRes[2].set_xticklabels(labels)\naxRes[2].plot([mse_ADC, mse_DT, mse_XGB, mse_KNN, mse_MLP, mse_RF, mse_SVC])\n","ec782f54":"# Summary of ML methods","a96cc58b":"# Kinematics plot","b7e43ecf":"# XGBoost","8ef2647c":"# Introduction\n\n\nParticle identification is a crucial component in the understanding of particle collisions. This applies for collisions of elementary particles, such as performed at the Large Electron Positron Collider in the 1990ies, as for hadron collisions at the Large Hadron Collider, which so far delivered proton-proton, proton-lead, Xenon-Xenon and Pb-Pb collisions at unprecedented centre-of-mass energies.\n\nThe given data set has some detectorquantities and some kinematic quantities which can be used to identify particles.\n","d99378ee":"# Adaboost","8bc22411":"# Random Forest","b918dbd2":"# Conclusions","ce3d7433":"# Decision Tree","3d6751f1":"# Multi-Layer Perceptron","db2df960":"It can be seen that the momentum together with the relativistic velocity are important features for thr particle identification as one could also expect from the beta vs momentum plot above.","17b608b0":"> # Support Vector Classification","8881d23e":"# k-Nearest Neighbour","3fc1dcd6":"From the figure above the different particle types can be seen. This is a so-called Time-Of-Flight plot, where on the y-axis is the relativistic velocity beta = v\/c, where v is the velocity and c the speed of light and on the x-axis is the momentum of the particle. Here Monte Carlo information is used to visualize the different particle behaviour due to the different particle masses. Pions which are the lightest hadrons get closer to the limiting speed of light, i.e. beta approximately unity, where the heavier kaons and protons still have smaller velocities. Particles can be in particular separated by their different mass.\n","35ce70cc":"# Data preparation for ML\n\nIn the followind the data is prepared for the ML processing. But it should be mentioned that the entire data sample is used, no particles were rejected due to low quality or similar. The reason is that simply not enough information is given about the different quantities.","0b2bdb17":"A couple of ML methods are tested for the particle identification in Monte Carlo simulations of electron-proton collisions. In general, Random Forests and the XGBoost classifier have a fairly high accuracy, precision and a small mean squared errors. The high precision of the MLP is somehow exaggerated since the values when the precision calculation is ill-defined are set to one. If set to zero, the precision of the MLP drops to about 0.45 (as can be seen above)."}}