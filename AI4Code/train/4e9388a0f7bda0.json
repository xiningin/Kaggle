{"cell_type":{"e29f04d5":"code","3a7a3579":"code","84a1cfe2":"code","248996b5":"code","e39da722":"code","c9b87b35":"code","a5a9b4a5":"code","05c9bd73":"code","645c3884":"code","2b3607a0":"code","dd327213":"code","eff14d8c":"code","9a2eb945":"code","3ebce250":"markdown","3db47781":"markdown","74d2a4c8":"markdown","21a54958":"markdown","c9af3f93":"markdown"},"source":{"e29f04d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3a7a3579":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n\n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            c_sum = df[col].sum()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_sum < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_sum < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_sum < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_sum < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_sum < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_sum < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","84a1cfe2":"import datatable as dt\n\ndf_train = (\n    dt.fread('..\/input\/jane-street-market-prediction\/train.csv')\n      .to_pandas()\n      .query('weight > 0')\n      .pipe(reduce_mem_usage)\n)\n\nfeature_names = df_train.columns[df_train.columns.str.contains('feature')]","248996b5":"id_field = 'ts_id'\ndate_field = 'date'\nfeatstr = [i for i in df_train.columns if 'feature_' in i]\n\ndf_train['action'] = np.where(df_train['resp'] > 0,1,0)\ndf_train = df_train[df_train.weight > 0.0]\ndf_train.fillna(df_train.mean(axis=0), inplace=True)\n\ndf_train.action = df_train.action.astype('category')\ndf_train.feature_0 = df_train.feature_0.astype('category')","e39da722":"df_train.set_index(['ts_id', 'date'], inplace=True)\n\ntarget = 'action'\nfeatures_list = ['weight'] + [feat for feat in df_train.columns if 'feature_' in str(feat)]\nfillna_dict = df_train.mean(axis=0).to_dict()","c9b87b35":"trainval_limit = 300\nvaltest_limit = 400\n\nX_trainval = df_train[df_train.index.get_level_values(1) <= valtest_limit][features_list]\ny_trainval = df_train[df_train.index.get_level_values(1) <= valtest_limit][target]\n\nX_test = df_train[df_train.index.get_level_values(1) > valtest_limit][features_list]\ny_test = df_train[df_train.index.get_level_values(1) > valtest_limit][target]","a5a9b4a5":"categ_vars = ['feature_0']\n\n# Selected based on positive feature importance\nut_pos_features = ['feature_1', 'feature_2', 'feature_3', 'feature_4', \n                   'feature_5', 'feature_6', 'feature_7', 'feature_8', \n                   'feature_10', 'feature_11', 'feature_12', 'feature_14', \n                   'feature_15', 'feature_16', 'feature_17', 'feature_18', \n                   'feature_19', 'feature_20', 'feature_21', 'feature_22', \n                   'feature_23', 'feature_24', 'feature_25', 'feature_26', \n                   'feature_27', 'feature_28', 'feature_29', 'feature_30', \n                   'feature_31', 'feature_32', 'feature_34', 'feature_35', \n                   'feature_36', 'feature_37', 'feature_38', 'feature_39', \n                   'feature_40', 'feature_41', 'feature_42', 'feature_43', \n                   'feature_44', 'feature_45', 'feature_46', 'feature_47', \n                   'feature_48', 'feature_49', 'feature_51', 'feature_52', \n                   'feature_54', 'feature_55', 'feature_56', 'feature_57', \n                   'feature_59', 'feature_60', 'feature_61', 'feature_62', \n                   'feature_63', 'feature_64', 'feature_66', 'feature_68', \n                   'feature_69', 'feature_70', 'feature_71', 'feature_72', \n                   'feature_73', 'feature_74', 'feature_75', 'feature_76', \n                   'feature_77', 'feature_78', 'feature_79', 'feature_82', \n                   'feature_83', 'feature_86', 'feature_87', 'feature_89', \n                   'feature_90', 'feature_91', 'feature_92', 'feature_95', \n                   'feature_96', 'feature_97', 'feature_98', 'feature_99', \n                   'feature_100', 'feature_101', 'feature_102', 'feature_103', \n                   'feature_104', 'feature_105', 'feature_106', 'feature_107', \n                   'feature_108', 'feature_109', 'feature_111', 'feature_112', \n                   'feature_113', 'feature_115', 'feature_116', 'feature_118', \n                   'feature_120', 'feature_121', 'feature_123', 'feature_124', \n                   'feature_125', 'feature_126', 'feature_127', 'feature_129']\n\nif 'feature_0' in ut_pos_features:\n    categ_vars = ['feature_0']\nelse:\n    categ_vars = []","05c9bd73":"def utility_function(X, model):\n    data = X.copy()\n    data = data.reset_index().set_index('ts_id')\n\n    data['action'] = np.round(model.predict_proba(X)[:, 1])\n    \n    data = data.reset_index().merge(df_train.reset_index()[['ts_id','resp']], how='left', on='ts_id').set_index('ts_id')\n    if 'weight' not in list(data.columns):\n        data = data.reset_index().merge(df_train.reset_index()[['ts_id','weight']], how='left', on='ts_id').set_index('ts_id')\n\n    data['prod'] = data['weight'] * data['resp'] * data['action']\n    data_agg = data.groupby('date')['prod'].sum()\n\n    t = data_agg.sum() \/ np.sqrt(np.power(data_agg, 2).sum()) * np.sqrt(250 \/ len(data_agg))\n\n    u = min(max(t, 0), 6) * data_agg.sum()\n    \n    return u \n\n# Check\n# utility_function(X_test_hyperopt, model)","645c3884":"def predict_compute_metrics(X, y, model):\n    \n    predictions = model.predict_proba(X)[:, 1]\n    predictions_l = model.predict(X)\n\n    print(\"Utility function: \" + str(np.round(utility_function(X, model), decimals=4)))\n    \n    return predictions","2b3607a0":"from catboost import CatBoostClassifier\n\n# Following HyperOpt Run\nmodel_tv = CatBoostClassifier(border_count=70, \n                             class_weights=(1, 0.9710117916727243),\n                             depth=5,\n                             iterations=875,\n                             l2_leaf_reg=37.89277395675028,\n                             learning_rate=0.0683720060874196,\n                             logging_level='Silent',\n                             task_type='CPU')\n\nmodel_tv.fit(X_trainval.loc[:,ut_pos_features], \n             y_trainval, \n             cat_features = categ_vars) ","dd327213":"print(\"Train & validation: \")\npred_train = predict_compute_metrics(X_trainval.loc[:,ut_pos_features], y_trainval, model_tv)\nprint(\"Test: \")\npred_test = predict_compute_metrics(X_test.loc[:,ut_pos_features], y_test, model_tv)","eff14d8c":"import janestreet\nfrom tqdm.notebook import tqdm\n\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\nfor (test_df, sample_prediction_df) in tqdm(iter_test):\n    sample_prediction_df.action = model_tv.predict(test_df.loc[:,ut_pos_features]) # make your 0\/1 prediction here\n    env.predict(sample_prediction_df)","9a2eb945":"!rm -r \/kaggle\/working\/catboost_info","3ebce250":"# Model Fitting & Results","3db47781":"# Data Import","74d2a4c8":"# Model Evaluation Functions","21a54958":"# Train, Validation & Test Split","c9af3f93":"# Data Adjustments"}}