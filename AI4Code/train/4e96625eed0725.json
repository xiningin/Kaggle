{"cell_type":{"697c7098":"code","1e7e5aca":"code","a53dad45":"code","d1a4e28c":"code","4004368c":"code","eb6d60f7":"code","87110920":"code","baf9eadf":"code","e1db0418":"code","7b0623e9":"code","1dce70f8":"code","81bf5161":"code","e87d2f20":"code","2b9bc8a8":"code","be2eb6f7":"code","c333935c":"code","a8c4b662":"code","173523cc":"code","5c609634":"code","89f1916a":"code","a0b249ed":"code","e38d0840":"code","13e0c5d5":"code","c40c6ee2":"code","954d35d9":"code","63e5440f":"code","c8044de5":"code","9d6aa69b":"code","c8b66000":"code","fe5dafec":"code","c260c43e":"code","86af6e31":"code","3f54f467":"code","43eec107":"code","28a9f5f0":"code","2ad71eba":"code","04f8eb72":"code","abdaaae3":"code","e87ab766":"code","2b1fb92a":"markdown","dbae3eae":"markdown","4d65fd37":"markdown","08f80b27":"markdown"},"source":{"697c7098":"\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nimport time\nimport random\n\nrandom.seed(100)\n","1e7e5aca":"\n# Importing the dataset\nwine = pd.read_csv('..\/input\/winequality-red.csv')","a53dad45":"wine.head()","d1a4e28c":"# #Making binary classificaion for the response variable.\nfrom sklearn.preprocessing import LabelEncoder\nbins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\nwine['quality'] = pd.cut(wine['quality'], bins = bins, labels = group_names)\nlabel_quality = LabelEncoder()\nwine['quality'] = label_quality.fit_transform(wine['quality'])\nwine['quality'].value_counts()\n","4004368c":"\n#plotting the response variable\nsns.countplot(wine['quality'])\n","eb6d60f7":"wine.columns","87110920":"sns.pairplot(wine)","baf9eadf":"wine[wine.columns[:11]].describe()","e1db0418":"## Histograms\nfig = plt.figure(figsize=(15, 12))\nplt.suptitle('Histograms of Numerical Columns', fontsize=20)\nfor i in range(wine.shape[1]):\n    plt.subplot(6, 3, i + 1)\n    f = plt.gca()\n    f.set_title(wine.columns.values[i])\n\n    vals = np.size(wine.iloc[:, i].unique())\n    if vals >= 100:\n        vals = 100\n    \n    plt.hist(wine.iloc[:, i], bins=vals, color='#3F5D7D')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","7b0623e9":"wine.isna().any()","1dce70f8":"#Correlation with Quality with respect to attributes\nwine.corrwith(wine.quality).plot.bar(\n        figsize = (20, 10), title = \"Correlation with quality\", fontsize = 15,\n        rot = 45, grid = True)","81bf5161":"## Correlation Matrix\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = wine.corr()","e87d2f20":"corr.head()","2b9bc8a8":"# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","be2eb6f7":"#Assigning and dividing the dataset\nX = wine.drop('quality',axis=1)\ny=wine['quality']","c333935c":"X.head()","a8c4b662":"y.head()","173523cc":"wine.columns[:11]","5c609634":"features_label = wine.columns[:11]","89f1916a":"#Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 200, criterion = 'entropy', random_state = 0)\nclassifier.fit(X, y)\nimportances = classifier.feature_importances_\nindices = np. argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i],importances[indices[i]]))","a0b249ed":"plt.title('Feature Importances')\nplt.bar(range(X.shape[1]),importances[indices], color=\"green\", align=\"center\")\nplt.xticks(range(X.shape[1]),features_label, rotation=90)\nplt.xlim([-1, X.shape[1]])\nplt.show()","e38d0840":"\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection  import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 5)","13e0c5d5":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train2 = pd.DataFrame(sc.fit_transform(X_train))\nX_test2 = pd.DataFrame(sc.transform(X_test))\nX_train2.columns = X_train.columns.values\nX_test2.columns = X_test.columns.values\nX_train2.index = X_train.index.values\nX_test2.index = X_test.index.values\nX_train = X_train2\nX_test = X_test2","c40c6ee2":"#Using Principal Dimensional Reduction\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 4)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\nprint(pd.DataFrame(explained_variance))","954d35d9":"#### Model Building ####\n\n### Comparing Models\n\n## Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0, penalty = 'l1')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['Logistic Regression', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nprint(results)","63e5440f":"## SVM (Linear)\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'linear')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (Linear)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","c8044de5":"## SVM (rbf)\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'rbf')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (RBF)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","9d6aa69b":"## Randomforest\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 100,\n                                    criterion = 'entropy')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest (n=100)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","c8b66000":"## K-fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X= X_train, y = y_train,\n                             cv = 10)\nprint(\"Random Forest Classifier Accuracy: %0.2f (+\/- %0.2f)\"  % (accuracies.mean(), accuracies.std() * 2))","fe5dafec":"# Applying Grid Search\n\n# Round 1: Entropy\nparameters = {\"max_depth\": [3, None],\n              \n              'min_samples_split': [2, 5, 10],\n              'min_samples_leaf': [1, 5, 10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"entropy\"]}\n\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(estimator = classifier, # Make sure classifier points to the RF model\n                           param_grid = parameters,\n                           scoring = \"accuracy\",\n                           cv = 10,\n                           n_jobs = -1)\n\nt0 = time.time()\ngrid_search = grid_search.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Took %0.2f seconds\" % (t1 - t0))\n\nrf_best_accuracy = grid_search.best_score_\nrf_best_parameters = grid_search.best_params_\nrf_best_accuracy, rf_best_parameters","c260c43e":"# Round 2: Entropy\nparameters = {\"max_depth\": [None],\n             \n              'min_samples_split': [8, 10, 12],\n              'min_samples_leaf': [1, 2, 3],\n              \"bootstrap\": [True],\n              \"criterion\": [\"entropy\"]}\n\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(estimator = classifier, # Make sure classifier points to the RF model\n                           param_grid = parameters,\n                           scoring = \"accuracy\",\n                           cv = 10,\n                           n_jobs = -1)\n\nt0 = time.time()\ngrid_search = grid_search.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Took %0.2f seconds\" % (t1 - t0))\n\nrf_best_accuracy = grid_search.best_score_\nrf_best_parameters = grid_search.best_params_\nrf_best_accuracy, rf_best_parameters\n","86af6e31":"# Round 1: Gini\nparameters = {\"max_depth\": [3, None],\n              \n              'min_samples_split': [2, 5, 10],\n              'min_samples_leaf': [1, 5, 10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\"]}\n# Make sure classifier points to the RF model\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(estimator = classifier, \n                           param_grid = parameters,\n                           scoring = \"accuracy\",\n                           cv = 10,\n                           n_jobs = -1)\n\nt0 = time.time()\ngrid_search = grid_search.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Took %0.2f seconds\" % (t1 - t0))\n\nrf_best_accuracy = grid_search.best_score_\nrf_best_parameters = grid_search.best_params_\nrf_best_accuracy, rf_best_parameters","3f54f467":"# Round 2: Gini\nparameters = {\"max_depth\": [None],\n              \n              'min_samples_split': [2, 3, 4],\n              'min_samples_leaf': [8, 10, 12],\n              \"bootstrap\": [True],\n              \"criterion\": [\"gini\"]}\n\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(estimator = classifier, # Make sure classifier points to the RF model\n                           param_grid = parameters,\n                           scoring = \"accuracy\",\n                           cv = 10,\n                           n_jobs = -1)\n\nt0 = time.time()\ngrid_search = grid_search.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Took %0.2f seconds\" % (t1 - t0))\n\nrf_best_accuracy = grid_search.best_score_\nrf_best_parameters = grid_search.best_params_\nrf_best_accuracy, rf_best_parameters\n","43eec107":"#So the best grid seacrh is for Gini round 1 so run the classifier of that then run the prediction results","28a9f5f0":"rf_best_accuracy, rf_best_parameters","2ad71eba":"# Predicting Test Set\ny_pred = grid_search.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest (n=100, GSx2 + Gini)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nresults","04f8eb72":"import matplotlib.pyplot as plt\nimport itertools\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n","abdaaae3":"# Making the Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n\n# Plot normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()\n","e87ab766":"#Let's see how our model performed\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","2b1fb92a":"# Exploratory Data Analysis and Visualisation","dbae3eae":"The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n\nThese datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).\nContent\nFor more information, read [Cortez et al., 2009].\nInput variables (based on physicochemical tests):\n1 - fixed acidity \n2 - volatile acidity \n3 - citric acid \n4 - residual sugar \n5 - chlorides \n6 - free sulfur dioxide \n7 - total sulfur dioxide \n8 - density \n9 - pH \n10 - sulphates \n11 - alcohol \nOutput variable (based on sensory data): \n12 - quality (score between 0 and 10) \nWhat might be an interesting thing to do, is aside from using regression modelling, is to set an arbitrary cutoff for your dependent variable (wine quality) at e.g. 7 or higher getting classified as 'good\/1' and the remainder as 'not good\/0'. This allows you to practice with hyper parameter tuning on e.g. decision tree algorithms looking at the ROC curve and the AUC value. Without doing any kind of feature engineering or overfitting you should be able to get an AUC of .88 (without even using random forest algorithm)\n\nKNIME is a great tool (GUI) that can be used for this.\n1 - File Reader (for csv) to linear correlation node and to interactive histogram for basic EDA.\n2- File Reader to 'Rule Engine Node' to turn the 10 point scale to dichtome variable (good wine and rest), the code to put in the rule engine is something like this:\n- $quality$ > 6.5 => \"good\"\n- TRUE => \"bad\" \n3- Rule Engine Node output to input of Column Filter node to filter out your original 10point feature (this prevent leaking)\n4- Column Filter Node output to input of Partitioning Node (your standard train\/tes split, e.g. 75%\/25%, choose 'random' or 'stratified')\n5- Partitioning Node train data split output to input of Train data split to input Decision Tree Learner node and \n6- Partitioning Node test data split output to input Decision Tree predictor Node\n7- Decision Tree learner Node output to input Decision Tree Node input\n8- Decision Tree output to input ROC Node.. (here you can evaluate your model base on AUC value)","4d65fd37":"# Model Evalutation","08f80b27":"# Feature Enggineering"}}