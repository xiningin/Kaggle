{"cell_type":{"01607914":"code","98169446":"code","4c9f2c0d":"code","024f50c8":"code","b431407f":"code","9871ea1d":"code","203c3639":"code","c6cd635a":"code","f0ddd60e":"code","99a31b2e":"markdown","604f1c4c":"markdown","3846f023":"markdown","56cdc984":"markdown","685b2fcb":"markdown"},"source":{"01607914":"import numpy as np\nimport pandas as pd\nimport re","98169446":"# cleaned_df from zeya\ntrain = pd.read_csv('\/kaggle\/input\/train-token\/cleaned_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/train-token\/cleaned_test.csv')","4c9f2c0d":"POI_in_raw_address = 75351\ntrain['POI_in_raw_address'] = np.where(train['POI'].isnull(), False, train.apply(lambda x: str(x.POI) in str(x.raw_address), axis=1))\nmapping_improved_POI = train['POI_in_raw_address'].sum() >= POI_in_raw_address\nprint(\"Mapping improved POI is in raw_address:\", mapping_improved_POI, train['POI_in_raw_address'].sum())\n\nstreet_in_raw_address = 212470\ntrain['street_in_raw_address'] = np.where(train['street'].isnull(), False, train.apply(lambda x: str(x.street) in str(x.raw_address), axis=1))\nmapping_improved_street = train['street_in_raw_address'].sum() >= street_in_raw_address\nprint(\"Mapping improved street is in raw_address:\",mapping_improved_street, train['street_in_raw_address'].sum())","024f50c8":"def tokenize_address_by_sep(df, sep = ' '):\n    df['raw_address'] = df['raw_address'].str.replace(\",\",\", \")\n    df['raw_address'] = df['raw_address'].str.replace(\", \",\" , \")\n    df['raw_address'] = df['raw_address'].str.replace(\"  \",\" \")\n    data = pd.concat([pd.Series(row['id'], row['raw_address'].split(sep))\n                      for _, row in df.iterrows()]).reset_index()\n    data.columns = ['word', 'sentence_idx']\n    data['tag'] = np.nan\n    \n    # auto tag all commas as comma\n    data['tag'] = np.where(data['word']==',', 'comma', data['tag'])\n    \n    data = data[['sentence_idx','word','tag']]\n    return data","b431407f":"test_tokens = tokenize_address_by_sep(test)\ntest_tokens['tag'] = test_tokens['tag'].astype(str)\ntest_tokens.to_csv('test_tokens.csv', index=False)","9871ea1d":"def tag_using_col(train_tokens, col):\n    train_tokens = train_tokens.merge(train[['id', col]].dropna(), how='left', left_on='sentence_idx', right_on='id')\n\n    if col == 'street':\n        # check if word is in street, split(\" \") to ensure that only full words are captured and substrings of words are ignored, eg. \"a\"\n        train_tokens['word_in_street'] = np.where(train_tokens[col].notnull(),\n                                                  train_tokens.apply(lambda x: str(x.word) in str(x.street).split(\" \"), axis=1),\n                                                  False)\n        # tag word as street\n        train_tokens['tag'] = np.where(train_tokens['word_in_street'], 'street_cont', train_tokens['tag'])\n    else:\n        # check if word is in POI, split(\" \") to ensure that only full words are captured and substrings of words are ignored, eg. \"a\"\n        train_tokens['word_in_POI'] = np.where(train_tokens[col].notnull(),\n                                               train_tokens.apply(lambda x: str(x.word) in str(x.POI).split(\" \"), axis=1),\n                                               False)\n        # tag word as POI\n        train_tokens['tag'] = np.where(train_tokens['word_in_POI'], 'POI_cont', train_tokens['tag'])\n                \n    # get first word of target col\n    train_tokens[col+'START'] = train_tokens[col].str.split(\" \").str[0]\n    # assign first word of target col as <target>_start            \n    train_tokens['tag'] = np.where(train_tokens['word']==train_tokens[col+'START'], col+'_start', train_tokens['tag'])\n    \n    # if word is longer than one word, assign an end\n    train_tokens[col+'END'] = train_tokens[col].str.split(\" \").str[-1]\n    train_tokens['tag'] = np.where((train_tokens['word']==train_tokens[col+'END'])&(train_tokens[col].str.count(' ')!=0), col+'_end', train_tokens['tag'])\n        \n    \n    train_tokens = train_tokens.drop(columns=['id', col, col+'START', col+'END', 'word_in_'+col])\n    return train_tokens","203c3639":"train_tokens = tokenize_address_by_sep(train)\ntrain_tokens = tag_using_col(train_tokens, 'POI')\ntrain_tokens = tag_using_col(train_tokens, 'street')\ntrain_tokens = train_tokens[train_tokens['word'].notnull()].reset_index(drop=True)\ntrain_tokens['tag'] = train_tokens['tag'].astype(str)\n\n# ensure that there are no duplicates in sentence_idx of test_tokens in train_tokens\ntrain_tokens['sentence_idx'] = train_tokens['sentence_idx']+len(test)\ntrain_tokens.to_csv('train_tokens.csv', index=False)","c6cd635a":"train_tokens['tag'].value_counts()","f0ddd60e":"train_tokens['tag'] = train_tokens['tag'].str.split(\"_\").str[0]\ntag_seq = train_tokens[train_tokens['tag']!='comma'].drop_duplicates(subset=['sentence_idx', 'tag']).groupby('sentence_idx')['tag'].apply(list).reset_index(name='tag_seq')\ntag_seq['tag_seq'] = tag_seq['tag_seq'].astype(str)\ntag_seq['tag_seq'].value_counts()","99a31b2e":"# Split each raw_address by space","604f1c4c":"# Load [Cleaned Data](https:\/\/www.kaggle.com\/zeyalt\/scl-2021-data-science-part-1-data-cleaning)","3846f023":"# Unique sequences in train","56cdc984":"# Tag each word if they matches the POI or street\n* first word in street = street_start\n* if street has more than one word\n    * last word in street = street_end\n    * words between first word and last word = stree_cont","685b2fcb":"# Verify that cleaning the data has improved matches\nThe is important because we can only tag each word based on exact matches"}}