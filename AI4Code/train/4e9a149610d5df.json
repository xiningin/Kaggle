{"cell_type":{"f686c94b":"code","b6992808":"code","65de29dc":"code","ff972301":"code","f712e11f":"code","d56f55f6":"code","c71b3f5f":"code","dfab7cdb":"code","a26caab8":"code","ec0d17a3":"code","bc2cab90":"code","72a996f7":"code","55ce1e73":"code","a3e5305d":"code","ddb82c3b":"code","db198a83":"code","ec3f8c51":"code","56e658e7":"code","5da4f9bd":"code","30178fb0":"code","869088a0":"code","4542d488":"code","9023b6eb":"code","8a6e3fd4":"code","9800c84f":"code","d3af91d9":"markdown","1ed31563":"markdown","12861952":"markdown","3a04ff96":"markdown","2a22d095":"markdown","badadff9":"markdown","558b7429":"markdown","ed24eed5":"markdown","48ebd739":"markdown","1ea55637":"markdown","71914451":"markdown","6576d0bb":"markdown","12ee6e34":"markdown"},"source":{"f686c94b":"%matplotlib inline\nimport numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n#import mathplotlib\nimport sklearn\nfrom pathlib import Path","b6992808":"#Reproduserbar notebook - ny boks\nseed = 42\nnp.random.seed(seed)","65de29dc":"#Sjekker om notebooken kj\u00f8rer p\u00e5 Colab - ny boks\nif 'google.colab' in str(get_ipython()):\n    print('Notebooken kj\u00f8rer p\u00e5 Colab. colab=True.')\n    colab=True\nelse:\n    print('Notebooken kj\u00f8rer ikke p\u00e5 Colab. colab=False')\n    calab=False","ff972301":"#train = pd.read_csv('https:\/\/raw.githubusercontent.com\/alu042\/DAT801\/master\/KaggleInClass\/train.csv')\n#test = pd.read_csv('https:\/\/raw.githubusercontent.com\/alu042\/DAT801\/master\/KaggleInClass\/test.csv')\n#sampleSubmission = pd.read_csv('https:\/\/raw.githubusercontent.com\/alu042\/DAT801\/master\/KaggleInClass\/sampleSubmission.csv')","f712e11f":"DATA = Path('..\/input\/dat801-2021-assignment1')\nlist(DATA.iterdir())","d56f55f6":"#Start:\ntrain = pd.read_csv(DATA\/'train.csv')\ntest = pd.read_csv(DATA\/'test.csv')\nsampleSubmission = pd.read_csv(DATA\/'sample_submission.csv')","c71b3f5f":"X = train.iloc[:,:-1]\ny = train.iloc[:,-1]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","dfab7cdb":"#Imputering\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(strategy='mean')\nX_train_imputed = imp.fit_transform(X_train)\nX_train_imputed = pd.DataFrame(data=X_train_imputed, columns=X_train.columns)\n\nX_test_imputed = imp.fit_transform(X_test)\nX_test_imputed = pd.DataFrame(data=X_test_imputed, columns=X_test.columns)","a26caab8":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=42, n_estimators=480)\nrf.fit(X_train_imputed, y_train)","ec0d17a3":"y_pred = rf.predict(X_test_imputed)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","bc2cab90":"#Ny boks - litt knn (fra handwritten digits) etc - kan fjernes\n# Import necessary modules\nfrom sklearn.neighbors import KNeighborsClassifier\n#from sklearn.model_selection import train_test_split\nfrom sklearn import datasets\n#import matplotlib.pyplot as plt\n\n# Load the digits dataset: digits\ndigits = datasets.load_digits()\n\n# Print the keys and DESCR of the dataset\nprint(digits.keys())\nprint(digits.DESCR)\n\n# Print the shape of the images and data keys\nprint(digits.images.shape)\nprint(digits.data.shape)\n\n# Display digit 1010\nplt.imshow(digits.images[1], cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()\n\n# Create feature and target arrays\nX = digits.data\ny = digits.target\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42, stratify=y)\n\n# Create a k-NN classifier with 16 neighbors: knn\nknn = KNeighborsClassifier(n_neighbors = 16)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Print the accuracy\nprint(knn.score(X_test, y_test))\n\n\n# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 20)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors = k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n","72a996f7":"svaret = rf.predict(test)\nsvar_data = pd.DataFrame()\nsvar_data['id'] = test['id']\nsvar_data['target'] = svaret\n\n#S\u00e5nn","55ce1e73":"svar_data","a3e5305d":"#train = pd.read_csv(DATA\/'train.csv')\n#test = pd.read_csv(DATA\/'test.csv')\n#sampleSubmission = pd.read_csv(DATA\/'sample_submission.csv')","ddb82c3b":"#train.head()","db198a83":"train","ec3f8c51":"#train.info()\ntrain.isnull().sum().sum()","56e658e7":"train['target'].value_counts()","5da4f9bd":"#test.head()","30178fb0":"test.info()","869088a0":"sampleSubmission.head()","4542d488":"target = np.random.choice(range(9), size=len(test))\ntarget","9023b6eb":"submission = pd.DataFrame({'id': test['id'], 'target': target})","8a6e3fd4":"submission.head()","9800c84f":"submission.to_csv('submission.csv', index=False)","d3af91d9":"Vi kan s\u00e5 bygge en dataframe via en Python dictionary med n\u00f8kler (\"keys\") 'id' og 'target':","1ed31563":"# Ta en titt p\u00e5 data","12861952":"# Submission","3a04ff96":"Etter at du har trent din modell \/ dine modeller og produsert prediksjoner p\u00e5 test-data, s\u00e5 m\u00e5 du lage en CSV-fil p\u00e5 korrekt form som kan scores av Kaggle. Den skal ha samme form som `sample_submission.csv`:","2a22d095":"# Utforsk data, preprosesser data, utforsk modeller (\"model selection\"), prediker & evaluer","badadff9":"Det er 50.000 test-instanser, og det ser ikke ut til \u00e5 mangle noen features. ","558b7429":"Jeg overlater disse delene til dere. Du b\u00f8r utforske features via statistikk og plots, fors\u00f8ke \u00e5 avdekke sammenhenger mellom features og mellom features og labels ved \u00e5 unders\u00f8ke korrelasjoner og lignende, og gjerne fors\u00f8ke \u00e5 konstruere nye features ved \u00e5 kombinere eksisterende. F\u00f8r du kan trene modeller m\u00e5 du s\u00f8rge for at data er p\u00e5 en egnet form. Du m\u00e5 blant annet finne ut hvordan du best kan takle manglende verdier (\"missing values\"). Husk \u00e5 designe et grundig evalueringsoppsett som du kan bruke underveis i utforskingen av modeller. Kryss-validering er naturlig. B\u00f8r du kanskje vurdere stratifisert splitting i trening- og validerings-data? Hva med ensembling av flere modeller?","ed24eed5":"## Hvis du bruker Kaggle","48ebd739":"Denne kan lagres som en CSV vi kan laste opp p\u00e5 Kaggle for scoring:","1ea55637":"Vi ser at det er 150.000 instanser i treningsdata. Det mangler noen verdier i hver av features (som er noe en m\u00e5 takle ved hjelp av imputering eller lignende).","71914451":"# Setup","6576d0bb":"Her er en fremgangsm\u00e5te:\n\nVi trenger f\u00f8rst en vektor med tilfeldige prediksjoner:","12ee6e34":"# Last inn data"}}