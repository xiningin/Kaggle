{"cell_type":{"0f4e9f0f":"code","570e4dda":"code","8b021d28":"code","8969f428":"code","72deba0c":"code","aa3af816":"code","6fa5e5f9":"code","58552e5b":"code","12f8c261":"code","1b4d5aa5":"code","d5882b5d":"code","bc2bb6c3":"code","3ee26360":"code","bb02c284":"code","1d2fa5db":"code","fd20b7c3":"code","d1067429":"code","31993caa":"code","3081ee8c":"code","b7eb2c2b":"code","a8ad93e6":"code","0696362e":"code","41ad1eeb":"code","c630d008":"markdown","868b60e3":"markdown","e48d033f":"markdown","fdefabea":"markdown","ec64cdc2":"markdown","53c87ea3":"markdown","61261da8":"markdown","d4f14277":"markdown","d8e2baa8":"markdown","93e22507":"markdown","a7fda6d0":"markdown","2ec746e6":"markdown","a705a651":"markdown","d8764b06":"markdown","d3fa5701":"markdown","8625abdb":"markdown","6ba4224f":"markdown","7a2d74d9":"markdown","30befb50":"markdown","17ec2c81":"markdown","ac309543":"markdown","25be5e97":"markdown","1a6f734e":"markdown","a64b6d5d":"markdown","a6461bd6":"markdown","bad3cc3f":"markdown","c1aa0023":"markdown","345f699f":"markdown","bb3619a7":"markdown","e399cbae":"markdown","ab04b000":"markdown"},"source":{"0f4e9f0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","570e4dda":"# Create Salary Dataset\n\nyr_x = [2,3,5,13,8,16,11,1,9]\nsalary_y = [15, 28,42,64, 50,90,58,8,54]\nsalary_df = pd.DataFrame({\"yr_Exp\":yr_x, \"salary\":salary_y})","8b021d28":"salary_df[\"(x-xAvg)\"] = salary_df[\"yr_Exp\"] - np.mean(salary_df[\"yr_Exp\"])\nsalary_df[\"(y-yAvg)\"] = salary_df[\"salary\"] - np.mean(salary_df[\"salary\"])\nsalary_df[\"(x-xAvg)(y-yAvg)\"] = salary_df[\"(x-xAvg)\"]*salary_df[\"(y-yAvg)\"]\nsalary_df[\"(x-xAvg)^2\"] = salary_df[\"(x-xAvg)\"]*salary_df[\"(x-xAvg)\"]","8969f428":"salary_df.head()","72deba0c":"# Plot Salary vs Years\n\nfig, ax = plt.subplots()\nax.scatter(salary_df[\"yr_Exp\"], salary_df[\"salary\"], c=\"b\")\nplt.title(\"Salary Dataset\", fontsize=16)\nplt.xlabel(\"Years\", fontsize=14)\nplt.ylabel(\"Salary 10,000s\", fontsize=14)\nplt.show()","aa3af816":"beta_1 = np.sum(salary_df[\"(x-xAvg)(y-yAvg)\"])\/np.sum(salary_df[\"(x-xAvg)^2\"])\nprint(\"The slope of the line is : \",beta_1)","6fa5e5f9":"beta_0 = np.mean(salary_df[\"salary\"]) - beta_1*np.mean(salary_df[\"yr_Exp\"])\nprint(\"The intercept of the line is : \",beta_0)\nprint(\"\\nSo the Linear Equation is y = {0:0.2f}x + {1:0.2f}\".format(beta_1,beta_0))","58552e5b":"# Some dummy data\nx = salary_df[\"yr_Exp\"]\ny = salary_df[\"salary\"]\n\n# Find the slope and intercept of the best fit line\n\n# Create a list of values in the best fit line\nols_predictions = [beta_1 * i + beta_0 for i in x]\n\n# Plot the best fit line over the actual values\nplt.scatter(x,y)\nplt.plot(x, ols_predictions, 'r')\nplt.title(\"Salary Dataset\", fontsize=16)\nplt.xlabel(\"Years\", fontsize=14)\nplt.ylabel(\"Salary 10,000s\", fontsize=14)\nplt.show()","12f8c261":"# Create Normal Equation Function\ndef NormalEquation(X,Y):\n    beta = np.linalg.inv(X.T @ X) @ (X.T @ Y)\n    return beta","1b4d5aa5":"X = np.array([np.ones(len(salary_df[\"yr_Exp\"])), salary_df[\"yr_Exp\"]]).T\nY = salary_df[\"salary\"][:, np.newaxis]","d5882b5d":"X","bc2bb6c3":"Y","3ee26360":"NormalEquation(X,Y)","bb02c284":"# y = b1x + b0\n# b1 is slope, b0 is y-intercept\n\ndef compute_cost_for_line_given_points(b0, b1, points):\n    totalError = 0\n    \n    for i in range(0, len(points)):\n        x = points[i, 0]\n        y = points[i, 1]\n        totalError += (y - (b1 * x + b0)) ** 2\n    return totalError \/ (2*float(len(points)))","1d2fa5db":"def step_gradient(b0_current, b1_current, points, learningRate):\n    b0_gradient = 0\n    b1_gradient = 0\n    N = float(len(points))\n\n    for i in range(0, len(points)):\n        x = points[i, 0]\n        y = points[i, 1]\n        b0_gradient += -(2\/N) * (y - ((b1_current * x) + b0_current))\n        b1_gradient += -(2\/N) * x * (y - ((b1_current * x) + b0_current))\n    new_b0 = b0_current - (learningRate * b0_gradient)\n    new_b1 = b1_current - (learningRate * b1_gradient)\n    return [new_b0, new_b1]","fd20b7c3":"def gradient_descent(points, starting_b0, starting_b1, learning_rate, num_iterations):\n    b0 = starting_b0\n    b1 = starting_b1\n    cost_history = np.zeros(num_iterations)  # create a vector to store the cost history\n    for i in range(num_iterations):\n        cost_history[i] = compute_cost_for_line_given_points(b0, b1, points) # compute and record the cost\n        b0, b1 = step_gradient(b0, b1, points, learning_rate)\n\n    return [b0, b1], cost_history","d1067429":"salary_df.head()","31993caa":"points = np.ones(shape=(len(salary_df[\"yr_Exp\"]), 2))\n\npoints[:, 0] = salary_df[\"yr_Exp\"]\npoints[:, 1] = salary_df[\"salary\"]","3081ee8c":"points.shape","b7eb2c2b":"learning_rate = 0.008\ninitial_b0 = np.random.randn(1,1)*0.01\ninitial_b1 = np.random.randn(1,1)*0.01\nnum_iterations = 2000\n\nprint(\"Starting gradient descent at b0 = {0}, b1 = {1}, error = {2}\".format(initial_b0, initial_b1, compute_cost_for_line_given_points(initial_b0, initial_b1, points)))\nprint(\"Running...\")","a8ad93e6":"[b0, b1], _ = gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)\n\nprint(\"After {0} iterations b0 = {1}, b1 = {2}, error = {3}\".format(num_iterations, b0, b1, compute_cost_for_line_given_points(b0, b1, points)))","0696362e":"num_iterations = 3000\nlearning_rates = [0.000008, 0.000005, 0.00001]\n\ninitial_b0 = np.random.randn(1,1)*0.01\ninitial_b1 = np.random.randn(1,1)*0.01\n\nfor lr in learning_rates:\n    _, cost_history = gradient_descent(points, initial_b0, initial_b1, lr, num_iterations)\n    plt.plot(cost_history, linewidth=2)\n\nplt.title(\"Gradient descent with different learning rates\", fontsize=16)\nplt.xlabel(\"number of iterations\", fontsize=14)\nplt.ylabel(\"cost\", fontsize=14)\nplt.legend(list(map(str, learning_rates)))","41ad1eeb":"learning_rate = 0.025\nnum_iterations = 200\n\ninitial_b0 = np.random.randn(1,1)*0.01\ninitial_b1 = np.random.randn(1,1)*0.01\n\n_, cost_history = gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)\n\nplt.plot(cost_history, linewidth=2)\nplt.title(\"Gradient descent with learning rate = \" + str(learning_rate), fontsize=12)\nplt.xlabel(\"number of iterations\", fontsize=14)\nplt.ylabel(\"cost\", fontsize=14)\nplt.grid()\nplt.show()","c630d008":"Find the slope ($\\beta_1$)","868b60e3":"<br>\n$$\n\\begin{align}\n\\hat y_i = \\beta_0 + \\beta_1 x_i\n\\end{align}\n$$\n<br>\n\n$\\hat y_i$ is the predicted value of $y_i$ for input value of $x_i$.\n<br>\n<br>\nCost function $J(\\beta_0,\\beta_1)$ = $\\frac {1}{2 n}\\sum_{i=1}^n (\\hat y_i - y_i)^2$\n<br>\n<br>\nOur goal is to minimize cost function $J(\\beta_0,\\beta_1)$ for $\\beta_0$ and $\\beta_1$.\n<br>\n<br>\n\\begin{align}\n&\\frac {\\partial J(\\beta_0,\\beta_1)}{\\partial \\beta_0} = \\frac {\\partial (\\frac {1}{2n} \\sum (\\beta_0+\\beta_1x_i - y_i)^2)}{\\partial \\beta_0}\\\\\\\\\n&\\frac {\\partial J(\\beta_0,\\beta_1)}{\\partial \\beta_0} = \\frac {1}{2n} .2 \\sum (\\beta_0+\\beta_1x_i - y_i).\\frac {\\partial (\\beta_0 + \\beta_1.x_i - y_i)}{\\partial \\beta_0}\\\\\\\\\n&\\frac {\\partial J(\\beta_0,\\beta_1)}{\\partial \\beta_0} = \\frac {1}{n}\\sum (\\beta_0+\\beta_1x_i - y_i)\\\\\\\\\n&\\frac {\\partial J(\\beta_0,\\beta_1)}{\\partial \\beta_0} = \\frac {1}{n}\\sum (\\hat y_i - y_i)\\tag{11}\\\\\\\\\n&\\frac {\\partial J(\\beta_0,\\beta_1)}{\\partial \\beta_1} = \\frac {\\partial (\\frac {1}{2n} \\sum (\\beta_0+\\beta_1x_i - y_i)^2)}{\\partial \\beta_1}\\\\\\\\\n&\\frac {\\partial J(\\beta_0,\\beta_1)}{\\partial \\beta_1} = \\frac {1}{2n} .2 \\sum (\\beta_0+\\beta_1x_i - y_i).\\frac {\\partial (\\beta_0 + \\beta_1.x_i - y_i)}{\\partial \\beta_1}\\\\\\\\\n&\\frac {\\partial J(\\beta_0,\\beta_1)}{\\partial \\beta_1} = \\frac {1}{n}\\sum (\\beta_0+\\beta_1x_i - y_i)(0 + x_i- 0 )\\\\\\\\\n&\\frac {\\partial J(\\beta_0,\\beta_1)}{\\partial \\beta_1} = \\frac {1}{n}\\sum (\\hat y_i - y_i)x_i\\tag{12}\\\\\n\\end{align}","e48d033f":"\n<br>\nlet's start with the simple case first. Consider the following simple linear regression function:\n\n\\begin{align}\ny_i & =\\beta_0+\\beta_1x_i+\\epsilon_i&\\;\\;\\;\\;\\;\\;\\; \\text for\\;\\; i=1, ... , n\n\\end{align}\n<br>\nIf we actually let i = 1, ..., n, we see that we obtain n equations:\n\n\n\\begin{align}\ny_1  & =\\beta_0+\\beta_1x_1+\\epsilon_1\\\\\ny_2  & =\\beta_0+\\beta_1x_2+\\epsilon_2\\\\\n\\vdots\\\\\ny_n  & = \\beta_0+\\beta_1x_n+\\epsilon_n\\\\\n\\end{align}\n\n\n<br>\n<br>\n\n\n\\begin{equation}\n\\begin{bmatrix}\ny_1\\\\\ny_2\\\\\n\\vdots\\\\\ny_n\\\\\n\\end{bmatrix}_{n \\times 1}\n\\begin{gathered}\n=\n&\n\\end{gathered}\n\\begin{bmatrix}\n1 & x_1\\\\\n1 & x_2\\\\\n\\vdots&\\vdots\\\\\n1&x_n\n\\end{bmatrix}_{n \\times 2}\n\\begin{gathered}\n*\n&\n\\end{gathered}\n\\begin{bmatrix}\n\\beta_0\\\\\n\\beta_1\\\\\n\\end{bmatrix}_{2 \\times 1}\n\\begin{gathered}\n&\n+\n&\n\\end{gathered}\n\\begin{bmatrix}\n\\epsilon_1\\\\\n\\epsilon_2\\\\\n\\vdots\\\\\n\\epsilon_n\\\\\n\\end{bmatrix}_{n \\times 1}\n\\end{equation}\n<br>\n<br>\n\\begin{gathered}\nY = X\\beta + \\varepsilon \n\\end{gathered}\n\n<br>\n\n\n<p>Now, what does this statement mean? Well, here's the answer:<\/p>\n\n- $X$  is an n \u00d7 2 matrix.\n- $Y$ is an n \u00d7 1 column vector.\n- $\\beta$ is a 2 \u00d7 1 column vector of parameters.\n- $\\varepsilon$ denotes n \u00d7 1 column vector of residuals.\n- The matrix $X$ and vector $\\beta$ are multiplied together using the techniques of *matrix multiplication*.\n- And, the vector $X\\beta$ is added to the vector $\\varepsilon$ using the techniques of *matrix addition*.\n","fdefabea":"Let's refine equation 4 further.\n\\begin{align}\n{\\beta_1} & = \\frac {(Bn - YX)}{(An - X^2)}\\tag{4}\\\\\\\\\n{} & = \\frac {n \\sum x_iy_i - \\sum y_i \\sum x_i}{n \\sum x_i^2 - (\\sum x_i)^2}\\\\\\\\\n{} & = \\frac {n \\sum x_iy_i - \\sum y_i \\sum x_i - \\sum x_i \\sum y_i + \\sum x_i \\sum y_i}{n \\sum x_i^2 + n^2(\\bar{x})^2 - 2 n^2(\\bar{x})^2}\\\\\\\\\n{} & = \\frac {n (\\sum x_iy_i - \\bar{y} \\sum x_i - \\bar x \\sum y_i + n\\bar x \\bar y)}{n \\sum x_i^2 + n^2(\\bar{x})^2 - 2 n^2(\\bar{x})^2}\\\\\\\\\n{} & = \\frac {n (\\sum x_iy_i - \\bar{y} \\sum x_i - \\bar x \\sum y_i + n\\bar x \\bar y)}{n (\\sum x_i^2 + n (\\bar{x})^2 - 2 \\bar{x} \\sum x_i)}\\\\\\\\\n{} & = \\frac {(\\sum x_iy_i - \\bar{y} \\sum x_i - \\bar x \\sum y_i + n\\bar x \\bar y)}{(\\sum x_i^2 + n (\\bar{x})^2 - 2 \\bar{x} \\sum x_i)}\\\\\\\\\n{} & = \\frac {\\sum( x_iy_i - x_i\\bar{y} - \\bar x y_i + \\bar x \\bar y)}{\\sum(x_i^2 + (\\bar{x})^2 - 2 x_i \\bar{x})}\\\\\\\\\n{} & = \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2}\\\\\\\\\n{\\beta_1} & = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\tag{6}\\\\\n\\end{align}","ec64cdc2":"![alt text](https:\/\/drive.google.com\/uc?id=1ISqYlDj87Uhr-oqsTbZne1IG1qIeTIFp)","53c87ea3":"### Find Beta Parameters (Slope and Intercept)","61261da8":"#### Background work\n<br>\n<br>\n1.\n<br>\n\\begin{align}\n&\\sum_{i=0}^n \\varepsilon^T.\\varepsilon = \\sum_{i=0}^n \\varepsilon^2 &\\text{how \n??}\\\\\n\\end{align}\n\n<br>\n\\begin{equation}\n\\varepsilon =\\begin{bmatrix}\n\\epsilon_1\\\\\n\\epsilon_2\\\\\n\\vdots\\\\\n\\epsilon_n \n\\end{bmatrix}_{n \\times 1}\n\\begin{gathered}\n,\n&\n\\end{gathered}\n\\varepsilon^T=\\begin{bmatrix}\n\\epsilon_1 & \n\\epsilon_2 &\\ldots&\\epsilon_n \n\\end{bmatrix}_{1 \\times n}\n\\end{equation}\n<br>\n\n\\begin{align}\n&\\sum_{i=0}^n \\varepsilon^T.\\varepsilon = \\epsilon_1^2 + \\epsilon_2^2 + ... +\\epsilon_n^n\\\\\n&\\sum_{i=0}^n \\varepsilon^T.\\varepsilon = \\sum_{i=0}^n \\varepsilon^2\n\\end{align}\n\n\n<br>\n<br>\n2.\n<br>\n\n\\begin{align}\n&\\frac{\\partial (\\beta^T.X^T.X.\\beta)}{\\partial \\beta} = \\frac{\\partial (\\beta^T.A.\\beta)}{\\partial \\beta} \\;\\;\\;\\text{where A is $X^T.X$}\\\\\n&\\text {By Rule}\\\\\n&\\\\\n&\\frac{\\partial (\\beta^T.A.\\beta)}{\\partial \\beta} = \\beta^T.(A+A^T)\\\\\n&\\text {So}\\\\\n&\\\\\n&\\frac{\\partial (\\beta^T.X^T.X.\\beta)}{\\partial \\beta} = \\beta^T.(X^T.X + (X^T.X)^T)\\\\\n&\\frac{\\partial (\\beta^T.X^T.X.\\beta)}{\\partial \\beta} =\\beta^T.(X^T.X + X^T.X)\\\\\n&\\frac{\\partial (\\beta^T.X^T.X.\\beta)}{\\partial \\beta} =2\\;\\beta^T.X^T.X\\\\\n\\end{align}\n\n","d4f14277":"Find the intercept ($\\beta_0$)","d8e2baa8":"Assume X = $\\sum x_i$,  Y = $\\sum y_i$ , A = $\\sum x_i^2$ ,  B = $\\sum x_iy_i$ and C = $\\sum y_i^2$  \n<br>\n\\begin{align}\nE(\\beta_0,\\beta_1) = C + \\beta_1^2A + \\beta_0^2n - 2 \\beta_1B - 2 \\beta_0Y + 2\\beta_0\\beta_1X\n\\end{align}  \n<br>\nWe would like to find the value of $\\beta_0$, $\\beta_1$ such that E($\\beta_0,\\beta_1$) results in minimum error.  \n<br>\nFind the value of $\\beta_1$ that minimize E, so we take the derivative of E with respect to $\\beta_1$\n\n<br>\n\n\\begin{align}\n\\frac {\\partial E(\\beta_0,\\beta_1)}{\\partial \\beta_1} = 2 \\beta_1A - 2 B + 2  \\beta_0 X\\\\\\\\\n\\end{align}\nwe know that if there is a minimum value for E, it will occur when $\\frac {\\partial E(\\beta0,\\beta_1)}{\\partial \\beta_1} = 0$  \n<br>\n$2 \\beta_1A - 2 B + 2  \\beta_0 X = 0 \\tag{1}$  \nsimilarily $\\frac {\\partial E(\\beta0,\\beta_1)}{\\partial \\beta_0}$ will also have to be 0.\n\\begin{align}\n\\frac {\\partial E(\\beta_0,\\beta_1)}{\\partial \\beta_0} = 2 \\beta_0n - 2 Y + 2  \\beta_1 X\\\\\\\\\n\\end{align}\n$\\frac {\\partial E(\\beta0,\\beta_1)}{\\partial \\beta_0} = 0$ so,\n\\begin{align}\n2 \\beta_0n - 2 Y + 2  \\beta_1 X = 0 \\tag{2}\\\\\\\\\n\\beta_0n = Y - \\beta_1 X\\\\\\\\\n\\mathbf {\\beta_0 = \\frac {(Y - \\beta_1 X)}{n}}\\tag{3}\n\\end{align}\n","93e22507":"\\begin{equation}\nX^T.Y =\\begin{bmatrix}\n1 & 1 & \\dots & 1\\\\\nx_1 & x_2 & \\dots & x_n\\\\ \n\\end{bmatrix}_{2 \\times n}\n\\begin{gathered}\n.\n&\n\\end{gathered}\n\\begin{bmatrix}\ny_1\\\\\ny_2\\\\\n\\vdots\\\\\ny_n\\\\ \n\\end{bmatrix}_{n \\times 1}\n\\end{equation}\n\n\\begin{equation}\nX^T.Y =\\begin{bmatrix}\n\\sum y_i\\\\\n\\sum x_iy_i\\tag{9}\\\\ \n\\end{bmatrix}\n\\begin{gathered}\n=\n\\end{gathered}\n\\begin{bmatrix}\nn \\bar y\\\\\n\\sum x_iy_i\\tag{9}\\\\ \n\\end{bmatrix}\n\\end{equation}\n\n<br>\nSo as per equation 7, 8 and 9\n<br>\n\\begin{align}\n\\beta = (X^T.X)^{-1}.(X^T.Y)\n\\end{align}\n<br>\n\\begin{equation}\n\\beta = \\frac {1}{n(\\sum x_i^2 - n\\bar x^2)}\\begin{bmatrix}\n\\sum x_i^2 & -n\\bar x\\\\\n-n\\bar x & n\n\\end{bmatrix}\n\\begin{gathered}\n.\n\\end{gathered}\n\\begin{bmatrix}\nn \\bar y\\\\\n\\sum x_iy_i\\\\\n\\end{bmatrix}\n\\end{equation}\n<br>\n\\begin{equation}\n\\beta = \\frac {1}{n(\\sum x_i^2 - n\\bar x^2)}\\begin{bmatrix}\nn \\bar y \\sum x_i^2 - n\\bar x \\sum x_iy_i\\\\\n-n^2\\bar x \\bar y + n \\sum x_i y_i\n\\end{bmatrix}\n\\end{equation}\n<br>\n\\begin{equation}\n\\beta = \\frac {1}{(\\sum x_i^2 - n\\bar x^2)}\\begin{bmatrix}\n\\bar y \\sum x_i^2 - \\bar x \\sum x_iy_i\\\\\n\\sum x_i y_i-n\\bar x \\bar y\n\\end{bmatrix}\n\\end{equation}\n<br>\n\\begin{equation}\n\\beta = \\frac {1}{(\\sum x_i^2 - n\\bar x^2)}\\begin{bmatrix}\n\\bar y \\sum x_i^2 - n\\bar y \\bar x^2 + n\\bar y \\bar x^2 - \\bar x \\sum x_iy_i\\\\\n\\sum x_i y_i - n\\bar x \\bar y - n\\bar x \\bar y + n\\bar x \\bar y\\\\\n\\end{bmatrix}\n\\end{equation}\n<br>\n\\begin{equation}\n\\beta = \\frac {1}{(\\sum x_i^2 - n\\bar x^2)}\\begin{bmatrix}\n\\bar y (\\sum x_i^2 - n\\bar x^2) - \\bar x(\\sum x_iy_i - n\\bar y \\bar x)\\\\\n\\sum x_i y_i - \\sum x_i \\bar y - \\bar x \\sum y_i + n\\bar x \\bar y\\\\\n\\end{bmatrix}\n\\end{equation}\n<br>\n\\begin{equation}\n\\beta = \\frac {1}{(\\sum x_i^2 + n\\bar x^2- 2 n\\bar x^2)}\\begin{bmatrix}\n\\bar y (\\sum x_i^2 - n\\bar x^2) - \\bar x(\\sum x_iy_i - n\\bar y \\bar x - n\\bar y \\bar x + n\\bar y \\bar x)\\\\\n\\sum x_i y_i - \\sum x_i \\bar y - \\bar x \\sum y_i + n\\bar x \\bar y\\\\\n\\end{bmatrix}\n\\end{equation}\n<br>\n\\begin{equation}\n\\beta = \\frac {1}{(\\sum x_i^2 + n\\bar x^2- 2 \\bar x \\sum x_i)}\\begin{bmatrix}\n\\bar y (\\sum x_i^2 - n\\bar x^2) - \\bar x(\\sum x_iy_i - \\sum y_i \\bar x - \\bar y \\sum x_i + n\\bar y \\bar x)\\\\\n\\sum x_i y_i - \\sum x_i \\bar y - \\bar x \\sum y_i + n\\bar x \\bar y\\\\\n\\end{bmatrix}\n\\end{equation}\n<br>\n\\begin{equation}\n\\beta = \\frac {1}{\\sum (x_i^2 + \\bar x^2- 2 \\bar x x_i)}\\begin{bmatrix}\n\\bar y (\\sum x_i^2 - n\\bar x^2) - \\bar x(\\sum (x_iy_i - y_i \\bar x - \\bar y x_i + \\bar y \\bar x))\\\\\n\\sum (x_i y_i - x_i \\bar y - \\bar x  y_i + \\bar x \\bar y)\\\\\n\\end{bmatrix}\n\\end{equation}\n<br>\n\\begin{equation}\n\\beta = \\frac {1}{\\sum (x_i - \\bar x)^2}\\begin{bmatrix}\n\\bar y (\\sum x_i^2 - n\\bar x^2) - \\bar x(\\sum (x_i - \\bar x)(y_i - \\bar y))\\\\\n\\sum (x_i - \\bar x)(y_i - \\bar y)\\\\\n\\end{bmatrix}\n\\end{equation}\n<br>\n\\begin{equation}\n\\beta = \\begin{bmatrix}\n\\bar y  - \\bar x\\beta_1\\\\\n\\beta_1\\\\\n\\end{bmatrix}\n\\end{equation}\n<br>\nNote $(\\sum x_i^2 - n\\bar x^2)$ is equal to $\\sum(x_i - \\bar x)^2$ and $\\beta_1 = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$.\n<br>\n\n\\begin{equation}\n\\beta = \\begin{bmatrix}\n\\beta_0\\\\\n\\beta_1 \\tag{10}\\\\\n\\end{bmatrix}\n\\end{equation}","a7fda6d0":"Let's find out $\\beta$ :\n<br>\n<br>\n\\begin{align}\n\\beta = (X^T.X)^{-1}.(X^T.Y)\\tag{7}\\\\\\\\\n\\end{align}\n\nLets' take this equation further\n\\begin{equation}\nX^T.X =\\begin{bmatrix}\n1 & 1 & \\dots & 1\\\\\nx_1 & x_2 & \\dots & x_n\\\\ \n\\end{bmatrix}_{2 \\times n}\n\\begin{gathered}\n.\n&\n\\end{gathered}\n\\begin{bmatrix}\n1 & x_1\\\\\n1 & x_2\\\\\n\\vdots&\\vdots\\\\\n1 & x_n\\\\ \n\\end{bmatrix}_{n \\times 2}\\\\\n\\end{equation}\n\n\\begin{equation}\nX^T.X =\\begin{bmatrix}\n\\sum 1 & \\sum x_i\\\\\n\\sum x_i & \\sum x_i^2 \\\\ \n\\end{bmatrix}\n\\end{equation}\n\n<br>\n\n\\begin{equation}\n\\text{As if     }A =\\begin{bmatrix}\na & b\\\\\nc & d\\\\ \n\\end{bmatrix}\n\\begin{gathered}\n&\n\\end{gathered}\n\\text{then   }A^{-1} = \\frac {1}{|A|}\\begin{bmatrix}\nd & -b\\\\\n-c & a\\\\ \n\\end{bmatrix}\n\\end{equation}\n<br>\n\\begin{equation}\n\\text{So    }A^{-1} =\\frac {1}{ab-bc}\\begin{bmatrix}\nd & -b\\\\\n-c & a\\\\ \n\\end{bmatrix}\n\\end{equation}\n<br>\nNow we will be applying this formula on $(X^T.X)^{-1}$","2ec746e6":"### Linear Regression Using Normal Equation Method","a705a651":"### Gradient Descent with Different Learning Rates\nNotice how the smaller the learning rate takes more iterations to minimize the cost function","d8764b06":"#### Large Learning Rate\nIf we use too large value of a learning rate then the cost function may never converge to a minimum","d3fa5701":"Let's refine equation 3 further.\n\\begin{align}\n{\\beta_0} & = \\frac {(Y - \\beta_1 X)}{n}\\tag{3}\\\\\n{} & = \\frac {1}{n}Y - \\frac {1}{n}\\beta_1X\\\\\n{} & = \\frac {1}{n}\\sum y_i - \\frac {1}{n}\\beta_1\\sum x_i\\\\\\\\\n{\\beta_0} & = \\bar {y} - \\beta_1 \\bar{x}\\tag{5}\\\\\\\\\n\\end{align}","8625abdb":"## Gradient Descent  \n\n### Gradient Descent is the process of minimizing a function by following the gradients of the cost function.  \n\n### This involves *knowing the form of the cost as well as the derivative so that from a given point you know the gradient and can move in that direction*, e.g. downhill towards the minimum value.  \n\n### In this optimization algorithm, each training instance is shown to the model one at a time. The model makes a prediction for a training instance, the error is calculated and the model is updated in order to reduce the error for the next prediction. This process is repeated for a fixed number of iterations.\n\n### This procedure can be used to find the set of coefficients in a model that result in the smallest error for the model on the training data. Each iteration, the coefficients $\\beta$ in machine learning language are updated using the equation:  \n\n$\\beta = \\beta - learning\\_rate * error * x$  \n\nWhere \n- $\\beta$ is the coefficient or weight being optimized, \n- learning_rate is a step size (e.g. 0.01), \n- error is the prediction error for the model on the training data attributed to the weight, \n- and x is the input value.  \n\n\n\n","6ba4224f":"### Linear Regression Using Gradient Descent","7a2d74d9":"#### Compute $\\beta_0$ and $\\beta_1$","30befb50":"# Linear Regression\n\n### Linear Regression is a method to model the relationship between a set of independent variables X (features, predictors, explanatory variables) and a dependent variable\/target variable Y.  \n\n# $$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$$\n\nWhere \n- $y_i$ is dependent\/target variable, \n- $\\beta_0$ is intercept on y axis, \n- $\\beta_1$ is slope, \n- $x_i$ is independent variable and \n- $\\epsilon_i$ is random error term.  \n\n$\\beta_0$, $\\beta_1$ are also called as model coefficients.  \n\n#Objective :\n##Want to fit a \"Best\" line to the data points that shows linear relation.  \n\n##Best Line :  \n### - Passing through as many points as possible.\n### - Minimize the residuals for each point i.e. minimize the error between actual data point and predicted data point. ","17ec2c81":"\\begin{equation}\n(X^T.X)^{-1} = \\frac {1}{n\\sum x_i^2 - n^2\\bar x^2}\\begin{bmatrix}\n\\sum x_i^2 & -n\\bar x\\\\\n-n\\bar x & n\\\\ \n\\end{bmatrix}\n\\end{equation}\n\n\\begin{equation}\n(X^T.X)^{-1} = \\frac {1}{n(\\sum x_i^2 - n\\bar x^2)}\\begin{bmatrix}\n\\sum x_i^2 & -n\\bar x\\\\\n-n\\bar x & n \\tag{8}\\\\ \n\\end{bmatrix}\n\\end{equation}","ac309543":"<br>\n\\begin{equation*}\n\\beta = (X^TX)^{-1}(X^TY)\n\\end{equation*}\n<br>\nIn the above equation,<br>\n$\\beta$ : hypothesis parameters that define it the best.<br>\nX : Input feature value of each instance.<br>\nY : Output value of each instance.  \n<br>","25be5e97":"### What are the Loss & Cost functions: \nThe loss function *computes the error for a single training example* while the cost function is the *average of the loss functions of the entire training set*. \n\nIn Andrew Ng's words-\n\"Finally, the loss function was defined with respect to a single training example. It measures *how well you're doing on a single training example*. I'm now going to define something called *the cost function, which measures how well you're doing an entire training set.*\"\n","1a6f734e":"### Linear Regression Using OLS Example","a64b6d5d":"#### Plot the best fit line","a6461bd6":"Using equation 11 and 12 Gradient Descent algorithm can be defined as  \n<br>\n\nrepeat until convergence  {\n<br>\n<br>\n$\\beta_j  = \\beta_j - \\alpha.\\frac {\\partial J(\\beta_0,\\beta_1)}{\\partial \\beta_j}$\n<br>\nfor j = 0, 1 \n<br>\n<br>\n}\n<br>\nHere $\\alpha$ is a learning rate.\n<br>\n<br>\nrepeat until convergence  {\n<br>\n<br>\n$\\beta_0  = \\beta_0 - \\alpha.\\frac {1}{n}\\sum (\\hat y_i - y_i)$\n<br>\n$\\beta_1  = \\beta_1 - \\alpha.\\frac {1}{n}\\sum (\\hat y_i - y_i)x_i$\n<br>\n<br>\n$\\beta_0$ and $\\beta_1$ should be updated simultaneously.\n<br>\n<br>\n}","bad3cc3f":"<br>\n<br>\n$$\n\\begin{gathered}\nY = X\\beta + \\varepsilon \n\\end{gathered}\n$$\n<p> Here \\(\\varepsilon\\) denotes n * 1 vector of residuals.\n\\begin{gathered}\n\\varepsilon = Y - X\\beta   \\\\\n\\end{gathered}\n<\/p>\n<p>We denotes transposition of matrices by $(^T)$<\/p>\n\n\\begin{equation}\n\\varepsilon =\\begin{bmatrix}\n\\epsilon_1\\\\\n\\epsilon_2\\\\\n\\vdots\\\\\n\\epsilon_n \n\\end{bmatrix}_{n \\times 1}\n\\begin{gathered}\n,\n&\n\\end{gathered}\n\\varepsilon^T=\\begin{bmatrix}\n\\epsilon_1 & \n\\epsilon_2 &\\ldots&\\epsilon_n \n\\end{bmatrix}_{1 \\times n}\n\\end{equation}\n\n<br>\n\n\\begin{align}\n&\\text{In least squared residual approach for the bivariate linear regression}\\\\\n&\\text{model, First, we calculate the sum of squared residuals and,second,}\\\\\n&\\text{find a set of estimators that minimize the sum. Thus, the minimizing}\\\\\n&\\text{problem of the sum of the squared residuals in matrix form is}\\\\\n\\end{align}\n\n<br>\n\n\\begin{align}\n{S(\\beta)} & = \\sum_{i=0}^n \\varepsilon^2 = \\sum_{i=0}^n \\varepsilon^T.\\varepsilon\\\\\\\\\n& = (Y - X\\beta)^T.(Y - X\\beta)\\\\\\\\\n& = (Y^T - \\beta^TX^T).(Y - X\\beta)\\\\\\\\\n& = Y^T.Y - Y^T.X.\\beta - \\beta^T.X^T.Y + \\beta^T.X^T.X.\\beta \\\\\\\\\n\\end{align}\n\n<br>\n$$\n\\begin{align}\n&\\text{Since $\\beta^T.X^T.Y$ is a 1*1 matrix i.e. scalar, always symmetric,}\\\\\n&\\text{so we can replace it with its transpose}\\\\\\\\\n&\\text{$ (\\beta^T.X^T.Y)^T = (X^T.Y)^T.\\beta = Y^T.X.\\beta$}\\\\\n\\end{align}\n$$\n<br>\n\\begin{align}\n& = Y^T.Y - Y^T.X.\\beta - Y^T.X.\\beta + \\beta^T.X^T.X.\\beta \\\\\\\\\n{S(\\beta)} & = Y^T.Y - 2\\;Y^T.X.\\beta + \\beta^T.X^T.X.\\beta \\\\\n\\end{align}\n<br>\n$$\n\\begin{align}\n&\\text{The minimum of $S(\\beta)$ is obtained by taking the derivatives of}\\\\ \n&\\text{$S(\\beta)$ with $\\beta$ and further equating the derivative to 0}\\\\\n\\end{align}\n$$\n<br>\n\\begin{align}\n\\frac{\\partial {S(\\beta)}}{\\partial \\beta} & = 0 - 2\\;Y^T.X + \\frac{\\partial (\\beta^T.X^T.X.\\beta)}{\\partial \\beta} = 0\\\\\\\\\n&-2\\;Y^T.X + 2\\;\\beta^T.X^T.X = 0\\\\\\\\\n&\\beta^T.X^T.X = Y^T.X\\\\\\\\\n&\\beta^T.(X^T.X) = Y^T.X\\\\\\\\\n&(\\beta^T.(X^T.X))^T = (Y^T.X)^T\\\\\\\\\n&(X^T.X)^T.\\beta = (X^T.Y)\\\\\\\\\n&(X^T.X).\\beta = (X^T.Y)\\\\\\\\\n&(X^T.X)^{-1}.(X^T.X).\\beta = (X^T.X)^{-1}.(X^T.Y)\\\\\n\\\\\n&\\mathbf{\\beta = (X^T.X)^{-1}.(X^T.Y)}\\\\\n\\end{align}\n\n\n","c1aa0023":"## Numerical Approximation Using Gradient Descent","345f699f":"## Normal Equation Method (Analytical Solution)","bb3619a7":"From equation **1**,  \n\n\\begin{align}\n2 \\beta_1A - 2 B + 2  \\beta_0 X = 0 \\\\\\\\\n\\beta_1A = B - \\beta_0 X\\\\\\\\\n\\beta_1A = B - \\frac {(Y - \\beta_1X)}{n} X\\\\\\\\\n\\beta_1An = Bn - YX + \\beta_1X^2\\\\\\\\\n\\beta_1An - \\beta_1X^2 = Bn - YX \\\\\\\\\n\\beta_1(An - X^2) = (Bn - YX) \\\\\\\\\n\\mathbf {\\beta_1 = \\frac {(Bn - YX)}{(An - X^2)}}\\tag{4}\n\\end{align}","e399cbae":"## Ordinary Least Squares (OLS) Method  \n### In statistics, ordinary least squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: *minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed)* in the given dataset and those predicted by the linear function.\n<br>\n<br>\n\\begin{align}\n\\beta_1 & = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}  \\\\\\\\\n\\beta_0 & = \\bar{y} - \\beta_1*\\bar{x}\n\\end{align}\n\nx = independent variables<br>\n$\\bar{x}$ = average of independent variables<br>\ny = dependent variables<br>\n$\\bar{y}$ = average of dependent variables<br>\n\n<br> \nHow to measure goodness of the fit, \nIf a data point is (x,y) and the line actually goes through (x,$\\hat y$), how do we decide the extent to which this is a problem?  \n\nPick some error measuring function $\\varepsilon(y,\\hat y)$ that tells us the error we attribiute to the line going through y' instead of y.  \n<br>\n\n$\\varepsilon(y,\\hat y)$ should be 0 for all y i.e. line goes exactly through the data points.  \nTo avoid neutralisation of positive and negative errors  \n<br>\n\\begin{align}\n\\varepsilon(y,\\hat y) = (y-\\hat y)^2\n\\end{align}\n<br>\nLets assume there are n data points $(x_1,y_1), (x_2,y_2), ....(x_n,y_n)$  \n<br>\nWe want to find out $\\beta_0 , \\beta_1$ for $y = \\beta_1x + \\beta_0$.   \n<br>\nTotal Error E($\\beta_0,\\beta_1$) = $\\sum_{i=1}^n \\varepsilon(y_i,\\hat y)$  \n<br>\nSince $\\varepsilon(y,\\hat y) = (y-\\hat y)^2$  \n\n\n\\begin{align}\nE(\\beta_0,\\beta_1) & = \\sum_{i=1}^n \\varepsilon(y_i,\\hat y)\\\\\n& = \\sum_{i=1}^n (y_i,\\hat y_i)^2\\\\\n& = \\sum_{i=1}^n (y_i-\\hat y_i)^2\\\\\n& = \\sum_{i=1}^n (y_i-\\beta_1x_i-\\beta_0)^2\\\\\n& = \\sum_{i=1}^n (y_i^2+\\beta_1^2x_i^2+\\beta_0^2-2\\beta_1x_iy_i-2\\beta_0y_i+2\\beta_0\\beta_1x_i)\\\\\n& =  \\sum_{i=1}^ny_i^2+\\beta_1^2\\sum_{i=1}^nx_i^2+\\beta_0^2\\sum_{i=1}^n-2\\beta_1\\sum_{i=1}^nx_iy_i-2\\beta_0\\sum_{i=1}^ny_i+2\\beta_0\\beta_1\\sum_{i=1}^nx_i\\\\\n\\end{align}  \n\n","ab04b000":"### How to find model coefficients : $\\beta_0$ and $\\beta_1$ (intercept & slope)\n\nHow do we find values for the intercept and slope?\n\nThere are two ways:\n\n- Analytical solution, it has further two techniques\n  - Ordinary Least Squares\n  - Normal Equation \n- Numerical approximation solution - Gradient Descent\n"}}