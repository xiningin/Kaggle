{"cell_type":{"98f10e25":"code","41de955a":"code","00dec782":"code","e4cf4b1d":"code","9aa4cf7c":"code","093c5bd8":"code","5487a81c":"code","1b18da95":"code","3bb6e80c":"code","c823efd4":"code","b4380c19":"markdown","8e960be0":"markdown","18cfe1ae":"markdown","6359b402":"markdown","1cb1a215":"markdown","b388b1ed":"markdown","1541180b":"markdown"},"source":{"98f10e25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","41de955a":"!pip install numpy pandas matplotlib seaborn scikit-learn imblearn -q","00dec782":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport time as t\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import resample\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, classification_report, roc_curve, auc, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","e4cf4b1d":"def data_load(): #check for the availability of the dataset and change cwd if not found\n    df = pd.read_csv(\"..\/input\/breast-cancer-prediction\/data.csv\") \n    return  df\ndef data_clean(df):\n    return df\ndef X_y_split(df):\n    X = df.drop(['diagnosis'],axis=1)\n    y = df['diagnosis']\n    return X,y\ndef data_split_scale(X,y):\n     #Splitting dataset into Train and Test Set\n    X_tr, X_test,y_tr, y_test=train_test_split(X,y,test_size=0.3)      \n    #Feature Scaling using Standardization\n    ss=StandardScaler()\n    X_tr=ss.fit_transform(X_tr)\n    X_test=ss.fit_transform(X_test)     \n    print(\"'For 'Sampling strategies', I have 3 options. \\n \\t'1' stands for 'Upsampling'\\n \\t'2' stands for 'downsampling'. \\n \\t'3' stands for 'SMOTE''\")\n    samp_sel=int(input(\"Now enter your selection for sampling strategy: \\t\"))\n    samp=[sampling.upsample, sampling.downsample, sampling.smote]\n    temp=samp[samp_sel-1]\n    X_train,y_train=temp(X_train=pd.DataFrame(X_tr), y_train=pd.DataFrame(y_tr))\n    return pd.DataFrame(X_train), pd.DataFrame(X_test), y_train, y_test ","9aa4cf7c":"class sampling:\n    def upsample(X_train,y_train):\n        #combine them back for resampling\n        train_data = pd.concat([X_train, y_train], axis=1)\n        # separate minority and majority classes\n        negative = train_data[train_data.diagnosis==0]\n        positive = train_data[train_data.diagnosis==1]\n        # upsample minority\n        pos_upsampled = resample(positive,replace=True,n_samples=len(negative), random_state=30)\n        # combine majority and upsampled minority\n        upsampled = pd.concat([negative, pos_upsampled])\n        # check new class counts\n        #print(upsampled.diagnosis.value_counts())\n        print(upsampled.diagnosis.value_counts())\n        upsampled = upsampled.sample(frac = 1)\n        X_train=upsampled.iloc[:,0:-2]\n        y_train=upsampled.iloc[:,-1]\n        #graph barplot counts        \n        return X_train, y_train\n    def downsample(X_train,y_train):\n        #combine them back for resampling\n        train_data = pd.concat([X_train, y_train], axis=1)\n        # separate minority and majority classes\n        negative = train_data[train_data.diagnosis==0]\n        positive = train_data[train_data.diagnosis==1]\n        # downsample majority\n        neg_downsampled = resample(negative,\n         replace=True, # sample with replacement\n         n_samples=len(positive), # match number in minority class\n         random_state=30) # reproducible results\n        # combine minority and downsampled majority\n        downsampled = pd.concat([positive, neg_downsampled])\n        downsampled = downsampled.sample(frac = 1)\n        X_train=downsampled.iloc[:,0:-2]\n        y_train=downsampled.iloc[:,-1]\n        # check new class counts\n        print(downsampled.diagnosis.value_counts())\n        #graph\n        return X_train, y_train\n    def smote(X_train,y_train):\n        sm = SMOTE(random_state=30)\n        X_train, y_train = sm.fit_resample(X_train, y_train)\n        y_train = pd.DataFrame(y_train, columns = ['diagnosis'])\n        print(y_train.diagnosis.value_counts())\n        #graph\n        return X_train,y_train","093c5bd8":"class feat:\n    def feat1():\n        # All Features\n        df= data_load() # Loading Dataset into Dataframe\n        X , y = X_y_split(data_clean(df))\n        return data_split_scale(X,y)   \n    def feat2():     \n       # Mean Features\n       df= data_load() # Loading Dataset into Dataframe\n       df = data_clean(df)\n       df_mean = df[df.columns[:11]]\n       X , y = X_y_split(df_mean)\n       return data_split_scale(X,y)  \n    def feat3():\n       # Squared error Features       \n       df= data_load() # Loading Dataset into Dataframe\n       df = data_clean(df) \n       df_se = df.drop(df.columns[1:11], axis=1); df_se = df_se.drop(df_se.columns[11:], axis=1)\n       X , y = X_y_split(df_se)\n       return data_split_scale(X,y)   \n    def feat4():\n       # Worst Features\n       df= data_load() # Loading Dataset into Dataframe\n       df = data_clean(df)\n       df_worst = df.drop(df.columns[1:21], axis=1)\n       X , y = X_y_split(df_worst)\n       return data_split_scale(X,y)   \n    def feat5():\n       # Selected Features  \n       df =  data_load() # Loading Dataset into Dataframe \n       df = data_clean(df)  \n       drop_cols = ['radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', \n            'symmetry_worst', 'fractal_dimension_worst','perimeter_mean','perimeter_se', \n             'area_mean', 'area_se','concavity_mean','concavity_se', 'concave points_mean', \n             'concave points_se']\n       df_sf = df.drop(drop_cols, axis=1) \n       X , y = X_y_split(df_sf)\n       return data_split_scale(X,y)       \n    def feature():\n        print(\"'\\t The number '1' stands for 'ALL- FEATURES'. \\n \\t The number '2' stands for 'MEAN- FEATURES' . \\n \\t The number '3' stands for 'SQUARED- ERROR FEATURES'. \\n \\t The number '4' stands for 'WORST- FEATURES'. \\n \\t The number '5' stands for 'SELECTED- FEATURES'.'\")\n        selection=input(\"\\t Enter your choice of feature selection: \\t\")\n        feat_options=[feat.feat1, feat.feat2, feat.feat3, feat.feat4, feat.feat5]\n        return feat_options[int(selection) - 1]()","5487a81c":"class models:\n    def lr(dat):\n        # Logistic Regression\n        start=t.time()\n        lr=LogisticRegression()\n        model_lr=lr.fit(dat[0],dat[2])\n        pred=model_lr.predict(dat[1]); pred_prob=model_lr.predict_proba(dat[1])\n        stop = t.time()\n        return model_lr, (stop-start),pred, pred_prob\n    def dtc(dat):\n        # Decision Tree Classifier\n        start=t.time()\n        dtc=DecisionTreeClassifier()\n        model_dtc=dtc.fit(dat[0],dat[2])\n        pred=model_dtc.predict(dat[1]); pred_prob=model_dtc.predict_proba(dat[1])\n        stop = t.time()\n        return model_dtc, (stop-start),pred, pred_prob\n    def rfc(dat):\n        # Random Forest Classifier\n        start=t.time()\n        rfc=RandomForestClassifier()\n        model_rfc = rfc.fit(dat[0],dat[2])\n        pred=model_rfc.predict(dat[1]); pred_prob=model_rfc.predict_proba(dat[1])\n        stop = t.time()\n        return model_rfc, (stop-start),pred, pred_prob\n    def knn(dat):\n        # K-Nearest Neighbors\n        start=t.time()\n        knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n        model_knn = knn.fit(dat[0],dat[2])\n        pred=model_knn.predict(dat[1]); pred_prob=model_knn.predict_proba(dat[1])\n        stop = t.time()\n        return model_knn, (stop-start),pred, pred_prob\n    def svc_l(dat):\n        # Linear SVM\n        start=t.time()\n        svc_l = SVC(kernel = 'linear', random_state = 0,probability=True)\n        model_svc_l = svc_l.fit(dat[0],dat[2])\n        pred=model_svc_l.predict(dat[1]); pred_prob=model_svc_l.predict_proba(dat[1])\n        stop = t.time()\n        return model_svc_l, (stop-start),pred, pred_prob\n    def svc_r(dat):\n        # Kernel SVM\n        start=t.time()\n        svc_r = SVC(kernel = 'rbf', random_state = 0,probability=True)\n        model_svc_r = svc_r.fit(dat[0],dat[2])\n        pred=model_svc_r.predict(dat[1]); pred_prob=model_svc_r.predict_proba(dat[1])\n        stop = t.time()\n        return model_svc_r, (stop-start),pred, pred_prob\n    def gnb(dat):\n        # GaussianNB\n        start=t.time()\n        gnb = GaussianNB()\n        model_gnb = gnb.fit(dat[0],dat[2])\n        pred=model_gnb.predict(dat[1]); pred_prob=model_gnb.predict_proba(dat[1])\n        stop = t.time()\n        return model_gnb, (stop-start), pred, pred_prob","1b18da95":"def train_n_test():\n    ft=feat.feature()\n    modelsss=[models.lr,models.dtc,models.rfc,models.knn,models.svc_l,models.svc_r,models.gnb]\n    print(\"'\\t The number '1' stands for 'LOGISTIC REGRESSION'. \\n \\t The number '2' stands for 'Decision Tree' . \\n \\t The number '3' stands for 'Random Forest Classifier'. \\n \\t The number '4' stands for 'KNN'. \\n \\t The number '5' stands for 'Liner SVM'. \\n \\t The number '6' stands for 'Kernal SVM'. \\n \\t The number '7' stands for 'Guassian NB'.'\")\n    mdl_selection=int(input(\"Please enter your selection for models: \\t\"))\n    model=modelsss[mdl_selection - 1]\n    return model(ft),ft[3], mdl_selection","3bb6e80c":"def performance():  \n    out, y_test, mdl_selection=train_n_test()\n    models=[\"Logistic Regression\",\"Desicion Tree Classifier\", \"Random Forest Classifier\", \"KNN\", \"Liner SVM\", \"Kernal SVM\", \"Guassian NB\"]\n    cm_lr=confusion_matrix(y_test,out[2])\n    sns.heatmap(cm_lr,annot=True, cmap=\"Reds\")\n    plt.title(\"Confusion Matrix for {}\".format(models[mdl_selection-1]))\n    acs = accuracy_score(y_test,out[2])\n    rs = recall_score(y_test, out[2])\n    fs = f1_score(y_test, out[2])\n    ps = precision_score(y_test, out[2])\n    #Report Bar Plot\n    report = pd.DataFrame(classification_report(y_test, out[2], output_dict=True))\n    rg = report.drop(report.index[3]).drop(report.columns[2:], axis=1)\n    plt.style.use('seaborn')\n    rg.plot(kind = 'bar', color=[\"red\",\"salmon\"])\n    plt.title(\"Classification Report of {}\".format(models[mdl_selection-1]))\n    plt.legend(report.columns, ncol=2 ,loc=\"lower center\", bbox_to_anchor=(0.5, -0.3))\n    plt.yticks(np.arange(0, 1.05, step=0.05))            \n    print('\\n\\t The accuracy score of {} with given parameters is: {}%.'.format(models[mdl_selection-1],acs*100))\n    print('\\n\\t The recall score of {} with given parameters is: {}%.'.format(models[mdl_selection-1],rs*100))\n    print('\\n\\t The precision score of {} with given parameters is: {}%.'.format(models[mdl_selection-1],ps*100))\n    print('\\n\\t The F1 score of {} with given parameters is: {}%.'.format(models[mdl_selection-1],fs*100))\n    print('\\n\\t The training and testing time taken by {} with given parameters is: {} seconds.'.format(models[mdl_selection-1],out[1]))\n    prob=out[3]\n    prob=prob[:,1]\n    #ROC \n    false_pos, true_pos, thresh=roc_curve(y_test, prob, pos_label=1)\n    auc_score=roc_auc_score(y_test, prob)\n    rand_pr=[0 for i in range(len(y_test))]\n    p_fpr, p_tpr, _ = roc_curve(y_test, rand_pr, pos_label=1)\n    plt.figure()\n    plt.style.use('seaborn')\n    plt.plot(false_pos, true_pos, linestyle='--',color='orange',label=models[mdl_selection-1])\n    plt.plot(p_fpr, p_tpr, linestyle='--', color='green')\n    plt.title('ROC Curve')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend(loc='best')\n\n    return out[0],out[2], auc_score","c823efd4":"trained_model,  pred, auc = performance()","b4380c19":"# Feature Selection\n1. All Features\n2. Mean Features\n3. Squared Error Features\n4. Worst Features\n5. Selected Features","8e960be0":"# Data Preprocessing\n1. Data Loading\n2. Data Cleaning\n3. X y split\n4. Data Scaling","18cfe1ae":"# Training and Testing","6359b402":"# Model Performance Evaluation","1cb1a215":"# Final Step [User Inputs Required]","b388b1ed":"# Class Balancing\n1. Upsampling\n2. Downsampling\n3. SMOTE","1541180b":"# Classification Algorithms\n1. Logistic Regression\n2. Decision Tree Classifier\n3. Random Forest Classifier\n4. K-Nearest Neighbors\n5. Linear SVM\n6. Kernal SVM\n7. Gaussian Naive Bayes"}}