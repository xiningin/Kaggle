{"cell_type":{"02fcee93":"code","ee6d1e54":"code","ece87c57":"code","63f6b6c5":"code","eece81f2":"code","58703b43":"code","1a8a232d":"code","7953f469":"code","94b48654":"code","bc05ed2f":"code","59c8d324":"code","03282247":"code","d23f011b":"code","51104016":"code","11995ac5":"code","8314cf56":"code","9fb15ace":"code","b80107cf":"code","6217629c":"code","91c380c1":"code","f9f57b3e":"code","a2df4c4c":"markdown","7b2642be":"markdown","e5004c71":"markdown","150dfe63":"markdown","e02d9d10":"markdown","f3f08635":"markdown","a47c235b":"markdown","d6267a49":"markdown","34572a29":"markdown","dace2fe4":"markdown","fb4c1a20":"markdown","8356a276":"markdown","54cd66bf":"markdown","65d04893":"markdown","34f27d36":"markdown","501cd6e6":"markdown","daab4128":"markdown","cce0abc8":"markdown"},"source":{"02fcee93":"import pandas as pd\npd.options.display.max_columns = 200\npd.options.display.max_rows = 100\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nimport  warnings\nwarnings.filterwarnings(\"ignore\")","ee6d1e54":"file = '\/kaggle\/input\/hmeq-data\/hmeq.csv'\ndf = pd.read_csv(file)\n# Visualiza\u00e7\u00e3o de informa\u00e7\u00f5es gerais e missings\ndf.info()\n\n# Visualiza\u00e7\u00e3o da distribui\u00e7\u00e3o de classes\nprint('\\nDistribui\u00e7\u00e3o da Categoria BAD:\\n', df['BAD'].value_counts(normalize=True))\n\n# Visualiza\u00e7\u00e3o das 5 primeiras linhas do dataset\ndf.head(5)","ece87c57":"fig, ax = plt.subplots(2,5, figsize=(30,15))\nrow_1 = ['LOAN','MORTDUE','VALUE','YOJ','DEROG']\nrow_2 = ['DELINQ','CLAGE','NINQ','CLNO','DEBTINC']\nrow_3 = []\nfor i, col in enumerate(row_1):\n    sns.boxplot(x=df['BAD'], y=df[col], ax=ax[0,i])\nfor i, col in enumerate(row_2):\n    sns.boxplot(x=df['BAD'], y=df[col], ax=ax[1,i])","63f6b6c5":"fig, ax = plt.subplots(figsize=(8,7))\nax = sns.barplot(x=\"REASON\", y=\"BAD\", hue='BAD', data=df, estimator=lambda x: len(x))\nax.annotate(df['REASON'][(df['REASON'] == 'HomeImp') & (df['BAD'] == 0)].shape[0], xy=(0.11, 0.42), xycoords=\"axes fraction\")\nax.annotate(df['REASON'][(df['REASON'] == 'HomeImp') & (df['BAD'] == 1)].shape[0], xy=(0.32, 0.12), xycoords=\"axes fraction\")\nax.annotate(df['REASON'][(df['REASON'] == 'DebtCon') & (df['BAD'] == 0)].shape[0], xy=(0.61, 0.97), xycoords=\"axes fraction\")\nax.annotate(df['REASON'][(df['REASON'] == 'DebtCon') & (df['BAD'] == 1)].shape[0], xy=(0.82, 0.23), xycoords=\"axes fraction\")\nplt.ylabel('Count')\nplt.tick_params(\n    axis='y',          \n    left=False,\n    labelleft=False)","eece81f2":"df = df.dropna(thresh=df.shape[1]\/2, axis=0)\ndf.info()","58703b43":"# =============================================================================\n# Tratando colunas categ\u00f3ricas\n# =============================================================================\nfor col in df.select_dtypes(include='object').columns:\n    if df[col].isna().sum() > 0:\n         df[col].fillna(df[col].mode()[0], inplace=True)   ","1a8a232d":"# =============================================================================\n# Tratando colunas num\u00e9ricas\n# =============================================================================\nfor col in df.select_dtypes(exclude='object').columns:\n    if df[col].isna().sum() > 0:\n        df[col].fillna(-1, inplace=True)      ","7953f469":"# =============================================================================\n# Resultado\n# =============================================================================\ndf.info()","94b48654":"def showBalance(df, col):\n    for c in col:\n        print('Distribui\u00e7\u00e3o da Coluna: ', c,'\\n',df[c].value_counts(normalize=True),'\\n')\n    else:\n       pass\n        \nshowBalance(df, col=['REASON','JOB','BAD'])","bc05ed2f":"# =============================================================================\n# Fun\u00e7\u00e3o para realizar balanceamento\n# =============================================================================\ndef balance(df, col):\n    df_maior = df[df[col] == df[col].mode()[0]]\n    df_menor = df[df[col] != df[col].mode()[0]]\n \n    # Upsample da menor classe\n    df_menor_upsampled = resample(df_menor, \n                                  replace=True,     \n                                  n_samples=df_maior.shape[0],\n                                  random_state=42) \n \n    # Combinar as classe predominante com a menor classe aumentada\n    df_upsample = pd.concat([df_maior, df_menor_upsampled])\n\n    # Display new class counts\n    print('Contagem de registros')\n    print(df_upsample['BAD'].value_counts())\n    print('\\nDistribui\u00e7\u00e3o dos registros')\n    print(df_upsample['BAD'].value_counts(normalize=True))\n\n    return df_upsample\n    \ndf_upsample = balance(df, 'BAD')\nprint('\\n')\nshowBalance(df_upsample, col=['REASON','JOB','BAD'])","59c8d324":"df_upsample.hist(figsize=(20,20))","03282247":"dummies = ['REASON', 'JOB']\ndf_upsample = pd.get_dummies(df_upsample, columns=dummies, drop_first=True, dtype='int64')\n\ncols = df_upsample.columns.tolist()\ncols = cols[1:] + cols[:1]\ndf_upsample = df_upsample[cols]\ndel cols","d23f011b":"# =============================================================================\n# Observa\u00e7\u00e3o sobre o dataset rebalanceado\n# =============================================================================\ndf_upsample.info()\ndf_upsample.describe().T","51104016":"# =============================================================================\n# Divis\u00e3o Treino e Teste\n# =============================================================================\ntrain, test = train_test_split(df_upsample, test_size=0.1, random_state=0)\n# =============================================================================\n# X e Y dos dados de treino\n# =============================================================================\nX = train.iloc[:,:-1].values\ny = train.iloc[:,-1].values\n# =============================================================================\n# Divis\u00e3o Treino e Valida\u00e7\u00e3o\n# =============================================================================\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","11995ac5":"# =============================================================================\n# Random Forest Classifier\n# =============================================================================\n''' Scikit-Learn Definition\nA random forest is a meta estimator that fits a number of decision tree classifiers \non various sub-samples of the dataset and uses averaging to improve the predictive \naccuracy and control over-fitting. The sub-sample size is always the same as the \noriginal input sample size but the samples are drawn with replacement if bootstrap=True (default).\n'''\nrf = RandomForestClassifier(n_estimators=700, criterion='entropy', random_state=42)\n\n# =============================================================================\n# Extra Trees Classifier\n# =============================================================================\n''' Scikit-Learn Definition\nThis class implements a meta estimator that fits a number of randomized \ndecision trees (a.k.a. extra-trees) on various sub-samples of the dataset \nand uses averaging to improve the predictive accuracy and control over-fitting.\n'''\nextc = ExtraTreesClassifier(n_estimators=700,\n                            criterion='entropy',\n                            min_samples_split=5,\n                            max_depth=50,\n                            min_samples_leaf=5,\n                            random_state=42) ","8314cf56":"# =============================================================================\n# Random Forest\n# =============================================================================\nrf.fit(X_train, y_train)\ny_rfpred = rf.predict(X_test)\n\n# =============================================================================\n# Extra Trees\n# =============================================================================\nextc.fit(X_train, y_train)\ny_extcpred = extc.predict(X_test)","9fb15ace":"# =============================================================================\n# Dados de Valida\u00e7\u00e3o\n# =============================================================================\nprint('Valida\u00e7\u00e3o Random Forest....Acur\u00e1cia de: ', \"{0:.1f}\".format(accuracy_score(y_test, y_rfpred)*100),'%')\nprint('Valida\u00e7\u00e3o Extra Trees......Acur\u00e1cia de: ', \"{0:.1f}\".format(accuracy_score(y_test, y_extcpred)*100),'%')\n\n# =============================================================================\n# Dados de Teste\n# =============================================================================\ny2_rf = rf.predict(test.iloc[:,:-1].values)\ny2_extc = extc.predict(test.iloc[:,:-1].values)\nprint('\\nTeste Random Forest....Acur\u00e1cia de: ', \"{0:.1f}\".format(accuracy_score(test.iloc[:,-1].values, y2_rf)*100),'%')\nprint('Teste Extra Trees......Acur\u00e1cia de: ', \"{0:.1f}\".format(accuracy_score(test.iloc[:,-1].values, y2_extc)*100),'%')","b80107cf":"scores = cross_val_score(rf, X_train, y_train, n_jobs=-1, cv=5)\nprint('Scores: ', [round(x,2) for x in scores])\nprint('Score m\u00e9dio: ', round(scores.mean(),2))","6217629c":"scores = cross_val_score(extc, X_train, y_train, n_jobs=-1, cv=5)\nprint('Scores: ', [round(x,2) for x in scores])\nprint('Score m\u00e9dio: ', round(scores.mean(),2))","91c380c1":"fig, ax = plt.subplots(1,2, figsize=(16,10), sharex=True)\nrf_feat_importances = pd.Series(rf.feature_importances_, index=df_upsample.iloc[:,:-1].columns)\nex_feat_importances = pd.Series(extc.feature_importances_, index=df_upsample.iloc[:,:-1].columns)\n\nrf_feat_importances.plot(kind='barh', ax=ax[0], title='Random Forest')\nex_feat_importances.plot(kind='barh', ax=ax[1], title='Extra Trees')\n\nplt.xlim((0.0, 0.35))\nplt.show()","f9f57b3e":"cm = confusion_matrix(y_test, y_rfpred, labels=[1, 0])\n'''\ndefault:\n[[TN, FP,\n  FN, TP]]\n  \nlabels = [1,0]:\n[[TP, FP,\n  FN, TN]]\n'''\n\nfig, ax = plt.subplots(figsize=(7,6))\nsns.heatmap(cm, annot=True, ax=ax, fmt='.0f'); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('Real labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(['1', '0'])\nax.yaxis.set_ticklabels(['1', '0']);\nplt.show()\n\nsens = cm[0,0] \/ (cm[0,0] + cm[1,0])\nesp = cm[1,1] \/ (cm[0,1] + cm[1,1])\n\nprint('Sensibilidade: ', round(sens,2))\nprint('Especificidade: ', round(esp,2))","a2df4c4c":"# 3. An\u00e1lises Explorat\u00f3rias<a name=\"analises\"><\/a>\nPrimeiro realizou-se a visualiza\u00e7\u00e3o r\u00e1pida das principais informa\u00e7\u00f5es (missings, formato de dados e balanceamento) para levantamento das estrat\u00e9gias de preprocessamento. As colunas foram importadas corretamente quanto ao seu formato, por\u00e9m foi verificada uma grande quantidade de missings, inclusive registros com apenas 1 valor preenchido e desbalanceamento das classes da vari\u00e1vel resposta \"BAD\".\n\nEssas inconsist\u00eancias serao tratadas em etapa posterior.\n\n","7b2642be":"# 2. Dados <a name=\"data\"><\/a>\nA base de dados est\u00e1 dispon\u00edvel em https:\/\/www.kaggle.com\/ajay1735\/hmeq-data, e cont\u00e9m 5.960 regitros de empr\u00e9stimos com apenas 13 categorias de dados, sendo elas:\n\n* BAD : se o cliente faltou com o pagamento do empr\u00e9stimo (1 = Sim)\n* LOAN : Valor solicitado de empr\u00e9stimo\n* MORTDUE : Amount due on existing mortgage\n* VALUE : Valor atual da propriedade\n* REASON : Raz\u00e3o da solicita\u00e7\u00e3o (DebtCon: consolida\u00e7\u00e3o de d\u00e9bitos | HomeImp: melhoria na casa)\n* JOB : Seis categoria ocupacionais\n* YOJ : Tempo no trabalho atual em anos\n* DEROG : Quantidade de principais relat\u00f3rios depreciativos\n* DELINQ : Quantidade de linhas de cr\u00e9dito inadimplentes\n* CLAGE : Idade da linha de negociaa\u00e7\u00e3o mais antiga em meses\n* NINQ : Quantidade de linhas de cr\u00e9dito recentes\n* CLNO : Quantidade de linhas de cr\u00e9dito\n* DEBTINC : raz\u00e3o d\u00e9bito sobre receita \n----\n","e5004c71":"### 4.1 Desempenho\nO desempenho de ambos os modelos foi muito semelhante e embora, inicialmente, tenha parecido que o modelo Extra Trees havia levado pequena vantagem sobre Random Forest, a valida\u00e7\u00e3o cruzada mostrou que este fato pode ter ocorrido por uma casualidade da amostra selecionada, visto que em 5 testes aleat\u00f3rios posteriores o Random Forest se mostrou superior. \u00c9 interessante tamb\u00e9m olhar o n\u00edvel de import\u00e2ncia que cada modelo definiu para as vari\u00e1veis do dataset por meio do gr\u00e1fico abaixo.","150dfe63":"### 3.4 Dummies\nOutra etapa realizada foi a de transoformar as vari\u00e1veis categ\u00f3ricas em vari\u00e1veis \"Dummy\", para que o modelo possa entender os pesos de cada classes dentro das categorias e ser capaz de prever corretamente a resposta.","e02d9d10":"O dataset ficou, ent\u00e3o, com um total de 9.346 observa\u00e7\u00f5es e 17 colunas ap\u00f3s todos os tratamentos realizados.","f3f08635":"### 3.3 Balanceamento de Classes\nO balanceamento de classes \u00e9 extremamente importante para a precis\u00e3o do nosso modelo, e para o m\u00e9todo escolhido foi o de \"upsampling\" da classe de menor frequ\u00eancia por meio de uma fun\u00e7\u00e3o do pacote Scikit-Learn.\n\nAbaixo uma fun\u00e7\u00e3o que ir\u00e1 mostrar a distribui\u00e7\u00e3o das classes em todas as vari\u00e1veis categ\u00f3ricas, mas s\u00f3 iremos balancear a vari\u00e1vel resposta, \"BAD\".","a47c235b":"<img style=\"float: left;\" src=\"http:\/\/sindser.org.br\/s\/wp-content\/uploads\/2013\/09\/iesb1.jpg\"  width=\"400\" height=\"400\">\n\n## Instituto de Educa\u00e7\u00e3o Superior de Bras\u00edlia\n## P\u00f3s Gradua\u00e7\u00e3o em Ci\u00eancia de Dados\n## Data Mining e Machine Learning II\n## Leandro Alencar \u2013 1931133007","d6267a49":"# 4. Conclus\u00e3o <a name=\"conclusao\"><\/a>\n\nEmbora os dados iniciais possu\u00edssem muitos valores missings, o tratamento de dados n\u00e3o apresentou grandes dificuldades e, passada essa etapa, foram bem tranquilas as etapas seguintes de transforma\u00e7\u00e3o de vari\u00e1veis dummies e treinamento do modelo. Tamb\u00e9m foi muito interessante perceber que a aleatoriedade acrescentada pela valida\u00e7\u00e3o cruzada enriquece bastante a avalia\u00e7\u00e3o do melhor modelo, pois a sele\u00e7\u00e3o inicial dos dados pode acarretar em um vi\u00e9s escondido. Por fim, o modelo de Random Forest demmonstrou \u00f3timo desempenho, ratificado pela valida\u00e7\u00e3o cruzada e pela matriz de confus\u00e3o.","34572a29":"#### Visualiza\u00e7\u00e3o da distribui\u00e7\u00e3o de frequ\u00eancia\nAbaixo podemos ver como ficou a distribui\u00e7\u00e3o de frequ\u00eancia de todas as vari\u00e1veis ap\u00f3s o processo de tratamento de dados.","dace2fe4":"### 2.1 Bibliotecas e depend\u00eancias do Python:\n* Pandas \u2013 Manipula\u00e7\u00e3o e an\u00e1lise de dados \n* Matplotlib \u2013 Visualiza\u00e7\u00e3o gr\u00e1fica\n* Seaborn - Visualiza\u00e7\u00e3o gr\u00e1fica         \n* Scikit-learn - Aprendizado de m\u00e1quina","fb4c1a20":"Por fim, a Matriz de Confus\u00e3o nos mostra que o modelo de Random Forest desempenhou de maneira ex\u00edmia, com alt\u00edssimos \u00edndices de Sensibilidade e Especificidade.","8356a276":"No caso da vari\u00e1vel categ\u00f3rica \"REASON\", para cada classe existente a distribui\u00e7\u00e3o entre \"BAD = 1\" e \"BAD = 0\" \u00e9 semelhante, em torno de 20% das observa\u00e7\u00f5es s\u00e3o do tipo que n\u00e3o honram com o pagamento do empr\u00e9stimo.","54cd66bf":"### Valida\u00e7\u00e3o Cruzada\nPara confirma\u00e7\u00e3o do desempenho do modelo, iremos utilizar a valida\u00e7\u00e3o cruzada em 5 agrupamentos de dados aleat\u00f3rios a fim de verificar o comportamento do mesmo modelo ao lidar com observa\u00e7oes diferentes.","65d04893":"# 1. Introdu\u00e7\u00e3o <a name=\"introducao\"><\/a>\nEste trabalho consiste na an\u00e1lise da base de dados HMEQ (Home Equity), que \u00e9 composta por informa\u00e7\u00f5es de quase 6 mil opera\u00e7\u00f5es de cr\u00e9dito, e desenvolvimento de um modelo que ser\u00e1 capaz de indicar se um indiv\u00edduo ir\u00e1 honrar o compromisso at\u00e9 o final ou n\u00e3o, de forma a subsidiar a tomada de decis\u00e3o no processo de aprova\u00e7\u00e3o de linha de cr\u00e9dito para um novo cliente de uma entidade financeira ou afim. Os modelos baseados em \u00e1rvore de decis\u00e3o escolhidos s\u00e3o: \n> Random Forest Classifier  \nExtra Trees Classifier","34f27d36":"### 3.2 Valores missing\nA primeira estrat\u00e9gia adotada foi de eliminar as linhas que apresentavam mais da metade das colunas com campo missing, uma vez que esses registros n\u00e3o correponderiam \u00e0 verdade caso realiz\u00e1ssemos a introdu\u00e7\u00e3o de novos valores baseado em medidas de agrupamento (m\u00e9dia, moda, etc). Esse passo resultou na redu\u00e7\u00e3o de 126 linhas. \n\nPosteriormente o tipo de tratamento foi realizado com base nos dois tipos de dados: substitui\u00e7\u00e3o dos missings pela moda, para os categ\u00f3ricos; preencimento dos missings por \"-1\", para os num\u00e9ricos. A moda foi escolhida por uma quest\u00e3o estat\u00edstica, imaginando que por se refletir um comportamento da sociedade, seria mais provavel que aquele indiv\u00edduo sem informa\u00e7\u00e3o pertencesse ao grupo de maior predomin\u00e2ncia, e tamb\u00e9m n\u00e3o resultaria em um enviesamento dos dados, pois a quantidade desses registros nulos era bem pequena. Para os dados num\u00e9ricos, o m\u00e9todo foi pensado tendo em mente o algor\u00edtimo escolhido para solucionar o problema, um algor\u00edtimo de arvore de decis\u00e3o. Optou-se, ent\u00e3o, pelo preenchimento por um valor que n\u00e3o influenciasse o resultado deste algor\u00edtmo, por isso a escolha do \"-1\".\n","501cd6e6":"### 3.1 Visualiza\u00e7\u00e3o Gr\u00e1fica\nA fim de verificar presen\u00e7a de vi\u00e9s, optou-se pela ilustra\u00e7\u00e3o gr\u00e1fica das vari\u00e1veis num\u00e9ricas e categ\u00f3rias distribu\u00eddas pela vari\u00e1vel resposta, \"BAD\", e ficou demonstrado que no caso das num\u00e9ricas, e com exce\u00e7\u00e3o de \"DEROG\" e \"DELINQ\", est\u00e3o bem distribu\u00eddas. ","daab4128":"----\n# Modelos de Classifica\u00e7\u00e3o por \u00c1rvore de Decis\u00e3o\n## Table of contents\n* [Introdu\u00e7\u00e3o](#introducao)\n* [Dados](#data)\n* [An\u00e1lises Descritivas](#analises)\n* [Modelo](#model)\n* [Conclus\u00e3o](#conclusao)","cce0abc8":"# 4. Modelo <a name=\"model\"><\/a>\nPara realoza\u00e7\u00e3o da tarefa de previs\u00e3o ser\u00e3o utilizados um algor\u00edtimo de Random Forest e um de Extra Trees, ambos baseados em modelos de \u00e1rvore de decis\u00e3o.\nRandom Forest consiste em um m\u00e9todo que utiliza diversas \u00e1rvores de decis\u00e3o durante o treinamento e retorna a m\u00e9dia dos resultados individuais dessas \u00e1rvores, uma grande vantagem desse modelo \u00e9 a diminui\u00e7\u00e3o da possibilidade de overfitting.\n\nExtra trees, de Extremely Randomized Trees ou \u00c1rvores Extremamente Aleat\u00f3reas, tem como principal diferencial a aleatoriadade dos par\u00e2metros do algor\u00edtimo. Esse fator resulta em um modelo com baixa vari\u00e2ncia e \u00e9 indicado principalmente para problemas com alta dimensionalidade."}}