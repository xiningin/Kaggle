{"cell_type":{"1d337720":"code","98c60494":"code","6d02136d":"code","a8ff5c44":"code","4a2a9ffe":"code","e96cd377":"code","0c870915":"code","b048c953":"code","e7747a90":"code","209ed86c":"code","ef438171":"code","f54abd6e":"code","81953051":"code","a6b6a223":"code","1b960897":"code","793e4686":"code","e0f6b94d":"code","632312f7":"markdown","629999f8":"markdown","258b2895":"markdown","e89ebb8a":"markdown","63283c59":"markdown","81a71fec":"markdown","6c77a038":"markdown","d959ab26":"markdown","6ea1d71d":"markdown","ff30f0da":"markdown","c962c045":"markdown"},"source":{"1d337720":"from keras.datasets import mnist\nfrom keras.layers import Input, Dense, Reshape, Flatten, Lambda\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom PIL import Image\n\n#For this project, we will only be using train_images\n#To further improve the accuracy of the GAN, you could involve labels\nPATH=\"..\/input\/\"\ntrain_images = np.load(PATH+'kmnist-train-imgs.npz')['arr_0']\ntest_images = np.load(PATH+'kmnist-test-imgs.npz')['arr_0']\ntrain_labels = np.load(PATH+'kmnist-train-labels.npz')['arr_0']\ntest_labels = np.load(PATH+'kmnist-test-labels.npz')['arr_0']\n","98c60494":"img_rows = 28\nimg_cols = 28\nchannels = 1\nimg_shape = (img_rows, img_cols, channels)\nlatent_dim = 10 #10 classes and hence 10 dimensions\nbatch_size = 16\nepsilon_std = 1.0\n\n# View the dataset to get an idea of what we're dealing with\ndef plot_sample_images_data(images, labels):\n    plt.figure(figsize=(12,12))\n    for i in range(10):\n        imgs = images[np.where(labels == i)]\n        lbls = labels[np.where(labels == i)]\n        for j in range(10):\n            plt.subplot(10,10,i*10+j+1)\n            plt.xticks([])\n            plt.yticks([])\n            plt.grid(False)\n            plt.imshow(imgs[j], cmap=plt.cm.binary)\n            plt.xlabel(lbls[j])\n","6d02136d":"plot_sample_images_data(train_images, train_labels)\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=epsilon_std)\n    return z_mean + K.exp(z_log_var \/ 2) * epsilon","a8ff5c44":"def build_encoder():\n    img = Input(shape=img_shape)\n    h = Flatten()(img)\n    h = Dense(512)(h)\n    h = LeakyReLU(alpha=0.2)(h)\n    h = Dense(512)(h)\n    h = LeakyReLU(alpha=0.2)(h)\n    mu = Dense(latent_dim)(h)\n    log_var = Dense(latent_dim)(h)\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([mu, log_var])\n    return Model(img, z)","4a2a9ffe":"def build_decoder():\n    model = Sequential()\n    model.add(Dense(512, input_dim=latent_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.2))\n    # tanh is more robust: gradient not equal to 0 around 0\n    model.add(Dense(np.prod(img_shape), activation='tanh'))\n    model.add(Reshape(img_shape))\n    model.summary()\n    z = Input(shape=(latent_dim,))\n    img = model(z)\n    return Model(z, img)","e96cd377":"def build_discriminator():\n    #Added 1024 layer in discrim\n    model = Sequential()\n    model.add(Dense(1024, input_dim=latent_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(256))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    model.summary()\n    encoded_repr = Input(shape=(latent_dim,))\n    validity = model(encoded_repr)\n    return Model(encoded_repr, validity)","0c870915":"optimizer = Adam(0.0002, 0.5)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator()\ndiscriminator.compile(loss='binary_crossentropy',\n                      optimizer=optimizer,\n                      metrics=['accuracy'])\n\n# Build the encoder \/ decoder\nencoder = build_encoder()\ndecoder = build_decoder()\n\nimg = Input(shape=img_shape)\n# The generator takes the image, encodes it and reconstructs it\n# from the encoding\nencoded_repr = encoder(img)\nreconstructed_img = decoder(encoded_repr)\n\n# First pass, train the generator only, then start training discrimnator\n# if discriminator is attached to generator, set this flag to fix discriminator\n#When we train the generator, we dont want to train the discriminator, and vice versa\n#Hence ideally you should run this in two loops\ndiscriminator.trainable = False\n\n# The discriminator determines validity of the encoding\nvalidity = discriminator(encoded_repr)\n\n# The adversarial_autoencoder model  (stacked generator and discriminator)\n#This is the generator part\n#We define the loss as MSE and binary_crossentropy\nadversarial_autoencoder = Model(img, [reconstructed_img, validity])\nadversarial_autoencoder.compile(loss=['mse', 'binary_crossentropy'], loss_weights=[0.999, 0.001], optimizer=optimizer)\nadversarial_autoencoder.trainable =True","b048c953":"def train(epochs, batch_size=128, sample_interval=50):\n    # Load the dataset\n    X_train =train_images \n\n    # Normalization: Rescale -1 to 1\n    X_train = (X_train.astype(np.float32) - 127.5) \/ 127.5\n    X_train = np.expand_dims(X_train, axis=3)\n\n    # Adversarial ground truths\n    valid = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        #  Train Discriminator\n\n        # Select a random batch of images\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        imgs = X_train[idx]\n\n        latent_fake = encoder.predict(imgs)\n        latent_real = np.random.normal(size=(batch_size, latent_dim))\n\n        # Train the discriminator, this should only have a real effect if the discriminator is set to trainable\n        # let latent_real's output is close to 1\n        d_loss_real = discriminator.train_on_batch(latent_real, valid)\n        # let latent_fake's output is close to 0\n        d_loss_fake = discriminator.train_on_batch(latent_fake, fake)\n        d_loss = 1* np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        # decrease reconstruction error and let discriminator's output is close to 1\n        g_loss = adversarial_autoencoder.train_on_batch(imgs, [imgs, valid])\n\n        # If at save interval\n        if epoch % sample_interval == 0:\n            # Plot the progress\n            print(\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (\n                epoch, d_loss[0], 100 * d_loss[1], g_loss[0], g_loss[1]))\n            # save generated image samples\n            sample_images(epoch)\n            \n        #Now that the intial training epoch has passed, we switch trainable roles\n        if(discriminator.trainable==False):\n            discriminator.trainable=True\n            adversarial_autoencoder.trainable=False\n        elif(discriminator.trainable==True):\n            discriminator.trainable=False\n            adversarial_autoencoder.trainable=True","e7747a90":"# Save generated images per specified epochs \ndef sample_images(epoch):\n    r, c = 5, 5\n    z = np.random.normal(size=(r * c, latent_dim))\n    gen_imgs = decoder.predict(z)\n    gen_imgs = 0.5 * gen_imgs + 0.5\n    fig, axs = plt.subplots(r, c)\n    cnt = 0\n    for i in range(r):\n        for j in range(c):\n            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap=plt.cm.binary)\n            axs[i, j].axis('off')\n            cnt += 1\n    fig.savefig(\"mnist_%d.png\" % epoch)\n    plt.close()","209ed86c":"epochs = 60000\nsample_interval = 2000\nsample_count = epochs\/sample_interval","ef438171":"train(epochs=epochs, batch_size=batch_size, sample_interval=sample_interval)","f54abd6e":"Image.open('mnist_6000.png')","81953051":"Image.open('mnist_10000.png')","a6b6a223":"Image.open('mnist_16000.png')","1b960897":"Image.open('mnist_20000.png')","793e4686":"Image.open('mnist_28000.png')","e0f6b94d":"z = np.random.normal(size=(1, latent_dim))\ngen_imgs = decoder.predict(z)\ngen_imgs = 0.5 * gen_imgs + 0.5\nplt.imshow(gen_imgs[0, :, :, 0], cmap='gray')","632312f7":"## Train GAN\nAs we've set the discrimnator to be not trainable, we are only training the generator","629999f8":"## Build GAN","258b2895":"\nThis kernel use GAN(Generative Adversarial Network) to generate different ancient Japanese Hiragana, using KMNIST images.\nBased on implementation \"GAN with MLP on MNIST\" by Vincent Kao, itself based on the reference by Erik Linderen.\n\nFirst step is to initalize and import the images","e89ebb8a":"## Show single generated image","63283c59":"## Set up network parameters\nThese will be handy later, also do some sampling parameters","81a71fec":"## Define a function to build an encoder\nEncoders job is to take an existing image and reduce it to its most simplest form?","6c77a038":"## Define a function to build an decoder\nThe decoders job is to build the image from an encoding, which is an artificial representation","d959ab26":"## Reference\n[Keras - Adversarial Autoencoder(AAE)](https:\/\/github.com\/eriklindernoren\/Keras-GAN#adversarial-autoencoder)","6ea1d71d":"## Define a function to build an discriminator\nThe discriminator's job is to judge the generated images for authenticity, whether it's real or fake","ff30f0da":"## Show generated MNIST images per 200 epochs","c962c045":"## Define a function to train GAN"}}