{"cell_type":{"886e5fe6":"code","115d9694":"code","d41f99f2":"code","44980c09":"code","80946102":"code","f4e22fea":"code","11b4080c":"code","d35ef7d0":"code","6011bf6e":"code","fd514233":"code","dfc37398":"code","9d87b52e":"code","97dc45de":"code","ab6c8da6":"code","aa96b98d":"code","a7e92349":"code","2b53a27d":"code","96a78710":"code","b532dcce":"code","0fad49cc":"code","2750f470":"code","fe66fbc3":"code","3918d626":"code","341bc5fe":"code","b4d9dc57":"code","d8e1e9bf":"code","7334269d":"code","fd2945ea":"code","89188f7d":"code","0fce55c1":"code","bad90fc1":"code","844affb2":"code","442d19d3":"code","071d3bed":"code","2cbfaeb7":"code","7002a44c":"code","9b6ea586":"code","1f7be892":"code","0f0ea191":"code","8da73895":"code","79a386d6":"code","06cd54fa":"markdown","af049a5d":"markdown","6c51cf2f":"markdown","d72bf87f":"markdown","9fdcde8e":"markdown","6e8db6a1":"markdown","54a7fb83":"markdown","54a6198a":"markdown","4f46f60d":"markdown","d72490d3":"markdown","fdab7050":"markdown","9d53659a":"markdown","3bca2291":"markdown","16134d8b":"markdown"},"source":{"886e5fe6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport nltk\nimport missingno as msno\nfrom wordcloud import WordCloud, STOPWORDS\nimport string\nimport re\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.initializers import Constant\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline","115d9694":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","d41f99f2":"print(train.shape, test.shape)","44980c09":"# Plotting the missing values\nmsno.bar(train)","80946102":"# Percentage missing values in keyword, location\ndef pert(data):\n  return (len(data[data[\"keyword\"].isnull() == True])\/data.shape[0]) * 100\n\ndef pert_loc(data):\n  return (len(data[data[\"location\"].isnull() == True])\/data.shape[0]) * 100\n","f4e22fea":"print(\"percentage of keyword values missing in train --> {}\".format(pert(train)))\nprint(\"percentage of keyword values missing in test --> {}\".format(pert(test)))\nprint(\"percentage of location values missing in train --> {}\".format(pert_loc(train)))\nprint(\"percentage of location values missing in test --> {}\".format(pert_loc(test)))\nprint(\"\\n\")\nprint(\"Percentage of missing values in both train and test looks quite similar.\\\nMay be both train and test data come from same sample\")","11b4080c":"# Plottin the most repetitive words in \"text\" column\nstopwords = set(STOPWORDS)\ndef word_cloud(data, title = None):\n  cloud = WordCloud(\n      background_color = \"black\",\n      stopwords = stopwords,\n      max_words=200,\n      max_font_size=40, \n      scale=3,\n  ).generate(str(data))\n  fig = plt.figure(figsize= (15, 15))\n  plt.axis(\"off\")\n  if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.25)\n\n  plt.imshow(cloud)\n  plt.show()","d35ef7d0":"word_cloud(train[\"text\"], \"Most repeated words in train['text']\")","6011bf6e":"word_cloud(test[\"text\"], \"Most repeated words in test['text']\")","fd514233":"# Target count\nplt.figure(figsize = (10, 8))\nuniques = train[\"target\"].value_counts()\nsns.barplot(x = uniques.index, y = uniques.values, data = uniques)\nplt.xlabel(\"Target Values\")\nplt.ylabel(\"Count Values\")\nsns.despine(left = True, bottom = True)\nplt.show()","dfc37398":"# Most repeated words in real disaster tweets\nword_cloud(train[train[\"target\"] == 1][\"text\"], \"Most repeated words in real disaster tweets in train data\")","9d87b52e":"word_cloud(train[train[\"target\"] == 0][\"text\"], \"Most repeated words in fake disaster tweets in train data\")","97dc45de":"l1 = list(train[\"keyword\"].unique())\nl2 = list(test[\"keyword\"].unique())\nif l1 == l2:\n  print(\"Two lists are identical\")\nelse:\n  print(\"Two lists are not identical\")","ab6c8da6":"# Distribution of keywords in real and fake tweets \nplt.figure(figsize = (10, 80), dpi = 100)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nsns.countplot(y = \"keyword\", hue = \"target\", data = train)\nplt.legend(loc = 1)\nplt.show()","aa96b98d":"# Number of words in the text \ntrain[\"num_words\"] = train[\"text\"].apply(lambda x : len(str(x).split()))\ntest[\"num_words\"] = test[\"text\"].apply(lambda x : len(str(x).split()))\n\n# Number of characters\ntrain[\"num_chars\"] = train[\"text\"].apply(lambda x : len(str(x)))\ntest[\"num_chars\"] = test[\"text\"].apply(lambda x : len(str(x)))\n\n# Number of stopwords\ntrain[\"num_stopwords\"] = train[\"text\"].apply(lambda x : len([word for word in str(x).lower().split()\\\n                                                             if word in stopwords]))\ntest[\"num_stopwords\"] = test[\"text\"].apply(lambda x : len([word for word in str(x).lower().split()\\\n                                                             if word in stopwords]))\n\n# Number of punctuations\ntrain[\"num_punctuation\"] = train[\"text\"].apply(lambda x : len([p for p in x.split() if p in string.punctuation]))\ntest[\"num_punctuation\"] = test[\"text\"].apply(lambda x : len([p for p in x.split() if p in string.punctuation]))","a7e92349":"fig, axes = plt.subplots(4, 1, figsize = (10, 20))\n\n# num_words\nsns.boxplot(x = \"target\", y = \"num_words\", data = train, ax = axes[0])\naxes[0].set_xlabel('Target', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\n# num_chars\nsns.boxplot(x = \"target\", y = \"num_chars\", data = train, ax = axes[1])\naxes[1].set_xlabel('Target', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\n# num_stopwords\nsns.boxplot(x = \"target\", y = \"num_stopwords\", data = train, ax = axes[2])\naxes[2].set_xlabel('Target', fontsize=12)\naxes[2].set_title(\"Number of stopwords in each class\", fontsize=15)\n\n# num_punctuation\nsns.boxplot(x = \"target\", y = \"num_punctuation\", data = train, ax = axes[3])\naxes[3].set_xlabel('Target', fontsize=12)\naxes[3].set_title(\"Number of punctuations in each class\", fontsize=15)","2b53a27d":"from collections import defaultdict\n\ntrain0 = train[train[\"target\"] == 0]\ntrain1 = train[train[\"target\"] == 1]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    \n    token = [token for token in text.lower().split() if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n\nfreq_dict = defaultdict(int)\nfor sent in train0[\"text\"]:\n  for word in generate_ngrams(sent):\n    freq_dict[word] += 1\nfd_sorted0 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted0.columns = [\"word\", \"wordcount\"]\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1[\"text\"]:\n  for word in generate_ngrams(sent):\n    freq_dict[word] += 1\nfd_sorted1 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted1.columns = [\"word\", \"wordcount\"]\n\nfig, axes = plt.subplots(1, 2, figsize = (12, 12))\nplt.tight_layout()\nsns.despine()\nfor i in range(2):\n    sns.barplot(x = \"wordcount\", y = \"word\", data = globals()[\"fd_sorted\" + str(i)].iloc[:50, :], ax = axes[i])\n    axes[i].set_xlabel('Count', fontsize=12)\n    axes[i].set_title(f\"Most repetitive words in {i} class\", fontsize=15)","96a78710":"train0 = train[train[\"target\"] == 0]\ntrain1 = train[train[\"target\"] == 1]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=2):\n    \n    token = [token for token in text.lower().split() if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n\nfreq_dict = defaultdict(int)\nfor sent in train0[\"text\"]:\n  for word in generate_ngrams(sent):\n    freq_dict[word] += 1\nfd_sorted0 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted0.columns = [\"word\", \"wordcount\"]\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1[\"text\"]:\n  for word in generate_ngrams(sent):\n    freq_dict[word] += 1\nfd_sorted1 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted1.columns = [\"word\", \"wordcount\"]\n\nfig, axes = plt.subplots(1, 2, figsize = (12, 12))\nplt.tight_layout()\nsns.despine()\nfor i in range(2):\n    sns.barplot(x = \"wordcount\", y = \"word\", data = globals()[\"fd_sorted\" + str(i)].iloc[:50, :], ax = axes[i])\n    axes[i].set_xlabel('Count', fontsize=12)\n    axes[i].set_title(f\"Most repetitive words in {i} class\", fontsize=15)","b532dcce":"train0 = train[train[\"target\"] == 0]\ntrain1 = train[train[\"target\"] == 1]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=3):\n    \n    token = [token for token in text.lower().split() if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n\nfreq_dict = defaultdict(int)\nfor sent in train0[\"text\"]:\n  for word in generate_ngrams(sent):\n    freq_dict[word] += 1\nfd_sorted0 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted0.columns = [\"word\", \"wordcount\"]\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1[\"text\"]:\n  for word in generate_ngrams(sent):\n    freq_dict[word] += 1\nfd_sorted1 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted1.columns = [\"word\", \"wordcount\"]\n\nfig, axes = plt.subplots(1, 2, figsize = (12, 12))\nplt.tight_layout()\nsns.despine()\nfor i in range(2):\n    sns.barplot(x = \"wordcount\", y = \"word\", data = globals()[\"fd_sorted\" + str(i)].iloc[:50, :], ax = axes[i])\n    axes[i].set_xlabel('Count', fontsize=12)\n    axes[i].set_title(f\"Most repetitive words in {i} class\", fontsize=15)","0fad49cc":"%%time\n\nglove_embeddings = np.load('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', allow_pickle=True)","2750f470":"df = train.append(test, ignore_index = True)","fe66fbc3":"def build_vocab(X):\n    \n    tweets = X.apply(lambda s: s.split()).values      \n    vocab = {}\n    \n    for tweet in tweets:\n        for word in tweet:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1                \n    return vocab\n\n\ndef check_embeddings_coverage(X, embeddings):\n    \n    vocab = build_vocab(X)    \n    \n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word] = embeddings[word]\n            n_covered += vocab[word]\n        except:\n            oov[word] = vocab[word]\n            n_oov += vocab[word]\n    return covered, oov, n_covered, n_oov\n","3918d626":"covered, oov, n_covered, n_oov = check_embeddings_coverage(df[\"text\"], glove_embeddings)\nprint(f\"Number of words covered by Glove embeddings --> {n_covered}\")\nprint(f\"Number of words not covered by Glove embeddings --> {n_oov}\")\nprint(f\"Percentage of words covered by Glove embeddings --> {(n_covered\/(n_covered + n_oov)) * 100}%\")","341bc5fe":"df[\"text\"] = df[\"text\"].apply(lambda x : x.lower())\ndf[\"keyword\"].fillna(\"keyword\", inplace = True)\ndf[\"text\"] = df[\"text\"] + \" \" + df[\"keyword\"]\ndf.drop([\"keyword\", \"location\"], axis = 1, inplace = True)","b4d9dc57":"list_all_words = \" \".join(df[\"text\"])\nnot_english = [word for word in list_all_words.split() if word.isalpha() == False]\n\ndef clean_data(data):\n    # Remove urls\n    data = re.sub(r'https?\\S+', '', data)\n    # Remove html tags\n    data = re.sub(r\"<.*?>\", \"\", data)\n    # Remove punctuations\n    t = [w for w in data if w not in string.punctuation]\n    data = \"\".join(t)\n    # Remove stopwords\n    t = [w for w in data.split() if w not in stopwords]\n    data = \" \".join(t)\n    # Removing numbers from text\n    data = re.sub(r\"\\d+\", \"\", data)\n\n    data = re.sub(r\"\\x89\u00db_\", \"\", data)\n    data = re.sub(r\"\\x89\u00db\u00d2\", \"\", data)\n    data = re.sub(r\"\\x89\u00db\u00d3\", \"\", data)\n    data = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", data)\n    data = re.sub(r\"\\x89\u00db\u00cf\", \"\", data)\n    data = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", data)\n    data = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", data)\n    data = re.sub(r\"\\x89\u00db\u00f7\", \"\", data)\n    data = re.sub(r\"\\x89\u00db\u00aa\", \"\", data)\n    data = re.sub(r\"\\x89\u00db\\x9d\", \"\", data)\n    data = re.sub(r\"\u00e5_\", \"\", data)\n    data = re.sub(r\"\\x89\u00db\u00a2\", \"\", data)\n    data = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", data)\n    data = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", data)\n    data = re.sub(r\"\u00e5\u00ca\", \"\", data)\n    data = re.sub(r\"\u00e5\u00c8\", \"\", data)\n    data = re.sub(r\"Jap\u00cc_n\", \"Japan\", data)    \n    data = re.sub(r\"\u00cc\u00a9\", \"e\", data)\n    data = re.sub(r\"\u00e5\u00a8\", \"\", data)\n    data = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", data)\n    data = re.sub(r\"\u00e5\u00c7\", \"\", data)\n    data = re.sub(r\"\u00e5\u00a33million\", \"3 million\", data)\n    data = re.sub(r\"\u00e5\u00c0\", \"\", data)\n    \n    # Remove words not alphabets\n    t = [w for w in data.split() if w not in not_english]\n    data = \" \".join(t)\n    \n    return data    \n\n\ndf[\"text\"] = df[\"text\"].apply(lambda x : clean_data(x))","d8e1e9bf":"covered, oov, n_covered, n_oov = check_embeddings_coverage(df[\"text\"], glove_embeddings)\nprint(f\"Number of words covered by Glove embeddings --> {n_covered}\")\nprint(f\"Number of words not covered by Glove embeddings --> {n_oov}\")\nprint(f\"Percentage of words covered by Glove embeddings --> {(n_covered\/(n_covered + n_oov)) * 100}%\")","7334269d":"embed_size = 300 # how big is each word vector\nmaxlen = 20 # max number of words in a comment to use\nmax_features = 20000","fd2945ea":"tokenizer = Tokenizer(oov_token = \"<OOV>\", num_words = max_features)\ntokenizer.fit_on_texts(df[\"text\"])\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(df[\"text\"])\npadded = pad_sequences(sequences, padding = \"post\", maxlen = maxlen)","89188f7d":"train_x = padded[:7613, :]\ntest = padded[7613:, :]\ntrain_y = df[df[\"target\"].isnull() == False][\"target\"].apply(int).values.reshape(-1, 1)","0fce55c1":"num_words = min(max_features, len(word_index)) + 1\nembedding_dim = 300\n# first create a matrix of zeros, this is our embedding matrix\nembedding_matrix = np.zeros((num_words, embedding_dim))\n# for each word in out tokenizer lets try to find that work in our w2v model\nfor word, i in word_index.items():\n    if i > max_features:\n        continue\n    embedding_vector = covered.get(word)\n    if embedding_vector is not None:\n        # we found the word - add that words vector to the matrix\n        embedding_matrix[i] = embedding_vector","bad90fc1":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(num_words,\n                    embedding_dim,\n                    embeddings_initializer=Constant(embedding_matrix),\n                    input_length=maxlen,\n                    trainable=False),\n    tf.keras.layers.SpatialDropout1D(0.2),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dropout(0.10),\n    tf.keras.layers.Dense(units=32, activation=\"relu\"),\n    tf.keras.layers.Dense(units=8, activation=\"relu\"),\n    tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n    \n    \n])\n\nmodel.compile(loss = \"binary_crossentropy\", optimizer='adam', metrics = [\"accuracy\"])\nmodel.summary()","844affb2":"batch_size = 128\nnum_epochs = 20\n\nhistory = model.fit(train_x, train_y, batch_size = batch_size, epochs = num_epochs)","442d19d3":"# Make predictions\n# Preparing test data\ny_pred = model.predict(test)\n\nmodel_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nmodel_submission['target'] = np.round(y_pred).astype('int')\nmodel_submission.to_csv('model_submission2.csv', index=False)","071d3bed":"pad_df = pd.DataFrame(padded)\nnew_df = pd.concat([df, pad_df], axis=1)","2cbfaeb7":"new_df.drop(\"text\", inplace = True, axis = 1)","7002a44c":"train_new_df = new_df[new_df[\"target\"].isnull() == False]\ntest_new_df = new_df[new_df[\"target\"].isnull() == True]\ntest_new_df.drop(\"target\", inplace = True, axis = 1)","9b6ea586":"X = train_new_df.drop(\"target\", axis = 1).values\ny = train_new_df[\"target\"].apply(int).values.reshape(-1, 1)","1f7be892":"# It will take a while to run. I have already run this on my local host. So I am just writing the code here.\n\nparam_test = {\n    \"max_depth\":range(3,10,2),\n    \"min_child_weight\":range(1,6,2),\n    \"gamma\":[i\/10.0 for i in range(0,5)]\n}\n\n\ngsearch = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n                                             min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n                                             objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n                                             param_grid = param_test, n_jobs=4,iid=False, cv=5)\n\ngsearch.fit(X,y)\ngsearch.best_params_","0f0ea191":"xgb = XGBClassifier(\n     learning_rate =0.1,\n     n_estimators=140,\n     max_depth=5,\n     min_child_weight=1,\n     gamma=0,\n     subsample=0.8,\n     colsample_bytree=0.8,\n     objective= 'binary:logistic',\n     nthread=4,\n     scale_pos_weight=1,\n     seed=27)","8da73895":"xgb.fit(X, y)","79a386d6":"preds = xgb.predict(test_new_df.values)\nxgb_pred = model.predict(test)\n\nxgb_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nxgb_submission['target'] = np.round(y_pred).astype('int')\nxgb_submission.to_csv('xgb_submission.csv', index=False)","06cd54fa":"https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n\nThe above kernel codes helped me a lot. Please upvote this kernel as well","af049a5d":"# Bi-directional LSTM with Glove embeddings","6c51cf2f":"**If you like this kernel do upvote. Let's start without any further delay**","d72bf87f":"We can see that all the keyword column elements in train data are also avaliable in test data. May be the missing values in keyword column can also be identical. So we can replace missing values in keyword column with \"key_word\" later on.","9fdcde8e":"Unigrams","6e8db6a1":"# N-GRAMS","54a7fb83":"# XG-BOOST","54a6198a":"**Meta Data**\n1. Number of words in the text\n2. Number of characters in the text\n3. Number of stopwords\n4. Number of punctuations\n5. Number of upper case","4f46f60d":"**In this Notebook you will learn about:**\n1. Exploratory Data Analysis\n2. Glove Word Embeddings\n3. Bi-Directional LSTMs\n4. Xg-Boost","d72490d3":"Bi-Gram","fdab7050":"It can be seen from above plots that number of characters is left skewed in case of target value = 1","9d53659a":"Tri-Gram","3bca2291":"# Data Cleaning","16134d8b":"# Data Preprocessing"}}