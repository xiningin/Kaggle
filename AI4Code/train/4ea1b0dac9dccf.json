{"cell_type":{"4dd26a17":"code","39aa44b0":"code","f6bb3cf4":"code","1ca546a4":"code","087afe44":"code","fb195c12":"code","376a87be":"code","944918ff":"code","ba67214a":"code","5ee24d7a":"code","437bc0de":"code","94ef42f7":"code","86130ef8":"code","770b06d4":"code","22ee8d7c":"code","1c8c9547":"code","ff54fed3":"code","a91fc0cd":"code","3fc12a8c":"code","6c055e49":"markdown","1f40b864":"markdown","12699060":"markdown","9c025785":"markdown","f68a8400":"markdown","823ad639":"markdown","0522e84e":"markdown","0ce84786":"markdown","3c5ac2cf":"markdown","bcd6b4e1":"markdown","fc626c73":"markdown","1ecd3c7f":"markdown","9e1e29e6":"markdown","819eccc7":"markdown","8da8ddd0":"markdown","7496628e":"markdown","6caf192f":"markdown","02782357":"markdown","6620772c":"markdown","3783bccb":"markdown","8db4e90d":"markdown"},"source":{"4dd26a17":"# Import all packages that we'll work with \nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nimport warnings\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Ignore deprecation warnings from sklearn\nwarnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)","39aa44b0":"# Load the training and test data into separate dataframes\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n# Use head() to look at the first 5 rows of the training data to understand what we will be working with \ntrain_data.head()","f6bb3cf4":"# describe() computes a summary of statistics for columns containing numeric data\ntrain_data.describe()","1ca546a4":"# Seaborn is imported as sns\ngender_analysis = sns.catplot(x=\"Sex\",y=\"Survived\",data=train_data, kind=\"bar\", height = 6, palette = \"muted\")\ngender_analysis = gender_analysis.set_ylabels(\"Survival Rate\")","087afe44":"gender_pclass = sns.FacetGrid(train_data, height=4.5, aspect=1.6)\ngender_pclass.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', order=None, hue_order=None )\ngender_pclass.add_legend();","fb195c12":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(16, 8))\nwomen = train_data[train_data['Sex']=='female']\nmen = train_data[train_data['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=30, label = survived, ax = axes[0], kde =False, color=\"green\")\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=30, label = not_survived, ax = axes[0], kde =False, color=\"red\")\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=30, label = survived, ax = axes[1], kde = False, color=\"green\")\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=30, label = not_survived, ax = axes[1], kde = False, color=\"red\")\nax.legend()\ntitle_male = ax.set_title('Male')","376a87be":"# Gender one hot encoding using dummies\ntrain_data['Gender'] = pd.get_dummies(train_data['Sex'], drop_first=True)\ntest_data['Gender'] = pd.get_dummies(test_data['Sex'], drop_first=True)\n\n# Drop the Sex, Embarked, and Fare columns as we will not build any further features using this data\ntrain_data.drop(columns=['Sex', 'Embarked', 'Fare'], inplace=True)\ntest_data.drop(columns=['Sex', 'Embarked', 'Fare'], inplace=True)","944918ff":"# Concatenate the train and test data together into a complete dataset for efficient feature engineering\n_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndataset = pd.concat([_train, _test], sort=True, ignore_index=True)\n\n# Note how many rows are in the training dataset. We'll use this later to easily add features created \n# in the concatenated dataset into the separate train and test datasets\ntrain_len = len(train_data)","ba67214a":"# Identify groups of passengers who traveled together (based on surname, passenger class, ticket number, and embarked) \n# and allocate each passenger in a particular group with the same GroupId\n\n# Extract surname from Name\nsurname = dataset['Name'].apply(lambda x: x[:x.find(',')])\n\n# Remove the last digit from ticket numbers as these are different for every passenger\nticket = dataset['Ticket'].apply(lambda x: x[:-1])\n\n# Create a new field called PassengerGroup, which combines surname, passenger class, ticket number, and embarked\ndataset['PassengerGroup'] = (surname.astype(str) + '-' + dataset['Pclass'].astype(str) + '-'\n           + ticket.astype(str) + '-' + dataset['Embarked'].astype(str))\n\n# Function to generate GroupIDs for each PassengerGroup. If there is only 1 PassengerId in the group then GroupId = 0, \n# meaning these are passengers who are not travelling in a group\ndef PassengerGroup_labeler(group):\n    group_elements = dataset.loc[dataset['PassengerGroup'] == group, 'PassengerId']\n    if len(group_elements) == 1:\n        return 0\n    else:\n        return group_elements.min()\n\n# Pass PassengerGroup through the above function to add GroupId to each passenger record    \ndataset['GroupId'] = dataset['PassengerGroup'].apply(PassengerGroup_labeler)\n\n# Drop PassengerGroup as no longer needed\ndataset.drop(columns='PassengerGroup', inplace=True)","5ee24d7a":"# InGroup is 1 for passengers with a GroupId otherwise 0\ndataset['InGroup'] = (dataset['GroupId'] > 0).astype(int)\n\n# Add InGroup to the train and test datasets using train_len\ntrain_data['InGroup'] = dataset.iloc[:train_len, -1]\ntest_data['InGroup'] = dataset.iloc[train_len:, -1].reset_index(drop=True)","437bc0de":"# Female-child-groups are groups of passengers traveling together whose members are either females or boys \n# (i.e. males with the title of Master in their name). InFCG is 1 if the passenger is in a female-child-group, otherwise 0\n\n# Extract title from passenger name\ndataset['Title'] = dataset['Name'].apply(lambda x: x[x.find(', ') + 2:x.find('.')])\n\n# Create a mask to account only for females or boys in groups\nmask = (dataset['GroupId'] != 0) & ((dataset['Title'] == 'Master') | (dataset['Sex'] == 'female'))\n\n# Get the number of females and boys in each group, discard groups with only one member\nfcg_groups = dataset.loc[mask, 'GroupId'].value_counts()\nfcg_groups = fcg_groups[fcg_groups > 1]\n\n# Update the mask to discard groups with only one female or boy\nmask = mask & (dataset['GroupId'].isin(fcg_groups.index))\n\n# Create the new feature using the updated mask\ndataset['InFCG'] = 0\ndataset.loc[mask, 'InFCG'] = 1\n\n# Add to the train and test datasets\ntrain_data['InFCG'] = dataset.iloc[:train_len, -1]\ntest_data['InFCG'] = dataset.iloc[train_len:, -1].reset_index(drop=True)","94ef42f7":"# For a passenger in a female-child-group, FCGSurvived is equal to 1 if all members of that group survived, otherwise 0\ndataset['FCGSurvived'] = dataset.loc[dataset['InFCG'] == 1].groupby('GroupId')['Survived'].transform(np.nanmean)\n\n# np.nanmean returns NaN for groups without survival information in test dataset. Replace the NaN with 0\ndataset.loc[dataset['FCGSurvived'].isna(), 'FCGSurvived'] = 0\ndataset['FCGSurvived'] = dataset['FCGSurvived'].astype(int)\n\n# Add to the train and test datasets\ntrain_data['FCGSurvived'] = dataset.iloc[:train_len, -1]\ntest_data['FCGSurvived'] = dataset.iloc[train_len:, -1].reset_index(drop=True)","86130ef8":"# Drop remaining columns that will not be used to train our model\ntrain_data.drop(columns=['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Age'], inplace=True)\ntest_data.drop(columns=['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Age'], inplace=True)","770b06d4":"# View the features that will be used to train our model\ntrain_data.head()","22ee8d7c":"# Split the training set into samples and targets\nX_train = train_data.drop(columns='Survived')\nY_train = train_data['Survived'].astype(int)\n\n# Test set samples to predict\nX_test = test_data\n\n# Standard Scaler is our friend. It helps to boost the score\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","1c8c9547":"model = KNeighborsClassifier(leaf_size=3, weights='uniform', n_neighbors=19)\nmodel.fit(X_train, Y_train)\n\n# Prediction\npred = model.predict(X_test)","ff54fed3":"from sklearn import tree\nclassifier = tree.DecisionTreeClassifier(criterion='gini',\n                                         splitter='best',\n                                         max_depth=3,\n                                         random_state=42)\nclassifier.fit(X_train, Y_train)\npred2 = classifier.predict(X_test)","a91fc0cd":"from sklearn.tree import export_graphviz\nimport graphviz\n\ndot_data = export_graphviz(classifier, \n                           feature_names=['Survived', 'Pclass', 'Gender', 'InGroup', 'InFCG', 'FCGSurvived'], \n                           class_names=['Died', 'Survived'], \n                           filled=True, \n                           rounded=True,\n                           proportion=False)\ngraphviz.Source(dot_data)","3fc12a8c":"# Generate .csv submission file from KNeighborsClassifier for Kaggle\nsubmission_df = pd.DataFrame({'PassengerId': _test['PassengerId'], 'Survived': pred})\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Your submission was successfully saved!\")","6c055e49":"Wow! There's a significantly lower survival rate for females traveling 3rd class. This confirms the historical account that first and second class passengers were most likely to reach the lifeboats, which were hastely launched partially loaded with people. Finally, while continuing to ignore the missing age values for the moment let's explore how age correlate with survival... ","1f40b864":"## FE: Passengers in a Female-Child-Group (InFCG)","12699060":"## FE: Passengers in a Female-Child-Group that all Survived (FCGSurvived)","9c025785":"### Feedback Welcome!\nPlease comment and upvote if you found this tutorial helpful.","f68a8400":"## Feature Engineering (FE)\nA key step in preparing data to train ML models is One-Hot Encoding. Many machine learning algorithms cannot operate on categorical data such as Sex (female\/male) directly. This means that categorical data must be converted to a numerical form. Here is a complete [one-hot encoding tutorial](https:\/\/towardsdatascience.com\/what-is-one-hot-encoding-and-how-to-use-pandas-get-dummies-function-922eb9bd4970) for beginners, including how to use Pandas get_dummies function.","823ad639":"## Age is missing 177 values\nThere are 891 rows in the training dataset. Using describe() we see the training dataset contains only 714 values for Age. From our historical analysis we know that women and children were preferentially loaded on to lifeboats. Therefore we will need a way to identify children. This presents a challenge due to the large amount of missing age data. We will return to this later...","0522e84e":"In the decision tree model (max_depth=3) all males who are not in a female-child-group that survived are expected to die. For females Pclass comes into play before membership in a female-child-group is considered.","0ce84786":"## FE: Gender","3c5ac2cf":"## Create a Model Using KNeighborsClassifier","bcd6b4e1":"## EDA: Gender","fc626c73":"## EDA: Age","1ecd3c7f":"What's most noticable in the above charts is the relatively large proportion of very young males that survived. Now we have a simple hypothesis to guide our feature engineering efforts: female-children-groups travelling in 1st and 2nd class are by far the most likely passengers to survive. But with so much missing Age data how do we reliably detect boys? **The simple answer is to rely on the titles within passenger names as a proxy**. Specifically \"Master\", [a form of address used for boys](https:\/\/en.wikipedia.org\/wiki\/Master_(form_of_address). ","9e1e29e6":"## Create a Model Using Decision Tree Classifier","819eccc7":"### If you find this notebook useful, support with an upvote \ud83d\udc4d\n\n\n### How to begin?\nThe offical [Titianic Tutorial](https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial) provides an excellent starting point for novices to begin participating in Kaggle competitions. \n\nHowever, you will not score > 0.78 following that guide.\n\n## You've scored ~0.78, now what?\nWant to improve your score and break into the top 3% on the leaderboard? This tutorial is for you. \n\nData science is a enormous field encompasing a plethora of technical knowledge and skills. This tutorial will walk you through the simplest possible approach to scoring > 0.8 on the Kaggle Titanic challenge. It will also direct you to important concepts to explore further. Helping to expand your understanding and guide the gradual development of your data science skillset. \n\n## First step, a little history...\nAn examination of the historical account of the Titanic ([Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Titanic) and [BBC](https:\/\/www.bbc.com\/news\/magazine-17515305)) provides valuable understanding of the passenger data as it pertains to survival: \n1. A disproportionate number of men were left aboard because of a \"women and children first\" protocol for loading lifeboats.\n2. First and second class passengers were most likely to reach the lifeboats. Third class passengers had to find their way through a maze of corridors and staircases to reach the boat deck.\n3. Many lifeboats were launched only partially loaded.\n\nGiven the Titanic sank after colliding with an iceburg in the north Atlantic it's reasonable to expect anyone who didn't make it into a lifeboat was highly unlikely to survive due to cold exposure. Thus anyone not in a female-children-group **and** who did not get on a lifeboat before they were all launched was unlikely to survive.\n\n### Let's get started!","8da8ddd0":"The above bar chart confirms females were far more likely to survive. Next, let's see if passenger class impacted on survival for males and females...","7496628e":"## Exploratory Data Analysis (EDA)\nWe know from historical accounts that women and children were preferrentially loaded on to lifeboats. Let's use Seaborn to explore the relationship between gender and survival in our training dataset. \n\nFor more information check out the tutorial: [Plotting with categorical data using Seaborn](https:\/\/seaborn.pydata.org\/tutorial\/categorical.html)","6caf192f":"## Where to from here? \nHow to keep learning while further improving your score...\n\n### More feature engineering\n1. Devise a sophisticated method for estimating missing age data, bucket passengers into appropriate age groupings, and build new features\n2. Some men did survive. Figure out how to find them. Hint: Pclass 1, husbands of women with young children, age > 75\n3. Identify new types of passenger groups\n\n### Compare model performance\nRun features through many different types of models and compare them to find the most effective\n\n### Hyperparameter tuning\nAutomate model parameter settings to generate most effective setting for the most effective model","02782357":"## Generate a Submission to the Titanic Kaggle Competition","6620772c":"## FE: Passengers Travelling in a Group (InGroup)","3783bccb":"## Interpret the Decison Tree Model","8db4e90d":"## EDA: Passenger Class"}}