{"cell_type":{"3e40f825":"code","46e62a67":"code","18a7091f":"code","635e4920":"code","f5a45aa4":"code","042e51a7":"code","0866a415":"code","e75a7ae8":"code","9b4fb223":"code","c135736f":"code","ca062f7d":"code","851def93":"code","d5fef0bb":"code","8adb1043":"code","ed620878":"code","2d2dc13e":"code","3ca9f5c2":"code","8c939e7c":"code","0da8e364":"code","2b020cab":"code","5a027c1b":"code","2e8d79d8":"code","3717cb4a":"code","56d2320c":"code","013729ed":"code","b81ef201":"code","aa8a3085":"code","875a7c1b":"code","086932d8":"code","83b7f261":"code","e14c2d54":"code","5ab12efb":"code","427ec434":"code","b7e6de2b":"code","98be471d":"code","5369f13d":"markdown","1b0dfc86":"markdown","1f1397f6":"markdown","90067c24":"markdown","988cce19":"markdown","1b68bbc0":"markdown","569e422c":"markdown","4afbc0cf":"markdown","9a860498":"markdown","29cad8ce":"markdown","17642f17":"markdown","55b59c04":"markdown","b302ea9d":"markdown","9b42de0b":"markdown"},"source":{"3e40f825":"!pip install numerapi\nfrom numerapi import NumerAPI","46e62a67":"napi = NumerAPI()\ncurrent_round = napi.get_current_round(tournament=8)","18a7091f":"train_pq_path = \"numerai_training_data_int8.parquet\"\nvalid_pq_path = \"numerai_validation_data_int8.parquet\"","635e4920":"napi.download_dataset(train_pq_path, train_pq_path)\nnapi.download_dataset(valid_pq_path, valid_pq_path)","f5a45aa4":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom typing import List\nfrom tqdm.auto import tqdm\nimport random","042e51a7":"training_data = pd.read_parquet(train_pq_path)\nvalidation_data = pd.read_parquet(valid_pq_path)","0866a415":"napi.download_dataset(\"features.json\", 'features.json')","e75a7ae8":"import json \nwith open('features.json', 'rb') as f:\n    feature_dict = json.load(f)\n    \nprint(feature_dict['feature_sets'].keys())\n\nboruta_feats = [\n                'feature_unwonted_trusted_fixative',\n                'feature_introvert_symphysial_assegai',\n                'feature_jerkwater_eustatic_electrocardiograph',\n                'feature_canalicular_peeling_lilienthal',\n                'feature_unvaried_social_bangkok',\n                'feature_crowning_frustrate_kampala',\n                'feature_store_apteral_isocheim',\n                'feature_haziest_lifelike_horseback',\n                'feature_grandmotherly_circumnavigable_homonymity',\n                'feature_assenting_darn_arthropod',\n                'feature_beery_somatologic_elimination',\n                'feature_cambial_bigoted_bacterioid',\n                'feature_unaired_operose_lactoprotein',\n                'feature_moralistic_heartier_typhoid',\n                'feature_twisty_adequate_minutia',\n                'feature_unsealed_suffixal_babar',\n                'feature_planned_superimposed_bend',\n                'feature_winsome_irreproachable_milkfish',\n                'feature_flintier_enslaved_borsch',\n                'feature_agile_unrespited_gaucho',\n                'feature_glare_factional_assessment',\n                'feature_slack_calefacient_tableau',\n                'feature_undivorced_unsatisfying_praetorium',\n                'feature_silver_handworked_scauper',\n                'feature_communicatory_unrecommended_velure',\n                'feature_stylistic_honduran_comprador',\n                'feature_travelled_semipermeable_perruquier',\n                'feature_bhutan_imagism_dolerite',\n                'feature_lofty_acceptable_challenge',\n                'feature_antichristian_slangiest_idyllist',\n                'feature_apomictical_motorized_vaporisation',\n                'feature_buxom_curtained_sienna',\n                'feature_gullable_sanguine_incongruity',\n                'feature_unforbidden_highbrow_kafir',\n                'feature_chuffier_analectic_conchiolin',\n                'feature_branched_dilatory_sunbelt',\n                'feature_univalve_abdicant_distrail',\n                'feature_exorbitant_myeloid_crinkle'\n                ]\n\n#features = feature_dict['feature_sets']['medium']\nfeatures = boruta_feats","9b4fb223":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import TensorDataset, DataLoader","c135736f":"def preproccess(df: pd.DataFrame, feats: List[str], target_name: str):\n    X = torch.Tensor(df[feats].values \/ 4 - 0.5)\n    y = torch.Tensor(df[target_name].values)\n    dataset = TensorDataset(X, y)\n    return dataset","ca062f7d":"train_dataset = preproccess(training_data, features, 'target')\nvalid_dataset = preproccess(validation_data, features, 'target')","851def93":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\ntrain_size = len(training_data)\nvalid_size = len(validation_data)\n\nbatch_size = 4096\nn_epoch = 30\nlearning_rate = 1e-2","d5fef0bb":"train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=valid_size, shuffle=False)","8adb1043":"class Net(nn.Module):\n    def __init__(self, D, hidden_1=256, hidden_2=256, seed=43):\n        super(Net, self).__init__()\n        torch.manual_seed(seed)\n        self.l1 = nn.Linear(D, 128)\n        self.l2 = nn.Linear(128, 128)\n        self.l3 = nn.Linear(128, 64)\n        self.l4 = nn.Linear(64, 1)\n        \n        nn.init.kaiming_normal_(self.l1.weight)\n        nn.init.kaiming_normal_(self.l2.weight)\n        nn.init.kaiming_normal_(self.l3.weight)\n        nn.init.kaiming_normal_(self.l4.weight)\n\n    def forward(self, x):\n        x = torch.relu(self.l1(x))\n        x = torch.relu(self.l2(x))\n        x = torch.relu(self.l3(x))\n        x = self.l4(x)\n        return x","ed620878":"def correlation(predictions, targets):\n    ranked_preds = predictions.rank(pct=True, method='first')\n    return np.corrcoef(ranked_preds, targets)[0, 1]\n\ndef score(df):\n    return correlation(df['prediction'], df['target'])\n\ndef calc_correlations(data_df):\n    correlations = data_df.groupby('era').apply(score)\n    return correlations\n\ndef validate(model, valid_df, valid_loader, valid_dize):\n    model.eval()\n    valid_df['prediction'] = 0\n    valid_loss = 0.0\n    with torch.no_grad():\n        valid_pred_array = np.zeros((valid_size))\n        for i, (X, y) in tqdm(enumerate(valid_loader)):\n            valid_pred = model(X.to(device))\n            valid_pred_array = valid_pred.detach().cpu().numpy()[:, 0]\n            loss = criterion(valid_pred.squeeze(), y.to(device))\n            valid_loss += float(loss)\n            del loss\n\n        valid_df['prediction'] = valid_pred_array\n        valid_corrs = calc_correlations(valid_df)\n    return valid_corrs, valid_loss","2d2dc13e":"model = Net(len(features))\nmodel = model.to(device)\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","3ca9f5c2":"for epoch in range(n_epoch):\n    model.train()\n    train_loss = 0.0\n    with tqdm(enumerate(train_loader)) as t:\n        for i, (X, y) in t:\n            optimizer.zero_grad()\n            outputs = model(X.to(device))\n            loss = criterion(outputs.squeeze(), y.to(device))\n            t.set_postfix(loss=float(loss))\n            t.update()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += float(loss)\n            del outputs\n            del loss\n            \n    valid_corrs, valid_loss = validate(model, validation_data, valid_loader, valid_size)\n    \n    print('epoch: {:d} loss: {:.5f}'.format(epoch + 1, train_loss \/ (train_size \/ batch_size)))\n    print('epoch: {:d} val loss: {:.5f}'.format(epoch + 1, valid_loss \/ (valid_size \/ batch_size)))\n    print(f\"epoch: {epoch + 1} valid corr mean: {valid_corrs.mean():.4f} corr sharpe: {valid_corrs.mean() \/ valid_corrs.std()}\")","8c939e7c":"!pip install pyro-ppl","0da8e364":"import pyro\nimport pyro.distributions as dist\nfrom pyro.nn import PyroModule, PyroSample\nimport torch.nn as nn\nfrom pyro.infer.autoguide import AutoDiagonalNormal, AutoDelta\nfrom pyro.infer import SVI, Trace_ELBO, Predictive","2b020cab":"class BNN(PyroModule):\n    def __init__(self, D, hidden_1=128, hidden_2=128, hidden_3=64, prior='norm', y_std=0.01, seed=43):\n        super().__init__()\n        self.y_std = y_std\n        \n        # https:\/\/pytorch.org\/docs\/stable\/nn.init.html\n        def prior_kaiming_uniform(fan_in):\n            val = torch.Tensor([np.sqrt(6 \/ fan_in)]).to(device)\n            return dist.Uniform(-val, val)\n        \n        def prior_kaiming_normal(fan_in):\n            mu = torch.Tensor([0.]).to(device)\n            std = torch.Tensor([np.sqrt(2) \/ np.sqrt(fan_in)]).to(device)\n            return dist.Normal(mu, std)\n        \n        def prior_kaiming_laplace(fan_in):\n            mu = torch.Tensor([0.]).to(device)\n            std = torch.Tensor([np.sqrt(2) \/ np.sqrt(fan_in)]).to(device)\n            return dist.Laplace(mu, std)\n        \n        if prior == 'normal':\n            print('set prior distribution: normal')\n            prior_sample = prior_kaiming_normal\n        elif prior == 'laplace':\n            print('set prior distribution: laplace')\n            prior_sample = prior_kaiming_laplace\n        elif prior == 'uniform':\n            print('set prior distribution: uniform')\n            prior_sample = prior_kaiming_uniform\n        elif prior == 'noninfo':\n            print('set prior distribution: non-information distribution')\n            prior_sample = lambda _: dist.Uniform(-5., 5.)\n        else:\n            print('no match distribution. so set uniform')\n            prior_sample = prior_kaiming_uniform\n        \n        self.l1 = PyroModule[nn.Linear](D, hidden_1)\n        self.l1.weight = PyroSample(prior_sample(D).expand([hidden_1, D]).to_event(2))\n        self.l1.bias = PyroSample(prior_sample(D).expand([hidden_1]).to_event(1))\n        self.l2 = PyroModule[nn.Linear](hidden_1, hidden_2)\n        self.l2.weight = PyroSample(prior_sample(hidden_1).expand([hidden_2, hidden_1]).to_event(2))\n        self.l2.bias = PyroSample(prior_sample(hidden_1).expand([hidden_2]).to_event(1))\n        self.l3 = PyroModule[nn.Linear](hidden_2, hidden_3)\n        self.l3.weight = PyroSample(prior_sample(hidden_2).expand([hidden_3, hidden_2]).to_event(2))\n        self.l3.bias = PyroSample(prior_sample(hidden_2).expand([hidden_3]).to_event(1))\n        self.l4 = PyroModule[nn.Linear](hidden_3, 1)\n        self.l4.weight = PyroSample(prior_sample(hidden_3).expand([1, hidden_3]).to_event(2))\n        self.l4.bias = PyroSample(prior_sample(hidden_3).expand([1]).to_event(1))\n\n    def forward(self, x, y=None):\n        x = torch.relu(self.l1(x))\n        x = torch.relu(self.l2(x))\n        x = torch.relu(self.l3(x))\n        mu = self.l4(x).squeeze()\n        \n        with pyro.plate(\"data\", x.shape[0]):\n            obs = pyro.sample(\"obs\", dist.Normal(mu, self.y_std), obs=y)\n        return mu    \n    \ndef bnn_train(loader, model, guide, optimizer, n_epoch, loss_func=Trace_ELBO(), seed=43):\n    svi = SVI(model, guide, optimizer,  loss=loss_func)\n    pyro.set_rng_seed(seed)\n    pyro.clear_param_store()\n    for epoch in range(n_epoch):\n        train_loss = 0.0\n        with tqdm(enumerate(loader)) as t:\n            for i, (X, y) in t:\n                loss = svi.step(X.to(device), y.to(device))\n                t.set_postfix(loss=float(loss))\n                t.update()\n                train_loss += float(loss)\n                del loss\n\n        print('epoch: {:d} loss: {:.5f}'.format(epoch + 1, train_loss \/ (train_size \/ batch_size)))\n        \ndef bnn_predict(data_array, model, guide, num_samples):\n    predictive = Predictive(model, guide=guide, num_samples=num_samples)\n    with torch.no_grad():\n        valid_pred = predictive(torch.Tensor(data_array).to(device))\n        valid_pred_array = valid_pred['obs'].T.detach().cpu().numpy()\n    return valid_pred_array","5a027c1b":"optimizer = pyro.optim.Adam({'lr': 1e-3})\ny_std = 0.01\n\nuniform_bnn = BNN(len(features), prior='uniform', y_std=y_std)\nuniform_guide = AutoDiagonalNormal(uniform_bnn)\nbnn_train(train_loader, uniform_bnn, uniform_guide, optimizer, n_epoch)\n\nnormal_bnn = BNN(len(features), prior='normal', y_std=y_std)\nnormal_guide = AutoDiagonalNormal(normal_bnn)\nbnn_train(train_loader, normal_bnn, normal_guide, optimizer, n_epoch)\n\nlaplace_bnn = BNN(len(features), prior='laplace', y_std=y_std)\nlaplace_guide = AutoDiagonalNormal(laplace_bnn)\nbnn_train(train_loader, laplace_bnn, laplace_guide, optimizer, n_epoch)","2e8d79d8":"num_samples=1000","3717cb4a":"uniform_valid_pred_array = bnn_predict(\n    validation_data[features].values \/ 4 - 0.5,\n    uniform_bnn,\n    uniform_guide,\n    num_samples\n)\nnormal_valid_pred_array = bnn_predict(\n    validation_data[features].values \/ 4 - 0.5,\n    normal_bnn,\n    normal_guide,\n    num_samples\n)\nlaplace_valid_pred_array = bnn_predict(\n    validation_data[features].values \/ 4 - 0.5,\n    laplace_bnn,\n    laplace_guide,\n    num_samples\n)\n\nprint('uniform')\nvalidation_data['prediction'] = uniform_valid_pred_array.mean(axis=1)\nvalidation_data['std'] = uniform_valid_pred_array.std(axis=1)\nvalid_corrs = calc_correlations(validation_data)\nvalidation_data['prediction'].to_csv('uniform_bnn_prediction.csv')\nvalidation_data[['prediction', 'std']].to_csv('uniform_bnn_prediction_std.csv')\nprint(f\"num_sample: {num_samples} valid corr mean: {valid_corrs.mean():.4f} corr sharpe: {valid_corrs.mean() \/ valid_corrs.std()}\")\n\nprint('normal')\nvalidation_data['prediction'] = normal_valid_pred_array.mean(axis=1)\nvalidation_data['std'] = normal_valid_pred_array.std(axis=1)\nvalid_corrs = calc_correlations(validation_data)\nvalidation_data['prediction'].to_csv('normal_bnn_prediction.csv')\nvalidation_data[['prediction', 'std']].to_csv('normal_bnn_prediction_std.csv')\nprint(f\"num_sample: {num_samples} valid corr mean: {valid_corrs.mean():.4f} corr sharpe: {valid_corrs.mean() \/ valid_corrs.std()}\")\n\nprint('laplace')\nvalidation_data['prediction'] = laplace_valid_pred_array.mean(axis=1)\nvalidation_data['std'] = laplace_valid_pred_array.std(axis=1)\nvalid_corrs = calc_correlations(validation_data)\nvalidation_data['prediction'].to_csv('laplace_bnn_prediction.csv')\nvalidation_data[['prediction', 'std']].to_csv('laplace_bnn_prediction_std.csv')\nprint(f\"num_sample: {num_samples} valid corr mean: {valid_corrs.mean():.4f} corr sharpe: {valid_corrs.mean() \/ valid_corrs.std()}\")\n","56d2320c":"def calc_valid_corrs(valid_pred_array, is_cum=False):\n    valid_corrs_list = []\n    for i in tqdm(range(valid_pred_array.shape[1])):\n        if is_cum:\n            validation_data['prediction'] = valid_pred_array[:, :i+1].mean(axis=1)\n        else:\n            validation_data['prediction'] = valid_pred_array[:, i]\n        valid_corrs = calc_correlations(validation_data)\n        valid_corrs_list.append(valid_corrs)\n    return valid_corrs_list","013729ed":"uniform_valid_corrs_list = calc_valid_corrs(uniform_valid_pred_array)\nnormal_valid_corrs_list = calc_valid_corrs(normal_valid_pred_array)\nlaplace_valid_corrs_list = calc_valid_corrs(laplace_valid_pred_array)","b81ef201":"uniform_mean_corrs_list = [corrs.mean() for corrs in uniform_valid_corrs_list]\nnormal_mean_corrs_list = [corrs.mean() for corrs in normal_valid_corrs_list]\nlaplace_mean_corrs_list = [corrs.mean() for corrs in laplace_valid_corrs_list]\n\nplt.title(\"Correrations mean Histogram\")\nplt.hist(uniform_mean_corrs_list, alpha=0.5, bins=50, label='uniform prior')\nplt.hist(normal_mean_corrs_list, alpha=0.5, bins=50, label='normal prior')\nplt.hist(laplace_mean_corrs_list, alpha=0.5, bins=50, label='laplace prior')\nplt.legend()\nplt.show()","aa8a3085":"uniform_sharpe_ratio_list = [corrs.mean() \/ corrs.std() for corrs in uniform_valid_corrs_list]\nnormal_sharpe_ratio_list = [corrs.mean() \/ corrs.std() for corrs in normal_valid_corrs_list]\nlaplace_sharpe_ratio_list = [corrs.mean() \/ corrs.std() for corrs in laplace_valid_corrs_list]\n\nplt.title(\"Sharpe Ratio Histogram\")\nplt.hist(uniform_sharpe_ratio_list, alpha=0.5, bins=50, label='uniform prior')\nplt.hist(normal_sharpe_ratio_list, alpha=0.5, bins=50, label='normal prior')\nplt.hist(laplace_sharpe_ratio_list, alpha=0.5, bins=50, label='laplace prior')\nplt.legend()\nplt.show()","875a7c1b":"def eval_corrs_list(corrs_list):\n    mean_corrs_list = [corrs.mean() for corrs in corrs_list]\n    mean_corrs_max = np.max(mean_corrs_list)\n    mean_corrs_min = np.min(mean_corrs_list)\n    mean_corrs_mean = np.mean(mean_corrs_list)\n    mean_corrs_std = np.std(mean_corrs_list)\n    \n    print('Correration Mean')\n    print(f\"Max: {mean_corrs_max} Min: {mean_corrs_min}\")\n    print(f\"Mean: {mean_corrs_mean} Std: {mean_corrs_std}\")\n    \n    sharpe_ratio_list = [corrs.mean() \/ corrs.std() for corrs in corrs_list]\n    \n    print('Sharpe Ratio')\n    sharpe_ratio_max = np.max(sharpe_ratio_list)\n    sharpe_ratio_min = np.min(sharpe_ratio_list)\n    sharpe_ratio_mean = np.mean(sharpe_ratio_list)\n    sharpe_ratio_std = np.std(sharpe_ratio_list)\n    double_sharpe_ratio = sharpe_ratio_mean \/ sharpe_ratio_std\n    print(f\"Max: {sharpe_ratio_max} Min: {sharpe_ratio_min}\")\n    print(f\"Mean: {sharpe_ratio_mean} Std: {sharpe_ratio_std}\")\n    print(f\"Double Sharpe Ratio: {double_sharpe_ratio}\")\n\nprint('prior distribution: uniform')\neval_corrs_list(uniform_valid_corrs_list)\nprint()\nprint('prior distribution: normal')\neval_corrs_list(normal_valid_corrs_list)\nprint()\nprint('prior distribution: laplace')\neval_corrs_list(laplace_valid_corrs_list)","086932d8":"from sklearn.metrics import roc_curve, roc_auc_score\n\n# detect burn era proccess using out of distribution\ndef burn_ood(validation_data, valid_corrs, prior_name):\n    std_mean = validation_data.groupby('era')['std'].mean().values\n    is_burn = (valid_corrs < 0).astype('int').values\n    fpr, tpr, thresholds = roc_curve(is_burn, std_mean)\n    auc = roc_auc_score(is_burn, std_mean)\n    print(f\"burn detection auc: {auc}\")\n    plt.plot(fpr, tpr, marker='o', label=prior_name + \": \" + str(auc)[:5])\n    plt.xlabel('FPR: False positive rate')\n    plt.ylabel('TPR: True positive rate')\n    plt.grid()\n","83b7f261":"print('uniform')\nvalidation_data['std'] = uniform_valid_pred_array.std(axis=1)\nburn_ood(validation_data, valid_corrs, 'uniform')\n\nprint('normal')\nvalidation_data['std'] = normal_valid_pred_array.std(axis=1)\nburn_ood(validation_data, valid_corrs, 'normal')\n\nprint('laplace')\nvalidation_data['std'] = laplace_valid_pred_array.std(axis=1)\nburn_ood(validation_data, valid_corrs, 'laplace')\n\nplt.legend()\nplt.show()","e14c2d54":"def sample_weight(model, samples=1000):\n    l1_weight = np.zeros((num_samples, len(features)))\n    for i in range(num_samples):\n        l1_weight[i, :] = model.l1.weight.numpy()[0, :]\n    return l1_weight","5ab12efb":"idx = 5\nplt.title(f\"{features[idx]}\")\nplt.hist(sample_weight(uniform_bnn)[:, idx], alpha=0.5, bins=50, label='uniform prior')\nplt.hist(sample_weight(normal_bnn)[:, idx], alpha=0.5, bins=50, label='normal prior')\nplt.hist(sample_weight(laplace_bnn)[:, idx], alpha=0.5, bins=50, label='laplace prior')\n\nplt.legend()\nplt.show()","427ec434":"feat_std = pd.DataFrame()\nfeat_std['feature'] = features\nfeat_std['std'] = [sample_weight(uniform_bnn)[:, idx].std() for idx in range(len(features))]\n\nsns.set(font_scale = 1)\n\nplt.figure(figsize=(15, 15))\nplt.title(f'uniform std higher')\nsns.barplot(x=\"std\", y=\"feature\", data=feat_std.sort_values(by=\"std\", ascending=False))\nplt.show()","b7e6de2b":"feat_std = pd.DataFrame()\nfeat_std['feature'] = features\nfeat_std['std'] = [sample_weight(normal_bnn)[:, idx].std() for idx in range(len(features))]\n\nsns.set(font_scale = 1)\n\nplt.figure(figsize=(15, 15))\nplt.title(f'normal std higher')\nsns.barplot(x=\"std\", y=\"feature\", data=feat_std.sort_values(by=\"std\", ascending=False))\nplt.show()","98be471d":"feat_std = pd.DataFrame()\nfeat_std['feature'] = features\nfeat_std['std'] = [sample_weight(laplace_bnn)[:, idx].std() for idx in range(len(features))]\n\nsns.set(font_scale = 1)\n\nplt.figure(figsize=(15, 15))\nplt.title(f'laplace std higher')\nsns.barplot(x=\"std\", y=\"feature\", data=feat_std.sort_values(by=\"std\", ascending=False))\nplt.show()","5369f13d":"### 1. Baysian Model Averaging\nAs a beginning of the evaluation of Bayesian NN, I evaluate the ensemble assuming actual operation.  \nI sample 1000 pairs of parameters from the posterior distribution and predict them using Bayesian Model Averaging, averaging the prediction results of each parameter for a single data.  \n\n\n\u30d9\u30a4\u30baNN\u306e\u8a55\u4fa1\u306e\u59cb\u3081\u3068\u3057\u3066\u3001\u5b9f\u969b\u306e\u904b\u7528\u3092\u60f3\u5b9a\u3057\u3066\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u306e\u8a55\u4fa1\u3092\u884c\u3044\u307e\u3059\u3002  \n\u4e8b\u5f8c\u5206\u5e03\u304b\u3089\u30d1\u30e9\u30e1\u30fc\u30bf\u30921000\u7d44\u30b5\u30f3\u30d7\u30eb\u3057\u3001\u4e00\u3064\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3001\u305d\u308c\u305e\u308c\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u4e88\u6e2c\u7d50\u679c\u3092\u5e73\u5747\u3057\u305fBayesian Model Averaging\u3092\u5229\u7528\u3057\u3066\u4e88\u6e2c\u3057\u307e\u3059\u3002","1b0dfc86":"The results of this experiment show that the results of the ensemble are more accurate than the Mean of each distribution.  \nFrom this, we can see that the ensemble works effectively in NN.   \nAlso, in terms of Double Sharpe Ratio, Bayesian NN with Laplace distribution seems to be the best.\n\n\u3053\u3061\u3089\u306e\u691c\u8a3c\u7d50\u679c\u3092\u898b\u308b\u3068\u3001\u5404\u5206\u5e03\u306eMean\u3088\u308a\u3082\u5148\u7a0b\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u306e\u7d50\u679c\u306e\u307b\u3046\u304c\u7cbe\u5ea6\u304c\u3088\u3044\u3067\u3059\u3002  \n\u3053\u3053\u304b\u3089\u3001NN\u306b\u304a\u3044\u3066\u3082\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u304c\u6709\u52b9\u306b\u50cd\u3044\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002  \n\u307e\u305f\u3001Double Sharpe Ratio\u7684\u306b\u898b\u308b\u3068\u3001\u30e9\u30d7\u30e9\u30b9\u5206\u5e03\u306e\u30d9\u30a4\u30baNN\u304c\u4e00\u756a\u826f\u3055\u305d\u3046\u3067\u3059\u3002","1f1397f6":"There is likely to be variation in the distribution of evaluations in a normal distribution.  \nThe next step is to calculate the statistics of the distribution of the evaluations.  \nThis time, in addition to the mean, minimum, maximum, and standard deviation, we calculated the \"Sharpe Ratio of Sharpe Ratio\".  \nFrom now on, we call it Double Sharpe Ratio.  \n(The original Double Sharpe Ratio is said to be calculated using the bootstrap method, so it is strictly different. https:\/\/papers.ssrn.com\/sol3\/papers.cfm?abstract_id=168748)  \n\n\u6b63\u898f\u5206\u5e03\u306b\u306f\u8a55\u4fa1\u306e\u5206\u5e03\u306b\u3070\u3089\u3064\u304d\u304c\u3042\u308a\u305d\u3046\u3067\u3059\u3002  \n\u6b21\u306b\u8a55\u4fa1\u306e\u5206\u5e03\u306e\u7d71\u8a08\u91cf\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002  \n\u4eca\u56de\u306f\u5e73\u5747\u3001\u6700\u5c0f\u5024\u3001\u6700\u5927\u5024\u3001\u6a19\u6e96\u504f\u5dee\u306b\u52a0\u3048\u3066\u3001Sharpe Ratio\u306eSharpe Ratio\u3092\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u305f\u3002  \n\u4eca\u5f8c\u306fDouble Sharpe Ratio\u3092\u547c\u3093\u3067\u3044\u307e\u3059\u3002  \n(\u672c\u6765\u306eDouble Sharpe Ratio\u306f\u30d6\u30fc\u30c8\u30b9\u30c8\u30e9\u30c3\u30d7\u6cd5\u3067\u8a08\u7b97\u3059\u308b\u305d\u3046\u306a\u306e\u3067\u3001\u53b3\u5bc6\u306b\u306f\u7570\u306a\u308a\u307e\u3059 https:\/\/papers.ssrn.com\/sol3\/papers.cfm?abstract_id=168748 \uff09","90067c24":"The uniform distribution is below the chance rate (0.5).  \nOn the other hand, the Normal and Laplace distributions may be able to detect the Burn Era somewhat above the chance rate.  \nWe may be able to improve the accuracy of Burn Era detection by devising a better architecture and uncertainty calculation method.  \n\n\u4e00\u69d8\u5206\u5e03\u306f\u30c1\u30e3\u30f3\u30b9\u30ec\u30fc\u30c8(0.5)\u4ee5\u4e0b\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002  \n\u4e00\u65b9\u3067\u3001\u6b63\u898f\u5206\u5e03\u3001\u30e9\u30d7\u30e9\u30b9\u5206\u5e03\u306f\u30c1\u30e3\u30f3\u30b9\u30ec\u30fc\u30c8\u4ee5\u4e0a\u3067\u591a\u5c11Burn Era\u3092\u691c\u51fa\u3067\u304d\u308b\u306e\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002  \n\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3084\u4e0d\u78ba\u5b9f\u6027\u306e\u8a08\u7b97\u65b9\u6cd5\u3092\u5de5\u592b\u3059\u308c\u3070\u3001Burn Era\u691c\u51fa\u306e\u7cbe\u5ea6\u3092\u5411\u4e0a\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u306d\u3002  ","988cce19":"## Evaluation for Baysian Neural Networks","1b68bbc0":"In terms of accuracy alone, the uniform distribution, Laplace distribution, and normal distribution seem to be good in that order.  \nSince this is a conventional analysis method, let's move on to the next evaluation.  \n\n\n\u7cbe\u5ea6\u3060\u3051\u3067\u898b\u308b\u3068\u3001\u4e00\u69d8\u5206\u5e03\u3001\u30e9\u30d7\u30e9\u30b9\u5206\u5e03\u3001\u6b63\u898f\u5206\u5e03\u306e\u9806\u3067\u826f\u3055\u305d\u3046\u3067\u3059\u3002  \n\u3053\u3053\u306f\u5f93\u6765\u306e\u5206\u6790\u624b\u6cd5\u306a\u306e\u3067\u3001\u6b21\u306e\u8a55\u4fa1\u306b\u3046\u3064\u308a\u307e\u3059\u3002","569e422c":"### 2. Analysis of Evaluation Distribution\nBy evaluating each of the predictions, we can calculate the distribution of the evaluations.  \nI use the distribution of evaluations to perform various types of analysis.\n\n\u305d\u308c\u305e\u308c\u306e\u4e88\u6e2c\u306b\u5bfe\u3057\u3066\u8a55\u4fa1\u3092\u3059\u308b\u3053\u3068\u3067\u3001\u8a55\u4fa1\u306e\u5206\u5e03\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002  \n\u8a55\u4fa1\u306e\u5206\u5e03\u3092\u6d3b\u7528\u3057\u3001\u69d8\u3005\u306a\u5206\u6790\u3092\u3057\u3066\u307f\u307e\u3059\u3002","4afbc0cf":"## Training and Evaluation for Neural Networks\nFirst, we are training regular Neural Networks as a baseline.  \nThe architecture of the model is based on [Notebook published by katsu](https:\/\/www.kaggle.com\/code1110\/numerai-metric-learning-and-live-era\/notebook).  \nThe changes are as follows  \n- Removed regularization layers (dropout, additive gaussian nosie).\n- Changed the output to only target_nomi  \n\n\n\u6700\u521d\u306b\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\u3068\u3057\u3066\u901a\u5e38\u306eNeural Networks\u3092\u5b66\u7fd2\u3057\u3066\u3044\u307e\u3059\u3002  \n\u30e2\u30c7\u30eb\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306f[katsu\u3055\u3093\u304c\u516c\u958b\u3055\u308c\u3066\u3044\u308bNotebook](https:\/\/www.kaggle.com\/code1110\/numerai-metric-learning-and-live-era\/notebook)\u3092\u53c2\u8003\u306b\u3057\u3066\u3044\u307e\u3059\u3002  \n\u5909\u66f4\u70b9\u3068\u3057\u3066\u306f\u4ee5\u4e0b\u306b\u306a\u308a\u307e\u3059\u3002  \n- \u6b63\u5247\u5316\u30ec\u30a4\u30e4\u30fc\uff08dropout, gaussian nosie)\u306e\u524a\u9664\n- \u51fa\u529b\u3092target_nomi\u306e1\u6b21\u5143\u306b\u5909\u66f4","9a860498":"## Conclusion\nBy using Bayesian NN, I was able to analyze the parameters and predictions as distributions rather than as expected values.  \nHowever, this is just the beginning, and there are a few things that I need to examine more before we can conclude our analysis.  \n### 1. Comparing ensembles with Bayesian NNs and ensembles with multiple models of regular NNs\nIn NN operations, it is often done to build multiple models and ensemble them.  \nThis time, I didn't do it because I didn't want to conclude that Bayesian NN is better in terms of performance, but I think this is the most interesting part of model operation.  \n\n### 2. Superiority of model selection by distribution of evaluation and Burn Era detection accuracy\nThe evaluation of the ensemble of Bayesian NN alone showed similar results, but the distribution of the evaluations showed that the Bayesian NN with a normal distribution as the prior distribution varied.  \nIf the Double Sharpe Ratio and Burn Era AUC are effective as a model selection, it may be a solution to the difficulty of model evaluation in Numerai.\n\n\u30d9\u30a4\u30baNN\u306b\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u3084\u4e88\u6e2c\u3092\u671f\u5f85\u5024\u3067\u306f\u306a\u304f\u3001\u5206\u5e03\u3068\u3057\u3066\u5206\u6790\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002  \n\u305f\u3060\u3001\u4eca\u56de\u306e\u691c\u8a3c\u306f\u59cb\u3081\u306b\u3059\u304e\u305a\u3001\u3082\u3046\u5c11\u3057\u691c\u8a3c\u3057\u306a\u3044\u3068\u7d50\u8ad6\u3065\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044\u5206\u6790\u5185\u5bb9\u304c\u3044\u304f\u3064\u304b\u3042\u308b\u306e\u3067\u3054\u7d39\u4ecb\u3057\u307e\u3059\u3002  \n\n### 1. \u30d9\u30a4\u30baNN\u306b\u3088\u308b\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3068\u901a\u5e38NN\u306e\u8907\u6570\u30e2\u30c7\u30eb\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u306e\u6bd4\u8f03\nNN\u306e\u904b\u7528\u3067\u306f\u3001\u8907\u6570\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u3066\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3092\u3059\u308b\u3053\u3068\u304c\u3088\u304f\u3084\u3089\u308c\u308b\u3068\u601d\u3044\u307e\u3059\u3002  \n\u4eca\u56de\u306f\u7cbe\u5ea6\u7684\u306b\u30d9\u30a4\u30baNN\u304c\u826f\u3044\u3068\u7d50\u8ad6\u3065\u3051\u308b\u3053\u3068\u304c\u76ee\u7684\u3067\u306f\u306a\u304b\u3063\u305f\u306e\u3067\u3084\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u304c\u3001\u30e2\u30c7\u30eb\u306e\u904b\u7528\u3068\u3057\u3066\u4e00\u756a\u6c17\u306b\u306a\u308b\u90e8\u5206\u3060\u3068\u601d\u3044\u307e\u3059\u3002  \n\n### 2. \u8a55\u4fa1\u306e\u5206\u5e03\u3084Burn Era\u691c\u51fa\u7cbe\u5ea6\u306b\u3088\u308b\u30e2\u30c7\u30eb\u9078\u629e\u306e\u512a\u4f4d\u6027\n\u30d9\u30a4\u30baNN\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u306e\u8a55\u4fa1\u3060\u3051\u3060\u3068\u3001\u4f3c\u305f\u3088\u3046\u306a\u6570\u5024\u7d50\u679c\u306b\u306a\u3063\u3066\u3044\u305f\u304c\u3001\u8a55\u4fa1\u306e\u5206\u5e03\u3092\u898b\u308b\u3068\u6b63\u898f\u5206\u5e03\u3092\u4e8b\u524d\u5206\u5e03\u3068\u3057\u305f\u30d9\u30a4\u30baNN\u306f\u3070\u3089\u3064\u304d\u304c\u3042\u308a\u307e\u3057\u305f\u3002  \n\u8a55\u4fa1\u306e\u3070\u3089\u3064\u304d\u5177\u5408\u3082\u8003\u616e\u3057\u3066\u7cbe\u5ea6\u3092\u5b9a\u91cf\u5316\u3057\u305fDouble Sharpe Ratio\u3084Burn Era\u306e\u691c\u51fa\u7cbe\u5ea6\u304c\u30e2\u30c7\u30eb\u9078\u629e\u3068\u3057\u3066\u6709\u52b9\u306b\u50cd\u304f\u306e\u3067\u3042\u308c\u3070\u3001  \nNumerai\u306e\u30e2\u30c7\u30eb\u8a55\u4fa1\u306e\u56f0\u96e3\u6027\u306b\u5bfe\u3059\u308b\u89e3\u6c7a\u7b56\u306b\u306a\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002","29cad8ce":"## Training Bayesian Neural Networks\nI implement the above NN by changing it to Bayesian NN.  \nFramework is Pyro.  \nComputation of posterior distribution is variational inference.  \n\n\u4e0a\u8a18\u306eNN\u3092\u30d9\u30a4\u30baNN\u306b\u5909\u66f4\u3057\u3066\u5b9f\u88c5\u3057\u307e\u3059\u3002  \nFramework\u3068\u3057\u3066Pyro\u3092\u5229\u7528\u3057\u3001\u5909\u5206\u63a8\u8ad6\u3067\u5b66\u7fd2\u3057\u3066\u3044\u307e\u3059\u3002","17642f17":"### 4. Analysis of weight distribution\nFinally, let's sample the weights and visualize them.  \nThis is not a particularly in-depth analysis, but you can sample the distribution of weights as shown here.  \n\n\n\u6700\u5f8c\u306b\u91cd\u307f\u3092\u30b5\u30f3\u30d7\u30eb\u3057\u3066\u53ef\u8996\u5316\u3057\u3066\u307f\u307e\u3059\u3002  \n\u7279\u306b\u6df1\u307c\u3063\u3066\u5206\u6790\u3057\u3066\u3044\u307e\u305b\u3093\u304c\u3001\u3053\u3061\u3089\u306e\u3088\u3046\u306b\u91cd\u307f\u306e\u5206\u5e03\u3092\u30b5\u30f3\u30d7\u30eb\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002","55b59c04":"### 3. Detection Burn Era using Uncertainty\nWe attempted to detect the Burn Era using the standard deviation of the predictive distribution as the uncertainty.  \nThe procedure of the verification method is as follows: \n1. Calculate the correlation between the target and the ensemble predictions for each Era, and define the Era with a negative correlation as the Burn Era.\n2. Calculate the standard deviation of the predicted values, and define it as the uncertainty of each data. \n3. Average the standard deviation for each Era and calculate the uncertainty of Era.\n4. Calculate the AUC to see if the Burn Era can be detected from the uncertainty of the Era.  \n\n\u4e88\u6e2c\u5206\u5e03\u306e\u6a19\u6e96\u504f\u5dee\u3092\u4e0d\u78ba\u5b9f\u6027\u3068\u3057\u3066\u3001Burn Era\u306e\u691c\u51fa\u3092\u8a66\u307f\u3066\u3044\u307e\u3059\u3002  \n\u691c\u8a3c\u65b9\u6cd5\u306e\u624b\u9806\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002  \n1. \u5404Era\u306etarget\u3068\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u4e88\u6e2c\u5024\u304b\u3089\u76f8\u95a2\u3092\u8a08\u7b97\u3057\u3001\u76f8\u95a2\u304c\u8ca0\u306b\u306a\u3063\u305fera\u3092Burn Era\u3068\u3059\u308b\n2. \u4e88\u6e2c\u5024\u306e\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u3057\u3001\u5404\u30c7\u30fc\u30bf\u306e\u4e0d\u78ba\u5b9f\u6027\u3068\u3059\u308b\n3. Era\u3054\u3068\u306b\u6a19\u6e96\u504f\u5dee\u3092\u5e73\u5747\u3057\u3001Era\u306e\u4e0d\u78ba\u5b9f\u6027\u3092\u8a08\u7b97\n4. Era\u306e\u4e0d\u78ba\u5b9f\u6027\u304b\u3089Burn Era\u3092\u691c\u51fa\u3067\u304d\u308b\u304b\u3069\u3046\u304bAUC\u3067\u8a08\u7b97","b302ea9d":"## Setup","9b42de0b":"## Introduction\n\nThis Notebook is the 21st day of [Numerai Advent Calendar 2021](https:\/\/adventar.org\/calendars\/6226) .  \nAs one of the ways to evaluate different perspectives for model selection, I'm trying to build Bayesian Neural Network and sample parameters for various evaluations.  \nBased on the experiment results of this Notebook, I have found the following\n- The ensemble evaluation by Bayesian NN is higher than the average of the distribution of the evaluations, so the ensemble is working effectively in NN.\n- Uncertainty that can be computed from Bayesian NN may be able to detect the Burn Era.\n\nThe contents of this presentation were also presented at Numerai Meetup JAPAN 2021 held recently.  \nPlease check the following for detailed discussion.    \nhttps:\/\/speakerdeck.com\/habakan\/nn-startermoderuwo-beizunnnisitefen-xi-sitemita   \n\nThis is the implementation code, but due to the fact that I was building multiple notebooks, it is only the code up to experiment 2 of the presentation.  \nThe code for experiment 3, which was presented at the Meetup, is not that difficult to implement, so I will skip it.  \n\nIf you appreciate this Notebook, or have an excess of NMRs, Please send NMR to my wallet address below!  \n```0x00000000000000000000000000000000000224f3```\n\n\u3053\u306eNotebook\u306f[Numerai Advent Calendar 2021](https:\/\/adventar.org\/calendars\/6226)\u306e21\u65e5\u76ee\u306e\u8a18\u4e8b\u3067\u3059\u3002  \n\u30e2\u30c7\u30eb\u9078\u629e\u306e\u305f\u3081\u306e\u5225\u8996\u70b9\u3067\u306e\u8a55\u4fa1\u306e\u4ed5\u65b9\u306e\u4e00\u3064\u3068\u3057\u3066\u3001Neural Network\u3092\u30d9\u30a4\u30ba\u5316\u3057\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30b5\u30f3\u30d7\u30eb\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u69d8\u3005\u306a\u8a55\u4fa1\u3092\u8a66\u307f\u3066\u3044\u307e\u3059\u3002  \n\u672cNotebook\u306e\u691c\u8a3c\u7d50\u679c\u3092\u3082\u3068\u306b\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\n- \u8a55\u4fa1\u306e\u5206\u5e03\u306e\u5e73\u5747\u3088\u308a\u3082\u3001\u30d9\u30a4\u30baNN\u306b\u3088\u308b\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u306e\u8a55\u4fa1\u306e\u307b\u3046\u304c\u9ad8\u3044\u3053\u3068\u304b\u3089\u3001NN\u306b\u304a\u3044\u3066\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u306f\u6709\u52b9\u306b\u50cd\u3044\u3066\u3044\u308b\n- \u30d9\u30a4\u30baNN\u304b\u3089\u8a08\u7b97\u3067\u304d\u308b\u4e0d\u78ba\u5b9f\u6027\u304cBurn Era\u3092\u691c\u51fa\u3067\u304d\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\n\n\u307e\u305f\u3001\u4eca\u56de\u306e\u5185\u5bb9\u306f\u5148\u65e5\u958b\u50ac\u3055\u308c\u305fNumerai Meetup JAPAN 2021\u3067\u767a\u8868\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u3066\u304a\u308a\u307e\u3059\u3002  \n\u8003\u5bdf\u5185\u5bb9\u306a\u3069\u304c\u91cd\u8907\u3057\u307e\u3059\u306e\u3067\u3001\u8a73\u7d30\u306a\u8003\u5bdf\u306f\u4e0b\u8a18\u3092\u78ba\u8a8d\u304f\u3060\u3055\u3044\u3002    \nhttps:\/\/speakerdeck.com\/habakan\/nn-startermoderuwo-beizunnnisitefen-xi-sitemita   \n\n\u306a\u304a\u5b9f\u88c5\u30b3\u30fc\u30c9\u3067\u3059\u304c\u3001Notebook\u3092\u8907\u6570\u69cb\u7bc9\u3057\u3066\u3044\u305f\u95a2\u4fc2\u4e0a\u3001\u767a\u8868\u5185\u5bb9\u306e\u691c\u8a3c2\u307e\u3067\u306e\u518d\u73fe\u30b3\u30fc\u30c9\u306b\u306a\u308a\u307e\u3059\u3002  \nMeetup\u3067\u767a\u8868\u3057\u305f\u691c\u8a3c3\u306e\u8907\u6570\u30bf\u30fc\u30b2\u30c3\u30c8\u5229\u7528\u306f\u305d\u3053\u307e\u3067\u5b9f\u88c5\u306f\u96e3\u3057\u304f\u306a\u3044\u306e\u3067\u7701\u7565\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3059\u3002  \n\n\u3082\u3057\u3053\u306eNotebook\u3092\u8a55\u4fa1\u3057\u3066\u3044\u305f\u3060\u3051\u305f\u308a\u3001NMR\u304c\u6709\u308a\u4f59\u3063\u3066\u3044\u308b\u65b9\u3001\u6295\u3052\u92ad\u6b53\u8fce\u3067\u3059\uff01\u7b11  \n```0x00000000000000000000000000000000000224f3```"}}