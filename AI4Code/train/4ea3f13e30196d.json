{"cell_type":{"36a0a4b5":"code","55744091":"code","61a3ff3f":"code","37296ac3":"code","aeccfb7a":"code","68dac844":"code","4ea153de":"code","91c9c8a3":"code","aa94f739":"code","a192ec27":"code","5e8a3fb4":"code","944ef4c5":"code","30603de5":"code","c2a9be17":"code","9d1b1223":"code","1e561879":"code","5421c340":"code","49a5d319":"code","29ac34ff":"code","0b859d90":"code","95aa169f":"code","abed3006":"code","723cf154":"code","8d5e8a22":"markdown","45b83383":"markdown","bb165fd0":"markdown","af3af79c":"markdown","528158fd":"markdown","bdebdca2":"markdown","a438b6ea":"markdown","3c7d9729":"markdown","396a6a04":"markdown"},"source":{"36a0a4b5":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression","55744091":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntrain_df.set_index('id', inplace=True)\nprint(f\"train_df: {train_df.shape}\")\ntrain_df.head()","61a3ff3f":"test_df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\ntest_df.set_index('id', inplace=True)\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","37296ac3":"features = test_df.columns.tolist()\n\ntrain_df['num_missing'] = train_df[features].isna().sum(axis=1)\ntrain_df['num_missing_std'] = train_df[features].isna().std(axis=1).astype('float')\ntrain_df['median'] = train_df[features].median(axis=1)\ntrain_df['std'] = train_df[features].std(axis=1)\ntrain_df['min'] = train_df[features].abs().min(axis=1)\ntrain_df['max'] = train_df[features].abs().max(axis=1)\ntrain_df['sem'] = train_df[features].sem(axis=1)\n\ntest_df['num_missing'] = test_df[features].isna().sum(axis=1)\ntest_df['num_missing_std'] = test_df[features].isna().std(axis=1).astype('float')\ntest_df['median'] = test_df[features].median(axis=1)\ntest_df['std'] = test_df[features].std(axis=1)\ntest_df['min'] = test_df[features].abs().min(axis=1)\ntest_df['max'] = test_df[features].abs().max(axis=1)\ntest_df['sem'] = test_df[features].sem(axis=1)\n\nprint(f\"train_df: {train_df.shape} \\ntest_df: {test_df.shape}\")\ntrain_df.head()","aeccfb7a":"fill_value_dict = {\n    'f1': 'Mean', \n    'f2': 'Mean', \n    'f3': 'Mode', \n    'f4': 'Mode', \n    'f5': 'Mode', \n    'f6': 'Mean', \n    'f7': 'Mean', \n    'f8': 'Median', \n    'f9': 'Mode', \n    'f10': 'Mode', \n    'f11': 'Mode', \n    'f12': 'Median', \n    'f13': 'Mode', \n    'f14': 'Median', \n    'f15': 'Mean', \n    'f16': 'Median', \n    'f17': 'Mode', \n    'f18': 'Median', \n    'f19': 'Median', \n    'f20': 'Median', \n    'f21': 'Median', \n    'f22': 'Mean', \n    'f23': 'Mode', \n    'f24': 'Median', \n    'f25': 'Median', \n    'f26': 'Median', \n    'f27': 'Median', \n    'f28': 'Median', \n    'f29': 'Mean', \n    'f30': 'Median', \n    'f31': 'Mode', \n    'f32': 'Median', \n    'f33': 'Median', \n    'f34': 'Mean', \n    'f35': 'Median', \n    'f36': 'Median', \n    'f37': 'Median', \n    'f38': 'Mode', \n    'f39': 'Median', \n    'f40': 'Mean', \n    'f41': 'Median', \n    'f42': 'Mean', \n    'f43': 'Mode', \n    'f44': 'Median', \n    'f45': 'Median', \n    'f46': 'Mean', \n    'f47': 'Mean', \n    'f48': 'Median', \n    'f49': 'Mode', \n    'f50': 'Mean', \n    'f51': 'Median', \n    'f52': 'Median', \n    'f53': 'Median', \n    'f54': 'Median', \n    'f55': 'Mode', \n    'f56': 'Mean', \n    'f57': 'Mean', \n    'f58': 'Median', \n    'f59': 'Median', \n    'f60': 'Mode', \n    'f61': 'Mode', \n    'f62': 'Median', \n    'f63': 'Median', \n    'f64': 'Median', \n    'f65': 'Mean', \n    'f66': 'Mode', \n    'f67': 'Median', \n    'f68': 'Median', \n    'f69': 'Mode', \n    'f70': 'Mean', \n    'f71': 'Median', \n    'f72': 'Median', \n    'f73': 'Median', \n    'f74': 'Median', \n    'f75': 'Mean', \n    'f76': 'Mean', \n    'f77': 'Median', \n    'f78': 'Median', \n    'f79': 'Median', \n    'f80': 'Median', \n    'f81': 'Median', \n    'f82': 'Median', \n    'f83': 'Median', \n    'f84': 'Median', \n    'f85': 'Median', \n    'f86': 'Median', \n    'f87': 'Median', \n    'f88': 'Median', \n    'f89': 'Median', \n    'f90': 'Mean', \n    'f91': 'Mode', \n    'f92': 'Median', \n    'f93': 'Median', \n    'f94': 'Mode', \n    'f95': 'Median', \n    'f96': 'Median', \n    'f97': 'Mean', \n    'f98': 'Median', \n    'f99': 'Median', \n    'f100': 'Mean', \n    'f101': 'Median', \n    'f102': 'Median', \n    'f103': 'Median', \n    'f104': 'Median', \n    'f105': 'Mode', \n    'f106': 'Median', \n    'f107': 'Median', \n    'f108': 'Median', \n    'f109': 'Median', \n    'f110': 'Mode', \n    'f111': 'Median', \n    'f112': 'Median', \n    'f113': 'Median', \n    'f114': 'Median', \n    'f115': 'Mode', \n    'f116': 'Median', \n    'f117': 'Median', \n    'f118': 'Mean'\n}\n\n\nfor col in tqdm(features):\n    if fill_value_dict.get(col)=='Mean':\n        fill_value = train_df[col].mean()\n    elif fill_value_dict.get(col)=='Median':\n        fill_value = train_df[col].median()\n    elif fill_value_dict.get(col)=='Mode':\n        fill_value = train_df[col].mode().iloc[0]\n    \n    train_df[col].fillna(fill_value, inplace=True)\n    test_df[col].fillna(fill_value, inplace=True)\n\ntrain_df.head()","68dac844":"features = [col for col in train_df.columns if col not in ['num_missing','num_missing_std','claim']]\n\nfor col in tqdm(features):\n    transformer = QuantileTransformer(n_quantiles=3000, \n                                      random_state=42, \n                                      output_distribution=\"normal\")\n    \n    vec_len = len(train_df[col].values)\n    vec_len_test = len(test_df[col].values)\n\n    raw_vec = train_df[col].values.reshape(vec_len, 1)\n    test_vec = test_df[col].values.reshape(vec_len_test, 1)\n    transformer.fit(raw_vec)\n    \n    train_df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_df[col] = transformer.transform(test_vec).reshape(1, vec_len_test)[0]\n\nprint(f\"train_df: {train_df.shape} \\ntest_df: {test_df.shape}\")","4ea153de":"def kmeans_fet(train, test, features, n_clusters):\n    \n    train_ = train[features].copy()\n    test_ = test[features].copy()\n    data = pd.concat([train_, test_], axis=0)\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(data)\n    \n    train[f'clusters_k'] = kmeans.labels_[:train.shape[0]]\n    test[f'clusters_k'] = kmeans.labels_[train.shape[0]:]\n    return train, test","91c9c8a3":"train_df, test_df = kmeans_fet(train_df, test_df, features, n_clusters=4)\n\nXtrain = train_df.loc[:, train_df.columns != 'claim'].copy()\nYtrain = train_df['claim'].copy()\nXtest = test_df.copy()\n\nprint(f\"Xtrain: {Xtrain.shape} \\nYtrain: {Ytrain.shape} \\nXtest: {Xtest.shape}\")\n\ndel train_df\ndel test_df\ngc.collect()","aa94f739":"FOLD = 5\nSEEDS = [24, 42]\n\n\nlgb_params1 = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n    'device_type': 'gpu', \n    'n_estimators': 10000, \n    'learning_rate': 0.12230165751633416, \n    'num_leaves': 1400, \n    'max_depth': 8, \n    'min_child_samples': 300, \n    'reg_alpha': 10, \n    'reg_lambda': 65, \n    'min_split_gain': 5.157818977461183, \n    'subsample': 0.5, \n    'subsample_freq': 1, \n    'colsample_bytree': 0.2,\n    'random_state': 42\n}\n\nlgb_params2 = {\n    'metric' : 'auc',\n    'max_depth' : 3,\n    'num_leaves' : 7,\n    'n_estimators' : 5000,\n    'colsample_bytree' : 0.3,\n    'subsample' : 0.5,\n    'random_state' : 42,\n    'reg_alpha' : 18,\n    'reg_lambda' : 17,\n    'learning_rate' : 0.095,\n    'device' : 'gpu',\n    'objective' : 'binary'\n}\n\ncb_params1 = {\n    'eval_metric' : 'AUC',\n    'iterations': 15585, \n    'objective': 'CrossEntropy',\n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0,\n    'random_state': 42\n}\n\ncb_params2 = {\n    'eval_metric' : 'AUC',\n    'depth' : 5,\n    'grow_policy' : 'SymmetricTree',\n    'l2_leaf_reg' : 3.0,\n    'random_strength' : 1.0,\n    'learning_rate' : 0.1,\n    'iterations' : 10000,\n    'loss_function' : 'CrossEntropy',\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0,\n    'random_state': 42\n}\n\nxgb_params1 = {\n    'eval_metric': 'auc', \n    'objective': 'binary:logistic', \n    'tree_method': 'gpu_hist', \n    'gpu_id': 0, \n    'predictor': 'gpu_predictor', \n    'n_estimators': 10000, \n    'learning_rate': 0.01063045229441343, \n    'gamma': 0.24652519525750877, \n    'max_depth': 4, \n    'min_child_weight': 366, \n    'subsample': 0.6423040816299684, \n    'colsample_bytree': 0.7751264493218339, \n    'colsample_bylevel': 0.8675692743597421, \n    'lambda': 0, \n    'alpha': 10,\n    'random_state': 42\n}\n\nxgb_params2 = {\n    'eval_metric': 'auc',\n    'max_depth': 3,\n    'subsample': 0.5,\n    'colsample_bytree': 0.5,\n    'learning_rate': 0.01187431306013263,\n    'n_estimators': 10000,\n    'n_jobs': -1,\n    'use_label_encoder': False,\n    'objective': 'binary:logistic',\n    'tree_method': 'gpu_hist',\n    'gpu_id': 0,\n    'predictor': 'gpu_predictor',\n    'random_state': 42\n}","a192ec27":"counter = 0\noof_score = 0\ny_pred_final_xgb1 = np.zeros((Xtest.shape[0], 1))\ny_pred_meta_xgb1 = np.zeros((Xtrain.shape[0], 1))\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain.values, Ytrain.values)):\n        counter += 1\n\n        train_x, train_y = Xtrain.iloc[train], Ytrain.iloc[train]\n        val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\n\n        model = XGBClassifier(**xgb_params1)\n\n        model.fit(train_x, train_y, eval_set=[(train_x, train_y), (val_x, val_y)], \n                  early_stopping_rounds=200, verbose=1000)\n        \n        y_pred = model.predict_proba(val_x, iteration_range=(0, model.best_iteration))[:,-1]\n        y_pred_meta_xgb1[val] += np.array([y_pred]).T\n        y_pred_final_xgb1 += np.array([model.predict_proba(Xtest, iteration_range=(0, model.best_iteration))[:,-1]]).T\n        \n        score = roc_auc_score(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        print(\"\\nXGBoost | Seed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n        \n        del model, y_pred\n        del train_x, train_y\n        del val_x, val_y\n        gc.collect()\n    \n    print(\"\\nXGBoost | Seed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_xgb1 = y_pred_meta_xgb1 \/ float(len(SEEDS))\ny_pred_final_xgb1 = y_pred_final_xgb1 \/ float(counter)\noof_score \/= float(counter)\nprint(\"XGBoost | Aggregate OOF Score: {}\".format(oof_score))","5e8a3fb4":"counter = 0\noof_score = 0\ny_pred_final_xgb2 = np.zeros((Xtest.shape[0], 1))\ny_pred_meta_xgb2 = np.zeros((Xtrain.shape[0], 1))\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain.values, Ytrain.values)):\n        counter += 1\n\n        train_x, train_y = Xtrain.iloc[train], Ytrain.iloc[train]\n        val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\n\n        model = XGBClassifier(**xgb_params2)\n\n        model.fit(train_x, train_y, eval_set=[(train_x, train_y), (val_x, val_y)], \n                  early_stopping_rounds=200, verbose=1000)\n        \n        y_pred = model.predict_proba(val_x, iteration_range=(0, model.best_iteration))[:,-1]\n        y_pred_meta_xgb2[val] += np.array([y_pred]).T\n        y_pred_final_xgb2 += np.array([model.predict_proba(Xtest, iteration_range=(0, model.best_iteration))[:,-1]]).T\n        \n        score = roc_auc_score(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        print(\"\\nXGBoost | Seed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n        \n        del model, y_pred\n        del train_x, train_y\n        del val_x, val_y\n        gc.collect()\n    \n    print(\"\\nXGBoost | Seed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_xgb2 = y_pred_meta_xgb2 \/ float(len(SEEDS))\ny_pred_final_xgb2 = y_pred_final_xgb2 \/ float(counter)\noof_score \/= float(counter)\nprint(\"XGBoost | Aggregate OOF Score: {}\".format(oof_score))","944ef4c5":"np.savez_compressed('.\/XGB_Meta_Features.npz',\n                    y_pred_meta_xgb1=y_pred_meta_xgb1, \n                    y_pred_meta_xgb2=y_pred_meta_xgb2, \n                    y_pred_final_xgb1=y_pred_final_xgb1,\n                    y_pred_final_xgb2=y_pred_final_xgb2)","30603de5":"counter = 0\noof_score = 0\ny_pred_final_lgb1 = np.zeros((Xtest.shape[0], 1))\ny_pred_meta_lgb1 = np.zeros((Xtrain.shape[0], 1))\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain, Ytrain)):\n        counter += 1\n\n        train_x, train_y = Xtrain.iloc[train], Ytrain.iloc[train]\n        val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\n\n        model = LGBMClassifier(**lgb_params1)\n        \n        model.fit(train_x, train_y, eval_metric='auc',\n                  eval_set=[(train_x, train_y), (val_x, val_y)],\n                  early_stopping_rounds=200, verbose=500)\n\n        y_pred = model.predict_proba(val_x, num_iteration=model.best_iteration_)[:,-1]\n        y_pred_meta_lgb1[val] += np.array([y_pred]).T\n        y_pred_final_lgb1 += np.array([model.predict_proba(Xtest, num_iteration=model.best_iteration_)[:,-1]]).T\n        \n        score = roc_auc_score(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        print(\"\\nLightGBM | Seed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n        \n        del model, y_pred\n        del train_x, train_y\n        del val_x, val_y\n        gc.collect()\n    \n    print(\"\\nLightGBM | Seed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_lgb1 = y_pred_meta_lgb1 \/ float(len(SEEDS))\ny_pred_final_lgb1 = y_pred_final_lgb1 \/ float(counter)\noof_score \/= float(counter)\nprint(\"LightGBM | Aggregate OOF Score: {}\".format(oof_score))","c2a9be17":"counter = 0\noof_score = 0\ny_pred_final_lgb2 = np.zeros((Xtest.shape[0], 1))\ny_pred_meta_lgb2 = np.zeros((Xtrain.shape[0], 1))\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain, Ytrain)):\n        counter += 1\n\n        train_x, train_y = Xtrain.iloc[train], Ytrain.iloc[train]\n        val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\n\n        model = LGBMClassifier(**lgb_params2)\n        \n        model.fit(train_x, train_y, eval_metric='auc',\n                  eval_set=[(train_x, train_y), (val_x, val_y)],\n                  early_stopping_rounds=200, verbose=500)\n\n        y_pred = model.predict_proba(val_x, num_iteration=model.best_iteration_)[:,-1]\n        y_pred_meta_lgb2[val] += np.array([y_pred]).T\n        y_pred_final_lgb2 += np.array([model.predict_proba(Xtest, num_iteration=model.best_iteration_)[:,-1]]).T\n        \n        score = roc_auc_score(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        print(\"\\nLightGBM | Seed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n        \n        del model, y_pred\n        del train_x, train_y\n        del val_x, val_y\n        gc.collect()\n    \n    print(\"\\nLightGBM | Seed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_lgb2 = y_pred_meta_lgb2 \/ float(len(SEEDS))\ny_pred_final_lgb2 = y_pred_final_lgb2 \/ float(counter)\noof_score \/= float(counter)\nprint(\"LightGBM | Aggregate OOF Score: {}\".format(oof_score))","9d1b1223":"np.savez_compressed('.\/LGB_Meta_Features.npz',\n                    y_pred_meta_lgb1=y_pred_meta_lgb1, \n                    y_pred_meta_lgb2=y_pred_meta_lgb2, \n                    y_pred_final_lgb1=y_pred_final_lgb1,\n                    y_pred_final_lgb2=y_pred_final_lgb2)","1e561879":"counter = 0\noof_score = 0\ny_pred_final_cb1 = np.zeros((Xtest.shape[0], 1))\ny_pred_meta_cb1 = np.zeros((Xtrain.shape[0], 1))\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain.values, Ytrain.values)):\n        counter += 1\n\n        train_x, train_y = Xtrain.iloc[train], Ytrain.iloc[train]\n        val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\n\n        model = CatBoostClassifier(**cb_params1)\n\n        model.fit(train_x, train_y, eval_set=[(val_x, val_y)], \n                  early_stopping_rounds=200, verbose=1000)\n\n        y_pred = model.predict_proba(val_x)[:,-1]\n        y_pred_meta_cb1[val] += np.array([y_pred]).T\n        y_pred_final_cb1 += np.array([model.predict_proba(Xtest)[:,-1]]).T\n        \n        score = roc_auc_score(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        print(\"\\nCatBoost | Seed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n        \n        del model, y_pred\n        del train_x, train_y\n        del val_x, val_y\n        gc.collect()\n    \n    print(\"\\nCatBoost | Seed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_cb1 = y_pred_meta_cb1 \/ float(len(SEEDS))\ny_pred_final_cb1 = y_pred_final_cb1 \/ float(counter)\noof_score \/= float(counter)\nprint(\"CatBoost | Aggregate OOF Score: {}\".format(oof_score))","5421c340":"counter = 0\noof_score = 0\ny_pred_final_cb2 = np.zeros((Xtest.shape[0], 1))\ny_pred_meta_cb2 = np.zeros((Xtrain.shape[0], 1))\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain.values, Ytrain.values)):\n        counter += 1\n\n        train_x, train_y = Xtrain.iloc[train], Ytrain.iloc[train]\n        val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\n\n        model = CatBoostClassifier(**cb_params1)\n\n        model.fit(train_x, train_y, eval_set=[(val_x, val_y)], \n                  early_stopping_rounds=200, verbose=500)\n\n        y_pred = model.predict_proba(val_x)[:,-1]\n        y_pred_meta_cb2[val] += np.array([y_pred]).T\n        y_pred_final_cb2 += np.array([model.predict_proba(Xtest)[:,-1]]).T\n        \n        score = roc_auc_score(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        print(\"\\nCatBoost | Seed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n        \n        del model, y_pred\n        del train_x, train_y\n        del val_x, val_y\n        gc.collect()\n    \n    print(\"\\nCatBoost | Seed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_cb2 = y_pred_meta_cb2 \/ float(len(SEEDS))\ny_pred_final_cb2 = y_pred_final_cb2 \/ float(counter)\noof_score \/= float(counter)\nprint(\"CatBoost | Aggregate OOF Score: {}\".format(oof_score))","49a5d319":"np.savez_compressed('.\/CB_Meta_Features.npz',\n                    y_pred_meta_cb1=y_pred_meta_cb1, \n                    y_pred_meta_cb2=y_pred_meta_cb2, \n                    y_pred_final_cb1=y_pred_final_cb1,\n                    y_pred_final_cb2=y_pred_final_cb2)","29ac34ff":"Xtrain_meta = np.concatenate((y_pred_meta_cb1, y_pred_meta_lgb1, y_pred_meta_xgb1,\n                              y_pred_meta_cb2, y_pred_meta_lgb2, y_pred_meta_xgb2), axis=1)\nXtest_meta = np.concatenate((y_pred_final_cb1, y_pred_final_lgb1, y_pred_final_xgb1,\n                             y_pred_final_cb2, y_pred_final_lgb2, y_pred_final_xgb2), axis=1)\n\nprint(\"Xtrain_meta shape: {}\".format(Xtrain_meta.shape))\nprint(\"Xtest_meta shape: {}\".format(Xtest_meta.shape))\n\ndel Xtrain, Xtest\ndel y_pred_meta_cb1, y_pred_meta_lgb1, y_pred_meta_xgb1\ndel y_pred_meta_cb2, y_pred_meta_lgb2, y_pred_meta_xgb2\ngc.collect()","0b859d90":"counter = 0\noof_score = 0\ny_pred_final_lr = np.zeros((Xtest_meta.shape[0], 1))\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain_meta, Ytrain)):\n        counter += 1\n\n        train_x, train_y = Xtrain_meta[train], Ytrain.iloc[train]\n        val_x, val_y = Xtrain_meta[val], Ytrain.iloc[val]\n        \n        model = LogisticRegression(\n            solver='saga',\n            max_iter=1000,\n            random_state=42\n        )\n\n        model.fit(train_x, train_y)\n\n        y_pred = model.predict_proba(val_x)[:,-1]\n        y_pred_final_lr += np.array([model.predict_proba(Xtest_meta)[:,-1]]).T\n        \n        score = roc_auc_score(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        print(\"LR | Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n    \n    print(\"\\nLR | Seed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_final_lr = y_pred_final_lr \/ float(counter)\noof_score \/= float(counter)\nprint(\"LR | Aggregate OOF Score: {}\".format(oof_score))","95aa169f":"submit_df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\nsubmit_df['claim'] = y_pred_final_lr.ravel()\nsubmit_df.to_csv(\"Stacked_Submission.csv\", index=False)\nsubmit_df.head(10)","abed3006":"y_pred_final_lgb = (y_pred_final_lgb1 + y_pred_final_lgb2)\/2\ny_pred_final_xgb = (y_pred_final_xgb1 + y_pred_final_xgb2)\/2\ny_pred_final_cb = (y_pred_final_cb1 + y_pred_final_cb2)\/2\n\nsubmit_df['claim'] = (y_pred_final_lr**4 + y_pred_final_xgb**4 + y_pred_final_lgb**4)\/3\nsubmit_df.to_csv(\"Power_Average_Submission.csv\", index=False)\nsubmit_df.head(10)","723cf154":"submit_df['claim'] = (y_pred_final_lr * 0.55) + (y_pred_final_xgb * 0.25) + (y_pred_final_lgb * 0.15) + (y_pred_final_cb * 0.05)\nsubmit_df.to_csv(\"Weighted_Average_Submission.csv\", index=False)\nsubmit_df.head(10)","8d5e8a22":"## XGBoost Model","45b83383":"## Model Hyperparameters","bb165fd0":"## Load source datasets","af3af79c":"## LightGBM Model","528158fd":"## Create submission files","bdebdca2":"## Import libraries","a438b6ea":"## Logistic Regression (Meta)","3c7d9729":"## Feature Engineering","396a6a04":"## CatBoost Model"}}