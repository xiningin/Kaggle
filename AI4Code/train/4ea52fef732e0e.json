{"cell_type":{"ac7b2558":"code","b45ead3b":"code","5a98ce99":"code","3df15aa9":"code","040415a2":"code","fa968653":"code","69a2c70a":"code","c1316d31":"code","139cab1b":"code","d0a858f6":"code","1255beb7":"code","55cc7c92":"code","684821b5":"code","ff1e4b94":"code","3b0a0220":"code","2cc40d81":"code","b46eaed1":"code","1c907ab2":"code","19b3a298":"code","b5c6cd61":"code","efeb5945":"code","6b2cc770":"code","c17f338f":"code","a4ad2e9b":"code","cf890cef":"code","841b01bc":"code","e7d0b0c6":"code","7827339b":"markdown","a5b2c6b2":"markdown","f7bc7d79":"markdown","85309d21":"markdown","73ccc469":"markdown","5a98a96f":"markdown","9d06f488":"markdown","bad56351":"markdown","e2f85d50":"markdown","a2d1a618":"markdown","891835d8":"markdown","62ce336c":"markdown","8dfb7b69":"markdown","14509c01":"markdown","6cc17d86":"markdown","a0db192b":"markdown","37a9d671":"markdown","dec3b85f":"markdown","393ad627":"markdown"},"source":{"ac7b2558":"import sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b45ead3b":"#loading dataset from sqlite database\ncon = sqlite3.connect('\/kaggle\/input\/amazon-fine-food-reviews\/database.sqlite') \n\nfiltered_data = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE score != 3\"\"\", con )\n#giving a postive rating to data points having score more than 3 and a negative rating for score less than 3.\n#We are removing data points where score is equal to 3 as it cannot be predicted as positive or negative.\ndef partition(x) :\n    if x < 3 :\n        return 'Positive'\n    else :\n        return 'Negative'","5a98ce99":"#replacing score column values with positve and negative values\nactualscore = filtered_data['Score'] \npositiveNegative = actualscore.map(partition)\nfiltered_data['Score'] = positiveNegative\nprint(\"No of data points\", filtered_data.shape)\nprint(filtered_data.head())","3df15aa9":"#sorting data according to ProductId\nsorted_data= filtered_data.sort_values('ProductId', axis =0 , ascending = True, inplace=False, kind ='quicksort', na_position='last')\nprint(sorted_data.head())","040415a2":"#dropping duplicate values\nfinal = sorted_data.drop_duplicates(subset = {'UserId', 'ProfileName', 'Time','Text' }, keep = 'first', inplace=False)\nprint(final.head())\nprint(final.shape)","fa968653":"#removing values with HelpfulnessNumerator>HelpfulnessDenominator\nfinal = final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\nprint(final.shape)\nprint(final['Score'].value_counts())","69a2c70a":"import re # regular expressions\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop = set(stopwords.words('english'))# set of English Stopwords\nsno = nltk.stem.SnowballStemmer('english')# initialising the snowball stemmer\n\ndef cleanhtml(sentence): #function to clear HTML tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\n\ndef cleanpunc(sentence): #function to remove punctuations\n    cleaned = re.sub(r'[?|!|\\'|\"|#]', ' ', sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]', ' ', cleaned)\n    return cleaned\nprint(stop)\n    ","c1316d31":"#code to implement all the preprocessing\nfrom tqdm import tqdm\ni =0 \nstrl = ' '\nfinal_string= []\nall_postive_words = []#stores all positive words\nall_negative_words = []#stores all negative words\ns = ''\nfor sent in tqdm(final['Text'].values) :\n    filtered_sentence = []\n    sent = cleanhtml(sent)\n    for w in sent.split() :\n        for cleaned_words in cleanpunc(w).split() :\n            if ((cleaned_words.isalpha()) & ((len(cleaned_words))>2)) :\n                if (cleaned_words.lower() not in stop) :\n                    s = (sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (final['Score'].values)[i] == 'postive':\n                        all_positive_words.append(s)\n                    if (final['Score'].values)[i] == 'negative':\n                        all_negative_words.append(s)\n                else :\n                    continue\n            else : \n                continue\n    strl = b\" \".join(filtered_sentence) #final string of all cleared words\n\n    final_string.append(strl)\n    i+=1\n    \n","139cab1b":"print(final_string[0:5])","d0a858f6":"final['CleanedText'] = final_string\n\nprint(final.head(3))\n\n#store final table into a SQLlite table for future use\nconn =sqlite3.connect('final.sqlite')\nc = conn.cursor()\nconn.text_factory = str\nfinal.to_sql('Reviews',conn, schema = None, if_exists='replace')","1255beb7":"from sklearn.feature_extraction.text import CountVectorizer #BoW\ncount_vect = CountVectorizer()\nfinal_counts = count_vect.fit_transform(final['CleanedText'].values)\n\nprint(final_counts.get_shape())\n","55cc7c92":"print(type(final_counts))\nprint(final_counts[0])","684821b5":"count_vect = CountVectorizer(ngram_range=(1,2), max_features = 5000) # all unigram and bigram values\nfinal_bigram_counts = count_vect.fit_transform(final['CleanedText'].values)\nprint(\"the type of count vectorizer \",type(final_bigram_counts))\nprint(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])","ff1e4b94":"print(count_vect.get_feature_names()[0:10])","3b0a0220":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntf_idf_vect = TfidfVectorizer( max_features = 5000 )\ntf_idf  = tf_idf_vect.fit_transform(final['CleanedText'].values)\n\nprint(tf_idf.get_shape())","2cc40d81":"from gensim.models import Word2Vec \n# making a list of all cleaned sentances.(removing HTML tags and punctuation)\nlist_of_sentance = []\nfor sentance in tqdm(final['Text'].values) :\n    filtered_sentence = []\n    sentance = cleanhtml(sentance)\n    for w in sentance.split() :\n        for cleaned_words in cleanpunc(w).split() :\n            if cleaned_words.isalpha() :\n                filtered_sentence.append(cleaned_words.lower())\n            else:\n                continue\n    list_of_sentance.append(filtered_sentence)\n            \nprint(list_of_sentance[0:10])\n","b46eaed1":"w2v_model = Word2Vec(list_of_sentance, min_count = 5, size = 50 , workers = 4 ) #training Word2Vec on list_of_sentance","1c907ab2":"print(w2v_model.wv['tasty'])","19b3a298":"print(w2v_model.wv.most_similar('tasty'))","b5c6cd61":"sentance_vector = []\nw2v_words = list(w2v_model.wv.vocab)\nfor sentance in tqdm(list_of_sentance) :\n    sent_vector = np.zeros(50)\n    cnt_words = 0\n    for word in sentance :\n        if word in w2v_words :\n            vec = w2v_model.wv[word]\n            sent_vector+=vec\n            cnt_words +=1\n        else :\n            continue\n    if cnt_words != 0:\n        sent_vector = sent_vector\/cnt_words\n    sentance_vector.append(sent_vector)\n\nprint(len(sentance_vector))\nprint(len(sentance_vector[0]))\n            \n            \n    ","efeb5945":"tfidf_feat = tf_idf_vect.get_feature_names()\nrow = 0\ntfidf_sent_vector = []\nfor sentance in tqdm(list_of_sentance):\n    tfidf_sent_vec = np.zeros(50)\n    weight_sum =0\n    for word in sentance :\n        if word in w2v_words and word in tfidf_feat :\n            vec = w2v_model.wv[word]\n            \n            tfidf = tf_idf[row, tfidf_feat.index(word)]\n            tfidf_sent_vec += (vec*tfidf)\n            weight_sum +=tfidf\n    if weight_sum != 0 :\n        tfidf_sent_vec \/=weight_sum\n    tfidf_sent_vector.append(tfidf_sent_vec)\n    row+=1\n\nprint(len(tfidf_sent_vector))","6b2cc770":"from sklearn.manifold import TSNE\nfrom scipy.sparse import csr_matrix\nfrom sklearn.preprocessing import StandardScaler\n\nbow_final_4k = final_counts[0:4000]\nscore_4k = (final[\"Score\"])[0:4000]\n\n#standardising the BoW values to lie between 0 and 1\nbow_final_4k = StandardScaler(with_mean=False).fit_transform(bow_final_4k)\n#converting sparse matrix to dense matrix as t-SNE accepts only dense matrix\nbow_final_4k = bow_final_4k.todense()  \n\ntsne = TSNE(n_components = 2, random_state = 0 ) \n#default perplexity = 30\n#default number of iterations = 1000\n\nbow_tsne = tsne.fit_transform(bow_final_4k)\n\nfor_bow = np.vstack((bow_tsne.T, score_4k)).T\nfor_bow_df = pd.DataFrame(for_bow , columns = ('dimension_1', 'dimension_2', 'Score'))\nsns.set_style('whitegrid')\nsns.FacetGrid(for_bow_df, hue = 'Score',height = 7).map(plt.scatter, 'dimension_1', 'dimension_2').add_legend()\nplt.show()","c17f338f":"from sklearn.manifold import TSNE\nfrom scipy.sparse import csr_matrix\n\ntf_idf_4k = tf_idf[0:4000]\nscore_4k = final[\"Score\"][0:4000]\ntf_idf_4k = StandardScaler(with_mean = False).fit_transform(tf_idf_4k)\ntf_idf_4k = tf_idf_4k.todense()\n\ntsne = TSNE(n_components = 2, random_state = 0, perplexity = 50, n_iter=2000 ) \n#default perplexity = 30\n#default number of iterations = 1000\n\ntfidf_tsne = tsne.fit_transform(tf_idf_4k)\n\nfor_tfidf = np.vstack((tfidf_tsne.T, score_4k )).T\nfor_tfidf_df = pd.DataFrame(for_tfidf , columns = ('dimension_1', 'dimension_2', 'Score'))\nsns.set_style('whitegrid')\nsns.FacetGrid(for_tfidf_df, hue = 'Score',height = 7).map(plt.scatter, 'dimension_1', 'dimension_2').add_legend()\nplt.show()","a4ad2e9b":"sentance_vector_4k = sentance_vector[0:4000]\nsentance_vector_4k = StandardScaler(with_mean = False).fit_transform(sentance_vector_4k)\n\ntsne = TSNE(n_components = 2 , random_state = 0, perplexity = 50 , n_iter=2000)\n\navgw2v_tsne = tsne.fit_transform(sentance_vector_4k)\n\nfor_avgw2v = np.vstack((avgw2v_tsne.T, score_4k)).T\navgw2v_df =pd.DataFrame(for_avgw2v, columns=('dimension_1', 'dimension_2', 'score'))\n\nsns.set_style('whitegrid')\nsns.FacetGrid(avgw2v_df, hue = 'score', height = 7).map(plt.scatter, 'dimension_1', 'dimension_2').add_legend()\nplt.title('average Word2Vec t-SNE')\nplt.show()\n","cf890cef":"tfidf_sent_vector_4k = tfidf_sent_vector[0:4000]\ntfidf_sent_vector_4k = StandardScaler(with_mean = False).fit_transform(tfidf_sent_vector_4k)\n\ntsne = TSNE(n_components = 2 , random_state = 0, perplexity = 50 , n_iter=2000)\ntfidf_w2v_tsne = tsne.fit_transform(tfidf_sent_vector_4k)\n\nfor_tfidf_w2v = np.vstack((tfidf_w2v_tsne.T, score_4k)).T\ntfidf_w2v_df = pd.DataFrame(for_tfidf_w2v, columns=('dimension_1', 'dimension_2', 'score'))\n\nsns.set_style('whitegrid')\nsns.FacetGrid(tfidf_w2v_df, hue = 'score', height = 7).map(plt.scatter, 'dimension_1', 'dimension_2').add_legend()\nplt.title('tf-idf weighted Word2Vec')\nplt.show()","841b01bc":"#storing values for future use\nfinal_avgw2v = pd.DataFrame(sentance_vector)\nprint(final_avgw2v.shape)\n\nfinal_avgw2v.to_csv(r'D:\\New folder\\Practicals\\amazon fine food\\avgw2v.csv')\n\n","e7d0b0c6":"final_tfidf_w2v = pd.DataFrame(tfidf_sent_vector)\nprint(final_tfidf_w2v.shape)\n\nfinal_tfidf_w2v.to_csv(r'D:\\New folder\\Practicals\\amazon fine food\\tfidf_w2v.csv')","7827339b":"### Conclusion\n1. None of the t-SNE plotted above gives a clear separation between positive and negative values.\n2. We would have to use alternate methods to separate the values. ","a5b2c6b2":"## [4.3] TF-IDF","f7bc7d79":"# Amazon fine food review","85309d21":"Downloaded from https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews\nThe amazon fine food review dataset consists of review of fine foods on amazon .\n- Number of reviews: 568,454\n- Number of users: 256,059\n- Number of products: 74,258\n- Timespan: Oct 1999 - Oct 2012\n- Number of Attributes\/Columns in data: 10\n\nAttribute Information:\n\n- Id\n- ProductId - unique identifier for the product\n- UserId - unqiue identifier for the user\n- ProfileName\n- HelpfulnessNumerator - number of users who found the review helpful\n- HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n- Score - rating between 1 and 5\n- Time - timestamp for the review\n- Summary - brief summary of the review\n- Text - text of the review\n### Objective\nTo determine whether the review given is positive(score rating of 4 or 5 stars) or negative(score rating of 1 or 2 stars).","73ccc469":"## [4.4] TF-IDF weighted Word2Vec","5a98a96f":"# [1]. Reading the data\n","9d06f488":"## [5] Data Visualization (Using t-SNE)","bad56351":"**Observation:** It was also observed in our data that some values have more value of HelpfulnessNumerator than HelpfulnessDenominator which is not possible so we will also clean those values","e2f85d50":"## [5.3] Applying t-SNE on  Text average Word2Vec ","a2d1a618":"## [5.2] Applying t-SNE on Text TF-IDF vectors","891835d8":"## Exploratory Data Analysis\n## [2] Data Cleaning(Deduplication)\nIt is observed that the data has many duplicate entries. Hence it is necessary to remove dupliactes in order to get unbiased results.","62ce336c":"## [3] Text Preprocessing\nNow that we have removed the deduplicated from our dataset we need to perform some preprocessing operations before we go for further analysis and make the prediction model.\n\nHence in the preprocessing phase we do the following preprocessing in the order below:\n1. Begin by removing HTML tags.\n2. Remove any punctuation or limited set of special characters like . , # etc.\n3. Check if the word is made up of English characters and is alpha-numeric.\n4. Remove the words having size less than 3 (as in English vocabulary no adjective has size less than 3).\n5. Convert words to lowercase.\n6. Remove all Stopwords.\n7. Finally Snowball Stemming the words .(As it was observed that it was better than Porter Stemming)\n","8dfb7b69":"## [5.1] Applying t-SNE on Text BoW vectors","14509c01":"## [4.4] Word2Vec\nHere we are training our own Word2Vec model on the dataset.\n","6cc17d86":"## [5.5] Applying t-SNE on Text TF-IDF weighted Word2Vec","a0db192b":"## [4.4] Average Word2Vec","37a9d671":"## Loading the data\nThe dataset is available in two formats i.e.\n1. sqlite database\n2. .csv file\nWe will load data using SQLITE database as it is easier to give commands and visualise the data.\n\nHere we only want to get the sentiment of th review(positive or negative). We will purposely remove the score of 3 and give a positive rating if the score is more than 3 and a negative rating if it is less than 3.","dec3b85f":"## [4] Featurization\n## [4.1] Bag of Words\n","393ad627":"## [4.2] Bi-Grams and N-Grams\n"}}