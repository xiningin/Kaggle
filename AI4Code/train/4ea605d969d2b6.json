{"cell_type":{"880a5fee":"code","942b7e11":"code","52961a0b":"code","04f52492":"code","45020883":"code","6cf21e60":"code","2da2f609":"code","12e34e77":"code","94fe4d34":"code","4f7198e9":"code","9e3c6190":"code","52ae9175":"code","c6d794b8":"code","962734cd":"code","cd38ee1f":"code","bc349ded":"code","de165ceb":"code","26f6dbaa":"code","e77eeda2":"code","0ed2623f":"code","0f5a8769":"code","3eb085d6":"code","ad16748b":"code","e1a88d47":"code","f110f145":"code","3bff34ee":"code","46fa84ff":"code","4d43863c":"code","89890abe":"code","97ac7d08":"code","6c66298f":"code","6e24b1d3":"code","fb8a1ee9":"markdown","c264a0ab":"markdown","ebb2b8f5":"markdown","75792eac":"markdown","e127768c":"markdown","e322ad16":"markdown","7825fe53":"markdown","56471dd0":"markdown","d25a0904":"markdown"},"source":{"880a5fee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","942b7e11":"train = pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-analysis\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-analysis\/test.csv\")","52961a0b":"train.head()","04f52492":"tweets = train['tweet']\nlabels = train['label']","45020883":"import string\nimport nltk","6cf21e60":"from string import punctuation\nfrom nltk.corpus import stopwords","2da2f609":"appos = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n\"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\", \"haven't\" : \"have not\", \"he'd\" : \"he would\",\n\"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\", \"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\"it's\" : \"it is\", \"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\", \"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\",\n\"shouldn't\" : \"should not\", \"that's\" : \"that is\", \"there's\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n\"they're\" : \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n\"we've\" : \"we have\", \"what'll\" : \"what will\",\"what're\" : \"what are\",\"what's\" : \"what is\", \"what've\" : \"what have\",\n\"where's\" : \"where is\", \"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\",\n\"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\", \"you'll\" : \"you will\",\n\"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\"}","12e34e77":"def preprocess(text):\n    all_tweets = list()\n    for txt in text:\n        lower_case = txt.lower()\n        words = lower_case.split()\n        formatted = [appos[word] if word in appos else word for word in words]\n        formatted_test = list()\n        for word in formatted:\n            if word not in stopwords.words(\"english\"):\n                formatted_test.append(word)\n        formatted = \" \".join(formatted_test)\n        punct_text = \"\".join([ch for ch in formatted if ch not in punctuation])\n        all_tweets.append(punct_text)\n    for i in range(len(all_tweets)):\n        if all_tweets[i].startswith(\"user\"):\n            all_tweets[i] = all_tweets[i].replace(\"user\", '')\n    all_text = \" \".join(all_tweets)\n    all_words = all_text.split()\n    \n    return all_tweets, all_words","94fe4d34":"all_tweets, all_words = preprocess(tweets)","4f7198e9":"all_tweets[3]","9e3c6190":"import re\n\nfor i in range(len(all_tweets)):\n    all_tweets[i] = re.sub('[^a-zA-Z0-9]', ' ', all_tweets[i])\n\nall_words = []\nfor sentence in all_tweets:\n    for word in sentence.split():\n        all_words.append(word)","52ae9175":"from collections import Counter\n\nword_counts = Counter(all_words)\nword_list = sorted(word_counts, reverse = True)\nword2int = {word : i+1 for (i, word) in enumerate(word_list)}\nint2word = {i : word for word, i in word2int.items()}\n\nencoded_tweets = [[word2int[word] for word in tweet.split()] for tweet in all_tweets]","c6d794b8":"encoded_labels = np.array([label for idx, label in enumerate(labels) if len(encoded_tweets[idx]) > 0])\nencoded_tweets = [tweet for tweet in encoded_tweets if len(tweet) > 0]","962734cd":"def pad_tweet(encoded_tweets, tweet_length):\n    Tweets = []\n    \n    for tweet in encoded_tweets:\n        if len(tweet) >= tweet_length:\n            Tweets.append(tweet[:tweet_length])\n        else:\n            Tweets.append([0] * (tweet_length - len(tweet)) + tweet)\n    return np.array(Tweets)","cd38ee1f":"padded_reviews = pad_tweet(encoded_tweets, 15)","bc349ded":"import torch\nimport torch.nn as nn","de165ceb":"train_ratio = 0.9\nvalid_ratio = 0.1\ntotal = padded_reviews.shape[0]\ntrain_cutoff = int(total * train_ratio)\n\nx_train, y_train = padded_reviews[:train_cutoff], encoded_labels[:train_cutoff]\nx_valid, y_valid = padded_reviews[train_cutoff:], encoded_labels[train_cutoff:]\n\n\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\nvalid_data = TensorDataset(torch.from_numpy(x_valid), torch.from_numpy(y_valid))\n\nbatch_size = 32\n\ntrain_loader = DataLoader(train_data, batch_size, shuffle = True, drop_last = True)\nvalid_loader = DataLoader(valid_data, batch_size, shuffle = True, drop_last = True)","26f6dbaa":"class sentimentLSTM(nn.Module):\n    def __init__(self, input_size, embedding_dim, hidden_size, output_size, n_layers, drop_prob = 0.3):\n        super().__init__()\n        self.input_size = input_size\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        \n        self.embed = nn.Embedding(input_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, n_layers, batch_first = True, dropout = drop_prob)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.sig = nn.Sigmoid()\n        \n    def forward(self, input_words, h):\n        #Input dimension = batch_size x tweet_length\n        batch_size = input_words.shape[0]\n        embedd = self.embed(input_words) #dimension = batch_size x tweet_length x embedding_dim\n        \n        lstm_out, h = self.lstm(embedd, h) #dimension = batch_size x tweet_length x hidden_size\n        lstm_out = self.dropout(lstm_out)\n        #stacking up the lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_size) #dimension = (batch_size * tweet_length) x hidden_size\n        \n        fc_out = self.fc(lstm_out) #dimension = (batch_size * tweet_length) x output_size\n        \n        sig_out = self.sig(fc_out) #dimension = (batch_size * tweet_length) x output_size\n        sig_out = sig_out.view(batch_size, -1) #dimension = batch_size x (tweet_length * output_size)\n        sig_out = sig_out[:, -1] #Extract only the last output of the element of each example in the batch\n        \n        return sig_out, h\n    \n    def init_hidden(self, batch_size):\n        weights = next(self.parameters()).data\n        \n        h = (weights.new(self.n_layers, batch_size, self.hidden_size).zero_(),\n             weights.new(self.n_layers, batch_size, self.hidden_size).zero_())\n        \n        return h","e77eeda2":"input_size = len(word2int)+1\nembedding_dim = 400\nhidden_size = 256\noutput_size = 1\nn_layers = 2","0ed2623f":"model = sentimentLSTM(input_size, embedding_dim, hidden_size, output_size, n_layers)","0f5a8769":"model","3eb085d6":"optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\ncriterion = nn.BCELoss()","ad16748b":"print_every = 100\nstep = 0\nn_epochs = 5\nclip = 5\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntrain_losses = []\n\nif torch.cuda.is_available():\n    model.cuda()\n\nmodel.train()\nfor epoch in range(1, n_epochs+1):\n    h = model.init_hidden(batch_size)\n    \n    for inputs, labels in train_loader:\n        step += 1\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        h = tuple([each.data for each in h]) \n        \n        model.zero_grad()\n        output, h = model(inputs, h)\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        if step % print_every == 0:\n            #Validation\n            valid_losses = []\n            valid_h = model.init_hidden(batch_size)\n            model.eval()\n            \n            for valid_inputs, valid_labels in valid_loader:\n                valid_inputs, valid_labels = valid_inputs.to(device), valid_labels.to(device)\n                \n                valid_h = tuple([each.data for each in valid_h])\n                \n                valid_output, valid_h = model(valid_inputs, valid_h)\n                valid_loss = criterion(valid_output.squeeze(), valid_labels.float())\n                valid_losses.append(valid_loss.item())\n                train_losses.append(loss.item())\n                \n            print(\"Epoch: {}\/{}\".format((epoch), n_epochs),\n                  \"Step: {}\".format(step),\n                  \"Training Loss: {:.4f}\".format(loss.item()),\n                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n            model.train()","e1a88d47":"test_tweets = test['tweet']\ntest_tweets, test_words = preprocess(test_tweets)","f110f145":"for i in range(len(test_tweets)):\n    test_tweets[i] = re.sub('[^a-zA-Z0-9]', ' ', test_tweets[i])\n\ntest_words = []\nfor sentence in test_tweets:\n    for word in sentence.split():\n        test_words.append(word)","3bff34ee":"encoded_test_tweets = []\nfor tweet in test_tweets:\n    encoded_tweet = []\n    for word in tweet.split():\n        if word not in word2int.keys():\n            encoded_tweet.append(0)\n        else:\n            encoded_tweet.append(word2int[word])\n    encoded_test_tweets.append(encoded_tweet)","46fa84ff":"padded_test_tweets = pad_tweet(encoded_test_tweets, 15)","4d43863c":"def test_model(test_input):\n    output_list = list()\n    model.eval()\n    with torch.no_grad():\n        for tweet in test_input:\n            feature_tensor = torch.from_numpy(tweet).view(1, -1)\n            if(torch.cuda.is_available()):\n                feature_tensor = feature_tensor.cuda()\n            batch_size = feature_tensor.size(0)\n            #initialize hidden state\n            h = model.init_hidden(batch_size)\n            #get the output from the model\n            output, h = model(feature_tensor, h)\n            pred = torch.round(output.squeeze())\n            output_list.append(pred)\n        test_labels = [int(i.data.cpu().numpy()) for i in output_list]\n        return test_labels","89890abe":"test_labels = test_model(padded_test_tweets)","97ac7d08":"output = pd.DataFrame()\noutput['id'] = test['id']\noutput['label'] = test_labels","6c66298f":"output","6e24b1d3":"output.to_csv(\"subm.csv\", index = False)","fb8a1ee9":"Text Preprocessing","c264a0ab":"Creating Dictionaries and encoding the words","ebb2b8f5":"Defining the LSTM Model","75792eac":"Split Data into train and validation sets and get (tweet, label) dataloaders","e127768c":"Loading the Data","e322ad16":"Getting rid of short reviews","7825fe53":"Encoding labels is not required as they themselves are binary (1 or 0)","56471dd0":"Initialize the hyperparameters of the model","d25a0904":"Padding the sequences (making the length of all tweets the same)"}}