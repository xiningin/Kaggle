{"cell_type":{"f3255b67":"code","321d5e5b":"code","6f987578":"code","5aa04d9d":"code","9e3bed40":"code","e905ceda":"code","1db5cf18":"code","b336407c":"code","83bbc685":"code","bb9de093":"code","f2736e04":"code","b293a561":"code","a91f0f76":"code","bc60ff0e":"code","ebfb1725":"code","6956b7f4":"code","ad8601ca":"code","e91a4c79":"code","8c1fca27":"code","0482c151":"code","ecec570a":"code","cf85a83b":"code","db103ca2":"code","9a82e005":"code","bd433d69":"code","3ccb5981":"code","652593c4":"code","e274a678":"code","ff178910":"code","de947ea3":"code","6521dd1f":"code","8869606a":"code","9ab12f02":"code","5aeb0e29":"code","57dbd407":"code","2d36bc49":"code","33ba0edf":"code","050838e1":"code","f62a83ec":"code","cd1c5df3":"code","c93f59b7":"code","7f6b15b9":"code","4dbd3fec":"code","6c9a8334":"code","0280a6ef":"code","52077874":"code","b9f42c42":"code","daf29975":"code","8287b525":"code","a1a24686":"code","cd341b9a":"code","b6bff063":"code","b0a1a30a":"code","ec83b508":"code","4507ab52":"code","abbddd14":"code","2a913b04":"code","87f050a1":"code","8b55fc55":"code","8f1b4f68":"code","535a3a42":"code","0a406d26":"code","94f14866":"code","94a35c7f":"code","2cac7554":"code","e87ab4bc":"code","88df69dd":"code","9af7ae1c":"code","192fdbce":"code","bc03fd22":"code","20983e67":"code","a4b441a5":"code","6fae2637":"code","39bd8404":"code","fd40aa45":"code","8d409856":"code","dd488247":"code","9d2f4edc":"code","1d0bbb75":"code","1508002a":"code","323642a5":"code","2e3b3d11":"code","08a8e2dc":"code","c83c0bfe":"code","c1a03bed":"code","aa02d594":"code","a5307d51":"code","dba8bc07":"code","abe86b6c":"code","75b4a4a0":"markdown","1acca34a":"markdown","382b4ba4":"markdown","4dc3fb91":"markdown","a824495b":"markdown","9a7aa3b5":"markdown","9d69770e":"markdown","66d74120":"markdown","c922b414":"markdown","bf711085":"markdown","a7af2288":"markdown","5f606772":"markdown","48168985":"markdown","ab4f912d":"markdown","a64072c0":"markdown","7f2905fa":"markdown","eb901ae0":"markdown","a4a1342b":"markdown","e4526ace":"markdown","1b0cb008":"markdown","5bdc62f0":"markdown","23df711f":"markdown","c4acea2c":"markdown"},"source":{"f3255b67":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","321d5e5b":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nfrom sklearn.linear_model import (LogisticRegression,SGDClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, VotingClassifier)\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import (GridSearchCV, StratifiedKFold, cross_val_score, learning_curve)\n\n\nfrom collections import Counter\n%matplotlib inline\nplt.style.use('ggplot')\nmpl.rcParams['axes.unicode_minus'] = False","6f987578":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nIDtest = test['PassengerId']","5aa04d9d":"# Outlier detection #### IMPORTANT\n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n","9e3bed40":"train.loc[Outliers_to_drop]","e905ceda":"train = train.drop(Outliers_to_drop, axis=0).reset_index(drop=True)","1db5cf18":"train_len= len(train)\ndataset= pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","b336407c":"#fill empty and NaNs values with NaN\ndataset = dataset.fillna(np.nan)\ndataset.isnull().sum()","83bbc685":"train.info()\ntrain.isnull().sum()","bb9de093":"train.head()","f2736e04":"#correlation matrix between numerical values\ng = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\", \"Age\", \"Fare\"]].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")","b293a561":"#Explore SibSp Feature vs Survived\n\ng = sns.factorplot(x=\"SibSp\", y=\"Survived\", data=train, kind=\"bar\", size=6,\n                  palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","a91f0f76":"# Explore Parch feature vs Survived\ng = sns.factorplot(x=\"Parch\", y=\"Survived\", data=train,\n                  kind=\"bar\", size=6, palette=\"muted\")\ng.despine(left=True)\ng= g.set_ylabels(\"survival probability\")\n","bc60ff0e":"#Explore Age vs Survived\ng = sns.FacetGrid(train, col = \"Survived\")\ng = g.map(sns.distplot, \"Age\")","ebfb1725":"# explore age distribution and split them\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"]==0) &\n               (train[\"Age\"].notnull())], color=\"Red\", shade=True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"]==1) &\n                            (train[\"Age\"].notnull())],\n               ax=g, color=\"Blue\", shade=True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","6956b7f4":"dataset[\"Fare\"].isnull().sum()","ad8601ca":"#Fill Fare missing values with the median value\ndataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())","e91a4c79":"#Explore Fare distribution\ng = sns.distplot(dataset[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","8c1fca27":"# Apply log to Fare to reduce skewness distribution\ndataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i\n                                      :np.log(i) if i>0 else 0)","0482c151":"g = sns.distplot(dataset[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","ecec570a":"g = sns.barplot(x=\"Sex\", y=\"Survived\", data=train)\ng = g.set_ylabel(\"Survival Probability\")","cf85a83b":"train[[\"Sex\",\"Survived\"]].groupby(\"Sex\").mean()","db103ca2":"#Explore PClass Vs Survived\ng = sns.factorplot(x=\"Pclass\", y=\"Survived\", data=train,\n                  kind=\"bar\", size=6, palette=\"muted\")\ng.despine(left=True)\ng= g.set_ylabels(\"survival probability\")","9a82e005":"#Explore Pclass vs Survived by Sex\ng = sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\",\n                  data=train, size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng= g.set_ylabels(\"survival probability\")","bd433d69":"dataset[\"Embarked\"].isnull().sum()","3ccb5981":"#Fill Embarked nan values of dataset with 'S' most frequent value\ndataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")","652593c4":"#Explore Embarked vs Survived\ng= sns.factorplot(x=\"Embarked\", y=\"Survived\", \n                 data=train, size=6, kind=\"bar\",\n                 palette=\"muted\")\ng.despine(left=True)\ng= g.set_ylabels(\"survival probability\")","e274a678":"#Explore Pclass vs Embarked\ng = sns.factorplot(\"Pclass\", col=\"Embarked\", data=train, size=6, kind=\"count\", palette=\"muted\")\ng.despine(left=True)\ng= g.set_ylabels(\"Count\")","ff178910":"# Age NAN=256 counts, leverage correlated features\n# Explore Age vs Sex, Parch, Pclass and SibSp\ng= sns.factorplot(y=\"Age\", x=\"Sex\", data=dataset, kind=\"box\")\ng= sns.factorplot(y=\"Age\", x=\"Sex\", hue=\"Pclass\",\n                 data=dataset, kind=\"box\")\ng= sns.factorplot(y=\"Age\", x=\"Parch\", data=dataset, kind=\"box\")\ng= sns.factorplot(y=\"Age\", x=\"SibSp\", data=dataset, kind=\"box\")\n","de947ea3":"#convert Sex into categorical value 0 for male and 1 for female\ndataset[\"Sex\"]= dataset[\"Sex\"].map({\"male\":0, \"female\":1})","6521dd1f":"g = sns.heatmap(dataset[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), cmap=\"BrBG\", annot=True)","8869606a":"#Filling missing value of Age  ###IMPORTANT\n#Fill Age with the median age of similar rows according to\n#Pclassm Parch and SibSp\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age:\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][((dataset[\"SibSp\"]==dataset.iloc[i][\"SibSp\"])\n                              & (dataset[\"Parch\"]==dataset.iloc[i][\"Parch\"])\n                              &(dataset[\"Pclass\"]==dataset.iloc[i][\"Pclass\"]))].median()\n    \n    if not np.isnan(age_pred):\n        dataset[\"Age\"].iloc[i] = age_pred\n    else:\n        dataset[\"Age\"].iloc[i] = age_med\n    ","9ab12f02":"### What if, \ub2e8\uc21c median\uc774 \uc544\ub2cc, corr\uac00 \ud070 PClass\uc5d0 w \uac00\uc911\uce58 \uc918\uc11c, median \ub098\uc624\ub294 \uac83??\n","5aeb0e29":"g = sns.factorplot(x=\"Survived\", y=\"Age\", data=train, kind=\"box\")\ng = sns.factorplot(x=\"Survived\", y=\"Age\", data=train, kind=\"violin\")","57dbd407":"dataset['Name'].head()","2d36bc49":"\n#Get Title from Name\n\ndataset_title=[i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]","33ba0edf":"dataset[\"Title\"]=pd.Series(dataset_title)\ndataset[\"Title\"].head()","050838e1":"g = sns.countplot(x=\"Title\", data=dataset)\ng = plt.setp(g.get_xticklabels(), rotation=45)","f62a83ec":"# Convert to categorical values Title \ndataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)\n","cd1c5df3":"g = sns.countplot(dataset[\"Title\"])\ng =g.set_xticklabels([\"Master\", \"Miss\/Ms\/Mme\/Mlle\/Mrs\", \"Mr\",\"Rare\"])","c93f59b7":"g = sns.factorplot(x=\"Title\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\ng = g.set_ylabels(\"survival probability\")","7f6b15b9":"#drop Name variable\ndataset.drop(labels=[\"Name\"], axis=1, inplace=True)","4dbd3fec":"#Create Family size descriptor from SibSp and Parch\ndataset[\"Fsize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] +1","6c9a8334":"g = sns.factorplot(x=\"Fsize\", y=\"Survived\", data=dataset)\ng = g.set_ylabels(\"Survival Probability\")","0280a6ef":"#1-2 Convert FSize into categorical value 0 for alone 1 for family\n#dataset[\"Alone\"] = dataset[\"Fsize\"].map(lambda s: 1 if s==1 else 0)\n#dataset.drop(labels=[\"Fsize\"], axis=1, inplace=True)","52077874":"#1-1Create new feature of family size\ndataset[\"Single\"] = dataset[\"Fsize\"].map(lambda s: 1 if s==1 else 0)\ndataset[\"SmallF\"] = dataset[\"Fsize\"].map(lambda s: 1 if s==2 else 0)\ndataset[\"MedF\"] = dataset[\"Fsize\"].map(lambda s: 1 if 3<=s <=4 else 0)\ndataset[\"LargeF\"] = dataset[\"Fsize\"].map(lambda s: 1 if s>=5 else 0)","b9f42c42":"#1-1\ng = sns.factorplot(x=\"Single\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"SmallF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"MedF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"LargeF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","daf29975":"#convert to indicator valuees Title an Embarked\ndataset = pd.get_dummies(dataset, columns=[\"Title\"])\ndataset = pd.get_dummies(dataset, columns=[\"Embarked\"], prefix=\"Em\")","8287b525":"dataset.head()","a1a24686":"g = sns.heatmap(dataset[[\"Survived\",\"SibSp\",\"Parch\",\"Fsize\",\"Single\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","cd341b9a":"dataset.drop(labels=[\"SibSp\"], axis=1, inplace=True)\ndataset.drop(labels=[\"Parch\"], axis=1, inplace=True)","b6bff063":"dataset[\"Cabin\"].head()","b0a1a30a":"dataset[\"Cabin\"].describe()","ec83b508":"dataset[\"Cabin\"].isnull().sum()","4507ab52":"dataset[\"Cabin\"][dataset[\"Cabin\"].notnull()].head()","abbddd14":"#Replace the Cabin Number by the type of cabin'X' if not\ndataset[\"Cabin\"]= pd.Series([i[0] if not pd.isnull(i)\n                            else 'X' for i in dataset[\"Cabin\"]])","2a913b04":"g = sns.countplot(dataset[\"Cabin\"], order=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"T\",\"X\"])","87f050a1":"g = sns.factorplot(y=\"Survived\", x=\"Cabin\", data=dataset, kind=\"bar\",\n                  order=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"T\",\"X\"])\ng= g.set_ylabels(\"Survival Probability\")","8b55fc55":"dataset = pd.get_dummies(dataset, columns=[\"Cabin\"],prefix=\"Cabin\")","8f1b4f68":"dataset[\"Ticket\"].head()","535a3a42":"#Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X.\n#1. Ticket Dummies\nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").\n                     strip().split(' ')[0])\n    else:\n        Ticket.append(\"X\")","0a406d26":"#dataset[\"Ticket\"] = Ticket\ndataset[\"Ticket\"].head()\ndataset.drop(labels=[\"Ticket\"], axis=1, inplace=True)","94f14866":"#dataset= pd.get_dummies(dataset, columns=[\"Ticket\"], prefix=\"T\")","94a35c7f":"#Create categorical values for Pclass\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset= pd.get_dummies(dataset, columns=[\"Pclass\"], prefix=\"Pc\")","2cac7554":"#Drop useless variables\ndataset.drop(labels = [\"PassengerId\"], axis=1, inplace=True)","e87ab4bc":"dataset.head()","88df69dd":"dataset.isnull().sum()","9af7ae1c":"train = dataset[:train_len]\ntest = dataset[train_len:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)\n\ntrain[\"Survived\"] = train[\"Survived\"].astype(int)\n\nY_train = train[\"Survived\"]\n\nX_train = train.drop(labels = [\"Survived\"],axis = 1)","192fdbce":"from sklearn.linear_model import (LogisticRegression,SGDClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, VotingClassifier)\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import (GridSearchCV, StratifiedKFold, cross_val_score, learning_curve)\nfrom collections import Counter","bc03fd22":"from sklearn.model_selection import KFold\nModel = []\nRMSE = []\nACC = []\ncv = KFold(10, random_state = 42)\n\ndef input_scores_defaultparam(name, model, X, Y):\n    Model.append(name)\n    RMSE.append(np.sqrt((-1) * cross_val_score(model, X, Y, cv=cv, \n                                               scoring='neg_mean_squared_error')))\n    ACC.append(cross_val_score(model, X, Y, cv=cv, scoring='accuracy'))","20983e67":"Y_train_raveled = np.ravel(Y_train)","a4b441a5":"names = [\"Logistic\",\"K-NN\",\"SVC\",\"RandomForest\",\"SGD\",\"DecisionTree\",\"GradientBoosting\",\"GaussianNB\"]\nmodels = [LogisticRegression(), KNeighborsClassifier(3), SVC(), RandomForestClassifier(),\n          SGDClassifier(),DecisionTreeClassifier(), GradientBoostingClassifier(),GaussianNB()]\n\n#Running all algorithms\nfor name, model in zip(names, models): \n    input_scores_defaultparam(name, model,X_train, Y_train_raveled)# x_train_scaled, y_train)","6fae2637":"evaluation_def = pd.DataFrame({'Model': Model,\n                           'RMSE': RMSE,\n                           'ACC': ACC})\nprint(\"FOLLOWING ARE THE TRAINING SCORES: \")\nevaluation_def ","39bd8404":"#RMSE mean_\uacc4\uc0b0 \nRMSE_mean=[]\nfor i in range(len(RMSE)):\n    RMSE_mean.append(np.mean(RMSE[i]))\n\n#R_squared mean_\uacc4\uc0b0 \nACC_mean=[]\nfor i in range(len(ACC )):\n    ACC_mean.append(np.mean(ACC[i]))    ","fd40aa45":"evaluation_mean =pd.DataFrame({'Model': Model,\n                           'RMSE': RMSE_mean,\n                           'ACC': ACC_mean})\nprint(\"FOLLOWING ARE THE TRAINING SCORES: \")\nevaluation_mean","8d409856":"a=[evaluation_def.loc[i,'RMSE'] for i in range(0,8)]\naa=[evaluation_def.loc[i,'ACC'] for i in range(0,8)]","dd488247":"b = pd.DataFrame(a, columns=['Fold_'+str(i) for i in range(1,11)]) #RMSE\nbb = pd.DataFrame(aa, columns=['Fold_'+str(i) for i in range(1,11)]) #'R-Squared'","9d2f4edc":"bb['Model']=[\"1.LogisticRegression\",\"2.K-NNClassifier\",\"3.SVC\",\"4.RandomForestClassifier\",\n            \"5.SGDClassifier\",\"6.DecisionTreeClassifier\",\"7.GradientBoostingClassifier\",\"8.GaussianNB\"]","1d0bbb75":"index_best = evaluation_def['Model'].ravel()","1508002a":"mpl.rcdefaults()\nfor i in range(1,11):\n    plt.plot(b['Fold_'+str(i)],index_best)\nplt.xlabel('RMSE')\nplt.grid(linestyle=\":\")\nplt.title('RMSE using Default Parameter')\nplt.show()","323642a5":"mpl.rcdefaults()\nfor i in range(1,11):\n    plt.plot(bb['Fold_'+str(i)],index_best)\nplt.xlabel('ACC')\nplt.grid(linestyle=\":\")\nplt.title('ACC using Default Parameter')\nplt.show()","2e3b3d11":"KNN = KNeighborsClassifier(3)\nkfold=10\nkn_param_grid = {'leaf_size':[10, 100, 10],\n             'n_neighbors':range(1, 11, 1), 'p':[1,2]}\n\ngsKNN= GridSearchCV(KNN, param_grid = kn_param_grid, scoring='accuracy', \n                   cv=kfold, n_jobs=4).fit(X_train, Y_train_raveled)\nKNN_best = gsKNN.best_estimator_\n\n# Best score\ngsKNN.best_score_","08a8e2dc":"RFC = RandomForestClassifier()\nkfold=10\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,Y_train_raveled)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","c83c0bfe":"#Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,Y_train_raveled)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","c1a03bed":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,Y_train_raveled)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","aa02d594":"test_Survived_KNN = pd.Series(KNN_best.predict(test), name=\"KNN\")\ntest_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")","a5307d51":"# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_KNN,test_Survived_RFC,test_Survived_GBC, test_Survived_SVMC],axis=1)\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","dba8bc07":"votingC = VotingClassifier(estimators=[('knn',KNN_best),('rfc', RFC_best), \n('svc', SVMC_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X_train, Y_train)","abe86b6c":"test_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nresults = pd.concat([IDtest,test_Survived],axis=1)\n\nresults.to_csv(\"ensemble_rp_python_voting.csv\",index=False)","75b4a4a0":"small families have more chance to survive, more than single(Parch 0), medium(Parch 3,4) and large families(Parch 5,6). ","1acca34a":"cabin feature : 292 values, 1007 missing values : \nwithout a cabin have a missing value displayed instead of the cabin number","382b4ba4":"## 6.Modeling","4dc3fb91":"Age distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age. However, 1st class passengers are older than 2nd class passengers, who are also older than 3rd class passengers.*\nMoreover, the more a passenger has parents\/children the older he is and the more a passenger has siblings\/spouses the younger he is \n\n(** famiilysize\ub85c  \ub450 \uac1c \ud569\uce58\ub294 \uac83 \ub300\ube44 \ucc28\uc774 \ud655\uc778)","a824495b":"5. Feature Engineering","9a7aa3b5":"indeed the third class is the most frequent for passenger coming from Southampton(S) and Queenstown (Q), whereas Cherbourg passengers are mostly in first class which have the highest survival rate.\nAt this point, I can't explain why first class has an higher survival rate. My hypothesis is that first class passengers were prioritised during the evacuation due to their influence. ","9d69770e":"family size seems to play an important role, worst for large families : dcided to create 4 categories of family size","66d74120":"since we have only one missing value, fill with median value","c922b414":"4. Fill missing values","bf711085":"seems to be a tailed distribution, maybe a gaussian distribution.  there's a peak corresponding to young passengers, that have survived. 60-80 less survived. ","a7af2288":"women and children first, as told","5f606772":"it could mean that tickets sharing the same prefixes could be booked for cabins placed together. lead to the actual placement of the cabins within the ship. same prefixes : may have a similar class and survial.   ","48168985":"no difference between median value of age in survived and not survived subpopulation. violin plot : very young passengers have high survive rate! \n***** violin \uc678\uc5d0\ub3c4 \ud2b9\uc774\ud55c \ubd84\ud3ec \uc2dc\uac01\ud654 \ud568\uc218 \ucccc\ucccc","ab4f912d":"the correlation map confirms the factorplots observations except for Parch. Age is not correlated with SEx but is negatively correlated with Pclass, Parch, and SibSp. \nIn the plot of Age in function of Parch, Age is growing with the number of Parch (was at front factorplot) but the general correlation is negative...  so I decided to use SibSp, Parch and Pclass to impute the missing ages.","a64072c0":"As we can see, Fare distribution is very very skewed. This can lead to overweighed very high valuees in the model, even if it is scaled. Better to transform it with the log function to reduce this skew.","7f2905fa":"first>second>third, and is consistent trend in both male and female","eb901ae0":"superimpose the two densities, we clearly see a peak corresponding(between 0 and 5) to babies and very young children ","a4a1342b":"only Fare feature seems to have a significant correlation with the survival probability. ","e4526ace":"### Tuning Trees****","1b0cb008":"passenger coming from Cherbourg(C) have more chance to survive. My hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown(Q), Southamption(S). \n\nLet's see the PClass distribution vs Embarked","5bdc62f0":"Because of the low number of passenger that have a cabin, survival probabilities have an important standard deviation and we can't distinguish between survival probability of passengers in the different desks. \n\nBut we can see that passngers with a cabin have generally more chance to survive than passngers without (x). \nIt is particularly true for cabin B, C, D,E and F. ","23df711f":"1. Understand the Data","c4acea2c":"passengers having a lot of siblings\/spouses have less chance to survive. Single passengers ( 0 SiBSP) or with two other persons(SibSp 1 or 2) have a more chance to survive"}}