{"cell_type":{"11c6cef2":"code","b067a626":"code","fd3200e0":"code","0f485960":"code","40b791a9":"code","29380686":"code","4ba623df":"code","e65d7366":"code","18fb86c1":"code","f2169e3d":"code","4731191a":"code","c7bd5227":"code","92955949":"code","e240eb59":"code","f2db8596":"code","4c6a3bf0":"code","0e4d3c7a":"code","3f48bf45":"code","c2423ccc":"code","3fbc3e0c":"code","60931b2c":"code","35de08ec":"code","1af63da6":"code","837c1669":"code","46a610e4":"code","17eb7271":"code","324dff67":"code","cbfbb903":"code","a4947d6f":"code","3d637eea":"code","dfe052c4":"code","9f511ed2":"code","c6538fcf":"code","07812fda":"markdown","5913a3bc":"markdown","31129d61":"markdown","8408b53e":"markdown","9db45b83":"markdown","4d158ab5":"markdown","a72ca1f3":"markdown","76102866":"markdown","6be3380e":"markdown","9616c0e9":"markdown","7d669979":"markdown","0806ad3e":"markdown","f027626c":"markdown","c57712a2":"markdown","605ce4ad":"markdown","1ed7b5d9":"markdown","d9acfabf":"markdown","ddeedd6d":"markdown"},"source":{"11c6cef2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b067a626":"import os\nimport glob","fd3200e0":"import matplotlib.pyplot as plt\nimport cv2","0f485960":"train_data_path = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/train\/\"","40b791a9":"data = []\n\nfor sub_dir_path in glob.glob(train_data_path + \"*\"):\n    if os.path.isdir(sub_dir_path):\n        dirname = sub_dir_path.split(\"\/\")[-1]\n        for filename in os.listdir(sub_dir_path):\n            image_path = sub_dir_path + \"\/\" + filename\n            data.extend([dirname, image_path])\n        \n        \ndf = pd.DataFrame(\n    {\n        \"Target\" : data[::2],\n        \"Path\" : data[1::2]\n    }\n)\n\ndf.head()","29380686":"df['Target'].value_counts()","4ba623df":"df.info()","e65d7366":"pwd","18fb86c1":"normal_dir = train_data_path+\"NORMAL\"\npneumonia_dir = train_data_path+\"PNEUMONIA\"\ndirectory_list = [pneumonia_dir,normal_dir]\nprint(normal_dir)\nprint(pneumonia_dir)","f2169e3d":"#Augmentor is a python library which is used for augmenting the images using different techniques such as rotation, cropping etc.\n!pip install Augmentor","4731191a":"import Augmentor","c7bd5227":"for directory in directory_list:\n    \n    #Here the first argument is the input directory,second_argument is the output directory\n    if directory == directory_list[0]:\n        p = Augmentor.Pipeline(directory,output_directory = \"\/kaggle\/working\/Pneumonia\" ) \n    else: \n        p = Augmentor.Pipeline(directory,output_directory = \"\/kaggle\/working\/Normal\" ) \n    \n    \n    p.rotate(probability=0.5, max_left_rotation=15, max_right_rotation=15) #adding rotation to 50% of images\n    p.zoom(probability=0.3, min_factor=1.1, max_factor=1.3) #adding zoom to 30% of the images\n    p.random_distortion(probability=0.1,grid_width=10,grid_height=10,magnitude=2) #adding distortions to 10% of the images\n    \n    p.sample(5000) #Specify the number of augmented images to generate","92955949":"count = 0\nfor sub_dir_path in glob.glob('\/kaggle\/working\/Pneumonia' + \"*\"):\n    if os.path.isdir(sub_dir_path):\n        dirname = sub_dir_path.split(\"\/\")[-1]\n        for filename in os.listdir(sub_dir_path):\n            count+=1\n\nprint(\"Total number of Pneumonia images : \"+str(count))","e240eb59":"count = 0\nfor sub_dir_path in glob.glob('\/kaggle\/working\/Normal' + \"*\"):\n    if os.path.isdir(sub_dir_path):\n        dirname = sub_dir_path.split(\"\/\")[-1]\n        for filename in os.listdir(sub_dir_path):\n            count+=1\n\nprint(\"Total number of Normal images : \"+str(count))","f2db8596":"count = 0\npath = \"\/kaggle\/working\/Pneumonia\/\"\nfor sub_dir_path in glob.glob('\/kaggle\/working\/Pneumonia' + \"*\"):\n    if os.path.isdir(sub_dir_path):\n        dirname = sub_dir_path.split(\"\/\")[-1]\n        for filename in os.listdir(sub_dir_path):\n            path+=filename\n            print(path)\n            count+=1\n            if count >= 1:\n                break","4c6a3bf0":"img = plt.imread(path)\nprint(img.shape)\nplt.imshow(img)\nplt.show()","0e4d3c7a":"from tensorflow.keras.preprocessing import image_dataset_from_directory","3f48bf45":"train_dataset = image_dataset_from_directory(\"\/kaggle\/working\/\", labels='inferred', label_mode='int',\n    class_names=None, color_mode='rgb', batch_size=64, image_size=(128,\n    128), shuffle=True, seed=123, validation_split=0.2, subset=\"training\")","c2423ccc":"validation_dataset = image_dataset_from_directory(\"\/kaggle\/working\/\", labels='inferred', label_mode='int',\n    class_names=None, color_mode='rgb', batch_size=64, image_size=(128,\n    128), shuffle=True, seed=123, validation_split=0.2, subset=\"training\")","3fbc3e0c":"class_names = train_dataset.class_names\nprint(class_names)","60931b2c":"for images,labels in train_dataset.take(1):\n    image = images[0].numpy()\/255.0\n    plt.imshow(images[0].numpy()\/255.0)\n    plt.show()\n    print(image.shape)","35de08ec":"import tensorflow as tf","1af63da6":"# AUTOTUNE = tf.data.experimental.AUTOTUNE\n# train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n# validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)","837c1669":"cnn = tf.keras.models.Sequential()\n\ncnn.add(tf.keras.layers.Rescaling(1.\/255, input_shape=(128, 128, 3)))\ncnn.add(tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding = 'same', activation='relu'))\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\ncnn.add(tf.keras.layers.Dropout(0.25))\ncnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding = 'same', activation='relu'))\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\ncnn.add(tf.keras.layers.Dropout(0.25))\ncnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding = 'same', activation='relu'))\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\ncnn.add(tf.keras.layers.Flatten())\ncnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\ncnn.add(tf.keras.layers.Dense(units=2, activation = \"sigmoid\"))","46a610e4":"cnn.summary()","17eb7271":"cnn.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n              metrics=['accuracy'])","324dff67":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.001)","cbfbb903":"history = cnn.fit(x=train_dataset, epochs=20, validation_data=validation_dataset,callbacks = [reduce_lr])","a4947d6f":"for images, labels in train_dataset.take(1):\n    \n    plt.imshow(images[0].numpy().astype('float32')\/255.0)\n    predictions = cnn.predict(images)\n    score = tf.nn.softmax(predictions[0])\n    \n    print(\"This image most likely belongs to {}.\".format(class_names[np.argmax(score)]))\n\n","3d637eea":"from tensorflow.keras.applications.vgg16 import VGG16\n\n# Initialize the Pretrained Model\nfeature_extractor = VGG16(weights='imagenet', \n                             input_shape=(128, 128, 3),\n                             include_top=False)\n\nfeature_extractor.trainable = False\n\n# Set the input layer\ninput_ = tf.keras.Input(shape=(128, 128, 3))\n\n# Set the feature extractor layer\nx = feature_extractor(input_, training=False)\n\n# Set the pooling layer\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\n\n# Set the final layer with sigmoid activation function\noutput_ = tf.keras.layers.Dense(2, activation='sigmoid')(x)\n\n# Create the new model object\nmodel = tf.keras.Model(input_, output_)\n\n# Compile it\nmodel.compile(optimizer='adam',\n             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n             metrics=['accuracy'])\n","dfe052c4":"model.summary()","9f511ed2":"history_two = model.fit(x=train_dataset, epochs=20, validation_data=validation_dataset,callbacks = [reduce_lr])","c6538fcf":"for images, labels in train_dataset.take(1):\n    \n    plt.imshow(images[0].numpy().astype('float32')\/255.0)\n    predictions = model.predict(images)\n    score = tf.nn.softmax(predictions[0])\n    \n    print(\"This image most likely belongs to {}.\".format(class_names[np.argmax(score)]))\n\n","07812fda":"# 1. Extracting Data and gathering insights","5913a3bc":"# 3.2 Pretrained VGG16 architecture","31129d61":"# Please check out my other notebooks on Machine Learning, NLP and Computer Vision as well :)","8408b53e":"**Note** : Here, we will be using VGG16 model for the demo purpose. Since, our custom CNN model was able to give good scores on both train and validation dataset, we do not need to use a pretrained model. In addition, we can also use other pretrained architectures such as ResNet50, MobileNetV3 and so on.","9db45b83":"# 2. TF Dataset creation","4d158ab5":"First, lets build a custom CNN image classification pipeline in tensorflow. Post the evaluation of the custom CNN image classification pipeline, let's explore building an image classification pipeline using a pretrained model like VGG16 and compare the evaluation metrics.","a72ca1f3":"Observe that the number of parameters here are less compared to the custom CNN architecture. Hence, we can assume that the model might not perform as well as custom CNN architecture. We can add in more layers to the VGG16 architecture as well after the global_average_pooling layer.","76102866":"Ensuring that there are no null values in the dataframe as an integrity check.","6be3380e":"# Please upvote this notebook if you found it helpful :)","9616c0e9":"We have now handled the class imbalance and introduced new augmented images to make the model more generalised.","7d669979":"Here, we observe that the number of classes belonging to Normal class is less compared to Pneumonia class. However, there is no extreme imbalance in the dataset. We will be handling any class imbalance in the dataset as well as image augmentation using **Augmentor library** which will be the **second step**:","0806ad3e":"# 3. Model Builing","f027626c":"Before creating tensorflow dataset, lets identify the shape (height,width) of the images:","c57712a2":"# 3.1 Custom CNN Architecture","605ce4ad":"**First step** is to perform some exploratory data analysis on this dataset such as identifying class imbalance. To achieve this, lets extract all the filenames along with their parent directory into a dataframe:","1ed7b5d9":"The **fourth step** is to build a Tensorflow classification model. \n<\/br>\n<\/br>\nThe basis of classification model are the **convolution layers** and **pooling layers** which are used to extract multiple features from the input image and aggregate those features. So, the initial convolutional layers extract features such as edges, horizontal and vertical lines etc, while the later layers extract more complex features such as textures. Finally, the output of these layers will be sent to the feed forward network for classifcation.\n\n![Intenet Source](https:\/\/miro.medium.com\/max\/1400\/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)\n\nImage Source : https:\/\/towardsdatascience.com\/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53","d9acfabf":"The **third step** is to push these images into a tensorflow dataset format using a tensorflow function called image_dataset_from_directory. <\/br>\n**Note** : There are other techniques also available to push the images and labels to a tensorflow dataset, which are mentioned below:\n<ol>\n    <li>Pass the image and label list to tf.data.dataset.from_tensor_slices(images,labels)<\/li>\n    <li>Using TF Records. Please refer to my notebook : ImageSegmentation_Pydicom_TFRecord_UNet_Demo for understanding the implementation of TF Records and thier usage<\/li>\n<\/ol>","ddeedd6d":"We have successfully pushed the augmented images to the tensorflow dataset now. train_dataset contains 8000 (image,label) pairs and validation dataset contains the remaining files."}}