{"cell_type":{"c4f25143":"code","6dad40a4":"code","00ee6a18":"code","50523ba2":"code","46e73ded":"code","7258c6e3":"code","ae0f46ce":"code","e161a831":"code","4b7affaf":"code","ce6a2c9d":"code","ca6aade5":"code","7b48557f":"code","1bb0f551":"code","90616a9f":"code","0a007233":"code","d897dd5d":"code","00fe45f7":"code","7977e57f":"code","2cb2eebf":"code","438900aa":"code","5949d61f":"code","29d5cd56":"code","4fa47b1c":"code","0d5e7953":"code","c13514a3":"code","c42d25df":"code","ef73773c":"code","82a0690e":"code","aa203fe9":"code","a7cd51dd":"code","d846c16f":"code","7e6f689e":"code","58d1667a":"code","6983409c":"code","e9c74a0e":"code","50787fa3":"code","d66cd979":"code","b5648c73":"code","3d2e4288":"code","6e885812":"code","e9222da9":"code","ac71cd98":"code","565caf3e":"code","68f9919d":"code","d17124cb":"code","e7a7dff5":"code","ff13220d":"code","6104a445":"code","b7d09bce":"code","e365b31a":"code","77b2eaff":"code","b64de70c":"code","ebb0c03b":"code","83983315":"code","28a7c1fb":"code","1f738b40":"code","3a43cecb":"code","069dd6e7":"code","2110b013":"code","2849a9fb":"code","f8581b1d":"code","41c3481a":"code","57caa620":"code","4bf7bbbc":"code","8a947209":"code","9dd9f2ff":"code","d51da90b":"code","2b60c6cc":"code","a71166f6":"code","f8a3ed2b":"code","0327dc46":"code","18e95260":"code","e4360da0":"code","c2c43e9b":"code","5f976923":"code","73f4566b":"code","c589182e":"code","cefd7a90":"code","26e9d2cb":"code","d7ac45cb":"code","7f9960c4":"code","d767dee6":"code","741a340d":"code","5974cb1e":"code","da2ac511":"code","decd8459":"code","ef3a162d":"code","9df3edeb":"code","17e18605":"code","f16fc88b":"code","21d8f73d":"code","a851678c":"code","47a06e4d":"code","2e4882f4":"code","5cc8e4d1":"code","2181e01b":"code","824e2851":"code","ec1fe66d":"code","9463565a":"markdown","20184335":"markdown","65e9318a":"markdown","35f14e79":"markdown","ac2642c2":"markdown","21ad8aa3":"markdown","44aff64b":"markdown","d50b8d25":"markdown","81090144":"markdown","b7ba95f1":"markdown","1361fab2":"markdown","006d9d2d":"markdown","17ee276d":"markdown","66407ad8":"markdown","d3288a86":"markdown","31ce4ec3":"markdown","4f006f49":"markdown","c0fa2e1a":"markdown","522165e5":"markdown","570f7e49":"markdown"},"source":{"c4f25143":"# Import module feature_selector- it will be used as part of Exploratory Data Analysis\n!cp -r ..\/input\/feature-selector\/feature-selector\/feature-selector-master\/feature_selector\/* .\/\nimport feature_selector","6dad40a4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom feature_selector import FeatureSelector","00ee6a18":"train_df = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/train.csv', low_memory=False, na_values= '-1')\npd.options.display.max_rows=None\npd.options.display.max_columns = None\ntrain_df.head()","50523ba2":"train_df.target.value_counts()","46e73ded":"train_df['target'].value_counts().plot(kind='bar', figsize=(5,5));\n","7258c6e3":"# Sample figsize in inches\nfig, ax = plt.subplots(figsize=(20,10))         \n# Imbalanced DataFrame Correlation\ncorr = train_df.corr()\nsns.heatmap(corr, cmap='RdYlBu', annot_kws={'size':30}, ax=ax)\nax.set_title(\"Feature Correlation Matrix\", fontsize=14)\nplt.show()","ae0f46ce":"# Make a copy of data\ntrain_df_copy = train_df.copy()","e161a831":"def remove_calc(data_df):\n  for label, content in data_df.items():\n    if '_calc' in label:\n      data_df.drop([label], axis=1, inplace=True)\n\n  return data_df\n","4b7affaf":"train_df_copy = remove_calc(train_df_copy)\n\ntrain_df_copy.columns.values","ce6a2c9d":"train_df_copy.columns.value_counts()","ca6aade5":"fig, ax = plt.subplots(figsize=(20,10))         \n(train_df.isna().sum()*100\/len(train_df)).round(2).plot(kind='bar', color='salmon');","7b48557f":"# from google.colab import drive\n# drive.mount('\/content\/drive')","1bb0f551":"\ntrain_df_copy.drop(['ps_car_03_cat', 'ps_car_05_cat'], axis=1, inplace=True)","90616a9f":"train_df_copy.info()","0a007233":"train_df_copy.columns.values","d897dd5d":"\ncategorical_column =[]\ncategorical_missing_data=[]\nnot_categorical = []  \n# train_target = []\n# train_id = []\n\ndef preprocess_data(data_df):\n  data_df_copy = data_df.copy()\n\n  if 'target' in data_df.columns:\n    train_target = data_df.target\n    data_df.drop(['target'], axis=1, inplace=True)\n  if 'id' in data_df.columns:\n    train_id = data_df.id\n    data_df.drop(['id'], axis=1, inplace=True) \n\n  \n\n  for label, content in data_df.items():    \n    if '_cat'  in label:\n      categorical_column.append(label)\n      data_df[label].fillna(value=content.mode()[0], inplace=True)\n      data_df[label] = data_df[label].astype('category')\n\n    elif '_bin' in label:\n      data_df[label].fillna(value=content.mode()[0], inplace=True)\n\n    else:\n      data_df[label].fillna(value=content.median(), inplace=True)\n      not_categorical.append(label)    \n\n    \n  print(categorical_column)\n  if 'target' in data_df_copy.columns:\n    data_df.insert(loc=0, column='target', value=train_target)    \n    # if (train_target.empty == True) :\n      \n  if ('id' in data_df_copy.columns):\n    data_df.insert(loc=0, column='id', value= train_id)\n    # if (train_id.empty == True):\n\n  ### Remove outliers\n  # #Dropping the outlier rows with standard deviation\n  # factor = 4\n  # for label, content in data_df.items():\n  #   upper_lim = data_df[label].mean () + data_df[label].std () * factor\n  #   lower_lim = data_df[label].mean () - data_df[label].std () * factor\n\n  #   data = data_df[(data_df[label] < upper_lim) & (data_df[label] > lower_lim)]     \n\n  return data_df       \n        ","00fe45f7":"preprocessed_train_data = preprocess_data(train_df_copy)\n\n","7977e57f":"preprocessed_train_data.isna().sum()","2cb2eebf":"preprocessed_train_data.info()","438900aa":"# shuffled_df = preprocessed_train_data","5949d61f":"# # shuffled_df.drop(['id'], axis=1, inplace=True)\n# shuffled_df[categorical_column].head(10)","29d5cd56":"len(preprocessed_train_data)","4fa47b1c":"# # Extract Features and target\n\n# X = shuffled_df.drop(['target', 'id'], axis=1)\n# y=  shuffled_df['target']\n\n#train_df_copy['ps_ind_02_cat'].value_counts()\nlen(categorical_column), len(categorical_missing_data), len(not_categorical)","0d5e7953":"# # from sklearn.preprocessing import OneHotEncoder\n# # from sklearn.compose import ColumnTransformer\n# # categorical_features = categorical_column\n# # one_hot = OneHotEncoder(sparse=False)\n# # transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')\n\n# # transformed_x = transformer.fit_transform(X)\n# shuffled_df_encoded = pd.get_dummies(shuffled_df[categorical_column])\n","c13514a3":"# shuffled_df_encoded.head()","c42d25df":"# shuffled_df_encoded.isna().sum()","ef73773c":"# shuffled_cat_dropped = shuffled_df.drop(categorical_column, axis=1)\n# shuffled_df_encoded.drop(['ps_ind_02_cat_3.0'], axis=1, inplace=True)\n# shuffled_cat_dropped.head()","82a0690e":"# shuffled_upd = pd.concat([shuffled_cat_dropped, shuffled_df_encoded], axis=1)","aa203fe9":"# shuffled_upd.head()","a7cd51dd":"preprocessed_train_data.head()","d846c16f":"def Encode_Scale(data_df,categorical_features):\n  \"\"\"\n  Function takes a dataframe, and a list of categorical features, encodes the categorical features\n  and scales same.\n\n  \"\"\"\n  data_df_copy = data_df.copy()\n\n  if 'target' in data_df.columns:\n    train_target = data_df.target\n    data_df.drop(['target'], axis=1, inplace=True)\n  if 'id' in data_df.columns:\n    train_id = data_df.id\n    data_df.drop(['id'], axis=1, inplace=True) \n\n\n\n  #One-Hot Encoding of categorical data\n  data_df_encoded = pd.get_dummies(data_df[categorical_column])\n  data_df_encoded.head()\n  data_df_encoded.isna().sum()\n\n  ### After the one-hot encoding, we drop the original unencoded categorical columns,\n  ### then one of the new encoded feature columns to reduce multicollinearity.\n\n\n  data_cat_dropped = data_df.drop(categorical_column, axis=1)\n  data_df_encoded.drop(['ps_ind_02_cat_3.0'], axis=1, inplace=True)\n  data_cat_dropped.head()\n\n  ### Concatenate the encoded categorical features with the other features less the unencoded categorical features\n\n  data_upd = pd.concat([data_cat_dropped, data_df_encoded], axis=1)\n\n  if 'target' in data_df_copy.columns:\n    data_upd.insert(loc=0, column='target', value=train_target)    \n    # if (train_target.empty == True) :\n      \n  if ('id' in data_df_copy.columns):\n    data_upd.insert(loc=0, column='id', value= train_id)\n    # if (train_id.empty == True):\n\n\n  data_upd.head()\n\n  # preferred_data = data_upd[preferred_features]\n\n  # from sklearn.preprocessing import StandardScaler\n  # X = StandardScaler().fit_transform(preferred_data)\n\n  # X = pd.DataFrame(X)\n\n\n  return data_upd\n","7e6f689e":"preprocessed_train_data.head()","58d1667a":"shuffled_upd = Encode_Scale(preprocessed_train_data, categorical_column)","6983409c":"shuffled_upd.head()","e9c74a0e":"# Extract Features and target\n\nX = shuffled_upd.drop(['target', 'id'], axis=1)\ny=  shuffled_upd['target']","50787fa3":"from feature_selector import FeatureSelector\nfs = FeatureSelector(X, y)","d66cd979":"fs.identify_all(selection_params = {'missing_threshold': 0.6, 'correlation_threshold': 0.98, \n                                    'task': 'classification', 'eval_metric': 'auc', \n                                     'cumulative_importance': 0.99})","b5648c73":"# justcheckit = fs.one_hot_features","3d2e4288":"shuffled_df_removed_all_once = fs.remove(methods = 'all', keep_one_hot = True)","6e885812":"shuffled_df_removed_all_once.shape","e9222da9":"fs.plot_feature_importances(plot_n = 15, threshold=0.99)","ac71cd98":"preferred_features = np.array(fs.feature_importances[fs.feature_importances['cumulative_importance']<0.990402]['feature'])","565caf3e":"len(preferred_features)","68f9919d":"preferred_data = fs.data[preferred_features]\npreferred_data.head()","d17124cb":"# Using get_dummies to encode categorical features\n# cat_df = pd.get_dummies(shuffled_df, columns=[categorical_column])\n# cat_df.head()","e7a7dff5":"### Lets scale the features to get them with same range of magnitude\n\nfrom sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(preferred_data)","ff13220d":"X = pd.DataFrame(X)\nX.head()\n","6104a445":"### Models used\n# Models from Scilit-Learn\n\nfrom sklearn.linear_model import LogisticRegression\n# from sklearn.model_selection import cross_val_score\n# from sklearn.model_selection import RepeatedStratifiedKFold\n# from xgboost import XGBClassifier\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import recall_score, f1_score, precision_score, accuracy_score, auc\n\n","b7d09bce":"# np.random.seed(42)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","e365b31a":"from sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as make_pipeline_imb\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport lightgbm as lgb\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\n\n# build model with embedded undersampling technique \n# param = {'num_leaves': 31, 'objective': 'binary'}\n# param['metric'] = 'auc'\nmpipeline = make_pipeline_imb(BalancedBaggingClassifier(base_estimator=lgb.LGBMClassifier(n_jobs=-1),\n                                                   sampling_strategy='auto',\n                                                   replacement=False,\n                                                   random_state=0))\nmodel = mpipeline.fit(X_train, y_train)\nmodel.score(X_val, y_val)\nbbc_pred = model.predict_proba(X_val)\n","77b2eaff":"# build model with SMOTE imblearn\nsmote = SMOTE(random_state=42)\nX_res, y_res = smote.fit_resample(X, y)\n\nX_train2, X_val2, y_train2, y_val2 = train_test_split(X_res, y_res, test_size = 0.2)\nsmote_model = LogisticRegression(n_jobs=-1)      #XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100)\nsmote_model.fit(X_train2, y_train2)\nsmote_score = smote_model.score(X_val2, y_val2)\n","b64de70c":"smote_score","ebb0c03b":"smote_pred = smote_model.predict_proba(X_val2)","83983315":"smote_model.score(X_train2, y_train2)","28a7c1fb":"smote_predict = smote_model.predict(X_val2)","1f738b40":"model.score(X_train, y_train)","3a43cecb":"model.score(X_val, y_val)","069dd6e7":"\n# cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n# scores = cross_val_score(smote_model, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n# score_accuracy = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)","2110b013":"# score_accuracy.mean()","2849a9fb":"# from google.colab import drive\n# drive.mount('\/content\/drive')","f8581b1d":"# Calculating the normalized gini coefficient.\ndef ginic(actual, pred):\n    actual = np.asarray(actual) #In case, someone passes Series or list\n    n = len(actual)\n    a_s = actual[np.argsort(pred)]\n    a_c = a_s.cumsum()\n    giniSum = a_c.sum() \/ a_s.sum() - (n + 1) \/ 2.0\n    return giniSum \/ n\n \ndef gini_normalizedc(a, p):\n    if p.ndim == 2:#Required for sklearn wrapper\n        p = p[:,1] #If proba array contains proba for both 0 and 1 classes, just pick class 1\n    return ginic(a, p) \/ ginic(a, a)\n","41c3481a":"smote_pred[:,1]","57caa620":"gini_normalizedc(y_val, bbc_pred[:,1])","4bf7bbbc":"gini_normalizedc(y_val2, smote_pred[:, 1])","8a947209":"# from google.colab import drive\n# drive.mount('\/content\/drive')","9dd9f2ff":"test_df = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/test.csv', low_memory=False, na_values='-1')\ntest_df.head()","d51da90b":"test_df.shape","2b60c6cc":"test_df.isna().sum()","a71166f6":"test_data_no_id = test_df.drop(['id'], axis=1)","f8a3ed2b":"### Check missing values in the test data\nfig, ax = plt.subplots(figsize=(20,10))         \n(test_df.isna().sum()*100\/len(test_df)).round(2).plot(kind='bar', color='salmon');","0327dc46":"### Remove fetures having more than 50% of it's data missing\ntest_df.drop(['ps_car_03_cat', 'ps_car_05_cat'], axis=1, inplace=True)","18e95260":"# test_df.drop(['id'], axis=1, inplace=True)\ncategorical_column=[]\ncategorical_missing_data = []\nnot_categorical = []\npreprocessed_test_df = preprocess_data(test_df)","e4360da0":"len(categorical_column), len(categorical_missing_data), len(not_categorical)","c2c43e9b":"preprocessed_test_df.isna().sum()","5f976923":"preprocessed_test_df.head()","73f4566b":"fig, ax = plt.subplots(figsize=(20,10))         \n# Imbalanced DataFrame Correlation\ncorr = preprocessed_test_df.corr()\nsns.heatmap(corr, cmap='RdYlBu', annot_kws={'size':30}, ax=ax)\nax.set_title(\"Feature Correlation Matrix\", fontsize=14)\nplt.show()","c589182e":"preprocessed_test_df   = remove_calc(preprocessed_test_df)","cefd7a90":"preprocessed_test_df.columns.values","26e9d2cb":"preprocessed_test_df.head()","d7ac45cb":"preprocessed_test_df_upd = Encode_Scale(preprocessed_test_df, categorical_column)","7f9960c4":"# sum_feature_df= pd.DataFrame(transformed_testData_x[:10])\n# sum_feature_df\npreprocessed_test_df_upd.drop(['id'], axis=1, inplace=True)","d767dee6":"preprocessed_test_data = preprocessed_test_df_upd[preferred_features]","741a340d":"preprocessed_test_data.shape","5974cb1e":"preprocessed_test_data.head()","da2ac511":"preprocessed_test_data.head()","decd8459":"X_test = StandardScaler().fit_transform(preprocessed_test_data)","ef3a162d":"X_test = pd.DataFrame(X_test)\nX_test.head()","9df3edeb":"# from google.colab import drive\n# drive.mount('\/content\/drive')","17e18605":"X_test.shape","f16fc88b":"test_pred = smote_model.predict_proba(X_test)","21d8f73d":"test_pred[:,1][:20]","a851678c":"test_pred2 = model.predict_proba(X_test)","47a06e4d":"test_pred2[:,1][:20]","2e4882f4":"# preprocessed_test_df.head()","5cc8e4d1":"PIC_Submission = pd.DataFrame(test_pred2[:,1], columns=['target'], index=np.arange(0,len(preprocessed_test_df)))","2181e01b":"PIC_Submission.head()","824e2851":"len(PIC_Submission)","ec1fe66d":"#PIC_Submission.to_csv('submit_pred_2.csv')","9463565a":"### Evaluation Metric for the model using Normalized Gini Coefficient as requested for the competition - all credits for the code due - `https:\/\/www.kaggle.com\/tezdhar\/faster-gini-calculation`","20184335":"### Concatenate the encoded categorical features with the other features less the unencoded categorical features","65e9318a":"### Let's further explore the individual features and their importance - using the resource - https:\/\/github.com\/WillKoehrsen\/feature-selector\/blob\/master\/Feature%20Selector%20Usage.ipynb","35f14e79":"### Hyperparameter Tunning of the model","ac2642c2":"### All features with `_calc_` designation in itself label has been remove since it has little or no correlation with other features - then of little consequence to the model","21ad8aa3":"### Load training dataset and covert missing data value to `NaN`","44aff64b":"### We are going to use learning models from imblearn due to the imbalanced nature of the datasets - ","d50b8d25":"### From the last cell, the target class, shows an imbalanced dataset -  it's clear from the class `0` far exceed class `1` - let's visualize","81090144":"### Check if dataset is balanced","b7ba95f1":"### From the above plot of missing values in each feature, features `ps_car_03_cat` and `ps_car_05_cat` both have above ~50% missing values - it's safe to drop them as they'll add minimal value to the model.","1361fab2":"## Preprocess Test Dataset","006d9d2d":"### The above plot suggest a further data exploration given that the dataset is not balanced. Let's check feature correlation","17ee276d":"### From the above it's clear the data needs balancing - Let's do it!","66407ad8":"### Convert Categorical Data to Numerical using one Hot Encoding","d3288a86":"### Given the imbalanced nature of the dataset, let's deploy two balancing techniques -  Synthetic Minority Oversampling technique with the RandomForestClassifier estimator, and the BalancedBaggingClassifier estimator with it's underlying undersampling technique.\n","31ce4ec3":"### Replace all missing data designated as -1 per Kaggle description","4f006f49":"### Load test Dataset","c0fa2e1a":"### Remove Features with little or no correlation either with the target or other features  - Looking at the above correlation matrix, features with label `_calc_` have ~ no correlation with target - let's drop'em","522165e5":"### Dataset files downloaded from Kaggle","570f7e49":"### After the one-hot encoding, we drop the original unencoded categorical columns, then one of the new encoded feature columns to reduce multicollinearity."}}