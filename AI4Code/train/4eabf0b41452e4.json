{"cell_type":{"5e195695":"code","5f09c319":"code","f785771e":"code","2680a949":"code","53f27814":"code","00d62f9d":"code","be44682a":"code","548a0070":"code","38ba19a8":"code","d9426ef1":"code","8c125fcc":"code","ea4beda9":"code","d17f2f88":"code","1a37376c":"code","a8a12db5":"code","85ca413d":"code","ca231998":"code","22cd0b60":"code","a874f0ee":"code","93873eff":"code","7bfedbb7":"code","8c09e5b1":"code","6f058869":"code","4b1bd71f":"code","6cb7eaa4":"code","bd431952":"code","bace0d95":"code","ac94d5d6":"code","f35c20e5":"code","7781cfa8":"code","0f0f758d":"code","291c0163":"code","cd9b4412":"code","ee332550":"code","23323d29":"code","ff6b7381":"code","202af186":"code","b2f6b5a0":"code","51699740":"code","2afca864":"code","6a2c6599":"code","c96cc6db":"code","6ab91908":"code","75ca0ef8":"code","359fda83":"code","6a8aef10":"markdown","9a84eca3":"markdown","33c38ea2":"markdown","ebd621c9":"markdown","7d463a2a":"markdown","1d315e2d":"markdown","ed2d5107":"markdown","0ca26d15":"markdown","d20a17b7":"markdown","aa2c144d":"markdown","8b71354a":"markdown","3e9e4d37":"markdown","4646269d":"markdown","6326c2de":"markdown","d358a57c":"markdown","49151d76":"markdown","3e76c74a":"markdown","0b04cabb":"markdown","4cf436b3":"markdown"},"source":{"5e195695":"!pip install vit_keras","5f09c319":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Input\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.applications.vgg16 import decode_predictions\nfrom keras import Model\n\nimport time\nfrom tensorflow.keras.applications import vgg16, VGG16, \\\n                                          vgg19, VGG19, \\\n                                          resnet50, ResNet50, \\\n                                          resnet_v2, ResNet50V2, \\\n                                          inception_v3, InceptionV3, \\\n                                          mobilenet_v3, MobileNetV3Large, \\\n                                          inception_resnet_v2, InceptionResNetV2, \\\n                                          efficientnet, EfficientNetB7\n\nimport joblib\nimport pandas as pd\nfrom vit_keras import vit, utils\nimport tensorflow_addons as tfa\nfrom vit_keras.vit import vit_b16, vit_b32, vit_l16, vit_l32\n\n%matplotlib inline ","f785771e":"p = Path('..\/input\/dogs-data\/output\/train\/')\n\nclasses = [str(x).split('\/')[-1] for x in p.iterdir() if x.is_dir()]\n\n","2680a949":"MODELS = []\n\n###### Callback pour la r\u00e9duction du learning rate ######\ndef scheduler(epoch, lr):\n    return lr\/1.8 \n\n###### Callback pour le calcul du temps d'entrainement ######\nclass timecallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Callback used to evaluate time et plot it\n    \"\"\"\n    def __init__(self):\n        self.times = []\n        self.timetaken = time.time()\n        \n    def on_epoch_end(self,epoch,logs = {}):\n        self.times.append((epoch+1, time.time() - self.timetaken))\n        \n    def on_train_end(self,logs = {}):\n        plt.xlabel('Epoch')\n        plt.ylabel('Total time taken until an epoch in seconds')\n        K_range = range(1, len(self.times)+1)\n        plt.plot(*zip(*self.times))\n        plt.show()\n\n\n##### fit function #####\ndef fit_model(model,\n              prep,\n              epochs=5,\n              batch_size=16,\n              target_size=(256, 256),\n              number_race=15,\n              l_r = 0.002,\n              aug=False,\n              e_s=True,\n              crop=False,\n              vit=False\n             ):\n    \"\"\"\n    \n    model: model choosed\n    prep: preprocessing_input choosed for the model\n    epochs: number of epochs for training\n    batch_size: size of batch\n    target_size: size of input image (height,width)\n    number_race: number of dog races used for training (from 2 to 120)\n    l_r: algorithm used is Adam, with choosable learning rate\n    aug: augmented data\n    e_s: early stop based on validation loss delata value (0.001)\n    crop: train dataset with cropped images\n    vit: use Vision Transformers model (only usable with vit-keras models)\n    \n    \"\"\"\n    n_classes = classes[:number_race]\n        \n    print(\"Building model...\")\n    \n    ###### creating model ######\n    if vit:\n        base_model = mod(\n            image_size = target_size[0],\n            activation = 'softmax',\n            pretrained = True,\n            include_top = False,\n            pretrained_top = False,\n            classes = number_race)\n    else:\n        base_model = mod(weights='imagenet', include_top=False)\n\n    ###### making it not trainable ######\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    ###### adding top dense level ######\n    inp = Input(shape=target_size+(3,), name='input')\n    x = base_model(inp)\n\n    x = Flatten(name='flatten')(x)\n    predictions = Dense(number_race, activation='softmax')(x)\n\n    model = Model(inputs=inp, outputs=predictions)\n\n    ###### compiling model ######\n    optim = tf.keras.optimizers.Adam(learning_rate=l_r)\n    model.compile(loss=\"categorical_crossentropy\",\n                  optimizer=optim,\n                  metrics=[\"accuracy\"])\n    print('Model built and compiled')\n    \n    ###### adding callbacks ######\n    timetaken = timecallback()\n    \n    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n    \n    callbacks = [timetaken, lr_scheduler]\n    \n    if e_s:\n        early_stop = tf.keras.callbacks.EarlyStopping(\n                    monitor='val_loss', min_delta=0.001, patience=1, verbose=0,\n                    mode='auto', baseline=None, restore_best_weights=True\n                )\n        callbacks.append(early_stop)\n    \n    print(\"#\"*10,'Training ...',\"#\"*10)\n    \n    ##### selecting cropped or not cropped images #####\n    if crop:\n        print(\"#\"*5, 'Cropped images selected', '#'*5)\n        path = \"..\/input\/croped\/croped\/\"\n    else:\n        print(\"#\"*5, 'No Cropped images selected', '#'*5)\n        path = \"..\/input\/dogs-data\/output\/train\/\"\n        \n    \n    if not aug:\n        ###### fit  without augmentation ######\n        datagen = ImageDataGenerator(validation_split=0.15, preprocessing_function=prep)\n        model_info = model.fit(datagen.flow_from_directory(\n                                    directory=path,\n                                    classes=n_classes,\n                                    target_size=(256, 256),\n                                    color_mode=\"rgb\",\n                                    batch_size=batch_size,\n                                    class_mode=\"categorical\",\n                                    shuffle=True,\n                                    seed=42,\n                                    subset='training'\n                                ),\n                               epochs=epochs,\n                               batch_size=batch_size,\n                               verbose=1,\n                               validation_data=datagen.flow_from_directory(\n                                    directory=path,\n                                    classes=n_classes,\n                                    target_size=(256, 256),\n                                    color_mode=\"rgb\",\n                                    batch_size=batch_size,\n                                    class_mode=\"categorical\",\n                                    shuffle=True,\n                                    seed=42,\n                                    subset='validation'\n                                ),\n                               callbacks=callbacks)\n    else:\n        ###### augmentation then fit ######\n        augmented_datagen = ImageDataGenerator(\n             validation_split=0.05,\n             preprocessing_function=prep,\n             rotation_range=20,\n             width_shift_range=0.2,\n             height_shift_range=0.2,\n             shear_range=0.2,\n             zoom_range=0.2,\n             horizontal_flip=True,\n             fill_mode='nearest')\n        \n        model_info = model.fit(\n            augmented_datagen.flow_from_directory(\n                            directory=path,\n                            classes=n_classes,\n                            target_size=(256, 256),\n                            color_mode=\"rgb\",\n                            batch_size=batch_size,\n                            class_mode=\"categorical\",\n                            shuffle=True,\n                            seed=42,\n                            subset='training'\n                        ),\n            epochs=epochs,\n            batch_size=batch_size,\n            validation_data = augmented_datagen.flow_from_directory(\n                            directory=path,\n                            classes=n_classes,\n                            target_size=(256, 256),\n                            color_mode=\"rgb\",\n                            batch_size=batch_size,\n                            class_mode=\"categorical\",\n                            shuffle=True,\n                            seed=42,\n                            subset='validation'\n                        ),\n            verbose=1,\n            callbacks= callbacks)\n    print(\"#\"*10,'Training done',\"#\"*10)\n    \n    ###### evaluation ######\n    print(\"#\"*10,\"Evaluate on test data\",\"#\"*10)\n    datagen = ImageDataGenerator(preprocessing_function=prep)\n    test_datagen = datagen.flow_from_directory(\n                                    directory=\"..\/input\/dogs-data\/output\/test\/\",\n                                    classes=n_classes,\n                                    target_size=(256, 256),\n                                    color_mode=\"rgb\",\n                                    batch_size=batch_size,\n                                    class_mode=\"categorical\",\n                                    shuffle=True,\n                                    seed=42,\n                                    \n                                )\n    \n    results = model.evaluate(test_datagen)\n\n    print(\"test loss, test acc:\", results)\n    \n    ###### plotting learning curves ######\n    learning_curve(model_info)\n    \n    ###### adding to MODELS for compare graph ######\n    MODELS.append({\n        'name': mod.__name__,\n        'accuracy_score': results[1],\n        'time_per_epoch': round(timetaken.times[-1][1]\/len(model_info.epoch),2)\n    })\n    \n    \n    return model, model_info\n    \ndef learning_curve(model_info):\n    \"\"\"\n    plot the curves of accuracy on training dataset and validation dataset\n    \"\"\"\n    loss_curve = model_info.history[\"loss\"]\n    acc_curve = model_info.history[\"accuracy\"]\n\n    loss_val_curve = model_info.history.get(\"val_loss\")\n    acc_val_curve = model_info.history.get(\"val_accuracy\")\n    \n    K_range = range(1, len(model_info.epoch)+1)\n    \n    plt.plot(K_range, loss_curve, label=\"Train\")\n    if loss_val_curve:\n        plt.plot(K_range, loss_val_curve, label=\"Val\")\n    plt.xticks(K_range);\n    plt.legend(loc='upper left')\n    plt.title(\"Loss\")\n    plt.show()\n    plt.plot(K_range, acc_curve, label=\"Train\")\n    if acc_val_curve:\n        plt.plot(K_range, acc_val_curve, label=\"Val\")\n    plt.xticks(K_range)\n    plt.legend(loc='upper left')\n    plt.title(\"Accuracy\")\n    plt.show()","53f27814":"with tf.device('\/GPU:0'):\n    prep = inception_v3.preprocess_input\n    mod = InceptionV3\n    model, model_info = fit_model(mod, prep)\n    \n\nMODELS","00d62f9d":"img = load_img(f'..\/input\/dogs-data\/output\/test\/Pembroke\/n02113023_1136.jpg', target_size=(256,256))\n\n### to numpy array\nimg = img_to_array(img)  \nimg = img.astype(np.float32) \n\n### preprocessing image \nimg = prep(img)\npred = model.predict(np.array([img]))[0]\n\n### to img for imshow\nimg = array_to_img(img)\nplt.imshow(img)\n\nprint('pr\u00e9diction: ',\n      np.array(classes)[np.argsort(pred)[-2:]][::-1][0],\n      f'({round(pred[np.argsort(pred)[-2:]][::-1][0] *100,2)} %)',\n      ' ; ',\n      np.array(classes)[np.argsort(pred)[-2:]][::-1][1],\n      f'({round(pred[np.argsort(pred)[-2:]][::-1][1] *100,2)} %)',\n     )\n     ","be44682a":"with tf.device('\/GPU:0'):\n    prep = vgg16.preprocess_input\n    mod = VGG16\n    fit_model(mod, prep)\n    \n","548a0070":"with tf.device('\/GPU:0'):\n    prep = vgg19.preprocess_input\n    mod = VGG19\n    fit_model(mod, prep)\n\n","38ba19a8":"with tf.device('\/GPU:0'):\n    prep = resnet50.preprocess_input\n    mod = ResNet50\n    fit_model(mod, prep)\n\n","d9426ef1":"with tf.device('\/GPU:0'):\n    prep = resnet_v2.preprocess_input\n    mod = ResNet50V2\n    fit_model(mod, prep)\n    \n\n","8c125fcc":"with tf.device('\/GPU:0'):\n    prep = mobilenet_v3.preprocess_input\n    mod = MobileNetV3Large\n    fit_model(mod, prep)\n\n","ea4beda9":"with tf.device('\/GPU:0'):\n    prep = inception_resnet_v2.preprocess_input\n    mod = InceptionResNetV2\n    fit_model(mod, prep)\n    \n","d17f2f88":"with tf.device('\/GPU:0'):\n    prep = efficientnet.preprocess_input\n    mod = EfficientNetB7\n    fit_model(mod, prep)\n    \n","1a37376c":"df = pd.DataFrame(MODELS)\ndf","a8a12db5":"ax = df.plot(\n    kind= 'bar',\n    secondary_y= 'time_per_epoch',\n    rot= 90,\n    figsize=(10,5)\n)\nax.set_xticks(df.index, df.name)\nh1, l1 = ax.get_legend_handles_labels()\nh2, l2 = ax.right_ax.get_legend_handles_labels()\nhandles = h1+h2\nlabels = l1+l2\nax.legend(handles, labels, loc='lower left', ncol=2,\n       bbox_to_anchor=(0.25, -.575))\nax.set_ylabel('Accuracy', fontsize=10);\nax.right_ax.set_ylabel('time_per_epoch', fontsize=10);\nfor p in ax.patches:\n    ax.annotate(str(round(p.get_height(),3)), (p.get_x() * 1, p.get_height() * 1.015))\n\n","85ca413d":"MODELS = []\nwith tf.device('\/GPU:0'):\n    prep = inception_v3.preprocess_input\n    mod = InceptionV3\n    model, model_info = fit_model(mod, prep, crop=True)\n    \n\n","ca231998":"with tf.device('\/GPU:0'):\n    prep = vgg16.preprocess_input\n    mod = VGG16\n    model, model_info = fit_model(mod, prep, crop=True)\n    \n","22cd0b60":"with tf.device('\/GPU:0'):\n    prep = vgg19.preprocess_input\n    mod = VGG19\n    model, model_info = fit_model(mod, prep, crop=True)\n\n","a874f0ee":"with tf.device('\/GPU:0'):\n    prep = resnet50.preprocess_input\n    mod = ResNet50\n    model, model_info = fit_model(mod, prep, crop=True)\n\n","93873eff":"with tf.device('\/GPU:0'):\n    prep = resnet_v2.preprocess_input\n    mod = ResNet50V2\n    model, model_info = fit_model(mod, prep, crop=True)\n    \n\n","7bfedbb7":"with tf.device('\/GPU:0'):\n    prep = mobilenet_v3.preprocess_input\n    mod = MobileNetV3Large\n    model, model_info = fit_model(mod, prep, crop=True)\n\n","8c09e5b1":"with tf.device('\/GPU:0'):\n    prep = inception_resnet_v2.preprocess_input\n    mod = InceptionResNetV2\n    model, model_info = fit_model(mod, prep, crop=True)\n    \n","6f058869":"with tf.device('\/GPU:0'):\n    prep = efficientnet.preprocess_input\n    mod = EfficientNetB7\n    model, model_info = fit_model(mod, prep, crop=True)\n    \n","4b1bd71f":"df = pd.DataFrame(MODELS)\nax = df.plot(\n    kind= 'bar',\n    secondary_y= 'time_per_epoch',\n    rot= 90,\n    figsize=(10,5)\n)\nax.set_xticks(df.index, df.name)\nh1, l1 = ax.get_legend_handles_labels()\nh2, l2 = ax.right_ax.get_legend_handles_labels()\nhandles = h1+h2\nlabels = l1+l2\nax.legend(handles, labels, loc='lower left', ncol=2,\n       bbox_to_anchor=(0.25, -.575))\nax.set_ylabel('Accuracy', fontsize=10);\nax.right_ax.set_ylabel('time_per_epoch', fontsize=10);\nfor p in ax.patches:\n    ax.annotate(str(round(p.get_height(),3)), (p.get_x() * 1, p.get_height() * 1.015))\n\n","6cb7eaa4":"MODELS = []\nwith tf.device('\/GPU:0'):\n    prep = inception_v3.preprocess_input\n    mod = InceptionV3\n    model, model_info = fit_model(mod, prep, number_race=120, epochs=5)\n    \n\n","bd431952":"with tf.device('\/GPU:0'):\n    prep = mobilenet_v3.preprocess_input\n    mod = MobileNetV3Large\n    model, model_info = fit_model(mod, prep, number_race=120, epochs=15)\n\n","bace0d95":"with tf.device('\/GPU:0'):\n    prep = inception_resnet_v2.preprocess_input\n    mod = InceptionResNetV2\n    model, model_info = fit_model(mod, prep, number_race=120, epochs=15)\n    \n","ac94d5d6":"df = pd.DataFrame(MODELS)\nax = df.plot(\n    kind= 'bar',\n    secondary_y= 'time_per_epoch',\n    rot= 90,\n    figsize=(10,5)\n)\nax.set_xticks(df.index, df.name)\nh1, l1 = ax.get_legend_handles_labels()\nh2, l2 = ax.right_ax.get_legend_handles_labels()\nhandles = h1+h2\nlabels = l1+l2\nax.legend(handles, labels, loc='lower left', ncol=2,\n       bbox_to_anchor=(0.25, -.575))\nax.set_ylabel('Accuracy', fontsize=10);\nax.right_ax.set_ylabel('time_per_epoch', fontsize=10);\nfor p in ax.patches:\n    ax.annotate(str(round(p.get_height(),3)), (p.get_x() * 1, p.get_height() * 1.015))\n\n","f35c20e5":"MODELS = []\nwith tf.device('\/GPU:0'):\n    prep = inception_v3.preprocess_input\n    mod = InceptionV3\n    model, model_info = fit_model(mod, prep, number_race=120, crop=True, epochs=15)","7781cfa8":"with tf.device('\/GPU:0'):\n    prep = mobilenet_v3.preprocess_input\n    mod = MobileNetV3Large\n    model, model_info = fit_model(mod, prep, number_race=120, crop=True,epochs=15)\n\n","0f0f758d":"with tf.device('\/GPU:0'):\n    prep = inception_resnet_v2.preprocess_input\n    mod = InceptionResNetV2\n    model, model_info = fit_model(mod, prep, number_race=120, crop=True, aug=True, epochs=15)\n    \n","291c0163":"df = pd.DataFrame(MODELS)\nax = df.plot(\n    kind= 'bar',\n    secondary_y= 'time_per_epoch',\n    rot= 90,\n    figsize=(10,5)\n)\nax.set_xticks(df.index, df.name)\nh1, l1 = ax.get_legend_handles_labels()\nh2, l2 = ax.right_ax.get_legend_handles_labels()\nhandles = h1+h2\nlabels = l1+l2\nax.legend(handles, labels, loc='lower left', ncol=2,\n       bbox_to_anchor=(0.25, -.575))\nax.set_ylabel('Accuracy', fontsize=10);\nax.right_ax.set_ylabel('time_per_epoch', fontsize=10);\nfor p in ax.patches:\n    ax.annotate(str(round(p.get_height(),3)), (p.get_x() * 1, p.get_height() * 1.015))\n\n","cd9b4412":"MODELS = []\nwith tf.device('\/GPU:0'):\n    prep = vit.preprocess_inputs\n    mod = vit_b16\n    fit_model(mod,prep,vit=True)\n    \n","ee332550":"with tf.device('\/GPU:0'):\n    mod = vit_b32\n    fit_model(mod,prep,vit=True)\n    \n","23323d29":"with tf.device('\/GPU:0'):\n    mod = vit_l16\n    fit_model(mod,prep,vit=True)\n    \n","ff6b7381":"with tf.device('\/GPU:0'):\n    mod = vit_l32\n    fit_model(mod,prep,vit=True)\n    \n","202af186":"df = pd.DataFrame(MODELS)\nax = df.plot(\n    kind= 'bar',\n    secondary_y= 'time_per_epoch',\n    rot= 90,\n    figsize=(10,5)\n)\nax.set_xticks(df.index, df.name)\nh1, l1 = ax.get_legend_handles_labels()\nh2, l2 = ax.right_ax.get_legend_handles_labels()\nhandles = h1+h2\nlabels = l1+l2\nax.legend(handles, labels, loc='lower left', ncol=2,\n       bbox_to_anchor=(0.25, -.575))\nax.set_ylabel('Accuracy', fontsize=10);\nax.right_ax.set_ylabel('time_per_epoch', fontsize=10);\nfor p in ax.patches:\n    ax.annotate(str(round(p.get_height(),3)), (p.get_x() * 1, p.get_height() * 1.015))\n\n","b2f6b5a0":"MODELS=[]\nwith tf.device('\/GPU:0'):\n    prep = vit.preprocess_inputs\n    mod = vit_b16\n    fit_model(mod, prep, vit=True, crop=True)\n    \n","51699740":"with tf.device('\/GPU:0'):\n    mod = vit_b32\n    fit_model(mod,prep,vit=True, crop=True)\n    \n","2afca864":"with tf.device('\/GPU:0'):\n    mod = vit_l16\n    fit_model(mod,prep,vit=True, crop=True)\n    \n","6a2c6599":"with tf.device('\/GPU:0'):\n    mod = vit_l32\n    fit_model(mod,prep,vit=True, crop=True)\n    \n","c96cc6db":"df = pd.DataFrame(MODELS)\nax = df.plot(\n    kind= 'bar',\n    secondary_y= 'time_per_epoch',\n    rot= 90,\n    figsize=(10,5)\n)\nax.set_xticks(df.index, df.name)\nh1, l1 = ax.get_legend_handles_labels()\nh2, l2 = ax.right_ax.get_legend_handles_labels()\nhandles = h1+h2\nlabels = l1+l2\nax.legend(handles, labels, loc='lower left', ncol=2,\n       bbox_to_anchor=(0.25, -.575))\nax.set_ylabel('Accuracy', fontsize=10);\nax.right_ax.set_ylabel('time_per_epoch', fontsize=10);\nfor p in ax.patches:\n    ax.annotate(str(round(p.get_height(),3)), (p.get_x() * 1, p.get_height() * 1.015))\n","6ab91908":"with tf.device('\/GPU:0'):\n    mod = vit_l32\n    fit_model(mod,prep,vit=True, number_race=120, epochs=8)","75ca0ef8":"with tf.device('\/GPU:0'):\n    mod = vit_l32\n    fit_model(mod,prep,vit=True, number_race=120, crop=True)","359fda83":"with tf.device('\/GPU:0'):\n    mod = vit_l32\n    fit_model(mod,prep,vit=True, number_race=120, aug=True)","6a8aef10":"### Same with cropped images","9a84eca3":"### With data augmentation","33c38ea2":"### Testing ViT-L32 with Transfert Leraning on 120 races","ebd621c9":"## Transformers","7d463a2a":"#### InceptionResNetV2, MobileNetV3Large and InceptionV3 still are the best but less than without cropping","1d315e2d":"### Testing models with Transfert Learning on 15 races cropped images","ed2d5107":"#### Best one is no cropped InceptionResNetV2","0ca26d15":"### Callbacks and fit function","d20a17b7":"### Testing models for transfert learning on 120 races and cropped images","aa2c144d":"### Testing transformers with Transfert learning on 15 races","8b71354a":"#### Best one is ViT-L32","3e9e4d37":"#### Testing best models with Transfert learning on 120 races","4646269d":"### Testing models with Transfert Learning on 15 races","6326c2de":"### testing our model on 1 test image","d358a57c":"#### Getting classes","49151d76":"#### Les mod\u00e9les les plus efficaces sont InceptionResNetV2, MobileNetV3Large et InceptionV3","3e76c74a":"### Testing transformers with Transfert learning on 15 races and cropped images","0b04cabb":"#### Best score is 0.9268 on 120 races","4cf436b3":"#### ViT-L32 still is the best one"}}