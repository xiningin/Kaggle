{"cell_type":{"e0dffd97":"code","bf6bceaf":"code","f054dcc6":"code","9ef7e231":"code","91d72270":"code","40dd1497":"code","b5fea45f":"code","66f1df86":"code","9ddeb771":"code","cda62b5f":"code","bd8c3ee3":"code","448e1653":"code","2a8a1029":"code","5ded342c":"code","f667138a":"code","50f4c4d0":"code","c55b89fd":"code","72a98d0e":"code","ac3ec9eb":"code","59e5068e":"code","056cdc4e":"code","9b890a0c":"code","1bb12200":"code","45869b75":"code","7fb8a308":"code","b8b5c42f":"code","62b37ba0":"markdown","66c37f16":"markdown","50ce366d":"markdown","2330cc46":"markdown","4c0ddc57":"markdown","29d27d35":"markdown","ab68b306":"markdown","6399fa8f":"markdown","aaa486ef":"markdown","91a93f19":"markdown","35b6c1e0":"markdown","23be145a":"markdown","dfca74ac":"markdown","ebb04b30":"markdown","6e16fb7b":"markdown","92ba9f09":"markdown","fa769a39":"markdown","3732b8c3":"markdown","7bd5aee2":"markdown","ea9e8fb5":"markdown","230464e8":"markdown","c048c0dc":"markdown","6d268c0a":"markdown","bf88ed69":"markdown","66e592fb":"markdown","1b357b64":"markdown","29c77fa2":"markdown","5d738e66":"markdown","9e574762":"markdown","46eec661":"markdown"},"source":{"e0dffd97":"#!pip freeze","bf6bceaf":"import glob\nfrom tqdm import notebook","f054dcc6":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\n\nfrom PIL import Image\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt\n\nDIR_INPUT = '..\/input\/global-wheat-detection'\nDIR_TRAIN = f'{DIR_INPUT}\/train'\nDIR_TEST = f'{DIR_INPUT}\/test'\n\nDIR_WEIGHTS = '..\/input\/lasttry\/fasterrcnn_resnet50_fpn12nd.pth'\n\nWEIGHTS_FILE = f'{DIR_WEIGHTS}' ","9ef7e231":"test_df = pd.read_csv(f'{DIR_INPUT}\/sample_submission.csv')\n#test_df","91d72270":"class WheatTestDataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        #self.image_ids = dataframe['image_id'].unique()\n        self.image_ids=np.array([path.split('\/')[-1][:-4]for path in glob.glob(f'{image_dir}\/*.jpg')])\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        #records = self.df[self.df['image_id'] == image_id]\n\n        image = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n\n        if self.transforms:\n            sample = {\n                'image': image,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","40dd1497":"# Albumentations\ndef get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","b5fea45f":"# Albumentations\ndef get_test_transform():\n    return A.Compose([\n        # A.Resize(512, 512),\n        ToTensorV2(p=1.0)\n    ])\n","66f1df86":"# load a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)","9ddeb771":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nnum_classes = 2  # 1 class (wheat) + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Load the trained weights\nmodel.load_state_dict(torch.load(WEIGHTS_FILE))\nmodel.eval()\n\nx = model.to(device)","cda62b5f":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntest_dataset = WheatTestDataset(test_df, DIR_TEST, get_test_transform())\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)","bd8c3ee3":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)","448e1653":"detection_threshold = 0.5\nresults = []\n\ntestdf_psuedo = []\nfor images, image_ids in test_data_loader:\n\n    images = list(image.to(device) for image in images)\n    outputs = model(images)\n\n    for i, image in enumerate(images):\n\n        boxes = outputs[i]['boxes'].data.cpu().numpy()\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        \n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n        image_id = image_ids[i]\n        \n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n        for box in boxes:\n            #print(box)\n            result = {\n                'image_id': 'nvnn'+image_id,\n                'width': 1024,\n                'height': 1024,\n                'source': 'nvnn',\n                'x': box[0],\n                'y': box[1],\n                'w': box[2],\n                'h': box[3]\n            }\n            testdf_psuedo.append(result)\n            \n","2a8a1029":"test_df_pseudo = pd.DataFrame(testdf_psuedo, columns=['image_id', 'width', 'height', 'source', 'x', 'y', 'w', 'h'])\ntest_df_pseudo.head()","5ded342c":"train_df = pd.read_csv(f'{DIR_INPUT}\/train.csv')\ntrain_df['x'] = -1\ntrain_df['y'] = -1\ntrain_df['w'] = -1\ntrain_df['h'] = -1\n\ndef expand_bbox(x):\n    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n    if len(r) == 0:\n        r = [-1, -1, -1, -1]\n    return r\n\ntrain_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\ntrain_df.drop(columns=['bbox'], inplace=True)\ntrain_df['x'] = train_df['x'].astype(np.float)\ntrain_df['y'] = train_df['y'].astype(np.float)\ntrain_df['w'] = train_df['w'].astype(np.float)\ntrain_df['h'] = train_df['h'].astype(np.float)\n\ntrain_df.head()","f667138a":"image_ids = train_df['image_id'].unique()\nvalid_ids = image_ids[-665:]\ntrain_ids = image_ids #[:-665]\n\nvalid_df = train_df[train_df['image_id'].isin(valid_ids)]\n#train_df = train_df[train_df['image_id'].isin(train_ids)]","50f4c4d0":"frames = [train_df, test_df_pseudo]\n\ntrain_df = pd.concat(frames)\ntrain_df.tail()","c55b89fd":"class WheatDataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n        if 'nvnn' in image_id:\n            image_id = image_id[4:]\n            image = cv2.imread(f'{DIR_TEST}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        else:\n            image = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n\n        boxes = records[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n\n        # there is only one class\n        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        # target['masks'] = None\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    ","72a98d0e":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n        \n        \ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = WheatDataset(train_df, DIR_TRAIN, get_train_transform())\nvalid_dataset = WheatDataset(valid_df, DIR_TRAIN, get_valid_transform())\n\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","ac3ec9eb":"images, targets, image_ids = next(iter(train_data_loader))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nboxes = targets[2]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[2].permute(1,2,0).cpu().numpy()\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(sample)","59e5068e":"model.train()\nmodel.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=0.0001)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n#lr_scheduler = None\n\nnum_epochs =30\n\nloss_hist = Averager()\nitr = 1\n\nfor epoch in range(num_epochs):\n    loss_hist.reset()\n    \n    for images, targets, image_ids in train_data_loader :\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.long().to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n\n        itr += 1\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")\n","056cdc4e":"torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn3nd.pth')","9b890a0c":"model.eval()\ndetection_threshold=0.2\nresults=[]\nfor images, image_ids in test_data_loader:\n\n    images = list(image.to(device) for image in images)\n    outputs = model(images)\n\n    for i, image in enumerate(images):\n\n        boxes = outputs[i]['boxes'].data.cpu().numpy()\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        \n        boxes = boxes[scores >= detection_threshold].astype(np.int32).clip(min=0,max=1023)\n        scores = scores[scores >= detection_threshold]\n        image_id = image_ids[i]\n        \n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n        \n            \n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n\n        \n        results.append(result)\n","1bb12200":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.head()","45869b75":"sample = images[1].permute(1,2,0).cpu().numpy()\nboxes = outputs[1]['boxes'].data.cpu().numpy()\nscores = outputs[1]['scores'].data.cpu().numpy()\n\nboxes = boxes[scores >= detection_threshold].astype(np.int32)","7fb8a308":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 2)\n    \nax.set_axis_off()\nax.imshow(sample)","b8b5c42f":"test_df.to_csv('submission.csv', index=False)","62b37ba0":"\u6b63\u5219\u7684\u8bed\u53e5\u548c\u65b9\u6cd5\u771f\u662f\u770b\u4e86\u5fd8\u770b\u4e86\u5fd8\uff0c\u53ea\u597dcopy\u4e86","66c37f16":"\u5176\u5b9edef collate_fn(batch):\u8fd9\u4e00\u4e2a\u8bed\u53e5\u6211\u8fd8\u662f\u6709\u70b9\u6ca1\u6709\u7406\u89e3\uff0c\u867d\u7136\u6211\u53bb\u67e5\u770b\u4e86DataLoader\u7684\u53c2\u6570","50ce366d":"### \u7b2c\u4e00\u6b21\u53c2\u52a0\u6bd4\u8d5b\u843d\u4e0b\u5e37\u5e55\uff0c\u4f7f\u7528\u7684\u662fFasterRCNN50\uff0c\u56fe\u50cf\u589e\u5f3a\u4f7f\u7528\u7684\u662f\u67d0\u4f4d\u5927\u4f6c\u63d0\u4f9b\u7684notebook\uff0c\u4e0d\u7b97\u4f2a\u4ee3\u7801\u7684\u8bdd\u8bad\u7ec3\u4e8650epoch\u5de6\u53f3\uff0c\u4f46\u662f\u4ece\u6700\u540e\u7684\u7ed3\u679c\u6765\u770b\uff0c\u54ea\u6015\u518d\u589e\u52a0\u8bad\u7ec3\u7684\u8f6e\u6570\u4e5f\u5df2\u7ecf\u4e0d\u80fd\u591f\u5f97\u5230\u66f4\u597d\u7684LB\u4e86\u3002\n### \u89c2\u770b\u5176\u4ed6\u5927\u4f6c\u7684notebook\u548cdiscussion\uff0c\u9700\u8981\u5b66\u4e60\u7684\u6709\u51e0\u70b9\uff1a\n1. \u989d\u5916\u6570\u636e\u7684\u5bfb\u627e\u548c\u5bfc\u5165\n2. \u6df7\u5408\u6a21\u578b\u7684\u4f7f\u7528\uff08\u867d\u7136\u5e76\u4e0d\u662f\u5f88\u61c2\u8fd9\u4e00\u4e2a\u6982\u5ff5\uff09\u6216\u8005\u8bf4\u5e94\u8be5\u5c1d\u8bd5\u66f4\u591a\u7684\u65b9\u6cd5\uff0c\u5f53\u7136\u8fd9\u4e00\u6b21\u662f\u56e0\u4e3a\u52a0\u5165\u4e4b\u540e\u5269\u4f59\u7684\u65f6\u95f4\u672c\u6765\u5c31\u4e0d\u591a\u4e86\n3. EDA\u7684\u6267\u884c\uff08\u8fd9\u6b21\u5e76\u6ca1\u6709\u5f88\u597d\u7684\u89c2\u5bdf\u6570\u636e\u4e0e\u56fe\u50cf\u672c\u8eab\uff09\n4. \u6574\u4f53\u6846\u67b6copy\uff0c\u6574\u5408\u4e86\u5176\u4ed6\u7684\u56fe\u50cf\u589e\u5f3a\u7684\u65b9\u5f0f\uff0c\u4f46\u662f\u5176\u4e2d\u90e8\u5206\u65b9\u6cd5\u5e76\u6ca1\u6709\u5f88\u597d\u7684\u7406\u89e3\uff0c\u9700\u8981\u540e\u7eed\u4ed4\u7ec6\u3002\u53e6\u5916\u5e94\u5f53\u6df1\u5165\u5230\u56fe\u50cf\u589e\u5f3a\u7684\u6e90\u4ee3\u7801\u5f53\u4e2d  \n\u4e0b\u9762\u5148\u5c06notebook\u4e2d\u4e0d\u61c2\u7684\u90e8\u5206\u6807\u51fa\uff0c\u7559\u5f85\u540e\u7eed\u89e3\u51b3\n\u4e0d\u8fc7\u597d\u6b79\u4e5f\u7b97\u5f80\u5165\u95e8\u66f4\u8fd1\u4e86\u4e00\u6b65\uff0c\u53e6\u5916\uff0c\u975e\u5e38\u611f\u8c22[Jacob](https:\/\/www.kaggle.com\/creatrol)\u5927\u4f6c [gzYou](https:\/\/www.kaggle.com\/guizengyou)\u5927\u4f6c\u4e0d\u538c\u5176\u70e6\u7684\u56de\u590d\u548c\u89e3\u7591\uff0c\u867d\u7136\u95ee\u7684\u95ee\u9898\u6709\u4e00\u70b9\u70b9(to be honest a lot)\u611a\u8822\u3002\n\u975e\u5e38\u559c\u6b22\u8fd9\u4e00\u4e2a\u6c1b\u56f4\uff0c\u5e0c\u671b\u80fd\u591f\u5b8c\u6210\u66f4\u591a\u7684\u6bd4\u8d5b\u5b66\u4e60\u5230\u66f4\u591a\u65b0\u7684\u4e1c\u897f\u3002","2330cc46":"In fact\uff0cWhen I run the code in my own computer,I add tqdm which I think let the running time more interesting.","4c0ddc57":"Although this step is a simple model import, I have to admit that for in_ I still have some doubts about the import of feature.  \nI have to take time to read the document of Pytorch.","29d27d35":"I need to pay attention to the records of parameters version which confused me this time.","ab68b306":"# **final Inference and submission","6399fa8f":"\u5176\u5b9e\u6211\u8fd9\u91cc\u5f88\u60f3\u5c1d\u8bd5\u4e00\u4e0b\uff0c\u4e0d\u540c\u7684detection_threshold\u4f1a\u5e26\u6765\u7684\u53d8\u5316\uff0c\u56e0\u6b64\u5148\u7801\u5728\u8fd9\u91cc\u3002\n\u53e6\u4e00\u4e2a\u5c31\u662f\u5f00\u59cb\u7684\u65f6\u5019\u6211\u8fd8\u6709\u7591\u95ee\u5c31\u662f\u4e3a\u4ec0\u4e48\u8fd9\u91cc\u7684\u6570\u636e\u8fd8\u8981\u5bfc\u5165\u5230cpu\uff0c\u540e\u6765\u624d\u77e5\u9053\u8981\u8fd0\u7b97\u7684\u8bdd\u53ea\u80fd\u8f6c\u6362\uff0c\u8fd9\u5927\u6982\u5c31\u662f\u57fa\u7840\u4e0d\u624e\u5b9e\u5427\u3002","aaa486ef":"The introduction of residual network pre trained by coco and training on this basis should be regarded as migration training.   \nHowever, I have no good understanding of the principle of migration training","91a93f19":"\u867d\u7136\u8fd9\u4e00\u6b65\u5c31\u662f\u7b80\u5355\u7684\u6a21\u578b\u5bfc\u5165\uff0c\u4f46\u662f\u4e0d\u5f97\u4e0d\u627f\u8ba4\uff0c\u5bf9\u4e8ein_feature\u7684\u5bfc\u5165\u8fd9\u4e00\u53e5\u6211\u8fd8\u662f\u6709\u70b9\u7591\u95ee\u7684\uff0c\u540e\u9762\u8fd8\u5f97\u62bd\u7a7a\u53bb\u770bPytorch\u7684\u6587\u6863\u3002","35b6c1e0":"# **Show a sample**","23be145a":"\u5bfc\u5165\u7ecf\u8fc7COCO\u9884\u8bad\u7ec3\u7684\u6b8b\u5dee\u7f51\u7edc\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e94\u8be5\u7b97\u662f\u8fc1\u79fb\u8bad\u7ec3\uff0c\u4e0d\u8fc7\u6211\u76ee\u524d\u5e76\u6ca1\u6709\u5f88\u597d\u7684\u4e86\u89e3\u8fc7\u8fc1\u79fb\u8bad\u7ec3\u7684\u539f\u7406","dfca74ac":"\u5bf9\u4e8e\u53c2\u6570\u4fdd\u5b58\u7684\u7248\u672c\u4e0b\u6b21\u4e00\u5b9a\u5f97\u505a\u597d\u8bb0\u5f55\uff0c\uff0c\u6211\u51e0\u6b21\u8bad\u7ec3\u4e4b\u540e\uff0c\uff0c\u5c31\u4e71\u5957\u4e86\u3002","ebb04b30":"In fact, I'd like to try different detection methods here_ Threshold will bring changes, so code here first.  \nThe other is that at the beginning, I still had a question about why the data here has to be imported into the CPU.   \nLater, I learned that if you want to calculate, you can only convert it. This is probably because my foundation is not solid.","6e16fb7b":"\u8fd9\u91cc\u6709\u4e00\u4e2a\u5c0f\u5c0f\u7684\u7591\u95ee\u5c31\u662f\u5728\u6570\u636e\u5bfc\u5165\u65f6\u5df2\u7ecf\u505a\u4e86\u5747\u4e00\u5316\uff0c\u5373\/255\u4e86\uff0c\u8fd9\u91cc\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5176\u4ed6\u64cd\u4f5c\u4fbf\u8fdb\u884cdisplay\u4e86\uff0c\u662f\u4e0d\u662fcv\u672c\u8eab\u81ea\u5e26\u7684\u5bf9\u5e94\u7684\u8865\u5168\u65b9\u6cd5","92ba9f09":"# # **Retrain model with pseudo labels","fa769a39":"# # ****retrain Faster RCNN****","3732b8c3":"The regular statements and methods are really forgotten, so I have to copy them","7bd5aee2":"Actually, def collate_ FN (batch): I still don't understand this statement, although I went to check the parameters of dataloader","ea9e8fb5":"~~At present, there is no good data enhancement method, and the method used is very simple flip~~  \nLater, data enhancement was used, but it was also copy. Although it took me a lot of time to debug the format problem, what I wanted to do was to integrate all the methods using the compose of Al library.   \nAfter all, it provided a way to add custom functions, but at present, it was a little difficult,But I see a notebook to talk about masico, and it's enough to cycle about ten rounds.   \nI seem to have trained a little bit more.\nThe following is an explanation of the parameters copied from the Al document:  \ntransforms (list) \u2013 list of transformations to compose.  \nbbox_ params (BboxParams) \u2013 Parameters for bounding boxes transforms","230464e8":"# # **Wheat dataset for Training**","c048c0dc":"# # **Detection and make Pseudo labels for test dataset**","6d268c0a":"\u52a0tqdm\u5305\uff0c\uff0c\u8fd9\u6837\u7684\u8bdd\u8bad\u7ec3\u7684\u8fc7\u7a0b\u66f4\u8d4f\u5fc3\u60a6\u76ee","bf88ed69":"After Wheat competition, I'd like to record something incluing the problems and the inspirations.  \nmodel\uff1aFasterRCNN50(after COCO pretrain)  \nepoch:50\nexternal way:Mosaic and Pseudo label\nThe question is in the main body and the inspiration is below:  \n1. Search and import of additional data\n2. The use of the hybrid model (although not very familiar with this concept) or should try more methods, of course, this time because there is not much time left after joining\n3. EDA implementation (there is no good observation data and image itself this time)\n4. The whole framework copy integrates other image enhancement methods, but some of them are not well understood and need to be followed up carefully. In addition, we should go deep into the source code of image enhancement\n\nNow I'll mark the part that I don't understand in the notebook, and I'll leave it for later solution. But somehow, it's a step closer to the beginning. Thank you very much for your untiring reply and answering questions, [Jacob](https:\/\/www.kaggle.com\/creatrol) and [gzYou](https:\/\/www.kaggle.com\/guizengyou)\uff0calthough the question I asked is a little bit stupid.   \nI like Kaggle\u2019s atmosphere very much. I hope I can finish more competitions and learn more new things.\n","66e592fb":"1. Read the image name from the directory instead of from the CSV. In this way, you can dock the hidden test in submit and avoid submission errors\n2. In most cases, the image format read by CV library needs to use RGB conversion. Compared with other image reading libraries, the speed of CV is faster, which makes it widely used in large-scale image reading of deep learning. Of course, there is an insignificant defect of converting RGB format\n3. For beginners like us, what we need to pay attention to is the change of format in the process of image import and transformation, such as the conversion between pandas, numpy, tensor, and of course, tuple. According to my shallow knowledge, tuple is safer because it can't be changed","1b357b64":"1.\u4ece\u76ee\u5f55\u4e2d\u8bfb\u53d6\u56fe\u7247\u540d\u79f0\u800c\u4e0d\u662f\u4ececsv\u4e2d\u8bfb\u53d6\uff0c\u8fd9\u6837\u5c31\u80fd\u591f\u5bf9\u63a5\u5728submit\u65f6\u7684\u9690\u85cftest\uff0c\u907f\u514d\u51fa\u73b0\u63d0\u4ea4\u9519\u8bef  \n2.cv\u5e93\u8bfb\u53d6\u7684\u56fe\u7247\u683c\u5f0f\u5728\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\u90fd\u9700\u8981\u4f7f\u7528RGB\u8f6c\u6362\uff0c\u76f8\u5bf9\u4e8e\u5176\u4ed6\u56fe\u7247\u8bfb\u53d6\u5e93\u800c\u8a00\uff0ccv\u7684\u901f\u5ea6\u66f4\u5feb\u901f\uff0c\u8fd9\u4f7f\u5f97\u5b83\u5728\u6df1\u5ea6\u5b66\u4e60\u5927\u89c4\u6a21\u56fe\u7247\u8bfb\u53d6\u4e2d\u5e94\u7528\u975e\u5e38\u5e7f\u6cdb\uff0c\u5f53\u7136\uff0c\u5b58\u5728\u9700\u8981\u8f6c\u6362RGB\u683c\u5f0f\u8fd9\u4e00\u4e2a\u65e0\u5173\u7d27\u8981\u7684\u7f3a\u9677  \n3.\u5bf9\u4e8e\u6211\u4eec\u8fd9\u6837\u7684\u521d\u5b66\u8005\u800c\u8a00\uff0c\u9700\u8981\u6ce8\u610f\u7684\u53cd\u800c\u662f\u56fe\u50cf\u5bfc\u5165\u8f6c\u5316\u8fc7\u7a0b\u4e2d\u683c\u5f0f\u7684\u53d8\u5316\uff0c\u5982pandas\u3001numpy\u3001tensor\u51e0\u4e2a\u6570\u636e\u683c\u5f0f\u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u5f53\u7136\u8fd8\u6709tuple\uff0c\u6839\u636e\u6211\u6d45\u8584\u7684\u89c1\u8bc6\uff0ctuple\u56e0\u4e3a\u4e0d\u80fd\u66f4\u6539\u66f4\u52a0\u5b89\u5168","29c77fa2":"~~\u76ee\u524d\u5e76\u6ca1\u6709\u5f88\u597d\u7684\u4f7f\u7528\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u4f7f\u7528\u7684\u624b\u6bb5\u662f\u975e\u5e38\u7b80\u6613\u7684\u7ffb\u8f6c~~\n\u540e\u6765\u8fd8\u662f\u4f7f\u7528\u4e86\u6570\u636e\u589e\u5f3a\uff0c\u4f46\u662f\u4e5f\u662fcopy\u7684\uff0c\u867d\u7136\u4e5f\u82b1\u4e86\u6211\u4e0d\u5c11\u65f6\u95f4\u53bb\u8c03\u8bd5\u683c\u5f0f\u7684\u95ee\u9898\uff0c\u5176\u5b9e\u60f3\u505a\u7684\u662f\u5c06\u6240\u6709\u65b9\u5f0f\u5168\u90e8\u4f7f\u7528AL\u5e93\u7684compose\u8fdb\u884c\u6574\u5408\uff0c\u6bd5\u7adf\u5176\u63d0\u4f9b\u4e86\u52a0\u5165\u81ea\u5b9a\u4e49\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u4e0d\u8fc7\u76ee\u524d\u6709\u70b9\u529b\u6240\u4e0d\u902e\uff0c\n\u4f46\u662f\u6211\u770b\u4e00\u4e2anotebook\u8bb2masico\u7684\u65b9\u5f0f\u5faa\u73af\u5341\u8f6e\u5de6\u53f3\u5c31\u6ee1\u8db3\u4e86\uff0c\u6211\u4f3c\u4e4e\u8bad\u7ec3\u7684\u6709\u70b9\u591a\u4e86\u3002\n\u4e0b\u9762\u662f\u4eceAL\u6587\u6863\u4e2dcopy\u8fc7\u6765\u7684\u53c2\u6570\u89e3\u91ca\uff1a\ntransforms (list) \u2013 list of transformations to compose.  \nbbox_params (BboxParams) \u2013 Parameters for bounding boxes transforms","5d738e66":"There is a small question here, that is, \/ 255 has been homogenized when data is imported. Display is performed without any other operation.  \nIs it the corresponding completion method of CV itself\uff1f","9e574762":"To tell you the truth, I've looked over and over for this data import function many times, but I always think it's almost meaningless,  \nsuch as target ['boxes']= torch.stack (tuple(map( torch.tensor , zip (* sample ['bboxes']))). Permute (1, 0)   \nThis several conversions will make me confused. I wonder if there is a big guy who can solve it.  \nThere is also the area calculation, I always feel a little strange,is it necessary\uff1f","46eec661":"\u8bf4\u5b9e\u8bdd\uff0c\u8fd9\u4e2a\u6570\u636e\u5bfc\u5165\u51fd\u6570\u6211\u5df2\u7ecf\u7ffb\u6765\u8986\u53bb\u770b\u6765\u5f88\u591a\u904d\u4e86\uff0c\u4f46\u662f\u603b\u89c9\u5f97\u8fd8\u5dee\u70b9\u610f\u601d\uff0c\u6bd4\u5982target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\u8fd9\u4e00\u53e5\uff0c\uff0c\u8fd9\u4e2a\u51e0\u91cd\u8f6c\u6362\u90fd\u8981\u628a\u6211\u641e\u6655\u4e86\uff0c\uff0c\u4e0d\u77e5\u9053\u6709\u6ca1\u6709\u5927\u4f6c\u53ef\u4ee5\u89e3\u7b54\u4e00\u4e0b\n\u8fd8\u6709\u5c31\u662f\u8fd9\u4e2a\u9762\u79ef\u8ba1\u7b97\u6211\u603b\u611f\u89c9\u6709\u90a3\u4e48\u4e00\u70b9\u602a"}}