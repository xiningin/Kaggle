{"cell_type":{"b3e228d6":"code","e1b7ffb4":"code","543775e0":"code","15145d90":"code","81e15efd":"code","3e3aa4ee":"code","b810ba40":"code","5ae0b0a7":"code","751ea530":"code","364014fa":"code","b06a39ef":"code","fd0cd9b7":"code","840339ca":"code","b7946773":"code","e2ad5b30":"code","1e181a58":"code","fea2aa5b":"code","b1586f96":"code","ec8e5bf1":"code","e0c2ca0d":"code","98eda84b":"code","7063e42a":"code","fd2f87db":"code","ed346953":"code","522e56cc":"code","036d3613":"code","5886c371":"code","d6140b74":"code","a9684e48":"code","de16c1f1":"code","a11866a0":"code","36acc513":"code","84c70569":"code","ed31e4a0":"code","66500d70":"code","7eb5aeb8":"code","07d84ba4":"code","28328d42":"code","959ccc01":"code","ba83546c":"markdown"},"source":{"b3e228d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e1b7ffb4":"#1.2 Keras libraries\nfrom keras.layers import Input, Dense\nfrom keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten,Embedding, GRU\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n","543775e0":"from keras.utils import  plot_model\n\n# 1.4 sklearn libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n# 1.4 For model plotting\nimport matplotlib.pyplot as plt\nimport pydot\nfrom skimage import io\n\n# 1.5 Misc\nimport os\nimport time\n","15145d90":"\n# 2.2\ndata = pd.read_csv(\"..\/input\/mimic3d.csv\",\n\t               compression='infer',\n                   encoding=\"ISO-8859-1\"      # 'utf-8' gives error, hence the choice\n                  )\n","81e15efd":"data.head(3)","3e3aa4ee":"\n\n         \ndata.columns.values   # Our target column is LOSdays\n","b810ba40":"data.tail(3)","5ae0b0a7":"data.dtypes  ","751ea530":"data.drop(['hadm_id'], axis = 'columns' , inplace = True)\n\n","364014fa":"\n# 2.5 Any missing value?\ndata.isnull().values.sum()        # 10611\n","b06a39ef":"data.columns[data.isnull().sum()  > 0]    # Three: Index(['AdmitDiagnosis', 'religion', 'marital_status'], dtype='object')\n","fd0cd9b7":"data.AdmitDiagnosis = data.AdmitDiagnosis.fillna(\"missing\")\ndata.religion = data.religion.fillna(\"missing\")\ndata.marital_status = data.marital_status.fillna(\"missing\")\ndata.isnull().values.sum()        # 0\n","840339ca":"dtrain,  dtest = train_test_split(data, test_size=0.33)\n\n","b7946773":"# 4. Which columns are 'object'\nobj_columns = data.select_dtypes(include = ['object']).columns.values\nobj_columns\n","e2ad5b30":"# 4.1 Which columns have numeric data\nnum = data.select_dtypes(include = ['int64', 'float64']).columns.values\nnum\n","1e181a58":"\n\n# 4.2 Among object, columns, let us check levels of each\n#     column\n\nfor i in obj_columns:\n\tprint(i,len(data[i].value_counts()))\n","fea2aa5b":"\nnum = ['age', 'NumCallouts', 'NumDiagnosis', 'NumProcs',\n       'NumCPTevents', 'NumInput', 'NumLabs', 'NumMicroLabs', 'NumNotes',\n       'NumOutput', 'NumRx', 'NumProcEvents', 'NumTransfers',\n       'NumChartEvents', 'TotalNumInteract']\n","b1586f96":"\n\n# 4.4 Final seven obj_columns for One Hot Encoding\nobj_cols = [\"gender\", \"admit_type\", \"admit_location\", \"insurance\" ,\"marital_status\", 'ExpiredHospital', 'LOSgroupNum']\nohe = OneHotEncoder()\n# 4.4.1 Traing on dtrain\nohe = ohe.fit(dtrain[obj_cols])\n# 4.4.2 Transform train (dtrain) and test (dtest) data\ndtrain_ohe = ohe.transform(dtrain[obj_cols])\ndtest_ohe = ohe.transform(dtest[obj_cols])\n# 4.4.3\ndtrain_ohe.shape       # (39513, 34)\ndtest_ohe.shape        # (19463, 34)\n","ec8e5bf1":"\n\n# 5.0 Label encode relegion and ethnicity\n# 5.1 First 'religion'\nle = LabelEncoder()\nle.fit(dtrain[\"religion\"])\ndtrain[\"re\"] = le.transform(dtrain['religion'])    # Create new column in dtrain\ndtest[\"re\"] = le.transform(dtest['religion'])      #   and in dtest\n","e0c2ca0d":"\n# 5.2 Now 'ethnicity'\nle = LabelEncoder()\nle.fit(dtrain[\"ethnicity\"])\ndtrain[\"eth\"]= le.transform(dtrain['ethnicity'])   # Create new column in dtrain\ndtest[\"eth\"]= le.transform(dtest['ethnicity'])     #   and in dtest\n","98eda84b":"# 6. Finally transform two obj_columns for tokenization\nte_ad = Tokenizer()\n# 6.1 Train tokenizer on train data ie 'dtrain'\nte_ad.fit_on_texts(data.AdmitDiagnosis.values)\n# 6.2 Transform both dtrain and dtest and create new columns\ndtrain[\"ad\"] = te_ad.texts_to_sequences(dtrain.AdmitDiagnosis)\ndtest[\"ad\"] = te_ad.texts_to_sequences(dtest.AdmitDiagnosis)\n\ndtrain.head(3)\ndtest.head(3)\n","7063e42a":"# 6.3 Similarly for column: AdmitProcedure\nte_ap = Tokenizer(oov_token='<unk>')\nte_ap.fit_on_texts(data.AdmitProcedure.values)\ndtrain[\"ap\"] = te_ap.texts_to_sequences(dtrain.AdmitProcedure)\ndtest[\"ap\"] = te_ap.texts_to_sequences(dtest.AdmitProcedure)\n\ndtrain.head(3)\ndtest.head(3)\n","fd2f87db":"\n# 7. Standardize numerical data\nse = StandardScaler()\n# 7.1 Train om dtrain\nse.fit(dtrain.loc[:,num])\n# 7.2 Then transform both dtrain and dtest\ndtrain[num] = se.transform(dtrain[num])\ndtest[num] = se.transform(dtest[num])\ndtest.loc[:,num].head(3)\n","ed346953":"\n# 8. Get max length of the sequences\n#    in dtrain[\"ad\"], dtest[\"ad\"]\nmaxlen_ad = 0\nfor i in dtrain[\"ad\"]:\n\tif maxlen_ad < len(i):\n\t\tmaxlen_ad = len(i)\n\nfor i in dtest[\"ad\"]:\n\tif maxlen_ad < len(i):\n\t\tmaxlen_ad = len(i)\n\nmaxlen_ad     # 24\n","522e56cc":"# 8.1 Get max length of the sequences\n#    in dtrain[\"ap\"], dtest[\"ap\"]\n\nmaxlen_ap = 0\nfor i in dtrain[\"ap\"]:\n\tif maxlen_ap < len(i):\n\t\tmaxlen_ap = len(i)\n\nmaxlen_ap      # 7\n\nfor i in dtest[\"ap\"]:\n\tif maxlen_ap < len(i):\n\t\tmaxlen_ap = len(i)\n\nmaxlen_ap     # 7\n","036d3613":"\n# 9. Get max vocabulary size ie value of highest\n#    integer in dtrain[\"ad\"] and in dtest[\"ad\"]\n\none = np.max([np.max(i) for i in dtrain[\"ad\"].tolist() ])\ntwo = np.max([np.max(i) for i in dtest[\"ad\"].tolist() ])\nMAX_VOCAB_AD = np.max([one,two])\n","5886c371":"# 9.1 Get max vocabulary size ie value of highest\n#     integer in dtrain[\"ap\"] and in dtest[\"ap\"]\n","d6140b74":"\none = np.max([np.max(i) for i in dtrain[\"ap\"].tolist() ])\ntwo = np.max([np.max(i) for i in dtest[\"ap\"].tolist() ])\nMAX_VOCAB_AP = np.max([one,two])\n","a9684e48":"\n# 9.2\nMAX_VOCAB_RE = len(dtrain.religion.value_counts())\nMAX_VOCAB_ETH = len(dtrain.ethnicity.value_counts())\n","de16c1f1":"# 10. Let us put our data in a dictionary form\n#     Required when we have multiple inputs\n#     to Deep Neural network. Each Input layer\n#     should also have the corresponding 'key'\n#     name\n\n# 10.1 Training data\nXtr = {\n\t\"num\" : dtrain[num].values,          # Note the name 'num'\n\t\"ohe\" : dtrain_ohe.toarray(),        # Note the name 'ohe'\n\t\"re\"  : dtrain[\"re\"].values,\n\t\"eth\" : dtrain[\"eth\"].values,\n\t\"ad\"  : pad_sequences(dtrain.ad, maxlen=maxlen_ad),\n\t\"ap\"  : pad_sequences(dtrain.ap, maxlen=maxlen_ap )\n      }\n","a11866a0":"# 10.2 Test data\nXte = {\n\t\"num\" : dtest[num].values,\n\t\"ohe\" : dtest_ohe.toarray(),\n\t\"re\"  : dtest[\"re\"].values,\n\t\"eth\" : dtest[\"eth\"].values,\n\t\"ad\"  : pad_sequences(dtest.ad, maxlen=maxlen_ad ),\n\t\"ap\"  : pad_sequences(dtest.ap, maxlen=maxlen_ap )\n      }\n","36acc513":"\n# 10.3 Just check shapes\nXtr[\"num\"].shape         # (39513, 15)\nXtr[\"ohe\"].shape         # (39513, 34)\nXtr[\"ad\"].shape          # (39513, 24)\nXtr[\"ap\"].shape          # (39513, 7)\nXtr[\"re\"].shape          # (39513,)  1D\nXtr[\"eth\"].shape         # (39513,)  1D\n","84c70569":"# 11. Design a simple model now\n\ndr_level = 0.1\n","ed31e4a0":"# 11.1\nnum = Input(\n                      shape= (Xtr[\"num\"].shape[1], ),\n\t\t\t\t\t  name = \"num\"            # Name 'num' should be a key in the dictionary for numpy array input\n\t\t\t\t\t                          #    That is, this name should be the same as that of key in the dictionary\n\t\t\t\t\t  )\n\n# 11.2\nohe =   Input(\n                      shape= (Xtr[\"ohe\"].shape[1], ),\n\t\t\t\t\t  name = \"ohe\"\n\t\t\t\t\t  )\n\n# 11.3\nre =   Input(\n                      shape= [1],  # 1D shape or one feature\n\t\t\t\t\t  name = \"re\"\n\t\t\t\t\t  )\n# 11.4\neth =   Input(\n                      shape= [1],  # 1D shape or one feature\n\t\t\t\t\t  name = \"eth\"\n\t\t\t\t\t  )\n# 11.5\nad =   Input(\n                      shape= (Xtr[\"ad\"].shape[1], ),\n\t\t\t\t\t  name = \"ad\"\n\t\t\t\t\t  )\n# 11.6\nap =   Input(\n                      shape= (Xtr[\"ap\"].shape[1],),\n\t\t\t\t\t  name = \"ap\"\n\t\t\t\t\t  )\n","66500d70":"\n# 12. Embedding layers for each of the two of the columns with sequence data\n#     Why add 1 to vocabulary?\n#     See: https:\/\/stackoverflow.com\/questions\/52968865\/invalidargumenterror-indices127-7-43-is-not-in-0-43-in-keras-r\n\nemb_ad  =      Embedding(MAX_VOCAB_AD+ 1 ,      32  )(ad )\nemb_ap  =      Embedding(MAX_VOCAB_AP+ 1 ,      32  )(ap)\n# 12.1 Embedding layers for the two categorical variables\nemb_re  =      Embedding(MAX_VOCAB_RE+ 1 ,      32  )(re)\nemb_eth =      Embedding(MAX_VOCAB_ETH+ 1 ,      32  )(eth)\n\n# 12.2 RNN layers for sequences\nrnn_ad = GRU(16) (emb_ad)          # Output of GRU is a vector of size 8\nrnn_ap = GRU(16) (emb_ap)\n\n\n\n","7eb5aeb8":"# 12.3 Interim model summary.\n#      For 'output' we have all the existing (unterminated) outputs\nmodel = Model([num, ohe, re, eth, ad,ap], [rnn_ad, rnn_ap, emb_re, emb_eth, num, ohe])\nmodel.summary()\n","07d84ba4":"# 12.4 Concatenate all outputs\nclass_l = concatenate([\n                      rnn_ad,        # GRU output is already 1D\n                      rnn_ap,\n                      \n                      num,                # 1D output. No need to flatten. See model summary\n\t\t\t\t\t  ohe,           # 1D output\n\t\t\t\t\t  Flatten()(emb_re),   # Why flatten? See model summary above\n\t\t\t\t\t  Flatten()(emb_eth)\n                      ]\n                     )\n\n\n# 12.5 Add classification layer\nclass_l = Dense(64) (class_l)\nclass_l = Dropout(0.1)(class_l)\nclass_l = Dense(32) (class_l)\nclass_l = Dropout(0.1) (class_l)\n\n# 12.6 Output neuron. Activation is linear\n#      as our output is continous\noutput = Dense(1, activation=\"linear\") (class_l)\n\n# 12.7 Formulate Model now\nmodel = Model(\n              inputs= [num, ohe, re, eth, ad, ap],\n              outputs= output\n             )\n\n# 12.8\nmodel.summary()\n\n# 12.9 Model plot uisng keras plot_model()\nplt.figure(figsize = (14,14))\nplot_model(model, to_file = \"model.png\")\nio.imshow(\"model.png\")\n","28328d42":"# 13. Compile model\nmodel.compile(loss=\"mse\",\n              optimizer=\"adam\",\n              metrics=[\"mae\"]\n\t\t\t  )","959ccc01":"# 13.1\nBATCH_SIZE = 5000\nepochs = 20\n\n# 13.2\nstart = time.time()\nhistory= model.fit(Xtr,\n                   dtrain.LOSdays,\n                   epochs=epochs,\n                   batch_size=BATCH_SIZE,\n\t\t\t\t   validation_data=(Xte, dtest.LOSdays),\n\t\t\t\t   verbose = 1\n                  )\nend = time.time()\nprint((end-start)\/60)\n","ba83546c":"# Adding addition RNN layers\nrnn_re = GRU(16) (emb_re)\nrnn_eth = GRU(16) (emb_eth)\n\n"}}