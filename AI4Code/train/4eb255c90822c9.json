{"cell_type":{"a0afb2b2":"code","78645ef2":"code","f600aa97":"code","aad870cc":"code","a0418ade":"code","dad90c2d":"code","7879dac3":"code","711ed1b7":"code","ba5e83b2":"code","f2f2879c":"code","0b520940":"code","a3ff4ee4":"code","e5bf84ee":"code","3f2060fd":"code","ebcc52c2":"code","3ec0e602":"code","e8f72a83":"code","da9ee2f3":"code","8de114fb":"code","d4cb0f50":"code","6027299d":"code","c3333e3d":"code","c3189048":"code","f9cd3c52":"code","cf4ef7f5":"code","98f45196":"code","ed62cbe3":"code","3b58aa48":"code","6b6a0248":"code","a0ed65c5":"code","c9796d38":"code","79ed074b":"code","8be876a6":"code","4fac813e":"code","65548e19":"code","66a8a0cf":"code","d6355715":"code","7d08a3f7":"code","ba70fe43":"code","466df9be":"code","4de9f634":"code","96c3ce39":"code","559ba331":"code","350cd6ad":"code","73f64ce6":"code","86e69a72":"code","9afb7026":"code","b0599c03":"code","91653af3":"code","c2dbeb37":"markdown","39e2a1be":"markdown","7e670c0e":"markdown","6b029135":"markdown","ee888c79":"markdown","109dd7bb":"markdown","9cbc70e9":"markdown","6a1a6636":"markdown","56801861":"markdown","2797b5f4":"markdown","2c9f1610":"markdown","8aedd4a3":"markdown","51ad8746":"markdown","f70389f5":"markdown","36bfa1ad":"markdown","c714281c":"markdown","ea9606ef":"markdown","8f0b45a9":"markdown","7bb50cdc":"markdown"},"source":{"a0afb2b2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","78645ef2":"from matplotlib import pyplot as plt","f600aa97":"src_file = '..\/input\/online-retail-data-set-from-uci-ml-repo\/Online Retail.xlsx'","aad870cc":"def intconverter(x):\n    if not np.isnan(x):\n        return np.int32(x)\n    else:\n        return np.nan\n    \ndef floatconverter(x):\n    if not np.isnan(x):\n        return np.float32(x)\n    else:\n        return np.nan","a0418ade":"converter_dict = {'Quantity': 'intconverter', 'UnitPrice': 'floatconverter', 'CustomerID': 'intconverter'}","dad90c2d":"%%time\n#original_df = pd.read_excel(inp_file, dtype = {'InvoiceNo': str, 'StockCode': str,'Description': str, 'Quantity': np.int32, \n                                               #'InvoiceDate':np.datetime64, 'UnitPrice': np.float32, 'CustomerID': np.int32, 'Country':str})\noriginal_df = pd.read_excel(src_file, parse_dates=True)","7879dac3":"%%time\noriginal_df.head(10)","711ed1b7":"%%time\n# Converting customerID to string\noriginal_df['CustomerID'] = original_df['CustomerID'].astype('str')\noriginal_df.describe(include=['object','int', 'float','datetime'])","ba5e83b2":"# Dropping duplicates if any\nduplicate_df_v1 = original_df.drop_duplicates()\n\nduplicate_df_v1.describe(include=['object','int', 'float','datetime'])","f2f2879c":"%%time\n# Getting null index for customer Ids\nnull_idx = duplicate_df_v1[duplicate_df_v1.CustomerID.isnull()].index.values\nnull_idx = null_idx.tolist()\nunique_invoiceno_with_nullcustID = duplicate_df_v1.iloc[null_idx,duplicate_df_v1.columns.get_loc('InvoiceNo')].unique()\n\n# First check wether we will be able to find any CustomerID for above invoice numbers. \nnone_InvNum_found_with_custID = True\nfor inv_num in unique_invoiceno_with_nullcustID:\n    x = original_df[original_df.InvoiceNo == inv_num].count()\n    if x.CustomerID == 0:\n        pass\n    else:\n        none_InvNum_found_with_custID = False\n\nprint('Any invoice number (for indexes with missing CustomerID) found? ', not none_InvNum_found_with_custID,'\\n')","0b520940":"%%time\nduplicate_df_v2 = duplicate_df_v1.drop(index=duplicate_df_v1[duplicate_df_v1['CustomerID']=='nan'].index.values)\n# Removing indexes with negative Quantity and UnitPrice\nduplicate_df_v2 = duplicate_df_v2[(duplicate_df_v2['Quantity'] > 0) & (duplicate_df_v2['UnitPrice'] > 0)]\nduplicate_df_v2.describe(include=['object','int', 'float','datetime']).T","a3ff4ee4":"fig = plt.figure(figsize=(15, 8))\nax = plt.axes()\n#a = duplicate_df_v2.Country.value_counts().plot(kind='bar')\nax.bar(range(0,len(duplicate_df_v2.Country.value_counts())), height=duplicate_df_v2.Country.value_counts())\nax.set_xticks(range(0,len(duplicate_df_v2.Country.value_counts())))\nax.set_xticklabels(duplicate_df_v2.Country.unique().tolist(), rotation='vertical')\nax.tick_params(axis='x', colors='white')\nax.tick_params(axis='y', colors='white')\nax.set_title('Figure 1: Country Counts')\nplt.grid(axis='y')","e5bf84ee":"#duplicate_df_v3 = duplicate_df_v2[duplicate_df_v2['Country'] == 'United Kingdom']\n\n# Dropping country column, stock code and Description\nduplicate_df_v3 = duplicate_df_v2.drop(columns=['Country','StockCode','Description'])\n\nduplicate_df_v3.describe(include=['object','int', 'float','datetime']).T","3f2060fd":"duplicate_df_v4 = duplicate_df_v3.sort_values(by=['CustomerID','InvoiceDate'])\n# Calulate total purchase\nduplicate_df_v4['TotalOrderValue'] = duplicate_df_v3['Quantity'] * duplicate_df_v3['UnitPrice']\nduplicate_df_v4.head(10)","ebcc52c2":"duplicate_df_v4_group=duplicate_df_v4.groupby('CustomerID').agg({'InvoiceDate': lambda date: (date.max() - date.min()).days,\n                                                                 'InvoiceNo': 'count',\n                                                                 'Quantity': 'sum',\n                                                                 'TotalOrderValue': 'sum'})\n# Renaming the column labels\nduplicate_df_v4_group.columns=['purchase_freshness','total_num_transactions','total_num_units','total_money_spent']\nduplicate_df_v4_group.head()","3ec0e602":"# Average Order Value\nduplicate_df_v4_group['avg_order_value'] = duplicate_df_v4_group['total_money_spent'] \/ duplicate_df_v4_group['total_num_transactions']","e8f72a83":"# Purchase Frequency\npurchase_frequency = sum(duplicate_df_v4_group['total_num_transactions']) \/ duplicate_df_v4_group.shape[0]","da9ee2f3":"# Churn Rate\nrepeat_rate = duplicate_df_v4_group[duplicate_df_v4_group.total_num_transactions > 1].shape[0] \/ duplicate_df_v4_group.shape[0]\nchurn_rate = 1 - repeat_rate","8de114fb":"# Profit Margin\n# Assuming that this Online Retail company have 7% Profit Margine out of Total Sales.\nduplicate_df_v4_group['profit_margin'] = duplicate_df_v4_group['total_money_spent'] * 0.07","d4cb0f50":"# Customer Lifeatime Value\nduplicate_df_v4_group['CLTV']=((duplicate_df_v4_group['avg_order_value'] * purchase_frequency) \/ churn_rate) * duplicate_df_v4_group['profit_margin']","6027299d":"duplicate_df_v4_group.head(10)","c3333e3d":"dv4_group = duplicate_df_v4.groupby('CustomerID').InvoiceDate.max().reset_index(drop=False)","c3189048":"dv4_group.columns = ['CustomerID', 'Max_InvoiceDate']\ndv4_group.head()","f9cd3c52":"duplicate_df_v5 = pd.merge(duplicate_df_v4, dv4_group, on='CustomerID', how='left')\nduplicate_df_v5.head(10)","cf4ef7f5":"duplicate_df_v5['Difference_Days_from_MaxDate'] = (duplicate_df_v5['Max_InvoiceDate'] - duplicate_df_v5['InvoiceDate']).dt.days\nduplicate_df_v5.describe()","98f45196":"duplicate_df_v4['Month_Yr'] = duplicate_df_v4['InvoiceDate'].apply(lambda x: x.strftime('%b-%Y'))\nduplicate_df_v4.head(10)","ed62cbe3":"%%time\n# Summerizing Data\nsale_by_Month_Yr = duplicate_df_v4.pivot_table(index=['CustomerID'],columns=['Month_Yr'],\n                                               values='TotalOrderValue',aggfunc='sum',fill_value=0).reset_index()\n\n# Sort columns in ascending order of dates \nfrom datetime import datetime\ndates = sale_by_Month_Yr.columns.to_list()[1:]\ndates.sort(key = lambda date: datetime.strptime(date,'%b-%Y'))\ndates.insert(0,'CustomerID')\nsale_by_Month_Yr = sale_by_Month_Yr[dates]\n\n# Calulating CLV\nsale_by_Month_Yr['Latest_6Months_Total_Purchase'] = sale_by_Month_Yr.iloc[:,8:].sum(axis=1)\nsale_by_Month_Yr.head(10)","3b58aa48":"#  Calculating Recency, Frequency and Monetary\nPRESENT = datetime(2011,7,1) # Because we are considering transcations upto month Jun-2011 thus taking 1st July as reference\nrfm = duplicate_df_v4[duplicate_df_v4['InvoiceDate'] < PRESENT].groupby('CustomerID').agg({'InvoiceDate': lambda date: (PRESENT - date.max()).days,\n                                        'InvoiceNo': 'count','TotalOrderValue': 'sum'})\n\nrfm.columns=['recency','frequency','monetary']\n\nsale_by_Month_Yr_withrfm = pd.merge(sale_by_Month_Yr, rfm, on='CustomerID', how='left')\n\nsale_by_Month_Yr_withrfm = sale_by_Month_Yr_withrfm[['CustomerID','Dec-2010','Jan-2011','Feb-2011',\t'Mar-2011',\t'Apr-2011',\t'May-2011','Jun-2011',\n                                                     'recency','frequency','Jul-2011','Aug-2011','Sep-2011','Oct-2011','Nov-2011','Dec-2011',\n                                                     'Latest_6Months_Total_Purchase']]\nsale_by_Month_Yr_withrfm.head(10)","6b6a0248":"sale_by_Month_Yr_withrfm_clean = sale_by_Month_Yr_withrfm.dropna()\nsale_by_Month_Yr_withrfm_clean.describe().T","a0ed65c5":"# Splitting into train and test set\ntrain_set = sale_by_Month_Yr_withrfm_clean.sample(frac=0.70, random_state=22)\ntest_set = sale_by_Month_Yr_withrfm_clean.drop(train_set.index)","c9796d38":"# Creating independent variables aka X for train and test.\nindependent_vars_train = train_set[train_set.columns.to_list()[1:-7]]\nindependent_vars_test = test_set[test_set.columns.to_list()[1:-7]]\nindependent_vars_test.head(10)","79ed074b":"# Creating target set for test and train\ntarget_vars_train = train_set['Latest_6Months_Total_Purchase']\ntarget_vars_test = test_set['Latest_6Months_Total_Purchase']","8be876a6":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()","4fac813e":"lr.fit(independent_vars_train.values, target_vars_train.values)","65548e19":"train_score = lr.score(independent_vars_train.values, target_vars_train.values)\nprint('Regressor score on train data:', train_score)\ntest_score = lr.score(independent_vars_test.values, target_vars_test.values)\nprint('Regressor score on test data:', test_score)","66a8a0cf":"from sklearn.preprocessing import MinMaxScaler","d6355715":"# Scaling data in default range\nmscalar = MinMaxScaler()\nindependent_vars_train_mmscaled = mscalar.fit_transform(independent_vars_train) # Fitting and transformation on train data\nindependent_vars_test_mmscaled = mscalar.transform(independent_vars_test) # only transformation on test data","7d08a3f7":"# Fitting linear regressor on scaled data\nlr_scaled = LinearRegression()\nlr_scaled.fit(independent_vars_train_mmscaled, target_vars_train)","ba70fe43":"train_score_sc = lr_scaled.score(independent_vars_train_mmscaled, target_vars_train)\nprint('Train score is:',train_score_sc)\ntest_score_sc = lr_scaled.score(independent_vars_test_mmscaled, target_vars_test)\nprint('Test score is:',test_score_sc)","466df9be":"import copy\nindependent_vars_train_mmscaled_stats  =  copy.deepcopy(independent_vars_train_mmscaled)\nones = np.ones((independent_vars_train_mmscaled_stats.shape[0],1))\n# OLS regressor expects first column to be a constant having all values 1. Thus, stacking a column to the independent variables\nnew_in_train = np.hstack((ones, independent_vars_train_mmscaled_stats)) # stacking on train only\nfrom statsmodels.regression.linear_model import OLS\nregressor_SLR_OLS = OLS(endog = target_vars_train, exog = new_in_train[:,:-1]).fit()","4de9f634":"regressor_SLR_OLS.summary()","96c3ce39":"#new_in_train = np.hstack((oness, independent_vars_train_mmscaled_stats))\n#from statsmodels.regression.linear_model import OLS\nregressor_SLR_OLS = OLS(endog = target_vars_train, exog = new_in_train[:,[0,1,3,4,5,6,7,8]]).fit()","559ba331":"regressor_SLR_OLS.summary()","350cd6ad":"# Again fitting regressor after dropping x7 feature\nregressor_SLR_OLS = OLS(endog = target_vars_train, exog = new_in_train[:,[0,1,3,4,5,6,7]]).fit()","73f64ce6":"regressor_SLR_OLS.summary()","86e69a72":"lin_reg2 = LinearRegression()\nlin_reg2.fit(independent_vars_train_mmscaled[:,[0,2,3,4,5,6,7]], target_vars_train)","9afb7026":"print('Train score is:',lin_reg2.score(independent_vars_train_mmscaled[:,[0,2,3,4,5,6,7]], target_vars_train))\nprint('Test score is:',lin_reg2.score(independent_vars_test_mmscaled[:,[0,2,3,4,5,6,7]], target_vars_test))","b0599c03":"from sklearn.tree import DecisionTreeRegressor\n\ndes_reg = DecisionTreeRegressor(random_state=42, max_depth = 6, min_samples_split= 60)\ndes_reg.fit(independent_vars_train, target_vars_train)","91653af3":"print('Decision Tree model score on training data:',des_reg.score(independent_vars_train, target_vars_train))\nprint('Decision Tree model score on testing data:', des_reg.score(independent_vars_test, target_vars_test))","c2dbeb37":"## Few observations from dataframe description table:\n* Data missing for 'Description' and 'CustomerID'\n* Max data is for 'United Kingdom'\n* Negative Unit Price found. No one is gonna give money to sell any item I think!\n* Negative Quantity found. This might be the return of item thus shown as negative (Stock Maintaining) but for now due to lack of information, I will take it as a mistake and drop indexes with negative Quantity values. Same goes for Unit Price also.\n* Last but not least, customer are buying 'WHITE HANGING HEART T-LIGHT HOLDER'. \n\n<img src=\"https:\/\/i.pinimg.com\/originals\/92\/0e\/d6\/920ed69e4ad155eb2b1ebdd492cbc711.jpg\" width=\"200\"><\/img>\n\n* Just forget last point !!","39e2a1be":"## Sorting dataframe\n","7e670c0e":"### With reference to model summary above:\n* With Confidence Interval of 95% two-sided, all variables having p-value greater than 0.05 will be removed.\n* Columns to remove, based on above summary are: x2 and x8","6b029135":"# Modelling for prediction of CLTV","ee888c79":"### From above snippet, we can see that:\n* Regressor score on test data is very low\n* Scaling data might help regressor to get better score","109dd7bb":"### From above figure we can see the most of the data is from a single country. There is a high class imbalance.\n\n### For now, dropping data of other countries data would be a better idea.","9cbc70e9":"### It is pretty much obvious from above scores that:\n* It seems there is no effect of scaling on training and testing score.\n\n### Using Ordinary Least Square(OLS) regressor from statsmodels library.","6a1a6636":"### Train and test scores seems to be very slightly increased but not significant at all.","56801861":"## Using Decision Tree for modelling.","2797b5f4":"### Further observations from above table:\n* 5268 duplicate columns deleted.\n* It seems like NaNs in CustomerID column are still posing a big problem and yes they have highest frequency also.\n* I will try to recover CustomerID by using InvoiceNo. Single InvoiceNo is alloted to single customer and single InvoiceNo can have multiple item descriptions. If say, for InvoiceNo 'x' we get CustomerID 'y' then we can safely say that for other 'x's if CustomerID is missing then it should be 'y'.","2c9f1610":"### Calculating CLTV using formula below:\n    CLTV = ((Average Order Value x Purchase Frequency)\/Churn Rate) x Profit margin.\n\n### Fields required:\n    * Average order value\n    * Purchase Frequency\n    * Churn Rate and,\n    * Profit Margin","8aedd4a3":"### With reference to the summary above: \n* removal of certain columns with p-value greater than 0.05, Adj. RSquare value and F-Statistics increased.\n\n### Doing linear regression again with selected columns. Lets check if this improves model score.","51ad8746":"## Explanation\/Reason(s) for Bad score:\n* One reason for bad test score is lack of data. Only 2960 datapoints are not enough to fit a linear model effectively.\n* There might be other reasons too which we will explore further.","f70389f5":"# To be continued for further exploration of techniques as well as data for better results.","36bfa1ad":"### In comparison to Linear Regression models, Decision Tree have shown far more better results. ","c714281c":"### x7 variable can also be dropped because of p-value greater than 0.05.","ea9606ef":"## Splitting into train_test\n* splitting data into 70:30 train to test ratio respectively","8f0b45a9":"### Since there is no InvoiceNo with which we can map missing CustomerID so, all rows with missing CustomerIDs will be dropped. 'df.dropna' will not work here because CustomerID column dtype is string and missin values are filled with 'nan', not 'NaN'.","7bb50cdc":"## Fitting Linear Regression over the data"}}