{"cell_type":{"8b1225d7":"code","9ef586d4":"code","29ae22d6":"code","50fd1d2b":"code","347f5a23":"code","ecf464cd":"code","108655c0":"code","309c996a":"code","ad80399b":"code","a7bba8a5":"code","c179987b":"code","f1dbf779":"code","3d2c25ea":"code","f08e1212":"code","8d4c3a6b":"code","cadc5758":"code","30920e81":"code","a60f6345":"code","f2226d4e":"code","28bf1e18":"code","717d4bac":"code","7f5b5759":"code","797804e6":"code","d0ae6373":"code","dcbed1c7":"code","92539215":"code","898ca9da":"code","3776a0a6":"code","47dc040d":"code","08720a19":"code","faab369e":"markdown","6395d4b8":"markdown","2c5df856":"markdown","4921927a":"markdown","7363e3ad":"markdown","e7c46d6c":"markdown","abedee12":"markdown","2ef72831":"markdown","834c1172":"markdown","b3e44124":"markdown","232a5ed5":"markdown","b35be648":"markdown","81ca1655":"markdown","815197e6":"markdown","5052077d":"markdown","92fbe8fc":"markdown","4bf7c090":"markdown","c086f611":"markdown","f0de84e8":"markdown","9e65f897":"markdown","ee78195e":"markdown","e74b28d9":"markdown","1ac04a79":"markdown","240266a0":"markdown","0d642e3b":"markdown","dbcdd93a":"markdown","e14bcf69":"markdown","7aa60a29":"markdown","941dd603":"markdown","e0012669":"markdown","e6e671aa":"markdown","2203352f":"markdown","f155573c":"markdown","f41c1a24":"markdown","c3ce9314":"markdown","e5a533f2":"markdown"},"source":{"8b1225d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9ef586d4":"# Libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\n\nimport scipy\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\n\nsns.set_palette('RdBu')","29ae22d6":"df = pd.read_csv('\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')","50fd1d2b":"print('Observations                                   : ', df.shape[0])\nprint('Features -- exclude Attrition and Satisfication: ', df.shape[1] - 2)","347f5a23":"df.head(10)","ecf464cd":"df.tail(10)","108655c0":"df.columns","309c996a":"print('Nan data points: ', df.isnull().sum().sum())","ad80399b":"df.Attrition.describe()","a7bba8a5":"df.Attrition.value_counts()","c179987b":"df.JobSatisfaction.describe(percentiles=[0.01, 0.45, 0.90])","f1dbf779":"# The big picture\nfig = make_subplots(rows=1, cols=2,\n                   specs=[[{\"type\": \"bar\"}, {\"type\": \"domain\"}]])\n\n# Sketch smaller details\ntrace0 = go.Histogram(x=df['Attrition'], name='In number', marker={'color':['red', 'blue']},\n                     showlegend=False)\ntrace1 = go.Pie(values=df['Attrition'].value_counts(), name='Percentage', labels=['No', 'Yes'],\n               textinfo='label+percent')\n\n# Add traces\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\n\n# Customize\nfig.update(layout_title_text='<b> Attrition <\/b>')\nfig.update_layout(showlegend=False)\n\n# Done\nfig.show()","3d2c25ea":"# The big picture\nfig = make_subplots(rows=3, cols=2,\n                   specs=[[{'rowspan':3}, {\"type\": \"domain\"}],\n                          [None,          {\"type\": \"domain\"}],\n                          [None,          {\"type\": \"domain\"}]])\n\n# Sketch smaller details\n\n## The bar chart - with Yes = negative columns.\nlabels = ['R&D', 'Sales', 'HR']\n\nyes = df['Department'][df.Attrition=='Yes'].value_counts()\ntrace_yes = go.Bar(x=labels, y=-yes, marker={'color':'red'}, showlegend=False) \n\nno  = df['Department'][df.Attrition=='No'].value_counts()\ntrace_no  = go.Bar(x=labels, y=no, marker={'color':'blue'}, showlegend=False )\n\n## Pie 1 -- upper right\nRD = df['Attrition'][df.Department=='Research & Development'].value_counts()\ntrace_3   = go.Pie(labels=['No', 'Yes'], values=RD, name='RD')\n\n## Pie 2\nSales = df['Attrition'][df.Department=='Sales'].value_counts()\ntrace_4   = go.Pie(labels=['No', 'Yes'], values=Sales, name='Sales')\n\n## Pie 3\nHR = df['Attrition'][df.Department=='Human Resources'].value_counts()\ntrace_5   = go.Pie(labels=['No', 'Yes'], values=HR, name='HR')\n\n# Add traces\nfig.append_trace(trace_yes, 1, 1)\nfig.append_trace(trace_no, 1, 1)\n\nfig.append_trace(trace_3, 1, 2)\nfig.append_trace(trace_4, 2, 2)\nfig.append_trace(trace_5, 3, 2)\n\n# Customize\nfig.update(layout_title_text='<b> Attrition by Department <\/b>')\n\n# Done\nfig.show()","f08e1212":"fig = px.box(df, y='MonthlyIncome', x='Gender', color='Gender', \n             points='all', \n             color_discrete_map={'Female':'red', 'Male':'Green'})\n\nfig.update(layout_title_text='<b> Monthly Income by Gender <\/b>')\nfig.update_layout(showlegend=False)\n\nfig.show()","8d4c3a6b":"# The big picture\nfig = make_subplots(rows=6, cols=2,\n                   specs=[[{'rowspan':6}, {\"type\": \"domain\"}], # 1  --  1\n                          [None,          {\"type\": \"domain\"}], # 0  --  2\n                          [None,          {\"type\": \"domain\"}], # 0  --  3\n                          [None,          {\"type\": \"domain\"}], # 0  --  4\n                          [None,          {\"type\": \"domain\"}], # 0  --  5\n                          [None,          {\"type\": \"domain\"}]])# 0  --  6\n\n# Sketching\n## Bar chart\nlabels=['Life Sciences', 'Medical','Marketing', 'Technical Degree', 'Other', 'Human Resources']\n\nyes = df['EducationField'][df.Attrition=='Yes'].value_counts(ascending=False)\nno = df['EducationField'][df.Attrition=='No'].value_counts(ascending=False)\n\nfig.add_bar(y=-yes, x=labels, col=1, row=1, marker={'color':'red'},  showlegend=False)\nfig.add_bar(y=no,   x=labels, col=1, row=1, marker={'color':'blue'}, showlegend=False)\n\n## Pie chart\nLS     = df['Attrition'][df.EducationField=='Life Sciences'].value_counts()\nMed    = df['Attrition'][df.EducationField=='Medical'].value_counts()\nMar    = df['Attrition'][df.EducationField=='Marketing'].value_counts()\nTech   = df['Attrition'][df.EducationField=='Technical Degree'].value_counts()\nOther  = df['Attrition'][df.EducationField=='Other'].value_counts()\nHR     = df['Attrition'][df.EducationField=='Human Resources'].value_counts()\n\nfig.add_pie(labels=['No', 'Yes'], values=LS,    name='LS',    col=2, row=1)\nfig.add_pie(labels=['No', 'Yes'], values=Med,   name='Med',   col=2, row=2)\nfig.add_pie(labels=['No', 'Yes'], values=Mar,   name='Mar',   col=2, row=3)\nfig.add_pie(labels=['No', 'Yes'], values=Tech,  name='Tech',  col=2, row=4)\nfig.add_pie(labels=['No', 'Yes'], values=Other, name='Other', col=2, row=5)\nfig.add_pie(labels=['No', 'Yes'], values=HR,    name='HR',    col=2, row=6)\n\n# Customize\nfig.update(layout_title_text='<b> Attrition by Education Field <\/b>')\n# Done\nfig.show()","cadc5758":"fig = make_subplots(rows=2, cols=2)\n\ntrace0 = go.Histogram(x=df['Department'], y=df['JobSatisfaction'], histfunc='avg')\ntrace1 = go.Histogram(x=df['EducationField'], y=df['JobSatisfaction'], histfunc='avg')\ntrace2 = go.Histogram(x=df['OverTime'], y=df['JobSatisfaction'], histfunc='avg')\ntrace3 = go.Histogram(x=df['MaritalStatus'], y=df['JobSatisfaction'], histfunc='avg')\n\nfig.add_trace(trace0, 1, 1)\nfig.add_trace(trace1, 1, 2)\nfig.add_trace(trace2, 2, 1)\nfig.add_trace(trace3, 2, 2)\n#fig = px.histogram(df, x='Department', y='JobSatisfaction',  histfunc='avg')\n\n\nfig.show()","30920e81":"g = sns.FacetGrid(data=df, row = 'Attrition', col = 'JobSatisfaction')\ng.map(plt.hist, 'MonthlyIncome', bins=10)","a60f6345":"sns.catplot(x='EducationField', y='MonthlyIncome',  data=df,\n           kind='violin')\nplt.xticks(rotation=45)","f2226d4e":"sns.countplot(x='DistanceFromHome', data=df)","28bf1e18":"# DistanceFromHome -- Attrition\nsns.catplot(x='Attrition', y='DistanceFromHome', data=df,\n           kind='box')","717d4bac":"fig = px.histogram(df, x='JobSatisfaction', color='JobSatisfaction')\n\nfig.update_layout(title='<b> JobSatisfaction <\/b>',\n                  xaxis={'tickmode': 'array',\n                         'tickvals': [1, 2, 3, 4]})\n\nfig.show()","7f5b5759":"features_to_analysis =      ['OverTime',         'MonthlyIncome',         'Age',\n                             'DistanceFromHome', 'TotalWorkingYears',     'MaritalStatus',\n                             'JobLevel',         'NumCompaniesWorked',    'YearsSinceLastPromotion',\n                             'MonthlyRate',      'TrainingTimesLastYear', 'YearsWithCurrManager',\n                             'Education',        'PercentSalaryHike']\nfeatures_to_analysis.sort()\nprint(features_to_analysis)","797804e6":"# Create table of feature datatypes.\ntable_datatypes = pd.DataFrame(columns=['Features', 'Datatype'])\n\n# 1st column: Features\ntable_datatypes['Features'] = features_to_analysis\n\n# 2nd column: Datatypes\ntable_datatypes['Datatype'] = [df[feature].dtypes for feature in features_to_analysis]\n\nprint(table_datatypes)","d0ae6373":"# Binary encoding: MaritalStatus and OverTime:\nlb = LabelEncoder()\n\ndf['MaritalStatus_encoded'] = lb.fit_transform(df['MaritalStatus']).astype(int)\ndf['OverTime_encoded'] = lb.fit_transform(df['OverTime']).astype(int)\n\n# Origins replaced by encoded\nfeatures_to_analysis = ['MaritalStatus_encoded' if x=='MaritalStatus' else x for x in features_to_analysis]\nfeatures_to_analysis = ['OverTime_encoded' if x=='OverTime' else x for x in features_to_analysis]","dcbed1c7":"# Split df to Yes-No Attrition\ndf_Attrition_yes = df[df.Attrition == 'Yes']\ndf_Attrition_no = df[df.Attrition == 'No']\n\n# Run: One sample Two-sided T-test\nt_statistic = []\np_value     = []\n\nfor feature in features_to_analysis:\n    # t-test\n    sample  = df_Attrition_yes[feature]\n    popmean = df_Attrition_no[feature].mean() # mean of population\n    t_stats, p = stats.ttest_1samp(sample, popmean)\n           \n    t_statistic.append(t_stats)\n    p_value.append(p)    \n    \n    print('Feature: ', feature)\n    print('t-statistic: %4.2f -- p-value: %4.4f \\n' %(t_stats, p))","92539215":"# Create tabel\ntable = pd.DataFrame()\ntable['Features'] = features_to_analysis\ntable['t-statistic'] = t_statistic\ntable['p-value'] = p_value\n\n# Conclusions\nalpha = 0.05\ntable['Decisions'] = ['Rejected' if x<alpha else 'Failed to reject' for x in table['p-value']]\ntable['Key factors'] = ['Yes' if x=='Rejected' else 'No' for x in table['Decisions']]\n\n# Drop not-needed\n#table = table.drop(['t-statistic', 'p-value'], axis=1)\n\nprint(table[['Features', 'Decisions', 'Key factors']].sort_values(by='Key factors', ascending=False))\n\n#print(table.sort_values(by='Decisions'))","898ca9da":"# Preparing samples for ANOVA\npopulation = df[['MaritalStatus', 'OverTime', 'JobSatisfaction']]\n\nanova_samples = {}\ni = 1\n\n# Create Samples by conditions\nfor MS in population['MaritalStatus'].unique():\n    for OT in population['OverTime'].unique():\n        sample = population['JobSatisfaction'][(df.MaritalStatus==MS) & (df.OverTime==OT)]\n        sample.reset_index(drop=True, inplace=True)\n        anova_samples[i] = sample\n        \n        i += 1","3776a0a6":"f, p = stats.f_oneway(anova_samples[1],\n                      anova_samples[2],\n                      anova_samples[3],\n                      anova_samples[4],\n                      anova_samples[5],\n                      anova_samples[6])\n\nprint('F-statistic: %4.2f' %(f))\nprint('p-value    : %4.2f' %(p))","47dc040d":"## Features to run correlation\n\n# Those be analyzed already.\nfeatures_to_analysis.remove('MaritalStatus_encoded')\nfeatures_to_analysis.remove('OverTime_encoded')\n\n# Put JobSatisfaction in to determine Correlation matrix latter\nfeatures_to_analysis.append('JobSatisfaction')\n\nprint(features_to_analysis)","08720a19":"corr_matrix = df[features_to_analysis].corr()\n\n# The heatmap\nfigure = plt.figure(figsize=(16,12))\n\nmask = np.triu(corr_matrix) # Hide the upper part.\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', linewidths=0.5, cmap=\"YlGnBu\", mask=mask)\n\nplt.show()","faab369e":"## Correlation determination for the rest of Features to Analysis","6395d4b8":"Look back to its distribution first.","2c5df856":"# 5. Statistic Tesing\n","4921927a":"3. ***Calculation***","7363e3ad":"## Determining statistic methods according to datatypes\n\nTo select the suitable methods, we need to answer two major questions:\n* What is **datatype** of the **target**?  \n* What is **datatype** of the **feature**?  \n\nThe below diagram will give us the answers.\n\n![Workflow.jpg](https:\/\/www.upsieutoc.com\/images\/2020\/04\/13\/Workflow.jpg)\n ","e7c46d6c":"# 2. Missing data\n\nI ran isnull(), read data desciption, manually look for any kind of missing data. There is no NaN nor any type of missing data in this set.  \n\n*** This dataset is way so clean.***","abedee12":"3. ***Calculation***\n\nTest: One way ANOVA","2ef72831":"Both Attrition and JobSafisfaction are categorical, so there is just small room for descriptive statistic here, which is: ***count*** and ***percentage***.","834c1172":"# Table of Contents\n\n1. Take a look at the dataset\n2. Missing data\n3. Descriptive statistic\n4. EDA\n5. Statistic Tesing\n * Which key factors to Attrition?\n * Which key factors to Job Satisfaction?\n6. Final verdict\n7. My potential mistakes","b3e44124":"### 1. Datatypes of Features","232a5ed5":"4. ***Conclusions***\n\n * p-value 0.36 > 0.05 -> ***Failed to reject*** H<sub>0<\/sub>: all 6 samples mean of Job Satisfaction are equal.  \n * MarituaStatus and OverTime: ***is not*** key values to JobSatisfaction","b35be648":"# 6. Final Verdict\n1. Key factors to Attrition.  \n\n * Age, DistanceFromHome, JobLevel,\n * MaritalStatus, MonthlyIncome, OverTime,\n * TotalWorkingYears, TrainingTimesLastYear, YearsWithCurrManager.\n\nThough there is a drawback: we just know they influence the Attrition, but how much?? T-test can not give the answer.\n\n2. Key factors to Job Satisfaction.\n\nMy procedure is unable to point out key factors influence the Job Satisfaction, but it indicates those does not:\n * Age, DistanceFromHome, Education,\n * JobLevel, MaritalStatus, MonthlyIncome,\n * MonthlyRate, NumCompaniesWorked, OverTime,\n * PercentSalaryHike, TotalWorkingYears, TrainingTimesLastYear, \n * YearsSinceLastPromotion, YearsWithCurrManager","81ca1655":"## Introduction\n\nHello everyone,  this notebook is an assignment in CBD Robotics internship, in order to exploit my basic acknowledge.\n\n***The assignment concludes 2 main parts.***  \n#### First, show off fundamental EDA skills, include:  \n * Visulization with matplotlib.pyplot, seaborn, and plotly\n * Missing value treatment with isnull()..\n * pd.DataFrame's methods: info(), describe(), head(), tail()..  \n\n#### Second, statistically questions:\n * Which key factors influence attrition rates?\n * Which key factors influence satisfaction rates?\n\nAlso, the statistics  must use ***following testing***:\n * T-test  \n * ANOVA and MANOVA\n * Correlation calculation in Pearman method.","815197e6":"### 3. The Filter suggests methods based on Datatypes\n\nLet's drop targets and features down the filter, we will find the way.\n\n#### Attrition: a binary category, so:\n * All the Features to analysis: using Hypothesis testing, includes MaritalStatus and Overtime which already encoded.\n \n#### Job Satisfication is orinal, but let deem it numerical for now.\n * MaritalStatus and OverTime: are partly categorical -> using Hypothesis testing.\n * All the rest: Correlation.","5052077d":"4. ***Conclusions***","92fbe8fc":"## My thoughts\n\n### The questions:\n * Which key factors influence attrition rates?\n * Which key factors influence satisfaction rates?\n\n### I am thinking\n\n1. ***I have not known any how-influence measure.*** I have several basic tools of hypothesis testing: t-test, ANOVA and MANOVA, but none of them directly returns whether this influences that, just giving statistic testing of mean, variance, and so on.  \n\n So, I paraphrase questions which key factors influence become: In the population of feature A (ex: MonthlyIncome), *** whether Yes-Attrition mean is statiscal different from No-Attrition mean?  ***\nIf the differences are significant, we conclusion this features **influences** the attrition rate.\n\n\n2. ***T-test downside is only working with every pair of variable***. One variable must be our target, Attrition or Satisfaction, the underconsidering features would be the other one. Therefore, it is impossible to directly apply t-test across all of features.  \n\n So, I make a work around. ***For *feature* in *all features*, does this feature influence Attrition or Satisfication***. Then each pair will outputs a conclusion.\n\n\n3. ***ANOVA and MANOVA have drawbacks, too.*** While put a set of features under the test, they can not tell specifically which one differs from which one. They give an alert that something wrong happen, that's all.  \n\n \n4. ***The correlation, as this measure is tailored for examining the relationship between numerical variables,*** so I would save it for *** Job Satisfication*** and its relationships.\n\nThe mention of correlation measure and numerical variables reminds to a crucial issue: datatypes. ***Which tests are suitable to apply with different datatypes?*** I will propose a solution right later.\n\n\n\n\n    ","4bf7c090":"### 4. Which key factors to Job Satisfaction?","c086f611":"<center>![alkir\/Thinkstock](https:\/\/compote.slate.com\/images\/75d251f2-6d54-4839-bfb1-96f40b237ef4.jpg) <\/center>","f0de84e8":"## Which features should be analyzed?  \n\n### For the Attrition\n*** True Positives *** are choosen according to [Attrition in an Organization || Why Workers Quit?](https:\/\/www.kaggle.com\/janiobachmann\/attrition-in-an-organization-why-workers-quit), there are:\n* Over Time\n* Monthly Income\n* Age. \n\n*** True Negatives *** should be:\n* Distance From Home\n* Total Working Years \n* Martial Status.\n\n*** The Unknown *** are:\n* Job Level, Num Companies Worked, Years Since Last Promotion,  \n* Years With Curr Manager, Training Times Last Year, Monthly Rate  \n* Education, Percent Salary Hike.\n\n### For Job Satisfication\nThe features are ***exactly the same*** without any clue of which True Positive, True Negative, or the Unknown is.","9e65f897":"1. ***Populations***  \n\n * Population: whole employees.\n * Samples:\n     * Sample 1: employees who is Single   and  Yes-OverTime.\n     * Sample 2: employees who is Married  and  Yes-OverTime.\n     * Sample 3: employees who is Divorced and  Yes-OverTime.\n     * Sample 4: employees who is Single   and  No-OverTime.\n     * Sample 5: employees who is Married  and  No-OverTime.\n     * Sample 6: employees who is Divorced and  No-OverTime.","ee78195e":"# 4. EDA","e74b28d9":"The above table classifies:\n * *** Binary categories:*** OverTime -> needed to encode to 0 - 1 format.\n * *** Trinary categories: *** MaritalStatus -> ANOVA could be appropriate.\n * *** Nominal:*** Education and JobLevel -> should be considered numerical.\n * *** Numerical:*** All the rest. \n ","1ac04a79":"2. ***Statements***  \n\n * H0: Means of JobSatisfaction in 6 samples are equal.  \n * H1: Existing at least one pair that breaks H0.\n \n ***Level of Significant***: 0.05","240266a0":"### 2. Preprocessing features","0d642e3b":"<center><h1> IBM HR analysis<\/h1><\/center>","dbcdd93a":"### Unbelievable!!!\n\nNone of Features analysising seems to have any dang correlation to JobSatisfaction!!","e14bcf69":"# 3. Descriptive statistic","7aa60a29":"### 4. Which key factors to Attrition?","941dd603":"5. Comments  \n\n * ***True Positives*** - Age, MonthlyIncome, and OverTime: all correct.\n * ***True Negatives*** - DistanceFromHome, TotalWorkingYears, and MaritalStatus: the hypothesis testing considers them key factors.\n * ***For-curious features*** - Age and Years WithCurr Manager are key factors.","e0012669":"## Hypothesis Testing for MaritalStatus and OverTime","e6e671aa":"# 1. Take a look","2203352f":"2. ***Statements***  \n\nFor each feature in the list of undertest features.  \n * H0: Mean of Population Yes == Mean of Population No  \n * H1: Mean of Population Yes != Mean of Population No","f155573c":"# 7. My potential mistakes\n1. ***Lacking of Assumptions checking***. T-test and ANOVA working based on concrete assumptions. If the data are not suitable for them, the testing could be incorrect.\n\n\n2. ***Problems with ordinal data.*** I ran ANOVA with assumption that Job Satisfaction is simply a integer, but it is not. Job Satisfaction is an ordinal, which by far different from a ninteger, so this assumption badly effects on ANOVA at certain level.","f41c1a24":"1. ***Populations***  \n\n * Population Yes: All employees who 'Yes' to Attrition.\n * Population No : All employee who 'No' to Attrition.","c3ce9314":"## Which features should be analyzed?  \n\nHaving know how to choose statistic methods due to the filter, now I have to pick out a number of features put in it.  \n\n***Running tests throughout all features is not necessary.*** If this notebook were a business project, it would be a must absolutely. However, the assignment is not a business but aiming to help to  get familiar with hypothesis testing, so I do not.\n\n\nBesides, I want to know if my answers are corrent, so the analyzing features should consist:\n * ***True positive*** - features really affect Attrition and Satisfaction.\n * ***True negative*** - features do not. \n * ***the Unknown*** - clueless, for my exploration.\n\nAlso, ***None of T-test nor ANOVA*** can handle pairs of ***a categorical target and a non-binary categorical features***, so I intently will not choose this kind of pair.\n","e5a533f2":"### Hypothesis Testing"}}