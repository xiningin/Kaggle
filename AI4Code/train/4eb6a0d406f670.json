{"cell_type":{"cae4fdf8":"code","e9a44967":"code","6fd60d75":"code","79fa3f89":"code","a11a8fe2":"code","55e9da2b":"code","52262607":"code","f432cf59":"code","8befee27":"code","8497a7e8":"code","63c1f242":"code","8a5e822a":"code","c6b1199a":"code","16c14db2":"code","50156d91":"code","d66f04da":"code","a8bdd06f":"code","2ee6d506":"code","9730cce7":"code","6287b269":"code","0f26dafd":"code","e29bd3ab":"code","a2959beb":"code","afceb492":"code","2a1ca149":"code","bb4ea4af":"code","00442bc7":"code","44b30f01":"code","6351bc8c":"code","445212f5":"code","33065cfc":"code","9d882191":"code","f8aeba29":"code","5b343354":"code","f5066d5f":"code","6ccf0edc":"code","cc5570e6":"code","b49cd221":"code","9b243532":"code","f521c96f":"code","cd085dde":"code","5a15908b":"code","9f6e64c1":"code","b165eff3":"code","0daf6b93":"code","946283c8":"code","1eed3ee7":"code","7c2682c4":"code","807961fe":"code","57bd6603":"code","5fc1d73f":"code","651a82ff":"code","23a12010":"code","05a2cb81":"code","dc3de2c3":"code","904d6399":"code","45b919d3":"code","1e0376cc":"code","d5e80a71":"code","b0d351ce":"code","f7716f67":"code","c8df0ffa":"code","ddfd5bb4":"code","64f870a4":"code","f9241798":"code","75302669":"code","3f7a87c6":"code","252b584f":"code","e84e3feb":"code","1d45cfd3":"code","5ed2d67f":"code","a54a0bfa":"code","35254de0":"code","6634b6a6":"code","37861fe6":"code","1b78da9f":"code","db534184":"code","5e33195a":"code","d28e33d3":"code","d31c54b9":"code","c7eb8c59":"code","fc8d0b9e":"code","19eb3203":"code","74c866f6":"code","ba68104f":"code","f681381f":"code","37d25998":"code","6f76aaef":"code","7d9322b5":"code","7aa19999":"code","e2bead64":"code","3036a9d2":"code","fb045bb8":"code","52c1bbca":"code","2d0aaa8a":"code","426b7538":"code","d22df5dd":"code","0a100c63":"code","8ae7c98e":"code","f278ff97":"code","297e0f2c":"code","5e652d0d":"code","549f8129":"code","983e7bfb":"code","0feb3bbc":"code","edcae7bf":"code","8dacf9fa":"code","440fa073":"code","3ebf8c85":"code","927216f1":"code","8f70032d":"code","e5415eff":"code","ad0babe1":"code","b03e706f":"code","84b814c2":"code","e7254188":"code","800030d0":"code","44da349f":"code","798569bf":"code","5353ca00":"code","54d186e6":"code","db87bfbe":"code","9d8bb9d5":"code","b5a30752":"code","35908e91":"code","b1b6b6d2":"code","f1cecd5f":"code","a051d5e0":"code","07809ef1":"code","e7fcc954":"code","f2acbc97":"code","fe15d9db":"code","42b01ab6":"code","2952e7ce":"code","a15fef03":"code","85c04a3a":"code","aa825a95":"code","db9de53a":"markdown","35ca315d":"markdown","e97dd689":"markdown","9ef125a8":"markdown","a526b13f":"markdown","957ff2eb":"markdown","4e033cd7":"markdown","f43c8160":"markdown","16bbffd3":"markdown","c84ee5f8":"markdown","84ffe647":"markdown","7ed75097":"markdown","6c8a7d03":"markdown","ce8e6fee":"markdown","efe172b4":"markdown","d03ce8ed":"markdown","8376daa5":"markdown","2b5fb881":"markdown","51af6e37":"markdown","331d5dda":"markdown","4919c9cc":"markdown","dfb71c8e":"markdown","7f926ba9":"markdown","5945921d":"markdown","05e16ecd":"markdown","f93c2951":"markdown","0efb4f01":"markdown","fe10db3e":"markdown","4547a0d7":"markdown","14aa3b7d":"markdown","373f15c5":"markdown","23051e68":"markdown","624caa69":"markdown","7793d26d":"markdown","7a06bd66":"markdown","814b38b4":"markdown","0a442f82":"markdown","779c89c4":"markdown","5d6572ea":"markdown","b19ce8d7":"markdown","0540807d":"markdown","0cdd730d":"markdown","787ff2e5":"markdown","0212acd3":"markdown","7dceaad5":"markdown","b7fa21f3":"markdown","17ce04a9":"markdown","06eda486":"markdown","5a836f42":"markdown","1499b4f6":"markdown","84797cbf":"markdown","aca28134":"markdown","c2b794d7":"markdown","feaa997c":"markdown","7fadbc6c":"markdown","6ba55e1e":"markdown","b37da574":"markdown","0781fdba":"markdown","d72f3805":"markdown","0a7aeee5":"markdown","16bdd0f2":"markdown","feea4a89":"markdown","c1afe0ed":"markdown","bd86f78c":"markdown","a6266ec8":"markdown","a3fdf76d":"markdown","492c5eed":"markdown","b0dbeaf7":"markdown","0f6998c3":"markdown","6560c4af":"markdown","b3690eef":"markdown","a53f3b17":"markdown","a17319e3":"markdown","7128f790":"markdown","bd2b5a58":"markdown","15349795":"markdown","69162198":"markdown","e9a3a90d":"markdown","59aaab74":"markdown","c2115a90":"markdown","64c34f53":"markdown","098a1cbb":"markdown","09a1dfed":"markdown","6ac4118b":"markdown","fd558939":"markdown","81841701":"markdown","77b26869":"markdown","694ab248":"markdown","3eeaadf4":"markdown","aed22ff8":"markdown","8e9d23fb":"markdown","beb1176c":"markdown","1a4371e0":"markdown","4529a3d3":"markdown","4360a3ff":"markdown","8397f23b":"markdown"},"source":{"cae4fdf8":"#importing libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#To ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Data visualization Library\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('fivethirtyeight')","e9a44967":"#importing csv files\ndf_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test =  pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n#Saving test IDs for submission\nID_data = df_test['PassengerId']","6fd60d75":"#Shapes of train and test dataset\nprint(\"Shape of train dataset :\",df_train.shape)\nprint(\"Shape of train dataset :\",df_test.shape)","79fa3f89":"#Size of our datasets\nprint(\"Size of train dataset :\",df_train.size) #size = rows * Columns\nprint(\"Size of test dataset :\",df_test.size) #size = rows * Columns","a11a8fe2":"#Displaying top5 rows training set\ndf_train.head()","55e9da2b":"#Displaying top rows testing set\ndf_test.head()","52262607":"#infos\ndf_train.info()","f432cf59":"#Types of datatypes \ndf_train.dtypes","8befee27":"#Visualizing #of datatypes in train dataset\nax = df_train.dtypes.value_counts().sort_values().plot.barh(color='green')\nax.set(ylabel='Datatypes',xlabel='Counts');","8497a7e8":"#Length of datasets\nprint (\"Length of train dataset :\",len(df_train))\nprint (\"Length of test dataset :\",len(df_test))","63c1f242":"#No of unique values in each feature(column)\ndf_train.nunique().to_frame()","8a5e822a":"#descriptive stats for numerical features(we can ues that to see text features too)\ndf_train.describe()","c6b1199a":"#Let's see above stat details for text features:\ndf_train.describe(include='object').transpose()","16c14db2":"#Stats by age \ndf_train.groupby('Survived').describe()['Age']","50156d91":"#Let's see correlation between features\ndf_train.corr()","d66f04da":"#Heatmap\nplt.figure(figsize=(14,8))\nsns.heatmap(df_train.corr(),annot=True,cmap='coolwarm')\nplt.title('Correaltion Co-eff Matrix',fontsize=16);\n","a8bdd06f":"#Expore corrleation coeff__ for survived feature:\ndf_train.corr()['Survived'].sort_values().to_frame()","2ee6d506":"#Correlation with survival features\nplt.figure(figsize=(14,6))\ndf_train.corr()['Survived'].sort_values()[:-1].plot.bar(color='r') #I'm leaving out survival feature \nplt.ylabel('Correlation Strength and Direction',fontsize=14)\nplt.xlabel('Features',fontsize=14);","9730cce7":"#Distribution of AGE feature\nsns.distplot(df_train['Age'].dropna(),bins=30,color='m');\nplt.title('AGE distribution ');","6287b269":"#outlier detection \nsns.boxplot(df_train['Survived'],df_train['Age']);","0f26dafd":"fig,ax= plt.subplots(1,3,figsize=(20,8))\nsns.violinplot(df_train['Survived'],df_train['Age'],ax=ax[0])\nsns.violinplot(df_train['Survived'],df_train['Age'],hue=df_train['Sex'],ax=ax[1],palette='winter')\nsns.violinplot(df_train['Survived'],df_train['Age'],hue=df_train['Pclass'],ax=ax[2],palette='winter');","e29bd3ab":"#Survived or not survived passengers\ndf_train.groupby([\"Survived\"]).agg({'Age':{'Min':'min','Max':'max','Avg':'mean'}})","a2959beb":"#VIsualizing saeborn different kinds of plot\nfig,ax= plt.subplots(1,3,figsize=(22,8))\nsns.stripplot(df_train['Embarked'],df_train['Age'],ax=ax[0])\nsns.violinplot(df_train['Embarked'],df_train['Age'],ax=ax[1],palette='plasma')\nsns.swarmplot(df_train['Embarked'],df_train['Age'],ax=ax[2],palette='plasma');","afceb492":"df_train.groupby([\"Embarked\"]).agg({'Age':{'Min':'min','Max':'max','Avg':'mean'}})","2a1ca149":"df_train.groupby([\"Embarked\",'Survived']).agg({'Age':{'Min':'min','Max':'max','Avg':'mean'}})","bb4ea4af":"#Explore Age distribution with respect to survived \ng = sns.FacetGrid(df_train,col='Survived')\ng.map(sns.distplot,'Age',kde=False,);","00442bc7":"#Avg age- survived or not survived by Passenger class\ndf_train.groupby(['Pclass']).mean()['Age'].to_frame()","44b30f01":"#pandas crosstab...\npd.crosstab(df_train['Survived'],df_train['Embarked'],values=df_train['Age'],aggfunc=np.mean)","6351bc8c":"#Outlier detection with boxplot and stripplot\nplt.figure(figsize=(12,6))\nsns.boxplot(df_train['Survived'],df_train['Age'],palette='viridis')\nsns.stripplot(df_train['Survived'],df_train['Age'],);","445212f5":"#Exploring pclass vs Age\ng = sns.FacetGrid(df_train,col='Pclass')\ng.map(sns.distplot,'Age',kde=False,color='indianred');","33065cfc":"#Explore sibsp feature\nplt.figure(figsize=(12,6))\nsns.boxplot(df_train['SibSp'],df_train['Age'],palette='winter');","9d882191":"#Distribution of Fare price feature\nplt.figure(figsize=(10,6))\nsns.distplot(df_train['Fare'].dropna(),bins=30);\nplt.title('Distribution of fare(Price)');","f8aeba29":"#Skewness \ndf_train['Fare'].skew()","5b343354":"#seaborn swarmplot for Fare prices\nsns.swarmplot(df_train['Survived'],y=df_train['Fare'],palette='Greens');","f5066d5f":"#Scatterplot - to plot two numerical values\n\nsns.scatterplot(df_train['Fare'],df_train['Age'],color='c')\nplt.title('Fare Vs Age');","6ccf0edc":"#Joinplot - Visualize both histogram and scatterplot\nfig,ax = plt.subplots(1,2,figsize=(18,6))\nsns.scatterplot(df_train['Fare'],df_train['Age'],color='r',ax=ax[0])\nsns.scatterplot(df_train['Fare'],df_train['Age'],color='r',hue=df_train['Survived'],ax=ax[1],palette='viridis');","cc5570e6":"#Outlier or anamolies detection \nfig,ax = plt.subplots(1,2,figsize=(14,6))\nsns.boxplot(df_train['Survived'],df_train['Fare'],ax=ax[0])\nsns.boxplot(df_train['Survived'],df_train['Fare'],hue=df_train['Sex'],ax=ax[1]);","b49cd221":"#Explore fare by survival and gender wise:\ndf_train.groupby(['Survived','Sex']).agg({'Fare':{'Min_fare':'min','Max_fare':'max','Avg_Fare':'mean','Median_fare':'median'}})","9b243532":"g = sns.FacetGrid(df_train, hue=\"Survived\", col=\"Pclass\", margin_titles=True,\n                  palette={1:\"seagreen\", 0:\"gray\"})\ng=g.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend();","f521c96f":"#Pclass fare details\ndf_train.groupby(['Pclass','Survived']).agg({'Fare':{'Min_fare':'min','Max_fare':'max','Avg_Fare':'mean','Median_fare':'median'}})","cd085dde":"#box and stripplot for distribution of prices\nplt.figure(figsize=(12,6))\nsns.boxplot(df_train['Survived'],df_train['Fare'])\nsns.stripplot(df_train['Survived'],df_train['Fare']);","5a15908b":"#Exploring fare by embarked feature\ndf_train.groupby(['Embarked','Survived']).agg({'Fare':{'Min_fare':'min','Max_fare':'max','Avg_Fare':'mean','Median_fare':'median'}})","9f6e64c1":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf_train['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=df_train,ax=ax[1])\nax[1].set_title('Survived');","b165eff3":"#Male vs Female passengers ratio\ndf_train['Sex'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',shadow=True);","0daf6b93":"#Exploring survival features \nfig,ax= plt.subplots(1,3,figsize=(20,8))\nsns.countplot(df_train['Survived'],ax=ax[0])\nsns.countplot(df_train['Survived'],hue=df_train['Sex'] ,ax=ax[1])\nsns.countplot(df_train['Survived'],hue=df_train['Pclass'] ,ax=ax[2],palette='winter');","946283c8":"#Exploring survival features \nfig,ax= plt.subplots(2,2,figsize=(20,8))\nsns.countplot(df_train['Survived'],ax=ax[0,0])\nsns.countplot(df_train['Survived'],hue=df_train['Sex'] ,ax=ax[0,1])\nsns.countplot(df_train['Survived'],hue=df_train['Pclass'] ,ax=ax[1,0])\nsns.countplot(df_train['Survived'],hue=df_train['Embarked'] ,ax=ax[1,1]);","1eed3ee7":"print('Counts of survival by pclass :')\ndf_train.groupby(['Survived','Pclass']).count()[['PassengerId']].unstack()","7c2682c4":"#Lets visualize sibsp features \nsns.countplot(df_train['SibSp'],hue=df_train['Survived'],palette='magma')\nplt.ylabel('Survival count')\nplt.legend(loc=1);","807961fe":"sns.countplot(df_train['SibSp'],hue=df_train['Embarked'],palette='plasma')\nplt.legend(loc=1);","57bd6603":"#crosstab\npd.crosstab([df_train.Sex,df_train.Survived],df_train.Pclass,margins=True)","5fc1d73f":"#factorplot\nsns.factorplot('Pclass','Survived',hue='Sex',data=df_train);","651a82ff":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,6))\n\nsns.factorplot('Embarked','Survived',data=df_train,ax=ax1)\n\nsns.factorplot('Embarked','Survived',hue='Sex',data=df_train,ax=ax2);","23a12010":"#merging train and test dataset\ndf = pd.concat([df_train.drop('Survived',axis=1),df_test],axis=0)","05a2cb81":"#data top rows\ndf.head(5)","dc3de2c3":"#visualising missing values\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis');","904d6399":"#Missing value percentage per variable \nmis_perc = round(df.isnull().sum().to_frame()\/len(df),5)*100\nmis_perc","45b919d3":"#How many unique labels are on cabin?\nprint(df.Cabin.value_counts())\nprint(f\"\\n No of unique labels {df.Cabin.nunique()}\")","1e0376cc":"#we could use numpy where function to create a variable taht captures null values\ndf['cabin_NA'] = np.where(df.Cabin.isnull(),1,0)","d5e80a71":"#ensure it worked\ndf.cabin_NA.value_counts(normalize=True) * 100","b0d351ce":"pd.crosstab(df.cabin_NA,df.Sex).plot.pie(subplots=True,autopct='%1.1f%%',shadow=True,figsize=(10,5));","f7716f67":"#Fill missing values with median\ndf['Fare'] = df['Fare'].fillna(df['Fare'].median)","c8df0ffa":"#When we filled missing values it converted to string \ndf['Fare']= df['Fare'].astype('str')\ndf['Fare'] = pd.to_numeric(df['Fare'],errors='coerce')","ddfd5bb4":"#filling embarked feature\ndf.Embarked.fillna(df.Embarked.mode,inplace=True)","64f870a4":"#we can drop cabin feature since most of data in that feature is missing\ndf =df.drop('Cabin',axis=1)","f9241798":"#Ensuring we dropped that column\ndf.head()","75302669":"#Checking realtionship between age and other features\ndf.corrwith(df.Age).to_frame()","3f7a87c6":"#Taking age mean for each class\ndf.groupby(['Pclass'])['Age'].mean().to_frame()","252b584f":"#Defining function to impute missing values\ndef impute_age(cols):\n    Age = cols[0] #grabbing 1st column\n    Pclass = cols[1] #Grabbing 2nd column\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 39\n\n        elif Pclass == 2:\n            return 29\n\n        else:\n            return 25\n\n    else:\n        return Age","e84e3feb":"#Lets apply that function\ndf['Age'] = df[['Age','Pclass']].apply(impute_age,axis=1)","1d45cfd3":"#Now let's check that heat map again!\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis');","5ed2d67f":"#Missing values \ndf.isnull().sum()","a54a0bfa":"#remove unnecessary columns\ndf = df.drop(['Name','PassengerId','Ticket'],axis=1)","35254de0":"#Lets checkout the frame\ndf.head()","6634b6a6":"#shape of dataset\ndf.shape","37861fe6":"#we can convert age column to categoical feature using pandas cut method(7 categories)\nbins = ( 0, 5,12,18,25,40,60,120)\ngroup_names = [ 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ndf['age_cat'] = pd.cut(df.Age,bins=bins,labels=group_names)","1b78da9f":"#Percentage of different age passengers on board\ndf['age_cat'].value_counts(normalize=True).round(4).to_frame() *100","db534184":"df.dtypes.to_frame()","5e33195a":"#Discretising Fare variable\ndf['fare'] = pd.cut(df.Fare,bins=[0,10,30,60,1000],labels=['Low','medium','high','very high'])","d28e33d3":"#Lets see what we have done\ndf.head()","d31c54b9":"#Lets convert categorical features to numerical \ncat1 = pd.get_dummies(df['age_cat'],drop_first=True)\ncat2 = pd.get_dummies(df['Sex'],drop_first=True)\ncat3 = pd.get_dummies(df['fare'],drop_first=True)\n\n#Concat those features to our original dataframe\ndf = pd.concat([df,cat1,cat2,cat3],axis=1)","c7eb8c59":"#Embarked feature encoding\nembark =pd.get_dummies(df['Embarked'].astype('str'),drop_first=True)\ndf = pd.concat([df,embark],axis=1)","fc8d0b9e":"df.head()","19eb3203":"#Drop unneccessary columns\ndf = df.drop(['Embarked','Age','Fare','Sex','age_cat','fare'],axis=1)","74c866f6":"#CHeck our final our dataframe\ndf.head()","ba68104f":"#lets check shpae our final dataset\ndf.shape","f681381f":"#train test split\ntrain = df.iloc[:891,:]\ntest = df.iloc[891:,:] # to submit our predictions\ny = df_train['Survived']","37d25998":"#import Ml algos\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score,KFold,GridSearchCV,RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,f1_score,precision_score,roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\n#Split\nx_train,x_val,y_train,y_val = train_test_split(train,y,test_size=.3,random_state=101)","6f76aaef":"#create a function to assses the model performance\n\ndef model_eval(model):\n    \n    train_preds = model.predict(x_train)\n    val_preds = model.predict(x_val)\n    scores = {\"Training Accuracy\": accuracy_score(y_train, train_preds),\n              \"Valid Accuracy\": accuracy_score(y_val, val_preds),\n              \"Training auc_roc\": roc_auc_score(y_train,model.predict_proba(x_train)[:,1]),\n              \"Valid auc_roc\": roc_auc_score(y_val,model.predict_proba(x_val)[:,1]),\n              }\n    \n    return scores","7d9322b5":"#set the reproductivity\nnp.random.seed(42)\n\n#Traing our model(Without Hyper parameter optimization)\nlog_model = LogisticRegression()\nlog_model.fit(x_train,y_train)\n\n#prediction\nlog_pred = log_model.predict(x_val)\n\n#Model evaluation\nmodel_eval(log_model)","7aa19999":"# Setup random seed\nnp.random.seed(42)\n\n#Searching best estiamtors using SKlearn gridsearch :\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],'penalty':['l1','l2'] }\nclf = GridSearchCV(LogisticRegression(), param_grid,cv=5,scoring='accuracy')\n\n#Finding best parameters\nclf.fit(x_train,y_train)\n\n#Predictions\nlmg = clf.predict(x_val)\n\n#Find the model best parameter\nprint('Best parameters \\n:' , clf.best_params_)\nprint ('Best score :', clf.best_score_ * 100)","e2bead64":"#Model assessment\nmodel_eval(clf)","3036a9d2":"#Trying different thresholds and assess their affect on results(experimental)\nfrom sklearn.preprocessing import binarize\n\nthresh = []\nacc_score =[]\n#predict probabiliy for each instances\nprobs = clf.predict_proba(x_val)\nfor i in np.arange(0.1,0.9,0.001):\n    preds = binarize(probs,i)[:,1]\n    thresh.append(i)\n    score = accuracy_score(y_val,preds) *100\n    acc_score.append(np.round(score,2))\n\n#See top 5 thresholds values which acn increase boost the aacuracy\npd.DataFrame([thresh,acc_score]).T.sort_values(by=1,ascending=False).set_index(0).head(5)","fb045bb8":"#Coefficient of each features\nco_eff = pd.DataFrame(log_model.coef_.T,index=x_train.columns,columns=['Co_eff']).sort_values(by='Co_eff',ascending=False)\nco_eff","52c1bbca":"#Lets plot the feature coefficients to understand better features\nco_eff.plot.barh(title='Logistic model Co-Efficient',figsize=(11,5));","2d0aaa8a":"fig,(ax1,ax2)= plt.subplots(nrows=1,ncols=2,sharex=True,sharey=True,figsize=(12,6))\n\n#Barplot \ndf_train.groupby(['Sex','Survived']).size().unstack().plot.bar(title='Survival ratio by Gender',ax=ax1)\n\n#Stacked barplot\ndf_train.groupby(['Sex','Survived']).size().unstack().plot.bar(stacked=True,title='Stacked survival ratio by Gender',ax=ax2);","426b7538":"#Combining train x & y to get some insights\ntrain_full = pd.concat([x_train,y_train],axis=1)\ntrain_full.groupby(['Survived','male']).size().to_frame()","d22df5dd":"print(pd.crosstab(train_full.Survived,train_full.male))\n\n#Groupby survived to get male&female survival ratio\nsurvival_rat = train_full.groupby(['Survived'])['male'].mean().round(4).to_frame() *100\nsurvival_rat.index = ['Not Survived','Survived']\nsurvival_rat['female'] = 100 - survival_rat['male']\nsurvival_rat","0a100c63":"#Groupby gender\ngender_ratio =train_full.groupby(['male'])['Survived'].mean().to_frame() *100 # 0 means female and 1 means male.\ngender_ratio.index = ['Female','Male']\ngender_ratio['Not Survived'] = 100 - gender_ratio['Survived']\ngender_ratio.stack().to_frame()","8ae7c98e":"#Compare cross validation score \ncv1 = cross_val_score(log_model,train,y,cv=10).round(5)\ncv2 = cross_val_score(clf,train,y,cv=10).round(5)","f278ff97":"#W\/o tuning\nprint(f\"Cv score : w\/o optimasation -- Mean_score  {cv1.mean()*100} Std {cv1.std()*100}\")\n#with tuning\nprint(f\"Cv score : with optimasation -- Mean_score  {cv2.mean()*100} Std {cv2.std()*100}\")","297e0f2c":"#Kaggle doesn't support latest verison of sklearn modules which makes it hard to implement latest features\n\n# from sklearn.metrics import plot_roc_curve\n\n# #Plotting Roc curve for diff models\n# fig,ax = plt.subplots(1,1,figsize=(10,5))\n# plot_roc_curve(log_model,x_val,y_val,ax=ax)\n# plot_roc_curve(clf,x_val,y_val,ax=ax);","5e652d0d":"tree = DecisionTreeClassifier()\ntree.fit(x_train,y_train)\n\n#Predictions\ntree_pred = tree.predict(x_val)\n\n#Model Evaluate\nmodel_eval(tree)","549f8129":"#finding the best parameter\nparams = {'criterion': ['gini', 'entropy'],'max_depth':[1,2,3,4,5],'random_state': [0]}\ntr_grid = GridSearchCV(tree,params)\ntr_grid.fit(x_train,y_train)\n\n#Make prediction\nGtr_pred = tr_grid.predict(x_val) \n\n#Find the model best parameter\nprint('Best parameters' , tr_grid.best_params_)\nprint ('\\n Best score :', tr_grid.best_score_ * 100)","983e7bfb":"#model assessment\nmodel_eval(tr_grid)","0feb3bbc":"#Compare cross validation score \ncv1 = cross_val_score(tree,train,y,cv=10).round(5)\ncv2 = cross_val_score(tr_grid,train,y,cv=10).round(5)\n\n#W\/o tuning\nprint(f\"Cv score : w\/o  optimasation -- Mean_score  {cv1.mean()*100} ---Std {cv1.std()*100}\")\n#with tuning\nprint(f\"Cv score : with optimasation -- Mean_score  {cv2.mean()*100} ---Std {cv2.std()*100}\")","edcae7bf":"# #Plotting Roc curve for diff curves\n# fig,ax = plt.subplots(1,1,figsize=(10,5))\n# #Model with deafult settings\n# plot_roc_curve(tree,x_val,y_val,ax=ax)\n# #Best model\n# plot_roc_curve(tr_grid,x_val,y_val,ax=ax);","8dacf9fa":"#Random forest \nrf = RandomForestClassifier(max_depth=4)\nrf.fit(x_train,y_train)\nrf_pred = rf.predict(x_val)\n\n#model evaluate\nmodel_eval(rf)","440fa073":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 50)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in range(1,15,2)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [20,25,30,40]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [5,7,10,14]\n\n# Create the grid\nparams = {     'criterion': ['gini', 'entropy'],\n               'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n                'random_state': [42]\n         }\n\nrf_grid = RandomizedSearchCV(rf,param_distributions=params)\nrf_grid.fit(x_train,y_train)\n\n\n#Find the model best parameter\nprint('Best parameters' , rf_grid.best_params_)\nprint ('\\n Best score :', rf_grid.best_score_ * 100)","3ebf8c85":"#Model assessment\nmodel_eval(rf_grid)","927216f1":"# #Plotting Roc curve for diff curves\n# fig,ax = plt.subplots(1,1,figsize=(10,5))\n# #Model with deafult settings\n# plot_roc_curve(rf,x_val,y_val,ax=ax)\n# #Best model\n# plot_roc_curve(rf_grid,x_val,y_val,ax=ax);","8f70032d":"#PLotting feature importance\npd.DataFrame(rf.feature_importances_,index=x_train.columns).sort_values(by=0).plot.barh(\n             color='salmon',\n             title='Forest Feature Importance',figsize=(10,5));","e5415eff":"#Lets combine to get a better idea\n\ntrain_full = pd.concat([x_train,y_train],axis=1)\n\n#Frequency analyse w.r.t survived\nprint(pd.crosstab(train_full.cabin_NA,train_full.Survived))\n\ntrain_full.cabin_NA.value_counts().plot.pie(autopct='%1.1f%%',shadow=True);","ad0babe1":"# group data by Survived vs Non-Survived\n# and find nulls for cabin\ncab_ratio = train_full.groupby(['Survived'])['cabin_NA'].mean().round(4) *100\ncab_ratio.index = ['Not Survived','Survived']\ncab_ratio.to_frame()","b03e706f":"#Visualizing those numbers to better understanding\nfig,(ax1,ax2) =plt.subplots(1,2,figsize=(14,6))\n\npd.crosstab(train_full.cabin_NA,train_full.Survived).plot.bar(ax=ax1)\nax1.legend(['Not survived','survived'])\n\npd.crosstab(train_full.cabin_NA,train_full.Survived).plot.bar(stacked=True,ax=ax2,label=['Not survived','survived'])\nax2.legend(['Not survived','survived']);","84b814c2":"gbm = XGBClassifier(\n #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 3,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)","e7254188":"pd.DataFrame(gbm.feature_importances_,index=x_train.columns).sort_values(by=0).plot.barh(title='XGBoost Feature IMPortance');","800030d0":"#Xgb with optimisation\nxg_pred = gbm.predict(test)\n\n#Model assessment\nmodel_eval(gbm)","44da349f":"#XGBoost with default settings\nxgb = XGBClassifier()\nxgb.fit(x_train,y_train)\nxg_pred = xgb.predict(x_val)\n\n#Model assessment\nmodel_eval(xgb)","798569bf":"#SVM with default parameters\nnp.random.seed(42) #For reprodcutivity\nsvm = SVC()\nsvm.fit(x_train,y_train,)\nsv_pred = svm.predict(x_val)\n\n#Svm model assessment\nprint(\"Svm (with deafult parameters) training accuracy\",accuracy_score(y_train,svm.predict(x_train))*100)\nprint(\"Svm (with deafult parameters) test accuracy    \",accuracy_score(y_val,sv_pred)*100)","5353ca00":"#Searching best parametres to our model\nparams= {'C':[0.1,1,10,100],'gamma':[0.1,0.01,0.001,.0001],'random_state':[42]}\n\n#Grid search\ngrid_svm = GridSearchCV(svm,param_grid=params,refit=True,scoring='accuracy',verbose=1)\ngrid_svm.fit(x_train,y_train)\n\nprint (f\"Best hyperparameters \\n:{grid_svm.best_params_}\")\nprint (f\"Grid search best Score {grid_svm.best_score_}\")","54d186e6":"#Make predictions with best SVM parameters\nsvm_pred = grid_svm.predict(x_val)\n\n#Model evaluate\n\nprint(f\"SVM after tuning parameters \\n\")\nprint(\"Accuary Score Train_set :\",accuracy_score(y_train,grid_svm.predict(x_train))*100)\nprint(f\"Accuary Score Test_set  : { accuracy_score(y_val,svm_pred)*100}\")","db87bfbe":"#Making predictiions using SVM(after tuned model)\n#submission1 = pd.DataFrame([ID_data,grid_svm.predict(test)]).T\n#submission1.to_csv('sub_svm_tuned.csv',index=False)","9d8bb9d5":"#Soft voting\nfrom sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=15)),\n                                              ('RBF',SVC(probability=True,kernel='rbf',C=1,gamma=0.1)),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR',LogisticRegression(C=100)),\n                                              ('DT',tr_grid)\n                                             ], \n                       voting='soft').fit(x_train,y_train)\nprint('The accuracy for ensembled model is:',ensemble_lin_rbf.score(x_val,y_val))\ncross=cross_val_score(ensemble_lin_rbf,train,y, cv = 10,scoring = \"accuracy\")\nprint('The cross validated score is',cross.mean())","b5a30752":"#Hard Voting\nfrom sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=15)),\n                                              ('RBF',SVC(probability=True,kernel='rbf',C=1,gamma=0.1)),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR',LogisticRegression(C=100)),\n                                              ('DT',tr_grid)\n                                             ], \n                       voting='hard').fit(x_train,y_train)\nprint('The accuracy for ensembled model is:',ensemble_lin_rbf.score(x_val,y_val))\ncross=cross_val_score(ensemble_lin_rbf,train,y, cv = 10,scoring = \"accuracy\")\nprint('The cross validated score is',cross.mean())","35908e91":"## Kaggle doesn't support latest verison of sklearn modules which makes it hard to implement latest features\n\n#Stacking classifier\n# from sklearn.ensemble import StackingClassifier\n\n# #Base level estimators\n# estimators =[('rf',rf_grid),('tr',tr_grid),('gbm',gbm), ('xgb',xgb),('log',log_model)]\n\n# #create a stacking model\n# stacking1 = StackingClassifier(estimators=estimators,\n#                               final_estimator =SVC(kernel = 'rbf')\n#                               )\n\n# #Training the stackers\n# stacking1.fit(x_train,y_train)","b1b6b6d2":"# #Stacking classifier\n# from sklearn.ensemble import StackingClassifier\n\n# #Base level estimators\n# estimators =[('rf',rf_grid),('tr',tr_grid),('gbm',gbm), ('xgb',xgb),('log',log_model)]\n\n# #create a stacking model\n# stacking2 = StackingClassifier(estimators=estimators,\n#                               final_estimator =LogisticRegression(C=100))\n\n# #Training the stackers\n# stacking2.fit(x_train,y_train)","f1cecd5f":"# #Stacking model assessment(train-set)\n# print('Stacking(svm) train-set score  ',stacking1.score(x_train,y_train)*100)\n# print('Stacking(logit) train-set score',stacking2.score(x_train,y_train)*100)\n\n# #Stacking model assessment(Test-set)\n# print('\\nStacking(svm) test-set score   ',stacking1.score(x_val,y_val)*100)\n# print('Stacking(logit) test-set score ',stacking2.score(x_val,y_val)*100)","a051d5e0":"#define convenient function \nfrom sklearn.model_selection import cross_val_predict\n\ndef model_fit(algorithm, X_train, y_train, cv):\n    \n    #cv fold test\n    y_pred =cross_val_predict(algorithm, X_train, y_train, cv = cv)\n    \n    #cv score\n    cv_acc = accuracy_score(y_train, y_pred)\n    \n    #error rate\n    error = np.mean(y_train != y_pred)\n    \n    return y_pred, cv_acc,error","07809ef1":"#define algorithms\nknn = KNeighborsClassifier(n_neighbors=15)\ntree_entr = DecisionTreeClassifier(criterion = 'entropy')\nsvm_lin = SVC(kernel = 'linear')\nsvm_rbf = SVC(kernel = 'rbf')\nlog = LogisticRegression(C=100)\nrf = RandomForestClassifier()","e7fcc954":"#run function to get scores for all above models\ny_pred_knn, cv_acc_knn,err1 = model_fit(knn, x_train, y_train, 10)\ny_pred_tree_gini, cv_acc_tree_best,err2 = model_fit(tr_grid, x_train, y_train, 10) #With best parameters\ny_pred_tree_entr, cv_acc_tree_ent,err3 = model_fit(tree_entr, x_train, y_train, 10)\ny_pred_svm_lin, cv_acc_svm_lin,err4 = model_fit(svm_lin, x_train, y_train, 10)\ny_pred_svm_rbf, cv_acc_svm_rbf,err5 = model_fit(svm_rbf, x_train, y_train, 10)\ny_pred_log, cv_acc_log,err6 = model_fit(log, x_train, y_train, 10)\ny_pred_rf, cv_acc_rf,err7 = model_fit(rf, x_train, y_train, 10)","f2acbc97":"#create dataframe to view how our models score\nmodels_eval = pd.DataFrame({\n    'Model': ['KNN', 'Decision Tree (Grid_search)', 'Decision Tree (Entropy)', \n              'SVM (Linear)', 'SVM (RBF)', \n              'Logistic Growth', 'Random Forest'],\n    'Score': [\n        cv_acc_knn, \n        cv_acc_tree_best,      \n        cv_acc_tree_ent, \n        cv_acc_svm_lin, \n        cv_acc_svm_rbf, \n        cv_acc_log,\n        cv_acc_rf,\n    ],'Error_Rate':[\n        err1,err2,err3,err4,err5,err6,err7\n    ]})\nprint('---Cross-validation Accuracy Scores (Train_set)---')\nmodels_eval.sort_values(by='Score',ascending=False)","fe15d9db":"# #Kaggle doesn't support latest verison of sklearn modules which makes it hard to implement latest features\n\n# #Lets plot confuison matrix (https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py)\n# from sklearn.metrics import plot_confusion_matrix\n\n# np.set_printoptions(precision=3)\n# # Plot non-normalized confusion matrix\n# titles_options = [(\"Confusion matrix, without normalization\", None),\n#                   (\"Normalized confusion matrix\", 'true')]\n\n# #svm with default settings\n# for title, normalize in titles_options:\n#     disp = plot_confusion_matrix(svm, x_val,y_val,\n#                                  display_labels=['Not Survived','Survived'],\n#                                  cmap=plt.cm.Blues,\n#                                  normalize=normalize)\n#     disp.ax_.set_title(title)\n\n#     print(title)\n#     print(disp.confusion_matrix)","42b01ab6":"from sklearn.metrics import recall_score\n\n#TPR or Sensitivity\nrecall_score(y_val,sv_pred)*100","2952e7ce":"# ## Plotting CM\n# np.set_printoptions(precision=3)\n\n# # Plot non-normalized confusion matrix\n# titles_options = [(\"Confusion matrix, without normalization\", None),\n#                   (\"Normalized confusion matrix\", 'true')]\n\n# #svm with default settings\n# for title, normalize in titles_options:\n#     disp = plot_confusion_matrix(grid_svm, x_val,y_val,\n#                                  display_labels=['Not Survived','Survived'],\n#                                  cmap=plt.cm.Blues,\n#                                  normalize=normalize)\n#     disp.ax_.set_title(title)\n\n#     print(title)\n#     print(disp.confusion_matrix)","a15fef03":"#TPR or Sensitivity\nrecall_score(y_val,svm_pred)*100","85c04a3a":"# #compare precision-Recall score\n# from sklearn.metrics import plot_precision_recall_curve\n\n# #compare both models\n# fig,ax = plt.subplots(1,1,figsize=(12,5))\n# plot_precision_recall_curve(svm, x_val,y_val,ax=ax)\n# ax.set_title('PR Curve')\n# plot_precision_recall_curve(grid_svm, x_val,y_val,ax=ax);","aa825a95":"#Making predictiions using SVM(after tuned model)\nsubmission1 = pd.DataFrame([ID_data,grid_svm.predict(test)]).T\n\nsubmission1.to_csv('sub_svm_tuned.csv',index=False)\nsubmission1.columns = ['Id','Survived']\nsubmission1.head(8)","db9de53a":"Oppps!!!...Stacking didn't give us the better results,Infact stacking tends to overfit,we could try different models instead of tree based models for base learners.\n*we could perform hyperoptimisation to prevent this problems,But,that's computationally expensive (Or) we can try different meta model to compare the results..*","35ca315d":"### Ensemble Methods","e97dd689":"***We will transform some features***\n\nFirst ,we will concat train and test dataset to deal with missing values!!!\nTo deal with missing values and feature engg,we'll combine train and test dataset ","9ef125a8":"# Data visualization :\n\n***First we visualize numerical variables,later we move to categorical variables***","a526b13f":"*It can be very confusing to interpret values from above table.* FIsrt, I grouped by survived feature that means\n\n 1.Passengers who did not survive that `85%` of male passengers and `15%` of female passengers.\n\n 2.passengers who survived that `30%` of male passengers and `70%` of female passengers.\n \nWe can see from different perspective,like,we could groupby gender and calculate the ratio for both classes. Let's see how we can do that stuff.","957ff2eb":"Pclass is negative correlated,that means As pclass increases, age decreases!!\n\nWe want to fill in missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers (imputation).\nHowever we can be smarter about this and check the average age by passenger class. For example:\n","4e033cd7":"*The less family member you have,then chances of survival increases!!!\nBut if you are alone,you have less chance to survive!!*\n\nLets check survival rate with **Sex and Pclass** Together.","f43c8160":"### Analysing fare feature","16bbffd3":"**Logistic Regression model**","c84ee5f8":"It's evident that passengers who has't CABIN are less likely to survive. That means,passengers who has Cabin are most likely to survive. But we must notice the ratio of passengers who has cabin are less on board OR  **the data is missing because after the people died, the information could not be retrieved** . It will definitely cause the overfit issues(and it did).Take a look at evaluation between both models. Both model tend to overfit. So we could remove that feature but i'll keep that!!!!\n\n`These types of variables often are not useful for our predictions, and we should remove them from the set of features that we are going to use to build machine learning models`","84ffe647":"Again,women dominates in this field too,male passengers are less likely to have a cable when compared to female passengers ..","7ed75097":"**Female passengers are less than male passengers on ship!!**","6c8a7d03":"**SVM model performance (with default settings)**","ce8e6fee":"Lets analyse, why tree based models choose Cabin_NA as a second best node to split the dataset. There has to be some explanation for that kind of activities\n","efe172b4":"Factorplot is useful when we deal with categorical features!","d03ce8ed":"*Swarmplot works just like Boxplot but can visualize distribution of given features*","8376daa5":"### Results\nWe will focus on the cross validation accuracy scores as opposed to the regular accuracy scores.","2b5fb881":"### Analysing age feature:","51af6e37":"It is easy to interpret values from these kind of table, we can see that:\n\n(This value taken only from train-set,but if we combine train and validation dataset results may get vary)\n\n1.Female passengers `73%` were survived,only `27%` female passengers were died on titanic disaster.\n\n2.Unlike female passengers, Male passengers only `17%` were survived, remaining `83%` Male passengers were died on titanic disaster.","331d5dda":"**Women passengers have survived more than male passengers...**\n*that's interesting!! Passengers travelling in class 1 survived most while pclass 3 passengers hasn't survived many!!!*","4919c9cc":"*Fare feature is positively skewed,we'll try to normalize this feature later!!*","dfb71c8e":"*Now we can easily say min and max ages of passengers has survived or not by Embarked feature*","7f926ba9":"We can visualize this above correaltion coeff with the help of seaborn Heatmap.Visualizing those numbers will be easy to understand!!\n**In statistics, the correlation coefficient are measures the strength and direction of a linear relationship between two numerical variables**","5945921d":"## Exploratory Data Analysis\nLet's begin some exploratory data analysis! We'll start by dealing with train dataset !!!\n","05e16ecd":"Almost 80 percentage of observations are missing in cabin feature.So we will create a new variable that will capture that obervations are whteher missing or not!","f93c2951":"### Visualizing categorical features:","0efb4f01":"**We fine tuned our model and we improved our model accuray by 1.5% which is quite great!!!**","fe10db3e":"We drop survived column,because since test set don't have survived feature.","4547a0d7":"Yeah,Great improvement in terms of RECALL score which is increased by **7 Percentage** but precision score has decreased by less than 0.5 percentage which won't hurt. Our main aim should be to predict RECALL score as much as High!!","14aa3b7d":"From the above barplot ,we can see male passengers are less likely to survive.If you are female ,the chance of survival increases...\nBut,female passengers on board ratio is less than male passengers...we already have discussed about that..","373f15c5":"# Understanding Titanic dataset","23051e68":"*Violinplot shows the distribution of age better than boxplot!! A violin plot carry all the information that a box plot would \u2014 it literally has a box plot inside the violin*","624caa69":"passsengers who has't cabin are higher than others. That maybe, it'll cost the passengers more (or) **the data is missing because after the people died, the information could not be retrieved.**","7793d26d":"**First class passengers spent more money than other class passengers!!Makes sense,wealthy passengers travels only on first class...**","7a06bd66":"*Looks like our Age feature is normally distributed!!!*\nNow we will analyse age distribution by survived or Not survived passengers!!\n\nFor that,we can leverage seaborn Facetgrid or pandas inbuilt visualization ..","814b38b4":"### Model Performance comparison","0a442f82":"### Visualizing numerical features:\n Numerical features in our dataset - Age,Fare\n \n  in our dataset,Age and fare features are continous variable!!","779c89c4":"We can definitely improve the accuracy of the model by doing experiments with thresholds. Defualt threshold is *0.5* for that we got 79% accuracy ,After done experimenting with threshold, the value of threshold *0.597* which boosted the accuracy of model by `2%`","5d6572ea":"**Support vector machine**","b19ce8d7":"#### Analysing Survived,embarked,Sex,Sibsp,Parch features","0540807d":"# Feature Engineering","0cdd730d":"Boxplot is a way of displaying the distribution of data based on five summary (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d)\n**It can tell you about your outliers and what their values are.**","787ff2e5":"**Tuned SVM model Performance(with best params)**","0212acd3":"After I submitted my reults which derived from best tuned SVM Model, I managed to sit on `Top 30 %` in the leaderboard!!","7dceaad5":"**Combining both box and stripplot ,we can easily detect outliers in our dataset!!! **","b7fa21f3":"**Xgboost - Xtreme gradient boosting**","17ce04a9":"We can our tuned logit model performs better in some areas compared to Default logit model!!!","06eda486":"*Some passengers traveled on ship with free of cost(lucky fellows!!)*\n***Passengers from [C] has spent more money!!!***","5a836f42":"It can be seen,our Rf model is overfitting issues!!!\n\n**Random Forest Tuning:**\n\nwe first need to create a parameter grid to sample from during fitting:","1499b4f6":"**Decision tree model**","84797cbf":"Almost 80% passengers travels on board are students or adults.and their age is 18 to between 40 years.","aca28134":"A violin plot plays a similar role as a box and whisker plot. It shows the distribution of quantitative data across several levels of one (or more) categorical variables such that those distributions can be compared.","c2b794d7":"***Logit model parameter tuning :***","feaa997c":"**Ploting Feature importance**","7fadbc6c":"*Lets see tuned DT model performance whether improved or not.*","6ba55e1e":"*Oldest passenger survived on board is 80 yrs old*","b37da574":"**Interesting,Money played a big role in survival ratio!!!The more we spend,the chances of survival we'll get!!\nMostly,Priority will be given to higher class passengers...**\n\n*Womens spend more money than men!!!Higher class women survived more than middle class women!!*","0781fdba":"All of our models will come from sklearn and the data we are training with is the same, let us create a function to save typing:","d72f3805":"we used factorplot,because they make the seperation of categorical values easy.\n\nLooking at the CrossTab and the FactorPlot, we can easily infer that survival for Women from Pclass1 is about **95-96%**, as only 3 out of 94 Women from Pclass1 died.\n\nIt is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate","0a7aeee5":"We could do one_hot encoding for every label in Cabin variable but the problem will arise when we try to split the dataset into training and validation set.Because most of the labels are very rare(only one time occurs in dataset),when we split, some label won't be present in training set...So we have to split dataset wisely...That's why it's good to create variable that capture null values in a dataset","16bdd0f2":"**Feature Importance**\n\nFeature importance is another way of asking, \"which features contributing most to the outcomes of the model?.\nSince we're using LogisticRegression, we'll look at one way we can calculate feature importance for it.\n","feea4a89":"# Model Building","c1afe0ed":"*The chances for survival for Port C is highest while it is lowest for S.*","bd86f78c":"*We will improve our model performance by just tuning few parameters.\nLuckily,sklearn gridsearch will search the best parameter for us*","a6266ec8":"### Survival Ratio","a3fdf76d":"## Converting Categorical Features \n\nWe'll need to convert categorical features to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs.","492c5eed":"*We have 17 features at this stage","b0dbeaf7":"We can see the wealthier passengers in the higher classes tend to be older, which makes sense. We'll use these average age values to impute based on Pclass for Age","0f6998c3":"Pretty impressive, our model accuracy and roc score has increased noticeably.","6560c4af":"Later on we will be dealing with outliers !!!","b3690eef":"##### We are going to explore train dataset ,then we'll try to concat train and test dataset to deal with missing values","a53f3b17":"**Discretisation (Fare variable)**\n\nDiscretisation is a technique used to convert continous variable to discrete(or Bucket).It can be used as a technique to deal with **outliers**.\n\n**Discretisation helps handle outliers and highly skewed variables**\n\nThere are many ways to discretise a variable.Those are:\n\n         1.Equal width Discretisation\n\n         2.Equal frequency or count discretisation\n\n         3.Domain knowledge discretisation\n\nEach methods has its own pros and cons. But `Equal frequency or count discretisation` is straightforward to implement and by spreading the values of the observations more evenly it may help boost the algorithm's performance and it will depend on the variable and the algorithm that we want to use to make the predictions. It guarantees **same number of observations in each partition\/bin\/interval**\n\nbut i'll stick to my knowledge to create bins(It will help to improve my skills)","a17319e3":"**Recall score(sensitivity or True positive Rate)** \n\n*Out of actual survived passengers,how many of them were correctly classified as **survive** by our model*","7128f790":"## Importing the usual libraries :\n","bd2b5a58":"***Interpreting above graphs:***\n\n**Pclass is negativily correlated with survived features(It means passengerclass increases, chance of survival decreases!!)\n\n**Age is negativily correlated with survived features(It means age increases,chance of survival is low..Older you get,the chance of survival is less!!)\n\n**Not surprisingly,Fare is positively correlated with survived feature,statistically speaking: Fareprice increases, chance of survival increases(It makes perfect sense,wealthy people spend more money,because of that they will get more previllege offers than other passengers!!!)\n\n**More interestingly,passengerID is positively correlated with survived feature!!!(It doesn't matter anyway,since we are gonna drop that feature!!)","15349795":"*Decision tree tuning*","69162198":"## Titanic Disaster - comprehensive EDA to ML \nWe'll be trying to predict a classification- survival or deceased. Let's begin our understanding of implementing our dataset then implement many classification based algo in Python for classification.\n\n**NOTE:** This is my first kernel on Kaggle competition,So if you find anything useful, please do upvote!!!!.if you have any suggestions,please leave a comment!!","e9a3a90d":"***Great! Successfully imputed missing values now move on to feature engg steps(Real)***","59aaab74":"It seems our titanic dataset has 5 object features,5 interger features and 2 float features(Continous)!!!","c2115a90":"Since embarked feature only missing two values,we will fill those values by most common observation","64c34f53":"Well,we created a variable that captured null values in cabin variable...","098a1cbb":"*As suspected , Decision tree is overfiting. we should avoid this kind of issues at any cost...Lets try to find a better parameters to avoid this issues..","09a1dfed":"*From this plot,clearly see -there are some outliers present in our age feature!!!*\nwe can further investigate with boxplot and stripplot in seaborn","6ac4118b":"The precision-recall curve shows the tradeoff between precision and recall for different threshold. it's a plot between precision and recall scores\n\n`Precision = TruePositives \/ (TruePositives + FalsePositives)`\n\n`Recall = TruePositives \/ (TruePositives + FalseNegatives)`\n\nOur `svm model with best parameters` showed us ,no mater what, the Tuned svm model outperformed default SVM model in all the way possible. Lets make a submission and see score on the leaderboard","fd558939":"## Conclusions:\n\n**1.We could experiment with One Hot Encoding and Label Encoding**\n \n In this notebook, we have one hot encoded all the categorical features. Perhaps label encoding would give us better results. Or a comibnation of the two.\n \n**2. We did not do any feature engg**\n\n   We did not perform any feature selection\/feature dropping. I am sure that doing so would greatly increase the performance of the model.\n","81841701":"**True positive Rate**\n\nOur svm with default settings, model performance aren't that good. Only **63 percentage** were correctly identified as they  will survive by model who are likely to be survived. Remaining **37Percenatage** passengers were incorrectly classified as they won't survive by our model.","77b26869":"*Note: As pclass increases ,passengers avg age on ship decreases*","694ab248":"**From above graph,we interpret the relation between age and Fare!!As age increases,Fare prices also increases!!!**\n*Scatterplot helps us to show the relationship between two numerical features*","3eeaadf4":"**Stacking**\n\nWtf is *STACKING*???\n\nStacking is an **ensemble learning** technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The **base level models** are trained based on a complete training set, then the **meta-model** is trained on the *outputs* of the base level model as features.\n\n\n**Fortuantely** ,latest version of scikit learn has made easy to create Stacking models","aed22ff8":"***Random  Forest Model :***","8e9d23fb":"Looks like we got a winner -SVM(rbf)!!! Lets make predictions with svm model... We already tried to search the best parameters for svm...Before make a submission we'll compare the both svm models","beb1176c":"WOW!! That's some magic., Even though Optimized decision tree performs way better than DT with default settings, it fails to beat the  Simple **Logistic** model performance.\n\nThough optimised Decision tree performs well, it overfits.We could prune the tree in order to get a better result","1a4371e0":"Now we can clearly see which features are positively and negatively correlated !!!!!","4529a3d3":"We observe that the percentage of Cabin missing values is higher for people who did not survive (0.87), respect to people that survived (0.60)","4360a3ff":"*From swarm and stripplot ,we can clearly see the #of passengers and distributions from different kinds of embarked!!!*","8397f23b":"**Differnt types of possible Encoding techniques used in DS:**\n\n- One hot encoding (which i used in this dataset)\n- Replacing labels by the count (count or frequency by each category)\n- Ordering labels according to target (ordinal which can be implemented by sklearn Label encoder)\n- Replacing labels by Risk (Aka Mean encoding , take mean(ratio) value by target feature)\n- WoE"}}