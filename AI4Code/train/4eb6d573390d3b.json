{"cell_type":{"81ccd244":"code","8f09be4f":"code","0283a00d":"code","c71df37c":"code","88c7851d":"code","43f4f8af":"code","499406f9":"code","51b845b0":"code","936d3c2d":"code","da8b8fe8":"code","be6b1c85":"code","7bee3442":"code","ccb9d32d":"code","c60b53e7":"code","bfd77454":"code","b6ed14fe":"code","64fa77b7":"code","3585632d":"code","1358454f":"code","798c9120":"code","9105a28b":"code","55a956e0":"code","4be94178":"code","306c7154":"code","0770cf7f":"code","43c3cdc5":"code","07fa7b72":"code","f1e1bdcc":"code","ab410f84":"code","796be249":"code","76023bfa":"code","81c462d3":"code","ae5b40dc":"code","a4aa2d75":"code","2415be37":"markdown","9e01408c":"markdown","af6dff35":"markdown","b9c5983f":"markdown","6ef9a7bb":"markdown","cf381804":"markdown","b0b9f06d":"markdown","eb3b5a9f":"markdown","ef8e8e76":"markdown","b972eac0":"markdown","f3ad3a7d":"markdown","09b955ab":"markdown","d2080e13":"markdown","888d1756":"markdown","18ed66be":"markdown","6446953e":"markdown","ed69897a":"markdown"},"source":{"81ccd244":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import Counter\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport os\nprint(os.listdir(\"..\/input\"))","8f09be4f":"data = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsubm = pd.read_csv('..\/input\/sample_submission.csv')","0283a00d":"data.head()","c71df37c":"data_1 = data.query('difficulty==1')","88c7851d":"data_1.head()","43f4f8af":"alp = pd.Series(Counter(''.join(data_1['ciphertext'])))\nalp.head(10)","499406f9":"len(alp)","51b845b0":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import GridSearchCV","936d3c2d":"X = data_1.drop('target', axis=1)\ny = data_1['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)","da8b8fe8":"def tokenize(text): \n    return text.split(\"1\")\n\nvectorizer = CountVectorizer(\n    analyzer = 'word',\n    tokenizer = tokenize,\n    lowercase = False,\n    ngram_range=(1, 1))\n\nestimator = LogisticRegression(random_state=0)","be6b1c85":"model = Pipeline([('selector', FunctionTransformer(lambda x: x['ciphertext'], validate=False)),\n                  ('vectorizer', vectorizer), \n                  ('tfidf', TfidfTransformer()),\n                  ('estimator', estimator)])","7bee3442":"def generate_tokenizer(separator):\n    def tokenizer(text):\n        return text.split(separator)\n    return tokenizer","ccb9d32d":"tokenize_1 = generate_tokenizer(\"1\")\n\nmodel.steps[1][1].set_params(tokenizer=tokenize_1)","c60b53e7":"model.fit(X_train, y_train)","bfd77454":"y_pred = model.predict(X_test)","b6ed14fe":"f1_score(y_test, y_pred, average='macro')","64fa77b7":"def evaluate_delimiters(data):    \n    X = data.drop('target', axis=1)\n    y = data['target']\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=.2, stratify=y, random_state=0)\n    \n    scores = {}\n    \n    # let's get all the chars that are used:\n    alp = pd.Series(Counter(''.join(data['ciphertext'])))\n\n    for c in alp.keys():\n        tokenize = generate_tokenizer(c)\n        model.steps[1][1].set_params(tokenizer=tokenize)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        score = f1_score(y_test, y_pred, average='macro')\n        scores[c] = score\n    return pd.Series(scores).sort_values(ascending=False)","3585632d":"scores_difficulty_1 = evaluate_delimiters(data.query('difficulty==1'))","1358454f":"scores_difficulty_1[:10]","798c9120":"scores_difficulty_2 = evaluate_delimiters(data.query('difficulty==2'))","9105a28b":"scores_difficulty_2[:10]","55a956e0":"scores_difficulty_3 = evaluate_delimiters(data.query('difficulty==3'))","4be94178":"scores_difficulty_3[:10]","306c7154":"scores_difficulty_4 = evaluate_delimiters(data.query('difficulty==4'))","0770cf7f":"scores_difficulty_4[:10]","43c3cdc5":"book = {'1': ' ',\n '\\x1b': 'e',\n 't': 't',\n 'O': 'a',\n '^': 'o',\n 'a': 'i',\n '\\x02': 'n',\n 'v': 's',\n '#': 'r',\n '0': 'h',\n '8': 'l',\n 's': '\\n',\n 'A': 'd',\n '_': 'c',\n 'c': 'u',\n '-': 'm',\n '\\x08': '.',\n 'q': '-',\n \"'\": 'p',\n 'd': 'g',\n 'o': 'y',\n ']': 'f',\n 'W': 'w',\n '\\x03': 'b',\n 'T': ',',\n 'z': 'v',\n ':': 'I',\n '[': '>',\n 'f': 'k',\n 'G': ':',\n 'L': '1',\n '>': 'S',\n '{': 'T',\n '\/': 'A',\n '\\\\': '0',\n '2': 'C',\n 'y': ')',\n 'e': 'M',\n ';': \"'\",\n '|': '(',\n 'Z': '=',\n 'H': '2',\n '\\x1c': '*',\n '\\x1e': 'R',\n 'x': 'D',\n '\\x7f': 'N',\n '%': 'O',\n 'Q': '\\t',\n '9': 'P',\n 'E': 'E',\n 'F': 'L',\n ')': 'E',\n 'u': '3',\n 'b': '@',\n 'J': 'B',\n '6': '\"',\n 'g': 'H',\n '*': 'F',\n '<': '9',\n '\\t': '5',\n ',': '4',\n '+': 'x',\n 'l': 'W',\n 'X': 'j',\n '5': '6',\n '\"': 'G',\n 'n': '8',\n '@': 'U',\n '&': '?',\n 'h': 'z',\n '?': '\/',\n '\\x06': '7',\n '}': 'J',\n '4': 'J',\n 'P': '!',\n 'w': 'K',\n '\\x18': 'V',\n '\\x10': 'Y',\n '!': 'X',\n '(': 'Y',\n ' ': '<',\n '\\x1a': 'q',\n '`': '>',\n '.': '#',\n 'B': '$',\n '~': '+',\n '3': ';',\n 'V': 'Q',\n 'm': 'q',\n '\\x0c': '%',\n 'U': '[',\n 'i': ']',\n 'r': '&',\n 'K': 'Z',\n 'Y': '~',\n 'I': '}',\n 'k': '{',\n 'S': '\\r',\n '$': '\\x08',\n 'p': '\\x02'}\n","07fa7b72":"dec_table = str.maketrans(book)","f1e1bdcc":"data_1_clean = data.query('difficulty==1').copy()","ab410f84":"data_1_clean['ciphertext'] = data_1_clean['ciphertext'].map(lambda x: x.translate(dec_table))","796be249":"data_1_clean['ciphertext'][1]","76023bfa":"X = data_1_clean.drop('target', axis=1)\ny = data_1_clean['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)","81c462d3":"tokenize_ws = generate_tokenizer(\" \")\nmodel.steps[1][1].set_params(tokenizer=tokenize_ws)\nmodel.fit(X_train, y_train)","ae5b40dc":"y_pred = model.predict(X_test)","a4aa2d75":"f1_score(y_test, y_pred, average='macro')","2415be37":"Let's see the delimiters with highest performance. It is pretty clear that '1' could be the delimiter we are looking for:","9e01408c":"Let's see now for the other difficulties:","af6dff35":"### Difficulty 3","b9c5983f":"## Appendix\nNow that the first cipher has been broken, we are going to see the performance of our model using plain text.  We are going to use the translation table obtained in this nice [kernel](https:\/\/www.kaggle.com\/rturley\/a-first-crack-tools-and-first-cipher-solution) to decrypt completely the texts with difficulty 1:","6ef9a7bb":"Although we get a similar performance, if we have the plain text it is possible to get better models thanks to preprocessing (stemming,  lemmatization, ...)","cf381804":"### Difficulty 4","b0b9f06d":"We obtain almost the same performance than working with the encrypted text and guessing the right delimiter to tokenize: ","eb3b5a9f":"The character '8' could be the encrypted version of the white space delimiter:","ef8e8e76":"## Summary\n\nThe title of this kernel is quite bold and, as all bold affirmations it is not completely true but, it is a good clickbait, isn't it? :-)\n\nI think we only need to identify the white space character. There are some interesting kernels trying to decrypt all the texts but, at least for the case of difficulty 1 (and maybe 2), it wouldn't be strictly neccessary. If we knew how the white space is encrypted we could tokenize properly (to models like Bag of Words the real words doesn't matter: if chars substitution has been applied it should work the same)\n\nI have tried to know what are the most probable encripted white space character using brute force: my assumption is that, the same model (Logistic Regression) should perform better if we tokenize using the right character. ","b972eac0":"As it happend with difficulty 3, the results for difficulty 4 aren't not very clear:","f3ad3a7d":"For difficulty 3 is not clear. This approach could not be suitable for this hader encryptation:","09b955ab":"In the case of difficulty = 1, from this [kernel](https:\/\/www.kaggle.com\/mithrillion\/enigma-was-gimped-by-weather-reports), I saw that the '1' could be the encrypted version of the white space character. Let's check:","d2080e13":"### Difficulty 1","888d1756":"I am going to use the tokenizer for the delimiter \"1\":","18ed66be":"It would be necessary to put this score in context: how the other characters could perform? Let's see:","6446953e":"## Conclusion\nWithout decrypting the full texts we have found that:\n- **Difficulty 1**: the character '1' is the best delimiter to tokenize the texts. It could be the encrypted version of the white space character.\n- **Difficulty 2**: the character '8' is the best delimiter to tokenize the texts. It could be the encrypted version of the white space character.\n\nFor the rest of the cases (difficulty 3 and 4) we can't be completely sure.\nThe model used doesn't provide a great performance but, that wasn't the objective here. Just to identify the possible delimiters to improve the tokenization. There is room to improve the model from that base.","ed69897a":"### Difficulty 2"}}