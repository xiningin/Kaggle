{"cell_type":{"621ce89c":"code","00a9dab8":"code","d7fa01f2":"code","66dfbfed":"code","bd6d2322":"code","bf78d222":"code","7d8a5ee1":"code","40fd6e3b":"code","0e24564e":"code","3eeb3433":"code","999e0035":"code","3ef0d00e":"code","05d69b5f":"code","54843144":"code","a98bc9e9":"code","a5bc3bc4":"markdown","9b89266c":"markdown","ba9dd547":"markdown","f7fb6fbc":"markdown","eb13361d":"markdown","5a5702d2":"markdown","16eac1b4":"markdown","f99d6d57":"markdown","80a8e712":"markdown","ccced297":"markdown","e59d7aaf":"markdown","3470c497":"markdown"},"source":{"621ce89c":"# Global variables for testing changes to this notebook quickly\nRANDOM_SEED = 0\nNUM_FOLDS = 3\nMAX_TREES = 20000\nEARLY_STOP = 150\nNUM_TRIALS = 50","00a9dab8":"# General imports\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nimport time\nimport gc\n\n# Model and evaluation\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom catboost import CatBoostClassifier\nimport catboost\n\n# Optuna\nimport optuna\nfrom optuna.visualization import plot_param_importances, plot_parallel_coordinate\nfrom optuna.pruners import PercentilePruner\n\n# Hide warnings (makes optuna output easier to parse)\nimport warnings\nwarnings.filterwarnings('ignore')","d7fa01f2":"# Helper function for downcasting \ndef reduce_memory_usage(df, verbose=True):\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col, dtype in df.dtypes.iteritems():\n        if dtype.name.startswith('int'):\n            df[col] = pd.to_numeric(df[col], downcast ='integer')\n        elif dtype.name == 'bool':\n            df[col] = df[col].astype('int8')\n        elif dtype.name.startswith('float'):\n            df[col] = pd.to_numeric(df[col], downcast ='float')\n        \n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","66dfbfed":"%%time\n\n# Load training data\ntrain = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas()\ntrain = reduce_memory_usage(train)\n\n# Holdout set for testing our models\ntrain, holdout = train_test_split(\n    train,\n    test_size = 0.5,\n    shuffle = True,\n    stratify = train['target'],\n    random_state = RANDOM_SEED,\n)\n\ntrain.reset_index(drop = True, inplace = True)\nholdout.reset_index(drop = True, inplace = True)\n\n# Save features and categorical features\nfeatures = [x for x in train.columns if x not in ['id','target']]\ncategorical_features = [i for i,x in enumerate(features) if train[x].dtype.name.startswith(\"int\")]","bd6d2322":"# Default CatBoost params, used for ALL models considered\ndefault_params = dict(            \n    random_state = RANDOM_SEED,\n    n_estimators = MAX_TREES,\n    early_stopping_rounds = EARLY_STOP,\n    boosting_type = 'Plain',\n    bootstrap_type = 'Bernoulli',\n    eval_metric = 'Logloss',\n    task_type = 'GPU',\n)","bf78d222":"def score_catboost(trial = None, model_params = {}, fit_params = {}):\n    \n    # Store the holdout predictions\n    holdout_preds = np.zeros((holdout.shape[0],))\n    scores = np.zeros(NUM_FOLDS)\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[features].iloc[train_idx], train['target'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['target'].iloc[valid_idx]\n        \n        start = time.time()\n        \n        # Define Model\n        model = CatBoostClassifier(**{**default_params, **model_params})\n        gc.collect()\n        \n        model.fit(\n            X_train, y_train,\n            verbose = False,\n            eval_set = [(X_valid, y_valid)],\n            cat_features = categorical_features,\n            use_best_model = True,\n            **fit_params\n        )\n        \n        # validation\/holdout predictions\n        valid_preds = model.predict_proba(X_valid)[:, 1]\n        holdout_preds += model.predict_proba(holdout[features])[:, 1] \/ NUM_FOLDS\n        valid_auc = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} AUC: {round(valid_auc, 6)} in {round((end-start) \/ 60, 2)} minutes.')\n        \n        if trial:\n            # Use pruning on fold AUC\n            trial.report(\n                value = valid_auc,\n                step = fold\n            )\n            # prune slow trials and bad fold AUCs\n            if trial.should_prune() or round(end - start, 1) > 480:\n                raise optuna.TrialPruned()\n        \n        time.sleep(0.5)\n        \n    return roc_auc_score(holdout['target'], holdout_preds)","7d8a5ee1":"# Tweak Pruner settings\npruner = PercentilePruner(\n    percentile = 0.66,\n    n_startup_trials = 5,\n    n_warmup_steps = 0,\n    interval_steps = 1,\n    n_min_trials = 5,\n)","40fd6e3b":"def parameter_search(trials):\n    \n    # Optuna objective function\n    def objective(trial):\n        model_params = dict( \n            # default \n            max_depth = trial.suggest_int(\n                \"max_depth\", 2, 8\n            ), \n            # default 0.03\n            learning_rate = trial.suggest_loguniform(\n                \"learning_rate\", 0.009, 0.03\n            ),\n            # default \n            min_child_samples = trial.suggest_int(\n                \"min_child_samples\", 1, 20000\n            ), \n            # default \n            random_strength = trial.suggest_uniform(\n                \"random_strength\", 1, 100\n            ), \n            # default \n            leaf_estimation_iterations = trial.suggest_int(\n                \"leaf_estimation_iterations\", 1, 20\n            ),             \n            subsample = trial.suggest_discrete_uniform(\n                'subsample', 0.2, 1.0, 0.001\n            ),\n            # default 3.0\n            reg_lambda = trial.suggest_loguniform(\n                'reg_lambda', 1e-10, 100\n            ),\n        )\n        \n        return score_catboost(trial, model_params = model_params)\n    \n    optuna.logging.set_verbosity(optuna.logging.DEBUG)\n    study = optuna.create_study(pruner = pruner, direction = \"maximize\")\n    # (nearly) default\n    study.enqueue_trial({\n        'max_depth': 6, \n        'learning_rate': 0.0125730000436306,\n        'min_child_samples': 1, \n        'random_strength': 1, \n        'leaf_estimation_iterations': 10,\n        'subsample': 1.0, \n        'reg_lambda': 3, \n    })\n    study.optimize(objective, n_trials=trials)\n    return study","0e24564e":"study = parameter_search(NUM_TRIALS)","3eeb3433":"print(\"Best Parameters:\", study.best_params)","999e0035":"plot_param_importances(study)","3ef0d00e":"plot_parallel_coordinate(study)","05d69b5f":"%%time\ndel train, holdout; gc.collect()\ntrain = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas()\ntest = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas()\nsubmission = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv').to_pandas()\n\ntrain = reduce_memory_usage(train)\ntest = reduce_memory_usage(test)\ngc.collect()","54843144":"# Similar to scoring function but trains on full data and predicts on test\ndef train_catboost(folds, model_params = {}):\n    \n    # Store the holdout predictions\n    test_preds = np.zeros((test.shape[0],))\n    print('')\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = folds, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n        \n        # Training and Validation Sets\n        start = time.time()\n        X_train, y_train = train[features].iloc[train_idx], train['target'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['target'].iloc[valid_idx]\n        \n        # Define Model\n        model = CatBoostClassifier(**{**default_params, **model_params})\n        gc.collect()\n        \n        model.fit(\n            X_train, y_train,\n            verbose = False,\n            eval_set = [(X_valid, y_valid)],\n            cat_features = categorical_features,\n            use_best_model = True,\n        )\n        \n        # validation and test predictions\n        valid_preds = model.predict_proba(X_valid)[:, 1]\n        test_preds += model.predict_proba(test[features])[:, 1] \/ folds\n        \n        # fold auc score\n        fold_auc = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} AUC: {round(fold_auc, 6)} in {round((end-start) \/ 60, 2)} minutes.')\n        \n    return test_preds","a98bc9e9":"# Make submission\nsubmission['target'] = train_catboost(6, study.best_params)\nsubmission.to_csv('catboost_submission.csv', index=False)","a5bc3bc4":"## 2. Parameter Importances","9b89266c":"## 1. Best Parameters","ba9dd547":"# Make Submission","f7fb6fbc":"## 3. Parallel Coordinate Plot","eb13361d":"# Evaluation","5a5702d2":"# Preparing the Data\n\n1. Load data with `datatable` and convert to `pandas`\n2. Reduce memory usage by downcasting datatypes\n3. Get holdout set from training data using a stratified scheme\n4. Save categorical features","16eac1b4":"# Hyperparameter Search","f99d6d57":"## 2. Scoring Function\n\nWe define a scoring function which performs cross-validation on a training sets and predicts on a holdout set. We prune based on cross-validation and evaluate using the holdout score.\n\n* `trial` - optuna trial object passed if used as part of an optuna trial\n* `model_params` - parameters passed to `CatBoostClassifier`\n* `fit_params` - parameters passed to the `fit` method.","80a8e712":"# CatBoost\n\nWe create a function to train a CatBoost model and return the holdout AUC.\n\n## 1. Default Parameters\n\n* `Bernoulli` bootstrap\n* `Plain` boosting type","ccced297":"# CatBoost Hyperparameter Search\n\nIn this notebook we use optuna to perform a hyperparameter search on a Catboost model with a custom [pruner](https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/pruners.html). Unfortunately, CatBoost does not yet have a built-in [integration](https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/integration.html) like LightGBM and XGBoost.\n\nWe check each set of parameters using k-fold cross validation. Our pruner checks the validation AUC on each fold and compares it to the previously seen models, if our current validation AUC is in the lower half of seen models, we exit the trial early (prune), thus saving some time by not training as many unpromising models.\n\n**Note:** This notebook will take several hours to run. To shorten the runtime adjust `NUM_TRIALS` below.","e59d7aaf":"\n\nHope you found this notebook useful, feel free to fork it and adapt it to your own uses.","3470c497":"## 3. Pruning\n\nThere's no built-in integration for CatBoost but we can still prune based on the fold AUC, which should still save a decent amount of time\n\n* `n_startup_trials` - number of trials (models trained) before pruning starts\n* `n_warmup_steps` - number of iterations before pruning checks\n* `interval_steps` - number of iterations between pruning checks\n* `n_min_trials` - skip pruning check if too few trials"}}