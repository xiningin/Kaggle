{"cell_type":{"2b96570b":"code","7dc2eec7":"code","740b9293":"code","6cd83618":"code","a5a909ef":"code","719ac084":"code","f5601311":"code","0fc85bc9":"code","d43f7b53":"code","39a727c5":"code","e406fb8a":"code","7ec278cb":"code","9fd8472e":"code","b2ca1274":"code","d4505461":"code","0c7c162c":"code","fd867aa4":"code","5e829a3e":"code","00d93f7e":"code","59d9c3ba":"code","315e78bd":"code","c9e7f76d":"code","66833450":"code","029a9460":"code","11062bdc":"code","10b00a41":"code","67ff7403":"code","1ec3dcaf":"code","c27cb7bc":"code","a2753b39":"code","438fa00d":"code","9592fc49":"code","9fc1434f":"code","32a31c10":"code","a009dd10":"code","1f46bca5":"code","0ae82add":"code","bdd3ec3a":"code","20a744c9":"code","1d1ffc1b":"code","bf5cce9e":"code","9ee585bc":"code","295d1ac6":"code","bf085176":"code","52a19157":"code","bc00c2eb":"code","06c0a352":"code","04ea3547":"markdown","658d4d06":"markdown","76dfe40b":"markdown","c45b27ca":"markdown","e439ddf7":"markdown","17001f51":"markdown","94c6e6a8":"markdown","2d307b5e":"markdown","92d317d3":"markdown","b57577c1":"markdown","63f9b301":"markdown","43586408":"markdown","7a30cf41":"markdown","c328238d":"markdown","50241448":"markdown","8d6e88c1":"markdown","cbcf5432":"markdown","9bd0d13f":"markdown","4a3564e4":"markdown","f787e072":"markdown","3e9fa11a":"markdown","aaf046de":"markdown","0aaff27d":"markdown","9c61cde3":"markdown","e5a5320a":"markdown","85e14ac4":"markdown","76b28837":"markdown"},"source":{"2b96570b":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7dc2eec7":"df = pd.read_csv(\"..\/input\/xAPI-Edu-Data\/xAPI-Edu-Data.csv\")\ndf","740b9293":"df.head()","6cd83618":"df.shape","a5a909ef":"df.info()","719ac084":"df.describe()","f5601311":"df.isnull().sum()","0fc85bc9":"df.rename(index=str, columns={'gender':'Gender', \n                              'NationalITy':'Nationality',\n                              'raisedhands':'RaisedHands',\n                              'VisITedResources':'VisitedResources'},\n                               inplace=True)\ndf.columns","d43f7b53":"for i in range(1,17):\n    print(df.iloc[:,i].value_counts())\n    print(\"*\"*20)","39a727c5":"print(\"Class Unique Values : \", df[\"Class\"].unique())\nprint(\"Topic Unique Values : \", df[\"Topic\"].unique())\nprint(\"StudentAbsenceDays Unique Values : \", df[\"StudentAbsenceDays\"].unique())\nprint(\"ParentschoolSatisfaction Unique Values : \", df[\"ParentschoolSatisfaction\"].unique())\nprint(\"Relation Unique Values : \", df[\"Relation\"].unique())\nprint(\"SectionID Unique Values : \", df[\"SectionID\"].unique())\nprint(\"Gender Unique Values : \", df[\"Gender\"].unique())","e406fb8a":"nationality = sns.countplot(x='Nationality', data=df ,palette='Set1')\nnationality.set(xlabel='\u00dclkeler', ylabel='Ki\u015fi Say\u0131s\u0131', title='Ki\u015filerin \u00dclkelere Da\u011f\u0131l\u0131m\u0131')\nplt.setp(nationality.get_xticklabels(), rotation=60)\nplt.show()","7ec278cb":"gender = sns.countplot(x='Class', hue='Gender', data=df, palette='rainbow')\ngender.set(xlabel='S\u0131n\u0131f', ylabel='Ki\u015fi Say\u0131s\u0131', title='S\u0131n\u0131flardaki Cinsiyet Da\u011f\u0131l\u0131m\u0131')\nplt.show()","9fd8472e":"sns.scatterplot(x=\"RaisedHands\", y=\"VisitedResources\", hue = 'Gender', data=df)","b2ca1274":"y=df.Class.values\nx=df.drop(\"Class\",axis=1)\nnp.unique(y)","d4505461":"#x = pd.get_dummies(data = x, columns = ['Class'] , prefix = ['Class'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['StudentAbsenceDays'] , prefix = ['StudentAbsenceDays'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['ParentschoolSatisfaction'] , prefix = ['ParentschoolSatisfaction'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Relation'] , prefix = ['Relation'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['SectionID'] , prefix = ['SectionID'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Gender'] , prefix = ['Gender'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['PlaceofBirth'] , prefix = ['PlaceofBirth'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Semester'] , prefix = ['Semester'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['ParentAnsweringSurvey'] , prefix = ['ParentAnsweringSurvey'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['GradeID'] , prefix = ['GradeID'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['StageID'] , prefix = ['StageID'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Topic'] , prefix = ['Topic'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Nationality'] , prefix = ['Nationality'] , drop_first = False)\nx.head()","0c7c162c":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=52)","fd867aa4":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(random_state=0)\nlr.fit(x_train, y_train)","5e829a3e":"y_pred=lr.predict(x_test)\nprint(y_pred)","00d93f7e":"print(\"test accuracy is {}\".format(lr.score(x_test,y_test)))","59d9c3ba":"#confusion matrix\n#18 ve 50 do\u011fru bilinen de\u011ferler, di\u011fer k\u00f6\u015fegenler yanl\u0131\u015f bilinen de\u011ferlerin say\u0131s\u0131 \nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)","315e78bd":"from sklearn.neighbors import KNeighborsClassifier","c9e7f76d":"error_rate = []\nfor i in range(1,51):    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    pred_i = knn.predict(x_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(8,4))\nplt.plot(range(1,51),error_rate,color='darkred', marker='o',markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","66833450":"knn=KNeighborsClassifier(n_neighbors=8, metric='minkowski')\nknn.fit(x_train,y_train)\n","029a9460":"y_pred=knn.predict(x_test)\nprint(y_pred)","11062bdc":"cm=confusion_matrix(y_test,y_pred)\nprint(cm)","10b00a41":"print(\"test accuracy is {}\".format(knn.score(x_test,y_test)))","67ff7403":"from sklearn.metrics import classification_report\nfrom sklearn.linear_model import SGDClassifier\nsgd =  SGDClassifier(loss='modified_huber', shuffle=True,random_state=101)\nsgd.fit(x_train, y_train)\ny_pred=sgd.predict(x_test)\nprint(sgd.score(x_test,y_test))\nprint('Classification report: \\n',classification_report(y_test,y_pred))","1ec3dcaf":"from sklearn.svm import SVC\nsvm=SVC(kernel='linear', random_state=1)\nsvm.fit(x_train,y_train)\n ","c27cb7bc":"y_pred=svm.predict(x_test)\nprint(y_pred)","a2753b39":"cm=confusion_matrix(y_test,y_pred)\nprint(cm)","438fa00d":"print(\"accuracy of svm algorithm: \",svm.score(x_test,y_test))","9592fc49":"from sklearn.naive_bayes import GaussianNB\ngnb=GaussianNB()\ngnb.fit(x_train,y_train)","9fc1434f":"y_pred=gnb.predict(x_test)","32a31c10":"cm=confusion_matrix(y_test,y_pred)\nprint(cm)","a009dd10":"print(\"Accuracy of naive bayees algorithm: \",gnb.score(x_test,y_test))","1f46bca5":"from sklearn.tree import DecisionTreeClassifier\ndtc=DecisionTreeClassifier()\ndtc.fit(x_train,y_train)","0ae82add":"y_pred=dtc.predict(x_test)\n","bdd3ec3a":"cm=confusion_matrix(y_test,y_pred)\nprint(cm)","20a744c9":"print(\"Accuracy of decision tree algorithm: \",dtc.score(x_test,y_test))","1d1ffc1b":"from sklearn.ensemble import RandomForestClassifier\nresults = []\nn_estimator_options = [20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\nfor trees in n_estimator_options:\n    model = RandomForestClassifier(trees, oob_score=True, n_jobs=-1, random_state=101)\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, n_estimator_options).plot(color=\"darkred\",marker=\"o\")","bf5cce9e":"results = []\nmax_features_options = ['auto',None,'sqrt',0.95,0.75,0.5,0.25,0.10]\nfor trees in max_features_options:\n    model = RandomForestClassifier(n_estimators=75, oob_score=True, n_jobs=-1, random_state=101, max_features = trees)\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, max_features_options).plot(kind=\"bar\",color=\"darkred\",ylim=(0.7,0.9))\n","9ee585bc":"results = []\nmin_samples_leaf_options = [5,10,15,20,25,30,35,40,45,50]\nfor trees in min_samples_leaf_options:\n    model = RandomForestClassifier(n_estimators=75, oob_score=True, n_jobs=-1, random_state=101, max_features = 0.5, min_samples_leaf = trees)\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, min_samples_leaf_options).plot(color=\"darkred\",marker=\"o\")","295d1ac6":"rf = RandomForestClassifier(n_estimators=75, oob_score=True, n_jobs=-1, random_state=101, max_features = 0.5 , min_samples_leaf = 50)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\nprint(rf.score(x_test,y_test))\n\nrf_cm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',rf_cm)\n\nprint('Classification report: \\n',classification_report(y_test,y_pred))","bf085176":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(n_estimators=10, criterion='entropy')\nrfc.fit(x_train,y_train)","52a19157":"y_pred=rfc.predict(x_test)\ny_proba=rfc.predict_proba(x_test)","bc00c2eb":"cm=confusion_matrix(y_test,y_pred)\nprint(cm)\nprint(y_proba[:,0]) ","06c0a352":"print(\"Accuracy of Random Forest algorithm: \",rfc.score(x_test,y_test))","04ea3547":"RandomForestClassifier\nhas parameters like n_estimators,max_features,min_samples_leaf \n\nlet's find the optimum values for these parameters","658d4d06":"Missing Values","76dfe40b":"## Classification","c45b27ca":"## KNN","e439ddf7":"Stochastic Gradient Descent","17001f51":"## Data Preprocessing","94c6e6a8":"If we change the kernel parameters type, we will see these accuracy ;\n* kernel:poly -> accuracy:0.70\n* kernel:rbf -> accuracy:0.59\n* kernel:sigmoid -> accuracy:0.40\n* kernel:linear -> accuracy:0.81\n\n\n Best solution is the linear, that's why we choose this one.","2d307b5e":"## Random Forest","92d317d3":"## Decision Tree","b57577c1":"n_estimators","63f9b301":"max_features","43586408":"## Support Vector Machine","7a30cf41":"### Logistic Regression","c328238d":"we will choose the point 0.5","50241448":"## Naive Bayes","8d6e88c1":"We will choose the point 75","cbcf5432":"One Hot Encoder","9bd0d13f":"RESULT:\n\nthe most convenient algorithm is Random Forest because accuracy value is 0.83\n\nalso linear regression and svm accuracy values are 0.82 \n\nWe can select random forest,linear regression or svm","4a3564e4":"## split","f787e072":"we can see that this grap has a minimum value in point 8","3e9fa11a":"min_samples_leaf_options","aaf046de":"For 'Class' ","0aaff27d":"EDA","9c61cde3":"# Students Academic Performans","e5a5320a":"import libraries","85e14ac4":"import data","76b28837":"point 50 is a min value"}}