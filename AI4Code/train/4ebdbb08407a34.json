{"cell_type":{"817ca25c":"code","6e5cbc8f":"code","d01d8095":"code","e19520aa":"code","589f444c":"code","06a46bb4":"code","198afbb8":"code","f38eb1b6":"code","96405962":"code","f8ea0fb5":"code","77a6fbb8":"code","4655cc32":"code","4780306e":"code","da90b193":"code","a081899e":"code","043c967c":"code","b9fde256":"code","f2f071f6":"code","bc82b3ff":"code","f092c730":"code","b7b0e34a":"markdown","53d41aba":"markdown","23c42e17":"markdown","b7dfb24a":"markdown","0db39fb3":"markdown","31968f0c":"markdown","b99a429c":"markdown","e27a0c29":"markdown","4bf0b2f7":"markdown","51c12df6":"markdown","e2204659":"markdown","cecf5673":"markdown","310254a7":"markdown","16ee96a1":"markdown","e3e9b26f":"markdown","d63e8f50":"markdown","17dca4ac":"markdown","8c825473":"markdown"},"source":{"817ca25c":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt","6e5cbc8f":"np.unique(plt.imread('..\/input\/lyft-udacity-challenge\/dataA\/dataA\/CameraSeg\/02_00_000.png')[:,:,0]*255)","d01d8095":"plt.figure()\n\nplt.imshow(plt.imread('..\/input\/lyft-udacity-challenge\/dataA\/dataA\/CameraRGB\/02_00_000.png'))\nplt.title('RGB image')\nplt.show()\n#subplot(r,c) provide the no. of rows and columns\n#f, axarr = plt.subplots(1,13,figsize=(15,12)) \n\n# use the created array to output your multiple images. In this case I have stacked 4 images vertically\n\nlabels = ['Unlabeled','Building','Fence','Other',\n                 'Pedestrian', 'Pole', 'Roadline', 'Road',\n                 'Sidewalk', 'Vegetation', 'Car','Wall',\n                 'Traffic sign']\n\nfor i in range(13):\n    mask = plt.imread('..\/input\/lyft-udacity-challenge\/dataA\/dataA\/CameraSeg\/02_00_000.png')*255\n    mask = np.where(mask == i, 255, 0)\n    mask = mask[:,:,0]\n    #axarr[i].imshow(mask)\n    plt.title(labels[i])\n    plt.imshow(mask)\n    plt.show()","e19520aa":"cameraRGB = []\ncameraSeg = []\nfor root, dirs, files in os.walk('..\/input\/lyft-udacity-challenge'):\n    for name in files:\n        f = os.path.join(root, name)\n        if 'CameraRGB' in f:\n            cameraRGB.append(f)\n        elif 'CameraSeg' in f:\n            cameraSeg.append(f)\n        else:\n            break","589f444c":"df = pd.DataFrame({'cameraRGB': cameraRGB, 'cameraSeg': cameraSeg})\ndf.sort_values(by='cameraRGB',inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.head(5)","06a46bb4":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torch","198afbb8":"from torch.nn import functional as F","f38eb1b6":"np.unique(F.interpolate(input=torch.from_numpy(\n                               plt.imread('..\/input\/lyft-udacity-challenge\/dataA\/dataA\/CameraSeg\/02_00_000.png')[:,:,0]*255).\\\n                        unsqueeze(0),\n             size=256,\n             mode='nearest'))","96405962":"class CustomDatasetFromImages(Dataset):\n    def __init__(self, data_info):\n        self.data_info = data_info\n        self.image_arr = self.data_info.iloc[:, 0]\n        self.label_arr = self.data_info.iloc[:, 1]\n        self.data_len = len(self.data_info.index)\n\n    def __getitem__(self, index):\n        img = np.asarray(Image.open(self.image_arr[index])).astype('float')\n        img = torch.as_tensor(img)\/255.\n        img = img.unsqueeze(0).permute(0,3,1,2)\n        img = F.interpolate(input=img,size=256,align_corners=False,mode='bicubic')\n        \n        lab = np.asarray(plt.imread(self.label_arr[index])).astype('float')[:,:,0]*255\n        lab = torch.as_tensor(lab).unsqueeze(0)\n        lab = lab.unsqueeze(0)#.permute(0,3,1,2)\n        lab = F.interpolate(input=lab,size=256,mode='nearest')\n\n        return (img.float(), lab.float())\n\n    def __len__(self):\n        return self.data_len","f8ea0fb5":"from sklearn.model_selection import train_test_split\n\nX_train, X_test = train_test_split(df,test_size=0.3)\n\nX_train.reset_index(drop=True,inplace=True)\nX_test.reset_index(drop=True,inplace=True)","77a6fbb8":"train_data = CustomDatasetFromImages(X_train)\ntest_data = CustomDatasetFromImages(X_test)","4655cc32":"train_data_loader = DataLoader(train_data,batch_size=1,shuffle=True)\ntest_data_loader = DataLoader(test_data,batch_size=1,shuffle=False)","4780306e":"!pip install segmentation_models_pytorch","da90b193":"import segmentation_models_pytorch as smp","a081899e":"model = smp.Unet('resnet34', classes=13, activation='softmax')","043c967c":"model = model.float()","b9fde256":"def dice_loss(output, target, weights=None, ignore_index=None):\n    \"\"\"\n    output : NxCxHxW Variable\n    target :  NxHxW LongTensor\n    weights : C FloatTensor\n    ignore_index : int index to ignore from loss\n    \"\"\"\n    eps = 0.0001\n\n    output = output.float().exp()\n    target = target.type(torch.int64)\n    encoded_target = output.detach() * 0\n    if ignore_index is not None:\n        mask = target == ignore_index\n        target = target.clone()\n        target[mask] = 0\n        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n        mask = mask.unsqueeze(1).expand_as(encoded_target)\n        encoded_target[mask] = 0\n    else:\n        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n\n    if weights is None:\n        weights = 1\n\n    intersection = output * encoded_target\n    numerator = 2 * intersection.sum(0).sum(1).sum(1)\n    denominator = output + encoded_target\n\n    if ignore_index is not None:\n        denominator[mask] = 0\n    denominator = denominator.sum(0).sum(1).sum(1) + eps\n    loss_per_channel = weights * (1 - (numerator \/ denominator))\n\n    return loss_per_channel.sum() \/ output.size(1)","f2f071f6":"optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)","bc82b3ff":"model = model.cuda()","f092c730":"for epoch in range(5):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    epoch_loss = []\n    t = 0\n    for i, data in enumerate(train_data_loader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs[0])\n        #print(outputs.shape)\n        #print(labels[0,0,0,:,:].shape)\n        loss = dice_loss(outputs,labels[0,0,:,:,:])\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        epoch_loss.append(loss.item())\n        t+=1\n        if t % 5000 == 4999:    # print every 5000 mini-batches\n\n            samp = int(np.random.randint(low=0,high=len(test_data)-1,size=1))\n            test = model(test_data[samp][0].cuda())\n            truth = test_data[samp][1].cuda()\n            \n            plt.figure()\n\n            #subplot(r,c) provide the no. of rows and columns\n            f, axarr = plt.subplots(1,2) \n            plt.suptitle(f'Epoch: {epoch}, batchcount: {t}, avg. loss for last 5000 images: {running_loss\/5000}')\n            # use the created array to output your multiple images. In this case I have stacked 4 images vertically\n            axarr[0]. imshow(torch.argmax(test.squeeze(), dim=0).detach().cpu().numpy())\n            axarr[0].set_title('Guessed labels')\n            \n            \n            axarr[1].imshow(truth.detach().cpu().numpy()[0,0,:,:])\n            axarr[1].set_title('Ground truth labels')\n            plt.show()\n            plt.gcf().show()\n            running_loss = 0.0\n    print(f'Epoch {epoch+1}, loss: ',np.mean(epoch_loss))","b7b0e34a":"Import initial libraries:","53d41aba":"We go through a single image, showing first the RGB image captured by the onboard camera, and then loop through the mask objects to show each of the 13 mask labels:","23c42e17":"We'll use the Adam optimizer with a learning rate of 1e-3","b7dfb24a":"Move model to GPU device","0db39fb3":"Train for 5 epochs, showing the predicted labels and ground truth labels after every 5000 mini-batches:","31968f0c":"Install a library allowing us to use pretrained segmentation models:","b99a429c":"# Semantic segmentation for Self-driving Cars\n**Gerhard Viljoen - Machine Learning Engineer**","e27a0c29":"Obtain the complete list of RGB images and their corresponding semantic segmentation labels:","4bf0b2f7":"Define a custom dataset object","51c12df6":"Split data into training and test sets:","e2204659":"Define dataloaders for training and test sets:","cecf5673":"We test the assumption that doing a 'nearest' interpolation on the semantic labels maintains its integrity (if we do something like bicubic, we lose the integer labels obtained by multiplying the pixel values by 255)","310254a7":"Some more library imports to work with images, as well as PyTorch:","16ee96a1":"Have a look at the unique labels in the semantic segmentation target:","e3e9b26f":"Cast our model as a float","d63e8f50":"Put this into a DataFrame:","17dca4ac":"We will use a Resnet 34 segmentation model, with 13 classes:","8c825473":"Define custom dice loss for segmentation task:"}}