{"cell_type":{"31efd098":"code","b38cfb5c":"code","3c66bfce":"code","ec9f0598":"code","89e4f802":"code","08ebc1ea":"code","bebefa95":"code","c236d664":"code","ff108d02":"code","4ad24855":"code","e92c79a2":"code","ea8755d0":"code","bf404abe":"code","4473cdb0":"code","a50a4b11":"code","f449cc53":"code","38d63085":"code","4eb30a4a":"code","b915b9d0":"code","9f1c416b":"code","5f379d00":"code","b8b73a6c":"code","5c86ffe7":"code","ae1ef62a":"code","8071f12f":"code","a17dac5c":"code","0bd72d12":"code","db1984b2":"code","a311529a":"code","cce3c4f2":"code","954b012d":"code","1259cff5":"code","c11df54a":"code","5a4b7e43":"code","5422b0b8":"code","8bbec015":"code","1e1011bf":"code","d5221b16":"code","f75a1d97":"code","b4ccf7d1":"code","76142769":"code","e8d1e064":"code","59ca73b2":"code","c42708ad":"code","9febe491":"code","ebe89c08":"code","b5c189bc":"code","72bd2578":"code","ef3467bc":"markdown","d1011a6b":"markdown","35e8a6d8":"markdown","ec05bccd":"markdown","fa276522":"markdown","5f3a995e":"markdown","68423edc":"markdown","31b51184":"markdown"},"source":{"31efd098":"\n# Importing the libraries\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn\nseaborn.set()\n\nimport seaborn as sns\n\nimport pandas as pd\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nimport math\n\n","b38cfb5c":"\n# import data \ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\nall_data = [train, test]\n","3c66bfce":"\ntrain.info()\n","ec9f0598":"#delete the cabin feature\/column and others previously stated to exclude in train dataset\ndrop_column = ['PassengerId','Cabin', 'Ticket']\nfor dataset in all_data:\n    dataset.drop(drop_column, axis=1, inplace = True)\n    \n\n","89e4f802":"test.info()","08ebc1ea":"train.info()","bebefa95":"\nprint('Train columns with null values:\\n', train.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values:\\n', test.isnull().sum())\nprint(\"-\"*10)\n","c236d664":"# COMPLETING: complete or delete missing values in train and test\/validation dataset\nfor dataset in all_data: \n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n# we will fill Age later on after creating a new feature named title\n","ff108d02":"\nfor dataset in all_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])\n","4ad24855":"\nax = plt.subplot()\nax.set_ylabel('Average age')\ntrain.groupby('Title').mean()['Age'].plot(kind='bar',figsize=(13,8), ax = ax)\n","e92c79a2":"\ntitle_mean_age=[]\ntitle_mean_age.append(list(set(train.Title)))  #set for unique values of the title, and transform into list\ntitle_mean_age.append(train.groupby('Title').Age.mean())\nprint(title_mean_age[1])\n#------------------------------------------------------------------------------------------------------\n","ea8755d0":"\n#------------------ Fill the missing Ages ---------------------------\n\n#--------------------------------------------------------------------\n# training dataset\nn_traning= train.shape[0]   #number of rows\nn_titles= len(title_mean_age[1])\nfor i in range(0, n_traning):\n    if np.isnan(train.Age[i])==True:\n        for j in range(0, n_titles):\n            if train.Title[i] == title_mean_age[0][j]:\n                train.Age[i] = title_mean_age[1][j]\n#-------------------------------------------------------------------- \n# test dataset\nn_test= test.shape[0]   #number of rows\nn_titles= len(title_mean_age[1])\nfor i in range(0, n_test):\n    if np.isnan(test.Age[i])==True:\n        for j in range(0, n_titles):\n            if test.Title[i] == title_mean_age[0][j]:\n                test.Age[i] = title_mean_age[1][j]\n                \n","bf404abe":"\nprint('Train columns with null values:\\n', train.isnull().sum())\nprint(\"-\"*20)\n\nprint('Test\/Validation columns with null values:\\n', test.isnull().sum())\nprint(\"-\"*20)\n","4473cdb0":"\nprint(title_mean_age[0])\npd.crosstab(train['Title'], train['Sex'])\n","a50a4b11":"\nfor dataset in all_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean().sort_values(by='Survived', ascending=True)\n","f449cc53":"\nfor dataset in all_data:\n    dataset.drop('Name', axis=1, inplace = True)\n    ","38d63085":"\ntrain.info()\n","4eb30a4a":"\n###CREATE: Feature Engineering for train and test\/validation dataset\nfor dataset in all_data:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes\/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n    \n    ","b915b9d0":"\nmax_ = train.Age.max()\nthres = 0\nsurvival = -1\nmin_person = 50\n\nfor i in range(math.ceil(max_)):\n    count1 = 0\n    count2 = 0\n    temp = train[train['Age'] < i][['Age', 'Survived']].mean()['Survived']\n    \n    count1 = train[train['Age'] < i][['Age', 'Survived']].count()['Survived']\n    count2 = train[train['Age'] > i][['Age', 'Survived']].count()['Survived']\n    \n    if temp > survival and count1 > min_person and count2 > min_person:\n        survival = temp\n        thres = i\n        \nprint('Threshold Age = ', thres)\nprint(train[train['Age'] < thres][['Age', 'Survived']].mean())\nprint(train[train['Age'] > thres][['Age', 'Survived']].mean())\n","9f1c416b":"\nmax = train.Fare.max()\nthres = 0\nsurvival = -1\nmin_person = 50\n\nfor i in range(math.ceil(max)):\n    count1 = 0\n    count2 = 0\n    \n    temp = train[train['Fare'] < i][['Fare', 'Survived']].mean()['Survived']\n    \n    count1 = train[train['Fare'] > i][['Fare', 'Survived']].count()['Survived']\n    count2 = train[train['Fare'] < i][['Fare', 'Survived']].count()['Survived']\n    \n    if temp > survival and count1 > min_person and count2 > min_person:\n        survival = temp\n        thres = i\n        \nprint('Threshold Fare = ',thres)\nprint(train[train['Fare'] < thres][['Fare', 'Survived']].mean())\nprint(train[train['Fare'] > thres][['Fare', 'Survived']].mean())\n","5f379d00":"\nfor dataset in all_data:\n    dataset['Child'] = 1\n    dataset['Child'].loc[dataset['Age'] > 9] = 0\n    \n    \n    dataset['Low_fare'] = 1\n    dataset['Low_fare'].loc[dataset['Fare'] > 107] = 0\n    \n","b8b73a6c":"\n#Map Data\nfor data in all_data:\n\n    #Mapping Sex\n    sex_map = { 'female':0 , 'male':1 }\n    data['Sex'] = data['Sex'].map(sex_map).astype(int)\n\n    #Mapping Title\n    title_map = {'Mr':1, 'Miss':4, 'Mrs':5, 'Master':3, 'Rare':2}\n    data['Title'] = data['Title'].map(title_map)\n    data['Title'] = data['Title'].fillna(0)\n\n    #Mapping Embarked\n    embark_map = {'S':0, 'C':1, 'Q':2}\n    data['Embarked'] = data['Embarked'].map(embark_map).astype(int)\n    \n    ","5c86ffe7":"\ntrain.columns\n","ae1ef62a":"\ndef rel_to_sur(feature):\n    return train[[feature, 'Survived']].groupby([feature], as_index=False).mean().sort_values(by='Survived', ascending=True)\n","8071f12f":"\nfor feature in ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Title', 'FamilySize', 'IsAlone', 'Child', 'Low_fare']:\n    print(rel_to_sur(feature))\n    print('-'*40)\n    ","a17dac5c":"train.dtypes","0bd72d12":"\nplt.subplot()\nplt.hist(x = [train[train['Survived']==1]['Age'], train[train['Survived']==0]['Age']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()\nplt.show()\n","db1984b2":"\nplt.subplots(figsize=(10,8))\nplt.hist(x = [train[train['Survived']==1]['Fare'], train[train['Survived']==0]['Fare']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Fare Histogram by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\nplt.show()\n\n","a311529a":"\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(train)\n","cce3c4f2":"train.shape","954b012d":"test.shape","1259cff5":"\nY = train['Survived']\nX = train.drop(\"Survived\", axis=1)\n","c11df54a":"X.shape, Y.shape","5a4b7e43":"\n# https:\/\/chrisalbon.com\/machine_learning\/model_selection\/hyperparameter_tuning_using_grid_search\/\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n","5422b0b8":"\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n\nclassifier = LogisticRegression()\n\n# Create regularization penalty space\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter space\nC = np.arange(0.1, 2, 0.1)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\n\nhyperparameters\n","8bbec015":"\n# Create grid search using 5-fold cross validation\nclf = GridSearchCV(classifier, hyperparameters, cv=5, verbose=0)\n\n# Fit grid search\nbest_model = clf.fit(X, Y)\n","1e1011bf":"\n# View best hyperparameters\nprint('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\n","d5221b16":"\nbest_model.best_estimator_\n","f75a1d97":"\n# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = best_model.best_estimator_, X=X , y=Y , cv = 10)\nprint(\"Logistic Regression:\\n Accuracy: %0.2f (+\/- %0.2f)\" % (accuracies.mean(), accuracies.std()),\"\\n\")\n","b4ccf7d1":"\npred = best_model.predict(test)\nsample = pd.read_csv('..\/input\/gender_submission.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": sample[\"PassengerId\"],\n        \"Survived\": pred\n    })\n# submission.to_csv('.\/submission_logistic_regression_00.csv', index=False)\n","76142769":"\nfrom sklearn.model_selection import GridSearchCV\n\n# Decision Tree\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nmodel = DecisionTreeClassifier()\n\ncriterion =['gini','entropy']\nmax_depth = [1,3,5,None]\nsplitter = ['best','random']\nmin_samples_leaf = np.arange(1, 20, 1)\nmin_samples_split = np.arange(2, 20, 2)\n\n\n","e8d1e064":"\n\ngrid = GridSearchCV(estimator=model,cv=5, param_grid=dict(criterion=criterion, max_depth=max_depth, \n                                                          splitter=splitter, min_samples_leaf=min_samples_leaf, \n                                                          min_samples_split = min_samples_split))\n\nbest_model_dt = grid.fit(X,Y)\nbest_model_dt.best_estimator_\n","59ca73b2":"\n\naccuracies = cross_val_score(estimator = best_model_dt.best_estimator_, X=X , y=Y, cv = 10)\nprint(\"Decision Tree:\\n Accuracy: %0.2f (+\/- %0.2f)\" % (accuracies.mean(), accuracies.std()),\"\\n\")\n\n","c42708ad":"\npred = best_model_dt.predict(test)\nsample = pd.read_csv('..\/input\/gender_submission.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": sample[\"PassengerId\"],\n        \"Survived\": pred\n    })\n# submission.to_csv('.\/submission_decision_tree_00.csv', index=False)\n","9febe491":"\n# Random forest\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()\n\nn_estimators = [10, 100, 200]\ncriterion =['gini','entropy']\nmax_depth = [1,5,None]\nmin_samples_leaf = [1, 10, 20]\nmin_samples_split = [2, 10, 20]\n\nparam_grid=dict(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth,\n                min_samples_leaf=min_samples_leaf, min_samples_split = min_samples_split)\nparam_grid\n","ebe89c08":"\ngrid = GridSearchCV(estimator=classifier, cv=5, param_grid=param_grid)\n\nbest_model_rf = grid.fit(X,Y)\nbest_model_rf.best_estimator_\n","b5c189bc":"\n\naccuracies = cross_val_score(estimator = best_model_rf.best_estimator_, X=X , y=Y, cv = 10)\nprint(\"Random Forest:\\n Accuracy: %0.2f (+\/- %0.2f)\" % (accuracies.mean(), accuracies.std()),\"\\n\")\n","72bd2578":"\npred = best_model_rf.predict(test)\nsample = pd.read_csv('..\/input\/gender_submission.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": sample[\"PassengerId\"],\n        \"Survived\": pred\n    })\nsubmission.to_csv('.\/submission_random_forrest_00.csv', index=False)\n","ef3467bc":"# Relation to Survival","d1011a6b":"## Random Forest","35e8a6d8":"## Logistic Regression","ec05bccd":"# Hajimemasho","fa276522":"## Decision Tree","5f3a995e":"# Missing Data","68423edc":"# Model Building","31b51184":"# Feature Eng"}}