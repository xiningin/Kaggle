{"cell_type":{"d2ad2b8c":"code","21692372":"code","09a1b88e":"code","8d42d924":"code","66dcb127":"code","d5484cb7":"code","c1e52574":"code","ec791a4d":"code","be747820":"code","8896d0a5":"code","20f070c6":"code","141d6026":"code","e58ea070":"code","8c1f6f71":"code","308e00a4":"code","8c594044":"code","a912c82e":"code","083da058":"code","bd69074d":"code","9cad941e":"code","d94ea6db":"code","e01531b2":"code","95191d83":"code","176a3d06":"code","20f0f1e4":"code","aefa6972":"code","789af34c":"code","7066dbf4":"code","ff51723b":"code","c51c8a6a":"code","5922253c":"code","d1105d13":"code","60103589":"markdown","6c4113f1":"markdown","91f68a88":"markdown","e93aa9c6":"markdown","076aff00":"markdown","6e16dfe2":"markdown","f6172cf0":"markdown","d184889c":"markdown","2a55d6bc":"markdown","d2ba3952":"markdown","9084e86a":"markdown","bf865b2a":"markdown","d02ded0d":"markdown","d6a858a1":"markdown","5aa4a196":"markdown","85bd9597":"markdown","7f08a7c9":"markdown","2b172566":"markdown","326276d2":"markdown","8c809414":"markdown"},"source":{"d2ad2b8c":"!pip install m2cgen --quiet --quiet","21692372":"import numpy as np\nimport pandas as pd\nimport m2cgen as m2c\nimport lightgbm as lgb\nimport xgboost as xgb\nimport ctypes\nimport io\nfrom numpy.ctypeslib import ndpointer","09a1b88e":"PATH = '..\/input\/jane-street-market-prediction'\nNA_REPLACEMENT = -9999.0\nN_TREES = 500\nN_TEST_ROWS_TO_SIMULATE = None # None means all\nN_JOBS = 4","8d42d924":"train = pd.read_csv(f'{PATH}\/train.csv', nrows=300000)\ntrain.shape","66dcb127":"train = train[train['weight'] != 0]\ntrain.shape","d5484cb7":"exclude = [\n    'date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp', 'ts_id'\n]\ninputs = train.drop(exclude, axis=1)\ninputs.shape","c1e52574":"features = inputs.columns\nlen(features)","ec791a4d":"x = inputs.fillna(NA_REPLACEMENT).astype('float64').values.copy('C')\nx.shape","be747820":"y = (train.eval('weight * resp') > 0).astype('int')\ny.shape","8896d0a5":"%%time\nreg = lgb.LGBMRegressor(n_estimators=N_TREES, n_jobs=N_JOBS)\nreg.fit(x, y)","20f070c6":"training_predictions = pd.Series(reg.predict(x))\ntraining_predictions.head()","141d6026":"%%time\ncode = m2c.export_to_c(reg)\nlen(code)","e58ea070":"with open('model.c', 'w') as f:\n    f.write(code)","8c1f6f71":"!wc model.c","308e00a4":"!head model.c","8c594044":"!gcc -Ofast -shared -o lgb_score.so -fPIC model.c\n!ls -l lgb_score.so","a912c82e":"lib = ctypes.CDLL('.\/lgb_score.so')\nscore = lib.score\n# Define the types of the output and arguments of this function.\nscore.restype = ctypes.c_double\nscore.argtypes = [ndpointer(ctypes.c_double)]","083da058":"compiled_predictions = pd.Series([score(row) for row in x])\ncompiled_predictions.head()","bd69074d":"(training_predictions - compiled_predictions).describe()","9cad941e":"one_row = x[[0], :]\none_row.shape","d94ea6db":"%timeit reg.predict(one_row)","e01531b2":"%timeit score(one_row)","95191d83":"class CompiledModel:\n    def __init__(self, library):\n        self.lib = ctypes.CDLL(library)\n        score = self.lib.score\n        score.restype = ctypes.c_double\n        score.argtypes = [ndpointer(ctypes.c_double)]\n        self.score = score\n\n    def predict(self, x):\n        return self.score(x)\n    \nmodel = CompiledModel('.\/lgb_score.so')","176a3d06":"with open(f'{PATH}\/example_test.csv', 'rb') as f:\n    test_csv = f.read()\n\ndef simulator(nrows=N_TEST_ROWS_TO_SIMULATE):\n    for chunk in pd.read_csv(io.BytesIO(test_csv), chunksize=1, nrows=nrows):\n        yield chunk, pd.DataFrame({'action': [0]})","20f0f1e4":"%%time\nlen(list(simulator(nrows=1000)))","aefa6972":"input_row = np.zeros((1, len(features)), dtype=np.float64, order='C')","789af34c":"%%time\nfor (test_df, sample_prediction_df) in simulator():\n    pass","7066dbf4":"%%time\nfor (test_df, sample_prediction_df) in simulator():\n    input_row[:] = test_df[features].fillna(NA_REPLACEMENT)\n    #env.predict(sample_prediction_df)","ff51723b":"%%time\nfor (test_df, sample_prediction_df) in simulator():\n    input_row[:] = test_df[features].fillna(NA_REPLACEMENT)\n    sample_prediction_df.action = (reg.predict(input_row) > 0.5) * 1\n    #env.predict(sample_prediction_df)","c51c8a6a":"%%time\nfor (test_df, sample_prediction_df) in simulator():\n    input_row[:] = test_df[features].fillna(NA_REPLACEMENT)\n    sample_prediction_df.action = (model.predict(input_row) > 0.5) * 1\n    #env.predict(sample_prediction_df)","5922253c":"import janestreet\nenv = janestreet.make_env()  # initialize the environment\niter_test = env.iter_test()  # an iterator which loops over the test set","d1105d13":"%%time\nfor (test_df, sample_prediction_df) in iter_test:\n    input_row[:] = test_df[features].fillna(NA_REPLACEMENT)\n    pred = model.predict(input_row)\n    sample_prediction_df.action = (pred > 0.5) * 1\n    env.predict(sample_prediction_df)","60103589":"Now compile the model into a shared code library.\n\n*This code first seen in this excellent Notebook [fast scoring using C (42 usec)][1] by [sekrier][2] (Python notebook using data from [Santa's Workshop Tour 2019][3]).*\n\n[1]: https:\/\/www.kaggle.com\/sekrier\/fast-scoring-using-c-42-usec\n[2]: https:\/\/www.kaggle.com\/sekrier\n[3]: https:\/\/www.kaggle.com\/c\/santa-workshop-tour-2019","6c4113f1":"# Submit\n\nTry the real submission API just to check the timing against what we saw above.","91f68a88":"We can load the library right away, into our running notebook","e93aa9c6":"# Test 2: Feature Prep Only\n\nSetting up feature array adds only a little time.","076aff00":"# Test 1: Bare Loop","6e16dfe2":"One array - copy features into this and use it for every iteration.","f6172cf0":"# Test 3: Feature Prep and LGB\n\nUsing LightGBM itself we see all four cores are used and are busy for ~18 minutes between them; but 4-5 minutes real time.","d184889c":"# Conclusions\n\n`m2cgen` looks really good!\n\nA 500 tree LGBM model is very fast, nearly all the time is taken up by the slow Python iteration loop.\n\nSome compiled models could benefit from compiler optimisations, for example if a condition like `(input[42] < 1.23456)` appears many times the result can be cached on the stack or in a register and re-used.\n(Compiler optimisations can be impressively complex, making it a hard puzzle to work out how the compiler got from the source code to the machine code!)\n\nSettings are important: deeper trees will require more time per tree.\nLightGBM's default settings do not limit the depth of trees; a deeper tree may require more feature tests on average.\nBut `m2cgen` works with XGBoost and all those other `sklearn` models.\n\nIt's a very easy step to add to the end of your training Notebook, save an `.so` library as an extra output.\nBut it is also flexible: if you save the model as text or a pickle it's possible to \"transpile\" to a language like C later.\n\n### Alternate Uses\n\nEven if you don't want the added complexity of this in your submission code, it might be useful for faster predictions when doing permutation testing (shuffle one column of validation data at a time, make predictions and track the validation metrics to see how important each feature is).\n\nAs mentioned in the [Hacker News thread][1] - super light-weight, dependency free deployment of real systems.\n\n\n### Alternatives\n\nLightGBM has a [convert_model][3] command line option which supports \"cpp\".\n\nSomeone listed other options from that same [Hacker News thread][1]:\n\n> <em><b>jononor<\/b> on Mar 5, 2019 [\u2013]<\/em>\n> \n> Other similar projects:\n> \n> https:\/\/github.com\/nok\/sklearn-porter Supports many scikit-learn models to Java\/C\/JavaScript\/Go\/Ruby, at least since 2016.\n> \n> https:\/\/github.com\/konstantint\/SKompiler transpiles to Excel\/SQL\n> \n> https:\/\/github.com\/jonnor\/emlearn To C only, focus on microcontrollers\/embedded devices. Includes feature extraction tools also. Disclaimer: I wrote it. \n\n**Update** This [recent notebook][5] shows another good alternative for XGBoost models using [Treelite][4]:\n\nhttps:\/\/www.kaggle.com\/code1110\/janestreet-faster-inference-by-xgb-with-treelite\n\n\n[1]: https:\/\/news.ycombinator.com\/item?id=19307740\n[2]: http:\/\/wiki.c2.com\/?PrematureOptimization\n[3]: https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html#convert-parameters\n[4]: https:\/\/treelite.readthedocs.io\/en\/latest\n[5]: https:\/\/www.kaggle.com\/code1110\/janestreet-faster-inference-by-xgb-with-treelite\n","2a55d6bc":"Looks like they match! But test them all:","d2ba3952":"# Simulate\n\nRead the test set into memory then use Pandas to iterate over it in a slow way!","9084e86a":"# Test 4: Feature Prep and Compiled Model\n\nUsing our compiled model with one thread ***is*** faster and adds only 10-15 seconds to \"Test 2: Feature Prep Only\"","bf865b2a":"We just generated over 60,000 lines of source code, in ~10 seconds. Or: 6,000 LoC per *second*. :)\n\nNotice that whilst LGB bins the data columns into histograms for training, the model itself does not refer to them, it holds raw feature values.","d02ded0d":"Some tiny discrepancies - probably to do with how LightGBM uses threading and aggregates results.\n\nNow slice one row of data to test timing:","d6a858a1":"# Transpiling Models with m2cgen\n\nThis is for the [Jane Street Market Prediction][1] competition, which presents us with the unique challenge of predicting for one row of the test set at a time.\n\nHere is an idea that might help: *transpile* models to C\/C++, *compile* that to a library, *load* the library and *execute* native code, from Python.\n\nIt is easier than it sounds! I will use [m2cgen][2], here is their documentation:\n\n<blockquote>\n    <b>m2cgen<\/b> (Model 2 Code Generator) - is a lightweight library which provides an easy way to transpile trained statistical models into a native code (Python, C, Java, Go, JavaScript, Visual Basic, C#, PowerShell, R, PHP, Dart, Haskell, Ruby, F#).\n<\/blockquote>\n\nAs mentioned in this [link][2], they support all these models:\n\n\n### Classification\n[AdaGradClassifier](https:\/\/duckduckgo.com\/?q=AdaGradClassifier),\n[CDClassifier](https:\/\/duckduckgo.com\/?q=CDClassifier),\n[DecisionTreeClassifier](https:\/\/duckduckgo.com\/?q=DecisionTreeClassifier),\n[ExtraTreeClassifier](https:\/\/duckduckgo.com\/?q=ExtraTreeClassifier),\n[ExtraTreesClassifier](https:\/\/duckduckgo.com\/?q=ExtraTreesClassifier),\n[FistaClassifier](https:\/\/duckduckgo.com\/?q=FistaClassifier),\n[KernelSVC](https:\/\/duckduckgo.com\/?q=KernelSVC),\n[LGBMClassifier](https:\/\/duckduckgo.com\/?q=LGBMClassifier),\n[LGBMClassifier](https:\/\/duckduckgo.com\/?q=LGBMClassifier),\n[LinearSVC](https:\/\/duckduckgo.com\/?q=LinearSVC),\n[LinearSVC](https:\/\/duckduckgo.com\/?q=LinearSVC),\n[LogisticRegression](https:\/\/duckduckgo.com\/?q=LogisticRegression),\n[LogisticRegressionCV](https:\/\/duckduckgo.com\/?q=LogisticRegressionCV),\n[NuSVC](https:\/\/duckduckgo.com\/?q=NuSVC),\n[PassiveAggressiveClassifier](https:\/\/duckduckgo.com\/?q=PassiveAggressiveClassifier),\n[Perceptron](https:\/\/duckduckgo.com\/?q=Perceptron),\n[RandomForestClassifier](https:\/\/duckduckgo.com\/?q=RandomForestClassifier),\n[RidgeClassifier](https:\/\/duckduckgo.com\/?q=RidgeClassifier),\n[RidgeClassifierCV](https:\/\/duckduckgo.com\/?q=RidgeClassifierCV),\n[SAGAClassifier](https:\/\/duckduckgo.com\/?q=SAGAClassifier),\n[SAGClassifier](https:\/\/duckduckgo.com\/?q=SAGClassifier),\n[SDCAClassifier](https:\/\/duckduckgo.com\/?q=SDCAClassifier),\n[SGDClassifier](https:\/\/duckduckgo.com\/?q=SGDClassifier),\n[SGDClassifier](https:\/\/duckduckgo.com\/?q=SGDClassifier),\n[SVC](https:\/\/duckduckgo.com\/?q=SVC),\n[XGBClassifier](https:\/\/duckduckgo.com\/?q=XGBClassifier)\nand\n[XGBRFClassifier](https:\/\/duckduckgo.com\/?q=XGBRFClassifier).\n\n### Regression\n[ARDRegression](https:\/\/duckduckgo.com\/?q=ARDRegression),\n[AdaGradRegressor](https:\/\/duckduckgo.com\/?q=AdaGradRegressor),\n[BayesianRidge](https:\/\/duckduckgo.com\/?q=BayesianRidge),\n[CDRegressor](https:\/\/duckduckgo.com\/?q=CDRegressor),\n[DecisionTreeRegressor](https:\/\/duckduckgo.com\/?q=DecisionTreeRegressor),\n[ElasticNet](https:\/\/duckduckgo.com\/?q=ElasticNet),\n[ElasticNetCV](https:\/\/duckduckgo.com\/?q=ElasticNetCV),\n[ExtraTreeRegressor](https:\/\/duckduckgo.com\/?q=ExtraTreeRegressor),\n[ExtraTreesRegressor](https:\/\/duckduckgo.com\/?q=ExtraTreesRegressor),\n[FistaRegressor](https:\/\/duckduckgo.com\/?q=FistaRegressor),\n[GammaRegressor](https:\/\/duckduckgo.com\/?q=GammaRegressor),\n[HuberRegressor](https:\/\/duckduckgo.com\/?q=HuberRegressor),\n[LGBMRegressor](https:\/\/duckduckgo.com\/?q=LGBMRegressor),\n[LGBMRegressor](https:\/\/duckduckgo.com\/?q=LGBMRegressor),\n[Lars](https:\/\/duckduckgo.com\/?q=Lars),\n[LarsCV](https:\/\/duckduckgo.com\/?q=LarsCV),\n[Lasso](https:\/\/duckduckgo.com\/?q=Lasso),\n[LassoCV](https:\/\/duckduckgo.com\/?q=LassoCV),\n[LassoLars](https:\/\/duckduckgo.com\/?q=LassoLars),\n[LassoLarsCV](https:\/\/duckduckgo.com\/?q=LassoLarsCV),\n[LassoLarsIC](https:\/\/duckduckgo.com\/?q=LassoLarsIC),\n[LinearRegression](https:\/\/duckduckgo.com\/?q=LinearRegression),\n[LinearSVR](https:\/\/duckduckgo.com\/?q=LinearSVR),\n[LinearSVR](https:\/\/duckduckgo.com\/?q=LinearSVR),\n[NuSVR](https:\/\/duckduckgo.com\/?q=NuSVR),\n[OrthogonalMatchingPursuit](https:\/\/duckduckgo.com\/?q=OrthogonalMatchingPursuit),\n[OrthogonalMatchingPursuitCV](https:\/\/duckduckgo.com\/?q=OrthogonalMatchingPursuitCV),\n[PassiveAggressiveRegressor](https:\/\/duckduckgo.com\/?q=PassiveAggressiveRegressor),\n[PoissonRegressor](https:\/\/duckduckgo.com\/?q=PoissonRegressor),\n[RANSACRegressor](https:\/\/duckduckgo.com\/?q=RANSACRegressor),\n[RandomForestRegressor](https:\/\/duckduckgo.com\/?q=RandomForestRegressor),\n[Ridge](https:\/\/duckduckgo.com\/?q=Ridge),\n[RidgeCV](https:\/\/duckduckgo.com\/?q=RidgeCV),\n[SAGARegressor](https:\/\/duckduckgo.com\/?q=SAGARegressor),\n[SAGRegressor](https:\/\/duckduckgo.com\/?q=SAGRegressor),\n[SDCARegressor](https:\/\/duckduckgo.com\/?q=SDCARegressor),\n[SGDRegressor](https:\/\/duckduckgo.com\/?q=SGDRegressor),\n[SVR](https:\/\/duckduckgo.com\/?q=SVR),\n[TheilSenRegressor](https:\/\/duckduckgo.com\/?q=TheilSenRegressor),\n[TweedieRegressor](https:\/\/duckduckgo.com\/?q=TweedieRegressor),\n[XGBRFRegressor](https:\/\/duckduckgo.com\/?q=XGBRFRegressor),\nand\n[XGBRegressor](https:\/\/duckduckgo.com\/?q=XGBRegressor).\n\n<!--\npbpaste | perl -ne 'chomp; print \"[\", $_, \"](https:\/\/duckduckgo.com\/?q=$_),\\n\"' | pbcopy\n-->\n\n[2]: https:\/\/github.com\/BayesWitnesses\/m2cgen\n[1]: https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\n","5aa4a196":"In microseconds, we went from over 200 down to ~15, over 10x faster :)\n\nBeware that this is optimistic.\nWe ran a prediction for the same row over and over again.\nCPU's have excellent [branch prediction algorithms][1] that can actually *learn* which pathways in our compiled code will be taken in the future.\n(Learning patterns from data collected in the past to improve performance in the future... hmm.)\n\nThe real test is later - is it actually faster to create a submission?\n\n[1]: https:\/\/en.wikipedia.org\/wiki\/Branch_predictor","85bd9597":"- `m2cgen` does not seem to handle NaN input well so we have to replace it\n- `m2cgen` creates a model that uses the `double` data type which is 64 bits - we *could* train on float32 but I am using the x array for making predictions later and using float64 simplifies things.\n- The compiled model expects a pointer to a single row of input (`double*`), so we need input arrays to be in \"C\" order where rows follow rows in the memory layout","7f08a7c9":"# Timing Test\n\n`reg` is a traditional LGBMRegressor and `score` is our compiled model function.","2b172566":"Just using a subset of data to speed up training.","326276d2":"# Reusable Version","8c809414":"Yep, ~13 seconds for 1000 rows is &nbsp;&nbsp;&nbsp;&nbsp;s&nbsp;&nbsp;&nbsp;l&nbsp;&nbsp;&nbsp;o&nbsp;&nbsp;&nbsp;w"}}