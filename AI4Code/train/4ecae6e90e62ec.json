{"cell_type":{"71dd313e":"code","c0d75ba1":"code","a9304004":"code","0dd48c0b":"code","f762d907":"code","7b63e5b7":"code","b06fc34a":"code","2be57f43":"code","c367ab9b":"code","c21b04cc":"markdown","2ae8af57":"markdown","e27f8017":"markdown","a78539d2":"markdown","ae1d1b39":"markdown","a45c3432":"markdown","0ca0a3e5":"markdown","870090da":"markdown"},"source":{"71dd313e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport surprise  #Scikit-Learn library for recommender systems. \n","c0d75ba1":"raw=pd.read_csv('..\/input\/ratings.csv')\nraw.drop_duplicates(inplace=True)\nprint('we have',raw.shape[0], 'ratings')\nprint('the number of unique users we have is:', len(raw.user_id.unique()))\nprint('the number of unique books we have is:', len(raw.book_id.unique()))\nprint(\"The median user rated %d books.\"%raw.user_id.value_counts().median())\nprint('The max rating is: %d'%raw.rating.max(),\"the min rating is: %d\"%raw.rating.min())\nraw.head()","a9304004":"#swapping columns\nraw=raw[['user_id','book_id','rating']] \n# when importing from a DF, you only need to specify the scale of the ratings.\nreader = surprise.Reader(rating_scale=(1,5)) \n#into surprise:\ndata = surprise.Dataset.load_from_df(raw,reader)","0dd48c0b":"class ProbabilisticMatrixFactorization(surprise.AlgoBase):\n# Randomly initializes two Matrices, Stochastic Gradient Descent to be able to optimize the best factorization for ratings.\n    def __init__(self,learning_rate,num_epochs,num_factors):\n       # super(surprise.AlgoBase)\n        self.alpha = learning_rate #learning rate for Stochastic Gradient Descent\n        self.num_epochs = num_epochs\n        self.num_factors = num_factors\n    def fit(self,train):\n        #randomly initialize user\/item factors from a Gaussian\n        P = np.random.normal(0,.1,(train.n_users,self.num_factors))\n        Q = np.random.normal(0,.1,(train.n_items,self.num_factors))\n        #print('fit')\n\n        for epoch in range(self.num_epochs):\n            for u,i,r_ui in train.all_ratings():\n                residual = r_ui - np.dot(P[u],Q[i])\n                temp = P[u,:] # we want to update them at the same time, so we make a temporary variable. \n                P[u,:] +=  self.alpha * residual * Q[i]\n                Q[i,:] +=  self.alpha * residual * temp \n\n                \n        self.P = P\n        self.Q = Q\n\n        self.trainset = train\n    \n    \n    def estimate(self,u,i):\n        #returns estimated rating for user u and item i. Prerequisite: Algorithm must be fit to training set.\n        #check to see if u and i are in the train set:\n        #print('gahh')\n\n        if self.trainset.knows_user(u) and self.trainset.knows_item(i):\n            #print(u,i, '\\n','yep:', self.P[u],self.Q[i])\n            #return scalar product of P[u] and Q[i]\n            nanCheck = np.dot(self.P[u],self.Q[i])\n            \n            if np.isnan(nanCheck):\n                return self.trainset.global_mean\n            else:\n                return np.dot(self.P[u,:],self.Q[i,:])\n        else:# if its not known we'll return the general average. \n           # print('global mean')\n            return self.trainset.global_mean\n                \n        ","f762d907":"Alg1 = ProbabilisticMatrixFactorization(learning_rate=0.05,num_epochs=4,num_factors=10)\ndata1 = data.build_full_trainset()\nAlg1.fit(data1)\nprint(raw.user_id.iloc[4],raw.book_id.iloc[4])\nAlg1.estimate(raw.user_id.iloc[4],raw.book_id.iloc[4])","7b63e5b7":"gs = surprise.model_selection.GridSearchCV(ProbabilisticMatrixFactorization, param_grid={'learning_rate':[0.005,0.01],\n                                                                            'num_epochs':[5,10],\n                                                                            'num_factors':[10,20]},measures=['rmse', 'mae'], cv=2)\ngs.fit(data)","b06fc34a":"print('rsme: ',gs.best_score['rmse'],'mae: ',gs.best_score['mae'])\nbest_params = gs.best_params['rmse']\nprint('rsme: ',gs.best_params['rmse'],'mae: ',gs.best_params['mae'])","2be57f43":"\nbestVersion = ProbabilisticMatrixFactorization(learning_rate=best_params['learning_rate'],num_epochs=best_params['num_epochs'],num_factors=best_params['num_factors'])","c367ab9b":"#we can use k-fold cross validation to evaluate the best model. \nkSplit = surprise.model_selection.KFold(n_splits=10,shuffle=True)\nfor train,test in kSplit.split(data):\n    bestVersion.fit(train)\n    prediction = bestVersion.test(test)\n    surprise.accuracy.rmse(prediction,verbose=True)","c21b04cc":"Now, lets train the data on the whole dataset, just to gauge the effectiveness of the model we just made.","2ae8af57":"With Surprise, Data Wrangling is almost automated. We'll load the dataset into Pandas, then leverage the surprise package ","e27f8017":"Probabilistic Matrix Factorization is a prediction method where you estimate the best factorization for the Ratings Matrix. Taking an example, lets say there's a rating matrix of 100k users and 10k items. We'll try to factor this dataset into its latent factors. Think of PCA or Singular Value Decompisition. That would be great to perform, but we have a huge issue. Our dataset is FILLED with NANs. This means that SVD and PCA is undefined, and so instead, we'll have to factor this differently. Primarily, we'll initialize two random matrices, one which estimates the altent factors for the user, and one which estimates the latent factors for the items. \n\nIn our example, we initialize a 100k by 40 matrix, to get 40 latent factors out of the 100k users we have, and a 40 by 10k matrix, which estimates the latent factors of our matrix. ","a78539d2":"Surprise lets you implement your own algorithm while still being able to use its very powerful tools for measuring error and choosing hyperparameters. \nTo do so, we have to make a class inheriting from the AlgoBase class. We'll define it along with a fit and estimate method.\nThe fit method uses Stochastic Gradient Descent to be able to estimate the best probabilistic matrix factors.","ae1d1b39":"Creating an Explicit Latent Matrix Factorization Recommender System for Books10k Dataset.\nWe're basing this off of this paper: https:\/\/papers.nips.cc\/paper\/3208-probabilistic-matrix-factorization.pdf ","a45c3432":"Which performed even better than in Grid Search Above, with even 0.2 less Root Means Squared error. Probabalistic Matrix Factorization is powerful tool to use, and Surprise allows us to alleviate some of the pain of data wrangling. ","0ca0a3e5":"Another cool thing about Suprise is it allows you to use their built in GridSearch, which is inspired by sk-learn's GridSearchCV. Its a handy tool that allows us to put in a whole range of hyper-parameters, and picks the best one. \n\nLets use it!","870090da":"Loading dataset into Surprise:\nFor a dataset to be loaded into surprise from a pandas dataframe, you have to specify a reader object and ensure that the columns the dataset are in the order: user,item, rating. "}}