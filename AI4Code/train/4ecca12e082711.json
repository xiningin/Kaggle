{"cell_type":{"48659459":"code","82c306bf":"code","7228885c":"code","b1f7d091":"code","763ed0fb":"code","418363fc":"code","66e348fe":"code","275e6bdb":"code","2cb46069":"code","be117981":"code","2d038b32":"code","dfe3f478":"code","bde7f7e4":"code","ef0ac76d":"code","d6217e0a":"code","57640879":"code","acf29e3e":"code","424f2624":"code","b556b708":"code","c8089d0b":"code","e2b3781b":"code","bc93a830":"code","13770cf8":"code","79ef0832":"code","8d975cbe":"code","c1e4fd5d":"code","5f3e1c01":"code","75d878ea":"code","0db8982d":"code","52340ffe":"code","d71160aa":"markdown","895fe43e":"markdown","107667a1":"markdown","df8107c2":"markdown","bc37a93a":"markdown","7c69fab7":"markdown","4a832b21":"markdown","dadbe18a":"markdown","7ce011a4":"markdown","27b3493a":"markdown"},"source":{"48659459":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix","82c306bf":"df= pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","7228885c":"df.head()","b1f7d091":"# this to show the data type , columns names ,sum of data that not a nans and memory usage . \ndf.info()","763ed0fb":"# show the number of missing values in each column.\ndf.isna().sum().sort_values(ascending=False)","418363fc":"# Removing some columns that have > 28% nan values\nmax_nans=len(df)*0.28\ndf=df.loc[:, (df.isnull().sum(axis=0) <= max_nans)]","66e348fe":"# show the number of the missing values \ndf.isna().sum().sort_values(ascending=False)","275e6bdb":"#filling the missing values by the next value ('bfill') because the temperatures are nearly the same for the next day\ndf.fillna(method='bfill',inplace=True)","2cb46069":"df.drop(columns=['RISK_MM','Date','Location'],inplace=True)","be117981":"# here we change the data type for these two column\ndf['RainTomorrow']=df['RainTomorrow'].astype('category')\ndf['RainToday']=df['RainToday'].astype('category')","2d038b32":"# here we transform them to numeric values.\ndf['RainTomorrow']=df['RainTomorrow'].cat.codes\ndf['RainToday']=df['RainToday'].cat.codes","dfe3f478":" # to know the number of row and columns .\ndf.shape","bde7f7e4":"sns.countplot(df['RainToday'])","ef0ac76d":"sns.countplot(df['RainTomorrow'])","d6217e0a":"g = sns.FacetGrid(df,hue='RainTomorrow',aspect=4)\ng.map(plt.hist,'MinTemp',alpha=0.6,bins=5)\nplt.legend()","57640879":"g = sns.FacetGrid(df,hue='RainTomorrow',height=6,aspect=2)\ng.map(plt.hist,'Humidity3pm',alpha=0.6,bins=5)\nplt.legend()","acf29e3e":"g = sns.FacetGrid(df,hue='RainTomorrow',height=6,aspect=2)\ng.map(plt.hist,'WindGustSpeed',alpha=0.6,bins=5)\nplt.legend()","424f2624":"#select the columns for the model.\nX=df.iloc[:,:16]","b556b708":"X.head()","c8089d0b":"# select our target .\ny=df['RainTomorrow']","e2b3781b":"# here we transform the non_numeric features to make the model dealing with it . \nX=pd.get_dummies(X,columns=['WindDir9am','WindDir3pm','WindGustDir'],drop_first=True)","bc93a830":"#Next, we split 75% of the data to the training set while 25% of the data to test set using below code.\nX_train, X_test, y_train, y_test =  train_test_split(X,y,test_size = 0.25, random_state= 0,stratify=y)","13770cf8":"#we need to bring all features to the same level of magnitudes. This can be achieved by a method called feature scaling.\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X), index=X.index, columns=X.columns)\nX.iloc[4:10]","79ef0832":"#Our next step is to K-NN model and train it with the training data. Here n_neighbors is the value of factor K.\nclassifier = KNeighborsClassifier(n_neighbors = 5)\nclassifier.fit(X_train,y_train)","8d975cbe":"# Let's train our test data and check its accuracy.\ny_pred = classifier.predict(X_test)\nscore = accuracy_score(y_test,y_pred)\nprint('Accuracy :',score)","c1e4fd5d":"# let's see the classification report .\ny_test_pred = classifier.predict(X_test)\nprint(classification_report(y_test,y_test_pred))","5f3e1c01":"#let's show the confusion matrix.\nconfusion_matrix(y_test,y_test_pred)","75d878ea":"classifier = KNeighborsClassifier(n_neighbors = 8, p = 2)\nclassifier.fit(X_train,y_train)","0db8982d":"from sklearn.metrics import accuracy_score\ny_pred = classifier.predict(X_test)\nscore = accuracy_score(y_test,y_pred)\nprint('Accuracy :',score)","52340ffe":"from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\ny_test_pred = classifier.predict(X_test)\nprint(classification_report(y_test,y_test_pred))","d71160aa":"### it's  time to dealing with  missing values ...","895fe43e":"### let's make some visualisation...","107667a1":"### Reading Data .. ","df8107c2":"### it's ML working time ...","bc37a93a":"### for K = 8 :","7c69fab7":"### let's working on the important features and drop some ...","4a832b21":"\n### So we tried more than 3 values on K and  as we can see that Accuracy is maximum that is 83.7 when k = 8  ss\n","dadbe18a":"### For k = 5 : ","7ce011a4":"### importing our libraries ","27b3493a":"### Let's explore our data"}}