{"cell_type":{"0e3c536b":"code","835d071d":"code","c196d76f":"code","4caf22c2":"code","92cbc2df":"code","b9f5e4e4":"code","8f8f6e8b":"code","2783e21a":"code","eb8aee29":"code","5a9dc4a3":"code","291556d3":"code","1b7be832":"code","376baa79":"code","b7d8b4fd":"code","cec7f60d":"code","1e0f3cf5":"code","47a99b45":"code","c857014e":"code","d9faf1b6":"code","56c43eb5":"code","6f1b74c9":"code","6381d271":"code","fd18a172":"code","4b2423f0":"code","6837c092":"code","c551ec3b":"code","174b5ee6":"code","f1249f12":"code","4e284ed3":"code","eec80a27":"code","d92c1dc9":"code","125d87fe":"code","289e8abd":"code","1da0693d":"code","9a87d9d4":"code","74aaadf3":"code","97131b8e":"code","ba5cffe2":"code","d3c1e164":"code","1bc65216":"code","0fa5e7d8":"code","c333df4b":"code","5562fa9f":"code","48524158":"markdown","b357d6bf":"markdown","b57243e7":"markdown","9a8a0d61":"markdown","635d0fcc":"markdown","57f81d78":"markdown","c99733ce":"markdown","e195f16e":"markdown","15ea2315":"markdown","9a2efc78":"markdown","2f411c97":"markdown","806c5e39":"markdown","bbb83680":"markdown","fe624fb2":"markdown","b8709958":"markdown","9d4be697":"markdown","a157b493":"markdown","588beba2":"markdown","4c5149a3":"markdown","ce4e9191":"markdown","892baa0f":"markdown","a02e9c8b":"markdown","206a722d":"markdown","9b96bc8b":"markdown","6bfe3279":"markdown","f69e68aa":"markdown","a9813061":"markdown","c8b9c68d":"markdown","a19feb40":"markdown","7f8616a0":"markdown","1b46d963":"markdown","bcac13ce":"markdown","ff9fdf82":"markdown","b27811a0":"markdown","8b04fda6":"markdown","b317c217":"markdown","34fb44ea":"markdown","93d25282":"markdown","f4a9568d":"markdown","3a5201ba":"markdown"},"source":{"0e3c536b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split, validation_curve, KFold, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import scale\nfrom sklearn.svm import SVC","835d071d":"# Checking version of imported libraries\nnp.__version__, pd.__version__, sns.__version__","c196d76f":"# Making miscellaneous setting for better experience\nimport warnings\nwarnings.filterwarnings('ignore')","4caf22c2":"# Importing training dataset (train.csv)\ntraining_dataframe = pd.read_csv('..\/input\/train.csv')\n\n# Importing testing dataset (test.csv)\ntesting_dataframe = pd.read_csv('..\/input\/test.csv')","92cbc2df":"# Understanding the training dataset | Shape\ntraining_dataframe.shape","b9f5e4e4":"# Understanding the training dataset | Meta Data\ntraining_dataframe.info()","8f8f6e8b":"# Understanding the training dataset | Data Content\ntraining_dataframe.describe()","2783e21a":"# Understanding the training dataset | Sample Data\ntraining_dataframe.head()","eb8aee29":"# Understanding the training dataset | Missing Values\nsum(training_dataframe.isnull().sum())","5a9dc4a3":"# Dropping Duplicate Values\ntraining_dataframe.drop_duplicates(inplace=True)","291556d3":"# Taking a random subset of training dataset (containing 100% of rows from the original dataset)\nrcount = int(1.0*training_dataframe.shape[0])\nsubset_training_dataframe = training_dataframe.sample(n=rcount)","1b7be832":"# Understanding the processed training dataset | Shape\nsubset_training_dataframe.shape","376baa79":"# Clecking if all labels are present almost equally in subset training dataset\nplt.figure(figsize=(8,4))\nsns.countplot(subset_training_dataframe['label'], palette = 'icefire')","b7d8b4fd":"# Checking for collinearity in dataset\nplt.figure(figsize=(16,8))\nsns.heatmap(data=subset_training_dataframe.corr(),annot=False)","cec7f60d":"# splitting into X and y\nX = subset_training_dataframe.drop(\"label\", axis = 1)\ny = subset_training_dataframe.label.values.astype(int)","1e0f3cf5":"# scaling the features\nX = scale(X)","47a99b45":"# split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4)","c857014e":"# confirm that splitting also has similar distribution\nprint(y_train.mean())\nprint(y_test.mean())","d9faf1b6":"# Model building\n\n# instantiate an object of class SVC() using cost C=1, gamma='auto'\nmodel = SVC(C = 1, gamma='auto')\n\n# fit\nmodel.fit(X_train, y_train)\n\n# predict\ny_pred = model.predict(X_test)","56c43eb5":"# Evaluate the model using confusion matrix \nconfusion_matrix(y_true=y_test, y_pred=y_pred)","6f1b74c9":"# Model Accuracy\nprint(\"Accuracy :\", accuracy_score(y_test, y_pred))","6381d271":"# K-Fold Cross Validation\n\n# Creating a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# Instantiating a model with cost=1, gamma='auto'\nmodel = SVC(C = 1, gamma='auto')\n\n# computing the cross-validation scores \n# Argument cv takes the 'folds' object, and we have specified 'accuracy' as the metric\ncv_results = cross_val_score(model, X_train, y_train, cv = folds, scoring = 'accuracy', n_jobs=-1)\n\n# print 5 accuracies obtained from the 5 folds\nprint(cv_results)\nprint(f'mean accuracy = {cv_results.mean()}')","fd18a172":"# Grid Search to Find Optimal Hyperparameter C\n\n# specify range of parameters (C) as a list\nparams = {\"C\": [0.1, 1, 10, 100, 1000]}\n\nmodel = SVC(gamma='auto')\n\n# set up grid search scheme\n# note that we are still using the 5 fold CV scheme we set up earlier\nmodel_cv = GridSearchCV(estimator = model, param_grid = params, \n                        scoring='accuracy', cv=folds, n_jobs=-1,\n                        verbose=1, return_train_score=True)\n\n# fit the model - it will fit 5 folds across all values of C\nmodel_cv.fit(X_train, y_train)  \n\n# results of grid search CV\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","4b2423f0":"# plot of C versus train and test scores\n\nplt.figure(figsize=(4, 4))\nplt.plot(cv_results['param_C'], cv_results['mean_test_score'])\nplt.plot(cv_results['param_C'], cv_results['mean_train_score'])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')","6837c092":"best_score = model_cv.best_score_\nbest_C = model_cv.best_params_['C']\n\nprint(\" The highest test accuracy is {0} at C = {1}\".format(best_score, best_C))","c551ec3b":"# model with the best value of C\nmodel = SVC(C=best_C, gamma='auto')\n\n# fit\nmodel.fit(X_train, y_train)\n\n# predict\ny_pred = model.predict(X_test)","174b5ee6":"# Optimal Final Linear SVM Model Accuracy\nprint(\"Accuracy :\", accuracy_score(y_test, y_pred))","f1249f12":"# Model building\n\n# instantiate an object of class SVC() using cost C=1, Gamma='auto', Kernel='rbf'\nmodel = SVC(C = 1, gamma='auto', kernel='rbf')\n\n# fit\nmodel.fit(X_train, y_train)\n\n# predict\ny_pred = model.predict(X_test)","4e284ed3":"# Evaluate the model using confusion matrix \nconfusion_matrix(y_true=y_test, y_pred=y_pred)","eec80a27":"# Model Accuracy\nprint(\"Accuracy :\", accuracy_score(y_test, y_pred))","d92c1dc9":"# K-Fold Cross Validation\n\n# Creating a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# Instantiating a model with cost=1, Gamma='auto', Kernel='rbf'\nmodelkernel=SVC(C = 1, gamma='auto', kernel='rbf')\n\n# computing the cross-validation scores \n# Argument cv takes the 'folds' object, and we have specified 'accuracy' as the metric\ncv_results = cross_val_score(model, X_train, y_train, cv=folds, scoring='accuracy', n_jobs=-1)\n\n# print 5 accuracies obtained from the 5 folds\nprint(cv_results)\nprint(f'mean accuracy = {cv_results.mean()}')","125d87fe":"# Grid Search to Find Optimal Hyperparameter C, Gamma\n\n# specify range of hyperparameters\n# Set the parameters by cross-validation\nhyper_params = [ {'gamma': [1e-1, 1e-2, 1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]}]\n\n# specify model\nmodel = SVC(kernel=\"rbf\")\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator=model, param_grid=hyper_params, \n                        scoring='accuracy', cv=folds, n_jobs=-1,\n                        verbose=1, return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train, y_train) \n\n# results of grid search CV\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","289e8abd":"# plot of C and Gamma versus train and test scores\n\n# converting C to numeric type for plotting on x-axis\ncv_results['param_C'] = cv_results['param_C'].astype('int')\n\n# plotting\nplt.figure(figsize=(16,4))\n\n# subplot 1\/4\nplt.subplot(141)\ngamma_1 = cv_results[cv_results['param_gamma']==0.1]\n\nplt.plot(gamma_1[\"param_C\"], gamma_1[\"mean_test_score\"])\nplt.plot(gamma_1[\"param_C\"], gamma_1[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.1\")\nplt.ylim([0.0, 1.1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n# subplot 2\/4\nplt.subplot(142)\ngamma_01 = cv_results[cv_results['param_gamma']==0.01]\n\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.01\")\nplt.ylim([0.6, 1.1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n# subplot 3\/4\nplt.subplot(143)\ngamma_001 = cv_results[cv_results['param_gamma']==0.001]\n\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.001\")\nplt.ylim([0.8, 1.1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n# subplot 4\/4\nplt.subplot(144)\ngamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.0001\")\nplt.ylim([0.8, 1.1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')","1da0693d":"best_score = model_cv.best_score_\nbest_hyperparams = model_cv.best_params_\n\nprint(f'The best test score is {best_score} corresponding to hyperparameters {best_hyperparams}')","9a87d9d4":"# model with the best value of C and Gamma\nmodel = SVC(C=best_hyperparams['C'], gamma=best_hyperparams['gamma'], kernel=\"rbf\")\n\n# fit\nmodel.fit(X_train, y_train)\n\n# predict\ny_pred = model.predict(X_test)","74aaadf3":"# Optimal Final Linear SVM Model Accuracy\nprint(\"Accuracy :\", accuracy_score(y_test, y_pred))","97131b8e":"# Predicting values for our Test Split of Training Dataset\ntest_predict = model.predict(X_test)","ba5cffe2":"# Plotting the distribution of our prediction\nd = {'ImageId': np.arange(1,test_predict.shape[0]+1), 'Label': test_predict}\ndataframe_to_export = pd.DataFrame(data=d)\nsns.countplot(dataframe_to_export['Label'], palette = 'icefire')","d3c1e164":"# Les't visualize our Final Model in Action for few unseen images from Training Dataset\n\na = np.random.randint(1,test_predict.shape[0]+1,5)\n\nplt.figure(figsize=(16,4))\nfor k,v in enumerate(a):\n    plt.subplot(150+k+1)\n    _2d = X_test[v].reshape(28,28)\n    plt.title(f'Predicted Label: {test_predict[v]}')\n    plt.imshow(_2d)\nplt.show()","1bc65216":"# Predicting values for unseen Test Dataset\n\n# scaling the features\ntesting_dataframe = scale(testing_dataframe)\n\ntest_predict = model.predict(testing_dataframe)","0fa5e7d8":"# Plotting the distribution of our prediction\nd = {'ImageId': np.arange(1,test_predict.shape[0]+1), 'Label': test_predict}\ndataframe_to_export = pd.DataFrame(data=d)\nsns.countplot(dataframe_to_export['Label'], palette = 'icefire')","c333df4b":"# Les't visualize our Final Model in Action for few images from Test Dataset\n\na = np.random.randint(1,test_predict.shape[0]+1,5)\n\nplt.figure(figsize=(16,4))\nfor k,v in enumerate(a):\n    plt.subplot(150+k+1)\n    _2d = testing_dataframe[v].reshape(28,28)\n    plt.title(f'Predicted Label: {test_predict[v]}')\n    plt.imshow(_2d)\nplt.show()","5562fa9f":"# Exporting the Predicted values for evaluation at Kaggle\ndataframe_to_export.to_csv(path_or_buf='submission.csv', index=False)","48524158":"### Observing the performance of our final Model","b357d6bf":"### Importing Libraries","b57243e7":"**Summary of Dataset Preparation:**\n1. We have 76 (785-709) insignificant columns, however, we are leaving them as it is for later use of plotting image.\n2. We can take a random subset (25% of our dataframe) since the dataset is large, however we took all data for accuracy.","9a8a0d61":"![image.png](attachment:image.png)","635d0fcc":"### Solution Overview","57f81d78":"### Problem Overview","c99733ce":"### Dataset Test and Train Split","e195f16e":"----","15ea2315":"----","9a2efc78":"----","2f411c97":"----","806c5e39":"### Optimizing Hyperparameter C Gamma and Evaluation of Final Non Linear SVM Model","bbb83680":"----","fe624fb2":"----","b8709958":"**Selection of best model:**\n* Accuracy is our primary concern, and there is no major different with Linear or a Non Linear SVM Model.\n* Considering our final model is the Final Non Linear SVM Model","9d4be697":"### Exploratory Data Analysis","a157b493":"----","588beba2":"----","4c5149a3":"----","ce4e9191":"__Handwritten digit recognition using SVM (Linear as well as Non Linear)__","892baa0f":"### Making predictions for Test Dataset using the final Model","a02e9c8b":"**Summary:**\n* Used 100% of the Training Dataset to build the Model\n* There is no major difference between the accuracy of Linear SVM vs. Non-Linear SVM\n* Our final model is a Non-Linear SVM Model since at times we observed 1% improvement in accuracy\n* For our final Model, optimized values of hyperparameters are C = 10 and Gamma = 0.001\n* For our final Model, accuracy is roughly 95%\n* Predictions made using our final Model got a score of 0.94085 upon submission at Kaggle","206a722d":"### Optimizing Hyperparameter C and Evaluation of Final Linear SVM Model","9b96bc8b":"### Importing Dataset","6bfe3279":"----","f69e68aa":"### Table Of Content:\n* [Problem Overview](#Problem-Overview)\n* [Solution Overview](#Solution-Overview)\n* [Importing Libraries](#Importing-Libraries)\n* [Importing Dataset](#Importing-Dataset)\n* [Understanding Dataset](#Understanding-Dataset)\n* [Preparing Dataset](#Preparing-Dataset)\n* [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n* [Dataset Test and Train Split](#Dataset-Test-and-Train-Split)\n* [Building Linear SVM Model](#Building-Linear-SVM-Model)\n* [Optimizing Hyperparameter C and Evaluation of Final Linear SVM Model](#Optimizing-Hyperparameter-C-and-Evaluation-of-Final-Linear-SVM-Model)\n* [Building Non Linear SVM Model](#Building-Non-Linear-SVM-Model)\n* [Optimizing Hyperparameter C Gamma and Evaluation of Final Non Linear SVM Model](#Optimizing-Hyperparameter-C-Gamma-and-Evaluation-of-Final-Non-Linear-SVM-Model)\n* [Observing the performance of our final Model](#Observing-the-performance-of-our-final-Model)\n* [Making predictions for Test Dataset using the final Model](#Making-predictions-for-Test-Dataset-using-the-final-Model)","a9813061":"----","c8b9c68d":"----","a19feb40":"### Building Non Linear SVM Model","7f8616a0":"----","1b46d963":"----","bcac13ce":"### Preparing Dataset","ff9fdf82":"### Understanding Dataset","b27811a0":"----","8b04fda6":"# MNIST Digit recognition (SVM)","b317c217":"### Building Linear SVM Model","34fb44ea":"* We will try to solve the problem firstly using Linear SVM Model.\n* We will optimize the values of Hyper-Parameter (C) to obtain best accuracy.\n* We will then try to solve the problem using Non Linear SVM Model.\n* We will optimize the values of Hyper-Parameters (C, Gamma) to obtain best accuracy.\n* We will compare the performance of each of these models and use the best model to predict for Test Dataset.","93d25282":"**Summary of Exploratory Data Analysis:**\n1. All labels are present almost equally in subset training dataset\n2. Since we see clear pattern in the heatmap, the dataset is highly correlated\n3. Adjacent\/Nearby pixel values are correlated, which we expect as well.","f4a9568d":"**Summary of Dataset Understanding:**\n1. Dataset is clean (no missing values)\n2. Dataset is large (42000 rows and 785 columns)\n3. Dataset is purely numeric (all 785 columns are int64)\n4. Dataset contains insignificant columns (several columns have single value)","3a5201ba":"* You are required to develop a model using Support Vector Machine which should correctly classify the handwritten digits.\n* The digits range from 0-9. The classification is based on the pixel values given as features. \n* Each image is of 28 x 28 pixels, and each pixel forms a feature, there are 784 features. \n* This is a 10-class classification problem. "}}