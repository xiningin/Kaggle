{"cell_type":{"6ec452b8":"code","e489149a":"code","c471c0f5":"code","9d75d628":"code","2017ce0d":"code","7605bf14":"code","7af6d6a8":"code","122c23d9":"code","d394d9c7":"code","8f375a33":"code","eaee4ab6":"code","954c974f":"code","2613f946":"code","2f8e7aca":"code","014b5602":"code","ab96cbf2":"code","4d440fda":"code","b9d52650":"code","aa0ef4a8":"code","c7c472bd":"markdown"},"source":{"6ec452b8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib notebook\n\nimport os\nimport glob\nfrom tqdm import tqdm_notebook\n\nimport librosa\nimport librosa.display\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import to_categorical","e489149a":"!ls ..\/input\/environmental-sound-classification-50\/","c471c0f5":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","9d75d628":"DF_PATH = \"\/kaggle\/input\/environmental-sound-classification-50\/esc50.csv\"\nS44_DIR = \"\/kaggle\/input\/environmental-sound-classification-50\/audio\/audio\/44100\"\nS16_DIR = \"\/kaggle\/input\/environmental-sound-classification-50\/audio\/audio\/16000\"","2017ce0d":"class conf:\n    # Preprocessing settings\n    sampling_rate = 44100\n    duration = 2\n    hop_length = 347*duration # to make time steps 128\n    fmin = 20\n    fmax = sampling_rate \/\/ 2\n    n_mels = 128\n    n_fft = n_mels * 20\n    samples = sampling_rate * duration","7605bf14":"def audio_to_melspectrogram(conf, audio):\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=conf.sampling_rate,\n                                                 n_mels=conf.n_mels,\n                                                 hop_length=conf.hop_length,\n                                                 n_fft=conf.n_fft,\n                                                 fmin=conf.fmin,\n                                                 fmax=conf.fmax)\n    spectrogram = librosa.power_to_db(spectrogram)\n    spectrogram = spectrogram.astype(np.float32)\n    return spectrogram\n\ndef show_melspectrogram(conf, mels, title='Log-frequency power spectrogram'):\n    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n                            fmin=conf.fmin, fmax=conf.fmax)\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(title)\n    plt.show()\n\ndef read_as_melspectrogram(conf, pathname, trim_long_data, debug_display=False):\n    x, sr = librosa.load(pathname , sr = conf.sampling_rate)\n    mels = audio_to_melspectrogram(conf, x)\n    if debug_display:\n        IPython.display.display(IPython.display.Audio(x, rate=conf.sampling_rate))\n        show_melspectrogram(conf, mels)\n    return mels","7af6d6a8":"def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n    # Stack X as [X,X,X]\n    X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    std = std or X.std()\n    Xstd = (X - mean) \/ (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        # Scale to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) \/ (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\ndef convert_wav_to_image(df, source, fold):\n    X = []\n    y = []\n    \n    temp_df = df[df['fold'].isin(fold)]\n    for i, row in tqdm_notebook(temp_df.iterrows()):\n        x = read_as_melspectrogram(conf, os.path.join(source , row.filename), trim_long_data=False)\n        x_color = mono_to_color(x)\n        X.append(x_color)\n        y.append(row.target)\n    return X, y","122c23d9":"df = pd.read_csv(DF_PATH)\ndf","d394d9c7":"df_10 = df[df['esc10'] == True]\ndf_10","8f375a33":"classes = df_10.category.unique()\n\nclass_dict = {x:i for i,x in enumerate(classes)}\ndf_10['target'] = df['category'].map(class_dict)\ndf_10['target'] = df_10['target'].astype('int32')","eaee4ab6":"df_10","954c974f":"x_train , y_train = convert_wav_to_image(df_10 , S44_DIR  ,fold = [1,2,3,4])\nx_val , y_val = convert_wav_to_image(df_10 , S44_DIR , fold = [5])","2613f946":"x_train , x_val  = np.array(x_train), np.array(x_val)","2f8e7aca":"x_train.shape","014b5602":"y_train , y_val = to_categorical(y_train, num_classes=10) , to_categorical(y_val, num_classes=10)","ab96cbf2":"def get_model(f1=3,f2=3):    \n    \n    model = keras.Sequential([\n                                layers.Conv2D(16, (f1,f2), padding= 'same', activation='relu', input_shape=(128,318,3) ),\n                                layers.MaxPooling2D(2, padding='same'),\n                                \n                                layers.Conv2D(32, (f1,f2), padding= 'same', activation='relu'),\n                                layers.MaxPooling2D(2, padding='same'),\n                                layers.Dropout(0.3),\n\n                                layers.Conv2D(64, (f1,f2), padding= 'same', activation='relu'),\n                                layers.MaxPooling2D(2, padding='same'),\n                                layers.Dropout(0.3),\n            \n                                layers.Conv2D(128, (f1,f2),padding='same', activation = 'relu'),\n                                layers.MaxPooling2D(2 ,padding='same'),\n                                layers.Dropout(0.3),\n            \n                                layers.GlobalAveragePooling2D(),\n            \n                                layers.Dense(10, activation='softmax')\n   \n                                ])\n    model.compile(loss= 'categorical_crossentropy', optimizer = 'adam', metrics= ['accuracy'])\n    model.summary()\n\n    return model","4d440fda":"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)","b9d52650":"model_1 = get_model()","aa0ef4a8":"history = model_1.fit(x_train ,y_train , epochs=50, batch_size =32 , shuffle=True, validation_data=(x_val,y_val), callbacks=[callback])","c7c472bd":"some functions are taken from this amazing notebook: https:\/\/www.kaggle.com\/daisukelab\/cnn-2d-basic-solution-powered-by-fast-ai"}}