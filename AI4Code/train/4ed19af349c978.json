{"cell_type":{"fd1246e4":"code","a3ce7463":"code","e6988ac7":"code","6920510d":"code","ea456960":"code","4e62d523":"code","2fa09fa7":"code","bd9d8b7f":"code","11121577":"code","116ac0c1":"code","872e69a6":"code","1ee75c25":"code","d4247034":"code","7ed6930a":"code","4f841f51":"code","b91cfcee":"code","2cd530d7":"code","c1fc7d50":"code","69d8c423":"code","cdfe20c7":"code","02622321":"code","fa8651e2":"code","297d367d":"code","67bf48c9":"code","e6e441ca":"code","6a765d30":"code","08a5a245":"code","038c3627":"code","9822d21a":"code","5b656808":"code","9c956871":"code","eeca44bf":"markdown","bb29205a":"markdown","fca22677":"markdown"},"source":{"fd1246e4":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a3ce7463":"!pip install bert-for-tf2","e6988ac7":"!pip install pytorch-pretrained-bert","6920510d":"!pip install pytorch-nlp\n","ea456960":"import sys\nimport numpy as np\nimport random as rn\nimport pandas as pd\nimport torch\nfrom pytorch_pretrained_bert import BertModel\nfrom torch import nn\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import Adam\nfrom torch.nn.utils import clip_grad_norm_\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n%matplotlib inline","4e62d523":"df = pd.read_csv('\/kaggle\/input\/hate-speech-and-offensive-language-dataset\/labeled_data.csv')\ndf","2fa09fa7":"sub_df = df[['class', 'tweet']]\nsub_df","bd9d8b7f":"train = sub_df[:10000]\ntest = sub_df[:2000]","11121577":"train = train.to_dict(orient='records')\ntest = test.to_dict(orient='records')","116ac0c1":"train_texts, train_labels = list(zip(*map(lambda d: (d['tweet'], d['class']), train)))\ntest_texts, test_labels = list(zip(*map(lambda d: (d['tweet'], d['class']), test)))\n\nlen(train_texts), len(train_labels), len(test_texts), len(test_labels)","872e69a6":"train_texts[0:5]","1ee75c25":"sentences = [len(sent) for sent in train_texts]\n\nplt.rcParams.update({'figure.figsize':(7,5), 'figure.dpi':100})\nplt.bar(range(1,10001), sentences, color = 'b')\nplt.gca().set(title='No. of characters in each sentence', xlabel='Number of sentence', ylabel='Number of Characters in each sentence');","d4247034":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","7ed6930a":"tokenizer.tokenize('This is a sample sentence to test my model. This was done by Rohith')","4f841f51":"train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\ntest_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n\nlen(train_tokens), len(test_tokens)","b91cfcee":"train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\ntest_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n\ntrain_tokens_ids.shape, test_tokens_ids.shape","2cd530d7":"train_y = np.array(train_labels)\ntest_y = np.array(test_labels)\ntrain_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)","c1fc7d50":"train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\ntest_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]","69d8c423":"train_tokens_ids","cdfe20c7":"class BertBinaryClassifier(nn.Module):\n    def __init__(self, dropout=0.1):\n        super(BertBinaryClassifier, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(768, 1)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, tokens, masks=None):\n        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n        dropout_output = self.dropout(pooled_output)\n        linear_output = self.linear(dropout_output)\n        proba = self.sigmoid(linear_output)\n        return proba","02622321":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","fa8651e2":"#Enable Kaggle GPU \n# running BERT on CUDA_GPU\n\nbert_clf = BertBinaryClassifier()\nbert_clf = bert_clf.cuda()    ","297d367d":"BATCH_SIZE = 1\nEPOCHS = 1","67bf48c9":"x = torch.tensor(train_tokens_ids[:3]).to(device)\ny, pooled = bert_clf.bert(x, output_all_encoded_layers=False)\nx.shape, y.shape, pooled.shape","e6e441ca":"train_tokens_tensor = torch.tensor(train_tokens_ids)\ntrain_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n\ntest_tokens_tensor = torch.tensor(test_tokens_ids)\ntest_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n\ntrain_masks_tensor = torch.tensor(train_masks)\ntest_masks_tensor = torch.tensor(test_masks)","6a765d30":"str(torch.cuda.memory_allocated(device)\/1000000 ) + 'M'","08a5a245":"train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\ntrain_sampler = RandomSampler(train_dataset)\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n\ntest_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n\nparam_optimizer = list(bert_clf.sigmoid.named_parameters()) \noptimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]","038c3627":"optimizer = Adam(bert_clf.parameters(), lr=3e-6)","9822d21a":"for epoch_num in range(EPOCHS):\n    bert_clf.train()\n    train_loss = 0\n    for step_num, batch_data in enumerate(train_dataloader):\n        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n        print(str(torch.cuda.memory_allocated(device)\/1000000 ) + 'M')\n        logits = bert_clf(token_ids, masks)\n        \n        loss_func = nn.BCELoss()\n\n        batch_loss = loss_func(logits, labels)\n        train_loss += batch_loss.item()\n        \n        \n        bert_clf.zero_grad()\n        batch_loss.backward()\n        \n\n        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        clear_output(wait=True)\n        print('Epoch: ', epoch_num + 1)\n        print(\"\\r\" + \"{0}\/{1} loss: {2} \".format(step_num, len(train) \/ BATCH_SIZE, train_loss \/ (step_num + 1)))","5b656808":"bert_clf.eval()\nbert_predicted = []\nall_logits = []\nwith torch.no_grad():\n    for step_num, batch_data in enumerate(test_dataloader):\n\n        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n\n        logits = bert_clf(token_ids, masks)\n        loss_func = nn.BCELoss()\n        loss = loss_func(logits, labels)\n        numpy_logits = logits.cpu().detach().numpy()\n        \n        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n        all_logits += list(numpy_logits[:, 0])","9c956871":"from sklearn.metrics import classification_report\nprint(classification_report(test_y, bert_predicted))","eeca44bf":"# Upvote if you find this useful!\n","bb29205a":"The dataset has 3 different columns \n If it is \n- hate speech it is labelled as 0\n- offensive it is labelled as 1\n- neither then it is class 2\n\n\nWe will drop all the columns and keep only the last two columns","fca22677":"## After running the following code, you can go for a cup of coffee :)\n\n#### It will take some time to process"}}