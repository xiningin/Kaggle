{"cell_type":{"d8b6a9aa":"code","9717d636":"code","a43eacc2":"code","72c6b2c8":"code","fc4cd777":"code","2f2f45d9":"code","1658fea2":"code","ef8e607b":"code","93c136d4":"markdown","60b0c659":"markdown","b260f1ba":"markdown","3bddc52d":"markdown","8464a767":"markdown","88520869":"markdown","fe7ebb7a":"markdown","eae3aafc":"markdown","0b702610":"markdown"},"source":{"d8b6a9aa":"from mlbox.preprocessing import *\nfrom mlbox.optimisation import *\nfrom mlbox.prediction import *","9717d636":"paths = [\"..\/input\/train.csv\", \"..\/input\/test.csv\"]\ntarget_name = \"target\"","a43eacc2":"rd = Reader(sep = \",\")\ndf = rd.train_test_split(paths, target_name)   #reading and preprocessing (dates, ...)","72c6b2c8":"dft = Drift_thresholder()\ndf = dft.fit_transform(df)   #removing non-stable features (like ID,...)","fc4cd777":"def gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() \/ totalLosses\n \n    giniSum -= (len(actual) + 1) \/ 2.\n    return giniSum \/ len(actual)\n \ndef gini_normalized(a, p):\n    return np.abs(gini(a, p) \/ gini(a, a))\n\n\nopt = Optimiser(scoring = make_scorer(gini_normalized, greater_is_better=True, needs_proba=True), n_folds=2)","2f2f45d9":"space = {\n    \n        'est__strategy':{\"search\":\"choice\",\n                                  \"space\":[\"LightGBM\"]},    \n        'est__n_estimators':{\"search\":\"choice\",\n                                  \"space\":[700]},    \n        'est__colsample_bytree':{\"search\":\"uniform\",\n                                  \"space\":[0.77,0.82]},\n        'est__subsample':{\"search\":\"uniform\",\n                                  \"space\":[0.73,0.8]},\n        'est__max_depth':{\"search\":\"choice\",\n                                  \"space\":[5,6,7]},\n        'est__learning_rate':{\"search\":\"uniform\",\n                                  \"space\":[0.008, 0.02]} \n    \n        }\n\nparams = opt.optimise(space, df, 7)","1658fea2":"prd = Predictor()\nprd.fit_predict(params, df)","ef8e607b":"submit = pd.read_csv(\"..\/input\/sample_submission.csv\",sep=',')\npreds = pd.read_csv(\"save\/\"+target_name+\"_predictions.csv\")\n\nsubmit[target_name] =  preds[\"1.0\"].values\n\nsubmit.to_csv(\"mlbox.csv\", index=False)","93c136d4":"## ... to predict","60b0c659":"# Inputs & imports : that's all you need to give !","b260f1ba":"But you can also tune the whole Pipeline ! Indeed, you can choose:\n\n* different strategies to impute missing values\n* different strategies to encode categorical features (entity embeddings, ...)\n* different strategies and thresholds to select relevant features (random forest feature importance, l1 regularization, ...)\n* to add stacking meta-features !\n* different models and hyper-parameters (XGBoost, Random Forest, Linear, ...)","3bddc52d":"Hi everyone ! My brand new Python package for Auto Machine Learning is now available on github\/PyPI\/Kaggle kernels ! :)\n\n**https:\/\/github.com\/AxeldeRomblay\/MLBox**\n\n- It is very easy to use (see **documentation** on github)\n- It provides state-of-the-art algorithms and technics such as deep learning\/entity embedding, stacking, leak detection, parallel processing, hyper-parameters optimization...\n- It has already been tested on Kaggle and performs well (see Kaggle \"Two Sigma Connect: Rental Listing Inquiries\" | Rank : **85\/2488**)\n\n**Please put a star on github and fork the script if you like it !** \n\nEnjoy :) ","8464a767":"# That's all !!\n\nIf you like my new auto-ml package, please **put a star on github and fork\/vote the Kaggle script :)**","88520869":"## ... to read and clean all the files ","fe7ebb7a":"### Formatting for submission","eae3aafc":"## ... to tune all the hyper-parameters","0b702610":"# Now let MLBox do the job ! "}}