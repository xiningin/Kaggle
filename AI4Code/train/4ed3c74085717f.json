{"cell_type":{"631cdcb1":"code","5f1e10fa":"code","9b9b8051":"code","f22660d2":"code","324b1623":"code","08aa1367":"code","0a866091":"code","8fe85571":"code","0d493687":"code","b4bcfae6":"code","63d2fccb":"code","8ed75eac":"code","303f4149":"code","41254d75":"code","7368cfdd":"code","e684338d":"code","07bff6b5":"code","72a4b890":"markdown","978a3eca":"markdown","47a452c7":"markdown","02016863":"markdown","1803f94b":"markdown","0ebf1d15":"markdown","fc12ee57":"markdown","cbcc780a":"markdown","5f2da4d1":"markdown","6eb5fc6e":"markdown","d152211f":"markdown","f4198051":"markdown","60349a2e":"markdown"},"source":{"631cdcb1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance","5f1e10fa":"# Global variables\ndata_dir = '\/kaggle\/input\/killer-shrimp-invasion\/'\nRANDOM_STATE = 0","9b9b8051":"# Import data\ntrain = pd.read_csv(data_dir + 'train.csv')\ntest = pd.read_csv(data_dir + 'test.csv')\n\n# Split data into different variables\nY_train = train['Presence']\nID_train = train['pointid']\nX_train = train.drop(['Presence', 'pointid'], axis = 1)\nID_test = test['pointid']\nX_test = test.drop(['pointid'], axis = 1)","f22660d2":"# Fill in NaN values using sklearn imputer\nimputer = IterativeImputer(max_iter = 10, random_state = RANDOM_STATE)\nimputer.fit(X_train)\ncolumn_names = list(X_train.columns)\nX_train = pd.DataFrame(imputer.transform(X_train), columns = column_names)\nX_test = pd.DataFrame(imputer.transform(X_test), columns = column_names)","324b1623":"# If you notice any errors with this function please let me know as it was transcribed by hand from the linked source\ndef ocean_density(temp, salin, depth):\n    # Get approximate pressure at given depth\n    pressure_approx = 1 + (depth \/ 10)\n    \n    # Standard Mean Ocean Water density\n    smow = 999.842594 + (6.793953e-2 * temp) - (9.095290e-3 * (temp ** 2)) + (1.001685e-4 * (temp ** 3)) - (1.120083e-6 * (temp ** 4)) + (6.536332e-9 * (temp ** 5))\n    \n    # Sea water density at normal atmospheric pressure\n    B = 0.82449 - (4.0899e-3 * temp) + (7.6438e-5 * (temp ** 2)) - (8.2467e-7 * (temp ** 3)) + (5.3875e-9 * (temp ** 4))\n    C = -5.7246e-3 + (1.0227e-4 * temp) + (-1.6546e-6 * (temp ** 2))\n    D = 4.8314e-4\n    density_normal = smow + (B * salin) + (C * (salin ** 1.5)) + (D * (salin ** 2))\n    \n    # Determination of compression module at pressure 0\n    Kw = 19652.21 + (148.4206 * temp) + (-2.327105 * (temp ** 2)) + (1.360477e-2 * (temp ** 3)) + (-5.155288e-5 * (temp ** 4))\n    F = 54.6746 + (-0.603459 * temp) + (1.099870e-2 * (temp ** 2)) + (-6.167e-5 * (temp ** 3))\n    G = 7.9440e-2 + (1.6483e-2 * temp) + (-5.3009e-4 * (temp ** 2))\n    K_0 = Kw + (F * salin) + (G * (salin ** 1.5))\n    \n    # Determination of final compressibility module\n    Aw = 3.23990 + (1.43713e-3 * temp) + (1.16092e-4 * (temp ** 2)) + (-5.77905e-7 * (temp ** 3))\n    A1 = Aw + ((2.28380e-3 + (-1.09810e-5 * temp) + (-1.60780e-6 * (temp ** 2))) * salin) + (1.91075e-4 * (salin ** 1.5))\n    Bw = 8.50935e-5 + (-6.12293e-6 * temp) + (5.27870e-8 * (temp ** 2))\n    B2 = Bw + (-9.9348e-7 + (2.0816e-8 * temp) + (9.1697e-10 * (temp ** 2))) * salin\n    K = K_0 + (A1 * pressure_approx) + (B2 * (pressure_approx ** 2))\n    \n    return (density_normal \/ (1 - (pressure_approx \/ K))) - 1000","08aa1367":"X_train['Density'] = X_train.apply(lambda x: ocean_density(x['Temperature_today'], x['Salinity_today'], x['Depth']), axis = 1)\nX_test['Density'] = X_test.apply(lambda x: ocean_density(x['Temperature_today'], x['Salinity_today'], x['Depth']), axis = 1)","0a866091":"def eunis_classification(exposure):\n    if exposure <= 1200:\n        return 'Ultra Sheltered'\n    elif exposure <= 4000:\n        return 'Extremely Sheltered'\n    elif exposure <= 10000:\n        return 'Very Sheltered'\n    elif exposure <= 100000:\n        return 'Sheltered'\n    elif exposure <= 500000:\n        return 'Moderately Exposed'\n    elif exposure <= 1000000:\n        return 'Exposed'\n    elif exposure <= 2000000:\n        return 'Very Exposed'\n    else:\n        return 'Extremely Exposed'","8fe85571":"for classifi in ['Ultra Sheltered', 'Extremely Sheltered', 'Very Sheltered', 'Sheltered', 'Moderately Exposed', 'Exposed', 'Very Exposed']:\n    X_train['EUNIS ' + classifi] = X_train['Exposure'].apply(lambda x: 1 if eunis_classification(x) == classifi else 0)\n    X_test['EUNIS ' + classifi] = X_test['Exposure'].apply(lambda x: 1 if eunis_classification(x) == classifi else 0)\n\nX_train['EUNIS Min Very Exposed'] = X_train['Exposure'].apply(lambda x: 1 if eunis_classification(x) == 'Extremely Exposed' or eunis_classification(x) == 'Very Exposed' else 0)\nX_test['EUNIS Min Very Exposed'] = X_test['Exposure'].apply(lambda x: 1 if eunis_classification(x) == 'Extremely Exposed' or eunis_classification(x) == 'Very Exposed' else 0)","0d493687":"def temperature_equation(t):\n    coefs = [-10.52910058, 2.21529641, -0.57623745, -2.06832485, 0.30713969, -2.21781761, -1.85363275]\n    temps = [1, t, t**2, t**3, t**4, t**5, t**6]\n    summed = np.dot(coefs, temps)\n    return 1 \/ (1 + np.exp(summed))","b4bcfae6":"X_train['Temperature Model'] = X_train['Temperature_today'].apply(lambda x: temperature_equation(x))\nX_test['Temperature Model'] = X_test['Temperature_today'].apply(lambda x: temperature_equation(x))","63d2fccb":"# Wrapper to remove outliers\nclass OutlierFeature():\n    def __init__(self, ratio, outlier_features):\n        self.ratio = ratio\n        self.outlier_features = outlier_features\n    \n    def exclude_outliers(self, row):\n        lower_multiplier = self.ratio\n        upper_multiplier = 1 \/ self.ratio\n        conditions = []\n        for feature, cmin, cmax in self.outlier_conditions:\n            conditions.append(row[feature] > cmin)\n            conditions.append(row[feature] < cmax)\n        return 1 if sum(conditions) == len(conditions) else 0\n    \n    def fit(self, x, y):\n        self.outlier_conditions = []\n        for feature in self.outlier_features:\n            fmin = x[y == 1][feature].min()\n            fmax = x[y == 1][feature].max()\n            lower_multiplier = self.ratio\n            upper_multiplier = 1 \/ self.ratio\n            \n            cmin = fmin * lower_multiplier if fmin > 0 else fmin * upper_multiplier\n            cmax = fmax * upper_multiplier if fmax > 0 else fmax * lower_multiplier\n            self.outlier_conditions.append((feature, cmin, cmax))\n    \n    def transform(self, x):\n        outliers = x.apply(lambda x: self.exclude_outliers(x), axis = 1)\n        return outliers","8ed75eac":"outlier_finder = OutlierFeature(0.82, ['Temperature_today', 'Salinity_today', 'Exposure', 'Depth'])\noutlier_finder.fit(X_train, Y_train)\nX_train['Outlier'] = outlier_finder.transform(X_train)\nX_test['Outlier'] = outlier_finder.transform(X_test)","303f4149":"# K-Fold Cross Validation\ndef five_fold_cv(model, X_train, Y_train, verbose = True):\n    skf = StratifiedKFold(n_splits = 5)\n    fold = 1\n    scores = []\n    \n    for train_index, test_index in skf.split(X_train, Y_train):\n        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n        Y_train_fold, Y_test_fold = Y_train.iloc[train_index], Y_train.iloc[test_index]\n\n        model.fit(X_train_fold, Y_train_fold)\n\n        preds = model.predict_proba(X_test_fold)\n        preds = [x[1] for x in preds]\n\n        score = roc_auc_score(Y_test_fold, preds)\n        scores.append(score)\n        if verbose:\n            print('Fold', fold, '     ', score)\n        fold += 1\n    \n    avg = np.mean(scores)\n    if verbose:\n        print()\n        print('Average:', avg)\n    return avg","41254d75":"features = ['Temperature_today', 'Salinity_today', 'Depth', 'EUNIS Ultra Sheltered', 'EUNIS Extremely Sheltered', 'EUNIS Very Sheltered', 'EUNIS Sheltered', 'EUNIS Moderately Exposed', 'EUNIS Exposed', 'EUNIS Very Exposed', 'Substrate', 'Outlier', 'Temperature Model', 'Density', 'EUNIS Min Very Exposed']","7368cfdd":"model = XGBClassifier(random_state = RANDOM_STATE, eval_metric = 'auc', objective = 'binary:logistic',\n                          learning_rate = 0.3, max_depth = 5, subsample = 1, reg_lambda = 0.5)\n\nscore = five_fold_cv(model, X_train[features], Y_train, verbose=True)","e684338d":"preds = pd.DataFrame(ID_test, columns=['pointid'])\n\nmodel.fit(X_train[features], Y_train)\npreds['Presence'] = model.predict_proba(X_test[features])[:,1]\n\npreds[['pointid', 'Presence']].to_csv('preds.csv', index=False)","07bff6b5":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\nplot_importance(model, importance_type='weight', height=0.5, xlabel='Weight', title='Feature Importance by Weight', ax=ax1)\nplot_importance(model, importance_type='gain', height=0.5, xlabel='Gain', title='Feature Importance by Gain', ax=ax2)\nplt.tight_layout()\nplt.show()","72a4b890":"# Fill In Missing Data\nAn EDA of the dataset would show that there is a lot of missing data. This is shown well in [the notebook by Fabio Chiusano](https:\/\/www.kaggle.com\/fabiochiusano\/eda-missing-data-imputation-simple-model). The notebook also gives a way for imputing values for this missing data - using the sklearn IterativeImputer. In this notebook we will use the same as it performs better than more simple methods such as filling in with the mean or median.","978a3eca":"## Temperature-Only Model Equation\nA full explanation of this feature can be found in [the linked notebook](https:\/\/www.kaggle.com\/cwthompson\/killer-shrimp-temperature-only-model). In the notebook, a logistic regression model is fitted to the training data using only the temperature feature. The resulting equation gives a decent model for the problem, but can also be used as an additional feature. Similar equations can be calculated for other features such as depth or salinity but the cross validation score is not as high as that for temperature. The cross validation score achieved for the temperature equation was approximately 0.87. The linked notebook shows that a salinity model only had an approximate score of 0.58. The temperature equation is given below:\n\n$$ p(Shrimp) = \\frac{1}{1 + e^{-f(t)}}$$  \n\n$$ f(t) = -10.52910058 + 2.21529641*t - 0.57623745*t^2 - 2.06832485*t^3\\\\ + 0.30713969*t^4 - 2.21781761*t^5 - 1.85363275*t^6$$","47a452c7":"Now we will create an XGB model and check its performance by doing cross validation. The specific model we are creating actually scores 0.99984 on the public leaderboard, the cross validation score is slightly below this but still high.","02016863":"# Feature Engineering\nWe only have five features in the training data (temperature, salinity, depth, exposure, and substrate).  We will use feature engineering to create more features. Those used in this notebook are:\n - Ocean Density\n - EUNIS Exposure Classifications (for which we actually create nine features)\n - Temperature Equation\n - Outlier (Bounded-Box)\n\nEach is dealt with in a separate cell.\n## Ocean Density\nOcean density can be calculated as a function of temperature, salinity, and depth. More information on [how to do this can be found here](https:\/\/link.springer.com\/content\/pdf\/bbm%3A978-3-319-18908-6%2F1.pdf). The interquartile range for ocean density is 3.77 to 5.31 in the training data, but this shifts to 4.19 to 5.42 when only considering positive instances. The main disadvantage of this feature, however, is the high linear correlation with salinity. The high correlation means that our model will draw similar conclusions from salinity and density, so using both may not necessarily be useful.","1803f94b":"We will also define a list of features that will be used in the model.","0ebf1d15":"With the previously defined data_dir, we can now import our training and test data. We will also split the data into X (features), Y (target variable), and IDs.","fc12ee57":"## Outlier (Bounded Box) Feature\nInformation on the habitat of Killer Shrimp is given in [the FAQ of the competition](https:\/\/www.kaggle.com\/c\/killer-shrimp-invasion\/overview\/faq). Notably they:\n - tolerate salinity up to 20ppt (12ppt is optimal)\n - tolerate temperatures between 0 and 35 degress Celsius (5-15 degrees is optimum)\n - are thought to occupy every substratum except for sand\n - are present in areas of low current velocity\n\nHard-coding these conditions into the model poses difficulties, particularly with the substratum (training data suggests they are found in sand substratum). Instead, we will create a bounded box around the data using our training data. Since it is possible that unseen data (such as our test data) could fall outside of this bounded box, we also introduce a ratio that expands the bounding box slightly. The ratio of 0.82 in this notebook was chosen experimentally.\n\nExperimentally, this outlier feature had greater use in models that used fewer other features. As the number of features increased, the model was better able to remove outliers without the explicit need for an outlier feature.","cbcc780a":"In the next cell we will set a few global variables. The data_dir is where we are storing our input data (training and test data). The random state is used throughout to remove the randomness of some algorithms, it will allow us to reproduce our results.","5f2da4d1":"# Models\nFirstly we will define a function that allows us the perform stratified cross validation. We will be using five folds. Setting verbose to True will output the AUROC score for each fold as well as the final average. The final average is also returned from the function.","6eb5fc6e":"# Feature Importance\nNow that we have created our final model and made our predictions with it, we will take a quick look at the feature importance to gain a better understanding of how our model works.\n\nFor a better understanding of the plot importance function, see the [XGBoost documentation](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.plotting). We will create two plots. The first is feature importance by weight, where weight is the number of times a feature appears in the tree. By this measure we can see that temperature and depth are both heavily used by the model, followed by density and salinity. These commonly used features are all ones given to us in the original dataset as opposed to ones that we engineered. Substrate and outlier also see some use, but the other features are rarely used.\n\nThe second plot is feature importance by gain, where gain is the average gain of splits using the feature. From this plot we can see that the outlier feature provides us with the most gain. Substrate and the EUNIS Exposed subclass also provide us with a lot of gain. Other exposure subclasses do provide us with some gain, but on a similar (lower) level like the temperature and salinity features.\n\nOverall we can see that the most important features to the model are those that were given to us in the original dataset. The outlier feature is also important to the model.","d152211f":"This notebook is part of the [Killer Shrimp Invasion challenge](https:\/\/www.kaggle.com\/c\/killer-shrimp-invasion), where our objective is to predict whether locations contain killer shrimp (Dikerogammarus villosus) based upon environmental data including temperature, salinity, depth, exposure, and substrate.\n\nThe solution came [second place](https:\/\/www.kaggle.com\/c\/killer-shrimp-invasion\/discussion\/155842) in the Killer Shrimp Invasion, with a final private leaderboard score of 0.99938. For comparison, the winning score was 0.99954, and the third place score was 0.99770. The public leaderboard score of this solution was 0.99984.\n\nIn this notebook we will first fill in missing data by using sklearn's IterativeImputer. We will then perform feature engineering to create several new features including an outlier (bounded box) feature and EUNIS exposure\/energy subclasses. Using these new features we will create an XGBoost model to make predictions on our test set. Finally, we will look at the feature importances of our model to see which features contribute the most to our model.\n\n**Curtis Thompson**","f4198051":"# Predictions\nWith our model created we can now make predictions. Our predictions will be outputted to _preds.csv_.","60349a2e":"## EUNIS Exposure Classifications\n\nEUNIS Exposure Classifications are categorical features for the exposure feature. They can be found in [the linked report](https:\/\/portal.helcom.fi\/Archive\/Shared%20Documents\/HABITAT%2010-2008-4.3-1%20EUNIS_BalticSea_Report_3.pdf), but the table has also been included below to make it easier.\n\n| EUNIS Subclass | SWM Minimum | SWM Maxium |\n| --- | --- | --- |\n| Extremely Exposed | 2000000 | 5000000 |\n| Very Exposed | 1000000 | 2000000 |\n| Exposed | 500000 | 1000000 |\n| Moderately Exposed | 100000 | 500000 |\n| Sheltered | 10000 | 100000 |\n| Very Sheltered | 4000 | 10000 |\n| Extremely Sheltered | 1200 | 4000 |\n| Ultra Sheltered | 1 | 1200 |\n\nIn the report these subclasses are grouped into three classes of energy-level; exposed (extremely exposed, very exposed, exposed), moderately exposed (moderately exposed), and sheltered (sheltered, very sheltered, extremely sheltered, ultra sheltered). We will not extract these energy classes as feature but we will take a subset of the exposed class in a feature that we call 'EUNIS Min Very Exposed'."}}