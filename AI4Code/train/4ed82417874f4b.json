{"cell_type":{"2e7a520a":"code","c0e34028":"code","cc1e9478":"code","21f9ecce":"code","705f238c":"code","89d30e0d":"code","5d75c267":"code","964843d5":"code","d6dfe9f0":"code","d7b6569e":"code","1ad998a5":"code","c0dbfc16":"code","3fa8cc76":"code","46736f0b":"code","3dd513f0":"code","8ba248be":"code","e2065bb5":"code","d9110d5e":"code","38480ba7":"code","9b67731c":"code","b774a9a5":"code","24efb47a":"code","9cd888cc":"code","b5a84826":"code","cf82205d":"code","92d7d2cb":"code","45e5af80":"code","8d1a89ce":"code","3dc5d03d":"code","e3efc674":"code","fc6668b1":"code","11362aa3":"code","e3cd036a":"code","1dcf03a4":"code","2ac0f260":"code","34f889dd":"code","d20548c7":"code","bdea7b9f":"code","b0c4eb9c":"code","6f486c4d":"code","144bf4a8":"code","9c89af2f":"code","f03715a2":"code","fa4b2f31":"code","21f55d29":"code","74a4ea74":"code","bcb7843c":"code","083fb286":"code","e13de43b":"code","d0984338":"code","3fd273e3":"code","2ae4fa42":"code","f5ca8899":"code","17f10986":"code","f9fb8d18":"code","9ed0298d":"code","c5407084":"code","2ab5e5d1":"code","e9a6c85c":"code","848ff340":"code","60bc570f":"code","fe7dbebf":"code","a2e45c25":"code","6fb425d7":"code","03b60fa3":"code","edd4f9f1":"code","598777db":"code","a02cbecf":"code","df082bd5":"code","e5171d71":"code","b8b81c5a":"code","d6d84960":"code","34e5609e":"code","88244a58":"code","2ceec193":"code","af89b76c":"code","10e5af8d":"code","66f6e7d2":"code","0cde7b68":"code","92e33e4f":"code","3d6c3d8b":"code","56df1095":"code","fb47fdbe":"code","327a8656":"code","6f5aff66":"code","620a1a92":"markdown","b21f7598":"markdown","af0218cf":"markdown","c40731a4":"markdown","d3ec5816":"markdown","66fc18c9":"markdown","0acfae26":"markdown","cf0e48bf":"markdown","32ea4211":"markdown","b7b311c9":"markdown","b4f1e94f":"markdown","74823d1f":"markdown","f3373e53":"markdown","52e436e5":"markdown","a0c4ccaa":"markdown","7033eb3a":"markdown","5e8ce7cc":"markdown","33c3837f":"markdown","73cd91e3":"markdown","e739c30c":"markdown","b239bba5":"markdown","909c2943":"markdown","5aa973d7":"markdown","f56f94e5":"markdown","551b109e":"markdown","58274ea1":"markdown","9de2e13f":"markdown","b5d2348e":"markdown","b98ffdf2":"markdown","ae024e2f":"markdown","61fe03ad":"markdown","4d5dbee7":"markdown","40d46396":"markdown","14f1b1e3":"markdown","8b2d40d1":"markdown","f2dba5fd":"markdown","55ea872f":"markdown","7eb3ccde":"markdown","1e20a778":"markdown","b9b2aa04":"markdown","fe0171f1":"markdown","e7b019a2":"markdown","6fb00af1":"markdown","065c9ca1":"markdown","5a84a922":"markdown","5ed02d0f":"markdown","df9e6104":"markdown","a8ab9b79":"markdown","22944a06":"markdown","451cd0db":"markdown","14050cce":"markdown","fc718fca":"markdown","e4593237":"markdown","31de3ef5":"markdown","df4a9025":"markdown","731b988d":"markdown","6c37262a":"markdown","d7d6055b":"markdown","45b83581":"markdown","1300a9c0":"markdown","076ea2b9":"markdown","b3d73ba5":"markdown","b9d6fe7e":"markdown","186f6b28":"markdown","47c5a5f8":"markdown","ccb3c101":"markdown","c780366e":"markdown","ba545e75":"markdown","182bc833":"markdown","fecc953d":"markdown","3e22153f":"markdown","135aa654":"markdown","40122c12":"markdown","c330110e":"markdown","3856a92f":"markdown","8b32b6be":"markdown","c00186f6":"markdown","78edfa07":"markdown","ceb97300":"markdown","026563b9":"markdown","f4b3e63a":"markdown","53cf6c3c":"markdown","6b6f1862":"markdown","bc0b83c1":"markdown","831962d2":"markdown","6fe3051b":"markdown","2a94fb72":"markdown","4b903401":"markdown","7b5a682c":"markdown","2b52602a":"markdown"},"source":{"2e7a520a":"from warnings import filterwarnings\nfilterwarnings(\"ignore\")","c0e34028":"pip install skompiler","cc1e9478":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mping\nimport seaborn as sns\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster import hierarchy\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,ShuffleSplit,GridSearchCV\nfrom sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import make_blobs,make_moons\nfrom yellowbrick.cluster import KElbowVisualizer\nimport time\nfrom matplotlib.colors import ListedColormap\nfrom skompiler import skompile\nfrom joblib import dump, load","21f9ecce":"pd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)","705f238c":"df = pd.read_csv(\"..\/input\/bank-marketing-data-set\/bank_marketing_dataset.csv\")","89d30e0d":"df.shape","5d75c267":"df.head()","964843d5":"df.describe().T","d6dfe9f0":"plt.figure(figsize=(12,6),dpi=200)\nsns.histplot(data=df,x=\"age\",color=\"red\")\nplt.title(\"Age Distribution of Customers\")\nplt.show()","d7b6569e":"plt.figure(figsize=(12,6),dpi=100)\nsns.histplot(data=df,x=\"age\",hue=\"loan\",bins=30)\nplt.title(\"Age Distribution of Customers vs their Loan status\")\nplt.show()","1ad998a5":"plt.figure(figsize=(9,3),dpi=200)\nsns.histplot(data=df,x=\"marital\",kde=True)\nplt.title(\"Marital Status Distribution of Customers\")\nplt.show()","c0dbfc16":"plt.figure(figsize=(12,6),dpi=200)\nsns.histplot(data=df,x=\"pdays\",color=\"green\")\nplt.show()","3fa8cc76":"pd.DataFrame(df.pdays.value_counts()).head()","46736f0b":"plt.figure(figsize=(12,6),dpi=200)\nsns.histplot(data=df[df.pdays!=999],x=\"pdays\",color=\"green\",kde=True)\nplt.show()","3dd513f0":"plt.figure(figsize=(12,6),dpi=200)\nsns.histplot(data=df,x=\"duration\",hue=\"contact\",color=\"purple\")\nplt.xlim(0,1000)\nplt.show()","8ba248be":"sns.countplot(data=df,x=\"loan\")\nplt.title(\"Personal Loan of Customers\")\nplt.show()","e2065bb5":"sns.countplot(data=df,x=\"housing\")\nplt.title(\"Housing Loan of Customers\")\nplt.show()","d9110d5e":"plt.figure(figsize=(12,6),dpi=200)\nsns.countplot(data=df,x=\"job\",order=df.job.value_counts().index)\nplt.title(\"Jobs of Customers\")\nplt.xticks(rotation=90)\nplt.show()","38480ba7":"plt.figure(figsize=(12,6),dpi=200)\nsns.countplot(data=df,x=\"education\",order=df.education.value_counts().index)\nplt.title(\"Education of Customers\")\nplt.xticks(rotation=90)\nplt.show()","9b67731c":"df.head()","b774a9a5":"df.shape","24efb47a":"df_dummies = pd.get_dummies(df)","9cd888cc":"df_dummies.head()","b5a84826":"df_dummies.shape","cf82205d":"scaler = StandardScaler()","92d7d2cb":"df_scaled = scaler.fit_transform(df_dummies)","45e5af80":"df_scaled[0]","8d1a89ce":"df_scaled[0]","3dc5d03d":"k_means_model = KMeans(n_clusters=2) #n_clusters is K value.","e3efc674":"k_means_model.fit(df_scaled)","fc6668b1":"cluster_labels = k_means_model.predict(df_scaled) #we can also do that with fit_predict() method.","11362aa3":"cluster_labels","e3cd036a":"df_dummies[\"Cluster\"] = cluster_labels","1dcf03a4":"df_dummies.head()","2ac0f260":"plt.figure(figsize=(12,6),dpi=200)\ndf_dummies.corr()[\"Cluster\"].iloc[:-1].sort_values().plot(kind=\"bar\")\nplt.title(\"Correlation between Clusters and Features\")\nplt.show()","34f889dd":"ssd = []\n\nfor k in range(2,10):\n    model = KMeans(n_clusters=k)\n    model.fit(df_dummies)\n    \n    ssd.append(model.inertia_) ## SSD Point to cluster centers","d20548c7":"ssd","bdea7b9f":"plt.plot(range(2,10),ssd,\"o--\")\nplt.title(\"Elbow Range\")\nplt.show()","b0c4eb9c":"pd.Series(ssd).diff()","6f486c4d":"df = pd.read_csv(\"..\/input\/autompg-dataset\/auto-mpg.csv\")","144bf4a8":"df.shape","9c89af2f":"df.head()","f03715a2":"df.origin.value_counts()","fa4b2f31":"df.describe()","21f55d29":"df_with_dummies = pd.get_dummies(df.drop(\"car name\",axis=1))","74a4ea74":"df_with_dummies.head()","bcb7843c":"scaler = MinMaxScaler()","083fb286":"df_scaled = scaler.fit_transform(df_with_dummies)","e13de43b":"df_scaled[:5]","d0984338":"df_scaled  = pd.DataFrame(df_scaled,columns=df_with_dummies.columns)","3fd273e3":"df_scaled.head()","2ae4fa42":"plt.figure(figsize=(20,9))\nsns.heatmap(df_scaled)\nplt.show()","f5ca8899":"plt.figure(figsize=(20,9))\nsns.clustermap(df_scaled,row_cluster=False)\nplt.show()","17f10986":"plt.figure(figsize=(15,9))\nsns.heatmap(df_scaled.corr())\nplt.title(\"Correlation between Features\")\nplt.show()","f9fb8d18":"hier_model = AgglomerativeClustering(n_clusters=5,affinity=\"euclidean\") #distance_threshold is also important.","9ed0298d":"cluster_labels = hier_model.fit_predict(df_scaled)","c5407084":"cluster_labels[:20]","2ab5e5d1":"plt.figure(figsize=(20,17))\nsns.scatterplot(data=df,x=\"mpg\",y=\"weight\",hue=cluster_labels,palette=\"viridis\")\nplt.grid(False)","e9a6c85c":"plt.figure(figsize=(20,7))\nsns.scatterplot(data=df,x=\"mpg\",y=\"horsepower\",hue=cluster_labels,palette=\"viridis\")\nplt.grid(False)","848ff340":"linkage_matrix = hierarchy.linkage(hier_model.children_)","60bc570f":"linkage_matrix","fe7dbebf":"linkage_df = pd.DataFrame(linkage_matrix,columns=[\"First Point\",\"Second Point\",\"Distance Between Points\",\"How many points are there in the cluster?\"])\nlinkage_df.head()","a2e45c25":"plt.figure(figsize=(20,10))\ndendrogram = hierarchy.dendrogram(linkage_matrix,truncate_mode=\"level\",p=5)\nplt.xticks(rotation=90)\nplt.grid(False)\nplt.show()","6fb425d7":"df = pd.read_csv(\"..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv\")","03b60fa3":"df.shape","edd4f9f1":"df.head()","598777db":"df.describe()","a02cbecf":"X = df.iloc[:, [3, 4]].values","df082bd5":"plt.figure(figsize=(20,9))\ndendrogram = hierarchy.dendrogram(hierarchy.linkage(X, method = 'ward'),leaf_font_size=10)\nplt.title('Dendrogram of Mall Customers')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean distances')\nplt.grid(False)\nplt.xticks(rotation=90)\nplt.show()","e5171d71":"hierearchical = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\ny_hierearchical = hierearchical.fit_predict(X)","b8b81c5a":"y_hierearchical","d6d84960":"plt.figure(figsize=(15,9))\nplt.scatter(X[y_hierearchical == 0, 0], X[y_hierearchical == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_hierearchical == 1, 0], X[y_hierearchical == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_hierearchical == 2, 0], X[y_hierearchical == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_hierearchical == 3, 0], X[y_hierearchical == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\nplt.scatter(X[y_hierearchical == 4, 0], X[y_hierearchical == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\nplt.title('Clusters of  Mall Customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.grid(False)\nplt.legend()\nplt.show()","34e5609e":"df = pd.read_csv(\"..\/input\/wholesale-customers-data-set\/Wholesale customers data.csv\")","88244a58":"df.shape","2ceec193":"df.head()","af89b76c":"df.describe()","10e5af8d":"sns.scatterplot(data=df,x='Milk',y='Grocery',hue='Channel')\nplt.grid(False)\nplt.show()","66f6e7d2":"sns.histplot(df,x='Milk',hue='Channel',multiple=\"stack\")\nplt.grid(False)\nplt.show()","0cde7b68":"print('Correlation Between Spending Categories')\nsns.clustermap(df.drop(['Region','Channel'],axis=1).corr(),annot=True)\nplt.grid(False)\nplt.show()","92e33e4f":"scaler = StandardScaler()\nscaled_X = scaler.fit_transform(df)","3d6c3d8b":"outlier_percent = []\n\nfor eps in np.linspace(0.001,3,50):\n    \n    # Create Model\n    dbscan = DBSCAN(eps=eps,min_samples=2*scaled_X.shape[1])\n    dbscan.fit(scaled_X)\n   \n     \n    # Log percentage of points that are outliers\n    perc_outliers = 100 * np.sum(dbscan.labels_ == -1) \/ len(dbscan.labels_)\n    \n    outlier_percent.append(perc_outliers)","56df1095":"sns.lineplot(x=np.linspace(0.001,3,50),y=outlier_percent)\nplt.ylabel(\"Percentage of Points Classified as Outliers\")\nplt.xlabel(\"Epsilon Value\")\nplt.grid(False)\nplt.show()","fb47fdbe":"dbscan = DBSCAN(eps=2)\ndbscan.fit(scaled_X)","327a8656":"df['Labels'] = dbscan.labels_","6f5aff66":"df.head()","620a1a92":"We will create a variety of models testing different epsilon values.","b21f7598":" Now, we would like to find an algorithm to solve this equation. This is actually a very difficult problem to solve precisely, since there are almost Kn ways to partition n observations into K clusters.\n \n **K-Means Clustering Algorithm**\n \n***\n\n**1.** Randomly assign a number, from 1 to K, to each of the observations.\n\n    These serve as initial cluster assignments for the observations.\n\n\n**2.** Iterate until the cluster assignments stop changing:\n\n    (a) For each of the K clusters, compute the cluster centroid. The kth cluster centroid is the vector of the p feature means for the observations in the kth cluster.\n\n    (b) Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).","af0218cf":"For a real world example, we will use *Mall Customer Segmentation Data*. This data set is created only for the learning purpose of the customer segmentation concepts , also known as market basket analysis.\n\nIt can be downloaded from [here](https:\/\/www.kaggle.com\/vjchoudhary7\/customer-segmentation-tutorial-in-python).\n\nWe will understand the dataset first.","c40731a4":" **Clustering** refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. Of course, to make this concrete, we must define what it means for two or more observations to be similar or different. Indeed, this is often a domain-specific consideration that must be made based on knowledge of the data being studied.","d3ec5816":"As we can see, there are many clients that was not previously contacted. Now we will look only contacted clients.","66fc18c9":"Now we will train the Hierarchical Clustering model on the dataset.","0acfae26":"Now we will do **cluster heatmap** by using *clustermap()* method.","cf0e48bf":"Now we will create an annotated clustermap of the correlations between spending on different categories.","32ea4211":"### Model","b7b311c9":"First, let's try to undertand age distribution of customers.","b4f1e94f":"## What's Unsupervised Learning?","74823d1f":" Density-Based Clustering refers to unsupervised learning methods that identify distinctive groups\/clusters in the data, based on the idea that a cluster in data space is a contiguous region of high point density, separated from other such clusters by contiguous regions of low point density.\n\n **Density-Based Spatial Clustering of Applications with Noise (DBSCAN)** is a base algorithm for density-based clustering. It can discover clusters of different shapes and sizes from a large amount of data, which is containing noise and outliers.\n ","f3373e53":"Now we will look jobs of customers.","52e436e5":"Now we will try to undertand marital status distribution of customers.","a0c4ccaa":"### Theory","7033eb3a":"Based on the plot, we will retrain a DBSCAN model with a reasonable epsilon value eps=2.","5e8ce7cc":"As we can see, it created new column for each category of each feature.","33c3837f":" As we have seen, to perform K-means clustering, we must decide how many clusters we expect in the data. The problem of selecting K is far from simple. This issue, along with other practical considerations that arise in performing K-means clustering.","73cd91e3":"![image.png](attachment:image.png)\n\n<div class=\"alert alert-block alert-info\">\n<b>Reference :<\/b>  Photo is taken by Dataiku Blog.\n<\/div>\n\n***","e739c30c":"![Screen%20Shot%202021-09-30%20at%2014.58.50.png](attachment:Screen%20Shot%202021-09-30%20at%2014.58.50.png)","b239bba5":" Since clustering is popular in many fields, there exist a great number of clustering methods. In this tutorial we focus on perhaps the two best-known clustering approaches: **K-means clustering and hierarchical clustering**. In K-means K-means clustering, we seek to partition the observations into a pre-specified clustering number of clusters. On the other hand, in hierarchical clustering, we do not know in advance how many clusters we want; in fact, we end up with a tree-like visual representation of the observations, called a *dendrogram*, that allows us to view at once the clusterings obtained for each possible number of clusters, from 1 to n. There are advantages and disadvantages\nto each of these clustering approaches.","909c2943":"It was in the same length of data set. Dummy variables seem black and white.","5aa973d7":"- **The Elements of  Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Jerome Friedman -  Data Mining, Inference, and Prediction (Springer Series in Statistics) \n\n- **An Introduction to Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Daniela Witten, Gareth James\n\n- [**Machine Learning by Shervine Amidi**](https:\/\/stanford.edu\/~shervine\/teaching\/cs-229\/cheatsheet-unsupervised-learning)\n\n- [**10 Clustering Algorithms With Python by Machine Learning Mastery**](https:\/\/machinelearningmastery.com\/clustering-algorithms-with-python\/)\n\n- [**K-Means: Getting The Optimal Number Of Clusters**](https:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/k-mean-getting-the-optimal-number-of-clusters\/)\n\n- [**Elbow Method for optimal value of k in K-Means**](https:\/\/www.geeksforgeeks.org\/elbow-method-for-optimal-value-of-k-in-kmeans\/)\n\n- [**Silhouette Algorithm to determine the optimal value of k**](https:\/\/www.geeksforgeeks.org\/silhouette-algorithm-to-determine-the-optimal-value-of-k\/?ref=rp)\n\n- [**4 Distance Measures for Machine Learning by Machine Learning Mastery**](https:\/\/machinelearningmastery.com\/distance-measures-for-machine-learning\/)\n\n- [**Working with Images in Python using Matplotlib**](https:\/\/www.geeksforgeeks.org\/working-with-images-in-python-using-matplotlib\/)\n\n- [**Hierarchical Clustering by Statquest**](https:\/\/www.youtube.com\/watch?v=7xHsRkOdVwo&ab_channel=StatQuestwithJoshStarmer)\n\n- [**ML | Hierarchical clustering (Agglomerative and Divisive clustering)**](https:\/\/www.geeksforgeeks.org\/ml-hierarchical-clustering-agglomerative-and-divisive-clustering\/)\n\n- [**DBSCAN Clustering Algorithm in Machine Learning**](https:\/\/www.kdnuggets.com\/2020\/04\/dbscan-clustering-algorithm-machine-learning.html)","f56f94e5":"## Resources","551b109e":"**Types of Hierarchical Clustering**\n\n1. Agglomerative Clustering: Also known as bottom-up approach or hierarchical agglomerative clustering (HAC). A structure that is more informative than the unstructured set of clusters returned by flat clustering. This clustering algorithm does not require us to prespecify the number of clusters. Bottom-up algorithms treat **each data as a singleton cluster at the outset and then successively agglomerates pairs of clusters until all clusters have been merged into a single cluster that contains all data**. \n\n2. Divisive clustering: Also known as top-down approach. This algorithm also does not require to prespecify the number of clusters. Top-down clustering requires a method for **splitting a cluster that contains the whole data and proceeds by splitting clusters recursively until individual data have been splitted into singleton cluster**.","58274ea1":" Each leaf of the dendrogram represents one of the observations in a data set. As we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are *similar to each other*. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other. On the other hand, observations that fuse later (near the top of the tree) can be quite different.","9de2e13f":"To make a reasonable analysis, we will work only with **Annual Income(k$)** and **Spending Score(1-100)** columns.","b5d2348e":"### Model","b98ffdf2":"***\n\n In order to calculate distances for K-Means cluestering, all features must be in numeric format. To solve this issue, we will apply **dummy method**.","ae024e2f":"Now we will create a scatterplot showing the relation between MILK and GROCERY spending, colored by Channel column.","61fe03ad":"![Screen%20Shot%202021-10-11%20at%2020.45.59.png](attachment:Screen%20Shot%202021-10-11%20at%2020.45.59.png)\n\n<div class=\"alert alert-block alert-info\">\n<b>Reference 5:<\/b>  Image is taken from *Machine Learning A-Z\u2122: Hands-On Python & R In Data Science* course.\n<\/div>","4d5dbee7":"## Importing Libraries","40d46396":" This formula says that we want to partition the observations into K clusters such that the total within-cluster variation, summed over all K clusters, is as small as possible.\n Solving seems like a reasonable idea, but in order to make it actionable we need to define the within-cluster variation. There are many possible ways to define this concept, but by far the most common choice involves squared *Euclidean distance*. That is, we define;\n \n \n![Screen%20Shot%202021-09-26%20at%2008.50.50.png](attachment:Screen%20Shot%202021-09-26%20at%2008.50.50.png)\n\nwhere |Ck| denotes the number of observations in the kth cluster. In other words, the within-cluster variation for the kth cluster is the sum of all of the pairwise squared Euclidean distances between the observations in the kth cluster, divided by the total number of observations in the kth cluster.","14f1b1e3":"***\n\n**How to find optimal number of clusters (K)?**\n\nThere are some methods to find optimal number of clusters (K):","8b2d40d1":"### Model","f2dba5fd":"Because we are dealing with distance metric,we will apply **Min Max scaling**. It transforms everything to scale between 0 and 1.","55ea872f":"### Theory","7eb3ccde":"For instance, if the ith observation is in the kth cluster, then i \u2208 Ck. The idea behind K-means clustering is that a good clustering is one for which the **within-cluster variation is as small as possible**. The within-cluster variation for cluster Ck is a measure W (Ck ) of the amount by which the observations within a cluster differ from each other. ","1e20a778":"For a real world example, we will use *Wholesale customers* dataset. The data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units (m.u.) on diverse product categories.\n\nIt can be downloaded from [here](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Wholesale+customers).\n\nAttribute Information:\n\n    1) FRESH: annual spending (m.u.) on fresh products (Continuous);\n    2) MILK: annual spending (m.u.) on milk products (Continuous);\n    3) GROCERY: annual spending (m.u.)on grocery products (Continuous);\n    4) FROZEN: annual spending (m.u.)on frozen products (Continuous)\n    5) DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous)\n    6) DELICATESSEN: annual spending (m.u.)on and delicatessen products (Continuous);\n    7) CHANNEL: customers  Channel - Horeca (Hotel\/Restaurant\/Caf\u00c3\u00a9) or Retail channel (Nominal)\n    8) REGION: customers  Region Lisnon, Oporto or Other (Nominal)\n\nWe will understand the dataset first.","b9b2aa04":"### Exploratory Data Analysis - Preprocessing","fe0171f1":" One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters K. **Hierarchical clustering** is an alternative approach which does not require that we commit to a particular choice of K. Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a *dendrogram*.","e7b019a2":"## Content\n","6fb00af1":"Now we will create a dendrogram to find the optimal number of clusters.","065c9ca1":"![image.png](attachment:image.png)\n\n<div class=\"alert alert-block alert-info\">\n<b>Reference 2:<\/b>  Image is taken from *Great Learning* website.\n<\/div>","5a84a922":"Now we will create the dendogram of the model by getting help from **linkage()** method. It performs hierarchical\/agglomerative clustering. If you want learn about more, you can read this [official explanation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.cluster.hierarchy.linkage.html) of the library.","5ed02d0f":"***\n\n In order to calculate distances for hierarchical cluestering, all features must be in numeric format. *Origin* column is in categorical format. To solve this issue, we will apply **dummy method**.","df9e6104":" An application example of clustering arises in marketing. We may have access to a large number of measurements (e.g. median household income, occupation, distance from nearest urban area, and so forth) for a large number of people. Our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product. The task of performing market segmentation amounts to clustering the people in the data set.","a8ab9b79":"For a real world example, we will use *Bank Marketing Data Set* dataset. The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution.\n\nIt can be downloaded from [here](https:\/\/www.kaggle.com\/berkayalan\/bank-marketing-data-set).\n\nWe will understand the dataset first.","22944a06":"## Clustering","451cd0db":" Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different. Y axis shows the distance.","14050cce":"pdays column means number of days that passed by after the client was last contacted from a previous campaign. 999 means client was not previously contacted. We will look at that.","fc718fca":" There is two particular types of unsupervised learning:\n \n- **Clustering :** a broad class of methods for discovering unknown subgroups in data.\n\n- **Principal Components Analysis(PCA) :** a tool used for data visualization or data pre-processing before supervised techniques are applied.\n","e4593237":"**Unsupervised Learning - Clustering**\n\n***\n\n- What's Unsupervised Learning?\n- Clustering\n- K-Means Clustering (Theory - Exploratory Data Analysis - Preprocessing - Model- Tuning)\n    - Color - Image Quantization \n- Hierarchical Clustering (Theory - Model)\n- DBSCAN (Density-based spatial clustering) (Theory - Model)","31de3ef5":"For a real world example, we will use *Auto-mpg dataset* dataset. The data is technical spec of cars. The dataset is downloaded from UCI Machine Learning Repository.\n\nIt can be downloaded from [here](https:\/\/www.kaggle.com\/uciml\/autompg-dataset).\n\nWe will understand the dataset first.","df4a9025":"![image.png](attachment:image.png)\n\n<div class=\"alert alert-block alert-info\">\n<b>Reference 5:<\/b>  Image is taken from *Towards Data Science* website.\n<\/div>\n","731b988d":"**Created by Berkay Alan**\n\n**Unsupervised Learning - Clustering**\n\n**13 October 2021**\n\n**For more Tutorial:** https:\/\/github.com\/berkayalan","6c37262a":"Centrally, all clustering methods use the same approach i.e. first we calculate similarities and then we use it to cluster the data points into groups or batches. Now we will focus on the Density-based spatial clustering of applications with noise (DBSCAN) clustering method. \n\n What\u2019s nice about DBSCAN is that we don\u2019t have to specify the number of clusters to use it. All we need is a function to calculate the distance between values and some guidance for what amount of distance is considered \u201cclose\u201d. DBSCAN also produces more reasonable results than k-means across a variety of different distributions.","d7d6055b":"***\n\n**How to find optimal number of clusters (K)?**\n\nThere are some methods to find optimal number of clusters (K):\n\n**1.** Elbow Curve Method\n\n    The elbow method runs k-means clustering on the dataset for a range of values of k (say 1 to 10). Perform K-means clustering with all these different values of K. For each of the K values, we calculate average distances to the centroid across all data points. Then we Plot these points and find the point where the average distance from the centroid falls suddenly (\u201cElbow\u201d).\n\n\n**2.** Silhouette Analysis\n\n    The silhouette coefficient is a measure of how similar a data point is within-cluster (cohesion) compared to other clusters (separation).\n\n","45b83581":"If you want to investigate more, please check out the documentation of [Agglomerative Clustering](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.AgglomerativeClustering.html).","1300a9c0":"Now we will merge cluster labels with our main data set.","076ea2b9":"***\n\n**Distance Measures for Unsupervised Learning**\n\n Distance measures play an important role in unsupervised learning. Different distance measures must be chosen and used depending on the types of the data. As such, it is important to know how to implement and calculate a range of different popular distance measures and the intuitions for the resulting scores.\n\n A distance measure is an objective score that summarizes the relative difference between two objects in a problem domain. Most commonly, the two objects are rows of data that describe a subject (such as a person, car, or house), or an event (such as a purchase, a claim, or a diagnosis).","b3d73ba5":"Now we will look loan status of customers.","b9d6fe7e":"Now we will look distribution of durations based on contact type.","186f6b28":"If you want to learn more about Unsupervised Learning - Clustering - Principal Components Analysis(PCA), you can check my [Github Notebook](https:\/\/github.com\/berkayalan\/Data-Science-Tutorials\/tree\/master\/Unsupervised%20Learning%20-%20Clustering%20-%20Principal%20Components%20Analysis(PCA))","47c5a5f8":"**Note:** To put it mathematically, there are *2^(n\u22121)* possible reorderings of the dendrogram, where n is the number of leaves. This is because at each of the n \u2212 1 points where fusions occur, the positions of the two fused branches could be swapped without affecting the meaning of the dendrogram. Therefore, we cannot draw conclusions about the similarity of two observations based on their proximity along the horizontal axis.","ccb3c101":"## Hierarchical Clustering (Theory - Model)","c780366e":"## K-Means Clustering (Theory - Model- Tuning)","ba545e75":"We also want to see if there is between ages and loan status of customers.","182bc833":"In order to see all rows and columns, we will increase max display numbers of dataframe.","fecc953d":"![Screen%20Shot%202021-09-26%20at%2008.48.13.png](attachment:Screen%20Shot%202021-09-26%20at%2008.48.13.png)","3e22153f":" **K-means clustering** is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters. To perform K-means clustering, we must first specify the desired number of clusters K; then the K-means algorithm will assign each observation to exactly one of the K clusters.","135aa654":"Now we will perform the clustering and assign the labels by using **sklearn**.","40122c12":"Because we are dealing with distance metric,we will apply **scaling**.","c330110e":"## DBSCAN (Density-based spatial clustering)  (Theory - Model)","3856a92f":"As we can see, it created new column for each category of each feature.","8b32b6be":"Now we can look at correlations of each feature with clusters that we assigned.","c00186f6":"The K-means clustering procedure results from a simple and intuitive mathematical problem. We begin by defining some notation. Let C1, . . . , CK denote sets containing the indices of the observations in each cluster. These sets satisfy two properties:\n\n1. C1 \u222a C2 \u222a ... \u222a CK = {1,...,n}. In other words, each observation belongs to at least one of the K clusters.\n2. Ck \u2229 Ck\u2032 = \u2205 for all k \u0338= k\u2032. In other words, the clusters are non- overlapping: no observation belongs to more than one cluster.\n","78edfa07":"In order to understand the relationships between features, we will make a heatmap of correlations.","ceb97300":" Most commonly used distance measures in unsupervised learning are as follows:\n\n- **Hamming Distance**\n\n    Hamming distance calculates the distance between two binary vectors, also referred to as binary strings or bitstrings for short.\n\n\n- **Euclidean Distance**\n\n    Euclidean distance calculates the distance between two real-valued vectors.\n\n\n- **Manhattan Distance**\n\n    The Manhattan distance, also called the Taxicab distance or the City Block distance, calculates the distance between two real-valued vectors.\n\n\n- **Minkowski Distance**\n\n    Minkowski distance calculates the distance between two real-valued vectors.\n    \n<div class=\"alert alert-block alert-success\">\n<b>Do you want to learn details?:<\/b> If you want to learn more, please check the post of Machine Learning Mastery.  \n<\/div>\n\n[Take me to the post](https:\/\/machinelearningmastery.com\/distance-measures-for-machine-learning\/)","026563b9":"### Theory","f4b3e63a":"![Screen%20Shot%202021-09-30%20at%2015.01.39.png](attachment:Screen%20Shot%202021-09-30%20at%2015.01.39.png)\n\n<div class=\"alert alert-block alert-info\">\n<b>Reference 4:<\/b>  Image is taken from *Introduction to Statistical Learning* book.\n<\/div>\n\nLeft: a dendrogram generated using Euclidean distance and complete linkage. Observations 5 and 7 are quite similar to each other, as are observations 1 and 6. However, observation 9 is no more similar to observation 2 than it is to observations 8, 5, and 7, even though observations 9 and 2 are close together in terms of horizontal distance. This is because observations 2,8,5, and 7 all fuse with observation 9 at the same height, approximately 1.8. \n\nRight: the raw data used to generate the dendrogram can be used to confirm that indeed, observation 9 is no more similar to observation 2 than it is to observations 8, 5, and 7.\n\n***","53cf6c3c":" Unsupervised learning is a set of statistical tools intended for the setting in which we have only a set of features X1, X2, . . . , Xp measured on n observations. We are not interested in prediction, because **we do not have an associated response variable Y** . Rather, the goal is to discover interesting things about the measurements on X1, X2, . . . , Xp. Is there an informative way to visualize the data? Can we discover subgroups among the variables or among the observations? Unsupervised learning refers to a diverse set of techniques for answering questions such as these. \n \n Unsupervised learning is often performed as part of an *exploratory data analysis*. The goal of unsupervised learning is to find hidden patterns in unlabeled data. Furthermore, it can be hard to assess the results obtained from unsupervised learning methods, since there is no universally accepted mechanism for performing cross- validation or validating results on an independent data set. The reason for this difference is simple. If we fit a predictive model using a supervised learning technique, then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, **in unsupervised learning, there is no way to check our work because we don\u2019t know the true answer**\u2014the problem is unsupervised.","6b6f1862":"### Extra Model","bc0b83c1":" Now, we would like to find an algorithm to solve this equation. \n \n **Hierarchical Clustering Algorithm**\n \n***\n\n1. Compare data points to find most similar data points to each other.\n\n2. Merge these to create a cluster.\n\n3. Compare clusters to find most similar clusters and merge again.\n\n4. Repeat until all data points in a single cluster.","831962d2":"<div class=\"alert alert-block alert-info\">\n<b>Reference 3:<\/b>  Image is taken from *Introduction to Statistical Learning* book.\n<\/div>\n\n***","6fe3051b":"![Screen%20Shot%202021-09-26%20at%2008.34.08.png](attachment:Screen%20Shot%202021-09-26%20at%2008.34.08.png)\n\n<div class=\"alert alert-block alert-info\">\n<b>Reference 1:<\/b>  Image is taken from *Introduction to Statistical Learning* book.\n<\/div>\n\n***\n\nFigure shows the results obtained from performing K-means clustering on a simulated example consisting of 150 observations in two dimensions, using three different values of K.","2a94fb72":"Now we will look education of customers.","4b903401":" The DBSCAN algorithm uses two parameters:\n\n- *eps (\u03b5)*: A distance measure that will be used to locate the points in the neighborhood of any point. For each instance, the algorithm counts how many instances are located within a small distance \u03b5 (epsilon) from it. This region is called the instance\u2019s \u03b5-neighborhood.\n\n- *minPts*: The minimum number of points (a threshold) clustered together for a region to be considered dense. If an instance has at least min_samples instances in its \u03b5-neighborhood (including itself), then it is considered a **core instance**. In other words, core instances are those that are located in dense regions. All instances in the neighborhood of a core instance belong to the same cluster. This neighborhood may include other core instances; therefore, a long sequence of neighboring core instances forms a single cluster.\n\nAny instance that is not a core instance and does not have one in its neighborhood is considered an anomaly.\n\nTo check out more, please visit [this](https:\/\/www.naftaliharris.com\/blog\/visualizing-dbscan-clustering\/) online visualisation tool.","7b5a682c":" Step 2(b), reallocating the observations can only improve. This means that as the algorithm is run, the clustering obtained will continually improve until the result no longer changes; the objective of will never increase. When the result no longer changes, a **local optimum** has been reached.\n \n K-means clustering derives its name from the fact that in Step 2(a), the cluster centroids are computed as the mean of the observations assigned to each cluster.\n \n Because the K-means algorithm finds a local rather than a global optimum, the results obtained will depend on the initial (random) cluster assignment of each observation in Step 1 of Algorithm. For this reason, it is important to run the algorithm multiple times from different random initial configurations. Then one selects the best solution, that for which the objective is smallest.","2b52602a":"**1. Elbow Curve Method**\n\nThe elbow method runs k-means clustering on the dataset for a range of values of k (say 1 to 10). Perform K-means clustering with all these different values of K. For each of the K values, we calculate average distances to the centroid across all data points. Then we Plot these points and find the point where the average distance from the centroid falls suddenly (\u201cElbow\u201d). We want high similarity(Max) inside clusters and low similarity interclusters(Min) to find optimized sum of squared distance(SSD)."}}