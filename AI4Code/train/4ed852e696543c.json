{"cell_type":{"67bd81e1":"code","e1a7059a":"code","60a38982":"code","73d51a92":"code","409f9fba":"code","148c0e7b":"code","3c23d11a":"code","99a73d38":"code","733a14ce":"code","38643ecd":"code","6f157428":"code","07f79076":"code","5f46d501":"code","388d20a9":"code","6d9ba980":"code","91aa30f2":"code","b4a6c4cd":"code","511cce21":"code","74fcb96d":"code","9cfbf588":"code","ecbb5c2b":"code","0f380ebf":"code","9c29d4d6":"code","27deb39e":"code","969fda7b":"code","037d8a78":"code","ced5ad2b":"code","95b3982e":"code","8023d333":"code","a9a6054f":"code","05884cf3":"code","c4ff9f2f":"code","5efa0c35":"code","26fbd62c":"code","f3379018":"code","e5825674":"code","704f1d35":"code","fadcbce1":"code","f2784388":"code","61607f52":"code","07a5c928":"code","511cf370":"code","08687017":"code","1d6b9654":"code","095d2155":"code","72c9288f":"code","740adfad":"code","30943f71":"code","c70b9b59":"code","1b693203":"code","c0479a2c":"code","96b7e94b":"code","7bb63623":"code","53f830e6":"code","39016d53":"code","6430659d":"markdown","b707ac0a":"markdown","ad3533c2":"markdown","fc34aa5c":"markdown","95e997d8":"markdown","1e0e10e7":"markdown","89837f8e":"markdown","3e8778c7":"markdown","c7bd0c51":"markdown","1bf8a9de":"markdown","48603d65":"markdown","269e2b72":"markdown","30cd013c":"markdown","2c4f1dde":"markdown","cd03650c":"markdown","91e9f511":"markdown","20e020b8":"markdown","7d89e1d3":"markdown","77b1decd":"markdown","a1aefef5":"markdown","37099cf4":"markdown","67c01e7f":"markdown","94a2563f":"markdown","18b28ad7":"markdown","2c2dfa09":"markdown","b391b6a8":"markdown","6628cef1":"markdown","d7c41a23":"markdown","6d26937a":"markdown","b03c2954":"markdown","6247ab0c":"markdown","84a61f3c":"markdown","5daf9eb5":"markdown","f76cb57f":"markdown","09b82f86":"markdown"},"source":{"67bd81e1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e1a7059a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","60a38982":"df_train = pd.read_csv(\"\/kaggle\/input\/santander-customer-transaction-prediction-dataset\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/santander-customer-transaction-prediction-dataset\/test.csv\")","73d51a92":"#train data\ndf_train.head()","409f9fba":"#test data\ndf_test.head()","148c0e7b":"df_train.shape,df_test.shape","3c23d11a":"sample_submission = pd.read_csv(\"\/kaggle\/input\/santander-customer-transaction-prediction\/sample_submission.csv\")","99a73d38":"sample_submission.head()","733a14ce":"sample_submission.shape","38643ecd":"# train data\ndf_train.isnull().sum().sum()","6f157428":"# test data\ndf_test.isnull().sum().sum()","07f79076":"df_train.info()","5f46d501":"df_train.describe()","388d20a9":"df_test.describe()","6d9ba980":"df_train.target.value_counts(normalize=True)","91aa30f2":"f,ax=plt.subplots(1,2, figsize=(12,4))\ndf_train.target.value_counts().plot.pie(explode=[0,0.12],autopct='%1.3f%%',ax=ax[0])\nsns.countplot('target',data=df_train)\nplt.show()","b4a6c4cd":"feat = df_train.columns.values[2:202]\nfeat","511cce21":"plt.figure(figsize=(15,5))\nsns.distplot(df_train[feat].mean(axis=1),color=\"green\", kde=True,bins=100, label='train')\nsns.distplot(df_test[feat].mean(axis=1),color=\"red\", kde=True,bins=100, label='test')\nplt.title(\"Distribution of mean values per row in the train and test dataset\")\nplt.legend()\nplt.show()","74fcb96d":"plt.figure(figsize=(15,5))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(df_train[feat].mean(axis=0),color=\"blue\",kde=True,bins=100, label='train')\nsns.distplot(df_test[feat].mean(axis=0),color=\"red\", kde=True,bins=100, label='test')\nplt.legend()\nplt.show()","9cfbf588":"plt.figure(figsize=(15,5))\nsns.distplot(df_train[feat].std(axis=1),color=\"green\", kde=True,bins=100, label='train')\nsns.distplot(df_test[feat].std(axis=1),color=\"red\", kde=True,bins=100, label='test')\nplt.title(\"Distribution of Standard Deviation values per row in the train and test dataset\")\nplt.legend()\nplt.show()","ecbb5c2b":"plt.figure(figsize=(15,5))\nsns.distplot(df_train[feat].std(axis=0),color=\"blue\", kde=True,bins=100, label='train')\nsns.distplot(df_test[feat].std(axis=0),color=\"red\", kde=True,bins=100, label='test')\nplt.title(\"Distribution of Standard Deviation values per columns in the train and test dataset\")\nplt.legend()\nplt.show()","0f380ebf":"# mean values per row in the train set grouped by Target = 0\ndf_train.loc[df_train.target == 0][feat].mean(axis=1)","9c29d4d6":"# mean values per row in the train set grouped by Target = 1\ndf_train.loc[df_train.target == 1][feat].mean(axis=1)","27deb39e":"plt.figure(figsize=(15,5))\nsns.distplot(df_train.loc[df_train.target == 0][feat].mean(axis=1),color=\"red\", kde=True,bins=100,label='target = 0')\nsns.distplot(df_train.loc[df_train.target == 1][feat].mean(axis=1),color=\"blue\", kde=True,bins=100,label='target = 1')\nplt.title(\"Distribution of mean values per row in the train set grouped by Target\")\nplt.legend()\nplt.show()","969fda7b":"# mean values per column in the train set grouped by Target = 0\ndf_train.loc[df_train.target == 0][feat].mean()","037d8a78":"# mean values per column in the train set grouped by Target = 1\ndf_train.loc[df_train.target == 1][feat].mean()","ced5ad2b":"plt.figure(figsize=(15,5))\nsns.distplot(df_train.loc[df_train.target == 0][feat].mean(),color=\"red\", kde=True,bins=100,label='target = 0')\nsns.distplot(df_train.loc[df_train.target == 1][feat].mean(),color=\"green\", kde=True,bins=100,label='target = 1')\nplt.title(\"Distribution of mean values per column in the train set grouped by Target\")\nplt.legend()\nplt.show()","95b3982e":"df_train.corr()","8023d333":"train_cor = df_train.drop([\"target\"], axis=1).corr()\ntrain_cor = train_cor.values.flatten()\ntrain_cor = train_cor[train_cor != 1]\nplt.figure(figsize=(15,5))\nsns.distplot(train_cor)\nplt.xlabel(\"Correlation values found in train excluding 1\")\nplt.ylabel(\"Density\")\nplt.title(\"Correlation between features\")\nplt.show()","a9a6054f":"df_test.corr()","05884cf3":"test_cor = df_test.corr()\ntest_cor = test_cor.values.flatten()\ntest_cor = test_cor[test_cor != 1]\nplt.figure(figsize=(15,5))\nsns.distplot(test_cor)\nplt.xlabel(\"Correlation values found in test excluding 1\")\nplt.ylabel(\"Density\")\nplt.title(\"Correlation between features\")\nplt.show()","c4ff9f2f":"import lightgbm as lgb\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold","5efa0c35":"X= df_train.drop([\"target\",\"ID_code\"],axis=1)\nX.head()","26fbd62c":"Y=df_train.target\nY.head()","f3379018":"X_test  = df_test.drop(\"ID_code\",axis=1)\nX_test.head()","e5825674":"params = {'objective' : \"binary\", \n               'boost':\"gbdt\",\n               'metric':\"auc\",\n               'boost_from_average':\"false\",\n               'num_threads':8,\n               'learning_rate' : 0.01,\n               'num_leaves' : 13,\n               'max_depth':-1,\n               'tree_learner' : \"serial\",\n               'feature_fraction' : 0.05,\n               'bagging_freq' : 5,\n               'bagging_fraction' : 0.4,\n               'min_data_in_leaf' : 80,\n               'min_sum_hessian_in_leaf' : 10.0,\n               'verbosity' : 1}","704f1d35":"num_folds = 10\nfolds = StratifiedKFold(n_splits=num_folds, shuffle=False, random_state=42)\noof = np.zeros(len(df_train))\ny_pred_lgbm = np.zeros(len(df_train.target))\n\nprint('Light GBM Model')\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values,df_train.target.values)):\n    print('Fold', fold_)\n    \n    X_train, Y_train = df_train.iloc[trn_idx][feat], df_train.target.iloc[trn_idx]\n    X_valid, Y_valid = df_train.iloc[val_idx][feat], df_train.target.iloc[val_idx]\n    \n    train_data = lgb.Dataset(X_train, label=Y_train)\n    valid_data = lgb.Dataset(X_valid, label=Y_valid)\n    \n    lgb_model = lgb.train(params, train_data, 1000000, valid_sets = [train_data, valid_data], verbose_eval=5000, early_stopping_rounds = 4000)\n    oof[val_idx] = lgb_model.predict(df_train.iloc[val_idx][feat], num_iteration=lgb_model.best_iteration)\n    y_pred_lgbm += lgb_model.predict(df_test[feat], num_iteration=lgb_model.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(df_train.target, oof)))","fadcbce1":"y_pred_lgbm","f2784388":"from sklearn.model_selection import train_test_split","61607f52":"from sklearn.ensemble import RandomForestClassifier\ntrain_X, val_X, train_y, val_y = train_test_split(X, Y, random_state=42)\nrfc_mod = RandomForestClassifier(random_state=42).fit(train_X, train_y)","07a5c928":"y_pred_rfc = rfc_mod.predict(X_test)","511cf370":"y_pred_rfc","08687017":"print(\"F1 Score :\",f1_score(y_pred_rfc,Y,average = \"weighted\"))\nprint('Report:\\n',classification_report(Y, y_pred_rfc))\nprint('Confusion Matrix: \\n',confusion_matrix(Y, y_pred_rfc))","1d6b9654":"from sklearn.tree import DecisionTreeClassifier\ntrain_X, val_X, train_y, val_y = train_test_split(X, Y, random_state=42)\ndectre_mod = DecisionTreeClassifier(random_state=42, max_depth=5, min_samples_split=5).fit(train_X, train_y)","095d2155":"y_pred_dec_tree = dectre_mod.predict(X_test)","72c9288f":"y_pred_dec_tree","740adfad":"print(\"F1 Score :\",f1_score(y_pred_dec_tree,Y,average = \"weighted\"))\nprint('Report:\\n',classification_report(Y, y_pred_dec_tree))\nprint('Confusion Matrix: \\n',confusion_matrix(Y, y_pred_dec_tree))","30943f71":"from sklearn.linear_model import LogisticRegression\ntrain_X, val_X, train_y, val_y = train_test_split(X,Y,random_state=42)\nlogreg_mod = LogisticRegression(random_state=42).fit(train_X, train_y)","c70b9b59":"y_pred_logreg = logreg_mod.predict(X_test)","1b693203":"y_pred_logreg","c0479a2c":"print(\"F1 Score :\",f1_score(y_pred_logreg,Y,average = \"weighted\"))\nprint('Report:\\n',classification_report(Y, y_pred_logreg))\nprint('Confusion Matrix: \\n',confusion_matrix(Y, y_pred_logreg))","96b7e94b":"submission_lgbm = pd.DataFrame({\n        \"ID_code\": df_test[\"ID_code\"],\n        \"target\": y_pred_lgbm\n    })\nsubmission_lgbm.to_csv('submission_lgbm.csv', index=False)","7bb63623":"submission_rfc = pd.DataFrame({\n        \"ID_code\": df_test[\"ID_code\"],\n        \"target\": y_pred_rfc\n    })\nsubmission_rfc.to_csv('submission_rfc.csv', index=False)","53f830e6":"submission_dec_tree = pd.DataFrame({\n        \"ID_code\": df_test[\"ID_code\"],\n        \"target\": y_pred_dec_tree\n    })\nsubmission_dec_tree.to_csv('submission_dec_tree.csv', index=False)","39016d53":"submission_logreg = pd.DataFrame({\n        \"ID_code\": df_test[\"ID_code\"],\n        \"target\": y_pred_logreg\n    })\nsubmission_logreg.to_csv('submission_logreg.csv', index=False)","6430659d":"### Import necessary packages","b707ac0a":"## 4.) Model Building","ad3533c2":"#### i) Distribution of Standard Deviation values per row in the train and test dataset","fc34aa5c":"### Target Variable ","95e997d8":"#### Thus, we can conclude that there is no correlation in test dataset.","1e0e10e7":"### Light GBM model","89837f8e":"#### Thus, we can conclude that there is no correlation in train dataset.","3e8778c7":"### i) Correlation between features in train dataset","c7bd0c51":"#### i) Distribution of mean values per row in the train and test dataset","1bf8a9de":"### Statistical Analysis","48603d65":"### Distribution of Mean ","269e2b72":"## 2) Load the dataset","30cd013c":"### ii) Distribution of the Standard Deviation values per columns in the train and test set.","2c4f1dde":"Thus, we can see that distribution of mean values per column is normally distributed.","cd03650c":"#### So from above graphs, we can conclude that the number of customers that will not make a transaction is much higher than those that will.","91e9f511":"### ii) Distribution of the mean values per columns in the train and test set.","20e020b8":"### Checking Missing values","7d89e1d3":"#### Thus, we can see that distribution of Standard Deviation values per row is of Standard Normal Distribution.","77b1decd":"#### Thus, we can see that distribution of Standard Deviation values per column is Positively Skewed.","a1aefef5":"### ii) Distribution of mean values per column in the train set grouped by Target","37099cf4":"### RandomForest Classifier Model","67c01e7f":"## 1) Import packages","94a2563f":"## 3) EDA","18b28ad7":"## Correlation","2c2dfa09":"### LogisticRegression Model","b391b6a8":"#### Hence the data is imbalanced with respect to target variable.","6628cef1":"### i) Distribution of mean values per row in the train set grouped by Target","d7c41a23":"### ii) Correlation between features in test dataset","6d26937a":"## 5.) Submission File","b03c2954":"### DecisionTree Classifier Model","6247ab0c":"#### Thus, we can see that distribution of mean values per row is of Standard Normal Distribution.","84a61f3c":"#### Hence, there are no missing values in train as well as test data.","5daf9eb5":"### Numerical values in train and test data","f76cb57f":"### Distribution of Standard Deviation","09b82f86":"### Distribution of feature grouped by Target variable"}}