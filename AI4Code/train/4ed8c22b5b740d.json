{"cell_type":{"2fed5ed3":"code","f7b9d378":"code","c6a80e38":"code","4197c188":"code","450b3dff":"code","c35f562a":"code","4725624c":"code","d2ba98dd":"code","838396dd":"code","e67e717f":"code","aeff0053":"markdown","2fbff0ad":"markdown","a1007663":"markdown","49b9a261":"markdown","8946829d":"markdown","20c63c6d":"markdown","049207a8":"markdown","f32894ed":"markdown","2659b617":"markdown","cfdf0e25":"markdown","ac84b68f":"markdown","c69bc1ac":"markdown","9bc2e283":"markdown","afefb0cc":"markdown","7aaadf06":"markdown","ae284f61":"markdown","449901d2":"markdown"},"source":{"2fed5ed3":"import types\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom scipy.stats import t\n!pip install openpyxl\n","f7b9d378":"df_data_0 = pd.read_excel(r'..\/input\/gaussian-spectrum\/shg3.xlsx')","c6a80e38":"df0=df_data_0[1:]\ndf0.columns=('wavelength(nm)','intensity')\ndf0.plot(x='wavelength(nm)',y='intensity')","4197c188":"df01=df0[300:2000]\ndf01.plot(x='wavelength(nm)',y='intensity')","450b3dff":"df1=df_data_0[763:1001]\ndf1.columns=('wavelength(nm)','intensity')\ndf1.plot(x='wavelength(nm)',y='intensity')","c35f562a":"#data conversion panda-numpy\ndf2=df1.values\nlen(df2)\nwavelengths=np.zeros(237)\nintensity=np.zeros(237)\ni=0\nwhile i<237:\n    wavelengths[i]=df2[i+1][0]\n    intensity[i]=df2[i+1][1]\n    i+=1","4725624c":"#Data normalization to allow application of statistical methods\nC=np.sum(intensity)\nY=intensity\/C\nY2=abs(Y)\nC2=np.sum(Y2)\nY3=Y2\/C2\nplt.plot(wavelengths,Y3)","d2ba98dd":"#random sample from distribution\ndef sample(n):\n    s=np.random.choice(wavelengths,n,p=Y3)\n    values=np.zeros(2)\n    values[0]=np.average(s)\n    values[1]=np.std(s)\n    return values","838396dd":"#Sampling of the average values and std from previous function\n#Basically this part repeats the sample function m times and collects m average values and m std's\ndef re_sample(n,m,confidence_interval):\n    s2_average=np.zeros(m)\n    s2_std=np.zeros(m)\n    i=0\n    while i<m:\n        s2_average[i]=sample(n)[0]\n        s2_std[i]=sample(n)[1]\n        i+=1\n    mc_ave_average=np.average(s2_average) #average of the average\n    mc_std_average=np.std(s2_average) #average's std\n    mc_ave_std=np.average(s2_std) #average std\n    mc_std_std=np.std(s2_std) #std of std\n    results=(mc_ave_average,mc_std_average,mc_ave_std,mc_std_std)\n    ci_ave_low=mc_ave_average-mc_ave_std*t.ppf((1+confidence_interval)\/2,m-1)\n    ci_ave_high=mc_ave_average+mc_ave_std*t.ppf((1+confidence_interval)\/2,m-1)\n    ci_std_low=mc_ave_std-mc_std_std*t.ppf((1+confidence_interval)\/2,m-1)\n    ci_std_high=mc_ave_std+mc_std_std*t.ppf((1+confidence_interval)\/2,m-1)\n    \n    results = {'quantity':['value','value\u00b4s std','value\u00b4s std %','value\u00b4s low confidence interval','value\u00b4s high confidence interval'],\n              'average':[mc_ave_average,mc_std_average,(mc_std_average\/mc_ave_average)*100,ci_ave_low,ci_ave_high],\n              'std':[mc_ave_std,mc_std_std,(mc_std_std\/mc_ave_std)*100,ci_std_low,ci_std_high]}\n    results_2=pd.DataFrame(data=results)\n    \n    print(results_2)  \n    plt.hist(s2_std,100)\n    plt.show()","e67e717f":"re_sample(10000,6000,0.95)","aeff0053":"The next part of the code does the rest of the job.  \nNote that this method rellies on the Central Limit Theorem.\nre_sample function does the following:  \n1. Generates m random samples, each with n elements, each drawn from the distribution $f(\\lambda_i)$;\n2. Creates sets $\\{\\overline{\\lambda_1},...\\overline{\\lambda_m}\\}$ and $\\{\\overline{s_1},...,\\overline{s_m}\\}$;\n3. Calculates average of each of the sets above, as well as standard deviations. (might sound confusin saying standard deviation of standard deviations);\n4. Calculates confidence intervals based on the function input for confidence_interval);\n5. Prints all the above described values as well as the corresponding confidence intervals (by use of Central Limit Theorem);\n6. Shows the graph.  \nNote that only one graph shows (graph for standard deviations) because this was the purpouse of the method. \nWith this we can succesfully calculate the width of the laser spectrum with controlled precision.","2fbff0ad":"We are interested in the width of the spectrum near 400, so we must perform some data cleaning.  \nNote that I'm paying no attention to the units involved, for they matter little for the porpouse of the method being demonstrated and I intend to be as more general as possible in the presentation of this method. When applied to concrete cases, units should be a matter of some importance and concern.","a1007663":"<h4> 3 - Draw m samples of size n of the probabilistic data","49b9a261":"<h3>Here is the main code.","8946829d":"<h1>Gaussian Regression via Monte Carlo Sampling","20c63c6d":"To create 1 sample of n wavelength values that vollow the intended distribution we must execute the following:","049207a8":"<H4>Step 1 - Import excel lab data into python readable data;  ","f32894ed":"To execute the code and get the desired calculation you should run **re_sample(n,m,confidence_interval)** as shown bellow.","2659b617":"Now we can see a funtion that closely resembles a gaussian ditribution.  \nNow we can go into the method.","cfdf0e25":"Here you can see what modules were used.  \nNote that some were only used form importation of the data file.","ac84b68f":"After the data is imported, it is assigned to the variable **df1**.  \nThe next step is to convert the panda data into numpy.","c69bc1ac":"This program intents to implement Monte Carlo Sampling method to calculate the values of statistical importance of a gaussian distribution.  \nIt's first purpose was to calculate the width of a laser spectrum which demonstrated a special form. Although it was developed for a particular laboratorial use, it shows a different method for calculating important characteristics of a gaussian distribution.\nDue to this particular spectrum form (gaussian) we can transform it so that it resembles a normal distrubution,\nallowing us to apply statistical methods to calculate its width (destribution's standard deviation).  \nThis process consists of 7 simple steps:  \n1. Import excel lab data into python readable data;\n2. Transform lab data into probabilistic data using normalization;\n3. Draw m samples of size n of the probabilistic data;\n4. For each sample calculate its average and standard deviation;\n5. Store previous values in a list of mean values and a list of sandard deviations;\n6. Now, that we can apply Central Limit Theorem, calculate the mean os each of the 2 lists;\n7. For each case (the mean of the means and mean of standard deviations) calculate the confidence intervals.  \n  ","9bc2e283":"You can see that the vertical axis values have now been changed.\nNow that we have transformed the lab results into a probabilistic distrubition, we can apply some statistical methods to determine the width of the spectrum.","afefb0cc":"Now we see that a large ammount of data we have at our disposal is not needed, therefore we should remove it, so that it doesn't skew our results.","7aaadf06":"Now we transform the laboratorial data into probabilistic data by normalizing the function.  \nOne of the conditions every probability ditributions must satisfy is $$\\int_\\Lambda f(\\lambda)d\\lambda = 1$$    \nHowever, since the lab data is not a continuous set, this normalization condition is written as $$\\sum_\\Lambda f(\\lambda_i)=1$$  \n  \nIn this case we have a set of $\\lambda_i \\in \\Lambda$ with the atributed function of **Intensity**, $I(\\lambda_i)$.  \nTo transform this function into a probability ditribution we must create a new function $$f(\\lambda_i)=\\frac{I(\\lambda_i)}{\\sum_\\Lambda I(\\lambda_i)} , \\forall \\lambda_i \\in\\Lambda$$  \nNow the normalization condition is satisfied by this new function $f(\\lambda_i)$.\nThis process is demonstrated bellow.","ae284f61":"As we can see, this method gives us a fairly good estimation of the desired values we, otherwise, might not been able to extract.\nBy simple observation of the initial graph we are not able to determine the needed values. For that we need a regression, however, here we have an alternative.\nIf you think, we estimated exact values using pseudo-random numbers and 'coin tossing' to calculate exact values, with user controlled precision.","449901d2":"<h4> Step 2 - Transform lab data into probabilistic data using normalization;"}}