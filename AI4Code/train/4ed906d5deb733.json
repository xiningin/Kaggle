{"cell_type":{"c0ec719d":"code","7fc89db8":"code","c667b421":"code","0f93f7ce":"code","5bd1c9b3":"code","5bacc8ab":"code","baf88780":"code","e99ab425":"code","1fba4797":"code","96baab3f":"code","ea4fac66":"code","a4801810":"code","8838ce2d":"code","b1adc6c8":"code","674d2080":"code","509e1817":"code","67ed4735":"code","61c74430":"code","c4833e38":"code","e613b2d2":"code","8a9e083d":"code","5eee2642":"code","a24928f1":"code","a56de28e":"code","e09503bf":"code","ab45da98":"code","2cd7bce4":"code","6d7777e7":"code","9f733417":"code","bedfaae7":"code","40a3cf3f":"code","fc88e9ad":"code","0233cf0c":"code","abb367f6":"code","4b2a39b0":"code","a81bf04e":"code","f972cff2":"code","c6e1eaa5":"code","a92edb74":"code","1b95c85a":"code","e2cfa289":"code","a9bcf057":"code","9c10dc8c":"code","a8fd3c62":"markdown","e330112d":"markdown","a93884c6":"markdown","c767bd55":"markdown","6b689c8f":"markdown","3c674b53":"markdown","6e3e8827":"markdown","eafb2960":"markdown","fe4cb943":"markdown","12dc73e5":"markdown","cc710fb9":"markdown","79dbf36a":"markdown","8dba06fc":"markdown","5080391f":"markdown","a4967462":"markdown","1b5aa036":"markdown","8b55436e":"markdown","76cbcd62":"markdown","bc994086":"markdown","4e430a09":"markdown","de253a02":"markdown","55dcfc94":"markdown","0d46872c":"markdown","c974a2b3":"markdown","87311f2f":"markdown","d7eead74":"markdown"},"source":{"c0ec719d":"import numpy as np\nimport pandas as pd\nimport gc\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport matplotlib.patches as patches\npd.set_option('max_columns', 150)\nfrom datetime import timedelta\nimport random\n\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer","7fc89db8":"metadata_dtype = {'site_id':\"uint8\",'building_id':'uint16','square_feet':'float32','year_built':'float32','floor_count':\"float16\"}\nweather_dtype = {\"site_id\":\"uint8\",'air_temperature':\"float16\",'cloud_coverage':\"float16\",'dew_temperature':\"float16\",'precip_depth_1_hr':\"float16\",\n                 'sea_level_pressure':\"float32\",'wind_direction':\"float16\",'wind_speed':\"float16\"}\ntrain_dtype = {'meter':\"uint8\",'building_id':'uint16'}","c667b421":"%%time\n\nweather_train = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_train.csv\", parse_dates=['timestamp'], dtype=weather_dtype)\nmetadata = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/building_metadata.csv\", dtype=metadata_dtype)\ntrain = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/train.csv\", parse_dates=['timestamp'], dtype=train_dtype)\n\nprint('Size of train_df data', train.shape)\nprint('Size of weather_train_df data', weather_train.shape)\nprint('Size of building_meta_df data', metadata.shape)","0f93f7ce":"train['meter'].replace({0:\"Electricity\",1:\"ChilledWater\",2:\"Steam\",3:\"HotWater\"},inplace=True)","5bd1c9b3":"def add_missing_weather_times(df):\n    first = df['timestamp'].iloc[0]\n    last = df['timestamp'].iloc[-1]\n    \n    for site in df['site_id'].unique():\n        site_data = df[df['site_id'] == site].sort_values('timestamp')\n        site_timestamps = list(site_data['timestamp'])\n        site_num = len(site_timestamps)\n    \n        i = 0\n        while True:\n            time = first + timedelta(hours=i)\n            if time not in site_timestamps:\n                dic = {'site_id': site, 'timestamp': time}\n                    \n                # Add new rows with NAs, because these will be taken care of later\n                for col in ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']:\n                        dic[col] = np.nan\n\n                df.loc[df.index[-1] + 1] = dic\n\n            if time == last:\n                break\n\n            i += 1\n\n        del site_data, site_timestamps, site_num\n        gc.collect()\n                \n    df.sort_values(['site_id', 'timestamp'], inplace=True)\n    df.reset_index(inplace=True, drop=True)","5bacc8ab":"print(len(weather_train))\nadd_missing_weather_times(weather_train)\nprint(len(weather_train))","baf88780":"def create_date_features(df):\n    df['Month'] = df['timestamp'].dt.month.astype(\"uint8\")\n    df['DayOfMonth'] = df['timestamp'].dt.day.astype(\"uint8\")\n    df['DayOfWeek'] = df['timestamp'].dt.dayofweek.astype(\"uint8\")\n    df['Hour'] = df['timestamp'].dt.hour.astype(\"uint8\")\n    df['Weekend'] = ((df['DayOfWeek'] == 6) | (df['DayOfWeek'] == 5)).astype('uint8')","e99ab425":"create_date_features(train)\ncreate_date_features(weather_train) # I'll also add these features to weather, which should help imputer","1fba4797":"weather_train.drop(['Weekend', 'DayOfWeek', 'DayOfMonth', 'precip_depth_1_hr'], axis=1, inplace=True)\nweather_train","96baab3f":"def replace_single_weather_nans(df):\n    for column in ['air_temperature', 'cloud_coverage', 'dew_temperature', 'sea_level_pressure', 'wind_speed']:\n        nan_index = df.index[df[column].isnull()]\n        \n        for i in range(0, len(nan_index)):\n            index = nan_index[i]\n            if index in [0, len(df)]:\n                continue\n            if  np.isfinite(df[column][index - 1]) and np.isfinite(df[column][index + 1]):\n                if df['site_id'][index-1] == df['site_id'][index] == df['site_id'][index + 1]:\n                    df[column][index] = (df[column][index-1] + df[column][index+1])\/2","ea4fac66":"print(sum(weather_train.isna().sum()))\nreplace_single_weather_nans(weather_train)\nprint(sum(weather_train.isna().sum()))","a4801810":"imputer = IterativeImputer()\nimputer.fit(weather_train.drop('timestamp', axis=1))","8838ce2d":"def replace_other_weather_nans(df):\n    col_names = list(df.columns)\n    col_names.remove('timestamp')\n    # Must remove timestamp, since it doesn't work with imputer\n    weather_temp = pd.DataFrame(imputer.transform(df[col_names]), columns=col_names)\n    weather_temp.insert(1, 'timestamp', df['timestamp']) # Reinsert timestamps\n\n    return weather_temp.drop(['Month', 'Hour'], axis=1)","b1adc6c8":"weather_train = replace_other_weather_nans(weather_train)","674d2080":"def map_wind_direction(df):\n    N_idx = (0 < df['wind_direction']) & ((315 < df['wind_direction']) | (df['wind_direction'] <= 45))\n    E_idx = (df['wind_direction'] > 45) & (df['wind_direction'] <= 135)\n    S_idx = (df['wind_direction'] > 135) & (df['wind_direction'] <= 225)\n    W_idx = (df['wind_direction'] > 225) & (df['wind_direction'] <= 315)\n    \n    df['wind_direction'][N_idx] = 1\n    df['wind_direction'][E_idx] = 2\n    df['wind_direction'][S_idx] = 3\n    df['wind_direction'][W_idx] = 4\n    \n    df['wind_direction'].astype('uint8')","509e1817":"map_wind_direction(weather_train)","67ed4735":"# Dropping floor_count variable as it has 75% missing values\nmetadata.drop('floor_count',axis=1,inplace=True)\nmetadata.drop('year_built',axis=1,inplace=True)","61c74430":"train['meter_reading'] = np.log1p(train['meter_reading'])","c4833e38":"metadata['primary_use'].replace({\"Healthcare\":\"Other\",\"Parking\":\"Other\",\"Warehouse\/storage\":\"Other\",\"Manufacturing\/industrial\":\"Other\",\n                                \"Retail\":\"Other\",\"Services\":\"Other\",\"Technology\/science\":\"Other\",\"Food sales and service\":\"Other\",\n                                \"Utility\":\"Other\",\"Religious worship\":\"Other\"},inplace=True)\nmetadata['square_feet'] = np.log1p(metadata['square_feet'])\nmetadata['square_feet'] = metadata['square_feet'].astype('float16') #Save space\n\n\n#metadata['year_built'].fillna(-999, inplace=True)\n#metadata['year_built'] = metadata['year_built'].astype('int16')","e613b2d2":"train = pd.merge(train,metadata,on='building_id',how='left')\nprint (\"Training Data+Metadata Shape {}\".format(train.shape))\ngc.collect()","8a9e083d":"train = pd.merge(train,weather_train,on=['site_id','timestamp'],how='left')\nprint (\"Training Data+Metadata+Weather Shape {}\".format(train.shape))\ngc.collect()","5eee2642":"del weather_train\ngc.collect()","a24928f1":"# Drop nonsense entries\n# As per the discussion in the following thread, https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/117083, there is some discrepancy in the meter_readings for different ste_id's and buildings. It makes sense to delete them\nidx_to_drop = list((train[(train['site_id'] == 0) & (train['timestamp'] < \"2016-05-21 00:00:00\")]).index)\nprint (len(idx_to_drop))\ntrain.drop(idx_to_drop,axis='rows',inplace=True)\n\n# dropping all the electricity meter readings that are 0, after considering them as anomalies.\nidx_to_drop = list(train[(train['meter'] == \"Electricity\") & (train['meter_reading'] == 0)].index)\nprint(len(idx_to_drop))\ntrain.drop(idx_to_drop,axis='rows',inplace=True)\n\n# Drop outliers from training data. Following https:\/\/www.kaggle.com\/juanmah\/ashrae-outliers\nidx_to_drop = list(train[(train.building_id == 1099) | \n                         (train.building_id == 799) | \n                         (train.building_id == 1088) | \n                         (train.building_id == 778) | \n                         (train.building_id == 1168) | \n                         (train.building_id == 1021)].index)\nprint(len(idx_to_drop))\ntrain.drop(idx_to_drop, axis='rows', inplace=True)\n","a56de28e":"train.drop('timestamp',axis=1,inplace=True)\n\nle = LabelEncoder()\ntrain['meter']= le.fit_transform(train['meter']).astype(\"uint8\")\ntrain['primary_use']= le.fit_transform(train['primary_use']).astype(\"uint8\")\n\nprint (train.shape)","e09503bf":"%%time\n# Let's check the correlation between the variables and eliminate the one's that have high correlation\n# Threshold for removing correlated variables\nthreshold = 0.9\n\n# Absolute value correlation matrix\ncorr_matrix = train.corr().abs()\n# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\ndel corr_matrix, upper\ngc.collect()\n\nprint('There are %d columns to remove.' % (len(to_drop)))\nprint (\"Following columns can be dropped {}\".format(to_drop))\n\ndef drop_correlated_features(df):\n    df.drop(to_drop,axis=1,inplace=True)","ab45da98":"drop_correlated_features(train)","2cd7bce4":"%%time\ny = train['meter_reading']\ntrain.drop('meter_reading',axis=1,inplace=True)","6d7777e7":"%%time\nx_trains = {meter: train[train['meter'] == meter] for meter in [0, 1, 2, 3]}\ny_trains = {meter: y[train['meter'] == meter] for meter in [0, 1, 2, 3]}","9f733417":"del y, train\ngc.collect()","bedfaae7":"import lightgbm as lgb","40a3cf3f":"def predict(X, step=1000000):\n    results = {}\n    for meter in [0, 1, 2, 3]:\n        x = X[X.meter == meter]\n        predictions = None\n        for model in models[meter]:\n            preds = []\n            for i in range(0, len(x), step):\n                preds.extend(model.predict(x.iloc[i: min(i+step, len(x)), :], num_iteration=model.best_iteration))\n\n            # Average results\n            if predictions is None:\n                predictions = np.array(preds)\/(len(models[meter]))\n            else:\n                predictions += np.array(preds)\/(len(models[meter]))\n \n            print('... ', end ='')\n        predictions = np.expm1(predictions) # Back to kWh\n        print()\n        \n        # Create DFs\n        results[meter] = pd.DataFrame(x.index, columns=['row_id'])\n        results[meter]['meter_reading'] = predictions\n        results[meter]['meter_reading'].clip(lower=0,upper=None,inplace=True)\n        \n        del preds, predictions, x\n        gc.collect()\n        \n    # Merge results\n    result = pd.concat([*results.values()])\n    result.sort_values('row_id', inplace=True)\n    \n    return result","fc88e9ad":"common_params = {'objective': 'regression',\n                'boosting_type': 'gbdt',\n                'bagging_seed': 11,\n                'metric': 'rmse',\n                'verbosity': -1,\n                'random_state': 47}\n\nparams = {0: {'feature_fraction': 0.75,\n          'bagging_fraction': 0.8,\n          'num_leaves': 300,\n          'max_depth': 15,\n          'learning_rate': 0.14,\n          'min_child_weight': 10,\n          'min_split_gain': 0.005,\n          'reg_alpha': 12.5,\n          'reg_lambda': 7.5,\n          **common_params\n         },\n         1: {'feature_fraction': 0.725,\n          'bagging_fraction': 0.75,\n          'num_leaves': 250, \n          'max_depth': 20,\n          'learning_rate': 0.17,\n          'min_child_weight': 1,\n          'min_split_gain': 0.005,\n          'reg_alpha': 3.,\n          'reg_lambda': 15.,\n          **common_params\n         },\n         2: {'feature_fraction': 0.75,\n          'bagging_fraction': 0.825,\n          'num_leaves': 300,\n          'max_depth': 25,\n          'learning_rate': 0.17,\n          'min_child_weight': 15,\n          'min_split_gain': 0.005,\n          'reg_alpha': 3.,\n          'reg_lambda': 5.,\n          **common_params\n         },\n         3: {'feature_fraction': 0.75,\n          'bagging_fraction': 0.750,\n          'num_leaves': 300,\n          'max_depth': 15,\n          'learning_rate': 0.16,\n          'min_child_weight': 15,\n          'min_split_gain': 0.01,\n          'reg_alpha': 1.,\n          'reg_lambda': 3.,\n          **common_params\n         }}","0233cf0c":"%%time\ncategorical_cols = ['building_id','Month','meter','Hour','Weekend','primary_use','DayOfWeek','DayOfMonth', 'wind_direction']\n\nmodels = {0: [], 1: [], 2:[], 3:[]}\nfor meter in [0, 1, 2, 3]:\n    X = x_trains[meter]\n    y = y_trains[meter]\n    \n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    strats = np.array(pd.cut(y, 50, labels=list(range(50))))\n    \n    kf = StratifiedKFold(n_splits=10, random_state=200)\n    for train_i, test_i in kf.split(X_np, strats):\n        X_train_kf, X_test_kf = X.iloc[train_i], X.iloc[test_i]\n        y_train_kf, y_test_kf = y.iloc[train_i], y.iloc[test_i]\n\n        lgb_train = lgb.Dataset(X_train_kf, y_train_kf, categorical_feature=categorical_cols)\n        lgb_test = lgb.Dataset(X_test_kf, y_test_kf, categorical_feature=categorical_cols)\n\n        reg = lgb.train(params[meter], lgb_train, num_boost_round=500, valid_sets=[lgb_train, lgb_test], \n                    early_stopping_rounds=50, verbose_eval=500)\n        \n        print()\n        del X_train_kf, X_test_kf, y_train_kf, y_test_kf, lgb_train, lgb_test\n        gc.collect()\n    \n        models[meter].append(reg)\n    \n    del X, y, X_np\n    gc.collect()\n    \n    print('\\n------------------------\\n')","abb367f6":"del x_trains, y_trains","4b2a39b0":"test = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/test.csv\", parse_dates=['timestamp'], usecols=['building_id','meter','timestamp'], dtype=train_dtype)\nweather_test = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_test.csv\", parse_dates=['timestamp'], dtype=weather_dtype)\n\ntest['meter'].replace({0:\"Electricity\",1:\"ChilledWater\",2:\"Steam\",3:\"HotWater\"},inplace=True)","a81bf04e":"add_missing_weather_times(weather_test)\ncreate_date_features(test)\ncreate_date_features(weather_test)\nweather_test.drop(['Weekend', 'DayOfWeek', 'DayOfMonth', 'precip_depth_1_hr'], axis=1, inplace=True)\nreplace_single_weather_nans(weather_test)\nweather_test = replace_other_weather_nans(weather_test)\nmap_wind_direction(weather_test)","f972cff2":"%%time\n# Merge data\ntest = pd.merge(test,metadata,on='building_id',how='left')\nprint (\"Training Data+Metadata Shape {}\".format(test.shape))\ngc.collect()\ntest = pd.merge(test,weather_test,on=['site_id','timestamp'],how='left')\nprint (\"Training Data+Metadata+Weather Shape {}\".format(test.shape))\ngc.collect()","c6e1eaa5":"del metadata, weather_test\ngc.collect()","a92edb74":"test.drop('timestamp',axis=1,inplace=True)\ntest['meter']= le.fit_transform(test['meter']).astype(\"uint8\")\ntest['primary_use']= le.fit_transform(test['primary_use']).astype(\"uint8\")","1b95c85a":"drop_correlated_features(test)","e2cfa289":"gc.collect()","a9bcf057":"%%time\npredictions = predict(test)","9c10dc8c":"predictions.to_csv(\"TiimTiim_submission_10.csv\",index=None)","a8fd3c62":"## Read and modify test data","e330112d":"If there is single NaN in weather data, it seems reasonable to linearly interpolate between previous and next measurement.","a93884c6":"## Drop features","c767bd55":"# Training data","6b689c8f":"## Engineer new features I\n\nDay, Month, Hour, Weekend","3c674b53":"Convert target to log scale","6e3e8827":"For other nan-s let's use sklearn's IterativeImputer with BayesianRidge estimator, to predict likely values for other nan-s.","eafb2960":"## KFold model","fe4cb943":"# Final predictions","12dc73e5":"## Predict","cc710fb9":"Preprocess metadata \n","79dbf36a":"There are some missing measurements alltogether as well. For example 2016.12.31 17:00. I simply interpolate between previous and next measurement. There are few days which have almost no weather data (2016.01.05 for example) and this will at the moment be left as is.","8dba06fc":"Kaggle competition submission from TiimTiim. Started out from LGBM Baseline model https:\/\/www.kaggle.com\/morituri\/lgbm-baseline but is quite modified, with additional logic for missing values and 10 fold stratified models for each meter type.","5080391f":"# Imports","a4967462":"Drop correlated variables","1b5aa036":"## Encode features","8b55436e":"## Merge data","76cbcd62":"# Model\n\nI'm going to build lightGBM based model with separate model for each meter type and using 10 fold ensamble for each, in order to reduce overfitting.","bc994086":"Load the data reducing its size","4e430a09":"## Map wind direction\n","de253a02":"Improve data readability","55dcfc94":"Drop some columns based on EDA","0d46872c":"## Drop some training data","c974a2b3":"## Prediction function","87311f2f":"## Deal with NaNs","d7eead74":"## Add missing measurements to weather"}}