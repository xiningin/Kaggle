{"cell_type":{"d2c5994f":"code","58a98647":"code","8935e252":"code","f4db27f4":"code","2adafd62":"code","c093814c":"code","79ffcdd7":"code","750cb6ff":"code","ff822d02":"code","a78c5bfe":"code","d728e60f":"code","4998f320":"code","235f0674":"code","4246cdc8":"code","51846aa3":"code","47084172":"code","703c07a9":"code","0f398a48":"code","c8341522":"code","0c5283ed":"code","7547f9ab":"code","c05517aa":"code","8f4bac80":"code","711a9683":"markdown","607f5a64":"markdown","e49fd60e":"markdown","08a08103":"markdown","670cd4a2":"markdown","f619d889":"markdown","0cbb457b":"markdown","463d7765":"markdown","807e275f":"markdown","352560c6":"markdown","aaad3636":"markdown","b9f7b912":"markdown","88d383a4":"markdown","6defb397":"markdown","b291ad9d":"markdown","bf8520f2":"markdown","d7bec06e":"markdown","c0ef8a62":"markdown","0c48406a":"markdown"},"source":{"d2c5994f":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","58a98647":"df = pd.read_csv('\/kaggle\/input\/news-category-dataset\/final_news_df.csv')\ndf.head()","8935e252":"df.info()","f4db27f4":"df['category'].value_counts()","2adafd62":"sentences = (df['headline'] + ' ' + df['short_description']).values\nlabels = df['category'].values\nlabels_dict = {\n    'BUSINESS':0,\n    'FOOD & DRINK':1,\n    'PARENTING':2,\n    'POLITICS':3,\n    'WELLNESS':4,\n    'WORLD NEWS':5\n}\nlabels_num = np.array([labels_dict[name] for name in labels])","c093814c":"!pip install sentence-transformers","79ffcdd7":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')","750cb6ff":"embeddings = model.encode(sentences)\nprint(embeddings.shape)","ff822d02":"import umap","a78c5bfe":"umap_embedding = umap.UMAP(n_neighbors = 30, random_state=42).fit_transform(embeddings)","d728e60f":"import matplotlib.pyplot as plt \nfig, ax = plt.subplots()\nscatter = ax.scatter(umap_embedding.T[0], umap_embedding.T[1], s=0.03, c=labels_num, cmap='Spectral')\n# produce a legend with the unique colors from the scatter\nlegend1 = ax.legend(*scatter.legend_elements(),\n                    loc=\"lower left\", title=\"Classes\")\nax.add_artist(legend1)\nplt.title(\"SentenceBert+umap-classes-viz\")\nplt.show()","4998f320":"from sklearn.cluster import KMeans","235f0674":"kmeans = KMeans(n_clusters = 6, random_state=42)\nkmeans.fit(embeddings)","4246cdc8":"from sklearn.metrics import accuracy_score","51846aa3":"def cluster_purity(y_true, y_pred):\n    y_pure = np.zeros(y_true.shape[0])\n    cluster_purities = {}\n    cluster2class = {}\n    for label in np.unique(y_pred):\n        \n        # getting indices in clusters array\n        indices =  np.where(y_pred == label)[0]\n        # taking corresponding true label\n        true = np.take(y_true, indices)\n        # compute most present label in cluster\n        u, c = np.unique(true, return_counts = True)\n        y = u[c == c.max()]\n        # Taking label randomly when\n        # there are multiple most present labels\n        if y.shape[0] > 1:\n            y = y[rd.randint(0,len(y)-1)]\n        # Mapping cluster number to true class number\n        cluster2class[label] = y[0] \n        # assigning most present label to\n        # all values in the current cluster\n        for ind in indices :\n            y_pure[ind] = int(y)\n        # computing rate of cluster purity\n        cluster_purities[y[0]] = c.max()\/len(indices)\n    return y_pure, cluster_purities, cluster2class","47084172":"y_pure, cluster_purities, cluster2class = cluster_purity(labels_num, kmeans.labels_)\nunique, counts = np.unique(y_pure, return_counts=True)\nreverse_dict = {v: k for k, v in labels_dict.items()}\nfor i in unique:\n    print('{} : counts={} ; purity={}'.format(reverse_dict[int(i)], counts[int(i)], cluster_purities[int(i)]))","703c07a9":"print('Purity Score :', accuracy_score(labels_num, y_pure))","0f398a48":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef c_tf_idf(documents, m, ngram_range=(1, 1)):\n    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n    t = count.transform(documents).toarray()\n    w = t.sum(axis=1)\n    tf = np.divide(t.T, w)\n    sum_t = t.sum(axis=0)\n    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n    tf_idf = np.multiply(tf, idf)\n\n    return tf_idf, count\n\ndef extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n    words = count.get_feature_names()\n    labels = list(docs_per_topic.cluster_nb)\n    tf_idf_transposed = tf_idf.T\n    indices = tf_idf_transposed.argsort()[:, -n:]\n    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n    return top_n_words","c8341522":"df = pd.DataFrame({'document':sentences, 'cluster_nb':kmeans.labels_})\ndocs_per_topic = df.groupby(['cluster_nb'], as_index = False).agg({'document': ' '.join})","0c5283ed":"tf_idf, count = c_tf_idf(docs_per_topic.document.values, m=len(sentences))\ntop_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)","7547f9ab":"for cluster_id, best_words_list in top_n_words.items():\n    print(\"Cluster \",reverse_dict[cluster2class[cluster_id]],\" topics\")\n    for wt in best_words_list[0:5]:\n        print(wt[0])","c05517aa":"centers = {}\nfor cluster_index in np.unique(kmeans.labels_):\n    indices = np.where(kmeans.labels_ == cluster_index)\n    if len(indices) > 0 :\n        values = np.take(umap_embedding, indices, axis=0)\n        centers[cluster_index] = np.mean(values, axis = 1)[0]","8f4bac80":"fig, ax = plt.subplots(figsize=(20,10))\nscatter = ax.scatter(umap_embedding.T[0], umap_embedding.T[1], s=0.03, c=kmeans.labels_, cmap='Spectral')\nfor cluster_nb,coords in centers.items():\n     ax.text(coords[0],coords[1],'\\n'.join([t[0] for t in top_n_words[cluster_nb][0:4]]))","711a9683":"Quite a good score (~78 % of articles are clustered in the right group).","607f5a64":"# \ud83d\udcda Clustering","e49fd60e":"In the end we can plot the words on each group with matplotlib","08a08103":"Then using this, we can see the repartition of classes and their purity (rate of well assigned points)","670cd4a2":"# \ud83d\udcc1 Loading dataset","f619d889":"To compute purity score, I define first a function identifying the class corresponding to each cluster. This function will count for each cluster the most present class and associate all the points in the cluster to the most present class.","0cbb457b":"We will only use headline and short description of articles.","463d7765":"I used the 'all-MiniLM-L6-v2' pretrained model because it is fast and accurate according to the [Sentence-Transformers Benchmarking](https:\/\/www.sbert.net\/docs\/pretrained_models.html). It is possible to use GPU to encode sentences.","807e275f":"I will use KMeans with 6 clusters (as there are 6 news types in the dataset).","352560c6":"# \ud83d\udcca Evaluation","aaad3636":"#  \ud83d\udcc9 Visualization of vectors using UMAP","b9f7b912":"In this Notebook, I will try to perform text clustering and Topic Modelling using SentenceBERT vectors. I will then evaluate clustering performance using purity score. Feel free to comment if you have any suggestion to improve it (I am not an expert) !","88d383a4":"# \t\ud83c\udd8e Topic Modelling","6defb397":"Defining purity score and labels","b291ad9d":"Based on the [article by Maarten Grootendorst](https:\/\/towardsdatascience.com\/topic-modeling-with-bert-779f7db187e6)","bf8520f2":"# \u27a1\ufe0f Getting sentence vectors","d7bec06e":"I used the [news category dataset](https:\/\/www.kaggle.com\/setseries\/news-category-dataset)","c0ef8a62":"For that part I used the [sentence-transformers library](https:\/\/www.sbert.net)","0c48406a":"Now we visualize the 2-dimensional vectors to see if we can already distinguish some groups of points"}}