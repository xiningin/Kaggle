{"cell_type":{"dc004f0c":"code","6caa0b37":"code","97d4576a":"code","df1e8340":"code","f3a7ef95":"code","c857a5a2":"code","6caf9847":"code","cd426592":"code","30189965":"code","ef42a62a":"code","5b8f4376":"code","338609fb":"code","a0a1e9f8":"code","73aaf6c7":"code","bcdc81d8":"code","eab5e0b9":"code","96cdc44d":"code","a9543fb4":"code","67c0947e":"code","3708fac5":"code","75f45f8e":"code","f1ed26c6":"code","f30c836d":"code","3415faa4":"code","ccea4612":"code","4a2e0651":"code","47f75be0":"code","8c0f8cae":"code","e2c80773":"code","20ea06f8":"code","3d26a447":"code","8e12a452":"code","3da42b3e":"code","3969f05e":"code","b9f68c9a":"code","9f120c5b":"code","9d4ecaf6":"code","ab74cc84":"code","af71631b":"code","9bc1149e":"code","2d4b6b03":"code","4e1018cd":"code","639e474c":"code","25d8cce0":"markdown","0ca4bb6f":"markdown","a29d0b6f":"markdown","665e6da5":"markdown","e2f9f1f4":"markdown","2ad97144":"markdown"},"source":{"dc004f0c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras","6caa0b37":"N = 200 #Number of observations per class\nD = 2   #Number of features\nK = 3   #Number of classes\nX = np.zeros((N * K, D))\ny = np.zeros(N * K, dtype = 'uint8')\nfor j in range(K):\n    ix = range(N * j, N * (j + 1))\n    r = np.linspace(0, 1, N) \n    np.random.seed(j)\n    t = np.linspace(j * 4,(j + 1) * 5, N) + np.random.randn(N) * 0.25 \n    X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n    y[ix] = j\n\n#Plot Data\ncdict = {0: 'red', 1: 'blue', 2: 'green'}\nplt.figure(figsize = (8, 8))\nfor i in np.unique(y):\n    indices = np.where(y == i)\n    plt.scatter(x = X[indices, 0], y = X[indices, 1], c = cdict[i], \n                label = i, marker = \"o\", alpha = 0.7)\nplt.legend()","97d4576a":"X.shape","df1e8340":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\nprint(f'The shape of X_train is: {X_train.shape}')\nprint(f'The shape of y_train is: {y_train.shape}')\nprint(f'The shape of X_test is: {X_test.shape}')\nprint(f'The shape of y_test is: {y_test.shape}')","f3a7ef95":"pd.Series(y_train).value_counts()\n# It seems that in each category the distribution of data is balanced.","c857a5a2":"pd.Series(y_test).value_counts()\n# It seems that in each category the distribution of data is balanced.","6caf9847":"# Visualize the train and test dataset to see how data are distributed.\ncdict = {0: 'red', 1: 'blue', 2: 'green'}\nplt.figure(figsize = (8, 8))\nfor i in np.unique(y_train):\n    indices = np.where(y_train == i)\n    plt.scatter(x = X_train[indices, 0], y = X_train[indices, 1], c = cdict[i], \n                label = i, marker = \"o\", alpha = 0.7)\nplt.legend()\nplt.title('X_train', size = 20)","cd426592":"# Visualize the train and test dataset to see how data are distributed.\ncdict = {0: 'red', 1: 'blue', 2: 'green'}\nplt.figure(figsize = (8, 8))\nfor i in np.unique(y_test):\n    indices = np.where(y_test == i)\n    plt.scatter(x = X_test[indices, 0], y = X_test[indices, 1], c = cdict[i], \n                label = i, marker = \"o\", alpha = 0.7)\nplt.legend()\nplt.title('X_test', size = 20)","30189965":"type(X_train)","ef42a62a":"features_train = tf.convert_to_tensor(X_train, dtype = tf.float32)\nfeatures_train","5b8f4376":"response_train = tf.convert_to_tensor(y_train, dtype = tf.float32)\nresponse_train","338609fb":"n_features = features_train.shape[1]\nn_outcomes = len(np.unique(response_train))\nprint(f'The Number of Features : {n_features}')\nprint(f'The Number of Outcomes : {n_outcomes}')\n","a0a1e9f8":"# One-hot Encoding for Categorical Variable\nresponse_one_hot = tf.one_hot(response_train.numpy(), depth = n_outcomes)\nresponse_one_hot","73aaf6c7":"# Reading Data in Batch\ndef read_batch(batch_size, X, y):\n    sample_size = X.shape[0]\n    indices = list(range(sample_size))\n    np.random.shuffle(indices)           #read data at random\n    for i in range(0, sample_size, batch_size):\n        batch_indices = tf.constant(indices[i : min(i + batch_size, sample_size)])\n        yield tf.gather(X, batch_indices), tf.gather(y, batch_indices)\n        #use yield to iterate over a sequence, but not to store the entire sequence in memory","bcdc81d8":"#Initialize Model Parameters\nw = tf.Variable(tf.random.normal(shape = (n_features, n_outcomes), mean = 0, stddev = 0.01))\nb = tf.Variable(tf.zeros(n_outcomes))","eab5e0b9":"# Softmax Function\ndef softmax(X):\n    return tf.exp(X) \/ tf.reduce_sum(tf.exp(X), 1, keepdims = True) #reduction over rows","96cdc44d":"# Prediction Function\ndef pred_func(w, b , X):\n    return softmax(tf.matmul(tf.reshape(X, (-1, w.shape[0])), w) + b)","a9543fb4":"# Define the Optimization Algorithm\ndef sgd(params, grads, learning_rate, batch_size):\n    # Batch Stochastic Gradient Descent\n    for param, grad in zip(params, grads):\n        param.assign_sub(learning_rate * grad \/ batch_size)","67c0947e":"batch_size = 30\nlearning_rate = 0.3\nnum_epochs = 100\nlosses = []\nfor epoch in range(num_epochs):\n    for X, y in read_batch(20, features_train, response_train):\n        #Compute Gradients and Update Parameters\n        with tf.GradientTape() as g:\n            #One-hot Encoding for Categorical Variable\n            y_one_hot = tf.one_hot(y.numpy(), depth = n_outcomes)\n            y_pred = pred_func(w, b, X)\n            loss   = tf.keras.losses.categorical_crossentropy(y_one_hot, y_pred)\n            dloss_w, dloss_b = g.gradient(loss, [w, b])\n        #Update parameters using their gradients\n        sgd([w, b], [dloss_w, dloss_b], learning_rate, batch_size)\n    train_l = tf.keras.losses.categorical_crossentropy(response_one_hot, pred_func(w, b, features_train))\n    losses.append(float(tf.reduce_mean(train_l)))\n    print(f'epoch {epoch + 1}, loss {float(tf.reduce_mean(train_l)): 0.4f}')","3708fac5":"plt.plot(losses);","75f45f8e":"#Prediction on Train \ny_pred_train = pred_func(w = w, b = b, X = features_train)\ny_pred_train","f1ed26c6":"y_pred_train = np.argmax(y_pred_train, axis = 1)\ny_pred_train","f30c836d":"from sklearn.metrics import accuracy_score, confusion_matrix\nprint(f'The accuracy is : {accuracy_score(y_pred_train, y_train)} \\n\\n The confusion matrix is :\\n\\n {confusion_matrix(y_pred_train, y_train)} ')","3415faa4":"features_test = tf.convert_to_tensor(X_test, dtype = tf.float32)\nfeatures_test","ccea4612":"response_test = tf.convert_to_tensor(y_test, dtype = tf.float32)\nresponse_test","4a2e0651":"#Prediction on Test\ny_pred_test = pred_func(w = w, b = b, X = features_test)\ny_pred_test","47f75be0":"y_pred_test = np.argmax(y_pred_test, axis = 1)\ny_pred_test","8c0f8cae":"from sklearn.metrics import accuracy_score, confusion_matrix\nprint(f'The accuracy is : {accuracy_score(y_pred_test, y_test)} \\n\\n The confusion matrix is :\\n\\n {confusion_matrix(y_pred_test, y_test)} ')","e2c80773":"#Plot Data\ncdict = {0: 'red', 1: 'blue', 2: 'green', 3: 'yellow'}\nplt.figure(figsize = (6, 6))\nfor i in np.unique(response_test):\n    indices = np.where(response_test == i)\n    plt.scatter(x = features_test.numpy()[indices, 0], y = features_test.numpy()[indices, 1], \n                c = cdict[i], label = i,\n                marker = \"o\", alpha = 0.7)\nplt.legend()\n\n#Plot Regions\nx1, x2 = np.meshgrid(np.linspace(features_test.numpy().min() - 1, features_test.numpy().max() + 1, 500), \n                     np.linspace(features_test.numpy().min() - 1, features_test.numpy().max() + 1, 500))\ngrids = np.array((x1.ravel(), x2.ravel())).T\ngrids = tf.convert_to_tensor(grids, dtype = tf.float32)\nregion_pred = pred_func(w = w, b = b, X = grids)\nregion_color = np.argmax(region_pred, axis = 1)\nregion_color = region_color.reshape(500, 500)\nplt.contourf(x1, x2, region_color, alpha = 0.1, levels = [0, 0.5, 1, 2, 3], \n             colors = ['red', 'blue', 'green', 'yellow'])","20ea06f8":"from keras import Sequential\nfrom keras.layers import Dense","3d26a447":"#Scale Features\nfrom sklearn.preprocessing import StandardScaler\n#Initialize the scaler\nscaler = StandardScaler()\n# fit Scaler on features\nX_train = scaler.fit_transform(X_train)","8e12a452":"y_train = keras.utils.to_categorical(y_train)\ny_train","3da42b3e":"model = Sequential()\nmodel.add(Dense(6, activation = 'tanh', input_shape = (2, )))\nmodel.add(Dense(6, activation = 'tanh'))\nmodel.add(Dense(6, activation = 'tanh'))\nmodel.add(Dense(6, activation = 'tanh'))\nmodel.add(Dense(3, activation = 'softmax'))\n\n#Configure the Model\nopt = keras.optimizers.RMSprop() \nmodel.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])","3969f05e":"#Train the Model\nhistory = model.fit(X_train, y_train, epochs = 500, batch_size = 32, verbose = 1, validation_split  = 0.2)","b9f68c9a":"#Loss - Epochs\nplt.figure(figsize = (8, 6))\nplt.plot(model.history.history['loss'], label = 'train')\nplt.plot(model.history.history['val_loss'], alpha = 0.7, label = 'test')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = 'upper left')\nplt.grid()","9f120c5b":"#Loss - Epochs\nplt.figure(figsize = (8, 6))\nplt.plot(model.history.history['accuracy'], label = 'train')\nplt.plot(model.history.history['val_accuracy'], alpha = 0.7, label = 'test')\nplt.ylabel('Accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = 'upper left')\nplt.grid()","9d4ecaf6":"y_pred = model.predict(X_train)\ny_pred","ab74cc84":"y_pred = np.argmax(y_pred, axis = 1)\ny_pred","af71631b":"accuracy_score(y_pred, np.argmax(y_train, axis = 1))","9bc1149e":"#Scale Features\nfrom sklearn.preprocessing import StandardScaler\n#Initialize the scaler\nscaler = StandardScaler()\n# fit Scaler on features\nX_test = scaler.fit_transform(X_test)","2d4b6b03":"y_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred, axis = 1)","4e1018cd":"accuracy_score(y_pred, y_test)","639e474c":"confusion_matrix(y_pred, y_test)","25d8cce0":"# Devide your dataset into train(80%) and test(20%).\n","0ca4bb6f":"# Multi-layer perceptron","a29d0b6f":"# Softmax Regression","665e6da5":"### Q1: Create softmax regression to classify the observations on train dataset.\n###    Use the model to predict on test dataset. Report the accuracy of your model.\n###    Visualize your results.","e2f9f1f4":"## Prediction On Test","2ad97144":"# Q2: Create a multi layer perceptron to classify the observations on train dataset.\n* **Use the model to predict on test dataset. Report the accuracy of your model.**\n* **Visualize your results.**"}}