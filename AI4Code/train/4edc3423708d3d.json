{"cell_type":{"b3e6ce9b":"code","5c7cd182":"code","3c3d7728":"code","d0b215b8":"code","89315cf0":"code","339fbc37":"code","b7e091f2":"code","5eeee06e":"code","90c0c65f":"code","1c686a5e":"code","eec867b4":"markdown","b64de039":"markdown","4c97497a":"markdown","6ed2bc6b":"markdown","86e9caf1":"markdown","91aa1ba9":"markdown","a92ffb73":"markdown","e32615cf":"markdown","bc28e3e4":"markdown"},"source":{"b3e6ce9b":"%%capture\n\n# Install Weights and Biases \n!pip3 install wandb --upgrade >> \/dev\/null\n\n# Packages\nimport os\nimport time\nimport logging\nimport re, math\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import KFold\nfrom kaggle_datasets import KaggleDatasets\n\n# Configure Logging Level\nlogger = tf.get_logger()\nlogger.setLevel(logging.ERROR)\n\n\n# Weights and Biases Setup\nimport wandb\nfrom wandb.keras import WandbCallback\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"WANDB_API_KEY\")\nwandb.login(key=api_key);","5c7cd182":"DEVICE = \"TPU\" \n\nGCS_PATH = KaggleDatasets().get_gcs_path('herb2021-256')\n\nIMG_SIZES = 256\n\nIMAGE_SIZE = [IMG_SIZES, IMG_SIZES]\n\nBATCH_SIZE_SINGLE = 64\n\nEPOCHS = 40\n\nFOLDS = 10\n\nN_CLASSES = 64500","3c3d7728":"if DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            \n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE == \"GPU\":\n    n_gpu = len(tf.config.experimental.list_physical_devices('GPU'))\n    print(\"Num GPUs Available: \", n_gpu)\n    \n    if n_gpu > 1:\n        print(\"Using strategy for multiple GPU\")\n        strategy = tf.distribute.MirroredStrategy()\n    else:\n        print('Standard strategy for GPU...')\n        strategy = tf.distribute.get_strategy()\n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\n\nprint(f'REPLICAS: {REPLICAS}')\n\nBATCH_SIZE = BATCH_SIZE_SINGLE * REPLICAS\nprint(f'BATCH_SIZE: {BATCH_SIZE}')","d0b215b8":"data_augmentation = tf.keras.Sequential([\n  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2, seed=12345),\n])\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_idx\": tf.io.FixedLenFeature([], tf.string),\n        'label' : tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = example['label']\n    return image, label \n\ndef read_labeled_tfrecord_for_test(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_idx\": tf.io.FixedLenFeature([], tf.string),\n        'label' : tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = example['label']\n        \n    return image, label \n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32)    \n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n         for filename in filenames]\n    return np.sum(n)\n\ndef load_dataset(filenames, labeled=True, ordered=False, isTest=False):\n    ignore_order = tf.data.Options()\n    \n    if not ordered:\n        ignore_order.experimental_deterministic = False\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    dataset = dataset.with_options(ignore_order)\n    \n    if isTest == False:\n        dataset = dataset.map(read_labeled_tfrecord)\n    else:\n        dataset = dataset.map(read_labeled_tfrecord_for_test)\n    \n    return dataset\n\ndef get_training_dataset(filenames):\n    dataset = load_dataset(filenames, labeled=True, isTest = False)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_valid_dataset(filenames):\n    dataset = load_dataset(filenames, labeled=True, isTest = True)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_test_dataset(filenames):\n    dataset = load_dataset(filenames, labeled=True, isTest = True, ordered=True)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","89315cf0":"tpu_data_augmentation = tf.keras.Sequential([\n  tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\", seed=12345),\n])\n\ndef build_model(dim = IMG_SIZES, ef = 0):\n    inp = tf.keras.layers.Input(shape=(*IMAGE_SIZE, 3))\n    \n    base = tf.keras.applications.EfficientNetB4(include_top=False, weights='imagenet', \n                          input_shape=(*IMAGE_SIZE, 3), pooling='avg')\n    \n    x = tpu_data_augmentation(inp)\n    x = base(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dense(512)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n    x = tf.keras.layers.Dense(N_CLASSES, activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs = inp,outputs = x)\n    \n    opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n    \n    fn_loss = tf.keras.losses.SparseCategoricalCrossentropy() \n\n    model.compile(optimizer = opt, loss = [fn_loss], metrics=['accuracy'])\n    \n    return model","339fbc37":"display_model = build_model(dim=IMG_SIZES)\n\ndisplay_model.summary()","b7e091f2":"def get_lr_callback(batch_size=8):\n    lr_start   = 0.0002\n    lr_max     = 0.0002 * 10\n    lr_min     = lr_start\/2\n    lr_ramp_ep = 6\n    lr_sus_ep  = 10\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n    \n    return lr_callback","5eeee06e":"all_files = tf.io.gfile.glob(GCS_PATH + '\/train*.tfrec')\n\nnum_total_files = len(all_files)\n\nn_images = count_data_items(all_files)\n\nprint('Total number of files for train-validation:', num_total_files)\nprint('Total number of image for train-validation:', n_images)","90c0c65f":"def train_one_fold(fold, files_train, files_valid):\n    VERBOSE = 1\n    tStart = time.time()\n    \n    # Better Performance\n    if DEVICE=='TPU':\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    # Build the Model\n    K.clear_session()\n    with strategy.scope():\n        print('Building model...')\n        model = build_model(dim=IMG_SIZES)\n    \n    # Callback to Save Model\n    sv = tf.keras.callbacks.ModelCheckpoint('fold-%i.h5'%fold, monitor='val_loss', verbose=1, save_best_only=True,\n                                            save_weights_only=True, mode='min', save_freq='epoch')\n    \n    # Train for One Fold\n    history = model.fit(get_training_dataset(files_train), \n                        epochs=EPOCHS, \n                        callbacks = [sv, get_lr_callback(BATCH_SIZE), WandbCallback()], \n                        steps_per_epoch = count_data_items(files_train)\/BATCH_SIZE\/\/REPLICAS,\n                        validation_data = get_valid_dataset(files_valid), \n                        validation_steps = count_data_items(files_valid)\/BATCH_SIZE\/\/REPLICAS,\n                        verbose=VERBOSE)\n    \n    model.save('b3-aug.h5')\n    \n    #save it as model artifact on W&B\n    artifact =  wandb.Artifact(name=\"b3-aug\", type=\"weights\")\n    artifact.add_file('b3-aug.h5')\n    wandb.log_artifact(artifact)\n    \n    # Record the Time Spent\n    tElapsed = round(time.time() - tStart, 1)\n    \n    print(' ')\n    print('Time (sec) elapsed: ', tElapsed)\n    print('...')\n    print('...')\n    \n    return history","1c686a5e":"SHOW_FILES = True\n\nSTOP_FOLDS = 0\n\nskf = KFold(n_splits = FOLDS, shuffle = True, random_state=54321)\n\nhistories = []\n\nfor fold,(idxT,idxV) in enumerate(skf.split(np.arange(num_total_files))):\n    print('')\n    print('#'*60) \n    print('#### FOLD', fold+1)\n    print('#### Epochs: %i' %(EPOCHS))\n    print('#'*60)\n    \n    train_files = tf.io.gfile.glob([GCS_PATH + '\/train%.3i*.tfrec'%x for x in idxT])\n    valid_files = tf.io.gfile.glob([GCS_PATH + '\/train%.3i*.tfrec'%x for x in idxV])\n    \n    if SHOW_FILES:\n        print('Number of training images', count_data_items(train_files))\n        print('Number of validation images', count_data_items(valid_files))\n        \n    run = wandb.init(project='Herbarium 2021', entity='sauravmaheshkar', reinit=True)\n    \n    history = train_one_fold(fold+1, train_files, valid_files)\n    \n    run.finish()\n    \n    histories.append(history)\n\n    if fold >= STOP_FOLDS:\n        break","eec867b4":"<a id = 'basic'><\/a>\n# Packages \ud83d\udce6 and Basic Setup","b64de039":"<a id = 'train'><\/a>\n# Training \ud83d\udcaa\ud83c\udffb","4c97497a":"<a id = 'data'><\/a>\n# \ud83d\udcbf Tensorflow Dataset from TFRecords","6ed2bc6b":"## Transfer Learning\n\nThe main aim of transfer learning (TL) is to implement a model quickly i.e. instead of creating a DNN (dense neural network) from scratch, the model will transfer the features it has learned from the different dataset that has performed the same task. This transaction is also known as **knowledge transfer**.\n\n---\n\n## EfficientNetB4\n\n![](https:\/\/github.com\/SauravMaheshkar\/X-Ray-Image-Classification\/blob\/main\/assets\/effnet.png?raw=true)\n\n> Excerpt from Google AI Blog\n\n**Convolutional neural networks (CNNs)** are commonly developed at a fixed resource cost, and then scaled up in order to achieve better accuracy when more resources are made available. For example, ResNet can be scaled up from ResNet-18 to ResNet-200 by increasing the number of layers. The conventional practice for model scaling is to arbitrarily increase the CNN depth or width, or to use larger input image resolution for training and evaluation. While these methods do improve accuracy, they usually require tedious manual tuning, and still often yield suboptimal performance. Instead, the authors of [**\"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (ICML 2019)\"**](https:\/\/arxiv.org\/abs\/1905.11946) found a more principled method to scale up a CNN to obtain better accuracy and efficiency.\n\nThey proposed a novel model scaling method that uses a simple yet highly effective **compound coefficient** to scale up CNNs in a more structured manner. Unlike conventional approaches that arbitrarily scale network dimensions, such as width, depth and resolution, their method uniformly scales each dimension with a fixed set of scaling coefficients. The resulting models named **EfficientNets**, superpassed state-of-the-art accuracy with up to **10x** better efficiency (**smaller and faster**).\n\nIn this project we'll use **`EfficientNetB4`** for training our Classifier. The Model can easily be instantiated using the **`tf.keras.applications`** Module, which provides canned architectures with pre-trained weights. For more details kindly visit [this](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications) link. Unhide the below cell to see the `build_model()` function","86e9caf1":"![](https:\/\/github.com\/SauravMaheshkar\/Herbarium2021\/blob\/main\/assets\/Banner.png?raw=true)\n\nThe Herbarium 2021: Half-Earth Challenge is to identify vascular plant specimens provided by the New York Botanical Garden (NY), Bishop Museum (BPBM), Naturalis Biodiversity Center (NL), Queensland Herbarium (BRI), and Auckland War Memorial Museum (AK).\n\nThe Herbarium 2021: Half-Earth Challenge dataset includes more than 2.5M images representing nearly 65,000 species from the Americas and Oceania that have been aligned to a standardized plant list (LCVP v1.0.2).\n\nThis kernel covers how to train a **EfficientNet** using a TFRecords dataset. The notebook is intended to be used on TPU.","91aa1ba9":"# The Model \ud83d\udc77\u200d\u2640\ufe0f","a92ffb73":"## Device Configuration \ud83d\udd0c","e32615cf":"## LearningRate Scheduler\n\n> From a [TowardsDataScience article](https:\/\/towardsdatascience.com\/learning-rate-scheduler-d8a55747dd90)\n\nIn training deep networks, it is helpful to reduce the learning rate as the number of training epochs increases. This is **based on the intuition** that with a high learning rate, the deep learning model would possess high kinetic energy. As a result, it\u2019s parameter vector bounces around chaotically. Thus, it\u2019s unable to settle down into deeper and narrower parts of the loss function (local minima). If the learning rate, on the other hand, was very small, the system then would have low kinetic energy. Thus, it would settle down into shallow and narrower parts of the loss function (false minima).\n\n<center> <img src = \"https:\/\/miro.medium.com\/max\/668\/1*iYWyu8hemMyaBlK6V-2vqg.png\"> <\/center>\n\nThe above figure depicts that a high learning rate will lead to random to and fro moment of the vector around local minima while a slow learning rate results in getting stuck into false minima. Thus, knowing when to decay the learning rate can be hard to find out.\n\nDecreasing the learning rate during training can lead to improved accuracy and (most perplexingly) reduced overfitting of the model. A piecewise decrease of the learning rate whenever progress has plateaued is effective in practice. Essentially this ensures that we converge efficiently to a suitable solution and only then reduce the inherent variance of the parameters by reducing the learning rate.\n\nHere, we'll demonstrate how to use LearningRate schedules to automatically **adapt learning rates** that achieve the **optimal rate of convergence** for stochastic gradient descent. Unhide the cell to see the custom callback.","bc28e3e4":"## Basic Hyperparameters \ud83e\udea1"}}