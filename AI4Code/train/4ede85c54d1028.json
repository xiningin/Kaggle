{"cell_type":{"f38b057c":"code","352716da":"code","f361c68c":"code","3f4de37d":"code","3bdf4c72":"code","7b3ad125":"code","2509b68b":"code","43c24418":"code","874912af":"code","e9148c1e":"code","6cdfaf56":"code","11825d1d":"code","a373d521":"code","fd82b291":"code","68406a8a":"code","4ad1866a":"code","d1972ba4":"code","698bcdae":"code","f6b67e62":"code","2c002f3e":"code","f999f764":"code","d1fb8f4a":"code","bcf39943":"code","266710e4":"code","2b05471b":"code","dc8b9d71":"code","98cab543":"code","2f08e0c4":"code","d9b42a02":"code","5cb8b152":"code","b1c07e5e":"code","8e204737":"code","fe8be9aa":"code","3768022e":"code","16b7d2ba":"code","53b6386d":"markdown","190b27cf":"markdown","a2b2f795":"markdown","2dddabf5":"markdown","34bd4c1d":"markdown","54e7470e":"markdown","26564d3d":"markdown","2476be74":"markdown","93507308":"markdown","a04093f8":"markdown","b96ecfc0":"markdown","8d3e51cf":"markdown","97e860b3":"markdown","ebf6ce6f":"markdown","c809623e":"markdown","dcca8730":"markdown","fdd55489":"markdown","aef750db":"markdown","77eccb80":"markdown","d37d59b2":"markdown","5696707f":"markdown","bd4415a3":"markdown","f2b40557":"markdown","60eea0b8":"markdown","7242d647":"markdown","1e94e679":"markdown","6df19cc6":"markdown","b52cf297":"markdown","5d159b3e":"markdown","7ffc7c28":"markdown","774ccd17":"markdown","d5417d43":"markdown","c1728bff":"markdown"},"source":{"f38b057c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n\n","352716da":"data=pd.read_csv(\"..\/input\/Mall_Customers.csv\")","f361c68c":"data.head()","3f4de37d":"data.shape","3bdf4c72":"data.info()","7b3ad125":"data.describe()","2509b68b":"data.isnull().any()","43c24418":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndata['Gender']=le.fit_transform(data['Gender'])","874912af":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap","e9148c1e":"sns.set(style=\"white\", palette=\"PuBuGn_d\", color_codes=True)\n","6cdfaf56":"sns.countplot('Gender',data=data,palette='winter')\nsize=data['Gender'].value_counts()\nprint('Female :',size[0]\/(size[0]+size[1])*100)\nprint('Male :',size[1]\/(size[0]+size[1])*100)\nplt.title(\"Gender distirbution\")","11825d1d":"plt.figure(1 , figsize = (15 ,6))\nn = 0 \ncolor=['red','green','blue']\ncount=0\nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace =0.5 , wspace = 0.5)\n    sns.distplot(data[x] , color=color[count])\n    plt.title('Distplot of {}'.format(x))\n    count+=1\nplt.show()","a373d521":"sns.pairplot(data)\nplt.plot()","fd82b291":"plt.rcParams['figure.figsize'] = (18, 8)\ncorr=data.corr()\nsns.heatmap(corr)\nplt.title(\"Data correleation\", fontsize=14)\nplt.plot()","68406a8a":"plt.rcParams['figure.figsize'] = (18, 6)\nsns.violinplot(data['Gender'], data['Spending Score (1-100)'], palette = 'pastel')\nplt.title('Gender vs Spending Score', fontsize = 14)\nplt.show()","4ad1866a":"plt.rcParams['figure.figsize'] = (18, 6)\nsns.violinplot(data['Age'], data['Spending Score (1-100)'], palette = 'pastel')\nplt.title('Age vs Spending Score', fontsize = 14)\nplt.show()","d1972ba4":"plt.rcParams['figure.figsize'] = (18, 6)\nsns.violinplot(data['Annual Income (k$)'], data['Spending Score (1-100)'], palette = 'pastel')\nplt.title('Gender vs Spending Score', fontsize = 14)\nplt.show()","698bcdae":"plt.rcParams['figure.figsize'] = (18, 6)\nsns.violinplot(data['Gender'], data['Annual Income (k$)'], palette = 'pastel')\nplt.title('Gender vs Annual Income (k$)', fontsize = 14)\nplt.show()","f6b67e62":"plt.rcParams['figure.figsize'] = (18, 6)\nsns.violinplot(data['Age'], data['Annual Income (k$)'], palette = 'pastel')\nplt.title('Gender vs Annual Income (k$)', fontsize = 14)\nplt.show()","2c002f3e":"X=data.iloc[:,:-1]\ny=data.iloc[:,-1]","f999f764":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=10, n_estimators=300)\nclf.fit(X,y)","d1fb8f4a":"shap_values = shap.TreeExplainer(clf).shap_values(X)","bcf39943":"shap.summary_plot(shap_values[0], X)","266710e4":"shap.dependence_plot(\"Age\", shap_values[0], X)","2b05471b":"shap.dependence_plot(\"Gender\", shap_values[0], X)","dc8b9d71":"shap.dependence_plot('Annual Income (k$)', shap_values[0], X)\nplt.show()","98cab543":"from mpl_toolkits.mplot3d import Axes3D\n\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(18,10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(data['Age'], data[\"Annual Income (k$)\"], data[\"Spending Score (1-100)\"], c='red', s=60)\nax.view_init(30, 185)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Annual Income (k$)\")\nax.set_zlabel('Spending Score (1-100)')\nplt.show()","2f08e0c4":"from sklearn.cluster import KMeans\n\nwcss = []\nfor k in range(1,11):\n    kmeans = KMeans(n_clusters=k, init=\"k-means++\")\n    kmeans.fit(data.iloc[:,1:])\n    wcss.append(kmeans.inertia_)\nplt.figure(figsize=(12,6))    \nplt.grid()\nplt.plot(range(1,11),wcss, linewidth=2, color=\"blue\", marker =\"8\")\nplt.xlabel(\"K Value\")\nplt.xticks(np.arange(1,11,1))\nplt.ylabel(\"WCSS\")\nplt.show()","d9b42a02":"km = KMeans(n_clusters=5)\nclusters = km.fit_predict(data.iloc[:,1:])\ndata[\"label\"] = clusters\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(data.Age[data.label == 0], data[\"Annual Income (k$)\"][data.label == 0], data[\"Spending Score (1-100)\"][data.label == 0], c='blue', s=60)\nax.scatter(data.Age[data.label == 1], data[\"Annual Income (k$)\"][data.label == 1], data[\"Spending Score (1-100)\"][data.label == 1], c='red', s=60)\nax.scatter(data.Age[data.label == 2], data[\"Annual Income (k$)\"][data.label == 2], data[\"Spending Score (1-100)\"][data.label == 2], c='green', s=60)\nax.scatter(data.Age[data.label == 3], data[\"Annual Income (k$)\"][data.label == 3], data[\"Spending Score (1-100)\"][data.label == 3], c='orange', s=60)\nax.scatter(data.Age[data.label == 4], data[\"Annual Income (k$)\"][data.label == 4], data[\"Spending Score (1-100)\"][data.label == 4], c='purple', s=60)\nax.view_init(30, 185)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Annual Income (k$)\")\nax.set_zlabel('Spending Score (1-100)')\nplt.show()","5cb8b152":"data['Spending Score (1-100)']=data['Spending Score (1-100)'].astype(float)","b1c07e5e":"import scipy.cluster.hierarchy as sch\ndendogram=sch.dendrogram(sch.linkage(data,method='ward'))\nplt.title('Dendogram', fontsize=20)\nplt.xlabel(\"Customers\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show()","8e204737":"import scipy.cluster.hierarchy as sch\ndendogram=sch.dendrogram(sch.linkage(data.iloc[:,3:5],method='ward'))\nplt.title('Dendogram', fontsize=20)\nplt.xlabel(\"Customers\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show()","fe8be9aa":"data.head(5)","3768022e":"import scipy.cluster.hierarchy as sch\ndendogram=sch.dendrogram(sch.linkage(data.iloc[:,[1,4]],method='ward'))\nplt.title('Dendogram', fontsize=20)\nplt.xlabel(\"Customers\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show()","16b7d2ba":"import scipy.cluster.hierarchy as sch\ndendogram=sch.dendrogram(sch.linkage(data.iloc[:,[2,4]],method='ward'))\nplt.title('Dendogram', fontsize=20)\nplt.xlabel(\"Customers\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show()","53b6386d":"<b>Loading dependencies for Visualization<\/b>","190b27cf":"> In hierarchical clustering, we assign each object (data point) to a separate cluster. Then compute the distance (similarity) between each of the clusters and join the two most similar clusters","a2b2f795":"We conclude that spending score is more distributed in female","2dddabf5":"## <h2>Hierarchical CLustering <\/h2>\n","34bd4c1d":"<b> Checking for the correleation<b>","54e7470e":"![K means clustering](https:\/\/cdn-images-1.medium.com\/max\/800\/0*rrzG3LyOnAvOepbJ.png)","26564d3d":"Note:- This Kernel is subject to get updated as soon as i find something which can be revelant to the context. Please Upvote if you like the Kernel","2476be74":"## <b>Understanding the distributionn and relation between the attributes<b>","93507308":"<h1 align=\"center\"> End Of Kernel <\/h1>","a04093f8":"## <h1> Determing Relationship with the attributes <\/h2>","b96ecfc0":"<b> Age ,Annual Income and Spending Score Distribution <\/b>","8d3e51cf":"> Implicit objective function in k-Means measures sum of distances of observations from their cluster centroids, called Within-Cluster-Sum-of-Squares (WCSS). This is computed as\n![](https:\/\/content.edupristine.com\/images\/blogs\/Beyond_the_k-Means_5.png)\n","97e860b3":"## <h1 align=\"center\">Introduction <\/h1>Mall Analytics measure the quality of relationships between the mall and the store. By tracking customers we analyize their shopping behaviour and spending index.\n![mall](https:\/\/www.dw.com\/image\/17955220_303.jpg)","ebf6ce6f":"> where Yi is centroid for observation Xi. By definition, this is geared towards maximizing number of clusters, and in limiting case each data point becomes its own cluster centroid. This is, naturally, neither practical nor desirable. Fig. 2 plots WCSS for k=1.20 and we can see that it continuously drops, indicating more clusters the better!","c809623e":"Taking gender and Spending Score in account","dcca8730":"Shape of the data","fdd55489":"We will be using pairs plot which allows us to see both distribution of single variables and relationships between two variables. Pair plots are a great method to identify trends for follow-up analysis and, fortunately and here in this example we will identify the pattern ","aef750db":"Basic Information ","77eccb80":"A great insight, why female contribute more to the shopping \n> The real reason is sobering.  In virtually every society in the world, women have primary care-giving responsibilities for both children and the elderly (and often, just about everybody else in-between). In this primary caregiving role, women find themselves buying on behalf of everyone else in their lives. More here \n> https:\/\/www.forbes.com\/sites\/bridgetbrennan\/2013\/03\/06\/the-real-reason-women-shop-more-than-men\/#1a0c65d174b9","d37d59b2":"When taking Annual Income and Spending Score in account","5696707f":"The one with the least inference with each other can be analysized by seeing the color saturity. We see that Age is highly uncorreleated with the spending index. The maximum correleation is represnted by the bright skin colour and least with the black colour. We analyized from the heatmap, that the data is not well correleated\n","bd4415a3":"K Means Clustering \n> k-means is one of the simplest unsupervised learning algorithms that solve the clustering problems. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters). The main idea is to define k centers, one for each cluster.\n\n> To start with k-means algorithm, you first have to randomly initialize points called the cluster centroids (K). K-means is an iterative algorithm and it does two steps: 1. Cluster assignment 2. Move centroid step.\n\n> 1. Cluster assignment\n\n> the algorithm goes through each of the data points and depending on which cluster is closer, It assigns the data points to one of the three cluster centroids.\n\n> 2. Move centroid\n\n> Here, K-means moves the centroids to the average of the points in a cluster. In other words, the algorithm calculates the average of all the points in a cluster and moves the centroid to that average location.\n\n> This process is repeated until there is no change in the clusters (or possibly until some other stopping condition is met). K is chosen randomly or by giving specific initial starting points by the user.","f2b40557":"From the pair plot , we figure out that the <b>Age<\/b> between <b>20-40<\/b> having high spending index and following it, the spending score doesn't show any frequent rise in the score. \nWe also conclude that <b>age between 20-40<\/b> have dense and higher Annual Income and the trend decreases down the age. We also see that <b>Spending score<\/b> is releatively less with higher Annual income (50-75)K compare to 25-50K Annual income. Spending Index (45-60) becomes constant for indiviudal with <b> Annual income between 50-75K dollar <\/b> and then the spending index increases for higher and lower Annual income. This is weird!\n","60eea0b8":"Taking age and spending score in account","7242d647":"<b>K=5<\/b>","1e94e679":"Taking every attribute in account","6df19cc6":"<b> Checking for the Null values <\/b>","b52cf297":"Violin plot vs the Box plot\n> a violin plot is more informative than a plain box plot. While a box plot only shows summary statistics such as mean\/median and interquartile ranges, the violin plot shows the full distribution of the data. The difference is particularly useful when the data distribution is multimodal (more than one peak). In this case a violin plot shows the presence of different peaks, their position and relative amplitude.","5d159b3e":"<b>The First Gaze<b>","7ffc7c28":"* 1 represents Male\n* 0 represents Female","774ccd17":"## <h1> CLUSTERING <\/h1>","d5417d43":"<b>Loading the datatset<b>","c1728bff":"<b> Gender Distribution <\/b>"}}