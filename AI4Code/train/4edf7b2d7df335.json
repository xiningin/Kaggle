{"cell_type":{"a852ecfd":"code","48cbdb18":"code","8a133c96":"code","5971a317":"code","ecefa383":"code","9ac4d9b5":"code","7ebdd23f":"code","95cd651d":"code","f8bbc764":"code","806b6c77":"code","9f2b89d9":"code","570c5045":"code","54a73c52":"code","5d079f00":"code","374390da":"code","3dec5e4d":"code","caaea8d4":"code","5b0db3a2":"code","cbcc925d":"code","40c60e56":"code","b7a4530e":"code","499e9ed5":"code","e8a2e4e6":"code","6969b5bf":"code","989a79cf":"code","899bb0b5":"code","820f0090":"code","9ab98d15":"code","833dde3b":"code","3cb8aedc":"code","fef0f2c8":"code","1a00d6ab":"code","7f7d6a5d":"code","65885137":"code","c87e8cda":"code","69bfa0d9":"code","e3e23368":"code","7792acf0":"code","e152bac2":"code","42c3eefa":"code","de294bc9":"code","f2100956":"code","f77ce654":"code","caf027dd":"code","b53fbf34":"code","2ca05c1b":"code","16a82f84":"code","82ba6e3e":"code","928e2026":"code","d4d8aaee":"code","94d3d902":"code","da64cb67":"code","6ca8c7c5":"code","80dc55d6":"code","ce62d3aa":"code","fbb92f3a":"code","8014c87c":"code","ac6df531":"markdown","4cfd76e4":"markdown","561c6f19":"markdown","2b180de6":"markdown","c4dfaf87":"markdown","2b83cf38":"markdown","dba13066":"markdown","b172ccce":"markdown","a2a7b152":"markdown","e5adf04c":"markdown","219bddc6":"markdown","e27623d8":"markdown","912db011":"markdown","b991c3b6":"markdown","713f97a8":"markdown","279b6e10":"markdown","a793a4c8":"markdown","48df2d61":"markdown","9003dfd7":"markdown","d4855698":"markdown","b86f486b":"markdown","7fe877bc":"markdown","df87e0ab":"markdown","b04af1a6":"markdown","26e1f26c":"markdown"},"source":{"a852ecfd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error, make_scorer\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","48cbdb18":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","8a133c96":"print (\"Train: \",train.shape[0],\"sales, and \",train.shape[1],\"features\")\nprint (\"Test: \",test.shape[0],\"sales, and \",test.shape[1],\"features\")","5971a317":"train.head()","ecefa383":"test.head()","9ac4d9b5":"num = np.array(train.dtypes!=\"object\")\nprint(\"numeric type\",sum(num) )\ncat = np.array(train.dtypes==\"object\")\nprint(\"object type\", sum(cat))","7ebdd23f":"numerical = [f for f in train.columns if train.dtypes[f] != 'object']\nnumerical.remove('SalePrice')\n# we don't required (Id not a feature)\nnumerical.remove('Id')\ncategorical = [f for f in train.columns if train.dtypes[f] == 'object']\nprint(numerical)","95cd651d":"print(categorical)","f8bbc764":"plt.hist(train['SalePrice'])\nplt.show()","806b6c77":"from statsmodels.graphics.gofplots import qqplot\n\nqqplot(train['SalePrice'], line='s')\nplt.show()","9f2b89d9":"from scipy.stats import shapiro\n# normality test\nstat, p = shapiro(train['SalePrice'])\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","570c5045":"import scipy.stats as st\ny = train['SalePrice']\nfig, ax =plt.subplots(1,3,figsize=(15,6))\nplt.suptitle('Johnson SU V\/S Normal V\/S Log Normal distribution')\nsns.distplot(y, kde=False, fit=st.johnsonsu,ax=ax[0])\nsns.distplot(y, kde=False, fit=st.norm,ax=ax[1])\nsns.distplot(y, kde=False, fit=st.lognorm,ax=ax[2])\nfig.show()","54a73c52":"num = numerical + ['SalePrice']\ncorrmat = train[num].corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, square=True);","5d079f00":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n","374390da":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","3dec5e4d":"# Dropping the outlier\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","caaea8d4":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","5b0db3a2":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny = train['SalePrice'].reset_index(drop=True)\nplt.hist(y)\nplt.show()","cbcc925d":"sns.distplot(y, kde=False, fit=st.norm)","40c60e56":"qqplot(y, line='s')\nplt.show()","b7a4530e":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","499e9ed5":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","e8a2e4e6":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","6969b5bf":"all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","989a79cf":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","899bb0b5":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","820f0090":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","9ab98d15":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","833dde3b":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","3cb8aedc":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","fef0f2c8":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","1a00d6ab":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    ","7f7d6a5d":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","65885137":"train = all_data[:ntrain]\ntest = all_data[ntrain:]\n","c87e8cda":"X_train,X_test,y_train,y_test = train_test_split(train,y,test_size = 0.3,random_state= 0)","69bfa0d9":"X_train.head(3)\n","e3e23368":"n_folds = 5\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import KFold\nscorer = make_scorer(mean_squared_error,greater_is_better = False)\ndef rmse_CV_train(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model,X_train,y_train,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)\ndef rmse_CV_test(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model,X_test,y_test,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)","7792acf0":"lr = LinearRegression()\nlr.fit(X_train,y_train)\ntest_pre = lr.predict(X_test)\ntrain_pre = lr.predict(X_train)\nprint('rmse on train',rmse_CV_train(lr).mean())\nprint('rmse on train',rmse_CV_test(lr).mean())","e152bac2":"#plot between predicted values and residuals\nplt.scatter(train_pre, train_pre - y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(test_pre,test_pre - y_test, c = \"red\",  label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"black\")\nplt.show()","42c3eefa":"# Plot predictions - Real values\nplt.scatter(train_pre, y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(test_pre, y_test, c = \"red\",  label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"black\")\nplt.show()","de294bc9":"ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 7, 8,8.5,9,10, 30, 60])\nridge.fit(X_train,y_train)\nalpha = ridge.alpha_\nprint('best alpha',alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4],cv = 5)\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\nprint(\"Ridge RMSE on Training set :\", rmse_CV_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_CV_test(ridge).mean())\ny_train_rdg = ridge.predict(X_train)\ny_test_rdg = ridge.predict(X_test)\n","f2100956":"coef = pd.Series(ridge.coef_, index = X_train.columns)\n\nprint(\"Ridge picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","f77ce654":"# Plot residuals\nplt.scatter(y_train_rdg, y_train_rdg - y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(y_test_rdg, y_test_rdg - y_test, c = \"red\", marker = \"v\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"black\")\nplt.show()\n","caf027dd":"# Plot predictions - Real values\nplt.scatter(y_train_rdg, y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(y_test_rdg, y_test, c = \"red\",  label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"black\")\nplt.show()","b53fbf34":"predict_y_ridge = ridge.predict(test)\npredict_y_ridge =np.expm1(predict_y_ridge)","2ca05c1b":"lasso = LassoCV(alphas = [0.0001,0.0006,0.0007,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100])\nlasso.fit(X_train,y_train)\nalpha = lasso.alpha_\nprint('best alpha',alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4],cv = 5)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\nprint(\"Lasso RMSE on Training set :\", rmse_CV_train(lasso).mean())\nprint(\"Lasso RMSE on Test set :\", rmse_CV_test(lasso).mean())\ny_train_lso = lasso.predict(X_train)\ny_test_lso = lasso.predict(X_test)\n","16a82f84":"coef = pd.Series(lasso.coef_, index = X_train.columns)\n\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","82ba6e3e":"# Plot residuals\nplt.scatter(y_train_lso, y_train_lso - y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(y_test_lso, y_test_lso - y_test, c = \"red\", marker = \"v\", label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"black\")\nplt.show()","928e2026":"# Plot predictions - Real values\nplt.scatter(y_train_lso, y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(y_test_lso, y_test, c = \"red\",  label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"black\")\nplt.show()","d4d8aaee":"predict_y_lasso = lasso.predict(test)\npredict_y_lasso=np.expm1(predict_y_lasso)","94d3d902":"Enet = ElasticNetCV(alphas = [10,1,0.1,0.01,0.001,0.002,0.003,0.004,0.005,0.00054255])\nEnet.fit(X_train,y_train)\nalpha = Enet.alpha_\nprint('best alpha',alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nEnet = ElasticNetCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4],cv = 5)\nEnet.fit(X_train, y_train)\nalpha = Enet.alpha_\nprint(\"Best alpha :\", alpha)\nprint(\"Elastic Net RMSE on Training set :\", rmse_CV_train(Enet).mean())\nprint(\"Elastic Net RMSE on Test set :\", rmse_CV_test(Enet).mean())\ny_train_Enet = Enet.predict(X_train)\ny_test_Enet = Enet.predict(X_test)\n","da64cb67":"coef = pd.Series(Enet.coef_, index = X_train.columns)\n\nprint(\"Elastic Net picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","6ca8c7c5":"# Plot residuals\nplt.scatter(y_train_Enet, y_train_Enet - y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(y_test_Enet, y_test_Enet - y_test, c = \"red\", marker = \"v\", label = \"Validation data\")\nplt.title(\"Linear regression with Elastic Net\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"black\")\nplt.show()","80dc55d6":"# Plot predictions - Real values\nplt.scatter(y_train_Enet, y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(y_test_Enet, y_test, c = \"red\",  label = \"Validation data\")\nplt.title(\"Linear regression with Elastic Net\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"black\")\nplt.show()","ce62d3aa":"predict_y_Enet = Enet.predict(test)\npredict_y_Enet=np.expm1(predict_y_Enet)","fbb92f3a":"models =(predict_y_ridge+predict_y_lasso+predict_y_Enet)\/3","8014c87c":"my_submission = pd.DataFrame({'Id': test_ID, 'SalePrice': models})\nprint(my_submission)\nmy_submission.to_csv('submission.csv', index=False)","ac6df531":"The above histogram shows Skewness in target variable \"Sale price\". Let's confirm the same from normality test","4cfd76e4":"## Elastic Net","561c6f19":"### 7.Dealing with missing data","2b180de6":"and the above one are categorical column","c4dfaf87":"All the above column are of numerical in nature","2b83cf38":"### 4.Correlation map of salesprice with numerical column","dba13066":"## Ridge Regression","b172ccce":"dividing features on the basis of their type numerical or categorical","a2a7b152":"## Lasso Regression","e5adf04c":"### Label Encoding","219bddc6":"#### Comparing Johnson su, normal and Log normal distribution","e27623d8":"#### Shapiro normality test","912db011":"All the missing value has been replaced","b991c3b6":"Replacing missing value","713f97a8":"### 1.Reading CSV files","279b6e10":"### 3.Distribution of the data","a793a4c8":"### 10. My Submission ","48df2d61":"#### Spread of salesprice over 4 quartile.","9003dfd7":"### Target variable","d4855698":"## Linear Regression","b86f486b":"### 8.Skewness and box cox Transformation","7fe877bc":"### 2.EDA","df87e0ab":"### 5. Transforming the target variable","b04af1a6":"we have 80 features to predict one target variable \"Salesprice\"","26e1f26c":"### 9.Model Building"}}