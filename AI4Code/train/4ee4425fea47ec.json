{"cell_type":{"65ceb38e":"code","459ef391":"code","b869b8cf":"code","79280073":"code","cb491407":"code","04702f4c":"code","3ba67f77":"code","657717ac":"code","454ceee4":"code","11e9b0bb":"code","8b22a504":"code","68f41b48":"code","15838848":"code","c12195f8":"code","2db6bb92":"markdown","629fe192":"markdown","1509de15":"markdown","a45c2ad4":"markdown","8abcfcd8":"markdown","71ebf408":"markdown","63cc2403":"markdown","7325c866":"markdown","fe81e307":"markdown","b5fae013":"markdown"},"source":{"65ceb38e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","459ef391":"# Import training data - review shape and columns\nimport pandas as pd\ntrain = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv', index_col='Id')\n#print(train.info())","b869b8cf":"# Make a copy of train df for ML experiments\ntrain_2 = train.copy()\ntrain_2.drop(['Soil_Type7','Soil_Type15', 'Soil_Type8','Soil_Type25'], axis = 1, inplace=True)\ntrain_2.columns\n\nprint(train_2.columns)\nprint(train_2.columns.get_loc('Wilderness_Area1'))\nprint(train_2.columns.get_loc('Soil_Type40'))","79280073":"numerical_df = train_2.iloc[:,[0,1,2,3,4,5,6,7,8,9,50]] # Include Cover Type as well along with numerical features\nX_num = numerical_df.drop('Cover_Type', axis=1)\ny_num = numerical_df.Cover_Type\nprint(X_num.columns)\nprint(y_num[:5])\ncat_df = train_2.iloc[:,10:51] # Include Cover Type along with categorical features\nX_cat = cat_df.drop('Cover_Type', axis= 1)\ny_cat = cat_df.Cover_Type\nprint(X_cat.columns)\nprint(y_cat[:5])","cb491407":"from sklearn.feature_selection import SelectKBest, f_classif,chi2\nselector_num = SelectKBest(score_func=f_classif,k=10)\nselector_num.fit(X_num,y_num)\n#print(\"scores_:\",selector_num.scores_)\n#print(\"pvalues_:\",selector_num.pvalues_)\n#print(pd.DataFrame(list(zip([list(X_num.columns)],selector_num.scores_,selector_num.pvalues_))))\ndf_num_kbest = pd.DataFrame(list(zip(list(X_num.columns),selector_num.scores_,selector_num.pvalues_)), columns=['Feature','Score','P-value'])\ndf_num_kbest.sort_values('Score', ascending=False)\nX_num_top7 = list(df_num_kbest.sort_values('Score', ascending=False).Feature[:7])\nprint('Top 7 Numerical Features: ',X_num_top7)\n","04702f4c":"from sklearn.feature_selection import SelectKBest, f_classif,chi2\nselector_cat = SelectKBest(score_func=chi2,k=40)\nselector_cat.fit(X_cat,y_cat)\n#print(\"scores_:\",selector_cat.scores_)\n#print(\"pvalues_:\",selector_cat.pvalues_)\n#print(pd.DataFrame(list(zip([list(X_num.columns)],selector_num.scores_,selector_num.pvalues_))))\ndf_kbest = pd.DataFrame(list(zip(list(X_cat.columns),selector_cat.scores_,selector_cat.pvalues_)), columns=['Feature','Score','P-value'])\ndf_kbest.sort_values('Score', ascending=False)\n","3ba67f77":"from sklearn.feature_selection import SelectKBest, f_classif,chi2\nselector_cat = SelectKBest(score_func=f_classif,k=40)\nselector_cat.fit(X_cat,y_cat)\n#print(\"scores_:\",selector_cat.scores_)\n#print(\"pvalues_:\",selector_cat.pvalues_)\n#print(pd.DataFrame(list(zip([list(X_num.columns)],selector_num.scores_,selector_num.pvalues_))))\ndf_cat_kbest = pd.DataFrame(list(zip(list(X_cat.columns),selector_cat.scores_,selector_cat.pvalues_)), columns=['Feature','Score','P-value'])\nprint(df_cat_kbest.sort_values('Score', ascending=False))\n\n","657717ac":"X_cat_top30 = list(df_cat_kbest.sort_values('Score', ascending=False).Feature[:30])\nprint('Top 30 Categorical Features: ',X_cat_top30)","454ceee4":"# Combine Top 7 numerical features and Top 30 categorical features\ntop_37_features = X_num_top7 + X_cat_top30\ntop_37_features","11e9b0bb":"top_37_features","8b22a504":"X_FE1 = train_2[top_37_features]\nX_FE1.shape\ny_FE1 = train_2.Cover_Type\ny_FE1[:5]\n\n# Split X and y into Train and Validation sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_FE1,y_FE1,test_size = 0.2,random_state = 99)","68f41b48":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Function to execute random forest\ndef rf_grid(X_train,y_train, param_grid, cv=5):\n    rf = RandomForestClassifier(random_state=99)\n    rf_grid = GridSearchCV(rf,param_grid, cv=5)\n    rf_grid.fit(X_train,y_train)\n    y_pred = rf_grid.predict(X_val)\n    print(pd.DataFrame(rf_grid.cv_results_)[['params','mean_test_score']])\n    print('Random Forest Best Parameters: ',rf_grid.best_params_)\n    print('Random Forest Best Training Score: ',rf_grid.best_score_)\n    print('Random Forest Validation Accuracy is: ', accuracy_score(y_val,y_pred))\n\nparam_grid_rf = {'n_estimators': [700,800,1000,1200,1400,1600,1800]}\n\n# Execute Random Forest\nrf_grid(X_train,y_train,param_grid_rf)","15838848":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Function to execute Extremely Randomized Trees\ndef extra_trees_grid(X_train,y_train, param_grid, cv=5):\n    extra_trees = ExtraTreesClassifier(random_state=99)\n    extra_trees_grid = GridSearchCV(extra_trees,param_grid, cv=5)\n    extra_trees_grid.fit(X_train,y_train)\n    y_pred = extra_trees_grid.predict(X_val)\n    print(pd.DataFrame(extra_trees_grid.cv_results_)[['params','mean_test_score']])\n    print('Extra Trees Best Parameters: ',extra_trees_grid.best_params_)\n    print('Extra Trees Best Training Score: ',extra_trees_grid.best_score_)\n    print('Extra Trees Validation Accuracy is: ', accuracy_score(y_val,y_pred))\n    \nparam_grid_extra = {'n_estimators': [700,1000,1400,1700,2000,2200,2400]}\n\n# Execute Extremely Randomized Trees\nextra_trees_grid(X_train,y_train,param_grid_extra)","c12195f8":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Function to execute lightGBM classifier\ndef lgbm_grid(X_train,y_train, param_grid, cv=5):\n    lgbm = LGBMClassifier(random_state=99)\n    lgbm_grid = GridSearchCV(lgbm,param_grid, cv=5)\n    lgbm_grid.fit(X_train,y_train)\n    y_pred = lgbm_grid.predict(X_val)\n    print(pd.DataFrame(lgbm_grid.cv_results_)[['params','mean_test_score']])\n    print('LightGBM Best Parameters: ',lgbm_grid.best_params_)\n    print('LightGBM Best Training Score: ',lgbm_grid.best_score_)\n    print('LightGBM Validation Accuracy is: ', accuracy_score(y_val,y_pred))\n\nparam_grid_lgbm = {'n_estimators': [1000,1200,1400,1600,1800]}\n\n# Execute LightGBM classifier\nlgbm_grid(X_train,y_train,param_grid_lgbm)","2db6bb92":"#### Get Top 30\/40 categorical features","629fe192":"## Run RF, Extra trees and LGBM with new reduced data set","1509de15":"# Univariate Feature Selection\n## 1. Select KBest\nIn SelectKBest, we derive statistically the metrics to calculate feature importance for a classification problem. It does so by evaluating one feature at a time in regards to its ability to influence the target class. We then rank the features based on the metric to pick only K best features for machine learning tasks.\n\nFor classification problems, there are 2 metrics that are used:\n1. chi2 - this is used to validate that occurance of a categorical feature influences occurance of a categorical outcome. So this is used when we are trying to evaluate feature importance for categorical features only.\n2. f_classif - this is used to evaluate the importance of numerical as well as categorical features for a categorical outcome, using a F-test. So we can check the numerical features as well as the categorical features we earlier evaluated with chi2. Typically both metrics give similar feature importance scores.\n\nBefore we apply SelectKBest, we need to split the features into numerical and categorical features for reasons stated above and then apply the feature selection in 2 steps.\n\n1. Using Chi2 as the metric - we select only the categorical features - 4 Wild Areas and 36 Soil Types (remember we removed the 4 Soil type features earlier)\n2. Using f_classif as the metric - we select only the 10 numerical features - first 10 columns of our dataset.\n\n\n","a45c2ad4":"### Apply SelectKBest to categorical features with score_func as chi2","8abcfcd8":"### Apply SelectKBest to categorical features with score_func as f_classif - Get Top 30 features","71ebf408":"Looks like bothf_classif as well as chi2 give similar ranking for categorical features. So we can just use f_classif rankings itself.","63cc2403":"### Apply SelectKBest to numerical features with score_func as f_classif\nWe will set K = Total number of features under consideration so that we can get a view of ranking for all features. This will be 10 for numerical features and 40 for categorical features (Wilderness Areas and Soil Types (excluding the 4 Soil Types we already removed)\n\nLets rank all numerical features and get top 7\/10 numerical features","7325c866":"### Conclusion from SelectKBest\n\nWe can either select the top 40 features from the overall list that as numerical as well as categorical features. Or we can take top 7 features from numerical and the top 30 categorical features. Lets take the latter approach and get the final list of features to be considered. We will then redetermine the feature and target arrays from this and proceed with machine learning tasks.\n\nThe final 37 features in our first feature selection model are :\n","fe81e307":"#### Get Numerical and Categorical Features separately for SelectKBest","b5fae013":"We see that Aspect, Vertical Distance to Hydrology are at the bottom of the stack. Vertical distance to hydrology has strong correlation with Horizontal distance to hydrology. so probably this is fine."}}