{"cell_type":{"8d1f16d8":"code","d4171eb2":"code","eb15032a":"code","99081d94":"code","20b5a288":"code","90248c95":"code","eb58e660":"code","d62bdc36":"code","15e01e47":"code","9231fbae":"code","bbff2e7b":"code","9195fd5b":"code","73b97ade":"code","d8f51637":"code","c66b669b":"code","e2b19d96":"code","509fb4c1":"code","269f7356":"code","a76d310b":"markdown","e0f39837":"markdown","91357717":"markdown","304699be":"markdown","1a3a1b3a":"markdown","e29262eb":"markdown","7610add2":"markdown","14c54a39":"markdown","16e23579":"markdown","ec0a0603":"markdown","bffef94b":"markdown","5ed2a451":"markdown","240b6c68":"markdown","f1683403":"markdown","1802cd3b":"markdown","bf8c3034":"markdown","8f0e510f":"markdown","1182125b":"markdown","b0842690":"markdown","3f8ccdaa":"markdown","f4963a4a":"markdown"},"source":{"8d1f16d8":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sea\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.inspection import permutation_importance\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import StackingClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\nimport matplotlib.pyplot as plt","d4171eb2":"sea.set_style(\"darkgrid\")","eb15032a":"data = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\n\ndata.head(10).style.set_precision(2). \\\n                    set_properties(**{\"min-width\": \"50px\"}). \\\n                    set_properties(**{\"color\": \"#111111\"}). \\\n                    set_properties(**{\"text-align\": \"center\"}). \\\n                    set_table_styles([\n                          {\"selector\": \"th\",\n                           \"props\": [(\"font-weight\", \"bold\"),\n                                     (\"font-size\", \"12px\"),\n                                     (\"text-align\", \"center\")]},\n                          {\"selector\": \"tr:nth-child(even)\",\n                           \"props\": [(\"background-color\", \"#f2f2f2\")]},\n                          {\"selector\": \"tr:nth-child(odd)\",\n                           \"props\": [(\"background-color\", \"#fdfdfd\")]},\n                          {\"selector\": \"tr:hover\",\n                           \"props\": [(\"background-color\", \"#bcbcbc\")]}])\n","99081d94":"# disable SettingWithCopyWarning\npd.options.mode.chained_assignment = None\n\ndata_X = data.loc[:, data.columns != \"target\"]\ndata_Y = data[[\"target\"]]","20b5a288":"print(\"\\ndata_X info:\\n\")\ndata_X.info()\nprint(\"\\ndata_Y info:\\n\")\ndata_Y.info()","90248c95":"print(\"Target classes \", data_Y[\"target\"].unique())\n\nweights = compute_class_weight(\"balanced\",\n                        classes = data_Y[\"target\"].unique().ravel(),\n                        y = data_Y[\"target\"]);\n\nprint(\"Class weights \", weights)","eb58e660":"train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y,\n                                                    test_size=0.2,\n                                                    random_state=0)\n\ntrain_X.reset_index(drop=True, inplace=True);\ntest_X.reset_index(drop=True, inplace=True);\ntrain_Y.reset_index(drop=True, inplace=True);\ntest_Y.reset_index(drop=True, inplace=True);\n\nfeature_names = train_X.columns","d62bdc36":"fig, axes = plt.subplots(len(train_X.columns), 2, figsize=(10,50))\n\nfor i, f in enumerate(train_X.columns):\n    sea.distplot(train_X[f], kde = False, color = \"#167c02\",\n                 hist_kws = dict(alpha=0.7), ax=axes[i][0]);\n    sea.violinplot(x=train_Y[\"target\"], y=train_X[f],\n                 palette = [\"#5294e3\", \"#a94157\"], ax=axes[i][1]);","15e01e47":"scaler = StandardScaler()\n\n# fit to train_X\nscaler.fit(train_X)\n\n# transform train_X\ntrain_X = scaler.transform(train_X)\ntrain_X = pd.DataFrame(train_X, columns = feature_names)\n\n# transform test_X\ntest_X = scaler.transform(test_X)\ntest_X = pd.DataFrame(test_X, columns = feature_names)","9231fbae":"corr_matrix = pd.concat([train_X, train_Y], axis=1).corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=np.bool))\n\nplt.figure(figsize=(10,8))\nsea.heatmap(corr_matrix,annot=True, fmt=\".2f\",\n            vmin=-1, vmax=1, linewidth = 1,\n            center=0, mask=mask,cmap=\"RdBu_r\");","bbff2e7b":"lr = LogisticRegression(max_iter = 1000)\nlr.fit(train_X, train_Y.values.ravel())\n\ns = permutation_importance(lr, train_X, train_Y, n_repeats = 200,\n                           scoring = \"accuracy\", random_state = 0)\n\nfor i in s.importances_mean.argsort()[::-1]:\n    print(\"{:10}\\t{: .4f}\\t{: .4f}\".format(feature_names[i],\n                                           s.importances_mean[i],\n                                           s.importances_std[i]))","9195fd5b":"drop = [\"slope\", \"age\", \"fbs\"]\nfor d in drop:\n    train_X.drop(d, axis=1, inplace=True)\n    test_X.drop(d, axis=1, inplace=True)\n\nfeature_names = train_X.columns","73b97ade":"def create_base():\n   \n    model = Sequential()\n    \n    model.add(Dense(128, input_dim=len(feature_names), activation=\"relu\"))\n    model.add(Dropout(0.1))\n    model.add(Dense(64, activation=\"relu\"))\n    model.add(Dropout(0.1))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    \n    model.compile(loss=\"binary_crossentropy\",\n                  optimizer=tf.keras.optimizers.Adam(0.01))\n    \n    return model\n\nbase_nn = KerasClassifier(build_fn=create_base,\n                       epochs=60, batch_size = 16)\nbase_nn._estimator_type = \"classifier\"\nbase_dt = DecisionTreeClassifier(random_state = 0) \nbase_svm = SVC(kernel=\"linear\", random_state = 10)","d8f51637":"model_names = [\"BaggingClassifier\",\n               \"RandomForestClassifier\",\n               \"AdaBoostClassifier\",\n               \"GradientBoostingClassifier\",\n               \"VotingClassifier\",\n               \"StackingClassifier\"]\n\nmodels = []\n\nmodels.append(BaggingClassifier(base_estimator = base_nn,\n                                n_estimators=10, random_state=0))\n\nmodels.append(RandomForestClassifier(n_estimators = 20,\n                                     random_state = 11))\n\nmodels.append(AdaBoostClassifier(base_estimator = base_dt,\n                                 n_estimators=10, random_state=10))\n\nmodels.append(GradientBoostingClassifier(n_estimators = 500,\n                                         random_state=8))\n\nmodels.append(VotingClassifier(estimators=[\n                    ('dt_0', base_dt),('dt_1', base_dt),('dt_2', base_dt)],\n                    voting='soft'))\n\nmodels.append(StackingClassifier(estimators=[\n            ('svm_0', base_svm), ('svm_1', base_svm), ('svm_2', base_svm)],\n            final_estimator=LogisticRegression(max_iter=1000),\n            stack_method=\"predict\", cv=5))","c66b669b":"scores = []\nfor m in models:\n    scores.append(cross_val_score(m, train_X, train_Y.values.ravel(),\n                        scoring = \"accuracy\", cv = 5, n_jobs = 1))","e2b19d96":"mean_scores = np.mean(scores, axis=1)\n   \nfor i in mean_scores.argsort()[::-1]:\n    print(\"{:30}\\t{:.4f}\".format(model_names[i], mean_scores[i]))","509fb4c1":"for m in models:\n    m.fit(train_X, train_Y.values.ravel())","269f7356":"acc = []\n\nfor m in models:\n    y_pred = m.predict(test_X)\n    acc.append(accuracy_score(test_Y, y_pred))\n    \nfor i in np.array(acc).argsort()[::-1]:\n    print(\"{:30}\\t{:.4f}\".format(model_names[i], acc[i]))","a76d310b":"There are 303 examples (rows) and 13 features (columns) in the dataset. Features and target are numeric.","e0f39837":"Features are assigned to **data_X** and corresponding labels to **data_Y**.","91357717":"A neural network is chosen to be used as base estimator for BaggingClassifier. Neural network model is designed with Keras and TensorFlow backend. It is wrapped with scikit-learn API **tf.keras.wrappers.scikit_learn.KerasClassifier**.\n\nDecision tree classifier is used as base estimator for AdaBoostClassifier and VotingClassifier. For StackingClassifier, support vector classifier with linear kernel is used as base estimator and logistic regression is used as meta learner.","304699be":"## Load and Analyze Data","1a3a1b3a":"After seeing cross validation results, the models are trained on full training data.","e29262eb":"The least important 3 features are dropped. Note that correlation of dropped features with target are also low (refer to correlation matrix). ","7610add2":"## Split Data\n\nDataset is split as training and test sets.","14c54a39":"Pandas **info()** shows column (feature) data types and number of non-null values.","16e23579":"## Data Visualization\n\nFor each feature, a histogram and a violin plot (feature vs target) are drawn.","ec0a0603":"Test accuracies are higher than cross validation results. Remember, in cross validation, training was performed on training split and validation was performed on test split of a fold. On the other hand, the models used to predict on test set were trained on whole training data.","bffef94b":"## Feature Importance\n\nPermutation feature importance method is used to evaluate the effect of each feature on the classification task. It is defined as the decrease in model performance when a specific feature is shuffled. Process is repeated a number of times for each feature and mean decrease is taken into account.","5ed2a451":"Some of scikit-learn ensemble methods are compared using **Heart Disease UCI** dataset. Features are given below.\n\n* age\n* sex\n* chest pain type (4 values)\n* resting blood pressure\n* serum cholestoral in mg\/dl\n* fasting blood sugar > 120 mg\/dl\n* resting electrocardiographic results (values 0,1,2)\n* maximum heart rate achieved\n* exercise induced angina\n* oldpeak, ST depression induced by exercise relative to rest\n* the slope of the peak exercise ST segment\n* number of major vessels colored by flourosopy (values 0,1,2,3,4)\n* thal (values 0,1,2,3)\n\nTarget is the column we want to predict.","240b6c68":"Models are defined and stored in a list.","f1683403":"Mean cross validated accuracies are shown below.","1802cd3b":"## Standardization\n\nStandardScaler is only fit to training data to prevent data leakage.","bf8c3034":"## Ensembles\n\n* [**Bagging Classifier**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html) The base estimators are fit to random subsets of training data. Base estimator is selectable and default is decision tree. During subset creation, samples may be drawn with or without replacement depending on the value of bootstrap parameter. The decisions of the base estimators are aggregated either with hard or soft voting. If base estimators don't have predict_proba method, default aggregation scheme is hard voting. Bagging is generally used to reduce variance. In scikit-learn implementation BaggingClassifier, it is also possible to subsample features for each base estimator. The name bagging comes from Bootstrap Aggregating.\n<br><br>\n* [**Random Forest Classifier**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) An implementation of bagging classifier where base estimator is fixed as decision tree.\n<br><br>\n* [**AdaBoost Classifier**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html) In bagging, the base estimators are trained and used in parallel independent of each other. In boosting, the base estimators are trained and used sequentially one after another. AdaBoost starts with a base estimator. Weights of the samples are adjusted for the following base estimator such that samples incorrectly classified by previous classifier have higher weights. In this way, the importance of incorrectly classified samples increases for the following base estimators. Base estimator is selectable and the classifier to be used as base estimator should have sample weighting ability for the obvious reason. The name AdaBoost comes from adaptive sample weights.\n<br><br>\n* [**Gradient Tree Boosting Classifier**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html) Base estimators are trained and used sequentially. This time subsequent classifiers fit on the negative gradient of loss function of the previous ones. The base estimator is fixed as regression tree.\n<br><br>\n* [**Voting Classifier**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html) A number of machine learning models are aggregated either by hard or soft voting. One of the differences with bagging is that in bagging, same base estimators are used wherease in voting classifier, same or different type of models can be used. If same base estimator is used, then ensemble is homogeneous, if different base estimators are used then ensemble is heterogeneous.\n<br><br>\n* [**Stacking Classifier**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingClassifier.html) Multilayer structure with base estimators in the first layer and a final (meta) estimator in the second layer. Base layer can be homogeneous or heterogeneous. Unlike voting classifier, the contribution of each base estimator to final decision is assessed by the meta estimator. ","8f0e510f":"There aren't any serious correlation issues between features.","1182125b":"## Training\n\nCross validated accuracies are computed on training set.","b0842690":"## Correlation Analysis","3f8ccdaa":"## Testing\n\nTrained models make predictions on test set and results are evaluated.","f4963a4a":"Target takes 2 values. The predictive model will be a binary classifier. Class weights are close, dataset is balanced."}}