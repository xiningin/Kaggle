{"cell_type":{"14e7691d":"code","247266de":"code","7b4bff56":"code","c0c7a5fd":"code","e0b70ef3":"code","30be527d":"code","c8b49d96":"code","be608e3b":"code","cebc696d":"code","f29a10a2":"code","54a277ad":"code","91d754b4":"code","b89915a9":"code","f97559b4":"code","40bc2396":"code","73d44bd5":"code","3525031a":"code","9bc35850":"code","b030fb64":"code","3e74da82":"code","1c41b2ee":"code","36a0f571":"code","19650203":"code","8db1d664":"code","393e29cf":"code","1d22eaa9":"code","c2df0335":"markdown","0c14a4a9":"markdown","0e7ee4d9":"markdown","18ace4b2":"markdown","5b3c6414":"markdown","c1ab93a8":"markdown","4500f95b":"markdown","7dd969e4":"markdown","471c3bf0":"markdown","5c605a02":"markdown","6753d7c5":"markdown","dd08c927":"markdown","59520eb5":"markdown","acda851c":"markdown","9671ab7b":"markdown","0efe6fba":"markdown","17db927c":"markdown","8c107f96":"markdown","bcf264d4":"markdown","4b957e5f":"markdown","5a8437d5":"markdown","838f721a":"markdown","80e21591":"markdown","8a3b6d5e":"markdown"},"source":{"14e7691d":"## Importing necessary modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","247266de":"df = pd.read_csv(\"..\/input\/nndb_flat.csv\")","7b4bff56":"df.head()","c0c7a5fd":"df.columns","e0b70ef3":"df.shape","30be527d":"correlation = df.corr()\ncorrelation","c8b49d96":"df.head()","be608e3b":"### Drop \ndf_drop = df.drop(labels=['ID','FoodGroup','ShortDescrip','Descrip','CommonName','MfgName','ScientificName'],axis=1)","cebc696d":"df_drop.head()","f29a10a2":"df_drop.shape","54a277ad":"X = df_drop.iloc[:,1:37].values\ny = df_drop.iloc[:,0].values\nX","91d754b4":"y","b89915a9":"np.shape(X)","f97559b4":"from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)","40bc2396":"## Covariance Matrix\nmean_vec = np.mean(X_std, axis=0)\ncov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) \/ (X_std.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat)","73d44bd5":"print('NumPy covariance matrix: \\n%s' %np.cov(X_std.T))","3525031a":"eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","9bc35850":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","b030fb64":"tot = sum(eig_vals)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]","3e74da82":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(36), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()\n","1c41b2ee":"matrix_w = np.hstack((eig_pairs[0][1].reshape(36,1), \n                      eig_pairs[1][1].reshape(36,1)\n                    ))\nprint('Matrix W:\\n', matrix_w)","36a0f571":"Y = X_std.dot(matrix_w)\nY","19650203":"## PCA IN SCIKIT LEARN\nfrom sklearn.decomposition import PCA\npca = PCA().fit(X_std)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,36,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","8db1d664":"from sklearn.decomposition import PCA \nsklearn_pca = PCA(n_components=20)\nY_sklearn = sklearn_pca.fit_transform(X_std)","393e29cf":"print(Y_sklearn)","1d22eaa9":"Y_sklearn.shape","c2df0335":"The plot above clearly shows that maximum variance (somewhere around 26%) can be explained by the first principal component alone. The second,third,fourth and fifth principal component share 6-10% variance.6th and 7th components onwards share less amount of information as compared to the rest of the Principal components.[](http:\/\/)","0c14a4a9":"### Dropping columns which are not needed","0e7ee4d9":"### Printing the head of dataset","18ace4b2":"In correlation features which have higher correlation have a positive value greater than 0 and those with negative values less than 0 show lesser correlation.\n\nThe features having bigger correlation values shows bigger relation between them.","5b3c6414":"### Selecting Principal Components\nIn order to decide which eigenvector(s) can dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped.","c1ab93a8":"### Finding the relationship between features using correlation","4500f95b":"### Columns or Features","7dd969e4":"### Size of dataset","471c3bf0":"### Proportion Variance-\nOn of the method is proportion variance.After sorting the eigenpairs, the next question is \"how many principal components are we going to choose for our new feature subspace?\" A useful measure is called \"proportion variance,\" which can be calculated from the eigenvalues. The proportion variance tells us how much information (variance) can be attributed to each of the principal components.","5c605a02":"### Implementing PCA with 20 components","6753d7c5":"### Seperating features of our dataframe from labels.","dd08c927":"### Principal Component Analysis using SCIKIT-Learn","59520eb5":"### Projection Matrix-\n Suppose only 1st and 2nd principal component shares the maximum amount of information say around 90%.Hence we can drop other components. Here, we are reducing the 36-dimensional feature space to a 2-dimensional feature subspace, by choosing the \u201ctop 2\u201d eigenvectors with the highest eigenvalues to construct our d\u00d7k-dimensional eigenvector matrix W","acda851c":"### Conclusion\nThus PCA can be understood as a feature extraction technique to remove redundant or unnecessary features without losing much information. These features are low dimesional in nature.The first component has the highest variance followed by second,third and so on.\nPCA works best on data set having 3 or more dimension.\n","9671ab7b":"## Applying PCA","0efe6fba":"### Thus X is an matrix of rows 8618 and 36 columns.","17db927c":"### Importing necessary Modules","8c107f96":"From above we can see that dataset contains of 8618 rows and 45 columns.","bcf264d4":"## Computing Covariance Matrix ","4b957e5f":"### DATA STANDARDIZARION\nTo standrize each colomn of X to make sure each column has a mean 0 and standard deviation of 1.","5a8437d5":"### Input the data which is taken from\n https:\/\/data.world\/craigkelly\/usda-national-nutrient-db\n### About data set-\n It is a USDA nutritional data.\n Each record is for 100 grams.\n The columns are mostly self-explanatory. The nutrient columns end with the units, so:\n Nutrient_g is in grams\n Nutrient_mg is in milligrams\n Nutrient_mcg is in micrograms\n Nutrient_USRDA is in percentage of US Recommended Daily Allows (e.g. 0.50 is 50%)","838f721a":"### EIGEN DECOMPOSITION\nFinding Eigen Values and Eigen Vectors","80e21591":"From above we can see that 90% of information(variance) is covered uptill 20th components,so we can discard further components.","8a3b6d5e":"Projection Onto the New Feature Space In this last step we will use the 36\u00d72-dimensional projection matrix W to transform our samples onto the new subspace via the equation Y=X\u00d7W."}}