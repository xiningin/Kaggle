{"cell_type":{"a1881b36":"code","da371e92":"code","c957bbc1":"code","e81bb3ca":"code","436d43b5":"code","14a5cb5c":"code","26e48e5d":"code","25e39e37":"code","a26aaa74":"code","2f04db5c":"code","8039b345":"code","ce3f7994":"code","8d0a328a":"markdown"},"source":{"a1881b36":"# !pip install lightgbm==2.3.1\n# import lightgbm\n# lightgbm.__version__","da371e92":"# load libraries\nimport gc\nimport re\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport warnings\n\nwarnings.filterwarnings('ignore')","c957bbc1":"# run functions and pre_settings\ndef one_hot_encoder(df, nan_as_category=True):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n\ndef group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n                               for e in agg_df.columns.tolist()])\n    return agg_df.reset_index()\n\ndef group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)\n\ndef do_sum(dataframe, group_cols, counted, agg_name):\n    gp = dataframe[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(columns={counted: agg_name})\n    dataframe = dataframe.merge(gp, on=group_cols, how='left')\n    return dataframe\n\ndef reduce_mem_usage(dataframe):\n    m_start = dataframe.memory_usage().sum() \/ 1024 ** 2\n    for col in dataframe.columns:\n        col_type = dataframe[col].dtype\n        if col_type != object:\n            c_min = dataframe[col].min()\n            c_max = dataframe[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    dataframe[col] = dataframe[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    dataframe[col] = dataframe[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    dataframe[col] = dataframe[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    dataframe[col] = dataframe[col].astype(np.int64)\n            elif str(col_type)[:5] == 'float':\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    dataframe[col] = dataframe[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    dataframe[col] = dataframe[col].astype(np.float32)\n                else:\n                    dataframe[col] = dataframe[col].astype(np.float64)\n\n    m_end = dataframe.memory_usage().sum() \/ 1024 ** 2\n    return dataframe\n\nnan_as_category = True\n\n\ndef risk_groupanizer(dataframe, column_names, target_val=1, upper_limit_ratio=8.2, lower_limit_ratio=8.2):\n    # one-hot encoder killer :-)\n    all_cols = dataframe.columns\n    for col in column_names:\n\n        temp_df = dataframe.groupby([col] + ['TARGET'])[['SK_ID_CURR']].count().reset_index()\n        temp_df['ratio%'] = round(temp_df['SK_ID_CURR']*100\/temp_df.groupby([col])['SK_ID_CURR'].transform('sum'), 1)\n        col_groups_high_risk = temp_df[(temp_df['TARGET'] == target_val) &\n                                       (temp_df['ratio%'] >= upper_limit_ratio)][col].tolist()\n        col_groups_low_risk = temp_df[(temp_df['TARGET'] == target_val) &\n                                      (lower_limit_ratio >= temp_df['ratio%'])][col].tolist()\n        if upper_limit_ratio != lower_limit_ratio:\n            col_groups_medium_risk = temp_df[(temp_df['TARGET'] == target_val) &\n                (upper_limit_ratio > temp_df['ratio%']) & (temp_df['ratio%'] > lower_limit_ratio)][col].tolist()\n\n            for risk, col_groups in zip(['_high_risk', '_medium_risk', '_low_risk'],\n                                        [col_groups_high_risk, col_groups_medium_risk, col_groups_low_risk]):\n                dataframe[col + risk] = [1 if val in col_groups else 0 for val in dataframe[col].values]\n        else:\n            for risk, col_groups in zip(['_high_risk', '_low_risk'], [col_groups_high_risk, col_groups_low_risk]):\n                dataframe[col + risk] = [1 if val in col_groups else 0 for val in dataframe[col].values]\n        if dataframe[col].dtype == 'O' or dataframe[col].dtype == 'object':\n            dataframe.drop(col, axis=1, inplace=True)\n    return dataframe, list(set(dataframe.columns).difference(set(all_cols)))\n\n\ndef ligthgbm_feature_selection(dataframe, index_cols, auc_limit=0.7):\n    dataframe = dataframe.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x))\n    clf = LGBMClassifier(random_state=0)\n    train_df = dataframe[dataframe['TARGET'].notnull()]\n    train_df_X = train_df.drop('TARGET', axis=1)\n    train_df_y = train_df['TARGET']\n    train_columns = [col for col in train_df_X.columns if col not in index_cols]\n\n    max_auc_score = 1\n    best_cols = []\n    while max_auc_score > auc_limit:\n        train_columns = [col for col in train_columns if col not in best_cols]\n        clf.fit(train_df_X[train_columns], train_df_y)\n        feats_imp = pd.Series(clf.feature_importances_, index=train_columns)\n        max_auc_score = roc_auc_score(train_df_y, clf.predict_proba(train_df_X[train_columns])[:, 1])\n        best_cols = feats_imp[feats_imp > 0].index.tolist()\n\n    dataframe.drop(train_columns, axis=1, inplace=True)\n    return dataframe","e81bb3ca":"def application():\n    df = pd.read_csv(r'..\/input\/home-credit-default-risk\/application_train.csv')\n    test_df = pd.read_csv(r'..\/input\/home-credit-default-risk\/application_test.csv')\n    df = df.append(test_df).reset_index()\n\n    # general cleaning procedures\n    df = df[df['CODE_GENDER'] != 'XNA']\n    df = df[df['AMT_INCOME_TOTAL'] < 20000000] # remove a outlier 117M\n    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True) # set null value\n    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True) # set null value\n\n    # Categorical features with Binary encode (0 or 1; two categories)\n    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    \n    # Categorical features with One-Hot encode\n    df, cat_cols = one_hot_encoder(df, nan_as_category)\n\n    # Flag_document features - count and kurtosis\n    docs = [f for f in df.columns if 'FLAG_DOC' in f]\n    df['DOCUMENT_COUNT'] = df[docs].sum(axis=1)\n    df['NEW_DOC_KURT'] = df[docs].kurtosis(axis=1)\n\n    def get_age_label(days_birth):\n        \"\"\" Return the age group label (int). \"\"\"\n        age_years = -days_birth \/ 365\n        if age_years < 27: return 1\n        elif age_years < 40: return 2\n        elif age_years < 50: return 3\n        elif age_years < 65: return 4\n        elif age_years < 99: return 5\n        else: return 0\n    # Categorical age - based on target=1 plot\n    df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n\n    # New features based on External sources\n    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    df['EXT_SOURCES_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n    np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n        df[feature_name] = eval('np.{}'.format(function_name))(\n            df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n\n    # Some simple new features (percentages)\n    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\n    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] \/ df['AMT_CREDIT']\n    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] \/ df['CNT_FAM_MEMBERS']\n    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] \/ df['AMT_INCOME_TOTAL']\n    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] \/ df['AMT_CREDIT']\n\n    # Credit ratios\n    df['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] \/ df['AMT_GOODS_PRICE']\n    \n    # Income ratios\n    df['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['DAYS_EMPLOYED']\n    df['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['DAYS_BIRTH']\n    \n    # Time ratios\n    df['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] \/ df['DAYS_BIRTH']\n    df['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] \/ df['DAYS_BIRTH']\n    df['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] \/ df['DAYS_EMPLOYED']\n    df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] \/ df['DAYS_BIRTH']\n\n    # EXT_SOURCE_X FEATURE\n    df['APPS_EXT_SOURCE_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n    df['APPS_EXT_SOURCE_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n    df['APPS_EXT_SOURCE_STD'] = df['APPS_EXT_SOURCE_STD'].fillna(df['APPS_EXT_SOURCE_STD'].mean())\n    df['APP_SCORE1_TO_BIRTH_RATIO'] = df['EXT_SOURCE_1'] \/ (df['DAYS_BIRTH'] \/ 365.25)\n    df['APP_SCORE2_TO_BIRTH_RATIO'] = df['EXT_SOURCE_2'] \/ (df['DAYS_BIRTH'] \/ 365.25)\n    df['APP_SCORE3_TO_BIRTH_RATIO'] = df['EXT_SOURCE_3'] \/ (df['DAYS_BIRTH'] \/ 365.25)\n    df['APP_SCORE1_TO_EMPLOY_RATIO'] = df['EXT_SOURCE_1'] \/ (df['DAYS_EMPLOYED'] \/ 365.25)\n    df['APP_EXT_SOURCE_2*EXT_SOURCE_3*DAYS_BIRTH'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['DAYS_BIRTH']\n    df['APP_SCORE1_TO_FAM_CNT_RATIO'] = df['EXT_SOURCE_1'] \/ df['CNT_FAM_MEMBERS']\n    df['APP_SCORE1_TO_GOODS_RATIO'] = df['EXT_SOURCE_1'] \/ df['AMT_GOODS_PRICE']\n    df['APP_SCORE1_TO_CREDIT_RATIO'] = df['EXT_SOURCE_1'] \/ df['AMT_CREDIT']\n    df['APP_SCORE1_TO_SCORE2_RATIO'] = df['EXT_SOURCE_1'] \/ df['EXT_SOURCE_2']\n    df['APP_SCORE1_TO_SCORE3_RATIO'] = df['EXT_SOURCE_1'] \/ df['EXT_SOURCE_3']\n    df['APP_SCORE2_TO_CREDIT_RATIO'] = df['EXT_SOURCE_2'] \/ df['AMT_CREDIT']\n    df['APP_SCORE2_TO_REGION_RATING_RATIO'] = df['EXT_SOURCE_2'] \/ df['REGION_RATING_CLIENT']\n    df['APP_SCORE2_TO_CITY_RATING_RATIO'] = df['EXT_SOURCE_2'] \/ df['REGION_RATING_CLIENT_W_CITY']\n    df['APP_SCORE2_TO_POP_RATIO'] = df['EXT_SOURCE_2'] \/ df['REGION_POPULATION_RELATIVE']\n    df['APP_SCORE2_TO_PHONE_CHANGE_RATIO'] = df['EXT_SOURCE_2'] \/ df['DAYS_LAST_PHONE_CHANGE']\n    df['APP_EXT_SOURCE_1*EXT_SOURCE_2'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2']\n    df['APP_EXT_SOURCE_1*EXT_SOURCE_3'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_3']\n    df['APP_EXT_SOURCE_2*EXT_SOURCE_3'] = df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    df['APP_EXT_SOURCE_1*DAYS_EMPLOYED'] = df['EXT_SOURCE_1'] * df['DAYS_EMPLOYED']\n    df['APP_EXT_SOURCE_2*DAYS_EMPLOYED'] = df['EXT_SOURCE_2'] * df['DAYS_EMPLOYED']\n    df['APP_EXT_SOURCE_3*DAYS_EMPLOYED'] = df['EXT_SOURCE_3'] * df['DAYS_EMPLOYED']\n\n    # AMT_INCOME_TOTAL : income\n    # CNT_FAM_MEMBERS  : the number of family members\n    df['APPS_GOODS_INCOME_RATIO'] = df['AMT_GOODS_PRICE'] \/ df['AMT_INCOME_TOTAL']\n    df['APPS_CNT_FAM_INCOME_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['CNT_FAM_MEMBERS']\n    \n    # DAYS_BIRTH : Client's age in days at the time of application\n    # DAYS_EMPLOYED : How many days before the application the person started current employment\n    df['APPS_INCOME_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['DAYS_EMPLOYED']\n\n    # other feature from better than 0.8\n    df['CREDIT_TO_GOODS_RATIO_2'] = df['AMT_CREDIT'] \/ df['AMT_GOODS_PRICE']\n    df['APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio'] = df['AMT_INCOME_TOTAL'] \/ 12. - df['AMT_ANNUITY']\n    df['APP_INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['DAYS_EMPLOYED']\n    df['APP_DAYS_LAST_PHONE_CHANGE_DAYS_EMPLOYED_ratio'] = df['DAYS_LAST_PHONE_CHANGE'] \/ df['DAYS_EMPLOYED']\n    df['APP_DAYS_EMPLOYED_DAYS_BIRTH_diff'] = df['DAYS_EMPLOYED'] - df['DAYS_BIRTH']\n\n    print('\"Application_Train_Test\" final shape:', df.shape)\n    return df","436d43b5":"def bureau_bb():\n    bureau = pd.read_csv(r'..\/input\/home-credit-default-risk\/bureau.csv')\n    bb = pd.read_csv(r'..\/input\/home-credit-default-risk\/bureau_balance.csv')\n\n    # Credit duration and credit\/account end date difference\n    bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n    bureau['ENDDATE_DIF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n    \n    # Credit to debt ratio and difference\n    bureau['DEBT_PERCENTAGE'] = bureau['AMT_CREDIT_SUM'] \/ bureau['AMT_CREDIT_SUM_DEBT']\n    bureau['DEBT_CREDIT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n    bureau['CREDIT_TO_ANNUITY_RATIO'] = bureau['AMT_CREDIT_SUM'] \/ bureau['AMT_ANNUITY']\n    bureau['BUREAU_CREDIT_FACT_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_ENDDATE_FACT']\n    bureau['BUREAU_CREDIT_ENDDATE_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n    bureau['BUREAU_CREDIT_DEBT_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] \/ bureau['AMT_CREDIT_SUM']\n\n    # CREDIT_DAY_OVERDUE :\n    bureau['BUREAU_IS_DPD'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n    bureau['BUREAU_IS_DPD_OVER120'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 120 else 0)\n\n    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n\n    # Bureau balance: Perform aggregations and merge with bureau.csv\n    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size', 'mean']}\n    for col in bb_cat:\n        bb_aggregations[col] = ['mean']\n\n    #Status of Credit Bureau loan during the month\n    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n\n    # Bureau and bureau_balance numeric features\n    num_aggregations = {\n        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n        'DAYS_CREDIT_UPDATE': ['mean'],\n        'CREDIT_DAY_OVERDUE': ['max', 'mean', 'min'],\n        'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],\n        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['mean', 'max', 'sum'],\n        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean', 'sum'],\n        'CNT_CREDIT_PROLONG': ['sum'],\n        'MONTHS_BALANCE_MIN': ['min'],\n        'MONTHS_BALANCE_MAX': ['max'],\n        'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n        'SK_ID_BUREAU': ['count'],\n        'DAYS_ENDDATE_FACT': ['min', 'max', 'mean'],\n        'ENDDATE_DIF': ['min', 'max', 'mean'],\n        'BUREAU_CREDIT_FACT_DIFF': ['min', 'max', 'mean'],\n        'BUREAU_CREDIT_ENDDATE_DIFF': ['min', 'max', 'mean'],\n        'BUREAU_CREDIT_DEBT_RATIO': ['min', 'max', 'mean'],\n        'DEBT_CREDIT_DIFF': ['min', 'max', 'mean'],\n        'BUREAU_IS_DPD': ['mean', 'sum'],\n        'BUREAU_IS_DPD_OVER120': ['mean', 'sum']\n        }\n\n    # Bureau and bureau_balance categorical features\n    cat_aggregations = {}\n    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n\n    # Bureau: Active credits - using only numerical aggregations\n    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n\n    # Bureau: Closed credits - using only numerical aggregations\n    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n\n    print('\"Bureau\/Bureau Balance\" final shape:', bureau_agg.shape)\n    return bureau_agg","14a5cb5c":"def previous_application():\n    prev = pd.read_csv(r'..\/input\/home-credit-default-risk\/previous_application.csv')\n\n    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n\n    # Days 365.243 values -> nan\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n\n    # Add feature: value ask \/ value received percentage\n    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] \/ prev['AMT_CREDIT']\n\n    # Feature engineering: ratios and difference\n    prev['APPLICATION_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n    prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_CREDIT'] \/ prev['AMT_ANNUITY']\n    prev['DOWN_PAYMENT_TO_CREDIT'] = prev['AMT_DOWN_PAYMENT'] \/ prev['AMT_CREDIT']\n\n    # Interest ratio on previous application (simplified)\n    total_payment = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n    prev['SIMPLE_INTERESTS'] = (total_payment \/ prev['AMT_CREDIT'] - 1) \/ prev['CNT_PAYMENT']\n\n    # Days last due difference (scheduled x done)\n    prev['DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n\n    # from off\n    prev['PREV_GOODS_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n    prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']\/prev['AMT_APPLICATION']\n    prev['PREV_GOODS_APPL_RATIO'] = prev['AMT_GOODS_PRICE'] \/ prev['AMT_APPLICATION']\n\n    # Previous applications numeric features\n    num_aggregations = {\n        'AMT_ANNUITY': ['min', 'max', 'mean', 'sum'],\n        'AMT_APPLICATION': ['min', 'max', 'mean', 'sum'],\n        'AMT_CREDIT': ['min', 'max', 'mean', 'sum'],\n        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean', 'sum'],\n        'AMT_GOODS_PRICE': ['min', 'max', 'mean', 'sum'],\n        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'DAYS_DECISION': ['min', 'max', 'mean'],\n        'CNT_PAYMENT': ['mean', 'sum'],\n        'SK_ID_PREV': ['nunique'],\n        'DAYS_TERMINATION': ['max'],\n        'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n        'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'sum'],\n        'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n        'PREV_GOODS_DIFF': ['mean', 'max', 'sum'],\n        'PREV_GOODS_APPL_RATIO': ['mean', 'max'],\n        'DAYS_LAST_DUE_DIFF': ['mean', 'max', 'sum'],\n        'SIMPLE_INTERESTS': ['mean', 'max']\n    }\n\n    # Previous applications categorical features\n    cat_aggregations = {}\n    for cat in cat_cols:\n        cat_aggregations[cat] = ['mean']\n\n    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n\n    # Previous Applications: Approved Applications - only numerical features\n    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n\n    # Previous Applications: Refused Applications - only numerical features\n    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n\n    print('\"Previous Applications\" final shape:', prev_agg.shape)\n    return prev_agg","26e48e5d":"def pos_cash():\n    pos = pd.read_csv(r'..\/input\/home-credit-default-risk\/POS_CASH_balance.csv')\n\n    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n\n    # Flag months with late payment\n    pos['LATE_PAYMENT'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n    pos['POS_IS_DPD'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0) # <-- same with ['LATE_PAYMENT']\n    pos['POS_IS_DPD_UNDER_120'] = pos['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n    pos['POS_IS_DPD_OVER_120'] = pos['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n\n    # Features\n    aggregations = {\n        'MONTHS_BALANCE': ['max', 'mean', 'size', 'min'],\n        'SK_DPD': ['max', 'mean', 'sum', 'var', 'min'],\n        'SK_DPD_DEF': ['max', 'mean', 'sum'],\n        'SK_ID_PREV': ['nunique'],\n        'LATE_PAYMENT': ['mean'],\n        'SK_ID_CURR': ['count'],\n        'CNT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n        'CNT_INSTALMENT_FUTURE': ['min', 'max', 'mean', 'sum'],\n        'POS_IS_DPD': ['mean', 'sum'],\n        'POS_IS_DPD_UNDER_120': ['mean', 'sum'],\n        'POS_IS_DPD_OVER_120': ['mean', 'sum'],\n    }\n\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n\n    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n\n    # Count pos cash accounts\n    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n\n\n    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n    gp = sort_pos.groupby('SK_ID_PREV')\n    df_pos = pd.DataFrame()\n    df_pos['SK_ID_CURR'] = gp['SK_ID_CURR'].first()\n    df_pos['MONTHS_BALANCE_MAX'] = gp['MONTHS_BALANCE'].max()\n\n    # Percentage of previous loans completed and completed before initial term\n    df_pos['POS_LOAN_COMPLETED_MEAN'] = gp['NAME_CONTRACT_STATUS_Completed'].mean()\n    df_pos['POS_COMPLETED_BEFORE_MEAN'] = gp['CNT_INSTALMENT'].first() - gp['CNT_INSTALMENT'].last()\n    df_pos['POS_COMPLETED_BEFORE_MEAN'] = df_pos.apply(lambda x: 1 if x['POS_COMPLETED_BEFORE_MEAN'] > 0 \\\n                                                                      and x['POS_LOAN_COMPLETED_MEAN'] > 0 else 0, axis=1)\n    # Number of remaining installments (future installments) and percentage from total\n    df_pos['POS_REMAINING_INSTALMENTS'] = gp['CNT_INSTALMENT_FUTURE'].last()\n    df_pos['POS_REMAINING_INSTALMENTS_RATIO'] = gp['CNT_INSTALMENT_FUTURE'].last()\/gp['CNT_INSTALMENT'].last()\n\n    # Group by SK_ID_CURR and merge\n    df_gp = df_pos.groupby('SK_ID_CURR').sum().reset_index()\n    df_gp.drop(['MONTHS_BALANCE_MAX'], axis=1, inplace= True)\n    pos_agg = pd.merge(pos_agg, df_gp, on= 'SK_ID_CURR', how= 'left')\n\n    # Percentage of late payments for the 3 most recent applications\n    pos = do_sum(pos, ['SK_ID_PREV'], 'LATE_PAYMENT', 'LATE_PAYMENT_SUM')\n\n    # Last month of each application\n    last_month_df = pos.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n\n    # Most recent applications (last 3)\n    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n    gp = sort_pos.iloc[last_month_df].groupby('SK_ID_CURR').tail(3)\n    gp_mean = gp.groupby('SK_ID_CURR').mean().reset_index()\n    pos_agg = pd.merge(pos_agg, gp_mean[['SK_ID_CURR', 'LATE_PAYMENT_SUM']], on='SK_ID_CURR', how='left')\n\n    print('\"Pos-Cash\" balance final shape:', pos_agg.shape) \n    return pos_agg","25e39e37":"def installment():\n    ins = pd.read_csv(r'..\/input\/home-credit-default-risk\/installments_payments.csv')\n\n    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n\n    # Group payments and get Payment difference\n    ins = do_sum(ins, ['SK_ID_PREV', 'NUM_INSTALMENT_NUMBER'], 'AMT_PAYMENT', 'AMT_PAYMENT_GROUPED')\n    ins['PAYMENT_DIFFERENCE'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT_GROUPED']\n    ins['PAYMENT_RATIO'] = ins['AMT_INSTALMENT'] \/ ins['AMT_PAYMENT_GROUPED']\n    ins['PAID_OVER_AMOUNT'] = ins['AMT_PAYMENT'] - ins['AMT_INSTALMENT']\n    ins['PAID_OVER'] = (ins['PAID_OVER_AMOUNT'] > 0).astype(int)\n\n    # Percentage and difference paid in each installment (amount paid and installment value)\n    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] \/ ins['AMT_INSTALMENT']\n    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n\n    # Days past due and days before due (no negative values)\n    ins['DPD_diff'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n    ins['DBD_diff'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n    ins['DPD'] = ins['DPD_diff'].apply(lambda x: x if x > 0 else 0)\n    ins['DBD'] = ins['DBD_diff'].apply(lambda x: x if x > 0 else 0)\n\n    # Flag late payment\n    ins['LATE_PAYMENT'] = ins['DBD'].apply(lambda x: 1 if x > 0 else 0)\n    ins['INSTALMENT_PAYMENT_RATIO'] = ins['AMT_PAYMENT'] \/ ins['AMT_INSTALMENT']\n    ins['LATE_PAYMENT_RATIO'] = ins.apply(lambda x: x['INSTALMENT_PAYMENT_RATIO'] if x['LATE_PAYMENT'] == 1 else 0, axis=1)\n\n    # Flag late payments that have a significant amount\n    ins['SIGNIFICANT_LATE_PAYMENT'] = ins['LATE_PAYMENT_RATIO'].apply(lambda x: 1 if x > 0.05 else 0)\n    \n    # Flag k threshold late payments\n    ins['DPD_7'] = ins['DPD'].apply(lambda x: 1 if x >= 7 else 0)\n    ins['DPD_15'] = ins['DPD'].apply(lambda x: 1 if x >= 15 else 0)\n\n    ins['INS_IS_DPD_UNDER_120'] = ins['DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n    ins['INS_IS_DPD_OVER_120'] = ins['DPD'].apply(lambda x: 1 if (x >= 120) else 0)\n\n    # Features: Perform aggregations\n    aggregations = {\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DPD': ['max', 'mean', 'sum', 'var'],\n        'DBD': ['max', 'mean', 'sum', 'var'],\n        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n        'AMT_INSTALMENT': ['max', 'mean', 'sum', 'min'],\n        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum', 'min'],\n        'SK_ID_PREV': ['size', 'nunique'],\n        'PAYMENT_DIFFERENCE': ['mean'],\n        'PAYMENT_RATIO': ['mean', 'max'],\n        'LATE_PAYMENT': ['mean', 'sum'],\n        'SIGNIFICANT_LATE_PAYMENT': ['mean', 'sum'],\n        'LATE_PAYMENT_RATIO': ['mean'],\n        'DPD_7': ['mean'],\n        'DPD_15': ['mean'],\n        'PAID_OVER': ['mean'],\n        'DPD_diff':['mean', 'min', 'max'],\n        'DBD_diff':['mean', 'min', 'max'],\n        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n        'INS_IS_DPD_OVER_120': ['mean', 'sum']\n    }\n\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n\n    # Count installments accounts\n    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n\n    # from oof (DAYS_ENTRY_PAYMENT)\n    cond_day = ins['DAYS_ENTRY_PAYMENT'] >= -365\n    ins_d365_grp = ins[cond_day].groupby('SK_ID_CURR')\n    ins_d365_agg_dict = {\n        'SK_ID_CURR': ['count'],\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DAYS_ENTRY_PAYMENT': ['mean', 'max', 'sum'],\n        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n        'AMT_INSTALMENT': ['mean', 'max', 'sum'],\n        'AMT_PAYMENT': ['mean', 'max', 'sum'],\n        'PAYMENT_DIFF': ['mean', 'min', 'max', 'sum'],\n        'PAYMENT_PERC': ['mean', 'max'],\n        'DPD_diff': ['mean', 'min', 'max'],\n        'DPD': ['mean', 'sum'],\n        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n        'INS_IS_DPD_OVER_120': ['mean', 'sum']}\n\n    ins_d365_agg = ins_d365_grp.agg(ins_d365_agg_dict)\n    ins_d365_agg.columns = ['INS_D365' + ('_').join(column).upper() for column in ins_d365_agg.columns.ravel()]\n\n    ins_agg = ins_agg.merge(ins_d365_agg, on='SK_ID_CURR', how='left')\n\n    print('\"Installments Payments\" final shape:', ins_agg.shape)\n    return ins_agg","a26aaa74":"def credit_card():    \n    cc = pd.read_csv(r'..\/input\/home-credit-default-risk\/credit_card_balance.csv')\n\n    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n\n    # Amount used from limit\n    cc['LIMIT_USE'] = cc['AMT_BALANCE'] \/ cc['AMT_CREDIT_LIMIT_ACTUAL']\n    # Current payment \/ Min payment\n    cc['PAYMENT_DIV_MIN'] = cc['AMT_PAYMENT_CURRENT'] \/ cc['AMT_INST_MIN_REGULARITY']\n    # Late payment <-- 'CARD_IS_DPD'\n    cc['LATE_PAYMENT'] = cc['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n    # How much drawing of limit\n    cc['DRAWING_LIMIT_RATIO'] = cc['AMT_DRAWINGS_ATM_CURRENT'] \/ cc['AMT_CREDIT_LIMIT_ACTUAL']\n\n    cc['CARD_IS_DPD_UNDER_120'] = cc['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n    cc['CARD_IS_DPD_OVER_120'] = cc['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n\n    # General aggregations\n    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n\n    # Count credit card lines\n    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n\n    # Last month balance of each credit card application\n    last_ids = cc.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n    last_months_df = cc[cc.index.isin(last_ids)]\n    cc_agg = group_and_merge(last_months_df,cc_agg,'CC_LAST_', {'AMT_BALANCE': ['mean', 'max']})\n\n    CREDIT_CARD_TIME_AGG = {\n        'AMT_BALANCE': ['mean', 'max'],\n        'LIMIT_USE': ['max', 'mean'],\n        'AMT_CREDIT_LIMIT_ACTUAL':['max'],\n        'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n        'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n        'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n        'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n        'AMT_PAYMENT_TOTAL_CURRENT': ['max','sum'],\n        'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n        'CNT_DRAWINGS_ATM_CURRENT': ['max','sum', 'mean'],\n        'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n        'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n        'SK_DPD': ['mean', 'max', 'sum'],\n        'LIMIT_USE': ['min', 'max'],\n        'DRAWING_LIMIT_RATIO': ['min', 'max'],\n        'LATE_PAYMENT': ['mean', 'sum'],\n        'CARD_IS_DPD_UNDER_120': ['mean', 'sum'],\n        'CARD_IS_DPD_OVER_120': ['mean', 'sum']\n    }\n\n    for months in [12, 24, 48]:\n        cc_prev_id = cc[cc['MONTHS_BALANCE'] >= -months]['SK_ID_PREV'].unique()\n        cc_recent = cc[cc['SK_ID_PREV'].isin(cc_prev_id)]\n        prefix = 'INS_{}M_'.format(months)\n        cc_agg = group_and_merge(cc_recent, cc_agg, prefix, CREDIT_CARD_TIME_AGG)\n\n\n    print('\"Credit Card Balance\" final shape:', cc_agg.shape)\n    return cc_agg","2f04db5c":"def data_post_processing(dataframe):\n    print(f'---=> the DATA POST-PROCESSING is beginning, the dataset has {dataframe.shape[1]} features')\n    # keep index related columns\n    index_cols = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index']\n\n    dataframe = dataframe.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x))\n    print('names of feature are renamed')\n\n    # Reduced memory usage\n    dataframe = reduce_mem_usage(dataframe)\n    print(f'---=> pandas data types of features in the dataset are converted for a reduced memory usage')\n\n    # Remove non-informative columns\n    noninformative_cols = []\n    for col in dataframe.columns:\n        if len(dataframe[col].value_counts()) < 2:\n            noninformative_cols.append(col)\n\n    dataframe.drop(noninformative_cols, axis=1, inplace=True)\n    print(f'---=> {dataframe.shape[1]} features are remained after removing non-informative features')\n\n    # Removing features not interesting for classifier\n    feature_num = dataframe.shape[1]\n    #this function does not work reason of insufficient memory, I added selected_feature manually!\n    auc_limit = 0.7\n    # dataframe = ligthgbm_feature_selection(dataframe, index_cols, auc_limit=auc_limit)\n    all_features = dataframe.columns.tolist()\n    selected_feature_df = pd.read_csv('..\/input\/homecredit-best-subs\/removed_cols_lgbm.csv')\n    selected_features = selected_feature_df.removed_cols.tolist()\n    remained_features = set(all_features).difference(set(selected_features))\n    dataframe = dataframe[remained_features]\n    print(f'{feature_num - dataframe.shape[1]} features are eliminated by LightGBM classifier with an {auc_limit} auc score limit in step I')\n    print(f'---=> {dataframe.shape[1]} features are remained after removing features not interesting for LightGBM classifier')\n\n\n    # generate new columns with risk_groupanizer\n    start_feats_num = dataframe.shape[1]\n    cat_cols = [col for col in dataframe.columns if 3 < len(dataframe[col].value_counts()) < 20 and col not in index_cols]\n    dataframe, _ = risk_groupanizer(dataframe, column_names=cat_cols, upper_limit_ratio=8.1, lower_limit_ratio=8.1)\n    print(f'---=> {dataframe.shape[1] - start_feats_num} features are generated with the risk_groupanizer')\n\n\n    # ending message of DATA POST-PROCESSING\n    print(f'---=> the DATA POST-PROCESSING is ended!, now the dataset has a total {dataframe.shape[1]} features')\n\n    gc.collect()\n    return dataframe","8039b345":"def Kfold_LightGBM(df):\n    print('===============================================', '\\n', '##### the ML in processing...')\n\n    # loading predicted result \n    df_subx = pd.read_csv(r'..\/input\/homecredit-best-subs\/df_subs_3.csv')\n    df_sub = df_subx[['SK_ID_CURR', '23']]\n    df_sub.columns = ['SK_ID_CURR', 'TARGET']\n\n    # split train, and test datasets\n    train_df = df[df['TARGET'].notnull()]\n    test_df = df[df['TARGET'].isnull()]\n    \n    \n    # delete main dataframe for saving memory\n    del df\n    gc.collect()\n\n        # Expand train dataset with two times of test dataset including predicted results\n    test_df.TARGET = np.where(df_sub.TARGET > 0.75, 1, 0)\n    train_df = pd.concat([train_df, test_df], axis=0)\n    train_df = pd.concat([train_df, test_df], axis=0)\n    train_df = pd.concat([train_df, test_df], axis=0)\n    print(f'Train shape: {train_df.shape}, test shape: {test_df.shape} are loaded.')\n\n    # Cross validation model\n    folds = KFold(n_splits=5, shuffle=True, random_state=2020)\n\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n\n    # limit number of feature to only 174!!!\n    feats = [f for f in train_df.columns if f not in ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV']]\n    \n    # print final shape of dataset to evaluate by LightGBM\n    print(f'only {len(feats)} features from a total {train_df.shape[1]} features are used for ML analysis')\n\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n        clf = LGBMClassifier(nthread=-1,\n                            #device_type='gpu',\n                            n_estimators=5000,\n                            learning_rate=0.01,\n                            max_depth=11,\n                            num_leaves=58,\n                            colsample_bytree=0.613,\n                            subsample=0.708,\n                            max_bin=407,\n                            reg_alpha=3.564,\n                            reg_lambda=4.930,\n                            min_child_weight=6,\n                            min_child_samples=165,\n                            #keep_training_booster=True,\n                            silent=-1,\n                            verbose=-1,)\n\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric='auc', verbose=500, early_stopping_rounds=500)\n\n        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits\n\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        del clf, train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n\n    # create submission file\n    test_df['TARGET'] = sub_preds\n    test_df[['SK_ID_CURR', 'TARGET']].to_csv('submission.csv', index=False)\n    print('a submission file is created')","ce3f7994":"df = application()\ndf = df.merge(bureau_bb(), how='left', on='SK_ID_CURR')\nprint('--=> df after merge with bureau:', df.shape)\ndf = df.merge(previous_application(), how='left', on='SK_ID_CURR')\nprint('--=> df after merge with previous application:', df.shape)\ndf = df.merge(pos_cash(), how='left', on='SK_ID_CURR')\nprint('--=> df after merge with pos cash :', df.shape)\ndf = df.merge(installment(), how='left', on='SK_ID_CURR')\nprint('--=> df after merge with installments:', df.shape)\ndf = df.merge(credit_card(), how='left', on='SK_ID_CURR')\nprint('--=> df after merge with credit card:', df.shape)\ndf = data_post_processing(df)\nprint('='*50, '\\n')\nprint('---=> df final shape:', df.shape, ' <=---', '\\n')\nprint('=' * 50)\nKfold_LightGBM(df)\nprint('--=> all calculations are done!! <=--')","8d0a328a":"## Base Model Study on Home Credit\n\nThe Home Credit Default Risk dataset on the Kaggle is subjected as a final project of my DS\/ML bootcamp, and I have spent a period of three weeks on this project. I developed various models and quite a large number of them having AUC scores better than 0.8 ( highest one +0.804). Unfortunately, I could not run any full version of my models on Kaggle because of insufficient RAM issue even though datasets are zipped to almost 4 times by integer\/float dtype conversion on my datasets. Moreover, I made a bleend boosting study to acheive highest AUC score (0.81128, much highers possible) on Kaggle (https:\/\/www.kaggle.com\/hikmetsezen\/blend-boosting-for-home-credit-default-risk). Furthermore, I developed also a micro model having only 174 features with a better than  0.8 AUC score (https:\/\/www.kaggle.com\/hikmetsezen\/micro-model-174-features-0-8-auc-on-home-credit)\n\n\nMostly I use Colab Pro to compute LigthGBM calculations with 5-fold CV on GPUs. My models have 900-1800 features. \n\nI have a limited knowledge about the credit finance, therefore, I combined many Kaggle notebooks for expending number of features as much as I desire and\/or acceptance of my LigthGBM models harvesting further enhance scores. I would like to thank these contributors. Some of them are listed here:\n* https:\/\/www.kaggle.com\/jsaguiar\/lightgbm-with-simple-features <=-- my models are based on this study\n* https:\/\/www.kaggle.com\/jsaguiar\/lightgbm-7th-place-solution\n* https:\/\/www.kaggle.com\/sangseoseo\/oof-all-home-credit-default-risk <=-- in most cases these hyperparameters are used\n* https:\/\/www.kaggle.com\/ashishpatel26\/different-basic-blends-possible <=-- thank for blending idea\n* https:\/\/www.kaggle.com\/mathchi\/home-credit-risk-with-detailed-feature-engineering\n* https:\/\/www.kaggle.com\/windofdl\/kernelf68f763785\n* https:\/\/www.kaggle.com\/meraxes10\/lgbm-credit-default-prediction\n* https:\/\/www.kaggle.com\/luudactam\/hc-v500\n* https:\/\/www.kaggle.com\/aantonova\/aggregating-all-tables-in-one-dataset\n* https:\/\/www.kaggle.com\/wanakon\/kernel24647bb75c"}}