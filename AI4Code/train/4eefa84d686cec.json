{"cell_type":{"59bde120":"code","ff5ebeca":"code","0d23f28a":"code","738ed003":"code","c8604792":"code","b726291b":"code","88cbd514":"markdown","931a8f3f":"markdown","af0054fe":"markdown","0d295baf":"markdown"},"source":{"59bde120":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import f_oneway\n\nX_train = pd.read_csv('..\/input\/X_train.csv')\nX_test = pd.read_csv('..\/input\/X_test.csv')\ny_train = pd.read_csv('..\/input\/y_train.csv')","ff5ebeca":"X_train.head()","0d23f28a":"# any null values?\nX_train.isnull().sum()","738ed003":"y_train.head()","c8604792":"# what are our surface materials i.e. targets?\nnp.unique(y_train['surface'])","b726291b":"# encode surface targets\nencoder = LabelEncoder()\nsurfaces = np.unique(y_train['surface'])\ny_train['surface'] = encoder.fit_transform(y_train['surface'])\n\n# do we have a strong variation in means across groups?\n# let's find out with pairwise t-Tests for groups with same surface\njoined = X_train.set_index('series_id').join(\n    y_train.set_index('series_id'))\n\ndef anova_across_surface(surface, X):\n    # helper function to calculate anovas for group samples of surface levels\n    records = X[X.loc[:, 'surface']==surface]\n    group_nos = np.unique(records.loc[:, 'group_id'])\n    anovas = []\n    for col in records.columns:\n        samples = [list(X[X.loc[:, 'group_id'] == i][col]) for i in group_nos]\n        aov = f_oneway(*samples)\n        anovas.append(aov[0])\n        anovas.append(aov[1])\n    return anovas\n\n# calculate all the anovas first\nanovas = dict()\nfor i in range(0, 9):\n    # for each surface level\n    anovas[i] = anova_across_surface(i, joined)\n    \n# make nice tables\nno_of_columns = 3\nnew_cols = ['row_id\\t', 'measr_no', 'orien_X', 'orient_Y',\n       'orient_Z', 'orient_W', 'velocity_X',\n       'velocity_Y', 'velocity_Z', 'accel_X',\n       'accel_Y', 'accel_Z', 'group_id',\n       'surface']\njoined.columns = new_cols\nline_1 = '\\n' + '\\t\\t|%s' * no_of_columns\nline_2 = 'surface\\t\\t' + '|F\\t      p-value\\t' * no_of_columns\nline_3 = '%12.12s' + '\\t|%9.3e   %8.3f' * no_of_columns\n\nfor i in range(no_of_columns, len(joined.columns), no_of_columns):\n    print(line_1 % tuple(joined.columns[i-no_of_columns:i]))\n    print(line_2)\n    print('=' * 22 * (no_of_columns + 1))\n    for j, surface in zip(range(0,9), encoder.inverse_transform(list(range(0, 9)))):\n        row = anovas[j][2*(i-no_of_columns):2*i]\n        print(line_3 % tuple([surface] + anovas[j][2*(i-no_of_columns):2*i]))","88cbd514":"Helping Robots with ANOVA\n==========================\n\nI had a first look at the data of this competition today and did some basic exploration. Especially the group_id feature made me curious. I was wondering, if all the samples were recorded under similar conditions and decided to carry out an analysis of variants to compare samples with the same targets taken in different recording sessions. This kernel basically gives you the results, leaving room for some interpretation.\n\n## Import libraries and data","931a8f3f":"## Some data exploration and ANOVA\nLet's have a look at our features and columns. And then carry out the ANOVA across measurement groups (i.e. recording sessions).","af0054fe":"Let's see what we can get out of the above table! For each combination of surface material and feature, we have the F-statistic and p-value for a one-way ANOVA that was calculated across samples taken during different recording sessions. In other words, for each surface material we asked, if the means of the features are the same across different recording sessions. A low p-value close to zero corresponds to the answer 'no' and we can assume, that the means are not very similar. A high p-value gives us a hint at similar means.\n\nAt first we notice, that for the hard_tiles material, ANOVA didn't provide us with results. This is due to the fact, we have only one sample for hard_tiles in our dataset. ANOVA is therefore obsolete in this case.\n\nLooking at the second row in table 1, we can sanity-check our calculations. The p-value for equal means of the measurement_id's is 1. This makes sense, because for each sample the measurement_id's are just integers from 0 to 127 and the means thus equal across all samples.\n\nWe can now have a look at the more interesting parts of the above tables. For many entries, we have very extreme values for F and thus p-values very close to 0 (so close, that for many entries we actually cut off the non-zero decimals while formatting). For these values, we have to reject the hypothesis of equal means.\n\nBut there's also a few tests, that indicate similar means:\n\n1. Samples of velocity_X for carpet, concrete, hard_tiles_large and tiled materials\n   (with p-values of 0.725, 0.953, 0.960 and 0.97 respectively).\n2. Samples of acceleration_Z for carpet, concrete, fine_concrete, hard_tiles_large, tiled and wood \n   (with p-values of 0.519, 1, 0.951, 0.989, 0.999, 0.997 respectively). \n\nInteresting to note are also some of the tests, where our hypothesis of similar means has to be rejected even a very low significance levels. I leave it up to the reader to interpret the above results.","0d295baf":"Our dataset consists of series with a length of 128 measurements and the values from the 10 sensor channels. Every series of measurements has one target, which is a description of the surface material. Luckily, we don't have to deal with any missing values. \n\nIn y_train we're furthermore given a group_id which indicates the recording session this particular series was taken. Across each session, the robot was only driving on one surface. We will make use of that information and see, if there is any variation in means across different recording sessions within the features (because of varying sensor calibrations, environment etc.)."}}