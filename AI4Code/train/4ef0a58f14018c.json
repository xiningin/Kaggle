{"cell_type":{"e9ecf6d8":"code","79f6e3b2":"code","354797b7":"code","7083965a":"code","17980f0b":"code","b6c207e0":"code","5175c452":"code","85e75507":"code","f6ef5701":"code","4e5f197c":"code","9625d89e":"code","1ba463a6":"code","6e8c1b4c":"code","f46e7ca3":"code","4857ba9c":"code","cbbef69f":"code","2c91e14c":"code","d142870f":"code","ace7f89b":"code","5ee9588e":"code","5395eb25":"code","cb8da683":"code","266209d2":"code","20a16cb5":"code","7d761c7a":"code","4f4c7aa0":"code","f84dee12":"markdown","74947302":"markdown","ee30291d":"markdown","7728beb6":"markdown","39167659":"markdown","cf7ad6dd":"markdown","60b31e09":"markdown","b26aaba7":"markdown","404c6121":"markdown","275089a9":"markdown","525ada3d":"markdown","db8cf7df":"markdown","ddc89dc3":"markdown","b2bea937":"markdown","63db333c":"markdown","9daffa8a":"markdown","7780d70d":"markdown","f788cade":"markdown","db7c330d":"markdown"},"source":{"e9ecf6d8":"import numpy as np\nimport pylab as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","79f6e3b2":"train = pd.read_csv(\"..\/input\/datasets-for-churn-telecom\/cell2celltrain.csv\")\ntest = pd.read_csv(\"..\/input\/datasets-for-churn-telecom\/cell2cellholdout.csv\")","354797b7":"train.info()\ntrain[0:10]","7083965a":"#Churn : Yes:1 , No:0\nChurn = {'Yes': 1,'No': 0} \n  \n# traversing through dataframe \n# values where key matches \ntrain.Churn = [Churn[item] for item in train.Churn] \nprint(train)","17980f0b":"print(\"Any missing sample in training set:\",train.isnull().values.any())\nprint(\"Any missing sample in test set:\",test.isnull().values.any(), \"\\n\")","b6c207e0":"# for column\n#train['MonthlyRevenue'].fillna((train['MonthlyRevenue'].median()), inplace=True)\n# for column\ntrain['MonthlyRevenue'] = train['MonthlyRevenue'].replace(np.nan, 0)\n\n# for whole dataframe\ntrain = train.replace(np.nan, 0)\n\n# inplace\ntrain.replace(np.nan, 0, inplace=True)\n\nprint(train)\n\n","5175c452":"# for column\n#train['MonthlyMinutes'].fillna((train['MonthlyMinutes'].median()), inplace=True)\ntrain['MonthlyMinutes'] = train['MonthlyMinutes'].replace(np.nan, 0)\n\n# for whole dataframe\ntrain = train.replace(np.nan, 0)\n\n# inplace\ntrain.replace(np.nan, 0, inplace=True)\n\nprint(train)","85e75507":"# for column\n#train['TotalRecurringCharge'].fillna((train['TotalRecurringCharge'].median()), inplace=True)\ntrain['TotalRecurringCharge'] = train['TotalRecurringCharge'].replace(np.nan, 0)\n\n# for whole dataframe\ntrain = train.replace(np.nan, 0)\n\n# inplace\ntrain.replace(np.nan, 0, inplace=True)\n\nprint(train)","f6ef5701":"# for column\n#train['DirectorAssistedCalls'].fillna((train['DirectorAssistedCalls'].median()), inplace=True)\ntrain['DirectorAssistedCalls'] = train['DirectorAssistedCalls'].replace(np.nan, 0)\n\n# for whole dataframe\ntrain = train.replace(np.nan, 0)\n\n# inplace\ntrain.replace(np.nan, 0, inplace=True)\n\nprint(train)","4e5f197c":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndef FunLabelEncoder(df):\n    for c in df.columns:\n        if df.dtypes[c] == object:\n            le.fit(df[c].astype(str))\n            df[c] = le.transform(df[c].astype(str))\n    return df","9625d89e":"train = FunLabelEncoder(train)\ntrain.info()\ntrain.iloc[235:300,:]","1ba463a6":"test = FunLabelEncoder(test)\ntest.info()\ntest.iloc[235:300,:]","6e8c1b4c":"test = test.drop(columns=['Churn'],\n\n                 axis=1)\ntest = test.dropna(how='any')\nprint(test.shape)","f46e7ca3":"#Frequency distribution of classes\"\ntrain_outcome = pd.crosstab(index=train[\"Churn\"],  # Make a crosstab\n                              columns=\"count\")      # Name the count column\n\ntrain_outcome","4857ba9c":"# Distribution of Churn\ntrain.Churn.value_counts()[0:30].plot(kind='bar')\nplt.show()","cbbef69f":"train = train[['CustomerID','MonthlyRevenue','MonthlyMinutes','TotalRecurringCharge','DirectorAssistedCalls','OverageMinutes',\n         'RoamingCalls','PercChangeMinutes','PercChangeRevenues','DroppedCalls','BlockedCalls','UnansweredCalls','CustomerCareCalls',\n         'ThreewayCalls','ReceivedCalls','OutboundCalls','InboundCalls','PeakCallsInOut','OffPeakCallsInOut','DroppedBlockedCalls','CallForwardingCalls'\n         ,'CallWaitingCalls','MonthsInService','UniqueSubs','ActiveSubs','ServiceArea','Handsets','HandsetModels',              \n'CurrentEquipmentDays','AgeHH1','AgeHH2','ChildrenInHH','HandsetRefurbished','HandsetWebCapable','TruckOwner','RVOwner','Homeownership','BuysViaMailOrder','RespondsToMailOffers','OptOutMailings',          \n'NonUSTravel','OwnsComputer','HasCreditCard','RetentionCalls','RetentionOffersAccepted','NewCellphoneUser',          \n'NotNewCellphoneUser','ReferralsMadeBySubscriber','IncomeGroup','OwnsMotorcycle','AdjustmentsToCreditRating', \n'HandsetPrice','MadeCallToRetentionTeam','CreditRating','PrizmCode','Occupation','MaritalStatus','Churn']] #Subsetting the data\ncor = train.corr() #Calculate the correlation of the above variables\nsns.heatmap(cor, square = True) #Plot the correlation as heat map","2c91e14c":"from sklearn.model_selection import train_test_split\nY = train['Churn']\nX = train.drop(columns=['Churn'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=9)","d142870f":"print('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","ace7f89b":"from sklearn.ensemble import RandomForestClassifier\n\n# We define the model\nrfcla = RandomForestClassifier(n_estimators=100,random_state=9,n_jobs=-1)\n\n# We train model\nrfcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict5 = rfcla.predict(X_test)","5ee9588e":"# The confusion matrix\nrfcla_cm = confusion_matrix(Y_test, Y_predict5)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(rfcla_cm, annot=True, linewidth=0.7, linecolor='black', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Random Forest Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","5395eb25":"# Test score\nscore_rfcla = rfcla.score(X_test, Y_test)\nprint(score_rfcla)","cb8da683":"from sklearn.naive_bayes import GaussianNB\n\n# We define the model\nnbcla = GaussianNB()\n\n# We train model\nnbcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict3 = nbcla.predict(X_test)","266209d2":"# The confusion matrix\nnbcla_cm = confusion_matrix(Y_test, Y_predict3)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(nbcla_cm, annot=True, linewidth=0.7, linecolor='black', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Naive Bayes Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","20a16cb5":"# Test score\nscore_nbcla = nbcla.score(X_test, Y_test)\nprint(score_nbcla)","7d761c7a":"Testscores = pd.Series([score_rfcla,score_nbcla, ], \n                        index=['Random Forest Score','Naive Bayes Score' ]) \nprint(Testscores)","4f4c7aa0":"from sklearn.metrics import roc_curve\n# Random Forest Classification\nY_predict5_proba = rfcla.predict_proba(X_test)\nY_predict5_proba = Y_predict5_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict5_proba)\nplt.subplot(331)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Random Forest')\nplt.grid(True)\nplt.subplots_adjust(top=2, bottom=0.08, left=0.10, right=1.4, hspace=0.45, wspace=0.45)\nplt.show()\n\n# Naive Bayes Classification\nY_predict3_proba = nbcla.predict_proba(X_test)\nY_predict3_proba = Y_predict3_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict3_proba)\nplt.subplot(332)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Naive Bayes')\nplt.grid(True)\nplt.subplots_adjust(top=2, bottom=0.08, left=0.10, right=1.4, hspace=0.45, wspace=0.45)\nplt.show()","f84dee12":"# ROC Curve\nis a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.","74947302":"# Handling missing data","ee30291d":"## 1. Random forest classification\n\nBased on the previous classification method, random forest is a supervised learning algorithm that creates a forest randomly. This forest, is a set of decision trees, most of the times trained with the bagging method. The essential idea of bagging is to average many noisy but approximately impartial models, and therefore reduce the variation. Each tree is constructed using the following algorithm:\n\n* Let $N$ be the number of test cases, $M$ is the number of variables in the classifier.\n* Let $m$ be the number of input variables to be used to determine the decision in a given node; $m<M$.\n* Choose a training set for this tree and use the rest of the test cases to estimate the error.\n* For each node of the tree, randomly choose $m$ variables on which to base the decision. Calculate the best partition of the training set from the $m$ variables.\n\nFor prediction a new case is pushed down the tree. Then it is assigned the label of the terminal node where it ends. This process is iterated by all the trees in the assembly, and the label that gets the most incidents is reported as the prediction. We define the number of trees in the forest in 100. ","7728beb6":"In this study, we tried to predict Customer Churn using Random Forest and Naive Bayesian classifier.\n\nVariable Prediction:\n1.    CustomerID                   \n1.   MonthlyRevenue             \n1.     MonthlyMinutes            \n1.     TotalRecurringCharge      \n1.     DirectorAssistedCalls      \n1.     OverageMinutes             \n1.    RoamingCalls              \n1.    PercChangeMinutes         \n1.     PercChangeRevenues        \n1.    DroppedCalls               \n1.    BlockedCalls               \n1.    UnansweredCalls           \n1.    CustomerCareCalls         \n1.    ThreewayCalls             \n1.    ReceivedCalls              \n1.    OutboundCalls              \n1.    InboundCalls              \n1.    PeakCallsInOut            \n1.   OffPeakCallsInOut          \n1.   DroppedBlockedCalls        \n1.    CallForwardingCalls        \n1.    CallWaitingCalls           \n1.    MonthsInService           \n1.   UniqueSubs               \n1.    ActiveSubs                \n1.   ServiceArea                \n1.   Handsets                  \n1.   HandsetModels              \n1.    CurrentEquipmentDays      \n1.   AgeHH1                     \n1.    AgeHH2                    \n1.    ChildrenInHH              \n1.    HandsetRefurbished         \n1.   HandsetWebCapable          \n1.    TruckOwner                 \n1.   RVOwner                   \n1.    Homeownership            \n1.    BuysViaMailOrder           \n1.    RespondsToMailOffers       \n1.    OptOutMailings            \n1.   NonUSTravel               \n1.    OwnsComputer              \n1.    HasCreditCard             \n1.   RetentionCalls            \n1.    RetentionOffersAccepted    \n1.   NewCellphoneUser         \n1.    NotNewCellphoneUser       \n1.    ReferralsMadeBySubscriber  \n1.    IncomeGroup                \n1.   OwnsMotorcycle           \n1.   AdjustmentsToCreditRating  \n1.    HandsetPrice               \n1.    MadeCallToRetentionTeam    \n1.   CreditRating               \n1.    PrizmCode                 \n1.    Occupation                \n1.   MaritalStatus              ","39167659":"# SPLITING DATA","cf7ad6dd":"As you can see above, we obtain the heatmap of correlation among the variables. The color palette in the side represents the amount of correlation among the variables. The lighter shade represents a high correlation. Here appear important variables (customer churn behavior):\n1. TotalRecurringCharge\n1. RoamingCalls\n1. DroppedCalls\n1. CustomerCareCalls\n1. OutboundCalls\n1. OffPeakCallsInOut\n1. CallWaitingCalls\n1. ActiveSubs\n1. HandsetModels\n1. AgeHH2\n1. HandsetWebCapable\n1. Homeownership\n1. OptOutMailings\n1. HasCreditCard\n1. NewCellphoneUser\n1. IncomeGroup\n1. HandsetPrice\n1. PrizmCode","60b31e09":"# Conclusion","b26aaba7":"import library","404c6121":"# Plotting Heatmap\nHeatmap can be defined as a method of graphically representing numerical data where individual data points contained in the matrix are represented using different colors. \nThe colors in the heatmap can denote the frequency of an event, the performance of various metrics in the data set, and so on. Different color schemes are selected by varying businesses to present the data they want to be plotted on a heatmap [[2](https:\/\/vwo.com\/blog\/heatmap\/)].","275089a9":"Random Forest perform better than Naive Bayes. Random Forest can handle categorical features very well and it can handle high dimensional spaces as well as a large number of training examples. I guess Naive Bayes is not good enough to represent complex behavior.","525ada3d":"## 2. Naive bayes classification\n\nThe naive Bayesian classifier is a probabilistic classifier based on Bayes' theorem with strong independence assumptions between the features. Thus, using Bayes theorem $\\left(P(X|Y)=\\frac{P(Y|X)P(X)}{P(Y)}\\right)$, we can find the probability of $X$ happening, given that $Y$ has occurred. Here, $Y$ is the evidence and $X$ is the hypothesis. The assumption made here is that the presence of one particular feature does not affect the other (the predictors\/features are independent). Hence it is called naive. In this case we will assume that we assume the values are sampled from a Gaussian distribution and therefore we consider a Gaussian Naive Bayes.","db8cf7df":"Some might quibble over our usage of missing. By \u201cmissing\u201d we simply mean NA (\u201cnot available\u201d) or \u201cnot present for whatever reason\u201d. Many data sets simply arrive with missing data, either because it exists and was not collected or it never existed.\n\n","ddc89dc3":"# Churn prediction \nis one of the most popular Big Data use cases in business. It consists of detecting customers who are likely to cancel a subscription to a service.\n\nAlthough originally a telcom giant thing, this concerns businesses of all sizes, including startups. Now, thanks to prediction services and APIs, predictive analytics are no longer exclusive to big players that can afford to hire teams of data scientists.\n\nAs an example of how to use churn prediction to improve your business, let\u2019s consider businesses that sell subscriptions. This can be telecom companies, SaaS companies, and any other company that sells a service for a monthly fee.\n\nThere are three possible strategies those businesses can use to generate more revenue: acquire more customers, upsell existing customers, or increase customer retention. All the efforts made as part of one of the strategies have a cost, and what we\u2019re ultimately interested in is the return on investment: the ratio between the extra revenue that results from these efforts and their cost[[**1**](https:\/\/neilpatel.com\/blog\/improve-by-predicting-churn\/#:~:text=Churn%20prediction%20is%20one%20of,a%20subscription%20to%20a%20service.&text=This%20can%20be%20telecom%20companies,service%20for%20a%20monthly%20fee.)]","b2bea937":"# Test score\n","63db333c":"Data for training and testing\nTo select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 30%, assumed it ideal ratio between training and testing","9daffa8a":"![](https:\/\/miro.medium.com\/max\/844\/1*MyKDLRda6yHGR_8kgVvckg.png)","7780d70d":"# Comparison of classification techniques","f788cade":"read dataset","db7c330d":"Here we handling missing value filled by zero rather than dropping NA values. Another technique of handling missing value in addition to filled by a single number like zero, or it might be some sort of imputation or interpolation from the good values. You could do this in-place using the isnull() method as a mask, but because it is such a common operation Pandas provides the fillna() method, which returns a copy of the array with the null values replaced."}}