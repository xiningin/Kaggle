{"cell_type":{"a48c0408":"code","f83992da":"code","9ebe6810":"code","8a135812":"code","3de12858":"code","14f5649b":"code","f5699bdb":"code","5c7b4800":"code","eba580d5":"code","49407656":"code","ac0fe7df":"code","2abb9901":"code","21c29597":"code","7b3d7b93":"code","cac6bed9":"code","58f5c423":"code","fc3e2ab2":"code","449abf0b":"code","d316f3f8":"code","df90f8cc":"code","0f2189de":"code","2bb7e744":"code","1cd422f8":"code","37a802b0":"code","39e3fd48":"code","8609c945":"code","5050708d":"code","1b48709b":"code","51911352":"code","cd7add01":"code","de56cb71":"code","152c5576":"code","70f6f974":"code","79be1042":"code","66d46b30":"code","6a2f6f47":"code","04164d55":"code","fb692794":"code","5cac49d9":"code","a9696658":"code","5dc4ffd9":"code","09c51896":"code","bfe40e0a":"code","22212576":"code","4992204f":"code","8fc78440":"code","bdbdbcc0":"code","e253b787":"code","ed84eaa3":"code","015cea21":"markdown","54e5c533":"markdown","41aac6c1":"markdown","b5d5ac82":"markdown","4e559b82":"markdown","b831e1fa":"markdown","44ec4671":"markdown","801cef49":"markdown","bd6f1ecc":"markdown","051551e9":"markdown","2a4683ef":"markdown","60a73675":"markdown","c6f529f7":"markdown","670caaa4":"markdown","dd070f79":"markdown","44b5a2d3":"markdown","27abea94":"markdown","7ba708d5":"markdown","dfe50fba":"markdown","9afa174c":"markdown","88437fc4":"markdown","7714e950":"markdown","8c65d7d5":"markdown","630c0fef":"markdown","f5775874":"markdown","8f86b951":"markdown","a6a5d944":"markdown","1cdc25c2":"markdown","ba51598c":"markdown","3fe493dc":"markdown","63844ab2":"markdown","099d37c8":"markdown","cd0357b1":"markdown","8dd03cde":"markdown","5ed2d76e":"markdown","320db481":"markdown","77805e28":"markdown","0a95a0f7":"markdown","ee51904b":"markdown"},"source":{"a48c0408":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n\n\nfrom sklearn.metrics import roc_curve \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, f1_score\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier","f83992da":"df = pd.read_csv(\"..\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv\")","9ebe6810":"df.shape","8a135812":"df.info()","3de12858":"# Check columns list and missing values\ndf.isnull().sum()","14f5649b":"# Get unique count for each variable\ndf.nunique()","f5699bdb":"drop_list = [\n'RowNumber',\n'CustomerId',\n'Surname'\n]\n\n\ndf=df.drop(drop_list,axis=1)","5c7b4800":"df.shape","eba580d5":"df.head()","49407656":"df[\"Exited\"].replace(to_replace=\"C\", value=1, inplace=True)\ndf[\"Exited\"].replace(to_replace=\"N\", value=0, inplace=True)","ac0fe7df":"df.info()","2abb9901":"df[\"Exited\"].value_counts()","21c29597":"labels = 'Churn', 'Not Churn'\nsizes = [df.Exited[df['Exited']==1].count(), df.Exited[df['Exited']==0].count()]\nexplode = (0, 0.1)\nfig1, ax1 = plt.subplots(figsize=(10, 8))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')\nplt.title(\"Distribution of Customers Churn and Not Churn\", size = 8)\nplt.show()","7b3d7b93":"# Correlation Matrix for Numerical Features\nf, ax = plt.subplots(figsize= [10,8])\ng = sns.heatmap(df[[\"CreditScore\",\"Age\",\"Tenure\",\"NumOfProducts\",\"HasCrCard\",\"Balance\", \"EstimatedSalary\", \"Exited\"]].corr(),annot=True, fmt = \".2f\", ax=ax, cmap = \"magma\")\nax.set_title(\"Correlation Matrix\", fontsize=20)\nplt.show()","cac6bed9":"g = sns.FacetGrid(df, col='Exited')\ng = g.map(sns.distplot, \"Age\", color=\"m\")","58f5c423":"g = sns.FacetGrid(df, col='Exited')\ng = g.map(sns.distplot, \"Balance\", color=\"c\")","fc3e2ab2":"g = sns.catplot(x=\"Geography\",y=\"Exited\", data=df, kind=\"bar\", height = 5 ,palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Churn Probability\")","449abf0b":"g = sns.barplot(x=\"Gender\",y=\"Exited\",data=df)\ng = g.set_ylabel(\"Churn Probability\")","d316f3f8":"df.isnull().sum()","df90f8cc":"# Outlier detection \n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, Balance, Duration\nOutliers_to_drop = detect_outliers(df,2,[\"CreditScore\",\"Age\",\"Tenure\", \"Balance\", \"NumOfProducts\", \"EstimatedSalary\"])","0f2189de":"# Drop outliers\ndf = df.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","2bb7e744":"df = pd.get_dummies(df)\n\ndf.head()","1cd422f8":"# Correlation Matrix for Wide Format Data\nf, ax = plt.subplots(figsize= [12,10])\ng = sns.heatmap(df.corr(),annot=True, fmt = \".2f\", ax=ax, cmap = \"magma\")\nax.set_title(\"Correlation Matrix\", fontsize=25)\nplt.show()","37a802b0":"y = df[\"Exited\"]\n\nX = df.drop(labels = [\"Exited\"],axis = 1)","39e3fd48":"# Splitting data into testing and training data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1992)","8609c945":"from imblearn.over_sampling import SMOTE","5050708d":"smote = SMOTE(random_state=42, k_neighbors=5, sampling_strategy=1)\nX_train_smo, y_train_smo = smote.fit_sample(X_train, y_train)","1b48709b":"y_train.value_counts()","51911352":"y_train_smo.value_counts()","cd7add01":"# Cross validate model with Kfold stratified cross validation\nkfold = StratifiedKFold(n_splits=5)","de56cb71":"# Modeling step Test differents algorithms \nrandom_state = 1992\nclassifiers = []\nclassifiers.append(LogisticRegression())\nclassifiers.append(RandomForestClassifier())\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(SVC())\nclassifiers.append(XGBClassifier())\n\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train_smo, y = y_train_smo, scoring = \"f1\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"LogisticRegression\",\"RandomForest\",\"KNeighboors\",\"SVC\",\"XGB\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean F1 Score\")\ng = g.set_title(\"Cross validation scores\")","152c5576":"# LogisticRegression\n\nLR = LogisticRegression()\n\nparam_grid = {\n                  \"C\":np.logspace(-3,3,7), \n                  \"penalty\":[\"l1\",\"l2\"]\n}\n\ngs_LR = GridSearchCV(LR, param_grid = param_grid, cv=kfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\n\ngs_LR.fit(X_train_smo,y_train_smo)\n","70f6f974":"y_pred = gs_LR.predict(X_test)","79be1042":"print(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test,y_pred))\n\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test,y_pred))","66d46b30":"# RandomForest \n\nRFC = RandomForestClassifier()\n\n\nparam_grid = {\n              \"max_depth\": [4,5,7],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[80,100,120],\n              \"criterion\": [\"gini\"]\n}\n\n\ngs_RFC = GridSearchCV(RFC,param_grid = param_grid, cv=kfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\n\ngs_RFC.fit(X_train_smo,y_train_smo)\n","6a2f6f47":"y_pred = gs_RFC.predict(X_test)","04164d55":"print(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test,y_pred))\n\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test,y_pred))","fb692794":"# KNeighbors\n\nKNN = KNeighborsClassifier()\n\nk_grid={\n        'n_neighbors':np.arange(2,20)\n}\n\ngs_KNN=GridSearchCV(KNN, k_grid, cv=kfold, refit=True, n_jobs=4)\n\ngs_KNN.fit(X_train_smo,y_train_smo)","5cac49d9":"y_pred = gs_KNN.predict(X_test)","a9696658":"print(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test,y_pred))\n\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test,y_pred))","5dc4ffd9":"# SVC\nSVM = SVC()\n\n\nparam_grid = {\n                'C': [0.1, 1, 10], \n                'gamma': [0.01, 0.1, 1],\n                'kernel': ['rbf']\n}\n\ngs_SVM = GridSearchCV(SVM, param_grid, cv=kfold, n_jobs=4)\n\ngs_SVM.fit(X_train_smo,y_train_smo)","09c51896":"y_pred = gs_SVM.predict(X_test)","bfe40e0a":"print(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test,y_pred))\n\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test,y_pred))","22212576":"# XGB\n\nXGB = XGBClassifier()\n\nparam_grid = {\n                'n_estimators': [50,80,100,120], \n                'gamma': [0.01,0.001,0.001], \n                'max_depth': [5,6,7,8],\n                'min_child_weight': [1,5,8,10], \n                'learning_rate': [0.05,0.1,0.2]\n}\n\ngs_XGB = GridSearchCV(estimator = XGB, param_grid =  param_grid,  cv=kfold, n_jobs=4)\n\ngs_XGB.fit(X_train_smo,y_train_smo)","4992204f":"y_pred = gs_XGB.predict(X_test)","8fc78440":"print(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test,y_pred))\n\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test,y_pred))","bdbdbcc0":"# Ensemble Modeling\nvoting_clas = VotingClassifier(estimators=[('XGB', gs_XGB.best_estimator_),('extc', gs_LR.best_estimator_),('ada',gs_RFC.best_estimator_)], voting='soft', n_jobs=4)\n\nvoting_clas = voting_clas.fit(X_train_smo, y_train_smo)","e253b787":"y_pred = voting_clas.predict(X_test)","ed84eaa3":"print(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test,y_pred))\n\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test,y_pred))","015cea21":"# EDA","54e5c533":"#### Logistic Regression","41aac6c1":"> Let's convert long format to wide format.","b5d5ac82":"### Oversampling","4e559b82":"Let's look into a voting classifier to combine the predictions coming from the best 3 classifiers","b831e1fa":"# Import Data","44ec4671":"#### SVC","801cef49":"What is Customer Churn in Banking? \n\n\u201cWhen a client ends their relationship with us by switching to another bank.\u201d","bd6f1ecc":"### Gender","051551e9":"#### Balance","2a4683ef":"> There is no missing value in the dataset. ","60a73675":"We applied Tukey Method to remove outliers from dataset. Let's remove them!","c6f529f7":"> It seems that there is no correlation between numerical features.","670caaa4":">  It seems that there is no balance between churn and not churn customers in the dataset. When we are developing a model, we have to take into consideration this situation.","dd070f79":"# Will Do Customers Stop Doing Business With the Bank?","44b5a2d3":"### Numerical Features","27abea94":"It seems that ensemble model gives best scores. ","7ba708d5":"### Cross Validation","dfe50fba":"### Data Definition","9afa174c":"#### XGB","88437fc4":"### Convert Data Format","7714e950":"> The aim is to predict whether the bank's customers leave the bank or not. ","8c65d7d5":"### Geography","630c0fef":"#### Random Forest","f5775874":"#### Age","8f86b951":"> Let's remove the features which has no impact on churn of the customers.","a6a5d944":"### Cathegorical Features","1cdc25c2":"> Let's perform a grid search optimization for each classifier.","ba51598c":"# Introduction","3fe493dc":"### Hyperparameter Tunning","63844ab2":"> This study aims to concisely describe a banking dataset was used to generate meaningful insights by exploring and clustering data through visualization, statistical analysis, and principal component analysis to develop a well-rounded classification model that can make an accurate prediction whether a bank\u2019s customers will churn or not.","099d37c8":"### Outlier Detection","cd0357b1":"#### KNeighboors","8dd03cde":"> In EDA part we noted that data is imbalance. \n\n> Let's apply SMOTE oversampling method to avoid overfitting.","5ed2d76e":"### Missing Values","320db481":"****RowNumber**** \u2014 represent to the row number. This column will be removed.\n\n**CustomerID** \u2014 contains random and unique values. This column will be removed.\n\n**Surname** \u2014 the surname of a customer has no impact on their decision to leave the bank. This column will be removed.\n\n**CreditScore** \u2014 it is a score which shows the customer's credit behaviors. We will keep this column.\n\n**Geography** \u2014 a customer\u2019s location can affect their decision to leave the bank. We will keep this column.\n\n**Gender** \u2014 it could be interesting to explore whether gender plays a role in a customer leaving the bank. \n\n**Age** \u2014 Age of the customer.\n\n**Tenure** \u2014 refers to the number of years that the customer has been a client of the bank. \n\n**Balance** \u2014 Balance of the customers.\n\n**NumOfProducts** \u2014 the number of products that a customer has purchased through the bank.\n\n**HasCrCard** \u2014 indicates whether or not a customer has a credit card. (0=No,1=Yes)\n\n**IsActiveMember** \u2014  indicates whether or not a customer is active or not. (0=No,1=Yes)\n\n**EstimatedSalary** \u2014 customer salary can has impact on their decision to leave the bank that's why we will keep it.\n\n**Exited** \u2014 whether or not the customer left the bank. This is what we have to predict. (0=No,1=Yes)","77805e28":"# Model","0a95a0f7":"![](http:\/\/)![churn%20pic.png](attachment:churn%20pic.png)                    ","ee51904b":"### Scope"}}