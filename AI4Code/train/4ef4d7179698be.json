{"cell_type":{"cec67e5a":"code","21152a5c":"code","5d654261":"code","bf324130":"code","46b0e00e":"code","454210d5":"code","83b4b81b":"code","ec5d536b":"code","8a2f8fc9":"code","39582cec":"code","1d3ffd26":"code","440e382e":"code","5374e8a2":"code","28a6d91f":"code","1abcd6e8":"code","57f1fa6a":"code","20eed817":"code","3e997c6f":"code","11fe66f3":"code","757bcdef":"code","34f28496":"code","bc021f46":"code","a6eba164":"code","98f138a2":"code","2a347a6e":"code","1d022ee2":"code","26483cab":"code","63279e49":"code","9ac6efef":"code","b3bd1b1c":"code","a0fcc0be":"code","5c3291f3":"code","66cd0f53":"code","dbdf58bd":"code","e5799ba7":"code","2a7e9457":"markdown","f89166f8":"markdown","b9d24a11":"markdown","dbee385a":"markdown","6f39ceaf":"markdown","2a7112fb":"markdown","9a963a95":"markdown","25d24183":"markdown","51bed44f":"markdown","a8d4c149":"markdown","2fbf1596":"markdown","591df3fc":"markdown","5af78b82":"markdown","8daee804":"markdown","79a6ed07":"markdown","d4f198b2":"markdown"},"source":{"cec67e5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21152a5c":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport cufflinks as cf\nimport plotly\nimport plotly.express as px\nimport seaborn as sns\n\nfrom IPython.core.display import HTML\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom datetime import datetime\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom pandas import DataFrame\nfrom collections import OrderedDict \nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL","5d654261":"df = pd.read_csv(r'..\/input\/source-based-news-classification\/news_articles.csv', encoding=\"latin\", index_col=0)\ndf = df.dropna()\ndf.count()","bf324130":"df.head(5)","46b0e00e":"df['type'].unique()","454210d5":"cf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","83b4b81b":"df['type'].value_counts().plot.pie(figsize = (8,8), startangle = 75)\nplt.title('Types of articles', fontsize = 20)\nplt.axis('off')\nplt.show()","ec5d536b":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","8a2f8fc9":"common_words = get_top_n_words(df['text_without_stopwords'], 20)\ndf2 = DataFrame (common_words,columns=['word','count'])\ndf2.groupby('word').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 unigrams used in articles',color='blue')","39582cec":"common_words = get_top_n_bigram(df['text_without_stopwords'], 20)\ndf3 = pd.DataFrame(common_words, columns = ['words' ,'count'])\ndf3.groupby('words').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams used in articles', color='blue')","1d3ffd26":"wc = WordCloud(background_color=\"black\", max_words=100,\n               max_font_size=256,\n               random_state=42, width=1000, height=1000)\nwc.generate(' '.join(df['text_without_stopwords']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","440e382e":"fig = px.bar(df, x='hasImage', y='label',title='Articles including images vs Label')\nfig.show()","5374e8a2":"def convert(path):\n    return '<img src=\"'+ path + '\" width=\"80\">'","28a6d91f":"df_sources = df[['site_url','label','main_img_url']]\ndf_r = df_sources.loc[df['label']== 'Real'].iloc[6:10,:]\ndf_f = df_sources.loc[df['label']== 'Fake'].head(6)","1abcd6e8":"HTML(df_r.to_html(escape=False,formatters=dict(main_img_url=convert)))","57f1fa6a":"HTML(df_f.to_html(escape=False,formatters=dict(main_img_url=convert)))","20eed817":"df['site_url'].unique()","3e997c6f":"type_label = {'Real': 0, 'Fake': 1}\ndf_sources.label = [type_label[item] for item in df_sources.label] ","11fe66f3":"val_real=[]\nval_fake=[]\n\nfor i,row in df_sources.iterrows():\n    val = row['site_url']\n    if row['label'] == 0:\n        val_real.append(val)\n    elif row['label']== 1:\n        val_fake.append(val)","757bcdef":"uniqueValues_real = list(OrderedDict.fromkeys(val_real)) \n\nprint(f\"{y_}Websites publishing real news:{g_}{uniqueValues_real}\\n\") ","34f28496":"uniqueValues_fake = list(OrderedDict.fromkeys(val_fake)) \nprint(f\"{y_}Websites publishing fake news:{r_}{uniqueValues_fake}\\n\")","bc021f46":"real_set = set(uniqueValues_real) \nfake_set = set(uniqueValues_fake) \n\nprint(f\"{y_}Websites publishing both real and fake news:{m_}{real_set & fake_set}\\n\")","a6eba164":"type1 = {'bias': 0, 'conspiracy': 1,'fake': 2,'bs': 3,'satire': 4, 'hate': 5,'junksci': 6, 'state': 7}\ndf.type = [type1[item] for item in df.type] ","98f138a2":"def plot_bar(df, feat_x, feat_y, normalize=True):\n    \"\"\" Plot with vertical bars of the requested dataframe and features\"\"\"\n    \n    ct = pd.crosstab(df[feat_x], df[feat_y])\n    if normalize == True:\n        ct = ct.div(ct.sum(axis=1), axis=0)\n    return ct.plot(kind='bar', stacked=True)","2a347a6e":"plot_bar(df,'type' , 'label')\nplt.show()","1d022ee2":"fig = px.sunburst(df, path=['label', 'type'])\nfig.show()","26483cab":"df_type = df[['site_url','type']]\n\nval_bias=[]\nval_conspiracy=[]\nval_fake1=[]\nval_bs=[]\nval_satire=[]\nval_hate=[]\nval_junksci=[]\nval_state=[]\n{'bias': 0, 'conspiracy': 1,'fake': 2,'bs': 3,'satire': 4, 'hate': 5,'junksci': 6, 'state': 7}\nfor i,row in df_type.iterrows():\n    val = row['site_url']\n    if row['type'] == 0:\n        val_bias.append(val)\n    elif row['type']== 1:\n        val_conspiracy.append(val)\n    elif row['type']== 2:\n        val_fake1.append(val)\n    elif row['type']== 3:\n        val_bs.append(val)\n    elif row['type']== 4:\n        val_satire.append(val)\n    elif row['type']== 5:\n        val_hate.append(val)\n    elif row['type']== 6:\n        val_junksci.append(val)\n    elif row['type']== 7:\n        val_state.append(val)","63279e49":"uv_bias = list(OrderedDict.fromkeys(val_bias)) \nuv_conspiracy = list(OrderedDict.fromkeys(val_conspiracy)) \nuv_fake = list(OrderedDict.fromkeys(val_fake1)) \nuv_bs = list(OrderedDict.fromkeys(val_bs)) \nuv_satire = list(OrderedDict.fromkeys(val_satire)) \nuv_hate = list(OrderedDict.fromkeys(val_hate)) \nuv_junksci = list(OrderedDict.fromkeys(val_junksci)) \nuv_state = list(OrderedDict.fromkeys(val_state)) \n\nprint(f\"{b_}{type1}\\n\")\ni=0\nfor lst in (uv_bias,uv_conspiracy,uv_fake,uv_bs,uv_satire, uv_hate,uv_junksci,uv_state): \n    print(f\"{y_}Source URLs for type:{b_}{i}{r_}{lst}\\n\") \n    i+=1","9ac6efef":"df1 = df.sample(frac=1)\ndf1.head()","b3bd1b1c":"y = df1.type\n\nx = df1.loc[:,['site_url','text_without_stopwords']]\nx['source'] = x[\"site_url\"].astype(str) +\" \"+ x[\"text_without_stopwords\"] \nx = x.drop(['site_url','text_without_stopwords'],axis=1)\nx = x.source","a0fcc0be":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30)\n\ntfidf_vect = TfidfVectorizer(stop_words = 'english')\ntfidf_train = tfidf_vect.fit_transform(x_train)\ntfidf_test = tfidf_vect.transform(x_test)\ntfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vect.get_feature_names())","5c3291f3":"tfidf_vect","66cd0f53":"tfidf_train.shape","dbdf58bd":"Adab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=10),n_estimators=5,random_state=1)\nAdab.fit(tfidf_train, y_train)\ny_pred3 = Adab.predict(tfidf_test)\nABscore = metrics.accuracy_score(y_test,y_pred3)\nprint(\"accuracy: %0.3f\" %ABscore)","e5799ba7":"Rando = RandomForestClassifier(n_estimators=100,random_state=0)\nRando.fit(tfidf_train,y_train)\ny_pred1 = Rando.predict(tfidf_test)\nRFscore = metrics.accuracy_score(y_test,y_pred1)\nprint(\"accuracy:  %0.3f\" %RFscore)","2a7e9457":"# Websites and types of news published","f89166f8":"# Reading the csv file","b9d24a11":"# WordCloud of articles","dbee385a":"# Shuffling values","6f39ceaf":"# Training and Testing","2a7112fb":"# Distrubution of types of articles","9a963a95":"# Label vs Type","25d24183":"![](https:\/\/www.gannett-cdn.com\/-mm-\/0b146ca359d26300e1d6a8b6a2d86ce731de39c2\/c=10-0-630-466\/local\/-\/media\/2016\/12\/10\/INGroup\/Indianapolis\/636169619660823711-121116.jpg?quality=50&width=640)\n\nThe spread of misinformation on social media platforms is an ever-growing problem. Organizations, politicians, individuals looking for personal gain and even certain news media outlets engage in propagating fake news to sway people's decisions as well as distorting events to fit a bias or prejudice. \n\nThe degree of authenticity of the news posted online cannot be definitively measured, since the manual classification of news is tedious and time-consuming and is also subject to bias. \n\nTo tackle the growing problem, detection, classification and mitigation tools are a need of the hour.\n\n# Methodology\nThe categories, bs (i.e. bullshit), junksci(i.e. junk science), hate, fake, conspiracy, bias, satire and state declare the\ncategory under which untrustworthy or false news fall under. \n\nThe first step, which is text preprocessing was performed using the following:\n* Taking care of null\/missing values \n* Transforming categorical data with the help of label encoders \n* Uppercase to lowercase \n* Number removal \n* Tokenization \n* Stop Word Removal, Stemming and Lemmatization (with POS tagging) using the Natural Language Toolkit Library \n\nFor feature engineering, the TF-IDF technique is used. \nThis processed and embedded text is provided as an input to Machine learning models, where the data is made to fit the model, to get a prediction as an output. ","51bed44f":"# Websites publishing both real and fake news","a8d4c149":"# Websites publishing fake news","2fbf1596":"# Unigrams and bigrams ","591df3fc":"> # Websites publishing real news","5af78b82":"# Imports","8daee804":"# Articles including images vs Label","79a6ed07":"Right after preprocessing, the output is a corpus of raw texts that are stripped of stopwords, stemmed and lemmatized. \n\nIn order to get a sparse matrix of TF\/IDF values, the following steps are taken:\n* Tokenization of texts\n* Counting of the tokens and\n* Transforming the raw tokens into TF\/IDF values\n\nThe above steps are done with the help of the TfidfVectorizer, which transforms text to feature vectors that can be used\nas input to estimators\/classifiers.","d4f198b2":"AdaBoost works in iterations with a base classifier to ensure accurate predictions of unusual observations.\n\nIt works in iterations and within each iteration, incorrect observations are given a higher probability for classification for the next iteration. \n\nThe AdaBoost implemented here has a Decision Tree Classifier as the base classifier with a max depth of the tree being 10."}}