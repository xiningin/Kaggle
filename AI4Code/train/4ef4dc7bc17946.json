{"cell_type":{"c1248403":"code","159b1c22":"code","35539450":"code","fe6e44bc":"code","2a156fcb":"code","5fa70547":"code","23c328db":"code","63879972":"code","4bf42423":"code","b0521c0b":"code","b19478a7":"code","f51c8124":"code","ec34842d":"code","b7a949e8":"markdown","61b42431":"markdown","4325e735":"markdown","e47f4db1":"markdown","89f32c2d":"markdown","d79e8d87":"markdown","775b7656":"markdown","b3180573":"markdown","6f9e7dd5":"markdown","5fa28d7b":"markdown","436bcc30":"markdown","612cd4b9":"markdown","a040af9d":"markdown","15ad7ce5":"markdown","c56181e9":"markdown","1a543a33":"markdown","47c4ab9d":"markdown","6f084eca":"markdown"},"source":{"c1248403":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, GridSearchCV\nfrom eli5 import show_weights\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","159b1c22":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ncombined = pd.concat([train, test], sort=True).reset_index(drop=True)","35539450":"print(\"Train #rows: {}\".format(train.shape[0]))\nprint(\"Train #columns: {}\\n\".format(train.shape[1]))\nprint(\"Test #rows: {}\".format(test.shape[0]))\nprint(\"Test #columns: {}\\n\".format(test.shape[1]))\nprint(combined.columns)","fe6e44bc":"print(\"Missing values in train set\")\nfor col in train.columns.tolist():\n    print(\"Column {} missing values: {}\".format(col, train[col].isnull().sum()))\nprint(\"\\nMissing values in test set\")\nfor col in test.columns.tolist():\n    print(\"Column {} missing values: {}\".format(col, test[col].isnull().sum()))","2a156fcb":"train['Age'] = combined.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\ntest['Age'] = combined.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\ntrain['Embarked'] = train['Embarked'].fillna(\"S\")\ntest['Fare'] = test['Fare'].fillna(combined.groupby(['Pclass']).Fare.median()[3])\ntrain['Fare'] = train['Fare'].astype(int)\ntest['Fare'] = test['Fare'].astype(int)","5fa70547":"deckreg = re.compile(r\"^([A-Z])\")\ndef deckletter(cabin):\n    match = deckreg.search(cabin)\n    if match:        \n        return match.group()\n    return cabin\n\ndef deckeng(data):\n    data['Cabin'] = data['Cabin'].fillna(\"M\")\n    data['Deck'] = data['Cabin'].apply(deckletter)\n    data['Deck'] = data['Deck'].replace(['A', 'B', 'C', 'T'], 'ABC')\n    data['Deck'] = data['Deck'].replace(['D', 'E'], 'DE')\n    data['Deck'] = data['Deck'].replace(['F', 'G'], 'FG')\n    return data","23c328db":"titlereg = re.compile(r\"([A-Za-z]+)\\.\")\ndef title(name):\n    match = titlereg.search(name)\n    if match:\n        return match.group()\n    return name\n\ndef titleeng(data):\n    data['Title'] = data['Name'].apply(title)\n    data['Title'] = data['Title'].replace(['Mlle.', 'Ms.'], 'Miss.')\n    data['Title'] = data['Title'].replace(['Mme.', 'Dona.', 'Countess.', 'Lady.'], 'Mrs.')\n    data['Title'] = data['Title'].replace(['Capt.', 'Col.', 'Don.', 'Dr.', 'Major.', 'Jonkheer.', 'Rev.', 'Sir.'], 'OtherM')\n    return data","63879972":"familysizemap = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Large', 6: 'Large', 7: 'Large', 8: 'Large', 11: 'Large' }\n\ndef familyeng(data):\n    data['Familysize'] = combined['SibSp'] + combined['Parch'] + 1\n    data['Familybin'] = data['Familysize'].map(familysizemap)\n    return data","4bf42423":"def ticketeng(data):\n    data['Ticketfreq'] = combined.groupby(['Ticket'])['Ticket'].transform('count')\n    return data","b0521c0b":"def featureeng(data):\n    deckfeat = deckeng(data)\n    titlefeat = titleeng(deckfeat)\n    familyfeat = familyeng(titlefeat)\n    ticketfeat = ticketeng(familyfeat)\n    dropped = ticketfeat.drop(['PassengerId', 'Cabin', 'Name', 'SibSp', 'Parch', 'Ticket', 'Familysize'], axis=1)\n    dummies = pd.get_dummies(dropped)\n    return dummies","b19478a7":"train = featureeng(train)\nX = train.drop('Survived', axis=1)\ny = train['Survived']\n\nmodel = xgb.XGBClassifier(\n    n_estimators=5000, \n    max_depth=2,\n    subsample=0.7,\n    colsample_bytree=1,\n    colsample_bylevel=0.7,\n    min_child_weight=20,\n    learning_rate=0.001,\n    eval_metric='logloss',\n    use_label_encoder=False)\n\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=144)\nscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\nmodel.fit(X, y)\nprint(scores)\nprint(\"Accuracy: {:.3f} stdev: {:.3f}\".format(np.mean(np.abs(scores)), np.std(scores)))","f51c8124":"show_weights(model)","ec34842d":"ids = test['PassengerId']\ntest = featureeng(test)\nX = test\ny = model.predict(X)\nsubmission = pd.DataFrame({\"PassengerID\": ids, \"Survived\": y})\nsubmission.to_csv('submission.csv', index=False)","b7a949e8":"#### 3.3 Making Predictions","61b42431":"#### 2.2 Title\nA new feature `Title` is created by extracting the first few letters of each name representing the title of the passenger. Titles are grouped  into \"Master\", \"Miss\", \"Mr\", \"Mrs\", and \"Other male\", considering similarities in age, gender, and frequency.\n","4325e735":"## **Titanic XGBoost Model with Feature Engineering**\n## 0. Introduction\nThe purpose of this notebook is to outline my attempt at familiarizing myself with XGBoost and applying it to the canonical Titanic survival prediction problem. I hope this can serve as a basis for others that wish to use XGBoost in their attempts at this problem. This is my first published notebook on Kaggle, feedback welcome.","e47f4db1":"#### 0.2 Loading data\n`combined` is a simple concatenation of both sets which is used for more complete data when filling in missing values and for feature engineering.","89f32c2d":"#### 2.1 Cabin\nA new feature called `Deck` is created by using the letters from the `Cabin` column. Passengers with multiple cabins are mostly on the same deck so only the first letter is used. Further, decks are grouped based on proximity as well as similarity in survival rates and class occupancy, as pointed out by Gunes's analysis.\n\nIf a passenger's cabin is missing then they are assigned `M` in an effort to create a useful category based on the fact the cabin info is missing.","d79e8d87":"#### 3.2 Feature Importance\nSummary of feature importances using eli5 library `show_weights`","775b7656":"#### 2.4 Ticket Frequency\nA feature that similarly records the size of a passenger's travelling group, but also includes friends and nannies that weren't counted as family. This feature is not binned so as to avoid too high of a correlation with the family size feature.","b3180573":"There are a significant number of `Age` values missing from both the train and test set. They are imputed by taking the taking the median age grouped by `Sex` and `Pclass`, as suggested by Gunes.\n\nThe 3 missing values of `Embarked` turn out to be S, based on background research.\n\nThere is one missing value of `Fare` in the test set, which will be imputed by taking the median fare of a 3rd class passenger.\n\nFinally, many `Cabin` values are missing. It is believed most such passengers did not survive, thus there is a potentially strong correlation with survival. This is used in feature engineering.","6f9e7dd5":"#### 2.3 Family Size\nAnother feature `Familysize` is created by adding `SibSp` and `Parch` + 1. This gives the total size of the passenger's family, including themselves. The sizes are then binned into \"Alone\", \"Small\", and \"Large\".","5fa28d7b":"#### 0.1 Imports","436bcc30":"#### 1.1 Overview","612cd4b9":"## 1. Exploratory Data Analysis\nAlthough some cursory analysis is performed, most ideas and insights (for dealing with missing values, for feature engineering, etc.) are based off of the wonderful EDA by Gunes Evitan in their notebook [Titanic - Advanced Feature Engineering Tutorial](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial)","a040af9d":"#### 1.2 Missing Values\nThere are a number of missing values from the train and test sets as shown below.","15ad7ce5":"## 2. Feature Engineering\nMost engineering ideas are based off of the insights obtained from the EDA by Gunes Evitan in their notebook [Titanic - Advanced Feature Engineering Tutorial](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial)","c56181e9":"The train set has 891 rows while the test set has 418. They share the same columns except for `Survived`, which is the prediction target. The column descriptions are as follows:\n\n**PassengerId** - ID number for each passenger\n\n**Survived** - Whether or not the passenger survived the incident, 1 if survived, 0 if not\n\n**Pclass** - The class of the passenger where 1 is upper class, 2 is middle class, and 3 is lower class\n\n**Name** - Name of passenger which includes titles such as Mr, Mrs, Master, and so forth\n\n**Sex** - Sex of passenger, either male or female\n\n**Age** - Age of passenger, some ages are decimal\n\n**SibSp** - Number of passenger's siblings + spouses on board\n\n**Parch** - Number of passenger's parents + children on board\n\n**Ticket** - Ticket number of passenger, which may include some prefix that could indicate where the ticket was purchased\n\n**Fare** - Fare of passenger's ticket \n\n**Cabin** - Which cabin the passenger was in, which includes a deck letter and cabin number (i.e D35)\n\n**Embarked** - From where the passenger embarked the titanic. Either C (Cherbourg), Q (Queenstown), or S (Southampton)\n","1a543a33":"## 3. Model and Prediction\n#### 3.1 Model Creation\nSince this is a classification problem, XGBoost's classifier is used. Evaluation is done using k-fold cross-validation, with the scores of each fold shown, as well as the mean scores. Parameters were obtained offline by tuning using gridsearchcv.","47c4ab9d":"Putting it all together completes feature engineering.","6f084eca":"## 4. Conclusion\nI hope this notebook can help others that would like to try XGBoost for the first time and apply it to this problem. Comments welcome.\n#### 4.1 Possible Improvements\/Extensions\nSome things to consider for possible improvements to the model:\n* Creating an ensemble model with models of different algorithms and\/or models with different features\/weights\n* Performing feature selection"}}