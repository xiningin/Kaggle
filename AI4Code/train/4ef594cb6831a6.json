{"cell_type":{"b6747eef":"code","698d87e3":"code","843fde2f":"code","1a679a25":"code","9b6bf6ed":"code","e8ce2f4b":"code","f2c8e849":"code","c58ed9f6":"code","82fb405a":"code","409032ad":"code","53fd9f14":"code","18817af0":"code","2693aa6b":"code","a621077d":"code","8134c38f":"markdown","77287fb4":"markdown","bdc0ea6e":"markdown","d19e0140":"markdown","a4b90aec":"markdown","585977c3":"markdown","083b2da6":"markdown","edf1894f":"markdown","6c60ab4f":"markdown","8a1a9b0b":"markdown","e7ec1b81":"markdown","221c0067":"markdown"},"source":{"b6747eef":"import glob, os \nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\npath=\"..\/input\/pollendataset\/PollenDataset\/images\/\"\nimlist= glob.glob(os.path.join(path, '*.jpg'))","698d87e3":"def dataset(file_list,size=(300,180),flattened=False):\n\tdata = []\n\tfor i, file in enumerate(file_list):\n\t\timage = io.imread(file)\n\t\timage = transform.resize(image, size, mode='constant')\n\t\tif flattened:\n\t\t\timage = image.flatten()\n\n\t\tdata.append(image)\n\n\tlabels = [1 if f.split(\"\/\")[-1][0] == 'P' else 0 for f in file_list]\n\n\treturn np.array(data), np.array(labels)","843fde2f":"# Load the dataset (may take a few seconds)\nX,y=dataset(imlist)","1a679a25":"# X has the following structure: X[imageid, y,x,channel]\nprint('The length of X: ',len(X))  # data\nprint('The shape of X: ',X.shape)  # target\nprint('The shape of Y', y.shape)","9b6bf6ed":"print(X[1,:,: ,0]) #let's look into the pixel data for one of the images","e8ce2f4b":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(\n\tX, y, test_size=0.20)\n\npartial_x_train, validation_x_train, partial_y_train, validation_y_train = train_test_split(\n\tx_train, y_train, test_size=0.15)","f2c8e849":"print('The size of the training set: ',len(x_train))\nprint('The size of the partial training set: ',len(partial_x_train))\nprint('The size of the validation training set: ',len(validation_x_train))\nprint('The size of the testing set: ',len(x_test))","c58ed9f6":"from keras import layers\nfrom keras import models\nfrom keras import optimizers\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(64,(3,3), activation='relu', input_shape=(300,180,3)))  #input shape must be the match the input image tensor shape\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(layers.Conv2D(64,(3,3), activation='relu'))\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(layers.Conv2D(128,(3,3), activation='relu'))\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(layers.Conv2D(128,(3,3), activation='relu'))\nmodel.add(layers.Conv2D(128,(3,3), activation='relu'))\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(layers.Conv2D(256,(3,3), activation='relu'))\nmodel.add(layers.Conv2D(256,(3,3), activation='relu'))\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(512, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])","82fb405a":"history = model.fit(\n    partial_x_train, \n    partial_y_train,\n    validation_data=(validation_x_train, validation_y_train),\n    epochs=100, \n    batch_size=15, \n    verbose =2) #hides some information while training","409032ad":"def smooth_curve(points, factor=0.8): #this function will make our plots more smooth\n\tsmoothed_points = []\n\tfor point in points:\n\t\tif smoothed_points:\n\t\t\tprevious = smoothed_points[-1]\n\t\t\tsmoothed_points.append(previous*factor+point*(1-factor))\n\t\telse:\n\t\t\tsmoothed_points.append(point)\n\treturn smoothed_points","53fd9f14":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n","18817af0":"epochs = range(1, len(acc)+1)\nplt.plot(epochs, smooth_curve(acc), 'bo', label='Training acc')\nplt.plot(epochs, smooth_curve(val_acc), 'r-', label='Training acc')\nplt.legend()\nplt.title('Training and Validation Acc')\nplt.figure()\n\nplt.plot(epochs, smooth_curve(loss), 'bo', label='Training loss')\nplt.plot(epochs, smooth_curve(val_loss), 'r-', label='Training acc')\nplt.legend()\nplt.title('Training and Validation loss')\nplt.show()","2693aa6b":"model1 = models.Sequential()\nmodel1.add(layers.Conv2D(64,(3,3), activation='relu', input_shape=(300,180,3)))  #input shape must be the match the input image tensor shape\nmodel1.add(layers.MaxPooling2D(2,2))\nmodel1.add(layers.Conv2D(64,(3,3), activation='relu'))\nmodel1.add(layers.MaxPooling2D(2,2))\nmodel1.add(layers.Conv2D(128,(3,3), activation='relu'))\nmodel1.add(layers.MaxPooling2D(2,2))\nmodel1.add(layers.Conv2D(128,(3,3), activation='relu'))\nmodel1.add(layers.Conv2D(128,(3,3), activation='relu'))\nmodel1.add(layers.MaxPooling2D(2,2))\nmodel1.add(layers.Conv2D(256,(3,3), activation='relu'))\nmodel1.add(layers.Conv2D(256,(3,3), activation='relu'))\nmodel1.add(layers.MaxPooling2D(2,2))\nmodel1.add(layers.Flatten())\nmodel1.add(layers.Dropout(0.5))\nmodel1.add(layers.Dense(512, activation = 'relu'))\nmodel1.add(layers.Dense(1, activation = 'sigmoid'))\n\nmodel1.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n\nhistory1 = model1.fit(\n    x_train, \n    y_train,\n    epochs=65, \n    batch_size=15, \n    verbose =2)","a621077d":"test_loss, test_acc = model1.evaluate(x_test, y_test, steps=10)\nprint('The final test accuracy: ',test_acc)","8134c38f":"<a name=\"Part 1: Organize the Data\"><\/a>\n## Part 1: Organize the Data    \n1. Import the images into a dataset\n2. Split the dataset into a training, validation, and testing sets","77287fb4":"Based on the plots above, it seems we start to overfit around 60 epochs!  \nWe will use around 60 epochs for our final training model.   \n**Result may vary**","bdc0ea6e":"# **[Beginner] CNN-based Honey Bee Pollen Binary Classifier**\n---\n**By: Peter Sunga** <br\/>\nThis is a simple CNN model used to classify whether or not a bee is carrying pollen.   \nThis model it able to obtain over 90% accuracy with 571 training images on 143 test images.     \n \n*I will try to improve accuracy in the future with image augmentation techniques*\n\n# Table of Contents\n1. [Part 1: Organize the Dat](#Part 1: Organize the Data)\n2. [Part 2: Build the CNN model](#Part 2: Build the CNN model)\n3. [Part 3: Train the CNN mode](#Part 3: Train the CNN model)\n4. [Part 4: Analyze the CNN mode](#Part 4: Analyze the CNN mode)\n5. [Part 5: Retrain the Model with all the Training data](#Part 5: Retrain the Model with all the Training data)\n5. [Part 6: Test the Model](#Part 6: Test the Model)","d19e0140":"Now we are ready to begin the training!\n<a name=\"Part 3: Train the CNN model\"><\/a>\n## Part 3: Train the CNN model","a4b90aec":"<a name=\"Part 4: Analyze the CNN mode\"><\/a>\n## Part 4: Analyze the CNN mode","585977c3":"Now that we have the training, partial training, validation, and testing set. It's time to design the CNN model...\n<a name=\"Part 2: Build the CNN model\"><\/a>\n## Part 2: Build the CNN model","083b2da6":"As a sanity check, let's check the length of each set...","edf1894f":"We do not need to scale the pixel values as they are already between 0-1\n### Now let's create the training, partial training,  valdiation, and testing sets <\/br>\n* We will split the dataset X and label matrix y into three partitions to achieve this\n* train_test_split module function from sklearn can automatically do this for us\n* Training dataset will be used to train the final model, partial training dataset is only used in conjunction with the validation dataset to ensure we do not overfit. (Determine ideal number of epoches)","6c60ab4f":"<a name=\"Part 6: Test the Model\"><\/a>\n## Part 6: Test the Model","8a1a9b0b":"* Load the dataset into the image tensor X\n* y will contain the labels for X","e7ec1b81":"<a name=\"Part 5: Retrain the Model with all the Training data\"><\/a>\n## Part 5: Retrain the Model with all the Training data","221c0067":"As displayed above there are **714**  tensor images in dataset X"}}