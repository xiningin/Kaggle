{"cell_type":{"7c45acf9":"code","19a3dc8b":"code","2f5ae109":"code","3b4b01b2":"code","f276d088":"code","0d504971":"code","d269f6c4":"code","a9d87e7a":"code","fd6004ed":"code","93656369":"code","0e1005ca":"code","0eedd10b":"code","4b02d344":"code","637c422e":"code","e841ea72":"code","62bc975e":"code","dd4cd944":"code","1fabff9a":"code","1b89064e":"code","81bdc8ca":"code","a7b30a55":"code","b8bf471b":"code","f5f4bce2":"code","13fa8a71":"code","db159ebf":"code","5c3c7d04":"code","8a5b423c":"code","e000e2cb":"code","731bee9f":"code","c2decc47":"code","22c57113":"code","dfd58713":"code","80b3c5d8":"code","20db4fdf":"code","408a1e8b":"code","84ec8c73":"code","aa382814":"code","797af9ec":"code","5f7dad07":"code","15aea4b5":"code","a9c96569":"code","0dffcb9b":"code","272f0c6b":"code","968a26ef":"code","33c77d2e":"code","6c1f93bb":"code","22b40a69":"code","ca451655":"markdown","11aa13b8":"markdown","6fb37b45":"markdown"},"source":{"7c45acf9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport os\nimport gc\nimport random\n\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\nfrom xgboost.sklearn import XGBClassifier\nfrom xgboost import plot_importance\n\nfrom bayes_opt import BayesianOptimization\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nfrom scipy.stats import uniform","19a3dc8b":"# Load the dataset\n#df = pd.read_csv('C:\\\\Users\\\\nt65000\\\\Downloads\\\\winequalityN.csv')\nos.chdir(\"..\/input\") \ndf = pd.read_csv(\"winequalityN.csv\")","2f5ae109":"# Show all the records\npd.set_option('display.max_columns', 100)","3b4b01b2":"df.head()","f276d088":"df.tail()","0d504971":"df.info()","d269f6c4":"df.shape","a9d87e7a":"df.describe","fd6004ed":"df['quality'].unique()","93656369":"# Corelation coefficient - To measure the strength of the relationship between two variables\ncorr=df.corr()","0e1005ca":"corr","0eedd10b":"# Checking for null values\ndf.isnull().sum()","4b02d344":"# Deleting the rows with null values\ndf.dropna(axis=0, inplace=True)","637c422e":"# Checking data after dropping null value rows\ndf.shape","e841ea72":"plt.figure(figsize=(14,6))\nsns.heatmap(corr,annot=True)","62bc975e":"# Find percentage of wine types\nplt.figure(figsize=(15,7))\n \n# Data to plot\nlabels = 'white', 'green'\nsizes = [4870,1593]\ncolors = ['green', 'yellow']\nexplode = (0.1, 0 )  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=140)\nplt.title('The percentage of type of wine',fontsize=20)\nplt.legend(('white', 'green'),fontsize=15)\nplt.axis('equal')\nplt.show()","dd4cd944":"data_red = df[df.type == \"red\"]\ndata_red.plot(kind = \"scatter\", x = \"residual sugar\", y = \"alcohol\", alpha = .5, color = \"r\")\nplt.title(\"Alcohol - Residual Sugar Scatter Plot\")\nplt.show()","1fabff9a":"g = sns.pairplot(df,palette=\"hls\",diag_kind=\"kde\",hue='type')","1b89064e":"# Split the data into predictors and target\nX = df.iloc[ :, 1:13]\ny = df.iloc[ : , 0]","81bdc8ca":"#  Transform type data to '1' and '0'\ny = y.map({'white':1, 'red' : 0})\ny.dtype          ","a7b30a55":"colnames = X.columns.tolist()\ncolnames","b8bf471b":"# Split dataset into train and validation parts\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.30,\n                                                    shuffle = True\n                                                    )","f5f4bce2":"X_train.shape ","13fa8a71":"#### Create pipeline ####\n#### Pipe using XGBoost\nsteps_xg = [('sts', ss() ),\n            ('pca', PCA()),\n            ('xg',  XGBClassifier(silent = False,\n                                  n_jobs=2)        \n            )\n            ]\n# Instantiate Pipeline object\npipe_xg = Pipeline(steps_xg)","db159ebf":"#####################  Randomized Search #################\n\n# Tune parameters using randomized search\n# Hyperparameters to tune and their ranges\nparameters = {'xg__learning_rate':  uniform(0, 1),\n              'xg__n_estimators':   range(50,100),\n              'xg__max_depth':      range(3,5),\n              'pca__n_components' : range(5,7)}","5c3c7d04":"#     Tune parameters using random search\n#     Create the object first\nrs = RandomizedSearchCV(pipe_xg,\n                        param_distributions=parameters,\n                        scoring= ['roc_auc', 'accuracy'],\n                        n_iter=15,          # Max combination of\n                                            # parameter to try. Default = 10\n                        verbose = 3,\n                        refit = 'roc_auc',\n                        n_jobs = 2,          # Use parallel cpu threads\n                        cv = 2               # No of folds.\n                                             # So n_iter * cv combinations\n                        )","8a5b423c":"# Run random search for 25 iterations. 21 minutes\nstart = time.time()\nrs.fit(X_train, y_train)\nend = time.time()\n(end - start)\/60","e000e2cb":"f\"Best score: {rs.best_score_} \"","731bee9f":"# Make predictions\ny_pred_rs = rs.predict(X_test)","c2decc47":"# Accuracy\naccuracy_rs = accuracy_score(y_test, y_pred_rs)\nf\"Accuracy: {accuracy_rs * 100.0}\"","22c57113":"###############  Tuning using Bayes Optimization ############\npara_set = {\n           'learning_rate':  (0, 1),                 \n           'n_estimators':   (50,100),               \n           'max_depth':      (3,5),                 \n           'n_components' :  (5,7)          \n            }","dfd58713":"#    Create a function that when passed some parameters\n#    evaluates results using cross-validation\ndef xg_eval(learning_rate,n_estimators, max_depth,n_components):\n    #  Make pipeline. Pass parameters directly here\n    pipe_xg1 = make_pipeline (ss(),                        # Why repeat this here for each evaluation?\n                              PCA(n_components=int(round(n_components))),\n                              XGBClassifier(\n                                           silent = False,\n                                           n_jobs=2,\n                                           learning_rate=learning_rate,\n                                           max_depth=int(round(max_depth)),\n                                           n_estimators=int(round(n_estimators))\n                                           )\n                             )\n\n    # Now fit the pipeline and evaluate\n    cv_result = cross_val_score(estimator = pipe_xg1,\n                                X= X_train,\n                                y = y_train,\n                                cv = 2,\n                                n_jobs = 2,\n                                scoring = 'f1'\n                                ).mean()             # take the average of all results\n\n\n    #  Finally return maximum\/average value of result\n    return cv_result","80b3c5d8":"# Instantiate BayesianOptimization() object\nxgBO = BayesianOptimization(\n                             xg_eval,     \n                             para_set     \n                             )","20db4fdf":"# Gaussian process parameters\ngp_params = {\"alpha\": 1e-5}      ","408a1e8b":"#  Fit\/train the BayesianOptimization() object\nstart = time.time()\nxgBO.maximize(init_points=5,    \n               n_iter=25,        \n              **gp_params\n               )\nend = time.time()\n(end-start)\/60","84ec8c73":"# Get the values of parameters that maximise the objective\nxgBO.res\nxgBO.max","aa382814":"#  Model with parameters of random search\nmodel_rs = XGBClassifier(\n                    learning_rate = rs.best_params_['xg__learning_rate'],\n                    max_depth = rs.best_params_['xg__max_depth'],\n                    n_estimators=rs.best_params_['xg__n_estimators']\n                    )\n\n#  Model with parameters of Bayesian Optimization\nmodel_bo = XGBClassifier(\n                    learning_rate = xgBO.max['params']['learning_rate'],\n                    max_depth = int(xgBO.max['params']['max_depth']),\n                    n_estimators= int(xgBO.max['params']['n_estimators'])\n                    )","797af9ec":"# Modeling with all the parameters\nstart = time.time()\nmodel_rs.fit(X_train, y_train)\nmodel_bo.fit(X_train, y_train)\nend = time.time()\n(end - start)\/60","5f7dad07":"# Predictions with all the models\ny_pred_rs = model_rs.predict(X_test)\ny_pred_bo = model_bo.predict(X_test)","15aea4b5":"# Accuracy from all the models\naccuracy_rs = accuracy_score(y_test, y_pred_rs)\naccuracy_bo = accuracy_score(y_test, y_pred_bo)\nprint(\"Random Search\",accuracy_rs)\nprint(\"Bayesian Optimization\",accuracy_bo)","a9c96569":"# Feature importances from all the models\nmodel_rs.feature_importances_\nmodel_bo.feature_importances_\nplot_importance(model_rs)\nplot_importance(model_bo)","0dffcb9b":"# Get probability of occurrence of each class\ny_pred_prob_rs = model_rs.predict_proba(X_test)\ny_pred_prob_bo = model_bo.predict_proba(X_test)\n\n# Draw ROC curve\nfpr_rs, tpr_rs, thresholds = roc_curve(y_test,\n                                 y_pred_prob_rs[: , 0],\n                                 pos_label= 0\n                                 )\n\nfpr_bo, tpr_bo, thresholds = roc_curve(y_test,\n                                 y_pred_prob_bo[: , 0],\n                                 pos_label= 0\n                                 )\n# AUC\nauc_rs = auc(fpr_rs,tpr_rs)\nauc_bo = auc(fpr_bo,tpr_bo)","272f0c6b":"performance = pd.DataFrame({ \"Classifiers\":[\"Random Search\",'Bayesian Optimization'],\n                             \"Accuracy\": [accuracy_score(y_test,y_pred_rs),accuracy_score(y_test,y_pred_bo)],\n                             \"Precision\": [precision_score(y_test,y_pred_rs),precision_score(y_test,y_pred_bo)],\n                             \"AUC\":[auc_rs,auc_bo],\n                             \"Recall\":[recall_score(y_test,y_pred_rs),recall_score(y_test,y_pred_bo)],\n                             \"f1_score\":[f1_score(y_test,y_pred_rs),f1_score(y_test,y_pred_bo)]})\nperformance","968a26ef":"fig = plt.figure(figsize=(12,10))   # Create window frame\nax = fig.add_subplot(111)   # Create axes\n\n#8.1 Connect diagonals\nax.plot([0, 1], [0, 1], ls=\"--\")  # Dashed diagonal line\n\n#8.2 Labels \nax.set_xlabel('False Positive Rate')  # Final plot decorations\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC curve for models')\n\n#8.3 Set graph limits\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.0])\n\n#8.4 Plot each graph now\nax.plot(fpr_rs, tpr_rs, label = \"rs\")\nax.plot(fpr_bo, tpr_bo, label = \"bo\")\n\n\n#8.5 Set legend and show plot\nax.legend(loc=\"lower right\")\nplt.show()","33c77d2e":"plt.bar(rs.best_params_.keys(), rs.best_params_.values(), color='y')\nplt.xticks(rotation=25)","6c1f93bb":"for features in xgBO.max.values(): \n    print(features)","22b40a69":"plt.bar(features.keys(), features.values(), color='y')\nplt.xticks(rotation=25)","ca451655":"# Fitting the parameters into the model","11aa13b8":"# Importing Required Libraries","6fb37b45":"# Parameter tuning using Random Search and Bayesian Optimization"}}