{"cell_type":{"dfc81bb2":"code","6d5982bb":"code","7a964b67":"code","56b42f89":"code","593e50e6":"code","32a277c2":"code","b5818fce":"code","e9f5b2ca":"code","6f516ad3":"code","94be6b73":"code","fdf38286":"code","70d2733d":"code","d96c5542":"code","b67854de":"code","79872575":"code","12abd32f":"code","e9332f24":"code","f145070e":"code","aff94c27":"code","161b502e":"code","7e384dea":"code","2a3c9b23":"code","ec9f9355":"code","e6eb50d7":"code","f41c5974":"code","23a73b40":"code","7a1c2106":"code","97cea82e":"markdown","c206b324":"markdown","7c6f5cfa":"markdown","468e41aa":"markdown","88708ec0":"markdown","0425e3a5":"markdown","ec2e1a41":"markdown","d4c6e670":"markdown","42b8eb52":"markdown","63a950a0":"markdown","2eea0c95":"markdown","d130aab6":"markdown","af2fff51":"markdown","ff4c398e":"markdown","5e122f4f":"markdown","379f1a45":"markdown","d279e8de":"markdown","acc3e7cb":"markdown","718b12de":"markdown","8e885bcc":"markdown","c1640d05":"markdown","69544853":"markdown"},"source":{"dfc81bb2":"import nltk \nw1 = set('AI is our friend and it has been friendly'.lower().split())\nw2 = set('AI and humans have always been friendly'.lower().split())\n \nprint (\"Jaccard similarity of above two sentences is\",1-nltk.jaccard_distance(w1, w2))","6d5982bb":"w1 = set('Kaggle is awesome'.lower().split())\nw2 = set('kaggle is great way of learning DS'.lower().split())\nprint(\"The Jaccard similarity is:\",1-nltk.jaccard_distance(w1, w2))","7a964b67":"!pip install \"\/kaggle\/input\/chart-studio\/chart_studio-1.0.0-py3-none-any.whl\"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom wordcloud import WordCloud, STOPWORDS\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nimport re\nimport string\n\nimport matplotlib.pyplot as plt\nfrom plotly import tools\nimport chart_studio.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nimport os\nimport tokenizers\nimport torch\nimport transformers\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n# word level jaccard score: https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/overview\/evaluation\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","56b42f89":"train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","593e50e6":"train['text'] = train['text'].str.replace('[{}]'.format(string.punctuation), '')\ntest['text'] = test['text'].str.replace('[{}]'.format(string.punctuation), '')","32a277c2":"train.head(3)","b5818fce":"test.head(3)","e9f5b2ca":"print('Sentiment of text : {} \\nOur training text :\\n{}\\nSelected text which we need to predict:\\n{}'.format(train['sentiment'][0],train['text'][0],train['selected_text'][0]))","6f516ad3":"# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=600, \n                    height=300,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train.loc[train['sentiment'] == 'neutral', 'text'].append(test.loc[test['sentiment'] == 'neutral', 'text']), title=\"Word Cloud of Neutral tweets\",color = 'white')","94be6b73":"plot_wordcloud(train.loc[train['sentiment'] == 'positive', 'text'].append(test.loc[test['sentiment'] == 'positive', 'text']), title=\"Word Cloud of Positive tweets\",color = 'green')","fdf38286":"plot_wordcloud(train.loc[train['sentiment'] == 'negative', 'text'].append(test.loc[test['sentiment'] == 'negative', 'text']), title=\"Word Cloud of negative tweets\",color = 'red')","70d2733d":"from collections import defaultdict\ntrain0_df = train[train[\"sentiment\"]=='positive'].dropna().append(test[test[\"sentiment\"]=='positive'].dropna())\ntrain1_df = train[train[\"sentiment\"]=='neutral'].dropna().append(test[test[\"sentiment\"]=='neutral'].dropna())\ntrain2_df = train[train[\"sentiment\"]=='negative'].dropna().append(test[test[\"sentiment\"]=='negative'].dropna())\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'red')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of positive tweets\", \"Frequent words of neutral tweets\",\n                                          \"Frequent words of negative tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\niplot(fig, filename='word-plots')\n","d96c5542":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'gray')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'orange')\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'brown')\n\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04,horizontal_spacing=0.25,\n                          subplot_titles=[\"Bigram plots of Positive tweets\", \n                                          \"Bigram plots of Neutral tweets\",\n                                          \"Bigram plots of Negative tweets\"\n                                          ])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\n\n\nfig['layout'].update(height=1000, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Plots\")\niplot(fig, filename='word-plots')","b67854de":"for sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'blue')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'violet')\n\n\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04, horizontal_spacing=0.05,\n                          subplot_titles=[\"Tri-gram plots of Positive tweets\", \n                                          \"Tri-gram plots of Neutral tweets\",\n                                          \"Tri-gram plots of Negative tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\niplot(fig, filename='word-plots')","79872575":"train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\ntrain['select_num_words'] = train[\"selected_text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\ntrain['select_num_unique_words'] = train[\"selected_text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\ntrain['select_num_chars'] = train[\"selected_text\"].apply(lambda x: len(str(x)))","12abd32f":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=train['num_words'],name = 'Number of words in text of train data'))\nfig.add_trace(go.Histogram(x=test['num_words'],name = 'Number of words in text of test data'))\nfig.add_trace(go.Histogram(x=train['select_num_words'],name = 'Number of words in selected text'))\n\n# Overlay both histograms\nfig.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","e9332f24":"fig_ = go.Figure()\nfig_.add_trace(go.Histogram(x=train['num_chars'],name = 'Number of characters in text of train data',marker = dict(color = 'rgba(222, 111, 33, 0.8)')))\nfig_.add_trace(go.Histogram(x=test['num_chars'],name = 'Number of characters in text of test data',marker = dict(color = 'rgba(33, 1, 222, 0.8)')))\nfig_.add_trace(go.Histogram(x=train['select_num_chars'],name = 'Number of characters in selected text',marker = dict(color = 'rgba(108, 25, 7, 0.8)')))\n\n# Overlay both histograms\nfig_.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig_.update_traces(opacity=0.75)\nfig_.show()","f145070e":"fig_ = go.Figure()\nfig_.add_trace(go.Histogram(x=train['num_unique_words'],name = 'Number of unique words in text of train data',marker = dict(color = 'rgba(222, 1, 3, 0.8)')))\nfig_.add_trace(go.Histogram(x=test['num_unique_words'],name = 'Number of unique words in text of test data',marker = dict(color = 'rgba(3, 221, 2, 0.8)')))\nfig_.add_trace(go.Histogram(x=train['select_num_unique_words'],name = 'Number of unique words in selected text',marker = dict(color = 'rgba(1, 2, 237, 0.8)')))\n\n# Overlay both histograms\nfig_.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig_.update_traces(opacity=0.75)\nfig_.show()","aff94c27":"MAX_LEN = 128\nVALID_BATCH_SIZE = 8\nBERT_PATH = \"..\/input\/roberta-base\/\"\nMODEL_PATH = \"model.bin\"\nTRAINING_FILE = \"..\/input\/train.csv\"\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{BERT_PATH}\/vocab.json\", \n    merges_file=f\"{BERT_PATH}\/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)","161b502e":"class TweetModel(nn.Module):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        self.bert = transformers.RobertaModel.from_pretrained(BERT_PATH)\n        self.l0 = nn.Linear(768, 2)\n    \n    def forward(self, ids, mask, token_type_ids):\n        sequence_output, pooled_output = self.bert(\n            ids, \n            attention_mask=mask\n        )\n        logits = self.l0(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","7e384dea":"device = torch.device(\"cuda\")\nmodel = TweetModel()\nmodel.to(device)\nmodel = nn.DataParallel(model)\nmodel.load_state_dict(torch.load(\"..\/input\/roberta-weights\/roberta_model_1.bin\"))\nmodel.eval()\n\nmodel1 = TweetModel()\nmodel1.to(device)\nmodel1 = nn.DataParallel(model1)\nmodel1.load_state_dict(torch.load(\"..\/input\/roberta-weights\/roberta_model_2.bin\"))\nmodel1.eval()","2a3c9b23":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n    \n    def __getitem__(self, item):\n    \n        tweet = \" \" + \" \".join(str(self.tweet[item]).split())\n        selected_text = \" \" + \" \".join(str(self.selected_text[item]).split())\n    \n        len_st = len(selected_text)\n        idx0 = -1\n        idx1 = -1\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n            if tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 != -1 and idx1 != -1:\n            for ct in range(idx0, idx1):\n                # if tweet[ct] != \" \":\n                char_targets[ct] = 1\n\n        #print(f\"char_targets: {char_targets}\")\n\n        tok_tweet = self.tokenizer.encode(tweet)\n        tok_tweet_tokens = tok_tweet.tokens\n        tok_tweet_ids = tok_tweet.ids\n        tok_tweet_offsets = tok_tweet.offsets\n        \n        targets = [0] * len(tok_tweet_ids)\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(tok_tweet_offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                targets[j] = 1\n                target_idx.append(j)\n\n        \n        targets_start = [0] * len(targets)\n        targets_end = [0] * len(targets)\n\n        non_zero = np.nonzero(targets)[0]\n        if len(non_zero) > 0:\n            targets_start[non_zero[0]] = 1\n            targets_end[non_zero[-1]] = 1\n\n        # check padding:\n        # <s> pos\/neg\/neu <\/s> <\/s> tweet <\/s>\n        if len(tok_tweet_tokens) > self.max_len - 5:\n            tok_tweet_tokens = tok_tweet_tokens[:self.max_len - 5]\n            tok_tweet_ids = tok_tweet_ids[:self.max_len - 5]\n            targets_start = targets_start[:self.max_len - 5]\n            targets_end = targets_end[:self.max_len - 5]\n\n\n        sentiment_id = {\n            'positive': 1313,\n            'negative': 2430,\n            'neutral': 7974\n        }\n\n        tok_tweet_ids = [0] + [sentiment_id[self.sentiment[item]]] + [2] + [2] + tok_tweet_ids + [2]\n        targets_start = [0] + [0] + [0] + [0] + targets_start + [0]\n        targets_end = [0] + [0] + [0] + [0] + targets_end + [0]\n        token_type_ids = [0, 0, 0, 0] + [0] * (len(tok_tweet_ids) - 5) + [0]\n        mask = [1] * len(token_type_ids)\n\n        padding_length = self.max_len - len(tok_tweet_ids)\n        \n        tok_tweet_ids = tok_tweet_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        targets_start = targets_start + ([0] * padding_length)\n        targets_end = targets_end + ([0] * padding_length)\n\n        return {\n            'ids': torch.tensor(tok_tweet_ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets_start': torch.tensor(targets_start, dtype=torch.float),\n            'targets_end': torch.tensor(targets_end, dtype=torch.float),\n            'padding_len': torch.tensor(padding_length, dtype=torch.long),\n            'orig_tweet': self.tweet[item],\n            'orig_selected': self.selected_text[item],\n            'sentiment': self.sentiment[item]\n        }\n","ec9f9355":"df_test = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","e6eb50d7":"test_dataset = TweetDataset(\n        tweet=df_test.text.values,\n        sentiment=df_test.sentiment.values,\n        selected_text=df_test.selected_text.values\n    )\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=VALID_BATCH_SIZE,\n    num_workers=1\n)","f41c5974":"all_outputs = []\nfin_outputs_start = []\nfin_outputs_end = []\nfin_outputs_start1 = []\nfin_outputs_end1 = []\nfin_padding_lens = []\nfin_orig_selected = []\nfin_orig_sentiment = []\nfin_orig_tweet = []\nfin_tweet_token_ids = []\n\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        padding_len = d[\"padding_len\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.float)\n        targets_end = targets_end.to(device, dtype=torch.float)\n\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start1, outputs_end1 = model1(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        \n\n        fin_outputs_start.append(torch.sigmoid(outputs_start).cpu().detach().numpy())\n        fin_outputs_end.append(torch.sigmoid(outputs_end).cpu().detach().numpy())\n        \n        fin_outputs_start1.append(torch.sigmoid(outputs_start1).cpu().detach().numpy())\n        fin_outputs_end1.append(torch.sigmoid(outputs_end1).cpu().detach().numpy())\n        \n        fin_padding_lens.extend(padding_len.cpu().detach().numpy().tolist())\n        fin_tweet_token_ids.append(ids.cpu().detach().numpy().tolist())\n\n        fin_orig_sentiment.extend(sentiment)\n        fin_orig_selected.extend(orig_selected)\n        fin_orig_tweet.extend(orig_tweet)\n\nfin_outputs_start = np.vstack(fin_outputs_start)\nfin_outputs_end = np.vstack(fin_outputs_end)\n\nfin_outputs_start1 = np.vstack(fin_outputs_start1)\nfin_outputs_end1 = np.vstack(fin_outputs_end1)\n\nfin_outputs_start = (fin_outputs_start + fin_outputs_start1) \/ 2\nfin_outputs_end = (fin_outputs_end + fin_outputs_end1) \/ 2\n\n\nfin_tweet_token_ids = np.vstack(fin_tweet_token_ids)\njaccards = []\nthreshold = 0.2\nfor j in range(fin_outputs_start.shape[0]):\n    target_string = fin_orig_selected[j]\n    padding_len = fin_padding_lens[j]\n    sentiment_val = fin_orig_sentiment[j]\n    original_tweet = fin_orig_tweet[j]\n\n    if padding_len > 0:\n        mask_start = fin_outputs_start[j, 4:-1][:-padding_len] >= threshold\n        mask_end = fin_outputs_end[j, 4:-1][:-padding_len] >= threshold\n        tweet_token_ids = fin_tweet_token_ids[j, 4:-1][:-padding_len]\n    else:\n        mask_start = fin_outputs_start[j, 4:-1] >= threshold\n        mask_end = fin_outputs_end[j, 4:-1] >= threshold\n        tweet_token_ids = fin_tweet_token_ids[j, 4:-1][:-padding_len]\n\n    mask = [0] * len(mask_start)\n    idx_start = np.nonzero(mask_start)[0]\n    idx_end = np.nonzero(mask_end)[0]\n    if len(idx_start) > 0:\n        idx_start = idx_start[0]\n        if len(idx_end) > 0:\n            idx_end = idx_end[0]\n        else:\n            idx_end = idx_start\n    else:\n        idx_start = 0\n        idx_end = 0\n\n    for mj in range(idx_start, idx_end + 1):\n        mask[mj] = 1\n\n    output_tokens = [x for p, x in enumerate(tweet_token_ids) if mask[p] == 1]\n\n    filtered_output = TOKENIZER.decode(output_tokens)\n    filtered_output = filtered_output.strip().lower()\n\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 4:\n        filtered_output = original_tweet\n\n    all_outputs.append(filtered_output.strip())","23a73b40":"sample = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = all_outputs\nsample.to_csv(\"submission.csv\", index=False)","7a1c2106":"sample.head()","97cea82e":"In this notebook i am using Roberta model. <br>\nThe whole training stratergy was similar to which shown in this [video](https:\/\/www.youtube.com\/watch?v=XaQ0CBlQ4cY). The few things that i changed is that i trained the model using KFold cross validation strategy. 20 Epochs for each fold total of 100 Epochs. I am only using the models which has better validation Jaccard score.<br>\n#### Credits : <br>\n* Author : [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek) <br>\n* [Reference notebook](https:\/\/www.kaggle.com\/abhishek\/tweet-text-extraction-roberta-infer) <br>\n<br>\nI Strong recommend you to have a look at that notebook and upvote if you like it.\n\n\n\n","c206b324":"we have 0.33 which is size of intersection of the set divided by total size of set.","7c6f5cfa":"# 5.Model:","468e41aa":"<img src=\"https:\/\/m.economictimes.com\/thumb\/msid-71590483,width-1200,height-900,resizemode-4,imgsize-178633\/tweet.jpg\" width=\"500\" height=\"500\" align=\"center\"\/>","88708ec0":"# Competition metric: <br>\nThe metric in this competition is the word-level Jaccard score. <br>\nJaccard similarity or intersection over union is defined as size of intersection divided by size of union of two sets.The Jaccard Index, also known as the Jaccard similarity coefficient, is a statistic used in understanding the similarities between sample sets. The measurement emphasizes similarity between finite sample sets, and is formally defined as the size of the intersection divided by the size of the union of the sample sets. The mathematical representation of the index is written as: <br>\n![](https:\/\/images.deepai.org\/glossary-terms\/jaccard-index-9707615.jpg) <br>\nSimilar to the Jaccard Index, which is a measurement of similarity, the Jaccard distance measures dissimilarity between sample sets. The Jaccard distance is calculated by finding the Jaccard index and subtracting it from 1, or alternatively dividing the differences ny the intersection of the two sets. The formula for the Jaccard distance is represented as: <br>\n![](https:\/\/images.deepai.org\/glossary-terms\/jaccard-index-391304.jpg) <br>\n#### How does the Jaccard Index work? <br>\nBreaking down the formula, the Jaccard Index is essentially the number in both sets, divided by the number in either set, multiplied by 100. This will produce a percentage measurement of similarity between the two sample sets. Accordingly, to find the Jaccard distance, simply subtract the percentage value from 1. For example, if the similarity measurement is 35%, then the Jaccard distance (1 - .35) is .65 or 65%. <br>\n\n#### Let\u2019s take example of two sentences: <br>\n**Sentence 1**: AI is our friend and it has been friendly <br>\n**Sentence 2**: AI and humans have always been friendly <br>\nIn order to calculate similarity using Jaccard similarity, we will first perform lemmatization to reduce words to the same root word. In our case, \u201cfriend\u201d and \u201cfriendly\u201d will both become \u201cfriend\u201d, \u201chas\u201d and \u201chave\u201d will both become \u201chas\u201d. Drawing a Venn diagram of the two sentences we get: <br>\n![](https:\/\/miro.medium.com\/max\/579\/1*u2ZZPh5er5YbmOg7k-s0-A.png)  <br>\nFor more : [read here](https:\/\/towardsdatascience.com\/overview-of-text-similarity-metrics-3397c4601f50) and [here](https:\/\/deepai.org\/machine-learning-glossary-and-terms\/jaccard-index)\n","0425e3a5":"# Importing Data:","ec2e1a41":"# About Competition: <br>\n\"My ridiculous dog is amazing.\" [sentiment: positive]\n\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.","d4c6e670":"We can observe from above histogram plot that the number of words in train text and test text ranges from 1 to 30.Selected text words mostly fall in range of 1-10. ","42b8eb52":"# Alot more to come.<font color='red'>If you like this kernel please consider Upvoting it<\/font>.I welcome suggestions to improve this kernel further.<br>","63a950a0":"# Calculating Jaccard similarity using NLTK Library:","2eea0c95":"# Importing libraries:","d130aab6":"### Key observations:\nFrom above Ngaram analysis we can observe that neutral tweets and negative tweets had more amount of repeteted words than positive tweets.","af2fff51":"# 4.Histogram plots of Number of characters","ff4c398e":"## Tri-gram Plots:","5e122f4f":"# 1. Word clouds of Text:","379f1a45":"## Bi-gram Plots:","d279e8de":"# 2.Ngram Analysis: <br>","acc3e7cb":"From above plot we can see that number of characters in test and train set was in same range.In selected text the range flows from 3 to 138 Characters.","718b12de":"We can see that number of unique words in train and test sets range from 1 to 26. In selected text most number  ","8e885bcc":"We can understand from above output that the text was neutral sentiment and our key words in the sentence is **my boss was not happy w\/ them. Lots of fun.** Which we need to predict.","c1640d05":"# 3. Histogram plot of Number of words","69544853":"### <font color='red'>If you find this kernel useful please consider upvoting it \ud83d\ude0a which keeps me motivated for doing hard work and to produce more quality content.<\/font>"}}