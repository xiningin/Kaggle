{"cell_type":{"0985d19c":"code","efffbf34":"code","97995e83":"code","5f860635":"code","d4b5f835":"code","f75a077c":"code","39591fd0":"code","b1c2b666":"code","88f97902":"code","b632a37b":"code","359d09f9":"code","a4ccdfdb":"code","e7817398":"code","f7626267":"code","45047c87":"code","d6269ed4":"code","48270e7c":"code","9c12ab32":"code","fc324264":"code","d1735b01":"code","2852f4d4":"code","f9cf04e3":"code","40ad54bc":"code","02fb8096":"code","abaf4007":"code","4b4a7c8a":"code","039df599":"code","455b400e":"code","c09c0d72":"markdown","6f630a89":"markdown","72b85422":"markdown","0e96476d":"markdown","cbe11fd4":"markdown","76f6d4d8":"markdown","27654826":"markdown","b99f92ea":"markdown"},"source":{"0985d19c":"# Data Analysis\nimport pandas as pd \nimport pandas as pd\n\n# Linear Algebra\nimport numpy as np \nfrom numpy import mean\nfrom numpy import std\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport plotly\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot, plot\ninit_notebook_mode(connected=True) \n\n# Model Creation\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\nfrom tensorflow.keras.layers import Conv1D,MaxPooling1D, Activation, Dropout, Flatten, Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import GridSearchCV","efffbf34":"column_names=[\"engine_no\",\"cycle\",\"op1\",\"op2\",\"op3\",\"s1\",\"s2\",\"s3\",\"s4\",\"s5\",\"s6\",\"s7\",\"s8\",\n         \"s9\",\"s10\",\"s11\",\"s12\",\"s13\",\"s14\",\"s15\",\"s16\",\"s17\",\"s18\",\"s19\",\"s20\",\"s21\",\n         \"s22\",\"s23\"]\n\ntrain=pd.read_csv(\"..\/input\/turbofan-predictive-maintenance-nasa\/train_FD001.txt\",sep=\" \",names=column_names)\ntest=pd.read_csv(\"..\/input\/turbofan-predictive-maintenance-nasa\/test_FD001.txt\",sep=\" \",names=column_names)\ntest_results=pd.read_csv(\"..\/input\/turbofan-predictive-maintenance-nasa\/RUL_FD001.txt\",sep=\" \",names=[\"rul\", \"null\"], header=None)","97995e83":"train.info()","5f860635":"test.info()","d4b5f835":"test_results.info()","f75a077c":"train.head()","39591fd0":"test.head()","b1c2b666":"train[train[\"engine_no\"]==1][\"cycle\"]","88f97902":"test_results.head()","b632a37b":"test_results.drop([\"null\"],axis=1,inplace=True)\ntest_results['engine_no']=test_results.index+1\ntest_results.head()","359d09f9":"rul = pd.DataFrame(test.groupby('engine_no')['cycle'].max()).reset_index()\nrul.columns = ['engine_no', 'max']\nrul.head()","a4ccdfdb":"# 'rul_failed' is is the rul at which the engine fails\ntest_results['rul_failed']=test_results['rul']+rul['max']\ntest_results.head()","e7817398":"# Calculate the rul of test dataset\ntest_results.drop([\"rul\"],axis=1,inplace=True)\ntest=test.merge(test_results,on=['engine_no'],how='left')\ntest[\"rul\"]=test[\"rul_failed\"]-test[\"cycle\"]","f7626267":"# Calculate rul of train dataset\ntrain['rul'] = train.groupby(['engine_no'])['cycle'].transform(max)-train['cycle']","45047c87":"# Drop sensors with empty values\ntrain.drop([\"s22\",\"s23\"],axis=1,inplace=True)\ntest.drop([\"s22\",\"s23\"],axis=1,inplace=True)","d6269ed4":"# plot all operational settings and sensors in the training set\n\nrows = len(train.columns.difference([\"cycle\", \"engine_no\", \"rul\"]))\nfig = make_subplots(\n    rows=rows, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.01,\n    subplot_titles=train.columns.difference([\"cycle\", \"engine_no\", \"rul\"])\n)\n\nrow = 1\nfor s in train.columns.difference([\"cycle\", \"engine_no\", \"rul\"]):\n  for engine, engine_no in train.groupby(\"engine_no\"):\n    fig.add_trace(go.Scatter(x=engine_no[\"cycle\"], y=engine_no[s],mode=\"lines\",name=engine),row=row, col=1)\n\n  row = row+1\n\nfig.update_layout(\n    height=2000,\n    showlegend=True\n)\n\nfig.show()","48270e7c":"# check the operational settings and sensors of a specific engine \nrows = len(train.columns.difference([\"cycle\", \"engine_no\", \"rul\"]))\nfig = make_subplots(\n    rows=rows, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.01,\n    subplot_titles=train.columns.difference([\"cycle\", \"engine_no\", \"rul\"])\n\n)\n\nengine_no = 43\nrow = 1\nfor s in train.columns.difference([\"cycle\", \"engine_no\", \"rul\"]): \n  fig.add_trace(go.Scatter(x=train[train[\"engine_no\"]==engine_no][\"cycle\"], y=train[train[\"engine_no\"]==engine_no ][s],mode=\"lines\",name=s),row=row, col=1)\n  \n  row = row+1\n\nfig.update_layout(\n    height=2000,\n    showlegend=True,\n    title_text=\"sensor recordings\"\n)\n\nfig.show()","9c12ab32":"# Drop the operational settings and sensors that do not useful information\n\ntrain.drop(columns=[\"op2\", \"op3\", \"s1\", \"s5\", \"s6\", \"s10\", \"s16\", \"s18\", \"s19\"], inplace=True)\ntest.drop(columns=[\"op2\", \"op3\", \"s1\", \"s5\", \"s6\", \"s10\", \"s16\", \"s18\", \"s19\", \"rul_failed\"], inplace=True)","fc324264":"from sklearn.preprocessing import MinMaxScaler\n\ndef normalize_feature(train_feat, test_feat): \n  norm = MinMaxScaler().fit(train_feat)\n\n  return norm.transform(train_feat), norm.transform(test_feat)\n\nfor col in train.columns.difference([\"cycle\", \"engine_no\", \"rul\"]): \n  train[col], test[col] = normalize_feature(train[[col]], test[[col]])","d1735b01":"def temporalize(X, y, lookback): \n    output_X = []\n    output_y = []\n    for i in range(len(X)-lookback-1):\n        t = []\n        for j in range(1,lookback+1):\n            # Gather past records upto the lookback period\n            t.append(X[[(i+j+1)], :])\n        output_X.append(t)\n        output_y.append(y[i+lookback+1])\n    return output_X, output_y","2852f4d4":"n_features = 15\ntimesteps = 50\nX_train = []\ny_train = []\nfor engine in train[\"engine_no\"].unique():\n  x = train[train[\"engine_no\"] == engine].drop(columns = [\"engine_no\", \"cycle\", \"rul\"])\n  y = train[train[\"engine_no\"] == engine][\"rul\"]\n  x, y = temporalize(x.values, y.values, timesteps)\n  X_train.extend(x)\n  y_train.extend(y)\n\nX_train = np.array(X_train)\nX_train = X_train.reshape(X_train.shape[0], timesteps, n_features)\nprint(X_train.shape)\ny_train = np.array(y_train)\nprint(y_train.shape)","f9cf04e3":"X_test = []\ny_test = []\nfor engine in test[\"engine_no\"].unique():\n  x = test[test[\"engine_no\"] == engine].drop(columns = [\"engine_no\", \"cycle\", \"rul\"])\n  y = test[test[\"engine_no\"] == engine][\"rul\"]\n  x, y = temporalize(x.values, y.values, timesteps)\n  X_test.extend(x)\n  y_test.extend(y)\n\nX_test = np.array(X_test)\nX_test = X_test.reshape(X_test.shape[0], timesteps, n_features)\nprint(X_test.shape)\ny_test = np.array(y_test)\nprint(y_test.shape)","40ad54bc":"def create_model(lr=0.001, drop_CNN=0, drop_dense=0.2, kernel_size=3):\n    model = Sequential()\n    intput_shape=(X_train.shape[1], X_train.shape[2])\n    model.add(Conv1D(128, kernel_size=kernel_size, padding = \"same\", activation=\"relu\", input_shape = intput_shape))\n    model.add(Dropout(drop_CNN))\n    model.add(MaxPooling1D(pool_size=2, padding='same'))\n    model.add(Conv1D(128,kernel_size=kernel_size, padding = \"same\", activation=\"relu\"))\n    model.add(Dropout(drop_CNN))\n    model.add(MaxPooling1D(pool_size=2, padding='same'))\n    model.add(TimeDistributed(Flatten()))\n    model.add(LSTM(units = 128, return_sequences=True))\n    model.add(LSTM(units = 128, return_sequences=False))\n    model.add(Dense(200, activation='relu'))\n    model.add(Dropout(drop_dense))\n    model.add(Dense(1, activation = 'linear'))\n    opt = keras.optimizers.Adam(learning_rate=lr)\n    model.compile(optimizer=opt, loss='mse')\n\n    return model","02fb8096":"if False: \n  from keras.wrappers.scikit_learn import KerasRegressor\n  from sklearn.model_selection import GridSearchCV\n\n\n  model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=256)\n\n  drop_CNN = [0, 0.2]\n  drop_dense = [0.25, 0.5, 0.75]\n  lr = [0.001, 0.01, 0.1]\n  kernel_size = [3, 5]\n\n  params = dict(lr=lr, drop_CNN=drop_CNN, drop_dense = drop_dense, kernel_size=kernel_size)\n  model = GridSearchCV(estimator=model, param_grid=params, verbose=0, cv=5)\n  grid_result = model.fit(X_train, y_train)\n  # summarize results\n  print(\"Best: %f using %s\" % (model.best_score_, model.best_params_))\n  means = model.cv_results_['mean_test_score']\n  stds = model.cv_results_['std_test_score']\n  params = model.cv_results_['params']\n  for mean, stdev, param in zip(means, stds, params):\n      print(\"%f (%f) with: %r\" % (mean, stdev, param))","abaf4007":"if True:\n    model=create_model(lr=0.001, drop_CNN=0, drop_dense= 0.5, kernel_size=3)\n    history = model.fit(X_train, y_train, epochs=100, batch_size=256, validation_split=0.2,\n                        shuffle=False, \n                        use_multiprocessing=True, verbose=2)\n    \n    plt.plot(history.history['loss'], linewidth=2, label='Train')\n    plt.plot(history.history['val_loss'], linewidth=2, label='Valid')\n    plt.legend(loc='upper right')\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.show()\n    \n\n   ","4b4a7c8a":"predictions = model.predict(X_test)\npredictions = predictions.reshape(predictions.shape[0])\nnp.mean(np.square(predictions - y_test))","039df599":"fig = go.Figure()\n\n\nfig.add_trace(go.Scatter(y=y_test, name=\"actual\"))\n\nfig.add_trace(go.Scatter(y=predictions, name=\"predicted\"))\n\nfig.update_traces(mode='lines', marker_line_width=0.2)\n\nfig.show()","455b400e":"# get a closer look\nfig = go.Figure()\n\n\nfig.add_trace(go.Scatter(y=y_test[:2000], name=\"actual\"))\n\nfig.add_trace(go.Scatter(y=predictions[:2000], name=\"predicted\"))\n\nfig.update_traces(mode='lines', marker_line_width=0.2)\n\nfig.show()","c09c0d72":"# Load Data","6f630a89":"**> Thank you! :) **","72b85422":"GridSearchCV is used for hyperparamater tuning. After running the gridsearch, the best parameters found were: \n- learning rate = 0.001\n- drop_CNN = 0\n- drop_dense = 0.5\n- kernel_size = 3","0e96476d":"This notebook shows the steps taken to predict the remaining useful life (RUL) of turbofan engines by using a 1D-CNN-LSTM model. \n\n**Questions and suggestions on how to improve the work done are always welcome.**","cbe11fd4":"# Plotting results","76f6d4d8":"# Data preparation (as per required by 1D-CNN-LSTM) and model creation","27654826":"# Data Analysis and Preprocessing","b99f92ea":"# Import Necessary Libraries"}}