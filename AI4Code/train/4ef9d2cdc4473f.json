{"cell_type":{"ec267c9c":"code","0ce220c3":"code","371a320b":"code","b0f08bec":"code","05ae7afe":"code","94465ad5":"code","f2fe5b7b":"code","46f42bdd":"code","f0aeda96":"code","1384c22e":"code","1f1ba014":"code","768adbec":"code","c49b85e4":"code","f6fdbde1":"code","dacb871e":"markdown","602d7eb5":"markdown","8d63ecbb":"markdown","64df7154":"markdown","e241d327":"markdown","2a75bc22":"markdown","ce933bc5":"markdown","301cc188":"markdown","29fadc04":"markdown","2ca713a4":"markdown","d7e50883":"markdown"},"source":{"ec267c9c":"#for mathematical calculations\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","0ce220c3":"data_path = '\/kaggle\/input\/digit-recognizer'\n\ntrain = pd.read_csv(data_path+\"\/train.csv\")\ntest = pd.read_csv(data_path+\"\/test.csv\")\n\ntrain_x = train.drop(labels=[\"label\"], axis=1)\ntrain_y = train[\"label\"]","371a320b":"test = np.array(test[:11])\ntest =  test \/ 255\ntrain_x = np.array(train_x)\ntrain_x = train_x \/255","b0f08bec":"train_X, test_X, train_Y, test_Y = train_test_split(train_x,\n                                                    train_y,\n                                                    test_size=0.2,\n                                                    random_state=42)","05ae7afe":"train_X = np.array(train_X)\ntest_X = np.array(test_X)\n\ntrain_Y = np.array(train_Y)\ntest_Y = np.array(test_Y)","94465ad5":"# one hot encoding\ndef vectorize(sequence, maxlen):\n    vectors = np.zeros((len(sequence), maxlen))\n    for i, nums in enumerate(sequence):\n        vectors[i, nums] = 1\n    return vectors","f2fe5b7b":"train_Y = vectorize(train_Y, 10)\ntest_Y = vectorize(test_Y, 10)","46f42bdd":"def softmax(x):\n    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return exps\/np.sum(exps, axis=1, keepdims=True)\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n        \ndef sigmoid_derivative(x):\n    return x*(1 - x)","f0aeda96":"def dropout(layer, dropout_prop):\n    rs = np.random.RandomState(123)\n    prop = 1 - dropout_prop\n    mask = rs.binomial(size=layer.shape, n=1, p=prop)\n    layer *= mask\/prop\n    return layer","1384c22e":"def acc(y, p):\n    accuracy = np.sum(np.argmax(y, axis=1) == np.argmax(p, axis=1)) \/ len(y)\n    return accuracy","1f1ba014":"weights_1 = 2*np.random.random((784, 784)) - 1\nweights_2 = 2*np.random.random((784, 10)) - 1\n\nbias_1 = 0\nbias_2 = 0\n\nepochs = 100\ndropout_prop = 0.5\nlearning_rate = 0.0001\nbranch_size = 512\n\nhistory = {\n    'train': [],\n    'test': []\n    }","768adbec":"for epoch  in range(epochs):\n\n    for branch in range(len(train_X)\/\/branch_size+1):\n        # branch data\n        X = train_X[branch*branch_size:(branch+1)*branch_size]\n        Y = train_Y[branch*branch_size:(branch+1)*branch_size]\n        \n        ### FEED FORWARD\n        l1 = sigmoid(np.dot(X, weights_1)) + bias_1\n        l2 = softmax(np.dot(l1, weights_2)) + bias_2\n        l2 = dropout(l2, dropout_prop)\n\n        ### BACK PROPOGATION\n\n        # first layer error\n        delta = l2 - Y\n\n        w_adjustment_1 = np.dot(l1.T, delta)\n        b_adjustment_1 = np.sum(delta, axis=0, keepdims=True)\n\n        # second layer error\n        error = np.dot(delta, weights_2.T)\n        delta = error*sigmoid_derivative(l1) \n\n        w_adjustment_2 = np.dot(X.T, delta)\n        b_adjustment_2 = np.sum(delta, axis=0)\n\n\n        # change weights\n        # first adjustment\n        weights_1 -= learning_rate * w_adjustment_2\n        bias_1 -= learning_rate * b_adjustment_2\n        \n        # second adjustment\n        weights_2 -= learning_rate * w_adjustment_1\n        bias_2 -= learning_rate * b_adjustment_1\n    \n    ### TEST ACCURACY\n    \n    # make prediction with train data\n    l1 = sigmoid(np.dot(train_X, weights_1)) + bias_1\n    train_P = softmax(np.dot(l1, weights_2)) + bias_2\n    \n    # accuracy of train data \n    train_acc = round(acc(train_Y, train_P), 4)\n    history['train'].append(train_acc)\n\n    # make prediction with validation data\n    l1 = sigmoid(np.dot(test_X, weights_1)) + bias_1\n    test_P = softmax(np.dot(l1, weights_2)) + bias_2\n\n    # accuracy of test data\n    test_acc = round(acc(test_Y, test_P), 4)\n    history['test'].append(test_acc)\n    \n    # view error\n    error_line = f'Epoch: {epoch+1}  Train  [acc: {train_acc}]  Validation  [acc: {test_acc}]'\n    print(error_line)","c49b85e4":"plt.plot(history['test'], label='test')\nplt.plot(history['train'], label='train')\n\nplt.xlabel('epoch')\nplt.ylabel('true answers')\nplt.legend()\nplt.show()","f6fdbde1":"i = 5\ntest_case = test[i:i+1]\n\n# make prediction with test case\nl1 = sigmoid(np.dot(test_case, weights_1)) + bias_1\nl2 = softmax(np.dot(l1, weights_2)) + bias_2\n\n# display prediction\nplt.bar([str(i) for i in range(0,10)], l2[0])\nplt.xlabel(\"numbers\")\nplt.ylabel(\"probability\")\nplt.show()\n\n# display test case number\ntest_case = test_case[0]\ntest_case = test_case.reshape(28,28)\ntest_case *= 255\n\nplt.imshow(test_case)\nplt.show()","dacb871e":"# **activation function**","602d7eb5":"# split data","8d63ecbb":"# test neural network","64df7154":"![explanation](https:\/\/i.ibb.co\/THVXrnL\/WP-20200626-21-29-34-Pro.jpg)","e241d327":"# dropout","2a75bc22":"# neural network structure","ce933bc5":"In code, the structure of the neural network seems confusing.\nHere is a visualization of back propagation as a flowchart.","301cc188":"# **build and train neural network**","29fadc04":"# prepare data","2ca713a4":"# load data","d7e50883":"# accuracy"}}