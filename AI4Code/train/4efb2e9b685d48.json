{"cell_type":{"b15ebe99":"code","23ca8b65":"code","ddc7692c":"code","7b6a6dd2":"code","ebc15fad":"code","f6c63a18":"code","011bbbc0":"code","54e9d378":"code","d60a08ed":"code","c7147cdd":"code","20f3c738":"code","209adad9":"code","0d8efa95":"code","ad76379c":"markdown","779da1e8":"markdown","999fe3b4":"markdown","c7e148ef":"markdown","392529b3":"markdown","8c2eba2f":"markdown","93c70247":"markdown","0768bad0":"markdown","16b3c348":"markdown","80cfcbd2":"markdown","bb3c03f4":"markdown"},"source":{"b15ebe99":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\npath = '..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv'\nfeatures = ['pid','sid','p','s']\nsms = pd.read_csv(path,names=features,sep='\\t',header=0)\nprint(sms.shape)\nprint(sms.head(10))\n","23ca8b65":"import matplotlib.pyplot as plt\ndef plot_bars(auto_prices, cols):\n    for col in cols:\n        fig = plt.figure(figsize=(6,6)) # define plot area\n        ax = fig.gca() # define axis    \n        counts = auto_prices[col].value_counts() # find the counts for each unique category\n        counts.plot.bar(ax = ax, color = 'blue') # Use the plot.bar method on the counts data frame\n        ax.set_title('Number sentiments' + col) # Give the plot a main title\n        ax.set_xlabel(col) # Set text for the x axis\n        ax.set_ylabel('freq')# Set text for y axis\n        plt.show()\n\nplot_cols = ['s']\nplot_bars(sms, plot_cols)  \nprint(sms.s.value_counts())","ddc7692c":"X=sms.p\nY=sms.s","7b6a6dd2":"import nltk\nfrom nltk.stem import PorterStemmer\nps=PorterStemmer()\nl2=[]\nreview=[]\ns2=''\nfor row in X:\n    for words in nltk.word_tokenize(row):\n            #print(words)\n            l2.append(words.lower())\n            l2.append(' ')\n    s2=''.join(l2)\n    review.append(s2)\n    s2=''\n    l2=[]\nX=review\nprint(X[:1])","ebc15fad":"from sklearn.cross_validation import train_test_split\nX_train, X_inter, Y_train, Y_inter = train_test_split(X, Y,test_size=0.3,random_state=1)\nX_val, X_test, Y_val, Y_test = train_test_split(X_inter, Y_inter,test_size=0.5,random_state=1)\nprint(len(X_train))\nprint(len(X_val))\nprint(len(X_test))","f6c63a18":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\nmax_sentence=len(max(X,key=len))\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\n","011bbbc0":"from numpy import asarray\nembeddings_index = dict()\nf = open('..\/input\/glove-t\/glove.twitter.27B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","54e9d378":"from numpy import zeros\nvocab_size = len(tokenizer.word_index) + 1\nembedding_matrix = zeros((vocab_size, 100))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","d60a08ed":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\nmax_sentence=len(max(X,key=len))\n\n#tokenizer = Tokenizer()\n#tokenizer.fit_on_texts(X_train)\nencoded_docs = tokenizer.texts_to_sequences(X_train)\ntrain_x = pad_sequences(encoded_docs, maxlen=max_sentence, padding='post')\nprint(train_x[0])    \n\nencoded_docs=0\nencoded_docs = tokenizer.texts_to_sequences(X_val)\nval_x = pad_sequences(encoded_docs, maxlen=max_sentence, padding='post')\nprint(val_x[1])\n\nencoded_docs=0\nencoded_docs = tokenizer.texts_to_sequences(X_test)\ntest_x = pad_sequences(encoded_docs, maxlen=max_sentence, padding='post')\nprint(test_x[1])\n\nencoder = LabelEncoder()\nencoder.fit(Y_train)\nencoded_Y_train = encoder.transform(Y_train)\ndummy_y_train = np_utils.to_categorical(encoded_Y_train)\nprint(dummy_y_train[:3])\n\nencoded_Y_val = encoder.transform(Y_val)\n# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y_val = np_utils.to_categorical(encoded_Y_val)\n\n\n\n\nvocab_size = len(tokenizer.word_index) + 1","c7147cdd":"from keras.layers import Input, Dense, concatenate, Activation\nfrom keras.models import Model\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Embedding,CuDNNGRU,Bidirectional\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\nfrom keras.layers import LocallyConnected1D\nfrom keras.layers import Flatten\n\n\ntweet_input = Input(shape=(max_sentence,), dtype='int32')\n\ntweet_encoder = Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sentence, trainable=True, weights=[embedding_matrix])(tweet_input)\nbigram_branch = LocallyConnected1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\nbigram_branch = GlobalMaxPooling1D()(bigram_branch)\ntrigram_branch = LocallyConnected1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\ntrigram_branch = GlobalMaxPooling1D()(trigram_branch)\nfourgram_branch = LocallyConnected1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\nfourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\nmerged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n\nmerged = Dense(256, activation='relu')(merged)\n#merged = Dropout(0.2)(merged)\nmerged = Dense(5, activation='softmax')(merged)\nmodel = Model(inputs=[tweet_input], outputs=[merged])\nprint(model.summary())\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(train_x, dummy_y_train,  validation_data=(val_x, dummy_y_val), epochs=3,batch_size=128,verbose=1)\n","20f3c738":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Embedding,CuDNNGRU,Bidirectional\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\nfrom keras.layers import LocallyConnected1D\nfrom keras.layers import Flatten\n\n\"\"\"\nmodel=0\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sentence, trainable=True, weights=[embedding_matrix] ))\nmodel.add(LocallyConnected1D(128, 2,strides=1,padding='valid', activation='relu'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(256, activation='relu'))          \nmodel.add(Dense(5, activation='softmax'))\nprint(model.summary())\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(train_x, dummy_y_train,  validation_data=(val_x, dummy_y_val), epochs=2,batch_size=128,verbose=1)\n\"\"\"","209adad9":"import sklearn.metrics as sklm\npredictions=model.predict(test_x)\npred=[]\nfor idx,val in enumerate(predictions):\n    pred.append(np.argmax(val))\n\n\nprint(len(Y_test))\nprint(len(pred))\nprint(set(Y_test))\nprint(set(pred))\nmetrics = sklm.precision_recall_fscore_support(Y_test, pred)\n\n\nprint('Accuracy  %0.2f' % sklm.accuracy_score(Y_test, pred))\nprint('           0     1     2     3     4')\nprint('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1]+ '        %6.2f' % metrics[0][2]+ '        %6.2f' % metrics[0][3]+ '        %6.2f' % metrics[0][4])\nprint('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1]+ '        %6.2f' % metrics[1][2]+ '        %6.2f' % metrics[1][3]+ '        %6.2f' % metrics[1][4])\nprint('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1]+ '        %6.2f' % metrics[2][2]+ '        %6.2f' % metrics[2][3]+ '        %6.2f' % metrics[2][4])\n\nY_test=pd.Series(Y_test)\npred=pd.Series(pred)\npd.crosstab(Y_test, pred, rownames=['True'], colnames=['Predicted'], margins=True)\n","0d8efa95":"\n\npath = '..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv'\nfeatures = ['PhraseId','sid','p']\ntest_frame = pd.read_csv(path,names=features,sep='\\t',header=0)\n\n\n#For test dataset\n\nl2=[]\nreview=[]\ns2=''\nfor row in test_frame['p']:\n    for words in nltk.word_tokenize(row):\n            #print(words)\n            l2.append(words.lower())\n            l2.append(' ')\n    s2=''.join(l2)\n    review.append(s2)\n    s2=''\n    l2=[]\ntest_frame['p_stemmed']=review\n\n\n\n\nencoded_docs=0\nencoded_docs = tokenizer.texts_to_sequences(test_frame['p_stemmed'])\ntemp_test = pad_sequences(encoded_docs, maxlen=max_sentence, padding='post')\n\npredictions=model.predict(temp_test)\npred=[]\nfor idx,val in enumerate(predictions):\n    pred.append(np.argmax(val))\n\ntest_frame['Sentiment']=pred\ntest_frame.drop(['p','sid','p_stemmed'],axis=1,inplace=True)\nprint(test_frame.head(10))\ntest_frame.to_csv('output.csv',index=False)","ad76379c":"**Creating Dict out of Glove twitter embeddings file.**","779da1e8":"**Padding and Conversion of Text into Sequences**","999fe3b4":"**Model Creation and Training**","c7e148ef":"**Creating Embedding Matrix with respect to tokenizer indexing**","392529b3":">** Stemming and Lower casing.**","8c2eba2f":"**Fitting training text on tokenizer for latter use tokenizer indexing.**","93c70247":"**Train, Test, Validation Split**","0768bad0":"**Visualizing Class Imbalance**","16b3c348":"**Metrics **","80cfcbd2":"**test.csv Prediction, for the sake of simplicity i have not merged test and train code.**","bb3c03f4":"**Using Glove Embeddings**"}}