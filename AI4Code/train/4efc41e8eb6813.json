{"cell_type":{"b4ad13fe":"code","40d3eb9e":"code","ea05be2d":"code","8fc11380":"code","4b0ff5e0":"code","2a1afce0":"code","432ead80":"code","cc7bea7c":"code","59cf6890":"code","3c51470e":"code","eae64205":"code","09801b77":"code","e31ccd28":"code","7f4d8845":"code","2a19ba2a":"code","1e5f2741":"code","59ca86cc":"code","f1ba05e6":"code","02c079bf":"code","64dd984c":"code","88937257":"code","5f185392":"code","df371b82":"code","fffbe491":"markdown","fa33fbad":"markdown"},"source":{"b4ad13fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nimport math\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","40d3eb9e":"df = pd.read_csv('\/kaggle\/input\/tesla-stock-data-from-2010\/TSLA.csv', parse_dates = ['Date'], index_col = 'Date')\ndf","ea05be2d":"print('First Trade observation date: %s \\nLast Trade observation date: %s' % (df.index[0].date(), df.index[-1].date()))","8fc11380":"df.info()","4b0ff5e0":"df.describe()","2a1afce0":"# Check for missing values\ndf.isnull().sum()","432ead80":"#We can observe that Close and Adj Close has no varaince. So, we can drop Adj Close column.\nprint(np.var(df['Close'] - df['Adj Close']))","cc7bea7c":"df.drop(['Adj Close'], inplace = True, axis = 1)","59cf6890":"#Plot Time-Series\nfrom matplotlib import pyplot as plt\n# Define a function to draw time_series plot\ndef timeseries (x_axis, y_axis, x_label, y_label):\n    plt.figure(figsize = (10, 6))\n    plt.plot(x_axis, y_axis)\n    #plt.plot(x_axis, y_axis, color ='black')\n    plt.xlabel(x_label, {'fontsize': 12})\n    plt.ylabel(y_label, {'fontsize': 12})\n\ntimeseries(df.index, df['Open'], 'Trading day', 'Stock Open Price')","3c51470e":"def plot_histogram(x):\n    plt.hist(x, bins = 20, alpha=0.8, edgecolor = 'black')\n    plt.title(\"Histogram of '{var_name}'\".format(var_name=x.name))\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n\nplot_histogram(df['Open'])","eae64205":"train_size = int(len(df)*0.8)\ntrain_dataset, test_dataset = df.iloc[:train_size], df.iloc[train_size:]\n\n# Plot train and test data\nplt.figure(figsize = (10, 6))\nplt.plot(train_dataset.Open)\nplt.plot(test_dataset.Open)\nplt.xlabel('Trading day')\nplt.ylabel('Stock Open Price')\nplt.legend(['Train set', 'Test set'], loc='upper right')\n#plt.savefig('C:\/Users\/nious\/Documents\/Medium\/LSTM&GRU\/2.jpg', format='jpg', dpi=1000)\n\nprint('Dimension of train data: ',train_dataset.shape)\nprint('Dimension of test data: ', test_dataset.shape)","09801b77":"# Split train data to X and y\nX_train = train_dataset.drop('Open', axis = 1)\ny_train = train_dataset.loc[:,['Open']]\n\n# Split test data to X and y\nX_test = test_dataset.drop('Open', axis = 1)\ny_test = test_dataset.loc[:,['Open']]","e31ccd28":"X_test.shape, y_test.shape","7f4d8845":"#Good rule of thumb is that normalized data lead to better performance in Neural Networks. I used MinMaxScaler from sklearn.\nfrom sklearn.preprocessing import MinMaxScaler\n# Different scaler for input and output\nscaler_x = MinMaxScaler(feature_range = (0,1))\nscaler_y = MinMaxScaler(feature_range = (0,1))\n\n# Fit the scaler using available training data\ninput_scaler = scaler_x.fit(X_train)\noutput_scaler = scaler_y.fit(y_train)\n\n# Apply the scaler to training data\ntrain_y_norm = output_scaler.transform(y_train)\ntrain_x_norm = input_scaler.transform(X_train)\n\n# Apply the scaler to test data\ntest_y_norm = output_scaler.transform(y_test)\ntest_x_norm = input_scaler.transform(X_test)","2a19ba2a":"X_test.shape, y_test.shape","1e5f2741":"#LSTM, GRU and BiLSTM take a 3D input (num_samples, num_timesteps, num_features).\n#Time_steps = 30. It means that the model makes predictions based on the last 30-day data \n# In the loop, it takes 30 days data and create a list out of it and append. For target value 30 th day will become the first ouput. \ndef create_dataset (X, y, time_steps = 1):\n    Xs, ys = [], []\n    \n    for i in range(len(X)-time_steps):\n        v = X[i:i+time_steps, :]\n        Xs.append(v)\n        ys.append(y[i+time_steps])\n        \n    return np.array(Xs), np.array(ys)\n\n\nTIME_STEPS = 30\n\nX_test, y_test = create_dataset(test_x_norm, test_y_norm, TIME_STEPS)\nX_train, y_train = create_dataset(train_x_norm, train_y_norm, TIME_STEPS)\nprint('X_train.shape: ', X_train.shape)\nprint('y_train.shape: ', y_train.shape)\nprint('X_test.shape: ', X_test.shape) \nprint('y_test.shape: ', y_test.shape)","59ca86cc":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential, layers, callbacks\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n\n# Create BiLSTM model, no of neurons is imput\ndef create_model_bilstm(units):\n    model = Sequential()\n    # First layer of BiLSTM\n    model.add(Bidirectional(LSTM(units = units, return_sequences=True), \n                            input_shape=(X_train.shape[1], X_train.shape[2])))\n    # Second layer of BiLSTM\n    model.add(Bidirectional(LSTM(units = units)))\n    model.add(Dense(1))\n    #Compile model\n    model.compile(loss='mse', optimizer='adam')\n    return model\n\n\n# Create LSTM or GRU model, Input is neurons and model type\ndef create_model(units, m):\n    model = Sequential()\n    # First layer of LSTM\n    model.add(m (units = units, return_sequences = True, \n                 input_shape = [X_train.shape[1], X_train.shape[2]]))\n    model.add(Dropout(0.2)) \n    # Second layer of LSTM\n    model.add(m (units = units))                 \n    model.add(Dropout(0.2))\n    model.add(Dense(units = 1)) \n    #Compile model\n    model.compile(loss='mse', optimizer='adam')\n    return model\n\n\n# BiLSTM\nmodel_bilstm = create_model_bilstm(64)\n\n# GRU and LSTM \nmodel_gru = create_model(64, GRU)\nmodel_lstm = create_model(64, LSTM)","f1ba05e6":"# Fit BiLSTM, LSTM and GRU\ndef fit_model(model):\n    early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss',\n                                               patience = 10)\n\n    # shuffle = False because the order of the data matters\n    history = model.fit(X_train, y_train, epochs = 100, validation_split = 0.2,\n                    batch_size = 32, shuffle = False, callbacks = [early_stop])\n    return history\n\nhistory_bilstm = fit_model(model_bilstm)\nhistory_lstm = fit_model(model_lstm)\nhistory_gru = fit_model(model_gru)","02c079bf":"#Plot train loss vs validation loss\ndef plot_loss (history, model_name):\n    plt.figure(figsize = (10, 6))\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model Train vs Validation Loss for ' + model_name)\n    plt.ylabel('Loss')\n    plt.xlabel('epoch')\n    plt.legend(['Train loss', 'Validation loss'], loc='upper right')\n    #plt.savefig('C:\/Users\/nious\/Documents\/Medium\/LSTM&GRU\/loss_'+model_name+'.jpg', format='jpg', dpi=1000)\n\nplot_loss (history_bilstm, 'BiLSTM')\nplot_loss (history_lstm, 'LSTM')\nplot_loss (history_gru, 'GRU')","64dd984c":"#After building the model, I have to transform the target variable back to original data space for train and test data using scaler_y.inverse_transform.\ny_test = scaler_y.inverse_transform(y_test)\ny_train = scaler_y.inverse_transform(y_train)","88937257":"#Make prediction using BiLSTM, LSTM and GRU\ndef prediction(model):\n    prediction = model.predict(X_test)\n    #model predicted values will be scaled values and we need to transform to actual stock values\n    prediction = scaler_y.inverse_transform(prediction)\n    return prediction\n\nprediction_bilstm = prediction(model_bilstm)\nprediction_lstm = prediction(model_lstm)\nprediction_gru = prediction(model_gru)","5f185392":"#Plot true future vs prediction\u00b6\ndef plot_future(prediction, model_name, y_test):\n    \n    plt.figure(figsize=(10, 6))\n    \n    range_future = len(prediction)\n\n    plt.plot(np.arange(range_future), np.array(y_test), label='True Future')\n    plt.plot(np.arange(range_future), np.array(prediction),label='Prediction')\n\n    plt.title('True future vs prediction for ' + model_name)\n    plt.legend(loc='upper left')\n    plt.xlabel('Trading day')\n    plt.ylabel('Open Stcok Price')\n    #plt.savefig('C:\/Users\/nious\/Documents\/Medium\/LSTM&GRU\/predic_'+model_name+'.jpg', format='jpg', dpi=1000)\n    \n    \nplot_future(prediction_bilstm, 'BiLSTM', y_test)\nplot_future(prediction_lstm, 'LSTM', y_test)\nplot_future(prediction_gru, 'GRU', y_test)","df371b82":"# Define a function to calculate MAE and RMSE\ndef evaluate_prediction(predictions, actual, model_name):\n    errors = predictions - actual\n    mse = np.square(errors).mean()\n    rmse = np.sqrt(mse)\n    mae = np.abs(errors).mean()\n\n    print(model_name + ':')\n    print('Mean Absolute Error: {:.4f}'.format(mae))\n    print('Root Mean Square Error: {:.4f}'.format(rmse))\n    print('')\n\n\nevaluate_prediction(prediction_bilstm, y_test, 'Bidirectional LSTM')\nevaluate_prediction(prediction_lstm, y_test, 'LSTM')\nevaluate_prediction(prediction_gru, y_test, 'GRU')","fffbe491":"#Referred other stcok prediction model by a different user.","fa33fbad":"When we split the data by 80-20, training set included data it 2019. Where as many variations to stock price happened in 2020 which is our test data here. It feels like traing data is not sufficeient to catch the trend in 2020 to do predictions correctly."}}