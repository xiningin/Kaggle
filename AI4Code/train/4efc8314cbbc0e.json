{"cell_type":{"9fdc4e49":"code","4fba5839":"code","19934992":"code","bf90b40c":"code","3c2d7296":"code","b63de04a":"code","34be93f6":"code","f452513b":"code","c6ee9648":"code","2479fe11":"code","fade6fc9":"code","72651a41":"code","ad5ef696":"code","76c4b3ce":"code","8e0a0f86":"code","6022cddd":"code","12b380a8":"code","a85b04cf":"code","b1c48338":"code","2985993f":"code","9ad202ad":"code","6ad2032c":"code","796e8944":"code","16cbffea":"code","2787bc53":"code","a0b6107e":"code","41815cb4":"code","1ee0d081":"code","d0d3cc72":"markdown","4743a34f":"markdown","9f749bcb":"markdown","89fdef3c":"markdown"},"source":{"9fdc4e49":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4fba5839":"# Importing tensorflow and keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Softmax\nfrom tensorflow.keras import optimizers","19934992":"# Checking the tensorflow libraries  \nprint(tf.__version__)\n","bf90b40c":"#Loading the dataset\n\nhousing_df=pd.read_csv(\"\/kaggle\/input\/california-housing-prices\/housing.csv\")","3c2d7296":"housing_df.shape","b63de04a":"# Displaying the data \n\nhousing_df.head()","34be93f6":"# Checking for the null values \n\nhousing_df.isna().any()","f452513b":"#Count of the null values in each columns\n\nhousing_df.isna().sum()","c6ee9648":"# We can drop the null values as their count is less than 5 %\n\nhousing_df.dropna(inplace=True)","2479fe11":"housing_df.isna().sum()","fade6fc9":"housing_df.shape","72651a41":"# Dividing the dataset into independant and dependant variables \nX=pd.DataFrame(columns=['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income','ocean_proximity'],data=housing_df)\ny=pd.DataFrame(columns=['median_house_value'],data=housing_df)","ad5ef696":"X.head()","76c4b3ce":"y.head()","8e0a0f86":"#Creating the dummy values for ocean_proximity\n\nX = pd.get_dummies(data = X, columns = ['ocean_proximity'] , prefix = ['ocean_proximity'] , drop_first = True)","6022cddd":"X.head()","12b380a8":"#Dividing the training data into test and train \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","a85b04cf":"X_train.shape","b1c48338":"#Feature Standardization\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","2985993f":"X_train","9ad202ad":"model = Sequential()\n\n#Input Layer\nmodel.add(Dense(X.shape[1], activation='relu', input_dim = X.shape[1]))\n\n#Hidden Layer\nmodel.add(Dense(512,kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(512,kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(256,kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(128,kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(64,kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(32,kernel_initializer='normal', activation='relu'))\n#Output Layer\nmodel.add(Dense(1,kernel_initializer='normal', activation = 'relu'))\n","6ad2032c":"X.shape[1]\n","796e8944":"#Compile the network \n\nmodel.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\nmodel.summary()","16cbffea":"history = model.fit(X_train, y_train.to_numpy(), batch_size = 10, epochs = 10, verbose = 1)\n","2787bc53":"y_pred = model.predict(X_test)\n","a0b6107e":"y_pred\n","41815cb4":"y_test\n","1ee0d081":"model.evaluate(X_test, y_test)\n","d0d3cc72":"## 1. Import libraries","4743a34f":"# 2. Data exploration","9f749bcb":"# 3. Feature Scaling and test train split","89fdef3c":"# ANN"}}