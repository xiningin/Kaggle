{"cell_type":{"e22b7ecf":"code","1bb439f5":"code","d82b17ab":"code","3ed8375f":"code","3dc27195":"markdown","9922b0b4":"markdown","5c29386c":"markdown","797a7233":"markdown","10707d5c":"markdown"},"source":{"e22b7ecf":"import numpy as np \nimport pandas as pd \n\n# Reading the files\ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\ntrain = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\n\n# Raw data\ntest_raw = test.copy()\ntrain_raw = train.copy()\n\n# target \ny_target = train.target\ntrain.drop(['target'],axis=1,inplace = True)\n\n# Let's divide numerical and categorical columns\n# Numerical\nnum_cols = train.select_dtypes(include=['int','float64']).columns\n# Categorical\ncat_cols = train.select_dtypes(include=['object']).columns","1bb439f5":"# with CPU we have less bias and viariance \n\nxgb_params = {'n_estimators': 10000,\n              'learning_rate': 0.35,\n              'subsample': 0.926,\n              'colsample_bytree': 0.84,\n              'max_depth': 2,\n               'booster': 'gbtree', \n              'reg_lambda': 35.1,\n              'reg_alpha': 34.9,\n#               'tree_method' : 'gpu_hist',\n#               'predictor': \"gpu_predictor\"\n             }","d82b17ab":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import KFold\n\npreds_valid = {}\npreds_test = []\ntotal_rmse = []\n\nkf = KFold(n_splits=5,random_state=0,shuffle=True)\n\nfor fold,(train_index, valid_index) in enumerate(kf.split(train,y_target)):\n\n    X_train,X_valid = train.loc[train_index], train.loc[valid_index]\n    y_train,y_valid = y_target.loc[train_index], y_target.loc[valid_index]\n    \n    # Preprocessing\n    test = test_raw.copy()\n    \n    # One hot Encoder\n    ohe = OneHotEncoder(sparse = False, drop = 'first')\n    X_train_ohe = pd.DataFrame(ohe.fit_transform(X_train[cat_cols])).set_index(X_train.index)\n    X_valid_ohe = pd.DataFrame(ohe.transform(X_valid[cat_cols])).set_index(X_valid.index)\n    test_ohe = pd.DataFrame(ohe.transform(test[cat_cols]))\n    \n    # Merging\n    X_train = pd.concat([X_train,X_train_ohe], axis = 1)\n    X_valid = pd.concat([X_valid,X_valid_ohe], axis = 1)\n    test = pd.concat([test,test_ohe],axis = 1)\n    X_train.drop(cat_cols,axis = 1, inplace= True)\n    X_valid.drop(cat_cols,axis = 1, inplace= True)\n    test.drop(cat_cols,axis = 1, inplace= True)\n    \n    # Standar Scaler\n    ss = StandardScaler()\n    X_train[num_cols] = ss.fit_transform(X_train[num_cols])\n    X_valid[num_cols] = ss.transform(X_valid[num_cols])\n    test[num_cols] = ss.transform(test[num_cols])\n    \n    # drop id\n    X_train.drop('id', axis = 1, inplace = True)\n    index_valid  = X_valid.id\n    X_valid.drop('id', axis = 1, inplace = True)\n    test.drop('id', axis = 1, inplace = True)\n    \n    # model\n    model = XGBRegressor(**xgb_params,random_state = 0)\n    model.fit(X_train, y_train,\n              verbose=False,\n              # These three parameters will stop training before a model starts overfitting \n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=300,\n              )\n    \n    # x_valid\n    preds = model.predict(X_valid)\n    preds_valid.update(dict(zip(index_valid, preds)))\n    \n    # test\n    preds_test.append(model.predict(test))\n    \n    # Getting score for a fold model\n    fold_rmse = mean_squared_error(y_valid, preds, squared=False)\n    print(f\"Fold {fold} RMSE: {fold_rmse}\")\n\n    # Total rmse\n    total_rmse.append(fold_rmse)\n    \nprint(f\"mean RMSE: {np.mean(total_rmse)}, std: {np.std(total_rmse)}\")\n","3ed8375f":"output = pd.DataFrame({'id': test_raw.id,\n                       'target': np.mean(preds_test, axis = 0)})\noutput.to_csv('submission.csv', index=False)\noutput.head(10)","3dc27195":"# Do you want to get a good score in 30 days of ML competition?\n### Check it out my kernel and pay attention.","9922b0b4":"#### Model 1","5c29386c":"## Model - XGBoost ","797a7233":"## Insights\n#### After an EDA exploratory I got the following insights:\n* ##### The dataset has negative numbers. \n* ##### Catogorical columns have less than 16 values.\n* ##### There are not missing values.","10707d5c":"## Predictions"}}