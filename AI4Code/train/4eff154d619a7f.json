{"cell_type":{"32165eba":"code","45f14004":"code","fdad0dae":"code","b467b23f":"code","cbe657c4":"code","b4c7c0e5":"code","89f7e072":"code","96c06352":"code","e193d199":"code","96770370":"code","9bac39b2":"code","06d022c7":"code","39f543b5":"code","1dd4bc92":"code","325cca06":"code","a7d1981d":"code","c2a13d70":"code","68a8efab":"markdown","09094fff":"markdown","9a6dd032":"markdown"},"source":{"32165eba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","45f14004":"import random","fdad0dae":"#import tensorflow as tf\n#import tensorflow_datasets as tfds","b467b23f":"train_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')","cbe657c4":"#train, test = tfds.load('mnist',\n #                           split = ['train', 'test'],\n  #                          shuffle_files = True,\n   #                         as_supervised = True)","b4c7c0e5":"def sigmoid(z):\n    return 1.0 \/ (1.0 + np.exp(-z))\n\ndef cost_derivative(activations, y):\n        return (activations - y)\n    \ndef sigmoid_prime(z):\n    return sigmoid(z) * (1 - sigmoid(z))\n    \nclass NeuralNetwork:\n    \n    def __init__(self, sizes):\n        ## The first layer is the input layer\n        self.layers = len(sizes)\n        self.sizes = sizes\n        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n        self.biases = [np.random.randn(x, 1) for x in sizes[1:]]\n        \n    #def sigmoid(self, z):\n     #   return 1.0 \/ (1.0 + np.exp(-z))\n    \n    def feedforward(self, a):\n        for w, b in zip(self.weights, self.biases):\n            a = sigmoid(np.dot(w, a) + b)\n        return a\n    \n    def backpropagation(self, x, y):\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        \n        activation = x\n        activations = [x]\n        zs = []\n        \n        for weight, bias in zip(self.weights, self.biases):\n            z = np.dot(weight, activation) + bias\n            zs.append(z)\n            activation = sigmoid(z)\n            activations.append(activation)\n        \n        delta = cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activations[-2].T)\n        \n        for l in range(2, self.layers):\n            z = zs[-l]\n            sp = sigmoid_prime(z)\n            delta = np.dot(self.weights[-l + 1].T, delta) * sp\n            nabla_b[-l] = delta\n            nabla_w[-l] = np.dot(delta, activations[-l - 1].T)\n        return (nabla_b, nabla_w)\n    \n    def gradient_descent(self, training_data, eta):\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        \n        n = len(training_data)\n        \n        X = training_data.copy()\n        Y = X.pop('label')\n        X = X.T\n        \n        for i in range(n):\n            x = X.iloc[:, i].values.reshape(784, 1)\n            y = Y[i]\n            delta_nabla_b, delta_nabla_w = self.backpropagation(x, y)\n            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n        self.weights = [w - eta * nw for w, nw in zip(self.weights, nabla_w)]\n        self.biases = [b - eta * nb for b, nb in zip(self.biases, nabla_b)]\n        \n        return self.feedforward(X.iloc[:, 1].values.reshape(784, 1)), Y[1]","89f7e072":"class LoadDataset:\n    \n    def __init__(self, train):\n        self.train = train\n        \n    def transform(self):\n        self.train['label'] = self.train['label'].apply(self.transform_target)\n    \n    def transform_target(self, num):\n        x = np.zeros([10, 1])\n        x[num] = 1\n        return x","96c06352":"df = LoadDataset(train_data)\ndf.transform()","e193d199":"X = train_data.copy()\ny = X.pop('label')","96770370":"nn = NeuralNetwork([784, 30, 10])","9bac39b2":"val = X.T[1].values\nval = val.reshape(784, 1)\nnn.feedforward(val)","06d022c7":"y[0]","39f543b5":"nn.backpropagation(val, y[0])","1dd4bc92":"nn.gradient_descent(train_data[:1000], 0.005)","325cca06":"t = [3, 4, 4, 1]\n\nt[:-1]","a7d1981d":"t[1:]","c2a13d70":"t = [1, 2, 3, 4]\nd = 1\nn = len(t)\n\nt[d%n:] + t[:d%n - 1]","68a8efab":"# The module","09094fff":"# An example","9a6dd032":"# **APPENDIX**"}}