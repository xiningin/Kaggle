{"cell_type":{"f5740120":"code","2128b170":"code","8f7bef85":"code","9c4e2efd":"code","64f39a4c":"code","3fc8c8ae":"code","35bdc7e1":"code","80ecddc4":"code","71b7a160":"code","322e10f7":"code","fe4a581c":"code","a7a25405":"code","54565ab8":"markdown","fc38512f":"markdown","7d5558ee":"markdown","3c8e629a":"markdown","71f1e134":"markdown","63019c6e":"markdown","329c672c":"markdown","4493da22":"markdown","8d15e002":"markdown","b2975b66":"markdown","97f311b2":"markdown","42f1c5e5":"markdown","74258877":"markdown","80151b65":"markdown","4fcfd129":"markdown","3160ef1a":"markdown","58d3703a":"markdown"},"source":{"f5740120":"''' Import libraries '''\nimport torch \nimport torch.nn as nn\n\n# using librosa for audio processing\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nfrom IPython.core.display import display\nimport numpy as np\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset","2128b170":"# Check if device is available\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Currently running on\", device)","8f7bef85":"# Hyper Parameters to be used later\nSAMPLE_RATE = 16000\nLEARNING_RATE = 5e-5\nBATCH_SIZE = 128\nZ_DIM = 100\nNUM_EPOCHS = 5\nFEATURES_DISC = 32\nFEATURES_GEN = 32\nNUM_CLASSES = 10\nCRITIC_ITERATIONS = 5\nWEIGHT_CLIP = 0.01","9c4e2efd":"sample_data, _ = librosa.load('..\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/classical\/classical.00000.wav', sr=SAMPLE_RATE)\nipd.Audio(data=sample_data, rate=SAMPLE_RATE)","64f39a4c":"# A list of all the data samples are formed to be used in data loader\n\ncategories = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock'] \n\nfile_numbers = []\nfor i in range(10):\n  file_numbers.append(\"0000\" + str(i))\n\nfor i in range(10, 100):\n  file_numbers.append(\"000\" + str(i))\n\nfile_names = []\n\nfor category in categories:\n  for num in file_numbers:\n    \n    # the data is divided into three chunks of 10s data\n    for _ in range(3):\n      file_names.append(category + \"\/\" + category + \".\" + num)","3fc8c8ae":"# loading data set\n\n# We form a Custom Dataset object to be used later\n\nclass CustomAudioDataset(Dataset):\n  def __init__(self, file_dir, file_names, categories):\n    super(CustomAudioDataset, self).__init__()\n    self.file_dir = file_dir\n    self.categories = categories\n    self.file_names = file_names\n\n  def __len__(self):\n    return len(file_names)\n\n  def __getitem__(self, idx):\n    \n    try: \n        data, sample_rate = librosa.load(self.file_dir + self.file_names[idx] + \".wav\", 16000)\n    except:\n        print(\"Failed loading file\", self.file_names[idx])\n        return torch.zeros(1, 160008).to(device), int (idx\/300)\n    \n    # the input size is set to be 160008, so the data is truncated or padded accordingly\n    data = data[:480024]\n    data = np.pad(data, [0, 480024 - len(data)])\n    \n    # the data is split into three parts and stored\n    split_data = np.array_split(data, 3)\n    data_tensor = torch.from_numpy(split_data[idx % 3]).to(device)\n    data_tensor = torch.reshape(data_tensor, (1, 160008))\n    return data_tensor, int(idx \/ 300)","35bdc7e1":"audio_data_set = CustomAudioDataset('..\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/', file_names, categories)\nloader = DataLoader(audio_data_set, batch_size=BATCH_SIZE, shuffle=True)","80ecddc4":"class ConditionalDiscriminator(nn.Module):\n  def __init__(self, features_disc, input_size=160008, num_classes=10, embed_size=4000):\n    super(ConditionalDiscriminator, self).__init__()\n    \n    '''\n    The architecture follows the DCGAN Model but with some modifications\n    '''\n\n    self.input_size = input_size\n\n    # An embed layer is introduced to implemented the conditional GAN, the input is expanded and added as a channel to the original input\n    self.embed_layer = nn.Sequential(\n        nn.Embedding(num_classes, input_size),\n    )\n\n    # The discriminator follows the methodology of DCGAN with changes as mentioned above\n    self.disc = nn.Sequential(    #(n, 160008, 1)\n        nn.Conv1d( 2, features_disc, 16, 8, 2), # (n, 20000, 32)\n        nn.LeakyReLU(0.2),\n        self.block(features_disc, features_disc * 2, 16, 8, 2), #(n, 2499, 64)\n        self.block(features_disc * 2, features_disc * 4, 16, 8, 2), #(n, 311, 128)\n        self.block(features_disc * 4, features_disc * 8, 16, 8, 2), #(n, 38, 256)\n        nn.Flatten(),\n        nn.Dropout(0.2),\n        nn.Linear(38 * 256, 1),\n        nn.Sigmoid(),\n    )\n\n  def block(self, in_channels, out_channels, kernel_size, stride, padding):\n    return nn.Sequential(\n        nn.Conv1d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding\n        ),\n        nn.LeakyReLU(0.2)\n    )\n\n  def forward(self, x, label):\n\n    # A normal forward of input is done with embedding of label added as an additional channel\n    embed = self.embed_layer(label)\n    embed = torch.reshape(embed, (label.shape[0], 1, self.input_size)) \n    x = torch.cat([x, embed], dim=1)\n\n    return self.disc(x)\n    ","71b7a160":"class ConditionalGenerator(nn.Module):\n  def __init__(self, z_dim, features_gen, num_classes=10, input_size=160008, embed_size=100):\n    super(ConditionalGenerator, self).__init__()\n    \n    '''\n    The architecture follows the DCGAN Model but with some modifications\n    '''\n\n    self.embed_size = embed_size\n\n    # The generator follows the methodology of DCGAN with changes as mentioned above\n    self.net = nn.Sequential(\n        self.block(z_dim + embed_size, features_gen * 16, 39, 4, 0),\n        self.block(features_gen * 16, features_gen * 8, 16, 8, 4),\n        self.block(features_gen * 8, features_gen * 4, 16, 8, 2),\n        self.block(features_gen * 4, features_gen * 2, 16, 8, 2),\n        nn.ConvTranspose1d(features_gen * 2, 1, 16, 8, 2),\n        nn.Tanh()\n    )\n\n    # An embed layer is introduced to implemented the conditional GAN, the input is expanded and added as a channel to the original input\n    self.embed_layer = (\n        nn.Embedding(num_classes, embed_size)\n    )\n\n  def block(self, in_channels, out_channels, kernel_size, stride, padding):\n    return nn.Sequential(\n        nn.ConvTranspose1d(\n            in_channels,\n            out_channels,\n            kernel_size, \n            stride,\n            padding, \n            bias=False\n        ),\n        nn.ReLU(),\n    )\n\n  def forward(self, x, label):\n    \n    # An embedding of the input is formed and added as an channel to noise\n    embedding = self.embed_layer(label)\n    embedding = torch.reshape(embedding, (label.shape[0], self.embed_size, 1))\n    \n    x = torch.cat([x, embedding], dim=1)\n    output = self.net(x)\n    \n    # the output is more than the required dimension so the last few samples are dropped\n    return output[:,:,:160008]","322e10f7":"def initialize_weights(model):\n  for m in model.modules():\n    if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d)):\n      nn.init.normal_(m.weight.data, 0.0, 0.2)","fe4a581c":"def train_WGAN(loader, lr, batch_size, num_epochs, critic_iter, weight_clip):\n\n  # initialize weights\n  initialize_weights(model_gen)\n  initialize_weights(model_disc)\n\n  # initialize optimizatioin functions as suggested in WGAN\n  opt_gen = optim.RMSprop(model_gen.parameters(), lr=lr)\n  opt_disc = optim.RMSprop(model_disc.parameters(), lr=lr)\n\n  # set both model to train\n  model_gen.train()\n  model_disc.train()\n\n  for epoch in range(num_epochs):\n    for batch_idx, (real, label) in enumerate(loader):\n      real = real.to(device)\n      label = label.to(device)\n      noise = torch.randn(real.shape[0], Z_DIM, 1).to(device)\n      fake = model_gen(noise, label)\n\n      # Following the WGAN implementaion, the backward_prop of discriminator is done CRITIC_ITERATIONS times more than Generator with a changed loss function\n      for _ in range(CRITIC_ITERATIONS):\n        noise = torch.randn(real.shape[0], Z_DIM, 1).to(device)\n        fake = model_gen(noise, label)\n        critic_real = model_disc(real, label).reshape(-1)\n        critic_fake = model_disc(fake, label).reshape(-1)\n        loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))\n        model_disc.zero_grad()\n        loss_critic.backward(retain_graph=True)\n        opt_disc.step()\n\n        # Clipping of weights\n        for p in model_disc.parameters():\n          p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n\n      output = model_disc(fake, label).reshape(-1)\n      loss_gen = -torch.mean(output)\n      model_gen.zero_grad()\n      loss_gen.backward()\n      opt_gen.step()\n\n      # Printing results every 5 batches\n      if batch_idx % 10 == 0:\n        \n        \n          print(\"Epoch\", epoch, \" batch\", batch_idx + 1, \" completed.\")\n          test_noise = torch.randn(1, 100, 1).to(device)\n          test_label = torch.ones(1, 1).int().to(device) # As label for classical is 1\n          gen_audio = model_gen(test_noise, test_label)\n          gen_audio = gen_audio.reshape(-1)\n          gen_audio = gen_audio.cpu().detach().numpy()\n          display(ipd.Audio(gen_audio, rate=SAMPLE_RATE))\n","a7a25405":"# data loader\nfrom torch.utils.data import DataLoader\n\naudio_data_set = CustomAudioDataset('..\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/', file_names, categories)\nloader = DataLoader(audio_data_set, batch_size=BATCH_SIZE, shuffle=True)\n\n# torch.autograd.set_detect_anomaly(True)\n\nmodel_gen = ConditionalGenerator(Z_DIM, FEATURES_GEN).to(device)\nmodel_disc = ConditionalDiscriminator(FEATURES_DISC).to(device)\n\ntrain_WGAN(loader, LEARNING_RATE, BATCH_SIZE, NUM_EPOCHS, CRITIC_ITERATIONS, WEIGHT_CLIP)","54565ab8":"### The results obtained were not satisfactory and the model almost always output static noise. Possible changes that we think might work are Hyperparmeter tuning, Implementing Gradient Penalty, Looking at other representations of audio data such as MFCC.","fc38512f":"# DL Assignment 2\n","7d5558ee":"## Helper Function","3c8e629a":"## Prerequisites","71f1e134":"## [Notebook Link](https:\/\/www.kaggle.com\/nileshkumargupta\/dl-assignment-2)","63019c6e":"![image.png](attachment:e2be1cee-2810-4132-aa04-a7a453585975.png)","329c672c":"<h4>\n    Group Members\n    \n    Nilesh Kumar Gupta -> GAN Implementaion\n    Soham Sachin Sarpotdar -> Helper Functions\n    Atishay Jain -> Preprocessing\n    \n<\/h4>","4493da22":"### The function here initializes the weight of the three layers. This initailization method is mentioned in DCGAN","8d15e002":"## Discriminator Model","b2975b66":"## Defining the Training Function","97f311b2":"<h3>The task is to create a generative model (either a GAN or a VAE) that would generate\nan audio clip based on an input parameter that specifies the genre of the audio clip.<\/h3>","42f1c5e5":"### All the suggestion are followed with some changes to fit the size of data. For the last suggestion, only WGAN loss model was used instead of WGAN-GP","74258877":"## Dataset and Loading","80151b65":"## Generator Model","4fcfd129":"<h3>Adding and playing audio files<\/h3>","3160ef1a":"### Both Discriminator Model and Generator Model follow the architecture of DCGAN with suggestion from WaveGAN Model\n","58d3703a":"## Approach\n\n### We follow the approach presented in the paper \"[Adversarial Audio Synthesis](https:\/\/arxiv.org\/abs\/1802.04208)\". In the approach presented there they take the [DCGAN](https:\/\/arxiv.org\/abs\/1511.06434) model which was found to be extremely succesful for generation of images and try to modify it to suit the generation of audio. Most of the architecture of the model is referred to from these two papers and is adjusted to suit the input and output size of our data. We also implement conditional GAN on top of this model to achieve our results. The modification is done in the form of adding an embedding to both the generator and discriminator part of the model. The loss function used is also different from standard DCGAN and is referred from [WGAN](https:\/\/arxiv.org\/abs\/1701.07875). This loss function is preferred as it has been shown that it results in better stability of the training process. The conditional GAN architecture is referred from [here](https:\/\/www.youtube.com\/watch?v=Hp-jWm2SzR8)."}}