{"cell_type":{"62843240":"code","c016a708":"code","845c66f1":"code","432a1b02":"code","0477b160":"code","d22733ca":"code","14f12c3e":"code","f451865d":"code","e25f1a50":"code","ed5d191a":"code","23a37073":"code","9e54604b":"code","27faa5e7":"code","d033f219":"markdown","9c1aa28a":"markdown","d681eacc":"markdown","0add8e78":"markdown","330f0ad0":"markdown","65c0c116":"markdown","626ee5d1":"markdown"},"source":{"62843240":"import os\nimport optuna\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport sklearn.metrics as metrics\nimport sklearn.preprocessing as prep\nimport sklearn.model_selection as ms\nfrom functools import partial","c016a708":"train_csv = pd.read_csv(\"..\/input\/tps-september-xgb-in-gpu-baseline\/train_mean_filling.csv\")","845c66f1":"target = \"claim\"\nfeatures = [f for f in train_csv.columns if f not in [\"id\", target]]\nprint(features)","432a1b02":"# crossvalidation utility\nclass CrossValidation:\n    def __init__(self, df, shuffle,random_state=None):\n        self.df = df\n        self.random_state = random_state\n        self.shuffle = shuffle\n        if shuffle is True:\n            self.df = df.sample(frac=1,\n                random_state=self.random_state).reset_index(drop=True)\n\n    def hold_out_split(self,percent,stratify=None):\n        if stratify is not None:\n            y = self.df[stratify]\n            train,val = ms.train_test_split(self.df, test_size=percent\/100,\n                stratify=y, random_state=self.random_state)\n            return train,val\n        size = len(self.df) - int(len(self.df)*(percent\/100))\n        train = self.df.iloc[:size,:]\n        val = self.df.iloc[size:,:]\n        return train,val\n\n    def kfold_split(self, splits, stratify=None):\n        if stratify is not None:\n            kf = ms.StratifiedKFold(n_splits=splits,\n                shuffle=self.shuffle,\n                random_state=self.random_state)\n            y = self.df[stratify]\n            for train, val in kf.split(X=self.df,y=y):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v\n        else:\n            kf = ms.KFold(n_splits=splits, shuffle=self.shuffle,\n                random_state=self.random_state)\n            for train, val in kf.split(X=self.df):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v","0477b160":"seed = 42\nfolds = 5","d22733ca":"cv = CrossValidation(train_csv,\n                     shuffle=True,\n                     random_state=seed\n                    )","14f12c3e":"del train_csv","f451865d":"def tuning_params(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 1000, 11000, step=1000)\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n    \n    for train_, val_ in cv.kfold_split(folds):\n        trainX = train_[features]\n        trainY = train_[target]\n        valX = val_[features]\n        valY = val_[target]\n\n        model = xgb.XGBClassifier(\n            seed=1,\n            tree_method=\"gpu_hist\",\n            gpu_id=0,\n            predictor=\"gpu_predictor\",\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            reg_lambda=reg_lambda,\n            reg_alpha=reg_alpha,\n            subsample=subsample,\n            colsample_bytree=colsample_bytree,\n            max_depth=max_depth,\n            use_label_encoder=False\n        )\n        model.fit(trainX, trainY, \n                  early_stopping_rounds=300, \n                  eval_set=[(valX, valY)],\n                  eval_metric=\"auc\",\n                  verbose=1000\n                 )\n        \n        predY = model.predict(valX)\n        val_auc = metrics.roc_auc_score(valY, predY)\n        return val_auc","e25f1a50":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(tuning_params, n_trials=5, gc_after_trial=True)","ed5d191a":"print(study.best_params)","23a37073":"def tuning_fold_params(train_, val_, trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 1000, 11000, step=1000)\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n    \n    trainX = train_[features]\n    trainY = train_[target]\n    valX = val_[features]\n    valY = val_[target]\n\n    model = xgb.XGBClassifier(\n        seed=1,\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        predictor=\"gpu_predictor\",\n        n_estimators=n_estimators,\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth,\n        use_label_encoder=False\n    )\n    model.fit(trainX, trainY, \n              early_stopping_rounds=300, \n              eval_set=[(valX, valY)],\n              eval_metric=\"auc\",\n              verbose=1000\n             )\n\n    predY = model.predict(valX)\n    val_auc = metrics.roc_auc_score(valY, predY)\n    return val_auc","9e54604b":"best_params = []\nstudy = optuna.create_study(direction=\"maximize\")\nfor fold, (train_, val_) in enumerate(cv.kfold_split(folds)):\n    print(\"Trial Fold: \", fold+1)\n    trial_fn = partial(tuning_fold_params, train_, val_)\n    study.optimize(trial_fn, n_trials=5, gc_after_trial=True)\n    best_params.append(study.best_params)","27faa5e7":"for best in best_params:\n    print(best)","d033f219":"# Cross Validation","9c1aa28a":"### setup an optuna for one of the fold and get the best hyperparams, then repeat it for others","d681eacc":"## Installing Dependencies","0add8e78":"# Hyperparameter Tuning using Optuna","330f0ad0":"### Creating feature and target set","65c0c116":"### optuna for the whole training set and pickup the best hyperparams from all trials","626ee5d1":"# XGB HyperParameter Tuning using Optuna\n\nParams to Tune (from xgb docs)\n\n#### Some params to tune\n\n- booster [default= gbtree ]: \nWhich booster to use. Can be **gbtree**, **gblinear** or **dart**; gbtree and dart use tree based models while gblinear uses linear functions.\n\n- num_feature [set automatically by XGBoost, no need to be set by user]: \nFeature dimension used in boosting, set to maximum dimension of the feature.\n\n- eta [default=0.3, alias: learning_rate]\nStep size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative. range: [0,1]\n\n- gamma [default=0, alias: min_split_loss]\nMinimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. range: [0,\u221e]\n\n- max_depth [default=6]\nMaximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 is only accepted in lossguide growing policy when tree_method is set as hist or gpu_hist and it indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree. range: [0,\u221e]\n\n- min_child_weight [default=1]\nMinimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be. range: [0,\u221e]\n\n- max_delta_step [default=0]\nMaximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. range: [0,\u221e]\n\n- max_leaves [default=0]\nMaximum number of nodes to be added. Only relevant when grow_policy=lossguide is set.\n\n- max_bin, [default=256]\nOnly used if tree_method is set to hist or gpu_hist. Maximum number of discrete bins to bucket continuous features. Increasing this number improves the optimality of splits at the cost of higher computation time.\n\n- lambda [default=1, alias: reg_lambda]\nL2 regularization term on weights. Increasing this value will make model more conservative.\n\n- alpha [default=0, alias: reg_alpha]\nL1 regularization term on weights. Increasing this value will make model more conservative.\n\n- colsample_bytree, colsample_bylevel, colsample_bynode [default=1]\nThis is a family of parameters for subsampling of columns.\n\n    - All colsample_by* parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.\n\n    - colsample_bytree is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n\n    - colsample_bylevel is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\n\n    - colsample_bynode is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level.\n\n    - colsample_by* parameters work cumulatively. For instance, the combination {'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5} with 64 features will leave 8 features to choose from at each split."}}