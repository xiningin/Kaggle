{"cell_type":{"e1a93e70":"code","bd73f766":"code","9134b813":"code","34225d47":"code","c1c17141":"code","6930f588":"code","956b4082":"code","37edb79e":"code","dd189d06":"code","d6d243e9":"code","28a5b696":"code","121032d7":"code","b4b6f335":"code","a45d4978":"code","5460298f":"code","3b842d1d":"code","8fd810b3":"code","38801f54":"code","4f0e55e0":"code","6360c2a4":"code","72b6b0ca":"code","fb1d0270":"code","2d1accf2":"code","9ccaaef3":"code","e1018ac9":"code","6f0d9559":"code","e283d09b":"code","ef289b06":"code","a20e74ef":"code","38b8ee2c":"code","609acf5f":"code","81ec4a83":"code","23c2022f":"code","e1b69e6f":"code","52989b89":"code","705b716e":"code","7e8a9778":"code","17560ae7":"code","ac892f79":"code","054f281a":"code","e98c2196":"code","946ac13e":"code","6f9a2b3e":"code","67656800":"code","287cf1b1":"code","7dfa906e":"code","94da083c":"code","93be6357":"code","a289239e":"code","186b6208":"code","b71d5d8a":"code","f4c740b5":"code","95cb5c30":"code","55ff1ba4":"code","db4ea5bc":"code","0f766c34":"code","b396a4b8":"code","04d1b69f":"code","4eff7efe":"code","396178d8":"code","8133f2fa":"code","0fe26518":"code","43b95366":"code","abe46541":"code","2fbdb5ce":"code","76ddea87":"code","81ae096c":"code","9970081c":"markdown","f8908d08":"markdown","557be90b":"markdown","a1636b89":"markdown","9de7ae80":"markdown","db8593ff":"markdown","5b74a407":"markdown","02626210":"markdown","23937f68":"markdown","54569113":"markdown","6662ba81":"markdown","d9a089b8":"markdown","b8d0c917":"markdown","7adadd9a":"markdown","23f9b32c":"markdown","e3afd378":"markdown","eb88be0f":"markdown","7aca8e1c":"markdown","89fe3cbd":"markdown","455e24cf":"markdown","e0ce758d":"markdown","ea68fd4e":"markdown","61f1ce71":"markdown","54f359d5":"markdown","12f8f01f":"markdown","19bfe2c2":"markdown","ba869b0c":"markdown","94a2dd47":"markdown","23fe85b1":"markdown","7280cc12":"markdown","5f1f5be9":"markdown","f3d98cf1":"markdown","5cbac80d":"markdown","2f3ef9a4":"markdown","4751fe7f":"markdown","a9f9d56e":"markdown","14b45bc8":"markdown","2fc22718":"markdown","77821dae":"markdown","a79cc6c8":"markdown","4d1ed4bf":"markdown","79adda20":"markdown","cc09bc17":"markdown","ffdffdde":"markdown","46478d9a":"markdown","a0d40fd0":"markdown","a08e05e1":"markdown","d33cd9cc":"markdown","349da52f":"markdown","5de0c633":"markdown","975aa732":"markdown","947f0787":"markdown","04e08821":"markdown","bcb18869":"markdown","a0168bae":"markdown","019517ee":"markdown","7579069d":"markdown","03c704da":"markdown","e03cf190":"markdown","0740c2cf":"markdown","d3dcb5dc":"markdown","ee341cdc":"markdown","a41bd5a6":"markdown","3fe3aee1":"markdown","d8421039":"markdown","d4361cfd":"markdown","fe27f981":"markdown","9cd63ed6":"markdown","668db6f2":"markdown","4eb9e860":"markdown","3a14a8ac":"markdown","e3c44b8b":"markdown","19c2f9d7":"markdown","ef7cd103":"markdown","86244a04":"markdown","31cfd7ef":"markdown","bcfb54ba":"markdown","a52d05db":"markdown","9179fa44":"markdown","481fd42b":"markdown","e3a3237a":"markdown","7bb16c3b":"markdown","8df798d6":"markdown","457ec19a":"markdown","2ae34650":"markdown","a628373d":"markdown","cdc91b4f":"markdown","7e32e981":"markdown","9f73ce22":"markdown","930c0f4a":"markdown","1160c51f":"markdown","ac1823fa":"markdown","122a6f5e":"markdown","fe4a698a":"markdown"},"source":{"e1a93e70":"#Starting by resetting all variables previously defined in this jupyter notebook\n%reset -f","bd73f766":"import numpy as np\nimport seaborn as sns\nfrom scipy import stats, integrate\nimport matplotlib.pyplot as plt\nimport statistics\nimport math\nimport pandas as pd\npd.options.display.max_rows==1000\npd.options.display.max_columns==1000\nfrom datetime import datetime\nimport pandas_datareader.data as web","9134b813":"%%time\n#Computational time ~ 25.9 s\nUS_data = pd.read_csv('\/kaggle\/input\/us-accidents\/US_Accidents_Dec19.csv')\nprint(\"The shape of US_data is:\",(US_data.shape))\ndisplay(US_data.head(3))","34225d47":"%%time\n#Computational time ~ 12.3 s\n#-----------------------------\n#Function to summarize details in US accidents data\ndef summary_fun(US_data):\n    NAN_cnt=[] #Number of cells with NAN\n    prcnt_NAN=[] #\n    action=[]\n    remove_columns=[]\n    replace_columns=[]\n    uniq_cnt=[]\n    data_type=[]\n\n    Nd=US_data.shape[0]\n    for clmn in US_data.columns:\n\n        NAN_c=Nd-US_data[clmn].count()\n        prcnt_c=NAN_c\/Nd*100\n        uniq_c=US_data[clmn].nunique()\n        dtype_c=US_data[clmn].dtypes\n\n        NAN_cnt.append(NAN_c)\n        prcnt_NAN.append(prcnt_c)\n        uniq_cnt.append(uniq_c)\n        data_type.append(str(dtype_c))\n\n        if prcnt_c>0.0:\n            act=\"ReplaceNANs\"\n            replace_columns.append(clmn)\n        else:\n            act=\"None\"\n\n        if uniq_c==1:\n            act=\"Remove Column\"\n            remove_columns.append(clmn)\n\n        action.append(act)\n\n\n    data_details=pd.DataFrame({\"Data Type\":data_type,\n                           \"Unique count\":uniq_cnt,\n                           \"NAN Count\":NAN_cnt,\n                           \"Percent(NAN)\":prcnt_NAN,\n                           \"Action\":action},index=US_data.columns)\n    \n    return remove_columns,replace_columns,data_details\n#-----------------------\ndef highlight_remove(s):\n    '''\n    highlight the remove in a Series with red.\n    '''\n    #Ref: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/style.html\n    \n    if s==\"ReplaceNANs\":\n        color=\"green\"\n    elif s==\"Remove Column\":\n        color=\"red\"\n    else:\n        color=\"white\"\n  \n    return 'background-color: %s'% color\n#------------------------\ndef display_pandas_data(pandas_data):\n    display(pandas_data.style.applymap(highlight_remove))\n#----------------------------------------------------------------------------------------\nremove_columns,replace_columns,data_details=summary_fun(US_data)\n\nprint(\"This function will summarize the details of columns of accidents dataset like data type, unique count, percentage of NAN count and shows the action that needs to be done during EDA.\")\ndisplay_pandas_data(data_details)\n","c1c17141":"%%time\n#Computational time: 8.81 s\n#-----------------------------------------------------------------------------------------\n#Function to append remove_columns list\ndef remove_columns_append(remove_columns,columns):   \n    for clmn in columns:\n        remove_columns.append(clmn)\n    return remove_columns\n#----------------------------------------------------------------------------------------\n#Function to remove columns that are in remove_columns from pandas data\ndef fun_remove_columns(pandas_data,remove_columns):\n    for clmn in remove_columns:\n        if clmn in pandas_data.columns:\n            pandas_data.pop(clmn)\n    return pandas_data\n#----------------------------------------------------------------------------------------\n#Removing some columns\nremove_columns=remove_columns_append(remove_columns,[\"Description\",\"End_Lat\",\"End_Lng\",\"ID\"])\n\nprint(\"Removing columns:\",remove_columns)\nUS_data=fun_remove_columns(US_data,remove_columns)\n\nremove_columns,replace_columns,data_details=summary_fun(US_data)\n","6930f588":"%%time\n#Computational time: 9.42 s\n# Converting start_time, end_time to get year, month, day, hour and minute of a day and get duration \n#to clear accident from Start_time and End_time\n# Ref for pd.to_datetime :https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.to_datetime.html\nUS_data[\"Start_Time\"] = pd.to_datetime(US_data[\"Start_Time\"], errors='coerce')\nUS_data[\"End_Time\"] = pd.to_datetime(US_data[\"End_Time\"], errors='coerce')\n\n#Finding the number of year\nUS_data['Year']=US_data[\"Start_Time\"].dt.year\n\n#Finding the month\nnmonth=US_data[\"Start_Time\"].dt.month\nUS_data['Month']=nmonth\n\n#Finding the day of a year\ndays_each_month=np.cumsum(np.array([0,31,28,31,30,31,30,31,31,30,31,30,31]))\nnday=[days_each_month[arg-1] for arg in nmonth.values]\nnday=nday+US_data[\"Start_Time\"].dt.day.values\nUS_data['Day']=nday\n\n#Finding the weekday\nUS_data['Weekday']=US_data[\"Start_Time\"].dt.weekday\n\n#Finding the hour of day\nUS_data['Hour']=US_data[\"Start_Time\"].dt.hour\n\n#Finding the minute of the day\nUS_data['Minute']=US_data['Hour']*60.0+US_data[\"Start_Time\"].dt.minute\n\n# Ref for np.timedelta64: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/timedeltas.html\nUS_data['Duration']=(US_data[\"End_Time\"]-US_data[\"Start_Time\"])\/np.timedelta64(1,'m')","956b4082":"%%time\n#Removing columns\n#Computation time ~ 4 ms\nremove_columns=remove_columns_append(remove_columns,[\"Start_Time\",\"End_Time\"])\nUS_data=fun_remove_columns(US_data,remove_columns)","37edb79e":"%%time\n#Computational time: 1.79 s\n#Visualizing Severity\nseverity_vals=US_data[\"Severity\"].unique()\nprint(\"Seveirty values:\",severity_vals)\n\nseverity_count={}\nseverity_count[1]=US_data[US_data[\"Severity\"]==1].shape[0]\nseverity_count[2]=US_data[US_data[\"Severity\"]==2].shape[0]\nseverity_count[3]=US_data[US_data[\"Severity\"]==3].shape[0]\nseverity_count[4]=US_data[US_data[\"Severity\"]==4].shape[0]\n\nframe=pd.DataFrame(severity_count,index=[\"Severity\"])\ndisplay(frame)\n\nfig=plt.figure(figsize=(12,4))\nsns.barplot(list(severity_count.keys()),list(severity_count.values()))\nplt.xlabel(\"Severity\")\nplt.ylabel(\"Count\")\nplt.show()\n","dd189d06":"%%time\n#Computation time ~ 43 ms\n#Categorizing columns into numerical, boolian and categorical columns\nnum_columns_details=data_details.loc[data_details[\"Data Type\"].isin([\"int64\",\"float64\"])]\nbool_columns_details=data_details[data_details[\"Data Type\"].isin([\"bool\"])]\ncat_columns_details=data_details[data_details[\"Data Type\"]==\"object\"]\n\nprint(\"First visualizing categorical columns:\")\ndisplay_pandas_data(cat_columns_details)","d6d243e9":"%%time\n#Computation time ~ 1.19 s\nremove_columns=remove_columns_append(remove_columns,[\"Zipcode\",\"Airport_Code\",\"Weather_Timestamp\",\"Wind_Direction\"])  \nUS_data=fun_remove_columns(US_data,remove_columns)","28a5b696":"%%time\n#Computational time ~ 3.17 s\n#First visualizing source\n#------------------------------\n#Functiong to create a frame visualizing number of accidents with each severity for a given column name\ndef visualize_severity_detailed(US_data,column_name,decending_order=True):\n    \n    unique_types=np.sort(US_data[column_name].unique())\n    unique_details={\"Severity 1\":[],\"Severity 2\":[],\"Severity 3\":[],\"Severity 4\":[],\"Total\":[]}\n\n    for arg in unique_types:\n        dum=US_data[US_data[column_name]==arg]\n\n        unique_details[\"Total\"].append(dum.shape[0])\n        unique_details[\"Severity 1\"].append(dum[dum[\"Severity\"]==1].shape[0])\n        unique_details[\"Severity 2\"].append(dum[dum[\"Severity\"]==2].shape[0])\n        unique_details[\"Severity 3\"].append(dum[dum[\"Severity\"]==3].shape[0])\n        unique_details[\"Severity 4\"].append(dum[dum[\"Severity\"]==4].shape[0])\n\n    unique_details[\"Total\"]=np.asarray(unique_details[\"Total\"])\n    unique_details[\"Severity 1\"]=np.asarray(unique_details[\"Severity 1\"])\n    unique_details[\"Severity 2\"]=np.asarray(unique_details[\"Severity 2\"])\n    unique_details[\"Severity 3\"]=np.asarray(unique_details[\"Severity 3\"])\n    unique_details[\"Severity 4\"]=np.asarray(unique_details[\"Severity 4\"])\n    \n    if decending_order:\n\n        ind=np.argsort(unique_details['Total'])\n        ind=np.flip(ind)\n\n        unique_types=unique_types[list(ind)]\n        unique_details['Total']=unique_details['Total'][ind]\n        unique_details['Severity 1']=unique_details['Severity 1'][ind]\n        unique_details['Severity 2']=unique_details['Severity 2'][ind]\n        unique_details['Severity 3']=unique_details['Severity 3'][ind]\n        unique_details['Severity 4']=unique_details['Severity 4'][ind]    \n\n    frame=pd.DataFrame(unique_details,index=unique_types)\n    display(frame)\n    \n    return unique_types,unique_details\n#------------------------------\n#Visualizing source details\nsource_types,source_details=visualize_severity_detailed(US_data,column_name=\"Source\")\n\n#Barplot\nfig=plt.figure(figsize=(15,5))\nplt.subplot(121)\nsns.barplot(source_types,source_details[\"Severity 1\"],color=\"blue\")\nsns.barplot(source_types,source_details[\"Severity 2\"],bottom=source_details[\"Severity 1\"],color=\"orange\")\nsns.barplot(source_types,source_details[\"Severity 3\"],bottom=source_details[\"Severity 2\"]+source_details[\"Severity 1\"],color=\"green\")\nsns.barplot(source_types,source_details[\"Severity 4\"],bottom=source_details[\"Severity 3\"]+source_details[\"Severity 2\"]+source_details[\"Severity 1\"],color=\"red\")\nplt.title(\"Severity 1:blue 2:orange 3:green 4:red\")\nplt.xlabel(\"Source\")\nplt.ylabel(\"Count\")\n\n#Pie chart\nsizes = [arg\/sum(source_details[\"Total\"]) for arg in source_details[\"Total\"]]\nplt.subplot(122)\npatches, texts = plt.pie(sizes)\nplt.title(\"Total accidents reported\")\nplt.legend(patches, source_types, loc=\"best\")\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n","121032d7":"%%time\n#Computational time ~ 157 ms\n#Source only reports accidents but not affect severity\nremove_columns=remove_columns_append(remove_columns,[\"Source\"])\nUS_data=fun_remove_columns(US_data,remove_columns)","b4b6f335":"%%time\n#Computational time ~ 12.5\n#Unique state names\nstate_names=list(US_data[\"State\"].unique())\n\nstate_details={\"Total accidents\":[],\"Severity(%) 1\":[],\"Severity(%) 2\":[],\n               \"Severity(%) 3\":[],\"Severity(%) 4\":[]}\n\nfor state in state_names:\n    dum=US_data[US_data[\"State\"]==state]\n    tot_acci=dum.shape[0]\n    \n    state_details[\"Total accidents\"].append(tot_acci)\n    \n    sev_cnt=dum[dum[\"Severity\"]==1].shape[0]        \n    state_details[\"Severity(%) 1\"].append(sev_cnt\/tot_acci*100)\n\n    sev_cnt=dum[dum[\"Severity\"]==2].shape[0]        \n    state_details[\"Severity(%) 2\"].append(sev_cnt\/tot_acci*100)\n    \n    sev_cnt=dum[dum[\"Severity\"]==3].shape[0]        \n    state_details[\"Severity(%) 3\"].append(sev_cnt\/tot_acci*100)\n\n    sev_cnt=dum[dum[\"Severity\"]==4].shape[0]        \n    state_details[\"Severity(%) 4\"].append(sev_cnt\/tot_acci*100)\n\nprint(\"Frame listing total number of accidents in each sate and percentage of accidents with different severity level\")\nframe=pd.DataFrame(state_details,index=state_names)\ndisplay(frame)\n","a45d4978":"%%time\n#COmputational time ~ 2.99 s\nfig=plt.figure(figsize=(15,4))\nsns.barplot(state_names,state_details[\"Total accidents\"])\nplt.xlabel(\"State\")\nmu=np.mean(state_details[\"Total accidents\"])\nplt.title(\"Total accidents, Mean:\"+str(mu))\nplt.show()\n\nmu=np.mean(state_details[\"Severity(%) 1\"])\nfig=plt.figure(figsize=(15,4))\nsns.barplot(state_names,state_details[\"Severity(%) 1\"])\nplt.xlabel(\"State\")\nplt.title(\"Severity(%) 1, Mean:\"+str(mu))\nplt.show()\n\nfig=plt.figure(figsize=(15,4))\nsns.barplot(state_names,state_details[\"Severity(%) 2\"])\nplt.xlabel(\"State\")\nmu=np.mean(state_details[\"Severity(%) 2\"])\nplt.title(\"Severity(%) 2, Mean:\"+str(mu))\nplt.show()\n\nfig=plt.figure(figsize=(15,4))\nsns.barplot(state_names,state_details[\"Severity(%) 3\"])\nplt.xlabel(\"State\")\nmu=np.mean(state_details[\"Severity(%) 3\"])\nplt.title(\"Severity(%) 3, Mean:\"+str(mu))\nplt.show()\n\nfig=plt.figure(figsize=(15,4))\nsns.barplot(state_names,state_details[\"Severity(%) 4\"])\nplt.xlabel(\"State\")\nmu=np.mean(state_details[\"Severity(%) 4\"])\nplt.title(\"Severity(%) 4, Mean:\"+str(mu))\nplt.show()\n\nprint(\"Sum of means of all sverity: 99.97 %\")","5460298f":"%%time\n#Computational time ~ 1 min 37 sec\n#Plotting latitudes and longitude (Takes long time)\nplt.figure(figsize=(12, 6))\n# (Scatter plot is Taking long time due to color(c))\nplt.scatter(US_data[\"Start_Lng\"],US_data[\"Start_Lat\"],s=8,c=US_data[\"Severity\"])\nplt.colorbar()\n# plt.plot(US_data[\"Start_Lng\"],US_data[\"Start_Lat\"],'o',markersize=2)\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.title(\"US accidents map (Severity)\")\nplt.show()","3b842d1d":"%%time\n#Computational time ~ 1 min 38 s\n#Plotting Time zones (Takes long time)\nTimeZone=US_data[[\"Start_Lng\",\"Start_Lat\",\"Timezone\"]]\nTimeZone=TimeZone.dropna()\nTimeZone_vals=list(TimeZone[\"Timezone\"].unique())\n\n#Labeling the time zones to plot\nlabels={}\nflag=1\nfor arg in TimeZone_vals:\n    labels[arg]=flag\n    flag=flag+1\nprint(labels)\ncolorbars=[labels[arg] for arg in TimeZone[\"Timezone\"]]\n\n#Plotting the Latidue and longitude with time zones\nplt.figure(figsize=(12, 6))\n# (Scatter plot is Taking long time due to color(c))\nplt.scatter(TimeZone[\"Start_Lng\"],TimeZone[\"Start_Lat\"],s=8,c=colorbars)\nplt.colorbar()\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.title(\"US accidents map (Time Zones)\")\nplt.show()\n","8fd810b3":"%%time\n#Computation time ~ 0 ns\nremove_columns=remove_columns_append(remove_columns,[\"Timezone\"])\nUS_data=fun_remove_columns(US_data,remove_columns)","38801f54":"%%time\n#Computation time ~ 771 ms\n#Accidents in top7 and in \"Norfolk\",\"Virginia Beach\",\"Hampton\"\ncities=US_data[\"City\"].unique()\n\ndum=US_data[\"City\"].value_counts().sort_values(ascending=False)\nint_cities=list(dum[:7].index)\nnacci_cities=list(dum[:7].values)\ncity_rank=list(np.arange(1,8))\n\nint_cities.append(\"Norfolk\"); \nnacci_cities.append(dum[\"Norfolk\"])\ncity_rank.append(list(dum.index).index(\"Norfolk\")+1)\n\nint_cities.append(\"Virginia Beach\"); \nnacci_cities.append(dum[\"Virginia Beach\"])\ncity_rank.append(list(dum.index).index(\"Virginia Beach\")+1)\n\nint_cities.append(\"Hampton\");\nnacci_cities.append(dum[\"Hampton\"])\ncity_rank.append(list(dum.index).index(\"Hampton\"))\n\nframe=pd.DataFrame({\"Number\":nacci_cities,\"City rank\":city_rank},index=int_cities)\ndisplay(frame)\n\nfig=plt.figure(figsize=(15,3))\nsns.barplot(int_cities,nacci_cities)\nplt.xlabel(\"Cities\")\nplt.ylabel(\"Count\")\nplt.show()","4f0e55e0":"%%time\n#Computational time ~ 286 ms\n#Visualizing accidients in counties around \"Norfolk\",\"Virginia Beach\",\"Hampton\"\nint_cities=[\"Norfolk\",\"Virginia Beach\",\"Hampton\"]\nVA_accid=US_data.loc[US_data[\"State\"].isin([\"VA\"])]\nlocal_accid=VA_accid.loc[VA_accid[\"City\"].isin(int_cities)]\nVA_county=local_accid[\"County\"].value_counts().sort_values(ascending=False)\n\nframe=pd.DataFrame(VA_county)\ndisplay(frame)\n\nfig=plt.figure(figsize=(15,3))\nsns.barplot(VA_county.index,VA_county.values)\nplt.xlabel(\"County\")\nplt.ylabel(\"Count\")\nplt.show()\n","6360c2a4":"%%time\n#COmputation time ~\n#Accidents in Norfolk streets\nint_cities=[\"Norfolk\"]\nVA_accid=US_data.loc[US_data[\"State\"].isin([\"VA\"])]\nlocal_accid=VA_accid.loc[VA_accid[\"City\"].isin(int_cities)]\n\nNorfolk_street=local_accid[\"Street\"].value_counts().sort_values(ascending=False)\nprint(\"Total number of streets:\",len(Norfolk_street.unique()))\n\nprint(\"Displaying for top 10 streets\")\nNorfolk_street=Norfolk_street[:10]\nframe=pd.DataFrame(Norfolk_street)\ndisplay(frame)","72b6b0ca":"%%time\n#Computation time ~ 301 ms\nremove_columns=remove_columns_append(remove_columns,[\"County\",\"Street\"])\nUS_data=fun_remove_columns(US_data,remove_columns)\n","fb1d0270":"%%time\n#Computation time ~ 3.29 s\nside_types=US_data[\"Side\"].unique()\nprint(\"Unique side types in original data:\",side_types)\n\nUS_data=US_data[~US_data['Side'].isin([' '])]\nside_types=US_data[\"Side\"].unique()\nprint(\"Side types in original data after removing null values:\",side_types)\n\nside_types,side_details=visualize_severity_detailed(US_data,column_name=\"Side\")\n\nfig=plt.figure(figsize=(10,6))\nplt.subplot(121)\nsns.barplot(side_types,side_details[\"Severity 1\"],color=\"blue\")\nsns.barplot(side_types,side_details[\"Severity 2\"],bottom=side_details[\"Severity 1\"],color=\"orange\")\nsns.barplot(side_types,side_details[\"Severity 3\"],bottom=side_details[\"Severity 2\"]+side_details[\"Severity 1\"],color=\"green\")\nsns.barplot(side_types,side_details[\"Severity 4\"],bottom=side_details[\"Severity 3\"]+side_details[\"Severity 2\"]+side_details[\"Severity 1\"],color=\"red\")\nplt.title(\"Severity 1:blue 2:orange 3:green 4:red\")\nplt.xlabel(\"Side\")\nplt.ylabel(\"Count\")\n\nsizes = [arg\/sum(side_details[\"Total\"]) for arg in side_details[\"Total\"]]\nplt.subplot(122)\npatches, texts = plt.pie(sizes)\nplt.legend(patches, side_types, loc=\"best\")\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n","2d1accf2":"%%time\n#Computational time ~ 1.42 s\n#Side might affect the severity. So it will be used for the prediction of severity\n#Side will be labelled to Right:1 or Left:2\nside_types=US_data[\"Side\"].unique()\nlabels={}\nflag=1\nfor arg in side_types:\n    labels[arg]=flag\n    flag=flag+1\nprint(labels)\n\nlabel_vals=[labels[arg] for arg in US_data[\"Side\"]]\nUS_data[\"Side\"]=label_vals\n","9ccaaef3":"%%time\n#Coputation time ~ 163 ms\n#Filling NAN values in weather_condition by clear\nUS_data[\"Weather_Condition\"]=US_data[\"Weather_Condition\"].fillna('Clear')\n","e1018ac9":"%%time\n#Coputation time ~ 24.3 s\nweather_types,weather_details=visualize_severity_detailed(US_data,column_name=\"Weather_Condition\")\nntop=10\nfig=plt.figure(figsize=(15,6))\nsns.barplot(weather_types[:ntop],weather_details[\"Severity 1\"][:ntop],color=\"blue\")\nsns.barplot(weather_types[:ntop],weather_details[\"Severity 2\"][:ntop],bottom=weather_details[\"Severity 1\"][:ntop],color=\"orange\")\nsns.barplot(weather_types[:ntop],weather_details[\"Severity 3\"][:ntop],bottom=weather_details[\"Severity 2\"][:ntop]+weather_details[\"Severity 1\"][:ntop],color=\"green\")\nsns.barplot(weather_types[:ntop],weather_details[\"Severity 4\"][:ntop],bottom=weather_details[\"Severity 3\"][:ntop]+weather_details[\"Severity 2\"][:ntop]+weather_details[\"Severity 1\"][:ntop],color=\"red\")\nplt.title(\"Severity 1:blue 2:orange 3:green 4:red\")\nplt.xlabel(\"Weather Condition\")\nplt.ylabel(\"Count\")\nplt.show()","6f0d9559":"%%time\n#COmputation time ~ 1.4 s\n#Labelling weather_types\nlabels={}\nflag=1\nfor arg in weather_types:\n    labels[arg]=flag\n    flag=flag+1\n\nlabel_vals=[labels[arg] for arg in US_data[\"Weather_Condition\"]]\nUS_data[\"Weather_Condition\"]=label_vals\n","e283d09b":"%%time\n#Computationa time ~ 12 s\nprint(\"Sunrise_Sunset Details\")\nntop=US_data[\"Sunrise_Sunset\"].nunique()\nUS_data[\"Sunrise_Sunset\"]=US_data[\"Sunrise_Sunset\"].fillna('Day')\nsunrise_types,sunrise_details=visualize_severity_detailed(US_data,column_name=\"Sunrise_Sunset\")\n\n#COmputational time ~ 125 ms\ndef barplot_customized(data_types,data_details,ntop,xlabel_val):\n\n    sns.barplot(data_types[:ntop],data_details[\"Severity 1\"][:ntop],\n                color=\"blue\",order=data_types[:ntop])    \n    sns.barplot(data_types[:ntop],data_details[\"Severity 2\"][:ntop],\n                bottom=data_details[\"Severity 1\"][:ntop],color=\"orange\",order=data_types[:ntop])    \n    sns.barplot(data_types[:ntop],data_details[\"Severity 3\"][:ntop],\n                bottom=data_details[\"Severity 2\"][:ntop]+data_details[\"Severity 1\"][:ntop],color=\"green\",order=data_types[:ntop])\n    sns.barplot(data_types[:ntop],data_details[\"Severity 4\"][:ntop],\n                bottom=data_details[\"Severity 3\"][:ntop]+data_details[\"Severity 2\"][:ntop]+data_details[\"Severity 1\"][:ntop],color=\"red\",order=data_types[:ntop])\n    plt.title(\"Severity 1:blue 2:orange 3:green 4:red\")\n    plt.xlabel(xlabel_val)\n    plt.ylabel(\"Count\")    \n    \n    \nfig=plt.figure(figsize=(10,6))\nntop=len(sunrise_types)\nbarplot_customized(sunrise_types,sunrise_details,ntop,\"Sunrise_Sunset\")\nplt.show()","ef289b06":"%%time\n#COmputational time ~ \nlabels={}\nflag=1\nfor arg in sunrise_types:\n    labels[arg]=flag\n    flag=flag+1\nprint(labels)\n\nlabel_vals=[labels[arg] for arg in US_data[\"Sunrise_Sunset\"]]\nUS_data[\"Sunrise_Sunset\"]=label_vals\n\n#Appending \"Civil_Twilight\",\"Nautical_Twilight\",\"Astronomical_Twilight\" to remove columns\nremove_columns=remove_columns_append(remove_columns,\n                                     [\"Civil_Twilight\",\"Nautical_Twilight\",\"Astronomical_Twilight\"])\nUS_data=fun_remove_columns(US_data,remove_columns)","a20e74ef":"%%time\n#Computational time ~ 1.35 s\nunique_types=bool_columns_details.index\nunique_details={\"Severity 1\":[],\"Severity 2\":[],\"Severity 3\":[],\n                \"Severity 4\":[],\"Total\":[],\"Accidents(%)\":[]}\n\nNd=US_data.shape[0]\nfor arg in unique_types:\n    dum=US_data[US_data[arg]]\n\n    unique_details[\"Total\"].append(dum.shape[0])\n    unique_details[\"Severity 1\"].append(dum[dum[\"Severity\"]==1].shape[0])\n    unique_details[\"Severity 2\"].append(dum[dum[\"Severity\"]==2].shape[0])\n    unique_details[\"Severity 3\"].append(dum[dum[\"Severity\"]==3].shape[0])\n    unique_details[\"Severity 4\"].append(dum[dum[\"Severity\"]==4].shape[0])\n    unique_details[\"Accidents(%)\"].append(unique_details[\"Total\"][-1]\/Nd*100)\n\nunique_details[\"Total\"]=np.asarray(unique_details[\"Total\"])\nunique_details[\"Severity 1\"]=np.asarray(unique_details[\"Severity 1\"])\nunique_details[\"Severity 2\"]=np.asarray(unique_details[\"Severity 2\"])\nunique_details[\"Severity 3\"]=np.asarray(unique_details[\"Severity 3\"])\nunique_details[\"Severity 4\"]=np.asarray(unique_details[\"Severity 4\"])\nunique_details[\"Accidents(%)\"]=np.asarray(unique_details[\"Accidents(%)\"])\n\nind=np.argsort(unique_details['Total'])\nind=np.flip(ind)\n\nunique_types=unique_types[list(ind)]\nunique_details['Total']=unique_details['Total'][ind]\nunique_details['Severity 1']=unique_details['Severity 1'][ind]\nunique_details['Severity 2']=unique_details['Severity 2'][ind]\nunique_details['Severity 3']=unique_details['Severity 3'][ind]\nunique_details['Severity 4']=unique_details['Severity 4'][ind]    \nunique_details['Accidents(%)']=unique_details['Accidents(%)'][ind]\n\nframe=pd.DataFrame(unique_details,index=unique_types)\ndisplay(frame)\n\nntop=len(unique_types)\nfig=plt.figure(figsize=(15,6))\nbarplot_customized(unique_types,unique_details,ntop,\"Location\")\nplt.show()","38b8ee2c":"%%time\n#Computation time ~ 6.55 s\nfor clmn in bool_columns_details.index:\n    label_vals=[float(arg) for arg in US_data[clmn]]\n    US_data[clmn]=label_vals\n\nremove_columns=remove_columns_append(remove_columns\n                                     ,[\"Give_Way\",\"No_Exit\",\"Traffic_Calming\",\"Bump\",\"Roundabout\",\"Number\"])\nUS_data=fun_remove_columns(US_data,remove_columns)\n","609acf5f":"%%time\n#COmputation time ~ 1.9 s\nTMC_codes=[201,241,245,229,203]\nTMC_names={\"Traffic Message Channel (TMC) Description\":[\"Accident(s)\",\"Accident(s). Right lane blocked\",\"(Q) accident(s). Two lanes blocked\",\n           \"(Q) accident(s). Slow traffic\",\"multi-vehicle accident\"]}\n\nframe=pd.DataFrame(TMC_names,TMC_codes)\ndisplay(frame)\n\nTMC_types,TMC_details=visualize_severity_detailed(US_data,column_name=\"TMC\")","81ec4a83":"%%time\n#Computational time ~ 310 ms\nntop=3\nfig=plt.figure(figsize=(10,4))\nbarplot_customized(TMC_types,TMC_details,ntop,\"TMC\")\nplt.show()","23c2022f":"%%time\n#Computation time ~ 2.02 s\nUS_data[\"TMC\"]=US_data[\"TMC\"].fillna(201)\n\nlabels={}\nflag=1\nfor arg in TMC_types:\n    labels[arg]=flag\n    flag=flag+1\n\nlabel_vals=[labels[arg] for arg in US_data[\"TMC\"]]\nUS_data[\"TMC\"]=label_vals","e1b69e6f":"%%time\n# COmputation time ~ 2.8 s\ndesc_distance=US_data[\"Distance(mi)\"].describe([.25,.50,.75,.80,.90,.95,0.975,0.99])\ndisplay(desc_distance)\n\nfig=plt.figure(figsize=(10,4))\nsns.boxplot(x=\"Severity\", y=\"Distance(mi)\", data=US_data)\nplt.ylabel('Distance(mi)', fontsize=12)\nplt.xlabel('Severity', fontsize=12)\nplt.title(\"Box plot\")\nplt.show()","52989b89":"remove_columns=remove_columns_append(remove_columns,[\"Distance(mi)\"])\nUS_data=fun_remove_columns(US_data,remove_columns)","705b716e":"%%time\n#Computational time ~\nfig=plt.figure(figsize=(12,6))\nplt.subplot(121)\nsns.boxplot(x=\"Severity\", y=\"Temperature(F)\", data=US_data)\nplt.ylabel('Temperature(F)', fontsize=12)\nplt.xlabel('Severity', fontsize=12)\nplt.title(\"Box plot\")\n\nplt.subplot(122)\nsns.violinplot(x='Severity', y='Temperature(F)', data=US_data)\nplt.xlabel('Severity', fontsize=12)\nplt.show()\n","7e8a9778":"%%time\n#COmputational time ~ \nremove_columns=remove_columns_append(remove_columns,[\"Wind_Chill(F)\"])\nUS_data=fun_remove_columns(US_data,remove_columns)\n","17560ae7":"%%time\n#COmputationa time ~ \ndisplay(US_data[[\"Humidity(%)\",\"Pressure(in)\",\"Wind_Speed(mph)\",\"Precipitation(in)\"]].describe())","ac892f79":"%%time\n#COmputational time ~ \nremove_columns=remove_columns_append(remove_columns,[\"Pressure(in)\",\"Precipitation(in)\"])\nUS_data=fun_remove_columns(US_data,remove_columns)\n","054f281a":"%%time\n#Computational time ~\n#Plotting humidity\ndisplay(US_data[\"Humidity(%)\"].describe([.25,.50,.75,.80,.90,.95,0.975,0.99]))\n\nfig=plt.figure(figsize=(12,6))\nplt.subplot(121)\nsns.boxplot(x=\"Severity\", y=\"Humidity(%)\", data=US_data)\nplt.ylabel('Humidity(%)', fontsize=12)\nplt.xlabel('Severity', fontsize=12)\nplt.title(\"Box plot\")\n\nplt.subplot(122)\nsns.violinplot(x='Severity', y='Humidity(%)', data=US_data)\nplt.xlabel('Severity', fontsize=12)\nplt.ylabel('Humidity(%)', fontsize=12)\nplt.show()","e98c2196":"%%time\n#Computational time ~ 22 s\ndesc=US_data[\"Wind_Speed(mph)\"].describe(percentiles=[.25,.50,.75,.80,.90,.95,0.975,0.99])\ndisplay(desc)\n\nfig=plt.figure(figsize=(12,4))\nplt.subplot(121)\nsns.boxplot(x=\"Severity\", y=\"Wind_Speed(mph)\", data=US_data)\nplt.xlabel('Severity', fontsize=12)\nplt.ylabel('Wind_Speed(mph)', fontsize=12)\nplt.title(\"Box plot\")\n\nplt.subplot(122)\nsns.violinplot(x='Severity', y='Wind_Speed(mph)', data=US_data)\nplt.xlabel('Severity', fontsize=12)\nplt.ylabel('Wind_Speed(mph)', fontsize=12)\nplt.show()\n","946ac13e":"%%time\n#Computational time ~\nUS_data.loc[US_data[\"Wind_Speed(mph)\"]>desc[\"99%\"], 'Wind_Speed(mph)']=np.nan","6f9a2b3e":"%%time\n#Computational time ~ \n\nfig=plt.figure(figsize=(12,6))\nplt.subplot(121)\nsns.boxplot(x=\"Severity\", y=\"Wind_Speed(mph)\", data=US_data)\nplt.xlabel('Severity', fontsize=12)\nplt.ylabel('Wind_Speed(mph)', fontsize=12)\nplt.title(\"Box plot\")\n\nplt.subplot(122)\nsns.violinplot(x='Severity', y='Wind_Speed(mph)', data=US_data)\nplt.xlabel('Severity', fontsize=12)\nplt.ylabel('Wind_Speed(mph)', fontsize=12)\nplt.show()","67656800":"%%time\ndisplay(US_data[\"Visibility(mi)\"].describe([.25,.50,.75,.80,.90,.95,0.975,0.99]))\n\nfig=plt.figure(figsize=(12,6))\nplt.subplot(121)\nsns.boxplot(x=\"Severity\", y=\"Visibility(mi)\", data=US_data)\nplt.ylabel('Visibility(mi)', fontsize=12)\nplt.xlabel('Severity', fontsize=12)\nplt.title(\"Box plot\")\n\nplt.subplot(122)\nsns.violinplot(x='Severity', y='Visibility(mi)', data=US_data)\nplt.xlabel('Severity', fontsize=12)\nplt.ylabel('Visibility(mi)', fontsize=12)\nplt.show()","287cf1b1":"%%time\nremove_columns=remove_columns_append(remove_columns,[\"Visibility(mi)\"])\nUS_data=fun_remove_columns(US_data,remove_columns)","7dfa906e":"%%time\n#Computational time ~ \nyear_types,year_details=visualize_severity_detailed(US_data,column_name=\"Year\",decending_order=False)\n\nntop=len(year_types)\nfig=plt.figure(figsize=(10,4))\nbarplot_customized(year_types,year_details,ntop,\"Year\")\nplt.show()","94da083c":"%%time\n#COmputational time ~\n#Removing Year\nremove_columns=remove_columns_append(remove_columns,[\"Year\"])\nUS_data=fun_remove_columns(US_data,remove_columns)\n\nmonth_types,month_details=visualize_severity_detailed(US_data,column_name=\"Month\",decending_order=False)\nmonth_names=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\nntop=len(month_names)\nfig=plt.figure(figsize=(10,4))\nbarplot_customized(month_names,month_details,ntop,\"Month\")\nplt.show()","93be6357":"%%time\nday_types,day_details=visualize_severity_detailed(US_data,column_name=\"Day\",decending_order=False)\n\nfig=plt.figure(figsize=(15,6))\nplt.plot(day_types,day_details[\"Severity 1\"],color='b')\nplt.plot(day_types,day_details[\"Severity 2\"],color='m')\nplt.plot(day_types,day_details[\"Severity 3\"],color='g')\nplt.plot(day_types,day_details[\"Severity 4\"],color='r')\nplt.plot(day_types,day_details[\"Total\"],color='k')\nplt.xlabel(\"Day\")\nplt.ylabel(\"Count\")\nplt.legend([\"Severity 1\",\"Severity 2\",\"Severity 3\",\"Severity 4\",\"Total\"])\nplt.show()","a289239e":"%%time\n#COmputational time ~ \nweek_types,week_details=visualize_severity_detailed(US_data,column_name=\"Weekday\",decending_order=False)\nweek_names=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n\nntop=len(week_types)\nfig=plt.figure(figsize=(10,4))\nbarplot_customized(week_names,week_details,ntop,\"Weekday\")\nplt.show()","186b6208":"%%time\n#COmputational time ~ \nhour_types,hour_details=visualize_severity_detailed(US_data,column_name=\"Hour\",decending_order=False)\n\nntop=len(hour_types)\nfig=plt.figure(figsize=(15,4))\nbarplot_customized(hour_types,hour_details,ntop,\"Hour\")\nplt.show()","b71d5d8a":"%%time\n#COmputational time ~ \n\n#Plotting minutes\nminute_types,minute_details=visualize_severity_detailed(US_data,column_name=\"Minute\",decending_order=False)\n\nfig=plt.figure(figsize=(15,6))\nplt.plot(minute_types,minute_details[\"Severity 1\"],color='b')\nplt.plot(minute_types,minute_details[\"Severity 2\"],color='m')\nplt.plot(minute_types,minute_details[\"Severity 3\"],color='g')\nplt.plot(minute_types,minute_details[\"Severity 4\"],color='r')\nplt.plot(minute_types,minute_details[\"Total\"],color='k')\n\nplt.xlabel(\"Minute\")\nplt.ylabel(\"Count\")\nplt.legend([\"Severity 1\",\"Severity 2\",\"Severity 3\",\"Severity 4\",\"Total\"])\nplt.show()","f4c740b5":"%%time\ndesc=US_data[\"Duration\"].describe([.25,.50,.75,.80,.90,.95,0.975,0.99])\n\nprint(desc)","95cb5c30":"%%time\nnmeans=4\nfig=plt.figure(figsize=(12,6))\nplt.subplot(121)\nsns.boxplot(x=\"Severity\",y=\"Duration\",data=US_data[US_data[\"Duration\"]<=desc[\"99%\"]])\nplt.ylabel('Duration(min)', fontsize=12)\nplt.xlabel('Severity', fontsize=12)\nplt.title(\"Box plot\")\n\nplt.subplot(122)\nsns.violinplot(x=\"Severity\",y=\"Duration\",data=US_data[US_data[\"Duration\"]<=desc[\"99%\"]])\nplt.xlabel('Severity', fontsize=12)\nplt.show()","55ff1ba4":"%%time\n\nUS_data.loc[US_data[\"Duration\"]>desc[\"99%\"], 'Duration']=np.nan\nUS_data.loc[US_data[\"Duration\"]<0, 'Duration']=np.nan\n\n#Appending Year to remove columns\nremove_columns=remove_columns_append(remove_columns,[\"Month\",\"Year\"])\nUS_data=fun_remove_columns(US_data,remove_columns)\n\nnum_cols=US_data.select_dtypes(include=['float64', 'int64']).columns.values \nUS_data[num_cols]=US_data[num_cols].fillna(US_data.median())\n\nprint(\"Removing columns:\",remove_columns)\nUS_data=fun_remove_columns(US_data,remove_columns)\nprint(\"US_data details after removing above columns\")\nremove_columns,replace_columns,data_details=summary_fun(US_data)\ndisplay_pandas_data(data_details)","db4ea5bc":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","0f766c34":"%%time\n#Computational time ~ 5.28 s\nreq_data=US_data[US_data[\"State\"]==\"CA\"]\n\n# display(req_data)\ndum=req_data[\"City\"].value_counts().sort_values(ascending=False)\n\nreq_cities=['San Francisco']\n\nreq_cities=['Alameda','Albany','American','Antioch','Atherton','Belmont','San Mateo',\n            'Belvedere','Benicia','Berkeley','Brentwood','Brisbane','Burlingame',\n            'Calistoga','Campbell','Clayton','Cloverdale','Colma','Concord',\n            'Corte Madera','Cotati','Cupertino','Daly City','Danville','Dixon\tCity',\n            'Dublin','East Palo Alto','El Cerrito','Emeryville','Fairfax','Fairfield',\n            'Foster City','Fremont','Gilroy','Half Moon Bay','Hayward','Healdsburg',\n            'Hercules','Hillsborough','Lafayette','Larkspur','Livermore','Los Altos',\n            'Los Altos Hills','Los Gatos','Martinez','Menlo Park','Mill Valley',\n            'Millbrae','Milpitas','Monte Sereno','Moraga','Morgan Hill','Mountain View',\n            'Napa','Newark','Novato','Oakland','Oakley','Orinda','Pacifica','Palo Alto',\n            'Petaluma','Piedmont','Pinole','Pittsburg','Pleasant Hill','Pleasanton',\n            'Portola Valley','Redwood','Richmond','Rio Vista','Rohnert Park','Ross',\n            'St. Helena','San Anselmo','San Bruno','San Carlos','San Francisco',\n            'San Jose','San Leandro','San Mateo','San Pablo','San Rafael','San Ramon',\n            'Santa Clara','Santa Rosa','Saratoga','Sausalito','Sebastopol','Sonoma',\n            'South San Francisco','Suisun City','Sunnyvale','Tiburon','Union City',\n            'Vacaville','Vallejo','Walnut Creek','Windsor','Woodside','Yountville']\n\nreq_data2=req_data.loc[req_data[\"City\"].isin(req_cities)]\n\nseverity_count={}\nseverity_count[1]=req_data2[req_data2[\"Severity\"]==1].shape[0]\nseverity_count[2]=req_data2[req_data2[\"Severity\"]==2].shape[0]\nseverity_count[3]=req_data2[req_data2[\"Severity\"]==3].shape[0]\nseverity_count[4]=req_data2[req_data2[\"Severity\"]==4].shape[0]\n\nframe=pd.DataFrame(severity_count,index=[\"Severity\"])\nprint(\"Severity in california bay area\")\ndisplay(frame)\n\nplt.figure(figsize=(12, 6))\nplt.scatter(req_data2[\"Start_Lng\"],req_data2[\"Start_Lat\"],s=8,c=req_data2[\"Severity\"])\nplt.colorbar()\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.title(\"US accidents map (Severity)\")\nplt.show()\n\n","b396a4b8":"%%time\n#COmputational time ~\ncorr=req_data2.corr()\n\nf, ax = plt.subplots(figsize=(20, 10))\ng=sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=False, ax=ax, annot=True, vmax=1.0, vmin=-1.0, cbar_kws={\"shrink\": 0.9})\ng.set_yticklabels(g.get_yticklabels(), rotation = 0)\nplt.plot()","04d1b69f":"%%time\n#Computational time ~\n#selecting input columns and output columns for training\n\nx_clmns=list(US_data.columns)\nx_clmns.remove(\"Severity\")\nx_clmns.remove(\"City\")\nx_clmns.remove(\"State\")\ny_clmns=[\"Severity\"]\n\nX=req_data2[x_clmns].values\ny=req_data2[y_clmns].values\n\ny=np.reshape(y,(-1,1))\ny=y[:,]\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X,y[:,0], test_size=0.1,random_state=21)\n\nprint(\"Train size:\",X_train.shape[0])\nprint(\"Test size:\",X_test.shape[0])\n\nseverity_count_y_test={}\nseverity_count_y_test[1]=y_test[y_test==1].shape[0]\nseverity_count_y_test[2]=y_test[y_test==2].shape[0]\nseverity_count_y_test[3]=y_test[y_test==3].shape[0]\nseverity_count_y_test[4]=y_test[y_test==4].shape[0]\n\nframe=pd.DataFrame(severity_count_y_test,index=[\"Severity count in test_data\"])\ndisplay(frame)","4eff7efe":"%%time\n#COmputationa time ~ 2 min 47 seconds\nlr = LogisticRegression(random_state=0,solver='saga',max_iter=1000)\nlr.fit(X_train,y_train)\n\ny_pred=lr.predict(X_train)\nacc_train_lr=accuracy_score(y_train, y_pred)\nprint(\"Accuracy of train data:\",acc_train_lr)\n\ny_pred=lr.predict(X_test)\nacc_test_lr=accuracy_score(y_test, y_pred)\nprint(\"Accuracy of test data:\",acc_test_lr)\n\nmat_lr = confusion_matrix(y_pred,y_test)\nsns.heatmap(mat_lr, square=True, annot=True, fmt='d', cbar=False,xticklabels=[1,2,3,4],yticklabels=[1,2,3,4])\nplt.xlabel('true label')\nplt.ylabel('predicted label')\nplt.title(\"Confusion matrix\")\nplt.show()\n\nframe=pd.DataFrame(severity_count_y_test,index=[\"Severity.count in test_data\"])\ndisplay(frame)\n","396178d8":"%%time\n# COmputational time ~\ntrain_accuracy=[]\ntest_accuracy=[]\nk_array=range(1,10)\n\nfor i in k_array:\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n\n    y_pred = knn.predict(X_train)\n    acc=accuracy_score(y_train, y_pred)\n    train_accuracy.append(acc)\n\n    y_pred = knn.predict(X_test)\n    acc=accuracy_score(y_test, y_pred)\n    test_accuracy.append(acc)\n\n    print(\"k=\",i,\" Train Accuracy:\",train_accuracy[-1],\"Test accuracy:\",test_accuracy[-1])\n\nplt.figure(figsize=(10,4))\nplt.plot(k_array,train_accuracy)\nplt.plot(k_array,test_accuracy)\nplt.legend([\"Train\",\"Test\"])\nplt.title(\"Accuracy score\")\nplt.show()\n\nframe=pd.DataFrame(severity_count_y_test,index=[\"Severity.count in test_data\"])\ndisplay(frame)","8133f2fa":"%%time\n#COmputational time ~ \nknn = KNeighborsClassifier(n_neighbors=4)\nknn.fit(X_train,y_train)\n\ny_pred = knn.predict(X_train)\nacc_train_knn=accuracy_score(y_train, y_pred)\n\ny_pred = knn.predict(X_test)\nacc_test_knn=accuracy_score(y_test, y_pred)\n\nmat_knn = confusion_matrix(y_pred,y_test)\n\nframe=pd.DataFrame(severity_count_y_test,index=[\"Severity count in test_data\"])\ndisplay(frame)\n\nframe=pd.DataFrame({\"Train\":[acc_train_lr,acc_train_knn],\n                    \"Test\":[acc_test_lr,acc_test_knn]},\n                   index=[\"Logistic\",\"KNN\"])\ndisplay(frame)\n\nplt.figure(figsize=(8,4))\n\nplt.subplot(121)\nsns.heatmap(mat_lr, square=True, annot=True, fmt='d', cbar=False,xticklabels=[1,2,3,4],yticklabels=[1,2,3,4])\nplt.xlabel('true label')\nplt.ylabel('predicted label')\nplt.title('Logitstic regression')\n\nplt.subplot(122)\nsns.heatmap(mat_knn, square=True, annot=True, fmt='d', cbar=False,xticklabels=[1,2,3,4],yticklabels=[1,2,3,4])\nplt.xlabel('true label')\nplt.title('KNN')\n\nplt.show()\n\n","0fe26518":"%%time\n#Computational time ~\n#REF https:\/\/towardsdatascience.com\/decision-tree-classification-de64fc4d5aac\n#REF https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\ntree = DecisionTreeClassifier(max_depth=20,class_weight='balanced')\ntree.fit(X_train, y_train)\n\ny_pred = tree.predict(X_train)\nacc_train_dt=accuracy_score(y_train, y_pred)\n\ny_pred = tree.predict(X_test)\nacc_test_dt=accuracy_score(y_pred,y_test)\nmat_dt = confusion_matrix(y_pred,y_test)\n\nframe=pd.DataFrame(severity_count_y_test,index=[\"Severity count in test_data\"])\ndisplay(frame)\n\nframe=pd.DataFrame({\"Train\":[acc_train_lr,acc_train_knn,acc_train_dt],\n                    \"Test\":[acc_test_lr,acc_test_knn,acc_test_dt]},\n                   index=[\"Logistic\",\"KNN\",\"Decision tree\"])\ndisplay(frame)\n\nplt.figure(figsize=(12,4))\n\nplt.subplot(131)\nsns.heatmap(mat_lr, square=True, annot=True, fmt='d', cbar=False,xticklabels=[1,2,3,4],yticklabels=[1,2,3,4])\nplt.xlabel('True label')\nplt.ylabel('predicted label')\nplt.title('Logistic regression')\n\nplt.subplot(132)\nsns.heatmap(mat_knn, square=True, annot=True, fmt='d', cbar=False,xticklabels=[1,2,3,4],yticklabels=[1,2,3,4])\nplt.xlabel('True label')\nplt.title('KNN')\n\nplt.subplot(133)\nsns.heatmap(mat_dt, square=True, annot=True, fmt='d', cbar=False,xticklabels=[1,2,3,4],yticklabels=[1,2,3,4])\nplt.xlabel('True label')\nplt.title('Decision tree')\n\nplt.show()\n","43b95366":"%%time\n#COmputational time ~ \n#REF https:\/\/towardsdatascience.com\/understanding-random-forest-58381e0602d2\n#REF https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\nRF= RandomForestClassifier(n_estimators=100,class_weight='balanced')\nRF.fit(X_train, y_train)\n\ny_pred = RF.predict(X_train)\nacc_train_RF=accuracy_score(y_train, y_pred)\n\ny_pred = RF.predict(X_test)\nacc_test_RF=accuracy_score(y_pred,y_test)\nmat_RF = confusion_matrix(y_pred,y_test)\n\nframe=pd.DataFrame(severity_count_y_test,index=[\"Severity count in test_data\"])\ndisplay(frame)\n\nframe=pd.DataFrame({\"Train\":[acc_train_lr,acc_train_knn,acc_train_dt,acc_train_RF],\n                    \"Test\":[acc_test_lr,acc_test_knn,acc_test_dt,acc_test_RF]},\n                   index=[\"Logistic\",\"KNN\",\"Decision tree\",\"Random forest\"])\ndisplay(frame)\n\nplt.figure(figsize=(16,4))\n\nplt.subplot(141)\nsns.heatmap(mat_lr, square=True, annot=True, fmt='d', cbar=False,xticklabels=[1,2,3,4],yticklabels=[1,2,3,4])\nplt.xlabel('True label')\nplt.ylabel('predicted label')\nplt.title('Logistic regression')\n\nplt.subplot(142)\nsns.heatmap(mat_knn, square=True, annot=True, fmt='d', cbar=False,xticklabels=[1,2,3,4],yticklabels=[1,2,3,4])\nplt.xlabel('True label')\nplt.title('KNN')\n\nplt.subplot(143)\nsns.heatmap(mat_dt, square=True, annot=True, fmt='d', cbar=False,xticklabels=[1,2,3,4],yticklabels=[1,2,3,4])\nplt.xlabel('True label')\nplt.title('Decision tree')\n\nplt.subplot(144)\nsns.heatmap(mat_RF, square=True, annot=True, fmt='d', cbar=False,xticklabels=[1,2,3,4],yticklabels=[1,2,3,4])\nplt.xlabel('True label')\nplt.title('Random forest')\n\nplt.show()\n\nframe=pd.DataFrame(severity_count_y_test,index=[\"Severity.count in test_data\"])\ndisplay(frame)","abe46541":"req_data3=req_data.loc[req_data[\"City\"].isin(req_cities)]\n\nseverity_count={}\nseverity_count[1]=req_data3[req_data3[\"Severity\"]==1].shape[0]\nseverity_count[2]=req_data3[req_data3[\"Severity\"]==2].shape[0]\nseverity_count[3]=req_data3[req_data3[\"Severity\"]==3].shape[0]\nseverity_count[4]=req_data3[req_data3[\"Severity\"]==4].shape[0]\ntot_count=severity_count[1]+severity_count[2]+severity_count[3]+severity_count[4]\n\n\nframe=pd.DataFrame(severity_count,index=[\"Severity Count\"])\ndisplay(frame)\n\nseverity_count={}\nseverity_count[1]=req_data3[req_data3[\"Severity\"]==1].shape[0]\/tot_count*100\nseverity_count[2]=req_data3[req_data3[\"Severity\"]==2].shape[0]\/tot_count*100\nseverity_count[3]=req_data3[req_data3[\"Severity\"]==3].shape[0]\/tot_count*100\nseverity_count[4]=req_data3[req_data3[\"Severity\"]==4].shape[0]\/tot_count*100\n\nframe=pd.DataFrame(severity_count,index=[\"Severity percentage\"])\ndisplay(frame)\n\nprint(\"Removing outliers\")\nreq_data3=req_data3[~(req_data3[\"Severity\"]==1)]\nreq_data3=req_data3[~(req_data3[\"Severity\"]==4)]\n\nseverity_count={}\nseverity_count[1]=req_data3[req_data3[\"Severity\"]==1].shape[0]\nseverity_count[2]=req_data3[req_data3[\"Severity\"]==2].shape[0]\nseverity_count[3]=req_data3[req_data3[\"Severity\"]==3].shape[0]\nseverity_count[4]=req_data3[req_data3[\"Severity\"]==4].shape[0]\n\nframe=pd.DataFrame(severity_count,index=[\"Severity count\"])\ndisplay(frame)\n","2fbdb5ce":"%%time\n#Computational time ~\n#selecting input columns and output columns for training\n\nx_clmns=list(US_data.columns)\nx_clmns.remove(\"Severity\")\nx_clmns.remove(\"City\")\nx_clmns.remove(\"State\")\ny_clmns=[\"Severity\"]\n\nX=req_data3[x_clmns].values\ny=req_data3[y_clmns].values\n\ny=np.reshape(y,(-1,1))\ny=y[:,]\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X,y[:,0], test_size=0.1,random_state=21)\n\nprint(\"Train size:\",X_train.shape[0])\nprint(\"Test size:\",X_test.shape[0])\n\nseverity_count_y_test_no_outliers={}\nseverity_count_y_test_no_outliers[1]=y_test[y_test==1].shape[0]\nseverity_count_y_test_no_outliers[2]=y_test[y_test==2].shape[0]\nseverity_count_y_test_no_outliers[3]=y_test[y_test==3].shape[0]\nseverity_count_y_test_no_outliers[4]=y_test[y_test==4].shape[0]\n\nframe=pd.DataFrame(severity_count_y_test_no_outliers,index=[\"Severity count in test_data\"])\ndisplay(frame)","76ddea87":"%%time\n#COmputational time ~\n#Decision Tree Classifier after removing outliers\ntree = DecisionTreeClassifier(max_depth=20,class_weight='balanced')\ntree.fit(X_train, y_train)\n\ny_pred = tree.predict(X_train)\nacc_train_dt_no_outliers=accuracy_score(y_train, y_pred)\n\ny_pred = tree.predict(X_test)\nacc_test_dt_no_outliers=accuracy_score(y_pred,y_test)\nmat_dt_no_outliers = confusion_matrix(y_pred,y_test)\n\nframe=pd.DataFrame(severity_count_y_test_no_outliers,index=[\"Severity count in test_data\"])\ndisplay(frame)\n\nframe=pd.DataFrame({\"Train\":[acc_train_lr,acc_train_knn,acc_train_dt,acc_train_RF,acc_train_dt_no_outliers],\n                    \"Test\":[acc_test_lr,acc_test_knn,acc_test_dt,acc_test_RF,acc_test_dt_no_outliers]},\n                   index=[\"Logistic\",\"KNN\",\"Decision tree\",\"Random forest\",\"Decision tree:No outliers\"])\ndisplay(frame)\n\nplt.figure(figsize=(4,4))\nsns.heatmap(mat_dt_no_outliers, square=True, annot=True, fmt='d', cbar=False,xticklabels=[2,3],yticklabels=[2,3])\nplt.xlabel('True label')\nplt.title('Decision tree (No outliers)')\nplt.show()\n","81ae096c":"%%time\n#Computational time ~ \nRF= RandomForestClassifier(n_estimators=100,class_weight='balanced')\nRF.fit(X_train, y_train)\n\ny_pred = RF.predict(X_train)\nacc_train_RF_outliers=accuracy_score(y_train, y_pred)\n\ny_pred = RF.predict(X_test)\nacc_test_RF_outliers=accuracy_score(y_pred,y_test)\nmat_rf_no_outliers = confusion_matrix(y_pred,y_test)\n\nframe=pd.DataFrame(severity_count_y_test_no_outliers,index=[\"Severity count in test_data\"])\ndisplay(frame)\n\nframe=pd.DataFrame({\"Train\":[acc_train_lr,acc_train_knn,acc_train_dt,acc_train_RF,acc_train_dt_no_outliers,acc_train_RF_outliers],\n                    \"Test\":[acc_test_lr,acc_test_knn,acc_test_dt,acc_test_RF,acc_test_dt_no_outliers,acc_test_RF_outliers]},\n                   index=[\"Logistic\",\"KNN\",\"Decision tree\",\"Random forest\",\"Decision tree:No outliers\",\"Random forest:No outliers\"])\ndisplay(frame)\n\nplt.figure(figsize=(8,4))\nplt.subplot(121)\nsns.heatmap(mat_dt_no_outliers, square=True, annot=True, fmt='d', cbar=False,xticklabels=[2,3],yticklabels=[2,3])\nplt.xlabel('True label')\nplt.title('Decision tree')\n\nplt.subplot(122)\nsns.heatmap(mat_rf_no_outliers, square=True, annot=True, fmt='d', cbar=False,xticklabels=[2,3],yticklabels=[2,3])\nplt.xlabel('True label')\nplt.title('Random forest')\n\nplt.show()\n","9970081c":"The k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression.","f8908d08":"Severity doesnot show strong correlation with any of the feature and hence all the features are used for machine learning model.","557be90b":"### Project Overview\n\nThe dataset consists of location and time of the accident, weather conditions, road conditions, and accident severity (On a scale of 1 to 4). First, data cleaning of the dataset is performed to detect and correct or remove the corrupt records. Graphical techniques of EDA is used visualize the factors affecting the accident severity. Based on the EDA analysis, machine learning (ML) techniques: Logistic regression, KNN, Decision Tree Classifier, and Random Forest Classifier are used to develop a predictive model for accident severity. ","a1636b89":"Observation: After removing outliers, train accuracy using Decision Tree has improved from 0.893 to 0.937 and test accuracy of Decision Tree is improved from 0.808 to 0.869.","9de7ae80":"<a id='LR'><\/a>\n#### 3.3 Logistic regression","db8593ff":"#### Decision Tree Classifier","5b74a407":"The Traffic Message Channel (TMC) Description for different accidents along with their Severity levels.","02626210":"Observation: TMC stands for traffic message signal. For example,  code 201 is for Accident(s) and code  203 is for multi-vehicle accidents. The accidents are more with code 201.","23937f68":"Observation: The above frame list the top 10 streets in Norfolk with the maximum number of accidents. It is observed that a lot of accidents happen on highways like I-64 E, I64-W, etc.","54569113":"Observation: The above output shows the statistics of humidity, pressure, wind speed, and precipitation. It is observed that 75% of the accidents have precipitation below 0.0 and occur around the pressure of 29.8 inches. Based on the statistics, only humidity and windspeed are visualized and used for further analysis.","6662ba81":"<a id='RFC'><\/a>\n#### 3.6 Random Forest Classifier","d9a089b8":"Observation: This plot shows the temperature effects on severity.  Most of the accidents are between 10 F to 110 F and the violin plot shows that severity levels 2 and 3 have long tails because of outliers.","b8d0c917":"<a id='Summary'><\/a>\n#### 4.0 Summary and conclusions","7adadd9a":"<a id='EDA'><\/a>\n#### 2.0 Exploratory Data Analysis","23f9b32c":"A large dataset of around 1 GB consists of 3 million accidents is used in this project. To analyze the accidents, the dataset is cleaned systematically and EDA is performed using several iterations. \n\nThe computational power of my laptop is not sufficient to build a model on the entire dataset. From Exploratory Data Analysis (EDA), it is observed that the state California has more accidents compared to other states, and the San Francisco bay area has around one-fifth of California accidents. Hence machine learning models are developed to predict accident severity for the California bay area.\n\nInitially, basic models such as Logistic Regression and KNN were used to build the predictive model. The models yielded poor accuracy and hence the accuracy was improved by using models: Decision tree Classifier, Random Forest Classifier. Random forest algorithm predicted severity with a training and test accuracy of 0.99996 and 0.854195 respectively. \n\nTo further improve the accuracy, outliers in severity levels are removed and a train and test accuracy of 0.999976 and    0.879851 is achieved respectively.","e3afd378":"In this method, a set of training examples is broken down into smaller and smaller subsets while at the same time an associated decision tree gets incrementally developed. At the end of the learning process, a decision tree covering the training set is returned.","eb88be0f":"<a id='distance'><\/a>\n#### 2.9 Length of the road extent affected by the accident","7aca8e1c":"Observation: This plot shows the variation of the number of accidents at a different minute of a day starting from 0 minutes, which corresponds to 12:00 AM. The minute plot is similar to the hourly plot where the curve peaks at 8:AM and 5:PM.","89fe3cbd":"This scatter plot shows the accidents map in california area.","455e24cf":"<a id='side'><\/a>\n#### 2.6 Side (Right\/Left) of the accidents","e0ce758d":"<a id='visibility'><\/a>\n#### 2.13 Affect of visibility on accidents","ea68fd4e":"Observation: Statistics of wind speed show that 99% of values lie below 23 miles per hour and the maximum value is 828 miles per hour. It says that rest 1% of the data is varying a lot ranging from 23 to 822 and hence there are outliers in the box plot.","61f1ce71":"Observation: This above output shows the box and violin plot of traffic congestion. The plot shows there is a clear effect of traffic congestion on the severity.","54f359d5":"<a id='converting_time'><\/a>\n#### 1.3 Converting accident start and end time to year, month, day and time","12f8f01f":"<a id='location'><\/a>\n#### 2.9 Location of accidents","19bfe2c2":"Observation: The first total number of accidents with different severity levels is plotted. Orange bar in the plot shows the total number of accidents with severity level 2, which is the most with around 2 million accidents followed by severity level 3 shown by the green bar and then followed by severity level 4. Whereas there are very few numbers of accidents with severity level 1.","ba869b0c":"<a id='Reading_Data'><\/a>\n#### 1. Reading Data","94a2dd47":"# Exploratory Data Analysis (EDA) of US Accidents and Prediction of Accident Severity in San Fransisco bay area.","23fe85b1":"Observation: In the above output, frame lists the top 7 cities with the most number of accidents followed by the Norfolk, Virginia Beach, and Hampton. Houston has the most number of accidents with 93000, followed by Charlotte and Los Angels. Compared to the top 7 cities, the number of accidents in Norfolk, Virginia Beach, and Hampton is negligible and they rank more than 123.","7280cc12":"<a id='hour'><\/a>\n#### 2.18 Accidents by hourly basis","5f1f5be9":"<a id='source'><\/a>\n#### 2.1 Reporting source of the accidents","f3d98cf1":"#### Random Forest Classifier","5cbac80d":"Observation: This plot shows the box plot and leaf plot of humidity at different severity levels. There are no outliers in the humidity data. ","2f3ef9a4":"Observation: The above bar plot shows that accidents based on location. It can be observed that accidents are most near the traffic signal followed by the junction and railway crossing. The maximum number of accidents at different locations is with severity level 2 and the percentage of accidents with severity 3 is more at junction compared to other locations.","4751fe7f":"<a id='month'><\/a>\n#### 2.15 Accidents in different months","a9f9d56e":"<a id='cities'><\/a>\n#### 2.3 Accidents in top 7 cities and in Norfolk, Virginia Beach and Hampton","14b45bc8":"Observation: The scatter plot shows the accident map and color represents the severity level. It can be observed that the east coast, west coast, and Michigan areas have a lot of accidents. This is because these areas are most industrialized and have large population density. Whereas central America has a few numbers of accidents. It is also observed that the accident severity level is high in Michigan which is highlighted by yellow color.","2fc22718":"<a id='humidity'><\/a>\n#### 2.12 Humidity, Pressure, wind speed and precipivation affects on accidents","77821dae":"Logistic regression is returning very poor results and is not converging even after increasing the maximum number of iterations to 1000.","a79cc6c8":"### Project  Objective\n\nThere are 276.1 million registered vehicles in the USA according to Hedges and Company. More than 38000 people die every year in crashes on US roadways and the traffic fatality rate is 12.4 deaths per 100,000 inhabitants. An additional 4.4 million people are injured seriously enough to get medical attention. Road crashes are the leading cause of death in the US.\n\nThe objective of this project is to perform Exploratory Data Analysis (EDA) of the US accidents dataset to visualize the number of accidents by state, county, city, and the factors affecting the accident severity. Develop a model to accurately predict the accident severity which could enable emergency medical services (EMS) for a better allocation of the resources. ","4d1ed4bf":"Observation: This shows the bar plot of accidents by month. The number of accidents is mostly uniform until July then increases from august.","79adda20":"Observation: In the above figure, I have created a frame listing the total number of accidents reported by different sources and plotted a stacked bar chart and a pie chart. It can be observed that MapQUest reported the maximum number of accidents as represented by the blue color in the pie chart. The bar chart shows the total number of accidents reported by each source and also the number of accidents with different severity levels is highlighted in the bar chart. Orange shows the total number of accidents with severity level 2, which is maximum, green shows severity 3, red shows severity 4 and blue shows severity 1, which cannot be seen in the barplot because of very less number of accidents with severity 1. In this entire notebook, the colors for the severity level are retained whenever a stacked bar chart is plotted.","cc09bc17":"Observation: The above plot shows the accidents in counties around Norfolk, Virginia Beach, and Hampton. Norfolk county has more number of accidents.","ffdffdde":"<a id='TTsplit'><\/a>\n#### 3.2 Train Test split","46478d9a":"Observation: The above plot shows the daily variation of the total number of accidents and the total number of accidents with different severity levels. In this plot, day number 1 corresponds to Jan 1st and day number 365 corresponds to December 31st. The plot shows the number of accidents is oscillating and the maximum value of oscillations is increasing towards the end of the year. The troughs in the oscillation correspond to the weekends because the accidents are decreasing on the weekends and there are approximately 53 troughs.","a0d40fd0":"Observation: This plot shows the variation in the number of accidents with a weekday. This plot supports the previous argument that the number of accidents is decreasing on weekends.","a08e05e1":"Plotting the latitude and longitude with time zones.","d33cd9cc":"<a id='DTC'><\/a>\n#### 3.5 Decision Tree Classifier","349da52f":"<a id='weekday'><\/a>\n#### 2.17 Accidents on different days of a weel","5de0c633":"Observation: The above plot shows the accidents on right and left sides of the lane. The total number of accidents is more on the right side as shown by the blue region in the pie chart. From the stacked bar plot, it can be observed that the number of accidents with severity levels 2 and 3 is considerable on the right side but most of the accidents on the left side are with severity level 2. Overall, the accidents with severity level 2 is more compared to other severity levels on both right and left lanes.","975aa732":"<a id='TOC'><\/a>\nTabel of content\n-----------------------\n\n[1. Reading Data](#Reading_Data)\n\n $\\;\\;\\;\\;\\;\\;$[1.1. Summarizing Data](#Summarizing_Data)\n\n $\\;\\;\\;\\;\\;\\;$[1.2. Converting accident start and end time to year, month, day and time](#converting_time)\n\n[2. Exploratory Data Analysis](#EDA)\n\n$\\;\\;\\;\\;\\;\\;$[2.1. Reporting source of the accidents](#accidents_source)\n\n$\\;\\;\\;\\;\\;\\;$[2.2. Accidents in different states](#state)\n\n$\\;\\;\\;\\;\\;\\;$[2.3. Accidents in top 7 cities and in Norfolk, Virginia Beach and Hampton](#cities)\n\n$\\;\\;\\;\\;\\;\\;$[2.4. Accidents in counties around Norfolk, Virginia Beach and Hampton](#county)\n\n$\\;\\;\\;\\;\\;\\;$[2.5. Accidents in Norfolk streets](#streets)\n\n$\\;\\;\\;\\;\\;\\;$[2.6. Side (Right\/Left) of the accidents](#side)\n\n$\\;\\;\\;\\;\\;\\;$[2.7. Influence of weather condition on accidents](#weather_condition)\n\n$\\;\\;\\;\\;\\;\\;$[2.8. Accidents at different periods of the day](#period_day)\n\n$\\;\\;\\;\\;\\;\\;$[2.9. Location of accidents](#location)\n\n$\\;\\;\\;\\;\\;\\;$[2.10. Traffic Message Channel (TMC) Description for different accident](#TMC)\n\n$\\;\\;\\;\\;\\;\\;$[2.11. Temperature affects on accidents](#temperature)\n\n$\\;\\;\\;\\;\\;\\;$[2.12. Humidity, Pressure, wind speed and precipitation affects on accidents](#humidity)\n\n$\\;\\;\\;\\;\\;\\;$[2.13. Affect of visibility on accidents](#visibility)\n\n$\\;\\;\\;\\;\\;\\;$[2.14. Accidents by year](#year)\n\n$\\;\\;\\;\\;\\;\\;$[2.15. Accidents in different months](#month)\n\n$\\;\\;\\;\\;\\;\\;$[2.16. Accidents variation on daily basis](#day)\n\n$\\;\\;\\;\\;\\;\\;$[2.17. Accidents on different days of a weel](#weekday)\n\n$\\;\\;\\;\\;\\;\\;$[2.18. Accidents by hourly basis](#hour)\n\n$\\;\\;\\;\\;\\;\\;$[2.19. Duration of traffic congestion due to accident](#duration)\n\n[3.0. Machine learning model to predict severity in bay area](#ML)\n\n$\\;\\;\\;\\;\\;\\;$[3.1. Correlation analysis](#CA)\n\n$\\;\\;\\;\\;\\;\\;$[3.2. Train Test split](#TTsplit)\n\n$\\;\\;\\;\\;\\;\\;$[3.3. Logistic regression](#LR)\n\n$\\;\\;\\;\\;\\;\\;$[3.4. KNN](#KNN)\n\n$\\;\\;\\;\\;\\;\\;$[3.5. Decision Tree Classifier](#DTC)\n\n$\\;\\;\\;\\;\\;\\;$[3.6. Random Forest Classifier](#RFC)\n\n$\\;\\;\\;\\;\\;\\;$[3.7. Removing Severity outliers](#Routliers)\n\n[4.0. Summary and conclusions](#Summary)","947f0787":"This plot shows the box and violin plot of wind speed after removing the data above 23 miles per hour. It can be observed that there are a few numbers of outliers.","04e08821":"Accidents reported on different days.","bcb18869":"The total number of accidents reported by month have calculated along with the Severity levels.","a0168bae":"Observation: Statistics of visibility show that most of the values of visibility are around 10 miles and there are a lot of outliers as shown in the box plot. So the visibility is removed from further analysis.","019517ee":"<a id='day'><\/a>\n#### 2.16 Accidents variation on daily basis","7579069d":"This frame shows the summary of the data after cleaning.","03c704da":"Displaying values of Humidity, Pressure, Wind Speed, and Precipitation. From the display, it is clear that pressure and precipitation have almost zero values. Hence assigning them to remove columns.","e03cf190":"Observation: The above plot shows the number of accidents by year. The number of accidents is increasing by the year and there is not enough data for 2020 and I would guess the accidents may decreases by the end of 2020 because of current lockdown.","0740c2cf":"<a id='duration'><\/a>\n#### 2.19 Duration of traffic congestion","d3dcb5dc":"The Start_time and End_time are converted in to year, month, weekday, hour,minute and duration.","ee341cdc":"Observation: The above plot shows the number of accidents at different hours of the day. The number of accidents peaks at 8:00 AM and 5:00 PM because these are peak traffic periods as many people travel to offices and back to home.","a41bd5a6":"<a id='TMC'><\/a>\n#### 2.10 Traffic Message Channel (TMC) Description for different accident","3fe3aee1":"Observation: The above plot shows the number of accidents under different weather conditions. The accidents are more on a clear day, followed by mostly cloudy, overcast, and other conditions. Under all weather conditions, the number of accidents with severity level 2 is more.","d8421039":"Observation: The above output shows the statistics and box plot of the road extent affected by accidents. It can be observed that 50% of the data is below 0 and 90% of data is 0.59 miles and there are so many outliers. So, this column is removed from further analysis.","d4361cfd":"Logistic Regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary). Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.","fe27f981":"<a id='temperature'><\/a>\n#### 2.11 Temperature affects on accidents","9cd63ed6":"<a id='period_day'><\/a>\n#### 2.8 Accidents at different periods of the day","668db6f2":"<a id='year'><\/a>\n#### 2.14 Accidents by year","4eb9e860":"<a id='county'><\/a>\n#### 2.4 Accidents in counties around Norfolk, Virginia Beach and Hampton","3a14a8ac":"<a id='Routliers'><\/a>\n#### 3.7 Removing outliers","e3c44b8b":"<a id='summarizing_data'><\/a>\n#### 1.2 Summarizing Data","19c2f9d7":"Observation: By using Random Forest Classifier, the train and test accuracy is improved to 0.99 and 0.85 respectively. The confusion matrix shows that the number of correct labeling for severity 2 has improved. An investigation is done to further improve the accuracy and by observing the outliers, it is determined that outliers in severity may be affecting accuracy.","ef7cd103":"Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model\u2019s prediction ","86244a04":"Observation: In the above frames, it is observed that severity levels 1 and 4 are only around 1%. The analysis is done by removing the data with severity levels 1 and 4 to improve prediction accuracy.","31cfd7ef":"Observation: The above figure shows US time zones and as exaplined in previous plot, the accidents are more in east coast and west coast.","bcfb54ba":"#### Train test split","a52d05db":"<a id='CA'><\/a>\n#### 3.1 Correlation analysis","9179fa44":"<a id='ML'><\/a>\n#### 3.0 Machine learning model to predict severity in bay area","481fd42b":"Note: The function removes the required column names without any error if the cell is rerun.","e3a3237a":"Observation: By using the decision tree, the training accuracy improved to 0.89 and test accuracy to 0.80. The confusion matrix shows that the number of points for which the decision tree is correctly labeling the accidents with severity 3 has improved. Similarly for severity 2. To improve accuracy further, the random forest algorithm is used.","7bb16c3b":"<a id='KNN'><\/a>\n#### 3.4 K-NN","8df798d6":"Observation: In the above figure, I have created a frame listing the total number of accidents reported by different sources and plotted a stacked bar chart and a pie chart. It can be observed that MapQUest reported the maximum number of accidents as represented by the blue color in the pie chart. The bar chart shows the total number of accidents reported by each source and also the number of accidents with different severity levels is highlighted in the bar chart. Orange shows the total number of accidents with severity level 2, which is maximum, green shows severity 3, red shows severity 4 and blue shows severity 1, which cannot be observed in the barplot because of very less number of accidents with severity 1. In this entire notebook, the colors for the severity level are retained whenever a stacked bar chart is plotted.","457ec19a":"<a id='streets'><\/a>\n#### 2.5 Accidents in Norfolk streets","2ae34650":"## Dataset Overview\n\nThe dataset is a 1GB file consist of 3 million accident data points with 49 columns. This is a countrywide car accident data set that covers 49 states of the USA. It has features like reporting sources of the accident, TMC, state, source, weather conditions, etc.\n\nLink for kaggle dataset: https:\/\/www.kaggle.com\/sobhanmoosavi\/us-accidents","a628373d":"<a id='state'><\/a>\n#### 2.2 Accidents in different states","cdc91b4f":"Using k-NN I have calculated accuracy score for different number of neighbours. k=4 is the optimum value with maximum accuracy score of 65%.","7e32e981":"Observations: This plot shows the total number of accidents based on day and night. The plot shows that the number of accidents is more during the day compared to night.","9f73ce22":"Splitting the data using train_test split. Training number of points are around 123 thousand and test size is around 13 thousand.","930c0f4a":"By using random forest, the test accuracy has been improved to 0.88 and number of correct labeling for severity level 2 has improved ","1160c51f":"Observation: The accident dataset is around 3 million and the computational power of my laptop is not enough to build a model on large data. From Exploratory Data Analysis (EDA), the state of California has more accidents compared to other states and the San Francisco bay area has around one-fifth of accidents. Machine learning models are built for the California bay area.","ac1823fa":"Observation: The above output shows that train accuracy has improved with KNN but and test accuracy is poor. The confusion matrix shows the performance of the classification model on test data. The diagonal of the matrix shows the number of data points with correctly labeled and off-diagonal is the number of data points that are wrongly labeled. From the confusion matrix, it can be observed that both models are wrongly labeling most of the accidents with severity level 3 as 2. Decision Tree Classifier is used to improve accuracy.","122a6f5e":"Observation: Dividing the columns into categorical, boolean, and numerical. First visualizing the categorical columns of the US_data set.","fe4a698a":"<a id='weather_condition'><\/a>\n#### 2.7 Influence of weather condition on accidents"}}