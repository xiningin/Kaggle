{"cell_type":{"bf42b48c":"code","f24959d7":"code","478784de":"code","899d6ef2":"code","e5292ca4":"code","c4792116":"code","7a2e151c":"code","15d94454":"code","530224a6":"code","5d621c60":"code","47ae9088":"code","3f4e468f":"code","91a006c0":"code","96d91f1f":"code","af8418af":"code","64d61ea9":"code","d80d512a":"code","d836f8d5":"code","d9f6914e":"code","f016a46c":"code","4a0c0069":"code","03851b31":"code","c8a2d5d7":"code","390b1787":"code","06a7102d":"code","7659695a":"code","c4fd863d":"code","ca8ce65c":"code","94b5f0bf":"code","5eed269f":"code","a859a216":"code","11e7db6d":"code","622b102a":"code","38c30a1d":"code","89c72fb1":"code","920ad249":"code","c1e8539c":"code","c56ff00d":"code","e88e30c2":"code","fa620fa6":"code","d5392744":"code","6ba8668a":"code","10d2c686":"code","24d6f90d":"code","e656b0db":"code","48bb0961":"code","07ba81e2":"code","bc5dff87":"code","893ba91c":"code","7b2ee48e":"code","ba29b302":"code","63adcfdf":"code","1358ee40":"code","4626817c":"code","a3b2fcd1":"code","a26dc63b":"code","a0bf2024":"code","6d432bc8":"code","abf52c8c":"code","18747296":"code","a5940fd6":"code","791d0d55":"code","55c0aeab":"code","bcca9a3d":"code","c40a36ef":"code","2788cc9d":"code","c35b610f":"code","162b813e":"code","43473962":"code","de4df7f9":"code","ffa19c77":"code","5fa5e595":"code","7b739a54":"code","e4ad762d":"code","6c9c4673":"code","990b4532":"code","c4201526":"code","e0f54f44":"code","60be0fe3":"code","5fcb5472":"code","e0527f50":"code","502b7a44":"code","6863993b":"code","c4cf4ce4":"code","8921e647":"code","a7e75ab5":"code","7b8ffe96":"code","6a427a40":"code","cd5f69dd":"code","6b465702":"code","2cc2c1d1":"code","edff0a99":"code","d1e26de9":"code","653c741e":"code","2eb3bf24":"markdown","10a46f95":"markdown","50f3604c":"markdown","7aef9cc8":"markdown","0c108f31":"markdown","09a5904b":"markdown","90178988":"markdown","1396ef4b":"markdown","1c2273bb":"markdown","f091e92a":"markdown","4d18dbdb":"markdown","a06681cd":"markdown","fe733194":"markdown","e885c810":"markdown","cac9ce8b":"markdown","5fff60a3":"markdown","86ed27a9":"markdown","7d40e74a":"markdown","a9ba3223":"markdown","0820366b":"markdown","c8548fb3":"markdown","997a952c":"markdown","8bb9984b":"markdown","1d09b930":"markdown","5f61a447":"markdown","a89a1395":"markdown","0cdccb8b":"markdown","59773c5d":"markdown","27c746d2":"markdown","7420e19f":"markdown","2bdb0b9b":"markdown","314969b1":"markdown","946d8cff":"markdown","38a4bd95":"markdown","7fe33d41":"markdown","c6443367":"markdown","cd55a151":"markdown","99c22007":"markdown","0b7ac981":"markdown","04ffdf70":"markdown","e597b638":"markdown","7847b0aa":"markdown","f63bb1d0":"markdown","d626e63f":"markdown","d57ee354":"markdown","60983ac3":"markdown","7240e538":"markdown","bb17b511":"markdown","2accb45b":"markdown","1920bd45":"markdown","a55c0421":"markdown","1425ae1d":"markdown","71228e8b":"markdown","52a51b20":"markdown","73a2d202":"markdown","92c88ec9":"markdown","f29bb0eb":"markdown","fae38bcc":"markdown","450ecdd9":"markdown","8197a91e":"markdown","1a08d707":"markdown","cb954e44":"markdown","ed1ed31f":"markdown","e9a6f3a4":"markdown","dc322c3e":"markdown","f9afc396":"markdown","1e018d81":"markdown","c9b66433":"markdown","71574a23":"markdown","266c9381":"markdown","74e00f86":"markdown","efca054a":"markdown","9c9d16b3":"markdown","615e54f0":"markdown","b57bd11e":"markdown","bb96896d":"markdown","39622630":"markdown","83c7b88e":"markdown","ea82822f":"markdown","46d466ca":"markdown"},"source":{"bf42b48c":"# Importing the dataset\nimport pandas as pd\ntrain_dataset = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nprint(train_dataset.head())","f24959d7":"# Load dataset for test set\ntest_dataset = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nprint(test_dataset.head())","478784de":"# Displaying Shape of training set\ntrain_dataset.shape","899d6ef2":"# Displaying Shape of test dataset\ntest_dataset.shape","e5292ca4":"# Displaying datatype of each attributes for train dataset\ntrain_dataset.dtypes","c4792116":"# Displaying datatype of each attributes for test dataset\ntest_dataset.dtypes","7a2e151c":"# Displaying descritive statistical analysis for train dataset\n\ntrain_dataset.describe()","15d94454":"# Displaying descritive statistical analysis for test dataset\ntest_dataset.describe()","530224a6":"# Histogram plot statistical visualisation analysis for train dataset\nimport matplotlib.pyplot as plt\ntrain_dataset.hist(bins=20,figsize=(12,12),layout=(3,3))\nplt.show()","5d621c60":"# Density plot statistical visualisation analysis for train datasets\n\ntrain_dataset.plot(kind='density',figsize=(10,10),layout=(3,3),subplots=True,sharex=False,sharey=False)","47ae9088":"# Histogram plot statstical visualisation analysis for test dataset\n\ntest_dataset.hist(bins=20,figsize=(12,12),layout=(3,3))\nplt.show()","3f4e468f":"# Density plot Stastistical Visualisation analysis for test dataset\ntest_dataset.plot(kind='density',figsize=(10,10),layout=(3,3),sharex=False,sharey=False,subplots=True)\nplt.show()","91a006c0":"# extracting insights of \"Pclass\" attribute using statistical visualisation distribution.\n\n# kde--- kind distribution wave\n# color--- color of graphical plot\n# norm_hist --- True or false makes normal distribution graph should be there or not.\n\n\nimport seaborn as sb\nsb.distplot(train_dataset['Pclass'],bins=5,hist=True,norm_hist=True,color='green',vertical=False,kde=True,label='Pclass')\nplt.legend()\nplt.show()","96d91f1f":"# Survived output attribute( its optional to visualise )\n\nsb.distplot(train_dataset['Survived'],color='red',bins=5,hist=True,norm_hist=True,vertical=False,kde=True,label='Survied')\nplt.legend()\nplt.show()","af8418af":"# Age attribute\n\nsb.distplot(train_dataset['Age'],bins=5,hist=True,norm_hist=True,vertical=False,label='Age')\nplt.legend()","64d61ea9":"# Fare Attribute\n\nsb.distplot(train_dataset['Fare'],bins=35,vertical=False,hist=True,norm_hist=True,label='Fare')\nplt.legend()","d80d512a":"# SibSp attribute\n\nsb.distplot(train_dataset['SibSp'],label='SibSp', bins=10, hist=True, vertical=False,norm_hist=True)\nplt.legend()","d836f8d5":"# Displaying the null values list for each attribute\n\n# train dataset\ntrain_dataset.info()","d9f6914e":"# test dataset\n\ntest_dataset.info()","f016a46c":"# Displaying the sum of null value count in each attribute from train dataset\n\ntrain_dataset.isna().sum()","4a0c0069":"train_dataset_null=(train_dataset.isna().sum()\/len(train_dataset))*100.0\n\n# Now we are droping all values ratio is equal zero in all_data_null variable\nall_data_null= train_dataset_null.drop(train_dataset_null[train_dataset_null==0].index).sort_values(ascending=False)\n\nprint(all_data_null)","03851b31":"sb.barplot(x=all_data_null.index,y=all_data_null)\nplt.xlabel('Feature')\nplt.xticks(rotation='90')\nplt.xlabel('percentage of missing ratio')\nplt.title('percentage of missing ratio on barplot')","c8a2d5d7":"# Displaying the sum of null value count in each attribute from test dataset\n\ntest_dataset.isna().sum()","390b1787":"# Removing the Cabin column in both train and test dataset\n\ntrain_dataset=train_dataset.drop('Cabin',axis=1)","06a7102d":"train_dataset=train_dataset.fillna(train_dataset.mean())","7659695a":"train_dataset.isna().sum()","c4fd863d":"\ntrain_dataset=train_dataset.dropna(axis=0)","ca8ce65c":"train_dataset.isna().sum()","94b5f0bf":"test_null_ratio=(test_dataset.isna().sum()\/len(test_dataset))*100.0\n\n# Now we are droping all values ratio is equal zero in all_data_null variable\n\nall_null_value_ratio=test_null_ratio.drop(test_null_ratio[test_null_ratio==0].index).sort_values(ascending=False)\n\nprint(all_null_value_ratio)","5eed269f":"# Missing value ratio using visualising the barplot\nsb.barplot(x=all_null_value_ratio.index,y=all_null_value_ratio)\nplt.xlabel('Percentage')\nplt.xticks(rotation='90')\nplt.ylabel('missing value ratio')\nplt.title('visualising the Missing value ratio on barplot')","a859a216":"# Removing the null values from test dataset\ntest_dataset=test_dataset.drop('Cabin',axis=1)","11e7db6d":"test_dataset=test_dataset.fillna(test_dataset.mean())","622b102a":"# Checking the null values removed or not from test dataset\n\ntest_dataset.isna().sum()","38c30a1d":"train_dataset=train_dataset.drop('PassengerId',axis=1)\ntrain_dataset=train_dataset.drop('Name',axis=1)\ntrain_dataset.head()","89c72fb1":"# Making duplicate of test_dataset\ntest_dataset1=test_dataset\n\ntest_dataset=test_dataset.drop('PassengerId',axis=1)\ntest_dataset=test_dataset.drop('Name',axis=1)\ntest_dataset.head()","920ad249":"train_dataset['Sex'].unique()","c1e8539c":"train_dataset['Sex']=train_dataset['Sex'].map({'female':1,'male':0})\ntrain_dataset.head()","c56ff00d":"from sklearn.preprocessing import LabelEncoder\nlabel=LabelEncoder()\ntrain_dataset['Ticket']=label.fit_transform(train_dataset['Ticket'])\ntrain_dataset['Embarked']=label.fit_transform(train_dataset['Embarked'])\ntrain_dataset.head()","e88e30c2":"from sklearn.preprocessing import LabelEncoder\nlabel=LabelEncoder()\ntest_dataset['Ticket']=label.fit_transform(test_dataset['Ticket'])\ntest_dataset['Embarked']=label.fit_transform(test_dataset['Embarked'])\ntest_dataset.head()","fa620fa6":"test_dataset['Sex']=label.fit_transform(test_dataset['Sex'])\ntest_dataset.head()","d5392744":"# outlier checking between Survived and Pclass attributes....\n\nplt.scatter(train_dataset['Pclass'],train_dataset['Survived'])\nplt.xlabel(\"Pclass\")\nplt.ylabel(\"Survived\")\nplt.title(\"Outlier between Pclass and Survived\")\nplt.show()","6ba8668a":"# outlier checking between Survived and Sex attributes....\n\nplt.scatter(train_dataset['Sex'],train_dataset['Survived'])\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Survived\")\nplt.title(\"Outlier between Sex and Survived\")\nplt.show()","10d2c686":"# outlier checking between Survived and Age attributes....\n\nplt.scatter(train_dataset['Age'],train_dataset['Survived'])\nplt.xlabel('Age')\nplt.ylabel('Survived')\nplt.title('Outlier between Survived and Age')\nplt.show()","24d6f90d":"# removing outlier between age and survived...\n\n# Deleting outliers\n\ntrain_dataset=train_dataset.drop(train_dataset[(train_dataset['Age']>70)].index)\n\nplt.scatter(train_dataset['Age'],train_dataset['Survived'])\nplt.xlabel('Age')\nplt.ylabel('Survived')\nplt.show()","e656b0db":"# Outlier checking between SibSp and Survived\n\nplt.scatter(train_dataset['SibSp'],train_dataset['Survived'])\nplt.xlabel('SibSp')\nplt.ylabel('Survived')\nplt.title('Outlier between SibSp and Survived')\nplt.show()","48bb0961":"# We found outlier between SibSp and Survived, We are going to remove all values after 5 sbisp\n\ntrain_dataset=train_dataset.drop(train_dataset[(train_dataset['SibSp'])>5].index)\n\nplt.scatter(train_dataset['SibSp'],train_dataset['Survived'])\nplt.xlabel('SibSp')\nplt.ylabel('Survived')\nplt.title('After removed outlier between SibSp and Survived')\nplt.show()","07ba81e2":"# Outlier check between Parch and Survived\n\nplt.scatter(train_dataset['Parch'],train_dataset['Survived'])\nplt.xlabel('Parch')\nplt.ylabel('Survived')\nplt.title('Outlier between Parch and Survived')\nplt.show()","bc5dff87":"# Checking outlier between Ticket and survived\n\nplt.scatter(train_dataset['Ticket'],train_dataset['Survived'])\nplt.xlabel('Ticket')\nplt.ylabel('Survived')\nplt.title('Outlier between Ticket and Survived')\nplt.show()","893ba91c":"# Checking outliers between Fare and Survived\n\nplt.scatter(train_dataset['Fare'],train_dataset['Survived'])\nplt.xlabel('Fare')\nplt.ylabel('Survived')\nplt.title('Outliers between Fare snd Survived')\nplt.show()","7b2ee48e":"# Removing the outliers between Fare and Survived\n\ntrain_dataset=train_dataset.drop(train_dataset[train_dataset['Fare']>300].index)\n\nplt.scatter(train_dataset['Fare'],train_dataset['Survived'])\nplt.xlabel('Fare')\nplt.ylabel('Survived')\nplt.title('Outliers between Fare and Survived')\nplt.show()","ba29b302":"# Checking Outliers between Embarked and Survived\n\nplt.scatter(train_dataset['Embarked'],train_dataset['Survived'])\nplt.xlabel('Embarked')\nplt.ylabel('Survived')\nplt.title('Outlier Between Embarked and Survived')\nplt.show()","63adcfdf":"servived_data= train_dataset['Survived'].value_counts()[1]\/len(train_dataset)*100.0\nnon_servived_data=train_dataset['Survived'].value_counts()[0]\/len(train_dataset)*100.0\n\nprint(\"servived dataset\",servived_data)\nprint(\"non_servived dataset\",non_servived_data)\nprint(\"servived data rows \",train_dataset['Survived'].value_counts()[1])\nprint(\"non servived data rows \",train_dataset['Survived'].value_counts()[0])","1358ee40":"# Visualise the class distribution.\n\nsb.countplot('Survived',data=train_dataset)\nplt.title('class Distribution \\n(0 : Non Servived || 1: Servived)',fontsize=12)","4626817c":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n#train_dataset=train_dataset.sample(frac=1)\n\nservived_data=train_dataset.loc[train_dataset['Survived'] == 1]\nnon_servived_data=train_dataset.loc[train_dataset['Survived'] == 0][:336]\n\nnomal_distribution_dataset=pd.concat([servived_data,non_servived_data])\n\n# Shuffle dataframe rows\nbalanced_train_dataset=nomal_distribution_dataset.sample(frac=1,random_state=5)\n\nbalanced_train_dataset.head()","a3b2fcd1":"print(\"Distribution of the classes in the subsample dataset\")\nprint(balanced_train_dataset['Survived'].value_counts()\/len(train_dataset))\n\nsb.countplot('Survived',data=balanced_train_dataset)\nplt.title(\"Equally Distribution classes\",fontsize=12)\nplt.show()","a26dc63b":"'''apply SelectKBest class to extract top 10 best features... \nif you have more than 10 features it will show top 10 and if you have less than 10 features then,\nwe have use k=all or else it will through an error (ValueError: k should be >=0, <= n_features = 8; got 10. Use k='all' to return all features), \nwe still can keep 10 as max features'''\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nx=balanced_train_dataset.iloc[:,1:]\ny=balanced_train_dataset.iloc[:,0:1]\n\n# Applying SelectKBest class to extract\n# k-- No of features we want to display or top features we want to visualise\nbestfeature=SelectKBest(score_func=chi2,k=8)\nfit_selectkbest=bestfeature.fit(x,y)\n\ndataframe_scores=pd.DataFrame(fit_selectkbest.scores_)\ndataframe_columns=pd.DataFrame(x.columns)\n\n# concat two data frame for better visualization\nfeature_score=pd.concat([dataframe_columns,dataframe_scores],axis=1)\n\n# Naming the dataframe columns\nfeature_score.columns=['Specs','Score']\n\n# print top 10 best features\nprint(feature_score.nlargest(8,'Score'))","a0bf2024":"# Feature importance using extratreesclassifier\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel=ExtraTreesClassifier()\nmodel.fit(x,y)\n\n# user inbuild class feature_importances of tree based classifier\nprint(model.feature_importances_)\n\n# plotting the graph of feature importances for better visualization\nimportance=pd.Series(model.feature_importances_,index=x.columns)\nimportance.nlargest(8).plot(kind='barh')\nplt.show()","6d432bc8":"# correlation matrix is used to identify the relation percentage between each other...\n\ncorrelation_matrix=balanced_train_dataset.corr()\nsb.heatmap(correlation_matrix,square=True)","abf52c8c":"#1.\n# correlation coefficient value for survived output attribute by dataframe technique\nimport numpy as np\nk=10 # selecting top 10 correlation matrix to visualise\ncolumns=correlation_matrix.nlargest(k,'Survived')['Survived'].index\n\ncorrelation_coefficient=np.corrcoef(balanced_train_dataset[columns].values.T)\nsb.set(font_scale=1.25)\n\nZoomin_heatmap=sb.heatmap(correlation_coefficient,cbar=True,annot=True,square=True,fmt='.2f',annot_kws={'size':10},yticklabels=columns.values,\n                         xticklabels=columns.values)\nplt.show()","18747296":"#2.\n# Correlatio coefficient value for survived attribute by after convert from dataframe to array of matrix technique\n\n#get correlations of each features in dataset\nfeature_maps=balanced_train_dataset.corr()\ntop_corr_features=feature_maps.index\nplt.figure(figsize=(15,15))\n\n# plot for heatmap\ng=sb.heatmap(balanced_train_dataset[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","a5940fd6":"# Correlation input attribute relation with output attribute\n\ncorrelation=balanced_train_dataset.corr()['Survived']\ncorrelation.abs().sort_values(ascending=False)","791d0d55":"# Removing less correlated values Age and SibSp\n\ncolumns=['Age','SibSp']\nbalanced_train_dataset=balanced_train_dataset.drop(columns,axis=1)\nbalanced_train_dataset.head()","55c0aeab":"# Removing less correlated values Age and SibSp\n\ncolumns=['Age','SibSp']\ntest_dataset=test_dataset.drop(columns,axis=1)\ntest_dataset.head()","bcca9a3d":"# 1. first technique\narray_data=balanced_train_dataset.values\nx1=array_data[:,1:]\ny1=array_data[:,0:1]\nprint(x1[:5,])\nprint(y1[:5,])","c40a36ef":"# 2. Second technique is used iloc funcation to slice the dataframe from certian range\n\nx2=balanced_train_dataset.iloc[:,1:].values\ny2=balanced_train_dataset.iloc[:,0:1].values\nprint(x2[:5,:])\nprint(y2[:5,])","2788cc9d":"# Importing classifier algorithms\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nneighbors=4","c35b610f":"# Keep all model in one pipeline\n\nmodels=[]\nmodels.append(('Logistic Regression',LogisticRegression()))\nmodels.append(('KNN',KNeighborsClassifier(n_neighbors=neighbors)))\nmodels.append(('SVC',SVC()))\nmodels.append(('Naive Bayes',GaussianNB()))","162b813e":"# Evaluate each model\nimport warnings; \nwarnings.simplefilter('ignore')\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nclassifier_names=[]\nclassifier_predictions=[]\nclassifier_scoring='accuracy'\n\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=5)\n    result=cross_val_score(model,x2,y2,cv=fold,scoring=classifier_scoring)\n    classifier_predictions.append(result)\n    classifier_names.append(name)\n    msg=\"%s: %f and %f\"%(name,result.mean(),result.std())\n    print(msg)","43473962":"# plotting the traning accuracy using boxplot\n\nfig=plt.figure()\nplt.suptitle('Compare algorithm accuracies')\nplt.boxplot(classifier_predictions)\nplt.show()","de4df7f9":"# creating pipeline with feature scaling followed by algorithm\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Normalizer","ffa19c77":"# Algorithms\n\nscaled_models=[]\nscaled_models.append((\"Scaled Logisitic Regression\",Pipeline([('normalizer',Normalizer()),\n                                                              ('scaled Logistic Regression',LogisticRegression())])))\nscaled_models.append(('Scaled KNN',Pipeline([('normalizer',Normalizer()),\n                                             ('scaled Knn',KNeighborsClassifier(n_neighbors=neighbors))])))\nscaled_models.append(('Scaled Naive Bayes',Pipeline([('Normalizer',Normalizer()),\n                                                     ('Scaled Naive bayes',GaussianNB())])))\nscaled_models.append(('scaled SVC',Pipeline([('normalizer',Normalizer()),\n                                             ('Scaled SVC',SVC())])))","5fa5e595":"# Evaluate each model\n\nscaled_names=[]\nscaled_predictions=[]\nfor name,model in scaled_models:\n    fold=KFold(n_splits=10,random_state=5)\n    result=cross_val_score(model,x2,y2,cv=fold,scoring=classifier_scoring)\n    scaled_predictions.append(result)\n    scaled_names.append(name)\n    msg=\"Algorithm : %s, mean: %f, Std_dev:%f\"%(name,result.mean(),result.std())\n    print(msg)","7b739a54":"fig=plt.figure()\nplt.suptitle(\"Scaled Algorithm Accuracies\")\nplt.boxplot(scaled_predictions)\nplt.show()","e4ad762d":"# Navie Bayes doesn't have parameters to tune, So accuracy remains same.\nGaussianNB().get_params().keys()","6c9c4673":"# Regularization hyperparameter tunning for KNN.\nKNeighborsClassifier().get_params().keys()","990b4532":"# Regularization Hyperparameter Tunning\n\nfrom sklearn.model_selection import GridSearchCV\nscaled_xtrain=Normalizer().fit_transform(x2)","c4201526":"# Hyper parameters for KNN algorithm\nparam_grid=dict(n_neighbors=[2,3,4,5,6,7,8,9,10],weights=['uniform', 'distance'],p=[1,2])\n\n# GridSearchCV to tune and identify proper parameter.\nmodel=KNeighborsClassifier()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=classifier_scoring,cv=fold,n_jobs=-1)\ngrid_result=grid.fit(scaled_xtrain,y2)\n\nprint(\"best parameters: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","e0f54f44":"# Importing the ensemble boosting and bagging algorithms\n\n# Bagging \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n'''import xgboost as xgb''' # Beacuse i didnt installed XGBoost in my system i will update once i installed","60be0fe3":"# holding feature scaling and algorithm as a pipeline technqiue\n\nestimators=10\nensemble_models=[]\nensemble_models.append(('Scaled Random Forest',Pipeline([('Scaling',Normalizer()),\n                                                         ('Random Forest',RandomForestClassifier(n_estimators=estimators))])))\nensemble_models.append(('Scaled Decision Tree',Pipeline([('Scaling',Normalizer()),\n                                                         ('Decision Tree',DecisionTreeClassifier())])))\nensemble_models.append(('Scaled Extra Tree',Pipeline([('SCaling',Normalizer()),\n                                                      ('Extra Tree',ExtraTreesClassifier(n_estimators=estimators))])))\nensemble_models.append(('Scaled Ada Boost',Pipeline([('Scaling',Normalizer()),\n                                                     ('Ada Boost',AdaBoostClassifier())])))\nensemble_models.append(('Scaled Gradient Boost',Pipeline([('Scaling',Normalizer()),\n                                                          ('Gradient Boosting',GradientBoostingClassifier())])))","5fcb5472":"# Evaluating each ensemble models\n\nensemble_names=[]\nensemble_predictions=[]\n\nfor name,model in ensemble_models:\n    fold=KFold(n_splits=10,random_state=5)\n    result=cross_val_score(model,x2,y2,cv=fold,scoring=classifier_scoring)\n    ensemble_predictions.append(result)\n    ensemble_names.append(name)\n    msg= \" algorithm: %s , mean accuracy: %f and Std_dev: %f\"%(name,result.mean(),result.std())\n    print(msg)","e0527f50":"# Visualize the result\nfig=plt.figure()\nplt.suptitle('Ensemble Algorithms')\nplt.boxplot(ensemble_predictions)\nplt.show()","502b7a44":"# Regularization hyper parameter tunning for Random Forest using GridSearchCV\nscaled_xtrain=Normalizer().fit_transform(x2)\nmodel=RandomForestClassifier()\nfold=KFold(n_splits=10,random_state=5)\n\nparam_grid=dict(n_estimators=[2,4,6,8,10,12,14,16,18,20,25,30,35,40,45,50,100],criterion=['gini', 'entropy'])\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,cv=fold,scoring=classifier_scoring,n_jobs=-1)\n\ngrid_result=grid.fit(scaled_xtrain,y2)\nprint(\"Best:%f using %s \"%(grid_result.best_score_,grid_result.best_params_))","6863993b":"# Regularization hyper parameter tunning for Random Forest using RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nparam_grid=dict(n_estimators=[2,4,6,8,10,12,14,16,18,20,25,30,35,40,45,50,100],criterion=['gini', 'entropy'])\ngrid=RandomizedSearchCV(estimator=model,param_distributions=param_grid,cv=fold,scoring=classifier_scoring,n_jobs=-1)\n\ngrid_result=grid.fit(scaled_xtrain,y2)\nprint(\"Best:%f using %s \"%(grid_result.best_score_,grid_result.best_params_))","c4cf4ce4":"# Regularization hyper parameter tunning for Extra Tree using GridSearchCV\nscaled_xtrain=Normalizer().fit_transform(x2)\nmodel=ExtraTreesClassifier()\nfold=KFold(n_splits=10,random_state=5)\n\nparam_grid=dict(n_estimators=[2,4,6,8,10,12,14,16,18,20,25,30,35,40,45,50,100],criterion=['gini', 'entropy'])\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,cv=fold,scoring=classifier_scoring,n_jobs=-1)\n\ngrid_result=grid.fit(scaled_xtrain,y2)\nprint(\"Best:%f using %s \"%(grid_result.best_score_,grid_result.best_params_))","8921e647":"# Regularization hyper parameter tunning for Extra Tree using RandomizedSearchCV\nparam_grid=dict(n_estimators=[2,4,6,8,10,12,14,16,18,20,25,30,35,40,45,50,100],criterion=['gini', 'entropy'])\ngrid=RandomizedSearchCV(estimator=model,param_distributions=param_grid,cv=fold,scoring=classifier_scoring,n_jobs=-1)\n\ngrid_result=grid.fit(scaled_xtrain,y2)\nprint(\"Best:%f using %s \"%(grid_result.best_score_,grid_result.best_params_))","a7e75ab5":"# Regualarization hyper parameter tunning for Ada Boost using GridSearchCV\nscaled_xtrain=Normalizer().fit_transform(x2)\nmodel=AdaBoostClassifier()\nfold=KFold(n_splits=10,random_state=5)\n\nparam_grid=dict(n_estimators=[10,15,20,25,30,35,40,45,50,100,150,200,250,300],learning_rate=[0.01,0.1,0.2,0.3,0.4,0.5,0.6])\ngrid=GridSearchCV(estimator=model,cv=fold,param_grid=param_grid,n_jobs=-1,scoring=classifier_scoring)\n\ngrid_result=grid.fit(scaled_xtrain,y2)\nprint(\"Best %f using %s\"%(grid_result.best_score_,grid_result.best_params_))","7b8ffe96":"# Regularization hyper parameter tunning for Ada Boost using RandomizedSearchCV\nparam_grid=dict(n_estimators=[10,15,20,25,30,35,40,45,50,100,150,200,250,300],learning_rate=[0.01,0.1,0.2,0.3,0.4,0.5,0.6])\ngrid=RandomizedSearchCV(estimator=model,param_distributions=param_grid,cv=fold,scoring=classifier_scoring,n_jobs=-1)\n\ngrid_result=grid.fit(scaled_xtrain,y2)\nprint(\"Best:%f using %s \"%(grid_result.best_score_,grid_result.best_params_))","6a427a40":"# Regularisation hyper prameter tunning for Gradient boosting using GridSearchCV\n\nmodel=GradientBoostingClassifier()\nfold=KFold(n_splits=10,random_state=5)\n\nparam_grid=dict(learning_rate=[0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7],\n            n_estimators=[10,20,30,50,100,150,200,250])\ngrid=GridSearchCV(estimator=model,cv=fold,scoring=classifier_scoring,param_grid=param_grid,n_jobs=-1)\n\ngrid_result=grid.fit(scaled_xtrain,y2)\nprint(\"Best : %f using %s\"%(grid_result.best_score_,grid_result.best_params_))","cd5f69dd":"#Regularisation hyper prameter tunning for Gradient boosting using RandomizedSearchCV\nparam_grid=dict(learning_rate=[0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7],\n            n_estimators=[10,20,30,50,100,150,200,250])\ngrid=RandomizedSearchCV(estimator=model,param_distributions=param_grid,cv=fold,scoring=classifier_scoring,n_jobs=-1)\n\ngrid_result=grid.fit(scaled_xtrain,y2)\nprint(\"Best:%f using %s \"%(grid_result.best_score_,grid_result.best_params_))","6b465702":"# importing libraries\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score","2cc2c1d1":"# Rescaling training and test set\nscaled=Normalizer().fit(x2)\nscaled_xtrain=scaled.transform(x2)\nscaled_xtest=scaled.transform(test_dataset)","edff0a99":"# Algorithm fitting\nmodel=GradientBoostingClassifier(n_estimators=100,learning_rate=0.1)\nmodel_fit=model.fit(scaled_xtrain,y2)","d1e26de9":"# Predicting Algorithm \ny_pred=model_fit.predict(scaled_xtest)\nprint(y_pred)","653c741e":"# output\noutput=pd.DataFrame({'PassengerId':test_dataset1.PassengerId,'Survived':y_pred})\noutput.to_csv(\"my_submission.csv\",index=False)\nprint(\"Submission sucessfully\")","2eb3bf24":"Thus, the boosting algorithm combines a number of weak learners to form a strong learner. The individual models would not perform well on the entire dataset, but they work well for some part of the dataset. Thus, each model actually boosts the performance of the ensemble.\n![image.png](attachment:image.png)","10a46f95":"**1a.Importing Dataset**","50f3604c":"Sounds really great, accuracy increased from 63.6% to 72.2%, with best parameter tunning... Lets look how it works on ensemble algorithms\n\n**4d. Ensemble Algorithms (Bagging and Boosting Algorithms)**\n\nWhen you want to purchase a new car, will you walk up to the first car shop and purchase one based on the advice of the dealer? It\u2019s highly unlikely.\n\nYou would likely browser a few web portals where people have posted their reviews and compare different car models, checking for their features and prices. You will also probably ask your friends and colleagues for their opinion. In short, you wouldn\u2019t directly reach a conclusion, but will instead make a decision considering the opinions of other people as well.\n\nEnsemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. This can be achieved in various ways, which you will discover\n\nLet\u2019s understand the concept of ensemble learning with an example. Suppose you are a movie director and you have created a short movie on a very important and interesting topic. Now, you want to take preliminary feedback (ratings) on the movie before making it public. What are the possible ways by which you can do that?\n\nA: You may ask one of your friends to rate the movie for you. Now it\u2019s entirely possible that the person you have chosen loves you very much and doesn\u2019t want to break your heart by providing a 1-star rating to the horrible work you have created.\n\nB: Another way could be by asking 5 colleagues of yours to rate the movie. This should provide a better idea of the movie. This method may provide honest ratings for your movie. But a problem still exists. These 5 people may not be \u201cSubject Matter Experts\u201d on the topic of your movie. Sure, they might understand the cinematography, the shots, or the audio, but at the same time may not be the best judges of dark humour.\n\nC: How about asking 50 people to rate the movie? Some of which can be your friends, some of them can be your colleagues and some may even be total strangers.\n\nThe responses, in this case, would be more generalized and diversified since now you have people with different sets of skills. And as it turns out \u2013 this is a better approach to get honest ratings than the previous cases we saw.\n\nWith these examples, you can infer that a diverse group of people are likely to make better decisions as compared to individuals. Similar is true for a diverse set of models in comparison to single models. This diversification in Machine Learning is achieved by a technique called Ensemble Learning.","7aef9cc8":"Above plot was bit akward. but its clear there is not","0c108f31":"We can see more information about categorical encoding here https:\/\/pbpython.com\/categorical-encoding.html","09a5904b":"As per above result, we have 38% servived dataset and 62% non- survived peoples.\n\nNow we have 2 options, they are:\n\nwe can go ahead with this dataset. its not bad! we can say ok to build the model.\nIf we need accurate accuracy we should balance this unablanced dataset into balanced. so we will get better predictions.\n\nNote: Notice how imbalanced is our original dataset! Most of the dataset are non-servived. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most dataset are not servived. But we don't want our model to assume, we want our model to detect patterns that give signs of servived!","90178988":"Data preprocessing is the important pipeline to Building a data science project. We do lot of process of cleaning and extract insights for bussiness problem.","1396ef4b":"**1b. Descriptive Statistical Analysis**","1c2273bb":"**4g. Regularization Hyper Parameter Tunning For Ada Boost Classifier**","f091e92a":"Outliers found between Fare and Survived, we'r gone remove greater than 300 fare.","4d18dbdb":"By using train_test_split class we are going to split the training and test set bsaed on percentage ratio.....","a06681cd":"Bagging Algorithms:\n    \n1. Bagged Decision Tree\n2. Random Forest\n3. Extra Tree\n4. Boosting Algorithms:\n\nAdaBoost\n\n1. Gradient Descent (Stochastic Gradient Boosting)\n2. XGBoost (Extra Gradient Boosting)","fe733194":"***Our Goals:***\n1. Understand the little distribution of the \"little\" data that was provided to us.\n2. Create a 50\/50 sub-dataframe ratio of \"Survived\" and \"Non-Survived\" onboarded people.\n3. Determine the Classifiers we are going to use and decide which one has a higher accuracy.\n4. Create a Neural Network and compare the accuracy to our best classifier.\n5. Understand common mistaked made with imbalanced datasets.","e885c810":"1. Bagging:\n\nThe idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result. Here\u2019s a question: If you create all the models on the same set of data and combine it, will it be useful? There is a high chance that these models will give the same result since they are getting the same input. So how can we solve this problem? One of the techniques is bootstrapping.\n\nBootstrapping is a sampling technique in which we create subsets of observations from the original dataset, with replacement. The size of the subsets is the same as the size of the original set.\n\nBagging (or Bootstrap Aggregating) technique uses these subsets (bags) to get a fair idea of the distribution (complete set). The size of subsets created for bagging may be less than the original set.\n\n![image.png](attachment:image.png)","cac9ce8b":"***2d. Handling Outliers ( Removing outlier rows)***\n\nIf outliers exist in our dataset, then our model predicition wont be accuracte.\n\nIt might overfit or underfit the training dataset. so it wont extract the insights and test set performance wont be good.\n\nI mainly focused on 2 techniques to remove outliers.\n\nAnalyze each attribute and remove outlier mannual threshold..\nUsing threshold upper and lower values..\ntechnique 1 will be used when we have less no.of feature attribute below 15. if more attribute then better option is go with technique 2..\n\n***Technique 1: Analyze attribute and remove outlier with mannual thredhold***","5fff60a3":"# 2. Feature Engineering","86ed27a9":"**2a. Imputation or Data Cleaning**","7d40e74a":"**2c. OneHotEncoder ( Encoding numerical values in dummy values)**\n\nOne hot encoder is very useful to encode value from numerical values into dummy values...like we have 1,2,3,4 different kind of values in an attribute.. So by applying onehot encoder it will create 3 attribute like binary code( 1- 0 0 1, 2 - 0 1 0, 3 - 0 1 1, 4- 1 0 0) likewise.\n\nIt will be a good technique, if we have few number of numerical values. so dummy values attribute will create few only. suppose if you have more then 100 numerica value then dummy attribute will increase gradually. dimensation and vector quantity will increase rapidely.\n\nonehotencoder having an issue with dummy variable trap. to handle, we need to perform this logic (dummy attribute=1-total dummy attributes).\n\nso better we use feature scaling it will rescale all values from different range to same range -1 to 1 or 0 to 1....","a9ba3223":"# 4. Model Selection","0820366b":"**4a. Model Selection Using Spot Check and Compare Algorithms Without Feature Scaling**","c8548fb3":"There are no outliers!!!","997a952c":"**3b. Feature Importance**\n\nYou can get the feature importance of each feature of your dataset by using the feature importance property of the model.\n\nFeature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\n\nFeature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset basically if we have less than 10 features then better use all features.","8bb9984b":"**4b. Model Selection Using Spot Check and Compare Algorithms With Feature Scaling**","1d09b930":"As per the above train and test dataset display. we have lot of info like datatypes,null values, null type( null or non null).\n\nSo we dont no exactly, How? many missing values from each attribute. so below isna() will print numerical number count for each attribute.","5f61a447":"Correlation value between 2 indepedent variable","a89a1395":"Encoding the categorical text to numerical values.","0cdccb8b":"Feature selection is to ignore irrelevant or partially relevant features can negatively impact model performance.\n\nIdentifying the related features from a set of data and removing the irrelevant or less important features with do not contribute much to our target variable in order to achieve better accuracy for our model.\n\nFeature Selection is one of the core concepts in machine learning which hugely impacts the performance of your model. The data features that you use to train your machine learning models have a huge influence on the performance you can achieve.\n\nFeature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in.\n\nHaving irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.\n\nHow to select features and what are Benefits of performing feature selection before modeling your data? \u00b7 Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise. \u00b7 Improves Accuracy: Less misleading data means modeling accuracy improves. \u00b7 Reduces Training Time: fewer data points reduce algorithm complexity and algorithms train faster.\n\nI want to share my personal experience with this.\n\nI prepared a model by selecting all the features and I got an accuracy of around 65% which is not pretty good for a predictive model and after doing some feature selection and feature engineering without doing any logical changes in my model code my accuracy jumped to 81% which is quite impressive\n\nNow you know why I say feature selection should be the first and most important step of your model design.\n\nFeature Selection Methods:\n\nI will share 3 Feature selection techniques that are easy to use and also gives good results.\n\n1. Univariate Selection\n2. Feature Importance\n3. Correlation Matrix with Heatmap","59773c5d":"**2b. Label Encoder**","27c746d2":"**2h. Feature Scaling( scaling input attributes).**\n\nI'm going to apply feature scaling within model itself.\n\n**2i. Feature Split ( splitting training and test set).**\n\nSplitting the features into training and test sets.\n\nBefore Splitting the dataset into training and tests. We need to convert the dataset from dataframe to array of matrix for that, I preferd to approach 2 techniques. They are...","7420e19f":"# 3. Feature Selection","2bdb0b9b":"# x2 is the input attribute for training set and y2 is the output attirbute for training set.","314969b1":"We got Good accuray above 75% that sounds fabulous.... Without applying the regualarization hyper parameter tunning..\n\nNow we are applying hyper parameter tunning to all this algorithms","946d8cff":"As absorbed above both train and test null values, we have maximum missing values in Cabin attibute. so we are going to drop that feature attribute.\n\nWe are going to replace the age attribute missing values with mean of that attribute.\n\nDisplaying the missing ratio for test set\n\nWe have 2 options either we can remove missing value row or else replace missing values with mean of that attribute.\n\nso here, I'm selected 2 option missing values replace with mean.","38a4bd95":"8. Similarly, multiple models are created, each correcting the errors of the previous model.\n9. The final model (strong learner) is the weighted mean of all the models (weak learners)\n![image.png](attachment:image.png)","7fe33d41":"Statistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n\nThe example below uses the chi-squared (chi\u00b2) statistical test for non-negative features to select 10 of the best features from the Titanic Survived Prediction Dataset.","c6443367":"**4f. Regularization Hyper Parameter Tunning For Extra Tree Classifier**","cd55a151":"We are going to remove the embarked 2 missing values, beacuse embarked is object attribute.","99c22007":"2. Boosting\n\nBefore we go further, here\u2019s another question for you: If a data point is incorrectly predicted by the first model, and then the next (probably all models), will combining the predictions provide better results? Such situations are taken care of by boosting.\n\nBoosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. Let\u2019s understand the way boosting works in the below steps.\n\n1. A subset is created from the original dataset.\n2. Initially, all data points are given equal weights.\n3. A base model is created on this subset.\n4. This model is used to make predictions on the whole dataset\n![image.png](attachment:image.png)","0b7ac981":"As per above plot its clear that logistic regression giving the best accuracy followed by navie bayes classifier....\n\nLets go ahead with model selection with feature scaling","04ffdf70":"After Used below table of content pipeline accuracy imporved from 77.08% to 82%... I will keep updating this kernel with latest techniques","e597b638":"No Outlier found!!!!","7847b0aa":"**4e. Regularization Hyper Parameter Tunning For Random Forest Classifier**","f63bb1d0":"Correlation values isnt visible.... and its bit confusing also, lets go ahead with indeapth analysis..\n\nI am going to show you 2 logical way to see correlation values in heatmap","d626e63f":"As per above selectkbest feature selection we can ingoner irrelavent or partial relevent features (Embarked and Sibsp) columns","d57ee354":"If the original data follows a log-normal distribution or approximately so, then the log-transformed data follows a normal or near normal distribution. In this case, the log-transformation does remove or reduce skewness. Unfortunately, data arising from many studies do not approximate the log-normal distribution so applying this transformation does not reduce the skewness of the distribution. In fact, in some cases applying the transformation can make the distribution more skewed than the original data.\n\nTo show how this can happen, we first simulated data ui which is uniformly distributed between 0 and 1,and then constructed two variables as follows: xi=100(exp(\u03bci-1)+1, yi=log(xi).\n\nShown in the left panel in Figure 1 is the histogram of xi, while the right panel is the histogram of yi (the log-transformed version of xi) based on a sample size of n=10,000. While the distribution of xi is right-skewed, the log-transformed data yi is clearly left-skewed. In fact, the log-transformed data yi is more skewed than the original xi, since the skewness coefficient for yi is 1.16 while that for xi is 0.34. Thus, the log-transformation actually exacerbated the problem of skewness in this particular example.\n\nAn external file that holds a picture, illustration, etc. ![image.png](attachment:image.png)\n\nIn general, for right-skewed data, the log-transformation may make it either right-or left-skewed. If the original data does follow a log-normal distribution, the log-transformed data will follow or approximately follow the normal distribution. However, in general there is no guarantee that the log-transformation will reduce skewness and make the data a better approximation of the normal distribution.\n\nSurvived is the variable we need to predict. So let's do some analysis on this variable first.\n\n''' identiying what kind of skewed we need to apply right or left ? '''\n\nfrom scipy.stats import norm, skew\n\nsb.distplot(balanced_train_dataset['Survived'],fit=norm)\n\n(mean,std)=norm.fit(balanced_train_dataset['Survived']) print(\"mean : %f std: %f\"%(mean,std))\n\nplt.legend(['Normal distribution mean: %f and std: %f'%(mean,std)]) plt.xlabel('Survived') plt.ylabel('Frequency')\n\nfig=plt.figure()\n\nfrom scipy import stats result=stats.probplot(balanced_train_dataset['Survived'],plot=plt) plt.show()\n\nThe target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n\nimport numpy as np balanced_train_dataset['Survived']=np.log1p(balanced_train_dataset['Survived'])\n\nsb.distplot(balanced_train_dataset['Survived'],fit=norm)\n\n(mean,std)=norm.fit(balanced_train_dataset['Survived'])\n\nplt.legend('Norm Distribution mean %.3f and std %.3f'%(mean,std)) plt.xlabel('Frequency') plt.ylabel('Survived') plt.title('Sales Distribution')\n\nfig=plt.figure() stats.probplot(balanced_train_dataset['Survived'],plot=plt) plt.show()\n\nAs per above distribution is skewed happened little bit (frequency compressed a bit)... Because its classification either 0 or 1 so output is discrete... It will work better in regression(continues intervels)","60983ac3":"**3a. Univariate Selection**","7240e538":"Now, we are going to extract the insights of datasets like shape, describe and datatype statistical analysis. Which it helps to analyze the dataset...","bb17b511":"**3c. Correlation Matrix With HeatMap**\n\nCorrelation states how the features are related to each other or the target variable.\n\nCorrelation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)\n\nHeatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the seaborn library.","2accb45b":"1. Multiple subsets are created from the original dataset, selecting observations with replacement.\n2. A base model (weak model) is created on each of these subsets.\n3. The models run in parallel and are independent of each other.\n4. The final predictions are determined by combining the predictions from all the models.\n![image.png](attachment:image.png)","1920bd45":"**2g. Log Transformation**\n\nThe log-transformation is widely used in biomedical and psychosocial research to deal with skewed data. This paper highlights serious problems in this classic approach for dealing with skewed data. Despite the common belief that the log transformation can decrease the variability of data and make data conform more closely to the normal distribution, this is usually not the case. Moreover, the results of standard statistical tests performed on log-transformed data are often not relevant for the original, non-transformed data.We demonstrate these problems by presenting examples that use simulated data. We conclude that if used at all, data transformations must be applied very cautiously. We recommend that in most circumstances researchers abandon these traditional methods of dealing with skewed data and, instead, use newer analytic methods that are not dependent on the distribution the data, such as generalized estimating equations (GEE).\n\nKeywords: hypothesis testing, outliners, lon-normal distribution, normal distribution, skewness\n\n**Using the log transformation to make data conform to normality:**\n\nThe normal distribution is widely used in basic and clinical research studies to model continuous outcomes. Unfortunately, the symmetric bell-shaped distribution often does not adequately describe the observed data from research projects. Quite often data arising in real studies are so skewed that standard statistical analyses of these data yield invalid results. Many methods have been developed to test the normality assumption of observed data. When the distribution of the continuous data is non-normal, transformations of data are applied to make the data as \"normal\" as possible and, thus, increase the validity of the associated statistical analyses. The log transformation is, arguably, the most popular among the different types of transformations used to transform skewed data to approximately conform to normality.\n","a55c0421":"As per above scaled result KNN and naive bayes accuracy is good. Accuracy is not good as excepted, but we have a ways to imporve accuries by using hyper parameter tunning\n\n**4c. Regularization Hyper Parameter Tunning For Navie Bayes and KNN**\n\n**GridSearchCV**\n\nGrid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.\n\nIf you work with ML, you know what a nightmare it is to stipulate values for hyper parameters. There are libraries that have been implemented, such as GridSearchCV of the sklearn library, in order to automate this process and make life a little bit easier for ML enthusiasts.\n\nFirst, we need to import GridSearchCV from the sklearn library, a machine learning library for python. The estimator parameter of GridSearchCV requires the model we are using for the hyper parameter tuning process. For this example, we are using the rbf kernel of the Support Vector Regression model(SVR). The param_grid parameter requires a list of parameters and the range of values for each parameter of the specified estimator. The most significant parameters required when working with the rbf kernel of the SVR model are c, gamma and epsilon. A list of values to choose from should be given to each hyper parameter of the model. You can change these values and experiment more to see which value ranges give better performance. A cross validation process is performed in order to determine the hyper parameter value set which provides the best accuracy levels.\n\n1. gsc = GridSearchCV(\n\n   estimator=SVR(kernel='rbf'),\n   param_grid={\n     'C': [0.1, 1, 100, 1000],\n     'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n     'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]\n   },\n   cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n   grid_result = gsc.fit(X, y)\n\n   best_params = grid_result.best_params_\n\n2. best_svr = SVR(kernel='rbf', C=best_params[\"C\"], epsilon=best_params[\"epsilon\"], gamma=best_params[\"gamma\"],\n\n     coef0=0.1, shrinking=True,tol=0.001, cache_size=200, verbose=False, max_iter=-1)\n\nWe then use the best set of hyper parameter values chosen in the grid search, in the actual model as shown above.","1425ae1d":"**Technique 2:Using threshold upper and lower values..**\n\n**Anomaly Detection:**\n\nOur main aim in this section is to remove \"extreme outliers\" from features that have a high correlation with our classes. This will have a positive impact on the accuracy of our models.\n\n**Interquartile Range Method:**\n\n**Interquartile Range (IQR):** We calculate this by the difference between the 75th percentile and 25th percentile. Our aim is to create a threshold beyond the 75th and 25th percentile that in case some instance pass this threshold the instance will be deleted.\n\n**Boxplots:** Besides easily seeing the 25th and 75th percentiles (both end of the squares) it is also easy to see extreme outliers (points beyond the lower and higher extreme).\n\n**Outlier Removal Tradeoff:** We have to be careful as to how far do we want the threshold for removing outliers. We determine the threshold by multiplying a number (ex: 1.5) by the (Interquartile Range). The higher this threshold is, the less outliers will detect (multiplying by a higher number ex: 3), and the lower this threshold is the more outliers it will detect.\n\n**The Tradeoff:** The lower the threshold the more outliers it will remove however, we want to focus more on \"extreme outliers\" rather than just outliers. Why? because we might run the risk of information loss which will cause our models to have a lower accuracy. You can play with this threshold and see how it affects the accuracy of our classification models.\n\n**Summary:**\n\nVisualize Distributions: We first start by visualizing the distribution of the feature we are going to use to eliminate some of the outliers. V14 is the only feature that has a Gaussian distribution. Determining the threshold: After we decide which number we will use to multiply with the iqr (the lower more outliers removed), we will proceed in determining the upper and lower thresholds by substrating q25 - threshold (lower extreme threshold) and adding q75 + threshold (upper extreme threshold).\n\nConditional Dropping: Lastly, we create a conditional dropping stating that if the \"threshold\" is exceeded in both extremes, the instances will be removed. Boxplot Representation: Visualize through the boxplot that the number of \"extreme outliers\" have been reduced to a considerable amount.\n\n**Note:** After implementing outlier reduction our accuracy has been improved by over 3%! Some outliers can distort the accuracy of our models but remember, we have to avoid an extreme amount of information loss or else our model runs the risk of underfitting.\n\nReference: More information on Interquartile Range Method: How to Use Statistics to Identify Outliers in Data by Jason Brownless https:\/\/machinelearningmastery.com\/how-to-use-statistics-to-identify-outliers-in-data\/ (Machine Learning Mastery blog)","71228e8b":"No outlier identified!!!","52a51b20":"For more information about imbalanced dataset refer this site https:\/\/towardsdatascience.com\/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb","73a2d202":"***Before we Begin:***\n\nIf you liked my work, please upvote this kernel since it will keep me motivated to perform more in-depth reserach towards this subject and will look for more efficient ways so that our models are able to detect more accurately both survived and non-survived people. For more kernel please follow me and any questions please mention below discussion.","92c88ec9":"**1c. Descriptive Statistical Visualisation Analysis**","f29bb0eb":"As we can absorb above density plot different column attributes obey different distribution... We need to apply Feature scaling to keep all attribute values in same range scale.","fae38bcc":"**4h. Regularization Hyper Parameter Tunning For Gradient Boosting Classifier**","450ecdd9":"# 1. Data Preprocessing","8197a91e":"Feature Engineering is very important step in data preprocessing...\n\nFeature engineering techniques:\n    \n    2a. Imputation or Data Cleaning\n    2b. Label Encoder\n    2c. OneHotEncoder\n    2d. Handling Outliers\n    2e. Converting Imbalance Dataset to Balanced Dataset\n    2f. Binning\n    2g. Log Transformation\n    2h. Feature Scaling\n    2i. Feature Split","1a08d707":"**Correlation values based on output attribute**","cb954e44":"Above Pclass attribute look like exponential distribution. Lets go ahead with other attributes","ed1ed31f":"Handling the missing or empty values in both train and test dataset.","e9a6f3a4":"5. Errors are calculated using the actual values and predicted values.\n6. The observations which are incorrectly predicted, are given higher weights.(Here, the three misclassified blue-plus points will be given higher weights)\n7. Another model is created and predictions are made on the dataset.(This model tries to correct the errors from the previous model)\n![image.png](attachment:image.png)","dc322c3e":"We have 177, 687 and 2 null values on respective attributes.\n\n**Missing value ratio on both training and test set**\n\nDisplaying the missing value ratio on training dataset","f9afc396":"**Removing unwanted attributes**","1e018d81":"No outliers found!!!","c9b66433":"Train dataset having 891 rows(passangers) and 12 column (Features)attributes.","71574a23":"**1d. Visualisation Each Attribute Using Seaborn**","266c9381":"**Converting Imbalance Dataset to Balanced Dataset**\n\nIdentifying the dataset is whether balanced or imbalanced data ?","74e00f86":"All null values removed from train and test dataset, So now we can safely go ahead with other process.","efca054a":"We can remove the unwanted attribute like name and passengerid's. this attributes are not much important to extract insights.","9c9d16b3":"I showed 3 feature selection techniques... so i am taking count of correlation matrix with heatmap... you guys can try 3 selection and keep best selection technqiue based on best prediction one....\n\nAs per above correlation values Age and sibsp attribute have less correlation.. so now we can neglet those 2 rows now...","615e54f0":"**What is a sub-Sample?** In this scenario, our subsample will be a dataframe with a 50\/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n\nW**hy do we create a sub-Sample?** In the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe will cause the following issues:\n\n**Overfitting:** Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs.\n\n**Wrong Correlations:** Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features.\n\n**Random Under-Sampling:**\n\nIn this phase of the project we will implement \"Random Under Sampling\" which basically consists of removing data in order to have a more balanced dataset and thus avoiding our models to overfitting.\n\n**Steps:**\n\n1. The first thing we have to do is determine how imbalanced is our class (use \"value_counts()\" on the class column to determine the amount for each label)\n2. Once we determine how many instances are considered servived dataset (servived = \"1\") , we should bring the non-servivide dataset to the same amount as servived dataset (assuming we want a 50\/50 ratio), this will be almost equivalent to 336 cases of servived and 336 cases of non-servived dataset.\n3. After implementing this technique, we have a sub-sample of our dataframe with a 50\/50 ratio with regards to our classes. Then the next step we will implement is to shuffle the data to see if our models can maintain a certain accuracy everytime we run this script.\n\n**Note:** The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of information loss (bringing 336 non-servived dataset from 538 non-servived dataset)","b57bd11e":"We tunned lot of classifier algorithms as well as ensemble algorithms....Among them Gradient Boosting giving the best accuracy around 81.9% accuracy... So lets train and test gradient boosting algorithm\n\n**4i. Gradient Boosting Algorithm For Titanic Survived Predictions.**","bb96896d":"**Without DataFrame:**\n\nIf we already converted dataset from dataframe into array vector, With the help of sklearn library we can eassly convert categorical to numerical\n\n1. from sklear.preprocessing import LabelEncoder\n2. label=LabelEncoder()\n3. dataset[:,2] = label.fit_transform(dataset[:,2]) --- encoding sex attribute\n4. dataset[:,6] = label.fit_transform(dataset[:,6]) --- encoding ticket attribute\n5. dataset[:,8] = label.fit_transform(dataset[:,8]) --- encoding embarked attribute\n6. dataset.head()\nsee result all those attribute are encoded with numerical values. then safely we can go ahead.\n\n**With Dataframe:**\n\nIf we not yet converted dataset from dataframe to array of vector then label encoding technique we should use uniquly.....\n\nHere also we can use 2 techniques,\n\n**First technique(mannual technique):** If we have more than 100 attribute it will hard to follow this technique\n\n1. dataset['Sex'].unqiue()---- We will get different kind of text like male and female\n2. dataset['sex']=dataset['sex'].map({'male':0,'female':1}) --- we mapped male the male value as 0 and female value as 1...\n3. dataset.head() ---- we can check those text update with numerics....\n\n**Second technique ( Automatic technique) :** This technique use sklearn library.....\n\n1. from sklearn.preprocessing import LabelEncoder\n2. label=LabelEncoder()\n3. dataset['sex']=label.fit_transform(dataset['sex'])----it look bit simlar with above code but this syntax uses for dataframe\n4. dataset.head() ----- we can check those text update with numerics....","39622630":"We sucessfully removed outlier value after age 70 years...","83c7b88e":"As per above plot, we have some outlier after 80 age, so we are going to remove after 80 years age.","ea82822f":"Test dataset having 418 rows(passangers) and 11 column (Features) attributes.","46d466ca":"**Table of content:**\n\n   **1. Data Preprocessing**\n\n    1a. Importing Datasets\n    1b. Descriptive Statistical Analysis\n    1c. Descriptive Statistical Visualisation Analysis\n    1d. Visualisation Each Attribute Using Seaborn\n    \n  **2. Feature Engineering**\n  \n     2a. Imputation or Data Cleaning\n     2b. Label Encoder\n     2c. OneHotEncoder\n     2d. Handling Outliers\n     2e. Converting Imbalance Dataset to Balanced Dataset\n     2f. Binning\n     2g. Log Transformation\n     2h. Feature Scaling\n     2i. Feature Split\n     \n   **3. Feature Selection**\n   \n     3a. Univariate Selection\n     3b. Feature Importance\n     3c. Correlation Matrix With HeatMap\n     3d. Principal Component Analysis (PCA)\n     3e. Discrimenent Component Analysis (DCA)\n    \n   **4. Model Selection**\n   \n     4a. Model Selection Using Spot Check and Compare Algorithms Without Feature Scaling\n     4b. Model Selection Using Spot Check and Compare Algorithms With Feature Scaling\n     4c. Regularization Hyper Parameter Tunning For Navie Bayes and KNN\n     4d. Ensemble Algorithms (Bagging and Boosting Algorithms)\n     4e. Regularization Hyper Parameter Tunning For Random Forest Classifier\n     4f. Regularization Hyper Parameter Tunning For Extra Tree Classifier\n     4g. Regularization Hyper Parameter Tunning For Ada Boost Classifier\n     4h. Regularization Hyper Parameter Tunning For Gradient Boosting Classifier\n     4i. Gradient Boosting Algorithm For Titanic Survived Predictions."}}