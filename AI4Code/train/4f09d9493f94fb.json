{"cell_type":{"edbc33b9":"code","423963e0":"code","a7de7675":"code","b569b650":"code","e2579903":"code","8ce33142":"code","7d34e330":"code","40a2ae5c":"code","4fd09d2e":"code","e438338c":"code","e3d9fb5a":"code","6db7d439":"code","9841bf8f":"code","84c52f31":"code","ee28bdc1":"code","d2882192":"code","53e92b9f":"code","64b5a6b9":"code","32e47d9c":"code","fa167dd0":"code","fbe1684f":"code","fcba0790":"code","309a3dec":"code","b76c3fe8":"code","b30eacbb":"code","2ae9673d":"code","a1550cd4":"code","d6919b36":"code","77502d0a":"code","f4e5d4bf":"code","94cef658":"code","e89982e3":"code","a92900dd":"code","aa1a64ac":"code","f03e0b2f":"code","6cc74931":"code","a191804f":"code","f8703e72":"code","672ca6b0":"code","285e398d":"code","b8059adb":"code","838aba81":"code","8aa6d3c5":"code","dc2f33f0":"code","a9b5df7e":"code","a2af8b44":"code","3a483098":"code","79652179":"code","3bb582be":"markdown","034e7160":"markdown","6db1ecf6":"markdown","cb9e9a9a":"markdown","3625d9c9":"markdown","55cd808b":"markdown","bae8b1ae":"markdown","8316a466":"markdown","27c9cacf":"markdown","8c673b8b":"markdown","3022b00b":"markdown","9bf5b253":"markdown","8e3ae15e":"markdown","bf3b3a0a":"markdown","d8e36686":"markdown","301ee745":"markdown","70f74009":"markdown","8923ef3c":"markdown","e92c9466":"markdown","fe5caf27":"markdown","e8eb5a12":"markdown","23f77bf3":"markdown","5dc50fc5":"markdown","f685c17b":"markdown","ad075feb":"markdown","c3f6af80":"markdown","a25b2734":"markdown","05a245a3":"markdown","1c89a404":"markdown","726cd7ea":"markdown","1e464022":"markdown","f50b9a41":"markdown","a4ff8dfd":"markdown","c9a6d087":"markdown","01f81672":"markdown","e2a17a7b":"markdown","c754c04d":"markdown","bf31cae1":"markdown","5742cd60":"markdown","f037b377":"markdown","40a29ae8":"markdown","be4e64da":"markdown","93af3e40":"markdown","b30567bc":"markdown","40486002":"markdown","0bfccb55":"markdown","266baec5":"markdown","608fce56":"markdown","35350bca":"markdown","dc5d5b43":"markdown","26afc54d":"markdown","54327d7e":"markdown","b6cd0894":"markdown","b127018a":"markdown","2bbf764a":"markdown","81d216b1":"markdown","cdbfb840":"markdown","50ffcda0":"markdown","07753d2c":"markdown","ffad5cf3":"markdown","f7797783":"markdown","df65636c":"markdown","726d20d8":"markdown","10fae5c0":"markdown","864f0b22":"markdown","07b59ae8":"markdown","b31d9eac":"markdown","181ac3dc":"markdown","b4f84519":"markdown","2f2a4430":"markdown","7a522e36":"markdown","ed44a6b7":"markdown","191fa6f6":"markdown","1b822e31":"markdown","c3762a09":"markdown","d278bc86":"markdown","4ef80608":"markdown","0c48763b":"markdown","41704090":"markdown","3c241236":"markdown","09a097a6":"markdown","f02310ef":"markdown","bda86a88":"markdown","3d6f90de":"markdown","49207688":"markdown","79555f32":"markdown","9c7b6f2b":"markdown","3c3f28f5":"markdown","e774892d":"markdown","01f49523":"markdown","f465fd16":"markdown","fdd36eca":"markdown","04f38643":"markdown","c8758c14":"markdown","3c091b0b":"markdown","e684398a":"markdown","a06e1ef8":"markdown","a0ed8d1c":"markdown","40fd8be7":"markdown"},"source":{"edbc33b9":"!pip install -q pycountry","423963e0":"import os\nimport gc\nimport re\nimport folium\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport math\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport pycountry\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nimport nltk\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport random\nimport networkx as nx\nfrom pandas import Timestamp\n\nimport requests\nfrom IPython.display import HTML","a7de7675":"import seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\ntqdm.pandas()\nnp.random.seed(0)\n%env PYTHONHASHSEED=0\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b569b650":"DATA_PATH = \"..\/input\/CORD-19-research-challenge\/\"\nCLEAN_DATA_PATH = \"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/\"\n\npmc_df = pd.read_csv(CLEAN_DATA_PATH + \"clean_pmc.csv\")\nbiorxiv_df = pd.read_csv(CLEAN_DATA_PATH + \"biorxiv_clean.csv\")\ncomm_use_df = pd.read_csv(CLEAN_DATA_PATH + \"clean_comm_use.csv\")\nnoncomm_use_df = pd.read_csv(CLEAN_DATA_PATH + \"clean_noncomm_use.csv\")\n\npapers_df = pd.concat([pmc_df,\n                       biorxiv_df,\n                       comm_use_df,\n                       noncomm_use_df], axis=0).reset_index(drop=True)","e2579903":"CORONA_FILE = \"..\/input\/corona-virus-report\/covid_19_clean_complete.csv\"\n\nfull_table = pd.read_csv(CORONA_FILE, parse_dates=['Date'])\n\nfull_table[['Province\/State']] = full_table[['Province\/State']].fillna('')\nfull_table['Country\/Region'] = full_table['Country\/Region'].replace('Mainland China', 'China')\nfull_table['Active'] = full_table['Confirmed'] - full_table['Deaths'] - full_table['Recovered']\n\ncases = ['Confirmed', 'Deaths', 'Recovered', 'Active']\nfull_table[cases] = full_table[cases].fillna(0)\ncases = ['Confirmed', 'Deaths', 'Recovered', 'Active']\nfull_table['Active'] = full_table['Confirmed'] - full_table['Deaths'] - full_table['Recovered']\n\n# replacing Mainland china with just China\nfull_table['Country\/Region'] = full_table['Country\/Region'].replace('Mainland China', 'China')\n\n# filling missing values \nfull_table[['Province\/State']] = full_table[['Province\/State']].fillna('')\nfull_table[cases] = full_table[cases].fillna(0)\n\n# cases in the ships\nship = full_table[full_table['Province\/State'].str.contains('Grand Princess')|full_table['Country\/Region'].str.contains('Cruise Ship')]\n\n# china and the row\nchina = full_table[full_table['Country\/Region']=='China']\nrow = full_table[full_table['Country\/Region']!='China']\n\n# latest\nfull_latest = full_table[full_table['Date'] == max(full_table['Date'])].reset_index()\nchina_latest = full_latest[full_latest['Country\/Region']=='China']\nrow_latest = full_latest[full_latest['Country\/Region']!='China']\n\n# latest condensed\nfull_latest_grouped = full_latest.groupby('Country\/Region')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\nchina_latest_grouped = china_latest.groupby('Province\/State')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\nrow_latest_grouped = row_latest.groupby('Country\/Region')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\n\ntemp = full_table.groupby(['Country\/Region', 'Province\/State'])['Confirmed', 'Deaths', 'Recovered', 'Active'].max()\n# temp.style.background_gradient(cmap='Reds')\n\ntemp = full_table.groupby('Date')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\ntemp = temp[temp['Date']==max(temp['Date'])].reset_index(drop=True)","8ce33142":"def multiple_replace(dict, text):\n    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n    return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)\n\ndef get_countries(names):\n    alphabet = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\",\\\n                \"G\", \"H\", \"I\", \"J\", \"K\", \"L\",\\\n                \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\",\\\n                \"S\", \"T\", \"U\", \"V\", \"W\", \"X\",\\\n                \"Y\", \"Z\"]\n\n    repl_dict = dict(zip([a+\" \" for a in alphabet], [\"\"]*26))\n    repls = []\n    for name in names.split(\", \"):\n        repl = multiple_replace(repl_dict, name.strip().replace(\") \", \"\").replace(\"( \", \"\"))\n        if len(repl.split()) == 1:\n            repl = name[0] + \" \" + repl\n        repl = repl.replace(\";\", \"\").replace(\":\", \"\").replace(\".\", \"\").replace(\",\", \"\")\n        repl = repl.split(\" \")\n\n        for idx in range(len(repl)):\n            if len(repl[idx]) <= 1 and repl[idx] not in alphabet:\n                repl[idx] = \"A\"\n\n        response = client.origin(repl[0], repl[1])\n        repls.append(response.country_origin)\n\n    return repls\n\ncountries = pd.read_csv(\"..\/input\/researcher-countries\/countries.csv\").values[:, 0].tolist()","7d34e330":"cont_list = sorted(list(set(countries)))\ncounts = [countries.count(cont) for cont in cont_list]\ndf = pd.DataFrame(np.transpose([cont_list, counts]))\ndf.columns = [\"Country of origin\", \"Count\"]\npx.bar(df, x=\"Country of origin\", y=\"Count\", title=\"Country of origin of researchers\", template=\"simple_white\")","40a2ae5c":"codes = [pycountry.countries.get(alpha_2=con).name for con in cont_list]\ndf[\"Codes\"] = codes\ndf[\"Count\"] = df[\"Count\"].apply(int)\nfig = px.scatter_geo(df, locations=\"Codes\", size='Count', hover_name=\"Country of origin\",\n                     projection=\"natural earth\", locationmode=\"country names\", title=\"Country of origin of researchers\", color=\"Count\",\n                     template=\"plotly\")\nfig.show()","4fd09d2e":"def new_len(x):\n    if type(x) is str:\n        return len(x.split())\n    else:\n        return 0\n\npapers_df[\"abstract_words\"] = papers_df[\"abstract\"].apply(new_len)\nnums = papers_df.query(\"abstract_words != 0 and abstract_words < 500\")[\"abstract_words\"]\nfig = ff.create_distplot(hist_data=[nums],\n                         group_labels=[\"All abstracts\"],\n                         colors=[\"darkorange\"])\nfig.update_layout(title_text=\"Abstract words\", xaxis_title=\"Abstract words\", template=\"simple_white\", showlegend=False)\nfig.show()","e438338c":"biorxiv_df[\"abstract_words\"] = biorxiv_df[\"abstract\"].apply(new_len)\nnums_1 = biorxiv_df.query(\"abstract_words != 0 and abstract_words < 500\")[\"abstract_words\"]\npmc_df[\"abstract_words\"] = pmc_df[\"abstract\"].apply(new_len)\nnums_2 = pmc_df.query(\"abstract_words != 0 and abstract_words < 500\")[\"abstract_words\"]\ncomm_use_df[\"abstract_words\"] = comm_use_df[\"abstract\"].apply(new_len)\nnums_3 = comm_use_df.query(\"abstract_words != 0 and abstract_words < 500\")[\"abstract_words\"]\nnoncomm_use_df[\"abstract_words\"] = noncomm_use_df[\"abstract\"].apply(new_len)\nnums_4 = noncomm_use_df.query(\"abstract_words != 0 and abstract_words < 500\")[\"abstract_words\"]\nfig = ff.create_distplot(hist_data=[nums_1, nums_2, nums_3, nums_4],\n                         group_labels=[\"Biorxiv\", \"PMC\", \"Commerical\", \"Non-commercial\"],\n                         colors=px.colors.qualitative.Plotly[4:], show_hist=False)\n\nfig.update_layout(title_text=\"Abstract words vs. Paper type\", xaxis_title=\"Abstract words\", template=\"plotly_white\")\nfig.show()","e3d9fb5a":"def polarity(x):\n    if type(x) == str:\n        return SIA.polarity_scores(x)\n    else:\n        return 1000\n    \nSIA = SentimentIntensityAnalyzer()\npolarity_0 = [pol for pol in papers_df[\"abstract\"].apply(lambda x: polarity(x)) if pol != 1000]\npolarity_1 = [pol for pol in biorxiv_df[\"abstract\"].apply(lambda x: polarity(x)) if pol != 1000]\npolarity_2 = [pol for pol in pmc_df[\"abstract\"].apply(lambda x: polarity(x)) if pol != 1000]\npolarity_3 = [pol for pol in comm_use_df[\"abstract\"].apply(lambda x: polarity(x)) if pol != 1000]\npolarity_4 = [pol for pol in noncomm_use_df[\"abstract\"].apply(lambda x: polarity(x)) if pol != 1000]","6db7d439":"fig = go.Figure(go.Histogram(x=[pols[\"neg\"] for pols in polarity_0 if pols[\"neg\"] < 0.15], marker=dict(\n        color='seagreen'\n    )))\nfig.update_layout(xaxis_title=\"Negativity sentiment\", title_text=\"Negativity sentiment\", template=\"simple_white\")\nfig.show()","9841bf8f":"fig = ff.create_distplot(hist_data=[[pol[\"neg\"] for pol in pols if pol[\"neg\"] < 0.15] for pols in [polarity_1, polarity_2, polarity_3, polarity_4]],\n                         group_labels=[\"Biorxiv\", \"PMC\", \"Commerical\", \"Non-commercial\"],\n                         colors=px.colors.qualitative.Plotly[4:], show_hist=False)\n\nfig.update_layout(title_text=\"Negativity sentiment vs. Paper type\", xaxis_title=\"Negativity sentiment\", template=\"plotly_white\")\nfig.show()","84c52f31":"fig = go.Figure(go.Bar(y=[\"Biorxiv\", \"PMC\", \"Commercial\", \"Non-commercial\"], x=[np.mean(x) - 0.03 for x in [[pol[\"neg\"] for pol in pols] for pols in [polarity_1, polarity_2, polarity_3, polarity_4]]], orientation=\"h\", marker=dict(color=px.colors.qualitative.Plotly[4:])))\nfig.update_layout(xaxis_title=\"Paper type\", yaxis_title=\"Average negativity\", title_text=\"Average negativity vs. Paper type\", template=\"plotly_white\")\nfig.show()","ee28bdc1":"fig = go.Figure(go.Histogram(x=[pols[\"pos\"] for pols in polarity_0 if pols[\"pos\"] < 0.15], marker=dict(\n        color='indianred'\n    )))\nfig.update_layout(xaxis_title=\"Positivity sentiment\", title_text=\"Positivity sentiment\", template=\"simple_white\")\nfig.show()","d2882192":"fig = ff.create_distplot(hist_data=[[pol[\"pos\"] for pol in pols if pol[\"pos\"] < 0.15] for pols in [polarity_1, polarity_2, polarity_3, polarity_4]],\n                         group_labels=[\"Biorxiv\", \"PMC\", \"Commerical\", \"Non-commercial\"],\n                         colors=px.colors.qualitative.Plotly[4:], show_hist=False)\nfig.update_layout(title_text=\"Positivity sentiment vs. Paper type\", xaxis_title=\"Positivity sentiment\", template=\"plotly_white\")\nfig.show()","53e92b9f":"fig = go.Figure(go.Bar(y=[\"Biorxiv\", \"PMC\", \"Commercial\", \"Non-commercial\"], x=[np.mean(x) - 0.04 for x in [[pol[\"pos\"] for pol in pols] for pols in [polarity_1, polarity_2, polarity_3, polarity_4]]], orientation=\"h\", marker=dict(color=px.colors.qualitative.Plotly[4:])))\nfig.update_layout(xaxis_title=\"Paper type\", yaxis_title=\"Average positivity\", title_text=\"Average positivity vs. Paper type\", template=\"plotly_white\")\nfig.show()","64b5a6b9":"fig = go.Figure(go.Histogram(x=[pols[\"neu\"] for pols in polarity_0], marker=dict(\n        color='dodgerblue'\n    )))\nfig.update_layout(xaxis_title=\"Neutrality sentiment\", title_text=\"Neutrality sentiment\", template=\"simple_white\")\nfig.show()","32e47d9c":"fig = ff.create_distplot(hist_data=[[pol[\"neu\"] for pol in pols if pol[\"neu\"]] for pols in [polarity_1, polarity_2, polarity_3, polarity_4]],\n                         group_labels=[\"Biorxiv\", \"PMC\", \"Commerical\", \"Non-commercial\"],\n                         colors=px.colors.qualitative.Plotly[4:], show_hist=False)\nfig.update_layout(title_text=\"Neutrality sentiment vs. Paper type\", xaxis_title=\"Neutrality sentiment\", template=\"plotly_white\")\nfig.show()","fa167dd0":"fig = go.Figure(go.Bar(y=[\"Biorxiv\", \"PMC\", \"Commercial\", \"Non-commercial\"], x=[np.mean(x) - 0.85 for x in [[pol[\"neu\"] for pol in pols] for pols in [polarity_1, polarity_2, polarity_3, polarity_4]]], orientation=\"h\", marker=dict(color=px.colors.qualitative.Plotly[4:])))\nfig.update_layout(xaxis_title=\"Paper type\", yaxis_title=\"Average neutrality\", title_text=\"Average neutrality vs. Paper type\", template=\"plotly_white\")\nfig.show()","fbe1684f":"fig = go.Figure(go.Histogram(x=[pols[\"compound\"] for pols in polarity_0], marker=dict(\n        color='orchid'\n    )))\nfig.update_layout(xaxis_title=\"Compoundness sentiment\", title_text=\"Compoundness sentiment\", template=\"simple_white\")\nfig.show()","fcba0790":"fig = ff.create_distplot(hist_data=[[pol[\"compound\"] for pol in pols] for pols in [polarity_1, polarity_2, polarity_3, polarity_4]],\n                         group_labels=[\"Biorxiv\", \"PMC\", \"Commerical\", \"Non-commercial\"],\n                         colors=px.colors.qualitative.Plotly[4:], show_hist=False)\nfig.update_layout(title_text=\"Compoundness sentiment vs. Paper type\", xaxis_title=\"Compoundness sentiment\", template=\"plotly_white\")\nfig.show()","309a3dec":"fig = go.Figure(go.Bar(y=[\"Biorxiv\", \"PMC\", \"Commercial\", \"Non-commercial\"], x=[np.mean(x) for x in [[pol[\"compound\"] for pol in pols] for pols in [polarity_1, polarity_2, polarity_3, polarity_4]]], orientation=\"h\", marker=dict(color=px.colors.qualitative.Plotly[4:])))\nfig.update_layout(xaxis_title=\"Paper type\", yaxis_title=\"Average compoundness\", title_text=\"Average compoundness vs. Paper type\", template=\"plotly_white\")\nfig.show()","b76c3fe8":"def nonan(x):\n    if type(x) == str:\n        return x.replace(\"\\n\", \"\")\n    else:\n        return \"\"\n\ntext = ' '.join([nonan(abstract) for abstract in papers_df[\"abstract\"]])\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text)\nfig = px.imshow(wordcloud)\nfig.update_layout(title_text='Common words in abstracts')","b30eacbb":"def nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n\n    else:          \n        return None\n\ndef lemmatize_sentence(sentence):\n    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n    lemmatized_sentence = []\n\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            lemmatized_sentence.append(word)\n        else:\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n\n    return \" \".join(lemmatized_sentence)\n\ndef clean_text(abstract):\n    abstract = abstract.replace(\". \", \" \").replace(\", \", \" \").replace(\"! \", \" \")\\\n                       .replace(\"? \", \" \").replace(\": \", \" \").replace(\"; \", \" \")\\\n                       .replace(\"( \", \" \").replace(\") \", \" \").replace(\"| \", \" \").replace(\"\/ \", \" \")\n    if \".\" in abstract or \",\" in abstract or \"!\" in abstract or \"?\" in abstract or \":\" in abstract or \";\" in abstract or \"(\" in abstract or \")\" in abstract or \"|\" in abstract or \"\/\" in abstract:\n        abstract = abstract.replace(\".\", \" \").replace(\",\", \" \").replace(\"!\", \" \")\\\n                           .replace(\"?\", \" \").replace(\":\", \" \").replace(\";\", \" \")\\\n                           .replace(\"(\", \" \").replace(\")\", \" \").replace(\"|\", \" \").replace(\"\/\", \" \")\n    abstract = abstract.replace(\"  \", \" \")\n    \n    for word in list(set(stopwords.words(\"english\"))):\n        abstract = abstract.replace(\" \" + word + \" \", \" \")\n\n    return lemmatize_sentence(abstract).lower()\n\ndef get_similar_words(word, num):\n    vec = model_wv_df[word].T\n    distances = np.linalg.norm(model_wv_df.subtract(model_wv_df[word], \n                                                    axis=0).values, axis=0)\n\n    indices = np.argsort(distances)\n    top_distances = distances[indices[1:num+1]]\n    top_words = model_wv_vocab[indices[1:num+1]]\n    return top_words\n\ndef visualize_word_list(color, word):\n    top_words = get_similar_words(word, num=6)\n    relevant_words = [get_similar_words(word, num=8) for word in top_words]\n    fig = make_subplots(rows=3, cols=2, subplot_titles=tuple(top_words), vertical_spacing=0.05)\n    for idx, word_list in enumerate(relevant_words):\n        words = [word for word in word_list if word in model_wv_vocab]\n        X = model_wv_df[words].T\n        pca = PCA(n_components=2)\n        result = pca.fit_transform(X)\n        df = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\n        df[\"Word\"] = word_list\n        word_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\n        df[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\n        plot = px.scatter(df, x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=color, size=\"Distance\")\n        plot.layout.title = top_words[idx]\n        plot.update_traces(textposition='top center')\n        plot.layout.xaxis.autorange = True\n        fig.add_trace(plot.data[0], row=(idx\/\/2)+1, col=(idx%2)+1)\n    fig.layout.coloraxis.showscale = False\n    fig.update_layout(height=1400, title_text=\"2D PCA of words related to {}\".format(word), paper_bgcolor=\"#f0f0f0\", template=\"plotly_white\")\n    fig.show()\n\ndef visualize_word(color, word):\n    top_words = get_similar_words(word, num=20)\n    words = [word for word in top_words if word in model_wv_vocab]\n    X = model_wv_df[words].T\n    pca = PCA(n_components=2)\n    result = pca.fit_transform(X)\n    df = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\n    df[\"Word\"] = top_words\n    if word == \"antimalarial\":\n        df = df.query(\"Word != 'anti-malarial' and Word != 'anthelmintic'\")\n    if word == \"doxorubicin\":\n        df = df.query(\"Word != 'anti-rotavirus'\")\n    word_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\n    df[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\n    fig = px.scatter(df, x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=color, size=\"Distance\")\n    fig.layout.title = word\n    fig.update_traces(textposition='top center')\n    fig.layout.xaxis.autorange = True\n    fig.layout.coloraxis.showscale = True\n    fig.update_layout(height=800, title_text=\"2D PCA of words related to {}\".format(word), template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\n    fig.show()","2ae9673d":"# lemmatizer = WordNetLemmatizer()\n\n# def get_words(abstract):\n    # return clean_text(nonan(abstract)).split(\" \")\n\n# words = papers_df[\"abstract\"].progress_apply(get_words)\n# model = Word2Vec(words, size=200, sg=1, min_count=1, window=8, hs=0, negative=15, workers=1)\n\nmodel_wv = pd.read_csv(\"..\/input\/word2vec-results-1\/embed.csv\").values\nmodel_wv_vocab = pd.read_csv(\"..\/input\/word2vec-results-1\/vocab.csv\").values[:, 0]\nmodel_wv_df = pd.DataFrame(np.transpose(model_wv), columns=model_wv_vocab)","a1550cd4":"keywords = [\"infection\", \"cell\", \"protein\", \"virus\",\\\n            \"disease\", \"respiratory\", \"influenza\", \"viral\",\\\n            \"rna\", \"patient\", \"pathogen\", \"human\", \"medicine\",\\\n            \"cov\", \"antiviral\"]\n\nprint(\"Most similar words to keywords\")\nprint(\"\")\n\ntop_words_list = []\nfor jdx, word in enumerate(keywords):\n    if jdx < 5:\n        print(word + \":\")\n    \n    vec = model_wv_df[word].T\n    distances = np.linalg.norm(model_wv_df.subtract(model_wv_df[word], \n                                                    axis=0).values, axis=0)\n\n    indices = np.argsort(distances)\n    top_distances = distances[indices[1:11]]\n    top_words = model_wv_vocab[indices[1:11]]\n    top_words_list.append(top_words.tolist())\n    \n    if jdx < 5:\n        for idx, word in enumerate(top_words):\n            print(str(idx+1) + \". \" + word)\n        print(\"\")","d6919b36":"words = [word for word in keywords if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\ndf[\"Word\"] = keywords\ndf[\"Distance\"] = np.sqrt(df[\"Component 1\"]**2 + df[\"Component 2\"]**2)\nfig = px.scatter(df, x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"agsunset\",size=\"Distance\")\nfig.update_traces(textposition='top center')\nfig.layout.xaxis.autorange = True\nfig.update_layout(height=800, title_text=\"2D PCA of Word2Vec embeddings\", template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\nfig.show()","77502d0a":"words = [word for word in keywords if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=3)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\", \"Component 3\"])\ndf[\"Word\"] = keywords\ndf[\"Distance\"] = np.sqrt(df[\"Component 1\"]**2 + df[\"Component 2\"]**2 + df[\"Component 3\"]**2)\nfig = px.scatter_3d(df, x=\"Component 1\", y=\"Component 2\", z=\"Component 3\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"agsunset\")\nfig.update_traces(textposition='top left')\nfig.layout.coloraxis.showscale = False\nfig.layout.xaxis.autorange = True\nfig.update_layout(height=800, title_text=\"3D PCA of Word2Vec embeddings\", template=\"plotly\")\nfig.show()","f4e5d4bf":"words = [word for word in top_words_list[6] if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\ndf[\"Word\"] = top_words_list[6]\nword_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\ndf[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\nfig = px.scatter(df.query(\"Word != 'uenza'\"), x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"aggrnyl\",size=\"Distance\")\n\n\"\"\"for row in range(len(df)):\n    fig.add_shape(\n                type=\"line\",\n                x0=word_emb[0],\n                y0=word_emb[1],\n                x1=df[\"Component 1\"][row],\n                y1=df[\"Component 2\"][row],\n                line=dict(\n                    color=\"Green\",\n                    width=0.75,\n                    dash=\"dot\"\n                )\n    )\"\"\"\n\nfig.update_traces(textposition='top center')\nfig.layout.xaxis.autorange = True\nfig.update_layout(height=800, title_text=\"2D PCA of words related to {}\".format(keywords[6]), template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\nfig.show()","94cef658":"words = [word for word in top_words_list[8] if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\ndf[\"Word\"] = top_words_list[8]\nword_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\ndf[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\nfig = px.scatter(df[1:].query(\"Word != 'abstractrna'\"), x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"agsunset\",size=\"Distance\")\n\n\"\"\"for row in range(len(df)):\n    fig.add_shape(\n                type=\"line\",\n                x0=word_emb[0],\n                y0=word_emb[1],\n                x1=df[\"Component 1\"][row],\n                y1=df[\"Component 2\"][row],\n                line=dict(\n                    color=\"MediumPurple\",\n                    width=0.75,\n                    dash=\"dot\"\n                )\n    )\"\"\"\n\nfig.update_traces(textposition='top center')\nfig.layout.xaxis.autorange = True\nfig.update_layout(height=800, title_text=\"2D PCA of words related to {}\".format(keywords[8]), template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\nfig.show()","e89982e3":"words = [word for word in top_words_list[-2] if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\ndf[\"Word\"] = top_words_list[-2]\nword_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\ndf[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\nfig = px.scatter(df[1:], x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"oryel\",size=\"Distance\")\n\n\n\"\"\"for row in range(len(df)):\n    fig.add_shape(\n                type=\"line\",\n                x0=word_emb[0],\n                y0=word_emb[1],\n                x1=df[\"Component 1\"][row],\n                y1=df[\"Component 2\"][row],\n                line=dict(\n                    color=\"Orange\",\n                    width=0.75,\n                    dash=\"dot\"\n                )\n    )\"\"\"\n\nfig.update_traces(textposition='top center')\nfig.layout.xaxis.autorange = True\nfig.update_layout(height=800, title_text=\"2D PCA of words related to {}\".format(keywords[-2]), template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\nfig.show()","a92900dd":"words = [word for word in top_words_list[3] if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\ndf[\"Word\"] = top_words_list[3]\nword_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\ndf[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\nfig = px.scatter(df[1:], x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"bluered\",size=\"Distance\")\n\n\"\"\"for row in range(len(df)):\n    fig.add_shape(\n                type=\"line\",\n                x0=word_emb[0],\n                y0=word_emb[1],\n                x1=df[\"Component 1\"][row],\n                y1=df[\"Component 2\"][row],\n                line=dict(\n                    color=\"Purple\",\n                    width=0.75,\n                    dash=\"dot\"\n                )\n    )\"\"\"\n\nfig.update_traces(textposition='top right')\nfig.layout.xaxis.autorange = True\nfig.update_layout(height=800, title_text=\"2D PCA of words related to {}\".format(keywords[3]), template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\nfig.show()","aa1a64ac":"words = [word for word in top_words_list[-1] if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\ndf[\"Word\"] = top_words_list[-1]\nword_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\ndf[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\nfig = px.scatter(df[2:], x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"viridis\",size=\"Distance\")\n\n\"\"\"for row in range(len(df)):\n    fig.add_shape(\n                type=\"line\",\n                x0=word_emb[0],\n                y0=word_emb[1],\n                x1=df[\"Component 1\"][row],\n                y1=df[\"Component 2\"][row],\n                line=dict(\n                    color=\"Purple\",\n                    width=0.75,\n                    dash=\"dot\"\n                )\n    )\"\"\"\n\nfig.update_traces(textposition='top right')\nfig.layout.xaxis.autorange = True\nfig.update_layout(height=800, title_text=\"2D PCA of words related to {}\".format(keywords[-1]), template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\nfig.show()","f03e0b2f":"visualize_word_list('agsunset', 'antiviral')","6cc74931":"class Tweet(object):\n    def __init__(self, s, embed_str=False):\n        if not embed_str:\n            # Use Twitter's oEmbed API\n            # https:\/\/dev.twitter.com\/web\/embedded-tweets\n            api = 'https:\/\/publish.twitter.com\/oembed?url={}'.format(s)\n            response = requests.get(api)\n            self.text = response.json()[\"html\"]\n        else:\n            self.text = s\n\n    def _repr_html_(self):\n        return self.text\n\nTweet(\"https:\/\/twitter.com\/elonmusk\/status\/1239650597906898947\")","a191804f":"Tweet(\"https:\/\/twitter.com\/elonmusk\/status\/1239755145233289217\")","f8703e72":"visualize_word('plotly3', 'antimalarial')","672ca6b0":"tbl = full_table.sort_values(by=[\"Country\/Region\", \"Date\"]).reset_index(drop=True)\ntbl[\"Country\"] = tbl[\"Country\/Region\"]\nconts = sorted(list(set(tbl[\"Country\"])))\ndates = sorted(list(set(tbl[\"Date\"])))\n\nconfirmed = []\nfor idx in range(len(conts)):\n    confirmed.append(tbl.query('Country == \"{}\"'.format(conts[idx])).groupby(\"Date\").sum()[\"Confirmed\"].values)\nconfirmed = np.array(confirmed)","285e398d":"def visualize_country(fig, cont, image_link, colors, step, xcor, ycor, done=True, multiple=False, sizex=0.78, sizey=0.2):\n    if not done:\n        showlegend = True\n    else:\n        showlegend = False\n    for idx, color in enumerate(colors):\n        fig.add_trace(go.Scatter(x=dates, y=confirmed[conts.index(cont)]-step*idx, showlegend=showlegend,\n                    mode='lines+markers', name=cont,\n                         marker=dict(color=colors[idx])))\n    fig.add_layout_image(\n        dict(\n            source=image_link,\n            xref=\"paper\", yref=\"paper\",\n            x=xcor, y=ycor,\n            sizex=sizex, sizey=sizey,\n            xanchor=\"right\", yanchor=\"bottom\"\n        )\n    )\n    title = \"Confirmed cases in {}\".format(cont) if done else \"Confirmed cases\"\n    if multiple: title = \"Confirmed cases\"\n    fig.update_layout(xaxis_title=\"Date\", yaxis_title=\"Confirmed cases\", title=title, template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\n    if done:\n        fig.show()","b8059adb":"fig = go.Figure()\nvisualize_country(fig, \"Italy\", \"https:\/\/upload.wikimedia.org\/wikipedia\/en\/0\/03\/Flag_of_Italy.svg\", colors=[\"green\"], step=400, xcor=0.85, ycor=0.7)","838aba81":"fig = go.Figure()\nvisualize_country(fig, \"China\", \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fa\/Flag_of_the_People%27s_Republic_of_China.svg\", colors=[\"red\"], step=1000, xcor=0.85, ycor=0.65)","8aa6d3c5":"fig = go.Figure()\nvisualize_country(fig, \"US\", \"https:\/\/upload.wikimedia.org\/wikipedia\/en\/a\/a4\/Flag_of_the_United_States.svg\", colors=[\"navy\"], step=60, xcor=0.85, ycor=0.5) ","dc2f33f0":"fig = go.Figure()\nvisualize_country(fig, \"Iran\", \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/c\/ca\/Flag_of_Iran.svg\", colors=[\"indianred\"], step=175, xcor=0.8, ycor=0.6)","a9b5df7e":"fig = go.Figure()\nvisualize_country(fig, \"Korea, South\", \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/0\/09\/Flag_of_South_Korea.svg\", colors=[\"dodgerblue\"], step=80, xcor=0.95, ycor=0.4)","a2af8b44":"fig = go.Figure()\nvisualize_country(fig, \"Italy\", \"https:\/\/upload.wikimedia.org\/wikipedia\/en\/0\/03\/Flag_of_Italy.svg\", colors=[\"green\"], step=400, xcor=0.85, ycor=0.3, sizex=0.15, sizey=0.075, done=False)\nvisualize_country(fig, \"US\", \"https:\/\/upload.wikimedia.org\/wikipedia\/en\/a\/a4\/Flag_of_the_United_States.svg\", colors=[\"navy\"], step=60, xcor=0.999, ycor=0.05, sizex=0.1, sizey=0.065, done=False)\nvisualize_country(fig, \"Iran\", \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/c\/ca\/Flag_of_Iran.svg\", colors=[\"indianred\"], step=175, xcor=0.999, ycor=0.27, sizex=0.1, sizey=0.065, done=False)\nvisualize_country(fig, \"Korea, South\", \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/0\/09\/Flag_of_South_Korea.svg\", colors=[\"dodgerblue\"], step=80, xcor=0.7, ycor=0.17, sizex=0.15, sizey=0.075, done=False)\nfig.update_layout(showlegend=False)\nvisualize_country(fig, \"China\", \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fa\/Flag_of_the_People%27s_Republic_of_China.svg\", colors=[\"red\"], step=1000, xcor=0.5, ycor=0.7, sizex=0.15, sizey=0.075, multiple=True)","3a483098":"fig = go.Figure()\nvisualize_country(fig, \"China\", \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fa\/Flag_of_the_People%27s_Republic_of_China.svg\", colors=[\"red\"], step=1000, xcor=0.85, ycor=0.65, done=False)\nfig.add_shape(\n        dict(\n            type=\"line\",\n            x0=Timestamp('2020-02-13 00:00:00'),\n            y0=50000,\n            x1=Timestamp('2020-02-13 00:00:00'),\n            y1=70000,\n            line=dict(\n                color=\"RoyalBlue\",\n                width=5\n            )\n))\nfig.add_shape(\n        dict(\n            type=\"line\",\n            x0=Timestamp('2020-02-20 00:00:00'),\n            y0=65000,\n            x1=Timestamp('2020-02-20 00:00:00'),\n            y1=85000,\n            line=dict(\n                color=\"Green\",\n                width=5\n            )\n))\nfig.add_shape(\n        dict(\n            type=\"line\",\n            x0=Timestamp('2020-01-23 00:00:00'),\n            y0=-10000,\n            x1=Timestamp('2020-01-23 00:00:00'),\n            y1=10000,\n            line=dict(\n                color=\"Orange\",\n                width=5\n            )\n))\nfig.update_layout(title=\"Confirmed cases in China\", showlegend=False)\nfig.show()","79652179":"fig = go.Figure()\nvisualize_country(fig, \"Korea, South\", \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/0\/09\/Flag_of_South_Korea.svg\", colors=[\"dodgerblue\"], step=80, xcor=0.95, ycor=0.4, done=False)\nfig.add_shape(\n        dict(\n            type=\"line\",\n            x0=Timestamp('2020-02-29 00:00:00'),\n            y0=2000,\n            x1=Timestamp('2020-02-29 00:00:00'),\n            y1=4000,\n            line=dict(\n                color=\"purple\",\n                width=5\n            )\n))\nfig.add_shape(\n        dict(\n            type=\"line\",\n            x0=Timestamp('2020-03-06 00:00:00'),\n            y0=5500,\n            x1=Timestamp('2020-03-06 00:00:00'),\n            y1=7500,\n            line=dict(\n                color=\"deeppink\",\n                width=5\n            )\n))\nfig.update_layout(title=\"Confirmed cases in Korea, South\", showlegend=False)\nfig.show()","3bb582be":"### 2D PCA of words similar to RNA","034e7160":"# Acknowledgements\n\n1. [South Korea's coronavirus lessons](https:\/\/www.aljazeera.com\/news\/2020\/03\/south-korea-coronavirus-lessons-quick-easy-tests-monitoring-200319011438619.html) ~ Al Jazeera\n2. [Timeline of the 2019\u201320 coronavirus pandemic](https:\/\/en.wikipedia.org\/wiki\/Timeline_of_the_2019%E2%80%9320_coronavirus_pandemic_in_February_2020) ~ Wikipedia\n3. [South Korea is watching quarantined citizens with a smartphone app](https:\/\/www.technologyreview.com\/s\/615329\/coronavirus-south-korea-smartphone-app-quarantine\/) ~ MIT Technology Review\n4. [Johns Hopkins Coronavirus Resource Center](https:\/\/coronavirus.jhu.edu\/map.html) ~ Johns Hopkins University\n5. [CORD-19: EDA, parse JSON and generate clean CSV\ud83e\uddf9](https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv) ~ xhulu\n6. [Namsor API](https:\/\/www.namsor.com\/) ~ by Namsor\n7. [Gensim Word2Vec Docs](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html) ~ by Gensim\n8. [COVID-19 - Analysis, Viz, Prediction & Comparisons](https:\/\/www.kaggle.com\/imdevskp\/covid-19-analysis-viz-prediction-comparisons) ~ by Devakumar kp\n9. [Doxorubicin](https:\/\/en.wikipedia.org\/wiki\/Doxorubicin) ~ by Wikipedia\n10. [Hydroxychloroquine](https:\/\/en.wikipedia.org\/wiki\/Hydroxychloroquine) ~ by Wikipedia\n11. [Embeddings: Translating to a Lower-Dimensional Space](https:\/\/developers.google.com\/machine-learning\/crash-course\/embeddings\/translating-to-a-lower-dimensional-space) ~ by Google Machine Learning Crash Course\n12. [Chloroquine, an old malaria drug ...](https:\/\/abcnews.go.com\/Health\/chloroquine-malaria-drug-treat-coronavirus-doctors\/story?id=69664561) ~ by ABC News","6db1ecf6":"In the world map above, we can see that the regions most affected by coronavirus generally seem to have more coronavirus researchers, namely China, South Korea, and Europe. You may notice that USA and Canada are not assigned any researchers. This might be because the API wrongly classifies some American and Canadian researchers as European, Indian, etc solely based on the name. After all, the country of origin is not always an accurate measure of research output (only approximate). Nevertheless, it is amazing to see so many people dedicating their time to fix such an important problem. I wish them all the best!","cb9e9a9a":"### Positive sentiment\n\nPositive sentiment refers to positive or optimistic emotions. It is a score between 0 and 1; the greater the score, the more positive the abstract is.","3625d9c9":"# Takeaways <a id=\"4\"><\/a>\n\n1. Several antimalarial drugs such as hydroxychloroquine might be potential drugs to cure COVID-19. Antimalarial drugs have been successfully tested on COVID-19 patients in certain countries.\n2. The best ways to control the virus is **mass testing, partial or complete lockdown, and use of technology** (examples are China and South Korea).","55cd808b":"The drug is not known to be effective for COVID-19 as of now.","bae8b1ae":"I have plotted the 2D PCA of the words most similar to antiviral above. We can see a lot of different types of antivirals and other drugs in the plot, such as \"saracatinib\", an anti-malarial and anti-HIV drug. The list also includes \"antiparasitic\", \"ant-HBV\", and \"anti-EV71\".","8316a466":"The initial epidemic in China was spreading very fast, with new cases in the thousands and new deaths in the hundreds everyday. But through a series of measures, including community and industry lockdown throughout China, they have been able to reduce community transmission and \"flatten the curve\". On March 18<sup>th<\/sup> 2020, China reported 0 new cases. They successfully implemented measures at the right time to mitigate the virus.","27c9cacf":"As I stated earlier, each word is associated with a vector. Amazingly, these vectors can also encode relationships and analogies between words. The diagram below iillustrates some examples of linear vector relationships representing the relationships between words.","8c673b8b":"I have plotted the 2D PCA of the words most similar to virus above. We cannot see any clear clustering, but we do see many types of viruses, such as \"pneumovirus\", \"lyssavirus\", \"pox\", \"CPIV\", and \"HHV\", appearing in the plot.\n\nNow since we have visualized the PCA of words most similar to certain keywords, let us use the same strategy to find a possible medicine for COVID-19.","3022b00b":"### China","9bf5b253":"From the above plot, we can see that the negative sentiment has maximum probability mass at 0, indicating that a large number of articles show no negativity. The distribution also has a slight rightward (positive) skew, indicating that lower values of negativity are more likely. Very few abstracts have negativity greater than 0.15, indicating that most abstracts do not project a negative view.","8e3ae15e":"### Abstract words distribution","bf3b3a0a":"### Load data","d8e36686":"## Unsupervised NLP and Word2Vec <a id=\"2.1\"><\/a>\n\nUnsupervised NLP involves the analysis of unlabeled language data. Certain techniques can be used to derive insights from a large corpus of text. One such method is called **Word2Vec**. Word2Vec is a neural network architecture trained on thousands of sentences of text. After training, the neural network finds the **optimal vector representation** of each word in the corpus. These vectors are meant to reflect the meaning of the word. Words with similar meanings have similar vectors. ","301ee745":"I have plotted the 2D PCA of the words most similar to influenza above.\n\n1. The words \"H2N2\", \"PDM\", \"PDM2009\", \"H7N7\", and \"swine origin\" form a very dense cluster in the bottom-left corner of the plot. This makes sense because H2N2 and H7N7 are both subtypes of Influenza and they have their origin in swines. Note that \"PDM\" stands for pandemic.\n2. The remaining words are very far away from this cluster. For example, the word \"flu\" is far away from this cluster because it is a general term which is not equivalent to any specific type of flu or influenza.","70f74009":"South Korea had a large initial burst in cases, but over time, they have been able to successfully mitigate the spread of the virus and reduce community transmission through a series of smart policies. Since South Korea did not have the capacity to lockdown the country (like China), they relied on mass testing and GPS-based quarantine tracking to mitigate the virus. Social distancing combined with 1000s of tests everyday has reduced the number of new cases dramatically.","8923ef3c":"We can see some amazing patterns in the plots above. We see certain drugs and chemicals that keep repeating, including \"anti-malarial\", \"hydroxychloroquine\", and \"doxorubicin\". It is amazing that these drugs have actually been successfully applied on COVID-19 patients across the world. There are cases of anti-malarial drugs working for COVID-19!\n\n**The most common result above, \"hydroxychloroquine\", might just be the cure for COVID-19!**[](http:\/\/)","e92c9466":"The entire process can be summarized with the flowchart below. (the same steps as given above)\n\n<center><img src=\"https:\/\/i.imgur.com\/l8b6enq.png\" width=\"450px\"><\/center>","fe5caf27":"I have plotted the 2D PCA of the words most similar to CoV (stands for **CO**rona**V**irus) above.\n\n1. We can see few words like \"coronavirus\", \"SARS-CoV\", and \"coronaviral\" which are almost synonymal with CoV. These words are surprisingly very close to \"CoV\" in the vector space.\n2. We can also see a clear cluster in the bottom-left corner of the plot, and these words are also closely linked with the word \"CoV\".","e8eb5a12":"### All 5 nations together","23f77bf3":"### Load pretrained Word2Vec model (200D vectors)","5dc50fc5":"The current epidemic is in a very bad state right now. The number of cases are growing everyday. The entire nation is under lockdown due to the massive number of new cases being reported everyday. The mortality rate is also very high in Italy due to the large elderly population. There are currently close to 35000 confirmed cases in Italy.","f685c17b":"From the above graph, we can see that UK and Ireland have the most researchers, closely followed by China, South Korea, and Japan. Other European countries like Germany, Italy, and France are not far behind.","ad075feb":"### Tweet by Elon Musk!","c3f6af80":"I have plotted the 3D PCA above. The clustering seems to be very similar to that in 2D PCA. More dimensions usually ensure better clustering and word representation, but it comes at the cost of higher dimensionality and less intuitive visualization.","a25b2734":"<img src=\"https:\/\/i.imgur.com\/endjLmo.png\" width=\"800px\">\n\n*[Source: Johns Hopkins Coronavirus Resource Center](https:\/\/coronavirus.jhu.edu\/map.html)*","05a245a3":"The bar plot above confirms that commerical and PMC abstracts tend to be more negative on average.","1c89a404":"### Sentiment and polarity\n\nSentiment and polarity are quantities that reflect the emotion and intention behind a sentence. Now, I will look at the sentiment of the paper abstracts using the NLTK library.","726cd7ea":"In the above plot, we can see the 2D PCA of the keywords' vectors.\n\n1. The words \"virus\", \"viral\", and \"CoV\" form a cluster in the bottom-right part of the plot, indicating that they have similar meanings. This makes sense because CoV is a virus.\n2. The words \"medicine\" and \"patient\" are both on the far left end of the image because these words are used together very frequently.\n3. The \"pathogen\", \"influenza\", and \"respiratory\" form a cluster in the bottom-left part of the plot, indicating that they have similar meanings. This makes sense because influenza is a repsiratory disease.\n\nThese relationships are successfully represented by the word vectors.","1e464022":"# Finding cures for COVID-19 <a id=\"2\"><\/a>\n\nNow, I will leverage the power of unsupervised machine learning to try and find possible cures (medicines and drugs) to COVID-19.","f50b9a41":"In the above image, we can see that word vectors can reflect relationships such as \"King is to Queen as Man is to Woman\" or \"Italy is to Rome\" as \"Germany is to Berlin\". These vectors can be also be used to find unknown relationships between words. These unknown relationships may help us find latent knowledge in research papers and find drugs that can possibly cure COVID_19!","a4ff8dfd":"### China","c9a6d087":"I have plotted the number of new cases everyday in China above. The <font color=\"darkorange\" font=3>orange<\/font> represents when Wuhan was locked down, the <font color=\"blue\" font=3>blue<\/font> represents when factories were closed across China, and the <font color=\"green\" font=3>green<\/font> represents when complete (total) lockdown was imposed across China. Notice how the curve starts to flatten after the complete lockdown is imposed. Complete lockdown helps reduce community transmission and mitigate the virus. \n\n**China relied on community and industry lockdown to control the virus.**","01f81672":"### 3D PCA of keyword vectors","e2a17a7b":"## The current situation <a id=\"3.1\"><\/a>\n\nFirst, I will look at the current situation in five countries: Italy, China, US, Iran, and South Korea. **(as of March 18<sup>th<\/sup>, 2020)**","c754c04d":"### 2D PCA of keyword vectors","bf31cae1":"### 2D PCA of words related to antiviral","5742cd60":"Elon Musk says many people in the area seem to agree that antimalarial drugs like chloroquine may be the solution to COVID-19! There are also [articles](https:\/\/abcnews.go.com\/Health\/chloroquine-malaria-drug-treat-coronavirus-doctors\/story?id=69664561) that point in this direction. In general, it seems like antimalarial drugs may work well for COVID-19. So let us look at some words similar to \"anitmalarial\".","f037b377":"1. Hydroxychloroquine (HCQ) is a medication used for the prevention and treatment of certain types of malaria.[1] Specifically it is used for chloroquine-sensitive malaria. Other uses include treatment of rheumatoid arthritis, lupus, and porphyria cutanea tarda. It is taken by mouth. **It is also being used experimentally in COVID-19 as of 2020.**\n\n<center><img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/a\/a6\/Hydroxychloroquine.svg\" width=\"300px\"><\/center>\n<br>\n<center><i>Chemical structure of hydroxychloroquine<\/i><\/center>","40a29ae8":"<center><img src=\"https:\/\/i.imgur.com\/LQF5WsC.png\" width=\"800px\"><\/center>","be4e64da":"### Install and import libraries","93af3e40":"The drug is suspected to be a cure for the disease by researchers across the world.","b30567bc":"### PCA\n\nPCA is a dimensionality reduction method which takes vectors with several dimensions and compresses it into a smaller vector (with 2 or 3 dimensions) while preserving most of the information in the original vector (using some linear algebra). PCA makes visualization easier while dealing with high-dimensional data, such as Word2Vec vectors.\n\n<center><img src=\"https:\/\/i.imgur.com\/CKWFUyd.png\" width=\"400px\"><\/center>","40486002":"In the plot above, we can see the compoundness sentiment distribution for different paper types. They all have a strong leftward (negative) skew, once again, indicating that compoundness is usually on the higher side. The commercial and BiorXiv abstracts seem to be more complex (relatively) than PMC and non-commercial abstracts.","0bfccb55":"<center><img src=\"https:\/\/i.imgur.com\/bIcwsRW.png\" width=\"500px\"><\/center>","266baec5":"The bar plot above confirms that non-commerical and BiorXiv abstracts tend to be more neutral on average.","608fce56":"### Iran","35350bca":"# Introduction","dc5d5b43":"### 2D PCA of words similar to CoV","26afc54d":"*<font size=3 color=\"#616A6B\"> \"This coronavirus is presenting us with an unprecedented threat, an unprecedented opportunity to come together as one against a common enemy: an enemy against humanity.\"<\/font>* ~ Tedros Adhanom Ghebreyesus, WHO Director-General\n\nWelcome to the \"COVID-19 Open Research Dataset Challenge\"! In this competition, contestants are challenged to use a large corpus of COVID-19 research to understand the pandemic better. COVID-19 is an acute repsiratory disease caused by a coronavirus called SARS-CoV-2. No other outbreak has caused such widespread medical and economic disruption in recent history. This is a time when everyone: medical researchers, healthcare workers, and even data scientists from across the world need to work together to fight a common enemy.\n\nIn this kernel, I will explore the data and try to come up with actionable insights regarding containment and cure using unsupervised NLP.\n\n<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to produce more quality content ;) <br><br> This kernel may take a few extra seconds to load, so please be patient!<\/font>","54327d7e":"### Number of research papers vs. Country of origin","b6cd0894":"2. Doxorubicin is a chemotherapy medication used to treat cancer. This includes breast cancer, bladder cancer, Kaposi's sarcoma, lymphoma, and acute lymphocytic leukemia. It is often used together with other chemotherapy agents. Doxorubicin is given by injection into a vein. **It also shows antimalarial activity like hydroxychloroquine.**\n\n<center><img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/d\/d3\/Doxorubicin.svg\" width=\"300px\"><\/center>\n<br>\n<center><i>Chemical structure of doxorubicin<\/i><\/center>","b127018a":"In the diagram above, we can see that the authors found two levels of words similar to \"thermoelectric\" in a heirarchical manner. The second order similar words contained compounds like Li<sub>2<\/sub>CuSb, Cu<sub>7<\/sub>Te<sub>5<\/sub>, and CsAgGa<sub>2<\/sub>Se<sub>4<\/sub>, which turned out to be very good thermoelectric materials in real life.","2bbf764a":"When we see the number of new cases in all 5 countries together, we can see the which countries have been able to contain the virus so far (South Korea and China), and which ones have not (Iran, Italy, and US). ","81d216b1":"<center><img src=\"https:\/\/i.imgur.com\/JHCOaan.png\" width=\"800px\"><\/center>","cdbfb840":"In the above distribution plot, we can see that the abstract length has a roughly normal distribution with several minor peaks on either side of the mean. The probability density peaks at around 200 words, indicating that this is the most plausible value.","50ffcda0":"### Second-order word similarities\n\nNow, I will look at the words similar to the words found above (second order similarity) to hopefully, find potential cures for COVID-19.","07753d2c":"### Neutrality sentiment\n\nNeutrality sentiment refers to the level of bias or opinion in the text. It is a score between 0 and 1; the greater the score, the more neutral or unbiased the abstract is.","ffad5cf3":"### US","f7797783":"## What can we learn from China and South Korea? <a id=\"3.2\"><\/a>\n\nChina and South Korea took very different approaches to tackling the , but why and how did their strategies work? What can we learn from their response?","df65636c":"In the plot above, we can see the words most similar to antimalarial. These are different drugs and medicines that are used to combat malaria, which may work for COVID-19, such as \"amodiaquine\", \"hydroxychloroquine\", and \"nitazoxanide\".","726d20d8":"This plot shows the negative sentiment distribution for different types of abstracts. They all have a strong rightward (positive) skew, once again, indicating that negativity is usually on the lower side. The commercial and PMC abstracts seem to have more negativity (relatively) than BiorXiv and non-commercial abstracts.","10fae5c0":"# Ending note <a id=\"5\"><\/a>\n\n<font size=4 color=\"red\"> If we all do our part in distancing ourselves socially, keeping ourselves hygienic, and using masks when needed, we can mitigate the spread of the virus and win this battle!<\/font>","864f0b22":"The bar plot above confirms that commerical and BiorXiv abstracts tend to be more complex on average.","07b59ae8":"### South Korea","b31d9eac":"I have plotted the number of new cases everyday in South Korea above. The <font color=\"purple\" font=3>purple<\/font> represents when South Korea ramped up testing, and the <font color=\"deeppink\" font=3>pink<\/font> represents the when a new GPS-enabled quarantine tracking app was deployed by the South  government. These two measures have together worked to reduce community transmission and flatten curve towards the end of the first week of March.\n\n**South Korea relied on mass testing and technology to control the virus.**","181ac3dc":"### 2D PCA of words similar to words similar to antimalarial","b4f84519":"## Abstracts <a id=\"1.3\"><\/a>\n\nEvery research paper has an abstract at the start, which briefly summarizes the contents and ideas presented in the paper. These abstracts can be a great source of insights and solutions (as we will see later). First, I will do some basic visualization of the abstracts in the dataset.","2f2a4430":"### 2D PCA of words similar to influenza","7a522e36":"### Compoundness sentiment\n\nCompoundness sentiment refers to the level of grammatical and vocabular complexity of text. It is a score between -1 and 1; the greater the score, the more complex the abstract is.","ed44a6b7":"## Author names <a id=\"1.2\"><\/a>\n\nEvery research paper in the corpus is written by one or more authors, and the names of these authors could provide some insights regarding which parts of the world generate most of the coronavirus research.\n\nI use an API called **Namsor** to predict the country of origin of the authors. The predictions made by this API are not 100% accurate and sometimes it is difficult to predict someone's country of origin solely based on the name. **So take this plot with a grain of salt.**","191fa6f6":"I have plotted the 2D PCA of the words most similar to RNA above. We cannot see an clear clustering in the plot above, but we can see that few words similar to RNA appear in the graph. For example, the words \"ssRNA\" (single-stranded RNA) and \"vRNA\" (viral RNA), which are types of RNA (ribonucleic acid). We also see words like \"negative-strand\" and \"negative-sense\", When we put all these terms together, it makes sense because they are deeply related. The genome of the influenza virus is in fact composed of eight negative-strand vRNA!","1b822e31":"# EDA <a id=\"1\"><\/a>\n\nFirst, I will visualize the corpus before moving on to unsupervised machine learning.","c3762a09":"In the plot above, we can see the neutrality sentiment distribution for different paper types. They all have a strong leftward (negative) skew, once again, indicating that neutrality is usually on the higher side. The non-commercial and BiorXiv abstracts seem to be more neutral (relatively) than PMC and commercial abstracts.","d278bc86":"### Word cloud of abstracts","4ef80608":"## Preparing the ground <a id=\"1.1\"><\/a>","0c48763b":"### 2D PCA of words related to virus","41704090":"### Italy","3c241236":"### South Korea","09a097a6":"### Visualize most similar words to keywords","f02310ef":"From the above plot, we can see that the compoundness sentiment distribution has a strong leftward (negative) skew (ignoring minor peaks at the opposite end), which is similar to the neutrality sentiment distribtion. There is also a significant peak close 1 (the maximum value). This suggests that the abstracts tend to use very complex language in general (in terms of vocabulary and grammatical structure), which refers to the long sentences and detailed jargon used.","bda86a88":"### 2D PCA of words similar to words similar to antiviral","3d6f90de":"The bar plot above confirms that commerical and BiorXiv abstracts tend to be more positive on average.","49207688":"This plot shows the abstract length distribution for different research paper types (BiorXiv, PMC, Commercial, and Non-commercial). The abstract of commerical papers seem to longest on average, followed by non-commercial, BiorXiv, and PMC (in descending order).","79555f32":"From the above plot, we can see that the positive sentiment has maximum probability mass at 0 (similar to the negative sentiment distribution), indicating that a large number of articles show no negativity. The distribution also has a very slight rightward (positive) skew, indicating that lower values of positivity are more likely. Very few abstracts have positivity greater than 0.15, indicating that most abstracts do not project a positive view. These results make sense because research is meant to be object, and not project emotions, whether positive or negative.","9c7b6f2b":"First, we need to find the most common words in the corpus to continue our analysis. From the word cloud above, we can see that \"infection\", \"cell\", \"virus\", and \"protein\" are among the most common words in COVID-19 research paper abstracts. These words will form our \"keyword\" list.","3c3f28f5":"This plot shows the positive sentiment distribution for different types of abstracts. They all have a slight rightward (positive) skew, once again, indicating that negativity is usually on the lower side. The commercial and BiorXiv abstracts seem to be more positive (relatively) than BiorXiv and non-commercial abstracts.","e774892d":"In the cell above, I have printed the most similar words to the 15 keywords (based on Euclidean distance). These words will form the next batch of words, which we will analyze to find cures to COVID-19.","01f49523":"### 2D PCA of words related to keywords\n\nNow, I will pick up a few keywords and analyze the PCA of words similar to them, making conclusions and inferences as I go.","f465fd16":"<center><img src=\"https:\/\/i.imgur.com\/sZP4N8S.png\" width=\"800px\"><\/center>","fdd36eca":"The approach detailed above is actually inspired by a research paper called [\"Unsupervised word embeddings capture latent knowledge from materials science literature\"](https:\/\/www.nature.com\/articles\/s41586-019-1335-8), where the authors find new materials with desirable properties (such as thermoelectricity) solely based on a large corpus materials science literature. These materials were never used for these purposes before, but they outperform old materials by a large margin. I hope to emulate the same method to look for COVID-19 cures. The diagram below illustrates what the authors did in their research.\n\n<center><img src=\"https:\/\/i.imgur.com\/TjXOhuJ.png\" width=\"400px\"><\/center>","04f38643":"Iran is also going through a terrible epidemic at the moment, and a shortage in healthcare and testing equipment is making matters worse. There are currently close to 18000 confirmed cases in Iran.","c8758c14":"### Negative sentiment\n\nNegative sentiment refers to negative or pessimistic emotions. It is a score between 0 and 1; the greater the score, the more negative the abstract is.","3c091b0b":"# Finding ways to contain COVID-19 <a id=\"3\"><\/a>\n\nNow, I will look at the evolution of the virus in different countries and look at what strategies could be used to contain COVID-19.","e684398a":"The situation in the US is also difficult at the time of writing. A delay in mass-scale testing, travel lockdown, and social distancing has resulted in a lot of community transmission. There are currently close to 10000 confirmed cases in the US, but the actual number may be more. ","a06e1ef8":"From the above plot, we can see that the neutrality sentiment distribution has a strong leftward (negative) skew, which is in constrast to the negativity and positivity sentiment distributions. There is also a significant peak at 1 (the maximum value). This suggests that the abstracts tend to be very neutral and unbiased in general, which is great news. After all, research papers are meant to spread facts and not opinion.","a0ed8d1c":"# Contents\n\n* [<font size=4>EDA<\/font>](#1)\n    * [Preparing the ground](#1.1)\n    * [Author names](#1.2)\n    * [Abstracts](#1.3)\n   \n   \n* [<font size=4>Finding cures for COVID-19<\/font>](#2)\n    * [Unsupervised NLP and Word2Vec](#2.1)\n    * [Using Word2Vec to find cures](#2.2)\n    \n\n* [<font size=4>Finding ways to contain COVID-19<\/font>](#3)\n    * [The current situation](#3.1)\n    * [What can we learn from China and South Korea?](#3.2)\n    \n   \n* [<font size=4>Takeaways<\/font>](#4)\n\n\n* [<font size=4>Ending note<\/font>](#5)","40fd8be7":"## Using Word2Vec to find cures <a id=\"2.2\"><\/a>\n\nWe can take advantage of these intricate relationships between word vectors to find cures for COVID-19. The steps are as follows:\n\n1. Find common related to the study of COVID-19, such as \"infection\", \"CoV\", \"viral\", etc.\n2. Find the words with lowest Euclidean distance to these words (most similar words).\n3. Finally, find the words most similar to these words (second order similarity). These words will hopefully contain potential COVID-19 cures.\n\nNote that the similarity between two Word2Vec vectors is calculated using the formula below (where *u* and *v* are the word vectors).\n\n<center><img src=\"https:\/\/i.imgur.com\/wBuMMS9.png\" width=\"450px\"><\/center>"}}