{"cell_type":{"230bfea0":"code","3e4c02fc":"code","9f5ab196":"code","6ea48cb5":"code","5bf716c3":"code","ee4dcd72":"code","6a61f6a3":"code","9a666841":"code","23299a46":"code","c515ec4d":"code","7305d4ea":"code","c7a5e8bb":"code","0ae5e31a":"code","8ff74584":"code","56dcb9d1":"code","9256ef52":"code","b10c5e0d":"code","44379bae":"code","e2498b5a":"code","4112d32e":"markdown","2954ae94":"markdown","dcbe55f1":"markdown","3350dc75":"markdown","38354453":"markdown","847356e0":"markdown","f19fee9a":"markdown","b3569d2f":"markdown","e5d118d0":"markdown","a06ad321":"markdown"},"source":{"230bfea0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","3e4c02fc":"df = pd.read_csv('\/kaggle\/input\/insurance-premium-prediction\/insurance.csv')\ndf","9f5ab196":"df.shape","6ea48cb5":"df.describe()","5bf716c3":"df.info()","ee4dcd72":"df.isnull().sum()","6a61f6a3":"#There are 7 columns, 6 of which are features which can be subdivided into categorical_cols and numerical_cols\nall_columns = list(df.columns)\nprint(all_columns)\ncategorical_cols = ['sex', 'smoker', 'region']\nnumerical_cols = ['age', 'children', 'bmi']\ntarget_col = 'expenses'","9a666841":"#Creating two dataframes, one to store numerical data and one to store categorical data\n\n#I needed to transform categorical data to some type of numerical format. This was required if I ever to train my \n#linear model with categorical data. \n\n# I used PD.GET_DUMMIES function to tranform my categorical data. \n\ncategorical_X = pd.DataFrame()\nfor i in categorical_cols:\n    dummies = pd.get_dummies(df[i])\n    categorical_X = pd.concat([categorical_X, dummies], axis=1)\nnumerical_X = pd.DataFrame(df[numerical_cols])","23299a46":"cat_fig, axes=plt.subplots(1, 3, figsize=(15, 4))\nsns.swarmplot(ax=axes[0], x='smoker', y='expenses', data=df)\nsns.swarmplot(ax=axes[1], x='sex', y='expenses', data=df)\nsns.swarmplot(ax=axes[2], x='region', y='expenses', data=df)\nfig.suptitle('Categorical Variables vs Expenses', fontsize=16)","c515ec4d":"fig, axes=plt.subplots(1, 3, figsize=(15, 4))\nsns.regplot(ax=axes[0], x=\"age\", y=\"expenses\", data=df, scatter_kws={'s':10})\nsns.regplot(ax=axes[1], x=\"children\", y=\"expenses\", data=df, scatter_kws={'s':10})\nsns.regplot(ax=axes[2], x=\"bmi\", y=\"expenses\", data=df, scatter_kws={'s':10})\nfig.suptitle('Numerical Variables vs Expenses', fontsize=16)","7305d4ea":"df[['age', 'children', 'bmi', 'expenses']].corr().iloc[:-1, -1].round(2)","c7a5e8bb":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score","0ae5e31a":"#As discussed before, this process drops irrelevant features\n\ncategorical_X.drop(['female', 'male', 'northeast', 'northwest', 'southeast',\n       'southwest'], axis=1, inplace=True)\nnumerical_X.drop(['children'], axis=1, inplace=True)","8ff74584":"linearModel = LinearRegression()","56dcb9d1":"X = pd.concat([categorical_X, numerical_X], axis=1)\nY = df[target_col]","9256ef52":"train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.3, random_state=0)","b10c5e0d":"linearModel.fit(train_X, train_Y)","44379bae":"pred_Y = linearModel.predict(test_X)","e2498b5a":"print(\"MEAN ABSOLUTE ERROR: \", mean_absolute_error(test_Y, pred_Y))\nprint(\"R2 SCORE: \", r2_score(test_Y, pred_Y))\nprint(\"MEAN SQUARED ERROR: \", mean_squared_error(test_Y, pred_Y))\nprint(\"ROOT MEAN SQUARED ERROR: \", (mean_squared_error(test_Y, pred_Y)**0.5))","4112d32e":"The best way to understand numerical data is through a scatter plot shown above. It also contains the line of best fit to give some perspective. \n\nThe first plot displayed shows that age generally is positively correlated with expenses. We will keep this feature. \n\nThe second plot, has a very weak correlation (almost 0). We can drop this feature. \n\nThe third plot compares BMI (body mass index) to expenses which has some positive correlation with expenses. This makes sense as a high bmi indicates obesity and thus further medical issues. \n\nI have plotted correlations of features with expenses below. ","2954ae94":"The best way to understand any categorical data is through relative frequency, which is why, it made so much more sense to use swarm plot to visualise my data. \n\nThe first plot displayed shows that the smokers generally incur more expense than non-smokers. This makes sense as smoking is dangerous to health and makes people who practise it susceptible to more health problems. This feature definitely has a place in the multiple linear regression model that we will later train. \n\nThe second plot, compares expenses to the sex of an individual. There isnt much variation in expense which makes sense as if the sample is large enough, and is random enough. It is reasonable to drop this feature. \n\nThe third plot compares expense to the region where an individual belongs from. This also shows hardly any eye-popping variation and it doesnt make sense to train our model with this data. ","dcbe55f1":"## Graphing Numerical Variables vs Expenses","3350dc75":"## Creating a Multiple Linear Regression Model and Metrics","38354453":"### Goals\n\n1. Divide the features into two sections: numerical and categorical in order to clean and transform each appropriately. \n2. For categorical features:\n    a. create a dataframe and clean missing data\n    b. represent data visually in an appropriate format\n    c. drop irrelevant features within reason and logic presented. \n    d. convert categorical variables to represent in a binary format.  \n3. For numerical features:\n    a. create a dataframe and clean missing data\n    b. represent data visually in an appropriate format\n    c. drop irrelevant features within reason and logic presented. \n4. Create a Linear Model, fit and predict the relevant features. \n5. Use appropriate metrics to measure effectiveness of model. \n","847356e0":"## Graphing Categorical Variables vs Expenses","f19fee9a":"### Correlation of Numerical Variables with Expenses","b3569d2f":"# Insurance Regression Analysis by TanishHP","e5d118d0":"##### It is quite lucky that there are no null values to deal with. ","a06ad321":"### General Description\nThe dataset is retrieved from Machine Learning Website by Professor Eric Suess at http:\/\/www.sci.csueastbay.edu\/~esuess\/stat6620\/#week-6. It contains 7 features (4 numerical and 3 categorical), and the target column contains medical expenses incurred by each individual.  "}}