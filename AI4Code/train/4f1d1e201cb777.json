{"cell_type":{"b8176982":"code","695aa1eb":"code","9b662c90":"code","935d5abf":"code","a20c2f49":"code","cd541318":"code","4301901a":"code","b249e1f8":"code","8104e4af":"code","9982201c":"code","90294fe8":"code","bc26f033":"code","0b343907":"markdown","a2c69720":"markdown","9eb31b14":"markdown","a154e484":"markdown","a3252cb6":"markdown","ea163534":"markdown"},"source":{"b8176982":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nimport warnings\n\nsns.set(style=\"white\")\nwarnings.filterwarnings('ignore')","695aa1eb":"#Acquire data\ndataset = pd.read_csv(\"..\/input\/winequality-red.csv\")\ndataset.head()","9b662c90":"#To see if the data need to be corrected\ndataset.info()","935d5abf":"dataset.describe()","a20c2f49":"sns.countplot(dataset['quality'])","cd541318":"#Make the problem binary, 0 for bad wine and 1 for good wine\ndataset[\"quality\"] = dataset.quality.map(lambda x : 1 if x > 6 else 0)\nsns.countplot(dataset[\"quality\"])","4301901a":"#Analyse the correlation between the features\ncorr = dataset.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","b249e1f8":"#Separate input and output variables \ny = dataset[\"quality\"]\nX = dataset.drop(\"quality\", axis=1)\n#Standardize features by removing the mean and scaling to unit variance\nscaler = StandardScaler()\nX[X.columns] = scaler.fit_transform(X[X.columns])\nX.head()","8104e4af":"def train_evaluate(clf, X, y, n_splits, name):\n    results = {}\n    results[\"Precision\"] = cross_val_score(clf, X, y, cv=n_splits, scoring = make_scorer(precision_score, average=\"micro\")).mean()\n    results[\"Recall\"] = cross_val_score(clf, X, y, cv=n_splits, scoring = make_scorer(recall_score, average=\"micro\")).mean()\n    results[\"F1\"] = cross_val_score(clf, X, y, cv=n_splits, scoring = make_scorer(f1_score, average=\"micro\")).mean()\n    df_results = pd.DataFrame(data=results, index=[name])\n    return df_results","9982201c":"df_results = pd.DataFrame()\nclf_A = XGBClassifier()\nclf_B = SVC(gamma=\"auto\")\nclf_C = RandomForestClassifier()\nclf_D = ExtraTreesClassifier()\nfor clf in [clf_A, clf_B, clf_C, clf_D]:\n    df_result = train_evaluate(clf, X, y, 4, clf.__class__.__name__)\n    df_results = df_results.append(df_result)\nprint(df_results.sort_values(\"F1\", ascending=False))","90294fe8":"# Optimizing SVC model\nparameters = {\n            'C': [0.01, 0.1, 1, 1.2, 1.4],\n            'kernel':['linear', 'rbf'],\n            'gamma' :[0.01, 0.1, 0.5, 0.9, 1]\n}\nlearner = SVC()\nscorer = make_scorer(f1_score, average=\"micro\")\nclf = GridSearchCV(learner, parameters, scoring=scorer, cv=4)\nclf = clf.fit(X, y)\nbest_clf = clf.best_estimator_\ndf_result = train_evaluate(clf, X, y, 4, \"SVC Optimized\")\ndf_results = df_results.append(df_result)\nprint(df_results.sort_values(\"F1\", ascending=False))","bc26f033":"clf = ExtraTreesClassifier()\nclf.fit(X,y)\ndf = pd.DataFrame({\"importance\":clf.feature_importances_, \"features\":X.columns.values})\ndf = df.sort_values(\"importance\", ascending=False)\nsns.barplot(x=\"importance\", y=\"features\", data=df)","0b343907":"**Training and Testing**","a2c69720":"**Data Preparation**","9eb31b14":"**Feature Importance**","a154e484":"## Summary\n1. Data Preparation\n2. Training and Evaluation\n3. Parameter tuning\n4. Feature importance\n","a3252cb6":"**Parameter Tuning**","ea163534":"# **Quality of Wine**"}}