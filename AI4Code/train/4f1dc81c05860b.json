{"cell_type":{"57abf261":"code","c8252227":"code","7babd783":"code","51599893":"code","5b6f8302":"code","6a962bfe":"code","b1133483":"code","c7bd0614":"code","3f00da20":"code","76d3bf1c":"code","2606e6bc":"code","91e5d36e":"code","0279f104":"code","0f1315ed":"code","4061b2f5":"code","fecee897":"code","3b510616":"code","b4e972bb":"code","5b53cd99":"code","63398657":"code","68a4820e":"code","9311a6cb":"code","360e3f14":"code","b78d8030":"code","59d1fca4":"code","dfe5a1f7":"code","3d661674":"code","ba0a05c4":"code","0b459cd8":"code","b390c7d5":"code","d449fbad":"code","fdb4a240":"code","e015bf1e":"code","61da892b":"code","5a1088e9":"code","45307745":"code","31711843":"code","e7602121":"code","00215b4a":"code","977e3422":"code","e48268e5":"code","7fd6c849":"code","69968039":"code","eec110c9":"code","0d9d70a3":"code","ed2ca683":"code","4633fbd3":"code","9cfbe2e7":"code","e380832b":"code","86b99b60":"code","1185e7ca":"code","204a260b":"code","e8041792":"code","f8cfe740":"code","0a5512bc":"code","8ae2662d":"code","4bce1546":"code","c42fb314":"code","817ca757":"code","3f5872e7":"code","89178938":"code","2206bab3":"code","557c84fe":"code","21058031":"code","96adddb3":"code","619e7bce":"code","f91c6912":"code","31100066":"code","2866096c":"code","3bf3d023":"markdown","a609554d":"markdown","d9cf8a2a":"markdown","a0a4b979":"markdown","da67825d":"markdown","933b90b0":"markdown","8376e6db":"markdown","e1becdf5":"markdown","e3dcea2c":"markdown","c2da6019":"markdown","93eb1fd0":"markdown","0dba1059":"markdown","1406e1d0":"markdown","a1b401e4":"markdown","ff3deb25":"markdown","6cfc518d":"markdown","65b69f86":"markdown","48e4c6cf":"markdown","19985d19":"markdown","23cce36e":"markdown","73077eb1":"markdown","a3e2d4a8":"markdown","3c58acb0":"markdown","2fd649a2":"markdown","74d42a26":"markdown","e773ef8d":"markdown","daa9ebc5":"markdown"},"source":{"57abf261":"# Data processing libraries\nimport numpy as np\nimport pandas as pd\n\n# Plotting libraries for data exploration\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Misc libraries\nimport math\n\n# List all files in working directory:\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c8252227":"# Allows us to see every column in the [dataset].head() calls, keep this commented\n# out unless you want to see the name of every variable (takes a while to run)\n#pd.set_option('display.max_columns', None)\n\ntrain_trans = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv')\ntrain_trans = train_trans.set_index('TransactionID').sort_index()\nprint(\"Training transaction set has %d rows and %d columns\" % train_trans.shape)\ntrain_trans.head()","7babd783":"train_id = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv')\ntrain_id = train_id.set_index('TransactionID').sort_index()\nprint(\"Training identification set has %d rows and %d columns\" % train_id.shape)\ntrain_id.head()","51599893":"test_trans = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')\ntest_trans = test_trans.set_index('TransactionID').sort_index()\nprint(\"Test transaction set has %d rows and %d columns\" % test_trans.shape)\ntest_trans.head()","5b6f8302":"test_id = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv')\ntest_id = test_id.set_index('TransactionID').sort_index()\nprint(\"Test identification set has %d rows and %d columns\" % test_id.shape)\ntest_id.head()","6a962bfe":"### Preparing submission file\nsubmission = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv')\nprint(\"Submission file has %d rows and %d columns\" % submission.shape)\nsubmission.head()","b1133483":"def checkMatches(transaction, identification):\n    isIdentified = []\n    trans_ids = transaction.index.array\n    id_ids = identification.index.array\n    # Since we guarantee order of the transaction ids, we don't need to check the\n    # entirety of id_ids for each id and can just check the current index instead\n    length = len(id_ids)\n    i = 0\n    for id in trans_ids:\n        if i >= length or id != id_ids[i]:\n            isIdentified.append(0) # False - no match found\n        else:\n            i += 1\n            isIdentified.append(1) # True - match found\n    transaction[\"isIdentified\"] = isIdentified\n    return transaction","c7bd0614":"checkMatches(train_trans,train_id)\ntrain_trans.head()","3f00da20":"checkMatches(test_trans,test_id)\ntest_trans.head()","76d3bf1c":"train_combined = train_trans.join(train_id)\nprint(\"Combined training set has %d rows and %d columns\" % train_combined.shape)\ntrain_combined.head()","2606e6bc":"test_combined = test_trans.join(test_id)\nprint(\"Combined test set has %d rows and %d columns\" % test_combined.shape)\ntest_combined.head()","91e5d36e":"numGraphs = 3 # If multiple variables are plotted, this is how many graphs to plot per row\nsubWidth = 12 # Width to allocate for each row of subplots\nsubHeight = 6 # Height to allocate for each subplot\nfont = 8 # Font size for subplots\n\ndef plotNum(setType,dataset,fields):\n    # Plots KDE plots of numerical data, to show distribution of frequencies\n    # setType is a string, either \"train\" or \"test\"\n    sns.set(style='darkgrid')\n    n = len(fields) # Number of variables to plot\n    if n == 1:\n        field = fields[0]\n        data = dataset[field].dropna()\n        plt.xticks(rotation=90)\n        sns.kdeplot(data).set_title(\"%s set %s\" % (setType, field))\n        #print(\"Minimum %s value: %d\" % (field, data.min()))\n        #print(\"Maximum %s value: %d\" % (field, data.max()))\n        #print(\"Average %s value: %d\" % (field, (data.sum()\/len(data))))\n        #print(\"Median %s value: %d\" % (field, data.median()))\n    else:\n        size = (subWidth, subHeight * math.ceil(n\/numGraphs)) # Allot 4 in of height per row\n        if n > numGraphs: \n            fig, axes = plt.subplots(math.ceil(n\/numGraphs), numGraphs, figsize=size)\n        else:\n            fig, axes = plt.subplots(1, n, figsize=size)\n        for i in range(n):\n            field = fields[i]\n            data = dataset[field].dropna()\n            if n > numGraphs:\n                sns.kdeplot(data,ax=axes[i\/\/numGraphs,i % numGraphs]).set_title(\"%s set %s\" % (setType, field))\n            else:\n                sns.kdeplot(data,ax=axes[i]).set_title(\"%s set %s\" % (setType, field))\n            #print(\"Minimum %s value: %d\" % (field, data.min()))\n            #print(\"Maximum %s value: %d\" % (field, data.max()))\n            #print(\"Average %s value: %d\" % (field, (data.sum()\/len(data))))\n            #print(\"Median %s value: %d\" % (field, data.median()))\n        for ax in axes.flatten():\n            for tick in ax.get_xticklabels():\n                tick.set_rotation(90)\n            ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n    plt.show()\n    return None\n\ndef plotCat(setType,dataset,fields):\n    maxCat = 10 # Number of categories to reduce plots to showing\n    sns.set(style='darkgrid')\n    n = len(fields) # Number of variables to plot\n    if n == 1:\n        field = fields[0]\n        data = dataset[field].dropna()\n        plt.xticks(rotation=90)\n        sns.countplot(data, order = data.value_counts().iloc[:maxCat].index).set_title(\"%s set %s\" % (setType, field))\n    else:\n        size = (subWidth, subHeight * math.ceil(n\/numGraphs))\n        if n > numGraphs:\n            fig, axes = plt.subplots(math.ceil(n\/numGraphs), numGraphs, figsize=size)\n        else:\n            fig, axes = plt.subplots(1, n, figsize=size)\n        for i in range(n):\n            field = fields[i]\n            data = dataset[field].dropna()\n            if n > numGraphs:\n                sns.countplot(data, order = data.value_counts().iloc[:maxCat].index,\n                    ax=axes[i\/\/numGraphs,i % numGraphs]).set_title(\"%s set %s\" % (setType, field))\n            else:\n                sns.countplot(data, order = data.value_counts().iloc[:maxCat].index,\n                    ax=axes[i]).set_title(\"%s set %s\" % (setType, field))\n        for ax in axes.flatten():\n            for tick in ax.get_xticklabels():\n                tick.set_rotation(90)\n    plt.show()\n    return None","0279f104":"nulls = train_combined.isnull().sum()\n#nulls = nulls.sort_values()\nnulls.plot(kind='barh', figsize=(10,60), fontsize=10, \n           title = \"Number of Training Set Null Values\")","0f1315ed":"nulls = test_combined.isnull().sum()\nprint(nulls)\n#nulls = nulls.sort_values()\nnulls.plot(kind='barh', figsize=(10,60), fontsize=10, \n           title = \"Number of Test Set Null Values\")","4061b2f5":"plotNum('train',train_combined,['TransactionAmt'])","fecee897":"plotNum('test',test_combined,['TransactionAmt'])","3b510616":"plotCat('train',train_combined,['ProductCD'])","b4e972bb":"plotCat('test',test_combined,['ProductCD'])","5b53cd99":"catFields = []\nnumFields = []\nfor i in [4,6]:\n    catFields.append('card%d' % i)\nfor i in [1,2,3,5]:\n    numFields.append('card%d' % i)","63398657":"plotCat('train',train_combined,catFields)","68a4820e":"plotCat('test',test_combined,catFields)","9311a6cb":"plotNum('train',train_combined,numFields)","360e3f14":"plotNum('test',test_combined,numFields)","b78d8030":"catFields = ['addr1','addr2']","59d1fca4":"plotNum('train',train_combined,catFields)","dfe5a1f7":"plotNum('test',test_combined,catFields)","3d661674":"numFields = ['dist1','dist2']","ba0a05c4":"plotNum('train',train_combined,numFields)","0b459cd8":"plotNum('test',test_combined,numFields)","b390c7d5":"# Extracting email services from a list of addresses:\ndef emailServices(emails):\n    services = []\n    top = 10 # How many of the most frequent providers to display before lumping into \"other\"\n    for email in emails:\n        if type(email) != str:\n            service = float('nan')\n        else:\n            splitIndex = email.find('@')\n            service = email[splitIndex + 1:]\n        services.append(service)\n    return services","d449fbad":"# Adding each set of servicers to the datasets:\ntrain_combined['P_emailprovider'] = emailServices(train_combined['P_emaildomain'])\ntrain_combined['R_emailprovider'] = emailServices(train_combined['R_emaildomain'])\ntest_combined['P_emailprovider'] = emailServices(test_combined['P_emaildomain'])\ntest_combined['R_emailprovider'] = emailServices(test_combined['R_emaildomain'])","fdb4a240":"catFields = ['P_emailprovider','R_emailprovider']","e015bf1e":"plotCat('train',train_combined,catFields)","61da892b":"plotCat('test',test_combined,catFields)","5a1088e9":"numFields = []\nfor i in range(1,15):\n    numFields.append('C%d' % i)","45307745":"plotNum('train',train_combined,numFields)","31711843":"plotNum('test',test_combined,numFields)","e7602121":"numFields = []\nfor i in range(1,16):\n    numFields.append('D%d' % i)","00215b4a":"plotNum('train',train_combined,numFields)","977e3422":"plotNum('test',test_combined,numFields)","e48268e5":"catFields = []\nfor i in range(1,10):\n    catFields.append('M%d' % i)","7fd6c849":"plotCat('train',train_combined,catFields)","69968039":"plotCat('test',test_combined,catFields)","eec110c9":"catFields = ['DeviceType','DeviceInfo']","0d9d70a3":"plotCat('train',train_combined,catFields)","ed2ca683":"plotCat('test',test_combined,catFields)","4633fbd3":"ids = []\nnumFields = []\ncatFields = []\nfor i in range(1,10):\n    ids.append('id_0%d' % i)\nfor i in range(10,39):\n    ids.append('id_%d' % i)\nfor id in ids:\n    idType = train_combined[id].dtype\n    if idType == 'float64':\n        numFields.append(id)\n    else:\n        catFields.append(id)\n    print(id, train_combined[id].dtype)","9cfbe2e7":"plotNum('train',train_combined,numFields)","e380832b":"plotNum('test',test_combined,numFields)","86b99b60":"plotCat('train',train_combined,catFields)","1185e7ca":"plotCat('test',test_combined,catFields)","204a260b":"def resolutionParams(res):\n    # Retrieves width, height, and area from a list of strings\n    widths = []\n    heights = []\n    areas = []\n    for r in res:\n        if type(r) != str:\n            # Catch missing values\n            width = float('nan')\n            height = float('nan')\n            area = float('nan')\n        else:\n            w, h = r.split('x')\n            width = int(w)\n            height = int(h)\n            area = width * height\n        widths.append(width)\n        heights.append(height)\n        areas.append(area)\n    return widths, heights, areas","e8041792":"widths, heights, areas = resolutionParams(train_combined[\"id_33\"])\ntrain_combined[\"width\"] = widths\ntrain_combined[\"height\"] = heights\ntrain_combined[\"area\"] = areas\n\nwidths, heights, areas = resolutionParams(test_combined[\"id_33\"])\ntest_combined[\"width\"] = widths\ntest_combined[\"height\"] = heights\ntest_combined[\"area\"] = areas","f8cfe740":"numFields = [\"width\", \"height\", \"area\"]","0a5512bc":"plotNum('train',train_combined,numFields)","8ae2662d":"plotNum('test',test_combined,numFields)","4bce1546":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split","c42fb314":"catColumns = []\nfor col in train_combined.columns:\n    if train_combined[col].dtype == 'object':\n        catColumns.append(col)\n\ncats = {}\nfor col in catColumns:\n    train_combined[col].fillna('missing',inplace=True)\n    test_combined[col].fillna('missing',inplace=True)\n    \n    train_combined[col] = train_combined[col].astype('category')\n    train_combined[col].cat.add_categories('unknown',inplace=True)\n    cats[col] = train_combined[col].cat.categories","817ca757":"for k, v in cats.items():\n    test_combined[k][~test_combined[k].isin(v)] = 'unknown'","3f5872e7":"from pandas.api.types import CategoricalDtype\n\nfor k, v in cats.items():\n    new_dtype = CategoricalDtype(categories=v, ordered=True)\n    test_combined[k] = test_combined[k].astype(new_dtype)","89178938":"for col in catColumns:\n    train_combined[col] = train_combined[col].cat.codes\n    test_combined[col] = test_combined[col].cat.codes","2206bab3":"train_combined.fillna(-999,inplace=True)\ntest_combined.fillna(-999,inplace=True)","557c84fe":"y = train_combined.pop(\"isFraud\") # Separates results from parameters\nx_train, x_test, y_train, y_test = train_test_split(train_combined, y, train_size = .2)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","21058031":"model = RandomForestRegressor(\n    n_estimators=400, max_features=0.3,\n    min_samples_leaf=20, n_jobs=-1, verbose=1)\nmodel.fit(x_train, y_train)","96adddb3":"pred_test = model.predict(x_test)","619e7bce":"roc_auc_score(y_test, pred_test)","f91c6912":"model = RandomForestRegressor(\n    n_estimators=400, max_features=0.3,\n    min_samples_leaf=20, n_jobs=-1, verbose=1)\nmodel.fit(train_combined,y)","31100066":"pred = model.predict(test_combined)","2866096c":"submission[\"isFraud\"] = pred\nsubmission.to_csv('submission.csv', index=False)","3bf3d023":"# Random Forest Regression Fraud Classification\n\nIn this entry for the IEEE CIS Fraud Detection competition on Kaggle (https:\/\/www.kaggle.com\/c\/ieee-fraud-detection), where a set of credit card transactions must be analyzed to predict a likelihood of fraud, I will be using Scikit-learn to build a Random Forest Regression model for predicting fraud from credit card transaction data.\n\nWhile I didn't finish in time to build my model for submission, I still learned a lot about properly visualizing and manipulating data to fit a regression problem.","a609554d":"### Timedelta (D1-15) Variables\nThese are all numeric variables that measure some time delays between unknown events of importance (ex. possibly measuring time between purchases)","d9cf8a2a":"Now we will display the numeric fields for each set, followed by the categorical fields.","a0a4b979":"### Address 1 and 2\n\nBoth fields are numerical, addr2 is often undefined (Likely due to it only being used if an address is too specific to only be defined by one number)","da67825d":"The datasets seem to be abnormally weighted in favor of small transactions, with some bias as well in favor of the maximum transaction amounts. While the maximum transaction amount is far higher in the training set, the mean and median are virtually identical in both the train and test sets, and so the training set likely is indicative of the test set.","933b90b0":"Each dataset has similar distributions of ProductCD, heavily imbalanced in favor of 'W'.","8376e6db":"### Card 1-6\n\nCard fields 1-3 and 5 are numerical data, whereas card4 and card6 are categorical.","e1becdf5":"### Final Submission","e3dcea2c":"First, we will need to label encode the categorical columns to be understood by the regression algorithm. Credit goes to Yoong Kang Lim for the sections in label encoding the categorical columns: https:\/\/www.kaggle.com\/yoongkang\/beginner-s-random-forest-example","c2da6019":"Some peculiarities to observe from the distribution of null values:\n* It seems that the first few features through card1 are always defined, but the other card# values may rarely be missing values.\n* On the training set, V279-V321 are virtually always defined, but a few of these variables are more frequently undefined on the test set (thus, it would be dangerous to assume engineered features that are never missing on the test set are never going to be missing on real data).\n* There are several clear \"groups\" of features with similar probabilities of being missing values.","93eb1fd0":"### Device Features\n\nDeviceType and DeviceInfo are always strings if defined, and so should be treated as categorical information. DeviceInfo can be a very wide array of values, so I will display only the top 10.","0dba1059":"### ID 01-38\n\nThese are obfuscated values that are of different types, we should first examine what variables are categorical or numeric. Also, note that id numbers 1-9 are listed as 01-09 in the datasets.","1406e1d0":"### Significance of the Features\n\nAs per the discussion post (https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203#latest-624125), the data fields in each dataset have the following summaries as to their significance:\n#### Transaction Tables\n* TransactionDT - timedelta from an unknown reference date (ex. 3 days after an unknown origin)\n* TransactionAMT - payment amount in USD\n* ProductCD - code of the product in the transaction\n* card(1-6) - card info including type, bank, country, etc.\n* addr(1,2) - address\n* dist(1,2) - distance\n* P_emaildomain, R_emaildomain - Purchaser and Recipient email domains\n* C(1-14) - counts of various factors, their meaning is intentionally obfuscated\n* D(1-15) - timedeltas between certain events, such as previous purchases\n* M(1-9) - match, such as name on card and address, etc\n* V(1-339) - engineered features added by Vesta\n\n#### Identification Tables\n* DeviceType - mobile or desktop\n* DeviceInfo - category of computing device used\n* id_(01-38) - a series of variables with their meanings masked, however it is clear that id_33 is device resolution, id_31 is browser used, and id_30 is the OS used. Other id fields include information on IP, ISP, and other networking information\n\nThe transactions in each table are linked via a matching TransactionID, and are rated for being fraud or not in the transaction table's isFraud field (0 for False, 1 for True)\n\nThe modifications I make to the stock dataset include:\n* Adding the sum of N\/A values as a feature for each transaction\n* Splitting id_33 into numerical horizontal and vertical resolution integers\n* Extracting the email service provider as an additional feature from the email domain features","a1b401e4":"### Match 1-9\nThese are categorical variables indicating a match (or lack thereof) being found for some unknown conditions","ff3deb25":"### Data Linking\n\nNow, the transaction and identity tables will be linked together into a singular pandas Dataframe and modified as necessary. I will also be extensively exploring the distributions of each data field for insights to guide my approach in tuning models.\n\nIt is clear from the start that only about 1 in 4 of all the transactions in both the train and test sets have identification data to match them. If a matching ID isn't found in the identification table, we'll simply default all the extra fields for that transaction as 'NaN' and create a new variable 'isIdentified' in the Transaction table that equates to 1 (True) if a matching record was found for that transaction, 0 (False) otherwise.","6cfc518d":"### Model Training\n\nNow for generating a final model, we will import the appropriate Scikit-learn libraries and create train test splits for validation, before generating a final prediction file.","65b69f86":"### Distance 1 and 2\n\nBoth fields are numerical","48e4c6cf":"### Count 1-14 Variables\n\nThese are all numeric data with obfuscated meaning","19985d19":"### Product CD","23cce36e":"As stated in the intro, it would likely be more useful to split the resolution field (id_33) into numeric fields of width and height (I will also include a measure of screen area by multiplying the two).","73077eb1":"Now to join the transaction and identification datasets:","a3e2d4a8":"### TransactionAmt","3c58acb0":"To get a better idea of what formats email addresses are often used, I'll instead put countplots of only the 10 most frequent providers here:","2fd649a2":"Also, it may be of significance to the model to take into account the presence of missing values in certain fields. In the code snippets below, I will be analyzing the frequency of null values on a per-column basis, afterwards replacing all null values with 0:","74d42a26":"### Email Addresses (and Providers)\n\n'P_emaildomain' is a categorical purchaser email address, while 'R_emaildomain' is a categorical recipient email address. The 'P_emailprovider' and 'R_emailprovider' are my own features of the email service extracted from the raw address (everything after the @ in the strings).","e773ef8d":"### Dataset Retrieval\n\nWe now load in each CSV file for the sets of Transaction and Identification tables for the train and test sets. As a first step, we will put them in ascending order according to the TransactionID and display the dataframes' formats.","daa9ebc5":"## Data Exploration\n\nHere, we extensively explore distributions of the features and their relations to each other, in order to inform the model-building process. To consolidate the code, helper functions are defined here for each type of data processed, numerical or categorical."}}