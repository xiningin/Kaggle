{"cell_type":{"1db78028":"code","aa226fa2":"code","1c8bb562":"code","c3efd22c":"code","d6da8a1a":"code","1c8fd88d":"code","395ac7cc":"code","ed9248ec":"code","57e39cc1":"code","c7301553":"code","188a82cc":"code","e22dce2c":"code","d39c6efb":"code","2efa8a3e":"code","fae61a2c":"code","96887e4e":"code","6608df7a":"code","2e47bde2":"code","f2613b61":"code","5f484a7d":"code","c4932bd2":"code","b9271e7b":"code","31a18243":"code","460e2bd3":"code","993f9b12":"code","4672d1ca":"code","eb8ba160":"code","96779753":"code","71168a7d":"code","01bb4daa":"code","24b00636":"code","149ba3eb":"code","1a50a4a4":"code","450d188b":"code","6fdaf0bc":"code","1f0cb1f6":"code","55f32ae9":"code","e480d633":"code","30643820":"code","d789b785":"code","6289167f":"code","510b728e":"code","a9cbd4f4":"code","547b52d5":"code","c2ee0a47":"code","948d9dd3":"code","6f8d575f":"code","4444c366":"code","7ab66cb5":"code","32b877b9":"code","e8f10b23":"code","8be3ec47":"code","1c7b7545":"code","1898bedb":"code","c41f4136":"code","93208a43":"code","d9f5f924":"code","bb77e0fe":"code","b8b48929":"code","5a31a87f":"code","7709a8a6":"code","cf227565":"code","c968fd8b":"code","f1df739c":"code","e2bcbf85":"code","bba0e9c9":"code","b961b98c":"code","4e9fdf54":"code","0778f124":"code","b2a6f92a":"code","5d02a057":"code","e8564572":"markdown","1f7e86df":"markdown","47f62aaa":"markdown","e6ad28bf":"markdown","73c5939b":"markdown","24570da7":"markdown","38246dd6":"markdown","d1528c9d":"markdown","0a37fa6a":"markdown","f7e6c304":"markdown","fdbdd487":"markdown","5f4ad816":"markdown","df7ac369":"markdown","7e50fa00":"markdown","c0d87438":"markdown","bcc05859":"markdown","7bc888e8":"markdown","0c44206d":"markdown","168b5cc1":"markdown","3f050cc2":"markdown","64efc6b1":"markdown","01207796":"markdown","6c9f1f0d":"markdown","86da89d5":"markdown","922abb98":"markdown","4def5f67":"markdown","9730b8d6":"markdown","61def22a":"markdown","ce4b4686":"markdown","f1dddc96":"markdown","014c5f5c":"markdown","2c6b9c98":"markdown","3cd2b53a":"markdown","acc308b9":"markdown","bfebf9e5":"markdown","e33445a1":"markdown","b16abc64":"markdown","3ee1e0a3":"markdown","eccf50e7":"markdown","995ee8d6":"markdown","a08ea86a":"markdown","b8f43be7":"markdown","ce105bff":"markdown","188fc0b6":"markdown","85713e24":"markdown","e1f72e18":"markdown"},"source":{"1db78028":"# Importing Pandas and NumPy\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","aa226fa2":"# Importing all datasets\nchurn_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-analyis\/customer_data.csv\")\ncustomer_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-analyis\/churn_data.csv\")\ninternet_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-analyis\/internet_data.csv\")","1c8bb562":"print(len(churn_data))\nprint(len(customer_data))\nprint(len(internet_data))","c3efd22c":"#Merging on 'customerID'\ndf_1 = pd.merge(churn_data, customer_data, how='inner', on='customerID')","d6da8a1a":"#Final dataframe with all predictor variables\ntelecom = pd.merge(df_1, internet_data, how='inner', on='customerID')","1c8fd88d":"# Let's see the head of our master dataset\ntelecom.head()","395ac7cc":"telecom.describe()","ed9248ec":"# Converting Yes to 1 and No to 0\ntelecom['PhoneService'] = telecom['PhoneService'].map({'Yes': 1, 'No': 0})\ntelecom['PaperlessBilling'] = telecom['PaperlessBilling'].map({'Yes': 1, 'No': 0})\ntelecom['Churn'] = telecom['Churn'].map({'Yes': 1, 'No': 0})\ntelecom['Partner'] = telecom['Partner'].map({'Yes': 1, 'No': 0})\ntelecom['Dependents'] = telecom['Dependents'].map({'Yes': 1, 'No': 0})","57e39cc1":"# Creating a dummy variable for the variable 'Contract' and dropping the first one.\ncont = pd.get_dummies(telecom['Contract'],prefix='Contract',drop_first=True)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,cont],axis=1)\n\n# Creating a dummy variable for the variable 'PaymentMethod' and dropping the first one.\npm = pd.get_dummies(telecom['PaymentMethod'],prefix='PaymentMethod',drop_first=True)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,pm],axis=1)\n\n# Creating a dummy variable for the variable 'gender' and dropping the first one.\ngen = pd.get_dummies(telecom['gender'],prefix='gender',drop_first=True)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,gen],axis=1)\n\n# Creating a dummy variable for the variable 'MultipleLines' and dropping the first one.\nml = pd.get_dummies(telecom['MultipleLines'],prefix='MultipleLines')\n#  dropping MultipleLines_No phone service column\nml1 = ml.drop(['MultipleLines_No phone service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ml1],axis=1)\n\n# Creating a dummy variable for the variable 'InternetService' and dropping the first one.\niser = pd.get_dummies(telecom['InternetService'],prefix='InternetService',drop_first=True)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,iser],axis=1)\n\n# Creating a dummy variable for the variable 'OnlineSecurity'.\nos = pd.get_dummies(telecom['OnlineSecurity'],prefix='OnlineSecurity')\nos1= os.drop(['OnlineSecurity_No internet service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,os1],axis=1)\n\n# Creating a dummy variable for the variable 'OnlineBackup'.\nob =pd.get_dummies(telecom['OnlineBackup'],prefix='OnlineBackup')\nob1 =ob.drop(['OnlineBackup_No internet service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ob1],axis=1)\n\n# Creating a dummy variable for the variable 'DeviceProtection'. \ndp =pd.get_dummies(telecom['DeviceProtection'],prefix='DeviceProtection')\ndp1 = dp.drop(['DeviceProtection_No internet service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,dp1],axis=1)\n\n# Creating a dummy variable for the variable 'TechSupport'. \nts =pd.get_dummies(telecom['TechSupport'],prefix='TechSupport')\nts1 = ts.drop(['TechSupport_No internet service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ts1],axis=1)\n\n# Creating a dummy variable for the variable 'StreamingTV'.\nst =pd.get_dummies(telecom['StreamingTV'],prefix='StreamingTV')\nst1 = st.drop(['StreamingTV_No internet service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,st1],axis=1)\n\n# Creating a dummy variable for the variable 'StreamingMovies'. \nsm =pd.get_dummies(telecom['StreamingMovies'],prefix='StreamingMovies')\nsm1 = sm.drop(['StreamingMovies_No internet service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,sm1],axis=1)","c7301553":"# We have created dummies for the below variables, so we can drop them\ntelecom = telecom.drop(['Contract','PaymentMethod','gender','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n       'TechSupport', 'StreamingTV', 'StreamingMovies'], 1)","188a82cc":"#The varaible was imported as a string we need to convert it to float\ntelecom['TotalCharges'] =pd.to_numeric(telecom['TotalCharges'],errors='coerce')\n#telecom['tenure'] = telecom['tenure'].astype(int).astype(float)","e22dce2c":"telecom.info()","d39c6efb":"# Checking for outliers in the continuous variables\nnum_telecom = telecom[['tenure','MonthlyCharges','SeniorCitizen','TotalCharges']]","2efa8a3e":"# Checking outliers at 25%,50%,75%,90%,95% and 99%\nnum_telecom.describe(percentiles=[.25,.5,.75,.90,.95,.99])","fae61a2c":"# Checking the percentage of missing values\nround(100*(telecom.isnull().sum()\/len(telecom.index)), 2)","96887e4e":"# Removing NaN TotalCharges rows\ntelecom = telecom[~np.isnan(telecom['TotalCharges'])]","6608df7a":"# Checking percentage of missing values after removing the missing values\nround(100*(telecom.isnull().sum()\/len(telecom.index)), 2)","2e47bde2":"# Normalising continuous features\ndf = telecom[['tenure','MonthlyCharges','TotalCharges']]","f2613b61":"normalized_df=(df-df.mean())\/df.std()\ntelecom = telecom.drop(['tenure','MonthlyCharges','TotalCharges'], 1)\ntelecom = pd.concat([telecom,normalized_df],axis=1)\ntelecom.head()","5f484a7d":"churn = (sum(telecom['Churn'])\/len(telecom['Churn'].index))*100\nchurn","c4932bd2":"from sklearn.model_selection import train_test_split\n\n# Putting feature variable to X\nX = telecom.drop(['Churn','customerID'],axis=1)\n\n# Putting response variable to y\ny = telecom['Churn']\n\ny.head()","b9271e7b":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)","31a18243":"import statsmodels.api as sm","460e2bd3":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","993f9b12":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","4672d1ca":"# Let's see the correlation matrix \nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(telecom.corr(),annot = True)","eb8ba160":"X_test2 = X_test.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No','StreamingTV_No','StreamingMovies_No'],1)\nX_train2 = X_train.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No','StreamingTV_No','StreamingMovies_No'],1)","96779753":"plt.figure(figsize = (20,10))\nsns.heatmap(X_train2.corr(),annot = True)","71168a7d":"logm2 = sm.GLM(y_train,(sm.add_constant(X_train2)), family = sm.families.Binomial())\nlogm2.fit().summary()","01bb4daa":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 13)             # running RFE with 13 variables as output\nrfe = rfe.fit(X,y)\nprint(rfe.support_)           # Printing the boolean results\nprint(rfe.ranking_)           # Printing the ranking","24b00636":"# Variables selected by RFE \ncol = ['PhoneService', 'PaperlessBilling', 'Contract_One year', 'Contract_Two year',\n       'PaymentMethod_Electronic check','MultipleLines_No','InternetService_Fiber optic', 'InternetService_No',\n       'OnlineSecurity_Yes','TechSupport_Yes','StreamingMovies_No','tenure','TotalCharges']","149ba3eb":"# Let's run the model using the selected variables\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogsk = LogisticRegression(C=1e9)\n#logsk.fit(X_train[col], y_train)\nlogsk.fit(X_train, y_train)","1a50a4a4":"#Comparing the model with StatsModels\n#logm4 = sm.GLM(y_train,(sm.add_constant(X_train[col])), family = sm.families.Binomial())\nlogm4 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nmodres = logm4.fit()\nlogm4.fit().summary()","450d188b":"X_test[col].shape\n#res = modres.predict(X_test[col])","6fdaf0bc":"# Predicted probabilities\ny_pred = logsk.predict_proba(X_test)\n# Converting y_pred to a dataframe which is an array\ny_pred_df = pd.DataFrame(y_pred)\n# Converting to column dataframe\ny_pred_1 = y_pred_df.iloc[:,[1]]\n# Let's see the head\ny_pred_1.head()","1f0cb1f6":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)\ny_test_df.head()","55f32ae9":"# Putting CustID to index\ny_test_df['CustID'] = y_test_df.index\n# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\n# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df,y_pred_1],axis=1)\n# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 1 : 'Churn_Prob'})\n# Rearranging the columns\ny_pred_final = y_pred_final.reindex(['CustID','Churn','Churn_Prob'], axis=1)\n# Let's see the head of y_pred_final\ny_pred_final.head()","e480d633":"# Creating new column 'predicted' with 1 if Churn_Prob>0.5 else 0\ny_pred_final['predicted'] = y_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.5 else 0)\n# Let's see the head\ny_pred_final.head()","30643820":"from sklearn import metrics","d789b785":"# Confusion matrix \nconfusion = metrics.confusion_matrix( y_pred_final.Churn, y_pred_final.predicted )\nconfusion","6289167f":"# Predicted     Churn  not_churn  __all__\n# Actual\n# Churn            1359   169     1528\n# not_churn         256   326      582\n# __all__          1615   751     2110","510b728e":"#Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Churn, y_pred_final.predicted)","a9cbd4f4":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(6, 6))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr, tpr, thresholds","547b52d5":"draw_roc(y_pred_final.Churn, y_pred_final.predicted)","c2ee0a47":"#draw_roc(y_pred_final.Churn, y_pred_final.predicted)\n\"{:2.2f}\".format(metrics.roc_auc_score(y_pred_final.Churn, y_pred_final.Churn_Prob))","948d9dd3":"X_train.shape\n# We have 30 variables after creating our dummy variables for our categories","6f8d575f":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","4444c366":"#Doing the PCA on the train data\npca.fit(X_train)","7ab66cb5":"pca.components_","32b877b9":"colnames = list(X_train.columns)\npcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\npcs_df.head()","e8f10b23":"%matplotlib inline\nfig = plt.figure(figsize = (8,8))\nplt.scatter(pcs_df.PC1, pcs_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pcs_df.Feature):\n    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\nplt.tight_layout()\nplt.show()","8be3ec47":"pca.explained_variance_ratio_","1c7b7545":"#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","1898bedb":"#Using incremental PCA for efficiency - saves a lot of time on larger datasets\nfrom sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=16)","c41f4136":"df_train_pca = pca_final.fit_transform(X_train)\ndf_train_pca.shape","93208a43":"#creating correlation matrix for the principal components\ncorrmat = np.corrcoef(df_train_pca.transpose())","d9f5f924":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (20,10))\nsns.heatmap(corrmat,annot = True)","bb77e0fe":"# 1s -> 0s in diagonals\ncorrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\nprint(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)\n# we see that correlations are indeed very close to 0","b8b48929":"#Applying selected components to the test data - 16 components\ndf_test_pca = pca_final.transform(X_test)\ndf_test_pca.shape","5a31a87f":"#Training the model on the train data\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlearner_pca = LogisticRegression()\nmodel_pca = learner_pca.fit(df_train_pca,y_train)","7709a8a6":"#Making prediction on the test data\npred_probs_test = model_pca.predict_proba(df_test_pca)[:,1]\n\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test))","cf227565":"pca_again = PCA(0.90)","c968fd8b":"df_train_pca2 = pca_again.fit_transform(X_train)\ndf_train_pca2.shape\n# we see that PCA selected 14 components","f1df739c":"#training the regression model\nlearner_pca2 = LogisticRegression()\nmodel_pca2 = learner_pca2.fit(df_train_pca2,y_train)","e2bcbf85":"df_test_pca2 = pca_again.transform(X_test)\ndf_test_pca2.shape","bba0e9c9":"#Making prediction on the test data\npred_probs_test2 = model_pca2.predict_proba(df_test_pca2)[:,1]\n\"{:2.2f}\".format(metrics.roc_auc_score(y_test, pred_probs_test2))","b961b98c":"%matplotlib inline\nfig = plt.figure(figsize = (8,8))\nplt.scatter(df_train_pca[:,0], df_train_pca[:,1], c = y_train.map({0:'green',1:'red'}))\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.tight_layout()\nplt.show()","4e9fdf54":"%matplotlib notebook\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(8,8))\nax = Axes3D(fig)\n# ax = plt.axes(projection='3d')\nax.scatter(df_train_pca[:,2], df_train_pca[:,0], df_train_pca[:,1], c=y_train.map({0:'green',1:'red'}))","0778f124":"pca_last = PCA(n_components=3)\ndf_train_pca3 = pca_last.fit_transform(X_train)\ndf_test_pca3 = pca_last.transform(X_test)\ndf_test_pca3.shape","b2a6f92a":"#training the regression model\nlearner_pca3 = LogisticRegression()\nmodel_pca3 = learner_pca3.fit(df_train_pca3,y_train)\n#Making prediction on the test data\npred_probs_test3 = model_pca3.predict_proba(df_test_pca3)[:,1]\n\"{:2.2f}\".format(metrics.roc_auc_score(y_test, pred_probs_test3))","5d02a057":"---------------------------------------#Thank You-------------------------------------------------------------------------------------------","e8564572":"### Checking the Correlation Matrix","1f7e86df":"### Dummy Variable Creation","47f62aaa":"#### Applying a logistic regression on our Principal Components\n- We expect to get similar model performance with significantly lower features\n- If we can do so, we would have done effective dimensionality reduction without losing any import information","e6ad28bf":"### Feature Selection Using RFE","73c5939b":"#### Before closing, let's also visualize the data to see if we can spot any patterns","24570da7":"We have almost 27% churn rate","38246dd6":"**Note**\n\nNote that we are fitting the original variable y with the transformed variables (principal components). This is not a problem becuase the transformation done in PCA is *linear*, which implies that you've only changed the way the new x variables are represented, though the nature of relationship between X and Y is still linear. ","d1528c9d":"#### So there it is - a very similar result, without all the hassles. We have not only achieved dimensionality reduction, but also saved a lot of effort on feature selection.","0a37fa6a":"### Dropping highly correlated variables.","f7e6c304":"#### Indeed - there is no correlation between any two components! Good job, PCA!\n- We effectively have removed multicollinearity from our situation, and our models will be much more stable","fdbdd487":"## Telecom Churn: Logistic Regression with PCA\n\nWith 21 predictor variables, we need to predict whether a particular customer will switch to another telecom provider or not. In telecom terminology, customer attrition is referred to as 'churn'.","5f4ad816":"#### We see an overall AUC score of 0.83 looks like we did a decent job.\n- But we did spend a lot of effort on the features and their selection.\n- Can PCA help reduce our effort?","df7ac369":"#### 0.82! Isn't that just amazing!","7e50fa00":"Now we don't have any missing values","c0d87438":"### Checking for Missing Values and Inputing Them","bcc05859":"### Making Predictions","7bc888e8":"#### Note - \n- While computng the principal components, we must not include the entire dataset. Model building is all about doing well on the data we haven't seen yet!\n- So we'll calculate the PCs using the train data, and apply them later on the test data","0c44206d":"#### Let's plot the principal components and try to make sense of them\n- We'll plot original features on the first 2 principal components as axes","168b5cc1":"## Model Building\nLet's start by splitting our data into a training set and a test set.","3f050cc2":"We see that the fist component is in the direction where the 'charges' variables are heavy\n - These 3 components also have the highest of the loadings","64efc6b1":"### Dropping the repeated variables","01207796":"### Data Preparation","6c9f1f0d":"### Checking the Churn Rate","86da89d5":"#### Creating correlation matrix for the principal components - we expect little to no correlation","922abb98":"### PCA on the data","4def5f67":"### Feature Standardisation","9730b8d6":"### Checking for Outliers","61def22a":"### Re-Running the Model","ce4b4686":"After dropping highly correlated variables now let's check the correlation matrix again.","f1dddc96":"### Splitting Data into Training and Test Sets","014c5f5c":"### Model Evaluation","2c6b9c98":"It means that 11\/7043 = 0.001561834 i.e 0.1%, best is to remove these observations from the analysis","3cd2b53a":"Now we can see we have all variables as integer.","acc308b9":"Why not take it a step further and get a little more 'unsupervised' in our approach?\nThis time, we'll let PCA select the number of components basen on a variance cutoff we provide","bfebf9e5":"### Running Your First Training Model","e33445a1":"#### So let's try building the model with just 3 principal components!","b16abc64":"### Importing and Merging Data","3ee1e0a3":"#### Basis transformation - getting the data onto our PCs","eccf50e7":"#### Looking at the screeplot to assess the number of needed principal components","995ee8d6":"From the distribution shown above, you can see that there no outliner in your data. The numbers are gradually increasing.","a08ea86a":"#### Impressive! The same result, without all the hard work on feature selection!","b8f43be7":"Looks like there is a good amount of separation in 2D, but probably not enough\n\nLet's look at it in 3D, and we expect spread to be better (dimensions of variance, remember?)","ce105bff":"#### Looks like 16 components are enough to describe 95% of the variance in the dataset\n- We'll choose 16 components for our modeling","188fc0b6":"### Let's understand the structure of our dataframe","85713e24":"Now let's run our model again after dropping highly correlated variables","e1f72e18":"### Correlation Matrix"}}