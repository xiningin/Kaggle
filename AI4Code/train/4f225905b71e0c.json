{"cell_type":{"eb8f0e2c":"code","dd81df39":"code","642600e7":"code","a9c07665":"code","0219bf96":"code","ac015f0a":"code","d546343d":"code","fe0ecba3":"code","8b3d87be":"code","8a6800cd":"code","42a9cee9":"code","1587564d":"code","1643e13b":"code","f1897e6d":"code","10a1e37b":"code","32caf16b":"code","97850b00":"code","3aa35545":"code","ecf01aa1":"code","99be2ba2":"code","2a7a3eb3":"code","fcc253ec":"code","1fc9bb9f":"code","4fb049f0":"code","cc75002f":"markdown","d4fa59e5":"markdown","326168d1":"markdown","9e840b3e":"markdown","e593eb95":"markdown","2e3b78dc":"markdown","d48204b0":"markdown","0b2a521b":"markdown","f1d6ef54":"markdown","214ce6b3":"markdown","a14803c6":"markdown","f9d14bc2":"markdown","81929573":"markdown","c0e27720":"markdown","5d81bfed":"markdown","75b24223":"markdown","b8bfda24":"markdown"},"source":{"eb8f0e2c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dd81df39":"# This code cell reads the concrete compressive strenght .csv into a Pandas data frame.\nconcrete = pd.read_csv('..\/input\/concrete-compressive-strength-uci\/Concrete_Data.csv')\nconcrete.head()","642600e7":"# As seen above, the column names in this dataset are quite long. \n# The code below will rename the columns so they are easier to work with.\ncolumns = ['cement', 'slag', 'flyash', 'water', 'superplasticizer', 'coarseagg', 'fineagg', 'age', 'strength']\nconcrete.columns = columns\nconcrete.head()","a9c07665":"# Check column data types.\nconcrete.dtypes","0219bf96":"# Check for missing values in the dataset.\nconcrete.isnull().sum()","ac015f0a":"# There is no missing data. Thus our data cleaning efforts will be minimal.\n# This cell provides summary information for the dataframe.\nconcrete.describe()","d546343d":"# The code below allows us to gain a first pass look at the correlation between different feature and our target feature, (compressive) strength.\nconcrete.corr()","fe0ecba3":"# This cell plots key features vs. compressive strength. \nfig = plt.figure(figsize=(15, 10))\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222)\nax3 = fig.add_subplot(223)\nax4 = fig.add_subplot(224)\n\nax1.scatter(concrete.cement, concrete.strength, color='gray')\nax2.scatter(concrete.age, concrete.strength, color='gray')\nax3.scatter(concrete.superplasticizer, concrete.strength, color='gray')\nax4.scatter(concrete.water, concrete.strength, color='gray')\n\nax1.title.set_text('Strength vs. Cement')\nax2.title.set_text('Strength vs. Age')\nax3.title.set_text('Strength vs. Superplasticizer')\nax4.title.set_text('Strength vs. Water')","8b3d87be":"# This cell copies the original DataFrame in order to engineer new features without disturbing the original data.\nconcrete_eng = concrete.copy()\nconcrete_eng.head()","8a6800cd":"# This code cell creates new features for the DataFrame following the procedure of Yeh (1998). \nconcrete_eng['total'] = concrete_eng.cement + concrete_eng.slag + concrete_eng.flyash + concrete_eng.water + concrete_eng.superplasticizer + concrete_eng.coarseagg + concrete_eng.fineagg\nconcrete_eng['binder'] = concrete_eng.cement + concrete_eng.flyash + concrete_eng.slag\nconcrete_eng['wcratio'] = concrete_eng.water\/concrete_eng.cement\nconcrete_eng['wbratio'] = concrete_eng.water\/concrete_eng.binder\nconcrete_eng['spbratio'] = concrete_eng.superplasticizer\/concrete_eng.binder\nconcrete_eng['fabratio'] = concrete_eng.flyash\/concrete_eng.binder\nconcrete_eng['sbratio'] = concrete_eng.slag\/concrete_eng.binder\nconcrete_eng['fasbratio'] = (concrete_eng.flyash + concrete_eng.slag)\/concrete_eng.binder\nconcrete_eng.describe()","42a9cee9":"# Initial look at correlation between engineered features and compressive strength, which helps select key features for visualization.\nconcrete_eng.corr()","1587564d":"# Plots of key engineered features vs. compressive strength.\nengfig = plt.figure(figsize=(15, 10))\nax1 = engfig.add_subplot(221)\nax2 = engfig.add_subplot(222)\nax3 = engfig.add_subplot(223)\nax4 = engfig.add_subplot(224)\n\nax1.scatter(concrete_eng.wcratio, concrete_eng.strength, color='gray')\nax2.scatter(concrete_eng.wbratio, concrete_eng.strength, color='gray')\nax3.scatter(concrete_eng.spbratio, concrete_eng.strength, color='gray')\nax4.scatter(concrete_eng.binder, concrete_eng.strength, color='gray')\n\nax1.title.set_text('Strength vs. W\/C Ratio')\nax2.title.set_text('Strength vs. W\/B Ratio')\nax3.title.set_text('Strength vs. Superplasticizer to Binder Ratio')\nax4.title.set_text('Strength vs. Binder')","1643e13b":"# Create the final DataFrame for model development based on observations from the previous sections.\nfinal_columns = ['cement', 'slag', 'flyash', 'water', 'superplasticizer', 'coarseagg', 'fineagg', 'age', 'strength', 'binder', 'wcratio', 'wbratio']\nconcrete_final = concrete_eng[final_columns]\nconcrete_final.head(10)","f1897e6d":"# The code below sets up the variables for prediction and splits the data into training and testing sets.\nfrom sklearn.model_selection import train_test_split\n\nindep_var = ['cement', 'slag', 'flyash', 'water', 'superplasticizer', 'coarseagg', 'fineagg', 'age', 'binder', 'wcratio', 'wbratio']\ndep_var = ['strength']\nX = concrete_final[indep_var]\ny = concrete_final[dep_var]\nX_test, X_train, y_test, y_train = train_test_split(X, y, test_size = 0.8, random_state=1)","10a1e37b":"# Here four models are set up. The predictions of various models will be compared later in this analysis.\nfrom sklearn import linear_model\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nlinear = linear_model.LinearRegression()\nridge = linear_model.BayesianRidge()\ntree = DecisionTreeRegressor(random_state=1)\nforest = RandomForestRegressor(random_state=1)","32caf16b":"# Each model is fit using the validation data.\nlinear.fit(X_train, y_train.values.ravel())\nridge.fit(X_train, y_train.values.ravel())\ntree.fit(X_train, y_train.values.ravel())\nforest.fit(X_train, y_train.values.ravel())","97850b00":"# This code cell uses the models created above to predict concrete strength with the test data.\nlinear_preds = linear.predict(X_test)\nridge_preds = ridge.predict(X_test)\ntree_preds = tree.predict(X_test)\nforest_preds = forest.predict(X_test)","3aa35545":"# Here I visualize the predicted values vs. the known values.\nengfig = plt.figure(figsize=(15, 10))\nax1 = engfig.add_subplot(221)\nax2 = engfig.add_subplot(222)\nax3 = engfig.add_subplot(223)\nax4 = engfig.add_subplot(224)\n\nax1.scatter(linear_preds, y_test, color='gray')\nax2.scatter(ridge_preds, y_test, color='gray')\nax3.scatter(tree_preds, y_test, color='gray')\nax4.scatter(forest_preds, y_test, color='gray')\n\nax1.title.set_text('Linear Model')\nax2.title.set_text('Ridge Model')\nax3.title.set_text('Decision Tree Model')\nax4.title.set_text('Random Forest Model')","ecf01aa1":"# To compare models beyond visualizations, a measure of accuracy is needed. For this analysis, I use...\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import r2_score\n\nlinear_mse = mean_squared_error(y_test, linear_preds)\nridge_mse = mean_squared_error(y_test, ridge_preds)\ntree_mse = mean_squared_error(y_test, tree_preds)\nforest_mse = mean_squared_error(y_test, forest_preds)\n\nlinear_max = max_error(y_test, linear_preds)\nridge_max = max_error(y_test, ridge_preds)\ntree_max = max_error(y_test, tree_preds)\nforest_max = max_error(y_test, forest_preds)\n\nlinear_r2 = r2_score(y_test, linear_preds)\nridge_r2 = r2_score(y_test, ridge_preds)\ntree_r2 = r2_score(y_test, tree_preds)\nforest_r2 = r2_score(y_test, forest_preds)\n\nprint('Linear model MSE, max error, R2 =', linear_mse, ',', linear_max, ',', linear_r2)\nprint('Ridge model MSE, max error, R2 =', ridge_mse, ',', ridge_max, ',', ridge_r2)\nprint('Decision tree model MSE, max error, R2 =', tree_mse, ',', tree_max, ',', tree_r2)\nprint('Random forest model MSE, max error, R2 =', forest_mse, ',', forest_max, ',', forest_r2)","99be2ba2":"# This cell imports and establishes the permutation importance function.\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nlinear_perm = PermutationImportance(linear, random_state=1).fit(X_train, y_train)\nridge_perm = PermutationImportance(ridge, random_state=1).fit(X_train, y_train)\ntree_perm = PermutationImportance(tree, random_state=1).fit(X_train, y_train)\nforest_perm = PermutationImportance(forest, random_state=1).fit(X_train, y_train)","2a7a3eb3":"# The code below from the eli5 library creates a visualization of permutation importance for the linear model.\neli5.show_weights(linear_perm, feature_names = X_train.columns.tolist())","fcc253ec":"# The code below from the eli5 library creates a visualization of permutation importance for the ridge model.\neli5.show_weights(ridge_perm, feature_names = X_train.columns.tolist())","1fc9bb9f":"# The code below from the eli5 library creates a visualization of permutation importance for the decision tree model.\neli5.show_weights(tree_perm, feature_names = X_train.columns.tolist())","4fb049f0":"# The code below from the eli5 library creates a visualization of permutation importance for the random forest model.\neli5.show_weights(forest_perm, feature_names = X_train.columns.tolist())","cc75002f":"## **DATA EXPLORATION AND VISUALIZATION**","d4fa59e5":"Visualizations are a key step in understanding the dataset. Key features for visualization were selected using the first pass look at correlation coefficients in the **Data Exploration** section. Below are plots of these key features (x-axis) vs. compressive strength of HPC (y-axis). ","326168d1":"## **MODEL DEVELOPMENT**","9e840b3e":"Here I use Permutation Importance to gain insight into which features are most important to the various models. Permutation Importance was selected because it is quick to calculate and relatively easy to understand.","e593eb95":"Traditionally (per Yeh, 1998), the Abrams rule has been used, which states that generally an increase in the water to cement ration decreases concrete strength. The observation from this feature importance analysis tells us that this is generally still true, yet additional cementitious material have been added to HPC, thus they must be included in this ratio, leading to the water to binder ratio acting as a primary control, along with age, of HPC compressive strength. The water to cement ratio and the total amount of binder both play an important role in the models as well. ","2e3b78dc":"Now that mulitple models are defined, we can predict the compressive strength of our test dataset and evaluation\/compare the accuracy of the different models chosen for this problem.  \n  \nI once again create a plot with four subplots, one for each model in this project. Beyond the visual comparison, we can also measure accuracy using MSE, max error, and R2. MSE and R2 were chosen because they are common measures and easily understood. Max error was specifically for this problem. If using HPC for a construction or infrastructure project, it may be useful to know which model has a tendency to make predictions that are way off, as we want to minimize the risk of material failure for safety purposes. ","d48204b0":"## **FEATURE ENGINEERING**","0b2a521b":"The above plots illustrate that none of our current features display a particularly strong relationship with compressive strength. Feature engineering may provide us with features which display stronger relationships. ","f1d6ef54":"## **INTRODUCTION**","214ce6b3":"## **CONCLUSIONS**","a14803c6":"**Findings:** The above analysis demonstrates that the compressive strength of high performace concrete is primarily controlled by the age of the material and the water to binder ratio of the material. This finding is consistent with the work of Yeh (1998). The Decision Tree and Random Forest regression models used here were able to predict the compressive strength of HPC with accuracy in line with the work of Yeh (1998). \n  \n**Objectives:** I was able to successfully accomplish my objectives for this project, my first independent machine learning project. The use of different regression models, along with visualizing and comparing measures of accuracy, allowed me to understand why certain models are used more frequently. While completing Kaggle's Microcourses, I was curious why the machine learning courses began with Decision Tree and Random Forest regression models without any discussion of a basic Linear model, however, this analysis helps demonstrate why (they are significantly more accurate). The feature importance analysis conducted also helped to illustrate what features had the largest impact of the models and led to a potential next step for this project. \n  \n**Learnings and Next Steps:** Not suprisingly, the ensemble model (Random Forest) performed better than the more simple Linear, Ridge, and Decision Tree models, which reinforces the benefit of ensemble methods. Potential next steps to improve the prediction of compressive strength of HPC are: 1) select features for model building based on cutoff values from feature importance values, 2) select additional ensemble methods for model building, and 3) all models used here are first pass models, future models could alter the characteristics (i.e. setting value for max_leaf_nodes) to improve accuracy. ","f9d14bc2":"**Data Source:** Prof. I.-C. Yeh, Chung-Hua University via UC Irvine Machine Learning Repository  \n**Citation:** I-Cheng Yeh, \"Modeling of strength of high performance concrete using artificial neural networks,\" Cement and Concrete Research, Vol. 28, No. 12, pp. 1797-1808 (1998). \n  \n**Motivation:** I have spent the previous four months learning to program with Python and applying it to machine learning applications. In deciding on an initial independent data science project, I searched for a dataset that was relatively easy to work with, of interest to me, and suitable for regression analysis. I selected this dataset because: it is fairly easy to work with, it is suitable for regression analysis, and, as a structural geologist, the compressive strength of materials (usually rocks, but concrete is neat too) is of interest to me.  \n  \n**Abstract:** Concrete has widespread use as a building material and is a primary constituent of many infrastructure and construction projects. Traditional concrete has three ingredients: Portland cement, aggreagates, and water. High-performance concrete (HPC), the focus of this dataset, uses additional material that have a cementitious properties (Yeh, 1998). The addition of these material makes for a complex material, of which the compressive strength is difficult to predict. Given this problem, I.-C. Yeh, the original owner of this dataset, set out to predict the strength of HPC using neural networks. His work produced a model of compressive strength with an R2 of ~0.9 +\/- ~0.05. This measure of accuracy can be used as a benchmark against the models produced in this analysis.  \n  \n**Objective:** The main objective of this project is to understand variability in the prediction of different regression models. I will accomplish this in three steps: 1) predict compressive strength with various regression models, 2) compare the models using visualizations and measures of accuracy, and 3) analyze the importance of features in each model.","81929573":"Using the work of Yeh (1998) as a guide, the following section creates additional features that may be useful in predicting the compressive strength of HPC. Many of the additives to HPC are cementitious by nature, meaning they add to the bonding or binding strength of the mixture. We can create features that look at the ratio of these materials ('Binder') to other ingredients within the HPC (i.e. Water\/Binder Ratio of Yeh, 1998). ","c0e27720":"Within this section, basics data exploration steps are carried out to understand the data, make it easier to work with as a DataFrame, and make initial observations about the distributions of the data and the correlation between each feature and the compressive strength, the target for prediction.","5d81bfed":"## **FEATURE IMPORTANCE ANALYSIS**","75b24223":"The plots above show a relatively strong correlation between water to cement ratio vs. strength and water to binder ratio vs. strength.  The remainder of the engineered features do not show strong correlations. Based on these analyses, it may be helpful to include the water to cement and water to binder ratios in our model, but remove the other engineered features.  ","b8bfda24":"As is clear in both the visualization and the measures of accuracy, the Random Forest model is the most accurate predictor of HPC compressive strength, with the Decision Tree model as a close second. R2 values of 0.92 and 0.88 are within the range of R2 values from the work of Yeh (1998). We could potentially make adjustments to the models used here to increase accuracy, but for now that is beyond the scope of this project. When considering these two models, Random Forest is most likely more accurate than Decision Tree because it is an ensemble learning method.\n  \nAdditionally, we can see the importance of using the max error measure of accuracy. While the Decision Tree model is significanly better in the MSE and R2 measures when compared to the Linear and Ridge models, it is actually worse when using the max error measure. In a real world situation, if a compressive is strength is predicted that is significantly higher than the actual strength, there is a higher likelyhood of failure of the material during its lifespan and thus a higher chance of damage to property or people."}}