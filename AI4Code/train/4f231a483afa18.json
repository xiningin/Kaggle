{"cell_type":{"ca946ab7":"code","946238db":"code","376e89c8":"code","bd38d0eb":"code","3b04b127":"code","f10e0339":"code","ea585848":"code","769ebecb":"code","4d5224fe":"code","6d2fdbee":"code","7fa78dc6":"code","b75337f7":"code","141320bc":"code","40b7cb22":"code","5dd6ac33":"code","f11ac178":"code","470003b8":"code","5a9d9385":"code","28c5550a":"code","85c4ddd1":"code","c9be35a1":"code","67d03e61":"code","793d72ea":"code","fef0d9d7":"code","f25f42c6":"code","3e0b109f":"code","97454174":"code","4159c796":"code","68e429c7":"code","528798a0":"code","2a66123a":"code","efdf040e":"code","50f096a0":"code","fc102605":"code","1e523429":"code","d16575d6":"code","e1573459":"code","50e2ab77":"code","d1f9585e":"code","2820d09c":"code","cac03a5f":"code","c10bdbec":"code","7bac69dd":"markdown","923d7f26":"markdown","dfa6d459":"markdown","e3ebe841":"markdown","7af305c3":"markdown","49008f40":"markdown","a87e50dd":"markdown","2293734a":"markdown","dcdb359a":"markdown","e80c271a":"markdown","ce87dd77":"markdown","9573fff5":"markdown","c53a471f":"markdown","d5d3bdff":"markdown","84254751":"markdown","394a5077":"markdown","612c9460":"markdown","f87ff321":"markdown","80c35e4b":"markdown","2fe7669f":"markdown"},"source":{"ca946ab7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","946238db":"! pip install pandas-profiling[notebook] >> \/dev\/null","376e89c8":"import numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot as plt\n\n\ndf_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndf_gender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","bd38d0eb":"df_train.head()","3b04b127":"df_test.head()","f10e0339":"profile = ProfileReport(df_train, title='Pandas Profiling Report', explorative=True)\nprofile","ea585848":"df_train['Survived'].value_counts(normalize=True)","769ebecb":"sns.countplot(x='Pclass', data=df_train, hue='Survived')","4d5224fe":"for pclass in sorted(df_train['Pclass'].unique()):\n    ratio = df_train[df_train['Pclass'] == pclass]['Survived'].mean()\n    print(f'{pclass}, {ratio:.3f}')","6d2fdbee":"df_train['Family_Size'] = df_train['Parch'] + df_train['SibSp'] + 1\ndf_test['Family_Size'] = df_test['Parch'] + df_test['SibSp'] + 1\nsns.countplot(x='Family_Size', data=df_train, hue='Survived')","7fa78dc6":"for Family_Size in sorted(df_train['Family_Size'].unique()):\n    ratio = df_train[df_train['Family_Size'] == Family_Size]['Survived'].mean()\n    print(f'{Family_Size}, {ratio:.3f}')","b75337f7":"df_train['Sex'].replace(['male','female'], [0, 1], inplace=True)\ndf_train['Embarked'].fillna(('S'), inplace=True)\ndf_train['Embarked'] = df_train['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ndf_train['Fare'].fillna(np.mean(df_train['Fare']), inplace=True)\ndf_train['Age'].fillna(df_train['Age'].median(), inplace=True)\ndf_train['FamilySize'] = df_train['Parch'] + df_train['SibSp'] + 1\ndf_train['IsAlone'] = 0\ndf_train.loc[df_train['FamilySize'] == 1, 'IsAlone'] = 1","141320bc":"df_test['Sex'].replace(['male','female'], [0, 1], inplace=True)\ndf_test['Embarked'].fillna(('S'), inplace=True)\ndf_test['Embarked'] = df_test['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ndf_test['Fare'].fillna(np.mean(df_test['Fare']), inplace=True)\ndf_test['Age'].fillna(df_test['Age'].median(), inplace=True)\ndf_test['FamilySize'] = df_test['Parch'] + df_test['SibSp'] + 1\ndf_test['IsAlone'] = 0\ndf_test.loc[df_test['FamilySize'] == 1, 'IsAlone'] = 1","40b7cb22":"df_train['Title'] = df_train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf_test['Title'] = df_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","5dd6ac33":"df_train['Title'] = df_train['Title'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ndf_train['Title'] = df_train['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ndf_train['Title'] = df_train['Title'].replace('Mlle', 'Miss')\ndf_train['Title'] = df_train['Title'].replace('Ms', 'Miss')\ndf_train['Title'] = df_train['Title'].replace('Mme', 'Mrs')\n\ndf_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","f11ac178":"df_test['Title'] = df_test['Title'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ndf_test['Title'] = df_test['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ndf_test['Title'] = df_test['Title'].replace('Mlle', 'Miss')\ndf_test['Title'] = df_test['Title'].replace('Ms', 'Miss')\ndf_test['Title'] = df_test['Title'].replace('Mme', 'Mrs')","470003b8":"#map each of the title groups to a numerical value\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\n\ndf_train['Title'] = df_train['Title'].map(title_mapping)\ndf_train['Title'] = df_train['Title'].fillna(0)\ndf_test['Title'] = df_test['Title'].map(title_mapping)\ndf_test['Title'] = df_test['Title'].fillna(0)","5a9d9385":"df_train","28c5550a":"df_test","85c4ddd1":"df_train.shape, df_test.shape","c9be35a1":"df_train.head()","67d03e61":"df_test","793d72ea":"df_all = pd.concat([df_train[df_test.columns], df_test], axis=0)\ndf_all","fef0d9d7":"Cabin2num = dict(zip(list(df_all['Cabin'].unique()), np.arange(len(list(df_all['Cabin'].unique())))))\ndf_train['Cabin'] = df_train['Cabin'].map(Cabin2num)\ndf_test['Cabin'] = df_test['Cabin'].map(Cabin2num)","f25f42c6":"Embarked2num = dict(zip(list(df_all['Embarked'].unique()), np.arange(len(list(df_all['Embarked'].unique())))))\ndf_train['Embarked'] = df_train['Embarked'].map(Embarked2num)\ndf_test['Embarked'] = df_test['Embarked'].map(Embarked2num)","3e0b109f":"Sex2num = dict(zip(list(df_all['Sex'].unique()), np.arange(len(list(df_all['Sex'].unique())))))\ndf_train['Sex'] = df_train['Sex'].map(Sex2num)\ndf_test['Sex'] = df_test['Sex'].map(Sex2num)","97454174":"df_train.head()","4159c796":"df_test.head()","68e429c7":"params = {\n    'objective': 'binary',\n    'learning_rate': 0.05,\n    'max_depth': 4,\n    'min_data_in_leaf': 5\n}","528798a0":"target_col = 'Survived'\ny = df_train[target_col]\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n\n\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(df_train, y)):\n    print('*'*20, fold_id, '*'*20)\n    X_train, y_train = df_train.iloc[train_index], y.iloc[train_index]\n    X_val, y_val = df_train.iloc[valid_index], y.iloc[valid_index]\n    print(X_train.shape, X_val.shape)\n    print('train\\n', y_train.value_counts() \/ len(y_train))\n    print('val\\n',y_val.value_counts() \/ len(y_val))","2a66123a":"target_col = 'Survived'\nmodels = []\n# Ouot of fild\u306e\u7565. \u624b\u5143\u306e\u30c7\u30fc\u30bf\u304cvalidation\u3060\u3063\u305f\u6642\u306e\u4e88\u6e2c\u5024\noof = np.zeros(len(df_train))\ny = df_train[target_col]\nimportances = pd.DataFrame()\nscores = []\ndrop_cols = ['PassengerId', 'Name', 'Ticket', 'Survived']\ncate_cols = ['Cabin', 'Embarked', 'Pclass', 'Sex']\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n\n\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(df_train, y)):\n    X_train, y_train = df_train.iloc[train_index], y.iloc[train_index]\n    X_val, y_val = df_train.iloc[valid_index], y.iloc[valid_index]\n    \n    # \u4e0d\u8981\u5217\u306e\u524a\u9664\n    X_train = X_train.drop(drop_cols, axis=1)  # \u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f7f\u308f\u306a\u3044\u5217\u306e\u524a\u9664\n    X_val = X_val.drop(drop_cols, axis=1)\n    \n    print(X_train.shape, y_train.shape)\n    features = sorted(X_train.columns)\n    print(y_train.mean(), y_val.mean())\n    X_train = X_train[features]\n    X_val = X_val[features]\n\n    # dataset\n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cate_cols)\n    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cate_cols)\n    print(X_train.shape, X_val.shape)\n\n    # \u5b66\u7fd2\n    print('train...')\n    lgb_model = lgb.train(params,\n                          train_data,\n                          num_boost_round=1000,\n                          early_stopping_rounds=20,\n                          valid_sets=[train_data, val_data],\n                          verbose_eval=200)\n    print('write..')\n    models.append(lgb_model)\n    lgb_model.save_model(f'{fold_id+1}_model.lgb')\n    \n    # oof\n    print('oof...')\n    y_pred = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration)\n    oof[valid_index] = y_pred\n    \n    print('importance...')\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = features\n    imp_df['gain'] = lgb_model.feature_importance()\n    imp_df['fold'] = 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    # validaation\u30c7\u30fc\u30bf\u3078\u306e\u7cbe\u5ea6. \u4e88\u6e2c\u5024\u304c0.5\u4ee5\u4e0a\u306e\u3082\u306e\u3092\u751f\u5b58\u4e88\u6e2c\u3068\u3059\u308b\n    score = accuracy_score(y_val, y_pred>0.5)\n    scores.append(score)\n    print('*'*10, f'cv_score_{score}', '*'*10)","efdf040e":"# 5\u3064\u306ecv\u306e\u4e88\u6e2c\u7cbe\u5ea6\nscores","50f096a0":"def save_importances(importances_: pd.DataFrame):\n    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n    plt.figure(figsize=(8, 12))\n    sns.barplot(\n        x='gain',\n        y='feature',\n        data=importances_.sort_values('mean_gain', ascending=False)[:300])\n    plt.tight_layout()\n    plt.savefig('importances.png')\nsave_importances(importances)","fc102605":"# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3078\u306e\u4e88\u6e2c. 5cv\u3067\u4f5c\u3089\u308c\u305f5\u3064\u306e\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u5024\u306e\u5e73\u5747\u5024\ny_test_pred  = np.zeros(len(df_test))\nfor model in models:\n    y_test_pred += model.predict(df_test[features])\ny_test_pred = y_test_pred \/ len(models)","1e523429":"df_sub = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ndf_sub","d16575d6":"plt.hist(y_test_pred)","e1573459":"y_test_pred = y_test_pred > 0.5","50e2ab77":"df_sub['Survived'] = y_test_pred","d1f9585e":"df_sub['Survived'] = df_sub['Survived'].astype(int)\ndf_sub","2820d09c":"df_sub['Survived'].value_counts() \/ len(df_sub)","cac03a5f":"df_train['Survived'].value_counts() \/ len(df_train)","c10bdbec":"df_sub.to_csv(\"submission.csv\", index=False)","7bac69dd":"# \u30c7\u30fc\u30bf\u306e\u78ba\u8a8d","923d7f26":"\u2191train\u3068val\u3067\u76ee\u7684\u5909\u6570\u306e\u5206\u5e03\u304c\u4e00\u81f4","dfa6d459":"Pclass(Ticket class)\u306b\u3088\u308a\u751f\u5b58\u7387\u304c\u5927\u304d\u304f\u5909\u308f\u308a\u305d\u3046","e3ebe841":"# \u4eee\u8aac\u30d9\u30fc\u30b9\u3067\u306e\u7279\u5fb4\u91cf\u4f5c\u6210","7af305c3":"CV\u3092\u5207\u308b  \n\u624b\u5143\u306b\u3042\u308b\u30c7\u30fc\u30bf\u304b\u3089\u306a\u3093\u3068\u304b\u672a\u77e5\u306e\u65b0\u898f\u30c7\u30fc\u30bf\u3063\u307d\u3044\u72b6\u6cc1\u3092\u4f5c\u308a\u51fa\u3059\u3092\u8907\u6570\u56de\u7e70\u308a\u8fd4\u3059  \n\u4eca\u56de\u306fStratifiedKFold(\u6b63\u76f4\u305d\u3053\u307e\u3067\u4e0d\u5747\u8861\u3067\u306a\u3044\u306e\u3067\u3001random split\u3067\u3044\u3044\u304b\u3082)  \nvalidation\u30c7\u30fc\u30bf\u3078\u306e\u4e88\u6e2c\u7cbe\u5ea6\u3067\u771f\u306e\u4e88\u6e2c\u7cbe\u5ea6\u3092\u985e\u63a8\u3059\u308b","49008f40":"# EDA","a87e50dd":"## pandas_profiling","2293734a":"\u30b3\u30e1\u30f3\u30c8\n- \u6642\u9593\u306e\u95a2\u4fc2\u7aef\u6298\u308a\u307e\u3059\u304c\u3001\u672c\u5f53\u306f\u3082\u3063\u3068\u3061\u3083\u3093\u3068\u53ef\u8996\u5316\u3059\u3079\u304d\n- \u3084\u308a\u8fbc\u3082\u3046\u3068\u601d\u3048\u3070\u7121\u9650\u306bEDA\u51fa\u6765\u308b\u306e\u3067\u7279\u306b\u4ed5\u4e8b\u3067\u306f\u5fc5\u8981\u306a\u90e8\u5206\u3092\u6700\u77ed\u3067\u884c\u3046\u80fd\u529b\u304c\u6c42\u3081\u3089\u308c\u308b\n- \u53c2\u8003: https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic","dcdb359a":"# \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092map","e80c271a":"kaggle\u306e\u4ed5\u69d8\u3068\u3057\u3066\u3001'submission.csv'\u3092\u6240\u5b9a\u306e\u4f4d\u7f6e\u306b\u66f8\u304d\u51fa\u3059\u3068submit\u3067\u304d\u308b","ce87dd77":"\u30b3\u30e1\u30f3\u30c8: \n- \u3053\u3093\u306a\u611f\u3058\u3067\u4eee\u8aac\u30d9\u30fc\u30b9\u3067\u7279\u5fb4\u91cf\u3092\u4f5c\u308b\u3053\u3068\u304c\u591a\u3044\n- \u3082\u3061\u308d\u3093\u30b3\u30f3\u30da\u7279\u6709\u306e\u30a2\u30a4\u30c7\u30a3\u30a2\u3082\u5fc5\u8981\u3060\u304c\u3001\u8133\u5185DB\u304b\u3089\u904e\u53bb\u30b3\u30f3\u30da\u3067\u3053\u3093\u306a\u7279\u5fb4\u91cf\u304c\u52b9\u3044\u3066\u3044\u305f\u3068\u3044\u3046\u60c5\u5831\u3092\u5f15\u3063\u5f35\u308a\u51fa\u3059\u3053\u3068\u3082\u5927\u5207\u3002\n- \u5168\u7279\u5fb4\u91cf\u53ef\u8996\u5316\u3092\u3059\u308b\u304b\u3068\u8a00\u308f\u308c\u308c\u3070\u3001\u3059\u308b\u3079\u304d\u3060\u304c\u9762\u5012\u306a\u306e\u3067\u3057\u306a\u3044\u3002\u7279\u5fb4\u91cf\u8ffd\u52a0\u3057\u3066CV\u5024\u304c\u4e0a\u304c\u308b\u304b\u3092\u78ba\u8a8d","9573fff5":"## \u7c21\u5358\u306b\u53ef\u8996\u5316","c53a471f":"\u500b\u4eba\u7684\u306b\u300c\u3078\u30fc\u300d\u3068\u601d\u3063\u305f\u30e9\u30f3\u30ad\u30f3\u30b01\u4f4d\u306e\u540d\u524d\u306e\u80a9\u66f8\u304d\u7279\u5fb4\u91cf","d5d3bdff":"# \u305d\u306e\u4ed6\u7279\u5fb4\u91cf\u8ffd\u52a0","84254751":"# sub\u3092\u4f5c\u308b","394a5077":"## \u4eee\u8aac\u306b\u57fa\u3065\u304d\u7279\u5fb4\u91cf\u3092\u4f5c\u308b\n\n\u4f8b\u3048\u3070\u5bb6\u65cf\u306e\u4eba\u6570(Parch + sibsp)\u306f\u751f\u5b58\u7387\u306b\u52b9\u304d\u305d\u3046\u3068\u3044\u3046\u4eee\u8aac","612c9460":"# \u4e00\u56de\u30c7\u30fc\u30bf\u306e\u78ba\u8a8d","f87ff321":"Warning\u306f\u300c\u3053\u308c\u4ee5\u4e0a\u6728\u3092\u6210\u9577\u3067\u304d\u307e\u305b\u3093\u300d\u3068\u3044\u3046\u610f\u5473  \nhttps:\/\/github.com\/microsoft\/LightGBM\/issues\/640  \n(\u30bf\u30a4\u30bf\u30cb\u30c3\u30af\u306bLightGBM\u306f\u3060\u3044\u3076\u5927\u9248\u306a\u306e\u3067\u3001\u6728\u3092\u5207\u308c\u306a\u304f\u306a\u308b\u3053\u3068\u306b\u7d0d\u5f97\u306f\u3057\u3066\u3044\u308b)  \n(\u3053\u306e\u7a0b\u5ea6\u306e\u30c7\u30fc\u30bf\u306a\u3089\u3001\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u65b9\u304c\u3044\u3044\u304b\u3082\u3057\u308c\u306a\u3044)","80c35e4b":"\u9762\u5012\u306a\u306e\u3067\u3001\u8272\u3005\u306anotebook\u304b\u3089\u30b3\u30d4\u30da","2fe7669f":"# cv\u3092\u4f5c\u6210\u3057\u3001\u5b66\u7fd2"}}