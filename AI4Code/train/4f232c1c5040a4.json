{"cell_type":{"557156d0":"code","d82528d3":"code","874c6d16":"code","8bb15269":"code","55d2d9f8":"code","3ea0c0ab":"code","7a4fdc7a":"code","a05130e0":"code","74142777":"code","fcd140bd":"code","c842180a":"code","6b6c2458":"code","ebc459fb":"code","108031ab":"code","d73dec52":"code","52298a0e":"code","60d80653":"code","e3b54a3c":"code","76066af7":"code","76bf355d":"code","bf61efe0":"code","78e161cb":"code","a938a571":"code","5333781c":"code","aedf8d3f":"code","09da7608":"code","4fc53fb4":"code","46fe80c8":"code","e05f2488":"code","b8099194":"code","369d08b6":"code","8cbf7739":"code","262b17af":"code","fb39b7c4":"code","fab3a37e":"code","a1d3f55d":"code","6966302d":"code","54463146":"code","bc1e2a36":"code","69654356":"code","e87e65b7":"code","cbeafce6":"code","9dbffa44":"code","b4a2c66c":"code","f1a53525":"code","61dd0c6e":"code","ad7ad1ee":"code","714294a8":"code","f1793a72":"code","1e64e0f5":"code","702ee67c":"code","63aecdd3":"code","36ce92dd":"code","19537c87":"code","cef2196a":"code","53cd9541":"code","39feba40":"code","4a5ecdc1":"code","9bb78fb5":"code","1297a4cf":"code","7dcbbdd9":"code","2d16fddb":"code","78c17133":"code","372ae22b":"code","3cac311b":"code","611872f0":"code","027b4143":"code","e9585ee4":"code","ea2872b9":"code","b94fbeaf":"code","af2e130b":"code","7a13e027":"code","57752709":"code","7f2c6373":"code","1a6904d3":"code","5ff9325a":"code","e1331f62":"code","f3d4e3ed":"code","7085a397":"code","16e455cc":"code","49d2c6ae":"code","2b1c175a":"code","ce553be6":"code","923215a7":"code","3ac58fd4":"code","847058d1":"code","c167f9e4":"code","25386fd5":"markdown","226c1830":"markdown","b893463b":"markdown","63a10c56":"markdown","1ac0d1a1":"markdown","63cec812":"markdown","70cc4d63":"markdown","c86bb698":"markdown","09b125dd":"markdown","a7740e2f":"markdown","1460c135":"markdown","bf00319a":"markdown","b46e9a96":"markdown","e0d20fe9":"markdown","c7c7848a":"markdown"},"source":{"557156d0":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","d82528d3":"df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","874c6d16":"df.head()","8bb15269":"df[\"target\"].value_counts(), df.info(), df.describe()","55d2d9f8":"y = df[\"target\"]\nX = df.drop([\"target\", \"id\"], axis = 1)","3ea0c0ab":"def logisticRegression(X,y):\n    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42)\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score, log_loss\n    lrModel = LogisticRegression()\n    lrModel.fit(train_x, train_y)\n    print(accuracy_score(lrModel.predict(test_x), test_y))\n    print(log_loss(lrModel.predict(test_x), test_y))\n    return lrModel","7a4fdc7a":"logisticRegression(X,y)","a05130e0":"y.value_counts()","74142777":"# from imblearn.over_sampling import SMOTE\n# sm = SMOTE(random_state=42, k_neighbors=3, n_jobs = 5)\n# X,y = sm.fit_resample(X,y)","fcd140bd":"logisticRegression(X,y)","c842180a":"X = pd.DataFrame(X)\nX.std().plot()","6b6c2458":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX = ss.fit_transform(X)\nlrModel = logisticRegression(X,y)\ntest_df_pred = lrModel.predict_proba(test_df.iloc[:,1:])","ebc459fb":"X = pd.DataFrame(X)\nX.head()","108031ab":"test_df_pred","d73dec52":"submit_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmit_df.head()\nsubmit_df[\"target\"] = test_df_pred\n# submit_df.to_csv(\"lrModel.csv\", index = False) ","52298a0e":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression(),\n    XGBClassifier()]","60d80653":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)","e3b54a3c":"for clf in classifiers:\n    name = clf.__class__.__name__\n    clf.fit(X_train, y_train)\n    acc = clf.score(X_test,y_test)\n    print(\"{0}: {1}\".format(name,acc))","76066af7":"def applyModel(model, X,y):\n    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42)\n    from sklearn.metrics import accuracy_score, log_loss\n    model.fit(train_x, train_y)\n#     print(accuracy_score(model.predict(test_x), test_y))\n#     print(log_loss(model.predict(test_x), test_y))\n    return (accuracy_score(model.predict(test_x), test_y), log_loss(model.predict(test_x), test_y))","76bf355d":"qdaModel = QuadraticDiscriminantAnalysis()\napplyModel(qdaModel, X, y)","bf61efe0":"submit_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmit_df.head()\ntest_df_pred = qdaModel.predict_proba(test_df.iloc[:,1:])\nsubmit_df[\"target\"] = test_df_pred\n# submit_df.to_csv(\"qdaModel Base 2.csv\", index = False)","78e161cb":"from sklearn.model_selection import StratifiedShuffleSplit\nStratifiedShuffleSplit()","a938a571":"def featureExtractionAndModel(stat_model, feature_ext_model, X, y):\n    if(clf.__class__.__name__ == \"LinearDiscriminantAnalysis\") or (clf.__class__.__name__ == \"RFE\") or (clf.__class__.__name__ == \"SelectKBest\"):\n        train_x, test_x, train_y, test_y = train_test_split(feature_ext_model.fit_transform(X, y), y, test_size = 0.2, random_state = 42)\n    else:\n        train_x, test_x, train_y, test_y = train_test_split(feature_ext_model.fit_transform(X), y, test_size = 0.2, random_state = 42)\n    from sklearn.metrics import accuracy_score, log_loss\n    stat_model.fit(train_x, train_y)\n#     print(accuracy_score(model.predict(test_x), test_y))\n#     print(log_loss(model.predict(test_x), test_y))\n    \n    return (accuracy_score(stat_model.predict(test_x), test_y), log_loss(stat_model.predict(test_x), test_y))","5333781c":"lrModel_FE = LogisticRegression()","aedf8d3f":"from sklearn.manifold import LocallyLinearEmbedding\nfrom sklearn.manifold import SpectralEmbedding\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import SelectKBest\n\nextractors = [\n    LocallyLinearEmbedding(n_components = 150),\n    SpectralEmbedding(n_components = 150),\n    PCA(n_components=150,svd_solver='full'),\n    LinearDiscriminantAnalysis(n_components = 150),\n    FastICA(n_components = 150),\n    TSNE(n_components = 3),\n    RFE(lrModel_FE, n_features_to_select = 100),\n    SelectKBest(k = 150)\n    ]","09da7608":"for clf in extractors:\n    name = clf.__class__.__name__\n#     print(name)\n    acc = featureExtractionAndModel(lrModel_FE, clf, X, y)\n    print(\"{0}: {1:.4f} {2:.4f}\".format(name,acc[0], acc[1]))","4fc53fb4":"acc_lst = []\nlog_loss_lst = []\nfor i in range(25, 150, 5):\n    acc = applyModel(lrModel_FE, RFE(lrModel_FE, n_features_to_select = i).fit_transform(X, y), y)\n#     print(\"{0}: {1:.4f} {2:.4f}\".format(i, acc[0], acc[1]))\n    acc_lst.append(acc[0])\n    log_loss_lst.append(acc[1])\nplt.plot(range(25, 150, 5), acc_lst)","46fe80c8":"acc_lst = []\nlog_loss_lst = []\nfor i in range(155, 250, 1):\n    acc = applyModel(lrModel_FE, SelectKBest(k = i).fit_transform(X, y), y)\n    print(\"{0}: {1:.4f} {2:.4f}\".format(i, acc[0], acc[1]))\n    acc_lst.append(acc[0])\n    log_loss_lst.append(acc[1])\nplt.plot(range(155, 250, 1), acc_lst)","e05f2488":"rfe = RFE(lrModel_FE, n_features_to_select = 50)\napplyModel(lrModel_FE, rfe.fit_transform(X, y), y)\nsubmit_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmit_df.head()\ntest_df_pred = lrModel_FE.predict(rfe.transform(test_df.iloc[:,1:]))\nsubmit_df[\"target\"] = test_df_pred\n# submit_df.to_csv(\"lrModel RFE.csv\", index = False) #0.519","b8099194":"kbest = SelectKBest(k = 173)\napplyModel(lrModel_FE, kbest.fit_transform(X, y), y)\nsubmit_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmit_df.head()\ntest_df_pred = lrModel_FE.predict(kbest.transform(test_df.iloc[:,1:]))\nsubmit_df[\"target\"] = test_df_pred\n# submit_df.to_csv(\"lrModel SelectKBest.csv\", index = False) 0.500","369d08b6":"acc_lst = []\nlog_loss_lst = []\nfor i in range(70, 299, 1):\n    acc = applyModel(tuned_lr_model, PCA(n_components= i).fit_transform(X), y)\n    print(\"{0}: {1:.4f} {2:.4f}\".format(i, acc[0], acc[1]))\n    acc_lst.append(acc[0])\n    log_loss_lst.append(acc[1])\nplt.plot(range(70, 299, 1), acc_lst)","8cbf7739":"tuned_lr_model = LogisticRegression(C=2.4, class_weight=None, \n                                                      dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=8, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n          tol=0.0001, warm_start=False)","262b17af":"pca = PCA(n_components = 73)\napplyModel(tuned_lr_model, pca.fit_transform(X), y)\nsubmit_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmit_df[\"target\"] = tuned_lr_model.predict(pca.transform(test_df.iloc[:,1:]))\n# submit_df.to_csv(\"lrModel tuned and PCA.csv\", index = False) #0.499","fb39b7c4":"import eli5\nweights_df = eli5.formatters.as_dataframe.explain_weights_df(tuned_lr_model, top=200)\nweights_df[\"weight\"] = weights_df[\"weight\"].abs()\nweights_df = weights_df.sort_values(by=\"weight\", ascending=False)\nweights_df.iloc[:20,:]","fab3a37e":"best_features = [int(feature[1:]) for feature in weights_df[\"feature\"] if feature!=\"<BIAS>\"][:10]\n# X.loc[:, best_features]","a1d3f55d":"acc_lst = []\nlog_loss_lst = []\nfor i in range(5, 150, 1):\n    best_features = [int(feature[1:]) for feature in weights_df[\"feature\"] if feature!=\"<BIAS>\"][:i]\n    acc = applyModel(lrModel_FE, X.loc[:, best_features], y)\n    print(\"{0}: {1:.4f} {2:.4f}\".format(i, acc[0], acc[1]))\n    acc_lst.append(acc[0])\n    log_loss_lst.append(acc[1])\nplt.plot(range(5, 150, 1), acc_lst)","6966302d":"best_features = [int(feature[1:]) for feature in weights_df[\"feature\"] if feature!=\"<BIAS>\"][:10]\napplyModel(tuned_lr_model, X.loc[:, best_features], y)\nsubmit_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmit_df[\"target\"] = tuned_lr_model.predict(test_df.iloc[:,best_features])\nsubmit_df.to_csv(\"lrModel tuned and ELI5.csv\", index = False) #0.499","54463146":"best_features = [int(feature[1:]) for feature in weights_df[\"feature\"] if feature!=\"<BIAS>\"][:10]\napplyModel(lrModel_FE, X.loc[:, best_features], y)\nsubmit_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmit_df[\"target\"] = lrModel_FE.predict(test_df.iloc[:,best_features])\nsubmit_df.to_csv(\"lrModel_FE and ELI5.csv\", index = False) #0.499","bc1e2a36":"### Lets create a model on GaussianNB and use it on test data","69654356":"# gnb = GaussianNB()\n# gnb.fit(X,y)\n# test_df_pred = pd.DataFrame({\"target\": gnb.predict_proba(test_df.iloc[:,1:])})","e87e65b7":"# pd.read_csv(\"..\/input\/sample_submission.csv\").head()","cbeafce6":"# test_df_pred[\"id\"] = test_df[\"id\"]","9dbffa44":"# test_df_pred.head()","b4a2c66c":"# test_df_pred.to_csv(\"Submission.csv\", index=False)","f1a53525":"# import matplotlib.pyplot as plt","61dd0c6e":"# from sklearn.decomposition import PCA\n# pca = PCA().fit(X)\n# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n# plt.xlim(0,250,1)","ad7ad1ee":"# sklearn_pca = PCA(n_components=250)\n# print(sklearn_pca)","714294a8":"# X_train_pca = sklearn_pca.fit_transform(X_train)\n# print(X_train_pca.shape)\n\n# X_test_pca = sklearn_pca.transform(X_test)\n# print(X_test_pca.shape)","f1793a72":"# classifiers = [\n#     KNeighborsClassifier(3),\n#     SVC(probability=True),\n#     DecisionTreeClassifier(),\n#     RandomForestClassifier(),\n#     AdaBoostClassifier(),\n#     GradientBoostingClassifier(),\n#     GaussianNB(),\n#     LinearDiscriminantAnalysis(),\n#     QuadraticDiscriminantAnalysis(),\n#     LogisticRegression(),\n#     XGBClassifier()]\n# for clf in classifiers:\n#     name = clf.__class__.__name__\n#     clf.fit(X_train_pca, y_train)\n#     acc = clf.score(X_test_pca,y_test)\n#     print(\"{0}: {1}\".format(name,acc))","1e64e0f5":"# sklearn_pca = PCA(n_components=200)\n# print(sklearn_pca)","702ee67c":"# X_pca = sklearn_pca.fit_transform(X)\n# print(X_pca.shape)\n\n# test_pca = sklearn_pca.transform(test_df.iloc[:,1:])\n# print(test_pca.shape)","63aecdd3":"# qda = QuadraticDiscriminantAnalysis()\n# qda.fit(X_pca,y)\n# test_df_pred = pd.DataFrame({\"target\": qda.predict_proba(test_pca)})\n# test_df_pred[\"id\"] = test_df[\"id\"]\n# test_df_pred.to_csv(\"Submission with pca and qda.csv\", index=False)","36ce92dd":"# qda = QuadraticDiscriminantAnalysis()\n# qda.fit(X,y)\n# test_df_pred = pd.DataFrame({\"target\": qda.predict_proba(test_df)})\n# test_df_pred[\"id\"] = test_df[\"id\"]\n# test_df_pred.to_csv(\"Submission with qda.csv\", index=False)","19537c87":"# from sklearn.model_selection import GridSearchCV","cef2196a":"# xgb_tuning = XGBClassifier(learning_rate =0.1,\n#  n_estimators=1000,\n#  max_depth=5,\n#  min_child_weight=1,\n#  gamma=0,\n#  subsample=0.8,\n#  colsample_bytree=0.8,\n#  objective= 'binary:logistic',\n#  nthread=4,\n#  scale_pos_weight=1,\n#  seed=27,\n# n_gpus=1)","53cd9541":"# xgb_tuning.fit(X_train,y_train)","39feba40":"# xgb_tuning.score(X_test,y_test)","4a5ecdc1":"# param_test1 = {\n#  'max_depth':[7,8,9,10],\n#  'min_child_weight':[0,1,2]\n# }","9bb78fb5":"# gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n#                                                  min_child_weight=1, gamma=0, \n#                                                   subsample=0.8, colsample_bytree=0.8,\n#                                                   objective= 'binary:logistic',\n#                                                   nthread=32, scale_pos_weight=1, seed=27,n_gpus=1), \n#                         param_grid = param_test1, scoring='roc_auc',n_jobs=32,iid=False, cv=5,verbose=4)\n# gsearch1.fit(X_train,y_train)\n# gsearch1.best_params_, gsearch1.best_score_","1297a4cf":"# xgboost_params = { \"n_estimators\": 400, 'tree_method':'gpu_hist', 'predictor':'gpu_predictor' }","7dcbbdd9":"# param_test3 = {\n#  'gamma':[i\/10.0 for i in range(0,5)]\n# }\n# gsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, max_depth=9,\n#  min_child_weight=0, gamma=0, subsample=0.8, colsample_bytree=0.8,\n#  objective= 'binary:logistic', nthread=32, scale_pos_weight=1,seed=27,n_gpus=1,**xgboost_params), \n#  param_grid = param_test3, scoring='roc_auc',n_jobs=32,iid=False,verbose=4 ,cv=5)\n# gsearch3.fit(X,y)\n# gsearch3.best_params_, gsearch3.best_score_","2d16fddb":"# param_test4 = {\n#  'colsample_bytree':[i\/10.0 for i in range(7,10)],\n#     'subsample': [i\/10.0 for i in range(7,10)]\n# }\n# gsearch4 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, max_depth=9,\n#  min_child_weight=0, gamma=0.0, subsample=0.8, colsample_bytree=0.8,\n#  objective= 'binary:logistic', nthread=32, scale_pos_weight=1,seed=27,n_gpus=1,**xgboost_params), \n#  param_grid = param_test4, scoring='roc_auc',n_jobs=32,iid=False,verbose=5 ,cv=5)\n# gsearch4.fit(X,y)\n# gsearch4.best_params_, gsearch4.best_score_","78c17133":"# param_test5 = {\n#     'subsample': [i\/10.0 for i in range(5,8)]\n# }\n# gsearch5 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, max_depth=9,\n#  min_child_weight=0, gamma=0.0, subsample=0.8, colsample_bytree=0.8,\n#  objective= 'binary:logistic', nthread=32, scale_pos_weight=1,seed=27,n_gpus=1,**xgboost_params), \n#  param_grid = param_test5, scoring='roc_auc',n_jobs=32,iid=False,verbose=5 ,cv=5)\n# gsearch5.fit(X,y)\n# gsearch5.best_params_, gsearch5.best_score_","372ae22b":"# param_test7 = {\n#  'reg_alpha':[i\/10.0 for i in range(0,3)],\n#     'reg_lambda':[i\/10.0 for i in range(9,11)]\n# }\n# gsearch7 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.11, max_depth=9,\n#  min_child_weight=0, gamma=0.0, subsample=0.7, colsample_bytree=0.8,\n#  objective= 'binary:logistic', nthread=5, scale_pos_weight=1,seed=27,n_gpus=1,**xgboost_params), \n#  param_grid = param_test7, scoring='roc_auc',n_jobs=32,iid=False,verbose=5 ,cv=5)\n# gsearch7.fit(X,y)\n# gsearch7.best_params_, gsearch7.best_score_","3cac311b":"# param_test8 = {\n#  'learning_rate' :[0.08,0.09,0.1,0.11,0.12]\n# }\n# gsearch8 = GridSearchCV(estimator = XGBClassifier(\n#                                 learning_rate =0.1,max_depth=9,min_child_weight=0, gamma=0.0, \n#                                 subsample=0.7, colsample_bytree=0.8,objective= 'binary:logistic', \n#                                 nthread=5,reg_alpha= 0.1, reg_lambda= 0.9, \n#                                 scale_pos_weight=1,seed=27,n_gpus=1,**xgboost_params), \n#                         param_grid = param_test8, scoring='roc_auc',\n#                         n_jobs=32,iid=False,verbose=5 ,cv=5\n#                        )\n# gsearch8.fit(X,y)\n# gsearch8.best_params_, gsearch8.best_score_","611872f0":"# xgb_tuning = XGBClassifier(learning_rate =0.11,\n#  n_estimators=400,\n#  max_depth=9,\n#  min_child_weight=0,\n#  gamma=0.0,\n#  subsample=0.7,\n#  colsample_bytree=0.8,\n#  objective= 'binary:logistic',\n#  nthread=4,\n#  scale_pos_weight=1,\n#  seed=27,\n# reg_alpha= 0.1, reg_lambda= 0.9,\n# n_gpus=1,\n# tree_method='gpu_hist',\n# predictor='gpu_predictor')","027b4143":"# xgb_tuning.fit(X_train,y_train)","e9585ee4":"# xgb_tuning.score(X_test,y_test)","ea2872b9":"# X.shape","b94fbeaf":"# test_df.head()","af2e130b":"# xgb_tuning.fit(X,y)\n# test_df_pred = pd.DataFrame({\"target\": xgb_tuning.predict_proba(np.array(test_df.drop(\"id\",axis=1)))})\n# test_df_pred[\"id\"] = test_df[\"id\"]\n# test_df_pred.to_csv(\"Submission xgb tuning.csv\", index=False)","7a13e027":"# from sklearn.linear_model import LogisticRegression","57752709":"# lrcv = LogisticRegression(C=50000,penalty=\"l2\")\n# lrcv.fit(X_train,y_train)","7f2c6373":"# lrcv.score(X_test,y_test)","1a6904d3":"# param1 = {\n#  'penalty':[\"l1\",\"l2\"]\n# }\n# gsearch = GridSearchCV(estimator = LogisticRegression(C=5, class_weight=None, dual=False,\n#                                                       fit_intercept=True,\n#           intercept_scaling=1, max_iter=100, multi_class='warn',\n#           n_jobs=None, penalty='l1', random_state=42, solver='warn',\n#           tol=0.0001, verbose=0, warm_start=False), \n#  param_grid = param1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n# gsearch.fit(X,y)\n# gsearch.best_params_,gsearch.best_score_","5ff9325a":"# param2 = {\n#  'C':[i\/1000 if i!=0 else 1 for i in range(0,100000,10)]\n# }\n# gsearch2 = GridSearchCV(estimator = LogisticRegression(C=5, class_weight=None, dual=False,\n#                                                        fit_intercept=True,\n#           intercept_scaling=1, max_iter=100, multi_class='warn',\n#           n_jobs=None, penalty='l2', random_state=42, solver='warn',\n#           tol=0.0001, verbose=0, warm_start=False), \n#  param_grid = param2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n# gsearch2.fit(X,y)\n# gsearch2.best_params_, gsearch2.best_score_","e1331f62":"# param3 = {\n#  'tol':[i\/10000 if i!=0 else 1 for i in range(0,1000,1)]\n# }\n# gsearch3 = GridSearchCV(estimator = LogisticRegression(C=2.4, class_weight=None, \n#                                                       dual=False, fit_intercept=True,\n#           intercept_scaling=1, max_iter=100, multi_class='warn',\n#           n_jobs=None, penalty='l2', random_state=42, solver='warn',\n#           tol=0.0001, verbose=0, warm_start=False), \n#  param_grid = param3, scoring='roc_auc',verbose=4,n_jobs=32,iid=False, cv=5)\n# gsearch3.fit(X,y)\n# gsearch3.best_params_, gsearch3.best_score_","f3d4e3ed":"# param4 = {\n#  'max_iter':[i for i in range(8,100,1)]\n# }\n# gsearch4 = GridSearchCV(estimator = LogisticRegression(C=2.4, class_weight=None, \n#                                                       dual=False, fit_intercept=True,\n#           intercept_scaling=1, max_iter=100, multi_class='warn',\n#           n_jobs=None, penalty='l2', random_state=42, solver='warn',\n#           tol=0.0001, verbose=0, warm_start=False), \n#  param_grid = param4, scoring='roc_auc',verbose=4,n_jobs=32,iid=False, cv=5)\n# gsearch4.fit(X,y)\n# gsearch4.best_params_, gsearch4.best_score_","7085a397":"# param5 = {\n#  'class_weight':[\"balanced\",None],\n# }\n# gsearch5 = GridSearchCV(estimator = LogisticRegression(C=2.4, class_weight=None, \n#                                                       dual=False, fit_intercept=True,\n#           intercept_scaling=1, max_iter=8, multi_class='warn',\n#           n_jobs=None, penalty='l2', random_state=42, solver='warn',\n#           tol=0.0001, verbose=0, warm_start=False), \n#  param_grid = param5, scoring='roc_auc',verbose=4,n_jobs=-1,iid=False, cv=5)\n# gsearch5.fit(X,y)\n# gsearch5.best_params_, gsearch5.best_score_","16e455cc":"# from sklearn.linear_model import LogisticRegression","49d2c6ae":"# lr = LogisticRegression(C=2.4, class_weight=\"balanced\", \n#                                                       dual=False, fit_intercept=True,\n#           intercept_scaling=1, max_iter=8, multi_class='warn',\n#           n_jobs=None, penalty='l2', random_state=42, solver='warn',\n#           tol=0.0001, warm_start=False)\n# lr.fit(X_train,y_train)","2b1c175a":"# lr.score(X_test,y_test)","ce553be6":"# lr.fit(X,y)\n# y_pred = pd.DataFrame({\"target\":lr.predict_proba(test_df.iloc[:,1:]).flatten()}, index=test_df.index)\n# y_pred[\"id\"] = test_df[\"id\"] \n# y_pred.to_csv(\"submission with tuned logistic regression.csv\", index = False)","923215a7":"# from sklearn.ensemble import RandomForestClassifier ","3ac58fd4":"# rf = RandomForestClassifier(n_estimators=200, n_jobs=4, class_weight='balanced', max_depth=6)\n# rf.fit(X_train,y_train)\n# rf.score(X_test, y_test)","847058d1":"# from sklearn.feature_selection import RFE","c167f9e4":"# lr_rfe = RFE(lr, 75, step=1)\n# lr_rfe.fit(X_train,y_train)\n# # scores_table(selector, 'selector_clf')\n# lr_rfe.score(X_test, y_test) \n# y_pred = lr_rfe.predict_proba(test_df.iloc[:,1:])\n# s = pd.read_csv('..\/input\/sample_submission.csv')\n# s[\"target\"] = y_pred\n# s.to_csv(\"submission with RFE.csv\", index = False)","25386fd5":"### So we have 0.611 as our score. We can improve this with a bit of a feature engineering.  Lets apply decomposition on the dataset and see what happens.","226c1830":"* Higly imbalanced data. Applying SMOTE to even out the target data.","b893463b":"* Feature engineering for figuring out the kind of data available to us and building pipeline for it.","63a10c56":"* #### Below code is for practice. check out original by [Andrew Lukyanenko](https:\/\/www.kaggle.com\/artgor\/how-to-not-overfit\/notebook)","1ac0d1a1":"### XGBoostClassifier is not getting us anywhere. It needs more data to perform better. Lets try logistic regression","63cec812":"## Problem objective\n\n### To fit a model to the data given such that it does not overfit. ","70cc4d63":"### We will try Recursive feature elimination with our lr object","c86bb698":"#### RFE did not work well with the data. Other feature extraction techniques that can be tried are PCA, SelectKBest, ICA, LLE. Let's create a function to iteratively try all the techniques","09b125dd":"* Significant improvement from just using SMOTE on our highly imbalanced target. We will try to apply other set of normalizations based on data. ","a7740e2f":"#### Let's apply QuadraticDiscriminantAnalysis and figure out our LB score. ","1460c135":"#### Worse off compared to our logistic regression with 0.500 score. ","bf00319a":"### Preliminary hypothesis \n* Fit a base classification model without any feature engineering. Choosing logistic regression","b46e9a96":"#### LB score 0.504. Let's create another base model with the same X and y. ","e0d20fe9":"## GridSearchCV on QDA","c7c7848a":"#### RFE and SelectKBest seem promising let's delve into it further"}}