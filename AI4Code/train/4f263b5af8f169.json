{"cell_type":{"60407733":"code","f9f76a50":"code","1f8f1c33":"code","50c89831":"code","444ba4c1":"code","7e2c65c2":"code","9d027b1f":"code","ec909fa8":"code","e9ab0c97":"code","b1a75ba8":"code","afce21d7":"code","2204c8cd":"code","a3cc57a3":"code","94834bdf":"code","d6bc20c5":"code","95ff2fca":"code","19273cba":"code","bbe95cc9":"code","45b9a128":"markdown","f461e9a9":"markdown","0715fdff":"markdown","a7c814a7":"markdown","aa253152":"markdown","001a93ed":"markdown","d456117c":"markdown","b0e974db":"markdown","377f8bb9":"markdown","06c9b297":"markdown","612813b8":"markdown","3ad1d76c":"markdown","33a97e71":"markdown"},"source":{"60407733":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Flatten, Dense, Dropout\nimport time\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.callbacks import ReduceLROnPlateau\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import LearningRateScheduler\n\n\n# Any results you write to the current directory are saved as output.","f9f76a50":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","1f8f1c33":"train.shape","50c89831":"y_train = train['label']\nX_train  = train.drop(labels = \"label\", axis=1)","444ba4c1":"X_train.head(10)","7e2c65c2":"del train","9d027b1f":"sns.countplot(y_train)\nplt.show()","ec909fa8":"print(X_train.isnull().any().describe())\nprint(test.isnull().any().describe())","e9ab0c97":"X_train = X_train\/255.0\ntest = test \/ 255.0","b1a75ba8":"X_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","afce21d7":"y_train= to_categorical(y_train)\ncount_classes = y_train.shape[1]\ncount_classes","2204c8cd":"random_seed = 2\n# Split the train and the validation set for the fitting\nx_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state=random_seed)","a3cc57a3":"cnn = Sequential()\n#Layer 1 : 16 filters\ncnn.add(Conv2D(filters=16,\n               kernel_size=(5,5),\n               strides=(1,1),\n               padding='same',\n               input_shape=(28,28,1),\n               data_format='channels_last'))\ncnn.add(Activation('relu'))\n\ncnn.add(MaxPooling2D(pool_size=(2,2),\n                     strides=2))\n\ncnn.add(Dropout(0.25))\n\n\n# Layer 2: 32 Filters\ncnn.add(Conv2D(filters=32,\n               kernel_size=(5,5),\n               strides=(1,1),\n               padding='same'))\ncnn.add(Activation('relu'))\ncnn.add(MaxPooling2D(pool_size=(2,2),\n                     strides=2))\n\ncnn.add(Dropout(0.2))\n\n\n# Layer 3: 64 Filters\ncnn.add(Conv2D(filters=64,\n               kernel_size=(5,5),\n               strides=(1,1),\n               padding='same'))\ncnn.add(Activation('relu'))\ncnn.add(MaxPooling2D(pool_size=(2,2),\n                     strides=2))\n\ncnn.add(Dropout(0.2))\n\n\n# Layer : 128 Filters\ncnn.add(Conv2D(filters=128,\n               kernel_size=(5,5),\n               strides=(1,1),\n               padding='same'))\ncnn.add(Activation('relu'))\ncnn.add(MaxPooling2D(pool_size=(2,2),\n                     strides=2))\n\ncnn.add(Dropout(0.2))\n\ncnn.add(BatchNormalization())\n\n\n# Output of layer 4 is flattened, to send to fully connected layers\ncnn.add(Flatten())\n\n# Fully connected layer 1\ncnn.add(Dense(1024))\ncnn.add(Activation('relu'))\n\ncnn.add(Dropout(0.25))\n\n# Fully connected layer 2\ncnn.add(Dense(1024))\ncnn.add(Activation('relu'))\n\ncnn.add(Dropout(0.5))\n\n# Final output layer to predict 10 target classes\ncnn.add(Dense(10))\ncnn.add(Activation('softmax'))","94834bdf":"shift = 0.2\ndatagen = ImageDataGenerator(zoom_range = 0.1,\n                            height_shift_range = shift,\n                            width_shift_range = shift)","d6bc20c5":"cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n# Fit model with training set and check on validation set, to avoid overfitting\nstart = time.time()\ncnn_result=cnn.fit_generator(\n    datagen.flow(x_train, y_train, batch_size=16),\n    steps_per_epoch = 1000,\n    epochs=30,\n    validation_data=(x_val[:1000,:], y_val[:1000,:]),\n    validation_steps = 100, callbacks= [annealer])\nend = time.time()\nprint('Training processing time:',(end - start)\/60)","95ff2fca":"final_loss, final_acc = cnn.evaluate(x_val, y_val, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))","19273cba":"plt.plot(cnn_result.history['loss'], color='b')\nplt.plot(cnn_result.history['val_loss'], color='r')\nplt.show()\nplt.plot(cnn_result.history['acc'], color='b')\nplt.plot(cnn_result.history['val_acc'], color='r')\nplt.show()","bbe95cc9":"test_set = test.reshape(-1, 28, 28 , 1).astype('float32')\n\nresult = cnn.predict(test_set)\nresult = np.argmax(result,axis = 1)\nresult = pd.Series(result, name=\"Label\")\nsubmit = pd.concat([pd.Series(range(1 ,28001) ,name = \"ImageId\"),   result],axis = 1)\nsubmit.to_csv(\"MNIST_Test.csv\",index=False)\nsubmit.head(10)","45b9a128":"**This kernel explains the classification of MNIST dataset from 0 to 9 using Convolutional Neural Networks with Data Augmentation using Keras**","f461e9a9":"Convolutional neural networks are complex than standard multi-layer perceptrons, therfore, a simple structure is used that uses all of the elements for state of the art results. Below is the network architecture.\n\nThe first hidden layer is a convolutional layer called as Convolution2D which has 16 feature maps with the size of 5\u00d75. This is the input layer. It gets with the structure outline above [pixels][width][height].\nNext is a pooling layer that takes the max called MaxPooling2D. It is configured with a pool size of 2\u00d72 and 2 strides.\nThe next layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 25% of neurons in the layer in order to reduce overfitting.\n\nThe second hidden layer is a convolutional layer is Convolution2D which has 32 feature maps with the size of 5\u00d75 . This is the input layer. Then is a pooling layer and is configured with a pool size of 2\u00d72 and 2 strides.\nThe next layer is a dropout to randomly exclude 20% of neurons in the layer in order to reduce overfitting.\n\nThe third hidden layer is a convolutional layer is Convolution2D which has 64 feature maps with the size of 5\u00d75 . This is the input layer followed by pooling layer and is configured with a pool size of 2\u00d72 and 2 strides.\nThe next layer is a dropout to randomly exclude 20% of neurons in the layer in order to reduce overfitting.\n\nThe fourth hidden layer is a convolutional layer is Convolution2D which has 128 feature maps with the size of 5\u00d75 . This is the input layer followed by pooling layer and is configured with a pool size of 2\u00d72 and 2 strides.\nThe next layer is a dropout to randomly exclude 20% of neurons in the layer.\n\nNext comes a layer that converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by standard fully connected layers.\n\nFollowed by flatten layer is a fully connected layer with 1024 neurons and rectifier activation function 'relu' and dropout to exclude 25% neurons. Next again is a fully connected layer with 1024 neurons and dropout to exclude 50% neurons.\nFinally is the output layer has 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class.","0715fdff":"Layer that has parameters (filters\/ weights), is only counted as one layer,i.e. Pooling and dropouts are part of convolutional layer and not independent layers\n\n","a7c814a7":"Data augmentation of image is a technique that is used to artificially expand the size of a training dataset by creating modified versions of images in the dataset. The Keras deep learning neural network library provides the function to fit models using image data augmentation via the ImageDataGenerator class. A various techniques are supported, as well as pixel scaling methods. We will focus on three main types of data augmentation techniques for image data as follows:\n\n* Image zoom via the zoom_range argument.\n* Image shifts via the width_shift_range.\n* Image shifts via height_shift_range arguments.\n* Random rotations of the MNIST digits up to 90 degrees by setting the rotation_range argument\n* Random flipping along both the vertical and horizontal axes using the vertical_flip and horizontal_flip arguments\n* ZCA whitening transform by setting the zca_whitening argument to True.\n\n","aa253152":"Delete unwanted variables","001a93ed":"\u00b4Counplot shows the contsof all the classes from 0 to 9","d456117c":"The labels or the y_train is  between 0 and 9 and  we need to convert these to one-hot encoding,Onen hot encoding removes the encoded variable and adds a new inary variable for each unique integer. Thus, here we have a 10x1 array with one 1 and nine 0:s","b0e974db":"# Data Augmentation","377f8bb9":"Splitting the dataset into train and validation set. As we already have a test set, ew will later predict the result using test set.","06c9b297":"We train once with a smaller learning rate to ensure convergence. We then speed things up, only to reduce the learning rate by 10% every epoch. Keras has a function for this is LearningRateScheduler.\nA part of the validation set is taken to reduce time consumption and speed up things.","612813b8":"Load the training and test sets","3ad1d76c":"Now we know that there are no missing values and the data is super clean. Therefore, we can easily go ahead.","33a97e71":"Each image is 28 x 28 pixel (784 pixels)"}}