{"cell_type":{"e9b00490":"code","0019efb6":"code","b6949324":"code","8eb6be18":"code","7eace58d":"code","5681265b":"code","1424b910":"code","bd58f65a":"code","bf6cd759":"code","44a8c521":"code","c97147dd":"code","3ea95d84":"code","3e8ef9c3":"code","29a02126":"code","7d78d6f6":"code","6cd2bb11":"code","6633f39d":"markdown","01d84af5":"markdown","1c3d75d3":"markdown","069f438a":"markdown","d91ad593":"markdown","1838e55d":"markdown","ff1c8b8f":"markdown","e023ae3f":"markdown","801222c7":"markdown","e6d56b80":"markdown","33b4a4b1":"markdown"},"source":{"e9b00490":"import os\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection as sk_model_selection\nfrom sklearn import preprocessing as sk_preprocessing\nfrom sklearn import linear_model as sk_linear_model\nfrom sklearn import svm as sk_svm\nfrom sklearn import tree as sk_tree\nfrom sklearn import ensemble as sk_ensemble\nfrom sklearn import neighbors as sk_neighbors\nfrom sklearn import metrics as sk_metrics\nimport lightgbm as lgbm","0019efb6":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n\n\nSEED = 42\nset_seed(SEED)","b6949324":"def print_metrics(y_true, y_pred):\n    accuracy = sk_metrics.accuracy_score(y_true, y_pred)\n    f1 = sk_metrics.f1_score(y_true, y_pred)\n    precision = sk_metrics.precision_score(y_true, y_pred)\n    recall = sk_metrics.recall_score(y_true, y_pred)\n    \n    print(f'Accuracy (test set)\\t| {accuracy:.4f}')\n    print(f'F1 (test set)\\t\\t| {f1:.4f}')\n    print(f'Precision (test set)\\t| {precision:.4f}')\n    print(f'Recall (test set)\\t| {recall:.4f}')\n    \n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n    }","8eb6be18":"def fit_model_with_grid_search(model, parameters, scoring='f1', verbose=1):\n    model = sk_model_selection.GridSearchCV(\n        model,\n        parameters,\n        scoring=scoring\n    )\n    \n    model.fit(X_train, y_train)\n    \n    if verbose:\n        print(f'best_params_: {model.best_params_}')\n        print(f'Mean cross-validated F1 score of the best_estimator: {model.best_score_:.4f}')\n        \n    return model","7eace58d":"dict_results = {}","5681265b":"df = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\nprint(df.shape)\ndf.head(5)","1424b910":"X_data = df.drop(['DEATH_EVENT'], axis=1).values\ny_data = df['DEATH_EVENT'].values\nX_data.shape, y_data.shape","bd58f65a":"X_train, X_test, y_train, y_test = sk_model_selection.train_test_split(\n    X_data, \n    y_data, \n    test_size=0.2, \n    random_state=42, \n    shuffle=True, \n    stratify=y_data\n)\nprint(f'X_train shape: {X_train.shape} y_train.shape: {y_train.shape}')\nprint(f'X_test shape: {X_test.shape} y_test.shape: {y_test.shape}')","bf6cd759":"scaler = sk_preprocessing.StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","44a8c521":"model_logistic_regression = sk_linear_model.LogisticRegression(\n    class_weight='balanced', \n    random_state=SEED,\n)\n\nparameters = {\n    'C': [0.01, 0.1, 1],\n}\n\nmodel_logistic_regression = fit_model_with_grid_search(\n    model_logistic_regression,\n    parameters,\n    scoring='f1',\n)\n\ny_test_pred = model_logistic_regression.predict(X_test)\n\nprint()\ndict_results['Logistic Regression'] = print_metrics(y_test, y_test_pred)","c97147dd":"model_svc = sk_svm.SVC(\n    class_weight='balanced', \n    random_state=SEED,\n)\n\nparameters = {\n    'C': [0.01, 0.1, 1],\n    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n    'gamma': ['scale', 'auto'],\n}\n\nmodel_svc = fit_model_with_grid_search(\n    model_svc,\n    parameters,\n    scoring='f1',\n)\n\ny_test_pred = model_svc.predict(X_test)\n\nprint()\ndict_results['SVC'] = print_metrics(y_test, y_test_pred)","3ea95d84":"model_decision_tree = sk_tree.DecisionTreeClassifier(\n    class_weight='balanced', \n    random_state=SEED,\n)\n\nparameters = {\n    \"max_depth\": [1, 2, 3, 5, 10, None], \n    \"min_samples_leaf\": [1, 5, 10, 20],\n}\n\nmodel_decision_tree = fit_model_with_grid_search(\n    model_decision_tree,\n    parameters,\n    scoring='f1',\n)\n\ny_test_pred = model_decision_tree.predict(X_test)\n\nprint()\ndict_results['Decision Tree'] = print_metrics(y_test, y_test_pred)","3e8ef9c3":"model_random_forest = sk_ensemble.RandomForestClassifier(\n    class_weight='balanced', \n    random_state=SEED,\n)\n\nparameters = {\n    \"n_estimators\": [5, 10, 15, 20],\n    \"max_depth\": [1, 2, 3, 5, 10, None],\n    \"min_samples_leaf\": [1, 5, 10, 20]\n}\n\nmodel_random_forest = fit_model_with_grid_search(\n    model_random_forest,\n    parameters,\n    scoring='f1',\n)\n\ny_test_pred = model_random_forest.predict(X_test)\n\nprint()\ndict_results['Random Forest'] = print_metrics(y_test, y_test_pred)","29a02126":"model_k_neighbors = sk_neighbors.KNeighborsClassifier()\n\nparameters = {\n    \"n_neighbors\": list(range(1, 11)),\n    \"weights\": ['uniform', 'distance'],\n}\n\nmodel_k_neighbors = fit_model_with_grid_search(\n    model_k_neighbors,\n    parameters,\n    scoring='f1',\n)\n\ny_test_pred = model_k_neighbors.predict(X_test)\n\nprint()\ndict_results['K-Neighbors'] = print_metrics(y_test, y_test_pred)","7d78d6f6":"model_lgbm = lgbm.LGBMClassifier(\n    class_weight='balanced',\n    random_state=SEED,\n)\n\nparameters = {\n    'num_leaves': [7, 15, 31],\n    'learning_rate': [0.001, 0.01, 0.1],\n    'n_estimators': [100, 200, 300],\n    'reg_alpha': [1],\n    'reg_lambda': [1],\n    'colsample_bytree': [0.5, 0.75, 1.]\n}\n\nmodel_lgbm = fit_model_with_grid_search(\n    model_lgbm,\n    parameters,\n    scoring='f1',\n)\n\ny_test_pred = model_lgbm.predict(X_test)\n\nprint()\ndict_results['LightGBM'] = print_metrics(y_test, y_test_pred)","6cd2bb11":"pd.DataFrame(dict_results).T","6633f39d":"<a id=\"7\"><\/a>\n<h2 style='background:red; border:0; color:white'><center>K-Nearest Neighbor<center><h2>","01d84af5":"<a id=\"2\"><\/a>\n<h2 style='background:red; border:0; color:white'><center>Data Scaling<center><h2>","1c3d75d3":"<a id=\"5\"><\/a>\n<h2 style='background:red; border:0; color:white'><center>Decision Tree<center><h2>","069f438a":"<a id=\"9\"><\/a>\n<h2 style='background:red; border:0; color:white'><center>Final Comparison<center><h2>","d91ad593":"<a id=\"1\"><\/a>\n<h2 style='background:red; border:0; color:white'><center>Data Loading and Splitting<center><h2>","1838e55d":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:red; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h3>\n\n* [1. Data Loading and Splitting](#1)\n* [2. Data Scaling](#2)\n* [3. Logistic Regression](#3)\n* [4. Support Vector Machine](#4)\n* [5. Decision Tree](#5)    \n* [6. Random Forest](#6)\n* [7. K-Nearest Neighbor](#7)\n* [8. LightGBM](#8)\n* [9. Final Comparison](#9)","ff1c8b8f":"# Heart Failure Prediction - Basic Machine Learning Models with Scikit-learn\n\n![](https:\/\/www.redcross.org.au\/kenticoimage.axd\/bc07808c-0e79-4572-b601-7e91e0839090.jpg?v=010505&width=585&height=329&mode=crop)","e023ae3f":"<a id=\"4\"><\/a>\n<h2 style='background:red; border:0; color:white'><center>Support Vector Machine<center><h2>","801222c7":"<a id=\"6\"><\/a>\n<h2 style='background:red; border:0; color:white'><center>Random Forest<center><h2>","e6d56b80":"<a id=\"3\"><\/a>\n<h2 style='background:red; border:0; color:white'><center>Logistic Regression<center><h2>","33b4a4b1":"<a id=\"8\"><\/a>\n<h2 style='background:red; border:0; color:white'><center>LightGBM<center><h2>"}}