{"cell_type":{"34ce90c5":"code","1d281e0b":"code","af8acf8e":"code","37988376":"code","a8ed9ccf":"code","c5edd1bd":"code","e3a46af4":"code","789aea90":"code","9bd8904f":"code","575821c1":"code","c3ce78a0":"code","a97efe33":"code","82738d4a":"code","c33bdc74":"code","a268b09f":"code","95ba261e":"code","1b33bb84":"code","87a028ef":"code","f26a5923":"code","140d7f18":"code","75a4233a":"code","4ef81f70":"code","fe81fde9":"code","9134300f":"code","6c687b18":"code","66242b62":"markdown","398ec741":"markdown","7a0709d8":"markdown","79552a24":"markdown","35ad7f32":"markdown","4acac9c5":"markdown","8c7ab3fe":"markdown","9fcc2ef2":"markdown","3838dcbd":"markdown","5d38754e":"markdown","9bc2063d":"markdown","35b052db":"markdown","ba328789":"markdown","2c187313":"markdown","c764d4aa":"markdown","2fa96259":"markdown","ec64791e":"markdown","e0694868":"markdown","1c325505":"markdown","cd317841":"markdown","22e84eba":"markdown","5324757e":"markdown","033de585":"markdown"},"source":{"34ce90c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1d281e0b":"# Importing the dataset\nimport pandas as pd\ndataset = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","af8acf8e":"# Displaying the head and tail of the dataset\ndataset.head()","37988376":"dataset.tail()","a8ed9ccf":"# Column key for each attributes\ndataset.columns","c5edd1bd":"# Displaying the shape and data type for each attribute.\n\nprint(dataset.shape)\ndataset.dtypes","e3a46af4":"# Displaying the describe statistics in each column\n\ndataset.describe()","789aea90":"# Displaying the only non empty count\ndataset.info()","9bd8904f":"# Displaying the Empty cell and total cell count in each attribute.\n\ndataset.isna().sum()","575821c1":"# Checking percentage of each class rows.\nnon_fraud=round(dataset['Class'].value_counts()[0])\/len(dataset)*100.0\nfraud=round(dataset['Class'].value_counts()[1])\/len(dataset)*100.0\n\nprint(\"Non-Fraud Transaction data percentage %f\"%(non_fraud))\nprint(\"Fraud Trabsaction data percentage %f\"%(fraud))","c3ce78a0":"# Dispalying our class row count in bar plot manner.\n\ncolors=['red','blue']\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nsb.countplot('Class',data=dataset,palette=colors)\nplt.title(\"Class Distribution 0-Not Fraud and 1- Fraud\")","a97efe33":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\ndataset = dataset.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_dataset = dataset.loc[dataset['Class'] == 1]\nnon_fraud_dataset = dataset.loc[dataset['Class'] == 0][:492]\n\nnormal_distributed_dataset = pd.concat([fraud_dataset, non_fraud_dataset])\n\n# Shuffle dataframe rows\nnew_dataset = normal_distributed_dataset.sample(frac=1, random_state=42)\n\nnew_dataset.head()","82738d4a":"print('Distribution of the Classes in the subsample dataset')\nprint(new_dataset['Class'].value_counts()\/len(new_dataset))\n\n\n\nsb.countplot('Class', data=new_dataset, palette=colors)\nplt.title('Equally Distributed Classes')\nplt.show()","c33bdc74":"y=new_dataset['Class']\nx=new_dataset.drop(['Class'],axis=1)","a268b09f":"x=x.values\ny=y.values\nx[:1,:]","95ba261e":"# Splitting the dataset into training and test set\ntrain_size=0.80\ntest_size=0.20\nseed=5\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,train_size=train_size,\n                                               test_size=test_size,random_state=seed)","1b33bb84":"# Spotcheck and compare algorithms with out applying feature scale.......\n\nn_neighbors=5\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# keeping all models in one list\nmodels=[]\nmodels.append(('LogisticRegression',LogisticRegression()))\nmodels.append(('knn',KNeighborsClassifier(n_neighbors=n_neighbors)))\nmodels.append(('SVC',SVC()))\nmodels.append((\"decision_tree\",DecisionTreeClassifier()))\nmodels.append(('Naive Bayes',GaussianNB()))\n\n# Evaluating Each model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnames=[]\npredictions=[]\nerror='accuracy'\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=0)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n\n# Visualizing the Model accuracy\nfig=plt.figure()\nfig.suptitle(\"Comparing Algorithms\")\nplt.boxplot(predictions)\nplt.show()","87a028ef":"# Spot Checking and Comparing Algorithms With StandardScaler Scaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn. preprocessing import StandardScaler\npipelines=[]\npipelines.append(('scaled Logisitic Regression',Pipeline([('scaler',StandardScaler()),('LogisticRegression',LogisticRegression())])))\npipelines.append(('scaled KNN',Pipeline([('scaler',StandardScaler()),('KNN',KNeighborsClassifier(n_neighbors=n_neighbors))])))\npipelines.append(('scaled SVC',Pipeline([('scaler',StandardScaler()),('SVC',SVC())])))\npipelines.append(('scaled DecisionTree',Pipeline([('scaler',StandardScaler()),('decision',DecisionTreeClassifier())])))\npipelines.append(('scaled naive bayes',Pipeline([('scaler',StandardScaler()),('scaled Naive Bayes',GaussianNB())])))\n\n# Evaluating Each model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnames=[]\npredictions=[]\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=0)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n\n# Visualizing the Model accuracy\nfig=plt.figure()\nfig.suptitle(\"Comparing Algorithms\")\nplt.boxplot(predictions)\nplt.show()","f26a5923":"# tuning to Logistic Regression\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=StandardScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nc=[0.01,0.1,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nparam_grid=dict(C=c)\nmodel=LogisticRegression()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","140d7f18":"# tuning to Decision Tree Classification Algorithm\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=StandardScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nparam_grid=dict()\nmodel=DecisionTreeClassifier()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","75a4233a":"# Ensemble and Boosting algorithm to improve performance\n\n#Ensemble\n# Boosting methods\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Bagging methods\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nensembles=[]\nensembles.append(('scaledAB',Pipeline([('scale',StandardScaler()),('AB',AdaBoostClassifier())])))\nensembles.append(('scaledGBC',Pipeline([('scale',StandardScaler()),('GBc',GradientBoostingClassifier())])))\nensembles.append(('scaledRFC',Pipeline([('scale',StandardScaler()),('rf',RandomForestClassifier(n_estimators=10))])))\nensembles.append(('scaledETC',Pipeline([('scale',StandardScaler()),('ETC',ExtraTreesClassifier(n_estimators=10))])))\n\n# Evaluate each Ensemble Techinique\nresults=[]\nnames=[]\nfor name,model in ensembles:\n    fold=KFold(n_splits=10,random_state=5)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    results.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n# Visualizing the compared Ensemble Algorithms\nfig=plt.figure()\nfig.suptitle('Ensemble Compared Algorithms')\nplt.boxplot(results)\nplt.show()","4ef81f70":"# Random forest Classifier Tuning\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=StandardScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nn_estimators=[10,20,30,40,50,100,150,200]\nparam_grid=dict(n_estimators=n_estimators)\nmodel=RandomForestClassifier()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","fe81fde9":"# Gradient Boosting Classifier Tuning\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=StandardScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nlearning_rate=[0.01,0.05,0.1,0.2,0.3,0.4]\nn_estimators=[10,20,30,40,50,100,150,200]\nparam_grid=dict(n_estimators=n_estimators,learning_rate=learning_rate)\nmodel=GradientBoostingClassifier()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","9134300f":"# Finalize Model\n# we finalized the Gradient Boosting Algorithm and evaluate the model for Hotel Booking Demand Dataset\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nscaler=StandardScaler().fit(x_train)\nscaler_x=scaler.transform(x_train)\nmodel=GradientBoostingClassifier(learning_rate=0.2,n_estimators=50)\nmodel.fit(scaler_x,y_train)\n\n#Transform the validation test set data\nscaledx_test=scaler.transform(x_test)\ny_pred=model.predict(scaledx_test)\ny_trainpred=model.predict(scaler_x)","6c687b18":"accuracy_mean=accuracy_score(y_train,y_trainpred)\naccuracy_matric=confusion_matrix(y_train,y_trainpred)\nprint(\"train set %f\"%accuracy_mean)\nprint(\"train set \",accuracy_matric)\n\n\n\naccuracy_mean=accuracy_score(y_test,y_pred)\naccuracy_matric=confusion_matrix(y_test,y_pred)\nprint(\"test set %f\"%accuracy_mean)\nprint(\"test set \",accuracy_matric)","66242b62":"# 2(x). Resample Module Evaluate Performance","398ec741":"We got good accuracy for this ensemble models.\n\n1. Ada Boost Classifier Algorithm 0.931289 (0.028229)\n2. GradientBoosting Classifier Algorithm 0.949059 (0.032391)\n3. Random Forest Classifier Algorithm 0.938916 (0.035196)\n4. Extra Tree Classifier Algorithm 0.932554 (0.037078)\n\nNow we are going to tuning the Random Forest and Gradient Boosting CLassification algorithms.....","7a0709d8":"We dont have any missing values in our dataset. So no need to clean our dataset.\n\nNow we can safely go ahead with other preprocessings.","79552a24":"Well Done we got accuracy for training set is 100% and 92.89% for test set...","35ad7f32":"# Checking Balanced or Imbalanced Dataset","4acac9c5":"* **1. Introduction**\n* **2. Feature Engineering\/Data Preprocessing**\n*       2(i). Importing Dataset\n*       2(ii). Descriptive Statistics\n*       2(iii). Handle Null or Empty Values (Data Cleaning)\n*       2(iv). Visualising Descriptive Statistics\n*       2(v). HeatMap Visualisation\n*       2(vi). Correlation Coefficient Values Each Attribute.\n*       2(vii). Label Encoder\/One Hot Encoder\n*       2(viii). Feature Split\n*       2(ix). Feature scale\n*       2(x). Resample Module Evaluate Performance\n* **3. Modeling**","8c7ab3fe":"we will first scale the columns comprise of Time and Amount . Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases.\n\nIn this scenario, our subsample will be a dataframe with a 50\/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n\nScaled amount and scaled time are the columns with scaled values.\nThere are 492 cases of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe.\nWe concat the 492 cases of fraud and non fraud, creating a new sub-sample.\n","9fcc2ef2":"# It's ImBalance Dataset\nAs per above class percentage its clear that our dataset is imbalanced dataset.We can seed Non-fraud transaction having 99.8% data and fraud transaction only 0.17%.\n\nBy using this imabalance dataset we never get accurate perfromance.\n\nThe base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!","3838dcbd":"Now we are going to apply tuning to Logistic Regression and Decision Tree...","5d38754e":"We got an accuracy likes..\n\n1. LogisticRegression : 0.927459 (0.028305)\n2. knn : 0.637861 (0.054821)\n3. SVC : 0.515807 (0.060123)\n4. decision_tree : 0.908439 (0.031689)\n5. Naive Bayes : 0.880542 (0.028052)","9bc2063d":"After Applying tuning to classification algorithm and ensemble algorithm we got top 4 accuracy algorithm\n\n1. Random Forest classification algorithm 0.942746 using {'n_estimators': 30} \n2. Gradient Boosting Classification algorithm 0.950357 using {'learning_rate': 0.2, 'n_estimators': 50} \n3. Logistic Regression 0.943995 using {'C': 0.9} \n4. Decision Tree Classifier Best: 0.905972 using {} ","35b052db":"As per above algorithm Ada Boosting Algorithm Giving the best Accuracy So now we are going to use Ada boost algorithm to fit and predict our model.","ba328789":"As per above visuval we have 284807 rows and 31 columns.\n\nAll Attribute Having Float and Integer values only we dont have any categorical values.","2c187313":"# **2(iii). Handle Null or Empty Values (Data Cleaning)**\n\nCleaning any null or empty values available in our dataset","c764d4aa":"# Converting all imbalanced to balanced dataset","2fa96259":"# **1. Introduction**\n\nwe will use various predictive models to see how accurate they are in detecting whether a transaction is a normal payment or a fraud. As described in the dataset.\n\nIn this dataset we have imbalanced dataset we have more transaction list of non-fraud and very less number of fraud transaction.\n\nWhich is highly imbalanced dataset..With this imabalanced dataset we won't get accuracte predictions now we need to balance both fraud and non-fraud transcation to balanced like 50:50...\n\nLet have a look how i will extract features and convert imbalanced dataset into balanced dataset\n\nWe have Non-Fraud (99.83%) of the time, while Fraud transactions occurs (0.17%) in our dataset","ec64791e":"After Applying tuning to those top 2 algorithms we got accuray and with best hyper parameter and its values.\n\n1. Logistic Regression 0.943995 using {'C': 0.9} \n2. Decision Tree Classifier Best: 0.905972 using {} ","e0694868":"Now we are going to apply tuning Random Forest and Gradient Boosting classification algorithms.","1c325505":"Classification Algorithm accuracy without feature scale.\n\n1. LogisticRegression : 0.921243 (0.021070)\n2. knn : 0.664589 (0.038532)\n3. SVC : 0.575576 (0.048506)\n4. decision_tree : 0.923856 (0.032898)\n5. Naive Bayes : 0.871633 (0.025133)","cd317841":"# 2(viii). Feature Split","22e84eba":"# 3. Modeling\n\n# Classification.","5324757e":"**2(ii). Descriptive Statistics**","033de585":"# **2. Feature Engineering\/Data Preprocessing**\n\n **2(i). Importing Dataset**"}}