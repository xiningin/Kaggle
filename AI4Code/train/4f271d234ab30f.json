{"cell_type":{"99439cc8":"code","13730be0":"code","931ef450":"code","a2e663a0":"code","d741be43":"code","c7cb38a1":"code","d7895e56":"code","4f75e42d":"code","57069ff3":"code","04e74b0b":"code","4f81335c":"code","ddab4748":"code","4ca42145":"code","3866e12b":"code","c560a416":"code","4b9c99af":"code","0ab4b041":"code","2c0888f4":"code","814df4cb":"code","7777cde6":"code","0cee2158":"code","0c96e2e8":"code","274f74ed":"code","9a314fba":"code","93458506":"code","a8aa0e04":"code","ee23fd47":"code","64acb591":"markdown","0bcd7363":"markdown","ab96f6ec":"markdown","91b98571":"markdown","4ddd2b8b":"markdown"},"source":{"99439cc8":"import numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport os\nimport PIL\nimport typing","13730be0":"ROOT_PATH: str = '..\/input\/landcover-ai-preprocessed'\nINPUT_DIRECTORY: str = os.path.join(ROOT_PATH, \"LandCoverData\")\n    \nWORKING_DIRECTORY: str = \".\/\"\nIMAGE_PATH: str = os.path.join(WORKING_DIRECTORY, \"images\")\nLABELS_PATH: str = os.path.join(WORKING_DIRECTORY, \"labels\")\n\n!mkdir -p $LABELS_PATH\n!mkdir -p $IMAGE_PATH\n\n!cp $INPUT_DIRECTORY\/M*.jpg $IMAGE_PATH\n!cp $INPUT_DIRECTORY\/N*.jpg $IMAGE_PATH\n!cp $INPUT_DIRECTORY\/M*.png $LABELS_PATH\n!cp $INPUT_DIRECTORY\/N*.png $LABELS_PATH","931ef450":"os.listdir(IMAGE_PATH)[:4]","a2e663a0":"os.listdir(LABELS_PATH)[:4]","d741be43":"def map_labels_file(inputs_file: str) -> str:\n    file_name = os.path.split(inputs_file)[-1]\n    file_name_no_ext = os.path.splitext(file_name)[0]\n    labels_local_name = f'{file_name_no_ext}_m.png'\n    return os.path.join(LABELS_PATH, labels_local_name)\n\n\ninput_sample_path: str = os.listdir(IMAGE_PATH)[-10]\ninput_sample_path = os.path.join(IMAGE_PATH, input_sample_path)\nlabels_sample_path = map_labels_file(input_sample_path)\n\nassert os.path.exists(labels_sample_path)\n\ninput_sample_path, labels_sample_path","c7cb38a1":"def load_image_and_labels(\n        input_sample_path: str,\n        labels_sample_path: str\n    ) -> typing.Union[np.ndarray, np.ndarray]:\n    image = cv2.imread(input_sample_path)\n    labels = cv2.imread(labels_sample_path)\n    \n    assert np.allclose(labels[:, :, 0], labels[:, :, 1])\n    assert np.allclose(labels[:, :, 1], labels[:, :, 2])\n    \n    classes_at_image = np.unique(labels)\n    labels = labels[:, :, 0]\n    new_label_array = np.zeros_like(labels)\n\n    for cls in classes_at_image:\n        new_label_array[labels == cls] = cls\n    \n    return image, new_label_array\n\nimage, labels = load_image_and_labels(input_sample_path, labels_sample_path)","d7895e56":"labels.shape","4f75e42d":"def plot_image_and_labels(image, labels):\n    #if labels.max() > 1:\n    #    labels = (labels \/ labels.max()).astype(np.float32)\n    f, axarr = plt.subplots(2,2, figsize=(24, 24))\n    labels = labels.astype('float32')\n    axarr[0, 0].imshow(image)\n    axarr[0, 0].title.set_text('Image')\n    axarr[0, 1].imshow(labels == 1, cmap='gray')\n    axarr[0, 1].title.set_text('Class 0')\n    axarr[1, 0].imshow(labels == 2, cmap='gray')\n    axarr[1, 0].title.set_text('Class 1')\n    axarr[1, 1].imshow(labels == 3, cmap='gray')\n    axarr[1, 1].title.set_text('Class 2')\n    \nplot_image_and_labels(image, labels)\nplt.show()","57069ff3":"import tensorflow as tf\nimport tensorflow_addons as tfa\n\ndef augment(input_image, input_mask, sw, IMG_SIZE=512):\n    if tf.random.uniform(()) > 0.5:\n        # use original image to preserve high resolution\n        \n        # random brightness adjustment illumination\n        input_image = tf.image.random_brightness(input_image, 0.3)\n        # random contrast adjustment\n        input_image = tf.image.random_contrast(input_image, 0.2, 0.5)\n        \n        # flipping random horizontal or vertical\n        if tf.random.uniform(()) > 0.5:\n            input_image = tf.image.flip_left_right(input_image)\n            input_mask = tf.image.flip_left_right(input_mask)\n            sw = tf.image.flip_left_right(sw)\n        if tf.random.uniform(()) > 0.5:\n            input_image = tf.image.flip_up_down(input_image)\n            input_mask = tf.image.flip_up_down(input_mask)\n            sw = tf.image.flip_up_down(sw)\n        # rotation in 30\u00b0 steps\n        rot_factor = tf.cast(tf.random.uniform(shape=[], maxval=12, dtype=tf.int32), tf.float32)\n        angle = np.pi\/12*rot_factor\n        input_image = tfa.image.rotate(input_image, angle)\n        input_mask = tfa.image.rotate(input_mask, angle)\n        sw = tfa.image.rotate(sw, angle)\n    \n    return tf.convert_to_tensor(input_image), tf.convert_to_tensor(input_mask), tf.convert_to_tensor(sw)\n","04e74b0b":"from typing import List, Optional\nimport tensorflow as tf\n\ndef load_image_and_labels(\n        input_sample_path: str,\n        labels_sample_path: str,\n        total_classes: int = 4\n    ) -> typing.Union[np.ndarray, np.ndarray]:\n\n    image = cv2.imread(input_sample_path.numpy().decode())\n    labels = cv2.imread(labels_sample_path.numpy().decode())\n    \n    assert image is not None\n    assert labels is not None\n    \n    assert np.allclose(labels[:, :, 0], labels[:, :, 1])\n    assert np.allclose(labels[:, :, 1], labels[:, :, 2])\n    \n    classes_at_image = np.unique(labels)\n    labels = labels[:, :, 0]\n    new_label_array = np.zeros_like(labels)\n\n    for cls in classes_at_image:\n        new_label_array[labels == cls] = cls\n    \n    return image \/ 255., new_label_array\n\n\ndef loader_function(\n        input_sample_path: str,\n        labels_sample_path: str,\n        total_classes: int = 4\n    ):\n    image, label = tf.py_function(\n        load_image_and_labels,\n        [input_sample_path, labels_sample_path],\n        (tf.float32, tf.float32)\n    )\n\n    image.set_shape((512, 512, 3))\n    label.set_shape((512, 512))\n    \n    weights = get_sample_weights(label)\n    weights.set_shape((512, 512))\n    \n    return image, label, weights\n\ndef get_sample_weights(labels, scale=1.):\n    weights = tf.zeros(labels.shape, dtype=tf.float32)\n    \n    flat_labels = tf.reshape(labels, (-1, ))\n    flat_labels = tf.cast(flat_labels, tf.int32)\n\n    y, idx, count = tf.unique_with_counts(\n        tf.concat([tf.convert_to_tensor([0, 1, 2, 3], dtype=tf.int32), flat_labels], -1)\n    )\n    \n    count = tf.reduce_max(count) \/ count * scale\n    gather = tf.gather(count, flat_labels)\n    return tf.reshape(gather, labels.shape)\n\ndef create_tf_dataset(\n        list_of_image_names: List[str],\n        batch_size: int = 8,\n        drop_remainder: bool = False,\n        shuffle_buffer_size: Optional[int] = None\n    ) -> tf.data.Dataset:\n    list_of_image_names = [os.path.join(IMAGE_PATH, x) for x in list_of_image_names]\n    labels_sample_path = [map_labels_file(x) for x in list_of_image_names]\n\n    image_dataset = tf.data.Dataset.from_tensor_slices(list_of_image_names)\n    labels_dataset = tf.data.Dataset.from_tensor_slices(labels_sample_path)\n    dataset = tf.data.Dataset.zip((image_dataset, labels_dataset))\n    dataset = dataset.map(loader_function)\n\n    if shuffle_buffer_size is not None:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n    if shuffle_buffer_size is not None:\n        dataset = dataset.repeat()\n        dataset = dataset.map(augment)\n\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n    \ndataset = create_tf_dataset(os.listdir(IMAGE_PATH))\nfor x, y,sw in dataset.skip(1).take(1):\n    print(x.shape)\n    print(y.shape)\n    print(sw.shape)\n    \nprint(f'Labels: {y[0]}')\nprint(f'weights: {sw[0]}')\ntf.data.experimental.get_structure(dataset)","4f81335c":"from tensorflow.keras.layers import (\n    Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate, BatchNormalization,\n    Conv2DTranspose\n)\n\nfrom tensorflow.keras import Model\n\ndef bn_conv_relu(input, filters, bachnorm_momentum, **conv2d_args):\n    x = BatchNormalization(momentum=bachnorm_momentum)(input)\n    x = Conv2D(filters, **conv2d_args)(x)\n    return x\n\ndef bn_upconv_relu(input, filters, bachnorm_momentum, **conv2d_trans_args):\n    x = BatchNormalization(momentum=bachnorm_momentum)(input)\n    x = Conv2DTranspose(filters, **conv2d_trans_args)(x)\n    return x\n\ndef satellite_unet(\n    input_shape,\n    num_classes=1,\n    output_activation='linear',\n    num_layers=4):\n\n    inputs = Input(input_shape)   \n    \n    filters = 64\n    upconv_filters = 96\n\n    kernel_size = (3,3)\n    activation = 'relu'\n    strides = (1,1)\n    padding = 'same'\n    kernel_initializer = 'he_normal'\n\n    conv2d_args = {\n        'kernel_size':kernel_size,\n        'activation':activation, \n        'strides':strides,\n        'padding':padding,\n        'kernel_initializer':kernel_initializer\n        }\n\n    conv2d_trans_args = {\n        'kernel_size':kernel_size,\n        'activation':activation, \n        'strides':(2,2),\n        'padding':padding,\n        'output_padding':(1,1)\n        }\n\n    bachnorm_momentum = 0.01\n\n    pool_size = (2,2)\n    pool_strides = (2,2)\n    pool_padding = 'valid'\n\n    maxpool2d_args = {\n        'pool_size':pool_size,\n        'strides':pool_strides,\n        'padding':pool_padding,\n        }\n    \n    x = Conv2D(filters, **conv2d_args)(inputs)\n    c1 = bn_conv_relu(x, filters, bachnorm_momentum, **conv2d_args)    \n    x = bn_conv_relu(c1, filters, bachnorm_momentum, **conv2d_args)\n    x = MaxPooling2D(**maxpool2d_args)(x)\n\n    down_layers = []\n\n    for l in range(num_layers):\n        x = bn_conv_relu(x, filters, bachnorm_momentum, **conv2d_args)\n        x = bn_conv_relu(x, filters, bachnorm_momentum, **conv2d_args)\n        down_layers.append(x)\n        x = bn_conv_relu(x, filters, bachnorm_momentum, **conv2d_args)\n        x = MaxPooling2D(**maxpool2d_args)(x)\n\n    x = bn_conv_relu(x, filters, bachnorm_momentum, **conv2d_args)\n    x = bn_conv_relu(x, filters, bachnorm_momentum, **conv2d_args)\n    x = bn_upconv_relu(x, filters, bachnorm_momentum, **conv2d_trans_args)\n\n    for conv in reversed(down_layers):        \n        x = concatenate([x, conv])  \n        x = bn_conv_relu(x, upconv_filters, bachnorm_momentum, **conv2d_args)\n        x = bn_conv_relu(x, filters, bachnorm_momentum, **conv2d_args)\n        x = bn_upconv_relu(x, filters, bachnorm_momentum, **conv2d_trans_args)\n\n    x = concatenate([x, c1])\n    x = bn_conv_relu(x, upconv_filters, bachnorm_momentum, **conv2d_args)\n    x = bn_conv_relu(x, filters, bachnorm_momentum, **conv2d_args)\n           \n    outputs = Conv2D(num_classes, kernel_size=(1,1), strides=(1,1), activation=output_activation, padding='valid') (x)       \n    \n    model = Model(inputs=[inputs], outputs=[outputs])\n    return model\n\n\nclass MeanIoU(tf.keras.metrics.MeanIoU):\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        return super().update_state(y_true, tf.argmax(y_pred, -1), sample_weight=sample_weight)\n    \ndef compile_model(model, pretrained_weights=None):\n    model.compile(\n        optimizer = tf.keras.optimizers.RMSprop(lr = 1e-4),\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics = [\n            'sparse_categorical_accuracy',\n            MeanIoU(model.outputs[0].shape[-1])\n        ]\n    )\n    if(pretrained_weights):\n    \tmodel.load_weights(pretrained_weights)\n\n    return model\n\n\nunet = satellite_unet(num_classes=4, input_shape=(512, 512, 3))\nunet = compile_model(unet)","ddab4748":"unet.inputs, unet.outputs","4ca42145":"unet.compiled_metrics","3866e12b":"from sklearn.model_selection import train_test_split\n\nEPOCHS = 15\nSTEPS_PER_EPOCH = 4000\n\nall_images = os.listdir(IMAGE_PATH)\n\ntrain, test, _, __ = train_test_split(all_images, all_images, test_size=0.2)\ntrain, val, _, __ = train_test_split(train, train, test_size=0.1)\n\ntrain_dataset =  create_tf_dataset(train, batch_size=2, shuffle_buffer_size=128)\nval_dataset =  create_tf_dataset(val, batch_size=2)\ntest_dataset =  create_tf_dataset(test, batch_size=2)\n\nunet.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, validation_data=val_dataset)","c560a416":"unet.evaluate(test_dataset, return_dict=True)","4b9c99af":"from skimage import io, color\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef display_prediction(model, x, y):\n    pred = model(tf.expand_dims(x, 0), training=False)\n    pred = tf.argmax(pred, axis=-1)\n    pred = tf.squeeze(pred).numpy()\n    masked = color.label2rgb(\n        pred,\n        x.numpy(),\n        colors=[(255,0,0),(0,255,0), (0, 0, 255)],\n        alpha=0.01, bg_label=0, bg_color=None\n    )\n\n    f, axarr = plt.subplots(3,2, figsize=(24, 24))\n    axarr[0, 0].imshow(x.numpy())\n    axarr[0, 0].title.set_text('Image')\n    axarr[0, 1].imshow(pred == 1, cmap='gray')\n    axarr[0, 1].title.set_text('Class 0')\n    axarr[1, 0].imshow(pred == 2, cmap='gray')\n    axarr[1, 0].title.set_text('Class 1')\n    axarr[1, 1].imshow(pred == 3, cmap='gray')\n    axarr[1, 1].title.set_text('Class 2')\n    axarr[2, 0].imshow(y)\n    axarr[2, 0].title.set_text('GT')\n    axarr[2, 1].imshow(masked)\n    axarr[2, 1].title.set_text('Prediction')\n    ","0ab4b041":"data_to_visualize = []\n\nfor x, y, sw in test_dataset.take(10):\n    x, y = x[0], y[0]\n    data_to_visualize.append((x, y))\n","2c0888f4":"display_prediction(unet, *data_to_visualize[0])","814df4cb":"display_prediction(unet, *data_to_visualize[1])","7777cde6":"display_prediction(unet, *data_to_visualize[2])","0cee2158":"display_prediction(unet, *data_to_visualize[3])","0c96e2e8":"display_prediction(unet, *data_to_visualize[4])","274f74ed":"display_prediction(unet, *data_to_visualize[5])","9a314fba":"display_prediction(unet, *data_to_visualize[6])","93458506":"display_prediction(unet, *data_to_visualize[7])","a8aa0e04":"display_prediction(unet, *data_to_visualize[8])","ee23fd47":"display_prediction(unet, *data_to_visualize[9])","64acb591":"\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0441\u0442\u0430\u0442\u044c\u044e \u043f\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0443: https:\/\/arxiv.org\/pdf\/2005.02264v1.pdf\n\u0413\u043b\u0430\u0432\u043d\u043e\u0435: \u0435\u0441\u0442\u044c 4 \u0440\u0430\u0437\u043c\u0435\u0447\u0435\u043d\u043d\u044b\u0445 \u043a\u043b\u0430\u0441\u0441\u0430:\n0. \u0424\u043e\u043d\n1. \u0417\u0434\u0430\u043d\u0438\u044f\n2. \u041b\u0435\u0441\n3. \u0412\u043e\u0434\u0430\n\n\u0420\u0435\u0448\u0435\u043d\u0438\u0435 \u0438\u0437 \u0441\u0442\u0430\u0442\u044c\u0438 \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438: <br>\nClass Buildings Woodlands Water Background Overall <br>\nmIoU 74.81% 90.33% 92.43% 93.26% 87.71%","0bcd7363":"# LandCover dataset\nhttps:\/\/landcover.ai\/","ab96f6ec":"\u041c\u0435\u0442\u0440\u0438\u043a\u0430 mIoU - mean intersection over union. <br>\n\u0412\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435: \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e \u0432\u044b\u0441\u0447\u0438\u0442\u0430\u0442\u044c IoU \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430, \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e\u0433\u043e \u0434\u043b\u044f \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438, \u0437\u0430\u0442\u0435\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0443\u0441\u0440\u0435\u0434\u043d\u0438\u0442\u044c.\n$$\nIoU_{i} = \\frac{TP_{i}}{TP_{i} + FP_{i} + FN_{i}} \\\\\nmIoU = \\frac{1}{n}\\sum_{i=1}^n IoU_{i}\n$$\n\n\u0411\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430 Keras \u0438\u043c\u0435\u0435\u0442 \u0433\u043e\u0442\u043e\u0432\u0443\u044e \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e, \u0445\u043e\u0442\u044c \u0437\u0434\u0435\u0441\u044c \u043f\u043e\u0432\u0435\u0437\u043b\u043e: https:\/\/keras.io\/api\/metrics\/segmentation_metrics\/#meaniou-class","91b98571":"# \u0427\u0442\u043e \u0432\u043d\u0443\u0442\u0440\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430","4ddd2b8b":"\u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u043e\u0431\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438"}}