{"cell_type":{"58b49e6f":"code","94ca3a92":"code","b0e41512":"code","9fc475cf":"code","7676825d":"code","aedbc3ac":"code","842330b8":"code","e65faea3":"code","6a4e2abe":"code","3c3f7a87":"code","1f9bdda4":"code","8a6a96d8":"code","0119c355":"code","498030db":"code","2a2cb4e5":"code","79bdf62b":"code","f424bc93":"code","3ffd403a":"code","1121cd85":"code","41918930":"markdown","acc47506":"markdown","4a532908":"markdown","15c8fb39":"markdown","f0dcb4f0":"markdown","3e12038a":"markdown","d67cad21":"markdown","eaa7b97f":"markdown","08ac7fd3":"markdown","c1d721de":"markdown","5a24f3ea":"markdown","ae169d8e":"markdown","5f47c06c":"markdown","ce0d6eaf":"markdown","44d2f24b":"markdown","910acf95":"markdown","6db8e97d":"markdown","78ae2772":"markdown"},"source":{"58b49e6f":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","94ca3a92":"from fastai import *\nfrom fastai.vision import *\n\nimport os\nimport sys\nimport shutil\nimport requests\n# from PIL import Image\nfrom io import BytesIO","b0e41512":"# copy dataset to working (to enable manipulating the directory)\npath = '\/kaggle\/input\/apparel-dataset\/'   \ndest = '\/kaggle\/working\/dataset\/'\nshutil.copytree(path, dest, copy_function = shutil.copy)  ","9fc475cf":"os.listdir('\/kaggle\/working\/dataset\/')","7676825d":"tfms = get_transforms()\n\nimg_src = '\/kaggle\/working\/dataset\/'\nsrc = (ImageList.from_folder(img_src) #set image folder\n       .split_by_rand_pct(0.2) #set the split of training and validation to 80\/20\n       .label_from_folder(label_delim='_')) #get label names from folder and split by underscore\n\ndata = (src.transform(tfms, size=256) #set image size to 256\n        .databunch(num_workers=0).normalize(imagenet_stats))","aedbc3ac":"data.show_batch(rows=3, figsize=(12,9))\nprint(f\"\"\"Classes in our data: {data.classes}\\n\nNumber of classes: {data.c}\\n\nTraining Dataset Length: {len(data.train_ds)}\\n\nValidation Dataset Length: {len(data.valid_ds)}\"\"\")","842330b8":"acc_02 = partial(accuracy_thresh, thresh=0.2)\nlearn = cnn_learner(data, models.resnet50, metrics=acc_02, model_dir='\/kaggle\/working\/models')","e65faea3":"learn.fit_one_cycle(5)","6a4e2abe":"learn.save('stage-1-rn50')","3c3f7a87":"learn.unfreeze()","1f9bdda4":"learn.lr_find()\nlearn.recorder.plot()","8a6a96d8":"learn.fit_one_cycle(5, slice(3e-5, 5e-4))","0119c355":"learn.save('stage-2-rn50')","498030db":"learn.recorder.plot_losses()","2a2cb4e5":"# If you need to load a model, use the funciton below\n# learn.load('\/kaggle\/input\/multilabel-models\/models\/stage-2-rn50')","79bdf62b":"learn = load_learner('\/kaggle\/input\/multilabel-models\/models\/', \n                     test=ImageList.from_folder('\/kaggle\/input\/apparel\/black_pants')) #loading from training set as an example only\npreds,_ = learn.get_preds(ds_type=DatasetType.Test)","f424bc93":"\"\"\"\nGet the prediction labels and their accuracies, then return the results as a dictionary.\n\n[obj] - tensor matrix containing the predicted accuracy given from the model\n[learn] - fastai learner needed to get the labels\n[thresh] - minimum accuracy threshold to returning results\n\"\"\"\ndef get_preds(obj, learn, thresh = 15):\n    labels = []\n    # get list of classes from Learner object\n    for item in learn.data.c2i:\n        labels.append(item)\n\n    predictions = {}\n    x=0\n    for item in obj:\n        acc= round(item.item(), 3)*100\n#         acc= int(item.item()*100) # no decimal places\n        if acc > thresh:\n            predictions[labels[x]] = acc\n        x+=1\n        \n    # sorting predictions by highest accuracy\n    predictions ={k: v for k, v in sorted(predictions.items(), key=lambda item: item[1], reverse=True)}\n\n    return predictions","3ffd403a":"from io import BytesIO\nimport requests\n\nurl = \"https:\/\/live.staticflickr.com\/8188\/28638701352_1aa058d0c6_b.jpg\" \nresponse = requests.get(url).content #get request contents\n\nimg = open_image(BytesIO(response)) #convert to image\n# img = open_image(path_to_img) #for local image file\n\nimg.show() #show image\n_, _, pred_pct = learn.predict(img) #predict while ignoring first 2 array inputs\nprint(get_preds(pred_pct, learn))","1121cd85":"learn.export()","41918930":"To [export](https:\/\/docs.fast.ai\/basic_train.html#Learner.export) the state of the Learner, simply use `learn.export()`. This will be needed to predict new images when deploying your trained model online. By default, the exported learner file name is `export.pkl`","acc47506":"# <div align=\"center\">Multi-label Classification using FastAi Library<\/div>\n\n## **Context**\nApplying Fastai library on apparel image dataset and creating a multi-label classification model based on what I learned from Jeremy Howard's [lesson 3 of the fastai course](https:\/\/course.fast.ai\/videos\/?lesson=3).\n\n## **Dataset**\n\nWhile searching the internet for a good dataset to apply the multi-label classification on, I stumbled upon pyimagesearch's  [multi-label classification with keras's](https:\/\/www.pyimagesearch.com\/2018\/05\/07\/multi-label-classification-with-keras\/) article, and Adrian used a small simple dataset containing 3 clothing categories. But to expand on the dataset, I combined it with [trolukovich's dataset](https:\/\/www.kaggle.com\/trolukovich\/apparel-images-dataset) and my own by scraping Google and Bing using [cwerner's fastclass](https:\/\/github.com\/cwerner\/fastclass) package. Now it contains 8 different apparel categories in 9 different colours. It is published on Kaggle under the name [Apparel Dataset](https:\/\/www.kaggle.com\/kaiska\/apparel-dataset)\n\nIf you want to create your own image set, I highly recommend using Christian Warner's [fastclass package](https:\/\/github.com\/cwerner\/fastclass), he explains how to use it in a [short article](https:\/\/www.christianwerner.net\/tech\/Build-your-image-dataset-faster\/).\nAdditionally, there is a [tutorial](https:\/\/www.pyimagesearch.com\/2018\/04\/09\/how-to-quickly-build-a-deep-learning-image-dataset\/) on pyimagesearch which helps you build an image dataset by scraping bing, but it uses a more difficult approach and requires bing API which, if you are not a student, will require you to input your credit card information along side phone verification.\n\nFor this kernel, I will be applying the fastai library to classify the apparel and its colour within an image.","4a532908":"Using `data.show_batch()` we can have a look at some of our pictures with the labels associated with them. Since this is a multi-label classification data, there are multiple labels associated with each of our images separated by ;.","15c8fb39":"In this dataset, each picture can have multiple labels. If we take a look at the folder names, we see that each folder contains two labels seperated by an underscore.","f0dcb4f0":"### Moving files \n\nLet's first copy our dataset to `\/kaggle\/working\/` to be able to apply changes to the dataset without having to change the directory later. This is because the input directory on kaggle is read-only.","3e12038a":"It's a good rule of thumb to save your models as you go along. Particularly, you want to know if it is before or after the unfreeze (stage 1 or 2), what size you were training on, and what architecture were you training on. That way you can always load the models without having to retrain them, help you go back and experimenting pretty easily.","d67cad21":"## Loading relevant libraries\n\nEvery notebook starts with the following three lines; they ensure that any edits to libraries you make are reloaded here automatically, and \nalso that any charts or images displayed are shown in this notebook.","eaa7b97f":"To add the image dataset to your kaggle kernel, simply click on **File** and then **Add or upload data** from within a kernel you are editing, then paste [`kaiska\/apparel-dataset`](https:\/\/kaggle.com\/kaiska\/apparel-dataset) in the search box and click `Add`.\n\n![add apparel dataset](https:\/\/i.imgur.com\/pfI09eB.gif)","08ac7fd3":"### Predicting imges from URL or local file \n\nThere are two main ways to predict images, either by uploading a file or by reading an already hosted image.\n\n* To predict a hosted image, we will simply load this image, convert it to an image file and predict.\n* To predict a local file, simply open the image using `open_image(path_to_img)`\n\nBelow is the code on how to do it.","c1d721de":"To train a model properly, we should first train it without unfreezing the learner as that will train the head weights to better understand and categories our images. I recommend reading [Poonam's](https:\/\/forums.fast.ai\/t\/why-do-we-need-to-unfreeze-the-learner-everytime-before-retarining-even-if-learn-fit-one-cycle-works-fine-without-learn-unfreeze\/41614\/5) advice on this.","5a24f3ea":"As you can see, the learning rate is at its lowest loss between 1e<sup>-5<\/sup> and 1e<sup>-4<\/sup>. We add the constraints around those numbers to our next cycle and train the model again.","ae169d8e":"We import all the necessary packages. We are going to work with the [fastai V1 library](http:\/\/www.fast.ai\/2018\/10\/02\/fastai-ai\/) which sits on top of [Pytorch 1.0](https:\/\/hackernoon.com\/pytorch-1-0-468332ba5163). The fastai library provides many useful functions that enable us to quickly and easily build neural networks and train our models.","5f47c06c":"## Deployment\n\nIn order to deploy your trained model online, I recommend checking out fastai's tutorial on how to [deploy on Render](https:\/\/course.fast.ai\/deployment_render.html). It's a free and easy method which will take no more than 30 minutes to set up.\nYou will need:\n* A google drive or dropbox account for uploading model files.\n* A github account to fork fastai's repository which uses starlette.\n* A render account to host the code.\n\nIf you are interested in deploying using other methods, there are 5 other ways listed on fastai.\n","ce0d6eaf":"## Predictions\n### Predicting a test set\n\nTo predict a testset, we simply refer to the [fastai documentation](https:\/\/docs.fast.ai\/basic_train.html#Learner.get_preds).\nWe simply need use `load_learner` method and point it to the path of `export.pkl` file, and define the test folder. Then just run `learn.get_preds()` to get the predictions.\n\nNote here that the predictions are ordered based on the labels order. You can get the list labels by running `learn.data.c2i`.","44d2f24b":"Now we have a better accuracy overall for our model and at a point where it will perform well on most given images. We save the model and prepare to use it for product.","910acf95":"### Creating DataBunch\n\nTo put this in a `DataBunch` while using the [data block API](https:\/\/docs.fast.ai\/data_block.html), we then need to be using ImageList (and not ImageDataBunch). This will make sure the model created has the proper loss function to deal with the multiple classes. Also, the main difference for using `ImageList` over `ImageDataBunch` is that the later has pre-set constrains, while using `ImageList` gives you [more flexibility](https:\/\/forums.fast.ai\/t\/dataset-creation-imagedatabunch-vs-imagelists\/45427\/2).","6db8e97d":"Now we unfreeze the model, and we check our learning rate to find the best learning rate with the minimal loss so we can retrain our model on more restricted learning rate.","78ae2772":"To create a Learner we use [`cnn_learner`](https:\/\/docs.fast.ai\/vision.learner.html#cnn_learner) instead of `create_cnn` which has been deprecated. Our base architecture is `resnet50` for this classification, but the metrics are a little bit different; we will use accuracy_thresh instead of accuracy.\n\nWhen dealing with single classification problems, we determined the prediction for a given class by simply picking the prediction that had the highest accuracy, but for this problem, each activation can be 0 or 1. `accuracy_thresh` selects the ones that are above a certain threshold (0.5 being the default) and compares them to the ground truth.\n\nSince there are 17 possible classes, we're going to have one probability for each of those. But then we're not just going to pick out one of those 17, we're going to pick out n of those 17. So what we do is, we compare each probability to some threshold. Then we say anything that's higher than that threshold, we're going to assume that the models saying it does have that feature. So we can pick that threshold.\n\nFor this kernel, we will adjust the threshold to 0.2."}}