{"cell_type":{"0cb3669b":"code","8d4909b6":"code","664571ed":"code","6f7e371f":"code","07e86a70":"code","1a59f08b":"code","26f45a25":"code","6b471769":"code","1cb93aa2":"code","519b69e4":"code","48351cd8":"code","c36f9a02":"code","b3b4a040":"code","65005b83":"code","ce936cff":"code","c028db36":"code","c06f7e2b":"code","1185fd94":"code","d08f9f06":"code","1642836b":"code","1c676406":"code","0a2d7883":"code","5caeb4b9":"code","950c9495":"code","bcbdf287":"code","c8edd969":"code","23e7c6d7":"code","52d21d91":"code","44c48003":"code","6f51ea1e":"markdown","3c1f66ca":"markdown","9f916912":"markdown","027670e5":"markdown","9e485005":"markdown","7bcb3b4a":"markdown","4f6e7a72":"markdown","e833200c":"markdown","e5e1d52d":"markdown","99a73d25":"markdown","0fb50dd9":"markdown","48e77bdf":"markdown","55519b52":"markdown","fa0d7be1":"markdown","a5336462":"markdown","3d88a466":"markdown","23f58491":"markdown","c44ea490":"markdown","78b53408":"markdown","4100e792":"markdown"},"source":{"0cb3669b":"!pip install imutils\n","8d4909b6":"import numpy as np \nimport pandas as pd \nimport os\nfrom os import listdir\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport imutils    \n\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.layers import Conv2D,Input,ZeroPadding2D,BatchNormalization,Flatten,Activation,Dense,MaxPooling2D\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle #shuffling the data improves the model","664571ed":"image_dir=\"..\/input\/brain-mri-images-for-brain-tumor-detection\/\"","6f7e371f":"os.makedirs('..\/output\/kaggle\/working\/augmented-images', exist_ok = True)\nos.makedirs('..\/output\/kaggle\/working\/augmented-images\/yes', exist_ok = True)\nos.makedirs('..\/output\/kaggle\/working\/augmented-images\/no', exist_ok = True)\n\naugmented_data_path ='..\/output\/kaggle\/working\/augmented-images\/'\n\naugmented_yes =augmented_data_path+'yes'\naugmented_no = augmented_data_path+'no'\n\nIMG_SIZE = (224,224)","07e86a70":"def augment_data(file_dir, n_generated_samples, save_to_dir):\n    data_gen = ImageDataGenerator(rotation_range=10, \n                                  width_shift_range=0.1, \n                                  height_shift_range=0.1, \n                                  shear_range=0.1, \n                                  brightness_range=(0.3, 1.0),\n                                  horizontal_flip=True, \n                                  vertical_flip=True, \n                                  fill_mode='nearest'\n                                 )\n\n    for filename in listdir(file_dir):\n        image = cv2.imread(file_dir + '\/' + filename)\n        # reshape the image\n        image = image.reshape((1,)+image.shape)\n        save_prefix = 'aug_' + filename[:-4]\n        i=0\n        for batch in data_gen.flow(x=image, batch_size=1, save_to_dir=save_to_dir,save_prefix=save_prefix, save_format='jpg'):\n                i += 1\n                if i > n_generated_samples:\n                    break","1a59f08b":"dir = os.listdir('..\/output\/kaggle\/working\/augmented-images\/yes') \n\nif len(dir) == 0: \n\n    # augment data for the examples with label equal to 'yes' representing tumurous examples\n    augment_data(file_dir=image_dir+'yes',n_generated_samples=4, save_to_dir=augmented_data_path+'yes')\n    # augment data for the examples with label equal to 'no' rpresenting non-tumurous examples\n    augment_data(file_dir=image_dir+'no', n_generated_samples=5, save_to_dir=augmented_data_path+'no')\nelse:\n    print(len(dir))","26f45a25":"def load_data(dir_list):\n\n    # load all images in a directory\n    X = []\n    y = []\n#     image_width, image_height = image_size\n    \n    for directory in dir_list:\n        for filename in listdir(directory):\n            image = cv2.imread(directory+'\/'+filename)\n#             image = crop_brain_contour(image, plot=False)\n#             image = cv2.resize(image, dsize=(image_width, image_height), interpolation=cv2.INTER_CUBIC)\n#             # normalize values\n#             image = image \/ 255.\n#             # convert image to numpy array and append it to X\n            X.append(image)\n            # append a value of 1 to the target array if the image\n            # is in the folder named 'yes', otherwise append 0.\n            if directory[-3:] == 'yes':\n                y.append([1])\n            else:\n                y.append([0])\n                \n    X = np.array(X)\n    y = np.array(y)\n    \n    # Shuffle the data\n    X, y = shuffle(X, y)\n    \n    print(f'Number of examples is: {len(X)}')\n    print(f'X shape is: {X.shape}')\n    print(f'y shape is: {y.shape}')\n    \n    return X, y","6b471769":"def plot_samples(X, y, labels_dict, n=50):\n    \"\"\"\n    Creates a gridplot for desired number of images (n) from the specified set\n    \"\"\"\n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n\/j)\n\n        plt.figure(figsize=(15,6))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n        plt.show()","1cb93aa2":"\nX_train, y_train = load_data([augmented_yes, augmented_no])","519b69e4":"plot_samples(X_train, y_train, ['yes','no'], 20)","48351cd8":"def crop_brain_contour(image, plot=False):\n    \n    # Convert the image to grayscale, and blur it slightly\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n    \n    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n    thresh = cv2.erode(thresh, None, iterations=2)\n    thresh = cv2.dilate(thresh, None, iterations=2)\n\n    # Find contours in thresholded image, then grab the largest one\n    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    c = max(cnts, key=cv2.contourArea)\n    # extreme points\n    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n    extRight = tuple(c[c[:, :, 0].argmax()][0])\n    extTop = tuple(c[c[:, :, 1].argmin()][0])\n    extBot = tuple(c[c[:, :, 1].argmax()][0])\n    \n    # crop new image out of the original image using the four extreme points (left, right, top, bottom)\n    new_image = image[extTop[1]:extBot[1], extLeft[0]:extRight[0]]            \n\n    if plot:\n        plt.figure()\n        plt.subplot(1, 2, 1)\n        plt.imshow(image)\n        plt.tick_params(axis='both', which='both', top=False, bottom=False, left=False, right=False,labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n        plt.title('Original Image')\n        plt.subplot(1, 2, 2)\n        plt.imshow(new_image)\n        plt.tick_params(axis='both', which='both',top=False, bottom=False, left=False, right=False,labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n        plt.title('Cropped Image')\n        plt.show()\n    \n    return new_image","c36f9a02":"img = cv2.imread('..\/input\/brain-mri-images-for-brain-tumor-detection\/brain_tumor_dataset\/yes\/Y108.jpg')\nimg = cv2.resize(\n            img,\n            dsize=IMG_SIZE,\n            interpolation=cv2.INTER_CUBIC\n        )\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# threshold the image, then perform a series of erosions +\n# dilations to remove any small regions of noise\nthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\nthresh = cv2.erode(thresh, None, iterations=2)\nthresh = cv2.dilate(thresh, None, iterations=2)\n\n# find contours in thresholded image, then grab the largest one\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nc = max(cnts, key=cv2.contourArea)\n\n# find the extreme points\nextLeft = tuple(c[c[:, :, 0].argmin()][0])\nextRight = tuple(c[c[:, :, 0].argmax()][0])\nextTop = tuple(c[c[:, :, 1].argmin()][0])\nextBot = tuple(c[c[:, :, 1].argmax()][0])\n\n# add contour on the image\nimg_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n\n# add extreme points\nimg_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\nimg_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n\n# crop\nADD_PIXELS = 0\nnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()","b3b4a040":"plt.figure(figsize=(15,6))\nplt.subplot(141)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 1. Get the original image')\nplt.subplot(142)\nplt.imshow(img_cnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 2. Find the biggest contour')\nplt.subplot(143)\nplt.imshow(img_pnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 3. Find the extreme points')\nplt.subplot(144)\nplt.imshow(new_img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 4. Crop the image')\nplt.show()","65005b83":"def Croping_Data(train):\n\n    # load all images in a directory\n    X = []\n    y = []\n    \n    for img in train:\n        image = crop_brain_contour(img, plot=False)\n        X.append(image)\n                \n    X = np.array(X)\n    \n    return X","ce936cff":"X = Croping_Data(X_train)","c028db36":"plot_samples(X, y_train, ['yes','no'], 20)","c06f7e2b":"def Resize_Data(train):\n\n    # load all images in a directory\n    X = []\n    y = []\n    \n    IMG_WIDTH, IMG_HEIGHT = (240, 240)\n    \n    for img in train:\n        image = cv2.resize(img, dsize=(IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_CUBIC)\n        # normalize values\n        image = image \/ 255.\n        # convert image to numpy array and append it to X\n        X.append(image)\n                \n    X = np.array(X)\n    \n    return X","1185fd94":"augmented_yes =augmented_data_path+'yes'\naugmented_no = augmented_data_path+'no'\n\nIMG_WIDTH, IMG_HEIGHT = (240, 240)\n\nX = Resize_Data(X)\ny = y_train\n\n","d08f9f06":"plot_samples(X, y_train, ['yes','no'],10)","1642836b":"def split_data(X, y, test_size=0.2):\n       \n    X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=test_size)\n    X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5)\n    \n    return X_train, y_train, X_val, y_val, X_test, y_test","1c676406":"X_train, y_train, X_val, y_val, X_test, y_test = split_data(X, y, test_size=0.05)","0a2d7883":"print (\"number of training examples = \" + str(X_train.shape[0]))\nprint (\"number of validation examples = \" + str(X_val.shape[0]))\nprint (\"number of test examples = \" + str(X_test.shape[0]))","5caeb4b9":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\n\ny = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_train, y_val, y_test):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[0],\n    name='No',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[1],\n    name='Yes',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Count of classes in each set',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\nfig = go.Figure(data, layout)\niplot(fig)","950c9495":"def build_model(input_shape):\n    X_input = Input(input_shape) \n    X = ZeroPadding2D((2, 2))(X_input) \n    \n    X = Conv2D(32, (7, 7), strides = (1, 1))(X)\n    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n    X = Activation('relu')(X) \n    \n    X = MaxPooling2D((4, 4))(X) \n    X = MaxPooling2D((4, 4))(X) \n    X = Flatten()(X) \n    X = Dense(1, activation='sigmoid')(X) \n    model = Model(inputs = X_input, outputs = X)\n    \n    return model","bcbdf287":"IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3)\nmodel=build_model(IMG_SHAPE)\nmodel.summary()","c8edd969":"def plot_metrics(history):\n    \n    train_loss = history['loss']\n    val_loss = history['val_loss']\n    train_acc = history['accuracy']\n    val_acc = history['val_accuracy']\n    \n    # Loss\n    plt.figure()\n    plt.plot(train_loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.title('Loss')\n    plt.legend()\n    plt.show()\n    \n    # Accuracy\n    plt.figure()\n    plt.plot(train_acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.legend()\n    plt.show()","23e7c6d7":"\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","52d21d91":"import itertools\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nlabels = ['yes','no']","44c48003":"from sklearn.model_selection import KFold\nimport numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras import layers\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping\n\nnum_folds = 5\n\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n\n# Merge inputs and targets\ninputs = np.concatenate((X_train, X_val), axis=0)\ntargets = np.concatenate((y_train, y_val), axis=0)\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=num_folds, shuffle=True)\n\n# K-fold Cross Validation model evaluation\nfold_no = 1\n\nvgg16_weight_path = '..\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nvgg16_model = VGG16(weights=vgg16_weight_path,include_top=False, input_shape=IMG_SHAPE)\n\nNUM_CLASSES = 1\n\nfor train, test in kfold.split(inputs, targets):\n    \n  # Define the model architecture\n    #model=build_model(IMG_SHAPE)\n    \n    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    # model.fit(x=X_train, y=y_train, batch_size=32, epochs=25, validation_data=(X_val, y_val))\n    \n    #IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3)\n    #model=build_model(IMG_SHAPE)\n    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    model = Sequential()\n    model.add(vgg16_model)\n    model.add(layers.Flatten())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n\n    model.layers[0].trainable = False\n\n    model.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=1e-4),metrics=['accuracy'])\n \n    # Generate a print\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n\n    # Fit data to model\n    model.fit(inputs[train], targets[train],batch_size=32,epochs=25, verbose=0)\n    \n    history = model.history.history\n\n      # Generate generalization metrics\n    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    acc_per_fold.append(scores[1] * 100)\n    loss_per_fold.append(scores[0])\n    # Increase fold number\n    fold_no = fold_no + 1\n\n# == Provide average scores ==\nprint('------------------------------------------------------------------------')\nprint('Score per fold')\nfor i in range(0, len(acc_per_fold)):\n  print('------------------------------------------------------------------------')\n  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\nprint('------------------------------------------------------------------------')\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\nprint(f'> Loss: {np.mean(loss_per_fold)}')\nprint('------------------------------------------------------------------------')","6f51ea1e":"Now that we understand how K-fold Cross Validation works, it\u2019s time to code an example with the Keras deep learning framework \ud83d\ude42\nCoding it will be a multi-stage process:\n* Firstly, we\u2019ll take a look at what we need in order to run our model successfully.\n* Subsequently, we add K-fold Cross Validation, train the model instances, and average performance.\n* Finally, we output the performance metrics on screen.","3c1f66ca":"The result when we applied VGG 16 model with epochs 25 with Augenmtation 5 images per image and applied the training data to be 95% ","9f916912":"# <a id='import'>3. Data Import and Preprocessing<\/a>","027670e5":"# <a id='env'>2. Setting up the Environment<\/a>","9e485005":"A more expensive and less na\u00efve approach would be to perform K-fold Cross Validation. Here, you set some value for \ud835\udc3e. The dataset is split into \ud835\udc3e partitions of equal size. \ud835\udc3e\u20131 are used for training, while one is used for testing. This process is repeated \ud835\udc3e times, with a different partition used for testing each time.\nFor example, this would be the scenario for our dataset with \ud835\udc3e=5 (i.e., once again the 80\/20 split, but then 5 times!):\n\n![](https:\/\/www.machinecurve.com\/wp-content\/uploads\/2020\/02\/KTraintest.png)\n\nFor each split, the same model is trained, and performance is displayed per fold. For evaluation purposes, you can obviously also average it across all folds. While this produces better estimates, K-fold Cross Validation also increases training cost: in the \ud835\udc3e=5 scenario above, the model must be trained for 5 times.\nLet\u2019s now extend our viewpoint with a few variations of K-fold Cross Validation.\nIf you have no computational limitations whatsoever, you might wish to try a special case of K-fold Cross Validation, called Leave One Out Cross Validation (or LOOCV, Khandelwal 2019). LOOCV means \ud835\udc3e=\ud835\udc41, where \ud835\udc41 is the number of samples in your dataset. As the number of models trained is maximized, the precision of the model performance average is maximized too, but so is the cost of training due to the sheer amount of models that must be trained.\nIf you have a binary classification problem, you might also wish to take a look at Stratified Cross Validation (Khandelwal, 2019). It extends K-fold Cross Validation by ensuring an equal distribution of the target classes over the splits. This ensures that your classification problem is balanced. It doesn\u2019t work for multiclass classification due to the way that samples are distributed.","7bcb3b4a":"# <a id='concl'>7. References<\/a>\n\n* https:\/\/www.kaggle.com\/ethernext\/brain-tumour-detection-with-cnn-96-accuracy\n* https:\/\/www.kaggle.com\/keras\/vgg19\/home?select=vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n* https:\/\/www.kaggle.com\/shivamb\/cnn-architectures-vgg-resnet-inception-tl\n* https:\/\/www.kaggle.com\/loaiabdalslam\/brain-tumor-detection-cnn\n* https:\/\/www.kaggle.com\/ruslankl\/brain-tumor-detection-v1-0-cnn-vgg-16\n* https:\/\/github.com\/nickbiso\/Keras-Class-Activation-Map\/blob\/master\/Class%20Activation%20Map(CAM).ipynb\n* https:\/\/www.pyimagesearch.com\/2016\/04\/11\/finding-extreme-points-in-contours-with-opencv\/\n* https:\/\/www.machinecurve.com\/index.php\/2020\/02\/18\/how-to-use-k-fold-cross-validation-with-keras\/","4f6e7a72":"## Preprocessing the data\n\nIn order to crop the specific part of the image containing tumour,cropping technique via OpenCv is used, the details can be found here.[How to find extreme points in OpenCv?](https:\/\/www.pyimagesearch.com\/2016\/04\/11\/finding-extreme-points-in-contours-with-opencv\/)","e833200c":"### Plotting of aaccuracy","e5e1d52d":"### Making directory for augmented images","99a73d25":"**<center><font size=5>Brain Tumor Detection<\/font><\/center>**\n***\n**Authors**: Hussien Elgabry, Moustafa Bahnasawy and Youssef Aziz\n\n**date**: 2nd August, 2020\n\n**Table of Contents**\n- <a href='#intro'>1. Project Overview and Objectives<\/a> \n    - <a href='#dataset'>1.1. Data Set Description<\/a>\n    - <a href='#tumor'>1.2. What is Brain Tumor?<\/a>\n- <a href='#env'>2. Setting up the Environment<\/a>\n- <a href='#import'>3. Data Import and Preprocessing<\/a>\n- <a href='#cnn'>4. CNN Model<\/a>\n    - <a href='#aug'>4.1. Model Building<\/a>\n    - <a href='#build'>4.2. Pretrained Model - VGG 16<\/a>\n- <a href='#cnn'>5. CAM<\/a>\n- <a href='#concl'>6. Conclusions<\/a>\n- <a href='#concl'>7. References<\/a>","0fb50dd9":"**A directory is formed using os.makedirs() function for augmented images(yes\/ no). Note- custom directory is obtained in outputs folder.**","48e77bdf":"# <a id='intro'>1. Project Overview and Objectives<\/a>\n\nThe main purpose of this project was to build a CNN model that would classify if subject has a tumor or not based on MRI scan.We used some models architecture and weights to train the model for this binary problem. We used `accuracy` as a metric to justify the model performance which can be defined as:\n\n$\\textrm{Accuracy} = \\frac{\\textrm{Number of correclty predicted images}}{\\textrm{Total number of tested images}} \\times 100\\%$\n\n<br>\n\n- **validation set** - is the set used during the model training to adjust the hyperparameters.\n- **test set** - is the small set that We don't touch for the whole training process at all. It's been used for final model performance evaluation.\n\n\n## <a id='dataset'>1.1. Data Set Description<\/a>\n\nThe image data that was used for this problem is [Brain MRI Images for Brain Tumor Detection](https:\/\/www.kaggle.com\/navoneel\/brain-mri-images-for-brain-tumor-detection). It conists of MRI scans of two classes:\n\n* `NO` - no tumor, encoded as `0`\n* `YES` - tumor, encoded as `1`\n\n## <a id='tumor'>1.2. What is Brain Tumor?<\/a>\n\n> A brain tumor occurs when abnormal cells form within the brain. There are two main types of tumors: cancerous (malignant) tumors and benign tumors. Cancerous tumors can be divided into primary tumors, which start within the brain, and secondary tumors, which have spread from elsewhere, known as brain metastasis tumors. All types of brain tumors may produce symptoms that vary depending on the part of the brain involved. These symptoms may include headaches, seizures, problems with vision, vomiting and mental changes. The headache is classically worse in the morning and goes away with vomiting. Other symptoms may include difficulty walking, speaking or with sensations. As the disease progresses, unconsciousness may occur.\n>\n> ![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/5f\/Hirnmetastase_MRT-T1_KM.jpg)\n>\n> *Brain metastasis in the right cerebral hemisphere from lung cancer, shown on magnetic resonance imaging.*\n\nSource: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Brain_tumor)","55519b52":"## Augmentation of images \n\n**Data augmentation** is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. \n\nData augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.\n\nAbout the data:\nThe dataset contains 2 folders: yes and no which contains **253 Brain MRI Images**. The folder yes contains 155 Brain MRI Images that are tumorous andno contains 98 Brain MRI Images that are non-tumorous. After applying augmentation we will have **2065 samples**\n\n![](https:\/\/i.ibb.co\/tpBWTX2\/Screen-Shot-2020-08-01-at-9-11-51-PM.png)\n\n","fa0d7be1":"### <a id='build'>4.1 Model building<\/a>","a5336462":"# <a id='concl'>6. Conclusions<\/a>\n\nThis project was a combination of CNN model classification problem (to predict wheter the subject has brain tumor or not) & Computer Vision problem (to automate the process of brain cropping from MRI scans). Accuracy increased while increasing augemntation which is very logical, K-folds reflect the average accuracy we got","3d88a466":" Right now all images are in one folder with yes and no subfolders. I will split the data into train, val and test folders which makes its easier to work for me. The new folder heirarchy will look as follows:","23f58491":"### Adding K-fold Cross Validation","c44ea490":"## Visualization of data","78b53408":"## <a id='build'>4. CNN Modeling<\/a>","4100e792":"## After applying the cropping function"}}