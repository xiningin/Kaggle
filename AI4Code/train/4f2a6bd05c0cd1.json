{"cell_type":{"10c5d41f":"code","31ed15e1":"code","7208aad2":"code","7d46d060":"code","a0b4b044":"code","c4d0912d":"code","56d6c136":"code","c3b7bd5d":"code","5f3d28a8":"code","06a291af":"code","528f4230":"code","7bea7a23":"code","2b9c2132":"code","b6037186":"code","33436807":"code","57fe382b":"code","01b900c6":"code","b0f812a8":"code","c9f71c2e":"code","f760eed8":"code","b9523bcb":"code","c37af16e":"code","fcdbaf58":"code","79cf5e2e":"code","cdc743e3":"code","9ffdd7b3":"code","0ada1e90":"code","7913fe0a":"code","e16a1184":"code","8544fc9c":"code","640c0f7d":"code","a2a84775":"code","cb53ffc6":"code","93ace30b":"code","ee1c9d91":"code","5e04ad9f":"code","9661983a":"code","ccb73478":"code","8075e804":"code","01ef15ef":"code","68e099ce":"code","f0426964":"markdown","6536438e":"markdown","0e301e0d":"markdown","9f02d4ba":"markdown","55bdf8d9":"markdown","36b1c829":"markdown","c43b59b1":"markdown","74fdcec0":"markdown","987c04a7":"markdown","8cc84a95":"markdown","7d08810a":"markdown","58589904":"markdown","7e021ec6":"markdown","37efa658":"markdown","138967a2":"markdown","5837cd3a":"markdown","946163ca":"markdown","c6f12b59":"markdown","b59dc8d7":"markdown","42e0c668":"markdown","3d919e53":"markdown","859093d6":"markdown","6315f1c9":"markdown","d93e6855":"markdown","ac60ed03":"markdown","76df7763":"markdown","b4aebf06":"markdown","7f4057f3":"markdown","652272f4":"markdown","3b647271":"markdown","ba357cf5":"markdown","224420fa":"markdown","718eb587":"markdown","48a0f182":"markdown","3c74ba83":"markdown","90c21c1c":"markdown","15926cf2":"markdown","3097cd71":"markdown","4d40709e":"markdown","069f428e":"markdown","e9a773da":"markdown","cb285254":"markdown","ba3d8d66":"markdown","ab6be376":"markdown","c8176106":"markdown","7be63159":"markdown","db411838":"markdown","4944fdea":"markdown"},"source":{"10c5d41f":"import numpy as np \nimport pandas as pd \nimport os\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV, KFold\nimport shap\nimport eli5\nfrom tqdm import tqdm\nfrom pdpbox import pdp\nfrom lime import lime_tabular\nfrom sklearn.preprocessing import KBinsDiscretizer\n\n%matplotlib inline","31ed15e1":"data = pd.read_csv('..\/input\/adult.csv')\ndata.loc[:, 'income'] = data['income'].map(lambda x: 0 if x == '<=50K' else 1)\ndata = data.select_dtypes(include=np.number)\ndata.head()","7208aad2":"def make_correlated_feature(dataset, feature='education.num', n_correlated=10, scale=0.1):\n    \"\"\"\n    Makes dataset with bunch of correlated features.\n    Drops 'feature' from dataset and adds n_correlated copies of this feature plus Gaussian noise\n    Args:\n        dataset: dataset to work on\n        feature: name of feature to make correlated features from\n        n_correlated: number of correlated features to create\n        scale: scale of the Gaussian noise\n    Returns:\n        dataset with correlated features bunch, dataset without correlated features bunch\n    \"\"\"\n    dataset = dataset.copy()\n    no_corr_columns = dataset.columns.drop(feature)\\\n    .append(pd.Index([feature + '_0'])).copy()\n    for i in range(n_correlated):\n        np.random.seed(i)\n        dataset[feature + '_' + str(i)] = data[feature] + np.random.normal(size=(len(dataset)),\n                                                                           scale=scale)\n    dataset = dataset.drop(feature, axis=1)\n    no_corr_dataset = dataset.loc[:, no_corr_columns].copy()\n    return dataset, no_corr_dataset\n\n\ndef get_logreg_weights(features, logreg):\n    \"\"\"\n    Args:\n        features: features names\n        logreg: fitted LogisticRegression\n    Returns:\n        Weights of the logistic regression model\n    \"\"\"\n    return pd.Series(index=features, data=logreg.coef_.flatten())\\\n           .sort_values(ascending=False)\n\n\ndef fit_log_reg_cv(dataset, target_col='income', penalty='l2', reg_range=np.logspace(-3, 3, 10)):\n    \"\"\"\n    Does GridSearchCV over range of C for LogisticRegression.\n    Args:\n        dataset: dataset to fit the model\n        target_col: target column name\n        penalty: penalty type for LogisticRegression\n        reg_range: range of C fo LogisticRegression\n    Returns:\n        Fitted GridSearchCV object\n    \"\"\"\n    estimator = LogisticRegression(penalty=penalty, solver='liblinear')\n    standardizer = StandardScaler()\n    pipeline_log_reg = make_pipeline(standardizer, estimator)\n    fold = KFold(n_splits=5, random_state=0, shuffle=True)\n    grid_search = GridSearchCV(estimator=pipeline_log_reg, param_grid={\n                                'logisticregression__C': reg_range}, \n                           scoring='roc_auc', cv=fold, n_jobs=-1,\n                          return_train_score=True, verbose=False)\n    grid_search.fit(dataset.drop(target_col, axis=1).astype(float), dataset[target_col])\n    return grid_search\n\n\ndef plot_gridsearchcv_res(gs):\n    \"\"\"\n    Plots GridSearchCV results.\n    Args:\n        gs: fitted GridSearchCV object\n    \"\"\"\n    plt.semilogx(gs.param_grid['logisticregression__C'], gs.cv_results_['mean_test_score'])\n    plt.xlabel('C')\n    plt.ylabel('roc-auc test')\n    plt.show()\n    \n    \ndef fit_and_validate_gbdt(estimator, dataset, target_col='income'):\n    \"\"\"\n    Fits gbdt model and validates it on random subsample of the dataset\n    Agrs:\n        estimator: estimator object\n        dataset: dataset to fit and validate the estimator\n        target_col: target column name\n    Returns:\n        Fitted estimator\n    \"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(dataset.drop(target_col, axis=1),\n                                                        dataset[target_col], \n                                                    shuffle=True, random_state=0, test_size=0.1)\n    estimator.fit(X_train, y_train)\n    pred = estimator.predict_proba(X_test)[:, 1]\n    print('Roc-auc on test: {}'.format(roc_auc_score(y_test, pred)))\n    return estimator\n\n\ndef plot_feature_importances(w, figsize=(5, 11), xlabel='weight'):\n    \"\"\"\n    Makes barplot with feature weights (importances).\n    Args:\n        w: pandas Series of weights (importances)\n        figsize: size of the plot\n        xlabel: label of horizontal axis (importance type)\n    Returns:\n        Barplot\n    \"\"\"\n    w = w.sort_values(ascending=True)\n    ylocs = np.arange(len(w.values))\n    _, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.barh(ylocs, w.values, align='center', height=0.2)\n    ax.set_yticks(ylocs)\n    ax.set_yticklabels(w.index)\n    ax.set_xlabel(xlabel)\n    return ax\n\n\ndef gbdt_feature_importances(features, importances):\n    \"\"\"\n    Args:\n        features: features names\n        importances: array of feature importances\n    Returns:\n        Dataframe with feature importances\n    \"\"\"\n    return pd.Series(index=features, data=importances)\\\n         .sort_values(ascending=False)\n\n\ndef eli5_feature_importance(estimator, dataset, target_col='income', subset=500):\n    \"\"\"\n    Calculates average absolute contribution of features using eli5\n    Args:\n        estimator: estimator object\n        dataset: dataset to calculate feature weights on\n        target_col: target column name\n        subset: random subset size for calculation\n    Returns:\n        Feature importance by eli5 (average absolute contributions of features)\n    \"\"\"\n    res = []\n    for i in tqdm(np.random.choice(dataset.index, subset, False)):\n        df = eli5.format_as_dataframe(eli5.explain_prediction(estimator, dataset.drop(target_col, axis=1).loc[i, :], \n                                                    targets=[True]))\n        res.append(df)\n    res = pd.concat(res, ignore_index=True)\n    res.loc[:, 'weight'] = res.loc[:, 'weight'].abs()\n    return res.groupby('feature')['weight'].mean().drop('<BIAS>').sort_values(ascending=False)\n\n\ndef lime_feature_importance(estimator, dataset, target_col='income', subset=100):\n    \"\"\"\n    Calculates average absolute contribution of features using lime\n    Args:\n        estimator: estimator object\n        dataset: dataset to calculate feature contributions on\n        target_col: target column name\n        subset: random subset size for calculation\n    Returns:\n        Feature importance by lime (average absolute contribution of features)\n    \"\"\"\n    res = []\n    explainer = lime_tabular.LimeTabularExplainer(dataset.drop(target_col, axis=1).values, feature_names=dataset.columns.drop(target_col),\n                           discretize_continuous=True)\n    for i in tqdm(np.random.choice(dataset.index, subset, False)):\n        exp = sorted(explainer.explain_instance(dataset.drop(target_col, axis=1).loc[i, :].values, estimator.predict_proba, \n                                                num_features=len(dataset.columns) - 1).as_map()[1])\n        df = pd.DataFrame(columns=['feature', 'weight'])\n        df.loc[:, 'feature'] = dataset.columns.drop(target_col)\n        df.loc[:, 'weight'] = exp\n        df.loc[:, 'weight'] = df.loc[:, 'weight'].apply(lambda x: x[1])\n        res.append(df)\n    res = pd.concat(res, ignore_index=True)\n    res.loc[:, 'weight'] = res.loc[:, 'weight'].abs()\n    return res.groupby('feature')['weight'].mean().sort_values(ascending=False)\n\n\ndef asses_predictive_power(estimator, dataset, target_col='income', scoring=roc_auc_score):\n    \"\"\"\n    Assesses predictive power of every individual feature by fitting and validating model,\n    based on this feature.\n    Args:\n        estimator: estimator to use\n        dataset: dataset to use\n        target_col: target column name\n        scoring: function to assess performance of the model on test set\n    Returns:\n        Features' predictive power (scoring value measured on test set)\n    \"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(dataset.drop(target_col, axis=1), dataset.income, \n                                                    shuffle=True, random_state=0, test_size=0.1)\n    res = pd.Series()\n    for c in X_train.columns:\n        estimator.fit(X_train.loc[:, c].values.reshape(-1, 1), y_train)\n        pred = estimator.predict_proba(X_test.loc[:, c].values.reshape(-1, 1))[:, 1]\n        res.loc[c] = scoring(y_test, pred)\n    return res.sort_values(ascending=False)\n\n\ndef target_dep_plot(X, y, n_bins=100):\n    \"\"\"\n    Plots the dependence of average target on feature's value.\n    Args:\n        X: feature\n        y: target\n        n_bins: number of quantile bins to split feature\n    \"\"\"\n    transformer = KBinsDiscretizer(n_bins, encode='ordinal')\n    bins = transformer.fit_transform(X.reshape(-1, 1)).flatten()\n    df = pd.DataFrame(columns=['x', 'bin', 'y'])\n    df.loc[:, 'x'] = X\n    df.loc[:, 'bin'] = bins\n    df.loc[:, 'y'] = y\n    targ_means = df.groupby('bin')['y'].mean().values\n    x_meds = df.groupby('bin')['x'].median().values\n    errors = (df.groupby('bin')['y'].std().values \/ \n             np.sqrt(df.groupby('bin')['y'].count().values))\n    plt.errorbar(x=x_meds, y=targ_means, yerr=errors)\n    plt.ylabel('Avg target')\n    plt.xlabel('Feature value (bin median)')","7d46d060":"data_corr, data_no_corr = make_correlated_feature(data, 'capital.gain', 20, scale=1000)","a0b4b044":"data_corr.head()","c4d0912d":"data_no_corr.head()","56d6c136":"corr_plot = data_corr.corr()\nsns.heatmap(corr_plot, \n            xticklabels=corr_plot.columns.values,\n            yticklabels=corr_plot.columns.values)\nplt.show()","c3b7bd5d":"gs_no_corr = fit_log_reg_cv(data_no_corr, penalty='l2')\nplot_gridsearchcv_res(gs_no_corr)","5f3d28a8":"wt = get_logreg_weights(data_no_corr.columns.drop('income'), gs_no_corr.best_estimator_.steps[-1][1])\nplot_feature_importances(wt, figsize=(5, 3))\nplt.show()","06a291af":"gs_corr = fit_log_reg_cv(data_corr, penalty='l2')\nplot_gridsearchcv_res(gs_corr)","528f4230":"wt = get_logreg_weights(data_corr.columns.drop('income'), gs_corr.best_estimator_.steps[-1][1])\nplot_feature_importances(wt)\nplt.show()","7bea7a23":"gs_no_corr = fit_log_reg_cv(data_no_corr, penalty='l1')\nplot_gridsearchcv_res(gs_no_corr)","2b9c2132":"wt = get_logreg_weights(data_no_corr.columns.drop('income'), gs_no_corr.best_estimator_.steps[-1][1])\nplot_feature_importances(wt, figsize=(5, 3))\nplt.show()","b6037186":"gs_corr = fit_log_reg_cv(data_corr, penalty='l1')\nplot_gridsearchcv_res(gs_corr)","33436807":"wt = get_logreg_weights(data_corr.columns.drop('income'), gs_corr.best_estimator_.steps[-1][1])\nplot_feature_importances(wt)\nplt.show()","57fe382b":"estimator = LogisticRegression(solver='liblinear')\nstandardizer = StandardScaler()\nest = make_pipeline(standardizer, estimator)\nwt = asses_predictive_power(est, data_no_corr, 'income')\nplot_feature_importances(wt, figsize=(5, 3), xlabel='Single-feature roc-auc on test')\nplt.show()","01b900c6":"estimator = LogisticRegression(solver='liblinear')\nstandardizer = StandardScaler()\nest = make_pipeline(standardizer, estimator)\nwt = asses_predictive_power(est, data_corr, 'income')\nplot_feature_importances(wt, xlabel='Single-feature roc-auc on test')\nplt.show()","b0f812a8":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(importance_type='weight'), data_no_corr)\nimp = gbdt_feature_importances(data_no_corr.columns.drop('income'), estimator.feature_importances_)\nplot_feature_importances(imp, figsize=(5, 3), xlabel='Feature importance (weight, split)')\nplt.show()","c9f71c2e":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(importance_type='weight'), data_corr)\nimp = gbdt_feature_importances(data_corr.columns.drop('income'), estimator.feature_importances_)\nplot_feature_importances(imp, xlabel='Feature importance (weight, split)')\nplt.show()","f760eed8":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(importance_type='total_gain'), data_no_corr)\nimp = gbdt_feature_importances(data_no_corr.columns.drop('income'), estimator.feature_importances_)\nplot_feature_importances(imp, figsize=(5, 3), xlabel='Feature importance (total gain)')\nplt.show()","b9523bcb":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(importance_type='total_gain'), data_corr)\nimp = gbdt_feature_importances(data_corr.columns.drop('income'), estimator.feature_importances_)\nplot_feature_importances(imp, xlabel='Feature importance (total gain)')\nplt.show()","c37af16e":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(importance_type='total_cover'), data_no_corr)\nimp = gbdt_feature_importances(data_no_corr.columns.drop('income'), estimator.feature_importances_)\nplot_feature_importances(imp, figsize=(5, 3), xlabel='Feature importance (total cover)')\nplt.show()","fcdbaf58":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(importance_type='total_cover'), data_corr)\nimp = gbdt_feature_importances(data_corr.columns.drop('income'), estimator.feature_importances_)\nplot_feature_importances(imp, xlabel='Feature importance (total cover)')\nplt.show()","79cf5e2e":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(importance_type='gain'), data_no_corr)\nimp = gbdt_feature_importances(data_no_corr.columns.drop('income'), estimator.feature_importances_)\nplot_feature_importances(imp, figsize=(5, 3), xlabel='Feature importance (average gain - xgb gain)')\nplt.show()","cdc743e3":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(importance_type='gain'), data_corr)\nimp = gbdt_feature_importances(data_corr.columns.drop('income'), estimator.feature_importances_)\nplot_feature_importances(imp, xlabel='Feature importance (average gain - xgb gain)')\nplt.show()","9ffdd7b3":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(), data_no_corr)\nexplainer = shap.TreeExplainer(estimator)\nshap_values = explainer.shap_values(data_no_corr.drop('income', axis=1))\nshap.summary_plot(shap_values, data_no_corr.drop('income', axis=1))","0ada1e90":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(), data_corr)\nexplainer = shap.TreeExplainer(estimator)\nshap_values = explainer.shap_values(data_corr.drop('income', axis=1))\nshap.summary_plot(shap_values, data_corr.drop('income', axis=1))","7913fe0a":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(), data_no_corr)\nimp = eli5_feature_importance(estimator, data_no_corr)\nplot_feature_importances(imp, figsize=(5, 3), xlabel='Feature importance (eli5 absolute avg contribution)')\nplt.show()","e16a1184":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(), data_corr)\nimp = eli5_feature_importance(estimator, data_corr)\nplot_feature_importances(imp, xlabel='Feature importance (eli5 absolute avg contribution)')\nplt.show()","8544fc9c":"estimator = xgb.XGBClassifier()\nX_train, X_test, y_train, y_test = train_test_split(data_no_corr.drop('income', axis=1), data_no_corr.income, \n                                                    shuffle=True, random_state=0, test_size=0.1)\nestimator.fit(X_train.values, y_train)\npred = estimator.predict_proba(X_test.values)[:, 1]\nprint('Roc-auc on test: {}'.format(roc_auc_score(y_test, pred)))\ntest_data = X_test.copy()\ntest_data.loc[:, 'income'] = y_test\nimp = lime_feature_importance(estimator, test_data)\nplot_feature_importances(imp, figsize=(5, 3), xlabel='Feature importance (LIME absolute avg contribution)')\nplt.show()","640c0f7d":"estimator = xgb.XGBClassifier()\nX_train, X_test, y_train, y_test = train_test_split(data_corr.drop('income', axis=1), data_corr.income, \n                                                    shuffle=True, random_state=0, test_size=0.1)\nestimator.fit(X_train.values, y_train)\npred = estimator.predict_proba(X_test.values)[:, 1]\nprint('Roc-auc on test: {}'.format(roc_auc_score(y_test, pred)))\ntest_data = X_test.copy()\ntest_data.loc[:, 'income'] = y_test\nimp = lime_feature_importance(estimator, test_data)\nplot_feature_importances(imp, xlabel='Feature importance (LIME absolute avg contribution)')\nplt.show()","a2a84775":"est = xgb.XGBClassifier()\nimp = asses_predictive_power(est, data_no_corr, 'income')\nplot_feature_importances(imp, figsize=(5, 3), xlabel='Single-feature roc-auc on test')\nplt.show()","cb53ffc6":"est = xgb.XGBClassifier()\nimp = asses_predictive_power(est, data_corr, 'income')\nplot_feature_importances(imp, xlabel='Single-feature roc-auc on test')\nplt.show()","93ace30b":"estimator = xgb.XGBClassifier()\nX_train, X_test, y_train, y_test = train_test_split(data_no_corr.drop('income', axis=1), data_no_corr.income, \n                                                    shuffle=True, random_state=0, test_size=0.1)\nestimator.fit(X_train, y_train)\npred = estimator.predict_proba(X_test)[:, 1]\nprint('Roc-auc on test: {}'.format(roc_auc_score(y_test, pred)))\n\npdp_capital_gain = pdp.pdp_isolate(\n    model=estimator, dataset=X_test, model_features=X_test.columns, feature='capital.gain_0', num_grid_points=100, grid_type='equal')\npdp.pdp_plot(\n    pdp_capital_gain, 'capital.gain_0', plot_pts_dist=True, plot_lines=True\n)","ee1c9d91":"estimator = xgb.XGBClassifier()\nX_train, X_test, y_train, y_test = train_test_split(data_corr.drop('income', axis=1), data_corr.income, \n                                                    shuffle=True, random_state=0, test_size=0.1)\nestimator.fit(X_train, y_train)\npred = estimator.predict_proba(X_test)[:, 1]\nprint('Roc-auc on test: {}'.format(roc_auc_score(y_test, pred)))\n\npdp_capital_gain = pdp.pdp_isolate(\n    model=estimator, dataset=X_test, model_features=X_test.columns, feature='capital.gain_0', num_grid_points=100, grid_type='equal')\npdp.pdp_plot(\n    pdp_capital_gain, 'capital.gain_0', plot_pts_dist=True, plot_lines=True\n)","5e04ad9f":"estimator = xgb.XGBClassifier()\nX_train, X_test, y_train, y_test = train_test_split(data_corr.drop('income', axis=1), data_corr.income, \n                                                    shuffle=True, random_state=0, test_size=0.1)\nestimator.fit(X_train, y_train)\npred = estimator.predict_proba(X_test)[:, 1]\nprint('Roc-auc on test: {}'.format(roc_auc_score(y_test, pred)))\n\npdp_capital_gain = pdp.pdp_isolate(\n    model=estimator, dataset=X_test, model_features=X_test.columns, feature='capital.gain_1', num_grid_points=100, grid_type='equal')\npdp.pdp_plot(\n    pdp_capital_gain, 'capital.gain_1', plot_pts_dist=True, plot_lines=True\n)","9661983a":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(), data_no_corr)\nexplainer = shap.TreeExplainer(estimator)\nshap_values = explainer.shap_values(data_no_corr.drop('income', axis=1))\nshap.dependence_plot(ind='capital.gain_0',\n                     shap_values=shap_values, \n                     features=data_no_corr.drop('income', axis=1))","ccb73478":"estimator = fit_and_validate_gbdt(xgb.XGBClassifier(), data_corr)\nexplainer = shap.TreeExplainer(estimator)\nshap_values = explainer.shap_values(data_corr.drop('income', axis=1))\nshap.dependence_plot(ind='capital.gain_0',\n                     shap_values=shap_values, \n                     features=data_corr.drop('income', axis=1))","8075e804":"shap.dependence_plot(ind='capital.gain_1',\n                     shap_values=shap_values, \n                     features=data_corr.drop('income', axis=1))","01ef15ef":"target_dep_plot(data_no_corr['capital.gain_0'].values, data_no_corr['income'].values, 100)","68e099ce":"target_dep_plot(data_corr['capital.gain_0'].values, data_corr['income'].values, 100)","f0426964":"Capital.gain seems to be the best predictor. Recall that we have StardardScaler in the pipeline, so all features have same scale. At least in terms of linear separation of classes capital.gain is the best feature.  \nLet's now do the same thing using the dataset with bunch of correlated features.","6536438e":"### Importance type: gain (LightGBM), total_gain (XGBoost)","0e301e0d":"## All in all:\n* Model interpretation explains the dependency of the outcome on features. __Outcome is not the target, target could depend on features in very different way even if the model is good__\n* Assessing features' predictive power using weights of the linear model of feature importances of GBDT can be misleading in case of correlated data\n* The fair way to assess predictive power is to fit and validate model for every single feature. In case of non-linear model it's a trade-off while we're not accounting for feature interactions\n* PDPBox, SHAP and some other libs can plot the dependency of model's outcome on feature. __This is not the dependency of target on feature and can differ significantly especially in case of correlated dataset__\n* The fair way to plot the dependency of target on feature is to split feature into bins and plot average target in bins. Simple, fair method which gives good result without trade-offs","9f02d4ba":"## Gradient boosted decision trees: feature importances","55bdf8d9":"Again the importance is splitted across correlated features and we would have thought that capital.gain is less important than it is.  \n_I should note once again that feature importance and predictive power are quite different things._ When the model is dealing with bunch of correlated capital.gain_x features the importance of each feature _should_ be small. Feature importance is the measure of how important is the feature __for the model__. The model can use any of 20 correlated capital.gain_x features, so each individual one is not so important. If we will drop for example capital.gain_1 the model's performance will not change significantly.  \nIn this sense SHAP, total_gain, total_cover and even weight are more reliable than average gain in xgboost. But if we are interested in predictive power of each individual feature they can be misleading. Average gain in xgboost in our example is misleading in terms of feature importance, but it could give us a hint that capital.gain is actually a very powerful feature.","36b1c829":"### Importance type: weight (split) - LightGBM default","c43b59b1":"As an alternative (a good one) we can build these plots directly using target. Let's split feature values into bins, calculate average target in each bin, estimate standard error and plot it.  \nActually this would be kind of ground truth dependency plot for target.","74fdcec0":"We'll define some helper functions.  \nThe most important one is for generating artificial correlated features. Presence of correlated features will cause discrepancy between features' predictive power and their importance in machine learning models.","987c04a7":"Let's take a look at the correlation plot of our data_corr. We have a bunch of 20 strongly correlated features.  \nDataset like this is quite realistic, I've encountered data like this in real life many times. The most obvious example is some per user statistic measured during different months (e.g. online store visits per January, Feburary and March).  \nLet's notice that the predictive power of all the features capital.gain_x should be roughly the same because the correlation is > 0.98.","8cc84a95":"Doing so on dataset with correlated features yields bad result.  ","7d08810a":"The dependence plot for capital.gain_1 looks much better, but is less expressive than initial plot for capital.gain_0.","58589904":"Now let's try LIME. It calculates feature's contribution by local linear approximation.  \nThe overall picture is the same: if we had bunch of correlated capital.gain features we would not recognize it as a strong predictor.","7e021ec6":"# Interpretable ML and dataset analytics: will interpretability help? ","37efa658":"Overall picture is the same","138967a2":"As we should expect, these estimates are quite robust to the presence of correlated features. If you're interested in measuring predictive power of individual features it is probably the best way to do it.  \nBut nevertheless oftentimes we're trying to estimate features' predictive power using model built on the entire dataset by treating model's feature weights or feature importances as estimates for features' predictive power.  \nAs we could see, doing so with linear models is not the best idea. May be GBDT is better...","5837cd3a":"## Logistic Regression\nLet's start with a linear model. We will fit the pipeline (StandardScaler, LogisticRegression) with L2 regularization using GridSearchCV.  \nWe'll do it with the initial dataset first.","946163ca":"Let's try L1 regularization now.","c6f12b59":"## ELI5","b59dc8d7":"### Partial dependence using PDPBox  \nThe idea behind this is pretty simple. We change just one feature for a bunch of objects while keeping other features constant and observe changes in average prediction.  \nIf we would do so on the initial dataset, we would observe an expressive dependency of the outcome on capital.gain_0.","42e0c668":"There is another thing we're often interested in. We're wondering what does the dependency of target on the feature look like.  \nAnd there is another popular model interpretation technique - partial dependencies. This plots outcome (not target) against feature. But it's again tempting to use this technique to get an idea of the dependence of target on feature.  \nLet's see what happens.","3d919e53":"Interpretable ML is a hot topic nowadays. When interpreting models we are trying to figure out _how does every single feature affect the model's prediction._\nSome models (e.g., linear models) are interpretable by their nature, some models are quite challenging to be interpreted (e.g., deep neural networks). There is a rapid progress and a lot of attention to model's interpretability these days. We're able to somehow interpret even the most sophisticated black-box algorithms nowadays.  \nBut oftentimes we want to know even more. We're not satisfied by knowing how does each feature affect the model's prediction, we want to know _how is each feature related to the target variable._ Although these tasks seem similar they are actually quite different. Let's observe the differences.  \nLet's start with the most basic analytic task: we will be looking for the features with the highest predictive power. We will see that using model interpretation methods for this task directly can be misleading.","859093d6":"Let's consider some fresh staff: SHAP.","6315f1c9":"### Importance type: total_cover","d93e6855":"And it looks the same no matter if there are correlated predictors.","ac60ed03":"All in all, if you want to get an idea of the dependencies - try to get rid of correlated predictors.","76df7763":"Let's take a look at gbdt's feature importances and how does it work with correlated features.  \nWe'll examine different strategies of assessing feature importance. The most simple (and misleading) one is weight (or split).  \nThe idea is just to count splits in decision trees that use specific feature and claim these counts as the importances. The main problem with this approach is that not all splits are same. Some splits tend to improve classification a lot while others provide only marginal improvement. We should account for this fact, but this approach doesn't.  \nLet's take a look at the feature importances (weight) on our dataset and observe it's change when correlated features added.","b4aebf06":"We're counting number of samples affected by splits using a specific feature.","7f4057f3":"We will use numeric features of Adult Census Income dataset","652272f4":"We're having similar problem again. But in this case some of the capital.gain features have 0 or roughly 0 weights. _It's very important to remember that 0 weight is not because the feature is bad (actually all capital.gain features are good). Zero weight is just because the features are correlated and we are using L1 regularization.  \nThe main take away here is that finding best predictors using regularized linear models can be misleading._ ","3b647271":"Observe capital.gain_1 and capital.gain_17. They both had high total_gain and low weight. And eventually they are at the top by gain.  \nThis is not a very solid effect: you can activate subsampling (set subsample<1), change random seed and get different results. But in any case some of these correlated features tend to be at the top.  \nProbably it would be a good idea to fit xgboost with different random seeds several times to get an idea of top predictors in your dataset. This averaging across splits happens to help with correlated predictors. But of course this is not the silver bullet: the effect is weird and unstable.","ba357cf5":"Actually we can measure predictive power of each individual feature quite naturally. Let's fit the model using only one feature and measure the performance of this model on the test set. This can be considered as an estimate of the predictive power of the feature.  \nLet's do it for initial dataset first.","224420fa":"But recall that capital.gain_1 got higher importance almost every time we tried to measure it. The plot for capital.gain_1 looks somewhat better, but defenetly worse then initial plot (notice the scale of y axis).","718eb587":"Other popular strategy of measuring feature importance is total gain. We sum up all the improvements of the objective caused by splits using specific feature.","48a0f182":"We will make two version of the dataset. The first one (data_corr) has 20 highly correlated features (capital.gain_x), the second has only initial features.","3c74ba83":"It seems that we've found the optimal value of C. Let's take a look at the weights.","90c21c1c":"Do you see what happens? _The weight is splitted across the correlated versions of capital.gain. If we had this dataset initially we would have concluded that capital.gain is much less important than education.num, age, hours.per.week._  \nCorrelated features tend to split the weight in case of L2 regularization. The important thing is that any of the capital.gain_x features is better predictor than education.num, age, hours.per.week, but due to the regularization and correlation they get less weight.","15926cf2":"But anyways we are free to do the same thing as we did for LogisticRegression. Let's try to fit model on one feature and assess it's performance on the test set. This estimate is roboust to the presence of the correlated features.  \nAs a trade-off we're not accounting for the features' interaction now which could be significant.","3097cd71":"### Dependence plot using SHAP\nNow we'll try to do the same thing with SHAP library. Here we're plotting feature contributions against feature values.  \nAnd the overall picture is the same.","4d40709e":"Now let's try eli5. Eli5 computes for every sample and for every feature the contribution of this feature to the outcome. If we sum the absolute values of these contributions we will get the feature importance - the measure of how much does the feature contributes to the outcome.  \nThe overall picture with correlated features is as usual. We get a good estimate of feature importance, but bad estimate of the feature's predictive power.","069f428e":"### Importance type: gain (XGBoost default)","e9a773da":"The feature importance is again splitted across the correlated features. This is actually what we should expect: splits in decision trees are distributed across the correlated features.  \nIf we had this bunch of correlated features initially we would have thought that capital.gain is not an important feature while it clearly is.","cb285254":"Here we compute __average__ gain for each feature across all splits. We actually divide total gain by number of splits.  \nIt sounds a bit weird: we're dividing two noisy quantities and we're expecting to get a noisy outcome. But in our case it works somewhat better: some of the correlated features are getting high importances.","ba3d8d66":"What do we have at the end of the day?  \nOnly feature importance 'gain' in xgboost which is gain averaged across splits can somehow help us to recognize the predictive power of correlated features. It's defenitely not the best way to assess feature importance, but the only way that is somehow resistant to the correlated features.","ab6be376":"## LIME","c8176106":"Again the same problem.","7be63159":"We should notice here that the top-1 predictor changed. Now it is education.num. Actually even when data is properly scaled the magnitude of feature weight depends on many factors, not only on the predictive power of the feature. In this case features education.num and capital.gain have significantly different distributions, so it's not a very good idea to compare their weights.  \nStrictly speaking, the linear model\u2019s weights could be compared only if 1) features\u2019 distributions are same, 2) dependencies are linear.  \nLet's try the same thing with correlated features and see what happens.","db411838":"### SHAP-values","4944fdea":"## Partial dependencies"}}