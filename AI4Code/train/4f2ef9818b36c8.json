{"cell_type":{"7c871c2c":"code","1cc73564":"code","9137cd9d":"code","13ef99fb":"code","99da024a":"code","3911ae7b":"code","fcaf558a":"code","ed012160":"code","5e769594":"code","1f04d2c0":"code","7835f53b":"code","3b78f277":"code","ead267ff":"code","4dca69b3":"code","dc3bc0f9":"code","c2584cc0":"code","2f336834":"code","a17ef8cd":"code","fef22eca":"code","21b92c92":"code","5a2cce40":"code","f3518c41":"code","26d074e2":"code","67871e5f":"code","00cb3bd1":"code","b8166bbb":"code","3490fa08":"code","fd06c811":"code","9d941856":"code","e9186bc9":"code","f4883f6c":"code","4b7d716b":"code","6586092d":"code","bccc06e2":"code","bdb27459":"code","b60c84e6":"code","b1171fc4":"code","6f95b914":"code","3c9d67d8":"code","4b21e5c5":"code","e143040e":"code","4d7cf2ea":"code","baef25de":"code","061cfb48":"code","f0e14b2e":"code","971ce391":"code","ca32939b":"code","ebaf8fc7":"code","4c0f17e6":"code","1d312035":"code","b4f10fdc":"code","0e96a8cb":"code","4bc0f63c":"code","210b72c0":"code","13988300":"code","35919108":"code","1f34a447":"code","98d469d4":"code","34bd6782":"code","59462d3d":"code","3c5e66d6":"code","e3db2d2d":"code","0aed5c3e":"code","f8b6a253":"code","b79d6931":"code","a7d88ff2":"code","4ebe9212":"code","f0c9319b":"code","139351cd":"markdown","1fa8f227":"markdown","7b8e7c88":"markdown","c422f20d":"markdown","68fff9f4":"markdown","33348759":"markdown","e5df6e3f":"markdown","adfe8dd7":"markdown","56eff847":"markdown","89d1da38":"markdown","53868cab":"markdown","14181cde":"markdown","14b1814b":"markdown","2e5a4edc":"markdown","7897a82f":"markdown","7daaaa7e":"markdown","7c7e62e1":"markdown","be9c9054":"markdown","02608393":"markdown","9bbc200a":"markdown","dada2f45":"markdown","2a1ccb92":"markdown","25ab4a12":"markdown","ed3f3e18":"markdown","04144d18":"markdown","c749cdae":"markdown","a14a6437":"markdown","5ed8006c":"markdown","f764eb21":"markdown","1b1611ed":"markdown","2b2e5734":"markdown","94b912d7":"markdown","527ec73d":"markdown","6f6b7760":"markdown","1aaa0866":"markdown","646174fd":"markdown","19f4801e":"markdown","ad5de319":"markdown","61ce65a0":"markdown","ee18085e":"markdown","c4f4b13a":"markdown","d5b4ad4c":"markdown","c8afc3b1":"markdown","411cd1c1":"markdown","52fabad8":"markdown","11cdbbbc":"markdown","8ff65645":"markdown","b1850bee":"markdown","c1db5569":"markdown","ae42c861":"markdown","6813763b":"markdown","406b40ff":"markdown","5e404c7f":"markdown","f47a8582":"markdown","28af8ad0":"markdown","0b9f3dda":"markdown","75576b1e":"markdown","022629e1":"markdown","44e750ce":"markdown","e990ef49":"markdown","f56b8995":"markdown","7f82a0d4":"markdown","f1078c2c":"markdown","2930c2d6":"markdown","d4e40a91":"markdown","c7993b33":"markdown","14a2e3c4":"markdown","5af5e93f":"markdown","7b2ae506":"markdown","c1c1ffbb":"markdown","4d4b8b40":"markdown","2ddac18d":"markdown","d265a069":"markdown","b9baa8dc":"markdown","b103752c":"markdown","56c1e3b3":"markdown","63607b96":"markdown","bbae5960":"markdown","5ab40680":"markdown","537cee12":"markdown","675f14c2":"markdown","eb44fbc0":"markdown","4b65a04c":"markdown","89577b84":"markdown","6c299334":"markdown","b0582f6e":"markdown","425884ec":"markdown","dc6de6e2":"markdown","a6ceb0e0":"markdown","73777caa":"markdown","c26ddbae":"markdown","1ae36ce4":"markdown","687fe5ff":"markdown","56221068":"markdown","6802abe1":"markdown","e528177c":"markdown","aa43d6bc":"markdown","f3e91201":"markdown","8d2b20c5":"markdown","0830a710":"markdown","c06dc2b2":"markdown","b58a6566":"markdown","c6e9e05d":"markdown","6e2f8215":"markdown","afae87a9":"markdown","2dc0be2e":"markdown","996fcbbd":"markdown"},"source":{"7c871c2c":"pip install folium --upgrade","1cc73564":"# Standard libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib.gridspec import GridSpec\npd.set_option('display.max_columns', 100)\nimport plotly.offline as py\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport json\nimport requests\nimport folium\nfrom folium.plugins import FastMarkerCluster, Fullscreen, MiniMap, HeatMap, HeatMapWithTime, LocateControl\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom PIL import Image\n\n# Utilities\nfrom viz_utils import *\nfrom custom_transformers import *\nfrom ml_utils import *\n\n# DataPrep\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import RSLPStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nimport joblib\n\n# Modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nimport lightgbm as lgb","9137cd9d":"# Reading all the files\nraw_path = '..\/input\/brazilian-ecommerce\/'\nolist_customer = pd.read_csv(raw_path + 'olist_customers_dataset.csv')\nolist_geolocation = pd.read_csv(raw_path + 'olist_geolocation_dataset.csv')\nolist_orders = pd.read_csv(raw_path + 'olist_orders_dataset.csv')\nolist_order_items = pd.read_csv(raw_path + 'olist_order_items_dataset.csv')\nolist_order_payments = pd.read_csv(raw_path + 'olist_order_payments_dataset.csv')\nolist_order_reviews = pd.read_csv(raw_path + 'olist_order_reviews_dataset.csv')\nolist_products = pd.read_csv(raw_path + 'olist_products_dataset.csv')\nolist_sellers = pd.read_csv(raw_path + 'olist_sellers_dataset.csv')","13ef99fb":"# Collections for each dataset\ndatasets = [olist_customer, olist_geolocation, olist_orders, olist_order_items, olist_order_payments,\n            olist_order_reviews, olist_products, olist_sellers]\nnames = ['olist_customer', 'olist_geolocation', 'olist_orders', 'olist_order_items', 'olist_order_payments',\n         'olist_order_reviews', 'olist_products', 'olist_sellers']\n\n# Creating a DataFrame with useful information about all datasets\ndata_info = pd.DataFrame({})\ndata_info['dataset'] = names\ndata_info['n_rows'] = [df.shape[0] for df in datasets]\ndata_info['n_cols'] = [df.shape[1] for df in datasets]\ndata_info['null_amount'] = [df.isnull().sum().sum() for df in datasets]\ndata_info['qty_null_columns'] = [len([col for col, null in df.isnull().sum().items() if null > 0]) for df in datasets]\ndata_info['null_columns'] = [', '.join([col for col, null in df.isnull().sum().items() if null > 0]) for df in datasets]\n\ndata_info.style.background_gradient()","99da024a":"df_overview = pd.DataFrame({})\nname_col = []\nfor name, df in zip(names, datasets):\n    name_col += [name] * df.shape[1]\n    df_overview = df_overview.append(data_overview(df))\n    df_overview['dataset_name'] = name_col\n\ndf_overview = df_overview.loc[:, ['dataset_name', 'feature', 'qtd_null', 'percent_null', 'dtype', 'qtd_cat']]\ndf_overview","3911ae7b":"df_orders = olist_orders.merge(olist_customer, how='left', on='customer_id')\nfig, ax = plt.subplots(figsize=(14, 6))\nsingle_countplot(df_orders, x='order_status', ax=ax)\nplt.show()","fcaf558a":"# Changing the data type for date columns\ntimestamp_cols = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', \n                  'order_estimated_delivery_date']\nfor col in timestamp_cols:\n    df_orders[col] = pd.to_datetime(df_orders[col])\n    \n# Extracting attributes for purchase date - Year and Month\ndf_orders['order_purchase_year'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.year)\ndf_orders['order_purchase_month'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.month)\ndf_orders['order_purchase_month_name'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.strftime('%b'))\ndf_orders['order_purchase_year_month'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.strftime('%Y%m'))\ndf_orders['order_purchase_date'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.strftime('%Y%m%d'))\n\n# Extracting attributes for purchase date - Day and Day of Week\ndf_orders['order_purchase_day'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.day)\ndf_orders['order_purchase_dayofweek'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.dayofweek)\ndf_orders['order_purchase_dayofweek_name'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.strftime('%a'))\n\n# Extracting attributes for purchase date - Hour and Time of the Day\ndf_orders['order_purchase_hour'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.hour)\nhours_bins = [-0.1, 6, 12, 18, 23]\nhours_labels = ['Dawn', 'Morning', 'Afternoon', 'Night']\ndf_orders['order_purchase_time_day'] = pd.cut(df_orders['order_purchase_hour'], hours_bins, labels=hours_labels)\n\n# New DataFrame after transformations\ndf_orders.head()","ed012160":"fig = plt.figure(constrained_layout=True, figsize=(13, 10))\n\n# Axis definition\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, :])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[1, 1])\n\n# Lineplot - Evolution of e-commerce orders along time \nsns.lineplot(data=df_orders['order_purchase_year_month'].value_counts().sort_index(), ax=ax1, \n             color='darkslateblue', linewidth=2)\nax1.annotate(f'Highest orders \\nreceived', (13, 7500), xytext=(-75, -25), \n             textcoords='offset points', bbox=dict(boxstyle=\"round4\", fc=\"w\", pad=.8),\n             arrowprops=dict(arrowstyle='-|>', fc='w'), color='dimgrey', ha='center')\nax1.annotate(f'Noise on data \\n(huge decrease)', (23, 0), xytext=(48, 25), \n             textcoords='offset points', bbox=dict(boxstyle=\"round4\", fc=\"w\", pad=.5),\n             arrowprops=dict(arrowstyle='-|>', fc='w'), color='dimgrey', ha='center')\nformat_spines(ax1, right_border=False)  \nfor tick in ax1.get_xticklabels():\n    tick.set_rotation(45)\nax1.set_title('Evolution of Total Orders in Brazilian E-Commerce', size=14, color='dimgrey')\n\n# Barchart - Total of orders by day of week\nsingle_countplot(df_orders, x='order_purchase_dayofweek', ax=ax2, order=False, palette='YlGnBu')\nweekday_label = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\nax2.set_xticklabels(weekday_label)\nax2.set_title('Total Orders by Day of Week', size=14, color='dimgrey', pad=20)\n\n# Barchart - Total of orders by time of the day\nday_color_list = ['darkslateblue', 'deepskyblue', 'darkorange', 'purple']\nsingle_countplot(df_orders, x='order_purchase_time_day', ax=ax3, order=False, palette=day_color_list)\nax3.set_title('Total Orders by Time of the Day', size=14, color='dimgrey', pad=20)\n\nplt.tight_layout()\nplt.show()","5e769594":"# Creating figure\nfig = plt.figure(constrained_layout=True, figsize=(13, 5))\n\n# Axis definition\ngs = GridSpec(1, 3, figure=fig)\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[0, 1:])\n\n# Annotation - Grown on e-commerce orders between 2017 and 2018\ndf_orders_compare = df_orders.query('order_purchase_year in (2017, 2018) & order_purchase_month <= 8')\nyear_orders = df_orders_compare['order_purchase_year'].value_counts()\ngrowth = int(round(100 * (1 + year_orders[2017] \/ year_orders[2018]), 0))\nax1.text(0.00, 0.73, f'{year_orders[2017]}', fontsize=40, color='mediumseagreen', ha='center')\nax1.text(0.00, 0.64, 'orders registered in 2017\\nbetween January and August', fontsize=10, ha='center')\nax1.text(0.00, 0.40, f'{year_orders[2018]}', fontsize=60, color='darkslateblue', ha='center')\nax1.text(0.00, 0.31, 'orders registered in 2018\\nbetween January and August', fontsize=10, ha='center')\nsignal = '+' if growth > 0 else '-'\nax1.text(0.00, 0.20, f'{signal}{growth}%', fontsize=14, ha='center', color='white', style='italic', weight='bold',\n         bbox=dict(facecolor='navy', alpha=0.5, pad=10, boxstyle='round, pad=.7'))\nax1.axis('off')\n\n# Bar chart - Comparison between monthly sales between 2017 and 2018\nsingle_countplot(df_orders_compare, x='order_purchase_month', hue='order_purchase_year', ax=ax2, order=False,\n                 palette='YlGnBu')\nmonth_label = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug']\nax2.set_xticklabels(month_label)\nax2.set_title('Total Orders Comparison Between 2017 and 2018 (January to August)', size=12, color='dimgrey', pad=20)\nplt.legend(loc='lower right')\nplt.show()","1f04d2c0":"# Merging orders and order_items\ndf_orders_items = df_orders.merge(olist_order_items, how='left', on='order_id')\n\n# Using the API to bring the region to the data\nr = requests.get('https:\/\/servicodados.ibge.gov.br\/api\/v1\/localidades\/mesorregioes')\ncontent = [c['UF'] for c in json.loads(r.text)]\nbr_info = pd.DataFrame(content)\nbr_info['nome_regiao'] = br_info['regiao'].apply(lambda x: x['nome'])\nbr_info.drop('regiao', axis=1, inplace=True)\nbr_info.drop_duplicates(inplace=True)\n\n# Threting geolocations outside brazilian map\n\n#Brazils most Northern spot is at 5 deg 16\u2032 27.8\u2033 N latitude.;\ngeo_prep = olist_geolocation[olist_geolocation.geolocation_lat <= 5.27438888]\n#it\u2019s most Western spot is at 73 deg, 58\u2032 58.19\u2033W Long.\ngeo_prep = geo_prep[geo_prep.geolocation_lng >= -73.98283055]\n#It\u2019s most southern spot is at 33 deg, 45\u2032 04.21\u2033 S Latitude.\ngeo_prep = geo_prep[geo_prep.geolocation_lat >= -33.75116944]\n#It\u2019s most Eastern spot is 34 deg, 47\u2032 35.33\u2033 W Long.\ngeo_prep = geo_prep[geo_prep.geolocation_lng <=  -34.79314722]\ngeo_group = geo_prep.groupby(by='geolocation_zip_code_prefix', as_index=False).min()\n\n# Merging all the informations\ndf_orders_items = df_orders_items.merge(br_info, how='left', left_on='customer_state', right_on='sigla')\ndf_orders_items = df_orders_items.merge(geo_group, how='left', left_on='customer_zip_code_prefix', \n                                        right_on='geolocation_zip_code_prefix')\ndf_orders_items.head()","7835f53b":"# Filtering data between 201701 and 201808\ndf_orders_filt = df_orders_items[(df_orders_items['order_purchase_year_month'].astype(int) >= 201701)]\ndf_orders_filt = df_orders_filt[(df_orders_filt['order_purchase_year_month'].astype(int) <= 201808)]\n\n# Grouping data by region\ndf_regions_group = df_orders_filt.groupby(by=['order_purchase_year_month', 'nome_regiao'], as_index=False)\ndf_regions_group = df_regions_group.agg({'customer_id': 'count', 'price': 'sum'}).sort_values(by='order_purchase_year_month')\ndf_regions_group.columns = ['month', 'region', 'order_count', 'order_amount']\ndf_regions_group.reset_index(drop=True, inplace=True)\n\n# Grouping data by city (top 10)\ndf_cities_group = df_orders_filt.groupby(by='geolocation_city', \n                                       as_index=False).count().loc[:, ['geolocation_city', 'order_id']]\ndf_cities_group = df_cities_group.sort_values(by='order_id', ascending=False).reset_index(drop=True)\ndf_cities_group = df_cities_group.iloc[:10, :]","3b78f277":"# Creating and preparing figure and axis\nfig = plt.figure(constrained_layout=True, figsize=(15, 10))\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[:, 1])\n\n# Count of orders by region\nsns.lineplot(x='month', y='order_count', ax=ax1, data=df_regions_group, hue='region', \n             size='region', style='region', palette='magma', markers=['o'] * 5)\nformat_spines(ax1, right_border=False)\nax1.set_title('Evolution of E-Commerce Orders on Brazilian Regions', size=12, color='dimgrey')\nax1.set_ylabel('')\nfor tick in ax1.get_xticklabels():\n    tick.set_rotation(45)\n\n# Top cities with more customers orders in Brazil\nsns.barplot(y='geolocation_city', x='order_id', data=df_cities_group, ax=ax2, palette='magma')\nAnnotateBars(n_dec=0, font_size=10, color='black').horizontal(ax2)\nformat_spines(ax2, right_border=False)\nax2.set_title('Top 10 Brazilian Cities with More Orders', size=12, color='dimgrey')\nax2.set_ylabel('')\n\n# Total orders by state\nsingle_countplot(y='customer_state', ax=ax3, df=df_orders_filt, palette='viridis')\nax3.set_title('Total of Customers Orders by State', size=12, color='dimgrey')\nax3.set_ylabel('')\n\nplt.show()","ead267ff":"# Zipping locations\nlats = list(df_orders_items.query('order_purchase_year == 2018')['geolocation_lat'].dropna().values)[:30000]\nlongs = list(df_orders_items.query('order_purchase_year == 2018')['geolocation_lng'].dropna().values)[:30000]\nlocations = list(zip(lats, longs))\n\n# Creating a mapa using folium\nmap1 = folium.Map(location=[-15, -50], zoom_start=4.0)\n\n# Plugin: FastMarkerCluster\nFastMarkerCluster(data=locations).add_to(map1)\n\nmap1","4dca69b3":"# Grouping geolocation data for plotting a heatmap\nheat_data = df_orders_filt.groupby(by=['geolocation_lat', 'geolocation_lng'], as_index=False).count().iloc[:, :3]\n\n# Creating a mapa using folium\nmap1 = folium.Map(\n    location=[-15, -50], \n    zoom_start=4.0, \n    tiles='cartodbdark_matter'\n)\n\n# Plugin: HeatMap\nHeatMap(\n    name='Mapa de Calor',\n    data=heat_data,\n    radius=10,\n    max_zoom=13\n).add_to(map1)\n\nmap1","dc3bc0f9":"epoch_list = []\nheatmap_evl_data = df_orders_items[(df_orders_items['order_purchase_year_month'].astype(int) >= 201801)]\nheatmap_evl_data = heatmap_evl_data[(heatmap_evl_data['order_purchase_year_month'].astype(int) <= 201807)]\ntime_index = heatmap_evl_data['order_purchase_year_month'].sort_values().unique()\nfor epoch in time_index:\n    data_temp = heatmap_evl_data.query('order_purchase_year_month == @epoch')\n    data_temp = data_temp.groupby(by=['geolocation_lat', 'geolocation_lng'], as_index=False).count()\n    data_temp = data_temp.sort_values(by='order_id', ascending=False).iloc[:, :3]\n    epoch_list.append(data_temp.values.tolist())\n    \n# Creating a mapa using folium\nmap2 = folium.Map(\n    location=[-15, -50], \n    zoom_start=4.0, \n    tiles='cartodbdark_matter'\n)\n\n# Plugin: HeatMapWithTime\nHeatMapWithTime(\n    name='Evolution of Orders in a Geolocation Perspective',\n    data=epoch_list,\n    radius=10,\n    index=list(time_index)\n).add_to(map2)\n\nmap2","c2584cc0":"# Grouping data\ndf_month_aggreg = df_orders_filt.groupby(by=['order_purchase_year', 'order_purchase_year_month'], as_index=False)\ndf_month_aggreg = df_month_aggreg.agg({\n    'order_id': 'count',\n    'price': 'sum',\n    'freight_value': 'sum'\n})\n\n# Adding new columns for analysis\ndf_month_aggreg['price_per_order'] = df_month_aggreg['price'] \/ df_month_aggreg['order_id']\ndf_month_aggreg['freight_per_order'] = df_month_aggreg['freight_value'] \/ df_month_aggreg['order_id']\ndf_month_aggreg.head()","2f336834":"# Creating figure\nfig = plt.figure(constrained_layout=True, figsize=(15, 12))\n\n# Axis definition\ngs = GridSpec(2, 3, figure=fig)\nax1 = fig.add_subplot(gs[0, :])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[1, 1:])\n\n# Plot 1 - Evolution of total orders and total sales on e-commerce\nsns.lineplot(x='order_purchase_year_month', y='price', ax=ax1, data=df_month_aggreg, linewidth=2, \n             color='darkslateblue', marker='o', label='Total Amount')\nax1_twx = ax1.twinx()\nsingle_countplot(df_orders_filt, x='order_purchase_year_month', ax=ax1_twx, order=False, palette='YlGnBu_r')\nax1_twx.set_yticks(np.arange(0, 20000, 2500))\n\n# Customizing the first plot\nformat_spines(ax1)\nfor tick in ax1.get_xticklabels():\n    tick.set_rotation(45)\nfor x, y in df_month_aggreg.price.items():\n    ax1.annotate(str(round(y\/1000, 1))+'K', xy=(x, y), textcoords='offset points', xytext=(0, 10),\n                ha='center', color='dimgrey')\nax1.annotate(f'Highest Value Sold on History\\n(Black Friday?)', (10, 1000000), xytext=(-120, -20), \n             textcoords='offset points', bbox=dict(boxstyle=\"round4\", fc=\"w\", pad=.8),\n             arrowprops=dict(arrowstyle='-|>', fc='w'), color='dimgrey', ha='center')\nax1.set_title('Evolution of E-commerce: Total Orders and Total Amount Sold (R$)', size=14, color='dimgrey', pad=20)\n\n# Plot 2 - Big Numbers of Sales Evolution\nmonth_comparison = ['201701', '201702', '201703', '201704', '201705', '201706', '201707', '201708',\n                    '201801', '201802', '201803', '201804', '201805', '201806', '201807', '201808']\ndf_sales_compare = df_month_aggreg.query('order_purchase_year_month in (@month_comparison)')\nsold_2017 = df_sales_compare.query('order_purchase_year == 2017')['price'].sum()\nsold_2018 = df_sales_compare.query('order_purchase_year == 2018')['price'].sum()\ngrowth = 1 + (sold_2017 \/ sold_2018)\nax2.text(0.50, 0.73, 'R$' + str(round(sold_2017\/1000000, 2)) + 'M', fontsize=40, color='mediumseagreen', ha='center')\nax2.text(0.50, 0.60, 'total amount sold in 2017\\nbetween January and August', fontsize=10, ha='center')\nax2.text(0.50, 0.36, 'R$' + str(round(sold_2018\/1000000, 2)) + 'M', fontsize=60, color='darkslateblue', ha='center')\nax2.text(0.50, 0.25, 'total amount sold in 2018\\nbetween January and August', fontsize=10, ha='center')\nsignal = '+' if growth > 0 else '-'\nax2.text(0.50, 0.13, f'{signal}{str(round(100 * growth, 2))}%', fontsize=14, ha='center', color='white', style='italic', weight='bold',\n         bbox=dict(facecolor='navy', alpha=0.5, pad=10, boxstyle='round, pad=.7'))\nax2.axis('off')\n\n# Plot 3 - Evolution of mean freight value paid by the customers\nsns.lineplot(x='order_purchase_year_month', y='freight_per_order', data=df_month_aggreg, linewidth=2, \n             color='silver', marker='o', ax=ax3)\nformat_spines(ax3, right_border=False)\nfor tick in ax3.get_xticklabels():\n    tick.set_rotation(45)\nfor x, y in df_month_aggreg.freight_per_order.items():\n    ax3.annotate(round(y, 2), xy=(x, y), textcoords='offset points', xytext=(0, 10),\n                ha='center', color='dimgrey')\nax3.set_title('Evolution of Average Freight Value (RS) Paid by Customers', size=14, color='dimgrey', pad=20)\n\nplt.tight_layout()\nplt.show()","a17ef8cd":"mean_sum_analysis(df_orders_filt, 'customer_state', 'price', palette='viridis', figsize=(15, 10))","fef22eca":"mean_sum_analysis(df_orders_filt, 'customer_state', 'freight_value', palette='viridis', figsize=(15, 10))","21b92c92":"# Calculating working days between purchasing, delivering and estimated delivery\npurchasing = df_orders_filt['order_purchase_timestamp']\ndelivered = df_orders_filt['order_delivered_customer_date']\nestimated = df_orders_filt['order_estimated_delivery_date']\ndf_orders_filt['time_to_delivery'] = calc_working_days(purchasing, delivered, convert=True)\ndf_orders_filt['diff_estimated_delivery'] = calc_working_days(estimated, delivered, convert=True)\n\n# Grouping data by state\nstates_avg_grouped = df_orders_filt.groupby(by='customer_state', as_index=False).mean()\nstates_freight_paid = states_avg_grouped.loc[:, ['customer_state', 'freight_value']]\nstates_time_to_delivery = states_avg_grouped.loc[:, ['customer_state', 'time_to_delivery']]\nstates_estimated_delivery = states_avg_grouped.loc[:, ['customer_state', 'diff_estimated_delivery']]\n\n# Sorting data\nstates_freight_paid = states_freight_paid.sort_values(by='freight_value', ascending=False)\nstates_time_to_delivery = states_time_to_delivery.sort_values(by='time_to_delivery', ascending=False)\nstates_estimated_delivery = states_estimated_delivery.sort_values(by='diff_estimated_delivery')","5a2cce40":"fig, axs = plt.subplots(3, 3, figsize=(15, 10))\n\n# Plot Pack 01 - Freight value paid on states\nsns.barplot(x='freight_value', y='customer_state', data=states_freight_paid.head(), ax=axs[1, 0], palette='viridis')\naxs[1, 0].set_title('Top 5 States with Highest \\nAverage Freight Value', size=12, color='black')\nsns.barplot(x='freight_value', y='customer_state', data=states_freight_paid.tail(), ax=axs[2, 0], palette='viridis_r')\naxs[2, 0].set_title('Top 5 States with Lowest \\nAverage Freight Value', size=12, color='black')\nfor ax in axs[1, 0], axs[2, 0]:\n    ax.set_xlabel('Mean Freight Value')\n    ax.set_xlim(0, states_freight_paid['freight_value'].max())\n    format_spines(ax, right_border=False)\n    ax.set_ylabel('')\n\n# Annotations\naxs[0, 0].text(0.50, 0.30, f'R${round(df_orders_filt.freight_value.mean(), 2)}', fontsize=45, ha='center')\naxs[0, 0].text(0.50, 0.12, 'is the mean value of freight paid', fontsize=12, ha='center')\naxs[0, 0].text(0.50, 0.00, 'for online shopping', fontsize=12, ha='center')\naxs[0, 0].axis('off')\n\n# Plot Pack 02 - Time to delivery on states\nsns.barplot(x='time_to_delivery', y='customer_state', data=states_time_to_delivery.head(), ax=axs[1, 1], palette='viridis')\naxs[1, 1].set_title('Top 5 States with Highest \\nAverage Time to Delivery', size=12, color='black')\nsns.barplot(x='time_to_delivery', y='customer_state', data=states_time_to_delivery.tail(), ax=axs[2, 1], palette='viridis_r')\naxs[2, 1].set_title('Top 5 States with Lowest \\nAverage Time do Delivery', size=12, color='black')\nfor ax in axs[1, 1], axs[2, 1]:\n    ax.set_xlabel('Time to Delivery')\n    ax.set_xlim(0, states_time_to_delivery['time_to_delivery'].max())\n    format_spines(ax, right_border=False)\n    ax.set_ylabel('')\n    \n# Annotations\naxs[0, 1].text(0.40, 0.30, f'{int(df_orders_filt.time_to_delivery.mean())}', fontsize=45, ha='center')\naxs[0, 1].text(0.60, 0.30, 'working days', fontsize=12, ha='center')\naxs[0, 1].text(0.50, 0.12, 'is the average delay for delivery', fontsize=12, ha='center')\naxs[0, 1].text(0.50, 0.00, 'for online shopping', fontsize=12, ha='center')\naxs[0, 1].axis('off')\n    \n# Plot Pack 03 - Differnece between delivered and estimated on states\nsns.barplot(x='diff_estimated_delivery', y='customer_state', data=states_estimated_delivery.head(), ax=axs[1, 2], palette='viridis')\naxs[1, 2].set_title('Top 5 States where Delivery is \\nReally Fast Comparing to Estimated', size=12, color='black')\nsns.barplot(x='diff_estimated_delivery', y='customer_state', data=states_estimated_delivery.tail(), ax=axs[2, 2], palette='viridis_r')\naxs[2, 2].set_title('Top 5 States where Delivery is \\nNot So Fast Comparing to Estimated', size=12, color='black')\nfor ax in axs[1, 2], axs[2, 2]:\n    ax.set_xlabel('Days Between Delivery and Estimated')\n    ax.set_xlim(states_estimated_delivery['diff_estimated_delivery'].min()-1, \n                states_estimated_delivery['diff_estimated_delivery'].max()+1)\n    format_spines(ax, right_border=False)\n    ax.set_ylabel('')\n\n# Annotations\naxs[0, 2].text(0.40, 0.30, f'{int(df_orders_filt.diff_estimated_delivery.mean())}', fontsize=45, ha='center')\naxs[0, 2].text(0.60, 0.30, 'working days', fontsize=12, ha='center')\naxs[0, 2].text(0.50, 0.12, 'is the average difference between', fontsize=12, ha='center')\naxs[0, 2].text(0.50, 0.00, 'delivery and estimated date', fontsize=12, ha='center')\naxs[0, 2].axis('off') \n    \nplt.suptitle('Comparative Study: E-Commerce on Brazilian States', size=16)\nplt.tight_layout()\nplt.show()","f3518c41":"# Grouping data\ndf_orders_pay = df_orders_filt.merge(olist_order_payments, how='left', on='order_id')\n\n# Creating figure\nfig = plt.figure(constrained_layout=True, figsize=(15, 12))\n\n# Axis definition\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[0, 1])\nax3 = fig.add_subplot(gs[1, :])\n\n# Plot 1 - Payment types in a donut chart\ncolors = ['darkslateblue', 'cornflowerblue', 'silver', 'darkviolet', 'crimson']\nlabel_names = df_orders_pay['payment_type'].value_counts().index\ndonut_plot(df_orders_pay, col='payment_type', ax=ax1, label_names=label_names, colors=colors,\n           title='Count of Transactions by Payment Type', text=f'{len(df_orders_pay)}\\npayments \\nregistered')\n\n# Plot 2 - Payment installments\nsingle_countplot(df_orders_pay, ax=ax2, y='payment_installments')\nax2.set_title('A Distribution of Payment Installments', color='dimgrey', size=12)\n\n# Plot 3 - Evolution of payment types\npayment_evl = df_orders_pay.groupby(by=['order_purchase_year_month', 'payment_type'], as_index=False).count()\npayment_evl = payment_evl.loc[:, ['order_purchase_year_month', 'payment_type', 'order_id']]\npayment_evl = payment_evl.sort_values(by=['order_purchase_year_month', 'order_id'], ascending=[True, False])\nsns.lineplot(x='order_purchase_year_month', y='order_id', data=payment_evl, ax=ax3, hue='payment_type',\n             style='payment_type', size='payment_type', palette=colors, marker='o')\nformat_spines(ax3, right_border=False)\nax3.set_title('Evolution of Payment Types in Brazilian E-Commerce', size=12, color='dimgrey')\nplt.show()","26d074e2":"df_comments = olist_order_reviews.loc[:, ['review_score', 'review_comment_message']]\ndf_comments = df_comments.dropna(subset=['review_comment_message'])\ndf_comments = df_comments.reset_index(drop=True)\nprint(f'Dataset shape: {df_comments.shape}')\ndf_comments.columns = ['score', 'comment']\ndf_comments.head()","67871e5f":"def find_patterns(re_pattern, text_list):\n    \"\"\"\n    Args:\n    ---------\n    re_pattern: regular expression pattern to be used on search [type: string]\n    text_list: list with text strings [type: list]\n    \n    Returns:\n    positions_dict: python dictionary with key-value pars as below:\n        text_idx: [(start_pattern1, end_pattern1), (start_pattern1, end_pattern2), ... (start_n, end_n)]\n    \"\"\"\n    \n    # Compiling the Regular Expression passed as a arg\n    p = re.compile(re_pattern)\n    positions_dict = {}\n    i = 0\n    for c in text_list:\n        match_list = []\n        iterator = p.finditer(c)\n        for match in iterator:\n            match_list.append(match.span())\n        control_key = f'Text idx {i}'\n        if len(match_list) == 0:\n            pass\n        else:\n            positions_dict[control_key] = match_list\n        i += 1\n        \n    \"\"\"p = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+'\n    pattern_dict = find_patterns(p, reviews_breakline)\n    print(len(pattern_dict))\n    pattern_dict\n    for idx in [int(c.split(' ')[-1]) for c in list(pattern_dict.keys())]:\n        print(f'{reviews_breakline[idx]}\\n')\"\"\"\n\n    return positions_dict\n\ndef print_step_result(text_list_before, text_list_after, idx_list):\n    \"\"\"\n    Args:\n    ----------\n    text_list_before: list object with text content before transformation [type: list]\n    text_list_after: list object with text content after transformation [type: list]\n    idx_list: list object with indexes to be printed [type: list]\n    \"\"\"\n    \n    # Iterating over string examples\n    i = 1\n    for idx in idx_list:\n        print(f'--- Text {i} ---\\n')\n        print(f'Before: \\n{text_list_before[idx]}\\n')\n        print(f'After: \\n{text_list_after[idx]}\\n')\n        i += 1","00cb3bd1":"def re_breakline(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_list: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    return [re.sub('[\\n\\r]', ' ', r) for r in text_list]","b8166bbb":"# Creating a list of comment reviews\nreviews = list(df_comments['comment'].values)\n\n# Applying RegEx\nreviews_breakline = re_breakline(reviews)\ndf_comments['re_breakline'] = reviews_breakline\n\n# Verifying results\nprint_step_result(reviews, reviews_breakline, idx_list=[48])","3490fa08":"def re_hiperlinks(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_list: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    pattern = 'http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    return [re.sub(pattern, ' link ', r) for r in text_list]","fd06c811":"# Applying RegEx\nreviews_hiperlinks = re_hiperlinks(reviews_breakline)\ndf_comments['re_hiperlinks'] = reviews_hiperlinks\n\n# Verifying results\nprint_step_result(reviews_breakline, reviews_hiperlinks, idx_list=[10796, 12782])","9d941856":"def re_dates(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_list: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    pattern = '([0-2][0-9]|(3)[0-1])(\\\/|\\.)(((0)[0-9])|((1)[0-2]))(\\\/|\\.)\\d{2,4}'\n    return [re.sub(pattern, ' data ', r) for r in text_list]","e9186bc9":"# Applying RegEx\nreviews_dates = re_dates(reviews_hiperlinks)\ndf_comments['re_dates'] = reviews_dates\n\n# Verifying results\nprint_step_result(reviews_hiperlinks, reviews_dates, idx_list=[26665, 41497, 41674])","f4883f6c":"def re_money(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_list: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    pattern = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+'\n    return [re.sub(pattern, ' dinheiro ', r) for r in text_list]","4b7d716b":"# Applying RegEx\nreviews_money = re_money(reviews_dates)\ndf_comments['re_money'] = reviews_money\n\n# Verifying results\nprint_step_result(reviews_dates, reviews_money, idx_list=[26020, 33297, 32998])","6586092d":"def re_numbers(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_series: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    return [re.sub('[0-9]+', ' numero ', r) for r in text_list]","bccc06e2":"# Applying RegEx\nreviews_numbers = re_numbers(reviews_money)\ndf_comments['re_numbers'] = reviews_numbers\n\n# Verifying results\nprint_step_result(reviews_money, reviews_numbers, idx_list=[68])","bdb27459":"def re_negation(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_series: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    return [re.sub('([nN][\u00e3\u00c3aA][oO]|[\u00f1\u00d1]| [nN] )', ' nega\u00e7\u00e3o ', r) for r in text_list]","b60c84e6":"# Applying RegEx\nreviews_negation = re_negation(reviews_numbers)\ndf_comments['re_negation'] = reviews_negation\n\n# Verifying results\nprint_step_result(reviews_numbers, reviews_negation, idx_list=[4783, 4627, 4856, 4904])","b1171fc4":"def re_special_chars(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_series: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    return [re.sub('\\W', ' ', r) for r in text_list]","6f95b914":"# Applying RegEx\nreviews_special_chars = re_special_chars(reviews_negation)\ndf_comments['re_special_chars'] = reviews_special_chars\n\n# Verifying results\nprint_step_result(reviews_negation, reviews_special_chars, idx_list=[45, 135, 234])","3c9d67d8":"def re_whitespaces(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_series: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    white_spaces = [re.sub('\\s+', ' ', r) for r in text_list]\n    white_spaces_end = [re.sub('[ \\t]+$', '', r) for r in white_spaces]\n    return white_spaces_end","4b21e5c5":"# Applying RegEx\nreviews_whitespaces = re_whitespaces(reviews_special_chars)\ndf_comments['re_whitespaces'] = reviews_whitespaces\n\n# Verifying results\nprint_step_result(reviews_special_chars, reviews_whitespaces, idx_list=[3, 4, -1])","e143040e":"# Examples of some portuguese stopwords\npt_stopwords = stopwords.words('portuguese')\nprint(f'Total portuguese stopwords in the nltk.corpous module: {len(pt_stopwords)}')\npt_stopwords[:10]","4d7cf2ea":"# Defining a function to remove the stopwords and to lower the comments\ndef stopwords_removal(text, cached_stopwords=stopwords.words('portuguese')):\n    \"\"\"\n    Args:\n    ----------\n    text: list object where the stopwords will be removed [type: list]\n    cached_stopwords: stopwords to be applied on the process [type: list, default: stopwords.words('portuguese')]\n    \"\"\"\n    \n    return [c.lower() for c in text.split() if c.lower() not in cached_stopwords]","baef25de":"# Removing stopwords and looking at some examples\nreviews_stopwords = [' '.join(stopwords_removal(review)) for review in reviews_whitespaces]\ndf_comments['stopwords_removed'] = reviews_stopwords\n\nprint_step_result(reviews_whitespaces, reviews_stopwords, idx_list=[0, 45, 500])","061cfb48":"# Defining a function to remove the stopwords and to lower the comments\ndef stemming_process(text, stemmer=RSLPStemmer()):\n    \"\"\"\n    Args:\n    ----------\n    text: list object where the stopwords will be removed [type: list]\n    stemmer: type of stemmer to be applied [type: class, default: RSLPStemmer()]\n    \"\"\"\n    \n    return [stemmer.stem(c) for c in text.split()]","f0e14b2e":"# Applying stemming and looking at some examples\nreviews_stemmer = [' '.join(stemming_process(review)) for review in reviews_stopwords]\ndf_comments['stemming'] = reviews_stemmer\n\nprint_step_result(reviews_stopwords, reviews_stemmer, idx_list=[0, 45, -1])","971ce391":"def extract_features_from_corpus(corpus, vectorizer, df=False):\n    \"\"\"\n    Args\n    ------------\n    text: text to be transformed into a document-term matrix [type: string]\n    vectorizer: engine to be used in the transformation [type: object]\n    \"\"\"\n    \n    # Extracting features\n    corpus_features = vectorizer.fit_transform(corpus).toarray()\n    features_names = vectorizer.get_feature_names()\n    \n    # Transforming into a dataframe to give interpetability to the process\n    df_corpus_features = None\n    if df:\n        df_corpus_features = pd.DataFrame(corpus_features, columns=features_names)\n    \n    return corpus_features, df_corpus_features","ca32939b":"# Creating an object for the CountVectorizer class\ncount_vectorizer = CountVectorizer(max_features=300, min_df=7, max_df=0.8, stop_words=pt_stopwords)\n\n# Extracting features for the corpus\ncountv_features, df_countv_features = extract_features_from_corpus(reviews_stemmer, count_vectorizer, df=True)\nprint(f'Shape of countv_features matrix: {countv_features.shape}\\n')\nprint(f'Example of DataFrame of corpus features:')\ndf_countv_features.head()","ebaf8fc7":"# Creating an object for the CountVectorizer class\ntfidf_vectorizer = TfidfVectorizer(max_features=300, min_df=7, max_df=0.8, stop_words=pt_stopwords)\n\n# Extracting features for the corpus\ntfidf_features, df_tfidf_features = extract_features_from_corpus(reviews_stemmer, tfidf_vectorizer, df=True)\nprint(f'Shape of tfidf_features matrix: {tfidf_features.shape}\\n')\nprint(f'Example of DataFrame of corpus features:')\ndf_tfidf_features.head()","4c0f17e6":"fig, ax = plt.subplots(figsize=(10, 5))\nsingle_countplot(x='score', df=df_comments, ax=ax)","1d312035":"# Labelling data\nscore_map = {\n    1: 'negative',\n    2: 'negative',\n    3: 'positive',\n    4: 'positive',\n    5: 'positive'\n}\ndf_comments['sentiment_label'] = df_comments['score'].map(score_map)\n\n# Verifying results\nfig, ax = plt.subplots(figsize=(7, 7))\ndonut_plot(df_comments.query('sentiment_label in (\"positive\", \"negative\")'), 'sentiment_label', \n           label_names=df_comments.query('sentiment_label in (\"positive\", \"negative\")')['sentiment_label'].value_counts().index,\n           ax=ax, colors=['darkslateblue', 'crimson'])","b4f10fdc":"def ngrams_count(corpus, ngram_range, n=-1, cached_stopwords=stopwords.words('portuguese')):\n    \"\"\"\n    Args\n    ----------\n    corpus: text to be analysed [type: pd.DataFrame]\n    ngram_range: type of n gram to be used on analysis [type: tuple]\n    n: top limit of ngrams to be shown [type: int, default: -1]\n    \"\"\"\n    \n    # Using CountVectorizer to build a bag of words using the given corpus\n    vectorizer = CountVectorizer(stop_words=cached_stopwords, ngram_range=ngram_range).fit(corpus)\n    bag_of_words = vectorizer.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    total_list = words_freq[:n]\n    \n    # Returning a DataFrame with the ngrams count\n    count_df = pd.DataFrame(total_list, columns=['ngram', 'count'])\n    return count_df","0e96a8cb":"# Splitting the corpus into positive and negative comments\npositive_comments = df_comments.query('sentiment_label == \"positive\"')['stemming']\nnegative_comments = df_comments.query('sentiment_label == \"negative\"')['stemming']\n\n# Extracting the top 10 unigrams by sentiment\nunigrams_pos = ngrams_count(positive_comments, (1, 1), 10)\nunigrams_neg = ngrams_count(negative_comments, (1, 1), 10)\n\n# Extracting the top 10 unigrams by sentiment\nbigrams_pos = ngrams_count(positive_comments, (2, 2), 10)\nbigrams_neg = ngrams_count(negative_comments, (2, 2), 10)\n\n# Extracting the top 10 unigrams by sentiment\ntrigrams_pos = ngrams_count(positive_comments, (3, 3), 10)\ntrigrams_neg = ngrams_count(negative_comments, (3, 3), 10)","4bc0f63c":"# Joining everything in a python dictionary to make the plots easier\nngram_dict_plot = {\n    'Top Unigrams on Positive Comments': unigrams_pos,\n    'Top Unigrams on Negative Comments': unigrams_neg,\n    'Top Bigrams on Positive Comments': bigrams_pos,\n    'Top Bigrams on Negative Comments': bigrams_neg,\n    'Top Trigrams on Positive Comments': trigrams_pos,\n    'Top Trigrams on Negative Comments': trigrams_neg,\n}\n\n# Plotting the ngrams analysis\nfig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 18))\ni, j = 0, 0\ncolors = ['Blues_d', 'Reds_d']\nfor title, ngram_data in ngram_dict_plot.items():\n    ax = axs[i, j]\n    sns.barplot(x='count', y='ngram', data=ngram_data, ax=ax, palette=colors[j])\n    \n    # Customizing plots\n    format_spines(ax, right_border=False)\n    ax.set_title(title, size=14)\n    ax.set_ylabel('')\n    ax.set_xlabel('')\n    \n    # Incrementing the index\n    j += 1\n    if j == 2:\n        j = 0\n        i += 1\nplt.tight_layout()\nplt.show()","210b72c0":"# Class for regular expressions application\nclass ApplyRegex(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, regex_transformers):\n        self.regex_transformers = regex_transformers\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # Applying all regex functions in the regex_transformers dictionary\n        for regex_name, regex_function in self.regex_transformers.items():\n            X = regex_function(X)\n            \n        return X\n\n# Class for stopwords removal from the corpus\nclass StopWordsRemoval(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, text_stopwords):\n        self.text_stopwords = text_stopwords\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return [' '.join(stopwords_removal(comment, self.text_stopwords)) for comment in X]\n\n# Class for apply the stemming process\nclass StemmingProcess(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, stemmer):\n        self.stemmer = stemmer\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return [' '.join(stemming_process(comment, self.stemmer)) for comment in X]\n    \n# Class for extracting features from corpus\nclass TextFeatureExtraction(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, vectorizer):\n        self.vectorizer = vectorizer\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return self.vectorizer.fit_transform(X).toarray()","13988300":"# Defining regex transformers to be applied\nregex_transformers = {\n    'break_line': re_breakline,\n    'hiperlinks': re_hiperlinks,\n    'dates': re_dates,\n    'money': re_money,\n    'numbers': re_numbers,\n    'negation': re_negation,\n    'special_chars': re_special_chars,\n    'whitespaces': re_whitespaces\n}\n\n# Defining the vectorizer to extract features from text\nvectorizer = TfidfVectorizer(max_features=300, min_df=7, max_df=0.8, stop_words=pt_stopwords)\n\n# Building the Pipeline\ntext_pipeline = Pipeline([\n    ('regex', ApplyRegex(regex_transformers)),\n    ('stopwords', StopWordsRemoval(stopwords.words('portuguese'))),\n    ('stemming', StemmingProcess(RSLPStemmer())),\n    ('text_features', TextFeatureExtraction(vectorizer))\n])","35919108":"# Defining X and y \nidx_reviews = olist_order_reviews['review_comment_message'].dropna().index\nscore = olist_order_reviews['review_score'][idx_reviews].map(score_map)\n\n# Splitting into train and test sets\nX = list(olist_order_reviews['review_comment_message'][idx_reviews].values)\ny = score.apply(lambda x: 1 if x == 'positive' else 0).values\n\n# Applying the pipeline and splitting the data\nX_processed = text_pipeline.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=.20, random_state=42)\n\n# Verifying results\nprint(f'Length of X_train_processed: {len(X_train)} - Length of one element: {len(X_train[0])}')\nprint(f'Length of X_test_processed: {len(X_test)} - Length of one element: {len(X_test[0])}')","1f34a447":"# Logistic Regression hyperparameters\nlogreg_param_grid = {\n    'C': np.linspace(0.1, 10, 20),\n    'penalty': ['l1', 'l2'],\n    'class_weight': ['balanced', None],\n    'random_state': [42],\n    'solver': ['liblinear']\n}\n\n# Setting up the classifiers\nset_classifiers = {\n    'LogisticRegression': {\n        'model': LogisticRegression(),\n        'params': logreg_param_grid\n    },\n    'Naive Bayes': {\n        'model': GaussianNB(),\n        'params': {}\n    }\n}","98d469d4":"# Creating an object and training the classifiers\nclf_tool = BinaryClassifiersAnalysis()\nclf_tool.fit(set_classifiers, X_train, y_train, random_search=True, scoring='accuracy')","34bd6782":"# Evaluating metrics\ndf_performances = clf_tool.evaluate_performance(X_train, y_train, X_test, y_test, cv=5)\ndf_performances.reset_index(drop=True).style.background_gradient(cmap='Blues')","59462d3d":"clf_tool.plot_confusion_matrix(classes=['Negative', 'Positive'])","3c5e66d6":"# Defining a function to plot the sentiment of a given phrase\ndef sentiment_analysis(text, pipeline, vectorizer, model):\n    \"\"\"\n    Args\n    -----------\n    text: text string \/ phrase \/ review comment to be analysed [type: string]\n    pipeline: text prep pipeline built for preparing the corpus [type: sklearn.Pipeline]\n    model: classification model trained to recognize positive and negative sentiment [type: model]\n    \"\"\"\n    \n    # Applying the pipeline\n    if type(text) is not list:\n        text = [text]\n    text_prep = pipeline.fit_transform(text)\n    matrix = vectorizer.transform(text_prep)\n    \n    # Predicting sentiment\n    pred = model.predict(matrix)\n    proba = model.predict_proba(matrix)\n    \n    # Plotting the sentiment and its score\n    fig, ax = plt.subplots(figsize=(5, 3))\n    if pred[0] == 1:\n        text = 'Positive'\n        class_proba = 100 * round(proba[0][1], 2)\n        color = 'seagreen'\n    else:\n        text = 'Negative'\n        class_proba = 100 * round(proba[0][0], 2)\n        color = 'crimson'\n    ax.text(0.5, 0.5, text, fontsize=50, ha='center', color=color)\n    ax.text(0.5, 0.20, str(class_proba) + '%', fontsize=14, ha='center')\n    ax.axis('off')\n    ax.set_title('Sentiment Analysis', fontsize=14)\n    plt.show()","e3db2d2d":"# Defining transformers for preparing the text input\nmodel = clf_tool.classifiers_info['LogisticRegression']['estimator']\nprod_pipeline = Pipeline([\n    ('regex', ApplyRegex(regex_transformers)),\n    ('stopwords', StopWordsRemoval(stopwords.words('portuguese'))),\n    ('stemming', StemmingProcess(RSLPStemmer()))\n])\nvectorizer = text_pipeline.named_steps['text_features'].vectorizer","0aed5c3e":"comment = 'P\u00e9ssimo produto! N\u00e3o compro nessa loja, a entrega atrasou e custou muito dinheiro!'\nsentiment_analysis(comment, pipeline=prod_pipeline, vectorizer=vectorizer, model=model)","f8b6a253":"comment = 'Adorei e realmente cumpriu as expectativas. Comprei por um valor barato. Maravilhoso'\nsentiment_analysis(comment, pipeline=prod_pipeline, vectorizer=vectorizer, model=model)","b79d6931":"comment = 'N\u00e3o sei gostei do produto. O custo foi barato mas veio com defeito. Se der sorte, vale a pena'\nsentiment_analysis(comment, pipeline=prod_pipeline, vectorizer=vectorizer, model=model)","a7d88ff2":"# Reading and preparing a mask for serving as wordcloud background\nlike_mask = np.array(Image.open(\"..\/input\/imgicons\/like.png\"))\nbomb_mask = np.array(Image.open(\"..\/input\/imgicons\/bomb3.png\"))\n#angry_mask = angry_mask[:, :, -1]\n\n# Transforming like mask\ntransf_like_mask = np.ndarray((like_mask.shape[0], like_mask.shape[1]), np.int32)\nfor i in range(len(like_mask)):\n    transf_like_mask[i] = [255 if px == 0 else 0 for px in like_mask[i]]\n\n# Transforming angry mask\ntransf_bomb_mask = np.ndarray((bomb_mask.shape[0], bomb_mask.shape[1]), np.int32)\nfor i in range(len(bomb_mask)):\n    transf_bomb_mask[i] = [255 if px == 0 else 0 for px in bomb_mask[i]]\n    \n# Generating words\npos_comments = list(df_comments.query('sentiment_label == \"positive\"')['stopwords_removed'].values)\npositive_words = ' '.join(pos_comments).split(' ')\nneg_comments = list(df_comments.query('sentiment_label == \"negative\"')['stopwords_removed'].values)\nnegative_words = ' '.join(neg_comments).split(' ')\n\n# Using Counter for creating a dictionary counting\npositive_dict = Counter(positive_words)\nnegative_dict = Counter(negative_words)\n\n# Generating wordclouds for both positive and negative comments\npositive_wc = WordCloud(width=1280, height=720, collocations=False, random_state=42, mask=transf_like_mask,\n                      colormap='Blues', background_color='white', max_words=50).generate_from_frequencies(positive_dict)\nnegative_wc = WordCloud(width=1280, height=720, collocations=False, random_state=42, mask=transf_bomb_mask,\n                      colormap='Reds', background_color='white', max_words=50).generate_from_frequencies(negative_dict)\n\n# Visualizing the WC created and the total for each cuisine\nfig, axs = plt.subplots(1, 2, figsize=(20, 20))\nax1 = axs[0]\nax2 = axs[1]\n\nax1.imshow(positive_wc)\nax1.axis('off')\nax1.set_title('WordCloud for Positive Words in Reviews', size=18, pad=20)\n\nax2.imshow(negative_wc)\nax2.axis('off')\nax2.set_title('WordCloud for Negative Words in Reviews', size=18, pad=20)\n\nplt.show()","4ebe9212":"\"\"\"\nThis python script will allocate all the custom transformers that are specific for the project task.\nThe idea is to encapsulate the classes and functions used on pipelines to make codes cleaner.\n\"\"\"\n\n# Importing libraries\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\n\"\"\"\n-----------------------------------\n----- 1. CUSTOM TRANSFORMERS ------\n           1.1 Classes\n-----------------------------------\n\"\"\"\n\n\nclass ColumnMapping(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class applies the map() function into a DataFrame for transforming a columns given a mapping dictionary\n\n    Parameters\n    ----------\n    :param old_col_name: name of the columns where mapping will be applied [type: string]\n    :param mapping_dict: python dictionary with key\/value mapping [type: dict]\n    :param new_col_name: name of the new column resulted by mapping [type: string, default: 'target]\n    :param drop: flag that guides the dropping of the old_target_name column [type: bool, default: True]\n\n    Returns\n    -------\n    :return X: pandas DataFrame object after mapping application [type: pd.DataFrame]\n\n    Application\n    -----------\n    # Transforming a DataFrame column given a mapping dictionary\n    mapper = ColumnMapping(old_col_name='col_1', mapping_dict=dictionary, new_col_name='col_2', drop=True)\n    df_mapped = mapper.fit_transform(df)\n    \"\"\"\n\n    def __init__(self, old_col_name, mapping_dict, new_col_name='target', drop=True):\n        self.old_col_name = old_col_name\n        self.mapping_dict = mapping_dict\n        self.new_col_name = new_col_name\n        self.drop = drop\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # Applying mapping\n        X[self.new_col_name] = X[self.old_col_name].map(self.mapping_dict)\n\n        # Dropping the old columns (if applicable)\n        if self.drop:\n            X.drop(self.old_col_name, axis=1, inplace=True)\n\n        return X","f0c9319b":"\"\"\"\nThis python script are responsible for reading, preparing and training a sentiment classification model from\ne-commerce reviews taken from brazilian web-sites\n\n* Metadata can be find at: https:\/\/www.kaggle.com\/olistbr\/brazilian-ecommerce\n* Reference notebook: ..\/notebooks\/EDA_BrazilianECommerce.ipynb\n\n--- SUMMARY ---\n\n1. Project Variables\n2. Reading Data\n3. Prep Pipelines\n    3.1 Initial Preparation\n    3.2 Text Transformers\n4. Modeling\n    4.1 Model Training\n    4.2 Evaluating Metrics\n    4.3 Complete Solution\n    4.4 Final Model Performance\n    4.5 Saving pkl Files\n\n---------------------------------------------------------------\nWritten by Thiago Panini - Latest version: September 24th 2020\n---------------------------------------------------------------\n\"\"\"\n\n\n# Importing libs\nimport os\nimport numpy as np\nimport pandas as pd\n#from dev.training.project_transformers import ColumnMapping\nfrom custom_transformers import import_data, DropNullData, DropDuplicates\nfrom text_utils import re_breakline, re_dates, re_hiperlinks, re_money, re_negation, re_numbers, \\\n    re_special_chars, re_whitespaces, ApplyRegex, StemmingProcess, StopWordsRemoval\nfrom nltk.corpus import stopwords\nfrom nltk.stem import RSLPStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom joblib import dump\nfrom sklearn.linear_model import LogisticRegression\nfrom ml_utils import BinaryClassifiersAnalysis, cross_val_performance\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve\n\n\n\"\"\"\n-----------------------------------\n------ 1. PROJECT VARIABLES -------\n-----------------------------------\n\"\"\"\n\n# Variables for address paths\nDATA_PATH = '..\/input\/brazilian-ecommerce'\nPIPELINES_PATH = '..\/..\/pipelines' # Take a look at your project structure\nMODELS_PATH = '..\/..\/models' # Take a look at your project structure\n\n# Variables for reading the data\nFILENAME = 'olist_order_reviews_dataset.csv'\nCOLS_READ = ['review_comment_message', 'review_score']\nCORPUS_COL = 'review_comment_message'\nTARGET_COL = 'target'\n\n# Defining stopwords\nPT_STOPWORDS = stopwords.words('portuguese')\n\n# Variables for saving data\nMETRICS_FILEPATH = 'metrics\/model_performance.csv' # Take a look at your project structure\n\n# Variables for retrieving model\nMODEL_KEY = 'LogisticRegression'\n\n\n\"\"\"\n-----------------------------------\n-------- 2. READING DATA ----------\n-----------------------------------\n\"\"\"\n\n# Reading the data with text corpus and score\ndf = import_data(os.path.join(DATA_PATH, FILENAME), usecols=COLS_READ)\n\n\n\"\"\"\n-----------------------------------\n------- 3. PREP PIPELINES ---------\n    3.1 Initial Preparation\n-----------------------------------\n\"\"\"\n\n# Creating a dictionary for mapping the target column based on review score\nscore_map = {\n    1: 0,\n    2: 0,\n    3: 0,\n    4: 1,\n    5: 1\n}\n\n# Creating a pipeline for the initial prep on the data\ninitial_prep_pipeline = Pipeline([\n    ('mapper', ColumnMapping(old_col_name='review_score', mapping_dict=score_map, new_col_name=TARGET_COL)),\n    ('null_dropper', DropNullData()),\n    ('dup_dropper', DropDuplicates())\n])\n\n# Applying initial prep pipeline\ndf_prep = initial_prep_pipeline.fit_transform(df)\n\n\n\"\"\"\n-----------------------------------\n------- 3. PREP PIPELINES ---------\n      3.2 Text Transformers\n-----------------------------------\n\"\"\"\n\n# Defining regex transformers to be applied\nregex_transformers = {\n    'break_line': re_breakline,\n    'hiperlinks': re_hiperlinks,\n    'dates': re_dates,\n    'money': re_money,\n    'numbers': re_numbers,\n    'negation': re_negation,\n    'special_chars': re_special_chars,\n    'whitespaces': re_whitespaces\n}\n\n# Building a text prep pipeline\ntext_prep_pipeline = Pipeline([\n    ('regex', ApplyRegex(regex_transformers)),\n    ('stopwords', StopWordsRemoval(PT_STOPWORDS)),\n    ('stemming', StemmingProcess(RSLPStemmer())),\n    ('vectorizer', TfidfVectorizer(max_features=300, min_df=7, max_df=0.8, stop_words=PT_STOPWORDS))\n])\n\n# Applying the pipeline\nX = df_prep[CORPUS_COL].tolist()\ny = df_prep[TARGET_COL]\nX_prep = text_prep_pipeline.fit_transform(X)\n\n# Splitting the data into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X_prep, y, test_size=.20, random_state=42)\n\n# Saving states before prep pipeline\n\"\"\"df_prep[CORPUS_COL].to_csv(os.path.join(DATA_PATH, 'X_data.csv'), index=False)\ndf_prep[TARGET_COL].to_csv(os.path.join(DATA_PATH, 'y_data.csv'), index=False)\"\"\"\n\n\n\"\"\"\n-----------------------------------\n--------- 4. MODELING  -----------\n       4.1 Model Training\n-----------------------------------\n\"\"\"\n\n# Specifing a Logistic Regression model for sentiment classification\nlogreg_param_grid = {\n    'C': np.linspace(0.1, 10, 20),\n    'penalty': ['l1', 'l2'],\n    'class_weight': ['balanced', None],\n    'random_state': [42],\n    'solver': ['liblinear']\n}\n\n# Setting up the classifiers\nset_classifiers = {\n    'LogisticRegression': {\n        'model': LogisticRegression(),\n        'params': logreg_param_grid\n    }\n}\n\n# Creating an object and training the classifiers\ntrainer = BinaryClassifiersAnalysis()\ntrainer.fit(set_classifiers, X_train, y_train, random_search=True, scoring='accuracy')\n\n\n\"\"\"\n-----------------------------------\n--------- 4. MODELING  -----------\n    4.2 Evaluating Metrics\n-----------------------------------\n\"\"\"\n\n# Evaluating metrics\nperformance = trainer.evaluate_performance(X_train, y_train, X_test, y_test, cv=5, save=False,\n                                           performances_filepath=METRICS_FILEPATH) # In your project env, save=True and overwrite=True may be useful\n\n\n\"\"\"\n-----------------------------------\n--------- 4. MODELING  -----------\n    4.3. Complete Solution\n-----------------------------------\n\"\"\"\n\n# Returning the model to be saved\nmodel = trainer.classifiers_info[MODEL_KEY]['estimator']\n\n# Creating a complete pipeline for prep and predict\ne2e_pipeline = Pipeline([\n    ('text_prep', text_prep_pipeline),\n    ('model', model)\n])\n\n# Defining a param grid for searching best pipelines options (reduced options for making the search faster)\n\"\"\"param_grid = [{\n    'text_prep__vectorizer__max_features': np.arange(500, 851, 50),\n    'text_prep__vectorizer__min_df': [7, 9, 12, 15, 30],\n    'text_prep__vectorizer__max_df': [.4, .5, .6, .7]\n}]\"\"\"\n\nparam_grid = [{\n    'text_prep__vectorizer__max_features': np.arange(500, 501, 50),\n    'text_prep__vectorizer__min_df': [7],\n    'text_prep__vectorizer__max_df': [.4]\n}]\n\n# Searching for best options\ngrid_search_prep = GridSearchCV(e2e_pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\ngrid_search_prep.fit(X, y)\nprint('Best params after a complete search:')\nprint(grid_search_prep.best_params_)\n\n# Returning the best options\nvectorizer_max_features = grid_search_prep.best_params_['text_prep__vectorizer__max_features']\nvectorizer_min_df = grid_search_prep.best_params_['text_prep__vectorizer__min_df']\nvectorizer_max_df = grid_search_prep.best_params_['text_prep__vectorizer__max_df']\n\n# Updating the e2e pipeline with the best options found on search\ne2e_pipeline.named_steps['text_prep'].named_steps['vectorizer'].max_features = vectorizer_max_features\ne2e_pipeline.named_steps['text_prep'].named_steps['vectorizer'].min_df = vectorizer_min_df\ne2e_pipeline.named_steps['text_prep'].named_steps['vectorizer'].max_df = vectorizer_max_df\n\n# Fitting the model again\ne2e_pipeline.fit(X, y)\n\n\n\"\"\"\n-----------------------------------\n--------- 4. MODELING  -----------\n    4.4 Final Model Performance\n-----------------------------------\n\"\"\"\n\n# Retrieving performance for te final model after hyperparam updating\nfinal_model = e2e_pipeline.named_steps['model']\nfinal_performance = cross_val_performance(final_model, X_prep, y, cv=5)\nfinal_performance = final_performance.append(performance)\nprint(final_performance)\n#final_performance.to_csv(METRICS_FILEPATH, index=False)\n\n\n\"\"\"\n-----------------------------------\n--------- 4. MODELING  -----------\n      4.5 Saving pkl files\n-----------------------------------\n\"\"\"\n\n\"\"\"# Creating folders for saving pkl files (if not exists)\nif not os.path.exists('..\/..\/models'):\n    os.makedirs('..\/..\/models')\nif not os.path.exists('..\/..\/pipelines'):\n    os.makedirs('..\/..\/pipelines')\n\n# Saving pkl files\ndump(initial_prep_pipeline, os.path.join(PIPELINES_PATH, 'initial_prep_pipeline.pkl'))\ndump(text_prep_pipeline, os.path.join(PIPELINES_PATH, 'text_prep_pipeline.pkl'))\ndump(e2e_pipeline, os.path.join(PIPELINES_PATH, 'e2e_pipeline.pkl'))\ndump(final_model, os.path.join(MODELS_PATH, 'sentiment_clf_model.pkl'))\"\"\"","139351cd":"___\n* _Confusion Matrix_\n___","1fa8f227":"<a id=\"4.5\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.5 Feature Extraction<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","7b8e7c88":"By the chart above we can conclude:\n\n* E-commerce on Brazil really has a growing trend along the time. We can see some seasonality with peaks at specific months, but in general we can see clear that customers are more prone to buy things online than before.\n* Monday are the prefered day for brazilian's customers and they tend to buy more at afternoons.\n\n_Obs: we have a sharp decrease between August 2018 and September 2018 and maybe the origin of that is related to noise on data. For further comparison between 2017 and 2018, let's just consider orders between January and August in both years_","c422f20d":"<a id=\"2.1\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>2.1 An Overview from the Data<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","68fff9f4":"Well, by now we have a text dataset without any pattern that we threated with RegEx and also without punctuations. In other words, we have a half-clean text with a rich transformation applied. \n\nSo, we are ready to apply some advanced text transformations like `stopwords` removal, `stemming` and the `TF-IDF` matrix process. Let's start with portuguese stopwords.","33348759":"With the _Bag of Words_ approach, each words has the same weight, wich maybe can't be true all the time, mainly for those words with a really low frequency on the corpus. So, the _TF-IDF (Term Frequency and Inverse Document Frequency)_ approach can be used with the scikit-learn library following the formulas:\n\n$$TF=\\frac{\\text{Frequency of a word in the document}}{\\text{Total words in the document}}$$","e5df6e3f":"At least, let's try to simulate a very neutral comment. Something like `I don't know if a liked this product. The cost was cheap but it was defectuous. If you're lucky, it worths`","adfe8dd7":"Here we will try to find numbers on reviews and replace them with another string `numero` (that means `number`, in english). We could just replace the numbers with whitespace but maybe this would generated some information loss. Let's see what we've got:","56eff847":"___\n* _E-commerce: a comparison between 2017 and 2018_\n___","89d1da38":"<a id=\"5\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>5. Sentiment Classification<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","53868cab":"<a id=\"4.1\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.1 Data Understanding<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","14181cde":"<a id=\"4.2.2\"><\/a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.2 Sites and Hiperlinks<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","14b1814b":"So now we will go trough an exploratory data analysis to get insights from E-Commerce in Brazil. The aim here is to divide this session into topics so we can explore graphics for each subject (orders, customers, products, items, and others).","2e5a4edc":"<a id=\"4.6\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.6 Labelling Data<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","7897a82f":"<a id=\"4.2.6\"><\/a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.6 Negation<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","7daaaa7e":"<a id=\"4.3\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.3 Stopwords<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","7c7e62e1":"The objective of this notebook is to propose an analytical view of e-commerce relationship in Brazil. For this we will first go trough an exploratory data analysis using graphical tools to create self explanatory plots for better understanding what is behind braziian online purchasing. Finally we will look at customers reviews and implement **_Sentimental Analysis_** to make a text classification using **_Natural Language Process_** tools.\n\nWe will go trough a extensive journey for understanding the data and plotting some useful charts to clarify the concepts and get insights from data and, at the end, we will go trough a step-by-step code on text preparating and sentiment classification using the reviews left from customer on online platforms. I hope you enjoy this notebook!\n\nBefore getting to work, I want to thanks [Raeshid David](https:\/\/www.kaggle.com\/raenish) for sharing his content and inspiring the style and organization of this notebook.","be9c9054":"So, we have in hands approximately 41k comments that could be used for training a sentimental analysis model. But, for this to becoming true, we have to go trough a long way of text preparation to transform the comment input into a vector that can be interpreted for a Machine Learning model. **Let's go ahead**","02608393":"<a id=\"3\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3. Exploratory Data Analysis<\/b><\/font>","9bbc200a":"___\n* _Project Transformers_\n___","dada2f45":"<a id=\"4\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>4. Natural Language Processing<\/b><\/font>","2a1ccb92":"For preparing the data to a workaround analysis on brazilian's states e-commerce, we will take the following steps:\n\n    1. Merge the orders data to order_items data;\n    2. Use an API (brazilian government) to return the region of each customer_state;\n    3. Purpose useful charts to answear business questions.","25ab4a12":"**Note: With this, we close our session for EDA and now we can start the Natural Language Process step on reviews! Keep in touch for more!**","ed3f3e18":"___\n* _How many orders we have for each status?_\n___","04144d18":"Well, we can now use the DataFrame above wherever we want to do dome data transformation or data analysis. It contains basically the main information about each column for each one of the datasets available. This is very useful!","c749cdae":"For answear this question, let's first group our data in a way to look at the evolution overall.","a14a6437":"Before creating a unique dataset with all useful information, let's look at the shape of each dataset, so we can be more assertive on how to use joining statements. ","5ed8006c":"___\n__*Important Update:*__ I'm proud to see that the job here grew up so far spreading insights among a lot of Kagglers. So, I want to share with you the development of a personal project called \"Sentimentor API\". The idea is to train a sentiment classification model to be put into a front-end interface so users can input their own data (text strings ou excel\/csv files) and take a look at the sentiment prediction given by this model. \n\nFor now, the idea is literally at the very beginning and you can take a look on my [github](https:\/\/github.com\/ThiagoPanini\/sentiment-analysis-ecommerce).\n___","f764eb21":"Well, we went trought a lot of steps together and this is the final one! After all the text preparation we've done, it's now time to put it together into a classification model to train an algorithm that understands wherever a text string has a `positive` or a `negative` feeling based on the features we extracted from the corpus.\n\nSo, let's use a homemade class for make the training and analysis easier.","1b1611ed":"<a id=\"3.1\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>3.1 Total Orders on E-Commerce<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","2b2e5734":"Now, by the end of our detailed implementation, I want to share with you how this project could be implemented as complete structure project in production. The script above is exactly the same the one I built in my machine to prepare myself for a real problem.","94b912d7":"<a id=\"4.2.7\"><\/a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.7 Special Characters<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","527ec73d":"Thanks to Andre Sionek that threated the geolocation lat and long on his kernel!","6f6b7760":"I want to give credits to Raenish David notebook [Tweet Sentiment - Insight EDA](https:\/\/www.kaggle.com\/raenish\/tweet-sentiment-insight-eda) that inspired this analysis.","1aaa0866":"<a id=\"7\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>7. Conclusion<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","646174fd":"___\n* _What's the main n-grams presentes in corpus on positive and negative classes?_\n___","19f4801e":"___\n* _Training selected Machine Learning models_\n___","ad5de319":"<a id=\"4.7\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.7 Pipeline<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","61ce65a0":"**Nice!** Another good view is to use the folium plugin _[HeatMapWithTime](https:\/\/nbviewer.jupyter.org\/github\/python-visualization\/folium\/blob\/master\/examples\/HeatMapWithTime.ipynb)_ to see the evolution of e-commerce orders among time.\n\nFor [limitations](https:\/\/github.com\/python-visualization\/folium\/issues\/859) purpose (i.e. jupyter and Chrome limitations for total number of points shown at HeatMapWithTime, we will show the evolution of orders from January 2018 to July 2018)\n\nAlso, it's possible that the plugin HeatMapWithTime doesn't work properly from a [issue](https:\/\/github.com\/python-visualization\/folium\/issues\/1221) fixed on version 0.11 (it's seems that the version of the kernel is 0.10). It it is the case for you, just updating the version of folium library would fix it.","ee18085e":"Another pattern that probably is very common on this kind of source is representations of money (R$ \\__,\\__). To improve our model, maybe it's a good idea to transform this pattern into a key word like `valor` (means `money` or `amount` in english).","c4f4b13a":"Well, as long as we are dealing with customers reviews on items bought online, probably date mentions are very common. Let's see some examples and apply a RegEx to change this to `data` (means `date` in english).","d5b4ad4c":"As long as we could improve our relationship with the data, the path is open to start the Natural Language Processing step to analyze the comments left on e-commerce orders. The goal is to use this as input to a `sentimental analysis` model for understanding the customer's sentiment on purchasing things online. Let's take a look on the reviews data.","c8afc3b1":"<a id=\"2\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>2. Reading the Data<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","411cd1c1":"___\n* _Complete Python Script for Training a Sentiment Classififier_\n___","52fabad8":"Excelent! Our model returned exactly we expected. Now, let' simulate a comment that says something like `I love it and it really fulfilled the expectations. I bought for a cheap value. Wonderful`.","11cdbbbc":"___\n* _Is that a growing trend on e-commerce in Brazil? How can we describe a complete scenario?_\n___","8ff65645":"For training a sentimental analysis model, we must need the label to apply in a supervisioned Machine Learning approach. The dataset we doesn't have a clearly label saying wich comment is positive or negative. For doing that, probably the best approach is to look at individual comments and label it handly with 1 (positive comment) and 0 (negative comment) but, thinking in a fast implementation, we will use the `review_score` column to label our data into those two classes. Let's take a look.","b1850bee":"___\n* _How customers are distributed in Brazil? (a 30k orders sample from 2018 in a map)_\n___","c1db5569":"How about the freight?","ae42c861":"The code below is end-to-end solution for creating a sentiment classification model using the olist e-commerce reviews dataset. As I said before, this script was built on my personal machine, so I commented the lines of code that saves something on the fileserver.","6813763b":"Just to remember, in the pipeline above we chose the TF-IDF approach to extract features from text using the same parameters we used on the examples (`max_features=300`, `min_df=7`, `max_df=0.8`). It means that every text string in our corpus will be 300 \"text features\" respecting the criteria defined by the min_df and max_df parameters. Let's apply it.","406b40ff":"<a id=\"4.5.2\"><\/a>\n<font color=\"dimgrey\" size=+1.5><b>4.5.2 TF-IDF<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","5e404c7f":"___\n* _**HeatMaps:** a good view to see where are the core of brazilian e-commerce customers_\n___","f47a8582":"___\n* _How about the e-commerce sales? Did they grow up over time?_\n___","28af8ad0":"In fact, we can see by the line chart that payments made by credit card really took marjority place on brazilian e-commerce. Besides that, since 201803 it's possible to see a little decrease on this type of payment. By the other side, payments made by debit card is showing a growing trend since 201805, wich is a good opportunity for investor to improve services for payments like this.\n\nOn the bar chart above, we can see how brazilian customers prefer to pay the orders: mostly of them pay once into 1 installment and it's worth to point out the quantity of payments done by 10 installments.","0b9f3dda":"For the next plots, let's dive into the real evolution of e-commerce in terms of purchase orders. For this, we have to extract some info on the `order_purchase_timestamp` following the topics:\n\n    1. Transform timestamp columns;\n    2. Extract time attributes from these datetime columns (year, month, day, day of week and hour);\n    3. Evaluate the e-commerce scenario using this attributes.","75576b1e":"So now we can purpose a complete analysis on orders amount of brazilian e-commerce during the period of the dataset. For that let's plot three graphs using a `GridSpec` with the aim answear the following questions:\n\n    1. Is there any growing trend on brazilian e-commerce?\n    2. On what day of week brazilians customers tend to do online purchasing?\n    3. What time brazilians customers tend do buy (Dawn, Morning, Afternoon or Night)?","022629e1":"<a id=\"3.4\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>3.4 Payment Type Analysis<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","44e750ce":"Finally we can build up our final step to delivery a Sentiment Analysis model! We have a full prep pipeline, a machine learning model (to choose) and now the only thing we can do to improve it is to build a connected solution that can receive a input text string (say an e-commerce comment) and return its sentiment. Let's try!","e990ef49":"<a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Content<\/h3>\n\n* [1. Libraries](#1)\n* [2. Reading the Data](#2)\n    - [1.1 An Overview from the Data](#2.1)\n* [3. Exploratory Data Analysis](#3) \n    - [3.1 Total Orders on E-Commerce](#3.1)\n    - [3.2 E-Commerce Around Brazil](#3.2)\n    - [3.3 E-Commerce Impact on Economy](#3.3)\n    - [3.4 Payment Type Analysis](#3.4)\n* [4. Natural Language Processing](#4)\n    - [4.1 Data Understanding](#4.1)\n    - [4.2 Regular Expressions](#4.2)\n        - [4.2.1 Break Line and Carriage Return](#4.2.1)\n        - [4.2.2 Sites and Hiperlinks](#4.2.2)\n        - [4.2.3 Dates](#4.2.3)\n        - [4.2.4 Money](#4.2.4)\n        - [4.2.5 Numbers](#4.2.5)\n        - [4.2.6 Negation](#4.2.6)\n        - [4.2.7 Special Characteres](#4.2.7)\n        - [4.2.8 Additional Whitespaces](#4.2.8)\n    - [4.3 Stopwords](#4.3)\n    - [4.4 Stemming](#4.4)\n    - [4.5 Feature Extraction](#4.5)\n        - [4.5.1 CountVectorizer](#4.5.1)\n        - [4.5.2 TF-IDF](#4.5.2)\n    - [4.6 Labeling Data](#4.6)\n    - [4.7 Pipeline](#4.7)\n* [5. Sentiment Classification](#5)\n* [6. Final Implementation](#6)\n* [7. Conclusion](#7)\n* [8. Complete Script](#8)","f56b8995":"___\n* _How the total sales (sum of price) are concentraded in brazilian states?_\n___","7f82a0d4":"<a id=\"6\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>6. Final Implementation<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","f1078c2c":"<font size=\"+1\" color=\"black\"><b>Please visit my other kernels by clicking on the buttons<\/b><\/font><br>\n\n<a href=\"https:\/\/www.kaggle.com\/thiagopanini\/predicting-restaurant-s-rate-in-bengaluru\" class=\"btn btn-primary\" style=\"color:white;\">Bengaluru's Restaurants<\/a>\n<a href=\"https:\/\/www.kaggle.com\/thiagopanini\/predicting-credit-risk-eda-viz-pipeline#Training-and-Evaluating-a-Model\" class=\"btn btn-primary\" style=\"color:white;\">Credit Risk Detection<\/a>\n<a href=\"https:\/\/www.kaggle.com\/thiagopanini\/credit-fraud-how-to-choose-the-best-classifier\" class=\"btn btn-primary\" style=\"color:white;\">Credit Card Fraud Detection<\/a>\n<a href=\"https:\/\/www.kaggle.com\/thiagopanini\/global-terrorism-eda-nlp\" class=\"btn btn-primary\" style=\"color:white;\">Global Terrorism<\/a>","2930c2d6":"<a id=\"1\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>1. Libraries<\/b><\/font>","d4e40a91":"By the end, let's plot a WordCloud for positive and negative words on our dataset.","c7993b33":"We know that e-commerce is really a growing trend in a global perspective. Let's dive into the orders dataset to see how this trend can be presented in Brazil, at least on the dataset range.\n\nLooking at the dataset columns, we can see orders with different `status` and with different timestamp columns like `purchase`, `approved`, `delivered` and `estimated delivery`. First, let's look at the status of the orders we have in this dataset.","14a2e3c4":"<a id=\"4.2.1\"><\/a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.1 Breakline and Carriage Return<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","5af5e93f":"Well, it was a long journey and I hope you all had experienced a really explained and useful notebook for a Sentimental Analysis task. We could detail step by step on how to input a dataset with comment reviews of online purchasing operations and extract the sentiment of people that left their reviews. We now are able to expand this to a high level application that automatic detects the sentiment of a given text or phrase.\n\n","7b2ae506":"This session was thought and discussed in a special way. The problem statement is that when we remove the stopwords, probabily we would loose the meaning of some phrases about removing the negation words like `n\u00e3o` (not), for example. So, because of this, maybe is a good idea to replace some negation words with some common words indicating a negation meaning.","c1c1ffbb":"[](http:\/\/)<a id=\"8\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>8. Complete Script<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","4d4b8b40":"The barcharts above are interesting and they really reflect the sentiment based on ngrams. We can clearly see negative words on bigrams and trigrams by the right side of the figure (the most frequent trigram `neg receb produt` in english maybe means something like `didn't receive the product` for example).\n\nThe positive bigrams and trigrams at the blue left side of the figure really consists of positive words (the most frequent trigram `entreg ant praz` means something like `delivery before time` in english). Also!","2ddac18d":"It's very interesting to see how some states have a high total amount sold and a low price per order. If we look at SP (S\u00e3o Paulo) for example, it's possible to see that it is the state with most valuable state for e-commerce (5,188,099 sold) but it is also where customers pay less per order (110.00 per order).","d265a069":"In this approach, let's consider that every comment with scores 1, 2 and 3 are negative comments. In the other hand, comments with score 4 and 5 will be considered as positive. Again, probably this is not the best way to train a sentimental analysis model, but for fastness, we will do this assumption and see if we can extract value from it.","b9baa8dc":"On the _Bag of Words_ approach, we create a dicitonary vocabulary with all the unique words and, for each word in each comment\/text string, we index the words into a vector that represents the occurence (1) or not (0) of each word. This is a way for transforming a text into a frequency vector considering a literal bag of words (dictionary vocabulary).","b103752c":"<a id=\"4.2.8\"><\/a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.8 Additional Whitespaces<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","56c1e3b3":"___\n* _An overview of customer's order by region, state and city_\n___","63607b96":"Here it's possible to see the tags \\r (_carriage return_ code ASCII 10) and \\n (_new line_ code ASCII 13). With RegEx, we could get rid of those patterns.","bbae5960":"___\n* _Evaluating models_\n___","5ab40680":"Well, once we have passe through RegEx, stopwords removal and stemming application, to give more meaning for the text we are analysing, we can use approachs like _Bag of Words_, _TF-IDF_ and _Word2Vec_. For make our analysis easier, let's define a function that receives a text and a `vectorizer` object and applies the feature extraction on the respective text.","537cee12":"<a id=\"3.2\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>3.2 E-Commerce Around Brazil<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","675f14c2":"After all the steps we have taken over here, it's important to clean our text eliminating unecessary whitespaces. Let's apply a RegEx for this and see what we've got.","eb44fbc0":"<a id=\"4.4\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.4 Stemming<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","4b65a04c":"<a id=\"4.2.4\"><\/a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.4 Money<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","89577b84":"Another pattern that must be threated is sites and hiperlinks. Let's define another function to apply RegEx on this.","6c299334":"___\n* _What are the best states to buy in Brazil? An analysis on sales, freight and delivery time_\n___","b0582f6e":"The attributes used during the instancing of each vectorizer objects can be explained as:\n\n- `max_features=300`: indicates that the matrix will be created using the 300 most common words from the corpus\n- `max_df=0.8`: indicates that we will use only words with at least 80% frequency in the corpus\n- `min_df=7`: indicates that we will use only words that occurs in at least 7 text strings in the corpus","425884ec":"So, after detailing all the steps considered on this text prep pipeline, let's build a code to apply a complete pipeline automatically to handle it. This is a important step on the project because with this we can receive a text input and apply all changes on it to make it ready for training or predicting the sentiment label.","dc6de6e2":"$$IDF = \\log\\left({\\frac{\\text{Total number of docs}}{\\text{Number of docs containing the words}}}\\right)$$","a6ceb0e0":"Here we can get insights about the customers states with highest mean freight value. For example, customers in Roraima (RR), Para\u00edba (PB), Rond\u00f4nia (RO) and Acre (AC) normaly pays more than anyone on freights.","73777caa":"Until now, we just answered questions on E-commerce scenario considering the number of orders received. We could see the volumetry amonth months, day of week, time of the day and even the geolocation states.\n\nNow, we will analyze the money movemented by e-commerce by looking at order prices, freights and others.","c26ddbae":"<a id=\"4.2.3\"><\/a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.3 Dates<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","1ae36ce4":"The search for special characteres is a really special one because we see a lot of this pattern on online comments. Let's build an RegEx motor to find those ones.","687fe5ff":"As long as we consider the global internet as the source of our comments, probably we have to deal with some HTML tags, break lines, special characteres and other content that could be part of the dataset. Let's dig a little bit more on `Regular Expressions` to search for those patterns.\n\nFirst of all, let's define a function that will be used for analysing the results of an applied regular expression. With whis we can validate our text pre processing in an easier way.","56221068":"<a id=\"3.3\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>3.3 E-Commerce Impact on Economy<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","6802abe1":"For this task we have differente data sources, each one describing a specific topic related to e-commerce sales. The files are:\n\n    olist_customers_dataset.csv\n    olist_geolocation_dataset.csv\n    olist_orders_dataset.csv\n    olist_order_items_dataset.csv\n    olist_order_payments_dataset.csv\n    olist_order_reviews_dataset.csv\n    olist_products_dataset.csv\n    olist_sellers_dataset.csv\n    product_category_name_translation.csv\n    \nThe relationship between these files are described on the documentation. So let's read the datasets and make an initial analysis with all of them. This step will help us a lot to take right decisions in a future exploratory data analysis.","e528177c":"Now let's use a homemade function found on the module `viz_utils.py` for looking at each dataset and bring some detailed parameters about the data content. With this function we can get the following information for each dataset column:\n\n    - Column name;\n    - Null amount;\n    - Null percentage among the respective dataset;\n    - Data type;\n    - total categorical entries;","aa43d6bc":"By the time this dataset was created, the highest amount of orders went from delivered ones. Only 3% of all orders came from the other status.","f3e91201":"___\n__*Important Update 2:*__ At the last session of this notebook, I shared with you what I consider as a \"real production implementation\" for creating a sentiment classification model using olist reviews data. The idea is to bring some real problems solved in a end-to-end python script with all we should have in a day-to-day work. I hope you enjoy it!\n___","8d2b20c5":"<a id=\"4.5.1\"><\/a>\n<font color=\"dimgrey\" size=+1.5><b>4.5.1 CountVectorizer<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","0830a710":"<a id=\"4.2.5\"><\/a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.5 Numbers<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","c06dc2b2":"**References:**\n\nhttps:\/\/www.kaggle.com\/raenish\/cheatsheet-text-helper-functions created by Raenish David\n\nhttps:\/\/www.kaggle.com\/andresionek\/geospatial-analysis-of-brazilian-e-commerce created by Andre Sionek","b58a6566":"By the map we showed above, we have already the insight that the southeast of Brazil has the highest number of orders given through e-commerce. So, let's see it in a HeatMap!","c6e9e05d":"One of the datasets provided have informations about order's payment. To see how payments can take influence on e-commerce, we can build a mini-dashboard with main concepts: `payments type` and `payments installments`. The idea is to present enough information to clarify how e-commerce buyers usually prefer to pay orders.","6e2f8215":"* Brazilian APIs ans links for geolocation info:\n\n        https:\/\/servicodados.ibge.gov.br\/api\/docs\/localidades?versao=1","afae87a9":"Let's define a function to apply the stemming process on the comments. We will also give examples of the results.","2dc0be2e":"<a id=\"4.2\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.2 Regular Expressions<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","996fcbbd":"Now let's try to write a phrase to feed our `sentiment_analysis` function. In production, we can adapt it to serve any application. So, let's simulate an online review that says something like `Very bad product! I don't buy on this store anymore, the delivery was late and it cost so much money`.\n\nWhat's the sentiment of this given phrase?"}}