{"cell_type":{"251231d5":"code","cce28822":"code","33f4ae5d":"code","52258d29":"code","06d66068":"code","1703952c":"code","785d34df":"code","732e4b46":"code","14d6c09f":"code","17401047":"code","f941f947":"code","bf585e5c":"code","963f200c":"code","cfb9326a":"code","1149720c":"code","27119460":"code","5a48b6c8":"code","cc298922":"code","1ec5212f":"code","b2779c36":"code","b714f7e3":"code","9c229e0c":"code","425f2b65":"code","8ecdd93a":"code","f8bb09f0":"code","75f7c11d":"code","cf3a72f9":"code","0ad471e5":"code","45cf1854":"code","ca9f4db7":"code","3b0f19df":"code","1b12d524":"code","c0ea39c9":"code","69d0e2a3":"code","140f7309":"code","c898e9fe":"code","b37fa15c":"code","1b21f968":"code","fddd2837":"code","7b8c78b1":"code","499396b0":"code","40a6ed19":"code","00e422ed":"code","74063373":"code","028c1cbf":"code","2ca063dc":"code","30280d34":"code","e9588b14":"code","22418b7c":"code","7cbe414b":"code","e7fe50a1":"code","2811d6e9":"code","bae84568":"code","4208b06e":"code","b9a8035b":"code","8484fee7":"code","e4723daf":"code","54f40568":"code","4e76709b":"code","086fd679":"code","99f65271":"code","8d23d1f8":"code","f4c180c1":"markdown","f7b1d6f4":"markdown","e82d34e1":"markdown","14e25d14":"markdown","92626303":"markdown","c8362aa9":"markdown","7e60bd52":"markdown","49b71059":"markdown","76413eb6":"markdown","d5f1b717":"markdown","6563d154":"markdown","b86028d4":"markdown","cabd4443":"markdown","26f5c47a":"markdown","56a57fad":"markdown","0c7fa055":"markdown","5068e33f":"markdown","9c57224f":"markdown","c1b55452":"markdown","5be919dd":"markdown","3c4b5e55":"markdown","4e49e6b7":"markdown","ae38d32a":"markdown","9ca8df9f":"markdown","4268b946":"markdown","8f4cc506":"markdown","c5b85c44":"markdown","e4dffaf9":"markdown","33e7a752":"markdown","dcabed5e":"markdown","a0bc6ed9":"markdown","15067ab8":"markdown","0520563d":"markdown","918e5e29":"markdown","af0afd7a":"markdown","177ed1eb":"markdown","e506cdae":"markdown"},"source":{"251231d5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","cce28822":"df = pd.read_csv('..\/input\/black-friday\/train.csv')\ndf_backup = pd.read_csv('..\/input\/black-friday\/train.csv')\ndf.head()","33f4ae5d":"df.info()","52258d29":"%matplotlib inline\nimport matplotlib as plt\nfrom matplotlib import pyplot\n\nplt.pyplot.hist(df[\"Gender\"])","06d66068":"dftemp = df[['Gender', 'Purchase']].groupby(['Gender'], as_index=False)\ndfG = dftemp.sum()\ndfG","1703952c":"dfG.plot(kind='bar')\nplt.xlabel = ('Gender')","785d34df":"plt.pyplot.hist(df[\"Age\"])","732e4b46":"df['Occupation'].plot(kind='hist', figsize=(20, 5), bins=20)","14d6c09f":"dftemp = df[['Occupation', 'Purchase']].groupby(['Occupation'], as_index=False)\ndfO = dftemp.sum()\ndfO","17401047":"dfO.plot(kind='bar')","f941f947":"plt.pyplot.hist(df[\"City_Category\"])","bf585e5c":"dftemp = df[['City_Category', 'Purchase']].groupby(['City_Category'], as_index=False)\ndfCity = dftemp.sum()\ndfCity","963f200c":"dfCity.plot(kind='bar')","cfb9326a":"plt.pyplot.hist(df[\"Stay_In_Current_City_Years\"])","1149720c":"dftemp = df[['Stay_In_Current_City_Years', 'Purchase']].groupby(['Stay_In_Current_City_Years'], as_index=False)\ndfStay = dftemp.sum()\ndfStay","27119460":"dfStay.plot(kind='bar')","5a48b6c8":"plt.pyplot.hist(df[\"Marital_Status\"], bins = 20)","cc298922":"dftemp = df[['Marital_Status', 'Purchase']].groupby(['Marital_Status'], as_index=False)\ndfMarried = dftemp.sum()\ndfMarried","1ec5212f":"plt.pyplot.hist(df[\"Product_Category_1\"], bins = 20)","b2779c36":"df['Product_Category_2'].unique()","b714f7e3":"plt.pyplot.hist(df[\"Product_Category_2\"], bins = 20)","9c229e0c":"df[\"Product_Category_2\"].value_counts()","425f2b65":"df['Product_Category_2'].isnull().sum()","8ecdd93a":"# here we filled null values with most frequently occuring values\ncnt=0\nfor i,j in df.iterrows():\n    if pd.isnull(j['Product_Category_2']):\n        if cnt <= 70000:\n            df['Product_Category_2'][i] = '8.0'\n            cnt+=1\n        elif cnt <=130000:\n            df['Product_Category_2'][i] = '14.0'\n            cnt+=1\n        else :\n            df['Product_Category_2'][i] = '2.0'\n            cnt+=1\n        print(cnt)\n                \n            ","f8bb09f0":"df['Product_Category_2']","75f7c11d":"df['Product_Category_2'].isnull().sum()","cf3a72f9":"df['Product_Category_2'] = df['Product_Category_2'].astype(int)\ndf['Product_Category_2'].dtype","0ad471e5":"df['Product_Category_3'].unique()","45cf1854":"df[\"Product_Category_3\"].value_counts()","ca9f4db7":"df['Product_Category_3'].isnull().sum()","3b0f19df":"cnt=0\nfor i,j in df.iterrows():\n    if pd.isnull(j['Product_Category_3']):\n        if cnt <= 125000:\n            df['Product_Category_3'][i] = '16.0'\n            cnt+=1\n        elif cnt <=240000:\n            df['Product_Category_3'][i] = '15.0'\n            cnt+=1\n        elif cnt <= 300000 :\n            df['Product_Category_3'][i] = '14.0'\n            cnt+=1\n        elif cnt <= 345000 :\n            df['Product_Category_3'][i] = '17.0'\n            cnt+=1\n        else :\n            df['Product_Category_3'][i] = '5.0'\n            cnt+=1\n        print(cnt)","1b12d524":"df['Product_Category_3'].isnull().sum()","c0ea39c9":"df['Product_Category_3'] = df['Product_Category_3'].astype(int)\ndf['Product_Category_3'].dtype","69d0e2a3":"df.info()","140f7309":"y_data = df['Purchase'].copy()\nx_data = df.copy()","c898e9fe":"x_data.info()","b37fa15c":"x_data.drop(['Purchase', 'User_ID', 'Product_ID'], axis=1, inplace=True)","1b21f968":"\nfrom sklearn.preprocessing import LabelEncoder\ncategorical_column = ['Gender','Age','City_Category','Stay_In_Current_City_Years']\nle = LabelEncoder()\nfor i in categorical_column:\n    x_data[i] = le.fit_transform(x_data[i])\nx_data.head()","fddd2837":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.20, random_state=1)","7b8c78b1":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(x_train, y_train)\n\ntest_y_hat = lm.predict(x_test)\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test) ** 2))\n\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(test_y_hat , y_test))","499396b0":"from sklearn.tree import DecisionTreeRegressor\nmodel = DecisionTreeRegressor()\nmodel.fit(x_train, y_train)\n\ntest_y_hat = model.predict(x_test)\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test) ** 2))\n\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(test_y_hat , y_test))","40a6ed19":"from sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, objective ='reg:linear')\nmy_model.fit(x_train,y_train)\npredictions = my_model.predict(x_test)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(predictions - y_test)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((predictions - y_test) ** 2))\nprint(\"Accuracy of train dataset is : \",my_model.score(x_train,y_train))\nprint(\"Accuracy of test dataset is : \",my_model.score(x_test,y_test))\n\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(predictions, y_test))","00e422ed":"x_data_new = x_data.copy()\nx_data_new.drop(['Stay_In_Current_City_Years', 'Marital_Status', 'Occupation'], axis=1, inplace=True)","74063373":"from sklearn.model_selection import train_test_split\nx_train_n, x_test_n, y_train_n, y_test_n = train_test_split(x_data_new, y_data, test_size=0.20, random_state=1)","028c1cbf":"from sklearn.tree import DecisionTreeRegressor\nmodel = DecisionTreeRegressor()\nmodel.fit(x_train_n, y_train_n)\n\ntest_y_hat = model.predict(x_test_n)\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_hat - y_test_n)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_hat - y_test_n) ** 2))\n\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(test_y_hat , y_test_n))","2ca063dc":"from sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, objective ='reg:linear')\nmy_model.fit(x_train_n,y_train_n)\npredictions = my_model.predict(x_test_n)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(predictions - y_test_n)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((predictions - y_test_n) ** 2))\nprint(\"Accuracy of train dataset is : \",my_model.score(x_train_n,y_train_n))\nprint(\"Accuracy of test dataset is : \",my_model.score(x_test_n,y_test_n))\n\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(predictions, y_test_n))","30280d34":"x_train_final = x_data.copy()\ny_train_final = y_data.copy()","e9588b14":"x_test_final = pd.read_csv('..\/input\/black-friday\/test.csv')\nx_test_final.head()","22418b7c":"x_test_final.info()","7cbe414b":"x_test_final[\"Product_Category_2\"].value_counts()","e7fe50a1":"x_test_final[\"Product_Category_2\"].isnull().sum()","2811d6e9":"# here we filled null values with most frequently occuring values\ncnt=0\nfor i,j in x_test_final.iterrows():\n    if pd.isnull(j['Product_Category_2']):\n        if cnt <= 35000:\n            x_test_final['Product_Category_2'][i] = '8.0'\n            cnt+=1\n        elif cnt <=60000:\n            x_test_final['Product_Category_2'][i] = '14.0'\n            cnt+=1\n        else :\n            x_test_final['Product_Category_2'][i] = '2.0'\n            cnt+=1\n        print(cnt)\n                ","bae84568":"x_test_final['Product_Category_2'] = x_test_final['Product_Category_2'].astype(int)","4208b06e":"from sklearn.preprocessing import LabelEncoder\ncategorical_column = ['Gender','Age','City_Category','Stay_In_Current_City_Years']\nle = LabelEncoder()\nfor i in categorical_column:\n    x_test_final[i] = le.fit_transform(x_test_final[i])\nx_test_final.head()","b9a8035b":"x_test_final.drop(['User_ID', 'Product_ID'], axis=1, inplace=True)","8484fee7":"x_train_final.drop('Product_Category_3', axis=1, inplace=True)\nx_test_final.drop('Product_Category_3', axis=1, inplace=True)\nx_test_final.info()","e4723daf":"x_train_final.info()","54f40568":"from sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(x_train_final,y_train_final)\npredictions = my_model.predict(x_test_final)\nprint(predictions)","4e76709b":"col_list = ['User_ID', 'Product_ID']\ndf_submission = pd.read_csv('..\/input\/black-friday\/test.csv',usecols=col_list)\ndf_submission.head()","086fd679":"df_submission['Purchase'] = predictions\ndf_submission.head()","99f65271":"df_submission.set_index('Purchase', inplace=True)\ndf_submission.head()","8d23d1f8":"df_submission.to_csv(\"submission.csv\")","f4c180c1":"26-35 years of people maximum buy products on this day.","f7b1d6f4":"We found that some categories has highest frequency. ","e82d34e1":"Now, we will fill nan values with top three highest occuring values.","14e25d14":"Here, in this column we will perform steps similar to performed in previous column.","92626303":"Removing unnecessary columns.","c8362aa9":"It's found that most of non-married person bought the products on Black Friday.","7e60bd52":"## (vi) Marital_Status ","49b71059":"Preparing Dataframe for submission","76413eb6":"# Problem: Predict purchase amount.","d5f1b717":"## Step 5 : Trying different regression technique","6563d154":"## Step 1 : Import important libraries.","b86028d4":"Due to lots of missing values, I decided to drop this column.","cabd4443":"## (viii) Product_Category_2 ","26f5c47a":"## Saving our dataframe to file","56a57fad":"Now, no nan values remaining. Changing data-type of column in necessary to uniform training.","0c7fa055":"It is found that man purchase 3 times as women.","5068e33f":"Now, no nan values remaining.","9c57224f":"## (ii) Age","c1b55452":"## (ix) Product_Category_3","5be919dd":"We found that category B has maximum frequency thus maximum purchase.","3c4b5e55":"## (iv) City_Category","4e49e6b7":"We found that 0, 4 occupation were maximum among the people. Thus thier purchase (in terms of occupation) is maximum.\nI dont think we should use occupation for final training data.","ae38d32a":"We need to deal with missimg values in test df.","9ca8df9f":"# Black Friday ","4268b946":"## (iii) Occupation","8f4cc506":"Converting category columns to labels is necessary.","c5b85c44":"## (vii) Product_Category_1","e4dffaf9":"<u>Thus, in terms of gender it is found that more man prefer to purchase than women on black friday, also man used to buy more expensive products on that day. So, man total purchase is almost 4 times than women.","33e7a752":"We saw that inly 2 columns have nan values.","dcabed5e":"# END","a0bc6ed9":"## Step 4 : Preparing Data for model","15067ab8":"## Step 3 : Data Analysis","0520563d":"## Step 6 : Deriving result for test dataset.csv ","918e5e29":"## (i) Gender","af0afd7a":"## (v) Stay_In_Current_City_Years","177ed1eb":"## Step 2 : Import Data file","e506cdae":"Trying different ways to increase accuracy."}}