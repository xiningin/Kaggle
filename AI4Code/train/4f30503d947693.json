{"cell_type":{"0accec5b":"code","b3c2fea0":"code","5b89c018":"code","0e833d05":"code","590c5157":"markdown","235e2ae7":"markdown","1ec58f8d":"markdown","10f3a8c7":"markdown"},"source":{"0accec5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport math\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3c2fea0":"class GaussNB:\n    \n    def __init__(self):\n        \"\"\"\n        No params are needed for basic functionality.\n        \"\"\"\n        pass\n    \n    def _mean(self,X): # CHECKED\n        \"\"\"\n        Returns class probability for each \n        \"\"\"\n        mu = dict()\n        for i in self.classes_:\n            idx = np.argwhere(self.y == i).flatten()\n            mean = []\n            for j in range(self.n_feats):\n                mean.append(np.mean( X[idx,j] ))\n            mu[i] = mean\n        return mu\n    \n    def _stddev(self,X): # CHECKED\n        sigma = dict()\n        for i in self.classes_:\n            idx = np.argwhere(self.y==i).flatten()\n            stddev = []\n            for j in range(self.n_feats):\n                stddev.append( np.std(X[idx,j]) )\n            sigma[i] = stddev\n        return sigma\n    \n    def _prior(self): # CHECKED\n        \"\"\"Prior probability, P(y) for each y\n        \"\"\"\n        P = {}\n        for i in self.classes_:\n            count = np.argwhere(self.y==i).flatten().shape[0]\n            probability = count \/ self.y.shape[0]\n            P[i] = probability\n        return P\n    \n    def _normal(self,x,mean,stddev): # CHECKED\n        \"\"\"\n        Gaussian Normal Distribution\n        $P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$\n        \"\"\"\n        \n        multiplier = (1\/ float(np.sqrt(2 * np.pi * stddev**2))) \n        exp = np.exp(-((x - mean)**2 \/ float(2 * stddev**2)))\n        return multiplier * exp\n\n    \n    def P_E_H(self,x,h):\n        \"\"\"\n        Uses Normal Distribution to get, P(E|H) = P(E1|H) * P(E2|H) .. * P(En|H)\n        \n        params\n        ------\n        X: 1dim array. \n            E in P(E|H)\n        H: class in y\n        \"\"\"\n        pdfs = []\n        \n        for i in range(self.n_feats):\n            mu = self.means_[h][i]\n            sigma = self.stddevs_[h][i]\n            pdfs.append( self._normal(x[i],mu,sigma) )\n            \n        p_e_h = np.prod(pdfs)\n        return p_e_h\n        \n        \n    def fit(self, X, y):\n        self.n_samples, self.n_feats = X.shape\n        self.n_classes = np.unique(y).shape[0]\n        self.classes_ = np.unique(y)\n        self.y = y\n        \n        self.means_ = self._mean(X) # dict of list {class:feats}\n        self.stddevs_ = self._stddev(X) # dict of list {class:feat}\n        self.priors_ = self._prior() # dict of priors \n        \n    def predict(self,X):\n        samples, feats = X.shape\n        if samples!=self.n_samples or feats!=self.n_feats:\n            raise DimensionError(\"No dimension match with training data!\")\n            \n        result = []\n        for i in range(samples):\n            distinct_likelyhoods = []\n            for h in self.classes_:\n                tmp = self.P_E_H(X[i],h)\n                distinct_likelyhoods.append( tmp * self.priors_[h])\n            marginal = np.sum(distinct_likelyhoods)\n            tmp = 0\n            probas = []\n            for h in self.classes_:\n                numerator = self.priors_[h] * distinct_likelyhoods[tmp]\n                denominator = marginal\n                probas.append( numerator \/ denominator )\n                tmp+=1\n            # predicting maximum\n            idx = np.argmax(probas)\n            result.append(self.classes_[idx])\n        return result","5b89c018":"X, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)","0e833d05":"gnb = GaussianNB()\nsk_pred = gnb.fit(X_train, y_train).predict(X_test)\nprint(\"Sci-kit Learn: \",accuracy_score(y_test,sk_pred))\n\nnb = GaussNB()\nnb.fit(X_train,y_train)\nme_pred = nb.predict(X_test)\nprint(\"Custom GaussNB: \",accuracy_score(y_test,me_pred))","590c5157":"# Bayes Theroem","235e2ae7":"# Implementation\nWe need to calculate priors,$P(A)$, mean $\\mu_y$ and standard deviation, $\\sigma_y$ of all features for each available classes in y. Then a function to crunch the above formula of $P(x_i \\mid y)$","1ec58f8d":"Any naive bayes approach including Gaussian Naive Bayes depends on the Bayes Theorem. Bayes theorem gives us the probability of an event, given that we have some extra knowledge about that event. $$P(A \\mid B) = \\frac{P(A) \\ P(B \\mid A)}{P(B)}$$ This is the mathmatical definition of the theorem. Here, $A$ and $B$ are events and $P(B) \\ne 0$\n* $P(A \\mid B)$ a conditional probability. It is read as the likelyhood of event A occuring given that event B is true. In an experiment this is our objective variable.\n* $P(A)$ is prior probability. We are supposed to observe the value of this in our experiment.\n* $P(B \\mid A)$ is the conditional probability of B happening given A true. \n\nThere is a rather intuitive explanation behind this legendary theroy.\n\n## Smoke and Fire Experiment\nOur hypothesis is that there will be fire if there is smoke. So, we are asking for $P(Fire \\mid Smoke)$. To answer that we'll start with an experiment sample. \n![](attachment:NB1.png)\n\nWhen we collect data about *Fire* and *No Fire* events, the area of the white square represents the total sample space. \n![NB2.png](attachment:NB2.png)\n\nAnd we see that we area on the left is *Fire*  and on the right is *No Fire* samples. \n![NB3.png](attachment:NB3.png)\n\nNow let's observe in which samples smoke was seen in a fire, and in which sample smoke was seen even though there were no fire. Note that, smoke seen in a fire can be written as $(Smoke \\mid Fire)$ and not seen in a fire can be written as $(Smoke \\mid No Fire)$. Red areas represents smoke was seen. These red areas that I have drawn is based on gut feeling. Smoke is common when there's a fire, so red area is bigger compared to the red area when there is no fire. Now, as we know probability of something happening is effectively the ratio of that thing against all other thing. Hence, $P(Smoke \\mid Fire)$ is equals to red area inside fire divided by total red are in experiment. \n![NB5.png](attachment:NB5.png)\n\nBut how the red areas can be calculated mathematically? Well, area is the product of height and width. In our experiment, height is the $P(Smoke \\mid Fire)$ and width is $P(Fire)$. Similarly the red area in *No Fire* can be calculated. \n![NB6%281%29.png](attachment:NB6%281%29.png)\nSo, we finally get this equation on top.\n\nNote: This is just a geometrical explanation of Bayes Theorem. Gaussian Naive Bayes and other naive bayes algorithms differentiate themselves from this by how they calculate $P(Smoke | Fire)$.","10f3a8c7":"# Gaussian Naive Bayes\nThis algorithm assumes likelyhoods of features are of gaussian distribution. $$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$"}}