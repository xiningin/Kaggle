{"cell_type":{"a3693acd":"code","5a4d3407":"code","c003833e":"code","cbb4be4f":"code","bdb400ee":"code","e097cd0f":"code","3b3ae032":"code","6d7c57b4":"code","82c1427d":"code","a4af52af":"code","920731e4":"code","2fbf0d10":"code","b8461959":"code","15e020cb":"code","fbec2646":"code","6f5ba3ce":"code","162bfb6e":"code","cfbf4d4b":"code","1bd131ab":"code","19b61d36":"code","6c36de98":"code","e1c00f55":"code","b84180fb":"code","d16e6d76":"code","8b7f8497":"code","bb355f1b":"code","577ee6d8":"code","68f5fa67":"code","20674cbe":"code","7f4da926":"code","598790f5":"code","38f3b16b":"code","ef480d30":"code","0c2c4f4a":"code","77f8831d":"code","bd1a2079":"code","112726c7":"code","f70112d1":"code","aebee93a":"code","ac37fe01":"code","90c1db44":"code","c86f2e21":"code","cf4ff1e8":"code","04e2d9e8":"code","ad736dab":"code","aa2279bf":"code","559ba3ab":"code","1e1664ff":"code","581ff3e8":"code","251ff374":"code","01be69e3":"code","dc545ea2":"code","ae41d386":"code","2899cb0d":"code","cf8a51ff":"code","e90f97df":"code","593fc88e":"code","502b865b":"code","c5904ca6":"code","ec1af461":"code","b49b2a01":"code","40146ef5":"code","08daac73":"code","7c529d8d":"code","9b53663b":"code","32c95fa1":"code","3c1b0a72":"code","7b42fa90":"code","5721e10b":"code","f927c379":"code","d8c9f8b8":"code","59d7e81f":"code","421f6258":"code","2d25b41a":"code","655bface":"code","8644f88e":"code","eea5f0ae":"code","ace86f86":"code","a7c48d4f":"code","fc4651da":"code","20d2518f":"code","b40b342c":"code","590ced18":"code","1c56377f":"code","78d28538":"code","8a8755e9":"code","e452a981":"code","8e22095a":"code","47363372":"code","7ad87e51":"code","f1c74d90":"code","7ee2f3b4":"code","e93c57ea":"code","6a8279d9":"markdown","0c77e56b":"markdown","8ab7738e":"markdown","120e7fe9":"markdown","227b81ba":"markdown","7c908afc":"markdown","225d2319":"markdown","e21c897d":"markdown","b83dbea8":"markdown","32093a95":"markdown","28aa6f7b":"markdown","f4a235fa":"markdown","1236ae38":"markdown","f202bb0e":"markdown","4f6474e9":"markdown","24f48198":"markdown","dd003a80":"markdown","69bd2cc5":"markdown","39d58216":"markdown","25b036cb":"markdown","c0b4099b":"markdown","91645c1b":"markdown","c223e636":"markdown","ac5df0f2":"markdown","703cde8f":"markdown","5ef0ed9d":"markdown","88d184ff":"markdown","c1e80958":"markdown","8d5dad34":"markdown","ebb8f06c":"markdown","09045005":"markdown","dde1709a":"markdown","ffe8302b":"markdown","89eec0c4":"markdown","d56d87e9":"markdown","c0b72cd3":"markdown","4c09b313":"markdown","b24b2d67":"markdown","305871dc":"markdown","d90f3305":"markdown","b818ef60":"markdown","32ae0263":"markdown","2d1928e4":"markdown","85a3652c":"markdown","bb174d3d":"markdown","b168ee32":"markdown","83b6ffff":"markdown","12c6ab69":"markdown","9d4c1e7c":"markdown","85b0032a":"markdown","0b4e49c3":"markdown","3b96776a":"markdown","63cc300b":"markdown","829e7acc":"markdown","413d7a64":"markdown","0dc104a9":"markdown","fd456191":"markdown","f59cf1fa":"markdown","193e55d6":"markdown","fbdeaadf":"markdown","614b0ff2":"markdown","5b14afc9":"markdown","f4a77616":"markdown","a22bd27e":"markdown","859a5bfa":"markdown","7f0b5467":"markdown","02e21e47":"markdown","c18f2057":"markdown","ad9d4ac2":"markdown","99cf7371":"markdown","30343464":"markdown","e035f4d4":"markdown","974481c2":"markdown","616f97f7":"markdown","f29a3049":"markdown","ba753b14":"markdown","b370d6aa":"markdown"},"source":{"a3693acd":"import pandas as pd","5a4d3407":"df = pd.read_csv('..\/input\/searchqueries\/searchquery_.csv',index_col=0)","c003833e":"df","cbb4be4f":"df.info()","bdb400ee":"df['Users'] = df['Users'].replace( ',','', regex=True ).astype('int')\ndf['Sessions'] = df['Sessions'].replace( ',','', regex=True ).astype('int')\ndf['Avg. Session Duration'] = pd.to_timedelta(df['Avg. Session Duration'].replace('<','', regex=True )).astype(int) \/ 1e9\/\/60\ndf['% New Sessions'] = df['% New Sessions'].str.rstrip('%').astype('float') \/ 100.0\ndf['Bounce Rate'] = df['Bounce Rate'].str.rstrip('%').astype('float') \/ 100.0\ndf['Goal Completions'] = df['Goal Completions'].replace( ',','', regex=True ).astype('int')\ndf['Revenue'] = df['Revenue'].replace( '[R$,]','', regex=True ).astype('float')","e097cd0f":"df.info()","3b3ae032":"df.columns","6d7c57b4":"# dropping 'sessions' less than 1\n\ndf = df[df['Sessions']>=1]\n\ndf.shape","82c1427d":"# dropping 'nan' \n\ndf = df.dropna()","a4af52af":"df.corr()","920731e4":"import plotly.express as px","2fbf0d10":"fig = px.scatter_matrix(df,width=1000, height=1000)\nfig.show()","b8461959":"# Droping and organizing the columns\n\ncols = ['Sessions',\n      'Avg. Session Duration',\n      'Bounce Rate',\n      'Revenue']\n    \n\ndf = df[cols]","15e020cb":"fig = px.scatter_matrix(df,width = 600, height = 600)\n\nfig.show()","fbec2646":"# Resetting the index and defining X as the DF without the Search Queries\n\ndf = df.reset_index()\n\nX = df[df.columns[1:]]","6f5ba3ce":"df.head()","162bfb6e":"X.head()","cfbf4d4b":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go","1bd131ab":"fig = make_subplots(rows=1, cols=4)\n\nfig.add_trace(go.Box(y = X['Sessions'],name='Sessions'),row=1,col=1)\nfig.add_trace(go.Box(y = X['Avg. Session Duration'],name ='Avg. Session Duration'),row=1,col=2)\nfig.add_trace(go.Box(y = X['Bounce Rate'],name ='Bounce Rate'),row=1,col=3)\nfig.add_trace(go.Box(y = X['Revenue'],name  ='Revenue'),row=1,col=4)\n\nfig.show()","19b61d36":"import numpy as np\nfrom numpy import inf","6c36de98":"X = np.log(X)\nX[X == -inf] = 0\nX","e1c00f55":"from sklearn import preprocessing","b84180fb":"min_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(X)\nX = pd.DataFrame(x_scaled,columns=cols)","d16e6d76":"fig = make_subplots(rows=1, cols=4)\n\nfig.add_trace(go.Box(y = X['Sessions'],name='Sessions'),row=1,col=1)\nfig.add_trace(go.Box(y = X['Avg. Session Duration'],name='Avg. Session Duration'),row=1,col=2)\nfig.add_trace(go.Box(y = X['Bounce Rate'],name='Bounce Rate'),row=1,col=3)\nfig.add_trace(go.Box(y = X['Revenue'],name='Revenue'),row=1,col=4)\n\nfig.show()","8b7f8497":"from sklearn.cluster import KMeans","bb355f1b":"from sklearn.metrics import silhouette_score","577ee6d8":"range_n_clusters = [2, 3, 4]\n    \nfor n_clusters in range_n_clusters:\n\n    clusterer = KMeans(n_clusters = n_clusters,\n                        random_state = 42)\n    preds = clusterer.fit_predict(X)\n\n    score = silhouette_score(X, preds)\n    print(\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))","68f5fa67":"kmeans = KMeans(n_clusters = 4, random_state = 42)\nkmeans.fit(X)","20674cbe":"# Labeling X (The data nomalized)\n\nclusters_kmeans = pd.DataFrame(X,columns=cols)\nclusters_kmeans['label'] = kmeans.predict(X)\nclusters_kmeans = clusters_kmeans.join(df['Search Query'])","7f4da926":"# Labeling df (The original data)\n\ndf_kmeans = df.join(clusters_kmeans['label'])","598790f5":"def overview(df):\n    \n    zero = df[df['label']==0]\n    one = df[df['label']==1]\n    two = df[df['label']==2]\n    three = df[df['label']==3]\n\n\n    index = ['zero_label',\n               'one_label',\n               'two_label',\n               'three_label']\n\n\n    total_df = pd.DataFrame({ 'Total Sessions':[zero['Sessions'].sum(),\n                                                one['Sessions'].sum(),\n                                                two['Sessions'].sum(),\n                                                three['Sessions'].sum()\n                                   ],\n                                  'Total Revenue':[zero['Revenue'].sum(),\n                                                   one['Revenue'].sum(),\n                                                   two['Revenue'].sum(),\n                                                   three['Revenue'].sum()\n                                                  ],\n                                'Size':[zero['Revenue'].shape[0],\n                                                 one['Revenue'].shape[0],\n                                                 two['Revenue'].shape[0],\n                                                 three['Revenue'].shape[0]]\n                            },index = index)\n\n\n    mean_df = pd.DataFrame({'Sessions Mean':[zero['Sessions'].mean(),\n                                            one['Sessions'].mean(),\n                                            two['Sessions'].mean(),\n                                            three['Sessions'].mean()\n                                  ],\n                            'Avg. Session Duration Mean':[zero['Avg. Session Duration'].mean(),\n                                                         one['Avg. Session Duration'].mean(),\n                                                         two['Avg. Session Duration'].mean(),\n                                                         three['Avg. Session Duration'].mean()\n                                                           ],\n\n                            'Bounce Rate Mean':[zero['Bounce Rate'].mean(),\n                                               one['Bounce Rate'].mean(),\n                                               two['Bounce Rate'].mean(),\n                                               three['Bounce Rate'].mean()\n                                                 ],\n\n                            'Revenue Mean':[zero['Revenue'].mean(),\n                                           one['Revenue'].mean(),\n                                           two['Revenue'].mean(),\n                                           three['Revenue'].mean()\n                                           ]\n            \n                           },index = index)\n\n    return total_df,mean_df\n","38f3b16b":"total_kmean,mean_kmean = overview(df_kmeans)","ef480d30":"total_kmean","0c2c4f4a":"mean_kmean","77f8831d":"zero_kmean = df_kmeans[df_kmeans['label']==0]\none_kmean = df_kmeans[df_kmeans['label']==1]\ntwo_kmean = df_kmeans[df_kmeans['label']==2]\nthree_kmean = df_kmeans[df_kmeans['label']==3]","bd1a2079":"print('\\033[1m' + 'Kmean Label Zero')\ndisplay(zero_kmean.head())\nprint('\\n','\\033[1m' + 'Kmean Label One')\ndisplay(one_kmean.head())\nprint('\\n','\\033[1m' + 'Kmean Label Two')\ndisplay(two_kmean.head())\nprint('\\n','\\033[1m' + 'Kmean Label Three')\ndisplay(three_kmean.head())","112726c7":"def cluster_bar_plot(df):\n    \n    quant = df['label'].value_counts().reset_index().rename(columns={\"index\": \"Cluster label\", \"label\": \"Quantity\"})\n\n    colors = {0:'blue',\n              1:'red',\n              2:'lightseagreen',\n              3:'purple'}\n    \n    fig=go.Figure()\n    \n    for i in quant[\"Cluster label\"].unique():\n        quant_ = quant[quant[\"Cluster label\"]==i]\n        fig.add_traces(go.Bar(x = quant_[\"Cluster label\"],\n                                     y = quant_[\"Quantity\"],\n                                     name = ('Cluster '+(i).astype(str)),\n                                     marker_color = colors[i]))\n    fig.show()","f70112d1":"cluster_bar_plot(df_kmeans)","aebee93a":"def polar_plot(df):\n    polar = df.groupby(\"label\").mean().reset_index()\n    polar = pd.melt(polar,id_vars=[\"label\"])\n\n    fig = px.line_polar(polar,\n                        r=\"value\",\n                        theta=\"variable\",\n                        color=\"label\",\n                        line_close=True,\n                        height=500,width=600)\n    fig.show()","ac37fe01":"# For this task we will use the normalized and scaled data. That will help us to have a better visualization.\n\npolar_plot(clusters_kmeans)","90c1db44":"def threed_plot(df):    \n    fig = px.scatter_3d(pd.DataFrame(df,columns = ['Sessions',\n                                                'Avg. Session Duration',\n                                                'Bounce Rate',\n                                                'Revenue']), x='Sessions', y='Bounce Rate',\n                         z = 'Revenue',size = 'Avg. Session Duration',size_max=25,\n                         color = df['label'].astype(str),\n                         color_discrete_map = {'0': 'blue', \n                                             '1': 'red', \n                                             '2': 'lightseagreen',\n                                             },\n                        hover_name = df['Search Query']\n                        \n                        )\n    fig.update_layout(title = \"5 Features Representation\")\n    fig.show()","c86f2e21":"threed_plot(clusters_kmeans)","cf4ff1e8":"from sklearn.decomposition import PCA","04e2d9e8":"def pca_cluster_plot(df):\n    \n    reduced_data = PCA(n_components = 2).fit_transform(X)\n    results = pd.DataFrame(reduced_data,columns=['pca1','pca2'])\n\n    fig = px.scatter(results,x = \"pca1\", y = \"pca2\",\n                     color = (df['label'].astype(str)),\n                     color_discrete_map = {'0': 'blue', \n                                             '1': 'red', \n                                             '2': 'lightseagreen',\n                                             '3':'purple'\n                                             },\n                    title = 'Clustering with 2 dimensions',\n                    hover_name = df['Search Query'])\n                    \n\n    fig.show()","ad736dab":"pca_cluster_plot(clusters_kmeans)","aa2279bf":" from sklearn.cluster import SpectralClustering","559ba3ab":"for n_clusters in range_n_clusters:\n    clusterer = SpectralClustering(n_clusters = n_clusters,\n                                   assign_labels = \"discretize\",\n                                   random_state = 0)\n    preds = clusterer.fit_predict(X)\n\n    score = silhouette_score(X, preds)\n    print(\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))","1e1664ff":"sc = SpectralClustering(\n        n_clusters = 4,\n        assign_labels = \"discretize\",\n        random_state = 0).fit(X)","581ff3e8":"# Labeling X (The normalized data)\n\nclusters_sc = pd.DataFrame(X,columns=cols)\nclusters_sc['label'] = sc.labels_\nclusters_sc = clusters_sc.join(df['Search Query'])","251ff374":"# Labeling df (The original data)\n\ndf_sc = df.join(clusters_sc['label'])","01be69e3":"total_sc,mean_sc = overview(df_sc)","dc545ea2":"total_sc","ae41d386":"mean_sc","2899cb0d":"zero_sc = df_sc[df_sc['label']==0]\none_sc = df_sc[df_sc['label']==1]\ntwo_sc = df_sc[df_sc['label']==2]\nthree_sc = df_sc[df_sc['label']==3]","cf8a51ff":"print('\\033[1m' + 'Spectral Clustering Label Zero')\ndisplay(zero_sc.head())\nprint('\\n','\\033[1m' + 'Spectral Clustering Label One')\ndisplay(one_sc.head())\nprint('\\n','\\033[1m' + 'Spectral Clustering Label Two')\ndisplay(two_sc.head())\nprint('\\n','\\033[1m' + 'Spectral Clustering Label Three')\ndisplay(three_sc.head())","e90f97df":"cluster_bar_plot(df_sc)","593fc88e":"polar_plot(clusters_sc)","502b865b":"threed_plot(clusters_sc)","c5904ca6":"pca_cluster_plot(clusters_sc)","ec1af461":"from sklearn.cluster import AgglomerativeClustering","b49b2a01":"for n_clusters in range_n_clusters:\n    clusterer = AgglomerativeClustering(n_clusters=n_clusters).fit(X)\n    preds = clusterer.fit_predict(X)\n\n    score = silhouette_score(X, preds)\n    print(\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))","40146ef5":"ac = AgglomerativeClustering(n_clusters=4).fit(X)","08daac73":"# Labeling X (The normalized data)\nclusters_ac = pd.DataFrame(X,columns=cols)\nclusters_ac['label'] = ac.labels_\nclusters_ac = clusters_ac.join(df['Search Query'])","7c529d8d":"# Labeling df (The original data)\ndf_ac = df.join(clusters_ac['label'])","9b53663b":"total_ac,mean_ac = overview(df_ac)","32c95fa1":"total_ac","3c1b0a72":"mean_ac","7b42fa90":"zero_ac = df_ac[df_ac['label']==0]\none_ac = df_ac[df_ac['label']==1]\ntwo_ac = df_ac[df_ac['label']==2]\nthree_ac = df_ac[df_ac['label']==3]","5721e10b":"print('\\033[1m' + 'Agglomerative Clustering Label Zero')\ndisplay(zero_ac.head())\nprint('\\n','\\033[1m' + 'Agglomerative Clustering Label One')\ndisplay(one_ac.head())\nprint('\\n','\\033[1m' + 'Agglomerative Clustering Label Two')\ndisplay(two_ac.head())\nprint('\\n','\\033[1m' + 'Agglomerative Clustering Label Three')\ndisplay(three_ac.head())","f927c379":"cluster_bar_plot(df_ac)","d8c9f8b8":"polar_plot(clusters_ac)","59d7e81f":"threed_plot(clusters_ac)","421f6258":"pca_cluster_plot(clusters_ac)","2d25b41a":"from sklearn.mixture import GaussianMixture","655bface":"for n_clusters in range_n_clusters:\n    clusterer = GaussianMixture(n_components = n_clusters, random_state = 0).fit(X)\n    preds = clusterer.fit_predict(X)\n\n    score = silhouette_score(X, preds)\n    print(\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))","8644f88e":"gm = GaussianMixture(n_components = 4, random_state = 0).fit(X)","eea5f0ae":"clusters_gm = pd.DataFrame(X,columns = cols)\nclusters_gm['label'] = gm.predict(X)\nclusters_gm = clusters_gm.join(df['Search Query'])","ace86f86":"# Labeling df (The original data)\n\ndf_gm = df.join(clusters_gm['label'])","a7c48d4f":"total_gm,mean_gm = overview(df_gm)","fc4651da":"total_gm","20d2518f":"mean_gm","b40b342c":"zero_gm = df_gm[df_gm['label']==0]\none_gm = df_gm[df_gm['label']==1]\ntwo_gm = df_gm[df_gm['label']==2]\nthree_gm = df_gm[df_gm['label']==3]","590ced18":"print('\\033[1m' + 'Gaussian Mixture Label Zero')\ndisplay(zero_gm.head())\nprint('\\n','\\033[1m' + 'Gaussian Mixture Label One')\ndisplay(one_gm.head())\nprint('\\n','\\033[1m' + 'Gaussian Mixture Label Two')\ndisplay(two_gm.head())\nprint('\\n','\\033[1m' + 'Gaussian Mixture Label Three')\ndisplay(three_gm.head())","1c56377f":"cluster_bar_plot(df_gm)","78d28538":"polar_plot(clusters_gm)","8a8755e9":"threed_plot(clusters_gm)","e452a981":"pca_cluster_plot(clusters_gm)","8e22095a":"def cut(x):\n    \n    if len(x.unique()) > 2 :\n        serie=pd.qcut(x.rank(method='first'),3,labels=['low','medium','high'])\n    else:\n        serie=pd.cut(x,2,labels=['low','high'])\n    \n    return serie      ","47363372":"display(mean_kmean.apply(lambda x: cut(x),axis=0))\ndisplay(mean_sc.apply(lambda x: cut(x),axis=0))\ndisplay(mean_ac.apply(lambda x: cut(x),axis=0))\ndisplay(mean_gm.apply(lambda x: cut(x),axis=0))","7ad87e51":"#mean df\nmean_kmean = mean_kmean.rename({'zero_label':'SQ_Sessions_Minimally_Interested-Kmean',\n                                  'one_label':'SQ_Rejection-Kmean',\n                                  'two_label':'SQ_Intereste_User-Kmean',\n                                  'three_label':'SQ_Buyer-Kmean'})\n\nmean_sc = mean_sc.rename({'zero_label':'SQ_Rejection-SC',\n                          'one_label':'SQ_Buyer-SC',\n                          'two_label':'SQ_Intereste_User-SC',\n                          'three_label':'SQ_Sessions_Minimally_Interested-SC'})\n\nmean_ac = mean_ac.rename({'zero_label':'SQ_Rejection-AC',\n                    'one_label':'SQ_Sessions_Minimally_Interested-AC',\n                    'two_label':'SQ_Intereste_User-AC',\n                    'three_label':'SQ_Buyer-AC'})\n\nmean_gm = mean_gm.rename({'zero_label':'SQ_Sessions_Minimally_Interested-GM',\n                    'one_label':'SQ_Rejection-GM', \n                    'two_label':'SQ_Intereste_User-GM',\n                    'three_label':'SQ_Buyer-GM'})\n\nmean_all_algos = pd.concat([mean_kmean,mean_sc,mean_ac,mean_gm]).sort_index().reset_index().rename(columns={'index':'Algo Clusters'})\n\n#total df\ntotal_kmean = total_kmean.rename({'zero_label':'SQ_Sessions_Minimally_Interested-Kmean',\n                                  'one_label':'SQ_Rejection-Kmean',\n                                  'two_label':'SQ_Intereste_User-Kmean',\n                                  'three_label':'SQ_Buyer-Kmean'})\n\ntotal_sc = total_sc.rename({'zero_label':'SQ_Rejection-SC',\n                          'one_label':'SQ_Buyer-SC',\n                          'two_label':'SQ_Intereste_User-SC',\n                          'three_label':'SQ_Sessions_Minimally_Interested-SC'})\n\ntotal_ac = total_ac.rename({'zero_label':'SQ_Rejection-AC',\n                    'one_label':'SQ_Sessions_Minimally_Interested-AC',\n                    'two_label':'SQ_Intereste_User-AC',\n                    'three_label':'SQ_Buyer-AC'})\n\ntotal_gm = total_gm.rename({'zero_label':'SQ_Sessions_Minimally_Interested-GM',\n                    'one_label':'SQ_Rejection-GM', \n                    'two_label':'SQ_Intereste_User-GM',\n                    'three_label':'SQ_Buyer-GM'})\n\ntotal_all_algos =pd.concat([total_kmean,total_sc,total_ac,total_gm]).sort_index() \n\ntotal_all_algos.index.name = 'Algo Clusters'\n\n# setting index\n\ntotal_all_algos['label']=''\n\ntotal_all_algos.loc[total_all_algos.index.str.contains(\"Buyer\"),\"label\"]=\"Buyer\"\ntotal_all_algos.loc[total_all_algos.index.str.contains(\"Intereste_User\"),\"label\"]=\"Intereste_User\"\ntotal_all_algos.loc[total_all_algos.index.str.contains(\"Rejection\"),\"label\"]=\"Rejection\"\ntotal_all_algos.loc[total_all_algos.index.str.contains(\"Sessions_Minimally_Interested\"),\"label\"]=\"Sessions_Minimally_Interested\"\n\n\ntotal_all_algos = mean_all_algos.merge(total_all_algos, on='Algo Clusters').set_index(['label','Algo Clusters'])\n","f1c74d90":"total_all_algos","7ee2f3b4":"clusters_gm = clusters_gm.replace({'label': {0:'SQ_Sessions_Minimally_Interested',\n                                1:'SQ_Rejection', \n                                2:'SQ_Intereste_User',\n                                3:'SQ_Buyer'}})\n\nclusters_gm = clusters_gm.set_index(['Search Query'])\n\nclusters_gm\n\n ","e93c57ea":"clusters_gm.to_csv('sq_labeled.csv')","6a8279d9":"<a id=\"data\"><\/a>\n# 2 - Data","0c77e56b":"## 5.4 - Size - Bar Plot - Kmeans<a id=\"sbpKM\"><\/a>","8ab7738e":"The Kmean, as the other 3 algoritmos  that we are going to use in this notebook, requires the number of clusters to be specified. The optimal number of clusters is somehow subjective and depends on the method. \n\nFor that project I choose to use the Silhouette Score method.\n\nThe [documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.silhouette_score.html) explain the Silhouette Score as calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) \/ max(a, b)\nThe best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n\nFor every algorithm used, we run a for loop in a range of 3 (to facilitate the comprehension of the clusters) numbers, and choose the number of clusters for the best silhouette score.\n","120e7fe9":"## Table of Contents <a name=\"top\"><\/a>\n\n* [1 - Introduction](#introduction)\n* [2 - Data](#data)\n* [3 - Methodology](#methodology)\n* [4 - Data Check](#Dc)\n    * [4.1 - Data Cleaning](#DC)\n    * [4.2 - Data Preprocessing](#Dp)\n    * [4.3 - Distribution](#dist)\n* [5 - Kmeans](#KM)\n    * [5.1 - Silhouette Score - Kmeans](#ssKM)\n    * [5.2 - Clusters Label - Kmeans](#alcKM)\n    * [5.3 - Tabular Overview - Kmeans](#toKM)\n    * [5.4 - Size - Bar Plot - Kmeans](#sbpKM)\n    * [5.5 - Mean of the Features - Polar Plot - Kmeans](#mfppKM)\n    * [5.6 - 5 Features Representation - 3D Plot - Kmeans](#5frKM)\n    * [5.7 - Principal Component Analysis (PCA) - Scatter Plot - Kmeans](#pcaspKM)\n    * [5.8 - Summary Description - Kmeans](#sdKM)\n* [6 - Spectral Clustering](#SC)\n    * [6.1 - Silhouette Score - Spectral Clustering](#ssSC)\n    * [6.2 - Clusters Label - Spectral Clustering](#alcSC)\n    * [6.3 - Tabular Overview - Spectral Clustering](#toSC)\n    * [6.4 - Size - Bar Plot - Spectral Clustering](#sbpSC)\n    * [6.5 - Mean of the Features - Polar Plot - Spectral Clustering](#mfppSC)\n    * [6.6 - 5 Features Representation - 3D Plot - Spectral Clustering](#5frSC)\n    * [6.7 - Principal Component Analysis (PCA) - Scatter Plot - Spectral Clustering](#pcaspSC)\n    * [6.8 - Summary Description - Spectral Clustering](#sdSC)\n* [7 - Agglomerative Clustering](#AC)\n    * [7.1 - Silhouette Score - Agglomerative Clustering](#ssAC)\n    * [7.2 - Clusters Label - Agglomerative Clustering](#alcAC)\n    * [7.3 - Tabular Overview - Agglomerative Clustering](#toAC)\n    * [7.4 - Size - Bar Plot - Agglomerative Clustering](#sbpAC)\n    * [7.5 - Mean of the Features - Polar Plot - Agglomerative Clustering](#mfppAC)\n    * [7.6 - 5 Features Representation - 3D Plot - Agglomerative Clustering](#5frAC)\n    * [7.7 - Principal Component Analysis (PCA) - Scatter Plot - Agglomerative Clustering](#pcaspAC)\n    * [7.8 - Summary Description - Agglomerative Clustering](#sdAC)\n* [8 - Gaussian Mixture](#GM)\n    * [8.1 - Silhouette Score - Gaussian Mixture](#ssGM)\n    * [8.2 - Clusters Label - Gaussian Mixture](#alcGM)\n    * [8.3 - Tabular Overview - Gaussian Mixture](#toGM)\n    * [8.4 - Size - Bar Plot - Gaussian Mixture](#sbpGM)\n    * [8.5 - Mean of the Features - Polar Plot - Gaussian Mixture](#mfppGM)\n    * [8.6 - 5 Features Representation - 3D Plot - Gaussian Mixture](#5frGM)\n    * [8.7 - Principal Component Analysis (PCA) - Scatter Plot - Gaussian Mixture](#pcaspGM)\n    * [8.8 - Summary Description - Gaussian Mixture](#sdGM)\n* [9 - The Best Choice](#TBC)\n* [10 - Conclusion](#CON)\n\n","227b81ba":"To find the best cluster model for our data set, first we will clean the data, drop the most correlated features and we will scale the data frame. For each of the four clustering models that we will test, we will be looking for the best silhouette score testing the optimal number of clusters in a range of 3 numbers. \nWe are going to create DF with the total and mean of the clustering labels. We are going to plot the result in a few ways.\nAfter that we are going to look for some patterns created by the algorithms and analyse the mean and totals to choose our best cluster model\n\n* For manipulation and analyse the data we are going to use the **Pandas** libraries.\n* We will use **Plotly** library and the modules **Express**, **make_subplots**, **graph_objects** to the visualization\n* For transform to **normalization** we will use **numpy log** and after that we will use **sklearn** **preprocessing** package to **scale** the data.\n* For choose the optimal number of clusters for our models we are going to look for the best **silhouette_score** from **sklearn.metrics** in a range of 3 numbers [2,3,4]. For an easy comprehension we will prefer a small number of clusters. \n* For visualization the result, we will use the **line_polar**, **scatter_3d** to visualize the 5 features in just one plot. The  **go.Bar** to visualize the amount of each label and use the **Principal component analysis (PCA)** from **sklearn.decomposition** module to visualize the labels in a two dimensional scatter plot.\n* To have a good understanding about the labels we will use basic **descriptive statistics**. easily that we can easily get with the *.describe()** method. \n* The cluster models that we will test in this project will be **KMeans**, **Spectral Clustering**, **Agglomerative Clustering** and **Gaussian Mixture**.\n\n\nLet's get started!\n","7c908afc":"## 5.8 - Summary Description - Kmeans<a id=\"sdKM\"><\/a>","225d2319":"## 5.7 -  Principal Component Analysis (PCA) - Scatter Plot - Kmeans<a id=\"pcaspKM\"><\/a>","e21c897d":"## 8.4 - Size - Bar Plot - Gaussian Mixture  <a id=\"sbpGM\"><\/a>","b83dbea8":"# 6 - Spectral Clustering <a id=\"SC\"><\/a>","32093a95":"Implementing K-Means clustering with the Optimal number of clusters equal 4. ","28aa6f7b":"## 6.8 - Summary Description - Spectral Clustering <a id=\"sdSC\"><\/a>","f4a235fa":"## 7.2 - Clusters Label - Agglomerative Clustering <a id=\"alcAC\"><\/a>","1236ae38":"<a id=\"Dp\"><\/a>\n## 4.2 - Data Preprocessing  ","f202bb0e":"## 6.1 - Silhouette Score - Spectral Clustering <a id=\"ssSC\"><\/a>","4f6474e9":"## 5.1 - Silhouette Score - Kmeans <a id=\"ssKM\"><\/a>","24f48198":"<a id=\"methodology\"><\/a>\n# 3 - Methodology","dd003a80":"The second cluster algo tested in this project is the **Spectral Clustering**. It is a technique with roots in graph theory, where the approach is used to identify groups of nodes in a graph based on the edges connecting them.\n\n The **Spectral Clustering** treats each data point as a graph-node and thus transforms the clustering problem into a graph-partitioning problem. A typical implementation consists of three fundamental steps - Building the similarity graph, projecting the data onto a lower dimensional space and clustering the data. \n\n\nSee more in the [Documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.SpectralClustering.html) or in the Geek for geeks: [ML | Spectral Clustering](https:\/\/www.geeksforgeeks.org\/ml-spectral-clustering\/)","69bd2cc5":"**Spectral Clustering**\n\n**Number of Clusters** - 4\n\n**Silhouette Score** - 0.70\n\n**Cluster 0** - **'Search Queries'** with zero **\"Revenue\"**, larger amount of **'Search Queries'**. Smaller **\"Sessions mean\"**, **\"Avg. Session Duration \"** and higher\"**\"Bounce Rate mean\"**.\n\nMost relevant feature: **\"Bounce Rate\"**.\n\n**Cluster 1** - Larger number of **\"Sessions\"** and **\"Revenue\"** with smaller amount of **'Search Queries'**. Second larger **\"Avg. Session Duration \"**. Larger **\"Session mean\"** and larger **\"Revenue mean\"**. \n\nMost relevant features: **\"Revenue\"**, **\"Bounce Rate\"**, **\"Sessions\"** and **\"Avg. Session Duration.\"**\n\n**Cluster 2** - **'Search Queries'** with the second large **\"Revenue\"**. Larger **\"Avg. Session Duration\"**. Second smaller **\"Session mean\"**. \n\nMost relevant features: **\"Bounce Rate\"** and **\"Avg. Session Duration\"**\n\n**Cluster 3** - **'Search Queries'** with a second large amount of **\"Sessions\"** and zero **\"Revenue\"**. Second higher **\"Session mean\"**. \n\nMost relevant features: **\"Bounce Rate\"**, **\"Sessions\"** and **\"Avg. Session Duration\"**\n\n","39d58216":"As shown above, the data imported from Google Analytics are complete and no columns have null values. However the majority of the Dtypes are objects. So, now let's set it as float or int.","25b036cb":"The next function will return two data frames that will help us to understand the differences between the labels.\n * The first data frame will contain the total of sessions and revenue of the Search Queries per label and the size of each label. \n\n * The second data frame will return the mean of the features per labels, so then we will have a good understanding of the differences.\n \n\n","c0b4099b":"## 5.6 -  5 Features Representation - 3D Plot - Kmeans<a id=\"5frKM\"><\/a>","91645c1b":"## 8.3 - Tabular Overview - Gaussian Mixture  <a id=\"toGM\"><\/a>","c223e636":"# 10 - Conclusion <a id=\"CON\"><\/a>","ac5df0f2":"<a id=\"DC\"><\/a>\n## 4.1 - Data Cleaning","703cde8f":"## 8.5 Mean Of The Features - Polar Plot - Gaussian Mixture <a id=\"mfppGM\"><\/a>","5ef0ed9d":"We can identify 4 types of Search Queries.\n\n* Queries with rejection\n* Queries that bring buyers\n* Queries that bring interested users\n* Queries that bring a lot of sessions of minimally interested users.\n\nNow, we will label the df mean and the df total with the name of each, merge them into one unique dataframe and index per label and Algo Clusters and make our choice. ","88d184ff":"The last cluster algorithm that we tested in this project is the **Gaussian Mixture**.  The documentation explains that algo as;\n\n \"*A probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians.*\"\n\n\nSee more about [ Gaussian Mixture ](https:\/\/scikit-learn.org\/stable\/modules\/mixture.html#gmm) \n","c1e80958":"## 5.2 - Clusters Label - Kmeans <a id=\"alcKM\"><\/a>","8d5dad34":"# 7 -  Agglomerative Clustering <a id=\"AC\"><\/a>","ebb8f06c":"After the function, we will split the df per label and use Display to show the head of each label.\n","09045005":"## 6.6 - Features Representation - Spectral Clustering <a id=\"5frSC\"><\/a>","dde1709a":"## 7.7 - Principal Component Analysis (PCA) - Scatter Plot - Agglomerative Clustering  <a id=\"pcaspAC\"><\/a>","ffe8302b":"## 6.3 - Tabular Overview - Spectral Clustering <a id=\"toSC\"><\/a>","89eec0c4":"## 8.8 Summary Description - Gaussian Mixture  <a id=\"sdGM\"><\/a>","d56d87e9":"The data was exported from Google Analytics, in a very hidden  option in Acquisition > Google Ads > Search Queries After that look for -> Site Usage (Secret Button) in the top left of the user graph of the period.\n<br>\n<br>\n\n![Screen%20Shot%202021-01-01%20at%2011.16.43.png](attachment:Screen%20Shot%202021-01-01%20at%2011.16.43.png)\n\n<br>\n<br>\n\n\nNow we have information about the Search Queries that bring users to the site and metrics related with behavior and ecommerce. The features are: \n\n\n* **Search Query** - The actual search query that triggered users to the site. However, here due to the privacity of the website, we modify it for random letter sequence.\n* **Users** - Users who have initiated at least one session during the date range. Learn more about how Analytics calculates the number of users.\n* **Sessions** - Total number of Sessions within the date range. A session is the period time a user is actively engaged with your website, app, etc. All usage data (Screen Views, Events, Ecommerce, etc.) is associated with a session.\n* **Pages \/ Session** - Pages\/Session (Average Page Depth) is the average number of pages viewed during a session. Repeated views of a single page are counted.\n* **Avg. Session** - The average length of a Session.\n* **% New Session** - An estimate of the percentage of first time visits.\n* **Goal Completions** - The total number of conversions.\n* **Revenue** - The total revenue from web ecommerce or in-app transactions. Depending on your implementation, this can include tax and shipping.\n\nThe shape of the data is 5000 rows and 8 columns.\n","c0b72cd3":"## 7.1 Silhouette Score - Agglomerative Clustering <a id=\"ssAC\"><\/a>","4c09b313":"Before applying the cluster models we will check the data distribution. To do that, we will use the Box plot for all the features.","b24b2d67":"<a id=\"Dc\"><\/a>\n# 4 - Data Check","305871dc":"##  Principal Component Analysis (PCA) - Scatter Plot - Gaussian Mixture  <a id=\"pcaspGM\"><\/a>","d90f3305":"For a bidimensional visualization, we are going to use the Principal Component Analysis(PCA) which is one of the most popular linear dimension reductions. \n\nPCA is a projection based method which transforms the data by projecting it onto a set of orthogonal axes.\n\nTo reduce the features we are going to use sklearn decomposition PCA and to plot px Scatter.","b818ef60":"To choose our best algorithme we analyzed the most important features of the labels:\n<br>\n<br>\n    \n   Label | Most Important Features.\n   ------|-----------------------\n   Buyer | Revenue\n   Intereste_User | Bounce rate and Avg Session Duration\n   Rejection | Bounce Rate, Avg Session Duration\n   Sessions_Minimally |Interested - Sessions - Sessions and Avg Session Duration\n   \n   \n<br>\n<br>\n\nAll of the algorithmes show very good clusters, but for us the most important thing here is to present a meaningful cluster.<br> We choose the **Gaussian Mixture** for some reasons. \n\n* SQ_Buyer-GM - Shows the higher revenue and also, this cluster algo was the unique that created a group with all buyers in only one label.\n\n* SQ_Intereste_User-GM - Shows the smaller Avg. Session Duration and the Higher Bounce rate, however the amount is good enough once the Avg. Session Duration mean of the df is 2.45 The size of that cluster is the larger when compared with the others algos to\n\n* SQ_Rejection-GM - Shows the higher Bounce rate and the smaller Avg Session Duration and Session mean per SQ.\n\n* SQ_Sessions_Minimally_Interested-GM - Shows the smaller Session mean. However the Total sessions is the higher, also that algo have the second higher Avg. Session Duration.\n\n\n\nLet's replace the clusters_gm with the labels ","32ae0263":"We just found the best clustering model for Pay Per Click Search Query extracted from Google Analytics and  accomplished our objective with this project. \n\nWe can see that a good silhouette score is not always decisive. Gaussian Mixtures have the smaller silhouette score, however when we look close to the continued values We can identify a few informations that make the Gaussian Mixture stand out. \n\nThe clusters identified for the algorithm are very consistentes in saying that there are 4 groups of queries. \n\n* Queries with rejection\n* Queries that bring buyers\n* Queries that bring interested users\n* Queries that bring a lot of sessions of minimally interested users.\n\n\nWith that information we can set strategies redus rejection, know what queries bring more interested users, how we can bring more sessions to the web site and also improve the revenue.\n\n\n If you know some company  that needs this kind of consulting please contact me at contato@bloco-b.com. \n\nIf you like this  notebook, please upvote and leave your feedback in the comments section!\nIt motivates me a lot to keep doing those types of projects and sharing here.\n\nThat's it for now.\n\n\nCheers \ud83c\udf40\n","2d1928e4":"Now, to plot all features in one single chart, we are going to use the px.scatter_3d. Defining \nx as 'Sessions', y as 'Bounce Rate', z as 'Revenue' the size as 'Avg. Session Duration' and colors as our labels.\n","85a3652c":"## 7.6 - 5 Features Representation - Agglomerative Clustering  <a id=\"5frAC\"><\/a>","bb174d3d":"The correlation matrix and the plot help us to choose the features to be dropped.\n\n* **'Users'** - Is very strong correlated with 'Sessions' the pearson value is 0.935921\n* **'Pages \/ Session'** - Is moderated correlated with 'Avg. Session Duration'. The pearson value is 0.760620\n* **'Goal Completions'** - Is very strongly correlated with Sessions. The pearson value is 0.997876.\n\nAfter a few tests we see that even the **'% New Session'** don't having a strong correlation with any other feature, the clusters are better defined without that feature. For that reason we will drop that feature as well.\n\n","b168ee32":"# 9 -  The Best Choice <a id=\"TBC\"><\/a>","83b6ffff":"The best silhouette score was 0.71 for 4 n_clusters.","12c6ab69":" # Google Ads Search Queries Clustering","9d4c1e7c":"## 5.3 - Tabular Overview - Kmeans<a id=\"toKM\"><\/a>","85b0032a":"<a id=\"introduction\"><\/a>\n# 1 - Introduction","0b4e49c3":"# 8 - Gaussian Mixture  <a id=\"GM\"><\/a>","3b96776a":"## 7.8 -  Summary Description - Agglomerative Clustering  <a id=\"sdAC\"><\/a>","63cc300b":"We will label 2 data frames.\nThe original data and the normalized data.\n","829e7acc":"## 8.1 - Clusters Label - Gaussian Mixture  <a id=\"alcGM\"><\/a>","413d7a64":"# 5 - Kmeans <a id=\"KM\"><\/a>\n\nKMeans is the most popular cluster algorithm. It tries to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares. \n \n\nThe best explanation that I read about the Kmean algorithm was in [documentation](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#k-means):\n\n   \"*In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose  samples from the dataset . After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly*\"\n","0dc104a9":"**Kmeans**\n\n**Number of Clusters** - 4\n\n**Silhouette Score** - 0.71\n\n**Cluster 0** - **'Search Queries'** with zero **\"Revenue\"**,the second largest **\"Sessions mean\"**. \n\nMost Relevant Features: **\"Bounce Rate\"**, **\"Sessions\"**, **\"Avg. Session Duration\"**.\n\n**Cluster 1** - Larger amount of **'Search Queries'**, and with the higher **\"Bounce Rate mean\"**. \n\nMost Relevant Features: **\"Bounce Rate\"**\n\n**Cluster 2** - **'Search Queries'** with a smaller number of **\"Sessions\"** and with the higher **\"AVG. Session Duration\"** and smaller **\"Bounce Rate mean\"** . \n\nMost relevant features: **\"Bounce Rate\"** and **\"AVG. Session Duration\"**\n\n**Cluster 3** - Larger amount of **\"Revenue\"** with smaller amount of **'Search Queries'**. Higher **\"Sessions mean\"** and **\"Revenue mean\"** .\n\nMost relevant features: **\"Revenue\"**, **\"Bounce Rate\"**, **\"Sessions\"** and **\"Avg. Session Duration\"**\n\n","fd456191":"## 6.5 - Mean of Features Polar Plot - Spectral Clustering <a id=\"mfppSC\"><\/a>","f59cf1fa":"The next function will return a bar plot using go.Bar to visualize the size of the label.","193e55d6":"## 7.7 - Principal Component Analysis (PCA) - Spectral Clustering <a id=\"pcaspSC\"><\/a>","fbdeaadf":"To make it easy for the algorithme identify the clusters and minimize the effect of outliers and noise in the data, we should transform to normal distribution. For that we will use numpy log and after that we will use sklearn preprocessing package to scale the data. ","614b0ff2":"As suggested by Mr. Letelier in his article, a good way to understand our labels is by the means of their features. Linear polar plot is perfect for this task because we can plot lines inside of a circle that can handle many variables. To do that we will use px.line_polar.","5b14afc9":"## 8.2 - Silhouett Score - Gaussian Mixture  <a id=\"ssGM\"><\/a>","f4a77616":"The third cluster algorithm that we tested in this project is the **Agglomerative Clustering**. That algo starts by treating each object as a singleton cluster.  A service deployed across multiple cluster nodes, which is never active in more than one node concurrently.  After that, pairs of clusters are successively merged until all clusters have been merged into one big cluster containing all objects. The result is a tree-based representation of the objects, named dendrogram. \n\n\nSee more in [Data Novia](https:\/\/www.datanovia.com\/en\/lessons\/agglomerative-hierarchical-clustering\/)\n","a22bd27e":"## 6.4 - Size -  Bar Plot - Spectral Clustering <a id=\"sbpSC\"><\/a>","859a5bfa":"## 5.5 - Mean Of The Features - Polar Plot - Kmeans<a id=\"mfppKM\"><\/a>","7f0b5467":"Before applying the ML cluster models in our data set, we need to choose the best features and preprocess the data.\n\nFor dropping, we are going to analyse the correlation of the features. High correlation represents the same characteristic of a segment. It is not useful for ML algorithms. To do the correlation analyse, we will use the .corr() method to find the pairwise correlation of all columns in the dataframe. After that we will plot a scatter matrix about all features:\n","02e21e47":"## 7.3 - Tabular Overview - Agglomerative Clustering <a id=\"toAC\"><\/a>","c18f2057":"## 8.6 - 5 Features Representation - Gaussian Mixture <a id=\"5frGM\"><\/a>","ad9d4ac2":"**Agglomerative Clustering**\n\n**Number of Clusters** - 4\n\n**Silhouette Score** - 0.68\n\n**Cluster 0** - **'Search Queries'** with zero **\"Revenue\"**, larger amount of **\"Search Queries\"**. Smaller **\"Session mean\"**, **\"Avg. Session Duration\"** and higher **\"Bounce Rate mean\"**. \n\nMost relevant feature: **\"Bounce Rate\"**.\n\n**Cluster 1** - Second larger number of **\"Sessions\"**, total of zero **\"Revenue\"**. Second large **\"Session mean\"**. \n\nMost relevant feature: **\"Bounce Rate\"**, **\"Sessions\"** and **\"Avg. Session Duration\"**. \n\n**Cluster 2** - **'Search Queries'** with a second large amount of **\"Revenue\"** and smaller amount of **\"Sessions\"**. The larger **\"Avg. Session Duration\"**, smaller **\"Bounce Rate\"** and second smaller **\"Session mean\"\"**. \n\nMost relevant feature: **\"Bounce Rate\"** and **\"Avg. Session Duration\"**\n\n**Cluster 3** - Smaller **'Search Queries'** label and with the large amount of **\"Sessions\"** and **\"Revenue\"**. \nHigher **\"Session mean\"** and **\"Revenue mean\"**, second smaller **\"Bounce Rate mean\"**. \n\nMost relevant feature: **\"Revenue\"**, **\"Bounce Rate\"**, **\"Session\"** and **\"Avg. Session Duration\"**","99cf7371":"I really wanna know what you guys think about that project. I'm open to all improvements, rewording or criticism. Please don't hesitate to leave me a comment or upvote. \nI will appreciate it.","30343464":"## 6.2 - Clusters Label - Spectral Clustering <a id=\"alcSC\"><\/a>","e035f4d4":"**Gaussian Mixture**\n\n**Number of Clusters** - 4\n\n**Silhouette Score** - 0.60\n\n**Cluster 0** - **'Search Queries'** with zero **\"Revenue\"**, second largest amount of **\"Sessions\"**. Second larger **\"Sessions mean\"**. \n\nMost relevant feature: **\"Bounce Rate\"**, **\"Sessions\"**, **\"Avg. Session Duration\"**\n\n\n**Cluster 1** - Larger number of **'Search Queries'**. Zero **\"Revenue\"**. Larger **\"Bounce Rate mean\"**  and smaller **\"Session mean\"**. \n\nMost relevant feature: Maybe for the reason that most of the **'Search Queries'** features are zero there are no important features in this case. \n\n**Cluster 2** - Second larger amount of **'Search Queries'** also with zero **\"Revenue\"**. Smaller **\"Bounce Rate mean\"** second large **\"Avg. Session Duration\"** and second smaller **\"Session mean\"**. \n\nMost relevant feature: **\"Bounce Rate\"** and **\"Avg. Session Duration\"**\n\n**Cluster 3** - **'Search Queries'** with the smaller size and with the large amount of **\"Sessions\"** and **\"Revenue\"**. Higher **\"Session mean\"** and **\"Revenue mean\"**, second smaller **\"Bounce Rate mean\"** and higher **\"Avg. Session Duration\"**.\n\nMost relevant feature: **\"Revenue\"**, **\"Bounce Rate\"**, **\"Sessions\"**, **\"Avg. Session Duration\"**.\n","974481c2":"The main objective of the project is to find the best clustering models for Pay Per Click Search Query extracted from Google Analytics.\n\nOnce each Query  click costs money, clustering and labeling the search queries with information about user behavior and ecommerce metrics can give us valuable insights that could reduce the cost of the campaign, allocating the budget better to improve the results. \n\nMore than have a good score, here we are going to look for meaningful cluster labels.\n\nI really wanna thank [Mr. Mauricio Letelier](https:\/\/www.linkedin.com\/in\/maletelier\/), the writer of [this amazing post on Medium](https:\/\/towardsdatascience.com\/clustering-with-more-than-two-features-try-this-to-explain-your-findings-b053007d680a) that helped me alot to accomplish this project. \n\n\n\nHave fun :)\n\n","616f97f7":"## 7.4 - Size - Bar Plot -  Agglomerative Clustering <a id=\"sbpAC\"><\/a>","f29a3049":"## 7.5 - Mean Of The Features - Polar Plot - Agglomerative Clustering <a id=\"mfppAC\"><\/a>","ba753b14":"<a id=\"dist\"><\/a>\n## 4.3 -  Distribution","b370d6aa":"To have a identify the patterns, here we display the \"mean tables\" of the clusters using the function that uses pd.qcut (Quantile-based discretization function.) and pd.cut (Bin values into discrete intervals.). So then we will see the low, medium and high values."}}