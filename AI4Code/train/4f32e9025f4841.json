{"cell_type":{"03832b3a":"code","b9720128":"code","0b16dd95":"code","3e6feb24":"code","3565b982":"code","f87f4c15":"code","dc655f95":"code","3c33cfb8":"code","37289d52":"code","6f0f4a45":"code","03f40762":"code","86cf85b0":"code","2ebbfa8a":"code","fb53d135":"code","cd3df975":"code","0f07c538":"code","1f83dd23":"code","5dd1a807":"code","3643839f":"code","7d440252":"code","3efde045":"markdown","1d847ff7":"markdown","6a2b436e":"markdown","1208dda3":"markdown","923c6fab":"markdown","f38d3e22":"markdown"},"source":{"03832b3a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b9720128":"#import the packaged that will be use for this analysis\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.linear_model import RidgeCV, ElasticNet\nfrom lightgbm import LGBMRegressor\nimport xgboost as xgb\nfrom xgboost import plot_importance","0b16dd95":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')","3e6feb24":"#review the dataset for trainning and testing prediction models\ndf_train","3565b982":"#review the meterials for the final prediction\ndf_test","f87f4c15":"# review whether there is a missing value\nmsno.matrix(df_train)\nplt.show()\nprint(df_train.isna().sum())","dc655f95":"# get the categorical columns\ncat_bool = (df_train.dtypes == 'object')\ncat_columns = df_train.columns[cat_bool].tolist()\nprint(df_train[cat_columns].head())","3c33cfb8":"# get the numeric columns\nnum_columns = [col for col in df_train.iloc[:, :-1].columns if df_train[col].dtype in ['float64']]\nprint(df_train[num_columns].head())","37289d52":"# review the correlation between numeric features\ncorr = df_train[df_train.columns[-cat_bool]].iloc[:, 1:].corr()\n\nsns.set_context(\"notebook\")\nplt.subplots(figsize=(12, 8))\ng1 = sns.heatmap(corr, cmap = 'YlOrBr', annot=True, fmt='.2f')\ng1.set_title(\"Correlation between Numeric Columns\", y=1)\nplt.xlabel('Numeric Column')\nplt.ylabel('Numeric Column')\nplt.xticks(rotation=0)\nplt.yticks(rotation=0)\nplt.show()","6f0f4a45":"# review the frequency of the variables in categorical columns\nplt.figure(figsize=(12,16))\nsns.set_context(\"notebook\")\nplt.style.use('ggplot') \nplt.subplots_adjust(hspace=0.6, wspace=0.6)\nsns.set_palette(\"Spectral\")\nfor i, cat_col in enumerate(cat_columns):\n    plt.subplot(5, 3, i+1)\n    sns.countplot(x=cat_col, data=df_train, \n                  order=df_train[cat_col].value_counts().index)\n    plt.title(cat_col)\nplt.show()","03f40762":"# review the distribution of the numeric variables\nplt.figure(figsize=(12,16))\nplt.subplots_adjust(hspace=0.6, wspace=0.6)\nfor i, num_col in enumerate(num_columns):\n    plt.subplot(5, 3, i+1)\n    sns.histplot(df_train[num_col], bins=20, kde=True, color='grey')\n    plt.title(num_col)\nplt.show()","86cf85b0":"# choose the features and target\nX = df_train.iloc[:, 1:-1]\ny = df_train.iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=44)","2ebbfa8a":"# prepare a process pipeline to get dummy variables for categorical columns\nnum_process = SimpleImputer(strategy='constant')\ncat_process = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_process, num_columns),\n        ('cat', cat_process, cat_columns)], \n        remainder = 'passthrough')","fb53d135":"# define a function to run the models\ndef run_reg_model(model, X_train, y_train, X_test, y_test, params):\n    GS = GridSearchCV(model, param_grid=params, scoring='neg_mean_squared_error', cv=3)\n    GS.fit(X_train, y_train)\n    y_pred = GS.predict(X_test)\n    mape = np.mean(np.abs(y_pred - y_test)\/np.abs(y_test))\n    me = np.mean(y_pred - y_test)\n    mae = np.mean(np.abs(y_pred - y_test))\n    mpe = np.mean((y_pred - y_test)\/y_test)\n    rmse = np.sqrt(np.abs(GS.best_score_))\n    corr = np.corrcoef(y_pred, y_test)[0,1]\n    \n    print('Accuracy:', )\n    print('MAE:', mae)\n    print('RMSE:', rmse)\n    print('Best Parameters:', GS.best_params_)\n\n    return mape, me, mae, mpe, rmse, corr","cd3df975":"#XGBRegressor\nprint('\\nXGBRegressor:')\nmodel = xgb.XGBRegressor(objective=\"reg:squarederror\")\nsteps = [('preprocessor', preprocessor), ('model', model)]\npipeline = Pipeline(steps)\nxgb_params = {'model__colsample_bytree': [0.3, 0.7], \n              'model__n_estimators': [50, 100, 200], \n              'model__max_depth': [2, 5]}\nmape_xgb, me_xgb, mae_xgb, mpe_xgb, rmse_xgb, corr_xgb = run_reg_model(pipeline, X_train, y_train, X_test, y_test, xgb_params)\n\n#Ridge\nprint('\\nRidge:')\nmodel = RidgeCV()\nsteps = [('preprocessor', preprocessor), ('model', model)]\npipeline = Pipeline(steps)\nrg_params = {\"model__fit_intercept\":[True, False]}\nmape_rg, me_rg, mae_rg, mpe_rg, rmse_rg, corr_rg = run_reg_model(pipeline, X_train, y_train, X_test, y_test, rg_params)\n\n#ElasticNet\nprint('\\nElasticNet:')\nmodel = ElasticNet()\nsteps = [('preprocessor', preprocessor), ('model', model)]\npipeline = Pipeline(steps)\nls_params = {\"model__fit_intercept\":[True, False]}\nmape_en, me_en, mae_en, mpe_en, rmse_en, corr_en = run_reg_model(pipeline, X_train, y_train, X_test, y_test, ls_params)\n\n#LGBM\nprint('\\nLGBMRegressor:')\nmodel = LGBMRegressor()\nsteps = [('preprocessor', preprocessor), ('model', model)]\npipeline = Pipeline(steps)\nlgbm_params = {'model__learning_rate': [0.005, 0.01],\n             'model__boosting_type' : ['gbdt', 'dart']}\nmape_lgbm, me_lgbm, mae_lgbm, mpe_lgbm, rmse_lgbm, corr_lgbm = run_reg_model(pipeline, X_train, y_train, X_test, y_test, lgbm_params)","0f07c538":"# visualize the MAE and RMSE of the predictions\nlinear_x = ['XGBRegressor', 'Ridge', 'ElasticNet', 'LGBMRegressor'] \nlinear_mae = [mae_xgb, mae_rg, mae_en, mae_lgbm]\nlinear_rmse = [rmse_xgb, rmse_rg, rmse_en, rmse_lgbm]\n\nlinear_mae = [round(num, 4) for num in linear_mae]\nlinear_rmse = [round(num, 4) for num in linear_rmse]\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=linear_x, y=linear_mae, name='MAE', \n                     marker_color='pink', text=linear_mae, \n                     textposition='auto'))\nfig.add_trace(go.Bar(x=linear_x, y=linear_rmse, name='RMSE', \n                     marker_color='lightblue', text=linear_rmse, \n                     textposition='auto'))\nfig.update_layout(barmode='group', xaxis_tickangle=0)\nfig.update_layout({'showlegend':True})\nfig.update_xaxes(\n        title_text = \"Machine Learning Models\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.update_yaxes(\n        title_text = \"Errors\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.update_layout(title_text=\"MAE and RMSE to Evaluate the Prediction (The Lower, The Better)\", \n                  title_x=0.5, title_y=0.88)\nfig.show()","1f83dd23":"# prepare the taget dataset by removing the ID column \nX_target = df_test.iloc[:, 1:]\nX_target","5dd1a807":"# apply the taget dataset to XGBRegressor with the optinal parameters\nmodel = xgb.XGBRegressor(objective=\"reg:squarederror\")\nsteps = [('preprocessor', preprocessor), ('model', model)]\npipeline = Pipeline(steps)\nxgb_params = {'model__colsample_bytree': [0.3], \n              'model__max_depth': [5], \n              'model__n_estimators': [100]}\nGS = GridSearchCV(pipeline, param_grid=xgb_params, scoring='neg_mean_squared_error', cv=3)\nGS.fit(X_train, y_train)\nGS_best = GS.best_estimator_\ny_target = GS_best.predict(X_target)\nprint('RMSE:', np.sqrt(np.abs(GS.best_score_)))","3643839f":"# review the structure of the predition data\nprint(len(y_target))\nprint(df_test.shape)","7d440252":"# assign the result to the target IDs and then output a final .csv file\ndf_taget = df_test.assign(target=y_target)\ndf_taget[['id', 'target']].to_csv('submission.csv', index=False)\nsubmission = pd.read_csv('submission.csv')\nsubmission","3efde045":"## 3.2 Exploratory Data Analysis Summary\nThe given dataset does not have missing data. Regarding the visualization, here are the insights below.\n\n1. There is a higher correlation between count0, count5, count7, count8, count9, count10, count11, and count12 columns.\n\n2. Regarding the bar charts and histograms, it seems these columns do not have a normal distribution.\n\nIn regards to viewpoint 2, it\u00a0might cause noise for the prediction. This analysis chooses not to normalize the dataset and see what the outcome is.\n\n# 4 Analyze\n## 4.1 Model Selection\nIn this section, the analysis will try several regression models and then choose an ideal model for the final prediction. The step would be as follows.\n\n1. Prepare the data for training and testing models.\n2. Run the models, and then review the MAE and RMSE to pick a better model.\n3. Use the ideal model to predict the target dataset\n4. Save the result as a .csv file","1d847ff7":"This heatmap shows the correlation coefficient between numeric variables. There is a higher correlation between count0, count5, count7, count8, count9, count10, count11, and count12 columns.","6a2b436e":"# 5 Conclusion\nThis analysis aims to select a prediction model for insurance claims. The dataset reflects a real-world situation in which getting a better prediction would help a company make a better plan for the business.\n\nRegarding the EDA, the findings are as below.\n\n1. Count0, count5, count7, count8, count9, count10, count11, and count12 correlate to each other.\n\n2. The variables in some categorical columns do not have a normal distribution.\n\nAlthough the distribution situation might cause noise for the prediction, this analysis will do an early analysis stage without normalizing the dataset. Further analysis will try to normalize the dataset to see whether the prediction could be improved.\n\nNext, this analysis chose 4 regression models to predict as follows.\n\n1. XGBRegressor\n2. Ridge\n3. ElasticNet\n4. LGBM\n\nBy reviewing the MAE and RMSE of the outcomes, XGBRegressor performs a better prediction with lower MAE and RMSE numbers. Thus, this analysis applied the target dataset to XGBRegressor and then got the final prediction, submission.csv,\n\nAs for further analysis, here are the suggestions.\n\n1. Try another algorithm that might help the prediction.\n\n2. Keep tuning the parameters of models to seek a better result.\n\n3. Since the variables of some columns do not have a normal distribution,  normalization might be a way to improve the prediction.\n\n# 6 Reference\nAcharya, S. (2021). What are RMSE and MAE? Towards Data Science. Retrieved from https:\/\/towardsdatascience.com\/","1208dda3":"# 1 Introduction\n## 1.1 Summary\nThis dataset is based on a real dataset dealing with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features. The purpose of this analysis is to get a predicted amount that helps a company do a better plan for the business.\n\n## 1.2 Analytics Tool and Dataset\nThis report uses Python as the analytics tool. The given dataset contains 10 categorical columns (cat0 - cat9) and 14 numeric columns with continuous variables (cont0 - cont13) despite ID and target columns. 300,000 pieces of data are used for training and testing the prediction model. 200,000 pieces of data are the subject to get the prediction.\n\n# 2 Prepare\n## 2.1 Analysis Plan\nThe analysis plan is to answer the questions.\n\n1. What is the correlation between numeric features?\n2. How does the distribution of the features look like?\n3. How accurate is the prediction model?\n4. What is the prediction for the 200,000 pieces of data?\n\n## 2.2 Performance Metrics\nThis analysis will use MAE (Mean Absolute Error) and RMSE (Root Mean Squared Error) to evaluate the performance of several prediction models. RMSE and MAE are usually used for evaluating Regression Models. These metrics indicate the deviation from the actual values and how accurate the predictions are (Acharya, 2021). The lower errors, the better accuracy.\n\n## 2.3 \u00a0Method\nThe analysis follows the steps as below.\n\n1. Review the data structure and clean it if it is necessary.\n2. Proceeding Exploratory Data Analysis to review the data structure.\n3. Predict the target insurance claim by several models and pick an optimal method\u00a0to go through the final prediction.\n4. Use the selected model with a better parameter to get the prediction file.\n\n# 3 Process\n## 3.1 Exploratory Data Analysis\nThis section involves the preparation of the dataset, including data cleansing. Then, an EDA (Exploratory Data Analysis) will help understand the main characteristics of the dataset.","923c6fab":"After the review, there is no missing value in this dataset. Thus, Here will go forward to the EDA section.","f38d3e22":"So far, XGBRegressor performs a better prediction with lower MAE and RMSE. Thus, the next step will apply the target dataset to XGBRegressor with the ideal parameter. "}}