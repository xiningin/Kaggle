{"cell_type":{"0fc9a71a":"code","9220640f":"code","8759c58d":"code","67ac3355":"code","6b4702e3":"code","63ba33a3":"code","efb78bae":"code","426b8099":"code","3c0dd3b1":"code","25dfb6cd":"code","384b23a9":"code","aba4ccc1":"code","376d2c8c":"code","acc0cf08":"code","38d5f23f":"code","e7b423ea":"code","eb89184d":"code","5c7e1ee7":"code","d3410494":"code","70450e94":"code","1997a9ff":"code","44a92367":"code","3e7fc046":"code","8d1cfe26":"code","f0aa474d":"code","15480e44":"code","21555bec":"code","203487f9":"code","840538de":"code","a3c87814":"code","ebf23c8c":"code","4a0ac80f":"code","98db8b3b":"code","1d363651":"code","59d43daf":"code","18cdf3c4":"code","2bd72ee9":"code","72d0f7ad":"code","d6a4fa92":"code","433f8360":"code","8223f2b2":"code","f1bb7ca1":"code","f6f35235":"code","f4271435":"code","e7a6fd35":"code","fd1f245f":"code","88727c49":"code","e1a28e2c":"code","1c2e1a19":"code","c7b7ffa2":"code","8e1c0574":"code","8f8f1dd3":"code","85d65c5b":"code","4367e9df":"code","e8cf296d":"code","0fc7913a":"code","8f24b6ec":"code","4c713951":"code","60a7a631":"code","aeb32dbd":"code","f7c1cb2f":"code","ef82bf2c":"code","dfed7904":"code","fad0d288":"code","af6fae8c":"code","02807a1d":"code","3178ff9e":"code","1638da0a":"markdown","831bc2be":"markdown","3414e94e":"markdown","e70b40a5":"markdown","5ccec45c":"markdown","c876ed31":"markdown","13389dde":"markdown","4ba22cf8":"markdown","85d27403":"markdown","1e58e9e5":"markdown","cf529fa7":"markdown","09419af3":"markdown","4fb45368":"markdown","908fbc37":"markdown","69119f83":"markdown","b8824fb1":"markdown","3e8a2a36":"markdown","bb63156d":"markdown","daba51af":"markdown"},"source":{"0fc9a71a":"#######  Author: Midhun Kumar #########\n########## Version: 1.0 ###############","9220640f":"#Importing Necessary Libraries for ploting and data Visualization \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#Supress warnings \nimport warnings \nwarnings.filterwarnings('ignore')\n\n#sklearn Libraries\n#from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV,KFold\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso\nfrom sklearn.metrics import r2_score\nfrom sklearn.feature_selection import RFE\n","8759c58d":"#Importing dataset\nmaster_df = pd.read_csv(\"..\/input\/house-price-data\/train.csv\")","67ac3355":"#understanding the data\nmaster_df.head()","6b4702e3":"master_df.shape","63ba33a3":"#understanding the data types and columns\nmaster_df.info()","efb78bae":"#Understanding Null values\nround(master_df.isnull().sum()\/len(master_df)*100,2).sort_values(ascending=False).head(50)","426b8099":"#Dropping top 6 Columns as they have more than 15 % Null values \nhouse_price_child_df = master_df.drop([\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\",\"LotFrontage\"], axis=1)","3c0dd3b1":"#Removing rows having more than 5 null values\nprint(len(house_price_child_df [house_price_child_df .isnull().sum(axis=1) > 5].index))\nhouse_price_child_df  = house_price_child_df [house_price_child_df .isnull().sum(axis=1) <=5 ]","25dfb6cd":"print(len(house_price_child_df [house_price_child_df .isnull().sum(axis=1) > 5].index))","384b23a9":"round(house_price_child_df .isnull().sum()\/len(house_price_child_df )*100,2).sort_values(ascending=False).head(50)","aba4ccc1":"house_price_child_df.info()","376d2c8c":"#checking Yr and Year columns in dataset\ncolumns = list(house_price_child_df.columns)\nyear_columns =[]\nfor word in columns:\n    if \"Year\" in word or \"Yr\" in word:\n            year_columns.append(word)\nprint(\"Below columns having Year\")\nyear_columns","acc0cf08":"#Need to work on the Years columns\nhouse_price_child_df[year_columns].sample(10)","38d5f23f":"house_price_child_df[year_columns].max()","e7b423ea":"#Adding years old columns for each year columns\nhouse_price_child_df['YearBuilt_old'] = house_price_child_df.YearBuilt.max()-house_price_child_df.YearBuilt\nhouse_price_child_df['YearRemodAdd_Old'] = house_price_child_df.YearRemodAdd.max()-house_price_child_df.YearRemodAdd\nhouse_price_child_df['GarageYrBlt_Old'] = house_price_child_df.GarageYrBlt.max()-house_price_child_df.GarageYrBlt\nhouse_price_child_df['YrSold_Old'] = house_price_child_df.YrSold.max()-house_price_child_df.YrSold","eb89184d":"#Will drop the year columns now\nhouse_price_child_df = house_price_child_df.drop(['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold'],axis=1)","5c7e1ee7":"house_price_child_df.info()","d3410494":"#Verifying the new columns by indexing iloc\nhouse_price_child_df.iloc[:,71:].sample(10)","70450e94":"#Lets identify the NaN values \nhouse_price_child_df.isna().sum().sort_values(ascending=False).head(20)","1997a9ff":"# Will take first 12 values and impute them \nimpute_cols = list(house_price_child_df.isna().sum().sort_values(ascending=False).head(13).index)","44a92367":"#Understanding the impute columns\nhouse_price_child_df[impute_cols].info()","3e7fc046":"house_price_child_df[impute_cols].sample(10)","8d1cfe26":"house_price_child_df[impute_cols].max()","f0aa474d":"# First We fill this to numeric values\nhouse_price_child_df.MasVnrArea.fillna(house_price_child_df.MasVnrArea.mean(),inplace=True)\n\n# We will fill 0 in place of GarageYrBlt_Old NaN values as they dont have GarageYrBlt_Old\nhouse_price_child_df.GarageYrBlt_Old.fillna(-1,inplace=True)","15480e44":"house_price_child_df[impute_cols].sample(10)","21555bec":"# Lets check the maximum count of each values\nimpute_cols.remove('MasVnrArea')\nimpute_cols.remove('GarageYrBlt_Old')\nfor feature in impute_cols:\n    #print('Countin-', feature)\n    print( house_price_child_df[feature].value_counts())","203487f9":"#Now will fill the NaN with max values\nhouse_price_child_df.MasVnrType.fillna('None',inplace=True)\nhouse_price_child_df.BsmtQual.fillna('TA',inplace=True)\nhouse_price_child_df.BsmtCond.fillna('TA',inplace=True)\nhouse_price_child_df.BsmtExposure.fillna('No',inplace=True)\nhouse_price_child_df.BsmtFinType1.fillna('Unf',inplace=True)\nhouse_price_child_df.BsmtFinType2.fillna('Unf',inplace=True)\nhouse_price_child_df.GarageType.fillna('Attchd',inplace=True)\nhouse_price_child_df.GarageFinish.fillna('Unf',inplace=True)\nhouse_price_child_df.GarageQual.fillna('TA',inplace=True)\nhouse_price_child_df.GarageCond.fillna('TA',inplace=True)\nhouse_price_child_df.Electrical.fillna('SBrkr',inplace=True)","840538de":"#verify the impute columns\nhouse_price_child_df.isna().sum().sort_values(ascending=False).head(20)\n","a3c87814":"# Now will check higly correalted coloumns with sales price\nprint(house_price_child_df.corr()['SalePrice'].sort_values())\ncorr_drop_col = house_price_child_df.corr()['SalePrice'].sort_values().head(12).index","ebf23c8c":"#dropping less corelated coloumns\nhouse_price_child_df = house_price_child_df.drop(corr_drop_col,axis=1)","4a0ac80f":"house_price_child_df.shape","98db8b3b":"# Lets extract higly corrrealted columns and plot them\ncorr_colms = house_price_child_df.corr()['SalePrice'].sort_values().tail(6).index\n","1d363651":"sns.pairplot(house_price_child_df[corr_colms],kind='reg')","59d43daf":"fig_size = plt.rcParams[\"figure.figsize\"]\nfig_size[0] =16.0\nfig_size[1] = 4.0\n\nx =house_price_child_df['SalePrice']\nplt.hist(x, bins=400)\nplt.ylabel('SalePrice');","18cdf3c4":"def reject_outliers(SalePrice):\n    filtered= [e for e in (house_price_child_df['SalePrice']) if (e < 450000)]\n    return filtered\n\nfig_size = plt.rcParams[\"figure.figsize\"]\nfig_size[0] =16.0\nfig_size[1] = 4.0\n\nfiltered = reject_outliers('SalePrice')\nplt.hist(filtered, 50)\nfig_size[0]=16.0\nfig_size[1]=8.0\nplt.show()\n\ndf_no_outliers = pd.DataFrame(filtered)\ndf_no_outliers.shape","2bd72ee9":"house_price_child_df = house_price_child_df[house_price_child_df['SalePrice']<450000]","72d0f7ad":"# now lets work on the catogarical variables \nprint(list(house_price_child_df.dtypes[house_price_child_df.dtypes=='object'].index))\ncat_var = (list(house_price_child_df.dtypes[house_price_child_df.dtypes=='object'].index))","d6a4fa92":"# Lets work on this columns and try maping with numeric values with dummies\nhouse_price_child_df[cat_var].head()","433f8360":"#encoder = OneHotEncoder(drop='first')\n#encoder.fit(house_price_child_df[cat_var])\n#house_price_child_df = encoder.fit_transform(house_price_child_df[cat_var])\ndummy_var = pd.get_dummies(house_price_child_df, columns=cat_var,drop_first=True)\nhouse_price_child_df = pd.concat([house_price_child_df,dummy_var],axis=1)","8223f2b2":"house_price_child_df.shape","f1bb7ca1":"# droping atogorical variable presents\nhouse_price_child_df = house_price_child_df.drop(cat_var, axis=1)","f6f35235":"house_price_child_df.shape","f4271435":"#lest verify the distribution\nplt.figure(figsize=(16,6))\nsns.distplot(house_price_child_df.SalePrice)\nplt.show()","e7a6fd35":"X = house_price_child_df.loc[:, house_price_child_df.columns !='SalePrice' ]\ny = house_price_child_df.loc[:, house_price_child_df.columns == 'SalePrice']\ny = y.iloc[:, 1:2] #Somewhere Sales price got addedd twice so removing :)\n\n# split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 100)","fd1f245f":"len(X_train.columns)","88727c49":"lm  = LinearRegression()\nlm.fit(X_train,y_train)\nrfe = RFE(lm,70)\nrfe.fit(X_train,y_train)","e1a28e2c":"rfe_scores = pd.DataFrame(list(zip(X_train.columns,rfe.support_,rfe.ranking_)))\nrfe_scores.columns = ['Column_Names','Status','Rank']","1c2e1a19":"#We having 245 features so let remove some of them via RFE\nrfe_sel_columns = list(rfe_scores[rfe_scores.Status==True].Column_Names)","c7b7ffa2":"rfe_sel_columns","8e1c0574":"#Selceting optimal Values selected by RFE\nX_train = X_train[rfe_sel_columns]\nX_test = X_test[rfe_sel_columns]","8f8f1dd3":"lm = Lasso(alpha=0.001)\nlm.fit(X_train,y_train)\n\ny_train_pred = lm.predict(X_train)\nprint(r2_score(y_true=y_train,y_pred=y_train_pred))\n\ny_test_pred  = lm.predict(X_test)\nprint(r2_score(y_true=y_test,y_pred=y_test_pred))","85d65c5b":"model_parameter = list(lm.coef_)\nmodel_parameter.insert(0,lm.intercept_)\nmodel_parameter = [x for x in model_parameter]\ncol = X_train.columns\ncol.insert(0,'Constant')\nlist(zip(col,model_parameter))","4367e9df":"# grid search CV\n\n# set up cross validation scheme\nfolds = KFold(n_splits=10,shuffle=True,random_state=42)\n\n# specify range of hyperparameters\nhyper_param = {'alpha':[0.001, 0.01, 0.1,1.0, 5.0, 10.0]}\n\n\n# grid search\n# lasso model\nmodel = Lasso()\n\nmodel_cv = GridSearchCV(estimator = model,\n                        param_grid=hyper_param,\n                        scoring='r2',\n                        cv=folds,\n                        verbose=1,\n                        return_train_score=True\n                       )\n\nmodel_cv.fit(X_train,y_train)","e8cf296d":"cv_result_l = pd.DataFrame(model_cv.cv_results_)\ncv_result_l['param_alpha'] = cv_result_l['param_alpha'].astype('float32')\ncv_result_l.head()","0fc7913a":"# ploting the \nplt.figure(figsize=(16,8))\nplt.plot(cv_result_l['param_alpha'],cv_result_l['mean_train_score'])\nplt.plot(cv_result_l['param_alpha'],cv_result_l['mean_test_score'])\nplt.xscale('log')\nplt.ylabel('R2 Score')\nplt.xlabel('Alpha')\nplt.show()","8f24b6ec":"\n# Checking the best parameter(Alpha value)\nmodel_cv.best_params_","4c713951":"# Verifying with the best Alpah Value\nlasso = Lasso(alpha = 10.0)\nlasso.fit(X_train,y_train)\n\ny_train_pred = lasso.predict(X_train)\ny_test_pred = lasso.predict(X_test)\n\nprint(r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(r2_score(y_true=y_test,y_pred=y_test_pred))","60a7a631":"# Identifying the Top Feature\nmodel_param = list(lasso.coef_)\nmodel_param.insert(0,lasso.intercept_)\ncols = X_train.columns\ncols.insert(0,'const')\nlasso_coef = pd.DataFrame(list(zip(cols,model_param)))\nlasso_coef.columns = ['Featuere','Coef']","aeb32dbd":"lasso_coef.sort_values(by='Coef',ascending=False).head(10)","f7c1cb2f":"ridge = Ridge(alpha=0.001)\nridge.fit(X_train,y_train)\n\ny_train_pred = ridge.predict(X_train)\nprint(r2_score(y_train,y_train_pred))\ny_test_pred = ridge.predict(X_test)\nprint(r2_score(y_test,y_test_pred))","ef82bf2c":"folds  = KFold(n_splits=10,shuffle=True,random_state=42)\n\nhyper_param = {'alpha':[0.001,0.01,0.1,0.2,0.5,0.9,1.0, 5.0, 10.0]}\n\nmodel = Ridge()\n\nmodel_cv = GridSearchCV(estimator=model,\n                        param_grid=hyper_param,\n                        scoring='r2',\n                        cv=folds,\n                        verbose=1,\n                        return_train_score=True)\n\nmodel_cv.fit(X_train,y_train)","dfed7904":"cv_result_r = pd.DataFrame(model_cv.cv_results_)\ncv_result_r['param_alpha'] = cv_result_r['param_alpha'].astype('float32')\ncv_result_r.head()","fad0d288":"plt.figure(figsize=(16,8))\nplt.plot(cv_result_r['param_alpha'],cv_result_r['mean_train_score'])\nplt.plot(cv_result_r['param_alpha'],cv_result_r['mean_test_score'])\nplt.xlabel('Alpha')\n# plt.xscale('log')\nplt.ylabel('R2 Score')\nplt.show()","af6fae8c":"\n# Checking the best parameter(Alpha value)\nmodel_cv.best_params_","02807a1d":"ridge = Ridge(alpha = 5.0)\nridge.fit(X_train,y_train)\n\ny_pred_train = ridge.predict(X_train)\nprint(r2_score(y_train,y_pred_train))\n\ny_pred_test = ridge.predict(X_test)\nprint(r2_score(y_test,y_pred_test))","3178ff9e":"# Verifying with the best Alpah Value\nlasso = Lasso(alpha = 10.0)\nlasso.fit(X_train,y_train)\n\ny_train_pred = lasso.predict(X_train)\ny_test_pred = lasso.predict(X_test)\n\nprint(r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(r2_score(y_true=y_test,y_pred=y_test_pred))","1638da0a":"### `Lets try with Lasso`","831bc2be":"`We could see this two numeric values and also if no garage is availbile which means GarageYrBlt_Old could be NaN`","3414e94e":"### `My next 5 Features would be`\n1. RoofMatl_Roll\t\n2.\tCondition2_PosN\t\n3.\tNeighborhood_NridgHt\t\n4.\tSaleCondition_Partial\t\n5.\tSaleType_New\t","e70b40a5":"### `now lets take a look at outliers and will remove them`","5ccec45c":"`so we can remove 14 sampls from data set`","c876ed31":"`lets try to improve our model with the optimal value of alpha using GridSearchCV`","13389dde":"`We have 38 catogarical variables will add dummey variables to them`","4ba22cf8":"`we can see there is a huge varience in outlier so let remove them `","85d27403":"`we could see some of the coloumns having NaN values`","1e58e9e5":"### `So far we have reduced some features as much as we can, now lets do some plots and check the EDA`","cf529fa7":"`From above plot we could see the modle normaly distributed`","09419af3":"## `Lets check the Regression R2 Score`","4fb45368":"### `From above r2_score we can finalize lasso is needed model so i will choose lasso for the modling and also we can easily eliminate the Features with Lasso`","908fbc37":"## `Now lets use the ridge regression`","69119f83":"### `Best Alpha value for lasso = 10.0`\n### `Best Alpha Value for lasso = 5.0`","b8824fb1":"Note:\n\n`So far, we able to understand the data and deletd the top 6 Null values coloums which has more than 15 % null values`\n\n`Also we could see some coloumns having Year values which we need to convert somthing understandable for model`\n","3e8a2a36":"## `Final Model`","bb63156d":"### `My Top 5 Features would be`\n1. Exterior1st_CBlock\t\n2. RoofMatl_WdShngl\t\n3. \tRoofMatl_Metal\t \n4. RoofMatl_Tar&Grv\t\n5. \tRoofMatl_WdShake\t","daba51af":"`Above Alpha value is not optimal,Now  lets try to improve our model with the optimal value of alpha using GridSearchCV`"}}