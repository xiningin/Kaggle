{"cell_type":{"22642018":"code","9204788a":"code","89545a35":"code","826c6d3d":"code","2b4a7b5e":"code","157de2e7":"code","59698967":"code","8ce3a626":"code","903b24f4":"code","895d115c":"code","53f33e36":"code","93e96f84":"code","513dc04a":"code","a9fcc753":"code","ab3bee69":"code","1016ca77":"code","cea8e1a2":"code","48c26ef1":"code","b8ef7fe0":"code","c0cfec4e":"code","f792f698":"code","2d9b24a3":"code","6742a33a":"code","de443209":"code","736eca13":"code","f7356c52":"code","88f5681b":"code","81839005":"code","2539750c":"code","52e18d11":"markdown","75edecbd":"markdown","1a0fa979":"markdown","2aea62df":"markdown","38cea044":"markdown","ccabdbdb":"markdown","cd19af87":"markdown","450c3781":"markdown","539f0f4d":"markdown","ceeb7558":"markdown","572ae4b8":"markdown","f2d17e4a":"markdown","4966f180":"markdown"},"source":{"22642018":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport csv\nfrom time import time\nimport json\n\nimport re\nimport string\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom gensim.models import FastText\n\nfrom keras.layers import Input, Dense, Embedding, Flatten, Dropout, SpatialDropout1D # General\nfrom keras.layers import CuDNNLSTM, Bidirectional # LSTM-RNN\nfrom keras.optimizers import Adam\n\nfrom keras import backend as K\nfrom keras.callbacks import EarlyStopping\n\nimport tensorflow as tf\n\n# Evaluation\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","9204788a":"df = pd.read_csv('..\/input\/ndsc-beginner\/train.csv')","89545a35":"table = str.maketrans('','', string.punctuation)\n\ndef removeNumbersAndPunctuations(text):\n    text = text.translate(table)\n    text = re.sub(r'\\d+', '', text)\n    return text","826c6d3d":"df['title'] = df['title'].apply(removeNumbersAndPunctuations)","2b4a7b5e":"X_train, X_test, y_train, y_test = train_test_split(df['title'], df['Category'], test_size=0.16, random_state=42)","157de2e7":"print('loading word embeddings...')\nembeddings_index = {}\nf = open('..\/input\/ftembeddings300all\/ftembeddings300all.txt', encoding='utf-8')\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('found %s word vectors' % len(embeddings_index))","59698967":"y_train = pd.get_dummies(y_train)\ny_test = pd.get_dummies(y_test)","8ce3a626":"NUM_CATEGORIES = 58\nMAX_SEQUENCE_LENGTH = 30\nMAX_NB_WORDS = 20000\nEMBED_DIM = 300\nHIDDEN = 256","903b24f4":"tok = Tokenizer(num_words=MAX_NB_WORDS, lower=True) \ntok.fit_on_texts(X_train)","895d115c":"word_index = tok.word_index\nprint('Found %s unique tokens.' % len(word_index))","53f33e36":"sequences = tok.texts_to_sequences(X_train)\ntrain_dtm = sequence.pad_sequences(sequences,maxlen=MAX_SEQUENCE_LENGTH)\n\ntest_sequences = tok.texts_to_sequences(X_test)\ntest_dtm = sequence.pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH)","93e96f84":"print('Shape of Train DTM:', train_dtm.shape)","513dc04a":"print('preparing embedding matrix...')\nwords_not_found = []\nNUM_WORDS = min(MAX_NB_WORDS, len(word_index))\nembedding_matrix = np.zeros((NUM_WORDS, EMBED_DIM))\nfor word, i in word_index.items():\n    if i >= NUM_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n    else:\n        words_not_found.append(word)\nprint('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\nprint(\"sample words not found: \", np.random.choice(words_not_found, 10))","a9fcc753":"def RNN_Model():\n    text_sequence = Input(shape=(MAX_SEQUENCE_LENGTH,), name='TEXT_SEQUENCE_INPUT')\n    \n    rnn_layer = Embedding(NUM_WORDS, EMBED_DIM, weights=[embedding_matrix], trainable=False, name='EMBEDDING')(text_sequence) \n    rnn_layer = SpatialDropout1D(0.5, name='EMBEDDING_DROPOUT')(rnn_layer)\n    rnn_layer = Bidirectional(CuDNNLSTM(HIDDEN, return_sequences=True), name='BILSTM_LAYER1')(rnn_layer)\n    rnn_layer = Bidirectional(CuDNNLSTM(HIDDEN), name='BILSTM_LAYER2')(rnn_layer)\n    rnn_layer = Dropout(0.5,name='RNN_DROPOUT')(rnn_layer)\n\n    output = Dense(NUM_CATEGORIES, activation='softmax', name='OUTPUT')(rnn_layer)\n    model = Model(inputs=text_sequence, outputs=output)\n    \n    return model","ab3bee69":"K.clear_session()\nmodel = RNN_Model()\nmodel.summary()","1016ca77":"ea = EarlyStopping(monitor='val_categorical_accuracy', patience=3, restore_best_weights=True)\nadam = Adam(lr=0.001, decay=0.000049, epsilon=1e-8)","cea8e1a2":"model.compile(loss='categorical_crossentropy',optimizer=adam, metrics=['categorical_accuracy'])\nmodel.fit(train_dtm, y_train, batch_size=128, epochs=30, validation_data=(test_dtm,y_test), verbose=1, callbacks=[ea])","48c26ef1":"ea2 = EarlyStopping(monitor='val_categorical_accuracy', patience=3, restore_best_weights=True)\nadam2 = Adam(lr=0.001, decay=0.00006, epsilon=1e-8)","b8ef7fe0":"model.layers[1].trainable = True\nmodel.compile(loss='categorical_crossentropy',optimizer=adam2, metrics=['categorical_accuracy'])\nmodel.fit(train_dtm, y_train, batch_size=128, epochs=20, validation_data=(test_dtm,y_test), verbose=1, callbacks=[ea2])","c0cfec4e":"model.evaluate(test_dtm, y_test)","f792f698":"y_pred = [np.argmax(pred) for pred in model.predict(test_dtm)]\ny_truth = [np.argmax(truth) for truth in y_test.values]","2d9b24a3":"with open('..\/input\/ndsc-beginner\/categories.json', 'rb') as handle:\n    catNames = json.load(handle)\n\ncatNameMapper = {}\nfor category in catNames.keys():\n    for key, value in catNames[category].items():\n        catNameMapper[value] = key","6742a33a":"catNameLabelsSorted = ['SPC', 'Icherry', 'Alcatel', 'Maxtron', 'Strawberry', 'Honor', 'Infinix', 'Realme', \n                       'Sharp', 'Smartfren', 'Motorola', 'Mito', 'Brandcode', 'Evercoss', 'Huawei', \n                       'Blackberry', 'Advan', 'Lenovo', 'Nokia', 'Sony', 'Asus', 'Vivo', 'Xiaomi', 'Oppo', \n                       'Iphone', 'Samsung', 'Others Mobile & Tablet', 'Big Size Top', 'Wedding Dress', \n                       'Others', 'Crop Top ', 'Big Size Dress', 'Tanktop', 'A Line Dress', 'Party Dress', \n                       'Bodycon Dress', 'Shirt', 'Maxi Dress', 'Blouse\\xa0', 'Tshirt', 'Casual Dress', \n                       'Lip Liner', 'Setting Spray', 'Contour', 'Other Lip Cosmetics', 'Lip Gloss', 'Lip Tint', \n                       'Face Palette', 'Bronzer', 'Highlighter', 'Primer', 'Blush On', 'Concealer', 'Lipstick', \n                       'Foundation', 'Other Face Cosmetics', 'BB & CC Cream', 'Powder']","de443209":"catNamePred = list(map(lambda x: catNameMapper[x], y_pred))\ncatNameActual = list(map(lambda x: catNameMapper[x], y_truth))","736eca13":"confMat = confusion_matrix(catNamePred, catNameActual, labels=catNameLabelsSorted)","f7356c52":"fig, ax = plt.subplots(figsize=(30,30))\nsns.heatmap(confMat, annot=True, fmt='d', xticklabels=catNameLabelsSorted, yticklabels=catNameLabelsSorted)\nplt.ylabel('PREDICTED')\nplt.xlabel('ACTUAL')\nplt.show()","88f5681b":"test_data = pd.read_csv('..\/input\/ndsc-beginner\/test.csv')\ntest_data['title'] = test_data['title'].apply(removeNumbersAndPunctuations)\n\ntest_sequences = tok.texts_to_sequences(test_data.title)\ntest_dtm = sequence.pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH)\n\ny_pred = [np.argmax(pred) for pred in model.predict(test_dtm)]\ntest_data['Category'] = y_pred","81839005":"test_data","2539750c":"df_submit = test_data[['itemid', 'Category']].copy()\ndf_submit.to_csv('submission_svc.csv', index=False)","52e18d11":"## Train with Frozen Embedding Layer\nCurrently, the Embedding Layer's trainable parameter is set to False. The weights of the neural network, less the embedding matrix, is random at initialization. What we want to do here is transfer the learned context and semantic meaning within the pre-trained embeddings to this neural network. \nIf the trainable parameter is set to True when the neural network is still untrained, it would confuse the pre-trained embedding matrix, with the possibility of causing it to lose most of things it had learnt. ","75edecbd":"### Remove Numbers and Punctuations","1a0fa979":"## Submission","2aea62df":"## Continue Training with Unfrozen Embedding Layer\nNow that the neural network is more or less trained, we unfreeze the embedding layer in hopes that it could continue to train further.","38cea044":"### Split dataset into Train and Test","ccabdbdb":"### Create RNN Model","cd19af87":"### Create One-Hot for Train and Test Ys","450c3781":"### Constants","539f0f4d":"### Create Embedding Matrix","ceeb7558":"We see that the model finds it hard to distinguish the dresses, and the Other Mobile and Tablets items are spread out across the rest of the categories. ","572ae4b8":"### Load Embeddings\nOur embeddings are trained on the training dataset using FastText's Skipgram model. For more details on FastText, you can visit the link here: https:\/\/fasttext.cc\/","f2d17e4a":"### Create Sequence Matrices (Features) for Train, Test","4966f180":"# FastText is so Powerful It Scares Me"}}