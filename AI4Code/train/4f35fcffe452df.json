{"cell_type":{"1e67e390":"code","62bdc21e":"code","eff49ed7":"code","1ee88de6":"code","f1a38780":"code","360f308e":"code","0ec251fc":"code","6be094e3":"code","0a8d5c37":"markdown"},"source":{"1e67e390":"import os\nimport torch\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom collections import OrderedDict, namedtuple\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport joblib\n\nimport logging\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\nimport sys\nfrom sklearn import metrics, model_selection\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","62bdc21e":"tokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')","eff49ed7":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","1ee88de6":"df_train1 = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\", usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\ndf_train2 = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\", usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\ndf_train_full = pd.concat([df_train1, df_train2], axis=0).reset_index(drop=True)\ndf_train = df_train_full.sample(frac=1).reset_index(drop=True).head(200000)\n\ndf_valid = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv', \n                       usecols=[\"comment_text\", \"toxic\"])\n\ndf_train = pd.concat([df_train, df_valid], axis=0).reset_index(drop=True)\ndf_train = df_train.sample(frac=1).reset_index(drop=True)\n\ndf_test = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv\")","f1a38780":"%%time\nx_train = regular_encode(df_train.comment_text.values, tokenizer, maxlen=192)\nx_valid = regular_encode(df_valid.comment_text.values, tokenizer, maxlen=192)\nx_test  = regular_encode(df_test.content.values,       tokenizer, maxlen=192)","360f308e":"np.save('x_train',x_train)\nnp.save('x_valid',x_valid)\nnp.save('x_test',x_test)","0ec251fc":"np.save('df_train_toxic',df_train.toxic.values)\nnp.save('df_valid_toxic',df_valid.toxic.values)","6be094e3":"np.save('test_df_ids',df_test.id.values)","0a8d5c37":"# XLM-Roberta Large tokenize dataset\n\nThis kernel tokenizes the whole (train+test) dataset ahead of time and saves it in npy file format for later loading in order to save time during training and inference.\n\nBased on [abhishek's](https:\/\/www.kaggle.com\/abhishek\/bert-multi-lingual-tpu-training-8-cores-w-valid) and [xhlulu's](https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-xlm-roberta) kernels."}}