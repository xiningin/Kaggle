{"cell_type":{"7c807e24":"code","e4c2b7bf":"code","0c227db9":"code","2f807167":"code","df1920f5":"code","a330066b":"code","41bf01a6":"code","3cbf9ef9":"code","51571e14":"code","e902c597":"code","a926eb5a":"code","631d0d89":"code","149d2a69":"code","4e2b7b19":"code","b2ba9b0a":"code","8be4fa64":"code","f5d71222":"code","17a82565":"code","04bf9f61":"code","5e1e8d83":"code","f4368e18":"code","50859d11":"markdown","b1a1609e":"markdown","d91df145":"markdown","9650e8a7":"markdown","450ce251":"markdown","3745fffc":"markdown","0dd2a16c":"markdown","96773666":"markdown","5d3e623a":"markdown","0ccb6aa4":"markdown","fd6f2b7e":"markdown"},"source":{"7c807e24":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n\nimport graphviz\nimport itertools\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn import tree\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e4c2b7bf":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","0c227db9":"df.shape","2f807167":"df.tail(10)","df1920f5":"df.groupby('diagnosis').size()","a330066b":"df.isnull().sum()","41bf01a6":"df = df.drop([\"Unnamed: 32\",\"id\"],axis = 1)\nX = df.drop(\"diagnosis\",axis = 1)\nY = df['diagnosis']\n\nX.head()","3cbf9ef9":"M = df[df.diagnosis == \"M\"]\nB = df[df.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"Malign\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"Benign\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","51571e14":"# Box plot\nfig, axes =  plt.subplots(1, 2, figsize=(15,5))\nsns.boxplot(ax = axes[0], x=df.diagnosis, y=df['area_mean'] ,palette=\"turbo\")\naxes[0].set_title('Size Difference')\n\nsns.boxplot(ax = axes[1], x=df.diagnosis, y=df['perimeter_mean'] ,palette=\"PRGn\")\naxes[1].set_title('Size Difference')\n\nplt.show()\n","e902c597":"# let's optimize our data\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2,random_state = 42)","a926eb5a":"fig =  plt.subplots(1, 1, figsize=(10,8))\ngrp = sns.heatmap(x_train.corr(),cmap=\"Spectral_r\",annot = False)","631d0d89":"model = tree.DecisionTreeClassifier(max_depth= 3 , min_samples_leaf=12)\n\nmodel.fit(x_train,y_train)\n\nACC = model.score(x_train,y_train)    \nprint(\"Train dataset Accuracy = %\"+ str(ACC*100))\n\nACC = model.score(x_test,y_test)    \nprint(\"Test dataset Accuracy = %\"+ str(ACC*100))\n\ny_pred = model.predict(x_test)\n\nACC = metrics.accuracy_score(y_test,y_pred)    \nprint(\"Accuracy = %\"+ str(ACC*100))","149d2a69":"# Confusion Matrix\nimport scikitplot.metrics as splt\n\nsplt.plot_confusion_matrix(y_test,y_pred,figsize=(7,7))\nplt.show()","4e2b7b19":"#\n\nattributes_name = X.columns.values\n\ndef Decision_Tree_draw (mdl,names):\n    ds = tree.export_graphviz(mdl,out_file = None,\n                             feature_names = names,\n                             class_names = ['Malign' ,'Benign'],\n                             filled = False, rounded = True,\n                             special_characters =False)\n    \n    grp = graphviz.Source(ds)\n    return grp\n\nDecision_Tree_draw(model,attributes_name)","b2ba9b0a":"X = X.drop(['smoothness_worst','perimeter_worst','radius_worst'],axis =1)\n\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.1,random_state = 42)\n\nmodel = tree.DecisionTreeClassifier(max_depth= 3 , min_samples_leaf=12)\n\nmodel.fit(x_train,y_train)\n\nACC = model.score(x_train,y_train)    \nprint(\"Train dataset Accuracy = %\"+ str(ACC*100))\n\nACC = model.score(x_test,y_test)    \nprint(\"Test dataset Accuracy = %\"+ str(ACC*100))\n","8be4fa64":"# RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\n\nmodel.fit(x_train,y_train)\n\nACC = model.score(x_train,y_train)    \nprint(\"Train dataset Accuracy = %\"+ str(ACC*100))\n\nACC = model.score(x_test,y_test)    \nprint(\"Test dataset Accuracy = %\"+ str(ACC*100))\n\ny_pred = model.predict(x_test)\n\nACC = metrics.accuracy_score(y_test,y_pred)    \nprint(\"Accuracy = %\"+ str(ACC*100))\n","f5d71222":"# K-NN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel =KNeighborsClassifier(3)\n\nmodel.fit(x_train,y_train)\n\nACC = model.score(x_train,y_train)    \nprint(\"Train dataset Accuracy = %\"+ str(ACC*100))\n\nACC = model.score(x_test,y_test)    \nprint(\"Test dataset Accuracy = %\"+ str(ACC*100))\n\ny_pred = model.predict(x_test)\n\nACC = metrics.accuracy_score(y_test,y_pred)    \nprint(\"Accuracy = %\"+ str(ACC*100))\n","17a82565":"# support vector machine\nfrom sklearn.svm import SVC\n\nsvm = SVC(random_state=1)\n\nsvm.fit(x_train,y_train)\n\nACC = svm.score(x_train,y_train)    \nprint(\"Train dataset Accuracy = %\"+ str(ACC*100))\n\nACC = svm.score(x_test,y_test)    \nprint(\"Test dataset Accuracy = % \"+ str(ACC*100))","04bf9f61":"from sklearn.ensemble import BaggingClassifier\n\nclf = BaggingClassifier(base_estimator=SVC(),\n                       n_estimators=10, random_state=0)\n\nclf.fit(x_train,y_train)\n\nACC = clf.score(x_train,y_train)    \nprint(\"Train dataset Accuracy = %\"+ str(ACC*100))\n\nACC = clf.score(x_test,y_test)    \nprint(\"Test dataset Accuracy = % \"+ str(ACC*100))","5e1e8d83":"from sklearn.ensemble import AdaBoostClassifier\n\nclf = AdaBoostClassifier(n_estimators=100, random_state=0)\n\nclf.fit(x_train,y_train)\n\nACC = clf.score(x_train,y_train)    \nprint(\"Train dataset Accuracy = %\"+ str(ACC*100))\n\nACC = clf.score(x_test,y_test)    \nprint(\"Test dataset Accuracy = % \"+ str(ACC*100))","f4368e18":"# Navi Bayes\nfrom sklearn.naive_bayes import MultinomialNB\n\nnb = MultinomialNB()\nnb.fit(x_train,y_train)\n\nACC = nb.score(x_train,y_train)    \nprint(\"Train dataset Accuracy = %\"+ str(ACC*100))\n\nACC = nb.score(x_test,y_test)    \nprint(\"Test dataset Accuracy = % \"+ str(ACC*100))","50859d11":"### Let's look at the correlation between the attributes","b1a1609e":"### EDA","d91df145":"## What will we learn from this project?\n* Bivariate Analysis\n\n* Comparison of Features\n* Many visualization techniques\n* Many of the popular Machine Learning techniques\\\n* confusion matrix\n","9650e8a7":"### Strongly correlated features cause poor generalization performance on the test set due to high variance and less model interpretability.\n\n### Another method is to use other classification algorithms\n","450ce251":"### Import Library","3745fffc":"<img src= \"https:\/\/idsb.tmgrup.com.tr\/ly\/uploads\/images\/2021\/10\/07\/150369.jpeg\">","0dd2a16c":"### Conclusion\n* **As a result, we learn;**\n *  **Sklearn library**\n *   **Correlation analysis**\n *   **Many Classifier algorithms.**\n *   **Visualization methods**\n *   **I will add new methods to this project in the future.** *\n \n \n * **If you found the content useful, don't forget to upvote and comment. Have a Kaggle day** *","96773666":"### Creat Models","5d3e623a":"### So how do we increase our accuracy rate?\n\n**I think we can discard features with high correlations.**","0ccb6aa4":"## Preprocessing\n\n* **As you can see we have two redundant features. We need to drop these columns 'id' and 'unnamed 32'**\n* **We have two labels M : Malign and B : Benign**","fd6f2b7e":"### Visualization"}}