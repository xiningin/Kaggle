{"cell_type":{"da714066":"code","56fd2dd8":"code","ae122585":"code","953abb61":"code","1a64664e":"code","8f931476":"code","6ffecd2c":"code","ff5ecf5c":"code","1f59dcc9":"code","9855dde5":"code","edc8c8df":"code","0d171c1f":"code","c3d0b754":"code","04b4c384":"code","4c7603bf":"code","4b0d1139":"code","a1859954":"code","e38701b0":"code","923ef533":"code","c12f8eb7":"markdown","8c7231a0":"markdown","dc551c09":"markdown","3a6dc9e6":"markdown","82282beb":"markdown","823b1021":"markdown","fd015a0d":"markdown","f4d44201":"markdown","d3806a80":"markdown","fcf5bae7":"markdown","0b60a846":"markdown","a588716b":"markdown","108c54ca":"markdown","2e8f6edb":"markdown"},"source":{"da714066":"import os\nimport json\nimport numpy as np\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\n\nrc('animation', html='jshtml')\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","56fd2dd8":"data_path = Path('\/kaggle\/input\/abstraction-and-reasoning-challenge')\n\ntrain_path = data_path \/ 'training'\neval_path = data_path \/ 'evaluation'\ntest_path = data_path \/ 'test'\n\ntrain_tasks = { task.stem: json.load(task.open()) for task in train_path.iterdir() }\neval_tasks = { task.stem: json.load(task.open()) for task in eval_path.iterdir() }\ntest_tasks = { task.stem: json.load(task.open()) for task in test_path.iterdir() }","ae122585":"cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\nnorm = colors.Normalize(vmin=0, vmax=9)\n    \ndef plot_pictures(pictures, labels):\n    fig, axs = plt.subplots(1, len(pictures), figsize=(2*len(pictures),32))\n    for i, (pict, label) in enumerate(zip(pictures, labels)):\n        axs[i].imshow(np.array(pict), cmap=cmap, norm=norm)\n        axs[i].set_title(label)\n    plt.show()\n    \ndef plot_sample(sample, predict=None):\n    if predict is None:\n        plot_pictures([sample['input'], sample['output']], ['Input', 'Output'])\n    else:\n        plot_pictures([sample['input'], sample['output'], predict], ['Input', 'Output', 'Predict'])\n        \ndef inp2img(inp):\n    inp = np.array(inp)\n    img = np.full((10, inp.shape[0], inp.shape[1]), 0, dtype=np.uint8)\n    for i in range(10):\n        img[i] = (inp==i)\n    return img\n\ndef input_output_shape_is_same(task):\n    return all([np.array(el['input']).shape == np.array(el['output']).shape for el in task['train']])\n\n\ndef calk_score(task_test, predict):\n    return [int(np.equal(sample['output'], pred).all()) for sample, pred in zip(task_test, predict)]","953abb61":"task = train_tasks[\"db3e9e38\"][\"train\"]\nfor sample in task:\n    plot_sample(sample)","1a64664e":"HIDDEN_SIZE = 128\nMAX_STEPS = 10\nTHRESHOLD = 0.99\nREMAINDERS_PEN = 0.0\n\n# Fix random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nclass CAModel(nn.Module):\n    def __init__(self, num_states):\n        super(CAModel, self).__init__()\n        self.embedding = nn.Sequential(\n            nn.Conv2d(num_states, HIDDEN_SIZE, kernel_size=1),\n            nn.InstanceNorm2d(HIDDEN_SIZE),\n        )\n        self.transition = nn.Sequential(\n            nn.InstanceNorm2d(HIDDEN_SIZE),\n            nn.Conv2d(HIDDEN_SIZE, HIDDEN_SIZE, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(HIDDEN_SIZE, HIDDEN_SIZE, kernel_size=1, padding=0),\n        )\n        self.projection_out = nn.Conv2d(HIDDEN_SIZE, num_states, kernel_size=1)\n        self.projection_halt = nn.Conv2d(HIDDEN_SIZE, 1, kernel_size=1)\n        \n    def forward(self, x, max_steps=None):\n        x = self.embedding(x)\n        # Initialize values\n        halting_probability = torch.zeros([1, 1, x.shape[2], x.shape[3]], device=x.device)\n        remainders = torch.zeros([1, 1, x.shape[2], x.shape[3]], device=x.device)\n        n_updates = torch.zeros([1, 1, x.shape[2], x.shape[3]], device=x.device)\n        # Cycle\n        max_steps = max_steps or MAX_STEPS\n        for i in range(max_steps):\n            p = torch.sigmoid(self.projection_halt(x) - 1)\n            # Formulas from https:\/\/arxiv.org\/pdf\/1807.03819.pdf APPENDIX C\n            still_running = (halting_probability <= THRESHOLD).to(torch.float)\n            new_halted = ((halting_probability + p * still_running) > THRESHOLD).to(torch.float) * still_running\n            still_running = ((halting_probability + p * still_running) <= THRESHOLD).to(torch.float) * still_running\n            halting_probability += p * still_running\n            remainders += new_halted * (1 - halting_probability)\n            halting_probability += new_halted * remainders\n            n_updates += still_running + new_halted\n            update_weights = p * still_running + new_halted * remainders\n            # Apply transformation to the state\n            transformed_state = self.transition(x)\n            # Interpolate transformed and previous states for non-halted inputs\n            x = transformed_state * update_weights + x * (1 - update_weights)\n            # Halt\n            if still_running.sum() == 0:\n                break\n        \n        x = self.projection_out(x)\n        self._remainders = remainders\n        self._n_updates = n_updates\n        return x","8f931476":"def solve_task(task, max_steps=10):\n    model = CAModel(10).to(device)\n    num_epochs = 100\n    criterion = nn.CrossEntropyLoss()\n    losses = np.zeros(num_epochs)\n    n_updates = np.zeros(num_epochs)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    for e in range(num_epochs):\n        optimizer.zero_grad()\n        loss = 0.0\n        \n        for sample in task:\n            # predict output from input\n            x = torch.from_numpy(inp2img(sample[\"input\"])).unsqueeze(0).float().to(device)\n            y = torch.tensor(sample[\"output\"]).long().unsqueeze(0).to(device)\n            y_pred = model(x)\n            loss += criterion(y_pred, y) + (model._remainders * REMAINDERS_PEN).mean(0).sum()\n            n_updates[e] += model._n_updates.detach().cpu().mean().numpy() \/ len(task)\n        \n        loss.backward()\n        optimizer.step()\n        losses[e] = loss.item()\n    return model, losses, n_updates\n\n@torch.no_grad()\ndef predict(model, task):\n    predictions = []\n    for sample in task:\n        x = torch.from_numpy(inp2img(sample[\"input\"])).unsqueeze(0).float().to(device)\n        pred = model(x).argmax(1).squeeze().cpu().numpy()\n        predictions.append(pred)\n    return predictions\n    \ntask = train_tasks[\"db3e9e38\"][\"train\"]\nmodel, losses, n_updates = solve_task(task)","6ffecd2c":"plt.plot(losses)","ff5ecf5c":"plt.plot(n_updates)","1f59dcc9":"predictions = predict(model, task)\nfor i in range(len(task)):\n    plot_sample(task[i], predictions[i])","9855dde5":"test = train_tasks[\"db3e9e38\"][\"test\"]\npredictions = predict(model, test)\nfor i in range(len(test)):\n    plot_sample(test[i], predictions[i])","edc8c8df":"def animate_solution(model, sample):\n    x = torch.from_numpy(inp2img(sample[\"input\"])).unsqueeze(0).float().to(device)\n\n    @torch.no_grad()\n    def animate(i):\n        pred = model(x, i)\n        im.set_data(pred.argmax(1).squeeze().cpu().numpy())\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(x.argmax(1).squeeze().cpu().numpy(), cmap=cmap, norm=norm)\n    return animation.FuncAnimation(fig, animate, frames=100, interval=120)\n    \nanim = animate_solution(model, train_tasks[\"db3e9e38\"][\"test\"][0])\nHTML(anim.to_jshtml())","0d171c1f":"def evaluate(tasks, is_test=False):\n    result = []\n    predictions = {}\n    for idx, task in tqdm(tasks.items()):\n        if input_output_shape_is_same(task):\n            model, _, _ = solve_task(task[\"train\"])\n            pred = predict(model, task[\"test\"])\n            if not is_test:\n                score = calk_score(task[\"test\"], pred)\n            else:\n                score = [0] * len(task[\"test\"])\n        else:\n            pred = [el[\"input\"] for el in task[\"test\"]]\n            score = [0] * len(task[\"test\"])\n\n        predictions[idx] = pred\n        result.append(score)\n    return result, predictions","c3d0b754":"train_result, train_predictions = evaluate(train_tasks)\ntrain_solved = [any(score) for score in train_result]\n\ntotal = sum([len(score) for score in train_result])\nprint(f\"solved : {sum(train_solved)} from {total} ({sum(train_solved)\/total})\")","04b4c384":"eval_result, eval_predictions = evaluate(eval_tasks)\neval_solved = [any(score) for score in eval_result]\n\ntotal_eval = sum([len(score) for score in eval_result])\nprint(f\"Eval: solved : {sum(eval_solved)} from {total_eval} ({sum(eval_solved)\/total})\")","4c7603bf":"test_result, test_predictions = evaluate(test_tasks, is_test=True)","4b0d1139":"for task, prediction, solved in tqdm(zip(train_tasks.values(), train_predictions.values(), train_solved)):\n    if solved:\n        for i in range(len(task['train'])):\n            plot_sample(task['train'][i])\n            \n        for i in range(len(task['test'])):\n            plot_sample(task['test'][i], prediction[i])","a1859954":"import pandas as pd\nsubmission = pd.read_csv(data_path \/ 'sample_submission.csv', index_col='output_id')\ndisplay(submission.head())","e38701b0":"def flattener(pred):\n    str_pred = str([list(row) for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred","923ef533":"for output_id in submission.index:\n    task_id = output_id.split('_')[0]\n    pair_id = int(output_id.split('_')[1])\n    f = str(test_path \/ str(task_id + '.json'))\n    with open(f, 'r') as read_file:\n        task = json.load(read_file)\n    # skipping over the training examples, since this will be naive predictions\n    # we will use the test input grid as the base, and make some modifications\n    #data = task['test'][pair_id]['input'] # test pair input\n    data = test_predictions[task_id][pair_id]\n    # for the first guess, predict that output is unchanged\n    pred_1 = flattener(data)\n    # for the second guess, change all 0s to 5s\n    data = [[5 if i==0 else i for i in j] for j in data]\n    pred_2 = flattener(data)\n    # for the last gues, change everything to 0\n    data = [[0 for i in j] for j in data]\n    pred_3 = flattener(data)\n    # concatenate and add to the submission output\n    pred = pred_1 + ' ' + pred_2 + ' ' + pred_3 + ' ' \n    submission.loc[output_id, 'output'] = pred\n\nsubmission.to_csv('submission.csv')\nsubmission.head()","c12f8eb7":"Fantastic! The coolest part now is that we can animate our solution to see the CA in action:","8c7231a0":"In my [previous notebook](https:\/\/www.kaggle.com\/teddykoker\/training-cellular-automata-part-i-game-of-life) we explored how we could use a CNN to create a cellular automata (CA) by recurrently passing the state of the grid through itself. Now we'll solve one of the tasks [arseny-n](https:\/\/www.kaggle.com\/arsenynerinovsky\/cellular-automata-as-a-language-for-reasoning) solved with a hard coded CA by learning the CA instead!","dc551c09":"That's all for now, thanks for reading!","3a6dc9e6":"## The Model\n\nThe model consists of a single 3x3 convolutional layer, followed by a 1x1 convolutional layer, just like my last notebook. Here `num_states` represents how many values a single cell could have; in this case 10, one for each color. Down the road, we may want to add a hidden state, concatinating it to the input, then removing it from the output.\n\nThe foward pass of the model will repeatedly pass the grid state through the CA transition for `steps` number of times.","82282beb":"## More Tasks\n\nNow that we know we can train a CA for one task, will it work on others?","823b1021":"We can see that the CA quickly gets to a solution and then stabilizes.","fd015a0d":"# Cellular automata (not really) CNN RNN with Adaptive Computation Time (ACT)\n### What I did:\n* copied [this notebook](https:\/\/www.kaggle.com\/teddykoker\/training-cellular-automata-part-ii-learning-tasks)\n* changed the model\n\nThe new model uses ACT concept from [this paper](https:\/\/arxiv.org\/pdf\/1603.08983.pdf). Also [this paper](https:\/\/arxiv.org\/pdf\/1807.03819.pdf) was useful.  \nACT replaces the original recurrency scheme.  \n\n### Results:\n* solves 26 train tasks instead of 20\n* twice faster\n* gifs aren't as beautiful as they were before\n* actually this is not cellular automata anymore\n\n---","f4d44201":"Now lets see if it at least correctly outputs the training set. To be save we'll give the model $n=100$ steps:","d3806a80":"We solve many of the tasks within the training set using our Neural Cellular Automata model! I did test on the validation set as well, and it correctly solved 17 of the tasks. There are a number of ways this model could be improved. Please let me know if you'd be interested in collaboration!\n\n## Solved Tasks","fcf5bae7":"## Training\n\nThis \"recurrent CNN\" can be quite to difficult to train. After trying a few ideas, this seemed to be the best approach that I encountered:\n\n* For every value $n$ = $1, ..., N$:\n    1. Train the model with $n$ `steps` to produce the output from input\n    2. Train the model with 1 `steps` to produce output from output\n        * This enforces that the CA stabilizes after reaching a solution\n        \nIn this way the model will try to get as close to a solution as possible in 1 step, then try to get closer in the next step, and so on until $N$ steps. For now I will use $N = 10$ = `max_steps`. I will also set the learning rate to decay with each additional step: $LR = 0.1 \/ (n * 2) $","0b60a846":"## First Task: db3e9e38\n\nThe task we'll first try is relitively straight foward; given a central orange \"pillar\", form stairs of alternating blue and orange in each direction. `arseny-n` showed that this could be solved with a CA consisting of three rules.","a588716b":"# Using the correct prediction format","108c54ca":"It works! Now lets see if it generalized to the test question:","2e8f6edb":"$n$ is incremented every 100 epochs, so we can see that it reaches a good solution after 3 steps (epoch 300)."}}