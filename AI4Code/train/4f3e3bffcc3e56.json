{"cell_type":{"fc3c8967":"code","c1244edb":"code","d76f1869":"code","e50cb6cb":"code","c7329b7c":"code","62f96e0d":"code","c011f34f":"code","7dd64b08":"code","4dfc55c8":"code","86d8b8f3":"code","56c358fa":"code","5101e9c3":"code","ea4c6567":"code","1a5085c5":"code","3bb0de96":"code","1dcd7d56":"code","f660b74c":"code","dfbd9ceb":"code","f9dead49":"code","0330b21e":"code","14ed9a73":"code","8ce99454":"code","47a190c9":"code","9854b97f":"code","b8e8d2ae":"code","c355e1f2":"code","16396126":"code","c6e65a78":"code","64c0058b":"code","af46b4e7":"markdown","dc2b7ed5":"markdown","96fc6225":"markdown","8928121b":"markdown","78c83dd0":"markdown","45c0737c":"markdown","d849c501":"markdown","80235815":"markdown","18f2946d":"markdown","4775ef0f":"markdown","b66b3f77":"markdown","763e9610":"markdown","7d42da4b":"markdown","e9982aea":"markdown","1e36903f":"markdown","9b842ecd":"markdown"},"source":{"fc3c8967":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('max_colwidth', 800)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport seaborn as sns\nimport matplotlib.pylab as plt\nfrom matplotlib import colors\n%matplotlib inline\n\nimport itertools\n\n#Preprocessing Library\nfrom sklearn import preprocessing\nimport re\nfrom scipy.stats import zscore\n\n#Regression Library\nfrom sklearn.linear_model import LinearRegression,ElasticNet,Lasso,Ridge,BayesianRidge\nfrom lightgbm.sklearn import LGBMRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom sklearn import neighbors\nfrom sklearn import svm\n\nfrom sklearn import metrics\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.model_selection import cross_val_predict,cross_val_score,cross_validate,train_test_split\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n#!pip install holidays\n#import holidays\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs","c1244edb":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","d76f1869":"train = reduce_mem_usage(pd.read_excel(\"..\/input\/Data_Train.xlsx\"))\ntest = reduce_mem_usage(pd.read_excel(\"..\/input\/Test_set.xlsx\"))","e50cb6cb":"train['Additional_Info'].unique()\ntrain.dtypes","c7329b7c":"train.isnull().sum()\ntest.isnull().sum()","62f96e0d":"#Best Fit from other look-up\ntrain['Route'].fillna('DEL \u2192 BLR \u2192 COK',inplace=True)\ntrain['Total_Stops'].fillna('1 stop',inplace=True)","c011f34f":"for combination in itertools.zip_longest(sorted(train['Airline'].unique()), sorted(test['Airline'].unique())):\n    print(combination)\nprint (\"-------------\")    \nfor combination in itertools.zip_longest(sorted(train['Source'].unique()), sorted(test['Source'].unique())):\n    print(combination)\nprint (\"-------------\")    \nfor combination in itertools.zip_longest(sorted(train['Destination'].unique()), sorted(test['Destination'].unique())):\n    print(combination)\nprint (\"-------------\")    \nfor combination in itertools.zip_longest(sorted(train['Total_Stops'].unique()), sorted(test['Total_Stops'].unique())):\n    print(combination)","7dd64b08":"plt.figure(figsize=(8,8))\nplt.hist(train.Price,bins=100,color='b')\nplt.title('Histogram of Flight Price')\nplt.show()","4dfc55c8":"train['Price'] = np.log(train['Price'])","86d8b8f3":"plt.figure(figsize=(8,8))\nplt.hist(train.Price,bins=100,color='b')\nplt.title('Histogram of Flight Price')\nplt.show()","56c358fa":"def tominutes(vr):\n    s=re.findall(\"\\d+\", str(vr))\n    if len(s)>1:\n        return (int(s[0]) *60 ) + int(s[1])\n    else:\n        return (int(s[0])*60)\n\n\nle = preprocessing.LabelEncoder()\nfor dataset in [train,test]:\n    dataset['airline_en']=le.fit_transform(dataset['Airline'])\n    dataset['doj_year_en']=pd.to_datetime(dataset['Date_of_Journey']).dt.year\n    dataset['doj_month_en']=pd.to_datetime(dataset['Date_of_Journey']).dt.month\n    dataset['doj_day_en']=pd.to_datetime(dataset['Date_of_Journey']).dt.day\n    dataset['doj_dayofyear_en']=pd.to_datetime(dataset['Date_of_Journey']).dt.dayofyear\n    dataset['doj_dayofweek_en']=pd.to_datetime(dataset['Date_of_Journey']).dt.dayofweek\n    dataset['doj_week_en']=pd.to_datetime(dataset['Date_of_Journey']).dt.week\n    dataset['doj_weekofyear_en']=pd.to_datetime(dataset['Date_of_Journey']).dt.weekofyear\n    dataset['doj_weekday_en']=pd.to_datetime(dataset['Date_of_Journey']).dt.weekday\n    dataset['doj_quarter_en']=pd.to_datetime(dataset['Date_of_Journey']).dt.quarter\n    dataset['ym']=dataset['doj_year_en'].astype(str).str.cat(dataset['doj_dayofyear_en'].astype(str),sep='.').astype(float)\n    #dataset.loc[pd.to_datetime(dataset['Date_of_Journey']).isin(ind_hol), 'hol_en'] = 0\n    \n    dataset['source_en']=le.fit_transform(dataset['Source'])\n    dataset.loc[(dataset.Destination=='New Delhi'),'Destination'] = 'Delhi'\n    dataset['dest_en']=le.fit_transform(dataset['Destination'])\n    dataset['route_merged']= dataset.Source.astype(str).str.cat(dataset.Destination.astype(str), sep='_')\n    dataset['route_merged_en']=le.fit_transform(dataset['route_merged'])\n    dataset['route_en']=le.fit_transform(dataset['Route'])\n    dataset['dep_hr_en']=pd.DatetimeIndex(dataset['Dep_Time']).hour\n    dataset['dep_min_en']=pd.DatetimeIndex(dataset['Dep_Time']).minute\n    dataset['dep_time_en']=dataset['dep_hr_en'].astype(str).str.cat(dataset['dep_min_en'].astype(str),sep='.').astype(float)\n    dataset['arr_hr_en']=pd.DatetimeIndex(dataset['Arrival_Time']).hour\n    dataset['arr_min_en']=pd.DatetimeIndex(dataset['Arrival_Time']).minute\n    dataset['arr_time_en']=dataset['arr_hr_en'].astype(str).str.cat(dataset['arr_min_en'].astype(str),sep='.').astype(float)\n    dataset['duration_en']=dataset['Duration'].map(lambda x: tominutes(x))\n    dataset['total_stops_en']=le.fit_transform(dataset['Total_Stops'])\n    dataset['Additional_Info_en']=le.fit_transform(dataset['Additional_Info'])\n    dataset['dep_slot_en']=le.fit_transform(pd.cut(pd.DatetimeIndex(dataset['Dep_Time']).hour,[-1, 12, 17, 24],labels=['Morning', 'Afternoon', 'Evening']))\n    dataset['arr_slot_en']=le.fit_transform(pd.cut(pd.DatetimeIndex(dataset['Arrival_Time']).hour,[-1, 12, 17, 24],labels=['Morning', 'Afternoon', 'Evening']))\n    #dataset['holiday_en'] = date(dataset['doj_year_en'],dataset['doj_month_en'],dataset['doj_day_en']).isin(ind_holidays)\n    #cleanup_stops = {\"Total_Stops\": {\"1 stop\": 1, \"2 stops\": 2,\"3 stops\":3,\"4 stops\":4,\"non-stop\":0}}\n    #dataset.replace(cleanup_stops, inplace=True)\n    #dataset['total_stops_ens']=dataset['Total_Stops'].astype(int)","5101e9c3":"# monthly change of prices\nym_summary = train.groupby(['ym'])['Price'].agg(['mean','count'])\n\nvmin = np.min(ym_summary['count'])\nvmax = np.max(ym_summary['count'])\nnorm = colors.Normalize(vmin,vmax)\n\nplt.figure(figsize=(15,7))\nplt.scatter(x=np.arange(ym_summary.shape[0]), y =ym_summary['mean'],c= ym_summary['count'],\n            s= ym_summary['count'],norm=norm ,alpha = 0.8, cmap='jet')\n\nplt.plot(np.arange(ym_summary.shape[0]), ym_summary['mean'] ,'--')\nplt.xticks(np.arange(ym_summary.shape[0]),ym_summary.index.values)\nplt.yscale('log')\nplt.xlabel('Year-Month')\nplt.ylabel('Price (log scale)')\nclb = plt.colorbar() \nclb.ax.set_title('number of sales')\nplt.title('Averge Price by Month')\nplt.show()","ea4c6567":"def scatter_p(fea,price):\n    n_f = len(fea)\n    n_row = n_f\/\/3+1\n    fig=plt.figure(figsize=(20,15))\n    i = 1\n    for f in fea:\n        x=train[f]\n        y=train[price]\n        m, b = np.polyfit(x, y, 1)    \n        \n        ax=fig.add_subplot(n_row,3,i)\n        plt.plot(x,y,'.',color='b')\n        plt.plot(x, m*x + b, '-',color='r')\n        plt.xlabel(f)\n        plt.ylabel(price)\n        i += 1","1a5085c5":"scatter_p(fea=['airline_en','source_en', 'dest_en','total_stops_en','route_en','Additional_Info_en'],price='Price')","3bb0de96":"scatter_p(fea=['dep_time_en','arr_time_en', 'duration_en','ym'],price='Price')","1dcd7d56":"train['log_dep_time_en'] = np.log(train['dep_time_en'])\ntrain['log_arr_time_en'] = np.log(train['arr_time_en'])\ntest['log_dep_time_en'] = np.log(test['dep_time_en'])\ntest['log_arr_time_en'] = np.log(test['arr_time_en'])","f660b74c":"scatter_p(fea=['log_dep_time_en','log_arr_time_en', 'duration_en','ym'],price='Price')","dfbd9ceb":"train_dum=train[['airline_en','source_en', 'dest_en','total_stops_en','route_en','log_dep_time_en','log_arr_time_en', 'duration_en','ym','Additional_Info_en','Price']]\ntest_dum=test[['airline_en','source_en', 'dest_en','total_stops_en','route_en','log_dep_time_en','log_arr_time_en', 'duration_en','ym','Additional_Info_en']]\n#train_dum = pd.get_dummies(train_dum,columns=['Airline','Source','Destination','Total_Stops'],prefix=['dum_','src_','dest_','st_'],drop_first=True)\n#test_dum = pd.get_dummies(test_dum,columns=['Airline','Source','Destination','Total_Stops'],prefix=['dum_','src_','dest_','st_'],drop_first=True)\n#train_dum.drop(['airline_en','source_en', 'dest_en','total_stops_en'],axis=1,inplace=True)\n#test_dum.drop(['airline_en','source_en', 'dest_en','total_stops_en'],axis=1,inplace=True)","f9dead49":"corr = train_dum.corr()\nmask = np.zeros_like(corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf,ax = plt.subplots(figsize=(8,8))\ncmap=sns.diverging_palette(120,10,as_cmap=True) #\nsns.heatmap(corr,mask=mask,cmap=cmap,center=0,square=False,linewidths=.5,cbar_kws={\"shrink\":.5})","0330b21e":"X=train_dum.loc[:,train_dum.columns!='Price']\ny=train_dum.loc[:,train_dum.columns=='Price']\n#train_dum,test_dum = X.align(test_dum, join='outer', axis=1, fill_value=0)","14ed9a73":"#Regression Library\nfrom sklearn.linear_model import LinearRegression,ElasticNet,Lasso,Ridge,BayesianRidge\nfrom lightgbm.sklearn import LGBMRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor,BaggingRegressor,VotingClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom sklearn import neighbors\nfrom sklearn import svm\n\nfrom sklearn.model_selection import KFold,train_test_split\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score","8ce99454":"def RMSE(estimator,X_train, Y_train, cv,n_jobs=-1):\n    cv_results = cross_val_score(estimator,X_train,Y_train,cv=cv,scoring=\"neg_mean_squared_error\",n_jobs=n_jobs)\n    return (np.sqrt(-cv_results)).mean()","47a190c9":"def baseModels(train_X,train_y):\n    model_EN=ElasticNet(random_state=0)\n    model_SVR=svm.SVR(kernel='rbf',C=0.005)\n    model_Lasso=Lasso(alpha=0.1,max_iter=1000)\n    model_Ridge=Ridge(alpha=0.1)\n    model_Linear=LinearRegression()\n    model_LGBM=LGBMRegressor(boosting='gbdt', n_jobs=-1, random_state=2018)\n    model_GBR=GradientBoostingRegressor(n_estimators=100,alpha=0.01)\n    model_XGB = xgb.XGBRegressor(n_estimators=100, learning_rate=0.02, gamma=0, subsample=0.75,colsample_bytree=1, max_depth=4)\n    model_DTR = DecisionTreeRegressor(max_depth=4,min_samples_split=5,max_leaf_nodes=10)\n    model_RFR=RandomForestRegressor(n_jobs=-1)\n    model_KNN=neighbors.KNeighborsRegressor(3,weights='uniform')\n    model_Bayesian=BayesianRidge()\n    model_adaboost=AdaBoostRegressor(base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n    model_bagreg=BaggingRegressor(base_estimator=None, n_estimators=100,  bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n    kf = KFold(n_splits=5, random_state=None, shuffle=True)\n\n    models={'ElasticNet':model_EN,'SVR':model_SVR,'Lasso':model_Lasso,'Ridge':model_Ridge,'LGBM':model_LGBM,\n            'GBR':model_GBR,'XGB':model_XGB,'DTR':model_DTR,'RandomForest':model_RFR,'KNN':model_KNN,\n            'Bayes':model_Bayesian,'Linear':model_Linear,'AdaBoost':model_adaboost,'Bagging':model_bagreg}\n    rmse=[]\n    for model in models.values():\n        #scores = cross_val_score(model, train_X, train_y, cv=kf,scoring='neg_mean_squared_error')\n        #rmse.append(abs(scores.mean()))\n        rmse.append(RMSE(model,train_X,train_y,kf))                         \n    dataz = pd.DataFrame(data={'RMSE':rmse},index=models.keys())\n    return  dataz","9854b97f":"baseModels(X,y)","b8e8d2ae":"model_rf=RandomForestRegressor(n_jobs=-1)\nRMSE(model_rf,X,y,10)","c355e1f2":"model_LGBM=LGBMRegressor(boosting='gbdt',learning_rate=0.5, n_estimators=850,num_leaves=8,random_state=400,\n                        colsample_bytree=0.65,subsample=0.7,reg_alpha=1,reg_lambda=1.4,n_jobs=-1)\nRMSE(model_LGBM,X,y,10)","16396126":"model_bagreg=BaggingRegressor(base_estimator=None, n_estimators=600,  bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=500, verbose=0)\nRMSE(model_bagreg,X,y,10)","c6e65a78":"from mlxtend.regressor import StackingRegressor\nmodel_SVR=svm.SVR(kernel='rbf',C=0.02)\nstregr = StackingRegressor(regressors=[model_bagreg, model_LGBM, model_rf], \n                           meta_regressor=model_LGBM)\nRMSE(stregr,X,y,5)","64c0058b":"stregr.fit(X,y)\npreds = stregr.predict(test_dum)\nsubmission = pd.DataFrame()\nsubmission['Price'] = np.exp(preds)\nsubmission.head()\nsubmission.to_excel('submission.xlsx')","af46b4e7":"LGBM,RandomForest,Bagging , let us do HyperParameter Tuning","dc2b7ed5":"transformer = ReduceVIF()\n\n# Only use 10 columns for speed in this example\nX = transformer.fit_transform(train_dum, y)\n\nX.head()","96fc6225":"Preprocess with Label Encoder","8928121b":"Check Nulls","78c83dd0":"model_LGBM=LGBMRegressor(n_jobs=-1)\ngridParams = {\n    'learning_rate': [0.005,0.01,0.06,0.03],\n    'n_estimators': [100,500,100],\n    'num_leaves': [6,8,5],\n    'boosting_type' : ['gbdt'],\n    'objective' : ['regression'],\n    'random_state' : [100,300,700,1000], # Updated from 'seed'\n    'colsample_bytree' : [0.65, 0.66],\n    'subsample' : [0.7,0.75],\n    'reg_alpha' : [1,1.2],\n    'reg_lambda' : [1,1.2,1.4],\n    }\ngrid = GridSearchCV(model_LGBM, gridParams,\n                    verbose=0,\n                    cv=3,\n                    scoring='neg_mean_squared_error',\n                    n_jobs=2)\n# Run the grid\ngrid.fit(train_dum, y)\n\n# Print the best parameters found\nprint(grid.best_params_)\nprint(grid.best_score_)","45c0737c":"model_bagreg=BaggingRegressor(n_jobs=-1)\ngridParams = {\n    'n_estimators': [100,500,100],\n    'max_samples':[2,5,8],\n    'random_state' : [100,300,600,1000],\n    }\ngrid = GridSearchCV(model_bagreg, gridParams,\n                    verbose=0,\n                    cv=3,\n                    scoring='neg_mean_squared_error',\n                    n_jobs=2)\n# Run the grid\ngrid.fit(X, y)\n\n# Print the best parameters found\nprint(grid.best_params_)\nprint(grid.best_score_)","d849c501":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import Imputer\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nclass ReduceVIF(BaseEstimator, TransformerMixin):\n    def __init__(self, thresh=5.0, impute=True, impute_strategy='median'):\n        # From looking at documentation, values between 5 and 10 are \"okay\".\n        # Above 10 is too high and so should be removed.\n        self.thresh = thresh\n        \n        # The statsmodel function will fail with NaN values, as such we have to impute them.\n        # By default we impute using the median value.\n        # This imputation could be taken out and added as part of an sklearn Pipeline.\n        if impute:\n            self.imputer = Imputer(strategy=impute_strategy)\n\n    def fit(self, X, y=None):\n        print('ReduceVIF fit')\n        if hasattr(self, 'imputer'):\n            self.imputer.fit(X)\n        return self\n\n    def transform(self, X, y=None):\n        print('ReduceVIF transform')\n        columns = X.columns.tolist()\n        if hasattr(self, 'imputer'):\n            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n        return ReduceVIF.calculate_vif(X, self.thresh)\n\n    @staticmethod\n    def calculate_vif(X, thresh=5.0):\n        # Taken from https:\/\/stats.stackexchange.com\/a\/253620\/53565 and modified\n        dropped=True\n        while dropped:\n            variables = X.columns\n            dropped = False\n            vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]\n            \n            max_vif = max(vif)\n            if max_vif > thresh:\n                maxloc = vif.index(max_vif)\n                print(f'Dropping {X.columns[maxloc]} with vif={max_vif}')\n                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n                dropped=True\n        return X\n","80235815":"We see the Destination has duplicate by name \"NewDelhi\" . Let us update the same with Delhi as both are same on preprocessing\n\nLet us check the distribution of Flight Price","18f2946d":"Match same columns on both Test and Train","4775ef0f":"model_RFR=RandomForestRegressor(n_jobs=-1)\ngridParams={'bootstrap': [True, False],\n 'max_depth': [4,5,6,8,3],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600,1000],}\n\ngrid = GridSearchCV(model_RFR, gridParams,\n                    verbose=0,\n                    cv=3,\n                    scoring='neg_mean_squared_error',\n                    n_jobs=2)\n# Run the grid\ngrid.fit(train_dum, y)\n\n# Print the best parameters found\nprint(grid.best_params_)\nprint(grid.best_score_)","b66b3f77":"Check Unique of category columns Airline .Source,Destination of both Train and Test","763e9610":"It's extremely skewed, we may consider to take the log transformation of it.\n","7d42da4b":"Bivariate  relationship with Price","e9982aea":"model_bagreg=BaggingRegressor(n_jobs=-1)\ngridParams = {\n    'n_estimators': [100,500,100],\n    'max_samples':[2,5,8],\n    'random_state' : [100,300,600,1000],\n    }\ngrid = GridSearchCV(model_bagreg, gridParams,\n                    verbose=0,\n                    cv=3,\n                    scoring='neg_mean_squared_error',\n                    n_jobs=2)\n# Run the grid\ngrid.fit(X, y)\n\n# Print the best parameters found\nprint(grid.best_params_)\nprint(grid.best_score_)","1e36903f":"Next we will check the seasonality of flight price. In the following plot the y axis is representing the flight price on log scale, the color of the bubbles shows the number of sales. When the number of sales is high, the flight price is also high, which is quite reasonable.","9b842ecd":"Categorical Variables:\n\nAirline , Source , Destination ,TotalStops ,Route . We will use dummies for all ,except route we will use Label encoder\n\nContinuous Variables:\n\nDeparture Time,Arrival Time ,Date Of Journey ,Duration"}}