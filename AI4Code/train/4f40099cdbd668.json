{"cell_type":{"980a4e52":"code","9b716a1a":"code","b8049d1c":"code","51bfd427":"code","a281f20b":"code","5d0d223c":"code","1ef84ff5":"code","7aad0022":"code","f9a61f81":"code","63b77327":"code","edfe903a":"code","3fcfe9ac":"code","64586810":"code","163c511f":"code","94e72b23":"code","c685c6c8":"code","8600c8f0":"code","f6e869a8":"code","3cac04c1":"code","44101d0f":"code","f4d8b154":"code","1fd56f0d":"code","b9a6542f":"markdown","81b7f736":"markdown"},"source":{"980a4e52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nimport gc\nimport numpy as np\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold, RepeatedKFold, GroupKFold\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import ADASYN\nimport category_encoders as ce\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9b716a1a":"def dprint(*args, **kwargs):\n    print(\"[{}] \".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")) + \\\n        \" \".join(map(str,args)), **kwargs)\n\nid_name = 'Id'\ntarget_name = 'Target'\n\n","b8049d1c":"# Load data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","51bfd427":"train['is_test'] = 0\ntest['is_test'] = 1\ndf_all = pd.concat([train, test], axis=0)\n","a281f20b":"dprint('Clean features...')\ncols = ['dependency']\nfor c in tqdm(cols):\n    x = df_all[c].values\n    strs = []\n    for i, v in enumerate(x):\n        try:\n            val = float(v)\n        except:\n            strs.append(v)\n            val = np.nan\n        x[i] = val\n    strs = np.unique(strs)\n\n    for s in strs:\n        df_all[c + '_' + s] = df_all[c].apply(lambda x: 1 if x == s else 0)\n\n    df_all[c] = x\n    df_all[c] = df_all[c].astype(float)\ndprint(\"Done.\")","5d0d223c":"dprint(\"Extracting features...\")\ndef extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']\/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']\/df['rooms']\n    df['rent_to_bedrooms'] = df['v2a1']\/df['bedrooms']\n    df['tamhog_to_rooms'] = df['tamhog']\/df['rooms'] # tamhog - size of the household\n    df['tamhog_to_bedrooms'] = df['tamhog']\/df['bedrooms']\n    df['r4t3_to_tamhog'] = df['r4t3']\/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']\/df['rooms'] # r4t3 - Total persons in the household\n    df['r4t3_to_bedrooms'] = df['r4t3']\/df['bedrooms']\n    df['rent_to_r4t3'] = df['v2a1']\/df['r4t3']\n    df['v2a1_to_r4t3'] = df['v2a1']\/(df['r4t3'] - df['r4t1'])\n    df['hhsize_to_rooms'] = df['hhsize']\/df['rooms']\n    df['hhsize_to_bedrooms'] = df['hhsize']\/df['bedrooms']\n    df['rent_to_hhsize'] = df['v2a1']\/df['hhsize']\n    df['qmobilephone_to_r4t3'] = df['qmobilephone']\/df['r4t3']\n    df['qmobilephone_to_v18q1'] = df['qmobilephone']\/df['v18q1']\n    \n\nextract_features(train)\nextract_features(test)\ndprint(\"Done.\")         ","1ef84ff5":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_data(df):\n   \n    yes_no_map = {'no': 0, 'yes': 1}\n    \n    df['dependency'] = df['dependency'].replace(yes_no_map).astype(np.float32)\n    \n    df['edjefe'] = df['edjefe'].replace(yes_no_map).astype(np.float32)\n    df['edjefa'] = df['edjefa'].replace(yes_no_map).astype(np.float32)\n    \n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n","7aad0022":"dprint(\"Encoding Data....\")\nencode_data(train)\nencode_data(test)\ndprint(\"Done...\")","f9a61f81":"def do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                 #('', '', ''),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] \/ df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n    # do something advanced above...\n    \n    # Drop SQB variables, as they are just squres of other vars \n    df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n    # Drop id's\n    df.drop(['Id', 'idhogar'], axis=1, inplace=True)\n    # Drop repeated columns\n    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n    return df\n    ","63b77327":"dprint(\"Do_feature Engineering....\")\ntrain = do_features(train)\ntest = do_features(test)\ndprint(\"Done....\")","edfe903a":"dprint(\"Fill Na value....\")\ntrain = train.fillna(0)\ntest = test.fillna(0)\ndprint(\"Done....\")","3fcfe9ac":"train.shape,test.shape","64586810":"cols_to_drop = [\n    id_name, \n    target_name,\n]\nX = train.drop(cols_to_drop, axis=1, errors='ignore')\ny = train[target_name].values\n","163c511f":"X.shape,test.shape","94e72b23":"import lightgbm as lgb\ngc.collect()","c685c6c8":"def get_lgb_model():\n    lgb_model = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.2, objective='multiclass',\n                             silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                            colsample_bytree= 0.93, \n                            min_child_samples= 56, num_leaves= 19, \n                                   subsample= 0.84) \n    return lgb_model","8600c8f0":"dprint(\"Prepared Model.....\")\nlgb_model = get_lgb_model()\nlgb_model.fit(X, y)\ndprint(\"Done Model.....\")\ndprint(\"Predict Test Value.....\")\ntarget_lgb = lgb_model.predict(test)\ndprint(\"Done Prediction.....\")","f6e869a8":"# def cat_model():\n#     cat_model = CatBoostClassifier(verbose=False, loss_function='MultiClass')\n\n    \n#     return cat_model","3cac04c1":"# %%time\n# dprint(\"Prepared Model.....\")\n# cat_model = cat_model()\n# cat_model.fit(X, y)\n# dprint(\"Done Model.....\")\n# dprint(\"Predict Test Value.....\")\n# target_cat = cat_model.predict(test).astype(\"int64\")\n# dprint(\"Done Prediction.....\")","44101d0f":"sub = pd.read_csv(\"..\/input\/sample_submission.csv\")","f4d8b154":"sub['Target'] = target_lgb","1fd56f0d":"sub.to_csv(\"cat_boost.csv\", index= False)","b9a6542f":"## Road Map of Clean Data \/ Feature Engineering \/ Encoding \/ Remove Null Value\n1.  Load Data.....\n1.  Clean features...\n1.  Extracting features...\n1.  Encoding Data....\n1.  Fill NA value.....\n1.  Prepared Model.....\n1.  Predict Value on test Dataset......\n1.  Submit your result .......","81b7f736":"## Challenge\n* The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?\n\n* Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It\u2019s especially tricky when a program focuses on the poorest segment of the population. The world\u2019s poorest typically can\u2019t provide the necessary income and expense records to prove that they qualify."}}