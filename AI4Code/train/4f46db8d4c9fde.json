{"cell_type":{"98c5a081":"code","f9298507":"code","3636318e":"code","7271ffdf":"code","bf5926b7":"code","9ad25fd1":"code","7b539a49":"code","df040f21":"code","71e90d08":"code","e65e818c":"code","0e7afc37":"code","6068967c":"code","7b803845":"code","bb4e02c8":"code","5657e497":"code","8f3d787f":"code","6559e15c":"code","ae084ae8":"code","94475ece":"code","4276f99a":"code","37c3dafa":"code","545fc3a3":"code","1ca71bca":"code","d78b595c":"code","36607848":"code","b4e3d9a5":"code","bbae1de3":"code","ae97159f":"code","be29aeb9":"code","6991946c":"code","0093e4eb":"code","10e3657e":"code","13f232f6":"code","565d198a":"code","f85fd7d2":"code","d2f13061":"code","2e2055e2":"code","f10363a6":"code","fc73416a":"code","46ef4964":"code","0d77f013":"code","aa7917c7":"code","4ec0711d":"code","d0894ae8":"code","6c82a092":"code","38c7ed9f":"code","593e2344":"code","e854110d":"code","b6322f91":"code","a28056b2":"code","97159802":"code","241ba593":"code","5ca0d755":"code","aa6a5b70":"code","4761805d":"code","ee90388f":"code","584d15ed":"code","8fe5b120":"code","b70f9556":"code","b05d8b0c":"code","052f1bec":"code","05cbdc5a":"code","c6477119":"code","20082e76":"code","69a1dc2d":"code","4fb1f9b4":"code","c948ede7":"code","557a4849":"code","dc89aea4":"code","bfbb4267":"code","79912f7a":"code","37c96f7e":"code","d713eb5e":"code","a74a9759":"markdown","78923eba":"markdown","0d455c70":"markdown","8574b8e6":"markdown","43c2d6ec":"markdown","cc48354e":"markdown","defc196b":"markdown","accae52b":"markdown","90e3cdf7":"markdown","17bf4c99":"markdown","0bf18bc3":"markdown","860b2c22":"markdown","80f0af38":"markdown","29c8ce58":"markdown","9a5a7a83":"markdown","a81b796b":"markdown","47039225":"markdown","c36d821b":"markdown","44635a52":"markdown","a99c922b":"markdown","eb0a46c9":"markdown","3725fe6c":"markdown","11a374dd":"markdown","6b0a0dc5":"markdown","a778284b":"markdown","f5f8f5b4":"markdown","869b099e":"markdown","67089345":"markdown","68c053e2":"markdown","a323a146":"markdown","d7ae2286":"markdown","cc367750":"markdown","cfe0dd31":"markdown","082f978a":"markdown","ae8c3f26":"markdown","f8532398":"markdown","52351991":"markdown","3c9c3c3c":"markdown","d4436633":"markdown","431e4db4":"markdown","5e2f60c3":"markdown","cb4cb928":"markdown","8d38c24e":"markdown","00c7883f":"markdown","85d1e4a9":"markdown","e5782241":"markdown","da3138e4":"markdown","305823e5":"markdown","268586d2":"markdown","32ea8ad8":"markdown","ac3b9c13":"markdown","54517b9d":"markdown","3f7cc146":"markdown","10215fd0":"markdown","13ec3661":"markdown","be7e1f38":"markdown","855c5209":"markdown","a3ef0eb1":"markdown","5c2504e3":"markdown","f59ba595":"markdown","c0c849e0":"markdown","2c986d12":"markdown","f20549e3":"markdown","05c209c0":"markdown","dcb0c7f7":"markdown","9806522a":"markdown","d687d293":"markdown","5fee4f3e":"markdown","e9070933":"markdown","5c044cf5":"markdown","817a3d56":"markdown","9abde1b1":"markdown"},"source":{"98c5a081":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9298507":"# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Data processing, evaluation metrics and modelling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc\nimport sklearn.metrics as sklm\nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Stats\nimport numpy.random as nr\nimport scipy.stats as ss\nfrom scipy.stats import randint \n\n# Ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore')","3636318e":"path = \"..\/input\/pima-indians-diabetes-database\/diabetes.csv\"\ndata = pd.read_csv(path)","7271ffdf":"data.head()","bf5926b7":"data.rename({'DiabetesPedigreeFunction':'DPF'},inplace = True,axis =1)","9ad25fd1":"data.info()","7b539a49":"data.describe()","df040f21":"data['Outcome'].describe()","71e90d08":"data['Outcome'].value_counts()","e65e818c":"# Creating two datasets\nDiabetic = data.loc[(data['Outcome'] != 0)]\nHealthy = data.loc[(data['Outcome'] == 0)]\ncolors = ['#FFD700', '#7EC0EE']\n\n# Creating a dataframe having label_count\nlabel_count = pd.DataFrame({'Condition': ['Diabetic', 'Healthy'], 'Count': [len(Diabetic.index), len(Healthy.index)]})\n\n# Visualizing this dataframe\nplt.figure(figsize=(7,5))\nsns.barplot(x=label_count['Condition'], y=label_count['Count'], data=label_count, palette=colors)\nplt.title(\"Count of Outcome variable\")\n\n\n# Pie chart for label_count distribution as in percentage\nlabels = ['Diabetic', 'Healthy']\nsizes = [len(Diabetic.index), len(Healthy.index)]\nexplode = (0.1, 0.05)\n\nfig,ax = plt.subplots()\nplt.title('Distribution of Outcome variable')\nax.pie(sizes,\n       explode = explode,\n       labels = labels,\n       autopct = '%1.1f%%',\n       shadow = True,\n       startangle = 100,\n       colors = colors)\n\nplt.show()","0e7afc37":"# Replacing all zeros in the following features with NaN(Not a Number), so that we can later replace with something more useful\ndata[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = data[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.NaN)","6068967c":"# Checking the missing values in each feature\ndata.isnull().sum()","7b803845":"missing_data = pd.DataFrame({'Columns': ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'],\n                             'Missing_values': [data['Glucose'].isnull().sum(), data['BloodPressure'].isnull().sum(),\n                                                data['SkinThickness'].isnull().sum(), data['Insulin'].isnull().sum(),\n                                                data['BMI'].isnull().sum()],\n                             'Percentage': [data['Glucose'].isnull().sum()\/len(data['Glucose'])*100,\n                                            data['BloodPressure'].isnull().sum()\/len(data['BloodPressure'])*100,\n                                            data['SkinThickness'].isnull().sum()\/len(data['SkinThickness'])*100,\n                                            data['Insulin'].isnull().sum()\/len(data['Insulin'])*100,\n                                            data['BMI'].isnull().sum()\/len(data['BMI'])*100]})\nmissing_data.head()","bb4e02c8":"plt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(11, 15))\n\nax.set_facecolor('#fafafa')\nax.set(xlim=(-.05, 200))\nplt.ylabel('Variables')\nplt.title(\"Overview Data Set\")\nax = sns.boxplot(data = data, \n  orient = 'h', \n  palette = 'Set2')","5657e497":"plt.figure(figsize=(15,12))\nsns.heatmap(data.corr(), cmap = 'Blues', annot = True)\nplt.show()","8f3d787f":"# Function to replace nan values with the median of observations in those features\ndef fill_with_median(x):\n    data[x].fillna(data[x].astype('float').median(axis=0))","6559e15c":"# Function to treat outliers\ndef z_score(col, data):\n    median = data[col].median()\n    std = data[col].std()\n    threshold = std * 3\n    lower, upper = median - threshold, median + threshold\n    new_df = data[(data[col] < upper) & (data[col] > lower)]\n    return new_df","ae084ae8":"plt.figure(figsize=(12,6))\nsns.distplot(Diabetic['Insulin'], hist=True, label='Diabetic')\nsns.distplot(Healthy['Insulin'], hist=True, label='Healthy')\nplt.legend()","94475ece":"Insulin = z_score('Insulin', data)\n\nDiabetic_ins = Insulin.loc[(Insulin['Outcome'] != 0)]\nHealthy_ins = Insulin.loc[(Insulin['Outcome'] == 0)]\n\nplt.figure(figsize=(12,6))\nsns.distplot(Diabetic_ins['Insulin'], hist=True, label='Diabetic')\nsns.distplot(Healthy_ins['Insulin'], hist=True, label='Healthy')\nplt.legend()","4276f99a":"fill_with_median('Insulin')","37c3dafa":"data.loc[(data['Outcome'] != 0) & (data['Insulin'].isnull()), 'Insulin'] = 166.0 # Median(Diabetic)\ndata.loc[(data['Outcome'] == 0) & (data['Insulin'].isnull()), 'Insulin'] = 100.0 # Median(Healthy)","545fc3a3":"plt.figure(figsize=(12,6))\nsns.distplot(Diabetic['SkinThickness'], hist=True, label='Diabetic')\nsns.distplot(Healthy['SkinThickness'], hist=True, label='Healthy')\nplt.legend()","1ca71bca":"SkinThickness = z_score('SkinThickness', data)\n\nDiabetic_skin = SkinThickness.loc[(SkinThickness['Outcome'] != 0)]\nHealthy_skin = SkinThickness.loc[(SkinThickness['Outcome'] == 0)]\n\nplt.figure(figsize=(12,6))\nsns.distplot(Diabetic_skin['SkinThickness'], hist=True, label='Diabetic')\nsns.distplot(Healthy_skin['SkinThickness'], hist=True, label='Healthy')\nplt.legend()","d78b595c":"fill_with_median('SkinThickness')","36607848":"data.loc[(data['Outcome'] != 0) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 32.0 \ndata.loc[(data['Outcome'] == 0) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 27.0","b4e3d9a5":"plt.figure(figsize=(12,6))\nsns.distplot(Diabetic['BloodPressure'], hist=True, label='Diabetic')\nsns.distplot(Healthy['BloodPressure'], hist=True, label='Healthy')\nplt.legend()","bbae1de3":"BP = z_score('BloodPressure', data)\n\nDiabetic_BP = BP.loc[(BP['Outcome'] != 0)]\nHealthy_BP = BP.loc[(BP['Outcome'] == 0)]\n\nplt.figure(figsize=(12,6))\nsns.distplot(Diabetic_BP['BloodPressure'], hist=True, label='Diabetic')\nsns.distplot(Healthy_BP ['BloodPressure'], hist=True, label='Healthy')\nplt.legend()","ae97159f":"fill_with_median('BloodPressure')","be29aeb9":"data.loc[(data['Outcome'] != 0) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 74.0\ndata.loc[(data['Outcome'] == 0) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 70.0","6991946c":"plt.figure(figsize=(12,6))\nsns.distplot(Diabetic['BMI'], hist=True, label='Diabetic')\nsns.distplot(Healthy['BMI'], hist=True, label='Healthy')\nplt.legend()","0093e4eb":"BMI = z_score('BMI', data)\n\nDiabetic_BMI = BMI.loc[(BMI['Outcome'] != 0)]\nHealthy_BMI = BMI.loc[(BMI['Outcome'] == 0)]\n\nplt.figure(figsize=(12,6))\nsns.distplot(Diabetic_BMI['BMI'], hist=True, label='Diabetic')\nsns.distplot(Healthy_BMI['BMI'], hist=True, label='Healthy')\nplt.legend()","10e3657e":"fill_with_median('BMI')","13f232f6":"data.loc[(data['Outcome'] != 0) & (data['BMI'].isnull()), 'BMI'] = 34.2\ndata.loc[(data['Outcome'] == 0) & (data['BMI'].isnull()), 'BMI'] = 30.1","565d198a":"plt.figure(figsize=(12,6))\nsns.distplot(Diabetic['Glucose'], hist=True, label='Diabetic')\nsns.distplot(Healthy['Glucose'], hist=True, label='Healthy')\nplt.legend()","f85fd7d2":"Glucose = z_score('Glucose', data)\n\nDiabetic_glu = Glucose.loc[(Glucose['Outcome'] != 0)]\nHealthy_glu = Glucose.loc[(Glucose['Outcome'] == 0)]\n\nplt.figure(figsize=(12,6))\nsns.distplot(Diabetic_glu['Glucose'], hist=True, label='Diabetic')\nsns.distplot(Healthy_glu['Glucose'], hist=True, label='Healthy')\nplt.legend()","d2f13061":"fill_with_median('Glucose')","2e2055e2":"data.loc[(data['Outcome'] != 0) & (data['Glucose'].isnull()), 'Glucose'] = 140.0\ndata.loc[(data['Outcome'] == 0) & (data['Glucose'].isnull()), 'Glucose'] = 107.0","f10363a6":"plt.figure(figsize=(12,6))\nsns.distplot(Diabetic['Age'], hist=True, label='Diabetic')\nsns.distplot(Healthy['Age'], hist=True, label='Healthy')\nplt.legend()","fc73416a":"Age = z_score('Age', data)\n\nDiabetic_age = Age.loc[(Age['Outcome'] != 0)]\nHealthy_age = Age.loc[(Age['Outcome'] == 0)]\n\nplt.figure(figsize=(12,6))\nsns.distplot(Diabetic_age['Age'], hist=True, label='Diabetic')\nsns.distplot(Healthy_age['Age'], hist=True, label='Healthy')\nplt.legend()","46ef4964":"plt.figure(figsize=(12,6))\nsns.distplot(Diabetic['Pregnancies'], hist=True, label='Diabetic')\nsns.distplot(Healthy['Pregnancies'], hist=True, label='Healthy')\nplt.legend()","0d77f013":"Preg = z_score('Pregnancies', data)\n\nDiabetic_preg = Preg.loc[(Preg['Outcome'] != 0)]\nHealthy_preg = Preg.loc[(Preg['Outcome'] == 0)]\n\nplt.figure(figsize=(12,6))\nsns.distplot(Diabetic_preg['Pregnancies'], hist=True, label='Diabetic')\nsns.distplot(Healthy_preg['Pregnancies'], hist=True, label='Healthy')\nplt.legend()","aa7917c7":"plt.figure(figsize=(12,6))\nsns.distplot(Diabetic['DPF'], hist=True, label='Diabetic')\nsns.distplot(Healthy['DPF'], hist=True, label='Healthy')\nplt.legend()","4ec0711d":"DPF = z_score('DPF', data)\n\nDiabetic_DPF = DPF.loc[(DPF['Outcome'] != 0)]\nHealthy_DPF = DPF.loc[(DPF['Outcome'] == 0)]\n\nplt.figure(figsize=(12,6))\nsns.distplot(Diabetic_DPF['DPF'], hist=True, label='Diabetic')\nsns.distplot(Healthy_DPF['DPF'], hist=True, label='Healthy')\nplt.legend()","d0894ae8":"cols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DPF', 'Age']\nfor col in cols:\n    data = z_score(col, data)","6c82a092":"data.shape","38c7ed9f":"data['Outcome'].value_counts()","593e2344":"# Seperating data into features and label\nX = data.drop(['Outcome'], axis=1)\ny = data['Outcome']","e854110d":"# Scaling Features\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(X_scaled, columns=X.columns)\nX.head()","b6322f91":"# Fixing the random_state in order to maintain consistency in results\nrand_state = 11\n\nX_train, X_test, y_train, y_test = train_test_split(X , y, random_state=rand_state, test_size=0.2, stratify=y)\nX_train.shape, X_test.shape","a28056b2":"DTC = DecisionTreeClassifier(random_state=rand_state)\nDTC.fit(X_train, y_train)","97159802":"ABC = AdaBoostClassifier(DecisionTreeClassifier(random_state=rand_state),random_state=rand_state,learning_rate=0.1)\nABC.fit(X_train, y_train)","241ba593":"XGBC = xgb.XGBClassifier(learning_rate=0.05,random_state =rand_state)\nXGBC.fit(X_train, y_train)","5ca0d755":"LGBMC = lgbm.LGBMClassifier(learning_rate=0.05)\nLGBMC.fit(X_train, y_train)","aa6a5b70":"RFC = RandomForestClassifier(random_state=rand_state)\nRFC.fit(X_train, y_train)","4761805d":"KNNC = KNeighborsClassifier(n_neighbors=7)\nKNNC.fit(X_train, y_train)","ee90388f":"LR = LogisticRegression(random_state=rand_state, class_weight={0:0.33, 1:0.67})\nLR.fit(X_train, y_train)","584d15ed":"probabilities = LR.predict_proba(X_test)\nprint(probabilities[:15,:])","8fe5b120":"def score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs[:, 1]])\nscores = score_model(probabilities, 0.5)\nprint((scores[: 15]))\nprint(y_test[: 15])","b70f9556":"def print_metrics(labels, scores):\n    metrics = precision_recall_fscore_support(labels, scores)\n    conf = confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('Accuracy  %0.2f' % accuracy_score(labels, scores))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n\n\n    \nprint_metrics(y_test, scores)   ","b05d8b0c":"def plot_auc(labels, probs):\n    ## Compute the false positive rate, true positive rate\n    ## and threshold along with the AUC\n    fpr, tpr, threshold = roc_curve(labels, probs[:,1])\n    auc = sklm.auc(fpr, tpr)\n    \n    ## Plot the result\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, color = 'orange', label = 'AUC = %0.2f' % auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    \nplot_auc(y_test, probabilities)    ","052f1bec":"# K-Fold Cross Validation function\nn_folds = 5\n\ndef scores_cv(model):\n    kf = StratifiedKFold(n_folds, shuffle=True, random_state=rand_state).get_n_splits(data.values)\n    scores = cross_val_score(model, X_train, y_train, scoring=\"accuracy\", cv = kf)\n    return(scores)","05cbdc5a":"scores = scores_cv(DTC)\nprint(\"\\nDecision Tree score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","c6477119":"scores = scores_cv(ABC)\nprint(\"\\nAda Boost score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","20082e76":"scores = scores_cv(XGBC)\nprint(\"\\nXG Boost score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","69a1dc2d":"scores = scores_cv(LGBMC)\nprint(\"\\nLightGBM score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","4fb1f9b4":"scores = scores_cv(RFC)\nprint(\"\\nRandom Forest score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","c948ede7":"scores = scores_cv(KNNC)\nprint(\"\\nKNN score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","557a4849":"scores = scores_cv(LR)\nprint(\"\\nLogistic Regression score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","dc89aea4":"param_dist = {'colsample_bytree': nr.random(1), \"learning_rate\": [0.05, 0.01, 0.1, 0.3]\n              , \"max_depth\": randint(1,20), \"min_child_weight\": nr.random(5)\n              , \"n_estimators\": [100, 300, 500]\n              , \"subsample\":nr.random(1)}\n\nXGBC = xgb.XGBClassifier(random_state=rand_state)\nXGBC_cv = RandomizedSearchCV(XGBC,param_distributions=param_dist, cv=5)\nXGBC_cv.fit(X_train, y_train)\n\nprint(\"Tuned XGBoost Parameters: {}\".format(XGBC_cv.best_params_)) \nprint(\"Best score is {}\".format(XGBC_cv.best_score_)) ","bfbb4267":"param_dist = {'num_leaves': randint(1,20), 'max_depth': randint(1,15), 'learning_rate': [0.05, 0.1, 0.3]\n              , 'n_estimators': [100, 300, 500], 'min_child_weight': nr.random(5), 'min_child_samples': randint(1,20)\n              , 'subsample': nr.random(1), 'colsample_bytree': nr.random(1)}\n\nLightGBM = lgbm.LGBMClassifier(random_state=rand_state)\nLightGBM_cv = RandomizedSearchCV(LightGBM,param_distributions=param_dist, cv=5)\nLightGBM_cv.fit(X_train, y_train)\n\nprint(\"Tuned LightGBM Parameters: {}\".format(LightGBM_cv.best_params_)) \nprint(\"Best score is {}\".format(LightGBM_cv.best_score_))","79912f7a":"param_dist = {'n_estimators': [100,200,300,400,500,600], 'criterion': ['gini','entropy']\n              , 'max_depth': randint(1,15), 'max_features': randint(1,9), 'min_samples_leaf': randint(1,9)}\n\nRFC = RandomForestClassifier(random_state=rand_state)\nRFC_cv = RandomizedSearchCV(RFC,param_distributions=param_dist, cv=5)\nRFC_cv.fit(X_train, y_train)\n\nprint(\"Tuned Random Forest Tree Parameters: {}\".format(RFC_cv.best_params_)) \nprint(\"Best score is {}\".format(RFC_cv.best_score_)) ","37c96f7e":"XGBC_best = xgb.XGBClassifier(random_state=rand_state, colsample_bytree=0.9250812043886706,\n                             learning_rate=0.01, max_depth=19, min_child_weight=0.9458610813134812,\n                             n_estimators=300, subsample=0.8977705383997567)\n\nLightGBM_best = lgbm.LGBMClassifier(random_state=rand_state, colsample_bytree=0.2991633157558994,\n                               learning_rate=0.3, max_depth=8,min_child_samples=16,\n                               min_child_weight=0.8711407655443546, n_estimators=100,\n                               num_leaves=4, subsample=0.3627412195264993)\n\nRFC_best = RandomForestClassifier(random_state=rand_state, criterion='entropy', max_depth=4,\n                                  max_features=3, min_samples_leaf=1, n_estimators=500)","d713eb5e":"voting = VotingClassifier(estimators=[('RFC', RFC_best), ('LightGBM', LightGBM_best),\n('XGBC', XGBC_best)], voting='soft', n_jobs=4)\n\nvoting.fit(X_train, y_train)\n\nscore = voting.predict(X_test)\nprint((score[: 15]))\nprint(np.array(y_test[: 15]))","a74a9759":"**Preparing final outlier-free dataset.**","78923eba":"**I have chosen the best performing models from the above 7 to be optimized in order to achieve maximum accuracy.**\n\n**One of the main theoretical backings to motivate the use of random search in place of grid search is the fact that for most cases, hyperparameters are not equally important and the grid search strategy spends redundant time exploring the unimportant parameter. and misses the optimal model.**\n\n*A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. - Bergstra, 2012*","0d455c70":"*Before we dive into this section, we have to set a baseline accuracy, so that we can check whether our model is upto the mark or not. Now, we have two scenarios; either we set the accuracy according to Random-guessing or according to the Zero Classifier rule.*\n\n1. Random guessing \n>In our case, the target split is 67% - 33% for class 0 and 1 respectively. Let's assume that I will guess randomly using this ratio. The theoritical accuracy of random guessing on a binary-classification problem is:\n\n= P(class is 0)* P(guess is 0) + P(class is 1)* P(guess is 1)\n\n=> 0.67* 0.67 + 0.33* 0.33 = 0.5578 or 56%\n\n2. Zero Classifier rule\n>This rule simply predicts majority class in your dataset. In the example above with a 67% - 33% for class 0 and 1, it would predict class 0 for every prediction and achieve an accuracy of 67%. This is 11% better than the theoritical maximum using random guessing.\n\nThis concludes two thhings:\n* Never use random guessing as your baseline accuracy.\n* In imbalanced classification problems, we should use metrics other than accuracy such as confusion matrix or ROC curve.","8574b8e6":"[1. Load libraries and read the data](http:\/\/)\n* > [1.1. Load libraries](http:\/\/)\n* > [1.2. Read the data](http:\/\/)\n\n[2. Overview](http:\/\/)\n* > [2.1. Head](http:\/\/)\n* > [2.2. Dependent variable](http:\/\/)\n* > [2.3. Missing Data](http:\/\/)\n\n[3. Replacing missing data, Outlier treatment and EDA](http:\/\/)\n* > [3.1. Insulin](http:\/\/)\n* > [3.2. SkinThickness ](http:\/\/)\n* > [3.3. BloodPressure ](http:\/\/)\n* > [3.4. BMI ](http:\/\/)\n* > [3.5. Glucose ](http:\/\/)\n* > [3.6. Age ](http:\/\/)\n* > [3.7. Pregnancies ](http:\/\/)\n* > [3.8. DPF ](http:\/\/)\n\n[4. Modelling](http:\/\/)\n* > [4.1. Splitting data into train and test set](http:\/\/)\n* > [4.2. Base models ](http:\/\/)\n> * > 4.2.1. Decision Tree Classifier\n> * > 4.2.2. AdaBoost Classifier\n> * > 4.2.3. XGBoost Classifier\n> * > 4.2.4. LightGBM Classifier\n> * > 4.2.5. Random Forest Classifier\n> * > 4.2.6. k-NN Classifier\n> * > 4.2.1. Logistic Regression\n* > [4.3. Evaluation Metrics ](http:\/\/)\n> * > 4.3.1. Confusion matrix\n> * > 4.3.2. ROC-AUC curve\n> * > 4.3.3. Accuracy\n* > [4.4. Hyperparamter tuning ](http:\/\/)\n> * > 4.4.1. XGBoost Classifier\n> * > 4.4.2. LightGBM Classifier\n> * > 4.4.3. Random Forest Classifer\n\n[5. Final Prediction](http:\/\/)","43c2d6ec":"### 4.2.6. k-NN Classifier","cc48354e":"\n## What is Diabetes ?\n\nAcccording to NIH, \"**Diabetes** \nis a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from the food get into your cells to be used for energy. Sometimes your body doesn\u2019t make enough\u2014or any\u2014insulin or doesn\u2019t use insulin well. Glucose then stays in your blood and doesn\u2019t reach your cells.\n\nOver time, having too much glucose in your blood can cause health problems. Although diabetes has no cure, you can take steps to manage your diabetes and stay healthy.\n\nSometimes people call diabetes \u201ca touch of sugar\u201d or \u201cborderline diabetes.\u201d These terms suggest that someone doesn\u2019t really have diabetes or has a less serious case, but every case of diabetes is serious.\n\n**What are the different types of diabetes?**\nThe most common types of diabetes are type 1, type 2, and gestational diabetes.\n\n**Type 1 diabetes**\nIf you have type 1 diabetes, your body does not make insulin. Your immune system attacks and destroys the cells in your pancreas that make insulin. Type 1 diabetes is usually diagnosed in children and young adults, although it can appear at any age. People with type 1 diabetes need to take insulin every day to stay alive.\n\n**Type 2 diabetes**\nIf you have type 2 diabetes, your body does not make or use insulin well. You can develop type 2 diabetes at any age, even during childhood. However, this type of diabetes occurs most often in middle-aged and older people. Type 2 is the most common type of diabetes.\n\n**Gestational diabetes**\nGestational diabetes develops in some women when they are pregnant. Most of the time, this type of diabetes goes away after the baby is born. However, if you\u2019ve had gestational diabetes, you have a greater chance of developing type 2 diabetes later in life. Sometimes diabetes diagnosed during pregnancy is actually type 2 diabetes.\n\n**Other types of diabetes** \nLess common types include monogenic diabetes, which is an inherited form of diabetes, and cystic fibrosis-related diabetes .\"","defc196b":"**In this section, I have made two datasets(Diabetic and Healthy) to visualize the target count and target percentage with the help of histogram and pie chart.**","accae52b":"# [1. Load libraries and read the data](http:\/\/)\n##  [1.1. Load libraries](http:\/\/)","90e3cdf7":"*Z_score is a popular method for treating outliers where we discard observations having a z_score > 3.*\n\n*For a normal distribution, it is estimated that*\n* 68% of the data points lie between +\/- 1 standard deviation.\n* 95% of the data points lie between +\/- 2 standard deviation\n* 99.7% of the data points lie between +\/- 3 standard deviation\n\n*z_score = (x - median)\/std. deviation.*\n\n*Here, I have taken median instead of mean, because the mean has one main disadvantage: it is particularly susceptible to the influence of outliers, which is why it is wise to use median when you have features which are slightly skewed(or not normally distributed).*","17bf4c99":"*I have made a cross-validation function to test the models based on accuracy on validation data.*","0bf18bc3":"## [4.1. Splitting data into train and test set](http:\/\/)","860b2c22":"### 4.2.7. Logistic Regression","80f0af38":"**Making a seperate dataframe for features containing missing values and the percentage they account out of all observations.**","29c8ce58":"##  [2.2. Dependent variable](http:\/\/)","9a5a7a83":"## [3.6. Age](http:\/\/)","a81b796b":"## [4.4. Hyperparamter tuning](http:\/\/)","47039225":"* XGBoost Classifier","c36d821b":"## [3.5. Glucose](http:\/\/)","44635a52":"* LightGBM Classifier","a99c922b":"* 4.3.3.1. XGBoost Classifier","eb0a46c9":"* Glucose : Plasma glucose concentration a 2 hours in an oral glucose tolerance test","3725fe6c":"### 4.2.3. XGBoost Classifier","11a374dd":"## [3.8. DPF](http:\/\/)","6b0a0dc5":"### 4.2.1. Decision Tree Classifier","a778284b":"* SkinThickness : Triceps skin fold thickness (mm)","f5f8f5b4":"*The Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the \u2018signal\u2019 from the \u2018noise\u2019. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.*\n\n* The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.","869b099e":"*For simplicity, I have renamed DiabetesPedigreeFunction as DPF.*","67089345":"* Logistic Regression","68c053e2":"## [3.4. BMI](http:\/\/)","a323a146":"### 4.3.2. ROC-AUC curve","d7ae2286":"* Age : Age (years)","cc367750":"* Decision Tree Classifier","cfe0dd31":"* DiabetesPedigreeFunction : Diabetes pedigree function","082f978a":"*When looked upon the dependent variable, we found that this is a case of imbalanced class distribution as healthy cases are almost twice the diabetic ones and we will treat it later in the modelling section.*","ae8c3f26":"## [3.3. BloodPressure](http:\/\/)","f8532398":"# [4. Modelling](http:\/\/)","52351991":"* BloodPressure : Diastolic blood pressure (mm Hg)","3c9c3c3c":"## [3.2. SkinThickness](http:\/\/)","d4436633":"*The reduced dataset shape tells us that some observations(Outliers) have been removed.*","431e4db4":"* Random Forest Classifier","5e2f60c3":"*In order to understand what confusion matrix is, let's take an example:*    \n    \n    There are a group of people waiting to examine if they have swine flu. The doctor goes through all the cases and tells the patient if they have the flu or no.\n\nCases in which the doctor predicted YES (they have the disease), and they do have the disease will be termed as TRUE POSITIVES (TP). The doctor has correctly predicted that the patient has the disease.\n\nCases in which the doctor predicted NO (they do not have the disease), and they don\u2019t have the disease will be termed as TRUE NEGATIVES (TN). The doctor has correctly predicted that the patient does not have the disease.\n\nCases in which the doctor predicted YES, and they do not have the disease will be termed as FALSE POSITIVES (FP). Also known as \u201cType I error\u201d.\n\nCases in which the doctor predicted NO, and they have the disease will be termed as FALSE NEGATIVES (FN). Also known as \u201cType II error\u201d.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/0*9r99oJ2PTRi4gYF_.jpg)","cb4cb928":"# [2. Overview](http:\/\/)\n##  [2.1. Head](http:\/\/)","8d38c24e":"*  AdaBoost Classifier","00c7883f":"##  [2.3. Missing Data](http:\/\/)","85d1e4a9":"**Filling the tuned paramters into the respective models.**","e5782241":"**Correlation matrix or Heatmap.**","da3138e4":"* LightGBM Classifier","305823e5":"* BMI : Body mass index (weight in kg\/(height in m)^2)","268586d2":"### 4.3.3. Accuracy","32ea8ad8":"## [3.7. Pregnancies](http:\/\/)","ac3b9c13":"\n## Who are Pima Indians ?\n\n\"The Pima (or Akimel O'odham, also spelled Akimel O'otham, \"River People\", formerly known as Pima) are a group of Native Americans living in an area consisting of what is now central and southern Arizona. The majority population of the surviving two bands of the Akimel O'odham are based in two reservations: the Keli Akimel O'otham on the Gila River Indian Community (GRIC) and the On'k Akimel O'odham on the Salt River Pima-Maricopa Indian Community (SRPMIC).\" Wikipedia","54517b9d":"* Accuracy = (True Positives + True Negatives)\/(True Positives + False Positives + False Negatives + True Negatives)\n\n* Specificity\/ True Negative Rate\/ Precision = True Negatives\/(True Negatives + False Positives)\n\n* Sensitivity\/ True Positive Rate\/ Recall = True Positives\/(True Positives + False Negatives)\n\n* F_1 score = 2\/((1\/Precision) + (1\/Recall))","3f7cc146":"## [1.2. Read the data](http:\/\/)","10215fd0":"*Here, I have made another dataset: Insulin which contains outlier-free observations in order to compare both the plots.*","13ec3661":"### 4.3.1. Confusion matrix","be7e1f38":"* VotingClassifier : It is a meta-classifier for combining similar or conceptually different machine learning classifiers for classification via majority or plurality voting","855c5209":"*This plot tells us about how much a specific feature is correlated with another feature. This can be useful in Feature Selection of most important features while wotking with thousands of features.*","a3ef0eb1":"* Pregnancies : Number of times pregnant","5c2504e3":"## [3.1. Insulin](http:\/\/)","f59ba595":"# [3. Replace missing data, Outlier treatment and EDA](http:\/\/)","c0c849e0":"* k-NN Classifier","2c986d12":"# [5. Final prediction](http:\/\/)","f20549e3":"### 4.2.2. AdaBoost Classifier","05c209c0":"## [4.2. Base models](http:\/\/)","dcb0c7f7":"*The data looks fine, but there's a catch. Some of the features like Glucose and BMI contain 0's which makes no sense, so they might me missing values which have been imputed as zeros to make the data look more clean.*","9806522a":"* Random Forest Classifier","d687d293":"*Now, you must be wondering what outliers and quartiles are?*\n\n*Outliers are those predictions which shows extreme behaviour e.g. they either lie below the min value, above the max value or both, a particular feature possess. It is very important to treat these outliers as these can lead to overfitting the model(or learning too much on training set and not generalising well on test set).*\n\n*A feature is divided into 4 parts or quartiles. In the above box plot, there are some cutoffs. These cut points are lower quartile (or first quartile), median (or second quartile) and upper quartile (or third quartile). Second quartile is the median of the feature. First quartile is the median of the data lower than second quartile. Third quartile is the median of the data grater than the second quartile. Interquartile range (IQR) is calculated by subtracting lower quartile from upper quartile. Outliers can be determined using IQR and lower and upper quartiles.*","5fee4f3e":"## [4.3. Evaluation Metrics](http:\/\/)","e9070933":"### 4.2.4. LightGBM Classifier","5c044cf5":"* Insulin : 2-Hour serum insulin (mu U\/ml)","817a3d56":"### 4.2.5. Random Forest Classifier","9abde1b1":"**Visualizing Outliers and quartiles.**"}}