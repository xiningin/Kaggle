{"cell_type":{"20d48ed7":"code","3229ae01":"code","76f5cb23":"code","b6365d22":"code","cd72ab5b":"code","2d853f36":"code","03c9ca1f":"code","e43b5db5":"code","bc2d6492":"code","4d89b4fe":"code","361af765":"code","11f3aae0":"code","af4ec8af":"code","80fbccfe":"code","49ab8738":"code","ebc808bd":"code","641f4ed3":"code","4d5fadf0":"code","b675fa7b":"code","a036d798":"code","ee110bec":"code","18fed9c9":"code","90432f22":"code","f7ed412d":"code","86b5a340":"code","73218324":"code","7515ea17":"code","060ae27f":"code","051aa5bd":"code","b8da0cf5":"code","5c57f1d9":"code","26b4ae23":"code","071b3269":"code","23b59b2f":"code","cd1168ee":"code","60453bc1":"code","f2b64809":"code","1710d6ee":"code","3283420b":"code","f3454b9d":"code","d85a081f":"code","d7c025b5":"code","06744e93":"code","f1d4fa25":"code","ce4111cf":"code","76937fac":"code","766e8fa7":"code","e2e8e7b4":"code","db6fe2be":"code","c74053bf":"code","8ab68b5f":"code","b7e06c52":"code","7af094f3":"code","1b9d8af4":"markdown","c32f80e3":"markdown","f609aa94":"markdown","6dca1e96":"markdown","34ebc527":"markdown","35fee591":"markdown","2afed8a2":"markdown","cee0ffdb":"markdown","93515f6c":"markdown","13438683":"markdown","7f91ce09":"markdown","ac7dc7fa":"markdown","d3ebd8d7":"markdown","43a2df3f":"markdown","5e35a18c":"markdown","c720a2af":"markdown","abb29388":"markdown","22560d38":"markdown","8e5f74fb":"markdown","449c5f11":"markdown","20dcbd9f":"markdown","b64b7ac2":"markdown","a75a3248":"markdown","48984cdd":"markdown","666a5e99":"markdown","5aecd2a9":"markdown","d476ce27":"markdown","f27a267d":"markdown","e74ead5d":"markdown","9703171c":"markdown","1dea0dac":"markdown"},"source":{"20d48ed7":"#import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3229ae01":"# read the csv file\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df =pd.read_csv('..\/input\/titanic\/test.csv')","76f5cb23":"train_df.head()","b6365d22":"train_df.info()\nprint('\\n')\ntest_df.info()","cd72ab5b":"train_df.describe()","2d853f36":"train_df.describe(include=['O'])","03c9ca1f":"print(pd.isnull(train_df).sum())","e43b5db5":"sns.heatmap(train_df.isnull(),yticklabels=False,cbar=False)","bc2d6492":"g = sns.FacetGrid(train_df, col='Survived',row='Pclass')\ng.map(plt.hist, 'Age', bins=30)","4d89b4fe":"sns.barplot(x=\"Pclass\", y=\"Survived\",data=train_df,palette='rainbow')\n#print percentages of classes that survive\nprint(\"Percentage of Class 1 who survived:\", train_df[\"Survived\"][train_df[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Class 2 who survived:\", train_df[\"Survived\"][train_df[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Class 3 who survived:\", train_df[\"Survived\"][train_df[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","361af765":"sns.countplot(x='Survived',hue='Sex',data=train_df,palette='rainbow')\n#print percentages of females vs. males that survive\nprint(\"Percentage of females who survived:\", train_df[\"Survived\"][train_df[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived:\", train_df[\"Survived\"][train_df[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","11f3aae0":"train_df[['SibSp','Survived']].groupby(by='SibSp').mean().sort_values(by='Survived',ascending=False)","af4ec8af":"train_df[['Parch','Survived']].groupby(by='Parch').mean().sort_values(by='Survived',ascending=False)","80fbccfe":"# look at the median of Age group by classes\ntrain_df.groupby('Pclass')['Age'].describe()","49ab8738":"# set up a function to impute median of Age based on individual's Pclass\ndef impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        \n        if Pclass == 1:\n            return 37\n        \n        elif Pclass == 2:\n            return 29\n        \n        else:\n            return 24\n        \n    else:\n        return Age","ebc808bd":"train_df['Age'] = train_df[['Age','Pclass']].apply(impute_age,axis=1)","641f4ed3":"test_df.groupby('Pclass')['Age'].describe()","4d5fadf0":"def impute_age_test(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        \n        if Pclass == 1:\n            return 42\n        \n        elif Pclass == 2:\n            return 26.5\n        \n        else:\n            return 24\n        \n    else:\n        return Age","b675fa7b":"test_df['Age'] = test_df[['Age','Pclass']].apply(impute_age_test,axis=1)","a036d798":"print(pd.isnull(train_df).sum())","ee110bec":"print(pd.isnull(test_df).sum())","18fed9c9":"# fill na value in Fare with the mean\ntest_df['Fare'].fillna(test_df['Fare'].mean(),inplace=True)","90432f22":"train_df.drop(['Cabin','Ticket'],axis=1,inplace=True)\ntest_df.drop(['Cabin','Ticket'],axis=1,inplace=True)","f7ed412d":"print(pd.isnull(train_df).sum())","86b5a340":"# only 2 missing value in Embarked and it is categorical data so I simply drop it\ntrain_df.dropna(inplace=True)","73218324":"train_df.info()","7515ea17":"# transform the categorical data into dummies variables\ntrain_sex = pd.get_dummies(train_df['Sex'],drop_first=True,dtype='int64')\ntrain_embark = pd.get_dummies(train_df['Embarked'],drop_first=True,dtype='int64')\ntest_sex = pd.get_dummies(test_df['Sex'],drop_first=True,dtype='int64')\ntest_embark = pd.get_dummies(test_df['Embarked'],drop_first=True,dtype='int64')","060ae27f":"# delete the original sex and embarked columns\ntrain_df.drop(['Sex','Embarked'],axis=1,inplace=True)\ntest_df.drop(['Sex','Embarked'],axis=1,inplace=True)","051aa5bd":"# Update the dataframe\ntrain_df = pd.concat([train_df,train_sex,train_embark],axis=1)\ntest_df = pd.concat([test_df,test_sex,test_embark],axis=1)","b8da0cf5":"train_df.info()","5c57f1d9":"#create a combined group\ncombine = [train_df, test_df]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['male'])","26b4ae23":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","071b3269":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)","23b59b2f":"train_df.info()","cd1168ee":"# Drop the orginal column\ntrain_df.drop('Name',axis=1,inplace=True)\ntest_df.drop('Name',axis=1,inplace=True)","60453bc1":"from sklearn.model_selection import train_test_split","f2b64809":"X = train_df.drop(['PassengerId','Survived'],axis=1)\ny = train_df['Survived']","1710d6ee":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","3283420b":"from sklearn.linear_model import LogisticRegression","f3454b9d":"logmodel = LogisticRegression(solver='liblinear')\nlogmodel.fit(X_train,y_train)","d85a081f":"pred_log = logmodel.predict(X_test)","d7c025b5":"from sklearn.metrics import accuracy_score,classification_report,confusion_matrix","06744e93":"print(accuracy_score(y_test,pred_log))\nprint('\\n')\nprint(classification_report(y_test,pred_log))\nprint('\\n')\nprint(confusion_matrix(y_test,pred_log))","f1d4fa25":"from sklearn.tree import DecisionTreeClassifier","ce4111cf":"dtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)","76937fac":"pred_tree = dtree.predict(X_test)","766e8fa7":"print(accuracy_score(y_test,pred_tree))\nprint('\\n')\nprint(classification_report(y_test,pred_tree))\nprint('\\n')\nprint(confusion_matrix(y_test,pred_tree))","e2e8e7b4":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train,y_train)","db6fe2be":"pred_rf = rfc.predict(X_test)","c74053bf":"print(accuracy_score(y_test,pred_rf))\nprint('\\n')\nprint(classification_report(y_test,pred_rf))\nprint('\\n')\nprint(confusion_matrix(y_test,pred_rf))","8ab68b5f":"predictions = rfc.predict(test_df.drop('PassengerId',axis=1))","b7e06c52":"submission = pd.DataFrame({'PassengerId': test_df['PassengerId'],'Survived': predictions})","7af094f3":"submission.to_csv('submission.csv', index=False)","1b9d8af4":"**Pclass feature**","c32f80e3":"**Age feature**\n\nmissing values will be filled by the median of Age grouped by different classes","f609aa94":"# **Data Exploration**","6dca1e96":"**Logistic Regression - train and predict**","34ebc527":"(Parch=3) are the most likely to survive (60%)","35fee591":"(SibSp=1) are the most likely to survive (53.6%)","2afed8a2":"# **Data**\n","cee0ffdb":"# **Data Cleaning**","93515f6c":"**Fare feature**\n\n1 missing value will be filled with the mean of the fare","13438683":"**Sex features**","7f91ce09":"**Drop Cabin and Ticket Column**","ac7dc7fa":"**Parch feature**","d3ebd8d7":"**Reference**\n1. [Titanic Survival Predictions (Beginner)](http:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner)\n2. [Titanic Data Science Solutions](http:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)","43a2df3f":"**Response to previous assumptions**\n* Sex: Womens(Sex=female) are more likely to survive.\n\n  Higly possible\n  \n* SibSp\/Parch: People traveling alone are more likely to survive\n\n  Not really. (SibSp=1) & (Parch=3) are the most likely to survive\n  \n* Age: Young children are more likely to survive.\n\n  Possible. Young children in class 1 & 2 are rarely dead\n  \n* Pclass: The upper-class passengers (Pclass=1) are more likely to survive\n\n  The proportion of survivals in class 1 & 2 are more than that of class 3\n  \n**Sex,Pclass,Age features should be correlated to survivals.**\n","5e35a18c":"I am going to use three models (Decision Tree, Logistic Regression & Random Forest model) to predict. Firstly, I will train up the model by extracting data from train_df. The predication results from three models will be evalued and the best model will be applied to final prediction on test_df.","c720a2af":"**Assumptions**\n\nIn order to predict accurately, I need to explore features which correlate with survivals.It is assumed that:\n\n* Sex: Womens(Sex=female) are more likely to survive.\n* SibSp\/Parch: People traveling alone are more likely to survive.\n* Age: Young children are more likely to survive.\n* Pclass: The upper-class passengers (Pclass=1) are more likely to survive.\n","abb29388":"**Title features**","22560d38":"There are two datasets:**(1) train.csv**, **(2) test.csv**.\n\n**(1) train.csv**\ntrain.csv contains the details of a subset of the passengers on board (891 passengers)\n\nThe values in the column (\"Survived\") can be used to determine whether each passenger survived or not:\n\n* if it's a \"1\", the passenger survived.\n* if it's a \"0\", the passenger died.\n\n**(2) test.csv**\nUsing the patterns I find in train.csv, I have to predict whether the other 418 passengers on board (in test.csv) survived\n\nIn the following parts, I am going to explore the train data and clean up the data. Then I will try to create models and choose the best model to predict based on the features of passengers.","8e5f74fb":"**Sex and Embarked features**","449c5f11":"**About the train_df data**\n* Total passengers in train.csv are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n* There are roughly 20% missing data in Age. I assume that Age is an important feature so I am going to fill the missing value with the mean age of passengers\n* There are roughly 80% missing data in Cabin column and this feature may be dropped.\n* There are 2 missing data in Embarked\n* Ticket may be dropped as there are over 200 duplicates and Ticket inforamtion may not be highly correlated to survival\n* Name is not directly related to survival so It may be dropped.","20dcbd9f":"# **Submmision file**","b64b7ac2":"# **Introduction**\nThis is my kick-started Machine Learning project using Titanic dataset. The purpose of this project is to predict who will survive and who will die. The prediction will be submitted to join the competition.","a75a3248":"**SibSp feature**","48984cdd":"Random forest model scores the hightest with 84.2 in accuracy_score. In the meantime, Decision tree model scores 77.90 and Logistic Regression model scores 83.90.\n\nI decide to apply **Random Forest model** for my prediction.","666a5e99":"# **Data Exploration**","5aecd2a9":"1. Missing value would be filled by imputation\n2. Sex and Embarked columns will be transformed into dummies variables\n3. Name could be transformed into new feature by extracting their titles\n4. Ticket and cabin will be dropped","d476ce27":"**Decision tree - train and test**","f27a267d":"**Indentify the data type of features**\n* **Categorical:** Survived,Pclass, Sex, and Embarked\n* **Numerical Features:** Age (Continuous), Fare (Continuous), SibSp (Discrete), Parch (Discrete)\n* **Alphanumeric Features:** Ticket, Cabin\n* **Typos:** Name","e74ead5d":"Clean up test_df using the same approach","9703171c":"# **Building model**","1dea0dac":"**Random forests - train and test**"}}