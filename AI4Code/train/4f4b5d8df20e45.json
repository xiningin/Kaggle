{"cell_type":{"cf5ff251":"code","6456f887":"code","ae17462a":"code","d8916f12":"code","f6dc5447":"code","87d65374":"code","fa12f6d7":"code","12386159":"code","72d55d67":"code","10781097":"code","bd5a7972":"code","10d655ba":"code","c81fd007":"code","f234ae30":"code","bfd06eda":"code","ebcbcecf":"code","3961e627":"code","781c20cd":"code","60edbb54":"code","53f6b275":"code","4e7d11fb":"code","c4b0ad25":"code","25a03ca8":"markdown","8e4abd5a":"markdown","e70be038":"markdown","a9b0198d":"markdown","9283d99b":"markdown","037007b7":"markdown","5fad44aa":"markdown","4d0c6961":"markdown","7b56560b":"markdown","0568543c":"markdown","093a5304":"markdown","d1caa73f":"markdown","305a26aa":"markdown","0ee9ba33":"markdown","db751955":"markdown","c059800b":"markdown","3f6a5c14":"markdown"},"source":{"cf5ff251":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6456f887":"# Importing required Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ae17462a":"import pandas as pd\ndataset = pd.read_csv(\"..\/input\/titanic\/train.csv\",header=0)\ndataset.head()","d8916f12":"dataset = dataset.drop(columns=['Name', 'PassengerId','Ticket'], axis=0)\ndataset.head()","f6dc5447":"ax = sns.countplot(x=\"Pclass\", hue = \"Survived\", data=dataset)","87d65374":"ax1 = sns.countplot(x=\"Survived\", hue = \"Sex\", data=dataset)","fa12f6d7":"ageclass = [\"0-20\",\"21-40\",\"41-60\",\"61-80\",\"81-100\",\"100+\"]\n#We make groups of age, in the range of 20 years\n\nagesurvival = pd.DataFrame(index = ageclass, columns = ['0','1'])\nagesurvival.loc[:,:] = 0\n#Initialising the Dataframe with 0\n\nfor index,rows in dataset.iterrows():\n    if rows[\"Age\"] <= 20:\n        if rows[\"Survived\"] == 0:\n            agesurvival.loc[\"0-20\"]['0'] += 1\n        else:\n            agesurvival.loc[\"0-20\"]['1'] += 1\n    elif rows[\"Age\"] > 20 and rows[\"Age\"] <= 40:\n        if rows[\"Survived\"] == 0:\n            agesurvival.loc[\"21-40\"]['0'] += 1\n        else:\n            agesurvival.loc[\"21-40\"]['1'] += 1\n    elif rows[\"Age\"] > 40 and rows[\"Age\"] <= 60:\n        if rows[\"Survived\"] == 0:\n            agesurvival.loc[\"41-60\"]['0'] += 1\n        else:\n            agesurvival.loc[\"41-60\"]['1'] += 1\n    elif rows[\"Age\"] > 60 and rows[\"Age\"] <= 80:\n        if rows[\"Survived\"] == 0:\n            agesurvival.loc[\"61-80\"]['0'] += 1\n        else:\n            agesurvival.loc[\"61-80\"]['1'] += 1\n    elif rows[\"Age\"] > 80 and rows[\"Age\"] <= 100:\n        if rows[\"Survived\"] == 0:\n            agesurvival.loc[\"81-100\"]['0'] += 1\n        else:\n            agesurvival.loc[\"81-100\"]['1'] += 1\n    elif rows[\"Age\"] > 100:\n        if rows[\"Survived\"] == 0:\n            agesurvival.loc[\"100+\"]['0'] += 1\n        else:\n            agesurvival.loc[\"100+\"]['1'] += 1\n        \nagesurvival.plot.bar(rot=0)\n\n#We get the following graph:","12386159":"#Separating dependent and independent variables\nx = dataset.iloc[:,1:]\ny = dataset.iloc[:,0]","72d55d67":"x.isna().sum()","10781097":"del dataset[\"Cabin\"]\n#dataset = dataset.drop(columns=['Name', 'PassengerId','Ticket'], axis=0)\ndataset.head()","bd5a7972":"#Separating dependent and independent variables. [Repeating without the 'Cabin' column]\nx = dataset.iloc[:,1:]\ny = dataset.iloc[:,0]\nx","10d655ba":"from sklearn.impute import SimpleImputer \nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer = imputer.fit(x.iloc[:, 2:3])\nx.iloc[:, 2:3] = imputer.transform(x.iloc[:, 2:3])\nx","c81fd007":"imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputer = imputer.fit(x.iloc[:, 6:7])\nx.iloc[:, 6:7] = imputer.transform(x.iloc[:, 6:7])\nx","f234ae30":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nlabelencoder_X = LabelEncoder()\n\nx.iloc[:, 0] = labelencoder_X.fit_transform(x.iloc[:,0])\nx.iloc[:, 1] = labelencoder_X.fit_transform(x.iloc[:,1])\nx.iloc[:, 6] = labelencoder_X.fit_transform(x.iloc[:,6])\n\ncolumnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0,1,6])], remainder='passthrough')\nx = columnTransformer.fit_transform(x)\n","bfd06eda":"x","ebcbcecf":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx = sc.fit_transform(x)","3961e627":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)","781c20cd":"import keras\n#sequential - initialize NN\nfrom keras.models import Sequential\n\n#dense - build NN\nfrom keras.layers import Dense,Dropout\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\n#init - \n#activation - \nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 12))\nclassifier.add(Dropout(0.3))\n# Adding the second hidden layer\nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.3))\n# Adding the third hidden layer\nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.3))\n# Adding the fourth hidden layer\nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\nclassifier.add(Dropout(0.3))\n# Adding the output layer\n#sigmoid - will give probabilities of classes\nclassifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6), loss = 'mean_squared_error', metrics = ['accuracy'])\n\n\n# Fitting the ANN to the Training set\nhistory = classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 500,validation_split=0.25)\n","60edbb54":"#Preparing the test set\ndatasetest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndatasetest = datasetest.drop(columns=['Name', 'PassengerId','Ticket','Cabin'])\n\n#Separating dependent and independent variables\nx_test = datasetest.iloc[:,:]\n#y_test = no independent variable in test set\n\n#Taking care of missing values\nfrom sklearn.impute import SimpleImputer \nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer = imputer.fit(x_test.iloc[:, 2:3])\nx_test.iloc[:, 2:3] = imputer.transform(x_test.iloc[:, 2:3])\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputer = imputer.fit(x_test.iloc[:, 6:7])\nx_test.iloc[:, 6:7] = imputer.transform(x_test.iloc[:, 6:7])\n\n\n#Taking care of categorical variables\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nlabelencoder_X = LabelEncoder()\n\nx_test.iloc[:, 0] = labelencoder_X.fit_transform(x_test.iloc[:,0])\nx_test.iloc[:, 1] = labelencoder_X.fit_transform(x_test.iloc[:,1])\nx_test.iloc[:, 6] = labelencoder_X.fit_transform(x_test.iloc[:,6])\n\ncolumnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0,1,6])], remainder='passthrough')\nx_test = columnTransformer.fit_transform(x_test)\n\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_test = sc.fit_transform(x_test)\n","53f6b275":"y_pred = classifier.predict(x_test)\ny_pred1 = []\nfor i in range(len(y_pred)):\n    if y_pred[i] > 0.5:\n        y_pred1.append(1)\n    else:\n        y_pred1.append(0)","4e7d11fb":"result = pd.DataFrame(columns=['PassengerId','Survived'])\nfor i in range(len(y_pred1)):\n    result = result.append([{'PassengerId':i+892, 'Survived':y_pred1[i]}], ignore_index = True)","c4b0ad25":"result.head()","25a03ca8":"**Taking care of categorical variables**","8e4abd5a":"Splitting data into test set and training set","e70be038":"*As we can see 687 rows in the Cabin column are nan. which is about 77% of the records. We can safely discard this column as well*","a9b0198d":"**The number of people who died in a certain age group, is maximum for the group \"21 to 40 years\". This might be because, children and senior citizens were given priority to evacuate from the ship**","9283d99b":"*Here we replace the nan values in 'Age' column with the mean of ages in other column*","037007b7":"**Taking care of missing values**","5fad44aa":"**In this plot, we can see the relationship between the gender of a passenger and their chance of survival. The barplot clearly shows that, among the passengers who did not survive, a higher number of them were males. On the other hand, among those who survived, a majority of them were females. The reason for this could be that women were evacuated from the ship before men, and hence the result **","4d0c6961":"**Once our model is ready, we perform similar prepossessing steps on our testing data:**","7b56560b":"*Here we replace the nan values in 'Embarked' column with the most frequently occuring value in that column*","0568543c":"**Importing the Keras libraries and packages**","093a5304":"Next we try to find the relation between the age of passengers and their survival chance:\nFor that, we need to do some more operations on the data, to make it useful for plotting a chart","d1caa73f":"*Like these, more graphs can be plotted, that show how many men and women of age group 21 to 40 years died \/ lived, how many people of class 3 that survived were men\/women and so on. For now, we plot these 3 graphs and move further.*","305a26aa":"**Preparing the data for predictions**","0ee9ba33":"*A few attributes were unnecessary, such as Name, passengerId and the ticket number, so we remove them*","db751955":"**Feature Scaling**","c059800b":"**As we can see, more people belonging to class 1 survived and more people belong to class 3 died. This shows that there is *some* relation between class of passenger and their survival rates. It could be that, passengers of class 1 were kept on the upper decks and could be rescued before the other classes, hence the result**","3f6a5c14":"**Analysis of data**\n\n*Next, I performed some exploratory data analysis, to look for some trends in data, like relation of Sex and Survival chance, age and survival chance and the class of passengers and their survival chance. Here are the plots*\n"}}