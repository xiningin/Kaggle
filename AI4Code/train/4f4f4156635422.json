{"cell_type":{"9ed67ff2":"code","ec52c073":"code","69b7ee45":"code","fff703be":"code","f6d78b02":"code","e01ccb04":"code","dec1c362":"code","4adcb456":"code","4c80675d":"code","0500fbee":"code","29918e7b":"code","31957e43":"code","7569e235":"code","79a71534":"code","3d6d4050":"code","903b4c37":"code","b7c460f6":"code","12ea07eb":"code","3462a8f2":"code","e965ccda":"code","4a150f98":"code","3e07182a":"code","8ee8b615":"code","280e11ef":"code","06e09f1a":"code","a46d57be":"code","72fc8680":"code","8bdf2784":"code","72fb7ded":"markdown","0585cf9b":"markdown","1eb66d46":"markdown","07341a3e":"markdown","d63f409b":"markdown","2a20672d":"markdown","5a089dcb":"markdown","8105a30b":"markdown"},"source":{"9ed67ff2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec52c073":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport os\nimport time","69b7ee45":"#check if decoding is needed: text may need to be decoded as utf-8\ntext = open('\/kaggle\/input\/shakespeare-text\/text.txt', 'r').read() \nprint(text[:200])","fff703be":"#Find Vocabulary (set of characters)\nvocabulary = sorted(set(text))\nprint('No. of unique characters: {}'.format(len(vocabulary)))","f6d78b02":"#character to index mapping\nchar2index = {c:i for i,c in enumerate(vocabulary)}\nint_text = np.array([char2index[i] for i in text])\n\n#Index to character mapping\nindex2char = np.array(vocabulary)","e01ccb04":"#Testing\nprint(\"Character to Index: \\n\")\nfor char,_ in zip(char2index, range(65)):\n    print('  {:4s}: {:3d}'.format(repr(char), char2index[char]))\n\nprint(\"\\nInput text to Integer: \\n\")\nprint('{} mapped to {}'.format(repr(text[:20]),int_text[:20])) #use repr() for debugging","dec1c362":"seq_length= 150 #max number of characters that can be fed as a single input\nexamples_per_epoch = len(text)\n\n#converts text (vector) into character index stream\n#Reference: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset\nchar_dataset = tf.data.Dataset.from_tensor_slices(int_text)","4adcb456":"#Create sequences from the individual characters. Our required size will be seq_length + 1 (character RNN)\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)","4c80675d":"#Testing\nprint(\"Character Stream: \\n\")\nfor i in char_dataset.take(10):\n  print(index2char[i.numpy()])  \n\nprint(\"\\nSequence: \\n\")\nfor i in sequences.take(10):\n  print(repr(''.join(index2char[i.numpy()])))  #use repr() for more clarity. str() keeps formatting it","0500fbee":"def create_input_target_pair(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(create_input_target_pair)","29918e7b":"#Testing\nfor input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(''.join(index2char[input_example.numpy()])))\n  print ('Target data:', repr(''.join(index2char[target_example.numpy()])))","31957e43":"#Creating batches\n\nBATCH_SIZE = 64\n\n# Buffer used to shuffle the dataset \n# Reference: https:\/\/stackoverflow.com\/questions\/46444018\/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","7569e235":"vocab_size = len(vocabulary)\nembedding_dim = 256\nrnn_units= 1024","79a71534":"def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.LSTM(rnn_units, \n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n    return model\n\n# Reference for theory: https:\/\/jhui.github.io\/2017\/03\/15\/RNN-LSTM-GRU\/","3d6d4050":"lstm_model = build_model_lstm(\n  vocab_size = vocab_size,\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)","903b4c37":"#Testing: shape\nfor input_example_batch, target_example_batch in dataset.take(1):\n    example_prediction = lstm_model(input_example_batch)\n    assert (example_prediction.shape == (BATCH_SIZE, seq_length, vocab_size)), \"Shape error\"\n    #print(example_prediction.shape)","b7c460f6":"#model.summary() \n#check shapes if necessary","12ea07eb":"sampled_indices = tf.random.categorical(example_prediction[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()","3462a8f2":"def loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\n#Loss Function reference: https:\/\/www.dlology.com\/blog\/how-to-use-keras-sparse_categorical_crossentropy\/\n\nexample_loss  = loss(target_example_batch, example_prediction)\nprint(\"Prediction shape: \", example_prediction.shape)\nprint(\"Loss:      \", example_loss.numpy().mean())","e965ccda":"lstm_model.compile(optimizer='adam', loss=loss)","4a150f98":"lstm_dir_checkpoints= '.\/training_checkpoints_LSTM'\ncheckpoint_prefix = os.path.join(lstm_dir_checkpoints, \"checkpt_{epoch}\") #name\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)","3e07182a":"EPOCHS=60 #increase number of epochs for better results (lesser loss)","8ee8b615":"history = lstm_model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","280e11ef":"tf.train.latest_checkpoint(lstm_dir_checkpoints)","06e09f1a":"lstm_model = build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size=1)\nlstm_model.load_weights(tf.train.latest_checkpoint(lstm_dir_checkpoints))\nlstm_model.build(tf.TensorShape([1, None]))\n\nlstm_model.summary()","a46d57be":"def generate_text(model, start_string):\n    num_generate = 1000 #Number of characters to be generated\n\n    input_eval = [char2index[s] for s in start_string] #vectorising input\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    text_generated = []\n\n    # Low temperatures results in more predictable text.\n    # Higher temperatures results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 0.5\n\n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # We pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(index2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","72fc8680":"#Testing\n#print(generate_text(lstm_model, start_string=u\"ROMEO: \"))","8bdf2784":"#Prediction with User Input\nlstm_test = input(\"Enter your starting string: \")\nprint(generate_text(lstm_model, start_string=lstm_test))\n","72fb7ded":"3 Layers used:\n\nInput Layer: Maps character to 256 dimension vector\n\nGRU Layer: RNN of size 1024\n\nDense Layer: Output with same size as vocabulary\n\nSince it is a character level RNN, we can use keras.Sequential model (All layers have single input and single output).","0585cf9b":"## Building the Model","1eb66d46":"## Prediction","07341a3e":"## Preprocessing Text","d63f409b":"## Model Training","2a20672d":"## Create Training Data","5a089dcb":"\nTarget value: for each sequence of characters, we return that sequence, shifted one position to the right, along with the new character that is predicted to follow the sequence.\n\nTo create training examples of (input, target) pairs, we take the given sequence. The input is sequence with last word removed. Target is sequence with first word removed. Example: sequence: abc d ef input: abc d e target: bc d ef","8105a30b":"# Generating Shakespearean Text with Character Based RNNs\n\nProblem Statement: Given a character or sequence of characters, we want to predict the next character at each time step. Model is trained to follow a language similar to the works of Shakespeare. The tinyshakespear dataset is used for training."}}