{"cell_type":{"dacd1651":"code","413d966c":"code","a02e8b77":"code","09a980cb":"code","66df6830":"code","3dc78c96":"code","fd082386":"code","f4598bbe":"code","724c8a48":"code","4aa93879":"code","1450f6ba":"code","763d4533":"code","df3773df":"markdown","11bf66af":"markdown","b35e5a81":"markdown","6396de3c":"markdown","2823e61f":"markdown","ce5a93ea":"markdown","244fd5e1":"markdown"},"source":{"dacd1651":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","413d966c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n%matplotlib inline\n","a02e8b77":"#resd csv file\ndf=pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\ndf.head()","09a980cb":"#check null values in Dataset\ndf.isnull().sum()","66df6830":"dummies = pd.get_dummies(df, drop_first=True)\ndummies.head()","3dc78c96":"# Split data features into dependent and independent\n# X is Independent variables\n# y is dependent variables\nX = dummies.iloc[:,1:]\ny = dummies.iloc[:,0]","fd082386":"model = ExtraTreesClassifier()\nmodel.fit(X,y)","f4598bbe":"model.feature_importances_","724c8a48":"plt.figure(figsize=(10, 25))\nfeature_rank = pd.Series(model.feature_importances_, index = X.columns)\nfeature_rank.nlargest(95).plot(kind = \"barh\")","4aa93879":"drop_column = feature_rank.nsmallest(17).index\nX.drop(drop_column, axis = 1, inplace=True )","1450f6ba":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\nrf_model = RandomForestClassifier()\nrf_model.fit(X_train, y_train)","763d4533":"y_predict = rf_model.predict(X_test)\nprint(\"Score : {}\".format(rf_model.score(X_test, y_test)))\nprint(\"Mean absolute error : {}\".format(metrics.mean_absolute_error(y_test, y_predict)))\nprint(\"R-square Value : {}\".format(metrics.r2_score(y_test, y_predict)))","df3773df":"### feature Score","11bf66af":"> ## Feature Selection","b35e5a81":"> ## Model Building","6396de3c":"> ## Essential Libraries","2823e61f":"> ## Features Encoding using One Hot Encoding Technique","ce5a93ea":"### In above Grraph We include, First 17 Columns have less importance so Drop this columns","244fd5e1":"> ## Check Accurracy of Model"}}