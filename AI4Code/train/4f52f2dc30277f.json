{"cell_type":{"338a8510":"code","f3b89292":"code","fa4542e8":"code","657b3745":"code","f8737673":"code","2bebd9c8":"code","b7cd5007":"code","218444a4":"code","692ba24f":"code","8f490b58":"markdown"},"source":{"338a8510":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f3b89292":"import requests\nimport requests\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom nltk.probability import FreqDist\nfrom heapq import nlargest\nfrom collections import defaultdict","fa4542e8":"#instantiate all global variables\nsoup = \"\"\ntext = \"\"\nsents = \"\"\nword_sent = \"\"\n_stopwords = \"\"\nword_sent = \"\"\nfreq = \"\"\nranking = \"\"\nsents_idx = \"\"","657b3745":"#\n\ndef inputURL():\n    \n    global soup\n    articleURL=\"https:\/\/arstechnica.com\/cars\/2018\/10\/honda-will-use-gms-self-driving-technology-invest-2-75-billion\/\"\n    response = requests.get(articleURL)\n    response.encoding = 'utf-8'\n    data = response.text\n    soup = BeautifulSoup(data)\n    ","f8737673":"#scrub text\n\ndef cleanText():\n    global text\n    \n    #initiallizes the text variable\n    text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\n    \n    text = text.replace(\"\\n\", \"\")  #This takes out all \\n in the text\n    text = text.replace(\".\", \". \")\n    text = text.replace(\"\\'s\", \"\") #this takes out the 's that has been showing up in the highest frequency table\n\n    text.encode('ascii', 'ignore')","2bebd9c8":"#create values for learner\n\ndef learnerVars():\n    global sents\n    global word_sent\n    global _stopwords\n    global word_sent\n    global freq\n    \n    sents = sent_tokenize(text)\n    word_sent = word_tokenize(text.lower())\n    _stopwords = set(stopwords.words('english') + list(punctuation))\n    word_sent=[word for word in word_sent if word not in _stopwords]\n    freq = FreqDist(word_sent)","b7cd5007":"# function for word rankings\n\ndef findRankings():\n    global ranking\n    \n    ranking = defaultdict(int)\n    \n    for i,sent in enumerate(sents):\n        for w in word_tokenize(sent.lower()):\n            if w in freq:\n                ranking[i] += freq[w]\n","218444a4":"#find top three sentences \n\ndef topThree():\n    global sents_idx\n    \n    sents_idx = nlargest(3, ranking, key=ranking.get)\n    newOutput = [sents[j] for j in sorted(sents_idx)]\n    \n    print(\"\\nYour Summary:\\n\")\n    \n    #print final summary\n    for j in newOutput:\n        print(j)","692ba24f":"#Final run through of all functions in one block\n\ninputURL()\ncleanText()\nlearnerVars()\nfindRankings()\ntopThree()","8f490b58":"# TL;DR - Automated Gist\n## Find the most important words\n### Word Importance = Word Frequency\n## Compute a significance score for sentences based on words they contain\n### Significant score = Sum(Word Importance)\n## Pick the top most significant sentences\n\n* Retrieve Text\n* Preprocess Text\n* Extract Sentences\n\n#### Source: PluralSight - Natural Langauge Processing"}}