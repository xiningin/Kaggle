{"cell_type":{"25965c8e":"code","5e3e5056":"code","4b93de5f":"code","a1b79096":"code","0d6da827":"code","de56302d":"code","c91cd1f3":"code","e77e3a0d":"code","067e2053":"code","25580b8f":"code","37502edf":"code","8d561010":"code","a52f826e":"code","4e332e9f":"code","d68ec357":"code","b8bd6f3f":"code","b5f493ed":"code","606280e5":"code","a7e4cef7":"code","8cbb9ea5":"code","b8ff0e02":"code","fe598d14":"code","d4387cbd":"code","4eb5af63":"code","0e346773":"code","f0a75fdd":"code","0468da0f":"code","a76356ac":"code","d1bd5b2e":"code","9dc3312a":"code","06811d94":"markdown","968dfc62":"markdown","e754badd":"markdown","7e62fa6d":"markdown","0ea71815":"markdown","f4e23345":"markdown","19deeb2b":"markdown","746fe2e9":"markdown","a99e0694":"markdown","cca5de7f":"markdown","6f55b312":"markdown","8af84e96":"markdown","aab12778":"markdown","7393892d":"markdown","b4a6c82a":"markdown","6dfad935":"markdown","98653953":"markdown","4689bedc":"markdown","52731320":"markdown","375adf62":"markdown","2ca52cff":"markdown","a1af5aa8":"markdown","a61325f8":"markdown","f862aa9b":"markdown","e8984b22":"markdown","e42922b4":"markdown","938f37b1":"markdown","edd578b5":"markdown","78d1397f":"markdown","2140c681":"markdown","e766d2ec":"markdown"},"source":{"25965c8e":"%matplotlib inline\nimport numpy as np                 # linear algebra\nimport pandas as pd                # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport datetime as dt\nfrom sklearn import linear_model\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as cal1\nimport calendar\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# let's remove some of the warnings we get from the libraries\n\n\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","5e3e5056":"MyTrain = pd.read_csv('..\/input\/train.csv',parse_dates=['date'], index_col=['date'])\nMyTest  = pd.read_csv('..\/input\/test.csv',parse_dates=['date'], index_col=['date'])\n\nprint(\"Number of training samples read : \", len(MyTrain) )\nprint(\"        Shape is (rows, columns): \", MyTrain.shape)\nprint(\"Number of testing samples read  : \", len(MyTest)  )\nprint(\"        Shape is (rows, columns): \", MyTest.shape )\n","4b93de5f":"cal = cal1()\nholidays = cal.holidays(start=MyTrain.index.min(), end=MyTrain.index.max())","a1b79096":"MyTrain.describe()","0d6da827":"ZeroSales = MyTrain[MyTrain.sales == 0]\nZeroSales","de56302d":"g = sns.FacetGrid(data=MyTrain,row='item',col='store')\ng = g.map(plt.plot,'sales') ","c91cd1f3":"MyTrain.groupby(MyTrain.index).sum()['sales'].plot(style='.',figsize=(20,5))","e77e3a0d":"MyTrain.groupby(MyTrain.index).sum()['sales'][0:160].plot(style='.',figsize=(20,5))","067e2053":"MyTrain['year']  = MyTrain.index.year\nMyTrain['month'] = MyTrain.index.month\nMyTrain['DoM']   = MyTrain.index.day\nMyTrain['DoW']   = MyTrain.index.dayofweek # Mondays are 0\nMyTrain['DoY']   = MyTrain.index.dayofyear\n\nMyTrain['Holiday'] = MyTrain.index.isin(holidays)\nMyTrainHolidays = MyTrain[MyTrain.Holiday==True]\n\nplt.figure(figsize=(20,14))\n\nplt.subplot(221)\nplt.title('Sales per individual store\/item for each day-of-week')\nsns.boxplot(x=MyTrain.DoW,y=MyTrain.sales)\n\ny = MyTrain.groupby(MyTrain.index).sum().sales\nx = MyTrain.groupby(MyTrain.index).min().DoW\n\nplt.subplot(222)\nplt.title('Aggregated sales over all stores\/items for each day-of-week')\nsns.boxplot(x=x,y=y)\n\n\n\nplt.subplot(223)\nplt.title('Holiday Sales per individual store\/item for each day-of-week')\nx=MyTrainHolidays.DoW\ny=MyTrainHolidays.sales\nsns.boxplot(x=x,y=y)\n\n\nplt.subplot(224)\nplt.title('Holiday Aggregates Sales over all store\/item for each day-of-week')\nx=MyTrainHolidays.groupby(MyTrainHolidays.index).min().DoW\ny=MyTrainHolidays.groupby(MyTrainHolidays.index).sum().sales\nsns.boxplot(x=x,y=y)\n\n","25580b8f":"# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\n\n# Take total sale for each day and index days with ordinal values.\nGroupedMyTrain = MyTrain.groupby(MyTrain.index).sum()['sales'].reset_index()\nGroupedMyTrain['OrdinalDate'] = GroupedMyTrain['date'].map(dt.datetime.toordinal)\n\n# prepare x and y data frames\nx = GroupedMyTrain[['OrdinalDate']] \ny = GroupedMyTrain[['sales']]\n\nfit = regr.fit(x,y)\nP   = regr.predict(x) \n\nplt.figure(figsize=(20,8))\nplt.annotate('Coef: {}'.format(regr.coef_) , xy=(x.iloc[750].OrdinalDate,P[[750]]),  xycoords='data',\n            xytext=(0.35, 0.85), textcoords='axes fraction',\n            arrowprops=dict(facecolor='green', shrink=1),\n            horizontalalignment='right', verticalalignment='top',\n            )\n\nplt.plot(GroupedMyTrain[['date']], y,  color='black', marker='.'  , ls=' ' , markersize=2 )\nplt.plot(GroupedMyTrain[['date']], P,  color='green' , linewidth=4 )\n","37502edf":"yCor = y - P\nyCor.plot(figsize=(20,5),marker='.',markersize=2,ls=' ')","8d561010":"yCor = y * P[0]\/P \nyCor.plot(figsize=(20,5),marker='.',markersize=2,ls=' ')","a52f826e":"plt.figure(figsize=(20,5))\n\nplt.subplot(121)\nplt.plot(\n    x.OrdinalDate[0:140],\n    MyTrain.reset_index().groupby(MyTrain.index).sum()['sales'][0:140],\n    marker='.',ls=' ',color='blue')\nplt.plot(\n    x.OrdinalDate[0:140],\n    yCor[0:140],\n    color='red',marker='.',ls=' ')\n\nplt.subplot(122)\nplt.plot(\n    x.OrdinalDate[-140:],\n    MyTrain.groupby(MyTrain.index).sum()['sales'][-140:],\n    marker='.',ls=' ',color='blue')\nplt.plot(\n    x.OrdinalDate[-140:],\n    yCor[-140:],\n    color='red',marker='.',ls=' ')\n","4e332e9f":"yS = yCor\nyS['date']  = GroupedMyTrain.date\nyS['DoY']   = yS.date.dt.dayofyear\nyS['year']  = yS.date.dt.year\nyS['DoW']   = yS.date.dt.dayofweek\nyS['month'] = yS.date.dt.month\n\n\npattern = yS.groupby(yS.DoY).sum()['sales'] \/ 10 \/50 \/ 5","d68ec357":"pattern.plot(marker='.',ls=' ',markersize=2,figsize=(20,5))","b8bd6f3f":"DoWFactor = yS.groupby(yS.DoW).sum().sales\nDoWFactor = DoWFactor \/ DoWFactor.mean()\nplt.bar(x=DoWFactor.index, height=DoWFactor)","b5f493ed":"yS2 = MyTrain.groupby([MyTrain.index,MyTrain.DoW]).sum()['sales'].reset_index()\nyS2['DoY']   = yS2.date.dt.dayofyear\nyS2['month'] = yS2.date.dt.month\nyS2['year'] = yS.date.dt.year\nyS2['DiM']   = yS2.date.dt.day\nyS2['DoW']   = yS2.date.dt.dayofweek\nyS2['DoWFactor']   = yS2.groupby(yS2.index).min()['DoW'].map(lambda x: DoWFactor[x])\nyS2['GFactor'] = P[0] \/ P\nyS2['CorrSales'] = yS2.sales * yS2.GFactor \/yS2.DoWFactor\n\nyS2 = yS2.set_index('date')\n\npattern = yS2.groupby('DoY').sum()['CorrSales'] \/ 10 \/50 \/ 5\npattern.plot(marker='.',ls=' ',markersize=2,figsize=(20,5))\n","606280e5":"plt.figure(figsize=(20,5))\nplt.subplot(121)\npattern[90:120].plot(marker='.',ls=' ',markersize=10)\nplt.subplot(122)\nsns.distplot(pattern[90:120])\nplt.show()","a7e4cef7":"pattern = yS2.groupby(['month','DiM']).sum()['CorrSales'] \/ 10 \/50 \/ 5\npattern.plot(marker='.',ls=' ',markersize=3,figsize=(20,5))","8cbb9ea5":"MonthFactor = yS2.groupby(yS2.month).sum().sales\nMonthFactor = MonthFactor \/ MonthFactor.mean()\nplt.bar(x=MonthFactor.index, height=MonthFactor)","b8ff0e02":"GroupedMyTrain['DoWFactor']   = MyTrain.groupby(MyTrain.index).min().reset_index()['DoW'].map(lambda x: DoWFactor[x])\nGroupedMyTrain['MonthFactor'] = MyTrain.groupby(MyTrain.index).min().reset_index().date.dt.month.map(lambda x: MonthFactor[x])\nGroupedMyTrain['GeomFactor']  = P \/P[0] # make sure that those P values still come from the Geom calculation","fe598d14":"RefVal = GroupedMyTrain.sales[0] \/ GroupedMyTrain['DoWFactor'][0] \/GroupedMyTrain['MonthFactor'][0]\nGroupedMyTrain['Predicted'] =       \\\n    RefVal                        * \\\n    GroupedMyTrain['DoWFactor']   * \\\n    GroupedMyTrain['MonthFactor'] * \\\n    GroupedMyTrain['GeomFactor']","d4387cbd":"GroupedMyTrain[['Predicted','sales']].plot(figsize=(20,5),marker='.',markersize=2,ls=' ')","4eb5af63":"GroupedMyTrain['Delta'] = GroupedMyTrain['Predicted'] - GroupedMyTrain['sales']\nGroupedMyTrain.plot(x='date',y='Delta',figsize=(20,5),marker='.',markersize=2,ls=' ',grid=True)","0e346773":"import calendar\n\ndef DiM(MyDate) :\n    MyYear = MyDate.year \n    MyMonth = MyDate.month\n    return calendar.monthrange(MyYear,MyMonth)[1]\n\nGroupedMyTrain['year'] = GroupedMyTrain.date.dt.year\nGroupedMyTrain['month'] = GroupedMyTrain.date.dt.month\nGroupedMyTrain['DayInMonth'] = GroupedMyTrain.date.map(lambda x: DiM(x))\nGroupedMyTrain['DiMFactor']  = 31 \/ GroupedMyTrain['DayInMonth'] \n\n\nGeomFactor2 = GroupedMyTrain.groupby('year').sum()['sales']\nGeomFactor2 =  GeomFactor2\/GeomFactor2[2013]\n\nGroupedMyTrain['GeomFactor2'] = GroupedMyTrain.year.map(lambda x: GeomFactor2[x])","f0a75fdd":"RefVal = GroupedMyTrain.sales[0] \/ GroupedMyTrain['DoWFactor'][0] \/GroupedMyTrain['MonthFactor'][0]\nGroupedMyTrain['Predicted'] =       \\\n    RefVal                        * \\\n    GroupedMyTrain['DoWFactor']   * \\\n    GroupedMyTrain['MonthFactor'] * \\\n    GroupedMyTrain['GeomFactor2'] * \\\n    GroupedMyTrain['DiMFactor']","0468da0f":"GroupedMyTrain['Delta'] = GroupedMyTrain['Predicted'] - GroupedMyTrain['sales']\nGroupedMyTrain.plot(x='date',y='Delta',figsize=(20,5),marker='.',markersize=2,ls=' ')\n\n","a76356ac":" GroupedMyTrain['DiMFactor'][(GroupedMyTrain.year==2016) & (GroupedMyTrain.month==2)] = 31.0 \/ 28\nGroupedMyTrain['Predicted'] =       \\\n    RefVal                        * \\\n    GroupedMyTrain['DoWFactor']   * \\\n    GroupedMyTrain['MonthFactor'] * \\\n    GroupedMyTrain['GeomFactor2'] * \\\n    GroupedMyTrain['DiMFactor']\nGroupedMyTrain['Delta'] = GroupedMyTrain['Predicted'] - GroupedMyTrain['sales']\nGroupedMyTrain.plot(x='date',y='Delta',figsize=(20,5),marker='.',markersize=2,ls=' ')\n\n\n","d1bd5b2e":"\nfrom scipy import stats\nz,p = stats.normaltest(GroupedMyTrain['Delta'])\nsns.distplot(GroupedMyTrain['Delta'])\nplt.title(\"z={:4.2e}    p={:4.2e}\".format(z,p))","9dc3312a":"from statsmodels.tsa.seasonal import seasonal_decompose\nSeasonal = MyTrain.groupby(MyTrain.index).sum()['sales']\nSDW = seasonal_decompose(Seasonal, model='additive',freq=7)\nSDA = seasonal_decompose(SDW.trend.dropna(), model='additive',freq=365)\nfig = plt.figure()  \nfig = SDW.plot()\nfig = SDA.plot()","06811d94":"This is much improved. It looks like we have achieved some stationarity.  Let's look at how this transformation affected the weekly structure. I will show the corrected sales and the original sales and zoom in at the first few days and the last few days.","968dfc62":"This did not quite work as well as I expected. The brush is still wide but it seemed it affected the weekly pattern. Of course not all years start on the same day of the week.\n\nI need to refine the model here. Perhaps I need to remove the day of week pattern before I can put in the month of year pattern.\n\nSo let's backtrack, we will first remove the weekly pattern and then compute the seasonal pattern.\n\n## 2.2.2 Correcting for Day of Week\n\nWe will calculate a multiplicative factor for each DoW. This factor applies to the average of the week depending on what day we are looking at.\n","e754badd":"# 1. High Level Look at the data\nFirst Look at the data:","7e62fa6d":"\nThis could be an effect of the leap year in 2016. Since we have done our computation based on DoY.... and not on month, the February 29th 2016 date, is sometimes added with march data.\nIn other words,  day 91 of 2016 is really March 31st.... so adding it to April distorted the April numbers.\nThe relevant variables should have been month and Day of Month.... not  Day of Year. ","0ea71815":"So DayInMonth Factor was pretty good at removing variations from month to month.\n\nThis is really surprising. The graph above shows daily sales. And somehow the sale amount each day of February depended on the number of days in February. Each day had sales higher than expected because it was a short month!\n\nOur approach was to multiply all februaries by the same factor.... we do see one more glitch in the graph for February 2016, which was 29 days long instead of 28.  A next iteration would be to fix that. \n\nThe only explanation I could find was that this is a synthetic data set, and the authors calculated monthly totals and distributed it within the month according to some rule.\n\nLet's fix February 2016!\n\n\n","f4e23345":"The Blue dots, showing predicted sales seem to do a relatively good job tracking total sales over the training set.What does the residual look like?\n","19deeb2b":"# 3 . Seasonal Analysis based on packages.\n\nIn this section, we will use a sklearn to do a seasonal analysis. ","746fe2e9":"The broad brush that we have observed in the data  is indeed due to  a weekly pattern. This answers the question we asked in cell 18 when we first visualize the data. This conclusion and the above drawings  suggest that:\n1. When we try to fit a model, perhaps we want to include a feature representing the day of week\n2. Maybe we want to try to remove the day-of-week variation from the data before we do seasonal analysis.\n3. Maybe we want to work on weekly totals rather than daily sales. (But doing that might cause issues at month boundaries or year boundaries which do not all start on a monday).\n4. Holidays have a mixed impact: They depress sales on Wednesday and they improve sales on friday. Their impact on other days is not as clear.\n\nIt looks like option 2 above might be a good choice, we will come back to it later.","a99e0694":"\n# 2. Structure in the dataset: looking at Total Sales\n\nOur approach here will be to look at the data, identify some structure, fit data to the structure and compute the residual.\nThen we can iterate on the residual.\n\n## 2.1 View of Total Sales.\n\nSince all the sales graph for item, store have a similar shape, we would suspect that if we look to total sales for each day, we would see the same structure.\n\n","cca5de7f":"\n\n## 2.3 Trend analysis on total sales\n\nIn this section, I would like to remove the linear growth trend and look at growth year over year.\n\nTo calculate the linear trend, I will average sales each day and fit a straight line through it. ( a variant could be to have different trends for each store\/item.... I could test to see which is best.\n\n\n\n","6f55b312":"Let's correct the growth by substracting the trend:","8af84e96":"## 1. Data input and high level description ","aab12778":"We have learned that there are a weekly and a yearly seasonal pattern. Let's try to apply both for seasonal analysis.","7393892d":"There is only one zero entry in the whole training set. Something to keep in mind but probably not statistically significant.","b4a6c82a":"# Data Analysis Approach\n\nThis notebook describes the main steps in my analysis of the \"Store Item Demand Forecasting Challenge\".  The task is about forecasting sales of 50 different items in a list of 10 stores over a three month period.  My objective was to get hands-on with a data forecast challenge and understand the structure of the data. I wanted to use this challenge as an opportunity to improve my skill with plotting and analyzing data  in Python.\n\nThe main phases of my analysis were as follows:\n\n1.  High Level look at the data\n\n2. Look for structure trends and seasonality in the data (I chose to do it myself rather than use available tools)\n\n3. Compare my effort with what a seasonal decomposition would have done out of the box.\n\nThe main Findings were:\n    *  This data is clean. It feels like synthetic data.\n    *  The sales pattern is similar for every item and store.\n    *  There is  year over year growth in sales volume for which I did not find a clear pattern.\n    *  There is a repeated seasonal pattern every year.\n    *  There is a repeated weekly pattern.\n    *  Within a month, variations are mostly linked to Day-Of-Week.\n    * The level of sales for a specific day is linked to number of days in a month.\n\nThe material is organized as a diary. It reads sequentially. It sometimes runs down a path and backtracks when the data shows something that I had not foreseen.\n\nIt concludes with an analysis of the training that reduces it to a random noise (though not quite white).","6dfad935":"# 4. Next Steps\n\nI have learned a lot about the dataset and total sales. \nMy next steps will be to take these learnings and use them to make predictions at the store and item level.\nThe list of new features (columns) we have added to the data set should be useful for this purpose.\n\n","98653953":"This last graph  is much more clear than our previous attempt at line 42.\nWe clearly see pretty stable total sales prediction throughout each month.\nBut a glitch immediately jumps at us: starting in the third month, there is an outlier at the beginning of every month.\nIt does not show in January and in February, but it shows in march.\n\nLet's zoom in and look at one month. Say April: \n\n\n","4689bedc":"> ## 2.2 Detailed structure analysis of Total Sales.\nWe see a clear repeated structure for seven points.... A  weekly \"seasonality\". Let's dig a little deeper in that. We wil add a few feature and look at weekly trends","52731320":"The above seems very regular. Another question I wanted to validate was weather there was an entry for each item for each store for each day. Perhaps simple math can tell us that:\n\nIf  $n_s$ is number of Stores, $n_i$ is number of items and $n_d$ is number of days, we can calculate the expected number of entries $n_r$ in the table:\n\n   \\begin{align} n_r &= n_s * n_i * n_d        \\\\\n                     &= 10 * 50 * (365 *5 +1 ) \\\\     \n                     &= 913,000 \\end{align} \n   \nNote the +1 for 2016 which has 366 days!   \nThis is exactly the number of rows we have read from the training file which pretty much confirms that our data table has exactly one entry per item per store per day.\n\n\n**Our Training Data is Clean!** No missing value and an entry for each row. Just as was announced in the description.\n\nNote that some rows in the data have a zero value. Let's have a quick look at how many they are and at the story, items and dates involved.\n","375adf62":"Now let's apply the DoW factor to the data set before we compute a seasonal pattern.","2ca52cff":"It seems that the weekly structure was pretty much preserved by the correction.  So let's keep working on it.\n\n### 2.2.1 Seasonal Pattern\n\nLet's sum sales over the five years to get a seasonal pattern.","a1af5aa8":"## 2.4 Validation\n\nIn this section we will take another look at the our findings, compute the residual and try to see if we can learn something additional about the data.\n\nSo far we have three factors:\n* A growth trend which we correct with a P[i]\/P[0] factor\n* A weekly pattern which we correct with a multiplicative DoWFactor\n* A monthly pattern which we correct with a multiplicative MonthFactor\n\nLet's apply all three to get a prediction over the training set and look at residual!","a61325f8":"This is much improved and it shows we are making progress in analyzing the data: \n\nIt looks like the weekly factor helped improve our view of total sales.\n1.  We clearly see the seasonal pattern \n2.  we clearly see the differences every month.\n3.  We see that year 2014 seems to have very high values.\n\nWe can now make a second attempt at looking at a seasonal pattern.\n\n## 2.2.3 Seasonal Pattern Revisited.\n\nLet's try to average up all five years and compute the seasonal pattern\n\n","f862aa9b":"This is interesting. We get two Surprises.\n\n***Surprise 1:\nWe see some additional structure that we did not expect. It looks like there is a monthly pattern.\nEvery other month (except month 7 and 8)  are alternatingly high and low. And month 2 is pretty low. \n![](http:\/\/)***The immediate thought is that this correlates to the number of days in a month !!!!\n\nI can't think of a rational reason for this, but we will now adjust the data for the number of days in a month and look at the residual again.\n\n***Surprise 2:\nthe difference grows over the year.... it looks like we were too ambitious thinking growth would be a day to day trend. It looks like growth might really just happens at the end of the year, and we are looking for a single factor every year.\n\n","e8984b22":"**** It looks like we have reduced our training data set to white noise and we are in a good position to use the learnings to make a predictive model.\n\nIn the next section we will try to see if we could have gotten the same results from a canned seasonal analysis.\n","e42922b4":"This did NOT quite work as the fifth year seems to have a wider range. Let's try a multiplicative correction instead.\n","938f37b1":"We have a clear pattern. Note the lonely point for Feb 29th 2016. \nlet's compute the associated factor for each month.","edd578b5":"500 very similar plots up there. I gather the following\n\n* All store and items follow a very similar pattern: Year over year growth (looks linear) and a seasonal pattern.\n* There are variation in volume between different stores.\n* There are significant variations in volume between differen items.\n\nSome additional questions:\n\n* Are the seasonality and the trend related to the store and item... or can I just derive them from the full dataset? I wil tackle this by working on total values first (over all items and stores) to build a first baseline.\n* The plots have a pretty broad brush... is that randomness or is that related to a shorter period in the signal (e.g. DOW)","78d1397f":"This graph is strange... We had already mentionned the broad brush.  It looks like there might be a structure to it... Let's zoom in.","2140c681":"# Let's visualize the data","e766d2ec":"This is pretty good... but not quite as effective as our detailed analysis above. "}}