{"cell_type":{"943e4c44":"code","1f6705cd":"code","18e61ad2":"code","ae62915c":"code","79f65032":"code","15456d41":"code","19fe0231":"code","c7af4048":"code","2ca798ea":"code","afaa6456":"code","757a765a":"code","ff2788e1":"code","6fb0ad5e":"code","f17eb632":"code","523bbeb9":"code","67d312d9":"code","e2dbf6e7":"code","a4f88bf1":"code","d9fbbb8d":"code","f5e848cc":"code","576fb571":"code","381df74a":"code","dc6dc55e":"code","92e5f18e":"code","c0310143":"code","07bb4641":"code","2e0cc38c":"code","bbab541c":"code","e9f5d2bc":"code","bb36b9ab":"code","a91d5cfb":"code","c9c934c5":"code","5d4d38a0":"code","0ede1f07":"code","1065f7e4":"code","d30da5b9":"code","57201739":"code","2ff44aa7":"code","85c72895":"code","ef1b8a7b":"code","ce9dcf30":"code","050205d7":"code","bf9e59d9":"code","3ad09c1a":"code","89db12a6":"code","81edba36":"code","db55107f":"markdown","f3789afd":"markdown","03453faf":"markdown","62ccb814":"markdown","4a4fd268":"markdown","de81b475":"markdown","fcadc3bc":"markdown","8049d614":"markdown","a6933df1":"markdown","3ebd2866":"markdown","cf9cd4dd":"markdown"},"source":{"943e4c44":"!pip install pyunpack\n!pip install patool\n!pip install py7zr\n!pip install sounddevice\n!pip install noisereduce\n!pip install librosa\n! pip install python_speech_features\n! pip install tensorflow==2.4\n! pip install malaya_speech\n! pip install webrtcvad","1f6705cd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom py7zr import unpack_7zarchive\nimport shutil\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport librosa\nimport IPython.display as ipd\nfrom scipy.io import wavfile\n\nimport noisereduce as nr\nimport tensorflow \nfrom malaya_speech import Pipeline\n\nimport malaya_speech\nimport os\n\nfrom python_speech_features import mfcc\n\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sn","18e61ad2":"shutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\nshutil.unpack_archive('\/kaggle\/input\/tensorflow-speech-recognition-challenge\/train.7z', '\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/')","ae62915c":"# from pyunpack import Archive\n# import shutil\n# if not os.path.exists('\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/'):\n#     os.makedirs('\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/')\n# Archive('\/kaggle\/input\/tensorflow-speech-recognition-challenge\/train.7z').extractall('\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/')\n\n#for dirname, _, filenames in os.walk('\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/train\/audio'):\n #   for filename in filename[:5]:\n  #      print(os.path.join(dirname, filename))","79f65032":"train_audio_path = '\/kaggle\/working\/tensorflow-speech-recognition-challenge\/train\/train\/audio\/'","15456d41":"#!apt-get install -y p7zip-full\n#!7z x ..\/input\/tensorflow-speech-recognition-challenge\/train.7z","19fe0231":"samples, sample_rate = librosa.load(train_audio_path+'on\/5a3712c9_nohash_1.wav', sr = 16000)\nfig = plt.figure(figsize=(14, 8))\nax1 = fig.add_subplot(211)\nax1.set_title('Raw wave of ' + '..\/input\/train\/audio\/on\/0a7c2a8d_nohash_0.wav')\nax1.set_xlabel('time')\nax1.set_ylabel('Amplitude')\nax1.plot(np.linspace(0, sample_rate\/len(samples), sample_rate), samples)","c7af4048":"ipd.Audio(samples, rate=sample_rate)","2ca798ea":"print(sample_rate)\nsig1=samples\nfs=sample_rate\nsr=fs","afaa6456":"time = np.linspace(0, len(sig1 - 1) \/ fs, len(sig1 - 1))\nreduced_noise1 = nr.reduce_noise(y=sig1, sr=fs,stationary=True)\nplt.plot(time, reduced_noise1)  # plot in seconds\n#reduced_noise2 = nr.reduce_noise(y=sig2, sr=fs,stationary=True)\n#plt.plot(time, reduced_noise2)  # plot in seconds\n#plt.title(\"Voice Signal\")\nplt.xlabel(\"Time [seconds]\")\nplt.ylabel(\"Voice amplitude\")\nplt.show()","757a765a":"ipd.Audio(reduced_noise1, rate=sample_rate)","ff2788e1":"#Silence Removal\nvad = malaya_speech.vad.webrtc()\ny=reduced_noise1\ny_= malaya_speech.resample(y, sr, 16000)\ny_ = malaya_speech.astype.float_to_int(y_)\nframes = malaya_speech.generator.frames(y, 30, sr)\nframes_ = list(malaya_speech.generator.frames(y_, 30, 16000, append_ending_trail = False))\nframes_webrtc = [(frames[no], vad(frame)) for no, frame in enumerate(frames_)]\ny_ = malaya_speech.combine.without_silent(frames_webrtc)\ny_","6fb0ad5e":"ipd.Audio(y_, rate = sr )","f17eb632":"zero = np.zeros((1*sr-y_.shape[0]))\nsignal = np.concatenate((y_,zero))\nsignal.shape\ntime = np.linspace(0, len(signal - 1) \/ fs, len(signal - 1))","523bbeb9":"plt.plot(time,signal)","67d312d9":"labels=os.listdir(train_audio_path)","e2dbf6e7":"#find count of each label and plot bar graph\nno_of_recordings=[]\nfor label in labels:\n    waves = [f for f in os.listdir(train_audio_path + '\/'+ label) if f.endswith('.wav')]\n    no_of_recordings.append(len(waves))\n    \n#plot\nplt.figure(figsize=(30,5))\nindex = np.arange(len(labels))\nplt.bar(index, no_of_recordings)\nplt.xlabel('Commands', fontsize=12)\nplt.ylabel('No of recordings', fontsize=12)\nplt.xticks(index, labels, fontsize=15, rotation=60)\nplt.title('No. of recordings for each command')\nplt.show()","a4f88bf1":"labels=[ \"on\",\"off\"]","d9fbbb8d":"sr=16000\nvad = malaya_speech.vad.webrtc()\nall_wave = []\nall_label = []\nfor label in labels:\n    print(label)\n    waves = [f for f in os.listdir(train_audio_path + '\/'+ label) if f.endswith('.wav')]\n    for wav in waves:\n        samples, sample_rate = librosa.load(train_audio_path + '\/' + label + '\/' + wav, sr = 16000)\n        samples = nr.reduce_noise(y=samples, sr=sr,stationary=True)\n        y_= malaya_speech.resample(samples, sr, 16000)\n        y_ = malaya_speech.astype.float_to_int(y_)\n        frames = malaya_speech.generator.frames(samples, 30, sr)\n        frames_ = list(malaya_speech.generator.frames(y_, 30, 16000, append_ending_trail = False))\n        frames_webrtc = [(frames[no], vad(frame)) for no, frame in enumerate(frames_)]\n        y_ = malaya_speech.combine.without_silent(frames_webrtc)\n        zero = np.zeros(((1*sr+4000)-y_.shape[0]))\n        signal = np.concatenate((y_,zero))\n        all_wave.append(signal)\n        all_label.append(label)","f5e848cc":"print(np.array(all_wave).shape)\nprint(np.array(all_label).shape)\ntime = np.linspace(0, len(signal - 1) \/ fs, len(signal - 1))\nplt.plot(time,np.array(all_wave)[2000,:])\nprint(np.array(all_label)[2000])\nipd.Audio(np.array(all_wave)[2000,:], rate = sr )","576fb571":"all_mfcc=[]\nfor wave in all_wave:\n    i=0\n    mfcc_feat = mfcc(wave , fs, winlen=256\/fs, winstep=256\/(2*fs), numcep=13, nfilt=26, nfft=256,\n                 lowfreq=0, highfreq=fs\/2, preemph=0.97, ceplifter=22, appendEnergy=True, winfunc=np.hamming)\n    mfcc_feat= np.transpose(mfcc_feat)\n    all_mfcc.append(mfcc_feat)\n    ","381df74a":"print(np.array(all_mfcc).shape)\nprint(np.array(all_label).shape)\nd1=np.array(all_mfcc).shape[1]\nd2=np.array(all_mfcc).shape[2]\nd=d1*d2\nprint(d)","dc6dc55e":"op_mfcc=np.array(all_mfcc)\nop_mfcc=op_mfcc.reshape(4724,-1)\nop_mfcc.shape","92e5f18e":"#all_label = all_label.tolist()\n\nle = LabelEncoder()\ny=le.fit_transform(all_label)\nclasses= list(le.classes_)","c0310143":"! pip install --upgrade tensorflow\n! pip install --upgrade tensorflow-gpu\n! pip install keras==2.3.1","07bb4641":"from keras.optimizers import SGD\nfrom keras.constraints import maxnorm\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense,Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint","2e0cc38c":"y=tensorflow.keras.utils.to_categorical(y, num_classes=len(labels), dtype='float32')\ny.shape","bbab541c":"from sklearn.model_selection import train_test_split\nx_tr, x_val, y_tr, y_val= train_test_split(op_mfcc,np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)","e9f5d2bc":"print(x_tr.shape)\nprint(y_tr.shape)\nprint(x_val.shape)\nprint(y_val.shape)","bb36b9ab":"#from keras.models import Sequential\n#from keras.layers import Dense, Dropout, Activation\n\n#Model Architecture\nmodel = Sequential()\nmodel.add(Dense(100, activation='sigmoid', input_shape=(d,), kernel_constraint=maxnorm(3)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax' , kernel_constraint=maxnorm(3)))","a91d5cfb":"tensorflow.keras.utils.plot_model(model, 'model.png',show_shapes=True)","c9c934c5":"model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","5d4d38a0":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001) \nmc = ModelCheckpoint('best_model.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')","0ede1f07":"#x_tr=np.expand_dims(x_tr,axis=0)\n#y_tr=np.expand_dims(y_tr,axis=0)","1065f7e4":"history=model.fit(x_tr, y_tr,validation_data=(x_val,y_val), epochs=120, batch_size=32)","d30da5b9":"train_score = model.evaluate(x_tr, y_tr, batch_size=12)\nprint(train_score)\n\nprint('----------------Training Complete-----------------')\n\ntest_score = model.evaluate(x_val, y_val, batch_size = 12)\nprint(test_score)","57201739":"history.history.keys()","2ff44aa7":"from matplotlib import pyplot\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","85c72895":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","ef1b8a7b":"y_predict=model.predict(x_val)\nconf_mat=tensorflow.math.confusion_matrix(np.argmax(y_val,axis=1) , np.argmax(y_predict,axis=1))","ce9dcf30":"df_cm = pd.DataFrame(np.array(conf_mat), index = [i for i in classes],\n                  columns = [i for i in classes])\nplt.figure(figsize = (13,7))\nax = sn.heatmap(df_cm, annot=True)\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"True Class\"     , fontsize=20)\nplt.xlabel(\"Predicted Class\", fontsize=20)\nplt.show()","050205d7":"x_val[1].shape\nmodel.predict(x_val[1].reshape((1,d)))","bf9e59d9":"def predict(audio):\n    print(samples.shape)\n    prob=model.predict(audio)\n    index=np.argmax(prob[0])\n    return classes[index]","3ad09c1a":"import random\nindex=random.randint(0,len(x_val)-1)\nprint(index)\nsamples=x_val[index]\nprint(\"Audio:\",classes[np.argmax(y_val[index])])\n#ipd.Audio(np.array(all_wave)[index,:], rate=16000)","89db12a6":"print(\"Text:\",predict(samples.reshape(1,d)))","81edba36":"from keras.models import load_model\nmodel.save(\"MME.h5\")","db55107f":"### **Model Architecture**","f3789afd":"**Duration of recordings**\n\nWhat\u2019s next? A look at the distribution of the duration of recordings:","03453faf":"Prediction time! Make predictions on the validation data:","62ccb814":"## **Model based on ANN** ","4a4fd268":"**Preprocessing the audio waves**\n\nIn the data exploration part earlier, we have seen that the duration of a few recordings is less than 1 second and the sampling rate is too high. So, let us read the audio waves and use the below-preprocessing steps to deal with this.\n\nHere are the two steps we\u2019ll follow:\n\n* Noise Reduction\n* Silence Removal\n\nLet us define these preprocessing steps in the below code snippet:","de81b475":"**Import the libraries**\n\nFirst, import all the necessary libraries into our notebook. LibROSA and SciPy are the Python libraries used for processing audio signals.","fcadc3bc":"# <center> Implementing the Speech-to-Text Model in Python\n**Understanding the Problem Statement for our Speech-to-Text Project**\n\nLet\u2019s understand the problem statement of our project before we move into the implementation part.\n\nWe might be on the verge of having too many screens around us. It seems like every day, new versions of common objects are \u201cre-invented\u201d with built-in wifi and bright touchscreens. A promising antidote to our screen addiction is voice interfaces. \n\n__You can download the dataset from__ [here](https:\/\/www.kaggle.com\/c\/tensorflow-speech-recognition-challenge).\n    \nTensorFlow recently released the Speech Commands Datasets. It includes 65,000 one-second long utterances of 30 short words, by thousands of different people. We\u2019ll build a speech recognition system that understands simple spoken commands. <br>    ","8049d614":"**Sampling rate **\n\nLet us now look at the sampling rate of the audio signals","a6933df1":"**Accessing each file in data**","3ebd2866":"**Data Exploration and Visualization**\n\nData Exploration and Visualization helps us to understand the data as well as pre-processing steps in a better way. \n\n**Visualization of Audio signal in time series domain**\n\nNow, we\u2019ll visualize the audio signal in the time series domain:","cf9cd4dd":"Define the function that predicts text for the given audio:"}}