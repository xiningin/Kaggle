{"cell_type":{"1a866c95":"code","70fec68c":"code","1d91484f":"code","65af056c":"code","cfce0504":"code","0fe1c9c0":"code","b6acf8c5":"code","5cec294d":"code","0f4a5f11":"code","d55d5108":"code","14230946":"code","52fa1ee2":"code","f9172822":"code","8a54f866":"code","bf3ed623":"code","2a1e4753":"code","9e85863f":"code","f88a5569":"code","ae72087f":"code","083a172b":"code","f5760374":"code","b6db94b9":"code","180fb682":"code","d47c301e":"code","84d00937":"code","4af06223":"code","03731a9e":"code","93bfaf4e":"code","6d57f6fa":"code","33864afe":"code","d0d70a29":"code","d13f6e97":"code","ac127702":"markdown","812778fd":"markdown","554c3fd9":"markdown","0c999224":"markdown","5b364808":"markdown","387c8466":"markdown","dee66f24":"markdown","18820916":"markdown","75e26e34":"markdown","3f06696c":"markdown","3b0d1937":"markdown","6e5f9e4f":"markdown","356c8caf":"markdown","94065689":"markdown","266230a1":"markdown","71a82dd7":"markdown","41755211":"markdown","17c01c04":"markdown","95831376":"markdown"},"source":{"1a866c95":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')","70fec68c":"data = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv', encoding='latin-1')","1d91484f":"data.head()","65af056c":"data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1, inplace=True)\ndata.rename(columns={'v1':'Label', 'v2':'Message',}, inplace=True)","cfce0504":"data.head()","0fe1c9c0":"sns.countplot('Label', data = data)","b6acf8c5":"data.groupby('Label').describe()","5cec294d":"X = data['Message']\ny = data['Label']","0f4a5f11":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)","d55d5108":"from sklearn.feature_extraction.text import TfidfVectorizer","14230946":"tfidf_vect = TfidfVectorizer()","52fa1ee2":"X_train = tfidf_vect.fit_transform(X_train)\nX_test = tfidf_vect.transform(X_test)","f9172822":"len(tfidf_vect.get_feature_names())","8a54f866":"# tfidf_vect.get_feature_names()","bf3ed623":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n\nlr = LogisticRegression()\nnb = MultinomialNB()\nknc = KNeighborsClassifier()\nsvc = SVC(gamma = 'auto')\ndtc = DecisionTreeClassifier()\nrfc = RandomForestClassifier(n_estimators=100)\ngbc = GradientBoostingClassifier()\nabc = AdaBoostClassifier()\n\n\n\nmodels = {'Logistic Regression':lr, 'Naive Bayes classifier':nb, 'k-nearest neighbors':knc, \n          'Support Vector Machine':svc, 'Decision Tree Classifier':dtc, \n          'Random Forest Classifier':rfc, 'Gradient Boosting Classifier':gbc, 'AdaBoost Classifier':abc}","2a1e4753":"def eval_model(model):\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    test_accuracy = accuracy_score(y_test, y_pred)\n    conf_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred), columns=['ham', 'spam'], index=['ham','spam'])\n    \n    return test_accuracy, conf_matrix","9e85863f":"test_accuracies = []\nconfusion_matrices = []\nfor name, model in models.items():\n    test_acc, conf_matrix = eval_model(model) \n    test_accuracies.append(test_acc)\n    confusion_matrices.append(conf_matrix)\n    print(f'{name} ---> Test accuracy - {test_acc*100:.2f}%')","f88a5569":"results = pd.DataFrame(test_accuracies, index=list(models.keys()), columns=['test_acc'])\nresults","ae72087f":"plt.figure(figsize=(10, 6))\nsns.barplot(x ='test_acc', y=results.index, data=results)\nplt.xlim(0.85, 1.0)\nplt.title('Performance comparision')\nplt.show()","083a172b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n\ntfidf_vect = TfidfVectorizer(stop_words='english')\n\nX_train = tfidf_vect.fit_transform(X_train)\nX_test = tfidf_vect.transform(X_test)","f5760374":"len(tfidf_vect.get_feature_names())","b6db94b9":"lr = LogisticRegression()\nnb = MultinomialNB()\nknc = KNeighborsClassifier()\nsvc = SVC(gamma = 'auto')\ndtc = DecisionTreeClassifier()\nrfc = RandomForestClassifier(n_estimators=100)\ngbc = GradientBoostingClassifier()\nabc = AdaBoostClassifier()\n\n\n\nmodels = {'Logistic Regression':lr, 'Naive Bayes classifier':nb, 'k-nearest neighbors':knc, \n          'Support Vector Machine':svc, 'Decision Tree Classifier':dtc, \n          'Random Forest Classifier':rfc, 'Gradient Boosting Classifier':gbc, 'AdaBoost Classifier':abc}","180fb682":"test_accuracies_no_stopwords = []\nconfusion_matrices_no_stopwords = []\nfor name, model in models.items():\n    test_acc, conf_matrix = eval_model(model) \n    test_accuracies_no_stopwords.append(test_acc)\n    confusion_matrices_no_stopwords.append(conf_matrix)\n    print(f'{name} ---> Test accuracy - {test_acc*100:.2f}%')","d47c301e":"results['test_acc_without_stopwords'] = pd.Series(test_accuracies_no_stopwords, index=list(models.keys()))\nresults","84d00937":"def plot_confusion_matrices(models, confusion_matrices):\n    fig, axs = plt.subplots(2,4, figsize=(10,5)) \n\n    m = 0\n    for i, ax_r in enumerate(axs):\n        for j, ax in enumerate(ax_r):\n            sns.heatmap(confusion_matrices[m], annot=True, cbar=False, cmap='Blues', fmt='g', ax = ax)\n            ax.set_xlabel('Predicted label')\n            ax.set_ylabel('True label')\n            ax.set_title(f'{list(models.keys())[m]}', fontsize=12, fontweight='bold')\n            m += 1\n\n    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                        wspace=0.35)\n    plt.tight_layout()\n    plt.show()\n    ","4af06223":"plot_confusion_matrices(models, confusion_matrices)","03731a9e":"from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('tfidf_vect', TfidfVectorizer()),\n    ('classifier', RandomForestClassifier(n_estimators=100))\n])","93bfaf4e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","6d57f6fa":"test_acc, conf_matrix = eval_model(pipeline) \n\nprint('Test accuracy - ',test_acc)\nprint('Confusion matrix - \\n', conf_matrix)","33864afe":"print('Classification Report \\n', classification_report(y_test, pipeline.predict(X_test)))","d0d70a29":"messages = ['Thank you for subscribing! You will be notified when you win your 1 Million Dollar prize money! Please call our customer service representative on 0800012345 for further details ',\n          'Hi, hope you are doing well. Please call me as soon as possible!']","d13f6e97":"pipeline.predict(messages)","ac127702":"We can also see the features\/tokens identified by the vectorizer by accessing the get_feature_names method.","812778fd":"As seen from the confusion matrix, 39 'spam' messages are being predicted as 'ham' messages","554c3fd9":"Finally let us build a pipeline to fit and predict on raw text data.","0c999224":"Now let us include the effect of stopwords by passing the 'english' keyword to the stopwords argument to the tfidf vectorizer.","5b364808":"Extracting features from the text data using tfidfvectorizer from sklearn.","387c8466":"We can use the same function to fit and predict with the pipeline","dee66f24":"As we can see from the results there is not much of an improvment after removing stopwords.","18820916":"As we can see most of them are numbers, abbreviations(short cuts). Also notice that we are not restricting the stopwords here as they might play an important role in classifying a message to be 'spam' or 'ham'.  \n\nWe will also see the wether removing stopwords might help with the classification task in the later step.","75e26e34":"Another good thing about pipeline is, we can directly make predictions on new raw text messages.\n\nLets create some text messages and predict if they are 'spam' or 'ham'.","3f06696c":"Selecting the features and labels.","3b0d1937":"Let us import different models and evaluation metrics. ","6e5f9e4f":"Splitting the data oncemore, to make sure the 'eval_model' function uses the right data for fitting and evaluating the model.","356c8caf":"Removing unnamed columns and renaming the columns v1 and v2","94065689":"There are more 'ham' or 'not-spam' messages than 'spam' messages.\n\n","266230a1":"The vectorizer is fit on the training data. Later the training and testing data is transformed using the vectorizer.","71a82dd7":"Looks like SVM and K-nearest neighbors are not a good choice for this task. The performance of all the other models is almost similar in this case.","41755211":"Writing a function to fit the model on training data and make predictions on test data.","17c01c04":"Notice a decrease in number of features of the vectorizer after removing the stopwords from 7206 to 6946.\n\n\nLets initialize the same models again to fit them on the features without stopwords.","95831376":"Splitting the data into training and testing sets before extracting features from the test data and building machine learning models."}}