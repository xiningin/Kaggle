{"cell_type":{"df4c735b":"code","26ceae59":"code","83668eb2":"code","69477e54":"code","e3494f94":"code","c2621006":"code","c1943187":"code","d46d6fdf":"code","c12b9e86":"code","d2845dac":"code","b022c6ff":"code","d1eb7c36":"code","5530df9b":"code","32b23ff6":"code","ec4d7911":"code","01032c35":"markdown","27ab2d87":"markdown","71e0fa26":"markdown"},"source":{"df4c735b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26ceae59":"import os\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom skimage.io import imread\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n%matplotlib inline\n\nimage = imread(r'..\/input\/figaro\/Figaro1k\/GT\/Training\/Frame00001-gt.pbm')\n\nplt.imshow(image)\n","83668eb2":"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, Dropout, Input, concatenate\nfrom tensorflow.keras.models import Model","69477e54":"def U_NetModel(input_shape):\n\n    X_input = Input(input_shape)\n\n    ######## Down-Sampling Part\n    d1 = Conv2D(filters = 16, kernel_size = (3,3), \n                name = \"downsamping-1a\", padding = \"same\", \n                kernel_initializer= \"he_normal\", activation = \"relu\")(X_input)\n    d1 = Dropout(0.1)(d1)\n    d1 = Conv2D(filters = 16, kernel_size = (3,3), \n                name = \"downsampling-1b\", padding = \"same\", \n                kernel_initializer= \"he_normal\", activation = \"relu\")(d1)\n    p1 = MaxPooling2D(pool_size= (2,2))(d1)\n\n\n\n\n    d2 = Conv2D(filters = 32, kernel_size = (3,3), \n                name = \"downsamping-2a\", padding = \"same\", \n                kernel_initializer= \"he_normal\", activation = \"relu\")(p1)\n    d2 = Dropout(0.1)(d2)\n    d2 = Conv2D(filters = 32, kernel_size = (3,3), \n                name = \"downsampling-2b\", padding = \"same\", \n                kernel_initializer= \"he_normal\", activation = \"relu\")(d2)\n    p2 = MaxPooling2D(pool_size= (2,2))(d2)\n\n\n\n\n    d3 = Conv2D(filters = 64, kernel_size = (3,3), \n                name = \"downsamping-3a\", padding = \"same\", \n                kernel_initializer= \"he_normal\", activation = \"relu\")(p2)\n    d3 = Dropout(0.1)(d3)\n    d3 = Conv2D(filters = 64, kernel_size = (3,3), \n                name = \"downsampling-3b\", padding = \"same\", \n                kernel_initializer= \"he_normal\", activation = \"relu\")(d3)\n    p3 = MaxPooling2D(pool_size= (2,2))(d3)\n\n\n\n\n\n    d4 = Conv2D(filters = 128, kernel_size = (3,3), \n                name = \"downsamping-4a\", padding = \"same\", \n                kernel_initializer= \"he_normal\", activation = \"relu\")(p3)\n    d4 = Dropout(0.1)(d4)\n    d4 = Conv2D(filters = 128, kernel_size = (3,3), \n                name = \"downsampling-4b\", padding = \"same\", \n                kernel_initializer= \"he_normal\", activation = \"relu\")(d4)\n    p4 = MaxPooling2D(pool_size= (2,2))(d4)\n\n\n\n\n\n\n    d5 = Conv2D(filters = 256, kernel_size = (3,3), \n                name = \"downsamping-5a\", padding = \"same\", \n                kernel_initializer= \"he_normal\", activation = \"relu\")(p4)\n    d5 = Dropout(0.1)(d5)\n    d5 = Conv2D(filters = 256, kernel_size = (3,3), \n                name = \"downsampling-5b\", padding = \"same\", \n                kernel_initializer= \"he_normal\", activation = \"relu\")(d5)\n    d5 = Dropout(0.1)(d5)\n    d5 = Conv2D(filters = 256, kernel_size = (3,3), \n                name = \"downsampling-5c\", padding = \"same\", \n                kernel_initializer= \"he_normal\", activation = \"relu\")(d5)\n    \n\n\n    u6 = Conv2DTranspose(filters = 128, kernel_size = (2,2), \n                         strides = (2,2), padding = \"same\",\n                         name = \"upsampling-6a\")(d5)\n    u6 = concatenate([u6, d4])\n    c6 = Conv2D(filters = 128, kernel_size = (2,2),\n                activation = \"relu\", padding = 'same',\n                kernel_initializer = 'he_normal', name = \"upsampling-6b\")(u6)\n    c6 = Dropout(0.2)(c6)\n    c6 = Conv2D(filters = 128, kernel_size = (2,2),\n                activation = \"relu\", padding = 'same',\n                kernel_initializer = 'he_normal', name = \"upsampling-6c\")(c6)\n\n    \n    \n    \n    u7 = Conv2DTranspose(filters = 64, kernel_size = (2,2), \n                         strides = (2,2), padding = \"same\",\n                         name = \"upsampling-7a\")(c6)\n    u7 = concatenate([u7, d3])\n    c7 = Conv2D(filters = 64, kernel_size = (2,2),\n                activation = \"relu\", padding = 'same',\n                kernel_initializer = 'he_normal', name = \"upsampling-7b\")(u7)\n    c7 = Dropout(0.2)(c7)\n    c7 = Conv2D(filters = 64, kernel_size = (2,2),\n                activation = \"relu\", padding = 'same',\n                kernel_initializer = 'he_normal', name = \"upsampling-7c\")(c7)\n\n    \n\n    \n    \n    u8 = Conv2DTranspose(filters = 32, kernel_size = (2,2), \n                         strides = (2,2), padding = \"same\",\n                         name = \"upsampling-8a\")(c7)\n    u8 = concatenate([u8, d2])\n    c8 = Conv2D(filters = 32, kernel_size = (2,2),\n                activation = \"relu\", padding = 'same',\n                kernel_initializer = 'he_normal', name = \"upsampling-8b\")(u8)\n    c8 = Dropout(0.2)(c8)\n    c8 = Conv2D(filters = 32, kernel_size = (2,2),\n                activation = \"relu\", padding = 'same',\n                kernel_initializer = 'he_normal', name = \"upsampling-8c\")(c8)\n\n    \n\n    \n    \n    u9 = Conv2DTranspose(filters = 16, kernel_size = (2,2), \n                         strides = (2,2), padding = \"same\",\n                         name = \"upsampling-9a\")(c8)\n    u9 = concatenate([u9, d1])\n    c9 = Conv2D(filters = 16, kernel_size = (2,2),\n                activation = \"relu\", padding = 'same',\n                kernel_initializer = 'he_normal', name = \"upsampling-9b\")(u9)\n    c9 = Dropout(0.2)(c9)\n    c9 = Conv2D(filters = 16, kernel_size = (2,2),\n                activation = \"relu\", padding = 'same',\n                kernel_initializer = 'he_normal', name = \"upsampling-9c\")(c9)\n\n\n\n    output = Conv2D(filters = 1, kernel_size = (1,1), name = \"output\", activation = \"sigmoid\")(c9)\n\n    model = Model(inputs = [X_input], outputs = [output], name = \"U_NET_MODEL\")\n    \n    return model","e3494f94":"# Free up RAM in case the model definition cells were run multiple times\ntf.keras.backend.clear_session()\n\n\nmodel = U_NetModel((256,256,3))\nmodel.summary()","c2621006":"img_size = (256, 256)\nnum_classes = 3\nbatch_size = 32\n\nclass HairData(tf.keras.utils.Sequence):\n    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n\n    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.input_img_paths = input_img_paths\n        self.target_img_paths = target_img_paths\n\n    def __len__(self):\n        return len(self.target_img_paths) \/\/ self.batch_size\n\n    def __getitem__(self, idx):\n        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n        i = idx * self.batch_size\n        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n        for j, path in enumerate(batch_input_img_paths):\n            img = mpimg.imread(path)\n            img = cv.resize(img, self.img_size)\n            img = img \/ 255.\n            x[j] = img\n        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n        for j, path in enumerate(batch_target_img_paths):\n            img = cv.imread(path, cv.IMREAD_GRAYSCALE)\n            img = cv.resize(img, self.img_size)\n            img = img \/ 255.\n            img = np.expand_dims(img, -1)\n            y[j] = img\n\n        return x, y","c1943187":"home_directory = '..\/input\/figaro\/Figaro1k'\n\ntrain_frame_path = os.path.join(home_directory, \"Original\",'Training')\ntrain_label_path = os.path.join(home_directory,'GT','Training')\nval_frame_path = os.path.join(home_directory,\"Original\",'Testing')\nval_label_path = os.path.join(home_directory,'GT','Testing')\n\ntrain_frame_name = sorted(\n    [\n        os.path.join(train_frame_path, fname)\n        for fname in os.listdir(train_frame_path)\n    ]\n)\n\n\ntrain_label_name = sorted(\n    [\n        os.path.join(train_label_path, fname)\n        for fname in os.listdir(train_label_path)\n    ]\n)\n\nval_frame_name = sorted(\n    [\n        os.path.join(val_frame_path, fname)\n        for fname in os.listdir(val_frame_path)\n    ]\n)\n\n\nval_label_name = sorted(\n    [\n        os.path.join(val_label_path, fname)\n        for fname in os.listdir(val_label_path)\n    ]\n)\n\n\ntrain_gen = HairData(batch_size, img_size,\n                     train_frame_name,\n                     train_label_name)\n\nval_gen = HairData(batch_size, img_size,\n                   val_frame_name,\n                   val_label_name)","d46d6fdf":"img = cv.imread(train_label_name[0], cv.IMREAD_GRAYSCALE)\nimg = cv.resize(img, (256, 256))\n\n\nimport matplotlib.pyplot as plt\nprint(type(img))\nprint(img.shape)\nimg = np.expand_dims(img, -1)\nprint(img.shape)\n\nplt.imshow(img, cmap = \"gray\")","c12b9e86":"import keras.backend as K\ndef castF(x):\n    return K.cast(x, K.floatx())\n\ndef castB(x):\n    return K.cast(x, bool)\n\ndef iou_loss_core(true,pred):  #this can be used as a loss if you make it negative\n    intersection = true * pred\n    notTrue = 1 - true\n    union = true + (notTrue * pred)\n\n    return (K.sum(intersection, axis=-1) + K.epsilon()) \/ (K.sum(union, axis=-1) + K.epsilon())\n\ndef competitionMetric2(true, pred): #any shape can go - can't be a loss function\n\n    tresholds = [0.5 + (i*.05)  for i in range(10)]\n\n    #flattened images (batch, pixels)\n    true = K.batch_flatten(true)\n    pred = K.batch_flatten(pred)\n    pred = castF(K.greater(pred, 0.5))\n\n    #total white pixels - (batch,)\n    trueSum = K.sum(true, axis=-1)\n    predSum = K.sum(pred, axis=-1)\n\n    #has mask or not per image - (batch,)\n    true1 = castF(K.greater(trueSum, 1))    \n    pred1 = castF(K.greater(predSum, 1))\n\n    #to get images that have mask in both true and pred\n    truePositiveMask = castB(true1 * pred1)\n\n    #separating only the possible true positives to check iou\n    testTrue = tf.boolean_mask(true, truePositiveMask)\n    testPred = tf.boolean_mask(pred, truePositiveMask)\n\n    #getting iou and threshold comparisons\n    iou = iou_loss_core(testTrue,testPred) \n    truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n\n    #mean of thressholds for true positives and total sum\n    truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n    truePositives = K.sum(truePositives)\n\n    #to get images that don't have mask in both true and pred\n    trueNegatives = (1-true1) * (1 - pred1) # = 1 -true1 - pred1 + true1*pred1\n    trueNegatives = K.sum(trueNegatives) \n\n    return (truePositives + trueNegatives) \/ castF(K.shape(true)[0])","d2845dac":"tf.compat.v1.enable_eager_execution()\nmodel.compile(optimizer= 'adam',\n              loss = 'binary_crossentropy',\n              metrics= ['accuracy'])","b022c6ff":"num_epochs = 45\n\ndef scheduler(epoch ,lr):\n    if epoch > num_epochs * 0.85:\n        lr = 0.0001\n    elif epoch > num_epochs * 0.30:\n        lr = 0.0001\n    else:\n        lr = 0.001\n    print(f\"Learning Rate : {lr}\")\n    return lr\n\nfrom keras.callbacks import LearningRateScheduler \nlearning_rate = LearningRateScheduler(scheduler)   \n\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor =\"val_loss\", \n                                        mode =\"min\", patience = 5, \n                                        restore_best_weights = True)\n\nhistory = model.fit(\n        train_gen, epochs = num_epochs,\n        validation_data= val_gen,\n        callbacks = [learning_rate , earlystopping],\n        verbose = 1)","d1eb7c36":"def predict_segmentation(input_img):\n    fig, ax = plt.subplots(2, figsize  = (10, 6))\n    img_train = mpimg.imread(input_img)\n    img_train = cv.resize(img_train, (256,256))\n    plt.imsave('img-1.jpg', img_train)\n    ax[0].imshow(img_train)\n    img_train = img_train \/ 255.\n\n    img_train = np.expand_dims(img_train, 0)\n    predict = model.predict(img_train, verbose = 1)\n    prt = (predict > 0.4).astype(np.uint8)\n    plt.imsave('mask-1.jpg', np.squeeze(prt), cmap = 'gray')\n    ax[1].imshow(np.squeeze(prt), cmap = 'gray')\n    plt.show()","5530df9b":"predict_segmentation('..\/input\/figaro\/Figaro1k\/Original\/Training\/Frame00001-org.jpg')","32b23ff6":"imgfile = 'img-1.jpg'\nmaskfile = 'mask-1.jpg'\n\nimg = cv.imread(imgfile ,1)\nmask = cv.imread(maskfile , 0)\n\ncontours , hierarchy = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\ncv.drawContours(img, contours, 0, (0,0,255), 1)\n\nimg = img [:,:,::-1] # BGR to RGB\nimg[...,2] = np.where(mask == 255, 255, img[...,2])\nplt.imsave(\"output.jpg\", img)\nplt.imshow(img)\nplt.show()","ec4d7911":"model.save('hair_unet.h5')","01032c35":"#  IMPORTS","27ab2d87":"# Hair Segmentation using UNET MODEL","71e0fa26":"# Data Generation"}}