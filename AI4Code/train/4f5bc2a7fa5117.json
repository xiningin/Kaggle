{"cell_type":{"532c8f34":"code","e2a2778b":"code","39a673fb":"code","545e0536":"code","0f2219e0":"code","4cbea3ac":"code","c4f1992c":"code","3e8f89c6":"code","3cc3d186":"code","ad5a2d16":"code","58eefb27":"code","b216a08b":"code","586697f7":"code","518841d6":"code","8a32bf29":"code","81528e17":"code","e142751a":"code","79623f01":"code","9128af48":"code","0c4e67f1":"code","72de2bf6":"code","eb2bf366":"code","2af1b640":"code","cdf210af":"code","2efeb864":"code","d3c151fb":"code","c16b57f9":"code","5f7eba5c":"code","bd10e1fe":"code","7738c97e":"code","183a2a28":"code","b1e9920f":"code","d0b9f265":"code","a7ca8f3d":"code","fe222a30":"code","9168597e":"code","c19b173a":"code","7544fc2b":"code","c666ca7d":"code","49d6a0fc":"code","dacc53a5":"code","92a2bf36":"code","28e1fab3":"code","9342abf4":"code","2a29993b":"code","7f551acd":"code","94667570":"code","9f8fa541":"code","e47c5a32":"code","52cca523":"code","c7ae8ccc":"code","96237bc1":"code","6373e882":"code","fd62c010":"code","68afb69d":"code","4cf7d77d":"code","c4cadb01":"code","61887ee2":"code","a5133a89":"code","9f4713cd":"code","83b24f5c":"code","5acc1d40":"code","fd35be1a":"code","c4ed9903":"code","c6b29454":"code","1e991c84":"code","fdcf88d6":"code","1dd5881c":"code","9442a3bc":"code","652ea3e0":"code","d5b20de1":"code","a720bf04":"code","62f222fa":"code","b766fbd7":"code","78290d63":"code","e370af73":"markdown","530ba6ef":"markdown","85a41d9c":"markdown","3c9bffac":"markdown","bc17da7d":"markdown","a8b82cf0":"markdown","4aa1c0ef":"markdown","c8c9bf1d":"markdown","6a07f0b3":"markdown","a82114fc":"markdown","144ccd15":"markdown","12a42500":"markdown","1b1a1a14":"markdown","54928fc6":"markdown","20db6e91":"markdown","8b45bc99":"markdown","25f6bd0e":"markdown"},"source":{"532c8f34":"# Installation of required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\n%config InlineBackend.figure_format = 'retina'\n\n# to display all columns and rows:\npd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);\n","e2a2778b":"# Reading the dataset\ndf = pd.read_csv(\"..\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv\", index_col=0)\ndf.columns = map(str.lower, df.columns)","39a673fb":"# The first 5 observation units of the data set were accessed.\ndf.head()","545e0536":"# The size of the data set was examined. It consists of 10000 observation units and 13 variables.\ndf.shape","0f2219e0":"# Feature information\ndf.info()","4cbea3ac":"# Descriptive statistics of the data set accessed.\ndf.describe().T","c4f1992c":"# The average of the age variable was taken according to the dependent variable.\ndf.groupby(\"exited\").agg(\"mean\")","3e8f89c6":"# The average of the age variable according to the gender variable was examined.\ndf.groupby(\"gender\").agg({\"age\": \"mean\"})","3cc3d186":"# The average of the dependent variable according to the gender variable was examined.\ndf.groupby(\"gender\").agg({\"exited\": \"mean\"})","ad5a2d16":"# The average of the dependent variable according to the geography variable was examined.\ndf.groupby(\"geography\").agg({\"exited\": \"mean\"})","58eefb27":"# The frequency of the dependent variable has been reached.\ndf[\"exited\"].value_counts()","b216a08b":"# Access to those who left us through the dependent variable. (exited == 1)\nchurn = df[df[\"exited\"] == 1]","586697f7":"# The first 5 observation units were reached.\nchurn.head()","518841d6":"# Size information has been accessed.\nchurn.shape","8a32bf29":"# Who left most than the gender variable?\nchurn.groupby(\"gender\").agg({\"exited\": \"count\"})","81528e17":"# Which country has left us the most?\nchurn.groupby(\"geography\").agg({\"exited\": \"count\"})","e142751a":"# Access to those who do not leave us on the dependent variable. (exited == 0)\nnon_churn = df[df[\"exited\"] == 0]","79623f01":"# The first 5 observation units were reached.\nnon_churn.head()","9128af48":"# Size information has been accessed.\nnon_churn.shape","0c4e67f1":"# Which country does not leave the most?\nnon_churn.groupby(\"geography\").agg({\"exited\": \"count\"})","72de2bf6":"# Unique observation units were examined. Surname variable will be examined.\ndf.nunique()","eb2bf366":"# What are the most commonly used surnames?\ndf.groupby('surname')['surname'].count().sort_values(ascending=False).head(10)","2af1b640":"# The most commonly used surname was examined and observed to be non-multiplexing.\ndf[df[\"surname\"] == \"Smith\"]","cdf210af":"# They were grouped by gender variable and looked at how many years on average they were clients.\ndf.groupby([\"gender\", \"exited\"]).agg({\"tenure\" : \"mean\"})","2efeb864":"# Min, mean and max values \u200b\u200bof all variables were taken according to the dependent variable.\ndf.groupby(\"exited\").agg([\"min\",\"mean\",\"max\"])","d3c151fb":"# Isactivemember is grouped according to the hascrcard variables and the dependent variable is examined.\ndf.groupby([\"isactivemember\", \"hascrcard\"]).agg({\"exited\" : \"count\"})","c16b57f9":"# Isactivemember is grouped according to hascrcard variables and the balance variable is examined.\ndf.groupby([\"isactivemember\", \"hascrcard\"]).agg({\"balance\" : \"mean\"})","5f7eba5c":"# The balance variable was examined according to the gender variable.\ndf.groupby(\"gender\").agg({\"balance\": \"mean\"})","bd10e1fe":"# The age variable was divided into 5 parts and the age range in which there is the most abandonment was examined.\ndf[\"NewAge\"] = pd.qcut(df['age'], 5)\ndf.groupby(\"NewAge\")[\"exited\"].value_counts()","7738c97e":"# The dependent variable was studied according to the gender variable and age range variable.\ndf.groupby([\"gender\",\"NewAge\" ])[\"exited\"].value_counts()","183a2a28":"# How many people whose balance is 0 and do not leave?\ndf[(df[\"balance\"] == 0) & (df[\"exited\"] == 0)].shape","b1e9920f":"# How many people whose balance is 0 leave?\ndf[(df[\"balance\"] == 0) & (df[\"exited\"] == 1)].shape","d0b9f265":"# Access to the correlation of the data set was provided. What kind of relationship is examined between the variables. \n# If the correlation value is> 0, there is a positive correlation. While the value of one variable increases, the value of the other variable also increases.\n# Correlation = 0 means no correlation.\n# If the correlation is <0, there is a negative correlation. While one variable increases, the other variable decreases. \n# When the correlations are examined, there are 1 variables that act as a positive correlation to the exited dependent variable.\n# This variable is Age. As this increases, the Result variable increases.\ndf.corr()","a7ca8f3d":"# The distribution of the dependent variable in the dataset is plotted as pie and columns graphs.\nf,ax=plt.subplots(1,2,figsize=(18,8))\ndf['exited'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('da\u011f\u0131l\u0131m')\nax[0].set_ylabel('')\nsns.countplot('exited',data=df,ax=ax[1])\nax[1].set_title('exited')\nplt.show()\n","fe222a30":"# Plotted the categorical variables on the basis of the graph of the column according to the dependent variable.\nfig, axarr = plt.subplots(2, 2, figsize=(20, 12))\nsns.countplot(x='geography', hue = 'exited',data = df, ax=axarr[0][0])\nsns.countplot(x='gender', hue = 'exited',data = df, ax=axarr[0][1])\nsns.countplot(x='hascrcard', hue = 'exited',data = df, ax=axarr[1][0])\nsns.countplot(x='isactivemember', hue = 'exited',data = df, ax=axarr[1][1])","9168597e":"# The distribution of the dependent variable according to the gender variable is plotted in the pie chart.\nimport plotly.express as px\nfig = px.pie(df, values =df.groupby(\"gender\")[\"exited\"].value_counts(), names = [\"Female,0\",\"Female 1\",\"Male,0\",\"Male,1\"])\nfig.show()","c19b173a":"# Dependent variable was plotted according to age and geography variable.\nimport plotly.express as px\nfig = px.bar(df,y = \"exited\", x = \"age\" , color = \"geography\")\nfig.show()","7544fc2b":"# Correlation Matrix\nf, ax = plt.subplots(figsize= [20,15])\nsns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap = \"magma\" )\nax.set_title(\"Correlation Matrix\", fontsize=20)\nplt.show()","c666ca7d":"# Boxplot graph for outlier observation analysis\nfig, axarr = plt.subplots(3, 2, figsize=(20, 12))\nsns.boxplot(y='creditscore',x = 'exited', hue = 'exited',data = df, ax=axarr[0][0])\nsns.boxplot(y='age',x = 'exited', hue = 'exited',data = df , ax=axarr[0][1])\nsns.boxplot(y='tenure',x = 'exited', hue = 'exited',data = df, ax=axarr[1][0])\nsns.boxplot(y='balance',x = 'exited', hue = 'exited',data = df, ax=axarr[1][1])\nsns.boxplot(y='numofproducts',x = 'exited', hue = 'exited',data = df, ax=axarr[2][0])\nsns.boxplot(y='estimatedsalary',x = 'exited', hue = 'exited',data = df, ax=axarr[2][1])","49d6a0fc":"# Missing Observation Analysis\ndf.isnull().sum()","dacc53a5":"# Outlier Observation Analysis\nfor feature in df[['creditscore','tenure', 'balance','estimatedsalary']]:\n    \n    Q1 = df[feature].quantile(0.25)\n    Q3 = df[feature].quantile(0.75)\n    IQR = Q3-Q1\n    lower = Q1- 1.5*IQR\n    upper = Q3 + 1.5*IQR\n    \n    if df[(df[feature] > upper)].any(axis=None):\n        print(feature,\"yes\")\n    else:\n        print(feature, \"no\")","92a2bf36":"df[\"NewAGT\"] = df[\"age\"] - df[\"tenure\"]\ndf[\"CreditsScore\"] = pd.qcut(df['creditscore'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ndf[\"AgeScore\"] = pd.qcut(df['age'], 8, labels = [1, 2, 3, 4, 5, 6, 7, 8])\ndf[\"BalanceScore\"] = pd.qcut(df['balance'].rank(method=\"first\"), 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ndf[\"EstSalaryScore\"] = pd.qcut(df['estimatedsalary'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ndf[\"NewEstimatedSalary\"] = df[\"estimatedsalary\"] \/ 12 ","28e1fab3":"df.head()","9342abf4":"df = pd.get_dummies(df, columns =[\"geography\", \"gender\"], drop_first = True)","2a29993b":"df.head()","7f551acd":"df = df.drop([\"customerid\",\"surname\",\"NewAge\"], axis = 1)","94667570":"df.head()","9f8fa541":"cat_df = df[[\"geography_Germany\", \"geography_Spain\", \"gender_Male\", \"hascrcard\",\"isactivemember\"]]","e47c5a32":"cat_df.head()","52cca523":"y = df[\"exited\"]\nX = df.drop([\"exited\",\"geography_Germany\", \"geography_Spain\", \"gender_Male\", \"hascrcard\",\"isactivemember\"], axis = 1)\ncols = X.columns\nindex = X.index","c7ae8ccc":"X.head()    ","96237bc1":"from sklearn.preprocessing import RobustScaler\ntransformer = RobustScaler().fit(X)\nX = transformer.transform(X)\nX = pd.DataFrame(X, columns = cols, index = index)","6373e882":"X = pd.concat([X,cat_df], axis = 1)","fd62c010":"X.head()","68afb69d":"y.head()","4cf7d77d":"print(X.shape, y.shape)","c4cadb01":"# Train-Test Separation\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=12345)\n","61887ee2":"# Because it's an unstable data set, we're going to increase the number of samples.\n# References: https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.combine.SMOTETomek.html\nfrom imblearn.combine import SMOTETomek\n\nsmk = SMOTETomek()\n# Oversample training  data\nX_train, y_train = smk.fit_sample(X_train, y_train)\n\n# Oversample validation data\nX_test, y_test = smk.fit_sample(X_test, y_test)","a5133a89":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","9f4713cd":"models = []\nmodels.append(('LR', LogisticRegression(random_state = 12345)))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier(random_state = 12345)))\nmodels.append(('RF', RandomForestClassifier(random_state = 12345)))\nmodels.append(('SVM', SVC(gamma='auto', random_state = 12345)))\nmodels.append(('XGB', GradientBoostingClassifier(random_state = 12345)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 12345)))\nmodels.append((\"CatBoost\", CatBoostClassifier(random_state = 12345, verbose = False)))\n\n# evaluate each model in turn\nresults = []\nnames = []","83b24f5c":"for name, model in models:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        msg = \"%s: (%f)\" % (name, accuracy)\n        print(msg)","5acc1d40":"models2 = []\nmodels2.append(('CART', DecisionTreeClassifier( random_state = 12345)))\nmodels2.append(('RF', RandomForestClassifier( random_state = 12345)))\nmodels2.append(('XGB', GradientBoostingClassifier( random_state = 12345)))\nmodels2.append((\"LightGBM\", LGBMClassifier( random_state = 12345)))\nmodels2.append((\"CatBoost\", CatBoostClassifier(random_state = 12345, verbose = False)))","fd35be1a":"for name, model in models2:\n        base = model.fit(X_train,y_train)\n        y_pred = base.predict(X_test)\n        acc_score = accuracy_score(y_test, y_pred)\n        feature_imp = pd.Series(base.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\n        sns.barplot(x=feature_imp, y=feature_imp.index)\n        plt.xlabel('De\u011fi\u015fken \u00d6nem Skorlar\u0131')\n        plt.ylabel('De\u011fi\u015fkenler')\n        plt.title(name)\n        plt.show()","c4ed9903":"# Hyperparameters have previously been obtained with the help of GridSearchCV.\nmodels = []\nmodels.append(('XGB', GradientBoostingClassifier(random_state = 12345,learning_rate = 0.05, max_depth = 5, min_samples_split = 2, n_estimators = 500, subsample = 0.8)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 12345,  learning_rate = 0.05, max_depth = 3, n_estimators = 1000)))\nmodels.append((\"CatBoost\", CatBoostClassifier(random_state = 12345, verbose = False, depth = 10, iterations = 1000, l2_leaf_reg = 5, learning_rate = 0.01)))\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        msg = \"%s: (%f)\" % (name, accuracy)\n        print(msg)","c6b29454":"models2 = []\nmodels2.append((\"LightGBM\", LGBMClassifier(random_state = 12345,  learning_rate = 0.05, max_depth = 3, n_estimators = 1000)))\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models2:\n        base = model.fit(X_train,y_train)\n        y_pred = base.predict(X_test)\n        acc_score = accuracy_score(y_test, y_pred)\n        feature_imp = pd.Series(base.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\n        sns.barplot(x=feature_imp, y=feature_imp.index)\n        plt.xlabel('De\u011fi\u015fken \u00d6nem Skorlar\u0131')\n        plt.ylabel('De\u011fi\u015fkenler')\n        plt.title(name)\n        plt.show()","1e991c84":"from sklearn.metrics import  accuracy_score, f1_score, precision_score,confusion_matrix, recall_score, roc_auc_score\ny_pred = model.predict(X_test)\ncm_xgb = confusion_matrix(y_test, y_pred=y_pred)\n\nTP = cm_xgb[1, 1]\nTN = cm_xgb[0, 0]\nFP = cm_xgb[0, 1]\nFN = cm_xgb[1, 0]","fdcf88d6":"from matplotlib import rc,rcParams\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    plt.rcParams.update({'font.size': 19})\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title,fontdict={'size':'16'})\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45,fontsize=12,color=\"blue\")\n    plt.yticks(tick_marks, classes,fontsize=12,color=\"blue\")\n    rc('font', weight='bold')\n    fmt = '.1f'\n    thresh = cm.max()\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"red\")\n\n    plt.ylabel('True label',fontdict={'size':'16'})\n    plt.xlabel('Predicted label',fontdict={'size':'16'})\n    plt.tight_layout()","1dd5881c":"import itertools\nplot_confusion_matrix(confusion_matrix(y_test, y_pred=y_pred), classes=['Non Churn','Churn'],\n                      title='Confusion matrix')","9442a3bc":"tn, fp, fn, tp = cm_xgb.ravel()\nprint(\"True Negatives: \",tn)\nprint(\"False Positives: \",fp)\nprint(\"False Negatives: \",fn)\nprint(\"True Positives: \",tp)","652ea3e0":"df_pred = pd.DataFrame(data=[],columns=[\"y_test\"])\ndf_pred[\"y_pred\"] = y_pred\ndf_pred[\"y_test\"] = y_test\ndf_pred.index = df_pred.index + 1\ndf_pred.head()","d5b20de1":"FP_predicts_indexes = [] \nTP_predicts_indexes=[]\nFN_predict_indexes =[]\nTN_predicts_indexes  = []\nfor index, row in df_pred.iterrows():\n    if row['y_test'] == 0 and row['y_pred'] == 1:\n        FP_predicts_indexes.append(row.name)\n    elif row['y_test'] == 1 and row['y_pred'] == 1:\n        TP_predicts_indexes.append(row.name)\n    elif row['y_test'] == 0 and row['y_pred'] == 0:\n        TN_predicts_indexes.append(row.name)\n    elif row['y_test'] == 1 and row['y_pred'] == 0:\n        FN_predict_indexes.append(row.name)    ","a720bf04":"df_pred.loc[TN_predicts_indexes,\"prediction_result\"] = \"TN\"\ndf_pred.loc[TP_predicts_indexes,\"prediction_result\"] = \"TP\"\ndf_pred.loc[FP_predicts_indexes,\"prediction_result\"] = \"FP\"\ndf_pred.loc[FN_predict_indexes,\"prediction_result\"] = \"FN\"\ndf_pred.head()","62f222fa":"df_pred[df_pred[\"prediction_result\"] == \"FP\"].head()","b766fbd7":"df_pred[df_pred[\"prediction_result\"] == \"FN\"].head()","78290d63":"lbgm_tuned = model\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, roc_curve, auc, average_precision_score\ny_pred_prob = lbgm_tuned.predict_proba(X_test)[:,1]\nfig, ax = plt.subplots()\nfpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_prob)\nroc_auc = auc(fpr,tpr)\nax.plot(fpr,tpr, label = \" area = {:0.2f}\".format(roc_auc))\nax.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\nax.set_xlabel(\"False Positive Rate\", fontsize = 10)\nax.set_ylabel(\"True Positive Rate\", fontsize = 10)\nax.set_title(\"ROC Curve\", fontsize = 18)\nax.legend(loc = 'best')\n\nclose_default = np.argmin(np.abs(thresholds_roc - 0.5))\nax.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\nplt.tight_layout()","e370af73":"# 1) Exploratory Data Analysis","530ba6ef":"## 3.1) Missing and Outlier Observation Analysis","85a41d9c":"## 4.2) Variable Importance Levels of All Models","3c9bffac":"# 7) ROC Curve","bc17da7d":"# 4) Modelling","a8b82cf0":"# Churn Prediction using Machine Learning\n\n![Churn.png](attachment:Churn.png)\n\n## Objective \n\n**Can you develop a model of machine learning that can predict customers who will leave the company?**\n\nThe aim is to estimate whether a bank's customers leave the bank or not. The event that defines the customer abandonment is the closing of the customer's bank account.\n\n## Details about the dataset:\nIt consists of 10000 observations and 12 variables. Independent variables contain information about customers. Dependent variable refers to customer abandonment status.\n\n### **Variables:**\n\n**RowNumber** \u2014 corresponds to the record (row) number and has no effect on the output. This column will be removed.\n\n**CustomerId** \u2014 contains random values and has no effect on customer leaving the bank. This column will be removed.\n\n**Surname** \u2014 the surname of a customer has no impact on their decision to leave the bank. This column will be removed.\n\n**CreditScore** \u2014 can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.\n\n**Geography** \u2014 a customer\u2019s location can affect their decision to leave the bank. We\u2019ll keep this column.\n\n**Gender** \u2014 it\u2019s interesting to explore whether gender plays a role in a customer leaving the bank. We\u2019ll include this column, too.\n\n**Age** \u2014 this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n\n**Tenure** \u2014 refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n\n**Balance** \u2014 also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n\n**NumOfProducts** \u2014 refers to the number of products that a customer has purchased through the bank.\n\n**HasCrCard** \u2014 denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank. (0=No,1=Yes)\n\n**IsActiveMember** \u2014 active customers are less likely to leave the bank, so we\u2019ll keep this. (0=No,1=Yes)\n\n**EstimatedSalary** \u2014 as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n\n**Exited** \u2014 whether or not the customer left the bank. This is what we have to predict. (0=No,1=Yes)\n\n## Result; \nThe model created as a result of LightGBM hyperparameter optimization became the model with the maxium Accuracy Score. (0.9116)","4aa1c0ef":"## 4.1) Accuracy Score","c8c9bf1d":"# 2) Data Visualization","6a07f0b3":"## 3.3) One Hot Encoding","a82114fc":"# 6) Confusion Matrix\n\n![TP%20TN%20FP%20FN.png](attachment:TP%20TN%20FP%20FN.png)","144ccd15":"## 5.1) Model Tuning Variable Importance Level","12a42500":"## 3.5) Train-Test Separation & Balancing","1b1a1a14":"## 3.4) Scaling","54928fc6":"## 3.2) Feature Engineering","20db6e91":"# 3) Data Preprocessing","8b45bc99":"# 5) Model Tuning","25f6bd0e":"# 8) Reporting\nThe aim of this study was to create classification models for the churn dataset and to predict whether a person abandons us by creating models and to obtain maximum accuracy score in the established models. The work done is as follows:\n\n1) Churn Data Set read.\n\n2) With Exploratory Data Analysis; The data set's structural data were checked. The types of variables in the dataset were examined. Size information of the dataset was accessed. Descriptive statistics of the data set were examined. It was concluded that there were no missing observations and outliers in the data set.\n\n4) During Model Building; Logistic Regression, KNN, SVM, CART, Random Forests, XGBoost, LightGBM, CatBoost like using machine learning models Accuracy  Score were calculated. Later XGBoost, LightGBM, CatBoost hyperparameter optimizations optimized to increase Accuracy score.\n\n5) Result; The model created as a result of LightGBM hyperparameter optimization became the model with the maxium Accuracy Score. **(0.9116)**"}}