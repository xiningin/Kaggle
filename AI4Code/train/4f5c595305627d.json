{"cell_type":{"29b7bf4a":"code","9c4ff755":"code","f21da646":"code","2984b4de":"code","3e1f2d9c":"code","7af019d6":"code","c80d78ed":"code","391f079c":"code","30fb40a8":"code","d96904d4":"code","71a57e45":"code","9b1970c6":"code","21ef68f8":"code","3e326694":"code","742503c2":"code","2c884450":"code","23d068b7":"code","8a8e8403":"code","30d1d2cc":"code","6c1084ff":"code","0707e9e1":"code","8c6180a9":"code","1fc6a920":"code","e3b94db7":"code","15eea652":"code","d183123d":"code","a59b711b":"code","0c89ed31":"code","3e6c684f":"code","121f44d6":"code","2eb17aab":"code","e74a7c7b":"code","65718fad":"code","2c0aac53":"code","dd318fe3":"code","b2592055":"code","c44e48a4":"code","844bdef9":"code","d14df6dc":"code","ac078ffc":"code","e34f0223":"code","637cabad":"code","e20a8675":"code","e57f8b3d":"code","9c759df2":"code","ebc857c5":"code","d920ad3e":"code","8122549f":"code","6d525dea":"code","057f2ba4":"code","131a4c72":"code","639aef20":"code","378bf569":"code","63e4f945":"code","fef64edd":"code","61918955":"code","59a475b8":"code","af2ff023":"code","50abddbd":"code","311c49ec":"code","137f2fa9":"code","ac14a28f":"code","20e17a7e":"code","0ee95000":"code","e8ab6438":"code","d048ad83":"code","bbd4f09f":"code","718d4af9":"code","febf2a4d":"code","d1650486":"code","23fc431b":"code","5aa7de40":"code","4f841a2f":"code","bc1c809a":"code","3d628020":"code","26919bed":"code","247df2b3":"code","af410d52":"code","20a8c41c":"code","13ae8910":"code","312adbb1":"markdown","ca1cda4d":"markdown","bd95693f":"markdown","be9fdf78":"markdown","94fcc8a4":"markdown","0faf1d46":"markdown","b59180d7":"markdown","bd5a8cd2":"markdown","bfaf70b3":"markdown","994905f0":"markdown","7c27a1f5":"markdown","485245b2":"markdown","2cd0d4f9":"markdown","24983041":"markdown","42c2e409":"markdown","50585965":"markdown","2490e1c9":"markdown","46d93a32":"markdown"},"source":{"29b7bf4a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c4ff755":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n## Setting max displayed rows to 500, in order to display the full output of any command \npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","f21da646":"# read the data \ndf = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","2984b4de":"df.head()","3e1f2d9c":"df.describe()","7af019d6":"df.corr()[\"SalePrice\"].sort_values(ascending = False)","c80d78ed":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.scatterplot(data = df, x = \"OverallQual\", y = \"SalePrice\");","391f079c":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.scatterplot(data = df, x = \"GrLivArea\", y = \"SalePrice\");","30fb40a8":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.scatterplot(data = df, x = \"TotalBsmtSF\", y = \"SalePrice\");","d96904d4":"df[(df[\"SalePrice\"] < 200000) & (df[\"OverallQual\"] > 8)]","71a57e45":"df[(df[\"SalePrice\"] < 200000) & (df[\"OverallQual\"] > 8) & (df[\"GrLivArea\"] > 4000)]","9b1970c6":"drop_index = df[(df[\"SalePrice\"] < 200000) & (df[\"OverallQual\"] > 8) & (df[\"GrLivArea\"] > 4000)].index","21ef68f8":"df = df.drop(drop_index, axis = 0)","3e326694":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.scatterplot(data = df, x = \"GrLivArea\", y = \"SalePrice\");","742503c2":"df.head()","2c884450":"df = df.drop(\"Id\", axis = 1)","23d068b7":"df.info()","8a8e8403":"## lets create a functions that can be used for any future data\ndef percent_missing_data(df):\n    missing_count = df.isna().sum().sort_values(ascending = False)\n    missing_percent = 100 * df.isna().sum().sort_values(ascending = False) \/ len(df)\n    missing_count = pd.DataFrame(missing_count[missing_count > 0])\n    missing_percent = pd.DataFrame(missing_percent[missing_percent > 0])\n    missing_table = pd.concat([missing_count,missing_percent], axis = 1)\n    missing_table.columns = [\"missing_count\", \"missing_percent\"]\n    \n    return missing_table","30d1d2cc":"percent_nan = percent_missing_data(df)\npercent_nan","6c1084ff":"plt.figure(figsize = (8,4), dpi = 100)\nsns.barplot(x = percent_nan.index, y = percent_nan.values[:,1])\nplt.xticks(rotation = 90)\nplt.show()","0707e9e1":"## lets see the features that has less than on percent missing\nplt.figure(figsize = (8,4), dpi = 100)\nsns.barplot(x = percent_nan.index, y = percent_nan.values[:,1])\nplt.xticks(rotation = 90)\nplt.ylim(0,1)\nplt.show()","8c6180a9":"percent_nan[percent_nan[\"missing_percent\"] < 1]","1fc6a920":"index = percent_nan[percent_nan[\"missing_percent\"] < 1].index\nfor name in index:\n    print(df[df[\"Electrical\"].isnull()][name])","e3b94db7":"df[df[\"GarageType\"].isnull()][\"GarageFinish\"]","15eea652":"df = df.dropna(axis = 0, subset = [\"GarageType\"])","d183123d":"percent_nan = percent_missing_data(df)\npercent_nan\n","a59b711b":"df[df[\"BsmtFinType1\"].isnull()]","0c89ed31":"for col in df.columns:\n    if \"Bsmt\" in col:\n        print(col)","3e6c684f":"## basement numeric features ==> fillna 0\nbsmt_num_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']\ndf[bsmt_num_cols] = df[bsmt_num_cols].fillna(0)\n\n## basement string features ==> fillna none\nbsmt_str_cols =  ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\ndf[bsmt_str_cols] = df[bsmt_str_cols].fillna('None')","121f44d6":"# now if you check again, you will find no nulls\ndf[df[\"BsmtFinSF1\"].isnull()]","2eb17aab":"percent_nan = percent_missing_data(df)\npercent_nan","e74a7c7b":"df[df[\"Electrical\"].isnull()]","65718fad":"# You have the choice of filling it with the mode or dropping it, I will drop it\ndf = df.dropna(axis = 0, subset = [\"Electrical\"])","2c0aac53":"percent_nan = percent_missing_data(df)\npercent_nan","dd318fe3":"df[[\"MasVnrArea\"]] = df[[\"MasVnrArea\"]].fillna(0)\ndf[[\"MasVnrType\"]] = df[[\"MasVnrType\"]].fillna(\"None\")","b2592055":"percent_nan = percent_missing_data(df)\npercent_nan","c44e48a4":"plt.figure(figsize = (8,4), dpi = 100)\nsns.barplot(x = percent_nan.index, y = percent_nan.values[:,1])\nplt.xticks(rotation = 90)\nplt.show()","844bdef9":"df = df.drop([\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\"], axis = 1)","d14df6dc":"percent_nan = percent_missing_data(df)\npercent_nan","ac078ffc":"plt.figure(figsize = (8,4), dpi = 100)\nsns.barplot(x = percent_nan.index, y = percent_nan.values[:,1])\nplt.xticks(rotation = 90)\nplt.show()","e34f0223":"df[\"FireplaceQu\"].value_counts()","637cabad":"df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(\"None\")","e20a8675":"percent_nan = percent_missing_data(df)\npercent_nan","e57f8b3d":"df[\"LotFrontage\"].value_counts()","9c759df2":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.boxplot(x = \"Neighborhood\", y = \"LotFrontage\", data = df)\nplt.xticks(rotation = 90)\nplt.show()","ebc857c5":"df.groupby(\"Neighborhood\")[\"LotFrontage\"].mean()","d920ad3e":"df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda value: value.fillna(value.mean()))","8122549f":"percent_nan = percent_missing_data(df)\npercent_nan","6d525dea":"df[\"MSSubClass\"].dtypes","057f2ba4":"df[\"MSSubClass\"] = df[\"MSSubClass\"].apply(str)","131a4c72":"df[\"MSSubClass\"].dtypes","639aef20":"#Select all Object Features \ndf.select_dtypes(include = \"object\")","378bf569":"df_object = df.select_dtypes(include = \"object\")\ndf_numeric = df.select_dtypes(exclude = \"object\")","63e4f945":"df_object_dummies = pd.get_dummies(df_object, drop_first = True)\ndf_object_dummies","fef64edd":"df_final = pd.concat([df_numeric, df_object_dummies], axis = 1)\ndf_final.head()","61918955":"print(df_final.shape)","59a475b8":"corr = abs(df_final.corr()[\"SalePrice\"]).sort_values(ascending = False)\nlarge_corr = corr[corr > 0.3]\n\nplt.figure(figsize = (10, 4), dpi = 100)\nsns.barplot(x = large_corr.index, y = large_corr.values)\nplt.xticks(rotation = 90)\nplt.show()","af2ff023":"# Split the data for X and y\nX = df_final.drop(\"SalePrice\", axis = 1)\ny = df_final[\"SalePrice\"]","50abddbd":"# Train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","311c49ec":"# scaling the X data \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train) # only fit to training data to aviod data leakage\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","137f2fa9":"# Create the Ridge model\nfrom sklearn.linear_model import Ridge\nridge1 = Ridge(alpha = 100)\nridge1.fit(X_train, y_train)\n\n# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = ridge1.predict(X_test)\nmean_absolute_error(y_test, y_predict)","ac14a28f":"# first split\nfrom sklearn.model_selection import train_test_split\nX_train, X_other, y_train, y_other = train_test_split(X, y, test_size=0.3, random_state=101)\n\n# second split: 50% of 30% = 15% of all data \nX_eval, X_test, y_eval, y_test = train_test_split(X_other, y_other, test_size=0.5, random_state=101)","20e17a7e":"# scaling the X data \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train) # only fit to training data to aviod data leakage\n\nX_train = scaler.transform(X_train)\nX_eval = scaler.transform(X_eval)\nX_test = scaler.transform(X_test)","0ee95000":"# Create the Ridge model\nfrom sklearn.linear_model import Ridge\nridge1 = Ridge(alpha = 100)\nridge1.fit(X_train, y_train)\n\n# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = ridge1.predict(X_eval)\nmean_absolute_error(y_eval, y_predict)","e8ab6438":"# Create the Ridge model\nalpha_list = []\nmse_list = []\nfor alpha_val in np.arange(0.01, 200):\n    from sklearn.linear_model import Ridge\n    ridge1 = Ridge(alpha = alpha_val)\n    ridge1.fit(X_train, y_train)\n    alpha_list.append(alpha_val)\n    \n    # testing the model\n    from sklearn.metrics import mean_absolute_error\n    y_predict = ridge1.predict(X_eval)\n    mse = mean_absolute_error(y_eval, y_predict)\n    mse_list.append(mse)","d048ad83":"alpha_list = pd.DataFrame(alpha_list)\nmse_list = pd.DataFrame(mse_list)\nalpha_mse = pd.concat([alpha_list, mse_list], axis = 1)\nalpha_mse.columns = [\"alpha_list\", \"mse_list\"]","bbd4f09f":"alpha_mse[alpha_mse[\"mse_list\"] == alpha_mse[\"mse_list\"].min()]","718d4af9":"# Create the Ridge model\nfrom sklearn.linear_model import Ridge\nridge3 = Ridge(alpha = 81.01)\nridge3.fit(X_train, y_train)\n\n# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = ridge3.predict(X_eval)\nprint(mean_absolute_error(y_eval, y_predict))\n\ny_final_test_predict = ridge3.predict(X_test)\nprint(mean_absolute_error(y_test, y_final_test_predict))","febf2a4d":"# Create the Ridge model\nfrom sklearn.linear_model import Lasso\nls = Lasso(alpha = 100)\nls.fit(X_train, y_train)\n\n# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = ls.predict(X_eval)\nprint(mean_absolute_error(y_eval, y_predict))\n\ny_final_test_predict = ls.predict(X_test)\nprint(mean_absolute_error(y_test, y_final_test_predict))","d1650486":"# Create the Ridge model\nalpha_list = []\nmse_list = []\nfor alpha_val in np.arange(0.01, 200):\n    from sklearn.linear_model import Lasso\n    ls = Lasso(alpha = alpha_val)\n    ls.fit(X_train, y_train)\n    alpha_list.append(alpha_val)\n    \n    # testing the model\n    from sklearn.metrics import mean_absolute_error\n    y_predict = ls.predict(X_eval)\n    mse = mean_absolute_error(y_eval, y_predict)\n    mse_list.append(mse)","23fc431b":"alpha_list = pd.DataFrame(alpha_list)\nmse_list = pd.DataFrame(mse_list)\nalpha_mse = pd.concat([alpha_list, mse_list], axis = 1)\nalpha_mse.columns = [\"alpha_list\", \"mse_list\"]","5aa7de40":"alpha_mse[alpha_mse[\"mse_list\"] == alpha_mse[\"mse_list\"].min()]","4f841a2f":"# Create the optimal Ridge model\nfrom sklearn.linear_model import Lasso\nls = Lasso(alpha = 199.01)\nls.fit(X_train, y_train)\n\n# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = ls.predict(X_eval)\nprint(mean_absolute_error(y_eval, y_predict))\n\ny_final_test_predict = ls.predict(X_test)\nprint(mean_absolute_error(y_test, y_final_test_predict))","bc1c809a":"from sklearn.linear_model import ElasticNetCV\nelastic_model = ElasticNetCV(l1_ratio= np.linspace(0.01, 1, 100),tol=0.01)\nelastic_model.fit(X_train,y_train)","3d628020":"elastic_model.l1_ratio_","26919bed":"# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = elastic_model.predict(X_eval)\nprint(mean_absolute_error(y_eval, y_predict))\n\ny_final_test_predict = elastic_model.predict(X_test)\nprint(mean_absolute_error(y_test, y_final_test_predict))","247df2b3":"#Import the poly conerter \nfrom sklearn.preprocessing import PolynomialFeatures\npolynomial_converter = PolynomialFeatures(degree=2,include_bias=False)\n\n#convert X data \npoly_features_train = polynomial_converter.fit_transform(X_train)\npoly_features_eval = polynomial_converter.fit_transform(X_eval)\npoly_features_test = polynomial_converter.fit_transform(X_test)","af410d52":"poly_features_train.shape","20a8c41c":"#import elastic net \nfrom sklearn.linear_model import ElasticNetCV\nelastic_model = ElasticNetCV(l1_ratio= 1,tol=0.01)\nelastic_model.fit(poly_features_train,y_train)","13ae8910":"# testing the model\nfrom sklearn.metrics import mean_absolute_error\ny_predict = elastic_model.predict(poly_features_eval)\nprint(mean_absolute_error(y_eval, y_predict))\n\ny_final_test_predict = elastic_model.predict(poly_features_test)\nprint(mean_absolute_error(y_test, y_final_test_predict))","312adbb1":"Lets now repeat one of the scatter plots that we had before","ca1cda4d":"It seems that all features related Basement have very high number of missing values. If we go back to data description you will find that Nan actually means that the house do not has a basement. It is not missing, it just has one. Therefore, it does make sense to replace nan values with a string saying that the house has no Basement. This will work for Basement string columns, as for Basement numeric columns we will replace them with zero.","bd95693f":"Note that we will not calculate coeff_ for each single feature. Using regularization, we will be able to drop non-important features lets now cocatinate the two data frames","be9fdf78":"Overall Quality is the most important feature for our model. But there is a problem. It is most likely to be generated by human judgment, therefore model deployment will be dependent on the existence of that human who judge the quality of the house and feed it to the model.\n\nLets now proceed to model building and evaluation\n3. Model Building and evaluation\nTrain | Test Split Procedure\nSplit Data in Train\/Test for both X and y\nFit\/Train Scaler on Training X Data\nScale X Test Data\nCreate Model\nFit\/Train Model on X Train Data\nEvaluate Model on X Test Data (by creating predictions and comparing to Y_test)\nAdjust Parameters as Necessary and repeat steps 5 and 6","94fcc8a4":"In principle we should go through each feature and decide whether we will keep it, fill it or drop it. When we speak about dropping we can drop columns or rows.\n\nFor example Pool QC values are missing for 99.6 percent of houses. This might be due to:\n\nThese houses have no pools, and instead of nan it should have been zero.\nThese houses have pools, but the data is actually missing.\nWe should go back to the description file and try to understand it better. But now, lets deal with columns with very few missing values.","0faf1d46":"Yeah! Congratulations! we did it. Nothing is missing any more!\n\nLets now move to encoding options. Essentially we will use one hot encoding with variables of the type \"Object\". However, There is one varaible that seems Numeric where in fact it is categorical. It is \"MS SubClass\".\n\nIf we go back to data description we will find that:\n\nMSSubClass: Identifies the type of dwelling involved in the sale.\n\n    20  1-STORY 1946 & NEWER ALL STYLES\n    30  1-STORY 1945 & OLDER\n    40  1-STORY W\/FINISHED ATTIC ALL AGES\n    45  1-1\/2 STORY - UNFINISHED ALL AGES\n    50  1-1\/2 STORY FINISHED ALL AGES\n    60  2-STORY 1946 & NEWER\n    70  2-STORY 1945 & OLDER\n    75  2-1\/2 STORY ALL AGES\n    80  SPLIT OR MULTI-LEVEL\n    85  SPLIT FOYER\n    90  DUPLEX - ALL STYLES AND AGES\n   120  1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n   150  1-1\/2 STORY PUD - ALL AGES\n   160  2-STORY PUD - 1946 & NEWER\n   180  PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\n   190  2 FAMILY CONVERSION - ALL STYLES AND AGES\n\nThese numbers has no ordinal meaning. So we should covert the variable from integer to text.","b59180d7":"As we can see there are some points with very high quality (10\/10) but very low price. Lets explore other highly correlated features with Sale Price","bd5a8cd2":"What to do with the rest?\nThe rest of the features have more than 1% missing data. We need to carefully look at each one and decide how to deal with them. For sure, dropping rows is not a possible strategy any more. so we need to figure out something else. We have two options:\n\nFill in missing values\nDrop thr feature column","bfaf70b3":"Disadvantages of classic train test split:\n\nGetting the right parameter is quite tedious\nIt is not the most fair evaluation, because we adjusted the parameters to have better performance on that specific test data.\nTherfore its useful to hold some data aside. The model has never been adjusted to this data before, therfore it reflects the true evaluation matrix.\n\nTrain | Validation | Test Split Procedure\nThis is often also called a \"hold-out\" set, since you should not adjust parameters based on the final test set, but instead use it only for reporting final expected performance.\n\nClean and adjust data as necessary for X and y\nSplit Data in Train\/Validation\/Test for both X and y\nFit\/Train Scaler on Training X Data\nScale X Eval Data\nCreate Model\nFit\/Train Model on X Train Data\nEvaluate Model on X Evaluation Data (by creating predictions and comparing to Y_eval)\nAdjust Parameters as Necessary and repeat steps 5 and 6\nGet final metrics on Test set (not allowed to go back and adjust after this!)","994905f0":"Both \"Mas Vnr Area\" and \"Mas Vnr Type\" have less than 1 percent of null values. How to deal with them?\n\nGoing back to data description, we found that there is a category for none: It does not have \"Mas Vnr\". We can assume that those missing values are also none but they are mistakenly filled with Nan.","7c27a1f5":"Now we are left with just two columns. You have to be carefull and do a lot of thinking because you can not just drop the rows nor the feature columns. Not enough to drop the feature but not too little to drop the rows.","485245b2":"The points that indicate very high price and also very high living area (at the top right corner) are not outliers. They make sense as they are follwing a trend, therefore they will not hurt our model.\n\nOn the other hand The 3 points at the right-lower corner indicate very high living area but very low price. They are very likely to be outliers because they are not following the general trend.\n\nLets now check those points closely","2cd0d4f9":"lets now look at these rows, there might be houses with missing values across all features","24983041":"Id is just an identifier, it has no numeric value for the model. Set it as index, or drop it. Dropping it will not make any problems, because we have the default identifier (0, 1, 2, 3, ... )","42c2e409":"It is tricky, it is numeric. I can not longer go back to the description and fill it with a convenient text. We will use the Neighborhood feature calculate the missing feature.\n\nNeighborhood: Physical locations within Ames city limits\n\nLotFrontage: Linear feet of street connected to property\n\nWe will operate under the assumption that the Lot Frontage is related to what neighborhood a house is in.","50585965":"1. Checking for outliers\nThe following example shows why outliers are very dangerous. They significantly affect the mean and the standard deviation and thus affecting the estimators of the model.\nIn order to visually see outliers, we need a box plot or a scatter plot. Therefore, lets see the most correlated features with sale price to plot them a gainst each others","2490e1c9":"Some of the above features have more than 99 percent missing data, dropping these features can be the best strategy to opt for.","46d93a32":"Electrical still has 1 missing value, lets look at it closely and decide"}}