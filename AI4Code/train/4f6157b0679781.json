{"cell_type":{"ceb5283d":"code","cd01c35c":"code","5f86b668":"code","a7fc6a19":"code","962324ca":"code","50e0434d":"code","fe0be4e7":"code","77938d02":"code","cfc3da1a":"code","787c2f96":"code","cccacd07":"code","72c5cc36":"code","aa254c6a":"code","0c6e29e7":"code","293cc8d1":"code","ff6a8323":"code","102a85f7":"code","f5f47599":"code","41703220":"code","0ca591ed":"code","34e3cdfe":"code","0109f39d":"code","ebd154c2":"code","065d8aa7":"code","18ebb953":"code","180efa8d":"code","0ce8923d":"code","11313a41":"code","aa317250":"code","ec252f6f":"code","d2c85063":"code","3feb175d":"code","9ad0989d":"code","2f162bb7":"code","01a08964":"code","003ae533":"code","124c777a":"code","bc1fd156":"code","4eb185cf":"code","dbc99d20":"code","ff1effda":"code","297a6b5f":"code","7407031e":"code","87597951":"code","8762611e":"code","b77a38da":"code","16d935a0":"code","5a83050d":"markdown","2f1397f9":"markdown","e316eb87":"markdown","8678eaec":"markdown","ceff170a":"markdown","ee4f6a0d":"markdown","305d591d":"markdown","4ca65e43":"markdown","70271022":"markdown","e8808d7d":"markdown","b669208f":"markdown","ff34a2c5":"markdown","56bff42b":"markdown","4e1e45dd":"markdown","1819fd52":"markdown"},"source":{"ceb5283d":"import warnings\nwarnings.filterwarnings('ignore')","cd01c35c":"import pandas as pd\nimport io\n\n  \ndf = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\nprint(df)","5f86b668":"df.head()","a7fc6a19":"df.describe()","962324ca":"df.info()","50e0434d":"df1 = df.copy()\n\ndf1 = df1.astype({\n    'anaemia': 'category',\n    'diabetes': 'category',\n    'sex': 'category',\n    'high_blood_pressure': 'category',\n    'smoking': 'category',\n    'DEATH_EVENT': 'category'\n})\n","fe0be4e7":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport numpy as np","77938d02":"def dist_box(data):\n # function plots a combined graph for univariate analysis of continous variable \n #to check spread, central tendency , dispersion and outliers  \n    Name=data.name.upper()\n    fig,(ax_box,ax_dis)  =plt.subplots(2,1,gridspec_kw = {\"height_ratios\": (.25, .75)},figsize=(8, 5))\n    mean=data.mean()\n    median=data.median()\n    mode=data.mode().tolist()[0]\n    fig.suptitle(\"SPREAD OF DATA FOR \"+ Name  , fontsize=18, fontweight='bold')\n    sns.boxplot(x=data,showmeans=True, orient='h',color=\"violet\",ax=ax_box)\n    ax_box.set(xlabel='')\n    sns.distplot(data,kde=False,color='blue',ax=ax_dis)\n    ax_dis.axvline(mean, color='r', linestyle='--',linewidth=2)\n    ax_dis.axvline(median, color='g', linestyle='-',linewidth=2)\n    ax_dis.axvline(mode, color='y', linestyle='-',linewidth=2)\n    plt.legend({'Mean':mean,'Median':median,'Mode':mode})","cfc3da1a":"#select all quantitative columns for checking the spread\nlist_col=  df1.select_dtypes([np.number]).columns\nfor i in range(len(list_col)):\n    dist_box(df[list_col[i]])","787c2f96":"# Function to create barplots that indicate percentage for each category.\ndef bar_perc(plot, feature):\n    total = len(feature) # length of the column\n    for p in plot.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total) # percentage of each class of the category\n        x = p.get_x() + p.get_width() \/ 2 - 0.05 # width of the plot\n        y = p.get_y() + p.get_height()           # hieght of the plot\n        plot.annotate(percentage, (x, y), size = 12) # annotate the percentage","cccacd07":"#get all category datatype \nlist_col=  df1.select_dtypes(['category']).columns\nfig1, axes1 =plt.subplots(1,6,figsize=(30, 10))\nfor i in range(len(list_col)):\n    order = df1[list_col[i]].value_counts(ascending=False).index # to display bar in ascending order\n    axis=sns.countplot(x=list_col[i], data=df1 , order=order,ax=axes1[i],palette='viridis').set(title=list_col[i].upper())\n    bar_perc(axes1[i],df1[list_col[i]])","72c5cc36":"plt.figure(figsize=(15,5))\nsns.heatmap(df.corr(),annot=True ,cmap=\"YlGn\")\nplt.show()","aa254c6a":"df1.drop(['time'], axis = 1, inplace = True)\n\ndf.drop(['time'], axis = 1, inplace = True)","0c6e29e7":"# AGE VS DEATH_EVENT\n\nsns.boxplot(x=df1['DEATH_EVENT'],\n              y=df1['age'])\n","293cc8d1":"# ANAEMIA VS DEATH_EVENT\n\nplt.scatter","ff6a8323":"# SERUM_CREATININE VS DEATH_EVENT\n\nsns.boxplot(x=df1['DEATH_EVENT'],\n              y=df1['serum_creatinine'])\n","102a85f7":"df_max_scaled = df.copy()\n\nnum_cols = [col for col in df_max_scaled.columns if df_max_scaled[col].dtype in ['int', 'float']]\n\nfor col in num_cols:\n  df_max_scaled[col] = df_max_scaled[col] \/ df_max_scaled[col].abs().max() \n\ndisplay(df_max_scaled)","f5f47599":"list_col=  df_max_scaled.select_dtypes([np.number]).columns\nfor i in range(len(list_col)):\n    dist_box(df_max_scaled[list_col[i]])","41703220":"death_1 = df.loc[df['DEATH_EVENT'] == 1]\n\ndeath_0 = df.loc[df['DEATH_EVENT'] == 0]","0ca591ed":"list_col=  df.select_dtypes([np.number]).columns\nfor i in range(len(list_col)):\n    dist_box(death_0[list_col[i]])\n    dist_box(death_1[list_col[i]])","34e3cdfe":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.metrics import mean_absolute_error, accuracy_score, classification_report,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold,StratifiedKFold","0109f39d":"y = df[\"DEATH_EVENT\"]\nX = df.drop('DEATH_EVENT',axis=1)\nX_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.20, random_state = 0)\nscaler = StandardScaler()\nX_train_orig = scaler.fit_transform(X_train_orig)\nX_test_orig = scaler.transform(X_test_orig)","ebd154c2":"df2 = df.copy()\n\n## REMOVE OUTLIERS\n\ndef remove_outlier(df_in, col_name):\n    q1 = df_in[col_name].quantile(0.25)\n    q3 = df_in[col_name].quantile(0.75)\n    iqr = q3-q1 #Interquartile range\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n    return df_out\n\ncols = ['creatinine_phosphokinase', 'serum_creatinine', 'platelets']\n\nfor i in cols:\n  df2 = remove_outlier(df2, i)\n\ny = df2[\"DEATH_EVENT\"]\nX = df2.drop('DEATH_EVENT',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","065d8aa7":"## LOGISTIC REGRESSION\n\nm1 = 'Logistic Regression'\nlr = LogisticRegression()\nmodel = lr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\nprint(\"confussion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,lr_predict))","18ebb953":"m1 = 'Logistic Regression'\nlr = LogisticRegression()\nmodel = lr.fit(X_train_orig, y_train_orig)\nlr_predict = lr.predict(X_test_orig)\nlr_conf_matrix = confusion_matrix(y_test_orig, lr_predict)\nlr_acc_score = accuracy_score(y_test_orig, lr_predict)\nprint(\"confussion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test_orig,lr_predict))","180efa8d":"## NAIVE BAYES\n\nm2 = 'Naive Bayes'\nnb = GaussianNB()\nnb.fit(X_train,y_train)\nnbpred = nb.predict(X_test)\nnb_conf_matrix = confusion_matrix(y_test, nbpred)\nnb_acc_score = accuracy_score(y_test, nbpred)\nprint(\"confussion matrix\")\nprint(nb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Naive Bayes model:\",nb_acc_score*100,'\\n')\nprint(classification_report(y_test,nbpred))","0ce8923d":"m2 = 'Naive Bayes'\nnb = GaussianNB()\nnb.fit(X_train_orig,y_train_orig)\nnbpred = nb.predict(X_test_orig)\nnb_conf_matrix = confusion_matrix(y_test_orig, nbpred)\nnb_acc_score = accuracy_score(y_test_orig, nbpred)\nprint(\"confussion matrix\")\nprint(nb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Naive Bayes model:\",nb_acc_score*100,'\\n')\nprint(classification_report(y_test_orig,nbpred))","11313a41":"## RANDOM FOREST\n\nm3 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=20, random_state=2,max_depth=5)\nrf.fit(X_train,y_train)\nrf_predicted = rf.predict(X_test)\nrf_conf_matrix = confusion_matrix(y_test, rf_predicted)\nrf_acc_score = accuracy_score(y_test, rf_predicted)\nprint(\"confussion matrix\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(classification_report(y_test,rf_predicted))","aa317250":"## XGBOOSTING\n\nm4 = 'Extreme Gradient Boost'\nxgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\nxgb.fit(X_train, y_train)\nxgb_predicted = xgb.predict(X_test)\nxgb_conf_matrix = confusion_matrix(y_test, xgb_predicted)\nxgb_acc_score = accuracy_score(y_test, xgb_predicted)\nprint(\"confussion matrix\")\nprint(xgb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Extreme Gradient Boost:\",xgb_acc_score*100,'\\n')\nprint(classification_report(y_test,xgb_predicted))","ec252f6f":"## KNN\n\nm5 = 'K-NeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\nknn_predicted = knn.predict(X_test)\nknn_conf_matrix = confusion_matrix(y_test, knn_predicted)\nknn_acc_score = accuracy_score(y_test, knn_predicted)\nprint(\"confussion matrix\")\nprint(knn_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of K-NeighborsClassifier:\",knn_acc_score*100,'\\n')\nprint(classification_report(y_test,knn_predicted))","d2c85063":"## DECISION TREE\n\nm6 = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)\ndt.fit(X_train, y_train)\ndt_predicted = dt.predict(X_test)\ndt_conf_matrix = confusion_matrix(y_test, dt_predicted)\ndt_acc_score = accuracy_score(y_test, dt_predicted)\nprint(\"confussion matrix\")\nprint(dt_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\nprint(classification_report(y_test,dt_predicted))","3feb175d":"## SVC\n\nm7 = 'Support Vector Classifier'\nsvc =  SVC(kernel='rbf', C=2)\nsvc.fit(X_train, y_train)\nsvc_predicted = svc.predict(X_test)\nsvc_conf_matrix = confusion_matrix(y_test, svc_predicted)\nsvc_acc_score = accuracy_score(y_test, svc_predicted)\nprint(\"confussion matrix\")\nprint(svc_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Support Vector Classifier:\",svc_acc_score*100,'\\n')\nprint(classification_report(y_test,svc_predicted))","9ad0989d":"df2.columns","2f162bb7":"grid_param = {\n    'penalty': ['l1', 'l2'],\n    'C' : [0.001, 0.01, 0.1, 0.005, 0.5, 1, 10]\n}\n\ngrid_search_lr = GridSearchCV(lr, grid_param, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search_lr.fit(X_train, y_train)","01a08964":"## best parameters\ngrid_search_lr.best_params_","003ae533":"## best score\ngrid_search_lr.best_score_","124c777a":"grid_search_lr.cv_results_","bc1fd156":"grid_param = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10],\n    'max_features' : ['auto', 'sqrt', 'log2']\n}\n\ngrid_search_rf = GridSearchCV(rf, grid_param, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search_rf.fit(X_train, y_train)","4eb185cf":"print(grid_search_rf.best_score_)\nprint(grid_search_rf.best_params_)","dbc99d20":"grid_param = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10],\n    'splitter' : ['best', 'random'],\n    'min_samples_leaf' : [1, 2, 3, 5, 7],\n    'min_samples_split' : [1, 2, 3, 5, 7],\n    'max_features' : ['auto', 'sqrt', 'log2']\n}\n\ngrid_search_dtc = GridSearchCV(dt, grid_param, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search_dtc.fit(X_train, y_train)","ff1effda":"print(grid_search_dtc.best_params_)\nprint(grid_search_dtc.best_score_)","297a6b5f":"from mlxtend.classifier import StackingCVClassifier","7407031e":"scv=StackingCVClassifier(classifiers=[lr,knn,xgb],\n                         meta_classifier= svc)\n\nscv.fit(np.asarray(X_train),np.asarray(y_train))\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","87597951":"predictions = scv.predict(X_test)","8762611e":"test = pd.DataFrame(X_test, columns = ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', 'ejection_fraction',\n                                 'high_blood_pressure', 'platelets', 'serum_creatinine', 'serum_sodium', 'sex',\n                                 'smoking'])","b77a38da":"pred = pd.DataFrame(predictions)\npred.columns = ['DEATH_EVENT']\nsub_df = pd.concat([test, pred])\nsub_df.to_csv('Submission.csv', index = False)","16d935a0":"pred","5a83050d":"#### *2) HYPERPARAMETER TUNING*","2f1397f9":"Drop 'time' feature since it is a useless variable","e316eb87":"Scale the data i.e Min - Max Scaling","8678eaec":"### MODEL BUILDING","ceff170a":"#### *1) PREPARATIONS*","ee4f6a0d":"#### *1) UNIVARIATE*","305d591d":"**- Parameter grid for Random Forest**","4ca65e43":"**Observations :**\n- Serum Creatinine is highly skewed, has a lot of outliers.\n- Creatinine Phosphokinase also has a lot of outliers\n","70271022":"**- Parameter grid for Dec Tree**","e8808d7d":"### EXPLORATORY DATA ANALYSIS","b669208f":"#### *2) BIVARIATE AND MULTIVARIATE ANALYSIS*","ff34a2c5":"**- Parameter grid for Logistic Regression**","56bff42b":"**We will use the following models :**\n\n*1) Logistic Regression*\n\n*2) Naive Bayes*\n\n*3) Random Forest Classifier*\n\n*4) Extreme Gradient Boost*\n\n*5) K-Nearest Neighbour*\n\n*6) Decision Tree*\n\n*7) Support Vector Machine*","4e1e45dd":"**Observations :**\n- Most useful features are age, serum_creatinine","1819fd52":"####*3) ENSEMBLING*"}}