{"cell_type":{"fab66ffc":"code","8ad37749":"code","9dcade6a":"code","8f00795d":"code","200c59be":"code","83e89a96":"code","bb65889a":"code","63c5a464":"code","4473e5e5":"code","f1adbe13":"code","571747a6":"code","468c3bf0":"code","f9609ee9":"code","ae092677":"code","44b26d65":"code","5c7bb107":"code","26862782":"code","cdb37326":"code","ee258ffc":"code","c247ae23":"code","73ba600d":"code","666948b6":"code","9bd0b2e0":"code","b93b0703":"code","2002e148":"code","394c2aad":"code","25e23c04":"code","d239b80d":"code","7c448cfb":"code","45fff75b":"code","587a4230":"code","aa5ec3e6":"code","23739575":"code","8399e5ef":"code","e67a609e":"code","f62fc46b":"code","23a968a6":"code","ef1cc337":"code","a25163a5":"code","06a545b7":"code","03901aec":"code","67600587":"code","7e36e987":"markdown","4c3f58fe":"markdown","9bca59a5":"markdown","c59cc1f7":"markdown","720f71c1":"markdown","b106f2dc":"markdown","c610c587":"markdown","15ce81b2":"markdown","da1fc178":"markdown","344c4199":"markdown","d56c6c15":"markdown","704d81ad":"markdown","860e59f9":"markdown","2d9abcad":"markdown","9fcc4803":"markdown","eb29ac18":"markdown","d86e2817":"markdown","110b8f2b":"markdown","3accb4a2":"markdown","0021da9b":"markdown"},"source":{"fab66ffc":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import parallel_coordinates\nimport eli5\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nimport lightgbm as lgbm\nimport xgboost as xgb\n\npd.set_option('max_columns',100)","8ad37749":"# Download data\ntraindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId')\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","9dcade6a":"traindf.head(3)","8f00795d":"traindf.info()","200c59be":"testdf.info()","83e89a96":"submission.head()","bb65889a":"# FE - thanks to:\n# https:\/\/www.kaggle.com\/mauricef\/titanic\n# https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n#\ndf = pd.concat([traindf, testdf], axis=0, sort=False)\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - \\\n                                    df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount \/ df.WomanOrBoyCount.replace(0, np.nan)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.replace(np.nan, 0)\ndf['Alone'] = (df.WomanOrBoyCount == 0)\n\n#Thanks to https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster\n#\"Title\" improvement\ndf['Title'] = df['Title'].replace('Ms','Miss')\ndf['Title'] = df['Title'].replace('Mlle','Miss')\ndf['Title'] = df['Title'].replace('Mme','Mrs')\n# Embarked\ndf['Embarked'] = df['Embarked'].fillna('S')\n# Cabin, Deck\ndf['Deck'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n#df.loc[(df['Deck'] == 'T'), 'Deck'] = 'A'\n\n# Thanks to https:\/\/www.kaggle.com\/erinsweet\/simpledetect\n# Fare\nmed_fare = df.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ndf['Fare'] = df['Fare'].fillna(med_fare)\n#Age\ndf['Age'] = df.groupby(['Sex', 'Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))\n# Family_Size\ndf['Family_Size'] = df['SibSp'] + df['Parch'] + 1\n\n# Thanks to https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis\ncols_to_drop = ['Name','Ticket','Cabin', 'IsWomanOrBoy', 'WomanOrBoyCount', 'FamilySurvivedCount']\ndf = df.drop(cols_to_drop, axis=1)\n\ndf.WomanOrBoySurvived = df.WomanOrBoySurvived.fillna(0)\ndf.Alone = df.Alone.fillna(0)\n\ntarget = df.Survived.loc[traindf.index]\ndf = df.drop(['Survived'], axis=1)\ntrain, test = df.loc[traindf.index], df.loc[testdf.index]","63c5a464":"train.head(5)","4473e5e5":"train.info()","f1adbe13":"test.info()","571747a6":"# Encoding categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train.columns.values.tolist()\nfor col in features:\n    if train[col].dtype in numerics: continue\n    categorical_columns.append(col)\nfor col in categorical_columns:\n    if col in train.columns:\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values)) ","468c3bf0":"train.info()","f9609ee9":"test.info()","ae092677":"#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.2, random_state=0)\ntrain_set = lgbm.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgbm.Dataset(Xval, Zval, silent=False)","44b26d65":"# Tuning LGB model\n# See parameters in the documentation https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'binary', # for regression task - \"regression\" or other\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 50 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'binary',     # eval_metric, for regression task - \"rmse\" or other\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 2,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=2000,\n                   early_stopping_rounds=10, verbose_eval=10, valid_sets=valid_set)","5c7bb107":"# FI diagram drawing\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","26862782":"# FI diagram saving\nfeature_score = pd.DataFrame(train.columns, columns = ['feature']) \nfeature_score['LGB'] = modelL.feature_importance()","cdb37326":"# Prediction\ny_preds_lgb = modelL.predict(test, num_iteration=modelL.best_iteration)","ee258ffc":"#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\ndata_train = xgb.DMatrix(train)\ndata_test  = xgb.DMatrix(test)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","c247ae23":"# Tuning XGB model\n# See parameters in the documentation https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\nparms = {'max_depth':5, # maximum depth of a tree\n         'objective':'reg:logistic', # for regression task - \"reg:squarederror\" or other\n         'eval_metric':'error',      # for regression task - \"rmse\" or other\n         'learning_rate':0.01,\n         'subsample':0.8, # SGD will use this percentage of data\n         'colsample_bylevel':0.9,\n         'min_child_weight': 2,\n         'seed': 0}\nmodelx = xgb.train(parms, data_tr, num_boost_round=2000, evals = evallist,\n                  early_stopping_rounds=300, maximize=False, \n                  verbose_eval=100)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","73ba600d":"# FI diagram drawing\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","666948b6":"# FI diagram saving\nfeature_score['XGB'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))","9bd0b2e0":"# Prediction\ny_preds_xgb = modelx.predict(data_test)","b93b0703":"# Standardization for regression models\nScaler_train = preprocessing.MinMaxScaler().fit(train)\ntrain = pd.DataFrame(Scaler_train.transform(train), columns=train.columns, index=train.index)\ntest = pd.DataFrame(Scaler_train.transform(test), columns=test.columns, index=test.index)","2002e148":"# Linear Regression Tuning\nlinreg = LinearRegression()\nlinreg.fit(train, target)","394c2aad":"# FI diagram drawing\ncoeff_linreg = pd.DataFrame(train.columns)\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"LinRegress\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='LinRegress', ascending=False)","25e23c04":"# Eli5 visualization\neli5.show_weights(linreg)","d239b80d":"# FI diagram saving\ncoeff_linreg[\"LinRegress\"] = coeff_linreg[\"LinRegress\"].abs()\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","7c448cfb":"# Prediction\ny_preds_linreg = linreg.predict(test)","45fff75b":"# MinMax scaling all feature importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['Mean'] = feature_score.mean(axis=1)","587a4230":"# Merging FI diagram\n\n# Set weight of models\nw_lgb = 0.4\nw_xgb = 0.5\nw_linreg = 1 - w_lgb - w_xgb\nw_linreg\n\n# Create merging column with different weights\nfeature_score['Merging'] = w_lgb*feature_score['LGB'] + w_xgb*feature_score['XGB'] + w_linreg*feature_score['LinRegress']\nfeature_score.sort_values('Merging', ascending=False)","aa5ec3e6":"# Plot the feature importances\nplot_title = \"Feature Importance - Advanced Visualization with Matplotlib\"\nfeature_score.sort_values('Merging', ascending=False).plot(kind='bar', figsize=(20, 10), title = plot_title)","23739575":"def plot_feature_parallel(df, title):\n    # Draw sns.parallel_coordinates for features of the given df\n    \n    plt.figure(figsize=(15,12))\n    parallel_coordinates(df, 'feature', colormap=plt.get_cmap(\"tab20c\"), lw=3)\n    plt.title(title)\n    plt.xlabel(\"Models\")\n    plt.ylabel(\"Feature importance\")\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.savefig('graph.png')\n    plt.show()","8399e5ef":"# List of models\nfeature_score_columns = feature_score.columns\nfeature_score_columns","e67a609e":"feature_score = feature_score.reset_index(drop=False)\nplot_feature_parallel(feature_score, f\"Feature Importance - Advanced Visualization with Seaborn\")","f62fc46b":"feature_score","23a968a6":"def features_selection_by_weights(df, threshold):\n    # Selection features with weights more threshold at least in a one column (model)\n\n    features_list = df.feature.tolist()\n    features_best = []\n    for i in range(len(df)):\n        feature_name = features_list[i]\n        feature_is_best = False\n        for col in feature_score_columns:\n            if df.loc[i, col] > threshold:\n                feature_is_best = True\n        if feature_is_best:\n            features_best.append(feature_name)\n    \n    return df[df['feature'].isin(features_best)].reset_index(drop=True)","ef1cc337":"# Selection the best features\nthreshold_fi = 0.25\nfeature_score_best = features_selection_by_weights(feature_score, threshold_fi)\nfeature_score_best","a25163a5":"plot_feature_parallel(feature_score_best, f\"Feature Importance of the best of features - Advanced Visualization with Seaborn\")","06a545b7":"# Merging solutions and submission\ny_preds = w_lgb*y_preds_lgb + w_xgb*y_preds_xgb + w_linreg*y_preds_linreg\nsubmission['Survived'] = [1 if x>0.5 else 0 for x in y_preds]\nsubmission.head()","03901aec":"submission['Survived'].hist()","67600587":"submission.to_csv('submission.csv', index=False)","7e36e987":"## 4. Preparing to modeling <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","4c3f58fe":"Your comments and feedback are most welcome.","9bca59a5":"I hope you find this kernel useful and enjoyable.","c59cc1f7":"### 5.3 Linear Regression <a class=\"anchor\" id=\"5.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","720f71c1":"This based on my notebook [Merging FE & Prediction - xgb, lgb, logr, linr](https:\/\/www.kaggle.com\/vbmokin\/merging-fe-prediction-xgb-lgb-logr-linr)","b106f2dc":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","c610c587":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download datasets](#2)\n1. [FE & EDA](#3)\n1. [Preparing to modeling](#4)\n1. [Tuning models, building the feature importance diagrams and prediction](#5)\n    -  [LGBM](#5.1)\n    -  [XGB](#5.2)\n    -  [Linear Regression](#5.3)\n1. [Comparison and merging of all feature importance diagrams](#6)\n1. [Feature Importance - Advanced Visualization](#7)\n    -  [Matplotlib](#7.1)\n    -  [Seaborn](#7.2)\n1. [Merging solutions and submission](#8)","15ce81b2":"### 7.2 Seaborn <a class=\"anchor\" id=\"7.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","da1fc178":"### 5.2 XGB<a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","344c4199":"## 5. Tuning models, building the feature importance diagrams and prediction<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","d56c6c15":"## 3. FE & EDA <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","704d81ad":"[Go to Top](#0)","860e59f9":"Then you can remove insignificant features or decide to change the weights of the models' solutions, or you can first find out what accuracy the previously selected weights will give, and then experiment with their options.","2d9abcad":"## 7. Feature Importance - Advanced Visualization <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","9fcc4803":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","eb29ac18":"### 5.1 LGBM <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","d86e2817":"## 8. Merging solutions and submission<a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","110b8f2b":"## 6. Comparison and merging of all feature importance diagrams <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","3accb4a2":"### 7.1 Matplotlib <a class=\"anchor\" id=\"7.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","0021da9b":"<a class=\"anchor\" id=\"0\"><\/a>\n\n# The importance of all features in different models - Advanced Visualization with Matplotlib and Seaborn (parallel_coordinates)\n## Feature Importance diagrams of 3 models (XGB, LGB, LinReg) and the solution as weighted average of its\n## The code is universal for both the Classification and the Regression tasks\n### For the example of competition [\"Titanic: Machine Learning from Disaster\"](https:\/\/www.kaggle.com\/c\/titanic)"}}