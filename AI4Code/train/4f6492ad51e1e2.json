{"cell_type":{"a06e6429":"code","92c93202":"code","90d7bf54":"code","862a2e82":"code","ed8f2689":"code","02ccc18c":"code","057d9f1d":"code","785fc843":"code","f1e10a09":"code","9173996c":"code","7ea2379c":"code","fdce7813":"code","cf1824ab":"code","23ff4cbd":"code","0aa2e89b":"code","fdc3bddb":"code","df389d31":"code","fbe6f98e":"code","dcbe3375":"code","607a063f":"code","40ba6d27":"code","48e10cb3":"code","f4b5e266":"code","29e1251c":"code","6b47cde3":"code","5c50e205":"code","53645862":"code","7646bd3f":"code","68f0e3a8":"code","8c0ff1d0":"code","f86941e6":"code","6d0722cf":"code","e1ff73c6":"code","368d07fe":"code","7f9a5d38":"code","13931f8a":"code","604f237d":"code","571db792":"code","03c1d514":"code","3563cccf":"code","5b5a4700":"code","c1c6d878":"markdown","31360112":"markdown","19075a61":"markdown","e389e21b":"markdown","65aa275a":"markdown","e8b6bb65":"markdown","d9dcc177":"markdown","42ff0ed0":"markdown","c9b1c15a":"markdown","2b116b60":"markdown","d1945640":"markdown","f510aae0":"markdown","550531f9":"markdown","48d91ccc":"markdown","4b4e6472":"markdown","c2a03984":"markdown","309b04f7":"markdown","08e5b6df":"markdown","a7180b65":"markdown","234486aa":"markdown"},"source":{"a06e6429":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Flatten\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers.experimental.preprocessing import Rescaling\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.models import Model\nimport pandas as pd\nimport itertools\nimport shutil\nimport glob\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport zipfile\nfrom PIL import Image\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","92c93202":"train = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv\")","90d7bf54":"train.head()","862a2e82":"test = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv\")","ed8f2689":"train.shape","02ccc18c":"test.shape","057d9f1d":"train.iloc[1]","785fc843":"train[\"pixel1\"]","f1e10a09":"train_sample = train.loc[:499,:]\ntrain_sample.shape","9173996c":"train_sample.head","7ea2379c":"valid_sample = train.iloc[-200:,:]\nvalid_sample.shape","fdce7813":"test_sample = test.loc[:199,:]\ntest_sample.shape","cf1824ab":"type(test_sample)","23ff4cbd":"train_sample.to_csv(\"train_sample.csv\",index=False)","0aa2e89b":"test_sample.to_csv(\"test_sample.csv\",index=False)","fdc3bddb":"valid_sample.to_csv(\"valid_sample.csv\",index=False)","df389d31":"!python ..\/input\/how-to-convert-csv-to-images\/make_imgs.py --label label .\/test_sample.csv .\/mnist-imgs\/sample\/test\/ ","fbe6f98e":"!python ..\/input\/how-to-convert-csv-to-images\/make_imgs.py --label label .\/train_sample.csv .\/mnist-imgs\/sample\/train\/ ","dcbe3375":"!python ..\/input\/how-to-convert-csv-to-images\/make_imgs.py --label label .\/valid_sample.csv mnist-imgs\/sample\/valid\/\n","607a063f":"train_path = \".\/mnist-imgs\/sample\/train\"\ntest_path = \".\/mnist-imgs\/sample\/test\"\nvalid_path = \".\/mnist-imgs\/sample\/valid\"","40ba6d27":"train_btch = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n    directory=train_path, target_size=(224,224), batch_size=10)\n\nvalid_btch = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n    directory=valid_path, target_size=(224,224), batch_size=10)\n\ntest_btch = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n    directory=test_path, target_size=(224,224), batch_size=10, shuffle=False)","48e10cb3":"mobil = tf.keras.applications.mobilenet.MobileNet()","f4b5e266":"mobil.summary()","29e1251c":"take = mobil.layers[-6].output\noutput = Dense(units=24,activation=\"softmax\")(take)","6b47cde3":"model = Model(inputs=mobil.input, outputs= output)","5c50e205":"len(model.trainable_variables)","53645862":"len(model.non_trainable_variables)","7646bd3f":"for layer in model.layers[:-23]:\n    layer.trainable =False ","68f0e3a8":"len(model.trainable_variables)","8c0ff1d0":"len(model.non_trainable_variables)","f86941e6":"model.summary()","6d0722cf":"model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])","e1ff73c6":"model.fit(x=train_btch,validation_data=valid_btch,epochs=20,verbose=2)","368d07fe":"test_label = test_btch.classes","7f9a5d38":"prediction = model.predict(x=test_btch, verbose=1)","13931f8a":"cm=confusion_matrix(y_true=test_label, y_pred=prediction.argmax(axis=1))","604f237d":"test_btch.class_indices","571db792":"cm_label=[str(x) for x in range(24 + 1)]\nprint(cm_label)","03c1d514":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    \n    \n#It is directly from the documents.","3563cccf":"\nplot_confusion_matrix(cm=cm,classes=cm_label, title =\"CONFUSION MATRIX\")\n\n","5b5a4700":"from sklearn.metrics import confusion_matrix\n\nclasses=cm_label\ny_true=test_label\ny_pred=prediction.argmax(axis=1)\n\nconfusion = confusion_matrix(y_true, y_pred)\nprint('Confusion Matrix\\n')\nprint(confusion)\n\n#importing accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nprint('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_true, y_pred)))\n\nprint('Micro Precision: {:.2f}'.format(precision_score(y_true, y_pred, average='micro')))\nprint('Micro Recall: {:.2f}'.format(recall_score(y_true, y_pred, average='micro')))\nprint('Micro F1-score: {:.2f}\\n'.format(f1_score(y_true, y_pred, average='micro')))\n\nprint('Macro Precision: {:.2f}'.format(precision_score(y_true, y_pred, average='macro')))\nprint('Macro Recall: {:.2f}'.format(recall_score(y_true, y_pred, average='macro')))\nprint('Macro F1-score: {:.2f}\\n'.format(f1_score(y_true, y_pred, average='macro')))\n\nprint('Weighted Precision: {:.2f}'.format(precision_score(y_true, y_pred, average='weighted')))\nprint('Weighted Recall: {:.2f}'.format(recall_score(y_true, y_pred, average='weighted')))\nprint('Weighted F1-score: {:.2f}'.format(f1_score(y_true, y_pred, average='weighted')))\n","c1c6d878":"    And look at one of the folder. (Also, it is in the gray format, not RGB)\n\n![image.png](attachment:7bec525b-ee78-4b4c-88b9-f6dc4eb9273c.png)\n","31360112":"\n\n***I welcome your comments for any corrections and improvements.**\n\n***If we get more pictures, of course the accuracy will increase.**\n\n***Please upvote if you like it.**\n\n***Thanks! ***\n","19075a61":"# Extras:\n\n    By setting verbose 0, 1 or 2:\n\n    verbose=0 will show you nothing (silent)\n\n    verbose=1 will show you an animated progress bar like this:\n    \n ![image.png](attachment:35d93919-8fbc-4e66-80fa-e6e485a90d5f.png)\n \n     verbose=2 will just mention the number of epoch like this:\n \n![image.png](attachment:996bb39a-0e35-4b77-9e6c-96a53a52f5f6.png)","e389e21b":"<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#super\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">1<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >UNDERSTAND THE DATA<\/button>\n <\/a>\n \n \n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#super2\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">2<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >CREATE SAMPLE<\/button>\n <\/a>\n \n\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#super3\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">3<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >CONVERT DATAFRAME TO CSV\n<\/button>\n <\/a>\n\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#super4\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">4<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >CONVERT CSV TO IMAGES\n<\/button>\n <\/a>\n \n \n <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#super5\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">5<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >INDICATE THE PATH\n<\/button>\n <\/a>\n \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#super6\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">6<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >CREATE THE MODEL\n<\/button>\n <\/a>\n \n <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#super8\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">7<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >BEFORE FREEZING\n<\/button>\n <\/a>\n \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#super9\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">8<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >AFTER FREEZING\n<\/button>\n <\/a>\n \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#super10\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">0<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >Extras\n<\/button>\n <\/a>\n \n \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#super11\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">9<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >PREDICTION&RESULT\n<\/button>\n <\/a>\n ","65aa275a":"> *Sigmoid is generally used for binary classification problems.\n> \n> *The softmax function can be used for multiclass classification problems.\n> \n> MobileNet is Functional API not Sequential API.\n","e8b6bb65":"     Let's look at the data first.\n\n    We have 24 classes.\n    \n   ![image.png](attachment:edb7c8fb-3b46-4402-8114-7a42798317a9.png)","d9dcc177":"   \n        1.Add this data from the right-side. Link: https:\/\/www.kaggle.com\/zehranrgi\/how-to-convert-csv-to-images\n        2.Make sure the file path is correct. ..\/input\/how-to-convert-csv-to-images\/make_imgs.py\n        3.Run the following code.\n        Your images are ready (:\n\n        !python make_imgs.py --label label .\/sign_mnist_test\/test_sample.csv mnist-imgs\/sample\/test\/ \n        !python make_imgs.py --label label .\/sign_mnist_train\/train_sample.csv mnist-imgs\/sample\/train\/ \n        !python make_imgs.py --label label .\/sign_mnist_valid\/valid_sample.csv mnist-imgs\/sample\/valid\/\n\n   ","42ff0ed0":"> OUR DATA IS NOT AN IMAGE, THEY SHARED IMAGES WITH THEIR PIXELS in a CSV file.","c9b1c15a":"<a id =\"super3\"><\/a> \n\n# CONVERT DATAFRAME TO CSV","2b116b60":"<a id =\"super5\"><\/a> \n\n# INDICATE THE PATH","d1945640":"<a id =\"super6\"><\/a> \n\n# CREATE THE MODEL","f510aae0":"<a id =\"super\"><\/a> \n\n# UNDERSTAND THE DATA","550531f9":"<a id =\"super11\"><\/a> \n\n# PREDICTION & RESULTS","48d91ccc":"> **It is not easy to understand this matrix so that I will look at the below.**","4b4e6472":"<a id =\"super4\"><\/a> \n\n# CONVERT CSV TO IMAGES","c2a03984":"<a id =\"super8\"><\/a> \n\n# BEFORE FREEZING","309b04f7":"![image.png](attachment:47e2b388-2407-4746-910a-f97ed9c518ef.png)","08e5b6df":"<a id =\"super10\"><\/a> \n\n# Ready to Train","a7180b65":"<a id =\"super9\"><\/a> \n\n# AFTER FREEZING","234486aa":"<a id =\"super2\"><\/a> \n\n# CREATE SAMPLE"}}