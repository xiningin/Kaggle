{"cell_type":{"9cbdcd5a":"code","077eaff6":"code","9a2fc407":"code","24b7570e":"code","775f914f":"code","042a2e56":"code","d3c21c0b":"code","c3a5d6c9":"code","b71f1bbb":"code","c36a3c2b":"code","df7d0e60":"code","a71ae1aa":"code","3bc63130":"code","b7905fc5":"code","9ff6475c":"code","b66e8af2":"code","ea62e17d":"code","bb296168":"code","e706d049":"code","1f527ee4":"code","6c10325d":"code","08cb8a01":"code","592c65ab":"code","307fa047":"code","edc1974f":"code","fc816f2a":"code","58d54792":"code","c759e7ce":"code","1808b577":"code","f52aeb2d":"code","b57f78db":"code","e0a6bd92":"code","a552af27":"code","b379edeb":"code","c8394605":"code","cb4c4961":"code","e378f4b9":"code","668d31f4":"code","3f785a2e":"code","4b1897b9":"code","0ed9c712":"code","5294884d":"code","bbd2ad82":"code","e2709613":"code","9f103065":"code","e3876d3f":"code","f0fb4aae":"code","0a89df7b":"code","e1491b0b":"code","b746593c":"code","5b43c74e":"code","d93d86bd":"code","2b3f20c2":"code","d7422e4f":"code","e9ce60b9":"code","d6f275a3":"code","e2f20099":"code","07367ad0":"code","50fc5002":"code","78fb64df":"code","203b8e5d":"code","141de61f":"code","7c4478c7":"code","7c2e2393":"code","1ecd85da":"code","6b6c5be9":"code","ce056461":"code","143cf82e":"code","d50396f3":"code","59c9ae86":"code","82c6137b":"code","1be4e4bc":"code","29c651dc":"code","7cd8f706":"code","5ebea7ca":"code","cc6c55b9":"code","c9c9cc0d":"code","67bab7d8":"code","38e58710":"code","c3f9ef9f":"code","83ef2b8c":"code","657b8210":"code","d9b85440":"code","c359767a":"code","1d0cfda9":"code","2dd81aeb":"code","b1347c68":"code","e7317ee5":"code","734ae85c":"code","e8405741":"code","9ef11cea":"code","a63f6f53":"code","4eaaf361":"code","b82bdd4e":"code","f66f36ec":"code","c56f287f":"code","582dff05":"code","eaeeb7ee":"code","cb4e28f1":"code","777fd960":"code","6d9e1c38":"code","b62c516e":"code","d96e6d6f":"code","be57401a":"code","87a81743":"code","f2905008":"code","fc9d28db":"code","c1249dba":"code","d69c90a1":"code","3529c310":"code","4bb28265":"code","3518aac4":"code","ddb0d4c1":"code","3bbec89d":"code","f9580ae8":"code","a3cc4156":"code","21010419":"code","f8dea2cc":"code","69bf529d":"code","3c146d42":"code","c51e050c":"code","0a3a0672":"code","9e075653":"code","e38c881e":"code","86328c5d":"code","88f570e2":"code","91ada1c3":"code","022a0770":"code","3249495b":"code","28113416":"code","7911b16a":"code","38733e10":"code","f628da60":"code","0e206596":"code","cf57e654":"code","dc06f014":"code","4c3cbe45":"code","e5da3b6f":"code","21f5f5dd":"code","bfb4848f":"code","efac7885":"code","a3f4e73b":"code","205da2db":"code","5758fbf7":"code","07681036":"code","3ecc78bb":"code","fd0c6d85":"code","2866f0c6":"code","2438a11e":"code","fa6aebb6":"code","a836d47b":"code","c79320e1":"code","01ce61ba":"code","9bc8d582":"code","38f7fa53":"code","089e1030":"code","f014d779":"code","1e569ee7":"code","f2bc2aed":"code","76b191a4":"code","8dabfa41":"code","db7e8eb7":"code","d799f44f":"code","6475f3f0":"code","88b56073":"code","33fa270f":"code","b4fdef24":"code","9f9803b0":"code","0ca6801b":"code","41d784e7":"code","f5e8d6b0":"code","bbbac868":"code","e06e3faa":"code","8f42444a":"code","881e49d4":"code","15c9839a":"code","e183b3cc":"code","833cc2cd":"code","6446502f":"code","f4f27ce9":"code","d1428009":"code","21581f84":"code","32bbf505":"code","34902e57":"code","87774d23":"code","e9f83bdd":"code","11d6167a":"code","04888fd9":"code","9f84b929":"code","49afd8a3":"code","f6f2d8db":"code","f7e31820":"code","cd5c44e7":"code","34394abb":"code","4a7aa73a":"code","3d2793c8":"code","59301d37":"code","bfda9d87":"code","e1e12521":"code","77e4d209":"code","69e531f8":"code","038858bf":"code","a79c5d89":"code","b88c7e2b":"code","be285684":"code","cf8036c3":"code","0d141e5e":"code","8c75ee40":"code","af0b1405":"code","6fec5c1b":"code","a56c0572":"code","593904bd":"code","11a39a1f":"code","a75b965b":"code","3fc0c539":"code","096dfc69":"code","a403a31e":"code","9d71c18f":"code","7b5c506a":"code","4e3e4078":"code","a45b648c":"code","442f0995":"code","8d24436e":"code","e4860e74":"code","966a3804":"code","f07762a3":"code","fb268574":"code","d48a9858":"code","ea2c9b80":"code","44f62992":"code","181bca20":"code","40b74db6":"code","adf69e8a":"code","6ddd62f5":"code","a2d38411":"code","e9253280":"code","d55e2002":"code","634452fd":"code","a934b475":"code","befb2f16":"markdown","e561bb02":"markdown","d0dcf1f7":"markdown","bd2dbc44":"markdown","10e4c5de":"markdown","1f521d22":"markdown","3639c326":"markdown","eafc6346":"markdown","fb05db36":"markdown","f3f411f0":"markdown","93553cd9":"markdown","cc83395f":"markdown","70b07bf6":"markdown","f9f6be1b":"markdown","c164906c":"markdown","77ae69d7":"markdown","6caeb69e":"markdown","c3454f9f":"markdown","3948798f":"markdown","0e8e35fd":"markdown","69afa8a0":"markdown","c0edf50f":"markdown","e5aeaf37":"markdown","b826cba2":"markdown","f910dc22":"markdown","dcd4d7cd":"markdown","3ba8f1b9":"markdown","72a1bd39":"markdown","93ca9e3c":"markdown","a73531d1":"markdown","2ee210d5":"markdown","acd242e2":"markdown","8cb45956":"markdown","1f546f45":"markdown","571ec390":"markdown","25663bc6":"markdown","3a55864c":"markdown","65f2b857":"markdown","77dd370b":"markdown","35e83e0e":"markdown","683f1eb5":"markdown","ef0ed22c":"markdown","893bcc4f":"markdown","037c3d04":"markdown","f5914c17":"markdown","e5370f1b":"markdown","cd1000ee":"markdown","2e16d017":"markdown","bbefcce0":"markdown","08bf839f":"markdown","da5991cf":"markdown","93b53925":"markdown","414b682d":"markdown","aed231e9":"markdown","4782847f":"markdown","651c074c":"markdown","2062595f":"markdown","c1dda295":"markdown","98151a8e":"markdown","d5c48063":"markdown","ef157ac0":"markdown","43333d10":"markdown","7f5adf67":"markdown","32554fea":"markdown","8bba09b9":"markdown","6c87ea88":"markdown","4275db88":"markdown","2648a606":"markdown","99d2af36":"markdown","3cffc7d7":"markdown","2c4ff00c":"markdown","b2b3e400":"markdown","4dc561fb":"markdown","1d17e2f0":"markdown","0939f80c":"markdown","c7099b7a":"markdown","6c42b392":"markdown","f4e4e449":"markdown","5c80926d":"markdown","7aaaee60":"markdown","5a96f9e7":"markdown","4582dd27":"markdown","ac97275a":"markdown","683ee8fa":"markdown","34f7b72f":"markdown","c8e7c7f7":"markdown","8389bee8":"markdown","02f55917":"markdown","fead2b69":"markdown","ee4801d3":"markdown","5fb21681":"markdown","8642f4cc":"markdown","fdb708fd":"markdown","9b2d65b3":"markdown","fb0b8068":"markdown","93da988d":"markdown","a8070f90":"markdown","26d2b27f":"markdown","dbafdfb1":"markdown"},"source":{"9cbdcd5a":"from IPython.display import Image\n","077eaff6":"Image(\"..\/input\/images2\/images\/images\/what_is_ml.png\")","9a2fc407":"Image(\"..\/input\/images2\/images\/images\/types_oh_ml.png\")","24b7570e":"Image(\"..\/input\/images2\/images\/images\/types_of_ml.png\")","775f914f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn\nimport scipy \n\nimport xgboost\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","042a2e56":"!pip install --upgrade git+https:\/\/github.com\/scikit-learn-contrib\/categorical-encoding","d3c21c0b":"import category_encoders","c3a5d6c9":"!pip install watermark","b71f1bbb":"%load_ext watermark","c36a3c2b":"%watermark --iversions","df7d0e60":"## Basic numpy:\n# 1d:\na = np.array([0, 1, 2, 3])\nprint(\"a = \", a)","a71ae1aa":"# 2x3 array:\nb = np.array([[0, 1, 2], [3, 4, 5]])    # 2 x 3 array\nprint(\"b = \", b)\n# https:\/\/scipy-lectures.org\/intro\/numpy\/array_object.html#what-are-numpy-and-numpy-arrays\n    ","3bc63130":"a.shape","b7905fc5":"b.shape","9ff6475c":"a.reshape(4,1)","b66e8af2":"a","ea62e17d":"a.shape","bb296168":"a = a.reshape(4,1)","e706d049":"a","1f527ee4":"a = np.arange(10) # evenly spaced\nprint(a)","6c10325d":"np.linspace(0, 1, 10) # Start, end, Number of points","08cb8a01":"?np.linspace # Get help on how to call the function!","592c65ab":"Image(\"..\/input\/images2\/images\/images\/numpy_slicing.png\")","307fa047":"# Indexing\na = np.arange(10) # evenly spaced\na[2:9:3] # [start:end:step]","edc1974f":"a[:4] # last index is not included.  Default start is 0, end is last, step is 1","fc816f2a":"a[::-1] # Can easily reverse!","58d54792":"Image(\"..\/input\/images2\/images\/images\/numpy_fancy_indexing.png\")","c759e7ce":"Image(\"..\/input\/images2\/images\/images\/pandas_cheetsheet.png\")","1808b577":"school_data = pd.read_csv('..\/input\/xAPI-Edu-Data\/xAPI-Edu-Data.csv') # pandas has lots of ways to read in data!","f52aeb2d":"school_data.head() #shows top 5 lines","b57f78db":"Image(\"..\/input\/images2\/images\/images\/xapi_data_attributes.png\")","e0a6bd92":"Image(\"..\/input\/images2\/images\/images\/xapi_class_outputs.png\")","a552af27":"type(school_data) # get the type of data","b379edeb":"type(school_data.gender)","c8394605":"school_data.iloc[0,3:5] #just like numpy indexing","cb4c4961":"school_data.columns","e378f4b9":"print(school_data.StageID[0])\nprint(school_data['StageID'][0])\nprint(school_data.loc[0, 'StageID'])\nprint(school_data.iloc[0, 3])\n\nprint(\"Caution: Indexing slightly differently will return a Series: school_data.iloc[0, 3:4]\")\nprint(school_data.iloc[0, 3:4])","668d31f4":"type(school_data.iloc[0, 3])","3f785a2e":"type(school_data.iloc[0, 3:4])","4b1897b9":"# Filter the data:\nschool_data[(school_data.gender=='F')].head() ","0ed9c712":"# Multiple filters\nschool_data[(school_data.gender=='F') & (school_data.Topic=='IT')].head() ","5294884d":"school_data[(school_data.gender=='F')].loc[5:9,:]","bbd2ad82":"school_data[(school_data.gender=='F')].iloc[0:3,:]","e2709613":"# Count the values in a column:\nschool_data.gender.value_counts()","9f103065":"# Group by and count:\nschool_data.groupby(['StageID', 'Topic']).count()\n","e3876d3f":"# Since we are just counting the number of rows, select one of them and sort \nschool_data.groupby(['StageID', 'Topic']).gender.count().sort_values(ascending=False)\n","f0fb4aae":"school_data.columns","0a89df7b":"# Get statistics on numeric column and sort by the mean of raisedhands\nschool_data.groupby(['StageID', 'Topic']).raisedhands.describe().sort_values(by='mean', ascending=False)","e1491b0b":"# Exercise:\nschool_data[(school_data.StageID=='MiddleSchool')].groupby('Topic').count()\nschool_data[(school_data.StageID=='MiddleSchool')].Topic.value_counts()","b746593c":"school_data.info(verbose=True, null_counts=True) # get information about the dataframe, how many nulls, and datatype","5b43c74e":"school_data.columns","d93d86bd":"## My method of taking an initial glance at the columns:\nfor each_column in school_data.columns:\n    print(\"Column = \", each_column)\n    print(school_data[each_column].describe()) #describe gives summary stats for numeric columns and count\/unique\/top value for strings\n    if school_data[each_column].nunique() < 50: # nunique get the number of unique values for the column\n        print(\"Counts of {} :\".format(each_column)) # can use format to format the string\n        print(school_data[each_column].value_counts())\n    print()","2b3f20c2":"## New packages, pandas-profiling and pandas_summary","d7422e4f":"school_data.shape # 480 rows and 17 columns","e9ce60b9":"# Exercise:\nschool_data.max()","d6f275a3":"from pandas_summary import DataFrameSummary # https:\/\/github.com\/mouradmourafiq\/pandas-summary","e2f20099":"school_data.columns","07367ad0":"## Explore with pandas","50fc5002":"dfs = DataFrameSummary(school_data)","78fb64df":"dfs.columns_types #one step further than pandas.info()","203b8e5d":"dfs.columns_stats","141de61f":"import pandas_profiling","7c4478c7":"pandas_profiling.ProfileReport(school_data)","7c2e2393":"%matplotlib inline","1ecd85da":"sns.set(style=\"ticks\")\n\n# Load the example dataset for Anscombe's quartet\ndf = sns.load_dataset(\"anscombe\") #same sample statistics, and regression line for 4 different datasets\n\n# Show the results of a linear regression within each dataset\nsns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\", data=df,\n           col_wrap=2, ci=None, palette=\"muted\", height=4,\n           scatter_kws={\"s\": 50, \"alpha\": 1})","6b6c5be9":"# Seaborn : http:\/\/seaborn.pydata.org\/index.html\nsns.set(style=\"ticks\")\nsns.pairplot(school_data, hue=\"Class\")\nplt.show()","ce056461":"school_data.info()","143cf82e":"for each in school_data.columns:\n    if school_data[each].dtype=='object':\n        sns.catplot(x=each, y=\"raisedhands\", hue=\"Class\", kind=\"swarm\", data=school_data)\n        plt.show()","d50396f3":"# Exercise:","59c9ae86":"school_data.columns","82c6137b":"school_data.gender.value_counts().plot(kind='bar') # Quick view of the data:","1be4e4bc":"school_data.AnnouncementsView.plot(kind='hist')","29c651dc":"school_data.AnnouncementsView.hist() # Another way to quickly plot","7cd8f706":"Image(\"..\/input\/images2\/images\/images\/types_of_ml.png\")","5ebea7ca":"Image(\"..\/input\/images2\/images\/images\/bias_variance_tradeoff_reg.png\")\n# Source: http:\/\/scott.fortmann-roe.com\/docs\/BiasVariance.html","cc6c55b9":"Image(\"..\/input\/images2\/images\/images\/overfitting_under_classification.png\")","c9c9cc0d":"school_data.info()","67bab7d8":"school_data.GradeID.head(10)","38e58710":"pd.get_dummies(school_data['GradeID']).head(10)","c3f9ef9f":"school_data.GradeID.head(10)","83ef2b8c":"school_data_dummies_df = pd.get_dummies(school_data)","657b8210":"school_data_dummies_df.head()","d9b85440":"school_data_dummies_df.corr()","c359767a":"corr = school_data_dummies_df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","1d0cfda9":"import seaborn as sns\ncorr = school_data_dummies_df.corr()\nsns.set(rc={'figure.figsize':(18,13)})\n#sns.heatmap(corr, \n #           xticklabels=corr.columns.values,\n #           yticklabels=corr.columns.values) # original didnt show colors that made it easy to see the correlations\n\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap='coolwarm', vmax=1, center=0,\n            square=True, linewidths=.5,  xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\n\n# https:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html","2dd81aeb":"from sklearn import preprocessing","b1347c68":"ohe = sklearn.preprocessing.OneHotEncoder() # create a one hot encoding object","e7317ee5":"ohe.get_feature_names()","734ae85c":"ohe_data = ohe.fit_transform(school_data) # fit_transform, fit will modify the ohe object","e8405741":"ohe","9ef11cea":"# Get the feature names:\nohe.get_feature_names()","a63f6f53":"ohe_data.shape","4eaaf361":"ohe.get_feature_names()","b82bdd4e":"school_data.columns","f66f36ec":"school_data.head()","c56f287f":"Image(\"..\/input\/images2\/images\/images\/wht_is_a_cluster.png\")","582dff05":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder","eaeeb7ee":"subset_features = ['GradeID','StudentAbsenceDays', 'raisedhands',\n       'VisITedResources', 'AnnouncementsView', 'Discussion']\n\nnumeric_features = ['raisedhands',  'VisITedResources', 'AnnouncementsView', 'Discussion']\n\ncategorical_features = ['GradeID', 'StudentAbsenceDays']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_features), ('nothing', 'passthrough', numeric_features)])\n","cb4e28f1":"ohe_data = preprocessor.fit_transform(school_data[subset_features])","777fd960":"ohe_data.shape","6d9e1c38":"preprocessor.named_transformers_['onehot'].get_feature_names() #Get the feature names of the transformer called ohe_transformer","b62c516e":"ohe_features = list(preprocessor.named_transformers_['onehot'].get_feature_names()) + list(numeric_features)","d96e6d6f":"ohe_features","be57401a":"ohe_data.shape","87a81743":"ohe_data","f2905008":"numeric_school_data = school_data[numeric_features]","fc9d28db":"numeric_school_data.mean(axis=0)","c1249dba":"Image(\"..\/input\/images2\/images\/images\/unevenly_sized_kmeans.png\")","d69c90a1":"Image(\"..\/input\/images2\/images\/images\/kmeans_nonspherical.png\")\n","3529c310":"from sklearn import cluster","4bb28265":"from sklearn import decomposition","3518aac4":"ss = sklearn.preprocessing.StandardScaler()","ddb0d4c1":"ss_data = ss.fit_transform(numeric_school_data)","3bbec89d":"numeric_school_data.shape","f9580ae8":"ss.mean_","a3cc4156":"ss_data.mean(axis=0)","21010419":"kmeans_scaled = sklearn.cluster.KMeans(n_clusters=3, random_state=42, n_jobs=-1)","f8dea2cc":"kclusters_scaled_only = kmeans_scaled.fit_predict(ss_data)","69bf529d":"kmeans_scaled.cluster_centers_.shape #cluster_centers_ attribute has values for the cluster centers, 3 clusters x 4 features","3c146d42":"pd.Series(kclusters_scaled_only).value_counts() # How many in each cluster","c51e050c":"pd.Series(kmeans_scaled.labels_).value_counts() # Alternative way to find cluster labels","0a3a0672":"school_data[kclusters_scaled_only==0][numeric_features].head(20)","9e075653":"dict(zip(numeric_features, kmeans_scaled.cluster_centers_[0]))","e38c881e":"# Need to look at what the data looked like before it was scaled\ndict(zip(numeric_features, ss.inverse_transform(kmeans_scaled.cluster_centers_[0]))) \n","86328c5d":"dict(zip(numeric_features, ss.inverse_transform(kmeans_scaled.cluster_centers_[1]))) ","88f570e2":"dict(zip(numeric_features, ss.inverse_transform(kmeans_scaled.cluster_centers_[2]))) ","91ada1c3":"Image(\"..\/input\/images2\/images\/images\/clustering_metrics.png\")","022a0770":"Image(\"..\/input\/images2\/images\/images\/clustering_algos.png\")","3249495b":"Image(\"..\/input\/images2\/images\/images\/curse_of_dimensionality.png\")","28113416":"Image(\"..\/input\/images2\/images\/images\/setosa_pca_pic.png\")","7911b16a":"svd = sklearn.decomposition.TruncatedSVD(n_components=2, random_state=42)","38733e10":"ss_data.shape","f628da60":"svdschool = svd.fit_transform(ss_data)","0e206596":"svd.explained_variance_ratio_.sum() # With our 2 components, we have accounted for 83% of the variability","cf57e654":"svdschool.shape # number of data points x number of dimensions in our lower dim space","dc06f014":"svd.components_.shape # each new component as a linear combination of the original space ","4c3cbe45":"kmeans = sklearn.cluster.KMeans(n_clusters=3, random_state=42, n_jobs=-1)","e5da3b6f":"kclusters_dimreduced = kmeans.fit_predict(svdschool)","21f5f5dd":"pd.Series(kclusters_dimreduced).value_counts()\n","bfb4848f":"school_data[kclusters_dimreduced==0][numeric_features].head()","efac7885":"kmeans.cluster_centers_ # cluster centers, 3 clusters x 2 dimensions in SVD space","a3f4e73b":"svd_centroid = svd.inverse_transform(kmeans.cluster_centers_) # Just like before, we want to know what the center is in our original feature space","205da2db":"svd_centroid.shape","5758fbf7":"dict(zip(numeric_features, ss.inverse_transform(svd_centroid[0])))","07681036":"dict(zip(numeric_features, ss.inverse_transform(svd_centroid[1])))","3ecc78bb":"dict(zip(numeric_features, ss.inverse_transform(svd_centroid[2])))","fd0c6d85":"ss_data.shape","2866f0c6":"svdschool.shape","2438a11e":"pd.Series(kclusters_scaled_only).nunique()","fa6aebb6":"plt.figure(figsize=(8, 6))\nplt.scatter(svdschool[:,0], svdschool[:,1], c=kclusters_scaled_only.astype(float), edgecolor='none', alpha=0.5,\n            cmap=plt.cm.get_cmap('rainbow', pd.Series(kclusters_scaled_only).nunique()))\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.title(\"2d Visualization of Numeric School Data - KMEANS on Scaled Data (4 Dimensions )\")\nplt.colorbar()\nplt.show()","a836d47b":"plt.figure(figsize=(8, 6))\nplt.scatter(svdschool[:,0], svdschool[:,1], c=kclusters_dimreduced.astype(float), edgecolor='none', alpha=0.5,\n            cmap=plt.cm.get_cmap('rainbow', pd.Series(kclusters_dimreduced).nunique()))\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.title(\"2d Visualization of Numeric School Data - KMEANS on Reduced Dimensional Data (2 Dimensions)\")\nplt.colorbar()\nplt.show()","c79320e1":"school_data.Class.values","01ce61ba":"class_vals = school_data.Class.map({'L': 0, 'M': 1, 'H': 2})\n#le = sklearn.preprocessing.OrdinalEncoder(categories=np.array(['L', 'M', 'H']) )\n#le.fit(np.array(['L', 'M', 'H']).reshape(-1,1))\n#class_vals = le.fit_transform(school_data.Class.values.reshape(-1,1))\n\n# LabelEncoder and OrdinalEncoder didn't let me set the order, so used basic pandas instead :)\n","9bc8d582":"plt.figure(figsize=(8, 6))\nplt.scatter(svdschool[:,0], svdschool[:,1], c=class_vals.astype(float), edgecolor='none', alpha=0.5,\n            cmap=plt.cm.get_cmap('rainbow', school_data.Class.nunique()))\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.title(\"2d Visualization of Numeric Data Colored by True Class Labels\")\nplt.colorbar()\nplt.show()","38f7fa53":"Image(\"..\/input\/images2\/images\/images\/clustering_metrics.png\")","089e1030":"Image(\"..\/input\/images2\/images\/images\/categorical_encoders.png\")","f014d779":"Image(\"..\/input\/images2\/images\/images\/mean_encoding.png\")","1e569ee7":"ce = category_encoders.TargetEncoder()","f2bc2aed":"# Go through the kmeans clustering again with the target encoding","76b191a4":"school_data_target_encoded_df = ce.fit_transform(school_data.drop('Class', axis=1), class_vals)","8dabfa41":"school_data_target_encoded_df.gender.nunique()","db7e8eb7":"school_data.head(20)","d799f44f":"school_data_target_encoded_df.head(20)","6475f3f0":"Image(\"..\/input\/images2\/images\/images\/classifier_comparison.png\")","88b56073":"from sklearn import model_selection","33fa270f":"X_train, X_test, y_train, y_test = model_selection.train_test_split(school_data.drop('Class', axis=1), \n                                                            class_vals, test_size=0.25, random_state = 42, stratify=class_vals)\n","b4fdef24":"pd.Series(y_train).value_counts() #Check class imbalance","9f9803b0":"ce = category_encoders.TargetEncoder() # Create TargetEncoder object","0ca6801b":"X_train_numeric = ce.fit_transform(X_train, y_train) # Transform our categorical data","41d784e7":"X_train_numeric.head()","f5e8d6b0":"ss = preprocessing.StandardScaler()","bbbac868":"X_train_scaled = ss.fit_transform(X_train_numeric) # Scale our data","e06e3faa":"X_train_scaled.shape ","8f42444a":"from sklearn import dummy # Make dummy clissifier as baseline","881e49d4":"dummy = sklearn.dummy.DummyClassifier(random_state=42)","15c9839a":"dummy.fit(X_train_scaled, y_train) # Fit the model","e183b3cc":"dummy_predictions = dummy.predict(X_train_scaled) # Make predictions","833cc2cd":"dummy.score(X_train_scaled, y_train) # Score our training data set for now","6446502f":"Image(\"..\/input\/images2\/images\/images\/truepos_trueneg_wiki.png\")","f4f27ce9":"Image(\"..\/input\/images2\/images\/images\/precision_recall.png\")","d1428009":"Image(\"..\/input\/images2\/images\/images\/roc_curve_pic.png\")","21581f84":"from sklearn import metrics","32bbf505":"print(metrics.classification_report(dummy_predictions, y_train))","34902e57":"metrics.confusion_matrix(dummy_predictions, y_train)","87774d23":"metrics.accuracy_score(dummy_predictions, y_train)","e9f83bdd":"Image(\"..\/input\/images2\/images\/images\/pipelines.png\")","11d6167a":"Image(\"..\/input\/images2\/images\/images\/cross_validation.png\")","04888fd9":"from sklearn.pipeline import Pipeline","9f84b929":"preprocessing_pipeline = Pipeline([('cat_encoder', category_encoders.TargetEncoder(return_df=False)), \n                                   ('scaler', preprocessing.StandardScaler())])","49afd8a3":"from sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.metrics import accuracy_score, f1_score ","f6f2d8db":"sklearn.metrics.SCORERS","f7e31820":"from sklearn.pipeline import make_pipeline","cd5c44e7":"Image(\"..\/input\/images2\/images\/images\/stratified_k_fold.png\")","34394abb":"preprocessing_pipeline = Pipeline([('cat_encoder', category_encoders.TargetEncoder(return_df=False)), \n                                   ('scaler', preprocessing.StandardScaler())])","4a7aa73a":"lr_pipeline = make_pipeline(preprocessing_pipeline,\n                         sklearn.linear_model.LogisticRegression(class_weight='balanced', random_state=42, solver='sag', multi_class='multinomial', n_jobs=1))\nscoring = ['balanced_accuracy', 'f1_weighted']\nlr_cv_scores = cross_validate(lr_pipeline, X_train, y_train, scoring=scoring, cv=10, return_train_score=False)","3d2793c8":"lr_cv_scores","59301d37":"for key in lr_cv_scores.keys():\n    print(\"%s: %0.2f (+\/- %0.2f)\" % (key, lr_cv_scores[key].mean(), lr_cv_scores[key].std() * 2))","bfda9d87":"def cross_validate_and_print(preprocessing_pipeline, model_type):\n    cv=10\n    pipeline = make_pipeline(preprocessing_pipeline, model_type)\n    print(\"Preprocessing Steps : \", preprocessing_pipeline)\n    print(\"Model Type : \", model_type)\n    scoring = ['balanced_accuracy', 'f1_weighted']\n    cv_scores = cross_validate(pipeline, X_train, y_train, scoring=scoring, cv=cv, return_train_score=False)\n    print(\"Running {} Fold Cross Validation :\".format(cv))\n    for key in cv_scores.keys():\n        print(\"%s: %0.2f (+\/- %0.2f)\" % (key, cv_scores[key].mean(), cv_scores[key].std() * 2))","e1e12521":"cross_validate_and_print(preprocessing_pipeline=preprocessing_pipeline, \n        model_type=sklearn.linear_model.LogisticRegression(class_weight='balanced', random_state=42, \n                                                           solver='sag', multi_class='multinomial', n_jobs=1))","77e4d209":"#Exercise:","69e531f8":"Image(\"..\/input\/images2\/images\/images\/tree_terminology.png\")","038858bf":"Image(\"..\/input\/images2\/images\/images\/tree_pruning.png\")","a79c5d89":"Image(\"..\/input\/images2\/images\/images\/decision_tree_wiki.png\")","b88c7e2b":"Image(\"..\/input\/images2\/images\/images\/dtc_variables.png\")","be285684":"Image(\"..\/input\/images2\/images\/images\/random_forest_simplified.png\")","cf8036c3":"Image(\"..\/input\/images2\/images\/images\/linreg_vs_xgboost.png\")","0d141e5e":"Image(\"..\/input\/images2\/images\/images\/ensembling_pic.png\")","8c75ee40":"Image(\"..\/input\/images2\/images\/images\/bagging_vs_boosting.png\")","af0b1405":"from sklearn import tree","6fec5c1b":"preprocessing_pipeline = Pipeline([('cat_encoder', category_encoders.TargetEncoder(return_df=False)), \n                                   ('scaler', preprocessing.StandardScaler())])","a56c0572":"cross_validate_and_print(preprocessing_pipeline, tree.DecisionTreeClassifier(random_state=42, class_weight='balanced'))","593904bd":"from sklearn import ensemble","11a39a1f":"forest = sklearn.ensemble.RandomForestClassifier(n_estimators=100, max_depth=10, \n                                                 n_jobs=-1, random_state=42, class_weight='balanced_subsample')\n","a75b965b":"X_train.shape","3fc0c539":"X_train_ss = preprocessing_pipeline.fit_transform(X_train, y_train)","096dfc69":"forest.fit(X_train_ss, y_train)","a403a31e":"forest.feature_importances_","9d71c18f":"forest.estimators_[0] # Can access each of the decision tree estimators - we have 100 - can use this for visualization","7b5c506a":"importances = forest.feature_importances_\n\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train_ss.shape[1]):\n    print(\"%d. feature %d = %s (%f)\" % (f + 1, indices[f], X_train.columns[indices[f]], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X_train_ss.shape[1]), importances[indices],\n       color=\"r\", align=\"center\")\nplt.xticks(range(X_train_ss.shape[1]), indices )\nplt.xlim([-1, X_train_ss.shape[1]])\nplt.show()","4e3e4078":"print(metrics.classification_report(forest.predict(X_train_ss), y_train))  # What an awesome classifier! ?","a45b648c":"rfc = ensemble.RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42, class_weight='balanced_subsample')","442f0995":"cross_validate_and_print(preprocessing_pipeline, rfc)","8d24436e":"rfc = ensemble.RandomForestClassifier(n_estimators=100, max_depth=3, n_jobs=-1, random_state=42, min_samples_split=5, class_weight='balanced_subsample')\ncross_validate_and_print(preprocessing_pipeline, rfc)","e4860e74":"import xgboost as xgb","966a3804":"xgb.XGBClassifier()","f07762a3":"Image(\"..\/input\/images2\/images\/images\/xgboost_learning_task_parameter.png\")","fb268574":"# Fit the model\nxgb_model = xgb.XGBClassifier(max_depth=5, random_state=42, \n                              subsample=0.8, colsample_bytree=0.8, n_jobs=-1, objective='multi:softmax').fit(X_train_ss, y_train)","d48a9858":"# feature importance\nprint(xgb_model.feature_importances_)","ea2c9b80":"from xgboost import plot_importance","44f62992":"plot_importance(xgb_model)","181bca20":"sorted_idx = np.argsort(xgb_model.feature_importances_)[::-1]\nfor index in sorted_idx:\n    print([X_train.columns[index], xgb_model.feature_importances_[index]]) \n","40b74db6":"xgbc = xgb.XGBClassifier(n_estimators=20, max_depth=3, random_state=42, subsample=0.8, colsample_bytree=0.8, \n                                           n_jobs=-1, objective='multi:softmax')","adf69e8a":"cross_validate_and_print(preprocessing_pipeline, xgbc)","6ddd62f5":"final_model = xgb.XGBClassifier(n_estimators=20, max_depth=3, random_state=42, subsample=0.8, colsample_bytree=0.8, \n                                           n_jobs=-1, objective='multi:softmax')","a2d38411":"final_model_pipeline = make_pipeline(preprocessing_pipeline, final_model)","e9253280":"final_model_pipeline.fit(X_train, y_train)","d55e2002":"final_model_pipeline.score(X_test, y_test)","634452fd":"y_test_predictions = final_model_pipeline.predict(X_test)","a934b475":"print(metrics.classification_report(y_test_predictions, y_test))","befb2f16":"Caution: Keep track of python packages using docker (kaggle does this), conda environments, virtualenv, or with watermark:  https:\/\/github.com\/rasbt\/watermark#installation-and-updating\n\nCaution: Important to remember reproducible code and research!\n\nCaution: Check internet connection for pip install","e561bb02":"Exercise: Try also reducing the dimensions to 10 and see how LR changes.  (Hint: add TruncatedSVD to preprocessing_pipeline)","d0dcf1f7":"### Decision Trees\n- One tree\nhttps:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning\n\nResources:\n- https:\/\/www.analyticsvidhya.com\/blog\/2016\/04\/complete-tutorial-tree-based-modeling-scratch-in-python\/\n- https:\/\/bricaud.github.io\/personal-blog\/entropy-in-decision-trees\/\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\npic\n","bd2dbc44":"Caution: We do not have any missing data, need to clean up if we did with imputing or dealing with rows\/columns.\n\nResource: Preprocessing - https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html","10e4c5de":"#### Logistic Regression:\n* Good one to start with\n* Fast to train and score","1f521d22":"Maybe our clustering would have been better if we used all our data and not just 4 numeric features.\n\nJust like KMeans has KModes for categorical, PCA on categoricals can be done with MCA (Multiple Correspondance Analysis)\n\nResource: \nhttps:\/\/github.com\/MaxHalford\/Prince (sklearn compatible package for categorical analysis)","3639c326":"Since we want to try out a bunch of models, lets make a function:","eafc6346":"Our dataset is mostly categorical - to leverage all of the data, we could use kmodes.\n\nResource: https:\/\/github.com\/nicodv\/kmodes","fb05db36":"### Random Forest\n- Forest of trees!\n- Also called 'ensemble' method - ensembling multiple decision trees\n- Randomly selects points to build each tree (can spcify with or without replacement)\n- Can use a subset of the features to reduce overfitting\n\n","f3f411f0":"Python packages used:\nnumpy, scipy, pandas, sklearn, xgboost\n\nHighlights of others:\nh2o - huge set of algos including AutoML (newly opensourced)\nLightGBM\ncatboost\n\ntext:\ngensim\nnltk\nspacy\n\nGraphs:\nnetworkx\n\nFeature engineering:\nhttps:\/\/www.featuretools.com\/\nhttps:\/\/towardsdatascience.com\/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159\nEncoding - https:\/\/github.com\/scikit-learn-contrib\/categorical-encoding\n\nImbalanced Learning:\nhttps:\/\/imbalanced-learn.org\/en\/stable\/index.html\n\nPython Visualization:\nmatplotlib\nseaborn\naltair\nholoviews\nbokeh\nplot.ly \/ Dash\n\nClustering:\nHDBSCAN\n\nHyperparameter Optimization:\nHyperopt\ntpot\nspearmint\n\nGeneral:\nhttps:\/\/github.com\/scikit-learn-contrib\n\n","93553cd9":"   Use min_samples_lead, min_samples_split, max_features to not overfit.","cc83395f":"Exercise: What columns look redundant?","70b07bf6":"Note how close the evaluation metrics are to Random Forest - how many estimators did we use? Go back and compare the score_times.","f9f6be1b":"Clustering metric exercise in extra credit section.\n\nResources:\n- https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#clustering-evaluation\n- https:\/\/scikit-learn.org\/stable\/modules\/classes.html#clustering-metrics\n\n\n","c164906c":"#### Example of preprocessing data with sklearn\nPrimarily done with \"fit_transform\"\n","77ae69d7":"## Additional Packages (My favorites)","6caeb69e":"What does this score mean?\n\nCaution: Scoring on the training data set will be better than scoring on validation or test set.\n\nCaution: Model evaluation - how to evaluate? Accuracy? Never only use it and \u201cif it seems too good to be true, it probably is\u201d .\n\n* e-book on Evaluating Machine Learning Models: https:\/\/www.oreilly.com\/ideas\/evaluating-machine-learning-models\n\nResource:\nhttps:\/\/machinelearningmastery.com\/classification-accuracy-is-not-enough-more-performance-measures-you-can-use\/\n\n\nSource: https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall","c3454f9f":"### Tree Creation\n- Maximum class separation\n- Pruning\n\nResources:\n- https:\/\/www.analyticsvidhya.com\/blog\/2016\/04\/complete-tutorial-tree-based-modeling-scratch-in-python\/\nhttps:\/\/medium.com\/deep-math-machine-learning-ai\/chapter-4-decision-trees-algorithms-b93975f7a1f1\n","3948798f":"#### Missing Values","0e8e35fd":"## Pandas\n- Part of NumFOCUS\n- Used for python data analysis\n\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/master\/doc\/cheatsheet\/Pandas_Cheat_Sheet.pdf\n\nhttps:\/\/numfocus.org\/sponsored-projects","69afa8a0":"Resource: \nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/improve-model-performance-cross-validation-in-python-r\/\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate","c0edf50f":"## KMeans Clustering\n\nCaution: Need to scale the data for kmeans to work!\nCaution: Make sure to look at the details of your algorithm to identify the assumpions of the expected input data\n\nKMeans: \n- shperical clusters\n- evenly sized clusters\n- need to provide K\n\nResources:\n- https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_assumptions.html\n- https:\/\/hdbscan.readthedocs.io\/en\/latest\/comparing_clustering_algorithms.html\n- https:\/\/stats.stackexchange.com\/questions\/89809\/is-it-important-to-scale-data-before-clustering\n\n\npic from http:\/\/bioinformaticsinstitute.ru\/sites\/default\/files\/preprocessing_unsupervised.pdf\npics from https:\/\/www.r-bloggers.com\/k-means-clustering-is-not-a-free-lunch\/\n\n\n","e5aeaf37":"Exercise: Go through and redo the exercise of reducing the dimensions of this new data and doing clustering - how do the clusters look? ","b826cba2":"\nResource: \n- https:\/\/github.com\/scikit-learn-contrib\/categorical-encoding\n- Video: https:\/\/www.coursera.org\/lecture\/competitive-data-science\/concept-of-mean-encoding-b5Gxv","f910dc22":"Difficult to evaluate, but there are clustering evaluation metrics out there.","dcd4d7cd":"## Extra Credit Assignments\n\nExtra Credit Cluster:\n* Since we have Class labels for our data - go through different clustering algorithms and evaluation metrics to find an approach that clusters our data well\n* Rerun Kmeans and evaluate, better with 5 clusters? better with 10 clusters? Evaluation of your choice\n* Use https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score metric to evaluate which approach gives us a better score, kmeans with pca or kmeans without scaling?\nResources:\nhttps:\/\/scikit-learn.org\/stable\/modules\/clustering.html\nhttps:\/\/scikit-learn.org\/stable\/modules\/clustering.html#clustering-performance-evaluation\n\nExtra Credit LDA:\n* Use LDA algorithm to reduce the dimension of the data using the Class labels and see how the performance of the tree based models changes with this transformed data\nResources:\nhttps:\/\/stackabuse.com\/implementing-lda-in-python-with-scikit-learn\/\n\nExtra Credit Trees:\n* Try building a decision tree, random forest, and GBT and reduce the dimensions to 5 using TruncatedSVD, how does this change the cross-validation accuracy?\n* How does removing scaling change the cross validation accuracy of Random Forest? or Decision Tree?\n","3ba8f1b9":"### Model Selection\n* Can run through many ML algorithms\n* Loop through hyperparamters\n* Where's my validation set? \n\nResource: https:\/\/machinelearningmastery.com\/difference-test-validation-datasets\/\n\n* Even better - cross-validation!\n\nCaution:  The purpose of cross-validation is model selection, not model building!\n\n### Pipelines\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\n- Pipelines make it easy to store \"steps\" - preprocessing steps, encoding, model itself - easy to call the pipeline to transform our validation set in the same way\n* Caution: Preprocessing all the training data once prior to doing cross validation is a form of 'leakage' - where part of the solution has leaked into the training data,\n\n\n","72a1bd39":"# Women in Analytics 2019 Workshop\n## March 22, 2019\n## Author: Jaya Zenchenko, @datanerd_jaya\n## https:\/\/www.linkedin.com\/in\/jayazenchenko\/\n## Machine Learning with Python\n\nAgenda:\n* Intro\n* What is machine learning?\n* Python package highlights\n* Quickest intro to numpy, scipy\n* Exploratory Data Analysis (EDA) with Pandas\n* Unsupervised Learning, Supervised Learning\n* Model evaluation, cross-validation\n* Wrap Up\n","93ca9e3c":"### Gradient Boosted Trees\n* Also builds many trees, but learns from the mistake of the previous tree\n* Can use a subset of the features and subset of the data to reduce overfitting\n\nCLICK ME: http:\/\/arogozhnikov.github.io\/2016\/07\/05\/gradient_boosting_playground.html\n\nhttps:\/\/medium.com\/mlreview\/gradient-boosting-from-scratch-1e317ae4587d\n\nResource: Excellent  explanation of each paramter for XGBoost: https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/","a73531d1":"Lets look at a subset of our data to dig into clustering.  To do similar one hot encoding as pandas get_dummies, need to use ColumnTransformer.","2ee210d5":"What does this mean?","acd242e2":"Exercise: Identify the maximum value for all the numerical columns. Which categorical column has the most unique values? ","8cb45956":"Let's visualize how well the clusters look when colored by the Class (L, M, H).","1f546f45":"## Dimensionality Reduction\n* Reduce the dimensions of your input data\n* Remove multi-collinearity\n* Some approaches are: PCA, LDA, SVD, t-SNE, etc\n\nCaution: Some ML algorithms need the data to be non-collinear (i.e. generalized linear models) make sure to check and remove multi-collinearity!  \nCaution: Some dimensionality reduction techniques find linear relationships and others find non-linear relationships\n\nResources:\n- http:\/\/setosa.io\/ev\/principal-component-analysis\/\n- https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py\n- https:\/\/www.analyticsvidhya.com\/blog\/2018\/08\/dimensionality-reduction-techniques-python\/","571ec390":"Caution: Calling \"fit\" or \"fit_transform\" on object \"ohe\" will modify the metadata!","25663bc6":"#### Split the data","3a55864c":"KMeans groups points together in the 16 dimensional space using Eucliean distance - anything concerning about this data set?","65f2b857":"Exercise: Try without scaling, does this change the cross-validation accuracy of GBT? \n\n","77dd370b":"## Getting started building a classifier:\n- Assuming data cleaning has been done ( outliers, missing, duplicates, etc)\n* Split data into training, validation, and test\n* Encode\/scale\/preprocess data as neccessary\n* Build baseline dummy classifier\n* Evaluate dummy classifier\n* Try other classifiers (make sure to preprocess the data as expected by the classifier)\n* Validate and evaluate other classifiers\n* Choose a classifier\n* Evaluate on the test set\n* Stop\n\nCaution: Split the data right away into training and test. Make sure to truly separate training and test data - dont have dirty data or data leakage \ud83d\ude42\n\nCaution: If there is student specific data in the training and test, this will make our test set evaluation look more optimisitic.  Careful how the data is split. Think about data availability at the time of a prediction.\n\n","35e83e0e":"### Visualize the data:","683f1eb5":"Need to change our objective because we have a multiclass problem.  \nResource:\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n","ef0ed22c":"How different do these clusters look?","893bcc4f":"Caution: When looking at your dataset, check to see how big it is and select methods that are appropriate for the size.  Beware of overfitting when working with small datasets.\n\nResources: \n- https:\/\/medium.com\/rants-on-machine-learning\/what-to-do-with-small-data-d253254d1a89\n- https:\/\/datascience.stackexchange.com\/questions\/19925\/what-are-the-most-suitable-machine-learning-algorithms-according-to-type-of-data","037c3d04":"## Machine Learning:","f5914c17":"\n\nResource: https:\/\/www.researchgate.net\/figure\/Examples-of-real-life-problems-in-the-context-of-supervised-and-unsupervised-learning_fig8_319093376\n\n\nNo free lunch! \n* \"All models are wrong but some are useful\"\n* Bias-Variance tradeoff\n* Curse of dimensionality\n\n        ","e5370f1b":"Resource: https:\/\/github.com\/andosa\/treeinterpreter\nhttps:\/\/medium.com\/@williamkoehrsen\/random-forest-simple-explanation-377895a60d2d","cd1000ee":"Caution: Stratify splits based on class imbalance instead of randomly.  Make sure to look at the class imbalance and deal with it appropriately based on the algorithm!\n\nResource: https:\/\/github.com\/scikit-learn-contrib\/imbalanced-learn \n\n","2e16d017":"Using all the data means we would have 17 dimensions.  Not huge, but it's bigger than the 4 we used.\n\n### NO FREE LUNCH - Curse of Dimensionality\nRemember, KMeans is finding 'distance' as euclidean distance in an n-dimensional space.  Points are few in these high dimensional space.","bbefcce0":"Source:\n\n- https:\/\/medium.com\/deep-math-machine-learning-ai\/introduction-of-machine-learning-why-how-what-84c881c70763\n- https:\/\/medium.com\/deep-math-machine-learning-ai\/different-types-of-machine-learning-and-their-types-34760b9128a2 (pic) ","08bf839f":"#### Reduce dimensions for visualization:\nhttps:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.09-principal-component-analysis.html","da5991cf":"### Explore the data more with pandas and seaborn:","93b53925":"Resource:\nhttps:\/\/quantdare.com\/what-is-the-difference-between-bagging-and-boosting\/\nhttps:\/\/medium.com\/mlreview\/gradient-boosting-from-scratch-1e317ae4587d","414b682d":"Explore the data with Pandas:\nhttps:\/\/github.com\/pandas-profiling\/pandas-profiling\n\nPCA: \nhttps:\/\/www.utdallas.edu\/~herve\/abdi-awPCA2010.pdf\nhttps:\/\/www.kaggle.com\/merckel\/preliminary-investigation-pca-boosting\nhttps:\/\/medium.com\/data-design\/how-to-not-be-dumb-at-applying-principal-component-analysis-pca-6c14de5b3c9d\n\nClustering:\nhttps:\/\/courses.cs.washington.edu\/courses\/cse546\/08sp\/slides\/cdr.pdf\nhttp:\/\/www.cbs.dtu.dk\/chipcourse\/Lectures\/ClusteringPCA_2010.pdf\nhttps:\/\/www.youtube.com\/watch?v=EUQY3hL38cw\nhttps:\/\/github.com\/nicodv\/kmodes\nhttps:\/\/pypi.org\/project\/pyclustering\/\n\nVisualizing Multidimensional Data :\nhttp:\/\/www.apnorton.com\/blog\/2016\/12\/19\/Visualizing-Multidimensional-Data-in-Python\/\nhttps:\/\/medium.com\/@luckylwk\/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n\n\nLDA can be used for dimensionality reduction as well as classification.\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/decomposition\/plot_pca_vs_lda.html\nhttps:\/\/scikit-learn.org\/stable\/modules\/lda_qda.html\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_lda.html#sphx-glr-auto-examples-classification-plot-lda-py\n\n\nPitfalls:  \nhttp:\/\/www.cs.colorado.edu\/~mozer\/Research\/Selected%20Publications\/white-paper3.html\nhttp:\/\/danielnee.com\/2015\/01\/common-pitfalls-in-machine-learning\/\n\nData Set Size:\nhttps:\/\/medium.com\/rants-on-machine-learning\/what-to-do-with-small-data-d253254d1a89\nhttps:\/\/datascience.stackexchange.com\/questions\/19925\/what-are-the-most-suitable-machine-learning-algorithms-according-to-type-of-data\nhttps:\/\/blog.myyellowroad.com\/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512\nhttps:\/\/www.datacamp.com\/community\/tutorials\/categorical-data\n\nOther good things:\nhttps:\/\/heartbeat.fritz.ai\/how-to-make-your-machine-learning-models-robust-to-outliers-44d404067d07\nhttps:\/\/github.com\/scikit-learn-contrib\/sklearn-pandas\nhttps:\/\/medium.com\/dunder-data\/from-pandas-to-scikit-learn-a-new-exciting-workflow-e88e2271ef62\nhttps:\/\/machinelearningmastery.com\/the-model-performance-mismatch-problem\/\nhttps:\/\/medium.com\/@outside2SDs\/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365\n\n\n\n\n\n","aed231e9":"Congratulations - you have now gone through a data set and built many models and selected one.  To continue your journey in improving this model, you can try additional things like hyperparamter tuning, feature engineering, ensembling linear and tree based models, etc!  For example, start digging into the misclassifications to gain more insight about your model and data.\n\nCaution: Digging into misclassification is an excellent way to find gaps in your modeling approach - especially when putting models in production! ","4782847f":"Caution: VERY important to keep track of the versions used - open source packages change frequently and the code may not work with new package versions.  Use Docker or virtual environments to keep track and test all your code anytime you want to update a package in your environment.\n\nI like to print it out in the notebook if I'm just sharing my notebook.\n","651c074c":"Let's cluster using our new data:","2062595f":"Resource:\nhttp:\/\/blog.kaggle.com\/2017\/01\/23\/a-kaggle-master-explains-gradient-boosting\/","c1dda295":"\nroc pic: https:\/\/machinelearningmastery.com\/assessing-comparing-classifier-performance-roc-curves-2\/\n\nSource: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc_crossval.html\n\nSource: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html\n\n- Multiclass metrics:\n\"weighted\" accounts for class imbalance by computing the average of binary metrics in which each class\u2019s score is weighted by its presence in the true data sample.\n\nCaution:\n- Many classifiers predict probabilities along with class labels, use ROC or precision-recall curves to find the best threshold for your classifier based on your problem!\n- Your model is always wrong, spend time thinking about what type of wrong you are comfortable with.","98151a8e":"STOP - After trying all the different paramters, identify a model to select, and train with all the data and show the test set accuracy.  DO NOT GO BACK AND CHOOSE A DIFFERENT MODEL IF TEST SET ACCURACY DOESN\"T MAKE YOU HAPPY.","d5c48063":"Great - now we have a baseline model - let's go beat it!\n","ef157ac0":"## Data Preprocessing:\n* Models require numeric data (most of the time)\n* Potential Steps:\n    * Clean up outliers\n    * Decide how to deal with missing values (i.e. impute missing values, remove rows or columns) \n    * Identify multicollinearity (and remove if neccessary)\n    * Scale or normalize (if neccessary)\n    * Encode categoricals into numeric (if neccessary, many ways to do this)\n\nCaution: Important to note the assumptions of your algorithm so you preprocess correctly!\n\nResources:\n- https:\/\/hackernoon.com\/what-steps-should-one-take-while-doing-data-preprocessing-502c993e1caa\n- https:\/\/towardsdatascience.com\/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9","43333d10":"Caution: To ensure reproducibility, check to see if your function accepts a random_state, and make sure to set it manually otherwise your results will change each time you re-run the function!","7f5adf67":"# Unsupervised Learning\n\nClustering and dimensionality reduction are 2 common approaches to unsupervised learning.\nUnsupervised learning can be a part of exploratory data analysis.\n\n* Find meaningful relationships\n* Low dimensional representations for visualization or compression\n\nResource: https:\/\/web.stanford.edu\/class\/stats202\/content\/lec2.pdf\n\n## Clustering\n* Clustering is to group the data\n* Some algorithms are: KMeans, heirarchical clustering, DBSCAN, etc\n* Downside is that it is difficult to evaluate the model\n\nCaution: Make sure to look at the details of your algorithm to identify the assumpions of the expected input data.  \n\nResource: https:\/\/www.r-bloggers.com\/k-means-clustering-is-not-a-free-lunch\/\n\nhttps:\/\/web.stanford.edu\/class\/stats202\/content\/lec2.pdf\n\n","32554fea":"## PCA\/SVD\n\nRelationship - PCA(X) = SVD(X-mean(X))\n- Sklearn calculates PCA using SVD\n- Sklearn's PCA doesn't support sparse matrix\n- Use TruncatedSVD for sparse matrices\n- PCA and SVD creates new features which are linear transformations of original features\n- Basis of Latent Semantic Indexing Topic Modeling \n\n\nResources:\n- CLICK ME: http:\/\/setosa.io\/ev\/principal-component-analysis\/\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html\n- Theoretical: https:\/\/arxiv.org\/pdf\/1404.1100.pdf\n- https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.09-principal-component-analysis.html\n- https:\/\/medium.com\/data-design\/how-to-not-be-dumb-at-applying-principal-component-analysis-pca-6c14de5b3c9d\n\n","8bba09b9":"### Tree Terminology:","6c87ea88":"Let's look back at our data set and pull out a different subset:","4275db88":"Note that 'Relation' moved up here in feature importance compared to RandomForest model.","2648a606":"Alternative ways to explore data !  Additional profiling, and visualization with pandas_summary and pandas_profiling.\n","99d2af36":"## Sci-kit Learn (sklearn)\n* Considered the \"gold standard\" interface to transforming, and fitting models\n* Spark's ML Lib interface is modeled after it\n* fit_transform, fit_predict\n* Can create pipelines to transform, preprocess, clean, and fit models\n\n","3cffc7d7":"## Wrap Up:\n- scipy, numpy, pandas\n- sklearn, pipelines\n- model evaluation\n- supervised, unsupervised approaches\n- model selection using cross validation","2c4ff00c":"## Let's get started! Import packages!","b2b3e400":"Resources: https:\/\/georgemdallas.wordpress.com\/2013\/10\/30\/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction\/\n","4dc561fb":"## Deep dive - Trees\n- Decision Tree\n- Random Forest\n- Gradient Boosted Trees (GBT)\n- One implementation of GBT - XGBoost - created 2016 - won a lot of kaggle competitions when it first came out. \n- Trees can handle multi-collinearity\n- Trees can handle categoricals\n- Caution: Tree based methods with feature importance will give more importance to variables with greater cardinality\n\nResources:\n- http:\/\/arogozhnikov.github.io\/2016\/06\/24\/gradient_boosting_explained.html\n- https:\/\/www.displayr.com\/gradient-boosting-the-coolest-kid-on-the-machine-learning-block\/\n- http:\/\/blog.kaggle.com\/2017\/01\/23\/a-kaggle-master-explains-gradient-boosting\/\n- https:\/\/stackoverflow.com\/questions\/51601122\/xgboost-minimize-influence-of-continuous-linear-features-as-opposed-to-categori\n- Brief history - http:\/\/www.ccs.neu.edu\/home\/vip\/teach\/MLcourse\/4_boosting\/slides\/gradient_boosting.pdf\n- Categorical Boosting - https:\/\/towardsdatascience.com\/catboost-vs-light-gbm-vs-xgboost-5f93620723db\n\nCLICK ME! http:\/\/www.r2d3.us\/visual-intro-to-machine-learning-part-1\/\n\n","1d17e2f0":"#### Most common: \n* np.reshape()\n* np.arange()\n* np.linspace()\n* np.zeros()\n* np.sum()\n* np.mean()\n* np.argmax()\n* np.argmin()\n* np.array()\n* np.sort()\n\n#### Cheat Sheets!\n* https:\/\/s3.amazonaws.com\/assets.datacamp.com\/blog_assets\/Numpy_Python_Cheat_Sheet.pdf\n* https:\/\/s3.amazonaws.com\/assets.datacamp.com\/blog_assets\/Scikit_Learn_Cheat_Sheet_Python.pdf\n* https:\/\/s3.amazonaws.com\/assets.datacamp.com\/blog_assets\/PandasPythonForDataScience.pdf\n* https:\/\/s3.amazonaws.com\/assets.datacamp.com\/blog_assets\/PythonForDataScience.pdf\n\n#### Additional Resources:\n* https:\/\/towardsdatascience.com\/lets-talk-about-numpy-for-datascience-beginners-b8088722309f\n","0939f80c":"Get a baseline and then let's create more models to compare it to. \n\nTopics:\n- Model evaluation - how do we evaluate our model\n- Pipelines - makes it easy to preprocessing training and test data\n- Cross-validation - how do we choose hyperparameters and final model","c7099b7a":"Fork this notebook!","6c42b392":"## KMeans Clustering\n* Simple clustering algorithm\n* Can handle very large datasets\n\nCaution: Make sure to look at the details of your algorithm to identify the assumpions of the expected input data\n\n* Hands on visualization of how it works: http:\/\/web.stanford.edu\/class\/ee103\/visualizations\/kmeans\/kmeans.html","f4e4e449":"Exciting times! Data sources, tools, compute resources readily available to get started!\n\nFree Data Sources:\n* Too many to list!\n* Caution: Data sources vs. Machine learning data - structured\/unstrctured data vs labeled data\n* Caution: Check out everyone's licensing before using it for your enterprise needs.\n\n\nQuick intro to open source data science and analytics - \n\nCompute resources to use:\nFree (or free trial):\n* https:\/\/data.world\/community\/open-community\/\n* https:\/\/colab.research.google.com\/notebooks\/welcome.ipynb\n* https:\/\/aws.amazon.com\/sagemaker\/pricing\/\n* https:\/\/cloud.google.com\/products\/ai\/\n* https:\/\/datastudio.google.com\/navigation\/reporting\n* https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/virtual-machines\/windows\/\n* https:\/\/www.dominodatalab.com\/domino-for-good\/\n* https:\/\/www.dominodatalab.com\/domino-for-good\/for-students\/\n* https:\/\/www.kaggle.com\/sigma23\/women-in-analytics-2019-workshop\/edit\n* https:\/\/medium.com\/@jamsawamsa\/running-a-google-cloud-gpu-for-fast-ai-for-free-5f89c707bae6\n\nCaution: Make sure you know how to shut them down to not rack up a huge bill!\n\nGenerally free tools \n* RStudio\n* Anaconda (Jupyter, Spyder, Orange)\n* Weka\n* KNIME\n* https:\/\/www.h2o.ai\/products\/h2o\/#how-it-works\n* https:\/\/public.tableau.com\/en-us\/s\/\n* https:\/\/plot.ly\/create\/#\/\n\n\nGreat Resources:\nFree Code!\n* https:\/\/github.com\/amueller\/scipy-2018-sklearn\n* https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\n* https:\/\/github.com\/rasbt\/python-machine-learning-book-2nd-edition\n* https:\/\/github.com\/josephmisiti\/awesome-machine-learning\n* https:\/\/github.com\/lazyprogrammer\/machine_learning_examples\n* https:\/\/github.com\/scikit-learn\/scikit-learn\n\nFree Courses:\nCoursera, edx, classcentral.com\n\nSoapbox: Free means people have dedicated time and resources to creating and maintaining these things.  Be a part of the open source community by contributing!\n\n\n\n","5c80926d":"## Supervised Learning:\n* Learning with labels provided.\n* Many classification methods\n","7aaaee60":"And this is why we cross validate! So easy to overfit! ","5a96f9e7":"What do you think could be the pros and cons of using a decision tree?","4582dd27":"# What is Machine Learning?\n\nMachine Learning\n\nGreat definition: https:\/\/emerj.com\/ai-glossary-terms\/what-is-machine-learning\/\n\n","ac97275a":"Note: We would use a combination of cross validation and grid search against all the parameters - called \"hyperparameter tuning\" to pick our final model.  Look at resources section at the end which has python packages that will do these in a 'smarter' way rather than brute force.\n","683ee8fa":"Resource: https:\/\/www.kdnuggets.com\/2017\/04\/must-know-curse-dimensionality.html","34f7b72f":"- Exercise: For all the kids in middle school (school_data.StageID=='MiddleSchool'), which subject (Topic) is taken the most?\n- Exercise: Do boys or girls in Middle school have the highest average Discussion ?","c8e7c7f7":"Caution: Generally we would want to deal with duplicate rows, here we assume that 2 students have the same data.  Data set would be better if it had some kind of studentID.\n\nResource: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.drop_duplicates.html","8389bee8":"Source:\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/04\/complete-tutorial-tree-based-modeling-scratch-in-python\/","02f55917":"Need latest package for categorical_encoders:","fead2b69":"Caution: Always important to visualize and not just rely on sample statistics! \n\nResources: \n- https:\/\/seaborn.pydata.org\/examples\/anscombes_quartet.html\n- HOT OFF THE PRESS: https:\/\/medium.com\/@plotlygraphs\/introducing-plotly-express-808df010143d - Plotly express!\n","ee4801d3":"Let's dig into our data!","5fb21681":"## Additional resources ","8642f4cc":"#### Convert categoricals into numeric\nQuick way to convert categoricals into numeric - \"get dummies\"","fdb708fd":"## Quickest intro to numpy\/scipy:\n* NumPy (1995 as numeric, 2006 as NumPy)\n    * Array data types and basic operations (with some overlap with scipy)\n* SciPy (Scientific Python) - created 2001\n    * Fully featured versions of the linear algebra and numerical algorithms\n    \n    \n* Fortran\/C\/C++ under the hood - fast! Don't rewrite these methods!\n* Incredible SciPy conference held yearly in Austin, TX! Meet many of the scikit-learn and other python open source contributers! https:\/\/conference.scipy.org\/ Watch all talks for free: https:\/\/www.youtube.com\/user\/EnthoughtMedia\n","9b2d65b3":"Notice that converted all the numeric columns into categoricals as well due to their discrete nature.\n","fb0b8068":"### Let's get fancy!\n### Convert our categorical data to numeric using target encoding:\n* Also called \"mean encoding\"\n* This is advanced feature engineering\n* Easy to overfit","93da988d":"Exercise: Look at the data with respect to another numeric variable (i.e y='VisITedResources').  Any interesting insights?","a8070f90":"XGBoost has a scikitlearn api wrapper so we can call it the same way we call other classification algorithms.","26d2b27f":"**Import Student Dataset:**\n\nhttps:\/\/www.kaggle.com\/aljarah\/xAPI-Edu-Data\n\n- Data already attached to this kernel so no need to import :)\n\nCitation:\n* Amrieh, E. A., Hamtini, T., & Aljarah, I. (2016). Mining Educational Data to Predict Student\u2019s academic Performance using Ensemble Methods. International Journal of Database Theory and Application, 9(8), 119-136.\n\n* Amrieh, E. A., Hamtini, T., & Aljarah, I. (2015, November). Preprocessing and analyzing educational data set using X-API for improving student's performance. In Applied Electrical Engineering and Computing Technologies (AEECT), 2015 IEEE Jordan Conference on (pp. 1-5). IEEE.","dbafdfb1":"Caution: Pandas has 2 basic methods of indexing, \"loc\" and \"iloc\" - \n* loc - gets rows\/columns with particular labels from the index\n* iloc - gets rows\/columns with positional index"}}