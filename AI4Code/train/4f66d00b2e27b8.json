{"cell_type":{"1aba715b":"code","47a85f6b":"code","9c61e396":"code","f4026d74":"code","6121c48a":"code","eb830d44":"code","3d1634e7":"code","ca982a91":"code","ba845353":"code","a8819178":"code","95776ac3":"code","a35ef531":"code","331b83ee":"code","4dfc1bc1":"code","b560689d":"code","67816acf":"code","e9a7d622":"code","8016096f":"code","31625331":"code","5680058e":"code","cf8b6802":"code","455880f4":"code","ea68fe86":"code","d7bbbe92":"code","ba56d2c8":"code","fdfa40f5":"code","9979b5b9":"code","673c827a":"code","b76bc79d":"code","9be39114":"markdown","b6fb4387":"markdown"},"source":{"1aba715b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","47a85f6b":"#Purpose: Importing Libraries\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore, chi2_contingency\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_validate, learning_curve","9c61e396":"#Purpose: Reading data in from Kaggle\ntraining = \"\/kaggle\/input\/\/tabular-playground-series-feb-2021\/train.csv\"\ntraining_df = pd.read_csv(training)\n\ntesting = \"\/kaggle\/input\/\/tabular-playground-series-feb-2021\/test.csv\"\ntesting_df = pd.read_csv(testing)","f4026d74":"#QC: Displaying head\ntraining_df.head(10)","6121c48a":"#Describe function\ntraining_df.describe()","eb830d44":"#Correlation Matrix \nfig, scatter = plt.subplots(figsize = (16,9))\ncorrMatrix = training_df.corr()\nscatter = sn.heatmap(corrMatrix, annot = True)\nscatter","3d1634e7":"#Purpose: Plotting a histogram to see the distribution\nfig = plt.figure(figsize = (15,20))\nax = fig.gca()\ntraining_df.hist(ax = ax, bins = 50)","ca982a91":"#Creating a dummy dataframe and removing id and target \ntraining_wo_id_target = training_df.drop(columns = [\"id\", \"target\"]) ","ba845353":"#Purpose: Plotting a boxplot to see the outlier distribution\nfig = plt.figure(figsize = (15,5))\nax = fig.gca()\ntraining_wo_id_target.boxplot(ax = ax)","a8819178":"#Calculating the score\nnumeric_cols = training_wo_id_target.select_dtypes(include=[np.number]).columns\nscores = training_wo_id_target[numeric_cols].apply(zscore)  ","95776ac3":"#Calculating the upper whisker outliers \nscore_max = pd.DataFrame(scores>3)\nfor col in score_max:\n    print(score_max[col].value_counts())","a35ef531":"#Calculating the lower whisker outliers \nscore_min = pd.DataFrame(scores<-3)\nfor col in score_min:\n    print(score_min[col].value_counts())","331b83ee":"columns = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']\nfor column in columns:\n    print(column)\n    print(training_df[column].value_counts())\n    print(\"======\")","4dfc1bc1":"y = training_df['target']\nx = training_df.drop(['target','id'], axis = 1)","b560689d":"training_df_cat = training_df.select_dtypes(\"object\").columns","67816acf":"training_df_cat","e9a7d622":"ct = ColumnTransformer(transformers=[['oe',OrdinalEncoder(),training_df_cat]],remainder='passthrough')","8016096f":"pipeline = Pipeline(steps=[['ord_encoder',ct],\n                          ['rfe',RFE(estimator=xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1),\n                                    n_features_to_select=20)],\n                          ['regressor',xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1,\n                                                       max_depth=4,n_estimators=200,reg_lambda=100)]])","31625331":"pipeline.fit(x,y)","5680058e":"cv = cross_validate(estimator=pipeline,X=x,y=y,scoring='neg_root_mean_squared_error',cv=5,n_jobs=-1,return_train_score=True)","cf8b6802":"cv","455880f4":"x_cont = x.select_dtypes(\"float64\")\nx_cont","ea68fe86":"from sklearn.decomposition import PCA","d7bbbe92":"pca_breast = PCA(n_components=2)\nprincipalComponents_breast = pca_breast.fit_transform(x_cont)","ba56d2c8":"principal_breast_Df = pd.DataFrame(data = principalComponents_breast, columns = ['pc1', 'pc2'])","fdfa40f5":"new  = training_df[training_df_cat].join(principal_breast_Df)","9979b5b9":"new","673c827a":"cv = cross_validate(estimator=pipeline,X=new,y=y,scoring='neg_root_mean_squared_error',cv=5,n_jobs=-1,return_train_score=True)","b76bc79d":"cv","9be39114":"**Outlier Detection using Z Score!** \n\n\nA negative Z-score means an observation is below the mean, while a positive one means it above it. The further away from 0 the Z-Score is, the further away from the mean your observation is.\n\nOne way to identify outliers is to determine which points have a z-score that's far from 0.","b6fb4387":"Findings: (pearson corr p value)\n1. cont0 has a +ve corr with cont5(0.58) cont8(0.58) cont9(0.52)\n2. cont5 has a +ve corr with cont8(0.61) cont9(0.62) cont11(0.51) cont12(0.63)\n3. cont8 has a +ve corr with cont9(0.56) cont12(0.53)\n4. cont9 has a +ve corr with cont11(0.52) cont12(0.54)\n5. cont10 has a +ve corr with cont11(0.56)\n\n6. cont2 has a -ve corr with cont0 cont3 cont5 cont6 cont7 cont8 cont9 cont10 cont11 cont12 cont13"}}