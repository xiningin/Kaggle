{"cell_type":{"c540ab94":"code","76abd7a0":"code","576e6f16":"code","07a09bcd":"code","927ef07b":"code","1839e05a":"code","65357b4d":"code","621acff7":"code","527b38dc":"code","dc967cc9":"code","b5fae7c5":"code","2f489a1c":"code","caeea0cd":"code","bd9b3f00":"code","14c8df66":"code","930b081a":"code","8d00dc37":"code","46b199a2":"code","ebe3e983":"code","dd7b7a37":"code","c889b979":"code","7385e8c7":"code","0dd58d73":"code","bf727b44":"code","1aa8ed46":"markdown","d4ddb9d9":"markdown","9784937e":"markdown","a9d24f9d":"markdown","2258f899":"markdown"},"source":{"c540ab94":"from keras.applications.vgg16 import VGG16\nfrom keras.utils import to_categorical, plot_model\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, BatchNormalization, Dropout\nfrom keras.layers import Flatten\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nimport xml.etree.ElementTree as ET\nfrom keras.layers import GlobalAveragePooling2D, Dense, Input\nfrom keras.applications.xception import Xception ","76abd7a0":"# READ IMAGES IN COLORED FORMAT\ndef read_image(path):\n    return cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n\n# CREATE IMAGE LIST\ndef create_image_list(image_path):\n    image_list = []\n    # ITERATE THROUGH IMAGES FOLDER\n    for image in os.listdir(image_path):\n        # APPEND THE NAME OF IMAGES TO THE LIST\n        image_list.append(image)\n    return image_list\n\n# CREATE MASK FOR BOUNDING BOX\ndef create_mask(bb, image):\n    # EXTRACT THE IMAGE SHAPE\n    rows,cols,*_ = image.shape\n    # CREATE A MATRIX OF ZERO OF THE IMAGE SHAPE\n    mask = np.zeros((rows, cols))\n    # FILL THE MATRIX CONTAINING THE BOUNDING BOX WITH VALUE 1\n    mask[bb[0]:bb[2], bb[1]:bb[3]] = 1.\n    return mask\n\n# CONVERT RESIZED MASK TO BOUNDING BOX\ndef convert_to_bb(mask):\n    # EXTRACT THE SHAPE OF THE MASK OF BOUNDING BOX CREATED\n    cols, rows = np.nonzero(mask)\n    # RETURN ZERO COORDINATES IF NO MASK\n    if len(cols)==0: \n        return np.zeros(4, dtype=np.float32)\n    # EXTRACT THE BOUNDING BOX COORDINATES\n    top_row = np.min(rows)\n    left_col = np.min(cols)\n    bottom_row = np.max(rows)\n    right_col = np.max(cols)\n    return np.array([left_col, top_row, right_col, bottom_row], dtype=np.float32)\n\n# RESIZE THE IMAGES AND SAVE IT IN ANOTHER FOLDER\ndef image_resize(image_path, new_path, bb, size):\n    # READ THE IMAGE FILE\n    image = read_image(image_path)\n    # RESIZE THE IMAGE\n    image_resized = cv2.resize(image, (int(1.49*size), size))\n    # CREATE MASK FROM THE BOUNDING BOX\n    mask = create_mask(bb, image)\n    # RESIZE THE MASK \n    mask_resized = cv2.resize(mask, (int(1.49*size), size))\n    # WRITE THE NEW IMAGE INTO ANOTHER FOLDER\n    cv2.imwrite(new_path, cv2.cvtColor(image_resized, cv2.COLOR_RGB2BGR))\n    return new_path, convert_to_bb(mask_resized)\n\n# PLOT THE BOUNDING BOX AROUND THE IMAGE\ndef plot_bb(path, bb):\n    image = read_image(path)\n    # CONVERT BOUNDING BOXES (BB) INTO FLOAT\n    bb = np.array(bb, dtype=np.float32)\n    # CREATE A RECTANGLE FROM THE BB\n    rect_box = plt.Rectangle((bb[1], bb[0]), bb[3]-bb[1], bb[2]-bb[0], color='red',\n                         fill=False, lw=3)\n    # RENDER THE IMAGE\n    plt.imshow(image)\n    # APPLY THE BB TO THE CURRENT AXIS RENDERING IMAGE\n    plt.gca().add_patch(rect_box)","576e6f16":"# EXTRACT BOUNDING BOX FROM THE ANNOTATION FILE\ndef extract_bb(anno_path):\n    # PARSE THE XML FILE TO EXTRACT BB COORDINATES AND CLASS_NAME\n    root = ET.parse(anno_path).getroot()\n    class_name = root.find(\".\/object\/name\").text\n    xmin = int(root.find(\".\/object\/bndbox\/xmin\").text)\n    ymin = int(root.find(\".\/object\/bndbox\/ymin\").text)\n    xmax = int(root.find(\".\/object\/bndbox\/xmax\").text)\n    ymax = int(root.find(\".\/object\/bndbox\/ymax\").text)\n    # RETURN BOUNDING BOX COORDINATES\n    bb = [ymin, xmin, ymax, xmax]\n    return bb, class_name\n\n# GENERATE DATAFRAME\ndef generate_dataframe(image_list, anno_path, image_path, new_path, size):\n    dataset = []\n    for image in image_list:\n        path = image_path + image\n        a_path = anno_path + image.split('.')[0] + '.xml'\n        # EXTRACT BB AND CLASS_NAME FROM ANNOTATION FILE\n        bb, class_name = extract_bb(a_path)\n        # FILENAME OF THE NEW RESIZED IMAGE\n        n_path = new_path + image \n        # RESIZE THE IMAGE AND CORRESPONDING BOUNDING BOX \n        img_path, resized_bb = image_resize(path, n_path, bb, size)\n        # APPEND EVERYTHING TO A DICTIONARY \n        data = dict()\n        data['filename'] = img_path\n        data['bb'] = resized_bb\n        data['class_name'] = class_name\n        # APPEND THE DICTIONARY TO THE LIST\n        dataset.append(data)\n    # APPEND THE LIST TO THE DATAFRAME \n    return pd.DataFrame(dataset) ","07a09bcd":"# prepare dataset\ndef generate_data_array(dataframe):\n    \n    train_img = []\n    classes = []\n    bounding_boxes = []\n    \n    for index, row in dataframe.iterrows():\n        path = row['filename']\n        x = read_image(path)\n        \n        # append image\n        train_img.append(x)\n        # append class labels\n        classes.append(row['class_name'])\n        # append bb\n        bounding_boxes.append(row['bb'])\n        \n    return train_img, classes, bounding_boxes","927ef07b":"!mkdir resized_image","1839e05a":"image_path = '..\/input\/currency-datasets\/images\/'\nanno_path =  '..\/input\/currency-datasets\/annotations\/'\nnew_path = '.\/resized_image\/'\n\n# CREATE IMAGE LIST\nimage_list = create_image_list(image_path)\n# SHUFFLE THE LIST\nnp.random.shuffle(image_list)\n\nimage_list[:5]","65357b4d":"data = generate_dataframe(image_list, anno_path, image_path, new_path, 300)\nlen(data)","621acff7":"data.head()","527b38dc":"train_img, classes, bounding_boxes = generate_data_array(data)\n# ENCODE THE LABEL\nle = LabelEncoder()\nY_class = le.fit_transform(classes)","dc967cc9":"# 0:'50Rs', 1:'500Rs', 2:'100Rs', 3:'10Rs', 4:'20Rs', 5:'200Rs', 6:'2000Rs'\nle.inverse_transform([0,1,2,3,4,5,6])","b5fae7c5":"X = np.array(train_img)\nY_class = to_categorical(Y_class)\nY_bb = np.array(bounding_boxes)\n\n# CHECK THE DIMENSIONS\nprint(\"shape of X \", X.shape)\nprint(\"shape of Y_class \", Y_class.shape)\nprint(\"shape of Y_bb \", Y_bb.shape)","2f489a1c":"# model = VGG16(include_top=False, input_shape=(224, 333, 3))\n# x = Flatten()(model.layers[-1].output)\n \nimage_input = Input(shape=(300, 447, 3))\nbase_model = Xception(include_top=False, input_tensor=image_input, weights='imagenet')\nx = base_model.layers[-1].output\nx = GlobalAveragePooling2D()(x)\n# x = BatchNormalization()(x)\n# x = Dropout(0.25)(x)\n\noutput_1 = Dense(4, activation='relu')(x)\noutput_2 = Dense(7, activation='softmax')(x)\n\nmodel = Model(inputs = image_input, outputs =[output_1, output_2])\nmodel.compile(loss=['mae', 'categorical_crossentropy'], optimizer='adam', metrics =['accuracy'])","caeea0cd":"# dense_loss = loss of bb\n# dense_1_loss = loss of class\nhistory = model.fit(X, [Y_bb, Y_class], epochs=200, batch_size=8, validation_split=0.2)","bd9b3f00":"# PLOT BOUNDING BOX TRAINING LOSS AND VALIDATION LOSS\n\nplt.plot(history.history['dense_loss'], color='deeppink')\nplt.plot(history.history['val_dense_loss'], color='yellowgreen')\nplt.title('bounding box loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()","14c8df66":"# PLOT CLASS LABEL TRAINING LOSS AND VALIDATION LOSS\n\nplt.plot(history.history['dense_1_loss'], color='darkorange')\nplt.plot(history.history['val_dense_1_loss'], color='cornflowerblue')\nplt.title('class_label loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()","930b081a":"test_image = '.\/resized_image\/2000_3.jpeg'\n\nimg = read_image(test_image)\nimg = np.array([img])\npredict = model.predict(img)\npredict","8d00dc37":"# INFERENCE CODE\ncurrency_dict = {0:'50Rs', 1:'500Rs', 2:'100Rs', 3:'10Rs', 4:'20Rs', 5:'200Rs', 6:'2000Rs'}\n\ndef identify_currency(image_path):\n    \n    x = read_image(image_path)\n    img = read_image(test_image)\n    img = np.array([img])\n    predict = model.predict(img)\n    confidence = predict[1][0][np.argmax(predict[1][0])]\n    curr = currency_dict[np.argmax(predict[1][0])]\n    print(\" The detected currency is {} and the model is {} % sure about it\".format(curr, round(confidence*100, 2)))\n    plot_bb(image_path, predict[0][0])","46b199a2":"test_image = '.\/resized_image\/2000_3.jpeg'\n\nidentify_currency(test_image)","ebe3e983":"test_image = '.\/resized_image\/500_64.jpeg'\n\nidentify_currency(test_image)","dd7b7a37":"test_image = '.\/resized_image\/200_46.jpeg'\n\nidentify_currency(test_image)","c889b979":"test_image = '.\/resized_image\/10_29 (copy).jpeg'\n\nidentify_currency(test_image)","7385e8c7":"test_image = '.\/resized_image\/50_43 (copy).jpeg'\n\nidentify_currency(test_image)","0dd58d73":"test_image = '.\/resized_image\/100_37.jpeg'\n\nidentify_currency(test_image)","bf727b44":"test_image = '.\/resized_image\/20_19 (copy).jpeg'\n\nidentify_currency(test_image)","1aa8ed46":"# OBJECT DETECTION (CLASSIFICATION + LOCALIZATION) ON CUSTOM DATA\n\nThis is the part 2 of the notebook https:\/\/www.kaggle.com\/shweta2407\/objectdetection-on-custom-dataset-resnet34. I have explained every part of the code in the above mentioned notebook in detail.\n\nThe above notebook is implemented in PyTorch using ResNet34 model, whereas this notebook is implemented in Keras using Xception model.","d4ddb9d9":"First array contains the bounding box coordinates\nSecond array contains probability of class labels","9784937e":"### TESTING OF THE MODEL","a9d24f9d":"# Plot training loss and validation loss","2258f899":"For more information, refer to this notebook https:\/\/www.kaggle.com\/shweta2407\/objectdetection-on-custom-dataset-resnet34\n\n#### THANK YOU"}}