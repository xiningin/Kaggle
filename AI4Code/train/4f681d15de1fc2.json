{"cell_type":{"cc9613fd":"code","95d4a79f":"code","b3f34dce":"code","76ce1cdb":"code","60fb01cf":"code","3da67e0d":"code","067e5ee0":"code","5ad96524":"code","7cdfac7c":"code","6c3d430c":"code","c218c01c":"code","9cd4f848":"code","38f25144":"code","fe2e6f62":"code","bbfc45c2":"code","da79e1c2":"code","818545b0":"code","79fa67b3":"code","5df36e14":"code","ab1ad696":"code","6a9bf63d":"code","7286851b":"code","537b2ad0":"code","651c1523":"code","f08d0cd4":"code","6ab22add":"code","ef229497":"code","f1c7da9d":"code","45551fde":"code","d085c299":"code","e62cabf5":"code","28c5d75c":"code","427f7f8b":"code","3f92c85e":"code","79d98aa7":"code","aa55cde3":"code","b9d3317a":"code","e7044612":"code","595f4646":"code","6914be47":"code","ebcd12ce":"code","b942fb6a":"code","d6a59ddc":"code","bf0e1521":"code","0bb318ca":"code","aa1c865a":"code","02ce9c05":"code","e259e37b":"code","2896f593":"code","fd5f450f":"code","a183534b":"code","4ef0ea64":"code","83744701":"code","752f3e41":"code","35be0043":"code","b56669de":"code","4d206bc0":"code","11428fa5":"code","555f64ae":"code","0cdb5d19":"code","dccaf36c":"code","c693104d":"code","daafe531":"code","e64d3270":"code","78d665ed":"code","a51e87ea":"code","e8d2f52c":"code","e1d7d26b":"code","7c98f2dd":"code","3f5d438a":"code","75f9dbdf":"code","b9836f3e":"code","efca8f87":"code","d29c3e55":"code","71406ac0":"code","5566afb5":"code","d581ef50":"code","666cedaf":"code","d3ba2c0a":"code","f86a54fc":"code","546f0151":"code","7c995906":"markdown","88483fdc":"markdown","fe550c92":"markdown","47aea20a":"markdown","470ed4b1":"markdown"},"source":{"cc9613fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport gc\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","95d4a79f":"# credit to @guiferviz for the memory reduction \ndef memory_usage_mb(df, *args, **kwargs):\n    \"\"\"Dataframe memory usage in MB. \"\"\"\n    return df.memory_usage(*args, **kwargs).sum() \/ 1024**2\n\ndef reduce_memory_usage(df, deep=True, verbose=True):\n    # All types that we want to change for \"lighter\" ones.\n    # int8 and float16 are not include because we cannot reduce\n    # those data types.\n    # float32 is not include because float16 has too low precision.\n    numeric2reduce = [\"int16\", \"int32\", \"int64\", \"float64\"]\n    start_mem = 0\n    if verbose:\n        start_mem = memory_usage_mb(df, deep=deep)\n\n    for col, col_type in df.dtypes.iteritems():\n        best_type = None\n        if col_type in numeric2reduce:\n            downcast = \"integer\" if \"int\" in str(col_type) else \"float\"\n            df[col] = pd.to_numeric(df[col], downcast=downcast)\n            best_type = df[col].dtype.name\n        # Log the conversion performed.\n        if verbose and best_type is not None and best_type != str(col_type):\n            print(f\"Column '{col}' converted from {col_type} to {best_type}\")\n    \n    if verbose:\n        end_mem = memory_usage_mb(df, deep=deep)\n        diff_mem = start_mem - end_mem\n        percent_mem = 100 * diff_mem \/ start_mem\n        print(f\"Memory usage decreased from\"\n              f\" {start_mem:.2f}MB to {end_mem:.2f}MB\"\n              f\" ({diff_mem:.2f}MB, {percent_mem:.2f}% reduction)\")\n        \n    return df\n            ","b3f34dce":"train_identity_data = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\ntrain_transaction_data = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')","76ce1cdb":"pd.set_option('display.max_columns', 500)\n# train_transaction_data.head(5)","60fb01cf":"# train_identity_data.head(5)","3da67e0d":"# https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature\ntrain_transaction_data['Transaction_dow'] = np.floor((train_transaction_data['TransactionDT'] \/ (3600 * 24) - 1) % 7)\ntrain_transaction_data['Transaction_hour'] = np.floor(train_transaction_data['TransactionDT'] \/ 3600) % 24","067e5ee0":"na_columns = train_identity_data.isna().sum()\nprint(na_columns[na_columns==0])","5ad96524":"# number of nan values in each column\npd.set_option('display.max_rows', 500)\nna_columns = train_transaction_data.isna().sum()\nprint(na_columns[na_columns==0])\nprint(na_columns[na_columns>0]\/train_transaction_data.shape[0])","7cdfac7c":"import seaborn as sns\nimport matplotlib.pyplot as plt\n# sns.distplot(train_identity_data['id_01'])","6c3d430c":"transaction_data_columns = train_transaction_data.columns\nnumericCols = train_transaction_data._get_numeric_data().columns\ncategoricalCols = list(set(transaction_data_columns) - set(numericCols))\nprint('The categorical columns in transaction data are: ',categoricalCols)","c218c01c":"train_transaction_data[categoricalCols] = train_transaction_data[categoricalCols].replace({ np.nan:'missing'})\ntrain_transaction_data[numericCols] = train_transaction_data[numericCols].replace({ np.nan:-1})","9cd4f848":"# sns.countplot(train_transaction_data['isFraud'])","38f25144":"# sns.countplot(train_transaction_data['card6'])","fe2e6f62":"# cardTypes = ['credit','debit','debit or credit','charge card']\n# for i,i_card in enumerate(cardTypes):\n#     cardData = eval('train_transaction_data.loc[train_transaction_data[\"card6\"]==\"'+i_card+'\"]')\n#     plt.figure(i)\n#     sns.countplot(cardData['isFraud']).set_title(i_card)","bbfc45c2":"# sns.countplot(train_transaction_data['card4'])","da79e1c2":"# cardTypes = ['discover','mastercard','visa','american express','missing']\n# for i,i_card in enumerate(cardTypes):\n#     cardData = eval('train_transaction_data.loc[train_transaction_data[\"card4\"]==\"'+i_card+'\"]')\n#     plt.figure(i)\n#     sns.countplot(cardData['isFraud']).set_title(i_card)","818545b0":"# print('The average transaction amount for non fraudulent transactions is: ', \n#       np.mean(train_transaction_data.loc[train_transaction_data[\"isFraud\"]==0]['TransactionAmt']))\n# print('The average transaction amount for fraudulent transactions is: ', \n#       np.mean(train_transaction_data.loc[train_transaction_data[\"isFraud\"]==1]['TransactionAmt']))","79fa67b3":"# print('The Maximum transaction amount for non fraudulent transactions is: ', \n#       np.max(train_transaction_data.loc[train_transaction_data[\"isFraud\"]==0]['TransactionAmt']))\n# print('The Maximum transaction amount for fraudulent transactions is: ', \n#       np.max(train_transaction_data.loc[train_transaction_data[\"isFraud\"]==1]['TransactionAmt']))","5df36e14":"# print('The Minimum transaction amount for non fraudulent transactions is: ', \n#       np.min(train_transaction_data.loc[train_transaction_data[\"isFraud\"]==0]['TransactionAmt']))\n# print('The Minimum transaction amount for fraudulent transactions is: ', \n#       np.min(train_transaction_data.loc[train_transaction_data[\"isFraud\"]==1]['TransactionAmt']))","ab1ad696":"# maxCardData = {}\n# minCardData = {}\n# meanCardData = {}\n# for i,i_card in enumerate(cardTypes):\n#     cardData = eval('train_transaction_data.loc[train_transaction_data[\"card4\"]==\"'+i_card+'\"]')\n#     maxCardData[i_card] = np.max(cardData['TransactionAmt'])\n#     minCardData[i_card] = np.min(cardData['TransactionAmt'])\n#     meanCardData[i_card] = np.mean(cardData['TransactionAmt'])","6a9bf63d":"# print('The maximum transactions by card are:', maxCardData)\n# print('The minimum transactions by card are:', minCardData)\n# print('The average transactions by card are:', meanCardData)","7286851b":"# normalDataTransaction = train_transaction_data.loc[train_transaction_data[\"isFraud\"]==0]\n# normalDataTransaction.TransactionAmt.describe()","537b2ad0":"# fraudDataTransaction = train_transaction_data.loc[train_transaction_data[\"isFraud\"]==1]\n# fraudDataTransaction.TransactionAmt.describe()","651c1523":"# prodTypes = train_transaction_data['ProductCD'].unique()\n# for i,i_prod in enumerate(prodTypes):\n#     productData = eval('train_transaction_data.loc[train_transaction_data[\"ProductCD\"]==\"'+i_prod+'\"]')\n#     plt.figure(i)\n#     ax = sns.barplot(x=\"isFraud\", y=\"isFraud\", data=productData, estimator=lambda x: len(x) \/ len(productData) * 100)\n#     ax.set(ylabel=\"Percent\")\n#     ax.set_title(i_prod)","f08d0cd4":"# for i in range(1,15):    \n#     plt.figure(figsize=(16, 16))\n#     sns.distplot(eval('train_transaction_data[\"C'+str(i)+'\"]'))\n#     ax.set(ylabel=\"Distribution\")\n#     ax.set_title(['C'+str(i)])    ","6ab22add":"# for i in range(1,16):    \n#     plt.figure(figsize=(16, 16))\n#     sns.distplot(eval('train_transaction_data[\"D'+str(i)+'\"]'))\n#     ax.set(ylabel=\"Distribution\")\n#     ax.set_title(['D'+str(i)])    ","ef229497":"# del fraudDataTransaction,normalDataTransaction,productData,cardData","f1c7da9d":"# print(train_identity_data.columns)","45551fde":"identity_data_columns = train_identity_data.columns\nnumericCols = train_identity_data._get_numeric_data().columns\ncategoricalCols = list(set(identity_data_columns) - set(numericCols))\nprint('The categorical columns in identity data are: ',categoricalCols)\ntrain_identity_data[categoricalCols] = train_identity_data[categoricalCols].replace({ np.nan:'missing'})\ntrain_identity_data[numericCols] = train_identity_data[numericCols].replace({ np.nan:-1})","d085c299":"# np.unique(train_identity_data['id_30'])","e62cabf5":"#collapse os types - id_30\ntrain_identity_data.loc[train_identity_data['id_30'].str.contains('Mac', na=False), 'id_30'] = 'mac'\ntrain_identity_data.loc[train_identity_data['id_30'].str.contains('iOS', na=False), 'id_30'] = 'iOS'\ntrain_identity_data.loc[train_identity_data['id_30'].str.contains('Android', na=False), 'id_30'] = 'android'\ntrain_identity_data.loc[train_identity_data['id_30'].str.contains('Windows', na=False), 'id_30'] = 'Windows'\ntrain_identity_data.loc[train_identity_data['id_30'].str.contains('Linux', na=False), 'id_30'] = 'Linux'","28c5d75c":"train_identity_data['device_name'] = train_identity_data['DeviceInfo'].str.split('\/', expand=True)[0]\n\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\ntrain_identity_data.loc[train_identity_data['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\ntrain_identity_data.loc[train_identity_data.device_name.isin(train_identity_data.device_name.value_counts()[train_identity_data.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"","427f7f8b":"# np.unique(train_identity_data['id_30'])","3f92c85e":"# print(np.unique(train_identity_data['id_31']))\ntrain_identity_data['id_31'] = train_identity_data['id_31'].str.replace('\\d+', '')\n# print(np.unique(train_identity_data['id_31']))","79d98aa7":"# sns.countplot(train_identity_data['DeviceType'])","aa55cde3":"raw_train_data = pd.merge(train_transaction_data, train_identity_data, on='TransactionID', how='left')\ndel train_transaction_data,train_identity_data","b9d3317a":"raw_train_data_columns = raw_train_data.columns\nnumericCols = raw_train_data._get_numeric_data().columns\ncategoricalCols = list(set(raw_train_data_columns) - set(numericCols))\nprint('The categorical columns in training data are: ',categoricalCols)\nraw_train_data[categoricalCols] = raw_train_data[categoricalCols].replace({ np.nan:'missing'})\nraw_train_data[numericCols] = raw_train_data[numericCols].replace({ np.nan:-1})","e7044612":"raw_train_data = reduce_memory_usage(raw_train_data, deep=True, verbose=True)\n# print(raw_train_data.head(10))","595f4646":"# deviceTypes = raw_train_data['DeviceType'].unique()\n# for i,i_device in enumerate(deviceTypes):\n#     deviceData = eval('raw_train_data.loc[raw_train_data[\"DeviceType\"]==\"'+i_device+'\"]')\n#     plt.figure(i)\n#     sns.countplot(deviceData['isFraud']).set_title(i_device)","6914be47":"variables = list(numericCols)\nvariables.remove('isFraud')\ncorrelationMatrix = raw_train_data.loc[:, variables].corr().abs()\n#plt.figure(figsize=(20,20))\n#heat = sns.heatmap(data=correlationMatrix)\n#plt.title('Heatmap of Correlation')\nna_vals = np.sum(raw_train_data.loc[:,variables]==-1)\/raw_train_data.shape[0]\ngoodNumericVars = []\nfor i_var in variables:    \n    if na_vals[i_var] < 1:        \n        goodNumericVars.append(i_var)\ngoodNumericVars.remove('TransactionDT')\ngoodNumericVars.remove('TransactionID')\ncorrThresh = 0.9\n# Select upper triangle of correlation matrix\nupper = correlationMatrix.where(np.triu(np.ones(correlationMatrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > corrThresh)]\nto_drop.remove('TransactionDT')","ebcd12ce":"for i_var in to_drop:\n    if i_var in goodNumericVars:\n        goodNumericVars.remove(i_var)","b942fb6a":"del to_drop,corrThresh,upper,correlationMatrix,na_vals","d6a59ddc":"variables = list(categoricalCols)\nna_vals = np.sum(raw_train_data.loc[:,variables]=='missing')\/raw_train_data.shape[0]\ngoodCategoricalVars = []\nfor i_var in variables:    \n    if na_vals[i_var] < 1:        \n        goodCategoricalVars.append(i_var)","bf0e1521":"featuresToUse = goodNumericVars+goodCategoricalVars\ntrain_data = raw_train_data.loc[:,featuresToUse]\ntarget_data = raw_train_data['isFraud']","0bb318ca":"featuresToUse","aa1c865a":"train_data['TransactionAmt_decimal'] = ((train_data['TransactionAmt'] - train_data['TransactionAmt'].astype(int)) * 1000).astype(int)","02ce9c05":"#https:\/\/www.kaggle.com\/artgor\/eda-and-models#Feature-engineering\ntrain_data['TransactionAmt_to_mean_card1'] = train_data['TransactionAmt'] \/ train_data.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_mean_card4'] = train_data['TransactionAmt'] \/ train_data.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_mean_card5'] = train_data['TransactionAmt'] \/ train_data.groupby(['card5'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_mean_ProductCD'] = train_data['TransactionAmt'] \/ train_data.groupby(['ProductCD'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_mean_addr1'] = train_data['TransactionAmt'] \/ train_data.groupby(['addr1'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_mean_id31'] = train_data['TransactionAmt'] \/ train_data.groupby(['id_31'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_mean_devicename'] = train_data['TransactionAmt'] \/ train_data.groupby(['device_name'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_mean_dow'] = train_data['TransactionAmt'] \/ train_data.groupby(['Transaction_dow'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_mean_hour'] = train_data['TransactionAmt'] \/ train_data.groupby(['Transaction_hour'])['TransactionAmt'].transform('mean')\ntrain_data['card1_card2'] = train_data['card1'].astype(str) + '_' + train_data['card2'].astype(str)\ntrain_data['addr1_dist1'] = train_data['addr1'].astype(str) + '_' + train_data['dist1'].astype(str)\ntrain_data['card1_addr1'] = train_data['card1'].astype(str) + '_' + train_data['addr1'].astype(str)\ntrain_data['card1_addr2'] = train_data['card1'].astype(str) + '_' + train_data['addr2'].astype(str)\ntrain_data['card2_addr1'] = train_data['card2'].astype(str) + '_' + train_data['addr1'].astype(str)\ntrain_data['card2_addr2'] = train_data['card2'].astype(str) + '_' + train_data['addr2'].astype(str)\ntrain_data['card4_addr1'] = train_data['card4'].astype(str) + '_' + train_data['addr1'].astype(str)\ntrain_data['card4_addr2'] = train_data['card4'].astype(str) + '_' + train_data['addr2'].astype(str)\ntrain_data['DeviceInfo_P_emaildomain'] = train_data['DeviceInfo'].astype(str) + '_' + train_data['P_emaildomain'].astype(str)\ntrain_data['P_emaildomain_addr1'] = train_data['P_emaildomain'].astype(str) + '_' + train_data['addr1'].astype(str)\ntrain_data['id01_addr1'] = train_data['id_01'].astype(str) + '_' + train_data['addr1'].astype(str)\ntrain_data['TransactionAmt_to_std_card1'] = train_data['TransactionAmt'] \/ train_data.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain_data['TransactionAmt_to_std_card4'] = train_data['TransactionAmt'] \/ train_data.groupby(['card4'])['TransactionAmt'].transform('std')\ntrain_data['TransactionAmt_to_std_card5'] = train_data['TransactionAmt'] \/ train_data.groupby(['card5'])['TransactionAmt'].transform('std')\ntrain_data['TransactionAmt_to_std_ProductCD'] = train_data['TransactionAmt'] \/ train_data.groupby(['ProductCD'])['TransactionAmt'].transform('std')\ntrain_data['TransactionAmt_to_std_addr1'] = train_data['TransactionAmt'] \/ train_data.groupby(['addr1'])['TransactionAmt'].transform('std')\ntrain_data['TransactionAmt_to_std_id31'] = train_data['TransactionAmt'] \/ train_data.groupby(['id_31'])['TransactionAmt'].transform('std')\ntrain_data['TransactionAmt_to_std_devicename'] = train_data['TransactionAmt'] \/ train_data.groupby(['device_name'])['TransactionAmt'].transform('std')\ntrain_data['TransactionAmt_to_std_dow'] = train_data['TransactionAmt'] \/ train_data.groupby(['Transaction_dow'])['TransactionAmt'].transform('std')\ntrain_data['TransactionAmt_to_std_hour'] = train_data['TransactionAmt'] \/ train_data.groupby(['Transaction_hour'])['TransactionAmt'].transform('std')","e259e37b":"train_data['TransactionAmt_to_mean_C1'] = train_data['TransactionAmt'] \/ train_data.groupby(['C1'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_mean_C3'] = train_data['TransactionAmt'] \/ train_data.groupby(['C3'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_mean_C5'] = train_data['TransactionAmt'] \/ train_data.groupby(['C5'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_mean_C13'] = train_data['TransactionAmt'] \/ train_data.groupby(['C13'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_mean_D15'] = train_data['TransactionAmt'] \/ train_data.groupby(['D15'])['TransactionAmt'].transform('mean')\ntrain_data['TransactionAmt_to_std_C1'] = train_data['TransactionAmt'] \/ train_data.groupby(['C1'])['TransactionAmt'].transform('std')\ntrain_data['TransactionAmt_to_std_C3'] = train_data['TransactionAmt'] \/ train_data.groupby(['C3'])['TransactionAmt'].transform('std')\ntrain_data['TransactionAmt_to_std_C5'] = train_data['TransactionAmt'] \/ train_data.groupby(['C5'])['TransactionAmt'].transform('std')\ntrain_data['TransactionAmt_to_std_D15'] = train_data['TransactionAmt'] \/ train_data.groupby(['D15'])['TransactionAmt'].transform('std')","2896f593":"scale_pos_weight = np.sqrt(len(target_data.loc[target_data==0])\/len(target_data.loc[target_data==1]))\ndel raw_train_data","fd5f450f":"test_identity_data = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')\ntest_transaction_data = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')","a183534b":"na_columns = test_transaction_data.isna().sum()\nprint(na_columns[na_columns==0])\nprint(na_columns[na_columns>0]\/test_transaction_data.shape[0])","4ef0ea64":"# https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature\ntest_transaction_data['Transaction_dow'] = np.floor((test_transaction_data['TransactionDT'] \/ (3600 * 24) - 1) % 7)\ntest_transaction_data['Transaction_hour'] = np.floor(test_transaction_data['TransactionDT'] \/ 3600) % 24\ntransaction_data_columns = test_transaction_data.columns\nnumericCols = test_transaction_data._get_numeric_data().columns\ncategoricalCols = list(set(transaction_data_columns) - set(numericCols))\ntest_transaction_data[categoricalCols] = test_transaction_data[categoricalCols].replace({ np.nan:'missing'})\ntest_transaction_data[numericCols] = test_transaction_data[numericCols].replace({ np.nan:-1})","83744701":"identity_data_columns = test_identity_data.columns\nnumericCols = test_identity_data._get_numeric_data().columns\ncategoricalCols = list(set(identity_data_columns) - set(numericCols))\ntest_identity_data[categoricalCols] = test_identity_data[categoricalCols].replace({ np.nan:'missing'})\ntest_identity_data[numericCols] = test_identity_data[numericCols].replace({ np.nan:-1})\ntest_identity_data['id_31'] = test_identity_data['id_31'].str.replace('\\d+', '')\n\ntest_identity_data.loc[test_identity_data['id_30'].str.contains('Mac', na=False), 'id_30'] = 'mac'\ntest_identity_data.loc[test_identity_data['id_30'].str.contains('iOS', na=False), 'id_30'] = 'iOS'\ntest_identity_data.loc[test_identity_data['id_30'].str.contains('Android', na=False), 'id_30'] = 'android'\ntest_identity_data.loc[test_identity_data['id_30'].str.contains('Windows', na=False), 'id_30'] = 'Windows'\ntest_identity_data.loc[test_identity_data['id_30'].str.contains('Linux', na=False), 'id_30'] = 'Linux'\ntest_identity_data['device_name'] = test_identity_data['DeviceInfo'].str.split('\/', expand=True)[0]\n\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\ntest_identity_data.loc[test_identity_data['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\ntest_identity_data.loc[test_identity_data.device_name.isin(test_identity_data.device_name.value_counts()[test_identity_data.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"","752f3e41":"raw_test_data = pd.merge(test_transaction_data, test_identity_data, on='TransactionID', how='left')\ntransactionID = raw_test_data.loc[:,'TransactionID']\ndel test_identity_data,test_transaction_data","35be0043":"raw_test_data_columns = raw_test_data.columns\nnumericCols = raw_test_data._get_numeric_data().columns\ncategoricalCols = list(set(raw_test_data_columns) - set(numericCols))\nprint('The categorical columns in training data are: ',categoricalCols)\nraw_test_data[categoricalCols] = raw_test_data[categoricalCols].replace({ np.nan:'missing'})\nraw_test_data[numericCols] = raw_test_data[numericCols].replace({ np.nan:-1})\nraw_test_data = reduce_memory_usage(raw_test_data, deep=True, verbose=True)","b56669de":"test_data = raw_test_data.loc[:,featuresToUse]\n# New feature - decimal part of the transaction amount.\ntest_data['TransactionAmt_decimal'] = ((test_data['TransactionAmt'] - test_data['TransactionAmt'].astype(int)) * 1000).astype(int)","4d206bc0":"#https:\/\/www.kaggle.com\/artgor\/eda-and-models#Feature-engineering\ntest_data['TransactionAmt_to_mean_card1'] = test_data['TransactionAmt'] \/ test_data.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_mean_card4'] = test_data['TransactionAmt'] \/ test_data.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_mean_card5'] = test_data['TransactionAmt'] \/ test_data.groupby(['card5'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_mean_ProductCD'] = test_data['TransactionAmt'] \/ test_data.groupby(['ProductCD'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_mean_addr1'] = test_data['TransactionAmt'] \/ test_data.groupby(['addr1'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_mean_id31'] = test_data['TransactionAmt'] \/ test_data.groupby(['id_31'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_mean_devicename'] = test_data['TransactionAmt'] \/ test_data.groupby(['device_name'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_mean_dow'] = test_data['TransactionAmt'] \/ test_data.groupby(['Transaction_dow'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_mean_hour'] = test_data['TransactionAmt'] \/ test_data.groupby(['Transaction_hour'])['TransactionAmt'].transform('mean')\ntest_data['card1_card2'] = test_data['card1'].astype(str) + '_' + test_data['card2'].astype(str)\ntest_data['addr1_dist1'] = test_data['addr1'].astype(str) + '_' + test_data['dist1'].astype(str)\ntest_data['card1_addr1'] = test_data['card1'].astype(str) + '_' + test_data['addr1'].astype(str)\ntest_data['card1_addr2'] = test_data['card1'].astype(str) + '_' + test_data['addr2'].astype(str)\ntest_data['card2_addr1'] = test_data['card2'].astype(str) + '_' + test_data['addr1'].astype(str)\ntest_data['card2_addr2'] = test_data['card2'].astype(str) + '_' + test_data['addr2'].astype(str)\ntest_data['card4_addr1'] = test_data['card4'].astype(str) + '_' + test_data['addr1'].astype(str)\ntest_data['card4_addr2'] = test_data['card4'].astype(str) + '_' + test_data['addr2'].astype(str)\ntest_data['DeviceInfo_P_emaildomain'] = test_data['DeviceInfo'].astype(str) + '_' + test_data['P_emaildomain'].astype(str)\ntest_data['P_emaildomain_addr1'] = test_data['P_emaildomain'].astype(str) + '_' + test_data['addr1'].astype(str)\ntest_data['id01_addr1'] = test_data['id_01'].astype(str) + '_' + test_data['addr1'].astype(str)\ntest_data['TransactionAmt_to_std_card1'] = test_data['TransactionAmt'] \/ test_data.groupby(['card1'])['TransactionAmt'].transform('std')\ntest_data['TransactionAmt_to_std_card4'] = test_data['TransactionAmt'] \/ test_data.groupby(['card4'])['TransactionAmt'].transform('std')\ntest_data['TransactionAmt_to_std_card5'] = test_data['TransactionAmt'] \/ test_data.groupby(['card5'])['TransactionAmt'].transform('std')\ntest_data['TransactionAmt_to_std_ProductCD'] = test_data['TransactionAmt'] \/ test_data.groupby(['ProductCD'])['TransactionAmt'].transform('std')\ntest_data['TransactionAmt_to_std_addr1'] = test_data['TransactionAmt'] \/ test_data.groupby(['addr1'])['TransactionAmt'].transform('std')\ntest_data['TransactionAmt_to_std_id31'] = test_data['TransactionAmt'] \/ test_data.groupby(['id_31'])['TransactionAmt'].transform('std')\ntest_data['TransactionAmt_to_std_devicename'] = test_data['TransactionAmt'] \/ test_data.groupby(['device_name'])['TransactionAmt'].transform('std')\ntest_data['TransactionAmt_to_std_dow'] = test_data['TransactionAmt'] \/ test_data.groupby(['Transaction_dow'])['TransactionAmt'].transform('std')\ntest_data['TransactionAmt_to_std_hour'] = test_data['TransactionAmt'] \/ test_data.groupby(['Transaction_hour'])['TransactionAmt'].transform('std')","11428fa5":"test_data['TransactionAmt_to_mean_C1'] = test_data['TransactionAmt'] \/ test_data.groupby(['C1'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_mean_C3'] = test_data['TransactionAmt'] \/ test_data.groupby(['C3'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_mean_C5'] = test_data['TransactionAmt'] \/ test_data.groupby(['C5'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_mean_C13'] = test_data['TransactionAmt'] \/ test_data.groupby(['C13'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_mean_D15'] = test_data['TransactionAmt'] \/ test_data.groupby(['D15'])['TransactionAmt'].transform('mean')\ntest_data['TransactionAmt_to_std_C1'] = test_data['TransactionAmt'] \/ test_data.groupby(['C1'])['TransactionAmt'].transform('std')\ntest_data['TransactionAmt_to_std_C3'] = test_data['TransactionAmt'] \/ test_data.groupby(['C3'])['TransactionAmt'].transform('std')\ntest_data['TransactionAmt_to_std_C5'] = test_data['TransactionAmt'] \/ test_data.groupby(['C5'])['TransactionAmt'].transform('std')\ntest_data['TransactionAmt_to_std_D15'] = test_data['TransactionAmt'] \/ test_data.groupby(['D15'])['TransactionAmt'].transform('std')","555f64ae":"goodCategoricalVars.append('card1_addr1')\ngoodCategoricalVars.append('card1_addr2')\ngoodCategoricalVars.append('card2_addr1')\ngoodCategoricalVars.append('card2_addr2')\ngoodCategoricalVars.append('card4_addr1')\ngoodCategoricalVars.append('card4_addr2')\ngoodCategoricalVars.append('DeviceInfo_P_emaildomain')\ngoodCategoricalVars.append('P_emaildomain_addr1')\ngoodCategoricalVars.append('id01_addr1')\ngoodCategoricalVars.append('card1_card2')\ngoodCategoricalVars.append('device_name')\ngoodCategoricalVars.append('addr1_dist1')\nfrom sklearn.preprocessing import LabelEncoder\nfor i_cat in goodCategoricalVars:\n    le = LabelEncoder()\n    curData = pd.concat([train_data.loc[:,i_cat],test_data.loc[:,i_cat]],axis = 0)\n    le.fit(curData)\n    train_data.loc[:,i_cat] = le.transform(train_data.loc[:,i_cat])   \n    test_data.loc[:,i_cat] = le.transform(test_data.loc[:,i_cat])","0cdb5d19":"train_data.columns","dccaf36c":"train_data.isna().sum()","c693104d":"# train_data=train_data.drop(columns=['TransactionAmt_to_std_card1','TransactionAmt_to_std_D15','TransactionAmt_to_std_C5','TransactionAmt_to_std_C3','TransactionAmt_to_std_C1',\n#                         'TransactionAmt_to_std_id31','TransactionAmt_to_std_addr1','TransactionAmt_to_std_card5'])\n# test_data=test_data.drop(columns=['TransactionAmt_to_std_card1','TransactionAmt_to_std_D15','TransactionAmt_to_std_C5','TransactionAmt_to_std_C3','TransactionAmt_to_std_C1',\n#                         'TransactionAmt_to_std_id31','TransactionAmt_to_std_addr1','TransactionAmt_to_std_card5'])","daafe531":"# from sklearn.feature_selection import SelectKBest\n# from sklearn.feature_selection import f_classif","e64d3270":"# np.where(train_data.values >= np.finfo(np.float64).max)\n# train_data = train_data.fillna(0)","78d665ed":"# train_data = train_data[train_data.replace([np.inf, -np.inf], np.nan).notnull().all(axis=1)]","a51e87ea":"# train_data = train_data.fillna(0)","e8d2f52c":"# select = SelectKBest(f_classif, k=100)\n# train_new = select.fit_transform(train_data, target_data)\n# test_new = select.transform(test_data)","e1d7d26b":"# train_data = pd.DataFrame(train_new)\n# test_data = pd.DataFrame(test_new)","7c98f2dd":"# train_data = train_new","3f5d438a":"print('The shape of training data is:',np.shape(train_data))\ndel raw_test_data","75f9dbdf":"from sklearn.model_selection import StratifiedKFold,GroupKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\n\n#group_kfold = GroupKFold(n_splits=5)\ncv = StratifiedKFold(n_splits=5, random_state=37, shuffle=True)","b9836f3e":"def compute_roc_auc(clf,index):\n    y_predict = clf.predict_proba(train_data.iloc[index])[:,1]\n    fpr, tpr, thresholds = roc_curve(target_data.iloc[index], y_predict)\n    auc_score = auc(fpr, tpr)\n    return fpr, tpr, auc_score","efca8f87":"# params = {'bagging_fraction': 0.7982116702024386, \n#           'feature_fraction': 0.3, \n#           'max_depth': int(49.17611603427576), \n#           'min_child_weight': 3.2852905549011155, \n#           'min_data_in_leaf': int(31.03480802715621), \n#           'n_estimators': 8000,#int(1491.3676131788188), \n#           'num_leaves': int(52.851307790411965), \n#           'reg_alpha': 0.45963319421692145, \n#           'reg_lambda': 0.6591286807489907, \n#           'metric':'auc',\n#           'boosting_type': 'gbdt', \n#           'colsample_bytree':.8, \n#           'subsample':.9, \n#           'min_split_gain':.01, \n#           'max_bin':127, \n#           'bagging_freq':5, \n#           'learning_rate':0.01 , \n#           'early_stopping_rounds':100 }\n\nparams = {'bagging_fraction': 0.8, \n          'feature_fraction': 0.3, \n          'max_depth': 50, \n          'min_child_weight': 3.2852905549011155, \n          'min_data_in_leaf': 30, \n          'n_estimators': 1500,#int(1491.3676131788188), \n          'num_leaves': 50, \n          'reg_alpha': 0.45963319421692145, \n          'reg_lambda': 0.6591286807489907, \n          'metric':'auc',\n          'boosting_type': 'gbdt', \n          'colsample_bytree':.8, \n          'subsample':.9, \n          'min_split_gain':.01, \n          'max_bin':255, \n          'bagging_freq':5, \n          'learning_rate':0.01 , \n          'early_stopping_rounds':100 }","d29c3e55":"fprs_lgb, tprs_lgb, scores_lgb = [], [], []\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = train_data.columns     \npredictions = np.zeros(len(test_data))\nfor (train, test), i in zip(cv.split(train_data, target_data), range(5)):\n    lgb_best = LGBMClassifier(boosting = params['boosting_type'],n_estimators =  params['n_estimators'],\n                     learning_rate =  params['learning_rate'],num_leaves =  params['num_leaves'],\n                     colsample_bytree = params['colsample_bytree'],subsample =  params['subsample'],\n                     max_depth =  params['max_depth'],reg_alpha =  params['reg_alpha'],\n                     reg_lambda =  params['reg_lambda'],min_split_gain =  params['min_split_gain'],\n                     min_child_weight =  params['min_child_weight'],max_bin =  params['max_bin'],\n                     bagging_freq =  params['bagging_freq'],feature_fraction =  params['feature_fraction'],\n                     bagging_fraction =  params['bagging_fraction'],min_data_in_leaf = params['min_data_in_leaf'],\n                             early_stopping_rounds = params['early_stopping_rounds'])\n    lgb_best.fit(train_data.iloc[train,:], target_data.iloc[train],\n                 eval_set = [(train_data.iloc[train,:], target_data.iloc[train]), \n                             (train_data.iloc[test,:], target_data.iloc[test])],eval_metric='auc',verbose = 200)\n    feature_importances['fold_{}'.format(i + 1)] = lgb_best.feature_importances_\n    _, _, auc_score_train = compute_roc_auc(lgb_best,train)\n    fpr, tpr, auc_score = compute_roc_auc(lgb_best,test)\n    scores_lgb.append((auc_score_train, auc_score))\n    fprs_lgb.append(fpr)\n    tprs_lgb.append(tpr) \n    predictions += lgb_best.predict_proba(test_data, num_iteration=lgb_best.best_iteration_)[:,1]\/cv.n_splits","71406ac0":"# plt.figure()\n# lw = 2\n# colors = ['black','red','blue','darkorange','green']\n# for i in range(0,5):\n#     plt.plot(fprs_lgb[i], tprs_lgb[i], color=colors[i],\n#              lw=lw, label='ROC curve (area = %0.5f)' % scores_lgb[i][1])\n    \n# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n# plt.xlabel('False Positive Rate')\n# plt.ylabel('True Positive Rate')\n# plt.legend(loc=\"lower right\")\n# plt.show()","5566afb5":"scores_lgb","d581ef50":"print('Mean AUC:', np.mean(scores_lgb,axis = 0))","666cedaf":"feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(cv.n_splits)]].mean(axis=1)\n# plt.figure(figsize=(16, 16))\n# sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\n# plt.title('50 TOP feature importance over {} folds average'.format(cv.n_splits));","d3ba2c0a":"del train_data,target_data","f86a54fc":"data = {'TransactionID':transactionID,'isFraud':predictions}\nsubmissionDF = pd.DataFrame(data)\nsubmissionDF.to_csv('submission.csv',index=False)","546f0151":"submissionDF.head()","7c995906":"# Bounded region of parameter space\nbounds_LGB = {\n    'num_leaves': (31, 500), \n    'min_data_in_leaf': (20, 200),\n    'bagging_fraction' : (0.1, 0.9),\n    'feature_fraction' : (0.1, 0.9),\n    #'learning_rate': (0.01, 0.3),\n    'min_child_weight': (1, 4),   \n    'reg_alpha': (0.2,2), \n    'reg_lambda': (0.2,2),\n    'max_depth':(-1,50),\n    'n_estimators':(750,1800)\n}\ntrain_ids = []\ntest_ids = []\nfor (train, test), i in zip(cv.split(train_data,target_data), range(5)):\n    train_ids.append(train)\n    test_ids.append(test)\nfrom bayes_opt import BayesianOptimization\n\ndef LGB_bayesian(num_leaves,bagging_fraction,feature_fraction,min_child_weight,\n                 min_data_in_leaf,max_depth,n_estimators,reg_alpha,reg_lambda):\n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth = int(max_depth)\n    n_estimators = int(n_estimators)\n    min_child_weight = int(min_child_weight)\n\n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n    assert type(n_estimators) == int\n    assert type(min_child_weight) == int\n    param = {'num_leaves': num_leaves,'min_data_in_leaf': min_data_in_leaf,'min_child_weight': min_child_weight,'bagging_fraction' : bagging_fraction,\n             'feature_fraction' : feature_fraction,'max_depth': max_depth,'reg_alpha': reg_alpha,'reg_lambda': reg_lambda,\n              'objective': 'binary','boosting_type': 'gbdt','colsample_bytree':.8,'subsample':.9,'min_split_gain':.01,'max_bin':127,\n             'bagging_freq':5,'learning_rate':0.1,'metric':'auc','n_estimators':n_estimators,'min_data_in_leaf':min_data_in_leaf} \n    lgb_bayes = LGBMClassifier(boosting = param['boosting_type'],n_estimators =  param['n_estimators'],\n                     learning_rate =  param['learning_rate'],num_leaves =  param['num_leaves'],\n                     colsample_bytree = param['colsample_bytree'],subsample =  param['subsample'],\n                     max_depth =  param['max_depth'],reg_alpha =  param['reg_alpha'],\n                     reg_lambda =  param['reg_lambda'],min_split_gain =  param['min_split_gain'],\n                     min_child_weight =  param['min_child_weight'],max_bin =  param['max_bin'],\n                     bagging_freq =  param['bagging_freq'],feature_fraction =  param['feature_fraction'],\n                     bagging_fraction =  param['bagging_fraction'],min_data_in_leaf = param['min_data_in_leaf'])\n    all_auc_val_score = []\n    for i in range(cv.n_splits):\n            lgb_bayes.fit(train_data.iloc[train_ids[i],:], target_data.iloc[train_ids[i]],eval_metric='auc')\n            fpr, tpr, auc_score = compute_roc_auc(lgb_bayes,test_ids[i])\n            all_auc_val_score.append(auc_score)    \n    return np.mean(all_auc_val_score)\nlightGBM_bo = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)\nprint(lightGBM_bo.space.keys)\ninit_points = 10\nn_iter = 15\nprint('-' * 130)\n\nlightGBM_bo.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\nprint(lightGBM_bo.max['target'])\nlightGBM_bo.max['params']\nparams = {\n        'n_estimators': int(lightGBM_bo.max['params']['n_estimators']), \n        'num_leaves': int(lightGBM_bo.max['params']['num_leaves']), \n        'min_child_weight': lightGBM_bo.max['params']['min_child_weight'],\n        'min_data_in_leaf': int(lightGBM_bo.max['params']['min_data_in_leaf']),\n        'bagging_fraction': lightGBM_bo.max['params']['bagging_fraction'], \n        'feature_fraction': lightGBM_bo.max['params']['feature_fraction'],\n        'reg_lambda': lightGBM_bo.max['params']['reg_lambda'],\n        'reg_alpha': lightGBM_bo.max['params']['reg_alpha'],\n        'max_depth': int(lightGBM_bo.max['params']['max_depth']), \n        'metric':'auc',\n        'boosting_type': 'dart',\n        'colsample_bytree':.8,'subsample':.9,\n        'min_split_gain':.01,\n        'max_bin':127,\n        'bagging_freq':5,\n        'learning_rate':0.1    \n    }","88483fdc":"looks like merging the two data tables has created a lot of missing values. let us first try to identify good features from these.","fe550c92":"Now that we have identified good numeric variables, let us use just the missing data as benchmark for the categorical variables","47aea20a":"test_data['card1_counts'] = test_data['card1'].map(test_data['card1'].value_counts(dropna=False))\ntest_data['card3_counts'] = test_data['card3'].map(test_data['card3'].value_counts(dropna=False))\ntest_data['card5_counts'] = test_data['card5'].map(test_data['card5'].value_counts(dropna=False))\ntest_data['card2_counts'] = test_data['card2'].map(test_data['card2'].value_counts(dropna=False))","470ed4b1":"train_data['card1_counts'] = train_data['card1'].map(train_data['card1'].value_counts(dropna=False))\ntrain_data['card3_counts'] = train_data['card3'].map(train_data['card3'].value_counts(dropna=False))\ntrain_data['card5_counts'] = train_data['card5'].map(train_data['card5'].value_counts(dropna=False))\ntrain_data['card2_counts'] = train_data['card2'].map(train_data['card2'].value_counts(dropna=False))"}}