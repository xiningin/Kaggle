{"cell_type":{"adf03948":"code","a59db01c":"code","d5db9e65":"code","36c4829f":"code","767439d7":"code","d10c79f9":"code","7cd95dc7":"code","38895099":"code","7a3adc48":"code","604679f0":"code","d10cc354":"code","e784f1ce":"code","72c7e534":"code","d8b6f4b8":"code","d8f0f5c0":"code","de18f170":"code","59580207":"code","bc5e9314":"code","0ac51908":"code","4f825d71":"code","cec5db3e":"code","3ef1c727":"code","e8347c56":"code","a6cb79fc":"code","18db3a57":"code","824361fe":"code","cf4c0fff":"code","f6d25e15":"code","df134a9f":"code","a67de9a3":"code","477ed58a":"code","f143e8b4":"code","f159497b":"code","9d3b43c5":"code","1acd667c":"code","fdf6bd3b":"code","3281e454":"code","86f9fb80":"code","0782ba50":"code","886bf488":"code","152de537":"code","4427a4ff":"code","79c3c9c8":"code","d9ebeacc":"code","b27547c8":"code","d32773ff":"code","cc0f1e2c":"code","85e040e1":"code","0ed8e6ef":"code","9005cfc6":"code","22ebe54d":"code","15b19cef":"code","67e9f030":"code","74ae1548":"code","8f022392":"code","1b670ae1":"code","55593f7a":"code","b5dbc55b":"code","ffc5b48d":"code","ac5b1497":"code","84694e83":"code","246c3a71":"code","4237cb96":"code","8e069d4b":"code","01ca84d5":"code","84773054":"code","4ac2634d":"code","6d6b6bca":"code","267d60b1":"code","9ab44e06":"code","58f6ef5e":"code","f6667eec":"code","8557f8f4":"code","0e2b9a0d":"code","5b5f694c":"code","46b56194":"code","c40e1e34":"code","39e2334c":"code","10e3a808":"code","eee8af2a":"code","6df5df21":"code","263d7a63":"code","6ad4022d":"code","c2f75fee":"code","0c1f9280":"code","7f23a913":"code","49743332":"code","04ffc6ad":"code","2481276e":"code","81d0c5ed":"code","50978ca5":"code","843b300e":"code","e0ebd32f":"code","95e56825":"code","bbea2ea0":"code","39b68028":"code","b0f33187":"code","ac63b2e7":"code","6564e678":"code","6d302819":"code","db859794":"code","d5fdb6f2":"markdown","bc310442":"markdown","347ce871":"markdown","18c5812e":"markdown","85eae29c":"markdown","1374c6bc":"markdown","3ba01084":"markdown","29ff788f":"markdown","99fa6459":"markdown","56e7a6b4":"markdown","916be620":"markdown","ae150a58":"markdown","cdd01f11":"markdown","e7943788":"markdown","a6cd4aa1":"markdown","b3555a00":"markdown","a292fc9b":"markdown","7c389947":"markdown","11c3b184":"markdown","e5607cf6":"markdown","5e87706f":"markdown"},"source":{"adf03948":"##loading various helpful packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# for data visulization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for modeling estimators\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as gbm\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import  OneHotEncoder as ohe\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.compose import ColumnTransformer as ct\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import  DecisionTreeClassifier as dt\nimport lightgbm as lgb\n\n\n\n#for data processing\nfrom sklearn.model_selection import train_test_split\n\n#for tuning parameters\nfrom bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\n\n# Misc.\nimport os\nimport time\nimport gc\n\n\n#Other Libraries\n\nimport random\nfrom scipy.stats import uniform\nimport warnings\n","a59db01c":"# Read in data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nids=test['Id']","d5db9e65":"train.head(3)","36c4829f":"test.head(3)","767439d7":"train.shape, test.shape","d10c79f9":"train.info() ","7cd95dc7":"train.plot(figsize = (12,10))","38895099":"sns.countplot(\"Target\", data=train)","7a3adc48":" sns.countplot(x=\"r4t3\",hue=\"Target\",data=train)","604679f0":"sns.countplot(x=\"hhsize\",hue=\"Target\",data=train)","d10cc354":"from pandas.plotting import scatter_matrix\nscatter_matrix(train.select_dtypes('float'), alpha=0.2, figsize=(26, 20), diagonal='kde')","e784f1ce":"from collections import OrderedDict\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n# Iterate through the float columns\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax = plt.subplot(4, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","72c7e534":"train.select_dtypes('object').head()","d8b6f4b8":"yes_no_map = {'no':0,'yes':1}\ntrain['dependency'] = train['dependency'].replace(yes_no_map).astype(np.float32)\ntrain['edjefe'] = train['edjefe'].replace(yes_no_map).astype(np.float32)\ntrain['edjefa'] = train['edjefa'].replace(yes_no_map).astype(np.float32)","d8f0f5c0":"yes_no_map = {'no':0,'yes':1}\ntest['dependency'] = test['dependency'].replace(yes_no_map).astype(np.float32)\ntest['edjefe'] = test['edjefe'].replace(yes_no_map).astype(np.float32)\ntest['edjefa'] = test['edjefa'].replace(yes_no_map).astype(np.float32)","de18f170":"train[[\"dependency\",\"edjefe\",\"edjefa\"]].describe()","59580207":"train[[\"dependency\",\"edjefe\",\"edjefa\"]].hist()","bc5e9314":"plt.figure(figsize = (16, 12))\n\n# Iterate through the float columns\nfor i, col in enumerate(['dependency', 'edjefa', 'edjefe']):\n    ax = plt.subplot(3, 1, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","0ac51908":"# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","4f825d71":"train['v18q1'] = train['v18q1'].fillna(0)\ntest['v18q1'] = test['v18q1'].fillna(0)\ntrain['v2a1'] = train['v2a1'].fillna(0)\ntest['v2a1'] = test['v2a1'].fillna(0)\n\ntrain['rez_esc'] = train['rez_esc'].fillna(0)\ntest['rez_esc'] = test['rez_esc'].fillna(0)\ntrain['SQBmeaned'] = train['SQBmeaned'].fillna(0)\ntest['SQBmeaned'] = test['SQBmeaned'].fillna(0)\ntrain['meaneduc'] = train['meaneduc'].fillna(0)\ntest['meaneduc'] = test['meaneduc'].fillna(0)","cec5db3e":"#Checking for missing values again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","3ef1c727":"#Checking for missing values again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(test.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","e8347c56":"train.drop(['Id','idhogar'], inplace = True, axis =1)\n\ntest.drop(['Id','idhogar'], inplace = True, axis =1)","a6cb79fc":"train.shape, test.shape","18db3a57":"y = train.iloc[:,140]\ny.unique()","824361fe":"X = train.iloc[:,1:141]\nX.shape","cf4c0fff":"X_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.2)","f6d25e15":"modelgbm=gbm()","df134a9f":"start = time.time()\nmodelgbm = modelgbm.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","a67de9a3":"classes = modelgbm.predict(X_test)\n\nclasses","477ed58a":"(classes == y_test).sum()\/y_test.size ","f143e8b4":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    gbm(\n               # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        \n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 2                # Number of cross-validation folds\n)","f159497b":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","9d3b43c5":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","1acd667c":"modelgbmTuned=gbm(\n               max_depth=31,\n               max_features=29,\n               min_weight_fraction_leaf=0.02067,\n               n_estimators=489)","fdf6bd3b":"start = time.time()\nmodelgbmTuned = modelgbmTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","3281e454":"ygbm=modelgbmTuned.predict(X_test)\nygbmtest=modelgbmTuned.predict(test)","86f9fb80":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","0782ba50":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","886bf488":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","152de537":"modelneigh = KNeighborsClassifier(n_neighbors=7)","4427a4ff":"start = time.time()\nmodelneigh = modelneigh.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","79c3c9c8":"classes = modelneigh.predict(X_test)\n\nclasses","d9ebeacc":"(classes == y_test).sum()\/y_test.size ","b27547c8":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    KNeighborsClassifier(\n       n_neighbors=7         # No need to tune this parameter value\n      ),\n    {\"metric\": [\"euclidean\", \"cityblock\"]},\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n   )","d32773ff":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","cc0f1e2c":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","85e040e1":"modelneighTuned = KNeighborsClassifier(n_neighbors=7,\n               metric=\"cityblock\")","0ed8e6ef":"start = time.time()\nmodelneighTuned = modelneighTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","9005cfc6":"yneigh=modelneighTuned.predict(X_test)","22ebe54d":"yneightest=modelneighTuned.predict(test)","15b19cef":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","67e9f030":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","74ae1548":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","8f022392":"modeletf = ExtraTreesClassifier()","1b670ae1":"start = time.time()\nmodeletf = modeletf.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","55593f7a":"classes = modeletf.predict(X_test)\n\nclasses","b5dbc55b":"(classes == y_test).sum()\/y_test.size","ffc5b48d":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    ExtraTreesClassifier( ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {   'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n)","ac5b1497":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","84694e83":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","246c3a71":"modeletfTuned=ExtraTreesClassifier(criterion=\"entropy\",\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=100)","4237cb96":"start = time.time()\nmodeletfTuned = modeletfTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","8e069d4b":"yetf=modeletfTuned.predict(X_test)\nyetftest=modeletfTuned.predict(test)","01ca84d5":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","84773054":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","4ac2634d":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","6d6b6bca":"modelxgb=XGBClassifier()","267d60b1":"start = time.time()\nmodelxgb = modelxgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","9ab44e06":"classes = modelxgb.predict(X_test)\n\nclasses","58f6ef5e":"(classes == y_test).sum()\/y_test.size ","f6667eec":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    XGBClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 300),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","8557f8f4":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","0e2b9a0d":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","5b5f694c":"modelxgbTuned=XGBClassifier(criterion=\"gini\",\n               max_depth=85,\n               max_features=47,\n               min_weight_fraction_leaf=0.035997,\n               n_estimators=178)","46b56194":"start = time.time()\nmodelxgbTuned = modelxgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","c40e1e34":"#yxgb=modelxgbTuned.predict(X_test)\n#yxgbtest=modelxgbTuned.predict(test)","39e2334c":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","10e3a808":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","eee8af2a":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","6df5df21":"modellgb = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)\n","263d7a63":"start = time.time()\nmodellgb = modellgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","6ad4022d":"classes = modellgb.predict(X_test)\n\nclasses","c2f75fee":"(classes == y_test).sum()\/y_test.size ","0c1f9280":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    lgb.LGBMClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","7f23a913":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","49743332":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","04ffc6ad":"modellgbTuned = lgb.LGBMClassifier(criterion=\"entropy\",\n               max_depth=35,\n               max_features=14,\n               min_weight_fraction_leaf=0.18611,\n               n_estimators=148)","2481276e":"start = time.time()\nmodellgbTuned = modellgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","81d0c5ed":"ylgb=modellgbTuned.predict(X_test)\nylgbtest=modellgbTuned.predict(test)","50978ca5":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","843b300e":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","e0ebd32f":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","95e56825":"NewTrain = pd.DataFrame()\n#NewTrain['yrf'] = yrf.tolist()\nNewTrain['yetf'] = yetf.tolist()\nNewTrain['yneigh'] = yneigh.tolist()\nNewTrain['ygbm'] = ygbm.tolist()\n#NewTrain['yxgb'] = yxgb.tolist()\nNewTrain['ylgb'] = ylgb.tolist()\n\nNewTrain.head(5), NewTrain.shape","bbea2ea0":"NewTest = pd.DataFrame()\n#NewTest['yrf'] = yrftest.tolist()\nNewTest['yetf'] = yetftest.tolist()\nNewTest['yneigh'] = yneightest.tolist()\nNewTest['ygbm'] = ygbmtest.tolist()\n#NewTest['yxgb'] = yxgbtest.tolist()\nNewTest['ylgb'] = ylgbtest.tolist()\nNewTest.head(5), NewTest.shape","39b68028":"NewModel=rf(criterion=\"entropy\",\n               max_depth=87,\n               max_features=4,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=600)","b0f33187":"start = time.time()\nNewModel = NewModel.fit(NewTrain, y_test)\nend = time.time()\n(end-start)\/60","ac63b2e7":"ypredict=NewModel.predict(NewTest)\nypredict","6564e678":"#submit=pd.DataFrame({'Id': ids, 'Target': ylgbtest})\nsubmit=pd.DataFrame({'Id': ids, 'Target': ypredict})\nsubmit.head(5)","6d302819":"submit.to_csv('submit.csv', index=False)","db859794":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = ypredict\nsub.drop(sub.columns[[1]], axis=1, inplace=True)\nsub.to_csv('submission.csv',index=False)","d5fdb6f2":"BUILDING a new dataset with predicted results with all these models","bc310442":"Performing tuning using Bayesian Optimization.","347ce871":"Fill in missing values (NULL values) using 1 for yes and 0 for no","18c5812e":"Performing tuning using Bayesian Optimization.","85eae29c":"Splitting the data into train & test","1374c6bc":"Performing tuning using Bayesian Optimization.","3ba01084":"Modelling with ExtraTreeClassifier","29ff788f":"Perform data visualization\nThe aim is to plot graphs to understand the distribution and relationships of attributes.","99fa6459":"Modelling with XGBoosterClassifier","56e7a6b4":"Modelling with KNeighborsClassifier","916be620":"Performing tuning using Bayesian Optimization","ae150a58":"The below are Distribution plots using seaborn","cdd01f11":"Dividing the data into predictors & target","e7943788":"Modelling with Light Gradient Booster","a6cd4aa1":"Feature-Target Relationships\n\nThe next important relationship to explore is that of each attribute to the \"Target\" attribute.","b3555a00":"Many social programs have a hard time making sure the right people are given enough aid. It\u2019s especially tricky when a program focuses on the poorest segment of the population. The world\u2019s poorest typically can\u2019t provide the necessary income and expense records to prove that they qualify.\n\nIn Latin America, one popular method uses an algorithm to verify income qualification. It\u2019s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family\u2019s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.\n\nWhile this is an improvement, accuracy remains a problem as the region\u2019s population grows and poverty declines.\n\nExpected Outcome of the Project\n\na. Explore data and perform data visualization\nb. Fill in missing values (NULL values) either using mean or median (if the attribute is numeric) or most-frequently occurring value if the attribute is 'object' or categorical.\nb. Perform feature engineering, may be using some selected features and only from numeric features.\nc. Scale numeric features, AND IF REQUIRED, perform One HOT Encoding of categorical features\nd. IF number of features is very large, please do not forget to do PCA.\ne. Select some estimators for your work. May be select some (or all) of these:\n\n        GradientBoostingClassifier\n        RandomForestClassifier\n        KNeighborsClassifier\n        ExtraTreesClassifier\n        XGBoost\n        LightGBM\n   \n   First perform modeling with default parameter values and get accuracy.\n\nf. Then perform tuning using Bayesian Optimization. ","a292fc9b":"Dropping unnecesary columns","7c389947":"Modeling\n\nModelling with GradientBoostingClassifier","11c3b184":"Converting categorical objects into numericals","e5607cf6":"Performing tuning using Bayesian Optimization.","5e87706f":"Feature-Feature Relationships\n\nThe final important relationship to explore is that of the relationships between the attributes.\n\nWe can review the relationships between attributes by looking at the distribution of the interactions of each pair of attributes.\n\nThis uses a built function to create a matrix of scatter plots of all attributes versus all attributes. The diagonal where each attribute would be plotted against itself shows the Kernel Density Estimation of the attribute instead."}}