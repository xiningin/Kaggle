{"cell_type":{"574e791c":"code","3f2e1db7":"code","c57be38e":"code","5f2a3a71":"code","6f10acd3":"code","70434b65":"code","570b26ca":"code","bdedfd21":"code","9ba42e51":"markdown","22221549":"markdown","6834281a":"markdown","82eee503":"markdown","23cf00f1":"markdown","2bd392ab":"markdown","7e180ce2":"markdown"},"source":{"574e791c":"import pandas as pd\nimport numpy as np\nimport datatable as dt\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nimport tensorflow as tf\n\nplot = False # Plot model or plot summary\nVERBOSE = False # Show all outputs","3f2e1db7":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","c57be38e":"train_df = dt.fread(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest_df = dt.fread(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\ntest_df = reduce_memory_usage(test_df.to_pandas())\ntrain_df = reduce_memory_usage(train_df.to_pandas())\n\nINPUT_SHAPE = test_df.shape[1:] # Used to decide first layer of nn\nNUM_CLASSES = train_df[\"Cover_Type\"].nunique() # For output layer of nn\n\n# Remove sample with cover_type = 5\nidx_to_drop5 = train_df[train_df[\"Cover_Type\"] == 5].index\nprint(f\"Nr of cover_type = 5: {len(idx_to_drop5)}\")\ntrain_df.drop(idx_to_drop5,\n              axis=0,\n              inplace=True)\n\n# Very few is 4 aswell\n\"\"\"idx_to_drop4 = train_df[train_df[\"Cover_Type\"] == 4].index\nprint(f\"Nr of cover_type = 4: {len(idx_to_drop4)}\")\ntrain_df.drop(idx_to_drop4,\n              axis=0,\n              inplace=True)\"\"\"\n\n\nencoder = LabelEncoder()\ntrain_df[\"Cover_Type\"] = encoder.fit_transform(train_df[\"Cover_Type\"])\n\nbool_features = [i for i in train_df.columns if \"area\" in i.lower() or \"soil\" in i.lower()]\ntest_df[bool_features] = test_df[bool_features].astype(np.int8)\ntrain_df[bool_features] = train_df[bool_features].astype(np.int8)\n","5f2a3a71":"cols_to_scale = train_df.loc[:,[(train_df[col] > 7).any() for col in train_df.columns]].columns\nprint(f\"Scaled Columns: {cols_to_scale}\\n\\n  \\\nNumber of scaled Columns: {len(cols_to_scale)}\")\n\nscaler = RobustScaler()\ntrain_df[cols_to_scale] = scaler.fit_transform(train_df[cols_to_scale])\ntest_df[cols_to_scale] = scaler.fit_transform(test_df[cols_to_scale])\n\ny = train_df.pop(\"Cover_Type\").values\nX = train_df.values","6f10acd3":"reduce_lr = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=2,\n    verbose=VERBOSE\n)\nearly_stop = EarlyStopping(\n    monitor=\"val_accuracy\",\n    patience=15,\n    restore_best_weights=True,\n    verbose=True # Always show on which fold it stopped early\n)\ncallbacks = [reduce_lr, early_stop]","70434b65":"def build_model():\n    # To run on TPU\n    build_with_TPU = False\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        BATCH_SIZE = 1024\n        # print(f\"Running on TPU: {tpu.master()}\")\n        # print(f\"Batch Size on TPU: {BATCH_SIZE}\")\n        build_with_TPU = True\n    except ValueError:\n        BATCH_SIZE = 1024\n        # print(\"Not running on TPU\")\n        # strategy = tf.distribute.get_strategy()\n        # BATCH_SIZE = 512\n        # print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n        # print(f\"Batch Size: {BATCH_SIZE}\")\n        \n    if build_with_TPU:\n        with strategy.scope():\n            model = Sequential([\n                Dense(units=300, kernel_initializer='random_normal', activation='gelu',\n                      input_shape=INPUT_SHAPE),\n                BatchNormalization(),\n                Dense(units=200, kernel_initializer='random_normal', activation='gelu'),\n                BatchNormalization(),\n                Dense(units=100, kernel_initializer='random_normal', activation='gelu'),\n                BatchNormalization(),\n                Dense(units=30, kernel_initializer='random_normal', activation='gelu'),\n                BatchNormalization(),\n                Dense(units=6, activation=\"softmax\")\n            ])\n            model.compile(\n                optimizer='adam',\n                loss = 'sparse_categorical_crossentropy',\n                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n                steps_per_execution=32 # Just a random value, don't know what to use here\n            )\n    else:\n        model = Sequential([\n                Dense(units=300, kernel_initializer='random_normal', activation='gelu',\n                      input_shape=INPUT_SHAPE),\n                BatchNormalization(),\n                Dense(units=200, kernel_initializer='random_normal', activation='gelu'),\n                BatchNormalization(),\n                Dense(units=100, kernel_initializer='random_normal', activation='gelu'),\n                BatchNormalization(),\n                Dense(units=30, kernel_initializer='random_normal', activation='gelu'),\n                BatchNormalization(),\n                Dense(units=6, activation=\"softmax\")\n            ])\n        model.compile(\n            optimizer='adam',\n            loss = 'sparse_categorical_crossentropy',\n            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n        )\n            \n    return model\n\nif plot:\n    plot_model(\n        build_model(),\n        show_shapes=True,\n        show_layer_names=True\n    )\nelse:\n    build_model().summary()","570b26ca":"print(\"Num GPUs available: \", len(tf.config.list_physical_devices('GPU')))\n\nFOLDS = 5\nEPOCHS = 200\nBATCH_SIZE = 2048\nSTEPS_PER_EPOCH = 4*BATCH_SIZE # Not used, chosen if wanted faster epochs\ntest_preds = np.zeros((1,1))\nscores = []\n\ncv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=0)\n\nfor fold, (train_idx, test_idx) in enumerate(cv.split(X,y), start=1):\n    X_train, X_val = X[train_idx], X[test_idx]\n    y_train, y_val = y[train_idx], y[test_idx]\n\n    model = build_model()\n    model.fit(\n        X_train,\n        y_train,\n        validation_data=(X_val, y_val),\n        # steps_per_epoch=STEPS_PER_EPOCH,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        callbacks=callbacks,\n        verbose=VERBOSE\n    )\n\n    y_pred = np.argmax(model.predict(X_val), axis=1)\n\n    score = accuracy_score(y_val, y_pred)\n    print(f\"Fold {fold}\/{FOLDS} Validation Accuracy: {score}\")\n    scores.append(score)\n\n    test_preds = test_preds + model.predict(test_df)\n    \nprint(f\"\\n\\nMean accuracy over all folds: {np.mean(scores)}\")","bdedfd21":"sample = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\npreds = np.argmax(test_preds, axis=1)\npreds = encoder.inverse_transform(preds)\n\nsample.Cover_Type = preds\nsample.to_csv(\"Submission.csv\", index=False)","9ba42e51":"# My application of a simple neural net on playground december 2021\n### Please let me know of any improvements, I'm here to learn\n\n### Ideas for improvement\n* Feature engineering, Cover_Type = 5 is only 1 sample, remove? DONE\n* Encode using sklearn labelencoder (need to use encoder.inverse_transform for test preds later) DONE\n* Scale data using sklearn robustscaler DONE\n* Plot model using tf.keras.utils plot_model\n* Use some tool to do feature importance\n* Can run on TPU, DONE\n* Get lower TPU idle time. Does anyone have any idea how?\n* Would be interesting to see if more folds give more accuracy\n* Try not removing cover_type = 4\n\nUsed https:\/\/www.kaggle.com\/gulshanmishra\/tps-dec-21-tensorflow-nn-feature-engineering as inspiration, please go give that notebook a thumbs up\n","22221549":"## Function to reduce memory of dataframes","6834281a":"## Functions to use when training later\nReduce learningrate when accuracy is plateauing and stop early if accuracy is not improving","82eee503":"# Importing training and testing data\nReading using datatable and converting to pandas is often faster than reading directly using pandas","23cf00f1":"# Train the model\nTrains the model {FOLDS} times, and adds result to predictions to make all models effect result","2bd392ab":"### Scale unscaled data\nGreat article on interesting ways to select pandas columns:\nhttps:\/\/towardsdatascience.com\/interesting-ways-to-select-pandas-dataframe-columns-b29b82bbfb33","7e180ce2":"# Define the model and compile it"}}