{"cell_type":{"d9bc5e5d":"code","bf7d9abc":"code","6799c8a7":"code","95ec6c7d":"code","577ecf52":"code","e8ed8be5":"code","5ac60cff":"code","33697458":"code","95979423":"code","d26f9795":"code","468eee62":"code","68dde9b9":"code","c9b9c149":"code","65be819e":"code","bd311660":"code","e789b73a":"code","0e1a945d":"code","cc3a9c3f":"code","81459cb5":"code","153e8004":"code","26d1385d":"code","e59a8ae7":"code","746af290":"code","9b637bd1":"code","6453a0fb":"code","1691cb7b":"code","86052444":"markdown","61bd6fb1":"markdown","7695f7c7":"markdown","f0a68134":"markdown","7e3d71fe":"markdown","4eeecd73":"markdown","8b3310a2":"markdown","7855a6a8":"markdown","4f7917fc":"markdown","f84914e0":"markdown","e09fa58a":"markdown"},"source":{"d9bc5e5d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf7d9abc":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport os\nimport time","6799c8a7":"#check if decoding is needed: text may need to be decoded as utf-8\ntext = open('\/kaggle\/input\/oz-books-from-ruth-plumly-frank-baum\/frank_baum\/the_road_to_oz.txt', 'r').read() \nprint(text[:2000])","95ec6c7d":"#Find Vocabulary (set of characters)\nvocabulary = sorted(set(text))\nprint('No. of unique characters: {}'.format(len(vocabulary)))","577ecf52":"#character to index mapping\nchar2index = {c:i for i,c in enumerate(vocabulary)}\nint_text = np.array([char2index[i] for i in text])\n\n#Index to character mapping\nindex2char = np.array(vocabulary)","e8ed8be5":"#Testing\nprint(\"Character to Index: \\n\")\nfor char,_ in zip(char2index, range(65)):\n    print('  {:4s}: {:3d}'.format(repr(char), char2index[char]))\n\nprint(\"\\nInput text to Integer: \\n\")\nprint('{} mapped to {}'.format(repr(text[:20]),int_text[:20])) #use repr() for debugging","5ac60cff":"seq_length= 150 #max number of characters that can be fed as a single input\nexamples_per_epoch = len(text)\n\n#converts text (vector) into character index stream\n#Reference: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset\nchar_dataset = tf.data.Dataset.from_tensor_slices(int_text)","33697458":"#Create sequences from the individual characters. Our required size will be seq_length + 1 (character RNN)\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)","95979423":"#Testing\nprint(\"Character Stream: \\n\")\nfor i in char_dataset.take(10):\n  print(index2char[i.numpy()])  \n\nprint(\"\\nSequence: \\n\")\nfor i in sequences.take(10):\n  print(repr(''.join(index2char[i.numpy()])))  #use repr() for more clarity. str() keeps formatting it","d26f9795":"def create_input_target_pair(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(create_input_target_pair)","468eee62":"#Testing\nfor input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(''.join(index2char[input_example.numpy()])))\n  print ('Target data:', repr(''.join(index2char[target_example.numpy()])))","68dde9b9":"#Creating batches\n\nBATCH_SIZE = 64\n\n# Buffer used to shuffle the dataset \n# Reference: https:\/\/stackoverflow.com\/questions\/46444018\/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","c9b9c149":"vocab_size = len(vocabulary)\nembedding_dim = 256\nrnn_units= 1024","65be819e":"def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.LSTM(rnn_units, \n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n    return model\n\n# Reference for theory: https:\/\/jhui.github.io\/2017\/03\/15\/RNN-LSTM-GRU\/","bd311660":"lstm_model = build_model_lstm(\n  vocab_size = vocab_size,\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)","e789b73a":"#Testing: shape\nfor input_example_batch, target_example_batch in dataset.take(1):\n    example_prediction = lstm_model(input_example_batch)\n    assert (example_prediction.shape == (BATCH_SIZE, seq_length, vocab_size)), \"Shape error\"\n    #print(example_prediction.shape)","0e1a945d":"sampled_indices = tf.random.categorical(example_prediction[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()","cc3a9c3f":"def loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\n#Loss Function reference: https:\/\/www.dlology.com\/blog\/how-to-use-keras-sparse_categorical_crossentropy\/\n\nexample_loss  = loss(target_example_batch, example_prediction)\nprint(\"Prediction shape: \", example_prediction.shape)\nprint(\"Loss:      \", example_loss.numpy().mean())","81459cb5":"lstm_model.compile(optimizer='adam', loss=loss)","153e8004":"lstm_dir_checkpoints= '.\/training_checkpoints_LSTM'\ncheckpoint_prefix = os.path.join(lstm_dir_checkpoints, \"checkpt_{epoch}\") #name\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)","26d1385d":"EPOCHS=60 #increase number of epochs for better results (lesser loss)","e59a8ae7":"history = lstm_model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","746af290":"tf.train.latest_checkpoint(lstm_dir_checkpoints)","9b637bd1":"lstm_model = build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size=1)\nlstm_model.load_weights(tf.train.latest_checkpoint(lstm_dir_checkpoints))\nlstm_model.build(tf.TensorShape([1, None]))\n\nlstm_model.summary()","6453a0fb":"def generate_text(model, start_string):\n    num_generate = 1000 #Number of characters to be generated\n\n    input_eval = [char2index[s] for s in start_string] #vectorising input\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    text_generated = []\n\n    # Low temperatures results in more predictable text.\n    # Higher temperatures results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 0.5\n\n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # We pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(index2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","1691cb7b":"#Prediction with User Input\nlstm_test = input(\"Enter your starting string: \")\nprint(generate_text(lstm_model, start_string=lstm_test))","86052444":"![](https:\/\/media2.giphy.com\/media\/eKllGEFrMOmPKrLkbR\/200w.webp?cid=ecf05e47xqbep69e3igjd09jg96jiqf935qlr0kz32w917ul&rid=200w.webp)","61bd6fb1":"#Create Training Data","7695f7c7":"#Building the Model","f0a68134":"#The snippet above take a long time. I can reach Oz and come back Epochs are still running.\n\n40 minutes and we still are in 13\/60.","7e3d71fe":"#Preprocessing Text","4eeecd73":"#Model Training","8b3310a2":"#The Road to Oz\n\nThe Road to Oz is the fifth of L. Frank Baum's Land of Oz books. It was originally published on July 10, 1909 and documents the adventures of Dorothy Gale's fourth visit to the Land of Oz.\n\nThis is the only Oz book to be printed on colored pages instead of with colored pictures. The colored pages represent the signature colors of the various countries of Oz that Dorothy and her companions travel through on their way to the Emerald City.\n\nhttps:\/\/en.wikipedia.org\/wiki\/The_Road_to_Oz","7855a6a8":"#Codes by Aashka Trivedi https:\/\/www.kaggle.com\/aashkatrivedi\/shakespeare-text-generation\/notebook\n\n#Generating Text with Character Based RNNs","4f7917fc":"#Prediction","f84914e0":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSx7lAEskNehNNZjgrR-If5Baxpn8GBpRwvIg&usqp=CAU)quoteslyfe.com","e09fa58a":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRWkeuT3K-mbGDgBlQCjwtSEvIqZFk_33Jcmg&usqp=CAU)quoteslyfe.com"}}