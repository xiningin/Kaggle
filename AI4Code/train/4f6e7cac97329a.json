{"cell_type":{"b398b895":"code","2fcbcbf5":"code","e47fcc4f":"code","8cca05b6":"code","15bf716f":"code","25e22dc5":"code","659b4ef2":"code","f1d0bc5f":"code","0ec3e1f7":"code","590df027":"code","6ab0d980":"code","1a6dabc3":"code","fd915505":"code","413bc687":"code","873cac5d":"code","0534af40":"code","0704ecd1":"code","6579b791":"code","220670e6":"code","88fe7a04":"code","4bad4811":"code","f77be4e6":"code","f66a866c":"code","c104bbce":"code","8248c55c":"code","475b9e87":"code","452f10a0":"code","48106a94":"code","c351a884":"code","7c98bdcb":"code","1fc59889":"markdown","6714a32f":"markdown","cef57cc2":"markdown","9bdb3bba":"markdown","985385dd":"markdown","ce71d4ce":"markdown","4aec2b25":"markdown","842bef64":"markdown","05c953b4":"markdown","52387d35":"markdown","3ab48805":"markdown","2b1b9074":"markdown","7e8e69cc":"markdown","14d44811":"markdown","8c679b52":"markdown","3d30fc6e":"markdown","8459ee86":"markdown","05a8b36f":"markdown","03590e2c":"markdown","22920ad8":"markdown","be052c3d":"markdown","376e0f92":"markdown","e38b6dd5":"markdown","0b7b3099":"markdown","2b812d3a":"markdown","5025aba7":"markdown","e54c5d11":"markdown","5c309426":"markdown","64cd64ca":"markdown"},"source":{"b398b895":"#!pip install -U pip setuptools wheel\n!pip install -U -q spacy\n!python -m spacy download en_core_web_sm","2fcbcbf5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport spacy\nimport re, string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom textblob import TextBlob # sentiment analysis\nfrom sklearn.decomposition import NMF # topic modeling\nfrom sklearn.feature_extraction.text import TfidfVectorizer # document-term matrix creation\n\n%matplotlib inline","e47fcc4f":"# Set some default options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\nsns.set_style('darkgrid')\nplt.rc('figure', facecolor='#00000000', figsize=(10,8))\nplt.rc('font', size=12)","8cca05b6":"posts = pd.read_csv('..\/input\/reddit-vaccine-myths\/reddit_vm.csv')\nposts.head()","15bf716f":"posts.info()","25e22dc5":"posts.drop(columns=['id', 'url', 'created'], inplace=True)\nposts.head()","659b4ef2":"posts['timestamp'] =  pd.to_datetime(posts['timestamp'], format='%Y-%m-%d %H:%M:%S')\nposts.info()","f1d0bc5f":"sum(posts['title'] == 'Comment')","0ec3e1f7":"posts.sort_values(by=['score'], ascending=False).head(10)","590df027":"# Extract year and month of the data\nposts['year'] = pd.DatetimeIndex(posts['timestamp']).year\nposts['month'] = pd.DatetimeIndex(posts['timestamp']).month\n\n# Create a count plot\nsns.countplot(x='year', data=posts)\nplt.title('Number of posts and comments by year')\nplt.show()","6ab0d980":"sns.countplot(x='month', data=posts.loc[posts['year'] == 2019])\nplt.title('Number of posts and comments by month (2019)')\nplt.show()","1a6dabc3":"def preprocess(sentence):\n    \"\"\"\n    Convert text to lowercase\n    Remove website link, special characters, newline character and extra spaces\n    \"\"\"\n    sentence = str(sentence)\n    sentence = sentence.lower()\n    rem_spe_chr = re.sub(r\"[-()\\\"#\/@;:<>\\[\\]{}`+=~|.!?,]\", \"\", sentence)\n    rem_url = re.sub(r'http\\S+', '', rem_spe_chr)\n    rem_url = re.sub(r'www\\S+', '', rem_url)\n    rem_nl = re.sub(r'\\n', ' ', rem_url)\n    rem_num = re.sub(r'[0-9]+', '', rem_nl)\n    rem_space = re.sub(r'\\s\\s+', ' ', rem_num)\n    return rem_space\n\n# Load spacy model\nnlp = spacy.load('en_core_web_sm')\n\n# Add stopwords\nSTOP_WORDS.update(['vaccine', 'vaccination', 'vaccinate', 'vaccinated', 'use', 'people', 'person', 'like', 'think', 'know', 'case', 'want',\n                   'mean', 'find', 'read', 'point'])\n\ndef lemmatize(sentence):\n    # Create Doc object and disable components we don't need (for efficiency)\n    doc = nlp(sentence, disable=['ner', 'parser'])\n\n    # Generate lemmatized tokens\n    lemmas = [token.lemma_ for token in doc]\n\n    # Remove stopwords and non-alphabetic tokens\n    alp_lemmas = [lemma for lemma in lemmas \n                  if lemma.isalpha() and lemma not in STOP_WORDS] \n\n    return ' '.join(alp_lemmas)","fd915505":"# Create another dataframe that contains the cleaned text\nposts_cleaned = posts.copy()\n\n# Preprocess and lemmatize the title (non-comment) and body\nposts_cleaned.loc[posts_cleaned['title'] != 'Comment', 'title'] = posts_cleaned.loc[posts_cleaned['title'] != 'Comment', 'title'].apply(preprocess).apply(lemmatize)\nposts_cleaned.loc[~posts_cleaned['body'].isnull(), 'body'] = posts_cleaned.loc[~posts_cleaned['body'].isnull(), 'body'].apply(preprocess).apply(lemmatize)\nposts_cleaned.tail()","413bc687":"# Import the word cloud function  \nfrom wordcloud import WordCloud\n\n# Iterate through body column to join the text in all rows\nbody_words = ''\nfor sentence in posts_cleaned.loc[~posts_cleaned['body'].isnull(), 'body']:\n    body_words += str(sentence) + ' '\n\n# Create and generate a word cloud image \nmy_cloud = WordCloud(background_color='white', stopwords=STOP_WORDS, max_words=200).generate(body_words)\n\n# Display the generated wordcloud image\nplt.figure(figsize=(15,10))\nplt.imshow(my_cloud, interpolation='bilinear') \nplt.axis(\"off\")\nplt.show()","873cac5d":"# Create columns for sentiment and subjectivity\nposts['sentiment'] = np.NaN\nposts['subjectivity'] = np.NaN\n\ndef basic_preprocess(sentence):\n    \"\"\"Perform basic preprocessing of text without removing special characters\"\"\"\n    sentence = str(sentence)\n    rem_url = re.sub(r'http\\S+', '', sentence)\n    rem_url = re.sub(r'www\\S+', '', rem_url)\n    rem_nl = re.sub(r'\\n', ' ', rem_url)\n    rem_space = re.sub(r'\\s\\s+', ' ', rem_nl)\n    return rem_space\n\ndef get_sentiment(sentence):\n    \"\"\"Extract sentiment from text based on polarity value\"\"\"\n    if sentence != '':\n      score = TextBlob(sentence).sentiment.polarity\n      if score < 0:\n          return \"Negative\"\n      elif score == 0:\n          return \"Neutral\"\n      else:\n          return \"Positive\"\n\ndef get_subjectivity(sentence):\n    \"\"\"Calculate subjectivity of text\"\"\"\n    return TextBlob(sentence).sentiment.subjectivity","0534af40":"# Apply basic preprocessing for body text\nposts.loc[~posts['body'].isnull(), 'body'] = posts.loc[~posts['body'].isnull(), 'body'].apply(basic_preprocess)\n\n# Populate the sentiment and subjectivity\nposts.loc[~posts['body'].isnull(), 'sentiment'] = posts.loc[~posts['body'].isnull(), 'body'].apply(get_sentiment)\nposts.loc[~posts['body'].isnull(), 'subjectivity'] = posts.loc[~posts['body'].isnull(), 'body'].apply(get_subjectivity)","0704ecd1":"# Create a pie chart\nfig, ax = plt.subplots(figsize=(8,5))\nax.pie(posts['sentiment'].value_counts(normalize=True), labels = ['Positive', 'Neutral', 'Negative'], autopct='%1.1f%%')\nax.set_title('Sentiment on vaccines')\nplt.show();","6579b791":"list(posts.loc[posts['sentiment'] == 'Positive', 'body'].sample(5))","220670e6":"list(posts.loc[posts['sentiment'] == 'Neutral', 'body'].sample(5))","88fe7a04":"list(posts.loc[posts['sentiment'] == 'Negative', 'body'].sample(5))","4bad4811":"posts['subjectivity'].describe()","f77be4e6":"# Create a histogram\nplt.figure(figsize=(10, 6))\nplt.hist(posts['subjectivity'])\nplt.title('Subjectivity of body text')\nplt.xlabel('subjectivity')\nplt.ylabel('count')\nplt.show();","f66a866c":"def count_words(string):\n    \"\"\"Count the number of words in each string\"\"\"\n    words = string.split()\n    return len(words)\n\n# Extract posts with title (exclude comments)\nposts_with_title = posts_cleaned[posts_cleaned['title'] != 'Comment'].copy()\n\n# Calculate the sentence length\nposts_with_title['title_word_count'] = posts_with_title['title'].apply(count_words)\n\n# Plot sentence length distribution\nplt.figure(figsize=(8,6))\nplt.hist(posts_with_title['title_word_count'])\nplt.xlabel('length of post title')\nplt.ylabel('count')\nplt.show();","c104bbce":"# Create tf-idf matrix - ignore terms that have a document frequency strictly higher than 95% and less than 2\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n                                   stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(posts_with_title['title'].values.astype(str))\n\nprint(tfidf.shape)","8248c55c":"# Fit NMF model to post titles\nnmf = NMF(n_components=5, random_state=1).fit(tfidf)\nnmf_output = nmf.fit_transform(tfidf)\nprint(nmf_output.shape)","475b9e87":"def get_topic_keywords(vectorizer, model, n_words):\n    keywords = np.array(vectorizer.get_feature_names())\n    topic_keywords = []\n    for topic_weights in model.components_:\n        # Find the indices of top n words with the highest weightage \n        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n\n        # Extract the keywords based on indices\n        topic_keywords.append(keywords.take(top_keyword_locs))\n    \n    return topic_keywords\n\n# Get the top 10 words for each topic\ntopic_keywords = get_topic_keywords(vectorizer=tfidf_vectorizer, model=nmf, n_words=10)   \n\n# Display the topic and its top 10 keywords\nfor i in range(len(topic_keywords)):\n    print(\"Topic {}: {}\".format(i, ' '.join(topic_keywords[i])))","452f10a0":"# Get the index of the highest topic weightage\nposts_with_title['topic'] = nmf_output.argmax(axis=1)\nposts_with_title.head()","48106a94":"posts_with_title['topic'].value_counts(normalize=True)","c351a884":"# Extract the document that has the highest weightage under each topic\nposts_with_title.iloc[nmf_output.argmax(axis=0)]","7c98bdcb":"list(posts_with_title.loc[posts_with_title['topic'] == 2, 'title'].sample(10))","1fc59889":"From the plot, we know that Reddit community was created in 2014. Ever since, the number of posts and comments was downtrending until a surge in 2019. Let's see if the surge of vaccine discussions was related to COVID-19, which was first known in December 2019.","6714a32f":"Great! Now, we are ready to take the preprocessed text to perform sentiment analysis and topic modeling. ","cef57cc2":"It seems like the classification is reasonable, although not highly accurate. This is expected as lexicon-based sentiment analysis might not handle different words (e.g., slang, informal words used on social media) in different contexts well because the sentiment is predicted based on pre-defined rules which might not be adapted to the specific domain. \n\nIf we have labeled historical datasets, we can train a machine learning model (e.g., Naive Bayes classifier, logistic regression) to predict the sentiments. Generally, this approach would yield a higher accuracy as the model can learn \"patterns\" from the labeled dataset.\n\nBefore we move on to topic modelling, let's take a quick look at the summary statistics and distribution of subjectivity of the texts.","9bdb3bba":"Looking at the keywords from each topic, we can postulate what each topic might be.\n\n* Topic 0: debate about link of vaccine (mmr) with autism \n* Topic 1: approval\/use of COVID-19 vaccine in the world\n* Topic 2: safety of vaccine\n* Topic 3: vaccination uptake in children and its link with parent\n* Topic 4: anti-vaccination and vaccine myths\n\nLet's assign the post titles to the topic that constitutes the highest weightage among all the 5 topics. \n\n","985385dd":"# Explore the Dataset \n\nLet's do a quick exploration of the dataset for understanding. First, load the dataset into `Pandas` dataframe.\n\n","ce71d4ce":"# References\n\n1. [WHO COVID-19 Dashboard](https:\/\/covid19.who.int\/)\n2. [Sentiment Analysis Using TextBlob](https:\/\/towardsdatascience.com\/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524)\n3. [Non-negative Matrix Factorization - Python Implementation](https:\/\/towardsdatascience.com\/nmf-a-visual-explainer-and-python-implementation-7ecdd73491f8)\n4. [Topic Modeling with NMF](https:\/\/towardsdatascience.com\/topic-modeling-articles-with-nmf-8c6b2a227a45)","4aec2b25":"The mean subjectivity is ~0.4. If using 0.5 as a guide, a higher proportion of posts and comments are less subjective.","842bef64":"There are some missing values under `body`, which is expected as some users might only share the post with title and `url` (which we already removed). Let's convert `timestamp` from *object* to *datetime* for subsequent analysis and plotting.","05c953b4":"## Explore Creation Date and Impact of Posts","52387d35":"From the wordcloud, we can see some common words like study, child, autism, measle, good and time. As we cannot infer the semantic meaning of a sentence using wordcloud, let's use **TextBlob**, a simple Python Library which allows us to perform sentiment analysis using lexicon-based approaches (i.e., define sentiment by semantic orientation and intensity of each word in a sentence). \n\nTextBlob returns two properties of a given input sentence:\n*   **Polarity**: lies within [-1, 1]. -1 defines a negative sentiment, 1 defines a positive sentiment and 0 defines a neutral sentiment. TextBlob has semantic labels to recognize emoticons, exclamation marks, emojis, etc. Hence, we'll use the original `body` column (before removal of special characters) for analysis to preserve its semantics.\n*   **Subjectivity**: lies within [0, 1]. The higher the score (toward 1), the more personal opinion (instead of factual information) the sentence contains.\n\n","3ab48805":"Let's see the breakdown of posts and comments in the dataset. There are 1,130 comments (~70%) out of 1,597 rows. The remaining 30% (n=467) are individual posts.","2b1b9074":"A lot of vaccine discussions already took place before the first known case of COVID-19 in December 2019.\n\nWe've completed the data exploration. Next, let's analyze sentiments and extract hidden topics (based on `title`) from the data. We'll start by preprocessing the texts.\n","7e8e69cc":"# Introduction\n\nCOVID-19 pandemic has affected the world in an unprecedented manner since the virus (SARS-CoV-2) was first identified in December 2019. To date, the virus has infected hundreds of millions of people and caused millions of deaths worldwide. Although more than 3 billion people globally were fully vaccinated, new variants of concern (e.g., Delta, Omicron) continued to emerge and strain the public health systems. Today, many of us still live with some levels of social distancing and travel restrictions.\n\nLooking at the latest development of Covid-19 pandemic, it is likely that we need to take routine booster vaccine shots to protect ourselves and to \"co-live\" with the virus. Hence, it is insightful to look at how people think about vaccines (not limited to COVID-19 vaccines) as vaccine receptivity is essential to minimize the overall disease burden. \n\n*\\\"It\u2019s not vaccines that will stop the pandemic, it\u2019s vaccination.\\\" - World Health Organization (WHO).*\n\nIn this project, we'll analyze Reddit posts from Subreddit **VaccineMyths** to perform the following: \n- Sentiment analysis of user's posts and comments\n- Topic modeling to extract hidden topics from the posts\n\nFrom the results, we'll identify potential measures that might be helpful to debunk the common myths. Let's start by loading the required libraries and exploring the dataset.","14d44811":"We can see that ~48% of the text sentiments are classified as positive, ~26% as neutral and ~26% as negative. Let's randomly select a few positive and negative sentiments to see how TextBlob has performed in classifying the sentiment.","8c679b52":"We can see that there are 1,587 rows and 8 columns. Let's first understand what each column entails.\n\n* `title` - title of post\n* `score` - score of post based on impact, number of comments\n* `id` - unique id for posts\/comments\n* `url` - url of post thread\n* `commns_num` - number of comments to this post\n* `created` - date of creation\n* `body` - text of the post or comment\n* `timestamp` - timestamp of creation\n\nWe do not need `id` and `url` for this analysis. Also, `created` (UNIX timestamp) and `timestamp` contains the same information, so we'll remove `created`. ","3d30fc6e":"# Topic Modeling\n\nIt is interesting to see if we can extract prevailing topics from the posts. Here, we would use a technique called non-negative matrix factorization (NMF), which decomposes the **document-term matrix** into two smaller matrices - **document-topic matrix** and **topic-term matrix**. A document is composed of a mix of topics, and a topic is composed of a mix of terms. \n\n\n![image](https:\/\/miro.medium.com\/max\/500\/1*ru1Ek9T3FjkehDkD8FZoHQ.jpeg)\n\n[Image source](https:\/\/towardsdatascience.com\/nmf-a-visual-explainer-and-python-implementation-7ecdd73491f8)\n\nWe will exclude pure comments from the topic modeling as the comments are the responses to the `title`. Before fitting the model, let's look at the word count distribution of each title.","8459ee86":"Next, we sort by `score` to see what are the top 10 posts with the highest impact and\/or number of comments.\n\n","05a8b36f":"The NMF output is a non-negative matrix that represents the document-topic matrix with a shape of (467, 5). Let's create a function to display the topics and the top 10 words in each topic.","03590e2c":"We now have a document-term matrix with shape of (467, 507), i.e., 467 titles (documents) and 507 vocabularies. We can use this as the input to scikit-learn [NMF](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.NMF.html) model. Let's set the number of topics as 5.\n\n**It is important to pick the right number of topics as too few topics produce results that are too broad, while too many topics product results that are highly similar. However, there is currently no means to score NMF model in scikit-learn. For those who are interested, you can try Gensim's LDA topic modeling, for which a metric called \"coherence score\" can be used to compare models with different number of topics. In this analysis, we will stick to NMF model.*","22920ad8":"# Sentiment Analysis \n\nLet's perform sentiment analysis to understand the opinion of the authors about the vaccines. We'll first generate a wordcloud to see what were the common words mentioned by the author.  ","be052c3d":"We can see the most impactful post has much more comments (n=595) than the other posts. We will analyze the sentiment of the comments later on. Looking at the post title, more of the top 10 posts do not seem to advocate vaccination.\n\nLet's see the distribution of these posts and comments over the years.","376e0f92":"These posts seem to be related to safety profile of vaccine (topic 2). Our NMF topic model has a reasonable performance. \n\nFrom the topic modeling outputs, we can see potential areas (e.g., vaccine use and autism, thimerosal\/mercury in vaccine) that might contribute to vaccine hesitancy and skepticism. More targeted education on vaccination can be considered.","e38b6dd5":"Let's take a look at the breakdown of sentiments of the body text. ","0b7b3099":"We can look at the percentage of each topic and find out the most representative post title for each topic.","2b812d3a":"In general, >50% of the post titles are less than 10 words. \n\nNow, we need to turn the post titles into numbers as the model does not understand texts. We'll use scikit-learn [TfidfVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html) to create term frequency-inverse document frequency (tf-idf) matrix. In essence, this formula will weigh each word based on its appearance in a document (i.e., a single post title here) and its appearance across the entire corpus (i.e., all the post titles here). If a word appears in many documents, its weightage (tf-idf) will be reduced as it doesn't mean much to the document in particular. \n\nRead [here](https:\/\/towardsdatascience.com\/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089) for more detailed explanation of tf-idf.","5025aba7":"Most posts (\\~34%) were related to topic 4 (anti-vaccination and vaccine myths), while topic 1 (COVID-19 vaccine approval and use) has the lowest number of posts (~8%). \n\nThe most representative post title each topic seems to tally with our postulation (except for topic 2). Let's look at more posts under topic 2.","e54c5d11":"# Conclusion\n\nWe have come to the end of this analysis. From sentiment analysis and topic modeling, we observed the following:\n* Around half of the sentiments were positive, with another half split between neutral and negative\n* The discussions on Reddit were centered around autism, COVID-19 vaccine, vaccine safety, vaccination in children and anti-vaccination.\n\nIt is important not to forget the limitations of data, methodology and findings. Some of the limitations we have are listed:  \n* Social media data is subjected to high bias - social media users are not representative of the underlying population.\n* Results from Reddit data are not generalizable to all countries (a large proportion of Reddit users are located in the United States).\n* We only have data from one Reddit community. There are other vaccine communities discussing different aspects of vaccines.\n* We do not have labeled historical dataset to train machine learning model for sentiment prediction.\n* NMF topic modeling in scikit-learn doesn't have score to compare models with different number of chosen topics. Other models such as Gensim's LDA can be considered. \n","5c309426":"***If you like this analysis, please give it an upvote :) I'd also like to hear from you if you have any feedback\/suggestion.***","64cd64ca":"# Text Preprocessing\n\nIt is important to make the text machine friendly before analysis. Hence, we'll apply the following steps to preprocess the text: convert words into lowercase, remove website link, remove special characters (e.g., number, emoji), remove punctuation, remove newline character, remove extra spaces and remove stopwords.\n\nWe'll be using [spaCy](https:\/\/spacy.io\/), which is an open-source software library for advanced natural language processing (NLP), written in the programming languages Python and Cython."}}