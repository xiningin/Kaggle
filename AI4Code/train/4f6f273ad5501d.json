{"cell_type":{"53541f59":"code","377db378":"code","3d81f468":"code","7f08b12d":"code","dc3a1dc6":"code","9e556070":"code","bc472ad5":"code","0e9c1e72":"code","b16245e6":"code","cfc6c529":"code","add13d08":"code","d40f3113":"code","b722b309":"code","57051143":"code","ef84011d":"code","57bcab37":"code","17052471":"code","d6b65109":"code","270649c4":"code","26a79642":"code","599883b5":"code","fd9ce1ec":"code","faec9608":"code","6edf3988":"code","c8ddfb3e":"code","f08ced37":"code","95f55029":"code","10cf366e":"code","1dc2c5af":"code","2fb36b58":"code","0a116548":"code","c9d4ecef":"code","dd5c2bc3":"code","e2921723":"code","7f12a1b8":"code","64fc34e1":"code","e1d4acef":"code","99b6e769":"markdown","a301221f":"markdown","8048a3e6":"markdown","41c2691c":"markdown","67598ff6":"markdown","d2d0dae4":"markdown","20dc698e":"markdown","9b6eef30":"markdown","b6a50698":"markdown","4933d607":"markdown","6aeec6b0":"markdown","5bc65cca":"markdown","409138ee":"markdown","e38e2f76":"markdown","d99d7f38":"markdown"},"source":{"53541f59":"# Importing Required packages\nimport os\nimport torch\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.auto import tqdm\nimport shutil as sh\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, clear_output\n%matplotlib inline","377db378":"# Reading the dataframe\ndf = pd.read_csv('..\/input\/global-wheat-detection\/train.csv')\ndf.head()","3d81f468":"# read the training data.\n\ndf = pd.read_csv('..\/input\/global-wheat-detection\/train.csv')\nbboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\nfor i, column in enumerate(['x', 'y', 'w', 'h']):\n    df[column] = bboxs[:,i]\ndf.drop(columns=['bbox'], inplace=True)\ndf['x_center'] = (df['x'] + df['w'])\/2\ndf['y_center'] = (df['y'] + df['h'])\/2\ndf['classes'] = 0\n\n# Making new dataframe, suitable to make suitable dataset for yolov5\ndf = df[['image_id','x', 'y', 'w', 'h','x_center','y_center','classes']]\ndf.head()","7f08b12d":"index = list(set(df.image_id))\nprint(\"Total Images: \",len(index))","dc3a1dc6":"# This cell will automatically make the dataset and save it to convertor folder\nsource = 'train'\nif True:\n    for fold in [0]:\n        val_index = index[len(index)*fold\/\/5:len(index)*(fold+1)\/\/5]\n        for name,mini in tqdm(df.groupby('image_id')):\n            if name in val_index:\n                path2save = 'val\/'\n            else:\n                path2save = 'train\/'\n            if not os.path.exists('convertor\/fold{}\/labels\/'.format(fold)+path2save):\n                os.makedirs('convertor\/fold{}\/labels\/'.format(fold)+path2save)\n            with open('convertor\/fold{}\/labels\/'.format(fold)+path2save+name+\".txt\", 'w+') as f:\n                row = mini[['classes','x_center','y_center','w','h']].astype(float).values\n                row = row\/1024\n                row = row.astype(str)\n                for j in range(len(row)):\n                    text = ' '.join(row[j])\n                    f.write(text)\n                    f.write(\"\\n\")\n            if not os.path.exists('convertor\/fold{}\/images\/{}'.format(fold,path2save)):\n                os.makedirs('convertor\/fold{}\/images\/{}'.format(fold,path2save))\n            sh.copy(\"..\/input\/global-wheat-detection\/{}\/{}.jpg\".format(source,name),'convertor\/fold{}\/images\/{}\/{}.jpg'.format(fold,path2save,name))\n    \n# Bases on this notebook: https:\/\/www.kaggle.com\/orkatz2\/yolov5-train","9e556070":"# Cloning the repo\n!git clone https:\/\/github.com\/ultralytics\/yolov5.git\nclear_output()","bc472ad5":"# Moving the folders to our working directory\n!mv .\/yolov5\/* .\/","0e9c1e72":"# Checking if the files correctly cloned and moved\n!ls","b16245e6":"# installing the requirements file\n!pip install -r requirements.txt\nclear_output()","cfc6c529":"#customize iPython writefile so we can write variables\n\nfrom IPython.core.magic import register_line_cell_magic\n@register_line_cell_magic\ndef writetemplate(line, cell):\n    with open(line, 'w') as f:\n        f.write(cell.format(**globals()))","add13d08":"# Our image and annotation files are saved into this directory\n\nprint(os.listdir(\".\/convertor\/fold0\"))","d40f3113":"# Making a directory for storing our data.yaml and custom YOLOv5(..).yml files\n!mkdir DataFile","b722b309":"%%writetemplate .\/DataFile\/data.yaml\n\ntrain: .\/convertor\/fold0\/images\/train # training directory\nval: .\/convertor\/fold0\/images\/val # validation directory\ntest: .\/convertor\/fold0\/images\/val # I'll use validation directory for test image\nnc: 1 # number of class\nnames: ['Wheat'] # name of the class","57051143":"print(os.listdir(\".\/models\"))","ef84011d":"# checking the yolov5s model architecture\n!cat .\/models\/yolov5.yaml","57bcab37":"%%writetemplate .\/DataFile\/customYOLOv5x.yaml\n\n# parameters\nnc: 80  # number of classes\ndepth_multiple: 1.33  # model depth multiple\nwidth_multiple: 1.25  # layer channel multiple\n\n# anchors\nanchors:\n  - [10,13, 16,30, 33,23]  # P3\/8\n  - [30,61, 62,45, 59,119]  # P4\/16\n  - [116,90, 156,198, 373,326]  # P5\/32\n\n# YOLOv5 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Focus, [64, 3]],  # 0-P1\/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2\/4\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3\/8\n   [-1, 9, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4\/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5\/32\n   [-1, 1, SPP, [1024, [5, 9, 13]]],\n   [-1, 3, C3, [1024, False]],  # 9\n  ]\n\n# YOLOv5 head\nhead:\n  [[-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, C3, [512, False]],  # 13\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, C3, [256, False]],  # 17 (P3\/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 14], 1, Concat, [1]],  # cat head P4\n   [-1, 3, C3, [512, False]],  # 20 (P4\/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 10], 1, Concat, [1]],  # cat head P5\n   [-1, 3, C3, [1024, False]],  # 23 (P5\/32-large)\n\n   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n  ]","17052471":"# You should skip this line\n!wandb off","d6b65109":"%%time\n!python train.py --img 640  --batch 16 --epochs 1 --data .\/DataFile\/data.yaml --cfg .\/DataFile\/customYOLOv5x.yaml --weights yolov5x.pt  --name Result --cache","270649c4":"# let's check the training result directory.\n# Here model and result are saved\n!ls -R .\/runs\/train","26a79642":"# Analize the training and validation result\nImage('.\/runs\/train\/Result\/results.png')","599883b5":"# Analize the Confusion matrix\nImage('.\/runs\/train\/Result\/confusion_matrix.png',width=400)","fd9ce1ec":"!python detect.py --img-size 800  --conf 0.2 --source ..\/input\/global-wheat-detection\/test --weights .\/runs\/train\/Result\/weights\/best.pt --augment ","faec9608":"# Detected images\/video are saved in this path\n!ls -R runs\/detect","6edf3988":"Image('runs\/detect\/exp\/2fd875eaa.jpg',width=400)","c8ddfb3e":"# hyper parameters data are saved in this location\n!ls .\/data","f08ced37":"%cat .\/data\/hyp.finetune.yaml","95f55029":"%%writetemplate .\/DataFile\/hyp.custom.yaml\n\nlr0: 0.0032\nlrf: 0.12\nmomentum: 0.843\nweight_decay: 0.00036\nwarmup_epochs: 2.0\nwarmup_momentum: 0.5\nwarmup_bias_lr: 0.05\nbox: 0.0296\ncls: 0.243\ncls_pw: 0.631\nobj: 0.301\nobj_pw: 0.911\niou_t: 0.2\nanchor_t: 2.91\n# anchors: 3.63\nfl_gamma: 0.0   # \nhsv_h: 0.0138\nhsv_s: 0.002\nhsv_v: 0.2\ndegrees: 0.2\ntranslate: 0.0\nscale: 0.3\nshear: 0.0\nperspective: 0.0\nflipud: 0.00856\nfliplr: 0.5\nmosaic: 1.0\nmixup: 0.5","10cf366e":"%%time\n!python train.py --img 1024  --batch 4 --epochs 1 --data .\/DataFile\/data.yaml --cfg .\/DataFile\/customYOLOv5x.yaml --hyp .\/DataFile\/hyp.custom.yaml  --weights .\/runs\/train\/Result\/weights\/best.pt  --name FineTuned","1dc2c5af":"# new training result\nos.listdir('.\/runs\/train')","2fb36b58":"!ls -R .\/runs\/train\/FineTuned","0a116548":"!python detect.py  --img 800 --weights .\/runs\/train\/Result\/weights\/best.pt .\/runs\/train\/FineTuned\/weights\/best.pt --source ..\/input\/global-wheat-detection\/test","c9d4ecef":"!ls runs\/detect","dd5c2bc3":"!ls -R runs\/detect\/exp","e2921723":"Image('runs\/detect\/exp\/2fd875eaa.jpg', width=400)","7f12a1b8":"!python test.py --weights .\/runs\/train\/Result\/weights\/best.pt .\/runs\/train\/FineTuned\/weights\/best.pt --data .\/DataFile\/data.yaml --img 800 --augment","64fc34e1":"!ls -R runs\/test\/exp","e1d4acef":"# That's all for today, I'll update the notebook for pseudo labelling soon.\n# Please let me know if you face any issue or want have any confusion","99b6e769":"- ```--hyp```: Location of new custom hyper parameter","a301221f":"With our __data.yaml__ and __custom_yolov5s.yaml__ files ready to go we are ready to train!\n\nTo kick off training we running the training command with the following options:\n\n- ```img```: define input image size\n- ```batch```: determine batch size\n- ```epochs```: define the number of training epochs.\n- ```data```: set the path to our yaml file\n- ```cfg```: specify our model configuration\n- ```weights```: specify a custom path to weights. (Note: you can download weights from the Ultralytics Google Drive folder)\n- ```name```: result names\n- ```cache```: cache images for faster training\n- ```evolve```: evolve hyperparameters","8048a3e6":"# Model Ensembling\nI've used two models during detection. It'll increase detection result.","41c2691c":"Modifying the yolov5s model architecture for nc: 1","67598ff6":"# Training is completed.\nResults are saved in .\/runs\/train directory","d2d0dae4":"**In the notebook, We'll detect head of the wheat**\n\n<img src='https:\/\/i.ytimg.com\/vi\/yqvMuw-uedU\/maxresdefault.jpg' height=500 width=800\/>","20dc698e":"# Now it's turn to train the dataset using YOLOv5\nOur stategy is to:\n1. Clone [yolov5](https:\/\/github.com\/ultralytics\/yolov5.git) directory from github\n\n2. Install the requirements for yolov5\n\n3. Make a [data.yml](https:\/\/github.com\/ultralytics\/yolov5\/blob\/master\/data\/coco.yaml) file indicating our training directory, validation directory, number of classes, and classname \n\n4. Chose a model bases on your requirement(YOLOv5s, YOLOv5m, YOLOv5l, YOLOv5x)\n\n5. Change the YOLOv5(..).yml acording to your dataset. As we have only one class, we'll use nc: 1\n\n6. Start the Training","9b6eef30":"# What is YOLO?\n**YOLO** refers to \u201cYou Only Look Once\u201d is one of the most versatile and famous object detection models. For every **real-time object detection** work.\n\nYOLO algorithms divide all the given input images into the SxS grid system. Each grid is responsible for object detection. Now those Grid cells predict the boundary boxes for the detected object. For every box, we have five main attributes: x and y for coordinates, w and h for width and height of the object, and a confidence score for the probability that the box containing the object.\n# YOLOv5\nYOLOv5 got released Utralytics. It is publicly released on [Github](https:\/\/github.com\/ultralytics\/yolov5). \nUtralytics introduced the YOLOv5 Pytorch based approach, and Yes! YOLOv5 is written in the **Pytorch framework**.That's why it is super fast.\nAnd it comes with some default augmentation technique, which makes help it to converge it toward accuracy\n\n# Comparison\nYOLOv5 set the benchmark for object detection models very high. It already surpussed previous state of the art object detection model EfficientDet and other models from YOLO family.\n\n<img src='https:\/\/user-images.githubusercontent.com\/26833433\/114313216-f0a5e100-9af5-11eb-8445-c682b60da2e3.png' height=500 width=800\/>","b6a50698":"# Firstly we'll preprocess the dataset in suitable format for YOLOv5.\nThe format should look like this:\n\n    - converter(main directory)\n        - val\n            - labels (contains all the box dimensions)\n            - images (contains images)\n        - train\n            - labels\n            - images","4933d607":"**Let's train the model with better hyper parameter**","6aeec6b0":"# TTA(Test Time Augmentation) and Model Ensembling","5bc65cca":"Our models structure files are save here. These are used for training [coco dataset](https:\/\/cocodataset.org\/#home). But for using for our custom dataset, we have to change the nc parameter to 1, insted of 80.","409138ee":"# Techniques to push the training result\n\n* Experimenting with hyper parameters\n* increasing the image size","e38e2f76":"# I've used weight of previous trained model.So, this training will initilize from the best result(weight) of the previous model","d99d7f38":"# Let's run detection on test images\nParameters to consider:\n\n- ```img-size```: define input image size\n- ```conf```: Minimum threshold of confidence\n- ```source```: Location of the image\/video file\n- ```weights```: specify our trined weight file\n- ```augment```: Augmentation of the images during detection for better result"}}