{"cell_type":{"13296958":"code","aa1aa9fd":"code","957bc631":"code","020ba796":"code","cff6b1b8":"code","3b8790cf":"code","0efd64f0":"code","1fdec01d":"code","1877820c":"code","0b41cc3d":"code","104e56d2":"code","f235e133":"code","05303c7b":"code","8efa868f":"code","d8e065e4":"code","87ccea08":"code","1e3d2359":"code","c4dff852":"code","49145c2c":"code","a7da112a":"code","de926e94":"code","9b9715a2":"code","27f77a4c":"code","87cf6d8a":"code","c4c3b4ef":"code","20e249ce":"code","c8deca0e":"code","f5fedc68":"code","0902b04f":"code","301dd2f1":"code","8063b014":"code","eff15995":"markdown"},"source":{"13296958":"import physionet_challenge_utility_script as pc\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils import plot_model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import layers\nfrom keras.layers import Input, Dense, Dropout, Activation, BatchNormalization, Add\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, MaxPool1D, ZeroPadding1D, LSTM, Bidirectional, Flatten, GlobalAveragePooling2D\nfrom keras.models import Sequential, Model\nfrom keras.utils import plot_model\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom keras.layers.merge import concatenate\nfrom scipy import optimize\nfrom scipy.io import loadmat\nimport os\n%load_ext autoreload\n%autoreload\n%reload_ext autoreload","aa1aa9fd":"gender, age, labels, ecg_filenames = pc.import_key_data(\"\/kaggle\/input\/\")\necg_filenames = np.asarray(ecg_filenames)","957bc631":"age, gender = pc.import_gender_and_age(age, gender)","020ba796":"SNOMED_scored=pd.read_csv(\"\/kaggle\/input\/physionet-snomed-mappings\/SNOMED_mappings_scored.csv\", sep=\";\")\nSNOMED_unscored=pd.read_csv(\"\/kaggle\/input\/physionet-snomed-mappings\/SNOMED_mappings_unscored.csv\", sep=\";\")\ndf_labels = pc.make_undefined_class(labels,SNOMED_unscored)","cff6b1b8":"y , snomed_classes = pc.onehot_encode(df_labels)","3b8790cf":"snomed_abbr = []\nfor j in range(len(snomed_classes)):\n    for i in range(len(SNOMED_scored.iloc[:,1])):\n        if (str(SNOMED_scored.iloc[:,1][i]) == snomed_classes[j]):\n            snomed_abbr.append(SNOMED_scored.iloc[:,2][i])\n            \nsnomed_abbr = np.asarray(snomed_abbr)","0efd64f0":"y_all_comb = pc.get_labels_for_all_combinations(y)\nprint(\"Total number of unique combinations of diagnosis: {}\".format(len(np.unique(y_all_comb))))","1fdec01d":"folds = pc.split_data(labels, y_all_comb)\norder_array = folds[0][0]","1877820c":"def shuffle_batch_generator_demo(batch_size, gen_x,gen_y, gen_z): \n    np.random.shuffle(order_array)\n    batch_features = np.zeros((batch_size,5000, 12))\n    batch_labels = np.zeros((batch_size,snomed_classes.shape[0])) #drop undef class\n    batch_demo_data = np.zeros((batch_size,2))\n    while True:\n        for i in range(batch_size):\n\n            batch_features[i] = next(gen_x)\n            batch_labels[i] = next(gen_y)\n            batch_demo_data[i] = next(gen_z)\n\n        X_combined = [batch_features, batch_demo_data]\n        yield X_combined, batch_labels\n        \ndef shuffle_batch_generator(batch_size, gen_x,gen_y): \n    np.random.shuffle(order_array)\n    batch_features = np.zeros((batch_size,5000, 12))\n    batch_labels = np.zeros((batch_size,snomed_classes.shape[0])) #drop undef class\n    while True:\n        for i in range(batch_size):\n\n            batch_features[i] = next(gen_x)\n            batch_labels[i] = next(gen_y)\n            \n        yield batch_features, batch_labels\n\ndef generate_y_shuffle(y_train):\n    while True:\n        for i in order_array:\n            y_shuffled = y_train[i]\n            yield y_shuffled\n\n\ndef generate_X_shuffle(X_train):\n    while True:\n        for i in order_array:\n                #if filepath.endswith(\".mat\"):\n                    data, header_data = pc.load_challenge_data(X_train[i])\n                    X_train_new = pad_sequences(data, maxlen=5000, truncating='post',padding=\"post\")\n                    X_train_new = X_train_new.reshape(5000,12)\n                    yield X_train_new\n\ndef generate_z_shuffle(age_train, gender_train):\n    while True:\n        for i in order_array:\n            gen_age = age_train[i]\n            gen_gender = gender_train[i]\n            z_train = [gen_age , gen_gender]\n            yield z_train","0b41cc3d":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_AUC', factor=0.1, patience=1, verbose=1, mode='max',\n    min_delta=0.0001, cooldown=0, min_lr=0\n)","104e56d2":"from scipy import optimize\ndef thr_chall_metrics(thr, label, output_prob):\n    return -pc.compute_challenge_metric_for_opt(label, np.array(output_prob>thr))","f235e133":"def load_challenge_data(filename):\n    x = loadmat(filename)\n    data = np.asarray(x['val'], dtype=np.float64)\n    new_file = filename.replace('.mat','.hea')\n    input_header_file = os.path.join(new_file)\n    with open(input_header_file,'r') as f:\n        header_data=f.readlines()\n    return data, header_data\n\n\ndef generate_validation_data(ecg_filenames, y,test_order_array):\n    y_train_gridsearch=y[test_order_array]\n    ecg_filenames_train_gridsearch=ecg_filenames[test_order_array]\n\n    ecg_train_timeseries=[]\n    for names in ecg_filenames_train_gridsearch:\n        data, header_data = load_challenge_data(names)\n        data = pad_sequences(data, maxlen=5000, truncating='post',padding=\"post\")\n        ecg_train_timeseries.append(data)\n    X_train_gridsearch = np.asarray(ecg_train_timeseries)\n\n    X_train_gridsearch = X_train_gridsearch.reshape(ecg_filenames_train_gridsearch.shape[0],5000,12)\n\n    return X_train_gridsearch, y_train_gridsearch\n\n\ndef compute_modified_confusion_matrix(labels, outputs):\n    # Compute a binary multi-class, multi-label confusion matrix, where the rows\n    # are the labels and the columns are the outputs.\n    num_recordings, num_classes = np.shape(labels)\n    A = np.zeros((num_classes, num_classes))\n\n    # Iterate over all of the recordings.\n    for i in range(num_recordings):\n        # Calculate the number of positive labels and\/or outputs.\n        normalization = float(max(np.sum(np.any((labels[i, :], outputs[i, :]), axis=0)), 1))\n        # Iterate over all of the classes.\n        for j in range(num_classes):\n            # Assign full and\/or partial credit for each positive class.\n            if labels[i, j]:\n                for k in range(num_classes):\n                    if outputs[i, k]:\n                        A[j, k] += 1.0\/normalization\n\n    return A\n\n\ndef plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, val_fold, threshold, snomedclasses):\n    df_cm = pd.DataFrame(compute_modified_confusion_matrix(generate_validation_data(ecg_filenames,y,val_fold)[1], (y_pred>threshold)*1), columns=snomedclasses, index = snomedclasses)\n    df_cm = df_cm.fillna(0)\n    df_cm.index.name = 'Actual'\n    df_cm.columns.name = 'Predicted'\n    df_norm_col=(df_cm-df_cm.mean())\/df_cm.std()\n    plt.figure(figsize = (36,14))\n    sns.set(font_scale=1.4)\n    sns.heatmap(df_norm_col, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16},fmt=\".2f\",cbar=False)# font size","05303c7b":"new_weights=pc.calculating_class_weights(y)","8efa868f":"keys = np.arange(0,27,1)\nweight_dictionary = dict(zip(keys, new_weights.T[1]))\nweight_dictionary","d8e065e4":"def inception_block(prev_layer):\n    \n    conv1=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(prev_layer)\n    conv1=BatchNormalization()(conv1)\n    conv1=Activation('relu')(conv1)\n    \n    \n    conv3=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(prev_layer)\n    conv3=BatchNormalization()(conv3)\n    conv3=Activation('relu')(conv3)\n    \n    conv3=Conv1D(filters = 64, kernel_size = 3, padding = 'same')(conv3)\n    conv3=BatchNormalization()(conv3)\n    conv3=Activation('relu')(conv3)\n   \n    \n    conv5=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(prev_layer)\n    conv5=BatchNormalization()(conv5)\n    conv5=Activation('relu')(conv5)\n    \n    conv5=Conv1D(filters = 64, kernel_size = 5, padding = 'same')(conv5)\n    conv5=BatchNormalization()(conv5)\n    conv5=Activation('relu')(conv5)\n    \n    pool= MaxPool1D(pool_size=3, strides=1, padding='same')(prev_layer)\n    convmax=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(pool)\n    convmax=BatchNormalization()(convmax)\n    convmax=Activation('relu')(convmax)\n    \n    layer_out = concatenate([conv1, conv3, conv5, convmax], axis=1)\n    \n    return layer_out\n\ndef inception_model(input_shape):\n    X_input=Input(input_shape)\n    \n    #X = ZeroPadding1D(3)(X_input)\n    \n    X = Conv1D(filters = 64, kernel_size = 1, padding = 'same')(X_input)\n    #X = MaxPool1D(pool_size=3, strides=2, padding='same')(X)\n    X = BatchNormalization()(X)\n    X = Activation('relu')(X)\n    \n    X = Conv1D(filters = 64, kernel_size = 1, padding = 'same')(X)\n    X = BatchNormalization()(X)\n    X = Activation('relu')(X)\n    \n    \n    X = inception_block(X)\n    X = MaxPool1D(pool_size=2, strides=4, padding='same')(X)\n    X = inception_block(X)\n    X = MaxPool1D(pool_size=2, strides=4, padding='same')(X)\n    X = inception_block(X)\n    X = MaxPool1D(pool_size=2, strides=4, padding='same')(X)\n    X = inception_block(X)\n    X = MaxPool1D(pool_size=2, strides=4, padding='same')(X)\n    \n    \n    X = inception_block(X)\n    X = MaxPool1D(pool_size=2, strides=4, padding='same')(X)\n    \"\"\"X = inception_block(X)\n    X = MaxPool1D(pool_size=2, strides=4, padding='same')(X)\n    X = inception_block(X)\n    X = MaxPool1D(pool_size=2, strides=4, padding='same')(X)\n    X = inception_block(X)\n    X = MaxPool1D(pool_size=2, strides=4, padding='same')(X)\n    X = inception_block(X)\n    X = MaxPool1D(pool_size=2, strides=4, padding='same')(X)\n    X = inception_block(X)\n    X = MaxPool1D(pool_size=2, strides=4, padding='same')(X)\"\"\"\n    \n    X = GlobalAveragePooling1D()(X)\n    X = Dense(64,activation='relu')(X)\n    X = Dense(64,activation='relu')(X)\n    X = Dense(27,activation='softmax')(X)\n    \n    model = Model(inputs = X_input, outputs = X, name='Inception')\n    \n    return model","87ccea08":"inception_model = inception_model(input_shape = (5000,12))","1e3d2359":"inception_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.BinaryAccuracy(\n        name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), \n                    tf.keras.metrics.AUC(\n        num_thresholds=200,\n        curve=\"ROC\",\n        summation_method=\"interpolation\",\n        name=\"AUC\",\n        dtype=None,\n        thresholds=None,\n        multi_label=True,\n        label_weights=None,\n    )])","c4dff852":"inception_model.summary()","49145c2c":"plot_model(inception_model)","a7da112a":"inception_model.load_weights(\".\/inception_model.h5\")","de926e94":"batchsize = 10\ninception_model.fit(x=shuffle_batch_generator(batch_size=batchsize, gen_x=generate_X_shuffle(ecg_filenames), gen_y=generate_y_shuffle(y)), epochs=30, steps_per_epoch=(len(order_array)\/(batchsize*10)), validation_data=pc.generate_validation_data(ecg_filenames,y,folds[0][1]))","9b9715a2":"plt.plot(inception_model.history.history['accuracy'])\nplt.plot(inception_model.history.history['Precision'])\nplt.plot(inception_model.history.history['val_accuracy'])\nplt.plot(inception_model.history.history['val_Precision'])\nplt.legend((\"accuracy\",\"precision\",\"val_accuracy\",\"val_Precision\"))\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","27f77a4c":"plt.plot(inception_model.history.history['loss'])\nplt.plot(inception_model.history.history['val_loss'])\nplt.legend((\"loss\",\"val_loss\"))\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","87cf6d8a":"plt.plot(inception_model.history.history['Recall'])\nplt.plot(inception_model.history.history['Precision'])\nplt.plot(inception_model.history.history['AUC'])\nplt.legend((\"Recall\",\"Precision\",\"AUC\"))\nplt.xlabel('Epoch')","c4c3b4ef":"y_pred = inception_model.predict(x=pc.generate_validation_data(ecg_filenames,y,folds[0][1])[0])\ninit_thresholds = np.arange(0,1,0.05)\nall_scores = pc.iterate_threshold(y_pred, ecg_filenames, y ,folds[0][1] )","20e249ce":"new_best_thr = optimize.fmin(thr_chall_metrics, args=(pc.generate_validation_data(ecg_filenames,y,folds[0][1])[1],y_pred), x0=init_thresholds[all_scores.argmax()]*np.ones(27))","c8deca0e":"pc.plot_normalized_conf_matrix(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes, snomed_abbr)\nplt.savefig(\"confusion_matrix_inception.png\", dpi=100)","f5fedc68":"plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes)\nplt.savefig(\"confusion_matrix_inception_v2.png\", dpi=100)","0902b04f":"pc.plot_normalized_conf_matrix(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes, snomed_abbr)\nplt.savefig(\"confusion_matrix_inception.png\", dpi=100)","301dd2f1":"plot_normalized_conf_matrix_dev(y_pred, ecg_filenames, y, folds[0][1], new_best_thr, snomed_classes)\nplt.savefig(\"confusion_matrix_inception_v2.png\", dpi=100)","8063b014":"inception_model.save(\"inception_model.h5\")","eff15995":"# Inception"}}