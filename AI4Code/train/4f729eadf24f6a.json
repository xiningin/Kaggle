{"cell_type":{"5a66042f":"code","6775ff77":"code","f2b017f8":"code","823b3b2b":"code","165dc759":"code","430f8061":"code","87035205":"code","227d6905":"code","420c3409":"code","e0e0ea37":"code","ae47bbaf":"code","508521cf":"code","c16ebdd2":"code","9c9b2435":"code","c3e8738b":"code","9acc6d82":"code","154054bd":"code","557ae3b5":"code","228204e4":"code","8e92e6dd":"code","5a2982eb":"code","fabde0fe":"code","7ef7de99":"code","98eb8e15":"code","fce724ec":"code","c5c9b5cc":"code","fa266d1a":"code","01020008":"markdown","350d8460":"markdown","1c3e17b6":"markdown","3413cddb":"markdown","ce5cf525":"markdown","b7e24924":"markdown","0ce92256":"markdown","f7da626d":"markdown","e206bd99":"markdown","c4970a04":"markdown"},"source":{"5a66042f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6775ff77":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","f2b017f8":"!pip install pyspark","823b3b2b":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()","165dc759":"#Load data into spark \ndata = spark.read.csv('\/kaggle\/input\/heart-disease-uci\/heart.csv', header = True)","430f8061":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndf.head()","87035205":"df.info()","227d6905":"#Columns in data\ndata.printSchema()","420c3409":"from pyspark.sql.types import *\n\n# Write a custom function to convert the data type of DataFrame columns\ndef convertColumn(df, names, newType):\n  for name in names: \n     df = df.withColumn(name, df[name].cast(newType))\n  return df \nint_columns = ['age','sex','cp', 'trestbps','chol','fbs','restecg','thalach','exang','slope','ca','thal','target']\nfloat_column = ['oldpeak']\ndata = convertColumn(data, int_columns, IntegerType())\ndata = convertColumn(data, float_column, FloatType())","e0e0ea37":"data.persist()","ae47bbaf":"#Take one sample\ndata.take(1)","508521cf":"data.describe().show() # It is hard to see","c16ebdd2":"data.describe('target').show()","9c9b2435":"data.describe('age').show()","c3e8738b":"#Count number of samples\ndata.count()","9acc6d82":"#Change spark to pandas dataframe\npd_data =data.toPandas()\npd_data","154054bd":"pd_data.info()","557ae3b5":"#One-hot encoding maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values.\nfrom pyspark.ml.feature import OneHotEncoder\nencoder = OneHotEncoder(inputCols=['sex','cp','fbs','restecg','exang','slope','ca','thal'],\n                           outputCols =['sexVec','cpVec','fbsVec','restecgVec','exangVec','slopeVec','caVec','thalVec'])\nencoded =  encoder.fit(data).transform(data)\ndata_encoded = encoded.drop('sex','cp','fbs','restecg','exang','slope','ca','thal')\ndata_encoded.show()","228204e4":"from pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler(inputCols =['age','trestbps','chol','thalach','oldpeak','sexVec','cpVec','fbsVec','restecgVec','exangVec','slopeVec','caVec','thalVec'],\n                           outputCol=\"features\")\nassembled= assembler.transform(data_encoded)\n\ndata_asb =assembled.select(\"features\",\"target\")\ndata_asb.show()\n","8e92e6dd":"from pyspark.ml.feature import PCA\n\npca = PCA(k=13, inputCol=\"features\", outputCol=\"pcaFeatures\")\nmodel = pca.fit(data_asb).transform(data_asb)\n\ndata_pca = model.select(\"pcaFeatures\",\"target\")\ndata_pca.head()","5a2982eb":"train_data, test_data = data_pca.randomSplit([.7,.3],seed=1234)","fabde0fe":"from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Initialize\nlr = LogisticRegression(labelCol=\"target\", featuresCol=\"pcaFeatures\", maxIter=10, regParam=0.3, elasticNetParam=0.8,family=\"binomial\")\n# Fit the data to the model\nlr_model = lr.fit(train_data)\nlr_predictions = lr_model.transform(test_data)\n# Calculate accuracy\nevaluator = MulticlassClassificationEvaluator(labelCol='target',predictionCol='prediction', metricName='accuracy')\nlr_accuracy = evaluator.evaluate(lr_predictions)\nprint('Test Accuracy = ', lr_accuracy)","7ef7de99":"from pyspark.ml.classification import DecisionTreeClassifier\n# Initialize\ndt = DecisionTreeClassifier(labelCol=\"target\", featuresCol=\"pcaFeatures\")\n# Fit the data to the model\ndt_model = dt.fit(train_data)\ndt_predictions = dt_model.transform(test_data)\n# Calculate accuracy\nevaluator = MulticlassClassificationEvaluator(labelCol='target',predictionCol='prediction', metricName='accuracy')\ndt_accuracy = evaluator.evaluate(dt_predictions)\nprint('Test Accuracy = ', dt_accuracy)","98eb8e15":"from pyspark.ml.classification import RandomForestClassifier\n# Initialize\nrf = RandomForestClassifier(labelCol=\"target\", featuresCol=\"pcaFeatures\")\n# Fit the data to the model\nrf_model = rf.fit(train_data)\nrf_predictions = rf_model.transform(test_data)\n# Calculate accuracy\nevaluator = MulticlassClassificationEvaluator(labelCol='target',predictionCol='prediction', metricName='accuracy')\nrf_accuracy = evaluator.evaluate(rf_predictions)\nprint('Test Accuracy = ', rf_accuracy)","fce724ec":"from pyspark.ml.classification import GBTClassifier\n# Initialize\ngbt = GBTClassifier(labelCol=\"target\", featuresCol=\"pcaFeatures\")\n# Fit the data to the model\ngbt_model = gbt.fit(train_data)\ngbt_predictions = gbt_model.transform(test_data)\n# Calculate accuracy\nevaluator = MulticlassClassificationEvaluator(labelCol='target',predictionCol='prediction', metricName='accuracy')\ngbt_accuracy = evaluator.evaluate(gbt_predictions)\nprint('Test Accuracy = ', gbt_accuracy)","c5c9b5cc":"from pyspark.ml.classification import MultilayerPerceptronClassifier\n# Initialize\nmpc = MultilayerPerceptronClassifier(labelCol=\"target\", featuresCol=\"pcaFeatures\", seed=1234, layers=[13,5,4,2])\n# Fit the data to the model\nmpc_model = mpc.fit(train_data)\nmpc_predictions = mpc_model.transform(test_data)\n# Calculate accuracy\nevaluator = MulticlassClassificationEvaluator(labelCol='target',predictionCol='prediction', metricName='accuracy')\nmpc_accuracy = evaluator.evaluate(mpc_predictions)\nprint('Test Accuracy = ', mpc_accuracy)","fa266d1a":"from pyspark.ml.classification import NaiveBayes\n# Initialize\nnb = NaiveBayes(labelCol=\"target\", featuresCol=\"pcaFeatures\",smoothing=1.0, modelType=\"gaussian\")\n# Fit the data to the model\nnb_model = nb.fit(train_data)\nnb_predictions = nb_model.transform(test_data)\n# Calculate accuracy\nevaluator = MulticlassClassificationEvaluator(labelCol='target',predictionCol='prediction', metricName='accuracy')\nnb_accuracy = evaluator.evaluate(nb_predictions)\nprint('Test Accuracy = ', nb_accuracy)","01020008":"# Decision Tree","350d8460":"# 2. Preprocessing","1c3e17b6":"# Random Forest","3413cddb":"# Gradient-boosted tree classifier","ce5cf525":"# 3. PCA","b7e24924":"# Multilayer perceptron classifier","0ce92256":"# 4. Models (PCA)","f7da626d":"# Logistic Regression ","e206bd99":"# 1. Import Data","c4970a04":"# Naive Bayes"}}