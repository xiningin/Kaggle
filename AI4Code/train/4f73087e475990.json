{"cell_type":{"be0d5a35":"code","d1c27902":"code","6fb3e705":"code","af82353f":"code","7ac4ca00":"code","b8acb25d":"code","345a5fe7":"code","2e44e9be":"code","dea61c89":"markdown"},"source":{"be0d5a35":"from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\nfrom keras.models import Model, Sequential\nfrom keras.regularizers import l2\nfrom keras import backend as K\nfrom keras.optimizers import SGD,Adam\nfrom keras.losses import binary_crossentropy\nimport numpy.random as rng\nimport numpy as np\nimport os\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import shuffle\n%matplotlib inline\ndef W_init(shape,name=None):\n    \"\"\"Initialize weights as in paper\"\"\"\n    values = rng.normal(loc=0,scale=1e-2,size=shape)\n    return K.variable(values,name=name)\n#\/\/TODO: figure out how to initialize layer biases in keras.\ndef b_init(shape,name=None):\n    \"\"\"Initialize bias as in paper\"\"\"\n    values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n    return K.variable(values,name=name)\n\ninput_shape = (105, 105, 1)\nleft_input = Input(input_shape)\nright_input = Input(input_shape)\n#build convnet to use in each siamese 'leg'\nconvnet = Sequential()\nconvnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape,\n                   kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\nconvnet.add(MaxPooling2D())\nconvnet.add(Conv2D(128,(7,7),activation='relu',\n                   kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\nconvnet.add(MaxPooling2D())\nconvnet.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\nconvnet.add(MaxPooling2D())\nconvnet.add(Conv2D(256,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\nconvnet.add(Flatten())\nconvnet.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))\n\n#call the convnet Sequential model on each of the input tensors so params will be shared\nencoded_l = convnet(left_input)\nencoded_r = convnet(right_input)\n#layer to merge two encoded inputs with the l1 distance between them\nL1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n#call this layer on list of two input tensors.\nL1_distance = L1_layer([encoded_l, encoded_r])\nprediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(L1_distance)\nsiamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n\noptimizer = Adam(0.00006)\n#\/\/TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\nsiamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n\nsiamese_net.count_params()\n","d1c27902":"PATH = \"\/home\/soren\/Desktop\/keras-oneshot\" #CHANGE THIS - path where the pickled data is stored\n\nwith open(os.path.join(PATH, \"train.pickle\"), \"rb\") as f:\n    (X,c) = pickle.load(f)\n\nwith open(os.path.join(PATH, \"val.pickle\"), \"rb\") as f:\n    (Xval,cval) = pickle.load(f)\n    \nprint(\"training alphabets\")\nprint(c.keys())\nprint(\"validation alphabets:\")\nprint(cval.keys())","6fb3e705":"class Siamese_Loader:\n    \"\"\"For loading batches and testing tasks to a siamese net\"\"\"\n    def __init__(self, path, data_subsets = [\"train\", \"val\"]):\n        self.data = {}\n        self.categories = {}\n        self.info = {}\n        \n        for name in data_subsets:\n            file_path = os.path.join(path, name + \".pickle\")\n            print(\"loading data from {}\".format(file_path))\n            with open(file_path,\"rb\") as f:\n                (X,c) = pickle.load(f)\n                self.data[name] = X\n                self.categories[name] = c\n\n    def get_batch(self,batch_size,s=\"train\"):\n        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n        X=self.data[s]\n        n_classes, n_examples, w, h = X.shape\n\n        #randomly sample several classes to use in the batch\n        categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n        #initialize 2 empty arrays for the input image batch\n        pairs=[np.zeros((batch_size, h, w,1)) for i in range(2)]\n        #initialize vector for the targets, and make one half of it '1's, so 2nd half of batch has same class\n        targets=np.zeros((batch_size,))\n        targets[batch_size\/\/2:] = 1\n        for i in range(batch_size):\n            category = categories[i]\n            idx_1 = rng.randint(0, n_examples)\n            pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n            idx_2 = rng.randint(0, n_examples)\n            #pick images of same class for 1st half, different for 2nd\n            if i >= batch_size \/\/ 2:\n                category_2 = category  \n            else: \n                #add a random number to the category modulo n classes to ensure 2nd image has\n                # ..different category\n                category_2 = (category + rng.randint(1,n_classes)) % n_classes\n            pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n        return pairs, targets\n    \n    def generate(self, batch_size, s=\"train\"):\n        \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n        while True:\n            pairs, targets = self.get_batch(batch_size,s)\n            yield (pairs, targets)    \n\n    def make_oneshot_task(self,N,s=\"val\",language=None):\n        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n        X=self.data[s]\n        n_classes, n_examples, w, h = X.shape\n        indices = rng.randint(0,n_examples,size=(N,))\n        if language is not None:\n            low, high = self.categories[s][language]\n            if N > high - low:\n                raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n            categories = rng.choice(range(low,high),size=(N,),replace=False)\n            \n        else:#if no language specified just pick a bunch of random letters\n            categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n        true_category = categories[0]\n        ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n        test_image = np.asarray([X[true_category,ex1,:,:]]*N).reshape(N, w, h,1)\n        support_set = X[categories,indices,:,:]\n        support_set[0,:,:] = X[true_category,ex2]\n        support_set = support_set.reshape(N, w, h,1)\n        targets = np.zeros((N,))\n        targets[0] = 1\n        targets, test_image, support_set = shuffle(targets, test_image, support_set)\n        pairs = [test_image,support_set]\n\n        return pairs, targets\n    \n    def test_oneshot(self,model,N,k,s=\"val\",verbose=0):\n        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n        n_correct = 0\n        if verbose:\n            print(\"Evaluating model on {} random {} way one-shot learning tasks ...\".format(k,N))\n        for i in range(k):\n            inputs, targets = self.make_oneshot_task(N,s)\n            probs = model.predict(inputs)\n            if np.argmax(probs) == np.argmax(targets):\n                n_correct+=1\n        percent_correct = (100.0*n_correct \/ k)\n        if verbose:\n            print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n        return percent_correct\n    \n    def train(self, model, epochs, verbosity):\n        model.fit_generator(self.generate(batch_size),\n                            \n                             )\n    \n    \n#Instantiate the class\nloader = Siamese_Loader(PATH)","af82353f":"\ndef concat_images(X):\n    \"\"\"Concatenates a bunch of images into a big matrix for plotting purposes.\"\"\"\n    nc,h,w,_ = X.shape\n    X = X.reshape(nc,h,w)\n    n = np.ceil(np.sqrt(nc)).astype(\"int8\")\n    img = np.zeros((n*w,n*h))\n    x = 0\n    y = 0\n    for example in range(nc):\n        img[x*w:(x+1)*w,y*h:(y+1)*h] = X[example]\n        y += 1\n        if y >= n:\n            y = 0\n            x += 1\n    return img\n\n\ndef plot_oneshot_task(pairs):\n    \"\"\"Takes a one-shot task given to a siamese net and  \"\"\"\n    fig,(ax1,ax2) = plt.subplots(2)\n    ax1.matshow(pairs[0][0].reshape(105,105),cmap='gray')\n    img = concat_images(pairs[1])\n    ax1.get_yaxis().set_visible(False)\n    ax1.get_xaxis().set_visible(False)\n    ax2.matshow(img,cmap='gray')\n    plt.xticks([])\n    plt.yticks([])\n    plt.show()\n#example of a one-shot learning task\npairs, targets = loader.make_oneshot_task(20,\"train\",\"Japanese_(katakana)\")\nplot_oneshot_task(pairs)\n","7ac4ca00":"\n#Training loop\nprint(\"!\")\nevaluate_every = 1 # interval for evaluating on one-shot tasks\nloss_every=50 # interval for printing loss (iterations)\nbatch_size = 32\nn_iter = 90000\nN_way = 20 # how many classes for testing one-shot tasks>\nn_val = 250 #how mahy one-shot tasks to validate on?\nbest = -1\nweights_path = os.path.join(PATH, \"weights\")\nprint(\"training\")\nfor i in range(1, n_iter):\n    (inputs,targets)=loader.get_batch(batch_size)\n    loss=siamese_net.train_on_batch(inputs,targets)\n    print(loss)\n    if i % evaluate_every == 0:\n        print(\"evaluating\")\n        val_acc = loader.test_oneshot(siamese_net,N_way,n_val,verbose=True)\n        if val_acc >= best:\n            print(\"saving\")\n            siamese_net.save(weights_path)\n            best=val_acc\n\n    if i % loss_every == 0:\n        print(\"iteration {}, training loss: {:.2f},\".format(i,loss))\n","b8acb25d":"def nearest_neighbour_correct(pairs,targets):\n    \"\"\"returns 1 if nearest neighbour gets the correct answer for a one-shot task\n        given by (pairs, targets)\"\"\"\n    L2_distances = np.zeros_like(targets)\n    for i in range(len(targets)):\n        L2_distances[i] = np.sum(np.sqrt(pairs[0][i]**2 - pairs[1][i]**2))\n    if np.argmin(L2_distances) == np.argmax(targets):\n        return 1\n    return 0\n\n\ndef test_nn_accuracy(N_ways,n_trials,loader):\n    \"\"\"Returns accuracy of one shot \"\"\"\n    print(\"Evaluating nearest neighbour on {} unique {} way one-shot learning tasks ...\".format(n_trials,N_ways))\n\n    n_right = 0\n    \n    for i in range(n_trials):\n        pairs,targets = loader.make_oneshot_task(N_ways,\"val\")\n        correct = nearest_neighbour_correct(pairs,targets)\n        n_right += correct\n    return 100.0 * n_right \/ n_trials\n\n\nways = np.arange(1, 60, 2)\nresume =  False\nval_accs, train_accs,nn_accs = [], [], []\ntrials = 450\nfor N in ways:\n    val_accs.append(loader.test_oneshot(siamese_net, N,trials, \"val\", verbose=True))\n    train_accs.append(loader.test_oneshot(siamese_net, N,trials, \"train\", verbose=True))\n    nn_accs.append(test_nn_accuracy(N,trials, loader))\n    \n#plot the accuracy vs num categories for each\nplt.plot(ways, val_accs, \"m\")\nplt.plot(ways, train_accs, \"y\")\nplt.plot(ways, nn_accs, \"c\")\n\nplt.plot(ways,100.0\/ways,\"r\")\nplt.show()","345a5fe7":"fig,ax = plt.subplots(1)\nax.plot(ways,val_accs,\"m\",label=\"Siamese(val set)\")\nax.plot(ways,train_accs,\"y\",label=\"Siamese(train set)\")\nplt.plot(ways,nn_accs,label=\"Nearest neighbour\")\n\nax.plot(ways,100.0\/ways,\"g\",label=\"Random guessing\")\nplt.xlabel(\"Number of possible classes in one-shot tasks\")\nplt.ylabel(\"% Accuracy\")\nplt.title(\"Omiglot One-Shot Learning Performance of a Siamese Network\")\nbox = ax.get_position()\nax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\ninputs,targets = loader.make_oneshot_task(20,\"val\")\nplt.show()\n\nprint(inputs[0].shape)\nplot_oneshot_task(inputs)\np=siamese_net.predict(inputs)\nprint(p)","2e44e9be":"\na=test_nn_accuracy(3,500,loader)\nprint(a)","dea61c89":"## Data \nThe data is pickled as an N_classes x n_examples x width x height array, and there is an accompanyng dictionary to specify which indexes belong to which languages."}}