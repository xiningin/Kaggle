{"cell_type":{"85952dfc":"code","a5df0111":"code","0ff85bc1":"code","5f0e0011":"code","699cc2d7":"code","8984723f":"code","8a749d30":"code","53ac97c8":"code","c640d40d":"code","9271e7e2":"code","1ad34f5a":"code","d5c1a149":"code","a2b48b3c":"code","b1c5d6d1":"code","aecba14d":"code","e9650c8e":"code","653bb185":"code","b9b54ff2":"code","78a7124b":"code","44d0a15d":"code","0c81547c":"code","ed42578c":"code","22208e77":"code","db391b4b":"code","daa532b7":"code","37581684":"code","490cb7b6":"code","b345fcbd":"code","dd41b055":"code","6dfd1b35":"code","e2481a62":"code","e35d9cd0":"code","122a66f2":"code","ff12f71c":"code","c0fb3ba0":"code","df41a83f":"code","1c11479b":"markdown","9f1fd1f4":"markdown","2839d1f1":"markdown","656ed6b9":"markdown","32ee9d11":"markdown","e627b215":"markdown","0826a900":"markdown","6e3ce74a":"markdown","f8c3c536":"markdown","734bc174":"markdown","46247b08":"markdown","6963022e":"markdown","388e31ac":"markdown","75cc0084":"markdown","810058e5":"markdown","3db40bf8":"markdown","9deae5a3":"markdown","781c44a2":"markdown","d7de1c6d":"markdown","32adffe3":"markdown","a4742a5e":"markdown","461b9edb":"markdown","66a42e18":"markdown","0bc38542":"markdown","5bc9a214":"markdown","4fd87cf5":"markdown","d8d396e4":"markdown","953fa171":"markdown","d0f0a0c7":"markdown","68502e41":"markdown"},"source":{"85952dfc":"# Import basic libraries\nimport numpy as np\nimport pandas as pd\n\n# Graph visualization library\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# Data preprocessing library\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Machine learning library\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Evaluation library\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc","a5df0111":"df_train = pd.read_csv(\"..\/input\/titanic\/train.csv\", header=0)\ndf_train.head()","0ff85bc1":"df_test = pd.read_csv(\"..\/input\/titanic\/test.csv\", header=0)\ndf_test.head()","5f0e0011":"# Data size\nprint(\"Training data size:{}\".format(df_train.shape))\nprint(\"Test data size:{}\".format(df_test.shape))","699cc2d7":"# Data info, training_data\ndf_train.info()","8984723f":"# Data info, test_data\ndf_test.info()","8a749d30":"# Null data check, training_data\ndf_train.isnull().sum()","53ac97c8":"# Null data check, test_data\ndf_test.isnull().sum()","c640d40d":"# Unique count check, about string data.\n# Ticket data\ndf_train[\"Ticket\"].value_counts()","9271e7e2":"# Cabin data\ndf_train[\"Cabin\"].value_counts()","1ad34f5a":"print(\"0 count:{}\".format(df_train.query(\"Survived==0\").shape[0]))\nprint(\"1 count:{}\".format(df_train.query(\"Survived==1\").shape[0]))\nsns.countplot(df_train[\"Survived\"])\nplt.title(\"Survived flag count for training data\\n 0 for deceased, 1 for survived\")","d5c1a149":"fig, ax = plt.subplots(1,5, figsize=(25,4))\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\n\n# Pclass\nsns.countplot(df_train[\"Pclass\"], ax=ax[0])\nax[0].set_title(\"Passenger class\")\n\n# Sex\nsns.countplot(df_train[\"Sex\"], ax=ax[1])\nax[1].set_title(\"Sex\")\n\n# SibSp\nsns.countplot(df_train[\"SibSp\"], ax=ax[2])\nax[2].set_title(\"family relations SibSp\")\n\n# Parch\nsns.countplot(df_train[\"Parch\"], ax=ax[3])\nax[3].set_title(\"family relations Parch\")\n\n# Embarked\nsns.countplot(df_train[\"Embarked\"], ax=ax[4])\nax[4].set_title(\"Embarked\")","a2b48b3c":"# Visualization of ratio with barplot\nfig, ax = plt.subplots(1,5, figsize=(25,4))\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\n\n# Pclass\n# Pivot_table\npivot = pd.pivot_table(df_train, index=\"Pclass\", columns=\"Survived\", values=\"PassengerId\", aggfunc=\"count\").reset_index()\npivot[\"sum\"] = pivot[0] + pivot[1]\n\nax[0].bar(pivot[\"Pclass\"], pivot[0]\/pivot[\"sum\"]*100, color='blue', alpha=0.5)\nax[0].bar(pivot[\"Pclass\"], pivot[1]\/pivot[\"sum\"]*100, bottom=pivot[0]\/pivot[\"sum\"]*100, color='red', alpha=0.5)\nax[0].set_xlim([0.5,3.5])\nax[0].set_xticks([1,2,3])\nax[0].set_title(\"Passenger class\")\nax[0].legend(labels=[\"deceased\", \"survived\"], loc=\"lower right\", facecolor=\"white\")\n\n# Sex\n# Pivot_table\npivot = pd.pivot_table(df_train, index=\"Sex\", columns=\"Survived\", values=\"PassengerId\", aggfunc=\"count\").reset_index()\npivot[\"sum\"] = pivot[0] + pivot[1]\n\nax[1].bar(pivot[\"Sex\"], pivot[0]\/pivot[\"sum\"]*100, color='blue', alpha=0.5)\nax[1].bar(pivot[\"Sex\"], pivot[1]\/pivot[\"sum\"]*100, bottom=pivot[0]\/pivot[\"sum\"]*100, color='red', alpha=0.5)\nax[1].set_title(\"Sex\")\nax[1].legend(labels=[\"deceased\", \"survived\"], loc=\"lower right\", facecolor=\"white\")\n\n# SibSp\n# Pivot_table\npivot = pd.pivot_table(df_train, index=\"SibSp\", columns=\"Survived\", values=\"PassengerId\", aggfunc=\"count\").reset_index()\npivot[\"sum\"] = pivot[0] + pivot[1]\n\nax[2].bar(pivot[\"SibSp\"], pivot[0]\/pivot[\"sum\"]*100, color='blue', alpha=0.5)\nax[2].bar(pivot[\"SibSp\"], pivot[1]\/pivot[\"sum\"]*100, bottom=pivot[0]\/pivot[\"sum\"]*100, color='red', alpha=0.5)\nax[2].set_xlim([-0.5,2.5])\nax[2].set_xticks([0,1,2])\nax[2].set_title(\"SibSp\")\nax[2].legend(labels=[\"deceased\", \"survived\"], loc=\"lower right\", facecolor=\"white\")\n\n# Parch\n# Pivot_table\npivot = pd.pivot_table(df_train, index=\"Parch\", columns=\"Survived\", values=\"PassengerId\", aggfunc=\"count\").reset_index()\npivot[\"sum\"] = pivot[0] + pivot[1]\n\nax[3].bar(pivot[\"Parch\"], pivot[0]\/pivot[\"sum\"]*100, color='blue', alpha=0.5)\nax[3].bar(pivot[\"Parch\"], pivot[1]\/pivot[\"sum\"]*100, bottom=pivot[0]\/pivot[\"sum\"]*100, color='red', alpha=0.5)\nax[3].set_xlim([-0.5,2.5])\nax[3].set_xticks([0,1,2])\nax[3].set_title(\"Parch\")\nax[3].legend(labels=[\"deceased\", \"survived\"], loc=\"lower right\", facecolor=\"white\")\n\n# Embarked\n# Pivot_table\npivot = pd.pivot_table(df_train, index=\"Embarked\", columns=\"Survived\", values=\"PassengerId\", aggfunc=\"count\").reset_index()\npivot[\"sum\"] = pivot[0] + pivot[1]\n\nax[4].bar(pivot[\"Embarked\"], pivot[0]\/pivot[\"sum\"]*100, color='blue', alpha=0.5)\nax[4].bar(pivot[\"Embarked\"], pivot[1]\/pivot[\"sum\"]*100, bottom=pivot[0]\/pivot[\"sum\"]*100, color='red', alpha=0.5)\nax[4].set_title(\"Embarked\")\nax[4].legend(labels=[\"deceased\", \"survived\"], loc=\"lower right\", facecolor=\"white\")","b1c5d6d1":"fig, ax = plt.subplots(1,2, figsize=(10,4))\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\n\n# Age *Null data is dropped, tempolary. \nsns.distplot(df_train.dropna()[\"Age\"], ax=ax[0], kde=False, bins=20)\nax[0].set_title(\"Age\")\nax[0].set_ylabel(\"count\")\n\n# Fare\nsns.distplot(df_train[\"Fare\"], ax=ax[1], kde=False, bins=20)\nax[1].set_title(\"Fare\")\nax[1].set_ylabel(\"count\")","aecba14d":"fig, ax = plt.subplots(1,2, figsize=(10,4))\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\n\n# Age *Null data is dropped, tempolary. \nsns.distplot(df_train.dropna().query(\"Survived==0\")[\"Age\"], ax=ax[0], kde=False, bins=20, norm_hist=True)\nsns.distplot(df_train.dropna().query(\"Survived==1\")[\"Age\"], ax=ax[0], kde=False, bins=20, norm_hist=True)\nax[0].set_title(\"Age\")\nax[0].set_ylabel(\"count\")\nax[0].legend(labels=[\"deceased\", \"survived\"], loc=\"upper right\", facecolor=\"white\")\n\n# Fare\nsns.distplot(df_train.query(\"Survived==0\")[\"Fare\"], ax=ax[1], kde=False, bins=20, norm_hist=True)\nsns.distplot(df_train.query(\"Survived==1\")[\"Fare\"], ax=ax[1], kde=False, bins=20, norm_hist=True)\nax[1].set_title(\"Fare\")\nax[1].set_ylabel(\"count\")\nax[1].legend(labels=[\"deceased\", \"survived\"], loc=\"upper right\", facecolor=\"white\")","e9650c8e":"# training data\ndf_train[\"Age\"].fillna(np.mean(df_train[\"Age\"]), inplace=True)\ndf_train[\"Fare\"].fillna(np.mean(df_train[\"Fare\"]), inplace=True)\ndf_train[\"Embarked\"].fillna('S', inplace=True)\ndf_train.dropna(axis=1, inplace=True)\n\n# test data\ndf_test[\"Age\"].fillna(np.mean(df_train[\"Age\"]), inplace=True)\ndf_test[\"Fare\"].fillna(np.mean(df_train[\"Fare\"]), inplace=True)\ndf_test.dropna(axis=1, inplace=True)","653bb185":"# Sex\ndef sex(x):\n    if x[\"Sex\"] == \"male\":\n        res = 0\n    else :\n        res = 1\n    return res\n\ndf_train[\"Sex_cate\"] = df_train.apply(sex, axis=1)\ndf_test[\"Sex_cate\"] = df_test.apply(sex, axis=1)\n\n# Age\ndef age_band(x):\n    if x[\"Age\"] <= 10:\n        res = 0\n    elif x[\"Age\"] <= 20 and x[\"Age\"] > 10:\n        res = 1\n    elif x[\"Age\"] <= 30 and x[\"Age\"] > 20:\n        res = 2\n    elif x[\"Age\"] <= 40 and x[\"Age\"] > 30:\n        res = 3\n    elif x[\"Age\"] <= 50 and x[\"Age\"] > 40:\n        res = 4\n    elif x[\"Age\"] <= 60 and x[\"Age\"] > 50:\n        res = 5\n    else :\n        res = 6\n    return res\n\ndf_train[\"Age_band\"] = df_train.apply(age_band, axis=1)\ndf_test[\"Age_band\"] = df_test.apply(age_band, axis=1)\n\n# Fare\ndef fare_band(x):\n    if x[\"Fare\"] <= 25:\n        res = 0\n    elif x[\"Fare\"] <= 50 and x[\"Fare\"] > 25:\n        res = 1\n    elif x[\"Fare\"] <= 75 and x[\"Fare\"] > 50:\n        res = 2\n    elif x[\"Fare\"] <= 100 and x[\"Fare\"] > 75:\n        res = 3\n    elif x[\"Fare\"] <= 125 and x[\"Fare\"] > 100:\n        res = 4\n    else :\n        res = 5\n    return res\n\ndf_train[\"Fare_band\"] = df_train.apply(fare_band, axis=1)\ndf_test[\"Fare_band\"] = df_test.apply(fare_band, axis=1)\n\n# Embarked\ndef embarked_flg(x):\n    if x[\"Embarked\"] == 'S':\n        res = 0\n    elif x[\"Embarked\"] == 'C':\n        res = 1\n    else:\n        res = 2\n    return res\n\ndf_train[\"Embarked_flg\"] = df_train.apply(embarked_flg, axis=1)\ndf_test[\"Embarked_flg\"] = df_test.apply(embarked_flg, axis=1)","b9b54ff2":"# Confirming the dataframe.\ndf_train.head()","78a7124b":"# Checking by visualization with heatmap.\nplt.figure(figsize=(12,8))\nhm = sns.heatmap(df_train[['Survived', 'Pclass','SibSp','Parch','Sex_cate', 'Age_band', 'Fare_band', 'Embarked_flg']].corr(),\n                cbar=True,\n                annot=True,\n                square=True,\n                cmap=\"RdBu_r\",\n                fmt=\".2f\",\n                annot_kws={\"size\":10},\n                yticklabels=df_train[['Survived', 'Pclass','SibSp','Parch','Sex_cate', 'Age_band', 'Fare_band', 'Embarked_flg']].columns,\n                vmax=1,\n                vmin=-1,\n                center=0)\nplt.xlabel(\"Variables\")\nplt.ylabel(\"Variables\")","44d0a15d":"# Make the target data and explanatry data\nX = df_train[['Pclass','SibSp','Parch','Sex_cate', 'Age_band', 'Fare_band', 'Embarked_flg']]\ny = df_train[['Survived']]\n\n# Data splitting to make the training data and validation data\n# training data :80%, validation(test data) :20%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Taking veryfing to Standarlized data\nsc = StandardScaler()\nsc.fit(X_train)\n\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)","0c81547c":"# Logistic Regression\nlr = LogisticRegression()\n\nparam_range = [0.001, 0.01, 0.1, 1.0]\npenalty = ['l1', 'l2']\nparam_grid = [{\"C\":param_range, \"penalty\":penalty}]\n\ngs_lr = GridSearchCV(estimator=lr, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_lr = gs_lr.fit(X_train_std, y_train)\n\nprint(gs_lr.best_score_.round(3))\nprint(gs_lr.best_params_)","ed42578c":"# Support vector machine, SVM\nsvm = SVC(random_state=10, probability=True)\n\nparam_range = [0.001, 0.01, 0.1, 1.0]\nparam_grid = [{'C':param_range, 'kernel':['linear']}]\n\ngs_svm = GridSearchCV(estimator=svm, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1)\ngs_svm = gs_svm.fit(X_train_std, y_train)\n\nprint(gs_svm.best_score_.round(3))\nprint(gs_svm.best_params_)","22208e77":"# KNeithborsClassfier\nknn = KNeighborsClassifier(metric='minkowski')\n\nparam_range = [5, 10, 15, 20]\nparam_grid = [{\"n_neighbors\":param_range, \"p\":[1,2]}]\n\ngs_knn = GridSearchCV(estimator=knn, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_knn = gs_knn.fit(X_train_std, y_train)\n\nprint(gs_knn.best_score_.round(3))\nprint(gs_knn.best_params_)","db391b4b":"# Decision tree\ntree = DecisionTreeClassifier(max_depth=4, random_state=10)\n\nparam_range = [3, 6, 9, 12]\nleaf = [10, 15, 20]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\ngs_tree = GridSearchCV(estimator=tree, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_tree = gs_tree.fit(X_train, y_train)\n\nprint(gs_tree.best_score_.round(3))\nprint(gs_tree.best_params_)","daa532b7":"# Random Forest\nforest = RandomForestClassifier(n_estimators=100, random_state=10)\n\nparam_range = [5, 10, 15]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"n_estimators\":param_range, \"criterion\":criterion}]\n\ngs_forest = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_forest = gs_forest.fit(X_train, y_train)\n\nprint(gs_forest.best_score_.round(3))\nprint(gs_forest.best_params_)","37581684":"# XGB\nxgbc = xgb.XGBClassifier(random_state=10)\n\n# prameters\nmax_depth = [10, 15, 20, 25]\nmin_samples_leaf = [1,3,5]\nmin_samples_split = [1,2,4]\n\nparam_grid = [{\"max_depth\":max_depth,\n               \"min_samples_leaf\":min_samples_leaf, \"min_samples_split\":min_samples_split}]\n\n# Optimization by Grid search\ngs_xgb = GridSearchCV(estimator=xgbc, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\n\ngs_xgb = gs_xgb.fit(X_train, y_train)\n\nprint(gs_xgb.best_score_)\nprint(gs_xgb.best_params_)","490cb7b6":"# LGBM\nlgbm = lgb.LGBMClassifier()\n\n# prameters\nmax_depth = [3, 5, 10]\nmin_samples_leaf = [1,3,5,7]\nmin_samples_split = [2, 4, 6, 8]\n\nparam_grid = [{\"max_depth\":max_depth,\n               \"min_samples_leaf\":min_samples_leaf, \"min_samples_split\":min_samples_split}]\n\n# Optimization by Grid search\ngs_lgbm = GridSearchCV(estimator=lgbm, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\ngs_lgbm = gs_lgbm.fit(X_train, y_train)\n\nprint(gs_lgbm.best_score_.round(3))\nprint(gs_lgbm.best_params_)","b345fcbd":"print(\"-\"*50)\n# Logistic Regression Result\ny_pred = gs_lr.best_estimator_.predict(X_test_std)\nprint(\"Logistic Regression Result\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)\n\n# Support vector machine, SVM\ny_pred = gs_svm.best_estimator_.predict(X_test_std)\nprint(\"Support vector machine, SVM\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)\n\n# KNeithborsClassfier\ny_pred = gs_knn.best_estimator_.predict(X_test_std)\nprint(\"KNeithborsClassfier\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)\n\n# Decision tree\ny_pred = gs_tree.best_estimator_.predict(X_test)\nprint(\"Decision tree\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)\n\n# Random Forest\ny_pred = gs_forest.best_estimator_.predict(X_test)\nprint(\"Random Forest\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)\n\n# XGB\ny_pred = gs_xgb.best_estimator_.predict(X_test)\nprint(\"XGB\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)\n\n# LGBM\ny_pred = gs_lgbm.best_estimator_.predict(X_test)\nprint(\"LGBM\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)","dd41b055":"# Visualization of roc curve\nfig, ax = plt.subplots(2,4, figsize=(25, 10))\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\n\n# Logistic Regression\ny_score = gs_lr.best_estimator_.predict_proba(X_test_std)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_score)\n\nax[0,0].plot(fpr, tpr, label=\"roc curve (area = %.3f)\" % auc(fpr, tpr))\nax[0,0].plot([0,1], [0,1], linestyle='--', label='random')\nax[0,0].plot([0,0,1], [0,1,1], linestyle='--', label=\"ideal\")\nax[0,0].legend()\nax[0,0].set_xlabel(\"false positive rate\")\nax[0,0].set_ylabel(\"true positive rate\")\nax[0,0].set_title(\"Logistic Regression\")\n\n# Support vector machine, SVM\ny_score = gs_svm.best_estimator_.predict_proba(X_test_std)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_score)\n\nax[0,1].plot(fpr, tpr, label=\"roc curve (area = %.3f)\" % auc(fpr, tpr))\nax[0,1].plot([0,1], [0,1], linestyle='--', label='random')\nax[0,1].plot([0,0,1], [0,1,1], linestyle='--', label=\"ideal\")\nax[0,1].legend()\nax[0,1].set_xlabel(\"false positive rate\")\nax[0,1].set_ylabel(\"true positive rate\")\nax[0,1].set_title(\"Support vector machine, SVM\")\n\n# KNeithborsClassfier\ny_score = gs_knn.best_estimator_.predict_proba(X_test_std)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_score)\n\nax[0,2].plot(fpr, tpr, label=\"roc curve (area = %.3f)\" % auc(fpr, tpr))\nax[0,2].plot([0,1], [0,1], linestyle='--', label='random')\nax[0,2].plot([0,0,1], [0,1,1], linestyle='--', label=\"ideal\")\nax[0,2].legend()\nax[0,2].set_xlabel(\"false positive rate\")\nax[0,2].set_ylabel(\"true positive rate\")\nax[0,2].set_title(\"KNeithborsClassfier\")\n\n# Decision tree\ny_score = gs_tree.best_estimator_.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_score)\n\nax[0,3].plot(fpr, tpr, label=\"roc curve (area = %.3f)\" % auc(fpr, tpr))\nax[0,3].plot([0,1], [0,1], linestyle='--', label='random')\nax[0,3].plot([0,0,1], [0,1,1], linestyle='--', label=\"ideal\")\nax[0,3].legend()\nax[0,3].set_xlabel(\"false positive rate\")\nax[0,3].set_ylabel(\"true positive rate\")\nax[0,3].set_title(\"Decision tree\")\n\n# Forest\ny_score = gs_forest.best_estimator_.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_score)\n\nax[1,0].plot(fpr, tpr, label=\"roc curve (area = %.3f)\" % auc(fpr, tpr))\nax[1,0].plot([0,1], [0,1], linestyle='--', label='random')\nax[1,0].plot([0,0,1], [0,1,1], linestyle='--', label=\"ideal\")\nax[1,0].legend()\nax[1,0].set_xlabel(\"false positive rate\")\nax[1,0].set_ylabel(\"true positive rate\")\nax[1,0].set_title(\"Random Forest\")\n\n# xgb\ny_score = gs_xgb.best_estimator_.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_score)\n\nax[1,1].plot(fpr, tpr, label=\"roc curve (area = %.3f)\" % auc(fpr, tpr))\nax[1,1].plot([0,1], [0,1], linestyle='--', label='random')\nax[1,1].plot([0,0,1], [0,1,1], linestyle='--', label=\"ideal\")\nax[1,1].legend()\nax[1,1].set_xlabel(\"false positive rate\")\nax[1,1].set_ylabel(\"true positive rate\")\nax[1,1].set_title(\"XGB\")\n\n# lgbm\ny_score = gs_lgbm.best_estimator_.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_score)\n\nax[1,2].plot(fpr, tpr, label=\"roc curve (area = %.3f)\" % auc(fpr, tpr))\nax[1,2].plot([0,1], [0,1], linestyle='--', label='random')\nax[1,2].plot([0,0,1], [0,1,1], linestyle='--', label=\"ideal\")\nax[1,2].legend()\nax[1,2].set_xlabel(\"false positive rate\")\nax[1,2].set_ylabel(\"true positive rate\")\nax[1,2].set_title(\"LGBM\")\n","6dfd1b35":"test = df_test[['Pclass', 'SibSp', 'Parch', 'Sex_cate', 'Age_band', 'Fare_band', 'Embarked_flg']]\n\n# Decision tree\ny_pred_test = gs_tree.best_estimator_.predict(test)\n\nsubmit = pd.DataFrame({\"PassengerId\":df_test[\"PassengerId\"], \"Survived\":y_pred_test})","e2481a62":"submit.head()","e35d9cd0":"submit.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","122a66f2":"# prediction\ny_pred_test_lgbm = gs_lgbm.best_estimator_.predict(test)\ny_pred_test_xgb = gs_xgb.best_estimator_.predict(test)\n\n# Ensemble\ny_pred_test_en = (y_pred_test*0.3 + y_pred_test_lgbm*0.4 + y_pred_test_xgb*0.3)\n\nsubmit_en = pd.DataFrame({\"PassengerId\":df_test[\"PassengerId\"], \"Survived\":y_pred_test_en}).round(0)\nsubmit_en[\"Survived\"] = [int(i) for i in submit_en[\"Survived\"]]","ff12f71c":"submit_en.to_csv('my_submission_en.csv', index=False)\nprint(\"Your submission was successfully saved!\")","c0fb3ba0":"submit_en","df41a83f":"submit","1c11479b":"Result<br>\n- \"Age\" & \"Cabin\" & \"Embarked\" has null data, especially Cabin data has many null data. How we take this, we need to judge.<br>\n- \"Name\" & \"Sex\" & \"Ticket\" & \"Cabin\" & \"Embarked\" is strings data. If we takes them to ML model, we need to change to conver that can be numerical variables.<br>\n- \"Ticket\" & \"Cabin\" is mixed data of numbers and character strings. Features need to be carefully captured.<br>","9f1fd1f4":"I tried some machine learning method, as below.<br>\n\n1. Logistic Regression<br>\n2. Support vector machine, SVM<br>\n3. KNeithborsClassfier<br>\n4. Decision tree<br>\n5. Random Forest<br>\n\nAnd I used GridSearcCV to tune each model parameters.","2839d1f1":"### Numerical values<br>\nAge, Fare","656ed6b9":"## 7. Evaluation of result","32ee9d11":"Preparing the data for machine learning","e627b215":"### Null data<br>\nThese are needed to cleaning as below, and checking dataset, I decided to direction.<br>\n- Null data of age : TO fill by mean of data.\n- Null data of Fare : To fill by mean of data.\n- Null data of Cabin : To drop from dataset.\n- Null data of Embarked in test data : To fill by mode.","0826a900":"### Confirming the data<br>\n-------------------------------------------------------------<br>\nTarget variables<br>\n\nSurvived : Class label, 0 for deceased, 1 for survived<br>\n-------------------------------------------------------------<br>\nExplanatory variables<br>\n\nPassengerId : As it is, <br>\n\nPclass : Ticket class 1=1st or 2=2nd or 3=3rd<br>\n\nName : As it is, <br>\n\nSex : As it is, female or male<br>\n\nAge : Age in years <br>\n  *Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br>\n\nSibSp : # of siblings \/ spouses aboard the Titanic. This defines family relations as belows,<br>\n- Sibling = brother, sister, stepbrother, stepsister <br>\n- Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)<br>\n\nParch : # of parents \/ children aboard the Titanic. This defines family relations as belows,<br>\n- Parent = mother, father<br>\n- Child = daughter, son, stepdaughter, stepson<br>\n- Some children travelled only with a nanny, therefore parch=0 for them.<br>\n\nTicket : Ticket number<br>\n\nFare : Passenger fare<br>\n\nCabin : Cabin number<br>\n\nEmbarked : Port of Embarkation<br>\n-------------------------------------------------------------<br>","6e3ce74a":"## Explanatory variables data distributions<br>\n### Categorycal values<br>\nPclass, Sex, SibSP, Parch, Embarked","f8c3c536":"We need to confirm the overview and features of data to make prediction model by machine learning.<br>\nThis phase, we check the data with visualization, and we decide how to arrange the variables to ML model.<br>\n\nSo, I analyzed the data using the training data.","734bc174":"## 3. Exploratory Data Analysis, EDA","46247b08":"- Age values are widely distributed.\n- Fare values are biased below 100.\n\nNext, Checking distribution, survive or not, for Age and Fare.","6963022e":"## 1. Introduction, target setting <br>\nConcern : To predict survivors of titanic by some customer information<br>\nOutput : Prediction accyarcy with 0 or 1 signal each customer<br>\nSubmission : With csv file, that is prediction of test file data. It should have 2 columns (Passenger id & Survived (contains your binary predictions: 1 for survived, 0 for deceased))","388e31ac":"- Given that the overall rate of deceased is 62%, Passenger class and Sex may have important information of classification.<br>\n*Maybe it is need to check a statistical test this difference is significant, but this time I stopped.","75cc0084":"- About age, younger people may be more survive.\n- Fare, it may not confirm the features that seem to be effective.\n\nDistributions are widely and relatively evenly distributed. So, I decide to change categorical value by dividing equaly each range.","810058e5":"## 5. Decision of variables for Machine learning","3db40bf8":"### Data loading<br>\nThis data is provided by .csv file.","9deae5a3":"## Agenda of this notebook<br>\n\n### 1. Introduction, target setting <br>\n\n### 2. Data loading and Confirming the data <br>\n\n### 3. Exploratory Data Analysis, EDA <br>\n\n### 4. Data cleaning & Preprocessing<br>\n\n### 5. Decision of variables for Machine learning<br>\n\n### 6. Prediction with \"Machine learning method\" <br>\n\n### 7. Evaluation of result\n\n### 8. Test data prediction","781c44a2":"## 8. Test data prediction","d7de1c6d":"## Best estimator : Decision tree , accuracy = 0.827\nDecision tree model is \n- Accuracy is best.\n- F1 score is best.\n- AUC is best.","32adffe3":"- Passenger class, 3 is most class.\n- Sex, male > female.\n- SibSP & Parch, people are almost 0, 1, 2.\n- Embarked, S is the most.\n\nNext, Checking ratio of survive or not, for each values.","a4742a5e":"## ensemble submission data (Decision tree and XGB and LGBM)","461b9edb":"### Change the categolical value to integer<br>\n- Sex : male=0, female=1\n- Age : change to age band, 7 categories (0 ~ 6 category, by split per 10)\n- Fare : change to Fare band, 5 categories (0 ~ 5 category, by split per 25)\n- Embarked : Add to categorical values, 3 categories (S: 0, C: 1, Q: 2)","66a42e18":"Target variables(Survived flag) data count","0bc38542":"# Starting, Titanic : Prediction of survivor\n\nI will try to predict this \"Survivor of titanic\" by some machine learning method. This approach is taken by basic stage. \nI'm a beginner, to confirm my own steps, I summarized them by step. I hope if you'll find it helpful for those who are just starting out like me.<br>\nAnd I apologize my poor English.","5bc9a214":"### Selecting the variables<br>\n\nI decide values \u21d2 'Pclass','SibSp','Parch','Sex_cate', 'Age_band', 'Fare_band', 'Embarked_flg'<br>\n\nConfirming if there is a strong correlation by each variables.","4fd87cf5":"- Overall survival and deceased ratio is about 62:38 (deceased:survived).\n- This data set does not need to take imbalanced data method (Like under sampling or over sampling method).","d8d396e4":"## 4. Data cleaning & Preprocessing<br>","953fa171":"Maybe, Fare_band & Pclass have nagetive strong correlation.<br>\nBut, It is on about border that is difficult to judge, so I decided to continue.","d0f0a0c7":"## 2. Data loading and Confirming the data","68502e41":"## 6. Prediction with \"Machine learning method\""}}