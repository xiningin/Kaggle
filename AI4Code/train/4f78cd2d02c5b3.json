{"cell_type":{"3be6355f":"code","d209c16a":"code","855edc7a":"code","0c63a942":"code","2c023060":"code","65b7036f":"code","833546f9":"code","52964186":"code","030800bd":"code","25f2e773":"code","7dad201f":"code","2609934b":"code","a18b09e6":"code","7253fc6c":"code","25666c83":"code","f5971027":"code","1a87cb91":"code","dd1607ca":"code","e48621ce":"code","603b51dd":"code","6c6cb176":"code","06ebb133":"code","ad869f49":"code","ea2d51b2":"code","9c7f6a7f":"code","73793558":"code","abfcad2e":"code","a60ad068":"code","caffd339":"code","192b3e58":"code","731a54ad":"code","12d362f8":"code","2f31951d":"code","ef24ea10":"code","f74727d6":"code","2c5c18a6":"code","bd2962f8":"code","6d7af156":"code","173d79c2":"code","28d0fa0f":"code","de06abf6":"code","2b3bf298":"code","a47fdd93":"code","b623f75c":"code","d1c5c1ef":"code","136de9d8":"markdown","fcfe15ac":"markdown","67267566":"markdown","4694bb95":"markdown","33328328":"markdown","d743c513":"markdown","265e22fb":"markdown","8c266be0":"markdown","b88b6778":"markdown","1a8d8cad":"markdown"},"source":{"3be6355f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d209c16a":"import pandas as pd\ntitanic=pd.read_csv(\"\/kaggle\/input\/mlcourse\/titanic_train.csv\")\ntitanic.head()","855edc7a":"# tail shows last 5 rows\ntitanic.tail()","0c63a942":"# columns gives column names of features\ntitanic.columns","2c023060":"# shape gives number of rows and columns in a tuble\ntitanic.shape\n#Our dataSet has 12 columns ,891 rows","65b7036f":"# info gives data type like dataframe, number of sample or row, number of feature or column, feature types and memory usage\ntitanic.info()","833546f9":"# For example lets look frequency of gender in ship\nprint(titanic['Sex'].value_counts(dropna=False))  # if there are nan values that also be counted\n# As it can be seen below there are 577 male and 314 female person","52964186":"# For example max HP is 255 or min defense is 5\ntitanic.describe() #ignore null entries","030800bd":"# For example: compare attack of pokemons that are legendary  or not\n# Black line at top is max\n# Blue line at top is 75%\n# Red line is median (50%)\n# Blue line at bottom is 25%\n# Black line at bottom is min\n# There are no outliers\ntitanic.boxplot(column='SibSp',by = 'Sex')","25f2e773":"titanic.boxplot(column='Fare',by = 'Sex')","7dad201f":"topTen=titanic.head(10)\ntopTen","2609934b":"melted = pd.melt(frame=topTen,id_vars = 'Name', value_vars= ['Age','Fare'])\nmelted","a18b09e6":"# Index is name\n# I want to make that columns are variable\n# Finally values in columns are value\nmelted.pivot(index = 'Name', columns = 'variable',values='value')","7253fc6c":"#Vertical Concatenating\n# Firstly lets create 2 data frame\ndata1 = titanic.head()\ndata2= titanic.tail()\nconc_data_row = pd.concat([data1,data2],axis =0,ignore_index =True) # axis = 0 : adds dataframes in row\n#ignore_index=True :ignore the old indexis.Assign new indexis\nconc_data_row","25666c83":"#Horizontal Concatenating\ndata1 = titanic['Name'].head()\ndata2= titanic['Age'].head()\nconc_data_col = pd.concat([data1,data2],axis =1) # axis = 1 : adds dataframes in column\nconc_data_col\n\n","f5971027":"titanic.dtypes","1a87cb91":"# lets convert object(str) to categorical and int to float.\ntitanic['Sex'] = titanic['Sex'].astype('category')\ntitanic['Parch'] = titanic['Parch'].astype('float')\ntitanic.dtypes","dd1607ca":"titanic.info()","e48621ce":"\n# Lets chech Age\ntitanic[\"Age\"].value_counts(dropna =False)\n# As you can see, there are 177 NAN value","603b51dd":"# Lets drop nan values\ndata1=titanic   # also we will use data to fill missing value so I assign it to data1 variable\ndata1[\"Age\"].dropna(inplace = True)  # inplace = True means we do not assign it to new variable.\n#Changes automatically assigned to data\n# So does it work ?\n#(Age i olmayan insanlar\u0131 \u00e7\u0131kar.inplace=True :\u00e7\u0131kar ve \u00e7\u0131kar\u0131lm\u0131\u015f halini data1 e kaydet)","6c6cb176":"#  Lets check with assert statement\n# Assert statement:\nassert 1==1 # return nothing because it is true\n# assert 1==2 # return error because it is false","06ebb133":"assert  titanic['Age'].notnull().all() # returns nothing because we drop nan values","ad869f49":"titanic[\"Age\"].fillna('empty',inplace = True)","ea2d51b2":"assert  titanic['Age'].notnull().all() # returns nothing because we do not have nan values","9c7f6a7f":"# # With assert statement we can check a lot of thing. For example\nassert titanic.columns[0] == 'PassengerId' #true\nassert titanic.PassengerId.dtypes == np.int64 #true ,so it wont show us any error","73793558":"titanic.info()","abfcad2e":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport timeit\ntitanic[\n    (titanic.Sex=='female')\n    &( titanic[\"Pclass\"].isin([1,3]))\n    &( titanic.Age >40)\n    & (titanic.Survived == 0)\n]","a60ad068":"towns_dic = {\n    'name': ['Southampton', 'Cherbourg', 'Queenstown', 'Montevideo'],\n    'country': ['United Kingdom', 'France', 'United Kingdom', 'Uruguay'],\n    'population': [236900, 37121, 12347, 1305000],\n    'age': [np.random.randint(500, 1000) for _ in range(4)]\n}\ntowns_df = pd.DataFrame(towns_dic)","caffd339":"sns.distplot(titanic.Age.dropna())\n","192b3e58":"g = sns.FacetGrid(titanic, row='Survived', col='Pclass')\ng.map(sns.distplot, \"Age\")","731a54ad":"#sns.jointplot(data=titanic, x='Age', y='Pclass', kind='reg', color='g')\n","12d362f8":"df = titanic.pivot_table(index='Embarked', columns='Survived', values='Fare', aggfunc=np.median)\nsns.heatmap(df, annot=True, fmt=\".1f\")\n#corelation matrix","2f31951d":"data=titanic.copy()","ef24ea10":"data.tail()","f74727d6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\nimport re\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam","2c5c18a6":"data.Cabin","bd2962f8":"def preprocess(data):\n    data.Cabin.fillna('0',inplace=True)\n    data.loc[data.Cabin.str[0]=='A','Cabin']=1\n    data.loc[data.Cabin.str[0]=='B','Cabin']=2\n    data.loc[data.Cabin.str[0]=='C','Cabin']=3\n    data.loc[data.Cabin.str[0]=='D','Cabin']=4\n    data.loc[data.Cabin.str[0]=='E','Cabin']=5\n    data.loc[data.Cabin.str[0]=='F','Cabin']=6\n    data.loc[data.Cabin.str[0]=='G','Cabin']=7\n    data.loc[data.Cabin.str[0]=='T','Cabin']=8\n    \n    data['Sex'].replace('female',1,inplace=True)\n    data['Sex'].replace('male',2,inplace=True)\n\n    data['Embarked'].replace('S',1,inplace=True)\n    data['Embarked'].replace('C',2,inplace=True)\n    data['Embarked'].replace('Q',3,inplace=True)\n    \n    #I wanna replace empty spaces with median value\n    data['Age'].fillna(data['Age'].median(),inplace=True) \n    data['Fare'].fillna(data['Fare'].median(),inplace=True)\n    data['Embarked'].fillna(data['Embarked'].median(),inplace=True)\n    \n    #If u wanna delete all empty values:\n    #data.dropna(subset=['Fare','Embarked'],inplace=True,how='any')\n    #BUT we dont prefer this option because when yu do it, u ll lose all info about these people\n    \n    return data","6d7af156":"def group_titles(data):\n    data['Names'] = data['Name'].map(lambda x: len(re.split(' ', x)))\n    data['Title'] = data['Name'].map(lambda x: re.search(', (.+?) ', x).group(1))\n    data['Title'].replace('Master.', 0, inplace=True)\n    data['Title'].replace('Mr.', 1, inplace=True)\n    data['Title'].replace(['Ms.','Mlle.', 'Miss.'], 2, inplace=True)\n    data['Title'].replace(['Mme.', 'Mrs.'], 3, inplace=True)\n    data['Title'].replace(['Dona.', 'Lady.', 'the Countess.', 'Capt.', 'Col.', 'Don.', 'Dr.', 'Major.', 'Rev.', 'Sir.', 'Jonkheer.', 'the'], 4, inplace=True)\n\n     \n    ","173d79c2":"def data_subset(data):\n    features = ['Pclass', 'SibSp', 'Parch', 'Sex', 'Names', 'Title', 'Age', 'Cabin'] #, 'Fare', 'Embarked']\n    lengh_features = len(features)\n    subset = data[features]#.fillna(0)\n    return subset, lengh_features","28d0fa0f":"#Design the model\n#batch_size= cluster size \n#Dense =layer\n#activation function=Can be softplus,ReLU,Sigmoid ..etc.\n#lr =learning rate\n\ndef create_model(train_set_size, input_length, num_epochs, batch_size):\n    model = Sequential()\n    model.add(Dense(7, input_dim=input_length, activation='softplus'))\n    model.add(Dense(3, activation='softplus'))\n    model.add(Dense(1, activation='softplus'))\n\n    lr = .001\n    adam0 = Adam(lr = lr)\n\n    #Execute the model ,if you find better results save these weights\n    \n    model.compile(loss='binary_crossentropy', optimizer=adam0, metrics=['accuracy'])\n    filepath = 'weights.best.hdf5'\n    checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n    callbacks_list = [checkpoint]\n\n    history_model = model.fit(X_train[:train_set_size], Y_train[:train_set_size], callbacks=callbacks_list, epochs=num_epochs, batch_size=batch_size, verbose=0) #40, 32\n    return model, history_model","de06abf6":"def plots(history):\n    loss_history = history.history['loss']\n    acc_history = history.history['accuracy']\n    epochs = [(i + 1) for i in range(num_epochs)]\n\n    ax = plt.subplot(211)\n    ax.plot(epochs, loss_history, color='red')\n    ax.set_xlabel('Epochs')\n    ax.set_ylabel('Error Rate\\n')\n    ax.set_title('Error Rate per Epoch\\n')\n\n    ax2 = plt.subplot(212)\n    ax2.plot(epochs, acc_history, color='blue')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Accuracy\\n')\n    ax2.set_title('Accuracy per Epoch\\n')\n\n    plt.subplots_adjust(hspace=0.8)\n    plt.savefig('Accuracy_loss.png')\n    plt.close()","2b3bf298":"\n#test=pd.read_csv(\"\/kaggle\/input\/mlcourse\/titanic_test.csv\")\n\ndef test(batch_size):\n    test = pd.read_csv(\"\/kaggle\/input\/mlcourse\/titanic_test.csv\", header=0)\n    test_ids = test['PassengerId']\n    test = preprocess(test)\n    group_titles(test)\n    testdata, _ = data_subset(test)\n\n    X_test = np.array(testdata).astype(float)\n\n    output = model.predict(X_test, batch_size=batch_size, verbose=0)\n    output = output.reshape((418,))\n\n    # Sonu\u00e7lar\u0131 ondal\u0131k say\u0131 yerine 0-1 olarak de\u011fi\u015ftirebilirsiniz\n    #outputBin = np.zeros(0)\n    #for element in output:\n    #    if element <= .5:\n    #         outputBin = np.append(outputBin, 0)\n    #    else:\n    #        outputBin = np.append(outputBin, 1)\n    #output = np.array(outputBin).astype(int)\n\n    column_1 = np.concatenate((['PassengerId'], test_ids ), axis=0 )\n    column_2 = np.concatenate( ( ['Survived'], output ), axis=0 )\n\n    f = open(\"output.csv\", \"w\")\n    writer = csv.writer(f)\n    for i in range(len(column_1)):\n        writer.writerow( [column_1[i]] + [column_2[i]])\n    f.close()","a47fdd93":"# results should be reproductible\nseed = 7\nnp.random.seed(seed)\n\n\ntrain = pd.read_csv('\/kaggle\/input\/mlcourse\/titanic_train.csv', header=0)\n\n\npreprocess(train)\ngroup_titles(train)\n\n\nnum_epochs = 100\nbatch_size = 32\n\n\n\ntraindata, lengh_features = data_subset(train)\n\nY_train = np.array(train['Survived']).astype(int)\nX_train = np.array(traindata).astype(float)\n\n\ntrain_set_size = int(.67 * len(X_train))\n\n\nmodel, history_model = create_model(train_set_size, lengh_features, num_epochs, batch_size)\n\nplots(history_model)\n\n\nX_validation = X_train[train_set_size:]\nY_validation = Y_train[train_set_size:]\n\n\nloss_and_metrics = model.evaluate(X_validation, Y_validation, batch_size=batch_size)\nprint (\"loss_and_metrics\")\n\ntest(batch_size)","b623f75c":"#\n#plt.show(\".\/Accuracy_loss.png\")\n\nfrom IPython.display import Image\nImage(\".\/Accuracy_loss.png\")","d1c5c1ef":"\noutput = pd.read_csv('.\/output.csv', header=0)\noutput.head()\n","136de9d8":"# TIDY DATA\nWe tidy data with melt().<br>\nMelting and casting are the functions that can be used efficiently to reshape the data. The functions used to do this are called melt() and cast().\n","fcfe15ac":"### **CATEGORICAL DATA TYPE**\nCategoricals are a pandas data type corresponding to categorical variables in statistics. A categorical variable takes on a limited, and usually fixed, number of possible values (categories; levels in R). Examples are gender, social class, blood type, country affiliation, observation time or rating via Likert scales.\n<br> Why is category important: \n* make dataframe smaller in memory \n* can be utilized for analysis especially for sklearn\n","67267566":"![](https:\/\/www.google.com.tr\/url?sa=i&source=images&cd=&ved=2ahUKEwiNrsbTjvjkAhUBfFAKHWKlAIoQjRx6BAgBEAQ&url=http%3A%2F%2Friti-ritesh.blogspot.com%2F&psig=AOvVaw1W2AO1NMm9ZDJ57nb5nVGa&ust=1569918058490414)\n","4694bb95":"# PIVOTING DATA\n*(Reverse of melting)*<br>\nA pivot transformation is one way to transform data from a tall\/skinny format to a short\/wide format. The data is distributed into columns usually aggregating the values. This means that multiple values from the original data end up in the same place in the new data table.\n","33328328":"# EXPLORATORY DATA ANALYSIS(EDA)\nvalue_counts(): Frequency counts\n<br>outliers: the value that is considerably higher or lower from rest of the data\n* Lets say value at 75% is Q3 and value at 25% is Q1. \n* Outlier are smaller than Q1 - 1.5(Q3-Q1) and bigger than Q3 + 1.5(Q3-Q1). (Q3-Q1) = IQR\n<br>We will use describe() method. Describe method includes:\n* count: number of entries\n* mean: average of entries\n* std: standart deviation\n* min: minimum entry\n* 25%: first quantile\n* 50%: median or second quantile\n* 75%: third quantile\n* max: maximum entry\n\n<br> What is quantile?\n\n* 2,4,5,6,8,9,11,12,13,14,15,16,47\n* The median is the number that is in **middle** of the sequence. In this case it would be 11.\n\n* The lower quartile is the median in between the smallest number and the median i.e. in between 2 and 11, which is 6.(Q1)(%25)\n* The upper quartile, you find the median between the median and the largest number i.e. between 11 and 47, which will be 14 according to the question above.(Q3)(%75)","d743c513":"# MISSING DATA and TESTING WITH ASSERT\nIf we encounter with missing data, what we can do:\n* leave as is\n* drop them with dropna() (We can remove it from dataset)\n* fill missing value with fillna() (We can fill them with Nan) \n* fill missing values with test statistics like mean\n<br>Assert statement: check that you can turn on or turn off when you are done with your testing of the program","265e22fb":"# CONCATENATING DATA\nWe can concatenate two dataframe ","8c266be0":"![image.png](attachment:image.png)","b88b6778":"# VISUAL EXPLORATORY DATA ANALYSIS\n**Box plots:**\n<br>Visualize basic statistics like outliers, min\/max or quantiles\nIn descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram.","1a8d8cad":"*Summary<br>*\nOur dataSet has 891 Passenger ID. Maximum fare for ticket is 512 and minimum is 0. In the middle of our dataset there is a person whose age is 28 and he or she paid 14$ for a single ticket.\n<br><br>**Why are we looking for median,lower and upper quartile .. etc**\n<br>To make an accurate overview of our data\n"}}