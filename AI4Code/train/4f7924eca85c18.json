{"cell_type":{"264e5d35":"code","23cff4b0":"code","6b89a334":"code","e8048055":"code","f6e4ae37":"code","5438fdee":"code","a0d3e4e6":"code","c60d55d2":"code","4d6a5774":"code","0bdba8b8":"code","6ae2e760":"code","f4b978fb":"code","d32475f0":"code","9b9945b8":"code","63560448":"code","1e55c662":"code","ab6c6cd2":"code","14f68a6d":"code","980bfc77":"code","d13794d0":"code","5c6be4e7":"code","0ebd59ce":"code","8c967d42":"code","00c4244d":"code","6ef030bc":"code","b81d9d79":"code","f4ebb0f5":"code","66f021ab":"code","674f6a21":"code","448137ea":"code","a2ea4dad":"code","bec1b5c2":"code","923530c4":"markdown"},"source":{"264e5d35":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname,_,filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(filename)\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","23cff4b0":"import torch\nimport torchvision \nfrom torchvision import datasets, transforms, models\nfrom torch import optim as optim\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\nimport matplotlib\nimport matplotlib.patches as patches\nimport glob\nimport xml.etree.ElementTree as ET\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom tqdm import tqdm","6b89a334":"label = list(sorted(os.listdir(\"..\/input\/stanford-dogs-dataset\/images\/Images\/\")))","e8048055":"list_annotations = []\nlist_images = []","f6e4ae37":"for idx, l in enumerate(label):\n    labels_annotation = list(sorted(os.listdir(os.path.join(\"..\/input\/stanford-dogs-dataset\/annotations\/Annotation\", l))))[51:80]\n    images = list(sorted(os.listdir(os.path.join(\"..\/input\/stanford-dogs-dataset\/images\/Images\", l))))[51:80]\n    labels_annotation_ = [{labels_: idx} for labels_ in labels_annotation]\n    images_ = [{image_: idx} for image_ in images]\n    list_annotations += labels_annotation_\n    list_images += images_","5438fdee":"class Dogs(torch.utils.data.Dataset):\n    def __init__(self, list_images, list_annotations, transform):\n        self.img_path = \"..\/input\/stanford-dogs-dataset\/images\/Images\/\"\n        self.annotation_path = \"..\/input\/stanford-dogs-dataset\/annotations\/Annotation\/\"\n        self.list_images = list_images\n        self.list_annotations = list_annotations\n        self.transform = transform\n        \n    def __getitem__(self, idx):\n        for x, y in self.list_images[idx].items():\n            image_name, l = x, y\n        for x_, y_ in self.list_annotations[idx].items():\n            annotation_name, l_ = x_, y_\n        image = cv2.imread(self.img_path + label[l] + \"\/\" + image_name, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.\n\n        image_id = torch.tensor([idx])\n        label_image = torch.as_tensor(1, dtype=torch.int64).view(-1,)\n        \n        tree = ET.parse(self.annotation_path + label[l] + \"\/\" + annotation_name)\n        root = tree.getroot()\n        boxes = []\n        value = []\n        for x in root[5][4][:]:\n            value.append(int(x.text))\n        boxes.append(value)\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n#         boxes[0][0] += 2\n#         boxes[0][1] += 2\n#         boxes[0][2] -= 2\n#         boxes[0][3] -= 2\n\n            \n        area = (boxes[0][3]-boxes[0][1])*(boxes[0][2]-boxes[0][0])\n        area = torch.as_tensor(area, dtype=torch.float32).view(-1, )\n        \n        iscrowd = torch.zeros((1, ), dtype=torch.int64)\n        \n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = label_image\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n#         print(target['boxes'])\n        target['boxes'][0][2] = torch.clamp(target['boxes'][0][2], min=0, max=image.shape[1])\n        target['boxes'][0][3] = torch.clamp(target['boxes'][0][3], min=0, max=image.shape[0])\n\n        \n        if self.transform is not None:\n            \n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': label_image\n            }\n            try:\n                sample = self.transform(**sample)\n            except:\n                print('Fail case')\n                print(sample['bboxes'])\n                print(image.shape)\n            image = sample['image']\n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n            target['boxes'] = target['boxes'].float()\n#             target['boxes'][0][2] = torch.clamp(target['boxes'][0][2], min=0, max=image.shape[1])\n#             target['boxes'][0][3] = torch.clamp(target['boxes'][0][3], min=0, max=image.shape[2])\n\n        \n        return image, target\n    \n    \n    def __len__(self):\n        return len(self.list_images)","a0d3e4e6":"def get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n#         A.Resize(256, 256),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 't', 'label_fields': ['labels']})","c60d55d2":"dog_dataset = Dogs(list_images, list_annotations, get_train_transform())","4d6a5774":"rs = dog_dataset.__getitem__(1000)\nprint(rs)","0bdba8b8":"print(dog_dataset.__len__())","6ae2e760":"# fig, ax = plt.subplots()\n# ax.imshow(rs[0].numpy().transpose(1, 2,0))\n# rect = patches.Rectangle((rs[1][\"boxes\"][0][0], rs[1][\"boxes\"][0][1]), (rs[1][\"boxes\"][0][2]-rs[1][\"boxes\"][0][0]), (rs[1][\"boxes\"][0][3]-rs[1][\"boxes\"][0][1]), linewidth=2, edgecolor=\"r\", facecolor='none')\n# ax.add_patch(rect)\n# ax.text(rs[1][\"boxes\"][0][0], rs[1][\"boxes\"][0][1]-5, label[rs[1][\"labels\"]], color=\"r\", fontsize=10)","f4b978fb":"def my_collate(batch):\n    return tuple(zip(*batch))","d32475f0":"dt_loader = torch.utils.data.DataLoader(dog_dataset, 4, shuffle=True, collate_fn=my_collate)","9b9945b8":"# targets","63560448":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)","1e55c662":"valid_data = Dogs(list_images, list_annotations, get_valid_transform())\nvalid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=8, shuffle=True, collate_fn=my_collate)","ab6c6cd2":"itr_ = iter(valid_dataloader)\nimages, targets = next(itr_)\nimages = list(img for img in images)\ntargets = [{k: v for k, v in t.items()} for t in targets]","14f68a6d":"# model.eval()\n# cpu_device = torch.device(\"cpu\")\n\n# outputs = model(images)\n# outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]","980bfc77":"# print(outputs)","d13794d0":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","5c6be4e7":"device = torch.device(\"cuda\")","0ebd59ce":"# def predict(image, model, device, detection_threshold):\n#     image = transform(image).to(device)\n#     image = image.unsqueeze(0)\n#     out = model(image)\n    \n#     pred_classes = [coco_names[i] for i in out[0]['labels'].cpu().numpy()]\n    \n#     pred_scores = out[0]['scores'].detach().cpu().numpy()\n#     pred_bboxes = out[0]['boxes'].detach().cpu().numpy()\n    \n#     boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)\n    \n#     print(out)\n    \n#     return boxes, pred_classes, out[0]['labels']","8c967d42":"# def draw_bboxes(boxes, classes, labels, image):\n# #     image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n# #     image = image.cpu()\n#     fig, ax = plt.subplots()\n#     ax.imshow(image)\n#     for i, box in enumerate(boxes):\n#         color = COLORS[labels[i]]\n#         rect = patches.Rectangle((box[0], box[1]), (box[2]-box[0]), (box[3]-box[1]), linewidth=2, edgecolor=color, facecolor='none')\n#         ax.add_patch(rect)\n#         ax.text(box[0], box[1]-5, classes[i], color=color, fontsize=10)\n# #         cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)\n# #         cv2.putText(image, classes[i], (int(box[0]), int(box[1]-5)),\n# #                     cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n# #                     lineType=cv2.LINE_AA)","00c4244d":"model.train()\nnum_epochs = 1\nloss_hist = Averager()\nmodel = model.to(device)\nitr = 1\nlr_scheduler = None\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\nfor epoch in range(num_epochs):\n    for images, targets in tqdm(dt_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n#         print(loss_dict)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n        loss_hist.send(loss_value)\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n        itr += 1\n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")","6ef030bc":"# valid_data = Dogs(list_images, list_annotations, get_valid_transform())","b81d9d79":"# valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=8, shuffle=True, collate_fn=my_collate)","f4ebb0f5":"itr_ = iter(valid_dataloader)\nimages, targets = next(itr_)\nimages = list(img.to(device) for img in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]","66f021ab":"boxes = targets[1]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[1].permute(1,2,0).cpu().numpy()","674f6a21":"model.eval()\ncpu_device = torch.device(\"cpu\")\n\noutputs = model(images)\noutputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]","448137ea":"scores = outputs[0]['scores']\nboxes = outputs[0]['boxes']\n\nboxes_threshold = 0.5\nboxes = boxes[scores > boxes_threshold].astype(np.int32)","a2ea4dad":"print(outputs)","bec1b5c2":"fig, ax = plt.subplots(1, 1, figsize=(16, 8 ))\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(sample)","923530c4":"itr = iter(dt_loader)\nimgs, targets = next(itr)\nimages = list(image for image in imgs)\ntargets = [{k: v for k, v in t.items()} for t in targets]"}}