{"cell_type":{"4e3f888c":"code","9edbed10":"code","470e4588":"code","bee188b4":"code","22c0df22":"code","814eb36d":"code","e883f6b4":"code","bab18f2b":"code","92360665":"code","1390af00":"code","996c58b1":"code","b06646ee":"code","fd1505db":"code","ba5328d2":"code","193938d8":"code","b248e280":"code","0ccdf517":"code","8a4c5d5e":"code","97b4e1d0":"code","eaf7fe58":"code","23f9c9b2":"code","c316b334":"code","06ec05a4":"code","5d35f280":"code","d99a71d5":"code","b55b8f21":"code","ae653ad4":"code","43682ac0":"code","f381c748":"code","94c530bd":"code","9cec6016":"code","92ea463f":"code","b8abddd6":"code","17fc2e99":"code","adf920f2":"code","d23d2d02":"code","76be83e5":"code","939c7748":"code","d83ff190":"code","188e92a3":"code","dfc46a1c":"code","04796168":"code","7efab95b":"code","7ca9d183":"code","1ce851e0":"code","a4b4f596":"code","6a2c8c1d":"code","2c91fae9":"markdown","3fe44536":"markdown","ce4014d1":"markdown","50dfa94c":"markdown","fd892c71":"markdown","48b490a5":"markdown","988b48ef":"markdown","cd87975a":"markdown","83786632":"markdown","691985e8":"markdown","ce3ccd2c":"markdown","6db0cd5a":"markdown","7766ed65":"markdown","5c255b5e":"markdown","821af30a":"markdown","3708ac12":"markdown","83118599":"markdown","af2d1eae":"markdown","e7d50171":"markdown","113cc0ec":"markdown","3bc42b44":"markdown","6bb4e012":"markdown","60f9c9f0":"markdown","07dd6b8e":"markdown","49d6e3cd":"markdown","e996a786":"markdown","1129adc7":"markdown","ff0c9738":"markdown","1b1d5c1e":"markdown","c22beb2c":"markdown","63fa963f":"markdown","177d7d29":"markdown","7bebf134":"markdown","ff006fca":"markdown","80020c3b":"markdown","0a89c788":"markdown","bf387150":"markdown","b32398d7":"markdown","57da8c00":"markdown","a2e2e72e":"markdown","c05bdaa0":"markdown","190afc6c":"markdown","47ab371f":"markdown","375cf308":"markdown","1ed641cb":"markdown","06da83d3":"markdown","505f379a":"markdown","26c27f2f":"markdown"},"source":{"4e3f888c":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt","9edbed10":"import warnings\nwarnings.simplefilter(\"ignore\")","470e4588":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bee188b4":"df = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')\n# Read your datasets from the installed directory\ndf.head() #prints the first 5 rows of the dataset","22c0df22":"df.info()","814eb36d":"df.describe()","e883f6b4":"df.shape","bab18f2b":"from pandas_profiling import ProfileReport\nreport = ProfileReport(df,title='Summary Report of Student Placements')\nreport","92360665":"greater_70 = (df.ssc_p > 70) & (df.hsc_p > 70) & (df.mba_p >70)","1390af00":"df_70 = df[greater_70]\ndf_70.shape","996c58b1":"plt.hist(df_70.salary,bins=20)\nplt.show()","b06646ee":"df['workex'].value_counts() #Since its a categorical variable it does not require normalization","fd1505db":"df['specialisation'].value_counts()","ba5328d2":"df['degree_t'].value_counts()","193938d8":"df['hsc_s'].value_counts()","b248e280":"df['status'].value_counts()","0ccdf517":"df['gender'].value_counts()","8a4c5d5e":"import matplotlib.pyplot as plt ","97b4e1d0":"plt.hist(df['salary'],bins=20)\nplt.show()","eaf7fe58":"plt.scatter(df['ssc_p'],df['salary'])\nplt.xlabel('Percentage in SSC')\nplt.ylabel('Salary Offered')\nplt.title('Salary offered wrt SSC Percentage')\nplt.show()","23f9c9b2":"plt.scatter(df['hsc_p'],df['salary'])\nplt.xlabel('Percentage in HSC')\nplt.ylabel('Salary Offered')\nplt.title('Salary offered wrt HSC Percentage')\nplt.show()","c316b334":"plt.scatter(df['degree_p'],df['salary'])\nplt.xlabel('Percentage in Degree')\nplt.ylabel('Salary Offered')\nplt.title('Salary offered wrt Degree Percentage')\nplt.show()","06ec05a4":"plt.scatter(df['mba_p'],df['salary'])\nplt.xlabel('Percentage in MBA')\nplt.ylabel('Salary Offered')\nplt.title('Salary offered wrt MBA Percentage')\nplt.show()","5d35f280":"plt.hist(df['salary'],bins=20)\nplt.show()","d99a71d5":"from scipy import stats\n\ndegree_p = stats.norm.rvs(df['degree_p'])\nssc_p = stats.norm.rvs(df['ssc_p'])\nhsc_p = stats.norm.rvs(df['hsc_p'])\nmba_p = stats.norm.rvs(df['mba_p'])\nsalary = stats.norm.rvs(df['salary'])\netest_p = stats.norm.rvs(df['etest_p'])\nprint(\"Stat for degree:\", stats.shapiro(degree_p)) # Null Accepted\nprint(\"Stat for ssc:\", stats.shapiro(ssc_p)) # Null Accepted\nprint(\"Stat for hsc:\", stats.shapiro(hsc_p)) # Null Rejected\nprint(\"Stat for mba:\", stats.shapiro(mba_p)) # Null Accepted\nprint(\"Stat for salary:\", stats.shapiro(salary)) # Null Accepted \nprint(\"Stat for etest:\", stats.shapiro(etest_p)) # Null Rejected","b55b8f21":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ncolumns = df[['ssc_p','hsc_p','degree_p','mba_p','etest_p']]\nx_scaled = pd.DataFrame(scaler.fit_transform(columns))\nx_scaled.columns = ['ssc_p','hsc_p','degree_p','mba_p','etest_p']\nx_scaled.reset_index(drop=True, inplace=True)\nx_scaled","ae653ad4":"x_cat = df[['gender','ssc_b','hsc_b','hsc_s','degree_t','specialisation']]\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nx_cat['gender'] = le.fit_transform(x_cat.gender)\nx_cat['ssc_b'] = le.fit_transform(x_cat.ssc_b)\nx_cat['hsc_b'] = le.fit_transform(x_cat.hsc_b)\nx_cat['hsc_s'] = le.fit_transform(x_cat.hsc_s)\nx_cat['degree_t'] = le.fit_transform(x_cat.degree_t)\nx_cat['specialisation'] = le.fit_transform(x_cat.specialisation)\nx_cat.reset_index(drop=True, inplace=True)\nx_cat","43682ac0":"x = pd.concat([x_cat,x_scaled],join='outer',axis=1)\nx.isnull().sum()\nx","f381c748":"y = le.fit_transform(df.status)","94c530bd":"from sklearn.model_selection import train_test_split as tts\n\nx_train,x_test,y_train,y_test = tts(x,y,test_size=0.3,random_state=42)","9cec6016":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=42)\nlr.fit(x_train,y_train)\ny_pred = lr.predict(x_test)\nlrscore = lr.score(x_test,y_test)\nlrscore","92ea463f":"from sklearn.neighbors import KNeighborsClassifier\nknc = KNeighborsClassifier()\nknc.fit(x_train,y_train)\ny_pred = knc.predict(x_test)\nkncscore = knc.score(x_test,y_test)\nkncscore","b8abddd6":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\ndtr = DecisionTreeClassifier(random_state=42)\ndtr.fit(x_train,y_train)\ny_pred = dtr.predict(x_test)\n\ndtcscore =  metrics.accuracy_score(y_test,y_pred)\nprint(f'Decision Tree Classification Score = {dtcscore:4.1f}%\\n')\nprint(f'Classification Report:\\n {metrics.classification_report(y_test, y_pred)}\\n')","17fc2e99":"from sklearn.ensemble import RandomForestClassifier\n\nrfr = RandomForestClassifier(n_estimators=10,random_state=42)\nrfr.fit(x_train,y_train)\n\nfrom sklearn import metrics\n\npredicted = rfr.predict(x_test)\nrfcscore =  metrics.accuracy_score(y_test, predicted)\nprint(f'Random Forest Classification Score = {rfcscore:4.1f}%\\n')\nprint(f'Classification Report:\\n {metrics.classification_report(y_test, predicted)}\\n')","adf920f2":"from sklearn.linear_model import RidgeClassifier\nrc = RidgeClassifier(random_state=42)\nrc.fit(x_train,y_train)\nl_pred = rc.predict(x_test)\nrcscore = rc.score(x_test,y_test)\nrcscore","d23d2d02":"from sklearn.linear_model import SGDClassifier\nSGDC = SGDClassifier(random_state=42)\nSGDC.fit(x_train,y_train)\nresult = SGDC.predict(x_test)\nsgdcscore = SGDC.score(x_test,y_test)\nsgdcscore","76be83e5":"from sklearn.linear_model import Perceptron\np = Perceptron(random_state=42)\np.fit(x_train,y_train)\nresult = p.predict(x_test)\npscore = p.score(x_test,y_test)\npscore","939c7748":"from sklearn.linear_model import PassiveAggressiveClassifier\npac = PassiveAggressiveClassifier(random_state=42)\npac.fit(x_train,y_train)\nresult_pac = pac.predict(x_test)\npacscore = pac.score(x_test,y_test)\npacscore","d83ff190":"from sklearn.svm import SVC\nsvc = SVC(random_state=42)\nsvc.fit(x_train,y_train)\ny_pred = svc.predict(x_test)\nsvcscore = svc.score(x_test,y_test)\nsvcscore","188e92a3":"from sklearn.ensemble import BaggingClassifier\nbc = BaggingClassifier(random_state=43)\nbc.fit(x_train,y_train)\ny_pred = bc.predict(x_test)\nbcscore = bc.score(x_test,y_test)\nbcscore","dfc46a1c":"d = {'Algorithms Used': ['Logistic Regression','K Neighbors Classifier','Decision Tree Classifier','Random Forest Classifier',\n                         'Ridge Classifier','Stochastic Gradient Descent','Perceptron','Passive Aggressive Classifier',\n                        'Support Vector Classifier','Bagging Classifier'],\n    'Accuracy Achieved': [lrscore,kncscore,dtcscore,rfcscore,rcscore,sgdcscore,pscore,pacscore,svcscore,bcscore]}","04796168":"Accuracy_df = pd.DataFrame(d)\nAccuracy_df = Accuracy_df.sort_values(by=['Accuracy Achieved'],ascending=False)\nAccuracy_df","7efab95b":"from sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.feature_selection import RFE\n\npac = PassiveAggressiveClassifier(random_state=42)\nrfe = RFE(pac,1)\nrfe.fit(x_train,y_train)\nfor var, name in sorted(zip(rfe.ranking_,x), key=lambda x: x[0]):\n    print(f'{name:>18} rank = {var}')","7ca9d183":"from sklearn import metrics\nimport seaborn as sns","1ce851e0":"matrix = metrics.confusion_matrix(y_test,result_pac)\nreport = metrics.classification_report(y_test,result_pac)\nprint(f'Classification Report:\\n {metrics.classification_report(y_test,result_pac)}\\n')","a4b4f596":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom time import time\nfrom sklearn.linear_model import PassiveAggressiveClassifier\n\n# Start clock\nstart = time()\n\npac = PassiveAggressiveClassifier()\nskf = StratifiedKFold(n_splits=10)\n\nfit_intercept = [True]\nvalidation_fraction = [0.1,0.2,0.3,0.4,0.5,0.6]\nloss = ['hinge','squared_hinge']\nrandom_state = [42,33]\nclass_weight = ['weight','balanced',None]\n\n# Create a dictionary of hyperparameters and values\nparams = {'fit_intercept':fit_intercept, 'validation_fraction':validation_fraction,'loss':loss,'random_state':random_state,'class_weight':class_weight}\n\n# Number of random parameter samples\nnum_samples = 20\n\n# Run randomized search\nrscv = RandomizedSearchCV(pac, param_distributions=params, n_iter=num_samples, random_state=23)\n\n# Fit grid search estimator and display results\nrscv.fit(x_train, y_train)\n\nprint(f'Compute time = {time() - start:4.2f} seconds', end='')\nprint(f' for {num_samples} parameter combinations')","6a2c8c1d":"# Get best esimtator\nbe = rscv.best_estimator_\n\n# Display parameter values\nprint(f'Best fit_intercept={be.get_params()[\"fit_intercept\"]:5.4f}')\nprint(f'Best validation_fraction={be.get_params()[\"validation_fraction\"]}')\nprint(f'Best loss={be.get_params()[\"loss\"]}')\nprint(f'Best Class_Weight={be.get_params()[\"class_weight\"]}')\n\n# Display best score\nprint(f'Best CV Score = {rscv.best_score_:4.3f}')","2c91fae9":"#### PASSIVE AGRESSIVE CLASSIFIER","3fe44536":"* This shows that there are only 12 students who scored more than 70 in their studies","ce4014d1":"## DATA PREPROCESSING","50dfa94c":"## FEATURE SELECTION  (MODEL OPTIMIZATION)","fd892c71":"* Looking at no. of placed and not placed","48b490a5":"#### STOCHASTIC GRADIENT DESCENT CLASSIFIER","988b48ef":"# College Placement Dataset ","cd87975a":"#### RIDGE CLASSIFIER","83786632":"## Plotting the range of Salaries Offered","691985e8":"## Understanding the columns in the DataFrame\n* <b>sl_no<\/b>          : The id no. of the student \n* <b>gender<\/b>         : The gender of the student\n* <b>ssc_p<\/b>          : The percentage of marks obtained in SSC (Senior Secondary Certificate)\n* <b>ssc_b<\/b>          : The board in which the student has studied SSC\n* <b>hsc_p<\/b>          : The percentage of marks obtained in HSC (Higher Secondary Certificate)\n* <b>hsc_b<\/b>          : The board in which the student has studied HSC \n* <b>hsc_s<\/b>          : The subject chosen for HSC\n* <b>degree_p<\/b>       : The percentage of marks obtained in Degree\n* <b>degree_t<\/b>       : The subject chosen for Degree\n* <b>workex<\/b>         : Work Experience of the student \n* <b>etest_p<\/b>        : Employability Test Percentage\n* <b>specialisation<\/b> : Specialization chosen in MBA\n* <b>mba_p<\/b>          : Percentage of marks obtained in MBA\n* <b>status<\/b>         : The placement status of the student\n* <b>salary<\/b>         : The salary offered to the students who are placed ","ce3ccd2c":"### Splitting the Data into Training and Testing sets","6db0cd5a":"## Takeaways from the Profile Report \n* There are <b>32.1%<\/b> empty values in Salary i.e 32.1% people have not been placed \n* There are <b>7<\/b> Numerical Variables \n* There are <b>7<\/b> Categorical Variables\n* There is <b>1<\/b> Boolean Variable","7766ed65":"#### SUPPORT VECTOR CLASSIFIER","5c255b5e":"## Plot for MBA_P vs Salary","821af30a":"* The above tests prove that the data is not normal therefore we can scale the Data\n* This can be done using StandardScaler from scikit-learn","3708ac12":"* The major evaluation metrics used for a classification problem are \n* <b>Accuracy Score\n* Classification Report\n* Confusion Matrix<\/b>","83118599":"* Checking the types of specializations offered and students enrolled","af2d1eae":"## Plot for SSC_P vs Salary ","e7d50171":"#### RANDOM FOREST CLASSIFIER","113cc0ec":"* Checking the no. of students with and without work ex","3bc42b44":"* Plotting the salaries of those who scored more than 60% in all their studies","6bb4e012":"* This dataset is related to the placement statistics of an MBA college. \n* It is available on kaggle : https:\/\/www.kaggle.com\/benroshan\/factors-affecting-campus-placement\n* Thanks Ben Roshan D for providing the Dataset\n* Here we use machine learning to predict the placement chances of placement and the salary offered if placed ","60f9c9f0":"<b>Visual measures to be implemented:<\/b>\n* Box Plots\n* QQ Plots","07dd6b8e":"* We will continue with the Passive Aggressive Classifier for Future processes\n* In feature selection we will understand which variable affects the result the most ","49d6e3cd":"* Checking the types of degree done in undergraduation","e996a786":"## EXPLORATORY DATA ANALYSIS","1129adc7":"#### DECISION TREE CLASSIFIER","ff0c9738":"#### KNEIGHBORS CLASSIFIER","1b1d5c1e":"* Checking the gender difference in the batch","c22beb2c":"#### LOGISTIC REGRESSION","63fa963f":"## Plot for HSC_P vs Salary","177d7d29":"#### BAGGING CLASSIFIER","7bebf134":"## MODEL SELECTION (HYPERPARAMETER TUNING)","ff006fca":"## Using Matplotlib to Visualize the Data ","80020c3b":"## EVALUATION METRICS","0a89c788":"### Wrapper Methods - RECURSIVE FEATURE ELIMINATION","bf387150":"<b>Why is Normality Required:<\/b>\n* It is a (a bit strongly stated) fact that formal normality tests always reject on the huge sample sizes we work with today. It\u2019s even easy to prove that when n gets large, even the smallest deviation from perfect normality will lead to a significant result. And as every dataset has some degree of randomness, no single dataset will be a perfectly normally distributed sample. But in applied statistics the question is not whether the data\/residuals \u2026 are perfectly normal, but normal enough for the assumptions to hold.\n* As we can see from the code below, the Shapiro-Wilk test has <b>rejected normality for MBA Percentage<\/b>. Therefore, we might have to use some additional measure to see if the null hypothesis for MBA Percentage should indeed be rejected.","b32398d7":"### Label Encoding for all the Categorical Variables","57da8c00":"* We have achieved the highest accuracy using <b>PASSIVE AGGRESSIVE CLASSIFIER<\/b>","a2e2e72e":"#### PERCEPTRON","c05bdaa0":"## Applying Classification Algorithms for prediction ","190afc6c":"<b>Tests to check Normality:<\/b>\n* The Shapiro-Wilk test\n* The Anderson-Darling test\n* The Kolmogorov-Smirnov test","47ab371f":"# What are we predicting :\n* We are trying to predict the chance of a person getting a placement \n* Therefore we have to make the training and testing sets accordingly ","375cf308":"## Plot for Degree_P vs Salary","1ed641cb":"* The outliers in the dataset have to be removed so that the algorithm can work equally well on new data\n* Therefore we can remove the data where salary is greater than 5,00,000","06da83d3":"## Checking Wether the numerical data is Normally Distributed","505f379a":"## We can get started by importing the Dataset","26c27f2f":"* Checking the background of student in their +1 and +2"}}