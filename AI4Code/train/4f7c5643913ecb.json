{"cell_type":{"539a9247":"code","492db024":"code","788a93f2":"code","ef9d84c3":"code","4feee5b5":"code","23643257":"code","71001c87":"code","fd62bc79":"code","54539d21":"code","d8a67a46":"code","7f97b5c7":"code","4e7a96af":"code","8311ef1c":"code","2f97b518":"code","70068be2":"code","960a15e7":"code","19fae599":"code","51e319e7":"code","09aabde2":"code","97356cfa":"code","46ac40ce":"code","5c288175":"code","3b7ae205":"code","75060c6d":"code","0314e55f":"code","c1c4eeb9":"code","96e1aa86":"code","5dbb6b4d":"code","a9680ffd":"code","991292bf":"code","733df8ba":"code","b5f25f6c":"code","2a0c8f5c":"code","fefe9ab5":"code","43659866":"code","6737b8ce":"code","c776a181":"code","9b99d8c2":"code","8a137379":"code","9c511c0c":"code","4ce6283b":"code","c5a1abae":"markdown","f3d7b846":"markdown","eb6fe22f":"markdown","68845717":"markdown","d926768d":"markdown","e6a15d2c":"markdown","8e0f10dc":"markdown","ac695e7a":"markdown","712079fa":"markdown","ebecc5e3":"markdown","9c6db63d":"markdown","9aadde38":"markdown","49ae590b":"markdown","ee634c9c":"markdown","0f286b93":"markdown","668c8c7b":"markdown","0660fb5f":"markdown","ab7c1023":"markdown","0a5482f4":"markdown","3706620d":"markdown","e2e4c374":"markdown","a8e749ae":"markdown","16f6d5c0":"markdown","503e4eb9":"markdown","572862da":"markdown","2d5ca028":"markdown","45e0c6cf":"markdown","2284d4ed":"markdown","f0944f52":"markdown","a5b5e944":"markdown","57035641":"markdown","8cc23e2a":"markdown","8f5cf419":"markdown","1addef34":"markdown","58685e54":"markdown","2dc00d67":"markdown","0338ac14":"markdown","5f856dce":"markdown","b2c90a23":"markdown","d459e837":"markdown","eb148f40":"markdown","a39096fb":"markdown","555eb767":"markdown","b3ee53b0":"markdown","7f181dcc":"markdown","30128544":"markdown","04a06dfe":"markdown","33a412d4":"markdown","f9e4ff1b":"markdown","5c069f43":"markdown","65b9ac09":"markdown","b5c0cf79":"markdown","2a9308b1":"markdown","099460a3":"markdown","1703c13d":"markdown"},"source":{"539a9247":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\n\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss,confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","492db024":"df= pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')\ndf.head()","788a93f2":"df.shape","ef9d84c3":"df.info()","4feee5b5":"df['FastingBS']=df['FastingBS'].astype('category')","23643257":"df['HeartDisease']=df['HeartDisease'].astype('category')","71001c87":"Duplicate = df[df.duplicated(keep='first')]\n  \nprint(\"Duplicate Entries :\")\n  \nDuplicate.shape","fd62bc79":"num_cols=df.select_dtypes(include=['int64','float64']).columns.tolist()\n\ncat_cols=df.select_dtypes(include=['category','object']).columns.tolist()","54539d21":"print(num_cols)\nprint(cat_cols)","d8a67a46":"for i in cat_cols:\n     print(\"{} : {} \".format(i,df[i].unique()))","7f97b5c7":"df.describe().T","4e7a96af":"for i in num_cols:\n    print('{} \\n\\n {}'.format('\\033[1m',i.upper()))\n    f, axes = plt.subplots(1, 2, figsize=(15,5))\n    sns.boxplot(x = i, data=df,  orient='h' , ax=axes[1])\n    sns.distplot(df[i],  ax=axes[0])\n    axes[0].set_title('Distribution plot')\n    axes[1].set_title('Box plot')\n    plt.show()\n    \n    \n    q25,q75=np.percentile(df[i],25),np.percentile(df[i],75)\n    IQR=q75-q25\n    Threshold=IQR*1.5\n    lower,upper=q25-Threshold,q75+Threshold\n    Outliers=[i for i in df[i] if i<lower or i>upper]\n    print('Total number of outliers in {}: {}'.format(i,len(Outliers)))\n    percent=round((len(Outliers)\/len(df[i]))*100,2)\n    print('Percentage of outliers in {}: {} %'.format(i,percent))","8311ef1c":"for i in cat_cols:\n    plt.figure(figsize=(10,5))\n    plot=sns.countplot(df[i])\n    \n    total = len(df[i])\n    for p in plot.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n        x = p.get_x() + p.get_width() \/ 2 - 0.05\n        y = p.get_y() + p.get_height()\n        plot.annotate(percentage, (x, y), size = 12)\n    \n    plt.show() ","2f97b518":"for i in num_cols:\n    f,axes=plt.subplots(1,2,figsize=(15,6))\n    sns.boxplot(x='HeartDisease', y=i, data= df, ax=axes[0])\n    ax=df.groupby(by=['HeartDisease'])[i].mean().reset_index().sort_values(i,ascending=True).plot(x='HeartDisease',y=i,kind='bar',ax=axes[1])\n    \n    for p in ax.patches:\n        ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))","70068be2":"for i in cat_cols:\n    if i!='HeartDisease':\n        plt.figure(figsize=(10,5))\n        sns.countplot(df[i],hue=df['HeartDisease'])\n        plt.show()","960a15e7":"sns.pairplot(df);","19fae599":"corr = df.corr()\nf, ax = plt.subplots(figsize=(12, 10))\nmask = np.triu(np.ones_like(corr, dtype=bool))[1:, :-1]\ncorr = corr.iloc[1:,:-1].copy()\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)\nplt.yticks(rotation=0)\nplt.show()","51e319e7":"for i in num_cols:\n    x = np.array(df[df.HeartDisease==0][i])\n    y = np.array(df[df.HeartDisease==1][i])\n    \n    t, p_value  = stats.ttest_ind(x,y, axis = 0,equal_var=False) \n    \n    print('{} P_Value:{}'.format('\\033[1m',p_value))\n    if p_value < 0.05:  # Setting our significance level at 5%\n        print('{} Rejecting Null Hypothesis. \\n There is significant difference in {} Feature for different category of target variable'.format('\\033[1m',i))\n    else:\n        print('{} Fail to Reject Null Hypothesis.\\n There is no significant difference in {} Feature for different category of target variable'.format('\\033[1m',i))\n    print('\\n')","09aabde2":"for i in cat_cols:\n    crosstab=pd.crosstab(df[\"HeartDisease\"],df[i])\n    \n    chi,p_value,dof,expeced = stats.chi2_contingency(crosstab)\n    \n    if p_value < 0.05:  # Setting our significance level at 5%\n        print('{} Rejecting Null Hypothesis. \\n There is significant difference in {} Feature for different category of target variable'.format('\\033[1m',i))\n    else:\n        print('{} Fail to Reject Null Hypothesis.\\n There is no significant difference in {} Feature for different category of target variable'.format('\\033[1m',i))\n    print(crosstab)    \n    print('\\n')","97356cfa":"for c in num_cols:\n    #getting upper lower quartile values\n    q25,q75=np.percentile(df[c],25),np.percentile(df[c],75)\n    IQR=q75-q25\n    Threshold=IQR*1.5\n    lower,upper=q25-Threshold,q75+Threshold\n    \n    df[c]=np.where(df[c]>upper,np.nan,df[c])\n    df[c]=np.where(df[c]<lower,np.nan,df[c])","46ac40ce":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","5c288175":"imp=IterativeImputer(missing_values=np.nan)\nidf=pd.DataFrame(imp.fit_transform(df[num_cols]))\nidf.columns=df[num_cols].columns\nidf.index=df.index","3b7ae205":"df[['RestingBP','Cholesterol','MaxHR','Oldpeak']]=idf[['RestingBP','Cholesterol','MaxHR','Oldpeak']]","75060c6d":"f,axes=plt.subplots(1,2,figsize=(17,7))\ndf['HeartDisease'].value_counts().plot.pie(autopct='%1.1f%%',ax=axes[0],shadow=True)\nsns.countplot('HeartDisease',data=df,ax=axes[1],order=[0,1])\naxes[0].set_title('HeartDisease Variable Pie Chart')\naxes[1].set_title('HeartDisease Variable Bar Graph')\nplt.show()","0314e55f":"X=df.drop(columns=\"HeartDisease\")\ny=df[\"HeartDisease\"]","c1c4eeb9":"X=pd.get_dummies(X,drop_first=True)","96e1aa86":"scaler = StandardScaler()\ntemp=X[num_cols] #Temporary dataframe\ntemp[num_cols]= scaler.fit_transform(temp)","5dbb6b4d":"X[num_cols]=temp","a9680ffd":"X[['Age','RestingBP','Cholesterol','MaxHR','Oldpeak']]=temp","991292bf":"X.head()","733df8ba":"new_df=X.join(y,how='inner')\nnew_df.to_csv(\"clean_dataset\")","b5f25f6c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10,stratify = y)","2a0c8f5c":"def fit_models(model, X_train, X_test, y_train, y_test):\n    \n#     start = time.time()\n    \n    model.fit(X_train, y_train)\n    \n#     train_score = mod.score(X_train, y_train)\n#     test_score = mod.score(X_test, y_test)\n    \n    pred = model.predict(X_test)\n    \n    accuracy = metrics.accuracy_score(y_test,pred)\n    \n    precision = metrics.precision_score(y_test,pred)\n    \n    recall = metrics.recall_score(y_test,pred)\n    \n    f1 = metrics.f1_score(y_test,pred)\n    \n    loss = metrics.log_loss(y_test,pred)\n    \n    scorer = {'accuracy' : metrics.make_scorer(metrics.accuracy_score),\n              'precision' : metrics.make_scorer(metrics.precision_score),\n              'recall' : metrics.make_scorer(metrics.recall_score),\n              'f1' : metrics.make_scorer(metrics.f1_score),\n              'loss' : metrics.make_scorer(metrics.log_loss)\n             }\n    cv = cross_validate(model, X_train, y_train, cv=10, scoring = scorer)\n    \n    accuracy_cv = cv['test_accuracy'].mean()\n    precision_cv = cv['test_precision'].mean()\n    recall_cv = cv['test_recall'].mean()\n    f1_cv = cv['test_f1'].mean()\n    loss_cv = cv['test_loss'].mean()\n    \n    return accuracy, precision, recall, f1, loss, accuracy_cv, precision_cv,recall_cv,f1_cv,loss_cv","fefe9ab5":"lr = LogisticRegression()\ngnb = GaussianNB()\nknn = KNeighborsClassifier(n_neighbors=5,metric='euclidean')\nsvm = SVC(kernel='linear', C=1,gamma='auto')\ndT_gini = DecisionTreeClassifier(criterion = 'gini', random_state=1)\ndT_entropy = DecisionTreeClassifier(criterion = 'entropy', random_state=1)\nbgcl = BaggingClassifier(n_estimators=10,random_state=1)\nabcl = AdaBoostClassifier(n_estimators=10, random_state=1)\ngbcl = GradientBoostingClassifier(n_estimators = 50,random_state=1)\nrfcl = RandomForestClassifier(n_estimators = 50, random_state=1,max_features=12)\n\n\nresult = {}\n\nfor model,name in zip([lr, gnb, knn, svm, dT_gini, dT_entropy, bgcl, abcl, gbcl, rfcl],\n                     ['Logistic Regression', 'Gaussian NB', 'KNN', 'SVC', 'Decision Tree(gini)',\n                     'Decision Tree(entropy)', 'Bagging Classifier', 'Adaptive Boosting', \n                      'Gradient Boosting', 'Random Forest Classifier']):\n    result[name] = fit_models(model,X_train, X_test,y_train, y_test)","43659866":"result1 = pd.DataFrame(np.array(list(result.values())),    # make a dataframe out of the metrics from result dictionary \n                       columns= ['ACCURACY', 'PRECISION', 'RECALL', 'F1-SCORE', 'LOG LOSS', \n                                 'ACCURACY CV', 'PRECISION CV', 'RECALL CV', 'F1-SCORE CV', 'LOG LOSS CV'],\n                      index= result.keys())   # use the model names as index\n\nresult1.index.name = 'Model'   # name the index of the result1 dataframe as 'Model'\n\nresult1 ","6737b8ce":"Log_clf = GridSearchCV(LogisticRegression(),{\n    'C' : [100, 10, 1.0, 0.1, 0.01],\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n    'penalty' : ['l1','l2','elasticnet']\n}, cv=10, return_train_score=False)\n\nLog_clf_fit=Log_clf.fit(X,y)\nprint(Log_clf_fit.best_score_)\nprint(Log_clf_fit.best_params_)","c776a181":"logit = LogisticRegression(C = 1.0, penalty='l2', solver = 'liblinear')\nlogit.fit(X_train, y_train)\nprint('Accuracy on Training data:',logit.score(X_train, y_train) )\nprint('Accuracy on Test data:',logit.score(X_test, y_test) )\n\nlogit_pred = logit.predict(X_test)\nprint(\"\\n Classification  Report:\\n\",classification_report(y_test,logit_pred))\nprint(\"f1_score of Logistic Regression: \",metrics.f1_score(y_test,logit_pred))","9b99d8c2":"svc_clf = GridSearchCV(SVC(),{\n    'C' : [100,10,1,0.1,0.01,0.001],\n    'gamma' : [100,10,0.1,0.01,0.001],\n    'kernel' : ['rbf','linear','poly']\n}, cv=10, return_train_score=False)\n\nsvc_clf_fit=svc_clf.fit(X,y)\nprint(svc_clf_fit.best_score_)\nprint(svc_clf_fit.best_params_)","8a137379":"svc = SVC(kernel = 'rbf', C = 1, gamma = 0.1)\nsvc.fit(X_train,y_train)\nprint('Accuracy on Training data: ',svc.score(X_train,y_train))\nprint('Accuracy on Test data:',svc.score(X_test,y_test))\n\npredicted=svc.predict(X_test)\nprint(\"\\n Classification  Matrix:\\n\",classification_report(y_test,predicted))\n\nprint(\"f1_score of SVC: \",metrics.f1_score(y_test,predicted))","9c511c0c":"gdb_clf = GridSearchCV(GradientBoostingClassifier(),{\n    'n_estimators' : [10, 100, 1000],\n    'learning_rate' : [0.001, 0.01, 0.1],\n    'subsample' : [0.5, 0.7, 1.0],\n    'max_depth' : [3,7,9]\n}, cv=10, return_train_score=False)\n\ngdb_clf_fit=gdb_clf.fit(X,y)\nprint(gdb_clf_fit.best_score_)\nprint(gdb_clf_fit.best_params_)","4ce6283b":"gbcl = GradientBoostingClassifier(n_estimators = 1000, max_depth = 3, learning_rate = 0.01, subsample = 0.7, random_state = 1)\ngbcl = gbcl.fit(X_train, y_train)\nprint(\"Ensemble learning(GradientBoosting) Training data score: \",gbcl.score(X_train,y_train))\nprint(\"Ensemble learning(GradientBoosting) Testing data score: \",gbcl.score(X_test,y_test))\n\ny_predict = gbcl.predict(X_test)\nprint(\"classification  Matrix:\\n\",classification_report(y_test,y_predict))\n\nprint(\"f1_score of Gradient boost: \",metrics.f1_score(y_test,y_predict))","c5a1abae":"**Segregating categorical features and numerical features**","f3d7b846":"### Univariate analysis","eb6fe22f":"***Univariate analysis on numerical features***","68845717":"***Insights***\n\n    - People with the average age of about 56 have high risk of being affected by heart disease\n    \n    - RestingBP does not provide any clear conclusion on heartdisease as their averages are almost similar\n    \n    - People having high cholestrol levels have high risk of being affected by heart disease\n    \n    - People with heart diseases can reach the maximum heart rate of 127(on average) which is lesser than the maximum heart rate of people with no heart disease, therefore we can say that the people with low heart rate have high risk of being    affected by heart disease\n    \n    - oldpeak = ST [Numeric value measured in depression] - People having large oldpeak(large oldpeak = depression)  \n    have high risk of being affected by heart disease","d926768d":"The dataset is almost balanced","e6a15d2c":"**Scaling the continuous features**","8e0f10dc":"**Changing the FastingBS from int64 to object\/Categorical**","ac695e7a":"### Hyperparameter tuning-Logistic Regression","712079fa":"## Conclusion","ebecc5e3":"***Statistical testing of categorical feature with target feature***","9c6db63d":"### Train-Test split","9aadde38":"## Feature Selection","49ae590b":"SVC performs well when compared to the other two models(we are considering accuracy and f1_score as our metrics for choosing the best model), So let us choose SVC as our final model with an accuracy of about 89% and f1_score of 90%","ee634c9c":"***Insights***\n\n-**Sex** - Distribution of male is more when compared to female distribution\n\n-**ChestPainType** - Majority of the people have chest pain of the type \"ASY\"\n\n-**FastingBS** - About 76.7% of people have fastingBS<120 mg\/dl\n\n-**RestingECG** - Majority of them have restingECG as normal, while about 40% have abnormal restingECG\n\n-**ExerciseAngina** - 40% of the people have exercise-induced angina\n\n-**ST_Slope** - Half of the people have flat slope while 43% have Up slope and the remaining have down slope\n\n-**HeartDisease** - 53% of the the people are found to have heart disease","0f286b93":"**Checking for Duplicate entries in the dataset**","668c8c7b":"Applying the scaled data to the dataframe X","0660fb5f":"***Insights***\n\n    mean and median of \"Age\" is almost equal - Normally distributed\n    \n    mean and median of \"RestingBP\" is similar - Normally distributed\n    \n    mean of \"Cholestrol\" is lesser than its median - Left skewed\n    \n    mean and median of \"MaxHR\" is almost equal - Normally distributed\n    \n    mean of \"Oldpeak\" is greater than its median - Right skewed\n","ab7c1023":"### Outlier analysis","0a5482f4":"***Statistical testing of continuous feature with target feature***","3706620d":"## Importing the dataset","e2e4c374":"**Checking for Values in each categorical feature and looking for any special\/improper characters**","a8e749ae":"### Checking for Target Imbalance","16f6d5c0":"***Multivariate analysis***","503e4eb9":"Saving the cleaned dataset as CSV file for future use","572862da":"***Univariate analysis on categorical features***","2d5ca028":"***Insights***\n\nCholestrol and RestingBP with value 0 should be handled before building the model","45e0c6cf":"***Correlation heatmap***","2284d4ed":"There are no NULL values in the dataset.\n\n'FastingBS' and 'HeartDisease' have only two values, so let us change them to categorial datatype for better analysis . Otherthan that all the other features seems to have their corresponding datatypes. \n\nNumerical features are correctly defined with their respective datatypes, So there are no possiblities for improper\/special characters in Numerical features.   (But we have to check for improper\/special characters in Categorical features- which we will be doing in upcoming codes)","f0944f52":"***Categorical variable vs Target variable***","a5b5e944":"### Hyperparameter tuning - Gradient Boosting","57035641":"All the features required for our model,therefore no feature is removed from the dataset","8cc23e2a":"There are no special\/improper characters present in categorical features","8f5cf419":"There are no duplicate entries in the dataset","1addef34":"***Insights***\n\n    There are some outliers in the dataset, we need to handle them before building the model.\n    \n    Also we can see that cholestrol level of some people is zero, which should be handled before building the model because cholestrol levels can never be zero this maybe because of some human errors.","58685e54":"## Model training,testing and tuning","2dc00d67":"### Hyperparameter tuning-SVC","0338ac14":"***Numerical variables vs Target variable***","5f856dce":"### Segregating Predictors vs Target features","b2c90a23":"## Data analysis and visualization","d459e837":"***Gradient Boosting with the best chosen parameters***","eb148f40":"Replacing outliers with NaN- Replacing outtliers by treating them as missing values","a39096fb":"**One hot encoding**","555eb767":"***Insights***\n\n-**Sex** - Male have high risk of being affected by heart disease when compared to female \n\n-**ChestPainType** - ChestPainType-ASY indicates that the patient is highly prone to heart disease\n\n-**FastingBS** - fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise] - People with FastingBS 1 have chances of being affected by heart disease\n\n-**RestingECG** - RestingECG does not provide any clear conclusion on heartdisease\n\n-**ExerciseAngina** - People with exercise-induced angina have chances of being affected by heart disease\n\n-**ST_Solpe** - People with flat ST_Slope have heart disease","b3ee53b0":"## Data Cleansing","7f181dcc":"### Information about data","30128544":"Since there are no Ordinal categorical features we can go for One-Hot encoding","04a06dfe":"**Changing the Target feature(HeartDisease) from int64 to object\/Categorical**","33a412d4":"### Bivariate Analysis","f9e4ff1b":"***SVC with the best chosen parameters***","5c069f43":"## Data preprocessing","65b9ac09":"Let us use hypothesis testing for feature selection process","b5c0cf79":"### 5 point summary","2a9308b1":"From the above dataframe we can notice that Logistic Regression, SVC, Gradient Boosting have higher F1-SCORE, We shall select any one of them to build the final model.","099460a3":"***Logistic Regression with the best chosen parameters***","1703c13d":"## Importing necessary packages"}}