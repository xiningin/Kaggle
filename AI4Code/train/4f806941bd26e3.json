{"cell_type":{"f14acff4":"code","d76ba874":"code","1a394226":"code","19116af6":"code","4c761dc7":"code","68f797c4":"code","bcf41f1c":"code","b5237563":"code","13a2d554":"code","971c0e3d":"code","dcd98420":"code","1e44995e":"code","c15a414f":"code","81e1cf4b":"code","59e46f80":"code","2b2a2273":"code","6e608298":"code","f02a792f":"code","30724060":"code","a41a1c3a":"code","7077e1b3":"code","82a8cda9":"code","ff496440":"markdown","3a4bde8e":"markdown","06b03ff7":"markdown","528fae67":"markdown","b46e07e9":"markdown","af181561":"markdown","47e1a565":"markdown","9256109b":"markdown","81ca4811":"markdown","763f441d":"markdown","ac96cd9e":"markdown","6388c7c8":"markdown","a71e1fcd":"markdown","904887d2":"markdown","90e11e31":"markdown","ab4c329f":"markdown","9d555daf":"markdown","f9a15f76":"markdown","a846b048":"markdown","b9fbfa3a":"markdown","aa43054c":"markdown","b9532d11":"markdown","471de6c0":"markdown","0a87f66b":"markdown","a6c8d4ec":"markdown","90bf59e7":"markdown","63f4e898":"markdown","e41b966e":"markdown","5978d2b3":"markdown","4d64b660":"markdown","091a22d3":"markdown","d38eb653":"markdown","5662e9c3":"markdown","95d77673":"markdown"},"source":{"f14acff4":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nimport cv2\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\nimport json\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport shap\nfrom operator import itemgetter","d76ba874":"my_data_dir = '\/kaggle\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data\/'\ntrain_path = my_data_dir + 'train\/'\ntest_path = my_data_dir + 'test\/'","1a394226":"plt.figure(figsize=(10,8))\nok = plt.imread(train_path + 'ok_front\/cast_ok_0_1.jpeg')\nplt.subplot(1, 2, 1)\nplt.axis('off')\nplt.title(\"ok\", weight='bold', size=20)\nplt.imshow(ok,cmap='gray')\n\nng = plt.imread(train_path + 'def_front\/cast_def_0_1001.jpeg')\nplt.subplot(1, 2, 2)\nplt.axis('off')\nplt.title(\"def\", weight='bold', size=20)\nplt.imshow(ng,cmap='gray')\n\nplt.show()","19116af6":"img = cv2.imread(train_path + 'ok_front\/cast_ok_0_1.jpeg')\nimg_4d = img[np.newaxis]\nplt.figure(figsize=(25,10))\ngenerators = {\"rotation\":ImageDataGenerator(rotation_range=180), \n              \"zoom\":ImageDataGenerator(zoom_range=0.7), \n              \"brightness\":ImageDataGenerator(brightness_range=[0.2,1.0]), \n              \"height_shift\":ImageDataGenerator(height_shift_range=0.7), \n              \"width_shift\":ImageDataGenerator(width_shift_range=0.7)}\n\nplt.subplot(1, 6, 1)\nplt.title(\"Original\", weight='bold', size=15)\nplt.imshow(img)\nplt.axis('off')\ncnt = 2\nfor param, generator in generators.items():\n    image_gen = generator\n    gen = image_gen.flow(img_4d, batch_size=1)\n    batches = next(gen)\n    g_img = batches[0].astype(np.uint8)\n    plt.subplot(1, 6, cnt)\n    plt.title(param, weight='bold', size=15)\n    plt.imshow(g_img)\n    plt.axis('off')\n    cnt += 1\nplt.show()","4c761dc7":"image_gen = ImageDataGenerator(rescale=1\/255, \n                               zoom_range=0.1, \n                               brightness_range=[0.9,1.0])","68f797c4":"image_shape = (300,300,1) # 300 \u00d7 300\u3001graysclaed (full-color : 3)\nbatch_size = 32\n\ntrain_set = image_gen.flow_from_directory(train_path,\n                                            target_size=image_shape[:2],\n                                            color_mode=\"grayscale\",\n                                            classes={'def_front': 0, 'ok_front': 1},\n                                            batch_size=batch_size,\n                                            class_mode='binary',\n                                            shuffle=True,\n                                            seed=0)\n\ntest_set = image_gen.flow_from_directory(test_path,\n                                           target_size=image_shape[:2],\n                                           color_mode=\"grayscale\",\n                                           classes={'def_front': 0, 'ok_front': 1},\n                                           batch_size=batch_size,\n                                           class_mode='binary',\n                                           shuffle=False,\n                                           seed=0)","bcf41f1c":"train_set.class_indices","b5237563":"backend.clear_session()\nmodel = Sequential()\nmodel.add(Conv2D(filters=16, kernel_size=(7,7), strides=2, input_shape=image_shape, activation='relu', padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=2))\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), strides=1, input_shape=image_shape, activation='relu', padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=2))\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, input_shape=image_shape, activation='relu', padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=2))\nmodel.add(Flatten())\nmodel.add(Dense(units=224, activation='relu'))\nmodel.add(Dropout(rate=0.2))\nmodel.add(Dense(units=1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.summary()","13a2d554":"plot_model(model, show_shapes=True, expand_nested=True, dpi=60)","971c0e3d":"model_save_path = 'casting_product_detection.hdf5'\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\ncheckpoint = ModelCheckpoint(filepath=model_save_path, verbose=1, save_best_only=True, monitor='val_loss')","dcd98420":"n_epochs = 20\nresults = model.fit_generator(train_set, epochs=n_epochs, validation_data=test_set, callbacks=[early_stop,checkpoint])","1e44995e":"model_history = results.history\njson.dump(model_history, open('model_history.json', 'w'))","c15a414f":"#model = load_model(\"\/kaggle\/input\/casting-product-detectionhdf5\/casting_product_detection.hdf5\")\n#model_history = json.load(open('\/kaggle\/input\/model-historyjson\/model_history.json', 'r'))","81e1cf4b":"losses = pd.DataFrame(model_history)\nlosses.index = map(lambda x : x+1, losses.index)\nlosses.head(3)","59e46f80":"g = hv.Curve(losses.loss, label='Training Loss') * hv.Curve(losses.val_loss, label='Validation Loss') \\\n    * hv.Curve(losses.accuracy, label='Training Accuracy') * hv.Curve(losses.val_accuracy, label='Validation Accuracy')\ng.opts(opts.Curve(xlabel=\"Epochs\", ylabel=\"Loss \/ Accuracy\", width=700, height=400,tools=['hover'],show_grid=True,title='Model Evaluation')).opts(legend_position='bottom')","2b2a2273":"pred_probability = model.predict_generator(test_set)\npredictions = pred_probability > 0.5\n\nplt.figure(figsize=(10,6))\nplt.title(\"Confusion Matrix\", size=20, weight='bold')\nsns.heatmap(\n    confusion_matrix(test_set.classes, predictions),\n    annot=True,\n    annot_kws={'size':14, 'weight':'bold'},\n    fmt='d',\n    xticklabels=['Defect', 'OK'],\n    yticklabels=['Defect', 'OK'])\nplt.tick_params(axis='both', labelsize=14)\nplt.ylabel('Actual', size=14, weight='bold')\nplt.xlabel('Predicted', size=14, weight='bold')\nplt.show()","6e608298":"print(classification_report(test_set.classes, predictions, digits=3))","f02a792f":"test_cases = ['ok_front\/cast_ok_0_10.jpeg', 'ok_front\/cast_ok_0_1026.jpeg', 'ok_front\/cast_ok_0_1031.jpeg', 'ok_front\/cast_ok_0_1121.jpeg', \\\n              'ok_front\/cast_ok_0_1144.jpeg','def_front\/cast_def_0_1059.jpeg', 'def_front\/cast_def_0_108.jpeg', 'def_front\/cast_def_0_1153.jpeg',\\\n              'def_front\/cast_def_0_1238.jpeg', 'def_front\/cast_def_0_1269.jpeg']\n\nplt.figure(figsize=(20,8))\nfor i in range(len(test_cases)):\n    img_pred = cv2.imread(test_path + test_cases[i], cv2.IMREAD_GRAYSCALE)\n    img_pred = img_pred \/ 255 # rescale\n    prediction = model.predict(img_pred.reshape(1, *image_shape))\n    \n    img = cv2.imread(test_path + test_cases[i])\n    label = test_cases[i].split(\"_\")[0]\n    \n    plt.subplot(2, 5, i+1)\n    plt.title(f\"{test_cases[i].split('\/')[1]}\\n Actual Label : {label}\", weight='bold', size=12)\n    if (prediction < 0.5):\n        predicted_label = \"def\"\n        prob = (1-prediction.sum()) * 100\n    else:\n        predicted_label = \"ok\"\n        prob = prediction.sum() * 100\n        \n    cv2.putText(img=img, text=f\"Predicted Label : {predicted_label}\", org=(10, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, color=(255, 0, 255), thickness=2)\n    cv2.putText(img=img, text=f\"Probability : {'{:.3f}'.format(prob)}%\", org=(10, 280), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=(0, 255, 0), thickness=2)\n    plt.imshow(img,cmap='gray')\n    plt.axis('off')\n\nplt.show()","30724060":"test_cases = ['ok_front\/'+i for i in os.listdir('\/kaggle\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data\/test\/ok_front')]\ntest_cases.extend(['def_front\/'+i for i in os.listdir('\/kaggle\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data\/test\/def_front')])\n\nplt.figure(figsize=(20,8))\ncnt = 0 \nmisclassified = []\nfor i in range(len(test_cases)):\n    img_pred = cv2.imread(test_path + test_cases[i], cv2.IMREAD_GRAYSCALE)\n    img_pred = img_pred \/ 255 # rescale\n    prediction = model.predict(img_pred.reshape(1, *image_shape))\n    \n    img = cv2.imread(test_path + test_cases[i])\n    label = test_cases[i].split(\"_\")[0]\n    \n    if (prediction < 0.5):\n        predicted_label = \"def\"\n        prob = (1-prediction.sum()) * 100\n    else:\n        predicted_label = \"ok\"\n        prob = prediction.sum() * 100\n    \n    if label != predicted_label:\n        misclassified.append(test_cases[i])\n        plt.subplot(2, 5, cnt+1)\n        plt.title(f\"{test_cases[i].split('\/')[1]}\\n Actual Label : {label}\", weight='bold', size=12)\n        cv2.putText(img=img, text=f\"Predicted Label : {predicted_label}\", org=(10, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, color=(255, 0, 255), thickness=2)\n        cv2.putText(img=img, text=f\"Probability : {'{:.3f}'.format(prob)}%\", org=(10, 280), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=(0, 255, 0), thickness=2)\n        plt.imshow(img,cmap='gray')\n        plt.axis('off')\n        cnt += 1\n    else:\n        continue\n\nplt.show()","a41a1c3a":"train_cases = ['ok_front\/'+i for i in os.listdir('\/kaggle\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data\/train\/ok_front')]\ntrain_cases.extend(['def_front\/'+i for i in os.listdir('\/kaggle\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data\/train\/def_front')])\ntrain_sample = [cv2.imread(train_path + i, cv2.IMREAD_GRAYSCALE).reshape(1, *image_shape) \/ 255 for i in np.random.choice(train_cases, 3000, replace=False)]\nexplainer = shap.DeepExplainer(model, train_sample[0])","7077e1b3":"test_cases = ['def_front\/cast_def_0_1059.jpeg', 'def_front\/cast_def_0_108.jpeg', 'def_front\/cast_def_0_1153.jpeg',\\\n              'def_front\/cast_def_0_1238.jpeg', 'def_front\/cast_def_0_1269.jpeg']\n\nfor i in test_cases:\n    img = cv2.imread(test_path + i, cv2.IMREAD_GRAYSCALE).reshape(1, *image_shape) \/ 255\n    shap_values = explainer.shap_values(img)\n    shap.image_plot(shap_values, img, show=False) \n    plt.title(f\"{i.split('\/')[1]}\\n Actual Label : {i.split('_')[0]}\", weight='bold', size=12)\n    plt.axis('off')\nplt.show()","82a8cda9":"for i in misclassified:\n    img = cv2.imread(test_path + i, cv2.IMREAD_GRAYSCALE).reshape(1, *image_shape) \/ 255\n    shap_values = explainer.shap_values(img)\n    shap.image_plot(shap_values, img, show=False) \n    plt.title(f\"{i.split('\/')[1]}\\n Actual Label : {i.split('_')[0]}\", weight='bold', size=12)\n    plt.axis('off')\nplt.show()","ff496440":"## Model Performance","3a4bde8e":"## Explain Why Misclassified\nConsider why misclassified images were misclassified using SHAP as above. It is considered that <u>the shadows, scratches, and holes in the images are overlooked<\/u> and judged to be <b>normal<\/b>.","06b03ff7":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","528fae67":"# 7. References\n> * **Keras Official Manual**  \n> https:\/\/keras.io\/ja\/  \n> * **Image Data Pre-Processing Techniques**  \n> https:\/\/pynote.hatenablog.com\/entry\/keras-image-data-generator  \n> * **Keras ImageDataGenerator**  \n> https:\/\/keras.io\/ja\/preprocessing\/image\/\n> * **OpenCV puttext**  \n> http:\/\/opencv.jp\/opencv-2svn\/cpp\/drawing_functions.html#cv-puttext  \n> * **CNN Basic Explanation**  \n> https:\/\/products.sint.co.jp\/aisia\/blog\/vol1-16  \n> * **CNN Implementations using Keras**  \n> https:\/\/qiita.com\/kenichiro-yamato\/items\/60affeb7ca9f67c87a17  \n> https:\/\/child-programmer.com\/ai\/keras\/conv2d\/  \n> https:\/\/child-programmer.com\/ai\/keras\/maxpooling2d\/  \n> * **SHAP Deep Explainer**  \n> https:\/\/shap-lrjball.readthedocs.io\/en\/latest\/generated\/shap.DeepExplainer.html  \n> * **SHAP Image Plot**  \n> https:\/\/shap-lrjball.readthedocs.io\/en\/latest\/generated\/shap.image_plot.html  \n> * **Referenced Notebooks**  \n>    * **CNN Implementation**  \n>    https:\/\/www.kaggle.com\/ravirajsinh45\/simple-model-for-casting-product-classification  \n>    https:\/\/www.kaggle.com\/ginsaputra\/visual-inspection-of-casting-products-using-cnn  \n>    https:\/\/www.kaggle.com\/tomythoven\/casting-inspection-with-data-augmentation-cnn  \n>    * **Feature Map**  \n>    https:\/\/www.kaggle.com\/vannak\/magical-localized-fault-detection\n> * **MISC**  \n>    * **convert markdown to html**  \n>    https:\/\/codepen.io\/ikapper\/pen\/EqaVjP","b46e07e9":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","af181561":"# 6. Conclusion\n<div class=\"alert alert-success\" role=\"alert\">\n    <ul>\n        <li>Data augmentation proces can be easily incorporated into training process by using ImageDataGenerator.<\/li>\n        <li>In data augmentation process, we should avoid excessive conversion and may require subtle adjustments depending on the dataset.<\/li>\n        <li>According to the result of model interpretation, it was found that the scratches\/holes on the surface of the products and the unevenness around the products are regarded as the important features of defective products.<\/li>\n        <li>Since we successfully built a model with relatively high accuracy, it is considered possible to incorporate the model into the camera of the inspection line and proceed with the automation of inspection.\n            <ul>\n                <li>However, considering the damage caused by mis-detection, there is no full-reliability until the model is used for full automation (there are also difficulties related to image acquisition such as lighting). So we should introduce it as one of tools to support inspection workers or a labor saving method.<\/li>\n            <\/ul>\n        <\/li>\n    <\/ul>\n<\/div>","47e1a565":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","9256109b":"# 1. Overview\n## Project Detail  \n> In the manufacturing industry, reducing processing errors in the manufacturing process is important for maximizing profits. In order to reduce processing errors, it is necessary to secure a budget for quality assurance, implement manual inspection work, and review the manufacturing process. Particularly, the inspection process is carried out by many companies, but there are problems such as uneven accuracy denpending on inspection workers and increased labor costs.  \n> In this analysis, we will verify whether the bottleneck of \"manual inspection\" can be cleared by automating the inspection process by machine learning in the manufacturing process of [casting products](https:\/\/en.wikipedia.org\/wiki\/Casting_%28metalworking%29). [Casting](https:\/\/en.wikipedia.org\/wiki\/Casting) is a technique in which molten metal is poured into a mold and processed into a desired shape.  \n> According to [this article](https:\/\/en.wikipedia.org\/wiki\/Casting_defect), the following are some of the defects in the casting process.\n> * blow holes\n> * pinholes\n> * burr\n> * shrinkage defects\n> * mould material defects\n> * pouring metal defects\n> * metallurgical defects\n\n## About Dataset\n> This dataset provides image data of impellers for submersible pumps.<br\/><br\/>\n> <table border=\"0\">\n>     <tr style=\"background-color: white !important;\">\n>        <td>\n>            <img src=\"https:\/\/static.turbosquid.com\/Preview\/2020\/06\/07__08_34_27\/11R131.JPGB3B4468C-B515-4E11-92F7-4CA67966DB2BZoom.jpg\" width=\"300\">\n>            <figcaption style=\"text-align:center\">Submersible Pump<\/figcaption>\n>        <\/td>\n>        <td>\n>            <img src=\"https:\/\/5.imimg.com\/data5\/WI\/KC\/MY-6121640\/submersible-pump-impeller-500x500.jpg\" width=\"300\">\n>            <figcaption style=\"text-align:center\">Impeller<\/figcaption>\n>        <\/td>\n>    <\/tr>\n> <\/table><br\/>\n> The image data is labeled with ok(normal) and def(defect\/anomaly) in advance. In addition, since it is necessary to illuminate the image in a stable condition when acquiring the image, the data was acquired based on a special lighting setting.","81ca4811":"## What is Data Augmentation?\n<div class=\"alert alert-success\" role=\"alert\">\n    <p>When training with image data without data augmentation, we simply need the specified number of data and create a mini-batch. When executing data augmentation, after acquiring the data, various augmentation techniques are applied to the image to create a new mini-batch. <br\/>\n    The main parameters of data augmentation techniques(Keras <a href='https:\/\/keras.io\/ja\/preprocessing\/image\/'>ImageDataGenerator<\/a>) are as follows : <\/p>\n    <ul>\n        <li><b>rotation_range<\/b> : Rotate the image (ex. 50 -> rotate randomly in -50\u00b0~50\u00b0)<\/li>\n        <li><b>zoom_range<\/b> : Zoom in\/out on the image (ex. 0.5 -> zoom in\/out randomly in 1-0.5~1+0.5)<\/li>\n        <li><b>brightness_range<\/b> : Change the brightness (ex. [0.3,1.0] -> change randomly in [0.3,1.0])<\/li>\n        <li><b>vertical_flip<\/b> : Flip the image upside down<\/li>\n        <li><b>horizontal_flip<\/b> : Flip the image left or right<\/li>\n        <li><b>height_shift_range<\/b> : Move the image up or down in parallel (ex. 0.3 -> move up\/down randomly in [-0.3*Height, 0.3*Height])<\/li>\n        <li><b>width_shift_range<\/b> : Move the image left or right in parallel (ex. 0.3 -> move left\/right randomly in [-0.3*Width, 0.3*Width])<\/li>\n        <li><b>rescale<\/b> : The image is normalized by multiplying each pixel value by a constant. (ex. 1\/255 -> normalize the RGB value of each pixel between 0.0 and 1.0)<\/li>\n    <\/ul>\n<\/div>","763f441d":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","ac96cd9e":"<div class=\"alert alert-success\" role=\"alert\">\n    <p>Evaluate the model with the test set.<br>\n       According to the confusion matrix, out of the 715 images, 1 was misclassified to be defective (False Positive), and 2 were misclassified to be normal (False Negative).<br>\n       In the problem of image inspection of industrial products, it is considered that the following effects will occur due to misclassification.<\/p>\n    <ul>\n        <li><b>False Negative<\/b> : Since it is a <u>\"mis-detection\"<\/u> case which means the model cannot classify defect images as 'defect', there is a risk that it may adversely affect the subsequent process or lead to defects in the finished products.<\/li>\n        <li><b>False Positive<\/b> : Since it is a <u>\"over-detection\"<\/u> case which means the model misclassify normal images as 'defect', there is a risk that the line downtime will be unnecessarily increased or the cost for reproduction will be unnecessarily increased.<\/li>\n    <\/ul>\n    <p>Whether to consider False Positive or False Negative (maybe depending on business requirements) determines which of <b>Precision<\/b> or <b>Recall<\/b> should be selected as a performance indicator for model evaluation. If you want to consider both, choose <b>F1 score<\/b>, which is the harmonic mean of Precision and Recall.<\/p>\n    <ul>\n        <li>Accuracy : 99.6%<\/li>\n        <li>Precision : 99.8%<\/li>\n        <li>Recall : 99.6%<\/li>\n        <li>F1 Score : 99.7%<\/li>\n    <\/ul>\n<\/div>","6388c7c8":"## Model Settings\n<div class=\"alert alert-success\" role=\"alert\">\n    <p>The elements of the model are listed below.\uff08<a href=\"https:\/\/www.kaggle.com\/ginsaputra\/visual-inspection-of-casting-products-using-cnn\">Referenced Notebook<\/a>\uff09<\/p>\n    <ul>\n        <li><b>Sequential<\/b> : model container\uff08<a href=\"https:\/\/keras.io\/ja\/getting-started\/sequential-model-guide\/\">Official Manual<\/a>\uff09<\/li>\n        <li><b>Conv2D<\/b> : convolutional layer for 2D images\uff08<a href=\"https:\/\/keras.io\/ja\/layers\/convolutional\/#conv2d\">Official Manual<\/a>\uff09\n            <ul>\n                <li><b>filters<\/b> : number of filters\n                    <ul>\n                        <li>numbers such as <u>16, 32, 64, 128, 256 and 512<\/u> tend to be used, and there is a technique to increase the number of filter for the complicated task and decrease it for simple one.<\/li>\n                    <\/ul>\n                <\/li>\n                <li><b>kernel_size<\/b> : filter size (width * height)\n                    <ul>\n                        <li>combinations of odd numbers such as <u>3x3, 5x5, 7x7<\/u> tend to be used.<\/li>\n                    <\/ul>\n                <\/li>\n                <li><b>strides<\/b> : window size used for convolution<\/li>\n                <li><b>input_shape<\/b> : size of input images (width\/height, color channel)\n                    <ul>\n                        <li>if you input color images as it is, the model will need convolutions for 3 RGB channels, which will increase the amount of calculation(graysclaed images need less calculations).<\/li>\n                    <\/ul>\n                <\/li>\n                <li><b>activation<\/b> : activation function<\/li>\n                <li><b>padding<\/b> : adjust the size of the layer output. When set to 'same', the pixels are filled with 0 so that the input and output sizes are the same.<\/li>\n            <\/ul>\n        <\/li>\n        <li><b>MaxPooling2D<\/b> : pooling layer for 2D images\uff08<a href=\"https:\/\/keras.io\/ja\/layers\/pooling\/#maxpooling2d\">Official Manual<\/a>\uff09\n            <ul>\n                <li><b>pool_size<\/b> : specify width\/height range and extract the largest pixel in this range to downscale the input<\/li>\n                <li><b>strides<\/b> : window size used for pooling<\/li>\n            <\/ul>\n        <\/li>\n        <li><b>Flatten<\/b> : convert input to linear vector\uff08<a href=\"https:\/\/keras.io\/ja\/layers\/core\/#flatten\">Official Manual<\/a>\uff09<\/li>\n        <li><b>Dropout<\/b> : apply dropout and randomly set the input to the unit to 0 to prevent overfitting when updating weights\uff08<a href=\"https:\/\/keras.io\/ja\/layers\/core\/#dropout\">Official Manual<\/a>\uff09\n            <ul>\n                <li><b>rate<\/b> : ratio of dropping the input to the unit<\/li>\n            <\/ul>\n        <\/li>\n        <li><b>Dense<\/b> : fully connected layer\uff08<a href=\"https:\/\/keras.io\/ja\/layers\/core\/#dense\">Official Manual<\/a>\uff09\n            <ul>\n                <li><b>units<\/b> : number of dimensions of output<\/li>\n                <li><b>activation<\/b> : activation function\uff08binary classification : <u>sigmoid<\/u>, other objectives : <u>softmax<\/u>\uff09 <\/li>\n            <\/ul>\n        <\/li>\n    <\/ul>\n<\/div>","a71e1fcd":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","904887d2":"Since the loss at the time of training and validation are steadily decreasing for each epoch, and the accuracy at the time of training and the validation are steadily increasing, it can be said that the training is generally successful.","90e11e31":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","ab4c329f":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","9d555daf":"# 3. Load the dataset","f9a15f76":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","a846b048":"# 5. Modeling\nSince the purpose of this analysis is to learn the basics of image analysis technology, we will train the model using a simple CNN, and not pursue the accuracy of the model.","b9fbfa3a":"Reusing a trained model and training history","aa43054c":"## Build Model\n<div class=\"alert alert-success\" role=\"alert\">\n    <ul>\n        <li><b>EarlyStopping<\/b> : Conditions to stop training early (<a href='https:\/\/keras.io\/ja\/callbacks\/#earlystopping'>Official Manual<\/a>)<\/li>\n            <ul>\n                <li>ex. validation loss not improved continuously in 2 epochs<\/li>\n            <\/ul>\n        <li><b>ModelCheckpoint<\/b> : Model saving settings for each epoch (<a href='https:\/\/keras.io\/ja\/callbacks\/#modelcheckpoint'>Official Manual<\/a>)<\/li>\n    <\/ul>\n<\/div>","b9532d11":"## Execute Data Augmentation\n<div class=\"alert alert-success\" role=\"alert\">\n    <p>In Keras, you can pass <a href='https:\/\/keras.io\/ja\/preprocessing\/image\/'>ImageDataGenerator<\/a> class as a dataset when training a model, and it creates a mini-batch by randomly applying the parameters. Empirically, if the parameter is set to an extreme high\/low value, the image will be strongly converted and the training will be difficult to proceed. In addition, it seems that fine adjustment of parameters is required for each target data or analysis objectives in order to proceed with training successfully.<br\/>\n    In this analysis, we will execute data augmentation with a slight conversion by referring to other notebooks.<\/p>\n<\/div>","471de6c0":"# 4. Pre-Processing","0a87f66b":"## Misclassified Images\nLet's take a look at the misclassified images in the test set. It is not clear to the human eye why it was misclassified.","a6c8d4ec":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","90bf59e7":"<h2 style=\"text-align:center;font-size:200%;;\">Model Explainability in Industrial Image Detection<\/h2>\n<h3  style=\"text-align:center;\">Keywords : \n    <span class=\"label label-success\">Image Classification<\/span> \n    <span class=\"label label-success\">Data Augmentation<\/span> \n    <span class=\"label label-success\">CNN<\/span> \n    <span class=\"label label-success\">Model Explanation<\/span> \n    <span class=\"label label-success\">Error Analysis<\/span> \n<\/h3>","63f4e898":"# 2. Import libraries","e41b966e":"# Table of Contents<a id='top'><\/a>\n>1. [Overview](#1.-Overview)\n>    * [Project Detail](#Project-Detail)\n>    * [About Dataset](#About-Dataset)\n>1. [Import libraries](#2.-Import-libraries)\n>1. [Load the dataset](#3.-Load-the-dataset)\n>1. [Pre-Processing](#4.-Pre-Processing)\n>    * [What is Data Augmentation?](#What-is-Data-Augmentation?)\n>    * [Execute Data Augmentation](#Execute-Data-Augmentation)\n>1. [Modeling](#5.-Modeling)\n>    * [Model Settings](#Model-Settings)\n>    * [Build Model](#Build-Model)\n>    * [Model Performance](#Model-Performance)\n>    * [Predict on Some Images](#Predict-on-Some-Images)\n>    * [Misclassified Images](#Misclassified-Images)\n>    * [Explain Model](#Explain-Model)\n>    * [Explain Why Misclassified](#Explain-Why-Misclassified)\n>1. [Conclusion](#6.-Conclusion)\n>1. [References](#7.-References)","5978d2b3":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","4d64b660":"## Predict on Some Images\nSelect images and apply it to the model.","091a22d3":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","d38eb653":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","5662e9c3":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","95d77673":"## Explain Model\nPerform model interpretation by using <a href='https:\/\/github.com\/slundberg\/shap'>SHAP<\/a>. Let's use some of the images that were classified as <b>defect<\/b> to see what part of images were regarded as important features. Focusing on the blue part of SHAP results, it is considered that the model regards <u>the scratches\/holes on the surface of the products and the unevenness around the products<\/u> as the features of defective products."}}