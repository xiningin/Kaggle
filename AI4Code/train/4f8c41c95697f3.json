{"cell_type":{"a4880a87":"code","15db1206":"code","020b3f49":"code","c2dde072":"code","834f1b8c":"code","61a82d2b":"code","a45a8df9":"code","3a55af01":"markdown","44721a02":"markdown","332f3e05":"markdown","413c25df":"markdown","bf6ebbdb":"markdown","684d8cf3":"markdown","7a4ee2bd":"markdown","7ca87cbb":"markdown"},"source":{"a4880a87":"import numpy  as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nimport pydash\nimport math\nimport os\nimport time\nfrom pydash import flatten\nfrom collections import Counter, OrderedDict\nfrom humanize import intcomma\nfrom operator import itemgetter\nfrom typing import *\nfrom sklearn.model_selection import train_test_split\n\n# import spacy\n# nlp = spacy.load('en')","15db1206":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col=0)\ndf_test  = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', index_col=0)\ndf_train","020b3f49":"def tokenize_df(\n    dfs: List[pd.DataFrame], \n    keys          = ('text', 'keyword', 'location'), \n    stemmer       = False, \n    ngrams        = 1,\n    preserve_case = True, \n    reduce_len    = False, \n    strip_handles = True,\n    use_stopwords = True,\n    **kwargs,\n) -> List[List[str]]:\n    # tokenizer = nltk.TweetTokenizer(preserve_case=True,  reduce_len=False, strip_handles=False)  # defaults \n    tokenizer = nltk.TweetTokenizer(preserve_case=preserve_case, reduce_len=reduce_len, strip_handles=strip_handles) \n    porter    = nltk.PorterStemmer()\n    stopwords = set(nltk.corpus.stopwords.words('english') + [ 'nan' ])\n\n    output    = []\n    for df in flatten([ dfs ]):\n        for index, row in df.iterrows():\n            tokens = flatten([\n                tokenizer.tokenize(str(row[key] or \"\"))\n                for key in keys    \n            ])\n            if use_stopwords:\n                tokens = [ \n                    token \n                    for token in tokens \n                    if token.lower() not in stopwords\n                    and len(token) >= 2\n                ]        \n            if stemmer:\n                tokens = [ \n                    porter.stem(token) \n                    for token in tokens \n                ]\n            if ngrams:\n                tokens = [\n                    \" \".join(tokens[i:i+n])\n                    for n in range(1,ngrams+1)\n                    for i in range(0,len(tokens)-n+1)\n                ]            \n            output.append(tokens)\n\n    return output\n\n\ndef get_labeled_tokens(df, **kwargs) -> Dict[int, List[str]]:\n    tokens = {\n        0: flatten(tokenize_df( df[df['target'] == 0], **kwargs )),\n        1: flatten(tokenize_df( df[df['target'] == 1], **kwargs )),\n    }\n    return tokens\n\n\ndef get_word_frequencies(df, **kwargs) -> Dict[int, Counter]:\n    tokens = get_labeled_tokens(df, **kwargs)\n    freqs = { \n        0: Counter(dict(Counter(tokens[0]).most_common())), \n        1: Counter(dict(Counter(tokens[1]).most_common())), \n    }  # sort and cast\n    return freqs\n\n\ndef get_log_likelihood(df, vocab_df, **kwargs):\n    vocab  = set(flatten(tokenize_df(vocab_df, **kwargs)))\n    tokens = tokenize_df( df, **kwargs )\n    freqs  = get_word_frequencies(df, **kwargs)\n    log_likelihood = {}\n    for token in vocab:\n        # Implement Laplacian Smoothing\n        p_false = (freqs[0].get(token, 0) + 1) \/ ( len(tokens[0]) + len(vocab) )  # [0] == False \n        p_true  = (freqs[1].get(token, 0) + 1) \/ ( len(tokens[1]) + len(vocab) )  # [1] == True\n        log_likelihood[token] = np.log( p_true \/ p_false )\n    return log_likelihood\n    \n    \ndef get_logprior(df, **kwargs):\n    \"\"\" Log probability of a word being positive given imbalanced data \"\"\"\n    tokens = tokenize_df( df, **kwargs )\n    return np.log( len(tokens[0]) \/ len(tokens[1]) ) if len(tokens[1]) else 0   \n    return np.log( len(tokens[0]) \/ len(tokens[1]) ) if len(tokens[1]) else 0   ","c2dde072":"def print_logprior():\n    tokens   = get_labeled_tokens(df_train)\n    logprior = get_logprior(df_train)\n\n    print('len(tokens[0])                    =', len(tokens[0]))\n    print('len(tokens[1])                    =', len(tokens[1]))\n    print('logprior(df_test)                 =', logprior)\n    print('math.exp(logprior(df_test))       =', math.exp(logprior))\n    print('math.exp(logprior(df_test)) ** -1 =', math.exp(logprior)**-1)\n    \nprint_logprior()","834f1b8c":"def naive_bayes_classifier( df_train, df_test, **kwargs ) -> np.array:\n    vocab_df       = [ df_train, df_test ]\n    log_likelihood = get_log_likelihood( df_train, vocab_df, **kwargs )    \n    logprior       = get_logprior(df_train, **kwargs)\n    \n    predictions = []\n    for tweet_tokens in tokenize_df(df_test, **kwargs):\n        log_prob = np.sum([ \n            log_likelihood.get(token, 0)\n            for token in tweet_tokens\n        ]) + logprior\n        prediction = int(log_prob > 0)\n        predictions.append(prediction)\n    \n    return np.array(predictions)            ","61a82d2b":"def test_accuracy(splits=3, **kwargs):\n    time_start  = time.perf_counter()\n\n    accuracy = 0\n    for _ in range(splits):\n        train, test = train_test_split(df_train, test_size=1\/splits)      \n        predictions = naive_bayes_classifier(train, test, **kwargs)\n        accuracy   += np.sum( test['target'] == predictions ) \/ len(predictions) \/ splits\n        \n    time_taken  = time.perf_counter() - time_start\n    time_taken \/= splits\n    print(f'ngrams = {ngrams} | accuracy = {accuracy*100:.2f}% | time = {time_taken:.1f}s')\n    \nfor ngrams in [1,2,3,4,5]:\n    test_accuracy( splits=3, ngrams=ngrams )","a45a8df9":"kwargs = { \"ngrams\": 3 }\ndf_submission = pd.DataFrame({\n    \"id\":     df_test.index,\n    \"target\": naive_bayes_classifier(df_train, df_test, **kwargs)\n})\ndf_submission.to_csv('submission.csv', index=False)\n! head submission.csv","3a55af01":"# Tokenization and Word Frequencies\n\nHere we tokenize the text using nltk.TweetTokenizer, apply lowercasing, tweet preprocessing, and stemming.\n\nThen compute a dictionary lookup of word counts for each label","44721a02":"# CSV Data","332f3e05":"# NLP Naive Bayes\n\n[Disaster Tweets Dataset](https:\/\/www.kaggle.com\/c\/nlp-getting-started)\n\nNaive Bayes computes the probability of each word appearing under each label, based on token frequency, the sums the log Likelihood of all words in a tweet added to the log prior (to account for an unbalanced dataset).","413c25df":"# Submission","bf6ebbdb":"# Further Reading\n\nThis notebook is part of a series exploring Natural Language Processing\n- 0.77536 - [NLP TF-IDF Classifier](https:\/\/www.kaggle.com\/jamesmcguigan\/disaster-tweets-tf-idf-classifier)\n- 0.74164 - [NLP Logistic Regression](https:\/\/www.kaggle.com\/jamesmcguigan\/disaster-tweets-logistic-regression\/)\n- 0.79773 - [NLP Naive Bayes](https:\/\/www.kaggle.com\/jamesmcguigan\/nlp-naive-bayes)","684d8cf3":"# Imports","7a4ee2bd":"# Logprior\n\n`exp()` undoes `log()` and `** -1` inverts the ratio. \n\nThis shows we have a nearly balanced dataset with about 15% more tokens in the disaster category","7ca87cbb":"# Naive Bayes Solver\n\nSimply sum up the negative log likelihood and logprior for each word in each tweet and check the number is positive"}}