{"cell_type":{"973f9850":"code","b808f49d":"code","9e96267f":"code","386a1d87":"code","bcc625c9":"code","5429fb25":"code","ae54708e":"code","fac9120a":"code","7623dc8a":"code","e518ac24":"code","0504e9bc":"code","4940d9f0":"code","579618dd":"code","1bf8f173":"code","78441fa2":"code","7ea577b1":"code","8acdcbf7":"code","b7023a21":"code","098c96e1":"code","f8f5c74f":"code","a9b197a5":"code","aa197804":"code","13c38cb2":"code","45cbe947":"code","590c423e":"code","6a654f44":"code","dba2e1cc":"code","3b681c15":"code","49c20de6":"code","dff5ddfb":"code","af01a5a7":"code","bd8daccc":"code","2a1f0a6a":"code","0e0e4011":"code","7a5addbd":"code","bf02ae40":"code","0f624dc3":"code","5bfc7f68":"code","54dc3084":"markdown","f609d03e":"markdown","8429a7cc":"markdown","50b268cf":"markdown","6b7287cd":"markdown","28d7d950":"markdown","ea60644f":"markdown","efc482f2":"markdown","5e6fc024":"markdown","c62dc03a":"markdown","6fffdfc1":"markdown","719990dd":"markdown","b3579d3f":"markdown","c0bfbc80":"markdown","b5a9980c":"markdown"},"source":{"973f9850":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport  json\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nMETADATA_PATH = \"\/kaggle\/input\/arxiv\/arxiv-metadata-oai-snapshot-2020-08-14.json\"        \n        \n!gsutil cp gs:\/\/arxiv-dataset\/metadata-v5\/internal-citations.json \/tmp","b808f49d":"%%time\n## create a metadata citation csv\n\nwith open(\"\/tmp\/internal-citations.json\") as f:\n    citations = json.load(f)\n\nwith open(\"arxiv-metadata-ext-citation.csv\",\"w+\") as f_out :\n    f_out.write(\"id,id_reference\\n\")\n    for i,id in enumerate(citations):\n        for k in citations[id]:\n            f_out.write(f'{id},{k}\\n')\n                \ndf_citations = pd.read_csv(\"arxiv-metadata-ext-citation.csv\",dtype={\"id\":object,\"id_reference\":object})\ndf_citations.head()","9e96267f":"%%time\n## create a metadata category csv\n\nwith open(\"arxiv-metadata-ext-category.csv\",\"w+\") as f_out :\n    f_out.write(\"id,category_id\\n\")\n\n\n    with open(METADATA_PATH) as f_in:\n        for i,line in enumerate(f_in):\n            row = json.loads(line)\n            id = row[\"id\"]\n            categories = row[\"categories\"].split()\n            for c in categories:\n                f_out.write ( f'\"{id}\",\"{c}\"\\n'  )\n                \ndf_categories = pd.read_csv(\"arxiv-metadata-ext-category.csv\",dtype={'id': object})\ndf_categories.head()                                     ","386a1d87":"%%time\n## create a metadata paper csv\n\ntitles = []\nabstracts = []\nids = []\nauthors = []\njournal_refs = []\nlicenses = []\n\nwith open(METADATA_PATH) as f_in:\n    for i,line in enumerate(f_in):\n        row = json.loads(line)\n\n        titles.append(row[\"title\"])\n        abstracts.append(row[\"abstract\"])\n        ids.append(row[\"id\"])\n        authors.append(row[\"authors\"]) \n        journal_refs.append(row[\"journal-ref\"])\n        licenses.append(row[\"license\"])\n        \ndf_papers = pd.DataFrame({\n    'id' : ids,\n    'title' : titles,\n    'abstract' : abstracts,\n    'authors' : authors,\n    'journal-ref' : journal_refs,\n    'license':licenses\n    \n})\ndf_papers.to_csv(\"arxiv-metadata-ext-paper.csv\", index=False)\ndf_papers.head()        ","bcc625c9":"%%time\n## create a metadata version csv\nwith open(\"arxiv-metadata-ext-version.csv\",\"w+\") as f_out :\n    f_out.write(\"id,version,created\\n\")\n\n    with open(METADATA_PATH) as f_in:\n        for i,line in enumerate(f_in):\n            row = json.loads(line)\n            id = row[\"id\"]\n            versions = row[\"versions\"]\n            for v in versions:\n                f_out.write (f'{id},{v[\"version\"]},\\\"{v[\"created\"]}\\\"\\n')\n\ndf_versions = pd.read_csv(\"arxiv-metadata-ext-version.csv\",dtype={'id': object})\ndf_versions.head()                     ","5429fb25":"%%time\n## add datetime year and month to create field\ndf_versions[\"created\"] = pd.to_datetime(df_versions[\"created\"]) \ndf_versions[\"year\"] = df_versions[\"created\"].dt.year\ndf_versions[\"month\"] = df_versions[\"created\"].dt.month\n\ndf_versions.to_csv(\"arxiv-metadata-ext-version.csv\", index=False)","ae54708e":"def count_by_category_and_year(group_name):\n    cats = df_categories.merge(df_taxonomy, on=\"category_id\").query(\"group_name == @group_name\").merge(df_versions.query(\"version =='v1'\")[[\"id\",\"year\"]], on =\"id\") \n    cats = cats.groupby([\"year\",\"category_name\"]).count().reset_index().pivot(index=\"category_name\", columns=\"year\",values=\"id\") \n    return cats\n\ndef count_by_archive_and_year(group_name):\n    cats = df_categories.merge(df_taxonomy, on=\"category_id\").query(\"group_name == @group_name\").merge(df_versions.query(\"version =='v1'\")[[\"id\",\"year\"]], on =\"id\") \n    cats = cats.groupby([\"year\",\"archive_name\"]).count().reset_index().pivot(index=\"archive_name\", columns=\"year\",values=\"id\") \n    return cats\n\n\ndef show_count_by_category_and_year(group_name,figsize=(10,5)):\n    plt.figure(figsize=figsize)\n    plt.title(f\"{group_name} papers by category and year\")\n    sns.heatmap(count_by_category_and_year(group_name),cmap=\"Greens\", linewidths=0.01, linecolor='palegreen')\n    plt.show()\n\ndef show_count_by_archive_and_year(group_name=\"Physics\",figsize=(10,5)):\n    plt.figure(figsize=figsize)\n    plt.title(f\"{group_name} papers by archive and year\")\n    sns.heatmap(count_by_archive_and_year(group_name),cmap=\"Greens\", linewidths=0.01, linecolor='palegreen')\n    plt.show()\n    \ndef top_k_influential (group_name, top_k=5, threshold=100):\n    ids = df_categories.merge(df_taxonomy, on=\"category_id\").query(\"group_name ==@group_name\")[\"id\"].values\n    cits = df_citations.query( 'id.isin(@ids)', engine=\"python\" ).merge( df_versions.query(\"version == 'v1'\")[[\"id\",\"year\"]], on =\"id\").groupby([\"year\",\"id_reference\"]).count()\n    cits = cits.reset_index()\n    cits = cits.loc[cits.groupby('year')['id'].nlargest(top_k).reset_index()['level_1']]\n    cits = cits.query ( \"id > @threshold\" )\n    cits = cits.rename(columns={\"id\":\"references\", \"id_reference\":\"id\"})\n    cits = cits.merge(df_papers,on=\"id\")\n    \n    return cits\n\ndef show_influential_heatmap (group_name, cits, figsize=(10,25)):\n    hm_cits =  cits.pivot(index=\"title\", columns=\"year\",values=\"references\")     \n\n    plt.figure(figsize=figsize)\n    plt.title(\"Top influential papers by year\")\n    sns.heatmap(hm_cits,cmap=\"Greens\", linewidths=0.01, linecolor='palegreen')\n    plt.show()\n\ndef make_clickable(val):\n    # target _blank to open new window\n    return '<a target=\"_blank\" href=\"{}\">{}<\/a>'.format(val, val)\n    \n    \ndef show_influential_table ( cits):\n    df = cits.groupby([\"id\",\"title\",\"authors\",\"abstract\"]).agg({\"references\":\"sum\"}).reset_index()\n    df = df.sort_values(by=\"references\",ascending = False).reset_index(drop=True)\n    df [\"url\"] = df[\"id\"].map(lambda x:  f'https:\/\/arxiv.org\/pdf\/{x}' ) \n    df [\"authors\"] = df[\"authors\"].map(lambda x: x if len(str(x)) < 50 else str(x)[:47] + \"...\" )\n\n    df =df [[\"title\",\"authors\",\"abstract\",\"url\",\"references\"]]\n    return df.style.format({'url': make_clickable})\n\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'We', 'paper', 'new'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask,\n                    min_word_length = 4,\n                    #normalize_plurals = True, \n                    #collocations = True,      \n                    #collocation_threshold = 10      \n                         )\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n","fac9120a":"## load taxonomy from https:\/\/arxiv.org\/category_taxonomy\nwebsite_url = requests.get('https:\/\/arxiv.org\/category_taxonomy').text\nsoup = BeautifulSoup(website_url,'lxml')\n\nroot = soup.find('div',{'id':'category_taxonomy_list'})\n\ntags = root.find_all([\"h2\",\"h3\",\"h4\",\"p\"], recursive=True)\n\nlevel_1_name = \"\"\nlevel_2_code = \"\"\nlevel_2_name = \"\"\n\nlevel_1_names = []\nlevel_2_codes = []\nlevel_2_names = []\nlevel_3_codes = []\nlevel_3_names = []\nlevel_3_notes = []\n\nfor t in tags:\n    if t.name == \"h2\":\n        level_1_name = t.text    \n        level_2_code = t.text\n        level_2_name = t.text\n    elif t.name == \"h3\":\n        raw = t.text\n        level_2_code = re.sub(r\"(.*)\\((.*)\\)\",r\"\\2\",raw)\n        level_2_name = re.sub(r\"(.*)\\((.*)\\)\",r\"\\1\",raw)\n    elif t.name == \"h4\":\n        raw = t.text\n        level_3_code = re.sub(r\"(.*) \\((.*)\\)\",r\"\\1\",raw)\n        level_3_name = re.sub(r\"(.*) \\((.*)\\)\",r\"\\2\",raw)\n    elif t.name == \"p\":\n        notes = t.text\n        level_1_names.append(level_1_name)\n        level_2_names.append(level_2_name)\n        level_2_codes.append(level_2_code)\n        level_3_names.append(level_3_name)\n        level_3_codes.append(level_3_code)\n        level_3_notes.append(notes)\n\ndf_taxonomy = pd.DataFrame({\n    'group_name' : level_1_names,\n    'archive_name' : level_2_names,\n    'archive_id' : level_2_codes,\n    'category_name' : level_3_names,\n    'category_id' : level_3_codes,\n    'category_description': level_3_notes\n    \n})\ndf_taxonomy.to_csv(\"arxiv-metadata-ext-taxonomy.csv\", index=False)\ndf_taxonomy.groupby([\"group_name\",\"archive_name\"]).head(3)","7623dc8a":"df = df_versions.query(\"version =='v1'\").groupby([\"year\",\"month\"]).agg({\"id\":'count'}).reset_index()\ndf[\"tot\"] = df[\"id\"].cumsum()\n\ndf = df.query(\"year > 1990 and ( year != 2020 or month < 8)\")\ndf[\"month\"] =  df[\"year\"].astype(str) + \"-\" + df[\"month\"].astype(str)  \n\n\n_df = df_categories.merge(df_taxonomy, on=\"category_id\", how=\"left\").drop_duplicates([\"id\",\"group_name\"]).groupby(\"group_name\").agg({\"id\":\"count\"}).sort_values(by=\"id\",ascending=False).reset_index()\n\nfig = plt.figure(figsize=(20,10))\n\n\nax1 = plt.subplot2grid((2, 2), (0, 0))\nax1.title.set_text('ArXiv papers')\nax1.plot(df[\"month\"], df[\"tot\"])\nax1.hlines(y=1e6, xmin=0, xmax=len(df), color='green', linestyle=\"dotted\")\nax1.hlines(y=1.5e6, xmin=0, xmax=len(df), color='green', linestyle=\"dotted\")\nax1.set_xticks(np.arange(0, len(df), 12.0))\nax1.tick_params('x',labelrotation=90)\n\n\nax2 = plt.subplot2grid((2, 2), (1, 0))\nax2.title.set_text(\"ArXiv papers by month\")\nax2.plot(df[\"month\"], df[\"id\"])\nax2.hlines(y=10000, xmin=0, xmax=len(df), color='green', linestyle=\"dotted\")\nax2.hlines(y=15000, xmin=0, xmax=len(df), color='green', linestyle=\"dotted\")\nax2.set_xticks(np.arange(0, len(df), 12.0))\nax2.tick_params('x',labelrotation=90)\n\nax3 = plt.subplot2grid((2, 2), (0, 1), rowspan=2)\nax3.title.set_text(\"ArXiv papers by group\")\nexplode = (0, 0, 0, 0.2, 0.3, 0.3, 0.2, 0.1) \nax3.pie(_df[\"id\"],  labels=_df[\"group_name\"], autopct='%1.1f%%', startangle=160, explode=explode)\n\n\nplt.tight_layout()\nplt.show()\n\n\n","e518ac24":"cats = df_categories.merge(df_taxonomy, on=\"category_id\").merge(df_versions.query(\"version =='v1'\")[[\"id\",\"year\"]], on =\"id\") \ncats = cats.groupby([\"year\",\"group_name\"]).count().reset_index().pivot(index=\"group_name\", columns=\"year\",values=\"id\") \n\nplt.figure(figsize=(10,5))\nplt.title(\"ArXiv papers by group and year\")\nsns.heatmap(cats,cmap=\"Greens\", linewidths=0.01, linecolor='palegreen')\nplt.show()\n\n","0504e9bc":"group_name=\"Physics\"\nshow_count_by_archive_and_year(group_name)","4940d9f0":"cits = top_k_influential (group_name, top_k=3)\nshow_influential_heatmap (group_name, cits=cits, figsize=(10,14))\n","579618dd":"show_influential_table (cits)","1bf8f173":"group_name=\"Mathematics\"\nshow_count_by_category_and_year (group_name,figsize=(10,6))","78441fa2":"cits = top_k_influential (group_name, top_k=3, threshold=10)\nshow_influential_heatmap (group_name, cits=cits, figsize=(10,12))","7ea577b1":"show_influential_table (cits)","8acdcbf7":"group_name=\"Computer Science\"\nshow_count_by_category_and_year (group_name,figsize=(10,6))","b7023a21":"cits = top_k_influential (group_name, top_k=5)\nshow_influential_heatmap (group_name, cits=cits, figsize=(10,6))","098c96e1":"show_influential_table (cits)","f8f5c74f":"group_name=\"Statistics\"\nshow_count_by_category_and_year (group_name,figsize=(10,3))","a9b197a5":"cits = top_k_influential (group_name, top_k=5)\nshow_influential_heatmap (group_name, cits=cits, figsize=(10,4))","aa197804":"show_influential_table (cits)","13c38cb2":"group_name=\"Quantitative Biology\"\nshow_count_by_category_and_year (group_name,figsize=(10,4))","45cbe947":"cits = top_k_influential (group_name, top_k=1, threshold=3)\nshow_influential_heatmap (group_name, cits=cits, figsize=(10,6))","590c423e":"show_influential_table (cits)","6a654f44":"group_name=\"Electrical Engineering and Systems Science\"\n\nshow_count_by_category_and_year (group_name,figsize=(10,3))","dba2e1cc":"cits = top_k_influential (group_name, top_k=1, threshold=1)\nshow_influential_heatmap (group_name, cits=cits, figsize=(6,1))","3b681c15":"show_influential_table (cits)","49c20de6":"group_name=\"Quantitative Finance\"\n\nshow_count_by_category_and_year (group_name,figsize=(10,4))","dff5ddfb":"cits = top_k_influential (group_name, top_k=1, threshold=3)\nshow_influential_heatmap (group_name, cits=cits, figsize=(10,6))","af01a5a7":"show_influential_table (cits)","bd8daccc":"group_name=\"Economics\"\n\nshow_count_by_category_and_year (group_name,figsize=(10,2))","2a1f0a6a":"cits = top_k_influential (group_name, top_k=1, threshold=3)\nshow_influential_heatmap (group_name, cits=cits, figsize=(10,3))","0e0e4011":"show_influential_table (cits)","7a5addbd":"df = df_citations.query(\"id_reference == '1412.6980'\")\\\n.merge(df_categories,on=\"id\")\\\n.merge(df_taxonomy,on=\"category_id\").drop_duplicates([\"id\",\"group_name\"])\\\n.merge(df_versions.query(\"version =='v1'\")[[\"id\",\"year\"]], on =\"id\")\n\n\nhmap =df.groupby([\"group_name\",\"year\"]).agg({\"id\":\"count\"}).reset_index().pivot(index=[\"group_name\"], columns=\"year\",values=\"id\") \n\nplt.figure(figsize=(10,5))\nplt.title(\"Papers that reference 'Adam: A Method for Stochastic Optimization'\")\nsns.heatmap(hmap,cmap=\"Greens\", linewidths=0.01, linecolor='palegreen', annot=True, fmt=\".0f\")\nplt.show()\n\n","bf02ae40":"\n_df = df.drop_duplicates([\"id\"]).merge(df_papers,on=\"id\")\n_df = _df.sample(frac=1, axis=1).reset_index(drop=True)\n\n_df [\"url\"] = _df[\"id\"].map(lambda x:  f'https:\/\/arxiv.org\/pdf\/{x}' ) \n_df [\"authors\"] = _df[\"authors\"].map(lambda x: x if len(str(x)) < 50 else str(x)[:47] + \"...\" )\n\n_df =_df [[\"group_name\",\"title\",\"authors\",\"abstract\",\"url\",\"year\"]]\n    \n    \nplot_wordcloud(_df[\"title\"], title=\"Word Cloud of titles that reference 'Adam'\")","0f624dc3":"plot_wordcloud(_df[\"abstract\"], title=\"Word Cloud of abstract that reference 'Adam'\")","5bfc7f68":"_sample_df = _df.groupby([\"group_name\"]).head(3)\n_sample_df.style.format({'url': make_clickable})","54dc3084":"## Electrical Engineering and Systems Science","f609d03e":"## Quantitative Biology","8429a7cc":"# Preprocess data and utility functions","50b268cf":"## Computer Science","6b7287cd":"# Taxonomy And Top Influential Papers","28d7d950":"## Mathematics","ea60644f":"## Statistics","efc482f2":"arXiv (pronounced \"archive\"\u2014the X represents the Greek letter chi) is an open-access repository of electronic preprints (known as e-prints) approved for posting after moderation, but not full peer review. \n\nIn many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository before publication in a peer-reviewed journal. \n\nBegun on August 14, 1991, arXiv.org passed the half-million-article milestone on October 3, 2008, and had hit a million by the end of 2014. On August 2020 there are more then 1.7M papers in arXiv repository. \n\n\nAlthough arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic, or reject submissions that are not scientific papers, or sometimes for undisclosed reasons. See [https:\/\/arxiv.org\/help\/moderation](https:\/\/arxiv.org\/help\/moderation) \n\nThe lists of moderators of arXiv are publicly available [here](https:\/\/arxiv.org\/moderators\/).\n\nsource: [wikipedia](https:\/\/en.wikipedia.org\/wiki\/ArXiv)\n","5e6fc024":"Here some random papers that reference this paper","c62dc03a":"## Physics","6fffdfc1":"## Economics","719990dd":"In this notebook I explore and visualize the arXiv taxonomy and the most influential articles in various arXiv fields over the years, counting the citations of each paper to the other papers on arXiv","b3579d3f":"\narXiv consists of scientific papers [categorized in the fields of these groups](https:\/\/arxiv.org\/category_taxonomy) \n\n* Computer Science\n* Economics\n* Electrical Engineering and Systems Science\n* Mathematics\n* Physics\n* Quantitative Biology\n* Quantitative Finance\n* Statistics\n\nEach group is in turn subdivided into different categories and - for Physics group - in different archives.","c0bfbc80":"## Quantitative Finance","b5a9980c":"## 'Adam: A Method for Stochastic Optimization'\n\nUnsurprisingly, this 2015 paper by Diederik P. Kingma and Jimmy Ba is the most interdisciplinary referenced paper, for the effectiveness of its use with deep neural networks.\n"}}