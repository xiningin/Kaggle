{"cell_type":{"bc06a1c6":"code","91d5461c":"code","c460d858":"code","d79d3ca1":"code","ab3fb59d":"code","8cc01c29":"code","1dbfe9fe":"code","8a141cc8":"code","3d3ba055":"code","e67af288":"code","55cfb495":"code","20085dd1":"code","45fe6b5e":"code","3160b473":"code","c63e90e2":"code","b99be137":"code","dfcba2ff":"code","3b8bbbb5":"code","aa0671a6":"code","37116e65":"code","8d7a3b50":"code","4d9deac0":"code","00dda2cc":"code","938e6473":"code","cef1ffbb":"code","872631fa":"code","7a804519":"code","d0886ec9":"code","d080dd11":"code","ae3d0c04":"code","b5dfda1d":"code","cba89ec1":"code","beff32ff":"code","5a39051b":"code","92153ee3":"code","b4e72ca4":"code","25dd88be":"code","365d688c":"code","4e60d319":"code","872b6f5c":"code","d332e6f3":"code","a36a27b4":"code","bd7bf361":"code","de9dacfb":"code","a853b60d":"code","7ffc4df3":"code","8d53a1ee":"code","5cde3fc1":"code","9d10d4c4":"code","df2b6f36":"code","8677bee1":"code","b0d1f964":"code","c5e600a4":"markdown","77330520":"markdown","1d68eed3":"markdown","50df2fa1":"markdown","45966b08":"markdown","eabffd47":"markdown","422ce58b":"markdown","a7b1c895":"markdown","33587651":"markdown","e1abc581":"markdown","e923c6ce":"markdown","ebc6f06b":"markdown","c10a20b3":"markdown"},"source":{"bc06a1c6":"!pip install bs4 # Install required library","91d5461c":"import urllib.request as urllib2 # To get webpage content\nfrom bs4 import BeautifulSoup as bs # To parse(read and understand) webpage","c460d858":"response = urllib2.urlopen('https:\/\/keithgalli.github.io\/web-scraping\/example.html') # To get the webpage content\nhtml_doc = response.read() # To read html content of webpage ","d79d3ca1":"print(html_doc)\n# Below we can see that we have read html page and printed but it is difficult to understand in one line","ab3fb59d":"soup = bs(html_doc, 'html.parser') \n# Here we have created bs4 object and passed the html content after which bs4 will parse usinng default html.parser\n# There are 2 types of parser,\n# 1. html.parser (default) :- This parser is not strict in reading opening and closing tags\n# 2. lxml parser :- This parser requires proper opening and closing tags, if any tag is not well defined this parser ignores that particular tag","8cc01c29":"print(soup.prettify()) \n# prettify() is used to pretty print","1dbfe9fe":"print(soup.title) # To read title of webpage\n\nprint(soup.title.string) # To read only string content","8a141cc8":"soup.head # To get contents in head of webpage","3d3ba055":"soup.body # To get contents in body of webpage","e67af288":"soup.body.div # To get contents of div in body of webpage","55cfb495":"soup.a # To get anchor tags","20085dd1":"print(soup.a['href']) # To get href of anchor tags\n\nprint(soup.a.string) # To get string of anchor tags","45fe6b5e":"print(soup.body.get_text()) # get_text() return all the string of child elements","3160b473":"soup.find('p') # find() by default finds 1 apperance of the given tag in the webpage","c63e90e2":"soup.find(['p','h2']) # find([list of tags]) by default finds and return any of 1 tag apperance in given list from the webpage","b99be137":"soup.find_all('p') # find_all() will find and return all the appearance of the tag in comma seperated list from webpage","dfcba2ff":"soup.find_all('p',attrs={'id':'paragraph-id'}) \n# attrs={} will find and return only matching attributes of given tag ","3b8bbbb5":"soup.find_all(id=\"paragraph-id\")\n# code in previous cell can also be written like this, where in this code it will return all the tags with given id","aa0671a6":"for _ in soup.find_all('p'):\n  print(_.string) # will return string of each p tag","37116e65":"for _ in soup.find_all('p'):\n  print(_.get_text()) # will return string of each p tag child elements also","8d7a3b50":"for _ in soup.find_all('p'):\n  print(_.i) # will return i tag inside each p tag, if not present returns None ","4d9deac0":"import re\n\nsoup.find_all(re.compile('p|h2')) # Using regex to find all p and h2 tag","00dda2cc":"soup.find_all(string=re.compile('Some')) # Get all the string that contains 'Some' keyword","938e6473":"soup.find_all('p',string=re.compile('Some')) # Get all the p tag that contains 'Some' keyword","cef1ffbb":"soup.find_all('h2',string=re.compile('(H|h)eader')) # Get all the h2 tag that contains 'header' or 'Header' keyword","872631fa":"soup.select('p') # will return all p tags","7a804519":"soup.select('div p') # will return only p tags present in div elements","d0886ec9":"soup.select('div ~ p') # will return p tags preceeded by div tag","d080dd11":"soup.select('h2 + p') # will return p tags placed immediately after h2 tag","ae3d0c04":"soup.select('p:nth-of-type(3)') # will return 3rd p tag","b5dfda1d":"soup.select('#paragraph-id') # will return elements that has id(#) as 'paragraph-id'","cba89ec1":"soup.select('#paragraph-id b') # will return b tag present after id(#) as 'paragraph-id'","beff32ff":"soup.select('body > p') # will return only p tags where parent is body element\n# Only 1 p tag was not fetched as it's parent is div tag and not body","5a39051b":"soup.select('body > p:nth-of-type(1)') # will return only 1 p tag where parent is body element","92153ee3":"soup.body.findChildren()\n# findChildren() will find all children of body element","b4e72ca4":"print(len(soup.body.findChildren()))","25dd88be":"soup.body.findChild()\n# findChildren() will find first child of body element","365d688c":"soup.div.findChildren()","4e60d319":"soup.div.findChildren('a')","872b6f5c":"soup.div.findNextSiblings()\n# findNextSiblings() will return all siblings of div element","d332e6f3":"soup.div.findParent()\n# findParent() will return parent of div element i.e., body","a36a27b4":"soup.body.findChildren('h2') # Will return all children having h2 tag","bd7bf361":"soup.body.findChildren('p',id='paragraph-id') # Will return all p tag children having id='paragraph-id'","de9dacfb":"response2 = urllib2.urlopen('https:\/\/keithgalli.github.io\/web-scraping\/webpage.html')\nhtml_doc2 = response2.read()","a853b60d":"soup_2 = bs(html_doc2,'html.parser')","7ffc4df3":"print(soup_2.prettify()[:300])","8d53a1ee":"for link in soup_2.select('ul.socials a'):\n  print(link['href'])","5cde3fc1":"for link in soup_2.select('li.social a'):\n  print(link['href'])","9d10d4c4":"for link in soup_2.find_all('a',string=re.compile('keithgalli')):\n  print(link['href'])","df2b6f36":"for i in soup_2.select('table.hockey-stats tbody tr td.season.sorted'):\n  print(i.string.strip())","8677bee1":"soup_2.find('ul',attrs={'class':'fun-facts'}).find_all(string=re.compile('is'))","b0d1f964":"for i in soup_2.select('ul.fun-facts li'):\n  if 'is' in i.get_text():\n    print(i.get_text())","c5e600a4":"# Fetching elements with their tag name","77330520":"## Fetch all the social media links from webpage","1d68eed3":"# Find Parent, Siblings, Children\n\n<img src='https:\/\/drive.google.com\/uc?export=view&id=10SfMIPTfSHYl3MpqaPZLnYJa19agQuMh'  >\n","50df2fa1":"# Exercise","45966b08":"## Grab fun facts that contains the word 'is'\n\n<img src='https:\/\/drive.google.com\/uc?export=view&id=1Ie07p9cuAD-ty0HROQCllyBY8xEGVesu'>","eabffd47":"## Scrape the table","422ce58b":"\n<img src='https:\/\/drive.google.com\/uc?export=view&id=1EHYXBRqdD1krQ_srE8oumUtnP5R5ZmGv'>","a7b1c895":"# Reference \n\n1. Keith Galli [GitHub](https:\/\/github.com\/KeithGalli\/web-scraping)\n2. [Cheatsheet](http:\/\/akul.me\/blog\/2016\/beautifulsoup-cheatsheet\/)","33587651":"Get season from above table","e1abc581":"# find() and find_all()","e923c6ce":"We will be using below webpage to understand bs4 library\n\n<img src='https:\/\/drive.google.com\/uc?export=view&id=1fpIGvG46TChEVTkoU-5a5AOVoNZmCJ4u'  >","ebc6f06b":"# BeautifulSoup\n\n\n<img src='https:\/\/drive.google.com\/uc?export=view&id=1UucK756xzylFk7OGMmfgInw7Yf_siGlD' height=400 >\n\nIn this notebook we will be learning about BeautifulSoup library that will help us in scrap websites.\n\n\n\n","c10a20b3":"# CSS Selectors\n\nreference :- [CSS Selector](https:\/\/www.w3schools.com\/cssref\/css_selectors.asp)"}}