{"cell_type":{"17734176":"code","081e71e5":"code","c520102d":"code","32bfdba3":"code","b07b6c85":"code","9c4c05c7":"code","5743d2e1":"code","28922168":"code","a2089cba":"code","2493b1de":"code","cb47b6ab":"code","0b9d306c":"code","7aa67e35":"code","7866afa6":"code","ddf12d65":"code","88f21076":"code","9ff1a065":"code","b64917d7":"code","0cbf2bc3":"code","17f1a10b":"code","5829946c":"code","b4b30ae5":"code","a97f9cb6":"code","fde6b545":"code","fd04537e":"code","04c99b82":"code","46217e2c":"code","649bf480":"markdown","00be7158":"markdown","0ab48269":"markdown","8613fcf3":"markdown","0038ead6":"markdown","dbf38e07":"markdown","2930b155":"markdown","2cfcdc4b":"markdown","86d84737":"markdown","32875b90":"markdown","a8bb9a93":"markdown","65c1a6df":"markdown","6e99cf4b":"markdown","fb6db782":"markdown","63924bae":"markdown","8e40e6d2":"markdown"},"source":{"17734176":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","081e71e5":"#importing library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score,f1_score,classification_report\nfrom sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler,MinMaxScaler,PowerTransformer,FunctionTransformer\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom xgboost.sklearn import XGBClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n%matplotlib inline \n","c520102d":"#load the data and check the shape\ndf=pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndf.shape","32bfdba3":"#check the data\ndf.head()","b07b6c85":"#check the data info\ndf.info()","9c4c05c7":"#check the duplicate rows\nprint(\"Number of duplicated rows are: \",df.duplicated().sum())\n\n#dropping the duplicated rows \ndf=df.drop_duplicates(keep=\"first\")\nprint(\"After removing,now number of duplicated rows are:\",df.duplicated().sum())","5743d2e1":"#check for null values \ndf.isnull().sum()","28922168":"#describing the data \ndf.describe()","a2089cba":"#check the correlation between the quality to each of columns \ndf.corr()[\"quality\"]","2493b1de":"#Graphically represent the correlation with heatmap\nplt.figure(figsize=(18,8))\nplt.title(\"Correlation heatmap\")\nsns.heatmap(df.corr(),annot=True,cmap=\"mako\")","cb47b6ab":"df[\"quality\"].unique()# here 6 types of rating available in quality columns","0b9d306c":"#get all the features in X.\nX=df.iloc[:,0:-1]\n\n#get the terget columns \ny=df[\"quality\"]\ny=pd.DataFrame(y,columns=[\"quality\"])\ny.value_counts()","7aa67e35":"# check the classes is balanced or not....\nplt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\ny[\"quality\"].value_counts().plot(kind=\"pie\",autopct=\"%.2f\",shadow=True, startangle=90)\n\nplt.subplot(1,2,2)\nsns.countplot(y[\"quality\"])\n\nplt.show()","7866afa6":"#analysing features cloumns with histogram  and boxplot\n#using histogram we perform univariate analysis\n#using box plot we perform bi-variate analysis\ni=1\nplt.figure(figsize=(30,90))\nfor col in X.columns:\n    plt.subplot(11,2,i)\n    sns.histplot(X[col],color=\"orange\")\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    plt.xlabel(col,fontsize=25)\n    plt.ylabel(\"count\",fontsize=25)\n    \n    i=i+1\n    plt.subplot(11,2,i)\n    sns.boxplot(y[\"quality\"],X[col])\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    plt.xlabel(col,fontsize=25)\n    plt.ylabel(\"quality\",fontsize=25)\n    i=i+1\n\nplt.show()","ddf12d65":"# Analysis each of the columns with quality col using BarPlot\ni=1\nplt.figure(figsize=(30,90))\nfor col in X.columns:\n    plt.subplot(11,2,i)\n    sns.barplot(df[\"quality\"],df[col],palette = \"mako\")\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    plt.xlabel(\"Quality\",fontsize=25)\n    plt.ylabel(col,fontsize=25)\n    \n    i=i+1\n\nplt.show()","88f21076":"#Balanc\u00edng the data point for each classes.\nfrom imblearn.combine import SMOTEENN\nos=SMOTEENN(random_state=42)\nX,y=os.fit_resample(X,y)","9ff1a065":"#check the shape and class of y after up-sampling\nprint(X.shape,y.shape)\nprint(y.value_counts())\nprint(\"Numbers of duplicated point after up sampling:\",X.duplicated().sum())","b64917d7":"# After up sampling ,check the classes is balanced or not....\nplt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\ny[\"quality\"].value_counts().plot(kind=\"pie\",autopct=\"%.2f\",shadow=True, startangle=90)\n\nplt.subplot(1,2,2)\nsns.countplot(y[\"quality\"])\n\nplt.show()","0cbf2bc3":"#It is a good practise to split the data to avoiding the data leakage \n#train test split  \nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\nprint(\"X_train shape: \",x_train.shape)\nprint(\"X_test shape: \",x_test.shape)\nprint(\"Y_train shape: \",y_train.shape)\nprint(\"Y_test shape: \",y_test.shape)\n","17f1a10b":"# Let's check for skewed columns\ndef distribution(data):\n    #let find the skewed col and fix them\n    skew_limit=0.75 # limit for skewed col\n    skew_vals=data[X.columns].skew()\n    skew_col=skew_vals[abs(skew_vals)>skew_limit].sort_values(ascending =False)\n    \n    #graphically represent the skewed col\n    i=1    \n    print(\"Columns names: \",skew_col.index)\n    print(\"\\n\\n\")\n    print(skew_col)\n    plt.figure(figsize=(50,90))\n    for col in skew_col.index:\n        plt.subplot(8,2,i)\n        sns.distplot(X[col],color=\"r\")\n        plt.xticks(fontsize=25)\n        plt.yticks(fontsize=25)\n        plt.xlabel(col,fontsize=25)\n        i=i+1\n        \n\n    plt.show()","5829946c":"#calling the function for checking the skewed colmuns\ndistribution(x_train)","b4b30ae5":"#lets fix them with power transformer (mathematical transformation)\nskewed_col=['chlorides', 'residual sugar', 'total sulfur dioxide', 'sulphates',\n       'free sulfur dioxide', 'volatile acidity'] #our skewed col list.\npt=PowerTransformer(standardize=False)# power transformer function.\nx_train[skewed_col]=pt.fit_transform(x_train[skewed_col])\nx_test[skewed_col]=pt.transform(x_test[skewed_col])\nx_train.head()","a97f9cb6":"# after applying power transformer ...\nx_train[skewed_col].skew()","fde6b545":"# Scaling all the features with min max scaler\nsc_m=MinMaxScaler()\nx_train=sc_m.fit_transform(x_train)\nx_test=sc_m.transform(x_test)","fd04537e":"#Evaluating the models with data using this function..\ndef evaluate(model):\n    model.fit(x_train,y_train)\n    y_pred=model.predict(x_test) \n    \n    #printing the model name and accuracy !!!!!\n    print(\"Model name:---->>>\",model)\n    print(\"accuracy score:--->>\",accuracy_score(y_test,y_pred))\n    print(\"\\n\")\n    print(classification_report(y_test,y_pred))\n    print(\"<<<<-------------------------------------------------------------------->>>>\")\n    ","04c99b82":"#Initialize the models \nLR=LogisticRegression(solver=\"newton-cg\") #Logistic regression.\nknn=KNeighborsClassifier(metric='manhattan',n_neighbors=1,weights='uniform') \nsvc=SVC(C=100,gamma=1,kernel='rbf',random_state=42) # support vector classifier.\ndtc=DecisionTreeClassifier()\n\n\n\nmodels=[LR,knn,svc,dtc] #create a list of models \n\nfor model in models:\n    evaluate(model)\n    ","46217e2c":"#let's play with our ensamble models. \nrf=RandomForestClassifier(random_state=42) #random forest classifier.\ngbr=GradientBoostingClassifier(learning_rate= 1,n_estimators=300,criterion='mse',random_state=42)\nxgb=XGBClassifier(learning_rate=0.1,n_estimators=500,random_state=42) #extream gradient boosting classifier.\n\nmodels=[rf,gbr,xgb] #create a list of models \n\nfor model in models:\n    evaluate(model)\n\n#it's show time ,yaaaaaaaaaaaaaaaahhhh!!!!!!!!","649bf480":"There are no pair of highly correlated independent features (Multicolinarity problems) ..that's great!!!!!!!","00be7158":"Here among all the Ensamble models ,ours best model is XGBClassifier(97.5% accuracy),2nd best model is GradientBoostingClassifier(97.2% accuracy)...great!!!!!!","0ab48269":"we got 6 skewed columns ,and almost all of them are right skewed ","8613fcf3":"Here our best model is KNeighborsClassifier (97.2% accuracy) and Support vector classifier (97.2% accuracy) and 2 nd best model is DecisionTreeClassifier(93% accuracy)....great job!!!","0038ead6":"here we got some skewed col ,we want to tranformed this using power transformer or function transformer !!!!!!!!!!\n","dbf38e07":"great now we don't have any skewed columns in our features !!!!!!!","2930b155":"great !!!!!!.we remove all the duplicated rows ,lets move .\n    ","2cfcdc4b":"# Evaluating models on the data","86d84737":"we observed that alcohol and sulphates is highly propotional to quality and volatile acidity is inversely propotional to the quality col..","32875b90":"# Among all the models our top 3 models are:---\n# 1.XGBClassifier(97.5% accuracy)\n\n# 2.KNeighborsClassifier (97.2% accuracy) , Support vector classifier (97.2% accuracy) and GradientBoostingClassifier(97.2% accuracy)\n\n# 3.RandomForestClassifier(95.9% accuracy)\n\n# Yaahooooo!we got great accuracy !!!!!","a8bb9a93":"# If you like this notebook plz consider a upvote for me ..........Happy kaggling !!!!!!!","65c1a6df":"this is totally imbalanced data or class ,class 5 and 6 contain the most of the data compare to the other classes.class 5 contain 577 data and class 6 contain 535 data and other classes contain rest of the data.....we should fix it to balanced the data or classes..","6e99cf4b":"There are no null values in our dataset ,great!!!!!!!!","fb6db782":"# EDA for data analysis","63924bae":"our classes are not fully balanced but almost balanced now , great!!!!!!","8e40e6d2":"great,,, our features are ready to fit in the modol,all the step we have done !!!!!!!"}}