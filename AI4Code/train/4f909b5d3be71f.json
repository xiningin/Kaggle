{"cell_type":{"e30a4c08":"code","e460477b":"code","3e8045c3":"code","dc93d175":"code","7a46c845":"code","4db58945":"code","7a153d17":"code","921d3c91":"code","6cbc697a":"code","7c437fa0":"code","88c9d6ad":"code","22a18a9a":"code","6dee3d0b":"code","1b118566":"code","32a017f1":"code","649d90e8":"code","de180b86":"code","61319858":"code","0cad6601":"code","5f87fdc1":"code","b3633230":"code","e2b82ed6":"code","19cdb054":"code","c4f1bbc4":"code","80c207f4":"code","bf4a5602":"code","c2169014":"code","947a029a":"code","9ee29bd3":"code","e6e35469":"code","71871f74":"code","379c2f67":"code","5462496c":"code","5a12add3":"code","c047adc5":"code","90c8eee4":"code","52e2d085":"code","47636c3d":"code","8c3c1778":"code","3f0999de":"code","fea7039a":"code","061d49e3":"code","3d4f7665":"code","dc66c797":"code","868a0b39":"code","b313fb4c":"code","0182978f":"code","79d2e4ca":"code","9c96c25b":"code","6e1a7e02":"code","386ff946":"code","bcdc53e9":"code","3bf71f9c":"markdown","90e296bb":"markdown","9ffc2479":"markdown","4ce1b07d":"markdown","7737068e":"markdown","57563ceb":"markdown","631bff4d":"markdown","a9fa2c31":"markdown","1db80709":"markdown","11da30d1":"markdown","8cb1cfa9":"markdown","d5a46030":"markdown","df667f38":"markdown","2152cdbf":"markdown","7508de72":"markdown","dd4ceb0a":"markdown","9543e471":"markdown","db151806":"markdown","8239bb26":"markdown","04bca340":"markdown","53283be8":"markdown","fd76320a":"markdown","1f24d551":"markdown","ae55e665":"markdown","e3b09974":"markdown","8d616458":"markdown","e36460f8":"markdown","c7a371a7":"markdown","15b27fa0":"markdown","90e2dfe7":"markdown"},"source":{"e30a4c08":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e460477b":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain.head()","3e8045c3":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","dc93d175":"x = train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')\nprint(x)","7a46c845":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntrain_len = train[train['target']==1]['text'].str.len()\nax1.hist(train_len,color='red')\nax1.set_title('disaster tweets')\n\ntrain_len = train[train['target']==0]['text'].str.len()\nax2.hist(train_len,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","4db58945":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntrain_len=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(train_len,color='red')\nax1.set_title('disaster tweets')\n\ntrain_len=train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(train_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","7a153d17":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword = train[train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\n\nword = train[train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","921d3c91":"from nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nstop=set(stopwords.words('english'))","6cbc697a":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","7c437fa0":"# Class 0 - not disaster\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n\nx,y=zip(*top)\nplt.bar(x,y)\n\n# Class 1 - disaster\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\nx,y=zip(*top)\nplt.bar(x,y)","88c9d6ad":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,5))\n#plt.figure(figsize=(10,5))\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nax1.bar(x,y,color='green')\n\n#plt.figure(figsize=(10,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nax2.bar(x,y)","22a18a9a":"counter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\n\nsns.barplot(x=y,y=x)","6dee3d0b":"def get_top_train_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","1b118566":"plt.figure(figsize=(10,5))\ntop_train_bigrams=get_top_train_bigrams(train['text'])[:10]\nx,y=map(list,zip(*top_train_bigrams))\nsns.barplot(x=y,y=x)","32a017f1":"slices = list(train['text'])\ntokenized_slices = [r.split() for r in slices]\nlen_by_token = [len(t) for t in tokenized_slices]\nlen_by_eumjeol = [len(s.replace(' ','')) for s in slices]","649d90e8":"plt.figure(figsize = (12,5))\n\n#train_len = train[train['target']==1]['text'].str.len()\nplt.hist(len_by_token, bins = 50, alpha=0.5, color=\"r\", label=\"word\")\nplt.hist(len_by_eumjeol, bins = 50, alpha=0.5, color=\"b\", label=\"aplt.yscallphabet\")\n\nplt.yscale('log', nonposy = 'clip')\nplt.title('Tweet Length Histogram')\nplt.xlabel('Tweets length')\nplt.ylabel('number of Tweets')","de180b86":"# Based on Words\nprint('Maximum : {}'.format(np.max(len_by_token)))\nprint('Minimum : {}'.format(np.min(len_by_token)))\nprint('Average : {:.2f}'.format(np.mean(len_by_token)))\nprint('Std : {:.2f}'.format(np.std(len_by_token)))\nprint('Median : {}'.format(np.median(len_by_token)))\nprint('1st quarter : {}'.format(np.percentile(len_by_token, 25)))\nprint('3rd quarter : {}'.format(np.percentile(len_by_token, 75)))\n\n#plt.figure(figsize=(12,5))\n#plt.boxplot([len_by_token],\n#            labels = ['word'],\n#            showmeans=True)","61319858":"# Based on characters\nprint('Maximum : {}'.format(np.max(len_by_eumjeol)))\nprint('Minimum : {}'.format(np.min(len_by_eumjeol)))\nprint('Average : {:.2f}'.format(np.mean(len_by_eumjeol)))\nprint('Std : {:.2f}'.format(np.std(len_by_eumjeol)))\nprint('Median : {}'.format(np.median(len_by_eumjeol)))\nprint('1st Quarter : {}'.format(np.percentile(len_by_eumjeol, 25)))\nprint('3rd Quarter : {}'.format(np.percentile(len_by_eumjeol, 75)))\n\n#plt.figure(figsize=(12,5))\n#plt.boxplot([len_by_eumjeol],\n#            labels = ['Alphabet'],\n#            showmeans=True)","0cad6601":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nwordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=800, height=600)\nwordcloud = wordcloud.generate(' '.join(train['text']))\n\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","5f87fdc1":"missing_cols = ['keyword', 'location']\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 4), dpi=100)\n\nsns.barplot(x=train[missing_cols].isnull().sum().index, y=train[missing_cols].isnull().sum().values, ax=axes[0])\nsns.barplot(x=test[missing_cols].isnull().sum().index, y=test[missing_cols].isnull().sum().values, ax=axes[1])\n\naxes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Training Set', fontsize=13)\naxes[1].set_title('Test Set', fontsize=13)\n\nplt.show()\n\nfor df in [train, test]:\n    for col in ['keyword', 'location']:\n        df[col] = df[col].fillna(f'no_{col}')","b3633230":"import re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string","e2b82ed6":"df = pd.concat([train,test])\ndf.shape","19cdb054":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nexample = \"New competition launched :https:\/\/www.kaggle.com\"\nremove_URL(example)","c4f1bbc4":"df['text'] = df['text'].apply(lambda x : remove_URL(x))","80c207f4":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\nexample = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"\nprint(remove_html(example))","bf4a5602":"df['text'] = df['text'].apply(lambda x : remove_html(x))","c2169014":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","947a029a":"df['text'] = df['text'].apply(lambda x : remove_emoji(x))","9ee29bd3":"def remove_punct(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample = \"I am a #king\"\nprint(remove_punct(example))","e6e35469":"df['text'] = df['text'].apply(lambda x : remove_punct(x))","71871f74":"#!pip install pyspellchecker","379c2f67":"#from spellchecker import SpellChecker\n\n#spell = SpellChecker()\n#def correct_spellings(text):\n#    corrected_text = []\n#    misspelled_words = spell.unknown(text.split())\n#    for word in text.split():\n#        if word in misspelled_words:\n#            corrected_text.append(spell.correction(word))\n#        else:\n#            corrected_text.append(word)\n#    return \" \".join(corrected_text)\n        \n#text = \"corect me plese\"\n#correct_spellings(text)","5462496c":"#df['text']=df['text'].apply(lambda x : correct_spellings(x))","5a12add3":"wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=800, height=600)\nwordcloud = wordcloud.generate(' '.join(df['text']))\n\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","c047adc5":"from nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm","90c8eee4":"def create_corpus(df):\n    corpus = []\n    for train in tqdm(df['text']):\n        words = [word.lower() for word in word_tokenize(train) if ((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n\ncorpus = create_corpus(df)","52e2d085":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","47636c3d":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt',encoding=\"utf8\",mode='r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","8c3c1778":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntrain_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","3f0999de":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","fea7039a":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","061d49e3":"from keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","3d4f7665":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(LSTM(64, dropout=0.4, recurrent_dropout=0.4))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-3)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","dc66c797":"model.summary()","868a0b39":"df_train = train_pad[:train.shape[0]]\ndf_test = train_pad[train.shape[0]:]\ndf_train","b313fb4c":"X_train,X_test,y_train,y_test=train_test_split(df_train,train['target'].values,test_size=0.20)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","0182978f":"history = model.fit(X_train, y_train, batch_size=4, epochs=5, validation_data = (X_test, y_test), verbose=0)","79d2e4ca":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","9c96c25b":"train.head()","6e1a7e02":"test.head()","386ff946":"test = test['id'].astype(int) # TYPE issue\ny_pre=model.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)","bcdc53e9":"sub.head(10)","3bf71f9c":"## 6. Make Submission file","90e296bb":"Word Cloud","9ffc2479":"### Analyzing punctuations","4ce1b07d":"### Ngram analysis\n\nN-gram refers to a method used statistically by dividing entire strings such as text and binary into sub-strings by N-values.<br>\nFor example, \"\uc778\uacf5\uc9c0\ub2a5\" 2-gram -> \"\uc778\uacf5\", \"\uc9c0\ub2a5\", \"\uacf5\uc9c0\" <br>\n\nIt is a learning method that belongs to the category of \"inductive learning\" and is a form of extracting common points through specific cases.<br>\nN-gram is used by memorizing(learning) the appearance frequency that occurs through fragmented strings.<br>\n\nWe will do a bigram (n=2) analysis over the tweets.Let's check the most common bigrams in tweets.","7737068e":"### Number of characters","57563ceb":"There are a lot of missing values in location.<br>\nOn the other hand, the number of missing values in keyword is quite small.","631bff4d":"### Length of languages","a9fa2c31":"### Number of words","1db80709":"## 1. Introduction\nThis notebook is written by a novice to NLP. <br>\nIt deals with the basic concept of NLP and how express natural langauage data (EDA).<br>\nThe reference notebooks are as follows.<br>\n* [Basic EDA,Cleaning and GloVe](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)\n* [NLP with Disaster Tweets - EDA, Cleaning and BERT](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert)","11da30d1":"### Load data","8cb1cfa9":"In this part, it refers to the Korean [tistory blog page](https:\/\/soyoung-new-challenge.tistory.com\/35) ","d5a46030":"The distribution of both seems to be almost same.<br>\n120 to 140 characters in a tweet are the most common among both.","df667f38":"### Check class distribution\nThis competition is binary classification task.<br>\nTherefore, we should check the class condition whether the data is skewed or not.","2152cdbf":"### Missing Values\n\n**0.8%** of keyword is missing in both training and test set<br>\n**33%** of location is missing in both training and test set<br>\n\nMissing value ratio between train and test data in keyword and location are quite similar.<br>\nWe can guess that they are most probably taken from the same data.<br>\n\nMissing values in those features are filled with no_keyword and no_location respectively.","7508de72":"## 5. Model Baseline","dd4ceb0a":"### URL","9543e471":"### Common words","db151806":"There is a type issue on test['id'] <br>\nAdd type converting code for type matching.","8239bb26":"### Common stopwords","04bca340":"Spelling Correction","53283be8":"Word cloud after data cleaning","fd76320a":"## 2. Exploratory Data Analysis\n* character level\n* word level\n* sentence level analysis.","1f24d551":"## 3. Data Cleaning\n\n* URL\n* HTML tags\n* Emojis\n* Punctuations (#,\",...)\n\n* Spelling Correction","ae55e665":"The data does not skewed.<br>\nAmong the number of total data is 7,613 and 0(no disaster) is 4342(57%) and 1(disaster) is 3271(43%).","e3b09974":"### Average word length","8d616458":"### HTML tags","e36460f8":"* Red: historgram for the number of words<br>\n* Blue: histogram for the number of characters","c7a371a7":"### Puncutations","15b27fa0":"## 4. GloVe\nGlove stands for global vectors for word representation.<br>\n* It is a word embedding method using both count-based and predictive-based.<br>\n* It pointed out the shortcomings of the count-based LSA and prediction-based Word2Vec.\n* GloVe is an unsupervised learning algorithm developed by Stanford University for generating word embeddings by aggregating global word-word-co-occurrence matrix from a corpus.\n\nWe will use GloVe pretrained corpus model to represent our words with 100 D.","90e2dfe7":"### Emojis\n[References](https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b)"}}