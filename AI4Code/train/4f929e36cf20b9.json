{"cell_type":{"26ce57d8":"code","7c782697":"code","e144ed8a":"code","df1647be":"code","318c69b6":"code","12b037c3":"code","e52dadf7":"code","7b3227c3":"code","8381346d":"code","b1fc06a8":"code","723efac3":"code","a5f6def3":"code","3d5a7ef4":"code","baa5aa8c":"code","32e0330a":"code","e203e573":"code","0264c614":"code","f13ca86c":"markdown","071ac83b":"markdown","169841a6":"markdown","ae4c51ba":"markdown","b0b0066f":"markdown","a6058e2e":"markdown","d80c5947":"markdown","dc49a64f":"markdown","f62929fb":"markdown"},"source":{"26ce57d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7c782697":"import tensorflow\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.layers import Input,Conv2D,UpSampling2D,Dense,\\\nLeakyReLU,BatchNormalization,Dropout,Reshape,Flatten,\\\nLambda,Conv2DTranspose,Activation\nimport keras.backend as K\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")","e144ed8a":"from tensorflow.python.framework.ops import disable_eager_execution\ndisable_eager_execution()","df1647be":"(train_data,train_label),(test_data,test_label) = keras.datasets.mnist.load_data()\nprint(f\"Train Data shape >> {train_data.shape}\")\nprint(f\"Train Label shape >> {train_label.shape}\")\nprint(f\"Test Data shape >> {test_data.shape}\")\nprint(f\"Test Label shape >> {test_label.shape}\")","318c69b6":"train_data,_,train_label,_ = train_test_split(train_data,train_label,stratify=train_label,test_size=0.5)\ntest_data,_,test_label,_ = train_test_split(test_data,test_label,stratify=test_label,test_size=0.9)\n\nprint(f\"Train Data shape >> {train_data.shape}\")\nprint(f\"Train Label shape >> {train_label.shape}\")\nprint(f\"Test Data shape >> {test_data.shape}\")\nprint(f\"Test Label shape >> {test_label.shape}\")","12b037c3":"print(train_data.max())\nprint(train_data.min(),'\\n')\ntrain_data = train_data\/255.0\n\nprint(test_data.max())\nprint(test_data.min())\ntest_data = test_data\/255.0","e52dadf7":"train_data = train_data.reshape((-1,28,28,1))\ntest_data = test_data.reshape((-1,28,28,1))\n\nprint(f\"Train Data shape >> {train_data.shape}\")\nprint(f\"Test Data shape >> {test_data.shape}\")","7b3227c3":"class VAE():\n    def __init__(self,input_dim,encoder_conv_filters,encoder_conv_kernel_size,\\\n                 encoder_conv_strides,decoder_conv_filters,decoder_conv_kernel_size,\\\n                 decoder_conv_strides,z_dim,momentum,dropout_rate):\n        self.input_dim= input_dim\n        \n        self.encoder_conv_filters = encoder_conv_filters\n        self.encoder_conv_kernel_size =encoder_conv_kernel_size\n        self.encoder_conv_strides = encoder_conv_strides\n        \n        self.decoder_conv_filters = decoder_conv_filters\n        self.decoder_conv_kernel_size = decoder_conv_kernel_size\n        self.decoder_conv_strides = decoder_conv_strides\n        \n        self.z_dim = z_dim\n        self.momentum = momentum\n        self.dropout_rate =dropout_rate\n        \n        self._build()\n        \n    def _build(self):\n        ##encoder\n        encoder_input = Input(shape=self.input_dim)\n        print(f\"Input Shape : {self.input_dim}\")\n        encoder_H = encoder_input\n\n        for i in range(len(self.encoder_conv_filters)):\n            encoder_H = Conv2D(self.encoder_conv_filters[i],self.encoder_conv_kernel_size[i],\\\n                               self.encoder_conv_strides[i],padding='same')(encoder_H)\n            encoder_H = BatchNormalization(momentum=self.momentum)(encoder_H)\n            encoder_H = LeakyReLU()(encoder_H)\n            encoder_H = Dropout(self.dropout_rate)(encoder_H)\n    \n        shape_before_flatten = K.int_shape(encoder_H)[1:]\n        print(f\"Shape after Encoder Conv layers : {shape_before_flatten}\")\n        encoder_H = Flatten()(encoder_H)\n        print(f\"Shape after Flatten Layer : {np.prod(shape_before_flatten)}\")\n\n        self.m = Dense(self.z_dim)(encoder_H)\n        self.log_var = Dense(self.z_dim)(encoder_H)\n        self.encoder_m_log_var = Model(encoder_input,[self.m,self.log_var])\n\n        def sampling(args):\n            m,log_var = args\n            epsilon = K.random_normal(shape=K.shape(m),mean=0,stddev=1.0)\n            sampled = m + K.exp((1\/2)*log_var)*epsilon\n            return sampled\n\n        encoder_output = Lambda(sampling)([self.m,self.log_var])\n        self.encoder = Model(encoder_input,encoder_output)\n        \n        ##decoder\n        decoder_input = Input(shape=(self.z_dim,))\n        decoder_H = Dense(np.prod(shape_before_flatten))(decoder_input)\n        decoder_H = Reshape(shape_before_flatten)(decoder_H)\n\n        for i in range(len(self.decoder_conv_filters)):\n            if i <2:\n                decoder_H = UpSampling2D()(decoder_H)\n            decoder_H = Conv2D(self.decoder_conv_filters[i],self.decoder_conv_kernel_size[i],\\\n                               self.decoder_conv_strides[i],padding='same')(decoder_H)\n            if i<3:\n                decoder_H = BatchNormalization(momentum=self.momentum)(decoder_H)\n                decoder_H = LeakyReLU()(decoder_H)\n                decoder_H = Dropout(self.dropout_rate)(decoder_H)\n\n        decoder_output = Activation('sigmoid')(decoder_H)\n        self.decoder = Model(decoder_input,decoder_output)\n        \n        ##full model\n        model_input = encoder_input\n        model_output = self.decoder(encoder_output)\n        self.model = Model(model_input,model_output)\n        \n        print(\"\\n\\nVAE MODEL CONSTRUCTED!\\n\\n\")\n        \n    def compile(self,lr=0.0001,r_loss_factor=1000):\n        self.lr = lr\n        \n        def recon_loss(y_true,y_pred):\n            r_loss = K.mean(K.square(y_true-y_pred),axis=[1,2,3])\n            return r_loss_factor * r_loss\n\n        def kl_loss(y_true,y_pred):\n            kl_loss = -0.5 * K.sum(1+self.log_var-K.square(self.m)-K.exp(self.log_var),axis=1)\n            return kl_loss\n\n        def vae_loss(y_true,y_pred):\n            return recon_loss(y_true,y_pred) + kl_loss(y_true,y_pred)\n        \n        adam = Adam(lr=self.lr)\n        self.model.compile(loss=vae_loss,optimizer=adam,metrics=[recon_loss,kl_loss])\n        print(\"\\n\\nVAE MODEL COMPILED!\\n\\n\")\n        \n    def train(self,train_data,batch_size,epochs):\n        self.model.fit(train_data,train_data,epochs=epochs,batch_size=batch_size,shuffle=True)","8381346d":"vae = VAE(input_dim=K.int_shape(train_data)[1:],encoder_conv_filters=[32,64,64,64],\\\n          encoder_conv_kernel_size=[3,3,3,3],encoder_conv_strides=[1,2,2,1],\\\n          decoder_conv_filters=[64,64,32,1],decoder_conv_kernel_size=[3,3,3,3],\\\n          decoder_conv_strides=[1,1,1,1],z_dim=2,momentum=0.9,dropout_rate=0.2)\nvae.compile(lr=0.0005,r_loss_factor=1000)","b1fc06a8":"history = vae.train(train_data,epochs=1000,batch_size=128)","723efac3":"model = vae.model","a5f6def3":"# Results of Reconstruction on Train dataset\nl = np.arange(30000)\nidx = np.random.choice(l,36)\n\ntrain_samples = train_data[idx]\n\nplt.figure(figsize=(5,5))\nfor i in range(len(idx)):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_samples[i],cmap=plt.cm.binary)\nplt.show()\n","3d5a7ef4":"train_results = model.predict(train_samples)\nplt.figure(figsize=(5,5))\nfor i in range(len(idx)):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_results[i],cmap=plt.cm.binary)\nplt.show()","baa5aa8c":"# Results of Reconstruction on Test dataset\nl = np.arange(1000)\nidx = np.random.choice(l,36)\n\ntest_samples = test_data[idx]\n\nplt.figure(figsize=(5,5))\nfor i in range(len(idx)):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(test_samples[i],cmap=plt.cm.binary)\nplt.show()","32e0330a":"test_results = model.predict(test_samples)\nplt.figure(figsize=(5,5))\nfor i in range(len(idx)):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(test_results[i],cmap=plt.cm.binary)\nplt.show()","e203e573":"encoder = vae.encoder_m_log_var\n\ncolors=['red','pink','orange','blue','purple','black','brown','green','olive','lavender']\n\nplt.figure(figsize=(16,16))\nfor i in range(10):\n    mask = train_label==i\n    mask = mask[:400]\n    train_samples = train_data[mask]\n    z_mean,_ = encoder.predict(train_samples)\n    plt.scatter(z_mean[:,0],z_mean[:,1],c=colors[i],alpha=0.2)\nplt.show()    ","0264c614":"plt.figure(figsize=(16,16))\nfor i in range(10):\n    mask = test_label==i\n    mask = mask[:400]\n    test_samples = test_data[mask]\n    z_mean,_ = encoder.predict(test_samples)\n    plt.scatter(z_mean[:,0],z_mean[:,1],c=colors[i],alpha=0.2)\nplt.show()    ","f13ca86c":"**z-vector visualization**","071ac83b":"**(1) Visualization on Train dataset**","169841a6":"**VAE model**","ae4c51ba":"**scaling** ","b0b0066f":"**Reshape images**","a6058e2e":"* **Code above is because we will use a user-defined loss function that will take numpy arrasy as arguments. If tensors are being entered as arguments of loss function, then there will be no problem though**\n* https:\/\/github.com\/tensorflow\/tensorflow\/issues\/47311\n* \"The main issue here is that you are using a custom loss callback that takes an argument advantage (from your data generator, most likely numpy arrays).\nIn Tensorflow 2 eager execution, the advantage argument will be numpy, whereas y_true, y_pred are symbolic. The way to solve this is to turn off eager execution\"","d80c5947":"* **from tensorflow.python.framework.ops import disable_eager_execution**\n* **disable_eager_execution()**","dc49a64f":"**(1) Visualization on Test dataset**","f62929fb":"**Let's just use only 50% of train & 10% of test datasets**"}}