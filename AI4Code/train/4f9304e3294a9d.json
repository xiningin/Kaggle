{"cell_type":{"d53a2fb8":"code","468063d1":"code","78cc8480":"code","df46e97e":"code","c2345f36":"code","94900542":"code","94962f58":"code","7128aee1":"code","16d8c7d8":"code","b4052b99":"code","3c44899e":"code","7cc001e3":"code","da48e811":"code","7f7753c6":"code","71514bbc":"code","1c222656":"code","7d23fcaf":"code","fb2be4fe":"code","8be25ffe":"code","000604b3":"code","a922608c":"code","ec351b67":"code","238465af":"code","9be864cc":"code","ff9c7be3":"code","9df2c667":"code","e51e2bf5":"code","a2472372":"code","b813c723":"code","6c66dade":"code","0285ba49":"code","12e8b96e":"code","5d826eb7":"markdown","4d9a32e7":"markdown","9e09de53":"markdown","ee109471":"markdown","0fa7c966":"markdown","e04a5f93":"markdown","de5ee5ca":"markdown","3231f44a":"markdown","5bd172e4":"markdown","71b8febc":"markdown","495a462a":"markdown","b8835436":"markdown","6625b2c8":"markdown","ab83bfdb":"markdown","cf16902d":"markdown","db0acf07":"markdown","519cf0f0":"markdown","91bd7425":"markdown","c3616d0a":"markdown","9ddd9f64":"markdown","4defd805":"markdown","d70fdae2":"markdown","bc0c42c5":"markdown","73ce1913":"markdown","8293b31a":"markdown","efef4989":"markdown","1282e91a":"markdown","bf7f0684":"markdown"},"source":{"d53a2fb8":"# Uncomment and run the commands below if imports fail\n# !conda install numpy pandas pytorch torchvision cpuonly -c pytorch -y\n# !pip install matplotlib --upgrade --quiet","468063d1":"import torch\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split\n%matplotlib inline","78cc8480":"dataset = MNIST(root='data\/', download=True, transform=ToTensor())","df46e97e":"val_size = 10000\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","c2345f36":"batch_size=128","94900542":"train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)","94962f58":"for images, _ in train_loader:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n    break","7128aee1":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","16d8c7d8":"class MnistModel(nn.Module):\n    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n    def __init__(self, in_size, hidden_size, out_size):\n        super().__init__()\n        # hidden layer\n        self.linear1 = nn.Linear(in_size, hidden_size)\n        # output layer\n        self.linear2 = nn.Linear(hidden_size, out_size)\n        \n    def forward(self, xb):\n        # Flatten the image tensors\n        xb = xb.view(xb.size(0), -1)\n        # Get intermediate outputs using hidden layer\n        out = self.linear1(xb)\n        # Apply activation function\n        out = F.relu(out)\n        # Get predictions using output layer\n        out = self.linear2(out)\n        return out\n    \n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss, 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))","b4052b99":"input_size = 784\nhidden_size = 32 # you can change this\nnum_classes = 10","3c44899e":"model = MnistModel(input_size, hidden_size=32, out_size=num_classes)","7cc001e3":"for t in model.parameters():\n    print(t.shape)","da48e811":"for images, labels in train_loader:\n    outputs = model(images)\n    loss = F.cross_entropy(outputs, labels)\n    print('Loss:', loss.item())\n    break\n\nprint('outputs.shape : ', outputs.shape)\nprint('Sample outputs :\\n', outputs[:2].data)","7f7753c6":"torch.cuda.is_available()","71514bbc":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","1c222656":"device = get_default_device()\ndevice","7d23fcaf":"def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)","fb2be4fe":"for images, labels in train_loader:\n    print(images.shape)\n    images = to_device(images, device)\n    print(images.device)\n    break","8be25ffe":"class DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","000604b3":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)","a922608c":"for xb, yb in val_loader:\n    print('xb.device:', xb.device)\n    print('yb:', yb)\n    break","ec351b67":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","238465af":"# Model (on GPU)\nmodel = MnistModel(input_size, hidden_size=hidden_size, out_size=num_classes)\nto_device(model, device)","9be864cc":"history = [evaluate(model, val_loader)]\nhistory","ff9c7be3":"history += fit(5, 0.5, model, train_loader, val_loader)","9df2c667":"history += fit(5, 0.1, model, train_loader, val_loader)","e51e2bf5":"import matplotlib.pyplot as plt\n%matplotlib inline","a2472372":"losses = [x['val_loss'] for x in history]\nplt.plot(losses, '-x')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('Loss vs. No. of epochs');","b813c723":"accuracies = [x['val_acc'] for x in history]\nplt.plot(accuracies, '-x')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('Accuracy vs. No. of epochs');","6c66dade":"!pip install jovian --upgrade -q","0285ba49":"import jovian","12e8b96e":"jovian.commit(project='04-feedforward-nn', environment=None)","5d826eb7":"# Training Deep Neural Networks on a GPU with PyTorch\n\n### Part 4 of \"PyTorch: Zero to GANs\"\n\n*This notebook is the fourth in a series of tutorials on building deep learning models with PyTorch, an open source neural networks library. Check out the full series:*\n\n1. [PyTorch Basics: Tensors & Gradients](https:\/\/jovian.ml\/aakashns\/01-pytorch-basics)\n2. [Linear Regression & Gradient Descent](https:\/\/jovian.ml\/aakashns\/02-linear-regression)\n3. [Image Classfication using Logistic Regression](https:\/\/jovian.ml\/aakashns\/03-logistic-regression) \n4. [Training Deep Neural Networks on a GPU](https:\/\/jovian.ml\/aakashns\/04-feedforward-nn)\n5. [Image Classification using Convolutional Neural Networks](https:\/\/jovian.ml\/aakashns\/05-cifar10-cnn)\n6. [Data Augmentation, Regularization and ResNets](https:\/\/jovian.ml\/aakashns\/05b-cifar10-resnet)\n7. [Generating Images using Generative Adverserial Networks](https:\/\/jovian.ml\/aakashns\/06-mnist-gan)\n\nIn [the previous tutorial](https:\/\/jovian.ml\/aakashns\/03-logistic-regression), we trained a logistic regression model to identify handwritten digits from the MNIST dataset with an accuracy of around 86%. \n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*q2nyeRU8uvjPeKpo_lMXgQ.jpeg)\n\nHowever, we also noticed that it's quite difficult to improve the accuracy beyond 87%, due to the limited power of the model. In this post, we'll try to improve upon it using a *feedforward neural network*. ","4d9a32e7":"We can now wrap our data loaders using `DeviceDataLoader`.","9e09de53":"## Model\n\nTo improve upon [logistic regression](https:\/\/jvn.io\/aakashns\/a1b40b04f5174a18bd05b17e3dffb0f0), we'll create a neural network with one **hidden layer**. Here's what this means:\n\n* Instead of using a single `nn.Linear` object to transform a batch of inputs (pixel intensities) into a batch of outputs (class probabilities), we'll use two `nn.Linear` objects. Each of these is called a layer in the network. \n\n* The first layer (also known as the hidden layer) will transform the input matrix of shape `batch_size x 784` into an intermediate output matrix of shape `batch_size x hidden_size`, where `hidden_size` is a preconfigured parameter (e.g. 32 or 64).\n\n* The intermediate outputs are then passed into a non-linear *activation function*, which operates on individual elements of the output matrix.\n\n* The result of the activation function, which is also of size `batch_size x hidden_size`, is passed into the second layer (also knowns as the output layer), which transforms it into a matrix of size `batch_size x 10`, identical to the output of the logistic regression model.\n\nIntroducing a hidden layer and an activation function allows the model to learn more complex, multi-layered and non-linear relationships between the inputs and the targets. Here's what it looks like visually:\n\n![](https:\/\/i.imgur.com\/vDOGEkG.png)\n\nThe activation function we'll use here is called a **Rectified Linear Unit** or **ReLU**, and it has a really simple formula: `relu(x) = max(0,x)` i.e. if an element is negative, we replace it by 0, otherwise we leave it unchanged.\n\nTo define the model, we extend the `nn.Module` class, just as we did with logistic regression.","ee109471":"Next, let's define a function that can move data and model to a chosen device.","0fa7c966":"The initial accuracy is around 10%, which is what one might expect from a randomly intialized model (since it has a 1 in 10 chance of getting a label right by guessing randomly).\n\nWe are now ready to train the model. Let's train for 5 epochs and look at the results. We can use a relatively higher learning of 0.5.","e04a5f93":"## Preparing the Data\n\nThe data preparation is identical to the [previous tutorial](https:\/\/jvn.io\/aakashns\/a1b40b04f5174a18bd05b17e3dffb0f0). We begin by importing the required modules & classes.","de5ee5ca":"## Training the Model","3231f44a":"We'll create a model that contains a hidden layer with 32 activations.","5bd172e4":"We can now create PyTorch data loaders for training and validation.","71b8febc":"Our current model outperforms the logistic regression model (which could only reach around 86% accuracy) by a huge margin! It quickly reaches an accuracy of 97%, but doesn't improve much beyond this. To improve the accuracy further, we need to make the model more powerful. As you can probably guess, this can be achieved by increasing the size of the hidden layer, or adding more hidden layers. I encourage you to try out both these approaches and see which one works better.","495a462a":"We download the data and create a PyTorch dataset using the `MNIST` class from `torchvision.datasets`.","b8835436":"We can use the exact same training loops from the logistic regression notebook.","6625b2c8":"We also define an `accuracy` function which calculates the overall accuracy of the model on an entire batch of outputs, so that we can use it as a metric in `fit`.","ab83bfdb":"Can you figure out what the `num_workers` and `pin_memory` are used for? Try looking into the documentation: https:\/\/pytorch.org\/docs\/stable\/data.html\n\nLet's visualize a batch of data in a grid using the `make_grid` function from `torchvision`. We'll also use the `.permute` method on the tensor to move the channels to the last dimension, as expected by `matplotlib`.","cf16902d":"Let's see how the model performs on the validation set with the initial set of weights and biases.","db0acf07":"Before we train the model, we need to ensure that the data and the model's parameters (weights and biases) are on the same device (CPU or GPU). We can reuse the `to_device` function to move the model's parameters to the right device. ","519cf0f0":"Finally, we define a `DeviceDataLoader` class to wrap our existing data loaders and move data to the selected device, as a batches are accessed. Interestingly, we don't need to extend an existing class to create a PyTorch dataloader. All we need is an `__iter__` method to retrieve batches of data, and an `__len__` method to get the number of batches.","91bd7425":"## System Setup\n\nThis tutorial takes a code-first approach, and you should try to follow along by running and experimenting with the code yourself. The easiest way to start executing this notebook is to click the **\"Run\"** button at the top of this page, and select **\"Run on Kaggle\"**. This will run the notebook on Kaggle, a free online service for running Jupyter notebooks (you might need to create an account).\n\n### Running on your computer locally\n*(Skip this if you are running on Kaggle)* You can clone this notebook, install the required dependencies using conda, and start Jupyter by running the following commands on the terminal:\n\n```\npip install jovian --upgrade                # Install the jovian library \njovian clone aakashns\/04-feedforward-nn     # Download notebook\ncd 04-feedforward-nn                        # Enter the created directory \nconda create -n 04-feedfoward-nn python=3.8 # Create a conda environment\nconda activate 04-feedforward-nn            # Activate virtual environment\nconda install jupyter                       # Install Jupyter\njupyter notebook                            # Start Jupyter\n```\n\nOn older versions of conda, you might need to run `source activate 04-feedfoward-nn` to activate the virtual environment. For a more detailed explanation of the above steps, check out the System setup section in [the first notebook](https:\/\/jovian.ml\/aakashns\/01-pytorch-basics).","c3616d0a":"Tensors that have been moved to the GPU's RAM have a `device` property which includes the word `cuda`. Let's verify this by looking at a batch of data from `valid_dl`.","9ddd9f64":"We can now plot the losses & accuracies to study how the model improves over time.","4defd805":"## Commit and upload the notebook\n\nAs a final step, we can save and commit our work using the jovian library.","d70fdae2":"Next, let's use the `random_split` helper function to set aside 10000 images for our validation set.","bc0c42c5":"## Summary and Further Reading\n\nHere is a summary of the topics covered in this tutorial:\n\n* We created a neural network with one hidden layer to improve upon the logistic regression model from the previous tutorial. We also used the ReLU activation function to introduce non-linearity into the model, allowing it to learn more complex relationships between the inputs (pixel densities) and outputs (class probabilities).\n\n* We defined some utilities like `get_default_device`, `to_device` and `DeviceDataLoader` to leverage a GPU if available, by moving the input data and model parameters to the appropriate device.\n\n* We were able to use the exact same training loop: the `fit` function we had define earlier to train out model and evaluate it using the validation dataset.\n\nThere's a lot of scope to experiment here, and I encourage you to use the interactive nature of Jupyter to play around with the various parameters. Here are a few ideas:\n\n* Try changing the size of the hidden layer, or add more hidden layers and see if you can achieve a higher accuracy.\n\n* Try changing the batch size and learning rate to see if you can achieve the same accuracy in fewer epochs.\n\n* Compare the training times on a CPU vs. GPU. Do you see a significant difference. How does it vary with the size of the dataset and the size of the model (no. of weights and parameters)?\n\n* Try building a model for a different dataset, such as the [CIFAR10 or CIFAR100 datasets](https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html).\n\nHere are some references for further reading:\n\n* [A visual proof that neural networks can compute any function](http:\/\/neuralnetworksanddeeplearning.com\/chap4.html), also known as the Universal Approximation Theorem.\n\n* [But what *is* a neural network?](https:\/\/www.youtube.com\/watch?v=aircAruvnKk) - A visual and intuitive introduction to what neural networks are and what the intermediate layers represent\n\n* [Stanford CS229 Lecture notes on Backpropagation](http:\/\/cs229.stanford.edu\/notes\/cs229-notes-backprop.pdf) - for a more mathematical treatment of how gradients are calculated and weights are updated for neural networks with multiple layers.","73ce1913":"96% is pretty good! Let's train the model for 5 more epochs at a lower learning rate of 0.1, to further improve the accuracy.","8293b31a":"Let's define a helper function to ensure that our code uses the GPU if available, and defaults to using the CPU if it isn't.","efef4989":"## Using a GPU\n\nAs the sizes of our models and datasets increase, we need to use GPUs to train our models within a reasonable amount of time. GPUs contain hundreds of cores that are optimized for performing expensive matrix operations on floating point numbers in a short time, which makes them ideal for training deep neural networks with many layers. You can use GPUs for free on [Kaggle kernels](https:\/\/www.kaggle.com\/kernels) or [Google Colab](https:\/\/colab.research.google.com\/), or rent GPU-powered machines on services like [Google Cloud Platform](https:\/\/cloud.google.com\/gpu\/), [Amazon Web Services](https:\/\/docs.aws.amazon.com\/dlami\/latest\/devguide\/gpu.html) or [Paperspace](https:\/\/www.paperspace.com\/).\n\nWe can check if a GPU is available and the required NVIDIA CUDA drivers are installed using `torch.cuda.is_available`.","1282e91a":"Let's try and generate some outputs using our model. We'll take the first batch of 128 images from our dataset, and pass them into our model.","bf7f0684":"Let's take a look at the model's parameters. We expect to see one weight and bias matrix for each of the layers."}}