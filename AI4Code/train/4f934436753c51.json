{"cell_type":{"15e3aa4d":"code","43ccbf25":"code","8e9c6927":"code","7737d38e":"code","f5822e42":"code","caf902e0":"code","813cef7e":"code","67c12dab":"code","fbdea8d4":"code","813a1e5d":"code","f9eb8def":"code","08435e0b":"code","7a2397f8":"code","c9466135":"code","c23acd6e":"code","ef9dfbd4":"code","3dc1bea5":"code","30766559":"code","a016bc54":"code","3b506406":"code","b6ca075f":"code","c99d1c98":"code","d2308e55":"code","574ff5a9":"code","59c691d8":"code","04cdaca5":"code","04ad5281":"code","c6c11a99":"code","f78cc868":"code","a1379a28":"code","d26d1c31":"code","bc3d70ee":"code","43ccdb68":"code","7f57ab53":"code","a4c55495":"code","3820f25e":"code","40cfdb12":"code","dc04a5a0":"code","ac94220f":"code","dd6e46ae":"code","8a936e22":"markdown","742e5233":"markdown","863c5121":"markdown","f1b1d7ac":"markdown","3593bb33":"markdown","9d733848":"markdown","0efb4ecd":"markdown","b8db521c":"markdown","73e5bf79":"markdown","ca6bcc84":"markdown","62b89337":"markdown","2298b3ab":"markdown","27779ee9":"markdown","6868051b":"markdown","21cb8890":"markdown","3455d26b":"markdown","eba0fbeb":"markdown"},"source":{"15e3aa4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","43ccbf25":"import pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n#load the data\ndf=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')","8e9c6927":"#first five rows\ndf.head()","7737d38e":"#dataframe info\ndf.info()","f5822e42":"#Dataframe describe\ndf.describe()","caf902e0":"#create a new dataframe with most important columns for us\ndf=df[['publish_time','authors','title','abstract']]\ndf.head()","813cef7e":"df.info()","67c12dab":"#check the total null cell for the column of abstract\ndf['abstract'].isnull().sum()","fbdea8d4":"#delete rows where  abstract are null\ndf.dropna(subset=['abstract'], inplace=True)\ndf.info()","813a1e5d":"#Fetch word count for each abstract\ndf['word_count'] = df['abstract'].apply(lambda x: len(str(x).split(\" \")))\ndf.head()","f9eb8def":"#Descriptive statistics of word counts\ndf.describe()","08435e0b":"#Identify common words (20 top words)\nfreq = pd.Series(' '.join(df['abstract']).split()).value_counts()[:20]\nfreq","7a2397f8":"#plot the most 20 common words\nfreq.plot()","c9466135":"#Identify uncommon words (top 20)\nfreq1 =  pd.Series(' '.join(df['abstract']).split()).value_counts()[-20:]\nfreq1","c23acd6e":"#Import the required libraries for the text processing\nimport re\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nnltk.download('wordnet') \nfrom nltk.stem.wordnet import WordNetLemmatizer","ef9dfbd4":"#Removing stopwords\n    ##Creating a list of stop words and adding custom stopwords\nstop_words = set(stopwords.words(\"english\"))\n\n    ##Creating a list of custom stopwords (all other words you want to remove from the text)\nnew_words = [\"using\", \"show\", \"result\", \"also\", \"iv\", \"one\", 'however',\"two\", \"new\", \"previously\", \"shown\"]\nstop_words = stop_words.union(new_words)","3dc1bea5":"#carry out the pre-processing tasks step-by-step to get a cleaned and normalised text corpus:\ncorpus = []\nfor i in list(df.index.values): # list of index of the dataframe [0,1,2......]'\n    #Remove punctuations\n    text = re.sub('[^a-zA-Z]', ' ', df['abstract'][i])\n    #Convert to lowercase\n    text = text.lower()\n    #remove tags\n    text=re.sub(\"&lt;\/?.*?&gt;\",\" &lt;&gt; \",text)\n    #remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    #Convert to list from string\n    text = text.split()\n    #Stemming\n    ps=PorterStemmer()\n    #Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  \n            stop_words] \n    text = \" \".join(text)\n    corpus.append(text)","30766559":"#View corpus item\ncorpus[1000]","a016bc54":"#Word cloud: Vizualize the corpus (frequency or the importance of each word)\n#from os import path\n#from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%matplotlib inline\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stop_words,\n                          max_words=100,\n                          max_font_size=70, \n                          random_state=42\n                         ).generate(str(corpus))\nprint(wordcloud)\nfig = plt.figure(1,figsize=(20,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","3b506406":"#Identify common words in the corpus  (20 top words)\nfreq = pd.Series(' '.join(corpus).split()).value_counts()[:20]\nfreq","b6ca075f":"#plot the result (top 20 words in the corpus)\n#Convert most freq words to dataframe for plotting bar plot\ntop_words = pd.Series(' '.join(corpus).split()).value_counts()[:20]\ntop_df = pd.DataFrame(top_words).reset_index()\ntop_df.columns=[\"Word\", \"Freq\"]\n\n#Barplot of most freq words\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\ng.set_title('Top 20 words in the corpus')\ng.set_xticklabels(g.get_xticklabels(), rotation=30)\n","c99d1c98":"#Creating a vector of word counts\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\ncv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\nX=cv.fit_transform(corpus)","d2308e55":"#shape of X\nX.shape","574ff5a9":"#print a list of 10 vocabulary from the list of vocabulary\nlist(cv.vocabulary_.keys())[:10]","59c691d8":"#Uni-grams\n    #Most frequently occuring words\ndef get_top_unigram_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                       reverse=True)\n    return words_freq[:n]\n    #Convert most freq words to dataframe for plotting bar plot\ntop_words = get_top_unigram_words(corpus, n=20)\ntop_df = pd.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]\n    #Barplot of most freq words\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\ng.set_title('Top 20 Uni_grams')\ng.set_xticklabels(g.get_xticklabels(), rotation=30)","04cdaca5":"#Bi_grams\n    #Most frequently occuring Bi-grams\ndef get_top_bi_grams_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(2,2), max_features=4000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop2_words = get_top_bi_grams_words(corpus, n=20)\ntop2_df = pd.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\n\n    #Barplot of most freq Bi-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\nh.set_title('Top 20 Bi_grams')\nh.set_xticklabels(h.get_xticklabels(), rotation=45)","04ad5281":"#Tri_Grams\n    #Most frequently occuring Tri-grams\ndef get_top_n3_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(3,3), max_features=4000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop3_words = get_top_n3_words(corpus, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Tri-gram\", \"Freq\"]\nprint(top3_df)\n    #Barplot of most freq Tri-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nj=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\nj.set_title('Top 20 Tri_grams')\nj.set_xticklabels(j.get_xticklabels(), rotation=45)","c6c11a99":"#4_Grams\n    #Most frequently occuring 4-grams\ndef get_top_n4_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(3,3), max_features=4000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop3_words = get_top_n4_words(corpus, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"4-gram\", \"Freq\"]\nprint(top3_df)\n    #Barplot of most freq Tri-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nl=sns.barplot(x=\"4-gram\", y=\"Freq\", data=top3_df)\nl.set_title('Top 20 4_grams')\nl.set_xticklabels(j.get_xticklabels(), rotation=45)","f78cc868":"#Converting to a matrix of integers\nfrom sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(X)\n\n# get feature names\nfeature_names=cv.get_feature_names()","a1379a28":"# Define Function for sorting tf_idf in descending order\n\nfrom scipy.sparse import coo_matrix\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n \ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n \n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","d26d1c31":"#Extract the keywords for the abstract number 304 (1)\nabstract_335=corpus[335]\n    #generate tf-idf for the given document\ntf_idf_vector_abstract_335=tfidf_transformer.transform(cv.transform([abstract_335]))\n#sort the tf-idf vectors by descending order of scores\n\nsorted_items=sort_coo(tf_idf_vector_abstract_335.tocoo())\n\n#extract only the top n; n here is 5\nkeywords=extract_topn_from_vector(feature_names,sorted_items,5)\n    \n \n# now print the results\nprint(\"\\nAbstract 335:\")\nprint(abstract_335)\nprint(\"\\nKeywords:\")\nfor k in keywords:\n    print(k,keywords[k])","bc3d70ee":"#sort the tf-idf vectors by descending order of scores\ntf_idf_vector_corpus=tfidf_transformer.transform(cv.transform(corpus))\nkeywords=[]\nfor b in tf_idf_vector_corpus:\n    sorted_items=sort_coo(b.tocoo())\n    keywords.append(extract_topn_from_vector(feature_names,sorted_items,5))","43ccdb68":"#add the keywords for each abstract in the Dataframe\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_columns', None)\ndf['keywords']=keywords\ndf1=df.drop(columns='word_count', axis=1)\ndf1.head()","7f57ab53":"#Use the the algorith MinisBatch as a Classifier\n    #Import the required libraries\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans","a4c55495":"#predict the cluster\nX1=tf_idf_vector_corpus\n\n#Make the prediction for 10 clusters\nk = 10\n\nkmeans = MiniBatchKMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(X1)\ny=y_pred","3820f25e":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\npca_result = pca.fit_transform(X1.toarray())","40cfdb12":"#Vizualize the clusters\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n# colors\npalette = sns.color_palette(\"bright\", len(set(y)))\n# plot\nsns.scatterplot(pca_result[:,0], pca_result[:,1], hue=y, legend='full', palette=palette)\nplt.title(\"Covid-19 Abstracts - Clustered (K-Means)\")\nplt.show()","dc04a5a0":"#vizualize in 3D\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nax = plt.figure(figsize=(16,10)).gca(projection='3d')\nax.scatter(\n    xs=pca_result[:,0], \n    ys=pca_result[:,1], \n    zs=pca_result[:,2], \n    c=y, \n    cmap='tab10'\n)\nax.set_xlabel('PCA_1')\nax.set_ylabel('PCA_2')\nax.set_zlabel('PCA_3')\nplt.title(\"Covid-19 Abstracts - Clustered (K-Means)\")\nplt.show()","ac94220f":"df1['cluster']=y\ndf1.head()","dd6e46ae":"#Generate the size of each cluster\ndf1.groupby('cluster').apply(len)","8a936e22":"Extract keywords for the corpus (each abstract) and add them to the dataframe (colunm=keywords)","742e5233":"# **CORPUS CLUSTERING**","863c5121":"Explore and visualize the corpus","f1b1d7ac":"**Normalize the data**: stemming and lemmatization","3593bb33":"Visualize top N uni-grams, bi-grams, tri-grams and 4-grams","9d733848":"    -Vectorization\nAs the first step of conversion, we will use the CountVectoriser to tokenise the text and build a vocabulary of known words. We first create a variable \u201ccv\u201d of the CountVectoriser class, and then evoke the fit_transform function to learn and build the vocabulary.\n","0efb4ecd":"#Create and explore a new dataframe with less columns","b8db521c":"# EXPLORE AND MANIPULATE THE DATA","73e5bf79":"**Example****: Extract the keywords for the abstract number 304 in the corpus","ca6bcc84":"# **KEYWORDS EXTRACTION FOR EACH ABSTRACT OF THE CORPUS**","62b89337":"    Visualize the clusters\nUse the Principal component analysis (PCA) to decompoze the data in project it to a lower dimensional space.","2298b3ab":"Text in the corpus needs to be converted to a format that can be interpreted by the machine learning algorithms. There are 2 parts of this conversion \u2014 Tokenisation and Vectorisation.\n\nTokenisation is the process of converting the continuous text into a list of words. The list of words is then converted to a matrix of integers by the process of vectorisation. Vectorisation is also called feature extraction.\n\nFor text preparation we use the bag of words model which ignores the sequence of the words and only considers word frequencies.","27779ee9":"# TEXT PREPARATION","6868051b":"# TEXT PRE-PROCESSING\n     # Steps:\n- Text clean up\n- Shrinking the vocabulary to retain only the relevant\/important words\n- Reduce sparsity  ","21cb8890":"Explore the text in the colunm abstract","3455d26b":"We got a pretty good results!!","eba0fbeb":"Generate the cluster of each abstract in the the DataFrame"}}