{"cell_type":{"72653c39":"code","8fe047df":"code","fb461a02":"code","66520c9f":"code","ef6c4f8d":"code","5907c49a":"code","ead58ce7":"code","62aed2bd":"code","0eb6554a":"code","368e56a4":"code","f3c3678d":"code","076dd392":"code","05ca6d3f":"code","0f072348":"code","71973449":"code","536b4e95":"code","a48fa51b":"code","367a3e08":"code","b4e2b214":"code","445acf8e":"code","808cba64":"code","fa4341c8":"code","21a155ed":"code","f961e406":"code","170b5f49":"code","0ca6c762":"code","960db532":"code","86ce85cf":"code","82558cc2":"markdown","2633546c":"markdown","ee4dae59":"markdown","e23b7ce9":"markdown","6c72a6d2":"markdown","a9e418e5":"markdown","9a6febe7":"markdown","68a0576c":"markdown","9f091a9d":"markdown","6ebc5885":"markdown","6be1e54a":"markdown","63e481f0":"markdown","e280fbcc":"markdown","162f57f5":"markdown","755c44cb":"markdown","6c11db8c":"markdown","c47f2f5f":"markdown","0eb945e4":"markdown","55e4a68a":"markdown","a994ecd1":"markdown","692ad807":"markdown","2136664a":"markdown","e6d9497d":"markdown","9f7df3d7":"markdown","1f3ee873":"markdown","b031f0b0":"markdown","90ab2710":"markdown"},"source":{"72653c39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8fe047df":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","fb461a02":"df=pd.read_csv('..\/input\/crop-recommendation-dataset\/Crop_recommendation.csv')\ndf.head()","66520c9f":"df.describe()","ef6c4f8d":"sns.heatmap(df.isnull(),cmap=\"coolwarm\")\nplt.show()","5907c49a":"plt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\n# sns.distplot(df_setosa['sepal_length'],kde=True,color='green',bins=20,hist_kws={'alpha':0.3})\nsns.distplot(df['temperature'],color=\"purple\",bins=15,hist_kws={'alpha':0.2})\nplt.subplot(1, 2, 2)\nsns.distplot(df['ph'],color=\"green\",bins=15,hist_kws={'alpha':0.2})","ead58ce7":"sns.countplot(y='label',data=df, palette=\"plasma_r\")","62aed2bd":"sns.pairplot(df, hue = 'label')","0eb6554a":"sns.jointplot(x=\"rainfall\",y=\"humidity\",data=df[(df['temperature']<30) & (df['rainfall']>120)],hue=\"label\")","368e56a4":"sns.jointplot(x=\"K\",y=\"N\",data=df[(df['N']>40)&(df['K']>40)],hue=\"label\")","f3c3678d":"sns.jointplot(x=\"K\",y=\"humidity\",data=df,hue='label',size=8,s=30,alpha=0.7)","076dd392":"sns.boxplot(y='label',x='ph',data=df)","05ca6d3f":"sns.boxplot(y='label',x='P',data=df[df['rainfall']>150])","0f072348":"sns.lineplot(data = df[(df['humidity']<65)], x = \"K\", y = \"rainfall\",hue=\"label\")","71973449":"c=df.label.astype('category')\ntargets = dict(enumerate(c.cat.categories))\ndf['target']=c.cat.codes\n\ny=df.target\nX=df[['N','P','K','temperature','humidity','ph','rainfall']]","536b4e95":"sns.heatmap(X.corr())","a48fa51b":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state=1)\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n\n# we must apply the scaling to the test set as well that we are computing for the training set\nX_test_scaled = scaler.transform(X_test)","367a3e08":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train_scaled, y_train)\nknn.score(X_test_scaled, y_test)","b4e2b214":"from sklearn.metrics import confusion_matrix\nmat=confusion_matrix(y_test,knn.predict(X_test_scaled))\ndf_cm = pd.DataFrame(mat, list(targets.values()), list(targets.values()))\nsns.set(font_scale=1.0) # for label size\nplt.figure(figsize = (12,8))\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 12},cmap=\"terrain\")","445acf8e":"k_range = range(1,11)\nscores = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train_scaled, y_train)\n    scores.append(knn.score(X_test_scaled, y_test))\n\nplt.xlabel('k')\nplt.ylabel('accuracy')\nplt.scatter(k_range, scores)\nplt.vlines(k_range,0, scores, linestyle=\"dashed\")\nplt.ylim(0.96,0.99)\nplt.xticks([i for i in range(1,11)]);","808cba64":"from sklearn.svm import SVC\n\nsvc_linear = SVC(kernel = 'linear').fit(X_train_scaled, y_train)\nprint(\"Linear Kernel Accuracy: \",svc_linear.score(X_test_scaled,y_test))\n\nsvc_poly = SVC(kernel = 'rbf').fit(X_train_scaled, y_train)\nprint(\"Rbf Kernel Accuracy: \", svc_poly.score(X_test_scaled,y_test))\n\nsvc_poly = SVC(kernel = 'poly').fit(X_train_scaled, y_train)\nprint(\"Poly Kernel Accuracy: \", svc_poly.score(X_test_scaled,y_test))","fa4341c8":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'C': np.logspace(-3, 2, 6).tolist(), 'gamma': np.logspace(-3, 2, 6).tolist()}\n# 'degree': np.arange(0,5,1).tolist(), 'kernel':['linear','rbf','poly']\n\nmodel = GridSearchCV(estimator = SVC(kernel=\"linear\"), param_grid=parameters, n_jobs=-1, cv=4)\nmodel.fit(X_train, y_train)","21a155ed":"print(model.best_score_ )\nprint(model.best_params_ )","f961e406":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)\nclf.score(X_test,y_test)","170b5f49":"plt.figure(figsize=(10,4), dpi=80)\nc_features = len(X_train.columns)\nplt.barh(range(c_features), clf.feature_importances_)\nplt.xlabel(\"Feature importance\")\nplt.ylabel(\"Feature name\")\nplt.yticks(np.arange(c_features), X_train.columns)\nplt.show()","0ca6c762":"'''\nmax depth and n_estimator are important to fine tune otherwise trees will be densely graphed which will be a classic case of overfitting. max_depth=4 and n_estimators=10 gives pretty much satisfying results by making sure model is able to generalize well.\n'''\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=4,n_estimators=100,random_state=42).fit(X_train, y_train)\n\nprint('RF Accuracy on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('RF Accuracy on test set: {:.2f}'.format(clf.score(X_test, y_test)))","960db532":"from yellowbrick.classifier import ClassificationReport\nclasses=list(targets.values())\nvisualizer = ClassificationReport(clf, classes=classes, support=True,cmap=\"Blues\")\n\nvisualizer.fit(X_train, y_train)  # Fit the visualizer and the model\nvisualizer.score(X_test, y_test)  # Evaluate the model on the test data\nvisualizer.show()","86ce85cf":"from sklearn.ensemble import GradientBoostingClassifier\ngrad = GradientBoostingClassifier().fit(X_train, y_train)\nprint('Gradient Boosting accuracy : {}'.format(grad.score(X_test,y_test)))","82558cc2":"### Let's try different values of n_neighbors to fine tune and get better results","2633546c":"## Classification using Gradient Boosting\n<hr>","ee4dae59":"#### Another interesting analysis where Phosphorous levels are quite differentiable when it rains heavily (above 150 mm).","e23b7ce9":"### <div style=\"color:blue;\"><b> Classification report <\/b><\/div>\n\n#### **Let's use <u>yellowbrick<\/u> for classification report as they are great for visualizing in a tabular format**","6c72a6d2":"<h4> <u>Let's have a closer look at the distribution of temperature and ph.<\/u><br><br>\n    \nIt is symmetrical and bell shaped, showing that trials will usually give a result near the average, but will occasionally deviate by large amounts. It's also fascinating how these two really resemble each other!<\/h4>","a9e418e5":"<h4> A very important plot to visualize the diagonal distribution between two features for all the combinations! It is great to visualize how classes differ from each other in a particular space.","9a6febe7":"### Confusion Matrix","68a0576c":"### Let's visualize the import features which are taken into consideration by decision trees.","9f091a9d":"\n<h2><center> If you learnt something new or liked this kernel, please consider upvoting. Happy Kaggling.<\/center><\/h2>","6ebc5885":"# FEATURE SCALING\n**Feature scaling is required before creating training data and feeding it to the model.**\n\nAs we saw earlier, two of our features (temperature and ph) are gaussian distributed, therefore scaling them between 0 and 1 with MinMaxScaler.","6be1e54a":"# MODEL SELECTION\n\n## KNN Classifier for Crop prediction. \n<hr>","63e481f0":"**Correlation visualization between features. We can see how Phosphorous levels and Potassium levels are highly correlated.**","e280fbcc":"#### During rainy season, average rainfall is high (average 120 mm) and temperature is mildly chill (less than 30'C).\n\n#### Rain affects soil moisture which affects ph of the soil. Here are the crops which are likely to be planted during this season. \n\n- <b> Rice needs heavy rainfall (>200 mm) and a humidity above 80%. No wonder major rice production in India comes from East Coasts which has average of 220 mm rainfall every year!\n- <b> Coconut is a tropical crop and needs high humidity therefore explaining massive exports from coastal areas around the country.","162f57f5":"#### We can see ph values are critical when it comes to soil. A stability between 6 and 7 is preffered","755c44cb":"#### This graph correlates with average potassium (K) and average nitrogen (N) value (both>50). \n#### These soil ingredients direcly affects nutrition value of the food. Fruits which have high nutrients typically has consistent potassium values.","6c11db8c":"<h4>Let's try to plot a specfic case of pairplot between `humidity` and `K` (potassium levels in the soil.)<\/h4>\n\n#### `sns.jointplot()` can be used for bivariate analysis to plot between humidity and K levels based on Label type. It further generates frequency distribution of classes with respect to features","c47f2f5f":"### Let's try to increase SVC Linear model accuracy by parameter tuning.\n\n**GridSearchCV can help us find the best parameters.**","0eb945e4":"# Exploratory Data Analysis\n\n### Heatmap to check null\/missing values","55e4a68a":"# DATA PRE-PROCESSING\n\n### Let's make the data ready for machine learning model","a994ecd1":"# <center>CROP RECOMMENDATION USING WEATHER AND SOIL CONTENT<\/center>\n<center><img src= \"https:\/\/media.nationalgeographic.org\/assets\/photos\/120\/983\/091a0e2f-b93d-481b-9a60-db520c87ec33.jpg\" alt =\"Titanic\" style='width:500px;'><\/center><br>\n","692ad807":"#### Further analyzing phosphorous levels.\n\nWhen humidity is less than 65, almost same phosphor levels(approx 14 to 25) are required for 6 crops which could be grown just based on the amount of rain expected over the next few weeks.","2136664a":"## Classification using Random Forest.\n<hr>","e6d9497d":"## Classification using Support Vector Classifer (SVC)\n<hr>","9f7df3d7":"- <h3> Machine Learning is well equipped when it comes to analyzing data regarding soil conditions, including moisture level, temperature, and chemical makeup, all of which have an impact upon crop growth and livestock well-being.<br>\n- <h3> Today in agriculture, this can allow crops to be grown at much higher precision, enabling farmers to treat plants and animals almost individually, which in turn significantly increases the effectiveness of farmers' decisions.<br>\n- <h3> Using this can develop means to even predict harvest yields and evaluate crop quality for individual plant species to detect crop disease and weed infestations which were previouly impossible!","1f3ee873":"<h4> A quick check if the dataset is balanced or not. If found imbalanced, we would have to downsample some targets which are more in quantity but so far everything looks good! <h4>","b031f0b0":"## Classifying using decision tree\n<hr>","90ab2710":"**POINTS TO BE HIGHLIGHTED**\n1. *Interestingly liner kernel also gives satisfactory results but fine tuning increases the computation and might be inefficient in some cases*\n2. *The accuracy can be increased in poly kernel by tweaking parameters but might lead to intensive overfitting.*\n3. *RBF has better result than linear kernel.*\n4. *Poly kernel so far wins by a small margin.*"}}