{"cell_type":{"238baa4e":"code","d1aa6683":"code","39f2f6b1":"code","131ea20c":"code","cb8e8ceb":"code","ef08fff8":"code","abfa3430":"code","802da8b2":"code","8416b5cf":"code","7bc571e8":"code","783b15fc":"code","a176533c":"code","5cd20456":"code","5bebbcd6":"code","2ace37d2":"code","e6f17c3e":"code","512e62d2":"code","b6879e69":"code","09a391d8":"code","364b4cd1":"code","e4217372":"code","10bc30e1":"code","65404313":"markdown","4f0d534f":"markdown","b5476c7f":"markdown","714e673e":"markdown","3b2fe175":"markdown","63cda55a":"markdown","19f28081":"markdown","92e4623f":"markdown","fffecc4e":"markdown","be497fd7":"markdown","0039f83b":"markdown","5646f430":"markdown","e5c54f22":"markdown","4e83c9c9":"markdown","ff2f3430":"markdown","a638d377":"markdown","b3267289":"markdown","85cb3360":"markdown","2fea85d0":"markdown","be674743":"markdown","2bd09343":"markdown","a2e6b6f5":"markdown","4ee38287":"markdown","dd4d986a":"markdown","42d03209":"markdown","9683ab69":"markdown","f28deafc":"markdown"},"source":{"238baa4e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import ElasticNet, Lasso,LinearRegression,RidgeCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d1aa6683":"train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ntest  = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\nsub = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","39f2f6b1":"train.head(5)","131ea20c":"test.head()","cb8e8ceb":"categorical_cols=['cat'+str(i) for i in range(10)]\ncontinous_cols=['cont'+str(i) for i in range(14)]\ncolumns = categorical_cols+continous_cols","ef08fff8":"for e in categorical_cols:\n    le = LabelEncoder()\n    train[e]=le.fit_transform(train[e])\n    test[e]=le.transform(test[e])","abfa3430":"params_xgb = {'lambda': 0.7044156083795233, 'alpha': 9.681476940192473, 'colsample_bytree': 0.3, 'subsample': 0.8,\n           'learning_rate': 0.015, 'max_depth': 3, 'min_child_weight': 235,'random_state': 48,'n_estimators': 30000}\n\nparams_lgb = {'reg_alpha': 4.973064761998367, 'reg_lambda': 0.06365096912006087,'colsample_bytree': 0.24,\n              'subsample': 0.8, 'learning_rate': 0.015, 'max_depth': 100, 'num_leaves': 43,'min_child_samples': 141,\n              'cat_smooth': 18,'metric': 'rmse', 'random_state': 48,'n_estimators': 40000}","802da8b2":"pred1 = np.zeros(train.shape[0])\npred2 = np.zeros(train.shape[0])\npred3 = np.zeros(train.shape[0])\npred4 = np.zeros(train.shape[0])\n\ntest1 = np.zeros(test.shape[0])\ntest2 = np.zeros(test.shape[0])\ntest3 = np.zeros(test.shape[0])\ntest4 = np.zeros(test.shape[0])\n\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\nn=0\n\nfor trn_idx, test_idx in kf.split(train[columns],train['target']):\n    print(f\"fold: {n+1}\")\n    X_tr,X_val=train[columns].iloc[trn_idx],train[columns].iloc[test_idx]\n    y_tr,y_val=train['target'].iloc[trn_idx],train['target'].iloc[test_idx]\n    \n    \n    model1 = lgb.LGBMRegressor(**params_lgb)\n    model1.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=200,verbose=False)\n    pred1[test_idx] = model1.predict(X_val)\n    test1 += model1.predict(test[columns])\/kf.n_splits\n    rmse1 = mean_squared_error(y_val, model1.predict(X_val), squared=False)\n    print(\": model1 rmse = {}\".format(rmse1))\n\n    model2 = ElasticNet(alpha=0.00001)\n    model2.fit(X_tr,y_tr)\n    pred2[test_idx] = model2.predict(X_val)\n    test2 += model2.predict(test[columns])\/kf.n_splits\n    rmse2 = mean_squared_error(y_val, model2.predict(X_val), squared=False)\n    print(\": model2 rmse = {}\".format(rmse2))\n    \n    model3 = LinearRegression()\n    model3.fit(X_tr,y_tr)\n    pred3[test_idx] = model3.predict(X_val)\n    test3 += model3.predict(test[columns])\/kf.n_splits\n    rmse3 = mean_squared_error(y_val, model3.predict(X_val), squared=False)\n    print(\": model3 rmse = {}\".format(rmse3))\n    \n    model4 = xgb.XGBRegressor(**params_xgb)\n    model4.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=200,verbose=False)\n    pred4[test_idx] = model4.predict(X_val)\n    test4 += model4.predict(test[columns])\/kf.n_splits\n    rmse4 = mean_squared_error(y_val, model4.predict(X_val), squared=False)\n    print(\": model4 rmse = {}\".format(rmse4))\n    print(\": average all models rmse = {}\".format((rmse1+rmse2+rmse3+rmse4)\/4))\n\n    n+=1","8416b5cf":"stacked_predictions = np.column_stack((pred1,pred2,pred3,pred4))\nstacked_test_predictions = np.column_stack((test1,test2,test3,test4))","7bc571e8":"l1_train = pd.DataFrame(data={\n    \"lgbm\": pred1.tolist(),\n    \"ElasticNet\": pred2.tolist(),\n    \"LinearRegression\": pred3.tolist(),\n    \"xgb\": pred4.tolist(),\n    \"target\":train.target\n    })\nl1_test = pd.DataFrame(data={\n    \"lgbm\": test1.tolist(),\n    \"ElasticNet\": test2.tolist(),\n    \"LinearRegression\": test3.tolist(),\n    \"xgb\": test4.tolist()\n    })","783b15fc":"l1_train","a176533c":"plt.figure()\nplt.figure(figsize=(20,10),dpi=180)\nplt.plot(pred1[:100], 'gd', label='lgbm')\nplt.plot(pred2[:100], 'c^', label='ElasticNet')\nplt.plot(pred3[:100], 'k+', label='LinearRegression')\nplt.plot(pred4[:100], 'y*', label='xgb')\nplt.plot(train.target[:100], 'ro', label='target')\nplt.tick_params(axis='x', which='both', bottom=False, top=False,\n                labelbottom=False)\nplt.ylabel('predicted')\nplt.xlabel('training samples')\nplt.legend(loc=\"best\")\nplt.title('Regressor predictions and their average')\n\nplt.show()\n","5cd20456":"sns.set(rc={\"figure.figsize\":(16, 8)})\nsns.boxplot(data=l1_train,whis=1,linewidth=3,orient=\"h\",palette=\"Set1\")","5bebbcd6":"sns.displot(l1_train,kind=\"hist\",aspect=3)","2ace37d2":"sns.displot(l1_train,kind=\"kde\",aspect=3)","e6f17c3e":"i = 1\nplt.figure()\nfig, ax = plt.subplots(2, 2,figsize=(18, 12))\nfor feature in l1_train.columns[:4]:\n    plt.subplot(2, 2,i)\n    sns.histplot(l1_train[feature],color=\"blue\", kde=True,bins=100, label=str(feature))\n    sns.histplot(l1_train['target'],color=\"olive\", kde=True,bins=100, label='target')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","512e62d2":"params = {'lambda': 0.005719655325051427, 'alpha': 1.239701267723972, 'colsample_bytree': 0.24, 'subsample': 0.4,\n          'learning_rate': 0.01, 'max_depth': 2, 'min_child_weight': 198,'random_state': 48,'n_estimators': 30000}","b6879e69":"kf = KFold(n_splits=5,random_state=48,shuffle=True)\nfinal_prediction = np.zeros(test.shape[0])\nrmse=[]  # list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(stacked_predictions,train['target']):\n    X_tr,X_val=stacked_predictions[trn_idx],stacked_predictions[test_idx]\n    y_tr,y_val=train['target'].iloc[trn_idx],train['target'].iloc[test_idx]\n    \n    meta_model = Lasso(alpha =0.00001)\n    meta_model.fit(X_tr,y_tr)\n    \n    final_prediction +=meta_model.predict(stacked_test_predictions)\/kf.n_splits\n    rmse.append(mean_squared_error(y_val, meta_model.predict(X_val), squared=False))\n    print(f\"fold: {n+1}, rmse: {rmse[n]}\")\n    n+=1","09a391d8":"np.mean(rmse)","364b4cd1":"sub['target']=final_prediction\nsub.to_csv('submission.csv', index=False)","e4217372":"l1_train.to_csv('l1_train.csv', index=False)\nl1_test.to_csv('l1_test.csv', index=False)","10bc30e1":"sub","65404313":"## <h2 style=\"color:blue;\">4.3 Label Encoding for categorical features<\/h2>","4f0d534f":"## <h1 style=\"color:blue;\">2. The model stacking process<\/h1>\n\n####  Model stacking seems like a simple technique to improve your results when you understand what happens inside the algorithm. However, there are many components interacting and keeping track of all of them can be challenging, especially when first learning this concept.\n####  To start, when performing model stacking with cross-validation, we require three parameters: a Training data set, a Holdout data set (validation data set), and a list of models called models_to_train.\n####  The following picture discribes the processs\n","b5476c7f":"## <h1 style=\"color:blue;\">3. Layers and Meta-learner<\/h1>","714e673e":"## <h1 style=\"color:blue;\">4. Implementation Part<\/h1>","3b2fe175":"<h4>References:<\/h4>\n\n1. [https:\/\/developer.ibm.com\/articles\/stack-machine-learning-models-get-better-results\/](https:\/\/developer.ibm.com\/articles\/stack-machine-learning-models-get-better-results\/) <br>\n2. [https:\/\/www.kaggle.com\/craigmthomas\/tps-mar-2021-stacked-starter](https:\/\/www.kaggle.com\/craigmthomas\/tps-mar-2021-stacked-starter) <br>\n3. Also this is the [link](https:\/\/www.coursera.org\/learn\/competitive-data-science) to an awsome course of \"How to Win a Data Science Competition: Learn from Top Kagglers\" <br> from Coursera where the first time I learned Stacking.\n","63cda55a":"## <h2 style=\"color:blue;\">4.4 Level 1 (train different ML algorithms)<\/h2>","19f28081":"<div style=\"width:800px; margin:0 auto;\">\n<img src=\"https:\/\/developer.ibm.com\/developer\/default\/articles\/stack-machine-learning-models-get-better-results\/images\/model_stacking_new_diagram.png\" \/>\n<\/div>","92e4623f":"# I hope that you find this kernel usefull\ud83c\udfc4","fffecc4e":"* I used Lasso model from sklearn as the meta-model.","be497fd7":"## <h2 style=\"color:blue;\">4.6 Let's make a submission<\/h2>","0039f83b":"![img2](https:\/\/miro.medium.com\/max\/1352\/1*iCFqUkCpYN-X8DUPafVvSw.png)","5646f430":"## <h2 style=\"color:blue;\">4.2 Some EDA<\/h2>","e5c54f22":"<h2 style=\"color:green;\">Here the explanation of the picture:<\/h2>\n\n<h4>1- Gather models with optimized hyperparameters into a models_to_train array.<\/h4>\n<h4>2- Split the original data set into a Training and Holdout data set.<\/h4>\n<h4>3- Let Training go onward into the upcoming loop, and save Holdout until the last part in the upcoming loop.<\/h4>\n<h4>4- Make a for loop with KFold Cross-Validation where k=5.<\/h4>\n<h5 style=\"margin-left: 1.5em\">4.1 In each iteration, split the Training data set into another training and testing data set. Call them X_train, y_train, X_test, and y_test. The white parts in the last picture represent X_test and y_test, while the green parts represent X_train and y_train.<\/h5> \n<h5 style=\"margin-left: 1.5em\">4.2 Set the current model equal to models_to_train[k-1].<\/h5>\n<h5 style=\"margin-left: 1.5em\">4.3 Train the current model on X_train and y_train.<\/h5>\n<h5 style=\"margin-left: 1.5em\">4.4 Make predictions on the test data set X_test and call them y_test_pred. Extend an array full_y_pred with the predictions y_test_pred.<h5>\n<h5 style=\"margin-left: 1.5em\">4.5 Make predictions on the Holdout data set Holdout and call them holdout_pred.<\/h5>\n<h5 style=\"margin-left: 1.5em\">4.6 Add full_y_pred as a new feature in Training and add full_holdout_pred as a new feature in Holdout for use in the next layer.<\/h5>\n<h4>5- Average the holdout_pred arrays into a full_holdout_pred array.<\/h4>\n<h4>6- Add full_y_pred as a new feature in Training and add full_holdout_pred as a new feature in Holdout.<\/h4>\n<h4>7- Return the data sets Training and Holdout with the new features for use in the next layer.<\/h4>","4e83c9c9":"## <h2 style=\"color:blue;\">4.1 Import Libraries<\/h2>","ff2f3430":"<h4>Notes:<\/h4> \n* You can add another models to the first layers <br>\n* You can use more layers <br>\n* You can try to focus on improving each single model by doing hyperparameters Tunning or try better feature engineering techniques ==> better stacking result <br>\n* Try to change the meta-model","a638d377":"## <h1 style=\"color:blue;\">1. What is model stacking?<\/h1>\n\n\n####   Stacking is an efficient ensemble method in which the predictions, generated by using various machine learning algorithms, are used as inputs in a second-layer learning algorithm. This second-layer algorithm is trained to optimally combine the model predictions to form a new set of predictions.","b3267289":"<h4>Stacking machine learning models is done in <b style=\"color:blue;\">layers<\/b>, and there can be many arbitrary layers, dependent on exactly how many models you have trained along with the best combination of these models. For example, the first layer might be learning some feature that has great predictive power while the next layer might be doing something else such as reducing noise.<\/h4>\n<h4>We put model stacks in <b style=\"color:blue;\">layers<\/b>, and usually with a different purpose. At the end, we get a final data set that we feed into a last model. The last model is called a <b style=\"color:blue;\">meta-learner<\/b> (for example, meta-regressor or meta-classifier), and its purpose is to generalize all of the features from each layer into the final predictions.<\/h4>","85cb3360":"<div style=\"width:700px; margin:0 auto;\">\n<img src=\"https:\/\/media2.giphy.com\/media\/flbcUFdLSHwZC03p11\/200w.webp?cid=ecf05e47q1mt7k8sj3msbtpesuarrphwj0wj26w5b2p788kp&rid=200w.webp&ct=g\" width=\"400px\"\/>\n<\/div>","2fea85d0":"<h4>The following picture represent the stacking process with 3 levels.<\/h4>\n<h4>Also note that The <b style=\"color:blue;\">meta-learner<\/b> is always present in the <b style=\"color:red;\">last layer<\/b>.<\/h4>\n\n![img4](https:\/\/developer.ibm.com\/developer\/default\/articles\/stack-machine-learning-models-get-better-results\/images\/model_stacking_3_levels.png)","be674743":"<h4 style=\"color:green;\"> Notice that we use one model to predict each part of the total training data set. We predict one fold at a time, with a different model each time.<\/h4>\n\n<h4 style=\"color:green;\"> Now that we have some stacked machine learning models, how can we use them? The next steps takes the new Training and Holdout data sets and uses them as input in the next layer.<\/h4>","2bd09343":"### Let's see the performance of our models aginst the target by doing some visualization","a2e6b6f5":"* Let's check the prediction of each models for the first 100 samples","4ee38287":"<div style=\"width:700px; margin:0 auto;\">\n<img src=\"https:\/\/media0.giphy.com\/media\/lD76yTC5zxZPG\/200.webp?cid=ecf05e477al0slkure7x61742bsffmzqcvkt8itmyag7fuhf&rid=200.webp&ct=g\" width=\"450px\"\/>\n<\/div>","dd4d986a":"## <h2 style=\"color:blue;\">4.5 Level 2 (train the meta-learner)<\/h2>","42d03209":"## In this Kernel I'm going to use Stacking method for the 30 Days of ML competition","9683ab69":"<div style=\"width:800px; margin:0 auto;\">\n<img src=\"https:\/\/media3.giphy.com\/media\/dzaUX7CAG0Ihi\/200w.webp?cid=ecf05e47uj47emeszoce1pfmxgda9dtg5r4n0h0emo3p3vn8&rid=200w.webp&ct=g\" width=\"500px\"\/>\n<\/div>","f28deafc":" <h3>In this kernel I'm going to implement the stacking process with 2 levels.<\/h3>"}}