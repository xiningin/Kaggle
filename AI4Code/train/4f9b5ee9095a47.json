{"cell_type":{"ba106868":"code","bb0d5912":"code","1f907949":"code","b9d2756f":"code","4558ed1e":"code","ab51f3bc":"code","a6676b19":"code","59270b32":"code","baacb2f3":"markdown","460c9ffc":"markdown","2e3a150a":"markdown","9a52c16c":"markdown","b61fb87f":"markdown"},"source":{"ba106868":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","bb0d5912":"datafr = pd.read_csv('..\/input\/smoker_data.csv')\ndatafr.info()\ndatafr['Smoker'] = datafr['Smoker'].astype(int)","1f907949":"datafr.head()","b9d2756f":"X = datafr.iloc[:,3:5].values\nY = datafr.iloc[:,5].values\nrand_index = np.random.permutation(np.size(X,0)) # random shuffles\nk = 5 # number of folds\ntrain_test_ratio = (k-1)\/k\ntrain_index = np.zeros(shape=(k,int(train_test_ratio*np.size(X,0))))\ntest_index = np.zeros(shape=(k,int((1\/k)*np.size(X,0))))","4558ed1e":"# define logistic regression solver \ndef newtonGD(x,y,max_ite):\n    weight_vec = np.zeros((3,1))\n    for i in range(max_ite):\n        h_x = 1\/(1+np.exp(-x.dot(weight_vec)))\n        #cost_val_newton[i] = -(1\/(np.size(x,0)))*sum(y*np.log(h_x) + (1-y)*np.log(1-h_x))\n        grad = (1\/(np.size(x,0)))*(x.T.dot((h_x-y)))\n        # hessian matrix\n        H = (1\/(np.size(x,0)))*(x.T.dot(np.diag(h_x.reshape(np.size(x,0),))).dot(np.diag((1-h_x).reshape(np.size(x,0),))).dot(x))\n        weight_vec = weight_vec - np.linalg.pinv(H).dot(grad)\n    return weight_vec","ab51f3bc":"# define accuracy calculator\ndef misclass(y,y_predict):\n    mis=0;indexmis=[]\n    num_samples = np.size(y_predict,0)\n    for i in range(num_samples):\n        if y[i]== 1 and y_predict[i]<=0.5:\n            mis = mis + 1\n            indexmis.append(i)\n        elif y[i]== 0 and y_predict[i]>0.5:\n            mis = mis + 1\n            indexmis.append(i)\n    print('Number of misclassified data = ' + str(mis))\n    accuracy = 100*(num_samples - mis)\/num_samples\n    print('Accuracy of classifier = ' + str(accuracy))\n    return indexmis,accuracy ","a6676b19":"accuracy_all = np.zeros((k,1))   \nfor ii in range(k):\n    #ii = 0\n    jj = ii + 1\n    test_index[ii,:] = rand_index[ii*int(np.size(X,0)\/k):jj*int(np.size(X,0)\/k)]\n    train_index[ii,:] = np.setdiff1d(rand_index,test_index[ii,:])\n    ### Train model    \n    Y_in_train = Y[train_index[ii,:].astype(int)].reshape(int(train_test_ratio*np.size(X,0)),1)\n    X_in_train = X[train_index[ii,:].astype(int),:]\n    \n    X_in_train_pad = np.concatenate((np.ones((np.size(X_in_train,0),1)),X_in_train),axis=1)\n    weight_vec = newtonGD(X_in_train_pad,Y_in_train,20)\n    \n    x_plot = np.linspace(min(X_in_train[:,0]),max(X_in_train[:,0]),20)\n    y_plot = -weight_vec[0]\/weight_vec[2] - (weight_vec[1]\/weight_vec[2])*x_plot\n    \n    plt.scatter(X_in_train[:,0],X_in_train[:,1],c=Y[train_index[ii,:].astype(int)],label='train data')\n    plt.plot(x_plot,y_plot,'r',label='Newtons method' )\n    plt.xlabel('Diastolic');plt.ylabel('Systolic');plt.title('Train model with train data')\n    plt.legend()\n    plt.show()\n    \n    ### Test data used for testing model accuracy\n    Y_in_test = Y[test_index[ii,:].astype(int)].reshape(int((1\/k)*np.size(X,0)),1)\n    X_in_test = X[test_index[ii,:].astype(int),:]\n    \n    X_in_test_pad = np.concatenate((np.ones((np.size(X_in_test,0),1)),X_in_test),axis=1)\n    Y_predict_test = 1\/(1 + np.exp(-X_in_test_pad.dot(weight_vec)))\n    \n    ind_mix,accuracy = misclass(Y_in_test,Y_predict_test)\n    accuracy_all[ii] = accuracy\n    plt.figure(figsize=(10,5))\n    plt.scatter(X_in_test[:,0],X_in_test[:,1],c=Y[test_index[ii,:].astype(int)],label='test data')\n    plt.scatter(X_in_test[ind_mix,0],X_in_test[ind_mix,1],marker='s',facecolors='none',edgecolors='k',s=80,label='misclassified data')\n    plt.plot(x_plot,y_plot,'r',label='Newtons method' )\n    plt.xlabel('Diastolic');plt.ylabel('Systolic');plt.title('Test model performance with test data ')\n    plt.legend()\n    plt.show()","59270b32":"#Last, calculate average model accuracy from 5 different runs\nprint('Average accuracy for ' +str(k) + '-fold cross validations is: ' +str(np.mean(accuracy_all)))\n","baacb2f3":"**Implemet k-fold cross validation with logistic regression**\n* Import data\n* Randomize data and prepare k-fold data\n* Define logistic regression classifier\n* Define model accuracy function\n* Visualize process of k-fold cross validation\n* Calcualte average model accuracy ","460c9ffc":"Setup k-fold cross validation and I used 5 for k, which means split between train and test data is 4:1. We compute our model 5 different times with corresponding train data set, then test the model with test data for each run.","2e3a150a":"Note: This data is from Matlab built-in dataset: Briefly , data describes smoker and non-smoker patients medical and physical readings (attibutes).","9a52c16c":"Here, we compute label prediction first then compare them with correct label. This will tell us how accurate our model is.","b61fb87f":"This part tracks each fold and plots decision boundary with data (both train and test)."}}