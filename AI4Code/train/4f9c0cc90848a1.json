{"cell_type":{"d482f830":"code","9ce80bf3":"code","1ce637d3":"code","000361a9":"code","b4e5a130":"code","a2f119e1":"code","86f85fd9":"code","d8cb558d":"code","21e5858e":"code","5e8c2b77":"code","975a7d98":"code","04d239d9":"code","4bb0b74f":"code","ee4e4eee":"code","a1ee6149":"code","25581230":"code","a243a0b6":"code","7eb18ad7":"code","5499f603":"code","3cd3fdd8":"code","dda5993c":"code","322f5ea7":"code","86fad9f0":"code","2b75e613":"code","f482da9d":"code","f4b9c05e":"code","d4669589":"code","dee81685":"code","a0c41d42":"code","9ae71118":"code","096e173d":"code","69f99d16":"code","e134df79":"code","7f036572":"code","a6759e4a":"code","c563185d":"code","b44d9e74":"code","9241aca1":"code","e2a0dfd5":"code","22a0d525":"code","1bb52d81":"code","e6f8fd1a":"code","c4353566":"code","fbb6f364":"code","b16b88e5":"code","635e283d":"code","1113cf30":"code","6e25e252":"code","10b6e35e":"code","3e7ce7cf":"code","2d3343b1":"code","9b348f48":"code","ae19861c":"code","830ffcf1":"code","67d25f0d":"code","c6a32a22":"code","172ceedd":"code","42d3eceb":"code","efdfb0f5":"code","a9c5617f":"code","9efcfcbf":"code","894ad5c3":"code","fcc42f25":"code","956453c6":"markdown","45467c80":"markdown","62fa316a":"markdown","cbbd07db":"markdown","e6f8f710":"markdown","b5fb0685":"markdown","08165158":"markdown","854a460c":"markdown","4a13ccf6":"markdown","36b689aa":"markdown","edeb5bb6":"markdown","f04d6dd4":"markdown","befa11d3":"markdown","f0043d2f":"markdown","d506ad8c":"markdown","eea90d2e":"markdown","b5c68a09":"markdown","bfc1d593":"markdown","1ad7d38b":"markdown","548d187e":"markdown","37607e18":"markdown","60ecef2e":"markdown","7296c4c4":"markdown","1cf9c639":"markdown","101ad2ef":"markdown","b3d46cee":"markdown","69bdf323":"markdown","d824a966":"markdown","c83a6255":"markdown","b709b313":"markdown","fd3b88a8":"markdown","f41d8a38":"markdown","cfada227":"markdown","507ea7da":"markdown","d57d7144":"markdown","e81c34f1":"markdown","67bf7dd9":"markdown","7e83dd11":"markdown","8fc52a38":"markdown","80a505d6":"markdown","e288ac4c":"markdown","651e0257":"markdown","9a7ee33b":"markdown","58d59c54":"markdown","d223e166":"markdown"},"source":{"d482f830":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom matplotlib import style\nstyle.use('dark_background')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ce80bf3":"data_dir='\/kaggle\/input\/digit-recognizer\/'","1ce637d3":"train=pd.read_csv(data_dir+'train.csv')\ntest=pd.read_csv(data_dir+'test.csv')","000361a9":"train","b4e5a130":"train['label'].value_counts()","a2f119e1":"train.isnull().sum()","86f85fd9":"features=[i for i in train.columns]\nk=0\nfor feature in features:\n    if train[feature].isnull().sum()==0:\n        k=k+1\n    \n    else:\n         \n         print('{0} has {1} null values'.format(feature,train[feature].isnull().sum()))\n            \nif(k==train.shape[1]):\n    print('no nan\\'s in the train dataset,so  proceed')\n","d8cb558d":"features=[i for i in test.columns]\nk=0\nfor feature in features:\n    if test[feature].isnull().sum()==0:\n        k=k+1\n    \n    else:\n        \n         print('{0} has {1} null values'.format(feature,test[feature].isnull().sum()))\n            \nif(k==test.shape[1]):\n    print('no nan\\'s in the test dataset,so  proceed')","21e5858e":"train_y=train['label']\ntrain_x=train.drop('label',axis=1)","5e8c2b77":"print(train_x.shape)\nprint(train_y.shape)","975a7d98":"def image_printer(i,train_x):\n    idx=i\n    grid_data=train_x.iloc[idx].to_numpy().reshape(28,28).astype('uint8')\n    plt.imshow(grid_data)\n    \n    \n","04d239d9":"plt.figure(figsize=(12,10))\nfor i in range(100):\n    plt.subplot(10, 10, i+1)\n    image_printer(i, train_x)","4bb0b74f":"data=train_x[0:20000]\nlabel=train_y[0:20000]\nprint(data.shape,label.shape)","ee4e4eee":"label.value_counts()","a1ee6149":"from sklearn.preprocessing import StandardScaler\nstd_data=StandardScaler().fit_transform(data)\nprint(np.mean(std_data))\nprint(np.std(std_data))","25581230":"temp_data=std_data\ncovar_matrix=np.matmul(temp_data.T,temp_data)\nprint(f'shape of covar matrix is {covar_matrix.shape}')\nprint(f'shape of my data is {temp_data.shape}')","a243a0b6":"from scipy.linalg import eigh\n\n\neigh_values,eigh_vectors=eigh(covar_matrix,eigvals=(782,783))\n\nprint(f'shape of eigen vectors {eigh_vectors.shape}')","7eb18ad7":"pca_points=np.matmul(temp_data,eigh_vectors)\nprint(f'shape of my pca points is {pca_points.shape}')","5499f603":"pca_data=np.vstack((pca_points.T,label)).T\nprint(f'shape of my pca data is {pca_data.shape}')\n\npca_dataframe=pd.DataFrame(data=pca_data,columns=('1st Principal comp','2nd Principal comp','label'))\nprint(pca_dataframe.head(10))","3cd3fdd8":"sns.FacetGrid(pca_dataframe,hue='label',size=10).map(plt.scatter,'1st Principal comp','2nd Principal comp').add_legend()\nplt.show()","dda5993c":"from sklearn import decomposition\npca=decomposition.PCA()","322f5ea7":"temp_data1=std_data\n\npca.n_components=2\npca_data=pca.fit_transform(temp_data1)\n\npca_data=np.vstack((pca_data.T,label)).T\npca_df=pd.DataFrame(data=pca_data,columns=('1st principal','2nd principal','label'))\n\nsns.FacetGrid(pca_df,hue='label',size=10).map(plt.scatter,'1st principal','2nd principal').add_legend()\nplt.show()","86fad9f0":"pca.n_components=784\npca_data1=pca.fit_transform(temp_data1)\n\npercent_variance_explained=pca.explained_variance_\/np.sum(pca.explained_variance_)\ncummulative_variance_explained=np.cumsum(percent_variance_explained)\n\nplt.plot(cummulative_variance_explained,linewidth=2)\n\n\nplt.xlabel('no of top features')\nplt.ylabel('cummulative explained variance')\n\nplt.show()","2b75e613":"from sklearn.manifold import TSNE","f482da9d":"model=TSNE(n_components=2,perplexity=30,n_iter=5000,random_state=0,n_jobs=-1)\ntsne_data=model.fit_transform(std_data)\ntsne_data=np.vstack((tsne_data.T,label)).T\ntsne_df=pd.DataFrame(data=tsne_data,columns=(\"Dimension 1\",\"Dimension 2\",\"label\"))\nsns.FacetGrid(tsne_df,hue='label',size=9).map(plt.scatter,\"Dimension 1\",\"Dimension 2\").add_legend()\nplt.title(f'with perplexity {30} and iterations {5000}')\nplt.show()","f4b9c05e":"model=TSNE(n_components=2,perplexity=50,n_iter=5000,random_state=0,n_jobs=-1)\ntsne_data1=model.fit_transform(std_data)\ntsne_data1=np.vstack((tsne_data1.T,label)).T\ntsne_df1=pd.DataFrame(data=tsne_data1,columns=(\"Dimension 1\",\"Dimension 2\",\"label\"))\nsns.FacetGrid(tsne_df1,hue='label',size=9).map(plt.scatter,\"Dimension 1\",\"Dimension 2\").add_legend()\nplt.title(f'with perplexity {50} and iterations {5000}')\nplt.show()","d4669589":"temp_data1=std_data\n\npca.n_components=450\npca_data=pca.fit_transform(temp_data1)\n\npca_data_new=np.vstack((pca_data1.T,label)).T\nnew_data=pca_data","dee81685":"new_data.shape","a0c41d42":"label.shape","9ae71118":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score,cross_val_predict\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nfrom scipy.stats import uniform,truncnorm,randint","096e173d":"from sklearn.linear_model import LogisticRegression\nclf=LogisticRegression(multi_class='multinomial',n_jobs=-1)\ncv_scores=cross_val_score(clf,new_data,label,cv=10)\nprint(f'we can expect accuracy between {cv_scores.min()} and {cv_scores.max()} with avg accuracy of {cv_scores.mean()}')","69f99d16":"class_pred=cross_val_predict(clf,new_data,label,cv=10)\naccLR=accuracy_score(label,class_pred)\nprint(f'accuracy of our model is {accLR}')\nprint(f' confusion matrix is  ')\nprint(confusion_matrix(label,class_pred))\nprint(f'classification report is ')\nprint(classification_report(label,class_pred))","e134df79":"from sklearn import svm\n\nfor kernel in ('poly','rbf'):\n    clf=svm.SVC(kernel=kernel)\n    cv_scores=cross_val_score(clf,new_data,label,n_jobs=-1,cv=10)\n    print(f'accuracy of {kernel} kernel varies between {cv_scores.min()} and {cv_scores.max()} with mean {cv_scores.mean()}')\n","7f036572":"for C in (1,10,100):\n    clf=svm.SVC(kernel='rbf',C=C)\n    cv_scores=cross_val_score(clf,new_data,label,n_jobs=-1,cv=10)\n    print(f'accuracy of rbf kernel with C={C} varies between {cv_scores.min()} and {cv_scores.max()} with mean {cv_scores.mean()}')\n    \n    \n    ","a6759e4a":"for gamma in (0.001,0.01):\n    clf=svm.SVC(kernel='rbf',C=10,gamma=gamma)\n    cv_scores=cross_val_score(clf,new_data,label,n_jobs=-1,cv=10)\n    print(f'accuracy of rbf kernel with C=10 and gamma={gamma} varies between {cv_scores.min()} and {cv_scores.max()} with mean {cv_scores.mean()}')\n    \n    ","c563185d":"clf=svm.SVC(kernel='rbf',C=10,gamma=0.001)\ncv_scores=cross_val_score(clf,new_data,label,n_jobs=-1,cv=10)\nprint(f'accuracy of rbf kernel with C=10 and gamma= 0.001 varies between {cv_scores.min()} and {cv_scores.max()} with mean {cv_scores.mean()}')","b44d9e74":"class_pred=cross_val_predict(clf,new_data,label,cv=10)\naccSVM=accuracy_score(label,class_pred)\nprint(f'accuracy of our model is {accSVM}')\nprint(f' confusion matrix is  ')\nprint(confusion_matrix(label,class_pred))\nprint(f'classification report is ')\nprint(classification_report(label,class_pred))","9241aca1":"\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn import tree\nlst=list(range(0,20))\nneighbor=[i for i in lst if i%2!=0]\ncv_scores=[]\n\nfor k in neighbor:\n    print(k,end='')\n    clf=KNeighborsClassifier(n_neighbors=k,n_jobs=-1)\n    scores=cross_val_score(clf,new_data,label,cv=10,scoring='accuracy',n_jobs=-1)\n    cv_scores.append(scores.mean())\n    \n    \nMSE=[1-x for x in cv_scores]    \nbest_k=neighbor[MSE.index(min(MSE))]\nprint(f'best k here for knn is {best_k}')\nplt.plot(neighbor,MSE)\nplt.xlabel('K of KNN--->')\nplt.ylabel('Mean Squared Error')\nplt.show()\n","e2a0dfd5":"clf=KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\ncv_score=cross_val_score(clf,new_data,label,cv=10)\nprint(f' we can expect accuracy bwetween  {cv_score.min()} and {cv_score.max()} with a mean of {cv_score.mean()}')\n","22a0d525":"class_pred=cross_val_predict(clf,new_data,label,cv=10)\naccKNN=accuracy_score(label,class_pred)\nprint(f'accuracy of our model is {accKNN}')\nprint(f' confusion matrix is  ')\nprint(confusion_matrix(label,class_pred))\nprint(f'classification report is ')\nprint(classification_report(label,class_pred))","1bb52d81":"from sklearn.tree import DecisionTreeClassifier\nmax_depth_range=list(range(1,20))\ncv_score=[]\nfor depth in max_depth_range:\n    print(depth,end='')\n    clf=DecisionTreeClassifier(max_depth=depth,random_state=0)\n    score=cross_val_score(clf,new_data,label,cv=10,scoring='accuracy',n_jobs=-1)\n    cv_score.append(score.mean())\n    \nMSE=[1-x for x in cv_score]\nbest_n=max_depth_range[MSE.index(min(MSE))]\nprint(f'optimal no of tree is {best_n}')\nplt.plot(max_depth_range,MSE)\nplt.xlabel('Depth of Tree')\nplt.ylabel('Mean Squared Error')\nplt.show()","e6f8fd1a":"clf=DecisionTreeClassifier(max_depth=best_n,random_state=0)\ncv_score=cross_val_score(clf,new_data,label,cv=10,scoring='accuracy',n_jobs=-1)\nprint(f' with {best_n} as depth of trees we can expect accuracy bwetween  {cv_score.min()} and {cv_score.max()} with a mean of {cv_score.mean()}')","c4353566":"clf=DecisionTreeClassifier(max_depth=best_n,random_state=0)\nclf.fit(new_data,label)","fbb6f364":"import graphviz\nimport pylab\n\ncn=['0','1','2','3','4','5','6','7','8','9']\ndot_data = tree.export_graphviz(clf, out_file=None, \n                                \n                                class_names=cn,\n                                filled=True)\n\n\ngraph = graphviz.Source(dot_data,format=\"png\") \ngraph\n","b16b88e5":"class_pred=cross_val_predict(clf,new_data,label,cv=10)\naccDT=accuracy_score(label,class_pred)\nprint(f'accuracy of our model is {accDT}')\nprint(f' confusion matrix is  ')\nprint(confusion_matrix(label,class_pred))\nprint(f'classification report is ')\nprint(classification_report(label,class_pred))","635e283d":"from sklearn.ensemble import RandomForestClassifier\n# max_trees_range=list(range(100,1000))\n# lst=[i for i in max_trees_range if i%100==0]\n# cv_score=[]\n# for ntree in lst: \n#     print(ntree,end='')\n#     clf=RandomForestClassifier(n_estimators=ntree,max_depth=best_n,n_jobs=-1)\n#     score=cross_val_score(clf,new_data,label,cv=10,n_jobs=-1)\n#     cv_score.append(score.mean())\n    \n# MSE=[1-x for x in cv_score]\n# best_ntree=lst[MSE.index(min(MSE))]\n# print(f' best no of tree is {best_ntree}')\n# plt.plot(lst,MSE)\n# plt.xlabel('no of trees')\n# plt.ylabel('Mean Squared Error')\n# plt.show()\n    ","1113cf30":"clf=RandomForestClassifier(n_estimators=600,max_depth=best_n,random_state=0,n_jobs=-1)\ncv_score=cross_val_score(clf,new_data,label,cv=10,scoring='accuracy',n_jobs=-1)\nprint(f' with {best_n} as depth of trees and {600} as no of trees we can expect accuracy bwetween  {cv_score.min()} and {cv_score.max()} with a mean of {cv_score.mean()}')","6e25e252":"class_pred=cross_val_predict(clf,new_data,label,cv=10)\naccRF=accuracy_score(label,class_pred)\nprint(f'accuracy of our model is {accRF}')\nprint(f' confusion matrix is  ')\nprint(confusion_matrix(label,class_pred))\nprint(f'classification report is ')\nprint(classification_report(label,class_pred))","10b6e35e":"import tensorflow as tf\nimport keras\nfrom keras import backend as k\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import plot_model\nfrom keras.callbacks import EarlyStopping","3e7ce7cf":"train_x","2d3343b1":"test_x=test","9b348f48":"train_y","ae19861c":"print(train_x.shape)\nprint(test.shape)\nprint(train_y.shape)","830ffcf1":"img_cols=28\nimg_rows=28","67d25f0d":"if k.image_data_format=='channels_first':\n    train_x = train_x.values.reshape(train_x.shape[0], 1,img_cols,img_rows)\n    test = test.values.reshape(test.shape[0], 1,img_cols,img_rows)\n    train_x=train_x\/255.0\n    test=test\/255.0\n    input_shape = (1,img_cols,img_rows)\nelse:\n    train_x=train_x.values.reshape(train_x.shape[0],img_cols,img_rows,1)\n    test=test.values.reshape(test.shape[0],img_cols,img_rows,1)\n    train_x=train_x\/255.0\n    test=test\/255.0\n    input_shape = (img_cols,img_rows,1)","c6a32a22":"input_shape","172ceedd":"earlystopping = EarlyStopping(monitor =\"val_accuracy\",\n                              mode = 'auto', patience = 10,\n                              restore_best_weights = True)\n\n\nmodelacc = []\n\nnfilters = [32, 64, 128,256]\nconv_layers = [1, 2, 3,4]\ndense_layers = [0, 1, 2,3]\n#dropouts \ndp=0.5\n\nfor filters in nfilters:\n    for conv_layer in conv_layers:\n        for dense_layer in dense_layers:\n           \n                #lets iterate for all the combinations\n                cnnsays = 'No of filters\/feature maps: {} conv_layers: {} dense_layers: {} dropout: {}'.format(filters, conv_layer, dense_layer, dp)\n                print(cnnsays)\n\n                model = Sequential()\n\n                model.add(Conv2D(filters, (3, 3), input_shape = input_shape))\n                model.add(BatchNormalization())\n                model.add(Activation(\"relu\"))\n                model.add(MaxPooling2D(pool_size=(2, 2)))\n                model.add(Dropout(dp))\n\n                for i in range(conv_layer-1):\n\n                    model.add(Conv2D(filters, (3, 3),padding=\"same\"))\n                    model.add(BatchNormalization())\n                    model.add(Activation('relu'))\n                    model.add(MaxPooling2D(pool_size=(2, 2)))\n                    model.add(Dropout(dp))\n\n\n                model.add(Flatten())\n\n                for i in range(dense_layer):\n                    model.add(Dense(256))\n                    model.add(BatchNormalization())\n                    model.add(Activation('relu'))\n                    model.add(Dropout(dp))\n\n\n                model.add(Dense(10, activation='softmax'))\n\n\n                model.compile(optimizer = 'adam',\n                  loss = 'sparse_categorical_crossentropy',\n                  metrics = ['accuracy'])\n\n                EPOCHS = 30\n\n                history = model.fit(train_x, train_y, epochs=EPOCHS, batch_size=32, validation_split=0.2, callbacks=[earlystopping])\n\n                modelacc.append([round(100*max(history.history['val_accuracy']), 2), cnnsays])\n","42d3eceb":"print(\"Highest Validation Accuracy so far : {}%\".format(round(100*max(history.history['val_accuracy']), 2)))","efdfb0f5":"modelacc.sort(reverse=True)\nmodelacc","a9c5617f":"loss_train = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1,len(loss_val)+1)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='Validation loss')\nplt.title('Training & Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","9efcfcbf":"acc_train = history.history['accuracy']\nacc_val = history.history['val_accuracy']\nepochs = range(1,len(acc_val)+1)\nplt.plot(epochs, acc_train, 'g', label='Training accuracy')\nplt.plot(epochs, acc_val, 'b', label='Validation accuracy')\nplt.title('Training & Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","894ad5c3":"pred=model.predict([test])\nsoln=[]\nfor i in range(len(pred)):\n    soln.append(np.argmax(pred[i]))","fcc42f25":"final = pd.DataFrame()\nfinal['ImageId']=[i+1 for i in test_x.index]\nfinal['Label']=soln\nfinal.to_csv('mnistcnn.csv', index=False)","956453c6":"# Model Accuracy","45467c80":"# Generating Classification Report","62fa316a":"# not much of help as feature set is very large, this is not a good method to check for nan's","cbbd07db":"# well we all can now clearly see the clusters that are formed...tsne is time consuming but it's worth the effort","e6f8f710":"# Finally lets call the Navy Seals i.e CNN","b5fb0685":"# o\/p","08165158":"# this is our final pca dataframe","854a460c":"# lets generate our covariance matrix of dXd because it will help us to get top 2 eigen vectors for our 2d visualization ","4a13ccf6":"## Generating classification Report","36b689aa":"# Generating Classification Report","edeb5bb6":"# top 450 principal components that can explain almost 99% of total variance so no point of taking more features than 450 because it wilI l only lead to curse of dimensionality","f04d6dd4":"# Lets check for nan in train and test dataset","befa11d3":"# lets find the no of top best features that can best explain variance in our dataset","f0043d2f":"# fetching the top two eigen values and eigen vectors because for top two principal axis we need top two eigen vectors","d506ad8c":"# lets standardize it","eea90d2e":"# tree is very large to visualize...just scroll down left and right to visualize the tree.... i  failed to scale down the size...... if you know the trick then pls share with me","b5c68a09":"# lets train some models with different algos","bfc1d593":"# Reshaping and normalizing the data","1ad7d38b":"# Pca without Sklearn","548d187e":"# Producing the same using pca from sklearn","37607e18":"# Generating classification report for final svm model","60ecef2e":"# Applying KNN","7296c4c4":"# separate feature matrix and target vector","1cf9c639":"# Building new dataset with top 450 features","101ad2ef":"# Generating Classification Report","b3d46cee":"# Note: I could've used gridsearch and randomizedsearch but they are taking sooo long, i waited for 3-4 hours but still they still they didnt produced anything and also i donot want to redeuce my train pts.....so i modified my code to find the best params...here at least i can observe my progress ","69bdf323":"# Applying Decision Tree","d824a966":"# lets visualize a sample point, which label does it belong","c83a6255":"# Applying SVM","b709b313":"# Time to visualize","fd3b88a8":"# as eigen vect starts from 0 so the last eigen vector is 783 and second last is 782 which are two largest eigen vectors among 784 eigen vectors ","f41d8a38":"# it seems like rbf kernel with C=10 and gamma=0.001 is the best hyperparams set for my svm so lets train with the final model","cfada227":"## all classes are present in almost equal poportions, so no problem of imbalanced data","507ea7da":"# Best k for our knn is 5 so lets build our final model with k=5","d57d7144":"# lets read the data sets into train and test  ","e81c34f1":"# Preparing the data for CNN","67bf7dd9":"# Applying multiclass Logistic Regression","7e83dd11":"## training 42k points will take too long so slice for a 20k points","8fc52a38":"# Predictions","80a505d6":"# Note: i precomputed the best no of trees for randomforest is 600 and it took me 4.5 hours to realize this so i am not again running this coz it would take insane amount of time........code is below if you are born with iron patience you can use the code :)","e288ac4c":"# Applying Random Forest ","651e0257":"# Model loss","9a7ee33b":"# Now applying tsne to visualize better","58d59c54":"# lets have our pca_points for top two axis by matrix multiplication of our sample data with top two eigen vectors","d223e166":"# conclusion both of the methods with sklearn and without sklearn produces visually same o\/p (rotaion is not a problem so ignore it). so we can say we are successful in our task"}}