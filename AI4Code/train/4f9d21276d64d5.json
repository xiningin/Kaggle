{"cell_type":{"a5fbf674":"code","2b7fcf9b":"code","9d81f8a5":"code","afcb8690":"code","7b494288":"code","91b92470":"code","ae9ea341":"code","f8bcd94c":"code","c672536a":"code","02eeec53":"code","69f6cd2b":"code","f6dbb6b8":"code","82f44e54":"code","703dbe4c":"code","b1df71a1":"code","2d4f2ff8":"code","30c49727":"markdown","4349c6ae":"markdown","f8cbe70e":"markdown"},"source":{"a5fbf674":"!pip install --upgrade scikit-learn","2b7fcf9b":"import numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom skimage.filters import threshold_otsu\nfrom tqdm import tqdm\nimport gc\n\nSEED = 0","9d81f8a5":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv\", index_col='id')\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv\", index_col='id')","afcb8690":"pointy = [0,2,4,9,12,16,19,20,21,23,24,27,28,30,31,32,33,35,39,42,44,46,48,49,51,52,53,56,58,59,60,61,62,63,64,68,69,72,73,75,76,78,79,81,83,84,87,88,89,90,92,93,94,95,98,99]\nbimodal = [x for x in range(0,100) if x not in pointy]\n\npointy = list(map(lambda x: 'f'+str(x), pointy))\nbimodal = list(map(lambda x: 'f'+str(x), bimodal))\n\nfeatures = [x for x in train.columns.values if x[0]==\"f\"]","7b494288":"def create_features(df, cols, prefix='new_'):\n    df[prefix+'abs_sum'] = df[cols].abs().sum(axis=1)\n    df[prefix+'sem'] = df[cols].sem(axis=1)\n    df[prefix+'std'] = df[cols].std(axis=1)\n    df[prefix+'avg'] = df[cols].mean(axis=1)\n    df[prefix+'max'] = df[cols].max(axis=1)\n    df[prefix+'min'] = df[cols].min(axis=1)\n    \n    return df","91b92470":"train = create_features(train, pointy, 'point_')\ntrain = create_features(train, bimodal, 'bimodal_')\ntest = create_features(test, pointy, 'point_')\ntest = create_features(test, bimodal, 'bimodal_')","ae9ea341":"def check_peak(df, test_df, cols, suffix='_peak'):\n    for col in cols:\n        peak = threshold_otsu(df[col])\n        df[str(col)+suffix] = df[col] > peak\n        test_df[str(col)+suffix] = test_df[col] > peak","f8bcd94c":"check_peak(train, test, bimodal)","c672536a":"test.head()","02eeec53":"X = train.drop([\"target\"], axis=1)\nX_test = test\ny = train[\"target\"]","69f6cd2b":"scaler = RobustScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","f6dbb6b8":"del test, train, scaler\ngc.collect()","82f44e54":"from sklearn.neural_network import MLPClassifier","703dbe4c":"%%time\nmodel = MLPClassifier((5,128),random_state=SEED, verbose=1, warm_start=True, early_stopping=True)\nmodel.fit(X, y)\n\ngc.collect()","b1df71a1":"preds = model.predict_proba(X_test)[:,1]","2d4f2ff8":"submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv', index_col='id')\nsubmission['target'] = preds\nsubmission.to_csv('submission.csv')","30c49727":"0.735633","4349c6ae":"## contribution 2\nThe bimodal distributions clearly influence the target. We can create a boolean comparison as to which peak it sits under\n\nSee: www.kaggle.com\/realtimshady\/eda-feature-exploration","f8cbe70e":"## contribution 1\nI pretty much looked at whether the distribution was a unimodal point, or a bimodal distribution\nThen split the features according to which distribution it was under and then apply feature engineering to each"}}