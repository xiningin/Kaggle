{"cell_type":{"181407d8":"code","fc415f1b":"code","6321bd94":"code","d79667f3":"code","b91d02b7":"code","124e5d5d":"code","6cad457a":"code","115529b7":"code","c0f4248a":"code","14887672":"code","95ff7d5a":"code","5ad1f8a4":"code","38a6513c":"code","7aa2ef14":"code","3e6140bd":"code","63f5ffb5":"code","f4e9735f":"code","22b08bde":"markdown","aac3f0a5":"markdown","f522280f":"markdown","c06c22ce":"markdown","7b11b995":"markdown","9266fd04":"markdown","39c73f4e":"markdown","3d53c58b":"markdown","7bb28ea8":"markdown","0ef5baf2":"markdown","ed25a24d":"markdown","d593fe19":"markdown","083475c8":"markdown","c66a14b2":"markdown","d823f09d":"markdown","4cca3253":"markdown"},"source":{"181407d8":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom time import time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\n\ndata = pd.read_csv(\"..\/input\/weatherAUS.csv\")\ndata.insert(loc=1, column='Month', value = data['Date'].apply(lambda x: x[5:7])) #create column \"Month\"\ndata.insert(loc=2, column='Day', value = data['Date'].apply(lambda x: x[5:10])) #create column \"Month\"\ndata.insert(loc=3, column='Season', value = data['Month'].replace(['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'], ['summer','summer', 'summer', 'fall', 'fall', 'fall', 'winter', 'winter', 'winter', 'spring', 'spring', 'spring'])) #create column \"Season\"\ndata['RainTomorrow'] = data['RainTomorrow'].replace(['Yes', 'No'], [1,0])\nprint('Dataset shape:',data.shape)\ndisplay(data.head(n=5))","fc415f1b":"fill = pd.DataFrame(100*data.count().sort_values()\/data.shape[0])\nfill.reset_index(level=0, inplace=True)\nfill.columns = ['Variable','Fill (%)']\n\nsns.set()\nplt.figure(figsize=(16, 5))\ng = sns.barplot(x = 'Variable', y = 'Fill (%)', data = fill,color = 'orange')\ng = plt.xticks(rotation=75)","6321bd94":"import missingno as msno\nmsno.matrix(data)","d79667f3":"types = pd.DataFrame(data.dtypes)\ntypes.reset_index(level=0, inplace=True)\ntypes.columns = ['Variable','Type']\n\nnumerical_variables = list(types[types['Type'] == 'float64']['Variable'].values)\ncategorical_variables = list(types[types['Type'] == 'object']['Variable'].values)\n\nprint ('numerical_variables:', numerical_variables) \nprint ('\\ncategorical_variables:', categorical_variables) ","b91d02b7":"#pd.scatter_matrix(data, alpha = 0.3, figsize = (14,8), diagonal = 'kde')\nf, ax = plt.subplots(figsize=(16, 10))\ncorr = data.corr()\ncorr_mtx = sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=False, ax=ax, annot = True)","124e5d5d":"n_row = 4\nn_col = 4\nlist_of_variables = numerical_variables[0:-1]\nf, axes = plt.subplots(n_row, n_col, figsize=(16, 15))\ntotal = n_row*n_col\nk = 0\nfor i in range(n_row):\n    for j in range(n_col):\n        sns.distplot(data[data['RainTomorrow']==1][list_of_variables[k]].dropna(),hist = False, kde = True, label = 'Yes',ax=axes[i, j])#.set_title(list_of_variables[k])\n        sns.distplot(data[data['RainTomorrow']==0][list_of_variables[k]].dropna(), hist = False, kde = True, label = 'No',ax=axes[i, j])#.set_title(list_of_variables[k])\n        k = k + 1","6cad457a":"print('Probability of Rain Tomorrow:',np.mean(data['RainTomorrow']))","115529b7":"from numpy import mean\n\nn_row = 3\nn_col = 2\nlist_of_variables = ['Month', 'Season', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']\nf, axes = plt.subplots(n_row, n_col, figsize=(16, 12))\nk = 0\nfor i in list(range(n_row)):\n    for j in list(range(n_col)):\n        sns.barplot(x = list_of_variables[k], y = 'RainTomorrow', data = data, estimator = mean, color = 'orange', ax=axes[i, j])\n        #g.xticks(rotation=45)\n        k = k + 1","c0f4248a":"data_final_variables = data.drop(columns=['Sunshine','Evaporation','Cloud3pm','Cloud9am','Location','Date', 'Month', 'Day', 'RISK_MM'],axis=1)\n\ndata_final_variables = data_final_variables.dropna()\n\nfeatures_raw = data_final_variables.drop(columns = ['RainTomorrow'])\nincome_raw = data_final_variables['RainTomorrow']","14887672":"from sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\n\nscaler = MinMaxScaler()\n\ntypes_aux = pd.DataFrame(features_raw.dtypes)\ntypes_aux.reset_index(level=0, inplace=True)\ntypes_aux.columns = ['Variable','Type']\nnumerical = list(types_aux[types_aux['Type'] == 'float64']['Variable'].values)\n\nfeatures_minmax_transform = pd.DataFrame(data = features_raw)\nfeatures_minmax_transform[numerical] = scaler.fit_transform(features_raw[numerical])\n\nfeatures_minmax_transform['RainToday'] = features_minmax_transform['RainToday'].replace(['Yes', 'No'], [1,0])\n\nfeatures_minmax_transform.head()","95ff7d5a":"features_final = pd.get_dummies(features_minmax_transform)\n\nencoded = list(features_final.columns)\nprint (\"{} total features after one-hot encoding.\".format(len(encoded)))\n\n# Descomente a linha abaixo para ver as colunas ap\u00f3s o encode\nprint (encoded)","5ad1f8a4":"from sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\n\nX = features_final\ny = income_raw\n\nclf_a = MultinomialNB()\nclf_b = DecisionTreeClassifier(random_state = 0)\nclf_c = RandomForestClassifier(random_state = 0)\nclf_d = LogisticRegression(random_state = 0)\nclf_e = SGDClassifier(random_state = 0)\n\nlist_clf = [clf_a, clf_b, clf_c, clf_d, clf_e]\n\nresults = []\nfor clf in list_clf:\n    start = time()\n    clf_name = clf.__class__.__name__\n    scores_f1 = cross_val_score(clf, X, y, cv=5, scoring = 'f1')\n    scores_ = cross_val_score(clf, X, y, cv=5)\n    end = time()\n    train_time = end  - start\n    results.append([clf_name, np.mean(scores_f1), np.mean(scores_), train_time])\n\ndf_results = pd.DataFrame(np.array(results))\ndf_results.columns = ['Classifier', 'F1-Score', 'Accuracy', 'Train Time']\ndf_results.sort_values(by=['F1-Score'], ascending=False)","38a6513c":"from sklearn.decomposition import PCA\n\nclf_pca = LogisticRegression(random_state=0)\n\nlist_n = [10,20,30,40,50]\n\nresults_pca = []\nfor i in list_n:\n    start = time()\n    pca = PCA(n_components = i)\n    pca.fit(X)\n    X_pca = pca.fit_transform(X)\n    scores_f1 = cross_val_score(clf_pca, X_pca, y, cv=5, scoring = 'f1')\n    scores_ = cross_val_score(clf_pca, X_pca, y, cv=5)\n    explained_variance = np.sum(pca.explained_variance_ratio_)\n    end = time()\n    train_time = end  - start\n    results_pca.append([i, explained_variance, np.mean(scores_f1), np.mean(scores_), train_time])\n\ndf_results_pca = pd.DataFrame(np.array(results_pca))\ndf_results_pca.columns = ['Number of components', 'Cumulative Explained Variance Ration','F1-Score', 'Accuracy', 'Train Time']\ndf_results_pca.sort_values(by=['F1-Score'], ascending=False)","7aa2ef14":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nclf_feature_importance = RandomForestClassifier(random_state=0)\nmodel_feature_importance = clf_feature_importance.fit(X_train,y_train)\nimportances = model_feature_importance.feature_importances_\n\ndf_feature_importance = pd.DataFrame()\ndf_feature_importance['features'] = X.columns\ndf_feature_importance['importances'] = importances\ndf_feature_importance = df_feature_importance.sort_values(by=['importances'], ascending=False)\nfeatures = list(df_feature_importance['features'].values)\n\nlist_n_features = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60]\n\nresults_features = []\nfor i in list_n_features:\n    start = time()\n    X_selected_features = X[features[0:i]]\n    scores_f1 = cross_val_score(LogisticRegression(random_state=0), X_selected_features, y, cv=5, scoring = 'f1')\n    scores_ = cross_val_score(LogisticRegression(random_state=0), X_selected_features, y, cv=5)\n    cummulative_importance = sum(df_feature_importance.importances[0:i])\n    end = time()\n    train_time = end  - start\n    results_features.append([i, cummulative_importance, np.mean(scores_f1), np.mean(scores_), train_time])\n\ndf_results_features = pd.DataFrame(np.array(results_features))\ndf_results_features.columns = ['Number of features', 'Cumulative Importance','F1-Score', 'Accuracy', 'Train Time']\ndf_results_features.sort_values(by=['F1-Score'], ascending=False)","3e6140bd":"print ('Most important features:\\n')\nfor i in features[0:10]:\n    print(i)","63f5ffb5":"from sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nclf_grid = LogisticRegression(random_state=0, n_jobs = -1)\n\nparameters = {'C':np.logspace(0, 4, 10),\n             'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nscorer = make_scorer(fbeta_score, beta=0.5)\n\ngrid_obj = GridSearchCV(clf_grid, parameters, scoring=scorer, n_jobs = -1)\n\ngrid_fit = grid_obj.fit(X_train, y_train)\n\nbest_clf = grid_fit.best_estimator_\n\nprint (\"Logistic Regression\\n\")\nprint (\"Unoptimized model\\n------\")\nprint (\"Accuracy score on testing data: {:.4f}\".format(np.mean(cross_val_score(clf_grid, X, y, cv=5))))\nprint (\"F-score on testing data: {:.4f}\".format(np.mean(cross_val_score(clf_grid, X, y, cv=5, scoring = 'f1'))))\nprint (\"\\nOptimized Model\\n------\")\nprint (\"Accuracy score on testing data: {:.4f}\".format(np.mean(cross_val_score(best_clf, X, y, cv=5))))\nprint (\"F-score on testing data: {:.4f}\".format(np.mean(cross_val_score(best_clf, X, y, cv=5, scoring = 'f1'))))\nprint ('\\n')\nprint (best_clf)","f4e9735f":"from sklearn.metrics import confusion_matrix\n\nfinal_model = LogisticRegression()\nfinal_model.fit(X_train,y_train)\ny_pred = final_model.predict(X_test)\nconfusion_matrix(y_test, y_pred)","22b08bde":"## Predicting Rains in Australia\n\nTraditionally, weather forecasting has been done through physical simulations of the atmosphere. Its current state is sampled, and the future state is calculated from fluid dynamics and thermodynamics equations. However, this physical model is unstable under disturbances and uncertainties in the measurements, in addition to not fully understanding the complex atmospheric processes. Thus, machine learning represents a viable alternative in the weather forecast, since it is relatively robust to disturbances and does not require a complete understanding of physical processes ([Holmstromet al., 2016](http:\/\/www.ep.liu.se\/ecp\/153\/024\/ecp18153024.pdf)).\n\nOn the other hand, the use of machine learning still presents some resistance for this purpose. In the case of deep learning algorithms, it is not always possible to understand how a certain result has been achieved. That way, it becomes difficult to rely on \"black boxes\" for predicting emergency situations, such as floods ([Nature, 2017](https:\/\/go.nature.com\/2E1WYVr)).\n\nBased on the proposed by Joe Young on [kaggle](https:\/\/www.kaggle.com\/jsphyg\/weather-dataset-rattle-package), the objective is to predict whether there will be rain on a day, given the previous day weather conditions. The database relies on various weather stations measurements in Australia between 31st October 2007 and 24th June 2017.","aac3f0a5":"----\n## 1) Exploratory Data Analysis\n### 1.1) Importing Libraries and Inserting new columns\n\nTwo columns were added to the original dataset: `'Month'` and `'Season'`.","f522280f":"### 2.3) One-hot encoding","c06c22ce":"### 1.3) Variable Types\n\n\n\n**Numerical variables:** `MinTemp`, `MaxTemp`, `Rainfall`, `Evaporation`, `Sunshine`, `WindGustSpeed`, `WindSpeed9am`, `WindSpeed3pm`, `Humidity9am`, `Humidity3pm`, `Pressure9am`, `Pressure3pm`, `Cloud9am`, `Cloud3pm`, `Temp9am`, `Temp3pm`, `RISK_MM`\n\n\n**Categorical variables:** `Date`, `Month`, `Season`, `Location`, `WindGustDir`, `WindDir9am`, `WindDir3pm`, `RainToday`, `RainTomorrow`","7b11b995":"Confusion Matrix","9266fd04":"### 3.2) Dimensionality Reduction\n\n\nAs there are some variables that are highly correlated with each other, performing PCA could bring some improvements to the prediction.\n\nThe results show that the dimensionality reduction did not bring significant gains in the computational time that justified the loss of prediction capacity.","39c73f4e":"### 1.4) Variable Correlations\n\n- `Pressure9am` and `Pressure3pm` have very strong correlation, almost next to 1.\n\n- `MinTemp`, `MaxTemp`, `Temp9am`, `Temp3pm` have strong correlation between them.\n\n- `WindGustSpeed`, `WindSpeed9am`, `WindSpeed3pm` have moderate correlation between them.\n\n- `Humidity9am`, `Humidity3pm` have moderate correlation between them.\n\n- `Cloud9am`, `Cloud3pm` have moderate correlation between them.\n\nHaving some variables  with correlation between them is a signal that it will be interesting to perform Feature Selection or Dimensionality Reduction.","3d53c58b":"----\n## 2) Data Preprocessing\n### 2.1) Dropping variables\n\n\nThe variables `Sunshine`, `Evaporation`, `Cloud3pm`, `Cloud9am` were removed because they had a low fill percentage\n\n`Location` was removed, since we want forecast rain regardless the location.\n\n`Date`, `Month`, `Day` and were removed, because `Season` is going to be used instead.\n\n`RISK_MM` was removed to avoid data leakage.","7bb28ea8":"### 1.5) Influence of the predictor variables on target variable\n\n#### Numerical variables\n\nBelow, the distributions of each variable are plotted for when there is rain or not. If the distributions are similar, it is a sign that they do not influence the occurrence of rains the next day.\n\nFor example, `Sunshine` today when it is going to rain tomorrow is considerably different from when it is not going to rain tomorrow.\n\nOn the other hand, there is not such a considerable difference between `MinTemp` distributions.\n\nThis information is useful to identify the variables that are going to be used to predict wheter it will rain tomorrow.","0ef5baf2":"### 2.2) Feature Scaling","ed25a24d":"### 3.3) Feature Selection\n\n\nUnlike PCA, training the model with only best featuers brought significant gains in the computational time, without a significant loss in prediction capacity.\n\nAdopting the 10 most relevant features, the accuracy loss was 0.3% and F1-Score was 2%, while it performed 66% faster.","d593fe19":"### 3.4) Parameters Tuning\n\nCross-Validated Grid-Search was adopted to tune the hyperparameters of Logistic Regression.\n\nThe tuning brought insignificant improvement to the model.","083475c8":"#### Categorical variables\n\nBelow, the probabilities of rain tomorrow for the classes of each categorical variables are plotted.\n\nRegarding all the dataset, the probability of rain tomorrow is **22%**. This way, classes whose probabilities deviate from that number tend to influence prediction more\n\nFor example, regarding `RainToday`, the probability of rain tomorrow when it rains today is three times the probability when it does not rain today.","c66a14b2":"### 4) Conclusion\n\n\nFinally, the model performance is described below.\n\n\n\\begin{align}\nAccuracy & = 84.7\\%\n\\\\\nF_{1}-Score & = 59.5\\%\n\\end{align}","d823f09d":"### 1.2) Variable Fill\n\nThe variables `Sunshine`, `Evaporation`, `Cloud3pm`, and `Cloud9am` presented a low fill, less than 65%.","4cca3253":"----\n## 3) Model Training\n### 3.1) Evaluating Metrics\n\n\n**Accuracy** and **F-beta score** with **K-Fold Cross Validation** (k = 5) were adopted to evaluate the models.\n\nThe stategy is to test some classifiers, choose models with good performance and reasonable time training. Next, I workd to improve them. \n\nThe following models were tested:\n\n- MultinomialNB\n- Decision Tree\n- Random Forest\n- Logistic Regression\n- SGD\n\nLogistic Regression presented the best **Accuracy** and **F1-Score** with a reasonable time training.\n"}}