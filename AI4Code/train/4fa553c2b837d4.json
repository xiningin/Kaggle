{"cell_type":{"07d42d4e":"code","567ebaeb":"code","22262563":"code","1e123eb9":"code","91816e4f":"code","a51f7e0b":"code","3230f40e":"code","0d6910aa":"code","840d3f2a":"code","32374974":"code","bf0c90a4":"code","54774bab":"code","adf2a5d8":"code","09b27c41":"code","e33a0849":"code","e0340a3f":"code","7d14ecb0":"code","41529daf":"code","4a9e1fbe":"code","57af20d6":"code","660bc12a":"code","f2e10f8b":"code","04a822d0":"code","f6ce7530":"code","0ded7524":"code","fa69e820":"code","a785d93e":"code","7fefeffa":"markdown","221090aa":"markdown","3ba429f8":"markdown","558c3332":"markdown","17572b1f":"markdown","8f57e6ef":"markdown","e38eb460":"markdown","470120bd":"markdown","fa056bdb":"markdown","0596eaf0":"markdown","74fdc02a":"markdown","603d6011":"markdown","935a3a2b":"markdown","dcad8b95":"markdown","3ca8aa27":"markdown","1da99382":"markdown","9aaac86c":"markdown","040ab2ed":"markdown","01a69cbc":"markdown","8470ef52":"markdown","eba2cf7a":"markdown","599b580e":"markdown","1158713f":"markdown","d22ccc30":"markdown"},"source":{"07d42d4e":"import pandas as pd\n# Read CSV file using pandas library\nmain_file_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv' # this is the path to the Iowa data that you will use\ndata = pd.read_csv(main_file_path)\n\n# Run this code block with the control-enter keys on your keyboard. Or click the blue botton on the left\nprint(data.describe())","567ebaeb":"#This shows you the list of columns in a DataFrame\nprint (data.columns)","22262563":"#Print values of a particular columns.\nprice = data.SalePrice\n# head() returns the first few values as given.\nprint (price.head(5))","1e123eb9":"# To print Multiple columns values\nlot_price = ['Id','LotArea','SalePrice']\nprint (data[lot_price].head(5))","91816e4f":"y = data.SalePrice\nfeatures = ['Id','LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd']\nX = data[features]","a51f7e0b":"from sklearn.tree import DecisionTreeRegressor\n\n# Define model\niowa_model = DecisionTreeRegressor()\n\n# Fit model\niowa_model.fit(X, y)","3230f40e":"print(\"Making predictions for the following 5 houses:\")\nprint(X.head())\nprint(\"The predictions are\")\nprint(iowa_model.predict(X.head()))","0d6910aa":"from sklearn.model_selection import train_test_split\n\n# split data into training and validation data, for both predictors and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n","840d3f2a":"from sklearn.tree import DecisionTreeRegressor\n# Define model\niowa_model = DecisionTreeRegressor()\n# Fit model\niowa_model.fit(train_X, train_y)","32374974":"from sklearn.metrics import mean_absolute_error\n\npredicted_home_prices = iowa_model.predict(val_X)\nmean_absolute_error(val_y, predicted_home_prices)","bf0c90a4":"def get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)","54774bab":"# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","adf2a5d8":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nmodel = RandomForestRegressor()\nmodel.fit(train_X, train_y)\npreds = model.predict(val_X)\nrf_score = mean_absolute_error(val_y, preds)\nprint (rf_score)","09b27c41":"my_submission = pd.DataFrame({'Id': val_X.Id, 'SalePrice': preds})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","e33a0849":"# The below DataFrame contains null values\ndf_predictors = data.drop(['SalePrice'],axis=1)\ndf_target = data['SalePrice']\n\n# For the sake of keeping the example simple, we'll use only numeric predictors. \n#This will remove the fence data\ndf_numeric_predictors = df_predictors.select_dtypes(exclude=['object'])\n\nX_train, X_test, y_train, y_test = train_test_split(df_numeric_predictors, \n                                                    df_target,\n                                                    train_size=0.7, \n                                                    test_size=0.3,\n                                                    random_state=0)\n# We are using here random forest model.\ndef random_forest(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)","e0340a3f":"cols_with_missing = [col for col in X_train.columns \n                                 if X_train[col].isnull().any()]\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_test  = X_test.drop(cols_with_missing, axis=1)","7d14ecb0":"print(\"Mean Absolute Error from dropping columns with Missing Values:\")\nprint(random_forest(reduced_X_train, reduced_X_test, y_train, y_test))","41529daf":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\nimputed_X_train = my_imputer.fit_transform(X_train)\nimputed_X_test = my_imputer.transform(X_test)\nprint(\"Mean Absolute Error from Imputation:\")\nprint(random_forest(imputed_X_train, imputed_X_test, y_train, y_test))","4a9e1fbe":"imputed_X_train_plus = X_train.copy()\nimputed_X_test_plus = X_test.copy()\n\ncols_with_missing = (col for col in X_train.columns \n                                 if X_train[col].isnull().any())\nfor col in cols_with_missing:\n    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\nimputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n\nprint(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\nprint(random_forest(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))","57af20d6":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n# Drop houses where the target is missing\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\ntarget = train_data.SalePrice\n#Contains all the missing values columns name\ncols_with_missing = [col for col in train_data.columns \n                                 if train_data[col].isnull().any()]\n\ncandidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\ncandidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)","660bc12a":"# \"cardinality\" means the number of unique values in a column.\n# We use it as our only way to select categorical columns here. This is convenient, though\n# a little arbitrary.\nlow_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].nunique() < 10 and\n                                candidate_train_predictors[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\nmy_cols = low_cardinality_cols + numeric_cols\ntrain_predictors = candidate_train_predictors[my_cols]\ntest_predictors = candidate_test_predictors[my_cols]\n#train_predictors.dtypes.sample(20)","f2e10f8b":"one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)","04a822d0":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n# Evaluation using cross validation\ndef get_mae(X, y):\n    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n    return -1 * cross_val_score(RandomForestRegressor(50), \n                                X, y, \n                                scoring = 'neg_mean_absolute_error').mean()\n\npredictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n\nmae_without_categoricals = get_mae(predictors_without_categoricals, target)\n\nmae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n\nprint('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\nprint('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))","f6ce7530":"from sklearn.preprocessing import Imputer\n\ndata = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = data.SalePrice\nX = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)","0ded7524":"from xgboost import XGBRegressor\n\n# my_model = XGBRegressor()\n# # Add silent=True to avoid printing out updates with each cycle\n# my_model.fit(train_X, train_y, verbose=False)\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.03)\nmy_model.fit(train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(test_X, test_y)], verbose=False)\n\n# make predictions\npredictions = my_model.predict(test_X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))","fa69e820":"import pandas as pd\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.preprocessing import Imputer\n\ncols_to_use = ['LotArea','YearBuilt','1stFlrSF']\n\ndef get_some_data():\n    data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n    y = data.SalePrice\n    X = data[cols_to_use]\n    my_imputer = Imputer()\n    imputed_X = my_imputer.fit_transform(X)\n    return imputed_X, y\n    \nX, y = get_some_data()\nmy_model = GradientBoostingRegressor()\nmy_model.fit(X, y)\nmy_plots = plot_partial_dependence(my_model,\n                                   features=[0,2],\n                                   X=X,\n                                   feature_names=cols_to_use, \n                                   grid_resolution=10)","a785d93e":"data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = data.SalePrice\nX = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nmy_pipeline = make_pipeline(Imputer(), RandomForestRegressor())\n\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')\nprint(scores)\n\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))","7fefeffa":"It's most common to one-hot encode these \"object\" columns, since they can't be plugged directly into most models. Pandas offers a convenient function called get_dummies to get one-hot encodings. Call it like this:","221090aa":"# Data Wrangling\nData wranling is needed because a large dataset may contains many null values in it. These null values can make your model poor. So, there are three solutions to overcome those null values","3ba429f8":"##  Compare the Mean Absolute Error of each solution.","558c3332":"## Soln 3 : An Extension To Imputation","17572b1f":"The left plot shows the partial dependence between our target, Sales Price, and the Lot Area.\n\n**The partial dependence plot is calculated only after the model has been fit.**","8f57e6ef":"# Cross Validation\n\nThe diagram below shows an example of the training subsets and evaluation subsets generated in k-fold cross-validation. Here, we have total 25 instances. In first iteration we use the first 20 percent of data for evaluation, and the remaining 80 percent for training([1-5] testing and [5-25] training) while in the second iteration we use the second subset of 20 percent for evaluation, and the remaining three subsets of the data for training([5-10] testing and [1-5 and 10-25] training), and so on.\n\n![](https:\/\/cdncontribute.geeksforgeeks.org\/wp-content\/uploads\/crossValidation.jpg)\n\n**Unlike train_test_splits it does not divide the training and testing dataset into a given percentage.**","e38eb460":"1.  Choose **predicting label y**.\n2. Choose the **features x**.","470120bd":"So you can see that how the Mean Accuracy Error decresed gradually by applying different techniques.\nTry this on other dataset and see the results.\n\nComment if you have any doubts or you can give any suggestions to improve this notebook.\n> **If this notebook helped you in anyways, then Please Upvote.**","fa056bdb":"**Compare the MAE between**\n        1. Numerical predictors, where we drop categoricals.   \n        2. One-hot encoded categoricals as well as numeric predictors","0596eaf0":"In some cases this approach will meaningfully improve results. In other cases, it doesn't help at all.\n__________________________________________________________________________________________________________________________________________________________","74fdc02a":"# Submission\nThis is the way to submit your results in any of the kaggle competitions.","603d6011":"# Using Decision Tree Classifier\n\nYou can choose any model and run the dataset.","935a3a2b":"If some columns has some useful values i.e the columns with missing values. Then your model may result in an error.\n\nSo, this is not always the best solution. However it can be helpful when most values in the columns are missing.","dcad8b95":"# XGBoost\nXGBoost is an implementation of the Gradient Boosted Decision Trees algorithm.\n\n![](https:\/\/i.imgur.com\/e7MIgXk.png)\nWe go through cycles that repeatedly builds new models and combines them into an ensemble model. We start the cycle by calculating the errors for each observation in the dataset. We then build a new model to predict those. We add predictions from this error-predicting model to the \"ensemble of models.\"\n\nTo make a prediction, we add the predictions from all previous models. We can use these predictions to calculate new errors, build the next model, and add it to the ensemble.\n\nThere's one piece outside that cycle. We need some base prediction to start the cycle. In practice, the initial predictions can be pretty naive. Even if it's predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.\n\nThis process may sound complicated, but the code to use it is straightforward. We'll fill in some additional explanatory details in the model tuning section below.","3ca8aa27":"## Soln 2 : Imputation\nImputation fills in the missing value with some number i.e the mean value. The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.","1da99382":"# Partial Dependence Plot\n\nWe'll start with 2 partial dependence plots showing the relationship (according to our model) between Price and a couple variables from the Housing dataset. We'll walk through how these plots are created and interpreted.","9aaac86c":"Since string values cant be processed by any model, we convert the categorical data to numerical data using One Hot Encoding.\n\nOne Hot Encoding creates new columns and store the binary data on that particular columns to represent the presence of original data.\n\nFor Example : A gender column has two categorical value 'Male' and 'Female'. So using One Hot Encoding it create two columns named 'Male' and 'Female' and store 0 and 1 based on the presence of original data.","040ab2ed":"# Learn Machine Learning\nThis notebook will guide you to kick start with you first Machine Learning Model.\nYou can apply these techniques in other datasets for your practice. (Ex: Titanic Dataset)\n\n> **If this helps you in anyways, then Please Upvote.**","01a69cbc":"In many cases, you'll have both a training dataset and a test dataset. You will want to drop the same columns in both DataFrames. In that case, you would write","8470ef52":"# Accuracy Check","eba2cf7a":"# Random Forest","599b580e":"# Building the first model","1158713f":"# Categorize the categorical data","d22ccc30":"## Soln 1 : Drop columns with missing values"}}