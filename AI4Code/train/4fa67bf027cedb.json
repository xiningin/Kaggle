{"cell_type":{"e047054f":"code","8a9871f3":"code","9300cb17":"code","caf80db4":"code","75540dc9":"code","90e8f7a6":"code","9234d4af":"code","3cf48488":"code","6fd87182":"code","ec912edf":"code","051ffaa7":"code","052c9930":"code","382f3818":"code","575336f9":"code","ca385312":"code","a64762c1":"code","e2a3e980":"code","cda028cb":"code","6f12d555":"code","6a8fbb87":"code","59dd5e4b":"code","88fd53ac":"code","bfe09ec2":"code","b3c1b06f":"code","44c67957":"code","c22d796d":"code","38e4f0f9":"code","79ca7521":"code","2b5ec234":"code","aa1665f2":"markdown","5d124774":"markdown","69c40981":"markdown","25381762":"markdown","edd3c62e":"markdown","c6b24f56":"markdown","9f36d531":"markdown","2a670ee5":"markdown","6d1a2501":"markdown","ce03a97d":"markdown","d4429551":"markdown","d4469a71":"markdown","ae1b6b2d":"markdown","5ddef6b7":"markdown","e90090f8":"markdown","2af1b4a2":"markdown","bfebd103":"markdown","21457794":"markdown","1baf388e":"markdown","566b54a3":"markdown","b5b96501":"markdown","01b301fc":"markdown","25174746":"markdown","ff179fd9":"markdown","286917d5":"markdown","f4fd94f0":"markdown","811c26c0":"markdown"},"source":{"e047054f":"%%writefile file1.txt\nLearn NLP to play with text data","8a9871f3":"%%writefile file2.txt\nKeras is an Open Source Neural Network library written in Python that runs on top of Theano or Tensorflow","9300cb17":"%%writefile file3.txt\nSpacy has lot of features and its one of the best NLP library in Python","caf80db4":"vocabulary = {}\nnum = 1\nwith open('file1.txt') as f:\n    words = f.read().lower().split()\nfor word in words:\n    if word in vocabulary:\n        continue\n    else:\n        vocabulary[word]=num\n        num+=1\n\n    ","75540dc9":"print(vocabulary)","90e8f7a6":"with open('file2.txt') as f:\n    words = f.read().lower().split()\nfor word in words:\n    if word in vocabulary:\n        continue\n    else:\n        vocabulary[word]=num\n        num+=1","9234d4af":"print(vocabulary)","3cf48488":"with open('file3.txt') as f:\n    words = f.read().lower().split()\nfor word in words:\n    if word in vocabulary:\n        continue\n    else:\n        vocabulary[word]=num\n        num+=1","6fd87182":"print(vocabulary)","ec912edf":"# Create an empty vector with space for each word in the vocabulary:\none = ['file.txt']+[0]*len(vocabulary)\none","051ffaa7":"# map the frequencies of each word in 1.txt to our vector:\nwith open('file1.txt') as f:\n    x = f.read().lower().split()\n    \nfor word in x:\n    one[vocabulary[word]]=one[vocabulary[word]]+1\n    \none","052c9930":"two = ['file2.txt']+[0]*len(vocabulary)\n\nwith open('file2.txt') as f:\n    x = f.read().lower().split()\n    \nfor word in x:\n    two[vocabulary[word]]+=1","382f3818":"three = ['file3.txt']+[0]*len(vocabulary)\n\nwith open('file3.txt') as f:\n    x = f.read().lower().split()\n    \nfor word in x:\n    three[vocabulary[word]]+=1","575336f9":"print(f'{one}\\n{two}\\n{three}')","ca385312":"# Perform imports and load the dataset:\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding='latin')\ndf.head()","a64762c1":"df = df[['v1','v2']]","e2a3e980":"df.columns = ['label','sms']","cda028cb":"df.isnull().sum()","6f12d555":"df['label'].value_counts()","6a8fbb87":"from sklearn.model_selection import train_test_split\n\nX = df['sms']  \ny = df['label']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","59dd5e4b":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\n\nX_train_counts = count_vect.fit_transform(X_train)\nX_train_counts.shape","88fd53ac":"from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer()\n\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_train_tfidf.shape","bfe09ec2":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\nX_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\nX_train_tfidf.shape","b3c1b06f":"from sklearn.svm import LinearSVC\nclf = LinearSVC()\nclf.fit(X_train_tfidf,y_train)","44c67957":"from sklearn.pipeline import Pipeline\n\n\ntext_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC()),\n])\n\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train)","c22d796d":"# Form a prediction set\npredictions = text_clf.predict(X_test)","38e4f0f9":"# Report the confusion matrix\nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test,predictions))","79ca7521":"# Print a classification report\nprint(metrics.classification_report(y_test,predictions))","2b5ec234":"# Print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","aa1665f2":"Now that we've encapsulated our \"entire language\" in a dictionary, let's perform *feature extraction* on each of our original documents:","5d124774":"## Split the data into train & test sets:","69c40981":"**Will Update the notebook with detailed explanation of Adavanced Topics like Word2vec,Sentiment Analysis,Document Classification,Chatbots. Please hit an upvote, if you find it useful.**","25381762":"## Bag of Words and Tf-idf\nIn the above examples, each vector can be considered a *bag of words*. By itself these may not be helpful until we consider *term frequencies*, or how often individual words appear in documents. A simple way to calculate term frequencies is to divide the number of occurrences of a word by the total number of words in the document. In this way, the number of times a word appears in large documents can be compared to that of smaller documents.\n\nHowever, it may be hard to differentiate documents based on term frequency if a word shows up in a majority of documents. To handle this we also consider *inverse document frequency*, which is the total number of documents divided by the number of documents that contain the word. In practice we convert this value to a logarithmic scale, as described [here](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf#Inverse_document_frequency).\n\nTogether these terms become [**tf-idf**](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf).","edd3c62e":"**NLP**","c6b24f56":"## Train a Classifier\nHere we'll introduce an SVM classifier that's similar to SVC, called [LinearSVC](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html). LinearSVC handles sparse input better, and scales well to large numbers of samples.","9f36d531":"**Load Dataset **","2a670ee5":"<font color=blue>4825 out of 5572 messages, or 86.6%, are ham. This means that any text classification model we create has to perform **better than 86.6%** to beat random chance.<\/font>","6d1a2501":"## Scikit-learn's CountVectorizer\nText preprocessing, tokenizing and the ability to filter out stopwords are all included in [CountVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html), which builds a dictionary of features and transforms documents to feature vectors.","ce03a97d":"# Building a Natural Language Processor From Scratch\nIn this section we'll use basic Python to build a basic NLP system. We'll build a *corpus of documents* (three small text files), create a *vocabulary* from all the words in all documents, and then demonstrate a *Bag of Words* technique to extract features from each document.<br>","d4429551":"## Take a quick look at the *ham* and *spam* `label` column:","d4469a71":"## Test the classifier and display results","ae1b6b2d":"## Stop Words and Word Stems\nSome words like \"the\" and \"and\" appear so frequently, and in so many documents, that we needn't bother counting them. Also, it may make sense to only record the root of a word, say `dog` in place of both `dog` and `dogs`. This will shrink our vocab array and improve performance.","5ddef6b7":"## Combine Steps with TfidVectorizer\nIn the future, we can combine the CountVectorizer and TfidTransformer steps into one using [TfidVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html):","e90090f8":"## Vocabulary Collection\nhere we are building a numerical array from all the words that appear in every document. Later we'll create instances (vectors) for each individual document.","2af1b4a2":"# Feature Extraction","bfebd103":"## Tokenization and Tagging\nWhen we created our vectors the first thing we did was split the incoming text on whitespace with `.split()`. This was a crude form of *tokenization* - that is, dividing a document into individual words. In this simple example we didn't worry about punctuation or different parts of speech. In the real world we rely on some fairly sophisticated *morphology* to parse text appropriately.\n\nOnce the text is divided, we can go back and *tag* our tokens with information about parts of speech, grammatical dependencies, etc. This adds more dimensions to our data and enables a deeper understanding of the context of specific documents. For this reason, vectors become ***high dimensional sparse matrices***.","21457794":"This notebook is divided into two sections:\n* Building a Natural Language Processor From Scratch.\n* Next we'll show how to perform these steps using real tools.","1baf388e":"## Transform Counts to Frequencies with Tf-idf\nWhile counting words is helpful, longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n\nTo avoid this we can simply divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called **tf** for Term Frequencies.\n\nAnother refinement on top of **tf** is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n\nThis downscaling is called **tf\u2013idf** for \u201cTerm Frequency times Inverse Document Frequency\u201d.\n\nBoth tf and tf\u2013idf can be computed as follows using [TfidfTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfTransformer.html):","566b54a3":"## Check for missing values:","b5b96501":"Using the text of the messages, our model performed exceedingly well with high accuracy<br>","01b301fc":"## Build a Pipeline\nRemember that only our training set has been vectorized into a full vocabulary. In order to perform an analysis on our test set we'll have to submit it to the same procedures. Fortunately scikit-learn offers a [**Pipeline**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html) class that behaves like a compound classifier.","25174746":"Note: the `fit_transform()` method actually performs two operations: it fits an estimator to the data and then transforms our count-matrix to a tf-idf representation.","ff179fd9":"<font color=violet>This shows that our training set is comprised of 3733 documents, and 7082 features.<\/font>","286917d5":"By comparing the vectors we see that some words are common to all docs, some appear only in `file1.txt`, some appear only in `file2.txt` others only in `file3.txt`. Extending this logic to tens of thousands of documents, we would see the vocabulary dictionary grow to hundreds of thousands of words. Vectors would contain mostly zero values, making them *sparse matrices*.","f4fd94f0":"**Create a text file in a jupyter notebook** <br\/>\nuse %%writefile *filename* to create a text file in jupyter notebook ","811c26c0":"# Feature Extraction from Text Using Scikit-learn\nIn this section we'll actually look at the text of each message in SMSSpamCollection dataset and try to perform a classification based on content using scikit-learn's [feature extraction](https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction) tools."}}