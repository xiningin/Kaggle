{"cell_type":{"877dba61":"code","46b258df":"code","bcbe21f8":"code","4dd44992":"code","74ec5de5":"code","d1720947":"code","ed059890":"code","d833777b":"code","079b5206":"code","713c9e5e":"code","8d1b4264":"code","d84cf956":"code","679c2592":"markdown","e7c3bc6a":"markdown","b9b838ec":"markdown","0fc4b12f":"markdown","8875afef":"markdown","36766ae4":"markdown","2a569d8d":"markdown"},"source":{"877dba61":"# some imports\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\n\n# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n \n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nplt.rc('font', size=12) \nplt.rc('figure', figsize = (12, 5))\n\n# Settings for the visualizations\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})\n\nimport pandas as pd\npd.set_option('display.max_rows', 25)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', 50)\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n\n# create output folder\nif not os.path.exists('output'):\n    os.makedirs('output')\nif not os.path.exists('output\/session1'):\n    os.makedirs('output\/session1')","46b258df":"## load data\ntrain_set = pd.read_csv('\/kaggle\/input\/mlub-housing-house-prediction\/train_set.csv',index_col=0) \ntest_set = pd.read_csv('\/kaggle\/input\/mlub-housing-house-prediction\/test_set.csv',index_col=0) ","bcbe21f8":"# print the dataset size\nprint(\"There is\", train_set.shape[0], \"samples\")\nprint(\"Each sample has\", train_set.shape[1], \"features\")","4dd44992":"# print the top elements from the dataset\ntrain_set.head()","74ec5de5":"# As it can be seen the database contains several features, some of them numerical and some of them are categorical.\n# It is important to check each of the to understand it.","d1720947":"# we can see the type of each features as follows\ntrain_set.dtypes","ed059890":"# print those categorical features\ntrain_set.select_dtypes(include=['object']).head()","d833777b":"# We can check how many different type there is in the dataset using the folliwing line\ntrain_set[\"Type\"].value_counts()","079b5206":"sns.countplot(y=\"Type\", data=train_set, color=\"c\")","713c9e5e":"sns.distplot(train_set[\"Price\"])\nplt.show()","8d1b4264":"## the features\n\nfeatures = ['Rooms','Landsize', 'BuildingArea', 'YearBuilt']\n## DEFINE YOUR FEATURES\nX = train_set[features].fillna(0)\ny = train_set[['Price']]\n\n## the model\n# KNeighborsRegressor\nfrom sklearn import neighbors\nn_neighbors = 3 # you can modify this paramenter (ONLY THIS ONE!!!)\nmodel = neighbors.KNeighborsRegressor(n_neighbors)\n\n## fit the model\nmodel.fit(X, y)\n\n## predict training set\ny_pred = model.predict(X)\n\n## Evaluate the model and plot it\nfrom sklearn.metrics import mean_squared_error, r2_score\nprint(\"----- EVALUATION ON TRAIN SET ------\")\nprint(\"RMSE\",np.sqrt(mean_squared_error(y, y_pred)))\nprint(\"R^2: \",r2_score(y, y_pred))\n\n\nplt.scatter(y, y_pred)\nplt.xlabel('Price')\nplt.ylabel('Predicted price');\nplt.show()\n\n## predict the test set and generate the submission file\nX_test = test_set[features].fillna(0)\ny_pred = model.predict(X_test)\n\ndf_output = pd.DataFrame(y_pred)\ndf_output = df_output.reset_index()\ndf_output.columns = ['index','Price']\n\ndf_output.to_csv('output\/session1\/baseline.csv',index=False)","d84cf956":"## the features\nfeatures = ['Rooms','Bathroom','Car','Distance','Postcode','Landsize','BuildingArea','Propertycount','Lattitude','Longtitude','YearBuilt']\nX = train_set[features].copy()\n\nX['BuildingArea']=X['BuildingArea'].fillna(X['Landsize']-(X['Rooms']*50))\nX['YearBuilt'] = X['YearBuilt'].fillna(X['YearBuilt'].mean())\nX['Car'] = X['Car'].fillna(X['Rooms']\/2)\n\n#New feature: bathroom per bedroom\nbxb = X['Bathroom']\/X['Rooms']\nX['BxB'] = bxb \n\n#New feature: Not builded land\nfa = X['Landsize']-X['BuildingArea']\nX['FreeArea'] = fa \n\n#Single column to multiples columns:Type\nimport pandas as pd\ntypes = pd.get_dummies(train_set[['Type']])\nX = pd.concat([X, types],axis=1)\n\nca = pd.get_dummies(train_set[['CouncilArea']])\nX = pd.concat([X, ca,],axis=1)\n\nrn = pd.get_dummies(train_set[['Regionname']])\nX = pd.concat([X, rn],axis=1)\n\n#Normalizing data\na, b = 0,1\nx, y = X.min(), X.max()\nX = (X - x) \/ (y - x) * (b - a) + a\n\ny= train_set[['Price']]\n\n## the model\n# KNeighborsRegressor\nfrom sklearn import neighbors\nn_neighbors = 3 # you can modify this paramenter (ONLY THIS ONE!!!)\nmodel = neighbors.KNeighborsRegressor(n_neighbors)\n\n\n## fit the model\nmodel.fit(X, y)\n\n## predict training set\ny_pred = model.predict(X)\n\n## Evaluate the model and plot it\nfrom sklearn.metrics import mean_squared_error, r2_score\nprint(\"----- EVALUATION ON TRAIN SET ------\")\nprint(\"RMSE\",np.sqrt(mean_squared_error(y, y_pred)))\nprint(\"R^2: \",r2_score(y, y_pred))\n\n\nplt.scatter(y, y_pred)\nplt.xlabel('Price')\nplt.ylabel('Predicted price');\nplt.show()\n\n## predict the test set and generate the submission file\nX_test = test_set[features].copy()\nX_test['BuildingArea']=X_test['BuildingArea'].fillna(X_test['Landsize']-(X_test['Rooms']*50))\nX_test['YearBuilt'] = X_test['YearBuilt'].fillna(X_test['YearBuilt'].mean())\nX_test['Car'] = X_test['Car'].fillna(X_test['Rooms']\/2)\n\n#New feature: bathroom per bedroom\nbxb = X_test['Bathroom']\/X_test['Rooms']\nX_test['BxB'] = bxb \n\n#New feature: Not builded land\nfa = X_test['Landsize']-X_test['BuildingArea']\nX_test['FreeArea'] = fa \n\n#Single column to multiples columns:Type\ntypes = pd.get_dummies(test_set[['Type']])\nX_test = pd.concat([types, X_test], axis=1)\n\nca = pd.get_dummies(test_set[['CouncilArea']])\nX_test = pd.concat([ca, X_test], axis=1)\n\nrn = pd.get_dummies(test_set[['Regionname']])\nX_test = pd.concat([rn, X_test], axis=1)\n\na, b = 0,1\nx, y = X.min(), X.max()\nX = (X - x) \/ (y - x) * (b - a) + a\n\ny_pred = model.predict(X_test)\n\ndf_output = pd.DataFrame(y_pred)\ndf_output = df_output.reset_index()\ndf_output.columns = ['index','Price']\n\ndf_output.to_csv('output\/session1\/baseline.csv',index=False)","679c2592":"Explain the choosed model and why you think that is is the best","e7c3bc6a":"# Session 1 - Your \"first\" DataScience problem","b9b838ec":"The main reason to use KNeighborsRegresor was to be able to use the post code, the lattitude and the longtitude as they are categorical data and they don't have linear relation.\n\nThis is the best model because, after making several attempts with different feature combinations, the one that had the best results was this one.\n\nFor the features that had null values, I've tried to find a suitable way to fill them so as to get a more accurate model.\n\nIn addition, I've incorporated new features that represents important characteristics. These are the following:\n\n+ Bathroms per bedroom: describes the amount of bathrooms per bedroom. The amount of bathrooms could increase the prize.\n+ Free area: suposing that building area is the amount of land builded and that landsize is all the available land, this feature would represent the garden area. In this case, the amount of garden could make de house mouch more expensive.\n\nFinally, some features have been modified to be able to fit in this model. For example, the ones that weren't numerical have been converted. This conversion ended with a big amount of columns with binary values that led me to the decision of normalising all the data.","0fc4b12f":"## EXERCICE - TRAIN A MODEL and upload your best solution to the Kaggle Challenge\n\nTasks: \n+ Choose the best features for the problem. Some features are numercial but others categorical, think how to codify all of them.\n+ Choose the model. You have two choice: LinearRegression and KNeighborsRegressor\n    + You can chanege the paramenter n_neighbors from the KNeighborsRegressor\n    \nLINEAR REGRESSOR:\n>from sklearn.linear_model import LinearRegression\n\n>model = LinearRegression()\n\nKNeighborsRegressor\n> from sklearn import neighbors\n\n> n_neighbors = 3 # you can modify this paramenter (ONLY THIS ONE!!!)\n\n> model = neighbors.KNeighborsRegressor(n_neighbors=n_neighbors)\n","8875afef":"It would be interesting to visualize all features (numerical and catergorical) in order to undertand them.\n\nCheck out this blog for plotting distribution: https:\/\/seaborn.pydata.org\/tutorial\/distributions.html\n+ Seaborn version of this blog can be different from the one intalled in your machine (version 0.11 has been just realeased)\n\nCheck out this blog for plotting categorical data: https:\/\/seaborn.pydata.org\/tutorial\/categorical.html","36766ae4":"## The problem\nThe machine learning is to predict the house price, but before that it is imporntat to study the dataset and its features","2a569d8d":"## BASELINE MODEL\n### https:\/\/www.kaggle.com\/c\/mlub-housing-house-prediction\/notebooks\n\nThis is a simple model that uses the K-nearest Neighbors Regressor\n\nThis model only uses 4 feaures: 'Rooms','Landsize', 'BuildingArea', 'YearBuilt'"}}