{"cell_type":{"4be0f028":"code","5267152b":"code","04f2f466":"code","b7b17d83":"code","4db3d19d":"code","fe906d19":"code","d7344906":"code","8eb406f5":"code","471f5f64":"code","7c7a6d5d":"code","ed15f47f":"code","b6ad66e0":"code","8118018f":"code","6c6a8ad6":"code","64a1058e":"code","988e673e":"code","fb2ac382":"code","b86e27a1":"code","9dbb02c5":"code","f1f52086":"code","c27fa348":"code","dbe62d1f":"code","11468221":"code","d9c4a95b":"code","8870bb61":"code","17e8f091":"code","c7844f5e":"code","059fae0b":"code","0c73ed4f":"code","432fa6d6":"code","b9dcea2e":"code","82b24dcd":"code","98f78100":"code","5ebb6aed":"code","abf4354a":"code","03ce94e4":"code","2a66efd1":"code","4950cda1":"code","825fe1de":"code","f4aa5b06":"code","35ebf32d":"code","67c79788":"code","6abf1af6":"code","6703a7ee":"code","bb3f4229":"code","e2a12316":"code","33c7e258":"code","13dda0dc":"code","ee3cc159":"code","1715f95d":"code","a1d87145":"code","004e75f3":"code","17b3c247":"code","989f2f04":"code","68a62376":"code","005bfd62":"code","01f4ea5e":"code","4faca969":"code","cac2a45c":"code","319a2706":"code","a1052dca":"code","75cfba52":"code","63df6ecf":"code","a7b1fc0f":"code","f886e1de":"code","950d68a2":"markdown","8d4a0709":"markdown","dc71faea":"markdown","96dc0317":"markdown","67ba9876":"markdown","0961f587":"markdown","6b1d91af":"markdown","afcb23d5":"markdown","ae0d92c0":"markdown","7d7e5b67":"markdown","01d39116":"markdown","0c2c87db":"markdown","ffa34414":"markdown","df907730":"markdown","1020c8ac":"markdown","3f3be5a8":"markdown","79791f97":"markdown","126a895e":"markdown","81cb02ba":"markdown","fa1cee19":"markdown","5a0a3455":"markdown","6af63943":"markdown","77b5a6b9":"markdown","ae3c8178":"markdown"},"source":{"4be0f028":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport operator\n#import gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import describe\n%matplotlib inline\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb","5267152b":"print(os.listdir(\"..\/input\"))","04f2f466":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","b7b17d83":"train_df.shape","4db3d19d":"train_df.info()","fe906d19":"train_df.isnull().values.any()","d7344906":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head()","8eb406f5":"test_df.shape","471f5f64":"test_df.info()","7c7a6d5d":"test_df.isnull().values.any()","ed15f47f":"train_df_describe = train_df.describe()\ntrain_df_describe","b6ad66e0":"test_df_describe = test_df.describe()\ntest_df_describe","8118018f":"plt.figure(figsize=(12, 5))\nplt.hist(train_df.target.values, bins=100)\nplt.title('Histogram target counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","6c6a8ad6":"plt.figure(figsize=(12, 5))\nplt.hist(np.log(1+train_df.target.values), bins=100)\nplt.title('Histogram target counts')\nplt.xlabel('Count')\nplt.ylabel('Log 1+Target')\nplt.show()","64a1058e":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=np.log(1+train_df.target.values))\nplt.show()","988e673e":"train_log_target = train_df[['target']]\ntrain_log_target['target'] = np.log(1+train_df['target'].values)\ntrain_log_target.describe()","fb2ac382":"constant_train = train_df.loc[:, (train_df == train_df.iloc[0]).all()].columns.tolist()\nconstant_test = test_df.loc[:, (test_df == test_df.iloc[0]).all()].columns.tolist()","b86e27a1":"print('Number of constant columns in the train set:', len(constant_train))\nprint('Number of constant columns in the test set:', len(constant_test))","9dbb02c5":"columns_to_use = test_df.columns.tolist()\ndel columns_to_use[0] # Remove 'ID'\ncolumns_to_use = [x for x in columns_to_use if x not in constant_train] #Remove all 0 columns\nlen(columns_to_use)","f1f52086":"describe(train_df[columns_to_use].values, axis=None)","c27fa348":"plt.figure(figsize=(12, 5))\nplt.hist(train_df[columns_to_use].values.flatten(), bins=50)\nplt.title('Histogram all train counts')\nplt.xlabel('Count')\nplt.ylabel('Value')\nplt.show()","dbe62d1f":"plt.figure(figsize=(12, 5))\nplt.hist(np.log(train_df[columns_to_use].values.flatten()+1), bins=50)\nplt.title('Log Histogram all train counts')\nplt.xlabel('Count')\nplt.ylabel('Log value')\nplt.show()","11468221":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=np.log(train_df[columns_to_use].values.flatten()+1))\nplt.show()","d9c4a95b":"train_nz = np.log(train_df[columns_to_use].values.flatten()+1)\ntrain_nz = train_nz[np.nonzero(train_nz)]\nplt.figure(figsize=(12, 5))\nplt.hist(train_nz, bins=50)\nplt.title('Log Histogram nonzero train counts')\nplt.xlabel('Count')\nplt.ylabel('Log value')\nplt.show()","8870bb61":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=train_nz)\nplt.show()","17e8f091":"describe(train_nz)","c7844f5e":"test_nz = np.log(test_df[columns_to_use].values.flatten()+1)\ntest_nz = test_nz[np.nonzero(test_nz)]\nplt.figure(figsize=(12, 5))\nplt.hist(test_nz, bins=50)\nplt.title('Log Histogram nonzero test counts')\nplt.xlabel('Count')\nplt.ylabel('Log value')\nplt.show()","059fae0b":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=test_nz)\nplt.show()","0c73ed4f":"describe(test_nz)","432fa6d6":"train_df[columns_to_use].values.flatten().shape","b9dcea2e":"((train_df[columns_to_use].values.flatten())==0).mean()","82b24dcd":"train_zeros = pd.DataFrame({'Percentile':((train_df[columns_to_use].values)==0).mean(axis=0),\n                           'Column' : columns_to_use})\ntrain_zeros.head()","98f78100":"describe(train_zeros.Percentile.values)","5ebb6aed":"plt.figure(figsize=(12, 5))\nplt.hist(train_zeros.Percentile.values, bins=50)\nplt.title('Histogram percentage zeros train counts')\nplt.xlabel('Count')\nplt.ylabel('Value')\nplt.show()","abf4354a":"describe(np.log(train_df[columns_to_use].values+1), axis=None)","03ce94e4":"describe(test_df[columns_to_use].values, axis=None)","2a66efd1":"describe(np.log(test_df[columns_to_use].values+1), axis=None)","4950cda1":"test_zeros = pd.DataFrame({'Percentile':(np.log(1+test_df[columns_to_use].values)==0).mean(axis=0),\n                           'Column' : columns_to_use})\ntest_zeros.head()","825fe1de":"describe(test_zeros.Percentile.values)","f4aa5b06":"y = np.log(1+train_df.target.values)\ny.shape","35ebf32d":"y","67c79788":"train = lgb.Dataset(train_df[columns_to_use],y ,feature_name = \"auto\")","6abf1af6":"params = {'boosting_type': 'gbdt', \n          'objective': 'regression', \n          'metric': 'rmse', \n          'learning_rate': 0.01, \n          'num_leaves': 100, \n          'feature_fraction': 0.4, \n          'bagging_fraction': 0.6, \n          'max_depth': 5, \n          'min_child_weight': 10}\n\n\nclf = lgb.train(params,\n        train,\n        num_boost_round = 400,\n        verbose_eval=True)","6703a7ee":"\npreds = clf.predict(test_df[columns_to_use])\npreds","bb3f4229":"sample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsample_submission.target = np.exp(preds)-1\nsample_submission.to_csv('simple_lgbm_1.csv', index=False)\nsample_submission.head()","e2a12316":"nr_splits = 5\nrandom_state = 1054\n\ny_oof = np.zeros((y.shape[0]))\ntotal_preds = 0\n\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train_df[columns_to_use].iloc[train_index], train_df[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds += clf.predict(test_df[columns_to_use])\/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof)))","33c7e258":"params['max_depth'] = 4\n\ny_oof_2 = np.zeros((y.shape[0]))\ntotal_preds_2 = 0\n\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train_df[columns_to_use].iloc[train_index], train_df[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds_2 += clf.predict(test_df[columns_to_use])\/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof_2[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_2)))","13dda0dc":"params['max_depth'] = 6\n\ny_oof_3 = np.zeros((y.shape[0]))\ntotal_preds_3 = 0\n\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train_df[columns_to_use].iloc[train_index], train_df[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds_3 += clf.predict(test_df[columns_to_use])\/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof_3[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_3)))","ee3cc159":"params['max_depth'] = 7\n\ny_oof_4 = np.zeros((y.shape[0]))\ntotal_preds_4 = 0\n\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train_df[columns_to_use].iloc[train_index], train_df[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds_4 += clf.predict(test_df[columns_to_use])\/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof_4[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_4)))","1715f95d":"params['max_depth'] = 8\n\ny_oof_5 = np.zeros((y.shape[0]))\ntotal_preds_5 = 0\n\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train_df[columns_to_use].iloc[train_index], train_df[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds_5 += clf.predict(test_df[columns_to_use])\/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof_5[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_5)))","a1d87145":"params['max_depth'] = 10\n\ny_oof_6 = np.zeros((y.shape[0]))\ntotal_preds_6 = 0\n\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train_df[columns_to_use].iloc[train_index], train_df[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds_6 += clf.predict(test_df[columns_to_use])\/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof_6[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_6)))","004e75f3":"params['max_depth'] = 12\n\ny_oof_7 = np.zeros((y.shape[0]))\ntotal_preds_7 = 0\n\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train_df[columns_to_use].iloc[train_index], train_df[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds_7 += clf.predict(test_df[columns_to_use])\/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof_7[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_7)))","17b3c247":"print('Total error', np.sqrt(mean_squared_error(y, 1.4*(1.6*y_oof_7-0.6*y_oof_6)-0.4*y_oof_5)))","989f2f04":"print('Total error', np.sqrt(mean_squared_error(y, -0.5*y_oof-0.5*y_oof_2-y_oof_3\n                                                +3*y_oof_4)))","68a62376":"print('Total error', np.sqrt(mean_squared_error(y, 0.75*(1.4*(1.6*y_oof_7-0.6*y_oof_6)-0.4*y_oof_5)+\n                                                0.25*(-0.5*y_oof-0.5*y_oof_2-y_oof_3\n                                                +3*y_oof_4))))","005bfd62":"sub_preds = (0.75*(1.4*(1.6*total_preds_7-0.6*total_preds_6)-0.4*total_preds_5)+\n                                                0.25*(-0.5*total_preds-0.5*total_preds_2-total_preds_3\n                                                +3*total_preds_4))\n#sub_preds = (-0.5*total_preds-0.5*total_preds_2-total_preds_3+3*total_preds_4)\nsample_submission.target = np.exp(sub_preds)-1\nsample_submission.to_csv('blended_submission_2.csv', index=False)\nsample_submission.head()","01f4ea5e":"params = {'objective': 'reg:linear', \n          'eval_metric': 'rmse',\n          'eta': 0.01,\n          'max_depth': 10, \n          'subsample': 0.6, \n          'colsample_bytree': 0.6,\n          'alpha':0.001,\n          'random_state': 42, \n          'silent': True}\n\ny_oof_8 = np.zeros((y.shape[0]))\ntotal_preds_8 = 0\n\ndtest = xgb.DMatrix(test_df[columns_to_use])\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train_df[columns_to_use].iloc[train_index], train_df[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    \n    train = xgb.DMatrix(X_train, y_train)\n    val = xgb.DMatrix(X_val, y_val)\n    \n    watchlist = [(train, 'train'), (val, 'val')]\n    \n    clf = xgb.train(params, train, 1000, watchlist, \n                          maximize=False, early_stopping_rounds = 60, verbose_eval=100)\n\n    \n    total_preds_8 += clf.predict(dtest, ntree_limit=clf.best_ntree_limit)\/nr_splits\n    pred_oof = clf.predict(val, ntree_limit=clf.best_ntree_limit)\n    y_oof_8[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_8)))","4faca969":"print('Total error', np.sqrt(mean_squared_error(y, 0.7*(0.75*(1.4*(1.6*y_oof_7-0.6*y_oof_6)-0.4*y_oof_5)+\n                                                0.25*(-0.5*y_oof-0.5*y_oof_2-y_oof_3\n                                                +3*y_oof_4))+0.3*y_oof_8)))","cac2a45c":"sub_preds = (0.7*(0.75*(1.4*(1.6*total_preds_7-0.6*total_preds_6)-0.4*total_preds_5)+\n                                                0.25*(-0.5*total_preds-0.5*total_preds_2-total_preds_3\n                                                +3*total_preds_4))+0.3*total_preds_8)\n#sub_preds = (-0.5*total_preds-0.5*total_preds_2-total_preds_3+3*total_preds_4)\nsample_submission.target = np.exp(sub_preds)-1\nsample_submission.to_csv('blended_submission_3.csv', index=False)\nsample_submission.head()","319a2706":"feature_importances = clf.get_fscore()","a1052dca":"importance = sorted(feature_importances.items(), key=operator.itemgetter(1))","75cfba52":"best_2500 = importance[::-1][:2500]","63df6ecf":"best_2500 =[ x[0] for x in best_2500]","a7b1fc0f":"params = {'objective': 'reg:linear', \n          'eval_metric': 'rmse',\n          'eta': 0.01,\n          'max_depth': 10, \n          'subsample': 0.6, \n          'colsample_bytree': 0.6,\n          'alpha':0.001,\n          'random_state': 42, \n          'silent': True}\n\ny_oof_9 = np.zeros((y.shape[0]))\ntotal_preds_9 = 0\n\ndtest = xgb.DMatrix(test_df[best_2500])\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train_df[best_2500].iloc[train_index], train_df[best_2500].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    \n    train = xgb.DMatrix(X_train, y_train)\n    val = xgb.DMatrix(X_val, y_val)\n    \n    watchlist = [(train, 'train'), (val, 'val')]\n    \n    clf = xgb.train(params, train, 1500, watchlist, \n                          maximize=False, early_stopping_rounds = 60, verbose_eval=100)\n\n    \n    total_preds_9 += clf.predict(dtest, ntree_limit=clf.best_ntree_limit)\/nr_splits\n    pred_oof = clf.predict(val, ntree_limit=clf.best_ntree_limit)\n    y_oof_9[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_9)))","f886e1de":"print('Total error', np.sqrt(mean_squared_error(y, 0.5*y_oof_9+0.5*(0.7*(0.75*(1.4*(1.6*y_oof_7-0.6*y_oof_6)-0.4*y_oof_5)+\n                                                0.25*(-0.5*y_oof-0.5*y_oof_2-y_oof_3\n                                                +3*y_oof_4))+0.3*y_oof_8))))\n\nsub_preds = 0.5*total_preds_9+0.5*(0.7*(0.75*(1.4*(1.6*total_preds_7-0.6*total_preds_6)-0.4*total_preds_5)+\n                                                0.25*(-0.5*total_preds-0.5*total_preds_2-total_preds_3\n                                                +3*total_preds_4))+0.3*total_preds_8)\n#sub_preds = (-0.5*total_preds-0.5*total_preds_2-total_preds_3+3*total_preds_4)\nsample_submission.target = np.exp(sub_preds)-1\nsample_submission.to_csv('blended_submission_4.csv', index=False)\nsample_submission.head()","950d68a2":"Now let us look at the input folder. Here we find all the relevant files for this competition.","8d4a0709":"A few things to notice:\n\n1.  Target variable ranges over 4 orders of magnitude. (factor of 10,000)\n2. Most features have 0.0 for 75% - another indication that we are probably dealing with sparse data.\n3. Most features seem to have similarly wide spread of values as the target variable. Hmm, interesting ...\n4. The standard deviation for most features seems larger than the feature mean. \n5.. There are a few features (such as ```d5308d8bc```, ```c330f1a67```) that seem to be filled with zeros. These will need to be eliminated.\n\nNow let's look at the test set. ","dc71faea":"This is a highly skewed distribution, so let's try to re-plot it with with log transform of the target.","96dc0317":"Again, we see that these distributions look similar, but they are definitely not the same. \n\nNow let's take a closer look at the shape and content of the train data. We want to get a better numerical grasp of the true extent of zeros.","67ba9876":"That's ... revealing. And it looks like a fairly nice distribution, albeit still fairly asymetrical.\n\nLet's take a look at the statistics of the Log(1+target)","0961f587":"OK, that's much more interesting. \n\nLet's do the same thing with the test data.","6b1d91af":"As expected, this distribution looks much more, ahem, normal. This is probably one of the main reasons why the metric that we are trying to optimize for this competition is RMSLE - root mean square logarithmic error.\n\nAnother way of looking at the same distribution is with the help of violinplot.","afcb23d5":"A few things immediately stand out:\n\n1. As advertised, features are numeric and anonymized. \n2. Features seem sparse. We'll have to investigate this further. \n3. There are a LOT of features! Almost 5000! And they outnumber the number of rows in the training set!\n4. There are less than 5000 training rows. In fact, there are fewer rows than columns, which means we'll have to invest a lot of effort into feature selection \/ feature engineering. \n5. The memory size of the train dataset is fairly large - 170 MB, which is to be expected. \n6. Pandas is treating 1845 features as float, and 3147 as integer. It is possible that some of those int features are one-hot-encoded or label-encoded categorical variables. We'll have to investigate this later, and possibly do some reverse-engineering. :)\n7. There doesn't appear to be any missing values in the train set. This is, IMHO, overall a good thing, although a lot of times there is some signal in the missing values that's valuable and worth exploring. \n\nThis is going to be a very, very, interesting competition. :)\n\nNow let's take a look at the test dataset.","ae0d92c0":"So it seems that the vast majority of columns have 95+ percent of zeros in them. Let's see how would that look on a plot.","7d7e5b67":"### To do:\n\n1. Meke some plots\n2. Build a few models\n3. Do feature importance analysis\n\n## To be continued ...","01d39116":"Well, that's great - we made a prediction on the test set, and saved it to a file, which we were able to submit to the competition. Unfortunately, there was no way to tell how this model would perform on the unseen data. (This submission scored 1.53 on Public Leaderboard.)","0c2c87db":"Not really - the plot looks nicer, but the overall shape is pretty much the same. \n\nOK, let's take a look at the distribution of non-zero values.","ffa34414":"So as we suspected, almost 97% of all values in the train dataframe are zeros. That looks pretty sparse to me, but let's see how much variation is there between different columns.","df907730":"If we treat all the train matrix values as if they belonged to a single row vector, we see a huge amount of varience, far exceeding the similar variance for the target variable.\n\nNow let's plot it to see how diverse the numerical values are.","1020c8ac":"We see that the input folder only contains three files ```train.csv```, ```test.csv```, and ```sample_submission.csv```. It seems that for this competition we don't have to do any complicated combination and mergers of files.\n\nNow let's import and take a glimpse at these files.","3f3be5a8":"We see that the statistical properties of teh Log(1+Target) distribution are much more amenable.\n\nNow let's take a look at columns with constant value.","79791f97":"Wow, not very diverse at all! Most of the values are heavily concentrated around 0. \n\nMaybe if we used the log plot things would be better.","126a895e":"## Overview\n\nThe purpose of this kernel is to take a look at the data, come up with some insights, and attempt to create a predictive model or two. So let's get started!\n\n## Packages\n\nFirst, let's load a few useful Python packages. This section will keep growing in subsequent versions of this EDA.","81cb02ba":"Only marginal improvement - there is a verly small bump close to 15.\n\nCan the violin plot help?","fa1cee19":"We see a similar distribution of various statistical aggregates, but by no means the same: seems like there soem substantial distribution shifts between the train and test sets. This will probably be another major concern when it comes to feature selection\/engineering. \n\nNow let's do some plotting. We'll take a look at, naturally, the ```target``` variable. First, let's make a histogram of its raw value.","5a0a3455":"So this is interesting: there are 256 constant columns in the train set, but none in the test set. These constant columns are thus most likely an artifact of the way that the train and test sets were constructed, and not necessarily irrelevant in their own right. This is yet another byproduct of having a very small dataset. For most problems it would be useful to take a look at the description of these columns, but in this competition they are anonymized, and thus would not yield any useful information. \n\nSo let's subset the colums that we'd use to just those that are not constant.","6af63943":"OK, let's try to do some modeling. We'll start with a simple LighGBM regression, and see if that yields any results. First, let's set our target variable to be the log of 1 + target.","77b5a6b9":"So we have the total of 4735 columns to work with. However, as mentioned earlier, most of these columns seem to be filled predominatly with zeros. Let's try to get a better sense of this data.","ae3c8178":"Here we see that the number of features in the test set (4992) matches the number in the train set. Sanity check is always a good thing, and at leas at this level, hte Kaggle people did not mess things up.\n\nAccording to Pandas, there are no ```int``` values in the test set. Now I'm really curious about those ... \n\nThere also doesn't seem to be any missing values in the test set. \n\nWe also see that the number of rows in the test set far surpasses the number of rows in the train set. Yes, a very *interesting* competition indeed ...\n\n\n\nNow let's see some basic descriptive statistics for the train and test dataframes."}}