{"cell_type":{"c0579633":"code","32790bf2":"code","f2c53962":"code","d03b69f9":"code","880f74d9":"code","bb2b254a":"code","954b390c":"code","9b3d7316":"code","5ada128a":"code","cafa5d98":"code","f0c22e7b":"code","9ef684c3":"code","7f085377":"code","8298cd32":"code","caa2b4b3":"code","9a592f60":"code","ba67388c":"code","7db59712":"code","00a13f31":"code","d30610b5":"code","7f838c5d":"code","60ed2313":"code","5a6a62b9":"code","b5f6c026":"code","fde85adf":"code","354cf6d9":"code","b732481b":"code","61c1223b":"code","6154e876":"code","9fab8d81":"code","4dcca196":"code","73abb062":"code","e8ab3bd9":"code","c494231a":"code","ba5c55a1":"code","40d90880":"code","d120c42b":"code","43bfb096":"code","f7e3ccd3":"code","96087245":"code","c0899966":"code","66853e6a":"code","7c2a6da2":"code","57d16049":"code","aa97277c":"code","9613738c":"code","187c15a3":"code","ec18f911":"code","0be97198":"code","230ce392":"code","76fcb6c0":"markdown","07db984e":"markdown","b804a9b2":"markdown","030f38aa":"markdown","194659ea":"markdown","f9df4764":"markdown","a733065e":"markdown","6a590fd1":"markdown","1b9a1d88":"markdown","b01d6793":"markdown","b0e10b97":"markdown","f6ad23fe":"markdown","7dbc90ed":"markdown","3a8bc7a2":"markdown","c4701f90":"markdown","4ca0c8da":"markdown","ff4a4a7d":"markdown","eac1f6ed":"markdown","297ff917":"markdown","1a97b09a":"markdown","b91354df":"markdown","8d6fbaab":"markdown","ae29a7fd":"markdown"},"source":{"c0579633":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\n%matplotlib inline\n\nfrom sklearn.preprocessing import MinMaxScaler,LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\nfrom sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, plot_confusion_matrix\n\n#model selction tools\nfrom sklearn.model_selection import train_test_split,cross_val_score\n#models \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.neural_network import MLPClassifier\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","32790bf2":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)","f2c53962":"train_path = \"\/kaggle\/input\/titanic\/train.csv\"\ntest_path  = \"\/kaggle\/input\/titanic\/test.csv\"\n\n\ntrain_data = pd.read_csv(train_path,index_col='PassengerId')\ntest_data = pd.read_csv(test_path)\n\nprint(\"train_data shape:\",train_data.shape)\ntrain_data.head()","d03b69f9":"train_data.info()","880f74d9":"train_data.describe()","bb2b254a":"train_data.nunique().sort_values()","954b390c":"train_data.isna().sum()","9b3d7316":"(train_data.isna().sum()\/len(train_data)*100).sort_values(ascending=False)","5ada128a":"#drop NaN rows from Embarked column \ntrain_data.dropna(axis=0,subset=['Embarked'],inplace=True)\n#drop Cabin column\ntrain_data.drop(['Cabin'], axis=1,inplace=True)","cafa5d98":"#impute Age column with the median\ntrain_data['Age'] = train_data.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\n# train_data['Age'].fillna(train_data['Age'].median(),inplace=True)","f0c22e7b":"(train_data.isna().sum()\/len(train_data)*100).sort_values(ascending=False)","9ef684c3":"train_data['Age'].unique()","7f085377":"sum(train_data['Age']<1)","8298cd32":"#Age less than 1 year change it to 1 year\ntrain_data.loc[train_data['Age']<1,['Age']]= 1","caa2b4b3":"sum(train_data['Age']<1)","9a592f60":"#drop 'Name','Ticket' columns as they don't give us important information\ntrain_data.drop(['Name','Ticket'], axis=1,inplace=True)","ba67388c":"train_data","7db59712":"# Use LabelEncoder on 'Sex' column\nle = LabelEncoder()\ntrain_data['Sex_encoded'] = le.fit_transform(train_data['Sex'])\nprint(f\"classes encoded for Sex are {le.classes_}\")\ntrain_data","00a13f31":"plt.figure(figsize=(14,8))\nsns.countplot(x=\"Survived\", data=train_data,hue='Sex', palette=\"Set3\")\nplt.show()","d30610b5":"sns.displot(data=train_data, x=\"Age\",col=\"Sex\",hue=\"Survived\");","7f838c5d":"plt.figure(figsize=(14,8))\nsns.countplot(x=\"Survived\", data=train_data,hue='Pclass', palette=\"Set3\")\nplt.show()","60ed2313":"plt.figure(figsize=(14,8))\nsns.countplot(x=\"Survived\", data=train_data,hue='Embarked', palette=\"Set3\")\nplt.show()","5a6a62b9":"#drop 'Embarked' column\ntrain_data.drop(['Embarked'], axis=1,inplace=True)","b5f6c026":"plt.figure(figsize=(10,6))\nsns.scatterplot(x=\"Age\" , y=\"Pclass\" , hue=\"Sex\" , data=train_data)\nplt.title('Age of passengers in each class')\nplt.legend()\nplt.show()\nprint(train_data.groupby(['Pclass'])['Sex'].value_counts())","fde85adf":"plt.figure(figsize=(10,6))\nsns.scatterplot(x=\"Fare\" , y=\"Pclass\" , hue=\"Sex\" , data=train_data)\nplt.title('Fare of tickets in each class')\nplt.legend()\nplt.show()\nprint(\"min Fare:\\n \",train_data.groupby(['Pclass'])['Fare'].min())\nprint(\"\\nmax Fare:\\n\",train_data.groupby(['Pclass'])['Fare'].max())\nprint(\"\\nmedian Fare:\\n\",train_data.groupby(['Pclass'])['Fare'].median())","354cf6d9":"sns.pairplot(train_data, hue = 'Survived', palette='gist_rainbow')\nplt.show()","b732481b":"#Correlation Map\ncorrmatrix = train_data.corr()\nfig,ax = plt.subplots(figsize=(12,12))\nsns.heatmap(corrmatrix,annot=True,fmt='.2f',cmap='YlGnBu',vmin=-1,vmax=1, square=True,ax=ax)\nplt.xticks(rotation=90)\nplt.title('Correlation Map')\nplt.show()","61c1223b":"X = train_data.drop(['Survived','Sex'], axis=1)\ny = train_data.Survived\n\n#Split training data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.25,stratify = y,random_state=132)\n\nX_train.info()","6154e876":"X_train.describe()","9fab8d81":"# knn - k-nearest neighbours\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=3)\nmodel.fit(X_train, y_train)\n# print metric to get performance\nprint(\"Accuracy: \",model.score(X_val, y_val) * 100)\ny_pred = model.predict(X_val)\nf1 = f1_score(y_val, y_pred, average='micro')\nprint(\"f1_score: \",f1)","4dcca196":"print(\"  Validation set Classification Report:\")\nprint(classification_report(y_val, y_pred, digits=4))","73abb062":"fig, ax = plt.subplots(1, 2, figsize = (15, 5))\nax[0].set_title(\"Training Set Confusion Matrix\")\nplot_confusion_matrix(model, X_train, y_train, ax=ax[0], \n                      cmap=\"YlGnBu\", xticks_rotation=\"vertical\")\n\nax[1].set_title(\"Validation Set Confusion Matrix\")\nplot_confusion_matrix(model, X_val, y_val, ax=ax[1],\n                      cmap=\"YlGnBu\", xticks_rotation=\"vertical\")\nplt.show()","e8ab3bd9":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = BaggingClassifier(\n    DecisionTreeClassifier(), n_estimators=500,\n    max_samples=100, bootstrap=True, random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_val)\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy: \",accuracy_score(y_val, y_pred)*100)\nf1 = f1_score(y_val, y_pred, average='micro')\nprint(\"f1_score: \",f1)","c494231a":"print(\"  Validation set Classification Report:\")\nprint(classification_report(y_val, y_pred, digits=4))","ba5c55a1":"fig, ax = plt.subplots(1, 2, figsize = (15, 5))\nax[0].set_title(\"Training Set Confusion Matrix\")\nplot_confusion_matrix(clf, X_train, y_train, ax=ax[0], \n                      cmap=\"YlGnBu\", xticks_rotation=\"vertical\")\n\nax[1].set_title(\"Validation Set Confusion Matrix\")\nplot_confusion_matrix(clf, X_val, y_val, ax=ax[1],\n                      cmap=\"YlGnBu\", xticks_rotation=\"vertical\")\nplt.show()","40d90880":"clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\nclf.fit(X_train, y_train)\n# print metric to get performance\nprint(\"Accuracy: \",clf.score(X_val, y_val) * 100)\ny_pred = clf.predict(X_val)\nf1 = f1_score(y_val, y_pred, average='micro')\nprint(\"f1_score: \",f1)","d120c42b":"print(\"  Validation set Classification Report:\")\nprint(classification_report(y_val, y_pred, digits=4))","43bfb096":"fig, ax = plt.subplots(1, 2, figsize = (15, 5))\nax[0].set_title(\"Training Set Confusion Matrix\")\nplot_confusion_matrix(clf, X_train, y_train, ax=ax[0], \n                      cmap=\"YlGnBu\", xticks_rotation=\"vertical\")\n\nax[1].set_title(\"Validation Set Confusion Matrix\")\nplot_confusion_matrix(clf, X_val, y_val, ax=ax[1],\n                      cmap=\"YlGnBu\", xticks_rotation=\"vertical\")\nplt.show()","f7e3ccd3":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100,\n                               criterion=\"entropy\",\n                               max_depth=6,\n                               min_samples_split=4,\n                               bootstrap=True,\n                               max_samples=0.8,\n                               oob_score=True,\n                               n_jobs=-1,\n                               random_state=0)\n\nclf.fit(X_train, y_train)\nscores=cross_val_score(clf, X, y, cv=5)\nprint(\"Cross Validation:\\n  %0.5f accuracy with a standard deviation of %0.5f \\n\" % (scores.mean(), scores.std()))# print metric to get performance\nprint(\"Accuracy: \",clf.score(X_val, y_val) * 100)\ny_pred = clf.predict(X_val)\nf1 = f1_score(y_val, y_pred, average='micro')\nprint(\"f1_score: \",f1)","96087245":"print(\"  Validation set Classification Report:\")\nprint(classification_report(y_val, y_pred, digits=4))","c0899966":"fig, ax = plt.subplots(1, 2, figsize = (15, 5))\nax[0].set_title(\"Training Set Confusion Matrix\")\nplot_confusion_matrix(clf, X_train, y_train, ax=ax[0], \n                      cmap=\"YlGnBu\", xticks_rotation=\"vertical\")\n\nax[1].set_title(\"Validation Set Confusion Matrix\")\nplot_confusion_matrix(clf, X_val, y_val, ax=ax[1],\n                      cmap=\"YlGnBu\", xticks_rotation=\"vertical\")\nplt.show()","66853e6a":"test_data.head()","7c2a6da2":"X_test = test_data.drop(['PassengerId','Name','Embarked','Cabin','Ticket'], axis=1)\nX_test","57d16049":"X_test['Age'] = X_test.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\nX_test","aa97277c":"(X_test.isna().sum()\/len(X_test)*100).sort_values(ascending=False)","9613738c":"X_test['Fare'].fillna(X_test['Fare'].median(),inplace=True)","187c15a3":"(X_test.isna().sum()\/len(X_test)*100).sort_values(ascending=False)","ec18f911":"le = LabelEncoder()\nX_test['Sex'] = le.fit_transform(X_test['Sex'])\nprint(f\"classes encoded for Sex are {le.classes_}\")\nX_test","0be97198":"predictions = clf.predict(X_test)","230ce392":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","76fcb6c0":"# Test data","07db984e":"##### Number of Survived females  is greater than Number of Survived males","b804a9b2":"## Check for Missing Values","030f38aa":"## Bivariate Analysis","194659ea":"![121.PNG](attachment:53bf5f1c-a1a7-42e2-8ab1-d3fd2b02ab25.PNG)","f9df4764":"#####  **The Number  of Survived people doesn't depend on port of Embarkation**\nwe can drop it as it doesn't give us important information","a733065e":"### check for Age values ","6a590fd1":"There are 177 missing values in Age, 687 in Cabin,and 2 in Embarked","1b9a1d88":"## Training Model ","b01d6793":"## Load the data","b0e10b97":"#### Number of Survived females from all age categories is greater than Number of Survived males from all age categories\n#### Also male youth passengers (20:30) & babies(<10) had a higher chance of survival than other male passengers.","f6ad23fe":"## Univariate Analysis","7dbc90ed":"# Exploratory Data Analysis(EDA)","3a8bc7a2":"## Data Preparation ","c4701f90":"# Visualization","4ca0c8da":"## Get statistical info(mean,std,min,max,etc..)","ff4a4a7d":"#####  **Class 1 has the greatest Number of Survived members**\n#####  **Class 3 has the greatest Number of Unsurvived members**","eac1f6ed":"we have 891 unique Name, this indicates that there is no duplication in rows as the whole data consists of 891 row.","297ff917":"## Get information about Non-Null values & Dtype of columns","1a97b09a":"* Cabin Feature has 77.104377% missing values which is very large percentage, so We may drop this column from the data because we can't get clear informations from it.\n\n* Age Feature has 19.865320% missing values so we may try to impute them,as we don't have enough data to drop them.\n\n* Embarked Feature has 0.224467% missing values which is very small percentage(less than 1%), so it can be neglected.","b91354df":"#### There is high correlation between Fare & Pclass and between Survived","8d6fbaab":"## Check for Number of Unique Values","ae29a7fd":"## Get clearer view about the percentage of missing values"}}