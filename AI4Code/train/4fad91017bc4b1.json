{"cell_type":{"9fb0b40d":"code","888d0cda":"code","83038cc9":"code","8d5e16f6":"code","a04a5188":"code","e1747746":"code","483241d9":"code","0562c7da":"code","5646d1cd":"code","97a23eb9":"code","d83f64ed":"code","a9cb6836":"code","441dba38":"code","4147a61e":"code","587a902f":"code","db35e5e5":"code","b2732b2e":"code","70e4439b":"code","d9d31598":"code","bfdf30c3":"code","1ef90232":"code","2ffa3f09":"code","93897d91":"code","2ff3a782":"code","01642577":"code","6b20bcfe":"code","ff22dc48":"code","656c902c":"code","3f8325fd":"code","0edfb2ca":"code","3c3f1f59":"code","a99b0770":"code","e8c8f7ae":"code","ad77d192":"code","c993eb5a":"code","3455faac":"code","258981d6":"code","2bedca57":"code","8890039f":"code","00ac8d76":"code","06c97c25":"code","247a63dc":"code","b79339f1":"markdown","12866b69":"markdown","0415c749":"markdown","cf9d2849":"markdown","17967980":"markdown","a772c130":"markdown","0feb7252":"markdown","19175a7b":"markdown","1cc6f7d2":"markdown","fa5b9f02":"markdown","820a9cb7":"markdown","3495370a":"markdown","c4847098":"markdown","9426212b":"markdown","f869e229":"markdown","5637e491":"markdown"},"source":{"9fb0b40d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","888d0cda":"print(os.listdir(\"..\/input\/titanic\"))","83038cc9":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nresult = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","8d5e16f6":"train_data.info()","a04a5188":"train_data.head()","e1747746":"test_data.info()","483241d9":"# non imp fields which can be dropped because it adds no value in finding things\n# passenger id, name, ticket\n# drop column: cabin as there are more missing values then present values, in fact 25% values are present\n# Do not drop Passengerid from test data set as we need to find who will be survive","0562c7da":"train_data = train_data.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\ntest_data = test_data.drop(['Name','Ticket','Cabin'], axis=1)","5646d1cd":"train_data['Age'].plot()","97a23eb9":"mean = train_data['Age'].mean()\nmedian = train_data['Age'].median()\nstd = train_data['Age'].std()\nmean, median, std","d83f64ed":"train_data['Age'].fillna(29, inplace = True)\ntrain_data.isnull().sum()","a9cb6836":"train_data['Embarked'].value_counts()","441dba38":"train_data['Embarked'].fillna('S', inplace = True)\ntrain_data.isnull().sum()","4147a61e":"test_data.info()","587a902f":"test_mean = train_data['Age'].mean()\ntest_median = train_data['Age'].median()\ntest_std = train_data['Age'].std()\ntest_mean, test_median, test_std","db35e5e5":"test_data['Age'].fillna(29, inplace = True)\ntest_data.isnull().sum()","b2732b2e":"train_data['Age'] = train_data['Age'].astype(int)\ntest_data['Age'] = test_data['Age'].astype(int)","70e4439b":"test_data.info()","d9d31598":"mean = test_data['Fare'].mean()\nmedian = test_data['Fare'].median()\nstd = test_data['Fare'].std()\nmean, median, std","bfdf30c3":"test_data['Fare'].fillna(36, inplace = True)\ntest_data.isnull().sum()","1ef90232":"train_data['Fare'] = train_data['Fare'].astype(int)\ntest_data['Fare'] = test_data['Fare'].astype(int)","2ffa3f09":"train_data.head()","93897d91":"train_data.info()","2ff3a782":"pclass_dummies_titanic_train  = pd.get_dummies(train_data['Pclass'])\npclass_dummies_titanic_train.columns = ['Class_1','Class_2','Class_3']\npclass_dummies_titanic_train.drop(['Class_3'], axis=1, inplace=True)\ntrain_data = train_data.join(pclass_dummies_titanic_train)\n\npclass_dummies_titanic_test  = pd.get_dummies(test_data['Pclass'])\npclass_dummies_titanic_test.columns = ['Class_1','Class_2','Class_3']\npclass_dummies_titanic_test.drop(['Class_3'], axis=1, inplace=True)\ntest_data = test_data.join(pclass_dummies_titanic_test)","01642577":"train_data.drop(['Pclass'],axis=1,inplace=True)\ntest_data.drop(['Pclass'],axis=1,inplace=True)","6b20bcfe":"train_data.head()","ff22dc48":"person_dummies_titanic_train  = pd.get_dummies(train_data['Sex'])\nperson_dummies_titanic_train.columns = ['Female','Male']\nperson_dummies_titanic_train.drop(['Male'], axis=1, inplace=True)\ntrain_data = train_data.join(person_dummies_titanic_train)\ntrain_data.drop(['Sex'], axis=1, inplace=True)\n\nperson_dummies_titanic_test  = pd.get_dummies(test_data['Sex'])\nperson_dummies_titanic_test.columns = ['Female','Male']\nperson_dummies_titanic_test.drop(['Male'], axis=1, inplace=True)\ntest_data = test_data.join(person_dummies_titanic_test)\ntest_data.drop(['Sex'], axis=1, inplace=True)\ntest_data.info()","656c902c":"embarked_dummies_titanic_test  = pd.get_dummies(test_data['Embarked'])\nembarked_dummies_titanic_test.columns = ['S','C', 'Q']\nembarked_dummies_titanic_test.drop(['Q'], axis=1, inplace=True)\ntest_data = test_data.join(embarked_dummies_titanic_test)\ntest_data.drop(['Embarked'], axis=1, inplace=True)\n\nembarked_dummies_titanic_train  = pd.get_dummies(train_data['Embarked'])\nembarked_dummies_titanic_train.columns = ['S','C', 'Q']\nembarked_dummies_titanic_train.drop(['Q'], axis=1, inplace=True)\ntrain_data = train_data.join(embarked_dummies_titanic_train)\ntrain_data.drop(['Embarked'], axis=1, inplace=True)\n\ntest_data.info()\ntrain_data.info()","3f8325fd":"train_data['S'] = train_data['S'].astype(int)\ntest_data['S'] = test_data['S'].astype(int)\n\ntrain_data['C'] = train_data['C'].astype(int)\ntest_data['C'] = test_data['C'].astype(int)","0edfb2ca":"test_data.info()","3c3f1f59":"x_train = train_data.drop('Survived',axis=1)\ny_train = train_data['Survived']\nx_test  = test_data.drop(\"PassengerId\",axis=1)\ny_test = result['Survived']\nx_test.head()","a99b0770":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\nfrom sklearn.metrics import precision_recall_curve\n\ndef draw_graphs(y_pred):\n    auc = roc_auc_score(y_test, y_pred)\n\n    # calculate roc curve\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n    # plot no skill\n    pyplot.title('ROC Curve')\n    pyplot.plot([0, 1], [0, 1], linestyle='--')\n    # plot the roc curve for the model\n    pyplot.plot(fpr, tpr, marker='.')\n    # show the plot\n    pyplot.show()\n    \n    #----------------------------------\n    # Precision Recall curve\n    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n    pyplot.title('Precison Recall Curve')\n    pyplot.plot([0, 1], [0.1, 0.1], linestyle='--')\n    # plot the precision-recall curve for the model\n    pyplot.plot(recall, precision, marker='.')\n    # show the plot\n    pyplot.show()\n\n\nfrom sklearn.metrics import confusion_matrix\ndef evaluate_algo(algo, x_train, y_train, y_pred):\n    matrices = {'score': algo.score(x_test, y_test),\n                'confusion matrix':confusion_matrix(y_test, y_pred), \n                'F1 Score': f1_score(y_test, y_pred, average=\"macro\"), \n                'Precision Score': precision_score(y_test, y_pred, average=\"macro\"),\n                'Recall score': recall_score(y_test, y_pred, average=\"macro\"),\n                'ROC AUC Score': '%.3f' % (roc_auc_score(y_test, y_pred))\n               }\n    \n    return matrices\n    \n\n","e8c8f7ae":"logisticRegr = LogisticRegression()\nlogisticRegr.fit(x_train, y_train)\ny_pred = logisticRegr.predict(x_test)\nprint(classification_report(y_test,y_pred))\n\n#draw_graphs(y_pred)\n#evaluate_algo(logisticRegr,x_train, y_train, y_pred)","ad77d192":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# feature extraction\ntest = SelectKBest(score_func=chi2, k=4)\nfit = test.fit(x_train, y_train)\n# summarize scores\nnp.set_printoptions(precision=3)\nprint(fit.scores_)\nfeatures = fit.transform(x_train)\n# summarize selected features\nprint(features)","c993eb5a":"# For feature selection\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_regression\nSelector_f = SelectPercentile(f_regression, percentile=25)\nSelector_f.fit(x_train,y_train)\nSelector_f.scores_\n#for n,s in zip(boston.feature_names,Selector_f.scores_):\n#    print ('F-score: %3.2ft for feature %s ' % (s,n))","3455faac":"# Recursive Feature Elimination \n# https:\/\/towardsdatascience.com\/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nrfe = RFE(logreg, 20)\nrfe = rfe.fit(x_train, y_train)\nprint(rfe.support_)\nprint(rfe.ranking_)","258981d6":"# Get Correlation Coefficient for each feature using Logistic Regression\ncoeff_df = pd.DataFrame(train_data.columns.delete(0))\ncoeff_df.columns = ['Features']\ncoeff_df[\"Coefficient Estimate\"] = pd.Series(logisticRegr.coef_[0])\n\n# preview\ncoeff_df","2bedca57":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\ny_pred = nb.predict(x_test)\nprint(classification_report(y_test,y_pred))\ndraw_graphs(y_pred)\nevaluate_algo(nb,x_train, y_train, y_pred)","8890039f":"from sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier(loss='squared_hinge', shuffle=True, random_state=101, max_iter=150, early_stopping=False, fit_intercept=True )\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_test)\n\ndraw_graphs(y_pred)\nevaluate_algo(logisticRegr,x_train, y_train, y_pred)\n\n#try with diff loss function\n# \u2018hinge\u2019(75.86), \u2018log\u2019(73.62), \u2018modified_huber\u2019(default - 74.41), \u2018squared_hinge\u2019(80.13), \u2018perceptron\u2019(74.63)","00ac8d76":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\n\ndraw_graphs(y_pred)\nevaluate_algo(knn,x_train, y_train, y_pred)\n#try with different values with neighbors and try to find the optimum value","06c97c25":"from sklearn.ensemble import RandomForestClassifier\nrfm = RandomForestClassifier(n_estimators = 25, oob_score=True, n_jobs=3, random_state=101, max_features=None, min_samples_leaf=15)\nrfm.fit(x_train, y_train)\ny_pred = rfm.predict(x_test)\n\ndraw_graphs(y_pred)\nevaluate_algo(rfm,x_train, y_train, y_pred)","247a63dc":"from sklearn.svm import SVC\nsvm = SVC(kernel='rbf', gamma=0.01, C=10, random_state=101)\nsvm.fit(x_train, y_train)\ny_pred = svm.predict(x_test)\n\ndraw_graphs(y_pred)\nevaluate_algo(svm,x_train, y_train, y_pred)\n","b79339f1":"**SVM**","12866b69":"** Random Forest**","0415c749":"For column Embarked","cf9d2849":"**Defining functions for evaluation**","17967980":"Doint the same as train data","a772c130":"**Logistic Regression**","0feb7252":"**For test data**","19175a7b":"**Stochastic Gradient Descent**\nhttps:\/\/www.analyticsindiamag.com\/7-types-classification-algorithms\/\n","1cc6f7d2":"**Naive Bayes**","fa5b9f02":"Dealing with fare","820a9cb7":"For Gender create dummies","3495370a":"**Preparing X, Y for linear regression**","c4847098":"For embarked find and replace appropritat values","9426212b":"**K-Nearest Neighbours**","f869e229":"For column Age - preprocess","5637e491":"Convert categorical data to columns"}}