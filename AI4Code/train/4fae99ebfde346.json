{"cell_type":{"ba7d9dbc":"code","c50bc189":"code","e118a8bb":"code","b4a9b263":"code","1e17d58d":"code","b020442d":"code","757c96f4":"code","dfeaee7a":"code","fb3318dd":"code","510323f4":"code","5a2a0a6e":"code","c2232a89":"code","83e8fc38":"code","cbb19ab0":"code","6d375892":"code","fb2a1f77":"code","2634ed2e":"code","efc240df":"code","65d6b207":"markdown","87271b69":"markdown","410d6750":"markdown","fb430971":"markdown","3d85cdfe":"markdown","dc9f6e16":"markdown","c58e757d":"markdown","e1c58639":"markdown"},"source":{"ba7d9dbc":"from pathlib import Path\n\nDATA_DIR = Path(\"\/kaggle\/input\")\nif (DATA_DIR \/ \"ucfai-core-fa19-applications\").exists():\n    DATA_DIR \/= \"ucfai-core-fa19-applications\"\nelif DATA_DIR.exists():\n    # no-op to keep the proper data path for Kaggle\n    pass\nelse:\n    # You'll need to download the data from Kaggle and place it in the `data\/`\n    #   directory beside this notebook.\n    # The data should be here: https:\/\/kaggle.com\/c\/ucfai-core-fa19-applications\/data\n    DATA_DIR = Path(\"data\")","c50bc189":"# import all the libraries you need\n\n# torch for NNs\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import optim\n\n# general imports\nfrom sklearn.model_selection import train_test_split\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","e118a8bb":"dataset = pd.read_csv(DATA_DIR \/ \"master.csv\")","b4a9b263":"dataset.head()","1e17d58d":"print(\"Total entries: {}, null entries: {}\".format(len(dataset[\"HDI for year\"]), dataset[\"HDI for year\"].isnull().sum()))","b020442d":"dataset = dataset.drop(\"HDI for year\", axis=1).drop(\"country-year\", axis=1)\ndataset.head()","757c96f4":"dataset.describe()","dfeaee7a":"dataset.info()","fb3318dd":"country_set = sorted(set(dataset[\"country\"]))\ncountry_map = {country : i for i, country in enumerate(country_set)}\n\nsex_map = {'male': 0, 'female': 1}\n\nage_set = sorted(set(dataset[\"age\"]))\nage_map = {age: i for i, age in enumerate(age_set)}\n\ngen_set = sorted(set(dataset[\"generation\"]))\ngen_map = {gen: i for i, gen in enumerate(gen_set)}\n\ndef gdp_fix(x):\n    x = int(x.replace(\",\", \"\"))\n    return x\n\ndataset = dataset.replace({\"country\": country_map, \"sex\": sex_map, \"generation\": gen_map, \"age\": age_map})\ndataset[\" gdp_for_year ($) \"] = dataset.apply(lambda row: gdp_fix(row[\" gdp_for_year ($) \"]), axis=1)","510323f4":"dataset.head()","5a2a0a6e":"dataset.info()","c2232a89":"dataset.describe()","83e8fc38":"print((dataset[\"year\"] - 1985) \/ 31)","cbb19ab0":"X, Y = dataset.drop(\"suicides\/100k pop\", axis=1).values, dataset[\"suicides\/100k pop\"].values","6d375892":"# Split data here using train_test_split\n# YOUR CODE HERE\nraise NotImplementedError()","fb2a1f77":"print(\"X shape: {}, Y shape: {}\".format(X.shape, Y.shape))","2634ed2e":"# run this if you are using torch and a NN\nclass Torch_Dataset(Dataset):\n    def __init__(self, data, outputs):\n        self.data = data\n        self.outputs = outputs\n\n    def __len__(self):\n        #'Returns the total number of samples in this dataset'\n        return len(self.data)\n\n    def __getitem__(self, index):\n        #'Returns a row of data and its output'\n      \n        x = self.data[index]\n        y = self.outputs[index]\n\n        return x, y\n\n# use the above class to create pytorch datasets and dataloader below\n# REMEMBER: use torch.from_numpy before creating the dataset! Refer to the NN lecture before for examples","efc240df":"# Lets get this model!\n# for your output, it will be one node, that outputs the predicted value. What would the output activation function be?\n# YOUR CODE HERE\nraise NotImplementedError()","65d6b207":"Now that is looking much better! However, as you can see the values vary pretty different, such as the year can be 1985-2016 and suicide rate could be from 0 to about 225. While you can train on this, its better if all of your data is within the same range. To do this, you would need to divide each value in the column, subtract its minimum value, then divide by its max value. This isn't required but sometimes can make your model train a lot faster and converge on a lower loss. For example on changing the range of year:","87271b69":"## Load in data and process\nThe data contains many different datatypes, such as floats, integers, strings, dates etc. We need to load this in and transform it all properly to something the models can understand. Once this is done, its up to you to build a model to solve this problem!","410d6750":"<img\n    style=\"border-radius: 0.5em;\"\n    src=\"https:\/\/ucfai.org\/groups\/core\/fa19\/applications\/banner.png\">\n\n<div class=\"col-12\">\n    <h1> Machine Learning Applications <\/h1>\n    <hr>\n<\/div>\n\n<div style=\"line-height: 2em;\">\n    <p>by: \n        <a href=\"https:\/\/ucfai.org\/authors\/jarviseq\">@jarviseq<\/a> on Oct 09, 2019<\/p>\n<\/div>","fb430971":"Now that looks much better. We need to transform the categories that are objects (like sex, country, age ranges) to number representations. For example, sex will become `Male = 0` and `Female = 1`. The countries and age-ranges will be similiarly encoded to integer values. Then we can describe our data again and see the full stats.\n\nThis is done using dictionaries that map's these keys to values and apply that to the dataframe. The gdp_for_year however has commas in the numbers, so we need a function that can strip these and convert them to integers.","3d85cdfe":"## Dataset for the day: Suicide Preventation\n\nThe [dataset](https:\/\/www.kaggle.com\/russellyates88\/suicide-rates-overview-1985-to-2016) we will be using today is socio-economic data alongside suicide rates per country from 1985 to 2016. It is your task today to try to predict the suicide rate per 100,000 people in a give country. Building a good model for this can help find areas where there might be a high suicide rate so that prevention measures can be put in place to help people before it happens. \n\nWe cannot natively use a SVM, Logistic Regression, or RF because they predict on categorical data, while today we will be making predictions on continuous data. Check out Regression Trees if you want to see these types of models applied to regression problems.\n\nHowever, this problem can be changed to a categorical one so that you can use a RF or SVM. To do so, after the data analysis we will get some statistics on mean, min, max etc of suicide rate per 100,000 people column. Then, you can define ranges and assign them to a integer class. For example, assigning 0-5 suicides\/100k as \"Low\", 5-50 as \"High\" etc. You can make as many ranges as you want, then train on those classes. In this case, we want to focus on producing actual values for this, so we will stick with regression, but this is something really cool you can try on your own! (Hint: use pandas dataframe apply function for this!)\n\nLinear regression can work, although is a bit underwhelming for this task. So instead we will be using a Neural Network!\n\nLet's dive in!","dc9f6e16":"Here you need to split the data for input into your NN model. \n\nIf you using an NN, you need to use `torch.from_numpy()` to get torch tensors and use that to build a simple dataset class and dataloader. You'll also need to define the device to use GPU if you are using pytorch, check the previous lecture for how that works. The [pytorch documentation](https:\/\/pytorch.org\/docs\/stable\/index.html) is also a great resource!","c58e757d":"The first thing to notice is all the NaNs for HDI, meaning that there is missing data for those row entries. There is also a possibly useless row country-year as well. Lets see how many entires are missing HDI data first.","e1c58639":"As you can see, most entires are null, so lets remove this column and the country-year column."}}