{"cell_type":{"32d86e7a":"code","97c309a0":"code","6edabccb":"code","df32da1d":"code","9d545008":"code","a939f388":"code","790a6687":"code","217ba05b":"code","b2d7ebe2":"code","8629f09b":"code","c960270a":"code","5645801a":"code","633cfad0":"code","227df5bf":"code","422f0ecb":"code","003cbccf":"code","8ea925b7":"code","a252e5b3":"code","c2fcbc2b":"code","9b8780d6":"code","894f66bf":"code","772a3a94":"code","2366a3fc":"code","50153c82":"code","02bfe65b":"code","8e4fe818":"code","7b217d06":"code","ee045b6d":"markdown","7c791d0b":"markdown","5943c5ae":"markdown","e5dd2647":"markdown","e22dfded":"markdown","dcd9d390":"markdown","66929c2b":"markdown","412806c0":"markdown","e5c8d777":"markdown","39c06632":"markdown","2d360491":"markdown","a8d505e7":"markdown","0f33d1eb":"markdown","ad8735d2":"markdown","d269d4a2":"markdown","49588a60":"markdown","f17db802":"markdown","3d2cb8d4":"markdown","751c3d3b":"markdown","83aa41c9":"markdown","205fcb1d":"markdown"},"source":{"32d86e7a":"\nimport pandas as panda\n\nremote_location = '..\/input\/auto_sales.csv'\n\nheaders = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]\n","97c309a0":"\n## we had a look at the data placed in the remote location and found that the data is without any headers. \n## hence we pass the arguments, headers = None\ndata = panda.read_csv(remote_location, header = None) \n\ndata_dimensions = data.shape\nprint(\"The dimensions of data downloaded is %d rows * %d columns\" %(data_dimensions[0], data_dimensions[1] ))\n\n\n##dropping the first row and first columns which came automatically once i uploaded non header csv in kaggle\ndata.drop(labels=[0], inplace=True)\ndata.drop(labels=[0], inplace=True, axis=1)\ndata.shape\n","6edabccb":"## we are going to assign headers to the data downloaded.\n## we are also going to rename the headers all to lowercase for standardization purposes (this is an optional step)\n## and then we are going to print the first 10 rows and last ten rows for simply eyeballing part of the data\n\ndata.columns = [ i.lower() for i in headers]\n\nprint(\"************ Top ten rows ************\")\nprint(data.head(10))\n\n\n\nprint(\"************ Bottom ten rows ************\")\nprint(data.tail(10))\n","df32da1d":"## we will check for the data types in the data provided and give a count of the unique data types provided\n\ndata.info() ## for simple eyeballing of the data provided. gives us overview of datatypes across columns\ndata_types = data.dtypes ##data_types returned is an instance of pandas.DataSeries class\n\n## value_counts gives us a result of number of unique data types in the entire data. value_counts is an attribute only present in DataSeries and not in DataFrame\n\nprint(data_types.value_counts())\n\n\n## a longer way to achieve the above is given below, where we convert it to data frame and reset indices and run a groupby and count command\n## FYI - data.info() would have also given DataFrame instance automatically.\n# data_types = panda.DataFrame(data_types, columns = ['column_type'])\n# data_types.index.name = 'column_name'\n\n# data_types.reset_index(inplace = True)\n# print(data_types.groupby( by = 'column_type', as_index = False).count())\n\n\n\n","9d545008":"## we will call describe method of pandas data frame to show us data statistics for numeric columns.\n\n## the below two lines could have been combined using include =\u00e1ll keyword, but i prefer separation\nprint(data.describe())\n\nprint(data.describe(include = 'object'))","a939f388":"\nimport numpy\ndata.replace('?', numpy.nan, inplace = True) ## inplace marked as true , hence we are modifying original data\n\ntemp_null_data_check = data.isnull()\nempty_column_list = {}\n\nfor col in temp_null_data_check.columns.tolist():\n    values = temp_null_data_check[col].values\n    \n    if True in values :\n        empty_column_list[col] = {'missing_count' : list(values).count(True), 'data_type' : data[col].dtype.name} ##dtype is numpy.dtype instance, value is numpy.darray instance\n        \n\nprint(empty_column_list)\n\n\n## another way to achieve the above is give below.\n# empty_values = data.isnull().sum().to_frame()\n# empty_values= empty_values.assign(column_type = data.dtypes)\n# empty_values","790a6687":"\n## we can drop rows where all the values are NaN. THat is empty rows by using the command data.dropna(how='all')\n\n## drop row with empty price cell\n \ndata.dropna(inplace = True, axis =0, subset = ['price'])\n\ndata['price'] = data['price'].astype('float64')\n\n## check to see if the row has been dropped\nnumpy.nan in data['price'].values\n\n## convert data types for normalized losses, horsepower, bore, stroke, peak -rpm  since we are going to perform mathematical operation\n\ndata[['normalized-losses', 'bore', 'stroke', 'horsepower', 'peak-rpm']] = data[['normalized-losses', 'bore', 'stroke', 'horsepower', 'peak-rpm']].astype('float64')\n\n## calculate mean of normalized losses, horsepower, bore, stroke, peak -rpm\n\nnormalized_losses_mean = data['normalized-losses'].mean()\nbore_mean              = data['bore'].mean()\nstroke_mean            = data['stroke'].mean()\nhorsepower_mean        = data['horsepower'].mean()\npeak_rpm_mean          = data['peak-rpm'].mean()\n\n## assign missing values to the individual mean value\n\n\ndata['normalized-losses'].replace(to_replace = numpy.nan, value = normalized_losses_mean, inplace =True)\ndata['bore'].replace(to_replace = numpy.nan, value = bore_mean, inplace =True)\ndata['stroke'].replace(to_replace = numpy.nan, value = stroke_mean, inplace =True)\ndata['horsepower'].replace(to_replace = numpy.nan, value = horsepower_mean, inplace =True)\ndata['peak-rpm'].replace(to_replace = numpy.nan, value = peak_rpm_mean, inplace =True)\n\n## sklearn.impute.SimpleImputer is in latest version and not available yet.\n## code in case of SimpleIMputer would have been SimpleImputer(missing = numpy.nan, strategy = 'mean'). and then fit_trasnform\n\n# from sklearn.preprocessing import Imputer\n\n# imputer = Imputer(missing_values = numpy.nan, strategy = 'mean', axis = 0)\n# data = imputer.fit_transform(data.values)\n\nprint(\"peak rpm summation after mean calc %s\" %data['peak-rpm'].sum())\nprint(\"horsepower summation after mean calc %s\" %data['horsepower'].sum())\nprint(\"stroke summation after mean calc %s\" %data['stroke'].sum())\nprint(\"bore summation after mean calc %s\" %data['bore'].sum())\nprint(\"normalized-losses summation after mean calc %s\" %data['normalized-losses'].sum())\n## we can use the sklearn.impute.SimpleImputer with strategy mean to transform all the above 4 values at one shot\n\n## calculate top value for number of doors and replace empty value with it\n\nnumber_of_doors_top_value = data['num-of-doors'].value_counts().idxmax()\n\ndata['num-of-doors'].replace(to_replace = numpy.nan, value = number_of_doors_top_value, inplace =True)\n\n\n## run the empty values check again\n\n\ntemp_null_data_check = data.isnull()\nempty_column_list = {}\n\nfor col in temp_null_data_check.columns.tolist():\n    values = temp_null_data_check[col].values\n    \n    if True in values :\n        empty_column_list[col] = {'missing_count' : list(values).count(True), 'data_type' : data[col].dtype.name} ##dtype is numpy.dtype instance, value is numpy.darray instance\n        \n\nif not empty_column_list:\n    print(\"Successfully completed all empty values replacement\")\n# print(empty_column_list) ## empty dict means our empty values replacement is successfully completed\n\n","217ba05b":"\n\n###lets draw a histogram for showing data distribution\n%matplotlib inline\nimport matplotlib.pyplot as plot\nimport seaborn as sns\n\nplot.hist(data['horsepower'], bins = 3)\nplot.title('Horsepower Distribution')\nplot.xlabel(\"Horsepower values\")\nplot.ylabel(\"Count\")\n\nplot.show()\n\nsns.distplot(data['horsepower'], bins =3, kde=False, rug=True)\n\n\n\n## get the range of values for horsepower, the min and the max\n\nmin_horse_power = data['horsepower'].min()\nmax_horse_power = data['horsepower'].max()\n\n## set the number of bins you require and provide names for the same\n\nbin_labels = ['low', 'medium', 'high']\nbin_count = 4  ## cut function has default rightmost edge as true.\n\n## lets say we knew which numbers to classify as low, medium and high. say from a given range of [0,20]  we \n## say numbers within 0 - 7 is low, 7 - 14 is medium and 14 - 20 is high. so our bins are essentially [0,7,14,20] i.e one higher\n## than the number of labels. \n\n## we dont want to decide the ranges for bins . we let numpy decide the same\n## using linspace, which returns evenly spaced numbers using intervals\n\nbins = numpy.linspace(start = min_horse_power, stop = max_horse_power, num = 4)\n# print(bins,min_horse_power,max_horse_power)\n\n## convert data into bins\n\ndata['horsepower-binned'] = panda.cut(data['horsepower'], bins, labels = bin_labels ,include_lowest = True )\n\n# print(data[['horsepower','horsepower-binned']].head(20))\nprint(data['horsepower-binned'].value_counts())\n\n","b2d7ebe2":"\n## another way of achieving the below is to use the panda.get_dummies method\n## another technique is to use the map function. v = {'gas' :1, 'diesel' : 0}. data[].map(V)\n\ndata['fuel-type'] = data['fuel-type'].apply(lambda x: 1 if x == 'gas' else 0 )\ndata['fuel-type'] = data['fuel-type'].astype('int64')\n\n\ndata['fuel-type'].value_counts()\n","8629f09b":"data['aspiration'] = data['aspiration'].apply(lambda x: 1 if x == 'std' else 0 )\ndata['aspiration'] = data['aspiration'].astype('int64')\n\n\ndata['aspiration'].value_counts()","c960270a":"\n\n## describe the numeric columns which we think are likely to impact price target \n## (we would perform the same for categorical values later). we want to confirm our assumption\n\nprint(data[['symboling', 'normalized-losses' , 'length', 'width', 'height', 'curb-weight', 'engine-size']].describe(include = 'all'))\n\ndata[['symboling', 'normalized-losses' , 'length', 'width', 'height', 'curb-weight', 'engine-size']].corr()","5645801a":"\n\n## plotting boxplots for the above 7 attributes to check for outliers and inter quartile ranges\n\n%matplotlib inline\n\ndata[['symboling', 'normalized-losses' , 'length', 'width', 'height', 'curb-weight', 'engine-size']].plot(kind= 'box', figsize =(10,20))\nplot.show()\n","633cfad0":"from scipy import stats\nfrom scipy.stats import f_oneway\nimport matplotlib.patches as mpatches\n\n\ndef calculateCorrelationCoefficientsAndpValues(x_data, y_data, xlabel):\n    \n    pearson_coef, p_value = stats.pearsonr(x_data, y_data)\n    print(\"The Pearson Correlation Coefficient for %s is %s with a P-value of P = %s\" %(xlabel,pearson_coef, p_value))\n    \n    return (pearson_coef,p_value)\n\ndef plotRegressionBetweenTwoVariables(x_label,y_label, x_y_data, pearson_coef, p_value):\n    \n    plot.figure(figsize=(15,15))\n    \n    sns.regplot(x = x_label , y = y_label , data = x_y_data)\n\n\n    # plot.text(x = 1, y = 40000 , s =\"Pearson Correlation Coefficient = %s\"%pearson_coef, fontsize = 12 )\n    # plot.text(x = 1, y = 38000 , s =\"P value = %s\"%p_value, fontsize = 12 )\n\n    blue_patch = mpatches.Patch(color='blue', label='Pearson Correlation Coefficient = %s, p value is %s '%(pearson_coef, p_value))\n    plot.legend(handles=[blue_patch], loc ='best')\n    plot.title(\"Regression Plot %s vs %s\"%(x_label, y_label))\n    \n    \n\n    ","227df5bf":"from scipy import stats\n\n## checking correlation between symbolizing and target price\ndata['symboling'] = data['symboling'].astype('float64')\n\ncoeff_values = calculateCorrelationCoefficientsAndpValues(data['symboling'], data['price'],'symboling')\nplotRegressionBetweenTwoVariables( 'symboling', 'price', data[['symboling', 'price']], coeff_values[0], coeff_values[1])\n","422f0ecb":"## we will use the min max technique for normalization of length, width and height\n## min-max technique is x-min\/max-min. values range betoiwwen 0 and 1\n## other technique is z score which is x - mean\/std deviation. values range betwwen -inf to inf (however typical is 0 to -3 to +3)\n## the otehr is feature scaling which is x\/ max . values range between 0 and 1\n\n\n\ndata['length'] = (data['length'] - data['length'].min())\/ (data['length'].max() - data['length'].min())\n\ndata['width'] = (data['width'] - data['width'].min())\/ (data['width'].max() - data['width'].min())\n\ndata['height'] = (data['height'] - data['height'].min())\/ (data['height'].max() - data['height'].min())","003cbccf":"## drawing plots and calculation of coefficients of each\n\ncoeff_values = calculateCorrelationCoefficientsAndpValues(data['length'], data['price'],'length')\nplotRegressionBetweenTwoVariables( 'length', 'price', data[['length', 'price']], coeff_values[0], coeff_values[1])\n\n\n\n\n\n\n","8ea925b7":"coeff_values = calculateCorrelationCoefficientsAndpValues(data['width'], data['price'],'width')\nplotRegressionBetweenTwoVariables( 'width', 'price', data[['width', 'price']], coeff_values[0], coeff_values[1])\n\n\n","a252e5b3":"coeff_values = calculateCorrelationCoefficientsAndpValues(data['height'], data['price'],'height')\nplotRegressionBetweenTwoVariables( 'height', 'price', data[['height', 'price']], coeff_values[0], coeff_values[1])","c2fcbc2b":"\n\nfor item in data.describe().columns.tolist():\n    calculateCorrelationCoefficientsAndpValues(data[item], data['price'],item)\n\n","9b8780d6":"\n\n## lets deal with the rest of the object or caategorical variables and see if it needs to be included or discarded\n\n\ndata.describe(include = 'object')\n\n","894f66bf":"## lets draw a bar plot showing relationship between drive-wheels and avegrage price across drive wheels attribute\nfrom scipy.stats import f_oneway\n\ndef calculateANOVA(data, xlabel, ylabel):\n\n    f_val, p_val = stats.f_oneway(*data)  \n \n    print( \"ANOVA results for %s vs %s : F= %s , P = %s \" %(xlabel,ylabel,f_val,p_val))\n    \n    return f_val,p_val\n\n\ndef plotBarChartAcrossCategories(data, xlabel, ylabel, title, anova_results = None):        \n\n    data.plot(kind='bar', figsize= (10,10))\n\n    plot.title(title)\n    plot.xlabel(xlabel)\n    plot.ylabel(ylabel)\n    \n    if anova_results:\n        \n        label = 'F value = %s, p value is %s ' %(anova_results[0], anova_results[1])\n        patch = mpatches.Patch(color='orange', label=label)\n        plot.legend(handles=[patch], loc ='best')\n\n\ndef prepareANOVAData(data, indices):\n    anova_data = []\n\n    for item in indices:\n        anova_data.append(data.get_group(item)['price'])\n    \n    return anova_data\n    \n\n\n\n","772a3a94":"grouped_by_fuel_system =  data[['fuel-system','price']].groupby(['fuel-system'], as_index = False) \nmean_price_grouped_by_fuel_system = grouped_by_fuel_system.mean()\nmean_price_grouped_by_fuel_system.set_index('fuel-system', inplace= True)\nindices = list(mean_price_grouped_by_fuel_system.index)\n\n\nanova_results = calculateANOVA(data = prepareANOVAData(grouped_by_fuel_system, indices), xlabel='fuel-system', ylabel='price')\nplotBarChartAcrossCategories(mean_price_grouped_by_fuel_system, xlabel = 'Fuel System', \\\n                                 ylabel = 'Average Price', title = 'Fuel System vs Average Price', anova_results = anova_results)\n\n\n","2366a3fc":"grouped_by_engine_size =  data[['engine-size','price']].groupby(['engine-size'], as_index = False) \nmean_price_grouped_by_engine_size = grouped_by_engine_size.mean()\nmean_price_grouped_by_engine_size.set_index('engine-size', inplace= True)\nindices = list(mean_price_grouped_by_engine_size.index)\n\n\nanova_results = calculateANOVA(data = prepareANOVAData(grouped_by_engine_size, indices), xlabel='engine-size', ylabel='price')\nplotBarChartAcrossCategories(mean_price_grouped_by_engine_size, xlabel = 'Engine Size', \\\n                                 ylabel = 'Average Price', title = 'Engine Size vs Average Price', anova_results = anova_results)\n\n\n\n","50153c82":"grouped_by_body_style =  data[['body-style','price']].groupby(['body-style'], as_index = False) \nmean_price_grouped_by_body_style = grouped_by_body_style.mean()\nmean_price_grouped_by_body_style.set_index('body-style', inplace= True)\nindices = list(mean_price_grouped_by_body_style.index)\n\n\nanova_results = calculateANOVA(data = prepareANOVAData(grouped_by_body_style, indices), xlabel='body-style', ylabel='price')\nplotBarChartAcrossCategories(mean_price_grouped_by_body_style, xlabel = 'Body Style', \\\n                                 ylabel = 'Average Price', title = 'Body Style vs Average Price', anova_results = anova_results)\n\n\n","02bfe65b":"grouped_by_drive_wheels =  data[['drive-wheels','price']].groupby(['drive-wheels'], as_index = False) \nmean_price_groupedby_drive_wheels = grouped_by_drive_wheels.mean()\nmean_price_groupedby_drive_wheels.set_index('drive-wheels', inplace= True)\nindices = list(mean_price_groupedby_drive_wheels.index)\n\n\nanova_results = calculateANOVA(data = prepareANOVAData(grouped_by_drive_wheels, indices), xlabel='drive-wheels', ylabel='price')\nplotBarChartAcrossCategories(mean_price_groupedby_drive_wheels, xlabel = 'Drive Wheels', \\\n                                 ylabel = 'Average Price', title = 'Drive Wheels vs Average Price', anova_results = anova_results)\n\n\n\n","8e4fe818":"grouped_by_number_of_doors =  data[['num-of-doors','price']].groupby(['num-of-doors'], as_index = False) \nmean_price_grouped_by_number_of_doors = grouped_by_number_of_doors.mean()\nmean_price_grouped_by_number_of_doors.set_index('num-of-doors', inplace= True)\nindices = list(mean_price_grouped_by_number_of_doors.index)\n\n\nanova_results = calculateANOVA(data = prepareANOVAData(grouped_by_number_of_doors, indices), xlabel='num-of-doors', ylabel='price')\nplotBarChartAcrossCategories(mean_price_grouped_by_number_of_doors, xlabel = 'Number of Doors', \\\n                                 ylabel = 'Average Price', title = 'Number of Doors vs Average Price', anova_results = anova_results)\n\n\n\n","7b217d06":"grouped_by_number_of_cylinders =  data[['num-of-cylinders','price']].groupby(['num-of-cylinders'], as_index = False) \nmean_price_grouped_by_number_of_cylinders = grouped_by_number_of_cylinders.mean()\nmean_price_grouped_by_number_of_cylinders.set_index('num-of-cylinders', inplace= True)\nindices = list(mean_price_grouped_by_number_of_cylinders.index)\n\n\nanova_results = calculateANOVA(data = prepareANOVAData(grouped_by_number_of_cylinders, indices), xlabel='num-of-cylinders', ylabel='price')\nplotBarChartAcrossCategories(mean_price_grouped_by_number_of_doors, xlabel = 'Number of Cylinders', \\\n                                 ylabel = 'Average Price', title = 'Number of Cylinders vs Average Price', anova_results = anova_results)\n","ee045b6d":" THe following observations are clear from the above:\n \n 1. num-of-doors has 2 unique values and four door cars appear 115 times in the listing of 200 cars. This is clearly a categorical data and we will compute ANOVA values for the same\n \n \n 2. body-style has 5 appearances with sedan appearing close to 50% of the times. Now for this particular piece we would use ANOVA test to check if body style impacts price value.\n \n \n 3. engine-location as already discussed above, can be discarded.\n \n \n \n 4. engine-type also requires a ANOVA testing, and so does fuel-system\n \n \n 5. num-of-cylinders is also a categorical value and will receive treatment similar to point 1\n \n \n we are algo going to plot bar charts for each of these categorical values to see how differences impact the price.","7c791d0b":"We would now proceed to perform certain actions that are deemed necessary (based on business knowledge) eg data binning.\n\nWe can see horsepower attribute has 57 unique numerical values, however we are not interested in getting exact numeric values. Instead we are interested in calculations based on higher , lower , medium horsepower values. SO we would convert our numeric range of data for horsepower into bins of data.\n","5943c5ae":"#### Goal: Prepare data and finalize features from the entire data set which can be used to create our modelling algorithm\n","e5dd2647":"\n\nAt this point we are done with our steps 3 and 4.\n\nWe would now proceed to deal with missing data\n\n1. Show which columns, if any, have missing data\n\n2. Price and normalized-losses both have ? as missing values. That needs to be converted to NaN value\n\n3. Price is what we need to predict and having a target with blank value does not make sense and hence we would drop that row\n\n4. normalized-losses has numeric values. we would replace missing values( at this point numpy.NaN value) with mean of the data \n","e22dfded":"\n\nBased on the above observations we can see that :\n\n1. symboling : not related , will not be included in our data features\n\n2. height : not related , will not be included in our data features\n\n3. length and width :  related , will  be included in our data features\n\n\nSimilarly lets calculate the correlation coefficients for the remaining numeric columns .\nNote : we will not draw the regression plot for each and every one, only the ones we find valuable having close to 1 or -1 coefficient and very small p value","dcd9d390":"We would be using the data source archived in the location https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/autos\/imports-85.data.\n\nThe following steps would be undertaken as part of the data analysis exercise\n\n1. read the data from the remote location\n2. provide appropriate column headers\n3. provide description of the data along with division of numeric and non numeric values\n4. Provide stats such as unique counts, top values, inter quartile range\n5. Check for non empty values and decide on whether to drop values or replace. Provide justification for each\n6. Provide data normalization for numeric features. Decide which columns require data normalization\n7. Provide data bining functionality for categorical numeric or non numeric values. Provide justification for each.\n8. Perform grouping of features vs price exercises to display relation between price and features\n9. Provide heat map to display correlation between features. ALso provide any other charts\/graphs as deemed necessary.\n10. Provide coefficient values and p values to decide on which features to finalize for modelling of algorithm. \n","66929c2b":"These are the following actions we are going to take based on the above:\n\n1. price is our target value for our supervised learning data set. Having empty value for target does not make sense. Similarly, we cant assign average value for the empty cell as that would be incorrect understanding. Hence we are going to drop the row with empty price cell. Since we are dropping this particular row, all our mean calculations will be done only post this step\n\n\n2. we are going to convert the price to float data type\n\n\n\n3. normalized losses column has a huge number of empty values, close to 20% of our number of rows. From our earlier analysis we also know this is a numeric column. SO we are going to replace empty values with average of the normalized losses and check to see correlation with price later on based on that assumption\n\n\n\n4. we are also going to convert normalized losses to float64 datatype\n\n\n\n5. number of doors has a frequency of value 4 for 114 entries and it has only 2 values missing. We are going to replace missing value with the topmost appearing value which is 4. We are also going to convert the column to int64\n\n\n6. Actions for bore, horsepower,stroke,peak-rpm will be similar to step 3 and 4 above\n\n","412806c0":"\n\n\nLet us now normalize the values for lengt, width and height and draw the regplto and calculate coefficients\n\n\n","e5c8d777":"# Analysis of USA Auto Sales History in order to predict car prices\n","39c06632":"\n\nWe also observe engine-location also has only two unique values, however since we already decided previously that engine location does not impact the price of the vehicle we are going to ignore updating\/changing this particular attribute\n\nTime to find out correlation of the remaining attributes and price target.\n\n1. we would be drawing regression plots to view the dependence between attribute and target price\n\n\n2. we would calculate the pearson coefficient and p value and mark them as legends on the graph\n\n\n3. We also know that pearson coefficient is sensitive to outliers. Hence we would be plotting boxplots for the attributes to see if there are outliers\n\n\n\n*** How do we deal with outliers. We may remove rows with outlier values and check how they impact the analysis.\n\n\n\nLets decide on the following attributes which would undergo the above analysis:\n\n\n\n\n<li>symboling<\/li>\n<li>normalized-losses<\/li>\n<li>make<\/li>\n<li>fuel-type<\/li>\n<li>aspiration<\/li>\n<li>num<\/li>\n<li>body-style<\/li>\n<li>drive-wheels<\/li>\n<li>wheel-base<\/li>\n<li>length<\/li>\n<li>width<\/li>\n<li>height<\/li>\n<li>curb-weight<\/li>\n<li>engine-type<\/li>\n<li>num-of-cylinders<\/li>\n<li>engine-size<\/li>\n<li>fuel-system<\/li>\n<li>bore<\/li>\n<li>stroke<\/li>\n<li>compression-ratio<\/li>\n<li>horsepower<\/li>\n<li>peak-rpm<\/li>\n<li>city-mpg<\/li>\n<li>highway-mpg<\/li>\n\n\n\n\nA couple of the above values are numeric and some are categorical. Hence analysis would be separate for each\n\n\n\n\n","2d360491":"\n\nFrom the above analysis , we get the following data:\n\n-  'normalized-losses': {'missing_count': 41, 'data_type': 'object'}\n\n-  'num-of-doors': {'missing_count': 2, 'data_type': 'object'}\n\n-  'bore': {'missing_count': 4, 'data_type': 'object'}\n\n-  'stroke': {'missing_count': 4, 'data_type': 'object'}\n\n-  'horsepower': {'missing_count': 2, 'data_type': 'object'}\n\n-  'peak-rpm': {'missing_count': 2, 'data_type': 'object'}\n\n-  'price': {'missing_count': 4, 'data_type': 'object'}}\n\n\n","a8d505e7":"Observations we can make from the statistical description of each columns:\n\n1. Price, normalized-losses columns have value ?. This needs to be converted to numpy.NaN values so that our data casting can be done\n\n\n2. Curb weight has huge range of values which can lead to curb weight dictating models. Hence we would need to normalize our data\n\n\n3. Make of car Toyota appears 32 times in the 206 rows of data. So we may notice that data of only one model is not collected, our model data is also present. However the maximum amount of data is available for Toyota however this will not lead to skewed results since it appears only 32 times in 206 records (a bit over 10%)\n\n\n4. Engine location topmost value appears 202 times out of 206 records. We may assume engine location at top is a standard feature across cars and hence will not be impacting\/dictating price of cars. Hence we may remove this from our data features\n\n\n\n5. columns such has fuel-type, drive-wheels, aspirations, num-of-cylinders. num-of-doors, engine-type all have very few number of unique values and the top most occuring value appears more than 100 out of total of 205 rows. We would need to carefully check the correlation between these values and price in order to decide if they need to be considered as a data feature. (We would be using pearson coefficient and p value to decide)","0f33d1eb":"At this point we are done with our steps 1 and 2.\n\nA few things to notice so far:\n\n1. our data has 205 rows\n2. our data has 26 columns\n3. our data has the value that we would like to predict, i.e. price. This point is important since this signifies our learning algorithm can be a supervised learning algorithm\n","ad8735d2":" We can see that the breakdown of data types is as below:\n \nobject     16\n\nfloat64     5\n\nint64       5\n\n\n**********************************************************************************************************\n\nSome random observations just based on eyeballing the data and the datatype information above :\n\n1. There are too many string or object data types. Statistical analysis proves difficult for non numerical data and we will need to check if data types can be updated\n\n2. Data types which require updating (from simple eyeballing the data) are price, normalized losses, horsepower ,etc\n\nP.S :Before updating the data types though, we would like to check for empty values and how to handle the same\n\n************************************************************************************************************\n\n\n\nWe will now proceed to provide some statistics of numeric data and non numeric data as well\n\n\n \n ","d269d4a2":"<h5> Finally we can note that the following features are finalized based on our findings above:\n\n<br><br>\n\n\n<li>length<\/li>\n<li>width<\/li>\n<li>wheel-base<\/li>\n<li>curb-weight<\/li>\n<li>bore<\/li>\n<li>horsepower<\/li>\n<li>city-mpg<\/li>\n<li>highway-mpg<\/li>\n<li>drive-wheels<\/li>\n<li>engine-size<\/li>\n<li>make<\/li>\n\n<\/h5>","49588a60":"\n\n\n\nWe observe columns such as fuel-type and aspiration have only two unique values and both these values are non-numeric. Since non numeric values prove difficult values for regression analysis we are going to convert these values to a numeric number and change the data type.\n\nHow do we convert these categorical values to numeric values? Say for fuel type, diesel is assigned a value of 1 and gas is assigned a value of 0.\n\n","f17db802":"Based on our calculations above, we take the following features as valuable attributes:\n\n1. Length\n2. Width\n3. wheel-base\n4. curb-weight\n5. bore\n6. horsepower\n7. city-mpg\n8. highway-mpg\n","3d2cb8d4":"we will take the attributes with the highest f values , since in all cases the p value is pretty low. Based on the above, few more attributes we can finalize are:\n\n1. drive-wheels\n2. engine-size\n\nWe will also consider make as a valuable feature since make varies quite a bit across rows.\n\n\nadditionally we can note that number of cylinders, number of doors hardly alters across price range and hence can be ignored\n\n\n\n","751c3d3b":"Our next step would be to analyze the data further. We would like to check the data types of the data provided, and check if it requires updating. We would also check the data formats of the data provided and once again, check if it requires updating\n\n","83aa41c9":"\n\nNow that all our data has been cleared of empty values and data types has been assigned correctly, we would start by observing the range of values for numeric inputs and checking to see if certain columns require normalization.\n\nIn order to do so, we would again describe our data sets for only the numeric values\nTODO: to be done after question regarding normalization is answered\n","205fcb1d":"\n\n\nWe can make the following observations from the box plot above:\n    \n    1.  symboling has a very lower range of values with hardly any outliers. Hardly any outliers means our coefficients values are going to be truly representatice of correlation\n    \n    \n    2. width and height are almost in the same range. however length has very different upper scales , eg. min value of length is higher than max value of both these attributes. Its time to normalize these attributes and plot them again\n    \n    \n    3. engine size has quite a few outliers\n    \n    \nPearson coefficient is meant to be read as below:\n\n close to 1 : totally correlated\n \n close to 0 : no correlation, values do not impact one another\n \n close to -1 : negatively correlated\n \n P value is meant to be read as below: ( p value is probability that values are related to one another)\n \n < 0.001 : very strong correlation ( 99% probability we can say that values are correlated)\n \n < 0.05 : strong correlation ( 95% probability we can say that values are correlated)\n \n < 0.1  : there is weak evidence that the correlation is significant, and\n\n \\>  0.1 : there is no evidence that the correlation is significant.\n \n \n    \n    \n    "}}