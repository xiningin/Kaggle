{"cell_type":{"dfb7058a":"code","23dda633":"code","42b9aa44":"code","a64ed69b":"code","dfbc87bb":"code","462bcdf8":"code","83f0413a":"code","cbb2ac5f":"code","f8bdd767":"code","d6600dfe":"code","7f29681d":"code","9d2d6d72":"code","a632cb23":"code","f10f5853":"code","1963c65d":"code","382fee02":"code","f5434f8c":"code","035fc12d":"code","31e90235":"code","9651b516":"code","257d1d02":"code","26f3650f":"code","546fcd0b":"code","e3e542ae":"code","bd15b88c":"code","92bcfc7c":"code","fbd8c3f4":"code","bb7a0be9":"code","c09a4c82":"code","54fa6e8a":"code","d57c2042":"code","291e1267":"code","b8a3851e":"code","20a712d3":"code","3f455a82":"code","27daecf5":"code","d571ba2d":"markdown","e5e370e4":"markdown","b52abd09":"markdown","548ecb8f":"markdown","9281eeb8":"markdown","b3b05eab":"markdown","b76ba04a":"markdown","04e54966":"markdown","aa7e7ca1":"markdown","8bdbcb26":"markdown","81fea4b0":"markdown","a010ef34":"markdown"},"source":{"dfb7058a":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import SCORERS, mean_absolute_error, r2_score\n\nimport math","23dda633":"df = pd.read_csv('..\/input\/house-price-tehran-iran\/housePrice.csv').drop(columns=['Price'])\ndf.head(10)","42b9aa44":"df.info()","a64ed69b":"print('Check the data. Some values are missing.') if df.isnull().any().any() else print(\"There are no missing values in the data.\")","dfbc87bb":"df.isnull().sum()","462bcdf8":"# I will drop missing values although it is not necessary because used models can manage missing values.\ndf.dropna(inplace=True)","83f0413a":"cond = df['Area'].apply(lambda a: len(a) > 3)\ndf.where(cond).dropna()","cbb2ac5f":"# I've decided to delete these rows.\ncond = df['Area'].apply(lambda a: len(a) <= 3)\ndf = df.where(cond).dropna()","f8bdd767":"df['Area'] = pd.to_numeric(df['Area'])","d6600dfe":"sns.boxplot(data=df['Price(USD)'], orient='h', color='pink')","7f29681d":"# I will drop outliers whose predicted value is higher than 1 000 000.\ncond = df['Price(USD)'] < 1000000\ndf = df.where(cond).dropna()","9d2d6d72":"sns.boxplot(data=df['Price(USD)'], orient='h', color='forestgreen')","a632cb23":"fig, ax = plt.subplots(ncols=2, figsize=(14,7))\nsns.kdeplot(df['Area'], shade=True,  color='navy', ax=ax[0])\nsns.histplot(data=df, x='Room', color='skyblue', discrete=True, ax=ax[1])\nax[0].set_title('Area density')\nax[1].set_title('Room distribution')","f10f5853":"fig, ax = plt.subplots(ncols=3, figsize=(18,6))\n\ncolors = [['#ADEFD1FF', '#00203FFF'], ['#97BC62FF', '#2C5F2D'], ['#F5C7B8FF', '#FFA177FF']]\nexplode = [0, 0.2]\ncolumns = ['Parking', 'Warehouse', 'Elevator']\nfor i in range(3):\n        data = df[columns[i]].value_counts()\n        ax[i].pie(data, labels=data.values, explode=explode, colors=colors[i], shadow=True)\n        ax[i].legend(labels=data.index, fontsize='large')\n        ax[i].set_title('{} distribution'.format(columns[i]))","1963c65d":"df2 = df['Address'].value_counts().copy()\ndf2 = df2[:8]","382fee02":"fig, ax = plt.subplots(figsize=(6,10))\nsns.barplot(x=df2.values, y=df2.index, orient='h', color='tan', edgecolor='black', ax=ax)\nax.bar_label(ax.containers[0], label_type='center')\nplt.xlabel('numerical amount of flats')\nplt.title('Number of flats in location')","f5434f8c":"df3 = df.copy().drop(columns=['Price(USD)'])\ncondition = df3['Address'].value_counts() > 50\ndf3 = df3[df3['Address'].apply(lambda l: condition[l])]\ndf3 = df3.groupby('Address').mean().sort_values(by='Area', ascending=False)\ndf3.head()","035fc12d":"fig, ax = plt.subplots(figsize=(7, 7))\nplt.grid(b=True, linestyle='dashed')\nplt.title('The characteristics of popular locations')\nsns.scatterplot(x=df3['Area'], y=df3['Room'], color='firebrick', s=50, edgecolor='black', marker='s', ax=ax)\n\nax.text(x=df3['Area'][0]-13, y=df3['Room'][0], s=df3.index[0])\nax.text(x=df3['Area'][7]+1, y=df3['Room'][7], s=df3.index[7])\nax.text(x=df3['Area'][-1]+1, y=df3['Room'][-1], s=df3.index[-1])","31e90235":"stats = df.select_dtypes(['float', 'int64'])\nfig = plt.figure(figsize=(6, 6))\ncorr = stats.corr()\nsns.heatmap(corr,\n            vmin=-1, vmax=1, center=0, \n            cmap=\"BrBG\",\n            square=True, annot=True)","9651b516":"df_address = pd.get_dummies(df['Address'])\n\nX = pd.concat([df.drop(columns=['Price(USD)', 'Address']), pd.get_dummies(df['Address'])], axis=1)\ny = df['Price(USD)']\n\nfor col in ['Parking', 'Warehouse', 'Elevator']:\n    X[col] = pd.to_numeric(X[col])\n\nX.head()","257d1d02":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","26f3650f":"regressors = [DecisionTreeRegressor(), GradientBoostingRegressor(), \n              RandomForestRegressor(), AdaBoostRegressor()]\nscores = []","546fcd0b":"for reg in regressors:\n    reg.fit(X_train,y_train)\n    score = cross_val_score(reg, X_train, y_train, scoring ='neg_mean_absolute_error', cv=5).mean()\n    scores.append(score)\n    \n    print('model: ' + str(reg))\n    print('mean MEA: {}'.format(round(score, 3)))\n    print('----------------------------')","e3e542ae":"# removing AdaBoost score from score list\nscores.remove(scores[-1])","bd15b88c":"tunned_models = []\nbest_scores = []","92bcfc7c":"DT = DecisionTreeRegressor()\n\nDT_param = {'criterion': ['mae'], \n            'max_depth': [None, 5, 10, 20, 40], \n            'min_samples_leaf': [1, 5, 8, 20, 40]}\ngsDT = GridSearchCV(DT, param_grid = DT_param, cv=8, scoring=\"neg_mean_absolute_error\", n_jobs=4, verbose = 1)\ngsDT.fit(X_train, y_train)","fbd8c3f4":"tunned_models.append(gsDT.best_estimator_)\nbest_scores.append(round(gsDT.best_score_, 3))\n\nprint('----------------------------')\nprint('model: ' + str(gsDT.best_estimator_))\nprint('mean MAE: {}'.format(round(gsDT.best_score_, 3)))\nprint('----------------------------')","bb7a0be9":"GB = GradientBoostingRegressor()\n\nGB_param = {'learning_rate': [0.2, 0.3, 0.4], \n            'n_estimators': [100, 150, 200],\n            'min_samples_leaf': [5, 8]}\n\ngsGB = GridSearchCV(GB, param_grid = GB_param, cv=8, scoring=\"neg_mean_absolute_error\", n_jobs=4, verbose = 1)\ngsGB.fit(X_train, y_train)","c09a4c82":"tunned_models.append(gsGB.best_estimator_)\nbest_scores.append(round(gsGB.best_score_, 3))\n\nprint('----------------------------')\nprint('model: ' + str(gsGB.best_estimator_))\nprint('mean MAE: {}'.format(round(gsGB.best_score_, 3)))\nprint('----------------------------')","54fa6e8a":"RF = RandomForestRegressor()\n\nRF_param = {'max_depth': [None, 70, 100, 120],\n            'n_estimators': [120, 140, 160]}\n\ngsRF = GridSearchCV(RF, param_grid = RF_param, cv=8, scoring=\"neg_mean_absolute_error\", n_jobs=4, verbose = 1)\ngsRF.fit(X_train, y_train)","d57c2042":"tunned_models.append(gsRF.best_estimator_)\nbest_scores.append(round(gsRF.best_score_, 3))\n\nprint('----------------------------')\nprint('model: ' + str(gsRF.best_estimator_))\nprint('mean MAE: {}'.format(round(gsRF.best_score_, 3)))\nprint('----------------------------')","291e1267":"s = scores\nb_s = best_scores\nmodels = ['Decision Tree', 'Gradient Boosting', 'Random Forest']\n\nfig, ax = plt.subplots(figsize=(7, 7))\nplt.grid(True)\n\nax.plot(models, [s[0], s[1], s[2]], 'go', ms=15, mec='black')   \nax.plot(models, [b_s[0], b_s[1], b_s[2]], 'go', ms=20, mec='black')   \n\nfor i, model in enumerate(models):\n    ax.plot([model, model], [s[i], b_s[i]], 'g', linewidth = 3)\n\nax.set_ylim(min(s)-5000, max(b_s)+5000)","b8a3851e":"final_model = gsGB if b_s.index(max(b_s)) == 1 else gsRF\nmodel_name = models[b_s.index(max(b_s))]\n\nprint('In my analise {} has the best score. That is why I will use it in the final test.'.format(model_name))","20a712d3":"y_pred = final_model.predict(X_test)\nfinal_measurement = mean_absolute_error(y_test, y_pred)\nprint('The test data is predicted. MAE: {}.'.format(math.floor(final_measurement)))","3f455a82":"df_test = pd.DataFrame(data={'test': list(y_test), 'pred': list(y_pred)})\n\nq1 = df_test['test'].quantile(0.25)\nq2 = df_test['test'].quantile(0.5)\nq3 = df_test['test'].quantile(0.75)","27daecf5":"df1 = df_test.where(df_test['test'] <= q1).dropna()\nMAE1 = math.floor(mean_absolute_error(df1['test'], df1['pred']))\nprint('The 25% of the cheapest houses.')\nprint('MAE: {}'.format(MAE1))\nprint('MAE\/mean: {}'.format(round(MAE1\/math.floor(df1['test'].mean()), 2)))\nprint('------------------------------------------------------')\n\ndf4 = df_test.where(q3 < df_test['test']).dropna()\nMAE4 = math.floor(mean_absolute_error(df4['test'], df4['pred']))\nprint('The 25% of the most expensive houses.')\nprint('MAE: {}'.format(MAE4))\nprint('MAE\/mean: {}'.format(round(MAE4\/math.floor(df4['test'].mean()), 2)))\nprint('------------------------------------------------------')","d571ba2d":"## 3.2 Scoring base models","e5e370e4":"The MAE of the cheapest house is strongly smaller than the most expensive one. However, error in affordable houses is more significant as for percentage of mean. That means that in this model more luxury houses are better predicted.","b52abd09":"# Introduction\nHi. In this notebook I will use four ML models and tune the best three of them. Next I will choose one with the highest score and predict the price of the Tehran houses from created test data. In order to find the most proper model I will score predictions using the MAE metrics.","548ecb8f":"# 1. First look at the data","9281eeb8":"In my opinion this score is quite hard to interpret. The 25% of the most expensive houses are much bigger than 25% of the most cheapest. Considering the MAE by the cheapest houses this prediction is dissatisfying but while taking into account these expansive parts it is a quite good prediction. That is why I divide my predictions into more relevant groups.","b3b05eab":"I decided to pick three of the best models. While AdaBoostRegressor has the worst MEA score I won't tune it and use further.","b76ba04a":"The area cannot be changed into integer because of some not appropriate values. I've checked the data and found the problem- some areas are separated by comma.","04e54966":"The graph shows that there is a strong linear relationship between the number of rooms and areas. Moreover, locations seem to be also correlated to these variables. It can be claimed that Salsabil is a location with affordable houses while in Niavaran there are more expensive accommodations with more rooms.\n","aa7e7ca1":"# 3. Modeling\n## 3.1 Preparing data","8bdbcb26":"## 3.3 Tunning best models","81fea4b0":"# 2. Exploratory data analysis","a010ef34":"## 3.4 Testing the best model and summary"}}