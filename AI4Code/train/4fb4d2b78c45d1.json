{"cell_type":{"2fa71be1":"code","c097a216":"code","ddbc9ead":"code","5374d7b1":"code","d7eed55b":"code","a0ee7ab1":"code","83132b37":"code","021d4553":"code","0d38e463":"code","227f0677":"code","2ad870cd":"code","b48d9d0d":"code","d035b353":"code","d5eff857":"markdown","54227eb0":"markdown","d493cc01":"markdown","ce2e0339":"markdown","3f8f4f8c":"markdown","43cd1511":"markdown","fd1772d5":"markdown","54264df8":"markdown"},"source":{"2fa71be1":"#Import necessary modules\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c097a216":"#Read data\ndf=pd.read_csv('..\/input\/synchronous-machine-dataset\/SynchronousMachine.csv')\ndf.head()","ddbc9ead":"#Check data summary\ndf.info()","5374d7b1":"#Visual EDA\nfig, ax = plt.subplots(figsize=(5,5)) \nsns.heatmap(df.corr(),annot=True,linewidths=.5, ax=ax)\nplt.show()","d7eed55b":"sns.scatterplot(x='I_y',y='I_f',data=df)\nplt.show()","a0ee7ab1":"sns.scatterplot(x='PF',y='I_f',data=df)\nplt.show()\n","83132b37":"X=df.drop(['I_f','d_if'],axis=1).values\ny=df['I_f'].values","021d4553":"from sklearn.model_selection import train_test_split,GridSearchCV\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)","0d38e463":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error as MSE\n\nmodel=RandomForestRegressor(n_estimators=100,max_depth=5)\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint('RSE=',MSE(y_test,y_pred)**(0.5))","227f0677":"df=pd.DataFrame({'y_test':y_test,'y_pred':y_pred})\nsns.scatterplot(x='y_test',y='y_pred',data=df,label='Real Prediction')\nsns.lineplot(x='y_test',y='y_test',data=df,color='red',alpha=0.5,label='Optimal Prediction')\nplt.title('y_test vs y_pred')\nplt.legend()\nplt.show()","2ad870cd":"model2=RandomForestRegressor(n_estimators=100)\nrse_train=[]\nrse_test=[]\nfor x in np.arange(1,10,1):\n    model2.max_depth=x\n    model2.fit(X_train,y_train)\n    y_pred2=model2.predict(X_test)\n    y_train_pred2=model2.predict(X_train)\n    rse_train.append(MSE(y_train,y_train_pred2)**(0.5))\n    rse_test.append(MSE(y_test,y_pred2)**(0.5))\n\ndf2=pd.DataFrame({'max_depth':np.arange(1,10,1),'rse_train':rse_train,'rse_test':rse_test})\ndf2['rse_difference']=(df2['rse_test']-df2['rse_train'])\/df2['rse_test']*100\ndisplay(df2)\nsns.lineplot(x='max_depth',y='rse_train',data=df2,label='rse_train')\nsns.lineplot(x='max_depth',y='rse_test',data=df2,label='rse_test')\nplt.ylabel('RSE')\nplt.legend()\nplt.show()","b48d9d0d":"model2=RandomForestRegressor(max_depth=4)\nrse_train=[]\nrse_test=[]\nfor x in [50,100,150,200]:\n    model2.n_estimators=x\n    model2.fit(X_train,y_train)\n    y_pred2=model2.predict(X_test)\n    y_train_pred2=model2.predict(X_train)\n    rse_train.append(MSE(y_train,y_train_pred2)**(0.5))\n    rse_test.append(MSE(y_test,y_pred2)**(0.5))\n\ndf2=pd.DataFrame({'n_estimators':[50,100,150,200],'rse_train':rse_train,'rse_test':rse_test})\ndf2['rse_difference']=(df2['rse_test']-df2['rse_train'])\/df2['rse_test']*100\ndisplay(df2)\nsns.lineplot(x='n_estimators',y='rse_train',data=df2,label='rse_train')\nsns.lineplot(x='n_estimators',y='rse_test',data=df2,label='rse_test')\nplt.ylabel('RSE')\nplt.legend()\nplt.show()","d035b353":"model_final=RandomForestRegressor(n_estimators=100,max_depth=4)","d5eff857":"# CREATE THE MODEL","54227eb0":"As we can see, the n_estimators doesn't really affecting the performance of the model. Based on the graph, the number of estimators of 100 is indeed the most optimom number.","d493cc01":"# How does this hyperparameter compares to other models?","ce2e0339":"After evaluating the initial model, the final model to predict the excitation current of synchronous motor is:","3f8f4f8c":"# EXPLORATORY DATA ANALYSIS","43cd1511":"As we can see, by tuning the hyperparameter, we could get a model that has low RSE when predicting both the test and train dataset. By modifying the max_depth, the RSE is getting lower until it reaches constant, but the difference between the train and test RSE is getting bigger. As we can see, the max_depth of 4 is the most optimal value for the creating the model because it has lower RSE than max_depth of 3, but with similiar differences of RSE between the train and test dataset. Compare it with the max_depth of 5. The RSE is only a few value lower, but the difference is twice of the max_depth of 4.","fd1772d5":"I assummed this data is of the same machine because I_f is perfectly correlated with d_if. I_f is excitation current while d_if is the variation of excitation current. By using only the d_if, we could predict I_f with 100% accuracy. **Hence, to make the dataset more interesting, I'm going to drop d_if**. The lowest correlation score is 0.42 for I_f vs I_y and the second lowest is 0.86 for I_f vs PF. Let's see how those two relationship looks like on a scatterplot.","54264df8":"# The Final Model"}}