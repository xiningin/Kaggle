{"cell_type":{"5ea522f7":"code","886b00b0":"code","7199a013":"code","fba0d345":"code","0032d2e6":"code","82cbe115":"code","d583036f":"code","62eb663a":"code","3b4fd17d":"code","0d42b42d":"code","3b51c5e1":"code","2cf594bf":"code","c22cd844":"code","7dfc2263":"code","2d22cbc2":"markdown","dd6f9055":"markdown","063adcad":"markdown","b39c5234":"markdown","cdb84e38":"markdown","b849406a":"markdown","0fd1fad4":"markdown","b1d10c89":"markdown","97b8fcfa":"markdown","e40cff95":"markdown","9873fea9":"markdown","f1ef0b41":"markdown"},"source":{"5ea522f7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","886b00b0":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain_df.head()","7199a013":"train_df = train_df.drop('Id', axis=1)","fba0d345":"target = 'SalePrice'\ncategorical_features = []\nnumeric_features = []\nfeatures = train_df.columns.values.tolist()\nfor col in features:\n    if train_df[col].dtype != 'object': \n        if col != target:\n            numeric_features.append(col)\n    else:\n        categorical_features.append(col)\n        \nfor col in numeric_features:\n    mean = train_df[col].mean()\n    train_df[col] = train_df[col].fillna(mean)\n    \nfor col in categorical_features:\n    train_df[col] = train_df[col].fillna('None')","0032d2e6":"train_df['SalePrice'] = np.log1p(train_df['SalePrice'])","82cbe115":"from scipy.stats import skew\nskewed_feats = train_df[numeric_features].apply(lambda x: skew(x)) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\ntrain_df[skewed_feats] = np.log1p(train_df[skewed_feats])","d583036f":"from sklearn.preprocessing import LabelEncoder\n# Encoding categorical features\nfor col in categorical_features:\n    le = LabelEncoder()\n    le.fit(list(train_df[col].astype(str).values))\n    train_df[col] = le.transform(list(train_df[col].astype(str).values))","62eb663a":"y = train_df['SalePrice']\nX = train_df.drop('SalePrice', axis=1)","3b4fd17d":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\n\nalpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\nmax_iter = [1000, 10000]\nl1_ratio = np.arange(0.0, 1.0, 0.1)\ntol = [0.5]\n\nelasticnet_gscv = GridSearchCV(estimator=ElasticNet(), \n                                param_grid={'alpha': alpha,\n                                            'max_iter': max_iter,\n                                            'l1_ratio': l1_ratio,\n                                            'tol':tol},   \n                                scoring='r2',\n                                cv=5)","0d42b42d":"elasticnet_gscv.fit(X, y)\nelasticnet_gscv.best_params_","3b51c5e1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","2cf594bf":"elasticnet = ElasticNet(alpha = elasticnet_gscv.best_params_['alpha'], \n                        max_iter = elasticnet_gscv.best_params_['max_iter'],\n                        l1_ratio = elasticnet_gscv.best_params_['l1_ratio'],\n                        tol = elasticnet_gscv.best_params_['tol'])\nelasticnet.fit(X_train, y_train)","c22cd844":"print(\"Training set score: {:.2f}\".format(elasticnet.score(X_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(elasticnet.score(X_test, y_test))) ","7dfc2263":"coef = pd.Series(elasticnet.coef_, index = X_train.columns)\nimportant_features = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])\nimportant_features.plot(kind = \"barh\")\nplt.title(\"Coefficients in the ElasticNet Model\")","2d22cbc2":"When we know about all the features in the dataset, it is easier to choose between Lasso Regression or Ridge Regression.\n\nBut what if we have millions of features and we certainly can not know much about features and their usefulness. So how do we choose between Lasso Regression and Ridge Regression. \n\nThe good news is that we dont have to choose, instead we can use Elastic Net Regression.","dd6f9055":"Lasso Regression works best when dataset contains lot of useless features. Lasso Regression would eliminate useless features by setting their coefficients to exactly zero, creating a simple model, easier to interpret.","063adcad":"Elastic Net Regression tries to minimize ","b39c5234":"Hybrid elastic net(0 < l1_ratio < 1) is useful when there are multiple features that are correlated with one another. \n\nLasso is likely to pick one of these at random, and eliminate others.\n\nridge regression tries to shrink coefficients of all the correlated features together.\n\nelastic-net can pick more than one correlated feature and shrink coefficients of correlated features together. ","cdb84e38":"## Build Elastic Net Regression model","b849406a":"## Apply Elastic Net Regression on House Price Dataset","0fd1fad4":"${L}_{enet}({\\hat\\beta}) = \\frac{\\sum\\limits_{i=1}^{n}({Y}_i - \\hat{Y}_i)^2}{2n} + {\\alpha}{\\rho} * \\sum\\limits_{j=1}^{m}|{\\hat\\beta}_j| + \\frac{{\\alpha}{(1 -\\rho)}}{2} * \\sum\\limits_{j=1}^{m}{\\hat\\beta}_j^2 $","b1d10c89":"Elastic Net combines feature elimination from Lasso and feature coefficient reduction from the Ridge model to improve your model\u2019s predictions.","97b8fcfa":"Ridge Regression works best when most of the features in dataset are useful for making predictions. Ridge Regression will shrink the coefficients so that changes in the values of the feature has lower impact on prediction.","e40cff95":"# Elastic Net Regression","9873fea9":"## Summary","f1ef0b41":"Now there are two parameters alpha (${\\alpha}$) and l1_ratio (${\\rho}$).\n\n${\\alpha}$ = 0, elastic_net = linear regression.\n\nl1_ratio - parameter controls the mixing of ridge and lasso.\n\nl1_ratio = 0, penalty is an L2 penalty (ridge).\n\nl1_ratio = 1, penalty is an L1 penality (lasso).\n\nFor 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\nBoth the parameters are learnt by cross-validation."}}