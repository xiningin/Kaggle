{"cell_type":{"f60e069a":"code","d90e4fe7":"code","408af5a4":"code","633e6e30":"code","d47808dd":"code","238bf5a4":"code","848382bb":"code","d75f9f4e":"code","4043adf2":"code","4fc7d993":"code","aeef2292":"code","abc3280d":"code","d636cf44":"code","8641dca7":"code","47cbefda":"code","92eeb5a2":"code","179d53e5":"code","a8ad53b2":"code","2ca397f9":"code","a262c7b1":"code","e77d0aea":"code","03553129":"markdown","4ca8cadf":"markdown","6aa10601":"markdown","faa64e90":"markdown","64b3c1d6":"markdown","c4729e6d":"markdown","577e5d00":"markdown","6f42c6f2":"markdown","06503ab2":"markdown","89285086":"markdown","9b4a6bb1":"markdown","4204a93d":"markdown","d74c929f":"markdown","5ef809d7":"markdown","2eee9ea9":"markdown","777e5c74":"markdown","4f3affbe":"markdown","8079a16f":"markdown","a8cdfcb4":"markdown","a5a77c5c":"markdown","dad6c2ea":"markdown","a77507bb":"markdown"},"source":{"f60e069a":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt","d90e4fe7":"input_size = 1\noutput_size = 1\nnum_epochs = 10000\nlearning_rate = 0.001","408af5a4":"x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168],[9.779], [6.182], [7.59], [2.167], [7.042], [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n\ny_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], [3.366], [2.596], [2.53], [1.221], [2.827], [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)","633e6e30":"model = nn.Linear(input_size, output_size)\n\n# Loss Function:\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)","d47808dd":"for epoch in range(num_epochs):\n    inputs = torch.from_numpy(x_train)\n    targets = torch.from_numpy(y_train)\n    \n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch + 1) % 1000 == 0:\n        print(\"Epoch: {}\/{}; \\tLoss: {}\".format(epoch + 1, num_epochs, loss.item()))","238bf5a4":"predicted = model(torch.from_numpy(x_train)).detach().numpy()\n\nplt.scatter(x_train, y_train, label='original data')\nplt.scatter(x_train, predicted, label='predicted data')\nplt.legend()\nplt.show()","848382bb":"import torchvision\nimport torchvision.transforms as transforms","d75f9f4e":"input_size = 784\nnum_classes = 10\nnum_epochs = 20\nbatch_size = 100\nlearning_rate = 0.001","4043adf2":"train_dataset = torchvision.datasets.MNIST(root=\".\/data\", train=True, transform=transforms.ToTensor(), download=True)\ntest_dataset = torchvision.datasets.MNIST(root=\".\/data\", train = False, transform=transforms.ToTensor(), download=True)","4fc7d993":"plt.imshow(train_dataset.train_data[0])\nplt.show()","aeef2292":"trainloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntestloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)","abc3280d":"model = nn.Linear(input_size, num_classes)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)","d636cf44":"total_step = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(trainloader):\n        images = images.reshape(-1, 28 * 28)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        \n        optimizer.step()\n        \n        if (i + 1) % 200 == 0:\n            print(\"Epoch: {}\/{}, \\tIteration: {}\/{}, \\tLoss: {}\".format(epoch + 1, num_epochs, i + 1, len(trainloader), loss.item()))","8641dca7":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in testloader:\n        images = images.reshape(-1, 28 * 28)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        correct += (predicted == labels).sum()\n        total += labels.size(0)\n    print(\"Accuracy of the model: {}\".format(float(correct) \/ float(total)))\n        \n        ","47cbefda":"input_size = 28 * 28\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 20\nbatch_size = 100\nlearning_rate = 0.001","92eeb5a2":"train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle = True)","179d53e5":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","a8ad53b2":"import torch.nn.functional as F\n\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        out = F.relu(self.fc1(x))\n        out = self.fc2(out)\n        return out    ","2ca397f9":"model = NeuralNet(input_size, hidden_size, num_classes)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)","a262c7b1":"for epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.reshape(-1, 28 * 28).to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i + 1) % 200 == 0:\n            print(\"Epoch: {}\/{};\\tIteration: {}\/{}; Loss: {}\".format(epoch + 1, num_epochs, i + 1, len(train_loader), loss.item()))","e77d0aea":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for i, (images, labels) in enumerate(test_loader):\n        images = images.reshape(-1, 28 * 28).to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum()\n        total += labels.size(0)\nprint(\"Accuracy of the model {}\".format(float(correct)\/float(total)))","03553129":"#### Linear Regression Model","4ca8cadf":"#### Defining the network parameters","6aa10601":"#### loding the Dataset","faa64e90":"# Logistic Regression Model\nWe will use a logistic regression model to classify the MNIST dataset","64b3c1d6":"#### Plotting the outputs","c4729e6d":"#### Importing the packages","577e5d00":"# Foreword\nThis tutorial is part of a series of tutorials on PyTorch, the deep learning library developed by Facebook. This is the second tutorial of the series. To go to the first tutorial please go to this notebook: https:\/\/www.kaggle.com\/krishanudb\/pytorch-tutorial-for-beginners\n\nIn this tutorial we will learn how to make simple computation graphs like linear regression and logistic regression using PyTorch. So lets jumo right in.\n\n### Acknowledgements\nhttps:\/\/github.com\/yunjey\/pytorch-tutorial","6f42c6f2":"### Model definition","06503ab2":"#### Testing the model\nIn this phase we donot need to keep track of the gradients. So in order to save memory, we use the torch.no_grad() function","89285086":"#### Create DataLoader objects","9b4a6bb1":"# Linear Regression","4204a93d":"#### Device Configuration","d74c929f":"That brings us to the end of the second tutorial of the series. Hope you liked it.\nThe next tutorial of the series will introduce Convolutional Neural Networks and gradually move on to more advanced model architectures like inception networks and UNets. Do check out the tutorial at: https:\/\/www.kaggle.com\/krishanudb\/tutorial-on-convolutional-nets-in-pytorch","5ef809d7":"# Feed Forward Neural Network Model","2eee9ea9":"#### Defining the Hyperparameters","777e5c74":"#### Defining a Toy Dataset","4f3affbe":"#### Create DataLoader Objects","8079a16f":"#### Training the Model","a8cdfcb4":"#### Defining Hyperparameters","a5a77c5c":"#### Training the Model","dad6c2ea":"#### Training the Model","a77507bb":"## The neural network model performs slightly better than the Logistic Regression Model"}}