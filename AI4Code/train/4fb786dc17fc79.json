{"cell_type":{"74d428a5":"code","363f732f":"code","a2341efa":"code","4fc327bc":"code","ddb1bff7":"code","0e7e2b5d":"code","e41d62e2":"code","813abd91":"code","738108ce":"code","669071f0":"code","e5dc298c":"code","bae604cc":"code","635bb9b9":"code","cacc6d57":"code","875c600d":"code","d580ebd3":"code","12355753":"code","0967a565":"code","c751c39b":"code","a9894252":"code","28b652df":"code","454f80c5":"markdown","df13d360":"markdown","ed355b6c":"markdown","3facefe8":"markdown","6e035167":"markdown","52eb010a":"markdown","bf7ee4a2":"markdown","cf26d9a3":"markdown","9d57e9a5":"markdown","7bf8cc35":"markdown","3ded05ce":"markdown","82d6bc06":"markdown","2c0b473c":"markdown","be4655cc":"markdown","b6ee3e9f":"markdown","6b94ce9e":"markdown","2ca07948":"markdown","a7780f05":"markdown","7c5272ef":"markdown","609bb6b7":"markdown","58c10797":"markdown","ab447c7a":"markdown","c319ebf1":"markdown","348ea068":"markdown","4c1554fb":"markdown","230a044f":"markdown","ee1b6110":"markdown","520c0575":"markdown","97c374c5":"markdown","8509cda8":"markdown","a6fff8f6":"markdown","f5e4e770":"markdown"},"source":{"74d428a5":"import json\nimport re\nimport string\nimport textwrap\nimport itertools\nimport numpy as np","363f732f":"path_dataset = '..\/input\/arxiv\/arxiv-metadata-oai-snapshot.json'\n# Construct a generator to read the observations in the json file one by one\ngen_json = (json.loads(line) for line in itertools.islice(open(path_dataset,'r'),5000))\n# Take the 'categories' field of each record, split it by blank spaces to obtain the list of categories the record belongs to.\n# Then split each category name by a dot '.' and keep only the first part describing the broad category name.\ntemp_labels = [elem.split('.')[0] for line in [line['categories'].split() for line in gen_json] for elem in line]\ntemp_abs = [line['abstract'] for line in gen_json]\n# Sort the set of unique categories, record the number of categories (19) and display what these are \ncategories = list(sorted(set(temp_labels)))\nnum_cat = len(categories)\nprint(f'There are {num_cat} categories, these are the following:')\nprint(categories)","a2341efa":"# Number of observations to load\nnum_examples = 10000\ngen_json = (json.loads(line) for line in itertools.islice(open(path_dataset,'r'),num_examples))\n# Initialise an empty list to store the abstracts and an array of zeros to store the labels\n# The labels will be stored using one-hot encoding, thus, we know the exact dimensions of the array\nabstracts = []\nlabels = np.zeros((num_examples,num_cat))\nfor idx, line in enumerate(gen_json):\n    # Read abstract, transform to lowercase.\n    abs = line['abstract'].lower()\n    # Remove newline characters\n    abs = re.sub(r'(\\S)\\n(\\S)',r'\\1 \\2',abs)\n    # Fix corrupted characters\n    abs = abs.replace('\u00e2\\x80\\x99',\"'\")\n    abs = abs.replace('\\x7f',\"\")\n    abs = abs.replace('\u00e2\\x88\\x9e',\"'\")\n    abs = abs.replace('\u00e2\\x89\u00a4',\"'\")\n    abs = abs.replace('\u00e2\\x80\\x94',\"'\")\n    abs = abs.replace('\u00e2\\x80\\x93',\"-\")\n    # Replace latex in-line math expressions $...$ with 'xmathx'\n    abs = re.sub(r'\\$.+?\\$','xmathx',abs)\n    # Replace any number that may contain a sign and a decimal part with 'xnumx'\n    abs = re.sub(r'-*\\+*\\d+\\.*\\d*%*','xnumx',abs)\n    # Matches operations between numbers expressed by a single character, such as * - + ^ \/\n    abs = re.sub(r'\\(*xnumx\\)*(\\s*\\S{1}\\s*\\(*xnumx\\)*)*','xnumx',abs)\n    # Matches physical units to some power, e.g. m^3\n    abs = re.sub(r'(\\S)*\\^(xnumx)',r'xunitx',abs)\n    # Matches digits and characters with no whitspace between them, such as 'NGC6752'\n    abs = re.sub(r'((xnumx)+[a-z]+)|([a-z]+(xnumx)+)\\S*','xalphax',abs)\n    # Not all math formulas are enclosed in $...$. Try to catch some of these instances\n    # Matches non-whitespace that has an underscore or a caret\n    abs = re.sub(r'\\S+[_^]\\S+','xmathx',abs)\n    # Matches non-whitespace that has a double backslash except \\'\n    abs = re.sub(r'\\S*\\\\[^\\\\\\']\\S+','xmathx',abs)\n    # Matches two non-whitespaces with a binary comparator operator in the middle \n    # except '-' with optional whitespace\n    abs = re.sub(r'\\S+\\s*([=+><]|>=|=>|<=|=<|=)\\s*\\S+','xmathx',abs)\n    # Matches two math formulas with a '-' in the middle, i.e. xmathx-xmathx\n    # It does not match hyphenated text\n    abs = re.sub(r'xmathx\\s*-\\s*xmathx','xmathx',abs)\n    # Add standardised abstract to the list\n    abstracts.append(abs)\n    # Read list of categories of the current observation\n    cat = line['categories'].split()\n    # Record labels using one-hot encoding\n    for elem in cat:\n        labels[idx,categories.index(elem.split('.')[0])] = 1","4fc327bc":"print(len(abstracts))\nprint(labels.shape)","ddb1bff7":"import pandas as pd\ndf_labels = pd.DataFrame(labels,columns=categories)","0e7e2b5d":"df_labels.agg('sum',axis=0).plot(kind='bar');\ntotal_counts = df_labels.agg('sum',axis=0)\ntotal_counts = pd.concat([total_counts,total_counts\/total_counts.sum()*100],axis=1)\ntotal_counts.columns = ['count','proportion (%)']\ntotal_counts","e41d62e2":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.text import TextCollection\nnltk.download('stopwords');\nnltk.download('punkt');","813abd91":"textwrap.wrap(', '.join(stopwords.words('english')),70)","738108ce":"add_stopwords = [ 'show','results','also','two','using','study','present',\n                  'one','paper','find','new','based',\"''\",\"'s\",'used']","669071f0":"tokenized_abs = [nltk.word_tokenize(abs) for abs in abstracts]\nstop_words = set(stopwords.words('english')+add_stopwords)\ncl_tkn_abs = [[tkn for tkn in abs if \n                (tkn not in stop_words) and \n                (tkn not in '!\"#$%&\\'()*+,-.\/:;<=>?@[\\]^_`{|}~\\n')] \n              for abs in tokenized_abs]","e5dc298c":"txt_coll = TextCollection(cl_tkn_abs)","bae604cc":"vocab = txt_coll.vocab()\nvocab.most_common(20)","635bb9b9":"num_top_words = 2000\ntop_words = [k for k,v in vocab.most_common(num_top_words)]\n# Initialise an array of zeros to save the frequencies\nboolean_freq = np.zeros((num_examples,num_top_words))\nfor i,abs in enumerate(cl_tkn_abs):\n    temp_abs = set(abs)\n    for j,word in enumerate(top_words):\n        boolean_freq[i,j] = word in temp_abs","cacc6d57":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression","875c600d":"X_train, X_test, y_train, y_test = train_test_split(boolean_freq,labels,test_size=0.30,random_state=101)\nprint([X_train.shape, X_test.shape, y_train.shape, y_test.shape])","d580ebd3":"clf = {}\ny_pred = {}\nclf['logr'] = OneVsRestClassifier(LogisticRegression(solver='lbfgs')).fit(X_train,y_train)\n# clf['logr'] = OneVsRestClassifier(LogisticRegression(solver='sag',max_iter=500)).fit(X_train,y_train)\nclf['nb'] = OneVsRestClassifier(MultinomialNB()).fit(X_train,y_train)\ny_pred['logr'] = clf['logr'].predict(X_test)\ny_pred['nb'] = clf['nb'].predict(X_test)","12355753":"print('Logistic Regression')\nprint(classification_report(y_test,y_pred['logr'],target_names=categories));\nprint('Naive Bayes')\nprint(classification_report(y_test,y_pred['nb'],target_names=categories));","0967a565":"print('Logistic Regression\\n')\nprint(classification_report(y_test[:,9],y_pred['logr'][:,9],target_names=['not math','math']))\nprint(classification_report(y_test[:,0],y_pred['logr'][:,0],target_names=['not astro-ph','astro-ph']))\nprint(classification_report(y_test[:,1],y_pred['logr'][:,1],target_names=['not cond-mat','cond-mat']))\nprint('-'*70)\nprint('Naive Bayes\\n')\nprint(classification_report(y_test[:,9],y_pred['nb'][:,9],target_names=['not math','math']))\nprint(classification_report(y_test[:,0],y_pred['nb'][:,0],target_names=['not astro-ph','astro-ph']))\nprint(classification_report(y_test[:,1],y_pred['nb'][:,1],target_names=['not cond-mat','cond-mat']))","c751c39b":"# Weights in the logistic regression for 'math' (index 9), separated into positive and negative values\ntemp_coefs_p = [(word,coef) for word,coef in zip(top_words,clf['logr'].estimators_[9].coef_[0]) if coef>0]\ntemp_coefs_n = [(word,coef) for word,coef in zip(top_words,clf['logr'].estimators_[9].coef_[0]) if coef<0]\n# Note that weights are sorted according to their absolute value\ntemp_coefs_p = sorted(temp_coefs_p,key=lambda x:np.abs(x[-1]),reverse=True)\ntemp_coefs_n = sorted(temp_coefs_n,key=lambda x:np.abs(x[-1]),reverse=True)\n\n# Probability distribution for 'math' (index 9) in the naive Bayes classifier\n# Note that coef_ returns the logarithmic probability, so we pass it through an exponential to get the probability\ntemp_coefs_nb = [(word,np.exp(coef)) for word,coef in zip(top_words,clf['nb'].estimators_[9].coef_[0])]\ntemp_coefs_nb = sorted(temp_coefs_nb,key=lambda x:np.abs(x[-1]),reverse=True)\n\nprint('math (Mathematics)\\n')\nprint('Logistic regression decision words (discriminative model)')\nprint('-'*70)\nfor i in range(10):\n    # Logistic regression coefficients\n    (word_p,coef_p) = temp_coefs_p[i]\n    (word_n,coef_n) = temp_coefs_n[i]\n    print(f'{word_p:<15}: {coef_p:>+0.4f} \\t {word_n:<15}: {coef_n:>+0.4f}')\n\nprint('\\n')\nprint('Naive Bayes classifier probable\/improbable words (generative model)')\nprint('-'*70)\nfor i in range(10):\n    # Naive Bayes classifier coefficients\n    (word_high,coef_high) = temp_coefs_nb[i]\n    (word_low,coef_low) = temp_coefs_nb[-i-1]\n    print(f'{word_high:<15}: {coef_high:>+0.8f} \\t {word_low:<15}: {coef_low:>+0.8f}')","a9894252":"temp_coefs_p = [(word,coef) for word,coef in zip(top_words,clf['logr'].estimators_[0].coef_[0]) if coef>0]\ntemp_coefs_n = [(word,coef) for word,coef in zip(top_words,clf['logr'].estimators_[0].coef_[0]) if coef<0]\ntemp_coefs_nb = [(word,np.exp(coef)) for word,coef in zip(top_words,clf['nb'].estimators_[0].coef_[0])]\ntemp_coefs_p = sorted(temp_coefs_p,key=lambda x:np.abs(x[-1]),reverse=True)\ntemp_coefs_n = sorted(temp_coefs_n,key=lambda x:np.abs(x[-1]),reverse=True)\ntemp_coefs_nb = sorted(temp_coefs_nb,key=lambda x:np.abs(x[-1]),reverse=True)\nprint('astro-ph (Astrophysics)\\n')\nprint('Logistic regression decision words (discriminative model)')\nprint('-'*70)\nfor i in range(10):\n    # Logistic regression coefficients\n    (word_p,coef_p) = temp_coefs_p[i]\n    (word_n,coef_n) = temp_coefs_n[i]\n    print(f'{word_p:<15}: {coef_p:>+0.4f} \\t {word_n:<15}: {coef_n:>+0.4f}')\n\nprint('\\n')\nprint('Naive Bayes classifier probable\/improbable words (generative model)')\nprint('-'*70)\nfor i in range(10):\n    # Naive Bayes classifier coefficients\n    (word_high,coef_high) = temp_coefs_nb[i]\n    (word_low,coef_low) = temp_coefs_nb[-i-1]\n    print(f'{word_high:<15}: {coef_high:>+0.8f} \\t {word_low:<15}: {coef_low:>+0.8f}')","28b652df":"temp_coefs_p = [(word,coef) for word,coef in zip(top_words,clf['logr'].estimators_[1].coef_[0]) if coef>0]\ntemp_coefs_n = [(word,coef) for word,coef in zip(top_words,clf['logr'].estimators_[1].coef_[0]) if coef<0]\ntemp_coefs_nb = [(word,np.exp(coef)) for word,coef in zip(top_words,clf['nb'].estimators_[1].coef_[0])]\ntemp_coefs_p = sorted(temp_coefs_p,key=lambda x:np.abs(x[-1]),reverse=True)\ntemp_coefs_n = sorted(temp_coefs_n,key=lambda x:np.abs(x[-1]),reverse=True)\ntemp_coefs_nb = sorted(temp_coefs_nb,key=lambda x:np.abs(x[-1]),reverse=True)\nprint('cont-mat (Condensed matter)\\n')\nprint('Logistic regression decision words (discriminative model)')\nprint('-'*70)\nfor i in range(10):\n    # Logistic regression coefficients\n    (word_p,coef_p) = temp_coefs_p[i]\n    (word_n,coef_n) = temp_coefs_n[i]\n    print(f'{word_p:<15}: {coef_p:>+0.4f} \\t {word_n:<15}: {coef_n:>+0.4f}')\n\nprint('\\n')\nprint('Naive Bayes classifier probable\/improbable words (generative model)')\nprint('-'*70)\nfor i in range(10):\n    # Naive Bayes classifier coefficients\n    (word_high,coef_high) = temp_coefs_nb[i]\n    (word_low,coef_low) = temp_coefs_nb[-i-1]\n    print(f'{word_high:<15}: {coef_high:>+0.8f} \\t {word_low:<15}: {coef_low:>+0.8f}')","454f80c5":"# 4. Comparative analysis","df13d360":"We will construct a dictionary of classifiers with two entries. Each entry contains an instance of the OneVsRestClassifier, which in turns holds 19 classifiers, one for each label. The predicted labels are similarly captured in a dictionary holding two arrays of dimensions (# of examples, # of classes).\n\nAs a technical note, in the following cell you can change the solver for the logistic regression to improve performance depending on the size of the training set. For small data sets, e.g. 7k training examples, the default solver 'lbfgs' works well. For large sets e.g., >14k training examples, 'sag' is faster. You may need to increase the number of iterations in the latter case for the solver to converge.","ed355b6c":"Next we need to tokenise the text. We will use the nltk module to accomplish this. This means that the text is separated into units that we can use for training a classifier. Tokens are typically words, but depending on the application it could also be sentences or paragraphs. In this case, word tokenisation is more suitable.\n\nMoreover, we will be using a bag of words approach, i.e. we assume that the order of the words does not matter. This is a very rough approximation, however it allows for great simplicity and you will see that the results are not too bad. Thus, we should filter out useless words such as stop word, i.e., words so common that they appear everywhere and are not useful for separating the classes. The nltk module has a collection of stop words derived from common words in English","3facefe8":"Now that we have features and labels we can train the models. We will use two classifiers, one is a logistic regression which is a discriminative model, and the other, a naive Bayes classifier, a generative model. \n\nThe logistic regression is a linear regression passed through the logistic function. This function maps any value to the domain [0,+1]. The reason for this mapping is to give the output the interpretation of a probability. As with any regression, the weights characterise the relation between an input and the output ignoring variations on any other inputs.\n\nThe naive Bayes classifier constructs the conditional probability distribution of the inputs given a target value. In other words, it assigns a probability to each feature per each existing label based on the values in the training set. The 'naive' part refers to the fact that the model assumes all features to be independent of each other. Thus, the values assigned to the features are probabilities that sum up to one within each label.\n\nWe will use the scikit-learn implementation of these models as it's straightforward to use. Note that this implemention does not support multi-label classification by itself (not to be confused with multi-class classification, which is supported). Therefore, we will follow a one vs. rest strategy and train a classifier for each category. This can be conveniently achieved using the OneVsRestClassifier object from sklearn.multiclass","6e035167":"Papers in the arXiv are catalogued in a variety of broad categories. Some of these, in turn, may be subdivided into more specific categories. In this exercise, we will only consider classification on the broad categories. These can be inferred from looking at the categories on arxiv.org, or by going through the first few thousand observations in the data set. We might as well capture the abstracts since we're iterating over the dataset already.","52eb010a":"There are two main approaches to classification in machine learning, discriminative models and generative models. Loosely speaking, the former focuses on modeling the decision boundary between classes, while the latter, on modeling the classes themselves. There are plenty of resources online explaining in detail the differences between them  (see, for example, [[1](https:\/\/ai.stanford.edu\/~ang\/papers\/nips01-discriminativegenerative.pdf), [2](https:\/\/stackoverflow.com\/questions\/879432\/what-is-the-difference-between-a-generative-and-a-discriminative-algorithm), [3](https:\/\/towardsdatascience.com\/generative-vs-2528de43a836), [4](https:\/\/stats.stackexchange.com\/questions\/12421\/generative-vs-discriminative)]). However, the question still remains a hot topic among machine learning newcomers as of today.\n\nThis notebook presents an illustrative example of the difference between discriminative and generative models in a text classification task using simple linear classifiers. Hopefully, this can shed some light on what it is exactly that the two categories of models are doing differently. I find that this example is specially illumating since reading and classifying a text is something we are all familiar with. Moreover, the exercise is quite simple and the code is straightforward to follow.\n\nThe task is to classify documents in the appropriate category. The documents in question are abstracts from scientific articles published on the [arXiv](https:\/\/arxiv.org). Since an article may correspond to multiple categories, this is a multi-label classification task. For instance, a paper could concern both theoretical physics and general relativity (see cross-lists). The abstracts and categories are found in the [arXiv metedata dataset](https:\/\/kaggle.com\/Cornell-University\/arXiv) hosted in Kaggle. This Kaggle notebook already contains the dataset in the path '..\/input\/arxiv\/arxiv-metadata-oai-snapshot.json'\n\nThe outline of the notebook is as follows. The first section consists of loading the dataset and a brief data exploratory analysis to understand the problem. The second section constructs the set of features that will be used to train the models. In the the third section, the linear classifiers are introduced and trained. The fourth section provides a comparative analysis by inspecting the most and least informative features that the models have learnt. Finally, the fifth section provides a recap and the conclusions.","bf7ee4a2":"TextCollection also contains the frequency distribution of tokens in the vocabulary across the whole corpus. We can take a look at the 20 most common tokens to find that most documents contain mathematical formulas (xmathx), numbers (xnumx) and some type of alphanumeric words (xalphax). Recalling the distribution of labels we computed earlier this should not be surprising, as these are mostly physics and mathematics texts. Further down the list, some field-specific terms start to appear, such as 'field', 'quantum', 'data', 'mass' and 'magnetic'. These are all keywords that could be used to distinguish texts from different categories. For instance, an abstract containing the word 'magnetic' is not likely to be belong to the category of mathematics or general relativity but to condensed matter instead.","cf26d9a3":"Before reading further, take a moment to closely inspect the tokens shown in the lists and try to draw your own conclusions. Think about the following questions: \n* What is the difference between weights having a positive or a negative sign in the logistic regression? \n* Do you notice any differences between tokens associated with positive and negative signs?\n* What is the interpretation of the coefficients in the naive Bayes list?\n* What is the difference between the tokens in both columns in the naive Bayes list in the context of maths papers?\n* What are the similarities and differences between tokens in both lists?","9d57e9a5":"Note that 'xmathx' and 'xnumx' appear in all three classes with naive Bayes, as abstracts from these categories all make use of mathematical formulas and numbers. \n\nDiscriminative models return the words that are semantically related to each subject; maths: {combinatorial,torus,integral}, astrophysics:{cosmic,stars,accretion}, condensed matter:{graphene,device,semiconductor}. The weights define the decision boundary, interpreted in this case as words that occur in one context but not in the other.\n\nGenerative models return the most common words within each category; a maths abstract likely includes formulas, numbers and 'integral', an astrophysics one likely inlcudes formulas, numbers and 'stars', a condensed matter one, formulas, numbers and 'graphene'. The assumption is that the text was generated by drawing tokens at random from a probability distribution, this is the distribution the model tries to find. In theory, with a sufficiently sophisticated generative model, we could write an abstract of any chosen category by drawing tokens from this probability distribution (see e.g. [OpenAI's GPT-3](https:\/\/openai.com\/)).","7bf8cc35":"This notebook has attempted to illustrate the difference between discriminative and generative models using a simple document classification task. Discriminative models learn to model a decision boundary to distinguish different categories. Thus, a logistic regression identifies words in the semantic group of each category, e.g. maths:{combinatorial, torus}, astrophysics:{cosmic, stars}, condensed matter:{graphene, semiconductor}. On the other hand, generative models learn to construct a typical example of a given category. The naive Bayes classifier learns that all abstracts must include formulas and numbers, as well as keywords that often appear in abstracts of a given category.\n\nEach type of model may have their advantages and disadvatanges, and there is not one that is always best than the others. Thus, selecting a model is a non-trivial task that depends on the problem to solve and on the data and resources available.\n\nHope you found this notebook useful. Let me know your comments and questions below. Suggestions are also welcome as I'm also learning!","3ded05ce":"Most examples in the sample belong to three categories, 'math', 'astro-ph' and 'cond-mat', while very few correspond to 'econ' and 'q-fin'.","82d6bc06":"To understand the types of documents we are dealing with and what we may encounter in the data you should go through a few examples. The abstracts were saved in 'temp_abs' in the previous cell, you can print a few of them. In particular, it is instructive to look at the abstracts in indices 0, 2, 5, 19 and 138. For these examples, we can notice the following characteristics:\n* Newline characters are included within the text to serve as text wrap for aesthetic purposes. These can break sentences and even words.\n* There may be numbers with positive or negative signs, percentage signs and exponents\n* There may be physical units, such as m\/s^2 (meters over seconds squared)\n* Mathematical formulas are mostly expressed using latex in-line math notation, enclosed by \\\\$...$\n* However, formulas may sometimes be written as raw text\n* Abstracts may include proper nouns following scientific nomenclature, such as 'NGC6752'\n\nIt will be useful to standardise all these special cases to reduce the vocabulary in the data. For example, x = 2 and y = 3 are both examples of mathematical formulas and can be replaced by a single token. Tokens are the units that the text will be parsed into. Similarly, it is unlikely that a particular number would be more predictive than other, therefore, all ocurrences of numbers can be replaced by a single token as well. Based on this, we define a few rules to standardise the text. Additionally, in rare occasions a character may have become corrupted at some point during encoding. We can add special rules to treat these cases, however, if we stick to only the first 20k examples in the data set this should not be an issue.\n\nThe following cell reads the first (num_examples) abstracts and performs some transformations to standardise the text. The labels are encoding using a one-hot representation.","2c0b473c":"# 2. Construct features","be4655cc":"# 1. Loading the data and exploratory data analysis","b6ee3e9f":"# 3. Train the models","6b94ce9e":"Let's take a look at the distribution of labels in the data set. We will briefly use pandas to do this.","2ca07948":"In the approach we follow here, the tokens will define the features of the data via the document-term matrix, a matrix where each row is a document (observation) and each column is a token (feature). What the values in this matrix are is up to us. Common choices are to fill the matrix with boolean frequencies, frequency counts, normalised frequency counts and tf-idf (term frequency-inverse document frequency). It will be convenient to save the abstracts in a TextCollection object from the nltk.text module.","a7780f05":"Split the data into training and testing sets. Note that since we will be using the OneVsRestClassifier object, the targets can be passed in the one-hot representation as a matrix of dimensions (num_examples,num_labels)","7c5272ef":"The following modules will be useful for processing the dataset","609bb6b7":"# 5. Conclusions","58c10797":"In this example, we will keep only the top 2k most frequent tokens and compute their boolean frequencies, i.e. 1 if they appear in a given document and 0 otherwise.","ab447c7a":"At this point you may already have a good idea of what the models are doing differently. We'll write it down explicitly in here.\n\nIn the logistic regression, tokens with a positive weight indicate that if such a token is present, the document is more likely to belong to the 'math' category. Likewise, tokens with a negative weight indicate that if that token appears, the document is more likely to belong to other category. The larger the magnitude, the stronger the association. Thus, those with positive weights are tokens that can be found in maths papers but are not so commonly found in other papers. Inded, 'combinatorial', 'decomposition' and 'series' are mostly found in mathematics papers compared to others categories. The token 'p' can be found in the context of 'P vs NP', 'p-adic numbers' and is commonly used in mathematical notation. On the other hand, 'gravitational', 'spacetime', 'gauge' and 'supersymmetry' are words that are more commonly found in papers about general relativity or theoretical physics rather than  mathematics.\n\nIn the naive Bayes classifier, the coefficient represents the probability of a token to appear in a 'math' abstract. Tokens with higher probability are more likely to appear, while those with less probability as less likely. Note that this classifier does not care if the tokens also commonly appear in other categories, it just pays attention to the tokens that often appear in 'math'. Most abstracts in the 'math' category include a mathematical formula at some point, thus, the token 'xmathx' has the largest probability of appearing. Similarly, 'prove', 'space', 'finite' and 'group' also commonly appear in maths papers, as well as numbers, represented by 'xnumx'. On the other hand, words like 'insulator', 'singlet', 'photonic' and 'tevatron' seldomly appear in maths papers, thus they are assigned a small probability.\n\nComparing the two lists makes the difference between discriminative and generative models evident. Recall that 'xmathx' appears as the most frequent token in the corpus. Naive Bayes learnt that most math papers contain a formula, so it gave 'xmathx' a high probability. However, it does not care for the fact that a good amount of scientific abstracts of all categories also contain a formula at some point. This is the reason why 'xmathx' does not appear in the list for logistic regression. Logistic regression knows that 'xmathx' appears everywhere and hence, it is not a good discriminant (we said the word!). Logistic regression is identifying the tokens that appear mostly in maths but not in other categories, while naive Bayes, those that appear more frequently in maths, independent of whether they also appear in other categories. In other words, logistic regression is telling us how to distinguish (discriminate) a mathematics abstract from others, while naive Bayes is telling us how to write (generate) a mathematics abstract.\n\nTake a look at the lists for the classifiers of the 'astro-ph' and 'cond-mat' categories as well to convice yourself this makes sense.","c319ebf1":"This is the main section of the study where we take a closer inpection into what exactly the models are doing. As explained in the previous section, the logistic regression is learning weights which describe the relation of inputs to the labels. The larger the magnitude of this coefficient (weight), the stronger the association, which could be either positive or negative. Meanwhile, the naive Bayes classifier learns conditional probability distributions, i.e. the probability distribution over the features given a label. Probabilities can only be non-negative. The larger the probability of a feature given a label, the more likely that a text belonging to that label would contain that feature.\n\nIn the cell below, we extract the weights of the logistic regression, stored in 'coef_' for a given classifier (estimator) and order them according to their magnitude. Each weight is associated to a token. It is instructive to look at the tokens with the largest weight magnitude both with positive and negative signs. In the naive Bayes classifier, 'coef_' stores the logaritmic probability of the tokens for each classifier (estimator). Therefore, the tokens can be ordered from higher to smaller probability. The following cell displays the top and bottom ten tokens that the models have learn to use for deciding if a document belong to the 'math' category or not.","348ea068":"Although more complete approaches can undoubtedly achieve higher accuracy, the classifiers have learnt enough to illustrate the point we are trying to make in here, understanding the difference between discriminative and generative models. This is explained in detail in the following section.","4c1554fb":"Evidently, the classes are largely unbalanced. Thus, if we train the same classifier to predict all classes, we can expect that it will perform much better on the classes with the largest volume. Note that since this is a multi-label classification task, the sum of the the counts reported in the table above is larger than the number of observations.","230a044f":"These words are commonly used in scientific writing. A generic example of an abstract would go something like this:\n\n\"In this paper\/stuy, we present\/show a new [...]. Using the method of\/Based on [...], we find [...] . The results (also) show that [...]\"\n\nThus, we tokenise the text removing stopwords and any special or puctuation characters that were not captured by the previous processing rules.","ee1b6110":"Verify the dimensions of the abstract list and labels array","520c0575":"# Generative vs Discriminative models. An illustrated example in text classification","97c374c5":"In addition to common words in the language, we should also search for stop words specific to the corpus we are working with. You can do this by running the tokeniser once and inspecting the most common tokens that do not carry much semantic meaning. Doing this, you can identify that the following tokens should be added to the list of stop words:","8509cda8":"It is interesting to note that in this example, the logistic regression trades-off recall for precision, while in naive Bayes it is the other way around. This is not the case in general, however.\nThe global score is not great, nonetheless, the classifiers for three categories with the most volume of observations seem to be doing relatively well.","a6fff8f6":"The predefined stop words from nltk are the following:","f5e4e770":"Next, we print out the classification report with the precision, accuracy and f1-score of the classifiers. Note that the classifiers work better for categories with a high volume of observations, as expected."}}