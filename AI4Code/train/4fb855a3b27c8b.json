{"cell_type":{"668d0f12":"code","7212d01e":"code","c74c22f3":"code","3df98469":"code","a83c9d6f":"code","b2ffacb7":"code","2d2a27b9":"code","8e0e80ed":"code","b2e122ac":"code","2e97d313":"code","5889ee4e":"code","a88fa2f2":"code","51fbaf6e":"code","da5bbb04":"code","da4e06c2":"code","048c44ab":"code","2dd40c60":"code","b02c2079":"code","34ee17d5":"code","420160f6":"code","a71d09b2":"code","277c0dc4":"code","b9af8898":"code","4cccf596":"code","16d243a3":"markdown","1d4b97fa":"markdown","c988d375":"markdown","bba5c9df":"markdown","abe1c441":"markdown","8a75961e":"markdown","2a097869":"markdown","aa3b0a79":"markdown","8089136d":"markdown","3601c7dc":"markdown","644f771a":"markdown","7ab8c90f":"markdown","91e5bd6f":"markdown","9eb79c4b":"markdown","8d59e30f":"markdown","4873b10e":"markdown","c497ac11":"markdown","f863a4fd":"markdown","d759a373":"markdown","aa70bad7":"markdown","f6f0f227":"markdown","e3c33934":"markdown","d3f465ad":"markdown","3a7f60a0":"markdown","a4262eab":"markdown","ea203a91":"markdown","956a56f5":"markdown","4ca97932":"markdown","80994a7a":"markdown","2ee5ebe6":"markdown","5b817e30":"markdown","d3af0f1e":"markdown","b72884ad":"markdown","66a7910d":"markdown","5d11f8f0":"markdown","aea5fdb4":"markdown","eaf01deb":"markdown","8123c38f":"markdown","f9391068":"markdown","9cbeeffa":"markdown"},"source":{"668d0f12":"\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os,cv2\nfrom IPython.display import Image\nfrom keras.preprocessing import image\nfrom keras import optimizers\nfrom keras import layers,models\nfrom keras.applications.imagenet_utils import preprocess_input\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras import regularizers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16\nprint(os.listdir(\"..\/input\"))\n\nimport numpy as np\n\n","7212d01e":"train_dir=\"..\/input\/train\/train\"\ntest_dir=\"..\/input\/test\/test\"\ntrain=pd.read_csv('..\/input\/train.csv')\n\ndf_test=pd.read_csv('..\/input\/sample_submission.csv')","c74c22f3":"train.head(5)\ntrain.has_cactus=train.has_cactus.astype(str)","3df98469":"print('out dataset has {} rows and {} columns'.format(train.shape[0],train.shape[1]))","a83c9d6f":"train['has_cactus'].value_counts()","b2ffacb7":"print(\"The number of rows in test set is %d\"%(len(os.listdir('..\/input\/test\/test'))))","2d2a27b9":"Image(os.path.join(\"..\/input\/train\/train\",train.iloc[0,0]),width=250,height=250)","8e0e80ed":"datagen=ImageDataGenerator(rescale=1.\/255)\nbatch_size=150","b2e122ac":"train_generator=datagen.flow_from_dataframe(dataframe=train[:15001],directory=train_dir,x_col='id',\n                                            y_col='has_cactus',class_mode='binary',batch_size=batch_size,\n                                            target_size=(150,150))\n\n\nvalidation_generator=datagen.flow_from_dataframe(dataframe=train[15000:],directory=train_dir,x_col='id',\n                                                y_col='has_cactus',class_mode='binary',batch_size=50,\n                                                target_size=(150,150))","2e97d313":"model=models.Sequential()\nmodel.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Conv2D(64,(3,3),activation='relu',input_shape=(150,150,3)))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Conv2D(128,(3,3),activation='relu',input_shape=(150,150,3)))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Conv2D(128,(3,3),activation='relu',input_shape=(150,150,3)))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512,activation='relu'))\nmodel.add(layers.Dense(1,activation='sigmoid'))\n         ","5889ee4e":"model.summary()","a88fa2f2":"model.compile(loss='binary_crossentropy',optimizer=optimizers.rmsprop(),metrics=['acc'])\n","51fbaf6e":"epochs=10\nhistory=model.fit_generator(train_generator,steps_per_epoch=100,epochs=10,validation_data=validation_generator,validation_steps=50)","da5bbb04":"acc=history.history['acc']  ##getting  accuracy of each epochs\nepochs_=range(0,epochs)    \nplt.plot(epochs_,acc,label='training accuracy')\nplt.xlabel('no of epochs')\nplt.ylabel('accuracy')\n\nacc_val=history.history['val_acc']  ##getting validation accuracy of each epochs\nplt.scatter(epochs_,acc_val,label=\"validation accuracy\")\nplt.title(\"no of epochs vs accuracy\")\nplt.legend()\n\n\n\n\n    ","da4e06c2":"acc=history.history['loss']    ##getting  loss of each epochs\nepochs_=range(0,epochs)\nplt.plot(epochs_,acc,label='training loss')\nplt.xlabel('No of epochs')\nplt.ylabel('loss')\n\nacc_val=history.history['val_loss']  ## getting validation loss of each epochs\nplt.scatter(epochs_,acc_val,label=\"validation loss\")\nplt.title('no of epochs vs loss')\nplt.legend()","048c44ab":"model_vg=VGG16(weights='imagenet',include_top=False)\nmodel_vg.summary()","2dd40c60":"def extract_features(directory,samples,df):\n    \n    \n    features=np.zeros(shape=(samples,4,4,512))\n    labels=np.zeros(shape=(samples))\n    generator=datagen.flow_from_dataframe(dataframe=df,directory=directory,x_col='id',\n                                            y_col='has_cactus',class_mode='other',batch_size=batch_size,\n                                            target_size=(150,150))\n    i=0\n    for input_batch,label_batch in generator:\n        feature_batch=model_vg.predict(input_batch)\n        features[i*batch_size:(i+1)*batch_size]=feature_batch\n        labels[i*batch_size:(i+1)*batch_size]=label_batch\n        i+=1\n        if(i*batch_size>samples):\n            break\n    return(features,labels)\n\ntrain.has_cactus=train.has_cactus.astype(int)\nfeatures,labels=extract_features(train_dir,17500,train)\ntrain_features=features[:15001]\ntrain_labels=labels[:15001]\n\nvalidation_features=features[15000:]\nvalidation_labels=labels[15000:]\n\n","b02c2079":"#df_test.has_cactus=df_test.has_cactus.astype(str)\ntest_features,test_labels=extract_features(test_dir,4000,df_test)","34ee17d5":"train_features=train_features.reshape((15001,4*4*512))\nvalidation_features=validation_features.reshape((2500,4*4*512))\n\ntest_features=test_features.reshape((4000,4*4*512))\n","420160f6":"model=models.Sequential()\nmodel.add(layers.Dense(212,activation='relu',kernel_regularizer=regularizers.l1_l2(.001),input_dim=(4*4*512)))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(1,activation='sigmoid'))\n","a71d09b2":"model.compile(optimizer=optimizers.rmsprop(),loss='binary_crossentropy',metrics=['acc'])","277c0dc4":"history=model.fit(train_features,train_labels,epochs=30,batch_size=15,validation_data=(validation_features,validation_labels))","b9af8898":"y_pre=model.predict_proba(test_features)","4cccf596":"df=pd.DataFrame({'id':df_test['id'] })\ndf['has_cactus']=y_pre\ndf.to_csv(\"submission.csv\",index=False)\n","16d243a3":"### Displaying summary of our network","1d4b97fa":"**In this kernel,we will be learning about Convolution neural networks.We will learn**\n\n   ","c988d375":"## A brief intro to CNN[](#2) <a id='2'> <\/a><br>","bba5c9df":"### Fitting our model ","abe1c441":"# **Aerial Cactus Identification Introduction**","8a75961e":"**if you like my kernel,please consider upvoting**","2a097869":"### Displaying an image","aa3b0a79":"The convolution operation is the building block of a convolutional neural network as the name suggests it.\nNow, in the field of computer vision, an image can be expressed as a matrix of RGB values.\nTherefore, let\u2019s consider the 6x6 matrix below as a part of an image:\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*aGSthcPASa2OT1UBm7paOA.png)\nAnd the filter will be the following matrix:\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*591OPcvDKUN9liZ_VQ1M5g.png)\nThen, the convolution involves superimposing the filter onto the image matrix, adding the product of the values from the filter and and the values from the image matrix, which will generate a 4x4 convoluted layer.\n\nThis is very hard to put in words, but here is a nice animation that explains the convolution:\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*VJCoCYjnjBBtWDLrugCBYQ.gif)","8089136d":"and it's no rocket science,its fun.","3601c7dc":"### Setting our directories","644f771a":"Now we will plot training loss and validation loss vs number of epochs","7ab8c90f":"## Compiling our model","91e5bd6f":"### Loading required libraries","9eb79c4b":"- Read the picture files\n- Decode JPEG content to RGB pixels\n- Convert this into floating tensors\n- Rescale pixel values (between 0 to 255) to [0,1] interval.","8d59e30f":"In this step we will specify 3 important things related to our model\n","4873b10e":"## Getting a basic idea","c497ac11":"Now,after preprocessing is done with our data we will split our dataset to\ntraining and validation for training our model and validating the result repectively.\nWe will take first 15000 images to train our data and last 2500 images to validate our model later.\n","f863a4fd":"![](https:\/\/blog.algorithmia.com\/wp-content\/uploads\/2018\/05\/word-image.png)","d759a373":"**flow_from_dataframe Method**\n\nThis method is useful when the images are clustered in only one folder. To put in other words images from different class\/labels reside in only one folder. Generally, with such kind of data, some text files containing information on class and other parameters are provided. In this case, we will create a dataframe using pandas and text files provided, and create a meaningful dataframe with columns having file name (only the file names, not the path) and other classes to be used by the model. For this method, arguments to be used are:\n\n    dataframe value : Dataframe having meaningful data (file name, class columns are a must)\n    directory value : The path to the parent directory containing all images.\n    x_col value : which will be the name of column(in dataframe) having file names\n    y_col value : which will be the name of column(in dataframe) having class\/label","aa70bad7":"[](http:\/\/)Now we will build our network.We will build our model such that it contains\n5 **Conv2D + Maxpooling2D stages** with **relu** activation function.","f6f0f227":"As you know,data should be processed into appropriatly pre-processed floating point \ntensors before being fed to our network.So the steps for getting it into our network are roughly ","e3c33934":"- Instantiating the VGG16 convolution base","d3f465ad":"### Extracting features using VGG16","3a7f60a0":"we will make use of ImageDataGenerator method available in keras to do all the preprocessing.","a4262eab":"### Maxpooling","ea203a91":"### Splitting our train and validation dataset","956a56f5":"### Reshaping our features to feed into our dense layers","4ca97932":"Maxpooling consist of extracting features from input feature map and outputig maximum value of each channel.\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*Ziqq69FhwOAbBi9-FNruAA.png)","80994a7a":"we will plot our results using matplotlib.first we will plot training and validation accuracy.","2ee5ebe6":"## Data preparation [](#1)  <a id=\"1\"><\/a> <br>","5b817e30":"## making prediction","d3af0f1e":"## Evaluating our model[](#4)<a id=\"4\"><\/a><br>","b72884ad":"- loss: we will set our loss as binary_crossentropy since we are\n   attacking a binary classification problem\n- optimizer : optimizers shape and mold your model into its most accurate possible form by futzing with the weights.\n- metrics : This is the evaluation criteria that we choose to evaluate our model","66a7910d":"Here we will use keras fit_generator() method instead of fit() method because we have used ImageDataGenerator to generate values.\n","5d11f8f0":"### Define a densely connected network","aea5fdb4":"## Improving our model using VGG16[](#5)<a id=\"5\"><\/a><br> ","eaf01deb":"Convolutions are defined on two key parameters\n- The size of patches that are extracted from input feature map..ie here 3x3\n- The number of filters computed from convolutions","8123c38f":"### If you guys like my kernel,please do consider upvoting it.","f9391068":"## Building our model [](#3) <a id=\"3\"><\/a><br> ","9cbeeffa":"## Making a submission[](#6)<a id=\"6\"><\/a><br>"}}