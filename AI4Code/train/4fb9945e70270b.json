{"cell_type":{"ec29cff4":"code","dd0a1948":"code","9158d351":"code","f828110c":"code","ad5f2b0f":"code","524cb7b3":"code","1b611a36":"code","1763cc4d":"markdown","b3c9149f":"markdown","b8f26c21":"markdown","05079286":"markdown"},"source":{"ec29cff4":"import numpy as np\nimport pandas as pd\nimport torch\nimport time \n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\ndef generate_data_(X, Y, merge, multiplier=1., debug=True):  \n    \"\"\"Generates X.shape[0]*multiplier additional data points from (X, Y).\n    \n    Overall strategy. \n    Create new rows by merging gene expressions using provided function 'merge'. \n    The new data is created to preserve label distribution. \n        [0     1      2      3      4     5      6]\n        [ 9367 12532  1538   303    55    13     6]\n    This is done in in steps, for label 0, label 1, and so on. \n    \n    Note:  \n        This function assumes a specific dose and time and is called 6 times by 'generate_data'. \n        This function does not generate more controls. \n\n    Args:\n        X (torch.tensor): The training features (n, d). \n        Y (torch.tensor): The training targets (n, k).\n        merge (callable): Function that handles merging of gene expression. \n        debug (bool):     Print debug messages. \n\n    Returns:\n        X_ (torch.tensor): Generated training features (n*multiplier, d). \n        Y_ (torch.tensor): Generated training targets  (n*multiplier, k). \n    \n    \"\"\"\n    n, d = X.shape \n    n, k = Y.shape\n    \n    # Store control (c) and treatment (t) variants of (X,Y).  \n    cY = Y[X['cp_type'] != 'trt_cp']\n    tY = Y[X['cp_type'] == 'trt_cp']\n    cX = X[X['cp_type'] != 'trt_cp']\n    tX = X[X['cp_type'] == 'trt_cp']\n\n    # Remove treatment \/ dose information. \n    X  = torch.tensor(X.values [:, 4:].astype(np.float16)) \n    Y  = torch.tensor(Y.values [:, 1:].astype(np.float16))\n    cX = torch.tensor(cX.values[:, 4:].astype(np.float16)) \n    cY = torch.tensor(cY.values[:, 1:].astype(np.float16))\n    tX = torch.tensor(tX.values[:, 4:].astype(np.float16)) \n    tY = torch.tensor(tY.values[:, 1:].astype(np.float16))\n    X_ = []\n    Y_ = []\n    \n    # Compute label distribution in 'counts'.  \n    unique, counts = np.unique(tY.sum(-1), return_counts=True)\n\n    # -------------------- Label Strategy 0 -------------------- \n    # counts[0] is the number of occurrences of compounds with 0 MoA (controls are removed). \n    # Merge example with treatment that has zero MoA with control.   \n    # Keep cell viability of treated cell, only merge gene expression. \n    # Keep track of merged examples, never use the same twice. \n    zeros = counts[0] # this is 1001\n    tX0   = tX[tY.sum(-1) == 0]\n    tY0   = tY[tY.sum(-1) == 0]\n    dct0  = {}\n\n    while len(X_) < zeros * multiplier: \n        k = np.random.randint(tX0.shape[0])\n        l = np.random.randint(cX.shape[0])\n        \n        if (k, l) in dct0.keys(): continue\n        dct0[k, l] = 1\n\n        X_.append(torch.cat((merge(tX0[k, :-100], cX[l, :-100]), tX0[k, -100:]), 0).numpy()) \n        Y_.append(tY0[k].numpy())\n            \n    if debug: print(counts, np.unique(torch.tensor(Y_).sum(-1), return_counts=True)[1])\n\n    # -------------------- Label Strategy 1 --------------------  \n    # counts[1] is the number of occurrences of data points 1 MoA that are not controls. \n    # Similar to above. Merge example with treatment and one MoA with control compound. \n    ones = counts[1] \n    tX1  = tX[tY.sum(-1) == 1]\n    tY1  = tY[tY.sum(-1) == 1]\n    dct1 = {} \n    \n    while len(X_) < (zeros + ones)*multiplier: \n        k = np.random.randint(tX0.shape[0])\n        l = np.random.randint(cX.shape[0])\n        \n        if (k, l) in dct1.keys(): continue\n        dct1[k, l] = 1\n\n        X_.append(torch.cat((merge(tX1[k, :-100], cX[l, :-100]), tX1[k, -100:]), 0).numpy()) \n        Y_.append(tY1[k].numpy())\n\n    \n    if debug: print(counts, np.unique(torch.tensor(Y_).sum(-1), return_counts=True)[1])\n\n\n    # -------------------- Label Strategy 2 -------------------- \n    # Different ot above. \n    # (a)    p % are generated by adding control to compound with 2 MoAs. \n    # (b) (1-p)% are generated by merging two compounds with 1 MoA. \n    # In the last case case we also merge cell viability which may be problematic.\n    twos = counts[2] \n    tX2  = tX[tY.sum(-1) == 2]\n    tY2  = tY[tY.sum(-1) == 2]\n    dct2 = {}\n    p    = 0.5   \n\n    # merge 2 MoA drug with control (50%)\n    while len(X_) < (zeros + ones + twos*p)*multiplier: \n        k = np.random.randint(tX2.shape[0])\n        l = np.random.randint(cX.shape[0])\n\n        if (k, l) in dct2.keys(): continue\n        dct2[k, l] = 1\n        \n        X_.append(torch.cat((merge(tX2[k, :-100], cX[l, :-100]), tX2[k, -100:]), 0).numpy()) \n        Y_.append(tY2[k].numpy())\n    \n    dct2_ = {}\n    \n    # merge two 1 MoA      \n    while len(X_) < (zeros + ones + twos)*multiplier: \n        k = np.random.randint(tX1.shape[0])\n        l = np.random.randint(tX1.shape[0])\n\n        if (k, l) in dct2_.keys(): continue\n        dct2_[k, l] = 1\n        \n        if torch.sum(tY1[k] + tY1[l] > 1) == 0:  # if sum larger 1 they are co-activated.  \n            X_.append(merge(tX1[k], tX1[l]).numpy())\n            Y_.append( (tY1[k] + tY1[l]).numpy())\n            \n            \n    if debug: print(counts, np.unique(torch.tensor(Y_).sum(-1), return_counts=True)[1])\n    \n    # -------------------- Label Strategy 3 -------------------- \n    # 50% are generated by adding control to compound with 3 MoAs.  \n    # 50% are generated by merging compound with 1 and 2 MoAs respectively. \n    # In the last case case we also merge cell viability which may be problematic.\n    threes = counts[3] \n    tX3    = tX[tY.sum(-1) == 3]\n    tY3    = tY[tY.sum(-1) == 3]\n    dct3   = {}\n    p      = 0.5\n    \n    # Merge 3 MoA drug with control (50%)\n    while len(X_) < (zeros + ones + twos + threes*p)*multiplier: \n        k = np.random.randint(tX3.shape[0])\n        l = np.random.randint(cX.shape[0])\n\n        if (k, l) in dct3.keys(): continue\n        dct3[k, l] = 1\n        \n        X_.append(torch.cat((merge(tX3[k, :-100], cX[l, :-100]), tX3[k, -100:]), 0).numpy()) \n        Y_.append(tY3[k].numpy())\n    \n    dct3_ = {}\n    \n    # Kerge 1 MoA with 2 MoA (50%).      \n    while len(X_) < (zeros + ones + twos + threes)*multiplier: \n        k = np.random.randint(tX1.shape[0])\n        l = np.random.randint(tX2.shape[0])\n\n        if (k, l) in dct3_.keys(): continue\n        dct3_[k, l] = 1\n        \n        if torch.sum(tY1[k] + tY2[l] > 1) == 0:  # if sum larger 1 they are co-activated.  \n            X_.append(merge(tX1[k], tX2[l]).numpy())\n            Y_.append( (tY1[k] + tY2[l]).numpy())\n    \n    if debug: print(counts, np.unique(torch.tensor(Y_).sum(-1), return_counts=True)[1])\n    \n    # -------------------- Label Strategy 4,5,6 -------------------- \n    # Don't create more data for compounds with 4 or more MoAs. \n    # For dose1 time24 labels with 4,5,6,7 activations occur 11 times out of 3573 posible.\n    # I.e. this is less than 11\/3673*100=0.3% of the data so probably doesn't matter much. \n\n    return torch.tensor(X_), torch.tensor(Y_) \n\n\n\ndef generate_data(X, Y, merge, multiplier=1., debug=True):  \n    \"\"\"Generates data for all dose and time combinations. \"\"\"\n    \n    X_ = []\n    Y_ = []\n    for d in ['D1', 'D2']:\n        for t in [24, 48, 72]:\n            print(\"--- %s t%i ---\"%(d, t))\n            indxs = np.logical_and(train_features['cp_dose'] == d, train_features['cp_time'] == t)\n            X_add, Y_add = generate_data_(train_features[indxs], train_targets[indxs], merge=merge, multiplier=multiplier, debug=debug)\n            X_.append(X_add)\n            Y_.append(Y_add)\n            \n    return torch.cat(X_, 0), torch.cat(Y_, 0)\n\ndef merge(a, b): \n    \"\"\"Merges the gene expression data of a and b. \n    Strategy developed with medical doctor team-member.\n    \n     Args:\n        a (torch.tensor): Gene expression of example a.  \n        b (torch.tensor): Gene expression of example b.  \n\n    Returns:\n        comb (torch.tensor): Combined gene expression data.  \n    \"\"\" \n\n    assert a.shape == b.shape, (a.shape, b.shape)\n    \n    comb = torch.zeros_like(a)\n\n    # If prod is positive they share sign. \n    # If prod is negative they differ in sign. \n    pos_indxs = a*b > 0  \n    neg_indxs = a*b <= 0 \n\n    # For different sign we do mean. \n    comb[neg_indxs] = (a[neg_indxs] + b[neg_indxs]) \/ 2 \n\n    # For shared sign we do absolute max (due to shared sign this is just max)\n    share_sign_part = torch.zeros((len(pos_indxs)))\n    share_sign_part = torch.max(a[pos_indxs].abs(), b[pos_indxs].abs())\n    indxs           = np.logical_and(comb[pos_indxs] > a[pos_indxs], comb[pos_indxs] > b[pos_indxs]).bool()\n    share_sign_part[indxs] *= -1\n    comb[pos_indxs] = share_sign_part\n\n    return comb ","dd0a1948":"train_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets  = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')","9158d351":"# Generate more data, takes around 30 seconds. \nX_add, Y_add = generate_data(train_features, train_targets, merge=merge, multiplier=2)\nprint(\"Generated Data: \", X_add.shape, Y_add.shape)","f828110c":"# concatenate and PCA takes around 30s. \nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ntY = train_targets[ train_features['cp_type'] == 'trt_cp']\ntX = train_features[train_features['cp_type'] == 'trt_cp']\ntX = torch.tensor(tX.values[:, 4:].astype(np.float16)) \ntY = torch.tensor(tY.values[:, 1:].astype(np.float16))\n\n# include labels in PCA, we want to find differences in train (X, Y) and augmented (X, Y)\nX = np.concatenate((np.concatenate((tX, tY), 1), np.concatenate((X_add, Y_add), 1)), 0)\nr = PCA().fit_transform(X)\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 4))\nn = train_features.shape[0]\n\nax.plot(r[:n,0], r[:n,1], 'o', alpha=0.3, label=\"Real Data\")\nax.plot(r[n:,0], r[n:,1], 'x', alpha=0.3, label=\"New Data\")\nax.set_ylabel(\"Principal Component 1\")\nax.set_xlabel(\"Principal Component 0\")\nax.legend()\n\nprint(\"\\t\\tX_new.shape=%s\\tY_new.shape=%s\"%(str(X_add.numpy().shape), str(Y_add.numpy().shape)))\nplt.show()\n","ad5f2b0f":"# Train logistic regression classifier to tell real\/generated data apart.  \n# Use classifier to discared points that doesn't look real. \nfrom sklearn.linear_model import LogisticRegression\n\n# Construct dataset with real\/generated data. \nX_disc = np.concatenate((tX, X_add), 0)\nn = tX.shape[0]\nY_disc = np.ones(X_disc.shape[0], )\nY_disc[n:] = 0\n\nclf = LogisticRegression(verbose=1, max_iter=500).fit(X_disc, Y_disc)\nprint(\"Logitic Regression Accuracy: \", clf.score(X_disc, Y_disc))\n\npreds_fake = clf.predict_proba(X_disc[n:])\npreds_real = clf.predict_proba(X_disc[:n])\n\n# Keep points classifier predict more than p% real. \nn = tX.shape[0]\np = 0.5\nreduced_X = X_disc[n:][preds_fake[:, 1] > p] \nreduced_Y = Y_add[preds_fake[:, 1] > p]\n\n# Redo PCA\nr = PCA().fit_transform(np.concatenate((np.concatenate((tX, tY), 1), np.concatenate((reduced_X, reduced_Y), 1)), 0))\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 4))\nn = train_features.shape[0]\n\nax.plot(r[:n,0], r[:n,1], 'o', alpha=0.3, label=\"Real Data\")\nax.plot(r[n:,0], r[n:,1], 'x', alpha=0.3, label=\"New Data\")\nax.set_ylabel(\"Principal Component 1\")\nax.set_xlabel(\"Principal Component 0\")\nax.legend()\n\nprint(\"X.shape=%s\\tY.shape=%s\"%(str(reduced_X.shape), str(reduced_Y.shape)))\n\nplt.show()","524cb7b3":"# **DISCLAIMER** \n# The above trick discards points in a way that does not preserve the label distribution. \n# This could be fixed by\n# - use different probability limits for different labels (above uses 50% for all labels)\n# - alternatively using rejection sampling when generating the data based on the logistic regression classifier. \nreal_label_dist = np.unique(np.sum(train_targets.values[:, 1:], axis=-1), return_counts=True)[1]\ngen_label_dist  = np.unique(reduced_Y[:, -206:].sum(-1), return_counts=True)[1]\n\nprint(real_label_dist[:4] \/ np.sum(real_label_dist[:4]))\nprint(gen_label_dist \/ np.sum(gen_label_dist))","1b611a36":"# recompute above PCA plot but for different cut-off probabilities. \nfor p in [0.1, 0.2, 0.4, 0.6]: # Keep points classifier predict more than 50% real.\n    n = tX.shape[0]\n    reduced_X = X_disc[n:][preds_fake[:, 1] > p] \n    reduced_Y = Y_add[preds_fake[:, 1] > p]\n    \n    # Redo PCA\n    r = PCA().fit_transform(np.concatenate((np.concatenate((tX, tY), 1), np.concatenate((reduced_X, reduced_Y), 1)), 0))\n\n    fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n    n = train_features.shape[0]\n\n    ax.plot(r[:n,0], r[:n,1], 'o', alpha=0.3, label=\"Real Data\")\n    ax.plot(r[n:,0], r[n:,1], 'x', alpha=0.3, label=\"New Data\")\n    ax.set_ylabel(\"Principal Component 1\")\n    ax.set_xlabel(\"Principal Component 0\")\n    ax.legend()\n\n    print(\"-- probability cut off %-2f ---\"%p)\n    print(\"X.shape=%s\\tY.shape=%s\"%(str(reduced_X.shape), str(reduced_Y.shape)))\n    plt.show()","1763cc4d":"# Ideas for improvement. \n\n- Train feature extract like deep autoencoder, use encodings to find distinct gene expression to be used in data generation.   \n","b3c9149f":"# Compare real vs simulated data using PCA. ","b8f26c21":"# (Potentially) Improve With Adversarial Validation. ","05079286":"<div>\n<!--img src=\"attachment:image.png\" width=\"500\"\/-->\n<img src=\"https:\/\/i.imgur.com\/ee0Y0gb.png\" width=\"500\"\/>    \n<\/div>\n\n# More Data Is All You Need!\nThrowing more data at deep learning consistently improves performance. This notebook shows how we generated $ 43 748$ additional training examples. High-level idea: the provided $ 23814$ training examples can be used to predict the outcome of $ 43 748$ experiments without actually performing those experiments. \n\n## But How?\nAll cells were treated with 1 compound. Imagine treating the cells with 2 compounds. If the 2 compounds don't interact, we believe the outcome of the experiment can be computed using the provided \"1 compound data\". There are at least $\\frac{3000\\cdot 2999}{2}\\approx 4.5\\cdot 10^6$ compound-pairs, so if just $1\\%$ of the compound-pairs are don't interact we can predict the outcome of $45000$ experiments.  \n\nThis raises the following question: can we find compound-pairs that don't interact? \n\nThe code below attempts to find compounds that don't interact using a very simple strategy: *use compound-pairs with strictly different mechanisms of action*. In the end of the notebook we demonstrate a slightly more sophisticated idea. "}}