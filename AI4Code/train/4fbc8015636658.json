{"cell_type":{"ee960acf":"code","279bd84c":"code","6e43fd56":"code","e8d75a47":"code","cfbe1254":"code","a10ebe44":"code","a9d19a48":"code","fc5415f9":"code","223d84f6":"code","5f85e2ce":"code","55bd1320":"code","a925eba8":"markdown","11e6a8f7":"markdown","25af0bb9":"markdown","76f732be":"markdown","ab16576c":"markdown","c0b66b0e":"markdown","da9752ea":"markdown"},"source":{"ee960acf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","279bd84c":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, f_regression, f_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","6e43fd56":"train_data = pd.read_csv(\"\/kaggle\/input\/minor-project-2021\/train.csv\").fillna(value = 0)\ntest_data = pd.read_csv(\"\/kaggle\/input\/minor-project-2021\/test.csv\").fillna(value = 0)\n# for feature in test_data:\n#     if feature not in ['Result', 'id']:\n#         test_data[feature] = test_data.fillna(test_data[feature].mean())\n","e8d75a47":"# I had tried to reduce the dataset so it was less skewed, but the out-of-sample resuts were worse\n\n# train_data_1 = train_data[train_data['Result'] == 1].drop(['Result', 'id'], axis = 1).copy()\n# for feature in train_data_1:\n#     test_data[feature] = test_data[feature] \/ test_data[feature].max()\n\n# print(train_data_1.shape)\n# import matplotlib.pyplot as plt\n# min_dissimilarity_level = 0.3\n# i = 0\n# while i < len(train_data_1):\n#     arr = (abs(train_data_1 - train_data_1.iloc[i])).sum(axis = 1).drop(train_data_1.index[i])\n#     arr = arr\/max(arr)\n#     train_data_1 = train_data_1.drop(arr[arr <= min_dissimilarity_level].index)\n#     train_data = train_data.drop(arr[arr <= min_dissimilarity_level].index)\n#     print(i, train_data.shape)\n#     if train_data.shape[0] < 10000:\n#         break\n#     i += 1\n","cfbe1254":"X = train_data.drop(['Result', 'id'], axis = 1).copy()\ny = train_data['Result'].astype('int').copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\nprint(X_train.shape)\n# for feature in X_train:\n#     X_train[feature] = X_train.fillna(value = X_train[feature].mean())\n# for feature in X_test:\n#     X_test[feature] = X_test.fillna(value = X_test[feature].mean())\n","a10ebe44":"# scaler = MinMaxScaler().fit(X_train)\n# X_train = scaler.transform(X_train)\n# X_test = scaler.transform(X_test)\nnum_features = 70 # in range(50,101,10):\nsel = SelectKBest(f_classif, k = num_features)\nsel = sel.fit(X_train, y_train)\nX_train_new = sel.transform(X_train)\nX_test_new = sel.transform(X_test)\nmodel1 = XGBClassifier(random_state = 101)\nmodel1 = model1.fit(X_train_new, y_train)\ny_pred = model1.predict(X_test_new)\nprint(\"Results of \"+ str(model1))\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nprint('Accuracy is ' + str(100*accuracy_score(y_test, y_pred)))\nprint('Precision is ' + str(100*precision_score(y_test, y_pred)))\nprint('Recall is ' + str(100*recall_score(y_test, y_pred)))\nprint('F1 Score is ' + str(100*f1_score(y_test, y_pred)))\n# model2 = LogisticRegression()\n# tmp = pd.DataFrame(X_train_new)\n# tmp[70] = pd.DataFrame(model1.predict(X_train_new))\n# X_train_new2 = tmp.values\n# y_train2 = model1.predict(X_train_new) & y_train\n# model2 = XGBClassifier()\n# model2 = model2.fit(X_train_new2, y_train2)\n# tmp = pd.DataFrame(X_test_new)\n# tmp[70] = pd.DataFrame(y_pred)\n# X_test_new2 = tmp.values\n# y_pred = model2.predict(X_test_new2)\n# print(\"Results of \"+ str(model2))\n# print(confusion_matrix(y_test,y_pred))\n# print(classification_report(y_test,y_pred))\n# print('Accuracy is ' + str(100*accuracy_score(y_test, y_pred)))\n# print('Precision is ' + str(100*precision_score(y_test, y_pred)))\n# print('Recall is ' + str(100*recall_score(y_test, y_pred)))\n# print('F1 Score is ' + str(100*f1_score(y_test, y_pred)))\n","a9d19a48":"print(confusion_matrix(y_train, model1.predict(X_train_new)))\nprint(classification_report(y_train, model1.predict(X_train_new)))\n","fc5415f9":"classifier = XGBClassifier(random_state = 101)\nX_train = X\ny_train = y\nX_test = test_data.drop(['id'], axis = 1).copy()\nsel = SelectKBest(f_classif, k = 70)\nsel = sel.fit(X_train, y_train)\nX_train_new = sel.transform(X_train)\nX_test_new = sel.transform(X_test)\nclassifier = classifier.fit(X_train_new, y_train)\ny_pred = classifier.predict(X_test_new)\n","223d84f6":"results = pd.DataFrame(y_pred, columns = ['Expected'])\nresults['id'] = test_data['id']\nresults = results[['id', 'Expected']]","5f85e2ce":"results","55bd1320":"results.to_csv('original_results.csv', index = False)","a925eba8":"## Creating and using XGBoost model to predict on test data","11e6a8f7":"## Check to see how much the model was able to learn","25af0bb9":"## Importing test and training data","76f732be":"## Importing libraries","ab16576c":"## This segment selects 70 best features and fits an XGBoost model to X_train","c0b66b0e":"## Spliting the training data into X_train and X_test","da9752ea":"## Reorienting predicted data and exporting to CSV"}}