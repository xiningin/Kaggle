{"cell_type":{"45d379f7":"code","02347a9d":"code","ac14ab26":"code","18e8675b":"code","93fef673":"code","aad5537e":"code","c85168c0":"code","5894c730":"code","7f6aba33":"code","a00ff4cd":"code","d802e209":"code","68edf028":"code","6ff63ede":"code","16ab40ae":"code","2ea40348":"code","55019b6e":"code","b9a0e747":"code","d7017430":"code","69e818ed":"code","1d1dbd00":"code","e5895dbb":"code","b13f2b72":"code","4868cb29":"markdown","3d048bf9":"markdown","b546d7d2":"markdown","dbc2932c":"markdown","2315379d":"markdown","b9041e6b":"markdown","5428acb3":"markdown","952ffb48":"markdown","38e5980c":"markdown","1b08e17c":"markdown","e623265a":"markdown"},"source":{"45d379f7":"import pandas as pd\nimport itertools \nfrom bs4 import BeautifulSoup\nimport requests\nfrom requests import get\nimport time\nfrom random import seed\nfrom random import random\nfrom random import randint\n\n# specify the url format\nurl = 'https:\/\/www.pararius.com\/apartments\/amsterdam\/page-'\n# initialize a list called houses \nhouses = []\n# initialize variable count at 1\ncount = 1\n\n# first while loop that will run 100 times (adjust this to how many pages you want to scrape)\nwhile count <= 100:\n    # initialize variable new_count at 0\n    new_count = 0\n    # if loop that specifies the first page separately (many websites have a first page url format different than other pages)\n    if count == 1:\n        first_page = 'https:\/\/www.pararius.com\/apartments\/amsterdam\/page-1'\n        # request the response\n        response = get(first_page)\n        # parse through the html \n        html_soup = BeautifulSoup(response.text, 'html.parser')\n        # in the html of the page, find all the bins with <li> and class:\n        house_data = html_soup.find_all('li', class_=\"search-list__item search-list__item--listing\")\n        # I like to print where the program is on the screen so we can follow its progress and where any errors happened\n        print(first_page)\n        \n        # if the response was not empty (if something was actually scraped)\n        if house_data != []:\n            # add to the list houses\n            houses.extend(house_data)\n            # random wait times\n            value = random()\n            scaled_value = 1 + (value * (9 - 5))\n            print(scaled_value)\n            time.sleep(scaled_value)\n    # pages other than the first\n    elif count != 1:\n    # collect four and wait random times \n        url = 'https:\/\/www.pararius.com\/apartments\/amsterdam\/page-' + str(count)\n        print(url)\n        response = get(url)\n        html_soup = BeautifulSoup(response.text, 'html.parser')\n        print(response)\n        house_data = html_soup.find_all('li', class_=\"search-list__item search-list__item--listing\")\n\n        if house_data != []:\n            houses.extend(house_data)\n            value = random()\n            scaled_value = 1 + (value * (9 - 5))\n            print(scaled_value)\n            time.sleep(scaled_value)\n\n        # if you get empty response, stop the loop\n        else:\n            print('empty')\n            break\n            \n\n    count += 1","02347a9d":"count = 0\nhouse_price = []\nrental_agency = []\nlocation = []\ncity = []\nbedrooms = []\nsurface = []\n\nn = int(len(houses)) - 1\n\nwhile count <= n:\n    \n    num = houses[int(count)]\n    \n    price = num.find_all('span',{\"class\":\"listing-search-item__price\"})[0].text\n    house_price.append(price)\n    df1 = pd.DataFrame({'house_price':house_price})\n    df1['house_price'] = df1['house_price'].str.replace(\"\\D\",\"\")\n    df1['house_price'] = df1['house_price'].str.replace(\"per month\",\"\")\n    \n    try:\n        agency = num.find_all('a', href=True)[2].text\n    except IndexError:\n        agency = 'none'\n    rental_agency.append(agency)\n    df2 = pd.DataFrame({'rental_agency':rental_agency})\n    \n\n    postcode = num.find('div',{\"class\":\"listing-search-item__location\"}).text\n    location.append(postcode)\n    df3 = pd.DataFrame({'postcode':location})\n    df3['postcode'] = df3['postcode'].str.replace(\"\\nApartment\\n \",\"\")\n    df3['postcode'] = df3['postcode'].str.replace(\"\\n\",\"\")\n    df3['postcode'] = df3['postcode'].str.replace(\"\\s\",\"\")\n    df3['postcode'] = df3['postcode'].str.replace(\"               \",\"\")\n    df3['postcode'] = df3['postcode'].str.replace(\"new\",\"\")\n    df3['postcode'] = df3['postcode'].str[0:6]\n    \n    bedrooms_num = num.find_all('dd',{\"class\":\"illustrated-features__description\"})[1].text\n    bedrooms.append(bedrooms_num)\n    df4 = pd.DataFrame({'bedrooms':bedrooms})\n    df4['bedrooms'] = df4['bedrooms'].str.replace(\"\\D\",\"\")\n    \n    size = num.find_all('dd',{\"class\":\"illustrated-features__description\"})[0].text\n    surface.append(size)\n    df5 = pd.DataFrame({'surface':surface})\n    df5['surface'] = df5['surface'].str.replace(\"\\D\",\"\")\n    \n    print(count)\n    count += 1\n    \nresult = pd.concat([df1, df2], axis=1, sort=False)\nresult2 = pd.concat([result, df3], axis=1, sort=False)\nresult3 = pd.concat([result2, df4], axis=1, sort=False)\ndfa = pd.concat([result3, df5], axis=1, sort=False)","ac14ab26":"df = dfa.copy()\n\nfrom geopy.extra.rate_limiter import RateLimiter\nfrom geopy.exc import GeocoderTimedOut\nimport geopy.geocoders\nfrom geopy.geocoders import Nominatim\nimport geopy\nimport geopandas\nimport pandas as pd\nimport time \n\n\nlist_of_points = []\n\ndf['address'] = df['postcode']\n\ndf['address2'] = df['postcode'].str.replace('\\s','')\n\nlocator = Nominatim(user_agent = 'Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit\/601.3.9 (KHTML, like Gecko) Version\/9.0.2 Safari\/601.3.9')\n\ndf2 = pd.DataFrame(columns=['house_price', 'rental_agency',\n                            'postcode','bedrooms','surface',\n                            'address','address2'])\nn = int(len(houses)) - 1\n\ncount = 1\nwhile count <= n: \n    if count == 0:\n        df_new = df[0:1]\n    else:\n        a = int(count)\n        n = int(count) +1\n        print(a)\n        print(n)\n        df_new = df[a:n]\n    geocode = RateLimiter(locator.geocode, min_delay_seconds=1)\n    try:\n        df_new['location'] = df_new['address'].apply(geocode)\n        df_new['point'] = df_new['location'].apply(lambda loc: tuple(loc.point) if loc else None)\n        df_new[['latitude', 'longitude', 'altitude']] = pd.DataFrame(df_new['point'].tolist(), index=df_new.index)\n    except ValueError:\n        try:\n            df_new['location'] = df_new['address2'].apply(geocode)\n            print('trying second address')\n            df_new['point'] = df_new['location'].apply(lambda loc: tuple(loc.point) if loc else None)\n            df_new[['latitude', 'longitude', 'altitude']] = pd.DataFrame(df_new['point'].tolist(), index=df_new.index)\n        except ValueError:\n            df_new = df_new.dropna(subset=['location'])\n            list_of_points.append(a)\n                \n    df2 = pd.concat([df2, df_new], sort=False)\n    time.sleep(1)\n    count += 1","18e8675b":"# Importing the complete datasets to skip running the time-consuming operations above","93fef673":"import pandas as pd\ndf = pd.read_excel(\"..\/input\/amsterdam-rental-listings\/amsterdam_rental_data1.xlsx\")\ndf2 = pd.read_excel(\"..\/input\/amsterdam-rental-listings\/amsterdam_rental_data2.xlsx\")\ndel df2['address2']\n\ndf2 = pd.concat([df,df2])\ndf2 = df2.drop_duplicates()\ndf2 = df2.dropna()","aad5537e":"# Getting distance of the apartments from the city center","c85168c0":"import geopy.distance\n\ndf5 = df2.copy()\n\ndf5['lat2'] = 52.370216\ndf5['lon2'] = 4.895168\n\ndf5['coord1'] = df5['latitude'].astype(str) + ',' + df5['longitude'].astype(str)\ndf5['coord2'] = df5['lat2'].astype(str) + ',' + df5['lon2'].astype(str)\n\ndef get_distance(coord1,coord2):\n    dist = geopy.distance.vincenty(coord1, coord2).km\n    return dist\ndf5['dist'] = [get_distance(**df5[['coord1','coord2']].iloc[i].to_dict()) for i in range(df5.shape[0])]","5894c730":"# deleting columns we don't need anymore\n\ndel df5['address']\ndel df5['altitude']\ndel df5['latitude']\ndel df5['longitude']\ndel df5['point']\ndel df5['lat2']\ndel df5['lon2']\ndel df5['coord1']\ndel df5['coord2']\ndel df5['location']","7f6aba33":"import requests\nimport json\n\nrating = []\nzipcode = []\nprices =[]\n\napi_key=''\nheaders = {'Authorization': 'Bearer %s' % api_key}\nurl='https:\/\/api.yelp.com\/v3\/businesses\/search'\n\noffset = 0\nwhile offset <= 1000:\n    print(offset)\n    params={'term':'Restaurants', 'location': 'amsterdam', 'limit': 50, 'offset': offset}\n    req = requests.get(url, params=params, headers=headers)\n    parsed = json.loads(req.text)\n    for n in range(0,51):\n        try:\n            price_data = parsed[\"businesses\"][n]['price']\n            ratings_data = parsed[\"businesses\"][n]['rating']\n            zipcode_data = parsed[\"businesses\"][n][\"location\"][\"zip_code\"]\n\n            rating.append(ratings_data)\n            zipcode.append(zipcode_data)\n            prices.append(price_data)\n        except:\n            pass\n    offset += 1\n\nyelp1 = pd.DataFrame({'rating':rating,'zipcode':zipcode,'prices':prices})\n","a00ff4cd":"yelp1 = pd.read_excel('..\/input\/yelp-amsterdam\/yelp_updated.xlsx')","d802e209":"## get the length of the zipcode\nyelp1['zip_len'] = yelp1.zipcode.str.len()\n## NL postcodes have more than 4 digits, so make sure we only keep those\nyelp1 = yelp1[yelp1['zip_len'] > 4]\n## I like to copy data frames when I make radical changes to data that took a while to be generated\n## to ensure I can go back to the original data if I need\nyelp = yelp1.copy()\n\n## elimate whitespaces\nyelp['postcode2'] = yelp['zipcode'].str.replace('\\s','')\n## only get 4 first digits of the postcode\nyelp['postcode2']= yelp.postcode2.str[0:4]\n\n## prices in yelp are represented by $, $$, $$$ or $$$$ so the length of the string can tell us how \n## expensive a restaurant is\nyelp['price_len'] = yelp.prices.str.len()\n\n## group by and get means by postcode area\nyelp_prices = yelp.groupby(['postcode2']).price_len.mean()\nyelp_rate = yelp.groupby(['postcode2']).rating.mean()\n\n## create two dataframes and transform them into dictionaries \nyelp_prices = pd.DataFrame(data=yelp_prices)\nyelp_rate = pd.DataFrame(data=yelp_rate)\ndict1 = yelp_prices.to_dict()['price_len']\ndict2 = yelp_rate.to_dict()['rating']","68edf028":"df5['postcode'] = df5['postcode'].str.replace('\\s','')\ndf5['postcode2'] = df5['postcode'].str[0:4]\n\n## delete the non-digit characters of the postcodes and copy dataframe to amsmodel1\namsmodel1 = df5.copy()\n\n## map the yelp price means and ratings means into our rental data frame\namsmodel1['yelp_prices'] = amsmodel1['postcode2'].map(dict1)\namsmodel1['yelp_ratings'] = amsmodel1['postcode2'].map(dict2)\n\n## make sure all the integer columns are in fact integer types\namsmodel1['house_price'] = pd.to_numeric(amsmodel1['house_price'])\namsmodel1['bedrooms'] = pd.to_numeric(amsmodel1['bedrooms'])\namsmodel1['surface'] = pd.to_numeric(amsmodel1['surface'])\n## how many columns do we have?\nlen(amsmodel1.columns)","6ff63ede":"amsmodel1 = amsmodel1.dropna()","16ab40ae":"## creating dummy variables for the categorical columns \"rental agency\" and \"postcode\"\n\ndummies = pd.get_dummies(amsmodel1.postcode2,prefix=['p'])\namsmodel1 = pd.concat([amsmodel1,dummies],axis = 1)\ndummies2 = pd.get_dummies(amsmodel1.rental_agency,prefix=['ag'])\namsmodel1 = pd.concat([amsmodel1,dummies2],axis = 1)\n\ndel amsmodel1['rental_agency']\ndel amsmodel1['postcode2']\ndel amsmodel1['postcode']\n","2ea40348":"amsmodel1['house_price'] = pd.to_numeric(amsmodel1['house_price'])\namsmodel1 = amsmodel1.dropna()\nlen(amsmodel1.columns)","55019b6e":"import numpy as np\n\ntarget= np.array(amsmodel1['house_price'])\nfeatures = amsmodel1.drop('house_price', axis = 1)\nfeature_list = list(features.columns)\nfeatures = np.array(features)\n\n## RANDOM FOREST - KFOLD AND MODEL \n\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\n    \nkf = KFold(n_splits=10,random_state=42,shuffle=True)\naccuracies = []\nfor train_index, test_index in kf.split(features):\n\n    data_train   = features[train_index]\n    target_train = target[train_index]\n\n    data_test    = features[test_index]\n    target_test  = target[test_index]\n\n    rf = RandomForestRegressor(n_estimators = 1000, random_state = 42, criterion = 'mse',  bootstrap=True)\n    \n    rf.fit(data_train, target_train)\n\n    predictions = rf.predict(data_test)\n\n    errors = abs(predictions - target_test)\n\n    print('Mean Absolute Error:', round(np.mean(errors), 2))\n    \n    mape = 100 * (errors \/ target_test)\n    accuracy = 100 - np.mean(mape)\n    print('Accuracy:', round(accuracy, 2), '%.')\n\n    accuracies.append(accuracy)\n\naverage_accuracy = np.mean(accuracies)\nprint('Average accuracy:', average_accuracy)","b9a0e747":"# SAVING THE DECISION TREE \n\nfrom sklearn.tree import export_graphviz\nimport pydot\ntree = rf.estimators_[5]\nexport_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\ngraph.write_png('tree_amsterdam.png')","d7017430":"y = rf.feature_importances_\nlist_y = [a for a in y if a > 0.01]\nprint(list_y)\n\nlist_of_index = []\nfor i in list_y:\n    a = np.where(y==i)\n    list_of_index.append(a)\n    print(a)","69e818ed":"list_of_index = [0,1,2,3,4,24,90,105,131,170,173,230,238,259,282]","1d1dbd00":"col = []\nfor i in feature_list:\n    col.append(i)\nlabels = []\nfor i in list_of_index:\n    b = col[i]\n    labels.append(b)","e5895dbb":"import matplotlib.pyplot as plt \n\ny = list_y\nfig, ax = plt.subplots() \nwidth = 0.8\nind = np.arange(len(y)) \nax.barh(ind, y,width, color=\"pink\")\nax.set_yticks(ind+width\/10)\nax.set_yticklabels(labels, minor=False)\nplt.title('Feature importance in Random Forest Regression')\nplt.xlabel('Relative importance')\nplt.ylabel('feature') \nplt.figure(figsize=(10,8.5))\nfig.set_size_inches(10, 8.5, forward=True)","b13f2b72":"# create a new dataframe that is indexed like the trained model\nnewdata = pd.DataFrame().reindex_like(amsmodel1)\nnewdata.fillna(value=0, inplace=True)\n\n# delete the variable to be predicted\ndel newdata['house_price']\nnewdata = newdata.iloc[[1]]\n\n# insert information about your apartment \nnewdata['bedrooms'] = 1\nnewdata['surface'] = 45\nnewdata['yelp_prices'] = 2.234043\nnewdata['yelp_ratings'] = 4.113475\n\n# only change the number values in the postcode \n# and string values after the _ for the rental agency\nnewdata[\"['p']_1018\"]= 1\nnewdata[\"['ag']_JLG Real Estate\"] = 1\n\nrf.predict(newdata)","4868cb29":"In this tutorial, I will go through a Data Science project for the rental market of Amsterdam, from the basics of gathering data, data cleaning, visualization, up until using a machine-learning method to develop valuation models for the city\u2019s houses. Feel free to adapt the code and apply the project in your city to understand a bit more of where you stand as a renter\/buyer!If you'd rather this on medium, and run the Notebook here, here you go: https:\/\/towardsdatascience.com\/ai-and-real-state-renting-in-amsterdam-part-1-5fce18238dbc","3d048bf9":"# 3) Geolocation","b546d7d2":"# 4) Yelp Reviews and Prices","dbc2932c":"# 5) The Model","2315379d":"# 2) Data Cleaning","b9041e6b":"# Predictions","5428acb3":"# Feature Importance","952ffb48":"# Random Forest + K-Fold Cross Validation","38e5980c":"# Machine Learning and Real State: Predicting Rental Prices in Amsterdam","1b08e17c":"#  1) Webscraping (always check the website's TOS)","e623265a":"# Decision Tree"}}