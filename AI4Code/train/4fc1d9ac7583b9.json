{"cell_type":{"3436fe2d":"code","9f10c8a2":"code","a00bbbe3":"code","8151b963":"code","418d93da":"code","96a6f0c3":"code","9865737b":"code","0cbfc203":"code","e2ff4902":"code","8578d3b4":"code","b8440e22":"code","8b0dcf18":"code","f48a8634":"code","fcdd7c4b":"markdown"},"source":{"3436fe2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nlist_addrs=[]\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        list_addrs.append(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9f10c8a2":"list_addrs[21]","a00bbbe3":"len(list_addrs)","8151b963":"import pandas as pd\nimport numpy as np\nimport matplotlib as plt","418d93da":"import nltk\nimport re\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.cluster import KMeans\nfrom zipfile import ZipFile\nfrom nltk.tokenize import sent_tokenize \nnltk.download('punkt')\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.corpus import stopwords\n\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n  \nlemmatizer = WordNetLemmatizer()","96a6f0c3":"listof_docs=[]\ncount=0\nfor url in list_addrs:\n    train=pd.read_html(url,index_col=0)\n    \n    df=train[0]\n    #print(df)\n    list_of=list(df.index)\n    \n    print(count)\n    count=count+1\n    \n    cleanedlist = [x for x in list_of if str(x) != 'nan' if type(x)==str]\n    data=\"\".join(cleanedlist)\n    data=re.sub(\"[^a-zA-Z]+\", \" \",data)\n\n    if (len(data.split())<(len(df.index)\/3) or len(data.split())<10):\n        list_of=df[1].tolist()\n    #print(url,\":\",cleanedlist)\n    #data=\" \".join(list_of) \n    data","9865737b":"    df","0cbfc203":"    list_of","e2ff4902":"#no_count_index=[word for word in list_of if re.match('^[0-9]*$', word) if type(word) is str ]\n#len(no_count_index)> (len(df.index)\/2) #len(cleanedlist)-len(no_count_index)","8578d3b4":"    cleanedlist = [x for x in list_of if str(x) != 'nan'and type(x)==str]\n    print(cleanedlist)","b8440e22":"    data=\" \".join(cleanedlist)\n","8b0dcf18":"    \n    #for strng in list_of:\n    data=re.sub(\"[^a-zA-Z]+\", \" \",data)\n\n    data=' '.join(w for w in data.split() if len(w)>2)\n    data=data.lower()\n    data=data.split()\n\n    data=[lemmatizer.lemmatize(word) for word in data if not word in stopwords.words('english')]\n        #tokens=sent_tokenize(strng)\n    doc_string=' '.join(data)\n    listof_docs.append(doc_string)","f48a8634":"listof_docs","fcdd7c4b":"# Note this notebook is still under progress, I need some help got stuck as I wanted a list of docs i.e the feature as a paragraphs in list format like ['doc1', 'doc2','doc3...............'doc2573']. but I am only getting last url features. where I am wrong?? need help."}}