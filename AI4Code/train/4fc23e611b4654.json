{"cell_type":{"9d517d64":"code","81ebc6e7":"code","372ea5c3":"code","0dc6ca81":"code","19bfcaa4":"code","48fa37d9":"code","644855d9":"code","a44903be":"code","1cde4b09":"code","ce1e44c8":"code","2aaa5227":"code","a9e9ea29":"code","33c88654":"code","47f14c0d":"code","705c83fe":"code","10852d75":"code","16b09d90":"code","4248d539":"code","0a43c1df":"code","98095945":"code","ba98c090":"code","424dbfd9":"code","6eab504c":"code","35a6b7ac":"code","9db1d754":"code","ecc3502f":"code","1a514eca":"code","0e838d59":"code","9b8c32f1":"code","8ecd91f5":"code","73320f84":"code","19053fbe":"code","0fd1e455":"code","02960754":"code","13b066f0":"code","7344ff62":"code","934b53fc":"code","b0b55077":"code","baa589a5":"code","89d83b20":"code","51757fcc":"code","f12e0878":"code","7a8b8a0f":"code","d328f78b":"code","356570ae":"code","f02ff84c":"code","f6749d10":"code","dc1674cb":"code","0483d1ae":"code","fc643ddf":"code","f72ebb49":"code","79dc8211":"code","7c5a14f7":"code","14357563":"code","d2e08c5a":"code","bec79d7f":"code","31332bd3":"code","7c3e5783":"code","10432efa":"code","66c458c5":"code","22f943a9":"code","f438cecb":"code","6aedcfb2":"code","7eab3b76":"code","020c183a":"code","9328a242":"code","26bfa468":"code","a94a4ba8":"code","78688870":"code","c97ce077":"code","be756548":"markdown","4bcd5522":"markdown","543c52fa":"markdown","507b2d2b":"markdown","e1ac48aa":"markdown","64bee75c":"markdown","9463343e":"markdown","d39cbad5":"markdown","c936993f":"markdown","6628bcb4":"markdown","dc33b7cf":"markdown","7de7c1ef":"markdown","51125660":"markdown","59bcc6be":"markdown","2dbed135":"markdown","adfaf558":"markdown","8ce6e804":"markdown","099d3817":"markdown","ad19679b":"markdown","54a50ff8":"markdown","26354ef0":"markdown","8eae66cb":"markdown","2a8218e2":"markdown","5a528100":"markdown","c85ceaf0":"markdown","76342ea3":"markdown","bcc031c1":"markdown","3b4cf386":"markdown","85812270":"markdown","9b6dcd5c":"markdown","4da28ff6":"markdown","961684a7":"markdown","72c75bb8":"markdown","9bf691ab":"markdown","503fb130":"markdown","447b2f48":"markdown","346f4a64":"markdown","b52987e0":"markdown","cc93d134":"markdown","e9eeb1da":"markdown","e2403a80":"markdown","de229073":"markdown","05905dfe":"markdown","8ccb87b3":"markdown","cf36f5ef":"markdown","4c98f0db":"markdown","75d36da3":"markdown","e52b85c7":"markdown","8a574a49":"markdown","917061b0":"markdown","5ed21df9":"markdown","2027e330":"markdown","25185a1f":"markdown","92b40292":"markdown","acb79739":"markdown","792d7f0a":"markdown","2dfa3937":"markdown","ea579c1c":"markdown","9237fc63":"markdown","cf11336d":"markdown","fbbdc9cb":"markdown","f06e6292":"markdown","03aaee07":"markdown","f3b8d378":"markdown","21f2254c":"markdown","df795975":"markdown","9bf5b7ed":"markdown","01e0240d":"markdown","8f0f69b7":"markdown","0351a8a9":"markdown","dec6ff5f":"markdown","9a17961d":"markdown","b70cc94d":"markdown","ef8e5dd5":"markdown","980ae9c9":"markdown","9d34d238":"markdown"},"source":{"9d517d64":"import pandas as pd\nimport sklearn\nimport numpy as np","81ebc6e7":"cali_houses = pd.read_csv('..\/input\/housing.csv')","372ea5c3":"cali_houses.head()","0dc6ca81":"cali_houses.info()","19bfcaa4":"cali_houses[\"ocean_proximity\"].value_counts()","48fa37d9":"cali_houses.describe()","644855d9":"%matplotlib inline\nimport matplotlib.pyplot as plt \ncali_houses.hist(bins=40, figsize=(20,15))\nplt.show()","a44903be":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(cali_houses, test_size=0.2, random_state=34)","1cde4b09":"cali_houses[\"income_cat\"] = np.ceil(cali_houses[\"median_income\"] \/ 1.5)\ncali_houses[\"income_cat\"].where(cali_houses[\"income_cat\"] < 5, 5.0, inplace=True)","ce1e44c8":"cali_houses[\"income_cat\"].hist(bins=20, figsize=(5,5))\nplt.show()","2aaa5227":"from sklearn.model_selection import StratifiedShuffleSplit \nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=34) \nfor train_index, test_index in split.split(cali_houses, cali_houses[\"income_cat\"]):\n    strat_train_set = cali_houses.loc[train_index]\n    strat_test_set = cali_houses.loc[test_index]","a9e9ea29":"cali_houses[\"income_cat\"].value_counts() \/ len(cali_houses)","33c88654":"for set_ in (strat_train_set, strat_test_set): \n    set_.drop(\"income_cat\", axis=1, inplace=True)","47f14c0d":"cali_houses = strat_train_set.copy()","705c83fe":"cali_houses.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")","10852d75":"cali_houses.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)","16b09d90":"cali_houses.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, \n             s=cali_houses[\"population\"]\/100, label=\"population\", figsize=(12,8), \n             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n)\nplt.legend()","4248d539":"corr_matrix = cali_houses.corr()","0a43c1df":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)","98095945":"import pandas.plotting","ba98c090":"from pandas.plotting import scatter_matrix \nfeatures = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\nscatter_matrix(cali_houses[features], figsize=(12, 12))","424dbfd9":"cali_houses.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.5)","6eab504c":"cali_houses[\"rooms_per_household\"] = cali_houses[\"total_rooms\"]\/cali_houses[\"households\"]\ncali_houses[\"bedrooms_per_room\"] = cali_houses[\"total_bedrooms\"]\/cali_houses[\"total_rooms\"]\ncali_houses[\"population_per_household\"]=cali_houses[\"population\"]\/cali_houses[\"households\"]","35a6b7ac":"corr_matrix = cali_houses.corr() \ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","9db1d754":"cali_houses = strat_train_set.drop(\"median_house_value\", axis=1)\ncali_houses_labels = strat_train_set[\"median_house_value\"].copy()","ecc3502f":"sample_incomplete_rows = cali_houses[cali_houses.isnull().any(axis=1)].head()\nsample_incomplete_rows","1a514eca":"median = cali_houses[\"total_bedrooms\"].median() \ncali_houses[\"total_bedrooms\"].fillna(median, inplace=True)","0e838d59":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","9b8c32f1":"cali_houses_num =cali_houses.drop(\"ocean_proximity\", axis=1)","8ecd91f5":"imputer.fit(cali_houses_num)","73320f84":"imputer.statistics_","19053fbe":"cali_houses_num.median().values","0fd1e455":"X = imputer.transform(cali_houses_num)","02960754":"cali_houses_tr = pd.DataFrame(X, columns=cali_houses_num.columns,\n                          index = list(cali_houses.index.values))\n","13b066f0":"cali_houses_tr.loc[sample_incomplete_rows.index.values]","7344ff62":"cali_houses_cat = cali_houses[['ocean_proximity']]\ncali_houses_cat.head(10)","934b53fc":"from sklearn.preprocessing import OrdinalEncoder","b0b55077":"ordinal_encoder = OrdinalEncoder()\ncali_houses_cat_encoded = ordinal_encoder.fit_transform(cali_houses_cat)\ncali_houses_cat_encoded[:10]","baa589a5":"ordinal_encoder.categories_","89d83b20":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder() \ncali_houses_cat_1hot = cat_encoder.fit_transform(cali_houses_cat)\ncali_houses_cat_1hot","51757fcc":"cali_houses_cat_1hot.toarray()","f12e0878":"cat_encoder = OneHotEncoder(sparse=False)\ncali_houses_cat_1hot = cat_encoder.fit_transform(cali_houses_cat)\ncali_houses_cat_1hot","7a8b8a0f":"cat_encoder.categories_","d328f78b":"cali_houses.columns","356570ae":"\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nrooms_ix, bedrooms_ix, population_ix, household_ix = [\n    list(cali_houses.columns).index(col)\n    for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]\nfrom sklearn.preprocessing import FunctionTransformer\n\ndef add_extra_features(X, add_bedrooms_per_room=True):\n    rooms_per_household = X[:, rooms_ix] \/ X[:, household_ix]\n    population_per_household = X[:, population_ix] \/ X[:, household_ix]\n    if add_bedrooms_per_room:\n        bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n        return np.c_[X, rooms_per_household, population_per_household,\n                     bedrooms_per_room]\n    else:\n        return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = FunctionTransformer(add_extra_features, validate=False,\n                                 kw_args={\"add_bedrooms_per_room\": False})\ncali_houses_extra_attribs = attr_adder.fit_transform(cali_houses.values)","f02ff84c":"cali_houses_extra_attribs = pd.DataFrame(\n    cali_houses_extra_attribs,\n    columns=list(cali_houses.columns)+[\"rooms_per_household\", \"population_per_household\"])\ncali_houses_extra_attribs.head()","f6749d10":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n        ('std_scaler', StandardScaler()),\n    ])\n\ncali_houses_num_tr = num_pipeline.fit_transform(cali_houses_num)","dc1674cb":"cali_houses_num_tr","0483d1ae":"from sklearn.compose import ColumnTransformer","fc643ddf":"\nnum_attribs = list(cali_houses_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\ncali_houses_prepared = full_pipeline.fit_transform(cali_houses)","f72ebb49":"cali_houses_prepared","79dc8211":"cali_houses_prepared.shape","7c5a14f7":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(cali_houses_prepared, cali_houses_labels)","14357563":"\nsome_data = cali_houses.iloc[:5]\nsome_labels = cali_houses_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))","d2e08c5a":"print(\"Labels:\", list(some_labels))","bec79d7f":"from sklearn.metrics import mean_squared_error\n\ncali_houses_predictions = lin_reg.predict(cali_houses_prepared)\nlin_mse = mean_squared_error(cali_houses_labels, cali_houses_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","31332bd3":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(cali_houses_prepared, cali_houses_labels)","7c3e5783":"cali_houses_predictions = tree_reg.predict(cali_houses_prepared)\ntree_mse = mean_squared_error(cali_houses_labels, cali_houses_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","10432efa":"\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, cali_houses_prepared, cali_houses_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","66c458c5":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","22f943a9":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\nforest_reg.fit(cali_houses_prepared, cali_houses_labels)","f438cecb":"cali_houses_predictions = forest_reg.predict(cali_houses_prepared)\nforest_mse = mean_squared_error(cali_houses_labels, cali_houses_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","6aedcfb2":"from sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, cali_houses_prepared, cali_houses_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","7eab3b76":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    \n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n   \n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(cali_houses_prepared, cali_houses_labels)","020c183a":"grid_search.best_params_","9328a242":"grid_search.best_estimator_","26bfa468":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","a94a4ba8":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","78688870":"\nfinal_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","c97ce077":"final_rmse","be756548":"The first task you are asked to perform is to build a model of housing prices in California using the California census data. This data has metrics such as the population, median income, median housing price, and so on for each block group in California","4bcd5522":"Let\u2019s train a regression Model","543c52fa":"The correlation coefficient ranges from \u20131 to 1. When it is close to 1, it means that there is a strong positive correlation; for example, the median house value tends to go up when the median income goes up. When the coefficient is close to \u20131, it means that there is a strong negative correlation; you can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north). Finally, coefficients close to zero mean that there is no linear correlation.","507b2d2b":"We can use Scikit-Learn's FunctionTransformer class that lets you easily create a transformer based on a transformation function.","e1ac48aa":"##### Let\u2019s see if this worked as expected. You can start by looking at the income category proportions in the full housing dataset","64bee75c":"We left out the categorical feature ocean_proximity because it is a text feature so we cannot compute its median. Most Machine Learning algorithms prefer to work with numbers anyway, so let\u2019s convert these text labels to numbers.\n\nNow let's preprocess the categorical input feature, ocean_proximity:","9463343e":"The pipeline exposes the same methods as the final estimator. In this example, the last estimator is a StandardScaler, which is a transformer, so the pipeline has a transform() method that applies all the transforms to the data in sequence (it also has a fit_transform method that we could have used instead\nof calling fit() and then transform())","d39cbad5":"#### The info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute\u2019s type and number of non-null value.\n\nThere are 20,640 rows in the dataset. Notice that the total_bedrooms has only 20,433 nonnull values, meaning that 207 districts are missing this feature. All attributes are numerical, except the ocean_proximity field. Its type is object, so it could hold any kind of Python object, but since you loaded this data from a CSV file you know that it must be a text feature. When you looked at the top five rows, you probably noticed that the values in the ocean_proximity column were repetitive(nearbay), which means that it is probably a categorical feature. You can find out what categories exist and how many districts belong to each category by using the value_count method:","c936993f":"\nBy default, the OneHotEncoder class returns a sparse array, but we can convert it to a dense array if needed by calling the toarray() method:","6628bcb4":"Scikit-Learn provides a handy class to take care of missing values: Imputer. Here is how to use it. First, we need to create an Imputer instance, specifying that you want to replace each attribute\u2019s missing values with the median of that attribute","dc33b7cf":"Now we can use this \u201ctrained\u201d imputer to transform the training set by replacing missing values by the learned medians","7de7c1ef":"Since the median can only be computed on numerical attributes, we need to create a copy of the data without the text attribute ocean_proximity","51125660":"We can compute the standard correlation coefficient between every pair of attributes using the corr() method","59bcc6be":"##  significant features","2dbed135":"### Is there anything that might look strange in these histograms?","adfaf558":"## Transformation Pipelines","8ce6e804":"Another quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical feature. A histogram shows the number of rows (on the vertical axis) that have a given value range (on the horizontal axis). You can either plot this one feature at a time, or you can call the hist() method on the whole dataset, and it will plot a histogram for each numerical feature.","099d3817":"## Fine Tune Your Model","ad19679b":"Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is train_test_split. First there is a random state parameter that allows you to set the random generator seed and second you can pass it multiple datasets with an identical number of rows, and it will split them on the same indices.","54a50ff8":"Now the Decision Tree doesn\u2019t look as good as it did earlier, Notice that cross-validation allows we to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a score of approximately 71,052, generally \u00b11714, We would not have this information if you just used one validation set.","26354ef0":"##### Now you are ready to do stratified sampling based on the income category. For this you can use ScikitLearn\u2019s StratifiedShuffleSplit class:","8eae66cb":"Let's see some information about the features","2a8218e2":"Now you can fit the imputer instance to the training data using the fit() method:","5a528100":"This image tells you that the housing prices are related to the location and to the population density","c85ceaf0":"Another way to check for correlation between attributes is to use Pandas\u2019 scatter_matrix function, which plots every numerical attribute against every other numerical attribute. So let\u2019s focus on a few promising attributes that seem most correlated with the median housing value.","76342ea3":"It is much more likely that the model has badly overfit the data. How can we be sure? As we saw earlier, We don\u2019t want to touch the test set until you we ready to launch a model we are confident about, so we need to use part of the training set for training, and part for model validation.","bcc031c1":"##### Visualizing Geographical Dat","3b4cf386":"In this example, we obtain the best solution by setting the max_features hyperparameter to 8, and the n_estimators hyperparameter to 30. The RMSE score for this combination is 49,748, which is slightly better than the score you got earlier using the default hyperparameter values (which was 52, 649).\nFinally we have successfully fine-tuned your best model","85812270":"This plot reveals a few things. First, the correlation is indeed very strong; you can clearly see the upward trend and the points are not too dispersed. Second, the price cap that we noticed earlier is clearly visible as a horizontal line at $500,000.","9b6dcd5c":" \nNow let's build a pipeline for preprocessing the numerical features","4da28ff6":"## Preparing the Data","961684a7":"The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable.","72c75bb8":"## Exploring the Data","9bf691ab":"## Feature Scaling","503fb130":"\nOne of the most important transformations you need to apply to your data is feature scaling. For the most part, Machine Learning algorithms don\u2019t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. \n\nNote that scaling the target values is generally not required. There are two common ways to get all features to have the same scale: min-max scaling and standardization. Min-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. \n\nScikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don\u2019t want 0\u20131 for some reason. \n\nStandardization is quite different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the variance so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers. \n\nScikit-Learn provides a transformer called StandardScaler for standardization","447b2f48":"Let's create a custom transformer to add extra features:","346f4a64":"# End to end Machine Learning Project to predict median house values in California Districts.\n\n\n##### The task is to predict median house values in Californian districts, given a number of features from these districts.","b52987e0":"Most Machine Learning algorithms cannot work with missing features, so let\u2019s create a few functions to take care of them. We saw earlier that the total_bedrooms feature has some missing values, so let\u2019s fix this","cc93d134":"\nCheck that this is the same as manually computing the median of each feature:","e9eeb1da":"Now we have a working Linear Regression model. Let\u2019s try it out on a few instances from the training set","e2403a80":"We can now see the high-density areas, namely the Bay Area and around Los Angeles and San Diego.","de229073":"## Creating a Test set.","05905dfe":"Let's take a look at our sample now","8ccb87b3":"With similar code you can measure the income category proportions in the test set. The test set generated stratified sampling should have income category proportions almost identical to those in the full dataset. ","cf36f5ef":"We will Set the missing values to some value (zero, the mean, the median, etc.), In our case we will set to the median value","4c98f0db":"One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. Obviously this is not the case (for example, categories 0 and 4 are more similar than categories 0 and 1). To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is \u201c<1H OCEAN\u201d (and 0 otherwise), another attribute equal to 1 when the category is \u201cINLAND\u201d (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). \n\nScikit-Learn provides a OneHotEncoder encoder to convert integer categorical values into one-hot vectors. Let\u2019s encode the categories as one-hot vectors. Note that fit_transform() expects a 2D array, but cali_houses_cat_encoded is a 1D array, so we need to reshape it:","75d36da3":"It works, although the predictions are not exactly accurate. Let\u2019s measure this regression model\u2019s RMSE on the whole training set using Scikit-Learn\u2019s mean_squared_error function","e52b85c7":"let's try the full preprocessing pipeline on a few training instances","8a574a49":"### Handling Text and Categorical features","917061b0":"Evaluate Your System on the Test Set:\n\nAfter tweaking our models for a while, We finally have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. There is nothing special about this process; just get the predictors and the labels from your test set, run your full_pipeline to transform the data (call transform(), not fit_transform()), and evaluate the final model on the test set","5ed21df9":"### Combining Features\n\nWe identified a few data insights that we may want to clean up before feeding the data to a Machine Learning algorithm, and we found interesting correlations between features, in particular with the target featur. One last thing we may want to do before actually preparing the data for Machine Learning algorithms is to try out various feature combinations. For example, the total number of rooms in a district is not very useful if you don\u2019t know how many households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting feature combination to look at. Let\u2019s create these new features","2027e330":"We already know that the most important feature to predict the median house value is the median income, so let\u2019s zoom in on their correlation scatterplot.","25185a1f":"The result is a plain Numpy array containing the transformed features. If we want to put it back into a Pandas DataFrame, it\u2019s simple","92b40292":"##### We spent quite a bit of time on test set generation for a good reason: this is an often neglected but critical part of a Machine Learning project","acb79739":"Scikit-Learn\u2019s GridSearchCV search for hyperparameters good combination. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation. For example, the following code searches for the best combination of hyperparameter values for the RandomForestRegressor","792d7f0a":"This looks like California all right, but other than that it is hard to see any particular pattern. Setting the alpha option to 0.1 makes it much easier to visualize the places where there is a high density of data points. ","2dfa3937":"After looking at the data I can see that the median income is a very important attribute to predict median housing prices. We may want to ensure that the test set is representative of the various categories of incomes in the whole dataset. Since the median income is a continuous numerical attribute, We first need to create an income category attribute. Most median income values are clustered around 20,000\u201350,000, but some median incomes go far beyond 60,000. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of the stratum\u2019s importance may be biased. The following code creates an income category attribute by dividing the median income by 1.5 (to limit the number of income categories), and rounding up using ceil (to have discrete categories), and then merging all the categories greater than 5 into category 5.","ea579c1c":"The grid search will explore 12 + 6 = 18 combinations of RandomForestRegressor hyperparameter values, and it will train each model five times (since we are using five-fold cross validation). In other words there will be 18 \u00d7 5 = 90 rounds of training! It may take quite a long time, but when it is done you can get the best combination of parameters like this:","9237fc63":"Okay, this is better than nothing but clearly not a great score: most districts\u2019 median_housing_values range between  120,000 and  265,000, so a typical prediction error of $68,628 is not very satisfying. This is an example of a model underfitting the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough.","cf11336d":"Let's take a look at the first few rows","fbbdc9cb":"As we can see the new bedrooms_per_room attribute is much more correlated with the median house value than the total number of rooms or bedrooms. Apparently houses with a lower bedroom\/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district \u2014 obviously the larger the houses, the more expensive they are.","f06e6292":"Compare against the actual values:","03aaee07":"A great alternative is to use Scikit-Learn\u2019s cross-validation feature. The following code performs K-fold cross-validation: it randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:","f3b8d378":"First, make sure you have put the test set aside and you are only exploring the training set. Also, if the training set is very large, you may want to sample an exploration set, to make manipulations easy and fast. In our case, the set is quite small so you can just work directly on the full set. Let\u2019s create a copy so you\ncan play with it without harming the training set:","21f2254c":"But first let\u2019s revert to a clean training set (by copying strat_train_set once again), and let\u2019s separate the predictors and the labels since we don\u2019t necessarily want to apply the same transformations to the predictors and the target values (note that drop() creates a copy of the data and does not affect strat_train_set)","df795975":"Let\u2019s train a DecisionTreeRegressor. This is a powerful model, capable of finding complex nonlinear relationships in the data","9bf5b7ed":"Analyze the Best Models and Their Errors:\nWe will often gain good insights on the problem by inspecting the best models. For example, the RandomForestRegressor can indicate the relative importance of each attribute for making accurate prediction","01e0240d":"##### Now you should remove the income_cat attribute so the data is back to its original state","8f0f69b7":"### Data Cleaning","0351a8a9":"Thank You","dec6ff5f":"The housing median age and the median house value has a big bar at the end. We can make a guess that the housing median age is capped at 50 and the median house value is capped at 500,000. We have to confirm with the team that gave us this data. ","9a17961d":"## Select and Train a Model","b70cc94d":"Alternatively, you can set sparse=False when creating the OneHotEncoder:","ef8e5dd5":"## Correlations","980ae9c9":"The count, mean, min, and max rows are self-explanatory. Note that the null values are ignored (so, for example, count of total_bedrooms is 20,433, not 20,640). The std row shows the standard deviation, which measures how dispersed the values are. The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indicates the value below which a given percentage of observations in a group of observations falls. For example, 25% of the districts have a housing_median_age lower than 18, while 50% are lower than 29 and 75% are lower than 37. These are often called the 25th percentile (or 1st quartile), the median, and the 75th percentile (or 3rd quartile).","9d34d238":"Let\u2019s try another model now: the RandomForestRegressor. Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algorithms even further."}}