{"cell_type":{"c509f3df":"code","a3943537":"code","bff74d68":"code","77849aab":"code","f775a85b":"code","25a11fa8":"code","510d68b0":"code","8f86fa92":"code","809a08b3":"code","ec5b41dd":"code","6d894167":"code","02a47fd1":"code","f7131a63":"code","77347703":"code","ad8b5895":"code","5e7e9865":"code","ae49aff0":"code","41d5b92b":"code","dd7ebe42":"code","4c231c45":"code","4cd51440":"code","d639551b":"code","9b80c7c9":"code","c27a308d":"code","05def664":"code","77c836fc":"code","e8a3436d":"code","176e9ff6":"code","ec455763":"code","853b6330":"code","ba472496":"code","c39f1804":"code","1350ddb7":"code","43b282e4":"code","e21b5faf":"code","3ad15acc":"markdown","8bc5e41b":"markdown","c0e44855":"markdown","7acc3544":"markdown","c4787b63":"markdown","fcd7c447":"markdown","c67dcc9c":"markdown","ce0f760c":"markdown","0b4fdb0e":"markdown","839368e2":"markdown","280a359c":"markdown","351ea732":"markdown","0cbc28d5":"markdown","adee4293":"markdown","5de9485a":"markdown","40c938da":"markdown","073a5d08":"markdown","e7e11360":"markdown","d20cfa8f":"markdown","3a2ed439":"markdown","9bf548a5":"markdown","2ba0816f":"markdown","fb0a661d":"markdown","2fb86787":"markdown","43a24ba1":"markdown","3b3c7695":"markdown","deb2090b":"markdown","2eabdeeb":"markdown","9025afd3":"markdown","108d1540":"markdown","e67fa010":"markdown","1d776b70":"markdown","20fbe561":"markdown","f41e2e68":"markdown","48325da1":"markdown","926fb314":"markdown","9dda32a9":"markdown","1d641ff4":"markdown","c659018a":"markdown","0e0c3d2a":"markdown","51f8afe7":"markdown","57672280":"markdown","252945f8":"markdown"},"source":{"c509f3df":"import os\nimport re\n\nimport json\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport random\nfrom tqdm import tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom nltk import sent_tokenize\nfrom albumentations.core.transforms_interface import DualTransform, BasicTransform\n\nfrom termcolor import colored\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\n\nimport wandb\nfrom wandb.keras import WandbCallback\nwandb.login()","a3943537":"class config:\n    SEED = 42\n    DIRECTORY_PATH = \"..\/input\/feedback-prize-2021\"\n    TRAIN_CSV_PATH = os.path.join(DIRECTORY_PATH, 'train.csv')","bff74d68":"def set_seed(seed=config.SEED):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    \nset_seed()","77849aab":"train = pd.read_csv(config.TRAIN_CSV_PATH)","f775a85b":"train.head()","25a11fa8":"submission = pd.read_csv(\"..\/input\/feedback-prize-2021\/sample_submission.csv\")","510d68b0":"submission.head()","8f86fa92":"class TextAugmentation(BasicTransform):\n    \n    \"\"\" \n    Transform for NLP task.\n    \"\"\"\n\n    @property\n    def targets(self):\n        return {\"data\": self.apply}\n    \n    def update_params(self, params, **kwargs):\n        if hasattr(self, \"interpolation\"):\n            params[\"interpolation\"] = self.interpolation\n        if hasattr(self, \"fill_value\"):\n            params[\"fill_value\"] = self.fill_value\n        return params\n\n    def get_sentences(self, text):\n        return sent_tokenize(text)","809a08b3":"def demo(augmentation, text):\n    \n    \"\"\"\n    Function to demonstrate the applied augmentation.\n    \n    params:\n        augmentation - The augmentation to be applied on the text\n        text: Text on which transform will be applied\n        \n    \"\"\"\n    \n    output = augmentation(data=(text))['data']\n\n    print(\"Original Text\")\n    print(colored(text, 'red'))\n    \n    print()\n    \n    print(\"Augmented Text\")\n    print(colored(output, 'yellow'))","ec5b41dd":"class SentenceShuffleAugmentation(TextAugmentation):\n    \"\"\" Shuffle the sentences of the text. \"\"\"\n    \n    def __init__(self, always_apply=False, p=0.5):\n        super(SentenceShuffleAugmentation, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        sentences = self.get_sentences(text)\n        random.shuffle(sentences)\n        return ' '.join(sentences)","6d894167":"demo(\n    augmentation = SentenceShuffleAugmentation(p=1.0),\n    text = train.iloc[0, 4]\n)","02a47fd1":"class RemoveDuplicateSentencesAugmentation(TextAugmentation):\n    \"\"\" Exclude equal sentences \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(RemoveDuplicateSentencesAugmentation, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        sentences = []\n        for sentence in self.get_sentences(text):\n            sentence = sentence.strip()\n            if sentence not in sentences:\n                sentences.append(sentence)\n        return ' '.join(sentences)","f7131a63":"demo(\n    augmentation = RemoveDuplicateSentencesAugmentation(p=1.0),\n    text = train.iloc[0, 4][0:46] + \" \" + train.iloc[0, 4][0:46]\n)","77347703":"class RemoveNumbersAugmentation(TextAugmentation):\n    \"\"\" Exclude any numbers \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(RemoveNumbersAugmentation, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        text = re.sub(r'[0-9]', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text","ad8b5895":"demo(\n    augmentation = RemoveNumbersAugmentation(p=1.0),\n    text = \"There are 15594 samples of training data.\"\n)","5e7e9865":"class RemoveHashtagsAugmentation(TextAugmentation):\n    \"\"\" Exclude any hashtags with # \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(RemoveHashtagsAugmentation, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        text = re.sub(r'#[\\S]+\\b', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text","ae49aff0":"demo(\n    augmentation = RemoveHashtagsAugmentation(p=1.0),\n    text = \"Kaggle Competitions are fun. #MachineLearning\"\n)","41d5b92b":"class RemoveMentionsAugmentation(TextAugmentation):\n    \"\"\" Exclude @users \"\"\"\n    \n    def __init__(self, always_apply=False, p=0.5):\n        super(RemoveMentionsAugmentation, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        text = re.sub(r'@[\\S]+\\b', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text","dd7ebe42":"demo(\n    augmentation = RemoveMentionsAugmentation(p=1.0),\n    text = \"@AnthonyGoldbloom is the founder of Kaggle.\"\n)","4c231c45":"class RemoveUrlAugmentation(TextAugmentation):\n    \"\"\" Exclude urls \"\"\"\n    \n    def __init__(self, always_apply=False, p=0.5):\n        super(RemoveUrlAugmentation, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        text = re.sub(r'https?\\S+', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text","4cd51440":"demo(\n    augmentation = RemoveUrlAugmentation(p=1.0),\n    text = \"https:\/\/www.kaggle.com hosts the world's best Machine Learning Hackathons.\"\n)","d639551b":"class CutOutWordsAugmentation(TextAugmentation):\n    \"\"\" Remove random words \"\"\"\n    \n    def __init__(self, cutout_probability=0.05, always_apply=False, p=0.5):\n        super(CutOutWordsTransform, self).__init__(always_apply, p)\n        self.cutout_probability = cutout_probability\n\n    def apply(self, data, **params):\n        text = data\n        words = text.split()\n        words_count = len(words)\n        if words_count <= 1:\n            return text\n        \n        new_words = []\n        for i in range(words_count):\n            if random.random() < self.cutout_probability:\n                continue\n            new_words.append(words[i])\n\n        if len(new_words) == 0:\n            return words[random.randint(0, words_count-1)]\n\n        return ' '.join(new_words)","9b80c7c9":"demo(\n    augmentation = CutOutWordsTransform(p=1.0, cutout_probability=0.2),\n    text = \"Competition objective is to analyze argumentative writing elements from students grade 6-12.\"\n)","c27a308d":"vocab_size = 10000                    # Vocabulary size\nsequence_length = 1024                # Sequence Length\nbatch_size = 128                      # Batch size\nunk_token = \"<unk>\"                   # Unknownd token\nvectorizer_path = \"vectorizer.json\"\n\n# Use output dataset for inference\noutput_dataset_path = \"..\/input\/name-entity-recognition-with-keras-output\/\"\nmodel_path = \"model.h5\"\nembed_size = 64\nhidden_size = 64\n\nmodes = [\"training\", \"inference\"]     # There is training and inference mode\nmode = modes[1]\nepochs = 10\ndropout = 0.2                         # Dropout rate for the Model","05def664":"# wandb config\nWANDB_CONFIG = {\n     'competition': 'Feedback Prize', \n              '_wandb_kernel': 'neuracort'\n    }","77c836fc":"train[\"file_path\"] = train[\"id\"].apply(\n    lambda item: \"..\/input\/feedback-prize-2021\/train\/\" + item + \".txt\"\n)\n\nsubmission[\"file_path\"] = submission[\"id\"].apply(\n    lambda item: \"..\/input\/feedback-prize-2021\/test\/\" + item + \".txt\"\n)\n\ndiscourse_types = np.array(\n    [\"<PAD>\", \"<None>\"] + sorted(train[\"discourse_type\"].unique())\n)\n\ndiscourse_types_index = dict(\n    [(discoure_type, index) for (index, discoure_type) in enumerate(discourse_types)]\n)","e8a3436d":"def get_range(item):\n    locations = [int(location) for location in item[\"predictionstring\"].split(\" \")]\n    return (locations[0], locations[-1])","176e9ff6":"character_counter = defaultdict(int)\ncharacter_counter\nallow_set = set(\"'&%-_\/$+\u00c2\u00c3\u00c5\u00cb\u00d3\u00e2\u00e5\u00f3\u00fe@|~\u00a2\u00a3\u00a2\u00a3\")\n\ndef tokenize(text):\n    \n    tokens = []\n    chars = []\n    \n    for i in range(len(text)):\n        c = text[i].lower()\n        character_counter[c] += 1\n        is_valid = c.isalnum() or c in allow_set\n    \n        if i >= 1 and i < len(text) - 1:\n            if text[i-1].isdigit() and text[i+1].isdigit():\n                is_valid = True\n            elif text[i-1].isalpha() and text[i+1].isalpha() and c == \".\":\n                is_valid = True\n        \n        if is_valid:\n            chars.append(c)\n        \n        if (not is_valid or i == len(text) - 1) and len(chars) > 0:\n            tokens.append(\"\".join(chars))\n            chars.clear()\n    \n    return tokens","ec455763":"%%time\n\nbegin = time.time()\nlast_id = \"\"\ncontents = []\nwrong_samples = []\ntoken_list = []\nannotation_list = []\n\nnum_samples = len(train)\nunmaptch_count = 0              # Number of sentences extracted from predictionstring that doesn't discourse_text\nmatch_count = 0                 # Number of sentences extracted from predictionstring that matches discourse_text including shifting\ncompletely_match_count = 0      # Number of sentences extracted from predictionstring that matches discourse_text without shifting\nmismatch_count = 0\n\nfor i in range(len(train)):\n    item = train.iloc[i]\n    identifier = item[\"id\"] \n    discourse_type_id = discourse_types_index[item[\"discourse_type\"]]\n    \n    if identifier != last_id:\n        last_id = identifier\n    \n        with open(item[\"file_path\"]) as f:\n            content = \"\".join(f.readlines())\n            contents.append(content)\n            tokens = tokenize(content)\n            token_list.append(tokens)\n            annotations = [1] * len(tokens)\n            annotation_list.append(annotations)\n    \n    annotation_range = get_range(item)\n    extracted = tokens[annotation_range[0]:annotation_range[1]+1]\n    discourse = tokenize(item[\"discourse_text\"])\n    delta = None\n    num_tokens_to_compare = min(len(discourse), 3)\n    \n    # Compare text extracted from predictionstring with discourse_text, shift discourse_text or right if needed, just compare a few words for performance\n    for j in range(10):\n    \n        if len(extracted) < num_tokens_to_compare or len(discourse) <= j + num_tokens_to_compare:\n            break\n        \n        if extracted[0:num_tokens_to_compare] == discourse[j:num_tokens_to_compare+j]:\n            delta = j\n            break\n    \n    if delta == None:\n        for j in range(10):\n            if len(discourse) < num_tokens_to_compare and len(extracted) <= j + num_tokens_to_compare:\n                break\n            \n            if discourse[0:num_tokens_to_compare] == extracted[j:num_tokens_to_compare+j]:\n                delta = -j\n                break\n    \n    if delta == None:\n        unmaptch_count += 1\n    \n    else:\n        not_match = False\n        for j in range(annotation_range[0] - delta, min(min(annotation_range[1] - delta + 1, len(tokens)), len(discourse) + annotation_range[0] - delta)): \n            if tokens[j] != discourse[j - annotation_range[0] + delta]:\n                mismatch_count += 1\n                not_match = True\n                break\n        \n        if not not_match:\n            for j in range(annotation_range[0] - delta, min(min(annotation_range[1] - delta + 1, len(tokens)), len(discourse) + annotation_range[0] - delta)): \n                annotation_list[-1][j] = discourse_type_id\n            match_count += 1\n        \n        else:\n            unmaptch_count += 1\n        \n        if delta == 0:\n            completely_match_count += 1 \n\nprint(\"Unmatch count:%d Match Count: %d Completedly Match count: %d\"%(unmaptch_count, match_count, completely_match_count))\nprint(\"Mismatch count:\", mismatch_count)\nprint(token_list[0])\nprint(annotation_list[0])","853b6330":"class Vectorizer:\n    \n    def __init__(self, vocab_size = None, sequence_length = None, unk_token = \"<unk>\"):\n        \n        self.vocab_size = vocab_size\n        self.sequence_length = sequence_length\n        self.unk_token = unk_token\n        \n    def fit_transform(self, sentences):\n        \n        word_counter = dict()\n        \n        for tokens in sentences:\n            for token in tokens: \n                if token in word_counter:\n                    word_counter[token] += 1\n                else:\n                    word_counter[token] = 1\n        \n        word_counter = pd.DataFrame({\"key\": word_counter.keys(), \"count\": word_counter.values()})\n        word_counter.sort_values(by=\"count\", ascending=False, inplace=True)\n        vocab = set(word_counter[\"key\"][0:self.vocab_size-1])\n        \n        word_index = dict()\n        begin_index = 1 \n        word_index[self.unk_token] = begin_index\n        begin_index += 1\n        \n        Xs = []\n        \n        for i in range(len(sentences)):\n            X = []\n            \n            for token in sentences[i]:\n                \n                if token not in word_index and token in vocab:\n                    word_index[token] = begin_index\n                    begin_index += 1\n                \n                if token in word_index:\n                    X.append(word_index[token])\n                \n                else:\n                    X.append(word_index[self.unk_token])\n                \n                if len(X) == self.sequence_length:\n                    break\n            \n            if len(X) < self.sequence_length:\n                X += [0] * (self.sequence_length - len(X))\n            \n            Xs.append(X)\n        \n        self.word_index = word_index\n        self.vocab = vocab\n        \n        return Xs\n    \n    def transform(self, sentences):\n        \n        Xs = []\n        \n        for i in range(len(sentences)):\n            X = []\n            \n            for token in sentences[i]:\n                if token in self.word_index:\n                    X.append(self.word_index[token])\n                else:\n                    X.append(self.word_index[self.unk_token])\n                if len(X) == self.sequence_length:\n                    break\n            \n            if len(X) < self.sequence_length:\n                X += [0] * (self.sequence_length - len(X))\n            Xs.append(X)\n        \n        return Xs\n    \n    def load(self, path):\n        \n        with open(path, 'r') as f:\n            \n            dic = json.load(f)\n            self.vocab_size = dic['vocab_size']\n            self.sequence_length = dic['sequence_length']\n            self.unk_token = dic['unk_token']\n            self.word_index = dic['word_index']\n            \n    def save(self, path):\n        \n        with open(path, 'w') as f:\n            \n            data = json.dumps({\n                \"vocab_size\": self.vocab_size, \n                \"sequence_length\": self.sequence_length, \n                \"unk_token\": self.unk_token,\n                \"word_index\": self.word_index\n            })\n            \n            f.write(data)","ba472496":"%%time\n\nvectorizer = Vectorizer(vocab_size = vocab_size, sequence_length = sequence_length, unk_token = unk_token)\n\nif mode == modes[0]:\n    Xs = vectorizer.fit_transform(token_list)\n    vectorizer.save(vectorizer_path)\n\nelse:\n    vectorizer.load(output_dataset_path + vectorizer_path)\n    Xs = vectorizer.transform(token_list)\n\nys = []\nannotation_count = [0] * len(discourse_types_index)\n\nfor annotation in annotation_list:\n    if len(annotation) <= sequence_length:\n        ys.append(annotation + [0] * (sequence_length - len(annotation)))\n    else:\n        ys.append(annotation[0:sequence_length])\n    for item in ys[-1]:\n        annotation_count[item] += 1\n\nX_train, X_val, y_train, y_val = train_test_split(np.array(Xs), np.array(ys), test_size = 0.2, random_state=42)","c39f1804":"def make_dataset(X, y, batch_size, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices((X, y))\n    if mode == \"train\":\n        ds = ds.shuffle(512)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    \n    return ds","1350ddb7":"train_ds = make_dataset(X_train, y_train, batch_size)\nval_ds = make_dataset(X_val, y_val, batch_size, mode=\"valid\")","43b282e4":"model = keras.Sequential([\n    keras.layers.Embedding(vocab_size, embed_size, input_length=sequence_length),\n    keras.layers.SpatialDropout1D(dropout),\n    keras.layers.Bidirectional(keras.layers.LSTM(hidden_size, dropout=dropout, recurrent_dropout=dropout)),\n    keras.layers.RepeatVector(sequence_length),\n    keras.layers.Bidirectional(keras.layers.LSTM(hidden_size, return_sequences=True)),\n    keras.layers.TimeDistributed(keras.layers.Dense(len(discourse_types), activation=\"softmax\"))\n])\n\nmodel.summary()","e21b5faf":"# Initialize W&B\nrun = wandb.init(project='feedback-prize-nlp-augmentations', config=WANDB_CONFIG)\n\nif mode == modes[0]:\n    checkpoint = keras.callbacks.ModelCheckpoint(\n        model_path, \n        save_best_only=True,\n        save_weights_only=True\n    )\n    \n    early_stop = keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        patience=10\n    )\n    \n    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=2, \n        min_lr=1e-7\n    )\n    \n    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n    callbacks = [early_stop, checkpoint, reduce_lr, WandbCallback()]\n    optimizer = tf.keras.optimizers.Adam(1e-3)\n    model.compile(loss=loss, optimizer=optimizer)\n    model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks)\n    wandb.log({'loss':loss})\n\nelse:\n    model.load_weights(output_dataset_path + model_path)\n    \nwandb.finish()","3ad15acc":"I am demonstrating a simple NER model where you can try applying above transforms and see the change in results. \n\nNote that some of these transforms might not be suitable entirely for NER, you can try them for other applications as well. For the model I am referring to [this](https:\/\/www.kaggle.com\/lonnieqin\/name-entity-recognition-with-keras) notebook.","8bc5e41b":"## **<span style=\"color:orange;\">Model<\/span>**","c0e44855":"## **<span style=\"color:orange;\">Tokenization<\/span>**","7acc3544":"## **<span style=\"color:orange;\">5. Remove Mentions<\/span>**\n<a id=\"basic-5\"><\/a>\n\nIn this transform we remove any mentions (word beginning with '@') from the text.","c4787b63":"## **<span style=\"color:orange;\">4. Remove Hashtags<\/span>**\n<a id=\"basic-4\"><\/a>\n\nIn Remove Hashtags, we will remove any hashtag from the text. ","fcd7c447":"<a id=\"model\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Model<\/center><\/h2>","c67dcc9c":"## **<span style=\"color:orange;\">Model Configuration<\/span>**","ce0f760c":"<a id=\"weights-and-biases\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Weights and Biases<\/center><\/h2>","0b4fdb0e":"## **<span style=\"color:orange;\">2. Remove Duplicate Sentences<\/span>**\n<a id=\"basic-2\"><\/a>\n\nIn Remove Duplicates we will remove duplicate sentences from the text. \n\nTo demonstrate this, we concatenate a sentence with itself and output should only be the original text.","839368e2":"## **<span style=\"color:orange;\">1. Sentence Shuffling<\/span>**\n<a id=\"basic-1\"><\/a>\n\nIn Sentence Shuffling we will randomly shuffle the sentences of the text.","280a359c":"---","351ea732":"## **<span style=\"color:orange;\">Dataset<\/span>**","0cbc28d5":"<a id=\"basic-nlp-augmentations\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Basic NLP Augmentations<\/center><\/h2>","adee4293":"---","5de9485a":"--- \n\n## **<span style=\"color:orange;\">Let's have a Talk!<\/span>**\n> ### Reach out to me on [LinkedIn](https:\/\/www.linkedin.com\/in\/ishandutta0098)\n\n---","40c938da":"---","073a5d08":"<a id=\"competition-overview\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Competition Overview<\/center><\/h2>","e7e11360":"---","d20cfa8f":"## **<span style=\"color:orange;\">3. Remove Numbers<\/span>**\n<a id=\"basic-3\"><\/a>\n\nIn Remove Numbers, we will remove any number from the text. We can simply achieve this using regular expressions.","3a2ed439":"<a id=\"what-is-data-augmentation\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>What is Data Augmentation<\/center><\/h2>","9bf548a5":"<h1><center>NLP Augmentations Master Notebook<\/center><\/h1>\n<h3><center>Feedback Prize - Evaluating Student Writing<\/center><\/h3>\n\n<center><img src = \"https:\/\/www.gsu.edu\/wp-content\/themes\/gsu-flex-2\/images\/logo.png\" width = \"750\" height = \"500\"\/><\/center>                                                                          ","2ba0816f":"## **<span style=\"color:orange;\">Preprocessing<\/span>**","fb0a661d":"---","2fb86787":"## **<span style=\"color:orange;\">6. Remove URLs<\/span>**\n<a id=\"basic-6\"><\/a>\n\nIn this transform we remove any URLs from the text.","43a24ba1":"**Weights & Biases** is the machine learning platform for developers to build better models faster.\n\nYou can use W&B's lightweight, interoperable tools to\n\n- quickly track experiments,\n- version and iterate on datasets,\n- evaluate model performance,\n- reproduce models,\n- visualize results and spot regressions,\n- and share findings with colleagues.\n  \nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.\n\nIn this notebook I will use Weights and Biases's amazing features to perform wonderful visualizations and logging seamlessly.","3b3c7695":"In this section we will look at some simple transforms that can be performed on Text Data. To develop this section I am referring to [this](https:\/\/www.kaggle.com\/shonenkov\/nlp-albumentations) notebook by Alex Shonenkov. I will build upon those transforms with more examples and techniques.\n\n**I will demonstrate the 7 basic transforms, namely:**\n1. [Sentence Shuffling](#basic-1)\n2. [Remove Duplicate Sentences](#basic-2)\n3. [Remove Numbers](#basic-3)\n4. [Remove Hashtags](#basic-4)\n5. [Remove Mentions](#basic-5)\n6. [Remove URLs](#basic-6)\n7. [Cut Out Words](#basic-7)","deb2090b":"---","2eabdeeb":"## **<span style=\"color:orange;\">7. Cut Out Words<\/span>**\n<a id=\"basic-7\"><\/a>\n\nIn this transform, we remove some words from the text.","9025afd3":"## **<span style=\"color:orange;\">Vectorization<\/span>**","108d1540":"## **<span style=\"color:orange;\">Definition<\/span>**\n\nData Augmentation is a technique used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n\n## **<span style=\"color:orange;\">Why is Data Augmentation Important?<\/span>**\nData augmentation is useful to improve performance and outcomes of machine learning models by forming new and different examples to train datasets. If dataset in a machine learning model is rich and sufficient, the model performs better and more accurate.\n  \nFor machine learning models, collecting and labeling of data can be exhausting and costly processes. Transformations in datasets by using data augmentation techniques allow companies to reduce these operational costs.\n\n---","e67fa010":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.<\/center><\/h3>","1d776b70":"<a id=\"global-config\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Global Config<\/center><\/h2>","20fbe561":"## **<span style=\"color:orange;\">Description<\/span>**\n\n\nIn this competition, you\u2019ll identify elements in student writing. More specifically, you will automatically segment texts and classify argumentative and rhetorical elements in essays written by 6th-12th grade students. You'll have access to the largest dataset of student writing ever released in order to test your skills in natural language processing, a fast-growing area of data science.\n  \n---\n\n## **<span style=\"color:orange;\">Evaluation Metric<\/span>**\n\nSubmissions are evaluated on the overlap between ground truth and predicted word indices.\n\n1. For each sample, all ground truths and predictions for a given class are compared.\n2. If the overlap between the ground truth and prediction is >= 0.5, and the overlap between the prediction and the ground truth >= 0.5, the prediction is a match and considered a `true positive`. If multiple matches exist, the match with the highest pair of overlaps is taken.\n3. Any unmatched ground truths are `false negatives` and any unmatched predictions are `false positives`.\n\n---","f41e2e68":"<a id=\"load-datasets\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load Datasets<\/center><\/h2>","48325da1":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents<\/center><\/h2>","926fb314":"<center><img src = \"https:\/\/i.imgur.com\/1sm6x8P.png\" width = \"750\" height = \"500\"\/><\/center>  ","9dda32a9":"<a id=\"libraries\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries<\/center><\/h2>","1d641ff4":"---","c659018a":"---","0e0c3d2a":"> | S.No       |                   Heading                |\n> | :------------- | :-------------------:                |         \n> |  01 |  [**Competition Overview**](#competition-overview)  |                   \n> |  02 |  [**Libraries**](#libraries)                        |  \n> |  03 |  [**Global Config**](#global-config)                |\n> |  04 |  [**Weights and Biases**](#weights-and-biases)      |\n> |  05 |  [**Load Datasets**](#load-datasets)                |\n> |  06 |  [**What is Data Augmentation?**](#what-is-data-augmentation)  |\n> |  07 |  [**Basic NLP Augmentations**](#basic-nlp-augmentations)   |\n> |  08 |  [**Model**](#model)","51f8afe7":"After receiving great response (Gold Medal, 180+ Upvotes) for my [Image Augmentations Master Notebook](https:\/\/www.kaggle.com\/ishandutta\/petfinder-data-augmentations-master-notebook) I am creating a similar notebook but for Text based NLP Augmentations.\n\nI will demonstrate with examples how you can apply a variety of transformations to your dataset based on your application!","57672280":"---","252945f8":"---"}}