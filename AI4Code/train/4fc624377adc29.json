{"cell_type":{"5f1ec2c3":"code","4cf2cafd":"code","1145fea9":"code","3f5d9422":"code","519bdf55":"code","5fe70bba":"code","d7fadc15":"code","c0fdbdf4":"code","81743a41":"code","d493a83b":"code","f8adc269":"markdown","88c1f407":"markdown","72ccdca0":"markdown","0cd688ae":"markdown","ad33d818":"markdown","2fb084b5":"markdown","32a7c24c":"markdown","bf5eb39c":"markdown","f7925a71":"markdown","3bd656c1":"markdown","0260c7ca":"markdown","233c2622":"markdown","cb37c4db":"markdown"},"source":{"5f1ec2c3":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy import sparse\nimport re\nimport string","4cf2cafd":"HTML('<iframe width=\"800\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/59bMh59JQDo\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","1145fea9":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","3f5d9422":"text = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return text.sub(r' \\1 ', s).split()\nlength = train_df.shape[0]\nVectorize = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )","519bdf55":"train = Vectorize.fit_transform(train_df[\"comment_text\"])\ntest = Vectorize.transform(test_df[\"comment_text\"])","5fe70bba":"#Target\ny = np.where(train_df['target'] >= 0.5, 1, 0)","d7fadc15":"class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, dual=False, n_jobs=1):\n        self.C = C\n        self.dual = dual\n        self.n_jobs = n_jobs\n\n    def predict(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def predict_proba(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict_proba(x.multiply(self._r))\n\n    def fit(self, x, y):\n        y = y\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) \/ ((y==y_i).sum()+1)\n        \n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) \/ pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n        return self","c0fdbdf4":"NbSvm = NbSvmClassifier(C=1.5, dual=True, n_jobs=-1)\nNbSvm.fit(train, y)","81743a41":"prediction=NbSvm.predict_proba(test)[:,1]","d493a83b":"submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission['prediction'] = prediction\nsubmission.to_csv('submission.csv', index=False)","f8adc269":"### `Table of contents`\n\n1. [Loading packages](#load)\n2. [Understainding unintended bias](#bias)\n3. [loading data](#data)\n4. [Data Cleaning](#clean)\n5. [Training data](#traindata)\n6. [Baseline Model](#model)\n7. [Traning Stage](#modeltrain)\n8. [Prediction](#Prediction)\n9. [Submission](#Submission)","88c1f407":"# GOAL\nIn this competition we are asked to build a model that recognizes toxicity and minimizes unintended bias","72ccdca0":"## Model <a class=\"anchor\" id=\"model\"><\/a>","0cd688ae":"## loading data <a class=\"anchor\" id=\"data\"><\/a>","ad33d818":"## Submission <a class=\"anchor\" id=\"Submission\"><\/a>","2fb084b5":"# Traning Stage <a class=\"anchor\" id=\"modeltrain\"><\/a>","32a7c24c":"## Loading packages <a class=\"anchor\" id=\"load\"><\/a>","bf5eb39c":"## Prediction <a class=\"anchor\" id=\"Prediction\"><\/a>","f7925a71":"## Training data <a class=\"anchor\" id=\"traindata\"><\/a>","3bd656c1":"Examples\n\nSome examples of comments and their associated toxicity and identity labels. Label values range from 0.0 - 1.0 represented the fraction of raters who believed the label fit the comment.\n\n1. Comment: I'm a white woman in my late 60's and believe me, they are not too crazy about me either!!\n\n    Toxicity Labels:` All 0.0`\n    \n    Identity Mention Labels: `female: 1.0, white: 1.0 (all others 0.0)`\n\n2. Comment: Why would you assume that the nurses in this story were women?\n\n    Toxicity Labels: `All 0.0`\n    \n    Identity Mention Labels: `female: 0.8 (all others 0.0)`\n\n3. Comment: Continue to stand strong LGBT community. Yes, indeed, you'll overcome and you have.\n\n    Toxicity Labels: `All 0.0`\n    \n    Identity Mention Labels: `homosexual_gay_or_lesbian: 0.8, bisexual: 0.6, transgender: 0.3 (all others 0.0)`","0260c7ca":"## Data Cleaning <a class=\"anchor\" id=\"clean\"><\/a>\n\n`TfidfVectorizer` - It Transforms text to feature vectors that can be used as input to estimator.","233c2622":"## Understainding unintended bias <a class=\"anchor\" id=\"bias\"><\/a>","cb37c4db":"Refrence : \n[Kernel](https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline)"}}