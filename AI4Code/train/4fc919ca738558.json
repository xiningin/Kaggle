{"cell_type":{"94207a80":"code","bb4cdba2":"code","b7299478":"code","19335dce":"code","22b9fe5c":"code","babe669a":"code","cf046c33":"code","38ed1bff":"code","fac8a148":"code","f9036429":"code","74214cde":"code","15a010f9":"code","874a18d5":"code","26aeddc3":"code","c8c8336d":"code","35fa3675":"code","1e2818fa":"code","24a7185b":"code","dfd75f82":"code","ec63d94c":"code","091ed892":"code","18204754":"code","8abb51fc":"code","8855345a":"code","9ab0c459":"code","b0c0b1df":"code","992b3c94":"code","678d3d8b":"code","b2a62454":"code","f40cbefa":"code","cd24ec93":"code","96f448e0":"code","8305dde4":"code","78d5c659":"code","70e54f8c":"code","a166261b":"code","6efc5c1e":"markdown","c2a630fe":"markdown","ac11bcf9":"markdown","ca0c3cd2":"markdown","71064c5a":"markdown","bbc7e34a":"markdown","df637b96":"markdown","4f9a4db2":"markdown","034cbed2":"markdown","973da9e6":"markdown","f9be464d":"markdown","4d618267":"markdown","24e1814b":"markdown","427771b8":"markdown","2df6049f":"markdown","0c542084":"markdown","13e68e1b":"markdown","0d040535":"markdown","88d294d3":"markdown","442b9171":"markdown","7a1a9171":"markdown","dbb90ec3":"markdown","6bbfca5b":"markdown","be7c81dc":"markdown"},"source":{"94207a80":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import metrics\nfrom tensorflow.keras import backend as K","bb4cdba2":"# Reading the folder architecture of Kaggle to get the dataset path.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b7299478":"# Reading the Train and Test Datasets.\nmnist_train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nmnist_test = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","19335dce":"# Let's see the shape of the train and test data\nprint(mnist_train.shape, mnist_test.shape)","22b9fe5c":"# Looking at a few rows from the data isn't a bad idea.\nmnist_train.head()","babe669a":"# and yeah, here you will see the basic statistical insights of the numerical features of train data.\nmnist_train.describe()","cf046c33":"mnist_train.isna().any().any()","38ed1bff":"# dividing the data into the input and output features to train make the model learn based on what to take in and what to throw out.\nmnist_train_data = mnist_train.loc[:, \"pixel0\":]\nmnist_train_label = mnist_train.loc[:, \"label\"]\n\n# Notmailzing the images array to be in the range of 0-1 by dividing them by the max possible value. \n# Here is it 255 as we have 255 value range for pixels of an image. \nmnist_train_data = mnist_train_data\/255.0\nmnist_test = mnist_test\/255.0","fac8a148":"# Let's make some beautiful plots.\ndigit_array = mnist_train.loc[3, \"pixel0\":]\narr = np.array(digit_array) \n\n#.reshape(a, (28,28))\nimage_array = np.reshape(arr, (28,28))\n\ndigit_img = plt.imshow(image_array, cmap=plt.cm.binary)\nplt.colorbar(digit_img)\nprint(\"IMAGE LABEL: {}\".format(mnist_train.loc[3, \"label\"]))","f9036429":"from sklearn.preprocessing import StandardScaler\n\nstandardized_scalar = StandardScaler()\nstandardized_data = standardized_scalar.fit_transform(mnist_train_data)\nstandardized_data.shape","74214cde":"cov_matrix = np.matmul(standardized_data.T, standardized_data)\ncov_matrix.shape","15a010f9":"from scipy.linalg import eigh\n\nlambdas, vectors = eigh(cov_matrix, eigvals=(782, 783))\nvectors.shape","874a18d5":"vectors = vectors.T\nvectors.shape","26aeddc3":"new_coordinates = np.matmul(vectors, standardized_data.T)\nprint(new_coordinates.shape)\nnew_coordinates = np.vstack((new_coordinates, mnist_train_label)).T","c8c8336d":"df_new = pd.DataFrame(new_coordinates, columns=[\"f1\", \"f2\", \"labels\"])\ndf_new.head()","35fa3675":"sns.FacetGrid(df_new, hue=\"labels\", size=6).map(plt.scatter, \"f1\", \"f2\").add_legend()\nplt.show()","1e2818fa":"from sklearn import decomposition\n\npca = decomposition.PCA()\npca.n_components = 2\npca_data = pca.fit_transform(standardized_data)\npca_data.shape","24a7185b":"pca_data = np.vstack((pca_data.T, mnist_train_label)).T","dfd75f82":"df_PCA = pd.DataFrame(new_coordinates, columns=[\"f1\", \"f2\", \"labels\"])\ndf_PCA.head()","ec63d94c":"sns.FacetGrid(df_new, hue=\"labels\", size=12).map(plt.scatter, \"f1\", \"f2\").add_legend()\nplt.savefig(\"PCA_FacetGrid.png\")\nplt.show()","091ed892":"pca.n_components = 784\npca_data = pca.fit_transform(standardized_data)\npercent_variance_retained = pca.explained_variance_ \/ np.sum(pca.explained_variance_)\n\ncum_variance_retained = np.cumsum(percent_variance_retained)","18204754":"plt.figure(1, figsize=(10, 6))\nplt.clf()\nplt.plot(cum_variance_retained, linewidth=2)\nplt.axis(\"tight\")\nplt.grid()\nplt.xlabel(\"number of compoments\")\nplt.ylabel(\"cumulative variance retained\")\nplt.savefig(\"pca_cumulative_variance.png\")\nplt.show()\n","8abb51fc":"# Let's build a count plot to see the count of all the labels.\nsns.countplot(mnist_train.label)\nprint(list(mnist_train.label.value_counts().sort_index()))","8855345a":"# Converting dataframe into arrays\nmnist_train_data = np.array(mnist_train_data)\nmnist_train_label = np.array(mnist_train_label)","9ab0c459":"# Reshaping the input shapes to get it in the shape which the model expects to recieve later.\nmnist_train_data = mnist_train_data.reshape(mnist_train_data.shape[0], 28, 28, 1)\nprint(mnist_train_data.shape, mnist_train_label.shape)","b0c0b1df":"# But first import some cool libraries before getting our hands dirty!! \n# TensorFlow is Google's open source AI framework and we are using is here to build model.\n# Keras is built on top of Tensorflow and gives us\n# NO MORE GEEKY STUFF, Know more about them here:  https:\/\/www.tensorflow.org     https:\/\/keras.io\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Lambda, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, AvgPool2D\nfrom tensorflow.keras.optimizers import Adadelta\nfrom keras.utils.np_utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import LearningRateScheduler","992b3c94":"# Encoding the labels and making them as the class value and finally converting them as categorical values.\nnclasses = mnist_train_label.max() - mnist_train_label.min() + 1\nmnist_train_label = to_categorical(mnist_train_label, num_classes = nclasses)\nprint(\"Shape of ytrain after encoding: \", mnist_train_label.shape)","678d3d8b":"# Warning!!! Here comes the beast!!!\n\ndef build_model(input_shape=(28, 28, 1)):\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = input_shape))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    model.add(Dense(10, activation='softmax'))\n    return model\n\n    \ndef compile_model(model, optimizer='adam', loss='categorical_crossentropy'):\n    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n    \n    \ndef train_model(model, train, test, epochs, split):\n    history = model.fit(train, test, shuffle=True, epochs=epochs, validation_split=split)\n    return history","b2a62454":"# Training the model using the above function built to build, compile and train the model\ncnn_model = build_model((28, 28, 1))\ncompile_model(cnn_model, 'adam', 'categorical_crossentropy')\n\n# train the model for as many epochs as you want but I found training it above 80 will not help us and eventually increase overfitting.\nmodel_history = train_model(cnn_model, mnist_train_data, mnist_train_label, 80, 0.2)","f40cbefa":"def plot_model_performance(metric, validations_metric):\n    plt.plot(model_history.history[metric],label = str('Training ' + metric))\n    plt.plot(model_history.history[validations_metric],label = str('Validation ' + metric))\n    plt.legend()\n    plt.savefig(str(metric + '_plot.png'))","cd24ec93":"plot_model_performance('accuracy', 'val_accuracy')","96f448e0":"plot_model_performance('loss', 'val_loss')","8305dde4":"# reshaping the test arrays as we did to train images above somewhere.\nmnist_test_arr = np.array(mnist_test)\nmnist_test_arr = mnist_test_arr.reshape(mnist_test_arr.shape[0], 28, 28, 1)\nprint(mnist_test_arr.shape)","78d5c659":"# Now, since the model is trained, it's time to find the results for the unseen test images.\npredictions = cnn_model.predict(mnist_test_arr)","70e54f8c":"# Finally, making the final submissions assuming that we have to submit it in any comptition. P)\npredictions_test = []\n\nfor i in predictions:\n    predictions_test.append(np.argmax(i))","a166261b":"submission =  pd.DataFrame({\n        \"ImageId\": mnist_test.index+1,\n        \"Label\": predictions_test\n    })\n\nsubmission.to_csv('my_submission.csv', index=False)","6efc5c1e":"### Prediction & Submission","c2a630fe":"#### b) Calculate covariance matrix S(dxd)","ac11bcf9":"### Transforming testing data","ca0c3cd2":"## Important Points:\n> Have a look at these points before we continue to read forther!\n* Kaggle Competition Link: https:\/\/www.kaggle.com\/c\/digit-recognizer\n* In this notebook I am using the MNIST Digits dataset which can be accessed from the link above.\n* About the dataset: The dataset consists of 10 classes of handwritten Images pictures each with a number between 0-9.\n* I hope you enjoy reading this notebook! \ud83d\ude03","71064c5a":"### Model Performance Analysis","bbc7e34a":"### 2) Using Sci-kit Learn library:","df637b96":"### Model Building Process\n> Training a neural network with one input layer, one hidden layer and one output layer for learning the digits in images.","4f9a4db2":"## PCA Dimension Reduction","034cbed2":"#### c) Calculate Eigen values and eigen vectors","973da9e6":"### Building a Sequential Model","f9be464d":"#### d) Calculate unit vectors U1=V1 and new coordinates","4d618267":"## Data Cleaning and Normalization","24e1814b":"### Visulaize a single digit with an array","427771b8":"## Import Libraries","2df6049f":"Read my other notebooks at:\nhttps:\/\/www.kaggle.com\/blurredmachine\/notebooks\n\nI hope you like it.<br>\nI am continuously working on this notebook to keep an easy approache for beginners to understand the concepts easily.<br>","0c542084":"### Consider upvoting if it was helpful! \ud83d\ude03","13e68e1b":"### Data Manipulation","0d040535":"#### NOTE:\n* Data is totally clean in this case (since the final result says `False` which means it has no missing values)\n* There is no empty field. Data is clean already.","88d294d3":"# PCA Implementation on MNIST Digits\n\n### 1) Using manual approach:","442b9171":"#### a) Compute standardization of data","7a1a9171":"# MNIST Handwritten Digits Classification\n\n> If you are new to Image processing and Deep Learning, then this notebook is for you!<br>\n> It will give you a basic insight about how to work with images and leave you with a good model to get upto 99.7% accurate results on MNIST handwritten images for unseen images.","dbb90ec3":"#### e) Plot FacetGrid using seaborn","6bbfca5b":"### Data Normalization","be7c81dc":"### Encoding train labels"}}