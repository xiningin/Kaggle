{"cell_type":{"f4ee2191":"code","55c77b0a":"code","5f38c285":"code","8e6247ea":"code","7d8265bc":"code","f6cac4b8":"code","51d5e5fb":"code","8f4d0f8c":"code","f5ed59a3":"code","08479c49":"code","abfb59e5":"code","08c18f1f":"code","0be52264":"code","9e487c1d":"markdown","928d121a":"markdown","0b8343ce":"markdown","b3506c61":"markdown"},"source":{"f4ee2191":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\n\nimport lightgbm as lgb\nimport xgboost as xgb","55c77b0a":"train = pd.read_csv(\"..\/input\/ykc-cup-1st\/train.csv\")\ntest = pd.read_csv(\"..\/input\/ykc-cup-1st\/test.csv\")","5f38c285":"train[\"prefecture\"] = train[\"area_name\"].str.split(expand=True)[0]\ntrain[\"city\"] = train[\"area_name\"].str.split(expand=True)[1]\ntrain[\"weekend\"] = ((train[\"day_of_week\"] == \"Sunday\") | (train[\"day_of_week\"] == \"Saturday\"))\n\n\ntest[\"prefecture\"] = test[\"area_name\"].str.split(expand=True)[0]\ntest[\"city\"] = test[\"area_name\"].str.split(expand=True)[1]\ntest[\"weekend\"] = ((test[\"day_of_week\"] == \"Sunday\") | (test[\"day_of_week\"] == \"Saturday\"))","8e6247ea":"train.head()","7d8265bc":"from category_encoders import TargetEncoder\n\nenc = TargetEncoder(cols=['store_id', 'day_of_week', 'genre_name', 'area_name', 'city', 'prefecture'])\n\ntrain_df = enc.fit_transform(train.drop([\"log_visitors\"], axis=1), train[\"log_visitors\"])\ntrain_df[\"log_visitors\"] = train[\"log_visitors\"]\ntest_df = enc.transform(test)","f6cac4b8":"from sklearn.model_selection import GroupShuffleSplit, StratifiedKFold, train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error","51d5e5fb":"def oof_model_preds(train, test, model, num_folds, params):\n    # Divide in training\/validation and test data\n    train_df = train\n    test_df = test\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n    del train\n    del test\n    gc.collect()\n\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n\n    drop_features = ['id', 'log_visitors']\n    feats = [f for f in train_df.columns if f not in drop_features]\n\n    # Create model\n    if num_folds == 1:\n        train_x, valid_x, train_y, valid_y = train_test_split(train_df[feats], train_df['log_visitors'], test_size=0.2, random_state=1001)\n        model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n            eval_metric= 'rmse', verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'])\n\n        oof_preds = model.predict(train_df[feats])\n        sub_preds = model.predict(test_df[feats])\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = model.feature_importances_\n        fold_importance_df[\"fold\"] = 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('RMAE : %.6f' % (mean_squared_error(train_df['log_visitors'], oof_preds, squared=False)))\n        del train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    # Cross validation model\n    elif num_folds > 1:\n        folds = GroupShuffleSplit(n_splits= num_folds, random_state=1001) #shuffle=True random_state=1001\n        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['log_visitors'], train_df['day_of_week'])):\n            train_x, train_y = train_df[feats].iloc[train_idx], train_df['log_visitors'].iloc[train_idx]\n            valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['log_visitors'].iloc[valid_idx]\n\n            model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n                eval_metric= 'rmse', verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'])\n\n            oof_preds[valid_idx] = model.predict(valid_x)\n            sub_preds += model.predict(test_df[feats]) \/ folds.n_splits\n\n            fold_importance_df = pd.DataFrame()\n            fold_importance_df[\"feature\"] = feats\n            fold_importance_df[\"importance\"] = model.feature_importances_\n            fold_importance_df[\"fold\"] = n_fold + 1\n            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n            print('Fold %2d RMAE : %.6f' % (n_fold + 1, mean_squared_error(valid_y, oof_preds[valid_idx], squared=False)))\n            del train_x, train_y, valid_x, valid_y\n            gc.collect()\n\n#     print('Full RMAE score %.6f' % mean_squared_error(train_df['log_visitors'], oof_preds, squared=False))\n    return oof_preds, sub_preds, feature_importance_df","8f4d0f8c":"params = {\n    'num_leaves': 72,\n    'learning_rate': 0.1,\n    'n_estimators': 300,\n    'max_depth':16,\n    'max_bin':55,\n    'bagging_fraction':0.8,\n    'bagging_freq':5,\n    'feature_fraction':0.9,\n    'verbose':50,\n    'early_stopping_rounds':100\n    }","f5ed59a3":"# LightGBM parameters\nlgbm_reg = lgb.LGBMRegressor(num_leaves=params['num_leaves'], learning_rate=params['learning_rate'], \n                    n_estimators=params['n_estimators'], max_depth=params['max_depth'],\n                    max_bin = params['max_bin'], bagging_fraction = params['bagging_fraction'], \n                    bagging_freq = params['bagging_freq'], feature_fraction = params['feature_fraction'],\n                   )\n\nlgb_oof_preds, lgb_sub_preds, lgb_feature_importance_df = oof_model_preds(train_df, test_df, lgbm_reg, num_folds=4, params=params)","08479c49":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=24, \n                             min_child_weight=1.7817, n_estimators=3200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1) \n\nxgb_oof_preds, xgb_sub_preds, xgb_feature_importance_df = oof_model_preds(train_df, test_df, model_xgb, num_folds=5, params=params)","abfb59e5":"import seaborn as sns\nsns.set(style='white', context='notebook', palette='Set2')\nimport matplotlib.pyplot as plt\n\ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances.png')","08c18f1f":"display_importances(lgb_feature_importance_df)","0be52264":"sub = pd.DataFrame()\nsub['id'] = test_df['id']\nsub['log_visitors'] = lgb_sub_preds * 0.5 + xgb_sub_preds * 0.5\n\nsub.to_csv('lgb_submission.csv',index=False)","9e487c1d":"## model\n\noptimize\nhttps:\/\/github.com\/optuna\/optuna","928d121a":"## feature","0b8343ce":"\u3084\u308b\u3053\u3068\nstore x \u66dc\u65e5 \u3067\u3001log(\u6765\u5ba2\u6570)\u3092\u4e88\u6e2c\n\nEDA https:\/\/www.kaggle.com\/sorashido\/yck-cup-eda\n\n\n1. \u9069\u5f53\u306b\u7279\u5fb4\u91cf\u3092\u8db3\u3059\n2. \u9069\u5f53\u306b\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\n3. \u9069\u5f53\u306b\u5206\u6790\u3059\u308b","b3506c61":"## categorical features"}}