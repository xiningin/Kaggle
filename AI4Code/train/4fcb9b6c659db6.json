{"cell_type":{"c6913aac":"code","e92053a2":"code","8a8e1ae6":"code","fb533eb3":"code","02ce1ed4":"code","0e86cfe3":"code","8ea27b69":"code","bf22638d":"code","a8149651":"code","a4a797a7":"code","228f2cc3":"code","e11e9a91":"code","594e1f60":"code","26246174":"code","b0a70852":"code","d056a1ed":"code","b78dfefc":"code","dbc81d8c":"code","88967121":"code","98816dbe":"code","de848421":"markdown","49621328":"markdown","31980b5f":"markdown","121ad256":"markdown","5da9398e":"markdown"},"source":{"c6913aac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e92053a2":"test=pd.read_csv(\"\/kaggle\/input\/anomaly-detection\/test.csv\")\ntrain=pd.read_csv(\"\/kaggle\/input\/anomaly-detection\/train.csv\")","8a8e1ae6":"train.head() ","fb533eb3":"train.shape","02ce1ed4":"plt.figure(figsize = (30, 25))\nsns.heatmap(train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","0e86cfe3":"#duplicated value\ntrain.duplicated().sum()","8ea27b69":"train.isna().sum()","bf22638d":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","a8149651":"label=train['is_anomaly']\ndata=train.drop(['is_anomaly'],axis=1)\nprint(data.shape)","a4a797a7":"label.value_counts()","228f2cc3":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nx_train1,x_test1,train_label,test_label=train_test_split(data,label,test_size=0.50, random_state=1)\n\nfrom xgboost import XGBClassifier\nclf=XGBClassifier(learning_rate=1, n_estimators=1300,eta = 0.7, \n    max_depth= 3,  \n    objective= 'multi:softprob',  \n    num_class= 3)\n\n# gamma auto uses 1 \/ n_features\nimport time\ndebut=time.time()\n#fit permet de faire le train de modele \nclf.fit(x_train1,train_label)\nfin=time.time()-debut\n#pred predection selon database\npred=clf.predict(test)","e11e9a91":"\"\"\"# check version number\nimport imblearn\nprint(imblearn.__version__)\n\nfrom imblearn.over_sampling import RandomOverSampler\n\n# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy='minority')\n\n# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy=1.0)\n\n# fit and apply the transform\nX_over, y_over = oversample.fit_resample(data, label)\n\n#compter combien de 1 et de 0 dans dataset\ny_over.value_counts()\"\"\"\n","594e1f60":"\"\"\"data=X_over\nlabel=y_over\"\"\"","26246174":"\"\"\"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\ndtree = DecisionTreeClassifier()\nx_train,x_test,y_train,y_test = train_test_split(data,label,test_size = 0.3,random_state = 0)\nimport time\ndebut=time.time()\ndtree.fit(x_train,y_train)\nfin=time.time()-debut\nprediction = dtree.predict(test)\"\"\"","b0a70852":"\"\"\"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nx_train1,x_test1,train_label,test_label=train_test_split(data,label,test_size=0.33,random_state=0)\n\nfrom xgboost import XGBClassifier\nclf=XGBClassifier(learning_rate=1, n_estimators=1300,eta = 0.7, \n    max_depth= 3,  \n    objective= 'multi:softprob',  \n    num_class= 3)\n\n# gamma auto uses 1 \/ n_features\nimport time\ndebut=time.time()\n#fit permet de faire le train de modele \nclf.fit(x_train1,train_label)\nfin=time.time()-debut\n#pred predection selon database\npred=clf.predict(test)\"\"\"","d056a1ed":"\"\"\"from sklearn.metrics import confusion_matrix\nCM=confusion_matrix(test_label,pred)\nprint(CM)\n\n#heatmap de confusion matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nclass_names=[0,1] # name of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\n# CM1=pd.DataFrame(CM)\n# print(CM1)\nsns.heatmap(pd.DataFrame(CM), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\n#plt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\"\"\"","b78dfefc":"\"\"\"from sklearn.metrics import classification_report\nprint(classification_report(test_label,pred))\"\"\"","dbc81d8c":"data={\"timestamp\":[],\"is_anomaly\":[]}\nfor id,pred in zip(test[\"timestamp\"].unique(),pred):\n  data[\"timestamp\"].append(id)\n  data[\"is_anomaly\"].append(pred)\n","88967121":"output=pd.DataFrame(data,columns=[\"timestamp\",\"is_anomaly\"])\noutput","98816dbe":"output.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\noutput['is_anomaly'].value_counts()","de848421":"# oversamling technique","49621328":"# 1. understanding the dataset","31980b5f":"# Cleaning data","121ad256":"# XGBoost","5da9398e":"**===> unbalanced data**"}}