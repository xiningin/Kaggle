{"cell_type":{"63935346":"code","24d34fe9":"code","50236526":"code","677d1f80":"code","55a04358":"code","5bcc0427":"code","8ebdedc6":"code","bebd77ce":"code","ed963e92":"code","37d6b9be":"code","46daf7d2":"markdown","8076d3e1":"markdown","55661077":"markdown","06e4efae":"markdown","728da650":"markdown","e4969c7a":"markdown","cf9e45ea":"markdown","c057d1a5":"markdown","7be7c1b5":"markdown","4860181a":"markdown","c1565e50":"markdown"},"source":{"63935346":"# Familiar imports\nimport numpy as np\nimport pandas as pd\nfrom numpy import mean\nfrom numpy import std\nimport time\nfrom matplotlib import pyplot\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n# evaluate lightgbm ensemble for regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\n# REF: https:\/\/machinelearningmastery.com\/light-gradient-boosted-machine-lightgbm-ensemble\/","24d34fe9":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","50236526":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","677d1f80":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","55a04358":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# Define the model\nmy_model_1 = XGBRegressor(random_state=0) # Your code here\n\n# Fit the model\nmy_model_1.fit(X_train,y_train)","5bcc0427":"# Get predictions\npredictions_1 = my_model_1.predict(X_valid)\n\n# Calculate RMSE\nfrom sklearn.metrics import mean_squared_error\nrms = mean_squared_error(y_valid,predictions_1, squared=False)\n\n# print RMSE\nprint(\"Root Mean Square Error:\" , rms)","8ebdedc6":"# Define the model\nmy_model_2 = XGBRegressor(n_estimators=1000, learning_rate=0.05) \n\n# Fit the model\nmy_model_2.fit(X_train, y_train) \n\n# Get predictions\npredictions_2 = my_model_2.predict(X_valid) \n\n# Calculate RMSE\nrms2 = mean_squared_error(y_valid,predictions_2, squared=False) \n\n# print RMSE\nprint(\"Root Mean Square Error:\" , rms2)","bebd77ce":"my_model_3 = XGBRegressor(n_estimators=1000, learning_rate=0.01)\nmy_model_3.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\n\n# Get predictions\npredictions_3 = my_model_3.predict(X_valid) \n\n# Calculate RMSE\nrms3 = mean_squared_error(y_valid,predictions_3, squared=False) \n\n# print RMSE\nprint(\"Root Mean Square Error:\" , rms3)","ed963e92":"# define the final model\nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n\n# Train the final model with whole dataset\nmodel.fit(X, y)","37d6b9be":"# Use the model to generate predictions\npredictions = model.predict(X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","46daf7d2":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nIf you took the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** courses, then you learned about **[Random Forests](https:\/\/www.kaggle.com\/dansbecker\/random-forests)**.  In the code cell below, we fit a random forest model to the data.","8076d3e1":"#### Improve model by tuning parameters","55661077":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","06e4efae":"Welcome to the **[30 Days of ML competition](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview)**!  In this notebook, you'll learn how to make your first submission.\n\nBefore getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n\n# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course.","728da650":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","e4969c7a":"Next, we break off a validation set from the training data.","cf9e45ea":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`.","c057d1a5":"#### Now we will create final LightGBM model based on above observations:","7be7c1b5":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course, then you learned about **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset.","4860181a":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","c1565e50":"# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file."}}