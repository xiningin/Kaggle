{"cell_type":{"646a89ff":"code","941ef764":"code","a404375c":"code","5eff47bc":"code","e68c0b45":"code","acd8de03":"code","61e5f9cd":"code","b3de85f3":"code","8710c8f8":"code","f73815a5":"code","672b9c79":"code","3740111c":"code","680e1dc0":"code","51a0355a":"code","5987f635":"code","c31ddbe5":"code","596151c4":"code","97b86beb":"code","bf94dc6a":"code","482d72bd":"code","5fee1e29":"code","a3567462":"code","fcfc81b4":"code","ddaaa370":"code","e865c1bf":"code","8bf29fc7":"code","7d101c8e":"code","0370c9d0":"markdown","ad015c30":"markdown","cb0586a2":"markdown","21dfda0a":"markdown","5e18c3cb":"markdown","c39fce42":"markdown","d94e6d49":"markdown","59c9ba3f":"markdown","baff0e24":"markdown","06c7265d":"markdown","8f4e85bc":"markdown","0737c770":"markdown","afc38bdf":"markdown","3da9866a":"markdown","082daf2a":"markdown","9be682d9":"markdown","2a2aeddb":"markdown","77bf37a6":"markdown","b8ac7f43":"markdown","83821efe":"markdown","c0ceea65":"markdown","57837ab2":"markdown","6f3daaeb":"markdown","486f5f4e":"markdown","672cb99a":"markdown","4c90583d":"markdown","628fe149":"markdown","92a31ba2":"markdown","010a46ae":"markdown","b9515ac8":"markdown","b9f9391d":"markdown","7f94dded":"markdown","396e47ea":"markdown","47105e7c":"markdown","37906630":"markdown","031b8a5c":"markdown","f7904831":"markdown","43317eed":"markdown","c10e985b":"markdown","3144185e":"markdown","5f4967b2":"markdown","c78fff98":"markdown"},"source":{"646a89ff":"import numpy as np \nimport pandas as pd \nimport os\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport matplotlib_venn as venn\nimport seaborn as sns\n\n\nfrom tqdm import tqdm\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nfrom collections import defaultdict\nfrom collections import  Counter\n\n\n# sklearn \nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n#nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nstop=set(stopwords.words('english'))\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\n\n#Avoid warning messages\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#plotly libraries\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.subplots import make_subplots\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\n\nfrom datetime import datetime as dt\nimport math","941ef764":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\nActual_1 = 'Twitter Sentiment Analysis'\nPredict_1 = 'Sentiment Analysis'\n\nActual_2 = 'Twitter Sentiment Analysis'\nPredict_2 = 'Analysis'\n    \nprint(\"Jaccard score for first set of scentences: {}\".format(jaccard(Actual_1,Predict_1)))\nprint(\"Jaccard score for second set of scentences: {}\".format(jaccard(Actual_2,Predict_2)))","a404375c":"ori_train=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\nori_test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\n\ntrain = ori_train\ntest = ori_test\n\nori_train = ori_train.fillna(\"\")\nori_test  = ori_test.fillna(\"\")\n\ntrain.head(10)\n\nprint(\"There are {} rows and {} columns in train file\".format(train.shape[0],train.shape[1]))\nprint(\"There are {} rows and {} columns in test file\".format(test.shape[0],test.shape[1]))","5eff47bc":"print(train.shape[0])\nprint(train.info())","e68c0b45":"train = ori_train\ntest = ori_test","acd8de03":"# count unique values present in each column\ndef count_values(df,feature):\n    total=df.loc[:,feature].value_counts(dropna=False)\n    percent=round(df.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n    return pd.concat([total,percent],axis=1,keys=['Total','Percent'])","61e5f9cd":"sent_train=count_values(train,'sentiment')\ncolors = ['orange','green','red']\n\nfig = make_subplots(rows=1, cols=1,specs=[[{\"type\": \"pie\"}]])\n\nfig.add_trace(go.Pie(labels=list(sent_train.index), values=list(sent_train.Total.values), hoverinfo='label+percent', \n               textinfo='value+percent',marker=dict(colors=colors)),row=1,col=1)\n\nfig.update_layout( title_text=\"Positive Vs Negative Vs Neutral Counts\",title_x=0.5)\niplot(fig)","b3de85f3":"sent=train.sentiment.unique()\nfig,ax= plt.subplots(1,3,figsize=(12,6))\n\nfor i in range(0,3):\n    sns.distplot(train[train['sentiment']==sent[i]]['text'].str.split().str.len(), hist=True, kde=True, \n             bins=int(180\/5), color = colors[i], \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4}, ax = ax[i])\n    ax[i].set_title(sent[i])\n    ax[i].grid(False)\nfig.suptitle(\"Distribution of number of No: Words in Tweets\", fontsize=14)\n\nsent=train.sentiment.unique()\nfig,ax= plt.subplots(1,3,figsize=(12,6))\nplt.grid(b=None)\nfor i in range(0,3):\n    sns.distplot(train[train['sentiment']==sent[i]]['selected_text'].str.split().str.len(), hist=True, kde=True, \n             bins=int(180\/5), color = colors[i], \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4}, ax = ax[i])\n    ax[i].set_title(sent[i])\n    ax[i].grid(False)\nfig.suptitle(\"Distribution of number of No: Words in Selected text\", fontsize=14)","8710c8f8":"from IPython.display import HTML\n\ndef hover(hover_color=\"#ffff99\"):\n    return dict(selector=\"tr:hover\",\n                props=[(\"background-color\", \"%s\" % hover_color)])","f73815a5":"# Find URL\ndef find_link(string): \n    url = re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', string)\n    return \"\".join(url) \ntrain['target_url']=train['selected_text'].apply(lambda x: find_link(x))\ndf2=pd.DataFrame(train.loc[train['target_url']!=\"\"]['sentiment'].value_counts()).reset_index()\ndf2.rename(columns={\"index\": \"sentiment\", \"sentiment\": \"url_count\"},inplace = True)\n\nstyles = [\n    hover(),\n    dict(selector=\"th\", props=[(\"font-size\", \"150%\"),\n                               (\"text-align\", \"center\")]),\n    dict(selector=\"caption\", props=[(\"caption-side\", \"bottom\")])\n]\nhtml = (df2.style.set_table_styles(styles)\n          .set_caption(\"Hover to highlight.\"))\nhtml","672b9c79":"def create_corpus_text(target): \n    corpus=[]\n    for x in train[train['sentiment']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","3740111c":"corpus=create_corpus_text(\"positive\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop_0=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:20]\n\n#################################################\n\ncorpus=create_corpus_text(\"negative\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop_1=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:20]\n\n\n#################################################\n\ncorpus=create_corpus_text(\"neutral\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop_2=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:20]","680e1dc0":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nx0,y0=zip(*top_0)\nx1,y1=zip(*top_1)\nx2,y2=zip(*top_2)\nplt.bar(x0,y0, color=['green'], label = \"positive\")           \nplt.bar(x1,y1, color=['red'], label = \"negative\")              \nplt.bar(x2,y2, color=['orange'], label = \"neutral\")\nplt.legend()\n\n","51a0355a":"def create_corpus_selected_text(target): \n    corpus=[]\n    for x in train[train['sentiment']==target]['selected_text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","5987f635":"corpus=create_corpus_selected_text(\"positive\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop_0_selected_text=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:20]\n\n##################################################\n\ncorpus=create_corpus_selected_text(\"negative\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop_1_selected_text=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:20]\n\n#################################################\n\ncorpus=create_corpus_selected_text(\"neutral\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop_2_selected_text=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:20]","c31ddbe5":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nx0,y0=zip(*top_0_selected_text)\nx1,y1=zip(*top_1_selected_text)\nx2,y2=zip(*top_2_selected_text)\nplt.bar(x0,y0, color=['green'], label = \"positive\")           \nplt.bar(x1,y1, color=['red'], label = \"negative\")              \nplt.bar(x2,y2, color=['orange'], label = \"neutral\")\nplt.legend()","596151c4":"d = '..\/input\/newwordcloud\/'\npositive = np.array(Image.open(d + 'positive.jpg'))\nnegative = np.array(Image.open(\"..\/input\/negativeimage\/negative.jpeg\"))\nneutral = np.array(Image.open(d + 'neutral.jpg'))\nfig, ((ax1, ax2, ax3),(ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',mask=positive,colormap=\"Greens\",\n                        width=800,\n                        height=600).generate(\" \".join(train.loc[train['sentiment']==\"positive\"]['text']))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive Tweet Text',fontsize=35);\n\nwordcloud1_selected_text = WordCloud( background_color='white',mask=positive,colormap=\"Greens\",\n                        width=800,\n                        height=600).generate(\" \".join(train.loc[train['sentiment']==\"positive\"]['selected_text']))\nax4.imshow(wordcloud1_selected_text)\nax4.axis('off')\nax4.set_title('Positive Selected Text',fontsize=35);\n\nwordcloud2 = WordCloud( background_color='white',mask=negative,colormap=\"Reds\",\n                        width=800,\n                        height=600).generate(\" \".join(train.loc[train['sentiment']==\"negative\"]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative Tweet Text',fontsize=35);\n\n\nwordcloud2_selected_text = WordCloud( background_color='white',mask=negative,colormap=\"Reds\",\n                        width=800,\n                        height=600).generate(\" \".join(train.loc[train['sentiment']==\"negative\"]['selected_text']))\nax5.imshow(wordcloud2_selected_text)\nax5.axis('off')\nax5.set_title('Negative Selected Text',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask=neutral,colormap=\"Oranges\",\n                        width=800,\n                        height=600).generate(\" \".join(train.loc[train['sentiment']==\"neutral\"]['text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral Tweet Text',fontsize=35);\n\nwordcloud3_selected_text = WordCloud( background_color='white',mask=neutral,colormap=\"Oranges\",\n                        width=800,\n                        height=600).generate(\" \".join(train.loc[train['sentiment']==\"neutral\"]['selected_text']))\nax6.imshow(wordcloud3_selected_text)\nax6.axis('off')\nax6.set_title('Neutral Selected Text',fontsize=35);","97b86beb":"print('TF version',tf.__version__)","bf94dc6a":"MAX_LEN = 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nEPOCHS = 3\nBATCH_SIZE = 32\nPAD_ID = 1\nSEED = 88888\nLABEL_SMOOTHING = 0.1\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = ori_train\ntrain.head()","482d72bd":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1","5fee1e29":"test = ori_test\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask_t[k,:len(enc.ids)+3] = 1","a3567462":"import pickle\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss","fcfc81b4":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n    \n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n    x1 = tf.keras.layers.ReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n    x2 = tf.keras.layers.ReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n    \n    #x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    #x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    #x1 = tf.keras.layers.Flatten()(x1)\n    #x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    #x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    #x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    #x2 = tf.keras.layers.Flatten()(x2)\n    #x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss=loss_fn, optimizer=optimizer)\n    \n    # this is required as `model.predict` needs a fixed size!\n    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    return model, padded_model","ddaaa370":"jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=SEED)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model, padded_model = build_model()\n        \n    #sv = tf.keras.callbacks.ModelCheckpoint(\n    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n    #    save_weights_only=True, mode='auto', save_freq='epoch')\n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    for epoch in range(1, EPOCHS + 1):\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) \/ BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n        save_weights(model, weight_fn)\n\n    print('Loading model...')\n    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    load_weights(model, weight_fn)\n\n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/skf.n_splits\n    preds_end += preds[1]\/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-2:b-1])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","e865c1bf":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","8bf29fc7":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n    all.append(st)","7d101c8e":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","0370c9d0":"<font size=\"+2\" color=\"blue\"><b>7.3 Is selected_text word count different from original tweet text word count ?<\/b><\/font><br>","ad015c30":"> ## CONCLUSION : As we can clearly see that the distribution of words counts for text & selected_text for neutral tweets is identical which means that all the text is taken into selected_text for neutral tweets.Whereas there is difference in the words counts distribution plots of positive & negative tweets. ","cb0586a2":"> ### Printing stop words for Selected Texts","21dfda0a":"<font size=\"+2\" color=\"maroon\"><b>6. Loading & Describing the Dataset<\/b><\/font><br>\n","5e18c3cb":"> ## CONCLUSION : Neutral tweets have most of the stop words in both tweet texts & selected_texts as it clearly visible in the above bar plot. Equally high number of stop word in selected_texts is expected because in case of neutral tweets selected_texts~tweet texts. One more thing to note here is that the selected_text for positive & negative tweets contains only few stop words which is also expected because the selected_texts for + & - tweets contain words describing the sentiments which mostly doesn't belong to any stop words.","c39fce42":"| Columns       |      Description          | \n|---------------|:-------------------------:|\n| ID            |  Unique ID for each tweet |       \n| Text          |  Whole content of tweet   |   \n| Selected Text |  Selected Text of tweet   |    \n| Sentiment     |  Sentiment of tweet       |","d94e6d49":"> ## CONCLUSION : Since only one row values are missing so we will fill \"\" in place of that.\n\n*What if a large number of rows contain missing values? - In that case we cannot simply delete the missing rows because that will cause loss of data which was present in some other non-missing columns.In this case we would have to think about replacing those missing values with some appropriate values derived after observing the behviour of dataset.*","59c9ba3f":"<font size=\"+2\" color=\"lime\"><b>7.2 Imbalanced dataset Check<\/b><\/font><br>","baff0e24":"> ### Printing stop words for Tweet Texts","06c7265d":"<font size=\"+2\" color=\"teal\"><b>4. Importing relevant Libraries<\/b><\/font><br>","8f4e85bc":"> ### CREATING DICTIONARY FOR TWEET TEXTS","0737c770":"![image.png](attachment:image.png)","afc38bdf":"<font size=\"+2\" color=\"orange\"><b>7.4 Are URLs occuring all in all the three types of tweets<\/b><\/font><br>","3da9866a":"<font size=\"+2\" color=\"orange\"><b>2. Some important study material links related to this project<\/b><\/font><br>\n\n\nExplanation of the architecture of RoBERTa model - https:\/\/www.youtube.com\/watch?v=6a6L_9USZxg\n\nhttps:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds\n","082daf2a":"> ## Loading Tokenizer","9be682d9":"> ## CONCLUSION : Neutral tweets are appearing more in the datset followed by positive & negative tweets. Positive & Negative are almost equally probable. But the difference is not that large to classify this as an imbalanced dataset.So we will keep the dataset as it is.\n\n*What if we had imbalanced dataset : We then had to implement techniques like upsampling, creating synthetic points(Data Augmentation),etc. Downsampling majority class is generally not recommended because it results into loss of dataset which ultimately results into loss of information.*","2a2aeddb":"# \u2714\ufe0fPLEASE GIVE THIS NOTEBOOK AN UPVOTE IF YOU LIKED IT!!!","77bf37a6":"> ### Creating corpus of all the selected_text words in our dataset","b8ac7f43":"> ### Creating corpus of all the tweet text words in our dataset","83821efe":"> ## CONCLUSION : As we can see out of total 351 tweets containing URLs, 345 of them belong to neutral tweets. So it's a clear indication that tweets containing URLs will be mostly a neutral tweet & we can use the URL as a selected_text to justify this sentiment. ","c0ceea65":"<font size=\"+2\" color=\"indigo\"><b>1. General Advice<\/b><\/font><br>\n\n* If you are not familiar with some of the advanced NLP based models like RoBERTa then I would strongly recommend you to first visit the references that I have provided below. After that it would be very easy for you to understand the RoBERTa code that I have written in this notebook.\n \n* Ensure that you have enabled GPU accelerator for your notebook at the time of running your notebook for faster performance. You can enable it by clicking on the arrow appearing on the right of \"Save Version\" button. \n","57837ab2":"> ### Function to search for presence of URL in a text","6f3daaeb":"> ## Defining the architecuture of RoBERTa model\n\n","486f5f4e":"> ## Tokenizing test data as well","672cb99a":"<font size=\"+2\" color=\"green\"><b>3. References of some other notebooks used in this project<\/b><\/font><br>\n\n\n1. For EDA - https:\/\/www.kaggle.com\/raenish\/tweet-sentiment-insight-eda , https:\/\/towardsdatascience.com\/exploratory-data-analysis-8fc1cb20fd15\n\n2. For Word Clouds - https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes\n\n3. For RoBERTa - https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705 , https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds\n\n4. How to make RoBERTa run 2X faster - https:\/\/www.kaggle.com\/seesee\/faster-2x-tf-roberta\n","4c90583d":"> ### Function to create hovering effect in a dataframe. Just for fun.","628fe149":"> Jaccard Similarity -  It\u2019s a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations. You can relate it with the concept of substring matching. \n\n![image.png](attachment:image.png)","92a31ba2":"> ## How to submit the file","010a46ae":"<font size=\"+2\" color=\"red\"><b>7. EDA<\/b><\/font><br>\n\n> Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.\n\n*It is a good practice to understand the data first and try to gather as many insights from it. EDA is all about making sense of data in hand,before getting them dirty with it.*\n\n![image.png](attachment:image.png)","b9515ac8":"# *I would highly encourage you to go throught the reference links provided in the starting of this notebook to understand RoBERTa's model architecture, I\/O format etc.*","b9f9391d":"<font size=\"+2\" color=\"purple\"><b>7.1 Missing Values Check<\/b><\/font><br>","7f94dded":"> I have added some layers in addition to what you will see in most of the kernels to check whether this improves model's performance or not. And guess what. It worked.","396e47ea":"<font size=\"+2\" color=\"blue\"><b>Table of Content<\/b><\/font><br>\n1.  [ General Advice](#1)\n1.  [ Some important study material links related to this project](#2) \n1.  [ References of some other notebooks used in this project](#3)\n1.  [ Importing relevant Libraries](#4)\n1.  [ Evaluation Metric](#5)\n1.  [ Loading & Describing the Dataset](#6)\n1.  [ EDA](#7)\n       - [7.1 Missing Values Check](#7.1)\n       - [7.2 Imbalanced dataset Check](#7.2)\n       - [7.3 Is selected_text word count different from original tweet text word count ?](#7.3)\n       - [7.4 Are URLs occuring all in all the three types of tweets](#7.4)\n       - [7.5 Frequency of stop words in each category of tweets](#7.5)\n       - [7.6 Most frequently occuring words in each types of tweets](#7.6)\n       \n1.  [ Applying RoBERTa Model](#8)      \n","47105e7c":"> ## Training Data\n\n*We will have to convert the input into a way that RoBERTa model understands. Refer to the below example for reference.*\n\n![image.png](attachment:image.png)","37906630":"> ## Functions to save\/load model's weights","031b8a5c":"<font size=\"+2\" color=\"navy\"><b>7.6 Most frequently occuring words in each types of tweets<\/b><\/font><br>","f7904831":"<font size=\"+2\" color=\"coral\"><b>5. Evaluation Metric<\/b><\/font><br>","43317eed":"> ## Training RoBERTa model","c10e985b":"<font size=\"+2\" color=\"coral\"><b>7.5 Frequency of stop words in each category of tweets<\/b><\/font><br>","3144185e":"<font size=\"+2\" color=\"maroon\"><b>8 Applying RoBERTa Model<\/b><\/font><br>","5f4967b2":"> ### CREATING DICTIONARY FOR SELECTED_TEXTS","c78fff98":"> ## CONCLUSION : Words like \"love\", \"good\" ,\"happy\" appear in positive tweets whereas negative words like \"hate\", \"miss\", \"sick\" appears in negatvie tweets and general words like \"now\",\"going\",\"still\" appear in neutral type of tweets. All as expected.\n> \n*Why are we drawing Word Clouds : Generally we draw word clouds to have an idea about the most common words appearing in the document(tweets in this case). In reference to this dataset, it also gives an idea about the words people are most commonly using in all the three types of tweets.*"}}