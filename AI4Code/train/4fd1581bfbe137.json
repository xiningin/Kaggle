{"cell_type":{"e6090cf0":"code","e6854b70":"code","4cf2c9f3":"code","36d91ae5":"code","8b0c7320":"code","8bc17088":"code","9b925298":"code","6851d53c":"code","21224b4f":"code","5853c341":"code","f027d35a":"code","abc44022":"code","e30760ea":"code","72140189":"code","46748289":"code","23ed9d9e":"code","eeab6de7":"code","c9374f4e":"code","6e15686a":"code","f81909ef":"code","5629b186":"code","76b78f41":"code","a247c68d":"code","048ae914":"code","b81b77f8":"code","db70b106":"code","02d33211":"code","38b7c6cc":"code","4dab217e":"code","4ab8812d":"code","e87eb3dc":"code","6520f4c8":"code","69c02959":"code","79b2a4d6":"code","69d424ff":"code","be26a0f4":"code","8d2ffc71":"code","6df5b5fe":"code","9a248228":"code","e42feff2":"code","7a260fb4":"code","e6c11677":"code","28dc11de":"code","bd2ec42e":"code","e8e88fbf":"code","f462eab9":"code","0dc7e30a":"code","9d0ba733":"code","c2549534":"code","056b0632":"code","076d18c3":"code","749ac8e7":"code","fbc51c7f":"code","3f502a68":"code","fb2e618d":"code","472bf4f9":"code","5a5d6aff":"markdown","e15719b6":"markdown","6db386e1":"markdown","2c952a1f":"markdown","5bb33eef":"markdown","4efe732b":"markdown"},"source":{"e6090cf0":"import numpy as np # linear algebra\nimport pandas as pd\nimport cv2\nimport os\nimport math\nimport copy\nimport imageio\nimport seaborn as sns\nfrom glob import glob\nfrom pathlib import Path\nfrom collections import deque\nimport matplotlib.pyplot as plt\nfrom keras.utils import np_utils\nfrom IPython.display import Image\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split","e6854b70":"import seaborn as sns\nimport plotly.express as px\nfrom IPython.display import SVG\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n%matplotlib inline","4cf2c9f3":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.datasets import mnist\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.losses import categorical_crossentropy\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.applications import VGG16,inception_v3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\nfrom tensorflow.keras.layers import Input, Lambda,Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                                    Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\n                                    LSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape,\\\n                                    Conv2DTranspose, LeakyReLU, Conv1D, AveragePooling1D, MaxPooling1D,Activation, Conv3D,MaxPooling3D\n\n\n","36d91ae5":"Main_Video_Path = Path(\"..\/input\/real-life-violence-situations-dataset\/Real Life Violence Dataset\")\nVideo_Path = list(Main_Video_Path.glob(r\"*\/*.mp4\"))\nVideo_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"MP4\").astype(str)\nVideo_Labels_Series = pd.Series(Video_Labels,name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","8b0c7320":"Violence_Data = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"Violence\"]\nNonViolence_Data = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"NonViolence\"]\n\nViolence_Data = Violence_Data.reset_index()\nNonViolence_Data = NonViolence_Data.reset_index()","8bc17088":"Violence_Data","9b925298":"Main_Video_Path = Path(\"..\/input\/violencedetectionsystem\")\nVideo_Path = list(Main_Video_Path.glob(r\"*\/*.mp4\"))\nVideo_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"MP4\").astype(str)\nVideo_Labels_Series = pd.Series(Video_Labels,name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","6851d53c":"Main_MP4_Data[\"CATEGORY\"].replace({'fight':'Violence','noFight':'NonViolence'}, inplace=True)\nVD = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"Violence\"]\nNVD = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"NonViolence\"]\n\n","21224b4f":"Violence_Data = Violence_Data.append(VD,ignore_index=True, sort=False)\nNonViolence_Data = NonViolence_Data.append(NVD,ignore_index=True, sort=False)","5853c341":"Violence_Data","f027d35a":"NonViolence_Data","abc44022":"Main_Video_Path3 = Path(\"..\/input\/ucf-crime-full\/Normal_Videos_for_Event_Recognition\")\nVideo_Path = list(Main_Video_Path3.glob(r\"*.mp4\"))\n# Video_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"mp4\").astype(str)\nVideo_Labels_Series = pd.Series('NonViolence',name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","e30760ea":"Main_MP4_Data['CATEGORY'] = Main_MP4_Data['CATEGORY'].fillna(\"NonViolence\")","72140189":"Main_MP4_Data.rename(columns = {'mp4':'MP4'}, inplace = True)","46748289":"Main_MP4_Data","23ed9d9e":"NonViolence_Data = NonViolence_Data.append(Main_MP4_Data,ignore_index=True, sort=False)","eeab6de7":"NonViolence_Data","c9374f4e":"Main_Video_Path3 = Path(\"..\/input\/ucf-crime-full\/Fighting\")\nVideo_Path = list(Main_Video_Path3.glob(r\"*.mp4\"))\n# Video_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"mp4\").astype(str)\nVideo_Labels_Series = pd.Series('Violence',name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","6e15686a":"Main_MP4_Data['CATEGORY'] = Main_MP4_Data['CATEGORY'].fillna(\"Violence\")","f81909ef":"Main_MP4_Data.rename(columns = {'mp4':'MP4'}, inplace = True)","5629b186":"Main_MP4_Data","76b78f41":"Violence_Data = Violence_Data.append(Main_MP4_Data,ignore_index=True, sort=False)","a247c68d":"Violence_Data","048ae914":"NonViolence_Data","b81b77f8":"\nCdata = NonViolence_Data","db70b106":"Cdata = Cdata.append(Violence_Data,ignore_index=True, sort=False)","02d33211":"Cdata","38b7c6cc":"Cdata[\"CATEGORY\"].replace({'Violence':1,'NonViolence':0}, inplace=True)","4dab217e":"# Cdata.CATEGORY[Cdata.CATEGORY == 'Violence']","4ab8812d":"Violence_Data.MP4[1]","e87eb3dc":"! mkdir .\/Frames\n!cd .\/Frames; mkdir .\/violence; mkdir .\/nonviolence","6520f4c8":"violence_frame_list = []\nv = 0\nfor file_video in Violence_Data.MP4:\n    Video_File_Path = file_video\n    \n    Video_Caption = cv2.VideoCapture(Video_File_Path)\n    Frame_Rate = 15\n    count = 0\n    temp = []\n    if Video_Caption.isOpened():\n        os.mkdir('.\/Frames\/violence\/v{}'.format(v)) \n    while Video_Caption.isOpened():\n        \n        Current_Frame_ID = Video_Caption.get(1)\n        \n        ret,frame = Video_Caption.read()\n        \n        if ret != True:\n            break\n            \n        if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n            image = cv2.resize(frame,(256,256))\n            cv2.imwrite(\".\/Frames\/violence\/v{}\/frame{}.jpg\".format(v,count), image)\n            violence_frame_list.append([\".\/Frames\/violence\/v{}\/frame{}.jpg\".format(v,count),1])\n            count += 1\n    v += 1\n        \n    Video_Caption.release()\n    \nlen(violence_frame_list)","69c02959":"sorted(os.listdir(\".\/Frames\/nonviolence\"))","79b2a4d6":"non_violence_frame_list = []\nv = 0\nfor file_video in NonViolence_Data.MP4:\n    Video_File_Path = file_video\n    \n    Video_Caption = cv2.VideoCapture(Video_File_Path)\n    Frame_Rate = 10\n    count = 0\n    if Video_Caption.isOpened():\n        os.mkdir('.\/Frames\/nonviolence\/n{}'.format(v)) \n    while Video_Caption.isOpened():\n        \n        Current_Frame_ID = Video_Caption.get(1)\n        \n        ret,frame = Video_Caption.read()\n        \n        if ret != True:\n            break\n            \n        if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n            image = cv2.resize(frame,(256,256))\n            cv2.imwrite(\".\/Frames\/nonviolence\/n{}\/frame{}.jpg\".format(v,count), image)\n            non_violence_frame_list.append([\".\/Frames\/nonviolence\/n{}\/frame{}.jpg\".format(v,count),0])\n            count += 1\n    v += 1\n    Video_Caption.release()\n    \nlen(non_violence_frame_list)","69d424ff":"# non_violence_frame_list","be26a0f4":"class Config():\n    def __init__(self):\n        pass\n    \n    num_classes=2\n    labels_to_class = {0:'nonviolence',1:'violence'}\n    class_to_labels = {'nonviolence':0,'violence':1}\n    resize = 224\n    num_epochs =10\n    batch_size =10","8d2ffc71":"train_data_path = '.\/Frames'","6df5b5fe":"if not os.path.exists('data_files'):\n    os.mkdir('data_files')\nif not os.path.exists('data_files\/train'):\n    os.mkdir('data_files\/train') ","9a248228":"num_classes = 2\nlabels_name={'nonviolence':0,'violence':1}","e42feff2":"data_dir_list = os.listdir(train_data_path)\ndata_dir_list","7a260fb4":"data_dir_list = os.listdir(train_data_path)\nfor data_dir in data_dir_list: # looping over every activity\n    label = labels_name[str(data_dir)]\n    video_list = os.listdir(os.path.join(train_data_path,data_dir))\n    for vid in video_list: # looping over every video within an activity\n        train_df = pd.DataFrame(columns=['FileName', 'Label', 'ClassName'])\n        img_list = os.listdir(os.path.join(train_data_path,data_dir,vid))\n        for img in img_list:# looping over every frame within the video\n            img_path = os.path.join(train_data_path,data_dir,vid,img)\n            train_df = train_df.append({'FileName': img_path, 'Label': label,'ClassName':data_dir },ignore_index=True)\n        file_name='{}_{}.csv'.format(data_dir,vid)\n        train_df.to_csv('data_files\/train\/{}'.format(file_name))","e6c11677":"class ActionDataGenerator(object):\n    \n    def __init__(self,root_data_path,temporal_stride=1,temporal_length=16,resize=224):\n        \n        self.root_data_path = root_data_path\n        self.temporal_length = temporal_length\n        self.temporal_stride = temporal_stride\n        self.resize=resize\n    def file_generator(self,data_path,data_files):\n        '''\n        data_files - list of csv files to be read.\n        '''\n        for f in data_files:       \n            tmp_df = pd.read_csv(os.path.join(data_path,f))\n            label_list = list(tmp_df['Label'])\n            total_images = len(label_list) \n            if total_images>=self.temporal_length:\n                num_samples = int((total_images-self.temporal_length)\/self.temporal_stride)+1\n                print ('num of samples from vid seq-{}: {}'.format(f,num_samples))\n                img_list = list(tmp_df['FileName'])\n            else:\n                print ('num of frames is less than temporal length; hence discarding this file-{}'.format(f))\n                continue\n            \n            start_frame = 0\n            samples = deque()\n            samp_count=0\n            for img in img_list:\n                samples.append(img)\n                if len(samples)==self.temporal_length:\n                    samples_c=copy.deepcopy(samples)\n                    samp_count+=1\n                    for t in range(self.temporal_stride):\n                        samples.popleft() \n                    yield samples_c,label_list[0]\n\n    def load_samples(self,data_cat='train'):\n        data_path = os.path.join(self.root_data_path,data_cat)\n        csv_data_files = os.listdir(data_path)\n        file_gen = self.file_generator(data_path,csv_data_files)\n        iterator = True\n        data_list = []\n        while iterator:\n            try:\n                x,y = next(file_gen)\n                x=list(x)\n                data_list.append([x,y])\n            except Exception as e:\n                print ('the exception: ',e)\n                iterator = False\n                print ('end of data generator')\n        return data_list\n    \n    def shuffle_data(self,samples):\n        data = shuffle(samples,random_state=2)\n        return data\n    \n    def preprocess_image(self,img):\n        img = cv2.resize(img,(self.resize,self.resize))\n        img = img\/255\n        return img\n    \n    def data_generator(self,data,batch_size=10,shuffle=True):              \n        \"\"\"\n        Yields the next training batch.\n        data is an array [[img1_filename,img2_filename...,img16_filename],label1], [image2_filename,label2],...].\n        \"\"\"\n        num_samples = len(data)\n        if shuffle:\n            data = self.shuffle_data(data)\n        while True:   \n            for offset in range(0, num_samples, batch_size):\n                #print ('startring index: ', offset) \n                # Get the samples you'll use in this batch\n                batch_samples = data[offset:offset+batch_size]\n                # Initialise X_train and y_train arrays for this batch\n                X_train = []\n                y_train = []\n                # For each example\n                for batch_sample in batch_samples:\n                    # Load image (X)\n                    x = batch_sample[0]\n                    y = batch_sample[1]\n                    temp_data_list = []\n                    for img in x:\n                        try:\n                            img = cv2.imread(img)\n                            #apply any kind of preprocessing here\n                            #img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n                            img = self.preprocess_image(img)\n                            temp_data_list.append(img)\n    \n                        except Exception as e:\n                            print (e)\n                            print ('error reading file: ',img)  \n    \n                    # Read label (y)\n                    #label = label_names[y]\n                    # Add example to arrays\n                    X_train.append(temp_data_list)\n                    y_train.append(y)\n        \n                # Make sure they're numpy arrays (as opposed to lists)\n                X_train = np.array(X_train)\n                #X_train = np.rollaxis(X_train,1,4)\n                y_train = np.array(y_train)\n                y_train = np_utils.to_categorical(y_train, 2)\n\n                # The generator-y part: yield the next training batch            \n                yield X_train, y_train\n","28dc11de":"root_data_path='data_files'\n\ndata_gen_obj=ActionDataGenerator(root_data_path,temporal_stride=2,temporal_length=5)","bd2ec42e":"train_data = data_gen_obj.load_samples(data_cat='train')","e8e88fbf":"\nprint('num of train_samples: {}'.format(len(train_data)))","f462eab9":"train_generator = data_gen_obj.data_generator(train_data,batch_size=100,shuffle=True)","0dc7e30a":"def get_model():\n    # Define model\n    model = Sequential()\n    model.add(Conv3D(32, kernel_size=(3, 3, 3), input_shape=(\n        5,224,224,3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n    model.add(Dropout(0.25))\n\n    model.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(2))\n    model.add(Activation(\"softmax\"))\n    model.compile(loss=categorical_crossentropy,\n                  optimizer=Adam(), metrics=['accuracy'])\n    model.summary()\n    #plot_model(model, show_shapes=True,\n    #           to_file='model.png')\n    return model","9d0ba733":"\nmodel = get_model()","c2549534":"hist = model.fit_generator(train_generator, \n                steps_per_epoch=len(train_data),epochs=3)","056b0632":"datagen = ImageDataGenerator(validation_split=0.2, \n                             rescale=1.\/255,\n                             preprocessing_function=preprocess_input)","076d18c3":"# train_generator = datagen.flow_from_directory(\".\/Frames\",\n#                                               target_size=(224, 224), color_mode='rgb',\n#                                               class_mode='binary', batch_size=32,)","749ac8e7":"video = Input(shape=(224,224,3))\ncnn_base = VGG16(input_shape=(224,224,3),\n                 weights=\"imagenet\",\n                 include_top=False)\n# cnn_out = GlobalAveragePooling2D()(cnn_base.output)\ncnn_out = cnn_base.output\ncnn = Model(cnn_base.input,cnn_out)\ncnn.trainable = False\n\nmodel = keras.Sequential()\nmodel.add(keras.Input(shape=(224,224,3)))\nmodel.add(cnn)\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(Bidirectional(LSTM(256,return_sequences=True,\n                                  dropout=0.5,\n                                  recurrent_dropout=0.5)))\nmodel.add(LSTM(256,return_sequences=True,))\nmodel.add(Dense(128,\"relu\"))\nmodel.add(Dense(32,\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.summary()","fbc51c7f":"train = datagen.flow_from_directory(\".\/Frames\",\n                                              target_size=(224, 224), color_mode='rgb',\n                                              class_mode='binary',shuffle=True, batch_size=32,subset=\"training\" )\nvalid = datagen.flow_from_directory(\".\/Frames\",\n                                              target_size=(224, 224), color_mode='rgb',\n                                              class_mode='binary',shuffle=True, batch_size=8,subset=\"validation\")","3f502a68":"Callback_Stop_Early = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode='auto')\n\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n\nhistory = model.fit(train,\n                    validation_data=valid,\n                      callbacks=[Callback_Stop_Early],\n                      epochs=25)","fb2e618d":"model.save(\".\/vgg16_lstm1.hdf5\")","472bf4f9":"print(history.history.keys())\nsns.set()\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","5a5d6aff":"<a href=\".\/vgg16_lstm.hdf5\"> Download File <\/a>","e15719b6":"### **Merging all different Data**","6db386e1":"##  **Import Libraries**","2c952a1f":"## **Video Preprocessing**","5bb33eef":"# Part-3  VDG + VGG16 + LSTM","4efe732b":"## **Data Importing from different datasets**"}}