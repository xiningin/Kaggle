{"cell_type":{"c355db54":"code","6e3fc5ee":"code","71e8b605":"code","7d3e8272":"code","f65fd911":"code","29c056f5":"code","6ff4d4d9":"code","da248902":"code","e3e04dac":"code","2906e130":"code","365b6cc2":"code","d48baacc":"code","96a916b5":"code","b924a522":"code","e28cee94":"code","8270d067":"code","e8aa1a33":"code","4cc2df03":"code","e2f167a0":"code","6c2746ef":"code","84fa9ac5":"code","781c6490":"code","b3578316":"code","cc41553d":"code","22b9f278":"code","637d904e":"code","b3f39d95":"code","1e2addcb":"code","38d4f10a":"code","0fd0aa3f":"code","825c3607":"code","f1db30c6":"code","84b81d05":"code","4bc56c29":"code","86982e24":"code","a9075f0f":"code","80bb0318":"code","977ddef5":"code","c5750d1b":"code","d69baa20":"code","88c6c106":"code","fbded21e":"code","8926c8cb":"code","0d40a64e":"code","a6f567cd":"code","c6e5206b":"code","09bb2a99":"code","ebd54196":"code","081b0e02":"code","9e7011b0":"code","4678b8e4":"code","799fdfa8":"code","2f36777f":"code","767bce86":"code","ad82d8fd":"code","4be2e45d":"code","9e602a5f":"code","88d848ce":"code","dc437145":"code","58431017":"code","b8f411dd":"code","7253e793":"code","51de4fd2":"code","50b3f171":"code","b5ad1bbe":"code","257aa6ca":"code","7ddd1d0c":"code","1f1e86af":"code","77ddbf66":"code","f19d67e3":"code","11210214":"code","26a0b823":"code","a1496f30":"code","28a81752":"code","8ba2fac1":"code","9b91c262":"code","e30e7099":"code","f22e17fd":"code","507bbd3e":"code","8172babd":"code","2f649887":"code","23da962a":"code","847f72ff":"code","f436099d":"code","6e7ea209":"code","1c70bf01":"code","294c544c":"code","afeefc60":"code","aecebda5":"code","bb3a9c64":"code","448501a7":"markdown","9f283adf":"markdown","7bc2b5fd":"markdown","cbc34237":"markdown","dd8228ce":"markdown","2812d4ad":"markdown","af1fca58":"markdown","c9ebc1cb":"markdown","6ca99480":"markdown","92781ae5":"markdown","9e08c5c3":"markdown","db448568":"markdown","44a8b27e":"markdown","f556811d":"markdown","4c00bff6":"markdown","326ff5d0":"markdown","fa46913a":"markdown","9b5bb9c2":"markdown","ed2ef963":"markdown","a77c7df4":"markdown","7ba0aa50":"markdown","e0dcdfa1":"markdown","fbb31585":"markdown","4aaa6c11":"markdown","8b046b6e":"markdown","25fdcbde":"markdown","b6a14c9c":"markdown","c90c1dd3":"markdown","f6d232a2":"markdown","4331d6f8":"markdown","45315d2e":"markdown","816ebe3f":"markdown","01829e54":"markdown","20c2ed70":"markdown","b651ba9e":"markdown","126d0548":"markdown","acee6468":"markdown","e6b1ef9e":"markdown","e3f12d4f":"markdown","5505f235":"markdown","c4983310":"markdown","23a5552f":"markdown","27ec1519":"markdown","d47f825b":"markdown","36ac7a22":"markdown","3096a12a":"markdown","7649883d":"markdown","43df37c8":"markdown","fcff1dfb":"markdown","98daa719":"markdown","c4f121c4":"markdown","d406c69f":"markdown","44af5b51":"markdown"},"source":{"c355db54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6e3fc5ee":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\nimport re\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split, learning_curve, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier\n%matplotlib inline\n","71e8b605":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","7d3e8272":"train.head()","f65fd911":"train.info()","29c056f5":"train.describe()","6ff4d4d9":"train[train['Survived'] == 1]['Survived'].count()\/train['Survived'].count()","da248902":"sns.countplot(x='Survived', data=train, hue='Sex')","e3e04dac":"train[[\"Sex\", \"Survived\"]].groupby('Sex', as_index=False).mean()","2906e130":"sns.distplot(train['Fare'])","365b6cc2":"train['Fare'].mean()","d48baacc":"train[['Pclass', 'Survived']].groupby('Pclass', as_index=False).mean()","96a916b5":"# box plot of age for pclass\nplt.figure(figsize=(12, 8))\nsns.boxplot(x='Pclass', y='Age', hue='Survived', data=train)","b924a522":"sns.distplot(train['Age'].dropna())","e28cee94":"train['Age'].dropna().mean()","8270d067":"g = sns.FacetGrid(train, col='Survived', row='Pclass')\ng.map(plt.hist, 'Age', bins=20)","e8aa1a33":"sns.countplot(x='Embarked', hue='Survived', data=train)","4cc2df03":"plt.figure(figsize=(12, 5))\nsns.countplot(x='Parch', hue='Survived', data=train)","e2f167a0":"plt.figure(figsize=(12, 5))\nsns.countplot(x='SibSp', hue='Survived', data=train)","6c2746ef":"sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\")","84fa9ac5":"def detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","781c6490":"train.loc[Outliers_to_drop] ","b3578316":"train = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","cc41553d":"\ntrain_len = len(train)","22b9f278":"combined = pd.concat([train, test], ignore_index=True)","637d904e":"combined.drop('PassengerId', axis=1, inplace=True)","b3f39d95":"sns.heatmap(combined.isnull())","1e2addcb":"np.sum(train.isnull())","38d4f10a":"np.sum(test.isnull())","0fd0aa3f":"embarked_mode = train['Embarked'].mode()[0]","825c3607":"embarked_mode","f1db30c6":"combined['Embarked'] = combined['Embarked'].fillna(embarked_mode)","84b81d05":"combined['Cabin'].dropna().apply(str).apply(lambda x: x[0]).unique()","4bc56c29":"combined['Cabin'].fillna('U', inplace=True)","86982e24":"combined['Cabin'] = combined['Cabin'].apply(str).apply(lambda x : x[0])","a9075f0f":"sns.countplot(combined[\"Cabin\"])","80bb0318":"np.where(np.isnan(combined['Fare']))","977ddef5":"combined.iloc[1033]","c5750d1b":"sns.heatmap(train[[\"Fare\",\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot=True, cmap='coolwarm')","d69baa20":"train[(train['Pclass'] == 3)]['Fare'].median()","88c6c106":"combined['Fare'][1033] = train[(train['Pclass'] == 3)]['Fare'].median()","fbded21e":"# combined[\"Fare\"] = combined[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","8926c8cb":"train['Name'].head()","0d40a64e":"combined['Title'] = combined['Name'].apply(lambda x : (x.split(', ')[1].split('.')[0]))\ncombined.drop('Name', axis=1, inplace=True)","a6f567cd":"combined['Title'].unique()","c6e5206b":"combined['Title'].value_counts()","09bb2a99":"titles_map = {\n 'Capt' : 'Rare',\n 'Col' : 'Rare',\n 'Don': 'Rare',\n 'Dona': 'Rare',\n 'Dr' : 'Rare',\n 'Jonkheer' :'Rare' ,\n 'Lady': 'Rare',\n 'Major': 'Rare',\n 'Master': 'Master',\n 'Miss' : 'Miss',\n 'Mlle' : 'Rare',\n 'Mme': 'Rare',\n 'Mr': 'Mr',\n 'Mrs': 'Mrs',\n 'Ms': 'Rare',\n 'Rev': 'Rare',\n 'Sir': 'Rare',\n 'the Countess': 'Rare'\n}","ebd54196":"combined['Title'] = combined['Title'].map(titles_map)","081b0e02":"combined[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","9e7011b0":"sns.heatmap(train[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot=True, cmap='coolwarm')","4678b8e4":"def impute_age(row):\n    pclass = row['Pclass']\n    parch = row['Parch']\n    sibsp = row['SibSp']\n    age = row['Age']\n    if pd.isnull(age):\n        age_median = train['Age'].median()\n        similar_age =  train[(train['Pclass'] == pclass) & (train['Parch'] == parch)\n                       & (train['SibSp'] == sibsp)]['Age'].median()\n        if( similar_age > 0): return similar_age\n        else :  return age_median\n    else :return age","799fdfa8":"combined['Age'] = combined.apply(impute_age, axis=1)","2f36777f":"combined['Family_size'] = combined.apply(lambda row : 1 + (row['Parch'] + row['SibSp']), axis=1)\ncombined['Alone'] = combined.apply(lambda row : 1 if (row['Parch'] + row['SibSp']) == 0 else 0, axis=1)","767bce86":"sns.countplot(x='Family_size' , data=combined, hue='Survived')","ad82d8fd":"combined['Small_family'] = combined.apply(lambda row : 1 if 2 <= (row['Family_size']) <= 4 else 0, axis=1)","4be2e45d":"combined.drop(['Parch', 'SibSp'], axis=1, inplace=True)","9e602a5f":"combined['Ticket'] = combined['Ticket'].apply(lambda x : 'X' if x.isdigit() else x)","88d848ce":"combined['Ticket'] = combined['Ticket'].apply(lambda x : re.sub(\"[\\d\\.]\", \"\", x).split('\/')[0].strip() if not x.isdigit() else x)","dc437145":"combined = pd.get_dummies(combined, columns = [\"Embarked\"], prefix=\"Em\")\n\ncombined = pd.get_dummies(combined, columns = [\"Cabin\"], prefix=\"Cb\")\n\ncombined = pd.get_dummies(combined, columns = [\"Title\"], prefix=\"Title\")","58431017":"sex_encoder = LabelEncoder().fit(combined['Sex'])\ncombined['Sex'] = sex_encoder.transform(combined['Sex'])","b8f411dd":"ticket_encoder = LabelEncoder().fit(combined['Ticket'])\ncombined['Ticket'] = ticket_encoder.transform(combined['Ticket'])","7253e793":"np.sum((combined.drop('Survived', axis=1).isnull()))","51de4fd2":"combined.head()","50b3f171":"train = combined[:train_len]\ntest = combined[train_len:]","b5ad1bbe":"test.drop('Survived', axis=1, inplace=True)","257aa6ca":"np.sum((test.isnull()))","7ddd1d0c":"train.head()","1f1e86af":"train.head()","77ddbf66":"train['Survived'] = train['Survived'].astype(int) ","f19d67e3":"X = train.drop('Survived', axis=1)\ny = train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","11210214":"\nrandom_state = 42\n \nmodel_names = ['LogisticRegression', 'DecisionTreeClassifier', 'SVC', \n              'RandomForestClassifier', 'XGBClassifier', 'ExtraTreesClassifier'\n              , 'GradientBoostingClassifier','AdaBoostClassifier','GaussianNB']\nmodels = [ ('LogisticRegression',LogisticRegression(random_state=random_state)),\n          ('DecisionTreeClassifier', DecisionTreeClassifier(random_state=random_state)),\n          ('SVC', SVC(random_state=random_state)),\n          ('RandomForestClassifier',RandomForestClassifier(random_state=42)),\n          ('XGBClassifier',XGBClassifier(random_state=random_state)),\n          ('ExtraTreesClassifier',ExtraTreesClassifier(random_state=random_state)),\n          ('GradientBoostingClassifier',GradientBoostingClassifier(random_state=random_state)),\n          ('AdaBoostClassifier',AdaBoostClassifier(random_state=random_state)),\n          ('GaussianNB',GaussianNB())\n         ]\n\nmodel_accuracy = []\nfor k,model in models:\n    model.fit(X, y)\n    accuracy = cross_val_score(model, X_train, y_train, cv=10).mean()\n    model_accuracy.append(accuracy)\n","26a0b823":"pd.concat([pd.Series(model_names), pd.Series(model_accuracy)], axis=1).sort_values(by=1, ascending=False)","a1496f30":"best_models=[]\n\nxgboot_param_grid = {\n     'n_estimators': [100,200,300],\n     'max_depth': [4, 6, 8],\n     'learning_rate': [.4, .45, .5, .55, .6],\n     'colsample_bytree': [.6, .7, .8, .9, 1]\n}\n\nada_param_grid = {\n 'n_estimators':[100,200,300],\n 'learning_rate' : [0.01,0.05,0.1,0.3,1],\n 'algorithm' : ['SAMME', 'SAMME.R']\n }\n\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300, 400],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10, 15],\n              \"min_samples_split\": [2, 3, 10, 15],\n              \"min_samples_leaf\": [1, 3, 10, 15],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,200,300, 400],\n              \"criterion\": [\"gini\"]}\n\n\nrf_param_grid  = { \n    'n_estimators': [100,200,300, 400],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\nlog_param_grid = {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n\nsvv_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\nmodels = [ \n    ('AdaBoostClassifier',AdaBoostClassifier(), ada_param_grid),\n          ('XGBClassifier',XGBClassifier(), xgboot_param_grid),\n          ('GradientBoostingClassifier',GradientBoostingClassifier(), gb_param_grid),\n        ('RandomForestClassifier',RandomForestClassifier(), rf_param_grid),\n          ('ExtraTreesClassifier',ExtraTreesClassifier(), ex_param_grid),\n    ('SVC',SVC(probability=True), svv_param_grid),\n    ('LogisticRegression',LogisticRegression(), log_param_grid)\n         ]\n\n\nfor name, model, param in  models:\n    grid_search = GridSearchCV(model,\n                               scoring='accuracy',\n                               param_grid=param,\n                               cv=10,\n                               verbose=2,\n                               n_jobs=-1)\n    grid_search.fit(X, y)\n    print (name, ':', grid_search.best_score_, '\\n')\n    best_models.append(grid_search.best_estimator_)\n\n\n","28a81752":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\nfor model in best_models:\n    plot_learning_curve(model,model.__class__.__name__ + \" RF mearning curves\",X,y,cv=5)","8ba2fac1":"def plot_feature_importances(clf, X_train, y_train=None, \n                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n    '''\n    plot feature importances of a tree-based sklearn estimator\n    \n    Note: X_train and y_train are pandas DataFrames\n    \n    Note: Scikit-plot is a lovely package but I sometimes have issues\n              1. flexibility\/extendibility\n              2. complicated models\/datasets\n          But for many situations Scikit-plot is the way to go\n          see https:\/\/scikit-plot.readthedocs.io\/en\/latest\/Quickstart.html\n    \n    Parameters\n    ----------\n        clf         (sklearn estimator) if not fitted, this routine will fit it\n        \n        X_train     (pandas DataFrame)\n        \n        y_train     (pandas DataFrame)  optional\n                                        required only if clf has not already been fitted \n        \n        top_n       (int)               Plot the top_n most-important features\n                                        Default: 10\n                                        \n        figsize     ((int,int))         The physical size of the plot\n                                        Default: (8,8)\n        \n        print_table (boolean)           If True, print out the table of feature importances\n                                        Default: False\n        \n    Returns\n    -------\n        the pandas dataframe with the features and their importance\n        \n    Author\n    ------\n        George Fisher\n    '''\n    \n    __name__ = \"plot_feature_importances\"\n    \n    \n    from xgboost.core     import XGBoostError\n    from lightgbm.sklearn import LightGBMError\n    \n    try: \n        if not hasattr(clf, 'feature_importances_'):\n            clf.fit(X_train.values, y_train.values.ravel())\n\n            if not hasattr(clf, 'feature_importances_'):\n                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n                                    format(clf.__class__.__name__))\n                \n    except (XGBoostError, LightGBMError, ValueError):\n        clf.fit(X_train.values, y_train.values.ravel())\n            \n    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n    feat_imp = feat_imp.iloc[:top_n]\n    \n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n    \n    if print_table:\n        from IPython.display import display\n        print(\"Top {} features in descending order of importance\".format(top_n))\n        display(feat_imp.sort_values(by='importance', ascending=False))\n        \n    return feat_imp","9b91c262":"for model in best_models:\n    try:\n        _ = plot_feature_importances(model, X_train, y_train, top_n=X.shape[1], title=model.__class__.__name__)\n    except AttributeError as e:\n        print(e)","e30e7099":"pred = []\nfor model in best_models:\n    pred.append(pd.Series(model.predict(test), name=model.__class__.__name__))","f22e17fd":"pred = pd.DataFrame(pred).transpose()","507bbd3e":"pred","8172babd":"pred.sum()","2f649887":"g= sns.heatmap(pred.corr(),annot=True, cmap='coolwarm')","23da962a":"ids = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')['PassengerId']\nvotingC = VotingClassifier(estimators=[\n                                    ('ada', best_models[0]),\n                                       ('rf', best_models[3]),\n                                       ('ext', best_models[4]),\n                                       ('log', best_models[6]),\n                                      ], voting='soft', n_jobs=-1)\nvotingC.fit(X, y)\ntest_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"ensemble_prediction.csv\",index=False)","847f72ff":"ids = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')['PassengerId']\nvotingC = VotingClassifier(estimators=[\n                                    ('ada', best_models[0]),\n                                       ('rf', best_models[3]),\n                                       ('log', best_models[6]),\n                                      ], voting='soft', n_jobs=-1)\nvotingC.fit(X, y)\ntest_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"ensemble_prediction2.csv\",index=False)","f436099d":"ada_best = best_models[0]\nada_best.fit(X, y)\ntest_Survived = pd.Series(ada_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"Ada.csv\",index=False)","6e7ea209":"xgb_best = best_models[1]\nxgb_best.fit(X, y)\ntest_Survived = pd.Series(xgb_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"XGB.csv\",index=False)","1c70bf01":"grb_best = best_models[2]\ngrb_best.fit(X, y)\ntest_Survived = pd.Series(grb_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"GRB.csv\",index=False)","294c544c":"rnf_best = best_models[3]\nrnf_best.fit(X, y)\ntest_Survived = pd.Series(rnf_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"RNF.csv\",index=False)","afeefc60":"ext_best = best_models[4]\next_best.fit(X, y)\ntest_Survived = pd.Series(ext_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"EXT.csv\",index=False)","aecebda5":"svc_best = best_models[5]\nsvc_best.fit(X, y)\ntest_Survived = pd.Series(svc_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"SVC.csv\",index=False)","bb3a9c64":"log_best = best_models[6]\nlog_best.fit(X, y)\ntest_Survived = pd.Series(log_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"LOG.csv\",index=False)","448501a7":"### 1. Background","9f283adf":"process age","7bc2b5fd":"#### 7.1 One Hot Encoding","cbc34237":"74% of the women had been survived, and only 18% of the men had been survived","dd8228ce":"more than 62% of the passenger that were in class 1 had been survived.","2812d4ad":"we can see that AdaBosstClassifier, RandaomForest, ExtraTreeClassifier, LogisticRegression is  highly correlated together.","af1fca58":"we can see that the Age is negativly correlated to Parch, Pclass, Sibsp.\nso let us let us fill missing age rows, with the median of the ages based on gender and class","c9ebc1cb":"some statistic info","6ca99480":"Show the outliers rows","92781ae5":" ### 6. Data Cleaning","9e08c5c3":"process name","db448568":"we can see that average of fare paid for ticket is about 30","44a8b27e":"### 2. Problem","f556811d":"we can see that passenger with small family (2-4) had more change to survived","4c00bff6":"we can see that XGBClassifier has underfiting\nRF, LogisticRegression, AdaBoost has well fiting.","326ff5d0":"prcoess cabin - update cabin to be the first letter of cabin","fa46913a":"Our target is to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","9b5bb9c2":"### 7. Feature Engineering","ed2ef963":"#### 7.2 Label Encoding","a77c7df4":"process Embarked with the most fequent embarked","7ba0aa50":"### 4. Gathering data","e0dcdfa1":"process ticket - get only prefix of the ticket unsing regex","fbb31585":"we can see that people with more siblings or family have less probability to survived.","4aaa6c11":"let us remove the PassengerId column wich will not add anything to the machine learning process","8b046b6e":"we can see that the average age is about 30","25fdcbde":"### 10. Model Ensembling","b6a14c9c":"process parch and sbisp - create new columns","c90c1dd3":"## 8. Model - Machine learning","f6d232a2":"we can see that Fare is negativly correlated with Pclass","4331d6f8":"The sinking of the [Titanic](https:\/\/www.kaggle.com\/c\/titanic) is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.","45315d2e":"i will choose XGBClassifier, GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier  to continue with the hyperparameter tuning","816ebe3f":"#### 9.2 Hyperparameter tuning","01829e54":"try to grab the title of each passenger.","20c2ed70":"let's use the grid search to find the best estimator.","b651ba9e":"fill missing Cabin with 'U' as 'Unknown'","126d0548":"#### 9.1 Cross Validation","acee6468":"38% survived","e6b1ef9e":"#### 6.1 Outliers","e3f12d4f":"we can see that  Dr, Rev, Major, Col, Mlle, Capt, Don, Jonkheer, the Countess, Ms, Sir, Lady, Mme    \nis really rare titles, so i will map them as Rare","5505f235":"# Titanic Survival Prediction","c4983310":"Dataset Columns :\n\n* PassengerId - Unique ID of the passenger\n* Survived - Survived (1) or died (0)\n* Pclass - Passenger's class (1st, 2nd, or 3rd)\n* Name - Passenger's name\n* Sex - Passenger's sex\n* Age - Passenger's age\n* SibSp - Number of siblings\/spouses aboard the Titanic\n* Parch - Number of parents\/children aboard the Titanic\n* Ticket - Ticket number\n* Fare - Fare paid for ticket\n* Cabin - Cabin number\n* Embarked - Where the passenger got on the ship (C - Cherbourg, S - Southampton, Q = Queenstown)","23a5552f":"### 9. Model Evaluation","27ec1519":"take a brief look to the data","d47f825b":"test data is cleaned.","36ac7a22":"we can see that more females had been survived, and more males had been not survived","3096a12a":"### 3. Import Libraries","7649883d":"the row is 1033, lets get the mean of Fare according to the 152 row's features","43df37c8":"### 5. EDA - Exploratory data analysis","fcff1dfb":"let us combine the train and test data sets.","98daa719":"average fare is 32.2","c4f121c4":"#### 6.2 Missing Data","d406c69f":"First of all i will remove outliers from numeric features, that might badly influnce the machine learning process.","44af5b51":"no empty data - we are good to go."}}