{"cell_type":{"0f6050c0":"code","c3a16675":"code","948b631c":"code","a94ba5dd":"code","3975751b":"code","36539441":"code","74eab828":"code","56a525dc":"code","cd752ed8":"code","1b239d96":"code","5ae99e6d":"code","0008647d":"code","720c64e3":"code","09ec07f9":"code","9f2d546c":"code","5806d772":"code","19305a96":"code","300def10":"code","17f5f52f":"markdown","09e04084":"markdown","bc25ca39":"markdown","1480a609":"markdown","a9baeaa7":"markdown","a1ab43d5":"markdown","1c55d96a":"markdown","c001c924":"markdown"},"source":{"0f6050c0":"# Loading all necessary modules\n\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import interp\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n\npd.set_option('display.max_columns', 500)\n%matplotlib inline\n\n# ignore annoying warnings\nwarnings.filterwarnings('ignore')","c3a16675":"# loading our data and taking a quick look at it\n\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\ntrain_df.head()","948b631c":"test_df.head()","a94ba5dd":"print('Train_df shape: ', train_df.shape)\nprint('Test_df shape: ', test_df.shape)","3975751b":"train_df.describe()","36539441":"corr = train_df.corr()['target'].sort_values(ascending = False)","74eab828":"corr.head(20)","56a525dc":"corr.tail(20)","cd752ed8":"Y_train = train_df['target']\nX_train = train_df.drop(['target', 'id'], axis = 1)\nX_test = test_df.drop('id', axis = 1)\n\nprint(Y_train.shape)\nprint(X_train.shape)\nprint(X_test.shape)","1b239d96":"log = LogisticRegression(penalty = 'l1', random_state = 42)\n\n# Setting parameters for GridSearchCV\nparams = {'solver': ['liblinear', 'saga'], \n          'C': [0.001, 0.1, 1, 10, 50], \n          'tol': [0.00001, 0.0001, 0.001, 0.005], \n          'class_weight': ['balanced', None]}\n\nlog_gs = GridSearchCV(log, params, cv = StratifiedKFold(n_splits = 5), verbose = 1, n_jobs = -1, scoring = 'roc_auc')\n\n# Fitting our model\nlog_gs.fit(X_train, Y_train)\n\n# Looking for best estimator\nlog_best = log_gs.best_estimator_\n\nprint(log_best)\nprint(log_gs.best_score_)","5ae99e6d":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","0008647d":"g = plot_learning_curve(log_best,\"LR learning curves\",X_train,Y_train,cv=StratifiedKFold(n_splits = 5))","720c64e3":"def plot_roc(clf, X = X_train, y = Y_train, n = 6):\n    '''Plotting ROC curves with cross validation'''\n    \n    cv = StratifiedKFold(n_splits=n)\n    classifier = clf\n\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n\n    i = 0\n    plt.figure(figsize = (8, 7))\n    for train, test in cv.split(X, y):\n        probas_ = classifier.fit(X.iloc[train], y.iloc[train]).predict_proba(X.iloc[test])\n        # Compute ROC curve and area the curve\n        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        tprs[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n\n        i += 1\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n             label='Chance', alpha=.8)\n\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    plt.plot(mean_fpr, mean_tpr, color='b',\n             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n             lw=2, alpha=.8)\n\n    std_tpr = np.std(tprs, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                     label=r'$\\pm$ 1 std. dev.')\n\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()","09ec07f9":"plot_roc(log_best)","9f2d546c":"log_p = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='saga', random_state = 42)\ng = plot_learning_curve(log_p,\"LR learning curves\",X_train,Y_train,cv=StratifiedKFold(n_splits = 5))","5806d772":"plot_roc(log_p)","19305a96":"# This model will give us 0.847 public score\n\nlog_p.fit(X_train, Y_train)\nlog_preds = log_p.predict_proba(X_test)[:, 1]\n\n# predict_proba returns 2-dimensional array, where 1st value is a probability that target value = 0,\n# 2nd value - probability that target value = 1, we need only 2nd values.","300def10":"# Prepairing submission\nsubmission = pd.DataFrame({\n    'id': test_df['id'],\n    'target': log_preds\n})\n\nsubmission.to_csv('submission.csv', index = False)","17f5f52f":"We see that Training score = 100, but cv_score is far away from training score. It looks like overfitting and we still need to reduce complexity of our model.\nC - parameter that responsible for level of regularization, if we reduce its value - we increase level of regularization, so this is the main parameter that we need to tune.\n\nLet's also take a look at ROC curves:","09e04084":"We see that our model gives us 0.818 auc score, not bad, but lets take a look on learning curves.","bc25ca39":"After some experiments, I stopped on next parameters: \npenalty='l1', class_weight='balanced', C=0.1, solver='saga', random_state = 42\n\nLets plot learning curves for new model","1480a609":"We see that not too much fetures really significant in this data, and we need to do something to get rid of unsignificant features. So, let's start modelling.","a9baeaa7":"We can see interesting things here:\n - mean value of features is about 0\n - standard deviation of features is about 1\n - looks like data is scaled\n\n Let's look at correlation:","a1ab43d5":"We see that we have only continuous variables, and we don't know what this features mean. Let's try to gather more information about our data.","1c55d96a":"So, in this competition we have very small training dataset with only 250 samples and 300 continuous numeric features.\nWhat we can do in this case? Something tells me that simplest model is best for that type of data, so let's try to keep it simple.\n","c001c924":"Let's return to our features, we saw, that we need to reduce complexity of our model, so we need an algorithm with regularization, that can make a lot of work for us. During experiments, I stopped on logistic regression with L1 regularization, so lets train and tune it."}}