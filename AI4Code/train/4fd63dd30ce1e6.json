{"cell_type":{"2a35efc6":"code","db540b33":"code","fda9a4ca":"code","19795ece":"code","0cbb023b":"code","e0da352d":"code","e51593e0":"code","25fa7f5e":"code","c694b7f1":"code","962d75a8":"code","a66f0e68":"code","04342a7d":"code","d448b46e":"code","f3ff615c":"code","88d5fede":"code","3b1ece7c":"code","61b702fd":"code","2879fb21":"code","d4a6828e":"code","4221924d":"code","33d48718":"code","e98c137d":"code","041c9d29":"code","5f57a7e8":"code","00dc0b52":"code","bb213f21":"code","b0015347":"code","81d54d26":"code","6bbff788":"code","5ce22d66":"code","51896f35":"code","5e009a24":"code","e4bbbc01":"code","f0017970":"code","cce14dfa":"code","7da8cf16":"code","83c0eaa5":"code","2b02dcc3":"code","39f9f61e":"code","99dc2155":"code","ffb79e67":"code","5e7aec3e":"code","e8057a96":"code","1b67e4ca":"code","4bd685e6":"code","8270ee54":"code","2ff470c8":"code","8f208994":"code","31ccf8c3":"code","9d1638cf":"code","0d074845":"code","201c543b":"code","c462581f":"code","25191642":"code","43604b2b":"markdown","95f9a5b8":"markdown","adc568a4":"markdown","31ff1d99":"markdown","55ded845":"markdown","9bde447a":"markdown","0381ab4c":"markdown","b750652a":"markdown","7e6ff41a":"markdown","dcd5fb1e":"markdown","3881b404":"markdown","09416495":"markdown","c83b81f7":"markdown","19d6e615":"markdown","dc35b849":"markdown","b9a56514":"markdown","ca928bb1":"markdown","478429a9":"markdown","4116aa38":"markdown","abdb7e9d":"markdown","8b24b8fc":"markdown","7682bec8":"markdown","d80bb209":"markdown","99b184fc":"markdown","4e1c3595":"markdown"},"source":{"2a35efc6":"# data manipulation \nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split  # Seperate train-test samples\nfrom sklearn import preprocessing  # Scaling data\nfrom sklearn.decomposition import PCA  # PCA implementation\n#plotly fix\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\n# visualisation \nimport matplotlib.pyplot as plt   # Plots\nimport plotly.express as px # PCA plots \nimport seaborn as sns # Plots\nimport matplotlib as mpl\nfrom cycler import cycler   # can help with assigning colors\nfrom tabulate import tabulate # Creates tables\nfrom sklearn.ensemble import ExtraTreesClassifier # Decision trees - used for feature importance\nimport matplotlib.lines as mlines # To be used for legends triangles\/stars\/other markers\n# ML\nfrom tensorflow.keras.layers import Dense, Lambda\nfrom tensorflow.keras import Sequential, Input\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras.optimizers import Adam\nfrom keras import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras import regularizers\nimport tensorflow as tf\nimport tensorflow_probability as tfp","db540b33":"# Import Dataset\nwine = pd.read_csv(\"..\/input\/samples\/wine.csv\")\n\n# Descriptive Statistics per class\npd.set_option('display.expand_frame_repr', False)\n\nsum1 = pd.DataFrame(round(wine.groupby(\"type\").describe().stack(),3))\nsum1","fda9a4ca":"plt.figure(figsize = (20,15))\nplt.title(\"Histograms per variable and class on the wine dataset\")\nsns.set_theme(style=\"dark\")\ncolumns = [\"fixed.acidity\",\"volatile.acidity\",\"citric.acid\",\n            \"residual.sugar\",\n            \"chlorides\",\n            \"free.sulfur.dioxide\",\n            \"total.sulfur.dioxide\",\n            \"density\",\n            \"pH\",\n            \"sulphates\",\"alcohol\",\"quality\",\"type\"]\nmeasurements = [\"(g(tartaric acid)\/dm^3)\", \"(g(acetic acid)\/dm^3)\", \n               \"(g\/dm^3)\", \"(g\/dm^3)\", \"(g(sodium chloride)\/dm^3)\",\n                \"(mg\/dm^3)\", \"(g\/cm^3)\",\"\",\"(g(potassium sulphate)\/dm^3)\",\n                \"(%)\",\"\",\"\",\"\"\n               ]\ni = 1\nj = 0\nfor columns in columns:\n    plt.subplot(4,4,i)\n    sns.histplot(data = wine, x = columns, hue = \"type\", palette = \"tab10\")\n    plt.xlabel(\"\")\n    plt.title(\"{} {}\".format(columns,measurements[j]))\n    j = j+1\n    i = i+1\n","19795ece":"\"\"\"\nProduces a heatmap of the correlation matrix by using seaborn.\n\"\"\"\nplt.figure(figsize = (12,10))\nax = plt.axes()\n\nsns.heatmap(wine.drop([\"type\"], axis = 1).corr(), annot =True, cmap=\"PiYG\", ax = ax)\nax.set_title(\"Heatmap of Correlation Matrix - Wine Dataset\", loc = \"left\", fontsize = 15,\n            fontweight =\"bold\")","0cbb023b":"# preprocessing data\n\n\n# Classification task\n\n# quality is excluded from features and type is set as target\nfeatures= [\"fixed.acidity\",\n                 \"volatile.acidity\",\n                 \"citric.acid\",\n                 \"residual.sugar\",\n                 \"chlorides\",\n                 \"free.sulfur.dioxide\",\n                 \"total.sulfur.dioxide\",\n                 \"density\",\"pH\",\n                 \"sulphates\",\"alcohol\"]\nwine_class = wine.loc[:,features]\ntype_class = wine.loc[:,['type']]\n\n\n\n# Regression task\n\nwine_reg = wine.loc[:,features]\nquality = wine.loc[:,[\"quality\"]]","e0da352d":"\"\"\"\nThe extra trees classifier from scikit learn is implemented below, \nboth for regression and classification tasks.\n\nrandom_state = 4 is used to make the results reproducible\n\"\"\"\n\n\n# Classification Imporance\n\nft = ExtraTreesClassifier(random_state = 4)\nft.fit(wine_class,type_class.values.ravel())\nprint(ft.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(ft.feature_importances_, index=wine_class.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.title(\"Feature Importance for predicting type of wine (Extra-Trees Method)\", loc = \"left\", fontsize = 10,\n            fontweight =\"bold\")\nplt.xlabel(\"Score\")\nplt.show()\n\n# Regression Imporance\n\nft = ExtraTreesClassifier(random_state = 4)\nft.fit(wine_reg,quality.values.ravel())\nprint(ft.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(ft.feature_importances_, index=wine_reg.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.title(\"Feature Importance for predicting quality of wine (Extra-Trees Method)\",loc = \"left\", fontsize = 10,\n            fontweight =\"bold\") \nplt.xlabel(\"Score\")\nplt.show()","e51593e0":"\"Section where classification data is scaled and used in pca\"\n\n# Classification - Scale inputs\n\nwine_class = wine_class.values\ntype_class = type_class.values\n\nwine_class = preprocessing.scale(wine_class, axis=0, with_mean=True, with_std=True, copy= False)","25fa7f5e":"\"\"\"\nWe define a 2-PCA task below using the sklearn:: PCA function.\n\nA new dataframe (pca_wine_1) is then created with the corresponding 2 PC components \n\nDISCLAIMER: ALthough versions of this graph are implemented below, the R - ggplot2 counterpart was eventually used in my report. \nCode for its production is provided in the appendix section, however it is also provided below.\n\n\n______________________________\n{R CODE FOR PCA PLOT IN REPORT}\n\nlibrary(ggplot2)  # visualisation package\nlibrary(ggfortify) # visualise PCA with loadings\n\nwine = read.csv(\"..\/input\/samples\/wine.csv\")\n\nwine.pca <- prcomp(wine[,c(1:(ncol(wine)-2))], center = TRUE,scale. = TRUE)\nautoplot(wine.pca, data = wine, colour = \"type\", loadings = TRUE,\n        loadings.label = TRUE, loadings.label.size = 3)\n______________________________\n\"\"\"\n\n# Define PCA\npca = PCA(n_components = 2)\npca_wine = pca.fit_transform(wine_class)\n\n\n# Create dataframe for visualisation\npca_wine_1 = pd.DataFrame(data = pca_wine, \n                     columns = [\"PC 1\", \"PC 2\"])\npca_wine_1 = pd.concat([pca_wine_1, wine[[\"type\"]]], axis = 1)\n\n#Explained variance - to be used on title\npca2_variance = pca.explained_variance_ratio_.sum()*100\n\n# Plot parameters\nfig, ax = plt.subplots(1,1,figsize= (8,8))\nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title(f'2 component PCA: Explained Variance: {pca2_variance:.2f}%', fontsize = 20)\n\n# Set 1 color per type\ntargets = [1,2,3]\ncolors = [\"b\",\"g\",\"r\"]\nfor target, color in zip(targets,colors):\n    indicesToKeep = pca_wine_1['type'] == target\n    ax.scatter(pca_wine_1.loc[indicesToKeep, 'PC 1']\n               , pca_wine_1.loc[indicesToKeep, 'PC 2']\n               , c = color\n               , s = 50\n              )\nax.legend(targets)\nax.grid()","c694b7f1":"# Split test and train data - Classification Task\n\n\"\"\"\nAllow dataset splitting into a train and test counterpart.\nRandom state = 42 used to make results reproducible.\n\nX_xxxx_class: Features per xxxx (train or test) dataset.\nY_xxxx_class: Target per xxxx dataste in one Hot encoded mode (sparse = False to return an array)\n\"\"\"\n\n# Core train-test datasets\ntrain_class, test_class = train_test_split(wine, test_size=0.2, random_state = 42)\n\n# Data preprocessing\nX_train_class = train_class.drop([\"type\",\"quality\"], axis = 1)   # Ntrainx11\nY_train_class = OneHotEncoder(sparse=False).fit_transform(train_class[[\"type\"]].values) # Ntrainx3\n\nX_test_class = test_class.drop([\"type\",\"quality\"], axis = 1)  # Ntestx11\nY_test_class = OneHotEncoder(sparse=False).fit_transform(test_class[[\"type\"]].values) # Ntestx3\n\nd = X_train_class.shape[1]  # second index = 11\nbatch_size = len(X_train_class)  # = N train. Whole training dataset used per epoch\n\n# Print shapes \/ dataset lengths\nprint(X_train_class.shape, Y_train_class.shape)\nprint(X_test_class.shape, Y_test_class.shape)\n\n# Scale inputs to improve loss convergence\nX_train_class = preprocessing.scale(X_train_class, axis=0, with_mean=True, with_std=True, copy= False)\nX_test_class = preprocessing.scale(X_test_class, axis=0,with_mean=True,with_std = True, copy = False)","962d75a8":"# Set Seed value for reproducability\n\nseed_value = 0","a66f0e68":"\"\"\"\nKeras was used to implement the frequentist NN. Relevant acurracies and metrics are also printed.\n\nIn order to visualise the confusion matrix, a function is defined on the next section.\n\"\"\"\n\n# make NN reproducible\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)\n\n# Create model \nmodel_class = Sequential()\nmodel_class.add(Input(shape = (11)))\nmodel_class.add(Dense(5, kernel_regularizer = regularizers.l2(0.001), activation = \"relu\"))  # hidden layers. a L2 regularised was used\nmodel_class.add(Dense(3, activation = \"softmax\"))\n\n# compile and fit with 10% * train_test = validation test\nmodel_class.compile(optimizer=Adam(learning_rate=0.01), \n                    loss='categorical_crossentropy',\n                   metrics = [\"acc\",metrics.RootMeanSquaredError(),metrics.CategoricalCrossentropy()]\n                   )\n\nhistory = model_class.fit(x=X_train_class, y=Y_train_class, validation_split = 0.1,\n                         epochs=200, batch_size=batch_size, verbose = 0)\n\n# Predict Y fit in train and measure scores\npred_train= model_class.predict(X_train_class)\nscores = model_class.evaluate(X_train_class, Y_train_class, verbose=0)\nprint('Accuracy on training data: {}% \\n Error on training data: {}'.format(round(scores[1],3), round(scores[0],3)))   \n\n# Predict Y fit in test and measure scores\npred_test= model_class.predict(X_test_class)\nscores2 = model_class.evaluate(X_test_class, Y_test_class, verbose=0)\nprint('Accuracy on test data: {}% \\n Error on test data: {}'.format(round(scores2[1],3), round(scores2[0],3)))    \n\n# Test data confusion matrix\nconfusion_matrix(np.argmax(Y_test_class, axis=1),       # Input class membership\n                 np.argmax(pred_test, axis=1))  # Output class membership","04342a7d":"def heatmap(pred, text, scores):\n    \n    \"\"\"\n    Creates visualisation function for heatmaps on Test Data\n\n    Inputs:\n    pred: prediction vector by using the test dataset as inputs. (originating from: model.predict(x_test))\n    text: text to be used on plot title (depending on moden)\n    scores: print accouracy scores (originating from: model.evaluate(x_test,y_test))\n    \"\"\"\n\n    plt.figure(figsize = (5,5))\n    ax = plt.axes()\n\n    sns.heatmap(confusion_matrix(np.argmax(Y_test_class, axis=1),       # Input class membership\n                 np.argmax(pred, axis=1)),annot =True, cmap=\"PiYG\", fmt='g')\n    ax.set_title(\"Confusion Matrix - {}\".format(text))\n    ax.set_xlabel(\"Predicted Value\")\n    ax.set_ylabel(\"True Value\")\n    plt.text(0.45,0.01,\n         \"Acurracy on test data:{}%\".format(round(100*scores[1],2)),\n         horizontalalignment='center',\n         verticalalignment='center',\n         transform=plt.gcf().transFigure,\n         fontsize = 13\n            )","d448b46e":"\"Output of heatmap function presented above\"\n\nheatmap(pred_test, \"NN\", scores2)","f3ff615c":"\"\"\"\nIn order to visualise the loss and acurracy metric over the epochs, another function is \nintroduced. It takes the history object as an input and plots the loss function.\nIf the \"acc\" is true, it also plots the accuracy over time. \n\"\"\"\n\ndef plot_loss_acc(x, acc = True):\n   \n    \"\"\"\n    plot_loss_acc:\n    Simple function that plots loss and accurracy metrics over epochs\n    \n    Input: \n    x : Object to plot loss function (usually the history variable)\n    acc: set to True if accurraccy should be plotted as well\n    \n    Output:\n    Respective plots\n    \"\"\"\n    history = x.history\n\n    loss_values = history[\"loss\"]\n    val_loss_values = history[\"val_loss\"]\n\n    final_epoch = len(history[\"loss\"])+1\n    epochs = range(1,final_epoch)\n\n    plt.plot(epochs, loss_values, \"b\", label = \"Training Loss\",linewidth=2)\n    plt.plot(epochs, val_loss_values,\"r\", label = \"Validation Loss\", linewidth=2)\n    plt.title(\"Training and Validation Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n\n    plt.show()\n    \n    if acc == True:\n        plt.clf()\n    \n        acc_values = history['acc']\n        val_acc_values = history['val_acc']\n        plt.plot(epochs, acc_values, 'b', label='Training acc',linewidth=2)\n        plt.plot(epochs, val_acc_values, 'r', label='Validation acc',linewidth=2)\n        plt.title('Training and validation accuracy')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy (%)')\n        plt.legend(loc = \"right\")\n        plt.show()","88d5fede":"# call created function to make plots on frequentist NN\nplot_loss_acc(history, acc= True)","3b1ece7c":"\"\"\"\nWe will now use keras in order to transform the previous model to a Bayesian one.\n\nDenseFlipout layers are used, whereas Normal priors and posterios are set accordingly.\n\nThe kl_divergence_fn function also defines divergence method between these two.\n\nOptimisers and loss functions are used as previously\n\"\"\"\n\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)\n\n# KL divergence function - to be used as input for flipout layers\nkl_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) \/ (X_train_class.shape[0] *1.0)\n\nbnn = Sequential()\nbnn.add(Input(shape = 11))\nbnn.add(tfp.layers.DenseFlipout( units =20,\n                         kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n                         kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n                         kernel_divergence_fn = kl_divergence_fn,\n                         activation = \"relu\"\n                        )\n       )\n\nbnn.add(tfp.layers.DenseFlipout(units = 3, \n                                kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n                                kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn(), \n                                kernel_divergence_fn = kl_divergence_fn,\n                                activation = \"softmax\")\n       )\n\n\nbnn.compile( optimizer = \"adam\", loss = 'categorical_crossentropy',\n           metrics = [\"acc\", ]\n           ) \n\nhistory_bnn = bnn.fit(X_train_class,Y_train_class, epochs = 200, validation_split = 0.1, \n                      batch_size = batch_size,\n                      verbose = 0\n                     )\n\n# Original plot_loss_acc function presented previously\nplot_loss_acc(history_bnn, acc= True)\n\n# Predict Y fit in train and measure scores\npred_train_vi= bnn.predict(X_train_class)\nscores_vi = bnn.evaluate(X_train_class, Y_train_class, verbose=0)\nprint('Accuracy on training data: {}% \\n Error on training data: {}'.format(scores_vi[1],scores_vi[0]))   \n\n# Predict Y fit in test and measure scores\npred_test_vi= bnn.predict(X_test_class)\nscores2_vi = bnn.evaluate(X_test_class, Y_test_class, verbose=0)\nprint('Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2_vi[1],  scores2_vi[0]))    \n\n# Fitted values (outputs predicted from training inputs)\nconfusion_matrix(np.argmax(Y_test_class, axis=1),       # Input class membership\n                 np.argmax(pred_test_vi, axis=1))  # Output class membership","61b702fd":"\"\"\"\nOriginal heatmap function presented previously\n\"\"\"\nheatmap(pred_test_vi, \"BNN\", scores2_vi)","2879fb21":"\"\"\"\nLimitation: Model is unable to capture uncertainty for meaningless data, giving a clear prediction of a type 1 wine\n\"\"\"\n# Test what happens for data with + 100 data points - Out of distribution points\n# Outcome presented in limitations\nbnn.predict(X_test_class[0:1]+100)","d4a6828e":"# check that pred_test_vi displays softmax probabilities\n\"\"\"\nSimple numpy configuration to avoid scientific notation\n\"\"\"\nnp.set_printoptions(suppress=True) # outputs now has decimals","4221924d":"def mc(model, X_test, Y_test, n_iter , thres):\n    \"\"\"\n    We will run n_iter iterations of the prediciton process over the X_test sample. \n    The produced list has dimensions (number of iterations)x(sample size)x(number of labels) = 1000 x 980 x 3\n\n    To get a sample of 1000 assigned probabilites per observation (row in dataframe) \n    we need to reshape this list into a  980 x 3 x 100 (inserted to y_pred_per_obs vector)\n\n    Thus the first element will produce the softmax output probability distribution over 1000 iterations.\n    \n    Our goal it to visualise these distributions per class. Thus a new function will be implemented later.\n    \n    Inputs:\n    \n    model: neural network on which the .predict() function will be used\n    X_test,Y_test: corresponding datasets\n    n_iter: number of iterations\n    thres: threshold set upon median to organise predictions as valid \/ invalid. 0.5 was used in this analysis\n    \n    Outputs:\n    \n    n_iter: number of iterations (for possible printing purposes and calculations)\n    idx_valid \/ idx_invalid: positions of valid \/ invalid predictions on the test dataset\n    valid \/ invalid : position of valid \/ invalid predictions on the unsplitted wine dataset (\n    to be used for visualisation purposes on PCA)\n    y_pred:  Extracted for other possible calculations (explained in the code section)\n    y_pred_per_obs: SoftMax probabilities per iteration and observation. \n    \n    \"\"\"\n    n_iter = n_iter\n    thres = thres # threshold for medians\n\n    y_pred_bnn = [model.predict(X_test) for _ in range(n_iter)]  # perform n_iter iterations of predictions \n    # reshape the previous matrix from Iter*N_test  *outputshape to N_test* iter * outputshape  \n    y_pred_per_obs = np.concatenate([y[:,:, np.newaxis] for y in y_pred_bnn], axis = -1) \n    # if median of all single observation distributions >= threshold, insert it to y_pred vector\n    # This produces a logical vector with 0\/1 elements. Either only one of the elements will have a 1 (True).\n    # Otherwise all will be 0 (none of the distributions met the criteria)\n    y_pred = [[int(np.median(y) >= thres) for y in y_pred_per_obs] for y_pred_per_obs in y_pred_per_obs]\n    y_pred = np.array(y_pred)\n    \n    # Print shapes\n    print(\"Initial shape {} transformed to {}\".format(np.shape(y_pred_bnn),y_pred_per_obs.shape))\n          \n    \"\"\"\n    Find positions\/indices of invalid and valid classifications in test dataset\n    \"\"\"\n    idx_valid = [any(y) for y in y_pred]   # records element if at least one element in y_pred[y] is true\n    idx_invalid = [not any(y) for y in y_pred] # records element if all elements in y_pred[y] are false\n    idx_invalid = np.where(idx_invalid)[0]     # extract indices of the mentioned elements. These will indicate positions on the test dataset\n    idx_valid = np.where(idx_valid)[0]         # same as above \n    \n    \"\"\"\n    Extract positions ofvalid\/invalid classifications in the original dataset\n    \n    The rverse method is used. As we now have the valid\/invalid indices on the test dataset, we can print their location\n    and extract their index = location on the original dataset. \n    \n    For example, if test_data[100] == wine[300] is always the idx_invalid[0] = 100 index (split data reproducible), the index 300\n    will be recovered and inserted in the valid[0] vector.\n    \n    In that sense the observation defined by test_data[idx_invalid[i]] is the same as wine[invalid[i]] \n    \n    (test_data[idx_invalid[0]] = test_data[100] = wine[300] = wine[invalid[0]])\n    \n    Same goes with valid.\n    \"\"\"\n    invalid = []\n    for id in idx_invalid:\n        invalid.append(test_class.iloc[[id]].index[0]) # (test.class.iloc[[id] -> produces row)  .index[0] - > captures index\n\n    valid = []\n    for id in idx_valid:\n        valid.append(test_class.iloc[[id]].index[0]) \n    \n    return n_iter, idx_valid, idx_invalid, valid, invalid, y_pred, y_pred_per_obs","33d48718":"# Call function and save outputs\nn_iter, idx_valid, idx_invalid, valid, invalid, y_pred, y_pred_per_obs = mc(bnn, X_test_class,Y_test_class,1000,0.5)\n","e98c137d":"# Test that function manages to  correctly extract indices\n\n# Valid Case\ncheck = True\nfor i in range(len(idx_valid)):\n    if all(test_class.iloc[idx_valid[i]] == wine.iloc[valid[i]]):\n        # If all elements are the same betweeb the two locations, check will retain its status\n        check = check\n    else:\n        # However if above condition is violated, check will turn to False\n        check = False\nprint(check)\n\n# Invalid case\ncheck2 = True\nfor i in range(len(idx_invalid)):\n    if all(test_class.iloc[idx_invalid[i]] == wine.iloc[invalid[i]]):\n        # If all elements are the same betweeb the two locations, check will retain its status\n        check2 = check\n    else:\n        # However if above condition is violated, check will turn to False\n        check2 = False\nprint(check2)","041c9d29":"\"\"\"\nThis produces 890 valid and 90 invalid observations. (980 total observations)\nAs random state was used in the splitting process,these observations were ensured to be the same \ndifferent runs of the analysis\n\"\"\"\nprint(np.shape(idx_valid))\nprint(np.shape(idx_invalid))","5f57a7e8":"\"\"\"\n2-PCA Revisited - Plot points that were hard to be classified (invalid cases)\n\nThis version uses the invlaid indeces from the previous code section to produce \na pca plot with uncertain points (none of median softmax probabilities >= threshold)\n\nThe core structure is the same as above besides the fact that PC1-PC2 main points were made transparent.\n\nWe are actually looking ton pinpoint only the cases where the BNN produced invalid predictions.\n\"\"\"\n# SAME AS PREVIOUS PCA PLOT DOCUMENTED ABOVE\npca = PCA(n_components = 2)\npca_wine = pca.fit_transform(wine_class)\n\npca_wine = pd.DataFrame(data = pca_wine, \n                     columns = [\"PC 1\", \"PC 2\"])\npca_wine = pd.concat([pca_wine, wine[[\"type\"]]], axis = 1)\n\n#explained variance\npca2_variance = pca.explained_variance_ratio_.sum()*100\n\nfig, ax = plt.subplots(1,1,figsize= (8,8))\nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title(f'PCA Plot - BNN: Area of Invalid Predictions' , fontsize = 15)\n\ntargets = [1,2,3]\ncolors = [\"b\",\"g\",\"r\"]\nfor target, color in zip(targets,colors):\n    indicesToKeep = pca_wine['type'] == target\n    ax.scatter(pca_wine.loc[indicesToKeep, 'PC 1']\n               , pca_wine.loc[indicesToKeep, 'PC 2']\n               , c = color\n               , s = 20\n               , alpha = 0.05\n              )\n#----NEW SECTION: PLOT INVALID PREDICTIONS IN GOLD COLOUR-------------       \nax.scatter(pca_wine.loc[invalid,\"PC 1\"]\n           , pca_wine.loc[invalid, \"PC 2\"]\n           , c = \"gold\"\n           , s = 50\n           , marker = \"^\"        \n           )\n# pyplot.mlines used here to make custom legend\nyellow_triangle_class = mlines.Line2D([], [], color='gold', marker='^', linestyle='None',\n                          markersize=10, label='Invalid Predictions')\nax.legend(handles = [yellow_triangle_class])\nax.grid()","00dc0b52":"\"\"\"\nFunctions introduced to plot the y_pred_per_obs distributions (examples given below)\n\nAs we want to also print the CIs, functions that introduce bounds will be defined\n\"\"\"\n\n\ndef relu(x):\n    \"relU function to bound lower CI intervals on 0\"\n    if x<0:\n        y = 0\n    else: \n        y = x\n    return y\n\ndef bound1(x):\n    \"Bound probability\/CI upper end to 1\"\n    if x>1:\n        x = 1\n    else:\n        x=x\n    return x\n\ndef plot_pred(idx, inv, Y_test, bins, alpha, fig = (15,15)):\n    \"\"\" \n    Takes input the index (idx) of the observation we want to plot.\n    The function uses the recovered position of the observation in the original dataset\n    and plots the assigned predictive softamax probabilities of that label over the previous iteration period.\n    \n    Used to visualise the assigned probabilities after several iterations of predictions on the test dataset.\n    \n    input: \n    idx =  Scalar discrete positive. Uses the index of invalid \/ valid position and plots the distributions per class, \n    as defined by the mc function. Distributions to be plotted are thus idx_valid[idx]\/idx_invalid[idx]. More information\n    given in the code section below.\n    inv = Should be True for invalid cases \/ False for valid cases\n    Y_test = Y test to be used\n    bins = controls number of bins\n    alpha = controls transparency of distributions\n    fig = figure size\n    \n    Outputs:\n    Plot of distributions along with printed medians and confidence intervals per class \n    \"\"\"\n    inv = inv\n    color = [\"b\",\"g\",\"r\"]  # set colors per class\n    label = [\"Class 0\",\"Class 1\",\"Class 2\"]\n    text = []  # Store text for medians\n    CI = []    # Store text for Confidence intervals\n  \n    # Specialised title and CIs for invalid and valid  cases\n    if inv == True:\n            ax, fig = plt.subplots(1,1,figsize = fig)\n            for i in range(0,3):\n                # if inv == True -> plot idx_invalid[idx: (function input)] per class i. colors and labels are also defined.\n                plt.hist(y_pred_per_obs[idx_invalid[idx]][i], bins = bins, color = color[i], alpha = alpha, label = label[i])\n                text.append(\"type %2.f = %.2f\"%(i,np.median(y_pred_per_obs[idx_invalid[idx]][i])))   # store medians\n                # upper CI bound: mean per class + 1.96* sd per class. Bound to 1.\n                upper = bound1(round(y_pred_per_obs[idx_invalid[idx],i].mean()+1.96*y_pred_per_obs[idx_invalid[idx],i].std(),3))\n                # lower CI bound: ean per class - 1.96* sd per class. Used RelU to avoid negative bounds.\n                lower = relu(round(y_pred_per_obs[idx_invalid[idx],i].mean()-1.96*y_pred_per_obs[idx_invalid[idx],i].std(),3))   \n                CI.append(\"CI Type {}: [{},{}]\".format(i,lower,upper))\n            plt.legend()\n            plt.title(\"Observation {} with True Value: {} - NN Pred.Value: {} - BNN Pred.Value: {}\\n\".format(   \n                # On this section I will introduce why each index is chosen. Whenever we want to refer to a vector\n                # With the same length as the y_test, idx_invalid[] should be used. For extracting indeces on the\n                # Original Dataframe, invalid[] is the correct option\n                invalid[idx], # original position of observation on whole dataset                                                                                                      \n                np.argmax(Y_test[idx_invalid[idx]]), # Y test should be reffered to idx_invalid[idx]\n                np.argmax(pred_test[idx_invalid[idx]]), # pred_test (NN prediction) is produced by Y_test -> idx_invalid[idx]\n                np.argmax(pred_test_vi[idx_invalid[idx]])), #  BNN predictions - same length as Y_test\n                       fontsize = 25)\n    else:   \n            # Same as above for valid case (inv == False)\n            ax, fig = plt.subplots(1,1,figsize = fig)\n            for i in range(0,3):\n                plt.hist(y_pred_per_obs[idx_valid[idx]][i], bins = bins, color = color[i], alpha = alpha, label = label[i])\n                text.append(\"type %2.f = %.2f\"%(i,np.median(y_pred_per_obs[idx_valid[idx]][i])))\n                upper = bound1(round(y_pred_per_obs[idx_valid[idx],i].mean()+1.96*y_pred_per_obs[idx_valid[idx],i].std(),3))   \n                lower = relu(round(y_pred_per_obs[idx_valid[idx],i].mean()-1.96*y_pred_per_obs[idx_valid[idx],i].std(),3))   \n                CI.append(\"CI Type {}: [{},{}]\".format(i,lower,upper))\n            plt.legend()\n            plt.title(\"Observation {} with True Value: {} - NN Pred.Value: {} - BNN Pred.Value: {}\\n\".format(\n                valid[idx],\n                np.argmax(Y_test[idx_valid[idx]]),\n                np.argmax(pred_test[idx_valid[idx]]), \n                np.argmax(pred_test_vi[idx_valid[idx]])),\n                       fontsize = 25)\n    # Print Text on plot\n    # Medians\n    plt.text(0.5,0.08,\n         \"Medians:{}\".format(text),\n         horizontalalignment='center',\n         verticalalignment='center',\n         transform=plt.gcf().transFigure,\n         fontsize = 25\n            )\n    # CIs\n    plt.text(0.5,0.04,\n         \"CIs:{}\".format(CI),\n         horizontalalignment='center',\n         verticalalignment='center',\n         transform=plt.gcf().transFigure,\n         fontsize = 25\n            )","bb213f21":"def getCI(idx,invalid = True):\n    \"\"\"\n    Function that computes predictive intervals.\n    \n    To be used for visualisation and comparing CIs later on.\n    \n    Inputs:\n    \n    idx: Index scalar in the same sense as plot_pred function\n    invalid: True if invalid CI, False if Valid CI\n    \n    \n    Outputs:\n    Lower, Upper: Confidence Interval Bounds\n    \"\"\"\n    CI = []\n    if invalid == True:\n        for i in range(3):\n            # predictive interval bounds for sample of observations\n            u = bound1(round(y_pred_per_obs[idx_invalid[idx],i].mean()+1.96*y_pred_per_obs[idx_invalid[idx],i].std(),3))   \n            l = relu(round(y_pred_per_obs[idx_invalid[idx],i].mean()-1.96*y_pred_per_obs[idx_invalid[idx],i].std(),3)) \n            CI.append([l,u])\n    else:\n        for i in range(3):\n            u = bound1(round(y_pred_per_obs[idx_valid[idx],i].mean()+1.96*y_pred_per_obs[idx_valid[idx],i].std(),3))   \n            l = relu(round(y_pred_per_obs[idx_valid[idx],i].mean()-1.96*y_pred_per_obs[idx_valid[idx],i].std(),3))\n            CI.append([l,u])\n    return  CI","b0015347":"\"\"\"\nObervations are extracted to ensure reproducability of code, as minor computational differences \non the MC experiment might occur\n\"\"\"\n\n# We are dealing with valid observations so the second argument is set to False\nplot_indexes_valid = np.array([valid.index(3659),valid.index(2634), valid.index(1924), valid.index(4352)]) \nfor i in plot_indexes_valid:\n    plot_pred(i,False, Y_test_class, 30,0.7,(15,15)) # type 2 red","81d54d26":"# Save CIs for late use (p-BNN Valid section)\nlower_valid = [getCI(plot_indexes_valid[i],False)[j][0]  for i in range(4) for j in range(3)]\nupper_valid = [getCI(plot_indexes_valid[i],False)[j][1]  for i in range(4) for j in range(3)]","6bbff788":"\"\"\" \nExtracting observations used on the report by list.index(observation).\n\"\"\"\n# For Reproducability : finds indeces of observations used in report as \nplot_indexes_invalid = np.array([invalid.index(79), invalid.index(4230), invalid.index(371), invalid.index(3999)]) \nfor i in plot_indexes_invalid:\n    plot_pred(i, True, Y_test_class, 40,0.7,(15,15))","5ce22d66":"# Save CIs for late use (p-BNN Invalid Section)\nlower_invalid = [getCI(plot_indexes_invalid[i],True)[j][0]  for i in range(4) for j in range(3)]\nupper_invalid = [getCI(plot_indexes_invalid[i],True)[j][1]  for i in range(4) for j in range(3)]","51896f35":"\"\"\"\n2-PCA Revisited - Above Indices represented on the 2D - PCA space\n\"\"\"\npca = PCA(n_components = 2)\npca_wine = pca.fit_transform(wine_class)\n\npca_wine = pd.DataFrame(data = pca_wine, \n                     columns = [\"PC 1\", \"PC 2\"])\npca_wine = pd.concat([pca_wine, wine[[\"type\"]]], axis = 1)\n\n#explained variance\npca2_variance = pca.explained_variance_ratio_.sum()*100\n\nfig, ax = plt.subplots(1,1,figsize= (8,8))\nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title(f'PCA Plot - Valid\/Invalid Predictions on 2D Space' , fontsize = 15)\n\ntargets = [1,2,3]\ncolors = [\"b\",\"g\",\"r\"]\nfor target, color in zip(targets,colors):\n    indicesToKeep = pca_wine['type'] == target\n    ax.scatter(pca_wine.loc[indicesToKeep, 'PC 1']\n               , pca_wine.loc[indicesToKeep, 'PC 2']\n               , c = color\n               , s = 20\n               , alpha = 0.05\n              )\n\n    \n\"\"\"\nChanged part: represent new indices as dots with the corresponding colors\n\"\"\"    \n# VALID AS TRIANGLES\ncols_valid = [\"red\", \"blue\", \"green\", \"brown\"]\nj =0 # index for colour\nfor i in plot_indexes_valid:\n    ax.scatter(pca_wine.loc[valid[i],\"PC 1\"]\n               , pca_wine.loc[valid[i], \"PC 2\"]\n               , c = cols_valid[j]\n               , s = 80\n               , marker = \"^\"        \n               )\n    j = j+1\n    \n# INVALIDS AS STARS\nj = 0\ncols_invalid = [\"fuchsia\", \"cyan\", \"orange\", \"purple\"]\nfor i in plot_indexes_invalid:\n    ax.scatter(pca_wine.loc[invalid[i],\"PC 1\"]\n               , pca_wine.loc[invalid[i], \"PC 2\"]\n               , c = cols_invalid[j]\n               , s = 100\n               , marker = \"*\",\n               alpha = 0.8\n               )\n    j = j+1\n# Graphic Traits\ntriangle1 = mlines.Line2D([], [], color='red', marker='^', linestyle='None',\n                          markersize=10, label='Valid - (1)')\ntriangle2 = mlines.Line2D([], [], color='blue', marker='^', linestyle='None',\n                          markersize=10, label='Valid - (2)')\ntriangle3 = mlines.Line2D([], [], color='green', marker='^', linestyle='None',\n                          markersize=10, label='Valid - (3)')\ntriangle4 = mlines.Line2D([], [], color='brown', marker='^', linestyle='None',\n                          markersize=10, label='Valid - (4)')\n\nstar1 = mlines.Line2D([], [], color='fuchsia', marker='*', linestyle='None',\n                          markersize=10, label='Invalid - (5)')\nstar2 = mlines.Line2D([], [], color='cyan', marker='*', linestyle='None',\n                          markersize=10, label='Invalid - (6)')\nstar3 = mlines.Line2D([], [], color='orange', marker='*', linestyle='None',\n                          markersize=10, label='Invalid - (7)')\nstar4 = mlines.Line2D([], [], color='purple', marker='*', linestyle='None',\n                          markersize=10, label='Invalid - (8)')\nax.legend(handles = [triangle1,triangle2,triangle3,triangle4,\n                    star1,star2,star3,star4])\nax.grid()","5e009a24":"\"\"\"\nUse whole dataset in order to reduce uncertainty in model\n\"\"\"\n\nX_class_whole = np.concatenate((X_train_class,X_test_class))\nY_class_whole = np.concatenate((Y_train_class,Y_test_class))\n\ndef NLL (y, distr):\n    \"Negative Log Likelihood function to be used for optimising a distribution\"\n    return -distr.log_prob(y)\n\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)\n\nprob_bnn = Sequential()\nprob_bnn.add(Input(shape = 11))\nprob_bnn.add(tfp.layers.DenseFlipout( units =20,\n                         kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n                         kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n                         kernel_divergence_fn = kl_divergence_fn,\n                         activation = \"relu\"\n                        )\n       )\nprob_bnn.add(Dense(tfp.layers.OneHotCategorical.params_size(3))) #3 parameter size\nprob_bnn.add(tfp.layers.OneHotCategorical(3))  # 3 event draws\n        \n\n\nprob_bnn.compile( optimizer = \"adam\", loss = NLL,\n           metrics = [\"acc\"]\n           ) \n\nhistory_prob_bnn = prob_bnn.fit(X_class_whole,Y_class_whole,validation_split = 0.1, \n                                epochs = 200, batch_size = batch_size,\n                                verbose = 0)\n\nplot_loss_acc(history_prob_bnn, acc = True)\n\n# Predict Y fit in train and measure scores\n#pred_train_pbnn= bnn.predict(X_class_whole)\n#scores_vi_pbnn = bnn.evaluate(X_train_class, Y_train_class, verbose=0)\nprint('Accuracy on training data: {}% \\n Error on training data: {}'.format(round(scores_vi[1],3),round(scores_vi[0],3)))   \n\n# Predict Y fit in test and measure scores\npred_test_pbnn= prob_bnn.predict(X_test_class)\nscores2_pbnn = prob_bnn.evaluate(X_test_class, Y_test_class, verbose=0)\nprint('Accuracy on test data: {}% \\n Error on test data: {}'.format(round(scores2_pbnn[1],3),  round(scores2_pbnn[0],3)))    \n\n# Fitted values (outputs predicted from training inputs)\nconfusion_matrix(np.argmax(Y_test_class, axis=1),       # Input class membership\n                 np.argmax(pred_test_pbnn, axis=1))  # Output class membership","e4bbbc01":"heatmap(pred_test_pbnn, \"p-BNN\", scores2_pbnn)","f0017970":"\"\"\"\nCreate lists for CIs (valid observations)\nValues printed for checkup\n\"\"\"\n\nCIU_pd = []\nCIL_pd = []\nprint(\"pBNN class Confidence Intervals on above valid observations\")\nfor index in plot_indexes_valid:\n    upperbound = 1\n    lowerbound = 0\n    np.random.seed(seed_value)\n    tf.random.set_seed(seed_value)\n    mean  = prob_bnn(X_test_class[idx_valid[index]].reshape(1,11)).mean().numpy().tolist()  # reshaped for correct input\n    sd = prob_bnn(X_test_class[idx_valid[index]].reshape(1,11)).stddev().numpy().tolist() \n    print(\"Observation {}\".format(valid[index]))\n    for i in range(3):\n        CIU = np.round(np.clip(mean[0][i] + 1.96*sd[0][i], lowerbound,upperbound),3)\n        CIL = np.round(np.clip(mean[0][i] - 1.96*sd[0][i], lowerbound,upperbound),3)\n        CIU_pd.append(CIU)\n        CIL_pd.append(CIL)\n        print(\"95% CI of Class {} - [ {}, {} ]\".format(i,CIL,CIU))","cce14dfa":"\"\"\"\nInsert valid into dataframe\n\"\"\"\n\n# Valid Dataframe\nClass = [\"Class 0\",\"Class 1\",\"Class 2\"]*4   # Repeat vector 4 times\nObserv = [valid[plot_indexes_valid[i]] for i in range(4)] # get valid observations \nObserv = [ele for ele in Observ for i in range(3)]  # Repeat each element in list 3 times \n\ndata = {\"Observation\": Observ,\n        \"Class\": Class,\n        \"BNN Lower CI\": lower_valid,\n        \"BNN Upper CI\": upper_valid,\n        \"pBNN Lower CI\": CIL_pd,\n       \"pBNN Upper CI\": CIU_pd,\n        \n       }\n\npd.DataFrame(data)","7da8cf16":"\"\"\"\nSame with invalid\n\"\"\"\n\nCIU_pd_inv = []\nCIL_pd_inv = []\nprint(\"pBNN class Confidence Intervals on above valid observations\")\nfor index in plot_indexes_invalid:\n    upperbound = 1\n    lowerbound = 0\n    np.random.seed(seed_value)\n    tf.random.set_seed(seed_value)\n    mean  = prob_bnn(X_test_class[idx_invalid[index]].reshape(1,11)).mean().numpy().tolist()  \n    sd = prob_bnn(X_test_class[idx_invalid[index]].reshape(1,11)).stddev().numpy().tolist() \n    print(\"Observation {}\".format(invalid[index]))\n    for i in range(3):\n        CIU = np.round(np.clip(mean[0][i] + 1.96*sd[0][i], lowerbound,upperbound),3)\n        CIL = np.round(np.clip(mean[0][i] - 1.96*sd[0][i], lowerbound,upperbound),3)\n        CIU_pd_inv.append(CIU)\n        CIL_pd_inv.append(CIL)\n        print(\"95% CI of Class {} - [ {}, {} ]\".format(i,CIL,CIU))","83c0eaa5":"# Inalid Dataframe\n\nObserv1 = [invalid[plot_indexes_invalid[i]] for i in range(4)] # get invalid observations \nObserv1 = [ele for ele in Observ1 for i in range(3)]  # Repeat each element in list 3 times \n\ndata = {\"Observation\": Observ1,\n        \"Class\": Class,\n        \"BNN Lower CI\": lower_invalid,\n        \"BNN Upper CI\": upper_invalid,\n        \"pBNN Lower CI\": CIL_pd_inv,\n       \"pBNN Upper CI\": CIU_pd_inv,\n        \n       }\n\npd.DataFrame(data)","2b02dcc3":"\"\"\"\nLimitation: Model (p-BNN) is unable to capture uncertainty for meaningless data, \ngiving a clear prediction of a type 1 wine\n\"\"\"\n# Test what happens for data with + 100 data points - Out of distribution points\n# Outcome presented in limitations\nprob_bnn.predict(X_test_class[0:1]+100)","39f9f61e":"# Split test and train data \n\ntrain_reg, test_reg = train_test_split(wine, test_size=0.2, random_state = 42)\n\nX_train_reg = train_reg.drop([\"type\",\"quality\"], axis = 1)   # Ntrainx11\nY_train_reg = np.array(train_reg[[\"quality\"]]) # Ntrainx3\n\nX_test_reg = test_reg.drop([\"type\",\"quality\"], axis = 1)  # Ntestx11\nY_test_reg = np.array(test_reg[[\"quality\"]])","99dc2155":" # Ntestx3\n\nd = X_train_reg.shape[1]  # second index = 12\nbatch_size = len(X_train_reg)  # = N train \n\n#print shapes \nprint(X_train_reg.shape, Y_train_reg.shape)\nprint(X_test_reg.shape, Y_test_reg.shape)\n\n#scale inputs\nX_train_reg = preprocessing.scale(X_train_reg, axis=0, with_mean=True, with_std=True, copy= False)\nX_test_reg = preprocessing.scale(X_test_reg, axis=0,with_mean=True,with_std = True, copy = False)","ffb79e67":"np.random.seed(seed_value)\ntf.random.set_seed(seed_value)\n\n# KL divergence function - to be used as input for flipout layers\nkl_divergence_fn=lambda q, p, _: tfp.distributions.kl_divergence(q, p) \/ (X_train_reg.shape[0] *1.0)\n\nbnn_reg = Sequential()\nbnn_reg.add(Input(shape = 11))\nbnn_reg.add(tfp.layers.DenseFlipout( units =20,\n                         kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n                         kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n                         kernel_divergence_fn = kl_divergence_fn,\n                         activation = \"relu\"\n                        )\n       )\n\nbnn_reg.add(tfp.layers.DenseFlipout(units = 1, \n                                kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n                                kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn(), \n                                kernel_divergence_fn = kl_divergence_fn,\n                                activation = \"linear\")\n       )\n\n\nbnn_reg.compile( optimizer = \"adam\", loss = 'mse',\n           metrics = [metrics.RootMeanSquaredError()]\n           ) \n\nhistory_bnn_reg = bnn_reg.fit(X_train_reg,Y_train_reg, epochs = 500, validation_split = 0.1, \n                      batch_size = batch_size,\n                      verbose = 0\n                     )\n\n# Predict Y fit in train and measure scores\npred_train_vi_reg= bnn_reg.predict(X_train_reg)\nscores_vi_reg = bnn_reg.evaluate(X_train_reg, Y_train_reg, verbose=0)\nprint('RMSE on training data: {} \\n Error on training data: {}'.format(round(scores_vi_reg[1],3), round(scores_vi_reg[0],3)))   \n\n# Predict Y fit in test and measure scores\npred_test_vi_reg= bnn_reg.predict(X_test_reg)\nscores2_vi_reg = bnn_reg.evaluate(X_test_reg, Y_test_reg, verbose=0)\nprint('RMSE on test data: {} \\n Error on test data: {}'.format(round(scores2_vi_reg[1],3),  round(scores2_vi_reg[0],3)))    ","5e7aec3e":"plot_loss_acc(history_bnn_reg, acc= False)","e8057a96":"\"\"\"\nSame notion as previously. As we do not have a threshold, we will simply sort the lists based on their standard \ndeviation.\n\"\"\"\n\ny_pred_bnn_reg = [bnn_reg.predict(X_test_reg) for _ in range(500)]  # perform n_iter iterations of predictions \n\n# reshape the previous matrix from Iter*N_test*outputshape to N_test*iter*outputshape\ny_pred_per_obs_reg = np.concatenate([y[:,:, np.newaxis] for y in y_pred_bnn_reg], axis = -1) \ny_pred_reg_mean = [[np.mean(y) for y in j] for j in y_pred_per_obs_reg]\ny_pred_reg_std = [[np.std(y) for y in j] for j in y_pred_per_obs_reg]\n\ny_pred_reg_mean = np.array(y_pred_reg_mean)\ny_pred_reg_std = np.array(y_pred_reg_std)\n\nprint(\"Max variance: {} - Min variance: {}\".format( round(max(y_pred_reg_std[:,0]),2) , round(min(y_pred_reg_std[:,0]),3)))","1b67e4ca":"#simple check on random index to ensure values are assigned correctly\ncheck = [] \ncheck.append(y_pred_per_obs_reg[694].mean() == y_pred_reg_mean[694])    \ncheck.append(y_pred_per_obs_reg[694].std() == y_pred_reg_std[694])\ncheck","4bd685e6":"\"\"\"\nWe should remember that y_pred_per_obs_reg or otherwise the n_iter sample of predictions per observations\nis actually the same size of the Y_test. Therefore the first row of the vector will refer to the first element\nof the Y_test etc. Sorting the values based on the max\/min standarad error will thus reveal areas of high uncertainty.\nWe will take the first and last 20 observations indeces on the test set, extract their original index on the unsplitted dataset\nand plot them on the PCA plot as done before.\n\"\"\"\n\n# top 20 indeces with highest\/lowest sd - These refer to test data\nhighest_sd_reg = y_pred_reg_std.argsort(axis = 0)[-20:]\nhighest_sd_reg = highest_sd_reg[::-1] # reverse order of list so that first = minimum\nminimum_sd_reg = y_pred_reg_std.argsort(axis =0)[:20]\n\n# extract indeces on whole dataset\nreg_invalid =[]\nfor id in highest_sd_reg[:,0]:\n    reg_invalid.append(test_reg.iloc[[id]].index[0])\n\nreg_valid = []\nfor id in minimum_sd_reg[:,0]:\n    reg_valid.append(test_reg.iloc[[id]].index[0])","8270ee54":"# Check that correct assignment was made for test - whole dataset\nfor i in range(3):\n    # If all elements true print \"Correct\"\n    if any(test_reg.iloc[[highest_sd_reg[0].item()]] == wine.iloc[[reg_invalid[0]]]):\n        print(\"Correct\")","2ff470c8":"def bound10(x):\n    \"\"\"\n    Bound upper CI to 10 for quality\n    \"\"\"\n    if x>=10:\n        x = 10\n    return x","8f208994":"def getCIreg(idx):\n    \"\"\"\n    Calculate CIs Per observation - regression case\n    \n    Input:\n    idx: index based on test dataset. \n        - highest_sd_reg \/ minimum_sd_reg will be used in order to get the \n        n first indeces within an iteration\n    \n    output:\n        CI in a list\n    \"\"\"\n    CIreg = []\n    upper = (y_pred_reg_mean[idx] + 1.96*y_pred_reg_std[idx]).item()\n    lower =(y_pred_reg_mean[idx] - 1.96*y_pred_reg_std[idx]).item()\n    upper = bound10(upper)\n    CIreg.append([round(lower,3),round(upper,3)])\n    \n    return CIreg","31ccf8c3":"\"\"\"\nConstruct vectors to be used in a dataframe.\nWe use the highest\/minimum_sd_reg vectors as they refer to the test dataframe.\nTheir true values are also extracted\n\"\"\"\n\nbnn_high_sd_reg_u = []\nbnn_high_sd_reg_l = []\nbnn_low_sd_reg_u = []\nbnn_low_sd_reg_l = []\nobs_index_high = []\nobs_index_low =  []\ntrue_value_high = []\ntrue_value_low = []\nfor i in range(5):\n    # Highest sd\n    bnn_high_sd_reg_l.append(getCIreg(highest_sd_reg[i])[0][0]) #gives lower bound as number \n    bnn_high_sd_reg_u.append(getCIreg(highest_sd_reg[i])[0][1]) # gives upper bound as number \n    true_value_high.append(test_reg.iloc[[highest_sd_reg[i][0]]][\"quality\"].item()) # Extract true value\n    obs_index_high.append(reg_invalid[i]) # saves observation index (= first 5 indeces of reg_invalid, the sorted sd vector based on indeces on whole dataset)\n    # Lowest sd\n    bnn_low_sd_reg_l.append((getCIreg(minimum_sd_reg[i])[0][0]))\n    bnn_low_sd_reg_u.append((getCIreg(minimum_sd_reg[i])[0][1]))\n    true_value_low.append(test_reg.iloc[[minimum_sd_reg[i][0]]][\"quality\"].item())\n    obs_index_low.append(reg_valid[i])","9d1638cf":"\"\"\"\n2-PCA Revisited - Plot points on the that were hard\/easy to be predicted\n\n\"\"\"\npca = PCA(n_components = 2)\npca_wine = pca.fit_transform(wine_class)\n\npca_wine = pd.DataFrame(data = pca_wine, \n                     columns = [\"PC 1\", \"PC 2\"])\npca_wine = pd.concat([pca_wine, wine[[\"type\"]]], axis = 1)\n\n#explained variance\npca2_variance = pca.explained_variance_ratio_.sum()*100\n\nfig, ax = plt.subplots(1,1,figsize= (8,8))\nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title(f'PCA Plot - Uncertain areas of predicting quality' , fontsize = 15)\n\ntargets = [1,2,3]\ncolors = [\"b\",\"g\",\"r\"]\nfor target, color in zip(targets,colors):\n    indicesToKeep = pca_wine['type'] == target\n    ax.scatter(pca_wine.loc[indicesToKeep, 'PC 1']\n               , pca_wine.loc[indicesToKeep, 'PC 2']\n               , c = color\n               , s = 20\n               , alpha = 0.05\n              )\nax.scatter(pca_wine.loc[reg_invalid,\"PC 1\"]\n           , pca_wine.loc[reg_invalid, \"PC 2\"]\n           , c = \"gold\"\n           , s = 50\n           , marker = \"^\"        \n           )\nax.scatter(pca_wine.loc[reg_valid,\"PC 1\"]\n           , pca_wine.loc[reg_valid, \"PC 2\"]\n           , c = \"purple\"\n           , s = 50\n           , marker = \"^\"        \n           )\n\n\n\"\"\"\nREVISED PART - PLOT HIGH\/LOW SD POINTS ON FEATURE SPACE\n\"\"\"\npurple_triangle = mlines.Line2D([], [], color='purple', marker='^', linestyle='None',\n                          markersize=10, label='Lowest SD')\nyellow_triangle = mlines.Line2D([], [], color='yellow', marker='^', linestyle='None',\n                          markersize=10, label='Highest SD')\n\nax.legend(handles=[purple_triangle, yellow_triangle])\nax.grid()","0d074845":"\"\"\"\nUse whole dataset in order to reduce uncertainty in model\n\"\"\"\n# Reproducability \nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)\n\n# concatenate dataset\nX_reg_whole = np.concatenate((X_train_reg,X_test_reg))\nY_reg_whole = tf.cast( np.concatenate((Y_train_reg,Y_test_reg)),tf.float32) # reparameterize to float32\n\ndef NLL (y, distr):\n    \"Negative Log Likelihood function to be used for optimising a distribution\"\n    return -distr.log_prob(y)\n\nprob_bnn_reg = Sequential()\nprob_bnn_reg.add(Input(shape = 11))\nprob_bnn_reg.add(tfp.layers.DenseFlipout( units =20,\n                         kernel_prior_fn = tfp.layers.default_mean_field_normal_fn(),\n                         kernel_posterior_fn = tfp.layers.default_mean_field_normal_fn() ,\n                         kernel_divergence_fn = kl_divergence_fn,\n                         activation = \"relu\"\n                        )\n       )\nprob_bnn_reg.add(Dense(tfp.layers.IndependentNormal.params_size(1))) #2 parameter size\nprob_bnn_reg.add(tfp.layers.IndependentNormal(1))  # 1 event draws\n        \n\n\nprob_bnn_reg.compile( optimizer = \"adam\", loss = NLL,\n           metrics = []\n           ) \n\nhistory_prob_bnn_reg = prob_bnn_reg.fit(X_reg_whole,Y_reg_whole,validation_split = 0.1, epochs = 200, batch_size = batch_size,\n                   verbose = 0)\n\nplot_loss_acc(history_prob_bnn_reg, acc = False)","201c543b":"\"\"\"\nExtract CIs per case for p-BNN model\n\"\"\"\n\npBNN_High_CI = []\npBNN_Low_CI = []\npBNN_High_mean = [] \npBNN_Low_mean = []\nfor i in range(5):\n    # High SD mean\n    high_mean = prob_bnn_reg(X_test_reg[highest_sd_reg[i]].reshape(1,11)).mean().numpy().item()\n    pBNN_High_mean.append(high_mean)\n    # High SD CI Bounds\n    u_high = bound10(round(high_mean + 1.96*prob_bnn_reg(X_test_reg[highest_sd_reg[i]].reshape(1,11)).stddev().numpy().item(),3))\n    l_high = (round((high_mean - 1.96*prob_bnn_reg(X_test_reg[highest_sd_reg[i]].reshape(1,11)).stddev().numpy().item()),3))\n    pBNN_High_CI.append([l_high,u_high])\n    \n    # Low SD\n    low_mean = prob_bnn_reg(X_test_reg[minimum_sd_reg[i]].reshape(1,11)).mean().numpy().item()\n    pBNN_Low_mean.append(low_mean)\n    u_low = bound10(round(low_mean + 1.96*prob_bnn_reg(X_test_reg[minimum_sd_reg[i]].reshape(1,11)).stddev().numpy().item(),3))\n    l_low = relu(round(low_mean - 1.96*prob_bnn_reg(X_test_reg[minimum_sd_reg[i]].reshape(1,11)).stddev().numpy().item(),3))\n    pBNN_Low_CI.append([l_low,u_low])","c462581f":"# Create and print dataframe for Low SD cases\n\nLower_pBNN_Valid = [pBNN_Low_CI[i][0] for i in range(5)]\nUpper_pBNN_Valid = [pBNN_Low_CI[i][1] for i in range(5)]\n\npd.DataFrame({\"Observation with Lowest SD\": obs_index_low,\n              \"True Value\": true_value_low,\n              \"BNN Lower\": np.round(bnn_low_sd_reg_l,3),\n              \"BNN Upper:\": np.round(bnn_low_sd_reg_u,3),\n              \"pBNN Lower\": Lower_pBNN_Valid,\n              \"pBNN Upper\": Upper_pBNN_Valid                   \n             })","25191642":"# Create and print dataframe for High SD cases\n\nLower_pBNN_Invalid = [pBNN_High_CI[i][0] for i in range(5)]\nUpper_pBNN_Invalid = [pBNN_High_CI[i][1] for i in range(5)]\n\n# Bound Cases to 0 and 10\n\nLower_pBNN_Invalid = [relu(Lower_pBNN_Invalid[i]) for i in range(5)]\nbnn_high_sd_reg_l = [bound10(bnn_high_sd_reg_l[i]) for i in range(5)]\n\npd.DataFrame({\n    \"Observations with Highest SD\" :obs_index_high,\n    \"True Value\": true_value_high,\n    \"BNN Lower\": bnn_high_sd_reg_l,\n    \"BNN Upper\": bnn_high_sd_reg_u,\n    \"pBNN Lower\": Lower_pBNN_Invalid,\n    \"pBNN Upper\": Upper_pBNN_Invalid\n})","43604b2b":"### Bayesian Neural Network Model","95f9a5b8":" ## **Load dataset and explore descriptive statistics.**","adc568a4":"Observation with Lowest SD\tTrue Value\tBNN Lower\tBNN Upper:\tpBNN Lower\tpBNN Upper\n0\t4009\t6\t3.110\t3.564\t0\t4.499\n1\t4590\t6\t2.737\t3.217\t0\t4.773\n2\t1162\t7\t2.981\t3.469\t0\t5.252\n3\t3501\t6\t3.059\t3.555\t0\t4.486\n4\t3494\t6\t3.063\t3.562\t0\t4.644","31ff1d99":"### p-BNN Regression Task","55ded845":"### PCA","9bde447a":"### Plots","0381ab4c":"Introduction of a plot function in order to visualise output distributions.","b750652a":"Visualise confusion matrix on test data - NN","7e6ff41a":"## Classification Example on wine dataset","dcd5fb1e":"### Baseline Model - Frequentistic NN","3881b404":"The baseline model will be explored and its limitations will be established in the report.","09416495":"Normal priors with mean 0 and variance 1 and posteriors with trainable parameters will be set on the weight vectors.","c83b81f7":"Scikit Learn implementation for splitting the data into parts - avoid overfitting.","19d6e615":"### Monte carlo experiment on bayesian","dc35b849":"### Bayesian NN - Regression","b9a56514":"### Invalid Predictions\nThose who did not meet the threshold criteria","ca928bb1":"# Regression task","478429a9":"### Classification -  Distribution over the output (p-BNN)","4116aa38":"The PCA plot will be reexplored at this point in order to find difficult for the BNN areas to be classified.","abdb7e9d":"As mentioned, the wine dataset (UCI Machine Learning repository) will be used in this project.","8b24b8fc":"### Feature Importance\n\nImplemented for both regression and classification tasks.","7682bec8":"## Introduction\n\nThe scope of this Notebook is to summarise the techniques I have used in my dissertation project (Anomaly Detection using Bayesian Neural Networks - MSc Statistics in Data Science - University of Edinburgh), which incorporated techniques (Bayesian Neural Networks) to capture the epistemic, reducable model variance added to the predictions generated by a small training sample etc, and aleatoric uncertainty, non-reducable noise inherent in the observations, within a sample.\n\nThe goal was to provide a 5000 word non-technical summary, aimed to be widely understandable for people of all backgrounds, that explains the phenomenon adequately.\n\n### Data\n\nThe wine dataset was used for the base feedforward neural network structure. A regression and a classification task can be implemented under this scenario.\n\n### Methods\n\nBayesian Neural Network: Priors & Posteriors set on weight vectors. The predictive output is stochastic and a Monte Carlo simulation experiment was used to help access the predictive distribution.\nProbabilistic-Bayesian Neural Network: Extension of BNN by placing a distribution on the predictive output. The predictive distribution can be accessed immediately.\n\n### Future implementations\n\nI plan to experiment in anomaly detection by using image classification examples (MNIST dataset - Bayesian Convolutional Neural Networks) and time series prediction (LSTM NNs).","d80bb209":"## Valid predictions \nClear classifications based on threshold criteria. By changing the index a different plot is shown (index = 0 being the first element of each vector).","99b184fc":"Function created for producing heatmaps.","4e1c3595":"Principal Component Analysis section. The distinction of the three wine types is evident, with areas of uncertainty (overlapping areas) inbetween."}}