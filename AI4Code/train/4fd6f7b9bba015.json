{"cell_type":{"abcf4931":"code","7e02784d":"code","8a68ce2f":"code","a1ca6d5d":"code","f90f1b9f":"code","ed146423":"code","9c768cda":"code","d8e61f16":"code","4ef6ecfd":"code","03cf835f":"code","5f69c726":"code","8b434856":"code","ea80e1fa":"code","523af70f":"code","0b932c12":"code","2c3d3c61":"code","8537c41d":"code","234502e4":"markdown","cdcfa9b3":"markdown","78ba0345":"markdown","be64cb77":"markdown","9263c95d":"markdown","c1493466":"markdown","5a52f4b5":"markdown","408ae4a7":"markdown","40bc7d30":"markdown"},"source":{"abcf4931":"import numpy as np\nfrom numpy import array\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.preprocessing.sequence import pad_sequences\n","7e02784d":"def def_model(vocab_size, inputlength, epoch=400):\n    model = Sequential()\n    model.add(Embedding(vocab_size,3,input_length=inputlength))\n    model.add(LSTM(32, return_sequences=True))\n    model.add(LSTM(64))\n    model.add(Dense(vocab_size, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model\n\ndef generate_seq(model, tokenizer, max_length, seed_text, n_words):\n    in_text = seed_text\n  # generate a fixed number of words\n    for i in range(n_words):\n    # encode the text as integer\n        encoded = tokenizer.texts_to_sequences([in_text])[0]\n    # pre-pad sequences to a fixed length\n        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre') \n        # predict probabilities for each word\n        yhat = model.predict_classes(encoded, verbose=0)\n# map predicted word index to word\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat:\n                out_word = word\n                break\n    # append to input\n        in_text += ' ' + out_word \n    return in_text","8a68ce2f":"data = \"\"\" Jack and Jill went up the hill\n    To fetch a pail of water\n    Jack fell down and broke his crown\n    And Jill came tumbling after \"\"\"","a1ca6d5d":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts([data])\nencoded = tokenizer.texts_to_sequences([data])[0]","f90f1b9f":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts([data])\nencoded = tokenizer.texts_to_sequences([data])[0]\n\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size : ',vocab_size)\n\nsequence = list()\nfor i in range(1,len(encoded)):\n    sequence.append(encoded[i-1:i+1])\n\nprint('Sequence example : ',sequence[0])\n\nprint('Sequnces : ',len(sequence))\n\nmax_length = max([len(seq) for seq in sequence])\n\nsequence = array(sequence)\n\nX, y = sequence[:,0], sequence[:,1]\n\nprint(f'X : {X[0]}, y : {y[0]}')\n\ny = to_categorical(y, num_classes=vocab_size)","ed146423":"model = def_model(vocab_size,inputlength=1)","9c768cda":"model.fit(X, y, epochs=500, verbose=2)","d8e61f16":"# evaluate model\nprint(generate_seq(model, tokenizer, max_length-1, 'Jack', 5)) \nprint(generate_seq(model, tokenizer, max_length-1, 'Jill', 3)) \nprint(generate_seq(model, tokenizer, max_length-1, 'fell', 5)) \nprint(generate_seq(model, tokenizer, max_length-1, 'pail', 5))","4ef6ecfd":"sequence = list()\n\nfor i in range(len(tokenizer.word_index)):\n    sequence.append(encoded[:i+1])\nprint('Sequnces : ',len(sequence))\nprint('Sequence example : ')\nfor i in range(5):\n    print(sequence[i])\nmax_len = max([len(seq) for seq in sequence])\nprint('Maximum Length : ',max_len)\n\nsequence = pad_sequences(sequence, maxlen=max_len, padding='pre')\n\nsequence = array(sequence)\n\nX , y = sequence[:,:-1], sequence[:,-1]\n\ny = to_categorical(y, num_classes=vocab_size)\nprint(f'X : {X[4]}, \\ny : {y[4]}')","03cf835f":"model = def_model(vocab_size, inputlength=max_len-1)","5f69c726":"model.fit(X.astype('float32'), y, verbose=2, epochs=400)","8b434856":"print(generate_seq(model, tokenizer, max_length-1, 'Jack and', 1)) \nprint(generate_seq(model, tokenizer, max_length-1, 'Jill came tumbling', 3)) \nprint(generate_seq(model, tokenizer, max_length-1, 'jack fell down', 5)) \nprint(generate_seq(model, tokenizer, max_length-1, 'pail', 4))","ea80e1fa":"# integer encode sequences of words\ntokenizer = Tokenizer() \ntokenizer.fit_on_texts([data])\nencoded = tokenizer.texts_to_sequences([data])[0] \n\n# retrieve vocabulary size\nvocab_size = len(tokenizer.word_index) + 1 \nprint('Vocabulary Size: %d' % vocab_size)\n\n# encode 2 words -> 1 word\nsequences = list()\nfor i in range(2, len(encoded)):\n    sequence = encoded[i-2:i+1]\n    sequences.append(sequence)\nprint('Total Sequences: %d' % len(sequences))","523af70f":"# pad sequences\nmax_length = max([len(seq) for seq in sequences])\nsequences = pad_sequences(sequences, maxlen=max_length, padding='pre') \nprint('Max Sequence Length: %d' % max_length)\n\n# split into input and output elements\nsequences = array(sequences)\nX, y = sequences[:,:-1],sequences[:,-1]\ny = to_categorical(y, num_classes=vocab_size)","0b932c12":"# define model\nmodel = def_model(vocab_size, max_length-1)","2c3d3c61":"# fit network\nmodel.fit(X, y, epochs=500, verbose=2)","8537c41d":"# evaluate model\nprint(generate_seq(model, tokenizer, max_length-1, 'Jack and', 5)) \nprint(generate_seq(model, tokenizer, max_length-1, 'And Jill', 3)) \nprint(generate_seq(model, tokenizer, max_length-1, 'fell down', 5)) \nprint(generate_seq(model, tokenizer, max_length-1, 'pail of', 1))","234502e4":"<h2><center> Two words in -----> one word out<\/center><h3>","cdcfa9b3":"<h3> <center>Model and Sequence generation function <\/h3>","78ba0345":"<h3> <center>Tokenizing & Generating sequences","be64cb77":"<h2> <center>One Word in -----> One word out <\/center><\/h2>\n<p>    \nWe can start with a very simple model. Given one word as input, the model will learn to predict the next word in the sequence.<\/p> \n\nFor example:\n\n![image.png](attachment:image.png)","9263c95d":"<h3><center>Introduction<\/center><\/h3>\n<ul>\n   \n<li>Language modeling involves predicting the next word in a sequence given the sequence of words already present. \n<li>A language model is a key element in many natural language processing models such as machine translation and speech recognition. \n<li>The choice of how the language model is framed must match how the language model is intended to be used. \n<li>In this tutorial, you will discover how the framing of a language model affects the skill of the model when generating short sequences from a nursery rhyme.\n   ","c1493466":"<h2><center> Sentence in -----> one word out<\/center><h3>\n   \nAnother approach is to split up the source text line-by-line, then break each line down into a series of words that build up. For example:\n       \n![image.png](attachment:image.png)","5a52f4b5":"This approach may allow the model to use the context of each line to help the model in those cases where a simple one-word-in-and-out model creates ambiguity. In this case, this comes at the cost of predicting words across lines, which might be fine for now if we are only interested in modeling and generating lines of text. ","408ae4a7":"<h3><center>Training Model","40bc7d30":"This tutorial is divided into the following parts:\n\n    1. Model 1: One-Word-In, One-Word-Out Sequences \n    2. Model 2: Line-by-Line Sequence\n    3. Model 3: Two-Words-In, One-Word-Out Sequence"}}