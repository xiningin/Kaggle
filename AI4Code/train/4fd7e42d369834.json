{"cell_type":{"c29b53d5":"code","e5857826":"code","fa4c74df":"code","a31df59a":"code","6bbe3b00":"code","cabf4b22":"code","d8c81fe3":"code","72d8a050":"code","3a954d09":"code","8670d1c0":"code","f293d5e0":"code","c942e916":"code","623cec8d":"code","6fceb026":"code","08660e2c":"code","28dd9ed7":"code","ab5d84ad":"code","a98b4252":"code","5d769aa3":"code","f5b5e865":"code","3cf4bc90":"code","843a1e51":"code","0eab7a34":"code","a1e9a443":"code","55a0d01d":"code","61e3d646":"code","adec3621":"code","f7d5e0d1":"code","2d0c5398":"code","af11b4fe":"code","239309e2":"code","863633f6":"code","94cd9068":"code","de13f240":"code","40245148":"code","969f046f":"code","df76a6a9":"code","f9c1cb9a":"code","f59de0d2":"markdown","ae12a883":"markdown","96d33787":"markdown","9be49428":"markdown","a3a1691d":"markdown","fcb4c74d":"markdown","cbebdeea":"markdown","557531db":"markdown","d0230f37":"markdown","a0d4adb4":"markdown","efc362b7":"markdown","c2294da4":"markdown","efd8de4a":"markdown","a1b7aa6d":"markdown","63a8c568":"markdown","359e5e12":"markdown","04945657":"markdown","7c30d597":"markdown","f6d6348e":"markdown","66df84a5":"markdown","bb7d7076":"markdown","ed6a5c1c":"markdown","ca4d7eb1":"markdown","e1bf95c2":"markdown","8f8498e9":"markdown","bc9426aa":"markdown"},"source":{"c29b53d5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression","e5857826":"train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')  \ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')  \n\nprint(f'Train data Shape is {train.shape}')\nprint(f'Test data Shape is {test.shape}')","fa4c74df":"def Drop(feature) :\n    global data\n    data.drop([feature],axis=1, inplace=True)\n    data.head()\n    \ndef UniqueAll(show_value = True) : \n    global data\n    for col in data.columns : \n        print(f'Length of unique data for   {col}   is    {len(data[col].unique())} ')\n        if show_value == True  : \n            print(f'unique values ae {data[col].unique()}' )\n            print('-----------------------------')\n            \ndef Encoder(feature , new_feature, drop = True) : \n    global data\n    enc  = LabelEncoder()\n    enc.fit(data[feature])\n    data[new_feature] = enc.transform(data[feature])\n    if drop == True : \n        data.drop([feature],axis=1, inplace=True)\n        \ndef CPlot(feature) : \n    global data\n    sns.countplot(x=feature, data=data,facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\", 3))\n    \ndef Mapp(feature , new_feature ,f_dict, drop_feature = True) : \n    global data\n    data[new_feature] = data[feature].map(f_dict)\n    if drop_feature == True : \n        data.drop([feature],axis=1, inplace=True)\n    else :\n        data.head()\ndef Unique(feature) : \n    global data\n    print(f'Number of unique vaure are {len(list(data[feature].unique()))} which are : \\n {list(data[feature].unique())}')","a31df59a":"train.head()","6bbe3b00":"test.head()","cabf4b22":"X_train = train.drop(['id' , 'target'], axis=1, inplace=False)\nX_test = test.drop(['id'], axis=1, inplace=False)\n\nX_train.shape , X_test.shape","d8c81fe3":"X = pd.concat([X_train , X_test])\ndel X_train\ndel X_test\nX.shape","72d8a050":"X.head()","3a954d09":"data = X","8670d1c0":"CPlot('bin_0')","f293d5e0":"CPlot('bin_1')","c942e916":"CPlot('bin_2')","623cec8d":"CPlot('bin_3')","6fceb026":"CPlot('bin_4')","08660e2c":"CPlot('nom_0')","28dd9ed7":"CPlot('nom_1')","ab5d84ad":"CPlot('nom_2')","a98b4252":"CPlot('nom_3')","5d769aa3":"CPlot('nom_4')","f5b5e865":"CPlot('ord_0')","3cf4bc90":"CPlot('ord_1')","843a1e51":"CPlot('ord_2')","0eab7a34":"CPlot('ord_3')","a1e9a443":"CPlot('ord_4')","55a0d01d":"CPlot('ord_5')","61e3d646":"data.head()","adec3621":"OHE  = OneHotEncoder()\ndata_dummies = OHE.fit_transform(data)","f7d5e0d1":"data_dummies.shape","2d0c5398":"train_data = data_dummies[:train.shape[0],:]\ntest_data=  data_dummies[train.shape[0]:,:]\ntrain_data.shape , test_data.shape","af11b4fe":"X = train_data\ny = train['target']\nX.shape , y.shape","239309e2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=44, shuffle =True)\n\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","863633f6":"LogisticRegressionModel = LogisticRegression(penalty='l2',solver='lbfgs',C=1.0,random_state=33)\nLogisticRegressionModel.fit(X_train, y_train)","94cd9068":"print('LogisticRegressionModel Train Score is : ' , LogisticRegressionModel.score(X_train, y_train))\nprint('LogisticRegressionModel Test Score is : ' , LogisticRegressionModel.score(X_test, y_test))","de13f240":"y_pred = LogisticRegressionModel.predict_proba(test_data)\ny_pred.shape","40245148":"y_pred[:,1]","969f046f":"data = pd.read_csv('..\/input\/cat-in-the-dat\/sample_submission.csv')  \n\nprint(f'Test data Shape is {data.shape}')\ndata.head()","df76a6a9":"idd = data['id']\nFinalResults = pd.DataFrame(y_pred[:,1],columns= ['target'])\nFinalResults.insert(0,'id',idd)\nFinalResults.head()","f9c1cb9a":"FinalResults.to_csv(\"sample_submission.csv\",index=False)","f59de0d2":"and now to define X & y ","ae12a883":"_____\n\n\nnow to use OneHotEncoder to transform the whole data into data_dummies","96d33787":"_____\n\nhope you find it helpful !","9be49428":"& export the result file","a3a1691d":"great , now to redefine train_data & test_data","fcb4c74d":"so now we are ready for Build the model & train the data \n\n______\n\n# Build the Model\n\nfirst to prepare the data for training by defining trainging & testing data again \n","cbebdeea":"how it looks ? ","557531db":"although ccuracy here is about 76% , but it make a better score at the real test data which is about 80%","d0230f37":"at last we concatenate id column with the result","a0d4adb4":"now for plotting some features , to be sure its values are well represented","efc362b7":"\n_____\n\nnow to predict test data , but first we have to apply same scaler model to test data","c2294da4":"____\n\nas usual , start with heading data to have a look to it ","efd8de4a":"then read the data","a1b7aa6d":"ok , now predicting testing data , using predic_proba method , to calculate the probability of having a cat ","63a8c568":"how it looks like ? ","359e5e12":"great , now to open sample_submission , to read id columns from it","04945657":"now to concatenate them together into X","7c30d597":"then to split it into training & testing data","f6d6348e":"now we can use Logistic Regression Model to traing our data","66df84a5":"____\n\n# Forming the Data\n\nsince this example depend on categorical data , we have to slice features (X) from output (y) from training data , then concatenate X from training data to features from text data . \n\n& this step to make same data processing (like label encoder & so ) for all features \n\nso first to slice X_train & X_test","bb7d7076":"what is the shape ? ","ed6a5c1c":"how is the accuracy ? ","ca4d7eb1":"# Is There a Cat in the Dat ? ( Kernel 2 )\nBy : Hesham Asem\n\n______\n\nafter using LGB Model & gaining only 77% accuracy , let's try using OneHotEncoder method then Logistic Regression \n\nlet's start by importing libraries","e1bf95c2":"300K sample size for training & 200K for testing , great . \n\nnow to define needed functions","8f8498e9":"______\n\n# Data Processing\n\n\nno we'll call it data , so it be suitable for all functions we define , which depend on global data","bc9426aa":"& here is test data"}}