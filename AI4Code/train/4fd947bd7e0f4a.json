{"cell_type":{"5ad58962":"code","de1ede71":"code","5a1fe7b5":"code","7be002fc":"code","93d03c5e":"code","58a577ab":"code","4a7a673a":"code","4cab375b":"code","9e29f8aa":"code","b742fc1c":"code","bed52c56":"code","af650d25":"code","25550c1f":"code","e30b84d7":"code","6ccc457d":"code","3693031c":"code","c62dc635":"code","64ab0a70":"code","62151138":"code","a9e45e16":"code","f7d6728b":"code","11eeaf3b":"code","a75ca7fa":"code","eee91fd0":"code","7ac9b41d":"code","5858deee":"code","38639b7c":"code","8a2a1840":"code","63cc9181":"code","83f998cf":"code","271bcdff":"code","232ee10c":"code","d39b43e1":"code","6d630b06":"code","b440a565":"code","0b460a16":"code","4706dee6":"code","985ffb37":"markdown","18fcf688":"markdown","8ce3ce93":"markdown","6a89a8a3":"markdown","c40e2b3a":"markdown","e228afa2":"markdown","1b3d9021":"markdown","a19d16b8":"markdown","2285ea9b":"markdown","6cecef2c":"markdown","01b32e98":"markdown","ae527df2":"markdown","4b8ea47b":"markdown","4cdd4913":"markdown","066baf3a":"markdown","92e2c490":"markdown","8aa40c20":"markdown","10991fc9":"markdown","604291b2":"markdown","8e900a26":"markdown","f7abc174":"markdown","9f945145":"markdown","a52b617b":"markdown","c0052ab8":"markdown"},"source":{"5ad58962":"# essentials\nimport numpy as np\nimport pandas as pd\n\n#plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#stat\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n#warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# split\nfrom sklearn.model_selection import train_test_split, KFold\n\n# model\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing  import  RobustScaler, StandardScaler\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor \nfrom mlxtend.regressor import StackingCVRegressor\nfrom lightgbm import LGBMRegressor\n\n# evaluation\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\n\n# max displayed rows\/columns\npd.set_option('display.max_column', 200)\npd.set_option('display.max_rows', 1460)","de1ede71":"# train and test files\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\n\n# submission file\nsub = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","5a1fe7b5":"train.head()","7be002fc":"train.shape","93d03c5e":"train.dtypes.value_counts()","58a577ab":"train.info()","4a7a673a":"train.describe()","4cab375b":"missing_perc = train.isna().sum()\/train.shape[0] * 100\nwith_miss = missing_perc[missing_perc > 0].sort_values(ascending=False)\nwith_miss","9e29f8aa":"plt.figure(figsize=(12,6))\nplt.xticks(rotation=45)\nsns.barplot(x=with_miss.index, y=with_miss)","b742fc1c":"plt.figure(figsize=(12,6))\nsns.distplot(train.SalePrice)","bed52c56":"print(\"Skewness: %f\" % train['SalePrice'].skew())","af650d25":"# let's create a function to plot the distribution and qq-plot for comparison\ndef plotHistProb() :\n    # Plot histogram  \n    fig = plt.figure(figsize=(15,5))\n    plt.subplot(1,2,1)\n    sns.distplot(train['SalePrice'] , fit=norm);\n    (mu, sigma) = norm.fit(train['SalePrice'])\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\n    plt.ylabel('Frequency')\n    plt.title('SalePrice distribution')\n\n    # plot probability --> qq-plot\n    plt.subplot(1,2,2)\n    res = stats.probplot(train['SalePrice'], plot=plt)\n    \n    \nplotHistProb()","25550c1f":"train.SalePrice = np.log1p(train.SalePrice)","e30b84d7":"plotHistProb()","6ccc457d":"#finding the unique values in each column (type object)\nfor col in train.select_dtypes('O').columns:\n    print('We have {} unique values in {} column : {}'.format(len(train[col].unique()),col,train[col].unique()))\n    print('__'*30)","3693031c":"plt.figure(figsize=(30,15))\n\n#sett a palette\ncmap = sns.diverging_palette(180, 30, as_cmap=True)\n\nsns.heatmap(train.corr(),\n           square=True,\n           cmap='PiYG',\n           mask=np.triu(train.corr()))","c62dc635":"corr = train.corr()\nhighest_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(train[highest_corr_features].corr(),annot=True, cmap='RdYlGn')","64ab0a70":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols])","62151138":"df = train.append(test).reset_index(drop=True)","a9e45e16":"df.isna().sum()[df.isna().sum()>0]","f7d6728b":"# Filling Categorical NaN (That we know how to fill due to the description file )\ndf['Functional'] = df['Functional'].fillna('Typ')\ndf['Electrical'] = df['Electrical'].fillna(\"SBrkr\") # Standard Circuit\ndf['KitchenQual'] = df['KitchenQual'].fillna(\"TA\")\n\ndf['Exterior1st'] = df['Exterior1st'].fillna(train['Exterior1st'].mode()[0])\ndf['Exterior2nd'] = df['Exterior2nd'].fillna(train['Exterior2nd'].mode()[0])\ndf['SaleType'] = df['SaleType'].fillna(train['SaleType'].mode()[0])\n\nfor col in [\"PoolQC\", \"Alley\", 'FireplaceQu', 'Fence', 'MiscFeature', 'GarageType', 'GarageFinish', 'GarageQual', \n            'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \"MasVnrType\"] :\n    \n    df[col] = df[col].fillna(\"None\")\n\n\nfor col in ('GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath', \n            'TotalBsmtSF', \"MasVnrArea\"):\n    df[col] = df[col].fillna(0) # no basement or no garage\n\ndf['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\ndf['MSZoning'] = df.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","11eeaf3b":"to_drop = ['GarageYrBlt','YearRemodAdd', 'Utilities'] \ndf = df.drop(to_drop, axis = 1)","a75ca7fa":"df.isna().sum()[df.isna().sum()>0]","eee91fd0":"df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\ndf['Total_Home_Quality'] = df['OverallQual'] + df['OverallCond']","7ac9b41d":"df['MSSubClass'] = df['MSSubClass'].apply(str)\ndf['OverallCond'] = df['OverallCond'].astype(str)\ndf['YrSold'] = df['YrSold'].apply(str)\ndf['MoSold'] = df['MoSold'].apply(str)","5858deee":"# Creating dummy variables from categorical features\ndf = pd.get_dummies(df)","38639b7c":"df.head()","8a2a1840":"numeric_features = df.dtypes[df.dtypes != object].index\nskewed_features = df[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index\n\n\n# Normalize skewed features using log_transformation\n    \nfor i in skew_index:\n    df[i] = np.log1p(df[i])","63cc9181":"outliers = [30, 88, 462, 631, 1322]\n\ndf = df.drop(df.index[outliers])\noverfit = []\nfor i in df.columns:\n    counts = df[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(df) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\ndf = df.drop(overfit, axis=1)\n#X_sub = X_sub.drop(overfit, axis=1)","83f998cf":"n = len(train)\ntrain = df[:n-5]\ntest = df[n-5:]","271bcdff":"X = train.drop('SalePrice', axis = 1)\ny = train.SalePrice\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","232ee10c":"base_models = (KernelRidge(),\n               make_pipeline(RobustScaler(),Lasso(alpha=0.0005, random_state=1)),\n               make_pipeline(RobustScaler(),ElasticNet(alpha=0.0005, l1_ratio=0.9)),             \n               make_pipeline(StandardScaler(),GradientBoostingRegressor(learning_rate=0.005, \n                                                                        loss='huber',\n                                                                        max_depth=4, \n                                                                        max_features='sqrt',\n                                                                        min_samples_leaf=15,\n                                                                        min_samples_split=10,\n                                                                        n_estimators=3000,\n                                                                        random_state=1)))","d39b43e1":"meta_model = LGBMRegressor(bagging_fraction=0.8, bagging_freq=5, \n                           feature_fraction=0.2319, feature_fraction_seed=9,\n                           learning_rate=0.05, max_bin=55, min_data_in_leaf=6,\n                           min_sum_hessian_in_leaf=11, n_estimators=720, num_leaves=5,\n                           bagging_seed=9,objective='regression')","6d630b06":"#Building the stacking model\nkfold = KFold(n_splits=10)\n\nstack = StackingCVRegressor(regressors=base_models,\n                            meta_regressor=meta_model, \n                            use_features_in_secondary=True,\n                            store_train_meta_features=True,\n                            shuffle=False,cv=kfold,\n                            random_state=1)","b440a565":"#fitting the model to our data\nstack.fit(x_train,y_train)\npredictions = stack.predict(x_test)\n\nMSE = mean_squared_error(y_test,predictions)\nRMSE = np.sqrt(MSE)\nRMSE","0b460a16":"pred_sub = stack.predict(test.drop('SalePrice', axis = 1))\nsub.SalePrice = np.expm1(pred_sub)","4706dee6":"sub.to_csv('submission.csv' , index=False)","985ffb37":"<a id=\"head-1-2\"><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\"><b>Pre-processing<\/b><\/p>","18fcf688":"<a id=\"head-1-2\"><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\"><b>EDA  \ud83d\udd0d \ud83d\udd0d<\/b><\/p>\n\n- **Target** : SalePrice\n- **Number of rows and columns** : (1460, 80)\n- **Type of variables**:\n\n     Qualitative : 43  \n     Quantitative : 37\n     \n     \n- **Nan values** : more than 80% in the following columns :\nPoolQC, MiscFeature, Alley and Fence \n","8ce3ce93":"### Encoding :","6a89a8a3":"So yeah our target is right-skewed! And we are going to correct it now.","c40e2b3a":"It seems like adding total sqfootage and Total Home Quality features will improve the result","e228afa2":"What we really need from this heatmap? we need the highest correlation between features and SalesPrice, so let's do it.","1b3d9021":"<p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Importing Libraries & Data<\/b><\/p>\n","a19d16b8":"### Objective :\n*Predict sales prices, practice more detailled pre-processing and train with Stacking model !*\n","2285ea9b":"- Imputation\n- Feature Engineering\n- Encoding\n- Skewness \/ Normalization \n- Outliers \/ Columns full with zeros\n- Train-Test split\n","6cecef2c":"### Feature Engineering :","01b32e98":"#### Understand variables :","ae527df2":"We will check again if there is more Nan values :","4b8ea47b":"### Outliers \/ Columns full with zeros:","4cdd4913":"Let's convert non-numeric predictors stored as numbers into string :","066baf3a":"Our target variable does not have a normal distribution!","92e2c490":"Let's examine now the correlation between dependent and independent variables","8aa40c20":"Let's Fix now The Skewness in the other features, then normalize it","10991fc9":"### Train-test split :","604291b2":"### Skewness \/ Normalization :","8e900a26":"#### If you find this nnotebook useful, please don't forget to upvote it!","f7abc174":"### Imputation :","9f945145":"#### Target visualization :","a52b617b":"Let's examine more the highest correlated features :","c0052ab8":"<a id=\"head-1-2\"><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\"><b>Modeling and Evaluation<\/b><\/p>\n\n"}}