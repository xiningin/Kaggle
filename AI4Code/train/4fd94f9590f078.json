{"cell_type":{"1cdb8c14":"code","386b77d1":"code","447a6072":"code","acc4b37c":"code","ec1371b8":"code","dbfea17c":"code","4d06598c":"code","093d65e9":"code","33b658d5":"code","4cf8eca9":"code","0751c144":"code","a95573ca":"code","6532675d":"code","3db0b960":"code","23d7fe61":"code","3e2ed5b4":"code","7dde61c2":"code","877fc592":"code","6b2966d9":"code","966af067":"code","ccee9593":"code","e65f39cb":"code","4a1d777e":"code","68caf8a0":"code","845e6704":"code","4330e807":"code","3c881097":"code","6dcc1da9":"code","6c618172":"code","26ab2aff":"code","89529596":"code","c0c1c702":"code","7601d1f5":"markdown","4022c338":"markdown","85026939":"markdown","c0bb24a4":"markdown","8cc95641":"markdown","98e6ebed":"markdown","455560d3":"markdown","e7d0ce08":"markdown","559d7259":"markdown","745149b8":"markdown","2225a3fa":"markdown","d15172cb":"markdown","07b9f924":"markdown","16b0b5b3":"markdown","bad8333d":"markdown","a7861174":"markdown","d08f5928":"markdown","d3ff6692":"markdown","c96bd948":"markdown","3fe20ff8":"markdown","f2bc6de7":"markdown","5cc1c036":"markdown","34636d26":"markdown","c0920cb4":"markdown","d32b7f78":"markdown","c68eb9a2":"markdown"},"source":{"1cdb8c14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","386b77d1":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","447a6072":"sentences = [\n    'I love my dog',\n    'I love my cat',\n    'You love my dog!',\n    'Do you think my dog is amazing?'\n]\ntokenizer=Tokenizer(num_words=100)\n# Encodes the sentences\ntokenizer.fit_on_texts(sentences)\nword_to_index=tokenizer.word_index\nprint(word_to_index)\n# Tokenizer turn the sentences into a set of sequences.\nsequences=tokenizer.texts_to_sequences(sentences)\nprint(sequences)","acc4b37c":"test_text=['I really love my dog','My dog loves my manatee']\ntest_seq=tokenizer.texts_to_sequences(test_text)\nprint(test_text)\nprint(test_seq)","ec1371b8":"tokenizer = Tokenizer(num_words=100,oov_token='<OOV>')\ntokenizer.fit_on_texts(sentences)\nword_to_index = tokenizer.word_index\n\nsequences = tokenizer.texts_to_sequences(sentences)\ntest_seq = tokenizer.texts_to_sequences(test_text)\n\nprint(word_to_index)\nprint(test_seq)","dbfea17c":"padded = pad_sequences(sequences)\n\nprint('\\nDictionary: ',word_to_index)\nprint('\\n',sequences)\nprint('\\n',padded)","4d06598c":"padded = pad_sequences(sequences, padding='post',maxlen=5,truncating='post')\nprint(padded)","093d65e9":"df = pd.read_json(\"..\/input\/Sarcasm_Headlines_Dataset_v2.json\",lines=True)\ndf.head()","33b658d5":"sentences = df['headline']\nlabel =df['is_sarcastic']\nprint('number of sentences: ',len(sentences))\nprint(sentences[3])\nprint(label[3])","4cf8eca9":"idx = np.arange(len(sentences))\nnp.random.shuffle(idx)\nsentences = sentences.iloc[idx]\nlabel = label.iloc[idx]\nsentences.reset_index(drop=True,inplace=True)\nlabel.reset_index(drop=True,inplace=True)\nprint(sentences[3])\nprint(label[3])\n\ntrain_set = sentences[:22890]\ntrain_label = label[:22890]\ntest_set = sentences[22890:]\ntest_label = label[22890:]\nprint('number of sentences in training: ', len(train_set))\nprint('number of sentences in test: ', len(test_set))\n\ntrain_label=np.array(train_label)\ntest_label=np.array(test_label)","0751c144":"# Train set tokenize\ntokenizer = Tokenizer(oov_token='<OOV>')\ntokenizer.fit_on_texts(train_set)\nword_to_index = tokenizer.word_index\nprint('Length of dictionary is ',len(word_to_index), ' words.')\ntrain_sequences = tokenizer.texts_to_sequences(train_set)\ntrain_padded = pad_sequences(train_sequences, padding='post',maxlen=150,truncating='post')\nprint('\\n',train_sequences[3])\nprint(train_padded[3])\nprint('\\n Train matrix dimension: ',train_padded.shape)\n\n# Test set tokenize\ntest_sequences = tokenizer.texts_to_sequences(test_set)\ntest_padded = pad_sequences(test_sequences, padding='post')\nprint('\\n Test matrix dimension: ',test_padded.shape)","a95573ca":"vocab_size = len(word_to_index)\nembedding_dim = 100\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, embedding_dim))\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(keras.layers.Dense(100, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\nmodel.summary()\n# Input shape: (batch_size, input_length);\n# Output shape: (batch_size, input_length, output_dim)","6532675d":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","3db0b960":"history = model.fit(train_padded,\n                    train_label,\n                    epochs=10,\n                    batch_size=64,\n                    validation_data=(test_padded, test_label),\n                    verbose=1)","23d7fe61":"history_dict = history.history\nhistory_dict.keys()","3e2ed5b4":"import matplotlib.pyplot as plt\n\nacc = history_dict['acc']\nval_acc = history_dict['val_acc']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n","7dde61c2":"# The patience parameter is the amount of epochs to check for improvement\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\nmodel = keras.Sequential([keras.layers.Embedding(vocab_size, embedding_dim),\n                         keras.layers.GlobalAveragePooling1D(),\n                         keras.layers.Dense(100, activation=tf.nn.relu),\n                         keras.layers.Dense(1, activation=tf.nn.sigmoid)])\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\nhistory = model.fit(train_padded,\n                    train_label,\n                    epochs=20,\n                    batch_size=64,\n                    validation_data=(test_padded, test_label),\n                    callbacks=[early_stop],\n                    verbose=1)","877fc592":"val_acc_early = history.history['val_acc'][-1]","6b2966d9":"model = keras.models.Sequential([\n    keras.layers.Embedding(vocab_size, embedding_dim),\n    keras.layers.GlobalAveragePooling1D(),\n    keras.layers.Dense(100, kernel_regularizer=keras.regularizers.l2(0.003),\n                       activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid, kernel_regularizer=keras.regularizers.l2(0.003))\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\nhistory = model.fit(train_padded,\n                    train_label,\n                    epochs=10,\n                    batch_size=64,\n                    validation_data=(test_padded, test_label),\n                    verbose=1)","966af067":"val_acc_reg = history.history['val_acc'][-1]","ccee9593":"model = keras.models.Sequential([\n    keras.layers.Embedding(vocab_size, embedding_dim),\n    keras.layers.GlobalAveragePooling1D(),\n    keras.layers.Dense(100, activation=tf.nn.relu),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(train_padded,\n                    train_label,\n                    epochs=10,\n                    batch_size=64,\n                    validation_data=(test_padded, test_label),\n                    verbose=1)","e65f39cb":"val_acc_drop = history.history['val_acc'][-1]","4a1d777e":"print('Accuracy with early stopping:',val_acc_early)\nprint('Accuracy with regularization:',val_acc_reg)\nprint('Accuracy with drop out:      ',val_acc_drop)","68caf8a0":"model = keras.models.Sequential([\n    keras.layers.Embedding(vocab_size, embedding_dim),\n    keras.layers.GlobalAveragePooling1D(),\n    keras.layers.Dense(100, kernel_regularizer=keras.regularizers.l2(0.003),\n                       activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid, kernel_regularizer=keras.regularizers.l2(0.003))\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\nhistory = model.fit(train_padded,\n                    train_label,\n                    epochs=10,\n                    batch_size=64,\n                    validation_data=(test_padded, test_label),\n                    verbose=1)\nhistory_dict = history.history\nacc = history_dict['acc']\nval_acc = history_dict['val_acc']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","845e6704":"index_to_word = dict([(value, key) for (key, value) in word_to_index.items()])\n\ndef decode_text(text):\n    return ' '.join([index_to_word.get(i,'') for i in text])\n\nprint(test_padded[3])\ndecode_text(test_padded[3])","4330e807":"e=model.layers[0]\nembeddings = e.get_weights()[0]\nprint(embeddings.shape) ","3c881097":"indexWord=5000\nprint('The word',decode_text([indexWord]),'has embedding vector:')\na=embeddings[indexWord]\nprint('\\n',a)\nb=embeddings[indexWord+1]\ndista = np.linalg.norm(a)\ndistb = np.linalg.norm(b)\nsimilarity=a.dot(b)\/(dista*distb)\nprint('\\nThe similarity between word',decode_text([indexWord]),'and',decode_text([indexWord+1]),'is',similarity)","6dcc1da9":"# Function to draw visualization of distance between embeddings.\n  def plot_with_labels(low_dim_embs, labels):\n    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n    plt.figure(figsize=(18, 18))  # in inches\n    for i, label in enumerate(labels):\n        x, y = low_dim_embs[i, :]\n        plt.scatter(x, y)\n        plt.annotate(\n          label,\n          xy=(x, y),\n          xytext=(5, 2),\n          textcoords='offset points',\n          ha='right',\n          va='bottom')","6c618172":"from sklearn.manifold import TSNE\n\nplot_only = 500\ntsne = TSNE(perplexity=50, n_components=2, init='pca', n_iter=7000, method='exact')\nlow_dim_embs = tsne.fit_transform(embeddings[:plot_only, :])\nlabels = [index_to_word[i] for i in range(1,plot_only)]\nplot_with_labels(low_dim_embs, labels)","26ab2aff":"test_predictions = model.predict_classes(test_padded)\npred=np.squeeze(test_predictions)\nerror =  pred != test_label\nwrong_answers=np.sum(error)\/len(error)\nprint('Wrong answers:',wrong_answers*100,'%')\n\nresults = model.evaluate(test_padded, test_label)\nprint('The accuracy of the model:',results[1])\nprint('The loss of the model:',results[0])","89529596":"index=500\nprint(decode_text(test_padded[index]))\nanswer=pred[index]\nif 1==answer:\n    print('It\\'s sarcastic :)')\nelse: \n    print('No sarcastic :(')\n\nprint('Right answer:',test_label[index])","c0c1c702":"sentence=['I am so clever that sometimes I do not understand a single word of what I am saying.',\n         'Have no fear of perfection, you\\'ll never reach it.',\n         'If you\\'re going to tell people the truth, be funny or they\\'ll kill you.',\n         'I like long walks, especially when they are taken by people who annoy me',\n         'Only two things are infinite, the universe and human stupidity, and I\\'m not sure about the former.',\n         'One of the hardest things to imagine is that you are not smarter than average.']\nsequences = tokenizer.texts_to_sequences(sentence)\npadded = pad_sequences(sequences, padding='post',maxlen=100)\np=np.squeeze(model.predict(padded))\nfor i in range(len(sentence)):\n    print('\\n',sentence[i],'\\n','(probability of sarcasm --> {:.2%})'.format(p[i]))","7601d1f5":"**Create a graph of accuracy and loss over time**\n\n***model.fit()*** returns a History object that contains a dictionary with everything that happened during training. There are four entries: one for each monitored metric during training and validation. We can use these to plot the training and validation loss for comparison, as well as the training and validation accuracy:","4022c338":"**Tokenizing**\n\n*Tokenizer* generates a dictionary of word encodings and creates vectors out of sentences. The parameter num_words is the number of distinct words in the vocabulary. The tokenizer takes the top num_words words by frequency and just encode those, the most common words. It is a handy shortcut when dealing with lots of data, sometimes the impact of words less frequency can be minimal in training accuracy but huge in training time. But use it carefully. Word index property returns a dictionary containing a key value pairs, where the key is the word and the value is the index (token). Tokenizer lower case all the words and deletes punctuation.","85026939":"**Padding**\n\nWhen we fed some texts into the neuronal network, we need them to be uniform in size. This is done with *pad_sequences* once the tokenizer has created the sequence. Sentences get padded to the length of the longest sequence by adding zeros to the beginning of shorter ones by default.\n","c0bb24a4":"In this case the best strategies is regularization L2. So re-run the model with this strategy.\n","8cc95641":"## Sarcasm detection ##\n\nNow we use sarcasm dataset to preprocess a lot of sentences the same manner.","98e6ebed":"**Build the model**\n\nThe neural network is created by stacking layers with *Sequential*, this requires two main architectural decisions:\n\n\u2022 How many layers to use in the model?\n\n\u2022 How many hidden units to use for each layer?\n\nThe input data consists of an array of word-indices. The labels to predict are either 0 (no sarcasm) or 1 (sarcasm). Let's build a model for this problem:","455560d3":"Tokenizer encodes text, with his dictionary build by fit_on_text method, in integer lists, where tokens replace words. If there are words not learned by tokenizer it remove them out, because for it they are meaningless.","e7d0ce08":"Now the list of sentences is a Matrix, with the column width equal the longest sentence. \nYou can set padding after the sentences and set a maximum length for the sentences with maxlen parameter. If the sentence is longer than the maxlen it gets truncated, so you have to set from where with truncating parameter (default is pre)","559d7259":"Test the model with this sentence:","745149b8":"**Train the model**\n\nTrain the model for 10 epochs in mini-batches of 64 sentences. While training, monitor the model's loss and accuracy on the data from the test set:","2225a3fa":"**Visualize the embeddings**\n\nLet's retrieve the word embeddings learned during training. Thi is a matrix of shape (vocab_size, embedding_dim)","d15172cb":"The layers are stacked sequentially to build the classifier:\n\n1. The first layer is an **[Embedding](http:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Embedding?)** layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model is training. The vectors add a dimension to the output array. The resulting dimensions are: (batch size, sequence, embedding size). **embedding_dim** sets the number of features for words (number of hidden units). The result of embedding is a matrix with length of dictionary and embedding dimension, the total number of parameters is dictionary size * embeddings size.\n\n2. Next, a **GlobalAveragePooling1D** layer returns a fixed-length output vector for each example by averaging over the sequence dimension with output shape (batch_size, features). This allows the model to handle input of variable length, in the simplest way possible to feed into next layer.\n\n3. This fixed-length output vector is piped through a fully-connected (**Dense**) layer with 100 hidden units.\n \n4. The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability.\n\nWhile training this model the meaning of the words can come from the labeling of dataset and over time the embedding layer learns the feature vectors of the words beginning to cluster similar words with their associated sentiment.","07b9f924":"We split the sentences in training and testing data shuffling them.","16b0b5b3":"Now the tokenizer gets all words in the **training** dataset, because there is a variety of words to encode.","bad8333d":"** Regularization**\n\nGiven some training data and a network architecture, there are infinite sets of weights values (infinite models) that could explain the data, and simpler models are less likely to overfit than complex ones. Thus reducing the capacity of the model prevents overfitting. The capacity is the number of learnable parameters in a model. A model with more parameters will have more \"memorization capacity\" and therefore will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any generalization power, but this would be useless when making predictions on previously unseen data. On the other hand, if the network has limited memorization resources, it will not be able to learn the mapping as easily. To minimize its loss, it will have to learn compressed representations that have more predictive power. At the same time, if you make your model too small, it will have difficulty fitting to the training data. Too much capacity gives overfitting and not enough capacity gives underfitting. You will have to experiment using a series of different architectures in terms of the number of layers, or the right size for each layer to find an appropriate model size: it's best to start with relatively few layers and parameters, then begin increasing the size of the layers or adding new layers until you see diminishing returns on the validation loss.\n\nAnother common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more \"regular\". This is done by adding to the loss function of the network a penalty for having large weights.\n\n\u2022 L1 regularization, where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n\n\u2022 L2 regularization, where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the squared \"L2 norm\" of the weights). L2 regularization is also called weight decay.\n\nIn tf.keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Let's add L2 weight regularization now.\nl2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value^2 to the total loss of the network. Because this penalty is only added at training time, the loss for this network might be much higher at training than at test time","a7861174":"To show the 100 dimensional space of embeddings we use TSNE technique. Points in this space are words. The words tend to cluster in the two groups \"sarcastic\" and \"not sarcastic\". The closeness between the words indicates a certain similarity or the same contexts in sentences that the network has seen.","d08f5928":"**Drop out**\n\nDropout, applied to a layer during training, consists of randomly \"dropping out\" (i.e. set to zero) a number of output features of the layer during training specified by the \"dropout rate\", the fraction of the features that are being zeroed-out.\nAs neurons are randomly dropped out of the network other neurons will have to step in and handle the representation required to make predictions for the missing neurons the effect is that the network becomes less sensitive to the specific weights of neurons and network is capable of better generalization. At test time, no units are dropped out.\nIn tf.keras you can introduce dropout in a network via the Dropout layer, which gets applied to the output of layer right before.","d3ff6692":"**Word Embeddings**\n\n*Embeddings* represent the semantic of a word into a vector in n-dimensional space, instead of representing a word with only one integer. It is a featurized representation of the words, if two words have most of the features the same values, then they are quite similar and their verctors are close in n-dimensional space (as in figure A bottom), otherwise they are not similar and their vectors are far away (as in figure B). The similarity between two words can be measured from the angle between them (cosine similarity). In this way embedding table  groups together words with similar semantic meanings.\n\n![cosine_sim.png](attachment:cosine_sim.png)","c96bd948":"**Early stopping**\n\nIf you train for too long the model will start to overfit and learn patterns from the training data that don't generalize to the test data. We need to understand what is the appropriate number of epochs to learn the pattern data to generalize better on unseen data. [EarlyStopping](http:\/\/https:\/\/www.tensorflow.org\/versions\/master\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping) callback tests a training condition for every epoch: if a set amount of epochs elapses without showing improvement, then automatically stop the training. ","3fe20ff8":"**Convert the integers back to words**\n\nNow we have neuronal network. It may be useful to know how to convert integers back to text. Here, we'll create a helper function to decode a sequence of integers in sentence:","f2bc6de7":"To avoid removing some unseen words in the sequence:\n*      take a lot of training data (large corpus text) to get a broad vocabulary\n*      put a special value in the vocabulary to use when an unseen word is encountered with the property oov_token in the tokenizer constructor.","5cc1c036":"**Compile the model**\n\nA model needs a loss function and an **[optimizer](http:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers)** for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the binary_crossentropy loss function. It measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions. Now, we configure the model to train using an optimizer and a loss function and a metrics to evaluate the training:","34636d26":"**Test the model**\n\nTest the model with test dataset.","c0920cb4":"**Deal with overfitting**\n\nThe accuracy of our model on the validation data would peak after training for a number of epochs, and would then start decreasing. The difference between training and validation loss signals overfitting: high performance on training set but low on validation set with unseen data.\nThe strategies to reduce this issue are:\n* more training data;\n* early stopping;\n* regularization;\n* dropping out.\n\nTo prevent overfitting, the best solution is to use more training data. A model trained on more data will naturally generalize better. When that is no longer possible, the next best solution is to use techniques like regularization and early stopping.\n","d32b7f78":"Now we see the embedding of a particular word and its similarity with another word in the dictionary. The similarity is computed by the cosine that misure the angle between the two word embedding vectors. If the similarity is near to 1 than the two words are similar, if it is near to 0 than they are nont similar. if the similarity has negative sign then the two words are opposite meaning.   ","c68eb9a2":"Embeddings are just a fully connected layer the only difference than the fully connected layer in a standard neuronal network is it has not activation function: we use embeddings as a lookup table. See the figure bottom, for example \"heart\" is encoded as 958 in a dictionary of length 10000 words. Then to get hidden layer values for \"heart\", you just take the 958th row of the embedding matrix\n\n![tokenize_lookup.png](attachment:tokenize_lookup.png)\n\nThe embedding is just a weight matrix. The lookup is just a shortcut for the matrix multiplication. The lookup table is trained just like any weight matrix."}}