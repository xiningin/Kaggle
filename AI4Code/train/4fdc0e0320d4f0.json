{"cell_type":{"9c131f95":"code","a5315775":"code","4fbff489":"code","a5feb0e7":"code","19652234":"code","f6e4fa88":"code","5ef29841":"code","f9dcf953":"code","28c65e24":"code","f6f9d9c8":"code","44a19ed4":"code","70499cc6":"code","ae5ece04":"code","2e09400a":"code","f89c34bc":"code","5b38f919":"code","7ce332c1":"code","c1d0bf46":"code","fdd0d191":"code","f4e14afb":"code","4910b8ac":"code","fb7d9b54":"code","c37cd6be":"code","3caf162e":"code","b606d879":"code","35dba0cb":"code","1625f2fb":"code","fd4bcc7a":"code","62ef0223":"code","a64f44f1":"code","945ad127":"code","f1133cdf":"code","31171176":"markdown","00e9f5c7":"markdown","a17d2605":"markdown","74cd37f2":"markdown","91af78f1":"markdown","fb0a9146":"markdown","3e646411":"markdown","01793ce9":"markdown","c471c0aa":"markdown","c87390c5":"markdown"},"source":{"9c131f95":"import warnings\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier","a5315775":"PATH_TO_DATA = Path('..\/input\/flight-delays-fall-2018\/')","4fbff489":"train_df = pd.read_csv(PATH_TO_DATA \/ 'flight_delays_train.csv')","a5feb0e7":"train_df.head()","19652234":"train_df['target'] = train_df['dep_delayed_15min'].map({'Y':1, 'N':0})\ntrain_df.head()","f6e4fa88":"test_df = pd.read_csv(PATH_TO_DATA \/ 'flight_delays_test.csv')","5ef29841":"test_df.head()","f9dcf953":"train_df['flight'] = train_df['Origin'] + '-->' + train_df['Dest']\ntest_df['flight'] = test_df['Origin'] + '-->' + test_df['Dest']","28c65e24":"(train_df.groupby('flight')['target'].sum()\/train_df.groupby('flight')['target'].count()).sort_values(ascending =False)","f6f9d9c8":"import seaborn as sns","44a19ed4":"train_df.groupby('target')['Distance'].median()","70499cc6":"train_df['hour'] = train_df['DepTime']\/\/100\ntest_df['hour'] = test_df['DepTime']\/\/100\n\n#(train_df.groupby('hour')['target'].sum()\/train_df.groupby('hour')['target'].count()).sort_values(ascending =False)","ae5ece04":"train_df['Month'] = train_df['Month'].str.replace(r'\\D', '')\ntrain_df['DayofMonth'] = train_df['DayofMonth'].str.replace(r'\\D', '')\ntrain_df['DayOfWeek'] = train_df['DayOfWeek'].str.replace(r'\\D', '')\ntest_df['Month'] = train_df['Month'].str.replace(r'\\D', '')\ntest_df['DayofMonth'] = train_df['DayofMonth'].str.replace(r'\\D', '')\ntest_df['DayOfWeek'] = train_df['DayOfWeek'].str.replace(r'\\D', '')\n\ntrain_df.head()","2e09400a":"\ntrain_df.loc[train_df['hour'] == 24, 'hour'] = 0\ntrain_df.loc[train_df['hour'] == 25, 'hour'] = 1\ntrain_df['minute'] = train_df['DepTime'] % 100\ntest_df.loc[test_df['hour'] == 24, 'hour'] = 0\ntest_df.loc[test_df['hour'] == 25, 'hour'] = 1\ntest_df['minute'] = test_df['DepTime'] % 100\n\n# Season\ntrain_df['summer'] = (train_df['Month'].isin(['6', '7', '8'])).astype(np.int32)\ntrain_df['autumn'] = (train_df['Month'].isin(['9', '10', '11'])).astype(np.int32)\ntrain_df['winter'] = (train_df['Month'].isin(['12', '1', '2'])).astype(np.int32)\ntrain_df['spring'] = (train_df['Month'].isin(['3', '4', '5'])).astype(np.int32)\ntest_df['summer'] = (test_df['Month'].isin(['6', '7', '8'])).astype(np.int32)\ntest_df['autumn'] = (test_df['Month'].isin(['9', '10', '11'])).astype(np.int32)\ntest_df['winter'] = (test_df['Month'].isin(['12', '1', '2'])).astype(np.int32)\ntest_df['spring'] = (test_df['Month'].isin(['3', '4', '5'])).astype(np.int32)\ntrain_df.head()","f89c34bc":"test_df.head()","5b38f919":"temp = train_df.groupby('UniqueCarrier')['Distance'].sum()\n\ntrain_df['Summa'] = train_df['UniqueCarrier'].map(temp.to_dict())\n\ntemptest = test_df.groupby('UniqueCarrier')['Distance'].sum()\ntest_df['Summa'] = test_df['UniqueCarrier'].map(temptest.to_dict())\ntest_df.head()","7ce332c1":"test_df.drop('DepTime',axis =1)\ntrain_df.drop('DepTime',axis =1)","c1d0bf46":"train_df.drop('DepTime', axis =1)","fdd0d191":"sns.distplot(train_df['Distance'])","f4e14afb":"train_df = train_df.drop('target', axis=1)","4910b8ac":"categ_feat_idx = np.array([0,1,2,3,4,5,6,8,9,10,11,12,13,14,15])\ncateg_feat_idx","fb7d9b54":"X_train = train_df.drop('dep_delayed_15min', axis=1).values\ny_train = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values\nX_test = test_df.values","c37cd6be":"X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_train, y_train, \n                                                                test_size=0.3, \n                                                                random_state=17)","3caf162e":"ctb1 = CatBoostClassifier(random_seed=17, silent=True)","b606d879":"%%time\nctb1.fit(X_train_part, y_train_part,\n        cat_features=categ_feat_idx);","35dba0cb":"ctb1_valid_pred = ctb1.predict_proba(X_valid)[:, 1]\nctb1_valid_pred.sum()","1625f2fb":"roc_auc_score(y_valid, ctb1_valid_pred)","fd4bcc7a":"%%time\nctb1.fit(X_train, y_train,\n        cat_features=categ_feat_idx);","62ef0223":"ctb_pred = ctb1.predict_proba(X_train)[:, 1]\nroc_auc_score(y_train, ctb_pred)","a64f44f1":"ctb_test_pred = ctb1.predict_proba(X_test)[:, 1]","945ad127":"with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    sample_sub = pd.read_csv(PATH_TO_DATA \/ 'sample_submission.csv', \n                             index_col='id')\n    sample_sub['dep_delayed_15min'] = ctb_test_pred\n    sample_sub.to_csv('ctb_test_pred.csv')","f1133cdf":"!head ctb_test_pred.csv","31171176":"**Remember indexes of categorical features (to be passed to CatBoost)**","00e9f5c7":"**Train Catboost with default arguments, passing only the indexes of categorical features.**","a17d2605":"**Read the data**","74cd37f2":"**We got some 0.756 ROC AUC on the hold-out set.**","91af78f1":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\" \/>\n<\/center> \n     \n## <center>  [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \n\n#### <center> Author: [Yury Kashnitsky](https:\/\/yorko.github.io) (@yorko) \n\n# <center>Assignment #2. Fall 2019\n## <center> Part 2. Gradient boosting","fb0a9146":"Now's your turn! Go and improve the model to beat **\"A2 baseline (10 credits)\"** - **0.75914** LB score. It's crucial to come up with some good features. \n\nFor discussions, stick to the **#a2_kaggle_fall2019** thread in the **mlcourse_ai_news** [ODS Slack](http:\/\/opendatascience.slack.com) channel. Serhii Romanenko (@serhii_romanenko) will be there to help. \n\nWelcome to Kaggle!\n\n<img src='https:\/\/habrastorage.org\/webt\/fs\/42\/ms\/fs42ms0r7qsoj-da4x7yfntwrbq.jpeg' width=50%>\n*from the [\"Nerd Laughing Loud\"](https:\/\/www.kaggle.com\/general\/76963) thread.*","3e646411":"**Create only one feature - \u201cflight\u201d (this you need to improve - add more features)**","01793ce9":"**In this assignment, you're asked to beat a baseline in the [\"Flight delays\" competition](https:\/\/www.kaggle.com\/c\/flight-delays-fall-2018).**\n\nThis time we decided to share a pretty decent CatBoost baseline, you'll have to improve the provided solution.\n\nPrior to working on the assignment, you'd better check out the corresponding course material:\n 1. [Classification, Decision Trees and k Nearest Neighbors](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse_open\/blob\/master\/jupyter_english\/topic03_decision_trees_kNN\/topic3_decision_trees_kNN.ipynb?flush_cache=true), the same as an interactive web-based [Kaggle Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-3-decision-trees-and-knn) \n 2. Ensembles:\n  - [Bagging](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse_open\/blob\/master\/jupyter_english\/topic05_ensembles_random_forests\/topic5_part1_bagging.ipynb?flush_cache=true), the same as a [Kaggle Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-5-ensembles-part-1-bagging)\n  - [Random Forest](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse_open\/blob\/master\/jupyter_english\/topic05_ensembles_random_forests\/topic5_part2_random_forest.ipynb?flush_cache=true), the same as a [Kaggle Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-5-ensembles-part-2-random-forest)\n  - [Feature Importance](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse_open\/blob\/master\/jupyter_english\/topic05_ensembles_random_forests\/topic5_part3_feature_importance.ipynb?flush_cache=true), the same as a [Kaggle Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-5-ensembles-part-3-feature-importance)\n 3. - [Gradient boosting](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse_open\/blob\/master\/jupyter_english\/topic10_boosting\/topic10_gradient_boosting.ipynb?flush_cache=true), the same as a [Kaggle Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-10-gradient-boosting) \n   - Logistic regression, Random Forest, and LightGBM in the \"Kaggle Forest Cover Type Prediction\" competition: [Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-10-practice-with-logit-rf-and-lightgbm) \n 4. You can also practice with demo assignments, which are simpler and already shared with solutions:\n  - \"Decision trees with a toy task and the UCI Adult dataset\": [assignment](https:\/\/www.kaggle.com\/kashnitsky\/a3-demo-decision-trees) + [solution](https:\/\/www.kaggle.com\/kashnitsky\/a3-demo-decision-trees-solution)\n  - \"Logistic Regression and Random Forest in the credit scoring problem\": [assignment](https:\/\/www.kaggle.com\/kashnitsky\/assignment-5-logit-and-rf-for-credit-scoring) + [solution](https:\/\/www.kaggle.com\/kashnitsky\/a5-demo-logit-and-rf-for-credit-scoring-sol)\n 5. There are also 7 video lectures on trees, forests, boosting and their applications: [mlcourse.ai\/video](https:\/\/mlcourse.ai\/video) \n 6. mlcourse.ai tutorials on [categorical feature encoding](https:\/\/www.kaggle.com\/waydeherman\/tutorial-categorical-encoding) (by Wayde Herman) and [CatBoost](https:\/\/www.kaggle.com\/mitribunskiy\/tutorial-catboost-overview) (by Mikhail Tribunskiy)\n 7. Last but not the least: [Public Kernels](https:\/\/www.kaggle.com\/c\/flight-delays-fall-2018\/notebooks) in this competition\n\n### Your task is to:\n 1. beat **\"A2 baseline (10 credits)\"** on Public LB (**0.75914** LB score)\n 2. rename your [team](https:\/\/www.kaggle.com\/c\/flight-delays-fall-2018\/team) in full accordance with A1 and the [course rating](https:\/\/docs.google.com\/spreadsheets\/d\/15e1K0tg5ponA5R6YQkZfihrShTDLAKf5qeKaoVCiuhQ\/) (to appear on 16.09.2019)\n \nThis task is intended to be relatively easy. Here you are not required to upload your reproducible solution.\n \n### <center> Deadline for A2: 2019 October 6, 20:59 CET (London time)","c471c0aa":"**Allocate a hold-out set (a.k.a. a validation set) to validate the model**","c87390c5":"**Train on the whole train set, make prediction on the test set. We got ~0.734 in the competition - \"Catboost starter\" baseline**"}}