{"cell_type":{"7ec187d6":"code","270088fa":"code","eed64ca9":"code","20f0b23e":"code","60b776cb":"code","142315f9":"code","f21dc42c":"code","9313ee2b":"code","ae001974":"code","b5bcd93e":"code","c2a06196":"code","87475546":"code","d1900166":"code","33741a03":"code","39eb1e50":"code","4585f1b7":"code","20e33713":"code","b8eb8fb9":"code","c688f0a9":"markdown","01b88765":"markdown","bda4fd45":"markdown","0b706495":"markdown","87db128b":"markdown","8de6ba6f":"markdown","6a82ab36":"markdown","47c6cc48":"markdown","7a73316b":"markdown","38191d6c":"markdown","4688ece7":"markdown","2050b4b6":"markdown","e2b3f474":"markdown","7b6f87ca":"markdown","62b1670a":"markdown","715ecda3":"markdown","4fac63eb":"markdown","43e4bbe7":"markdown","db50ddfa":"markdown","4e93296a":"markdown"},"source":{"7ec187d6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score","270088fa":"df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n\ndf_float = df.select_dtypes(include=['float64']).copy()\ndf_float['SalePrice'] = df['SalePrice'].copy()\n\ndf_float['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].mean(), inplace=False)\ndf_float['MasVnrArea'] = df['MasVnrArea'].fillna(df['MasVnrArea'].mean(), inplace=False)\ndf_float['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['GarageYrBlt'].mean(), inplace=False)","eed64ca9":"lotFrontage = df_float[['LotFrontage']]\nmasVnrAre = df_float[['MasVnrArea']]\ngarageYrBlt = df_float[['GarageYrBlt']]\nsalePrice = df_float['SalePrice']\n\nlr1 = LinearRegression()\nlr1.fit(lotFrontage, salePrice)\n\nlr2 = LinearRegression()\nlr2.fit(masVnrAre, salePrice)\n\nlr3 = LinearRegression()\nlr3.fit(garageYrBlt, salePrice)","20f0b23e":"def plot_data_and_model(pos, input_attr, output_attr, df, lr):\n  ax = plt.subplot(pos[0], pos[1], pos[2])\n  plt.ylabel(output_attr)\n  plt.xlabel(input_attr)\n  ax = plt.scatter(df[input_attr], df[output_attr])\n  ax = plt.plot(df[input_attr], lr.predict(df[[input_attr]]), linewidth=5.0, color='orange')","60b776cb":"f = plt.figure()\nf, ax = plt.subplots(1, 3, figsize=(21, 15))\n\nplot_data_and_model((3, 3, 1), 'LotFrontage', 'SalePrice', df_float, lr1)\nplot_data_and_model((3, 3, 2), 'MasVnrArea', 'SalePrice', df_float, lr2)\nplot_data_and_model((3, 3, 3), 'GarageYrBlt', 'SalePrice', df_float, lr3)","142315f9":"df_norm = df_float.copy()\ndf_norm['LotFrontage'] = (df_float['LotFrontage'] - df_float['LotFrontage'].min()) \/ (df_float['LotFrontage'].max() - df_float['LotFrontage'].min())","f21dc42c":"df_norm['MasVnrArea'] = (df_float['MasVnrArea'] - df_float['MasVnrArea'].min()) \/ (df_float['MasVnrArea'].max() - df_float['MasVnrArea'].min())\ndf_norm['GarageYrBlt'] = (df_float['GarageYrBlt'] - df_float['GarageYrBlt'].min()) \/ (df_float['GarageYrBlt'].max() - df_float['GarageYrBlt'].min())\ndf_norm['SalePrice'] = (df_float['SalePrice'] - df_float['SalePrice'].min()) \/ (df_float['SalePrice'].max() - df_float['SalePrice'].min())","9313ee2b":"lotFrontage = df_norm[['LotFrontage']]\nmasVnrArea = df_norm[['MasVnrArea']]\ngarageYrBlt = df_norm[['GarageYrBlt']]\nsalePrice = df_norm['SalePrice']\n\nlr_norm1 = LinearRegression()\nlr_norm1.fit(lotFrontage, salePrice)\n\nlr_norm2 = LinearRegression()\nlr_norm2.fit(masVnrArea, salePrice)\n\nlr_norm3 = LinearRegression()\nlr_norm3.fit(garageYrBlt, salePrice)","ae001974":"df_std = df_float.copy()\ndf_std['LotFrontage'] = (df_float['LotFrontage'] - df_float['LotFrontage'].mean()) \/ df_float['LotFrontage'].std()","b5bcd93e":"df_std['MasVnrArea'] = (df_float['MasVnrArea'] - df_float['MasVnrArea'].mean()) \/ df_float['MasVnrArea'].std()\ndf_std['GarageYrBlt'] = (df_float['GarageYrBlt'] - df_float['GarageYrBlt'].mean()) \/ df_float['GarageYrBlt'].std()\ndf_std['SalePrice'] = (df_float['SalePrice'] - df_float['SalePrice'].mean()) \/ df_float['SalePrice'].std()","c2a06196":"lotFrontage = df_std[['LotFrontage']]\nmasVnrArea = df_std[['MasVnrArea']]\ngarageYrBlt = df_std[['GarageYrBlt']]\nsalePrice = df_std['SalePrice'] \nlr_std1 = LinearRegression()\nlr_std1.fit(lotFrontage, salePrice)\n\nlr_std2 = LinearRegression()\nlr_std2.fit(masVnrArea, salePrice)\n\nlr_std3 = LinearRegression()\nlr_std3.fit(garageYrBlt, salePrice)","87475546":"df_all = df_float\\\n  .join(df_norm, rsuffix='_norm')\\\n  .join(df_std, rsuffix='_std')\ndf_all.describe().round(4) ","d1900166":"data = {\n    'Dataset': ['Original', 'Normalized', 'Standardized'],\n    'Slope_1': [lr1.coef_[0], lr_norm1.coef_[0], lr_std1.coef_[0]],\n    'Intercept_1': [lr1.intercept_, lr_norm1.intercept_, lr_std1.intercept_],\n    'Slope_2': [lr2.coef_[0], lr_norm2.coef_[0], lr_std2.coef_[0]],\n    'Intercept_2': [lr2.intercept_, lr_norm2.intercept_, lr_std2.intercept_],\n    'Slope_3': [lr3.coef_[0], lr_norm3.coef_[0], lr_std3.coef_[0]],\n    'Intercept_3': [lr3.intercept_, lr_norm3.intercept_, lr_std3.intercept_]\n}\n\ndf_lr = pd.DataFrame(data, copy=True)\ndf_lr.round(3)","33741a03":"f = plt.figure()\nf, ax = plt.subplots(3, 3, figsize=(21, 15))\n\nplot_data_and_model((3, 3, 1), 'LotFrontage', 'SalePrice', df_float, lr1)\nplot_data_and_model((3, 3, 2), 'MasVnrArea', 'SalePrice', df_float, lr2)\nplot_data_and_model((3, 3, 3), 'GarageYrBlt', 'SalePrice', df_float, lr3)\n\nplot_data_and_model((3, 3, 4), 'LotFrontage', 'SalePrice', df_norm, lr_norm1)\nplot_data_and_model((3, 3, 5), 'MasVnrArea', 'SalePrice', df_norm, lr_norm2)\nplot_data_and_model((3, 3, 6), 'GarageYrBlt', 'SalePrice', df_norm, lr_norm3)\n\nplot_data_and_model((3, 3, 7), 'LotFrontage', 'SalePrice', df_std, lr_std1)\nplot_data_and_model((3, 3, 8), 'MasVnrArea', 'SalePrice', df_std, lr_std2)\nplot_data_and_model((3, 3, 9), 'GarageYrBlt', 'SalePrice', df_std, lr_std3)\n\nax = plt.show()","39eb1e50":"errors = {\n    'LotFrontage': {\n      'Original':mean_squared_error(df_float['SalePrice'], lr1.predict(df_float[['LotFrontage']])),\n      'Normalized': mean_squared_error(df_norm['SalePrice'], lr_norm1.predict(df_norm[['LotFrontage']])),\n      'Standardized': mean_squared_error(df_std['SalePrice'], lr_std1.predict(df_std[['LotFrontage']]))\n    },\n    'MasVnrArea': {\n      'Original': mean_squared_error(df_float['SalePrice'], lr2.predict(df_float[['MasVnrArea']])),\n      'Normalized': mean_squared_error(df_norm['SalePrice'], lr_norm2.predict(df_norm[['MasVnrArea']])),\n      'Standardized': mean_squared_error(df_std['SalePrice'], lr_std2.predict(df_std[['MasVnrArea']]))\n    },\n    'GarageYrBlt': {\n      'Original': mean_squared_error(df_float['SalePrice'], lr3.predict(df_float[['GarageYrBlt']])),\n      'Normalized': mean_squared_error(df_norm['SalePrice'], lr_norm3.predict(df_norm[['GarageYrBlt']])),\n      'Standardized': mean_squared_error(df_std['SalePrice'], lr_std3.predict(df_std[['GarageYrBlt']]))  \n    } \n}\n\ndf_error = pd.DataFrame(errors)\ndf_error","4585f1b7":"df_error.plot.bar(figsize=(8, 6))","20e33713":"errors = {\n    'LotFrontage': {\n      'Original':r2_score(df_float['SalePrice'], lr1.predict(df_float[['LotFrontage']])),\n      'Normalized': r2_score(df_norm['SalePrice'], lr_norm1.predict(df_norm[['LotFrontage']])),\n      'Standardized': r2_score(df_std['SalePrice'], lr_std1.predict(df_std[['LotFrontage']]))\n    },\n    'MasVnrArea': {\n      'Original': r2_score(df_float['SalePrice'], lr2.predict(df_float[['MasVnrArea']])),\n      'Normalized': r2_score(df_norm['SalePrice'], lr_norm2.predict(df_norm[['MasVnrArea']])),\n      'Standardized': r2_score(df_std['SalePrice'], lr_std2.predict(df_std[['MasVnrArea']]))\n    },\n    'GarageYrBlt': {\n      'Original': r2_score(df_float['SalePrice'], lr3.predict(df_float[['GarageYrBlt']])),\n      'Normalized': r2_score(df_norm['SalePrice'], lr_norm3.predict(df_norm[['GarageYrBlt']])),\n      'Standardized': r2_score(df_std['SalePrice'], lr_std3.predict(df_std[['GarageYrBlt']]))  \n    } \n}\n\ndf_error = pd.DataFrame(errors)\ndf_error","b8eb8fb9":"df_error.plot.bar(figsize=(15, 6))","c688f0a9":"With Pandas, we don't need to iterate over each attribute value. By using vectorized operations, we can perform operations directly using data columns.\n\nAfter normalizing the *LotFrontage* attribute, we apply the normalization to other attributes as well:","01b88765":"Finally, we compare the mean squared error of the model to check how scaling affects the performance of the models. We do this by calculating the mean squared errors of the models and saving them in a new dataframe. Them we plot a bar chart with the obtained scores of the models.","bda4fd45":"The score of the models built from the scaled attributes are much lower than the original models. But this do not mean that the new models are better. We have to consider that the resulting errors are scaled as well. So, to compare the results better, we use the R\u00b2 score, which is a relative score for regression models. With the R\u00b2 score, better models score closer to 1.0.","0b706495":"When using R\u00b2 score we observe that the scaled models have the same performance of the original models.","87db128b":"After applying the standardisation, we use the new attributes to create new Linear Regression models and compare them with the previous ones.","8de6ba6f":"Then, we use the normalize attributes to create new Linear Regression models:","6a82ab36":"# 1. Linear Regression with one variable\n\nIn this section, we reproduce the linear regression models trained in [link]. We start by loading the data, selecting the attributes and filling the missing values. Then, we save the selected attributes, with the missing values filled in a new data frame.\n\n\n\n","47c6cc48":"In this cell, with build a dataframe using the parameters of the Linear Regression models we built. As consequence of the attribute scaling, the models parameters were scaled as well. All the models built with the standardised data has the intercept equals to 0 (zero).","7a73316b":"Then we use the previous function to plot the charts of the selected attributes and the respective linear models.","38191d6c":"# 2. Attributes in different scales\n\nObserving the charts of the previous section, we noticed that the attributes have different intervals of values. This can make the data analysis harder. To overcome this problem, we can transform the attributes of a dataset to the same scale, making the comparison easier. Another advantage, is that some algorithms perform better with scaled features. In next sections, we are going to learn 2 (two) different methos of scaling attributes: normalization and standartisation. ","4688ece7":"We define the *plot_data_and_model* function to plot in a single chart both the real data and the respective linear model. This function will be reused many times in this notebook.","2050b4b6":"# Scalling attributes with normalization and standartisation\n\nIn this notebook, we revisit the example of Linear Regression with One Variable, and we compare the results of applying normalization and standartisation to the attributes before training the linear regression models.\n\nGithub version:\nhttps:\/\/github.com\/rodmsmendes\/reinforcementlearning4fun\/blob\/master\/Scaling_attributes_of_Linear_Regression_with_one_variable.ipynb\n\nKaggle kernel version:\nhttps:\/\/www.kaggle.com\/rodolfomendes\/scalling-attributes-with-normalization-and-standar","e2b3f474":"Next, we plot the data and the models for each dataframe. The shape of the ploted data does not change between the different dataframes. That's because the Matplotlib library scales the intervals automatically to fit the figure. But we can notice that for each type of scaling, the respective chart has different axis intervals. For the original dataframe, the axis show the original intervals of values. For the normalized dataframe, all axis vary from 0 to 1. And for the standardised dataframe, the axis values are represented as multiples of the standard deviation. ","7b6f87ca":"# 6. Conclusion\n\nIn this example, we applied the normalization and standartisation to scaled the attributes of our model and compared them with the original model. By observing the R\u00b2 score we concluded that the scaling does not affect the final performance of Linear Regression models. However, by scaling the attributes it's easier to compare them during exploratory data analysis.","62b1670a":"# 3. Scaling the attributes with normalization\n\n\n\nIn normalization, we scale an attribute so that all thew new values will be in the interval between 0 and 1. Considering the attribute $X$ is a column vector, we apply the formula:\n\n$$ X^{norm} = \\frac{X - X_{min}}{x_{max} - x_{min}}$$\n\nwhere:\n\n\n* $X^{norm}$: is the new scaled attribute\n* $X_{min}$: is a column vector where all elements are equal to $x_{min}$\n* $x_{min}$: is the minimum value of the $X$ attribute\n* $x_{max}$: is the maximum value of the $X$ attribute\n\nWhen normalizing an attribute, we subtract the minimum value from all values of the attribute and then divided them by the difference of the maximum and minimum value of the attribute. By applying this procedure, all elements of the new attribute will be in interval between 0 and 1. The mean value and standard deviation will be scaled as well, but the transforming will keep the data distribution.\n\nThe following section uses Pandas vectorized operations and the aggregate functions *min()* and *max()* to normalize the attribute *LotFrontage* and save the normalized attribute in new data frame.\n","715ecda3":"# 0. Importing libraries\n\nIn this section, we just import the necessaries libraries used in the example.","4fac63eb":"After selecting the attributes, we create 3 (three) linear regression models. One for each of the non-target attributes.","43e4bbe7":"In our example, we create a separated *DataFrame* to hold the standardised attributes. Then we use vectorization and the aggregate functions *mean()* and *std()* to standardise the original attributes. ","db50ddfa":"# 4. Scaling attributes with standartization\n\nIn standartisation, we scale the attributes so that all atributes have mean 0 (zero) and standard deviation 1 (one). To do this we apply the formula:\n\n$$ X^{std} = \\frac{X - \\mu}{\\sigma} $$\n\nWhere:\n* $X^{std}$: is the new scaled attribute\n* $X$: is a column vector representing our attribute\n* $\\mu$: is a column vector where all elements are the mean value of the attribute\n* $\\sigma$: is the standard deviation of the attribute\n","4e93296a":"# 5. Results\n\nFirst, we join the dataframes in a single one, so we can use the function *describe()* and obtain a table with the summary statistics of the 3 (three) dataframes together. We use the sufix *_norm* to identify the normalized attributes and the sufix *_std* to identify the standardised attributes.\n\nAs expected, for the normalized attributes all values fall between 0 and 1. The mean and standard deviation are scaled as well.\n\nFor the standardised attributes, each one has their own minimum and maximum values. However, all mean values are set to 0 and all the standard deviation are 1. Because we centered our distribution at 0, now we have negative values for the scaled attributes."}}