{"cell_type":{"25d6f3fa":"code","9d10d1f3":"code","c0979ddb":"code","25b565e2":"code","5cbd58c0":"code","34370aaf":"code","9b289614":"code","5f2d419a":"code","5fafe2bb":"code","27596ff7":"code","ce7c9d23":"code","3863ab82":"code","2a4527ae":"code","2e74eb18":"code","799511bd":"code","62372653":"code","da486ef5":"code","dc9f7704":"code","708b05bb":"code","d710e6aa":"markdown","4a2a0c70":"markdown","f5fda132":"markdown","86194e1a":"markdown","d10f190c":"markdown","b868100c":"markdown","c4625679":"markdown","9a05047b":"markdown","2d287770":"markdown","9c413628":"markdown","a6d2bcf0":"markdown","0cd33011":"markdown","4b4605d1":"markdown"},"source":{"25d6f3fa":"\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","9d10d1f3":"x_train,y_train,x_valid,y_valid = get_data()\n\nx_train,x_valid = normalize_to(x_train,x_valid)\ntrain_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n\nnh,bs = 50,512\nc = y_train.max().item()+1\nloss_func = F.cross_entropy\n\ndata = DataBunch(*get_dls(train_ds, valid_ds, bs), c)","c0979ddb":"mnist_view = view_tfm(1,28,28)\ncbfs = [Recorder,\n        partial(AvgStatsCallback,accuracy),\n        CudaCallback,\n        partial(BatchTransformXCallback, mnist_view)]","25b565e2":"nfs = [8,16,32,64,64]","5cbd58c0":"class ConvLayer(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2, sub=0., **kwargs):\n        super().__init__()\n        self.conv = nn.Conv2d(ni, nf, ks, padding=ks\/\/2, stride=stride, bias=True)\n        self.relu = GeneralRelu(sub=sub, **kwargs)\n    \n    def forward(self, x): return self.relu(self.conv(x))\n    \n    @property\n    def bias(self): return -self.relu.sub\n    @bias.setter\n    def bias(self,v): self.relu.sub = -v\n    @property\n    def weight(self): return self.conv.weight","34370aaf":"learn,run = get_learn_run(nfs, data, 0.6, ConvLayer, cbs=cbfs) #all above code is the as before tog get something to compare to \n","9b289614":"run.fit(2, learn)#we see here we get a acc. on about 95%","5f2d419a":"learn,run = get_learn_run(nfs, data, 0.6, ConvLayer, cbs=cbfs)","5fafe2bb":"def get_batch(dl, run):\n    run.xb,run.yb = next(iter(dl))\n    for cb in run.cbs: cb.set_runner(run)\n    run('begin_batch')\n    return run.xb,run.yb","27596ff7":"xb,yb = get_batch(data.train_dl, run) #one minibatch","ce7c9d23":"\n#export\ndef find_modules(m, cond):\n    if cond(m): return [m]\n    return sum([find_modules(o,cond) for o in m.children()], []) #note we are using recursing since moduels is like a tree and the mudiels has to finde the modules \n\ndef is_lin_layer(l):\n    lin_layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear, nn.ReLU)\n    return isinstance(l, lin_layers)","3863ab82":"\nmods = find_modules(learn.model, lambda o: isinstance(o,ConvLayer)) #find all the modules of type ConvLayer (find:modules does that)","2a4527ae":"\nmods #so here is our list of all the conv layers ","2e74eb18":"\ndef append_stat(hook, mod, inp, outp):\n    d = outp.data\n    hook.mean,hook.std = d.mean().item(),d.std().item() #... hooks grap the mean and std of a partigular module ","799511bd":"mdl = learn.model.cuda()","62372653":"with Hooks(mods, append_stat) as hooks: #so now we can create a hook ... that gonne grap the mean and std \n    mdl(xb)\n    for hook in hooks: print(hook.mean,hook.std) #print all of the means and stds \n        #and we can se that the means and stds are not 0 and 1. so the mean is to high and the std is to low ","da486ef5":"def lsuv_module(m, xb):\n    h = Hook(m, append_stat) #so first of all we hook it \n\n    while mdl(xb) is not None and abs(h.mean)  > 1e-3: m.bias -= h.mean #so to get a better mean we create a loop \n        #calls the model ( mdl(xb)) and passing in the minibatch we have(xb) and check is the absolut value of the mean is close to 0( abs(h.mean)> 1e-3)\n        #and if it is not then we subrat the the mean from the bias. so it just keep looping through intil we get about a zero mean \n    while mdl(xb) is not None and abs(h.std-1) > 1e-3: m.weight.data \/= h.std #we do the same thing for the std where  we check if std-1 is near zero (abs(h.std-1) > 1e-3)\n        #and if it is not then will divide the weights with the std till we get there \n\n    h.remove()\n    return h.mean,h.std","dc9f7704":"for m in mods: print(lsuv_module(m, xb)) #so we see we get something very close to what we want though the mean isnt 0 but it is close and the std is perfect ","708b05bb":"\n%time run.fit(2, learn)#and we now see the scc. is about 97 % so its better ","d710e6aa":"## This is fastai lesson 11, though it is mixed with my notes, eksperiments and what I found usefull, if you want the pure version, check fastai github or the following link: https:\/\/github.com\/fastai\/course-v3\/tree\/master\/nbs\/dl2","4a2a0c70":"# Layerwise Sequential Unit Variance (LSUV)\nGetting the MNIST data and a CNN","f5fda132":"This is a helper function to grab the mean and std of the output of a hooked layer.","86194e1a":"\n## LSUV is particularly useful for more complex and deeper architectures that are hard to initialize to get unit variance at the last layer.","d10f190c":"So now we can look at the mean and std of the conv layers of our model.","b868100c":"\nNow we're going to look at the paper [All You Need is a Good Init](https:\/\/arxiv.org\/pdf\/1511.06422.pdf), which introduces Layer-wise Sequential Unit-Variance (LSUV). We initialize our neural net with the usual technique, then we pass a batch through the model and check the outputs of the linear and convolutional layers. We can then rescale the weights according to the actual variance we observe on the activations, and subtract the mean we observe from the initial bias. That way we will have activations that stay normalized.\n\nWe repeat this process until we are satisfied with the mean\/variance we observe.\n\nLet's start by looking at a baseline:","c4625679":"We only want the outputs of convolutional or linear layers. To find them, we need a recursive function. We can use sum(list, []) to concatenate the lists the function finds (sum applies the + operate between the elements of the list you pass it, beginning with the initial state in the second argument).","9a05047b":"We execute that initialization on all the conv layers in order:","2d287770":"\nNote that the mean doesn't exactly stay at 0. since we change the standard deviation after by scaling the weight.\n\nThen training is beginning on better grounds.","9c413628":"We first adjust the bias terms to make the means 0, then we adjust the standard deviations to make the stds 1 (with a threshold of 1e-3). The mdl(xb) is not None clause is just there to pass xb through mdl and compute all the activations so that the hooks get updated.","a6d2bcf0":"note: this notebook inst currently working!","0cd33011":"Helper function to get one batch of a given dataloader, with the callbacks called to preprocess it.","4b4605d1":"Now we recreate our model and we'll try again with LSUV. Hopefully, we'll get better results!"}}