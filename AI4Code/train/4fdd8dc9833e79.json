{"cell_type":{"a963234a":"code","faf92a7f":"code","7fe51cf8":"code","585d2562":"code","11ce970b":"code","35734e16":"code","c0b6ac4b":"code","c68442c6":"code","c3c0d7db":"code","60a358f6":"code","4bbe6033":"code","6130e9f8":"code","7deffcea":"code","2a6a80e4":"code","23535851":"code","66fb01d8":"code","9010c903":"code","798ea58d":"code","fc189663":"markdown","f8e142f1":"markdown","727cccb8":"markdown","d1562941":"markdown","31b85f4d":"markdown","335c8a4c":"markdown","c102ddce":"markdown","6f5bc8ea":"markdown","4a6cc720":"markdown"},"source":{"a963234a":"from tqdm.auto import tqdm\nimport os\n\nimport numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split , StratifiedKFold\n\n\nimport tensorflow as tf \nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, load_model, save_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Input,Dense, LSTM, RNN, Bidirectional, GlobalAveragePooling2D , Dropout\n\nfrom transformers import TFAutoModel , AutoTokenizer\n","faf92a7f":"class config:\n  seed = 43\n  test_path= \"..\/input\/iiitd-abuse-detection-challenge\/eam2021-test-set-public\/eam2021-test-set-public.csv\"\n  train_path = \"..\/input\/iiitd-abuse-detection-challenge\/eam2021-train-set\/bq-results-20210825-203004-swh711l21gv2.csv\"\n  langs = {'Hindi':'hi','Telugu':'te','Marathi':'mr','Tamil':'ta','Malayalam':'ml','Bengali':'bn','Kannada':'kn','Odia':'or','Gujarati':'gu',}\n  save_dir = \".\/result\"\n  AUTOTUNE = tf.data.AUTOTUNE\n  \n  #model params\n  epochs = 12\n  max_len = 64\n  batch_size = 128\n  hf_path = \"google\/muril-base-cased\" \n\n  \ndef seed_everything(seed = config.seed):\n  print(f\"seeded everything to seed {seed}\")\n  os.environ['PYTHONHASHSEED'] = str(seed)\n  np.random.seed(seed)\n  tf.random.set_seed(seed)\n\nif not os.path.exists(config.save_dir):\n  os.makedirs(config.save_dir)\nseed_everything()","7fe51cf8":"df_train = pd.read_csv(config.train_path)\ndf_test = pd.read_csv(config.test_path)","585d2562":"df_train[df_train['language'] == 'Hindi']","11ce970b":"def count(dataframe):\n  lengths = []\n  for i in tqdm(dataframe['commentText']):\n    length = len(i.split(' '))\n    lengths.append(length)\n  lengths = np.array(lengths)\n  print(f\"the average length of the excerpts is {np.mean(lengths)}, the median length is {np.median(lengths)} ,  the maxium length is {np.max(lengths)},the maxium length is {np.min(lengths)}\")\n  return lengths","35734e16":"lengths = count(df_train)","c0b6ac4b":"sns.countplot(lengths)","c68442c6":"tokenizer = AutoTokenizer.from_pretrained(config.hf_path)\ntokenizer.save_pretrained(os.path.join(config.save_dir , \"muril_tokenizer\"))","c3c0d7db":"def fast_encode(texts, tokenizer, chunk_size=512, maxlen=config.max_len):\n    \n    input_ids = []\n    tt_ids = []\n    at_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size]\n        encs = tokenizer(\n                    text_chunk,\n                    max_length = config.max_len,\n                    padding='max_length',\n                    truncation=True\n                    )\n        \n        input_ids.extend(encs['input_ids'])\n        tt_ids.extend(encs['token_type_ids'])\n        at_ids.extend(encs['attention_mask'])\n    \n    return {'input_ids': input_ids, 'token_type_ids': tt_ids, 'attention_mask':at_ids}\n","60a358f6":"token_data = fast_encode(list(df_train['commentText'].values), tokenizer)\ntoken_data['index'] = list(df_train['post_index'].values)\ntoken_data['label'] = list(df_train['label'].values)","4bbe6033":"df_tokenized = pd.DataFrame(token_data)","6130e9f8":"len(df_tokenized['input_ids'][0])","7deffcea":"del token_data ","2a6a80e4":"@tf.function\ndef train_prep_function(embeddings , target):\n  input_ids = embeddings['input_ids']\n  attention_mask = embeddings['attention_mask']\n\n  target = tf.cast(target, tf.int32)\n  \n  return {'input_ids': input_ids , 'attention_mask': attention_mask}, target\n","23535851":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","66fb01d8":"def create_model(transformer_model):\n  input_id_layer = Input(shape=(config.max_len,) ,dtype = tf.int32 , name = 'input_ids')\n  attention_mask_layer = Input(shape=(config.max_len,) , dtype = tf.int32 , name = 'attention_mask')\n\n  transformer = transformer_model(input_ids = input_id_layer , attention_mask = attention_mask_layer)[0]\n  transformer_output = transformer[:,0,:]\n\n  x = Dropout(0.2)(transformer_output)\n  predictions = Dense(1, activation = \"sigmoid\")(x)\n\n  model = Model(inputs=[input_id_layer , attention_mask_layer], outputs = predictions)\n  model.compile(\n      optimizer = Adam(learning_rate= 1e-5),\n      metrics = 'AUC',\n      loss = 'binary_crossentropy'\n  )\n\n  return model\n","9010c903":"with strategy.scope():\n  transformer_model = TFAutoModel.from_pretrained(config.hf_path)\n  model = create_model(transformer_model)\nmodel.summary()","798ea58d":"scores = []\nhists = []\nkfold = StratifiedKFold(random_state= config.seed, shuffle = True,n_splits= 5)\nfor k,(train_idx , test_idx) in enumerate(kfold.split(df_tokenized['index'] , df_tokenized['label'])):\n  print(f\"*************** Fold running {k+1} ***************\")\n  print(f\"training on {len(train_idx)} excerpts , validating on {len(test_idx)} excerpts\")\n  train_embeddings = {'input_ids': df_tokenized['input_ids'][train_idx].tolist() ,\"attention_mask\":df_tokenized['attention_mask'][train_idx].tolist()}\n  test_embeddings = {'input_ids': df_tokenized['input_ids'][test_idx].tolist() ,\"attention_mask\":df_tokenized['attention_mask'][test_idx].tolist()}\n  \n  y_train = df_tokenized['label'].loc[train_idx]\n  y_test = df_tokenized['label'].loc[test_idx]\n\n  y_train = y_train.tolist()\n  y_test = y_test.tolist()\n  \n  train_steps = len(train_embeddings['input_ids'])\/\/config.batch_size\/\/4\n  validation_steps = len(test_embeddings['input_ids'])\/\/config.batch_size\n  print(f\"training steps {train_steps} , validation steps {validation_steps}\")\n\n  # creating Dataset\n  print(\"Creating Dataset\")\n  train_dataset = tf.data.Dataset.from_tensor_slices((train_embeddings , y_train))\n  train_dataset = (\n      train_dataset\n      .shuffle(1024*2)\n      .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n      .repeat()\n      .batch(config.batch_size)\n      .prefetch(config.AUTOTUNE)\n  )\n  test_dataset = tf.data.Dataset.from_tensor_slices((test_embeddings , y_test ))\n  test_dataset = (\n      test_dataset\n      .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n      .batch(config.batch_size)\n      .prefetch(config.AUTOTUNE)\n  )\n\n  #Clearing backend session\n  K.clear_session()\n  print(\"Backend Cleared\")\n\n  #fitting model\n  model_checkpoint = ModelCheckpoint(f'{config.save_dir}\/muril_fold{k+1}.h5',monitor = 'val_auc', verbose = 1, save_best_only = True, \n                                     save_weights_only = True, mode = 'max')\n  hist = model.fit(train_dataset,steps_per_epoch= train_steps,validation_data= test_dataset,validation_steps= validation_steps\n                   , epochs = config.epochs, callbacks = [model_checkpoint])\n  hists.append(hist)\n  #prediction\n  print(\"loading model\")\n  model1 = create_model(transformer_model)\n  model1.load_weights(f'{config.save_dir}\/muril_fold{k+1}.h5')\n  y_predict = model1.predict(test_dataset , verbose = 1)\n  score = roc_auc_score(y_test ,y_predict)\n  scores.append(score)\n  del model1\n\nprint(f'The average roc auc score after 5 folds is {np.mean(scores)}')","fc189663":"# \ud83d\udcca Dataset Preperation Function","f8e142f1":"# \ud83d\udcdd Abstract\nTrain and prototype your models quickly by using TPUs. This notebook shows easy and quick way to train \ud83e\udd17Transformers on TPUs.\n\n# Versions\nVersion 2: Max length 64 LB:**0.**  CV:**0.**\n\n\n**All the references are mentioned below**","727cccb8":"# \ud83d\udd0e Basic Inspection ","d1562941":"# \ud83d\udccc References\nThank You Harveen for providing such good kernels\n1. [Harveen's baseline notebook](https:\/\/www.kaggle.com\/harveenchadha\/iiitd-muril-hf-tf-w-b-baseline\/)\n2. [Harveen's Tokenization notebook](https:\/\/www.kaggle.com\/harveenchadha\/tokenize-train-data-using-bert-tokenizer)\n\n\nThanks for viewing, drop your suggestions down in the comments below. \ud83d\ude42","31b85f4d":"\n# \ud83d\ude9a Imports","335c8a4c":"# \u2699\ufe0f Configs","c102ddce":"# Tokenizer","6f5bc8ea":"# \ud83d\udd04 KFold Training","4a6cc720":"# \ud83e\udde0 Modelling"}}