{"cell_type":{"f74c42d8":"code","d6464b21":"code","6a5ee4f0":"code","f7585e47":"code","b73c8a11":"code","ea2cb31f":"code","710a70e4":"code","a951ba49":"code","b1f0e9ee":"code","1c1c38d7":"code","470d5367":"code","b4198fcf":"code","615b5697":"code","ec5d8672":"code","326131d8":"code","a0f3bc06":"code","f966a920":"code","b14ec9fc":"code","90bc66ca":"code","1f06a53e":"code","a39badd8":"code","d220a358":"code","473242cf":"code","7b3f96b0":"code","9849b649":"code","e9d6502a":"code","4258eec7":"code","331f3fcd":"code","a50cdc78":"code","6e184831":"code","8a985e58":"code","918da1eb":"code","b6200ff4":"code","9cf2cccf":"code","9e13b488":"code","d82b096d":"code","00f85438":"code","1f77eb46":"code","c2095e4f":"code","c3fdf311":"code","c25544d4":"code","d78b7fd2":"code","2350b261":"code","6c5858df":"code","842fc5d0":"code","707f8a6a":"code","5d27d594":"code","3c3e5afe":"code","87645435":"code","37cc6961":"code","898734bf":"code","6a5e0f01":"code","a330347d":"code","fd58e9c1":"code","fdbaa420":"code","1b238f96":"code","a81111e1":"code","2ce1b34f":"code","d472062b":"code","408d785e":"code","c98e6ffb":"code","bd4e5bfa":"code","fc303be0":"code","5c51bc34":"code","8373736a":"code","146fed78":"code","c7dc54ac":"code","e3a0351e":"markdown","353de5dc":"markdown","1f4da55d":"markdown","479944ca":"markdown","eb2a68e8":"markdown","b9a6ab40":"markdown","22b1786d":"markdown","a461a03c":"markdown","2b68b4b0":"markdown","21d65b8a":"markdown","ba750a2b":"markdown","3c5f6bbb":"markdown","e95ae703":"markdown","72cb05d8":"markdown","c3df2115":"markdown","e713c745":"markdown","3cde801d":"markdown","eb75c290":"markdown","7fe204a6":"markdown","027cc5df":"markdown","25d05975":"markdown","7ac1d1fa":"markdown","c3173cdb":"markdown","7f23583a":"markdown","b36257e1":"markdown","c25c6d78":"markdown","6bcc90a5":"markdown","3c8f5d37":"markdown","33c38ee4":"markdown","47e6821f":"markdown","ebb6249b":"markdown","c2f53d45":"markdown","034b4a42":"markdown","1bbea1a3":"markdown","4d41c490":"markdown","a4a10d97":"markdown","a742e757":"markdown","3a013255":"markdown","3d5a260e":"markdown","ab645d0e":"markdown","e77a9cd4":"markdown","b9a39c4f":"markdown","13c9627a":"markdown","8bbb1d2a":"markdown","ee01ef86":"markdown","1c7fc2b1":"markdown","71e6cac2":"markdown","c98cf2b4":"markdown","1e51d228":"markdown","29122918":"markdown","111b4d7f":"markdown","d96bf52e":"markdown","fb696326":"markdown","5b4e0880":"markdown","19d93224":"markdown","c3bfd6ef":"markdown","e022910c":"markdown","b0744b6c":"markdown","91f45bab":"markdown","71a7b0d5":"markdown","97f87d8f":"markdown","2c68d7e9":"markdown","46709530":"markdown","71818b95":"markdown","8520cbf4":"markdown","fee3994f":"markdown"},"source":{"f74c42d8":"import numpy as np\nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport missingno as msno\n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.preprocessing import scale\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression, PLSSVD\n\nimport warnings\nwarnings.filterwarnings(\"ignore\");","d6464b21":"shouse = pd.read_csv(\"..\/input\/sydney-house-prices\/SydneyHousePrices.csv\")\ndf = shouse.copy()","6a5ee4f0":"df.head()","f7585e47":"df.info()","b73c8a11":"df.columns","ea2cb31f":"df.drop([\"Id\"],axis=1,inplace=True)","710a70e4":"round(df.describe(),2).T","a951ba49":"# box and whisker plots \nplt.subplot(2,1,1)\ndf[\"propType\"].value_counts().plot(kind='pie', title='PropType', autopct='%.1f%%', figsize=[20,20]);","b1f0e9ee":"plt.subplot(4,1,1)\ndf.bed.plot(kind='hist',color='pink',bins=50,figsize=(15,15))\nplt.title(\"bed Variable Histogram Chart\");\n\n\nplt.subplot(4,1,2)\ndf.sellPrice.plot(kind='hist',color='pink',bins=50,figsize=(15,15))\nplt.title(\"sellPrice Variable Histogram Chart\");\n\n\nplt.subplot(4,1,3)\ndf.bath.plot(kind='hist',color='pink',bins=50,figsize=(15,15))\nplt.title(\"bath Variable Histogram Chart\");\n\n\nplt.subplot(4,1,4)\ndf.car.plot(kind='hist',color='pink',bins=50,figsize=(15,15))\nplt.title(\"car Variable Histogram Chart\");","1c1c38d7":"plt.figure(figsize=(12,5))\nsns.heatmap(df.corr(),annot=True,linewidth=2.5,fmt='.3F',linecolor='black');","470d5367":"sns.pairplot(df, hue = \"propType\");","b4198fcf":"df.isnull().values.any()","615b5697":"df.isnull().sum()","ec5d8672":"msno.bar(df,color = sns.color_palette('deep'));","326131d8":"msno.matrix(df, color = (0.1, 0.2, 0.3));","a0f3bc06":"msno.heatmap(df);","f966a920":"def missing_value_table(df):\n    missing_value = df.isna().sum().sort_values(ascending=False)\n    missing_value_percent = 100 * df.isna().sum()\/\/len(df)\n    missing_value_table = pd.concat([missing_value, missing_value_percent], axis=1)\n    missing_value_table_return = missing_value_table.rename(columns = {0 : 'Missing Values', 1 : '% Value'})\n    cm = sns.light_palette(\"darkred\", as_cmap=True)\n    missing_value_table_return = missing_value_table_return.style.background_gradient(cmap=cm)\n    return missing_value_table_return\n  \nmissing_value_table(df)","b14ec9fc":"df['car'] = df['car'].fillna(df['car'].mean())","90bc66ca":"df['bed'] = df['bed'].fillna(df['bed'].mean())","1f06a53e":"df.isnull().sum()","a39badd8":"df[\"propType\"].value_counts()","d220a358":"df['propType'] = pd.Categorical(df['propType'])\ndfDummies = pd.get_dummies(df['propType'], prefix = 'propType')\ndfDummies","473242cf":"df = pd.concat([df, dfDummies[\"propType_house\"]], axis=1)","7b3f96b0":"df.Date.value_counts()","9849b649":"Date_ = pd.to_datetime(df['Date'])\ndf['Year'] = Date_.dt.year\ndf['Months'] = Date_.dt.month\ndf","e9d6502a":"df.drop([\"Date\",\"suburb\",\"propType\"],axis = 1, inplace=True)","4258eec7":"df.head()","331f3fcd":"dff = df.drop([\"postalCode\",\"propType_house\",\"Year\",\"Months\"],axis = 1)","a50cdc78":"for i, col in enumerate(dff.columns):\n    plt.figure(i)\n    sns.boxplot(x=col, data=df)","6e184831":"y = df[[\"sellPrice\"]]\nX = df.drop(\"sellPrice\", axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25)","8a985e58":"columns = X_train.copy()","918da1eb":"del columns[\"postalCode\"]\ndel columns[\"propType_house\"]\ndel columns[\"Year\"]\ndel columns[\"Months\"]","b6200ff4":"lower_and_upper = {} # storage\nX_train_copy = X_train.copy() # train copy \n\nfor col in columns.columns: # outlier detect\n    q1 = X_train[col].describe()[4] # Q1 = Quartile 1 median 25 \n    q3 = X_train[col].describe()[6] # Q3 = Quartile 3 median 75 \n    iqr = q3-q1  #IQR Q3 -Q1\n    \n    lower_bound = q1-(1.5*iqr)\n    upper_bound = q3+(1.5*iqr)\n    \n    lower_and_upper[col] = (lower_bound, upper_bound)\n    X_train_copy.loc[(X_train_copy.loc[:,col]<lower_bound),col]=lower_bound*0.75\n    X_train_copy.loc[(X_train_copy.loc[:,col]>upper_bound),col]=upper_bound*1.25\n    \nlower_and_upper","9cf2cccf":"X_test_copy = X_test.copy() # test copy   \n\nfor col in columns.columns:\n    X_test_copy.loc[(X_test_copy.loc[:,col]<lower_and_upper[col][0]),col]=lower_and_upper[col][0]*0.75\n    X_test_copy.loc[(X_test_copy.loc[:,col]>lower_and_upper[col][1]),col]=lower_and_upper[col][1]*1.25","9e13b488":"for i, col in enumerate(X_train_copy.columns):\n    plt.figure(i)\n    sns.boxplot(x=col, data=X_train_copy)","d82b096d":"for i, col in enumerate(X_test_copy.columns):\n    plt.figure(i)\n    sns.boxplot(x=col, data=X_test_copy)","00f85438":"sns.boxplot(y_train);","1f77eb46":"sns.boxplot(y_test);","c2095e4f":"lower_and_upper = {} # storage\ny_train_copy = y_train.copy() # train copy \n\nfor col in y_train.columns: # outlier detect\n    q1 = y_train[col].describe()[4] # Q1 = Quartile 1 median 25 \n    q3 = y_train[col].describe()[6] # Q3 = Quartile 3 median 75 \n    iqr = q3-q1  #IQR Q3 -Q1\n    \n    lower_bound = q1-(1.5*iqr)\n    upper_bound = q3+(1.5*iqr)\n    \n    lower_and_upper[col] = (lower_bound, upper_bound)\n    y_train_copy.loc[(y_train_copy.loc[:,col]<lower_bound),col]=lower_bound*0.75\n    y_train_copy.loc[(y_train_copy.loc[:,col]>upper_bound),col]=upper_bound*1.25\n    \nlower_and_upper","c3fdf311":"y_test_copy = y_test.copy() # test copy   \n\nfor col in y_test.columns:\n    y_test_copy.loc[(y_test_copy.loc[:,col]<lower_and_upper[col][0]),col]=lower_and_upper[col][0]*0.75\n    y_test_copy.loc[(y_test_copy.loc[:,col]>lower_and_upper[col][1]),col]=lower_and_upper[col][1]*1.25","c25544d4":"sns.boxplot(y_train_copy);","d78b7fd2":"sns.boxplot(y_test_copy);","2350b261":"print(\"X_train:\",X_train_copy.shape)\nprint(\"y_train:\",y_train_copy.shape)\nprint(\"X_test:\",X_test_copy.shape)\nprint(\"y_test:\",y_test_copy.shape)","6c5858df":"def final_model(X_reduced_train, y_train, X_reduced_test, y_test, X_train, X_test):\n    \n    #Setting up final models with the best values\n    pcr_final_model = LinearRegression().fit(X_reduced_train[:,0:6],y_train)\n    pls_final_model = PLSRegression(n_components = 7).fit(X_train, y_train)\n    multi_linear_final_model = LinearRegression().fit(X_train, y_train)\n    \n    #Forecasting operations with final models.\n    y_pred_pcr = pcr_final_model.predict(X_reduced_test[:,0:6])\n    y_pred_pls = pls_final_model.predict(X_test)\n        \n    print(\"corrected bug of pcr model:\",np.sqrt(mean_squared_error(y_test, y_pred_pcr)))\n    print(\"corrected bug of multi linear regression model:\",np.sqrt(-cross_val_score(multi_linear_final_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean())\n    print(\"corrected bug of pls model:\",np.sqrt(mean_squared_error(y_test, y_pred_pls)))    ","842fc5d0":"def model_tuning(X_reduced_train, x_train, y_train):\n    cv_10 = model_selection.KFold(n_splits =10, shuffle = True, random_state = 1)\n    \n    lm = LinearRegression()\n    RMSE_pcr = []\n    RMSE_pls = []\n\n    #The best parameters are found with cross validation.\n    for i in np.arange(1, X_reduced_train.shape[1] + 1):    \n        score1 = np.sqrt(-1*cross_val_score(lm, X_reduced_train[:,:i], y_train.values.ravel(), cv = cv_10, scoring = \"neg_mean_squared_error\").mean())\n        RMSE_pcr.append(score1)\n    \n    #The best parameters are found with cross validation.\n    for i in np.arange(1, X_train.shape[1] + 1):\n        pls = PLSRegression(n_components=i)\n        score2 = np.sqrt(-1*cross_val_score(pls,  x_train, y_train, cv = cv_10, scoring = \"neg_mean_squared_error\").mean())\n        RMSE_pls.append(score2)\n    \n    \n    fig, axs = plt.subplots(2,figsize=(10,10))\n    fig.suptitle('PCR \/ PLS Model Tuning For Price Prediction Model')\n    axs[0].plot(RMSE_pcr, '-v')\n    axs[1].plot(np.arange(1, X_train.shape[1]+1), np.array(RMSE_pls), '-v', c = \"r\")\n    axs[0].set_xlabel('Number of components')\n    axs[0].set_ylabel('RMSE')\n    axs[1].set_xlabel('Number of components')\n    axs[1].set_ylabel('RMSE')","707f8a6a":"def model_predict(x_train ,y_train, x_test, y_test):\n    pca = PCA()\n    X_reduced_train = pca.fit_transform(scale(x_train)) #Conversion processes for pcr model x_train.\n    X_reduced_test = pca.fit_transform(scale(x_test)) #Conversion processes for pcr model x_test.\n    \n    \n    #Building a models\n    pcr_model = LinearRegression().fit(X_reduced_train, y_train)\n    reg_model = LinearRegression().fit(x_train, y_train)\n    pls_model = PLSRegression(n_components = 2).fit(x_train, y_train)\n    \n    #Predicted operations from created models.\n    y1_pred = pcr_model.predict(X_reduced_test)\n    y2_pred = reg_model.predict(x_test)\n    y3_pred = pls_model.predict(x_test)\n    \n    print(\"primitive error of pcr model:\", np.sqrt(mean_squared_error(y_test, y1_pred)))\n    print(\"primitive error of multiple linear regression model:\", np.sqrt(mean_squared_error(y_test, y2_pred)))\n    print(\"primitive error of pls model:\", np.sqrt(mean_squared_error(y_test, y3_pred)))\n    print(\"----------------------------------------------------------------------------------\")\n    \n    model_tuning(X_reduced_train, x_train, y_train)\n    final_model(X_reduced_train, y_train, X_reduced_test, y_test, X_train, X_test)","5d27d594":"model_predict(X_train_copy, y_train_copy, X_test_copy, y_test_copy)","3c3e5afe":"# A high alpha value means a higher constraint on the coefficients. Here we will experiment with three alpha values.\n#alfa = [0.00005,0.5,10] \nalfa = 10**np.linspace(10,-2,100)*0.5 \nCoef_=[]","87645435":"print('---------------')\nridge_model=Ridge()\nfor g\u00fcncelalfa in alfa:\n    ridge_model.set_params(alpha=g\u00fcncelalfa)\n    ridge_model.fit(X_train_copy,y_train_copy)\n    y_pred = ridge_model.predict(X_test_copy)\n    Coef_.append(ridge_model.coef_)\n    mse = np.mean((y_pred - y_test_copy)**2)\n    print('Alfas\u0131 ' + str(g\u00fcncelalfa) + ' olan Ridge regresyon modelin Train skoru: ',ridge_model.score(X_train_copy,y_train_copy))\n    print('Alfas\u0131 ' + str(g\u00fcncelalfa) + ' olan Ridge regresyon modelin Test skoru: ',ridge_model.score(X_test_copy,y_test_copy))\n    print('Kullan\u0131lan \u00f6znitelik say\u0131s\u0131: ',np.sum(ridge_model.coef_!=0))\n    print('Test Hatas\u0131 MSE: ', mse)\n    print('\\n')    ","37cc6961":"# Coef_\nprint(Coef_)","898734bf":"# Writing the Ridge_cv model to find the Optimal Lamp.\nalfa[0:5]\nridge_cv=RidgeCV(alphas=alfa,scoring=\"neg_mean_squared_error\",normalize=True)","6a5e0f01":"ridge_cv.fit(X_train_copy,y_train_copy)","a330347d":"# Finding the Optimal lambda.\nridge_cv.alpha_","fd58e9c1":"# Setting up Ridge regression model with optimal lambda value\nridge_tuned=Ridge(alpha=ridge_cv.alpha_,normalize=True).fit(X_train_copy,y_train_copy)","fdbaa420":"# Mean squared error values\nmse= np.sqrt(mean_squared_error(y_test_copy,ridge_tuned.predict(X_test_copy)))\nmse","1b238f96":"print('---------------')\nlasso_model=Lasso()\nfor g\u00fcncelalfa in alfa:\n    lasso_model.set_params(alpha=g\u00fcncelalfa)\n    lasso_model.fit(X_train_copy,y_train_copy)\n    y_pred = lasso_model.predict(X_test_copy)\n    Coef_.append(lasso_model.coef_)\n    mse= np.sqrt(mean_squared_error(y_test_copy,y_pred))\n    print('Alfas\u0131 ' + str(g\u00fcncelalfa) + ' olan Lasso regresyon modelin Train skoru: ',lasso_model.score(X_train_copy,y_train_copy))\n    print('Alfas\u0131 ' + str(g\u00fcncelalfa) + ' olan Lasso regresyon modelin Test skoru: ',lasso_model.score(X_test_copy,y_test_copy))\n    print('Kullan\u0131lan \u00f6znitelik say\u0131s\u0131: ',np.sum(lasso_model.coef_!=0))\n    print('Test Hatas\u0131 MSE: ', mse)\n    print('\\n')","a81111e1":"# Coef_\nprint(Coef_)","2ce1b34f":"# Writing the Lasso_cv model to find the Optimal Lamp.\nlasso_cv_model=LassoCV(alphas=None,cv=10,max_iter=10000,normalize=True).fit(X_train_copy,y_train_copy)","d472062b":"# Finding the Optimal lambda.\nlasso_cv_model.alpha_","408d785e":"# Setting up Lasso regression model with optimal lambda value\nlasso_tuned=Lasso(alpha=lasso_cv_model.alpha_).fit(X_train_copy,y_train_copy)","c98e6ffb":"# Mean squared error values\nmse= np.sqrt(mean_squared_error(y_test_copy,lasso_tuned.predict(X_test_copy)))\nmse","bd4e5bfa":"    elastikNet_model=ElasticNet().fit(X_train_copy,y_train_copy)\n    y_pred = elastikNet_model.predict(X_test_copy)\n    mse= np.sqrt(mean_squared_error(y_test_copy,y_pred))\n    r2=r2_score(y_test_copy,y_pred)\n    print('Alfas\u0131 ' + str(g\u00fcncelalfa) + ' olan ElasticNet regresyon modelin Train skoru: ',elastikNet_model.score(X_train_copy,y_train_copy))\n    print('Alfas\u0131 ' + str(g\u00fcncelalfa) + ' olan ElasticNet regresyon modelin Test skoru: ',elastikNet_model.score(X_test_copy,y_test_copy))\n    print('Kullan\u0131lan \u00f6znitelik say\u0131s\u0131: ',np.sum(elastikNet_model.coef_!=0))\n    print('Test Hatas\u0131 MSE: ', mse)\n    print('Test Hatas\u0131 R2 score: ', r2)\n    print('\\n')","fc303be0":"# Coef_\nelastikNet_model.coef_","5c51bc34":"# Writing the ElasticNet_cv model to find the Optimal Lamp.\nelastikNet_cv_model=ElasticNetCV(cv=10,random_state=0).fit(X_train_copy,y_train_copy)","8373736a":"# Finding the Optimal lambda.\nelastikNet_cv_model.alpha_","146fed78":"# Setting up Elastic Net regression model with optimal lambda value\nelastikNet_tuned=ElasticNet(alpha=elastikNet_cv_model.alpha_).fit(X_train_copy,y_train_copy)","c7dc54ac":"# Mean squared error values\nmse= np.sqrt(mean_squared_error(y_test_copy,elastikNet_tuned.predict(X_test_copy)))\nmse","e3a0351e":"<a id=\"4\"><\/a>\n## Defining and Visualizing Missing Values","353de5dc":"* The relationship between numerical variables is examined. It cannot be said that there is a linear relationship between price and point variables when the data are examined.","1f4da55d":"# Lasso Model","479944ca":"In statistics, an outlier is a data point that differs significantly from other observations.\n\n* Outlier is smaller than Q1-1.5(Q3-Q1) and higher than Q3+1.5(Q3-Q1) .\n\n    * (Q3-Q1) = IQR (INTER QUARTILE RANGE)\n    * Q3 = Third Quartile(%75)\n    * Q1 = First Quartile(%25)","eb2a68e8":"<a id=\"11\"><\/a>\n# Machine Learning With Regression Algorithms","b9a6ab40":"* Contrary observations of the test set were cleared.","22b1786d":"* Train-test separation process for outliers observation analysis.","a461a03c":"* Outlier observation analysis would be unnecessary for variables deleted above.","2b68b4b0":"* Missing value numbers and percentages.","21d65b8a":"* Check, delete successful. Our new number of variables is 8.","ba750a2b":"* Multiple Linear Regression\n* Principal Component Regression (PCR)\n* Partial Least Squares Regression (PLS)","3c5f6bbb":"**Model\/Estimation**","e95ae703":"### Variable Description","72cb05d8":"* Missing values in the dataset are filled with the average of the variables.","c3df2115":"<a id=\"1\"><\/a>\n# Dataset Description","e713c745":"The aim is to find the coefficients that minimize the sum of error squares by applying a penalty to these coefficients.\n\n* It has been proposed against the disadvantage of Ridge regression leaving all relevant \/ unrelated variables in the model.\n* Lasso approximates the coefficients to zero.\n* L1 form resets some coefficients when lambda is big enough. Therefore, it eliminates variables.\n* Ridge and lasso methods are not superior to each other.","3cde801d":"* Contrary observations of the train set were cleared.","eb75c290":"* With the info() function, we can see the total number of variables in the data set, the types of these variables and the number of observations in the variables.\n* Our Dataset consists of 199504 rows and 9 columns.\n* The most missing value is seen in the sellPrice column.\n* Variables and types:\n    - float64(1):bed,car\n    - int64(2):  Id, postalCode, sellPrice, bath\n    - object(8):Date,suburb, propType","7fe204a6":"The aim is to find the coefficients that minimize the mean square error by applying a penalty to these coefficients.\n\nIn the Ridge model, there is an extra term known as the term punishment. The \u03bb given here is actually specified by the alpha parameter in the ridge function. That's why we basically control the penalty term by changing alpha values. The higher the alpha values, the greater the penalty, and therefore the size of the coefficients decreases.","027cc5df":" Using two types of plots:\n\n* Univariate plots to better understand each attribute.\n* Multivariate plots to better understand the relationships between attributes.","25d05975":" 1. **Id:**:  A variable with no name and a variable specifying indexes will be deleted below because it is unnecessary.\n 1. **Date**: Sales dates of houses.\n 1. **suburb**: Suburban names in Australia.  \n 1. **propType**: The type of house.\n 1. **sellPrice**:  Prices of house.\n 1. **car**: No idea.\n 1. **postalCode**: Postal code.\n 1. **bed**: Number of bed.\n 1. **bath**: Number of bathrooms.","7ac1d1fa":"<a id=\"6\"><\/a>\n## Operations on Missing Values","c3173cdb":" * Dataset variable names.","7f23583a":"# ElasticNet Model","b36257e1":"* For Target","c25c6d78":"## Multivariate Plots","6bcc90a5":"* Load file","3c8f5d37":"## Ridge Model","33c38ee4":"**Model\/Estimation**","47e6821f":"<a id=\"3\"><\/a>\n# Missing Values","ebb6249b":"-Important points:\n* Reduces the parameters, so it is mostly used to prevent multiple connections.\n* It is resistant to over learning.\n* Reduces model complexity by coefficient shrinkage.\n* L2 uses regularization technique.","c2f53d45":"# Introduction\n\n![](https:\/\/res.akamaized.net\/domain\/image\/upload\/t_web\/c_fill,w_600\/v1554864563\/6_New_Jersey_Road_Five_Dock_NSW_Low_res_tq758d.jpg)\n\n## Context\n\nThis is a Sydney House Prices dataset.\n\nThis data set contains information on the houses sold in Sydney between 2000 and 2019. In this study, deficient value operations, outliers and eventually regression analysis will be applied.\n\n## Content\n1. [Load and Check Data](#0)\n1. [Dataset Description](#1)\n1. [Data Visualization](#2)\n1. [Missing Value Analysis](#3)\n    * [Defining and Visualizing Missing Values](#4)\n    * [Testing the Randomness of Missing Values](#5)\n    * [Operations on Missing Values](#6)\n1. [Variable Transformation](#7)\n1. [Outlier Value Analysis](#8)\n    * [Outlier Value Detection Using Boxplot](#9)\n    * [Outlier Value Analysis With IQR](#10) \n1. [Machine Learning With Regression Algorithms](#11)","034b4a42":"* Statistical information about the dataset.\n    * You can access information such as means, medians, standard deviations, minimum and maximum values of numerical variables with the describe() function.","1bbea1a3":"* Categorical variables are deleted and the data set consists only of numerical values.","4d41c490":"<a id=\"5\"><\/a>\n## Testing the Randomness of Missing Values","a4a10d97":"* First 5 records in the Dataset","a742e757":"The aim is to find the coefficients that minimize the sum of error squares by applying a penalty score to these coefficients. ElasticNet combines L1 and L2 approaches.","3a013255":"<a id=\"9\"><\/a>\n## Outlier Value Detection Using Boxplot","3d5a260e":"* When the graphic is examined, there are observation information in the data set on the left part, variable names in the upper part and missing observations on the right part.","ab645d0e":"*  The date variable is divided by year and month.","e77a9cd4":"**Model Tuning**","b9a39c4f":"* Contrary observations of the test set were cleared.","13c9627a":"* Univariate plots \u2013 plots of each individual variable.\n* Given that the input variables are numeric, we can create box and whisker plots of each.","8bbb1d2a":"* One of the propType values converted to One Hot Encoding is added to the data. ","ee01ef86":"<a id=\"2\"><\/a>\n# Data Visualization","1c7fc2b1":"<a id=\"8\"><\/a>\n# Outlier Value Analysis","71e6cac2":"* Creating histogram of each input variable to get an idea of the distribution.","c98cf2b4":"* Is there any missing value in the data set","1e51d228":"### Filling in the Missing Values","29122918":"* The correlation of numerical variables is examined in the dataset. It is determined that there is a moderate relationship between the variables.","111b4d7f":"<a id=\"10\"><\/a>\n## Outlier Value Analysis With IQR","d96bf52e":"* Looking at the graph, the data at the top shows the missing data in the variables. On the left, it shows the percentages in the dataset. On the right, it shows the number of observations in the dataset. At the bottom of the graph, there are variable names.","fb696326":"*Used when the values in the dataset are missing.\nMissing values are generally NA.*","5b4e0880":"Heat maps are used to learn the relationships between variables. The values in this graph range from -1 to 1. If the value is 1 there is a correct relationship between the two variables, if the value is -1 there is a inverse relationship between the two variables. If the value is 0, there is no relationship between the two variables.","19d93224":"* One Hot Encoding means that categorical variables are represented as binary.","c3bfd6ef":"### Model with Sklearn","e022910c":"<a id=\"0\"><\/a>\n# Load and Check Data","b0744b6c":"* Unnecessary variable deletion.","91f45bab":"<a id=\"7\"><\/a>\n# Variable Transformation","71a7b0d5":"* Contrary observations of the train set were cleared.","97f87d8f":"**Model\/Estimation**","2c68d7e9":"**Model Tuning**","46709530":"**Model Tuning**","71818b95":"* Ridge Regression\n* Lasso Regression\n* ElasticNet Regression","8520cbf4":"## Univariate Plots","fee3994f":"* Total missing values in the variables."}}