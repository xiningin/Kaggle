{"cell_type":{"4827d17a":"code","d7e29300":"code","1c15cf78":"code","ce7a70a7":"code","1fad6fd4":"code","f5723a7b":"code","ae1d2c0f":"code","df1bbb42":"code","6e008d27":"code","9a5874e4":"code","3fddbf98":"code","d0762ddf":"code","86350463":"code","bd7b80ca":"code","1f41dfca":"code","a5f4a18e":"code","3b1550c3":"code","2cc424d5":"code","4f35adfc":"code","1605211d":"code","8973e222":"code","7d2ed07b":"code","e42097c7":"code","f418c64d":"code","3d187064":"code","b5335059":"code","83fb10e2":"code","088a572f":"code","91327bd1":"code","cd5f1c20":"code","44c0706d":"code","b85d5a7a":"code","6da61987":"code","3520a1a7":"code","85959d3b":"code","fddf45d9":"code","6fffdb01":"code","adfddee0":"code","f093b046":"code","3cf1f99a":"code","8cfe243d":"code","4e92e091":"code","f3b3fac2":"markdown","14ac039c":"markdown","a2d8992e":"markdown","0859801b":"markdown","b9d4780b":"markdown","e5bed899":"markdown","672fc363":"markdown"},"source":{"4827d17a":"import cv2\n\ncam = cv2.VideoCapture(0)\n\ncv2.namedWindow(\"test\")\n\nimg_counter = 0\n\nwhile True:\n    ret, frame = cam.read()\n    if not ret:\n        print(\"failed to grab frame\")\n        break\n    cv2.imshow(\"test\", frame)\n\n    k = cv2.waitKey(1)\n    if k%256 == 27:\n        # ESC pressed\n        print(\"Escape hit, closing...\")\n        break\n    elif k%256 == 32:\n        # SPACE pressed\n        img_name = \"opencv_frame_{}.png\".format(img_counter)\n        cv2.imwrite(img_name, frame)\n        print(\"{} written!\".format(img_name))\n        img_counter += 1\n\ncam.release()\n\ncv2.destroyAllWindows()","d7e29300":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom skimage.filters import threshold_local\nfrom PIL import Image","1c15cf78":"file_name = \"..\/input\/prediict-ocr\/bizz_card.jpg\"\nimg = Image.open(file_name)\nimg.thumbnail((800,800), Image.ANTIALIAS)\nimg","ce7a70a7":"def opencv_resize(image, ratio):\n    width = int(image.shape[1] * ratio)\n    height = int(image.shape[0] * ratio)\n    dim = (width, height)\n    return cv2.resize(image, dim, interpolation = cv2.INTER_AREA)","1fad6fd4":"def plot_rgb(image):\n    plt.figure(figsize=(16,10))\n    return plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))","f5723a7b":"def plot_gray(image):\n    plt.figure(figsize=(16,10))\n    return plt.imshow(image, cmap='Greys_r')","ae1d2c0f":"image = cv2.imread(file_name)\n# Downscale image as finding receipt contour is more efficient on a small image\nresize_ratio = 500 \/ image.shape[0]\noriginal = image.copy()\nimage = opencv_resize(image, resize_ratio)","df1bbb42":"# Convert to grayscale for further processing\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nplot_gray(gray)","6e008d27":"# Get rid of noise with Gaussian Blur filter\nblurred = cv2.GaussianBlur(gray, (5, 5), 0)\nplot_gray(blurred)","9a5874e4":"# Detect white regions\nrectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 9))\ndilated = cv2.dilate(blurred, rectKernel)\n\nplot_gray(dilated)","3fddbf98":"edged = cv2.Canny(dilated, 100, 200, apertureSize=3)\nplot_gray(edged)","d0762ddf":"# Detect all contours in Canny-edged image\ncontours, hierarchy = cv2.findContours(edged, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\nimage_with_contours = cv2.drawContours(image.copy(), contours, -1, (0,255,0), 3)\nplot_rgb(image_with_contours)","86350463":"# Get 10 largest contours\nlargest_contours = sorted(contours, key = cv2.contourArea, reverse = True)[:10]\nimage_with_largest_contours = cv2.drawContours(image.copy(), largest_contours, -1, (0,255,0), 3)\nplot_rgb(image_with_largest_contours)","bd7b80ca":"# approximate the contour by a more primitive polygon shape\ndef approximate_contour(contour):\n    peri = cv2.arcLength(contour, True)\n    return cv2.approxPolyDP(contour, 0.032 * peri, True)","1f41dfca":"def get_receipt_contour(contours):    \n    # loop over the contours\n    for c in contours:\n        approx = approximate_contour(c)\n        # if our approximated contour has four points, we can assume it is receipt's rectangle\n        if len(approx) == 4:\n            return approx","a5f4a18e":"get_receipt_contour(largest_contours)","3b1550c3":"receipt_contour = get_receipt_contour(largest_contours)\nimage_with_receipt_contour = cv2.drawContours(image.copy(), [receipt_contour], -1, (0, 255, 0), 2)\nplot_rgb(image_with_receipt_contour)","2cc424d5":"def contour_to_rect(contour):\n    pts = contour.reshape(4, 2)\n    rect = np.zeros((4, 2), dtype = \"float32\")\n    # top-left point has the smallest sum\n    # bottom-right has the largest sum\n    s = pts.sum(axis = 1)\n    rect[0] = pts[np.argmin(s)]\n    rect[2] = pts[np.argmax(s)]\n    # compute the difference between the points:\n    # the top-right will have the minumum difference \n    # the bottom-left will have the maximum difference\n    diff = np.diff(pts, axis = 1)\n    rect[1] = pts[np.argmin(diff)]\n    rect[3] = pts[np.argmax(diff)]\n    return rect \/ resize_ratio","4f35adfc":"def wrap_perspective(img, rect):\n    # unpack rectangle points: top left, top right, bottom right, bottom left\n    (tl, tr, br, bl) = rect\n    # compute the width of the new image\n    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n    # compute the height of the new image\n    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n    # take the maximum of the width and height values to reach\n    # our final dimensions\n    maxWidth = max(int(widthA), int(widthB))\n    maxHeight = max(int(heightA), int(heightB))\n    # destination points which will be used to map the screen to a \"scanned\" view\n    dst = np.array([\n        [0, 0],\n        [maxWidth - 1, 0],\n        [maxWidth - 1, maxHeight - 1],\n        [0, maxHeight - 1]], dtype = \"float32\")\n    # calculate the perspective transform matrix\n    M = cv2.getPerspectiveTransform(rect, dst)\n    # warp the perspective to grab the screen\n    return cv2.warpPerspective(img, M, (maxWidth, maxHeight))","1605211d":"scanned = wrap_perspective(original.copy(), contour_to_rect(receipt_contour))\nplt.figure(figsize=(16,10))\nplt.imshow(scanned)","8973e222":"def bw_scanner(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    T = threshold_local(gray, 21, offset = 5, method = \"gaussian\")\n    return (gray > T).astype(\"uint8\") * 255","7d2ed07b":"result = bw_scanner(scanned)\nplot_gray(result)","e42097c7":"output = Image.fromarray(result)\noutput.save('result.png')","f418c64d":"# Generic Libraries\nfrom PIL import Image\nimport os\nimport pandas as pd\nimport numpy as np\nimport re,string,unicodedata\nimport tensorflow \n\n#Tesseract Library\nimport pytesseract\n\n#Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Garbage Collection\nimport gc\nimport cv2\nimport matplotlib.pyplot as plt","3d187064":"# image_to_string method reads all the characters!\ntext = pytesseract.image_to_string(img)\nprint(text)","b5335059":"#DICTIONARY TO STORE THE INFO\nextracted_text = {}","83fb10e2":"#E-MAIL\nimport re\ndef get_email_addresses(string):\n    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n    return r.findall(string)\n\nemail = get_email_addresses(text)\nprint(email)\nextracted_text['E-mail'] = email","088a572f":"#PHONE NUMBER\nimport re\ndef get_phone_numbers(string):\n    r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n    phone_numbers = r.findall(string)\n    return [re.sub(r'\\D', '', num) for num in phone_numbers]\n\nphone_number= get_phone_numbers(text)\nif len(phone_number) <= 10:\n    print(phone_number)\n    extracted_text['Phone number'] = phone_number\n","91327bd1":"pip install datefinder","cd5f1c20":"#DATE\nimport datefinder\nmatches = datefinder.find_dates(text)\nfor match in matches:  \n    print(match)\n    \nextracted_text['Date'] =  match","44c0706d":"# initialize matcher with a vocab\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nfrom spacy.matcher import Matcher\nmatcher = Matcher(nlp.vocab)\n\ndef extract_name(text):\n   nlp_text = nlp(text)\n  \n   # First name and Last name are always Proper Nouns\n   pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n  \n   matcher.add('NAME', None, pattern)\n  \n   matches = matcher(nlp_text)\n  \n   for match_id, start, end in matches:\n       span = nlp_text[start:end]\n       return span.text\n\nname = extract_name(text)\nprint(name)\nextracted_text['Name'] =  name","b85d5a7a":"#DISPLAYING THE DICTIONARY\nprint(extracted_text)","6da61987":"text = \"Kritika Female 208, Ganpati Apartments, Sheetla Road, Khandari, Agra - 282005 Dayalbagh Educational Institute, Agra\"","3520a1a7":"#Street No\nimport re\ndef get_street(string):\n    r = re.compile(r'[0-9]{1,6}|\"One\"|\"Two\"|\"Three\"')\n    return r.findall(string)\n\nstreet = get_street(text)\nprint(street)\n","85959d3b":"#zip\ndef get_zip(string):\n    r = re.compile(r'([1-9]{1}[0-9]{5}|[1-9]{1}[0-9]{3}\\\\s[0-9]{3})')\n    return r.findall(string)\n\nzip_code = get_zip(text)\nprint(zip_code)\n","fddf45d9":"cities = []","6fffdb01":"#address\ndef get_address(string):\n    r = re.compile(r'{street}[].{2,80}[ ,]+{3,15}[ ,]+{zip}$')\n    return r.findall(string)\n\naddress = get_address(text)\nprint(address)","adfddee0":"#SAVING THE OUTPUT IN A TEXT FILE\noutput = open(\"output.txt\", \"w\")\noutput.write(str(extracted_text))\n\noutput = open(\"output.txt\", \"r\")\nprint(output.read())","f093b046":"import spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\n\nfrom spacy import displacy\n\ndisplacy.render(nlp(doc.text), style = 'ent', jupyter = True)","3cf1f99a":"import spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\ndoc = nlp(text)\nprint([(X.text, X.label_) for X in doc.ents])","8cfe243d":"named_entities = []\nfor sentence in doc:\n    temp_entity_name = ''\n    temp_named_entity = None\n    sentence = nlp(text)\n    for word in sentence:\n        term = word.text \n        tag = word.ent_type_\n        if tag:\n            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n            temp_named_entity = (temp_entity_name, tag)\n        else:\n            if temp_named_entity:\n                named_entities.append(temp_named_entity)\n                temp_entity_name = ''\n                temp_named_entity = None\n\nentity_frame = pd.DataFrame(named_entities, \n                            columns=['Entity Name', 'Entity Type'])","4e92e091":"# get the top named entities\ntop_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n                           .size()\n                           .sort_values(ascending=False)\n                           .reset_index().rename(columns={0 : 'Frequency'}))\ntop_entities.T.iloc[:,:15]","f3b3fac2":"# EXTRACTING THE TEXT FROM IMAGE","14ac039c":"# CLEANING THE IMAGE","a2d8992e":"ADDRESS\n","0859801b":"**USING SPACY**","b9d4780b":"## STEP 2: CROPPING","e5bed899":"# READING A LIVE IMAGE","672fc363":"### STEP 1 : CONTOUR DETECTION"}}