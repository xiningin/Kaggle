{"cell_type":{"ffdb639c":"code","52132855":"code","e9eb9579":"code","a8c6ffa4":"code","90b36047":"code","261c131c":"code","18398dd8":"code","7aa4c738":"code","faa8c0cb":"code","8e15d73d":"code","6b00a62e":"code","5f8a4b65":"code","49cf0d1d":"code","143526ef":"code","4a138e2b":"code","2eabcf31":"code","379cedae":"code","505f3de2":"code","f50772ec":"code","e753ac4c":"markdown","61a21b14":"markdown","35815af2":"markdown","a9c5ef8b":"markdown","2498a86d":"markdown","a8d55079":"markdown","d14ba51f":"markdown","7aa694d5":"markdown","d28913df":"markdown","f3df91cb":"markdown","b650a106":"markdown","95fdb9bd":"markdown","8cbc3b07":"markdown","23eb7dbc":"markdown","ae859588":"markdown","b3f8a009":"markdown","6c1c8816":"markdown"},"source":{"ffdb639c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","52132855":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import display","e9eb9579":"titanic_trainset = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntitanic_trainset.head()","a8c6ffa4":"# Creation of the copy\ntitanic_trainset_copy = titanic_trainset.copy()\n# Elimination of the USELESS FEATURES\ntitanic_trainset_copy.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n\n#'Sex' conversion\ntitanic_trainset_copy['Sex'].replace(['male', 'female'], [0, 1], inplace=True)\n# 'Cabin' conversion\ntitanic_trainset_copy.fillna(value=0, inplace=True)\nfor i in range(titanic_trainset_copy.shape[0]):\n    if titanic_trainset_copy.at[i, 'Cabin'] != 0:\n        titanic_trainset_copy.at[i, 'Cabin'] = 1\n# 'Embarked' conversion\ntitanic_trainset_copy['Embarked'].replace(['C', 'Q', 'S'], [0, 1, 2], inplace=True)\n\ntitanic_trainset_copy = titanic_trainset_copy.apply(pd.to_numeric) # convert all columns of DataFrame\n\ntitanic_trainset_copy.head()","90b36047":"plt.figure()\ntitanic_trainset_copy['Survived'].value_counts().plot.bar(figsize=(8,5), color=['yellow', 'violet'])\nplt.title('TITANIC: CLASSES COUNT')\nplt.xlabel(\"Survived or Not-Survived\")\nplt.ylabel(\"Number of Elements\")\nplt.grid()","261c131c":"import sklearn\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","18398dd8":"X_titanic = titanic_trainset_copy.iloc[:, 1:]  \n\nscaler_titanic = StandardScaler()\nscaler_titanic.fit(X_titanic.values)\nX_titanic_scaled = scaler_titanic.transform(X_titanic.values)\n\npca_titanic_nostd = PCA()\npca_titanic = PCA()\n\npca_titanic_nostd.fit(X_titanic.values)\npca_titanic.fit(X_titanic_scaled)\n\nplt.figure()\nplt.plot(np.insert(np.cumsum(pca_titanic_nostd.explained_variance_ratio_), 0, 0))\nplt.title('TITANIC (NO STANDARDIZATION)')\nplt.xticks(ticks=np.arange(1, pca_titanic_nostd.n_features_ + 1), \n           labels=[f'PC{i}' for i in range(1, pca_titanic_nostd.n_features_ + 1)])\nplt.xlabel('Principal components')\nplt.ylabel('Cumulative explained variance')\nplt.grid()\nplt.show()\n\nplt.figure()\nplt.plot(np.insert(np.cumsum(pca_titanic.explained_variance_ratio_), 0, 0))\nplt.title('RED WINE (WITH STANDARDIZATION)')\nplt.xticks(ticks=np.arange(1, pca_titanic.n_features_ + 1), \n           labels=[f'PC{i}' for i in range(1, pca_titanic.n_features_ + 1)])\nplt.xlabel('Principal components')\nplt.ylabel('Cumulative explained variance')\nplt.grid()\nplt.show()","7aa4c738":"pca_titanic = PCA(n_components=3)\npca_titanic.fit(X_titanic_scaled)\nZ3p_titanic = pca_titanic.transform(X_titanic_scaled)\n\nfig_pca_titanicscore = plt.figure(figsize=(15,10)) \n\nax = fig_pca_titanicscore.add_subplot(1, 2, 1, projection='3d')\nax.scatter(Z3p_titanic[:, 0], Z3p_titanic[:, 1], Z3p_titanic[:, 2], c=titanic_trainset_copy.iloc[:, 0])\nplt.title('TITANIC - PCA - SCORE GRAPH')\nax.set_xlabel('PC1')\nax.set_ylabel('PC2') \nax.set_zlabel('PC3')\n","faa8c0cb":"titanic_testset = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n# Creation of the copy\ntitanic_testset_copy = titanic_testset.copy()\n# Elimination of the USELESS FEATURES\ntitanic_testset_copy.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n\n#'Sex' conversion\ntitanic_testset_copy['Sex'].replace(['male', 'female'], [0, 1], inplace=True)\n# 'Cabin' conversion\ntitanic_testset_copy.fillna(value=0, inplace=True)\nfor i in range(titanic_testset_copy.shape[0]):\n    if titanic_testset_copy.at[i, 'Cabin'] != 0:\n        titanic_testset_copy.at[i, 'Cabin'] = 1\n# 'Embarked' conversion\ntitanic_testset_copy['Embarked'].replace(['C', 'Q', 'S'], [0, 1, 2], inplace=True)\n\ntitanic_testset_copy = titanic_testset_copy.apply(pd.to_numeric) # convert all columns of DataFrame\n\ntitanic_testset_copy.head()","8e15d73d":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, make_scorer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split","6b00a62e":"X = titanic_trainset_copy.iloc[:, 1:].values\ny = titanic_trainset_copy['Survived'].values\n\nrandom_state = 20210526\n\nval_p = 0.4\n\nindices = np.arange(X.shape[0])\n\nind_train, ind_val = train_test_split(indices, test_size=val_p, random_state=random_state, shuffle=True)","5f8a4b65":"n_features = X.shape[1]\n\nC_list = [2 ** i for i in range(-2, 3)]\ngamma_list = [1 \/ (i * n_features) for i in np.arange(0.10, 1.75, 0.5)]\nker_list = ['rbf', 'poly', 'sigmoid', 'linear']\n\n# A pipeline is needed to perform the scaling ofthe dataset\npipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(class_weight='balanced'))])\nhparameters = {'svc__kernel':ker_list, 'svc__C':C_list, 'svc__gamma':gamma_list}\n\nsvm_gs = GridSearchCV(pipe, \n                      param_grid=hparameters, \n                      scoring='f1_weighted',\n                      return_train_score=True,\n                      cv=zip([ind_train], [ind_val]),verbose=True)\nsvm_gs.fit(X, y)","49cf0d1d":"df_results = pd.DataFrame(svm_gs.cv_results_)\n\ndisplay(df_results.sort_values(['rank_test_score'], ascending=True))","143526ef":"svm_gs.best_estimator_.fit(X[ind_train, :], y[ind_train])\ny_pred_train = svm_gs.best_estimator_.predict(X[ind_train, :])\ny_true_train = y[ind_train]\ny_pred_val = svm_gs.best_estimator_.predict(X[ind_val, :])\ny_true_val = y[ind_val]\n\ny_pred_GS = svm_gs.best_estimator_.predict(titanic_testset_copy.values)\n\nacc_train = svm_gs.best_estimator_.score(X[ind_train, :], y_true_train)\nprec_train = precision_score(y_true_train, y_pred_train, average='weighted')\nrec_train = recall_score(y_true_train, y_pred_train, average='weighted')\nf1_train = f1_score(y_true_train, y_pred_train, average='weighted')\n\nacc_val = svm_gs.best_estimator_.score(X[ind_val, :], y_true_val)\nprec_val = precision_score(y_true_val, y_pred_val, average='weighted')\nrec_val = recall_score(y_true_val, y_pred_val, average='weighted')\nf1_val = f1_score(y_true_val, y_pred_val, average='weighted')\n\ndf_perf = pd.DataFrame({'Accuracy': [acc_train, acc_val], \n                        'Precision': [prec_train, prec_val], \n                        'Recall': [rec_train, rec_val],\n                        'F1': [f1_train, f1_val]\n                       },\n                      index=['training', 'validation'])\n\ndisplay(df_perf)\n","4a138e2b":"from sklearn.neural_network import MLPClassifier\n","2eabcf31":"X_trainval = titanic_trainset_copy.iloc[:, 1:].values\ny_trainval = titanic_trainset_copy['Survived'].values\n\nX_test = titanic_testset_copy.values\n\nstandard_scaler = StandardScaler()\nX_trainval = standard_scaler.fit_transform(X_trainval)\nX_test = standard_scaler.transform(X_test)","379cedae":"random_state = 20210526\nval_p = 0.2\n\nhidden_layer_sizes = [256] * 5\nactivation = 'relu'\npatience = 75\nmax_epochs = 5000\nverbose = False\nbatch_sz = 2\nmlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n                    activation=activation,\n                    early_stopping=True,\n                    n_iter_no_change=patience,\n                    max_iter=max_epochs,\n                    validation_fraction=val_p,\n                    batch_size=batch_sz,\n                    verbose=verbose,\n                    random_state=random_state,\n                    solver='adam',\n                   )\n\nmlp.fit(X_trainval, y_trainval)","505f3de2":"y_pred_trainval = mlp.predict(X_trainval)\ny_pred_MLP = mlp.predict(X_test)\n\nacc_trainval = mlp.score(X_trainval, y_trainval)\nprec_trainval = precision_score(y_trainval, y_pred_trainval, average='weighted')\nrec_trainval = recall_score(y_trainval, y_pred_trainval, average='weighted')\nf1_trainval = f1_score(y_trainval, y_pred_trainval, average='weighted')\n\ndf_perf = pd.DataFrame({'Accuracy': [acc_trainval], \n                        'Precision': [prec_trainval], \n                        'Recall': [rec_trainval],\n                        'F1': [f1_trainval]\n                       },\n                      index=['train. + val.'])\ndisplay(df_perf)","f50772ec":"output = pd.DataFrame({'PassengerId': titanic_testset.PassengerId, 'Survived': y_pred_MLP})\ndisplay(output)\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","e753ac4c":"## 2.1 TOTAL VARIANCE EXPLAINED\nIf in a dataset some features are too big compared to others, there might be a problem with the ML algorithms used, meaning that a **scaling** of the features might be needed.\nA simple way to see if this is the case with our dataset could be to look to the **total variance explained** by the **principal components** of the PCA.\n\nLet's do the PCA both with and without the scaling. (I will use the **StandardScaler()** of **sklearn**)","61a21b14":"## 3.1 TESTSET IMPORTATION\nLet's prepare the Test set like we did with the train set","35815af2":"## 2.2 SCORE GRAPH\nUsing **matplotlib** and the **PCA** I will now plot the dataset in a 3D graph","a9c5ef8b":"# 1. **PRELEMINARY ANALYSIS**\nIn the first part of the notebook, after importing the dataset, I will do some **preleminary analysis**. The goal is to understand what it is made, doing some feature selection and data analysis. This way the dataset will be ready for **data visualization** and the testing of some **premonition algorithms**.","2498a86d":"## 3.2 SVM WITH GRID SEARCH","a8d55079":"# 2. **DATASET VISUALIZATION WITH PCA**","d14ba51f":"**Imbalanced classification** is the problem of classification when there is an unequal distribution of classes in the training dataset. If there is a considerable difference between the number of people who survived and the ones that didn't, the classification algorithm might have a hard time.","7aa694d5":"## 1.2 CHECKING FOR IMBALANCEMENT","d28913df":"**Principal component analysis** (**PCA**) is a technique for **reducing the dimensionality** of a dataset, increasing interpretability but at the same time minimizing information loss.","f3df91cb":"## 1.2 FEATURE SELECTION\nAfter importing the dataset it obvious that some features are not correlated with **survival**. Others might have an impact but are **categorical** so it is necessary to convert them in **numerical** values.\n* **USELESS FEATURES**: variables not important for the classification:\n * **PassengerId**: we can already use the index of the dataframe to identify the different people\n * **Ticket**: doesn't give any useful information\n * **Name**: even if people of certain families might byological be more inclined to survive than others, the difference would probably be minimal and it is better to leave it out for the moment\n* **CATEGORICAL FEATURES**: variables that might be useful but need to be converted:\n * **Sex**: important variable will be converted as follow: male==0, female==1\n * **Cabin**: having or not having a cabin could effect the survivability of the passengers: no_cabin==0, cabin==1\n * **Embarked**: people that came from different ports migth be more or less inclined to survive: C==0, Q==1, S==2\n \nLet's create a copy of the dataframe, using pandas, for applying those changes","b650a106":"## 3.4 EXPORTATION OF THE BEST PERFORMING ALGORITHM\n**The MLP outperfomed the Kernel SVM** so it will be used as **output**","95fdb9bd":"# 3 **PREMONITION ALGORITHMS**\nThe ML algorithm that I will use are: **Kernel SVM** (using a **Grid Search** to found the best hyperparameters) and **MultiLayer Perceptron** (MLP, an example of **neural network**)\n\nFor every algorithm I will split the train set into two sets:\n* **Train Set**: will be used to train the different models with different hyperparameters\n* **Validation Set**: will be used to test the models and found the one with the combination of parameters that perform best\n\nI will calculate the accuracy of the two algorithms and other evaluation scores and I will use the best one to calculate the output for the **submission**","8cbc3b07":"Even if the dataset is imbalanced, the difference in the two class is not too big. ","23eb7dbc":"## 3.3 NEURAL NETWORKS WITH MLP","ae859588":"## 1.1 TRAINSET IMPORTATION","b3f8a009":"# **MY FIRST KAGGLE PROJECT: THE TITANIC**\n## INTRODUCTION\nHi, I'm Marco Mungai Coppolino, a Mathematical Engineering Student at Polytechnic of Turin. \nAt my University, I have recently attended an exam called **Mathematics for Artificial Intelligence**. This exam was my first experience with **Data Science**, and it made me fall in love with the field. So I wanted to start studying Data Science on my own and, after some research I founded KAGGLE. \n**Titanic** is my **first KAGGLE competition**, so I have decided to test everything I learned in the course done at my University.\nThis way I will be able to check my progress in the upcoming months seeing how I improved during this learning journey.\n________________________________________________________________\n## INDEX:\n 1. **Preleminary Analysis**:\n  1. **TrainSet Importation**\n  2. **Checking For Imbalancement** \n 2. **DataSet Visualization With PCA**\n  1. **Total Variance Explained**\n  2. **Score Graph**\n 3. **Premonition Algorithms**\n  1. **TestSet Importation**\n  2. **SVM With Grid Search**\n  3. **Neural Networks With MLP**\n  4. **Exportation Of The Best Performing Algorithm**","6c1c8816":"Has can be seen in the first graph the majority of the total variance is explained by the first PC. This means that **scaling is needed**\n\nFrom the second graph (the one with the scaled dataset) we can see that 3 PC give only the 60% of the total variance. \nThis implies that using only 3 PC to represent the dataset will cause some information loss"}}