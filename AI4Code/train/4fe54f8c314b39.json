{"cell_type":{"a5e07f63":"code","090c6522":"code","012adf97":"code","ef397ffd":"code","13242616":"code","21523f60":"code","a3a711da":"code","32a198d4":"code","55099ae8":"code","bc6c318b":"code","d0ac23fe":"code","392f3ed5":"code","b5ee7e4a":"code","6ab2af38":"code","7d397b91":"code","daf1d4be":"code","1486d1aa":"code","6d2d682c":"code","f3b31815":"code","cf3631a0":"code","fa69b390":"code","55221cf5":"code","3cdf7610":"code","8eadb3d5":"code","ed340808":"code","106c9d38":"code","7eeff679":"code","2f8d5a2e":"code","501e1ef9":"code","187e2f9e":"code","646ecd27":"code","2afa7c6e":"code","ee685558":"code","3ef883fd":"code","ce4604db":"code","57aa707b":"code","b51d81f1":"code","50eaaded":"code","05d166df":"code","1adc75bb":"code","0c9635a6":"markdown","e2a3a672":"markdown","431146e2":"markdown","b44e0313":"markdown","b69912cd":"markdown","09a20195":"markdown","48c765b8":"markdown"},"source":{"a5e07f63":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'): \n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","090c6522":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport os\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense,LSTM,Bidirectional,Flatten,Dropout,BatchNormalization,Embedding,Input,TimeDistributed\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import plot_model","012adf97":"all_data=[]\nArticles_with_stopwords=[]\nArticles_without_stopwords=[]\nSummaries=[]\nstop_words=set(stopwords.words('english'))\nfor d,path,filenames in tqdm(os.walk('\/kaggle\/input')):\n    for file in filenames:\n        if os.path.isfile(d+'\/'+file):\n            if('Summaries' in d+'\/'+file):\n                with open(d+'\/'+file,'r',errors='ignore') as f:\n                    summary=''.join([i.rstrip() for i in f.readlines()])\n                    Summaries.append(summary)\n                    f.close()\n            else:\n                with open(d+'\/'+file,'r',errors='ignore') as f:\n                    Article=''.join([i.rstrip() for i in f.readlines()])\n                    Articles_with_stopwords.append(Article)\n                    Articles_without_stopwords.append(' '.join([w for w in Article.split() if w not in stop_words]))\n                    f.close()\n        \n                    ","ef397ffd":"print(len(Articles_with_stopwords),len(Articles_without_stopwords),len(Summaries))","13242616":"data=pd.DataFrame({'Articles without stop words':Articles_without_stopwords,'Article with stop words': Articles_with_stopwords,'Summary':Summaries})\ndata.head()","21523f60":"data['Articles without stop words'][0]","a3a711da":"for i in range(5):\n    print(data['Article with stop words'][i])\n    print(\"---------------------------------------------\")","32a198d4":"data['Summary'][0]","55099ae8":"import re","bc6c318b":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}","d0ac23fe":"def clean_text(text):\n    \n    text=text.lower()\n    text=' '.join([contraction_mapping[i] if i in contraction_mapping.keys() else i for i in text.split()])\n    text=re.sub(r'\\(.*\\)',\"\",text)\n    text=re.sub(\"'s\",\"\",text)\n    text=re.sub('\"','',text)\n    text=' '.join([i for i in text.split() if i.isalpha()])\n    text=re.sub('[^a-zA-Z]',\" \",text)\n    \n    return text","392f3ed5":"data['Article with stop words']=data['Article with stop words'].apply(clean_text)\ndata['Articles without stop words']=data['Articles without stop words'].apply(clean_text)\ndata['Summary']=data['Summary'].apply(clean_text)\ndata['Summary']='<START> '+data['Summary']+' <END>'","b5ee7e4a":"data['Article with stop words'][0]","6ab2af38":"data['Articles without stop words'][0]","7d397b91":"data['Summary'][0]","daf1d4be":"X_train,X_val,Y_train,Y_val=train_test_split(data['Article with stop words'],data['Summary'],test_size=0.3,random_state=29)\nprint(len(X_train),len(Y_train))\nprint(len(X_val),len(Y_val))","1486d1aa":"art_len=[len(i.split()) for i in X_train]\nsum_len=[len(i.split()) for i in Y_train]\nplt.hist(art_len,bins=100)\nplt.title('Article')\nplt.show()\nplt.hist(sum_len,bins=50)\nplt.title('Summary')\nplt.show()","6d2d682c":"max_art_len=500\nmax_sum_len=100","f3b31815":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","cf3631a0":"article_tokenizer=Tokenizer(oov_token='<UNK>')\narticle_tokenizer.fit_on_texts(X_train)\ntokenized_X_train=article_tokenizer.texts_to_sequences(X_train)\ntokenized_X_val=article_tokenizer.texts_to_sequences(X_val)","fa69b390":"art_vocab_size=len(article_tokenizer.word_index)+1","55221cf5":"padded_X_train=pad_sequences(tokenized_X_train,maxlen=max_art_len,padding='post',truncating='post')\npadded_X_val=pad_sequences(tokenized_X_val,maxlen=max_art_len,padding='post',truncating='post')","3cdf7610":"print(padded_X_train.shape,padded_X_val.shape)","8eadb3d5":"summary_tokenizer=Tokenizer(oov_token='<UNK>')\nsummary_tokenizer.fit_on_texts(Y_train)\ntokenized_Y_train=summary_tokenizer.texts_to_sequences(Y_train)\ntokenized_Y_val=summary_tokenizer.texts_to_sequences(Y_val)","ed340808":"sum_vocab_size=len(summary_tokenizer.word_index)+1","106c9d38":"padded_Y_train=pad_sequences(tokenized_Y_train,maxlen=max_sum_len,padding='post',truncating='post')\npadded_Y_val=pad_sequences(tokenized_Y_val,maxlen=max_sum_len,padding='post',truncating='post')","7eeff679":"print(padded_Y_train.shape,padded_Y_val.shape)","2f8d5a2e":"reverse_art_index=article_tokenizer.index_word\nreverse_sum_index=summary_tokenizer.index_word\nsum_wordindex=summary_tokenizer.word_index","501e1ef9":"encoder_inputs=Input(shape=(max_art_len,))\nencoder_emb=Embedding(art_vocab_size,100,trainable=True,name='Encoder_Embedding_layer')(encoder_inputs)\nencoder_lstm1=LSTM(300,return_sequences=True,return_state=True,name='Encoder_LSTM1')\nenclstm1_outputs,enclstm1_h,enclstm1_c=encoder_lstm1(encoder_emb)\n\n\ndecoder_inputs=Input(shape=(None,))\ndecoder_em=Embedding(sum_vocab_size,100,trainable=True,name='Decoder_Embedding_layer')\ndecoder_emb=decoder_em(decoder_inputs)\n\ndecoder_lstm1=LSTM(300,return_sequences=True,return_state=True,name='Decoder_LSTM1')\ndeclstm1_output,declstm1_h,declstm1_c=decoder_lstm1(decoder_emb,initial_state=[enclstm1_h,enclstm1_c])\n\noutput_layer=TimeDistributed(Dense(sum_vocab_size,activation='softmax',name='softmax'))\noutput=output_layer(declstm1_output)\n\nmodel=Model([encoder_inputs,decoder_inputs],output)\n\nmodel.summary()","187e2f9e":"# plot_model(model,show_shapes=True)","646ecd27":"model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy')","2afa7c6e":"model.fit([padded_X_train,padded_Y_train[:,:-1]],padded_Y_train[:,1:],\n          epochs=1,\n          validation_data=([padded_X_val,padded_Y_val[:,:-1]],padded_Y_val[:,1:]),\n          batch_size=64)","ee685558":"infencoder_model=Model(inputs=[encoder_inputs],outputs=[enclstm1_outputs,enclstm1_h,enclstm1_c])\n\ninfdecoder_model_state_input_h=Input(shape=(300,),name='infdec_I1')\ninfdecoder_model_state_input_c=Input(shape=(300,),name='infdec_I2')\n\ninfdeclstm1_output,infdec_h,infdec_c=decoder_lstm1(decoder_emb,initial_state=[infdecoder_model_state_input_h,\n                                                                                                infdecoder_model_state_input_c\n                                                                                               ])\n\ninfdec_output=output_layer(infdeclstm1_output)                         \n\ninfdecoder_model=Model(inputs=[decoder_inputs]+[infdecoder_model_state_input_h,infdecoder_model_state_input_c],\n                       outputs=[infdec_output]+[infdec_h,infdec_c])\n\n","3ef883fd":"infencoder_model.summary()","ce4604db":"plot_model(infencoder_model,show_shapes=True)","57aa707b":"infdecoder_model.summary()","b51d81f1":"plot_model(infdecoder_model,show_shapes=True)","50eaaded":"def decode_sequence(inp_seq):\n    \n    enc_out,enc_h,enc_c=infencoder_model.predict(inp_seq)\n    \n    tar_seq=np.zeros((1,1))\n    tar_seq[0,0]=sum_wordindex['start']\n    \n    stop_loop=False\n    decoded_string=''\n    \n    while not stop_loop:\n       \n        dec_out,dec_h,dec_c=infdecoder_model.predict([tar_seq]+[enc_h,enc_c])\n        \n        tar_token_index=np.argmax(dec_out[0,-1,:])\n        tar_token_word=sum_wordindex[tar_token_index]\n        \n        if tar_token_word =='end' or len(decoded_string)>=max_art_len:\n            \n            stop_loop=True\n        else:\n            decoded_string+=tar_token_word\n            \n            tar_seq=np.zeros((1,1))\n            tar_seq[0,0]=tar_token_index\n            \n            \n            enc_h=dec_h\n            enc_c=dec_c\n            \n    return decoded_string","05d166df":"def seq2art(inp_seq):\n    \n    art=''\n    \n    for i in range(len(inp_seq)):\n        \n        if inp_seq[i]==0:\n            break\n        art+=reverse_art_index[inp_seq[i]]+' '\n        \n    return art\n\n\ndef seq2sum(inp_seq):\n    \n    summary=''\n    \n    for i in range(len(inp_seq)):\n        \n        if inp_seq[i]==0:\n            break\n        word=reverse_sum_index[inp_seq[i]]\n        summary+=word+' '\n            \n    return summary\n        ","1adc75bb":"print('Example Articel : '+'\\n',seq2art(padded_X_val[2]))\nprint('Example Summary : '+'\\n',seq2sum(padded_Y_val[2]))\nprint('Predicted Summary : '+'\\n',decode_sequence(padded_X_val[2]))","0c9635a6":"## Text Cleaning","e2a3a672":"## Prediction","431146e2":"## Vectorization","b44e0313":"# Model\n","b69912cd":"## Train Val Split","09a20195":"## Inference","48c765b8":"# Data Preprocess"}}