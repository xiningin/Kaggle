{"cell_type":{"876f8cb3":"code","04891845":"code","b6bb1730":"code","79015851":"code","b3e47131":"code","d8ed0d15":"code","8e544b1a":"code","7f0b2bc0":"code","a8c63554":"code","84f3c471":"code","fdc46de0":"code","4076146d":"code","9a18e5fa":"code","98e263bd":"code","79579e96":"code","5727b3f8":"code","4959730e":"code","df8f07f8":"code","439b789e":"code","5444761a":"code","c6c860dd":"code","262901f8":"code","2240b3b8":"code","e3c19917":"code","b7d6f49c":"code","c8c2e725":"code","dcbb0ff7":"code","25fc24b3":"code","d4a7d50d":"code","17410e75":"code","492f9e5d":"code","3d2b00d6":"code","05c1a602":"code","9c19a027":"code","b3558507":"code","9c66ae85":"code","6ddf27eb":"code","fcbb47b5":"code","72b0840e":"code","a7bf92d6":"code","4758094b":"code","e569f2a8":"code","2fa1480c":"code","d9a5e443":"code","7df83918":"code","3c639a20":"markdown","119c7a94":"markdown","cd773c3f":"markdown","b608c775":"markdown","babf51a7":"markdown","5e8d53ee":"markdown","34641e33":"markdown","b59c6b97":"markdown","f4c1cf75":"markdown","2b4e0461":"markdown","6d2692ac":"markdown","c0cd77d3":"markdown","267c98bc":"markdown","46258d5b":"markdown","5ee679cb":"markdown","f4954fde":"markdown","5f3159cf":"markdown","b71bfc32":"markdown"},"source":{"876f8cb3":"pip install beautifulsoup4","04891845":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport string\n# Natural Language tool kit\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom bs4 import BeautifulSoup","b6bb1730":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","79015851":"train_df.head()","b3e47131":"test_df.head()","d8ed0d15":"train_df.info()","8e544b1a":"train_df.isnull().sum()","7f0b2bc0":"sns.heatmap(train_df.isnull())","a8c63554":"total = train_df.shape[0]","84f3c471":"train_df.isnull().sum()\/(train_df.shape[0])","fdc46de0":"train_df.drop(['location'],axis=1,inplace=True)","4076146d":"p = train_df['target'].value_counts()\/(train_df.shape[0])","9a18e5fa":"\nax = sns.countplot(y = 'target',data=train_df)\n\nfor p in ax.patches:\n    percent = '{:.1f}%'.format(100*(p.get_width()\/total))\n    x = p.get_x()+p.get_width()\n    y = p.get_y()+(p.get_height())\/2\n    ax.annotate(percent,(x,y))\n","98e263bd":"train_df[train_df['target'] == 0]","79579e96":"train_df['keyword'].dropna(inplace=True)","5727b3f8":"import re\n","4959730e":"## Removing URL's\ndef remove_url(text):\n    newtext=\" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \",text).split())\n    return newtext","df8f07f8":"## Making text to Lower case\ndef lower_case(text):\n    return text.lower()\n\ndef butiful(text):\n    text = BeautifulSoup(text).get_text()\n    return text\n\n## Removing numbers\ndef remove_num(text):\n    newtext= re.sub(r'\\d+',\"\",text)\n    return newtext\n\n## removing punctuation\ndef remove_punc(text):\n    trans = str.maketrans(\"\",\"\",string.punctuation)\n    return text.translate(trans)\n","439b789e":"#Tokenizing\ndef tokenize(text):\n    text = word_tokenize(text)\n    return text\n\n# Removing Stop Words\ndef remove_stop(text):\n    text  = [i for i in text if not i in stopwords.words('english')]\n    return text\n\n#Lemmatize\n\nword_lem = WordNetLemmatizer()\n#we are using recursive call since for eg: calling can be writen as (called,caller,call). So we are lemmatizing into the word call\ndef Lemmatize(text):\n    text = [word_lem.lemmatize(token) for token in text]\n    return text\n\n# Preprocessing\n\ndef preprocess(text):\n    text = remove_url(text)\n    text = butiful(text)\n    text = lower_case(text)\n    text = remove_num(text)\n    text = remove_punc(text)\n    text = tokenize(text)\n    text = remove_stop(text)\n    text = Lemmatize(text)\n    text = \" \".join(text)\n    return text","5444761a":"train_preprocess = []\nfor i in train_df['text']:\n    text_data = preprocess(i)\n    train_preprocess.append(text_data)\ntrain_df['processed_text'] = train_preprocess","c6c860dd":"nltk.download('punkt')","262901f8":"nltk.download('stopwords')","2240b3b8":"nltk.download('wordnet')","e3c19917":"a = test_df.shape\nb = test_df.isnull().sum()\nc = test_df.info()\nprint(a,b,c)","b7d6f49c":"test_df.drop('location',axis=1,inplace=True)","c8c2e725":"test_df['keyword'].dropna(inplace=True)","dcbb0ff7":"test_preprocess = []\nfor i in test_df['text']:\n    text_data_test = preprocess(i)\n    test_preprocess.append(text_data_test)\ntest_df['processed_text'] = test_preprocess","25fc24b3":"train_df.head()","d4a7d50d":"test_df.head()","17410e75":"from sklearn import feature_extraction,linear_model,preprocessing,model_selection\ncount_vect = feature_extraction.text.CountVectorizer()\nex_train_vec = count_vect.fit_transform(train_df['processed_text'])\nex_test_vec = count_vect.transform(test_df['processed_text'])","492f9e5d":"ex_train_vec.todense().shape","3d2b00d6":"ex_test_vec.todense().shape","05c1a602":"model = linear_model.RidgeClassifier()","9c19a027":"score = model_selection.cross_val_score(model,ex_train_vec,train_df['target'],cv =3)","b3558507":"score","9c66ae85":"train_list = list(train_df['processed_text'])\ntest_list = list(test_df['processed_text'])\ncorpus = train_list+test_list","6ddf27eb":"corpus[1:5]","fcbb47b5":"\"\"\"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\nfit_vec = tfidf.fit(corpus)\ntrain_trasform = tfidf.transform(train_df['processed_text'])\ny = train_df['target']\ntest_transform = tfidf.transform(test_df['processed_text'])\"\"\"","72b0840e":"from sklearn.feature_extraction.text import CountVectorizer\ncount = CountVectorizer()\nfit_count = count.fit(corpus)\ntrain_transform = count.transform(train_df['processed_text']).toarray()\ny = train_df['target']\ntest_transform = count.transform(test_df['processed_text']).toarray()","a7bf92d6":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\ncross_val = model_selection.cross_val_score(reg,train_transform,y,cv=3)","4758094b":"cross_val","e569f2a8":"reg.fit(train_transform,y)\npredict = reg.predict(test_transform) ","2fa1480c":"sub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","d9a5e443":"sub['target'] = predict","7df83918":"sub.to_csv('submission.csv',index=False)","3c639a20":"###### first we are using count vectorizer","119c7a94":"### Test Data Preprocessing","cd773c3f":"##### let Solve using tf-idf vectorisation","b608c775":"we can see there are 12840 unique text","babf51a7":"### Preprocessing NLP methods\n\nThe most common preprocessing methods are:\n\n> Tokenize (tokenizing means spliting string into words)\n\n> Lemmatize (Lemmatizeing given bu eg: if we have a word (learn,learning) learning can be lemmatized to learn.)\n\n> Removing stop words (stop wordsa are joining words used to join the text like(is,was,and,or etc.))","5e8d53ee":"the below are downloaded since it has shown error in module","34641e33":"Now we can vectorize the text data ","b59c6b97":"### Train Preprocessing","f4c1cf75":"we can see that most of the Null values are from Location and few are from keyword ","2b4e0461":"### Exploring the dataset","6d2692ac":"Since the model can't classify strings having special charecter and emails. we are using regular expresion to remove\n\n'(@[A-Za-z0-9]+)' indicates the string starts with @ and the string should contain Capital, small, number.'+' indicates it canbe one or more occurance of [A-Za-z0-9] thus giving @bbcmtd\n\n'([^0-9A-Za-z \\t])' indicates the string should not contain small letter, capital letter, and numbers and '\\t' indicates Tabs.THus giving all the special characters like '.,:,-' etc.\n\n'(\\w+:\\\/\\\/\\S+)' '\\w' indicates the string should have the character of letters and numbers,'+' indicates one or more characters,followed by :'\\' indicates escape which escapes \/ and followed by similar patter then '\\S' indicates one non whitspace character.'+' indicatesz one or more of '\\S' thus giving http:\/\/t.co\/lHYXEOHY6C'\n\nin order to get ms with formating we are replacing all the above regular exxpression answers with \" \" by using re.sub.\n\nspliting would give the letters in list and \" \".join will give sting with space. \n","c0cd77d3":"we can use text column to train the model, the words inside the text can indicate it is a disaster or not so we are using countvector to count the words in the tweet and turn them into data","267c98bc":"https:\/\/www.etutorialspoint.com\/index.php\/386-tf-idf-tfidfvectorizer-tutorial-with-examples\n\nAccording to scikit-learn\u2019s website, TfidfVectorizer is actually CountVectorizer followed by TfidfTransformer. CountVectorizer first takes our text documents and tokenizes them, as we did before (but then un-did because this function does not accept tokenized data as input). Once the data have been tokenized, CountVectorizer assembles a bag of words consisting of every unique token and assigning each a number. Finally, CountVectorizer represents the tokenized text data as a matrix of token counts, which looks like this:\n\nThis image shows the first six rows of the CountVectorizer matrix. These rows tell us that in document 0, the words 368, 3403, 4146, 5201, 8491, and 11223 all appear once. We are interested in these counts because if a word appears many times in a document, that word is probably very significant.\nTfidfTransformer simply transforms this matrix of token counts to a term frequency-inverse document frequency (tf-idf) representation. Using tf-idf is important because simply using token counts can be misleading. Previously, we assumed that if a word appeared many times in a document, it was important. What if that word is extremely common in the entire corpus? Then it\u2019s high frequency in our current document would be less significant, because the word appears so frequently elsewhere.\nTf-idf strikes a balance by taking the term frequency (basically the count) and multiplying it by the inverse document frequency (1\/document frequency). This means that if word 1 appears once in document A but also once in the total corpus, while word 2 appears four times in document A but 16 in the total corpus, word 1 will have a tf-idf score of 1.0 while word 2 will only receive a score of 0.25. Word 2\u2019s importance in document A is diluted by its high frequency in the corpus. (This is a simplified explanation of the actual tf-idf equation, which is more complicated.)\nHence, we arrive at this representation of document 0:\n\nNotice that in the CountVectorizer representation, all the tokens in document 1 appeared only once. Now, in the tf-idf representation, some tokens have higher scores than others. Tf-idf has added a layer of nuance to our data.","46258d5b":"### Next Step is Preprocessing of data in NLP.\n\nPreprocessing of data might vary for each use cases in NLP.here we can see from the above special characters, email id and twitter user name with many complex string are given to process.We first get data between a email like\n\n\" @shawn Titanic tragedy could have been prevented Economic Times: Telegraph.co.ukTitanic tragedy could have been preve... http:\/\/bet.ly\/tuN2wx\"\n\nwe have to get string like \"Titanic tragedy could have been prevented Economic Times Telegraph co ukTitanic tragedy could have been preve\"","5ee679cb":"##### Logistic Regression","f4954fde":"we can see that data is balanced","5f3159cf":"Lets combine test_df['processed_text'] and train_df['processed_text'] into corpus.\n\nby combining we can get the propotion of most occurance of the text ","b71bfc32":"we can see that 0.33% of Null values are missing which is huge amount which will make huge prediction errors, also location data might not be as useful so we can drop the data"}}