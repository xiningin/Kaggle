{"cell_type":{"e980c131":"code","4ec53de6":"code","22efac7c":"code","d4b8e804":"code","e9542adc":"code","7bb55716":"code","ed60d976":"code","e600419f":"code","10a60f17":"code","6250296d":"code","1d4507c1":"code","48841895":"code","5ab498dc":"code","1ebd23d2":"code","fcdd12d9":"code","c48c6edc":"code","12e65e9c":"code","8a4f39bd":"code","cb787c06":"code","3b4d456b":"code","5bc0946f":"code","046461c4":"code","f5487cad":"code","29d6a14b":"code","e4aca323":"code","4968d49a":"code","1dce57f4":"code","385ca00e":"code","67bedcf7":"code","6d1cd07a":"code","156403d5":"code","d8a93e5f":"code","4ef986e7":"markdown","6ea999aa":"markdown","06b12fe6":"markdown"},"source":{"e980c131":"import pandas as pd\nimport numpy as np","4ec53de6":"df=pd.read_csv('..\/input\/pratik\/prat.csv',error_bad_lines=False)\ndf=df[['Message','Category']]","22efac7c":"df.head(20)","d4b8e804":"df.head()","e9542adc":"import matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.decomposition import NMF\nfrom sklearn.preprocessing import normalize","7bb55716":"Message=df['Message'].iloc[0]","ed60d976":"Message","e600419f":"from bs4 import BeautifulSoup","10a60f17":"df1=BeautifulSoup(Message,'html.parser')\nsoup=df1.get_text()","6250296d":"soup","1d4507c1":"import re\nmessage=re.sub('\\[[^]]*\\']', ' ',Message)\nmessage=re.sub('[^a-zA-Z]', ' ',Message)\nmessage","48841895":"message.lower()","5ab498dc":"stopw=stopwords.words('english')","1ebd23d2":"message=message.split()\nmessage","fcdd12d9":"message=[word for word in message if not word in set(stopw)]\nmessage","c48c6edc":"from nltk.stem.porter import PorterStemmer\nps=PorterStemmer()\nmessage_s=[ps.stem(word) for word in message]\nmessage","12e65e9c":"from nltk.stem import WordNetLemmatizer\n\nlem=WordNetLemmatizer()\nmessage=[lem.lemmatize(word) for word in message]\nmessage","8a4f39bd":"message = ' '.join(message)\nmessage","cb787c06":"corpus=[]\ncorpus.append(message)","3b4d456b":"count_vec=CountVectorizer()\nmessage_count_vec=count_vec.fit_transform(corpus)\nmessage_count_vec.toarray()","5bc0946f":"count_vec_bin = CountVectorizer(binary=True)\nmessage_count_vec_bin = count_vec_bin.fit_transform(corpus)\n\nmessage_count_vec_bin.toarray()","046461c4":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vec=TfidfVectorizer()\nmessage_tfidf_vec=tfidf_vec.fit_transform(corpus)\nmessage_tfidf_vec.toarray()\n","f5487cad":"from sklearn.model_selection import train_test_split\ndataset_train,dataset_test,train_data_label,test_data_label=train_test_split(df['Message'],df['Category'],test_size=.30,random_state=42)","29d6a14b":"train_data_label = (train_data_label.replace({'spam': 1, 'ham': 0})).values\ntest_data_label  = (test_data_label.replace({'spam': 1, 'ham': 0})).values","e4aca323":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\ncorpus_train=[]\ncorpus_test=[]\n\nfor i in range(dataset_train.shape[0]):\n    soup = BeautifulSoup(dataset_train.iloc[i], \"html.parser\")\n    message = soup.get_text()\n    message = re.sub('\\[[^]]*\\]', ' ', Message)\n    message = re.sub('[^a-zA-Z]', ' ', Message)\n    message = message.lower()\n    message = message.split()\n    message = [word for word in message if not word in set(stopwords.words('english'))]\n    lem = WordNetLemmatizer()\n    message = [lem.lemmatize(word) for word in message]\n    message = ' '.join(message)\n    corpus_train.append(message)\n    \nfor j in range(dataset_test.shape[0]):\n    soup = BeautifulSoup(dataset_test.iloc[j], \"html.parser\")\n    message = soup.get_text()\n    message = re.sub('\\[[^]]*\\]', ' ', Message)\n    message = re.sub('[^a-zA-Z]', ' ', Message)\n    message = message.lower()\n    message = message.split()\n    message = [word for word in message if not word in set(stopwords.words('english'))]\n    lem = WordNetLemmatizer()\n    message = [lem.lemmatize(word) for word in message]\n    message = ' '.join(message)\n    corpus_test.append(message)\n    \n","4968d49a":"tfidf_vec = TfidfVectorizer(ngram_range=(1, 3))\n\ntfidf_vec_train = tfidf_vec.fit_transform(corpus_train)\ntfidf_vec_test = tfidf_vec.transform(corpus_test)","1dce57f4":"from sklearn.svm import LinearSVC\nLinear_svc=LinearSVC(C=.5,random_state=42)\nLinear_svc.fit(tfidf_vec_train,train_data_label)\npredict = Linear_svc.predict(tfidf_vec_test)","385ca00e":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(\"Classification Report: \\n\", classification_report(test_data_label, predict))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(test_data_label, predict))\nprint(\"Accuracy: \\n\", accuracy_score(test_data_label, predict))","67bedcf7":"dataset_predict = dataset_test.copy()\ndataset_predict = pd.DataFrame(dataset_predict)\ndataset_predict.columns = ['Message']\ndataset_predict = dataset_predict.reset_index()\ndataset_predict = dataset_predict.drop(['index'], axis=1)\ndataset_predict.head()","6d1cd07a":"test_actual_label = test_data_label.copy()\ntest_actual_label = pd.DataFrame(test_actual_label)\ntest_actual_label.columns = ['Category']\ntest_actual_label['Category'] = test_actual_label['Category'].replace({1: 'spam', 0: 'ham'})","156403d5":"test_predicted_label = predict.copy()\ntest_predicted_label = pd.DataFrame(test_predicted_label)\ntest_predicted_label.columns = ['predicted_sentiment']\ntest_predicted_label['predicted_sentiment'] = test_predicted_label['predicted_sentiment'].replace({1: 'spam', 0: 'ham'})","d8a93e5f":"test_result = pd.concat([dataset_predict, test_actual_label, test_predicted_label], axis=1)\ntest_result.head(120)","4ef986e7":"##So we can see the data has become numeric with 1,2 and 3s..... based on the number of times they appear in the text. There is another variation of CountVectorizer with binary=True and in that case all zero entries will have 1.j","6ea999aa":"## our next thing is to convert it into mathematical form","06b12fe6":"## Data cleaning"}}