{"cell_type":{"6a0ed240":"code","f26d9068":"code","5df12181":"code","f7b4146c":"code","6b3bca34":"code","cfa790f4":"code","3ae408b4":"code","1fbaa93a":"code","fc415f6a":"code","de804b98":"code","92539bf6":"code","39b83d07":"code","71496ca7":"code","70995bd8":"markdown","10ecff56":"markdown","6e159da6":"markdown","92a90b74":"markdown"},"source":{"6a0ed240":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pickle\nfrom collections import Counter\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\nimport os\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom xgboost import XGBRegressor","f26d9068":"# self exlanatory function\ndef load_year(y):\n    dtypes = pickle.load(open(\n        f'..\/input\/kdd2020-cpr\/{y}_dtypes.pkl', 'rb'\n    ))\n    del dtypes['date']\n    df = pd.read_csv(\n        f'..\/input\/kdd2020-cpr\/{y}.csv',\n        dtype=dtypes, parse_dates=['date']\n    )\n    return df","5df12181":"YEAR = 2018\nCOMPONENTS = 100\nXGB_ESTIMATORS = 50\nMAGICIAN = 'TruncatedSVD'","f7b4146c":"ls ..\/input","6b3bca34":"%%time\ndf = load_year(YEAR)","cfa790f4":"%%time\ndef drop_fullnull(df, inplace=False):\n    mask = df.isnull().all()\n    labels = df.columns[mask]\n    \n    shape = df.shape\n    print(labels)\n    if inplace:\n        df.drop(labels=labels, axis=1, inplace=True)\n    else:\n        df = df.drop(labels=labels, axis=1)\n    if labels.any():\n        if shape == df.shape:\n            print('lables:', labels)\n        else:\n            print(shape, df.shape)\n    return df\n\n# I tried this but thinks end up more complicated\n# since the shape may change\n# clean = drop_fullnull(df)\n# clean.shape","3ae408b4":"magician = TruncatedSVD(\n    n_components=COMPONENTS,\n    random_state=0\n)","1fbaa93a":"cp = df.copy()\n\n# Simplest imputation of all\ncp.fillna(0, inplace=True)\n\nout_mask = df.columns.str.contains('output')\nout_cols = df.columns[df.columns.str.contains('output')]\ncp_out = cp.loc[:, out_mask]\ncp.drop(['id'] + out_cols.tolist(),\n        axis=1, inplace=True\n)\ncp['day'] = cp.date.dt.day\ncp['month'] = cp.date.dt.month\ncp['year'] = cp.date.dt.year\ncp.drop('date', inplace=True, axis=1)\n\nmagician.fit(cp)","fc415f6a":"xgb_params = {\n    'n_estimators': XGB_ESTIMATORS,\n    'random_state': 0,\n    'n_jobs': -1,\n    'learning_rate': .1,\n    'max_depth': 10,\n    'tree_method': 'gpu_hist',\n    'verbosity': 2,\n    'objective': 'reg:squarederror',\n    \n}\nmodel = XGBRegressor(**xgb_params)\nclf = MultiOutputRegressor(model)\n\nclf.fit(pd.DataFrame(magician.transform(cp)), cp_out);","de804b98":"d9 = load_year(2019)","92539bf6":"def svd_preprocess(df):\n    cp = df.copy()\n    ids = df.id.copy()\n    \n    cp.fillna(0, inplace=True)\n    cp.drop(['id'], axis=1, inplace=True, errors='ignore')\n    cp['day'] = cp.date.dt.day\n    cp['month'] = cp.date.dt.month\n    cp['year'] = cp.date.dt.year\n    cp.drop('date', inplace=True, axis=1)\n    return cp, ids\n\nX, ids = svd_preprocess(d9)\n\ndf_in = pd.DataFrame(magician.transform(X))\ndf_in.index = ids","39b83d07":"y_pred = clf.predict(df_in)\n\nout_cols_flat = out_cols.ravel()\nid_col = []\nfor i in ids:\n    id_col.extend([f'{i}_{sufix}' for sufix in out_cols_flat])","71496ca7":"df_sub = pd.DataFrame(\n    {'id':id_col , 'value':y_pred.ravel()}\n)\ndf_sub.set_index('id', inplace=True)\ndf_sub.to_csv(\n    f'submission-{YEAR}-{COMPONENTS}{MAGICIAN}-{XGB_ESTIMATORS}xgb.csv',\n    index='id'\n)\n# If you want to use the model later, just uncomment\n# line below\n# pickle.dump(clf, open('clf.pkl', 'wb'))","70995bd8":"## If you enjoy this approach or it helped you somehow, please UPvote this kernel :)","10ecff56":"And now let's predict and submit","6e159da6":"## There are just *so many features*. Besides, their meaning are *unknown* to us. Therefore, why don't try to somehow reduce this high dimensionality?\n## SVD to rescue!","92a90b74":"## Let's \"train\" the SVD instance"}}