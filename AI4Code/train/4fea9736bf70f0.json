{"cell_type":{"deb2d80e":"code","a26837f4":"code","61bc16e0":"code","adbfa0dc":"code","9fa9c094":"code","7cad1c23":"code","9d4f7835":"code","f2c500b4":"code","ec52a5f0":"code","4728aac8":"code","3b13daf4":"code","e0551847":"code","4bdb0dea":"code","2d72fd89":"code","7bc60362":"code","a93ab544":"code","711e3b2a":"code","885efa98":"code","47aff23a":"code","e0b633a1":"code","4f31d3cd":"code","88858991":"code","54c20506":"code","44f62ea3":"code","0546da4a":"code","dd8dab9d":"code","ba178523":"code","4a37c35b":"code","e471af77":"code","e77ec80f":"markdown","913876a9":"markdown","0a3293c2":"markdown","4897a63a":"markdown","66cf5038":"markdown","e78b036b":"markdown","034b04e8":"markdown","f9c61ae2":"markdown","ca2a410c":"markdown","0535bddd":"markdown","13951fce":"markdown","98341956":"markdown","aa9d5b9f":"markdown","ecabb08f":"markdown","231a7014":"markdown","395c7242":"markdown","726503fd":"markdown","26240f02":"markdown","8f326645":"markdown","34aef530":"markdown","9498818f":"markdown","d9771d94":"markdown","c015e720":"markdown","1aa1bf54":"markdown","5a2c81de":"markdown","06df57ab":"markdown","b49efe3e":"markdown","cf709b37":"markdown","b5f43d17":"markdown","c0fd4619":"markdown","2185922e":"markdown","32e9ec8a":"markdown"},"source":{"deb2d80e":"import os\nimport warnings\n\n# the usual libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nimport optuna","a26837f4":"# Pandas setting\npd.set_option(\"display.max_columns\", None)\n\n# Mute warnings\n# warnings.filterwarnings('ignore')\n\n# Set Random State \nSEED = 42\n\n# Optuna Parameters\nTUNING_LASSO = False # When True, hyperparameter tuning  with Optuna will be executed. False to skip the tuning step\nTUNING_XGB = False\nTRAIN_TIME = 4 * 60 * 60 \nN_TRIALS = 50\nSTUDY_NAME = 'STUDY'","61bc16e0":"def clean(df):\n    \n    df.drop(['Id'], axis=1, inplace=True)\n    \n    # Create features for Shed, Gar2 and Othr\n    miscf = ['Shed','Gar2', 'Othr']\n    for f in miscf:\n        df['MiscFeature'+f] = df.apply(lambda x: x['MiscVal'] if x['MiscFeature']==f else np.NaN , axis=1)\n    df.drop(['MiscFeature','MiscVal'], axis=1, inplace=True)\n    \n    # Fix typos\n    df['Exterior1st'] = df['Exterior1st'].replace({\"WdShing\": \"Wd Shng\"})\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"CmentBd\": \"CemntBd\"})\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n    df[\"BldgType\"] = df[\"BldgType\"].replace({\"Twnhs\": \"TwnhsI\"})\n    \n    #Fix erroneous data\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt != 2207, 2007)\n    \n    return df","adbfa0dc":"def load_data():\n    \n    train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n    X_train = train.copy()\n    y_train = X_train['SalePrice'] #do not remove the target from X_train. We'll need it for target encoding.\n\n    test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n    X_test = test.copy()\n\n    X_train = clean(X_train)\n    X_test = clean(X_test)\n\n    all_features = ['MSSubClass','MSZoning','LotFrontage','LotArea','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','OverallQual','OverallCond','YearBuilt','YearRemodAdd','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','MasVnrArea','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','Heating','HeatingQC','CentralAir','Electrical','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','KitchenQual','TotRmsAbvGrd','Functional','Fireplaces','FireplaceQu','GarageType','GarageYrBlt','GarageFinish','GarageCars','GarageArea','GarageQual','GarageCond','PavedDrive','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','PoolQC','Fence','MoSold','YrSold','SaleType','SaleCondition','SalePrice','MiscFeatureShed','MiscFeatureGar2','MiscFeatureOthr'] #do not remove the target. We'll need it for target encoding.\n    num_features = ['BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars','LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscFeatureGar2','MiscFeatureOthr','MiscFeatureShed','MoSold','YearBuilt','YearRemodAdd','YrSold','GarageYrBlt']\n    num_continuous_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea', 'GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscFeatureGar2','MiscFeatureOthr','MiscFeatureShed','MoSold','YearBuilt','YearRemodAdd','YrSold','GarageYrBlt']\n    num_discrete_features=['BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars']\n    cat_features = ['LotShape','LandContour','OverallQual','OverallCond','Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','HeatingQC','KitchenQual','FireplaceQu','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MSSubClass','MSZoning','Street','Alley','LotConfig','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation','BsmtFinType1','BsmtFinType2','Heating','CentralAir','Electrical','Functional','GarageType','PavedDrive','SaleType','SaleCondition']\n    cat_features_to_encode = cat_features.copy()\n    cat_features_to_encode.remove('OverallQual') #qualitative feature already in numeric format and correctly ordered\n    cat_features_to_encode.remove('OverallCond') #qualitative feature already in numeric format and correctly ordered\n    \n    return train, test, X_train, y_train, X_test, all_features, num_features, num_continuous_features, num_discrete_features, cat_features, cat_features_to_encode","9fa9c094":"def cv_loop(\n        X_train, \n        y_train,\n        X_test,\n        model, \n        useful_features,\n        num_features,\n        cat_features,\n        cat_features_to_encode, \n        encoding = 'ohe', \n        new_features=[],\n        scaling=False,\n        clip=False, clipmin=np.log(34900), clipmax=np.log(755000),\n        tuning=True,\n        early_stopping=True\n        ):\n    \n    y_train = np.log(y_train)\n    \n    num_features = np.intersect1d(num_features, useful_features)\n    cat_features = np.intersect1d(cat_features, useful_features)\n    cat_features_to_encode = np.intersect1d(cat_features_to_encode, useful_features)\n    \n    cum_rmse_val = 0\n    iteration = 1\n    \n    N_SPLITS = 5\n    \n    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n    for train_index, val_index in kf.split(X_train, y_train): \n        X_train_, X_val_ = X_train.iloc[train_index], X_train.iloc[val_index]\n        y_train_, y_val_ = y_train[train_index], y_train[val_index]\n\n        X_train__ = X_train_.reset_index(drop=True)\n        X_val__ = X_val_.reset_index(drop=True)\n\n        # Select Features\n        X_train__ = X_train__.loc[:, useful_features]\n        X_val__ = X_val__.loc[:, useful_features]\n        if(tuning==False): \n            useful_features_ = useful_features.copy()\n            useful_features_.remove('SalePrice')\n            X_test__ = X_test.loc[:, useful_features_]\n        \n        # Impute missing values\n        if(new_features.count('NbNAs')==1):\n            X_train__['NbNAs'] = X_train__.isnull().sum(axis=1)\n            X_val__['NbNAs'] = X_val__.isnull().sum(axis=1)\n            if(tuning==False): X_test__['NbNAs'] = X_test__.isnull().sum(axis=1)\n        for col in num_features:\n            X_train__[col] = X_train__[col].fillna(0)\n            X_val__[col] = X_val__[col].fillna(0)\n            if(tuning==False): X_test__[col] = X_test__[col].fillna(0)\n        for col in cat_features:\n            X_train__[col] = X_train__[col].fillna(\"None\")\n            X_val__[col] = X_val__[col].fillna(\"None\")\n            if(tuning==False): X_test__[col] = X_test__[col].fillna(\"None\")\n        \n        # Create new features\n        #if(new_features.count('NbNAs')==1):\n            # Do nothing, managed when imputing NAs\n        if(new_features.count('LivLotRatio')==1):\n            X_train__['LivLotRatio'] = X_train__['GrLivArea'] \/ X_train__['LotArea']\n            X_val__['LivLotRatio'] = X_val__['GrLivArea'] \/ X_val__['LotArea']\n            if(tuning==False): X_test__['LivLotRatio'] = X_test__['GrLivArea'] \/ X_test__['LotArea']\n        if(new_features.count('Spaciousness')==1):\n            X_train__['Spaciousness'] = X_train__['GrLivArea'] \/ X_train__['TotRmsAbvGrd']\n            X_val__['Spaciousness'] = X_val__['GrLivArea'] \/ X_val__['TotRmsAbvGrd']\n            if(tuning==False): X_test__['Spaciousness'] = X_test__['GrLivArea'] \/ X_test__['TotRmsAbvGrd']\n        # How big\/small is the house in its neighboorhood\n        if(new_features.count('MedNhbdArea')==1): #MedNhbdArea = median of the GrLivArea in the neighboorhood\n            feat = X_train__.groupby('Neighborhood')['GrLivArea'].median()\n            feat = feat.to_dict()\n            X_train__.loc[:,'MedNhbdArea'] = X_train__['Neighborhood'].map(feat)\n            X_val__.loc[:,'MedNhbdArea'] = X_val__['Neighborhood'].map(feat)\n            if(tuning==False): X_test__.loc[:,'MedNhbdArea'] = X_test__['Neighborhood'].map(feat)\n        if(new_features.count('GrLivAreaInNbhd')==1): #GrLivAreaInNbhd = GrLivArea - MedNhbdArea\n            feat = X_train__.groupby('Neighborhood')['GrLivArea'].median()\n            feat = feat.to_dict()\n            X_train__.loc[:,'GrLivAreaInNbhd'] = X_train__['Neighborhood'].map(feat)\n            X_train__['GrLivAreaInNbhd'] = X_train__['GrLivArea'] - X_train__['GrLivAreaInNbhd']\n            X_val__.loc[:,'GrLivAreaInNbhd'] = X_val__['Neighborhood'].map(feat)\n            X_val__['GrLivAreaInNbhd'] = X_val__['GrLivArea'] - X_val__['GrLivAreaInNbhd']\n            if(tuning==False): \n                X_test__.loc[:,'GrLivAreaInNbhd'] = X_test__['Neighborhood'].map(feat)\n                X_test__['GrLivAreaInNbhd'] = X_test__['GrLivArea'] - X_test__['GrLivAreaInNbhd']\n        # How big\/small is the lot in its neighboorhood\n        if(new_features.count('MedNhbdArea_Ext')==1): #MedNhbdArea = median of the LotArea in the neighboorhood\n            feat = X_train__.groupby('Neighborhood')['LotArea'].median()\n            feat = feat.to_dict()\n            X_train__.loc[:,'MedNhbdArea_Ext'] = X_train__['Neighborhood'].map(feat)\n            X_val__.loc[:,'MedNhbdArea_Ext'] = X_val__['Neighborhood'].map(feat)\n            if(tuning==False): X_test__.loc[:,'MedNhbdArea_Ext'] = X_test__['Neighborhood'].map(feat)\n        if(new_features.count('LotAreaInNbhd')==1): #LotAreaInNbhd = LotArea - MedNhbdArea_Ext\n            feat = X_train__.groupby('Neighborhood')['LotArea'].median()\n            feat = feat.to_dict()\n            X_train__.loc[:,'LotAreaInNbhd'] = X_train__['Neighborhood'].map(feat)\n            X_train__['LotAreaInNbhd'] = X_train__['LotArea'] - X_train__['LotAreaInNbhd']\n            X_val__.loc[:,'LotAreaInNbhd'] = X_val__['Neighborhood'].map(feat)\n            X_val__['LotAreaInNbhd'] = X_val__['LotArea'] - X_val__['LotAreaInNbhd']\n            if(tuning==False): \n                X_test__.loc[:,'LotAreaInNbhd'] = X_test__['Neighborhood'].map(feat)\n                X_test__['LotAreaInNbhd'] = X_test__['LotArea'] - X_test__['LotAreaInNbhd']\n        if(new_features.count('OverallQualCondProduct')==1):\n            X_train__['OverallQualCondProduct'] = X_train__['OverallQual'] * X_train__['OverallCond']\n            X_val__['OverallQualCondProduct'] = X_val__['OverallQual'] * X_val__['OverallCond']\n            if(tuning==False): X_test__['OverallQualCondProduct'] = X_test__['OverallQual'] * X_test__['OverallCond']\n        #LowQualFinRatio\n        if(new_features.count('LowQualFinRatio')==1):\n            X_train__['LowQualFinRatio'] = X_train__['LowQualFinSF'] \/ X_train__['GrLivArea']\n            X_val__['LowQualFinRatio'] = X_val__['LowQualFinSF'] \/ X_val__['GrLivArea']\n            if(tuning==False): X_test__['LowQualFinRatio'] = X_test__['LowQualFinSF'] \/ X_test__['GrLivArea']\n        \n        # Encode categorical variables\n        if(encoding=='ohe'):\n            enc = OneHotEncoder(handle_unknown = 'ignore')\n            X_train__enc = pd.DataFrame(enc.fit_transform(X_train__[cat_features_to_encode]).toarray())\n            X_val__enc = pd.DataFrame(enc.transform(X_val__[cat_features_to_encode]).toarray())\n            X_train__enc.columns = enc.get_feature_names(cat_features_to_encode)\n            X_val__enc.columns = enc.get_feature_names(cat_features_to_encode)\n            X_train__ = X_train__.join(X_train__enc)\n            X_val__ = X_val__.join(X_val__enc)\n            if(tuning==False):\n                X_test__enc = pd.DataFrame(enc.transform(X_test__[cat_features_to_encode]).toarray())\n                X_test__enc.columns = enc.get_feature_names(cat_features_to_encode)\n                X_test__ = X_test__.join(X_test__enc)\n        elif(encoding=='ord'):\n            #enc = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value=-1)\n            enc = OrdinalEncoder()\n            X_train__enc = pd.DataFrame(enc.fit_transform(X_train__[cat_features_to_encode]))\n            X_val__enc = pd.DataFrame(enc.transform(X_val__[cat_features_to_encode]))\n            X_train__enc.columns = [cat_features_to_encode]\n            X_val__enc.columns = [cat_features_to_encode]\n            X_train__ = X_train__.join(X_train__enc, rsuffix='_ord_enc')\n            X_val__ = X_val__.join(X_val__enc, rsuffix='_ord_enc')\n            if(tuning==False):\n                X_test__enc = pd.DataFrame(enc.transform(X_test__[cat_features_to_encode]))\n                X_test__enc.columns = [cat_features_to_encode]\n                X_test__ = X_test__.join(X_test__enc, rsuffix='_ord_enc')\n        elif(encoding=='tar_enc'):\n            for f in cat_features_to_encode:\n                feat = X_train__.groupby(f)['SalePrice'].mean()\n                feat = feat.to_dict()\n                X_train__.loc[:,f\"tar_enc_{f}\"] = X_train__[f].map(feat)\n                X_val__.loc[:,f\"tar_enc_{f}\"] = X_val__[f].map(feat)\n                if(tuning==False): X_test__.loc[:,f\"tar_enc_{f}\"] = X_test__[f].map(feat)\n\n        X_train__.drop(columns='SalePrice', inplace=True)\n        X_train__.drop(columns=cat_features_to_encode, inplace=True)\n        X_val__.drop(columns='SalePrice', inplace=True)\n        X_val__.drop(columns=cat_features_to_encode, inplace=True)\n        if(tuning==False): X_test__.drop(columns=cat_features_to_encode, inplace=True)\n                \n        # Save files for debugging purpose\n        X_train__.to_csv(f'X_train__{iteration}.csv', index = False)\n        X_val__.to_csv(f'X_val__{iteration}.csv', index = False)\n        if(tuning==False): X_test__.to_csv(f'X_test__{iteration}.csv', index = False)\n        \n        # Scale the features\n        if scaling:\n            scaler = StandardScaler()\n            X_train__ = pd.DataFrame(scaler.fit_transform(X_train__), columns=X_train__.columns)\n            X_val__ = pd.DataFrame(scaler.transform(X_val__), columns=X_val__.columns)\n            if(tuning==False): X_test__ = pd.DataFrame(scaler.transform(X_test__), columns=X_test__.columns)\n            # Save files for debugging purpose\n            X_train__.to_csv(f'X_train__{iteration}__scaled.csv', index = False)\n            X_val__.to_csv(f'X_val__{iteration}__scaled.csv', index = False)\n            if(tuning==False): X_test__.to_csv(f'X_test__{iteration}__scaled.csv', index = False)\n                \n        # Train and Predict\n        if early_stopping:\n            model.fit(X_train__, y_train_, eval_set=[(X_val__, y_val_)], early_stopping_rounds=100, verbose=False)\n        else:\n            model.fit(X_train__, y_train_) \n        \n        y_val_preds = model.predict(X_val__)\n\n        if(clip): y_val_preds = np.clip(y_val_preds,clipmin,clipmax)\n        \n        if(tuning==False):\n            if iteration==1: \n                oof_preds = pd.Series(data=y_val_preds,index=val_index)\n            else: \n                oof_preds = pd.concat([oof_preds, pd.Series(data=y_val_preds,index=val_index)])\n        \n        rmse_val = mean_squared_error(y_val_, y_val_preds, squared=False)\n        print(str(iteration) + '\/' + str(N_SPLITS) + ' KFold RMSLE: ' + str(rmse_val))\n            \n        cum_rmse_val = cum_rmse_val + rmse_val\n        \n        if(tuning==False):\n            new_preds = model.predict(X_test__)\n            if(clip): new_preds = np.clip(new_preds,clipmin,clipmax)\n        \n            if iteration==1: \n                preds = new_preds\n            else: \n                preds = preds + new_preds\n                \n        iteration = iteration + 1\n\n    if(tuning==False): preds = preds\/N_SPLITS\n    avg_rmse = cum_rmse_val\/N_SPLITS\n    print('Average RMSLE: ' + str(avg_rmse))\n    \n    if tuning:\n        return avg_rmse, None, None\n    else:\n        return avg_rmse, np.exp(oof_preds.sort_index()), np.exp(preds)","7cad1c23":"_, _, X_train, y_train, X_test, all_features, num_features, num_continuous_features, num_discrete_features, cat_features, cat_features_to_encode = load_data()\nX_train.head(2)","9d4f7835":"feature_types = pd.DataFrame(data=[num_features, cat_features])\nfeature_types.index=['Numerical','Categorical']\nfeature_types.style.set_table_styles([\n    {'selector': 'thead', 'props': [('display', 'none')]}\n])","f2c500b4":"fig=plt.figure(figsize=(20,4))\nsns.histplot(y_train, kde=True, palette='pastel').set_title('Target Distribution', weight='bold')\nplt.show()","ec52a5f0":"full = pd.concat([X_train, X_test]).reset_index()\nfull.drop(columns='index')\nfull['Source']='train'\nfull.loc[full[full['SalePrice'].isna()].index,'Source']='test'\n\nfig=plt.figure(figsize=(20, 7))\n#fig.suptitle(\"Histograms of numeric continuous features\")\nrows = 2\ncols = 5\ni = 1\nfor f in num_continuous_features[0:10]:\n    fig.add_subplot(rows, cols, i)\n    sns.histplot(data=full, x=f, hue='Source', kde=True, palette='pastel')\n    i = i+1\nplt.show()","4728aac8":"f_with_na_train = X_train.isna().sum(axis=0)\nf_with_na_train = f_with_na_train[f_with_na_train>0]\nf_with_na_train.name='Nb of NaNs in train'\nf_with_na_test = X_test.isna().sum(axis=0)\nf_with_na_test = f_with_na_test[f_with_na_test>0]\nf_with_na_test.name='Nb of NaNs in test'\nf_with_na = pd.concat([f_with_na_train, f_with_na_test], axis=1)\nf_with_na.fillna(0, inplace=True)\nf_with_na = f_with_na[['Nb of NaNs in train','Nb of NaNs in test']]\nf_with_na.sort_values(['Nb of NaNs in train', 'Nb of NaNs in test'], ascending=False)[['Nb of NaNs in train', 'Nb of NaNs in test']].plot(kind='bar', figsize=(20,4))\nplt.show()","3b13daf4":"X = X_train.copy()\ny = X.pop(\"SalePrice\")\n# We are going to factorize the categorical features\ncat_features_to_encode = cat_features.copy()\ncat_features_to_encode.remove('OverallQual')\ncat_features_to_encode.remove('OverallCond')\nfor colname in cat_features_to_encode:\n    X[colname], _ = X[colname].factorize() #NaNs will be replaced by -1\n# Need to remove the NaNs in the numerical features\nX.fillna(0, inplace=True)\n# Calculate and plot MI scores\nmi_scores = mutual_info_regression(X, y, discrete_features='auto', random_state=0)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\nmi_scores = mi_scores.sort_values(ascending=False)\nfig=plt.figure(figsize=(15, 5))\nmi_scores.plot(kind='bar', figsize=(20,4))\nplt.show()","e0551847":"_, _, X_train, y_train, X_test, all_features, num_features, _, _, cat_features, cat_features_to_encode = load_data()\nuseful_features = [e for e in all_features if e not in ('PoolArea','3SsnPorch','MoSold','YrSold','RoofMatl','Utilities','MiscFeatureGar2','PoolQC')]\nnew_features = ['NbNAs','LivLotRatio','Spaciousness','MedNhbdArea','GrLivAreaInNbhd','MedNhbdArea_Ext','LotAreaInNbhd','OverallQualCondProduct','LowQualFinRatio']\nencoding = 'tar_enc'\nscaling = True #useless for tree based algo\nclip = False\nearly_stopping = True","4bdb0dea":"def objective(trial):\n    \n    param_grid = {\n        'max_depth': trial.suggest_int('max_depth', 3, 15), # default = 6 range = [0,\u221e]\n        'n_estimators': trial.suggest_int('n_estimators', 1000, 10000), # default = 100\n        'eta': trial.suggest_loguniform('eta', 0.001, 0.3), # default = 0.3 range = [0,1]\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.1, 1.0, 0.1), # default = 1 range = (0, 1]\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 1.0, 0.1), # default = 1 range = (0, 1]\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 1.0, 0.1), # default = 1 range = (0, 1]\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 0.1, 10), # default = 1 range: [0,\u221e]\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.1, 100), # default = 1\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.0001, 10), # default=0\n        'gamma': trial.suggest_loguniform('gamma', 0.001, 10), # default = 0 range: [0,\u221e]\n    }  \n    \n    model = XGBRegressor(\n        tree_method='gpu_hist',\n        predictor='gpu_predictor',\n        n_jobs=4,\n        **param_grid\n    )\n    \n    avg_rmse, _, _ = cv_loop(\n        X_train=X_train, \n        y_train=y_train,\n        X_test=X_test,\n        model=model, \n        useful_features=all_features,\n        num_features=num_features,\n        cat_features=cat_features,\n        cat_features_to_encode=cat_features_to_encode, \n        encoding = encoding, \n        new_features=new_features,\n        scaling = scaling,\n        clip = clip, clipmin=np.log(34900), clipmax=np.log(755000),\n        tuning=True,\n        early_stopping = early_stopping\n    )\n\n    return avg_rmse","2d72fd89":"if TUNING_XGB:\n    study = optuna.create_study(direction='minimize', study_name=STUDY_NAME)\n    study.optimize(objective, timeout=TRAIN_TIME)\n    \n    print('Number of finished trials: ', len(study.trials))\n    print('Best trial:')\n    trial = study.best_trial\n\n    print('\\tValue: {}'.format(trial.value))\n    print('\\tParams: ')\n    for key, value in trial.params.items():\n        print('\\t\\t{}: {}'.format(key, value))","7bc60362":"if TUNING_XGB :\n    fig = optuna.visualization.plot_contour(study, params=['max_depth','n_estimators'])\n    fig.show()\n    fig = optuna.visualization.plot_contour(study, params=['eta','subsample'])\n    fig.show()\n    fig = optuna.visualization.plot_contour(study, params=['min_child_weight','gamma'])\n    fig.show()","a93ab544":"if TUNING_XGB:\n    model = XGBRegressor(\n        tree_method='gpu_hist',\n        predictor='gpu_predictor',\n        n_jobs=4,\n        **trial.params)\nelse: \n    params = {\n        'max_depth': 5,\n        'n_estimators': 7779,\n        'eta': 0.0044144556312306175,\n        'subsample': 0.30000000000000004,\n        'colsample_bytree': 0.2,\n        'colsample_bylevel': 0.4,\n        'min_child_weight': 0.21792841014662054,\n        'reg_lambda': 5.06808562586094,\n        'reg_alpha': 0.036826697275635915,\n        'gamma': 0.002452743312016066,    \n    }\n\n    model = XGBRegressor(\n        tree_method='gpu_hist',\n        predictor='gpu_predictor',\n        n_jobs=4,\n        **params)\n    \navg_rmse, oof_preds, preds = cv_loop(\n        X_train=X_train, \n        y_train=y_train,\n        X_test=X_test,\n        model=model, \n        useful_features=all_features,\n        num_features=num_features,\n        cat_features=cat_features,\n        cat_features_to_encode=cat_features_to_encode, \n        encoding = encoding, \n        new_features=new_features,\n        scaling = scaling,\n        clip = clip, clipmin=np.log(34900), clipmax=np.log(755000),\n        tuning=False,\n        early_stopping = early_stopping\n    )","711e3b2a":"# Save the preds file\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission['SalePrice'] = preds\nsubmission.to_csv('xgb_preds.csv', index = False)\n\n# Save the oof preds file\noof_preds.to_csv('xgb_oof_preds.csv', header=False)","885efa98":"_, _, X_train, y_train, X_test, all_features, num_features, _, _, cat_features, cat_features_to_encode = load_data()\nuseful_features = [e for e in all_features if e not in ('PoolArea','3SsnPorch','MoSold','YrSold','RoofMatl','Utilities','MiscFeatureGar2','PoolQC')]\nnew_features = ['NbNAs','LivLotRatio','Spaciousness','MedNhbdArea','GrLivAreaInNbhd','MedNhbdArea_Ext','LotAreaInNbhd','OverallQualCondProduct','LowQualFinRatio']\nencoding = 'ohe'\nscaling = True\nclip = True # with LR models, better results when clipping\nearly_stopping = False","47aff23a":"def objective(trial):\n    \n    param_grid = {\n        'alpha': trial.suggest_loguniform('alpha', 0.0001, 10000), # default = 1\n        #'max_iter': trial.suggest_discrete_uniform('max_iter', 10000, 50000), # default = 1000\n        'max_iter': trial.suggest_loguniform('max_iter', 1000, 900000), # default = 1000\n        'random_state' : 42\n    } \n    \n    model = Lasso(\n        **param_grid\n    )\n    \n    avg_rmse, _, _ = cv_loop(\n        X_train = X_train, \n        y_train = y_train,\n        X_test = X_test,\n        model = model, \n        useful_features = useful_features,\n        num_features = num_features,\n        cat_features = cat_features,\n        cat_features_to_encode = cat_features_to_encode, \n        encoding = encoding, \n        new_features = new_features,\n        scaling = scaling,\n        clip = clip, clipmin=np.log(34900), clipmax=np.log(755000), #necessary to avoid exception?\n        tuning = True,\n        early_stopping = early_stopping\n    )\n\n    return avg_rmse","e0b633a1":"if TUNING_LASSO:\n    study = optuna.create_study(direction='minimize', study_name=STUDY_NAME)\n    #study.optimize(objective, timeout=TRAIN_TIME)\n    study.optimize(objective, n_trials=50)\n    \n    print('Number of finished trials: ', len(study.trials))\n    print('Best trial:')\n    trial = study.best_trial\n\n    print('\\tValue: {}'.format(trial.value))\n    print('\\tParams: ')\n    for key, value in trial.params.items():\n        print('\\t\\t{}: {}'.format(key, value))","4f31d3cd":"if TUNING_LASSO:\n    fig = optuna.visualization.plot_contour(study, params=['max_iter','alpha'])\n    fig.show()","88858991":"if TUNING_LASSO:\n    model = Lasso(**trial.params, random_state=42)\nelse:\n    params = { \n        'alpha': 0.0018185000964940012,\n        'max_iter': 21098,\n        'random_state' : 42\n    }\n\n    model = Lasso(**params)\n    \navg_rmse, oof_preds, preds = cv_loop(\n        X_train = X_train, \n        y_train = y_train,\n        X_test = X_test,\n        model = model, \n        useful_features = useful_features,\n        num_features = num_features,\n        cat_features = cat_features,\n        cat_features_to_encode = cat_features_to_encode, \n        encoding = encoding, \n        new_features = new_features,\n        scaling = scaling,\n        clip = clip, clipmin = np.log(34900), clipmax = np.log(755000),\n        tuning = False,\n        early_stopping = early_stopping\n    )","54c20506":"# Save the preds file\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission['SalePrice'] = preds\nsubmission.to_csv('lasso_preds.csv', index = False)\n\n# Save the oof preds file\noof_preds.to_csv('lasso_oof_preds.csv', header=False)","44f62ea3":"xgb_oof_preds = np.log(pd.read_csv('xgb_oof_preds.csv',header=None).iloc[:,1])\nlasso_oof_preds = np.log(pd.read_csv('lasso_oof_preds.csv',header=None).iloc[:,1])\noof_preds = [xgb_oof_preds, lasso_oof_preds]\nplot_titles = ['xgb_oof_preds', 'lasso_oof_preds']\n\nfig, ax = plt.subplots(1, 2, figsize=(20,5))\ni = 0\nfor oof_pred in oof_preds:\n    ax[i].scatter(x=oof_pred, y=np.log(y_train))\n    \n    lims = [\n        np.min([ax[i].get_xlim(), ax[i].get_ylim()]),  # min of both axes\n        np.max([ax[i].get_xlim(), ax[i].get_ylim()]),  # max of both axes\n    ]\n\n    # now plot both limits against eachother\n    ax[i].plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n    ax[i].set_aspect('equal')\n    ax[i].set_xlim(lims)\n    ax[i].set_ylim(lims)\n    ax[i].set_title(plot_titles[i])\n    ax[i].set_xlabel('Log(Sale Price) - Prediction')\n    ax[i].set_ylabel('Log(Sale Price) - Ground Truth')\n    i = i+1\nplt.show()","0546da4a":"fig=plt.figure(figsize=(20,8))\nsns.scatterplot(x=lasso_oof_preds.index, y=np.log(y_train), color='green')\nsns.scatterplot(x=lasso_oof_preds.index, y=lasso_oof_preds, color='red')\nsns.scatterplot(x=lasso_oof_preds.index, y=xgb_oof_preds, color='blue')\nplt.show()","dd8dab9d":"oof_preds = pd.concat([xgb_oof_preds, lasso_oof_preds, np.log(y_train)], axis=1)\noof_preds.columns = ['xgb_oof_preds','lasso_oof_preds','y_train']\nsns.heatmap(oof_preds.corr(), annot=True)\nplt.show()","ba178523":"X_train, y_train = oof_preds[['xgb_oof_preds','lasso_oof_preds']], oof_preds['y_train']\nxgb_preds = pd.read_csv('xgb_preds.csv').iloc[:,1]\nlasso_preds = pd.read_csv('lasso_preds.csv').iloc[:,1]\nX_test = pd.concat([np.log(xgb_preds), np.log(lasso_preds)], axis=1)","4a37c35b":"metamodel = LinearRegression()\ncum_rmse_val = 0\niteration = 1\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nfor train_index, val_index in kf.split(X_train, y_train):\n    X_train_, X_val_ = X_train.iloc[train_index], X_train.iloc[val_index]\n    y_train_, y_val_ = y_train[train_index], y_train[val_index]\n    metamodel.fit(X_train_, y_train_)\n    y_val_preds = metamodel.predict(X_val_)\n    rmse_val = mean_squared_error(y_val_, y_val_preds, squared=False)\n    print(rmse_val)\n    cum_rmse_val = cum_rmse_val + rmse_val\n    \n    new_preds=metamodel.predict(X_test)\n    if(iteration==1):\n        preds = new_preds\n    else:  \n        preds = preds + new_preds\n    iteration = iteration+1\n\nprint(cum_rmse_val\/5)\npreds = preds \/ 5    ","e471af77":"# Save the preds file\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission['SalePrice'] = np.exp(preds)\nsubmission.to_csv('stacking_preds.csv', index = False)","e77ec80f":"The plot below displays the Ground Truth in green, the Lasso predictions in red and the XGB predictions in blue. ","913876a9":"# Feature Distribution","0a3293c2":"# XGBRegressor - Hyperparameter tuning with Optuna","4897a63a":"# Target Distribution","66cf5038":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>Stacking<\/p>\n<\/div>","e78b036b":"The training set has 1460 entries and the test set has 1459. The target is the \"SalePrice\". Let's display the 2 first rows:","034b04e8":"There are different type of features: categorical (ordered and unordered) and numerical (continuous and discrete and time).","f9c61ae2":"# Missing values","ca2a410c":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>Preliminaries<\/p>\n<\/div>","0535bddd":"Optuna is an open source and SOTA hyperparameter optimization framework to automate hyperparameter search. I found it easy to use and it has plenty of plots to analyze the outputs of the study i.e. optimization task. I did not test its capability to prune unpromising trials for faster results though.","13951fce":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>Displaying predicitions<\/p>\n<\/div>","98341956":"# Feature and Target correlation","aa9d5b9f":"* The distributions of the features are similar between train and test sets. So if we find a model that performs well on the CV, we can expect this model to perform equally well on the test set.\n* Most distributions are skewed and the value ranges differ. As long as we use tree-based models, skewness and variance are not an issue. But we'll have to consider scaling and normalizing the numeric features if we come to use models that perform well when the features look more or less like standard normally distributed data.","ecabb08f":"* The categorical data needs encoding with the exception of 'OverallQual', 'OverallCond'. Indeed, these 2 variables are already numeric values and their enconding make sense : 10 for 'Very Excellent', 9 for 'Excellent', 8 for 'Very Good'...  \n* We can use OneHotEncoding and TargetEncoding for the the categorical variables that need encoding. We may also use the model's built-in capabilities to encode categorical variables when we use models like XGB or LGBM.","231a7014":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>Abstract<\/p>\n<\/div>","395c7242":"# Lasso - Hyperparameter tuning with Optuna","726503fd":"In this section, I'm using the OOF predictions. The plots below show the OOF predictions vs the Ground Truth for the 2 models (XGB and Lasso) :","26240f02":"Let's not display all the distributions below but just a few to illustrate the conclusion:","8f326645":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>Lasso<\/p>\n<\/div>","34aef530":"* There are a lot of features with missing values, either in the train or the test set but most of the time, in both.\n* For this dataset, imputing 0 for missing numeric values and None for categorical value is an appropriate choice.\n* There are a few, namely 'MiscFeatureOthr', 'MiscFeatureGar2', 'PoolQC', 'MiscFeatureShed', 'Alley', 'Fence' with a huge ratio of missing values. It may be a good choice to ignore these features in the model design as they may lead to overfitting.","9498818f":"# Lasso - Generating predictions with the best model","d9771d94":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>XGBRegressor<\/p>\n<\/div>","c015e720":"# Define Settings","1aa1bf54":"We can use the mutual information (MI) between each feature and the target to quantify the \"amount of information\" obtained about the target by observing the features (univariate analysis). This method is better than corr() a.k.a. Pearson's correlation coefficient as it considers linear and non-linear correlations. Using this approach makes more sense to add the categorical unordered features.\n\nLike the Pearson's Correlation Coefficient, having a MI close to 0 does not mean that the variable is uninformative. Maybe the interaction of this variable with another one is informative.","5a2c81de":"In the heatmap below, we can see that the XGB predictions and the Lasso predictions have a high correlation (98%). This is not the ideal condition for stacking to be efficient (stacking is appropriate when multiple different machine learning models have skill on a dataset, but have skill in different ways. Another way to say this is that the predictions made by the models or the errors in predictions made by the models are uncorrelated or have a low correlation) but we will test it in the next section and check if any improvement comes out of it.","06df57ab":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>EDA<\/p>\n<\/div>","b49efe3e":"# Import Libraries","cf709b37":"# Blending the 2 models (XGB and Lasso) with a simple Linear Regression","b5f43d17":"* There are many features that have a pretty high correlation with the target. One should be able to set up a model that performs \"well\" in predicting the target on this dataset.\n* We may drop the features with a very low MI as it is likely that they are uninformative (but this analysis is not sufficient to say this)\n* It is also interesting to realize that OverallQual, Neighboorhood, GrLivArea and YearBuilt are the key drivers of the estate price. ","c0fd4619":"# Define functions","2185922e":"# XGBRegressor - Generating predictions with the best model","32e9ec8a":"The goal of this competition is to predict the sales price for houses, having 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. The training set has 1460 entries and the test set has 1459 entries. \n\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.\n\nIn this notebook, I'm sharing my solution (CV : 0.1179  and LB : 0.1187) and the strategies that helped improved the score. When I started working on this dataset, I created a baseline model and it scored 0.1659. It required quite a lot of work to move from this initial score to 0.1187 : **creating new features, selecting the right model and hypertuning parameter really helped improved the score.**\n\n**Feature engineering:**\n* I tested One-Hot-Encoding and Target Encoding for categorical variables. The XGB model performed better with Target Encoding.\n* I added new features using mathematical transforms (e.g. LowQualFinRatio=LowQualFinSF\/GrLivArea), counts (e.g. Number of NaNs per house), group transforms (e.g. median of the LotArea per neighboorhood). Adding these features really helped improving the score.\n* Scaling the features was also necessary for the Linear models. I used the StandardScaler and did not test any other scaler.\n\n**Selecting the right model:**\n* I started using XGB and Light GBM. The XGB model performed better than the LGBM one.\n* I tried Linear models. It looks like on this dataset, Gradient Boosting Machines are more efficient than Linear models but Lasso (and Ridge) performed OK after clipping the predicted sales prices.\n\n**Hyperparameter tuning:**\n* I used Optuna to tune the hyperparameters. This was also a very useful step to improve the score.\n* Using CV with 5 fold helped as the scores may differ quite significantly from one validation set to another.\n* For GBM models, it was useful to let the tuning run a few hours.\n\n**Stacking:**\n* I blended my best XGB model with my best Linear model (Lasso). I used a basic Linear Regression to combine the predictions from the 2 models and generate the final prediction. Stacking did not really help, it decreased slightly the CV score and improved slightly the LB score. I kept it in this notebook but I wouldn't say it was useful.\n\n**cv_loop():**\n* I created a function cv_loop() and it was helpful to test different scenarios quickly.\n* cv_loop() implements an end-to-end ML pipeline: \n    * feature selection : it keeps only the features passed to the function (input parameter useful_features) \n    * missing value imputation : it imputes the missing values with None for categorical features and 0 for numerical features\n    * feature creation : it creates the new features (input parameter new_features)\n    * encoding of categorical features : it encodes the categorical features, either One-Hot-Encoding or Target Encoding (input parameter encoding)\n    * feature scaling : it scales the features\n    * model training and evaluation : it uses a 5-fold Cross-Validation scheme and stores the OOF predictions \n    * model prediction : generates the predictions for the test data \n* cv_loop() can be used during the hyperparameter tuning (input parameter tuning=True) or to train one model and predict the target (sale price) for the estates of the Test Set (input parameter tuning=False).\n* cv_loop() takes care of avoiding data leakage: data preparation (encoding, scaling, ...) happens within each fold of your cross validation cycle.\n\n**I would be happy to get your feedback and ideas for improvement.**\n"}}