{"cell_type":{"f2e59556":"code","a966ac79":"code","2c9e3b59":"code","3b138742":"code","ae29ee52":"code","95b82b0d":"code","c623410a":"code","8080414d":"code","2c39ecc4":"code","6d234025":"code","c500d8aa":"code","ecbe35df":"code","2d3b517f":"code","9347bac1":"code","1b437cae":"code","ffe2f42a":"code","cbbb47ad":"code","15f5800a":"code","ca5122d2":"code","f951e53e":"code","2e01ecf6":"code","288bce08":"code","7a9e3201":"code","ecbe72e5":"code","a34b5324":"code","63aad470":"code","326b7dbb":"code","ae3b92e4":"code","c4bd95e6":"code","0451a13c":"code","85d8b912":"code","c80b7ad6":"code","1de4d663":"code","972a2320":"code","a4ceb664":"code","4c3b6add":"code","f9592c79":"code","4241ab51":"code","f27f2e04":"code","60b12fc3":"code","84ad10cf":"code","6acde18e":"code","7bc0746a":"code","48cf1aaf":"code","6f269c87":"code","a04b0b74":"code","e325fe2d":"code","0d680773":"code","b138e8b6":"code","f3c9d66b":"code","653babd3":"code","ad2a73ae":"code","dfff3afd":"code","45c12993":"code","18503614":"code","fcb0d974":"code","9cd9248c":"code","73b8d3f1":"code","41990d0e":"code","f72efc20":"code","c0247060":"code","de7b1176":"code","990f606d":"code","4349c395":"code","754aab02":"code","09e4c316":"code","6416b51b":"code","df80d82c":"code","fca1eb8d":"code","cbf1dde2":"markdown","33ea3226":"markdown","0f017fa5":"markdown","c594dbea":"markdown","ea4c9c97":"markdown","a8211390":"markdown","220f39b2":"markdown","034271d6":"markdown","4a658f82":"markdown","5ae17974":"markdown","d06a62da":"markdown","290f744b":"markdown","ee2334ef":"markdown","1ae333e4":"markdown","5637fabc":"markdown","857d60c8":"markdown","f1ad1e16":"markdown","e7416011":"markdown","26b85caa":"markdown","710e0db5":"markdown","2c1577d6":"markdown","d809300e":"markdown","100bd186":"markdown","cdac57de":"markdown","085c4749":"markdown","f2a2e92f":"markdown","d9f1e10b":"markdown"},"source":{"f2e59556":"import numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport math\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# preprocessing\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, learning_curve, ShuffleSplit\nfrom sklearn.model_selection import cross_val_predict as cvp\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix, explained_variance_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest, RFE, chi2\n\n# models\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier, LassoCV\nfrom sklearn.svm import SVC, LinearSVC, SVR\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n# NN models\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a966ac79":"!pip3 install xlrd\n# Autoviz for automatic EDA\n!pip install autoviz\nfrom autoviz.AutoViz_Class import AutoViz_Class","2c9e3b59":"cv_n_split = 3\nrandom_state = 40\ntest_train_split_part = 0.2","3b138742":"metrics_all = {1 : 'r2_score', 2: 'acc', 3 : 'rmse', 4 : 're'}\nmetrics_now = [1, 2, 3, 4] # you can only select some numbers of metrics from metrics_all","ae29ee52":"data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","95b82b0d":"data.head(3)","c623410a":"data.describe([.05, .95])","8080414d":"# data = data[(data['chol'] <= 326.9) & (data['oldpeak'] <=3.4)].reset_index(drop=True)\n# data","2c39ecc4":"data.describe()","6d234025":"data.info()","c500d8aa":"data = data.drop_duplicates()\ndata.shape","ecbe35df":"data.describe()","2d3b517f":"data","9347bac1":"def fe_creation(df):\n    df['age2'] = df['age']\/\/10\n    df['trestbps2'] = df['trestbps']\/\/10 #10\n    df['chol2'] = df['chol']\/\/40\n    df['thalach2'] = df['thalach']\/\/40\n    df['oldpeak2'] = df['oldpeak']\/\/0.4\n    for i in ['sex', 'age2', 'fbs', 'restecg', 'exang','thal', ]:\n        for j in ['cp','trestbps2', 'chol2', 'thalach2', 'oldpeak2', 'slope', 'ca']:\n            df[i + \"_\" + j] = df[i].astype('str') + \"_\" + df[j].astype('str')\n    return df\n\ndata = fe_creation(data)","1b437cae":"pd.set_option('max_columns', len(data.columns)+1)\nlen(data.columns)","ffe2f42a":"# Determination categorical features\ncategorical_columns = []\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nfeatures = data.columns.values.tolist()\nfor col in features:\n    if data[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","cbbb47ad":"# Encoding categorical features\nfor col in categorical_columns:\n    if col in data.columns:\n        le = LabelEncoder()\n        le.fit(list(data[col].astype(str).values))\n        data[col] = le.transform(list(data[col].astype(str).values))","15f5800a":"data.head(3)","ca5122d2":"data.shape","f951e53e":"train = data.copy()\ntarget = train.pop('target')\ntrain.head(2)","2e01ecf6":"num_features_opt = 25   # the number of features that we need to choose as a result\nnum_features_max = 35   # the somewhat excessive number of features, which we will choose at each stage\nfeatures_best = []","288bce08":"# Threshold for removing correlated variables\nthreshold = 0.9\n\ndef highlight(value):\n    if value > threshold:\n        style = 'background-color: pink'\n    else:\n        style = 'background-color: palegreen'\n    return style\n\n# Absolute value correlation matrix\ncorr_matrix = data.corr().abs().round(2)\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.style.format(\"{:.2f}\").applymap(highlight)","7a9e3201":"# Select columns with correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\nfeatures_filtered = data.drop(columns = collinear_features)\nprint('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])\nfeatures_best.append(features_filtered.columns.tolist())","ecbe72e5":"lsvc = LinearSVC(C=0.1, penalty=\"l1\", dual=False).fit(train, target)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(train)\nX_selected_df = pd.DataFrame(X_new, columns=[train.columns[i] for i in range(len(train.columns)) if model.get_support()[i]])\nfeatures_best.append(X_selected_df.columns.tolist())","a34b5324":"lasso = LassoCV(cv=3).fit(train, target)\nmodel = SelectFromModel(lasso, prefit=True)\nX_new = model.transform(train)\nX_selected_df = pd.DataFrame(X_new, columns=[train.columns[i] for i in range(len(train.columns)) if model.get_support()[i]])\nfeatures_best.append(X_selected_df.columns.tolist())","63aad470":"# Visualization from https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n# but to k='all'\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(train, target)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(train.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nfeatures_best.append(featureScores.nlargest(num_features_max,'Score')['Feature'].tolist())\nprint(featureScores.nlargest(len(dfcolumns),'Score')) ","326b7dbb":"embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=200), threshold='1.25*median')\nembeded_rf_selector.fit(train, target)","ae3b92e4":"embeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = train.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","c4bd95e6":"features_best.append(embeded_rf_feature)","0451a13c":"# Check whether all features have a sufficiently different meaning\nselector = VarianceThreshold(threshold=10)\nnp.shape(selector.fit_transform(data))\nfeatures_best.append(list(np.array(data.columns)[selector.get_support(indices=False)]))","85d8b912":"features_best","c80b7ad6":"# The element is in at least one list of optimal features\nmain_cols_max = features_best[0]\nfor i in range(len(features_best)-1):\n    main_cols_max = list(set(main_cols_max) | set(features_best[i+1]))\nmain_cols_max","1de4d663":"len(main_cols_max)","972a2320":"# The element is in all lists of optimal features\nmain_cols_min = features_best[0]\nfor i in range(len(features_best)-1):\n    main_cols_min = list(set(main_cols_min).intersection(set(features_best[i+1])))\nmain_cols_min","a4ceb664":"# Most common items in all lists of optimal features\nmain_cols = []\nmain_cols_opt = {feature_name : 0 for feature_name in data.columns.tolist()}\nfor i in range(len(features_best)):\n    for feature_name in features_best[i]:\n        main_cols_opt[feature_name] += 1\ndf_main_cols_opt = pd.DataFrame.from_dict(main_cols_opt, orient='index', columns=['Num'])\ndf_main_cols_opt.sort_values(by=['Num'], ascending=False).head(num_features_opt)","4c3b6add":"main_cols = df_main_cols_opt.nlargest(num_features_opt, 'Num').index.tolist()\nif not 'target' in main_cols:\n    main_cols.append('target')\nmain_cols","f9592c79":"pd.set_option('max_columns', len(main_cols)+1)\nlen(main_cols)","4241ab51":"data.to_csv('data_EDA.csv', index=False)","f27f2e04":"# AV = AutoViz_Class()\n# data = pd.read_csv('.\/data_EDA.csv')\n# df = AV.AutoViz(filename=\"\",sep=',', depVar='target', dfte=data, header=0, verbose=2, lowess=False, \n#                 chart_format='svg',  max_cols_analyzed=30)","60b12fc3":"data[main_cols].describe()","84ad10cf":"# Target\ntarget_name = 'target'\ntarget0 = data[target_name]\ntrain0 = data[main_cols].drop([target_name], axis=1)","6acde18e":"# For boosting model\ntrain0b = train0.copy()\n\n# Synthesis valid as \"test\" for selection models\ntrainb, testb, targetb, target_testb = train_test_split(train0b, target0, test_size=test_train_split_part, random_state=random_state)","7bc0746a":"# For models from Sklearn\nscaler = MinMaxScaler()\ntrain0 = pd.DataFrame(scaler.fit_transform(train0), columns = train0.columns)\n#scaler2 = StandardScaler()\nscaler2 = RobustScaler()\ntrain0 = pd.DataFrame(scaler2.fit_transform(train0), columns = train0.columns)","48cf1aaf":"# Synthesis valid as test for selection models\ntrain, test, target, target_test = train_test_split(train0, target0, test_size=test_train_split_part, random_state=random_state)","6f269c87":"train.head(3)","a04b0b74":"test.head(3)","e325fe2d":"train.info()","0d680773":"test.info()","b138e8b6":"# list of accuracy of all model - amount of metrics_now * 2 (train & test datasets)\nnum_models = 20\nacc_train = []\nacc_test = []\nacc_all = np.empty((len(metrics_now)*2, 0)).tolist()\nacc_all","f3c9d66b":"acc_all_pred = np.empty((len(metrics_now), 0)).tolist()\nacc_all_pred","653babd3":"# Splitting train data for model tuning with cross-validation\ncv_train = ShuffleSplit(n_splits=cv_n_split, test_size=test_train_split_part, random_state=random_state)","ad2a73ae":"def acc_d(y_meas, y_pred):\n    # Relative error between predicted y_pred and measured y_meas values\n    return mean_absolute_error(y_meas, y_pred)*len(y_meas)\/sum(abs(y_meas))\n\ndef acc_rmse(y_meas, y_pred):\n    # RMSE between predicted y_pred and measured y_meas values\n    return (mean_squared_error(y_meas, y_pred))**0.5","dfff3afd":"def plot_cm(target, train_pred, target_test, test_pred):\n    # Building the confusion matrices\n    \n    def cm_calc(y_true, y_pred):\n        cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n        cm_sum = np.sum(cm, axis=1, keepdims=True)\n        cm_perc = cm \/ cm_sum.astype(float) * 100\n        annot = np.empty_like(cm).astype(str)\n        nrows, ncols = cm.shape\n        for i in range(nrows):\n            for j in range(ncols):\n                c = cm[i, j]\n                p = cm_perc[i, j]\n                if i == j:\n                    s = cm_sum[i]\n                    annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n                elif c == 0:\n                    annot[i, j] = ''\n                else:\n                    annot[i, j] = '%.1f%%\\n%d' % (p, c)\n        cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n        cm.index.name = 'Actual'\n        cm.columns.name = 'Predicted'\n        return cm, annot\n\n    \n    # Building the confusion matrices\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6), sharex=True)\n    \n    # Training data\n    ax = axes[0]\n    ax.set_title(\"for training data\")\n    cm0, annot0 = cm_calc(target, train_pred)    \n    sns.heatmap(cm0, cmap= \"YlGnBu\", annot=annot0, fmt='', ax=ax)\n    \n    # Test data\n    ax = axes[1]\n    ax.set_title(\"for test (validation) data\")\n    cm1, annot1 = cm_calc(target_test, test_pred)\n    sns.heatmap(cm1, cmap= \"YlGnBu\", annot=annot1, fmt='', ax=ax)\n    \n    fig.suptitle('CONFUSION MATRICES')\n    plt.show()","45c12993":"def acc_metrics_calc(num,model,train,test,target,target_test):\n    # The models selection stage\n    # Calculation of accuracy of model by different metrics\n    global acc_all\n\n    ytrain = model.predict(train).astype(int)\n    ytest = model.predict(test).astype(int)\n    if num != 17:\n        print('target = ', target[:5].values)\n        print('ytrain = ', ytrain[:5])\n        print('target_test =', target_test[:5].values)\n        print('ytest =', ytest[:5])\n\n    num_acc = 0\n    for x in metrics_now:\n        if x == 1:\n            #r2_score criterion\n            acc_train = round(r2_score(target, ytrain) * 100, 2)\n            acc_test = round(r2_score(target_test, ytest) * 100, 2)\n        elif x == 2:\n            #accuracy_score criterion\n            acc_train = round(metrics.accuracy_score(target, ytrain) * 100, 2)\n            acc_test = round(metrics.accuracy_score(target_test, ytest) * 100, 2)\n        elif x == 3:\n            #rmse criterion\n            acc_train = round(acc_rmse(target, ytrain) * 100, 2)\n            acc_test = round(acc_rmse(target_test, ytest) * 100, 2)\n        elif x == 4:\n            #relative error criterion\n            acc_train = round(acc_d(target, ytrain) * 100, 2)\n            acc_test = round(acc_d(target_test, ytest) * 100, 2)\n        \n        print('acc of', metrics_all[x], 'for train =', acc_train)\n        print('acc of', metrics_all[x], 'for test =', acc_test)\n        acc_all[num_acc].append(acc_train) #train\n        acc_all[num_acc+1].append(acc_test) #test\n        num_acc += 2\n    \n    #  Building the confusion matrices\n    plot_cm(target, ytrain, target_test, ytest)","18503614":"def acc_metrics_calc_pred(num,model,name_model,train,test,target):\n    # The prediction stage\n    # Calculation of accuracy of model for all different metrics and creates of the main submission file for the best model (num=0)\n    global acc_all_pred\n\n    ytrain = model.predict(train).astype(int)\n    ytest = model.predict(test).astype(int)\n\n    print('**********')\n    print(name_model)\n    if num != 17:\n        print('target = ', target[:15].values)\n        print('ytrain = ', ytrain[:15])\n        print('ytest =', ytest[:15])\n    \n    num_acc = 0\n    for x in metrics_now:\n        if x == 1:\n            #r2_score criterion\n            acc_train = round(r2_score(target, ytrain) * 100, 2)\n        elif x == 2:\n            #accuracy_score criterion\n            acc_train = round(metrics.accuracy_score(target, ytrain) * 100, 2)\n        elif x == 3:\n            #rmse criterion\n            acc_train = round(acc_rmse(target, ytrain) * 100, 2)\n        elif x == 4:\n            #relative error criterion\n            acc_train = round(acc_d(target, ytrain) * 100, 2)\n\n        print('acc of', metrics_all[x], 'for train =', acc_train)\n        acc_all_pred[num_acc].append(acc_train) #train\n        num_acc += 1\n    \n    # Save the submission file\n    submission[target_name] = ytest\n    submission.to_csv('submission_' + name_model + '.csv', index=False)    ","fcb0d974":"# # Thanks to https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\n# def plot_learning_curve(estimator, title, X, y, cv=None, axes=None, ylim=None, \n#                         n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), random_state=0):\n#     \"\"\"\n#     Generate 2 plots: \n#     - the test and training learning curve, \n#     - the training samples vs fit times curve.\n\n#     Parameters\n#     ----------\n#     estimator : object type that implements the \"fit\" and \"predict\" methods\n#         An object of that type which is cloned for each validation.\n\n#     title : string\n#         Title for the chart.\n\n#     X : array-like, shape (n_samples, n_features)\n#         Training vector, where n_samples is the number of samples and\n#         n_features is the number of features.\n\n#     y : array-like, shape (n_samples) or (n_samples, n_features), optional\n#         Target relative to X for classification or regression;\n#         None for unsupervised learning.\n\n#     axes : array of 3 axes, optional (default=None)\n#         Axes to use for plotting the curves.\n\n#     ylim : tuple, shape (ymin, ymax), optional\n#         Defines minimum and maximum yvalues plotted.\n\n#     cv : int, cross-validation generator or an iterable, optional\n#         Determines the cross-validation splitting strategy.\n#         Possible inputs for cv are:\n\n#           - None, to use the default 5-fold cross-validation,\n#           - integer, to specify the number of folds.\n#           - :term:`CV splitter`,\n#           - An iterable yielding (train, test) splits as arrays of indices.\n\n#         For integer\/None inputs, if ``y`` is binary or multiclass,\n#         :class:`StratifiedKFold` used. If the estimator is not a classifier\n#         or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n#         Refer :ref:`User Guide <cross_validation>` for the various\n#         cross-validators that can be used here.\n\n#     train_sizes : array-like, shape (n_ticks,), dtype float or int\n#         Relative or absolute numbers of training examples that will be used to\n#         generate the learning curve. If the dtype is float, it is regarded as a\n#         fraction of the maximum size of the training set (that is determined\n#         by the selected validation method), i.e. it has to be within (0, 1].\n#         Otherwise it is interpreted as absolute sizes of the training sets.\n#         Note that for classification the number of samples usually have to\n#         be big enough to contain at least one sample from each class.\n#         (default: np.linspace(0.1, 1.0, 5))\n    \n#     random_state : random_state\n    \n#     \"\"\"\n#     fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n    \n#     if axes is None:\n#         _, axes = plt.subplots(1, 2, figsize=(20, 5))\n\n#     axes[0].set_title(title)\n#     if ylim is not None:\n#         axes[0].set_ylim(*ylim)\n#     axes[0].set_xlabel(\"Training examples\")\n#     axes[0].set_ylabel(\"Score\")\n\n#     cv_train = ShuffleSplit(n_splits=cv_n_split, test_size=test_train_split_part, random_state=random_state)\n    \n#     train_sizes, train_scores, test_scores, fit_times, _ = \\\n#         learning_curve(estimator=estimator, X=X, y=y, cv=cv,\n#                        train_sizes=train_sizes,\n#                        return_times=True)\n#     train_scores_mean = np.mean(train_scores, axis=1)\n#     train_scores_std = np.std(train_scores, axis=1)\n#     test_scores_mean = np.mean(test_scores, axis=1)\n#     test_scores_std = np.std(test_scores, axis=1)\n#     fit_times_mean = np.mean(fit_times, axis=1)\n#     fit_times_std = np.std(fit_times, axis=1)\n\n#     # Plot learning curve\n#     axes[0].grid()\n#     axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n#                          train_scores_mean + train_scores_std, alpha=0.1,\n#                          color=\"r\")\n#     axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n#                          test_scores_mean + test_scores_std, alpha=0.1,\n#                          color=\"g\")\n#     axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n#                  label=\"Training score\")\n#     axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n#                  label=\"Cross-validation score\")\n#     axes[0].legend(loc=\"best\")\n\n#     # Plot n_samples vs fit_times\n#     axes[1].grid()\n#     axes[1].plot(train_sizes, fit_times_mean, 'o-')\n#     axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n#                          fit_times_mean + fit_times_std, alpha=0.1)\n#     axes[1].set_xlabel(\"Training examples\")\n#     axes[1].set_ylabel(\"fit_times\")\n#     axes[1].set_title(\"Scalability of the model\")\n\n#     plt.show()\n#     return","9cd9248c":"%%time\n# MLPClassifier\n\nmlp = MLPClassifier()\nparam_grid = {'hidden_layer_sizes': [i for i in range(2,5)],\n              'solver': ['sgd'],\n              'learning_rate': ['adaptive'],\n              'max_iter': [1000]\n              }\nmlp_GS = GridSearchCV(mlp, param_grid=param_grid, cv=cv_train, verbose=False)\nmlp_GS.fit(train, target)\nprint(mlp_GS.best_params_)\nacc_metrics_calc(3,mlp_GS,train,test,target,target_test)","73b8d3f1":"# Building learning curve of model\nplot_learning_curve(mlp, \"MLP Classifier\", train, target, cv=cv_train)","41990d0e":"# Extra Trees Classifier\n\netr = ExtraTreesClassifier()\netr_CV = GridSearchCV(estimator=etr, param_grid={'min_samples_leaf' : [11, 12, 13, 14]}, cv=cv_train, verbose=False)\netr_CV.fit(train, target)\nprint(etr_CV.best_params_)\nacc_metrics_calc(12,etr_CV,train,test,target,target_test)","f72efc20":"# Building learning curve of model\nplot_learning_curve(etr, \"Extra Trees Classifier\", train, target, cv=cv_train)","c0247060":"# Thanks to https:\/\/www.kaggle.com\/skrudals\/modification-of-neural-network-around-90\ndef build_nn(optimizer='adam'):\n\n    # Initializing the NN\n    nn = Sequential()\n\n    # Adding the input layer and the first hidden layer of the NN\n    nn.add(Dense(units=32, kernel_initializer='he_normal', activation='relu', input_shape=(len(train0.columns),)))\n    # Adding the output layer\n    nn.add(Dense(units=1, kernel_initializer='he_normal', activation='sigmoid'))\n\n    # Compiling the NN\n    nn.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n    return nn\n\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=test_train_split_part, random_state=random_state)\nnn_model = build_nn(optimizers.Adam(lr=0.0001))\nnn_model.fit(Xtrain, Ztrain, batch_size=16, epochs=100, validation_data=(Xval, Zval))\nacc_metrics_calc(17,nn_model,train,test,target,target_test)","de7b1176":"# Voting Classifier\n\nVoting_ens = VotingClassifier(estimators=[('log', logreg_CV), ('mlp', mlp_GS ), ('svc', linear_svc_CV)])\nVoting_ens.fit(train, target)\nacc_metrics_calc(19,Voting_ens,train,test,target,target_test)","990f606d":"models = pd.DataFrame({\n    'Model': ['MLP Classifier', 'ExtraTrees Classifier', \n              'NN model', 'VotingClassifier']})","4349c395":"for x in metrics_now:\n    xs = metrics_all[x]\n    models[xs + '_train'] = acc_all[(x-1)*2]\n    models[xs + '_test'] = acc_all[(x-1)*2+1]\n    if xs == \"acc\":\n        models[xs + '_diff'] = models[xs + '_train'] - models[xs + '_test']\n#models","754aab02":"print('Prediction accuracy for models')\nms = metrics_all[metrics_now[1]] # the first from metrics\nmodels[['Model', ms + '_train', ms + '_test', 'acc_diff']].sort_values(by=[(ms + '_test'), (ms + '_train')], ascending=False)","09e4c316":"pd.options.display.float_format = '{:,.2f}'.format","6416b51b":"# Choose the number of metric by which the best models will be determined =>  {1 : 'r2_score', 2: 'accuracy_score', 3 : 'relative_error', 4 : 'rmse'}\nmetrics_main = 2 \nxs = metrics_all[metrics_main]\nxs_train = metrics_all[metrics_main] + '_train'\nxs_test = metrics_all[metrics_main] + '_test'\nprint('The best models by the',xs,'criterion:')\ndirect_sort = False if (metrics_main >= 2) else True\nmodels_sort = models.sort_values(by=[xs_test, xs_train], ascending=direct_sort)","df80d82c":"# Selection the best models except VotingClassifier\nmodels_best = models_sort[(models_sort.acc_diff < 10) & (models_sort.acc_test > 86)]\nmodels_best[['Model', ms + '_train', ms + '_test']].sort_values(by=['acc_test'], ascending=False)","fca1eb8d":"# Selection the best models from the best\nmodels_best_best = models_best[(models_best.acc_test > 90)]\nmodels_best_best[['Model', ms + '_train', ms + '_test']].sort_values(by=['acc_test'], ascending=False)","cbf1dde2":"## Models evaluation <a class=\"anchor\" id=\"6\"><\/a>","33ea3226":"### FS by the SelectKBest with Chi-2 <a class=\"anchor\" id=\"3.2.2.4\"><\/a>\n","0f017fa5":"### FS by the SelectFromModel with Lasso <a class=\"anchor\" id=\"3.2.2.3\"><\/a>\n","c594dbea":"## 3. EDA & FE<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","ea4c9c97":"###  Selection the best features<a class=\"anchor\" id=\"3.2.2.8\"><\/a>\n\n","a8211390":"2. Pandas Describe","220f39b2":"### The best models:","034271d6":"### 3.1. Initial EDA for FE<a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","4a658f82":"###  FS by the SelectFromModel with LinearSVC <a class=\"anchor\" id=\"3.2.2.2\"><\/a>\n","5ae17974":"### <a class=\"anchor\" id=\"5.20\"><\/a>","d06a62da":"##  Preparing to modeling <a class=\"anchor\" id=\"4\"><\/a>\n","290f744b":"##  Conclusion <a class=\"anchor\" id=\"7\"><\/a>\n","ee2334ef":"## 5. Tuning models and test for all features <a class=\"anchor\" id=\"5\"><\/a>\n\n","1ae333e4":"The analysis revealed the presence of one duplicate line. Let's remove it.","5637fabc":"### FS with the Pearson correlation<a class=\"anchor\" id=\"3.2.2.1\"><\/a>\n","857d60c8":"The analysis showed that the available features are poorly divided according to the target values. It is advisable to generate a number of new features.","f1ad1e16":"### FS by the VarianceThreshold<a class=\"anchor\" id=\"3.2.2.7\"><\/a>\n","e7416011":"### 5.4 MLP Classifier<a class=\"anchor\" id=\"5.4\"><\/a>","26b85caa":"### 5.18 Neural network (NN) with Keras <a class=\"anchor\" id=\"5.18\"><\/a>\n","710e0db5":"### 5.13 Extra Trees Classifier <a class=\"anchor\" id=\"5.13\"><\/a>\n","2c1577d6":"## Download datasets <a class=\"anchor\" id=\"2\"><\/a>","d809300e":"Feature Selection","100bd186":"###  AutoViz<a class=\"anchor\" id=\"3.3.1\"><\/a>\n","cdac57de":"###  FS by the Recursive Feature Elimination (RFE) with Random Forest<a class=\"anchor\" id=\"3.2.2.6\"><\/a>\n","085c4749":"### EDA for Model selection<a class=\"anchor\" id=\"3.3\"><\/a>\n","f2a2e92f":"The next code from in my kernel [FE & EDA with Pandas Profiling](https:\/\/www.kaggle.com\/vbmokin\/fe-eda-with-pandas-profiling)","d9f1e10b":"1. Pandas Profiling"}}