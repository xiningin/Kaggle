{"cell_type":{"d29153e9":"code","dabfb017":"code","f5e5b8e3":"code","c3f7d7cb":"code","2a54486a":"code","a3816a8f":"code","f76d37f8":"code","1ba203c2":"code","b7400ea9":"code","37452bae":"code","7e8285f8":"code","29b7c108":"code","165f9220":"code","4df2f398":"code","7ec739d7":"code","e62e7003":"code","05e4c0c5":"code","9a5380f2":"code","c467b017":"code","c21314e0":"code","72f6dee7":"markdown"},"source":{"d29153e9":"# %% [markdown]\n# This is a week-2 version of my week-1 [3rd place solution](https:\/\/www.kaggle.com\/osciiart\/covid19-lightgbm?scriptVersionId=30830623) (as of 2020-3-31 update).  \n# I fixed some bugs of the week-1 version shows below.\n# - Valid data includes train data so that early stopping did not work preferably.\n# - In some countries, prediction started from a few days before correct test periods.\n# \n# I started my job as a medical doctor from April in Japan so that I'm not sure I can commit this competition hereafter.  \n# I hope you find something good from this notebook.\n\n# %% [code]\nimport os, gc, pickle, copy, datetime, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom sklearn import metrics\npd.set_option('display.max_columns', 100)\nwarnings.filterwarnings('ignore')\n\n# %% [markdown]\n# # Data loading\n\n# %% [code]\ndf_train = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\nprint(df_train.shape)\ndf_train.head()\n\n# %% [code]\ndf_test = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\nprint(df_test.shape)\ndf_test.head()\n\n# %% [code]\n# concat train and test\ndf_traintest = pd.concat([df_train, df_test])\nprint(df_train.shape, df_test.shape, df_traintest.shape)\n\n# %% [markdown]\n# ### Preprocessing\n\n# %% [code]\n# process date\ndf_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\ndf_traintest['day'] = df_traintest['Date'].apply(lambda x: x.dayofyear).astype(np.int16)\ndf_traintest.head()\n\n# %% [code]\nday_before_valid = 104-7 # 3-11 day  before of validation\nday_before_public = 104 # 3-18 last day of train\nday_before_private = df_traintest['day'][pd.isna(df_traintest['ForecastId'])].max() # last day of train\nprint(df_traintest['Date'][df_traintest['day']==day_before_valid].values[0])\nprint(df_traintest['Date'][df_traintest['day']==day_before_public].values[0])\nprint(df_traintest['Date'][df_traintest['day']==day_before_private].values[0])\n\n# %% [code]\n# concat Country\/Region and Province\/State\ndef func(x):\n    try:\n        x_new = x['Country_Region'] + \"\/\" + x['Province_State']\n    except:\n        x_new = x['Country_Region']\n    return x_new\n        \ndf_traintest['place_id'] = df_traintest.apply(lambda x: func(x), axis=1)\ndf_traintest.head()\n\n# %% [code]\ndf_traintest[(df_traintest['day']>=day_before_public-3) & (df_traintest['place_id']=='China\/Hubei')].head()\n\n# %% [code]\n# concat lat and long\ndf_latlong = pd.read_csv(\"..\/input\/smokingstats\/df_Latlong.csv\")\ndf_latlong.head()\n\n# %% [code]\n# concat Country\/Region and Province\/State\ndef func(x):\n    try:\n        x_new = x['Country\/Region'] + \"\/\" + x['Province\/State']\n    except:\n        x_new = x['Country\/Region']\n    return x_new\n        \ndf_latlong['place_id'] = df_latlong.apply(lambda x: func(x), axis=1)\ndf_latlong = df_latlong[df_latlong['place_id'].duplicated()==False]\ndf_latlong.head()\n\n# %% [code]\ndf_traintest = pd.merge(df_traintest, df_latlong[['place_id', 'Lat', 'Long']], on='place_id', how='left')\ndf_traintest.head()\n\n# %% [code]\nprint(pd.isna(df_traintest['Lat']).sum()) # count Nan\ndf_traintest[pd.isna(df_traintest['Lat'])].head()\n\n# %% [code]\n# get place list\nplaces = np.sort(df_traintest['place_id'].unique())\nprint(len(places))\n\n# %% [code]\n# calc cases, fatalities per day\ndf_traintest2 = copy.deepcopy(df_traintest)\ndf_traintest2['cases\/day'] = 0\ndf_traintest2['fatal\/day'] = 0\ntmp_list = np.zeros(len(df_traintest2))\nfor place in places:\n    tmp = df_traintest2['ConfirmedCases'][df_traintest2['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest2['cases\/day'][df_traintest2['place_id']==place] = tmp\n    tmp = df_traintest2['Fatalities'][df_traintest2['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest2['fatal\/day'][df_traintest2['place_id']==place] = tmp\nprint(df_traintest2.shape)\ndf_traintest2[df_traintest2['place_id']=='China\/Hubei'].head()\n\n# %% [code]\n# aggregate cases and fatalities\ndef do_aggregation(df, col, mean_range):\n    df_new = copy.deepcopy(df)\n    col_new = '{}_({}-{})'.format(col, mean_range[0], mean_range[1])\n    df_new[col_new] = 0\n    tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean()\n    df_new[col_new][mean_range[0]:] = tmp[:-(mean_range[0])]\n    df_new[col_new][pd.isna(df_new[col_new])] = 0\n    return df_new[[col_new]].reset_index(drop=True)\n\ndef do_aggregations(df):\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [15,21]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [15,21]).reset_index(drop=True)], axis=1)\n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['ConfirmedCases']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}cases'.format(threshold)] = tmp\n            \n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['Fatalities']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}fatal'.format(threshold)] = tmp\n    \n    # process China\/Hubei\n    if df['place_id'][0]=='China\/Hubei':\n        df['days_since_1cases'] += 35 # 2019\/12\/8\n        df['days_since_10cases'] += 35-13 # 2019\/12\/8-2020\/1\/2 assume 2019\/12\/8+13\n        df['days_since_100cases'] += 4 # 2020\/1\/18\n        df['days_since_1fatal'] += 13 # 2020\/1\/9\n    return df\n\n# %% [code]\ndf_traintest3 = []\nfor place in places[:]:\n    df_tmp = df_traintest2[df_traintest2['place_id']==place].reset_index(drop=True)\n    df_tmp = do_aggregations(df_tmp)\n    df_traintest3.append(df_tmp)\ndf_traintest3 = pd.concat(df_traintest3).reset_index(drop=True)\ndf_traintest3[df_traintest3['place_id']=='China\/Hubei'].head()\n\n# %% [code]\n# add Smoking rate per country\n# data of smoking rate is obtained from https:\/\/ourworldindata.org\/smoking\ndf_smoking = pd.read_csv(\"..\/input\/smokingstats\/share-of-adults-who-smoke.csv\")\nprint(np.sort(df_smoking['Entity'].unique())[:10])\ndf_smoking.head()\n\n# %% [code]\n# extract newest data\ndf_smoking_recent = df_smoking.sort_values('Year', ascending=False).reset_index(drop=True)\ndf_smoking_recent = df_smoking_recent[df_smoking_recent['Entity'].duplicated()==False]\ndf_smoking_recent['Country_Region'] = df_smoking_recent['Entity']\ndf_smoking_recent['SmokingRate'] = df_smoking_recent['Smoking prevalence, total (ages 15+) (% of adults)']\ndf_smoking_recent.head()\n\n# %% [code]\n# merge\ndf_traintest4 = pd.merge(df_traintest3, df_smoking_recent[['Country_Region', 'SmokingRate']], on='Country_Region', how='left')\nprint(df_traintest4.shape)\ndf_traintest4.head()\n\n# %% [code]\n# fill na with world smoking rate\nSmokingRate = df_smoking_recent['SmokingRate'][df_smoking_recent['Entity']=='World'].values[0]\nprint(\"Smoking rate of the world: {:.6f}\".format(SmokingRate))\ndf_traintest4['SmokingRate'][pd.isna(df_traintest4['SmokingRate'])] = SmokingRate\ndf_traintest4.head()\n\n# %% [code]\n# add data from World Economic Outlook Database\n# https:\/\/www.imf.org\/external\/pubs\/ft\/weo\/2017\/01\/weodata\/index.aspx\ndf_weo = pd.read_csv(\"..\/input\/smokingstats\/WEO.csv\")\ndf_weo.head()\n\n# %% [code]\nprint(df_weo['Subject Descriptor'].unique())\n\n# %% [code]\nsubs  = df_weo['Subject Descriptor'].unique()[:-1]\ndf_weo_agg = df_weo[['Country']][df_weo['Country'].duplicated()==False].reset_index(drop=True)\nfor sub in subs[:]:\n    df_tmp = df_weo[['Country', '2019']][df_weo['Subject Descriptor']==sub].reset_index(drop=True)\n    df_tmp = df_tmp[df_tmp['Country'].duplicated()==False].reset_index(drop=True)\n    df_tmp.columns = ['Country', sub]\n    df_weo_agg = df_weo_agg.merge(df_tmp, on='Country', how='left')\ndf_weo_agg.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df_weo_agg.columns]\ndf_weo_agg.columns\ndf_weo_agg['Country_Region'] = df_weo_agg['Country']\ndf_weo_agg.head()\n\n# %% [code]\n# merge\ndf_traintest5 = pd.merge(df_traintest4, df_weo_agg, on='Country_Region', how='left')\nprint(df_traintest5.shape)\ndf_traintest5.head()\n\n# %% [code]\n# add Life expectancy\n# Life expectancy at birth obtained from http:\/\/hdr.undp.org\/en\/data\ndf_life = pd.read_csv(\"..\/input\/smokingstats\/Life expectancy at birth.csv\")\ntmp = df_life.iloc[:,1].values.tolist()\ndf_life = df_life[['Country', '2018']]\ndef func(x):\n    x_new = 0\n    try:\n        x_new = float(x.replace(\",\", \"\"))\n    except:\n#         print(x)\n        x_new = np.nan\n    return x_new\n    \ndf_life['2018'] = df_life['2018'].apply(lambda x: func(x))\ndf_life.head()\n\n# %% [code]\ndf_life = df_life[['Country', '2018']]\ndf_life.columns = ['Country_Region', 'LifeExpectancy']\n\n# %% [code]\n# merge\ndf_traintest6 = pd.merge(df_traintest5, df_life, on='Country_Region', how='left')\nprint(len(df_traintest6))\ndf_traintest6.head()\n\n# %% [code]\n# add additional info from countryinfo dataset\ndf_country = pd.read_csv(\"..\/input\/countryinfo\/covid19countryinfo.csv\")\ndf_country.head()\n\n# %% [code]\ndf_country['Country_Region'] = df_country['country']\ndf_country = df_country[df_country['country'].duplicated()==False]\n\n# %% [code]\nprint(df_country[df_country['country'].duplicated()].shape)\n\n# %% [code]\ndf_country[df_country['country'].duplicated()]\n\n# %% [code]\ndf_traintest7 = pd.merge(df_traintest6, \n                         df_country.drop(['tests', 'testpop', 'country'], axis=1), \n                         on=['Country_Region',], how='left')\nprint(df_traintest7.shape)\ndf_traintest7.head()\n\n# %% [code]\ndef encode_label(df, col, freq_limit=0):\n    df[col][pd.isna(df[col])] = 'nan'\n    tmp = df[col].value_counts()\n    cols = tmp.index.values\n    freq = tmp.values\n    num_cols = (freq>=freq_limit).sum()\n    print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n\n    col_new = '{}_le'.format(col)\n    df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n    for i, item in enumerate(cols[:num_cols]):\n        df_new[col_new][df[col]==item] = i\n\n    return df_new\n\ndef get_df_le(df, col_index, col_cat):\n    df_new = df[[col_index]]\n    for col in col_cat:\n        df_tmp = encode_label(df, col)\n        df_new = pd.concat([df_new, df_tmp], axis=1)\n    return df_new\n\ndf_traintest7['id'] = np.arange(len(df_traintest7))\ndf_le = get_df_le(df_traintest7, 'id', ['Country_Region', 'Province_State'])\ndf_traintest8 = pd.merge(df_traintest7, df_le, on='id', how='left')\n\n# %% [code]\ndf_traintest8['cases\/day'] = df_traintest8['cases\/day'].astype(np.float)\ndf_traintest8['fatal\/day'] = df_traintest8['fatal\/day'].astype(np.float)\n\n# %% [code]\n# covert object type to float\ndef func(x):\n    x_new = 0\n    try:\n        x_new = float(x.replace(\",\", \"\"))\n    except:\n#         print(x)\n        x_new = np.nan\n    return x_new\ncols = [\n    'Gross_domestic_product__constant_prices', \n    'Gross_domestic_product__current_prices', \n    'Gross_domestic_product__deflator', \n    'Gross_domestic_product_per_capita__constant_prices', \n    'Gross_domestic_product_per_capita__current_prices', \n    'Output_gap_in_percent_of_potential_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total', \n    'Implied_PPP_conversion_rate', 'Total_investment', \n    'Gross_national_savings', 'Inflation__average_consumer_prices', \n    'Inflation__end_of_period_consumer_prices', \n    'Six_month_London_interbank_offered_rate__LIBOR_', \n    'Volume_of_imports_of_goods_and_services', \n    'Volume_of_Imports_of_goods', \n    'Volume_of_exports_of_goods_and_services', \n    'Volume_of_exports_of_goods', 'Unemployment_rate', 'Employment', 'Population', \n    'General_government_revenue', 'General_government_total_expenditure', \n    'General_government_net_lending_borrowing', 'General_government_structural_balance', \n    'General_government_primary_net_lending_borrowing', 'General_government_net_debt', \n    'General_government_gross_debt', 'Gross_domestic_product_corresponding_to_fiscal_year__current_prices', \n    'Current_account_balance', 'pop'\n]\nfor col in cols:\n    df_traintest8[col] = df_traintest8[col].apply(lambda x: func(x))  \nprint(df_traintest8['pop'].dtype)\n\n# %% [code]\ndf_traintest8[df_traintest8['place_id']=='China\/Hubei'].head()\n\n# %% [markdown]\n# # Model training\n# here I train an LGBM model. The target is cases or fatalities per day because LGBM is failed to learn accumulated cases\/fatalities.\n# ### train a model for public LB\n\n# %% [code]\nday_before_valid = 104-7 # 3-11 day  before of validation\nday_before_public = 104 # 3-18 last day of train\nday_before_launch = 104 # 4-1 last day before launch\n\n# %% [code]\ndef calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    score = metrics.mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    return score\n\n# %% [code]\n# train model to predict fatalities\/day\n# params\nSEED = 42\nparams = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  # 42,\n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  # 1.728910519108444,\n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n          'verbosity': 100,\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n          'min_child_weight': 5,  # 19.428902804238373,\n          'num_threads': 6,\n          }\n\n\n# %% [code]\n# train model to predict fatalities\/day\n# features are selected manually based on valid score\ncol_target = 'fatal\/day'\ncol_var = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n#     'days_since_10cases', \n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal', 'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases\/day_(1-1)', \n    'cases\/day_(1-7)', \n#     'cases\/day_(8-14)',  \n#     'cases\/day_(15-21)', \n    \n#     'fatal\/day_(1-1)', \n    'fatal\/day_(1-7)', \n    'fatal\/day_(8-14)', \n    'fatal\/day_(15-21)', \n    'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', 'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n    'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n]\ncol_cat = []\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_valid)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_valid<df_traintest8['day']) & (df_traintest8['day']<=day_before_public)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration\n\n# %% [code]\ny_true = df_valid['fatal\/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))\n\n# %% [code]\n# display feature importance\ntmp = pd.DataFrame()\ntmp[\"feature\"] = col_var\ntmp[\"importance\"] = model.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp\n\n# %% [code]\n# train with all data before public\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\n# %% [code]\n# train model to predict fatalities\/day\ncol_target2 = 'cases\/day'\ncol_var2 = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n    'days_since_10cases', #selected\n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal',\n#     'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases\/day_(1-1)', \n    'cases\/day_(1-7)', \n    'cases\/day_(8-14)',  \n    'cases\/day_(15-21)', \n    \n#     'fatal\/day_(1-1)', \n#     'fatal\/day_(1-7)', \n#     'fatal\/day_(8-14)', \n#     'fatal\/day_(15-21)', \n#     'recov\/day_(1-1)', 'recov\/day_(1-7)', \n#     'recov\/day_(8-14)',  'recov\/day_(15-21)',\n#     'active_(1-1)', \n#     'active_(1-7)', \n#     'active_(8-14)',  'active_(15-21)', \n#     'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', \n#     'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n#     'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n]\ncol_cat = []\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_valid)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_valid<df_traintest8['day']) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration\n\n# %% [code]\ny_true = df_valid['cases\/day'].values\ny_pred = np.exp(model2.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))\n\n\n# %% [code]\n# display feature importance\ntmp = pd.DataFrame()\ntmp[\"feature\"] = col_var2\ntmp[\"importance\"] = model2.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp\n\n# %% [code]\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\n# %% [markdown]\n# \n# ### train a model for private LB\n\n# %% [code]\n# train model to predict fatalities\/day\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_public<df_traintest8['day'])]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration\n\n# %% [code]\n# train with all data\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel_pri = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\n# %% [code]\n# train model to predict cases\/day\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_public<df_traintest8['day'])]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration\n\n# %% [code]\n# train with all data\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2_pri = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\n# %% [markdown]\n# # Prediction\n\n# %% [code]\n# remove overlap for public LB prediction\ndf_tmp = df_traintest8[\n    ((df_traintest8['day']<=day_before_public)  & (pd.isna(df_traintest8['ForecastId'])))\n    | ((day_before_public<df_traintest8['day']) & (pd.isna(df_traintest8['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n    'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest9 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest9.append(df_tmp2)\ndf_traintest9 = pd.concat(df_traintest9).reset_index(drop=True)\ndf_traintest9[df_traintest9['day']>day_before_public-2].head()\n\n# %% [code]\n# remove overlap for private LB prediction\ndf_tmp = df_traintest8[\n    ((df_traintest8['day']<=day_before_private)  & (pd.isna(df_traintest8['ForecastId'])))\n    | ((day_before_private<df_traintest8['day']) & (pd.isna(df_traintest8['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n    'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest10 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest10.append(df_tmp2)\ndf_traintest10 = pd.concat(df_traintest10).reset_index(drop=True)\ndf_traintest10[df_traintest10['day']>day_before_private-2].head()\n\n# %% [code]\n# predict test data in public\n# predict the cases and fatatilites one day at a time and use the predicts as next day's feature recursively.\ndf_preds = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True))\n    df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_public).sum()\n    len_unknown = (day_before_public<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model.predict(X_valid)\n        pred_c = model2.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal\/day'][j+len_known] = pred_f\n        df_interest['cases\/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n        df_interest = df_interest.drop([\n            'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n            'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n    df_preds.append(df_interest)\ndf_preds = pd.concat(df_preds)\n\n# %% [code]\n# predict test data in public\ndf_preds_pri = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest10[df_traintest10['place_id']==place].reset_index(drop=True))\n    df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_private).sum()\n    len_unknown = (day_before_private<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model_pri.predict(X_valid)\n        pred_c = model2_pri.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal\/day'][j+len_known] = pred_f\n        df_interest['cases\/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n        df_interest = df_interest.drop([\n            'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n            'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n    df_preds_pri.append(df_interest)\ndf_preds_pri = pd.concat(df_preds_pri)\n\n# %% [markdown]\n# # Visualize prediction\n\n# %% [code]\nplaces_sort = df_traintest10[['place_id', 'ConfirmedCases']][df_traintest10['day']==day_before_private]\nplaces_sort = places_sort.sort_values('ConfirmedCases', ascending=False).reset_index(drop=True)['place_id'].values\nprint(len(places_sort))\nplaces_sort[:5]\n\n# %% [code]\nprint(\"Fatalities \/ Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()\n\n# %% [code]\nprint(\"Confirmed Cases \/ Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()\n\n# %% [code]\nprint(\"Fatalities \/ Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()\n\n# %% [code]\nprint(\"ConfirmedCases \/ Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()\n\n# %% [markdown]\n# # Make submission\n\n# %% [code]\n# merge 2 preds\ndf_preds[df_preds['day']>day_before_private] = df_preds_pri[df_preds['day']>day_before_private]\n\n# %% [code]\ndf_preds.to_csv(\"df_preds.csv\", index=None)\n\n# %% [code]\n# load sample submission\ndf_sub = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/submission.csv\")\nprint(len(df_sub))\ndf_sub.head()\n\n# %% [code]\n# merge prediction with sub\ndf_sub = pd.merge(df_sub, df_traintest3[['ForecastId', 'place_id', 'day']])\ndf_sub = pd.merge(df_sub, df_preds[['place_id', 'day', 'cases_pred', 'fatal_pred']], on=['place_id', 'day',], how='left')\ndf_sub.head(10)\n\n# %% [code]\n# save\ndf_sub['ConfirmedCases'] = df_sub['cases_pred']\ndf_sub['Fatalities'] = df_sub['fatal_pred']\ndf_sub = df_sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\ndf_sub2 = df_sub.copy()\ndf_sub.to_csv(\"submission5.csv\", index=None)\ndf_sub.head(10)\n\n# %% [code]\n\n\n# %% [code]\n","dabfb017":"gc.collect()","f5e5b8e3":"# %% [markdown]\n# - [week-1 version (2nd place as of 2020-4-4 update)](https:\/\/www.kaggle.com\/osciiart\/covid19-lightgbm?scriptVersionId=30830623)\n# - [week-2 version](https:\/\/www.kaggle.com\/osciiart\/covid-19-lightgbm-no-leak?scriptVersionId=31248128)\n\n# %% [code]\nimport os, gc, pickle, copy, datetime, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn import metrics\npd.set_option('display.max_columns', 100)\nwarnings.filterwarnings('ignore')\n\n# %% [markdown]\n# # Data loading\n\n# %% [code]\ndf_train = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\nprint(df_train.shape)\ndf_train.head()\n\n# %% [code]\ndf_test = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\nprint(df_test.shape)\ndf_test.head()\n\n# %% [code]\n# concat train and test\ndf_traintest = pd.concat([df_train, df_test])\nprint(df_train.shape, df_test.shape, df_traintest.shape)\n\n# %% [markdown]\n# ### Preprocessing\n\n# %% [code]\n# process date\ndf_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\ndf_traintest['day'] = df_traintest['Date'].apply(lambda x: x.dayofyear).astype(np.int16)\ndf_traintest.head()\n\n# %% [code]\nday_before_valid = 104-7 # 3-18 day  before of validation\nday_before_public = 104 # 3-25, the day before public LB period\nday_before_private = df_traintest['day'][pd.isna(df_traintest['ForecastId'])].max() # last day of train\nprint(df_traintest['Date'][df_traintest['day']==day_before_valid].values[0])\nprint(df_traintest['Date'][df_traintest['day']==day_before_public].values[0])\nprint(df_traintest['Date'][df_traintest['day']==day_before_private].values[0])\n\n# %% [code]\n# concat Country\/Region and Province\/State\ndef func(x):\n    try:\n        x_new = x['Country_Region'] + \"\/\" + x['Province_State']\n    except:\n        x_new = x['Country_Region']\n    return x_new\n        \ndf_traintest['place_id'] = df_traintest.apply(lambda x: func(x), axis=1)\ndf_traintest.head()\n\n# %% [code]\ndf_traintest[(df_traintest['day']>=day_before_public-3) & (df_traintest['place_id']=='China\/Hubei')].head()\n\n# %% [code]\n# concat lat and long\ndf_latlong = pd.read_csv(\"..\/input\/smokingstats\/df_Latlong.csv\")\ndf_latlong.head()\n\n# %% [code]\n# concat Country\/Region and Province\/State\ndef func(x):\n    try:\n        x_new = x['Country\/Region'] + \"\/\" + x['Province\/State']\n    except:\n        x_new = x['Country\/Region']\n    return x_new\n        \ndf_latlong['place_id'] = df_latlong.apply(lambda x: func(x), axis=1)\ndf_latlong = df_latlong[df_latlong['place_id'].duplicated()==False]\ndf_latlong.head()\n\n# %% [code]\ndf_traintest = pd.merge(df_traintest, df_latlong[['place_id', 'Lat', 'Long']], on='place_id', how='left')\ndf_traintest.head()\n\n# %% [code]\n# count the places with no Lat and Long.\ntmp = np.sort(df_traintest['place_id'][pd.isna(df_traintest['Lat'])].unique())\nprint(len(tmp)) # count Nan\ntmp\n\n# %% [code]\n# get place list\nplaces = np.sort(df_traintest['place_id'].unique())\nprint(len(places))\n\n# %% [code]\n# calc cases, fatalities per day\ndf_traintest2 = copy.deepcopy(df_traintest)\ndf_traintest2['cases\/day'] = 0\ndf_traintest2['fatal\/day'] = 0\ntmp_list = np.zeros(len(df_traintest2))\nfor place in places:\n    tmp = df_traintest2['ConfirmedCases'][df_traintest2['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest2['cases\/day'][df_traintest2['place_id']==place] = tmp\n    tmp = df_traintest2['Fatalities'][df_traintest2['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest2['fatal\/day'][df_traintest2['place_id']==place] = tmp\nprint(df_traintest2.shape)\ndf_traintest2[df_traintest2['place_id']=='China\/Hubei'].head()\n\n# %% [code]\n# aggregate cases and fatalities\ndef do_aggregation(df, col, mean_range):\n    df_new = copy.deepcopy(df)\n    col_new = '{}_({}-{})'.format(col, mean_range[0], mean_range[1])\n    df_new[col_new] = 0\n    tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean()\n    df_new[col_new][mean_range[0]:] = tmp[:-(mean_range[0])]\n    df_new[col_new][pd.isna(df_new[col_new])] = 0\n    return df_new[[col_new]].reset_index(drop=True)\n\ndef do_aggregations(df):\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [15,21]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [15,21]).reset_index(drop=True)], axis=1)\n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['ConfirmedCases']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}cases'.format(threshold)] = tmp\n            \n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['Fatalities']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}fatal'.format(threshold)] = tmp\n    \n    # process China\/Hubei\n    if df['place_id'][0]=='China\/Hubei':\n        df['days_since_1cases'] += 35 # 2019\/12\/8\n        df['days_since_10cases'] += 35-13 # 2019\/12\/8-2020\/1\/2 assume 2019\/12\/8+13\n        df['days_since_100cases'] += 4 # 2020\/1\/18\n        df['days_since_1fatal'] += 13 # 2020\/1\/9\n    return df\n\n# %% [code]\ndf_traintest3 = []\nfor place in places[:]:\n    df_tmp = df_traintest2[df_traintest2['place_id']==place].reset_index(drop=True)\n    df_tmp = do_aggregations(df_tmp)\n    df_traintest3.append(df_tmp)\ndf_traintest3 = pd.concat(df_traintest3).reset_index(drop=True)\ndf_traintest3[df_traintest3['place_id']=='China\/Hubei'].head()\n\n# %% [code]\n# add Smoking rate per country\n# data of smoking rate is obtained from https:\/\/ourworldindata.org\/smoking\ndf_smoking = pd.read_csv(\"..\/input\/smokingstats\/share-of-adults-who-smoke.csv\")\nprint(np.sort(df_smoking['Entity'].unique())[:10])\ndf_smoking.head()\n\n# %% [code]\n# extract newest data\ndf_smoking_recent = df_smoking.sort_values('Year', ascending=False).reset_index(drop=True)\ndf_smoking_recent = df_smoking_recent[df_smoking_recent['Entity'].duplicated()==False]\ndf_smoking_recent['Country_Region'] = df_smoking_recent['Entity']\ndf_smoking_recent['SmokingRate'] = df_smoking_recent['Smoking prevalence, total (ages 15+) (% of adults)']\ndf_smoking_recent.head()\n\n# %% [code]\n# merge\ndf_traintest4 = pd.merge(df_traintest3, df_smoking_recent[['Country_Region', 'SmokingRate']], on='Country_Region', how='left')\nprint(df_traintest4.shape)\ndf_traintest4.head()\n\n# %% [code]\n# fill na with world smoking rate\nSmokingRate = df_smoking_recent['SmokingRate'][df_smoking_recent['Entity']=='World'].values[0]\nprint(\"Smoking rate of the world: {:.6f}\".format(SmokingRate))\ndf_traintest4['SmokingRate'][pd.isna(df_traintest4['SmokingRate'])] = SmokingRate\ndf_traintest4.head()\n\n# %% [code]\n# add data from World Economic Outlook Database\n# https:\/\/www.imf.org\/external\/pubs\/ft\/weo\/2017\/01\/weodata\/index.aspx\ndf_weo = pd.read_csv(\"..\/input\/smokingstats\/WEO.csv\")\ndf_weo.head()\n\n# %% [code]\nprint(df_weo['Subject Descriptor'].unique())\n\n# %% [code]\nsubs  = df_weo['Subject Descriptor'].unique()[:-1]\ndf_weo_agg = df_weo[['Country']][df_weo['Country'].duplicated()==False].reset_index(drop=True)\nfor sub in subs[:]:\n    df_tmp = df_weo[['Country', '2019']][df_weo['Subject Descriptor']==sub].reset_index(drop=True)\n    df_tmp = df_tmp[df_tmp['Country'].duplicated()==False].reset_index(drop=True)\n    df_tmp.columns = ['Country', sub]\n    df_weo_agg = df_weo_agg.merge(df_tmp, on='Country', how='left')\ndf_weo_agg.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df_weo_agg.columns]\ndf_weo_agg.columns\ndf_weo_agg['Country_Region'] = df_weo_agg['Country']\ndf_weo_agg.head()\n\n# %% [code]\n# merge\ndf_traintest5 = pd.merge(df_traintest4, df_weo_agg, on='Country_Region', how='left')\nprint(df_traintest5.shape)\ndf_traintest5.head()\n\n# %% [code]\n# add Life expectancy\n# Life expectancy at birth obtained from http:\/\/hdr.undp.org\/en\/data\ndf_life = pd.read_csv(\"..\/input\/smokingstats\/Life expectancy at birth.csv\")\ntmp = df_life.iloc[:,1].values.tolist()\ndf_life = df_life[['Country', '2018']]\ndef func(x):\n    x_new = 0\n    try:\n        x_new = float(x.replace(\",\", \"\"))\n    except:\n#         print(x)\n        x_new = np.nan\n    return x_new\n    \ndf_life['2018'] = df_life['2018'].apply(lambda x: func(x))\ndf_life.head()\n\n# %% [code]\ndf_life = df_life[['Country', '2018']]\ndf_life.columns = ['Country_Region', 'LifeExpectancy']\n\n# %% [code]\n# merge\ndf_traintest6 = pd.merge(df_traintest5, df_life, on='Country_Region', how='left')\nprint(len(df_traintest6))\ndf_traintest6.head()\n\n# %% [code]\n# add additional info from countryinfo dataset\ndf_country = pd.read_csv(\"..\/input\/countryinfo\/covid19countryinfo.csv\")\ndf_country.head()\n\n# %% [code]\ndf_country['Country_Region'] = df_country['country']\ndf_country = df_country[df_country['country'].duplicated()==False]\n\n# %% [code]\nprint(df_country[df_country['country'].duplicated()].shape)\n\n# %% [code]\ndf_country[df_country['country'].duplicated()]\n\n# %% [code]\ndf_traintest7 = pd.merge(df_traintest6, \n                         df_country.drop(['tests', 'testpop', 'country'], axis=1), \n                         on=['Country_Region',], how='left')\nprint(df_traintest7.shape)\ndf_traintest7.head()\n\n# %% [code]\ndef encode_label(df, col, freq_limit=0):\n    df[col][pd.isna(df[col])] = 'nan'\n    tmp = df[col].value_counts()\n    cols = tmp.index.values\n    freq = tmp.values\n    num_cols = (freq>=freq_limit).sum()\n    print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n\n    col_new = '{}_le'.format(col)\n    df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n    for i, item in enumerate(cols[:num_cols]):\n        df_new[col_new][df[col]==item] = i\n\n    return df_new\n\ndef get_df_le(df, col_index, col_cat):\n    df_new = df[[col_index]]\n    for col in col_cat:\n        df_tmp = encode_label(df, col)\n        df_new = pd.concat([df_new, df_tmp], axis=1)\n    return df_new\n\ndf_traintest7['id'] = np.arange(len(df_traintest7))\ndf_le = get_df_le(df_traintest7, 'id', ['Country_Region', 'Province_State'])\ndf_traintest8 = pd.merge(df_traintest7, df_le, on='id', how='left')\n\n# %% [code]\ndf_traintest8['cases\/day'] = df_traintest8['cases\/day'].astype(np.float)\ndf_traintest8['fatal\/day'] = df_traintest8['fatal\/day'].astype(np.float)\n\n# %% [code]\n# covert object type to float\ndef func(x):\n    x_new = 0\n    try:\n        x_new = float(x.replace(\",\", \"\"))\n    except:\n#         print(x)\n        x_new = np.nan\n    return x_new\ncols = [\n    'Gross_domestic_product__constant_prices', \n    'Gross_domestic_product__current_prices', \n    'Gross_domestic_product__deflator', \n    'Gross_domestic_product_per_capita__constant_prices', \n    'Gross_domestic_product_per_capita__current_prices', \n    'Output_gap_in_percent_of_potential_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total', \n    'Implied_PPP_conversion_rate', 'Total_investment', \n    'Gross_national_savings', 'Inflation__average_consumer_prices', \n    'Inflation__end_of_period_consumer_prices', \n    'Six_month_London_interbank_offered_rate__LIBOR_', \n    'Volume_of_imports_of_goods_and_services', \n    'Volume_of_Imports_of_goods', \n    'Volume_of_exports_of_goods_and_services', \n    'Volume_of_exports_of_goods', 'Unemployment_rate', 'Employment', 'Population', \n    'General_government_revenue', 'General_government_total_expenditure', \n    'General_government_net_lending_borrowing', 'General_government_structural_balance', \n    'General_government_primary_net_lending_borrowing', 'General_government_net_debt', \n    'General_government_gross_debt', 'Gross_domestic_product_corresponding_to_fiscal_year__current_prices', \n    'Current_account_balance', 'pop'\n]\nfor col in cols:\n    df_traintest8[col] = df_traintest8[col].apply(lambda x: func(x))  \nprint(df_traintest8['pop'].dtype)\n\n# %% [code]\ndf_traintest8[df_traintest8['place_id']=='China\/Hubei'].head()\n\n# %% [markdown]\n# # Model training\n# here I train an LGBM model. The target is cases or fatalities per day because LGBM is failed to learn accumulated cases\/fatalities.\n# ### train a model for public LB\n\n# %% [code]\ndef calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    score = metrics.mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    return score\n\n# %% [code]\n# train model to predict fatalities\/day\n# params\nSEED = 42\nparams = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  # 42,\n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  # 1.728910519108444,\n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n          'verbosity': 100,\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n          'min_child_weight': 5,  # 19.428902804238373,\n          'num_threads': 6,\n          }\n\n\n# %% [code]\n# train model to predict fatalities\/day\n# features are selected manually based on valid score\ncol_target = 'fatal\/day'\ncol_var = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n#     'days_since_10cases', \n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal', 'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases\/day_(1-1)', \n    'cases\/day_(1-7)', \n#     'cases\/day_(8-14)',  \n#     'cases\/day_(15-21)', \n    \n#     'fatal\/day_(1-1)', \n    'fatal\/day_(1-7)', \n    'fatal\/day_(8-14)', \n    'fatal\/day_(15-21)', \n    'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', 'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n    'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n]\ncol_cat = []\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_valid)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_valid<df_traintest8['day']) & (df_traintest8['day']<=day_before_public)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 30000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=300,)\n\nbest_itr = model.best_iteration\n\n# %% [code]\ny_true = df_valid['fatal\/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))\n\n# %% [code]\n# display feature importance\ntmp = pd.DataFrame()\ntmp[\"feature\"] = col_var\ntmp[\"importance\"] = model.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp\n\n# %% [code]\n# train with all data before public\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=300,)\n\n# %% [code]\n# train model to predict fatalities\/day\ncol_target2 = 'cases\/day'\ncol_var2 = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n    'days_since_10cases', #selected\n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal',\n#     'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases\/day_(1-1)', \n    'cases\/day_(1-7)', \n    'cases\/day_(8-14)',  \n    'cases\/day_(15-21)', \n    \n#     'fatal\/day_(1-1)', \n#     'fatal\/day_(1-7)', \n#     'fatal\/day_(8-14)', \n#     'fatal\/day_(15-21)', \n#     'recov\/day_(1-1)', 'recov\/day_(1-7)', \n#     'recov\/day_(8-14)',  'recov\/day_(15-21)',\n#     'active_(1-1)', \n#     'active_(1-7)', \n#     'active_(8-14)',  'active_(15-21)', \n#     'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', \n#     'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n#     'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n]\ncol_cat = []\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_valid)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_valid<df_traintest8['day']) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=300,)\nbest_itr2 = model2.best_iteration\n\n# %% [code]\ny_true = df_valid['cases\/day'].values\ny_pred = np.exp(model2.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))\n\n\n# %% [code]\n# display feature importance\ntmp = pd.DataFrame()\ntmp[\"feature\"] = col_var2\ntmp[\"importance\"] = model2.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp\n\n# %% [code]\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=300,)\n\n# %% [markdown]\n# \n# ### train a model for private LB\n\n# %% [code]\n# train model to predict fatalities\/day\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_public<df_traintest8['day'])]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 30000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=300,)\n\nbest_itr = model.best_iteration\n\n# %% [code]\n# train with all data\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel_pri = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=300,)\n\n# %% [code]\n# train model to predict cases\/day\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_public<df_traintest8['day'])]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=300,)\nbest_itr2 = model2.best_iteration\n\n# %% [code]\n# train with all data\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2_pri = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=300,)\n\n# %% [markdown]\n# # Prediction\n\n# %% [code]\n# remove overlap for public LB prediction\ndf_tmp = df_traintest8[\n    ((df_traintest8['day']<=day_before_public)  & (pd.isna(df_traintest8['ForecastId'])))\n    | ((day_before_public<df_traintest8['day']) & (pd.isna(df_traintest8['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n    'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest9 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest9.append(df_tmp2)\ndf_traintest9 = pd.concat(df_traintest9).reset_index(drop=True)\ndf_traintest9[df_traintest9['day']>day_before_public-2].head()\n\n# %% [code]\n# remove overlap for private LB prediction\ndf_tmp = df_traintest8[\n    ((df_traintest8['day']<=day_before_private)  & (pd.isna(df_traintest8['ForecastId'])))\n    | ((day_before_private<df_traintest8['day']) & (pd.isna(df_traintest8['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n    'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest10 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest10.append(df_tmp2)\ndf_traintest10 = pd.concat(df_traintest10).reset_index(drop=True)\ndf_traintest10[df_traintest10['day']>day_before_private-2].head()\n\n# %% [code]\n# predict test data in public\n# predict the cases and fatatilites one day at a time and use the predicts as next day's feature recursively.\ndf_preds = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True))\n    df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_public).sum()\n    len_unknown = (day_before_public<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model.predict(X_valid)\n        pred_c = model2.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal\/day'][j+len_known] = pred_f\n        df_interest['cases\/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n        df_interest = df_interest.drop([\n            'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n            'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n    df_preds.append(df_interest)\ndf_preds = pd.concat(df_preds)\n\n# %% [code]\n# predict test data in public\ndf_preds_pri = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest10[df_traintest10['place_id']==place].reset_index(drop=True))\n    df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_private).sum()\n    len_unknown = (day_before_private<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model_pri.predict(X_valid)\n        pred_c = model2_pri.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal\/day'][j+len_known] = pred_f\n        df_interest['cases\/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n        df_interest = df_interest.drop([\n            'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n            'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n    df_preds_pri.append(df_interest)\ndf_preds_pri = pd.concat(df_preds_pri)\n\n# %% [markdown]\n# # Visualize prediction\n\n# %% [code]\nplaces_sort = df_traintest10[['place_id', 'ConfirmedCases']][df_traintest10['day']==day_before_private]\nplaces_sort = places_sort.sort_values('ConfirmedCases', ascending=False).reset_index(drop=True)['place_id'].values\nprint(len(places_sort))\nplaces_sort[:5]\n\n# %% [code]\nprint(\"Fatalities \/ Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()\n\n# %% [code]\nprint(\"Confirmed Cases \/ Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()\n\n# %% [code]\nprint(\"Fatalities \/ Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()\n\n# %% [code]\nprint(\"ConfirmedCases \/ Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()\n\n# %% [markdown]\n# # Make submission\n\n# %% [code]\n# merge 2 preds\ndf_preds[df_preds['day']>day_before_private] = df_preds_pri[df_preds['day']>day_before_private]\n\n# %% [code]\ndf_preds.to_csv(\"df_preds.csv\", index=None)\n\n# %% [code]\n# load sample submission\ndf_sub = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/submission.csv\")\nprint(len(df_sub))\ndf_sub.head()\n\n# %% [code]\n# merge prediction with sub\ndf_sub = pd.merge(df_sub, df_traintest3[['ForecastId', 'place_id', 'day']])\ndf_sub = pd.merge(df_sub, df_preds[['place_id', 'day', 'cases_pred', 'fatal_pred']], on=['place_id', 'day',], how='left')\ndf_sub.head(10)\n\n# %% [code]\n# save\ndf_sub['ConfirmedCases'] = df_sub['cases_pred']\ndf_sub['Fatalities'] = df_sub['fatal_pred']\ndf_sub = df_sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\ndf_sub1 = df_sub.copy()\ndf_sub.to_csv(\"submission1.csv\", index=None)\ndf_sub.head(10)","c3f7d7cb":"gc.collect()","2a54486a":"# Parameters - can be changed\nBAGS = 25\nSEED = 1234\nSET_FRAC = 0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Parameters - Other\n\n\n\nTRUNCATED = False\n\n\nDROPS = True\nPRIVATE = True\nUSE_PRIORS = False\n\n\nSUP_DROP = 0.0\nACTIONS_DROP = 0.0\nPLACE_FRACTION = 1.0  # 0.4 \n\n#** FEATURE_DROP = 0.4 # drop random % of features (HIGH!!!, speeds it up)\n#** COUNTRY_DROP = 0.35 # drop random % of countries (20-30pct)\n#** FIRST_DATE_DROP = 0.5 # Date_f must be after a certain date, randomly applied\n\n# FEATURE_DROP_MAX = 0.3\nLT_DECAY_MAX = 0.3\nLT_DECAY_MIN = -0.4\n\nSINGLE_MODEL = False\nMODEL_Y = 'agg_dff' # 'slope'  # 'slope' or anything else for difference\/aggregate log gain\n\n\n# %% [code]\n\n\n# %% [markdown]\n# \n# ### Init\n\n# %% [code]\nimport pandas as pd\nimport numpy as np\nimport os\n\n# %% [code]\nfrom collections import Counter\nfrom random import shuffle\nimport math\n\n# %% [code]\nfrom scipy.stats.mstats import gmean\n\n\n# %% [code]\nimport datetime\n\n# %% [code]\nimport matplotlib.pyplot as plt\nimport matplotlib as matplotlib\nimport seaborn as sns\n\n# %% [code]\npd.options.display.float_format = '{:.8}'.format\n\n\n# %% [code]\nplt.rcParams[\"figure.figsize\"] = (12, 4.75)\n \n# %% [code]\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n \n\n    \npd.options.display.max_rows = 999\n    \n# %% [code]\n\n\n\n\n# %% [markdown]\n# ### Import and Adjust\n\n# %% [markdown]\n# #### Import\n\n# %% [code]\npath = '..\/input\/david1013orgdataset'\ninput_path = '\/kaggle\/input\/covid19-global-forecasting-week-4'\n\n# %% [code]\ntrain = pd.read_csv(input_path + '\/train.csv')\ntest = pd.read_csv(input_path  + '\/test.csv')\nsub = pd.read_csv(input_path + '\/submission.csv')\n\n\ntt = pd.merge(train, test, on=['Country_Region', 'Province_State', 'Date'], \n              how='right', validate=\"1:1\")\\\n                    .fillna(method = 'ffill')\npublic = tt[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n      \n# %% [raw]\n# len(train)\n# len(test)\n\n# %% [code]\ntrain.Date.max()\n\n# %% [code]\ntest_dates = test.Date.unique()\ntest_dates\n\n# %% [raw]\n# # simulate week 1 sort of \n# test = test[ test.Date >=  '2020-03-25']\n\n# %% [raw]\n# test\n\n# %% [code]\npp = 'public'\n\n# %% [code]\n#FINAL_PUBLIC_DATE = datetime.datetime(2020, 4, 8)\n\nif PRIVATE:\n    test = test[ pd.to_datetime(test.Date) >  train.Date.max()]\n    pp = 'private'\n\n# %% [code]\ntest.Date.unique()\n\n# %% [markdown]\n# ### Train Fix\n\n# %% [markdown]\n# #### Supplement Missing US Data\n\n# %% [code]\nrevised = pd.read_csv(path +  \n                          '\/covid19_train_data_us_states_before_march_09_new.csv')\n\n\n# %% [raw]\n# revised.Date = pd.to_datetime(revised.Date)\n# revised.Date = revised.Date.apply(datetime.datetime.strftime, args= ('%Y-%m-%d',))\n\n# %% [code]\nrevised = revised[['Province_State', 'Country_Region', 'Date', 'ConfirmedCases', 'Fatalities']]\n\n# %% [code]\ntrain.tail()\n\n# %% [code]\nrevised.head()\n\n# %% [code]\ntrain.Date = pd.to_datetime(train.Date)\nrevised.Date = pd.to_datetime(revised.Date)\n\n# %% [code]\nrev_train = pd.merge(train, revised, on=['Province_State', 'Country_Region', 'Date'],\n                            suffixes = ('', '_r'), how='left')\n\n# %% [code]\n\n\n# %% [code]\nrev_train[~rev_train.ConfirmedCases_r.isnull()].head()\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\nrev_train.ConfirmedCases = \\\n    np.where( (rev_train.ConfirmedCases == 0) & ((rev_train.ConfirmedCases_r > 0 )) &\n                 (rev_train.Country_Region == 'US'),\n        \n        rev_train.ConfirmedCases_r,\n            rev_train.ConfirmedCases)\n\n\n# %% [code]\nrev_train.Fatalities = \\\n    np.where( ~rev_train.Fatalities_r.isnull() & \n                (rev_train.Fatalities == 0) & ((rev_train.Fatalities_r > 0 )) &\n                 (rev_train.Country_Region == 'US')\n             ,\n        \n        rev_train.Fatalities_r,\n            rev_train.Fatalities)\n\n\n# %% [code]\nrev_train.drop(columns = ['ConfirmedCases_r', 'Fatalities_r'], inplace=True)\n\n# %% [code]\ntrain = rev_train\n\n# %% [raw]\n# train[train.Province_State == 'California']\n\n# %% [raw]\n# import sys\n# def sizeof_fmt(num, suffix='B'):\n#     ''' by Fred Cirera,  https:\/\/stackoverflow.com\/a\/1094933\/1870254, modified'''\n#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n#         if abs(num) < 1024.0:\n#             return \"%3.1f %s%s\" % (num, unit, suffix)\n#         num \/= 1024.0\n#     return \"%.1f %s%s\" % (num, 'Yi', suffix)\n# \n# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n#                          key= lambda x: -x[1])[:10]:\n#     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n# \n\n# %% [markdown]\n# ### Oxford Actions Database\n\n# %% [code]\n# contain_data = pd.read_excel(path + '\/outside_data' + \n#                           '\/OxCGRT_Download_latest_data.xlsx')\n\ncontain_data = pd.read_csv(path +  \n                          '\/OxCGRT_Download_070420_160027_Full.csv')\n\n# %% [code] {\"scrolled\":true}\ncontain_data = contain_data[[c for c in contain_data.columns if \n                      not any(z in c for z in ['_Notes','Unnamed', 'Confirmed',\n                                               'CountryCode',\n                                                      'S8', 'S9', 'S10','S11',\n                                              'StringencyIndexForDisplay'])] ]\\\n        \n\n# %% [code]\ncontain_data.rename(columns = {'CountryName': \"Country\"}, inplace=True)\n\n# %% [code]\ncontain_data.Date = contain_data.Date.astype(str)\\\n    .apply(datetime.datetime.strptime, args=('%Y%m%d', ))\n\n# %% [code]\n\n\n# %% [code]\ncontain_data_orig = contain_data.copy()\n\n# %% [code]\ncontain_data.columns\n\n# %% [raw]\n# contain_data.columns\n\n# %% [code]\n\n\n# %% [code]\ncds = []\nfor country in contain_data.Country.unique():\n    cd = contain_data[contain_data.Country==country]\n    cd = cd.fillna(method = 'ffill').fillna(0)\n    cd.StringencyIndex = cd.StringencyIndex.cummax()  # for now\n    col_count = cd.shape[1]\n    \n    # now do a diff columns\n    # and ewms of it\n    for col in [c for c in contain_data.columns if 'S' in c]:\n        col_diff = cd[col].diff()\n        cd[col+\"_chg_5d_ewm\"] = col_diff.ewm(span = 5).mean()\n        cd[col+\"_chg_20_ewm\"] = col_diff.ewm(span = 20).mean()\n        \n    # stringency\n    cd['StringencyIndex_5d_ewm'] = cd.StringencyIndex.ewm(span = 5).mean()\n    cd['StringencyIndex_20d_ewm'] = cd.StringencyIndex.ewm(span = 20).mean()\n    \n    cd['S_data_days'] =  (cd.Date - cd.Date.min()).dt.days\n    for s in [1, 10, 20, 30, 50, ]:\n        cd['days_since_Stringency_{}'.format(s)] = \\\n                np.clip((cd.Date - cd[(cd.StringencyIndex > s)].Date.min()).dt.days, 0, None)\n    \n    \n    cds.append(cd.fillna(0)[['Country', 'Date'] + cd.columns.to_list()[col_count:]])\ncontain_data = pd.concat(cds)\n\n# %% [raw]\n# contain_data.columns\n\n# %% [raw]\n# dataset.groupby('Country').S_data_days.max().sort_values(ascending = False)[-30:]\n\n# %% [raw]\n# contain_data.StringencyIndex.cummax()\n\n# %% [raw]\n# contain_data.groupby('Date').count()[90:]\n\n# %% [code]\ncontain_data.Date.max()\n\n# %% [code]\ncontain_data.columns\n\n# %% [code]\ncontain_data[contain_data.Country == 'Australia']\n\n# %% [code]\ncontain_data.shape\n\n# %% [raw]\n# contain_data.groupby('Country').Date.max()[:50]\n\n# %% [code]\ncontain_data.Country.replace({ 'United States': \"US\",\n                                 'South Korea': \"Korea, South\",\n                                    'Taiwan': \"Taiwan*\",\n                              'Myanmar': \"Burma\", 'Slovak Republic': \"Slovakia\",\n                                  'Czech Republic': 'Czechia',\n\n}, inplace=True)\n\n# %% [code]\nset(contain_data.Country) - set(test.Country_Region)\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Load in Supplementary Data\n\n# %% [code]\nsup_data = pd.read_excel(path + \n                          '\/Data Join - Copy1.xlsx')\n\n\n# %% [code]\nsup_data.columns = [c.replace(' ', '_') for c in sup_data.columns.to_list()]\n\n# %% [code]\nsup_data.drop(columns = [c for c in sup_data.columns.to_list() if 'Unnamed:' in c], inplace=True)\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [raw]\n# sup_data.drop(columns = ['longitude', 'temperature', 'humidity',\n#                         'latitude'], inplace=True)\n\n# %% [raw]\n# sup_data.columns\n\n# %% [raw]\n# sup_data.drop(columns = [c for c in sup_data.columns if \n#                                  any(z in c for z in ['state', 'STATE'])], inplace=True)\n\n# %% [raw]\n# sup_data = sup_data[['Province_State', 'Country_Region',\n#                      'Largest_City',\n#                      'IQ', 'GDP_region', \n#                      'TRUE_POPULATION', 'pct_in_largest_city', \n#                    'Migrant_pct',\n#                     'Avg_age',\n#                      'latitude', 'longitude',\n#                 'abs_latitude', #  'Personality_uai', 'Personality_ltowvs',\n#               'Personality_pdi',\n# \n#                  'murder',  'real_gdp_growth'\n#                     ]]\n\n# %% [raw]\n# sup_data = sup_data[['Province_State', 'Country_Region',\n#                      'Largest_City',\n#                      'IQ', 'GDP_region', \n#                      'TRUE_POPULATION', 'pct_in_largest_city', \n#                    #'Migrant_pct',\n#                     # 'Avg_age',\n#                      # 'latitude', 'longitude',\n#              #    'abs_latitude', #  'Personality_uai', 'Personality_ltowvs',\n#             #   'Personality_pdi',\n# \n#                  'murder', # 'real_gdp_growth'\n#                     ]]\n\n# %% [code]\nsup_data.drop(columns = [ 'Date', 'ConfirmedCases',\n       'Fatalities', 'log-cases', 'log-fatalities', 'continent'], inplace=True)\n\n# %% [raw]\n# sup_data.drop(columns = [ 'Largest_City',  \n#                         'continent_gdp_pc', 'continent_happiness', 'continent_generosity',\n#        'continent_corruption', 'continent_Life_expectancy', 'TRUE_CHINA',\n#                          'Happiness', 'Logged_GDP_per_capita',\n#        'Social_support','HDI', 'GDP_pc', 'pc_GDP_PPP', 'Gini',\n#                          'state_white', 'state_white_asian', 'state_black',\n#        'INNOVATIVE_STATE','pct_urban', 'Country_pop', \n#                         \n#                         ], inplace=True)\n\n# %% [raw]\n# sup_data.columns\n\n# %% [raw]\n# \n\n# %% [code]\nsup_data['Migrants_in'] = np.clip(sup_data.Migrants, 0, None)\nsup_data['Migrants_out'] = -np.clip(sup_data.Migrants, None, 0)\nsup_data.drop(columns = 'Migrants', inplace=True)\n\n# %% [raw]\n# sup_data.loc[:, 'Largest_City'] = np.log(sup_data.Largest_City + 1)\n\n# %% [code]\nsup_data.head()\n\n# %% [code]\n\n\n# %% [code]\nsup_data.shape\n\n# %% [raw]\n# sup_data.loc[4][:50]\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Revise Columns\n\n# %% [code]\ntrain.Date = pd.to_datetime(train.Date)\ntest.Date = pd.to_datetime(test.Date)\n#contain_data.Date = pd.to_datetime(contain_data.Date)\n\n# %% [code]\ntrain.rename(columns={'Country_Region': 'Country'}, inplace=True)\ntest.rename(columns={'Country_Region': 'Country'}, inplace=True)\nsup_data.rename(columns={'Country_Region': 'Country'}, inplace=True)\n\n\n# %% [code]\ntrain['Place'] = train.Country + train.Province_State.fillna(\"\")\ntest['Place'] = test.Country +  test.Province_State.fillna(\"\")\n\n\n\n\n\n\n\n\n\n# %% [code]\nsup_data['Place'] = sup_data.Country +  sup_data.Province_State.fillna(\"\")\n\n# %% [code]\nlen(train.Place.unique())\n\n# %% [code]\nsup_data = sup_data[    \n    sup_data.columns.to_list()[2:]]\n\n# %% [code]\nsup_data = sup_data.replace('N.A.', np.nan).fillna(-0.5)\n\n# %% [code]\nfor c in sup_data.columns[:-1]:\n    m = sup_data[c].max() #- sup_data \n    \n    if m > 300 and c!='TRUE_POPULATION':\n        print(c)\n        sup_data[c] = np.log(sup_data[c] + 1)\n        assert sup_data[c].min() > -1\n\n# %% [code]\nfor c in sup_data.columns[:-1]:\n    m = sup_data[c].max() #- sup_data \n    \n    if m > 300:\n        print(c)\n\n# %% [code]\n\n\n# %% [code]\nDEATHS = 'Fatalities'\n\n# %% [code]\n\n\n# %% [code]\nlen(train.Place.unique())\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Correct Drop-Offs with interpolation\n\n# %% [raw]\n# \n# train[(train.ConfirmedCases.shift(1) > train.ConfirmedCases) & \n#          (train.Place == train.Place.shift(1)) & (train.ConfirmedCases == 0)]\n\n# %% [code]\n\n\n# %% [code]\ntrain.ConfirmedCases = \\\n    np.where(\n        (train.ConfirmedCases.shift(1) > train.ConfirmedCases) & \n        (train.ConfirmedCases.shift(1) > 0) & (train.ConfirmedCases.shift(-1) > 0) &\n         (train.Place == train.Place.shift(1)) & (train.Place == train.Place.shift(-1)) & \n        ~train.ConfirmedCases.shift(-1).isnull(),\n        \n        np.sqrt(train.ConfirmedCases.shift(1) * train.ConfirmedCases.shift(-1)),\n        \n        train.ConfirmedCases)\n\n\n\n# %% [code]\ntrain.Fatalities = \\\n    np.where(\n        (train.Fatalities.shift(1) > train.Fatalities) & \n        (train.Fatalities.shift(1) > 0) & (train.Fatalities.shift(-1) > 0) &\n         (train.Place == train.Place.shift(1)) & (train.Place == train.Place.shift(-1)) & \n        ~train.Fatalities.shift(-1).isnull(),\n        \n        np.sqrt(train.Fatalities.shift(1) * train.Fatalities.shift(-1)),\n        \n        train.Fatalities)\n\n\n\n# %% [code]\n\n\n# %% [code]\nfor i in [0, -1]:\n    train.ConfirmedCases = \\\n        np.where(\n            (train.ConfirmedCases.shift(2+ i ) > train.ConfirmedCases) & \n            (train.ConfirmedCases.shift(2+ i) > 0) & (train.ConfirmedCases.shift(-1+ i) > 0) &\n         (train.Place == train.Place.shift(2+ i)) & (train.Place == train.Place.shift(-1+ i)) & \n            ~train.ConfirmedCases.shift(-1+ i).isnull(),\n\n            np.sqrt(train.ConfirmedCases.shift(2+ i) * train.ConfirmedCases.shift(-1+ i)),\n\n            train.ConfirmedCases)\n\n\n\n# %% [code]\n\n\n\n# %% [code]\n\n\n# %% [code]\ntrain[train.Place=='USVirgin Islands'][-10:]\n\n# %% [code] {\"scrolled\":true}\n\ntrain[(train.ConfirmedCases.shift(2) > 2* train.ConfirmedCases) & \n         (train.Place == train.Place.shift(2)) & (train.ConfirmedCases < 100000)]\n\n# %% [code]\n\ntrain[(train.Fatalities.shift(1) > train.Fatalities) & \n\n      (train.Place == train.Place.shift(1)) & (train.Fatalities < 10000)]\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Use Training Set that is Old Predictions\n\n# %% [code]\n\n# %% [code]\ntrain_bk = train.copy()\n\n# %% [raw]\n# train.Date.unique()\n\n# %% [markdown]\n# #### Possible Truncation for Test Set Prediction\n\n# %% [code]\nfull_train = train.copy()\n\n# %% [raw]\n# full_train[full_train.Place =='USVirgin Islands']\n\n# %% [markdown]\n# ### Graphs\n\n# %% [code]\ntrain_c = train[train.Country == 'China']\ntrain_nc = train[train.Country != 'China']\ntrain_us = train[train.Country == 'US']\n# train_nc = train[train.Country != 'China']\n\n# %% [raw]\n# data.shape\n# data[data.ConfirmedCases > 0].shape\n# data.ConfirmedCases\n\n# %% [code]\ndef lplot(data, minDate = datetime.datetime(2000, 1, 1), \n              columns = ['ConfirmedCases', 'Fatalities']):\n    return\n        \n\n# %% [code]\nREAL = datetime.datetime(2020, 2, 10)\n\n\n# %% [code]\ndataset = train.copy()\n\n\nif TRUNCATED:\n    dataset = dataset[dataset.Country.isin(\n        ['Italy', 'Spain', 'Germany', 'Portugal', 'Belgium', 'Austria', 'Switzerland' ])]\n\n# %% [code]\ndataset.head()\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Create Lagged Growth Rates (4, 7, 12, 20 day rates)\n\n# %% [code]\ndef rollDates(df, i, preserve=False):\n    df = df.copy()\n    if preserve:\n        df['Date_i'] = df.Date\n    df.Date = df.Date + datetime.timedelta(i)\n    return df\n\n# %% [code]\nWINDOWS = [1, 2,  4, 7, 12, 20, 30]\n\n# %% [code]\nfor window in WINDOWS:\n    csuffix = '_{}d_prior_value'.format(window)\n    \n    base = rollDates(dataset, window)\n    dataset = pd.merge(dataset, base[['Date', 'Place',\n                'ConfirmedCases', 'Fatalities']], on = ['Date', 'Place'],\n            suffixes = ('', csuffix), how='left')\n#     break;\n    for c in ['ConfirmedCases', 'Fatalities']:\n        dataset[c+ csuffix].fillna(0, inplace=True)\n        dataset[c+ csuffix] = np.log(dataset[c + csuffix] + 1)\n        dataset[c+ '_{}d_prior_slope'.format(window)] = \\\n                    (np.log(dataset[c] + 1) \\\n                         - dataset[c+ csuffix]) \/ window\n        dataset[c+ '_{}d_ago_zero'.format(window)] = 1.0*(dataset[c+ csuffix] == 0)     \n    \n    \n    \n\n# %% [code]\nfor window1 in WINDOWS:\n    for window2 in WINDOWS:\n        for c in ['ConfirmedCases', 'Fatalities']:\n            if window1 * 1.3 < window2 and window1 * 5 > window2:\n                dataset[ c +'_{}d_{}d_prior_slope_chg'.format(window1, window2) ] = \\\n                        dataset[c+ '_{}d_prior_slope'.format(window1)] \\\n                                - dataset[c+ '_{}d_prior_slope'.format(window2)]\n                \n                \n\n# %% [raw]\n# dataset.tail()\n\n# %% [raw]\n# dataset\n\n# %% [markdown]\n# #### First Case Etc.\n\n# %% [code]\nfirst_case = dataset[dataset.ConfirmedCases >= 1].groupby('Place').min() \ntenth_case = dataset[dataset.ConfirmedCases >= 10].groupby('Place').min()\nhundredth_case = dataset[dataset.ConfirmedCases >= 100].groupby('Place').min()\nthousandth_case = dataset[dataset.ConfirmedCases >= 1000].groupby('Place').min()\n\n# %% [code]\nfirst_fatality = dataset[dataset.Fatalities >= 1].groupby('Place').min()\ntenth_fatality = dataset[dataset.Fatalities >= 10].groupby('Place').min()\nhundredth_fatality = dataset[dataset.Fatalities >= 100].groupby('Place').min()\nthousandth_fatality = dataset[dataset.Fatalities >= 1000].groupby('Place').min()\n\n\n# %% [raw]\n# np.isinf(dataset.days_since_hundredth_case).sum()\n\n# %% [raw]\n# (dataset.Date - hundredth_case.loc[dataset.Place].Date.values).dt.days\n\n# %% [code]\ndataset['days_since_first_case'] = \\\n        np.clip((dataset.Date - first_case.loc[dataset.Place].Date.values).dt.days\\\n                            .fillna(-1), -1, None)\ndataset['days_since_tenth_case'] = \\\n        np.clip((dataset.Date - tenth_case.loc[dataset.Place].Date.values).dt.days\\\n                            .fillna(-1), -1, None)\ndataset['days_since_hundredth_case'] = \\\n        np.clip((dataset.Date - hundredth_case.loc[dataset.Place].Date.values).dt.days\\\n                            .fillna(-1), -1, None)\ndataset['days_since_thousandth_case'] = \\\n        np.clip((dataset.Date - thousandth_case.loc[dataset.Place].Date.values).dt.days\\\n                            .fillna(-1), -1, None)\n\n\n# %% [code]\ndataset['days_since_first_fatality'] = \\\n        np.clip((dataset.Date - first_fatality.loc[dataset.Place].Date.values).dt.days\\\n                    .fillna(-1), -1, None)\ndataset['days_since_tenth_fatality'] = \\\n        np.clip((dataset.Date - tenth_fatality.loc[dataset.Place].Date.values).dt.days\\\n                    .fillna(-1), -1, None)\ndataset['days_since_hundredth_fatality'] = \\\n        np.clip((dataset.Date - hundredth_fatality.loc[dataset.Place].Date.values).dt.days\\\n                    .fillna(-1), -1, None)\ndataset['days_since_thousandth_fatality'] = \\\n        np.clip((dataset.Date - thousandth_fatality.loc[dataset.Place].Date.values).dt.days\\\n                    .fillna(-1), -1, None)\n\n# %% [code]\n\n\n# %% [code]\ndataset['case_rate_since_first_case'] = \\\n    np.clip((np.log(dataset.ConfirmedCases + 1) \\\n             - np.log(first_case.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1)) \\\n                    \/ (dataset.days_since_first_case+0.01), 0, 1)\ndataset['case_rate_since_tenth_case'] = \\\n    np.clip((np.log(dataset.ConfirmedCases + 1) \\\n             - np.log(tenth_case.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1)) \\\n                    \/ (dataset.days_since_tenth_case+0.01), 0, 1)\ndataset['case_rate_since_hundredth_case'] = \\\n    np.clip((np.log(dataset.ConfirmedCases + 1) \\\n             - np.log(hundredth_case.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1)) \\\n                    \/ (dataset.days_since_first_case+0.01), 0, 1)\ndataset['case_rate_since_thousandth_case'] = \\\n    np.clip((np.log(dataset.ConfirmedCases + 1) \\\n             - np.log(thousandth_case.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1)) \\\n                    \/ (dataset.days_since_first_case+0.01), 0, 1)\n\n# %% [code]\ndataset['fatality_rate_since_first_case'] = \\\n    np.clip((np.log(dataset.Fatalities + 1) \\\n             - np.log(first_case.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \\\n                    \/ (dataset.days_since_first_case+0.01), 0, 1)\ndataset['fatality_rate_since_tenth_case'] = \\\n    np.clip((np.log(dataset.Fatalities + 1) \\\n             - np.log(tenth_case.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \\\n                    \/ (dataset.days_since_first_case+0.01), 0, 1)\ndataset['fatality_rate_since_hundredth_case'] = \\\n    np.clip((np.log(dataset.Fatalities + 1) \\\n             - np.log(hundredth_case.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \\\n                    \/ (dataset.days_since_first_case+0.01), 0, 1)\ndataset['fatality_rate_since_thousandth_case'] = \\\n    np.clip((np.log(dataset.Fatalities + 1) \\\n             - np.log(thousandth_case.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \\\n                    \/ (dataset.days_since_first_case+0.01), 0, 1)\n\n\n#.plot(kind='hist', bins = 150)\n\n# %% [code]\ndataset['fatality_rate_since_first_fatality'] = \\\n    np.clip((np.log(dataset.Fatalities + 1) \\\n             - np.log(first_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \\\n                    \/ (dataset.days_since_first_fatality+0.01), 0, 1)\ndataset['fatality_rate_since_tenth_fatality'] = \\\n    np.clip((np.log(dataset.Fatalities + 1) \\\n             - np.log(tenth_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \\\n                    \/ (dataset.days_since_tenth_fatality+0.01), 0, 1)\ndataset['fatality_rate_since_hundredth_fatality'] = \\\n    np.clip((np.log(dataset.Fatalities + 1) \\\n             - np.log(hundredth_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \\\n                    \/ (dataset.days_since_hundredth_fatality+0.01), 0, 1)\ndataset['fatality_rate_since_thousandth_fatality'] = \\\n    np.clip((np.log(dataset.Fatalities + 1) \\\n             - np.log(thousandth_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1)) \\\n                    \/ (dataset.days_since_thousandth_fatality+0.01), 0, 1)\n \n#.plot(kind='hist', bins = 150)\n\n# %% [code]\n\n\n# %% [code]\ndataset['first_case_ConfirmedCases'] = \\\n       np.log(first_case.loc[dataset.Place].ConfirmedCases.values + 1)\ndataset['first_case_Fatalities'] = \\\n       np.log(first_case.loc[dataset.Place].Fatalities.values + 1)\n\n# %% [code]\n\n\n# %% [code]\ndataset['first_fatality_ConfirmedCases'] = \\\n       np.log(first_fatality.loc[dataset.Place].ConfirmedCases.fillna(0).values + 1) \\\n            * (dataset.days_since_first_fatality >= 0 )\ndataset['first_fatality_Fatalities'] = \\\n       np.log(first_fatality.loc[dataset.Place].Fatalities.fillna(0).values + 1) \\\n            * (dataset.days_since_first_fatality >= 0 )\n\n# %% [code]\ndataset['first_fatality_cfr'] = \\\n    np.where(dataset.days_since_first_fatality < 0,\n            -8,\n        (dataset.first_fatality_Fatalities) -\n               (dataset.first_fatality_ConfirmedCases )   )\n\n# %% [code]\ndataset['first_fatality_lag_vs_first_case'] = \\\n    np.where(dataset.days_since_first_fatality >= 0,\n                 dataset.days_since_first_case - dataset.days_since_first_fatality , -1)\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Update Frequency, MAs of Change Rates, etc.\n\n# %% [code]\ndataset['case_chg'] = \\\n    np.clip(np.log(dataset.ConfirmedCases + 1 )\\\n            - np.log(dataset.ConfirmedCases.shift(1) +1), 0, None).fillna(0)\n\n# %% [code]\ndataset['case_chg_ema_3d'] = dataset.case_chg.ewm(span = 3).mean() \\\n                                * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/3, 0, 1)\ndataset['case_chg_ema_10d'] = dataset.case_chg.ewm(span = 10).mean() \\\n                             * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/10, 0, 1)\n\n# %% [code]\ndataset['case_chg_stdev_5d'] = dataset.case_chg.rolling(5).std() \\\n                                * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/5, 0, 1)\ndataset['case_chg_stdev_15d'] = dataset.case_chg.rolling(15).std() \\\n                                * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/15, 0, 1)\n\n# %% [raw]\n# dataset['max_case_chg_3d'] = dataset.case_chg.rolling(3).max() \\\n#                                  * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/3, 0, 1)\n# dataset['max_case_chg_10d'] = dataset.case_chg.rolling(10).max() \\\n#                                  * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/10, 0, 1)\n\n# %% [code]\ndataset['case_update_pct_3d_ewm'] = (dataset.case_chg > 0).ewm(span = 3).mean() \\\n                     * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/3, 0, 1), 2)\ndataset['case_update_pct_10d_ewm'] = (dataset.case_chg > 0).ewm(span = 10).mean() \\\n                     * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/10, 0, 1), 2)\ndataset['case_update_pct_30d_ewm'] = (dataset.case_chg > 0).ewm(span = 30).mean() \\\n                     * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/30, 0, 1), 2)\n\n \n\n# %% [code]\n\n\n# %% [code]\ndataset['fatality_chg'] = \\\n    np.clip(np.log(dataset.Fatalities + 1 )\\\n            - np.log(dataset.Fatalities.shift(1) +1), 0, None).fillna(0)\n\n# %% [code]\ndataset['fatality_chg_ema_3d'] = dataset.fatality_chg.ewm(span = 3).mean() \\\n                    * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/33, 0, 1)\ndataset['fatality_chg_ema_10d'] = dataset.fatality_chg.ewm(span = 10).mean() \\\n                    * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/10, 0, 1)\n\n# %% [code]\ndataset['fatality_chg_stdev_5d'] = dataset.fatality_chg.rolling(5).std() \\\n                                * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/5, 0, 1)\ndataset['fatality_chg_stdev_15d'] = dataset.fatality_chg.rolling(15).std() \\\n                                * np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/15, 0, 1)\n\n# %% [code]\ndataset['fatality_update_pct_3d_ewm'] = (dataset.fatality_chg > 0).ewm(span = 3).mean() \\\n                     * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/3, 0, 1), 2)\ndataset['fatality_update_pct_10d_ewm'] = (dataset.fatality_chg > 0).ewm(span = 10).mean() \\\n                     * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/10, 0, 1), 2)\ndataset['fatality_update_pct_30d_ewm'] = (dataset.fatality_chg > 0).ewm(span = 30).mean() \\\n                     * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/30, 0, 1), 2)\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\ndataset.tail()\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Add Supp Data\n\n# %% [code]\n# lag containment data as one week behind\ncontain_data.Date = contain_data.Date + datetime.timedelta(7)\n\n# %% [code]\ncontain_data.Date.max()\n\n# %% [code]\nassert set(dataset.Place.unique()) == set(dataset.Place.unique())\ndataset = pd.merge(dataset, sup_data, on='Place', how='left', validate='m:1')\ndataset = pd.merge(dataset, contain_data, on = ['Country', 'Date'], how='left', validate='m:1')\n\n# %% [code]\ndataset['log_true_population'] =   np.log(dataset.TRUE_POPULATION + 1)\n\n# %% [code]\ndataset['ConfirmedCases_percapita'] = np.log(dataset.ConfirmedCases + 1)\\\n                                        - np.log(dataset.TRUE_POPULATION + 1)\ndataset['Fatalities_percapita'] = np.log(dataset.Fatalities + 1)\\\n                                        - np.log(dataset.TRUE_POPULATION + 1)\n\n# %% [code]\n\n\n# %% [markdown]\n# ##### CFR\n\n# %% [raw]\n# np.log( 0 + 0.015\/1)\n\n# %% [raw]\n# BLCFR = -4.295015257684252\n\n# %% [code] {\"scrolled\":true}\n# dataset['log_cfr_bad'] = np.log(dataset.Fatalities + 1) - np.log(dataset.ConfirmedCases + 1)\ndataset['log_cfr'] = np.log(    (dataset.Fatalities \\\n                                         + np.clip(0.015 * dataset.ConfirmedCases, 0, 0.3)) \\\n                            \/ ( dataset.ConfirmedCases + 0.1) )\n\n# %% [code]\ndef cfr(case, fatality):\n    cfr_calc = np.log(    (fatality \\\n                                         + np.clip(0.015 * case, 0, 0.3)) \\\n                            \/ ( case + 0.1) )\n#     cfr_calc =np.array(cfr_calc)\n    return np.where(np.isnan(cfr_calc) | np.isinf(cfr_calc),\n                           BLCFR, cfr_calc)\n\n# %% [code]\nBLCFR = np.median(dataset[dataset.ConfirmedCases==1].log_cfr[::10])\ndataset.log_cfr.fillna(BLCFR, inplace=True)\ndataset.log_cfr = np.where(dataset.log_cfr.isnull() | np.isinf(dataset.log_cfr),\n                           BLCFR, dataset.log_cfr)\nBLCFR\n\n# %% [code]\ndataset['log_cfr_3d_ewm'] = BLCFR + \\\n                (dataset.log_cfr - BLCFR).ewm(span = 3).mean()  \\\n                     * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/3, 0, 1), 2)\n                     \ndataset['log_cfr_8d_ewm'] = BLCFR + \\\n                (dataset.log_cfr - BLCFR).ewm(span = 8).mean()  \\\n                     * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/8, 0, 1), 2)\n\ndataset['log_cfr_20d_ewm'] = BLCFR + \\\n                (dataset.log_cfr - BLCFR).ewm(span = 20).mean()  \\\n                     * np.power(np.clip( (dataset.Date - dataset.Date.min() ).dt.days\/20, 0, 1), 2)\n\ndataset['log_cfr_3d_20d_ewm_crossover'] = dataset.log_cfr_3d_ewm - dataset.log_cfr_20d_ewm\n\n\n# %% [code]\ndataset.drop(columns = 'log_cfr', inplace=True)\n\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ##### Per Capita vs. World and Similar Countries\n\n# %% [code]\ndate_totals = dataset.groupby('Date').sum()\n\n# %% [code]\nmean_7d_c_slope = dataset.groupby('Date')[['ConfirmedCases_7d_prior_slope']].apply(lambda x:\n                                        np.mean(x[x > 0]) ).ewm(span = 3).mean() \nmean_7d_f_slope = dataset.groupby('Date')[['Fatalities_7d_prior_slope']].apply(lambda x:\n                                        np.mean(x[x > 0]) ).ewm(span = 7).mean()\n\n# %% [raw]\n# mean_7d_c_slope.plot()\n\n# %% [raw]\n# dataset.columns[:100]\n\n# %% [raw]\n# mean_7d_c_slope.plot()\n\n# %% [raw]\n# date_totals.Fatalities_7d_prior_slope.plot()\n\n# %% [raw]\n# date_counts = dataset.groupby('Date').apply(lambda x:  x > 0)\n\n# %% [raw]\n# date_counts\n\n# %% [raw]\n# date_totals['world_cases_chg'] = (np.log(date_totals.ConfirmedCases + 1 )\\\n#                                     - np.log(date_totals.ConfirmedCases.shift(1) + 1) )\\\n#                                     .fillna(method='bfill')\n# date_totals['world_fatalities_chg'] = (np.log(date_totals.Fatalities + 1 )\\\n#                                     - np.log(date_totals.Fatalities.shift(1) + 1) )\\\n#                                     .fillna(method='bfill')\n# date_totals['world_cases_chg_10d_ewm'] = \\\n#         date_totals.world_cases_chg.ewm(span=10).mean()\n# date_totals['world_fatalities_chg_10d_ewm'] = \\\n#         date_totals.world_fatalities_chg.ewm(span=10).mean()  \n\n# %% [raw]\n# \n# dataset['world_cases_chg_10d_ewm'] = \\\n#         date_totals.loc[dataset.Date].world_cases_chg_10d_ewm.values\n# \n# dataset['world_fatalities_chg_10d_ewm'] = \\\n#         date_totals.loc[dataset.Date].world_fatalities_chg_10d_ewm.values\n# \n\n# %% [raw]\n# dataset.continent\n\n# %% [raw]\n# date_totals\n\n# %% [code]\ndataset['ConfirmedCases_percapita_vs_world'] = np.log(dataset.ConfirmedCases + 1)\\\n                                        - np.log(dataset.TRUE_POPULATION + 1) \\\n                                   -  (\n                                   np.log(date_totals.loc[dataset.Date].ConfirmedCases + 1)  \n                                       -np.log(date_totals.loc[dataset.Date].TRUE_POPULATION + 1)\n                                        ).values\n\ndataset['Fatalities_percapita_vs_world'] = np.log(dataset.Fatalities + 1)\\\n                                            - np.log(dataset.TRUE_POPULATION + 1) \\\n                                    -  (\n                                   np.log(date_totals.loc[dataset.Date].Fatalities + 1)  \n                                       -np.log(date_totals.loc[dataset.Date].TRUE_POPULATION + 1)\n                                        ).values\ndataset['cfr_vs_world'] = dataset.log_cfr_3d_ewm \\\n                            -    np.log(    date_totals.loc[dataset.Date].Fatalities   \\\n                            \/   date_totals.loc[dataset.Date].ConfirmedCases ).values\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Nearby Countries\n\n# %% [code]\ncont_date_totals = dataset.groupby(['Date', 'continent_generosity']).sum()\n\n# %% [raw]\n# cont_date_totals.iloc[dataset.Date]\n\n# %% [code]\nlen(dataset)\n\n# %% [raw]\n# dataset.columns\n\n# %% [raw]\n# dataset.TRUE_POPULATION\n\n# %% [raw]\n# dataset\n\n# %% [raw]\n# dataset\n\n# %% [code]\ndataset['ConfirmedCases_percapita_vs_continent_mean'] = 0\ndataset['Fatalities_percapita_vs_continent_mean'] = 0\ndataset['ConfirmedCases_percapita_vs_continent_median'] = 0\ndataset['Fatalities_percapita_vs_continent_median'] = 0\n\nfor cg in dataset.continent_generosity.unique():\n    ps = dataset.groupby(\"Place\").last()\n    tp = ps[ps.continent_generosity==cg].TRUE_POPULATION.sum()\n    print(tp \/ 1e9)\n    for Date in dataset.Date.unique():\n        cd =  dataset[(dataset.Date == Date) &\n                    (dataset.continent_generosity == cg)]\\\n                               [['ConfirmedCases', 'Fatalities', 'TRUE_POPULATION']]\n#         print(cd)\n        cmedian = np.median(np.log(cd.ConfirmedCases + 1)\\\n                                              - np.log(cd.TRUE_POPULATION+1))\n        cmean = np.log(cd.ConfirmedCases.sum() + 1) - np.log(tp + 1)\n        fmedian = np.median(np.log(cd.Fatalities + 1)\\\n                                              - np.log(cd.TRUE_POPULATION+1))\n        fmean = np.log(cd.Fatalities.sum() + 1) - np.log(tp + 1)\n        cfrmean = cfr( cd.ConfirmedCases.sum(),  cd.Fatalities.sum()   ) \n#         print(cmean)\n        \n#         break;\n        \n        dataset.loc[(dataset.Date == Date) &\n                    (dataset.continent_generosity == cg), \n                    'ConfirmedCases_percapita_vs_continent_mean'] = \\\n                                dataset['ConfirmedCases_percapita'] \\\n                                     - (cmean)\n        dataset.loc[(dataset.Date == Date) &\n                    (dataset.continent_generosity == cg), \n                    'ConfirmedCases_percapita_vs_continent_median'] = \\\n                                dataset['ConfirmedCases_percapita'] \\\n                                     - (cmedian)\n        \n        dataset.loc[(dataset.Date == Date) &\n                    (dataset.continent_generosity == cg), \n                    'Fatalities_percapita_vs_continent_mean'] = \\\n                                dataset['Fatalities_percapita']\\\n                                    - (fmean)\n        dataset.loc[(dataset.Date == Date) &\n                    (dataset.continent_generosity == cg), \n                    'Fatalities_percapita_vs_continent_median'] = \\\n                                dataset['Fatalities_percapita']\\\n                                    - (fmedian)\n        \n        dataset.loc[(dataset.Date == Date) &\n                    (dataset.continent_generosity == cg), \n                    'cfr_vs_continent'] = \\\n                                dataset.log_cfr_3d_ewm \\\n                            -    cfrmean\n#       \n#         r.ConfirmedCases\n#         r.Fatalities\n#         print(continent)\n    \n\n# %% [code]\n\n\n# %% [raw]\n# dataset[dataset.Country=='China'][['Place', 'Date', \n#                'ConfirmedCases_percapita_vs_continent_mean',\n#                'Fatalities_percapita_vs_continent_mean']][1000::10]\n\n# %% [raw]\n# dataset[['Place', 'Date', \n#                'cfr_vs_continent']][10000::5]\n\n# %% [code]\n\n\n# %% [code]\nall_places = dataset[['Place', 'latitude', 'longitude']].drop_duplicates().set_index('Place',\n                                                                                    drop=True)\nall_places.head()\n\n# %% [code]\ndef surroundingPlaces(place, d = 10):\n    dist = (all_places.latitude - all_places.loc[place].latitude)**2 \\\n                    + (all_places.longitude - all_places.loc[place].longitude) ** 2 \n    return all_places[dist < d**2][1:n+1]\n\n# %% [raw]\n# surroundingPlaces('Afghanistan', 5)\n\n# %% [code]\ndef nearestPlaces(place, n = 10):\n    dist = (all_places.latitude - all_places.loc[place].latitude)**2 \\\n                    + (all_places.longitude - all_places.loc[place].longitude) ** 2\n    ranked = np.argsort(dist) \n    return all_places.iloc[ranked][1:n+1]\n\n# %% [code]\n\n\n# %% [raw]\n# dataset.ConfirmedCases_percapita\n\n# %% [code]\ndgp = dataset.groupby('Place').last()\nfor n in [5, 10, 20]:\n#     dataset['ConfirmedCases_percapita_vs_nearest{}'.format(n)] = 0\n#     dataset['Fatalities_percapita_vs_nearest{}'.format(n)] = 0\n    \n    for place in dataset.Place.unique():\n        nps = nearestPlaces(place, n)\n        tp = dgp.loc[nps.index].TRUE_POPULATION.sum()\n#         print(tp)\n        \n        \n        dataset.loc[dataset.Place==place, \n                    'ratio_population_vs_nearest{}'.format(n)] = \\\n            np.log(dataset.loc[dataset.Place==place].TRUE_POPULATION.mean() + 1)\\\n                - np.log(tp+1)\n         \n#         dataset.loc[dataset.Place==place, \n#                     'avg_distance_to_nearest{}'.format(n)] = \\\n#             (dataset.loc[dataset.Place==place].latitude.mean() + 1)\\\n#                 - np.log(tp+1)\n        \n\n        nbps =  dataset[(dataset.Place.isin(nps.index))]\\\n                            .groupby('Date')[['ConfirmedCases', 'Fatalities']].sum()\n\n        nppc = (np.log( nbps.loc[dataset[dataset.Place==place].Date]\\\n                                          .fillna(0).ConfirmedCases + 1) - np.log(tp + 1))\n        nppf = (np.log( nbps.loc[dataset[dataset.Place==place].Date]\\\n                                          .fillna(0).Fatalities + 1) - np.log(tp + 1))\n        npp_cfr = cfr( nbps.loc[dataset[dataset.Place==place].Date]\\\n                                          .fillna(0).ConfirmedCases,\n                      nbps.loc[dataset[dataset.Place==place].Date]\\\n                                          .fillna(0).Fatalities)\n#         print(npp_cfr)\n#         continue;\n        \n        dataset.loc[\n                (dataset.Place == place),\n                    'ConfirmedCases_percapita_vs_nearest{}'.format(n)] = \\\n            dataset[(dataset.Place == place)].ConfirmedCases_percapita \\\n                            - nppc.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'Fatalities_percapita_vs_nearest{}'.format(n)] = \\\n            dataset[(dataset.Place == place)].Fatalities_percapita \\\n                            - nppf.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'cfr_vs_nearest{}'.format(n)] = \\\n            dataset[(dataset.Place == place)].log_cfr_3d_ewm \\\n                            - npp_cfr   \n        \n        dataset.loc[\n                (dataset.Place == place),\n                    'ConfirmedCases_nearest{}_percapita'.format(n)] = nppc.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'Fatalities_nearest{}_percapita'.format(n)] = nppf.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'cfr_nearest{}'.format(n)] = npp_cfr\n        \n        dataset.loc[\n                (dataset.Place == place),\n                    'ConfirmedCases_nearest{}_10d_slope'.format(n)] =   \\\n                               ( nppc.ewm(span = 1).mean() - nppc.ewm(span = 10).mean() ).values\n        dataset.loc[\n                (dataset.Place == place),\n                    'Fatalities_nearest{}_10d_slope'.format(n)] =   \\\n                               ( nppf.ewm(span = 1).mean() - nppf.ewm(span = 10).mean() ).values\n        \n        npp_cfr_s = pd.Series(npp_cfr)\n        dataset.loc[ \n                (dataset.Place == place),\n                    'cfr_nearest{}_10d_slope'.format(n)] = \\\n                            ( npp_cfr_s.ewm(span = 1).mean()\\\n                                     - npp_cfr_s.ewm(span = 10).mean() ) .values\n        \n#         print(( npp_cfr_s.ewm(span = 1).mean()\\\n#                                      - npp_cfr_s.ewm(span = 10).mean() ).values)\n        \n\n# %% [code]\n\n\n# %% [code]\ndgp = dataset.groupby('Place').last()\nfor d in [5, 10, 20]:\n#     dataset['ConfirmedCases_percapita_vs_nearest{}'.format(n)] = 0\n#     dataset['Fatalities_percapita_vs_nearest{}'.format(n)] = 0\n    \n    for place in dataset.Place.unique():\n        nps = surroundingPlaces(place, d)\n        dataset.loc[dataset.Place==place, 'num_surrounding_places_{}_degrees'.format(d)] = \\\n            len(nps)\n        \n        \n        tp = dgp.loc[nps.index].TRUE_POPULATION.sum()\n        \n        dataset.loc[dataset.Place==place, \n                    'ratio_population_vs_surrounding_places_{}_degrees'.format(d)] = \\\n            np.log(dataset.loc[dataset.Place==place].TRUE_POPULATION.mean() + 1)\\\n                - np.log(tp+1)\n        \n        if len(nps)==0:\n            continue;\n            \n#         print(place)\n#         print(nps)\n#         print(tp)\n        nbps =  dataset[(dataset.Place.isin(nps.index))]\\\n                            .groupby('Date')[['ConfirmedCases', 'Fatalities']].sum()\n\n#         print(nbps)\n        nppc = (np.log( nbps.loc[dataset[dataset.Place==place].Date]\\\n                                          .fillna(0).ConfirmedCases + 1) - np.log(tp + 1))\n        nppf = (np.log( nbps.loc[dataset[dataset.Place==place].Date]\\\n                                          .fillna(0).Fatalities + 1) - np.log(tp + 1))\n#         break;\n        npp_cfr = cfr( nbps.loc[dataset[dataset.Place==place].Date]\\\n                                          .fillna(0).ConfirmedCases,\n                      nbps.loc[dataset[dataset.Place==place].Date]\\\n                                          .fillna(0).Fatalities)\n        dataset.loc[\n                (dataset.Place == place),\n                    'ConfirmedCases_percapita_vs_surrounding_places_{}_degrees'.format(d)] = \\\n            dataset[(dataset.Place == place)].ConfirmedCases_percapita \\\n                            - nppc.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'Fatalities_percapita_vs_surrounding_places_{}_degrees'.format(d)] = \\\n            dataset[(dataset.Place == place)].Fatalities_percapita \\\n                            - nppf.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'cfr_vs_surrounding_places_{}_degrees'.format(d)] = \\\n            dataset[(dataset.Place == place)].log_cfr_3d_ewm \\\n                            - npp_cfr   \n        \n        \n        dataset.loc[\n                (dataset.Place == place),\n                    'ConfirmedCases_surrounding_places_{}_degrees_percapita'.format(d)] = nppc.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'Fatalities_surrounding_places_{}_degrees_percapita'.format(d)] = nppf.values\n        dataset.loc[ \n                (dataset.Place == place),\n                    'cfr_surrounding_places_{}_degrees'.format(d)] = npp_cfr\n        \n        dataset.loc[\n                (dataset.Place == place),\n                    'ConfirmedCases_surrounding_places_{}_degrees_10d_slope'.format(d)] =   \\\n                               ( nppc.ewm(span = 1).mean() - nppc.ewm(span = 10).mean() ).values\n        dataset.loc[\n                (dataset.Place == place),\n                    'Fatalities_surrounding_places_{}_degrees_10d_slope'.format(d)] =   \\\n                               ( nppf.ewm(span = 1).mean() - nppf.ewm(span = 10).mean() ).values\n        npp_cfr_s = pd.Series(npp_cfr)\n        dataset.loc[ \n                (dataset.Place == place),\n                    'cfr_surrounding_places_{}_degrees_10d_slope'.format(d)] = \\\n                            ( npp_cfr_s.ewm(span = 1).mean()\\\n                                     - npp_cfr_s.ewm(span = 10).mean() ) .values\n        \n\n# %% [code]\n\n\n# %% [code]\nfor col in [c for c in dataset.columns if 'surrounding_places' in c and 'num_sur' not in c]:\n    dataset[col] = dataset[col].fillna(0)\n    n_col = 'num_surrounding_places_{}_degrees'.format(col.split('degrees')[0]\\\n                                                           .split('_')[-2])\n\n    print(col)\n#     print(n_col)\n    dataset[col + \"_times_num_places\"] = dataset[col] * np.sqrt(dataset[n_col])\n#     print('num_surrounding_places_{}_degrees'.format(col.split('degrees')[0][-2:-1]))\n\n# %% [code]\ndataset[dataset.Country=='US'][['Place', 'Date'] \\\n                                     + [c for c in dataset.columns if 'ratio_p' in c]]\\\n                [::50]\n\n# %% [code]\n\n\n# %% [raw]\n# dataset[dataset.Country==\"US\"].groupby('Place').last()\\\n#         [[c for c in dataset.columns if 'cfr' in c]].iloc[:10, 8:]\n\n# %% [code]\n\n\n# %% [raw]\n# dataset[dataset.Place=='USAlabama'][['Place', 'Date'] \\\n#                                      + [c for c in dataset.columns if 'places_5_degree' in c]]\\\n#                [40::5]\n\n# %% [code]\n\n\n# %% [code]\ndataset.TRUE_POPULATION\n\n# %% [code]\ndataset.TRUE_POPULATION.sum()\n\n# %% [code]\ndataset.groupby('Date').sum().TRUE_POPULATION\n\n# %% [code]\n\n\n# %% [raw]\n# dataset[dataset.ConfirmedCases>0]['log_cfr'].plot(kind='hist', bins = 250)\n\n# %% [raw]\n# dataset.log_cfr.isnull().sum()\n\n# %% [code]\ndataset['first_case_ConfirmedCases_percapita'] = \\\n       np.log(dataset.first_case_ConfirmedCases + 1) \\\n          - np.log(dataset.TRUE_POPULATION + 1)\n\ndataset['first_case_Fatalities_percapita'] = \\\n       np.log(dataset.first_case_Fatalities + 1) \\\n          - np.log(dataset.TRUE_POPULATION + 1)\n\ndataset['first_fatality_Fatalities_percapita'] = \\\n       np.log(dataset.first_fatality_Fatalities + 1) \\\n          - np.log(dataset.TRUE_POPULATION + 1)\n\ndataset['first_fatality_ConfirmedCases_percapita'] = \\\n        np.log(dataset.first_fatality_ConfirmedCases + 1)\\\n            - np.log(dataset.TRUE_POPULATION + 1)\n\n# %% [code]\n\n\n# %% [code]\n \ndataset['days_to_saturation_ConfirmedCases_4d'] = \\\n                                ( - np.log(dataset.ConfirmedCases + 1)\\\n                                        + np.log(dataset.TRUE_POPULATION + 1)) \\\n                            \/ dataset.ConfirmedCases_4d_prior_slope         \ndataset['days_to_saturation_ConfirmedCases_7d'] = \\\n                                ( - np.log(dataset.ConfirmedCases + 1)\\\n                                        + np.log(dataset.TRUE_POPULATION + 1)) \\\n                            \/ dataset.ConfirmedCases_7d_prior_slope         \n\n    \ndataset['days_to_saturation_Fatalities_20d_cases'] = \\\n                                ( - np.log(dataset.Fatalities + 1)\\\n                                        + np.log(dataset.TRUE_POPULATION + 1)) \\\n                            \/ dataset.ConfirmedCases_20d_prior_slope         \ndataset['days_to_saturation_Fatalities_12d_cases'] = \\\n                                ( - np.log(dataset.Fatalities + 1)\\\n                                        + np.log(dataset.TRUE_POPULATION + 1)) \\\n                            \/ dataset.ConfirmedCases_12d_prior_slope         \n \n\n# %% [code]\ndataset['days_to_3pct_ConfirmedCases_4d'] = \\\n                                ( - np.log(dataset.ConfirmedCases + 1)\\\n                                        + np.log(dataset.TRUE_POPULATION + 1) - 3.5) \\\n                            \/ dataset.ConfirmedCases_4d_prior_slope         \ndataset['days_to_3pct_ConfirmedCases_7d'] = \\\n                                ( - np.log(dataset.ConfirmedCases + 1)\\\n                                        + np.log(dataset.TRUE_POPULATION + 1) - 3.5) \\\n                            \/ dataset.ConfirmedCases_7d_prior_slope         \n\n    \ndataset['days_to_0.3pct_Fatalities_20d_cases'] = \\\n                                ( - np.log(dataset.Fatalities + 1)\\\n                                        + np.log(dataset.TRUE_POPULATION + 1) - 5.8) \\\n                            \/ dataset.ConfirmedCases_20d_prior_slope         \ndataset['days_to_0.3pct_Fatalities_12d_cases'] = \\\n                                ( - np.log(dataset.Fatalities + 1)\\\n                                        + np.log(dataset.TRUE_POPULATION + 1) - 5.8) \\\n                            \/ dataset.ConfirmedCases_12d_prior_slope         \n \n\n# %% [code]\n\n\n# %% [raw]\n# \n\n# %% [code]\n\n\n# %% [code]\ndataset.tail()\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Build Intervals into Future\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\ndataset = dataset[dataset.ConfirmedCases > 0]\n\nlen(dataset)\n\n# %% [code]\ndatas = []\nfor window in range(1, 35):\n    base = rollDates(dataset, window, True)\n    datas.append(pd.merge(dataset[['Date', 'Place',\n                 'ConfirmedCases', 'Fatalities']], base, on = ['Date', 'Place'],\n                          how = 'right', \n            suffixes = ('_f', '')))\ndata = pd.concat(datas, axis =0).astype(np.float32, errors ='ignore')\n\n# %% [code]\nlen(data)\n\n# %% [raw]\n# data[data.Place=='USNew York']\n\n# %% [code]\ndata['Date_f'] = data.Date\ndata.Date = data.Date_i\n\n# %% [code]\ndata['elapsed'] = (data.Date_f - data.Date_i).dt.days\n\n# %% [code]\ndata['CaseChgRate'] = (np.log(data.ConfirmedCases_f + 1) - np.log(data.ConfirmedCases + 1))\\\n                            \/ data.elapsed;\ndata['FatalityChgRate'] = (np.log(data.Fatalities_f + 1) - np.log(data.Fatalities + 1))\\\n                            \/ data.elapsed;\n\n\n# %% [code]\n\n\n# %% [code]\ndata.elapsed\n\n# %% [code]\n\n\n# %% [raw]\n# data[slope_cols]\n\n# %% [raw]\n# [c for c in data.columns if any(z in c for z in [ 'rate']) ]\n\n# %% [code]\nfalloff_hash = {}\n\n# %% [code]\n\n\n# %% [code]\ndef true_agg(rate_i, elapsed, bend_rate):\n#     print(elapsed); \n    elapsed = int(elapsed)\n#     ar = 0\n#     rate = rate_i\n#     for i in range(0, elapsed):\n#         rate *= bend_rate\n#         ar += rate\n#     return ar\n\n    if (bend_rate, elapsed) not in falloff_hash:\n        falloff_hash[(bend_rate, elapsed)] = \\\n            np.sum( [  np.power(bend_rate, e) for e in range(1, elapsed+1)] )\n    return falloff_hash[(bend_rate, elapsed)] * rate_i\n     \n\n# %% [code]\ntrue_agg(0.3, 30, 0.9)\n\n# %% [raw]\n# %timeit true_agg(0.3, 30, 0.9)\n\n# %% [code]\nslope_cols = [c for c in data.columns if \n                      any(z in c for z in ['prior_slope', 'chg', 'rate'])\n           and not any(z in c for z in ['bend', 'prior_slope_chg', 'Country', 'ewm', \n                                        ]) ] # ** bid change; since rate too stationary\nprint(slope_cols)\nbend_rates = [1, 0.95, 0.90]\nfor bend_rate in bend_rates:\n    bend_agg = data[['elapsed']].apply(lambda x: true_agg(1, *x, bend_rate), axis=1)\n     \n    for sc in slope_cols:\n        if bend_rate < 1:\n            data[sc+\"_slope_bend_{}\".format(bend_rate)] =  data[sc]  \\\n                                    * np.power((bend_rate + 1)\/2, data.elapsed)\n         \n            data[sc+\"_true_slope_bend_{}\".format(bend_rate)] = \\\n                          bend_agg *  data[sc] \/ data.elapsed\n            \n        data[sc+\"_agg_bend_{}\".format(bend_rate)] =  data[sc] * data.elapsed \\\n                                * np.power((bend_rate + 1)\/2, data.elapsed)\n         \n        data[sc+\"_true_agg_bend_{}\".format(bend_rate)] = \\\n                        bend_agg *  data[sc]\n#                       data[[sc, 'elapsed']].apply(lambda x: true_agg(*x, bend_rate), axis=1) \n        \n         \n#         print(data[sc+\"_true_agg_bend_{}\".format(bend_rate)])\n\n# %% [raw]\n# data[[c for c in data.columns if 'Fatalities_7d_prior_slope' in c and 'true_agg' in c]]\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [raw]\n# data[data.Place=='USNew York'][['elapsed'] +[c for c in data.columns if 'ses_4d_prior_slope' in c]]\n\n# %% [code]\nslope_cols[:5]\n\n# %% [raw]\n# data\n\n# %% [code]\nfor col in [c for c in data.columns if any(z in c for z in \n                               ['vs_continent', 'nearest', 'vs_world', 'surrounding_places'])]:\n#     print(col)\n    data[col + '_times_days'] = data[col] * data.elapsed\n\n# %% [code]\ndata['saturation_slope_ConfirmedCases'] = (- np.log(data.ConfirmedCases + 1)\\\n                                                        + np.log(data.TRUE_POPULATION + 1)) \\\n                                                    \/ data.elapsed\ndata['saturation_slope_Fatalities'] = (- np.log(data.Fatalities + 1)\\\n                                                + np.log(data.TRUE_POPULATION + 1)) \\\n                                                    \/ data.elapsed\n\ndata['dist_to_ConfirmedCases_saturation_times_days'] = (- np.log(data.ConfirmedCases + 1)\\\n                                                        + np.log(data.TRUE_POPULATION + 1)) \\\n                                                    * data.elapsed\ndata['dist_to_Fatalities_saturation_times_days'] = (- np.log(data.Fatalities + 1)\\\n                                                + np.log(data.TRUE_POPULATION + 1)) \\\n                                                    * data.elapsed\n        \n\n\ndata['slope_to_1pct_ConfirmedCases'] = (- np.log(data.ConfirmedCases + 1)\\\n                                                        + np.log(data.TRUE_POPULATION + 1) - 4.6) \\\n                                                    \/ data.elapsed\ndata['slope_to_0.1pct_Fatalities'] = (- np.log(data.Fatalities + 1)\\\n                                                + np.log(data.TRUE_POPULATION + 1) - 6.9) \\\n                                                    \/ data.elapsed\n\ndata['dist_to_1pct_ConfirmedCases_times_days'] = (- np.log(data.ConfirmedCases + 1)\\\n                                                        + np.log(data.TRUE_POPULATION + 1) - 4.6) \\\n                                                    * data.elapsed\ndata['dist_to_0.1pct_Fatalities_times_days'] = (- np.log(data.Fatalities + 1)\\\n                                                + np.log(data.TRUE_POPULATION + 1) - 6.9) \\\n                                                    * data.elapsed\n\n# %% [raw]\n# data.ConfirmedCases_12d_prior_slope.plot(kind='hist')\n\n# %% [code]\ndata['trendline_per_capita_ConfirmedCases_4d_slope'] = ( np.log(data.ConfirmedCases + 1)\\\n                                                        - np.log(data.TRUE_POPULATION + 1)) \\\n                                       + (data.ConfirmedCases_4d_prior_slope * data.elapsed)\ndata['trendline_per_capita_ConfirmedCases_7d_slope'] = ( np.log(data.ConfirmedCases + 1)\\\n                                                        - np.log(data.TRUE_POPULATION + 1)) \\\n                                       + (data.ConfirmedCases_7d_prior_slope * data.elapsed)\n \n\ndata['trendline_per_capita_Fatalities_12d_slope'] = ( np.log(data.Fatalities + 1)\\\n                                                        - np.log(data.TRUE_POPULATION + 1)) \\\n                                       + (data.ConfirmedCases_12d_prior_slope * data.elapsed)\ndata['trendline_per_capita_Fatalities_20d_slope'] = ( np.log(data.Fatalities + 1)\\\n                                                        - np.log(data.TRUE_POPULATION + 1)) \\\n                                       + (data.ConfirmedCases_20d_prior_slope * data.elapsed)\n\n \n\n# %% [code]\n\n\n# %% [raw]\n# data[data.Place == 'USNew York']\n\n# %% [code]\nlen(data)\n\n# %% [raw]\n# data.CaseChgRate.plot(kind='hist', bins = 250);\n\n# %% [code]\n\n\n# %% [raw]\n# data_bk = data.copy()\n\n# %% [code]\n\n\n# %% [code]\ndata.groupby('Place').last()\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [raw]\n# # data['log_days_since_first_case'] =  np.log(data.days_since_first_case + 1)\n# # data['log_days_since_first_fatality'] = np.log(data.days_since_first_fatality + 1)\n# \n# data['sqrt_days_since_first_case'] = np.sqrt(data.days_since_first_case)\n# data['sqrt_days_since_first_fatality'] = np.sqrt(data.days_since_first_fatality)\n# \n# \n# \n\n \n# %% [code]\ndef logHist(x, b = 150):\n    return\n\n# %% [raw]\n# np.std(x.log_cases)\n\n# %% [raw]\n# np.std(x.log_fatalities)\n\n# %% [code]\n\n\n# %% [code]\ndata['log_fatalities'] = np.log(data.Fatalities + 1) #  + 0.4 * np.random.normal(0, 1, len(data))\ndata['log_cases'] = np.log(data.ConfirmedCases + 1) # + 0.2 *np.random.normal(0, 1, len(data))\n\n\n\n# %% [raw]\n# data.log_cases.plot(kind='hist', bins = 250)\n\n# %% [code]\ndata['is_China'] = (data.Country=='China') & (~data.Place.isin(['Hong Kong', 'Macau']))\n\n# %% [code]\nfor col in [c for c in data.columns if 'd_ewm' in c]:\n    data[col] += np.random.normal(0, 1, len(data)) * np.std(data[col]) * 0.2\n    \n\n# %% [raw]\n# data[data.log_cfr>-11].log_fatalities.plot(kind='hist', bins = 150)\n\n# %% [code]\ndata['is_province'] = 1.0* (~data.Province_State.isnull() )\n\n# %% [code]\ndata['log_elapsed'] = np.log(data.elapsed + 1)\n\n# %% [code]\ndata.columns\n\n# %% [code]\ndata.columns[::19]\n\n# %% [code]\ndata.shape\n\n# %% [code]\nlogHist(data.ConfirmedCases)\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Data Cleanup\n\n# %% [code]\ndata.drop(columns = ['TRUE_POPULATION'], inplace=True)\n\n# %% [code]\ndata['final_day_of_week'] = data.Date_f.apply(datetime.datetime.weekday)\n\n# %% [code]\ndata['base_date_day_of_week'] = data.Date.apply(datetime.datetime.weekday)\n\n# %% [code]\ndata['date_difference_modulo_7_days'] = (data.Date_f - data.Date).dt.days % 7\n\n# %% [raw]\n# for c in data.columns.to_list():\n#     if 'days_since' in c:\n#         data[c] = np.log(data[c]+1)\n\n# %% [code]\n\n\n# %% [code]\nfor c in data.columns.to_list():\n    if 'days_to' in c:\n#         print(c)\n        data[c] = data[c].where(~np.isinf(data[c]), 1e3)\n        data[c] = np.clip(data[c], 0, 365)\n        data[c] = np.sqrt(data[c])\n\n\n        \n        \nnew_places = train[(train.Date == test.Date.min() - datetime.timedelta(1)) &\n      (train.ConfirmedCases == 0)\n     ].Place\n\n        \n        \n        # %% [code]\n\n\n# %% [markdown]\n# ## II. Modeling\n\n# %% [markdown]\n# ### Data Prep\n\n# %% [code]\nmodel_data = data[ (( len(test) ==0 ) | (data.Date_f < test.Date.min()))\n                  & \n                  (data.ConfirmedCases > 0) &\n                 (~data.ConfirmedCases_f.isnull())].copy()\n\n# %% [raw]\n# data.Date_f\n\n# %% [code]\ntest.Date.min()\n\n# %% [code]\nmodel_data.Date_f.max()\n\n# %% [code]\nmodel_data.Date_f.max()\n\n# %% [code]\nmodel_data.Date.max()\n\n# %% [code]\nmodel_data.Date_f.min()\n\n# %% [code]\n\n\n# %% [code]\nmodel_data = model_data[~( \n                            ( np.random.rand(len(model_data)) < 0.8 )  &\n                          ( model_data.Country == 'China') &\n                              (model_data.Date < datetime.datetime(2020, 2, 15)) )]\n\n# %% [code]\nx_dates = model_data[['Date_i', 'Date_f', 'Place']]\n\n# %% [code]\nx = model_data[    \n    model_data.columns.to_list()[\n            model_data.columns.to_list().index('ConfirmedCases_1d_prior_value'):]]\\\n            .drop(columns = ['Date_i', 'Date_f', 'CaseChgRate', 'FatalityChgRate'])\n\n# %% [raw]\n# x.columns\n\n# %% [raw]\n# x\n\n\n\n\ntest.Date\n\n# %% [code]\nif PRIVATE:\n    data_test = data[ (data.Date_i == train.Date.max() ) & \n                     (data.Date_f.isin(test.Date.unique() ) ) ].copy()\nelse:\n    data_test = data[ (data.Date_i == test.Date.min() - datetime.timedelta(1) ) & \n                     (data.Date_f.isin(test.Date.unique() ) ) ].copy()\n\n# %% [code]\ndata_test.Date.unique()\n\n# %% [code]\ntest.Date.unique()\n\n# %% [raw]\n# data_test.Date_f\n\n# %% [code]\nx_test =  data_test[x.columns].copy()\n\n# %% [code]\ntrain.Date.max()\n\n# %% [code]\ntest.Date.max()\n\n# %% [raw]\n# data_test[data_test.Place=='San Marino'].Date_f\n\n# %% [raw]\n# data_test.groupby('Place').Date_f.count().sort_values()\n\n# %% [raw]\n# x_test\n\n# %% [code]\n\n\n# %% [raw]\n# x.columns\n\n# %% [code]\n\n\n# %% [code]\nif MODEL_Y is 'slope':\n    y_cases = model_data.CaseChgRate \n    y_fatalities = model_data.FatalityChgRate \nelse:\n    y_cases = model_data.CaseChgRate * model_data.elapsed\n    y_fatalities = model_data.FatalityChgRate * model_data.elapsed\n    \ny_cfr = np.log(    (model_data.Fatalities_f \\\n                                         + np.clip(0.015 * model_data.ConfirmedCases_f, 0, 0.3)) \\\n                            \/ ( model_data.ConfirmedCases_f + 0.1) )\n\n# %% [code]\ngroups = model_data.Country\nplaces = model_data.Place\n\n# %% [raw]\n# y_cfr\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Model Setup\n\n# %% [code]\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GroupKFold, GroupShuffleSplit, PredefinedSplit\nfrom sklearn.model_selection import ParameterSampler\nfrom sklearn.metrics import make_scorer\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import HuberRegressor, ElasticNet\nimport lightgbm as lgb\n\n\n# %% [code]\nnp.random.seed(SEED)\n\n# %% [code]\nenet_params = { 'alpha': [   3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3,  ],\n                'l1_ratio': [  0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 0.8, 0.9, 0.97, 0.99 ]}\n\n# %% [code]\net_params = {        'n_estimators': [50, 70, 100, 140],\n                    'max_depth': [3, 5, 7, 8, 9, 10],\n                      'min_samples_leaf': [30, 50, 70, 100, 130, 165, 200, 300, 600],\n                     'max_features': [0.4, 0.5, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85],\n                    'min_impurity_decrease': [0, 1e-5 ], #1e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2],\n                    'bootstrap': [ True, False], # False is clearly worse          \n                 #   'criterion': ['mae'],\n                   }\n\n# %% [code]\nlgb_params = {\n                'max_depth': [5, 12],\n                'n_estimators': [ 100, 200, 300, 500],   # continuous\n                'min_split_gain': [0, 0, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2],\n                'min_child_samples': [ 7, 10, 14, 20, 30, 40, 70, 100, 200, 400, 700, 1000, 2000],\n                'min_child_weight': [0], #, 1e-3],\n                'num_leaves': [5, 10, 20, 30],\n                'learning_rate': [0.05, 0.07, 0.1],   #, 0.1],       \n                'colsample_bytree': [0.1, 0.2, 0.33, 0.5, 0.65, 0.8, 0.9], \n                'colsample_bynode':[0.1, 0.2, 0.33, 0.5, 0.65, 0.81],\n                'reg_lambda': [1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000,   ],\n                'reg_alpha': [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 30, 1000,], # 1, 10, 100, 1000, 10000],\n                'subsample': [  0.8, 0.9, 1],\n                'subsample_freq': [1],\n                'max_bin': [ 7, 15, 31, 63, 127, 255],\n  #               'extra_trees': [True, False],\n#                 'boosting': ['gbdt', 'dart'],\n    #     'subsample_for_bin': [200000, 500000],\n               }    \n\n# %% [code]\nMSE = 'neg_mean_squared_error'\nMAE = 'neg_mean_absolute_error'\n\n# %% [code]\n\n\n# %% [code]\ndef trainENet(x, y, groups, cv = 0, **kwargs):\n    return trainModel(x, y, groups, \n                      clf = ElasticNet(normalize = True, selection = 'random', \n                                       max_iter = 3000),\n                      params = enet_params, \n                      cv = cv, **kwargs)\n\n# %% [code]\ndef trainETR(x, y, groups, cv = 0, n_jobs = 5,  **kwargs):\n    clf = ExtraTreesRegressor(n_jobs = 1)\n    params = et_params\n    return trainModel(x, y, groups, clf, params, cv, n_jobs, **kwargs)\n\n# %% [code]\ndef trainLGB(x, y, groups, cv = 0, n_jobs = 4, **kwargs):\n    clf = lgb.LGBMRegressor(verbosity=-1, hist_pool_size = 1000,  \n                      )\n    params = lgb_params\n    \n    return trainModel(x, y, groups, clf, params, cv, n_jobs,  **kwargs)\n\n# %% [code]\ndef trainModel(x, y, groups, clf, params, cv = 0, n_jobs = None, \n                   verbose=0, splits=None, **kwargs):\n#     if cv is 0:\n#         param_sets = list(ParameterSampler(params, n_iter=1))\n#         clf = clf.set_params(**param_sets[0] )\n#         if n_jobs is not None:\n#             clf = clf.set_params(** {'n_jobs': n_jobs } )\n#         f = clf.fit(x, y)\n#         return clf \n#     else:\n        if n_jobs is None:\n            n_jobs = 4\n        if np.random.rand() < 0.8: # all shuffle, don't want overfit models, just reasonable\n            folds = GroupShuffleSplit(n_splits=4, \n                                                   test_size= 0.2 + 0.10 * np.random.rand())\n        else:\n            folds = GroupKFold(4)\n        clf = RandomizedSearchCV(clf, params, \n                            cv=  folds, \n#                                  cv = GroupKFold(4),\n                                 n_iter=12, \n                                verbose = 0, n_jobs = n_jobs, scoring = MSE)\n        f = clf.fit(x, y, groups)\n        #if verbose > 0:\n        print(pd.DataFrame(clf.cv_results_['mean_test_score'])); print();  \n     #   print(pd.DataFrame(clf.cv_results_).to_string()); print();  \n        \n        \n        best = clf.best_estimator_;  print(best)\n        print(\"Best Score: {}\".format(np.round(clf.best_score_,4)))\n        \n        return best\n\n# %% [code] {\"scrolled\":true}\nnp.mean(y_cases)\n\n# %% [code]\n\n\n# %% [code]\ndef getSparseColumns(x, verbose = 0):\n    sc = []\n    for c in x.columns.to_list():\n        u = len(x[c].unique())\n        if u > 10 and u < 0.01*len(x) :\n            sc.append(c)\n            if verbose > 0:\n                print(\"{}: {}\".format(c, u))\n\n    return sc\n\n# %% [code]\ndef noisify(x, noise = 0.1):\n    x = x.copy()\n   # cols = x.columns.to_list()\n    cols = getSparseColumns(x)\n    for c in cols:\n        u = len(x[c].unique())\n        if u > 50:\n            x[c].values[:] = x[c].values + np.random.normal(0, noise, len(x)) * np.std(x[c])\n    return x;\n\n# %% [raw]\n# cols = getSparseColumns(x)\n# for c in cols:\n#     u = len(x[c].unique())\n#     if u > 50:\n#         print(\"{}: {}\".format(c, u)) #x[c].values[:] = x[c].values + np.random.normal(0, noise, len(x)) * np.std(x[c])\n# # return x;\n\n# %% [raw]\n# [c for c in x.columns if any(z in c for z in \n#                                  ['prior_slope', 'prior_value'])]\n\n# %% [raw]\n# getSparseColumns(x, verbose = 0)\n\n# %% [raw]\n# x.columns[::19]\n\n# %% [code]\ndef getMaxOverlap(row, df):\n#     max_overlap_frac = 0\n\n    df_place = df[df.Place == row.Place]\n    if len(df_place)==0:\n        return 0\n#     print(df_place)\n    overlap = \\\n        (np.clip( df_place.Date_f, None, row.Date_f) \\\n                            - np.clip( df_place.Date_i, row.Date_i, None) ).dt.days\n    overlap = np.clip(overlap, 0, None)\n    length = np.clip(  (df_place.Date_f - df_place.Date_i).dt.days, \n                        (row.Date_f - row.Date_i).days,  None)\n#     print(overlap)\n#     print(length)\n#     print(overlap)\n#     print(length)\n    return np.amax(overlap \/ length) \n#     print(row)\n#     print(df_place)\n#     return\n    \n#     for i in range(0, len(df_place)):\n#         selected = df_place.iloc[i]\n#        # if row.Place == selected.Place:\n#         overlap = (np.min((row.Date_f, selected.Date_f))\\\n#                      - np.max((row.Date_i, selected.Date_i )) ).days\n#         overlap_frac = overlap \/ (selected.Date_f - selected.Date_i).days \n#         if overlap_frac > max_overlap_frac:\n#             max_overlap_frac = overlap_frac\n#     return max_overlap_frac\n     \n\n# %% [raw]\n# \n\n# %% [code]\ndef getSampleWeight(x, groups):\n \n    \n    counter = Counter(groups)\n    median_count = np.median( [counter[group] for group in groups.unique()])\n#     print(median_count)\n    c_count = [counter[group] for group in groups]\n    \n    e_decay = np.round(LT_DECAY_MIN + np.random.rand() * ( LT_DECAY_MAX - LT_DECAY_MIN), 1) \n    print(\"LT weight decay: {:.2f}\".format(e_decay));\n    ssr =  np.power(  1 \/ np.clip( c_count \/ median_count , 0.1,  30) , \n                        0.1 + np.random.rand() * 0.6) \\\n                \/   np.power(x.elapsed \/ 3, e_decay) \\\n                    *  SET_FRAC * np.exp(  -    np.random.rand()  )\n    \n#     print(np.power(  1 \/ np.clip( c_count \/ median_count , 1,  10) , \n#                         0.1 + np.random.rand() * 0.3))\n#     print(np.power(x.elapsed \/ 3, e_decay))\n#     print(np.exp(  1.5 * (np.random.rand() - 0.5) ))\n        \n    # drop % of groups at random\n    group_drop = dict([(group, np.random.rand() < 0.15) for group in groups.unique()])\n    ssr = ssr * (  [ 1 -group_drop[group] for group in groups])\n#     print(ssr[::171])\n#     print(np.array([ 1 -group_drop[group] for group in groups]).sum() \/ len(groups))\n\n#     pd.Series(ssr).plot(kind='hist', bins = 100)\n    return ssr;\n\n# %% [raw]\n# group_drop = dict([(group, np.random.rand() < 0.20) for group in groups.unique()])\n#      \n# np.array([ 1 -group_drop[group] for group in groups]).sum() \/ len(groups)\n# \n\n# %% [raw]\n# [c for c in x.columns if 'continent' in c]\n\n# %% [raw]\n# x.columns[::10]\n\n# %% [raw]\n# x.shape\n\n# %% [raw]\n# contain_data.columns\n\n# %% [code]\ndef runBags(x, y, groups, cv, bags = 3, model_type = trainLGB, \n            noise = 0.1, splits = None, weights = None, **kwargs):\n    models = []\n    for bag in range(bags):\n        print(\"\\nBAG {}\".format(bag+1))\n        \n        x = x.copy()  # copy X to modify it with noise\n        \n        if DROPS:\n            # drop 0-70% of the bend\/slope\/prior features, just for speed and model diversity\n            for col in [c for c in x.columns if any(z in c for z in ['bend', 'slope', 'prior'])]:\n                if np.random.rand() < np.sqrt(np.random.rand()) * 0.7:\n                    x[col].values[:] = 0\n            \n        # 00% of the time drop all 'rate_since' features \n#         if np.random.rand() < 0.00:\n#             print('dropping rate_since features')\n#             for col in [c for c in x.columns if 'rate_since' in c]:    \n#                 x[col].values[:] = 0\n        \n        # 20% of the time drop all 'world' features \n#         if np.random.rand() < 0.00:\n#             print('dropping world features')\n#             for col in [c for c in x.columns if 'world' in c]:    \n#                 x[col].values[:] = 0\n        \n        # % of the time drop all 'nearest' features \n        if DROPS and (np.random.rand() < 0.30):\n            print('dropping nearest features')\n            for col in [c for c in x.columns if 'nearest' in c]:    \n                x[col].values[:] = 0\n        \n        #  % of the time drop all 'surrounding_places' features \n        if DROPS and (np.random.rand() < 0.25):\n            print('dropping \\'surrounding places\\' features')\n            for col in [c for c in x.columns if 'surrounding_places' in c]:    \n                x[col].values[:] = 0\n        \n        \n        # 20% of the time drop all 'continent' features \n#         if np.random.rand() < 0.20:\n#             print('dropping continent features')\n#             for col in [c for c in x.columns if 'continent' in c]:    \n#                 x[col].values[:] = 0\n        \n        # drop 0-50% of all features\n#         if DROPS:\n        col_drop_frac = np.sqrt(np.random.rand()) * 0.5\n        for col in [c for c in x.columns if 'elapsed' not in c ]:\n            if np.random.rand() < col_drop_frac:\n                x[col].values[:] = 0\n\n        \n        x = noisify(x, noise)\n        \n        \n        if DROPS and (np.random.rand() < SUP_DROP):\n            print(\"Dropping supplemental country data\")\n            for col in x[[c for c in x.columns if c in sup_data.columns]]:  \n                x[col].values[:] = 0\n                \n        if DROPS and (np.random.rand() < ACTIONS_DROP): \n            for col in x[[c for c in x.columns if c in contain_data.columns]]:  \n                x[col].values[:] = 0\n#             print(x.StringencyIndex_20d_ewm[::157])\n        else:\n            print(\"*using containment data\")\n            \n        if np.random.rand() < 0.6: \n            x.S_data_days = 0\n            \n        ssr = getSampleWeight(x, groups)\n        \n        date_falloff = 0 + (1\/30) * np.random.rand()\n        if weights is not None:\n            ssr = ssr * np.exp(-weights * date_falloff)\n        \n        ss = ( np.random.rand(len(y)) < ssr  )\n        print(\"n={}\".format(len(x[ss])))\n        \n        p1 =x.elapsed[ss].plot(kind='hist', bins = int(x.elapsed.max() - x.elapsed.min() + 1))\n        p1 = plt.figure();\n#         break\n#        print(Counter(groups[ss]))\n        print((ss).sum())\n        models.append(model_type(x[ss], y[ss], groups[ss], cv,   **kwargs))\n    return models\n\n# %% [code]\nx = x.astype(np.float32)\n\n# %% [raw]\n# x.elapsed\n\n# %% [code]\n\n\n# %% [code]\nBAG_MULT = 1\n\n# %% [code]\n\n\n# %% [code]\nx.shape\n\n# %% [code]\n\n\n# %% [code]\nlgb_c_clfs = []; lgb_c_noise = []\n\n# %% [code] {\"scrolled\":true}\ndate_weights =  np.abs((model_data.Date_f - test.Date.min()).dt.days) \n\n# %% [code]\nfor iteration in range(0, int(math.ceil(1.1 * BAGS))):\n    for noise in [ 0.05, 0.1, 0.2, 0.3, 0.4  ]:\n        print(\"\\n---\\n\\nNoise of {}\".format(noise));\n        num_bags = 1 * BAG_MULT;\n        if np.random.rand() < PLACE_FRACTION:\n            cv_group = places\n            print(\"CV by Place\")\n        else:\n            cv_group = groups\n            print(\"CV by Country\")\n             \n        \n        lgb_c_clfs.extend(runBags(x, y_cases, \n                          cv_group, #groups\n                          MSE, num_bags, trainLGB, verbose = 0, \n                                          noise = noise, weights = date_weights\n\n                                 ))\n        lgb_c_noise.extend([noise] * num_bags)\n        if SINGLE_MODEL:\n            break;\n\n# %% [code]\n\n\n# %% [raw]\n# np.isinf(x).sum().sort_values()\n\n# %% [code]\n\n\n# %% [raw]\n# enet_c_clfs = runBags(x, y_cases, groups, MSE, 1, trainENet, verbose = 1)\n\n# %% [code]\n\n\n# %% [code]\nlgb_f_clfs = []; lgb_f_noise = []\n\n# %% [code]\nfor iteration in range(0, int(np.ceil(np.sqrt(BAGS)))):\n    for noise in [  0.5,  1, 2, 3,  ]:\n        print(\"\\n---\\n\\nNoise of {}\".format(noise));\n        num_bags = 1 * int(np.ceil(np.sqrt(BAG_MULT)))\n        if np.random.rand() < PLACE_FRACTION  :\n            cv_group = places\n            print(\"CV by Place\")\n        else:\n            cv_group = groups\n            print(\"CV by Country\")\n            \n   \n        lgb_f_clfs.extend(runBags(x, y_fatalities, \n                                  cv_group, #places, # groups, \n                                  MSE, num_bags, trainLGB, \n                                  verbose = 0, noise = noise,\n                                  weights = date_weights\n                                 ))\n        lgb_f_noise.extend([noise] * num_bags)\n        if SINGLE_MODEL:\n            break;\n\n# %% [raw]\n# lgb_f_noise = lgb_f_noise[0:3]\n# lgb_f_clfs = lgb_f_clfs[0:3]\n\n# %% [raw]\n# lgb_f_noise = lgb_f_noise[2:]\n# lgb_f_clfs = lgb_f_clfs[2:]\n\n# %% [raw]\n# et_f_clfs = runBags(x, y_fatalities, groups, MSE, 1, trainETR, verbose = 1)\n# \n# \n\n# %% [raw]\n# enet_f_clfs = runBags(x, y_fatalities, groups, MSE, 1, trainENet, verbose = 1)\n# \n# \n\n# %% [raw]\n# y_cfr.plot(kind='hist', bins = 250)\n\n# %% [code]\nlgb_cfr_clfs = []; lgb_cfr_noise = [];\n\n# %% [code]\nfor iteration in range(0, int(np.ceil(np.sqrt(BAGS)))):\n    for noise in [    0.4, 1, 2, 3]:\n        print(\"\\n---\\n\\nNoise of {}\".format(noise));\n        num_bags = 1 * BAG_MULT;\n        if np.random.rand() < 0.5 * PLACE_FRACTION :\n            cv_group = places\n            print(\"CV by Place\")\n        else:\n            cv_group = groups\n            print(\"CV by Country\")\n \n        lgb_cfr_clfs.extend(runBags(x, y_cfr, \n                          cv_group, #groups\n                          MSE, num_bags, trainLGB, verbose = 0, \n                                          noise = noise, \n                                          weights = date_weights\n\n                                 ))\n        lgb_cfr_noise.extend([noise] * num_bags)\n        if SINGLE_MODEL:\n            break;\n\n# %% [raw]\n# x_test\n\n# %% [code]\nlgb_cfr_clfs[0].predict(x_test)\n\n# %% [raw]\n# \n\n# %% [code]\n# full sample, through 03\/28 (avail on 3\/30), lgb only: 0.0097 \/ 0.0036;   0.0092 \/ 0.0042\n#                                                       \n\n# %% [markdown]\n# ##### Feature Importance\n\n# %% [code]\ndef show_FI(model, featNames, featCount):\n   # show_FI_plot(model.feature_importances_, featNames, featCount)\n    fis = model.feature_importances_\n    fig, ax = plt.subplots(figsize=(6, 5))\n    indices = np.argsort(fis)[::-1][:featCount]\n    g = sns.barplot(y=featNames[indices][:featCount],\n                    x = fis[indices][:featCount] , orient='h' )\n    g.set_xlabel(\"Relative importance\")\n    g.set_ylabel(\"Features\")\n    g.tick_params(labelsize=12)\n    g.set_title( \" feature importance\")\n    \n\n# %% [code]\ndef avg_FI(all_clfs, featNames, featCount):\n    # 1. Sum\n    clfs = []\n    for clf_set in all_clfs:\n        for clf in clf_set:\n            clfs.append(clf);\n    print(\"{} classifiers\".format(len(clfs)))\n    fi = np.zeros( (len(clfs), len(clfs[0].feature_importances_)) )\n    for idx, clf in enumerate(clfs):\n        fi[idx, :] = clf.feature_importances_\n    avg_fi = np.mean(fi, axis = 0)\n\n    # 2. Plot\n    fis = avg_fi\n    fig, ax = plt.subplots(figsize=(6, 5))\n    indices = np.argsort(fis)[::-1]#[:featCount]\n    #print(indices)\n    g = sns.barplot(y=featNames[indices][:featCount],\n                    x = fis[indices][:featCount] , orient='h' )\n    g.set_xlabel(\"Relative importance\")\n    g.set_ylabel(\"Features\")\n    g.tick_params(labelsize=12)\n    g.set_title( \" feature importance\")\n    \n    return pd.Series(fis[indices], featNames[indices])\n\n# %% [code]\n\ndef linear_FI_plot(fi, featNames, featCount):\n   # show_FI_plot(model.feature_importances_, featNames, featCount)\n    fig, ax = plt.subplots(figsize=(6, 5))\n    indices = np.argsort(np.absolute(fi))[::-1]#[:featCount]\n    g = sns.barplot(y=featNames[indices][:featCount],\n                    x = fi[indices][:featCount] , orient='h' )\n    g.set_xlabel(\"Relative importance\")\n    g.set_ylabel(\"Features\")\n    g.tick_params(labelsize=12)\n    g.set_title( \" feature importance\")\n    return pd.Series(fi[indices], featNames[indices])\n\n# %% [code]\n\n\n# %% [raw]\n# fi_list = []\n# for clf in enet_c_clfs:\n#     fi = clf.coef_ * np.std(x, axis=0).values \n#     fi_list.append(fi)\n# fis = np.mean(np.array(fi_list), axis = 0)\n# fis = linear_FI_plot(fis, x.columns.values,25)\n\n# %% [raw]\n# lgb_c_clfs\n\n# %% [code]\nf = avg_FI([lgb_c_clfs], x.columns, 25)\n\n# %% [code]\nfor feat in ['bend', 'capita', 'cfr', 'slope', 'since', 'chg', 'ersonal', \n             'world', 'continent', 'nearest', 'surrounding']:\n    print(\"{}: {:.2f}\".format(feat, f.filter(like=feat).sum() \/ f.sum()))\n\n# %% [code]\nf[:100:3]\n\n# %% [code]\nprint(\"{}: {:.2f}\".format('sup_data', \n                       f[[c for c in f.index if c in sup_data.columns]].sum() \/ f.sum()))\nprint(\"{}: {:.2f}\".format('contain_data', \n                   f[[c for c in f.index if c in contain_data.columns]].sum() \/ f.sum()))\n\n# %% [raw]\n# I used a very simple Week 2 model like many. For Week 3:\n# \n# The right target is total change in the logged counts. This exactly mirrors the final evaluation metric, and using change rather than raw logged countes keeps it stationary.\n# \n# These can be put into a regressor for all windows from 1-30 days; ideally lightgbm or xgboost.  \n# \n# Cross-validation works well by place (country-level struggles to understand China's magic numbers; time-series would be too 1-2 week centric and couldn't be done for a full month). Each place is it's own outbreak so this works reasonably well.\n# \n# Feature Importance:\n# ~30-40%: current and past outbreak information (slopes and rates calculated *many* ways)\n# ~20-30%: nearby outbreak information, e.g. per capita rates vs. nearest 5, 10, 20 regions or within a specified latitude and longitude range--indicates not just spread but propensity to be tracking and reporting, severity, gov't management, likelihood of flattening, etc.\n# ~10-20%: place attributes (average age, personality, tfr, percent in largest city, etc)\n# ~10%: comparisons with world or continent, typically per capita prevalance compared with world or continent figures\n# ~5%: containment actions taken \n# ~5%: other \n# \n# The models started to get good once I put in world and continent and then proximity information--this 'state of the world' information gives a clue to where the country is compared to others and its likely pace that may mirror recent trends for similar countries. \n# \n# It might be possible to get better 1-10 day figures with time series models, but a lot of the error is in long-term drift, so these 1-30 day interval total aggregate change models are best suited to the competition overall.\n# \n# \n\n# %% [code]\nf = avg_FI([lgb_f_clfs], x.columns, 25)\n\n# %% [code]\nfor feat in ['bend', 'capita', 'cfr', 'slope', 'since', 'chg', 'ersonal', \n            'world', 'continent', 'nearest', 'surrounding']:\n    print(\"{}: {:.2f}\".format(feat, f.filter(like=feat).sum() \/ f.sum()))\n\n# %% [code]\nprint(\"{}: {:.2f}\".format('sup_data', \n                       f[[c for c in f.index if c in sup_data.columns]].sum() \/ f.sum()))\nprint(\"{}: {:.2f}\".format('contain_data', \n                   f[[c for c in f.index if c in contain_data.columns]].sum() \/ f.sum()))\n\n\n# %% [raw]\n# x.days_since_Stringency_1.plot(kind='hist', bins = 100)\n\n# %% [raw]\n# len(x.log_fatalities.unique())\n\n# %% [code]\nf = avg_FI([lgb_cfr_clfs], x.columns, 25)\n\n# %% [code]\nfor feat in ['bend', 'capita', 'cfr', 'slope', 'since', 'chg', 'ersonal', \n            'world', 'continent', 'nearest', 'surrounding']:\n    print(\"{}: {:.2f}\".format(feat, f.filter(like=feat).sum() \/ f.sum()))\n\n# %% [code]\nprint(\"{}: {:.2f}\".format('sup_data', \n                       f[[c for c in f.index if c in sup_data.columns]].sum() \/ f.sum()))\nprint(\"{}: {:.2f}\".format('contain_data', \n                   f[[c for c in f.index if c in contain_data.columns]].sum() \/ f.sum()))\n\n\n\n\n# %% [code]\nall_c_clfs = [lgb_c_clfs, ]#  enet_c_clfs]\nall_f_clfs = [lgb_f_clfs] #, enet_f_clfs]\nall_cfr_clfs = [lgb_cfr_clfs]\n\n\n# %% [code]\nall_c_noise = [lgb_c_noise]\nall_f_noise = [lgb_f_noise]\nall_cfr_noise = [lgb_cfr_noise]\n\n# %% [code]\n\n\n# %% [code]\nNUM_TEST_RUNS = 1\n\n# %% [code]\nc_preds = np.zeros((NUM_TEST_RUNS * sum([len(x) for x in all_c_clfs]), len(x_test)))\nf_preds = np.zeros((NUM_TEST_RUNS * sum([len(x) for x in all_f_clfs]), len(x_test)))\ncfr_preds = np.zeros((NUM_TEST_RUNS * sum([len(x) for x in all_cfr_clfs]), len(x_test)))\n\n\n# %% [code]\ndef avg(x):\n    return (np.mean(x, axis=0) + np.median(x, axis=0))\/2\n\n# %% [code]\ncount = 0\n\nfor idx, clf in enumerate(lgb_c_clfs):\n    for i in range(0, NUM_TEST_RUNS):\n        noise = lgb_c_noise[idx]\n        c_preds[count,:] = np.clip(clf.predict(noisify(x_test, noise)), -1 , 10)\n        count += 1\n#y_cases_pred_blended_full = avg(c_preds)\n\n# %% [code]\ncount = 0\n\nfor idx, clf in enumerate(lgb_f_clfs):\n    for i in range(0, NUM_TEST_RUNS):\n        noise = lgb_f_noise[idx]\n        f_preds[count,:] = np.clip(clf.predict(noisify(x_test, noise)), -1 , 10)\n        count += 1\n#y_fatalities_pred_blended_full = avg(f_preds)\n\n# %% [code]\ncount = 0\n\nfor idx, clf in enumerate(lgb_cfr_clfs):\n    for i in range(0, NUM_TEST_RUNS):\n        noise = lgb_cfr_noise[idx]\n        cfr_preds[count,:] = np.clip(clf.predict(noisify(x_test, noise)), -10 , 10)\n        count += 1\n#y_cfr_pred_blended_full = avg(cfr_preds)\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\ndef qPred(preds, pctile, simple=False):\n    q = np.percentile(preds, pctile, axis = 0)\n    if simple:\n        return q;\n    resid = preds - q\n    resid_wtg = 2\/100\/len(preds)* ( np.clip(resid, 0, None) * (pctile) \\\n                        + np.clip(resid, None, 0) * (100- pctile) )\n    adj = np.sum(resid_wtg, axis = 0)\n#     print(q)\n#     print(adj)\n#     print(q+adj)\n    return q + adj\n\n# %% [code]\n\n\n# %% [code]\nq = 50\n\n# %% [code]\ny_cases_pred_blended_full = qPred(c_preds, q) #avg(c_preds)\ny_fatalities_pred_blended_full = qPred(f_preds, q) # avg(f_preds)\ny_cfr_pred_blended_full = qPred(cfr_preds, q) #avg(cfr_preds)\n\n# %% [code]\n\n\n# %% [raw]\n# cfr_preds\n\n# %% [raw]\n# lgb_cfr_noise\n\n# %% [raw]\n# lgb_cfr_clfs[0].predict(noisify(x_test, 0.4))\n\n# %% [raw]\n# cfr_preds[0][0:500]\n\n# %% [raw]\n# x.log_cfr.plot(kind='hist', bins = 250)\n\n# %% [code]\n\n\n# %% [code]\nprint(np.mean(np.corrcoef(c_preds[::NUM_TEST_RUNS]),axis=0))\n\n# %% [code]\nprint(np.mean(np.corrcoef(f_preds[::NUM_TEST_RUNS]), axis=0))\n\n# %% [code]\nprint(np.mean(np.corrcoef(cfr_preds[::NUM_TEST_RUNS]), axis = 0))\n\n# %% [raw]\n# cfr_preds\n\n# %% [code]\npd.Series(np.std(c_preds, axis = 0)).plot(kind='hist', bins = 50)\n\n# %% [code]\npd.Series(np.std(f_preds, axis = 0)).plot(kind='hist', bins = 50)\n\n# %% [code]\npd.Series(np.std(cfr_preds, axis = 0)).plot(kind='hist', bins = 50)\n\n# %% [code]\ny_cfr\n\n# %% [code]\n(groups == 'Sierra Leone').sum()\n\n# %% [code]\npred = pd.DataFrame(np.hstack((np.transpose(c_preds),\n                              np.transpose(f_preds))), index=x_test.index)\npred['Place'] = data_test.Place\n\n\npred['Date'] = data_test.Date\npred['Date_f'] = data_test.Date_f\n\n# %% [code]\npred[(pred.Date == pred.Date.max()) & (pred.Date_f == pred.Date_f.max())][30: 60]\n\n# %% [code]\n(pred.Place=='Sierra Leone').sum()\n\n# %% [code]\nnp.round(pred[(pred.Date == pred.Date.max()) & (pred.Date_f == pred.Date_f.max())], 2)[190:220:]\n\n# %% [code] {\"scrolled\":false}\nnp.round(pred[(pred.Date == pred.Date.max()) & (pred.Date_f == pred.Date_f.max())][220:-20],2)\n\n# %% [code]\nc_preds.shape\nx_test.shape\n\n# %% [raw]\n# \n# data_test.shape\n\n# %% [raw]\n# pd.DataFrame({'c_mean': np.mean(c_preds, axis =0 ),\n#                   'c_median': np.median(c_preds, axis =0 ),\n#              }, index=data_test.Place)[::7]\n\n# %% [raw]\n# np.median(c_preds, axis =0 )[::71]\n\n# %% [code]\n\n\n# %% [markdown]\n# ### III. Other\n\n# %% [code]\n\n\n# %% [raw]\n# MAX_DATE = np.max(train.Date)\n\n# %% [raw]\n# final = train[train.Date == MAX_DATE]\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [raw]\n# train.groupby('Place')[['ConfirmedCases','Fatalities']].apply(lambda x: np.sum(x >0))\n\n# %% [raw]\n# num_changes = train.groupby('Place')[['ConfirmedCases','Fatalities']].apply(lambda x: np.sum(x - x.shift(1) >0))\n\n# %% [raw]\n# num_changes.Fatalities.plot(kind='hist', bins = 50);\n\n# %% [raw]\n# num_changes.ConfirmedCases.plot(kind='hist', bins = 50);\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Rate Calculation\n\n# %% [raw]\n# def getRate(train, window = 5):\n#     joined = pd.merge(train[train.Date == \n#                                     np.max(train.Date) - datetime.timedelta(window)], \n#                       final,  on=['Place'])\n#     joined['FatalityRate'] = (np.log(joined.Fatalities_y + 1)\\\n#                                   - np.log(joined.Fatalities_x + 1)) \/ window\n#     joined['CasesRate'] = (np.log(joined.ConfirmedCases_y + 1)\\\n#                                    - np.log(joined.ConfirmedCases_x + 1)) \/ window\n#     joined.set_index('Place', inplace=True)\n# \n#     rates = joined[[c for c in joined.columns.to_list() if 'Rate' in c]] \n#     return rates\n\n# %% [raw]\n# ltr = getRate(train, 14)\n\n# %% [raw]\n# lm = pd.merge(ltr, num_changes, on='Place')\n\n# %% [raw]\n# lm.filter(like='China', axis='rows')\n\n# %% [raw]\n# \n\n# %% [raw]\n# flat = lm[\n#     (lm.CasesRate < 0.01) & (lm.ConfirmedCases > 5)]\n\n# %% [raw]\n# flat\n\n# %% [raw]\n# \n\n# %% [raw]\n# c_rate = pd.Series(\n#     np.where(num_changes.ConfirmedCases >= 0, \n#          getRate(train, 7).CasesRate, \n#          getRate(train, 5).CasesRate),\n#     index = num_changes.index, name = 'CasesRate')\n# \n# f_rate = pd.Series(\n#     np.where(num_changes.Fatalities >= 0, \n#          getRate(train, 7).FatalityRate, \n#          getRate(train, 4).CasesRate),\n#     index = num_changes.index, name = 'FatalityRate')\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Plot of Changes\n\n# %% [raw]\n# def rollDates(df, i):\n#     df = df.copy()\n#     df.Date = df.Date + datetime.timedelta(i)\n#     return df\n\n# %% [raw]\n# m = pd.merge(rollDates(train, 7), train, on=['Place', 'Date'])\n# m['CaseChange'] = (np.log(m.ConfirmedCases_y + 1) - np.log(m.ConfirmedCases_x + 1))\/7\n\n# %% [raw]\n# m[m.Place=='USMaine']\n\n# %% [code]\n\n\n# %% [markdown]\n# #### Histograms of Case Counts\n\n# %% [code]\n\n\n# %% [raw]\n# m = pd.merge(rollDates(full_train, 1), full_train, on=['Place', 'Date'])\n# \n\n# %% [code]\n\n\n# %% [markdown]\n# ##### CFR Charts\n\n# %% [raw]\n# joined.Fatalities_y\n\n# %% [raw]\n# withcases = joined[joined.ConfirmedCases_y > 300]\n\n# %% [raw]\n# withcases.sort_values(by = ['Fatalities_y'])\n\n# %% [raw]\n# (withcases.Fatalities_y \/ withcases.ConfirmedCases_x).plot(kind='hist', bins = 150);\n\n# %% [raw]\n# (final.Fatalities \/ final.ConfirmedCases).plot(kind='hist', bins = 250);\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Predict on Test Set\n\n# %% [code]\ndata_wp = data_test.copy()\n\n# %% [code]\nif MODEL_Y is 'slope':\n    data_wp['case_slope'] = y_cases_pred_blended_full \n    data_wp['fatality_slope'] = y_fatalities_pred_blended_full \nelse:\n    data_wp['case_slope'] = y_cases_pred_blended_full \/ x_test.elapsed\n    data_wp['fatality_slope'] = y_fatalities_pred_blended_full \/ x_test.elapsed\n\ndata_wp['cfr_pred'] = y_cfr_pred_blended_full\n\n# %% [raw]\n# data_wp.head()\n\n# %% [raw]\n# data_wp.shape\n\n# %% [raw]\n# data_wp.Date_f.unique()\n\n# %% [code]\ntrain.Date.max()\n\n# %% [raw]\n# data_wp.Date\n\n# %% [code]\ntest.Date.min()\n\n# %% [raw]\n# test\n\n# %% [code]\nif len(test) > 0:\n    base_date = test.Date.min() - datetime.timedelta(1)\nelse:\n    base_date = train.Date.max()\n\n# %% [raw]\n# train\n\n# %% [raw]\n# len(test)\n\n# %% [code]\nbase_date\n\n# %% [code]\ndata_wp_ss = data_wp[data_wp.Date == base_date]\ndata_wp_ss = data_wp_ss.drop(columns='Date').rename(columns = {'Date_f': 'Date'})\n\n# %% [raw]\n# base_date\n\n# %% [raw]\n# data_wp_ss.head()\n\n# %% [raw]\n# test\n\n# %% [raw]\n# data_wp_ss.columns\n\n# %% [code]\n\n\n# %% [raw]\n# len(test);\n# len(x_test)\n\n# %% [code]\ntest_wp = pd.merge(test, data_wp_ss[['Date', 'Place', 'case_slope', 'fatality_slope', 'cfr_pred',\n                                    'elapsed']], \n            how='left', on = ['Date', 'Place'])\n\n# %% [raw]\n# test_wp[test_wp.Country == 'US']\n\n# %% [raw]\n# test_wp\n\n# %% [code]\nfirst_c_slope = test_wp[~test_wp.case_slope.isnull()].groupby('Place').first()\nlast_c_slope = test_wp[~test_wp.case_slope.isnull()].groupby('Place').last()\n\nfirst_f_slope = test_wp[~test_wp.fatality_slope.isnull()].groupby('Place').first()\nlast_f_slope = test_wp[~test_wp.fatality_slope.isnull()].groupby('Place').last()\n\nfirst_cfr_pred = test_wp[~test_wp.cfr_pred.isnull()].groupby('Place').first()\nlast_cfr_pred = test_wp[~test_wp.cfr_pred.isnull()].groupby('Place').last()\n\n# %% [raw]\n# test_wp\n\n# %% [raw]\n# first_c_slope\n\n# %% [raw]\n# test_wp\n\n# %% [raw]\n# test_wp\n\n# %% [code]\ntest_wp.case_slope = np.where(  test_wp.case_slope.isnull() & \n                     (test_wp.Date < first_c_slope.loc[test_wp.Place].Date.values),\n                   \n                  first_c_slope.loc[test_wp.Place].case_slope.values,\n                     test_wp.case_slope\n                  )\n\ntest_wp.case_slope = np.where(  test_wp.case_slope.isnull() & \n                     (test_wp.Date > last_c_slope.loc[test_wp.Place].Date.values),\n                   \n                  last_c_slope.loc[test_wp.Place].case_slope.values,\n                     test_wp.case_slope\n                  )\n\n# %% [code]\ntest_wp.fatality_slope = np.where(  test_wp.fatality_slope.isnull() & \n                     (test_wp.Date < first_f_slope.loc[test_wp.Place].Date.values),\n                   \n                  first_f_slope.loc[test_wp.Place].fatality_slope.values,\n                     test_wp.fatality_slope\n                  )\n\ntest_wp.fatality_slope = np.where(  test_wp.fatality_slope.isnull() & \n                     (test_wp.Date > last_f_slope.loc[test_wp.Place].Date.values),\n                   \n                  last_f_slope.loc[test_wp.Place].fatality_slope.values,\n                     test_wp.fatality_slope\n                  )\n\n# %% [code]\ntest_wp.cfr_pred = np.where(  test_wp.cfr_pred.isnull() & \n                     (test_wp.Date < first_cfr_pred.loc[test_wp.Place].Date.values),\n                   \n                  first_cfr_pred.loc[test_wp.Place].cfr_pred.values,\n                     test_wp.cfr_pred\n                  )\n\ntest_wp.cfr_pred = np.where(  test_wp.cfr_pred.isnull() & \n                     (test_wp.Date > last_cfr_pred.loc[test_wp.Place].Date.values),\n                   \n                  last_cfr_pred.loc[test_wp.Place].cfr_pred.values,\n                     test_wp.cfr_pred\n                  )\n\n# %% [code]\n\n\n# %% [code]\ntest_wp.case_slope = test_wp.case_slope.interpolate('linear')\ntest_wp.fatality_slope = test_wp.fatality_slope.interpolate('linear')\ntest_wp.cfr_pred = test_wp.cfr_pred.interpolate('linear')\n\n# %% [code]\ntest_wp.case_slope = test_wp.case_slope.fillna(0)\ntest_wp.fatality_slope = test_wp.fatality_slope.fillna(0)\n\n# test_wp.fatality_slope = test_wp.fatality_slope.fillna(0)\n\n# %% [raw]\n# test_wp.cfr_pred.isnull().sum()\n\n# %% [markdown]\n# #### Convert Slopes to Aggregate Counts\n\n# %% [code]\nLAST_DATE = test.Date.min() - datetime.timedelta(1)\n\n# %% [code]\nfinal = train_bk[train_bk.Date == LAST_DATE  ]\n\n# %% [raw]\n# train\n\n# %% [raw]\n# final\n\n# %% [code]\ntest_wp = pd.merge(test_wp, final[['Place', 'ConfirmedCases', 'Fatalities']], on='Place', \n                   how ='left', validate='m:1')\n\n# %% [raw]\n# test_wp\n\n# %% [code]\nLAST_DATE\n\n# %% [raw]\n# test_wp\n\n# %% [code]\ntest_wp.ConfirmedCases = np.exp( \n                            np.log(test_wp.ConfirmedCases + 1) \\\n                                + test_wp.case_slope * \n                                   (test_wp.Date - LAST_DATE).dt.days )- 1\n\ntest_wp.Fatalities = np.exp(\n                            np.log(test_wp.Fatalities + 1) \\\n                              + test_wp.fatality_slope * \n                                   (test_wp.Date - LAST_DATE).dt.days )  -1\n\n# test_wp.Fatalities = np.exp(\n#                             np.log(test_wp.ConfirmedCases + 1) \\\n#                               + test_wp.cfr_pred  )  -1\n                                     \n\n# %% [code]\nLAST_DATE\n\n# %% [raw]\n# final[final.Place=='Italy']\n\n# %% [code]\ntest_wp[ (test_wp.Country == 'Italy')].groupby('Date').sum()[:10]\n\n\n# %% [code]\ntest_wp[ (test_wp.Country == 'US')].groupby('Date').sum().iloc[-5:]\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Final Merge\n\n# %% [code]\nfinal = train_bk[train_bk.Date == test.Date.min() - datetime.timedelta(1) ]\n\n# %% [code]\nfinal.head()\n\n# %% [code]\ntest['elapsed'] = (test.Date - final.Date.max()).dt.days \n\n# %% [raw]\n# test.Date\n\n# %% [code]\ntest.elapsed\n\n# %% [code]\n\n\n# %% [markdown]\n# ### CFR Caps\n\n# %% [code]\nfull_bk = test_wp.copy()\n\n# %% [code]\nfull = test_wp.copy()\n\n# %% [code]\n\n\n# %% [code]\nBASE_RATE = 0.01\n\n# %% [code]\nCFR_CAP = 0.13\n\n# %% [code]\n\n\n# %% [code]\nlplot(full_bk)\n\n# %% [code]\nlplot(full_bk, columns = ['case_slope', 'fatality_slope'])\n\n# %% [code]\n\n\n# %% [code]\nfull['cfr_imputed_fatalities_low'] = full.ConfirmedCases * np.exp(full.cfr_pred) \/ np.exp(0.5)\nfull['cfr_imputed_fatalities_high'] = full.ConfirmedCases * np.exp(full.cfr_pred) * np.exp(0.5)\nfull['cfr_imputed_fatalities'] = full.ConfirmedCases * np.exp(full.cfr_pred)  \n\n# %% [code]\n\n\n# %% [raw]\n# full[(full.case_slope > 0.02) & \n#           (full.Fatalities < full.cfr_imputed_fatalities_low    ) &\n#                 (full.cfr_imputed_fatalities_low > 0.3) &\n#                 ( full.Fatalities < 100 ) &\n#     (full.Country!='China')] \\\n#      .groupby('Place').count()\\\n#     .sort_values('ConfirmedCases', ascending=False).iloc[:, 9:]\n\n# %% [code]\nfull[(full.case_slope > 0.02) & \n                   (full.Fatalities < full.cfr_imputed_fatalities_low    ) &\n                (full.cfr_imputed_fatalities_low > 0.3) &\n                ( full.Fatalities < 100000 ) &\n    (full.Country!='China') &\n     (full.Date == datetime.datetime(2020, 4,15))] \\\n     .groupby('Place').last()\\\n    .sort_values('Fatalities', ascending=False).iloc[:, 9:]\n\n# %% [code]\n(np.log(full.Fatalities + 1) -np.log(full.cfr_imputed_fatalities) ).plot(kind='hist', bins = 250)\n\n# %% [raw]\n# full[  \n#                    (np.log(full.Fatalities + 1) < np.log(full.cfr_imputed_fatalities_high + 1) -0.5    ) \n#     & (~full.Country.isin(['China', 'Korea, South']))\n#                 ][full.Date==train.Date.max()]\\\n#      .groupby('Place').first()\\\n#     .sort_values('cfr_imputed_fatalities', ascending=False).iloc[:, 9:]\n\n# %% [code]\nfull[(full.case_slope > 0.02) & \n                   (full.Fatalities < full.cfr_imputed_fatalities_low    ) &\n                (full.cfr_imputed_fatalities_low > 0.3) &\n                ( full.Fatalities < 100000 ) &\n    (~full.Country.isin(['China', 'Korea, South']))][full.Date==train.Date.max()]\\\n     .groupby('Place').first()\\\n    .sort_values('cfr_imputed_fatalities', ascending=False).iloc[:, 9:]\n\n# %% [code]\nfull.Fatalities = np.where(   \n    (full.case_slope > 0.02) & \n                   (full.Fatalities <= full.cfr_imputed_fatalities_low    ) &\n                (full.cfr_imputed_fatalities_low > 0.3) &\n                ( full.Fatalities < 100000 ) &\n    (~full.Country.isin(['China', 'Korea, South'])) ,\n                        \n                        (full.cfr_imputed_fatalities_high + full.cfr_imputed_fatalities)\/2,\n                                    full.Fatalities)\n    \n\n# %% [raw]\n# assert len(full) == len(data_wp)\n\n# %% [raw]\n# x_test.shape\n\n# %% [code]\nfull['elapsed'] = (test_wp.Date - LAST_DATE).dt.days\n\n# %% [code]\nfull[ (full.case_slope > 0.02) & \n          (np.log(full.Fatalities + 1) < np.log(full.ConfirmedCases * BASE_RATE + 1) - 0.5) &\n                           (full.Country != 'China')]\\\n            [full.Date == datetime.datetime(2020, 4, 5)] \\\n            .groupby('Place').last().sort_values('ConfirmedCases', ascending=False).iloc[:,8:]\n\n# %% [raw]\n# full.Fatalities.max()\n\n# %% [code]\nfull.Fatalities = np.where((full.case_slope > 0.02) & \n                      (full.Fatalities < full.ConfirmedCases * BASE_RATE) &\n                           (full.Country != 'China'), \n                                            \n            np.exp(   \n                    np.log( full.ConfirmedCases * BASE_RATE + 1) \\\n                           * np.clip(   0.5* (full.elapsed - 1) \/ 30, 0, 1) \\\n                           \n                     +  np.log(full.Fatalities +1 ) \\\n                           * np.clip(1 - 0.5* (full.elapsed - 1) \/ 30, 0, 1)\n            ) -1\n                           \n                           ,\n                                               full.Fatalities)  \n\n# %% [raw]\n# full.elapsed\n\n# %% [code]\nfull[(full.case_slope > 0.02) & \n                   (full.Fatalities > full.cfr_imputed_fatalities_high   ) &\n                (full.cfr_imputed_fatalities_low > 0.4) &\n    (full.Country!='China')]\\\n     .groupby('Place').count()\\\n    .sort_values('ConfirmedCases', ascending=False).iloc[:, 8:]\n\n# %% [raw]\n# full[full.Place=='United KingdomTurks and Caicos Islands']\n\n# %% [code]\nfull[(full.case_slope > 0.02) & \n                   (full.Fatalities > full.cfr_imputed_fatalities_high * 2   ) &\n                (full.cfr_imputed_fatalities_low > 0.4) &\n    (full.Country!='China')  ]\\\n     .groupby('Place').last()\\\n    .sort_values('ConfirmedCases', ascending=False).iloc[:, 8:]\n\n# %% [code]\nfull[(full.case_slope > 0.02) & \n                   (full.Fatalities > full.cfr_imputed_fatalities_high * 1.5   ) &\n                (full.cfr_imputed_fatalities_low > 0.4) &\n    (full.Country!='China')][full.Date==train.Date.max()]\\\n     .groupby('Place').first()\\\n    .sort_values('ConfirmedCases', ascending=False).iloc[:, 8:]\n\n# %% [code]\n\n\n# %% [code]\nfull.Fatalities =  np.where(  (full.case_slope > 0.02) & \n                   (full.Fatalities > full.cfr_imputed_fatalities_high      * 2   ) &\n                (full.cfr_imputed_fatalities_low > 0.4) &\n                (full.Country!='China') ,\n                            \n                     full.cfr_imputed_fatalities,\n                            \n                            full.Fatalities)\n\nfull.Fatalities =  np.where(  (full.case_slope > 0.02) & \n                   (full.Fatalities > full.cfr_imputed_fatalities_high   ) &\n                (full.cfr_imputed_fatalities_low > 0.4) &\n                (full.Country!='China') ,\n                    np.exp(        \n                            0.6667 * np.log(full.Fatalities + 1) \\\n                        + 0.3333 * np.log(full.cfr_imputed_fatalities + 1)\n                                ) - 1,\n                            \n                            full.Fatalities)\n\n# %% [code]\n\n\n# %% [code]\nfull[(full.Fatalities > full.ConfirmedCases * CFR_CAP) &\n                                          (full.ConfirmedCases > 1000)\n\n    ]                        .groupby('Place').last().sort_values('Fatalities', ascending=False)\n\n# %% [raw]\n# full.Fatalities =  np.where( (full.Fatalities > full.ConfirmedCases * CFR_CAP) &\n#                                           (full.ConfirmedCases > 1000)\n#                                         , \n#                              full.ConfirmedCases * CFR_CAP\\\n#                                            * np.clip((full.elapsed - 5) \/ 15, 0, 1) \\\n#                                  +  full.Fatalities * np.clip(1 - (full.elapsed - 5) \/ 15, 0, 1)\n#                             , \n#                                                full.Fatalities)\n\n# %% [raw]\n# train[train.Country=='Italy']\n\n# %% [raw]\n# final[final.Country=='US'].sum()\n\n# %% [code]\n(np.log(full.Fatalities + 1) -np.log(full.cfr_imputed_fatalities) ).plot(kind='hist', bins = 250)\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Fix Slopes now\n\n# %% [raw]\n# final\n\n# %% [code]\nassert len(pd.merge(full, final, on='Place', suffixes = ('', '_i'), validate='m:1')) == len(full)\n\n# %% [code]\nffm = pd.merge(full, final, on='Place', suffixes = ('', '_i'), validate='m:1')\nffm['fatality_slope'] = (np.log(ffm.Fatalities + 1 )\\\n                             - np.log(ffm.Fatalities_i + 1 ) ) \\\n                                 \/ ffm.elapsed\nffm['case_slope'] = (np.log(ffm.ConfirmedCases + 1 ) \\\n                             - np.log(ffm.ConfirmedCases_i + 1 ) ) \\\n                                 \/ ffm.elapsed\n\n# %% [markdown]\n# #### Fix Upward Slopers\n\n# %% [raw]\n# final_slope = (ffm.groupby('Place').last().case_slope)\n# final_slope.sort_values(ascending=False)\n# \n# high_final_slope = final_slope[final_slope > 0.1].index\n\n# %% [raw]\n# slope_change = (ffm.groupby('Place').last().case_slope - ffm.groupby('Place').first().case_slope)\n# slope_change.sort_values(ascending = False)\n# high_slope_increase = slope_change[slope_change > 0.05].index\n\n# %% [code]\n\n\n# %% [raw]\n# test.Date.min()\n\n# %% [raw]\n# set(high_slope_increase) & set(high_final_slope)\n\n# %% [raw]\n# ffm.groupby('Date').case_slope.median()\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Fix Drop-Offs\n\n# %% [code]\nffm[np.log(ffm.Fatalities+1) < np.log(ffm.Fatalities_i+1) - 0.2]\\\n    [['Place', 'Date', 'elapsed', 'Fatalities', 'Fatalities_i']]\n\n# %% [code]\nffm[np.log(ffm.ConfirmedCases + 1) < np.log(ffm.ConfirmedCases_i+1) - 0.2]\\\n    [['Place', 'elapsed', 'ConfirmedCases', 'ConfirmedCases_i']]\n\n# %% [code]\n\n\n# %% [raw]\n# (ffm.groupby('Place').last().fatality_slope - ffm.groupby('Place').first().fatality_slope)\\\n#     .sort_values(ascending = False)[:10]\n\n# %% [markdown]\n# ### Display\n\n# %% [raw]\n# full[full.Country=='US'].groupby('Date').agg(\n#     {'ForecastId': 'count',\n#      'case_slope': 'mean',\n#         'fatality_slope': 'mean',\n#             'ConfirmedCases': 'sum',\n#                 'Fatalities': 'sum',\n#                     })\n\n# %% [code]\nfull_bk[(full_bk.Date == test.Date.max() ) & \n   (~full_bk.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).sort_values('ConfirmedCases', ascending=False)\n\n# %% [raw]\n# full[full.Country=='China'].groupby('Date').agg(\n#     {'ForecastId': 'count',\n#      'case_slope': 'mean',\n#         'fatality_slope': 'mean',\n#             'ConfirmedCases': 'sum',\n#                 'Fatalities': 'sum',\n#                     })[::5]\n\n# %% [code]\n\n\n# %% [raw]\n# ffc = pd.merge(final, full, on='Place', validate = '1:m')\n# ffc[(np.log(ffc.Fatalities_x) - np.log(ffc.ConfirmedCase_x)) \/ ffc.elapsed_y ]\n\n# %% [raw]\n# ffm.groupby('Place').case_slope.last().sort_values(ascending = False)[:30]\n\n# %% [raw]\n# lplot(test_wp)\n\n# %% [raw]\n# lplot(test_wp, columns = ['case_slope', 'fatality_slope'])\n\n# %% [code]\n\n# %% [code]\n\n\n# %% [code]\nlplot(ffm[~ffm.Place.isin(new_places)])\n\n# %% [code]\n\n\n# %% [code]\nlplot(ffm[~ffm.Place.isin(new_places)], columns = ['case_slope', 'fatality_slope'])\n\n# %% [code]\n\n\n# %% [raw]\n# test.Date.min()\n\n# %% [code]\nffm.fatality_slope = np.clip(ffm.fatality_slope, None, 0.5)\n\n# %% [raw]\n# ffm.case_slope = np.clip(ffm.case_slope, None, 0.25)\n\n# %% [code]\n\n\n# %% [raw]\n# for lr in [0.05, 0.02, 0.01, 0.007, 0.005, 0.003]:\n# \n#     ffm.loc[ (ffm.Place==ffm.Place.shift(1) )\n#          & (ffm.Place==ffm.Place.shift(-1) ) &\n#      ( np.abs ( (ffm.case_slope.shift(-1) + ffm.case_slope.shift(1) ) \/ 2\n#                        - ffm.case_slope).fillna(0)\n#                     > lr ), 'case_slope'] = \\\n#                      ( ffm.case_slope.shift(-1) + ffm.case_slope.shift(1) ) \/ 2\n# \n\n# %% [code]\nfor lr in [0.2, 0.14, 0.1, 0.07, 0.05, 0.03, 0.01 ]:\n\n    ffm.loc[ (ffm.Place==ffm.Place.shift(4) )\n         & (ffm.Place==ffm.Place.shift(-4) ), 'fatality_slope'] = \\\n         ( ffm.fatality_slope.shift(-2) * 0.25 \\\n              + ffm.fatality_slope.shift(-1) * 0.5 \\\n                + ffm.fatality_slope \\\n                  + ffm.fatality_slope.shift(1) * 0.5 \\\n                    + ffm.fatality_slope.shift(2) * 0.25 ) \/ 2.5\n\n\n# %% [code]\n\n\n# %% [code]\nffm.ConfirmedCases = np.exp( \n                            np.log(ffm.ConfirmedCases_i + 1) \\\n                                + ffm.case_slope * \n                                   ffm.elapsed ) - 1\n\nffm.Fatalities = np.exp(\n                            np.log(ffm.Fatalities_i + 1) \\\n                              + ffm.fatality_slope * \n                                   ffm.elapsed ) - 1\n# test_wp.Fatalities = np.exp(\n#                             np.log(test_wp.ConfirmedCases + 1) \\\n#                               + test_wp.cfr_pred  )  -1\n                                     \n\n# %% [code]\n\n\n# %% [code]\nlplot(ffm[~ffm.Place.isin(new_places)])\n\n# %% [code]\n\n\n# %% [code]\nlplot(ffm[~ffm.Place.isin(new_places)], columns = ['case_slope', 'fatality_slope'])\n\n# %% [code]\n\n\n# %% [code]\nffm[(ffm.Date == test.Date.max() ) & \n   (~ffm.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).sort_values('ConfirmedCases', ascending=False)\n\n# %% [code]\n\n\n# %% [code]\nffm_bk = ffm.copy()\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\nffm = ffm_bk.copy()\n\n# %% [code]\ncounter = Counter(data.Place)\n# counter.most_common()\nmedian_count = np.median([ counter[group] for group in ffm.Place])\n# [ (group, np.round( np.power(counter[group] \/ median_count, -1),3) ) for group in \n#      counter.keys()]\nc_count = [ np.clip(\n            np.power(counter[group] \/ median_count, -1.5), None, 2.5) for group in ffm.Place]\n \n\n# %% [code]\nRATE_MULT = 0.00\nRATE_ADD = 0.003\nLAG_FALLOFF = 15\n\nma_factor = np.clip( ( ffm.elapsed - 14) \/ 14 , 0, 1)\n\nffm.case_slope = np.where(ffm.elapsed > 0,\n    0.7 * ffm.case_slope * (1+ ma_factor * RATE_MULT) \\\n         + 0.3 * (  ffm.case_slope.ewm(span=LAG_FALLOFF).mean()\\\n                                                      * np.clip(ma_factor, 0, 1)\n                      + ffm.case_slope    * np.clip( 1 - ma_factor, 0, 1)) \n                          \n                          + RATE_ADD * ma_factor * c_count,\n         ffm.case_slope)\n\n# --\n\nRATE_MULT = 0\nRATE_ADD = 0.015\nLAG_FALLOFF = 15\n\nma_factor = np.clip( ( ffm.elapsed - 10) \/ 14 , 0, 1)\n\n\nffm.fatality_slope = np.where(ffm.elapsed > 0,\n    0.3 * ffm.fatality_slope * (1+ ma_factor * RATE_MULT) \\\n         + 0.7* (  ffm.fatality_slope.ewm(span=LAG_FALLOFF).mean()\\\n                                                              * np.clip( ma_factor, 0, 1)\n                      + ffm.fatality_slope    * np.clip( 1 - ma_factor, 0, 1)   )\n                              \n                              + RATE_ADD * ma_factor * c_count \\\n                              \n                              \n                              * (ffm.Country != 'China')\n                              ,\n         ffm.case_slope)\n\n# %% [code]\nffm.ConfirmedCases = np.exp( \n                            np.log(ffm.ConfirmedCases_i + 1) \\\n                                + ffm.case_slope * \n                                   ffm.elapsed ) - 1\n\nffm.Fatalities = np.exp(\n                            np.log(ffm.Fatalities_i + 1) \\\n                              + ffm.fatality_slope * \n                                   ffm.elapsed ) - 1\n# test_wp.Fatalities = np.exp(\n#                             np.log(test_wp.ConfirmedCases + 1) \\\n#                               + test_wp.cfr_pred  )  -1\n                                     \n\n# %% [code]\n\n\n# %% [code]\nlplot(ffm[~ffm.Place.isin(new_places)])\n\n# %% [code]\n\n\n# %% [code]\nlplot(ffm[~ffm.Place.isin(new_places)], columns = ['case_slope', 'fatality_slope'])\n\n# %% [code]\n\n\n# %% [raw]\n# LAST_DATE\n\n# %% [code]\nffm_bk[(ffm_bk.Date == test.Date.max() ) & \n   (~ffm_bk.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).sort_values('ConfirmedCases', ascending=False)[:15]\n\n# %% [code]\nffm[(ffm.Date == test.Date.max() ) & \n   (~ffm.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).sort_values('ConfirmedCases', ascending=False)[:15]\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\nffm_bk[(ffm_bk.Date == test.Date.max() ) & \n   (~ffm_bk.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).sort_values('ConfirmedCases', ascending=False)[-50:]\n\n# %% [code]\nffm[(ffm.Date == test.Date.max() ) & \n   (~ffm.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).loc[ffm_bk[(ffm_bk.Date == test.Date.max() ) & \n   (~ffm_bk.Place.isin(new_places))].groupby('Country').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'last',\n        'fatality_slope': 'last',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    }\n).sort_values('ConfirmedCases', ascending=False)[-50:].index]\n\n# %% [code]\n\n\n# %% [code]\n# use country-specific CFR !!!!  helps cap US and raise up Italy !\n# could also use lagged CFR off cases as of 2 weeks ago...\n # ****  keep everything within ~0.5 order of magnitude of its predicted CFR.. !!\n\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Join\n\n# %% [raw]\n# assert len(test_wp) == len(full)\n# \n\n# %% [raw]\n# full = pd.merge(test_wp, full[['Place', 'Date', 'Fatalities']], on = ['Place', 'Date'],\n#             validate='1:1')\n\n# %% [code]\n\n\n# %% [markdown]\n# ### Fill in New Places with Ramp Average\n\n# %% [code]\nNUM_TEST_DATES = len(test.Date.unique())\n\nbase = np.zeros((2, NUM_TEST_DATES))\nbase2 = np.zeros((2, NUM_TEST_DATES))\n\n# %% [code]\nfor idx, c in enumerate(['ConfirmedCases', 'Fatalities']):\n    for n in range(0, NUM_TEST_DATES):\n        base[idx,n] = np.mean(\n            np.log(  train[((train.Date < test.Date.min())) & \n              (train.ConfirmedCases > 0)].groupby('Country').nth(n)[c]+1))\n\n# %% [code]\nbase = np.pad( base, ((0,0), (6,0)), mode='constant', constant_values = 0)\n\n# %% [code]\nfor n in range(0, base2.shape[1]):\n    base2[:, n] = np.mean(base[:, n+0: n+7], axis = 1)\n\n# %% [code]\nnew_places = train[(train.Date == test.Date.min() - datetime.timedelta(1)) &\n      (train.ConfirmedCases == 0)\n     ].Place\n\n# %% [code]\n# fill in new places \nffm.ConfirmedCases = \\\n    np.where(   ffm.Place.isin(new_places),\n          base2[ 0, (ffm.Date - test.Date.min()).dt.days],\n                 ffm.ConfirmedCases)\nffm.Fatalities = \\\n    np.where(   ffm.Place.isin(new_places),\n          base2[ 1, (ffm.Date - test.Date.min()).dt.days],\n                 ffm.Fatalities)\n\n# %% [code]\n\n\n# %% [code]\nffm[ffm.Country=='US'].groupby('Date').agg(\n    {'ForecastId': 'count',\n     'case_slope': 'mean',\n        'fatality_slope': 'mean',\n            'ConfirmedCases': 'sum',\n                'Fatalities': 'sum',\n                    })\n\n# %% [raw]\n# train[train.Country == 'US'].Province_State.unique()\n\n# %% [markdown]\n# ### Save\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\nsub = pd.read_csv(input_path + '\/submission.csv')\n\n# %% [code]\nscl = sub.columns.to_list()\n\n# %% [code]\n\n# print(full_bk.groupby('Place').last()[['Date', 'ConfirmedCases', 'Fatalities']])\n# print(ffm.groupby('Place').last()[['Date', 'ConfirmedCases', 'Fatalities']])\n\n\n# %% [code]\nif ffm[scl].isnull().sum().sum() == 0:\n    out = full_bk[scl] * 0.3 + ffm_bk[scl] * 0.3 + full[scl] * 0.3 + ffm[scl] * 0.1\nelse:\n    print('using full-bk')\n    out = full_bk[scl]\n\nout.ForecastId = np.round(out.ForecastId, 0).astype(int) \n\nprint(pd.merge(out, test[['ForecastId', 'Date', 'Place']], on='ForecastId')\\\n      .sort_values('ForecastId')\\\n          .groupby('Place').last()[['Date', 'ConfirmedCases', 'Fatalities']])\n\nout = np.round(out, 2)\nprivate = out[sub.columns.to_list()]\n  \n\n\nfull_pred = pd.concat((private, public[~public.ForecastId.isin(private.ForecastId)]),\n     ignore_index=True).sort_values('ForecastId')\n\nfull_pred.to_csv('submission3.csv', index=False)","a3816a8f":"gc.collect()","f76d37f8":"!pip install tensorflow_addons\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport xgboost as xgb\n\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\n\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom datetime import datetime\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nimport tensorflow_addons as tfa\n\n\nimport datetime\nimport gc\nfrom tqdm import tqdm\n\ndef swishE(x):\n   beta = 1.75 #1, 1.5 or 2\n   return beta * x * tf.keras.backend.sigmoid(x)\n\ndef swish(x):\n    return x * tf.keras.backend.sigmoid(x)\n\ndef phrishII(x):\n    return x*tf.keras.backend.tanh(1.75 * x * tf.keras.backend.sigmoid(x))\n\ndef mish(x):\n    return x*tf.keras.backend.tanh(tf.keras.backend.softplus(x))\n\ndef gelu_new(x):\n    \"\"\"Gaussian Error Linear Unit.\n    This is a smoother version of the RELU.\n    Original paper: https:\/\/arxiv.org\/abs\/1606.08415\n    Args:\n        x: float Tensor to perform activation.\n    Returns:\n        `x` with the GELU activation applied.\n    \"\"\"\n    cdf = 0.5 * (1.0 + tf.tanh(\n        (np.sqrt(2 \/ np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n    return x * cdf\n\n","1ba203c2":"def get_ridgeCV_sub(save_oof=False, save_public_test=False):\n    train = pd.read_csv('..\/input\/covid19-global-forecasting-week-4\/train.csv')\n    train['Province_State'].fillna('', inplace=True)\n    train['Date'] = pd.to_datetime(train['Date'])\n    train['day'] = train.Date.dt.dayofyear\n  #  train = train[train.day <= 101]\n    train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n    train\n\n    test = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv')\n    test['Province_State'].fillna('', inplace=True)\n    test['Date'] = pd.to_datetime(test['Date'])\n    test['day'] = test.Date.dt.dayofyear\n    test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n    test\n\n    day_min = train['day'].min()\n    train['day'] -= day_min\n    test['day'] -= day_min\n\n    min_test_val_day = test.day.min()\n    max_test_val_day = train.day.max()\n    max_test_day = test.day.max()\n    num_days = max_test_day + 1\n\n    min_test_val_day, max_test_val_day, num_days\n\n    train['ForecastId'] = -1\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    debug = False\n\n    data = pd.concat([train,\n                      test[test.day > max_test_val_day][train.columns]\n                     ]).reset_index(drop=True)\n    if debug:\n        data = data[data['geo'] >= 'France_'].reset_index(drop=True)\n    #del train, test\n    gc.collect()\n\n    dates = data[data['geo'] == 'France_'].Date.values\n\n    if 0:\n        gr = data.groupby('geo')\n        data['ConfirmedCases'] = gr.ConfirmedCases.transform('cummax')\n        data['Fatalities'] = gr.Fatalities.transform('cummax')\n\n    geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n    num_geo = geo_data.shape[0]\n    geo_data\n\n    geo_id = {}\n    for i,g in enumerate(geo_data.index):\n        geo_id[g] = i\n\n\n    ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n    Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n\n    if debug:\n        cases = ConfirmedCases.values\n        deaths = Fatalities.values\n    else:\n        cases = np.log1p(ConfirmedCases.values)\n        deaths = np.log1p(Fatalities.values)\n\n\n    def get_dataset(start_pred, num_train, lag_period):\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)\n        lag_cases = np.vstack([cases[:, d - lag_period : d] for d in days])\n        lag_deaths = np.vstack([deaths[:, d - lag_period : d] for d in days])\n        target_cases = np.vstack([cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([deaths[:, d : d + 1] for d in days])\n        geo_ids = np.vstack([geo_ids_base for d in days])\n        country_ids = np.vstack([country_ids_base for d in days])\n        return lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days\n\n    def update_valid_dataset(data, pred_death, pred_case):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = cases[:, day:day+1]\n        new_target_deaths = deaths[:, day:day+1] \n        new_geo_ids = geo_ids  \n        new_country_ids = country_ids  \n        new_days = 1 + days\n        return new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, new_geo_ids, new_country_ids, new_days\n\n    def fit_eval(lr_death, lr_case, data, start_lag_death, end_lag_death, num_lag_case, fit, score):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], country_ids])\n        X_death = np.hstack([lag_deaths[:, -num_lag_case:], country_ids])\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], lag_deaths[:, -num_lag_case:], country_ids])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n            if 0:\n                keep = (y_death > 0).ravel()\n                X_death = X_death[keep]\n                y_death = y_death[keep]\n                y_death_prev = y_death_prev[keep]\n            lr_death.fit(X_death, y_death)\n         #   pickle.dump(lr_death, open(\"lr_death.pickle.dat\", \"wb\"))\n         #   lr_death = pickle.load(open(\"..\/input\/savednnorg\/lr_death.pickle.dat\", \"rb\"))\n        y_pred_death = lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -num_lag_case:], geo_ids])\n        X_case = lag_cases[:, -num_lag_case:]\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            lr_case.fit(X_case, y_case)\n            #pickle.dump(lr_death, open(\"lr_case.pickle.dat\", \"wb\"))\n           # lr_case = pickle.load(open(\"..\/input\/lrcaseorg\/lr_case.pickle.dat\", \"rb\"))\n        y_pred_case = lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        if score:\n            death_score = val_score(y_death, y_pred_death)\n            case_score = val_score(y_case, y_pred_case)\n        else:\n            death_score = 0\n            case_score = 0\n\n        return death_score, case_score, y_pred_death, y_pred_case\n\n    def train_model(train, valid, start_lag_death, end_lag_death, num_lag_case, num_val, score=True):\n        alphas_alt = [3, 6, 7, 8, 10, 11, 14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n        alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n        e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n        e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n        kfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n        lr_death = RidgeCV(alphas=alphas_alt, fit_intercept=False)\n        lr_case = RidgeCV(alphas=alphas_alt, fit_intercept=True)\n\n        (train_death_score, train_case_score, train_pred_death, train_pred_case,\n        ) = fit_eval(lr_death, lr_case, train, start_lag_death, end_lag_death, num_lag_case, fit=True, score=score)\n\n        death_scores = []\n        case_scores = []\n\n        death_pred = []\n        case_pred = []\n\n        for i in range(num_val):\n\n            (valid_death_score, valid_case_score, valid_pred_death, valid_pred_case,\n            ) = fit_eval(lr_death, lr_case, valid, start_lag_death, end_lag_death, num_lag_case, fit=False, score=score)\n\n            death_scores.append(valid_death_score)\n            case_scores.append(valid_case_score)\n            death_pred.append(valid_pred_death)\n            case_pred.append(valid_pred_case)\n\n            if 0:\n                print('val death: %0.3f' %  valid_death_score,\n                      'val case: %0.3f' %  valid_case_score,\n                      'val : %0.3f' %  np.mean([valid_death_score, valid_case_score]),\n                      flush=True)\n            valid = update_valid_dataset(valid, valid_pred_death, valid_pred_case)\n\n        if score:\n            death_scores = np.sqrt(np.mean([s**2 for s in death_scores]))\n            case_scores = np.sqrt(np.mean([s**2 for s in case_scores]))\n            if 0:\n                print('train death: %0.3f' %  train_death_score,\n                      'train case: %0.3f' %  train_case_score,\n                      'val death: %0.3f' %  death_scores,\n                      'val case: %0.3f' %  case_scores,\n                      'val : %0.3f' % ( (death_scores + case_scores) \/ 2),\n                      flush=True)\n            else:\n                print('%0.4f' %  case_scores,\n                      ', %0.4f' %  death_scores,\n                      '= %0.4f' % ( (death_scores + case_scores) \/ 2),\n                      flush=True)\n        death_pred = np.hstack(death_pred)\n        case_pred = np.hstack(case_pred)\n        return death_scores, case_scores, death_pred, case_pred\n\n    countries = [g.split('_')[0] for g in geo_data.index]\n    countries = pd.factorize(countries)[0]\n\n    country_ids_base = countries.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n    country_ids_base.shape\n\n    geo_ids_base = np.arange(num_geo).reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    geo_ids_base = 0.1 * ohe.fit_transform(geo_ids_base)\n    geo_ids_base.shape\n\n    def val_score(true, pred):\n        pred = np.log1p(np.round(np.expm1(pred) - 0.2))\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n    def val_score(true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n\n\n    start_lag_death, end_lag_death = 14, 6,\n    num_train = 5\n    num_lag_case = 14\n    lag_period = max(start_lag_death, num_lag_case)\n\n    def get_oof(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[start_val], start_val, num_val)\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = train[['Date', 'Id', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[sub.day >= start_val]\n        sub = sub[['Id', 'ConfirmedCases', 'Fatalities']].copy()\n        return sub\n\n\n    if save_oof:\n        for start_val_delta, date in zip(range(3, -8, -3),\n                                  ['2020-04-01', '2020-03-29', '2020-03-26', '2020-03-23']):\n            print(date, end=' ')\n            oof = get_oof(start_val_delta)\n            oof.to_csv('..\/submissions\/cpmp-%s.csv' % date, index=None)\n\n    def get_sub(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = 14\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        return sub\n        return sub\n\n\n    known_test = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']\n              ].merge(test[['geo', 'day', 'ForecastId']], how='left', on=['geo', 'day'])\n    known_test = known_test[['ForecastId', 'ConfirmedCases', 'Fatalities']][known_test.ForecastId.notnull()].copy()\n    known_test\n\n    unknow_test = test[test.day > max_test_val_day]\n    unknow_test\n\n    def get_final_sub():   \n        start_val = max_test_val_day + 1\n        last_train = start_val - 1\n        num_val = max_test_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = num_val + 3\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        (_, _, val_death_preds, val_case_preds\n        ) = train_model(train_data, valid_data, start_lag_death, end_lag_death, num_lag_case, num_val, score=False)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n        print(unknow_test.shape, pred_deaths.shape, pred_cases.shape)\n\n        sub = unknow_test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        sub = pd.concat([known_test, sub])\n        return sub\n\n    if save_public_test:\n        sub = get_sub()\n    else:\n        sub = get_final_sub()\n    return sub","b7400ea9":"gc.collect()","37452bae":"\n\ndef get_nn_sub():\n    df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\n    sub_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\n\n    coo_df = pd.read_csv(\"..\/input\/covid19week1\/train.csv\").rename(columns={\"Country\/Region\": \"Country_Region\"})\n    coo_df = coo_df.groupby(\"Country_Region\")[[\"Lat\", \"Long\"]].mean().reset_index()\n    coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n        df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n        df[\"weekend\"] = df[\"Date\"].dt.dayofweek\/\/5\n\n        df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n        df[\"Lat\"] = (df[\"Lat\"] \/\/ 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] \/\/ 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 5\n\n    features = [\"Lat\", \"Long\"]\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n\n\n    def get_model1():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, gelu_new)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swishE)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model2():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, gelu_new)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation='linear')(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model3():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model4():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, mish)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, mish)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model5():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swish)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model6():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, phrishII)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, phrishII)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swish)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model7():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, gelu_new)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swish)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n\n\n    #get_model().summary()\n    \n\n    model1 = get_model1()\n    model2 = get_model2()\n    model3 = get_model3()\n    model4 = get_model4()\n    model5 = get_model5()\n    model6 = get_model6()\n    model7 = get_model7()\n\n    inp = KL.Input(shape=(len(features),))\n    y1 = model1(inp)\n    y2 = model2(inp)\n    y3 = model3(inp)\n    y4 = model4(inp)\n    y5 = model5(inp)\n    y6 = model6(inp)\n    y7 = model7(inp)\n    outputs = tf.keras.layers.average([y1, y2, y3, y4, y5, y6, y7])\n    ensemble_model = tf.keras.Model(inputs=inp, outputs=outputs)\n    ensemble_model.summary()\n    \n    from tensorflow.keras.utils import plot_model\n    plot_model(ensemble_model, to_file='model.png',show_shapes=True, show_layer_names=True, expand_nested=True)\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 10\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = ensemble_model#get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer= tfa.optimizers.Lookahead(tfa.optimizers.AdamW(lr=0.005,weight_decay=6.924e-5), sync_period=15, slow_step_size=0.5))\n            if save:\n                #checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model_1_{}.h5\".format(i), monitor='val_loss', verbose=0, save_best_only=True,save_weights_only=True, mode='min')\n                #hist = model.fit(get_input(df), df[TARGETS],batch_size=2048, epochs=500, verbose=0,shuffle=True,validation_split=0.2,callbacks=[checkpoint])\n                model.load_weights(\"..\/input\/nn10final\/model_1_{}.h5\".format(i))\n            else:\n                #checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model_1_dev_{}.h5\".format(i), monitor='val_loss', verbose=0, save_best_only=True,save_weights_only=True, mode='min')\n               # hist = model.fit(get_input(df), df[TARGETS],batch_size=2048, epochs=500, verbose=0,shuffle=True,validation_split=0.2,callbacks=[checkpoint])\n                model.load_weights(\"..\/input\/nn10final\/model_1_dev_{}.h5\".format(i))\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))\/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error\/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    sub_df_public = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n    sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = sub_df_private[\"Date\"].min()\n    SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n    sub_df_private = df.append(sub_df_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n    sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n    sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n    sub_df = sub_df_public.append(sub_df_private, sort=False)\n    sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n    return sub_df[[\"ForecastId\"] + TARGETS]\n\ndef get_nn_sub2():\n    df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\n    sub_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\n\n    coo_df = pd.read_csv(\"..\/input\/covid19week1\/train.csv\").rename(columns={\"Country\/Region\": \"Country_Region\"})\n    coo_df = coo_df.groupby(\"Country_Region\")[[\"Lat\", \"Long\"]].mean().reset_index()\n    coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n        df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n        df[\"weekend\"] = df[\"Date\"].dt.dayofweek\/\/5\n\n        df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n        df[\"Lat\"] = (df[\"Lat\"] \/\/ 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] \/\/ 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 5\n\n    features = [\"Lat\", \"Long\"]\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n\n\n    def get_model1():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, gelu_new)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swishE)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model2():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, gelu_new)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation='linear')(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model3():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model4():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, mish)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, mish)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model5():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swish)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model6():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, phrishII)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, phrishII)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swish)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model7():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, gelu_new)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swish)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n\n\n    #get_model().summary()\n    \n\n    model1 = get_model1()\n    model2 = get_model2()\n    model3 = get_model3()\n    model4 = get_model4()\n    model5 = get_model5()\n    model6 = get_model6()\n    model7 = get_model7()\n\n    inp = KL.Input(shape=(len(features),))\n    y1 = model1(inp)\n    y2 = model2(inp)\n    y3 = model3(inp)\n    y4 = model4(inp)\n    y5 = model5(inp)\n    y6 = model6(inp)\n    y7 = model7(inp)\n    outputs = tf.keras.layers.average([y1, y2, y3, y4, y5, y6, y7])\n    ensemble_model = tf.keras.Model(inputs=inp, outputs=outputs)\n    ensemble_model.summary()\n    \n    from tensorflow.keras.utils import plot_model\n    plot_model(ensemble_model, to_file='model.png',show_shapes=True, show_layer_names=True, expand_nested=True)\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 10\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = ensemble_model#get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0))\n            #hist = model.fit(get_input(df), df[TARGETS],\n            #                 batch_size=2048, epochs=500, verbose=0, shuffle=True)\n            if save:\n                #model.save_weights(\"model_2_{}.h5\".format(i))\n                model.load_weights(\"..\/input\/nn10final\/model_2_{}.h5\".format(i))\n            else:\n                #model.save_weights(\"model_2_Dev_{}.h5\".format(i))\n                model.load_weights(\"..\/input\/nn10final\/model_2_Dev_{}.h5\".format(i))\n\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))\/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error\/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    sub_df_public = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n    sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = sub_df_private[\"Date\"].min()\n    SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n    sub_df_private = df.append(sub_df_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n    sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n    sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n    sub_df = sub_df_public.append(sub_df_private, sort=False)\n    sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n    return sub_df[[\"ForecastId\"] + TARGETS]\n\nsub2 = get_nn_sub()\nsub4 = get_nn_sub2()\n\nsub3 = get_ridgeCV_sub()\nsub3['ForecastId'] = sub3['ForecastId'].astype('int')\n\nsub2.sort_values(\"ForecastId\", inplace=True)\nsub4.sort_values(\"ForecastId\", inplace=True)\nsub3.sort_values(\"ForecastId\", inplace=True)\n\nfrom sklearn.metrics import mean_squared_error\n\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n[np.sqrt(mean_squared_error(np.log1p(sub3[t].values), np.log1p(sub2[t].values))) for t in TARGETS]\n[np.sqrt(mean_squared_error(np.log1p(sub3[t].values), np.log1p(sub4[t].values))) for t in TARGETS]\n\nsub_df = sub3.copy()\nsub_df_final = sub3.copy()\nfor t in TARGETS:\n    sub_df[t] = np.expm1(np.log1p(sub2[t].values)*0.3 + np.log1p(sub3[t].values)*0.4 + np.log1p(sub4[t].values)*0.3)\n    \nsub_df.to_csv(\"submission1.csv\", index=False)","7e8285f8":"gc.collect()","29b7c108":"\n\ndef get_nn_sub():\n    df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\n    sub_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\n\n    coo_df = pd.read_csv(\"..\/input\/covid19week1\/train.csv\").rename(columns={\"Country\/Region\": \"Country_Region\"})\n    coo_df = coo_df.groupby(\"Country_Region\")[[\"Lat\", \"Long\"]].mean().reset_index()\n    coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n        df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n        df[\"weekend\"] = df[\"Date\"].dt.dayofweek\/\/5\n\n        df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n        df[\"Lat\"] = (df[\"Lat\"] \/\/ 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] \/\/ 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 5\n\n    features = [\"Lat\", \"Long\"]\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n\n\n    def get_model1():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, gelu_new)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swishE)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model2():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, gelu_new)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation='linear')(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model3():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model4():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, mish)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, mish)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model5():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swish)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model6():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, phrishII)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, phrishII)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swish)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model7():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, gelu_new)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swish)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n\n\n    #get_model().summary()\n    \n\n    model1 = get_model1()\n    model2 = get_model2()\n    model3 = get_model3()\n    model4 = get_model4()\n    model5 = get_model5()\n    model6 = get_model6()\n    model7 = get_model7()\n\n    inp = KL.Input(shape=(len(features),))\n    y1 = model1(inp)\n    y2 = model2(inp)\n    y3 = model3(inp)\n    y4 = model4(inp)\n    y5 = model5(inp)\n    y6 = model6(inp)\n    y7 = model7(inp)\n    outputs = tf.keras.layers.average([y1, y2, y3, y4, y5, y6, y7])\n    ensemble_model = tf.keras.Model(inputs=inp, outputs=outputs)\n    ensemble_model.summary()\n    \n    from tensorflow.keras.utils import plot_model\n    plot_model(ensemble_model, to_file='model.png',show_shapes=True, show_layer_names=True, expand_nested=True)\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 40\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = ensemble_model#get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer= tfa.optimizers.Lookahead(tfa.optimizers.AdamW(lr=0.005,weight_decay=6.924e-5), sync_period=15, slow_step_size=0.5))\n            if save:\n                #checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model_1_{}.h5\".format(i), monitor='val_loss', verbose=0, save_best_only=True,save_weights_only=True, mode='min')\n              #  hist = model.fit(get_input(df), df[TARGETS],batch_size=2048, epochs=500, verbose=0,shuffle=True,validation_split=0.2,callbacks=[checkpoint])\n               # model.load_weights(\"model_1_{}.h5\".format(i))\n                model.load_weights(\"..\/input\/n40final\/model_1_{}.h5\".format(i))\n            else:\n                #checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model_1_dev_{}.h5\".format(i), monitor='val_loss', verbose=0, save_best_only=True,save_weights_only=True, mode='min')\n             #   hist = model.fit(get_input(df), df[TARGETS],batch_size=2048, epochs=500, verbose=0,shuffle=True,validation_split=0.2,callbacks=[checkpoint])\n               # model.load_weights(\"model_1_dev_{}.h5\".format(i))\n                model.load_weights(\"..\/input\/n40final\/model_1_dev_{}.h5\".format(i))\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))\/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error\/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    sub_df_public = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n    sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = sub_df_private[\"Date\"].min()\n    SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n    sub_df_private = df.append(sub_df_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n    sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n    sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n    sub_df = sub_df_public.append(sub_df_private, sort=False)\n    sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n    return sub_df[[\"ForecastId\"] + TARGETS]\n\ndef get_nn_sub2():\n    df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\n    sub_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\n\n    coo_df = pd.read_csv(\"..\/input\/covid19week1\/train.csv\").rename(columns={\"Country\/Region\": \"Country_Region\"})\n    coo_df = coo_df.groupby(\"Country_Region\")[[\"Lat\", \"Long\"]].mean().reset_index()\n    coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n        df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n        df[\"weekend\"] = df[\"Date\"].dt.dayofweek\/\/5\n\n        df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n        df[\"Lat\"] = (df[\"Lat\"] \/\/ 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] \/\/ 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 5\n\n    features = [\"Lat\", \"Long\"]\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n\n\n    def get_model1():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, gelu_new)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swishE)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model2():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, gelu_new)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation='linear')(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model3():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model4():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, mish)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, mish)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model5():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swish)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model6():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, phrishII)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, phrishII)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swish)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    \n    def get_model7():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, gelu_new)\n        gate_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, gelu_new)\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=swish)(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n\n\n    #get_model().summary()\n    \n\n    model1 = get_model1()\n    model2 = get_model2()\n    model3 = get_model3()\n    model4 = get_model4()\n    model5 = get_model5()\n    model6 = get_model6()\n    model7 = get_model7()\n\n    inp = KL.Input(shape=(len(features),))\n    y1 = model1(inp)\n    y2 = model2(inp)\n    y3 = model3(inp)\n    y4 = model4(inp)\n    y5 = model5(inp)\n    y6 = model6(inp)\n    y7 = model7(inp)\n    outputs = tf.keras.layers.average([y1, y2, y3, y4, y5, y6, y7])\n    ensemble_model = tf.keras.Model(inputs=inp, outputs=outputs)\n    ensemble_model.summary()\n    \n    from tensorflow.keras.utils import plot_model\n    plot_model(ensemble_model, to_file='model.png',show_shapes=True, show_layer_names=True, expand_nested=True)\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 40\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = ensemble_model#get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0))\n            #hist = model.fit(get_input(df), df[TARGETS],\n            #                 batch_size=2048, epochs=500, verbose=0, shuffle=True)\n            if save:\n                model.load_weights(\"..\/input\/n40final\/model_2_{}.h5\".format(i))\n            else:\n                model.load_weights(\"..\/input\/n40final\/model_2_Dev_{}.h5\".format(i))\n\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))\/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error\/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    sub_df_public = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n    sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = sub_df_private[\"Date\"].min()\n    SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n    sub_df_private = df.append(sub_df_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n    sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n    sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n    sub_df = sub_df_public.append(sub_df_private, sort=False)\n    sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n    return sub_df[[\"ForecastId\"] + TARGETS]\n\nsub2 = get_nn_sub()\nsub4 = get_nn_sub2()\n\nsub2.sort_values(\"ForecastId\", inplace=True)\nsub4.sort_values(\"ForecastId\", inplace=True)\n\nfrom sklearn.metrics import mean_squared_error\n\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n[np.sqrt(mean_squared_error(np.log1p(sub3[t].values), np.log1p(sub2[t].values))) for t in TARGETS]\n[np.sqrt(mean_squared_error(np.log1p(sub3[t].values), np.log1p(sub4[t].values))) for t in TARGETS]\n\nsub_df40 = sub3.copy()\nfor t in TARGETS:\n    sub_df40[t] = np.expm1(np.log1p(sub2[t].values)*0.3 + np.log1p(sub3[t].values)*0.4 + np.log1p(sub4[t].values)*0.3)\n    \nsub_df40.to_csv(\"submission50.csv\", index=False)","165f9220":"gc.collect()","4df2f398":"\n\n# Any results you write to the current directory are saved as output.\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport xgboost as xgb\nimport tensorflow as tf\nimport pickle\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm\n\ndef get_cpmp_sub(save_oof=False, save_public_test=False):\n    train = pd.read_csv('..\/input\/covid19-global-forecasting-week-4\/train.csv')\n    train['Province_State'].fillna('', inplace=True)\n    train['Date'] = pd.to_datetime(train['Date'])\n    train['day'] = train.Date.dt.dayofyear\n    #train = train[train.day <= 85]\n    train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n    train\n\n    test = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv')\n    test['Province_State'].fillna('', inplace=True)\n    test['Date'] = pd.to_datetime(test['Date'])\n    test['day'] = test.Date.dt.dayofyear\n    test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n    test\n\n    day_min = train['day'].min()\n    train['day'] -= day_min\n    test['day'] -= day_min\n\n    min_test_val_day = test.day.min()\n    max_test_val_day = train.day.max()\n    max_test_day = test.day.max()\n    num_days = max_test_day + 1\n\n    min_test_val_day, max_test_val_day, num_days\n\n    train['ForecastId'] = -1\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    debug = False\n\n    data = pd.concat([train,\n                      test[test.day > max_test_val_day][train.columns]\n                     ]).reset_index(drop=True)\n    if debug:\n        data = data[data['geo'] >= 'France_'].reset_index(drop=True)\n    #del train, test\n    gc.collect()\n\n    dates = data[data['geo'] == 'France_'].Date.values\n\n    if 0:\n        gr = data.groupby('geo')\n        data['ConfirmedCases'] = gr.ConfirmedCases.transform('cummax')\n        data['Fatalities'] = gr.Fatalities.transform('cummax')\n\n    geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n    num_geo = geo_data.shape[0]\n    geo_data\n\n    geo_id = {}\n    for i,g in enumerate(geo_data.index):\n        geo_id[g] = i\n\n\n    ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n    Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n\n    if debug:\n        cases = ConfirmedCases.values\n        deaths = Fatalities.values\n    else:\n        cases = np.log1p(ConfirmedCases.values)\n        deaths = np.log1p(Fatalities.values)\n\n\n    def get_dataset(start_pred, num_train, lag_period):\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)\n        lag_cases = np.vstack([cases[:, d - lag_period : d] for d in days])\n        lag_deaths = np.vstack([deaths[:, d - lag_period : d] for d in days])\n        target_cases = np.vstack([cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([deaths[:, d : d + 1] for d in days])\n        geo_ids = np.vstack([geo_ids_base for d in days])\n        country_ids = np.vstack([country_ids_base for d in days])\n        return lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days\n\n    def update_valid_dataset(data, pred_death, pred_case):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = cases[:, day:day+1]\n        new_target_deaths = deaths[:, day:day+1] \n        new_geo_ids = geo_ids  \n        new_country_ids = country_ids  \n        new_days = 1 + days\n        return new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, new_geo_ids, new_country_ids, new_days\n\n    def fit_eval(lr_death, lr_case, data, start_lag_death, end_lag_death, num_lag_case, fit, score):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], country_ids])\n        X_death = np.hstack([lag_deaths[:, -num_lag_case:], country_ids])\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], lag_deaths[:, -num_lag_case:], country_ids])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n            if 0:\n                keep = (y_death > 0).ravel()\n                X_death = X_death[keep]\n                y_death = y_death[keep]\n                y_death_prev = y_death_prev[keep]\n            lr_death.fit(X_death, y_death)\n            pickle.dump(lr_death, open(\"lr_death.pickle.dat\", \"wb\"))\n        y_pred_death = lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -num_lag_case:], geo_ids])\n        X_case = lag_cases[:, -num_lag_case:]\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            lr_case.fit(X_case, y_case)\n            pickle.dump(lr_case, open(\"lr_case.pickle.dat\", \"wb\"))\n        y_pred_case = lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        if score:\n            death_score = val_score(y_death, y_pred_death)\n            case_score = val_score(y_case, y_pred_case)\n        else:\n            death_score = 0\n            case_score = 0\n\n        return death_score, case_score, y_pred_death, y_pred_case\n\n    def train_model(train, valid, start_lag_death, end_lag_death, num_lag_case, num_val, score=True):\n        alpha = 2\n        lr_death = Ridge(alpha=alpha, fit_intercept=False)\n        lr_case = Ridge(alpha=alpha, fit_intercept=True)\n\n        (train_death_score, train_case_score, train_pred_death, train_pred_case,\n        ) = fit_eval(lr_death, lr_case, train, start_lag_death, end_lag_death, num_lag_case, fit=True, score=score)\n\n        death_scores = []\n        case_scores = []\n\n        death_pred = []\n        case_pred = []\n\n        for i in range(num_val):\n\n            (valid_death_score, valid_case_score, valid_pred_death, valid_pred_case,\n            ) = fit_eval(lr_death, lr_case, valid, start_lag_death, end_lag_death, num_lag_case, fit=False, score=score)\n\n            death_scores.append(valid_death_score)\n            case_scores.append(valid_case_score)\n            death_pred.append(valid_pred_death)\n            case_pred.append(valid_pred_case)\n\n            if 0:\n                print('val death: %0.3f' %  valid_death_score,\n                      'val case: %0.3f' %  valid_case_score,\n                      'val : %0.3f' %  np.mean([valid_death_score, valid_case_score]),\n                      flush=True)\n            valid = update_valid_dataset(valid, valid_pred_death, valid_pred_case)\n\n        if score:\n            death_scores = np.sqrt(np.mean([s**2 for s in death_scores]))\n            case_scores = np.sqrt(np.mean([s**2 for s in case_scores]))\n            if 0:\n                print('train death: %0.3f' %  train_death_score,\n                      'train case: %0.3f' %  train_case_score,\n                      'val death: %0.3f' %  death_scores,\n                      'val case: %0.3f' %  case_scores,\n                      'val : %0.3f' % ( (death_scores + case_scores) \/ 2),\n                      flush=True)\n            else:\n                print('%0.4f' %  case_scores,\n                      ', %0.4f' %  death_scores,\n                      '= %0.4f' % ( (death_scores + case_scores) \/ 2),\n                      flush=True)\n        death_pred = np.hstack(death_pred)\n        case_pred = np.hstack(case_pred)\n        return death_scores, case_scores, death_pred, case_pred\n\n    countries = [g.split('_')[0] for g in geo_data.index]\n    countries = pd.factorize(countries)[0]\n\n    country_ids_base = countries.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n    country_ids_base.shape\n\n    geo_ids_base = np.arange(num_geo).reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    geo_ids_base = 0.1 * ohe.fit_transform(geo_ids_base)\n    geo_ids_base.shape\n\n    def val_score(true, pred):\n        pred = np.log1p(np.round(np.expm1(pred) - 0.2))\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n    def val_score(true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n\n\n    start_lag_death, end_lag_death = 14, 6,\n    num_train = 6\n    num_lag_case = 14\n    lag_period = max(start_lag_death, num_lag_case)\n\n    def get_oof(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[start_val], start_val, num_val)\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = train[['Date', 'Id', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[sub.day >= start_val]\n        sub = sub[['Id', 'ConfirmedCases', 'Fatalities']].copy()\n        return sub\n\n\n    if save_oof:\n        for start_val_delta, date in zip(range(3, -8, -3),\n                                  ['2020-03-22', '2020-03-19', '2020-03-16', '2020-03-13']):\n            print(date, end=' ')\n            oof = get_oof(start_val_delta)\n            oof.to_csv('..\/submissions\/cpmp-%s.csv' % date, index=None)\n\n    def get_sub(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = 14\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        return sub\n        return sub\n\n\n    known_test = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']\n              ].merge(test[['geo', 'day', 'ForecastId']], how='left', on=['geo', 'day'])\n    known_test = known_test[['ForecastId', 'ConfirmedCases', 'Fatalities']][known_test.ForecastId.notnull()].copy()\n    known_test\n\n    unknow_test = test[test.day > max_test_val_day]\n    unknow_test\n\n    def get_final_sub():   \n        start_val = max_test_val_day + 1\n        last_train = start_val - 1\n        num_val = max_test_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = num_val + 3\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        (_, _, val_death_preds, val_case_preds\n        ) = train_model(train_data, valid_data, start_lag_death, end_lag_death, num_lag_case, num_val, score=False)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n        print(unknow_test.shape, pred_deaths.shape, pred_cases.shape)\n\n        sub = unknow_test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        sub = pd.concat([known_test, sub])\n        return sub\n\n    if save_public_test:\n        sub = get_sub()\n    else:\n        sub = get_final_sub()\n    return sub\n\n\n\n","7ec739d7":"gc.collect()","e62e7003":"def get_nn_sub3():\n    df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\n    sub_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\n\n    coo_df = pd.read_csv(\"..\/input\/covid19week1\/train.csv\").rename(columns={\"Country\/Region\": \"Country_Region\"})\n    coo_df = coo_df.groupby(\"Country_Region\")[[\"Lat\", \"Long\"]].mean().reset_index()\n    coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n        df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n        df[\"weekend\"] = df[\"Date\"].dt.dayofweek\/\/5\n\n        df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n        df[\"Lat\"] = (df[\"Lat\"] \/\/ 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] \/\/ 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 5\n\n    features = [\"Lat\", \"Long\"]\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n\n\n    def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 208, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 104, 0.0, \"hard_sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 104, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n\n    get_model().summary()\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 100\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer=Nadam(lr=1e-4))\n         #   hist = model.fit(get_input(df), df[TARGETS],\n            #                 batch_size=1775, epochs=900, verbose=0, shuffle=True)\n            if save:\n               # model.save_weights(\"model_2_{}.h5\".format(i))\n                model.load_weights(\"..\/input\/nn100final\/model_2_{}.h5\".format(i))\n            else:\n                #model.save_weights(\"model_2_Dev_{}.h5\".format(i))\n                model.load_weights(\"..\/input\/nn100final\/model_2_Dev_{}.h5\".format(i))\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))\/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error\/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    sub_df_public = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n    sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = sub_df_private[\"Date\"].min()\n    SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n    sub_df_private = df.append(sub_df_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n    sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n    sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n    sub_df = sub_df_public.append(sub_df_private, sort=False)\n    sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n    return sub_df[[\"ForecastId\"] + TARGETS]\n\nsub20 = get_nn_sub3()\nsub10 = get_cpmp_sub()\nsub10['ForecastId'] = sub10['ForecastId'].astype('int')\n\n\nsub10.sort_values(\"ForecastId\", inplace=True)\nsub20.sort_values(\"ForecastId\", inplace=True)\n\n\nfrom sklearn.metrics import mean_squared_error\n\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n[np.sqrt(mean_squared_error(np.log1p(sub10[t].values), np.log1p(sub20[t].values))) for t in TARGETS]\n\nsub_df20 = sub10.copy()\nfor t in TARGETS:\n    sub_df20[t] = np.expm1(np.log1p(sub10[t].values)*0.5 + np.log1p(sub20[t].values)*0.5)","05e4c0c5":"gc.collect()","9a5380f2":"import pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\n\ntrain = pd.read_csv('..\/input\/covid19-global-forecasting-week-4\/train.csv')\ntrain['Date'] = pd.to_datetime(train['Date'])\ndef dealing_with_null_values(dataset):\n    dataset = dataset\n    for i in dataset.columns:\n        replace = []\n        data  = dataset[i].isnull()\n        count = 0\n        for j,k in zip(data,dataset[i]):\n            if (j==True):\n                count = count+1\n                replace.append('No Information Available')\n            else:\n                replace.append(k)\n        print(\"Num of null values (\",i,\"):\",count)\n        dataset[i] = replace\n    return dataset\ntrain = dealing_with_null_values(train)\ndef fillState(state, country):\n    if state == 'No Information Available': return country\n    return state\ntrain['Province_State'] = train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ntrain.loc[:, 'Date'] = train.Date.dt.strftime(\"%m%d\")\ntrain[\"Date\"]  = train[\"Date\"].astype(int)\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\n\ntrain.Country_Region = le.fit_transform(train.Country_Region)\ntrain.Province_State = le.fit_transform(train.Province_State)\ndata = pd.DataFrame()\ndata['Province_State']=train['Province_State']\ndata['Country_Region']=train['Country_Region']\ndata['Date']=train['Date']\n\nxmodel1 = XGBRegressor(n_estimators=1000) \nxmodel1.fit(data,train['ConfirmedCases'])\nxmodel2 = XGBRegressor(n_estimators=1000) \nxmodel2.fit(data,train['Fatalities'])\ntest = pd.read_csv('..\/input\/covid19-global-forecasting-week-4\/test.csv')\ntest['Date'] = pd.to_datetime(test['Date'], infer_datetime_format=True)\ntest = dealing_with_null_values(test)\ntest['Province_State'] = test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ntest.loc[:, 'Date'] = test.Date.dt.strftime(\"%m%d\")\ntest[\"Date\"]  = test[\"Date\"].astype(int)\nle = preprocessing.LabelEncoder()\n\ntest.Country_Region = le.fit_transform(test.Country_Region)\ntest.Province_State = le.fit_transform(test.Province_State)\n\ntest_data = pd.DataFrame()\ntest_data['Province_State']  = test['Province_State'] \ntest_data['Country_Region']  = test['Country_Region']\ntest_data['Date'] = test['Date']\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\noutput =  pd.DataFrame({'ForecastId': [], 'ConfirmedCases': [], 'Fatalities': []})\nfor i in test['Country_Region'].unique():\n    s = test[test['Country_Region'] == i].Province_State.unique()\n    for j in s:\n        output_train = data[(data['Country_Region'] == i) & (data['Province_State'] == j)]\n        output_test = test_data[(test_data['Country_Region']==i) & (test_data['Province_State'] == j)]\n    \n        output_train_with_labels = train[(train['Country_Region'] == i) & (train['Province_State'] == j)]\n    \n        index_test = test[(test['Country_Region']==i) & (test['Province_State'] == j)]\n        index_test = index_test['ForecastId']\n    \n        xmodel1 = XGBRegressor(n_estimators=2000)\n        xmodel1.fit(output_train,output_train_with_labels['ConfirmedCases'])\n    \n        xmodel2 = XGBRegressor(n_estimators=2000)\n        xmodel2.fit(output_train,output_train_with_labels['Fatalities'])\n        \n        xmodel11 = XGBRegressor(n_estimators=1500)\n        xmodel11.fit(output_train,output_train_with_labels['ConfirmedCases'])\n    \n        xmodel22 = XGBRegressor(n_estimators=1500)\n        xmodel22.fit(output_train,output_train_with_labels['Fatalities'])\n        \n        xmodel111 = XGBRegressor(n_estimators=2500)\n        xmodel111.fit(output_train,output_train_with_labels['ConfirmedCases'])\n    \n        xmodel222 = XGBRegressor(n_estimators=2500)\n        xmodel222.fit(output_train,output_train_with_labels['Fatalities'])\n        \n        xmodel1111 = XGBRegressor(n_estimators = 2000, num_parallel_tree=2)\n        xmodel1111.fit(output_train,output_train_with_labels['ConfirmedCases'])\n    \n        xmodel2222 = XGBRegressor(n_estimators = 2000, num_parallel_tree=2)\n        xmodel2222.fit(output_train,output_train_with_labels['Fatalities'])\n                                        \n        y1_xpred_output = .25*xmodel1.predict(output_test) + .25*xmodel11.predict(output_test) + .25*xmodel111.predict(output_test) + .25*xmodel1111.predict(output_test)\n        y2_xpred_output = .25*xmodel2.predict(output_test) + .25*xmodel22.predict(output_test) + .25*xmodel222.predict(output_test) + .25*xmodel2222.predict(output_test)\n    \n    \n        for_output = pd.DataFrame()\n        for_output['ForecastId'] = index_test\n        for_output['ConfirmedCases'] = y1_xpred_output\n        for_output['Fatalities']=y2_xpred_output\n    \n        output = pd.concat([output, for_output], axis=0)\n        \noutput['ForecastId']= output['ForecastId'].astype('int')","c467b017":"gc.collect()","c21314e0":"\nbuscc = output[\"ConfirmedCases\"]\nbusf = output[\"Fatalities\"]\nsdfcc = sub_df[\"ConfirmedCases\"]\nsdff = sub_df[\"Fatalities\"]\nsdfcc2 = sub_df20[\"ConfirmedCases\"]\nsdff2 = sub_df20[\"Fatalities\"]\nsdfcc3 = sub_df40[\"ConfirmedCases\"]\nsdff3 = sub_df40[\"Fatalities\"]\nsdfcc4 = df_sub1[\"ConfirmedCases\"]\nsdff4 = df_sub1[\"Fatalities\"]\nsdfcc5 = full_pred[\"ConfirmedCases\"]\nsdff5 = full_pred[\"Fatalities\"]\nsdfcc6 = df_sub2[\"ConfirmedCases\"]\nsdff6 = df_sub2[\"Fatalities\"]\nsub_df_final[\"ConfirmedCases\"] = 0.15 * buscc.values +  0.20 * sdfcc.values +  0.075 * sdfcc2.values +  0.075 * sdfcc3.values +  0.1 * sdfcc4.values +  0.3 * sdfcc5.values +  0.1 * sdfcc6.values\nsub_df_final[\"Fatalities\"] = 0.15 * busf.values  +  0.20 * sdff.values +  0.075 * sdff2.values +  0.075 * sdff3.values  +  0.1 * sdff4.values +  0.3 * sdff5.values +  0.1 * sdff6.values\nsub_df_final.to_csv('submission.csv',index=False)\n\n","72f6dee7":"I build a ship for some of the competition's kernels, most of them with some modification to the training\/prediction,\nbut they had their respective eda\/data in the baggage and models like Ridge, RidgeCV, ExtraTreesRegressor, ElasticNet, LGBM, NN and XBG etc.\nCredit to @david1013 , @haplophyrne, @osciiart, @titericz , @cpmpml and @aerdem4\n\n- @david1013 W3 25-bag version\n- @haplophyrne W3 https:\/\/www.kaggle.com\/haplophyrne\/ensemble?scriptVersionId=31615563 but added some more XGB to the ensemble.\n- @OsciiArt - I used two models, from W2 and W3, with some modifications to the LGBM parameters.\n- Team @titericz @cpmpml @aerdem4 https:\/\/www.kaggle.com\/aerdem4\/covid19-w2-final-v2.\n\nMy changes to the orginal training\/model:\nRidgeCV with Kfold over 5 folds.\nNN models each with 7 connected NN models with a last average layer. \nIn every connected NN I used different activations, Mish, Gelu, Swish, Relu, Swish-beta, and one self-made \"Phrish\"(Mish and Swish with Beta) and alternated last-layer with swish.\nFirst NN trained with RMSprop.\nSecond NN trained with Ranger but used with AdamW instead and weight_decay=1\/steps-in-epoch and Lookahead with period 15.\nIn Second NN I also loaded best saved checkpoint instead of earlystop or running all epochs.\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F3924325%2F6c9c574e47720fe1caf35aac613bded0%2Fmodel.png?generation=1586428155487874&alt=media)\nAlso added a longer tranied version of this model.\n\nAll in all over 900 models to the ensemble due to the many NN with last average layer with thier own models.\nThe weights to the ensemble are counted from the leaderboard and from my own pre-validation period.\n\nThanks for an important competition\/project and the great work from all of you. \u201cMany hands make light work.\u201d"}}