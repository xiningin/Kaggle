{"cell_type":{"3a89953d":"code","d28c4a25":"code","4a9ee338":"code","ceb77851":"code","d970d658":"code","81527e04":"code","e1cdb151":"code","6364658c":"code","810c4320":"code","21517922":"code","7b9fadb1":"code","647f5119":"code","56bf33a1":"code","501ba0da":"code","17027c44":"code","b9054c68":"code","633f296d":"code","186dc500":"code","d0a56707":"code","27a21217":"code","0ab6475e":"code","e6b487df":"code","a8330e68":"code","12ce22ac":"code","79eede60":"code","79058ea6":"code","c4c2bcf5":"code","1c3b56fe":"code","9411eb82":"code","76f9c33d":"code","0ef52559":"code","8f5402b5":"code","8fccb7c7":"code","cfd5e5eb":"code","a36864fc":"code","134d7f19":"code","327e51c8":"code","91633c04":"code","5e99cfcd":"code","e0f7fb35":"code","70c79ced":"code","51b26d69":"code","869f4d15":"code","340a3a32":"code","c66c5231":"code","3cf121d3":"code","cd122ef8":"code","1d7338f9":"code","53f38f59":"code","d859daa0":"code","119a1005":"code","66e8b97d":"code","aa111f7a":"code","e79801f4":"code","80daeaa2":"code","95b447b6":"code","c47a2747":"code","67aae776":"code","2c9cb4e1":"code","f54b9907":"code","7ad1bf98":"code","fe73137a":"code","96e2e5c4":"code","d6e3eb7c":"code","8af1b154":"code","243f702e":"code","246c280e":"code","45bfa20d":"code","d7899157":"code","9338496c":"code","0912ffca":"code","bb76ed9b":"markdown","72d0be06":"markdown","5594b27d":"markdown","5e0c8ce2":"markdown","bc5dc0fd":"markdown","1a5bf417":"markdown","fe4c17bb":"markdown","7ba0968c":"markdown","630383f2":"markdown","d28e1211":"markdown","eed83e18":"markdown","639d59ce":"markdown","099efd0e":"markdown","71ccb83b":"markdown","c6554ce4":"markdown","14f938bf":"markdown","503b15fb":"markdown","deeadf36":"markdown"},"source":{"3a89953d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d28c4a25":"import random\nrandom_seed = 0\nrandom.seed(random_seed)\nnp.random.seed(random_seed)","4a9ee338":"def RMSLE(actual, predicted):\n\n    predicted = np.array([np.log(np.abs(x+1.0)) for x in predicted])  # doing np.abs for handling neg values  \n    actual = np.array([np.log(np.abs(x+1.0)) for x in actual])\n    log_err = actual-predicted\n    \n    return 1000*np.sqrt(np.mean(log_err**2))","ceb77851":"train = pd.read_csv('\/kaggle\/input\/demand-forecasting\/train_0irEZ2H.csv')\ntest = pd.read_csv('\/kaggle\/input\/demand-forecasting\/test_nfaJ3J5.csv')\nsubmit = pd.read_csv('\/kaggle\/input\/demand-forecasting\/sample_submission_pzljTaX.csv')","d970d658":"train.head(2)","81527e04":"print(train.shape, test.shape, submit.shape)","e1cdb151":"train['store_sku'] = (train['store_id'].astype('str') + \"_\" + train['sku_id'].astype('str'))\ntest['store_sku'] = (test['store_id'].astype('str') + \"_\" + test['sku_id'].astype('str'))\nlen(train['store_sku'].unique()) - len(test['store_sku'].unique()) # checking if the combination of store and sku is same across train and test.","6364658c":"assert len(np.intersect1d(train['store_sku'].unique(), test['store_sku'].unique())) == len(test['store_sku'].unique())","810c4320":"train.info()","21517922":"temp = train[train['total_price'].isnull()]['base_price']\ntrain['total_price'] = train['total_price'].fillna(temp)","7b9fadb1":"#Appending train and test together for faster manipulation of data\ntest['units_sold'] = -1\ndata = train.append(test, ignore_index = True)","647f5119":"data.info()","56bf33a1":"print('Checking Data distribution for Train! \\n')\nfor col in train.columns:\n    print(f'Distinct entries in {col}: {train[col].nunique()}')\n    print(f'Common # of {col} entries in test and train: {len(np.intersect1d(train[col].unique(), test[col].unique()))}')","501ba0da":"data.describe()","17027c44":"train.units_sold.describe()","b9054c68":"(train[train.units_sold <= 200].units_sold).hist()","633f296d":"train['units_sold'].hist()","186dc500":"np.log1p(train['units_sold']).hist()","d0a56707":"data[['base_price', 'total_price']].plot.box()","27a21217":"# Making price based new features\n\ntrain['diff'] = train['base_price'] - train['total_price']\n\ntrain['relative_diff_base'] = train['diff']\/train['base_price']\ntrain['relative_diff_total'] = train['diff']\/train['total_price']\n\ntrain.head(2)","0ab6475e":"test['diff'] = test['base_price'] - test['total_price']\ntest['relative_diff_base'] = test['diff']\/test['base_price']\ntest['relative_diff_total'] = test['diff']\/test['total_price']\ntest.head(2)","e6b487df":"# Studying correlation between features and the target. This will help us in regression later.\ncols = ['base_price', 'total_price', 'diff', 'relative_diff_base', 'relative_diff_total'\n        , 'is_featured_sku', 'is_display_sku', 'units_sold']\ntrain[cols].corr().loc['units_sold']","a8330e68":"print(f'current # of features in cols: {len(cols)}')\ncols.remove('units_sold')\nprint(f'current # of features to be used: {len(cols)}')","12ce22ac":"from sklearn.model_selection import train_test_split\n\nX = train[cols]\ny = np.log1p(train['units_sold']) # Transforming target into normal via logarithmic operation\n\nXtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state = 1)\nprint(Xtrain.shape, ytrain.shape, Xval.shape, yval.shape)","79eede60":"Xtrain.isnull().sum()","79058ea6":"from sklearn.ensemble import RandomForestRegressor\nreg = RandomForestRegressor()\nreg.fit(Xtrain, ytrain)","c4c2bcf5":"preds = reg.predict(Xval)\nprint(f'The validation RMSLE error for baseline model is: {RMSLE(np.exp(yval), np.exp(preds))}')","1c3b56fe":"sub_preds = reg.predict(test[cols])\nsubmit['units_sold'] = np.exp(sub_preds)\nsubmit.head(2)","9411eb82":"submit.to_csv('sub_baseline_v1.csv', index = False)","76f9c33d":"from category_encoders import TargetEncoder, MEstimateEncoder\nencoder = MEstimateEncoder()\nencoder.fit(train['store_id'], train['units_sold'])\ntrain['store_encoded'] = encoder.transform(train['store_id'], train['units_sold'])\ntest['store_encoded'] = encoder.transform(test['store_id'], test['units_sold'])","0ef52559":"encoder.fit(train['sku_id'], train['units_sold'])\ntrain['sku_encoded'] = encoder.transform(train['sku_id'], train['units_sold'])\ntest['sku_encoded'] = encoder.transform(test['sku_id'], test['units_sold'])","8f5402b5":"skus = train.sku_id.unique()\nprint(skus[:2])\n\ntest_preds = test.copy()\ntest_preds.tail(2)","8fccb7c7":"def sku_model(sku, cols_to_use, reg):\n    X = train[train['sku_id'] == sku][cols_to_use]\n    y = train[train['sku_id'] == sku]['units_sold']\n    \n    Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state = 1)\n    reg.fit(X,np.log1p(y))\n    \n    y_pred = reg.predict(Xval)\n    err = RMSLE(yval, np.exp(y_pred))\n    print(f'RMSLE for {sku} is: {err}')\n    \n    preds = reg.predict(test[test['sku_id'] == sku][cols_to_use])    \n    temp_df =  pd.DataFrame.from_dict({'record_ID': test_preds[test_preds['sku_id'] == sku]['record_ID'],\n                                       'units_sold':  np.exp(preds)})\n    return err, temp_df","cfd5e5eb":"cols_to_use = cols + ['store_encoded', 'sku_encoded']","a36864fc":"err = dict() # for documenting error for each sku type\nsub = pd.DataFrame(None, columns = ['record_ID', 'units_sold'])\nreg = RandomForestRegressor(random_state = 2288)\n\nfor sku in skus:\n    err[sku], temp = sku_model(sku, cols_to_use, reg)\n    sub = sub.append(temp)\n\nprint(np.mean(list(err.values())))","134d7f19":"sub.sort_values(by = ['record_ID']).to_csv('sub_sku_RF_v2.csv', index = False)","327e51c8":"import lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score, KFold, StratifiedKFold","91633c04":"cols_to_use","5e99cfcd":"cols_to_use += ['store_id', 'sku_id']\n# For defining categorical features to the model, we will build `cat_cols`\ncat_cols = ['is_featured_sku', 'is_display_sku', 'store_id', 'sku_id']","e0f7fb35":"X = train[cols_to_use]\ny = np.log1p(train['units_sold']) # Transforming target into normal via logarithmic operation\n\nXtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state = 1)\nprint(Xtrain.shape, ytrain.shape, Xval.shape, yval.shape)","70c79ced":"Xtest = test[cols_to_use]","51b26d69":"def runLGB(Xtrain, ytrain, Xval, yval, cat_cols, Xtest = None):\n    params = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'l1',\n    #'max_depth': 9, \n    'learning_rate': 0.1\n    ,'verbose': 1\n    , \"min_data_in_leaf\" : 10\n    }\n\n    n_estimators = 800\n    early_stopping_rounds = 10\n\n    d_train = lgb.Dataset(Xtrain.copy(), label=ytrain.copy(), categorical_feature=cat_cols)\n    d_valid = lgb.Dataset(Xval.copy(), label=yval.copy(), categorical_feature=cat_cols)\n    watchlist = [d_train, d_valid]\n\n    model = lgb.train(params, d_train, n_estimators\n                      , valid_sets = [d_train, d_valid]\n                      , verbose_eval=n_estimators\n                      , early_stopping_rounds=early_stopping_rounds)\n\n    preds = model.predict(Xval, num_iteration=model.best_iteration)\n    err = RMSLE(yval, np.exp(preds))\n    \n    preds_test = model.predict(Xtest, num_iteration=model.best_iteration)\n    return  preds, err, np.exp(preds_test), model","869f4d15":"pred_val, err, pred_test,model = runLGB(Xtrain, ytrain, Xval, yval, cat_cols, Xtest)","340a3a32":"submit['units_sold'] = pred_test\nsubmit.to_csv('lgb_sub_store_sku_v3.csv', index = False)","c66c5231":"a =model.feature_importance(importance_type='split')\nfeature = pd.DataFrame(model.feature_name(), columns = ['feature'])\nfeature['importance'] = a\nfeature = feature.sort_values(by = ['importance'], ascending = False)\nfeature.head(11)","3cf121d3":"from datetime import datetime\ntrain['week'] = train['week'].astype('str')\ntrain['week'] = [datetime.strptime(x, '%d\/%m\/%y') for x in train['week']]","cd122ef8":"train.head()","1d7338f9":"test['week'] = test['week'].astype('str')\ntest['week'] = [datetime.strptime(x, '%d\/%m\/%y') for x in test['week']]","53f38f59":"import datetime\ntrain['weekend_date'] = [x + datetime.timedelta(days=6) for x in train['week']]\ntest['weekend_date'] = [x + datetime.timedelta(days=6) for x in test['week']]","d859daa0":"current_cols = list(train.columns)","119a1005":"import datetime \ndef extract_time_features(df):\n    \n    start_date = datetime.datetime(2011,1, 17)\n    \n    print('starting basic feature extraction for week start date!')\n\n    df['year'] = df['week'].dt.year\n    df['date'] = [x.day for x in df['week']]\n    df['month'] = df['week'].dt.month\n    df['weekday'] = df['week'].dt.dayofweek\n    df['weeknum'] = df['week'].dt.weekofyear\n    \n    df['week_serial']  = [divmod((x-start_date).total_seconds(), 86400)[0]\/7 for x in df['week']]\n    \n\n    '''\n    print('starting month end related feature extraction for week start date!')\n\n    df['quarter'] = [x.quarter for x in df['week']]\n    df['is_month_start'] = [x.is_month_start for x in df['week']]\n    df['is_month_end'] = [x.is_month_end for x in df['week']]\n    df['is_month_start'] = df['is_month_start'].astype(int)\n    df['is_month_end'] = df['is_month_end'].astype(int)\n    \n    df['start_week']= df.assign(start_week=pd.cut(df.date,[0,9,15,23,31],labels=[1,2,3,4]))['start_week']\n    df['start_week'] = df['start_week'].astype(int)\n    '''\n\n    print('Starting basic feature extraction for week end date!')\n    \n    df['end_year'] = df['weekend_date'].dt.year\n    df['end_date'] = [x.day for x in df['weekend_date']]\n    df['end_month'] = df['weekend_date'].dt.month\n    df['end_weekday'] = df['weekend_date'].dt.dayofweek\n    df['end_weeknum'] = df['weekend_date'].dt.weekofyear\n    df['end_week_serial']  = [divmod((x-start_date).total_seconds(), 86400)[0]\/7 for x in df['weekend_date']]\n\n    '''\n    print('starting month end related feature extraction for week start date!')\n\n    df['end_quarter'] = [x.quarter for x in df['weekend_date']]\n    df['end_is_month_start'] = [x.is_month_start for x in df['weekend_date']]\n    df['end_is_month_end'] = [x.is_month_end for x in df['weekend_date']]\n    df['end_is_month_start'] = df['end_is_month_start'].astype(int)\n    df['end_is_month_end'] = df['end_is_month_end'].astype(int)\n    \n    df['end_week'] = df.assign(end_week=pd.cut(df.end_date,[0,9,15,23,31],labels=[1,2,3,4]))['end_week']\n    df['end_week'] = df['end_week'].astype(int)\n    '''\n    return df","66e8b97d":"train = extract_time_features(train)\ntrain.tail()","aa111f7a":"train.info()","e79801f4":"test = extract_time_features(test)\ntest.tail()","80daeaa2":"def Diff(li1, li2): \n    return list(set(li1) - set(li2))\n\ntotal_cols = list(test.columns)\nnew_feat = Diff(total_cols, current_cols)\nnew_feat","95b447b6":"train[new_feat].info()","c47a2747":"print(f'The number of features used before: {len(cols_to_use)}')\nprint(f'The number of categorical features used before: {len(cat_cols)}')","67aae776":"cols_to_use += new_feat\n#cat_cols += new_feat\n\nprint(f'The number of features to be used now: {len(cols_to_use)}')\nprint(f'The number of categorical features to be used now: {len(cat_cols)}')","2c9cb4e1":"cols_to_use","f54b9907":"Xtest = test[cols_to_use]","7ad1bf98":"X = train[cols_to_use]\ny = np.log1p(train['units_sold']) # Transforming target into normal via logarithmic operation\n\nXtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state = 1)\nprint(Xtrain.shape, ytrain.shape, Xval.shape, yval.shape)","fe73137a":"pred_val, err, pred_test,model = runLGB(Xtrain, ytrain, Xval, yval, cat_cols, Xtest)","96e2e5c4":"submit['units_sold'] = pred_test\nsubmit.to_csv('lgb_time_store_sku_v4.csv', index = False)","d6e3eb7c":"a =model.feature_importance(importance_type='split')\nfeature = pd.DataFrame(model.feature_name(), columns = ['feature'])\nfeature['importance'] = a\nfeature = feature.sort_values(by = ['importance'], ascending = False)\nfeature.head(20)","8af1b154":"# making changes in the LGB model to improve the predictions (Tuning!)\ndef runLGB2(Xtrain, ytrain, Xval, yval, cat_cols, Xtest = None):\n    params = {\n    'boosting_type': 'dart', #dropout aided regressive trees (DART) # improves accuracy\n    'objective': 'regression',\n    'metric': 'l1', \n    #'max_depth': 10, \n    'learning_rate': 0.5\n    ,'verbose': 1\n    }\n    \n    #regularising for overfitting with inf depth\n    params[\"min_data_in_leaf\"] = 15 \n    params[\"bagging_fraction\"] = 0.7\n    params[\"feature_fraction\"] = 0.7\n    #params[\"bagging_freq\"] = 3\n    params[\"bagging_seed\"] = 50\n\n    n_estimators = 575\n    early_stopping_rounds = 30\n\n    d_train = lgb.Dataset(Xtrain.copy(), label=np.log1p(ytrain.copy()), categorical_feature=cat_cols)\n    d_valid = lgb.Dataset(Xval.copy(), label=np.log1p(yval.copy()), categorical_feature=cat_cols)\n    watchlist = [d_train, d_valid]\n\n    model = lgb.train(params, d_train, n_estimators\n                      , valid_sets = [d_train, d_valid]\n                      , verbose_eval=125\n                      , early_stopping_rounds=early_stopping_rounds)\n\n    preds = model.predict(Xval, num_iteration=model.best_iteration)\n    err = RMSLE(yval['units_sold'], np.exp(preds))\n    \n    preds_test = np.exp(model.predict(Xtest, num_iteration=model.best_iteration))\n    return  preds, err, preds_test, model","243f702e":"encoder.fit(train[new_feat], train['units_sold'])\ntrain[new_feat] = encoder.transform(train[new_feat], train['units_sold'])\ntest[new_feat] = encoder.transform(test[new_feat], test['units_sold'])","246c280e":"import time\n\npreds_buff = 0\nerr_buff = []\n\nX = train[cols_to_use]\ny = train[['units_sold']]\n\nn_splits = 10\nkf = StratifiedKFold(n_splits=n_splits, shuffle= True, random_state=22)\n\nfor dev_index, val_index in kf.split(X, y):\n    start = time.time()\n    Xtrain, Xval = X.iloc[dev_index], X.iloc[val_index]\n    ytrain, yval = y.iloc[dev_index], y.iloc[val_index]    \n    \n    pred_val, err, pred_test,model = runLGB2(Xtrain, ytrain, Xval, yval, cat_cols, Xtest)\n    preds_buff += pred_test\n    err_buff.append(err)\n    print(f'Mean Error: {np.mean(err_buff)}; Split error: {err}')\n    print(f'Total time in seconds for this fold: {time.time()-start}')\n    print('\\n')\n\npreds_buff \/= n_splits","45bfa20d":"print(err_buff, np.mean(err_buff))","d7899157":"submit['units_sold'] = np.abs(preds_buff)\nsubmit.to_csv('lgb_time_10cv_v5.csv', index = False)","9338496c":"a =model.feature_importance(importance_type='split')\nfeature = pd.DataFrame(model.feature_name(), columns = ['feature'])\nfeature['importance'] = a\nfeature = feature.sort_values(by = ['importance'], ascending = False)\nfeature.head(20)","0912ffca":"submit.head()","bb76ed9b":"We have good correlation of features with the target variable, hence doing a baseline regression won't be a bad start.","72d0be06":"# Extracting datetime features","5594b27d":"- Public LB score: 419.7016\n- Private LB score: ?","5e0c8ce2":"Since, the data is on (store X sku) level, let's make a separate identifier to pick it later. Below, I have concatenated the store and sku id by making new column `store_sku`. <br>\nI have also checked if the number of such combinations is same across the train and test set. By making sure it is 0, we can rest assured that no cold-start needs to be done.","bc5dc0fd":"Comparing for base and total price. Let's see if we can gain some insights about the target data from these two.","1a5bf417":"Notice that the target data is highly skewed. To make accurate predictions, we must normalise it before using.","fe4c17bb":"#### Leaderboard scores:\n- Public: 731.0198\n- Private: ?\n\nNot a bad start!\nThe time series data that we have behaves differently for different sku and hence we should try fitting multiple models for each sku\/store, i.e., different models for different store_sku combination. Let's try that out and see if that makes any improvement.","7ba0968c":"Leaderboard scores:\n1. After adding basic time related features\n    - Public LB score: 363.6342\n    - Private LB score: ?\n2. After encoding time related features\n    - Public LB score: 365.7862\n    - Private LB score: ?\n3. After fine-tuning the hyperparameters with 10 fold CV\n    - Public LB score: 360.7086\n    - Private LB score: ?","630383f2":"# Baseline Regression","d28e1211":"# LightGBM Regressor","eed83e18":"# SKU level base model fitting","639d59ce":"## Training LGB Model with all the date\/time features created","099efd0e":"Without using any date and time features, we have scored `419.7016` on LB which is a huge improvement.\nLet's see if we can extract any useful information from the `week` feature.\nWe will try to build numeric features based on the week's start as well as end date. <br>\nI'll also be using 5-fold CV to strengthen the predictions later.","71ccb83b":"- Public LB score: 377.3146\n- Private LB score: ","c6554ce4":"Till now we have done:\n- RF regressor for all data at once: Public LB score `731.0198`\n- RF regressor for data at sku level: Public LB score `481.0016`\n\nWow! That's a huge improvement. This proves that using data at sku level will certainly help the cause.","14f938bf":"Notice that one entry for `total_price` is null in the train set. Lets replace it with the `base_price` for now.","503b15fb":"Before fitting the model, we would like to encode our store and sku information. There are multiple ways of doing this:\n    1. One-hot encoding\n    2. Label encoding\n    3. Category encoding\nSince, the features here have high cardinality, we should go for Category encoding. I'll be using `MEstimateEncoder` for this purpose.\nYou can find its documentation at this link: http:\/\/contrib.scikit-learn.org\/category_encoders\/mestimate.html","deeadf36":"Now, we would also like to incorporate the `store_id` as one of the defining features of the multiple models that we are building. But as the problem statement defines, there are 76 different models for each one of them across each sku will be a herculean task. \nThen, how do we do it?\n\nWe will be using LightGBM's categorical features' input to our rescue. This can be used to provide categorical inputs to the model with a single line of code. Moreover, as the documentation says, it is 8 times faster than one-hot encoding. Find its link here.\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html#categorical_feature"}}