{"cell_type":{"97b083ab":"code","cabcbe1a":"code","1fc21089":"code","719cedbe":"code","14c2d456":"code","9c7ad953":"code","a2a7647f":"code","edea6e40":"code","17d22de2":"code","cfd7dd3f":"markdown","6e2fa22e":"markdown","c73d5e39":"markdown","46bebc89":"markdown","a9a9625e":"markdown","dcd9aadf":"markdown","1b76a219":"markdown","13f133b7":"markdown","d63b8009":"markdown","7d36143a":"markdown","ee8f87fa":"markdown","1abd2580":"markdown","03892505":"markdown"},"source":{"97b083ab":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nfrom matplotlib.colors import ListedColormap","cabcbe1a":"class AdalineGD(object):\n    \n    \"\"\"Adaline classifier.\n    Parameters\n    ------------\n    eta : float\n        Learning rate (between 0.0 and 1.0)\n    n_iter : int\n        Passes over the training dataset.\n    random_state : int\n        Random number generator seed for random weight initialization.\n        \n    Attributes\n    -----------\n    w_ : 1d-array\n        Weights after fitting.\n    cost_ : list\n        Sum-of-squares cost function value in each epoch.\n    \"\"\"\n    \n    def __init__(self,eta = 0.01,n_iter=50,random_state=1):\n        self.eta = eta;\n        self.n_iter = n_iter;\n        self.random_state = random_state;\n    \n    def fit(self,X,Y):\n        \"\"\"Fit training data.\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and n_features is the number of features.\n        Y : array-like, shape = [n_samples]\n            Target values.\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # defining random generator\n        rgen = np.random.RandomState(self.random_state);\n        # initializing weights randomly from a normal distribution with mean 0 and std 0.01 because\n        # we don't want the weights to be all zero for effective learning. Here w_[0] is the bias b\n        self.w_ = rgen.normal(loc=0.0,scale=0.01,size = 1 + X.shape[1]);\n        \n        self.cost_ = [];\n        \n        # running through the dataset for n_iter epochs\n        for _ in range(self.n_iter):\n            # applying Gradient Decsent on whole dataset\n            # getting Z = W^T*X + b\n            net_input = self.net_input(X);\n            # getting the output i.e. activation(Z)\n            output = self.activation(net_input);\n            # getting the errors i.e (Y - Y^)\n            errors = (Y - output);\n            # updating weigths i.e. w = w + alpha * dJ\/dw = w + aplha * (Y - Y^) * X, here alpha ie the learning rate\n            self.w_[1:] += self.eta * X.T.dot(errors);\n            # updating bias i.e b = b + alpha * sum(Y - Y^), here Y and Y^ are matrices hence summing all elements\n            self.w_[0] += self.eta * errors.sum();\n            # findong the cost i.e. 1\/2 * sum(Y - Y^)^2\n            cost = (errors**2).sum()\/2.0;\n            # recording cost at each epoch\n            self.cost_.append(cost);\n        return self;\n    \n    def net_input(self,X):\n        \"\"\"Calculate net input\"\"\"\n        # calculating Y(or Z) = W^T * X + b, here X is matrix with all the training examples\n        # print(X)\n        # print(self.w_[1:])\n        return np.dot(X,self.w_[1:]) + self.w_[0];\n    \n    def activation(self,X):\n        \"\"\"Compute linear activation\"\"\"\n        return X;\n    \n    def predict(self,X):\n        \"\"\"Return class label after unit step\"\"\"\n        # return correct class label based upon Y^i(or Zi), here X is ith training example\n        return np.where(self.net_input(X) >= 0.0,1,-1);","1fc21089":"df = pd.read_csv('..\/input\/Iris.csv');\ndf.tail()","719cedbe":"# select setosa and versicolor samples\n# an overview of first 100 samples\ndf.iloc[0:100,5].value_counts();\n# get the class labels for first 100 samples\nY = df.iloc[0:100,5].values;\n# set class labels as 1 and -1\nY = np.where(Y == 'Iris-setosa',-1,1);\n# select sepal and petal length freatures\nX = df.iloc[0:100,[1,3]].values\n\n# plot the data for setosa class\nplt.scatter(X[:50,0],X[:50,1],color='red',marker='o',label='setosa')\n# plot the data for versicolor class\nplt.scatter(X[50:100,0],X[50:100,1],color='blue',marker='x',label='vesicolor')\nplt.xlabel('sepal length[cm]')\nplt.ylabel('petal length[cm]')\nplt.legend(loc='upper left')\nplt.show()","14c2d456":"# get the class labels for first 100 samples\nY = df.iloc[0:100,5].values;\n# set class labels as 1 and -1\nY = np.where(Y == 'Iris-setosa',-1,1);\n# select sepal and petal length freatures\nX = df.iloc[0:100,[1,3]].values\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\nada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, Y)\nax[0].plot(range(1, len(ada1.cost_) + 1),np.log10(ada1.cost_), marker='o')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('log(Sum-squared-error)')\nax[0].set_title('Adaline - Learning rate 0.01')\nada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, Y)\nax[1].plot(range(1, len(ada2.cost_) + 1),ada2.cost_, marker='o')\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Sum-squared-error')\nax[1].set_title('Adaline - Learning rate 0.0001')\nplt.show()","9c7ad953":"# making a copy of the dataset\nX_std = np.copy(X)\n# standardizing sepal length\nX_std[:,0] = (X[:,0] - X[:,0].mean()) \/ X[:,0].std()\n# standardizing petal length\nX_std[:,1] = (X[:,1] - X[:,1].mean()) \/ X[:,1].std()","a2a7647f":"ada = AdalineGD(n_iter=15, eta=0.01)\nada.fit(X_std, Y)\nplt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o')\nplt.xlabel('Epochs')\nplt.ylabel('Sum-squared-error')\nplt.show()","edea6e40":"def plot_decision_regions(X,Y,model,resolution = 0.02):\n    # setup colormap and markers\n    markers = ('s','x','o','^','v');\n    colors = ('red','blue','lightgreen','gray','cyan');\n    cmap = ListedColormap(colors[:len(np.unique(Y))]);\n    # plot the decision surface\n    # determin minimim ans maximum values for the two features\n    x1_min,x1_max =  X[:,0].min() - 1, X[:,0].max() + 1;\n    x2_min,x2_max =  X[:,1].min() - 1, X[:,1].max() + 1; \n    # create a pair of grid arrays\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),np.arange(x2_min, x2_max, resolution))\n    # predict on our model by flattening the arrays and making it into a matrix\n    Z = model.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    \n    # plot class samples\n    for idx, cl in enumerate(np.unique(Y)):\n        plt.scatter(x=X[Y == cl, 0],y=X[Y == cl, 1],alpha=0.8,c=colors[idx],marker=markers[idx],label=cl,edgecolor='black')","17d22de2":"plot_decision_regions(X_std, Y, model=ada)\nplt.title('Adaline - Gradient Descent')\nplt.xlabel('sepal length [standardized]')\nplt.ylabel('petal length [standardized]')\nplt.legend(loc='upper left')\nplt.tight_layout()\nplt.show()","cfd7dd3f":"Import necessary packages:","6e2fa22e":"Now we will visualize the first 100 samples(50 for iris-versicolor class i.e. 1 and another 50 for iris-setosa class i.e. -1) and will use only two feature i.e. sepal lenght and petal length:","c73d5e39":"Now lets see how our AdalineGD model converges on a learning rate of 0.01:","46bebc89":"Load the Iris dataset in a panda dataframe:\n\n","a9a9625e":"Creating the model(classifier) via Object Oriented approach:\n\n","dcd9aadf":"Voilla, we have achieved a good convergence with a small learning rate.","1b76a219":"Looks identical to the Vanilla Perceptron's(https:\/\/www.kaggle.com\/rajmiglani\/vanilla-perceptron-py-ml-1) output right? Yes but it doesn't suffer from the gotcha that the classes need to be linearly seperable by a hyperplane(or a line in case of binary classification).","13f133b7":"One way to faster converge the cost function to the Global Minimum is to use feature scaling. One of many available is standardization which sets the data to resemble standard normal distribution with 0 mean and 1 standard deviation. Experiments prove that the cost function J converges faster after standardization of the features.\n\nWell lets work it out on our dataset:\n","d63b8009":"This notebook implements the Adaline(Adaptive Linear Neuron) model as explained in the book \"**Python Machine Learning**\" by **Sebastian Raschka and Vahid Mirjalili**.\n\n**Prerequisites:**\n\nPython\npandas\nnumpy\n\n**Dataset:** IRIS\n\n**Note: **Descriptive comments explain the code in a better way","7d36143a":"What we can observe above is that a small learning rate of 0.01 never finds the Global Minimum(always overshoots it) and thus the cost keeps on increasing as in figure on the left. On the other hand the right graph with learning rate of 0.0001 shows that the cost decreases with increasing epochs and this is the right way to go ahead. Just one gotcha here, keeping the learning rate too small might require many more epochs for the cost function to converge at the Global Minimum.","ee8f87fa":"Plot the cost for the two different learning rates i.e. 0.01 and 0.0001:","1abd2580":"Plot the decision boundary on our data:","03892505":"Standardize the dataset i.e. subtract the sample mean of the feature from each training example and divide by the sample standard deviation of the feature. We will do it for all the features :"}}