{"cell_type":{"72f486b9":"code","412d567b":"code","1cd55736":"code","b762d02a":"code","dead96a6":"code","7f398727":"code","e46d8099":"code","2d7afb9f":"code","84c5856d":"code","3783473e":"code","032174cb":"code","bf3deb57":"code","0763c576":"code","124c5312":"code","c73dd051":"code","bc7fcf81":"code","71f660e7":"markdown","d9518ae2":"markdown","b5991ebc":"markdown","e696fb51":"markdown","e3094edb":"markdown","f46249c3":"markdown","d4207ed9":"markdown","0230340e":"markdown","b766f2db":"markdown","ebdf8476":"markdown","65092104":"markdown","15348eae":"markdown","0f7ea128":"markdown","557b7c4d":"markdown"},"source":{"72f486b9":"import numpy as np\nimport pandas as pd \nimport os\nfrom matplotlib import pyplot as plt \nimport seaborn as sns\nimport nltk\nimport re\nfrom sklearn.metrics import classification_report, accuracy_score\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential, layers, preprocessing\nimport warnings\nwarnings.filterwarnings('ignore')\nos.chdir('\/kaggle\/input\/andalusian-hotels-reviews-unbalanced')","412d567b":"df = pd.read_csv('Big_AHR.csv', index_col=0)\ndf = df[df.label != 3] # Dropping the neutral reviews \ndf.head()","1cd55736":"# some useful functions to eliminate the noise from texts and measure sequence length\ndef clean_text(text):\n    return re.sub('[^A-Za-z0-9\u00e1\u00e9\u00ed\u00f3\u00fa]', ' ', text.lower())\n\n\ndef seq_len(text):\n    return len(text.split(' '))","b762d02a":"df.review_text = df.review_text.apply(lambda x: clean_text(x))\ndf['len'] = df.review_text.apply(lambda x: seq_len(x))\ndf.head()","dead96a6":"print('Maximum sequence length is', df.len.max())\nprint('Minimum sequence length is', df.len.min())\nprint('Average sequence length is', df.len.mean())","7f398727":"from sklearn.model_selection import train_test_split\nX = np.array(df['review_text'])\ny = np.array(df['label'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=327)\n\nprint('Train set contains {} entries'.format(len(X_train)))\nprint(\"Test set contains {} entries\".format(len(y_test)))","e46d8099":"sns.set_theme(style=\"darkgrid\")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\nfig.suptitle('Review per label distribution')\nax1 = sns.countplot(x=y_train, ax=ax1)\nax1.set_title('Train Set')\nax2 = sns.countplot(x=y_test, ax=ax2)\nax2.set_title('Test Set')\nplt.show()","2d7afb9f":"num_words = 28000 # the numer of words to tokenize \nmax_seq = 300 # maximum sequence size\n\n# fit the tokenizer on trainig texts\ntokenizer = preprocessing.text.Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(X_train)\n\n# Dictionary of words ordered in terms of their frequency \nword_index = tokenizer.word_index\nvocab_size = len(word_index) + 1\n\nprint(f\" {vocab_size-1} unique tokens found\")\nprint(list(word_index)[:10])","84c5856d":"print(\"Original phrase:\\n\", X_train[0])\nprint()\n\n# Turn the phrases into their numerical equivalents \nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\nprint('Phrase as sequence:', X_train_seq[0])\nprint()\n\n# Padding of our sequences \nX_train_processed = preprocessing.sequence.pad_sequences(X_train_seq, padding='post', maxlen=max_seq)\nX_test_processed = preprocessing.sequence.pad_sequences(X_test_seq, padding='post', maxlen=max_seq)\nprint('Padded sequence:', X_train_processed[0])","3783473e":"os.chdir('\/kaggle\/input\/pretrained-word-vectors-for-spanish')\nembedding_index = {}\nwith open(\"SBW-vectors-300-min5.txt\") as f:\n  for line in f:\n    word, coef = line.split(maxsplit=1)\n    coef = np.fromstring(coef, 'f', sep=' ')\n    embedding_index[word] = coef \nprint('{} embeddings found'.format(len(embedding_index)))","032174cb":"embedding_dim = 300\nhits = 0 \nmisses = 0\n\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor w, i in word_index.items():\n  embedding_vector = embedding_index.get(word)\n  if embedding_vector is not None:\n    hits+=1\n    embedding_matrix[i] = embedding_vector\n  else:\n    misses += 1\n\nprint('{} words encoded with an embedding from SBW-vectors'.format(hits))\nprint('{} word embeddings not found'.format(misses))","bf3deb57":"model = Sequential()\n\nmodel.add(\n    layers.Embedding(input_dim=vocab_size,\n                     embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n                     mask_zero=True,\n                     output_dim=embedding_dim, # dimension of dense embedding. \n                     input_length=max_seq)\n)\n\n# LSTM layer\nmodel.add(layers.LSTM(128))\n\n# Dense output layer\nmodel.add(layers.Dense(units=1, activation='sigmoid'))\n\n# Model compilation \nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n#Model recap\nmodel.summary()","0763c576":"with tf.device('\/gpu:0'):\n    history = model.fit(\n    x=X_train_processed,\n    y=y_train,\n    epochs=10, \n    batch_size=128\n)","124c5312":"# Loss plot\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()\n\n# Accuracy plot\nplt.plot(history.history['accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.show()","c73dd051":"y_pred = model.predict(x=X_test_processed, batch_size=128)\ny_pred = [0 if y < 0.5 else 1 for y in y_pred]\n\nprint(classification_report(y_test, y_pred))\nprint('Accuracy:', accuracy_score(y_test, y_pred))","bc7fcf81":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred, normalize='true')\nsns.heatmap(cm, annot=True, cmap=\"YlGnBu\")\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion matrix')\nplt.show()","71f660e7":"## Using pre-trained word embeddings\n\nIn this step we are going to unpack all the word vectors from the .txt file into embedding_index - a dictionary, where the keys are the words and the values are the 300-dimension vectors.","d9518ae2":"## Padding\n\nOnce we have our dictionary we can encode each text as a sequence of numbers which correspond to their position of the word_index dictionary. \n\nAs LSTMs process sequence of a fixed lenght, we need to normalize our collection, making all sequence of the same length. We have set the maximum length parameter at 300, taking into account that the longest review contains 1,134 words, but the average number of words is ~90,8. The sequences longer than 300 elements will be truncated, while shorter sequences will be filled with zeros. This process is called **padding**.","b5991ebc":"We are working with the big and unbalanced version of AHR Corpus, so there are much more positive reviews than negatives. Let's check if it is going to leverage model's performance.  ","e696fb51":"# Initializing the model\nTo adress the problem of binary classification we are going to compile a three-layer model organized as follows:\n* **Embedding layer** that takes the sequences and converts their elements into corresponding word vectors from the previously generated embedding matrix \n* **LSTM layer**\n* **Dense layer**","e3094edb":"# Feature representation\nOnce we have our training and evaluation sets ready, we can jump into the next process, that is feature representation. \n\nAs we said in the introduction, we are going to work with pre-trainded word-embeddings for spanish, introduced by FastText in 2018 in this [paper](http:\/\/https:\/\/arxiv.org\/abs\/1802.06893). \n\nWord-embeddings, also known as word vectors are distributed feature representation of words' semantic meaning. The vectors we use were learned from a corpus of Spanish texts extracted from Wikipedia and Common Crawl with a total number of 11,951,805 tokens. The result collection consists of 1,000,654 300-dimensional word vectors. \n\nTo use pretrained word embeddings we need first to **tokenize** our reviews and **extract the vocabulary** of the collection. Once we have our collection of unique tokens that appear in AHR, we'll generate **a dictionary of words ordered in terms of their frequency** to proceed with the encoding of our sequences.","f46249c3":"# Pre-processing \nPre-processing is an essentian step in every NLP task, since it contributes to normalize the texts and eliminate the noise. In this case we will:\n* convert our texts to lowercase \n* eliminate special characters and punctuation signs","d4207ed9":"# Testing and Results\n\nAs we can se on the classification report down below, the model, despite of it's simplicity, works well on both classes. ","0230340e":"As LSTMs take as input sequences of fixed length, we need to know how long our reviews are.","b766f2db":"# Training\n\nAfter some experiments, I've concluded that the optimal number of trainig epochs can be set at 10.","ebdf8476":"Now, when our data is ready, it's time to split it into training and evaluation subsets. ","65092104":"Then we are going to generate an embedding_matrix whose dimensions coincide with the size of the corpus' vocabulary (27660 + the OOV token) and the dimesions of the embeddings (300). This embedding matrix will stand for initializer in the first layer of our model. ","15348eae":"We also can note that it identifies the positive reviews slightly better that the negatives, which is probably due to the class imbalance shown on the preprocessing step. ","0f7ea128":"Thank you for your attention. \nDon't forget to upvote if you find my work interesting and to express our opinion, doubts and suggestions in the comment section. ","557b7c4d":"# Introduction \nIn this notebook we are going to solve a **sentiment analysis problem**, that is binary classification of hotels reviews based on their overall subjective burden. For this task we selected AHR Corpus - a dataset of spanish texts retrieved from TripAdvisor. \n\nTo classify reviews between negative and positive, we are going to compile a three-layer Long Short-Term Memory (LSTM) neural network. This model will encode each word with it's semantic vector extracted from the FastText's word vector collection. "}}