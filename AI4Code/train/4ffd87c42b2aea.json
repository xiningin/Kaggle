{"cell_type":{"ca669707":"code","51a784a6":"code","6d20432c":"code","49df3526":"code","f044a14e":"code","8bc2d200":"code","7938fdcd":"code","78e3688c":"code","7c750269":"code","00112385":"code","b60d6e54":"code","e8371b66":"code","dbf1b732":"code","660bab86":"code","8906acf2":"code","748cc30c":"code","7168f715":"code","0be9c151":"code","e4d8ec79":"code","b9dd0cd8":"code","f6d4e10c":"code","79604602":"code","7bf7cdd8":"code","f83d26c0":"code","19e2d81e":"code","fef127d6":"code","eac4a528":"code","9aee2187":"code","e399c09e":"code","213068a2":"code","3b0dbd04":"code","5128af8e":"code","82e606f0":"code","55d9eb45":"code","8af06403":"code","88ed4cd9":"code","21715d5b":"code","809e4a8d":"code","07461d68":"code","4302f9ed":"code","fd24dec7":"code","fc336479":"code","fb6df281":"code","ecb7e4e6":"code","78dca9e1":"code","bcfe5931":"code","70462919":"code","f16b0746":"code","9b0cd8aa":"code","09e46e8f":"markdown","7e559791":"markdown","41a12813":"markdown","c75b732c":"markdown","490caa96":"markdown","53add6f7":"markdown","14cecdc3":"markdown","fbf2b451":"markdown","369809f1":"markdown","56ed7e69":"markdown","4d8df371":"markdown","63cf7ab9":"markdown","638a082b":"markdown","014e799a":"markdown","2a6b48d9":"markdown","d8a1220c":"markdown","aad6487f":"markdown","9d4017a8":"markdown","1c2cfc2b":"markdown","74b05e6f":"markdown","b7bf814b":"markdown","a4d4f9b7":"markdown","4c5393d8":"markdown","acd11413":"markdown","9618d65a":"markdown","e4e86e9e":"markdown","fe965a3f":"markdown"},"source":{"ca669707":"# Setting up environment\nimport os\nimport pandas as pd\nimport copy\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import levene\nimport re\n%matplotlib inline\nsns.set(color_codes=True)\n\n# Setting up environment for language pre processing\nfrom nltk.tokenize import word_tokenize\nimport nltk\nfrom nltk.corpus import stopwords\nimport copy\nimport string\nfrom textblob import Blobber\nfrom textblob.sentiments import NaiveBayesAnalyzer","51a784a6":"# importing our data from .csv\ndata = pd.read_csv('..\/input\/wine-reviews\/winemag-data-130k-v2.csv' )\ndata.drop(data.filter(regex=\"Unname\"),axis=1, inplace=True)","6d20432c":"# inspecting the shape of the dataset\nprint(f'The dataframe has', data.shape[0], 'rows and', data.shape[1], 'columns.')\ndata.head()","49df3526":"# inspecting the type of the variables\nprint(data.dtypes)\ndata.points.describe()[['min', 'max']]","f044a14e":"# correct the type variables\ndata.country = data['country'].astype('category')\ndata.province = data['province'].astype('category')\ndata.variety = data['variety'].astype('category')","8bc2d200":"# inspecting for duplicates\ndup_rows = data[data.duplicated(\n    subset=['description','title','taster_name','winery'])].sort_values(by='title')\n\n# We found some duplicates. And so, we'll delete them.\nprint(f'The dataframe has', dup_rows.shape[0], 'duplicates that we need to remove.')\nclean_data = copy.deepcopy(data.drop_duplicates(subset=['description','title','taster_name', 'winery']))\nprint(f'The dataframe now has', clean_data.shape[0], 'rows instead of', data.shape[0], 'rows.')","7938fdcd":"print(clean_data.isnull().sum())","78e3688c":"# inspecting missing values in designation\nclean_data['designation'] = clean_data['designation'].fillna(clean_data.title)\nis_designation_in_title = clean_data.apply(lambda x: x['designation'] in x['title'] , axis=1).astype(int)\n\nprint(f' When we subtract the sum of binary variable is_designation_in_title to the number of rows'\n      f' in our original dataset we obtain the value',is_designation_in_title.sum() - clean_data.shape[0])","7c750269":"#cleanning up environment\ndel is_designation_in_title\ndel dup_rows\n\n# removing multiple columns (designation, region_1, region_2, taster_name and\n# taster_twitter_handle\nclean_data = clean_data.drop(['designation', 'region_1', 'region_2', 'taster_name',\n                        'taster_twitter_handle'], axis =1)\nprint(f'The dataframe now has', clean_data.shape[1],'columns instead of',\n      data.shape[1])\n\n# inspecting for missing values\nprint(clean_data.isnull().sum())","00112385":"#Dropping rows where nan values are found in the columns country and variety\nclean_data = clean_data.dropna(subset=['country', 'variety'])\nprint(clean_data.isnull().sum())","b60d6e54":"# creating new binary variable describing whether or not the observation doesn't\n# have a price.\nclean_data['has_price'] = clean_data['price'].notnull().astype(int)\npoints_with_price= clean_data['points'][clean_data['has_price'] == 1]\npoints_without_price= clean_data['points'][clean_data['has_price'] == 0]\n\n# box-plot of the two groups\nsns.boxplot(x=clean_data['has_price'], y=clean_data['points'])","e8371b66":"# levene test to test for equality of variance\nlevene(points_with_price,points_without_price)\n","dbf1b732":"t_stat, p_value = stats.ttest_ind(points_with_price,points_without_price, equal_var=False)\nprint(f't_stat  = {t_stat:+4.4f}')\nprint(f'p_value =  {p_value:+4.4f}')","660bab86":"# cleaning up environment\ndel points_with_price\ndel points_without_price\n\n# imputation\nclean_data['price'].fillna(clean_data['price'].mean(), inplace = True)\nprint(clean_data.isnull().sum())","8906acf2":"# make a deepcopy of the final dataset\ndf = copy.deepcopy(clean_data)","748cc30c":"# descriptive statistics of the variable points\ndf.points.describe()","7168f715":"# visualizations of points\nplt.figure(figsize=(9, 8))\nsns.set(style=\"ticks\")\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True\\\n    ,gridspec_kw={\"height_ratios\": (.15, .85)})\n\nsns.boxplot(df['points'], ax=ax_box)\nsns.distplot(df['points'], ax=ax_hist, bins=20, kde=False)\n\nax_box.set(yticks=[])\nsns.despine(ax=ax_hist)\nsns.despine(ax=ax_box, left=True)\n\nprint(f'Skewness : {df.points.skew():+4.2f}\\n'\n      f'Kurtosis : {df.points.kurt():+4.2f}')","0be9c151":"# descriptive statistics of the variable price\ndf.price.describe()","e4d8ec79":"# visualizations of price\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True\\\n    ,gridspec_kw={\"height_ratios\": (.15, .85)})\n\nsns.boxplot(df['price'], ax=ax_box)\nsns.distplot(df['price'], ax=ax_hist, kde=False)\n\nax_box.set(yticks=[])\nsns.despine(ax=ax_hist)\nsns.despine(ax=ax_box, left=True)\n\nprint(f'Skewness : {df.price.skew():+4.2f}\\n'\n      f'Kurtosis : {df.price.kurt():+4.2f}')","b9dd0cd8":"# visualizations of log price\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True\\\n    ,gridspec_kw={\"height_ratios\": (.15, .85)})\n\nsns.boxplot(np.log(df['price']), ax=ax_box)\nsns.distplot(np.log(df['price']), ax=ax_hist, kde=False)\n\nax_box.set(yticks=[])\nsns.despine(ax=ax_hist)\nsns.despine(ax=ax_box, left=True)\n\nprint(f'Skewness : {np.log(df.price).skew():+4.2f}\\n'\n      f'Kurtosis : {np.log(df.price).kurt():+4.2f}')","f6d4e10c":"# create new variable log_price\ndf['log_price'] =np.log(df.price)","79604602":"df['num_wine_from_winery'] = df.groupby(['winery'])['country'].transform(np.size)","7bf7cdd8":"print(df.title[213])\nprint(df.title[1530])\nprint(df.title[2262])\nprint(df.title[63])","f83d26c0":"# function to extract the largest 4-digits value\ndef extractmax(str1):\n    nums=re.findall(\"(?:19|20)[0-9][0-9]\",str1)\n    return max(nums, default=0)\n\n# https:\/\/stackoverflow.com\/questions\/21544159\/python-finding-largest-integer-in-string","19e2d81e":"# extracting the vintage\nyears= [extractmax(line) for line in df['title']]\nyears = np.array(years)\ndf['vintage'] = years","fef127d6":"# changing the type of `vintage` to float \ndf.vintage = df.vintage.astype(str).astype(float)","eac4a528":"# replacing the zeros with nan\ndf['vintage'].replace(0, np.nan, inplace=True)","9aee2187":"# checking if our tactic worked\n\nprint(f'title                                           vintage\\n\\n'\n      f'{df.title[213]:<20}{df.vintage[213]:>22}\\n'\n      f'{df.title[1530]:<20}{df.vintage[1530]:>13}\\n'\n      f'{df.title[2262]:<20}{df.vintage[2262]:>8}\\n'\n      f'{df.title[63]:<20}{df.vintage[63]:>12}\\n')","e399c09e":"# creating variables to play with\ndf['has_vintage'] = df['vintage'].notnull().astype(int)\npoints_with_vintage= df['points'][df['has_vintage'] == 1]\npoints_without_vintage= df['points'][df['has_vintage'] == 0]\n\n# box-plot of the two groups\nsns.boxplot(x=df['has_vintage'], y=df['points'])\n","213068a2":"# levene test to test for equality of variance\nlevene(points_with_vintage,points_without_vintage)\n","3b0dbd04":"# We can test whether the rows with missing data differ\n# from the ones without missing data on target\n\nt_stat, p_value = stats.ttest_ind(points_with_vintage,points_without_vintage, equal_var=False)\nprint(f't_stat  = {t_stat:+4.4f}')\nprint(f'p_value =  {p_value:+4.4f}')","5128af8e":"# imputation of vintage with the mean\ndf['vintage'].fillna(df['vintage'].mean(), inplace = True)\nprint(df.isnull().sum())","82e606f0":"# descriptive statistics \ndf.vintage.describe()","55d9eb45":"# visualization of vintage\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True\\\n    ,gridspec_kw={\"height_ratios\": (.15, .85)})\n\nsns.boxplot(df.vintage, ax=ax_box)\nsns.distplot(df.vintage, ax=ax_hist, kde=False)\n\nax_box.set(yticks=[])\nsns.despine(ax=ax_hist)\nsns.despine(ax=ax_box, left=True)\n\nprint(f'Skewness : {df.vintage.skew():+4.2f}\\n'\n      f'Kurtosis : {df.vintage.kurt():+4.2f}')","8af06403":"# dropping unrealistic observations\ndf = df.drop(df[df.vintage > 2050].index)","88ed4cd9":"# checking the mean of wines with old vintage\nx = df[df.vintage < 1920]\nprint(f'The average price of the wines with a vintage under year 1920 is {x.price.mean():.2f}$') ","21715d5b":"# checking out the prices of old wines starting from 1970\nold_wines = df[df.vintage<1970]\nprint(old_wines.price)","809e4a8d":"# dropping unrealistic observations\ndf = df.drop(df[(df.vintage < 1970) & (df.price<50)].index)                 \n","07461d68":"f, (ax_box, ax_hist) = plt.subplots(2, sharex=True\\\n    ,gridspec_kw={\"height_ratios\": (.15, .85)})\n\nsns.boxplot(df.vintage, ax=ax_box)\nsns.distplot(df.vintage, ax=ax_hist, kde=False)\n\nax_box.set(yticks=[])\nsns.despine(ax=ax_hist)\nsns.despine(ax=ax_box, left=True)\n\nprint(f'Skewness : {df.vintage.skew():+4.2f}\\n'\n      f'Kurtosis : {df.vintage.kurt():+4.2f}')\n","4302f9ed":"# create a copy description to work with\ndesc = list(copy.deepcopy(df['description']))\ndesc[0]","fd24dec7":"# creating a set of punctuation\npunc = set(string.punctuation)\n\n#loading stop_words\nnltk.download('stopwords')\n\n# creating a set of stop words\nstop_words = set(stopwords.words('english'))\n\n# combining the 2 sets with an \"or\" operator (i.e. \"|\")\nall_stops = stop_words | punc\n\n# loop to pre-process data\nclean_desc =[]\nfor item in desc:\n    tok_desc = word_tokenize(item)\n    lower_data = [i.lower() for i in tok_desc]\n    tok_desc_no_num = [i for i in lower_data if i.isalpha()]\n    filtered_desc = [i for i in tok_desc_no_num if i not in all_stops]\n    clean_desc.append(filtered_desc)","fc336479":"# Organizing the data in a new dataframe\nclean_desc_untok = [' '.join(i) for i in clean_desc]\ncolumn_names = ['original_desc', 'cleaned_description', 'untok_description']\ndata_tuple= list(zip(clean_data['description'], clean_desc, clean_desc_untok))\ndesc_df = pd.DataFrame(data_tuple, columns=column_names)","fb6df281":"# Setting up the blobber with a Naive Bayes Analyzer\ntb = Blobber(analyzer=NaiveBayesAnalyzer())\nblob = [tb(text) for text in desc_df['untok_description']]\nsentiment_values = [text.sentiment for text in blob]","ecb7e4e6":"# example of positive sentiment\nprint(f'For this string: \\n {desc[1]} \\n\\n We got this analysis:\\n {sentiment_values[1]}')","78dca9e1":"# example of negative sentiment\nprint(f'For this string: \\n {desc[111]} \\n\\n We got this analysis:\\n {sentiment_values[111]}')","bcfe5931":"# creating a dataframe that isolate each sentiment values\nstats = pd.DataFrame(zip(*sentiment_values)).T\nstats.columns=['classification','pos','neg']","70462919":"# Add the positive sentiment score to the clean dataframe\ndf['pos_sentiment'] = 0\ndf['pos_sentiment'] = list(stats.pos)","f16b0746":"# save dataframe\ndf.to_pickle('processed_data.p')","9b0cd8aa":"print(f'The final dataframe has {df.shape[0]} rows and {df.shape[1]} columns.')\n ","09e46e8f":"### Variable: `positive_sentiment` obtained with `description`\n\nIn this sub-section, we'll use a lexicon based framework for NLP to detect the sentiment in the description\nof wines. In order to obtain a positive sentiment score, we need to pre-process the text found\nin the column `description`. The pre-processing helps to present the text data in a way that the\nsentiment analysis more precise.\n\nPre-processing includes:\n1. Stripping away the punctuation\n2. Removing [stop-words](https:\/\/towardsdatascience.com\/stop-words-in-nlp-5b248dadad47)\n3. Convert all the characters to lowercase\n4. [Tokenizing](https:\/\/www.guru99.com\/tokenize-words-sentences-nltk.html) each words\n5. Removing all alpha numeric characters like 123456{}#$?*","7e559791":"At a confidence level of 95%, we can reject the null hypothesis. We can conclude that there's \nsignificant difference between the two means observed (p-value = 0.0000 < 0.05). The average\nscore of priced wines are lower than the unpriced wines. For that reason, **we have an indication\nthat the missing prices are not missing at random.** \n\nWe'll impute the missing values with the average of the `price`.","41a12813":"GOOD! We are almost done with our data wrangling! There are few lines with missing\nvalues for `country`, `province` and `variety` so we'll get rid of\nthese lines.","c75b732c":"The target variable `points` is a integer that range between 80 and 100. Let's see if our dataset\nhas duplicated rows. We have 12 *X* variables. There are 11 textual variables and 1 float variable.\nOf the 11 textual variables, at least 3 variables should be transform as categorical variables. \n","490caa96":"## Conclusion\n\n","53add6f7":"## Feature engineering\n\n### Variable: `num_wine_from_winery`\n\nWe'll start by creating a new variable `num_wine_from_winery`. This variable will\ngive a sense of the size of the winery by counting the number of wines reviewed\nfrom a winery. Let's say that winery that have many wines reviewed by the magazine\nwill tend to have larger wineyards. ","14cecdc3":"You made it to the end!\n\nIn this notebook, we organized the data in a more suitable way to make predictions for the `points` variable. \n\nTHANK YOU for taking the time to read my code. ","fbf2b451":"Clearly, we do not have the same story here!\n\nThe distribution is squished on the left-side with many outliers on the right.\nIn order words, the variable `price` is definitely not normally distributed.\nThese graphical observation are also confirmed with the positive high values\nobtained for skewness and kurtosis.\n\nSince, we are planning to try linear models to predict the target variable\nlatter in this notebook, we'll go ahead and create a new variable `log(price)`.\nThe log transformation will linearize the variable. And therefore, linear\nmodels will be able to better grasp information in relation to the target\nvariable.\n\nBelow, we can see that the log transformation normalizes in a good enough a way\nthe distribution of the `price` variable.","369809f1":"## Environment and Loading Data","56ed7e69":"### Quick EDA on the variable `vintage`\n\nLike when we looked at the missing values for `price`, we'll try to identify whether the lines with\nmissing values for `vintage` are missing at random or not.\n\n\n","4d8df371":"## Data Wrangling\n### Getting familiar with the data","63cf7ab9":"The samples doesn't seem that have the same variance p-value < 0.05.","638a082b":"### Variable: `vintage`\n\nAs mentioned in the introduction of this dataset, a `vintage` variable can be created by examining the \nvariable `title`. This operation can be tricky because some of the row contains 2 years. See the example\nbelow.","014e799a":"![vineyard.jpg](attachment:vineyard.jpg)\n<span>Photo by <a href=\"https:\/\/unsplash.com\/@svenwilhelm?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Sven Wilhelm<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/wine?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash<\/a><\/span>","2a6b48d9":"At a confidence level of 95%, we can reject the null hypothesis. We can conclude that there's \nsignificant difference between the two means observed (p-value = 0.0000 < 0.05). The average\nscore of wines with an unknown vintage are different than the wines with a known vintage. For\nthat reason, we have **an indication that the missing vintage are not missing at random.** Therefore, it\ndoesn't seems reasonable to drop rows with missing vintage since we are confident that they\nare representing a sub-population when comparing the feature points.\n\nWe'll impute the missing values with the average of the `price`.\n","d8a1220c":"Our tactic for engineering this feature will be to extract all four-digits numeric values from the string `title`.\nAt the same time, we'll only consider numeric values that either starts with `19`or `20` (since, we don't want to\nextract the value 3055 for instance). Then, from this list, only the largest numeric value will be use for the\nvariable `vintage`. (N.B. when no value is found, the default value will be 0.)","aad6487f":"### Missing values\n\nLet's see if our dataset has missing values.","9d4017a8":"We can see that our dataset is not so clean. There are a lot of missing values\nin multiple columns. Let's try to clean that up. But, first, we will drop a few\ncolumns such as `designation`, `region_1`, `region_2`, `taster_name`,\n`taster_twitter_handle`.\n\nWe don't need to keep the twitter handles since we are not planning to\ncollect additional data through the Twitter API. We will discard the taster's names\nas well because we assume that the experts scored the wines objectively. (This\nis a big assumption but it will simplify our analysis greatly for the time\nbeing.)\n\nSince, there almost no missing value for geographical variable `provinces`,\nwe'll get rid of the more precise geographical variables such as `region_1`\nand  `region_2`.\n\nFinally, if we take a look at the text values in the\n`designation` column, we see that, its values are always contained in the\n`title` column.","1c2cfc2b":"### Missing values for `price`\n\nThere are now only 8391 missing values in the column `price`. Since, it's\nprobably a variable that could bring a lot a explicative power, let's examine it\ncloser. First, we'll check whether the target variable as the same distribution\nwhen the column `price`contains a value or when it is missing. \n\nThe goal of this inspection is to identify whether the lines with missing values\nare missing at random or not.","74b05e6f":"Dealing with the ouliers helped impove the skewness and the kurtosis. However, even with these\nmanipulations, the distribution of `vintage` is not normal. We'll keep it like that for now.","b7bf814b":"This average doesn't make any sense for wines with vintages that are that old! It must be an error. Let's delete it.","a4d4f9b7":"We have a clean dataset named `df` ready to go!\n\n## EDA\n\nIn this section, we'll look into more details the variables we are left to work\nwith. We'll get ourselves familiar with the dataset using visual methods and\ndescriptive statistics. Let's start with the target variable: `points`.\n\n### EDA on `points`\n\nLike noted before, we already know that the `points` variable ranges from 80\nto 100. But what else?","4c5393d8":"## Introduction\n\nHello there! This is my first notebook, I'm open to any comments, suggestions or improvements!\nPlease don't hesitate to leave me a comment or upvote it if you found it useful.\n\n\nIn this notebook, We'll take a look at the wine-reviews dataset. So the game plan is to go through these steps:\n\n1. Setting up environment\n2. Loading Data\n3. Data Wrangling\n4. EDA (Exploratory data analysis)\n5. Feature Engineering\n    - Number of wine from each winery\n    - Vintage\n    - Sentiment analysis score with *TextBlob*\n6. Conclusion\n    \nLet's get started!","acd11413":"### Duplicates","9618d65a":"We can see with the histogram that the variable `points` seems normally\ndistributed. The skewness score (0.04) is pretty close to 0. Thus, we're\nconfident in saying that the distribution is symmetric. Also, the tails of the\ndistribution are not too narrow or too flat. The kurtosis score (-1 <-0.33 < 1)\nconfirmed the same evaluation. There are two outliers on the higher end of the\nspectrum but nothing to worry about.\n\nLet's look at the distribution of the variable price.\n\n### EDA on `price`","e4e86e9e":"The samples doesn't seem that have the same variance p-value < 0.05.","fe965a3f":"Here, the distribution is definitely not normal. The tails are very short (kurtosis =  25.83).\nThe distribution is negatively skewed. Also, we can see that there's wine with a vintage from the\nfuture... We'll that take observation away. Let's take a look at the ouliers."}}