{"cell_type":{"b51e6187":"code","e0323ece":"code","98d8b445":"code","75ae1359":"code","ebfccebf":"code","d9968670":"code","680f6084":"code","1f591322":"code","2afb34da":"code","93f6bd33":"code","1dbf28ac":"code","7b3693a4":"code","18e5db63":"code","6c8c5781":"code","d1bee883":"code","916ee01a":"code","249988f6":"code","a10eb7be":"code","806bea23":"code","6680d5f3":"code","e8a892a3":"code","120e6425":"code","38d3198d":"code","f3bb2ac1":"code","0edca248":"code","679955a3":"code","45634a42":"code","9c4b1f06":"code","d4e35d6d":"code","eba07de5":"code","6ff2e3ed":"code","3eba72dc":"code","719da44b":"code","fb8d283e":"code","4af8f8dd":"code","ffdd80da":"code","deccc5b7":"code","29059639":"code","10d51a6d":"code","8e78b8e1":"code","a689dc58":"code","306bfddd":"code","08fcf87c":"markdown","438cdfed":"markdown","04b8e29f":"markdown","76616027":"markdown"},"source":{"b51e6187":"# !pip uninstall tensorflow\n# !pip3 install --upgrade tensorflow-gpu\n# # !pip install --upgrade tensorflow-gpu\n# !pip install grpcio>=1.24.3 --upgrade\n# # !pip install -U keras-tuner\n# !pip install tensorflow-addons\n!pip install keras-rectified-adam\n!pip install sounddevice\n!apt-get -y install libsndfile1\n","e0323ece":"#%tensorflow_version 2.x\nimport tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif device_name != '\/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))","98d8b445":"import os\nos.environ['TF_KERAS'] = '1'\n!echo $TF_KERAS","75ae1359":"from keras_radam import RAdam\n\n","ebfccebf":"import librosa\nimport pandas as pd\nimport os\nimport datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport IPython.display as ipd\nimport librosa.display\nimport scipy\nimport glob\nimport numpy as np\nimport math\nimport warnings\nimport pickle\nfrom sklearn.utils import shuffle\n\n# Load the TensorBoard notebook extension.\n%load_ext tensorboard","d9968670":"from tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","680f6084":"tf.random.set_seed(999)\nnp.random.seed(999)","1f591322":"!apt-get -y install libportaudio2\n!apt-get -y install libasound-dev\n\nimport numpy as np\nimport pickle\nimport librosa\nimport sounddevice as sd\nimport tensorflow as tf\nimport multiprocessing\nimport os\nfrom sklearn.preprocessing import StandardScaler\n\n\ndef inverse_stft_transform(stft_features, window_length, overlap):\n    return librosa.istft(stft_features, win_length=window_length, hop_length=overlap)\n\n\ndef revert_features_to_audio(features, phase, window_length, overlap, cleanMean=None, cleanStd=None):\n    # scale the outpus back to the original range\n    if cleanMean and cleanStd:\n        features = cleanStd * features + cleanMean\n\n    phase = np.transpose(phase, (1, 0))\n    features = np.squeeze(features)\n    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n\n    features = np.transpose(features, (1, 0))\n    return inverse_stft_transform(features, window_length=window_length, overlap=overlap)\n\n\ndef play(audio, sample_rate):\n    # ipd.display(ipd.Audio(data=audio, rate=sample_rate))  # load a local WAV file\n    sd.play(audio, sample_rate, blocking=True)\n\n\ndef add_noise_to_clean_audio(clean_audio, noise_signal):\n    if len(clean_audio) >= len(noise_signal):\n        # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n        while len(clean_audio) >= len(noise_signal):\n            noise_signal = np.append(noise_signal, noise_signal)\n\n    ## Extract a noise segment from a random location in the noise file\n    ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n\n    noiseSegment = noise_signal[ind: ind + clean_audio.size]\n\n    speech_power = np.sum(clean_audio ** 2)\n    noise_power = np.sum(noiseSegment ** 2)\n    noisyAudio = clean_audio + np.sqrt(speech_power \/ noise_power) * noiseSegment\n    return noisyAudio\n\ndef read_audio(filepath, sample_rate, normalize=True):\n    audio, sr = librosa.load(filepath, sr=sample_rate)\n    if normalize is True:\n        div_fac = 1 \/ np.max(np.abs(audio)) \/ 3.0\n        audio = audio * div_fac\n        # audio = librosa.util.normalize(audio)\n    return audio, sr\n\n\ndef prepare_input_features(stft_features, numSegments, numFeatures):\n    noisySTFT = np.concatenate([stft_features[:, 0:numSegments - 1], stft_features], axis=1)\n    stftSegments = np.zeros((numFeatures, numSegments, noisySTFT.shape[1] - numSegments + 1))\n\n    for index in range(noisySTFT.shape[1] - numSegments + 1):\n        stftSegments[:, :, index] = noisySTFT[:, index:index + numSegments]\n    return stftSegments\n\n\ndef get_input_features(predictorsList):\n    predictors = []\n    for noisy_stft_mag_features in predictorsList:\n        # For CNN, the input feature consisted of 8 consecutive noisy\n        # STFT magnitude vectors of size: 129 \u00d7 8,\n        # TODO: duration: 100ms\n        inputFeatures = prepare_input_features(noisy_stft_mag_features)\n        # print(\"inputFeatures.shape\", inputFeatures.shape)\n        predictors.append(inputFeatures)\n\n    return predictors\n\n\ndef _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _float_feature(value):\n    \"\"\"Returns a float_list from a float \/ double.\"\"\"\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\n\ndef _int64_feature(value):\n    \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef get_tf_feature(noise_stft_mag_features, clean_stft_magnitude, noise_stft_phase):\n    noise_stft_mag_features = noise_stft_mag_features.astype(np.float32).tostring()\n    clean_stft_magnitude = clean_stft_magnitude.astype(np.float32).tostring()\n    noise_stft_phase = noise_stft_phase.astype(np.float32).tostring()\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        'noise_stft_phase': _bytes_feature(noise_stft_phase),\n        'noise_stft_mag_features': _bytes_feature(noise_stft_mag_features),\n        'clean_stft_magnitude': _bytes_feature(clean_stft_magnitude)}))\n    return example\n\n\t\n#utils over\n\n#feature extractor\nclass FeatureExtractor:\n    def __init__(self, audio, *, windowLength, overlap, sample_rate):\n        self.audio = audio\n        self.ffT_length = windowLength\n        self.window_length = windowLength\n        self.overlap = overlap\n        self.sample_rate = sample_rate\n        self.window = scipy.signal.hamming(self.window_length, sym=False)\n\n    def get_stft_spectrogram(self):\n        return librosa.stft(self.audio, n_fft=self.ffT_length, win_length=self.window_length, hop_length=self.overlap,\n                            window=self.window, center=True)\n\n    def get_audio_from_stft_spectrogram(self, stft_features):\n        return librosa.istft(stft_features, win_length=self.window_length, hop_length=self.overlap,\n                             window=self.window, center=True)\n\n    def get_mel_spectrogram(self):\n        return librosa.feature.melspectrogram(self.audio, sr=self.sample_rate, power=2.0, pad_mode='reflect',\n                                              n_fft=self.ffT_length, hop_length=self.overlap, center=True)\n\n    def get_audio_from_mel_spectrogram(self, M):\n        return librosa.feature.inverse.mel_to_audio(M, sr=self.sample_rate, n_fft=self.ffT_length,\n                                                    hop_length=self.overlap,\n                                                    win_length=self.window_length, window=self.window,\n                                                    center=True, pad_mode='reflect', power=2.0, n_iter=32, length=None)\n\t\t\t\t\t\t\t\t\t\t\t\t\t\n#Dataset\nclass Dataset:\n    def __init__(self, clean_filenames, noise_filenames, **config):\n        self.clean_filenames = clean_filenames\n        self.noise_filenames = noise_filenames\n        self.sample_rate = config['fs']\n        self.overlap = config['overlap']\n        self.window_length = config['windowLength']\n        self.audio_max_duration = config['audio_max_duration']\n\n    def _sample_noise_filename(self):\n        return np.random.choice(self.noise_filenames)\n\n    def _remove_silent_frames(self, audio):\n        trimed_audio = []\n        indices = librosa.effects.split(audio, hop_length=self.overlap, top_db=20)\n\n        for index in indices:\n            trimed_audio.extend(audio[index[0]: index[1]])\n        return np.array(trimed_audio)\n\n    def _phase_aware_scaling(self, clean_spectral_magnitude, clean_phase, noise_phase):\n        assert clean_phase.shape == noise_phase.shape, \"Shapes must match.\"\n        return clean_spectral_magnitude * np.cos(clean_phase - noise_phase)\n\n    def get_noisy_audio(self, *, filename):\n        return read_audio(filename, self.sample_rate)\n\n    def _audio_random_crop(self, audio, duration):\n        audio_duration_secs = librosa.core.get_duration(audio, self.sample_rate)\n\n        ## duration: length of the cropped audio in seconds\n        if duration >= audio_duration_secs:\n            # print(\"Passed duration greater than audio duration of: \", audio_duration_secs)\n            return audio\n\n        audio_duration_ms = math.floor(audio_duration_secs * self.sample_rate)\n        duration_ms = math.floor(duration * self.sample_rate)\n        idx = np.random.randint(0, audio_duration_ms - duration_ms)\n        return audio[idx: idx + duration_ms]\n\n    def _add_noise_to_clean_audio(self, clean_audio, noise_signal):\n        if len(clean_audio) >= len(noise_signal):\n            # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n            while len(clean_audio) >= len(noise_signal):\n                noise_signal = np.append(noise_signal, noise_signal)\n\n        ## Extract a noise segment from a random location in the noise file\n        ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n\n        noiseSegment = noise_signal[ind: ind + clean_audio.size]\n\n        speech_power = np.sum(clean_audio ** 2)\n        noise_power = np.sum(noiseSegment ** 2)\n        noisyAudio = clean_audio + np.sqrt(speech_power \/ noise_power) * noiseSegment\n        return noisyAudio\n\n    def parallel_audio_processing(self, clean_filename):\n\n        clean_audio, _ = read_audio(clean_filename, self.sample_rate)\n\n        # remove silent frame from clean audio\n        clean_audio = self._remove_silent_frames(clean_audio)\n\n        noise_filename = self._sample_noise_filename()\n\n        # read the noise filename\n        noise_audio, sr = read_audio(noise_filename, self.sample_rate)\n\n        # remove silent frame from noise audio\n        noise_audio = self._remove_silent_frames(noise_audio)\n\n        # sample random fixed-sized snippets of audio\n        clean_audio = self._audio_random_crop(clean_audio, duration=self.audio_max_duration)\n\n        # add noise to input image\n        noiseInput = self._add_noise_to_clean_audio(clean_audio, noise_audio)\n\n        # extract stft features from noisy audio\n        noisy_input_fe = FeatureExtractor(noiseInput, windowLength=self.window_length, overlap=self.overlap,\n                                          sample_rate=self.sample_rate)\n        noise_spectrogram = noisy_input_fe.get_stft_spectrogram()\n\n        # Or get the phase angle (in radians)\n        # noisy_stft_magnitude, noisy_stft_phase = librosa.magphase(noisy_stft_features)\n        noise_phase = np.angle(noise_spectrogram)\n\n        # get the magnitude of the spectral\n        noise_magnitude = np.abs(noise_spectrogram)\n\n        # extract stft features from clean audio\n        clean_audio_fe = FeatureExtractor(clean_audio, windowLength=self.window_length, overlap=self.overlap,\n                                          sample_rate=self.sample_rate)\n        clean_spectrogram = clean_audio_fe.get_stft_spectrogram()\n        # clean_spectrogram = cleanAudioFE.get_mel_spectrogram()\n\n        # get the clean phase\n        clean_phase = np.angle(clean_spectrogram)\n\n        # get the clean spectral magnitude\n        clean_magnitude = np.abs(clean_spectrogram)\n        # clean_magnitude = 2 * clean_magnitude \/ np.sum(scipy.signal.hamming(self.window_length, sym=False))\n\n        clean_magnitude = self._phase_aware_scaling(clean_magnitude, clean_phase, noise_phase)\n\n        scaler = StandardScaler(copy=False, with_mean=True, with_std=True)\n        noise_magnitude = scaler.fit_transform(noise_magnitude)\n        clean_magnitude = scaler.transform(clean_magnitude)\n\n        return noise_magnitude, clean_magnitude, noise_phase\n\n    def create_tf_record(self, *, prefix, subset_size, parallel=True):\n        counter = 0\n        p = multiprocessing.Pool(multiprocessing.cpu_count())\n\n        for i in range(0, len(self.clean_filenames), subset_size):\n\n            tfrecord_filename = prefix + '_' + str(counter) + '.tfrecords'\n\n            if os.path.isfile(tfrecord_filename):\n                print(f\"Skipping {tfrecord_filename}\")\n                counter += 1\n                continue\n\n            writer = tf.io.TFRecordWriter(tfrecord_filename)\n            clean_filenames_sublist = self.clean_filenames[i:i + subset_size]\n            print(*clean_filenames_sublist)\n            print(f\"Processing files from: {i} to {i + subset_size}\")\n            if parallel:\n                out = p.map(self.parallel_audio_processing, clean_filenames_sublist)\n            else:\n                out = [self.parallel_audio_processing(filename) for filename in clean_filenames_sublist]\n\n            for o in out:\n                noise_stft_magnitude = o[0]\n                clean_stft_magnitude = o[1]\n                noise_stft_phase = o[2]\n\n                noise_stft_mag_features = prepare_input_features(noise_stft_magnitude, numSegments=8, numFeatures=129)\n\n                noise_stft_mag_features = np.transpose(noise_stft_mag_features, (2, 0, 1))\n                clean_stft_magnitude = np.transpose(clean_stft_magnitude, (1, 0))\n                noise_stft_phase = np.transpose(noise_stft_phase, (1, 0))\n\n                noise_stft_mag_features = np.expand_dims(noise_stft_mag_features, axis=3)\n                clean_stft_magnitude = np.expand_dims(clean_stft_magnitude, axis=2)\n\n                for x_, y_, p_ in zip(noise_stft_mag_features, clean_stft_magnitude, noise_stft_phase):\n                    y_ = np.expand_dims(y_, 2)\n                    example = get_tf_feature(x_, y_, p_)\n                    writer.write(example.SerializeToString())\n\n            counter += 1\n            writer.close()\n#Mozilla\nclass MozillaCommonVoiceDataset:\n\n    def __init__(self, basepath, *, val_dataset_size):\n        self.basepath = basepath\n        self.val_dataset_size = val_dataset_size\n\n    def _get_common_voice_filenames(self, dataframe_name='train.tsv'):\n        mozilla_metadata = pd.read_csv(os.path.join(self.basepath, dataframe_name), sep='\\t')\n        clean_files = mozilla_metadata['path'].values\n        np.random.shuffle(clean_files)\n        print(\"Total number of training examples:\", len(clean_files))\n        return clean_files\n\n    def get_train_val_filenames(self):\n        clean_files = self._get_common_voice_filenames(dataframe_name='train.tsv')\n\n        # resolve full path\n        clean_files = [os.path.join(self.basepath, 'train',  filename) for filename in clean_files]\n\n        clean_files = clean_files[:-self.val_dataset_size]\n        clean_val_files = clean_files[-self.val_dataset_size:]\n        print(\"# of Training clean files:\", len(clean_files))\n        print(\"# of  Validation clean files:\", len(clean_val_files))\n        return clean_files, clean_val_files\n\n\n    def get_test_filenames(self):\n        clean_files = self._get_common_voice_filenames(dataframe_name='test.tsv')\n\n        # resolve full path\n        clean_files = [os.path.join(self.basepath, 'test',  filename) for filename in clean_files]\n\n        print(\"# of Testing clean files:\", len(clean_files))\n        return clean_files\n#Urban\nnp.random.seed(999)\n\n\nclass UrbanSound8K:\n    def __init__(self, basepath, *, val_dataset_size, class_ids=None):\n        self.basepath = basepath\n        self.val_dataset_size = val_dataset_size\n        self.class_ids = class_ids\n\n    def _get_urban_sound_8K_filenames(self):\n        urbansound_metadata = pd.read_csv(os.path.join(self.basepath, 'UrbanSound8K.csv'))\n\n        # shuffle the dataframe\n        urbansound_metadata.reindex(np.random.permutation(urbansound_metadata.index))\n\n        return urbansound_metadata\n\n    def _get_filenames_by_class_id(self, metadata):\n\n        if self.class_ids is None:\n            self.class_ids = np.unique(metadata['classID'].values)\n            print(\"Number of classes:\", self.class_ids)\n\n        all_files = []\n        file_counter = 0\n        for c in self.class_ids:\n            per_class_files = metadata[metadata['classID'] == c][['slice_file_name', 'fold']].values\n            per_class_files = [os.path.join(self.basepath,  'fold' + str(file[1]), file[0]) for file in\n                               per_class_files]\n            print(\"Class c:\", str(c), 'has:', len(per_class_files), 'files')\n            file_counter += len(per_class_files)\n            all_files.extend(per_class_files)\n\n        assert len(all_files) == file_counter\n        return all_files\n\n    def get_train_val_filenames(self):\n        urbansound_metadata = self._get_urban_sound_8K_filenames()\n\n        # folds from 0 to 9 are used for training\n        urbansound_train = urbansound_metadata[urbansound_metadata.fold != 10]\n\n        urbansound_train_filenames = self._get_filenames_by_class_id(urbansound_train)\n        np.random.shuffle(urbansound_train_filenames)\n\n        # separate noise files for train\/validation\n        urbansound_val = urbansound_train_filenames[-self.val_dataset_size:]\n        urbansound_train = urbansound_train_filenames[:-self.val_dataset_size]\n        print(\"Noise training:\", len(urbansound_train))\n        print(\"Noise validation:\", len(urbansound_val))\n\n        return urbansound_train, urbansound_val\n\n    def get_test_filenames(self):\n        urbansound_metadata = self._get_urban_sound_8K_filenames()\n\n        # fold 10 is used for testing only\n        urbansound_train = urbansound_metadata[urbansound_metadata.fold == 10]\n\n        urbansound_test_filenames = self._get_filenames_by_class_id(urbansound_train)\n        np.random.shuffle(urbansound_test_filenames)\n\n        print(\"# of Noise testing files:\", len(urbansound_test_filenames))\n        return urbansound_test_filenames\n\nclass Dataset:\n    def __init__(self, clean_filenames, noise_filenames, **config):\n        self.clean_filenames = clean_filenames\n        self.noise_filenames = noise_filenames\n        self.sample_rate = config['fs']\n        self.overlap = config['overlap']\n        self.window_length = config['windowLength']\n        self.audio_max_duration = config['audio_max_duration']\n\n    def _sample_noise_filename(self):\n        return np.random.choice(self.noise_filenames)\n\n    def _remove_silent_frames(self, audio):\n        trimed_audio = []\n        indices = librosa.effects.split(audio, hop_length=self.overlap, top_db=20)\n\n        for index in indices:\n            trimed_audio.extend(audio[index[0]: index[1]])\n        return np.array(trimed_audio)\n\n    def _phase_aware_scaling(self, clean_spectral_magnitude, clean_phase, noise_phase):\n        assert clean_phase.shape == noise_phase.shape, \"Shapes must match.\"\n        return clean_spectral_magnitude * np.cos(clean_phase - noise_phase)\n\n    def get_noisy_audio(self, *, filename):\n        return read_audio(filename, self.sample_rate)\n\n    def _audio_random_crop(self, audio, duration):\n        audio_duration_secs = librosa.core.get_duration(audio, self.sample_rate)\n\n        ## duration: length of the cropped audio in seconds\n        if duration >= audio_duration_secs:\n            # print(\"Passed duration greater than audio duration of: \", audio_duration_secs)\n            return audio\n\n        audio_duration_ms = math.floor(audio_duration_secs * self.sample_rate)\n        duration_ms = math.floor(duration * self.sample_rate)\n        idx = np.random.randint(0, audio_duration_ms - duration_ms)\n        return audio[idx: idx + duration_ms]\n\n    def _add_noise_to_clean_audio(self, clean_audio, noise_signal):\n        if len(clean_audio) >= len(noise_signal):\n            # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n            while len(clean_audio) >= len(noise_signal):\n                noise_signal = np.append(noise_signal, noise_signal)\n\n        ## Extract a noise segment from a random location in the noise file\n        ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n\n        noiseSegment = noise_signal[ind: ind + clean_audio.size]\n\n        speech_power = np.sum(clean_audio ** 2)\n        noise_power = np.sum(noiseSegment ** 2)\n        noisyAudio = clean_audio + np.sqrt(speech_power \/ noise_power) * noiseSegment\n        return noisyAudio\n\n    def parallel_audio_processing(self, clean_filename):\n        \n       \n        clean_audio, _ = read_audio(clean_filename, self.sample_rate)\n\n        # remove silent frame from clean audio\n        clean_audio = self._remove_silent_frames(clean_audio)\n\n        noise_filename = self._sample_noise_filename()\n\n        # read the noise filename\n        noise_audio, sr = read_audio(noise_filename, self.sample_rate)\n\n        # remove silent frame from noise audio\n        noise_audio = self._remove_silent_frames(noise_audio)\n\n        # sample random fixed-sized snippets of audio\n        clean_audio = self._audio_random_crop(clean_audio, duration=self.audio_max_duration)\n\n        # add noise to input image\n        noiseInput = self._add_noise_to_clean_audio(clean_audio, noise_audio)\n\n        # extract stft features from noisy audio\n        noisy_input_fe = FeatureExtractor(noiseInput, windowLength=self.window_length, overlap=self.overlap,\n                                          sample_rate=self.sample_rate)\n        noise_spectrogram = noisy_input_fe.get_stft_spectrogram()\n\n        # Or get the phase angle (in radians)\n        # noisy_stft_magnitude, noisy_stft_phase = librosa.magphase(noisy_stft_features)\n        noise_phase = np.angle(noise_spectrogram)\n\n        # get the magnitude of the spectral\n        noise_magnitude = np.abs(noise_spectrogram)\n\n        # extract stft features from clean audio\n        clean_audio_fe = FeatureExtractor(clean_audio, windowLength=self.window_length, overlap=self.overlap,\n                                          sample_rate=self.sample_rate)\n        clean_spectrogram = clean_audio_fe.get_stft_spectrogram()\n        # clean_spectrogram = cleanAudioFE.get_mel_spectrogram()\n\n        # get the clean phase\n        clean_phase = np.angle(clean_spectrogram)\n\n        # get the clean spectral magnitude\n        clean_magnitude = np.abs(clean_spectrogram)\n        # clean_magnitude = 2 * clean_magnitude \/ np.sum(scipy.signal.hamming(self.window_length, sym=False))\n\n        clean_magnitude = self._phase_aware_scaling(clean_magnitude, clean_phase, noise_phase)\n\n        scaler = StandardScaler(copy=False, with_mean=True, with_std=True)\n        noise_magnitude = scaler.fit_transform(noise_magnitude)\n        clean_magnitude = scaler.transform(clean_magnitude)\n\n        return noise_magnitude, clean_magnitude, noise_phase\n\n    def create_tf_record(self, *, prefix, subset_size, parallel=True):\n        counter = 0\n        p = multiprocessing.Pool(multiprocessing.cpu_count())\n\n        for i in range(0, len(self.clean_filenames), subset_size):\n            \n            tfrecord_filename = prefix + '_' + str(counter) + '.tfrecords'\n\n            if os.path.isfile(tfrecord_filename):\n                print(f\"Skipping {tfrecord_filename}\")\n                counter += 1\n                #continue\n\n            writer = tf.io.TFRecordWriter(tfrecord_filename)\n            clean_filenames_sublist = self.clean_filenames[i:i + subset_size]\n\n            print(f\"Processing files from: {i} to {i + subset_size}\")\n            print(*clean_filenames_sublist)\n            \n            if parallel:\n                out = p.map(self.parallel_audio_processing, clean_filenames_sublist)\n            else:\n                out = [self.parallel_audio_processing(filename) for filename in clean_filenames_sublist]\n\n            for o in out:\n                noise_stft_magnitude = o[0]\n                clean_stft_magnitude = o[1]\n                noise_stft_phase = o[2]\n\n                noise_stft_mag_features = prepare_input_features(noise_stft_magnitude, numSegments=8, numFeatures=129)\n\n                noise_stft_mag_features = np.transpose(noise_stft_mag_features, (2, 0, 1))\n                clean_stft_magnitude = np.transpose(clean_stft_magnitude, (1, 0))\n                noise_stft_phase = np.transpose(noise_stft_phase, (1, 0))\n\n                noise_stft_mag_features = np.expand_dims(noise_stft_mag_features, axis=3)\n                clean_stft_magnitude = np.expand_dims(clean_stft_magnitude, axis=2)\n\n                for x_, y_, p_ in zip(noise_stft_mag_features, clean_stft_magnitude, noise_stft_phase):\n                    y_ = np.expand_dims(y_, 2)\n                    example = get_tf_feature(x_, y_, p_)\n                    writer.write(example.SerializeToString())\n\n            counter += 1\n            writer.close()\n\nimport warnings\n\nwarnings.filterwarnings(action='ignore')\n\nmozilla_basepath = '..\/input\/tamilvoiceclipsmozilla\/'\nurbansound_basepath = '..\/input\/ultrasound\/'\n\nmcv = MozillaCommonVoiceDataset(mozilla_basepath, val_dataset_size=1000)\nclean_train_filenames, clean_val_filenames = mcv.get_train_val_filenames()\n\nus8K = UrbanSound8K(urbansound_basepath, val_dataset_size=200)\nnoise_train_filenames, noise_val_filenames = us8K.get_train_val_filenames()\n\nwindowLength = 256\nconfig = {'windowLength': windowLength,\n          'overlap': round(0.25 * windowLength),\n          'fs': 16000,\n          'audio_max_duration': 0.8}\n\n#val_dataset = Dataset(clean_val_filenames, noise_val_filenames, **config)\n#val_dataset.create_tf_record(prefix='val', subset_size=2000)\n\n#train_dataset = Dataset(clean_train_filenames, noise_train_filenames, **config)\n#train_dataset.create_tf_record(prefix='train', subset_size=3000)\n\n## Create Test Set\n#clean_test_filenames = mcv.get_test_filenames()\n\n#noise_test_filenames = us8K.get_test_filenames()\n\n#test_dataset = Dataset(clean_test_filenames, noise_test_filenames, **config)\n#test_dataset.create_tf_record(prefix='test', subset_size=1000, parallel=False)\n\n\n","2afb34da":"train_tfrecords_filenames = glob.glob('..\/input\/tamilvoiceclipsmozilla\/tfRecords\/train_*')\nnp.random.shuffle(train_tfrecords_filenames)\ntrain_tfrecords_filenames = list(train_tfrecords_filenames)\nprint(train_tfrecords_filenames)\nval_tfrecords_filenames = glob.glob('..\/input\/tamilvoiceclipsmozilla\/tfRecords\/val_*')\n#!ls '..\/input\/tamilvoiceclipsmozilla\/tfRecords\/'","93f6bd33":"windowLength = 256\noverlap      = round(0.25 * windowLength) # overlap of 75%\nffTLength    = windowLength\ninputFs      = 48e3\nfs           = 16e3\nnumFeatures  = ffTLength\/\/2 + 1\nnumSegments  = 8\nprint(\"windowLength:\",windowLength)\nprint(\"overlap:\",overlap)\nprint(\"ffTLength:\",ffTLength)\nprint(\"inputFs:\",inputFs)\nprint(\"fs:\",fs)\nprint(\"numFeatures:\",numFeatures)\nprint(\"numSegments:\",numSegments)","1dbf28ac":"mozilla_basepath = '..\/input\/tamilvoiceclipsmozilla\/test\/'\nUrbanSound8K_basepath = '..\/input\/ultrasound\/'","7b3693a4":"def tf_record_parser(record):\n    keys_to_features = {\n        \"noise_stft_phase\": tf.io.FixedLenFeature((), tf.string, default_value=\"\"),\n        'noise_stft_mag_features': tf.io.FixedLenFeature([], tf.string),\n        \"clean_stft_magnitude\": tf.io.FixedLenFeature((), tf.string)\n    }\n\n    features = tf.io.parse_single_example(record, keys_to_features)\n\n    noise_stft_mag_features = tf.io.decode_raw(features['noise_stft_mag_features'], tf.float32)\n    clean_stft_magnitude = tf.io.decode_raw(features['clean_stft_magnitude'], tf.float32)\n    noise_stft_phase = tf.io.decode_raw(features['noise_stft_phase'], tf.float32)\n\n    # reshape input and annotation images\n    noise_stft_mag_features = tf.reshape(noise_stft_mag_features, (129, 8, 1), name=\"noise_stft_mag_features\")\n    clean_stft_magnitude = tf.reshape(clean_stft_magnitude, (129, 1, 1), name=\"clean_stft_magnitude\")\n    noise_stft_phase = tf.reshape(noise_stft_phase, (129,), name=\"noise_stft_phase\")\n\n    return noise_stft_mag_features, clean_stft_magnitude","18e5db63":"train_dataset = tf.data.TFRecordDataset([train_tfrecords_filenames])\ntrain_dataset = train_dataset.map(tf_record_parser)\ntrain_dataset = train_dataset.shuffle(8192)\ntrain_dataset = train_dataset.repeat()\ntrain_dataset = train_dataset.batch(512+256)\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","6c8c5781":"test_dataset = tf.data.TFRecordDataset([val_tfrecords_filenames])\ntest_dataset = test_dataset.map(tf_record_parser)\ntest_dataset = test_dataset.repeat(1)\ntest_dataset = test_dataset.batch(512)","d1bee883":"from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, Flatten, Dense, Reshape, Conv2DTranspose, BatchNormalization, Activation\nfrom tensorflow.keras import Model, Sequential","916ee01a":"def conv_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(0.0006))(x)\n  x = Activation('relu')(x)\n  if use_bn:\n    x = BatchNormalization()(x)\n  return x","249988f6":"def full_pre_activation_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n  shortcut = x\n  in_channels = x.shape[-1]\n\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n  x = Conv2D(filters=in_channels, kernel_size=kernel_size, strides=strides, padding='same')(x)\n\n  return shortcut + x","a10eb7be":"def build_model(l2_strength):\n  inputs = Input(shape=[numFeatures,numSegments,1])\n  x = inputs\n\n  # -----\n  x = tf.keras.layers.ZeroPadding2D(((4,4), (0,0)))(x)\n  x = Conv2D(filters=18, kernel_size=[9,8], strides=[1, 1], padding='valid', use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n\n  skip0 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n                 kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(skip0)\n  x = BatchNormalization()(x)\n\n  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n\n  # -----\n  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n\n  skip1 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n                 kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(skip1)\n  x = BatchNormalization()(x)\n\n  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n\n  # ----\n  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n  \n  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n\n  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n\n  # ----\n  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n\n  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n             kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = x + skip1\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n\n  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n\n  # ----\n  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n\n  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n             kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = x + skip0\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n\n  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n  x = Activation('relu')(x)\n  x = BatchNormalization()(x)\n\n  # ----\n  x = tf.keras.layers.SpatialDropout2D(0.2)(x)\n  x = Conv2D(filters=1, kernel_size=[129,1], strides=[1, 1], padding='same')(x)\n\n  model = Model(inputs=inputs, outputs=x)\n\n  optimizer = tf.keras.optimizers.Adam(3e-4)\n  #optimizer = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=3e-4)\n\n  model.compile(optimizer=optimizer, loss='mse', \n                metrics=[tf.keras.metrics.RootMeanSquaredError('rmse')])\n  return model","806bea23":"model = build_model(l2_strength=0.0)\nmodel.summary()","6680d5f3":"tf.keras.utils.plot_model(model, show_shapes=True, dpi=64)","e8a892a3":"%tensorboard --logdir logs","120e6425":"model.load_weights('..\/input\/weights\/denoiser_cnn_log_mel_generator.h5')\nbaseline_val_loss = model.evaluate(test_dataset)[0]\nprint(baseline_val_loss)","38d3198d":"def l2_norm(vector):\n    return np.square(vector)\n\ndef SDR(denoised, cleaned, eps=1e-7): # Signal to Distortion Ratio\n    a = l2_norm(denoised)\n    b = l2_norm(denoised - cleaned)\n    a_b = a \/ b\n    return np.mean(10 * np.log10(a_b + eps))","f3bb2ac1":"early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, baseline=baseline_val_loss)\n\nlogdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='denoiser_cnn_log_mel_generator.h5', \n                                                         monitor='val_loss', save_best_only=True)\n\nmodel.fit(train_dataset,\n         steps_per_epoch=400,\n         validation_data=test_dataset,\n         epochs=50,\n         callbacks=[early_stopping_callback, tensorboard_callback, checkpoint_callback]\n        )","0edca248":"val_loss = model.evaluate(test_dataset)[0]\n\nif val_loss < baseline_val_loss:\n    print(\"New .model saved\")\nmodel.save('denoiser_cnn_log_mel_generator.h5')","679955a3":"#model.save('denoiser_cnn_generator.h5')\n#model.load_weights('drive\/My Drive\/datasets\/one_noise_data\/model.h5')","45634a42":"def read_audio(filepath, sample_rate, normalize=True):\n    # print(f\"Reading: {filepath}\").\n    audio, sr = librosa.load(filepath, sr=sample_rate)\n    if normalize:\n      div_fac = 1 \/ np.max(np.abs(audio)) \/ 3.0\n      audio = audio * div_fac\n    return audio, sr\n        \ndef add_noise_to_clean_audio(clean_audio, noise_signal):\n    if len(clean_audio) >= len(noise_signal):\n        # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n        while len(clean_audio) >= len(noise_signal):\n            noise_signal = np.append(noise_signal, noise_signal)\n\n    ## Extract a noise segment from a random location in the noise file\n    ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n\n    noiseSegment = noise_signal[ind: ind + clean_audio.size]\n\n    speech_power = np.sum(clean_audio ** 2)\n    noise_power = np.sum(noiseSegment ** 2)\n    noisyAudio = clean_audio + np.sqrt(speech_power \/ noise_power) * noiseSegment\n    return noisyAudio\n\ndef play(audio, sample_rate):\n    ipd.display(ipd.Audio(data=audio, rate=sample_rate))  # load a local WAV file","9c4b1f06":"class FeatureExtractor:\n    def __init__(self, audio, *, windowLength, overlap, sample_rate):\n        self.audio = audio\n        self.ffT_length = windowLength\n        self.window_length = windowLength\n        self.overlap = overlap\n        self.sample_rate = sample_rate\n        self.window = scipy.signal.hamming(self.window_length, sym=False)\n\n    def get_stft_spectrogram(self):\n        return librosa.stft(self.audio, n_fft=self.ffT_length, win_length=self.window_length, hop_length=self.overlap,\n                            window=self.window, center=True)\n\n    def get_audio_from_stft_spectrogram(self, stft_features):\n        return librosa.istft(stft_features, win_length=self.window_length, hop_length=self.overlap,\n                             window=self.window, center=True)\n\n    def get_mel_spectrogram(self):\n        return librosa.feature.melspectrogram(self.audio, sr=self.sample_rate, power=2.0, pad_mode='reflect',\n                                           n_fft=self.ffT_length, hop_length=self.overlap, center=True)\n\n    def get_audio_from_mel_spectrogram(self, M):\n        return librosa.feature.inverse.mel_to_audio(M, sr=self.sample_rate, n_fft=self.ffT_length, hop_length=self.overlap,\n                                             win_length=self.window_length, window=self.window,\n                                             center=True, pad_mode='reflect', power=2.0, n_iter=32, length=None)","d4e35d6d":"cleanAudio, sr = read_audio(os.path.join(mozilla_basepath, 'test',  'common_voice_ta_19071679.mp3'), sample_rate=fs)\nprint(\"Min:\", np.min(cleanAudio),\"Max:\",np.max(cleanAudio))\nipd.Audio(data=cleanAudio, rate=sr) # load a local WAV file","eba07de5":"noiseAudio, sr = read_audio(os.path.join(UrbanSound8K_basepath, 'fold10', '7965-3-16-0.wav'), sample_rate=fs)\n# 'fold10', '7913-3-0-0.wav'\nprint(\"Min:\", np.min(noiseAudio),\"Max:\",np.max(noiseAudio))\nipd.Audio(data=noiseAudio, rate=sr) # load a local WAV file","6ff2e3ed":"cleanAudioFeatureExtractor = FeatureExtractor(cleanAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\nstft_features = cleanAudioFeatureExtractor.get_stft_spectrogram()\nstft_features = np.abs(stft_features)\nprint(\"Min:\", np.min(stft_features),\"Max:\",np.max(stft_features))","3eba72dc":"noisyAudio = add_noise_to_clean_audio(cleanAudio, noiseAudio)\nipd.Audio(data=noisyAudio, rate=fs) # load a local WAV file","719da44b":"def prepare_input_features(stft_features):\n    # Phase Aware Scaling: To avoid extreme differences (more than\n    # 45 degree) between the noisy and clean phase, the clean spectral magnitude was encoded as similar to [21]:\n    noisySTFT = np.concatenate([stft_features[:,0:numSegments-1], stft_features], axis=1)\n    stftSegments = np.zeros((numFeatures, numSegments , noisySTFT.shape[1] - numSegments + 1))\n\n    for index in range(noisySTFT.shape[1] - numSegments + 1):\n        stftSegments[:,:,index] = noisySTFT[:,index:index + numSegments]\n    return stftSegments","fb8d283e":"noiseAudioFeatureExtractor = FeatureExtractor(noisyAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\nnoise_stft_features = noiseAudioFeatureExtractor.get_stft_spectrogram()\n\n# Paper: Besides, spectral phase was not used in the training phase.\n# At reconstruction, noisy spectral phase was used instead to\n# perform in- verse STFT and recover human speech.\nnoisyPhase = np.angle(noise_stft_features)\nprint(noisyPhase.shape)\nnoise_stft_features = np.abs(noise_stft_features)\n\nmean = np.mean(noise_stft_features)\nstd = np.std(noise_stft_features)\nnoise_stft_features = (noise_stft_features - mean) \/ std","4af8f8dd":"predictors = prepare_input_features(noise_stft_features)","ffdd80da":"predictors = np.reshape(predictors, (predictors.shape[0], predictors.shape[1], 1, predictors.shape[2]))\npredictors = np.transpose(predictors, (3, 0, 1, 2)).astype(np.float32)\nprint('predictors.shape:', predictors.shape)","deccc5b7":"STFTFullyConvolutional = model.predict(predictors)\nprint(STFTFullyConvolutional.shape)","29059639":"def revert_features_to_audio(features, phase, cleanMean=None, cleanStd=None):\n    # scale the outpus back to the original range\n    if cleanMean and cleanStd:\n        features = cleanStd * features + cleanMean\n\n    phase = np.transpose(phase, (1, 0))\n    features = np.squeeze(features)\n\n    # features = librosa.db_to_power(features)\n    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n\n    features = np.transpose(features, (1, 0))\n    return noiseAudioFeatureExtractor.get_audio_from_stft_spectrogram(features)","10d51a6d":"noisyAudio = add_noise_to_clean_audio(cleanAudio, noiseAudio)\nipd.Audio(data=noisyAudio, rate=fs) # load a local WAV file","8e78b8e1":"denoisedAudioFullyConvolutional = revert_features_to_audio(STFTFullyConvolutional, noisyPhase, mean, std)\nprint(\"Min:\", np.min(denoisedAudioFullyConvolutional),\"Max:\",np.max(denoisedAudioFullyConvolutional))\nipd.Audio(data=denoisedAudioFullyConvolutional, rate=fs) # load a local WAV file\n\n","a689dc58":"# A numeric identifier of the sound class -- Types of noise\n# 0 = air_conditioner\n# 1 = car_horn\n# 2 = children_playing\n# 3 = dog_bark\n# 4 = drilling\n# 5 = engine_idling\n# 6 = gun_shot\n# 7 = jackhammer\n# 8 = siren\n# 9 = street_music","306bfddd":"f, (ax1, ax2, ax3) = plt.subplots(3, 1, sharey=True)\n\nax1.plot(cleanAudio)\nax1.set_title(\"Clean Audio\")\n\nax2.plot(noisyAudio)\nax2.set_title(\"Noisy Audio\")\n\nax3.plot(denoisedAudioFullyConvolutional)\nax3.set_title(\"Denoised Audio\")","08fcf87c":"## Prepare Input features","438cdfed":"## Testing","04b8e29f":"## Model Training","76616027":"## Create tf.Data.Dataset"}}