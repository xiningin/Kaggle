{"cell_type":{"b9131267":"code","428d7689":"code","21022e40":"code","6c6018ef":"code","91dcf857":"code","6dd07b70":"code","081a2450":"code","dd96c52b":"code","8c1d781c":"code","d87db32a":"code","d411ad5a":"code","9313879d":"code","69af2952":"code","d4c6b2c6":"code","12cff7fa":"code","0afe997e":"code","40ed2774":"code","c0c98ab7":"code","9d637d29":"code","a1e09dad":"code","dd684e67":"code","a097eb64":"code","063a81cb":"code","4fa28c8d":"code","3748d631":"code","11093b35":"code","bab1ef88":"code","2f137d51":"code","36d1f253":"code","b26124cd":"code","27aad5de":"code","c60c031a":"code","ef855ed4":"code","e710635e":"code","520ffda5":"code","6eaa98a0":"code","5a681adc":"code","867866f8":"code","e8fb889a":"code","ad41dbb3":"code","5fee759f":"code","5bd6eb1d":"code","be7d2128":"code","2f5b9fc8":"code","2810f13e":"code","69fb911d":"code","3877dc5a":"code","977778e4":"code","35b71316":"code","5e96bbdf":"code","6d6fad0b":"code","a159a4de":"code","3d419447":"code","b75cfc4c":"code","640d5291":"code","146df873":"code","9e17322d":"code","5f4a769a":"code","7bbd4ed3":"code","51a51838":"code","a3cb0b60":"code","e4ad46c9":"code","403cf48d":"code","c595e0b8":"code","98c73f47":"code","9d6b3cad":"code","31678faa":"code","9c11f1cd":"code","77e24d2b":"code","f99e236d":"code","f4e3cc76":"code","88814cfa":"code","b21c8c96":"code","d6870a37":"code","79e56d52":"code","5626f1b0":"code","f5895d90":"code","733529e3":"code","d0865b9b":"code","fefb286a":"code","79ce697b":"code","99c155c5":"code","e64de986":"code","717e4048":"code","57c2a0ad":"code","ae2d6da1":"code","ac727d64":"code","6120c4fd":"code","f89393e7":"code","1ebe0b53":"code","3fe3216d":"code","4648ccd1":"code","5c1e16b0":"code","156008fc":"code","4b3631b0":"code","50133657":"code","739fedae":"code","7ae5744b":"code","0b2b1025":"code","4ae3253e":"code","af05a645":"code","0fcac228":"code","d340a13b":"code","6cd88399":"code","cb6f52ab":"code","7867e360":"code","f5adcc03":"code","26b88155":"code","3b069c85":"code","40dbc7b8":"code","9ee78caa":"markdown","10ea8c5e":"markdown","af94f81c":"markdown","7e74cc6a":"markdown","a4f50b17":"markdown","c7860909":"markdown","4b2b6efc":"markdown","c541ed0e":"markdown","96d413ce":"markdown","e0b75fb8":"markdown","2038d934":"markdown","0bffbf43":"markdown","88b54f60":"markdown","59cdcaa5":"markdown","bf8eb8ea":"markdown","812dabbf":"markdown","de83ffd4":"markdown","14fb9f83":"markdown","8396e08b":"markdown","d7a9d473":"markdown","0e2415f4":"markdown","d6fc3e38":"markdown","5ec22511":"markdown","ff02309a":"markdown","43a2ea40":"markdown","d8270fba":"markdown","e18fbcda":"markdown","ab4ca952":"markdown","cb92a4bc":"markdown","0f30e93a":"markdown","5be757f5":"markdown","23a5c6c3":"markdown","dc7d77fa":"markdown","4b48e40d":"markdown","31e7112c":"markdown","58051afa":"markdown","198e13f6":"markdown","513b5e36":"markdown","5df43e91":"markdown","a974198b":"markdown","37463030":"markdown","12163e9b":"markdown","69810447":"markdown","48140e81":"markdown","af4ce87b":"markdown","f2ff21bc":"markdown"},"source":{"b9131267":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm, tqdm_notebook\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","428d7689":"from sklearn.model_selection import train_test_split, KFold, StratifiedKFold,  cross_val_score\n\n# Libraries\nfrom scipy import stats\n#from scipy.sparse import hstack, csr_matrix\n#from sklearn.model_selection import train_test_split, KFold\n\n#import xgboost as xgb\n#from sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nimport json\n# import ast\n# import eli5\n# import shap\n#from catboost import CatBoostRegressor\nfrom urllib.request import urlopen\n# from PIL import Image\n#from sklearn.preprocessing import LabelEncoder\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model\n\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\n\n#from sklearn.model_selection import\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer\n# import lightgbm as lgbm","21022e40":"from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n","6c6018ef":"from keras.preprocessing import text, sequence\nimport xgboost\nimport scipy","91dcf857":"import shap","6dd07b70":"plt.style.use('fivethirtyeight')","081a2450":"df_train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ndf_test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\ndf_submission = pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')","dd96c52b":"df_train.head()","8c1d781c":"df_test = df_test.assign(selected_text=lambda x: x.text)","d87db32a":"#df_all.shape","d411ad5a":"df_train[df_train['selected_text'].isna()]","9313879d":"df_train[df_train['text'].isna()]","69af2952":"df_train.iloc[314]['text'] = 'None'","d4c6b2c6":"df_train.iloc[314]['selected_text'] = 'None'","12cff7fa":"df_all = pd.concat([df_train, df_test])","0afe997e":"df_all[df_all.text.isna()]","40ed2774":"df_train.sentiment.unique()","c0c98ab7":"# encode the target variable\n# label encode the target variable \nencoder = preprocessing.LabelEncoder()\ntrain_y_POS = encoder.fit_transform(df_train.sentiment=='positive')\ntest_y_POS = encoder.fit_transform(df_test.sentiment=='positive')","9d637d29":"np.where(train_y_POS==1)","a1e09dad":"np.where(df_train.sentiment=='positive')","dd684e67":"train_y_NEG = encoder.fit_transform(df_train.sentiment=='negative')\ntest_y_NEG = encoder.fit_transform(df_test.sentiment=='negative')","a097eb64":"from nltk.tokenize import TweetTokenizer\n# create a count vectorizer object \ntweet_token = TweetTokenizer(preserve_case=True, strip_handles=True)","063a81cb":"#list(df_train['text'].fillna(\"\"))","4fa28c8d":"df_train['text'].apply(lambda x: tweet_token.tokenize(x))","3748d631":"def tweet_token_proces(x):\n    return tweet_token.tokenize(x)\ncount_vect = CountVectorizer(tokenizer=tweet_token_proces)\ncount_vect.fit(df_all['text']) ## fillna","11093b35":"column_index = count_vect.get_feature_names()","bab1ef88":"# transform the training and validation data using count vectorizer object\nxtrain_count =  count_vect.transform(df_train.text)\nxtest_count =  count_vect.transform(df_test.text)","2f137d51":"# ## tf idf\n\n# # word level tf-idf\n# tfidf_vect = TfidfVectorizer(tokenizer=tweet_token_proces, max_features=5000)\n# tfidf_vect.fit(df_all['text'].fillna(' '))\n# xtrain_tfidf =  tfidf_vect.transform(df_train.text.fillna(' '))\n# xtest_tfidf =  tfidf_vect.transform(df_test.text.fillna(' '))","36d1f253":"# # ngram level tf-idf \n# tfidf_vect_ngram = TfidfVectorizer(tokenizer=tweet_token_proces, ngram_range=(2,5), max_features=5000)\n# tfidf_vect_ngram.fit(df_all['text'].fillna(' '))\n# xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(df_train.text.fillna(' '))\n# xtest_tfidf_ngram =  tfidf_vect_ngram.transform(df_test.text.fillna(' '))\n\n# # characters level tf-idf\n# tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,5), max_features=5000)\n# tfidf_vect_ngram_chars.fit(df_all['text'].fillna(' '))\n# xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(df_train.text.fillna(' ')) \n# xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(df_test.text.fillna(' ')) ","b26124cd":"# # load the pre-trained word-embedding vectors \n# embeddings_index = {}\n# for i, line in enumerate(open('..\/input\/wiki-news-300d-1M.vec')):\n#     values = line.split()\n#     embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')","27aad5de":"# # create a tokenizer \n# # use keras.preprocessing.text\n# token = text.Tokenizer()\n# token.fit_on_texts(df_all['text'].fillna(' '))\n# word_index = token.word_index","c60c031a":"# # convert text to sequence of tokens and pad them to ensure equal length vectors \n# train_seq_x = sequence.pad_sequences(token.texts_to_sequences(df_train.text.fillna(' ')), maxlen=70)\n# test_seq_x = sequence.pad_sequences(token.texts_to_sequences(df_test.text.fillna(' ')), maxlen=70)","ef855ed4":"# # create token-embedding mapping\n# embedding_matrix = np.zeros((len(word_index) + 1, 300))\n# for word, i in word_index.items():\n#     embedding_vector = embeddings_index.get(word)\n#     if embedding_vector is not None:\n#         embedding_matrix[i] = embedding_vector","e710635e":"# df_train['text'] = df_train['text'].fillna('None')\n# df_test['text'] = df_test['text'].fillna('None')","520ffda5":"# df_train[df_train.text=='None']","6eaa98a0":"# df_train['char_count'] = df_train['text'].apply(len)\n# df_test['char_count'] = df_test['text'].apply(len)\n\n# df_train['word_count'] = df_train['text'].apply(lambda x: len(x.split()))\n# df_test['word_count'] = df_test['text'].apply(lambda x: len(x.split()))\n\n# df_train['word_density'] = df_train['char_count'] \/ (df_train['word_count']+1)\n# df_test['word_density'] = df_test['char_count'] \/ (df_test['word_count']+1)\n\n# df_train['punctuation_count'] = df_train['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n# df_test['punctuation_count'] = df_test['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n\n# df_train['title_word_count'] = df_train['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n# df_test['title_word_count'] = df_test['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n\n# df_train['upper_case_word_count'] = df_train['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n# df_test['upper_case_word_count'] = df_test['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))","5a681adc":"## from the original notebook\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\nresults_jaccard=[]\n\nfor ind,row in df_train.iterrows():\n    sentence1 = row.text\n    sentence2 = row.selected_text\n    #print(ind)\n    jaccard_score = jaccard(sentence1,sentence2)\n    results_jaccard.append(jaccard_score)\n\ndf_train['jaccard'] = results_jaccard\n\ndf_train['Num_words_ST'] = df_train['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ndf_train['Num_word_text'] = df_train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ndf_train['difference_in_words'] = df_train['Num_word_text'] - df_train['Num_words_ST'] #Difference in Number of words text and Selected Text\n\nresults_jaccard=[]\n\nfor ind,row in df_test.iterrows():\n    sentence1 = row.text\n    sentence2 = row.selected_text\n\n    jaccard_score = jaccard(sentence1,sentence2)\n    results_jaccard.append(jaccard_score)\n\ndf_test['jaccard'] = results_jaccard\ndf_test['Num_words_ST'] = df_test['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ndf_test['Num_word_text'] = df_test['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ndf_test['difference_in_words'] = df_test['Num_word_text'] - df_test['Num_words_ST'] #Difference in Number of words text and Selected Text","867866f8":"df_train.iloc[314]","e8fb889a":"def train_model(classifier, feature_vector_train, label, feature_vector_test,  test_y,is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on testation dataset\n    predictions = classifier.predict(feature_vector_test)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n    \n    #return metrics.accuracy_score(predictions, test_y)\n    return metrics.accuracy_score(predictions, test_y)","ad41dbb3":"\n# Extereme Gradient Boosting on Count Vectors\n## Positive\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y_POS, xtest_count.tocsc(), test_y_POS)\nprint(\"Xgb, Count Vectors POS: \", accuracy)\n## negative \naccuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y_NEG, xtest_count.tocsc(), test_y_NEG)\nprint(\"Xgb, Count Vectors NEG: \", accuracy)\n","5fee759f":"# # Extereme Gradient Boosting on Word Level TF IDF Vectors\n# ## POS\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y_POS, xtest_tfidf.tocsc(), test_y_POS)\n# print(\"Xgb, WordLevel TF-IDF POS: \", accuracy)\n# ## NEG\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y_NEG, xtest_tfidf.tocsc(), test_y_NEG)\n# print(\"Xgb, WordLevel TF-IDF NEG: \", accuracy)","5bd6eb1d":"# # Extereme Gradient Boosting on ngram TF IDF Vectors\n# ## POS\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram.tocsc(), train_y_POS, \\\n#                        xtest_tfidf_ngram.tocsc(), test_y_POS)\n# print(\"Xgb, ngram Vectors POS: \", accuracy)\n\n# ## NEG\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram.tocsc(), train_y_NEG, \\\n#                        xtest_tfidf_ngram.tocsc(), test_y_NEG)\n# print(\"Xgb, ngram Vectors NEG: \", accuracy)","be7d2128":"# # Extereme Gradient Boosting on char level tfidf\n# ## POS\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y_POS,\\\n#                        xtest_tfidf_ngram_chars.tocsc(), test_y_POS)\n# print(\"Xgb, CharLevel Vectors POS: \", accuracy)\n\n# ## NEG\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y_NEG, \\\n#                        xtest_tfidf_ngram_chars.tocsc(), test_y_NEG)\n# print(\"Xgb, CharLevel Vectors NEG: \", accuracy)","2f5b9fc8":"#type(train_seq_x)","2810f13e":"# # Extereme Gradient Boosting on word embeddings\n# ## POS\n# accuracy = train_model(xgboost.XGBClassifier(), train_seq_x, train_y_POS,\\\n#                        test_seq_x, test_y_POS)\n# print(\"Xgb, word embedding POS: \", accuracy)\n\n# ## NEG\n# accuracy = train_model(xgboost.XGBClassifier(), train_seq_x, train_y_NEG, \\\n#                        test_seq_x, test_y_NEG)\n# print(\"Xgb, word embedding NEG: \", accuracy)","69fb911d":"# ######### tfidf and word cout\n# xtrain_tfidf_wc = scipy.sparse.hstack((xtrain_tfidf, xtrain_count))\n# xtest_tfidf_wc = scipy.sparse.hstack((xtest_tfidf, xtest_count))\n# ## POS\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_wc.tocsc(), train_y_POS, \\\n#                       xtest_tfidf_wc.tocsc(), test_y_POS)\n\n# print(\"POS: \", accuracy)\n\n# ## NEG\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_wc.tocsc(), train_y_NEG, \\\n#                       xtest_tfidf_wc.tocsc(), test_y_NEG)\n\n# print(\"NEG: \", accuracy)","3877dc5a":"# ########### all tfidf features\n\n# xtrain_tfidf_all = scipy.sparse.hstack((xtrain_tfidf_ngram_chars, xtrain_tfidf, xtrain_tfidf_ngram))\n\n# xtest_tfidf_all = scipy.sparse.hstack((xtest_tfidf_ngram_chars, xtest_tfidf, xtest_tfidf_ngram))\n\n\n# ## POS\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_all.tocsc(), train_y_POS, \\\n#                       xtest_tfidf_all.tocsc(), test_y_POS)\n\n# print(\"POS: \", accuracy)\n\n# ## NEG\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_all.tocsc(), train_y_NEG, \\\n#                       xtest_tfidf_all.tocsc(), test_y_NEG)\n\n# print(\"NEG: \", accuracy)","977778e4":"# ###########  tfidf features with count\n\n# xtrain_tfidf_all_2 = scipy.sparse.hstack(( xtrain_tfidf, xtrain_tfidf_ngram, xtrain_count))\n\n# xtest_tfidf_all_2 = scipy.sparse.hstack(( xtest_tfidf, xtest_tfidf_ngram, xtest_count))\n\n\n# ## POS\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_all_2.tocsc(), train_y_POS, \\\n#                       xtest_tfidf_all_2.tocsc(), test_y_POS)\n\n# print(\"POS: \", accuracy)\n\n# ## NEG\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_all_2.tocsc(), train_y_NEG, \\\n#                       xtest_tfidf_all_2.tocsc(), test_y_NEG)\n\n# print(\"NEG: \", accuracy)","35b71316":"# ###########  tfidf features with count\n\n# xtrain_tfidf_all_3 = scipy.sparse.hstack(( xtrain_tfidf, xtrain_tfidf_ngram))\n\n# xtest_tfidf_all_3 = scipy.sparse.hstack(( xtest_tfidf, xtest_tfidf_ngram))\n\n\n# ## POS\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_all_3.tocsc(), train_y_POS, \\\n#                       xtest_tfidf_all_3.tocsc(), test_y_POS)\n\n# print(\"POS: \", accuracy)\n\n# ## NEG\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_all_3.tocsc(), train_y_NEG, \\\n#                       xtest_tfidf_all_3.tocsc(), test_y_NEG)\n\n# print(\"NEG: \", accuracy)","5e96bbdf":"# ########### tfidf nlp wc\n# nlp_features = ['char_count', 'word_count', 'word_density', \\\n#                 'punctuation_count', 'title_word_count','upper_case_word_count']\n# xtrain_tfidf_wc_nlp = scipy.sparse.hstack((xtrain_tfidf, xtrain_count, df_train[nlp_features]))\n# xtest_tfidf_wc_nlp = scipy.sparse.hstack((xtest_tfidf, xtest_count, df_test[nlp_features]))\n# ## POS\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_wc_nlp.tocsc(), train_y_POS, \\\n#                       xtest_tfidf_wc_nlp.tocsc(), test_y_POS)\n\n# print(\"POS: \", accuracy)\n\n# ## NEG\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_wc_nlp.tocsc(), train_y_NEG, \\\n#                       xtest_tfidf_wc_nlp.tocsc(), test_y_NEG)\n\n# print(\"NEG: \", accuracy)\n","6d6fad0b":"######### tfidf wc special\n\n# xtrain_tfidf_wc_special = scipy.sparse.hstack((xtrain_tfidf, xtrain_count, df_train[special_features]))\n# xtest_tfidf_wc_special = scipy.sparse.hstack((xtest_tfidf, xtest_count, df_test[special_features]))\n# ## POS\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_wc_special.tocsc(), train_y_POS, \\\n#                       xtest_tfidf_wc_special.tocsc(), test_y_POS)\n\n# print(\"POS: \", accuracy)\n\n# ## NEG\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_wc_special.tocsc(), train_y_NEG, \\\n#                       xtest_tfidf_wc_special.tocsc(), test_y_NEG)\n\n# print(\"NEG: \", accuracy)\n\n","a159a4de":"########## mash up all features together\n\n# xtrain_all = scipy.sparse.hstack((xtrain_tfidf, xtrain_count, df_train[special_features + nlp_features]))\n# xtest_all = scipy.sparse.hstack((xtest_tfidf, xtest_count, df_test[special_features + nlp_features]))\n\n# ## POS\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_all.tocsc(), train_y_POS, \\\n#                       xtest_all.tocsc(), test_y_POS)\n\n# print(\"POS: \", accuracy)\n\n# ## NEG\n# accuracy = train_model(xgboost.XGBClassifier(), xtrain_all.tocsc(), train_y_NEG, \\\n#                       xtest_all.tocsc(), test_y_NEG)\n\n# print(\"NEG: \", accuracy)\n\n\n","3d419447":"baseline = xgboost.XGBClassifier()","b75cfc4c":"model_baseline = baseline.fit(X=xtrain_count, y=train_y_POS)","640d5291":"model_baseline.get_params()","146df873":"def objective(params):\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'subsample': \"{:.2f}\".format(params['subsample']),\n        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n    }\n    \n    clf = xgboost.XGBClassifier(\n        n_estimators=100,\n        #learning_rate=0.1,\n        n_jobs=4,\n        \n        **params\n    )\n    \n    score = cross_val_score(clf, xtrain_count, train_y_POS, scoring='accuracy', cv=KFold(n_splits=5)).mean()\n    print(\"accuracy {:.3f} params {}\".format(score, params))\n    return -score","9e17322d":"## code here is mostly borrowed from here: https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\nspace = {\n    # The maximum depth of a tree, same as GBM.\n    # Used to control over-fitting as higher depth will allow model \n    # to learn relations very specific to a particular sample.\n    # Should be tuned using CV.\n    # Typical values: 3-10\n    'max_depth': hp.quniform('max_depth', 3, 6, 1),\n    \n    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n    # (meaning pulling weights to 0). It can be more useful when the objective\n    # is logistic regression since you might need help with feature selection.\n    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n    \n    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n    # approach can be more useful in tree-models where zeroing \n    # features might not make much sense.\n    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n    \n    # eta: Analogous to learning rate in GBM\n    # Makes the model more robust by shrinking the weights on each step\n    # Typical final values to be used: 0.01-0.2\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    \n    # colsample_bytree: Similar to max_features in GBM. Denotes the \n    # fraction of columns to be randomly samples for each tree.\n    # Typical values: 0.5-1\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n    \n    # A node is split only when the resulting split gives a positive\n    # reduction in the loss function. Gamma specifies the \n    # minimum loss reduction required to make a split.\n    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n    'gamma': hp.uniform('gamma', 0.01, .7),\n    \n    # more increases accuracy, but may lead to overfitting.\n    # num_leaves: the number of leaf nodes to use. Having a large number \n    # of leaves will improve accuracy, but will also lead to overfitting.\n    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n    \n    # specifies the minimum samples per leaf node.\n    # the minimum number of samples (data) to group into a leaf. \n    # The parameter can greatly assist with overfitting: larger sample\n    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n    \n    # subsample: represents a fraction of the rows (observations) to be \n    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n    # in their paper A Scalable Tree Boosting System recommend \n    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n    \n    # randomly select a fraction of the features.\n    # feature_fraction: controls the subsampling of features used\n    # for training (as opposed to subsampling the actual training data in \n    # the case of bagging). Smaller fractions reduce overfitting.\n    'feature_fraction': hp.uniform('feature_fraction', 0.8, .9),\n    \n    # randomly bag or subsample training data.\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.8, .9)\n    \n    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n    # of the training data. Both values need to be set for bagging to be used.\n    # The frequency controls how often (iteration) bagging is used. Smaller\n    # fractions and frequencies reduce overfitting.\n}","5f4a769a":"%%time\n# this steps takes at least 20mins\n# best = fmin(fn=objective,\n#             space=space,\n#             algo=tpe.suggest, max_evals=30) ","7bbd4ed3":"best = {'bagging_fraction': 0.8768575337571937,\n 'colsample_bytree': 0.9933592930641432,\n 'feature_fraction': 0.816825176108506,\n 'gamma': 0.05587328363633812,\n 'learning_rate': 0.19879098664834996,\n 'max_depth': 6.0,\n 'min_child_samples': 9,\n 'num_leaves': 7,\n 'reg_alpha': 0.11806338517600543,\n 'reg_lambda': 0.23269341544465222,\n 'subsample': 0.6}","51a51838":"best['max_depth'] = 6\nbest['subsample'] = .6","a3cb0b60":"#params = {'max_depth': 5, 'gamma': '0.332', 'subsample': '0.80', 'reg_alpha': '0.365', 'reg_lambda': '0.070', 'learning_rate': '0.200', 'num_leaves': '150.000', 'colsample_bytree': '0.792', 'min_child_samples': '120.000', 'feature_fraction': '0.710', 'bagging_fraction': '0.436'}","e4ad46c9":"params = best\n#params['max_depth'] = 6\n#params['subsample'] = .9","403cf48d":"xgb = xgboost.XGBClassifier(**params, n_estimators=100,\n        #learning_rate=0.05,\n        n_jobs=4)","c595e0b8":"## POSITIVE\nxgb_fit_POS = xgb.fit(xtrain_count.tocsc(), train_y_POS)\n\npredictions = xgb_fit_POS.predict(xtest_count.tocsc())\n\nmetrics.accuracy_score(predictions, test_y_POS) ## improved from 0.816","98c73f47":"## NEGATIVE\nxgb_neg = xgboost.XGBClassifier(**params, n_estimators=100,\n        #learning_rate=0.05,\n        n_jobs=4)\nxgb_fit_NEG = xgb_neg.fit(xtrain_count.tocsc(), train_y_NEG)\npredictions = xgb_fit_NEG.predict(xtest_count.tocsc())","9d6b3cad":"metrics.accuracy_score(predictions, test_y_NEG)","31678faa":"xtrain_all_dense = pd.DataFrame(xtrain_count.tocsc().todense())\nxtrain_all_dense.shape","9c11f1cd":"column_index = count_vect.get_feature_names()\nlen(column_index)","77e24d2b":"shap.initjs()","f99e236d":"#xgb_fit_POS","f4e3cc76":"#import shap\nexplainer = shap.TreeExplainer(xgb_fit_POS)\nshap_values = explainer.shap_values(xtrain_all_dense)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\nshap.force_plot(explainer.expected_value, shap_values[1,:],\\\n                xtrain_all_dense.iloc[1,:])","88814cfa":"column_index[10248], column_index[8282]","b21c8c96":"np.mean(shap_values[:, 10248]), np.mean(shap_values[:, 8282])","d6870a37":"shap_values.shape","79e56d52":"shap.force_plot(explainer.expected_value, shap_values[9011,:],\\\n                xtrain_all_dense.iloc[9011,:])","5626f1b0":"column_index[4898], column_index[7229]","f5895d90":"np.mean(shap_values[:, 4898]), np.mean(shap_values[:, 7229])","733529e3":"def get_word_index(x):\n    result = []\n    for i in x.split(' '):\n        try:\n            #current_word = wordnet_lemmatizer.lemmatize(i.lower())\n            result.append(column_index.index(i))\n        except ValueError:\n            pass\n            #result.append(-1)\n    return result\n\ndef get_word(x):\n    result = []\n    for i in x.split(' '):\n        try:\n            #current_word = wordnet_lemmatizer.lemmatize(i.lower())\n            column_index.index(i)\n            result.append(i)\n        except ValueError:\n            pass\n            #result.append('None')\n           \n    return result","d0865b9b":"column_index.index('making')","fefb286a":"df_train['text_word_index'] = df_train.text.apply(lambda x: get_word_index(x))\ndf_train['text_word_has_index'] = df_train.text.apply(lambda x: get_word(x))","79ce697b":"## now apply the same for selected text\ndf_train['selected_text_word_index'] = df_train.selected_text.apply(lambda x: get_word_index(x))\ndf_train['selected_text_word_has_index'] = df_train.selected_text.apply(lambda x: get_word(x))","99c155c5":"## now map all the text for each words with shap value\nshap_extracted = []\n\n\nfor i in np.arange(len(df_train)):\n    shap_extracted.append(shap_values[i, df_train['text_word_index'][i]])","e64de986":"shap_extracted_mean = [np.mean(i) for i in shap_extracted]","717e4048":"## now do the same for selected text\nselected_shap_extracted = []\nfor i in np.arange(len(df_train)):\n    selected_shap_extracted.append(shap_values[i, df_train['selected_text_word_index'][i]])","57c2a0ad":"selected_shap_extracted_mean = [np.mean(i) for i in selected_shap_extracted]","ae2d6da1":"df_train['shap_extracted'] = shap_extracted","ac727d64":"df_train['shap_extracted_mean'] = shap_extracted_mean\ndf_train['selected_shap_extracted_mean'] = selected_shap_extracted_mean","6120c4fd":"sns.stripplot(df_train.shap_extracted_mean,df_train.sentiment, jitter=True)","f89393e7":"sns.stripplot(df_train.selected_shap_extracted_mean, df_train.sentiment,jitter=True )","1ebe0b53":"concat_pd = pd.concat([df_train[df_train.sentiment=='positive']['shap_extracted_mean'],\\\n           df_train[df_train.sentiment=='positive']['selected_shap_extracted_mean']])\n\nconcat_pd = pd.DataFrame(concat_pd).reset_index()\nconcat_pd['text'] = 'text'\nconcat_pd.columns = ['index', 'mean', 'text']\nconcat_pd.loc[8582:, 'text'] = 'selected_text'","3fe3216d":"sns.stripplot(concat_pd['mean'], concat_pd.text)","4648ccd1":"df_train['mean_diff'] = df_train.selected_shap_extracted_mean - df_train.shap_extracted_mean","5c1e16b0":"#df_train.mean_diff.hist(bins=30)\nsns.kdeplot(df_train[df_train['sentiment']=='positive']['mean_diff'], shade=True, color=\"b\")","156008fc":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(df_train[df_train['sentiment']=='positive']['shap_extracted_mean'], shade=True, color=\"r\")\\\n.set_title('Kernel Distribution of SHAP values (raw text vs selected text)')\np1=sns.kdeplot(df_train[df_train['sentiment']=='positive']['selected_shap_extracted_mean'], shade=True, color=\"b\")\nplt.vlines(x=-0.15, ymax=7, ymin=0, linestyles='dashed' )\nplt.vlines(x=0.01, ymax=7, ymin=0, linestyles='dashed')","4b3631b0":"# uncomment it if you want to run it","50133657":"# def shap_cutoff_2(cutoff1, cutoff2, x):\n#     '''\n#     input: cutoff for shap value and x is the extracted shap value\n#     output: a tuple with indices for text selection\n#     '''\n#     try: \n#         min_idx_1 = np.where(x == np.min([i for i in x if i < cutoff1]))[0][0]\n#         max_idx_1 = np.where(x==np.max([i for i in x if i < cutoff1]))[0][0]\n#         min_idx_2 = np.where(x == np.min([i for i in x if i > cutoff2]))[0][0]\n#         max_idx_2 = np.where(x==np.max([i for i in x if i > cutoff2]))[0][0]\n        \n#         return (min_idx_1, max_idx_1, min_idx_2, max_idx_2)\n#     except:\n#         # if x is empty, then return an impossivle value\n#         return (100, 100, 100, 100)\n    \n# def jaccard(str1, str2): \n#     a = set(str1.lower().split()) \n#     b = set(str2.lower().split())\n#     c = a.intersection(b)\n#     return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n# #for c1, c2 in zip(np.linspace(-.5, -0.01, 15), np.linspace(0.01, 0.5, 15)):\n#     ## use a cutoff to find word indices \n# for c1 in np.linspace(-.5, -.3, 5):\n#     for c2 in np.linspace(0.5, 0.6, 2):\n#         df_train['shap_cutoff_2'] = df_train.shap_extracted.apply(lambda x: shap_cutoff_2(c1,c2, x))\n#         partial_text=[]\n#         # get selected text\n#         for i,row in df_train.iterrows():\n#             if row.shap_cutoff_2== (100, 100, 100, 100):\n#                 partial_text.append(row.text.split(' '))\n#             else:\n#                 min_idx = np.min(row.shap_cutoff_2)\n#                 max_idx = np.max(row.shap_cutoff_2) + 1\n#                 partial_text.append(row.text_word_has_index[min_idx: max_idx])\n\n#         soln_sentence = [' '.join(i) for i in partial_text]\n#         df_train['selected_soln_1'] = soln_sentence\n\n#         # calculate jaccard\n#         results_jaccard=[]\n\n#         for ind,row in df_train.iterrows():\n#             sentence1 = row.selected_text\n#             sentence2 = row.selected_soln_1\n#             #print(ind)\n#             jaccard_score = jaccard(sentence1,sentence2)\n#             results_jaccard.append(jaccard_score)\n#         df_train['results_jaccard'] = results_jaccard\n\n#         print('cutoff: ', c1, ', ', c2)\n#         print('mean jaccard: ',df_train[df_train.sentiment=='positive'].results_jaccard.mean())\n#         sns.kdeplot(df_train[df_train['sentiment']=='positive']['results_jaccard'], shade=True, color=\"b\")\n#         plt.show()\n","739fedae":"import numpy as np\nfrom scipy.spatial import distance\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier","7ae5744b":"from sklearn.svm import SVC","0b2b1025":"import lime\nimport lime.lime_tabular\n\n","4ae3253e":"df_train[df_train.sentiment=='positive'][:10].index","af05a645":"count_vect = CountVectorizer(tokenizer=tweet_token_proces)\ngrad_boost = GradientBoostingClassifier(max_depth=6)","0fcac228":"pipe = make_pipeline(count_vect, grad_boost)","d340a13b":"pipe.fit(df_train.text, train_y_POS)","6cd88399":"pipe.score(df_test.text, test_y_POS)","cb6f52ab":"\n\nfrom lime.lime_text import LimeTextExplainer\nexplainer = LimeTextExplainer(class_names=['nonPOS', 'POS'])","7867e360":"idx = [6, 9, 11, 21, 25, 28, 30, 31, 33, 39, 27474]","f5adcc03":"for i in idx:\n    exp = explainer.explain_instance(df_train.text[i], pipe.predict_proba)\n    print(df_train.selected_text.values[i])\n    exp.show_in_notebook()\n    ","26b88155":"exp.as_list()","3b069c85":"column_index = count_vect.get_feature_names()","40dbc7b8":"# threshould = .07\n# select_text_list = []\n# for i in tqdm_notebook(range(len(df_train))):\n#     row = df_train.iloc[i]\n#     if row.sentiment=='positive':\n#         try:\n#             exp = explainer.explain_instance(row.text, pipe.predict_proba).as_list()\n#             exp_words =[k[0] for k in exp if k[1] > threshould]\n#             text_list = tweet_token_proces(row.text)\n#             idx = [text_list.index(k) for k in exp_words]\n#             if len(idx) < 1:\n#                 select_text_list.append([])\n#             if len(idx) ==1:\n#                 select_text_list.append(exp_words[0])\n#             else:\n\n#                 max_idx = max(idx)\n#                 min_idx = min(idx)\n#                 cur_text = text_list[min_idx:max_idx]\n#                     #cur_text_processed = [i if i.isalpha() else i + ' ' for i in cur_text]\n#                     #print(' '.join(cur_text))\n#                 select_text_list.append(cur_text)\n#         except:\n#             select_text_list.append([])\n#     else:\n#         select_text_list.append([])","9ee78caa":"## Heuristic 1","10ea8c5e":"#### Positive tag","af94f81c":"## LIME","7e74cc6a":"## Goal \ud83c\udfaf\n\n* To build text classification model and use SHAP value to **explain** sentiment \n* Use some sort of heuristic rules to select text\n\nMy idea is mostly sprouted from these discussions... so please make sure to check out these\n\n* https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/139335\n* https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/139803\n\n## Also referenced these awesome notebooks\n\n* https:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-eda-and-model\n* https:\/\/towardsdatascience.com\/explain-nlp-models-with-lime-shap-5c5a9f84d59b\n\n## Thoughs on number of models\n\nsince the goal of using classification model is **not about getting labels but rather getting interpretation labels**, we may want to consider \n\n* a model for prediction positive sentiment\n* a model for negative sentiment\n* for neutral sentiment, we use the original text (seems like a popular solution here)\n\n***\ud83c\udf69 please upvote if you like this work,***\n\n#### Update\n\n* I added the model with word count and the shap value output seems make sense now. Not sure why TFIDF doesn't make much sense in the previous version.\n* Added LIME's solution\n\n***Note: This is currently a working solution, will keep updating the current solution \ud83c\udf55***","a4f50b17":"<a id='tag3'><\/a>","c7860909":"#### Now we want to map out all the words in text that has an index in our shap value columns","4b2b6efc":"By inspecting the shap value by each observation, we find that *** words with positive sentiment tend to have higher shap value, and vice versa***; since positive words are trying to push the target value to the positive side, vice versa.","c541ed0e":"## Feature Engineering","96d413ce":"#### Positive","e0b75fb8":"## Hyperparameter optimization","2038d934":"Here, the shap value is smaller for negative sentiment tweets; and positive words tend to have positive shap values. Now let us look at the words in the selected text. ","0bffbf43":"It seems that combining feature does imporve the model performance; although here the real question for ourself is :\n\n\n***should we prioritize building a better model or should we improve our heuristics? *** \n\nAdding too many additional feature also makes the interpretation of the model more complex; after few trial and error, we decided to use only word count feature, while not sacreficing too much performance reduction. ","88b54f60":"<a id='tag5'><\/a>","59cdcaa5":"### Special features for this challenge","bf8eb8ea":"### missing values","812dabbf":"## Now use one set of parameters from this","de83ffd4":"### Data","14fb9f83":"#### Negative words","8396e08b":"To summeraize the density distribution of SHAP value from text and seleted text: \n\n* Seleted text has a thicker (left and right) tail than raw text","d7a9d473":"<a id='tag2'><\/a>","0e2415f4":"Best cutoff: SHAP value < -0.4125 , and SHAP Value >  0.5","d6fc3e38":"#### Positive words","5ec22511":"This short analysis gave us some heuristics to label the selected text; we can use a hard thresold based on the probability we get from the model.","ff02309a":"Interestingly, we see that **selected word tend to have more extreme shape value than the original text**","43a2ea40":"#### Negative","d8270fba":"<a id='tag1'><\/a>","e18fbcda":"### CountVectorizer","ab4ca952":"## Classification XGboost","cb92a4bc":"<a id='tag7'><\/a>","0f30e93a":"<a id='tag4'><\/a>","5be757f5":"### word embeddings","23a5c6c3":"The performance of SHAP value doesn't seem to be satisfying. We have to try something else to make accurate prediction.","dc7d77fa":"*** Now let's look at the mean shap value by sentiment ***","4b48e40d":"* Adding selection methods for LIME ","31e7112c":"## TODO","58051afa":"## SHAP value","198e13f6":"### Library\n","513b5e36":"<a id='tag6'><\/a>","5df43e91":"## Table of Contents\n\n[Feature engineering](#tag1)\n\n[Classification model](#tag2)\n\n[Improving model by hstack](#tag3)\n\n[Hyperparameter tuning](#tag4)\n\n[SHAP value](#tag5)\n\n[Heuristics for selecting texts 1](#tag6)\n\n[LIME value](#tag7)","a974198b":"### Target label encoding\n\nSince we are gonna build two separate models, we generate two list of labels","37463030":"### tf-idf\n\n* Word level\n* ngram level\n* character level","12163e9b":"### NLP based features ","69810447":"This plot gives us some hint on how to find out the selected text by SHAP value; selected text tend to have smaller values. ","48140e81":"Here we are trying to use ***Jaccard similarity*** between predicted selected text and selected text to find out a good cutoff for the shap value.\n\nThere are several steps:\n\n* Loop through a range of values, and find two cutoffs\n* Find out the indices of words that mets the criteria for shap values\n* Selected the text based on the indices\n* Calculate jaccard similarity of predicted selected text and true selected text","af4ce87b":"[see this documentation for f1 multiclass](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html)","f2ff21bc":"## improve model by combining feature (hstack)\n\n"}}