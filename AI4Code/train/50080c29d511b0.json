{"cell_type":{"9028d13a":"code","88c1082c":"code","f169c07a":"code","a43eaef8":"code","026c9f57":"code","ad4ceb0f":"code","167fff4d":"code","581e713e":"code","d5bc35cf":"code","8653d0fc":"code","8cbb1a92":"code","b22eaf47":"code","4b3811d6":"code","3126472d":"code","bb83145b":"code","9cc5fd31":"code","609a656e":"code","096fb120":"markdown","1abe471a":"markdown","19eafca7":"markdown","05bf6bce":"markdown"},"source":{"9028d13a":"import os\nimport pathlib\n\nif os.path.exists(\"\/kaggle\"):\n    input_dir = pathlib.Path(\"\/kaggle\/input\/commonlitreadabilityprize\")\n    \n    !pip install ..\/input\/textstat\/Pyphen-0.10.0-py3-none-any.whl\n    !pip install ..\/input\/textstat\/textstat-0.7.0-py3-none-any.whl\n    !pip install ..\/input\/pandarallel151whl\/pandarallel-1.5.1-py3-none-any.whl\nelse:\n    input_dir = pathlib.Path(\"..\/data\/raw\")","88c1082c":"import os\nimport re\nimport time\nimport pathlib\nimport textstat\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nfrom pandarallel import pandarallel\nfrom pandas_profiling import ProfileReport\n\nfrom typing import AnyStr\n\nimport nltk\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\n\ntqdm.pandas()\n\npandarallel.initialize(progress_bar=True)\n\nplt.style.use('seaborn-darkgrid')","f169c07a":"train = pd.read_csv(input_dir \/ 'train.csv')\ntest = pd.read_csv(input_dir \/ 'test.csv')\nsmpl_sub = pd.read_csv(input_dir \/ 'sample_submission.csv')","a43eaef8":"def preprocess_excerpt(text: AnyStr):\n    text = re.sub(\"[^a-zA-Z]\", \" \", text).lower()\n    text = nltk.word_tokenize(text)  # NOTE: \u82f1\u6587\u3092\u5358\u8a9e\u5206\u5272\u3059\u308b\n    text = [word for word in text if not word in set(stopwords.words(\"english\"))]\n    \n    lemma = nltk.WordNetLemmatizer()  # NOTE: \u8907\u6570\u5f62\u306e\u5358\u8a9e\u3092\u5358\u6570\u5f62\u306b\u5909\u63db\u3059\u308b\n    text =  \" \".join([lemma.lemmatize(word) for word in text])\n    return text","026c9f57":"%%time\n\ntrain['excerpt_preprocessed'] = train['excerpt'].parallel_apply(preprocess_excerpt)\ntest['excerpt_preprocessed'] = test['excerpt'].parallel_apply(preprocess_excerpt)","ad4ceb0f":"from sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA","167fff4d":"# Ref: https:\/\/nigimitama.hatenablog.jp\/entry\/2020\/11\/09\/080000\n\nclass BaseTransformer(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return self\n\nclass TextstatProcessing(BaseTransformer):\n    \n    def transform(self, X):\n        X['excerpt_len'] = X['excerpt_preprocessed'].str.len()\n        X['avg_word_len'] = X['excerpt_preprocessed'].apply(lambda x: [len(s) for s in x.split()]).map(np.mean)\n        X['char_count'] = X['excerpt'].map(textstat.char_count)\n        X['word_count'] = X['excerpt_preprocessed'].map(textstat.lexicon_count)\n        X['sentence_count'] = X['excerpt'].map(textstat.sentence_count)\n        X['syllable_count'] = X['excerpt'].apply(textstat.syllable_count)\n        X['smog_index'] = X['excerpt'].apply(textstat.smog_index)\n        X['automated_readability_index'] = X['excerpt'].apply(textstat.automated_readability_index)\n        X['coleman_liau_index'] = X['excerpt'].apply(textstat.coleman_liau_index)\n        X['linsear_write_formula'] = X['excerpt'].apply(textstat.linsear_write_formula)\n        return X\n    \n\nclass TfidfTransformer(BaseTransformer):\n    \n    def transform(self, X):\n        tfidf_vec = TfidfVectorizer(binary=True, ngram_range=(1,1))\n        vector = tfidf_vec.fit_transform(X['excerpt'])\n        vector = pd.DataFrame(vector.toarray(), columns=tfidf_vec.get_feature_names())\n        pca = PCA(n_components=5)\n        pca_vector = pd.DataFrame(pca.fit_transform(vector))\n        X = pd.concat([X, pca_vector], axis=1)\n        return X\n    \n\nclass DropFeature(BaseTransformer):\n    \n    def transform(self, X):\n        X.drop(['excerpt', 'excerpt_preprocessed'], axis=1, inplace=True)\n        return X","581e713e":"%%time\nsrc_X = train[['excerpt', 'excerpt_preprocessed']].copy()\n\npipe = Pipeline(steps=[\n    ('textstat_processing', TextstatProcessing()),\n    ('tfidf', TfidfTransformer()),\n    ('drop_feature', DropFeature()),\n])\n\npipe.fit(src_X)","d5bc35cf":"dst_X = pipe.transform(src_X)\ndst_X.head()","8653d0fc":"from sklearn import model_selection\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor","8cbb1a92":"def rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred, squared=False)","b22eaf47":"def custom_cv(n_splits: int, target: pd.Series, shuffle: bool = True):\n    target = target.to_frame()\n    num_bins = int(np.floor(1 + np.log2(len(target))))\n    target.loc[:, \"bins\"] = pd.cut(\n        target[\"target\"], bins=num_bins, labels=False\n    )\n\n    kf = model_selection.StratifiedKFold(n_splits=n_splits)\n    for train_idx, test_idx in kf.split(X=target, y=target['bins']):\n        yield train_idx, test_idx","4b3811d6":"X = pipe.transform(src_X)\ny = train['target']\ncv = custom_cv(5, y)\n\nmodel = make_pipeline(\n    StandardScaler(),\n    LinearRegression(normalize=True, n_jobs=-1),\n)\n\nresult = cross_validate(\n    model, \n    X, \n    y, \n    cv=5, \n    scoring={'rmse': make_scorer(rmse)}, \n    return_train_score=True,\n    return_estimator=True,\n    n_jobs=-1,\n)","3126472d":"print(f\"fit_time   : {result['fit_time'].mean():.5f}\")\nprint(f\"score_time : {result['score_time'].mean():.5f}\")\nprint(f\"train_rmse : {result['train_rmse'].mean():.5f}\") \nprint(f\"test_rmse  : {result['test_rmse'].mean():.5f}\") ","bb83145b":"X_test = test[['excerpt', 'excerpt_preprocessed']].copy()\nX_test = pipe.transform(X_test)\npred = np.zeros(X_test.shape[0])\n\nfor estimator in result['estimator']:\n    pred += estimator.predict(X_test) \/ len(result['estimator'])","9cc5fd31":"plt.title('Target Distribution')\nplt.hist(pred, bins=50)\nplt.axvline(pred.mean(), color='tab:orange', linewidth=2, linestyle='--', label='mean')\nplt.xlabel('target')\nplt.ylabel('count')\nplt.legend()","609a656e":"smpl_sub['target'] = pred\nsmpl_sub.to_csv('submission.csv', index=False)","096fb120":"## Preprocessing","1abe471a":"## Train Model","19eafca7":"# Simple Baseline","05bf6bce":"## Prediction"}}