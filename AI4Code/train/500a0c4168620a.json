{"cell_type":{"3083446e":"code","b150c99d":"code","c932aa4b":"code","407fb936":"code","01774b67":"code","55c9a6cb":"code","d20d16e5":"code","d537dfcc":"code","dec9b200":"code","a05ef575":"code","4eab4e3c":"code","cfc1a171":"code","7bb3002c":"code","ed33c981":"code","7d316e69":"code","ccf916da":"code","c1a07e19":"markdown","c96dd713":"markdown"},"source":{"3083446e":"# First of all, we should install Keras itself and packages for translation and language detection\n!pip install -q --upgrade tensorflow\n!pip install -q keras\n!pip install -q -U deep-translator\n!pip install -q langid","b150c99d":"# Next, we import preprocessing packages...\nimport pandas as pd\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom deep_translator import GoogleTranslator\nimport langid\n\n# ...Keras packages for network training and evaluation...\nfrom keras import models, layers, callbacks\nfrom keras.metrics import Precision, Recall, AUC\nfrom keras import backend as K\n\n# ...and Sklearn's train_test_split to define train, test and validation sets\nfrom sklearn.model_selection import train_test_split\n\npd.options.mode.chained_assignment = None","c932aa4b":"# Taking a look at our data;\n# it's easy to se that we have some English and Hindi texts;\n# we should translate Hindi to English as most of NLP methods perform better on English texts\nart_data = pd.read_csv('..\/input\/mobilenonmobile-tech-articles-and-tweets\/dev_data_article.csv').drop(columns=['Text_ID'])\nlen_data = len(art_data)\nart_data","407fb936":"# Concatenating headers and texts of articles\nart_data['Text_Headline'] = art_data['Headline'] + \" \" + art_data['Text']\nart_data = art_data.drop(columns=['Headline', 'Text'])","01774b67":"# For each article, we detect its language; if it's not English,\n# we ask Google Translate to translate it into English, otherwise we keep the source text\ngt = GoogleTranslator(source='auto', target='en')\nmax_art_len = 1000  # Here, we truncate the content to first 1000 characters; Google Translate accepts texts\n                    # not longer than 5000 characters, but the volume of 1000 can be enough for classification\n                    # and enables us to reduce the time of working with translation service\n\ntranslated_arts = []\n\nfor i in range(len(art_data)):\n    if i % 200 == 0:\n        print(f\"Translated {i} \/ {len_data}\")\n    cur_art = art_data.iloc[i]['Text_Headline'][:max_art_len]\n    translated_arts.append(cur_art if langid.classify(cur_art)[0] == 'en' else gt.translate(cur_art))\n                           \nart_data['translated_text'] = translated_arts","55c9a6cb":"# We can compare source and translated articles\nart_data","d20d16e5":"# The amount of articles related to mobile tech is 3 times lower than \"non-mobile\" articles; this info may be useful\n# for classification\nart_data.Mobile_Tech_Flag.value_counts()","d537dfcc":"# Let's clean our data from stopwords and truncate the word endings to make the different forms\n# of one word represent this word for the network\nstop_words = stopwords.words('english')\nsbs = SnowballStemmer(language='english')\n\nclear_texts = []\n\nfor i in range(len(art_data)):\n    if i % 100 == 0:\n        print(f\"Processing {i} \/ {len(art_data)} article...\")\n\n    cell_text = art_data.iloc[i].translated_text\n    clear_text = ''\n\n    # Tokenizing, stopwords filtering and stemming\n    cell_tokens = word_tokenize(cell_text)\n    for token in cell_tokens:\n        tok_low = token.lower()\n        if tok_low not in stop_words:\n            tok_stem = sbs.stem(tok_low)\n            clear_text += tok_stem + \" \"\n\n    clear_texts.append(clear_text)\n\nart_data['clean_text'] = clear_texts","dec9b200":"# Preprocessed texts are stored in 'clean_text' column\nart_data","a05ef575":"# Dividing our dataset into train, test and validation sets in ratio of 0.75\/0.125\/0.125,\n# keeping the target variable distribution in each set by 'stratify' parameter\nX_train, X_test, Y_train, Y_test = train_test_split(art_data.clean_text, art_data.Mobile_Tech_Flag, random_state=42,\n                                                   train_size=0.75, stratify=art_data.Mobile_Tech_Flag)\nX_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, random_state=42, train_size=0.5, stratify=Y_test)","4eab4e3c":"# Tokenization is the operation of transforming texts into numerical vectors\n# We can modify 'num_words' parameter to choose how many most frequent words\n# in dataset should be represented in vectors\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nvocab_size = len(tokenizer.word_index) + 1\n\n# Padding allows us to have equal length in all texts;\n# short text vectors will be extended with zeros,\n# long ones will be truncated\n# The 'maxlen' parameter also can be modified\nX_train = np.asarray(pad_sequences(X_train, padding='post', maxlen=200)).astype(np.int)\nX_val = np.asarray(pad_sequences(X_val, padding='post', maxlen=200)).astype(np.int)\nX_test = np.asarray(pad_sequences(X_test, padding='post', maxlen=200)).astype(np.int)","cfc1a171":"# F1 score is a good option to evaluate a binary classifier;\n# unfortunately, Keras does't have F1 metric, so we should define it by ourselves\n# The credit for this piece of code goes to https:\/\/datascience.stackexchange.com\/a\/45166\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","7bb3002c":"# Defining a model which embeds the word vector, flattens it to 1D array\n# and passes them through Dense layers\nmodel = models.Sequential()\n\nembedding_dim = 100\n\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=200))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","ed33c981":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=[Precision(), Recall(), AUC(), f1])\n\nmodel.summary()","7d316e69":"# Training and evaluating; EarlyStopping is added in order not to wait for all 100 epochs\n# and reduce possible overfitting\nmodel.fit(X_train, Y_train,\n        epochs=100, validation_data=(X_val, Y_val),\n        callbacks=[callbacks.EarlyStopping(monitor='val_f1',\n                                   patience=5,\n                                   verbose=1,\n                                   restore_best_weights=True,\n                                   mode='max')])","ccf916da":"model.evaluate(X_test, Y_test)","c1a07e19":"**Greetings!**\n\nToday we'll use a Keras neural network and automatic translation tools to find out whether the articles written on English or Hindi are related to mobile devices or not.","c96dd713":"Though this solution doesn't include any unusual steps except of, maybe, automatic translation, it shows good performance in terms of both F1 and AUC. I guess we can do even better by using the recurrent layers such as SimpleRNN and LSTM which are the state-of-art for analyzing sequential data, including textual information.\n\nThe next versions of this notebook will include the task solution for tweets data and (maybe) the recurrent networks usage.\n\n**Thanks for your attention and any possible comments :)**"}}