{"cell_type":{"5ab1d4a4":"code","35670d16":"code","07fa8f5f":"code","36f79bde":"code","466e1aeb":"code","da27adbc":"code","a4647c9f":"code","ff42d33a":"code","1374a7fb":"code","c86802b0":"code","a4908cc7":"code","470b911b":"code","1994335e":"code","fdc99fe8":"code","310415a0":"code","4ccba6a5":"code","baf6e04a":"code","e418363b":"code","5545a919":"code","c1110626":"code","2777bee4":"code","46030a7a":"code","e1f322b7":"code","97aca04b":"code","7d8bd96c":"code","f781b199":"code","031361e8":"code","6a2791f8":"code","0ff96bc5":"code","f4802dc6":"code","84b8a4d2":"code","14e9e49d":"code","75734893":"code","9b8d0d63":"code","79d1517e":"code","b9385f3b":"markdown","92e301cf":"markdown"},"source":{"5ab1d4a4":"def plot_history(history, desc = ''):\n    \n    fig = plt.figure(figsize = (18 , 6))\n    \n    if desc:\n        plt.title('{}'.format(desc), fontsize = 16, y = -0.1)\n\n    subplot = (1, 2, 1)\n    fig.add_subplot(*subplot)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['train loss', 'valid loss'])\n    plt.grid(True)\n    plt.plot()\n    \n    subplot = (1, 2, 2)\n    fig.add_subplot(*subplot)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.legend(['train acc', 'valid acc'])\n    plt.grid(True)\n    plt.plot()\n\ndef crop_brain_contour(image, plot=False):\n    \n    # Convert the image to grayscale, and blur it slightly\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n    \n    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n    thresh = cv2.erode(thresh, None, iterations=2)\n    thresh = cv2.dilate(thresh, None, iterations=2)\n\n    # Find contours in thresholded image, then grab the largest one\n    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    c = max(cnts, key=cv2.contourArea)\n    \n    # extreme points\n    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n    extRight = tuple(c[c[:, :, 0].argmax()][0])\n    extTop = tuple(c[c[:, :, 1].argmin()][0])\n    extBot = tuple(c[c[:, :, 1].argmax()][0])\n    \n    # crop new image out of the original image using the four extreme points (left, right, top, bottom)\n    new_image = image[extTop[1]:extBot[1], extLeft[0]:extRight[0]]  \n    \n    return new_image\n\n","35670d16":"import os \nimport re\nimport cv2\nimport numpy as np\nimport seaborn as sns\n!pip install imutils\nimport imutils\nimport tensorflow as tf\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom IPython.display import Image\nimport matplotlib.cm as cm\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nbarsize = '{l_bar}{bar:10}{r_bar}{bar:-10b}'","07fa8f5f":"IMGS = 256","36f79bde":"main_path = '..\/input\/mri-augmented\/brain_tumor'\nclasses = ['no','yes']\nimages = []\nlabels = []\nfor i in classes:\n    sub_path = os.path.join(main_path, i)\n    temp = os.listdir(sub_path)\n    for x in temp:\n        addr = os.path.join(sub_path, x)\n        img_arr = cv2.imread(addr)\n#         img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\n        img_arr = crop_brain_contour(img_arr, False)\n        img_arr = cv2.resize(img_arr, (IMGS, IMGS))\n        images.append(img_arr)\n        if i == 'yes':\n            l = 1\n        else:\n            l = 0\n        labels.append(l)\n\nimages = np.array(images)\nlabels = np.array(labels)\nprint(images.shape, labels.shape)","466e1aeb":"main_path = '..\/input\/mri-augmented\/brain_tumor'\nclasses = ['no','yes']\nprint(classes)","da27adbc":"count = {}\npath = main_path\nfor z in tqdm(classes, bar_format = barsize):\n    count[z] = len(os.listdir(os.path.join(path, z)))\nprint('Classes : ', count)","a4647c9f":"from sklearn.model_selection import train_test_split as tts\nx_train, x_test, y_train, y_test = tts(images, labels, random_state = 42, test_size = .30)\nx_valid, x_test, y_valid, y_test = tts(x_test, y_test, random_state = 42, test_size = .50)","ff42d33a":"x_train.shape","1374a7fb":"fig = plt.figure(figsize = (12, 4))\nplt.grid(True)\nplt.axis(False)\n\nfig.add_subplot(1, 4, 1)\nsns.countplot(labels, palette = 'autumn')\nplt.xlabel('All')\n\nfig.add_subplot(1, 4, 2)\nsns.countplot(y_train, palette = 'autumn')\nplt.xlabel('Train')\n\nfig.add_subplot(1, 4, 3)\nsns.countplot(y_test, palette = 'autumn')\nplt.xlabel('Test')\n\nfig.add_subplot(1, 4, 4)\nsns.countplot(y_valid, palette = 'autumn')\nplt.xlabel('Valid')\n\nplt.show()","c86802b0":"x_train = x_train.reshape(-1, IMGS, IMGS, 3)\nx_valid = x_valid.reshape(-1, IMGS, IMGS, 3)\nx_test = x_test.reshape(-1, IMGS, IMGS, 3)","a4908cc7":"fig = plt.figure(figsize = (16,7))\nz = np.random.randint(1, 250, 11)\nrows = 2\ncolumns = 5\nfor i in range(1, rows*columns+1):\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(images[z[i]])\n    plt.title(classes[labels[z[i]]])\n    plt.axis(False)\nplt.show()","470b911b":"print(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\nprint(x_valid.shape, y_valid.shape)","1994335e":"class my_callbacks(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs = {}):\n        if (logs.get('val_accuracy') > .99):\n            print(\"Accuracy is High Enough so Stopping Training\")\n            self.model.stop_training= True","fdc99fe8":"base_model = tf.keras.applications.VGG16(weights = 'imagenet',\n                                            include_top = False,\n                                            input_shape = (IMGS, IMGS, 3))\n\nlast = base_model.get_layer('block3_pool').output\nx = layers.GlobalAveragePooling2D()(last)\nx = layers.Dense(256, activation = 'relu')(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(128, activation = 'relu')(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(32, activation = 'relu')(x)\npred = layers.Dense(1, activation = 'sigmoid')(x)","310415a0":"import keras\nmodel = keras.engine.Model(base_model.input, pred)\nmodel.compile(loss='binary_crossentropy',optimizer = tf.keras.optimizers.Adam(lr=0.0001),metrics=['accuracy'])","4ccba6a5":"callback = my_callbacks()\nhistory = model.fit(x_train, y_train, epochs = 100, batch_size = 32, validation_data = (x_valid, y_valid), callbacks = [callback])","baf6e04a":"plot_history(history)","e418363b":"model.evaluate(x_test, y_test)","5545a919":"img_arr = cv2.imread('..\/input\/mri-data\/1.jpg')\n# img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\nimg_arr = crop_brain_contour(img_arr, False)\nimg_arr = cv2.resize(img_arr, (IMGS, IMGS))\nplt.imshow(img_arr)\nplt.show()\nimg_arr = img_arr.reshape(1, IMGS, IMGS, 3)\nprint(classes[int(np.round(model.predict(img_arr)[0][0]))])","c1110626":"lay = []\nfor layer in model.layers:\n    lay.append(layer.name)\n    print(layer.name)","2777bee4":"model_builder = model\nimg_size = (IMGS, IMGS)\ndef preprocess_input(img):\n    img_arr = cv2.imread(img)\n#     img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\n    img_arr = crop_brain_contour(img_arr, False)\n    img_arr = cv2.resize(img_arr, img_size)\n    img_arr = img_arr.reshape(IMGS, IMGS, 3)\n    return img_arr\n\ndef decode_prediction(prediction):\n    return classes[int(prediction[0])]\n\nlast_conv_layer_name = lay[9]\n\nclassifier_layer_names = lay[10:]\n\nimg_path = '..\/input\/mri-data\/1.jpg'\n\nplt.imshow(preprocess_input(img_path))\nplt.show()","46030a7a":"def get_img_array(img_path, size):\n    img_arr = cv2.imread(img_path)\n#     img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\n    img_arr = crop_brain_contour(img_arr, False)\n    img_arr = cv2.resize(img_arr, (IMGS, IMGS))\n    img_arr = img_arr.reshape(1, IMGS, IMGS, 3)\n    return img_arr\n#     return img_arr","e1f322b7":"def make_gradcam_heatmap(img_array, model, last_conv_layer_name, classifier_layer_names):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer\n    last_conv_layer = model.get_layer(last_conv_layer_name)\n    last_conv_layer_model = tf.keras.Model(model.inputs, last_conv_layer.output)\n\n    # Second, we create a model that maps the activations of the last conv\n    # layer to the final class predictions\n    classifier_input = tf.keras.Input(shape = last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for layer_name in classifier_layer_names:\n        x = model.get_layer(layer_name)(x)\n    classifier_model = tf.keras.Model(classifier_input, x)\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(img_array)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) \/ np.max(heatmap)\n    return heatmap\n","97aca04b":"img_array = get_img_array(img_path, size = img_size)\nprint(img_array.shape)\n\n# # Make model\n# model = model_builder(weights=\"imagenet\")\n# # Print what the top predicted class is\npreds = model.predict(img_array)\n# print(preds)\nprint(\"Predicted:\", classes[int(np.round(model.predict(img_arr)[0][0]))])\n\n# Generate class activation heatmap\nheatmap = make_gradcam_heatmap(\n    img_array, model, last_conv_layer_name, classifier_layer_names\n)\n# Display heatmap\nplt.matshow(heatmap)\nplt.show()","7d8bd96c":"import tensorflow\nimg = tf.keras.preprocessing.image.load_img(img_path)\nimg = tf.keras.preprocessing.image.img_to_array(img)\n\n# We rescale heatmap to a range 0-255\nheatmap = np.uint8(255 * heatmap)\n\n# We use jet colormap to colorize heatmap\njet = cm.get_cmap(\"jet\")\n\n# We use RGB values of the colormap\njet_colors = jet(np.arange(256))[:, :3]\njet_heatmap = jet_colors[heatmap]\n\n# We create an image with RGB colorized heatmap\njet_heatmap = tensorflow.keras.preprocessing.image.array_to_img(jet_heatmap)\njet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\njet_heatmap = tensorflow.keras.preprocessing.image.img_to_array(jet_heatmap)\n\n# Superimpose the heatmap on original image\nsuperimposed_img = jet_heatmap * 0.4 + img\nsuperimposed_img = tensorflow.keras.preprocessing.image.array_to_img(superimposed_img)\n\n# Save the superimposed image\nsave_path = \"elephant_cam.jpg\"\nsuperimposed_img.save(save_path)\n\n# Display Grad CAM\ndisplay(Image(save_path))","f781b199":"y_pred = model.predict(x_test)","031361e8":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(np.round(y_pred),y_test))","6a2791f8":"y_pred = np.round(y_pred)\ny_pred = y_pred.reshape(-1,)","0ff96bc5":"y_test = y_test.astype('float')\ny_test.shape","f4802dc6":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\nprint('''[\u2018True Neg\u2019,\u2019False Pos\u2019\n,\u2019False Neg\u2019,\u2019True Pos\u2019]''')\nplt.show()","84b8a4d2":"from keras.models import load_model\nmodel.save('best_model.h5')\nnew_model = load_model('best_model.h5')","14e9e49d":"new_model = load_model('best_model.h5')","75734893":"new_model.evaluate(x_test, y_test)","9b8d0d63":"new_model.summary()","79d1517e":"keras.utils.plot_model(model)","b9385f3b":"##### For Image Augmentation Purposes i have flipped the images.","92e301cf":"# Declaring the Image Size and Importing"}}