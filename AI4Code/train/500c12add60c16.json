{"cell_type":{"a8fcca19":"code","79c128ec":"code","67ca35f3":"code","6688a01d":"code","d3c8ec8a":"code","53b2e524":"code","6a10885b":"code","1affb879":"code","2507e432":"code","d133d42b":"code","f665aac4":"code","5cbe1b6f":"code","b9f05e54":"code","790bebe0":"code","da23c237":"code","fbe1a08e":"markdown","1805244d":"markdown","0eb3ca60":"markdown","c4ca08dc":"markdown"},"source":{"a8fcca19":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79c128ec":"train_df = pd.read_csv('\/kaggle\/input\/nlpgettingstarted\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlpgettingstarted\/test.csv')\nsample_submission_df = pd.read_csv('\/kaggle\/input\/nlpgettingstarted\/sample_submission.csv')\nprint(train_df.shape)\nprint(test_df.shape)\nprint(sample_submission_df.shape)","67ca35f3":"train_df.head()","6688a01d":"train_df.isna().sum()","d3c8ec8a":"import re\ndef preprocess_text(text):\n    text = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','URL', text)\n    text = re.sub('@[^\\s]+','USER', text)\n    text = text.lower().replace(\"\u0451\", \"\u0435\")\n    text = re.sub('[^a-zA-Z\u0430-\u044f\u0410-\u042f1-9]+', ' ', text)\n    text = re.sub(' +',' ', text)\n    return text.strip()","53b2e524":"train_df.text = train_df.text.apply(preprocess_text)\ntest_df.text = test_df.text.apply(preprocess_text)","6a10885b":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\ntokenizer = Tokenizer(num_words=1000, lower=True)\ntokenizer.fit_on_texts(train_df.text)\n\nword_seq_train = tokenizer.texts_to_sequences(train_df.text)\n# word_seq_test = tokenizer.texts_to_sequences(test_df.text)\nword_index = tokenizer.word_index\n\nword_seq_train = sequence.pad_sequences(word_seq_train, padding='post', truncating='post')\n# word_seq_test = sequence.pad_sequences(word_seq_test)","1affb879":"# # Download GloVe pre-trained \n\n# import requests, zipfile, io\n# zip_file_url = \"http:\/\/nlp.stanford.edu\/data\/glove.840B.300d.zip\"\n# r = requests.get(zip_file_url)\n# z = zipfile.ZipFile(io.BytesIO(r.content))\n# z.extractall()","2507e432":"# Get glove embeddings\nimport tqdm\noutput_dir = '\/kaggle\/working\/'\n\nglove_file = \"glove.6B.100d.txt\"\nif not os.path.exists(output_dir + glove_file):\n    if glove_file.startswith('glove.6B.'):\n        url = \"https:\/\/nlp.stanford.edu\/data\/glove.6B.zip\"\n    else:\n        url = f\"https:\/\/nlp.stanford.edu\/data\/{glove_file.rstrip('.txt')}.zip\"\n    local_filename = output_dir + url.split('\/')[-1]\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        with open(local_filename, 'wb') as f:\n            for chunk in tqdm.tqdm(r.iter_content(chunk_size=8192)): \n                f.write(chunk)\n    import zipfile\n    with zipfile.ZipFile(local_filename, 'r') as zip_ref:\n        zip_ref.extractall(output_dir)","d133d42b":"# Load GloVe pre-trained \n\nimport codecs\nfrom tqdm import tqdm\nembeddings_index = {}\n\n# f = codecs.open('glove.840B.300d.txt', encoding='utf-8')\nf = codecs.open('glove.6B.100d.txt', encoding='utf-8')\n\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","f665aac4":"# Create Embedding Matrix to be used in the Embedding layer\nwords_not_found = []\n\nMAX_NB_WORDS = 100000\n# embed_dim = 300\nembed_dim = 100\n\nnb_words = min(MAX_NB_WORDS, len(word_index)+1)\nembedding_matrix = np.zeros((nb_words, embed_dim))\n\nfor word, i in word_index.items():\n  if i >= nb_words:\n     continue\n  embedding_vector = embeddings_index.get(word)\n  \n  if (embedding_vector is not None) and len(embedding_vector) > 0:\n     embedding_matrix[i] = embedding_vector\n  else:\n     words_not_found.append(word)\nprint('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","5cbe1b6f":"# from keras.layers import BatchNormalization, Embedding, Bidirectional, LSTM, Dense, Dropout, GlobalMaxPool1D, GlobalAveragePooling1D\n# import tensorflow as tf\n\n# max_seq_len=30\n\n# model = tf.keras.Sequential()\n# model.add(Embedding(nb_words, embed_dim, input_length=max_seq_len, weights=[embedding_matrix],trainable=False))\n# # model.add(Bidirectional(LSTM(32, return_sequences= False)))\n# model.add(Bidirectional(LSTM(32, return_sequences= True)))\n# # model.add(GlobalMaxPool1D())\n# model.add(GlobalAveragePooling1D())\n\n# model.add(Dense(32,activation='relu'))\n# model.add(Dropout(0.3))\n# model.add(Dense(1,activation='sigmoid'))\n# model.summary()","b9f05e54":"from keras.layers import BatchNormalization, Embedding, Bidirectional, LSTM, Dense, Dropout, GlobalMaxPool1D, GlobalAveragePooling1D, Input, Concatenate\nfrom keras import Model\nimport tensorflow as tf\n\ninp = Input(shape=word_seq_train.shape[1])\nemb = Embedding(input_dim=embedding_matrix.shape[0], \n                output_dim=embedding_matrix.shape[1], \n                embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n                trainable=False)(inp)\nbi_lstm = Bidirectional(LSTM(32, return_sequences= True))(emb)\ngmax_pool = GlobalMaxPool1D()(bi_lstm)\ngavg_pool = GlobalAveragePooling1D()(bi_lstm)\nconcat = Concatenate()([gmax_pool, gavg_pool])\ndense_1 = Dense(32,activation='relu')(concat)\nout = Dense(1, activation='sigmoid')(dense_1)\n\nmodel = Model(inputs = [inp], outputs = [out])\nmodel.summary()","790bebe0":"from tensorflow.keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nes_callback = EarlyStopping(monitor='val_loss', patience=10)\nhistory = model.fit(word_seq_train, train_df.target, batch_size=256, epochs=30, validation_split=0.2, callbacks=[es_callback], shuffle=False)","da23c237":"import matplotlib.pyplot as plt\n\nprint(history.history.keys())\n#  \"Accuracy\"\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","fbe1a08e":"# Model","1805244d":"# Word Embedding","0eb3ca60":"# Preprocessing Data","c4ca08dc":"# Input Data"}}