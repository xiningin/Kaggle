{"cell_type":{"a027ffb0":"code","7d2736ca":"code","b0b3c017":"code","4374778d":"code","dd45538d":"code","12ffd2e0":"code","610221c9":"code","037a2c65":"code","0b351591":"code","1f758dd3":"code","b773acf4":"code","9e89ce74":"code","b364c26e":"code","62ace632":"code","3be28573":"code","41501dfe":"code","1d0a5914":"code","4388b3c2":"code","2853f70c":"code","baa49ef7":"code","e5223682":"markdown","ef2498b2":"markdown","e7471a18":"markdown","b9d364df":"markdown","da89f67d":"markdown","d5e137ef":"markdown","89344c16":"markdown","f40f2020":"markdown","8f18e35b":"markdown","938b2d25":"markdown","7e930c82":"markdown","db366959":"markdown"},"source":{"a027ffb0":"import numpy as np\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport os\nimport fnmatch\nimport cv2\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')","7d2736ca":"img_dog=Image.open(\"..\/input\/catpic\/cat.10.jpg\")\nimg_cat=Image.open(\"..\/input\/dogpic\/dog.1.jpg\")\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_dog);\nax[1].imshow(img_cat);","b0b3c017":"filename = \"cats\" # filename in my pycharm\n# This is a function for finding images in my file \ndef find_files(directory, pattern):\n    for root, dirs, files in os.walk(directory):\n        for basename in files:\n            if fnmatch.fnmatch(basename, pattern):\n                filename = os.path.join(root, basename)\n                yield filename","4374778d":"cat_files=[] # empty list \nfor filename in find_files(filename,\"*jpg\"): # Defined function begin to magic show\n    file,ext = os.path.split(filename)\n    img = Image.open(filename)\n    img = img.resize((64,64), Image.ANTIALIAS) # Arrange images size 64x64\n    data = np.array( img,np.uint8) # Convert images to matrix\n    data=data.sum(axis=2) # This code deacrease column two from three for each array\n    np.save(\"cat\",data)\n    c = np.load(\"cat.npy\")\n    cat_files.append(c)  \nnp.asarray(cat_files) # Convert list to array\nnp.save(\"cat1\",cat_files) # final form ","dd45538d":"x1 = np.load(\"..\/input\/images\/cat1.npy\")\nx2 =np.load(\"..\/input\/images\/dog1.npy\")\n\nxc = np.concatenate((x1,x2),axis=0)\n\nprint(xc.shape)\n\nimg_size = 64\nplt.subplot(1,2,1)\nplt.imshow(xc[1].reshape(img_size,img_size))\nplt.axis(\"off\")\nplt.subplot(1,2,2)\nplt.imshow(xc[300].reshape(img_size,img_size))\nplt.axis(\"off\")\nplt.show()","12ffd2e0":"x1 = np.load(\"..\/input\/images\/cat1.npy\")\nx2 =np.load(\"..\/input\/images\/dog1.npy\")\n\nx = np.concatenate((x1,x2),axis=0)\n\nz = np.zeros(300) # Create 300 zeros for 300 cat file\no = np.ones(300)  # Create 300 ones for 300 dog file\ny = np.concatenate((z,o),axis=0).reshape(x.shape[0],1) # do vector\nprint(\"x shape\",x.shape)\nprint(\"y shape\",y.shape)\n\n#Normalization\nx = (x-np.min(x))\/(np.max(x)-np.min(x))\n","610221c9":"from sklearn.model_selection import train_test_split # from sklearn module, create train and test data\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.45,random_state=42) # %45 will be test data\nprint(\"x train shape\",x_train.shape)\nprint(\"x test shape\",x_test.shape)\nprint(\"y train shape\",y_train.shape)\nprint(\"y test shape\",y_test.shape)","037a2c65":"# now we have 3 dimensional input array x so we need to make it flatten (2d) in order to use as input for our fist deep learning model.\n\nnumber_of_train = x_train.shape[0]\nnumber_of_test = x_test.shape[0]\n\nx_train_flat = x_train.reshape(number_of_train,x_train.shape[1]*x_train.shape[2])\nx_test_flat = x_test.reshape(number_of_test,x_test.shape[1]*x_test.shape[2]) # now in two dimension\nprint(\"x train flat\",x_train_flat.shape)\nprint(\"x test flat\",x_test_flat.shape)","0b351591":"# Transpose to our test and train datas for matrix calculation\nx_train = x_train_flat.T\ny_train = y_train.T\nx_test  = x_test_flat.T\ny_test  = y_test.T\n\nprint(\"x train shape\",x_train.shape)\nprint(\"x test shape\",x_test.shape)\nprint(\"y train shape\",y_train.shape)\nprint(\"y test shape\",y_test.shape)\nprint(\"x train :\",x_train)\nprint(\"y train: \",y_train)","1f758dd3":"from sklearn import linear_model\nlr = linear_model.LogisticRegression(random_state =42,max_iter= 15000)\nprint(\"test accuracy: {} \".format(lr.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(lr.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","b773acf4":"# Define sigmoid function to predict y values between 0 and 1 \ndef sigmoid(z):\n    y_head=1\/(1+np.exp(-z))\n    return y_head","9e89ce74":"# Define the inital parameters \ndef initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n                  \"bias1\": np.zeros((3,1)),\n                  \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n                  \"bias2\": np.zeros((y_train.shape[0],1))}\n    return parameters","b364c26e":"# Begin to process with initial parameters\ndef forward_propagation_NN(x_train, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","62ace632":"def compute_cost_NN(A2, Y, parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    return cost","3be28573":"# Bacward propagation\ndef backward_propagation_NN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","41501dfe":"# Update parameters with information which came from backward process\ndef update_parameters_NN(parameters, grads, learning_rate = 0.001):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","1d0a5914":"# Create prediction between 0 and 1\ndef predict_NN(parameters,x_test):\n    \n    A2, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n\n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","4388b3c2":"# 2 - Layer neural network\ndef two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n         # forward propagation\n        A2, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n         # backward propagation\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n         # update parameters\n        parameters = update_parameters_NN(parameters, grads)\n        \n        if i % 10000 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.xticks(fontsize=20)\n    plt.yticks(fontsize=20)\n    plt.show()\n    \n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\nparameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=100000)","2853f70c":"from sklearn.model_selection import cross_val_score\naccuracy = cross_val_score(estimator= lr, X= x_train.T, y=y_train.T,cv=10)\nprint(\"average accuracy\" ,np.mean(accuracy))\nprint(\"average standard deviation\",np.std(accuracy))\n\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy:\",lr.score(x_test.T,y_test.T))","baa49ef7":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100,random_state=42)\nrf.fit(x_train.T,y_train.T)\nprint(\"Random Forest Classification Test Score:\",rf.score(x_test.T,y_test.T))\n\nimport seaborn as sns\nprint(x_test.shape)\n\ny_prediction = rf.predict(x_test.T)\ny_true = y_test.T\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_prediction)\n\n\nf, ax= plt.subplots(figsize=(6,6))\nsns.heatmap(cm,annot= True,linewidths=0.4,linecolor=\"red\",fmt= \".0f\",ax=ax)\nplt.xlabel(\"Prediction number of y\")\nplt.ylabel(\"True number of y\")\nplt.show()","e5223682":"**Conclusion**\n*  The accuracy results are not acceptable .\n* Iteration number and learning rate are not dicrease the cost value, alwas I get different cost value.\n* Problem:\n    * can be images' array set\n    *  can be Iteration number but I try different iteration and learning rate there is no significant change \n* Number of the images did not affect the accuracy result, I done  these processes with 10000 sample of image but result not change signi\ufb01cantly.\n\n**Edit**\n* Normalization of the data (x) increase accuracy values for each algorithm.(Thanks to the DATAI TEAM)\n* I add two Chapter five and six.\n* Random Forest clasification give us to higher test score.\n* I will update the kernel for others algorithms.","ef2498b2":"* Test accuary is not acceptable. When we use the logistic regression , quite likely our prediction not correct.","e7471a18":"**Chapeter 2 : Visualization of Datas**","b9d364df":"**Chapter 1 : jpg to array**\n*  In this chapter , I show that how we can arrange the images to matrix form. These code define for my pycharm library","da89f67d":"**Chapter 5: Cross Value Score**\n*  Standard deviation give us to hint about distirubition of data on the graph","d5e137ef":"**Chapter 4: 2-Layer Neural Network**\n* There are one hidden layer between the features and output.\n* We create model like logistic regression model but we have four parameters. One of them weight 1 and bias 1 use in input layer same as logistic regression model, then the other one weight 2 and bias 2 use in the hidden layer. \n* You can reach more Information about Logistic regression and classifaction  with my prevous kernels:\n    *  [Machine Learning Classification](https:\/\/www.kaggle.com\/zayon5\/machine-learning-classification)\n    * [Machine Learning Review](https:\/\/www.kaggle.com\/zayon5\/machine-learning-review)","89344c16":"**Intoduction**\n* In this kernel, I challenge myself to create image classification with deep learning(2 layer neural network). \n* I use dogs and cats images","f40f2020":"**Chapter 6 :  Random Forest Clasification With Confusion Matrix**","8f18e35b":"* Repeat same codes for dogs' images","938b2d25":"*  0 ---> Negative and  1---> Positive\n* True Pozitive : 70\n* True Negative : 95\n* False Pozitive : 47\n* False Negative: 58","7e930c82":"**Chapter 4 : Train and Test**","db366959":"**Content**\n1. jpg to array\n2. Visualization of Datas\n3. Train and Test Datas\n4. 2-Layer Neural Network\n5. Cross Value Score \n6. Random Forest Clasification With Confusion Matrix "}}