{"cell_type":{"c41789f2":"code","210a7121":"code","419eb7b4":"markdown"},"source":{"c41789f2":"#-> feed batches of equal sized images to the model (all 1728s, all 2048s, and so on)\n\n\nsegmentator_even_faster_scale512 = CellSegmentator(\n    NUC_MODEL,\n    CELL_MODEL,\n    device=\"cuda\",\n    multi_channel_model=True,\n    model_width_height = 512,\n    return_without_scale_restore=True\n)\n\n\nsegmentator_even_faster_scale_factor512 = CellSegmentator(\n    NUC_MODEL,\n    CELL_MODEL,\n    device=\"cuda\",\n    multi_channel_model=True,\n    scale_factor = 0.25,\n    return_without_scale_restore=True\n)","210a7121":"\"\"\"Package for loading and running the nuclei and cell segmentation models programmaticly.\"\"\"\nimport os\nimport sys\n\nimport cv2\nimport imageio\nimport numpy as np\nimport torch\nimport torch.nn\nimport torch.nn.functional as F\nfrom skimage import transform, util\n\nfrom hpacellseg.constants import (MULTI_CHANNEL_CELL_MODEL_URL,\n                                  NUCLEI_MODEL_URL, TWO_CHANNEL_CELL_MODEL_URL)\nfrom hpacellseg.utils import download_with_url\n\nNORMALIZE = {\"mean\": [124 \/ 255, 117 \/ 255, 104 \/ 255], \"std\": [1 \/ (0.0167 * 255)] * 3}\n\n\nclass CellSegmentator(object):\n    \"\"\"Uses pretrained DPN-Unet models to segment cells from images.\"\"\"\n\n    def __init__(\n            self,\n            nuclei_model=\".\/nuclei_model.pth\",\n            cell_model=\".\/cell_model.pth\",\n            model_width_height=None,\n            device=\"cuda\",\n            multi_channel_model=True,\n            return_without_scale_restore=False,\n            scale_factor=0.25,\n            padding=False\n    ):\n\n        if device != \"cuda\" and device != \"cpu\" and \"cuda\" not in device:\n            raise ValueError(f\"{device} is not a valid device (cuda\/cpu)\")\n        if device != \"cpu\":\n            try:\n                assert torch.cuda.is_available()\n            except AssertionError:\n                print(\"No GPU found, using CPU.\", file=sys.stderr)\n                device = \"cpu\"\n        self.device = device\n\n        if isinstance(nuclei_model, str):\n            if not os.path.exists(nuclei_model):\n                print(\n                    f\"Could not find {nuclei_model}. Downloading it now\",\n                    file=sys.stderr,\n                )\n                download_with_url(NUCLEI_MODEL_URL, nuclei_model)\n            nuclei_model = torch.load(\n                nuclei_model, map_location=torch.device(self.device)\n            )\n        if isinstance(nuclei_model, torch.nn.DataParallel) and device == \"cpu\":\n            nuclei_model = nuclei_model.module\n\n        self.nuclei_model = nuclei_model.to(self.device)\n\n        self.multi_channel_model = multi_channel_model\n        if isinstance(cell_model, str):\n            if not os.path.exists(cell_model):\n                print(\n                    f\"Could not find {cell_model}. Downloading it now\", file=sys.stderr\n                )\n                if self.multi_channel_model:\n                    download_with_url(MULTI_CHANNEL_CELL_MODEL_URL, cell_model)\n                else:\n                    download_with_url(TWO_CHANNEL_CELL_MODEL_URL, cell_model)\n            cell_model = torch.load(cell_model, map_location=torch.device(self.device))\n        self.cell_model = cell_model.to(self.device)\n        self.model_width_height = model_width_height\n        self.return_without_scale_restore = return_without_scale_restore\n        self.scale_factor = scale_factor\n        self.padding = padding\n\n    def _image_conversion(self, images):\n\n        microtubule_imgs, er_imgs, nuclei_imgs = images\n        if self.multi_channel_model:\n            if not isinstance(er_imgs, list):\n                raise ValueError(\"Please speicify the image path(s) for er channels!\")\n        else:\n            if not er_imgs is None:\n                raise ValueError(\n                    \"second channel should be None for two channel model predition!\"\n                )\n\n        if not isinstance(microtubule_imgs, list):\n            raise ValueError(\"The microtubule images should be a list\")\n        if not isinstance(nuclei_imgs, list):\n            raise ValueError(\"The microtubule images should be a list\")\n\n        if er_imgs:\n            if not len(microtubule_imgs) == len(er_imgs) == len(nuclei_imgs):\n                raise ValueError(\"The lists of images needs to be the same length\")\n        else:\n            if not len(microtubule_imgs) == len(nuclei_imgs):\n                raise ValueError(\"The lists of images needs to be the same length\")\n\n        if not all(isinstance(item, np.ndarray) for item in microtubule_imgs):\n            microtubule_imgs = [\n                os.path.expanduser(item) for _, item in enumerate(microtubule_imgs)\n            ]\n            nuclei_imgs = [\n                os.path.expanduser(item) for _, item in enumerate(nuclei_imgs)\n            ]\n\n            microtubule_imgs = list(\n                map(lambda item: imageio.imread(item), microtubule_imgs)\n            )\n            nuclei_imgs = list(map(lambda item: imageio.imread(item), nuclei_imgs))\n            if er_imgs:\n                er_imgs = [os.path.expanduser(item) for _, item in enumerate(er_imgs)]\n                er_imgs = list(map(lambda item: imageio.imread(item), er_imgs))\n\n        if not er_imgs:\n            er_imgs = [\n                np.zeros(item.shape, dtype=item.dtype)\n                for _, item in enumerate(microtubule_imgs)\n            ]\n        cell_imgs = list(\n            map(\n                lambda item: np.dstack((item[0], item[1], item[2])),\n                list(zip(microtubule_imgs, er_imgs, nuclei_imgs)),\n            )\n        )\n\n        return cell_imgs\n\n    def _pad(self, image):\n          rows, cols = image.shape[:2]\n          self.scaled_shape = rows, cols\n          img_pad= cv2.copyMakeBorder(\n                    image,\n                    32,\n                    (32 - rows % 32),\n                    32,\n                    (32 - cols % 32),\n                    cv2.BORDER_REFLECT,\n                )\n          return img_pad\n\n    def pred_nuclei(self, images):\n        \n\n        def _preprocess(images):\n            if isinstance(images[0], str):\n                raise NotImplementedError('Currently the model requires images as numpy arrays, not paths.')\n                # images = [imageio.imread(image_path) for image_path in images]\n            self.target_shapes = [image.shape for image in images]\n            #print(images.shape)\n            #resize like in original implementation with https:\/\/scikit-image.org\/docs\/dev\/api\/skimage.transform.html#skimage.transform.resize\n            if self.model_width_height:\n                images = np.array([transform.resize(image, (self.model_width_height,self.model_width_height)) \n                                  for image in images])\n            else:\n                images = [transform.rescale(image, self.scale_factor) for image in images]\n\n            if self.padding:\n              images = [self._pad(image) for image in images]\n\n            nuc_images = np.array([np.dstack((image[..., 2], image[..., 2], image[..., 2])) if len(image.shape) >= 3\n                                   else np.dstack((image, image, image)) for image in images])\n            \n            nuc_images = nuc_images.transpose([0, 3, 1, 2])\n            #print(\"nuc\", nuc_images.shape)\n\n            return nuc_images\n\n        def _segment_helper(imgs):\n            with torch.no_grad():\n                mean = torch.as_tensor(NORMALIZE[\"mean\"], device=self.device)\n                std = torch.as_tensor(NORMALIZE[\"std\"], device=self.device)\n                imgs = torch.tensor(imgs).float()\n                imgs = imgs.to(self.device)\n                imgs = imgs.sub_(mean[:, None, None]).div_(std[:, None, None])\n\n                imgs = self.nuclei_model(imgs)\n                imgs = F.softmax(imgs, dim=1)\n                return imgs\n\n        preprocessed_imgs = _preprocess(images)\n        predictions = _segment_helper(preprocessed_imgs)\n        predictions = predictions.to(\"cpu\").numpy()\n        #dont restore scaling, just save and scale later ...\n        predictions = [self._restore_scaling(util.img_as_ubyte(pred), target_shape)\n                       for pred, target_shape in zip(predictions, self.target_shapes)]\n        return predictions\n\n    def _restore_scaling(self, n_prediction, target_shape):\n        \"\"\"Restore an image from scaling and padding.\n        This method is intended for internal use.\n        It takes the output from the nuclei model as input.\n        \"\"\"\n        n_prediction = n_prediction.transpose([1, 2, 0])\n        if self.padding:\n          n_prediction = n_prediction[\n                32 : 32 + self.scaled_shape[0], 32 : 32 + self.scaled_shape[1], ...\n            ]\n        n_prediction[..., 0] = 0\n        if not self.return_without_scale_restore:\n            n_prediction = cv2.resize(\n                n_prediction,\n                (target_shape[0], target_shape[1]),\n                #try INTER_NEAREST_EXACT\n                interpolation=cv2.INTER_AREA,\n            )\n        return n_prediction\n\n    def pred_cells(self, images, precombined=False):\n\n        def _preprocess(images):\n            self.target_shapes = [image.shape for image in images]\n            for image in images:\n                if not len(image.shape) == 3:\n                    raise ValueError(\"image should has 3 channels\")\n            #resize like in original implementation with https:\/\/scikit-image.org\/docs\/dev\/api\/skimage.transform.html#skimage.transform.resize\n            if self.model_width_height:\n                images = np.array([transform.resize(image, (self.model_width_height,self.model_width_height)) \n                                  for image in images])\n            else:\n                images = np.array([transform.rescale(image, self.scale_factor, multichannel=True) for image in images])\n\n            if self.padding:\n              images = np.array([self._pad(image) for image in images])\n\n            cell_images = images.transpose([0, 3, 1, 2])\n\n            return cell_images\n\n        def _segment_helper(imgs):\n            with torch.no_grad():\n                mean = torch.as_tensor(NORMALIZE[\"mean\"], device=self.device)\n                std = torch.as_tensor(NORMALIZE[\"std\"], device=self.device)\n                imgs = torch.tensor(imgs).float()\n                imgs = imgs.to(self.device)\n                imgs = imgs.sub_(mean[:, None, None]).div_(std[:, None, None])\n                imgs = self.cell_model(imgs)\n                imgs = F.softmax(imgs, dim=1)\n                return imgs\n\n        if not precombined:\n            images = self._image_conversion(images)\n        preprocessed_imgs = _preprocess(images)\n        predictions = _segment_helper(preprocessed_imgs)\n        predictions = predictions.to(\"cpu\").numpy()\n        predictions = [self._restore_scaling(util.img_as_ubyte(pred), target_shape)\n                       for pred, target_shape in zip(predictions, self.target_shapes)]\n        return predictions","419eb7b4":"This is basically [Ramans](https:\/\/www.kaggle.com\/samusram) implementation of making the HPA Cell Segmentation faster. There , all the images are resized to 512 before being fed into the segmentation models. This works well for the public test data, as most of the images are 2048x2048 which 512 is 0.25 of. Using a scaling factor of 0.25 is recommended by the HPA Team. But Images being 3072 are also being resized to 512, which results in a rescaling factor of 0.16, which might give errors on detection.\n\nI rewrote parts of this, giving you still the possibility to run the segmentation on full batches, but using a rescale factor instead of a fixed resizing. This boosts my score 0.007 which is not too much but it might help. \n\nIn this implementation you can either use a scaling factor or a fixed size\n"}}