{"cell_type":{"778d4661":"code","17b86244":"code","88549142":"code","8aaa1d52":"code","f36fdf7e":"code","45e97b9d":"code","3f409e8d":"code","31db6d3d":"code","bec4034f":"markdown","68fe9782":"markdown","0fad9ee1":"markdown","8f06d25e":"markdown"},"source":{"778d4661":"!pip install git+https:\/\/www.github.com\/keras-team\/keras-contrib.git","17b86244":"import scipy\n\nfrom keras.datasets import mnist\n# from keras_contrib.layers.normalization import InstanceNormalization\nfrom keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\n\nimport datetime\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\nimport os\nimport glob\n\n%matplotlib inline","88549142":"class DataLoader():\n    def __init__(self, dataset_name, img_res=(128, 128)):\n        self.dataset_name = dataset_name\n        self.img_res = img_res\n\n    def load_data(self, domain, batch_size=1, is_testing=False):\n        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n        path = glob.glob('..\/input\/%s\/%s\/%s\/*' % (self.dataset_name, self.dataset_name, data_type))\n\n        batch_images = np.random.choice(path, size=batch_size)\n\n        imgs = []\n        for img_path in batch_images:\n            img = self.imread(img_path)\n            if not is_testing:\n                img = scipy.misc.imresize(img, self.img_res)\n\n                if np.random.random() > 0.5:\n                    img = np.fliplr(img)\n            else:\n                img = scipy.misc.imresize(img, self.img_res)\n            imgs.append(img)\n\n        imgs = np.array(imgs)\/127.5 - 1.\n\n        return imgs\n\n    def load_batch(self, batch_size=1, is_testing=False):\n        data_type = \"train\" if not is_testing else \"val\"\n        path_A = glob.glob('..\/input\/%s\/%s\/%sA\/*' % (self.dataset_name, self.dataset_name, data_type))\n        path_B = glob.glob('..\/input\/%s\/%s\/%sB\/*' % (self.dataset_name, self.dataset_name, data_type))\n\n        self.n_batches = int(min(len(path_A), len(path_B)) \/ batch_size)\n        total_samples = self.n_batches * batch_size\n\n        # Sample n_batches * batch_size from each path list so that model sees all\n        # samples from both domains\n        path_A = np.random.choice(path_A, total_samples, replace=False)\n        path_B = np.random.choice(path_B, total_samples, replace=False)\n\n        for i in range(self.n_batches-1):\n            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n            imgs_A, imgs_B = [], []\n            for img_A, img_B in zip(batch_A, batch_B):\n                img_A = self.imread(img_A)\n                img_B = self.imread(img_B)\n\n                img_A = scipy.misc.imresize(img_A, self.img_res)\n                img_B = scipy.misc.imresize(img_B, self.img_res)\n\n                if not is_testing and np.random.random() > 0.5:\n                        img_A = np.fliplr(img_A)\n                        img_B = np.fliplr(img_B)\n\n                imgs_A.append(img_A)\n                imgs_B.append(img_B)\n\n            imgs_A = np.array(imgs_A)\/127.5 - 1.\n            imgs_B = np.array(imgs_B)\/127.5 - 1.\n\n            yield imgs_A, imgs_B\n\n    def imread(self, path):\n        return scipy.misc.imread(path, mode='RGB').astype(np.float)\n","8aaa1d52":"class CycleGAN():\n    def __init__(self, img_rows=128, img_cols=128, channels=3, dataset_name='apple2orange'):\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.channels = channels\n        self.img_shape = (img_rows, img_cols, channels)\n        \n        self.dataset_name = dataset_name\n        self.data_loader = DataLoader(self.dataset_name, img_res=(self.img_rows, self.img_cols))\n        \n        if not os.path.isdir('images'):\n            os.mkdir('images', 0o755)\n        if not os.path.isdir(os.path.join('images', self.dataset_name)):\n            os.mkdir(os.path.join('images', self.dataset_name), 0o755)\n        \n        # Calculate output shape of D (PatchGAN)\n        patch = int(self.img_rows \/ 2**4)\n        self.disc_patch = (patch, patch, 1)\n        \n        # Number of filters in the first layer of G and D\n        self.gf = 32\n        self.df = 64\n        \n        # controls how strictly the cycle-consistency loss is enforced. Setting this value higher will ensure that you your\n        # original and reconstructed image are as close together as possible.\n        self.lambda_cycle = 10.0\n        # this value influences how dramatic are the changes\u2014especially early in the training process.\n        # Setting a lower value leads to unnecessary changes\u2014e.g. completely inverting the colors early on\n        self.lambda_id = 0.9 * self.lambda_cycle\n        \n        optimizer = Adam(0.0002, 0.5)\n        \n        # Build and compile the discriminators\n        self.d_a = self.build_discriminator()\n        self.d_b = self.build_discriminator()\n\n        self.d_a.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n        self.d_b.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n\n        # Build the generators\n        self.g_ab = self.build_generator()\n        self.g_ba = self.build_generator()\n        \n        # Input images from both domains\n        img_a = Input(shape=self.img_shape)\n        img_b = Input(shape=self.img_shape)\n        \n        # Translate images to the other domain\n        fake_b = self.g_ab(img_a)\n        fake_a = self.g_ba(img_b)\n        \n        # Translate images back to original domain\n        recon_a = self.g_ba(fake_b)\n        recon_b = self.g_ab(fake_a)\n        \n        # Identity mapping of images\n        img_a_id = self.g_ba(img_a)\n        img_b_id = self.g_ab(img_b)\n        \n        # For the combined model we will only train the generators\n        self.d_a.trainable = False\n        self.d_b.trainable = False\n        \n        # Discriminators determines validity of translated images\n        valid_a = self.d_a(fake_a)\n        valid_b = self.d_b(fake_b)\n        \n        # Combined model trains generators to fool discriminators\n        self.combined = Model(inputs=[img_a, img_b], output=[valid_a, valid_b, recon_a, recon_b, img_a_id, img_b_id])\n        self.combined.compile(loss=['mse', 'mse', 'mae', 'mae', 'mae', 'mae'],\n                             loss_weights=[1, 1, self.lambda_cycle, self.lambda_cycle, self.lambda_id, self.lambda_id], optimizer=optimizer)\n        \n    @staticmethod\n    def conv2d(layer_input, filters, f_size=4, normalization=True):\n        \"\"\"Layers used during downsampling\"\"\"\n        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n        d = LeakyReLU(alpha=0.2)(d)\n        if normalization:\n            d = InstanceNormalization()(d)\n\n        return d\n\n    @staticmethod\n    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n        \"\"\"Layers used during upsampling\"\"\"\n        u = UpSampling2D(size=2)(layer_input)\n        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n        if dropout_rate:\n            u = Dropout(dropout_rate)(u)\n        u = InstanceNormalization()(u)\n        u = Concatenate()([u, skip_input])\n\n        return u\n        \n    # Next, we build the generator code, which uses the residual skip connections as we\n    # described earlier. This is a so-called \u201cU-Net\u201d architecture, which is simpler to write\n    # than the ResNet architecture, which some implementations use.\n    def build_generator(self):\n        \"\"\"U-net Generator\"\"\"\n        # Image input\n        d0 = Input(shape=self.img_shape)\n        \n        # Downsampling\n        d1 = self.conv2d(d0, self.gf)\n        d2 = self.conv2d(d1, self.gf * 2)\n        d3 = self.conv2d(d2, self.gf * 4)\n        d4 = self.conv2d(d3, self.gf * 8)\n        \n        # Upsampling\n        u1 = self.deconv2d(d4, d3, self.gf * 4)\n        u2 = self.deconv2d(u1, d2, self.gf * 2)\n        u3 = self.deconv2d(u2, d1, self.gf)\n        \n        u4 = UpSampling2D(size=2)(u3)\n        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n        \n        return Model(d0, output_img)\n        \n    def build_discriminator(self):\n        img = Input(shape=self.img_shape)\n        \n        d1 = self.conv2d(img, self.df, normalization=False)\n        d2 = self.conv2d(d1, self.df * 2)\n        d3 = self.conv2d(d2, self.df * 4)\n        d4 = self.conv2d(d3, self.df * 8)\n        \n        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n        \n        return Model(img, validity)\n    \n    def sample_images(self, epoch, batch_i):\n        r, c = 2, 3\n\n        imgs_a = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n        imgs_b = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n        \n        # Translate images to the other domain\n        fake_b = self.g_ab.predict(imgs_a)\n        fake_a = self.g_ba.predict(imgs_b)\n        # Translate back to original domain\n        reconstr_a = self.g_ba.predict(fake_b)\n        reconstr_b = self.g_ab.predict(fake_a)\n\n        gen_imgs = np.concatenate([imgs_a, fake_b, reconstr_a, imgs_b, fake_a, reconstr_b])\n\n        # Rescale images 0 - 1\n        gen_imgs = 0.5 * gen_imgs + 0.5\n\n        titles = ['Original', 'Translated', 'Reconstructed']\n        fig, axs = plt.subplots(r, c)\n        cnt = 0\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(gen_imgs[cnt])\n                axs[i, j].set_title(titles[j])\n                axs[i,j].axis('off')\n                cnt += 1\n        fig.savefig(\"images\/%s\/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n        plt.show()\n    \n    def train(self, epochs, batch_size=1, sample_interval=50):\n        start_time = datetime.datetime.now()\n        \n        valid = np.ones((batch_size,) + self.disc_patch)\n        fake = np.zeros((batch_size,) + self.disc_patch)\n        \n        for epoch in range(epochs):\n            for batch_i, (imgs_a, imgs_b) in enumerate(self.data_loader.load_batch(batch_size)):\n                # ----------------------\n                #  Train Discriminators\n                # ----------------------\n                \n                # Translate images to opposite domain\n                fake_b = self.g_ab.predict(imgs_a)\n                fake_a = self.g_ba.predict(imgs_b)\n                \n                # Train the discriminators (original images = real \/ translated = Fake)\n                da_loss_real = self.d_a.train_on_batch(imgs_a, valid)\n                da_loss_fake = self.d_a.train_on_batch(fake_a, fake)\n                da_loss = 0.5 * np.add(da_loss_real, da_loss_fake)\n\n                db_loss_real = self.d_b.train_on_batch(imgs_b, valid)\n                db_loss_fake = self.d_b.train_on_batch(fake_b, fake)\n                db_loss = 0.5 * np.add(db_loss_real, db_loss_fake)\n\n                # Total discriminator loss\n                d_loss = 0.5 * np.add(da_loss, db_loss)\n                \n                # ------------------\n                #  Train Generators\n                # ------------------\n                g_loss = self.combined.train_on_batch([imgs_a, imgs_b], [valid, valid, imgs_a, imgs_b, imgs_a, imgs_b])\n\n                if batch_i % sample_interval == 0:\n                    self.sample_images(epoch, batch_i)\n","f36fdf7e":"gan = CycleGAN()","45e97b9d":"print('=' * 50)\nprint('Domain A Descriminator Summary')\nprint('=' * 50)\nprint(gan.d_a.summary())\n\nprint('=' * 50)\nprint('Domain B Descriminator Summary')\nprint('=' * 50)\nprint(gan.d_b.summary())","3f409e8d":"print('=' * 50)\nprint('Domain A to B Generator Summary')\nprint('=' * 50)\nprint(gan.g_ab.summary())\n\nprint('=' * 50)\nprint('Domain B to A Generator Summary')\nprint('=' * 50)\nprint(gan.g_ba.summary())","31db6d3d":"gan.train(epochs=100, batch_size=64, sample_interval=10)","bec4034f":"## Data Loader\n\nWe define our key data-holding object in the (hidden) cell below. With it, we can:\n1. load_data from disk based on the dataset name, which we will define in CycleGAN's __init__\n2. load_batch during training\u2014note this defined as a generator for greater efficiency\n3. A helper functions called imread, which we use in load_data","68fe9782":"## Defining Cycle GAN Class and building the network\n\n1. creating the two discriminators DA and DB and compiling them\n2. creating the two generators:\n\n    1. Instantiating GAB and GBA\n    2. creating placeholders for the image input for both\n    3. linking them to an image in the other domain\n    4. creating placeholders for the reconstructed images back in the original domain\n    5. creating the \u201cidentity loss\u201d constraint for both directions\n    6. Not making the parameters of the discriminators trainable for now\n    7. and compiling the two generators","0fad9ee1":"## Running CycleGAN","8f06d25e":"## Imports"}}