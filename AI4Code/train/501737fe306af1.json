{"cell_type":{"aa492395":"code","de17fddc":"code","67932085":"code","c0b2f317":"code","f329cb7e":"code","c2c108cb":"code","fed747db":"code","9209872b":"code","992f9f1b":"code","61741ba2":"code","14046c16":"code","fca4c6de":"code","7ca4be7e":"code","2386e974":"code","cab42cd3":"code","fca62d5e":"code","cdc49c7f":"code","33eeb368":"code","7b2f7eca":"code","9a37fae9":"code","f40c514e":"code","bca0c683":"code","488ee1c2":"code","b1150a24":"code","c0b57a1e":"code","2e3daef4":"code","59eb618f":"code","c30269f6":"code","9d83f477":"code","5f70e904":"code","69a2dc5b":"code","7e3c548f":"code","84e5bb26":"code","79f6a4e8":"code","21d4268a":"code","f3391c55":"code","add53c48":"code","cd7cadc6":"code","8ea77fe8":"code","e01717a8":"code","649bd979":"code","204f415f":"code","82f6acc7":"code","87ab1aae":"code","69bd18af":"markdown","4f1fa02b":"markdown","af59d704":"markdown","8453a523":"markdown","32fc84b3":"markdown","b6b708a7":"markdown","00b66338":"markdown","d0fea5be":"markdown","9c19a4d5":"markdown"},"source":{"aa492395":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom numpy import mean\nfrom numpy import std\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom catboost import Pool, cv, CatBoostClassifier\n\nfrom sklearn.feature_selection import RFECV\n\nfrom sklearn.metrics import classification_report, auc, roc_auc_score, accuracy_score, recall_score, precision_score, f1_score, plot_confusion_matrix, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, VotingClassifier\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.svm import SVC\n\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\n\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import classification_report,confusion_matrix, roc_curve\n#parameter tuning\nfrom sklearn.model_selection import GridSearchCV \nfrom matplotlib.legend_handler import HandlerLine2D\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","de17fddc":"#loading dataset\ntrain_dataset = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\ntest_dataset = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')\ndata_dict = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv')\nprint(train_dataset.shape)\nprint(test_dataset.shape)","67932085":"train_dataset['train_data']=1\ntest_dataset['train_data']=0\ntest_dataset['diabetes_mellitus']=np.NaN\nall_data=pd.concat([train_dataset, test_dataset])\n","c0b2f317":"#analysing apache_cols and vital_cols as they have lot of duplicates\napache_cols = all_data.columns[all_data.columns.str.contains('apache')]\napache_cols = [c.split('_apache')[0] for c in apache_cols] \n\nvital_cols = all_data.columns[all_data.columns.str.startswith('d1') & all_data.columns.str.contains('_max')]\nvital_cols = [(c.split('d1_')[1]).split('_max')[0] for c in vital_cols]\n\ncommon_cols = [c for c in apache_cols if c in vital_cols]\n\nfor c in common_cols:\n    var1 = f\"d1_{c}_max\"\n    var2 = f\"{c}_apache\"\n    notna_condition = all_data[var1].notna() & all_data[var2].notna()\n\n    print(f\"{c} has {np.round((all_data[notna_condition][var2]==(all_data[notna_condition][var1])).sum()\/len(all_data[notna_condition])*100,2)}% duplicates\")\n\n\n","f329cb7e":"#filling missing values for d1 cols using corresponding apache cols\nfor c in common_cols:\n    if c not in ['resprate', 'temp']:\n        # Fill empty d1_..._max column from available ..._apache column\n        all_data[f\"d1_{c}_max\"] = np.where((all_data[f\"d1_{c}_max\"].isna() \n                                            & all_data[f\"{c}_apache\"].notna()), \n                                           all_data[f\"{c}_apache\"], \n                                           all_data[f\"d1_{c}_max\"])\n\n        # Drop ..._apache column\n        all_data.drop(f\"{c}_apache\", axis=1, inplace=True)\n\n        \n# Fill empty d1_heartrate_max column from available heart_rate_apache column\nall_data[\"d1_heartrate_max\"] = np.where((all_data[\"d1_heartrate_max\"].isna() \n                                    & all_data[\"heart_rate_apache\"].notna()), \n                                   all_data[\"heart_rate_apache\"], \n                                   all_data[\"d1_heartrate_max\"])\n\n# Drop ..._apache column\nall_data.drop(\"heart_rate_apache\", axis=1, inplace=True)\n","c2c108cb":"#interchanging f1_max and f1_min values where f1_max<f1_min\nfor col in vital_cols:\n    min_col = f\"d1_{col}_min\"\n    max_col = f\"d1_{col}_max\"\n    all_data.loc[all_data[min_col] > all_data[max_col],[min_col, max_col]] = all_data.loc[all_data[min_col] > all_data[max_col], [max_col, min_col]].values\n    ","fed747db":"#identifying columns having same values and dropping them\ncols_to_drop = []\nfor i, col_1 in enumerate(all_data.columns):\n    for col_2 in all_data.columns[(i+1):]:\n        if all_data[col_1].equals(all_data[col_2]):\n            print(f\"{col_1} and {col_2} are identical.\")\n            cols_to_drop.append(col_2)\n            \nall_data.drop(cols_to_drop, axis=1, inplace = True)","9209872b":"#filling missing values for 'd1_pao2fio2ratio_max'\nall_data[\"d1_pao2fio2ratio_max\"] = np.where((all_data[\"pao2_apache\"].notna() \n                                             & all_data[\"fio2_apache\"].notna()\n                                             & all_data[\"d1_pao2fio2ratio_max\"].isna() ), \n                                            all_data[\"pao2_apache\"] \/ all_data[\"fio2_apache\"], \n                                            all_data[\"d1_pao2fio2ratio_max\"])","992f9f1b":"#dropping h1 cols as h1 columns have lot of missing values and h1 and d1 values are similar\ndrop_columns = all_data.columns[all_data.columns.str.startswith('h1')]\nall_data.drop(drop_columns, axis=1, inplace=True)","61741ba2":"all_data.drop(columns = ['Unnamed: 0', 'encounter_id', 'hospital_id', 'readmission_status'], inplace=True)","14046c16":"#Dropping rows with age=0\nall_data = all_data[all_data['age']>=16].reset_index(drop=True)\n#filling missing values of 'ethnicity' col as 'Other\/Unknown' as we have more than 5000 rows with unknown value\nall_data['ethnicity'] = all_data['ethnicity'].fillna('Other\/Unknown')\nall_data['gender'] = all_data['gender'].fillna('M')","fca4c6de":"all_data['height'] = all_data.groupby('gender')['height'].transform(lambda x: x.fillna(x.mean()))\nall_data['weight'] = all_data.groupby('gender')['weight'].transform(lambda x: x.fillna(x.mean()))\nall_data['bmi'] = all_data.groupby('gender')['bmi'].transform(lambda x: x.fillna(x.mean()))","7ca4be7e":"all_data.loc[all_data['hospital_admit_source'] == 'Acute Care\/Floor', 'hospital_admit_source'] = 'Floor'\nall_data.loc[all_data['hospital_admit_source'] == 'Step-Down Unit (SDU)', 'hospital_admit_source'] = 'SDU'\nall_data.loc[all_data['hospital_admit_source'] == 'ICU to SDU', 'hospital_admit_source'] = 'SDU'\nall_data.loc[all_data['hospital_admit_source'] == 'Other ICU', 'hospital_admit_source'] = 'ICU'\nall_data.loc[all_data['hospital_admit_source'] == 'PACU', 'hospital_admit_source'] = 'Recovery Room'\nall_data.loc[all_data['hospital_admit_source'] == 'SDU', 'hospital_admit_source'] = 'ICU'\nall_data.loc[all_data['hospital_admit_source'] == 'Chest Pain Center', 'hospital_admit_source'] = 'Other'\nall_data.loc[all_data['hospital_admit_source'] == 'Observation', 'hospital_admit_source'] = 'Other'\nall_data['hospital_admit_source'].fillna('Other', inplace = True)\n\nall_data['icu_admit_source'].fillna(all_data['hospital_admit_source'], inplace = True)\nall_data.loc[all_data['icu_admit_source'] == 'Operating Room', 'icu_admit_source'] = 'Operating Room \/ Recovery'\nall_data.loc[all_data['icu_admit_source'] == 'Emergency Department', 'icu_admit_source'] = 'Accident & Emergency'\nall_data.loc[all_data['icu_admit_source'] == 'Direct Admit', 'icu_admit_source'] = 'Other'","2386e974":"categories = all_data.dtypes[all_data.dtypes == \"object\"].index\nnumeric_cols = all_data.dtypes[all_data.dtypes != \"object\"].index\n# Imputing numerical values\nall_data[numeric_cols] = all_data[numeric_cols].fillna(all_data[numeric_cols].mean())","cab42cd3":"#dummy1 = pd.get_dummies(all_data[['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type', 'train_test']])\n\nall_dummies = pd.get_dummies(all_data[categories])\n\nall_dummies.head()","fca62d5e":"all_data = pd.concat([all_data, all_dummies], axis = 1)\nall_data = all_data.drop(categories, axis = 1)\nall_data.head()","cdc49c7f":"train_data = all_data[all_data['train_data']==1]\ntrain_data.shape","33eeb368":"test_data = all_data[all_data['train_data']==0]\ntest_data.shape","7b2f7eca":"def subset_by_iqr(df, column, whisker_width=1.5):\n    \"\"\"Remove outliers from a dataframe by column, including optional \n       whiskers, removing rows for which the column value are \n       less than Q1-1.5IQR or greater than Q3+1.5IQR.\n    Args:\n        df (`:obj:pd.DataFrame`): A pandas dataframe to subset\n        column (str): Name of the column to calculate the subset from.\n        whisker_width (float): Optional, loosen the IQR filter by a\n                               factor of `whisker_width` * IQR.\n    Returns:\n        (`:obj:pd.DataFrame`): Filtered dataframe\n    \"\"\"\n    # Calculate Q1, Q2 and IQR\n    q1 = df[column].quantile(0.25)                 \n    q3 = df[column].quantile(0.75)\n    iqr = q3 - q1\n    # Apply filter with respect to IQR, including optional whiskers\n    filter = (df[column] >= q1 - whisker_width*iqr) & (df[column] <= q3 + whisker_width*iqr)\n    return df.loc[filter]                                                     \n\n\n#imp_features = ['d1_glucose_max', 'd1_glucose_min', 'd1_creatinine_max', 'd1_bun_max', 'bmi', 'd1_hemaglobin_max', 'd1_sysbp_max', 'd1_wbc_max', 'd1_heartrate_max', 'pre_icu_los_days', 'apache_3j_diagnosis', 'd1_sysbp_min', 'd1_platelets_max']\n#imp_features = ['d1_glucose_max', 'd1_glucose_min', 'd1_creatinine_max', 'd1_bun_max', 'bmi', 'd1_hemaglobin_max', 'd1_sysbp_max', 'd1_wbc_max', 'd1_heartrate_max', 'pre_icu_los_days', 'apache_3j_diagnosis', 'd1_sysbp_min', 'd1_platelets_max']\n\nfor feature in all_data.columns:\n    cleaned_train_data = subset_by_iqr(train_data, feature, whisker_width=1.5)\n\ncleaned_train_data.shape","9a37fae9":"all_data=pd.concat([cleaned_train_data, test_data])\nall_data.shape","f40c514e":"def get_correlation(data, threshold):\n    corr_col = set()\n    corrmat = data.corr()\n    for i in range(len(corrmat.columns)):\n        for j in range(i):\n            if abs(corrmat.iloc[i, j])> threshold:\n                colname = corrmat.columns[i]\n                corr_col.add(colname)\n    return corr_col","bca0c683":"#all_data = all_data.drop('train_test')\ncorr_features = get_correlation(all_data, 0.80)\nlen(corr_features)","488ee1c2":"all_data_uncorr = all_data.drop(labels=corr_features, axis = 1)\nprint('original size of data: ',all_data.shape)\nprint('After removing co related features: ',all_data_uncorr.shape)","b1150a24":"data = all_data_uncorr[all_data_uncorr.train_data==1].drop(['train_data'], axis =1)\nx = data.drop(['diabetes_mellitus'], axis =1)\ny = data['diabetes_mellitus']\nx_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.2, random_state = 40)\ntest = all_data_uncorr[all_data.train_data==0].drop(['train_data', 'diabetes_mellitus'], axis =1)","c0b57a1e":"cat_boost = CatBoostClassifier(verbose=0, n_estimators=100)\ncat_boost.fit(x_train, y_train)\ny_pred_cat_boost=cat_boost.predict(x_val)\n# Performance Evaluation\nprint(\"Accuracy:\",accuracy_score(y_val, y_pred_cat_boost))\nprint(\"Precision:\",precision_score(y_val, y_pred_cat_boost))\nprint(\"Recall:\",recall_score(y_val, y_pred_cat_boost))\nprint(\"F1 score:\",f1_score(y_val, y_pred_cat_boost))\n\n#print(classification_report(y_test, y_pred))\ny_pred_proba = cat_boost.predict_proba(x_val)[:,1]\nprint(\"AUC score:\",roc_auc_score(y_val, y_pred_proba))\n\nplot_confusion_matrix(cat_boost, x_val, y_val) ","2e3daef4":"y_pred_cat_boost=cat_boost.predict(test)\nsubmission = test_dataset[['encounter_id']]\nsubmission['diabetes_mellitus'] = cat_boost.predict_proba(test)[:,1]\nsubmission.to_csv('cat_boost_baseline.csv',index=False)\nsubmission.head()","59eb618f":"random_forest = RandomForestClassifier(n_estimators=100, random_state=40, verbose = 1, n_jobs = -1)\nrandom_forest.fit(x_train, y_train)\ny_pred_rf=random_forest.predict(x_val)\n# Performance Evaluation\nprint(\"Accuracy:\",accuracy_score(y_val, y_pred_rf))\nprint(\"Precision:\",precision_score(y_val, y_pred_rf))\nprint(\"Recall:\",recall_score(y_val, y_pred_rf))\nprint(\"F1 score:\",f1_score(y_val, y_pred_rf))\n\n#print(classification_report(y_test, y_pred))\ny_pred_proba = random_forest.predict_proba(x_val)[:,1]\nprint(\"AUC score:\",roc_auc_score(y_val, y_pred_proba))\n\nplot_confusion_matrix(random_forest, x_val, y_val) ","c30269f6":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","9d83f477":"# Extract feature importances\nfeatures = list(x_train.columns)\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","5f70e904":"xgb = XGBClassifier(random_state =40)\nxgb.fit(x_train, y_train)\ny_pred_xgb=xgb.predict(x_val)\n# Performance Evaluation\nprint(\"Accuracy:\",accuracy_score(y_val, y_pred_xgb))\nprint(\"Precision:\",precision_score(y_val, y_pred_xgb))\nprint(\"Recall:\",recall_score(y_val, y_pred_xgb))\nprint(\"F1 score:\",f1_score(y_val, y_pred_xgb))\n\n#print(classification_report(y_test, y_pred))\ny_pred_proba = xgb.predict_proba(x_val)[:,1]\nprint(\"AUC score:\",roc_auc_score(y_val, y_pred_proba))\n\nplot_confusion_matrix(xgb, x_val, y_val) ","69a2dc5b":"#features = list(x_train.columns)\nfeature_importance_values = xgb.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","7e3c548f":"y_pred_xgb=xgb.predict(test)\nsubmission = test_dataset[['encounter_id']]\nsubmission['diabetes_mellitus'] = xgb.predict_proba(test)[:,1]\nsubmission.to_csv('xgb_baseline.csv',index=False)\nsubmission.head()","84e5bb26":"#lightGBM\nlgb = lgb.LGBMClassifier(silent=False)\nlgb.fit(x_train, y_train)\ny_pred_lgb=lgb.predict(x_val)\n# Performance Evaluation\nprint(\"Accuracy:\",accuracy_score(y_val, y_pred_lgb))\nprint(\"Precision:\",precision_score(y_val, y_pred_lgb))\nprint(\"Recall:\",recall_score(y_val, y_pred_lgb))\nprint(\"F1 score:\",f1_score(y_val, y_pred_lgb))\n\n#print(classification_report(y_test, y_pred))\ny_pred_proba = lgb.predict_proba(x_val)[:,1]\nprint(\"AUC score:\",roc_auc_score(y_val, y_pred_proba))\nplot_confusion_matrix(lgb, x_val, y_val) ","79f6a4e8":"feature_importance_values = lgb.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","21d4268a":"y_pred_lgb=lgb.predict(test)\nsubmission = test_dataset[['encounter_id']]\nsubmission['diabetes_mellitus'] = lgb.predict_proba(test)[:,1]\nsubmission.to_csv('lgb_baseline.csv',index=False)\nsubmission.head()","f3391c55":"gbc=HistGradientBoostingClassifier(random_state=40).fit(x_train,y_train)\ny_pred_gbc=gbc.predict(x_val)\n# Performance Evaluation\nprint(\"Accuracy:\",accuracy_score(y_val, y_pred_gbc))\nprint(\"Precision:\",precision_score(y_val, y_pred_gbc))\nprint(\"Recall:\",recall_score(y_val, y_pred_gbc))\nprint(\"F1 score:\",f1_score(y_val, y_pred_gbc))\n\n#print(classification_report(y_test, y_pred))\ny_pred_proba = gbc.predict_proba(x_val)[:,1]\nprint(\"AUC score:\",roc_auc_score(y_val, y_pred_proba))\nplot_confusion_matrix(gbc, x_val, y_val) \n","add53c48":"y_pred_gbc=gbc.predict(test)\nsubmission = test_dataset[['encounter_id']]\nsubmission['diabetes_mellitus'] = gbc.predict_proba(test)[:,1]\nsubmission.to_csv('lgb_baseline.csv',index=False)\nsubmission.head()","cd7cadc6":"xgb = XGBClassifier( \n                        n_estimators=200,\n                        max_depth=8,\n                        learning_rate=0.05, \n                        subsample=0.8,\n                        min_child_weight=10,\n                        colsample_bytree=0.8\n                       )\nxgb.fit(x_train, y_train)\ny_pred_xgb=xgb.predict(x_val)\n# Performance Evaluation\nprint(\"Accuracy:\",accuracy_score(y_val, y_pred_xgb))\nprint(\"Precision:\",precision_score(y_val, y_pred_xgb))\nprint(\"Recall:\",recall_score(y_val, y_pred_xgb))\nprint(\"F1 score:\",f1_score(y_val, y_pred_xgb))\n\n#print(classification_report(y_test, y_pred))\ny_pred_proba = xgb.predict_proba(x_val)[:,1]\nprint(\"AUC score:\",roc_auc_score(y_val, y_pred_proba))\n\nplot_confusion_matrix(xgb, x_val, y_val) \n","8ea77fe8":"submission = test_dataset[['encounter_id']]\nsubmission['diabetes_mellitus'] = xgb.predict_proba(test)[:,1]\nsubmission.to_csv('xgb_tuned.csv',index=False)\nsubmission.head()","e01717a8":"#lightGBM\nimport lightgbm as lgb\nlgb = lgb.LGBMClassifier(boosting_type= 'gbdt', \n        objective= 'binary', metric= 'auc', \n        learning_rate= 0.007, subsample= 1, \n        colsample_bytree=  0.2,\n        reg_alpha= 3, reg_lambda= 1,\n        scale_pos_weight= 4, n_estimators= 10000,\n        verbose= 1, max_depth=  -1, seed= 100, \n        force_col_wise= True)\nlgb.fit(x_train, y_train)\ny_pred_lgb=lgb.predict(x_val)\n# Performance Evaluation\nprint(\"Accuracy:\",accuracy_score(y_val, y_pred_lgb))\nprint(\"Precision:\",precision_score(y_val, y_pred_lgb))\nprint(\"Recall:\",recall_score(y_val, y_pred_lgb))\nprint(\"F1 score:\",f1_score(y_val, y_pred_lgb))\n\n#print(classification_report(y_test, y_pred))\ny_pred_proba = lgb.predict_proba(x_val)[:,1]\nprint(\"AUC score:\",roc_auc_score(y_val, y_pred_proba))\nplot_confusion_matrix(lgb, x_val, y_val) ","649bd979":"feature_importance_values = lgb.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","204f415f":"y_pred_lgb=lgb.predict(test)\nsubmission = test_dataset[['encounter_id']]\nsubmission['diabetes_mellitus'] = lgb.predict_proba(test)[:,1]\nsubmission.to_csv('lgb_tuned.csv',index=False)\nsubmission.head()","82f6acc7":"import lightgbm as lgb\nxgb = XGBClassifier( \n                        n_estimators=200,\n                        max_depth=8,\n                        learning_rate=0.05, \n                        subsample=0.8,\n                        min_child_weight=10,\n                        colsample_bytree=0.8\n                       )\n\n#lightGBM\nlgb = lgb.LGBMClassifier(boosting_type= 'gbdt', \n        objective= 'binary', metric= 'auc', \n        learning_rate= 0.007, subsample= 1, \n        colsample_bytree=  0.2,\n        reg_alpha= 3, reg_lambda= 1,\n        scale_pos_weight= 4, n_estimators= 10000,\n        verbose= 1, max_depth=  -1, seed= 100, \n        force_col_wise= True)\n\ngbc=HistGradientBoostingClassifier(random_state=40)\nvoting_classifier = VotingClassifier(estimators=[('xgb', xgb), ('lgb', lgb), ('gbc', gbc)], voting='soft', weights=[1, 3, 1])\nvoting_classifier.fit(x_train, y_train)\nvoting_pred = voting_classifier.predict(x_val)\n\n# Performance Evaluation\nprint(\"Accuracy:\",accuracy_score(y_val, voting_pred))\nprint(\"Precision:\",precision_score(y_val, voting_pred))\nprint(\"Recall:\",recall_score(y_val, voting_pred))\nprint(\"F1 score:\",f1_score(y_val, voting_pred))\n\n#print(classification_report(y_test, y_pred))\ny_pred_proba = voting_classifier.predict_proba(x_val)[:,1]\nprint(\"AUC score:\",roc_auc_score(y_val, y_pred_proba))\nplot_confusion_matrix(voting_classifier, x_val, y_val) ","87ab1aae":"submission = test_dataset[['encounter_id']]\nsubmission['diabetes_mellitus'] = voting_classifier.predict_proba(test)[:,1]\nsubmission.to_csv('voting_baseline.csv',index=False)\nsubmission.head()","69bd18af":"The oxigenation index or Horowitz index is the ratio of partial pressure of oxygen in blood (PaO2) and the fraction of oxygen in the inhaled air (FiO2). Therefore, we can fill the missing values of d1_pao2fio2ratio_max with pao2_apache\/fio2_apache.","4f1fa02b":"References\n\n* https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_digits_pipe.html#sphx-glr-auto-examples-compose-plot-digits-pipe-py\n\n* https:\/\/www.kaggle.com\/iamleonie\/wids-datathon-2021-diabetes-detection\n\n* https:\/\/www.kaggle.com\/paolagasp\/wids-eda-building-models\n\n* https:\/\/towardsdatascience.com\/improving-random-forest-in-python-part-1-893916666cd\n\n* https:\/\/www.kaggle.com\/cosmosankur\/xgboost-classifier#SELECTING-TRAIN-AND-TEST-DATA\n\n* https:\/\/www.kaggle.com\/parulpandey\/starter-code-with-baseline\n\n* https:\/\/www.datacamp.com\/community\/tutorials\/categorical-data\n\n* https:\/\/www.kaggle.com\/srinidhi123\/wids2020-ensemblelearning-votingclassifier\/comments\n\n* https:\/\/www.kaggle.com\/willkoehrsen\/automated-model-tuning\n\n* https:\/\/www.kaggle.com\/c\/widsdatathon2021\/discussion\/209053\n\n* https:\/\/www.kaggle.com\/nasere\/wids2021-using-lightgbm-encoding","af59d704":"# Baseline Models","8453a523":"# **LightGBM**","32fc84b3":"# Outlier Detection","b6b708a7":"# Removing correlated features","00b66338":"**Tuning XGBoost Classifier**","d0fea5be":"# **Hyperparameter Tuning**\n","9c19a4d5":"The objective of this notebook is to create models to predict diabetes in ICU patients. A detailed explanation can be accessed [here](https:\/\/towardsdatascience.com\/wids-datathon-2021-my-first-kaggle-datathon-806a99cbfb19?sk=b4883e44c6985380c213cf313d690405).\n"}}