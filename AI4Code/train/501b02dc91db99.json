{"cell_type":{"bc1a5060":"code","b23d64c7":"code","bdfeef01":"code","5113e207":"code","866351c0":"code","acd26aa6":"code","0dd2db1e":"code","7f16b4ed":"code","982877c1":"code","515d506a":"code","e29531a4":"code","17fccae0":"code","036d4321":"code","c8ccdab9":"code","7d619fa1":"code","a308cc15":"code","8b14ffa4":"code","114512a3":"code","d268c983":"code","7a7d3eaf":"code","7c8a4c9c":"code","5e3d736e":"code","12da89e9":"code","80022362":"code","a1bfc309":"code","11496ce8":"code","a1798aa0":"code","578a38dc":"code","9c82f551":"code","a0cfbdc8":"code","ea5fe04e":"code","43ba228b":"code","1952d468":"code","32b34494":"code","d2a3f9e9":"code","87f251d6":"markdown","da38317f":"markdown","a0fe57f8":"markdown","dcf387f1":"markdown","6d6c10c1":"markdown","404b17ea":"markdown","d92b2255":"markdown","425cb759":"markdown","c49ba116":"markdown","5944fd32":"markdown","780d3f94":"markdown","1b7d9145":"markdown","9e74082e":"markdown","6f191e39":"markdown","cefa8dad":"markdown","ef98eb08":"markdown"},"source":{"bc1a5060":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn import *\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport seaborn as sns  # visualization tool\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\nimport plotly.tools as tls\nimport squarify\nfrom mpl_toolkits.basemap import Basemap\nfrom numpy import array\nfrom matplotlib import cm\n\n# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Print all rows and columns\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nimport datetime as dt\nimport warnings\nimport string\nimport time\n# stop_words = []\nstop_words = list(set(stopwords.words('english')))\nwarnings.filterwarnings('ignore')\npunctuation = string.punctuation\n\n\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\n\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom mlxtend.preprocessing import minmax_scaling\n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import punkt\nstop_words = stopwords.words('english')\nfrom textblob import TextBlob\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = '{:.1f}'.format\n\nfrom tqdm import tqdm\nfrom scipy.sparse import hstack, vstack, csr_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport pandas as pd\nimport random\nfrom sklearn import *\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nfrom scipy.sparse import hstack, vstack, csr_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\nimport lightgbm as lgb\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import punkt\nstop_words = stopwords.words('english')\n\nimport string\nimport re\nimport gc","b23d64c7":"train = pd.read_csv(\"..\/input\/train.csv\", dtype={\"project_essay_3\": object, \"project_essay_4\": object})\n\ntarget = train['project_is_approved']\ntrain = train.drop('project_is_approved', axis=1)\n\ntest = pd.read_csv(\"..\/input\/test.csv\", dtype={\"project_essay_3\": object, \"project_essay_4\": object})\nresources = pd.read_csv(\"..\/input\/resources.csv\")\n\ntrain.fillna(('unk'), inplace=True)\ntest.fillna(('unk'), inplace=True)","bdfeef01":"print(\"Size of training data : \",train.shape)\nprint(\"Size of test data : \",test.shape)\nprint(\"Size of resource data : \",resources.shape)","5113e207":"train.head()","866351c0":"train.dtypes","acd26aa6":"train.info()","0dd2db1e":"train.describe()","7f16b4ed":"train.describe(include=[\"O\"])","982877c1":"for label in ['teacher_prefix', 'project_grade_category']:\n    print(train[label].value_counts())\n    \n# Teacher Prefix     \nidx = np.where(train['teacher_prefix'].isna() == True)\nprint(idx)\nprint(train.iloc[idx])\n\n# States    \nstates = np.where(train['school_state'].unique())\nprint(train['school_state'].iloc[states])","515d506a":"resources.head()","e29531a4":"resources.dtypes","17fccae0":"resources.info()","036d4321":"zero_count = train[train['teacher_number_of_previously_posted_projects'] == 0]\nzero_project_percentage = (float(zero_count.shape[0]) \/ train.shape[0]) * 100\nprint(\"Percentage of teachers with their first project: \" + str(zero_project_percentage))\n\none_count = train[train['teacher_number_of_previously_posted_projects'] == 1]\none_count_percentage = (float(one_count.shape[0]) \/ train.shape[0]) * 100\nprint(\"Percentage of teachers with only one project: \" + str(one_count_percentage))\n\nmore_than_one = train[train['teacher_number_of_previously_posted_projects'] > 1]\nmore_than_one_percentage = (float(more_than_one.shape[0]) \/ train.shape[0]) * 100\nprint(\"Percentage of teachers with more than one project: \" + str(more_than_one_percentage))\n\nplt.figure(figsize = (12, 8))\n\nsns.distplot(train['teacher_number_of_previously_posted_projects'])\nplt.title('Histogram of number of previously posted applications by the submitting teacher')\nplt.xlabel('Number of previously posted applications by the submitting teacher', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.show()\n\nplt.figure(figsize = (12, 8))\nplt.hist(train['teacher_number_of_previously_posted_projects'], bins=[0, 10, 150, 450])\nplt.title('Histogram Counting # of Teachers that Previously Posted Projects)')\nplt.xlabel('Projects')\nplt.ylabel('Count')\nplt.show()\n\n# Check to see if all teacher_id are present in the data set\n# ['teacher_id'][0]\n# ['teacher_id'][4]\nprint(len(np.where(train['teacher_id'] == train['teacher_id'][0])[0]))\nprint(len(np.where(train['teacher_id'] == train['teacher_id'][4])[0]))\n\nproject_approved = target.value_counts()\nlabels = project_approved.index\nsizes = (project_approved \/ project_approved.sum())*100\ntrace = go.Pie(labels=labels, values=sizes, hoverinfo='label+percent')\nlayout = go.Layout(title='Status of Project Proposal')\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","c8ccdab9":"train['teacher_prefix'].value_counts()","7d619fa1":"temp = train[\"teacher_prefix\"].value_counts()\ntemp_y0 = []\ntemp_y1 = []\nfor val in temp.index:\n    temp_y1.append(np.sum(target[train[\"teacher_prefix\"]==val] == 1))\n    temp_y0.append(np.sum(target[train[\"teacher_prefix\"]==val] == 0))    \ntrace1 = go.Bar(\n    x = temp.index,\n    y = temp_y1,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = temp.index,\n    y = temp_y0, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Popular Teacher prefixes in terms of project acceptance rate and project rejection rate\",\n    barmode='stack',\n    width = 1000\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","a308cc15":"top_states = train[\"school_state\"].value_counts().head(10)\nplt.figure(figsize=(12,8))\nsns.barplot(top_states.index, top_states.values)\nplt.xlabel(\"State\", fontsize=15)\nplt.ylabel(\"Number of Projects\", fontsize=15)\nplt.show()\n\ntemp = train[\"school_state\"].value_counts()\ntemp_y0 = []\ntemp_y1 = []\nfor val in temp.index:\n    temp_y1.append(np.sum(target[train[\"school_state\"]==val] == 1))\n    temp_y0.append(np.sum(target[train[\"school_state\"]==val] == 0))    \ntrace1 = go.Bar(\n    x = temp.index,\n    y = temp_y1,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = temp.index,\n    y = temp_y0, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Popular School states in terms of project acceptance rate and project rejection rate\",\n    barmode='stack',\n    width = 1000\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","8b14ffa4":"temp = train[\"project_grade_category\"].value_counts()\ntemp_y0 = []\ntemp_y1 = []\nfor val in temp.index:\n    temp_y1.append(np.sum(target[train[\"project_grade_category\"]==val] == 1))\n    temp_y0.append(np.sum(target[train[\"project_grade_category\"]==val] == 0))    \ntrace1 = go.Bar(\n    x = temp.index,\n    y = temp_y1,\n    name='Accepted Proposals'\n)\ntrace2 = go.Bar(\n    x = temp.index,\n    y = temp_y0, \n    name='Rejected Proposals'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Popular school grade levels in terms of project acceptance rate and project rejection rate\",\n    barmode='stack',\n    width = 1000\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","114512a3":"plt.figure(figsize=(12,8))\nsubject_categories = train['project_subject_categories'].value_counts()\nax = subject_categories.iloc[:15].plot(kind=\"barh\")\nax.invert_yaxis()\nplt.xlabel(\"Project Subject Category\", fontsize=15)\nplt.ylabel(\"Number of Project\", fontsize=15)\nplt.show()","d268c983":"plt.figure(figsize=(12,8))\nproject_subject_subcategories = train['project_subject_subcategories'].value_counts()\nax = project_subject_subcategories.iloc[:15].plot(kind=\"barh\")\nax.invert_yaxis()\nplt.xlabel(\"Project Subject Subcategories\", fontsize=15)\nplt.ylabel(\"Number of Projects\", fontsize=15)\nplt.show()","7a7d3eaf":"total = train.isnull().sum().sort_values(ascending = False)\npercent = (train.isnull().sum()\/train.isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head()","7c8a4c9c":"train.head()","5e3d736e":"from sklearn import preprocessing\nfrom tqdm import tqdm\nimport gc\n\nfeatures = [\n    'teacher_id', \n    'teacher_prefix', \n    'school_state', \n    'project_grade_category',\n    'project_subject_categories', \n    'project_subject_subcategories']\n\ndf_all = pd.concat([train, test], axis=0)\n    \nfor c in tqdm(features):\n    le = LabelEncoder()\n    le.fit(df_all[c].astype(str))\n    train[c] = le.transform(train[c].astype(str))\n    test[c] = le.transform(test[c].astype(str))","12da89e9":"train.head()","80022362":"# Feature engineering\n\n# Date and time\ntrain['project_submitted_datetime'] = pd.to_datetime(train['project_submitted_datetime'])\ntest['project_submitted_datetime'] = pd.to_datetime(test['project_submitted_datetime'])\n\n# Date as int may contain some ordinal value\ntrain['datetime_int'] = train['project_submitted_datetime'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntest['datetime_int'] = test['project_submitted_datetime'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n\n# Date parts\ntrain[\"year\"] = train[\"project_submitted_datetime\"].dt.year\ntrain[\"month\"] = train[\"project_submitted_datetime\"].dt.month\n#train['weekday'] = train['project_submitted_datetime'].dt.weekday\ntrain[\"hour\"] = train[\"project_submitted_datetime\"].dt.hour\ntrain[\"month_Day\"] = train['project_submitted_datetime'].dt.day\n#train[\"year_Day\"] = train['project_submitted_datetime'].dt.dayofyear\ntrain['datetime_dow'] = train['project_submitted_datetime'].dt.dayofweek\ntrain = train.drop('project_submitted_datetime', axis=1)\n\n\n# ****** Test data *********\ntest[\"year\"] = test[\"project_submitted_datetime\"].dt.year\ntest[\"month\"] = test[\"project_submitted_datetime\"].dt.month\n#test['weekday'] = test['project_submitted_datetime'].dt.weekday\ntest[\"hour\"] = test[\"project_submitted_datetime\"].dt.hour\ntest[\"month_Day\"] = test['project_submitted_datetime'].dt.day\n#test[\"year_Day\"] = test['project_submitted_datetime'].dt.dayofyear\ntest['datetime_dow'] = test['project_submitted_datetime'].dt.dayofweek\ntest = test.drop('project_submitted_datetime', axis=1)\n\n# Essay length\ntrain['e1_length'] = train['project_essay_1'].apply(len)\ntest['e1_length'] = train['project_essay_1'].apply(len)\n\ntrain['e2_length'] = train['project_essay_2'].apply(len)\ntest['e2_length'] = train['project_essay_2'].apply(len)\n\n# Title length\ntrain['project_title_len'] = train['project_title'].apply(lambda x: len(str(x)))\ntest['project_title_len'] = test['project_title'].apply(lambda x: len(str(x)))\n\n# Project resource summary length\ntrain['project_resource_summary_len'] = train['project_resource_summary'].apply(lambda x: len(str(x)))\ntest['project_resource_summary_len'] = test['project_resource_summary'].apply(lambda x: len(str(x)))\n\n# Has more than 2 essays?\ntrain['has_gt2_essays'] = train['project_essay_3'].apply(lambda x: 0 if x == 'unk' else 1)\ntest['has_gt2_essays'] = test['project_essay_3'].apply(lambda x: 0 if x == 'unk' else 1)","a1bfc309":"resources['resources_total'] = resources['quantity'] * resources['price']\n\ndfr = resources.groupby(['id'], as_index=False)[['resources_total']].sum()\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\ndfr = resources.groupby(['id'], as_index=False)[['resources_total']].mean()\ndfr = dfr.rename(columns={'resources_total':'resources_total_mean'})\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\ndfr = resources.groupby(['id'], as_index=False)[['quantity']].count()\ndfr = dfr.rename(columns={'quantity':'resources_quantity_count'})\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\ndfr = resources.groupby(['id'], as_index=False)[['quantity']].sum()\ndfr = dfr.rename(columns={'quantity':'resources_quantity_sum'})\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\n# We're done with IDs for now\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","11496ce8":"train['project_essay'] = train.apply(lambda row: ' '.join([\n    str(row['project_title']),\n    str(row['project_essay_1']), \n    str(row['project_essay_2']), \n    str(row['project_essay_3']),\n    str(row['project_essay_4']),\n    str(row['project_resource_summary'])]), axis=1)\ntest['project_essay'] = test.apply(lambda row: ' '.join([\n    str(row['project_title']),\n    str(row['project_essay_1']), \n    str(row['project_essay_2']), \n    str(row['project_essay_3']),\n    str(row['project_essay_4']),\n    str(row['project_resource_summary'])]), axis=1)\n\ntrain = train.drop([\n    'project_title',\n    'project_essay_1', \n    'project_essay_2', \n    'project_essay_3', \n    'project_essay_4',\n    'project_resource_summary'], axis=1)\ntest = test.drop([\n    'project_title',\n    'project_essay_1', \n    'project_essay_2', \n    'project_essay_3', \n    'project_essay_4',\n    'project_resource_summary'], axis=1)","a1798aa0":"train.head()","578a38dc":"from nltk.corpus import stopwords\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import punkt\nimport re\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef prep_text(text):\n    q = \"[\\'\\\u2019\\\u00b4\\\u02bb]\"\n    text = text.strip().lower()\n    text = re.sub('\\W+',' ', text)\n    text = re.sub(r'(\\\")', ' ', text)\n    text = re.sub(r\"\\\\r|\\\\n\", \" \", text)\n    text = re.sub(re.compile(\"won%st\" % q), \"will not\", text)\n    text = re.sub(re.compile(\"can%st\" % q), \"can not\", text)\n    text = re.sub(re.compile(\"n%st\" % q), \" not\", text)\n    text = re.sub(re.compile(\"%sre\" % q), \" are\", text)\n    text = re.sub(re.compile(\"%ss\" % q), \" is\", text)\n    text = re.sub(re.compile(\"%sd\" % q), \" would\", text)\n    text = re.sub(re.compile(\"%sll\" % q), \" will\", text)\n    text = re.sub(re.compile(\"%st\" % q), \" not\", text)\n    text = re.sub(re.compile(\"%sve\" % q), \" have\", text)\n    text = re.sub(re.compile(\"%sm\" % q), \" am\", text)\n    text = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n    return text\n\ntrain['project_essay'] = train['project_essay'].apply(lambda x: prep_text(x))\ntest['project_essay'] = test['project_essay'].apply(lambda x: prep_text(x))","9c82f551":"tfv = TfidfVectorizer(norm='l2', min_df=0,  max_features=8000, \n            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n            ngram_range=(1,2), use_idf=True, smooth_idf=False, sublinear_tf=True,\n            stop_words = 'english')","a0cfbdc8":"train_text = train['project_essay'].apply(lambda x: ' '.join(x))\ntest_text = test['project_essay'].apply(lambda x: ' '.join(x))\n\n# Fitting tfidf on train + test might be leaky\ntfv.fit(list(train_text.values) + list(test_text.values))\ntrain_tfv = tfv.transform(train_text)\ntest_tfv = tfv.transform(test_text)","ea5fe04e":"feat_train = train.drop('project_essay', axis=1)\nfeat_test = test.drop('project_essay', axis=1)\n\nfeat_train = csr_matrix(feat_train.values)\nfeat_test = csr_matrix(feat_test.values)\n\nX_train_stack = hstack([feat_train, train_tfv[0:feat_train.shape[0]]])\nX_test_stack = hstack([feat_test, test_tfv[0:feat_test.shape[0]]])\n\nprint('Train shape: ', X_train_stack.shape, '\\n\\nTest Shape: ', X_test_stack.shape)","43ba228b":"print(\"Building model using Light GBM and finding AUC(Area Under Curve)\")\n\ncnt = 0\np_buf = []\nn_splits = 5\nn_repeats = 1\nkf = RepeatedKFold(\n    n_splits=n_splits, \n    n_repeats=n_repeats, \n    random_state=28)\nauc_buf = []  \n\nfor train_index, valid_index in kf.split(X_train_stack):\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train_stack, target, test_size=0.20, random_state=random.seed(28))\n    print('Fold {}\/{}'.format(cnt + 1, n_splits))\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'max_depth': 7,\n        'num_leaves': 32,\n        'learning_rate': 0.02,\n        'feature_fraction': 0.80,\n        'bagging_fraction': 0.80,\n        'bagging_freq': 5,\n        'verbose': 0,\n        'lambda_l2': 1,\n    }  \n\n    model = lgb.train(\n        params,\n        lgb.Dataset(X_train, y_train),\n        num_boost_round=10000,\n        valid_sets=[lgb.Dataset(X_valid, y_valid)],\n        early_stopping_rounds=50,\n        verbose_eval=100\n        )\n\n    p = model.predict(X_valid, num_iteration=model.best_iteration)\n    auc = roc_auc_score(y_valid, p)\n\n    print('{} AUC: {}'.format(cnt, auc))\n\n    p = model.predict(X_test_stack, num_iteration=model.best_iteration)\n    if len(p_buf) == 0:\n        p_buf = np.array(p)\n    else:\n        p_buf += np.array(p)\n    auc_buf.append(auc)\n\n    cnt += 1\n    \nauc_mean = np.mean(auc_buf)\nauc_std = np.std(auc_buf)\nprint('AUC = {:.6f} +\/- {:.6f}'.format(auc_mean, auc_std))\n\nlgb_preds = p_buf\/cnt","1952d468":"l_preds = pd.DataFrame(lgb_preds)\nl_preds.columns = ['project_is_approved']\nl_preds.head()\n\nsubid = sub['id']\nlsub = pd.concat([submid, l_preds], axis=1)","32b34494":"print(\"Building model using XGBoost and finding AUC(Area Under Curve)\")\n\nkf = KFold(n_splits = 5, random_state = 28, shuffle = True)\n\ncv_scores = []\nxgb_preds = []\n\nfor train_index, test_index in kf.split(X_train_stack):\n    \n    # Split out a validation set\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train_stack, target, test_size=0.20, random_state=random.seed(28))\n    \n    # params are tuned with kaggle kernels in mind\n    xgb_params = {'eta': 0.15, \n                  'max_depth': 7, \n                  'subsample': 0.80, \n                  'colsample_bytree': 0.80, \n                  'objective': 'binary:logistic', \n                  'eval_metric': 'auc', \n                  'seed': 28\n                 }\n    \n    d_train = xgb.DMatrix(X_train, y_train)\n    d_valid = xgb.DMatrix(X_valid, y_valid)\n    d_test = xgb.DMatrix(X_test_stack)\n    \n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    model = xgb.train(xgb_params, d_train, 2000, watchlist, verbose_eval=50, early_stopping_rounds=30)\n    cv_scores.append(float(model.attributes()['best_score']))\n    xgb_pred = model.predict(d_test)\n    xgb_preds.append(list(xgb_pred))","d2a3f9e9":"x_preds = pd.DataFrame(x_preds)\nx_preds.columns = ['project_is_approved']\n\nsubmid = sub['id']\nxsub = pd.concat([submid, x_preds], axis=1)","87f251d6":"**Data Cleaning**\n* Outlier Detection\n* Imputation","da38317f":"**Overview of Data**","a0fe57f8":"**Analysis**\n* Most of the projects are submitted by** Mrs, Ms** columns.","dcf387f1":"**Analysis **\n        * Training data has 182,080 rows and a total of 16 columns\n        * Test data has 78,035 rows and a total of 15 columns\n        * Resource data has 1,541,278 rows and a total of 4 columns","6d6c10c1":"**Data Visualization:**\nWe use visualization like charts, plots and graphs to summarize the distribution and understand the relationship between variables.","404b17ea":"**Analysis**\n* The first 5 top projects submited in project subject categories are \n    1. Literacy and Language\n    2. Math and Science\n    3. Literacty and Language, Math and Science\n    4. Health and Sports\n    5. Music and The Arts","d92b2255":"****Analysis**\n* Based on the numerical data, on average \"teacher_number_of_previously_posted_projects\" the average number of projects submitted are 12 and based on the above graph the percentage of projects approved is large. \n* If experience of submitting a projects is considered, we can consider the feature \"teacher_number_of_previously_posted_projects\". Based on the plots the values are\n    * Percentage of teachers with their first project: 27.4972539543058\n    * Percentage of teachers with one project: 14.680909490333919\n    * Percentage of teachers with more than one project: 57.821836555360285\n* Based on this data, it is said that only 27% percentage of teachers had success with their first project, while teachers with only one project had 14% and teachers with more than one project had 57%.  From this data, it is unclear that feature \"teacher_number_of_previously_posted_projects\" might not be the only factor influencing the success rate of the project. \n* As the dataset contains mostly approved projects, there might be a chance of dataset missing some of the projects. This is verified with .head(). From this we notice that the first teacher has submitted 26 submisions and fourth teacher has submitted 16 projects. if the feature 'teacher_id' is checked, it is observed that the first teacher has 26 projects and the dataset has only 10 entries available i.e., 16 of their application are not present in the dataset. Similarly the fourth has 16 submissions and only 10 are presents. This says the dataset is not complete. ","425cb759":"**Analysis**\n* Highest number of projects belong to Prek-2","c49ba116":"**Analysis**\n* Highest number of projects are submitted from California. ","5944fd32":"**Analysis**\n* The first 5 top projects submited in project subject subcategories are \n    1. Literacy\n    2. Literacy and Mathematics\n    3. Literacy and Writing, Mathematics\n    4. Literacy, Literature and Writing\n    5. Mathematics","780d3f94":" **Read the Data**","1b7d9145":"**Introduction**\n\nDonorChoose.org is a US-based 501(c)(3) nonprofit organization which allows individuals to donate directly to public school classroom projects. In January 2018, 1 million projects have been funded. In 77% of public schools in the US, at least one project has been requestedon DonorChoose.org. Schools from wealthy areas are more likely to make technology requests, while schools from less affluent areas are more likely to request basic supplies. It has been noted that donors on DonorChoose typically donate to projects they have no prior relationship with, and most often fund projects serving financially challenged students. \n\n**Problem Statement**\n\nDonorChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. In January 2018, 1 million projects were funded. DonorChoose.org screen application manually and due to which the number of volunteers for this is usually high. The goal of this project is to determine a model which could simplify the process by classifying the project into approved or disapproved. This could be done using the text of project descriptions, metadata of the project, teacher and school. \n\n**Project Design**\n\nThe steps are\n\n1.  Problem Framing: From the description and problem statement, it can concluded that classification algorithms like \n    1. Linear Classifiers - Logistic Regression, Naive Bayes Classifier\n    2. Support Vector Machines\n    3. Decision Trees\n    4. Boosted Trees\n    5. Random Forest\n    6. Neural Networks\n    7. Nearest Neighbor\n    \n2. Exploratory Data Analysis\n    * Understanding Data: In this step\n        1. Summary Statistics: We use statistical methods to summarize the dataset and relationship between variables in the dataset.\n        2. Data Visualization: We use visualization like charts, plots and graphs to summarize the distribution and understand the relationship between variables. \n    * Data Cleaning:  In this step, dataset is analysed, identified for issues in data. Statistical methods are used for data-cleaning and a few methods which could be used are\n        1. Outlier Detection: Methods for identifying observations that are far from the expected value in a distribution. \n        2. Imputation: Methods for repairing or filling in corrupt or missing values in observations. \n    * Data Selection: When modelling, every observation might not be relevant. The process of reducing the scope of data to those elements that are most useful for the predictions is called as Data Selections. The statistical methods used are\n        1. Sampling: In this method, small representative sample from larger datasets are created. \n        2. Feature Selection: In this method, the variables that most relevant to the outcome variable are identified. \n    * Data Preparation: In most cases, data cannot be used directly for modeling. Some transformation is required in order to change the shape or structure of the data to make it more suitable for the solving the problem. Data preparation is done using statistical methods like\n        1. Scaling: Methods like standardization and normalization are used.\n        2. Encoding: Methods like integer encoding and one hot encoding.\n        3. Transforms: Methods like power transforms.\n        \n3. Model Evaluation: Evaluating a learning method is the crucial step in Model Evaluation. This often requires an estimation of the skill of the model when making predictions on data which is not seen during the training of the model. The planning of this process of training and evaluating a predictive model is called experimental design. \n    1. Experimental Design: \n        Methods to design a systematic experiments to compare the efffect of independent variables on an outcome, such as the choice of a machine learning algorithm on prediction accuracy.  \n\nAs a part of implementing on experimental design, methods are used to resample a dataset in order to make economic use of available data in order to estimate the skill of the model. These two represent a subfiled of statistical methods. \n   1. Resampling Methods:\n        Methods for systematically splitting a dataset into subsets for the purpose of training and evaluating a predictive model. \n        \n4 . Model Selection: The process of selecting one method as a solutionis called as model selection. Two classes of statistical methods can be used to intrepret the esimated skill of different models for the purpose of model selection. They are\n    * Statistical Hypothesis Test: Methods that quantify the likelihood of observing the result given an assumption or expectation about the result. \n    * Estimation Statistics: Methods that quantify the uncertainty of a result using confidence intervals. \n    \n5. Model Presentation: Finallly, the final model to make predictions for new data where we do not know the real outcome is presented. As a part of making predictions, it is important to quantify the confidence of the prediction.","9e74082e":"**Exploratory Data Analysis:**","6f191e39":"**Analysis**\n\n* From both .head() and .dtypes, it can be said that quantity and prize are numerical features as the data-type of them is int and float. \n*  From .info method, it can be said that only feature description has null values.","cefa8dad":"**Analysis**\n\n* From both .head() and dtypes it can be said that teacher_number_of_previously_posted_projects, project_is_approved are both integers and rest all are object in which means str data type.     \n* From .info method, it can be said that project_essay_3, project_essay_4 both have null values. As the desciption states in data set states \n    Note: Prior to May 17, 2016, the prompts for the essays were as follows:\n    project_essay_1: \"Introduce us to your classroom\"\n    project_essay_2: \"Tell us more about your students\"\n    project_essay_3: \"Describe how your students will use the materials you're requesting\"\n    project_essay_4: \"Close by sharing why your project will make a difference\"\n    Starting on May 17, 2016, the number of essays was reduced from 4 to 2, and the prompts for the first 2 essays were changed to the following:\n    project_essay_1: \"Describe your students: What makes your students special? Specific details about their background, your neighborhood, and your school are all helpful.\"\n    project_essay_2: \"About your project: How will these materials make a difference in your students' learning and improve their school lives?\"\n    For all projects with project_submitted_datetime of 2016-05-17 and later, the values of project_essay_3 and project_essay_4 will be NaN.\nThus, it can be said that there the NaN columns in project 3, project 4 could most likely be projects starting on May 17, 2016.  \n* .describes gives information about the numerical features in the dataset - teacher_number_of_previously_posted_projects, project_is_approved\n* .describes gives information about the categorical features in the dataset.\n* **Features:**\n    *     teacher_prefix - There are 5 unique values in teacher_prefix. Mrs, Ms, Mr, Teacher, Dr and also there are 4 NaN\n    *     school_state - There are 51 unique values in states and CA has the maximum number of projects submitted.\n    *     project_submitted_datetime - There are 180439 unique values \n    *     project_subject_categories - There are 51 subject categories. \n    *     project_subject_subcategories - There are 407 subject categories. ","ef98eb08":"The first step in EDA, is to understand the given dataset, the two steps in this process are\n* Summary Statistics\n* Data Visualization\n\n**Summary Statistics**\nStatistical Overview of data. In the following steps, we will \n1.  Know about the size of the data\n2. Overview of train data, test data, resource data, train_resource, test_resource.    \n3. Information regarding training data\n4. Numerical features in train data, train_resource data\n5. Categorical features in train data, train_resource data"}}