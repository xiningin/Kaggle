{"cell_type":{"e956bc6f":"code","54e0e75c":"code","61a7d376":"code","9fc9581e":"code","4ed17bbb":"code","07deefc0":"code","e5edc004":"code","96a02d42":"code","0b43b9e3":"code","fef876bc":"code","6f9ac6df":"code","20fc76d1":"code","5922cd16":"code","d5507123":"code","6c9e289b":"code","cd149c2f":"code","7482caeb":"code","017448c5":"code","27041c21":"code","00f8d9ad":"code","a5f767e2":"code","89061ddd":"code","4169245f":"code","6871ed07":"markdown","4b2536d6":"markdown","eb2cd8cd":"markdown","e4eb987c":"markdown","11be33c0":"markdown","a7cad6f3":"markdown","de785484":"markdown","1a977fd7":"markdown","1cb4ed96":"markdown","4705e1fa":"markdown","966eea28":"markdown","8861a830":"markdown","383fd60a":"markdown","abe5b130":"markdown","41896392":"markdown","3f83b44f":"markdown","43e9c06f":"markdown","55d51def":"markdown","12ea96d4":"markdown","a64e51f0":"markdown","09eebee8":"markdown","1ce1b98e":"markdown","b4a1b014":"markdown","0aad960a":"markdown","179a3768":"markdown","90e30535":"markdown","142fd4cd":"markdown","1160e2e8":"markdown","81515af0":"markdown","509e4710":"markdown","de89b8af":"markdown","daa6f7c6":"markdown","5a52b640":"markdown","320afa8f":"markdown","39cbe646":"markdown","c04b4592":"markdown","b1e6f746":"markdown","93aacd1d":"markdown","c592b04b":"markdown","4e7580fd":"markdown"},"source":{"e956bc6f":"# Data wrangling\nimport pandas as pd\nimport numpy as np\n\n# Data viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport graphviz\nsns.set_style('whitegrid')\n\n# preprocessing\nfrom sklearn.feature_selection import RFE\n\n# Ml model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn.metrics import plot_confusion_matrix\n\n# over-sampling\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Scaling\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n\nnp.warnings.filterwarnings('ignore')","54e0e75c":"loan = pd.read_csv('..\/input\/loan-data\/loan_data.csv')\ndisplay(loan)\ndisplay(loan.describe())","61a7d376":"display(loan.info())","9fc9581e":"loan[\"credit.policy\"] = loan[\"credit.policy\"].astype(\"bool\")\nloan[\"not.fully.paid\"] = loan[\"not.fully.paid\"].astype(\"bool\")\nprint(loan.info())","4ed17bbb":"loan.drop_duplicates(inplace=True)","07deefc0":"sns.heatmap(loan.isnull())","e5edc004":"fig, ax = plt.subplots(figsize=(8,5))\n\ncat = loan.select_dtypes('object').columns\n\norder = list(loan[cat[0]].value_counts().keys())\nsns.countplot(cat[0], data=loan, palette=\"vlag\", order=order)\nax.tick_params(labelrotation=90)\nax.set_title(cat[0])\n\nplt.show()\n\ntable = pd.DataFrame(loan[cat[0]].value_counts())\ntable.rename(columns={'purpose':'count'}, inplace=True)\ntable['%'] = np.round((table['count']\/table['count'].sum()) * 100, 2)\ntable","96a02d42":"numbers = loan.select_dtypes(['int64', 'float64']).columns\nloan[numbers].hist(figsize=(20,10), edgecolor='white', color='#00afb9')\nplt.show()\n\nloan[numbers].describe()","0b43b9e3":"TotalLog = np.log(loan['revol.bal'] + 1)\nTotalLog.hist(color='#00afb9')\n\nplt.show()","fef876bc":"fig, ax = plt.subplots(1,2, figsize=(15,5))\n\ninte = ['delinq.2yrs', 'pub.rec']\n\nsns.countplot(inte[0], data=loan, ax=ax[0], palette=\"vlag\")\nsns.countplot(inte[1], data=loan, ax=ax[1], palette=\"vlag\")\n\nplt.show()\n\nprint(loan[inte[0]].value_counts())\nprint('\\n', loan[inte[1]].value_counts())","6f9ac6df":"fig, ax = plt.subplots(1,2, figsize=(15,5))\n\nboole = list(loan.select_dtypes(['bool']).columns)\n\nsns.countplot(boole[0], data=loan, ax=ax[0], palette=\"vlag\")\nax[0].tick_params(labelrotation=90)\nax[0].set_title(boole[0])\n\nsns.countplot(boole[1], data=loan, ax=ax[1], palette=\"vlag\")\nax[1].tick_params(labelrotation=90)\nax[1].set_title(boole[1])\n               \nplt.show()\n\nprint(loan[boole[0]].value_counts())\nprint('\\n', loan[boole[1]].value_counts())","20fc76d1":"numbers = loan.select_dtypes(['int64', 'float64']).columns\nnumbers = numbers[:-3]\n\nsns.histplot(data=loan, x=numbers[0], hue='not.fully.paid')","5922cd16":"fig, ax = plt.subplots(2,4, figsize=(22,10))\nax=ax.ravel()\n\ncount=0\nfor i in numbers:\n    sns.histplot(data=loan, x=i, hue='not.fully.paid', ax=ax[count])\n    count+=1","d5507123":"loan.groupby('not.fully.paid')[numbers].agg(['mean', 'std'])","6c9e289b":"# Features to graph\nnumbers = loan.select_dtypes(['int64', 'float64']).columns\nnumbers = list(numbers[-3:])\nnumbers.append(\"credit.policy\")\nprint(numbers)\n\n# Viz\nfig, ax = plt.subplots(2,2, figsize=(20,10))\n\nax=ax.ravel()\n\ncount=0\nfor i in numbers:\n    sns.countplot(x=i, data=loan, hue='not.fully.paid', ax=ax[count])\n    count+=1\n\nplt.show()","cd149c2f":"# Ml values\nnumbers = loan.select_dtypes(['int64', 'float64', 'bool']).columns\n\nX = loan[numbers].iloc[:,:-1].values\ny = loan.iloc[:,-1].values.reshape(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","7482caeb":"for i in range(1,13):\n\n    select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=i)\n\n    select.fit(X_train, y_train)\n\n    mask = select.get_support()\n\n    X_train_rfe = select.transform(X_train)\n    X_test_rfe = select.transform(X_test)\n\n    score = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n\n    print(\"Test score: {:.3f}\".format(score), \" number of features: {}\".format(i))","017448c5":"select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=5)\n\nselect.fit(X_train, y_train)\n\nmask = select.get_support()\n\nX_train_rfe = select.transform(X_train)\nX_test_rfe = select.transform(X_test)\n\nscore = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n\nprint(\"Test score: {:.3f}\".format(score), \" number of features: {}\".format(5))\n\nfeatures = pd.DataFrame({'features':list(loan[numbers].iloc[:,:-1].columns), 'select':list(mask)})\ndisplay(features.T)\nfeatures = list(features[features['select']==True]['features'])\nprint(\"The selected features are: \" \"\\n\")\ndisplay(features)\n","27041c21":"features.append('not.fully.paid')\n\nprint(\"Working dataset\", \"\\n\")\nloan[features]","00f8d9ad":"loan_ros = loan[features]\nprint(\"Data before over-sampling\")\nprint(loan_ros['not.fully.paid'].value_counts(), \"\\n\")\n\n","a5f767e2":"# over-sampling\nloan_ros = loan[features]\nX = loan_ros.iloc[:,:-1]\ny = loan_ros.iloc[:,-1]\n\nros = RandomOverSampler(random_state=42)\nx_ros, y_ros = ros.fit_resample(X, y)\n\nloan_ros = x_ros\nloan_ros['not.fully.paid'] = y_ros\n\n\n#visualazing samples\nfig, ax = plt.subplots(1,2, figsize=(15,5))\n\nsns.countplot('not.fully.paid', data=loan, ax=ax[0], palette=\"vlag\")\nax[0].tick_params(labelrotation=90)\nax[0].set_title(\"Data before over-sampling\")\n\nsns.countplot('not.fully.paid', data=loan_ros, ax=ax[1], palette=\"vlag\")\nax[1].tick_params(labelrotation=90)\nax[1].set_title(\"Data after over-sampling\")\n\nplt.show()\n\nprint(\"Data before over-sampling\")\nprint(loan['not.fully.paid'].value_counts(), \"\\n\")\n\nprint(\"Data after over-sampling\")\nprint(loan_ros['not.fully.paid'].value_counts())","89061ddd":"fig, ax = plt.subplots(3,2, figsize=(22,10))\nax=ax.ravel()\n\ncount=0\nfor i in loan_ros.keys():\n    sns.histplot(data=loan_ros, x=i, hue='not.fully.paid', ax=ax[count])\n    count+=1","4169245f":"# Selecting training values\nloan_model = loan_ros\nX = loan_model.iloc[:,:-1].values\ny = loan_model.iloc[:,-1].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n# Building the model\n\n\ntree = DecisionTreeClassifier(max_depth=12, random_state=0)\ntree.fit(X_train, y_train)\n\nprint(\"Accuracy on training set : {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set : {:.3f}\".format(tree.score(X_test, y_test)), \"\\n\")\n\n\n# Confusion matrix\ndisp = plot_confusion_matrix(tree, X_test, y_test,\n                             cmap=plt.cm.Blues, display_labels=[\"not fully paid\", \"fully paid\"])\nplt.show()\n\nprint(disp.confusion_matrix, \"\\n\")\n\nprint(\"Feature importances: \")\nprint(tree.feature_importances_)","6871ed07":"# Table of Contents\n1. [Introduction](#1.-Introduction)\n\n    1.1. [Objectives](#1.1.-Objectives)\n    \n    1.2. [Features](#1.2.-Features)\n    \n2. [Packages, data loading and cleaning](#2.-Packages,-data-loading-and-cleaning)\n\n    2.1. [Packages](#2.1.-Packages)\n    \n    2.2. [Data loading](#2.2.-Data-loading)\n    \n    2.3. [Data cleaning](#2.3.-Data-cleaning)\n \n3. [Descriptive analysis](#3.-Descriptive-analysis)\n\n    3.1. [Categories](#3.1.-Categories)\n    \n    3.2. [Numbers](#3.2.-Numbers)\n    \n    3.3. [Booleans](#3.3.-Booleans)\n    \n    3.4. [Descriptive analysis conclusions and considerations.](#3.4.-Descriptive-analysis-conclusions-and-considerations.)\n\n4. [Data Analysis and EDA](#4.-Data-Analysis-and-EDA)\n    \n    4.1. [Overlaped histograms](#4.1.-Overlaped-histograms)\n    \n    4.2. [Barplots](#4.2.-Barplots)\n    \n5. [Model-based feature selection.](#5.-Model-based-feature-selection.)\n\n6. [Decision Tree](#6.-Decision-Tree)\n\n    6.1. [Resampling](#6.1.-Resampling)\n    \n    6.2. [Tree model](#6.2.-Tree-model)\n    \n    ","4b2536d6":"## 4.2. Barplots","eb2cd8cd":"we have a case of inbalanced data that could be a problem for the ML model since most of are false. When building the model, we are going to apply sampling technique to deal with the imbalanced data.","e4eb987c":"## 1.2. Features\n\n- credit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.\n- purpose: The purpose of the loan (takes values \"creditcard\", \"debtconsolidation\", \"educational\", \"majorpurchase\", \"smallbusiness\", and \"all_other\").\n- int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.\n- installment: The monthly installments owed by the borrower if the loan is funded.\n- log.annual.inc: The natural log of the self-reported annual income of the borrower.\n- dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income).\n- fico: The FICO credit score of the borrower.\n- days.with.cr.line: The number of days the borrower has had a credit line.\n- revol.bal: The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).\n- revol.util: The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).\n- inq.last.6mths: The borrower's number of inquiries by creditors in the last 6 months.\n- delinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.\n- pub.rec: The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments).","11be33c0":"### Duplicates","a7cad6f3":"With balanced data, we can go ahead and build the model. Let's first check the distributions with the newly balanced data.","de785484":"not a pattern yet, lets see what the model tells us.","1a977fd7":"# 4. Data Analysis and EDA\n\nSince our target variable is a boolean, we are going to plot the 6 scatter plots with the highest correlations and hue them with the target variable, this will be more as a visual analysis. If we don't find any pattern, I'm going to apply a model-based feature selection to reduce the number of features and analyze the most relevants for the model.\n\n'Purpose' is a categorical variable, so a barplot will do.\n\n'inq.last.6mths', 'delinq.2yrs' and 'pub.rec'describbe the number of times something happened, a barplot will also work on these cases.","1cb4ed96":"## 2.2. Data loading","4705e1fa":"# 7. Conclusions\n- We couldn't find any obervable pattern. The data the target variable is equally distributed in all the variables. It almost looks as if the dataset in synthetic.\n- Nevertheless, The the algorithm was able to reach a decent and more real test score (working with the imbalanced data would have yield a better but not real result), and was able to classify both outcomes of the target variable in the confusion matrix.\n- We couldn't meet the objective of creating a cliente profile, but the algorith could be use by agents to see how probable it is a client will pay or not, easing the decision process. ","966eea28":"## 3.4. Descriptive analysis conclusions and considerations.\n\n- 'revol.bal' is rich in outliers. for the ML model, we might take the outliers out or apply a log formula.\n- 'delinq.2yrs' and 'pub.rec' are also rich in outliers. Since they are integers, i don't think a log formula would be wise, It's better to take out the outliers for the ML model.\n- 'not.fully.paid', the target variable'is highly imbalanced. After the data analysis and before the ML model, I'm going to use a sampling technique to deal with this case.","8861a830":"## 2.1. Packages","383fd60a":"## 3.2. Numbers","abe5b130":"## 6.2. Tree model","41896392":"### Null's\nBefore dropping the null's let's frst check how they are distributed with a heatmap","3f83b44f":"### Data types","43e9c06f":"We have our working features, let's add the target variable 'not.fully.paid' and proceed aplying a decision tree. Then, I'm going to keep mining and graph the model to see if i can finally find the pattern.","55d51def":"## 3.1. Categories","12ea96d4":"There's not much difference between test scores. I'm going to choose 5 features and see what the algorithm chooses.","a64e51f0":"We have achieved an acceptable score and confussion matrix. The feature importances shows why is so difficult to find a pattern in the data, all of the features have similar importance, which means that the model is a combination of fine tunnings from all of the features. Is hard to create a client profile given these features. But the algorithm can be useful when the bank operator enters the data in the system, the algorithm will tell the agents how probable it is for the client to fully pay or not.","09eebee8":"# 1. Introduction\n\npublicly available data from LendingClub.com. Lending Club connects people who need money (borrowers) with people who have money (investors). Hopefully, as an investor you would want to invest in people who showed a profile of having a high probability of paying you back.\n\nWe will use lending data from 2007-2010 and be trying to classify and predict whether or not the borrower paid back their loan in full.\n\nA model that predicts whether a consumer will pay or not could help companies distribute its their resources better. If the companies have a borrower profile, they can, for example, allocate their best debt collectors to each case. They can also come up with paying strategies that could help the borrower pay the debt.\n\nIn this kernel, we are going to determine relevant factors that determine whether a borrower will pay a debt or not, then create a borrower profile and finally conlcude and make recommendations to better the business.","1ce1b98e":"As expected, they do not differ a lot. The only concluson we can get here is that our target variable is almost equally distributed in each feature. Therefore, there's no relevant pattern for our model.","b4a1b014":"# 6. Decision Tree","0aad960a":"## 2.3. Data cleaning\nIn this step we are going to eliminate duplicates and examine Nulls values and see if they can be discarted as well. It might be possible that the cleaning process extend itself until the descriptive analysis, where is possible that suspicious data comes up.\n\nWe are also going to take a look at the data types and modify them if they are inconsistent.","179a3768":"## 3.3. Booleans","90e30535":"From the descriptive analysis we can see that most of the borrowers are for debt consolidation (41.31%). The least category is educational. It is too soon to start concluding, but my intuition tells me that the categories with the most borrowers are also the ones with a bigger proportion of paid debt, as educational sits at the bottom having students with the least amount of money. We will wait though until the analysis part where we are going to analyse further.","142fd4cd":"## 4.1. Overlaped histograms","1160e2e8":"We know from section 2.3. that most of the features are numeric, having only one categorical and two booleans, one of which being our target variable which is our target variable. This makes our descriptive analysis quite easy since we only have to graph histograms corresponding to the numeric values, a bar plot for the categorical feature an another barplot for the boolean.","81515af0":"# 3. Descriptive analysis","509e4710":"We could leave the datatypes like that since a boolean could also be interpreted as a boolean. But, tu ease the algorithm in the descritive section we are going to convert \"credit policy\" and \"not.fully.paid\" to booleans. The remaining features are all in order.","de89b8af":"## 6.1. Resampling\nWe know from section 3.4 that our data is highly imbalanced, let's apply a resampling algorithm before normalizing the dataset. For this case, I'm going to apply a random oversampling algorithm, this will create synthethic data in the minority class.","daa6f7c6":"We can see that both distributions have some extreme values, after the data analysis, when building the ML model, we are to consider getting rid of them","5a52b640":"We can see that most of the histograms follow a normal distribution, except for revol.bal, which, from the descriptive stats, know that have some very extreme values, this could be fixed by removing the outiler or applyieng a log formula such as:","320afa8f":"# 5. Model-based feature selection.\n\nModel-based feature selection uses a supervised machine learning model to judge the importance of each feature, and keeps only the most important ones. For this case, we are going to use a random forest classifier, since it usually yields good results without having to normalize the features. Let's take a look.","39cbe646":"With the log formula, the graph looks more 'normal'. We are going to take into account both method when building the ML model.\n\nRegarding 'delinq.2yrs' and 'pub.rec', these are distributions that have their values distributed through 4-5 values. They behave like a cateforical value but they are not. A count plot could give us a better representation of these distributions","c04b4592":"No relevant insights as well. Let's try and apply a model-based feature selection and let the algorithm show us what is not visible to the eye.","b1e6f746":"# 2. Packages, data loading and cleaning","93aacd1d":"## 1.1. Objectives\n- Find relevant factors that influence whether a borrower will pay or not.\n- Build a model that predicts wheter the borrower will pay or not.\n- Create a consumer profile of borrowers that pay or not.\n- Make conclusions and reccomendations to improve bussiness operations.","c592b04b":"Unfortunately, both distributions have the same shape but in a different proportion, looking for a pattern here will be difficult. Let's group our target variable by these variables and see how the mean and standard deviation differ.","4e7580fd":"The heatmap doesn't show any null value. We can then continue with the descriptive analysis."}}