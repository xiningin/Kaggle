{"cell_type":{"80daf440":"code","cc9e35e0":"code","92c3df96":"code","fbb305eb":"code","29a6831c":"code","84b977c7":"code","7120e311":"code","5c5d958b":"code","d3606a58":"code","0abf3a0d":"code","7d850ce7":"code","b6e8bcaa":"code","511040b6":"code","8d95d394":"code","73f8b166":"code","97d3b837":"code","3885b93f":"code","dd1dd34b":"code","f7b8f4d6":"code","3278828e":"code","e14dea59":"code","71db9d0a":"code","b29959fa":"code","a83b2390":"code","042d6c53":"code","0b932355":"code","20fd79ba":"code","b06dfcb7":"code","abc3aff6":"code","ffcf797e":"code","846849c6":"code","95da05ea":"code","644a38ec":"code","bfa92ee1":"code","7cb19079":"code","f14a7094":"code","64639be6":"code","a2994aee":"code","5a01839a":"code","79ec208b":"code","0eef72d8":"code","0161cad5":"code","45a91e1b":"code","ed2dc4d8":"code","d65acc94":"code","d9a033c7":"code","829bea6b":"code","851a1c8a":"code","e4e83d1d":"code","d2158a4d":"code","d08e90e3":"code","87398831":"code","c5c3725f":"code","c5128a86":"code","8d9826fa":"code","de5d2f2b":"code","0d41b46b":"code","d190da43":"code","1c45d780":"code","9847d70d":"code","7afc2846":"code","ee24076b":"code","c60817c2":"code","088998eb":"code","fcf79f78":"markdown","cbb1be1a":"markdown","290bfd1a":"markdown","07b31f09":"markdown","78d41938":"markdown","ef342790":"markdown","73a231b9":"markdown","2d8526d4":"markdown","6fa49415":"markdown","937fa1e1":"markdown","65a0de13":"markdown","5bfd9933":"markdown","fe121bd5":"markdown","e2852ef7":"markdown","f59c4973":"markdown","04cf4623":"markdown","395d0022":"markdown","c4e16030":"markdown","ffbf42bb":"markdown","5bc0a8db":"markdown","b931028b":"markdown"},"source":{"80daf440":"# Importing the basic libraries. \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas_profiling as pdb\nimport os\nplt.style.use('seaborn-darkgrid')","cc9e35e0":"# Loading the data. \nos.listdir('\/kaggle\/input\/titanic\/')","92c3df96":"# Loading the training and testing data. \n# we will check how good or bad the data is, that we already have. \ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","fbb305eb":"# checking the training and testing data. \nprint(train_data.shape)\nprint(test_data.shape)","29a6831c":"# checking the sample data for training data. \ntrain_data.head(20)","84b977c7":"# for each columns, let's quickly look into the distinct values as well. \n# this is one of the ways in which we can understand any gaps in the data model. \nfor each_col in train_data.columns:\n    print(each_col)\n    print(train_data[each_col].value_counts())\n    print(\"\")","7120e311":"# checking basic statistics of training data. \ntrain_data.describe()","5c5d958b":"# Checking the information about the data. \n# this will tell us how many and what type of columns\/rows we have in the data. \ntrain_data.info()","d3606a58":"# Creating a list of columns which are numeric. \nnumeric_cols = list(train_data.select_dtypes(exclude='object').columns)\nnumeric_cols","0abf3a0d":"# creating a list of columns which are categorical or contains string values. \nstring_cols = list(train_data.select_dtypes(include='object').columns)\nstring_cols","7d850ce7":"# For each numerical column, let's check the data distribution. \n# for this we are using seaborn library and try performing pairplotting\nsns.pairplot(train_data[numeric_cols])\nplt.show()","b6e8bcaa":"# Clearly we do not need the Passenger ID in this analysis, which we can drop at this stage. \ntrain_data.drop('PassengerId', axis=1, inplace=True)\ntest_data.drop('PassengerId', axis=1, inplace=True)","511040b6":"# Creating again, after dropping the passenger ID column, the list of columns which are numeric. \nnumeric_cols = list(train_data.select_dtypes(exclude='object').columns)\nnumeric_cols","8d95d394":"# using pandas profiler once to look into all the data columns. \ntrain_data_profile = pdb.ProfileReport(train_data)\ntrain_data_profile","73f8b166":"## it seems we have some missing values in certain columns. \n## let's look into them again .\ntrain_data.isna().sum()","97d3b837":"# dropping the Cabin column.\ntrain_data.drop('Cabin', axis=1, inplace=True)\ntest_data.drop('Cabin', axis=1, inplace=True)","3885b93f":"## Checking the name column. \ntrain_data['Name'].value_counts().head(20)","dd1dd34b":"# Let's check the ticket column once. \ntrain_data['Ticket'].value_counts().head(20)","f7b8f4d6":"# dropping the ticket column since we could not see any meaningful or logical patterns from the data. \ntrain_data.drop('Ticket', axis=1, inplace=True)\ntest_data.drop('Ticket', axis=1, inplace=True)","3278828e":"# Re-checking the dataframe information. \ntrain_data.info()","e14dea59":"# What about the testng data? \n# does it contain missing values as well ? \ntest_data.info()","71db9d0a":"# filling the missing data of Age in training and testing data. \nfor i in train_data.columns[train_data.isnull().any(axis=0)]:\n    print(i)\n    train_data[i].fillna(train_data[i].mode(),inplace=True)","b29959fa":"## For the training and testing data, where Age is missing, we can perform imputation using mode\ntrain_data['Age'] = train_data['Age'].fillna((train_data['Age'].mode().astype('float64')))\ntest_data['Age'] = test_data['Age'].fillna((test_data['Age'].mode().astype('float64')))","a83b2390":"# rechecking the training data. \ntrain_data.info()","042d6c53":"# Looks like the earlier imputation method did not work, hence trying other options. \nfrom sklearn.impute import SimpleImputer as SI\nimr = SI(missing_values=np.nan, strategy='median')\nimr = imr.fit(train_data[['Age']])\ntrain_data[['Age']] = imr.transform(train_data[['Age']])","0b932355":"# Checking Age column now. \ntrain_data['Age'].isna().sum()\ntrain_data.info()","20fd79ba":"# Performing similar operation for Embarked column. \n# since it is a categorical column, the 'strategy' will be different. \nimr = SI(missing_values=np.nan, strategy='most_frequent')\nimr = imr.fit(train_data[['Embarked']])\ntrain_data[['Embarked']] = imr.transform(train_data[['Embarked']])","b06dfcb7":"# rechecking the dataframe information once more. \ntrain_data.info()","abc3aff6":"## Let's repeat the above steps for the Testing data again. \n\n# for Age column\nimr = SI(missing_values=np.nan, strategy='median')\nimr = imr.fit(test_data[['Age']])\ntest_data[['Age']] = imr.transform(test_data[['Age']])\n\n# for Embarked column\nimr = SI(missing_values=np.nan, strategy='most_frequent')\nimr = imr.fit(test_data[['Embarked']])\ntest_data[['Embarked']] = imr.transform(test_data[['Embarked']])","ffcf797e":"# Checking the Testing data. \ntest_data.info()","846849c6":"## In the testing data we have Fare data missing. \n## We'll treat it similarly. \n\nimr = SI(missing_values=np.nan, strategy='median')\nimr = imr.fit(test_data[['Fare']])\ntest_data[['Fare']] = imr.transform(test_data[['Fare']])","95da05ea":"# Rechecking the Testing data. \ntest_data.info()","644a38ec":"# Extracting the Title information from Name. \ntrain_data['Title'] = train_data['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\ntest_data['Title'] = test_data['Name'].str.extract('([A-Za-z]+)\\.', expand=False)","bfa92ee1":"# Check the counts of each titles\ntitle_counts = train_data.Title.value_counts()\ntitle_counts.to_frame().T\n\ntitle_counts = test_data.Title.value_counts()\ntitle_counts.to_frame().T\n","7cb19079":"# check the sex of the passengers with different titles in the full dataset\n\n# creating empty sets\nfemale_titles = set()\nmale_titles = set()\n\n# loop through data of Title column\n# add values to the 'empty' sets \nfor t in train_data.Title.unique():\n    if ((train_data.Title == t) & (train_data.Sex =='female')).any():\n        female_titles.add(t)\n    if ((train_data.Title == t) & (train_data.Sex =='male')).any():\n        male_titles.add(t)\n\n# There will be cases of mixture of titles applicable to both males and females.\nmix_titles = female_titles & male_titles\n\n# Finally creating the sets which only belongs to males and females\nfemale_only_titles = female_titles - mix_titles\nmale_only_titles = male_titles - mix_titles\n\n# Printing the findings. \nprint('mix_titles:', mix_titles)\nprint('female_only_titles', female_only_titles)\nprint('male_only_titles', male_only_titles)","f14a7094":"# repeating above steps for Test Data too.\n\n# creating empty sets\nfemale_titles = set()\nmale_titles = set()\n\n# loop through data of Title column\n# add values to the 'empty' sets \nfor t in test_data.Title.unique():\n    if ((test_data.Title == t) & (test_data.Sex =='female')).any():\n        female_titles.add(t)\n    if ((test_data.Title == t) & (test_data.Sex =='male')).any():\n        male_titles.add(t)\n\n# There will be cases of mixture of titles applicable to both males and females.\nmix_titles = female_titles & male_titles\n\n# Finally creating the sets which only belongs to males and females\nfemale_only_titles = female_titles - mix_titles\nmale_only_titles = male_titles - mix_titles\n\n# Printing the findings. \nprint('mix_titles:', mix_titles)\nprint('female_only_titles', female_only_titles)\nprint('male_only_titles', male_only_titles)","64639be6":"## Visualizing the Title information and Age\nplt.figure(figsize=(20,10))\nsns.boxplot(x='Title', y='Age', data=train_data)\nplt.show()","a2994aee":"# change the title from 'Dr' to 'Dr (female)' for the female\ntrain_data.loc[(train_data.Title=='Dr') & (train_data.Sex == 'female'), 'Title'] = 'Dr (female)'\ntest_data.loc[(test_data.Title=='Dr') & (test_data.Sex == 'female'), 'Title'] = 'Dr (female)'","5a01839a":"# creating the title groupings\ntitle_groups = {'Male adult': ['Mr', 'Don', 'Rev', 'Dr',  'Sir', 'Major',  'Col', 'Capt', 'Countess', 'Jonkheer'],\n                'Boy': ['Master'],\n                'Miss': ['Miss'],\n               'Other female': ['Mrs', 'Dona', 'Mme', 'Mlle', 'Ms', 'Lady', 'Dr (female)']}","79ec208b":"# adding the new column in the training data for title groups. \ntrain_data['Title_group'] = train_data['Title']\nfor k in title_groups:\n    train_data['Title_group'].replace(title_groups[k], k, inplace=True)","0eef72d8":"# repeating the same for testing data. \n\ntest_data['Title_group'] = test_data['Title']\nfor k in title_groups:\n    test_data['Title_group'].replace(title_groups[k], k, inplace=True)","0161cad5":"# checking the dataframe information\ntrain_data.info()","45a91e1b":"# CHecking the value counts for title group. \ntrain_data.Title_group.value_counts()","ed2dc4d8":"# CHecking the test data too. \ntest_data.Title_group.value_counts()","d65acc94":"# We can now drop the Name column from the Training and Testing data. \ntrain_data.drop('Name', axis=1, inplace=True)\ntest_data.drop('Name', axis=1, inplace=True)","d9a033c7":"# Checking the training data. \nprint(train_data.info())","829bea6b":"# Importing the Label Encoder library\nfrom sklearn.preprocessing import LabelEncoder","851a1c8a":"# Creating an object of Label Encoder\nle = LabelEncoder()","e4e83d1d":"# creating list of columns from training and testing data which are object types. \nstr_cols_train_data = train_data.select_dtypes(include='object')\nstr_cols_test_data = test_data.select_dtypes(include='object')","d2158a4d":"# looping through each columns and label encoding them for both Training and Testing data. \n\nfor each_col in str_cols_train_data:\n    train_data[each_col] = le.fit_transform(train_data[each_col])\n    \nfor each_col in str_cols_test_data:\n    test_data[each_col] = le.fit_transform(test_data[each_col])","d08e90e3":"# Checking the training and testing data now. \nprint(train_data.info())\nprint(test_data.info())","87398831":"# importing seaborn \nimport seaborn as sns","c5c3725f":"# creating heatmap\nplt.figure(figsize=(14,11))\nsns.heatmap(train_data.corr(), annot=True)\nplt.show()","c5128a86":"# importing the library to perform VIF.\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor ","8d9826fa":"# Checking the list of columns once, in form of a list! \nlist(train_data.columns)","de5d2f2b":"# Creating a list of feature columns. \nfeature_cols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title', 'Title_group']","0d41b46b":"# Creating dataset with features. \nX = train_data[feature_cols]","d190da43":"# Checking the shape of features data. \nX.shape","1c45d780":"# creating a new dataframe to store the variance inflation factor outcomes. \nvif = pd.DataFrame()\nvif['Features'] = feature_cols\nvif['vif'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] ","9847d70d":"# Checking the VIF dataset. \n\nvif","7afc2846":"# We will drop the Title column first since it has a very high value. \ntrain_data.drop('Title', axis=1, inplace=True)\ntest_data.drop('Title', axis=1, inplace=True)","ee24076b":"# Rechecking the VIF information\nfeature_cols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title_group']\nX = train_data[feature_cols]\nvif = pd.DataFrame()\nvif['Features'] = feature_cols\nvif['vif'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] \nvif","c60817c2":"# Checking, finally, the data related to training and testing. \nprint(train_data.info())\nprint(test_data.info())","088998eb":"# Checking for outliers in the data. \nfor each_col in X.columns:\n    plt.figure(figsize=(14,12))\n    plt.boxplot(train_data[each_col])\n    plt.xlabel(each_col)\n    plt.show()","fcf79f78":"**Herewith, we will encode all the character data into numeric data. This will help in further modelling and more understanding of the dataset.**","cbb1be1a":"**We still have multicollinearity problem within the data, but we will keep it as it is and try fitting such models during our modelling stage which are not impacted by Multicollinearity problems within the data, like Trees!** ","290bfd1a":"**With the Name column, we can try and extract the title information and create new features which can be useful for analysis and later on for Statistical modelling.**\n\n[References](https:\/\/www.kaggle.com\/bofangli\/dealing-with-missing-values-p2-age-imputation)","07b31f09":"**Initially we will load the rudimentary libraries and load the data which is already provided to us. If need be, we will add and import more data from other data sources which will be mentioned as we go along. The objective of this section is merely to understand the type\/kind of data we have available with us and the bare minimal interpretation.**","78d41938":"## 3. Data Preparation (Encoding) & More EDA","ef342790":"**Now that we have converted or represented all our categorical or String data into Labels, we can try to see the correlations of each features with the target. For this we will need to use the Training data, since Testing data, as obviously, does not contain the Target label.**","73a231b9":"**Findings:**\n\n1. Fare and Title Groups have positive correlations, although not very strong, with the Survived column which is our Target data. \n2. Sex feature on the other hand has negative correlation with the Survived column. \n3. There seems to be multicollinearity problem within the features. For example, Sex and Title Group are highly negatively correlated. \n\n**To Address these issues we can use Variance Inflation factor to remove the variables with multicollinearity problems.**","2d8526d4":"![](https:\/\/i.ibb.co\/qFsL4yP\/1-d-W-Hv-Owhu-Zk-Num-Yxs4w6ww.png)","6fa49415":"**It seems we were able to extract the Title information from the Name of the Passengers successfully!!! This may help in further modelling. If not useful, we will not use the column. We will keep the Title column as of now and drop it later if it causes any issues with Multicollinearity.**","937fa1e1":"**In this section, we will simply prepare the data using simple targets of removing missing values, finding more columns from existing columns, profiling the data (as a start) to find detailed information about all columns, etc.**","65a0de13":"Cabin, Name and Ticket columns are not required for this analysis, since they have very high cardinality and hence they may not result into anything\ninteresting. However, we can check for data once to see if we can derive new features. \n\nHowever, since Cabin column has a lot of missing values, hence there is no point of keeping it for the analysis. \n","5bfd9933":"### 1. Data Loading & Basic Analysis","fe121bd5":"![](https:\/\/www.function1.com\/sites\/default\/files\/wp-content\/uploads\/2013\/04\/Loading-Data.png)","e2852ef7":"**It seems the features Age and Fare have some outliers in them, which are logical too, since these are essentially such features where we can have outliers in the data. We will not treat the data right now, but continue with a series of modelling, before we finally conclude on the best way to make the data more robust.**","f59c4973":"![](https:\/\/www.changequest.co.uk\/wp-content\/uploads\/cq_titanic_blog.png)","04cf4623":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTxMBtILMjt5WI9nc_5szdjxR-CB695P5fEBA&usqp=CAU)","395d0022":"![](https:\/\/www.disruptivestatic.com\/wp-content\/uploads\/2018\/05\/machine-learning-ecommerce-blog-1.jpg)","c4e16030":"## 2. Data Cleaning & Understanding ","ffbf42bb":"# Titanic Data Analysis\n\nThe objective of this data analysis notebook is to merely understand the various patterns, in as much depth as possible in the Titanic disaster which caused such hefty deaths of passengers in its maiden voyage. \n\n**None of the efforts can bring back or recover the dent left for the affected families, nor is it intended to hurt any specific individual or organization. **\n\nPost initial set of data analysis and data engineering, this notebook aims to provide predictive capabilities for which users would have survived or not. ","5bc0a8db":"## 4. Initial Modelling! ","b931028b":"**Shall we perform Feature scaling? It depends! We need to check first if we have outliers in the data or not, and if yes, we should ideally standard scale the data. Before doing so, the Training data should be divided into X and y sets, where X is the features data and y are the labels!**"}}