{"cell_type":{"9d8721ad":"code","237692e7":"code","cde5960b":"code","284a9b30":"code","5a8a85db":"code","0faf35bc":"code","c9e379dd":"code","183a6f2f":"code","c803d465":"code","026fb31f":"code","172dc6d2":"code","7e7d98ac":"code","5bf4659b":"code","1a3c2db2":"code","fafc67c6":"code","b4a5f537":"code","fbf5f3ee":"code","7a35ed52":"code","851d789a":"code","61d425b2":"code","6500612e":"code","bee21e6e":"code","fe75cf28":"code","45c5cad9":"code","cb1bbfb8":"code","1699137f":"code","95549094":"code","2c212ac9":"code","4153a075":"code","40856aa6":"code","4fc28295":"code","fea25241":"code","848f3d75":"code","28b9a946":"code","a933c2b3":"code","ca4faba0":"code","0883021f":"code","1e9eb9f4":"markdown","76b20fa3":"markdown","c896818b":"markdown","6c37198f":"markdown","a633e8fa":"markdown","1c417abb":"markdown","a7c7b8d5":"markdown","3d3e7cf0":"markdown","412075b4":"markdown","c610eaf1":"markdown","aef28d52":"markdown","82eef699":"markdown","d7f5ccd9":"markdown","f06736d1":"markdown","e06c0727":"markdown","33ba10f3":"markdown","870a9afc":"markdown","44b281f4":"markdown","b6b5962f":"markdown","75a1f83a":"markdown","e9fa25c9":"markdown","86e98d42":"markdown","31feade0":"markdown","40f422b2":"markdown","b1a6665f":"markdown","43573b70":"markdown","fe1afc43":"markdown","4f38d1a3":"markdown","a244dd7b":"markdown","e2c92080":"markdown","18accb2a":"markdown","dabb8696":"markdown","9e454599":"markdown","8f5fb8c9":"markdown","882f9fb6":"markdown","5586dcd4":"markdown"},"source":{"9d8721ad":"import pandas as pd\nimport tensorflow as tf\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n# seaborn is of use for visualizing.\nimport seaborn as sns\n\n# load train, test, and submission sample dataset.\ntrain_csv = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest_csv = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\nsubmission = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')","237692e7":"train_csv","cde5960b":"train_csv.isnull().sum()","284a9b30":"train_csv.drop_duplicates()","5a8a85db":"for i in train_csv.drop(['Id','Pawpularity'],axis=1):\n    sns.countplot(train_csv[i])\n    plt.show()","0faf35bc":"sns.distplot(train_csv['Pawpularity'])","c9e379dd":"test_csv","183a6f2f":"test_csv.isnull().sum()","c803d465":"submission","026fb31f":"# Set the path for loading image dataset.\nos.chdir('..\/input\/petfinder-pawpularity-score\/train')\n\n# We can find the size of each image data from this procedure\nsize_data = pd.DataFrame()\nfor file in os.listdir():\n    imgg = cv2.imread(file)\n    w,h,c = imgg.shape\n    size_data=size_data.append([[w,h,c,imgg.size\/3]])\nsize_data","172dc6d2":"size_data[size_data[3] == size_data[3].min()]","7e7d98ac":"size_data[3].value_counts()","5bf4659b":"size_data[size_data[3] == 691200]","1a3c2db2":"train_img = []\nfor i in os.listdir():\n    file = cv2.imread(i)\n    file=cv2.resize(file,(64,64), interpolation=cv2.INTER_AREA)\n    train_img.append(file\/255)\ntrain_img[:5]","fafc67c6":"train_img_name = []\nfor i in os.listdir():\n    train_img_name.append(i)\ntrain_img_name[:5]","b4a5f537":"for name in train_img_name:\n    if name[-4:] != '.jpg':\n        print(name)","fbf5f3ee":"train_csv_data = pd.DataFrame()\nfor img, name in zip(train_img, train_img_name):\n    name=name[:-4]\n    location = train_csv[train_csv['Id'] == name].index[0]\n    train_csv_data= train_csv_data.append([train_csv.loc[location]])\ntrain_csv_data","7a35ed52":"train_csv_data=train_csv_data.reset_index().drop(['index'],axis=1)\ntrain_csv_data","851d789a":"image_1 = cv2.imread('.\/'+train_csv_data['Id'][0]+'.jpg')\nplt.imshow(image_1)","61d425b2":"plt.imshow(train_img[0])","6500612e":"image_2 = cv2.imread('.\/'+train_csv_data['Id'][1]+'.jpg')\nplt.imshow(image_2)","bee21e6e":"plt.imshow(train_img[1])","fe75cf28":"os.chdir('..\/test')\n\nfor i in os.listdir():\n    file = cv2.imread(i)\n    print(file.shape)","45c5cad9":"test_img = []\nfor i in os.listdir():\n    file = cv2.imread(i)\n    file=cv2.resize(file,(64,64), interpolation=cv2.INTER_AREA)\n    test_img.append(file\/255)\ntest_img[:5]","cb1bbfb8":"test_img_name = []\nfor i in os.listdir():\n    test_img_name.append(i)\ntest_img_name[:5]","1699137f":"test_csv_data = pd.DataFrame()\nfor img, name in zip(test_img, test_img_name):\n    name=name[:-4]\n    location = test_csv[test_csv['Id'] == name].index[0]\n    test_csv_data= test_csv_data.append([test_csv.loc[location]])\ntest_csv_data=test_csv_data.reset_index().drop(['index'],axis=1)\ntest_csv_data","95549094":"test_1 = cv2.imread('.\/'+test_csv_data['Id'][0]+'.jpg')\nplt.imshow(test_1)","2c212ac9":"plt.imshow(test_img[0])","4153a075":"train_csv_x = train_csv_data.drop(['Id','Pawpularity'],axis=1)\ntrain_y = train_csv_data['Pawpularity']\n\ntest_csv_x = test_csv_data.drop(['Id'],axis=1)","40856aa6":"# ##################### CSV FILE INPUT & IMG FILE INPUT LAYER ###########################\n# csv_input = tf.keras.Input(shape = train_csv_x.shape[1:], name = 'CSV_Input')        ##\n# img_input = tf.keras.Input(shape = np.array(train_img).shape[1:], name = 'IMG_Input')##\n# #######################################################################################\n#                                         ##\n#                                         ##\n#                                         ##\n# ##################### CSV FILE HIDDEN LAYER STRUCTURE  ######################################\n# csv_hidden1 = tf.keras.layers.Dense(200, activation='relu', name='CSV_Hidden1')(csv_input)   ##\n# csv_hidden2 = tf.keras.layers.Dense(200, activation='relu', name='CSV_Hidden2')(csv_hidden1)##\n# csv_hidden3 = tf.keras.layers.Dense(200, activation='relu', name='CSV_Hidden3')(csv_hidden2)##\n# csv_dropout = tf.keras.layers.Dropout(0.5, name ='CSV_Dropout')(csv_hidden3)               ##\n# csv_hidden4 = tf.keras.layers.Dense(200, activation='relu', name='CSV_Hidden4')(csv_dropout)##\n# csv_hidden5 = tf.keras.layers.Dense(200, activation='relu', name='CSV_Hidden5')(csv_hidden4)##\n# #############################################################################################\n#                                          #\n#                                          #\n#                                          #\n# ##################### IMG FILE CONVOLUTIONAL LAYER STRUCTURE  ######################################\n# img_conv1 = tf.keras.layers.Conv2D(filters = 120, kernel_size = 5, strides= 1, padding = 'same',   ##\n#                                    activation = 'relu', name='IMG_Conv1')(img_input)              ##\n# img_pool1 = tf.keras.layers.MaxPool2D(3, name = 'IMG_Pool1')(img_conv1)                           ##\n# img_conv2 = tf.keras.layers.Conv2D(filters = 120, kernel_size = 4, strides= 1, padding = 'same',  ##\n#                                    activation = 'relu', name='IMG_Conv2')(img_pool1)              ##\n# img_conv3 = tf.keras.layers.Conv2D(filters = 120, kernel_size = 4, strides =1, padding = 'same',  ##\n#                                    activation = 'relu', name='IMG_Conv3')(img_conv2)              ##\n# img_pool2 = tf.keras.layers.MaxPool2D(3, name = 'IMG_Pool2')(img_conv3)                           ##\n# img_conv4 = tf.keras.layers.Conv2D(filters = 120, kernel_size = 3, strides =1, padding = 'same',  ##\n#                                    activation = 'relu', name='IMG_Conv4')(img_pool2)              ##\n# img_pool3 = tf.keras.layers.MaxPool2D(3, name = 'IMG_Pool3')(img_conv4)                           ##\n# img_conv5 = tf.keras.layers.Conv2D(filters = 120, kernel_size = 3, strides =1, padding = 'same',  ##\n#                                    activation = 'relu', name='IMG_Conv5')(img_pool3)              ##\n# img_conv6 = tf.keras.layers.Conv2D(filters = 120, kernel_size = 3, strides =1, padding = 'same',  ##\n#                                    activation = 'relu', name='IMG_Conv6')(img_conv5)              ##\n# img_pool4 = tf.keras.layers.MaxPool2D(2, name = 'IMG_Pool4')(img_conv6)                           ##\n# img_flatten = tf.keras.layers.Flatten(name = 'IMG_Flatten')(img_pool4)                            ##\n# img_dense1 = tf.keras.layers.Dense(300, activation = 'relu', name='IMG_Dense1')(img_flatten)     ##\n# img_dropout1 = tf.keras.layers.Dropout(0.5, name='IMG_Dropout1')(img_dense1)                      ##\n# img_dense2 = tf.keras.layers.Dense(300, activation = 'relu', name='IMG_Dense2')(img_dropout1)    ##\n# img_dropout2 = tf.keras.layers.Dropout(0.5, name='IMG_Dropout2')(img_dense2)                      ##\n# ####################################################################################################\n#                                         ##\n#                                         ##\n#                                         ##\n# ##################### CSV FILE INPUT & IMG FILE OUTPUT LAYER ############\n# csv_output = tf.keras.layers.Dense(1, name = 'CSV_Output')(csv_hidden5)##\n# img_output = tf.keras.layers.Dense(1,name = 'IMG_Output')(img_dropout2)##\n# #########################################################################\n#                                         ##\n#                                         ##\n#                                         ##\n# ############################################# MODEL SETTING  ####################################################\n# model = tf.keras.Model(inputs=[csv_input, img_input], outputs=[csv_output, img_output], name='Pythonash_model')##\n# #################################################################################################################","4fc28295":"csv_input = tf.keras.Input(shape = train_csv_x.shape[1:], name = 'CSV_Input')\nimg_input = tf.keras.Input(shape = np.array(train_img).shape[1:], name = 'IMG_Input')\n\ncsv_hidden1 = tf.keras.layers.Dense(200, activation='elu', kernel_initializer = 'he_normal', name='CSV_Hidden1')(csv_input)\ncsv_hidden2 = tf.keras.layers.Dense(200, activation='elu', kernel_initializer = 'he_normal', name='CSV_Hidden2')(csv_hidden1)\ncsv_hidden3 = tf.keras.layers.Dense(200, activation='elu', kernel_initializer = 'he_normal', name='CSV_Hidden3')(csv_hidden2)\ncsv_hidden4 = tf.keras.layers.Dense(200, activation='elu', kernel_initializer = 'he_normal', name='CSV_Hidden4')(csv_hidden3)\ncsv_hidden5 = tf.keras.layers.Dense(200, activation='elu', kernel_initializer = 'he_normal', name='CSV_Hidden5')(csv_hidden4)\ncsv_hidden6 = tf.keras.layers.Dense(200, activation='elu', kernel_initializer = 'he_normal', name='CSV_Hidden6')(csv_hidden5)\ncsv_dropout = tf.keras.layers.Dropout(0.5, name ='CSV_Dropout')(csv_hidden6)\n\nimg_conv1 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv1')(img_input)\nimg_conv2 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv2')(img_conv1)\nimg_pooling1 = tf.keras.layers.MaxPooling2D(4, name= 'IMG_Max1')(img_conv2)\n\nimg_conv3 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv3')(img_pooling1)\nimg_conv4 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv4')(img_conv3)\nimg_pooling2 = tf.keras.layers.MaxPooling2D(4, name= 'IMG_Max2')(img_conv4)\n\nimg_conv5 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv5')(img_pooling2)\nimg_conv6 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv6')(img_conv5)\nimg_pooling3 = tf.keras.layers.MaxPooling2D(3, name= 'IMG_Max3')(img_conv6)\n\nimg_dropout = tf.keras.layers.Dropout(0.5, name = 'IMG_Dropout')(img_pooling3)\nimg_conv7 = tf.keras.layers.Conv2D(120, 4, padding = 'same', activation = 'elu', kernel_initializer = 'he_normal',name='IMG_Conv7')(img_dropout)\n\nimg_hidden1 = tf.keras.layers.Dense(300, activation='elu', kernel_initializer = 'he_normal',name='IMG_hidden1')(img_conv7)\nimg_dropout1 = tf.keras.layers.Dropout(0.5, name='IMG_Dropout1')(img_hidden1)\n\nimg_hidden2 = tf.keras.layers.Dense(600, activation='elu', kernel_initializer = 'he_normal',name='IMG_hidden2')(img_dropout1)\n\nimg_gpool = tf.keras.layers.GlobalAvgPool2D(name = 'IMG_Gpool')(img_hidden2)\n\nimg_dropout2 = tf.keras.layers.Dropout(0.5, name='IMG_Dropout2')(img_gpool)\n\n\n\ncsv_output = tf.keras.layers.Dense(1, name = 'CSV_Output')(csv_dropout)\nimg_output = tf.keras.layers.Dense(1,name = 'IMG_Output')(img_dropout2)\n\nmodel = tf.keras.Model(inputs=[csv_input, img_input], outputs=[csv_output, img_output], name='Pythonash_model')","fea25241":"model.summary()","848f3d75":"# move you current directory to back.\nos.chdir('..\/')\nos.chdir('..\/')\nos.chdir('..\/')\ntf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, rankdir='TB')","28b9a946":"learning_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\ninitial_learning_rate = 0.002,\ndecay_steps = 10000,\ndecay_rate = 0.99)\n\nopt = tf.keras.optimizers.Adam(learning_rate = learning_schedule)\nmodel.compile(loss=['mse','mse'], loss_weights=[0.5, 0.5], optimizer = opt, metrics = tf.keras.metrics.RootMeanSquaredError())\n\nepoch_number = 20\n\ncheck_1 = tf.keras.callbacks.ModelCheckpoint('pythonash_model.h5', save_best_only=True, verbose=2)\n# check_2 = tf.keras.callbacks.EarlyStopping(patience = epoch_number * 0.1, monitoring ='val_loss', verbose=2,\n#                                           restore_best_weight=True)","a933c2b3":"model.fit( \n    x= [train_csv_x, np.array(train_img)], y = [train_y, train_y], epochs=epoch_number, \n    validation_split=0.2, verbose =2, workers=3, batch_size = 100, validation_batch_size = 100,\n    callbacks = [check_1])","ca4faba0":"best_model = tf.keras.models.load_model('pythonash_model.h5')\ncsv_result, img_result = best_model.predict([test_csv_x, np.array(test_img)])\nfinal_result = pd.DataFrame(0.5 * csv_result + 0.5 * img_result)\nfinal_result.columns =['Pawpularity']\nfinal_result","0883021f":"for ids, paw in zip(test_csv_data['Id'], final_result['Pawpularity']):\n    location = submission[submission['Id'] == ids].index[0]\n    submission['Pawpularity'].loc[location] = paw\nsubmission\nsubmission.to_csv('.\/working\/submission.csv',index=False)\n\n","1e9eb9f4":"It's resized and rescaled data.","76b20fa3":"## Checking the pixel structure of that.\n\n- It is derived from 960 * 720.","c896818b":"## Load train image dataset and rescaling.\n\nThis part is crucial for analysis and is devided into 3 parts.\n\n1. Loading the image dataset.\n\n2. Changing the shape of each image into 64 * 64.\n\n- Why 64 * 64 ?? the reason is that a large image dataset need many memories so, if you change your image pixel as small thing, memory allocation will be more free.\n\n3. Rescaling the pixels by deviding with 255.\n\nIn this procedure, the parameter, cv2.INTER_AREA, is useful for interpolation.\n\nThere are several interpolation methods, but I recommend this.","6c37198f":"# Read me\n\nHello, This is for begginers who want to know how you can handle this dataset which consists of some csv file and image datasaet.\n\nTraditionally, a large dataset needs to be transoformed with TF but, if you don't know how it works, this notebook is of use to you.\n\nThis notebook is just for handling data such as loading the dataset, stacking dataset, and analyzing with deep neural network (especially, CNN).\n\nIt will be helpful for preparing analysis to someone who wants to submit.\n\nIf you have any questions, please leave the comments.\n\n\n## **Knowledge can be improved by being shared.**\n\nPlease upvote!!\n\n\n## [You can learn more skills for handling dataset or neural network.]\n\n### [Advanced parallel combination DNN with CNN] - Pawpularity Contest\n - https:\/\/www.kaggle.com\/pythonash\/improved-dnn-cnn-models-for-beginners\n \n### [Image data handling without memory exploded] - Pawpularity Contest\n - https:\/\/www.kaggle.com\/pythonash\/how-to-handle-dataset-for-beginners\n\n### [Data handling & Deep learning] - Titanic competition (best score!!)\n - https:\/\/www.kaggle.com\/pythonash\/how-to-handle-raw-dataset-and-analyze-with-dl\n \n### [Deep learning model with SeLU activation function] - Titanic competition\n- https:\/\/www.kaggle.com\/pythonash\/selu-activation-function-in-dl\n\n### [Preparing a completed dataset with proper imputation method] - Titanic competition\n - https:\/\/www.kaggle.com\/pythonash\/making-completed-dataset\n\n**Let's start!**","a633e8fa":"## Finally, Let's identify the submission form.","1c417abb":"It has not null-values, either.","a7c7b8d5":"# Handling image dataset.","3d3e7cf0":"## Matching the image dataset order with csv file order.","412075b4":"## For corresponding the image with each id.\n\nEach image data has to correspond to its own id.","c610eaf1":"# Import some libraries for handling dataset.","aef28d52":"It's resized and rescaled data.","82eef699":"## Glancing the test dataset.","d7f5ccd9":"The target variable of train dataset is distributed as below.\n\nIn this part, you have to determine whether you truncate some outliers or not.\n\nHowever, I recommend that you don't truncate because I think those are a part of dataset, too.\n\nWe don't know what the truncated data affects in using deep learning.\n\nIt's just on my experience so, you can select and it's all up to you!","f06736d1":"test dataset resize","e06c0727":"## Glancing the train csv dataset.\n\nAll columns contain binary values except for 'Id' and 'Pawpularity'","33ba10f3":"## model summary\n\nIt shows that your model structure, simply.","870a9afc":"# It has done! \n\nYou are ready to use this dataset as train and test for getting score.\n\nNext step is preparing your submission.","44b281f4":"What is the shape of test dataset??\n\nIt's (128, 128, 3), but we have to transforme this simze into (64, 64, 3) because of memory allocation.\n\nFor analyzing the test dataset, we will match the sizes of both train image and test image, identically.","b6b5962f":"## Set your model\n\nIn this part, please read the codes carefully, we will use two input layers.","75a1f83a":"## Load test image dataset and rescaling","e9fa25c9":"## Count the binary values.\n\nAll data is unbalanced, but you don't have to worry about that.\n\nI will expalin the reason why it is okay in next notebook (coming soon).","86e98d42":"## Compile and fit your model.","31feade0":"## What is the minimum size?","40f422b2":"Furthermore, there is no duplicated data.","b1a6665f":"## plot your model.\n\nThis plot shows your model and you can figure out you model structure, intuitively.","43573b70":"It's original data.","fe1afc43":"Below procedure is identical with train dataset.","4f38d1a3":"It's original data.","a244dd7b":"## Checking the resized and rescaled image with original image data.","e2c92080":"# Analyzing with neural network.\n\n\nWe will use a special deep learning structure.\n\nThese file is devided into csv file and image file.\n\nSo, It is proper approach to use two input layers.\n\nHow can it work??\n\nLet's start!","18accb2a":"Wow, there is no null-value.\n\nSo, we don't have to mind about imputation.","dabb8696":"Reindexing the train csv data.","9e454599":"## What is the number of most things?","8f5fb8c9":"# It's your turn!!\n\nYou have many opportunities that you can change this model parameters and get your submission score.\n\nI recommend that you change the hyper parameters such as learning_rate, batch_size, activation function, the number of neurons, layers, and so on...\n\nIf you get any helps from my notebook, please upvote!!\n\nFingers crossed!!","882f9fb6":"## Checking the file name.\n\nBefore we match the image data with its own id, we have to check the file name whether file name has identical rule or not.\n\nIf a file name has not '.jpg', it will be shown.","5586dcd4":"## Prepare the train input, target and test input."}}