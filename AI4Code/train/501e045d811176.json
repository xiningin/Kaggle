{"cell_type":{"b8ec5628":"code","866b8a3f":"code","4aa15fd1":"code","9f13805c":"code","804fae50":"code","3de72235":"code","ed58c394":"code","0f2c2788":"code","6414cd15":"code","243dea16":"code","14454734":"code","6cc43360":"code","8e2ac61c":"code","e33a100d":"code","ff8905b6":"code","8449b747":"code","ea89e626":"code","f78b5522":"code","b9d07786":"code","307a52bd":"code","fc153cda":"code","4c263d05":"code","d21bedf5":"code","0545c650":"code","9fea180a":"code","36de0cc4":"code","f83207b7":"code","989360ad":"code","39b718d5":"code","277dae17":"code","13f4193e":"code","2796e162":"code","0a6a69ec":"code","15ab7118":"code","92ffb1b1":"code","122fdfaf":"code","66efd437":"code","33bff64e":"code","8499f0e0":"code","8c355e37":"code","727bb0fc":"code","d5e9de6c":"code","8f44bbf3":"code","8405363c":"markdown","7f2664bc":"markdown","106111fa":"markdown","168d3fea":"markdown","347245a2":"markdown","fb1c239c":"markdown","90daaef3":"markdown"},"source":{"b8ec5628":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport scipy.stats.distributions as dist\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA , TruncatedSVD\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom collections import defaultdict , Counter\nimport string\nimport re\nimport os\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding , LSTM , Dense , SpatialDropout1D , Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nimport xgboost as xgb\n\nplt.style.use('ggplot')\nstop = set(stopwords.words('english'))\n","866b8a3f":"os.listdir()\n#Training data\ndataset = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nprint('Training data shape: ', dataset.shape)\ndataset.head()","4aa15fd1":"dataset.isnull().sum()","9f13805c":"dataset['target'].value_counts(normalize = True)","804fae50":"disaster_tweets = dataset[dataset.target == 1]['text']\ndisaster_tweets.values[1]","3de72235":"print('Number of rows in Training {}'.format(dataset.shape[0]))\nprint('Number of rows in Test {}'.format(test.shape[0]))","ed58c394":"# Number of real and not real dataset in training\n\nreal = dataset[dataset.target == 1].shape[0]\nnot_real = dataset[dataset.target == 0].shape[0]\nplt.figure(figsize = (8,5))\nsns.barplot(x = ['Disaster' , 'Not Disaster']  , y = [real , not_real]).set(title = 'Proportion of Disaster and Not Disaster Tweets' , ylabel = 'Frequency')\nplt.grid(alpha = 1)","0f2c2788":"dataset['length'] = dataset['text'].apply(lambda x : len(x))","6414cd15":"dataset.length.head()","243dea16":"plt.figure(figsize = (15,7))\nsns.distplot(dataset[dataset.target == 1]['length'] , bins = 150 , color = 'blue' , kde = False , hist_kws = {'alpha':0.8})\nsns.distplot(dataset[dataset.target == 0]['length'] , bins = 150, color = 'yellow',kde = False , hist_kws = {'alpha':0.4}).set(ylabel = 'Numbers' , xlabel = 'Length', title = 'Distribution of Lengths for both Real and Non real tweets')\nplt.show()","14454734":"plt.figure(figsize = (15,5))\nplt.subplot(121)\nplt.title('Disaster Tweets')\nsns.distplot(dataset[dataset.target == 1]['length'] , kde = False , color= 'blue').set(title = 'Real Tweets')\n\nplt.subplot(122)\nsns.distplot(dataset[dataset.target == 0]['length'] , kde = False , color = 'red').set(title = 'Not Real Tweets')\n\nplt.show()","6cc43360":"plt.figure(figsize = (15,5))\nplt.subplot(121)\nsns.boxplot(dataset[dataset.target == 1]['length']  ,color= 'blue').set(title = 'Real Tweets')\n\nplt.subplot(122)\nsns.boxplot(dataset[dataset.target == 0]['length'] , color = 'red').set(title = 'Not Real Tweets')\n\nplt.show()","8e2ac61c":"mean1 = dataset[dataset.target == 1]['length'].mean()\nmean2 = dataset[dataset.target == 0]['length'].mean()\n\nstd1 = dataset[dataset.target == 1]['length'].std()\nstd2 = dataset[dataset.target == 0]['length'].std()\n\nlen1 = dataset[dataset.target == 1]['length'].shape[0]\nlen2 = dataset[dataset.target == 0]['length'].shape[0]\n((mean1 , std1 , len1) , (mean2 , std2 , len2))","e33a100d":"Margin_of_error = 1.96* std1\/np.sqrt(len1)\n\nlcb = mean1 - Margin_of_error\nucb = mean1 + Margin_of_error\n(lcb , ucb)","ff8905b6":"Margin_of_error = 1.96* std2\/np.sqrt(len2)\n\nlcb = mean2 - Margin_of_error\nucb = mean2 + Margin_of_error\n(lcb , ucb)","8449b747":"estimated_standard_error = np.sqrt(std1**2\/len1 + std2**2\/len2)\ntest_statistic = (mean1 - mean2)\/estimated_standard_error\np_val = 2*dist.norm.cdf(-np.abs(test_statistic))\n(test_statistic , p_val)","ea89e626":"plt.figure(figsize = (12,5))\nplt.subplot(121)\nwords = dataset[dataset.target == 1]['text'].str.split().map(lambda x : len(x))\nsns.distplot(words , kde = False , color = 'blue')\n\nplt.subplot(122)\nwords = dataset[dataset.target == 0]['text'].str.split().map(lambda x : len(x))\nsns.distplot(words , kde = False , color = 'red')","f78b5522":"plt.figure(figsize = (12,5))\nplt.subplot(121)\nwords = dataset[dataset.target == 1]['text'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x : np.mean(x))\nsns.distplot(words , color = 'blue' , hist_kws={'alpha':0.6} , kde = False)\n\nplt.subplot(122)\nwords = dataset[dataset.target == 0]['text'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x : np.mean(x))\nsns.distplot(words , color = 'red' , hist_kws = {'alpha': 0.6} , kde = False)","b9d07786":"def create_corpus(target):\n    corpus = []\n    \n    for x in dataset[dataset.target == target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\ndef create_corpus_df(tweet , target):\n    corpus = []\n    \n    for x in tweet[tweet.target == target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","307a52bd":"corpus = create_corpus(0)\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1\ntop = sorted(dic.items() , key = lambda x:x[1] , reverse=True)[:15]\n\nplt.figure(figsize = (15,5))\nx , y = zip(*top)\nsns.barplot(list(x) , list(y)).set(title = 'Frequency of Top 15 words in non real tweets')","fc153cda":"plt.figure(figsize = (15,5))\ncorpus = create_corpus(1)\n\ndic = defaultdict(int)\n\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1\ntop = sorted(dic.items() , key = lambda x:x[1] , reverse = True)[:15]\n\nx, y = zip(*top)\nsns.barplot(list(x) , list(y)).set(title = 'Frequency of Top 15 words for real tweets')\nplt.show()","4c263d05":"plt.figure(figsize = (12,6))\ncorpus = create_corpus(1)\ndic = defaultdict(int)\npunctuation = string.punctuation\n\nfor word in corpus:\n    if word in punctuation:\n        dic[word] += 1\n        \ntop = sorted(dic.items() , key = lambda x:x[1] , reverse = True)\n\nx,y = zip(*top)\nsns.barplot(list(x) , list(y)).set(title = 'Barplot for punctuation in real tweet')","d21bedf5":"plt.figure(figsize = (12,6))\ncorpus = create_corpus(0)\ndic = defaultdict(int)\npunctuation = string.punctuation\n\nfor word in corpus:\n    if word in punctuation:\n        dic[word] += 1\n        \ntop = sorted(dic.items() , key = lambda x:x[1] , reverse = True)\n\nx,y = zip(*top)\nsns.barplot(list(x) , list(y)).set(title = 'Barplot for punctuation in not real tweet')","0545c650":"def get_top_tweet_bigrams(corpus,n = None):\n    vec = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_of_words = bag_of_words.sum(axis = 0)\n    words_freq = [(word,sum_of_words[0,idx]) for word,idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq , key = lambda x:x[1] , reverse = True)\n    return words_freq[:n]\n","9fea180a":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(dataset['text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","36de0cc4":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ndef combine_text(text):\n    return ' '.join(text)\n\ndef text_preprocessing(text):\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    no_punc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(no_punc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = combine_text(remove_stopwords)\n    \n    return combined_text","f83207b7":"dataset['text'] = dataset['text'].apply(lambda x: text_preprocessing(x))\ntest['text'] = test['text'].apply(lambda x: text_preprocessing(x))","989360ad":"plt.figure(figsize = (12,5))\nsns.barplot(y = dataset[dataset.target == 1].keyword.value_counts()[:20].index, x = dataset[dataset.target == 1].keyword.value_counts()[:20])","39b718d5":"plt.figure(figsize = (12,5))\nsns.barplot(y = dataset[dataset.target == 0].keyword.value_counts()[:20].index, x = dataset[dataset.target == 0].keyword.value_counts()[:20])","277dae17":"disaster_tweets = dataset[dataset.target == 1]['text']\nnot_disaster_tweet = dataset[dataset.target == 0]['text']\nfrom wordcloud import WordCloud\n\nfig , (ax1 , ax2) = plt.subplots(1 , 2 , figsize = [26,8])\nwordcloud1 = WordCloud(background_color = 'black' , width = 600 , height = 400).generate(\" \".join(disaster_tweets))\n\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets' , fontsize = 40)\n\nwordcloud2 = WordCloud(background_color = 'black' , height = 400 , width = 600).generate(\" \".join(not_disaster_tweet))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Not Disaster Tweets' , fontsize = 40)","13f4193e":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(dataset['text'])\ntest_vectors = count_vectorizer.transform(test['text'])","2796e162":"tfidf_vectorizer = TfidfVectorizer(min_df = 2, max_df = 0.5, ngram_range = (1 , 2))\ntrain_tfidf = tfidf_vectorizer.fit_transform(dataset['text'])\ntest_tfidf = tfidf_vectorizer.transform(test['text'])","0a6a69ec":"clf = LogisticRegression(C = 1.0)\nscore = model_selection.cross_val_score(clf , train_vectors , dataset['target'] , cv = 5 , scoring = 'f1')\nscore","15ab7118":"clf.fit(train_vectors , dataset['target'])","92ffb1b1":"clf_tfidf = LogisticRegression(C = 1.0)\nscores = model_selection.cross_val_score(clf_tfidf , train_tfidf , dataset['target'] , cv = 5 , scoring = 'f1')\nscores","122fdfaf":"clf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB , train_vectors , dataset['target'] , cv = 5 , scoring = 'f1')\nscores","66efd437":"clf_NB.fit(train_vectors , dataset['target'])","33bff64e":"clf_NB_tfidf = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_tfidf , train_tfidf , dataset['target'] , cv = 5 , scoring = 'f1')\nscores","8499f0e0":"clf_NB_tfidf.fit(train_tfidf , dataset['target'])","8c355e37":"clf_xgb = xgb.XGBClassifier(max_depth = 7 , n_estimators = 200 , colsample_bytree = 0.8 , subsample = 0.8 , nthread = 10 , learning_rate = 0.1)\nscores = model_selection.cross_val_score(clf_xgb , train_vectors , dataset['target'] , cv = 5 , scoring = 'f1')\nscores","727bb0fc":"clf_xgb_tfidf = xgb.XGBClassifier(max_depth = 7 , n_estimators = 200 , colsample_bytree = 0.8 , subsample = 0.8 , nthread = 10 , learning_rate = 0.1)\nscores = model_selection.cross_val_score(clf_xgb_tfidf , train_tfidf , dataset['target'] , cv = 5 , scoring = 'f1')\nscores","d5e9de6c":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","8f44bbf3":"submission_file_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\ntest_vectors=test_tfidf\nsubmission(submission_file_path,clf_NB_tfidf,test_vectors)","8405363c":"Research Question: What is the average length for real tweet?\n\nTarget Population: Tweets \nParameter of Interest: Real Tweets Length","7f2664bc":"Research Question: Is there a significant difference between the length of text of real and not real tweet.\n\nHypotheses: \n\n$H_0$: mu1 - mu2 = 0\n\n$H_1$: mu1 - mu2 != 0\n\n1:real, 0:Not Real\n\nalpha = 0.05, significance level\n\n\nAs we can see our p_val is less than 0.05, that means we have enough evidence to reject the NULL hypotheses and go with Alternative which states that there is clearly difference between the length of real and non real tweet","106111fa":"The distribution of length of tweet is skewed right, centered around 125 with most lengths between 90 to 140, a range of rough 120, and some outliers are present below 15 ","168d3fea":"##### Analyze Punctuation\n\nLet's first do it for real tweet","347245a2":"###### From the given sample of data, with 95% confidence we estimate that the length of text for non real tweet is between 94.63 to 96.77","fb1c239c":"#### Analyze not real tweets","90daaef3":"###### From the given sample of data, with 95% confidence we estimate that the length of text for real tweet is between 107.10 to 109.11"}}