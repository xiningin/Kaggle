{"cell_type":{"675d4262":"code","d48591ba":"code","6c67d3ae":"code","231e40d7":"code","e643d0ca":"code","eeb7361d":"code","6342fff5":"code","cfddf39d":"code","255233d1":"code","4fb10521":"code","c7dccc11":"code","1ad447f8":"code","33a79867":"code","40f4de5c":"code","965cdc2d":"code","b4bc7bb4":"code","79e067cc":"code","44e4f1cf":"code","7c7b814b":"code","febb014f":"markdown","cdacbc00":"markdown"},"source":{"675d4262":"\n\nfrom IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\n\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}\n\nimport os      \npd.options.display.max_columns = 1000\npd.options.display.max_rows = 1000\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d48591ba":"train=pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv\")","6c67d3ae":"print(\"train shape is:\",train.shape)\nprint(\"test shape is:\",test.shape)","231e40d7":"train.head()","e643d0ca":"test.head()","eeb7361d":"# train.isnull().sum()","6342fff5":"train['target'].unique()","cfddf39d":"train.info()","255233d1":"train.describe()","4fb10521":"from numpy.random import seed\nseed(42)\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\nfrom sklearn.metrics import accuracy_score\ndef root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)\/ y_true )))\n    \nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n    mode='min')","c7dccc11":"colNames=[col for col in test.columns if col not in 'id']\n# colNames.remove('id')\ntrain.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\nqt_train = []\ntrain_nn=train[colNames].copy()\ntest_nn=test[colNames].copy()\n# for col in colNames:\n#     #print(col)\n#     qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n#     train_nn[col] = qt.fit_transform(train_nn[[col]])\n#     test_nn[col] = qt.transform(test_nn[[col]])    \n#     qt_train.append(qt)\n","1ad447f8":"train_nn['target']=train['target']","33a79867":"train_nn","40f4de5c":"#https:\/\/bignerdranch.com\/blog\/implementing-swish-activation-function-in-keras\/\nfrom keras.backend import sigmoid\nfrom sklearn.metrics import roc_auc_score\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nget_custom_objects().update({'swish': Activation(swish)})\n\nhidden_units = (100,64,32)#initialized number of neurons in each hidden layer\n\ndef base_model():\n\n    num_input = keras.Input(shape=(100,), name='num_data')#input layer\n\n\n    out = keras.layers.Concatenate()([num_input])\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n        \n\n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted target value probability\n    out = keras.layers.Dense(1, activation='sigmoid', name='prediction')(out)\n    \n    model = keras.Model(\n    inputs = [num_input],\n    outputs = out,\n    )\n    \n    return model","965cdc2d":"target_name='target'\nscores_folds = {}\nmodel_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 5\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=12)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train_nn)\n\nfeatures_to_consider.remove('target')\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\n\ntrain_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\ntest_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n\ntrain_nn[pred_name] = 0\ntest_nn[target_name] = 0\ntest_predictions_nn = np.zeros(test_nn.shape[0])\n\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(train)):\n    print(f'Training fold {fold + 1}')\n    X_train, X_test = train_nn.iloc[trn_ind][features_to_consider], train_nn.iloc[val_ind][features_to_consider]\n    y_train, y_test = train_nn.iloc[trn_ind]['target'], train_nn.iloc[val_ind]['target']\n    print('CV {}\/{}'.format(counter, n_folds)) \n    #############################################################################################\n    # NN\n    #############################################################################################\n    \n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.0007),\n        loss='binary_crossentropy',\n        metrics = ['AUC']\n    )\n    \n    num_data = X_train[features_to_consider]\n    \n    scaler = MinMaxScaler(feature_range=(0, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n      \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n\n    model.fit([num_data], \n              target,              \n              batch_size=2048,\n              epochs=700,\n              validation_data=([num_data_test], y_test),\n              callbacks=[es, plateau],\n              validation_batch_size=len(y_test),\n              shuffle=True,\n             verbose = 1)\n\n    preds = model.predict([num_data_test]).reshape(1,-1)[0]\n    score = round(roc_auc_score(y_test,preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    \n    tt =scaler.transform(test_nn[features_to_consider].values)\n    \n    test_predictions_nn += model.predict([tt]).reshape(1,-1)[0].clip(0,1e10)\/n_folds       \n    counter += 1","b4bc7bb4":"sub=pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\nsub","79e067cc":"sub['target']=test_predictions_nn","44e4f1cf":"sub","7c7b814b":"sub.to_csv(\"submission.csv\",index=False)","febb014f":"# OBSERVATIONS\n1. **No null value is present in the dataset**\n2. **data has 102 columns\/features including target**\n3. **100 features aree float 2 are integers**\n4. **The given problem is of binary classification.**\n\n","cdacbc00":"# **NN BASELINE**\nIn this notebook i have implemented a baseline model of Neural Network (do let me know if you find any doubts or bugs)"}}