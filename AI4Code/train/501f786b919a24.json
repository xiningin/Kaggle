{"cell_type":{"828a2b33":"code","95985a10":"code","f841eece":"code","3f4ac505":"code","d9b21e7a":"code","270c5a27":"code","075c9165":"code","fbb740fb":"code","1a8ec121":"code","15c91001":"code","2351a548":"code","b75f83ce":"code","3dd48f06":"code","ef816d54":"code","2b147b7e":"code","bb7f7b44":"code","0c1082b5":"code","66d9f9fa":"code","880a43a3":"code","a83dfec9":"code","d56746bb":"code","6142828f":"code","943b5838":"code","7e07620a":"code","73dc7286":"code","a452d187":"markdown","1a93eedd":"markdown","e8b8dbaa":"markdown","d87280d5":"markdown","57f270be":"markdown","d4b4a22b":"markdown","a615942b":"markdown","19216919":"markdown","387b02af":"markdown","e80afe09":"markdown","9ae0d585":"markdown","b8295c02":"markdown","c009c7f7":"markdown","6dfdea60":"markdown","4c5310a5":"markdown","88930007":"markdown","a618c9e9":"markdown","57a4df6f":"markdown","3f16a2a7":"markdown","59928ec4":"markdown","0aeadc9c":"markdown"},"source":{"828a2b33":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd \nimport os\nimport numpy as np\nfrom IPython.display import Image\nimport sys\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.neighbors import KNeighborsClassifier \n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import roc_auc_score\n\n","95985a10":"file_train = '..\/input\/train.csv'\nfile_valid = '..\/input\/valid.csv'\nfile_test = '..\/input\/test.csv'\nfile_exemplo = '..\/input\/exemplo_resultado.csv'\n\n\ndata_train = pd.read_csv(file_train)\ndata_valid = pd.read_csv(file_valid)\ndata_test = pd.read_csv(file_test)\n\n\ndata_exemplo = pd.read_csv(file_exemplo)\nprint(len(data_train), len(data_valid))#, len(data_test))","f841eece":"Y = data_train['default payment next month']\nID_predict = data_exemplo['ID']\n\nID_predict_valid = data_valid['ID']\nID_predict_test = data_test['ID']\nID_predict_final = ID_predict_valid.append(ID_predict_test)\n\ndata_train = data_train.drop('default payment next month', axis=1)\n\n\n# Os dados s\u00e3o juntados para um melhor tratamento\ndata_all = data_train.append(data_valid).append(data_test)\n#data_all = data_all.drop('MARRIAGE', axis=1)\n\nlen(data_all)","3f4ac505":"\n#Overview dos dados\n# N\u00famero de linhas e colunas\nprint(data_all.shape, \"\\n\")\n\n# Tipo de dados de cada coluna\nprint(data_all.dtypes, \"\\n\")\n\n#Descobrir a quantidade de valores nulos\nprint(data_all.isna().sum(), \"\\n\")","d9b21e7a":"data_all","270c5a27":"plt.figure(figsize=(15,15))\nax = plt.axes()\ncorr = data_all.drop(['ID'], axis=1).corr()\nsns.heatmap(corr, vmax=1,vmin=-1, square=True, annot=True, cmap='Spectral',linecolor=\"white\", linewidths=0.01, ax=ax)\nax.set_title('Gr\u00e1fico de par de coeficiente de correla\u00e7\u00e3o',fontweight=\"bold\", size=30)\nplt.show()","075c9165":"data_all['PAY_0'] = data_all['PAY_0'].replace(-2, 0)\ndata_all['PAY_0'] = data_all['PAY_0'].replace(4, 0)\n\ndata_all['PAY_2'] = data_all['PAY_2'].replace(-2, 0)\ndata_all['PAY_2'] = data_all['PAY_2'].replace(4, 0)\n\ndata_all['PAY_3'] = data_all['PAY_3'].replace(-2, 0)\ndata_all['PAY_3'] = data_all['PAY_3'].replace(4, 0)\n\ndata_all['PAY_4'] = data_all['PAY_4'].replace(-2, 0)\ndata_all['PAY_4'] = data_all['PAY_4'].replace(4, 0)\n\ndata_all['PAY_5'] = data_all['PAY_5'].replace(-2, 0)\ndata_all['PAY_5'] = data_all['PAY_5'].replace(4, 0)\n\ndata_all['PAY_6'] = data_all['PAY_6'].replace(-2, 0)\ndata_all['PAY_6'] = data_all['PAY_6'].replace(4, 0)","fbb740fb":"data_all","1a8ec121":"from sklearn.model_selection import train_test_split\n\n# Create X\ndata_train = data_all[:(len(data_train))]\ndata_valid = data_all[(len(data_train)):((len(data_train))+(len(data_valid)))]\ndata_test = data_all[((len(data_train))+(len(data_valid))):]\n\nprint()\n\nX = data_train\n#X = data_test\n\nprint(len(data_train), len(data_valid))#, len(data_test))\n","15c91001":"from sklearn import tree\n\nmodel_decision_tree = tree.DecisionTreeClassifier()\n","2351a548":"\"\"\"\nparam_grid = { \n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8,9,10],\n    'criterion' :['entropy','gini'],\n    'max_leaf_nodes' :[10,100]\n}\n\nCV_mdt = GridSearchCV(estimator=model_decision_tree, param_grid=param_grid, cv= 5)\nCV_mdt.fit(X, Y)\nCV_mdt.best_params_\n\"\"\"","b75f83ce":"model_decision_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10,  max_leaf_nodes=100,max_features = 'auto')\nmodel_decision_tree.fit(X, Y)\npred_model_decision_tree= model_decision_tree.predict(X)\n\nprint(\"ROC AUC score:\",roc_auc_score(Y, pred_model_decision_tree))\nprint(\"Acur\u00e1cia do Decision Tree Classifier: \", accuracy_score(Y,pred_model_decision_tree) )","3dd48f06":"rfc = RandomForestClassifier(random_state=42)","ef816d54":"\"\"\"\nparam_grid = { \n    'n_estimators': [10,20,30,40,50,500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['entropy']\n}\n\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(X, Y)\nCV_rfc.best_params_\n\"\"\"","2b147b7e":"model_RFC = RandomForestClassifier(criterion='entropy', max_depth=8,  max_features=8, n_estimators = 30)\nmodel_RFC.fit(X, Y)\npred_model_RFC=model_RFC.predict(X)\n\nprint(\"ROC AUC score:\",roc_auc_score(Y, pred_model_RFC))\n\nprint(\"Accuracy for Random Forest on CV data: \",accuracy_score(Y,pred_model_RFC))\n# 0.8319047619047619","bb7f7b44":"import pandas as pd\nfeature_importances = pd.DataFrame(model_RFC.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance']).sort_values('importance', ascending=False)\n\nfeature_importances","0c1082b5":"from sklearn.neighbors import KNeighborsClassifier \n\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X, Y)","66d9f9fa":"\"\"\"\nparam_grid = { \n    'n_neighbors': [5,6,7],\n    'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'leaf_size' : [10,30]\n}\n\nCV_knn = GridSearchCV(estimator=knn, param_grid=param_grid, cv= 5)\nCV_knn.fit(X, Y)\nCV_knn.best_params_\n\n# {'algorithm': 'auto', 'leaf_size': 10, 'n_neighbors': 6}\n\"\"\"","880a43a3":"\n# Ver a acur\u00e1cia\n# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntrain_auc_roc = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X, Y)\n    pred_knn = knn.predict(X)\n    #Compute accuracy on the training set\n    #train_accuracy[i] = knn.score(X, Y)\n    train_auc_roc[i] = roc_auc_score(Y, pred_knn)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, train_auc_roc, label = 'ROC AUC score')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('ROC AUC')\nplt.show()\n","a83dfec9":"from sklearn.neighbors import KNeighborsClassifier \n\nknn_p = KNeighborsClassifier(n_neighbors=3, algorithm='auto', leaf_size = 10)\nknn_p.fit(X, Y)\nknn_model_predict=knn_p.predict(X)\n\nprint(\"ROC AUC score:\",roc_auc_score(Y, knn_model_predict))\nprint(\"Acur\u00e1cia do KNN: \",accuracy_score(Y,knn_model_predict))\n","d56746bb":"from sklearn.neighbors import KNeighborsClassifier \n\nknn_p_2 = KNeighborsClassifier(n_neighbors=6, algorithm='auto', leaf_size = 10)\nknn_p_2.fit(X, Y)\nknn_model_predict_2=knn_p_2.predict(X)\n\nprint(\"Acur\u00e1cia do KNN: \",accuracy_score(Y,knn_model_predict_2))","6142828f":"data_predict = data_valid.append(data_test)\n\nY_predict = model_RFC.predict(data_predict)\n\nY_predict","943b5838":"print(len(Y_predict))","7e07620a":"Default_exemplo = data_exemplo['Default']\n\n#Y_final = np.concatenate((Y_predict, Default_exemplo[len(data_valid):]), axis=0)\nID_predict_final = ID_predict_valid.append(ID_predict_test)\n\n\nprint(len(Y_predict), len(ID_predict_final))","73dc7286":"\ndata_to_submit = pd.DataFrame({\n    'ID': ID_predict_final,\n    'Default':Y_predict\n})\n\ndata_to_submit.to_csv('csv_to_submit.csv', index = False)","a452d187":"Na seguinte c\u00e9lula s\u00e3o verificadas as import\u00e2ncias de cada feature","1a93eedd":"# Processamento","e8b8dbaa":"## Test using best model with data valid","d87280d5":"### DecisionTree Model\n\nUma estrutura em \u00e1rvore \u00e9 constru\u00edda para dividir o conjunto de dados em subconjuntos menores, eventualmente resultando em uma previs\u00e3o. Existem n\u00f3s de decis\u00f5es que particionam os n\u00f3s de dados e de folha que fornecem a previs\u00e3o que pode ser seguida pela simples l\u00f3gica IF..AND..AND ... .THEN de deslocamento dos n\u00f3s.\n\nO n\u00f3 raiz (o primeiro n\u00f3 de decis\u00e3o) particiona os dados com base no particionamento de recurso mais influente. Existem 2 medidas para isso, Gini Impurity and Entropy.","57f270be":"# Import dataset's","d4b4a22b":"A seguir ser\u00e3o testadas diferentes combina\u00e7\u00f5es de par\u00e2metros para verificar qual ter\u00e1 o melhor resultado ","a615942b":"Testando os resultados das duas diferentes combina\u00e7\u00f5es obtidas","19216919":"Inicialmente os valores inconsistentes ser\u00e3o substituidos pela vari\u00e1vel 0. Este valor foi escolhido por n\u00e3o representar nenhuma categoria de interesse do dataset.\n\n**X6 - X11: Hist\u00f3rico de Pagamentos -> X6 = Setembro \u00e0 X11 = Abril (PAY_...)**\n* -1: Pagou devidamente\n* 1: Pagou com atraso de um m\u00eas\n* 2: Pagou com atraso de dois meses\n* 3: Pagou com atraso de tr\u00eas meses\n","387b02af":"## Testando diferentes modelos","e80afe09":"\nPara analisar melhor como os dados se relacionam, foi utilizado o gr\u00e1fico par de coeficiente de correla\u00e7\u00e3o.\n\nEm estat\u00edstica descritiva, o coeficiente de correla\u00e7\u00e3o de Pearson, tamb\u00e9m chamado de \"coeficiente de correla\u00e7\u00e3o produto-momento\" ou simplesmente de \"\u03c1 de Pearson\" mede o grau da correla\u00e7\u00e3o (e a direc\u00e7\u00e3o dessa correla\u00e7\u00e3o - se positiva ou negativa) entre duas vari\u00e1veis de escala m\u00e9trica (intervalar ou de r\u00e1cio\/raz\u00e3o).\n\nEste coeficiente, normalmente representado por \u03c1 assume apenas valores entre -1 e 1.\n\n* \u03c1 = 1 Significa uma correla\u00e7\u00e3o perfeita positiva entre as duas vari\u00e1veis.\n* \u03c1 = -1 Significa uma correla\u00e7\u00e3o negativa perfeita entre as duas vari\u00e1veis - Isto \u00e9, se uma aumenta, a outra sempre diminui.\n* \u03c1 = 0 Significa que as duas vari\u00e1veis n\u00e3o dependem linearmente uma da outra. No entanto, pode existir uma depend\u00eancia n\u00e3o linear. Assim, o resultado deve ser investigado por outros meios.","9ae0d585":"A seguir ser\u00e3o testadas diferentes combina\u00e7\u00f5es de par\u00e2metros para verificar qual ter\u00e1 o melhor resultado ","b8295c02":"### KNN - KNeighborsClassifier Model\n\nk-Nearest-Neighbours (k-NN) \u00e9 um modelo supervisionado de aprendizado de m\u00e1quina. A aprendizagem supervisionada \u00e9 quando um modelo aprende com dados j\u00e1 rotulados. Um modelo de aprendizado supervisionado recebe um conjunto de objetos de entrada e valores de sa\u00edda. Em seguida, o modelo treina esses dados para aprender a mapear as entradas para a sa\u00edda desejada para aprender a fazer previs\u00f5es sobre dados n\u00e3o vistos.\n\nOs modelos k-NN funcionam tomando um ponto de dados e observando os \"k\" pontos de dados rotulados mais pr\u00f3ximos. O ponto de dados \u00e9 ent\u00e3o atribu\u00eddo ao r\u00f3tulo da maioria dos pontos mais pr\u00f3ximos do \"k\".","c009c7f7":"# Post processing","6dfdea60":"# Processamento dos dados","4c5310a5":"A vari\u00e1vel de interesse ser\u00e1 alocada em vari\u00e1veis, para poder ocorrer um tratamento dos dados e posteriormente o treinamento do modelo","88930007":"# Import libs","a618c9e9":"Inicialmente vamos importar todos os dataset's","57a4df6f":"## Generating output file.csv","3f16a2a7":"A seguir ser\u00e3o testadas diferentes combina\u00e7\u00f5es de par\u00e2metros para verificar qual ter\u00e1 o melhor resultado ","59928ec4":"### Modelo Random Forest Classifier\n\nFunciona em quatro etapas:\n\n1. Seleciona amostras aleat\u00f3rias de um determinado conjunto de dados.\n2. Constroi uma \u00e1rvore de decis\u00e3o para cada amostra e obt\u00e9m um resultado de previs\u00e3o de cada \u00e1rvore de decis\u00e3o.\n3. Faz um voto para cada resultado previsto.\n4. Selecione o resultado da previs\u00e3o com o maior n\u00famero de votos como previs\u00e3o final.","0aeadc9c":"# An\u00e1lise dos Dados\n\nA seguir os dados s\u00e3o verificados para an\u00e1lise dos tipos e como os mesmos se relacionam\n"}}