{"cell_type":{"0029c578":"code","3ab2beba":"code","2b4c1bab":"code","652101a6":"code","d7874553":"code","f5badb0c":"code","40a36044":"code","37592583":"code","71b053e0":"code","25987987":"code","49c0f5cd":"code","3f6ab15a":"code","ee0e2b73":"code","845ad9a0":"code","f97c4dfd":"code","d07ce8b8":"code","529f3191":"code","964e272f":"code","b45f13e5":"markdown","c4391245":"markdown","9103478e":"markdown","de4f6217":"markdown","6e3462d4":"markdown","e246c2b2":"markdown","1b23e447":"markdown","3b2c9824":"markdown","c4c2fc3f":"markdown","a29a990c":"markdown","317c06bf":"markdown","85212c2d":"markdown","570c3b99":"markdown","d9a02676":"markdown","83cfbb1b":"markdown","b9afd814":"markdown"},"source":{"0029c578":"#toxic question classification final exam","3ab2beba":"import numpy as np\nimport pandas as pd\n\n# natural language tool kit\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\n\n#sklearn things\nfrom sklearn.model_selection import train_test_split as df_split\nfrom sklearn.naive_bayes import MultinomialNB","2b4c1bab":"#get data\npath_train = '..\/input\/quora-insincere-questions-classification\/train.csv'\npath_test = '..\/input\/quora-insincere-questions-classification\/test.csv'\n\nraw_df_train = pd.read_csv(path_train)\nraw_df_train.head(10)","652101a6":"raw_df_train.groupby('target').describe()","d7874553":"#download stop word\nnltk.download('stopwords')","f5badb0c":"#declare a function for pre-processing sentence before feed into the model\ndef preprocess_text(text):\n    #remove punctuation\n    nopunc = [char for char in text if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    #remove stopwords\n    clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    #return a list of clean text words\n    return clean_words","40a36044":"from sklearn.feature_extraction.text import CountVectorizer\ntemp_text = 'I do not know but this is so damn bad, why it is so hard. wish i could quit cause i am so tired'\nprint('check 1: '+str(preprocess_text(temp_text)))\ntemp = CountVectorizer(analyzer = preprocess_text).fit_transform([temp_text])\nprint('check 2: '+str(temp))","37592583":"#separate question by it's label\ngroup_non_toxic = raw_df_train[raw_df_train['target']==0]\ngroup_toxic = raw_df_train[raw_df_train['target']==1]\n\n#down sampling non-toxic question\ndownsampled_non_toxic = group_non_toxic.sample(group_toxic.shape[0]*10)\nprint('raw group toxic size: '+str(group_toxic.shape[0]))\nprint('raw downsampled_non_toxic size: '+str(downsampled_non_toxic.shape[0]))","71b053e0":"#mix toxic and non-toxic quest to a new dataset\ntrain_df = pd.concat([group_toxic, downsampled_non_toxic])\nprint(train_df.shape)","25987987":"CV = CountVectorizer(analyzer = preprocess_text)","49c0f5cd":"message_ = CV.fit_transform(train_df['question_text'])","3f6ab15a":"x_train, x_test, y_train, y_test  = df_split(message_, train_df['target'],\n                                             test_size=0.05, random_state=4, stratify=train_df['target'])","ee0e2b73":"#model classifier\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB().fit(x_train,y_train)","845ad9a0":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\npred = classifier.predict(x_test)\nprint(classification_report(y_test, pred))\nprint('Confusion Matrix: \\n', confusion_matrix(y_test,pred))\nprint('\\nAccuracy: ', accuracy_score(y_test, pred))","f97c4dfd":"text = 'What is the currency in Langkawi?'\nprint(text)\ntext = CV.transform([text]).toarray()\nprint(text)\n# print(text)\ntest_ = classifier.predict(text)\nprint(test_)","d07ce8b8":"raw_df_test = pd.read_csv(path_test)\nraw_df_test.head(10)\ntest_df = CV.transform(raw_df_test['question_text'])\npred_test = classifier.predict(test_df)\nprint(pred_test)","529f3191":"sumbission = pd.read_csv('..\/input\/quora-insincere-questions-classification\/sample_submission.csv')\nsumbission['prediction'] = pred_test","964e272f":"sumbission.head(20)\nsumbission.to_csv(\"submission.csv\", encoding='utf-8', index=False)\n# sumbission.to_csv(r\"..\/input\/quora-insincere-questions-classification\/submission.csv\", encoding='utf-8', index=False)","b45f13e5":"**Ph\u00e2n t\u00edch d\u1eef li\u1ec7u**\nT\u1ec7p d\u1eef li\u1ec7u c\u1ee7a ch\u00fang ta bao g\u1ed3m:\n1. qid(Question ID): D\u1ea1ng chu\u1ed7i k\u00fd t\u1ef1 \u0111\u00e1nh d\u1ea5u ph\u00e2n bi\u1ec7t c\u00e1c c\u00e2u h\u1ecfi v\u1edbi nhau v\u00e0 \u0111\u1ed9c nh\u1ea5t. \n2. Question_text: l\u00e0 n\u1ed9i dung c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi c\u1ee7a Quora s\u1ebd \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 ph\u00e2n t\u00edch v\u00e0 hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh. \u0110\u00e2y l\u00e0 s\u1ebd tr\u1edf th\u00e0nh c\u00e1c feature c\u1ee7a m\u00f4 h\u00ecnh. \n3. Target: Ph\u00e2n bi\u1ec7t gi\u1eefa nh\u1eefng c\u00e2u h\u1ecfi toxic v\u00e0 c\u00e1c c\u00e2u h\u1ecfi b\u00ecnh th\u01b0\u1eddng. \u0110\u00e2y l\u00e0 label c\u1ee7a m\u00f4 h\u00ecnh","c4391245":"Ch\u1ea1y d\u1ef1 \u0111o\u00e1n v\u1edbi d\u1eef li\u1ec7u t\u1eeb file test c\u1ee7a cu\u1ed9c thi.","9103478e":"Sau khi downsampling l\u01b0\u1ee3ng c\u00e2u h\u1ecfi non-toxic, ta g\u1ed9p 2 t\u1eadp c\u00e2u h\u1ecfi l\u1ea1i th\u00e0nh 1 t\u1eadp l\u1edbn v\u00e0 tr\u1ed9n \u0111\u1ec1u th\u00e0nh m\u1ed9t t\u1eadp dataset l\u1edbn v\u00e0 s\u1eed d\u1ee5ng trong qu\u00e1 tr\u00ecnh training model","de4f6217":"T\u1eadn d\u1ee5ng file h\u01b0\u1edbng d\u1eabn sumbit, ta xu\u1ea5t ra c\u00e1c d\u1ef1 \u0111o\u00e1n c\u1ee7a m\u00f4 h\u00ecnh v\u00e0o file \"submission.csv\"","6e3462d4":"Ta c\u00f3 th\u1ec3 nh\u1eadn th\u1ea5y: \n- S\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi toxic: 80810\n- S\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi b\u00ecnh th\u01b0\u1eddng: 1225312\nS\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi toxic \u0111ang b\u00e9 h\u01a1n l\u01b0\u1ee3ng c\u00e2u h\u1ecfi b\u00ecnh th\u01b0\u1eddng. Gi\u1ea3 s\u1eed trong tr\u01b0\u1eddng h\u1ee3p n\u1ebfu ta s\u1eed d\u1ee5ng to\u00e0n b\u1ed9 dataset \u0111\u1ec3 hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh s\u1ebd g\u00e2y ra t\u00ecnh tr\u1ea1ng overfit v\u1edbi c\u00e1c c\u00e2u h\u1ecfi b\u00ecnh th\u01b0\u1eddng. Nh\u01b0 th\u1ebf ta s\u1ebd sampling t\u1eadp c\u00e2u h\u1ecfi kh\u00f4ng toxic sau \u0111\u00e2y \u0111\u1ec3 \u0111\u1ea3m b\u1ea3o \u0111i\u1ec1u n\u00e0y kh\u00f4ng x\u1ea3y ra.","e246c2b2":"**Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u:**\nH\u00e0m ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u \u0111\u1ec3 \u0111\u01b0a m\u1ed9t c\u00e2u v\u1ec1 th\u00e0nh d\u1ea1ng c\u00e1c feature:\n1. M\u1ed9t c\u00e2u \u0111\u01b0\u1ee3c \u0111\u01b0a v\u00e0o h\u00e0m s\u1ebd b\u1ecb t\u00e1ch r\u1eddi r\u1ea1c c\u00e1c t\u1eeb trong c\u00e2u ra.\n2. T\u1eadp c\u00e1c t\u1eeb r\u1eddi r\u1ea1c s\u1ebd b\u1ecb l\u01b0\u1ee3c b\u1ecf \u0111i c\u00e1c t\u1eeb stop word.\n3. Sau \u0111\u00f3 tr\u1ea3 ra m\u1ed9t list c\u00e1c t\u1eeb mang \u00fd ngh\u0129a ch\u00ednh c\u1ee7a c\u00e2u, ch\u00ednh l\u00e0 c\u00e1c feature.","1b23e447":"V\u00ed d\u1ee5: C\u00e2u \"I do not know but this is so damn bad, why it is so hard. wish i could quit cause i am so tired\"\n\u0110\u01b0\u1ee3c \u0111\u01b0a v\u00e0o h\u00e0m ti\u1ec1n x\u1eed l\u00fd s\u1ebd \u0111\u01b0\u1ee3c t\u00e1ch r\u1eddi v\u00e0 l\u01b0\u1ee3c b\u1ecf \u0111i c\u00e1c t\u1eeb stopword. \nK\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1 l\u00e0 m\u1ed9t chu\u1ed7i nh\u01b0 \u1edf check_1\n\nSau \u0111\u00f3 ta s\u1ebd s\u1eed d\u1ee5ng ph\u01b0\u01a1ng ph\u00e1p One Hot Coding v\u1edbi c\u00e1c t\u1eeb tr\u00ean. \u1edf \u0111\u00e2y ch\u1ec9 l\u00e0 th\u1eed nghi\u1ec7m n\u00ean c\u00f3 th\u1ec3 th\u1ea5y c\u00e1c h\u1ec7 s\u1ed1 bi\u1ec3u di\u1ec5n \u0111\u1ec1u \u0111\u01b0\u1ee3c set l\u00ean 1.\nTuy nhi\u00ean trong th\u1ef1c t\u1ebf sau khi hu\u1ea5n luy\u1ec7n th\u00ec c\u00e1c feature \u0111\u01b0\u1ee3c x\u1eafp x\u1ebfp tr\u1edf th\u00e0nh m\u1ed9t chu\u1ed7i t\u1eeb \u0111i\u1ec3n m\u00e0 v\u1edbi m\u1ed7i m\u1ed9t c\u00e2u \u0111\u01b0a v\u00e0o th\u00ec th\u1ef1c ch\u1ea5t ta \u0111\u01b0a v\u00e0o m\u00f4 h\u00ecnh m\u1ed9t \"chu\u1ed7i t\u1eeb \u0111i\u1ec3n\" m\u00e0 c\u00e1c feature trong c\u00e2u \u0111\u01b0\u1ee3c set l\u00ean 1 t\u1ea1i v\u1ecb tr\u00ed t\u01b0\u01a1ng \u1ee9ng.","3b2c9824":"C\u00e1c stop word l\u00e0 c\u00e1c t\u1eeb kh\u00f4ng mang qu\u00e1 nhi\u1ec1u \u00fd ngh\u0129a cho n\u1ed9i dung ch\u00ednh c\u1ee7a c\u00e2u v\u00e0 c\u00f3 th\u1ec3 g\u00e2y confuse cho m\u00f4 h\u00ecnh, v\u00ec th\u1ebf ta s\u1ebd l\u01b0\u1ee3c b\u1ecf\nv\u00ed d\u1ee5 c\u1ee7a stopwords: a, an , the, ...","c4c2fc3f":"Cho b\u1ed9 m\u00e3 h\u00f3a x\u1eed k\u00fd t\u1eadp c\u00e1c t\u1eeb c\u00f3 trong b\u1ed9 dataset s\u1eed d\u1ee5ng \u0111\u1ec3 hu\u1ea5n luy\u1ec7n.","a29a990c":"Ki\u1ec3m th\u1eed m\u00f4 h\u00ecnh v\u1edbi m\u1ed9t c\u00e2u \u0111\u01b0\u1ee3c tr\u00edch t\u1eeb file test c\u1ee7a cu\u1ed9c thi.","317c06bf":"Ki\u1ebfm tra qua m\u00f4 h\u00ecnh v\u1edbi t\u1eadp d\u1eef li\u1ec7u test, ta c\u00f3 th\u1ec3 th\u1ea5y \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a m\u00f4 h\u00ecnh l\u00e0 0.8619","85212c2d":"Kh\u1edfi t\u1ea1o m\u1ed9t b\u1ed9 m\u00e3 h\u00f3a One-Hot-Coding l\u1ea5y t\u1eeb th\u01b0 vi\u1ec7n c\u1ee7a Sklearn","570c3b99":"Kh\u1edfi t\u1ea1o m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n s\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n sklearn.\nSau \u0111\u00f3 ta hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh v\u1edbi t\u1eadp d\u1eef li\u1ec7u train.","d9a02676":"**M\u00f4 t\u1ea3 b\u00e0i to\u00e1n:**\nTrong th\u1eddi \u0111\u1ea1i ph\u00e1t tri\u1ec3n c\u1ee7a c\u00f4ng ngh\u1ec7 s\u1ed1, c\u00f4ng ngh\u1ec7 c\u00f9ng v\u1edbi m\u1ea1ng internet \u0111\u00e3 k\u1ebft n\u1ed1i con ng\u01b0\u1eddi t\u1eeb m\u1ecdi l\u1ee5c \u0111\u1ecba tr\u00ean kh\u1eafp tr\u00e1i \u0111\u1ea5t l\u1ea1i v\u1edbi nhau ch\u1ec9 trong m\u1ed9t t\u00edch t\u0103c. C\u00e1c di\u1ec5n \u0111\u00e0n c\u00f9ng v\u1edbi \u0111\u00f3 l\u00e0 c\u00e1c n\u1ec1n t\u1ea3ng m\u1ea1ng x\u00e3 h\u1ed9i ng\u00e0y m\u1ed9t tr\u1edf n\u00ean quan tr\u1ecdng h\u01a1n. Tuy nhi\u00ean ch\u00fang nh\u01b0 nh\u1eefng n\u01a1i c\u00f4ng c\u1ed9ng kh\u00e1c, kh\u00f4ng g\u00ec c\u00f3 th\u1ec3 \u0111\u1ea3m b\u1ea3o s\u1ef1 v\u0103n minh c\u1ee7a nh\u1eefng ng\u01b0\u1eddi tham gia \u0111\u01b0\u1ee3c c\u1ea3. Ch\u00ednh v\u00ec th\u1ebf, \u0111\u1ec3 g\u00f3p ph\u1ea7n \u0111\u1ea3m b\u1ea3o m\u1ed9t m\u00f4i tr\u01b0\u1eddng v\u0103n minh v\u00e0 \u0111\u1eb9p \u0111\u1ebd, Quora \u0111\u00e3 \u00e1p d\u1ee5ng ph\u01b0\u01a1ng ph\u00e1p d\u00f9ng h\u1ecdc m\u00e1y \u0111\u1ec3 h\u1ed7 tr\u1ee3 nh\u1eadn di\u1ec7n nh\u1eefng c\u00e2u h\u1ecfi toxic trong c\u00e1c topic!\n\nNh\u1eefng c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u1eb7t ra tr\u00ean di\u1ec5n \u0111\u00e0n n\u00e0y c\u00f3 \u0111\u1ee7 c\u00e1c th\u1ec3 lo\u1ea1i. \u0110a ph\u1ea7n c\u00e1c c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c y\u00eau c\u1ea7u \u0111\u1ec3 x\u1eed l\u00fd trong \u0111\u1ec1 t\u00e0i \u0111\u1ec1u l\u00e0 nh\u1eefng c\u00e2u h\u1ecfi ti\u1ebfng anh v\u00e0 ho\u1eb7c l\u00e0 ch\u00fang toxic ho\u1eb7c ch\u1ec9 l\u00e0 nh\u1eefng c\u00e2u h\u1ecfi b\u00ecnh th\u01b0\u1eddng. Vi\u1ec7c c\u1ee7a m\u00f4 h\u00ecnh b\u00e0i to\u00e1n \u0111\u00f3 l\u00e0 ph\u00e2n lo\u1ea1i xem c\u00e2u h\u1ecfi n\u00e0o l\u00e0 c\u00e2u h\u1ecfi toxic v\u00e0 c\u00e2u h\u1ecfi n\u00e0o kh\u00f4ng! \n- Input: M\u1ed9t c\u00e2u h\u1ecfi\n- Output: \u0110\u00e1nh gi\u00e1 toxic ho\u1eb7c kh\u00f4ng toxic","83cfbb1b":"Chia t\u1eadp dataset ra cho hai m\u1ee5c \u0111\u00edch ri\u00eang r\u1ebd l\u00e0 hu\u1ea5n luy\u1ec7n v\u00e0 ki\u1ec3m th\u1eed. T\u1ec9 l\u1ec7 l\u01b0\u1ee3ng d\u1eef li\u1ec7u test tr\u00ean to\u00e0n t\u1eadp d\u1eef li\u1ec7u l\u00e0 0.05\nC\u00f3 th\u00eam \u0111i\u1ec1u ki\u1ec7n v\u1ec1 t\u1ec9 l\u1ec7 c\u00e2n b\u1eb1ng c\u00e2u h\u1ecfi toxic v\u00e0 non-toxic \u0111\u1ec3 c\u00f3 th\u1ec3 cho ra m\u1ed9t k\u1ebft qu\u1ea3 tr\u1ef1c quan nh\u1ea5t.","b9afd814":"**Mi\u00eau t\u1ea3 m\u00f4 h\u00ecnh:**\nm\u00f4 h\u00ecnh \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u00e0 Multinomial Naive Bayes\nThu\u1eadt to\u00e1n Multinomial Naive Bayes l\u00e0 ph\u01b0\u01a1ng ph\u00e1p h\u1ecdc m\u00e1y d\u1ef1a tr\u00ean x\u00e1c su\u1ea5t th\u01b0\u1eddng s\u1eed d\u1ee5ng trong x\u1eed l\u00fd ng\u00f4n ng\u1eef - NLP (Natural Language Processing). Thu\u1eadt to\u00e1n d\u1ef1a tr\u00ean nguy\u00ean l\u00fd Bayes v\u00e0 d\u1ef1 \u0111o\u00e1n label c\u1ee7a m\u1ed9t t\u1eadp h\u1ee3p t\u1eeb. N\u00f3 t\u00ednh x\u00e1c su\u1ea5t c\u1ee7a m\u1ed7i label t\u1eeb nh\u1eefng feature v\u00e0 sau \u0111\u00f3 \u0111\u01b0a ra c\u00e1i label c\u00f3 t\u1ec9 l\u1ec7 cao nh\u1ea5t.  \n\nNguy\u00ean l\u00fd Bayes, \u0111\u01b0\u1ee3c ph\u00e1t tri\u1ec3n b\u1edfi Thomas Bayes, \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 t\u00ednh to\u00e1n t\u1ec9 l\u1ec7 x\u1ea3y ra c\u1ee7a m\u1ed9t s\u1ef1 ki\u1ec7n d\u1ef1a tr\u00ean nh\u1eefng \u0111i\u1ec1u ki\u1ec7n c\u1ee7a nh\u1eefng s\u1ef1 ki\u1ec7n \u0111\u00e3 bi\u1ebft x\u1ea3y ra tr\u01b0\u1edbc \u0111\u00f3. \u0110\u1ec3 bi\u1ec3u di\u1ec5n nguy\u00ean l\u00fd d\u01b0\u1edbi d\u1ea1ng to\u00e1n h\u1ecdc ta c\u00f3 c\u00f4ng th\u1ee9c:\n\n**P(A|B) = P(A) * P(B|A)\/P(B)**\nTrong \u0111\u00f3: \n- P(A), P(B): l\u00e0 x\u00e1c su\u1ea5t c\u1ee7a A v\u00e0 B\n- P(B|A): l\u00e0 x\u00e1c su\u1ea5t c\u1ee7a A khi \u0111\u00e3 bi\u1ebft t\u1ec9 l\u1ec7 x\u00e1c su\u1ea5t c\u1ee7a B\n\nM\u00f4 h\u00ecnh n\u00e0y c\u00f3 nh\u1eefng \u01b0u nh\u01b0\u1ee3c \u0111i\u1ec3m sau:\n- \u01afu \u0111i\u1ec3m:\n1. C\u00f3 th\u1ec3 d\u1ec5 d\u00e0ng s\u1eed d\u1ee5ng n\u1ebfu nh\u01b0 ch\u1ec9 c\u1ea7n t\u00ednh x\u00e1c su\u1ea5t\n2. C\u00f3 th\u1ec3 s\u1eed d\u1ee5ng tr\u00ean c\u1ea3 nh\u1eefng d\u1eef li\u1ec7u li\u00ean t\u1ee5c hay r\u1eddi r\u1ea1c.\n3. Thu\u1eadt to\u00e1n \u0111\u01a1n gi\u1ea3n v\u00e0 c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c d\u00f9ng \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n trong nh\u1eefng \u1ee9ng d\u1ee5ng th\u01a1\u00ec gian th\u1eadt.\n4. Linh ho\u1ea1t v\u00e0 d\u1ec5 d\u00e0ng l\u00e0m vi\u1ec7c v\u1edbi nh\u1eefng \u1ee9ng d\u1ee5ng th\u1eddi gian th\u1ef1c.\n\n- Nh\u01b0\u1ee3c \u0111i\u1ec3m:\n1. \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a thu\u1eadt to\u00e1n th\u1ea5p h\u01a1n nh\u1eefng thu\u1eadt to\u00e1n kh\u00e1c.\n2. Thu\u1eadt to\u00e1n kh\u00f4ng ph\u00f9 h\u1ee3p v\u1edbi nh\u1eefng b\u00e0i to\u00e1n h\u1ed3i quy.Thu\u1eadt to\u00e1n Naive Bayes ch\u1ec9 s\u1eed d\u1ee5ng cho vi\u1ec7c ph\u00e2n lo\u1ea1i d\u1eef li\u1ec7u d\u1ea1ng x\u00e2u k\u00fd t\u1ef1 v\u00e0 kh\u00f4ng th\u1ec3 d\u1ef1 \u0111o\u00e1n c\u00e1c d\u1eef li\u1ec7u s\u1ed1"}}