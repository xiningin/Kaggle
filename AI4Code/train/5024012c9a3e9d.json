{"cell_type":{"d9754add":"code","c4da752c":"code","0fc31c17":"code","4e626c92":"code","3ec0c264":"code","e31513d2":"code","03e89863":"code","9065fa08":"code","0e734114":"code","34c365a8":"code","b583b398":"code","af54d756":"code","3479592d":"code","cae50fce":"code","059a3247":"code","90777826":"markdown","adc3d3d6":"markdown","679e4fb9":"markdown"},"source":{"d9754add":"# import the necessary packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","c4da752c":"# generate variable\ntrue_slope = 10\ntrue_intercept = 4\ninput_var = np.arange(0.0,100.0)\noutput_var = true_slope * input_var + true_intercept + 500.0 * np.random.rand(len(input_var))\n\n# visualization\nplt.figure(figsize=(10,5))\nplt.scatter(input_var, output_var)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n\nprint(\"input_var: \", input_var[:20])\nprint(\"output_var: \", output_var[:20])","0fc31c17":"def compute_cost(input_var, output_var, params):\n  # Compute linear regression cost\n  num_samples = len(input_var)\n  cost_sum = 0.0\n  for x,y in zip(input_var, output_var):\n    y_hat = np.dot(params, np.array([1.0, x]))\n    cost_sum += (y_hat - y)**2\n  cost = cost_sum \/ (num_samples * 2.0)\n  return cost","4e626c92":"def lin_reg_batch_gradient_descent(input_var, output_var, params, alpha, max_iter):\n  # Compute the params for linear regression using batch gradient descent\n  iteration = 0\n  num_samples = len(input_var)\n  cost = np.zeros(max_iter)\n  params_store = np.zeros([2, max_iter])\n\n  while iteration < max_iter:\n    cost[iteration] = compute_cost(input_var, output_var, params)\n    params_store[:, iteration] = params\n    \n    for x,y in zip(input_var, output_var):\n      y_hat = np.dot(params, np.array([1.0, x]))\n      gradient = np.array([1.0, x]) * (y - y_hat)\n      params += alpha * gradient\/num_samples\n\n    iteration += 1\n  return params, cost, params_store","3ec0c264":"# model training\nx_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20, random_state=42)\nparams_0 = np.array([20.0, 80.0])\nalpha_batch = 1e-3\nmax_iter = 500\n\nparams_hat_batch, cost_batch, params_store_batch =\\\nlin_reg_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)\n\nprint(\"params_hat_batch: \", params_hat_batch)\n\nplt.figure(figsize=(10,7))\nplt.scatter(x_test, y_test)\nplt.plot(x_test, params_hat_batch[0] + params_hat_batch[1]*x_test, 'r', label='batch')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()","e31513d2":"def lin_reg_stoch_gradient_descent(input_var, output_var, params, alpha):\n    # Compute the params for linear regression using stochastic gradient descent\n    num_samples = len(input_var)\n    cost = np.zeros(num_samples)\n    params_store = np.zeros([2, num_samples])\n    i = 0\n\n    for x,y in zip(input_var, output_var):\n        cost[i] = compute_cost(input_var, output_var, params)\n        params_store[:, i] = params\n        \n        y_hat = np.dot(params, np.array([1.0, x]))\n        gradient = np.array([1.0, x]) * (y - y_hat)\n        params += alpha * gradient\/num_samples\n        \n        i += 1\n    return params, cost, params_store","03e89863":"alpha = 1e-3\nparams_0 = np.array([20.0, 80.0])\nparams_hat, cost, params_store =\\\nlin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)\n\nprint(\"params_hat: \", params_hat)\n\nplt.figure(figsize=(10,7))\nplt.scatter(x_test, y_test)\nplt.plot(x_test, params_hat[0] + params_hat[1]*x_test, 'g', label='batch')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()","9065fa08":"plt.figure(figsize=(10,7))\nplt.scatter(x_test, y_test)\nplt.plot(x_test, params_hat_batch[0] + params_hat_batch[1]*x_test, 'g', label='batch')\nplt.plot(x_test, params_hat[0] + params_hat[1]*x_test, '-r', label='stochastic')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\nprint(f'batch      T0, T1: {params_hat_batch[0]}, {params_hat_batch[1]}')\nprint(f'stochastic T0, T1: {params_hat[0]}, {params_hat[1]}')\nrms_batch = np.sqrt(np.mean(np.square(params_hat_batch[0] + params_hat_batch[1]*x_test - y_test)))\nrms_stochastic = np.sqrt(np.mean(np.square(params_hat[0] + params_hat[1]*x_test - y_test)))\nprint(f'batch rms:      {rms_batch}')\nprint(f'stochastic rms: {rms_stochastic}')","0e734114":"plt.figure(figsize=(10,7))\nplt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\nplt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\nplt.xlabel('iteration')\nplt.ylabel('normalized cost')\nplt.legend()\nplt.show()\nprint(f'min cost with BGD: {np.min(cost_batch)}')\nprint(f'min cost with SGD: {np.min(cost)}')","34c365a8":"from sklearn import linear_model\n\nn_samples, n_features = 10, 5\nrng = np.random.RandomState(0)\ny = y_train\nX = (x_train - x_train.max())\/(x_train.max() - x_train.min()) # minimize\nSGDReg =linear_model.SGDRegressor(\n   max_iter = 1000,penalty = \"elasticnet\",loss = 'huber',tol = 1e-3, average = True\n)\nSGDReg.fit(X.reshape(-1,1), y)","b583b398":"SGDReg.coef_","af54d756":"SGDReg.intercept_","3479592d":"SGDReg.t_","cae50fce":"print(\"Equation: y = {} + {}*x\".format(SGDReg.intercept_[0],SGDReg.coef_[0]))","059a3247":"# examples\nprint(\"predict(2) = \", SGDReg.predict([[2]]))\nprint(\"predict(4) = \", SGDReg.predict([[4]]))\nprint(\"predict(10) = \", SGDReg.predict([[10]]))\nprint(\"predict(29) = \", SGDReg.predict([[29]]))\nprint(\"predict(59) = \", SGDReg.predict([[59]]))\nprint(\"predict(198) = \", SGDReg.predict([[198]]))","90777826":"# Gradient Descent\n\nThere is a parameter value with a minimum cost. Goal; this is to reach its minimum value.\nIn the gradient descent algorithm, we start with the random pattern parameters and calculate the error for each learning repetition, keep updating the model parameters to approximate the values \u200b\u200bresulting in the minimum cost.\n\nIn the above equation, we update the model parameters after each iteration. The second term of the equation calculates the slope or gradient of the curve with each iteration.\nThere is an important point here. If the alpha value is too large, we may miss the minimum point. If the alpha value is too large, this process may take a long time since the number of samples to be observed will increase. Therefore, the alpha value should be determined well.\n\nThere are three ways to land a gradient:\n\n### Batch gradient descent\n- Uses all training examples to update model parameters with each iteration.\n\n### Mini-batch Gradient Descent\n- Instead of using all examples, mini group 'b' is used.\n\n### Stochastic Gradient Descent (SGD)\n- Updates parameters using only one training instance per iteration.\n- The training sample is usually chosen at random.\n- Stochastic gradient descent is often preferred to optimize cost functions when there are hundreds of thousands or more training instances as it will merge faster than batch gradient descent.","adc3d3d6":"<img src=\"https:\/\/i.ibb.co\/zGbsnjH\/cost-function.png\" \/>","679e4fb9":"## Easy Way"}}