{"cell_type":{"a8c3fecb":"code","5f142108":"code","22306b7a":"code","38f7ff6a":"code","34f89fd0":"code","9ce7b525":"code","423f19ee":"code","2499597f":"code","54c7585d":"code","0659e6d7":"code","f1ee3c90":"code","24e60171":"code","b3d40fd2":"code","dc727cab":"code","8272662f":"code","c3c153c3":"code","00bfa949":"code","401f78d5":"code","9cfb3494":"code","670ff0cb":"code","ede0b519":"code","82da23a6":"code","344b56a0":"code","292f0132":"code","7dabf6f0":"code","b4215bb0":"code","ab81d70e":"code","3c1e15fd":"code","7c68bb79":"code","ceea0889":"code","70914d4f":"code","7cc7214a":"code","aa4bf205":"code","7b581052":"code","d422fd3a":"code","62766f8a":"code","a797f951":"markdown","17d892be":"markdown","881b83c4":"markdown","0dbbcd55":"markdown","f60e0c50":"markdown","0166bdaf":"markdown"},"source":{"a8c3fecb":"!pip install pylops","5f142108":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nimport skimage\nfrom skimage.feature import hessian_matrix, hessian_matrix_eigvals\nfrom scipy.ndimage.filters import convolve\nfrom skimage import data, io, filters\nimport pylops\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN, LSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, \\\nConvolution2D, Reshape, GaussianNoise, ReLU, Conv2DTranspose\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.models import load_model\nfrom keras.regularizers import l1,l2,L1L2\n#SKLEARN CLASSIFIER\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","22306b7a":"Defocussed_Data = Path(\"..\/input\/blur-dataset\/defocused_blurred\")\nMotion_Blurred_Data = Path(\"..\/input\/blur-dataset\/motion_blurred\")","38f7ff6a":"Defocussed_List_jpg = list(Defocussed_Data.glob(r\"*.jpg\"))\nDefocussed_List_JPG = list(Defocussed_Data.glob(r\"*.JPG\"))\nDefocussed_List_jpeg = list(Defocussed_Data.glob(r\"*.jpeg\"))","34f89fd0":"Motion_Blurred_jpg = list(Motion_Blurred_Data.glob(r\"*.jpg\"))\nMotion_Blurred_JPG = list(Motion_Blurred_Data.glob(r\"*.JPG\"))\nMotion_Blurred_jpeg = list(Motion_Blurred_Data.glob(r\"*.jpeg\"))","9ce7b525":"Blurred_List = []\n\nfor Djpg_Blurred_IMG in Defocussed_List_jpg:\n    Blurred_List.append(Djpg_Blurred_IMG)\n    \nfor DJPGBlurred_IMG in Defocussed_List_JPG:\n    Blurred_List.append(DJPGBlurred_IMG)\n    \nfor DjpegBlurred_IMG in Defocussed_List_jpeg:\n    Blurred_List.append(DjpegBlurred_IMG)\n    \nfor MjpgBlurred_IMG in Motion_Blurred_jpg:\n    Blurred_List.append(MjpgBlurred_IMG)\n    \nfor MJPGBlurred_IMG in Motion_Blurred_JPG:\n    Blurred_List.append(MJPGBlurred_IMG)\n    \nfor MjpegBlurred_IMG in Motion_Blurred_jpeg:\n    Blurred_List.append(MjpegBlurred_IMG)","423f19ee":"Blurred_Series = pd.Series(Blurred_List,name=\"BLURRED\").astype(str)","2499597f":"plt.style.use(\"classic\")","54c7585d":"Example_IMG = Blurred_Series[4]\nReading_IMG = cv2.imread(Example_IMG)\nReading_IMG = cv2.cvtColor(Reading_IMG,cv2.COLOR_BGR2RGB)\nplt.xlabel(Reading_IMG.shape)\nplt.ylabel(Reading_IMG.size)\nplt.imshow(Reading_IMG)","0659e6d7":"Example_IMG = Blurred_Series[157]\nReading_IMG = cv2.imread(Example_IMG)\nReading_IMG = cv2.cvtColor(Reading_IMG,cv2.COLOR_BGR2RGB)\nplt.xlabel(Reading_IMG.shape)\nplt.ylabel(Reading_IMG.size)\nplt.imshow(Reading_IMG)","f1ee3c90":"Example_IMG = Blurred_Series[57]\nReading_IMG = cv2.imread(Example_IMG)\nReading_IMG = cv2.cvtColor(Reading_IMG,cv2.COLOR_BGR2RGB)\nplt.xlabel(Reading_IMG.shape)\nplt.ylabel(Reading_IMG.size)\nplt.imshow(Reading_IMG)","24e60171":"sharpen_kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n\nExample_IMG = Blurred_Series[57]\nReading_IMG = cv2.imread(Example_IMG)\nReading_IMG = cv2.cvtColor(Reading_IMG,cv2.COLOR_BGR2RGB)\nSharpen_IMG = cv2.filter2D(Reading_IMG, -1, sharpen_kernel)\nplt.xlabel(Sharpen_IMG.shape)\nplt.ylabel(Sharpen_IMG.size)\nplt.imshow(Sharpen_IMG)","b3d40fd2":"Example_IMG = Blurred_Series[57]\nReading_IMG = cv2.imread(Example_IMG,0)\n\nShapeZ,ShapeX = Reading_IMG.shape\nnh = [15, 25]\nhz = np.exp(-0.1*np.linspace(-(nh[0]\/\/2), nh[0]\/\/2, nh[0])**2)\nhx = np.exp(-0.03*np.linspace(-(nh[1]\/\/2), nh[1]\/\/2, nh[1])**2)\nhz \/= np.trapz(hz) \nhx \/= np.trapz(hx) \nh = hz[:, np.newaxis] * hx[np.newaxis, :]\n\n\nCo_IMG = pylops.signalprocessing.Convolve2D(ShapeZ * ShapeX,h=h,\n                                         offset=(nh[0] \/\/ 2,\n                                                 nh[1] \/\/ 2),\n                                         dims=(ShapeZ, ShapeX), dtype='float32')","dc727cab":"fig, ax = plt.subplots(1, 1, figsize=(5, 3))\nhim = ax.imshow(h)\nax.set_title('Blurring operator')\nfig.colorbar(him, ax=ax)\nax.axis('tight')","8272662f":"Wop = pylops.signalprocessing.DWT2D((ShapeZ, ShapeX), wavelet='haar', level=3)\nDop = [pylops.FirstDerivative(ShapeZ * ShapeX, dims=(ShapeZ, ShapeX), dir=0, edge=False),\n       pylops.FirstDerivative(ShapeZ * ShapeX, dims=(ShapeZ, ShapeX), dir=1, edge=False)]\nDWop = Dop + [Wop, ]","c3c153c3":"imdeblurfista = \\\n    pylops.optimization.sparsity.FISTA(Co_IMG * Wop.H, Reading_IMG, eps=1e-1,\n                                       niter=100)[0]\nimdeblurfista = Wop.H * imdeblurfista","00bfa949":"\n\nimdeblurfista = \\\n    pylops.optimization.sparsity.FISTA(Co_IMG * Wop.H, Reading_IMG, eps=1e-1,\n                                       niter=100)[0]\nimdeblurfista = Wop.H * imdeblurfista\n\nimdeblurtv = \\\n    pylops.optimization.sparsity.SplitBregman(Co_IMG, Dop, Reading_IMG.flatten(),\n                                              niter_outer=10, niter_inner=5,\n                                              mu=1.5, epsRL1s=[2e0, 2e0],\n                                              tol=1e-4, tau=1., show=False,\n                                              ** dict(iter_lim=5, damp=1e-4))[0]\n\nimdeblurtv1 = \\\n    pylops.optimization.sparsity.SplitBregman(Co_IMG, DWop,\n                                              Reading_IMG.flatten(),\n                                              niter_outer=10, niter_inner=5,\n                                              mu=1.5, epsRL1s=[1e0, 1e0, 1e0],\n                                              tol=1e-4, tau=1., show=False,\n                                              ** dict(iter_lim=5, damp=1e-4))[0]\n\n# Reshape images\n#imblur = imblur.reshape((ShapeZ, ShapeX))\n#imdeblur = imdeblur.reshape((ShapeZ, ShapeX))\nimdeblurfista = imdeblurfista.reshape((ShapeZ, ShapeX))\nimdeblurtv = imdeblurtv.reshape((ShapeZ, ShapeX))\nimdeblurtv1 = imdeblurtv1.reshape((ShapeZ, ShapeX))","401f78d5":"Splitting_Train = Blurred_Series[:200]","9cfb3494":"print(Splitting_Train)","670ff0cb":"Splitting_Test = Blurred_Series[200:400]","ede0b519":"print(Splitting_Test)","82da23a6":"Transformation_Train = []\n\nfor Blurred_IMG in Splitting_Train:\n    \n    IMG_G = cv2.imread(Blurred_IMG)\n    IMG_G = cv2.cvtColor(IMG_G,cv2.COLOR_BGR2RGB)\n    IMG_G = cv2.resize(IMG_G,(400,400))\n    IMG_G = IMG_G \/ 255.\n    Transformation_Train.append(IMG_G)","344b56a0":"Transformation_Test = []\n\nfor Blurred_IMG in Splitting_Test:\n    \n    IMG_G = cv2.imread(Blurred_IMG)\n    IMG_G = cv2.cvtColor(IMG_G,cv2.COLOR_BGR2RGB)\n    IMG_G = cv2.resize(IMG_G,(400,400))\n    IMG_G = IMG_G \/ 255.\n    Transformation_Test.append(IMG_G)","292f0132":"X_Train = np.asarray(Transformation_Train)\nX_Test = np.asarray(Transformation_Test)","7dabf6f0":"print(X_Train.shape)","b4215bb0":"print(X_Test.shape)","ab81d70e":"Encoder_G = Sequential()\nEncoder_G.add(Conv2D(32, (5,5)))\nEncoder_G.add(ReLU())\nEncoder_G.add(Conv2D(64, (5,5)))\nEncoder_G.add(ReLU())\nEncoder_G.add(Conv2D(128, (5,5)))\nEncoder_G.add(ReLU())","3c1e15fd":"Decoder_G = Sequential()\nDecoder_G.add(Conv2DTranspose(64,(5,5)))\nDecoder_G.add(ReLU())\nDecoder_G.add(Conv2DTranspose(32,(5,5)))\nDecoder_G.add(ReLU())\nDecoder_G.add(Conv2DTranspose(3,(5,5)))\nDecoder_G.add(ReLU())\n","7c68bb79":"Auto_Encoder = Sequential([Encoder_G,Decoder_G])","ceea0889":"Auto_Encoder.compile(loss=\"MSE\",optimizer=\"adam\",metrics=[\"accuracy\"])","70914d4f":"AE_Model = Auto_Encoder.fit(X_Train,X_Train,epochs=10,batch_size=32)","7cc7214a":"Auto_Encoder.save(\"AE_Model.h5\")\nAuto_Encoder.save_weights(\"AE_Weights.h5\")","aa4bf205":"Prediction_IMG = Auto_Encoder.predict(X_Test[:100])","7b581052":"prediction_img_number = 2\nprint(\"NORMAL\")\nplt.imshow(X_Test[prediction_img_number])\nplt.show()\nprint(\"AUTO-ENCODER OUTPUT\")\nplt.imshow(Prediction_IMG[prediction_img_number])","d422fd3a":"prediction_img_number = 25\nprint(\"NORMAL\")\nplt.imshow(X_Test[prediction_img_number])\nplt.show()\nprint(\"AUTO-ENCODER OUTPUT\")\nplt.imshow(Prediction_IMG[prediction_img_number])","62766f8a":"prediction_img_number = 82\nprint(\"NORMAL\")\nplt.imshow(X_Test[prediction_img_number])\nplt.show()\nprint(\"AUTO-ENCODER OUTPUT\")\nplt.imshow(Prediction_IMG[prediction_img_number])","a797f951":"# VISUALIZATION","17d892be":"# PACKAGES AND LIBRARIES","881b83c4":"# WAY I - FILTER 2D","0dbbcd55":"# PATH & TRANSFORMATION","f60e0c50":"# WAY III - AUTOENCODER","0166bdaf":"# WAY II - PYLOPS"}}