{"cell_type":{"ceb1b692":"code","2bb4c2d4":"code","d98d53b7":"code","9f91cb73":"code","a497aaca":"code","3cddce69":"code","5d441ebd":"code","68dfa84b":"markdown","d5b27eb0":"markdown","ebf53b1d":"markdown","ddd05689":"markdown","e555ec3f":"markdown","3d25ad39":"markdown","8e81cec9":"markdown"},"source":{"ceb1b692":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport gc\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import *\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom pickle import load\nimport matplotlib.pyplot as plt\nimport json\n!cp ..\/input\/ventilator-feature-engineering\/VFE.py .","2bb4c2d4":"train = np.load('..\/input\/ventilator-feature-engineering\/x_train.npy')\ntargets = np.load('..\/input\/ventilator-feature-engineering\/y_train.npy')","d98d53b7":"# model creation\ndef create_lstm_model():\n\n    x0 = tf.keras.layers.Input(shape=(train.shape[-2], train.shape[-1]))  \n\n    lstm_layers = 4 # number of LSTM layers\n    lstm_units = [940, 540, 462, 316]\n    lstm = Bidirectional(keras.layers.LSTM(lstm_units[0], return_sequences=True))(x0)\n    for i in range(lstm_layers-1):\n        lstm = Bidirectional(keras.layers.LSTM(lstm_units[i+1], return_sequences=True))(lstm)    \n    lstm = Dropout(0.002)(lstm)\n    lstm = Dense(lstm_units[-1], activation='swish')(lstm)\n    lstm = Dense(1)(lstm)\n\n    model = keras.Model(inputs=x0, outputs=lstm)\n    model.compile(optimizer=\"adam\", loss=\"mae\")\n    \n    return model","9f91cb73":"with open('..\/input\/train-ventilator-lstm-model-part-i\/train_params.json', 'r') as fp:\n    config = json.load(fp)\n\nBATCH_SIZE = config['BATCH_SIZE']\nNFOLDS = config['NFODLS']\nSEED = config['SEED']\nEPOCHS = config['EPOCHS']","a497aaca":"# Function to get hardware strategy\ndef get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        tf.config.optimizer.set_jit(True)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    return tpu, strategy\n\ntpu, strategy = get_hardware_strategy()","3cddce69":"hist = []\nfolds = [3] # folds to train\n\nwith strategy.scope():\n    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        if fold in folds:\n            print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n            folds.append(fold)\n            X_train, X_valid = train[train_idx], train[test_idx]\n            y_train, y_valid = targets[train_idx], targets[test_idx]\n            \n            model = create_lstm_model()\n            model.compile(optimizer=\"adam\", loss=\"mae\")\n            \n            #checkpoint_filepath = f\"lstm_fold_{fold}.hdf5\"\n            checkpoint_filepath = '\/kaggle\/working\/lstm_fold{}.hdf5'.format(fold)\n\n            scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)\/BATCH_SIZE), 1e-5)\n            #lr = LearningRateScheduler(scheduler, verbose=0)\n            lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n            es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n            sv = keras.callbacks.ModelCheckpoint(\n                checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n                save_weights_only=False, mode='auto', save_freq='epoch',\n                options=None\n            )\n            hist.append(model.fit(X_train, y_train, \n                                  validation_data=(X_valid, y_valid), \n                                  epochs=EPOCHS, batch_size=BATCH_SIZE, \n                                  callbacks=[lr, es, sv]))\n        \n            del X_train, X_valid, y_train, y_valid, model\n            gc.collect()","5d441ebd":"colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\nplt.figure(figsize=(16,16))\nfor i in range(len(hist)):\n    plt.plot(hist[i].history['loss'], linestyle='-', color=colors[i], label='Train, fold #{}'.format(str(folds[i])))\nfor i in range(len(hist)):\n    plt.plot(hist[i].history['val_loss'], linestyle='--', color=colors[i], label='Validation, fold #{}'.format(str(folds[i])))\nplt.ylim(top=1)\nplt.title('Model Loss')\nplt.ylabel('MAE')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(which='major', axis='both')\nplt.show();","68dfa84b":"HW strategy:","d5b27eb0":"# Training\nFirst define a few parameters that will also be used in other notebooks:","ebf53b1d":"![logo](https:\/\/cdn.freelogovectors.net\/wp-content\/uploads\/2018\/07\/tensorflow-logo.png)","ddd05689":"Let's take a look at the learning curves.","e555ec3f":"# Training - LSTM based model\nThis notebook is part of a series:  \n  * [Ventilator: Feature engineering](https:\/\/www.kaggle.com\/mistag\/ventilator-feature-engineering)\n  * [Keras model tuning with Optuna](https:\/\/www.kaggle.com\/mistag\/keras-model-tuning-with-optuna)\n  * [[train] Ventilator LSTM Model - part 1](https:\/\/www.kaggle.com\/mistag\/train-ventilator-lstm-model-part-i)\n  * [[train] Ventilator LSTM Model - part 2](https:\/\/www.kaggle.com\/mistag\/train-ventilator-lstm-model-part-ii)\n  * [[pred] Ventilator LSTM Model](https:\/\/www.kaggle.com\/mistag\/pred-ventilator-lstm-model)\n  \n## References\nThe code is based on these references:  \n  * [Improvement base on Tensor Bidirect LSTM](https:\/\/www.kaggle.com\/kensit\/improvement-base-on-tensor-bidirect-lstm-0-173\/notebook) by [Ken Sit](https:\/\/www.kaggle.com\/kensit)\n  * [Ensemble Folds with MEDIAN - [0.153]](https:\/\/www.kaggle.com\/cdeotte\/ensemble-folds-with-median-0-153) by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte)","3d25ad39":"# Model","8e81cec9":"# Dataset"}}