{"cell_type":{"9c7d63b0":"code","da6bcc29":"code","5ecc0126":"code","7f93bef4":"code","5d1dc98b":"code","869a8248":"code","bd065888":"code","abdcd8de":"code","8253b2d6":"code","034c9ea8":"code","1dd32577":"code","00443003":"code","d24b94bd":"code","a750b727":"code","27599cc5":"code","d004c984":"code","61d4a665":"code","07565c12":"code","1d3a5a04":"code","abb7adcb":"code","34f00a7b":"code","6a505b87":"code","ab70b6b1":"code","97410bf9":"code","1b5878f0":"code","eb461ffa":"code","3a0ea173":"code","e3da5e93":"markdown","cbcdba4d":"markdown","6d1be72b":"markdown","ed0a9b60":"markdown","ef5bdda6":"markdown","f03fc90d":"markdown","cbb443de":"markdown","bd0923bb":"markdown","c1293c3a":"markdown","11c81f7b":"markdown","25ae36cd":"markdown","b70d62cd":"markdown"},"source":{"9c7d63b0":"# Executando jupyter com acesso root caso vc este encontrando dificuldade em instalar as depend\u00eancias\n# sudo -E env \"PATH=$PATH\" jupyter notebook --allow-root","da6bcc29":"import os\nimport sys\nimport pathlib\nimport numpy as np \nimport pandas as pd \nfrom tqdm import tqdm\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom keras import models\nfrom keras.models import Sequential\nfrom keras.applications.vgg16 import VGG16\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.model_selection import train_test_split \n\nimport cv2\n%matplotlib inline\nimport matplotlib.pyplot as plt\n","5ecc0126":"if tf.test.gpu_device_name():\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\nelse:\n    print(\"Please install GPU version of TF\")","7f93bef4":"train_df = pd.read_csv(\"\/kaggle\/input\/planets-dataset\/planet\/planet\/train_classes.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/planets-dataset\/planet\/planet\/sample_submission.csv\")\n","5d1dc98b":"train_classes = train_df[:]['tags']\n\nno_classes = len(train_classes.unique())\nprint(f'Given {len(train_classes)} samples, there are {no_classes} unique classes.', '\\n')\n\ntrain_df.head()","869a8248":"# Normalizando nomes das tags\nflatten = lambda l: [item for sublist in l for item in sublist]\nlabels = list(set(flatten([l.split(' ') for l in train_df['tags'].values])))\n\n# Criando uma cole\u00e7\u00e3o key, value para categorizar de forma numerica nossos labels\nlabel_map = {l: i for i, l in enumerate(labels)}\nprint(f'labels = {labels},\\n length = {len(labels)}', '\\n')\n\nprint(f'label_map = {label_map},\\n length = {len(label_map)}')","bd065888":"\nnew_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(15, 15))\ni = 0\nfor f, tags in train_df[:9].values:\n    img = cv2.imread('\/kaggle\/input\/planets-dataset\/planet\/planet\/train-jpg\/{}.jpg'.format(f))\n    ax[i \/\/ 3, i % 3].imshow(img)\n    ax[i \/\/ 3, i % 3].set_title('{} - {}'.format(f, tags))\n  \n    i += 1\n    \nplt.show()","abdcd8de":"# Load the train-jpg file path\n\ntrain_img_dir = pathlib.Path('\/kaggle\/input\/planets-dataset\/planet\/planet\/train-jpg')\ntest_img_dir = pathlib.Path('\/kaggle\/input\/planets-dataset\/planet\/planet\/test-jpg')\ntest_add_img_dir = pathlib.Path('\/kaggle\/input\/planets-dataset\/test-jpg-additional\/test-jpg-additional')\n\ntrain_img_path = sorted(list(train_img_dir.glob('*.jpg')))\n\ntrain_img_count = len(train_img_path)\nprint('Quantidade de imgs chips para treino: ',str(train_img_count))","8253b2d6":"# first test jpg file path\n\ntest_img_path = sorted(list(test_img_dir.glob('*.jpg')))\n\ntest_img_count = len(test_img_path)\nprint('Quantidade de imgs chips para testes: ',str(test_img_count))","034c9ea8":"# second test jpg file path\n\ntest_add_img_path = sorted(list(test_add_img_dir.glob('*.jpg')))\n\ntest_add_img_count = len(test_add_img_path)\nprint('Quantidade de imgs chips para testes adicional: ',str(test_add_img_count))","1dd32577":"# verifica se o n\u00famero de imagens jpg seja igual ao n\u00famero de amostras no arquivo csv para cada conjunto de dados\n\n# train\nif len(train_img_path) == len(train_df):\n    print('Dataset de treino com a mesma quantidade de samples listada no csv')\n#caso n\u00e3o seja igual a execu\u00e7\u00e3o para aqui\nassert len(train_img_path) == len(train_df) \n\n\n# test\nif len(test_img_path)+len(test_add_img_path) == len(test_df):\n    print('Dataset de testes com a mesma quantidade de samples listada no csv')\n#caso n\u00e3o seja igual a execu\u00e7\u00e3o para aqui\nassert len(test_img_path)+len(test_add_img_path) == len(test_df)","00443003":"input_size = 64\ninput_channels = 3\n\nbatch_size = 64","d24b94bd":"x_train = []\ny_train = []\n\nfor f, tags in tqdm(train_df.values, miniters=1000):\n    img = cv2.imread('..\/input\/planets-dataset\/planet\/planet\/train-jpg\/{}.jpg'.format(f))\n    img = cv2.resize(img, (input_size, input_size))\n    targets = np.zeros(17)\n    for t in tags.split(' '):\n        targets[label_map[t]] = 1\n    x_train.append(img)\n    y_train.append(targets)\n        \nx_train = np.array(x_train, np.float32)\ny_train = np.array(y_train, np.uint8)\n\nprint(x_train.shape)\nprint(y_train.shape)","a750b727":"x_test = []\n\ntest_image_names = os.listdir(test_img_dir)\n\nn_test = len(test_image_names)\ntest_classes = test_df.iloc[:n_test, :]\nadd_classes = test_df.iloc[n_test:, :]\n\ntest_add_image_names = os.listdir(test_add_img_dir)\n\nfor img_name, _ in tqdm(test_classes.values, miniters=1000):\n    img = cv2.imread(str(test_img_dir) + '\/{}.jpg'.format(img_name))\n    x_test.append(cv2.resize(img, (64, 64)))\n    \nfor img_name, _ in tqdm(add_classes.values, miniters=1000):\n    img = cv2.imread(str(test_add_img_dir) + '\/{}.jpg'.format(img_name))\n    x_test.append(cv2.resize(img, (64, 64)))\n\nx_test = np.array(x_test, np.float32)\nprint(x_test.shape)","27599cc5":"X_train, X_valid, Y_train, Y_valid = train_test_split(x_train, y_train, test_size=0.2)","d004c984":"base_model = VGG16(include_top=False,\n                   weights='imagenet',\n                   input_shape=(input_size, input_size, input_channels))\n\nmodel = Sequential()\nmodel.add(BatchNormalization(input_shape=(input_size, input_size, input_channels)))\n\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(17, activation='sigmoid'))","61d4a665":"\nmodel.compile(loss='binary_crossentropy',optimizer=\"SGD\", metrics=['accuracy'])\n    \ncallbacks = [EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n                ModelCheckpoint(filepath='weights\/best_weights',\n                                 save_best_only=True,\n                                 save_weights_only=True)]\nmodel.summary()","07565c12":"history = model.fit(x=X_train, y=Y_train, validation_data=(X_valid, Y_valid),\n                  batch_size=batch_size,verbose=2, epochs=15,callbacks=callbacks,shuffle=True)","1d3a5a04":"model.save('.\/models\/vgg16-amazon2')","abb7adcb":"model_back = models.load_model(\".\/models\/vgg16-amazon2\")","34f00a7b":"p_valid = model_back.predict(X_valid, batch_size = batch_size, verbose=1)\n\nprint('Acur\u00e1cia: ',fbeta_score(Y_valid, np.array(p_valid) > 0.18, beta=2, average='samples'))","6a505b87":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","ab70b6b1":"y_pred = []\np_test = model_back.predict(x_test, batch_size=batch_size, verbose=2)\ny_pred.append(p_test)","97410bf9":"result = np.array(y_pred[0])\nfor i in range(1, len(y_pred)):\n    result += np.array(y_pred[i])\nresult = pd.DataFrame(result, columns=labels)","1b5878f0":"# Translating the probability predictions to the unique labels\npreds = []\nfor i in tqdm(range(result.shape[0]), miniters=1000):\n    a = result.loc[[i]]\n    a = a.apply(lambda x: x>0.2, axis=1)\n    a = a.transpose()\n    a = a.loc[a[i] == True]\n    ' '.join(list(a.index))\n    preds.append(' '.join(list(a.index)))","eb461ffa":"# Replacing the tags columns with the predicted labels\ntest_df['tags'] = preds\ntest_df.head()","3a0ea173":"# Converting the dataframe to a csv file for submission\ntest_df.to_csv('sample_testes_results.csv', index=False)","e3da5e93":"# Construindo o modelo","cbcdba4d":"# Conclus\u00e3o\n\nO modelo proposto obteve uma acur\u00e1cia de 91%, o mesmo conseguiu rotular de forma satisfat\u00f3ria os chips do dataset de testes. Foi observado que itera\u00e7\u00f5es acima de 15 passos ocasionava overfit e come\u00e7ava a convergir. Vale ressaltar que a acur\u00e1cia obtida foi relativamente boa, levando em considera\u00e7\u00e3o que foi utilizado chips\/imagens JPG com baixa qualidade, n\u00e3o foi poss\u00edvel utilizar os datastes com imagens TIF com maior qualidade, pois o Hardware usado n\u00e3o suportou todo o processamento. ","6d1be72b":"### Objetivo do Challenge\n\nO desafio apresentado \u00e9 criar um modelo que rotule essas imagens com base nas condi\u00e7\u00f5es atmosf\u00e9ricas e no uso do solo, com o objetivo geral de rastrear a pegada de carbono humana na maior floresta tropical do mundo. Vod\u00ea pode encontrar a competi\u00e7\u00e3o original [aqui](http:\/\/www.kaggle.com\/c\/planet-understanding-the-amazon-from-space).\n\n### Entendendo o Dataset\n\nA cada minuto, o mundo perde uma \u00e1rea de floresta do tamanho de 48 campos de futebol. E o desmatamento na Bacia Amaz\u00f4nica \u00e9 respons\u00e1vel pela maior parte, contribuindo para a redu\u00e7\u00e3o da biodiversidade, perda de habitat, mudan\u00e7a clim\u00e1tica e outros efeitos devastadores. Por\u00e9m, dados melhores sobre a localiza\u00e7\u00e3o do desmatamento e invas\u00e3o humana nas florestas podem ajudar os governos e as partes interessadas locais a responder com mais rapidez e efic\u00e1cia.\n\nOs chips para esta competi\u00e7\u00e3o foram derivados dos produtos de cena anal\u00edtica full-frame da Planet usando nossos sat\u00e9lites de 4 bandas em \u00f3rbita sincronizada com o sol (SSO) e a \u00f3rbita da Esta\u00e7\u00e3o Espacial Internacional (ISS).\n\nOs r\u00f3tulos podem ser amplamente divididos em tr\u00eas grupos: **_condi\u00e7\u00f5es atmosf\u00e9ricas, fen\u00f4menos comuns de uso do solo \/ cobertura do solo e fen\u00f4menos raros de uso do solo \/ cobertura do solo_**. Cada chip ter\u00e1 um e potencialmente mais de um r\u00f3tulo atmosf\u00e9rico e zero ou mais r\u00f3tulos comuns e raros. **Chips rotulados como turvos n\u00e3o devem ter outros r\u00f3tulos, mas pode haver erros de rotulagem.**\n\nAs nuvens s\u00e3o um grande desafio para imagens passivas de sat\u00e9lite, e a cobertura de nuvens e pancadas de chuva di\u00e1rias na bacia amaz\u00f4nica podem complicar significativamente o monitoramento na \u00e1rea. Por esse motivo, optamos por incluir uma etiqueta de cobertura de nuvens para cada chip. Esses r\u00f3tulos refletem de perto o que se veria em uma previs\u00e3o do tempo local: claro, parcialmente nublado, nublado e neblina.\n\nNota lateral: Os r\u00f3tulos comuns neste conjunto de dados s\u00e3o floresta tropical, agricultura, rios, vilas\/cidades e estradas.\n\n### Estrutura Dataset\n\nQuadrados de imagens de alta resolu\u00e7\u00e3o (256 x 256) em quatro bandas (RGB + IR) do Planet Flock 2 Satellites. Cada bloco pode ter v\u00e1rios r\u00f3tulos (comuns e menos comuns), mas apenas um dos r\u00f3tulos de cobertura de nuvem.\n\nR\u00f3tulos comuns | R\u00f3tulos menos comuns | Cloud Cover Labels\n------------ | ------------- | -------------\nPrimary Rain Forest | Slash and Burn | Clear\nWater (Rivers & Lakes) | Selective Logging | Partly cloudy\nHabitation | Blooming | Cloudy\nAgriculture | Conventional Mining | Haze\nRoad | Artisinal Mining |\nCultivation | Blow Down |\nBare Ground | |\n\n### File formats\n\nEste \u00e9 um conjunto de dados de competi\u00e7\u00e3o que foi contribu\u00eddo pela Planet. O conjunto de dados cont\u00e9m arquivos csv de treinamento e imagens de treinamento \/ teste de chips de imagem de sat\u00e9lite da floresta amaz\u00f4nica.\n\n- **rain.csv** - a list of training file names and their labels, the labels are space-delimited\n- **sample_submission.csv** - correct format of submission, contains all the files in the test set. For more information about the submission file, please go to the Evaluation page.\n- **[train\/test]-tif-v2.tar.7z** - tif files for the training\/test set (updated: May 5th, 2017)\n- **[train\/test]-jpg[-additional].tar.7z** - jpg files for the trainin\/test set (updated: May 5th, 2017)\n- **Kaggle-planet-[train\/test]-tif.torrent** - a BitTorrent file for downloading [train\/test]-tif-v2.tar.7z \n\n","ed0a9b60":"# SIDIA Challenge Amazonas do Espa\u00e7o \ud83c\udf0f\ud83c\udf32\ud83c\udf33\n## Autor: Douglas Queiroz G B.\n","ef5bdda6":"O dataset j\u00e1 existe no kaggle","f03fc90d":"A maioria das imagens tem dois r\u00f3tulos, tr\u00eas e quatro r\u00f3tulos s\u00e3o bastante iguais em n\u00famero e um, cinco, e seis r\u00f3tulos n\u00e3o aparecem com muita frequ\u00eancia. Imagens que podem ser classificadas sob sete, oito ou nove r\u00f3tulos raramente aparecem no conjunto de dados. Interessante; h\u00e1 um grande desequil\u00edbrio aqui.\n\nSeguindo em frente, \u00e9 uma boa ideia visualizar algumas das imagens para obter uma vis\u00e3o de como esses r\u00f3tulos devem ser:","cbb443de":"### Importando e baixando bibliotecas necess\u00e1ris","bd0923bb":"# Pr\u00e9 Procesamento","c1293c3a":"### Data Preprocessing","11c81f7b":"### Baixando e extraindo Dataset","25ae36cd":"## Arquiteturas Implementadas\n\n### VGG-16 \n\nA rede VGG \u00e9 uma rede neural convolucional inventada por [Simonyan e Zisserman do Visual Geometry Group (VGG)](https:\/\/www.robots.ox.ac.uk\/~vgg\/research\/very_deep\/) da University of Oxford em 2014. Tornou-se mais versada na comunidade de vis\u00e3o computacional ap\u00f3s ser nomeada vice-campe\u00e3 da tarefa de classifica\u00e7\u00e3o ILSVRC de 2014 . \u00c9 frequentemente associado ao VGG-19, com a diferen\u00e7a de que o VGG-16 tem 16 camadas com pesos trein\u00e1veis em vez de 19, da\u00ed o nome. ","b70d62cd":"### Data Visualization + Exploration"}}