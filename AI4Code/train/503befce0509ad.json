{"cell_type":{"9a0e85c7":"code","16d979f0":"code","1cf13c0b":"code","1f38f5f2":"code","ca106449":"code","7a0c82e9":"code","e2e74302":"code","cfa5ce3b":"code","2cc658c6":"code","6183d17b":"code","bc75b000":"code","6df3e80c":"code","0e734940":"code","70f285c2":"code","38a57eb9":"code","58cc85e7":"code","bb8e4737":"code","218da8b0":"code","0c28f509":"code","741c494f":"code","c95e77c7":"code","6eeb2bf3":"code","d6cb82c3":"code","30f98e45":"code","d7487715":"markdown","1731466a":"markdown","e8f5dfbd":"markdown","45a562b7":"markdown","c438152d":"markdown","999fe1a6":"markdown","a075c32c":"markdown","e808d83e":"markdown","46819efe":"markdown"},"source":{"9a0e85c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16d979f0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\nfrom sklearn.metrics import f1_score,recall_score,precision_score,accuracy_score\nfrom sklearn.metrics import roc_curve,roc_auc_score\n\nfrom collections import Counter\n\nfrom sklearn.cluster import KMeans\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport random\nrandom.seed(42)","1cf13c0b":"#reading the data and high level understanding\n\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nprint(df.shape)\ndf.head()","1f38f5f2":"df.describe()","ca106449":"#checking null data. from output we can see, we do not have any null data. So not going ahead to check feature specific null data.\nnp.sum(df.isnull().any())","7a0c82e9":"#checking the distribution of each class. \ndf['Class'].value_counts()","e2e74302":"#Let's compare the distribution for Genuine cases & Fraud cases for each feature\nfeatures_list = df.columns\nplt.figure(figsize=(12,31*4))\ngs = gridspec.GridSpec(31,1)\n\nfor i, col in enumerate(features_list):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[col][df['Class']==0],color='g',label='Genuine Case')\n    sns.distplot(df[col][df['Class']==1],color='r',label='Fraud Case')\n    ax.legend()\nplt.show()","cfa5ce3b":"#df.drop(labels = ['V8','V13','V15','V20','V22','V23','V24','V25','V26','V27','V28','Time'], axis = 1, inplace=True)\ndf.drop('Time', axis = 1, inplace=True)\ndf.columns","2cc658c6":"Scl = StandardScaler()\ndf['Amount_scl'] = Scl.fit_transform(df['Amount'].values.reshape(-1,1))\ndf.drop('Amount',axis = 1, inplace = True)\n","6183d17b":"df.columns","bc75b000":"#let's prepare the data\nfeatures_list = [x for x in df.columns if x not in ['Class']]\nfeatures = df.loc[:,features_list]\nlabel = df['Class']\nx_train,x_test,y_train,y_test = train_test_split(features,label,test_size=0.2,random_state=0)","6df3e80c":"x_train.columns","0e734940":"print(\"Train size: \", x_train.shape)\nprint(\"Test size: \", x_test.shape)","70f285c2":"sm = SMOTE(random_state=42)","38a57eb9":"#Create the oversample data\nsm_X, sm_y = sm.fit_resample(x_train,y_train,)\n\n#Counts of each class in updated data\nprint(sorted(Counter(sm_y).items()))","58cc85e7":"#LEt's create a method to calculate\ndef Random_forest_medel_and_scores(model,x_train,x_test,y_train,y_test):\n    model.fit(x_train,y_train)\n\n    y_pred = model.predict(x_train)\n    \n    print(\"\\n----------Accuracy Scores for Train data------------------------------------\")\n    print(\"F1 Score: \", f1_score(y_train,y_pred))\n    print(\"Precision Score: \", precision_score(y_train,y_pred))\n    print(\"Recall Score: \", recall_score(y_train,y_pred))\n\n\n    print(\"\\n----------Accuracy Scores for Test data------------------------------------\")\n    y_pred_test = model.predict(x_test)\n    \n    print(\"F1 Score: \", f1_score(y_test,y_pred_test))\n    print(\"Precision Score: \", precision_score(y_test,y_pred_test))\n    print(\"Recall Score: \", recall_score(y_test,y_pred_test))\n\n    #Confusion Matrix\n    plt.figure(figsize=(18,6))\n    gs = gridspec.GridSpec(1,2)\n\n    ax1 = plt.subplot(gs[0])\n    cnfs_matrix = confusion_matrix(y_train,y_pred)\n    row_sum = cnfs_matrix.sum(axis=1,keepdims=True)\n    cnfs_matrix_norm =cnfs_matrix \/ row_sum\n    sns.heatmap(cnfs_matrix_norm, cmap='YlGnBu', annot=True)\n    plt.title(\"Normalized Confusion Matrix - Train Data\")\n\n    ax2 = plt.subplot(gs[1])\n    cnfs_matrix = confusion_matrix(y_test,y_pred_test)\n    row_sum = cnfs_matrix.sum(axis=1,keepdims=True)\n    cnfs_matrix_norm =cnfs_matrix \/ row_sum\n    sns.heatmap(cnfs_matrix_norm,cmap='YlGnBu',annot=True)\n    plt.title(\"Normalized Confusion Matrix - Test Data\")","bb8e4737":"# Training over-sampled data set.  - RandomForest\nrnf_clf = RandomForestClassifier(n_estimators=100,criterion='gini',n_jobs=-1, random_state=0)\n\n#Train the model on oversampled data and check the performance on original test data\nRandom_forest_medel_and_scores(rnf_clf,sm_X,x_test,sm_y,y_test)","218da8b0":"kmean = KMeans(n_clusters = 11, init = 'k-means++', random_state = 0)\nclusters_Kmeans = kmean.fit_predict(x_train)","0c28f509":"#Merge clusters with other input features on Train Data\nx_train_K = np.c_[(x_train,clusters_Kmeans )]\nx_train_K.shape","741c494f":"#Predict the cluster for test data & merge it with other features\ntest_clusters = kmean.predict(x_test)\nx_testK = np.c_[(x_test,test_clusters )]\nx_testK.shape\n","c95e77c7":"#Generate the oversample data for training purpose\nsm_x2,sm_y2=os.fit_resample(x_train_K,y_train)\n\n#Counts of each class in oversampled data\nprint(sorted(Counter(sm_y2).items()))\n","6eeb2bf3":"#training on over-sampled data set + Kmeans segmentation - RandomForest\nrnf_clf2 = RandomForestClassifier(n_estimators=100,criterion='gini',n_jobs=-1, random_state=0)\n#Training the model on oversampled data and checking the performance on actual test data\nRandom_forest_medel_and_scores(rnf_clf2,sm_x2,x_testK,sm_y2,y_test)","d6cb82c3":"#Will check the consistency of this model by using cross validation scores based on the original train data.\ncv_score = cross_val_score(rnf_clf2,x_train_K,y_train,cv=5,scoring='f1')\nprint(\"Average F1 score CV\",cv_score.mean())\n","30f98e45":"cv_score = cross_val_score(rnf_clf2,x_train_K,y_train,cv=5,scoring='recall')\nprint(\"Average Recall score CV\",cv_score.mean())","d7487715":"#### We have 250 standard deviation for Amount data. Lets standardize the data. We will take help of StandardScaler() to standardize the data.\n\n","1731466a":"### We can see we have 492 (< 1%) records are fraud case records. rest > 99% are not fraud. So this is clearly an imbalanced data set.","e8f5dfbd":"\n# Data Preparetion:\n  We will add dummy data similar as 492 fraud cases. We will use Synthetic Minority Oversampling Technique (SMOTE) to balance the dataset. This offer a re-sampling techniques commonly used in datasets showing strong between-class imbalance. This way we will have  balance in genuine and fraud case\n","45a562b7":"#### After adding the clusters in the dataset, the performance of RandomForest model has been improved a little bit: F1 score of 87 and recall score of 85 on the orignal test data (without oversmapling). ","c438152d":"#### We imputed the fraud records and brought close to genuine records in this oversampled data using SMOTE. Both the classes are equally distributed now.","999fe1a6":"##Feature selection:\n#### 1) From above series of distributions, we can see Normal Distribution of genuine transactions (class = 0) is matching with Normal Distribution of fraud transactions (class = 1) for 'V8','V13','V15','V20','V22','V23','V24','V25','V26','V27','V28' features. There is no difference in behavior for these features if the transaction is genuine or fraud. We can delete these features as they may not be useful in finding fraud records.\n#### 2) Time contains the seconds elapsed between the transaction for that record and the first transaction in the dataset. Also the data is in increasing order. This is not a useful feature and we can remove this as well.\n\n#### For time being we will remove the Time data only and will see the effectiveness of model without removing the columns. then we can fine tune the model.\n","a075c32c":"#### RandomForest has given good results after balancing the training data using SMOTE approach: F1 score of 85 on orignal test data (without oversmapling)\n\n#### lets try with K-means clustering to identify the clusters in the dataset, this could improve the predictive power once we add the result in train set.","e808d83e":"\n####     using cross validation, recall score diped a little bit. However overall F1-score is still around 85. We can go ahead with this model.\n\n","46819efe":"# Conclusion: \n\n#### In general, oversampling techniques like SMOTE should provide better results on imbalanced datasets. We added clustering over the top of SMOTE to identify the patterns better."}}