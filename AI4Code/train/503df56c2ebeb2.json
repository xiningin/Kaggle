{"cell_type":{"22ee39d6":"code","de1ef5c1":"code","30efdc63":"code","9dc2bcbc":"code","8166cb2e":"code","bc46c79f":"code","f4a0632a":"code","c501cad7":"code","e0c7d04f":"code","c368a412":"code","0588d646":"code","d3bc1020":"code","4d77a163":"code","b5baad31":"code","671221ed":"code","6d02fc70":"code","c4f52673":"code","df8750e9":"code","cddd4c69":"code","f2f5177d":"code","ca49fa82":"code","0ff92de4":"code","9e0bed4b":"code","9a71e957":"code","625416dd":"code","c325cf06":"code","109ea8d8":"code","19be964c":"code","163f1637":"code","bd3b2ab0":"code","380ef058":"code","8ab08f77":"code","eae04b9f":"code","f2121907":"code","964c039d":"code","178998e4":"code","7e87c534":"code","96996ee6":"code","c6eb6edf":"code","9d4adaad":"code","921da276":"code","45da4182":"code","e7787453":"code","f309b748":"code","3599a49c":"code","c46ad1b6":"code","b3ea02d6":"code","afd4815d":"code","dcb26925":"code","0d097f79":"code","a8613d91":"code","e266ab6d":"code","573275ff":"code","53c903ff":"code","2cb0b951":"code","d3e620cf":"code","2823c0a2":"code","ccf2c1c5":"code","5fa17dea":"code","05653258":"code","8bb58dce":"code","581ca08c":"code","3f910597":"code","16cf71f2":"code","be06fd80":"code","72bff6e5":"code","8c432737":"code","9d5c86fe":"code","185b3885":"code","c2fe692b":"code","0b98d11b":"code","c9d7d433":"code","8d140d15":"code","55b616ed":"code","4a7f0ac9":"markdown","b61d22eb":"markdown","de65222c":"markdown","2ecebe56":"markdown","99446b7f":"markdown","9fc9103e":"markdown","df1020c8":"markdown","2bf854d1":"markdown","33cc4213":"markdown","899071ff":"markdown","86e344ac":"markdown","1e8a778c":"markdown","d73e836a":"markdown","e717a9ea":"markdown","bfc74324":"markdown","a8f8ec00":"markdown","67ddeedc":"markdown","77728757":"markdown","4164839d":"markdown","f74eaf1d":"markdown","3e098f01":"markdown","c9f8b026":"markdown","c880b3b3":"markdown","aabbdbdb":"markdown","d015f465":"markdown","4a4ecc2b":"markdown"},"source":{"22ee39d6":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","de1ef5c1":"# You have to include the full link to the csv file containing your dataset\n\nreviews_df = pd.read_csv('..\/input\/amazon-alexa-reviews\/amazon_alexa.tsv', sep='\\t')\n","30efdc63":"reviews_df.sample(3)","9dc2bcbc":"reviews_df.info()\nreviews_df.describe()","8166cb2e":"reviews_df['verified_reviews']","bc46c79f":" sns.heatmap(reviews_df.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")","f4a0632a":"reviews_df.hist(bins = 30, figsize = (13,5), color = 'r')\n","c501cad7":"# Let's get the length of the messages\n\nreviews_df['length'] = reviews_df['verified_reviews'].apply(len)\nreviews_df.head()\n","e0c7d04f":"reviews_df['length'].plot(bins=100, kind='hist') ","c368a412":"reviews_df.length.describe()","0588d646":"# Let's see the longest message 43952\nreviews_df[reviews_df['length'] == 2851]['verified_reviews'].iloc[0]","d3bc1020":"# Let's see the shortest message \nreviews_df[reviews_df['length'] == 1]['verified_reviews'].iloc[0]","4d77a163":"# Let's see the message with mean length \nreviews_df[reviews_df['length'] == 133]['verified_reviews'].iloc[0]","b5baad31":"positive = reviews_df[reviews_df['feedback']==1]","671221ed":"negative = reviews_df[reviews_df['feedback']==0]","6d02fc70":"negative.sample(2)","c4f52673":"positive.sample(2)","df8750e9":"sns.countplot(reviews_df['feedback'], label = \"Count\") ","cddd4c69":"sns.countplot(x = 'rating', data = reviews_df)","f2f5177d":"plt.figure(figsize = (40,15))\nsns.barplot(x = 'variation', y='rating', data = reviews_df, palette = 'deep')","ca49fa82":"sentences = reviews_df['verified_reviews'].tolist()\nlen(sentences)\n","0ff92de4":"print(sentences)","9e0bed4b":"sentences_as_one_string =\" \".join(sentences)","9a71e957":"sentences_as_one_string","625416dd":"from wordcloud import WordCloud\n\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(sentences_as_one_string))","c325cf06":"negative_list = negative['verified_reviews'].tolist()\n\nnegative_list","109ea8d8":"negative_sentences_as_one_string = \" \".join(negative_list)","19be964c":"\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(negative_sentences_as_one_string))","163f1637":"# Let's drop the date\nreviews_df = reviews_df.drop(['date', 'rating', 'length'],axis=1)","bd3b2ab0":"reviews_df.sample(3)","380ef058":"variation_dummies = pd.get_dummies(reviews_df['variation'], drop_first = True)\n# Avoid Dummy Variable trap which occurs when one variable can be predicted from the other.","8ab08f77":"variation_dummies.head(3)","eae04b9f":"# first let's drop the column\nreviews_df.drop(['variation'], axis=1, inplace=True)","f2121907":"# Now let's add the encoded column again\nreviews_df = pd.concat([reviews_df, variation_dummies], axis=1)","964c039d":"reviews_df.sample(3)","178998e4":"import string\nstring.punctuation","7e87c534":"Test = 'Hello Mr. Future, I am so happy to be learning AI now!!'","96996ee6":"Test_punc_removed = [char for char in Test if char not in string.punctuation]","c6eb6edf":"# Join the characters again to form the string.\nTest_punc_removed_join = ''.join(Test_punc_removed)","9d4adaad":"import nltk # Natural Language tool kit \n\nnltk.download('stopwords')","921da276":"# You have to download stopwords Package to execute this command\nfrom nltk.corpus import stopwords\nstopwords.words('english')","45da4182":"Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]","e7787453":"Test_punc_removed_join_clean # Only important (no so common) words are left","f309b748":"from sklearn.feature_extraction.text import CountVectorizer\nsample_data = ['This is the first document.','This document is the second document.','And this is the third one.','Is this the first document?']\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(sample_data)\n","3599a49c":"print(vectorizer.get_feature_names())\n","c46ad1b6":"print(X.toarray())  ","b3ea02d6":"# Let's define a pipeline to clean up all the messages \n# The pipeline performs the following: (1) remove punctuation, (2) remove stopwords\n\ndef message_cleaning(message):\n    Test_punc_removed = [char for char in message if char not in string.punctuation]\n    Test_punc_removed_join = ''.join(Test_punc_removed)\n    Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]\n    return Test_punc_removed_join_clean","afd4815d":"# Let's test the newly added function\nreviews_df_clean = reviews_df['verified_reviews'].apply(message_cleaning)","dcb26925":"print(reviews_df_clean[3]) # show the cleaned up version","0d097f79":"print(reviews_df['verified_reviews'][3]) # show the original version","a8613d91":"reviews_df_clean","e266ab6d":"from sklearn.feature_extraction.text import CountVectorizer\n# Define the cleaning pipeline we defined earlier\nvectorizer = CountVectorizer(analyzer = message_cleaning)\nreviews_countvectorizer = vectorizer.fit_transform(reviews_df['verified_reviews'])","573275ff":"print(vectorizer.get_feature_names())\n","53c903ff":"print(reviews_countvectorizer.toarray())  ","2cb0b951":"reviews_countvectorizer.shape","d3e620cf":"# first let's drop the column\nreviews_df.drop(['verified_reviews'], axis=1, inplace=True)\nreviews = pd.DataFrame(reviews_countvectorizer.toarray())\n","2823c0a2":"# Now let's concatenate them together\nreviews_df = pd.concat([reviews_df, reviews], axis=1)\n","ccf2c1c5":"reviews_df.head()","5fa17dea":"# Let's drop the target label coloumns\nX = reviews_df.drop(['feedback'],axis=1)\n","05653258":"y = reviews_df['feedback']","8bb58dce":"X.shape","581ca08c":"y.shape","3f910597":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","16cf71f2":"from sklearn.naive_bayes import MultinomialNB\n\nNB_classifier = MultinomialNB()\nNB_classifier.fit(X_train, y_train)","be06fd80":"from sklearn.metrics import classification_report, confusion_matrix\n","72bff6e5":"y_predict_train = NB_classifier.predict(X_train)\ny_predict_train\ncm = confusion_matrix(y_train, y_predict_train)\nsns.heatmap(cm, annot=True)","8c432737":"# Predicting the Test set results\ny_predict_test = NB_classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_predict_test)\nsns.heatmap(cm, annot=True)","9d5c86fe":"print(classification_report(y_test, y_predict_test))","185b3885":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","c2fe692b":"model = LogisticRegression()\nmodel.fit(X_train, y_train)","0b98d11b":"y_pred = model.predict(X_test)","c9d7d433":"from sklearn.metrics import confusion_matrix, classification_report\n\nprint('Accuracy {} %'.format( 100 * accuracy_score(y_pred, y_test)))","8d140d15":"cm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot = True)","55b616ed":"print(classification_report(y_test, y_pred))\n","4a7f0ac9":"# TASK #11: ASSESS TRAINED MODEL PERFORMANCE  ","b61d22eb":"![alt text](https:\/\/drive.google.com\/uc?id=1HfZvPCWAwKoYl1qogYlxD_CIZYxYw0aI)","de65222c":"![alt text](https:\/\/drive.google.com\/uc?id=1XGc89Cxi0ooFQIc6o041cz8-qwXg7l3g)","2ecebe56":"data source: https:\/\/www.kaggle.com\/sid321axn\/amazon-alexa-reviews\/kernels","99446b7f":"![alt text](https:\/\/drive.google.com\/uc?id=1AXTHZ9KVUsJjMm9Whc4Adi5T4OznsSYn)","9fc9103e":"![alt text](https:\/\/drive.google.com\/uc?id=1NT6Fm-lWUWNsu9i8uzVS4Q5pcm5gp8RK)","df1020c8":"![alt text](https:\/\/drive.google.com\/uc?id=1Xox54bvjhGOhrG-fSxEUIEgw1R3g-RIt)","2bf854d1":"# TASK 7: UNDERSTAND HOW TO PERFORM COUNT VECTORIZATION (TOKENIZATION)","33cc4213":"![alt text](https:\/\/drive.google.com\/uc?id=18Z4ug4UuyQG79lyPKs1zQwtrP_S4_yoU)","899071ff":"![alt text](https:\/\/drive.google.com\/uc?id=14_ft6Wiu-VaiU_5Ew2nS7EGGr3oLLQf8)","86e344ac":"![alt text](https:\/\/drive.google.com\/uc?id=1sVLtg8GaE3ZhNEZX1WJbxs7KAQyQ5dpX)","1e8a778c":"\n<table>\n  <tr><td>\n    <img src=\"https:\/\/drive.google.com\/uc?id=11BquVVgQTebvVO5NZ2TGA526rulbWBv5\"\n         alt=\"Fashion MNIST sprite\"  width=\"1000\">\n  <\/td><\/tr>\n  <tr><td align=\"center\">\n    <b>Figure 1. Analyzing Customer Sentiment\n  <\/td><\/tr>\n<\/table>\n","d73e836a":"# TASK #2: IMPORT LIBRARIES AND DATASETS","e717a9ea":"# TASK 6: UNDERSTAND HOW TO REMOVE STOPWORDS","bfc74324":"![alt text](https:\/\/drive.google.com\/uc?id=1eQi-Gq66e-sNw1ZvGs-zkJg95mCYdFoJ)","a8f8ec00":"![alt text](https:\/\/drive.google.com\/uc?id=1C32q5Uguymr9012x1lzRD5btnvJ-kW9r)","67ddeedc":"# TASK #3: EXPLORE DATASET","77728757":"# TASK #9: UNDERSTAND THE THEORY AND INTUITION BEHIND NAIVE BAYES","4164839d":"# TASK #10: TRAIN A NAIVE BAYES CLASSIFIER MODEL","f74eaf1d":"# TASK #8: PERFORM DATA CLEANING BY APPLYING EVERYTHING WE LEARNED SO FAR!","3e098f01":"# TASK #1: UNDERSTAND THE PROBLEM STATEMENT AND BUSINESS CASE","c9f8b026":"# TASK #4: PERFORM DATA CLEANING","c880b3b3":"![alt text](https:\/\/drive.google.com\/uc?id=1g5aXo5E-RIjRBy6-LLLA8gjG2j9dIL5X)","aabbdbdb":"# TASK #5: LEARN HOW TO REMOVE PUNCTUATION FROM TEXT","d015f465":"![alt text](https:\/\/drive.google.com\/uc?id=106OXP_z89Hqh1JYVaROIbst0N0CgFRuT)","4a4ecc2b":"# # TASK #12: - TRAIN AND EVALUATE A LOGISTIC REGRESSION CLASSIFIER"}}