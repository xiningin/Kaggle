{"cell_type":{"0f429925":"code","7802f81b":"code","42ebbfae":"code","ba62c942":"code","0b7469c8":"code","5512db46":"code","1d4dbadf":"code","5c341758":"code","ac7cfa59":"code","fc75dee7":"code","1e92077c":"code","7e6b1ee2":"code","89ea57a2":"code","638aed06":"code","669a48e5":"code","f476ec95":"code","1791f866":"code","c4c151f3":"code","11a893c0":"code","26245e80":"code","f3529148":"code","128f9845":"code","83885fda":"code","e53e96e0":"code","c31a20f0":"code","9d77edd3":"code","f31f4a60":"code","f1460494":"code","d6a9571e":"code","5cb6347b":"code","c765459c":"code","500ed9f5":"code","842f99d3":"code","1054de77":"code","9a6aa365":"code","49a87f3a":"code","1c9888d1":"code","7ff03df3":"code","2eb99a92":"code","6bd26b4b":"code","701d98c0":"code","a5354e96":"code","bb5f80d6":"code","af672252":"code","0361ffbe":"code","292bb13f":"code","0185c1ee":"code","89b2d2c3":"code","7b329100":"code","ee9c06b9":"code","935164b9":"code","ca90d482":"code","5657c7f4":"code","f1675410":"code","e38858c7":"code","e82f0e14":"code","7f713002":"code","414fbf31":"code","abf5c243":"code","83225ad4":"code","3a22a491":"code","25c9c672":"code","69e6547c":"code","854047f7":"code","641858b2":"code","f63b0167":"code","99bc9024":"code","459e8d3e":"markdown","9351130d":"markdown","cc5afdf6":"markdown","9126ba9c":"markdown","5fdff0a3":"markdown","cdc0e213":"markdown","8e06c589":"markdown","9b8edf25":"markdown","d9aa177c":"markdown","23f0f005":"markdown","35b7d4c7":"markdown","6f78c19d":"markdown"},"source":{"0f429925":"import numpy as np\nimport pandas as pd\nimport sqlite3\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nimport statsmodels.api as sm\nfrom matplotlib import pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nimport os\nfrom IPython.display import FileLink\nimport xgboost as xgb","7802f81b":"# Upload the information from database and into the Data Frame\ncon = sqlite3.connect(\"\/kaggle\/input\/lending-club-loan-data\/database.sqlite\")\ndf = pd.read_sql_query(\"SELECT loan_amnt, emp_length, home_ownership, annual_inc ,purpose, dti, loan_status from loan\", con)","42ebbfae":"# Check the structure of the data\nprint(df.dtypes)\n\n# Check the first five rows of the data\nprint(df.head(5))","ba62c942":"# Convert numeric fields to floats and int\ndf['loan_amnt'] = pd.to_numeric(df['loan_amnt'])\ndf['annual_inc'] = pd.to_numeric(df['annual_inc'])\ndf['dti'] = pd.to_numeric(df['dti'])\n# Check the structure of the data\nprint(df.dtypes)","0b7469c8":"# Distribution of loan amounts\nprint(df['loan_amnt'].mean())\nprint(df['loan_amnt'].median())\nprint(df['loan_amnt'].min())\nprint(df['loan_amnt'].max())","5512db46":"# Distribution of dti\nprint(df['dti'].mean())\nprint(df['dti'].median())\nprint(df['dti'].min())\nprint(df['dti'].max())","1d4dbadf":"# Distribution of annual income\nprint(df['annual_inc'].mean())\nprint(df['annual_inc'].median())\nprint(df['annual_inc'].min())\nprint(df['annual_inc'].max())","5c341758":"# Distribution of loan status\ndf['loan_status'].value_counts()","ac7cfa59":"# Distribution of employee length\ndf['emp_length'].value_counts()","fc75dee7":"# Distribution of purpose\ndf['purpose'].value_counts()","1e92077c":"# Distribution of home ownership\ndf['home_ownership'].value_counts()","7e6b1ee2":"# Drop the Outliers (they could be replaced by mean. Since we have lots of data, we can dispose of them)\n# Dti\nindices = df[df['dti'] > 100].index\ndf = df.drop(indices)\nindices = df[df['dti'] < 0].index\ndf = df.drop(indices)\n# Annual Income\nindices = df[df['annual_inc'] > 1000000].index #This value can be reviewed if it doesn't make sense\ndf = df.drop(indices)\nindices = df[df['annual_inc'] < 1000].index\ndf = df.drop(indices)","89ea57a2":"#Check for missing data\nprint(df['dti'].isnull().sum())\nprint(df['annual_inc'].isnull().sum())\nprint(df['loan_amnt'].isnull().sum())\nprint(df['purpose'].isnull().sum())\nprint(df['loan_status'].isnull().sum())\nprint(df['emp_length'].isnull().sum())\nprint(df['home_ownership'].isnull().sum())","638aed06":"#Drop corrupt\/unuseful data\nindices = df[df['emp_length'] == 'n\/a'].index\ndf = df.drop(indices)\nindices = df[df['home_ownership'] == 'ANY'].index\ndf = df.drop(indices)\nindices = df[df['home_ownership'] == 'NONE'].index\ndf = df.drop(indices)\nindices = df[df['home_ownership'] == 'OTHER'].index\ndf = df.drop(indices)\nindices = df[df['purpose'] == 'other'].index\ndf = df.drop(indices)\nindices = df[df['annual_inc'].isnull()].index\ndf = df.drop(indices)","669a48e5":"# Select only terminal loans (Default\/Charged Off and Fully Paid) and replace for 1 and 0\ndf = df[df.loan_status != 'Current']\ndf = df[df.loan_status != 'In Grace Period']\ndf = df[df.loan_status != 'Late (16-30 days)']\ndf = df[df.loan_status != 'Late (31-120 days)']\ndf = df[df.loan_status != 'Does not meet the credit policy. Status:Fully Paid']\ndf = df[df.loan_status != 'Does not meet the credit policy. Status:Charged Off']\ndf = df[df.loan_status != 'Issued']\ndf['loan_status'] = df['loan_status'].replace({'Charged Off':'Default'})\ndf['loan_status'] = df['loan_status'].replace({'Default':1})\ndf['loan_status'] = df['loan_status'].replace({'Fully Paid':0})\ndf['loan_status'].value_counts()","f476ec95":"# Cross table of the purpose and status\nprint(pd.crosstab(df['purpose'], df['loan_status'], margins = True))","1791f866":"# Cross table of home ownership, loan status, and employee length\nprint(pd.crosstab(df['emp_length'],[df['loan_status'],df['home_ownership']]))","c4c151f3":"# Cross table of home ownership, loan status, and average debt to income\nprint(pd.crosstab(df['home_ownership'], df['loan_status'],\n                  values=df['dti'], aggfunc='mean'))","11a893c0":"# Box plot of debt to income by loan status\ndf.boxplot(column = ['dti'], by = 'loan_status')\nplt.title('Average Debt to Income by Loan Status')\nplt.suptitle('')\nplt.show()","26245e80":"# Set up bins for numeric variables\namnt_bins = [0,500,1000,10000,20000,30000,40000]\ndf['loan_amnt_binned'] = pd.cut(df['loan_amnt'],amnt_bins)\nincome_bins = [0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000,500000,1000000]\ndf['annual_inc_binned'] = pd.cut(df['annual_inc'],income_bins)\ndti_bins = [0,10,20,30,40,50,60,70,80,90,100]\ndf['dti_binned'] = pd.cut(df['dti'],dti_bins)","f3529148":"# Drop continuous columns for parsimony\ndel df['loan_amnt']\ndel df['annual_inc']\ndel df['dti']","128f9845":"# Create Dummy Variables\ndf_num = df.select_dtypes(exclude=['object','category'])\ndf_str = df.select_dtypes(include=['object','category'])\n\ndf_onehot = pd.get_dummies(df_str)\n\ndf_prep = pd.concat([df_num, df_onehot], axis=1)","83885fda":"# Function to clean infite and null values\ndef clean_dataset(df):\n    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n    df.dropna(inplace=True)\n    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n    return df[indices_to_keep]","e53e96e0":"# WOE calculation\ndf_att = df.drop('loan_status',axis=1)\ndf_WOE = pd.DataFrame(columns=['Attribute','WOE'])\ndf_IV = pd.DataFrame(columns=['Variable','IV'])\n\nfor col in df_att.columns:\n    Group = df_att[col].value_counts()\n    df2 = pd.DataFrame(Group)\n    df2 = df2.reset_index()\n    df2.columns = ['Group', 'counts']\n\n    Bad = pd.DataFrame (df_att[col][df.loan_status == 1].value_counts())\n    Bad = Bad.reset_index()\n    Bad.columns = ['Group', 'bad']\n\n    df2['bad'] = Bad['bad']\n    df2['good'] = df2['counts']-df2['bad']\n    df2['total_distri'] = df2.counts\/sum(df2.counts)\n    df2['bad_distri'] = df2.bad\/sum(df2.bad)\n    df2['good_distri'] = df2.good\/sum(df2.good)\n    df2['WOE'] = np.log(df2.good_distri \/ df2.bad_distri)*100\n    df2 = clean_dataset(df2)\n    IV = sum ((df2.good_distri - df2.bad_distri)*(df2.WOE\/100))\n    df_IV = df_IV.append({'Variable':col,'IV':IV},ignore_index=True)\n    df3 = df2[['Group','WOE']]\n    df3['Group'] = df3['Group'].astype(str)\n    \n    for ind in df3.index:\n        df_WOE = df_WOE.append({'Attribute': str(col)+'_'+str(df3['Group'].loc[ind]),'WOE':df3['WOE'].loc[ind]}, ignore_index=True)","c31a20f0":"# Replacing 1 for WOE in data frame\nfor i in range (0,54):\n        df_prep[str(df_WOE.iloc[i,0])] = df_prep[str(df_WOE.iloc[i,0])].replace(1,df_WOE.iloc[i,1])","9d77edd3":"# Logistic Regression\ny = df_prep['loan_status']\nX = df_prep.drop('loan_status',axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\ndf_logistic = LogisticRegression(solver='lbfgs',max_iter=1000).fit(X_train, np.ravel(y_train))","f31f4a60":"# Predicting Probability of Default and finding max and min values\npreds = df_logistic.predict_proba(X_test)\npreds_df = pd.DataFrame(preds[:,1], columns = ['prob_default'])\nprint (preds_df['prob_default'].max())\nprint (preds_df['prob_default'].min())","f1460494":"# Default Probabilities for Bins\npreds_df.sort_values(by=['prob_default'])\npreds_df['Bins'] = pd.cut(preds_df['prob_default'],10)\npreds_df_avg = pd.DataFrame(preds_df.groupby(['Bins']).mean())\nprint (preds_df_avg)","d6a9571e":"# Model Performance\npreds = df_logistic.predict_proba(X_test)\n\npreds_df = pd.DataFrame(preds[:,1], columns = ['prob_default'])\n\npreds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.50 else 0) # This threshold will be examined later\n\nprint(preds_df['loan_status'].value_counts())\n\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, preds_df['loan_status'], target_names=target_names))","5cb6347b":"# Accuracy score of the model\nprint(df_logistic.score(X_test, y_test))","c765459c":"# ROC curve of the probabilities of default\nprob_default = preds[:, 1]\nfallout, sensitivity, thresholds = roc_curve(y_test, prob_default)\nplt.plot(fallout, sensitivity, color = 'darkorange')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.show()","500ed9f5":"# AUC\nauc = roc_auc_score(y_test, prob_default)\nprint (auc)","842f99d3":"# Confusion Matrices\nthresholds = [0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55]\nfor i in thresholds:\n    preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > i else 0)\n    print (i)\n    print(confusion_matrix(y_test,preds_df['loan_status']))","1054de77":"# Threshold Plot\ndefault_recall = []\nnondefault_recall = []\naccuracy = []\nfor i in thresholds:\n    preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > i else 0)\n    default_recall.append(precision_recall_fscore_support(y_test,preds_df['loan_status'])[1][1])\n    nondefault_recall.append(precision_recall_fscore_support(y_test,preds_df['loan_status'])[0][1])\n    report = classification_report(y_test,preds_df['loan_status'],output_dict=True)\n    accuracy.append(report['accuracy'])\nplt.plot(thresholds,default_recall)\nplt.plot(thresholds,nondefault_recall)\nplt.plot(thresholds,accuracy)\nplt.xlabel(\"Probability Threshold\")\nplt.legend([\"Default Recall\",\"Non-default Recall\",\"Model Accuracy\"])\nplt.show()","9a6aa365":"# 0.20 is the probability that better fits this model\npreds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.20 else 0)\nprint(preds_df['loan_status'].value_counts())\n\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, preds_df['loan_status'], target_names=target_names))","49a87f3a":"# Balancing Proportions of Defaults and Non Defaults\nX_y_train = pd.concat([X_train.reset_index(drop = True),\n                       y_train.reset_index(drop = True)], axis = 1)\ncount_nondefault, count_default = X_y_train['loan_status'].value_counts()\n\nnondefaults = X_y_train[X_y_train['loan_status'] == 0]\ndefaults = X_y_train[X_y_train['loan_status'] == 1]\n\nnondefaults_under = nondefaults.sample(count_default)\n\nX_y_train_under = pd.concat([nondefaults_under.reset_index(drop = True),\n                             defaults.reset_index(drop = True)], axis = 0)\n\nprint(X_y_train_under['loan_status'].value_counts())","1c9888d1":"# Logistic Regression\ny_train_under = X_y_train_under['loan_status']\nX_train_under = X_y_train_under.drop('loan_status',axis=1)\n\ndf_logistic_under = LogisticRegression(solver='lbfgs',max_iter=1000).fit(X_train_under, np.ravel(y_train_under))\n\nprint (df_logistic_under.intercept_)","7ff03df3":"# Predicting Probability of Default and finding max and min values\npreds_under = df_logistic_under.predict_proba(X_test)\npreds_df_under = pd.DataFrame(preds_under[:,1], columns = ['prob_default'])\nprint (preds_df_under['prob_default'].max())\nprint (preds_df_under['prob_default'].min())","2eb99a92":"# Default Probabilities for Bins\npreds_df_under.sort_values(by=['prob_default'])\npreds_df_under['Bins'] = pd.cut(preds_df_under['prob_default'],10)\npreds_df_under_avg = pd.DataFrame(preds_df_under.groupby(['Bins']).mean())\nprint (preds_df_under_avg)","6bd26b4b":"# IV Analysis to decide which predictors to keep\nprint (df_IV)","701d98c0":"# We must keep those superior to 0.02, so employee length and home ownership cannot be used\ndf_wo_IV = df.drop(['emp_length','home_ownership'],axis=1)","a5354e96":"# Create Dummy Variables\ndf_num_IV = df_wo_IV.select_dtypes(exclude=['object','category'])\ndf_str_IV = df_wo_IV.select_dtypes(include=['object','category'])\n\ndf_onehot_IV = pd.get_dummies(df_str_IV)\n\ndf_prep_IV = pd.concat([df_num_IV, df_onehot_IV], axis=1)","bb5f80d6":"# Replacing 1 for WOE in data frame (only values for the remaining variables)\nfor i in range (14,53):\n        df_prep_IV[str(df_WOE.iloc[i,0])] = df_prep_IV[str(df_WOE.iloc[i,0])].replace(1,df_WOE.iloc[i,1])","af672252":"# Logistic Regression\ny_IV = df_prep_IV['loan_status']\nX_IV = df_prep_IV.drop('loan_status',axis=1)\n\nX_train_IV, X_test_IV, y_train_IV, y_test_IV = train_test_split(X_IV, y_IV, test_size=0.3, random_state=123)\ndf_logistic_IV = LogisticRegression(solver='lbfgs',max_iter=1000).fit(X_train_IV, np.ravel(y_train_IV))\n\nprint (df_logistic_IV.intercept_)","0361ffbe":"# Predicting Probability of Default and finding max and min values\npreds_IV = df_logistic_IV.predict_proba(X_test_IV)\npreds_df_IV = pd.DataFrame(preds_IV[:,1], columns = ['prob_default'])\nprint (preds_df_IV['prob_default'].max())\nprint (preds_df_IV['prob_default'].min())","292bb13f":"# Default Probabilities for Bins\npreds_df_IV.sort_values(by=['prob_default'])\npreds_df_IV['Bins'] = pd.cut(preds_df_IV['prob_default'],10)\npreds_df_IV_avg = pd.DataFrame(preds_df_IV.groupby(['Bins']).mean())\nprint (preds_df_IV_avg)","0185c1ee":"# Original Sample\nX_train_gbt = X_train\ny_train_gbt = y_train\nX_test_gbt = X_test\ny_test_gbt = y_test","89b2d2c3":"# Replace ] by ) since XGBoost has problems with ], < and ,\nfor col in X_train_gbt.columns:\n    X_train_gbt = X_train_gbt.rename({col:col.replace(\"]\",\")\")}, axis=1)\n    X_train_gbt = X_train_gbt.rename({col:col.replace(\",\",\" \")}, axis=1)\n    X_train_gbt = X_train_gbt.rename({col:col.replace(\"<\",\"less than\")}, axis=1)\nfor col in X_test_gbt.columns:\n    X_test_gbt = X_test_gbt.rename({col:col.replace(\"]\",\")\")}, axis=1)\n    X_test_gbt = X_test_gbt.rename({col:col.replace(\",\",\" \")}, axis=1)\n    X_test_gbt = X_test_gbt.rename({col:col.replace(\"<\",\"less than\")}, axis=1)","7b329100":"# Model Training\ngbt = xgb.XGBClassifier(booster='gblinear',\n                      objective='binary:logistic', \n                      n_estimators=100,\n                      max_depth=6).fit(X_train_gbt, np.ravel(y_train_gbt))","ee9c06b9":"# Predicting Probability of Default\ngbt_preds = gbt.predict_proba(X_test_gbt)\npreds_gbt_df = pd.DataFrame(gbt_preds[:,1], columns = ['prob_default'])\nprint(preds_gbt_df['prob_default'].min())\nprint(preds_gbt_df['prob_default'].max()) ","935164b9":"# Default Probabilities for Bins\npreds_gbt_df.sort_values(by=['prob_default'])\npreds_gbt_df['Bins'] = pd.cut(preds_gbt_df['prob_default'],10)\npreds_gbt_df_avg = pd.DataFrame(preds_gbt_df.groupby(['Bins']).mean())\nprint (preds_gbt_df_avg)","ca90d482":"# Undersampled sets\nX_train_gbt_under = X_train_under\ny_train_gbt_under = y_train_under","5657c7f4":"# Replace ] by ) since XGBoost has problems with ], < and ,\nfor col in X_train_gbt_under.columns:\n    X_train_gbt_under = X_train_gbt_under.rename({col:col.replace(\"]\",\")\")}, axis=1)\n    X_train_gbt_under = X_train_gbt_under.rename({col:col.replace(\",\",\" \")}, axis=1)\n    X_train_gbt_under = X_train_gbt_under.rename({col:col.replace(\"<\",\"less than\")}, axis=1)","f1675410":"# Model Training\ngbt_under = xgb.XGBClassifier(booster='gblinear',\n                      objective='binary:logistic', \n                      n_estimators=100,\n                      max_depth=6).fit(X_train_gbt_under, np.ravel(y_train_gbt_under))","e38858c7":"# Predicting Probability of Default\ngbt_under_preds = gbt_under.predict_proba(X_test_gbt)\npreds_gbt_under_df = pd.DataFrame(gbt_under_preds[:,1], columns = ['prob_default'])\nprint(preds_gbt_under_df['prob_default'].min())\nprint(preds_gbt_under_df['prob_default'].max())","e82f0e14":"# Default Probabilities for Bins\npreds_gbt_under_df.sort_values(by=['prob_default'])\npreds_gbt_under_df['Bins'] = pd.cut(preds_gbt_under_df['prob_default'],10)\npreds_gbt_under_df_avg = pd.DataFrame(preds_gbt_under_df.groupby(['Bins']).mean())\nprint (preds_gbt_under_df_avg)","7f713002":"# IV sets\nX_train_gbt_IV = X_train_IV\ny_train_gbt_IV = y_train_IV\nX_test_gbt_IV = X_test_IV\ny_test_gbt_IV = y_test_IV","414fbf31":"# Replace ] by ) since XGBoost has problems with ] and ,\nfor col in X_train_gbt_IV.columns:\n    X_train_gbt_IV = X_train_gbt_IV.rename({col:col.replace(\"]\",\")\")}, axis=1)\n    X_train_gbt_IV = X_train_gbt_IV.rename({col:col.replace(\",\",\" \")}, axis=1)\nfor col in X_test_gbt_IV.columns:\n    X_test_gbt_IV = X_test_gbt_IV.rename({col:col.replace(\"]\",\")\")}, axis=1)\n    X_test_gbt_IV = X_test_gbt_IV.rename({col:col.replace(\",\",\" \")}, axis=1)","abf5c243":"# Model Training\ngbt_IV = xgb.XGBClassifier(booster='gblinear',\n                      objective='binary:logistic', \n                      n_estimators=100,\n                      max_depth=6).fit(X_train_gbt_IV, np.ravel(y_train_gbt_IV))","83225ad4":"# Predicting Probability of Default\ngbt_preds_IV = gbt_IV.predict_proba(X_test_gbt_IV)\npreds_gbt_IV_df = pd.DataFrame(gbt_preds_IV[:,1], columns = ['prob_default'])\nprint(preds_gbt_IV_df['prob_default'].min())\nprint(preds_gbt_IV_df['prob_default'].max()) ","3a22a491":"# Default Probabilities for Bins\npreds_gbt_IV_df.sort_values(by=['prob_default'])\npreds_gbt_IV_df['Bins'] = pd.cut(preds_gbt_IV_df['prob_default'],10)\npreds_gbt_IV_df_avg = pd.DataFrame(preds_gbt_IV_df.groupby(['Bins']).mean())\nprint (preds_gbt_IV_df_avg)","25c9c672":"# LogReg\ndf_columns = pd.DataFrame (list (X.columns),columns=['Attribute'])\ndf_coef = pd.DataFrame(df_logistic.coef_).transpose()\ndf_attr_coeff = pd.concat([df_columns, df_coef], axis=1, ignore_index=True)\ndf_attr_coeff.columns = ['Attribute','Coeff']\ndf_attr_coeff.head(5)\ndf_final = pd.merge(df_WOE, df_attr_coeff, left_on='Attribute', right_on='Attribute', how='left', sort=False)\ndf_final.to_csv('Credit_Scorecard.csv', sep=';')\nprint (df_logistic.intercept_)","69e6547c":"# LogReg Under\ndf_columns_under = pd.DataFrame (list (X_train.columns),columns=['Attribute'])\ndf_coef_under = pd.DataFrame(df_logistic_under.coef_).transpose()\ndf_attr_coeff_under = pd.concat([df_columns_under, df_coef_under], axis=1, ignore_index=True)\ndf_attr_coeff_under.columns = ['Attribute','Coeff']\ndf_final_under = pd.merge(df_WOE, df_attr_coeff_under, left_on='Attribute', right_on='Attribute', how='left', sort=False)\ndf_final_under.to_csv('Credit_Scorecard_under.csv', sep=';')\nprint (df_logistic_under.intercept_)","854047f7":"# LogReg IV\ndf_columns_IV = pd.DataFrame (list (X_train_IV.columns),columns=['Attribute'])\ndf_coef_IV = pd.DataFrame(df_logistic_IV.coef_).transpose()\ndf_attr_coeff_IV = pd.concat([df_columns_IV, df_coef_IV], axis=1, ignore_index=True)\ndf_attr_coeff_IV.columns = ['Attribute','Coeff']\ndf_final_IV = pd.merge(df_WOE, df_attr_coeff_IV, left_on='Attribute', right_on='Attribute', how='inner', sort=False)\ndf_final_IV.to_csv('Credit_Scorecard_IV.csv', sep=';')\nprint (df_logistic_IV.intercept_)","641858b2":"# XGBoost\ndf_columns_gbt = pd.DataFrame (list (X_train.columns),columns=['Attribute']) # Columns in gbt set were altered\ndf_coef_gbt = pd.DataFrame(gbt.coef_)\ndf_attr_coeff_gbt = pd.concat([df_columns_gbt, df_coef_gbt], axis=1, ignore_index=True)\ndf_attr_coeff_gbt.columns = ['Attribute','Coeff']\ndf_final_gbt = pd.merge(df_WOE, df_attr_coeff_gbt, left_on='Attribute', right_on='Attribute', how='inner', sort=False)\ndf_final_gbt.to_csv('Credit_Scorecard_gbt.csv', sep=';')\nprint (gbt.intercept_)","f63b0167":"# XGBoost Under\ndf_columns_gbt_under = pd.DataFrame (list (X_train.columns),columns=['Attribute']) # Columns in gbt set were altered\ndf_coef_gbt_under = pd.DataFrame(gbt_under.coef_)\ndf_attr_coeff_gbt_under = pd.concat([df_columns_gbt_under, df_coef_gbt_under], axis=1, ignore_index=True)\ndf_attr_coeff_gbt_under.columns = ['Attribute','Coeff']\ndf_final_gbt_under = pd.merge(df_WOE, df_attr_coeff_gbt_under, left_on='Attribute', right_on='Attribute', how='inner', sort=False)\ndf_final_gbt_under.to_csv('Credit_Scorecard_gbt_under.csv', sep=';')\nprint (gbt_under.intercept_)","99bc9024":"# XGBoost IV\ndf_columns_gbt_IV = pd.DataFrame (list (X_train_IV.columns),columns=['Attribute']) # Columns in gbt set were altered\ndf_coef_gbt_IV = pd.DataFrame(gbt_IV.coef_)\ndf_attr_coeff_gbt_IV = pd.concat([df_columns_gbt_IV, df_coef_gbt_IV], axis=1, ignore_index=True)\ndf_attr_coeff_gbt_IV.columns = ['Attribute','Coeff']\ndf_final_gbt_IV = pd.merge(df_WOE, df_attr_coeff_gbt_IV, left_on='Attribute', right_on='Attribute', how='inner', sort=False)\ndf_final_gbt_IV.to_csv('Credit_Scorecard_gbt_IV.csv', sep=';')\nprint (gbt_IV.intercept_)","459e8d3e":"# Information Value Analysis","9351130d":"# **Exploration and Preparing Data**","cc5afdf6":"# CSVs Export","9126ba9c":"# Model Training - Logistic Regressions","5fdff0a3":"# WOE Calculation and Replacement","cdc0e213":"XGBoost with IV-approved variables only","8e06c589":"# Undersampling Training Data","9b8edf25":"XGBoost with Undersampled Training Data Set","d9aa177c":"> *Since the Model Performance isn't great, we'll try some alternatives to improve it*","23f0f005":"# Model Performance","35b7d4c7":"# Model Training - Gradient Boosted Trees","6f78c19d":"XGBoost with Original Training Data Set"}}