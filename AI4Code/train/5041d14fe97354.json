{"cell_type":{"b6887497":"code","fc577882":"code","3053405a":"code","c2bc4474":"code","8a1b3df0":"code","4ae8ff94":"code","a8fa3f21":"code","517ae77a":"code","d55e85b6":"code","3b338d22":"code","069b55d1":"code","572b8163":"code","6c0c08be":"code","8777ed3c":"code","4beea1c2":"code","d25db505":"code","7f893212":"code","d1b253d6":"code","bcbdff81":"code","2133e6e3":"code","96cb56f5":"code","2d2c723d":"markdown","36c03859":"markdown","8a8797e3":"markdown","b2a11ffd":"markdown","add8ccc3":"markdown"},"source":{"b6887497":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport torch\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nimport matplotlib.patches as patches\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc577882":"\ndef generate_box(obj):\n    \n    xmin = int(obj.find('xmin').text)\n    ymin = int(obj.find('ymin').text)\n    xmax = int(obj.find('xmax').text)\n    ymax = int(obj.find('ymax').text)\n    \n    return [xmin, ymin, xmax, ymax]\n\ndef generate_label(obj):\n    if obj.find('name').text == \"with_mask\":\n        return 1\n    elif obj.find('name').text == \"mask_weared_incorrect\":\n        return 2\n    return 0\n\ndef generate_target(image_id, file): \n    with open(file) as f:\n        data = f.read()\n        soup = BeautifulSoup(data, 'xml')\n        objects = soup.find_all('object')\n\n        num_objs = len(objects)\n\n        # Bounding boxes for objects\n        # In coco format, bbox = [xmin, ymin, width, height]\n        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n        boxes = []\n        labels = []\n        for i in objects:\n            boxes.append(generate_box(i))\n            labels.append(generate_label(i))\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # Labels (In my case, I only one class: target class or background)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        # Tensorise img_id\n        img_id = torch.tensor([image_id])\n        # Annotation is in dictionary format\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = img_id\n        \n        return target\n","3053405a":"imgs = list(sorted(os.listdir(\"\/kaggle\/input\/face-mask-detection\/images\/\")))","c2bc4474":"labels = list(sorted(os.listdir(\"\/kaggle\/input\/face-mask-detection\/annotations\/\")))","8a1b3df0":"class MaskDataset(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(\"\/kaggle\/input\/face-mask-detection\/images\/\")))\n#         self.labels = list(sorted(os.listdir(\"\/kaggle\/input\/face-mask-detection\/annotations\/\")))\n\n    def __getitem__(self, idx):\n        # load images ad masks\n        file_image = 'maksssksksss'+ str(idx) + '.png'\n        file_label = 'maksssksksss'+ str(idx) + '.xml'\n        img_path = os.path.join(\"\/kaggle\/input\/face-mask-detection\/images\/\", file_image)\n        label_path = os.path.join(\"\/kaggle\/input\/face-mask-detection\/annotations\/\", file_label)\n        img = Image.open(img_path).convert(\"RGB\")\n        #Generate Label\n        target = generate_target(idx, label_path)\n        \n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)","4ae8ff94":"data_transform = transforms.Compose([\n        transforms.ToTensor(), \n    ])","a8fa3f21":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndataset = MaskDataset(data_transform)\ndata_loader = torch.utils.data.DataLoader(\n dataset, batch_size=4, collate_fn=collate_fn)","517ae77a":"torch.cuda.is_available()","d55e85b6":"def get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model","3b338d22":"model = get_model_instance_segmentation(3)","069b55d1":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nfor imgs, annotations in data_loader:\n    imgs = list(img.to(device) for img in imgs)\n    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n    print(annotations)\n    break","572b8163":"\nnum_epochs = 25\nmodel.to(device)\n    \n# parameters\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                                momentum=0.9, weight_decay=0.0005)\n\nlen_dataloader = len(data_loader)\n\nfor epoch in range(num_epochs):\n    model.train()\n    i = 0    \n    epoch_loss = 0\n    for imgs, annotations in data_loader:\n        i += 1\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        loss_dict = model([imgs[0]], [annotations[0]])\n        losses = sum(loss for loss in loss_dict.values())        \n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step() \n#         print(f'Iteration: {i}\/{len_dataloader}, Loss: {losses}')\n        epoch_loss += losses\n    print(epoch_loss)\n","6c0c08be":"for imgs, annotations in data_loader:\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        break","8777ed3c":"model.eval()\npreds = model(imgs)\npreds","4beea1c2":"def plot_image(img_tensor, annotation):\n    \n    fig,ax = plt.subplots(1)\n    img = img_tensor.cpu().data\n\n    # Display the image\n    ax.imshow(img.permute(1, 2, 0))\n    \n    for box in annotation[\"boxes\"]:\n        xmin, ymin, xmax, ymax = box\n\n        # Create a Rectangle patch\n        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n\n    plt.show()","d25db505":"print(\"Prediction\")\nplot_image(imgs[2], preds[2])\nprint(\"Target\")\nplot_image(imgs[2], annotations[2])","7f893212":"torch.save(model.state_dict(),'model.pt')","d1b253d6":"model2 = get_model_instance_segmentation(3)","bcbdff81":"model2.load_state_dict(torch.load('model.pt'))\nmodel2.eval()\nmodel2.to(device)","2133e6e3":"pred2 = model2(imgs)","96cb56f5":"print(\"Predict with loaded model\")\nplot_image(imgs[3], pred2[3])","2d2c723d":"# Load Model","36c03859":"# Save Model","8a8797e3":"# Model","b2a11ffd":"# Function to plot image","add8ccc3":"# Train Model"}}