{"cell_type":{"04f592fe":"code","ab4b0983":"code","45dbf4c6":"code","fdd3ad83":"code","0ff835bc":"code","41c4b132":"code","92c5bff6":"code","2d9cc5a8":"code","a3df6075":"code","befc3e66":"code","945edf8d":"code","895aea62":"code","3ffb9244":"code","524f852c":"code","e5f9fc6d":"code","a6e4ed9a":"code","a546e912":"code","ad053913":"code","7c9bae8a":"code","46e9e6a3":"code","bc712ba1":"code","547a4d49":"code","3616a0b7":"markdown","e55c5545":"markdown","19609a2f":"markdown","a3de1dc1":"markdown"},"source":{"04f592fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.ticker import MultipleLocator\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master\/')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab4b0983":"train_features = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_features.csv\")\ntrain_targets_scored = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_targets_scored.csv\")\ntrain_targets_nonscored = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv\")\ntest_features = pd.read_csv(\"\/kaggle\/input\/lish-moa\/test_features.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/lish-moa\/sample_submission.csv\")","45dbf4c6":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","fdd3ad83":"def add_stats(df, columns, prefix):\n    df[prefix + '_mean'] = columns.mean(axis=1)\n    df[prefix + '_sum'] = columns.sum(axis=1)\n    df[prefix + '_std'] = columns.std(axis=1)\n    df[prefix + '_kurt'] = columns.kurtosis(axis = 1)\n    df[prefix + '_skew'] = columns.skew(axis = 1)","0ff835bc":"train_full = train_features.merge(train_targets_scored, on='sig_id')\ntrain_full = train_full[train_full.cp_type != 'ctl_vehicle']\ntrain_full = train_full.drop(columns=['sig_id', 'cp_type'])\ntrain_full['cp_dose'] = pd.get_dummies(train_full['cp_dose'], drop_first=True)\n\nX_train = train_full.iloc[:, :874]\ny_train = train_full.iloc[:, 874:]\n\nX_test = test_features\nX_test = X_test[X_test.cp_type != 'ctl_vehicle']\nX_test = X_test.drop(columns=['sig_id', 'cp_type'])\nX_test['cp_dose'] = pd.get_dummies(X_test['cp_dose'], drop_first=True)\n\nadd_stats(X_train, X_train.iloc[:, 2:774], 'g')\nadd_stats(X_train, X_train.iloc[:, 774:874], 'c')\nadd_stats(X_train, X_train.iloc[:, 2:874], 'gc')\n                    \nadd_stats(X_test, X_test.iloc[:, 2:774], 'g')\nadd_stats(X_test, X_test.iloc[:, 774:874], 'c')\nadd_stats(X_test, X_test.iloc[:, 2:874], 'gc')\n\nX_full = pd.concat([X_train, X_test])","41c4b132":"def pca_analyse(columns, num_comp, scaler=None):\n    pca = PCA(n_components=num_comp)\n    if scaler is not None:\n        columns = scaler.fit_transform(columns)\n    pca.fit(columns)\n    return np.cumsum(pca.explained_variance_ratio_)","92c5bff6":"pca_nonscaled = pca_analyse(X_full.iloc[:, 2:774], 700)\npca_stand = pca_analyse(X_full.iloc[:, 2:774], 700, StandardScaler())\npca_quant = pca_analyse(X_full.iloc[:, 2:774], 700, QuantileTransformer(output_distribution=\"normal\"))","2d9cc5a8":"fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(1, 1, 1)\n\nplt.plot(pca_nonscaled, label='nonscaled')\nplt.plot(pca_stand, label='stand')\nplt.plot(pca_quant, label='quant')\nax.yaxis.set_major_locator(MultipleLocator(0.05))\nax.grid(which='major')\nax.set_xlabel(\"\u0427\u0438\u0441\u043b\u043e \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\")\nax.set_ylabel(\"\u0414\u043e\u043b\u044f \u043e\u0431\u044a\u044f\u0441\u043d\u0435\u043d\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438\")\nax.set_title(\"g- \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\")\nplt.legend()","a3df6075":"pca_nonscaled = pca_analyse(X_full.iloc[:, 774:874], 100)\npca_stand = pca_analyse(X_full.iloc[:, 774:874], 100, StandardScaler())\npca_quant = pca_analyse(X_full.iloc[:, 774:874], 100, QuantileTransformer(output_distribution=\"normal\"))","befc3e66":"fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(1, 1, 1)\n\nplt.plot(pca_nonscaled, label='nonscaled')\nplt.plot(pca_stand, label='stand')\nplt.plot(pca_quant, label='quant')\nax.yaxis.set_major_locator(MultipleLocator(0.05))\nax.grid(which='major')\nax.set_xlabel(\"\u0427\u0438\u0441\u043b\u043e \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\")\nax.set_ylabel(\"\u0414\u043e\u043b\u044f \u043e\u0431\u044a\u044f\u0441\u043d\u0435\u043d\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438\")\nax.set_title(\"\u0441- \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\")\nplt.legend()","945edf8d":"def add_pca(df, columns, num_comp, prefix, scaler):\n    pca = PCA(n_components=num_comp)\n    if scaler is not None:\n        columns = scaler.fit_transform(columns)\n    comps = pca.fit_transform(columns)\n    pca_df = pd.DataFrame(comps, \n                          columns=[prefix + f\"_pca_{i}\".format(i) for i in range(num_comp)])\n    return pd.concat((df.reset_index(drop=True), pca_df.reset_index(drop=True)), axis=1)\n    ","895aea62":"def variance_threshold(df, columns, thr=0.85):\n    vt = VarianceThreshold(threshold=thr)\n    var = vt.fit(columns).variances_\n    \n    drop_cols = columns.columns[var < thr]\n    return df.drop(columns=drop_cols)","3ffb9244":"train_full = train_features.merge(train_targets_scored, on='sig_id')\n\nX_full = add_pca(X_full, X_full.iloc[:, 2:774], 600, 'g', \n                 QuantileTransformer(output_distribution=\"normal\"))\nX_full = add_pca(X_full, X_full.iloc[:, 774:874], 80, 'c', \n                 QuantileTransformer(output_distribution=\"normal\"))\n\nX_full = variance_threshold(X_full, X_full.drop(columns=['cp_time', 'cp_dose']), thr=0.9)\n\nX_train = X_full[:X_train.shape[0]]\nX_test = X_full[-X_test.shape[0]:]","524f852c":"X_train.shape","e5f9fc6d":"class MoaDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = X\n        self.y = y\n        self.device = device\n    \n    def __getitem__(self, idx):\n        if self.y is None:\n            return torch.tensor(self.X[idx], dtype=torch.float)\n        \n        return torch.tensor(self.X[idx], dtype=torch.float),  \\\n                   torch.tensor(self.y[idx], dtype=torch.float)\n    \n    def __len__(self):\n        return self.X.shape[0]\n        ","a6e4ed9a":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n        \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","a546e912":"def valid_epoch(model, criterion, dataloader, device):\n    model.eval()\n    \n    with torch.no_grad():\n        total_loss = 0.\n        total_num = 0\n        for X, y in dataloader:\n            X = X.to(device)\n            y = y.to(device)\n            outputs = model(X)\n            loss = criterion(outputs, y)\n            total_loss += loss.item() * X.shape[0]\n            total_num += X.shape[0]\n        \n        return total_loss \/ total_num\n\ndef train_epoch(model, criterion, optimizer, scheduler, train_dataloader, valid_dataloader, device):\n    model.train()\n    total_loss = 0.\n    total_num = 0\n    for X, y in train_dataloader:\n        X = X.to(device)\n        y = y.to(device)\n        optimizer.zero_grad()\n        \n        outputs = model(X)\n        loss = criterion(outputs, y)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += loss.detach().item() * X.shape[0]\n        total_num += X.shape[0]\n        \n    train_loss = total_loss \/ total_num\n        \n    val_loss = valid_epoch(model, criterion, val_dataloader, device)\n    return train_loss, val_loss\n        \ndef predict(model, dataset, device):\n    model.eval()\n    result = []\n    with torch.no_grad():\n        for elem in dataset:\n            elem = elem.unsqueeze(0).to(device)\n            output = model(elem)\n            result.append(torch.sigmoid(output).cpu().numpy())\n    \n    return np.concatenate(result)","ad053913":"device = 'cuda'\ntrain_loss_list = []\nval_loss_list = []\npred_list = []\ntest_dataset = MoaDataset(X_test.values)\nnum_features = X_train.shape[1]\nnum_targets = y_train.shape[1]\n\ncriterion = torch.nn.BCEWithLogitsLoss()\nseed_list = np.arange(7)\n\nfor seed in seed_list:\n    seed_everything(seed)\n    mskf = MultilabelStratifiedKFold(n_splits=7, shuffle=True, random_state=seed)\n    for fold_ind, (train_idx, val_idx) in enumerate(mskf.split(X_train, y_train)):\n        curr_train_loss_list = []\n        curr_val_loss_list = []\n\n        train_dataset = MoaDataset(X_train.values[train_idx], y_train.values[train_idx])\n        train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=1)\n\n        val_dataset = MoaDataset(X_train.values[val_idx], y_train.values[val_idx])\n        val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=True, num_workers=1)\n\n        model = Model(num_features, num_targets, 1024).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n        # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=3, verbose=True)\n        # scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=5, gamma=0.1)\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                                  max_lr=1e-2, epochs=25, steps_per_epoch=len(train_dataloader))\n\n\n        for epoch_num in range(25):\n            train_loss, val_loss = train_epoch(model, criterion, optimizer, scheduler, \n                                               train_dataloader, val_dataloader, device)\n\n            curr_train_loss_list.append(train_loss)\n            curr_val_loss_list.append(val_loss)\n\n            print('seed: ', seed, ' fold: ', fold_ind, ' epoch_num: ', epoch_num, ' train loss: ', train_loss)\n            print('seed: ', seed, ' fold: ', fold_ind, ' epoch_num: ', epoch_num, ' val loss: ', val_loss)\n\n        train_loss_list.append(curr_train_loss_list)\n        val_loss_list.append(curr_val_loss_list)\n\n        pred_list.append(predict(model, test_dataset, device))\n        \n    \n    \n    ","7c9bae8a":"print(\"curr_train_loss: \", sum([elem[-1] for elem in train_loss_list]) \/ len(train_loss_list))\nprint(\"curr_val_loss: \", sum([elem[-1] for elem in val_loss_list]) \/ len(val_loss_list))","46e9e6a3":"y_test = np.zeros((sample_submission.shape[0], y_train.shape[1]))\ny_test[test_features.cp_type != 'ctl_vehicle'] = sum(pred_list) \/ len(pred_list)\n","bc712ba1":"submission = pd.DataFrame(y_test, columns=train_targets_scored.columns[1:])\nsubmission['sig_id'] = test_features['sig_id']","547a4d49":"submission.to_csv('submission.csv', index=False)","3616a0b7":"PCA for g- features","e55c5545":"PCA for \u0441- features","19609a2f":"\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c QuantileTransformer \u0441 num_comp=600, \u0447\u0442\u043e\u0431\u044b \u0434\u043e\u043b\u044f \u043e\u0431\u044a\u044f\u0441\u043d\u0435\u043d\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u0440\u0430\u0432\u043d\u044f\u043b\u0430\u0441\u044c 0.95","a3de1dc1":"\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c QuantileTransformer \u0441 num_comp=80, \u0447\u0442\u043e\u0431\u044b \u0434\u043e\u043b\u044f \u043e\u0431\u044a\u044f\u0441\u043d\u0435\u043d\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u0440\u0430\u0432\u043d\u044f\u043b\u0430\u0441\u044c 0.95"}}