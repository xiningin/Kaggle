{"cell_type":{"3268b540":"code","5abec75f":"code","2952b4b6":"code","a1c38bba":"code","56cf51ef":"code","aaebd9f6":"code","22ccd913":"code","6630e68a":"code","97a79450":"code","5b6c753e":"code","93183d9d":"code","7f7fe5eb":"code","1d6d39ad":"code","aa1efc96":"code","7c5b210f":"code","0e4152c2":"code","c314bc23":"code","3a98fc90":"code","ff68c5d8":"code","8217e680":"code","ec1d1ecc":"code","a78c5fbe":"code","2fa42152":"code","9b75ab3f":"code","195bbca6":"code","f8c1145f":"code","582be70f":"code","385b6e93":"code","fe6c4102":"code","da866c74":"code","f9c69b41":"code","0731c7a7":"code","d4b2cbfc":"code","b141aef5":"code","61262e11":"code","942f0a31":"code","cdc95ae8":"code","db28e73a":"code","aacd1f6c":"code","9ac5b3f8":"code","fb551127":"code","8fc212d0":"code","0fb0a7ca":"code","92c1a893":"code","011561c1":"markdown","6fb0ceb3":"markdown","2eaedea0":"markdown","6f7e0e5a":"markdown","2849386c":"markdown","cd9128bf":"markdown","d9652368":"markdown","0d2894b8":"markdown","4e60c430":"markdown","e44c9198":"markdown","eff62b7c":"markdown","c7c55870":"markdown","41b9a525":"markdown","4719b378":"markdown","8ad12524":"markdown","f67fa17a":"markdown"},"source":{"3268b540":"# Importando bibliotecas\nimport pandas as pd\nimport numpy as np\nimport string\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('rslp')\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import *\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import WordNetLemmatizer\nfrom scipy.spatial.distance import cosine\nimport networkx as nx\n\n!pip install -q -U ktrain\n!pip install -q -U sentence-transformers\n!pip install -q -U gensim","5abec75f":"# Importa\u00e7\u00e3o dos dados\namazon_data = pd.read_csv('..\/input\/sentimentdataset\/amazon_cells_labelled.txt', header=None, names=['opinion','sentiment'], delimiter='\\t')\nimdb_data = pd.read_csv('..\/input\/nlp-in-python-1\/07\/imdb_labelled.txt', header=None, names=['opinion','sentiment'], delimiter='\\t')\nyelp_data = pd.read_csv('..\/input\/d\/hidngnguyna\/sentimentdataset\/AIMESOFT20181108_dataset.txt', header=None, names=['opinion','sentiment'], delimiter='\\t')","2952b4b6":"# Concatena\u00e7\u00e3o dos dados \ndata = pd.concat([amazon_data, imdb_data, yelp_data], ignore_index=True)\n# Divis\u00e3o em treino e teste\ndata_train = data.sample(2400, random_state=42)\ndata_test = data.loc[set(data.index.values) - set(data_train.index.values), :]","a1c38bba":"data_train.sort_index()","56cf51ef":"data_test.sort_index()","aaebd9f6":"data_train.sample()['opinion']","22ccd913":"# Remo\u00e7\u00e3o de pontua\u00e7\u00e3o, stopwords, palavras de dom\u00ednio e depois aplica\u00e7\u00e3o de lematiza\u00e7\u00e3o\ndef text_preprocess(text, lang, domain_stopwords=[]):\n  \n    stop_words = nltk.corpus.stopwords.words(lang) # lang='portuguese' or lang='english'\n\n    # Transforma em caixa baixa\n    s = str(text).lower() \n    \n    # Remove pontua\u00e7\u00e3o\n    table = str.maketrans({key: None for key in string.punctuation.replace(\"'\", \"\")})\n    s = s.translate(table) \n    s = s.translate(str.maketrans({\"'\" : \" \"}))\n\n    # Obten\u00e7\u00e3o dos tokens\n    tokens = word_tokenize(s) \n\n\n    words = [word for word in tokens if not word in stop_words and not word.isdigit()] # remove stopwords e d\u00edgitos\n\n    # Lemmatizar os tokens\n    lemmatizer = WordNetLemmatizer()\n    words = list(map(lemmatizer.lemmatize, words))\n    domain_stopwords = list(map(lemmatizer.lemmatize, domain_stopwords))\n\n    words = [word for word in words if not word in domain_stopwords] # remove palavras de dom\u00ednio\n\n\n    return \" \".join(words).strip()","6630e68a":"amazon_domain_words = ['movie', 'phone', 'film', 'food', 'place', 'time', 'service', 'product', 'sound', 'headset', 'battery']\nimdb_domain_words = ['story', 'script', 'watching', 'character', 'actor', 'scene', 'person', 'people']\nyelp_domain_words = ['food', 'restaurant', 'vegas', 'eat', 'us', 'chicken', 'pizza', 'salad', 'menu', 'sushi', 'buffet', 'meal', 'burger', 'steak']\ndomain_words = list(set(amazon_domain_words + imdb_domain_words + yelp_domain_words))\nprint(domain_words)","97a79450":"data_train['opinion'] = data_train['opinion'].apply(text_preprocess, args=('english', domain_words))\ndata_test['opinion'] = data_test['opinion'].apply(text_preprocess, args=('english', domain_words))\n\ndata_train.head(10)","5b6c753e":"# Obtendo a bag-of-words\ndef compute_bag_of_words(dataset, lang, domain_stopwords=[]):\n    vectorizer = TfidfVectorizer(max_features=1000)\n    X = vectorizer.fit_transform(dataset['opinion'])\n\n    count_vect_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n\n    return count_vect_df, vectorizer","93183d9d":"bow_train_df, vectorizer = compute_bag_of_words(data_train,'english')\nbow_test_df = pd.DataFrame(vectorizer.transform(data_test['opinion']).todense(), columns=vectorizer.get_feature_names())","7f7fe5eb":"bow_train_df.head()","1d6d39ad":"bow_test_df.head()","aa1efc96":"top_palavras = np.sum(bow_train_df, axis=0).sort_values(ascending=False)\ntop_palavras[:10]","7c5b210f":"from gensim.models import KeyedVectors\n\n!wget -P \/root\/input\/ -c \"https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz\"\nEMBEDDING_FILE = \"\/root\/input\/GoogleNews-vectors-negative300.bin.gz\"\nword2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)","0e4152c2":"doc_embeddings_train = []\nfor index, row in data_train.iterrows():\n  sentence = row['opinion'].split()\n  L = []\n  for token in sentence:\n    try:\n      L.append(word2vec[token])\n    except:\n      1\n  if len(L) > 0: opinion_vec = np.mean(np.array(L),axis=0)\n  else: opinion_vec = np.zeros(300)\n  doc_embeddings_train.append(opinion_vec)\n\nw2v_train_df = pd.DataFrame(np.array(doc_embeddings_train))\n\ndoc_embeddings_test = []\nfor index, row in data_test.iterrows():\n  sentence = row['opinion'].split()\n  L = []\n  for token in sentence:\n    try:\n      L.append(word2vec[token])\n    except:\n      1\n  if len(L) > 0: opinion_vec = np.mean(np.array(L),axis=0)\n  else: opinion_vec = np.zeros(300)\n  doc_embeddings_test.append(opinion_vec)\n  \nw2v_test_df = pd.DataFrame(np.array(doc_embeddings_test))","c314bc23":"w2v_train_df.head()","3a98fc90":"from sentence_transformers import SentenceTransformer\nimport numpy as np\nimport logging\n\nbert_model = SentenceTransformer('distiluse-base-multilingual-cased')","ff68c5d8":"bert_train_df = data_train['opinion'].apply(bert_model.encode)\nbert_test_df = data_test['opinion'].apply(bert_model.encode)\n\nbert_train_df = pd.DataFrame(np.array(list(map(np.array, bert_train_df))))\nbert_test_df = pd.DataFrame(np.array(list(map(np.array, bert_test_df))))","8217e680":"bert_train_df.head()","ec1d1ecc":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","a78c5fbe":"def test_model(model, X_train, y_train, X_test, y_test, score_func=accuracy_score):\n    model.fit(X_train, y_train)\n    train_score = score_func(y_train, model.predict(X_train))\n    test_score = score_func(y_test, model.predict(X_test))\n    return train_score, test_score\n\ndef print_scores(train_score, test_score, score_name='accuracy'):\n    score_name = score_name.lower() \n    print(f'\\tTrain {score_name} = {train_score:.4f}')\n    print(f'\\tTest {score_name} = {test_score:.4f}')","2fa42152":"# Usando hiperpar\u00e2metro de regulariza\u00e7\u00e3o C\nlog_reg = LogisticRegression(C=0.42)\nsvm = SVC()\nmlp = MLPClassifier(max_iter=300)\nrandom_forest = RandomForestClassifier(max_depth=5)\nknn = KNeighborsClassifier()\n\nmodels = [(log_reg, 'Logistic Regression'),\n          (svm, 'Support Vector Machine'),\n          (mlp, 'Multilayer Perceptron'),\n          (random_forest, 'Random Forest'),\n          (knn, 'KNN')]\n\nembeddings = [(bow_train_df, bow_test_df, 'Bag of words'),\n              (w2v_train_df, w2v_test_df, 'Word2Vec'),\n              (bert_train_df, bert_test_df, 'BERT')]\n\ny_train = data_train.iloc[:, -1]\ny_test = data_test.iloc[:, -1]\n\nall_scores = []\n\nfor model in models:\n    for embedding in embeddings:\n        model_scores = test_model(model[0], embedding[0], y_train, embedding[1], y_test)\n        print(f'Scores for {embedding[2]} using {model[1]}')\n        print_scores(*model_scores)\n        all_scores.append((model[1], embedding[2], *model_scores))\n        print()","9b75ab3f":"scores_df = pd.DataFrame(all_scores, columns=['Model', 'Embedding', 'Train acc', 'Test acc'])\nscores_df","195bbca6":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style='whitegrid', font_scale=1.2)\n\nplt.figure(figsize=(15,8))\nplt.ylim([0.6, 0.85])\nplt.title(\"Acur\u00e1cia no conjunto de teste\")\nsns.barplot(data=scores_df, x='Model', y='Test acc', hue='Embedding');","f8c1145f":"from sklearn.model_selection import GridSearchCV\n\nmodel = SVC()\n#model_params = {\n#    'C' : list(np.linspace(1, 1.1, num=20)) + [1]\n#}\n\n#gs = GridSearchCV(model,param_grid=model_params, scoring='accuracy', n_jobs=-1, cv=10, verbose=3)\n#gs.fit(w2v_train_df, y_train)\n#gs.best_estimator_\n\ngs = SVC(C=1.0947368421052632)\ngs.fit(w2v_train_df, y_train)","582be70f":"accuracy_score(y_test, gs.predict(w2v_test_df))","385b6e93":"def model_predict(texts, model):\n    document_embeddings = []\n    for text in texts:\n        sentence = text_preprocess(text, 'english', domain_words).split()\n        L = []\n        for token in sentence:\n            try:\n                L.append(word2vec[token])\n            except:\n                1\n        if len(L) > 0: opinion_vec = np.mean(np.array(L), axis=0)\n        else: opinion_vec = np.zeros(300)\n        document_embeddings.append(opinion_vec)\n\n    return model.predict(document_embeddings)","fe6c4102":"model_predict([\"Really good product\"], gs)","da866c74":"model_predict([\"We had a nice time\"], gs)","f9c69b41":"model_predict([\"That was the best movie I've ever seem\"], gs)","0731c7a7":"model_predict([\"Awful night\"], gs)","d4b2cbfc":"model_predict([\"The food was spoiled\"], gs)","b141aef5":"model_predict([\"The food was delicious\"], gs)","61262e11":"model_predict([\"The waiter was rude to me\"], gs)","942f0a31":"model_predict([\"My girlfriend broke up with me because I brought her here\"], gs)","cdc95ae8":"model_predict([\"I found a rat when visiting the kitchen\"], gs)","db28e73a":"model_predict([\"My first date with my wife was at this restaurant\"], gs)","aacd1f6c":"model_predict([\"A horror movie that didn't scare me at all\"], gs)","9ac5b3f8":"model_predict([\"I found a rat when visiting the kitchen, lovely\"], gs)","fb551127":"model_predict([\"The best movie of my life, I even stopped watching in the middle\"], gs)","8fc212d0":"model_predict([\"This was the bast day of mi lyfe\"], gs)","0fb0a7ca":"model_predict([\"Mi first date wif my uife was at thys restarent\"], gs)","92c1a893":"model_predict([\"My first data with my wife has at thus restaurant\"], gs)","011561c1":"_Esse notebook foi originalmente meu **Trabalho 1 da mat\u00e9ria de Minera\u00e7\u00e3o de Dados N\u00e3o Estruturados** - SCC0287 - no ano de 2021, segundo semestre com o Professor Ricardo Marcondes Marcacini. Ele foi feito em dupla com Gabriel Henrique Pinheiro Rodrigues._\n\n_O objetivo dele era aplicar um pipeline completo de NLP para classifica\u00e7\u00e3o de textos, encerrando os estudos sobre essa \u00e1rea._\n\n\n<br>\n\n**Objetivo do trabalho:** 1) Classifica\u00e7\u00e3o de textos\n\n<br>\n<br>\n\n**Descri\u00e7\u00e3o da base de dados:**\n\nVamos utilizar uma base de dados composta por reviews de tr\u00eas servi\u00e7os diferentes: a Amazon, uma loja de varejo, o IMDB, um site de cr\u00edtica cinem\u00e1tica e o YELP, um site de cr\u00edtica gastron\u00f4mica. \n\nAs bases da Amazon e YELP possuem 1000 documentos cada, dos quais 500 s\u00e3o positivos e 500 s\u00e3o negativos. J\u00e1 a do IMDB possui 748 documentos, dos quais 386 s\u00e3o positivos e 362 s\u00e3o negativos. Portanto, temos uma base com um total de 2748 documentos.\n\nOs arquivos vieram de uma base muito maior e foram selecionados documentos que s\u00e3o particularmente f\u00e1ceis de se classificar (isto \u00e9, que  palavras claras de emo\u00e7\u00e3o e que n\u00e3o s\u00e3o muito grandes ou rebuscados). \n\nPara mais informa\u00e7\u00f5es, veja o [link original](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Sentiment+Labelled+Sentences).\n\n<br>\n\n**Objetivos da minera\u00e7\u00e3o de textos:**\n\nNosso objetivo \u00e9 avaliar se a cr\u00edtica foi positiva ou negativa, portanto, vamos realizar a tarefa de _classifica\u00e7\u00e3o de textos_ voltada \u00e0 _an\u00e1lise de sentimentos_. \n\n","6fb0ceb3":"#### Escolha do modelo e embedding","2eaedea0":"Testando ironia.","6f7e0e5a":"## Conclus\u00e3o e poss\u00edveis aplica\u00e7\u00f5es\n\nPudemos observar que o modelo final consegue acertar bem os casos mais simples onde h\u00e1 uma palavra expl\u00edcita de sentimento. J\u00e1 nos casos em que o contexto est\u00e1 impl\u00edcito ou em que temos ironia, ele falhou um pouco, mas no geral tamb\u00e9m foi bom. Quando come\u00e7amos a testar com erros de escrita, o modelo se mostrou muito mais inst\u00e1vel, errando v\u00e1rios casos.\n\nEsses resultados j\u00e1 eram esperados, pois a embedding utilizada n\u00e3o verifica o contexto, mas analisa as palavras individualmente. Al\u00e9m disso, quando uma palavra possui erro de escrita, ele n\u00e3o consegue criar a embedding, pois n\u00e3o viu a palavra anteriormente.\n\n<br>\n\nVamos agora comentar um pouco sobre os poss\u00edveis usos de conhecimento dessa nossa aplica\u00e7\u00e3o.\n- O uso mais imediato \u00e9 para reconhecer reviews de empresas e servi\u00e7os com o objetivo de identificar rapidamente a imagem corporativa da empresa nos meios virtuais atrav\u00e9s da propor\u00e7\u00e3o de reviews positivos e negativos. \n- Outro uso \u00e9 que empresa pode tentar ler alguns desses reviews para tentar melhorar seu produto\/servi\u00e7o. Al\u00e9m disso, esse modelo nos possibilita saber quais reviews s\u00e3o \"mais positivos\" e quais s\u00e3o os \"mais negativos\" com base na probabilidade que o SVM associa a cada classe (ou seja, o qu\u00e3o seguro o modelo est\u00e1 em afirmar que um determinado review \u00e9 positivo ou negativo).\n- Tamb\u00e9m podemos avaliar o impacto de um novo produto\/servi\u00e7o lan\u00e7ado pela empresa a partir da varia\u00e7\u00e3o da taxa de reviews positivos. Se a taxa diminuir, ent\u00e3o significa que as pessoas n\u00e3o gostaram do novo produto\/servi\u00e7o, mas se aumentou, ent\u00e3o gostaram.","2849386c":"### BERT","cd9128bf":"## Pr\u00e9-processamento","d9652368":"Al\u00e9m da nossa base de dados, vamos testar em algumas senten\u00e7as.","0d2894b8":"Podemos observar que de todos os embeddings, o _Word2Vec_ \u00e9 o que possui o melhor desempenho no nosso conjunto de dados. Al\u00e9m disso, o modelo que possui melhores acur\u00e1cias foi o _Support Vector Machine_. Portanto, vamos finalizar tentando tunar esse modelo aplicado a essa embedding.","4e60c430":"Vamos remover algumas palavras de dom\u00ednio. \n\nPara encontr\u00e1-las, vimos quais eram as palavras mais frequentes em cada uma das bases (amazon, imdb, yelp). Depois pegamos, das mais frequentes, aquelas que julgamos que n\u00e3o influenciavam no sentimento da senten\u00e7a, como por exemplo a palavra \"movie\" (filme - que pode ser qualificada por um adjetivo).","e44c9198":"## Extra\u00e7\u00e3o de padr\u00f5es e p\u00f3s-processamento","eff62b7c":"Primeiro as mais b\u00e1sicas e \u00f3bvias.","c7c55870":"### Bag of Words","41b9a525":"Testando erros de escrita","4719b378":"### Word2Vec","8ad12524":"## Importa\u00e7\u00e3o de bibliotecas e dados","f67fa17a":"Agora vejamos algumas senten\u00e7as em que o sentimento est\u00e1 impl\u00edcito."}}