{"cell_type":{"00ccb918":"code","670f165c":"code","58e727cb":"code","134a05b2":"code","df4ecfdf":"code","474036ba":"code","16b37e5b":"code","10be2cdf":"code","e7bce246":"code","b09aff65":"code","6c72181c":"code","fb388dcd":"code","5a2254bf":"code","42443c32":"code","57a7a390":"markdown","a38b99e0":"markdown"},"source":{"00ccb918":"!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-development.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-test.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-validation.tsv -q","670f165c":"!pip install pytorch-pretrained-bert","58e727cb":"import time\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport spacy\n\nimport torch\nfrom torch.optim import Adam\nfrom torch.nn import Module, Linear, Dropout\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\nimport torch.nn.functional as F\n\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel, BertLayer\nfrom pytorch_pretrained_bert.optimization import BertAdam\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import normalize","134a05b2":"device = torch.device(\"cuda\")\n\nseed = 42\n\nrandom.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","df4ecfdf":"# Preprocessing\nmax_len = 300\ndo_lower_case = True\nN = \"nobody\"\n\n# Model\nbert_model = \"bert-base-uncased\"\nn_bertlayers = 12\ndropout = 0.1\nno_pooler = True\n\n# Training\noptim = \"bertadam\"\ntrain_batch_size = 20\nnum_train_epochs = 3\ngradient_accumulation_steps = 1\nlearning_rate = 2e-5\nwarmup_proportion = 0.1\nweight_decay = False\n\n# Evaluation\neval_batch_size = 32","474036ba":"nlp = spacy.load(\"en\")\ndef get_sentence(text, offset, token_after=\"[PRONOUN]\"):\n    \"\"\"\n    Extract a sentence containing a word at position offset by character and\n    replace the word with token_after.\n    output: Transformed sentence\n            token_before\n            a pos tag of the word.\n    \"\"\"\n    doc = nlp(text)\n    # idx: Character offset\n    idx_begin = 0\n    for token in doc:\n        if token.sent_start:\n            idx_begin = token.idx\n        if token.idx == offset:\n            sent = token.sent.string\n            pos_tag = token.pos_\n            idx_token = offset - idx_begin\n            break\n    token_before = token.string.strip()\n    subtxt_transformed = re.sub(\"^\" + token_before, token_after, sent[idx_token:])\n    sent_transformed = sent[:idx_token] + subtxt_transformed\n    return sent_transformed, token_before, pos_tag\n\n\ndef generate_choices(text, offset, A, B, N=None):\n    \"\"\"\n    Extract a sentence contain a pronoun at a offset position.\n    Then replace the pronoun with A, B or N.\n        3 choices.\n        [Pronoun] likes something. ==>\n          A likes something.\n          B likes something.\n          neigher A nor B likes something. (If N is None.)\n          N likes something. (If N is not None.)\n    text:  str\n    offset: int\n    A, B: Person's names. str\n    N: nobody or something. str\n    \"\"\"\n    sents = []\n    text_pronoun, pronoun, pos_tag = get_sentence(text, offset)\n    if pos_tag == \"ADJ\" or pronoun == \"hers\":\n        _post = \"'s\"\n    elif pronoun == \"his\":\n        _post = \"'s\"\n    else:\n        _post = \"\"\n    who_s = [A + _post, B + _post]\n    if N is None:\n        who_s += [\"neither \" + A + \" nor \" + B]\n    else:\n        who_s += [N + _post]\n    sents.extend([re.sub(\"\\[PRONOUN\\]\", who, text_pronoun) for who in who_s])\n\n    return sents","16b37e5b":"# https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\/blob\/master\/examples\/run_swag.py#L216\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n\nclass NLIDataset(Dataset):\n    \"\"\"\n    NLI Dataset\n    p_texts: Premise texts\n    h_texts: Hypothesis texts\n    tokenizer: BertTokenizer\n    y      : Target sequence\n    y_values: Class labels\n    \"\"\"\n    def __init__(self, p_texts, h_texts, tokenizer,\n                 y=None, y_values=None, max_len=100):\n        if y is None:\n            self.labels = None\n        else:\n            mapper = {label: i for i, label in enumerate(y_values)}\n            self.labels = torch.LongTensor([mapper[v] for v in y])\n\n        self.max_tokens = 0\n        self.inputs = []\n        for e, (p_text, h_text) in enumerate(zip(p_texts, h_texts)):\n            p_tokens = tokenizer.tokenize(p_text)\n            h_tokens = tokenizer.tokenize(h_text)\n            _truncate_seq_pair(p_tokens, h_tokens, max_len - 3)\n            p_len = len(p_tokens)\n            h_len = len(h_tokens)\n\n            tokens = [\"[CLS]\"] + p_tokens + [\"[SEP]\"] + h_tokens + [\"[SEP]\"]\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n            segment_ids = [0] * (p_len + 2) + [1] * (h_len + 1)\n            input_mask = [1] * len(input_ids)\n            self.inputs.append([torch.LongTensor(input_ids),\n                                torch.LongTensor(segment_ids),\n                                torch.LongTensor(input_mask)])\n            self.max_tokens = max(p_len + h_len + 3, self.max_tokens)\n            if e < 1:\n                print(\"tokens:\", p_tokens)\n\n        print(\"max_len:\", self.max_tokens)\n\n    def __getitem__(self, index):\n        return self.inputs[index], self.labels[index]\n\n    def __len__(self):\n        return len(self.inputs)\n\n\ndef get_gap_nli_dataset(df, tokenizer, max_len, labeled=True, N=\"\"):\n    p_texts = df[\"Text\"].repeat(3)\n    h_texts = df.apply(lambda x:\n                       generate_choices(x[\"Text\"], x[\"Pronoun-offset\"], x[\"A\"], x[\"B\"], N=N),                                     \n                       axis=1)\n    h_texts = sum(h_texts, [])\n    if labeled:\n        y_A = df[\"A-coref\"].astype(int)\n        y_B = df[\"B-coref\"].astype(int)\n        y_Neither = 1 - y_A - y_B\n        labels = np.column_stack((y_A, y_B, y_Neither)).reshape(-1)\n    return NLIDataset(p_texts, h_texts, tokenizer,\n                      y=labels, y_values=(0, 1), max_len=max_len)\n\n\ndef collate_fn(batch):\n    \"\"\"\n    Pad the inputs sequence.\n    \"\"\"\n    x_lst, y_lst = list(zip(*batch))\n    xy_batch = [pad_sequence(x, batch_first=True) for x in zip(*x_lst)]\n    xy_batch.append(torch.stack(y_lst, dim=0))\n    return xy_batch","10be2cdf":"def get_pretrained_bert(modelname, n_bertlayers=None):\n    bert = BertModel.from_pretrained(modelname)\n    if n_bertlayers is None:\n        return bert\n    if n_bertlayers < bert.config.num_hidden_layers:\n        # Only use the bottom n layers\n        del bert.encoder.layer[n_bertlayers:]\n        bert.config.num_hidden_layers = n_bertlayers\n    return bert\n\n\nclass BertCl(Module):\n    def __init__(self, modelname, n_bertlayers, dropout, num_labels,\n                 no_pooler=False):\n        super(BertCl, self).__init__()\n        self.bert = get_pretrained_bert(modelname, n_bertlayers)\n        self.dropout = Dropout(dropout)\n        self.classifier = Linear(self.bert.config.hidden_size, num_labels)\n        self.no_pooler = no_pooler\n\n    def forward(self, input_ids, segment_ids, input_mask):\n        encoded_layer, pooled_output = self.bert(input_ids, segment_ids, input_mask,\n                                                 output_all_encoded_layers=False)\n        if self.no_pooler:\n            x = self.classifier(self.dropout(encoded_layer[:, 0]))\n        else:\n            x = self.classifier(self.dropout(pooled_output))\n        return x\n\n\ndef predict(model, data_loader, device, proba=True, to_numpy=True):\n    model.eval()\n    preds = []\n    for step, batch in enumerate(data_loader):\n        batch = tuple(t.to(device) for t in batch)\n        with torch.no_grad():\n            logits = model(*batch[:-1])\n            preds.append(logits.detach().cpu())\n    preds = torch.cat(preds) if len(preds) > 1 else preds[0]\n    if proba:\n        if preds.size(-1) > 1:\n            preds = F.softmax(preds, dim=1)\n        else:\n            preds = torch.sigmoid(preds)\n    if to_numpy:\n        preds = preds.numpy()\n    return preds\n\n\ndef get_param_size(model):\n    trainable_psize = np.sum([np.prod(p.size()) for p in model.parameters() if p.requires_grad])\n    total_psize = np.sum([np.prod(p.size()) for p in model.parameters()])\n    return total_psize, trainable_psize","e7bce246":"def get_loader(train_df, val_df, test_df):\n    tokenizer = BertTokenizer.from_pretrained(bert_model,\n                                              do_lower_case=do_lower_case)\n\n    train_ds = get_gap_nli_dataset(train_df, tokenizer, max_len, labeled=True, N=N)                       \n    val_ds = get_gap_nli_dataset(val_df, tokenizer, max_len, labeled=True, N=N)\n    test_ds = get_gap_nli_dataset(test_df, tokenizer, max_len, labeled=True, N=N)\n    \n    train_loader = DataLoader(\n        train_ds,\n        batch_size=train_batch_size,\n        collate_fn=collate_fn,\n        shuffle=True,\n        drop_last=True)\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=eval_batch_size,\n        collate_fn=collate_fn,\n        shuffle=False)\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=eval_batch_size,\n        collate_fn=collate_fn,\n        shuffle=False)\n\n    return train_loader, val_loader, test_loader\n\n\ndef get_gap_cl_model(device, steps_per_epoch, bert_model, n_bertlayers, dropout,\n                     num_labels=1, no_pooler=False):\n    model = BertCl(bert_model, n_bertlayers, dropout,\n                   num_labels=num_labels, no_pooler=no_pooler)\n    model.to(device)\n\n    param_optimizer = list(model.named_parameters())\n\n    if weight_decay:\n        no_decay = [\"bias\", \"gamma\", \"beta\", \"classifier\"]\n        optimizer_grouped_parameters = [\n            {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n             \"weight_decay\": 0.01},\n            {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n             \"weight_decay\": 0.0}\n        ]\n    else:\n        optimizer_grouped_parameters = [\n            {\"params\": [p for n, p in param_optimizer], \"weight_decay\": 0.0}\n\n        ]\n\n    t_total = int(\n        steps_per_epoch \/ gradient_accumulation_steps * num_train_epochs)\n    if optim == \"bertadam\":\n        optimizer = BertAdam(optimizer_grouped_parameters,\n                             lr=learning_rate,\n                             warmup=warmup_proportion,\n                             t_total=t_total)\n    elif optim == \"adam\":\n        optimizer = Adam(optimizer_grouped_parameters,\n                         lr=learning_rate)\n\n    return model, optimizer\n\n\n\ndef run_epoch(model, dataloader, optimizer, criterion, device, verbose_step=10000):\n    model.train()\n    t1 = time.time()\n    tr_loss = 0\n    for step, batch in enumerate(dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        label_ids = batch[-1]\n        outputs = model(*batch[:-1])\n        if criterion._get_name() == \"BCEWithLogitsLoss\":\n            outputs = outputs[:, 0]\n            label_ids = label_ids.float()\n        loss = criterion(outputs, label_ids)\n        if gradient_accumulation_steps > 1:\n            loss = loss \/ gradient_accumulation_steps\n        loss.backward()\n        tr_loss += loss.item()\n        if (step + 1) % verbose_step == 0:\n            loss_now = gradient_accumulation_steps * tr_loss \/ (step + 1)\n            print(f\"step:{step+1} loss:{loss_now:.7f} time:{time.time() - t1:.1f}s\")\n        if (step + 1) % gradient_accumulation_steps == 0:\n            optimizer.step()\n            model.zero_grad()\n    return gradient_accumulation_steps * tr_loss \/ (step + 1)\n\n\ndef eval_model(model, dataloader, y, device):\n    pr_sgmd = predict(model, dataloader, device, proba=True, to_numpy=True)[:, 0].reshape((-1, 3))\n    loss_s = [log_loss(y[i::3], pr_sgmd[:, i]) for i in range(3)]\n    pr_ABN = normalize(pr_sgmd, norm=\"l1\")\n    ABN_loss = log_loss(y.reshape((-1, 3)), pr_ABN)\n    return {\"A_loss\": loss_s[0], \"B_loss\": loss_s[1], \"N_loss\": loss_s[2],\n            \"ABN_loss\": ABN_loss}","b09aff65":"train_df = pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\")\nval_df = pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\ntest_df = pd.read_csv(\"gap-development.tsv\", delimiter=\"\\t\")\n\nval_y_AB = val_df[[\"A-coref\", \"B-coref\"]].astype(int)\nval_y_N = 1 - val_y_AB.sum(1)\nval_y = np.column_stack((val_y_AB, val_y_N)).reshape(-1)\n\ntest_y_AB = test_df[[\"A-coref\", \"B-coref\"]].astype(int)\ntest_y_N = 1 - test_y_AB.sum(1)\ntest_y = np.column_stack((test_y_AB, test_y_N)).reshape(-1)","6c72181c":"train_loader, val_loader, test_loader = get_loader(train_df, val_df, test_df)","fb388dcd":"model, optimizer = get_gap_cl_model(device, len(train_loader),\n                                    bert_model, n_bertlayers, dropout, no_pooler=no_pooler)\ntotal_psize, trainalbe_psize = get_param_size(model)\nprint(f\"Total params: {total_psize}\\nTrainable params: {trainalbe_psize}\")","5a2254bf":"criterion = BCEWithLogitsLoss()\nfor e in range(num_train_epochs):\n    t1 = time.time()\n    tr_loss = run_epoch(model, train_loader, optimizer, criterion, device)\n    val_obj = eval_model(model, val_loader, val_y, device)\n    print(f\"Epoch:{e + 1} tr_loss:{tr_loss:.4f}\"\n          f\"\\n val  A_loss:{val_obj['A_loss']:.4f}\"\n          f\" B_loss:{val_obj['B_loss']:.4f}\"\n          f\" N_loss:{val_obj['N_loss']:.4f}\"\n          f\" ABN_loss: {val_obj['ABN_loss']:.4f}\",\n          f\" time:{time.time() - t1:.1f}s\")","42443c32":"test_obj = eval_model(model, test_loader, test_y, device)\nprint(f\" \\ntest   ABN_loss: {test_obj['ABN_loss']:.4f}\")","57a7a390":"A_loss, B_loss, N_loss: Binary logloss,  ABN_loss: 3-class multilogloss (target metric)","a38b99e0":"I borrow some code from [pytorch-pretrained-BERT\/example](https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\/tree\/master\/examples) and [[PyTorch] BERT Baseline (Public Score ~ 0.54)](https:\/\/www.kaggle.com\/ceshine\/pytorch-bert-baseline-public-score-0-54).\n\nIn this kernel, I'll convert this task into Natural Language Inference(NLI) problem. Given two texts, a premise and hypothesis, NLI is the task to predict whether the premise entails the hypothesis or not.\n\nA premise is \"Text\" property.\n\n3 hypotheses for each example. First extract a sentence containing the specified pronoun, then change the pronoun into A, B or nobody. They are hypotheses."}}