{"cell_type":{"0c84ff85":"code","a5d7dca8":"code","60ff5c4a":"code","e6ffca49":"code","c5634978":"code","51c8094a":"code","a95e695b":"code","305f9419":"code","ad8fe295":"code","18005512":"code","46119cd8":"code","8e3e6022":"code","440f2c19":"code","fc18485a":"code","6c62fbc3":"code","e4a0238b":"code","02679eba":"code","64f5e1cd":"code","0ec95ed9":"code","b9a52835":"code","36c79101":"code","561e9354":"code","4c1e5db8":"code","32ea712f":"code","6d6fcd6a":"code","f7464abf":"code","50bff0cc":"code","0f354f61":"code","052793a9":"code","afc9a494":"code","3546d3fd":"code","dc789270":"code","c844da12":"code","370ec758":"code","8dc76a04":"markdown","2a08237b":"markdown","3c599bbf":"markdown","90e5f28d":"markdown","3e088ce5":"markdown","51ede5fc":"markdown","588a6274":"markdown","04c7b59f":"markdown","f02609fb":"markdown","845ec7f8":"markdown","8fc9404b":"markdown","f168de03":"markdown","0c06c87e":"markdown","a45fd9fe":"markdown","1789f280":"markdown","e5218c7a":"markdown"},"source":{"0c84ff85":"import os\nos.environ['PYTHONHASHSEED']=str(1)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom pprint import pprint\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.ensemble as ske\nimport sklearn.preprocessing as skp\nimport random\nseed = 12","a5d7dca8":"import tensorflow as tf\nfrom tensorflow import keras as k\nprint(tf.__version__)\nfrom keras import backend as K\nfrom keras.utils import to_categorical\nimport os, shutil, re, string\nimport matplotlib.pyplot as plt\nimport spacy\nseed=100","60ff5c4a":"def set_seed(seed):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\nset_seed(seed)","e6ffca49":"# scanning data and merging with participant as person column for identification\nDIR = '\/kaggle\/input\/single-chestmounted-accelerometer\/'\ndf = pd.DataFrame()\nfor x in range(1,16):\n    df_temp = pd.read_csv(os.path.join(DIR, str(x)+'.csv'))\n    df_temp.columns = ['index','x','y','z','class']\n    df_temp['person'] = x\n    df_temp.drop('index', inplace=True, axis=1)\n#     df_temp.drop(df_temp[df_temp['class'] == 0].index.tolist(), inplace=True, axis=0)\n    df = pd.concat([df, df_temp], axis=0)\n\ndf = df.reset_index(drop=True)\nprint(\"Scanned\",len(df.person.unique()),\"participants recordings\")\ndf.tail()","c5634978":"# map index to label\nindex_label = dict()\nindex_label[0] = \"None\"\nindex_label[1] = \"Working at Computer\"\nindex_label[2] = \"Standing Up, Walking and Going up\\down stairs\"\nindex_label[3] = \"Standing\"\nindex_label[4] = \"Walking\"\nindex_label[5] = \"Going Up\\Down Stairs\"\nindex_label[6] = \"Walking and Talking with Someone\"\nindex_label[7] = \"Talking while Standing\"\n\nindex_label.values()","51c8094a":"# plot x, y, z acceleration\ndef plotXYZ(df_plot, p):\n    plt.figure()\n    for col in range(3):\n        plt.subplot(df_plot.shape[1], 1, col+1)\n        sns.lineplot(df_plot.index.tolist(), df_plot.iloc[:,col])\n    plt.show()\n\nfor x in range(1,5):\n    df_temp = df[df[\"person\"]==x]\n    plotXYZ(df_temp, x)","a95e695b":"# plot boxplot for all classes\nplt.figure(figsize=(16,8))\nsns.boxplot(x=\"class\", y=\"x\",\n            hue=\"person\", palette=[\"m\", \"g\"],\n            data=df)\nplt.title(\"Boxplot for x-acc\")\nplt.show()\n\n\nplt.figure(figsize=(16,8))\nsns.boxplot(x=\"class\", y=\"y\",\n            hue=\"person\", palette=[\"m\", \"g\"],\n            data=df)\nplt.title(\"Boxplot for y-acc\")\nplt.show()\n\n\nplt.figure(figsize=(16,8))\nsns.boxplot(x=\"class\", y=\"z\",\n            hue=\"person\", palette=[\"m\", \"g\"],\n            data=df)\nplt.title(\"Boxplot for z-acc\")\nplt.show()\n","305f9419":"# checking distribution of samples against classes\nplt.figure(figsize=(12,6))\nplt.pie(df['class'].value_counts(), labels=[index_label[x] for x in df['class'].value_counts().index])\nplt.show()","ad8fe295":"# checking distributions for x, y and z\nplt.figure(figsize=(16,6))\nplt.subplot(1,3,1)\nsns.distplot(df['x'])\n\nplt.subplot(1,3,2)\nsns.distplot(df['y'])\n\nplt.subplot(1,3,3)\nsns.distplot(df['z'])\nplt.show()","18005512":"# countplot distribution for each person per class\nplt.figure(figsize=(16,6))\nsns.countplot(x='person', hue='class', data=df)\nplt.show()","46119cd8":"# remove multi-activities classes - 0, 2, and 6\ndf = df[~df[\"class\"].isin([0,2,6])]\nprint(\"Remaining classes\",df['class'].unique())","8e3e6022":"# replacing class 0, and 2 with 5 and 7 respectively.\n# 0 => 5\n# 2 => 7\ndf['class'] = [0 if x is 5 else 2 if x is 7 else x for x in df['class']]\ndf['class'].value_counts()","440f2c19":"# removing outliers, values away from mean +- x*sd, from all classes for x, y and z\nindexesToRemove = []\nfor x in df['class'].unique().tolist():\n    # filter with specific class\n    print(\"Filtering class\",x)\n    df_temp = df[df['class'] == x]\n\n    for y in range(3):\n        # remove rows with values away from mean by 3SD, in x, y and z\n        mean = df_temp.iloc[:,y].mean()\n        sd = df_temp.iloc[:,y].std()\n        away = 3\n        upperlimit = mean + away * sd\n        lowerlimit = mean - away * sd\n        \n        i = df_temp[(df_temp.iloc[:,y] > upperlimit) | (df_temp.iloc[:,y] < lowerlimit)].index.tolist()\n        indexesToRemove.extend(i)\n\nindexesToRemove = list(set(indexesToRemove))\ndf.drop(indexesToRemove, axis=0, inplace=True)\ndf.reset_index(drop=True, inplace=True)\nprint(\"Removed\",len(indexesToRemove),\"rows.\")","fc18485a":"# checking distribution of x, y and z after removing outliers\ndf.describe()","6c62fbc3":"# splitting by person recordings, first 12 for training and last 3 for test\nX_train = df[df['person'] <= 12]\nX_test = df[df['person'] > 12]\ny_train = X_train.pop('class')\ny_test = X_test.pop('class')\nX_train.head()","e4a0238b":"# drop person column from both\nX_train.drop('person', axis=1, inplace=True)\nX_test.drop('person', axis=1, inplace=True)\nX_train.head()","02679eba":"# generate training class weights which will be fed into model while training\nfrom sklearn.utils import class_weight\nweights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\ntrain_class_weights = dict(zip(np.unique(y_train), weights))\npprint(train_class_weights)","64f5e1cd":"scaler = skp.MinMaxScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)","0ec95ed9":"samples_per_sec = 52\nwindow_seconds = 2\nwindow_size = samples_per_sec * window_seconds\n\ndef sampleData(df, y):\n    X_sampled = []\n    y_sampled = []\n    window = list(range(df.shape[0]))[::window_size]\n    for x in range(len(window)-1):\n        window_sample_X = df.iloc[window[x]:window[x+1],:].values.tolist()\n        X_sampled.append(window_sample_X)\n        \n        window_sample_y = y.iloc[window[x]:window[x+1]].tolist()\n        window_sample_y = max(window_sample_y, key=window_sample_y.count)\n        y_sampled.append(window_sample_y)\n        \n    return np.array(X_sampled), np.array(y_sampled)\n\nX_train_sampled, y_train_sampled = sampleData(X_train, y_train)\nX_test_sampled, y_test_sampled = sampleData(X_test, y_test)\n\nX_train_sampled.shape","b9a52835":"ACCURACY_THRESHOLD = 0.95\n\nclass myCallback(k.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_sparse_categorical_accuracy') > ACCURACY_THRESHOLD):\n            print(\"\\n\\nStopping training as we have reached our goal.\")   \n            self.model.stop_training = True\n\nacc_callback = myCallback()\n\ndef plotHistory(history):\n    print(\"Max. Val. Accuracy -\",max(history.history['val_sparse_categorical_accuracy']),\"for epoch\",str(int(np.argmax(history.history['val_sparse_categorical_accuracy'])+1)))\n    pd.DataFrame(history.history).plot(figsize=(12,6))\n    plt.show()\n\ndef setupCallbacks(bestModelPath):\n    checkpoint = k.callbacks.ModelCheckpoint(filepath=bestModelPath, monitor='val_loss', verbose=1, save_best_only=True)\n    LR = k.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, verbose=1, patience=5)\n    cbsList = [checkpoint]\n    # cbsList.append(LR)\n    return cbsList","36c79101":"def assessModelPerformance(modelPath, X_test, y_test):\n    print(\"Evaluating..\")\n    model = k.models.load_model(modelPath)\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n    print(\"Test Loss -\",loss)\n    print(\"Test Accuracy -\",accuracy)\n\n    y_test_pred = model.predict(X_test)\n    y_test_pred_class = [np.argmax(x) for x in y_test_pred]\n    print(skm.classification_report(y_test, y_test_pred_class))","561e9354":"model = k.Sequential([\n    \n    k.layers.Dense(2048, activation='relu', input_shape=(X_train.shape[1],)),\n\n    k.layers.Dense(1024, activation='relu'),\n    k.layers.BatchNormalization(),\n\n    k.layers.Dense(512, activation='relu'),\n    k.layers.BatchNormalization(),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(256, activation='relu'),\n    k.layers.BatchNormalization(),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(128, activation='relu'),\n    k.layers.Dense(5)\n])\nmodel.summary()","4c1e5db8":"# compiling with RMSProp & Adam to find best\nbestModelPath = '.\/best_model_1.hdf5'\ncallbacks_list = setupCallbacks(bestModelPath)\nbatch_size = 1024*16\nepochs = 50\nmodel.compile(\n    optimizer=k.optimizers.Adam(),\n    loss=k.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['sparse_categorical_accuracy']\n)\nset_seed(seed)\nhistory = model.fit(\n                    X_train, y_train, validation_data=(X_test, y_test), \n                    epochs=epochs, batch_size=batch_size, \n                    class_weight=train_class_weights,\n                    callbacks=[callbacks_list],\n                    verbose=0\n                   )","32ea712f":"plotHistory(history)","6d6fcd6a":"assessModelPerformance(bestModelPath, X_test, y_test)","f7464abf":"rfc = ske.RandomForestClassifier(random_state=seed, class_weight=train_class_weights)\nrfc.fit(X_train, y_train)\npredictions = rfc.predict(X_test)\nskm.accuracy_score(y_test, predictions)","50bff0cc":"model = k.Sequential([\n    \n#     k.layers.Conv1D(filters=512, kernel_size=5, activation='relu', input_shape=(window_size,3)),\n\n#     k.layers.Conv1D(filters=512, kernel_size=5, activation='relu'),\n#     k.layers.Dropout(0.3),\n\n#     k.layers.Conv1D(filters=256, kernel_size=5, activation='relu'),\n#     k.layers.Dropout(0.4),\n\n#     k.layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n#     k.layers.Dropout(0.4),\n\n#     k.layers.GlobalMaxPool1D(),\n#     k.layers.Dropout(0.3),\n\n#     k.layers.Bidirectional(k.layers.LSTM(512, return_sequences=True), input_shape=(window_size,3)),\n#     k.layers.Dropout(0.2),\n\n#     k.layers.Bidirectional(k.layers.LSTM(256, return_sequences=True)),\n#     k.layers.Dropout(0.2),\n\n#     k.layers.Bidirectional(k.layers.LSTM(128)),\n#     k.layers.Dropout(0.2),\n\n    k.layers.Bidirectional(k.layers.GRU(512, return_sequences=True), input_shape=(window_size,3)),\n#     k.layers.BatchNormalization(),\n    k.layers.Dropout(0.3),\n\n    k.layers.Bidirectional(k.layers.GRU(256, return_sequences=True)),\n#     k.layers.BatchNormalization(),\n    k.layers.Dropout(0.3),\n\n    k.layers.Bidirectional(k.layers.GRU(128)),\n    k.layers.Dropout(0.3),\n\n    k.layers.Flatten(),\n    k.layers.Dense(128, activation='relu'),\n    k.layers.Dense(5)\n])\nmodel.summary()","0f354f61":"# compiling with RMSProp & Adam to find best\nbestModelPath = '.\/best_model_2.hdf5'\ncallbacks_list = setupCallbacks(bestModelPath)\nbatch_size = 128\nepochs = 30\nmodel.compile(\n    optimizer=k.optimizers.Adam(),\n    loss=k.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['sparse_categorical_accuracy']\n)\nhistory = model.fit(\n                    X_train_sampled, y_train_sampled, validation_data=(X_test_sampled, y_test_sampled), \n                    epochs=epochs, batch_size=batch_size, \n                    class_weight=train_class_weights,\n                    callbacks=[callbacks_list],\n                    verbose=0\n                   )","052793a9":"plotHistory(history)","afc9a494":"assessModelPerformance(bestModelPath, X_test_sampled, y_test_sampled)","3546d3fd":"model = k.Sequential([\n    \n#     k.layers.Conv1D(filters=512, kernel_size=5, activation='relu', input_shape=(window_size,3)),\n\n#     k.layers.Conv1D(filters=512, kernel_size=5, activation='relu'),\n#     k.layers.Dropout(0.3),\n\n#     k.layers.Conv1D(filters=256, kernel_size=5, activation='relu'),\n#     k.layers.Dropout(0.4),\n\n#     k.layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n#     k.layers.Dropout(0.4),\n\n#     k.layers.GlobalMaxPool1D(),\n#     k.layers.Dropout(0.3),\n\n    k.layers.Bidirectional(k.layers.LSTM(512, return_sequences=True), input_shape=(window_size,3)),\n    k.layers.Dropout(0.3),\n\n    k.layers.Bidirectional(k.layers.LSTM(256, return_sequences=True)),\n    k.layers.Dropout(0.3),\n\n    k.layers.Bidirectional(k.layers.LSTM(128)),\n    k.layers.Dropout(0.3),\n\n#     k.layers.Bidirectional(k.layers.GRU(512, return_sequences=True), input_shape=(window_size,3)),\n#     k.layers.BatchNormalization(),\n#     k.layers.Dropout(0.3),\n\n#     k.layers.Bidirectional(k.layers.GRU(256, return_sequences=True)),\n#     k.layers.BatchNormalization(),\n#     k.layers.Dropout(0.3),\n\n#     k.layers.Bidirectional(k.layers.GRU(128)),\n#     k.layers.Dropout(0.3),\n\n    k.layers.Flatten(),\n    k.layers.Dense(128, activation='relu'),\n    k.layers.Dense(5)\n])\nmodel.summary()","dc789270":"# compiling with RMSProp & Adam to find best\nbestModelPath = '.\/best_model_3.hdf5'\ncallbacks_list = setupCallbacks(bestModelPath)\nbatch_size = 128\nepochs = 40\nmodel.compile(\n    optimizer=k.optimizers.Adam(),\n    loss=k.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['sparse_categorical_accuracy']\n)\nhistory = model.fit(\n                    X_train_sampled, y_train_sampled, validation_data=(X_test_sampled, y_test_sampled), \n                    epochs=epochs, batch_size=batch_size, \n                    class_weight=train_class_weights,\n                    callbacks=[callbacks_list],\n                    verbose=0\n                   )","c844da12":"plotHistory(history)","370ec758":"assessModelPerformance(bestModelPath, X_test_sampled, y_test_sampled)","8dc76a04":"## Data Sampling","2a08237b":"# Model Building","3c599bbf":"# Reading & Understanding the Data","90e5f28d":"Data is highly imbalanced and needs to be treated well.","3e088ce5":"## Scale Data","51ede5fc":"## Train-Test Split","588a6274":"# Human Activity Recognition with Chestmounted Accelerometer\n\nWe will be analyzing the data with respect to all the subjects as we want to classify the activities in general and not be specific to the person.\n\nThe modelling for the data will be with respect to the sliding window functionality as the data samples are already sampled with 1 second window with 50% overlap. \nThis approach will take into consideration the pre-activities that will be involved in the actions being taken by the person as a sequence and will help in making better recurrent neural networks models.\n\n'Working at Computer', 'Standing Up, Walking and Going up\\\\down stairs', 'Standing', 'Walking', 'Going Up\\\\Down Stairs', 'Walking and Talking with Someone', 'Talking while Standing'\n\n","04c7b59f":"## Dense Layer Model","f02609fb":"# Approach\n\n## Data Visualization\n\n- line plots for x, y and z for each person to check sample distribution\n- box plots for x, y and z for each class to check the outliers\n- pie chart for distribution of classes in total\n- distplots for x, y and z for gaussian distributions\n- countplot for each class per person to check class imbalance\n\n## Data Preparation\n\n- Remove classes with multi-activities that will help in fairer evaluation of the activities which are 0, 2, & 6.\n- Train Test Split with first 12 person for training and last 3 person for testing\n- Outlier Detection will be applied (values > 3 times SD away from mean should be removed).\n- Handle class imbalance problem in data by calculating class weights and setting them while model training.\n- Scale the overall data to improve model training.\n- Implemented multiple window sizes like 1, 2, 5 seconds to ensure what gives best result.\n\n\n## Data Modeling\n\n- Modeled with Dense layers of different combination with class weights but couldn't achieve good results.\n- Modeled with RandomForest Classifier with class weights but couldn't achieve better results.\n- Modeled with different combinations of Conv1D, GRUs, and LSTMs with Dropout and BatchNormalization between the layers with class weights but still could achieve around 64.62% of test accuracy and 65% of recall score.\n\n`Final Result: 3 layer LSTM model with Adam optimizer, with 65% recall rate due to class imabalnce issue which can be seen with recall for specific classes with less samples. These results could be achieved with class weights calculated and can be more improved with generating more augmented data.`\n\n## Future Work\n\n- Oversampling & Undersampling for Class Imbalance issue in the data.","845ec7f8":"Tried with different combinations of Dense layers but cannot get a good accuracy.","8fc9404b":"## Handle Class Imbalance","f168de03":"## Random Forest Model","0c06c87e":"## Conv1D-GRU-LSTM Model Combinations","a45fd9fe":"# Data Preparation","1789f280":"There are outliers present in the data. Class 1 has most number of outliers present in it and need to be treated.","e5218c7a":"# Data Visualization"}}