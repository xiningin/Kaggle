{"cell_type":{"69c715b8":"code","5f3b2306":"code","c2304f5a":"code","550bc44a":"code","23d6aa88":"code","d7a63a2f":"code","2c8889c1":"code","dfc661f6":"code","8d41e358":"code","b08607aa":"code","9d32be87":"markdown","020c9620":"markdown","4f428bbd":"markdown"},"source":{"69c715b8":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5f3b2306":"N_SPLITS = 5\n\ndef seed(seed=42):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nseed()","c2304f5a":"train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv').drop(['id'], axis=1)\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv').drop(['id'], axis=1)\nsample_sub = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","550bc44a":"X = train.drop(['target'], axis = 1)\ny = train['target']","23d6aa88":"le = LabelEncoder()\ntrain = train.assign(target = le.fit_transform(train.target))\ntrain.head()","d7a63a2f":"def train_and_eval_lgb(model_fn):\n    oof = np.zeros((len(train), 4))\n    test_preds = np.zeros((len(test), 4))\n    feature_importace = pd.DataFrame()\n\n    \n    cv = StratifiedKFold(N_SPLITS, shuffle=True, random_state = 42)\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        model = model_fn()\n        model = model.fit(\n            X.iloc[train_idx],\n            y.iloc[train_idx],\n            eval_set=[(X.iloc[val_idx], y.iloc[val_idx])],\n            eval_metric='multi_logloss',\n            early_stopping_rounds = 100,verbose=250)\n        \n        tmp_oof = model.predict_proba(X.iloc[val_idx].values)\n        oof[val_idx] += tmp_oof\n        test_preds += model.predict_proba(test.values) \/ N_SPLITS\n        \n        fe = pd.DataFrame()\n        fe['feature'] = model.feature_name_\n        fe['importance'] = model.feature_importances_\n        feature_importace = feature_importace.append(fe)\n        print(f'fold {fold + 1} logloss = {log_loss(y.iloc[val_idx], tmp_oof)}')\n    \n    print(f'oof logloss = {log_loss(y.values, oof)}')\n    return test_preds, feature_importace","2c8889c1":"params = {\n    'num_iterations': 20_000,\n    'learning_rate': 0.05,\n    'max_depth': 10,\n    'num_leaves' : 63,\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'bagging_seed': 42,\n    'boosting_type': 'gbdt',\n    'is_unbalance': True\n}\n\nmodel_fn  = lambda : LGBMClassifier(**params)","dfc661f6":"test_preds, feature_importace = train_and_eval_lgb(model_fn)\norder = list(feature_importace.groupby('feature').agg('mean').sort_values('importance', ascending=False).index)\n\nplt.figure(figsize=(16,8))\np = sns.barplot(x='feature', y='importance', data = feature_importace, order = order)\nplt.title(\"LGM Classifier importance\")\nplt.tight_layout()\n_ = p.set_xticklabels(p.get_xticklabels(), rotation=45)","8d41e358":"X['random'] = np.random.random((len(X)))\ntest['random'] = np.random.random((len(test)))","b08607aa":"test_preds, feature_importace = train_and_eval_lgb(model_fn)\norder = list(feature_importace.groupby('feature').agg('mean').sort_values('importance', ascending=False).index)\n\nplt.figure(figsize=(16,8))\np = sns.barplot(x='feature', y='importance', data = feature_importace, order = order)\nplt.title(\"LGM Classifier importance\")\nplt.tight_layout()\n_ = p.set_xticklabels(p.get_xticklabels(), rotation=45)","9d32be87":"It is quite popular to add random feature to data and observe which features have greater feature importance and which have smaller. But the results in this competition are quite interesting","020c9620":"# Original features","4f428bbd":"# Add random feature"}}