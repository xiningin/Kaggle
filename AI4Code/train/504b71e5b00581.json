{"cell_type":{"31f8699f":"code","6a09db03":"code","b7cab9d6":"code","f9158b7f":"code","e82850cb":"code","d261d4b8":"code","1d07a141":"code","bd2ea39e":"code","cc1c41b7":"code","20ced608":"code","723baefa":"code","3d6a9caf":"code","a1aa66f6":"code","01283cbb":"code","f8e3394c":"code","f40762a8":"code","4e0c2211":"code","1609a08a":"code","d7506869":"code","31748e94":"code","f9ee25f1":"code","33312a46":"code","88c85b75":"code","6f176e56":"code","0a9d7db2":"markdown","ddd3f0b9":"markdown","0fce945a":"markdown","6a1b7fe9":"markdown","b2d01ac5":"markdown","0d5fd794":"markdown","9589d590":"markdown","ba664a14":"markdown","18dc5adf":"markdown","fa40c127":"markdown","a54ef35a":"markdown","f2b4d9dd":"markdown"},"source":{"31f8699f":"!pip install pyspark","6a09db03":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n%env JOBLIB_TEMP_FOLDER=\/tmp \n#https:\/\/www.kaggle.com\/getting-started\/45288 - this helps some with 'no space left on device'","b7cab9d6":"import pyspark.sql.functions as sql_func\nfrom pyspark.sql.types import *\nfrom pyspark.ml.recommendation import ALS, ALSModel\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\n\nsc = SparkContext('local') #https:\/\/stackoverflow.com\/questions\/30763951\/spark-context-sc-not-defined\nspark = SparkSession(sc)","f9158b7f":"data_schema = StructType([\n    StructField('session_start_datetime',TimestampType(), False),\n    StructField('user_id',IntegerType(), False),\n    StructField('user_ip',IntegerType(), False),\n    StructField('primary_video_id',IntegerType(), False),\n    StructField('video_id',IntegerType(), False),\n    StructField('vod_type',StringType(), False),\n    StructField('session_duration',IntegerType(), False),\n    StructField('device_type',StringType(), False),\n    StructField('device_os',StringType(), False),\n    StructField('player_position_min',LongType(), False),\n    StructField('player_position_max',LongType(), False),\n    StructField('time_cumsum_max',LongType(), False),\n    StructField('video_duration',IntegerType(), False),\n    StructField('watching_percentage',FloatType(), False)\n])\nfinal_stat = spark.read.csv(\n    '..\/input\/train_data_full.csv', header=True, schema=data_schema\n).cache()","e82850cb":"ratings = (final_stat\n    .select(\n        'user_id',\n        'primary_video_id',\n        'watching_percentage',\n    )\n).cache()","d261d4b8":"%%time\nratings.count()","1d07a141":"import gc #This is to free up the memory\ngc.collect()\ngc.collect()","bd2ea39e":"%%time\nals = ALS(rank=100, #rank s the number of latent factors in the model (defaults to 10). Higher value - better accuracy (at this competition), longer training\n          maxIter=2, #maxIter is the maximum number of iterations to run (defaults to 10). Higher value - more memory used\n          implicitPrefs=True, #implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false)\n          regParam=1, #regParam specifies the regularization parameter in ALS (defaults to 1.0)\n          alpha=50, #alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0)\n          userCol=\"user_id\", itemCol=\"primary_video_id\", ratingCol=\"watching_percentage\",\n          numUserBlocks=32, numItemBlocks=32,\n          coldStartStrategy=\"drop\")\n\nmodel = als.fit(ratings)","cc1c41b7":"%%time\nuserRecsDf = model.recommendForAllUsers(10).cache()\nuserRecsDf.count()","20ced608":"userRecs = userRecsDf.toPandas()\nuserRecs.shape","723baefa":"userRecs[:2]","3d6a9caf":"predicted_dict = userRecs.set_index('user_id').to_dict('index')\npredicted_dict = {user_id:[r[0] for r in recs['recommendations']] for user_id, recs in predicted_dict.items()}\nlen(predicted_dict)","a1aa66f6":"sample_submission = pd.read_csv('..\/input\/sample_submission_full.csv')","01283cbb":"sample_submission['als_predicted_primary_video_id'] = sample_submission.user_id.apply(\n    lambda user_id: ' '.join([str(v) for v in predicted_dict[user_id]]) if user_id in predicted_dict else None)","f8e3394c":"sample_submission[:5]","f40762a8":"sample_submission['primary_video_id'] = sample_submission.als_predicted_primary_video_id.combine_first(\n    sample_submission.primary_video_id)\ndel sample_submission['als_predicted_primary_video_id']","4e0c2211":"sample_submission.to_csv('sample_submission_full_als.csv',\n                         header=True, index=False)","1609a08a":"train_data = pd.read_csv('..\/input\/train_data_full.csv')\ntrain_needed_users =  train_data[train_data.user_id.isin(sample_submission.user_id)]\nusers_with_history = list(set(train_needed_users.user_id))\ncold_users = list(set(sample_submission.user_id) - set(users_with_history))\nprint('number of users presented in history: ', len(users_with_history), ' % of users with hist data: ',len(users_with_history)\/len(sample_submission.user_id))\nprint('number of cold start users ', len(cold_users), ' % of users without hist data: ', len(cold_users)\/len(sample_submission.user_id))","d7506869":"top_10_videos = train_data[train_data['watching_percentage']>=0.5].loc[train_data.session_start_datetime >= '2018-09-20 00:00:00', # Supposing that 10 days closest to testing period is most representative\n                               'primary_video_id'].value_counts()[:10].index.tolist()","31748e94":"sample2 = sample_submission.copy()\nsample2['forcold'] = ' '.join([str(v) for v in top_10_videos])","f9ee25f1":"sample2.loc[sample2.user_id.isin(cold_users),'primary_video_id'] = sample2['forcold'][1]","33312a46":"del sample2['forcold']","88c85b75":"sample2.head()","6f176e56":"sample2.to_csv('sparkasl_withcold.csv', #forming another file, with basic processing of cold users \n                         header=True, index=False)","0a9d7db2":"** reading test data ** ","ddd3f0b9":"** training model **","0fce945a":"** now, let's make some very basic approach to proceed with cold users **","6a1b7fe9":"There is 2 ways to do it here. \n* Run magic command, as in cell below\n* Or go to Packages and enter pyspark (it will take some time, but this will form your own version of docker and you'll no need to waste time while running next versions of kernel)\n","b2d01ac5":"## Installing pyspark","0d5fd794":"## Conclusion\n\nIt's very nice that Kaggle Kernels allow to play out with Spark libs, which is giving quite good result in a competition. \nAlso many thanks to the organizers for sharing baselines. \n\nHope this kernel was useful as very short introduction to Spark MLib for recommendation task.\n\nPlease share your approach for this type of task in discussion. ","9589d590":"** made predicts **","ba664a14":"Now importing all needed spark modules. Pay attention how sc and spark variables were initialized. ","18dc5adf":"** let's read data **","fa40c127":"This will form results very close to shared baseline. With maxIter = 10 it will be almost same. ","a54ef35a":"## General information\n\nIn this notebook I am going to show how Spark MLlib can be used for movie recommendation right from Kaggle Kernels.\n\nDataset is taken from [this](https:\/\/www.kaggle.com\/c\/megogochallenge) challenge. \n\nTask was to recommend 10 movies for each user from test set. \nMetric is [MAP@10](http:\/\/sdsawtelle.github.io\/blog\/output\/mean-average-precision-MAP-for-recommender-systems.html). ","f2b4d9dd":"### Why spark? \n\nAt the moment this competition starts I have no any practical expirience in building recommendation systems. As well as in spark, by the way. \n\nOf course I read some great articles on the subject, including [this one](https:\/\/medium.com\/@james_aka_yale\/the-4-recommendation-engines-that-can-predict-your-movie-tastes-bbec857b8223) and [this kernel](https:\/\/www.kaggle.com\/ibtesama\/getting-started-with-a-movie-recommendation-system), but when real task has come I have no idea what to do.\nIt was good for me that organizers decided to share some [baselines](https:\/\/github.com\/SantyagoSeaman\/megogo_challenge_solutions). \n\nBest of it by score was a [Spark](https:\/\/spark.apache.org\/docs\/2.2.0\/ml-collaborative-filtering.html) one, so after trying to play with first two baselines I decided to reproduce it.\nI have few not really great hours looking how my local jupyter notebook freezes after I launch it. And that's how idea to run it in Kaggle comes to me. \n\nI tried to google some kernels, but there was no single one. That's why I write this one."}}