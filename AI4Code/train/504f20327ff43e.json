{"cell_type":{"6da568f4":"code","1dc79c2a":"code","c0639b30":"code","ea2fe37e":"code","44fae499":"code","51f53e08":"code","9d5adeb5":"markdown"},"source":{"6da568f4":"import os\nimport glob\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)","1dc79c2a":"import numpy as np\nfrom keras import backend as K\nfrom keras.layers import Concatenate\nfrom keras.layers import Input, Dense, Lambda, Subtract, Add, Reshape\nfrom keras.models import Model\n\n\nclass NBeatsNet:\n    GENERIC_BLOCK = 'generic'\n    TREND_BLOCK = 'trend'\n    SEASONALITY_BLOCK = 'seasonality'\n\n    _BACKCAST = 'backcast'\n    _FORECAST = 'forecast'\n\n    def __init__(self,\n                 input_dim=1,\n                 exo_dim=0,\n                 backcast_length=10,\n                 forecast_length=2,\n                 stack_types=(TREND_BLOCK, SEASONALITY_BLOCK),\n                 nb_blocks_per_stack=3,\n                 thetas_dim=(4, 8),\n                 share_weights_in_stack=False,\n                 hidden_layer_units=256,\n                 nb_harmonics=None):\n\n        self.stack_types = stack_types\n        self.nb_blocks_per_stack = nb_blocks_per_stack\n        self.thetas_dim = thetas_dim\n        self.units = hidden_layer_units\n        self.share_weights_in_stack = share_weights_in_stack\n        self.backcast_length = backcast_length\n        self.forecast_length = forecast_length\n        self.input_dim = input_dim\n        self.exo_dim = exo_dim\n        self.input_shape = (self.backcast_length, self.input_dim)\n        self.exo_shape = (self.backcast_length, self.exo_dim)\n        self.output_shape = (self.forecast_length, self.input_dim)\n        self.weights = {}\n        self.nb_harmonics = nb_harmonics\n        assert len(self.stack_types) == len(self.thetas_dim)\n\n        x = Input(shape=self.input_shape, name='input_variable')\n        x_ = {}\n        for k in range(self.input_dim):\n            x_[k] = Lambda(lambda z: z[..., k])(x)\n        e_ = {}\n        if self.has_exog():\n            e = Input(shape=self.exo_shape, name='exos_variables')\n            for k in range(self.exo_dim):\n                e_[k] = Lambda(lambda z: z[..., k])(e)\n        else:\n            e = None\n        y_ = {}\n\n        for stack_id in range(len(self.stack_types)):\n            stack_type = self.stack_types[stack_id]\n            nb_poly = self.thetas_dim[stack_id]\n            for block_id in range(self.nb_blocks_per_stack):\n                backcast, forecast = self.create_block(x_, e_, stack_id, block_id, stack_type, nb_poly)\n                for k in range(self.input_dim):\n                    x_[k] = Subtract()([x_[k], backcast[k]])\n                    if stack_id == 0 and block_id == 0:\n                        y_[k] = forecast[k]\n                    else:\n                        y_[k] = Add()([y_[k], forecast[k]])\n\n        for k in range(self.input_dim):\n            y_[k] = Reshape(target_shape=(self.forecast_length, 1))(y_[k])\n            x_[k] = Reshape(target_shape=(self.backcast_length, 1))(x_[k])\n        if self.input_dim > 1:\n            y_ = Concatenate()([y_[ll] for ll in range(self.input_dim)])\n            x_ = Concatenate()([x_[ll] for ll in range(self.input_dim)])\n        else:\n            y_ = y_[0]\n            x_ = x_[0]\n\n        if self.has_exog():\n            n_beats_forecast = Model([x, e], y_, name=self._FORECAST)\n            n_beats_backcast = Model([x, e], x_, name=self._BACKCAST)\n        else:\n            n_beats_forecast = Model(x, y_, name=self._FORECAST)\n            n_beats_backcast = Model(x, x_, name=self._BACKCAST)\n\n        self.models = {model.name: model for model in [n_beats_backcast, n_beats_forecast]}\n        self.cast_type = self._FORECAST\n\n    def has_exog(self):\n        # exo\/exog is short for 'exogenous variable', i.e. any input\n        # features other than the target time-series itself.\n        return self.exo_dim > 0\n\n    @staticmethod\n    def load(filepath, custom_objects=None, compile=True):\n        from tensorflow.keras.models import load_model\n        return load_model(filepath, custom_objects, compile)\n\n    def _r(self, layer_with_weights, stack_id):\n        # mechanism to restore weights when block share the same weights.\n        # only useful when share_weights_in_stack=True.\n        if self.share_weights_in_stack:\n            layer_name = layer_with_weights.name.split('\/')[-1]\n            try:\n                reused_weights = self.weights[stack_id][layer_name]\n                return reused_weights\n            except KeyError:\n                pass\n            if stack_id not in self.weights:\n                self.weights[stack_id] = {}\n            self.weights[stack_id][layer_name] = layer_with_weights\n        return layer_with_weights\n\n    def create_block(self, x, e, stack_id, block_id, stack_type, nb_poly):\n        # register weights (useful when share_weights_in_stack=True)\n        def reg(layer):\n            return self._r(layer, stack_id)\n\n        # update name (useful when share_weights_in_stack=True)\n        def n(layer_name):\n            return '\/'.join([str(stack_id), str(block_id), stack_type, layer_name])\n\n        backcast_ = {}\n        forecast_ = {}\n        d1 = reg(Dense(self.units, activation='relu', name=n('d1')))\n        d2 = reg(Dense(self.units, activation='relu', name=n('d2')))\n        d3 = reg(Dense(self.units, activation='relu', name=n('d3')))\n        d4 = reg(Dense(self.units, activation='relu', name=n('d4')))\n        if stack_type == 'generic':\n            theta_b = reg(Dense(nb_poly, activation='linear', use_bias=False, name=n('theta_b')))\n            theta_f = reg(Dense(nb_poly, activation='linear', use_bias=False, name=n('theta_f')))\n            backcast = reg(Dense(self.backcast_length, activation='linear', name=n('backcast')))\n            forecast = reg(Dense(self.forecast_length, activation='linear', name=n('forecast')))\n        elif stack_type == 'trend':\n            theta_f = theta_b = reg(Dense(nb_poly, activation='linear', use_bias=False, name=n('theta_f_b')))\n            backcast = Lambda(trend_model, arguments={'is_forecast': False, 'backcast_length': self.backcast_length,\n                                                      'forecast_length': self.forecast_length})\n            forecast = Lambda(trend_model, arguments={'is_forecast': True, 'backcast_length': self.backcast_length,\n                                                      'forecast_length': self.forecast_length})\n        else:  # 'seasonality'\n            if self.nb_harmonics:\n                theta_b = reg(Dense(self.nb_harmonics, activation='linear', use_bias=False, name=n('theta_b')))\n            else:\n                theta_b = reg(Dense(self.forecast_length, activation='linear', use_bias=False, name=n('theta_b')))\n            theta_f = reg(Dense(self.forecast_length, activation='linear', use_bias=False, name=n('theta_f')))\n            backcast = Lambda(seasonality_model,\n                              arguments={'is_forecast': False, 'backcast_length': self.backcast_length,\n                                         'forecast_length': self.forecast_length})\n            forecast = Lambda(seasonality_model,\n                              arguments={'is_forecast': True, 'backcast_length': self.backcast_length,\n                                         'forecast_length': self.forecast_length})\n        for k in range(self.input_dim):\n            if self.has_exog():\n                d0 = Concatenate()([x[k]] + [e[ll] for ll in range(self.exo_dim)])\n            else:\n                d0 = x[k]\n            d1_ = d1(d0)\n            d2_ = d2(d1_)\n            d3_ = d3(d2_)\n            d4_ = d4(d3_)\n            theta_f_ = theta_f(d4_)\n            theta_b_ = theta_b(d4_)\n            backcast_[k] = backcast(theta_b_)\n            forecast_[k] = forecast(theta_f_)\n\n        return backcast_, forecast_\n\n    def __getattr__(self, name):\n        # https:\/\/github.com\/faif\/python-patterns\n        # model.predict() instead of model.n_beats.predict()\n        # same for fit(), train_on_batch()...\n        attr = getattr(self.models[self._FORECAST], name)\n\n        if not callable(attr):\n            return attr\n\n        def wrapper(*args, **kwargs):\n            cast_type = self._FORECAST\n            if attr.__name__ == 'predict' and 'return_backcast' in kwargs and kwargs['return_backcast']:\n                del kwargs['return_backcast']\n                cast_type = self._BACKCAST\n            return getattr(self.models[cast_type], attr.__name__)(*args, **kwargs)\n\n        return wrapper\n\n\ndef linear_space(backcast_length, forecast_length, is_forecast=True):\n    ls = K.arange(-float(backcast_length), float(forecast_length), 1) \/ forecast_length\n    return ls[backcast_length:] if is_forecast else ls[:backcast_length]\n\n\ndef seasonality_model(thetas, backcast_length, forecast_length, is_forecast):\n    p = thetas.get_shape().as_list()[-1]\n    p1, p2 = (p \/\/ 2, p \/\/ 2) if p % 2 == 0 else (p \/\/ 2, p \/\/ 2 + 1)\n    t = linear_space(backcast_length, forecast_length, is_forecast=is_forecast)\n    s1 = K.stack([K.cos(2 * np.pi * i * t) for i in range(p1)])\n    s2 = K.stack([K.sin(2 * np.pi * i * t) for i in range(p2)])\n    if p == 1:\n        s = s2\n    else:\n        s = K.concatenate([s1, s2], axis=0)\n    s = K.cast(s, np.float32)\n    return K.dot(thetas, s)\n\n\ndef trend_model(thetas, backcast_length, forecast_length, is_forecast):\n    p = thetas.shape[-1]\n    t = linear_space(backcast_length, forecast_length, is_forecast=is_forecast)\n    t = K.transpose(K.stack([t ** i for i in range(p)]))\n    t = K.cast(t, np.float32)\n    return K.dot(thetas, K.transpose(t))","c0639b30":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std]\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\n\n\n# Read train and test\ntrain, test = read_train_test()\n\n# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","ea2fe37e":"def train_and_evaluate(train, test):\n    # Split features and target\n    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n    y = train['target']\n    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n    # Transform stock id to a numeric value\n    x['stock_id'] = x['stock_id'].astype(int)\n    x_test['stock_id'] = x_test['stock_id'].astype(int)\n    x_test = x_test.values.reshape(x_test.shape[0], 1, x_test.shape[1])\n    x = x.values.reshape(x.shape[0], 1, x.shape[1])\n    \n    # Create out of folds array\n    oof_predictions = np.zeros((x.shape[0], 3, 1))\n    # Create test array to store predictions\n    test_predictions = np.zeros((x_test.shape[0], 3, 1))\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = x[trn_ind], x[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        print(x_val.shape, x_train.shape, y_val.shape, y_train.shape)\n        \n        model = NBeatsNet(\n            backcast_length=1, forecast_length=3, input_dim=x_train.shape[1],\n            stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n            nb_blocks_per_stack=2, thetas_dim=(4, 4), share_weights_in_stack=True,\n            hidden_layer_units=64\n        )\n        model.compile(loss='mse', optimizer='adam')\n        print('Training...')\n        model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=20, batch_size=1024)\n        print(model.predict(x_test))\n        model.save(f'n_beats_model_fixed{fold}.h5')\n        \n\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val)\n        # Predict the test set\n        test_predictions += model.predict(x_test) \/ 5\n    del model, train, test, x, x_train, x_val, y_train, y_val\n#     rmspe_score = rmspe(y, oof_predictions)\n#     print(f'Our out of folds RMSPE is {rmspe_score}')\n    # Return test predictions\n    return test_predictions","44fae499":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2  \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)  \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","51f53e08":"# Traing and evaluate\ntest_predictions = train_and_evaluate(train, test)\n# Save test predictions\ntest['target'] = test_predictions[0]\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","9d5adeb5":"This is a fork of this notebook https:\/\/www.kaggle.com\/ragnar123\/optiver-realized-volatility-lgbm-baseline but I have optimized the LGB params."}}