{"cell_type":{"99fbef0a":"code","f52585c6":"code","46a692bb":"code","d826639d":"code","c5c61a35":"code","692ab618":"code","9a17bf26":"code","4397f4b3":"code","14e2a863":"code","d1c83143":"code","c3e8f997":"code","cc33a636":"code","fdbb403a":"code","cb59bc91":"code","880c3791":"code","38b7d0d8":"code","d5bb50a0":"code","203f6beb":"code","34e04c3d":"code","d3e1f310":"code","72b8140a":"code","78d14b6e":"code","e0a97282":"code","62e42edd":"code","f220a878":"code","cf9afd84":"code","8083d9c3":"code","1d5e0279":"code","a341d89d":"code","3dd5ab16":"code","c7cafbe5":"code","0c7566d6":"code","f11508ba":"code","c3cb9c59":"code","492b4a5d":"code","a581bfbf":"code","4811404f":"code","57ecddb7":"code","a7074687":"code","82472659":"code","b4c42934":"code","b31583f0":"code","bca91e17":"code","c1900079":"code","79eccf30":"code","7ceed86d":"code","00ea8108":"code","c72ebf78":"code","1c79947e":"code","06977e50":"code","df36a2e2":"markdown","3b329f99":"markdown","0fc244da":"markdown","29be69c1":"markdown","65ec944b":"markdown","061fa399":"markdown","629c1ee6":"markdown","30618f34":"markdown","b8a145a2":"markdown","503645b5":"markdown","dd04a977":"markdown","18223be2":"markdown","a715480f":"markdown","ab819fa4":"markdown","f8de9cd6":"markdown","ea2b92d8":"markdown","5605f971":"markdown","b516b056":"markdown","469de015":"markdown","ba3042a4":"markdown","40969a0b":"markdown","1239342d":"markdown"},"source":{"99fbef0a":"# imports\nimport torch\nimport gc\nfrom pytorch_utils_for_images import *","f52585c6":"SEED = 42\n\nset_global_seed(SEED)\nprepare_cudnn(deterministic=True)","46a692bb":"task = 'classification'\nencoder = 'densenet169'\nencoder_weights = 'imagenet'\nbatch_size = 8\npath = '..\/input\/understanding_cloud_organization'\nnum_workers = 0\nsegm_type = 'Unet'\nactivation = None\nn_classes = 4\nloss = 'BCE'\ngradient_accumulation = 'False'\nnum_epochs = 20\nlr = 1e-4","d826639d":"preprocessing_fn = smp.encoders.get_preprocessing_fn(encoder, encoder_weights)\nloaders = prepare_loaders(path=path, bs=batch_size,\n                          num_workers=num_workers, preprocessing_fn=preprocessing_fn, preload=False, task=task,\n                          image_size=(224, 224))\ntest_loader = loaders['test']\ndel loaders['test']\n\nmodel = get_model(model_type=segm_type, encoder=encoder, encoder_weights=encoder_weights,\n                  activation=activation, task=task, n_classes=n_classes, head='simple')\n","c5c61a35":"optimizer = get_optimizer(optimizer='RAdam', lookahead=False, model=model, separate_decoder=False, lr=lr, lr_e=lr)","692ab618":"scheduler = ReduceLROnPlateau(optimizer, factor=0.7, patience=2)\ncriterion = get_loss(loss)\ncriterion","9a17bf26":"if task == 'segmentation':\n    callbacks = [DiceCallback(), EarlyStoppingCallback(patience=5, min_delta=0.001), CriterionCallback()]\nelif task == 'classification':\n    callbacks = [AUCCallback(class_names=['Fish', 'Flower', 'Gravel', 'Sugar'], num_classes=4), EarlyStoppingCallback(patience=5, min_delta=0.001), CriterionCallback(), CustomCheckpointCallback()]\n\nrunner = SupervisedRunner()","4397f4b3":"train = pd.read_csv('..\/input\/understanding_cloud_organization\/train.csv')\ntrain['label'] = train['Image_Label'].apply(lambda x: x.split('_')[1])\ntrain['im_id'] = train['Image_Label'].apply(lambda x: x.split('_')[0])","14e2a863":"image_name = '8242ba0.jpg'\nimage = get_img(image_name, '..\/input\/understanding_cloud_organization\/train_images')\nmask = make_mask(train, image_name)","d1c83143":"plot_with_augmentation(image, mask, albu.HorizontalFlip(p=1))","c3e8f997":"plot_with_augmentation(image, mask, albu.Blur(p=1))","cc33a636":"plot_with_augmentation(image, mask, albu.CLAHE(p=1))","fdbb403a":"plot_with_augmentation(image, mask, albu.ShiftScaleRotate(p=1))","cb59bc91":"for i, param in list((model.named_parameters()))[:-10]:\n    param.requires_grad = False","880c3791":"logdir = '.\/logs\/classification'\n\n#num_epochs = 1\nrunner.train(\n            model=model,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            loaders=loaders,\n            callbacks=callbacks,\n            logdir=logdir,\n            num_epochs=num_epochs,\n            verbose=True\n        )","38b7d0d8":"plot_metrics(\n    logdir='..\/input\/cloud-segmentation-model', \n    # specify which metrics we want to plot\n    metrics=[\"loss\", \"auc\/_mean\", \"auc\/class_Fish\", \"auc\/class_Flower\", \"auc\/class_Gravel\", \"auc\/class_Sugar\", \"_base\/lr\"]\n)","d5bb50a0":"valid_predictions = runner.predict_loader(\n    model, loaders[\"valid\"],\n    resume=f\"{logdir}\/checkpoints\/best.pth\", verbose=True\n)\n\ny_valid = []\nfor img, label in loaders[\"valid\"].dataset:\n    y_valid.append(label)\ny_valid = np.array(y_valid)","203f6beb":"valid_predictions = sigmoid(valid_predictions)","34e04c3d":"class_dict = {0: 'Fish', 1: 'Flower', 2: 'Gravel', 3: 'Sugar'}\nfig, ax = plt.subplots(figsize = (16, 12))\nfor i in range(4):\n    plt.subplot(2, 2, i + 1)\n    plot_precision_recall(y_valid[:, i], valid_predictions[:, i], title=class_dict[i])","d3e1f310":"class_thresholds = {}\nfor i in range(4):\n    print(f\"Class: {class_dict[i]}\")\n    t = find_threshold(y_valid[:, i], valid_predictions[:, i])\n    print()\n    class_thresholds[i] = t","72b8140a":"def get_threshold_for_recall(y_true, y_pred, class_i, recall_threshold=0.95, precision_threshold=0.94, plot=False):\n    precision, recall, thresholds = metrics.precision_recall_curve(y_true[:, class_i], y_pred[:, class_i])\n    i = len(thresholds) - 1\n    best_recall_threshold = None\n    while best_recall_threshold is None:\n        next_threshold = thresholds[i]\n        next_recall = recall[i]\n        if next_recall >= recall_threshold:\n            best_recall_threshold = next_threshold\n        i -= 1\n        \n    # consice, even though unnecessary passing through all the values\n    best_precision_threshold = [thres for prec, thres in zip(precision, thresholds) if prec >= precision_threshold][0]\n    \n    if plot:\n        plt.figure(figsize=(10, 7))\n        plt.step(recall, precision, color='r', alpha=0.3, where='post')\n        plt.fill_between(recall, precision, alpha=0.3, color='r')\n        plt.axhline(y=precision[i + 1])\n        recall_for_prec_thres = [rec for rec, thres in zip(recall, thresholds) \n                                 if thres == best_precision_threshold][0]\n        plt.axvline(x=recall_for_prec_thres, color='g')\n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.ylim([0.0, 1.05])\n        plt.xlim([0.0, 1.0])\n        plt.legend(['PR curve', \n                    f'Precision {precision[i + 1]: .2f} corresponding to selected recall threshold',\n                    f'Recall {recall_for_prec_thres: .2f} corresponding to selected precision threshold'])\n        plt.title(f'Precision-Recall curve for Class {class_dict[class_i]}')\n    return best_recall_threshold, best_precision_threshold\n\nrecall_thresholds = dict()\nprecision_thresholds = dict()\nfor i, class_name in tqdm.tqdm(enumerate(class_dict.items())):\n    recall_thresholds[class_name], precision_thresholds[class_name] = get_threshold_for_recall(y_valid, valid_predictions, i, plot=True)\n    \nprint('recall_thresholds', recall_thresholds)","78d14b6e":"recall_thresholds = {k[0]: v for k, v in recall_thresholds.items()}","e0a97282":"# freeing memory\ntorch.cuda.empty_cache()\ngc.collect()\ndel runner","62e42edd":"task = 'segmentation'\nencoder = 'resnet50'\nbatch_size = 8\n\nmodel = get_model(model_type=segm_type, encoder=encoder, encoder_weights=encoder_weights,\n                  activation=activation, task=task, n_classes=n_classes, head='custom')\nloaders = prepare_loaders(path=path, bs=batch_size,\n                          num_workers=num_workers, preprocessing_fn=preprocessing_fn, preload=False, task=task)\n\ndel loaders['train']\ndel loaders['test']\ncheckpoint_path = '..\/input\/cloud-segmentation-model\/best_full.pth'\ncheckpoint = utils.load_checkpoint(checkpoint_path)\nmodel.cuda()\nutils.unpack_checkpoint(checkpoint, model=model)\nrunner = SupervisedRunner(model=model)","f220a878":"loaders = {\"infer\": loaders['valid']}\nrunner.infer(\n    model=runner.model,\n    loaders=loaders,\n    callbacks=[\n        CheckpointCallback(\n            resume=checkpoint_path),\n        InferCallback()\n    ],\n)\n\nvalid_masks = []\nprobabilities = np.zeros((len(runner.callbacks[0].predictions['logits']) * 4, 350, 525))\nfor i, (batch, output) in enumerate(zip(\n    loaders['infer'].dataset, runner.callbacks[0].predictions[\"logits\"])):\n    image, mask = batch\n    for m in mask:\n        if m.shape != (350, 525):\n            m = cv2.resize(m, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n        valid_masks.append(m)\n\n    for j, probability in enumerate(output):\n        if probability.shape != (350, 525):\n            probability = cv2.resize(probability, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n        probabilities[i * 4 + j, :, :] = probability","cf9afd84":"valid_ids = loaders['infer'].dataset.img_ids","8083d9c3":"original_dices = []\nd = []\nfor ind, (i, j) in enumerate(zip(probabilities, valid_masks)):\n    if (i.sum() == 0) & (j.sum() == 0):\n        d.append(1)\n    else:\n        d.append(dice(i, j))\n    if len(d) == 4:\n        d = [valid_ids[ind \/\/ 4]] + d\n        original_dices.append(d)\n        d = []","1d5e0279":"original_dices = pd.DataFrame(original_dices)\noriginal_dices.columns = ['img', 'Fish', 'Flower', 'Gravel', 'Sugar']\noriginal_dices['total_dice'] = original_dices[['Fish', 'Flower', 'Gravel', 'Sugar']].sum(1)\noriginal_dices['mean_dice'] = original_dices['total_dice'] \/ 4\noriginal_dices.head()","a341d89d":"for c in ['Fish', 'Flower', 'Gravel', 'Sugar']:\n    print(f\"Mean dice for {c} is {original_dices[c].mean():.4f}\")","3dd5ab16":"class_params = {}\nfor class_id in range(4):\n    print(class_id)\n    attempts = []\n    for t in range(0, 100, 10):\n        t \/= 100\n        for ms in [0, 100, 1000, 5000, 10000, 11000, 14000, 15000, 16000, 18000, 19000, 20000, 21000, 23000, 25000, 27000, 30000, 50000]:\n            masks = []\n            for i in range(class_id, len(probabilities), 4):\n                probability = probabilities[i]\n                predict, num_predict = post_process(sigmoid(probability), t, ms)\n                masks.append(predict)\n\n            d = []\n            for i, j in zip(masks, valid_masks[class_id::4]):\n                if (i.sum() == 0) & (j.sum() == 0):\n                    d.append(1)\n                else:\n                    d.append(dice(i, j))\n\n            attempts.append((t, ms, np.mean(d)))\n\n    attempts_df = pd.DataFrame(attempts, columns=['threshold', 'size', 'dice'])\n\n    attempts_df = attempts_df.sort_values('dice', ascending=False)\n    print(attempts_df.head())\n    best_threshold = attempts_df['threshold'].values[0]\n    best_size = attempts_df['size'].values[0]\n\n    class_params[class_id] = (best_threshold, best_size)\n\nprint(class_params)","c7cafbe5":"processed_dices = []\nd = []\nfor ind, (i, j) in enumerate(zip(probabilities, valid_masks)):\n    i, num_predict = post_process(sigmoid(i), class_params[ind % 4][0],\n                                                       class_params[ind % 4][1])\n    if (i.sum() == 0) & (j.sum() == 0):\n        d.append(1)\n    else:\n        d.append(dice(i, j))\n    if len(d) == 4:\n        d = [valid_ids[ind \/\/ 4]] + d\n        processed_dices.append(d)\n        d = []","0c7566d6":"processed_dices = pd.DataFrame(processed_dices)\nprocessed_dices.columns = ['img', 'Fish', 'Flower', 'Gravel', 'Sugar']\nprocessed_dices['total_dice'] = processed_dices[['Fish', 'Flower', 'Gravel', 'Sugar']].sum(1)\nprocessed_dices['mean_dice'] = processed_dices['total_dice'] \/ 4\nprocessed_dices.head()","f11508ba":"for c in ['Fish', 'Flower', 'Gravel', 'Sugar']:\n    print(f\"Mean dice for {c} is {processed_dices[c].mean():.4f}\")","c3cb9c59":"fig, ax = plt.subplots(figsize = (12, 6))\nplt.subplot(1, 2, 1)\nplt.hist(original_dices['Fish'], label='Fish');\nplt.hist(original_dices['Flower'], label='Flower');\nplt.hist(original_dices['Gravel'], label='Gravel');\nplt.hist(original_dices['Sugar'], label='Sugar');\nplt.title('Original dices');\n\nplt.subplot(1, 2, 2)\nplt.hist(processed_dices['Fish'], label='Fish');\nplt.hist(processed_dices['Flower'], label='Flower');\nplt.hist(processed_dices['Gravel'], label='Gravel');\nplt.hist(processed_dices['Sugar'], label='Sugar');\nplt.title('Processed dices');\nplt.legend();","492b4a5d":"processed_dices.columns = [f'{col}_processed' for col in processed_dices.columns]\ndices = pd.merge(original_dices, processed_dices, left_on='img', right_on='img_processed')\ndices['dice_diff'] = dices['mean_dice_processed'] - dices['mean_dice']","a581bfbf":"dices.sort_values('dice_diff').head()","4811404f":"dices.sort_values('dice_diff', ascending=False).head()","57ecddb7":"# converting probabilities from classifier to labels\nbinary_classifier_predictions = np.zeros_like(valid_predictions)\nfor i in range(4):\n    binary_classifier_predictions[:, i] = (valid_predictions[:, i] > class_thresholds[i]) * 1\n    \nbinary_classifier_predictions = binary_classifier_predictions.reshape(-1, 1)","a7074687":"classified_dices = []\nd = []\nfor ind, (i, j) in enumerate(zip(probabilities, valid_masks)):\n    i, num_predict = post_process(sigmoid(i), class_params[ind % 4][0],\n                                                       class_params[ind % 4][1])\n    i = i if binary_classifier_predictions[ind] == 1 else i * 0\n    if (i.sum() == 0) & (j.sum() == 0):\n        d.append(1)\n    else:\n        d.append(dice(i, j))\n    if len(d) == 4:\n        d = [valid_ids[ind \/\/ 4]] + d\n        classified_dices.append(d)\n        d = []","82472659":"classified_dices = pd.DataFrame(classified_dices)\nclassified_dices.columns = ['img', 'Fish', 'Flower', 'Gravel', 'Sugar']\nclassified_dices['total_dice'] = classified_dices[['Fish', 'Flower', 'Gravel', 'Sugar']].sum(1)\nclassified_dices['mean_dice'] = classified_dices['total_dice'] \/ 4\nclassified_dices.head()","b4c42934":"for c in ['Fish', 'Flower', 'Gravel', 'Sugar']:\n    print(f\"Mean dice for {c} is {classified_dices[c].mean():.4f}\")","b31583f0":"# converting probabilities from classifier to labels\nbinary_classifier_predictions = np.zeros_like(valid_predictions)\nfor i in range(4):\n    binary_classifier_predictions[:, i] = (valid_predictions[:, i] > recall_thresholds[i]) * 1\n    \nbinary_classifier_predictions = binary_classifier_predictions.reshape(-1, 1)\n\nclassified_dices = []\nd = []\nfor ind, (i, j) in enumerate(zip(probabilities, valid_masks)):\n    i, num_predict = post_process(sigmoid(i), class_params[ind % 4][0],\n                                                       class_params[ind % 4][1])\n    i = i if binary_classifier_predictions[ind] == 1 else i * 0\n    if (i.sum() == 0) & (j.sum() == 0):\n        d.append(1)\n    else:\n        d.append(dice(i, j))\n    if len(d) == 4:\n        d = [valid_ids[ind \/\/ 4]] + d\n        classified_dices.append(d)\n        d = []\n        \nclassified_dices = pd.DataFrame(classified_dices)\nclassified_dices.columns = ['img', 'Fish', 'Flower', 'Gravel', 'Sugar']\nclassified_dices['total_dice'] = classified_dices[['Fish', 'Flower', 'Gravel', 'Sugar']].sum(1)\nclassified_dices['mean_dice'] = classified_dices['total_dice'] \/ 4\nclassified_dices.head()","bca91e17":"for c in ['Fish', 'Flower', 'Gravel', 'Sugar']:\n    print(f\"Mean dice for {c} is {classified_dices[c].mean():.4f}\")","c1900079":"# predict with classifier\ntorch.cuda.empty_cache()\ndel runner\n\ntask = 'classification'\nencoder = 'densenet169'\nbatch_size = 8\n\nmodel = get_model(model_type=segm_type, encoder=encoder, encoder_weights=encoder_weights,\n                  activation=activation, task=task, n_classes=n_classes)\nloaders = prepare_loaders(path=path, bs=batch_size,\n                          num_workers=num_workers, preprocessing_fn=preprocessing_fn, preload=False, task=task,\n                          image_size=(224, 224))\ndel loaders['train']\ndel loaders['valid']\ncheckpoint_path = f\"{logdir}\/checkpoints\/best.pth\"\nrunner = SupervisedRunner()\ntest_predictions = runner.predict_loader(\n    model, loaders[\"test\"],\n    resume=checkpoint_path, verbose=True\n)","79eccf30":"# convert classifier predictions into labels\nbinary_classifier_predictions = np.zeros_like(test_predictions)\nfor i in range(4):\n    binary_classifier_predictions[:, i] = (test_predictions[:, i] > class_thresholds[i]) * 1\n    \nbinary_classifier_predictions = binary_classifier_predictions.reshape(-1, 1)","7ceed86d":"binary_classifier_predictions1 = np.zeros_like(test_predictions)\nfor i in range(4):\n    binary_classifier_predictions1[:, i] = (test_predictions[:, i] > recall_thresholds[i]) * 1\n    \nbinary_classifier_predictions1 = binary_classifier_predictions1.reshape(-1, 1)","00ea8108":"torch.cuda.empty_cache()\ndel runner\ndel test_predictions\ndel probabilities\ndel model\ngc.collect()","c72ebf78":"task = 'segmentation'\nencoder = 'resnet50'\nbatch_size = 8\n\nmodel = get_model(model_type=segm_type, encoder=encoder, encoder_weights=encoder_weights,\n                  activation=activation, task=task, n_classes=n_classes)\nloaders = prepare_loaders(path=path, bs=batch_size,\n                          num_workers=num_workers, preprocessing_fn=preprocessing_fn, preload=False, task=task)","1c79947e":"loaders['test'] = test_loader\ncheckpoint_path = '..\/input\/cloud-segmentation-model\/best_full.pth'\ncheckpoint = utils.load_checkpoint(checkpoint_path)\nmodel.cuda()\nutils.unpack_checkpoint(checkpoint, model=model)\nrunner = SupervisedRunner(model=model)","06977e50":"encoded_pixels = []\nencoded_pixels1 = []\nimage_id = 0\nfor _, test_batch in enumerate(loaders['test']):\n    runner_out = runner.predict_batch({\"features\": test_batch[0].cuda()})['logits']\n    for _, batch in enumerate(runner_out):\n        for probability in batch:\n\n            probability = probability.cpu().detach().numpy()\n            if probability.shape != (350, 525):\n                probability = cv2.resize(probability, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n                prediction, num_predict = post_process(sigmoid(probability), class_params[image_id % 4][0],\n                                                   class_params[image_id % 4][1])\n                prediction = prediction if binary_classifier_predictions[image_id] == 1 else prediction * 0\n                prediction1 = prediction if binary_classifier_predictions1[image_id] == 1 else prediction * 0\n            if num_predict == 0:\n                encoded_pixels.append('')\n                encoded_pixels1.append('')\n            else:\n                r = mask2rle(prediction)\n                encoded_pixels.append(r)\n                r1 = mask2rle(prediction1)\n                encoded_pixels1.append(r1)\n            image_id += 1\n\nsub = pd.read_csv(f'{path}\/sample_submission.csv')\nsub['EncodedPixels'] = encoded_pixels\nsub.to_csv(f'submission.csv', columns=['Image_Label', 'EncodedPixels'], index=False)\nsub['EncodedPixels'] = encoded_pixels1\nsub.to_csv(f'submission1.csv', columns=['Image_Label', 'EncodedPixels'], index=False)","df36a2e2":"Let's get predictions and the original labels.","3b329f99":"We can see not only mean AUC, but also AUC for each class.","0fc244da":"## Using different classifier","29be69c1":"Of course, now dice is much better.","65ec944b":"It seems that the main improvement is thanks to making some masks empty correctly! But is this always so? Let's have a look at images which had mean dice increased\/decreased the most.","061fa399":"For comparison I also want to try a function from this kernel: https:\/\/www.kaggle.com\/samusram\/cloud-classifier-for-post-processing","629c1ee6":"It seems that our post processing sometimes created additional problems. One of the ways to fix it is to apply a classifier for post-processing.","30618f34":"Now I'm setting various parameters for functions. Description for most of these parameters can be found in my utility script.\n\nWhen I train models locally, I prefer to either run the code in command line and pass parameters there or to set up config files.\n\nWith this approach it is easy to switch between classification and segmentation tasks, between model architectures and encoders.","b8a145a2":"## Preparing for training\n\nNow we need to get everything necessary for training the model:\n- get data loaders with training and validation datasets;\n- get the model itself. It is densenet169 with custom head right now (several linear layers with batch norm and dropout);\n- get optimizer, scheduler and criterion;\n- get catalyst callbacks;\n\nWe have the masks, but for multilabel classification we would need labels instead, I do it as in this great kernel: https:\/\/www.kaggle.com\/samusram\/cloud-classifier-for-post-processing (in my function `prepare_loaders`)\n```python\n    train = pd.read_csv(f'{path}\/train.csv')\n    train['label'] = train['Image_Label'].apply(lambda x: x.split('_')[1])\n    train['im_id'] = train['Image_Label'].apply(lambda x: x.split('_')[0])\n    if task == 'classification':\n        train_df = train[~train['EncodedPixels'].isnull()]\n        classes = train_df['label'].unique()\n        train_df = train_df.groupby('im_id')['label'].agg(set).reset_index()\n        for class_name in classes:\n            train_df[class_name] = train_df['label'].map(lambda x: 1 if class_name in x else 0)\n\n        img_2_ohe_vector = {img: np.float32(vec) for img, vec in zip(train_df['im_id'], train_df.iloc[:, 2:].values)}\n````","503645b5":"## Exploring augmentations with albumentations\n\nOne of important things while working with images is choosing good augmentations. There are a lot of them, let's have a look at augmentations from albumentations!\n\nFor classification the position of clouds is less important, so we could use more agressive augmentations.","dd04a977":"## Dice of raw predictions","18223be2":"## Setting up parameters\n\nAt first it is a good idea to set up random seeds.","a715480f":"## Training the model\n","ab819fa4":"## Predicting masks\n\nNow we will need to load a segmentation model to make predictions. This can be easily done.","f8de9cd6":"Let's have a look at precision-recall curves!","ea2b92d8":"## General information\n\nIt is usually a good idea to write modular code, if we want reproducibility, ability to easily change the code and conveniently reuse the same code for multiple purposes.\n\nCurrently there is[ a utility scripts competition](https:\/\/www.kaggle.com\/general\/109651) and have decided to contribute.\n\nThis kernel is inspired by this work: https:\/\/www.kaggle.com\/samusram\/cloud-classifier-for-post-processing\n\nThe main idea is also training a classifier for postprocessing. I combine it with models trained in my previous kernel: https:\/\/www.kaggle.com\/artgor\/segmentation-in-pytorch-using-convenient-tools\/\n\nIn this kernel I once again use the following libraries (or ideas from them):\n\n* [albumentations](https:\/\/github.com\/albu\/albumentations): this is a great library for image augmentation which makes it easier and more convenient\n* [catalyst](https:\/\/github.com\/catalyst-team\/catalyst): this is a great library which makes using PyTorch easier, helps with reprodicibility and contains a lot of useful utils\n* [segmentation_models_pytorch](https:\/\/github.com\/qubvel\/segmentation_models.pytorch): this is a great library with convenient wrappers for models, losses and other useful things\n* [pytorch-toolbelt](https:\/\/github.com\/BloodAxe\/pytorch-toolbelt): this is a great library with many useful shortcuts for building pytorch models\n\nBut in this case all the functions and classes are imported from my utility script: https:\/\/www.kaggle.com\/artgor\/pytorch-utils-for-images\n\nIt has functions to:\n- get models, optimizers and other things necessary for training;\n- get dataloaders;\n- train model;\n- make visualizations;\n- and so on.\n\nThis code isn't ideal and I continue working on improving it, but I actually use it to train models locally.\n\nAlso I want to say, that this code is split into several scripts: for models, augmentations, dataset and so on separately. But I think it is better to keep it in a single kaggle utility script.\n\nP. S. This code can be also used for severstal competition, you would only need to modify `prepare_loaders` function ;)\n![](https:\/\/cdn.technologynetworks.com\/tn\/images\/thumbs\/jpeg\/640_360\/repeatability-vs-reproducibility-317157.jpg)","5605f971":"## Using classifier","b516b056":"## Making predictions","469de015":"## Post processing","ba3042a4":"We can see that the raw predictions give quite a bad dice, which is to be expected. Let's try post-processing!","40969a0b":"## Ideas for improvement\n\nThis was only one of the ways of improving the score, here are some more ideas:\n- try applying classifier at first and then finding thresholds for post-processing;\n- train segmentation only on images on masks;\n- while optimizing classifier thresholds and post processing, do it at the same time, trying to maximize dice;\n- use different way to find classifier thresholds;\n- use some ideas from severstal competition;","1239342d":"Looks quite good for a first attempt.\n\nFor classification we would need to find some thresholds, so that we have labels and not probabilites."}}