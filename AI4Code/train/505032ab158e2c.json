{"cell_type":{"d7d364f5":"code","71290aa1":"code","bc9c3a10":"code","9bcd696c":"code","ed75ed02":"code","c0d8ddb8":"code","e92c38da":"code","abc8646d":"code","55818212":"code","c264bf40":"code","4ac72921":"code","2e80a3c3":"code","6672435d":"code","1d632778":"code","e9c2f984":"code","94e1e3b2":"code","fea9f003":"code","560021a7":"code","a415765f":"code","80c719bc":"code","c4a65454":"code","7828e1ee":"code","e4fbe551":"code","0e773bef":"code","f0720095":"code","9b5f765f":"code","23b4e9b7":"code","047d90dc":"code","139eacda":"code","d38c797b":"code","407eb5d9":"code","9c092ce7":"code","3177c03c":"code","25284b0d":"code","c6f46fd8":"code","a544b6d2":"code","be2baac6":"code","69095595":"code","a44ac5ee":"code","6f030764":"code","8a40a70a":"code","3aa9acc7":"code","01cd2bc9":"markdown","31cc578c":"markdown"},"source":{"d7d364f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71290aa1":"dev_en = pd.read_csv(\"\/kaggle\/input\/shopee-product-title-translation-open\/dev_en.csv\")\ndev_en.head()","bc9c3a10":"dev_tcn = pd.read_csv(\"\/kaggle\/input\/shopee-product-title-translation-open\/dev_tcn.csv\")\ndev_tcn.head()","9bcd696c":"import string\nimport re\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, RepeatVector\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom keras import optimizers\nimport matplotlib.pyplot as plt\nimport jieba","ed75ed02":"en_tcn = pd.concat([dev_en['translation_output'], dev_tcn['text']], axis=1).values\nen_tcn","c0d8ddb8":"# Remove punctuation, symbols, any non words\nen_tcn[:,0] = [re.sub(r'[^\\w]', ' ', s) for s in en_tcn[:,0]]\nen_tcn[:,1] = [re.sub(r'[^\\w]', ' ', s) for s in en_tcn[:,1]]\nen_tcn","e92c38da":"# convert text to lowercase\nfor i in range(len(en_tcn)):\n    en_tcn[i,0] = en_tcn[i,0].lower()\n    en_tcn[i,1] = en_tcn[i,1].lower()\nen_tcn","abc8646d":"## Cut with jieba before tokenize with keras tokenizer, remove extra spaces\nen_tcn[:,0] = [re.sub(' +', ' ', s) for s in en_tcn[:,0]]\nen_tcn[:,1] = [re.sub(' +', ' ', \" \".join(jieba.cut(s, cut_all=False))) for s in en_tcn[:,1]]\nen_tcn","55818212":"# empty lists\neng_l = []\ntcn_l = []\n\n# populate the lists with sentence lengths\nfor i in en_tcn[:,0]:\n      eng_l.append(len(i.split()))\n\nfor i in en_tcn[:,1]:\n      tcn_l.append(len(i.split()))\n\nlength_df = pd.DataFrame({'eng':eng_l, 'tcn':tcn_l})\n\nlength_df.hist(bins = 30)\nplt.show()","c264bf40":"max(eng_l), max(tcn_l)","4ac72921":"# function to build a tokenizer\ndef tokenization(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","2e80a3c3":"# prepare english tokenizer\neng_tokenizer = tokenization(en_tcn[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\n\neng_length = max(eng_l)\nprint('English Vocabulary Size: %d' % eng_vocab_size)","6672435d":"# prepare Chinese tokenizer\ntcn_tokenizer = tokenization(en_tcn[:, 1])\ntcn_vocab_size = len(tcn_tokenizer.word_index) + 1\n\ntcn_length = max(tcn_l)\nprint('Chineese Vocabulary Size: %d' % tcn_vocab_size)","1d632778":"# encode and pad sequences\ndef encode_sequences(tokenizer, length, lines):\n    # integer encode sequences\n    seq = tokenizer.texts_to_sequences(lines)\n    # pad sequences with 0 values\n    seq = pad_sequences(seq, maxlen=length, padding='post')\n    return seq","e9c2f984":"from sklearn.model_selection import train_test_split\n\n# split data into train and test set\ntrain, test = train_test_split(en_tcn, test_size=0.2, random_state = 12)","94e1e3b2":"# prepare training data\ntrainX = encode_sequences(tcn_tokenizer, tcn_length, train[:, 1])\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n\n# prepare validation data\ntestX = encode_sequences(tcn_tokenizer, tcn_length, test[:, 1])\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])","fea9f003":"# build NMT model\ndef define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n    model = Sequential()\n    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n    model.add(LSTM(units))\n    model.add(RepeatVector(out_timesteps))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(Dense(out_vocab, activation='softmax'))\n    return model","560021a7":"# model compilation\nmodel = define_model(tcn_vocab_size, eng_vocab_size, tcn_length, eng_length, 512)","a415765f":"rms = optimizers.RMSprop(lr=0.001)\nmodel.compile(optimizer=rms, loss='sparse_categorical_crossentropy')","80c719bc":"filename = 'model.h1.31_jul_20'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\n# train model\nhistory = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n                    verbose=1)","c4a65454":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train','validation'])\nplt.show()","7828e1ee":"model = load_model('model.h1.31_jul_20')\npreds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))","e4fbe551":"def get_word(n, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == n:\n            return word\n    return None","0e773bef":"preds_text = []\nfor i in preds:\n    temp = []\n    for j in range(len(i)):\n        t = get_word(i[j], eng_tokenizer)\n        if j > 0:\n            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n                 temp.append('')\n            else:\n                 temp.append(t)\n        else:\n            if(t == None):\n                  temp.append('')\n            else:\n                  temp.append(t) \n\n    preds_text.append(' '.join(temp))","f0720095":"pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\npred_df.sample(10)","9b5f765f":"# product_title_translation_eval_script.py\n\"\"\"Sample evaluation script for product title translation.\"\"\"\nfrom typing import List\nimport regex\n# !pip install sacrebleu\nfrom sacrebleu import corpus_bleu\n\nOTHERS_PATTERN: re.Pattern = regex.compile(r'\\p{So}')\n\n\ndef eval(preds: List[str], refs: List[str]) -> float:\n    \"\"\"BLEU score computation.\n\n    Strips all characters belonging to the unicode category \"So\".\n    Tokenize with standard WMT \"13a\" tokenizer.\n    Compute 4-BLEU.\n\n    Args:\n        preds (List[str]): List of translated texts.\n        refs (List[str]): List of target reference texts.\n    \"\"\"\n    preds = [OTHERS_PATTERN.sub(' ', text) for text in preds]\n    refs = [OTHERS_PATTERN.sub(' ', text) for text in refs]\n    return corpus_bleu(\n        preds, [refs],\n        lowercase=True,\n        tokenize='13a',\n        use_effective_order= False\n    ).score","23b4e9b7":"eval(pred_df['actual'], pred_df['predicted'])","047d90dc":"test_data = pd.read_csv(\"..\/input\/shopee-product-title-translation-open\/test_tcn.csv\")\ntest_data.head()","139eacda":"test_data.shape","d38c797b":"def process_test_data(arr):\n    ## Remove punctuation, symbols, extra spaces\n    arr = arr.apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n    arr = arr.apply(lambda x: re.sub(' +', ' ', \" \".join(jieba.cut(x, cut_all=False))))\n    ## To lowercase\n    arr = arr.apply(lambda x: x.lower())\n    ## Get max length\n    tcn_l = []\n    for i in arr:\n        tcn_l.append(len(i.split()))\n    tcn_length = max(tcn_l)\n    \n    return arr, tcn_length","407eb5d9":"tcn_arr, tcn_length_test = process_test_data(test_data['text'])","9c092ce7":"print(tcn_arr.head())\nprint(tcn_length_test)","3177c03c":"# prepare test data\ntestX_test = encode_sequences(tcn_tokenizer, tcn_length_test, tcn_arr)","25284b0d":"preds = model.predict_classes(testX_test.reshape((testX_test.shape[0],testX_test.shape[1])))","c6f46fd8":"preds","a544b6d2":"preds.shape","be2baac6":"preds_text = []\nfor i in preds:\n    temp = []\n    for j in range(len(i)):\n        t = get_word(i[j], eng_tokenizer)\n        if j > 0:\n            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n                 temp.append('')\n            else:\n                 temp.append(t)\n        else:\n            if(t == None):\n                  temp.append('')\n            else:\n                  temp.append(t) \n\n    preds_text.append(' '.join(temp))","69095595":"res = pd.DataFrame({\"translation_output\": preds_text})\nres.shape","a44ac5ee":"str_zero = res['translation_output'].value_counts().index[0]\nstr_zero\nstr_most = 'baby'","6f030764":"res['translation_output'] = res['translation_output'].apply(lambda x: \"baby\" if x == str_zero else x)\nres['translation_output'].value_counts()","8a40a70a":"res.to_csv(\"submission_title_translation.csv\", index=False)","3aa9acc7":"pd.read_csv(\".\/submission_title_translation.csv\").shape","01cd2bc9":"### Will be using the validation datasets first for the sake of understanding\nSource: https:\/\/www.analyticsvidhya.com\/blog\/2019\/01\/neural-machine-translation-keras\/\n\nNotes:\n1. Instead of using val data for training, use train data and create its own language pair.\n2. Preprocessing on TCN texts should have a different treatment (Removing symbols, Emoji, Tokenizing with Chineese specific tokenizer).","31cc578c":"### Use all val data to predict test data"}}