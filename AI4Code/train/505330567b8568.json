{"cell_type":{"114f87e8":"code","c57d6318":"code","9d31a6a2":"code","4f4dccf8":"code","8f60c38a":"code","11735f24":"code","3116550f":"code","a1838f05":"code","7665a34c":"code","8eca777f":"code","81813baa":"code","e61e19d4":"code","f754bd09":"code","83986bfb":"code","cd3af920":"code","a671f621":"code","164dd118":"code","50d8e512":"code","51ce4183":"code","d2bd0932":"code","4ad074a5":"code","0d8c7c4c":"code","634a55e3":"code","773011ea":"code","5e44955b":"code","b1ec2221":"code","1796289f":"code","47b94f75":"code","a91a7ff1":"code","ccc94a49":"code","d6f1d71e":"code","e05abce3":"code","9122f754":"code","30acfbef":"code","25974b1a":"code","d94e2467":"code","6b935c9b":"code","95cf00ac":"markdown","9fa4f611":"markdown","d5c49089":"markdown","d27283a7":"markdown","aceb7fb4":"markdown","036679d4":"markdown","7d2ca7a9":"markdown","3f9d7788":"markdown","f441bc54":"markdown","0cfa7151":"markdown","a5046ae1":"markdown","d08a18fc":"markdown","09e643f2":"markdown","ce0ff5ef":"markdown","85081840":"markdown","40a7456a":"markdown","626ba097":"markdown","2a7d8f3e":"markdown","bc13b538":"markdown","c54dd664":"markdown","8cd77f45":"markdown"},"source":{"114f87e8":"# Import all the tools we need\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model evaluators\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","c57d6318":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf.shape","9d31a6a2":"df.head() # first 5 elements","4f4dccf8":"# finding the number of positive and negative tests on the dataset\ndf[\"target\"].value_counts()","8f60c38a":"df[\"target\"].value_counts().plot(kind=\"bar\", color=[\"orange\", \"purple\"]);","11735f24":"df.info()","3116550f":"df.isna().sum()","a1838f05":"df.sex.value_counts()","7665a34c":"# Comparing target column with sex column\npd.crosstab(df.target, df.sex)","8eca777f":"pd.crosstab(df.target, df.sex).plot(kind=\"bar\",\n                                    figsize=(10, 6),\n                                    color=[\"orange\", \"purple\"])\n\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Number of patients\")\nplt.legend([\"Female\", \"Male\"]);\nplt.xticks(rotation=0);","81813baa":"# create a matplotlib figure\nplt.figure(figsize=(10, 8))\n\n# Scatter plot with positively tested patients\nplt.scatter(df.age[df.target==1],\n            df.thalach[df.target==1],\n            c=\"orange\")\n\n# Scatter with negatively tested patients\nplt.scatter(df.age[df.target==0],\n            df.thalach[df.target==0],\n            c=\"purple\")\n\nplt.title(\"Age versus max heart rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Disease\", \"No Disease\"]);","e61e19d4":"pd.crosstab(df.cp, df.target)","f754bd09":"\npd.crosstab(df.cp, df.target).plot(kind=\"bar\",\n                                   figsize=(10, 6),\n                                   color=[\"orange\", \"purple\"])\n\n# Add some communication\nplt.title(\"Heart Disease Frequency versus Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Amount\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation=0);","83986bfb":"# correlation matrix\ndf.corr()","cd3af920":"# Splitting data into X and y\nx = df.drop(\"target\", axis=1)\n\ny = df[\"target\"]","a671f621":"x","164dd118":"y","50d8e512":"# Split data into train and test sets\nnp.random.seed(42)\n\n# Split into train & test set\nx_train, x_test, y_train, y_test = train_test_split(x,\n                                                    y,\n                                                    test_size=0.2)","51ce4183":"len(x_train)","d2bd0932":"len(y_train)","4ad074a5":"# Put models in a dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(),\n          \"Random Forest\": RandomForestClassifier()}\n\n# Creating a function to fit and score models\ndef fit_and_score(models, x_train, x_test, y_train, y_test):\n    \n    np.random.seed(42)\n    \n    # Make a dictionary to keep model scores\n    model_scores = {}\n    \n    # Loop through models\n    for name, model in models.items():\n        \n        # Fit the model to the data\n        model.fit(x_train, y_train)\n        \n        # append the evaluated score to model_scores\n        model_scores[name] = model.score(x_test, y_test)\n    return model_scores","0d8c7c4c":"model_scores = fit_and_score(models=models,\n                             x_train=x_train,\n                             x_test=x_test,\n                             y_train=y_train,\n                             y_test=y_test)\n\nmodel_scores","634a55e3":"model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_compare.T.plot.bar();","773011ea":"# grid for hyperparameters of logistic regression\ngrid_1 = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n\ngrid_2 = {\"n_estimators\": np.arange(10,100,10),\n          \"max_depth\": [None,3,5,7,10],\n          \"min_samples_split\": np.arange(2,20,2),\n          \"min_samples_leaf\": np.arange(1,20,2),\n          \"max_features\": [0.5,1,\"sqrt\",\"auto\"],\n          \"max_samples\": [100]}","5e44955b":"# Tune LogisticRegression\n\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\nmodel_1 = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=grid_1,\n                                cv=5,\n                                n_iter=50,\n                                verbose=True)\n\n# Fitting random hyperparameter search model for LogisticRegression\nmodel_1.fit(x_train, y_train)","b1ec2221":"model_1.best_params_  # get the best parameters for the model","1796289f":"model_1.score(x_test, y_test)  # evaluate the model on the test set","47b94f75":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nmodel_2 = RandomizedSearchCV(RandomForestClassifier(), \n                           param_distributions=grid_2,\n                           cv=5,\n                           n_iter=100,\n                           verbose=True)\n\n# Fitting random hyperparameter search model for RandomForestClassifier()\nmodel_2.fit(x_train, y_train)","a91a7ff1":"model_2.best_params_","ccc94a49":"# Evaluate the model on the test set\nfinal_score=model_2.score(x_test, y_test)\nfinal_score","d6f1d71e":"# predcting the outcome based on input\ny_preds = model_2.predict(x_test)\ny_preds","e05abce3":"y_test","9122f754":"# Plot ROC curve and calculate and calculate AUC metric\nplot_roc_curve(model_2, x_test, y_test)","30acfbef":"# first we reassign the previously built logistic regression and random forest classification models their best parameters\nreg_model = LogisticRegression(solver= 'liblinear',\n                                C= 0.23357214690901212)\n\nrf_model = RandomForestClassifier(n_estimators = 60,\n                                  min_samples_split = 12,\n                                  min_samples_leaf = 1,\n                                  max_samples = 100,\n                                  max_features = 1,\n                                  max_depth = None)","25974b1a":"# defining a function to get evaluation metrics\ndef get_eval_metrics(model):\n    \n    # accuracy\n    cv_accuracy = cross_val_score(model,\n                         x,\n                         y,\n                         cv=5,\n                         scoring=\"accuracy\")\n    cv_accuracy = np.mean(cv_accuracy)\n    \n    # precision \n    cv_precision = cross_val_score(model,\n                         x,\n                         y,\n                         cv=5,\n                         scoring=\"precision\")\n    cv_precision=np.mean(cv_precision)\n    \n    # recall\n    cv_recall = cross_val_score(model,\n                         x,\n                         y,\n                         cv=5,\n                         scoring=\"recall\")\n    cv_recall = np.mean(cv_recall)\n    \n    # f1 score\n    cv_f1 = cross_val_score(model,\n                         x,\n                         y,\n                         cv=5,\n                         scoring=\"f1\")\n    cv_f1 = np.mean(cv_f1)\n    \n    return cv_accuracy , cv_precision , cv_recall , cv_f1","d94e2467":"get_eval_metrics(reg_model)","6b935c9b":"get_eval_metrics(rf_model)","95cf00ac":"First we tune the logistic regressor","9fa4f611":"# Heart disease classification using machine learning\n\nGiven the meidcal parameters of a patient, predict whether he\/she has a heart disease or not.\n\n\n### Data\n\nThe dataset used for this project is available on Kaggle. https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n<br>\nInformation about the dataset is given below and can also be found on the above mentioned link.\n\n<br>\n\n1.age\n<br>\n2.sex\n<br>\n3.chest pain type (4 values)\n<br>\n4.resting blood pressure\n<br>\n5.serum cholestoral in mg\/dl\n<br>\n6.fasting blood sugar > 120 mg\/dl\n<br>\n7.resting electrocardiographic results (values 0,1,2)\n<br>\n8.maximum heart rate achieved\n<br>\n9.exercise induced angina\n<br>\n10.oldpeak = ST depression induced by exercise relative to rest\n<br>\n11.the slope of the peak exercise ST segment\n<br>\n12.number of major vessels (0-3) colored by flourosopy\n<br>\n13.thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n<br>\n\n### We will be using sklearn pipelines for prediction and for cross validation , RandomizedSearchCV will be used.\n\n","d5c49089":"### Model Comparison","d27283a7":"In the above crosstab , sex value \"0\" indicates female patients and \"1\" indicates male patients.","aceb7fb4":"## Hyperparameter tuning with RandomizedSearchCV","036679d4":"# Testing out different parameters relating to heart disease","7d2ca7a9":"### Heart Disease Frequency versus Chest Pain Type\n","3f9d7788":"Now for random forest classifier","f441bc54":"In the above cell , \"1\" refers to patients having a heart disease and \"0\" refers to the patients not having a heart disease . Let's plot it on a matplotlib plot.","0cfa7151":"We will try 2 different classification models for this , i.e. logistic regressor and random forest classifier.","a5046ae1":"Here we can see that logistic regression has outperformed random forest classification.","d08a18fc":"## Now we can calculate  the evaluation metrics like accuracy , precision , f1 score and recall.","09e643f2":"# We were successfully able to increase the score of the random forest classification model , however the score for the logistic regression model remained the same. Further improvement can be done by implementing the GridSearchCV","ce0ff5ef":"### In this project , I have used random forest classifier and logistic regressor for training and testing the dataset. And in the end used the obtained best parameters from randomized search CV to calculate evaluation metrics. ","85081840":"### Heart disease versus gender","40a7456a":"### Age versus max heart rate","626ba097":"## 5. Modelling ","2a7d8f3e":"## Preparing the tools\n","bc13b538":"Our dataset does not have a missing value.","c54dd664":"## Loading data ","8cd77f45":"The area under curve for this roc curve is 93 percent"}}