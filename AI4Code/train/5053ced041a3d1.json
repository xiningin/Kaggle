{"cell_type":{"f66a1ded":"code","4231de18":"code","f21dad47":"code","4fc8b81f":"code","79827f17":"code","b7197d19":"code","b79c41ec":"code","28dc1e32":"code","5fd967e1":"code","a5f3f7d4":"code","29dcff04":"code","7c58524c":"code","aa053461":"code","f10002d9":"code","b670cb67":"markdown","caf7fa4b":"markdown","f76c8203":"markdown","32c50d67":"markdown","b19157c1":"markdown","2c4625a4":"markdown","2c1e817b":"markdown","02e6f5b8":"markdown"},"source":{"f66a1ded":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras","4231de18":"dataframe = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv', header=0)\namount = dataframe.Amount\ndataframe = dataframe.drop(columns=[\"Time\", \"Amount\"])\nraw_data = dataframe.values\ndataframe.head()","f21dad47":"pd.value_counts(dataframe['Class'], sort = True) #class comparison 0=Normal 1=Fraud","4fc8b81f":"labels = raw_data[:, -1]\ndata = raw_data[:, 0:-1]\n\ntrain_data, test_data, train_labels, test_labels, train_amount, test_amount = train_test_split(data, labels, amount, test_size=0.2, random_state=21, stratify=dataframe['Class'])\n\n# min-max train e test data\nmin_val = tf.reduce_min(train_data)\nmax_val = tf.reduce_max(train_data)\n\ntrain_data = (train_data - min_val) \/ (max_val - min_val)\ntest_data = (test_data - min_val) \/ (max_val - min_val)\n\ntrain_data = tf.cast(train_data, tf.float32)\ntest_data = tf.cast(test_data, tf.float32)\n\n# min-max train e test amount\nmin_val_amount = tf.reduce_min(train_amount)\nmax_val_amount = tf.reduce_max(train_amount)\n\ntrain_amount = (train_amount - min_val_amount) \/ (max_val_amount - min_val_amount)\ntest_amount = (test_amount - min_val_amount) \/ (max_val_amount - min_val_amount)\n\ntrain_amount = tf.cast(train_amount, tf.float32)\ntest_amount = tf.cast(test_amount, tf.float32)","79827f17":"normal_train_data = train_data[train_labels == 0]\nnormal_test_data = test_data[test_labels == 0]\n\nanomalous_train_data = train_data[train_labels == 1]\nanomalous_test_data = test_data[test_labels == 1]","b7197d19":"plt.grid()\nplt.plot(np.arange(28), normal_train_data[0])\nplt.title(\"A Normal Transition\")\nplt.show()\n\nplt.grid()\nplt.plot(np.arange(28), anomalous_train_data[0])\nplt.title(\"An Anomalous Transition\")\nplt.show()","b79c41ec":"tf.random.set_seed(0)\n\ninp = keras.layers.Input(shape=[28,])\nx = keras.layers.Dense(128, activation=\"relu\") (inp)\nx = keras.layers.Dense(64, activation=\"relu\") (x)\nx = keras.layers.Dense(32, activation=\"relu\") (x)\nx = keras.layers.Dense(64, activation=\"relu\") (x)\nx = keras.layers.Dense(128, activation=\"relu\") (x)\nout = keras.layers.Dense(28, activation=\"sigmoid\") (x)\n\nautoencoder = keras.models.Model(inp, out)\n\nautoencoder.compile(optimizer='adam', loss='mse')\n\nhistory = autoencoder.fit(normal_train_data, normal_train_data, \n                          epochs=100, \n                          batch_size=256,\n                          callbacks=[keras.callbacks.ModelCheckpoint(filepath=\"autoencoder_fraud.h5\", monitor=\"val_loss\", save_best_only=True, verbose=0)],\n                          validation_data=(normal_test_data, normal_test_data), #test_data, test_data\n                          shuffle=True)\n\nplt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.legend()\nplt.show()","28dc1e32":"autoencoder = keras.models.load_model(\".\/autoencoder_fraud.h5\")\nautoencoder.evaluate(normal_test_data, normal_test_data) #test_data, test_data","5fd967e1":"decoded_data = autoencoder(normal_test_data).numpy()\n\nplt.plot(normal_test_data[0], 'b')\nplt.plot(decoded_data[0], 'r')\nplt.fill_between(np.arange(28), decoded_data[0], normal_test_data[0], color='lightcoral')\nplt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\nplt.show()\n\ndecoded_data = autoencoder(anomalous_test_data).numpy()\n\nplt.plot(anomalous_test_data[0], 'b')\nplt.plot(decoded_data[0], 'r')\nplt.fill_between(np.arange(28), decoded_data[0], anomalous_test_data[0], color='lightcoral')\nplt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\nplt.show()","a5f3f7d4":"train_loss = tf.keras.losses.mae(autoencoder(normal_train_data), normal_train_data)\n\nthreshold = np.mean(train_loss) + np.std(train_loss)\nprint(\"Threshold: \", threshold)\n\nplt.hist(train_loss[None,:], bins=50)\nplt.axvline(x=threshold, c=\"red\")\nplt.xlabel(\"Train loss\")\nplt.ylabel(\"No of examples\")\nplt.show()\n\ntest_loss = tf.keras.losses.mae(autoencoder(anomalous_test_data), anomalous_test_data)\n\nplt.hist(test_loss[None, :], bins=50)\nplt.axvline(x=threshold, c=\"red\")\nplt.xlabel(\"Test loss only fraud\")\nplt.ylabel(\"No of examples\")\nplt.show()","29dcff04":"def predict(model, data, threshold):\n    reconstructions = model(data)\n    loss = tf.keras.losses.mae(reconstructions, data)\n    return tf.math.logical_not(tf.math.less(loss, threshold))\n\ndef print_stats(predictions, labels):\n    print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n    print(\"Precision = {}\".format(precision_score(labels, predictions)))\n    print(\"Recall = {}\".format(recall_score(labels, predictions)))\n\nprint(np.unique(test_labels, return_counts=True)[1])\npreds = predict(autoencoder, test_data, threshold)\nprint_stats(preds, test_labels)\nconfusion_matrix(preds, test_labels)","7c58524c":"weight_for_0 = (1 \/ len(train_labels[train_labels == 0])) * (len(train_labels) \/ 2.0)\nweight_for_1 = (1 \/ len(train_labels[train_labels == 1])) * (len(train_labels) \/ 2.0)\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\nclass_weight","aa053461":"tf.random.set_seed(0)\n\ninp = keras.layers.Input(shape=[28,])\nx = autoencoder (inp)\nx = keras.layers.BatchNormalization() (x)\nx = keras.layers.Dense(64, activation=\"elu\") (x)\n#x = keras.layers.Dropout(0.15) (x)\nout = keras.layers.Dense(1, activation=\"sigmoid\") (x)\n\nmlp = keras.models.Model([inp], out)\nmlp.layers[1].trainable = False # blocco i pesi dell'autoencoder\n\nmlp.summary()\nmlp.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', \n            metrics=[keras.metrics.AUC(name='auc'),\n                     keras.metrics.AUC(name='prc', curve='PR')])\n\nhistory = mlp.fit([train_data], \n                  train_labels,\n                  epochs=25, \n                  batch_size=128,\n                  callbacks=[keras.callbacks.ModelCheckpoint(filepath=\"mlp_fraud.h5\", monitor=\"val_prc\", save_best_only=True, verbose=0)],\n                  validation_data=([test_data], test_labels),\n                  class_weight=class_weight,\n                  shuffle=True)\n\nplt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.legend()\nplt.show()","f10002d9":"print(np.unique(test_labels, return_counts=True)[1])\n\nmlp = keras.models.load_model(\".\/mlp_fraud.h5\")\nprint_stats(np.rint(mlp(test_data).numpy()), test_labels)\nconfusion_matrix(np.rint(mlp(test_data).numpy()), test_labels) ","b670cb67":"# Training Autoencoder","caf7fa4b":"# Freeze Autoencoder weight + MLP","f76c8203":"# Baseline with std threshold","32c50d67":"* Based on confuzion matrix the mlp classifier hit 85\/98 frad in test set.\n* 13 fraud is confused with not fraud\n* 6208 not fraud is confused with fraud","b19157c1":"# Import data","2c4625a4":"# Split data in normal and fraud data","2c1e817b":"* Based on confuzion matrix the mlp classifier hit 94\/98 frad in test set.\n* 4 fraud is confused with not fraud\n* 3023 not fraud is confused with fraud","02e6f5b8":"# Split and Normalize [0-1]"}}