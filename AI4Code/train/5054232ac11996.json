{"cell_type":{"c931f4ec":"code","620ab65a":"code","4cf45eea":"code","e800e3a2":"code","2f0f3f46":"code","66b414fd":"code","2ce009c0":"code","80fc8991":"code","12a8784a":"code","7fe487ac":"code","c91b68ba":"code","154d2dc0":"code","905ae4ed":"code","0c666832":"code","278c2fc5":"code","879f3c24":"code","2935b0d2":"code","4f030ee9":"code","6fcae6c5":"code","ca6333a1":"code","e4b212e7":"code","058de025":"code","20612532":"code","5215816a":"code","b1cf4b7c":"markdown"},"source":{"c931f4ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","620ab65a":"# IMPORTING THE REQUIRED LIBRARIES\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.layers import Dense, LSTM\nfrom keras.models import Sequential\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom numpy.fft import *\n%matplotlib inline","4cf45eea":"train_1= pd.read_csv(\"..\/input\/web-traffic-time-series-forecasting\/train_1.csv.zip\")\ntrain_2= pd.read_csv(\"..\/input\/web-traffic-time-series-forecasting\/train_2.csv.zip\")\nkey_1= pd.read_csv(\"..\/input\/web-traffic-time-series-forecasting\/key_1.csv.zip\")\nkey_2= pd.read_csv(\"..\/input\/web-traffic-time-series-forecasting\/key_2.csv.zip\")","e800e3a2":"train_1.head(20)","2f0f3f46":"key_2.head()","66b414fd":"train_1= train_1.fillna(0)\ntrain_2= train_2.fillna","2ce009c0":"# FUNCTION FOR CREATING ANOTHER COLUMN IN TRAIN DATASET FOR THE PAGE LANGAUGE\ndef find_lang(page):\n    res= re.search(\"[a-z][a-z].wikipedia.org\", page)\n    if res:\n        return res[0][0:2]\n    return(\"na\")\n\ntrain_1[\"lang\"]= train_1.Page.map(find_lang)","80fc8991":"sns.countplot(train_1[\"lang\"])\nplt.title(\"Language Distribution\")\nplt.show()","12a8784a":"lang_set= {}\nlang_set[\"en\"]= train_1[train_1.lang==\"en\"].iloc[:,0:-1]\nlang_set[\"ja\"]= train_1[train_1.lang==\"ja\"].iloc[:,0:-1]\nlang_set[\"de\"]= train_1[train_1.lang==\"de\"].iloc[:,0:-1]\nlang_set[\"na\"]= train_1[train_1.lang==\"na\"].iloc[:,0:-1]\nlang_set[\"fr\"]= train_1[train_1.lang==\"fr\"].iloc[:,0:-1]\nlang_set[\"zh\"]= train_1[train_1.lang==\"zh\"].iloc[:,0:-1]\nlang_set[\"ru\"]= train_1[train_1.lang==\"ru\"].iloc[:,0:-1]\nlang_set[\"es\"]= train_1[train_1.lang==\"es\"].iloc[:,0:-1]","7fe487ac":"lang_set[\"en\"]\n","c91b68ba":"\nsums= {}\nfor key in lang_set:\n    sums[key]= lang_set[key].iloc[:,1:].sum(axis= 0) \/ lang_set[\"en\"].shape[0]","154d2dc0":"sums[\"en\"]\n","905ae4ed":"#plot and see individually traffic on particular language\ndays= [r for r in range(sums[\"en\"].shape[0])]\nfig= plt.figure(1, figsize= [10,10])\nplt.ylabel(\"Views per page\")\nplt.xlabel(\"Day\")\nplt.title(\"Page in Different Language\" )\nlabel= {\"en\":\"English\", \"ja\":\"Japanese\", \"de\":\"German\", \"na\": \"Media\",\n       \"fr\": \"French\", \"zh\": \"Chinese\", \"ru\": \"Russian\", \"es\": \"Spanish\"}\nfor key in sums:\n    plt.plot(days, sums[key], label= label[key])\n    \nplt.legend()\nplt.show()","0c666832":"def filter_signal(signal, threshold=1e3):\n    fourier = rfft(signal)\n    frequencies = rfftfreq(signal.size, d=20e-3\/signal.size)\n    fourier[frequencies > threshold] = 0\n    return irfft(fourier)\n\na= np.array(train_1.iloc[0,1:-1],np.float32)\nsc= MinMaxScaler()\n\nplt.figure(figsize= (12,12))\nplt.plot(a)\nplt.show()\n\nplt.figure(figsize= (12,12))\nplt.plot(filter_signal(a))\nplt.show()\n\nregressor= Sequential()\nregressor.add(LSTM(units= 10, activation=\"relu\",return_sequences=True, input_shape=(None, 1)))\nregressor.add(Dense(units = 1))\nregressor.compile(optimizer= \"adam\", loss= \"mean_squared_error\")","278c2fc5":"a= np.array(train_1.iloc[0,1:-1],np.float32)\n\nplt.figure(figsize= (12, 6))\nplt.title(\"Before Fourier Transform\")\nplt.plot(a)","879f3c24":"plt.figure(figsize= (12,6))\nplt.plot(filter_signal(a))\nplt.title(\"After Fourier Transform\")","2935b0d2":"regressor= Sequential()\nregressor.add(LSTM(units= 10, activation=\"relu\",return_sequences=True, input_shape=(None, 1)))\nregressor.add(Dense(units = 1))\nregressor.compile(optimizer= \"adam\", loss= \"mean_squared_error\")","4f030ee9":"arr= filter_signal(a)\narr= arr.reshape(-1,1)\n\n# performing standardization of values using min max scaler\narr= sc.fit_transform(arr)\narr= np.reshape(arr, (-1,1,1))\n\n# Train data\narr_X= arr[:400]\narr_y= arr[1:401]\narr_X= np.reshape(arr_X, (-1,1,1))\n\n# Test data\narr_TX= arr[401:-1]\narr_ty= arr[402:]\narr_TX= np.reshape(arr_TX, (-1,1,1))\n\n# Training the model\nregressor.fit(arr_X, arr_y, batch_size= 5, epochs= 100, verbose= 0)\n\n# perdicting the value\nres= regressor.predict(arr_TX)\n\n# Reshaping the value for plotting purpose\nres= res.reshape(148,1)\narr_ty= arr_ty.reshape(148,1)\n\n#plotting the data\nplt.plot(res, color=\"r\")\nplt.plot(arr_ty, color=\"b\")\nplt.show()","6fcae6c5":"def plot_entry(key, idx):\n    data= lang_set[key].iloc[idx,1:]\n    fig= plt.figure(1, figsize= (10, 5))\n    plt.plot(days, data)\n    plt.xlabel(\"day\")\n    plt.ylabel(\"views\")\n    plt.title(train_1.iloc[lang_set[key].index[idx],0])\n    \n    plt.show()","ca6333a1":"idx= [2, 7, 17, 57, 101, 257, 501, 757, 1117, 1517, 2777]\nfor i in idx:\n    plot_entry(\"en\",i)","e4b212e7":"idx= [2, 7, 17, 57, 101, 257, 501, 757, 1117, 1517, 2777]\nfor i in idx:\n    plot_entry(\"es\",i)","058de025":"# For each language get highest few pages\nnpages= 5\ntop_pages= {}\nfor key in lang_set:\n    print(key)\n    sum_set= pd.DataFrame(lang_set[key][[\"Page\"]])\n    sum_set[\"Total\"]= lang_set[key].sum(axis= 1)\n    sum_set = sum_set.sort_values('Total',ascending=False)\n    top_pages[key] = sum_set.index[0]\n    print(sum_set.head(10))\n    print('\\n\\n')","20612532":"for key in top_pages:\n    fig = plt.figure(1,figsize=(10,5))\n    cols = train_1.columns\n    cols = cols[1:-1]\n    data = train_1.loc[top_pages[key],cols]\n    plt.plot(days,data)\n    plt.xlabel('Days')\n    plt.ylabel('Views')\n    plt.title(train_1.loc[top_pages[key],'Page'])\n    plt.show()","5215816a":"from statsmodels.tsa.arima_model import ARIMA\nimport warnings\n\ncols= train_1.columns[1:-1]\nfor key in top_pages:\n    data= np.array(train_1.loc[top_pages[key],cols],\"f\")\n    result= None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\")\n        try:\n            arima= ARIMA(data, [2,1,4])\n            result= arima.fit(disp= False)\n        except:\n            try:\n                arima= ARIMA(data, [2,1,2])\n                result= arima.fit(disp= False)\n            except:\n                print(train_1.loc[top_pages[key], \"Page\"])\n                print(\"\\tARIMA FAILED\")\n    pred= result.predict(2,599, typ= \"levels\")\n    x= [i for i in range(600)]\n    i=0\n    plt.plot(x[2:len(data)],data[2:] ,label='Data')\n    plt.plot(x[2:],pred,label='ARIMA Model')\n    plt.title(train_1.loc[top_pages[key],'Page'])\n    plt.xlabel('Days')\n    plt.ylabel('Views')\n    plt.legend()\n    plt.show()\n","b1cf4b7c":"For Finding each page individually counts of no of most most in a Page"}}