{"cell_type":{"ce772024":"code","40e82b3a":"code","910b5a3d":"code","bf9d3be8":"code","22be5120":"code","7fc44869":"code","5ad9cba0":"code","7796f200":"code","9df7e45f":"code","151121af":"code","e4153577":"code","51807b5c":"code","5d1f4a82":"code","b1272316":"code","6bfd7af7":"markdown","6c0d74c9":"markdown","aebff3aa":"markdown","bbcfc747":"markdown","f8dbf77a":"markdown","91d52683":"markdown","9a32fd07":"markdown","4b0317d6":"markdown"},"source":{"ce772024":"# libraries import \nimport numpy as np  \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n","40e82b3a":"# reading the data set\ndata_set = pd.read_csv('..\/input\/social-network-ad\/Social_Network_Ads.csv')","910b5a3d":"# Analysis of the dataset \ndata_set.head()","bf9d3be8":"# Plot\nimport plotly.express as px\nfig = px.scatter_3d(data_set, x='Age', y= 'EstimatedSalary',z = 'Gender',\n              color='Purchased', symbol='Purchased', opacity=0.7)\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","22be5120":"fig = px.scatter(data_set, x='Age', y= 'EstimatedSalary',\n              color='Purchased', symbol='Purchased', opacity=0.7)\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","7fc44869":"X = data_set.iloc[:,[2,3]].values\nY = data_set.iloc[:,4].values\n# Splitting the dataset into train and test\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)","5ad9cba0":"# Preprocessing the dataset\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","7796f200":"# Applying the logistic regression classifier \nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=0)\nlr.fit(X_train, Y_train)","9df7e45f":"# Applying the SVM \nfrom sklearn.svm import SVC\nsv = SVC(kernel='rbf', random_state = 0)\nsv.fit(X_train, Y_train)","151121af":"from sklearn.model_selection import cross_val_score\nscore_lr = cross_val_score(estimator= lr, X= X_train,y=Y_train,cv= 10, n_jobs= -1)\nscore_sv = cross_val_score(estimator= sv, X= X_train,y=Y_train,cv= 10, n_jobs= -1)\n\n\nprint('SVM : Mean of the accuracies is %2f percent' % (score_sv.mean()*100))\nprint('SVM : the standard deviation of the accuracies is %2f percent' % (score_sv.std()*100))\n\nprint('log_Regression: Mean of the accuracies is %2f percent' % (score_lr.mean()*100))\nprint('log_Regression: the standard deviation of the accuracies is %2f percent' % (score_lr.std()*100))\n\nplt.plot(range(len(score_sv)), score_sv, label='SVM')\nplt.plot(range(len(score_lr)), score_lr, label = 'logistic regression')\nplt.xlabel('Fold')\nplt.ylabel('Accuracy')\nplt.legend()","e4153577":"from sklearn.model_selection import GridSearchCV\n# dictionary of parameters used as an input to gridsearch \nparameters = [\n    {'C':[1,10,100,1000], 'kernel': ['linear']},\n    {'C':[1,10,100,1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]}\n]\n# applying grid search\ngscv = GridSearchCV(estimator = sv, \n                   param_grid= parameters,\n                   scoring = 'accuracy', cv= 10, n_jobs= -1)\ngscv = gscv.fit(X_train, Y_train)\n\nbest_acc = gscv.best_score_\nprint('best accuracy is %2f percent ' %(best_acc*100))\nbest_parameters = gscv.best_params_\nprint('best parameters are : ', best_parameters)","51807b5c":"predictions_gridsearch = gscv.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\ncm_gridsearch = confusion_matrix(Y_test, predictions_gridsearch)\nprint(cm_gridsearch)","5d1f4a82":"# Visualisation\n# Training results \nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, Y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n\nplt.contourf(X1, X2, gscv.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('green','yellow')))\n\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('green', 'yellow'))(i), label = j)\nplt.title('Kernel SVM (Grid Search) -Training set')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","b1272316":"\nX_set, y_set = X_test, Y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, gscv.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('green','yellow')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('green','yellow'))(i), label = j)\nplt.title('Kernel SVM (Grid Search)-Test set')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","6bfd7af7":"**Thank you for reading this kernel. If you found this kernel useful, I would really appreciate if you upvote it or leave a short comment below.**","6c0d74c9":"The cross_validation scores shows that SVM is much better as compared to the Logistic regression model for our dataset which we already knew also. \n\n# Grid Search\nNow since we know that the better model among the Logistic regression and the SVM is SVM, we will now use the grid search method to do the parameter tuning.\nAs in SVM we have different parameters, for example 'C' which is basically the penalty parameter and is used to avoid the overfiting of our model, what is the best value for C for our dataset is determined by Grid Search, apart from this for SVC we have different kernels, now which kernel will work best for our dataset is also determined using grid search. apart from this all the parameters of a particular algorithm can be optimised using the Grid Search. \n\nLets apply this. \n","aebff3aa":"# K-Fold Cross Validation\n\nAs we are trying to learn, the model selection here in this section so, we will try to choose here two different classifiers\n\n1) Logistic regression\n\n2) Support Vector machine\n\nand then using the cross validation score we will decide which algorithm to choose. \nNote: This can be esily deduced by analysing the above figure that logistic regression classifier will not work good on our dataset as the data doesnt seems to be linearly seperable but as we are trying to learn the method here to use cross validation score to decide the better models. The logistic regression model is just taken as one of the example here. ","bbcfc747":"As we can see that the dataset contains total of 5 columns and the columns namely USER_ID, Gender, Age and Estimated Salary are the independent attribtes here and the Purchased is the dependent attribute which we need to classify.\n\nNow if we look at the attributes, 'Gender' is one of categorical type and if we want to use it in out machine learning algorithm as input than we have to process it further. The other attributes are of continous type and can be used as it is. ","f8dbf77a":"Now we have got the best parameters for our model we can then train with this model here and can see the results","91d52683":"This notebook contains some basic concepts to apply the model selection techniques in machine learning.\nI will try to give some over view here on K-Fold cross validation techniques and Grid search which are the part pf sklearn library. \nWe will use K cross validation technoques to get the score of different classifier implemented on one data set to know how they perform and will then use Grid Search technique to get the best parameters for the best model that we choose based on the cross validation scores. \nThe data set choosen here is Social Networking Ads dataset, which I found on the Kaggle dataset repositories. ","9a32fd07":"If we see the above figure, we can say that the Gender is not having that much of information as it seems to be distributed equally for both the choices, so we will try to visualise the relationship between purchased choice and the Ahe and income of the users.\n\nAs we can see in the figure below, the choices are seperable in relation to the age and income. So for our analysis we will try to use the Age and Estimated SAlary as the only input attributes. ","4b0317d6":"**Now we will apply the K-Fold Cross validation techniques to check which model is bettter for our dataset **"}}