{"cell_type":{"77dac6bd":"code","563fbc3d":"code","9488cd56":"code","8edf26b3":"code","9f01d31a":"code","a5e60421":"code","045199f2":"code","eee0fbe7":"code","1df283ad":"code","890a5c73":"code","1336076c":"code","6b6b15ab":"code","c04f5c01":"code","911db322":"code","f6895400":"code","013a99ca":"code","cf8e6958":"code","d20944a7":"code","719cb695":"code","7ae523c3":"code","decc6b57":"code","ab7b2994":"code","50bcbd69":"code","bfebd746":"code","53416ba8":"code","b9362c50":"code","b18ec1f5":"code","f6e84450":"code","f4a47c6d":"code","55909f57":"code","63f9d2e7":"code","e43e5b7d":"code","c3ba9b16":"markdown","e479e3af":"markdown","9429f656":"markdown","a370709c":"markdown","4a7c99e1":"markdown","00e633f5":"markdown","45a1957b":"markdown","5da29cd1":"markdown","9b713374":"markdown","513d69a5":"markdown"},"source":{"77dac6bd":"# Importing necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","563fbc3d":"data = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf = pd.DataFrame(data)","9488cd56":"df.head()","8edf26b3":"col = df.columns  #getting list of column names","9f01d31a":"# showing column wise %ge of NaN values they contains \n\nfor i in col:\n  print(i,\"\\t-\\t\", df[i].isna().mean()*100)\n","a5e60421":"num_df = df.select_dtypes(exclude=['object'])\ncat_df= df.drop(num_df, axis=1)","045199f2":"num_df.head()","eee0fbe7":"cormap = num_df.corr()\nfig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(cormap, annot = True)","1df283ad":"# Simple Function to get the name of top most corelated attributes\n\ndef get_corelated_col(cor_dat, threshold): \n  # Cor_data to be column along which corelation to be measured \n  #Threshold be the value above wich of corelation to considered\n  feature=[]\n  value=[]\n\n  for i ,index in enumerate(cor_dat.index):\n    if abs(cor_dat[index]) > threshold:\n      feature.append(index)\n      value.append(cor_dat[index])\n\n  df = pd.DataFrame(data = value, index = feature, columns=['corr value'])\n  return df\n","890a5c73":"top_corelated_values = get_corelated_col(cormap['Salary'], 0.40)\ntop_corelated_values","1336076c":"final_num_df = num_df[top_corelated_values.index]\nfinal_num_df.head()","6b6b15ab":"cat_df.head()","c04f5c01":"from sklearn.preprocessing import LabelEncoder\n\ncat_col = cat_df.columns\nfor i in cat_col:\n  enc = LabelEncoder()\n  cat_df[i] = enc.fit_transform(cat_df[i].astype('str'))","911db322":"cat_df.head()\n","f6895400":"cat_df['Salary'] = df['Salary']  # to get coreltion with target attribute which is Sales Price","013a99ca":"cormat = cat_df.corr()\nfig, ax = plt.subplots(figsize=(6,6))\nsns.heatmap(cormat, annot = True)","cf8e6958":"# Final selected 11 most favourable features for prediction\n\nfinal_df = final_num_df\nfinal_df.head()","d20944a7":"# Here we are splitting data in train and test as test set are rows with null values in salary column\n\ndf_train = final_df.dropna()\ndf_test = final_df[final_df.isnull().any(axis=1)]\n","719cb695":"X = df_train.drop(['Salary'], axis=1)\ny = df_train['Salary']","7ae523c3":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\nX.head()","decc6b57":"df_test = pd.DataFrame(scaler.transform(df_test.drop([\"Salary\"], axis=1)), columns=X.columns)\ndf_test.head()","ab7b2994":"#now lets split data in test train pairs\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)","50bcbd69":"# Here we choose LassoCV to per 5 fold cross validation on data so that we can get best alpha while training\n\nfrom sklearn.linear_model import LassoCV\n\nlasso = LassoCV(cv = 5)\nlasso.fit(X_train,y_train)","bfebd746":"# Prediction\n\ny_pred = lasso.predict(X_test)\n\npred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\npred_df.head()","53416ba8":"#Evaluating the Model\n\nfrom sklearn import metrics\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('R2 Value:', metrics.r2_score(y_test, y_pred))","b9362c50":"# Predicting Values of df_test using the above trained model\n\ndf_pred = lasso.predict(df_test)","b18ec1f5":"Predicted_df = pd.concat([ df_test, pd.DataFrame(df_pred, columns=[\"Predicted Salary\"])], axis = 1, sort=False)\nPredicted_df.head()","f6e84450":"# Here we choose RidgeCV to per 5 fold cross validation on data so that we can get best alpha while training\n\nfrom sklearn.linear_model import RidgeCV\n\nridge = RidgeCV(cv = 5)\nridge.fit(X_train,y_train)","f4a47c6d":"# Prediction\n\ny_pred_ = ridge.predict(X_test)\n\npred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_})\npred_df.head()","55909f57":"#Evaluating the Model\n\nfrom sklearn import metrics\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_)))\nprint('R2 Value:', metrics.r2_score(y_test, y_pred_))","63f9d2e7":"# Predicting Values of df_test using the above trained model\n\ndf_pred_ = ridge.predict(df_test)","e43e5b7d":"Predicted_df_ = pd.concat([ df_test, pd.DataFrame(df_pred_, columns=[\"Predicted Salary\"])], axis = 1, sort=False)\nPredicted_df_.head()","c3ba9b16":"> Since the given dataset contains both categorical and numerical dataset we have to separate them for further analysis.","e479e3af":"> R2 Score of the model is 0.53 which is not very good but since data quite small, upto this much accuracy is acceptable.\n","9429f656":"### LASSO Regression\n\n> Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters)","a370709c":"> Last row of pair plot graphs shows the plot of each column against our target column i.e. Salary. So from here we can se that none of the chosen column has skewness. ","4a7c99e1":"> Since no columns has appreciable co-relation, we are not using categorical collums for predictions","00e633f5":"## LASSO AND RIDGE Regression","45a1957b":"> R2 Score of the model is 0.54 which is not very good but since data quite small, upto this much accuracy is acceptable.\n","5da29cd1":"> Since  no cloumn has null values except Salary, we can move further without droping columns. (Not considering as null values of this columns will act as test set here)","9b713374":"### RIDGE Regression\n\n> Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value.","513d69a5":"> Since above mentioned columns have co relativity above 0.40, we are going to use them further"}}