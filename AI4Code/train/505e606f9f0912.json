{"cell_type":{"2d3a0d77":"code","fa1bf0f3":"code","32889933":"code","b936430d":"code","6579ffca":"code","96e7b858":"code","5acd21bf":"code","f49b5ea5":"code","e0074b59":"code","75cc7309":"code","db82adb7":"code","576b6733":"code","e7692aa4":"code","db2bc9f6":"code","c887bc91":"code","0fb4a508":"code","b1e9b956":"code","a02eeb09":"code","0702a29c":"code","0731d72f":"code","1a9efa4c":"code","97323372":"code","5aaabf57":"code","d5f16dc2":"code","aaa4f6d5":"code","a8259906":"code","8ef39ac3":"markdown","1935ac29":"markdown","4d583642":"markdown","0c097e48":"markdown","6d5b7533":"markdown","ba6f6164":"markdown","b8413f85":"markdown","fd805217":"markdown","3a626a97":"markdown","dd51c659":"markdown"},"source":{"2d3a0d77":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory'\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa1bf0f3":"df = pd.read_csv('\/kaggle\/input\/logistic-regression\/Social_Network_Ads.csv')","32889933":"df.head()","b936430d":"df.info()","6579ffca":"df.describe()","96e7b858":"df.isnull().all()","5acd21bf":"fig, axes = plt.subplots(2, 2, figsize=(15, 10))\nsns.boxplot(ax=axes[0,0],x= df['Age'], palette = \"Set1\")\naxes[0,0].set_title('Age Of People')\nsns.histplot(ax=axes[0,1],x='Age',data=df,color=\"g\")\naxes[0,1].set_title('Distribution Of Ages')\n\nsns.boxplot(ax=axes[1,0],data = df['EstimatedSalary'])\naxes[1,0].set_title('Estimated Salary Of People')\nsns.histplot(ax=axes[1,1],x='EstimatedSalary',data=df,color=\"y\")\naxes[1,1].set_title('Distribution Of Estimated Salary')\nplt.show()\n","f49b5ea5":"fig, axes = plt.subplots(1, 2, figsize=(15,5))\nsns.boxplot(ax=axes[0],x=df['Gender'], y=df['EstimatedSalary'], palette=\"PRGn\")\naxes[0].set_title('Estimated Salary By Gender')\n\nsns.boxplot(ax=axes[1],x=df['Gender'], y=df['Age'], palette=\"pink\")\naxes[1].set_title('Ages By Gender')\nplt.show()","e0074b59":"fig ,axes = plt.subplots(1,2, figsize=(15,5))\nsns.countplot(ax=axes[0],x='Purchased',data=df)\naxes[0].set_title('Number Of People Purchased')\nsns.countplot(ax=axes[1],x='Purchased',hue='Gender',data=df,palette=\"magma\")\naxes[1].set_title('Number Of People Purchased By Gender')\nplt.show()","75cc7309":"df.corr()","db82adb7":"f,ax = plt.subplots(figsize=(6, 5))\n\nsns.heatmap(df.corr(), annot=True, linewidths=0.5,linecolor=\"red\", fmt= '.1f',cmap='viridis',ax=ax)\nplt.show()","576b6733":"#preparing data\ndf.drop('User ID',axis = 1, inplace = True)\nlabel = {'Male': 0 ,\"Female\" : 1}\ndf['Gender'].replace(label, inplace= True)\n","e7692aa4":"# set inputs and outputs\nX = df.drop('Purchased',axis = 1)     \ny = df['Purchased']","db2bc9f6":"# we have to scale the data for better result\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nd_scaled = scaler.fit_transform(X)\ndata_scaled1 = pd.DataFrame(d_scaled)\ndata_scaled1.head()","c887bc91":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(d_scaled,y,test_size=0.20,random_state=42)\n","0fb4a508":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(C=0.1,max_iter = 500)\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)","b1e9b956":"# y = B + W*x1...\n\nprint(f'Weight Coefficient : {model.coef_}')\nprint(f'Bias : {model.intercept_}')","a02eeb09":"print(f'Test accuracy: {model.score(X_test,y_test)}')\nprint(f'Train accuracy: {model.score(X_train,y_train)}')","0702a29c":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","0731d72f":"from sklearn.metrics import confusion_matrix\n\ndf = pd.DataFrame(confusion_matrix(y_test, y_pred), columns = ['Predicted Positive', 'Predicted Negative'], \n                  index=['Actual Positive', 'Actual Negative'])\ndf","1a9efa4c":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nprint(\"Accuracy:\", accuracy_score(y_test,y_pred))\nprint(\"Precision:\", precision_score(y_test, y_pred, ))\nprint(\"Recall:\", recall_score(y_test,y_pred))\nprint(\"F1 Score:\", f1_score(y_test,y_pred))","97323372":"# We can visualize the confusion matrix\nimport scikitplot.metrics as splt\n\nsplt.plot_confusion_matrix(y_test,y_pred,figsize=(7,7))\nplt.show()","5aaabf57":"# Area Under Curve - AUC\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score, roc_curve\nmodel_roc_auc = roc_auc_score(y_test, model.predict(X_test))\n\nfpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='AUC (area = %0.2f)' % model_roc_auc)\nplt.plot([0, 1], [0, 1], 'r--')\nplt.xlim(([0.0, 1.0]))\nplt.ylim(([0.0, 1.05]))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()","d5f16dc2":"from imblearn.over_sampling import SMOTE\n\n\nsm = SMOTE(random_state = 2)\nX_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n\nclf = LogisticRegression()\nmodel_res = clf.fit(X_train_res, y_train_res)","aaa4f6d5":"print(f'Test accuracy {model_res.score(X_test,y_test)}')","a8259906":"print(f'Originally: {X_train.shape}')\nprint(f'Whit SMOTE: {X_train_res.shape}')","8ef39ac3":"### Prediction Results\n\n**we can see results in many different ways**\n","1935ac29":"### Finally, we can use the SMOTE method, which is a data augmentation process, to increase the learning rate.","4d583642":"<a id='4'><\/a>\n## Prediction","0c097e48":"<a id='5'><\/a>\n## Conclusion\n* As a result, we achieved 86 percent accuracy. This rate can be increased by changing some parameters. I will do that in the future.\n\n* Please give me feedback and comment. Thanks for everything...\n\n","6d5b7533":"* **As you can see, we have a perfectly clear dataset**. \n* **This is ridiculous because datasets should keep us busy :)**\n* **Ok so let's get started**","ba6f6164":"## Analysis Content\n1. [Python Libraries](#1)\n1. [Data analysis (EDA)](#2)\n1. [Data Distributions](#3)\n1. [Prediction](#4)\n1. [Conclusion](#5)\n\n\n\n","b8413f85":"<a id='2'><a\/>\n\n## Data analysis (EDA) ","fd805217":"## What will we learn this project? \n* Logistic regression methods\n* Seaborn library visualization techniques\n\n* SMOTE method","3a626a97":"<a id='3'><\/a>\n## Data Distributions","dd51c659":"## Introduction\n* This dataset includes whether people buy a product based on their age, gender, and estimated annual salary.\n\n* We will try to predict whether they bought it using the Logistic Regression method."}}