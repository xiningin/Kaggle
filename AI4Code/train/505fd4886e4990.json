{"cell_type":{"dec62915":"code","79d664b4":"code","8d961b0f":"code","f67b209c":"code","bf985aa4":"code","66b7a6af":"code","ec0d4166":"code","1f096ab0":"code","4e206e4d":"code","55ac95cb":"code","6b701384":"code","03784033":"code","9ca69524":"code","66a1c6eb":"code","644b6053":"code","179ae2ef":"code","9ab4b01d":"code","687a13b9":"code","746601b0":"code","f33eb277":"code","f89e5b8a":"code","7c37238b":"code","3dd8009b":"code","609b3403":"code","58d965f2":"code","11de114b":"code","edc032a6":"code","d934ef0f":"code","6e030dc9":"markdown","95cb5abc":"markdown","43968331":"markdown","4c0446ce":"markdown","e910d98d":"markdown","00c70291":"markdown","7b88e22b":"markdown","2cc5cec5":"markdown","36ef7b35":"markdown","b0b1a07f":"markdown","323eeb29":"markdown","a2894179":"markdown","27d6978f":"markdown","3622b801":"markdown","9519c24a":"markdown","cbf3ff9f":"markdown","3b1be070":"markdown","dbd0b3da":"markdown","159745c3":"markdown","e6c5244e":"markdown","ae82e1e8":"markdown","4bb7cbec":"markdown","486633da":"markdown","110136bf":"markdown","5b1c6b21":"markdown"},"source":{"dec62915":"!pip install torchinfo","79d664b4":"import os  \nimport glob\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\nimport PIL \nimport numpy as np\nimport matplotlib.pyplot as plt \n\nimport torch\nimport torch.nn as nn\nfrom torchinfo import summary \n\nimport torch.optim as optim\nfrom IPython.display import Image\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import transforms\n\nimport cv2","8d961b0f":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice ","f67b209c":"random_seed = 124\nnp.random.seed(random_seed)\n\ntorch.manual_seed(random_seed)\ntorch.backends.cudnn.deterministic = True","bf985aa4":"path = '\/kaggle\/input\/covidct\/'\n\npos_files = glob.glob(os.path.join(path, \"CT_COVID\",'*.*'))\nneg_files = glob.glob(os.path.join(path, 'CT_NonCOVID','*.*'))\n\nimages = pos_files + neg_files\nlabels = np.array([1]*len(pos_files)+[0]*len(neg_files))\n\nimages_tv, images_test, y_tv, y_test  = train_test_split(images, labels, shuffle=True, test_size=0.2, random_state=123)\nimages_train, images_val, y_train, y_val  = train_test_split(images_tv, y_tv, shuffle=True, test_size=0.25, random_state=123)","66b7a6af":"num_pos, num_neg = len(pos_files), len(neg_files)\n\nplt.title('Distribution of labels')\nplt.bar(['Positive', 'Negative'], [num_pos, num_neg])\nplt.show()","ec0d4166":"Image(images_train[1])\nImage(images_train[15])\nImage(images_train[66])","1f096ab0":"im = [cv2.imread(images_train[i]) for i in range(6)]\n\nfig,ax = plt.subplots(ncols=6, figsize=(18,6))\nfor i in range(len(im)):\n    ax[i].imshow(im[i],cmap='gray')\n\nplt.show()","4e206e4d":"print(f'Number of samples in each set (train, val, test): {len(y_train), len(y_val), len(y_test)}')\n\nprint(f'Number of positive samples in each set: {y_train.sum(), y_val.sum(), y_test.sum()}')","55ac95cb":"class CT_Dataset(Dataset):\n    def __init__(self, img_path, img_labels, img_transforms=None, grayscale=True):\n        self.img_path = img_path\n        self.img_labels = torch.Tensor(img_labels)\n        if (img_transforms is None) & (grayscale == True):\n            self.transforms = transforms.Compose([transforms.Grayscale(),\n                                                  transforms.Resize((250, 250)),\n                                                  transforms.ToTensor()])\n        elif grayscale == False:\n            self.transforms = transforms.Compose([transforms.Resize((250, 250)),\n                                                  transforms.ToTensor()])\n        else:\n            self.transforms = img_transforms\n    \n    def __getitem__(self, index):\n        # load image\n        cur_path = self.img_path[index]\n        cur_img = PIL.Image.open(cur_path).convert('RGB')\n        cur_img = self.transforms(cur_img)\n\n        return cur_img, self.img_labels[index]\n    \n    def __len__(self):\n        return len(self.img_path)","6b701384":"# define CNN mode\nclass Convnet(nn.Module):\n    \n    def __init__(self, dropout=0.5):\n        super(Convnet, self).__init__()\n        self.convnet = nn.Sequential(\n          # input (num_batch, 1, 250, 250)\n          nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3),  # (num_batch, 64, 248, 248)\n          nn.BatchNorm2d(64),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2),  # (num_batch, 64, 124, 124)\n\n          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3), # (num_batch, 128, 122, 122)\n          nn.BatchNorm2d(128),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2),  # (num_batch, 128, 61, 61)\n\n          nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3), # (num_batch, 256, 59, 59)\n          nn.BatchNorm2d(256),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2),  # (num_batch, 256, 29, 29)\n\n          nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3), # (num_batch, 128, 27, 27)\n          nn.BatchNorm2d(512),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2),  # (num_batch, 128, 13, 13)\n\n          nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3), # (num_batch, 64, 11, 11)\n          nn.BatchNorm2d(512),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2),  # (num_batch, 64, 5, 5)\n          nn.Flatten() # (num_batch, 1600)\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),  # Dropout before first linear layer since it has a large number of trainable parameters\n            nn.Linear(in_features= 12800, out_features=512),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(in_features=512, out_features=256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=128),\n            nn.ReLU(),\n            nn.Linear(in_features=128, out_features=1)\n        )\n    def forward(self, x):\n        x = self.convnet(x)\n        x = self.classifier(x)\n        return x","03784033":"vision_model = Convnet()\nsummary(vision_model, (32, 1, 250, 250))","9ca69524":"# define training function\n\ndef train_model(model, train_dataset, val_dataset, test_dataset, device, \n                lr=0.0001, epochs=30, batch_size=32, l2=0.00001, gamma=0.5,\n                patience=7):\n    model = model.to(device)\n\n    # construct dataloader\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n    # history\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\n    # set up loss function and optimizer\n    criterion = nn.BCEWithLogitsLoss()  \n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)  # pass in the parameters to be updated and learning rate\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=patience, gamma=gamma)\n\n    # Training Loop\n    print(\"Training Start:\")\n    for epoch in range(epochs):\n        model.train()  # start to train the model, activate training behavior\n\n        train_loss = 0\n        train_acc = 0\n        val_loss = 0\n        val_acc = 0\n\n        for i, (images, labels) in enumerate(train_loader):\n            # reshape images\n            images = images.to(device)  # reshape: from (128, 1, 28, 28) -> (128, 28 * 28) = (128, 284), move batch to device\n            labels = labels.to(device)  # move to device\n            # forward\n            outputs = model(images).view(-1)  # forward\n            pred = torch.sigmoid(outputs)\n            pred = torch.round(pred)\n    \n            cur_train_loss = criterion(outputs, labels)  # loss\n            cur_train_acc = (pred == labels).sum().item() \/ batch_size\n\n            # backward\n            cur_train_loss.backward()   # run back propagation\n            optimizer.step()            # optimizer update all model parameters\n            optimizer.zero_grad()       # set gradient to zero, avoid gradient accumulating\n\n            # loss\n            train_loss += cur_train_loss \n            train_acc += cur_train_acc\n        \n        # valid\n        model.eval()  # start to train the model, activate training behavior\n        with torch.no_grad():  # tell pytorch not to update parameters\n            for images, labels in val_loader:\n                # calculate validation loss\n                images = images.to(device)\n                labels = labels.to(device)\n\n                outputs = model(images).view(-1)\n\n                # loss\n                cur_valid_loss = criterion(outputs, labels)\n                val_loss += cur_valid_loss\n                # acc\n                pred = torch.sigmoid(outputs)\n                pred = torch.round(pred)\n                val_acc += (pred == labels).sum().item() \/ batch_size\n\n        # learning schedule step\n        scheduler.step()\n\n        # print training feedback\n        train_loss = train_loss \/ len(train_loader)\n        train_acc = train_acc \/ len(train_loader)\n        val_loss = val_loss \/ len(val_loader)\n        val_acc = val_acc \/ len(val_loader)\n\n        print(f\"Epoch:{epoch + 1} \/ {epochs}, lr: {optimizer.param_groups[0]['lr']:.5f} train loss:{train_loss:.5f}, train acc: {train_acc:.5f}, valid loss:{val_loss:.5f}, valid acc:{val_acc:.5f}\")\n    \n        # update history\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n    \n    test_acc = 0\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            # calculate outputs by running images through the network\n            outputs = model(images)\n\n            # the class with the highest energy is what we choose as prediction\n            pred = torch.sigmoid(outputs)\n            pred = torch.round(pred)\n            test_acc += (pred == labels).sum().item()\n\n    print(f'Test Accuracy:  {(test_acc \/ len(test_loader))}')\n\n    return history","66a1c6eb":"# Load the data\ntrain_dataset = CT_Dataset(img_path=images_train, img_labels=y_train)\nval_dataset = CT_Dataset(img_path=images_val, img_labels=y_val)\ntest_dataset = CT_Dataset(img_path=images_test, img_labels=y_test)\n\n# Train the CNN model\ncnn_model = Convnet(dropout=0.5)\nhist = train_model(cnn_model, train_dataset, val_dataset, test_dataset, device, lr=0.0002, batch_size=32, epochs=35, l2=0.09, patience=5)","644b6053":"# plot training curves\nepochs = range(1, len(hist['train_loss']) + 1)\n\nfig, ax = plt.subplots(1,2, figsize=(18,6))\nax[0].plot(epochs, hist['train_loss'], 'r-', label='Train')\nax[0].plot(epochs, hist['val_loss'], 'b-', label='Evaluation')\nax[0].set_title('Loss')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Loss')\nax[0].legend()\n\nax[1].plot(epochs, hist['train_acc'], 'r-', label='Train')\nax[1].plot(epochs, hist['val_acc'], 'b-', label='Evaluation')\nax[1].set_title('Accuracy')\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Acc')\nax[1].legend()\n\nplt.show()","179ae2ef":"img = PIL.Image.open(images_train[10])\n\nimg_trans = transforms.Compose([transforms.Grayscale(),\n                                transforms.RandomRotation(5),\n                                transforms.Resize((250, 250)),\n                                transforms.RandomAffine(degrees=0, scale=(1.1, 1.1), shear=0.9),\n                                transforms.ToTensor()\n                                ])\ntrans = img_trans(img)\n\nprint('Before Transformation')\ndisplay(img)\nprint('\\nAfter Transformation')\ndisplay(transforms.ToPILImage()(trans))","9ab4b01d":"train_dataset_full_aug = CT_Dataset(img_path=images_train, img_labels=y_train, img_transforms=img_trans)\nval_dataset = CT_Dataset(img_path=images_val, img_labels=y_val)\ntest_dataset = CT_Dataset(img_path=images_test, img_labels=y_test)","687a13b9":"# Train the CNN model\ncnn_model = Convnet()\nhist_full_aug = train_model(cnn_model, train_dataset_full_aug, val_dataset, \n                            test_dataset, device, lr=0.0001, batch_size=32, epochs=35,\n                            gamma=0.75, l2=0.09, patience=15)","746601b0":"# plot training curves\nepochs = range(1, len(hist_full_aug['train_loss']) + 1)\n\nfig, ax = plt.subplots(1,2, figsize=(20,6))\nax[0].plot(epochs, hist_full_aug['train_loss'], 'r-', label='Train')\nax[0].plot(epochs, hist_full_aug['val_loss'], 'b-', label='Evaluation')\nax[0].set_title('Loss')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Loss')\nax[0].legend()\n\nax[1].plot(epochs, hist_full_aug['train_acc'], 'r-', label='Train')\nax[1].plot(epochs, hist_full_aug['val_acc'], 'b-', label='Evaluation')\nax[1].set_title('Accuracy')\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Acc')\nax[1].legend()\n\nplt.show()","f33eb277":"train_dataset_og = CT_Dataset(img_path=images_train, img_labels=y_train)\ntrain_dataset_aug = CT_Dataset(img_path=images_train[:60], img_labels=y_train[:60], img_transforms=img_trans)\ntrain_dataset_fin = torch.utils.data.ConcatDataset([train_dataset_og,train_dataset_aug])\n\nval_dataset_og = CT_Dataset(img_path=images_val, img_labels=y_val)\nval_dataset_aug = CT_Dataset(img_path=images_val[:20], img_labels=y_val[:25], img_transforms=img_trans)\nval_dataset_fin = torch.utils.data.ConcatDataset([val_dataset_og, val_dataset_aug])\n\ntest_dataset = CT_Dataset(img_path=images_test, img_labels=y_test)","f89e5b8a":"print(len(train_dataset_fin))\nprint(len(val_dataset_fin))","7c37238b":"# Train the CNN model\ncnn_model = Convnet(dropout=0.5)\nhist_concat = train_model(cnn_model, train_dataset_fin, val_dataset_fin, test_dataset, device, lr=0.0001, l2=0.09, batch_size=32, epochs=75)","3dd8009b":"# plot training curves\nepochs = range(1, len(hist_concat['train_loss']) + 1)\n\nfig, ax = plt.subplots(1,2, figsize=(20,6))\nax[0].plot(epochs, hist_concat['train_loss'], 'r-', label='Train')\nax[0].plot(epochs, hist_concat['val_loss'], 'b-', label='Evaluation')\nax[0].set_title('Loss')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Loss')\nax[0].legend()\n\n\nax[1].plot(epochs, hist_concat['train_acc'], 'r-', label='Train')\nax[1].plot(epochs, hist_concat['val_acc'], 'b-', label='Evaluation')\nax[1].set_title('Accuracy')\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Acc')\nax[1].legend()\n\nplt.show()","609b3403":"import torchvision.models as models\n\nVGG_model = models.vgg16(pretrained=True)","58d965f2":"print(VGG_model)","11de114b":"VGG_model = models.vgg16(pretrained=True)\n\nfor name, param in VGG_model.named_parameters():\n    param.requires_grad = False\n\n# define out classifier\nbinary_classifier = nn.Sequential(\n   nn.Linear(in_features=25088, out_features=2048),\n   nn.ReLU(),\n   nn.Linear(in_features=2048, out_features=1024),\n   nn.ReLU(),\n   nn.Linear(in_features=1024, out_features=512),\n   nn.ReLU(),\n   nn.Linear(in_features=512, out_features=1)\n)\n\n# replace model class classifier attribute:\nVGG_model.classifier = binary_classifier","edc032a6":"train_dataset = CT_Dataset(img_path=images_train, img_labels=y_train, grayscale=False)\nval_dataset = CT_Dataset(img_path=images_val, img_labels=y_val, grayscale=False)\ntest_dataset = CT_Dataset(img_path=images_test, img_labels=y_test, grayscale=False)\n\n# Train the CNN model\nhist = train_model(VGG_model, train_dataset, val_dataset, test_dataset, device, lr=0.0001, batch_size=32, epochs=20, l2=0.2\n                   , patience=15)","d934ef0f":"# plot training curves\nepochs = range(1, len(hist['train_loss']) + 1)\n\nfig, ax = plt.subplots(1,2, figsize=(20,6))\nax[0].plot(epochs, hist['train_loss'], 'r-', label='Train')\nax[0].plot(epochs, hist['val_loss'], 'b-', label='Evaluation')\nax[0].set_title('Loss')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Loss')\nax[0].legend()\n\n\nax[1].plot(epochs, hist['train_acc'], 'r-', label='Train')\nax[1].plot(epochs, hist['val_acc'], 'b-', label='Evaluation')\nax[1].set_title('Accuracy')\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Acc')\nax[1].legend()\n\nplt.show()","6e030dc9":"Process the datasets and train the model","95cb5abc":"#### Print model summary to  visualize network structure","43968331":"# COVID-19 CT Image Classification Using PyTorch\n\n","4c0446ce":"## 1. Data Preparation\n\n* Import dependecies\n* Extract file paths and split into training, validation, and test partitions\n* Create Dataset Class to process and load images for PyTorch models ","e910d98d":"### 1.1 Import all dependencies","00c70291":"#### Results from Approach 1\n\n**Observations**\n\n*   Drop in overall accuracy for both training and validation sets\n*   Test accuracy came out to be 78\\% -- a considerable drop compared to the initial model \n* The drop in overall performance may suggest that perhaps lack of data could be a contributing factor rather than the diversity in available samples","7b88e22b":"Before we begin training, we need to prepare our images to be compatible with the VGG network. In particular, VGG-16 was built to handle color images, meaning the network processes our data using 3 channels for Red, Blue, and Green. Our dataset is a compilation of both color and grayscale images. In previous models, we handled this inconsistency by applying a grayscale transformation to the entire set. By setting the grayscale parameter for our dataset function to False, we can then transform our dataset to contain all color images.\n\nWith this done, we can then train our model and analyze the results","2cc5cec5":"### 3.1 Data Augmentation Approach \\#1 -- Augmenting the entire Training Set","36ef7b35":"### 3.2 Data Augmentation Approach \\#2 -- Concatenate Augmented Data to Original Dataset\nIn this approach, the aim is to see if adding augmented samples into our dataset can help fight overfitting. We know that our sample size is small, so perhaps adding variations of samples on top of our original data may allow our model to generalize better. In doing this, we also run the risk of increasing the bias of our model.\n\n\nIn the code below we will augment 60 images from the training set and 20 images from the validation set and concatenate them on the original data. ","b0b1a07f":"#### Augmentations Used\n* **Grayscale**\n* **Resize** $\\rightarrow$ 250 x 250\n* **RandomAffine**\n  * **Translate**: Width $\\rightarrow$ 0.01, Height $\\rightarrow$ 0.001\n  * **Image Scaling**: Width $\\rightarrow$ 1.2x, Height $\\rightarrow$ 1.2x\n  * **Shear**: 0.9\n* **RandomRotation**: 20 degrees\n* **Convert Image to Tensor**","323eeb29":"#### Results from Approach 2 \n**Observations**\n\n\n*   Test Accuracy: 87\\%\n*   Distinct gap between training and validation results remain as learning plateaus after 20 epochs\n*   The model achieves the highest accuracy for the hold out set compared to the previous models\n*   Further investigation is needed to reduce the gap between training in validation sets \n*   It would be interesting to see how the model reacts to adding a greater portion of augmented samples on top of the original dataset","a2894179":"## 3. Data Augmentation\n\nData Augmentation is a technique used to either diversify a training set or add more samples to it. To augment an image is to apply specified transformation to the pixels of the image array. Examples of augmentations include rescaling, resizing, and translating the image to name a few. Implementing these augmentationss in PyTorch is quite straightforward. In this section we will apply augmentations on the entire dataset as well as a fraction of our data to generate \"new\" samples to add to our dataset and run our model to compare performance.","27d6978f":"### 1.2 Generating Labels and Creating Sets for Modeling\n\nExtract file links for both postive and negative images and split the dataset into train, validation, and test sets","3622b801":"## 2. Model Development","9519c24a":"## Overview\n\n1. Data Preparation\n2. Initial Model Development\n3. Model Performance Using Data Augmentation\n4. Transfer Learning","cbf3ff9f":"### Results from Transfer Learning\n\n**Observations**:\n\n\n\n*   Learning curves remain stable, however the model overfits on the training set after 7 epochs\n*   Training loss converges to nearly 0.1 while validation loss plateaus at around 0.4 \n*   Validation accuracy peaks at 0.81 and converges to around 0.8 at the end of training\n*   Results on the test set yield an accuracy of 84\\% which beats our initial model\n\nOverall, the use of transfer learning for image classification is quite beneficial. VGG-16 provided us with just as good results even though the convolutional layers were trained on a completely different dataset. This showcases the power of transfer learning as we freezed the weights for the convolutional layers for this task. \n\n**Further Considerations**\n\nAfter applying several different techniques and models to tackle the task of detecting the presence of COVID-19 in CT images, I am left with only more ideas on how to address this task from different angles. Logically, the next step would be to apply data augmentation techniques to our VGG-16 model and compare performance. Since concatenating our original training set with the augmented images yielded the best results on the test set, this would be a great starting point. From there, I would love to try different pre-trained models such as AlexNet and see how performance compares with VGG-16. After that, perhaps creating an interface where users can submit their own images and get results in real time would be an interesting way to deploy this model. Such a design could be useful for practitioners in the medical field. Finally-- it goes without saying that further data collection would be of the highest priority. With new variants being discovered, it is essential for any model to adjust to take this into account.","3b1be070":"#### Results For Initial Model\n\nThe learning curves shown above show reasonable results.\n\n**Observations** \n\n\n*   The model begins to overfit on the training data after 10 epochs\n*   Training Accuracy generally reaches around 99\\% relatively quickly\n* Validation accuracy stabilizes at around 80\\%.\n* Training loss converges to near zero\n* Validation loss converges around 0.4\n* Accuracy on the test set is 88\\% -- a surprising result given the validation performance\n\nOverall, our first model achieved an accuracy of 88\\% for our hold out set. Given the size of our network and the lack of available data, these results are quite satisfying. With that being said, we cannot ignore the fact that our model overfits quite early in training. Since we achieve a near perfect score for training, I have no doubt we can continue to tune this model and achieve even better results on our hold-out set and bridge the gap between performance for training and validation.\n\n**Considerations to fight overfitting**\n\n\n*   Add L1 Regularization\n*   Data Augmentation (see next section)\n*   Transfer Learning (see later sections)\n*   More hyperparameter tuning","dbd0b3da":"## 4. Transfer Learning (VGG-16)\n\nIn this section, we will use the VGG-16 model which is pretrained on Image-Net to leverage the pre-trained parameters for this particular classification task. To achieve this, we will load the vgg-16 model using the torchvision library and then freeze the parameters for the \"features\" portiion of the model. We will then create our own custom \"classifier\" sequence of dense layers to overwrite the pre-trained classifier parameters trained on Image-Net. In this way, we leverage the visual power of VGG-16 while still training the model to classify positive or negative instances of COVID-19 in CT imgaes.","159745c3":"### 2.2 Model Training Procedure\nThe training sequence used for our CNN model is summarized below:\n\n\n*   **Loss Function**: *Binary Cross Entropy w\/ Logistic Loss*\n*   **Optimizer**: *Adam Optimization*\n  * To fight overfitting the following methods were used:\n      * **L2 Regularization**: Weight regularization using L2 Norm\n      * **Learning Schedule**: *Decrease the learning rate* over a set period of epochs\n\nParameters used for training:\n\n\n*   **Initial Learning Rate**: 0.0002\n*   Learning Schedule: \n  * **Gamma**: 0.5\n  * **Patience**: 7 epochs\n*   **Number of Epochs**: 35\n*   **Batch Size**: 32\n*   **L2 Weight Decay**: 0.09\n","e6c5244e":"Let's take a look at some of the images!","ae82e1e8":"### Dataset Class\n\n","4bb7cbec":"## Closing Remarks\n\nIf you've read up to this point, thanks for taking the time to explore my code! I hope you find my insights useful and I would love to hear any feedback\/critique as I am always open to learn from the experiences of others. Feel free to leave a comment and I would love to check out your notebook and see other approaches to this interesting problem. Alternatively you can also reach me at ayanga@stevens.edu !","486633da":"From the plot below, observe that we have a balanced yet very small dataset at our disposal. Not ideal, but never the end of the road. Let's do our best to make the most out of what we have before we collect more data.","110136bf":"Set Random Seed for reproducability","5b1c6b21":"### 2.1 Network Configuration\n\nThe model used for this exercise has several key layers:\n\n\n*   Conv2D $\\rightarrow$ Batch Normalization $\\rightarrow$ ReLU Activation $\\rightarrow$ AvgPooling2D\n  * Output Channels: 64\n  * Filter Size: 3x3\n  * AvgPooling Filter Size: 2x2\n*   Conv2D $\\rightarrow$ Batch Normalization $\\rightarrow$ ReLU Activation $\\rightarrow$ AvgPooling2D\n  * Output Channels: 128\n  * Filter Size: 3x3\n  * AvgPooling Filter Size: 2x2\n*   Conv2D $\\rightarrow$ Batch Normalization $\\rightarrow$ ReLU Activation $\\rightarrow$ AvgPooling2D\n  * Output Channels: 256\n  * Filter Size: 3x3\n  * AvgPooling Filter Size: 2x2\n*   Conv2D $\\rightarrow$ Batch Normalization $\\rightarrow$ ReLU Activation $\\rightarrow$ AvgPooling2D\n  * Output Channels: 512\n  * Filter Size: 3x3\n  * AvgPooling Filter Size: 2x2\n*   Conv2D $\\rightarrow$ Batch Normalization $\\rightarrow$ ReLU Activation $\\rightarrow$ AvgPooling2D $\\rightarrow$ Flatten\n  * Output Channels: 512\n  * Filter Size: 3x3\n  * AvgPooling Filter Size: 2x2\n  * Output Shape (After Flatten): (Batch Size, 1600)\n\nAfter the series of convolutions, we pass the parameters through a series of Dropout, Linear, and ReLU layers and produce an output of shape (Batch Size, 1)\n*   Dropout $\\rightarrow$ Linear $\\rightarrow$ ReLU Activation $\\rightarrow$ Dropout $\\rightarrow$ Linear $\\rightarrow$ ReLU Activation $\\rightarrow$ Linear $\\rightarrow$ ReLU Activation $\\rightarrow$ Linear\n  * Dropout Ratio: 0.5"}}