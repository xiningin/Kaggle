{"cell_type":{"344828dc":"code","ac53dcb2":"code","d00c630d":"code","acb7bf24":"code","cd313594":"code","2a611077":"code","e057ed50":"code","7326d03d":"code","059f3eee":"code","1e21c960":"code","7e8a63fa":"code","6b99bcc7":"code","eb3e665f":"code","1033bfc1":"code","2115edc0":"code","77e8bac8":"code","9c6b20a3":"code","c184f1a0":"code","aa673aec":"code","d678144a":"code","9ee6409a":"code","0058ac79":"code","06cca86d":"code","beb63214":"code","7509540d":"code","6d8c82fa":"code","bb0893f8":"code","fd6f5731":"code","4404e00e":"code","af181c86":"code","8605c3b2":"code","8eb40a62":"code","8b1c3669":"code","257e114c":"code","9ddbe51e":"code","c3eea596":"code","42f3fab0":"markdown","9113a6be":"markdown","c892f892":"markdown","adc12aca":"markdown","d3d0cb6c":"markdown","a24c63bf":"markdown","88d357da":"markdown","d4151559":"markdown","0d126a2a":"markdown","3d0f4bb2":"markdown","657117fa":"markdown","47098a10":"markdown","cf333cde":"markdown","89195162":"markdown","eead7bf4":"markdown","691a6a90":"markdown","a7737802":"markdown","a43d0420":"markdown","dbdf7ae0":"markdown","a2a999d9":"markdown","95cb6200":"markdown","056afa62":"markdown","b640feff":"markdown","e8759503":"markdown","493a5e01":"markdown","9c7e35a1":"markdown","f63fce92":"markdown","20491e6d":"markdown","b88cdb9e":"markdown"},"source":{"344828dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#data analysis libraries \n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n# Any results you write to the current directory are saved as output.","ac53dcb2":"#import train and test CSV files\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n#take a look at the training data and it's shape\nprint(train.shape, test.shape)\ntrain.describe(include=\"all\")\n","d00c630d":"print (train.columns)","acb7bf24":"train.head()\n","cd313594":"#Save the 'Id' column\ntrain_ID = train['PassengerId']\ntest_ID = test['PassengerId']\n\ntrain = train.drop('PassengerId',axis=1)\ntest = test.drop('PassengerId',axis=1)","2a611077":"corrmat = train.corr()\nplt.subplots(figsize=(20, 9))\nsns.heatmap(corrmat, vmax=.8, annot=True);\n","e057ed50":"sns.countplot(x='Sex', hue='Survived', data=train, palette='RdBu')\ntrain[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","7326d03d":"# What about the 'Embarked' feature ?\nsns.countplot(x='Embarked', hue='Survived', data=train, palette='RdBu')\nplt.xticks([0,1,2],['Southampton','Cherbourg ','Queenstown '])\n# train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","059f3eee":"sns.barplot(train.Pclass ,train.Survived)\n# train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","1e21c960":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","7e8a63fa":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.Survived.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\n# all_data.drop(['Survived'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","6b99bcc7":"missingData = all_data.isnull().sum().sort_values(ascending=False)\npercentageMissing = ((all_data.isnull().sum()\/all_data.isnull().count())*100).sort_values(ascending=False)\ntotalMissing = pd.concat([missingData, percentageMissing], axis=1, keys=['Total','Percentage'])\ntotalMissing","eb3e665f":"all_data[\"hasCabin\"] = (all_data[\"Cabin\"].notnull().astype('int'))\nsns.barplot(x=\"hasCabin\", y=\"Survived\", data=all_data)\nplt.show()\nall_data[['hasCabin', 'Survived']].groupby(['hasCabin'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n\n# all_data = all_data.dropna('Cabin',axis=1)\n# all_data = all_data.dropna('Embarked', axis=0)","1033bfc1":"all_data = all_data.drop('Cabin',axis=1)\nall_data = all_data.drop('Ticket',axis=1)\n\n\n# replacing the 2 missing values in the Embarked feature with S\n# since majority of people embarked in Southampton (S)\nall_data = all_data.fillna({\"Embarked\": \"S\"})\n\n\nall_data.head()\n","2115edc0":"all_data['Title'] = all_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(all_data['Title'], all_data['Sex'])","77e8bac8":"#get cell value: all_data.loc[0].at['Title']\n#set cell value: all_data.at[0,'Title'] = 'Mr'\n\nfor i,row in all_data.iterrows():\n    x = all_data.loc[i].at['Title']\n    if x in ['Capt','Col','Don' ,'Dr' ,'Major','Rev' ,'Sir']:\n        all_data.at[i,'Title']= 'Mr'\n    if x in ['Mlle','Ms' ,'Dona' ,'Lady']:\n        all_data.at[i,'Title']= 'Miss'\n    if x in ['Countess','Jonkheer','Mme']:\n        all_data.at[i,'Title'] = 'other'\n        \npd.crosstab(all_data['Title'], all_data['Sex'])","9c6b20a3":"allFemales = all_data[all_data['Sex']=='female'] # select all females\nThatOneFemale = allFemales[all_data['Title']=='Mr'] # select all females with title Mr\nThatOneFemale","c184f1a0":"# extracted the index of ThatOneFemale to be 796\nall_data.at[796,'Title']='Mrs'\n\npd.crosstab(all_data['Title'], all_data['Sex'])","aa673aec":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"other\": 5}\nfor i,row in all_data.iterrows():\n    if all_data.loc[i].at['Title'] in title_mapping:\n        all_data.at[i,'Title']= title_mapping[all_data.loc[i].at['Title']]\n# all_data['Title']\nall_data.head()","d678144a":"Mr_age = all_data[all_data['Title']==1].Age.mean()\nMiss_age = all_data[all_data['Title']==2].Age.mean()\nMrs_age = all_data[all_data['Title']==3].Age.mean()\nMaster_age = all_data[all_data['Title']==4].Age.mean()\nOther_age = all_data[all_data['Title']==5].Age.mean()\nprint(Mr_age, Miss_age, Mrs_age , Master_age, Other_age)\n\ngroup_age_mapping = {1:Mr_age, 2: Miss_age, 3:Mrs_age, 4:Master_age, 5:Other_age}\n\nfor index,row in all_data.iterrows():\n    if np.isnan(all_data.loc[index].at['Age']):\n        all_data.at[index,'Age'] = group_age_mapping[all_data.loc[index].at['Title']]\n        ","9ee6409a":"all_data.drop('Name',axis=1,inplace=True)","0058ac79":"sex_mapping = {\"male\": 0, \"female\": 1}\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n\nfor i,row in all_data.iterrows():\n    if all_data.loc[i].at['Sex'] in sex_mapping:\n        all_data.at[i,'Sex']= sex_mapping[all_data.loc[i].at['Sex']]\n    if all_data.loc[i].at['Embarked'] in embarked_mapping:\n        all_data.at[i,'Embarked']= embarked_mapping[all_data.loc[i].at['Embarked']]\nall_data.head()","06cca86d":"mode = all_data['Fare'].mode() # extract the mode\nall_data['Fare'].fillna(mode[0], inplace=True) # fill NaNs with the mode\n\nall_data.drop('Survived',axis=1, inplace=True) # drop survived column\n\nmissingData = all_data.isnull().sum().sort_values(ascending=False)\npercentageMissing = ((all_data.isnull().sum()\/all_data.isnull().count())*100).sort_values(ascending=False)\ntotalMissing = pd.concat([missingData, percentageMissing], axis=1, keys=['Total','Percentage'])\ntotalMissing\n","beb63214":"all_data.head()","7509540d":"FareBand = pd.qcut(all_data['Fare'], 4)\nFareBand.unique()","6d8c82fa":"# get cell value: all_data.loc[0].at['Title']\n# set cell value: all_data.at[0,'Title'] = 'Mr'\n\nfor i,row in all_data.iterrows():\n    currFare=all_data.loc[i].at['Fare']\n    if (currFare > -0.001 and currFare <=7.896):\n        all_data.at[i,'Fare'] = 1\n    if (currFare > 7.896 and currFare <=14.454):\n        all_data.at[i,'Fare'] = 2\n    if (currFare > 140454 and currFare <=31.275):\n        all_data.at[i,'Fare'] = 3\n    if (currFare > 31.275 and currFare <=512.329):\n        all_data.at[i,'Fare'] = 4\n        \nall_data.head(10)","bb0893f8":"target = train['Survived']\ntrainData = all_data[0:ntrain]\ntestData = all_data[ntrain:]\ntarget.shape, trainData.shape, testData.shape","fd6f5731":"from sklearn.model_selection import train_test_split\n\nX_train, X_test,y_train, y_test = train_test_split(trainData, target, test_size=0.2, random_state=0)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","4404e00e":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_test)\nacc_gaussian = round(accuracy_score(y_pred, y_test), 2)\nprint(acc_gaussian)","af181c86":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nacc_logreg = round(accuracy_score(y_pred, y_test), 2)\nprint(acc_logreg)","8605c3b2":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nacc_svc = round(accuracy_score(y_pred, y_test), 2)\nprint(acc_svc)","8eb40a62":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train, y_train)\ny_pred = decisiontree.predict(X_test)\nacc_decisiontree = round(accuracy_score(y_pred, y_test), 2)\nprint(acc_decisiontree)","8b1c3669":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_test)\nacc_randomforest = round(accuracy_score(y_pred, y_test) , 2)\nprint(acc_randomforest)","257e114c":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\ny_pred = gbk.predict(X_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) , 2)\nprint(acc_gbk)","9ddbe51e":"#predictions for submission\npredictions = gbk.predict(testData)","c3eea596":"output = pd.DataFrame({ 'PassengerId' : test_ID, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","42f3fab0":"Now let's see some correlations :)","9113a6be":"It seems that amongst the correlations in our heatmap (the numerical features), 'Pclass' seems to have the highest absolute value (0.34).\n\nBut wait, the categorical features (sex, cabin, ticket, embarked)?\n\n**What about the 'Sex' feature ?**","c892f892":"**Titanic Survival Predictions**\n \n \n**I hope you'll find value in this project of mine. Any upvotes or comments of support are highly appreciated :)**\n\nI tried to make this kernal a fun kernal as well as an educational one. you'll see later on a funny incedent when trying to deal with missing age data, however what was important about displaying it in that sense was not just for entertainment. **it's also about how to deal with such incidents since they are bound to happen.**\n\nWell let's get right to it shall we :)\n\nWe are here to predict survival rates of those unfortune lads on the RMS Titanic (Jack.. don't go jack..).\n(btw he was a fictional character... just saying...)\n\nThis project is divided into 4 parts:\n> 1. Importing Data\n> 2. Exploring the Data\n> 3. Feature Engineering and Dealing with Missing Data\n> 4. Fitting the data to several ML models.\n\nWho will survive? if you watched the movie you'd probably guess.. women and children only! \n\nWell, let's see if that true. I'm sure there's more to this tragic event.\n\n**First things First! let's import some libraries:**","adc12aca":"Now for the Fare, we can complete the missing value, with the most frequent value. \n\nand dropping th survived column since we won't be needing that anymore (but will be used for modeling).","d3d0cb6c":"Aha.. another important insight, from the count plot we can  understand that the majority of the passengers were embarking from Southampton.\n\nNow, let's check how many survived from each class (since it had the highest correlation with survived as we saw from the heatmap).","a24c63bf":"Aha.. we can see that from all survivors, about 3\/4 of them were women.\n\n\n**What about the 'Embarked' feature ?**","88d357da":"There are more classificationn algorithms that we can try. \n\n* like KNN or k-Nearest Neighbors\n* Perceptron\n* Stochastic Gradient Descent\n\nand more...\n\nhowever, it seems that Gradient Boosting Classifier gives us the best model. let's go ahead and submit the results ;)","d4151559":"Okay, after importing important libraries and inputting our data. let's take a peak.","0d126a2a":"Alrighty then, moving on!\n\nNow, let's convert the categorical titles to numeric.\n","3d0f4bb2":"Cool! no more missing values :)\n\nlet's take a look at our data and how it's coming along.\n","657117fa":"We can replace many titles with a more common name or classify them as 'other'.\n\n","47098a10":"As expected, those of the first class had the greatest survival rate while those of the third where the most unfortunate :(\n\nRemember that we said that it was mostly women and children?\n\nwell then, let's check the age feature and how to correlates with Survival.","cf333cde":"it doesn't seem like we don't need passengerId since it won't help us with our predictions (no correlation between it and 'Survived')\n\nlet's go ahead and drop it (but will save it for later submisson of the results file):","89195162":"Hmmm, everything seems in order. \n\nWe just have this 'Fare' feature that looks like it has potential to guide us with our predictions. \n\nLet catogrize the 'Fare' feature as well. we'll do that by dividing it to 4 caregories.","eead7bf4":"As suspected, Having a Cabin, as a good predictor of survival.\nThat's great. this could be an important feature in the endouver of predicting survival. \n\nlet's drop the 'Cabin' feature, and stay with 'hasCabin'.\n\nwe can also drop the Ticket column since it doesn't have any special pattern that could aid us with predictions.","691a6a90":"**Feature engineering & Dealing with missing data**\n\nMore than 3\/4 of Cabin data is missing. hmmm.. \nI want to understand why. could that be indicative of survival\/non-survival?\n\nwell, let's see..","a7737802":"As we saw from the missing data table, the second most missing data we are dealing with is the 'Age' feature. \n\nA creative way to deal with this would be to look at people's names and try to classify them to age groups based on their titles. \n\nlet's try to extract all the titles (notice that each title ends with a '.') ","a43d0420":"**Selecting the Best Model**\n\nAn accepted approch for selecting the best model is to try many differnt model and choose the model with the best results!","dbdf7ae0":"Now let's fill in missing Age data based on the mean for each Title:","a2a999d9":"Awesome! now that our data is ready. we can go ahead to the fun part of this project :)\n\n***Modeling and Machine Learn***\nFirst things first, let's split our data to training data (to fit the model) and test data (to evaluate the model later on).","95cb6200":"LOL!!!!\n\nit seems like we have a female Mr amongst our ship :P\n\nlet's try to figure out why...\n\nif you look at the Dr title, you'll realize that we have 7 male doctors and 1 female doctor.\n\nIt seems that i assumed all doctors were female (sorry for the sexism, wasn't intentional i swear :) )\n\nlet's fix that shall we ;)","056afa62":"After import libraries, naturally the first step after that would be to check the nature of our data. it's always important to see the shape of the data before doing anything else. \n\nWe have more rows for the training data than the test data, and it seems that we have an extra column for the training data (can you guess what it is?) \n\nIt seems that we have a lot of NaNs to deal with as well. that's to be expected though.\n\nlet's see our columns:","b640feff":"coverting the 'Sex' and 'Embarked' features to numeric:","e8759503":"**Some conclusions from the graph:**\n\n1. if it looks strange to you that there are people with ages 0 who survived, that's because their age was missing.\n\n2. most people that didn't survive were between the ages of 15-25\n\n3. most people who survived we between the ages of 20-35\n\n4. babies of ages <5 had high survival rate.\n\n(These insights are based on the graph. To get more accurate results, we could simply filter the data by age groups and plot a bar plot, or even add age groups as features to the dataframe. **we will do that later when we will try to fill missing data**)\n\nokay..\n\nBefore we dive into feature engineering, let's join our training data and test data so that we won't get lost later and stay consistent with changes across the data","493a5e01":"Now let's see how much of our data is missing:\n","9c7e35a1":"if you haven't figured out what's the extra column, it's actually 'Survived'. that's what we are trying to do here after all. to predict who survived and who didn't.\n\nIt doesn't seem that we have too many features to deal with. let's see the first few rows of the training data. maybe we can see something there:","f63fce92":"Now drop the Name column since we don't need it anymore.","20491e6d":"**Thank you for joining me on this wonderful journey to Data Science :)**\n\n**I hope you found value in this project of mine. Any upvotes or comments of support are highly appreciated :)**","b88cdb9e":"okay, let's adjust the 'Fare' feature:"}}