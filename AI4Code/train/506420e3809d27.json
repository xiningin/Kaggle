{"cell_type":{"b6d9f426":"code","0fbdbc28":"code","b21af72a":"code","d45f5c39":"code","d570d978":"code","9c9ffe4f":"code","d98624a4":"code","b109086f":"code","5fcb7a9a":"code","46825670":"code","6c6f8ecd":"code","fa9e5e4b":"code","8f13ba34":"code","3019924b":"code","e41ebcd5":"code","c87f0cfc":"code","f13fdf34":"code","4562a9e2":"code","17580f6a":"code","86305150":"code","747da395":"code","cbb93cfe":"code","71f4f6d2":"code","01d877c5":"code","9221c206":"code","5cb7e97b":"code","588261c2":"code","8ff3f88f":"code","8d89161b":"code","17af16b4":"code","f89d341c":"code","544262d1":"code","bb3cf97e":"code","934587aa":"code","e803afbe":"code","164a119f":"code","ff331ce1":"code","599d773a":"code","e6a7b0bb":"code","728f9c30":"code","de09345c":"code","d6c10d2e":"code","3865b049":"code","29801d64":"code","9fa37e5f":"code","9e7bc556":"code","07af6785":"code","5687ed12":"code","af31f397":"code","edc44bfe":"code","a2ba2087":"code","832a6dbe":"code","0a5c2fa3":"code","6c158633":"code","e5fb40c1":"code","fd6f1b27":"markdown","7173fab8":"markdown","fd301d2a":"markdown","e3aefc7f":"markdown","10907aaf":"markdown","a28f3b4e":"markdown","e9f41f62":"markdown","8ee79baa":"markdown"},"source":{"b6d9f426":"!wget https:\/\/raw.githubusercontent.com\/pandas-dev\/pandas\/master\/pandas\/tests\/data\/iris.csv","0fbdbc28":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","b21af72a":"df = pd.read_csv('iris.csv')","d45f5c39":"df.head()","d570d978":"df.columns","9c9ffe4f":"X = df.drop('Name',axis = 1)","d98624a4":"y = df[['Name']]","b109086f":"X.head()","5fcb7a9a":"y.head()","46825670":"from sklearn.preprocessing import StandardScaler","6c6f8ecd":"X = StandardScaler().fit_transform(X)","fa9e5e4b":"X","8f13ba34":"## it convert it to the numpy no problem","3019924b":"from sklearn.decomposition import PCA","e41ebcd5":"## how uch component you want\npca = PCA(n_components=2)","c87f0cfc":"principalComponents = pca.fit_transform(X)","f13fdf34":"principalComponents","4562a9e2":"## converting it to pandas dataframe\npdf = pd.DataFrame(data = principalComponents,columns=['pc1','pc2'])","17580f6a":"pdf","86305150":"pdf = pd.concat([pdf,df[['Name']]],axis = 1)","747da395":"pdf.head()","cbb93cfe":"pca.explained_variance_ratio_","71f4f6d2":"d = {'Iris-setosa':0,'Iris-versicolor':1,'Iris-virginica':2}","01d877c5":"target = pdf['Name'].map(d)","9221c206":"pdf['target']  = np.array(target)","5cb7e97b":"df = pdf.drop(['Name'],axis = 1)","588261c2":"df.target.unique()","8ff3f88f":"from sklearn.model_selection import train_test_split","8d89161b":"fm = list(df.columns)\nt = 'target'\nfm.remove(t)","17af16b4":"print(fm)\nprint(t)","f89d341c":"xt,xtst,yt,ytst = train_test_split(df[fm],df[t])","544262d1":"xt.head()","bb3cf97e":"xtst.head()","934587aa":"yt.head()","e803afbe":"ytst.head()","164a119f":"from sklearn.ensemble import RandomForestClassifier","ff331ce1":"r = RandomForestClassifier()","599d773a":"r.fit(np.array(xt),yt)","e6a7b0bb":"r.score(np.array(xtst),ytst)","728f9c30":"from sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784')","de09345c":"from sklearn.model_selection import train_test_split","d6c10d2e":"# test_size: what proportion of original data is used for test set\ntrain_img, test_img, train_lbl, test_lbl = train_test_split( mnist.data, mnist.target)","3865b049":"train_img = StandardScaler().fit_transform(train_img)\ntest_img = StandardScaler().fit_transform(test_img)","29801d64":"pca = PCA()","9fa37e5f":"pca.fit(train_img)","9e7bc556":"train_img = pca.transform(train_img)\ntest_img = pca.transform(test_img)","07af6785":"train_img","5687ed12":"test_img","af31f397":"from sklearn.linear_model import LogisticRegression","edc44bfe":"logisticRegr = LogisticRegression(solver = 'lbfgs')","a2ba2087":"logisticRegr.fit(train_img, train_lbl)","832a6dbe":"logisticRegr.predict(test_img[0].reshape(1,-1))","0a5c2fa3":"logisticRegr.predict(test_img[0:10])","6c158633":"logisticRegr.score(test_img, test_lbl)","e5fb40c1":"plt.imshow(train_img[2].reshape(28,28))","fd6f1b27":"![](http:\/\/i.stack.imgur.com\/jPw90.png)","7173fab8":"### suppose two soda charastics suppose the soda concentration and acidity are correlated so depending on them every blue dot is a particular soda. now if you want to create  a new property you can draw a line through the center of the soda cloud and project all points into the line .dont understand ??? see the picture","fd301d2a":"## first we normalizing the data","e3aefc7f":"####  The original data has 4 columns (sepal length, sepal width, petal length, and petal width). In this section, the code projects the original data which is 4 dimensional into 2 dimensions. I should note that after dimensionality reduction, there usually isn\u2019t a particular meaning assigned to each principal component. The new components are just the two main dimensions of variatio","10907aaf":"# Applying PCA in IRIS AND MNIST WITH SKLEARN","a28f3b4e":"![](http:\/\/i.stack.imgur.com\/Q7HIP.gif)","e9f41f62":"#### PCA will find the best line accroding to two criteria\n# 1) Variation value should be maximum\n### lets explain this: the variation values along this line should be maxium. pay attention to the spread (in stat we call it 'varience') of the red dot. the position of the red dot changes when the line rotates.you can see in some position of the line the variation become maximum\n\n### now  we reconstruct the original two charactaristictics of soda (position of blue dot) from the new one (position of red dot) \n\n# 2) Reconstruction error\n\n### so we reconstruct the error now we need to find the reconstruction errir. its easy you will find it with length of the connecting red line\n\n## you already understand what is the line\n## when the line is inline with the purple line that is our position\n## and you probably understand the new thing that\n# when the variation become maximum the reconstruction error become minimum\n\n# so this is the line that will be constructed by the PCA\n# thats how PCA find the main charctaristics that have a high varience","8ee79baa":"# What is a PCA ??\n## remember there are some math involved i am not going to cover it\n## because we have high lavel API. so i am just giving the intuation about the PCA\n\n### suppose you drink a lot of soda and you have different kinds of soda bottle in your table\n### if i tell you to make a list you can make list based on many charastics like\n#### 1) how old it is\n#### 2) the color\n#### 4) the color  etc etc\n\n####  so like that we can compose a various different kind of list but you see some of them has related propertise (common propertise) so what you are doing is redundent. what you can do to make it a little bit easier is suumarize the list with less charastices. this summarization is called PCA(principle component analysis).so what pca does is not exactly discard the redundant .it actually constructs some new charastics using the old one .may be like a combination of characteristics like [age-acidity+someting\/someting] in Fact PCA find the best possible charastics among all the liner combination.\n\n### now lets explain what i mean by the word \"summery\"\n\n### what are you looking for ??\n#### you are looking for some strong properties that make difference. there are a lot of properties that almost every soda can have and this is not a useful information.because they make them all the same.so PCA will find data that actually make the variation\n\n### What do you want??\n\n#### so if you want to predict some information that classify a certian soda from that information you need a data that have different variation in the training set .not almost the same.because if the data has greater varience you can do it better. look at the picture"}}