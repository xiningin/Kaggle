{"cell_type":{"ebf6c371":"code","3ea6f195":"code","02b5871a":"code","963a9768":"code","0cadbbd2":"code","dafb1507":"code","747dae5a":"code","00ededcf":"code","64c18825":"code","33811520":"code","aaa7be0e":"markdown","6bf6e7be":"markdown","690b120a":"markdown","78047928":"markdown","b31e80d7":"markdown","40b49fe7":"markdown","1b234e3e":"markdown","732e60d0":"markdown"},"source":{"ebf6c371":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/repository\/emanhamed-Houses-dataset-41e7392\/\"))\n\n# Any results you write to the current directory are saved as output.","3ea6f195":"# import the necessary packages\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nimport numpy as np\nimport glob\nimport cv2\nimport os\n\ndef load_house_attributes(inputPath):\n\t# initialize the list of column names in the CSV file and then\n\t# load it using Pandas\n\tcols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n\tdf = pd.read_csv(inputPath, sep=\" \", header=None, names=cols)\n\n\t# determine (1) the unique zip codes and (2) the number of data\n\t# points with each zip code\n\tzipcodes = df[\"zipcode\"].value_counts().keys().tolist()\n\tcounts = df[\"zipcode\"].value_counts().tolist()\n\n\t# loop over each of the unique zip codes and their corresponding\n\t# count\n\tfor (zipcode, count) in zip(zipcodes, counts):\n\t\t# the zip code counts for our housing dataset is *extremely*\n\t\t# unbalanced (some only having 1 or 2 houses per zip code)\n\t\t# so let's sanitize our data by removing any houses with less\n\t\t# than 25 houses per zip code\n\t\tif count < 25:\n\t\t\tidxs = df[df[\"zipcode\"] == zipcode].index\n\t\t\tdf.drop(idxs, inplace=True)\n\n\t# return the data frame\n\treturn df\n\ndef process_house_attributes(df, train, test):\n\t# initialize the column names of the continuous data\n\tcontinuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n\n\t# performin min-max scaling each continuous feature column to\n\t# the range [0, 1]\n\tcs = MinMaxScaler()\n\ttrainContinuous = cs.fit_transform(train[continuous])\n\ttestContinuous = cs.transform(test[continuous])\n\n\t# one-hot encode the zip code categorical data (by definition of\n\t# one-hot encoing, all output features are now in the range [0, 1])\n\tzipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n\ttrainCategorical = zipBinarizer.transform(train[\"zipcode\"])\n\ttestCategorical = zipBinarizer.transform(test[\"zipcode\"])\n\n\t# construct our training and testing data points by concatenating\n\t# the categorical features with the continuous features\n\ttrainX = np.hstack([trainCategorical, trainContinuous])\n\ttestX = np.hstack([testCategorical, testContinuous])\n\n\t# return the concatenated training and testing data\n\treturn (trainX, testX)\n\ndef load_house_images(df, inputPath):\n\t# initialize our images array (i.e., the house images themselves)\n\timages = []\n\n\t# loop over the indexes of the houses\n\tfor i in df.index.values:\n\t\t# find the four images for the house and sort the file paths,\n\t\t# ensuring the four are always in the *same order*\n\t\tbasePath = os.path.sep.join([inputPath, \"{}_*\".format(i + 1)])\n\t\thousePaths = sorted(list(glob.glob(basePath)))\n\n\t\t# initialize our list of input images along with the output image\n\t\t# after *combining* the four input images\n\t\tinputImages = []\n\t\toutputImage = np.zeros((64, 64, 3), dtype=\"uint8\")\n\n\t\t# loop over the input house paths\n\t\tfor housePath in housePaths:\n\t\t\t# load the input image, resize it to be 32 32, and then\n\t\t\t# update the list of input images\n\t\t\timage = cv2.imread(housePath)\n\t\t\timage = cv2.resize(image, (32, 32))\n\t\t\tinputImages.append(image)\n\n\t\t# tile the four input images in the output image such the first\n\t\t# image goes in the top-right corner, the second image in the\n\t\t# top-left corner, the third image in the bottom-right corner,\n\t\t# and the final image in the bottom-left corner\n\t\toutputImage[0:32, 0:32] = inputImages[0]\n\t\toutputImage[0:32, 32:64] = inputImages[1]\n\t\toutputImage[32:64, 32:64] = inputImages[2]\n\t\toutputImage[32:64, 0:32] = inputImages[3]\n\n\t\t# add the tiled image to our set of images the network will be\n\t\t# trained on\n\t\timages.append(outputImage)\n\n\t# return our set of images\n\treturn np.array(images)","02b5871a":"# import the necessary packages\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport os\n\n# construct the path to the input .txt file that contains information\n# on each house in the dataset and then load the dataset\nprint(\"[INFO] loading house attributes...\")\ninputPath = os.path.sep.join([\"..\/input\/repository\/emanhamed-Houses-dataset-41e7392\/Houses Dataset\/\",\"HousesInfo.txt\"]) \ndf = load_house_attributes(inputPath)","963a9768":"# load the house images and then scale the pixel intensities to the\n# range [0, 1]\nprint(\"[INFO] loading house images...\")\nimages = load_house_images(df, \"..\/input\/repository\/emanhamed-Houses-dataset-41e7392\/Houses Dataset\/\")\nimages = images \/ 255.0","0cadbbd2":"# partition the data into training and testing splits using 75% of\n# the data for training and the remaining 25% for testing\nsplit = train_test_split(df, images, test_size=0.25, random_state=42)\n(trainAttrX, testAttrX, trainImagesX, testImagesX) = split","dafb1507":"# find the largest house price in the training set and use it to\n# scale our house prices to the range [0, 1] (will lead to better\n# training and convergence)\nmaxPrice = trainAttrX[\"price\"].max()\ntrainY = trainAttrX[\"price\"] \/ maxPrice\ntestY = testAttrX[\"price\"] \/ maxPrice","747dae5a":"# import the necessary packages\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Dropout\nfrom keras.layers.core import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Input\nfrom keras.models import Model\n\ndef create_mlp(dim, regress=False):\n\t# define our MLP network\n\tmodel = Sequential()\n\tmodel.add(Dense(8, input_dim=dim, activation=\"relu\"))\n\tmodel.add(Dense(4, activation=\"relu\"))\n\n\t# check to see if the regression node should be added\n\tif regress:\n\t\tmodel.add(Dense(1, activation=\"linear\"))\n\n\t# return our model\n\treturn model\n\ndef create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n\t# initialize the input shape and channel dimension, assuming\n\t# TensorFlow\/channels-last ordering\n\tinputShape = (height, width, depth)\n\tchanDim = -1\n\n\t# define the model input\n\tinputs = Input(shape=inputShape)\n\n\t# loop over the number of filters\n\tfor (i, f) in enumerate(filters):\n\t\t# if this is the first CONV layer then set the input\n\t\t# appropriately\n\t\tif i == 0:\n\t\t\tx = inputs\n\n\t\t# CONV => RELU => BN => POOL\n\t\tx = Conv2D(f, (3, 3), padding=\"same\")(x)\n\t\tx = Activation(\"relu\")(x)\n\t\tx = BatchNormalization(axis=chanDim)(x)\n\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n\n\t# flatten the volume, then FC => RELU => BN => DROPOUT\n\tx = Flatten()(x)\n\tx = Dense(16)(x)\n\tx = Activation(\"relu\")(x)\n\tx = BatchNormalization(axis=chanDim)(x)\n\tx = Dropout(0.5)(x)\n\n\t# apply another FC layer, this one to match the number of nodes\n\t# coming out of the MLP\n\tx = Dense(4)(x)\n\tx = Activation(\"relu\")(x)\n\n\t# check to see if the regression node should be added\n\tif regress:\n\t\tx = Dense(1, activation=\"linear\")(x)\n\n\t# construct the CNN\n\tmodel = Model(inputs, x)\n\n\t# return the CNN\n\treturn model","00ededcf":"# create our Convolutional Neural Network and then compile the model\n# using mean absolute percentage error as our loss, implying that we\n# seek to minimize the absolute percentage difference between our\n# price *predictions* and the *actual prices*\nmodel = create_cnn(64, 64, 3, regress=True)\nopt = Adam(lr=1e-3, decay=1e-3 \/ 200)\nmodel.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)","64c18825":"# train the model\nprint(\"[INFO] training model...\")\nmodel.fit(trainImagesX, trainY, validation_data=(testImagesX, testY),\n\tepochs=200, batch_size=8)","33811520":"# make predictions on the testing data\nprint(\"[INFO] predicting house prices...\")\npreds = model.predict(testImagesX)\n\n# compute the difference between the *predicted* house prices and the\n# *actual* house prices, then compute the percentage difference and\n# the absolute percentage difference\ndiff = preds.flatten() - testY\npercentDiff = (diff \/ testY) * 100\nabsPercentDiff = np.abs(percentDiff)\n\n# compute the mean and standard deviation of the absolute percentage\n# difference\nmean = np.mean(absPercentDiff)\nstd = np.std(absPercentDiff)\n\n# finally, show some statistics on our model\nfrom babel.numbers import format_currency\nprint(\"[INFO] avg. house price: {}, std house price: {}\".format(\n\tformat_currency(df[\"price\"].mean(), 'USD', locale='en_US'),\n\tformat_currency(df[\"price\"].std(), 'USD', locale='en_US')))","aaa7be0e":"* To learn how we can title the images for each house, let\u2019s take a look at the load_house_images  function","6bf6e7be":"While our training loss is 38.64% our validation loss is at 58.45%, implying that, on average, our network will be ~58% off in its house price predictions.","690b120a":"Loading the house prices image dataset\n![house](https:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2019\/01\/keras_regression_cnns_houses.jpg)","78047928":"our house prices dataset includes four images associated with each house:\n\n    Bedroom\n    Bathroom\n    Kitchen\n    Frontal view of the house\n","b31e80d7":"this is the first publicly available dataset that includes both numerical\/categorical attributes along with images.\n\nThe numerical and categorical attributes include:\n\n    Number of bedrooms\n    Number of bathrooms\n    Area (i.e., square footage)\n    Zip code\n\nFour images of each house are also provided:\n\n    Bedroom\n    Bathroom\n    Kitchen\n    Frontal view of the house\n\nA total of 535 houses are included in the dataset, therefore there are 535 x 4 = 2,140 total images in the dataset.\n\nWe\u2019ll be pruning that number down to 362 houses (1,448 images) during our data cleaning.","40b49fe7":"Build CNN Model","1b234e3e":"For each house in our dataset, we will create a corresponding tiled image that that includes:\n\n    The bathroom image in the top-left\n    The bedroom image in the top-right\n    The frontal view in the bottom-right\n    The kitchen in the bottom-left\n\nThis tiled image will then be passed through the CNN using the house price as the target predicted value.","732e60d0":"Lets get Started \n\nThanks to @Adrian Pyimagesearch\n\nThe dataset we\u2019re using for this series of tutorials was curated by Ahmed and Moustafa in their 2016 paper, House price estimation from visual and textual features.\n\nPredicting house prices\u2026with images?\n![framework](https:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2019\/01\/keras_regression_cnns_flow-551x1024.jpg)"}}