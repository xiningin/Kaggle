{"cell_type":{"19ca63a9":"code","2ec886c7":"code","ef9023b9":"code","514bb2a7":"code","201eb787":"code","d7adf6ee":"code","806ffd87":"code","34ca9df7":"code","0c9e4058":"code","b133b47f":"code","09b37795":"code","8000f0cb":"code","f808ea8b":"code","e161fb0b":"code","7f0444ba":"code","d7467ae8":"code","4d7dcd19":"code","6a14ae44":"code","a3c34501":"code","9f51432b":"code","ff3dbb82":"markdown","80788c45":"markdown","bf26f9d3":"markdown","97e5d196":"markdown","05c8a927":"markdown","c23b2264":"markdown","7cb4de66":"markdown","d65803cc":"markdown","c19de74d":"markdown","3cf79a26":"markdown","22e3a533":"markdown","9596973a":"markdown","8db705e8":"markdown"},"source":{"19ca63a9":"import pandas as pd\n\nsurvey = pd.read_csv(\"..\/input\/masculinity\/masculinity.csv\")\nprint('total rows:',survey.shape[0])\nprint('total columns:',survey.shape[1])\nprint('breakdown of column types:')\ndisplay(survey.dtypes.value_counts())\npd.set_option('display.max_columns', None)\ndisplay(survey.head(3))","2ec886c7":"summary = survey.nunique().sort_values(ascending=False).reset_index()\nsummary = summary[summary['index'].str.contains(\"q\")]\nprint('total questions:',len(summary))\nprint('questions with more than 2 kind of answers:',len(summary[summary[0]>2]))\nprint('questions with only 2 kind of answers:',len(summary[summary[0]==2]))\ndisplay(summary[summary[0]>2],summary[summary[0]==2])","ef9023b9":"print(survey['q0007_0005'].unique())","514bb2a7":"cols_to_map = [\"q0007_0001\", \"q0007_0002\", \"q0007_0003\", \"q0007_0004\",\n       \"q0007_0005\", \"q0007_0006\", \"q0007_0007\", \"q0007_0008\", \"q0007_0009\",\n       \"q0007_0010\", \"q0007_0011\"]\ncount = 0\nfor i in cols_to_map:\n    survey[i] = survey[i].map({'Often':4 ,'Sometimes':3,'Rarely':2,'Never, but open to it':1,'Never, and not open to it':0})\n    count+=1\nprint('total',count,'columns mapped')","201eb787":"display(survey[\"q0007_0001\"].unique())\ndisplay(survey[\"q0007_0002\"].unique())\ndisplay(survey[\"q0007_0001\"].head(5),survey[\"q0007_0002\"].head(5))","d7adf6ee":"questions=[['Ask a friend for professional advice'],\n          ['Ask a friend for personal advice'],\n['Express physical affection to male friends, like hugging, rubbing shoulders'],\n['Cry'],\n['Get in a physical fight with another person'],\n['Have sexual relations with women, including anything from kissing to sex'],\n['Have sexual relations with men, including anything from kissing to sex'],\n['Watch sports of any kind'],\n['Work out'],\n['See a therapist'],\n['Feel lonely or isolated']]","806ffd87":"from matplotlib import pyplot as plt\nimport numpy as np\nplt.style.use('bmh')\nplt.figure(figsize=[15,12])\nplt.subplot(2,2,1)\nx=survey['q0007_0001']\ny=survey['q0007_0002']\nboth_finite = np.isfinite(x) & np.isfinite(y)\nplt.xlabel('professional advice')\nplt.ylabel('personal advice')\nm, b = np.polyfit(x[both_finite], y[both_finite], 1) #get best fit, 1 means linear.\nplt.plot(x,m*x+b)\nplt.scatter(x,y,alpha=0.06)\n\nplt.subplot(2,2,2)\nx=survey['q0007_0001']\ny=survey['q0007_0003']\nboth_finite = np.isfinite(x) & np.isfinite(y)\nplt.xlabel('professional advice')\nplt.ylabel('Express physical affection to male friends, like hugging, rubbing shoulders')\nm, b = np.polyfit(x[both_finite], y[both_finite], 1) #get best fit, 1 means linear.\nplt.plot(x,m*x+b)\nplt.scatter(x,y,alpha=0.06)\n\nplt.subplot(2,2,3)\nx=survey['q0007_0008']\ny=survey['q0007_0006']\nboth_finite = np.isfinite(x) & np.isfinite(y)\nplt.xlabel(questions[7])\nplt.ylabel(questions[5])\nm, b = np.polyfit(x[both_finite], y[both_finite], 1) #get best fit, 1 means linear.\nplt.plot(x,m*x+b)\nplt.scatter(x,y,alpha=0.06)\n\nplt.subplot(2,2,4)\nx=survey['q0007_0008']\ny=survey['q0007_0006']\nboth_finite = np.isfinite(x) & np.isfinite(y)\nplt.xlabel(questions[8])\nplt.ylabel(questions[5])\nm, b = np.polyfit(x[both_finite], y[both_finite], 1) #get best fit, 1 means linear.\nplt.plot(x,m*x+b)\nplt.scatter(x,y,alpha=0.06)","34ca9df7":"rows_to_cluster = survey.dropna(subset = [\"q0007_0001\", \"q0007_0002\", \"q0007_0003\", \"q0007_0004\", \"q0007_0005\", \"q0007_0008\", \"q0007_0009\"])\nprint('initial rows:',survey[[\"q0007_0001\", \"q0007_0002\", \"q0007_0003\", \"q0007_0004\", \"q0007_0005\", \"q0007_0008\", \"q0007_0009\"]].shape[0])\nprint('drop nan rows...')\nprint('total rows:',rows_to_cluster.shape[0])\nprint('total columns:',rows_to_cluster.shape[1])\n\nfrom sklearn.cluster import KMeans\nmodel = KMeans(2,random_state=42)\nmodel.fit(rows_to_cluster[[\"q0007_0001\", \"q0007_0002\", \"q0007_0003\", \"q0007_0004\", \"q0007_0005\", \"q0007_0008\", \"q0007_0009\"]])\nprint('The cluster centroids:\\n',model.cluster_centers_)\n","0c9e4058":"centroids = model.cluster_centers_\ncentroids_list = []\nfor i in range(len(centroids)):\n    centroids_list.append(list(centroids[i]))\n\nx=range(len([\"q0007_0001\", \"q0007_0002\", \"q0007_0003\", \"q0007_0004\", \"q0007_0005\", \"q0007_0008\", \"q0007_0009\"]))\nfor i in range(len(centroids_list)):\n    y = centroids_list[i]\n    plt.plot(x,y,label='cluster '+str(i))\nplt.legend()","b133b47f":"print(model.labels_)\nprint('total labels',len(model.labels_))\nprint('predicted labels\\n',pd.Series(model.labels_).value_counts())\ncluster_zero_indices = []\ncluster_one_indices = []\nfor i in range(len(model.labels_)):\n    if model.labels_[i] == 0: cluster_zero_indices.append(i)\n    else: cluster_one_indices.append(i)\nprint(cluster_zero_indices)","09b37795":"# survey['educ4'] = survey['educ4'].map({'Post graduate degree':3,'College or more':2,'Some college':1,'High school or less':0})","8000f0cb":"cluster_zero_df = rows_to_cluster.iloc[cluster_zero_indices]\ncluster_one_df = rows_to_cluster.iloc[cluster_one_indices]","f808ea8b":"from scipy.stats import chi2_contingency\ncols_compare = ['race2','racethn4', 'educ3', 'educ4', 'age3', 'kids', 'orientation']\noutcome = []\nsignificant_factors = []\n\nprint('\\nAnalyze significant factors to the clusters:')\nfor i in cols_compare:\n    x =list(zip(list(cluster_zero_df[i].value_counts()),list(cluster_one_df[i].value_counts())))\n    chi2, pval, dof, expect = chi2_contingency(x)\n    outcome.append([i,pval])\n    if pval <= 0.05: significant_factors.append([i,pval]) \n    print([i,pval])\n    \nprint('\\nThe factors that contribute to the different clusters could be:')\nfor i in range(len(significant_factors)):\n    print(significant_factors[i])","e161fb0b":"print('cluster 1 size:',len(cluster_zero_df))\nprint('cluster 2 size:',len(cluster_one_df))\nprint('\\nDistribution of education:')\nprint(cluster_zero_df['educ4'].value_counts()\/len(cluster_zero_df))\nprint(cluster_one_df['educ4'].value_counts()\/len(cluster_one_df))","7f0444ba":"#check optimal clusters\nmin_ = 1\nmax_ = 10\ninertia_ = []\ncluster_centers_ = []\nfor i in range(min_,max_):\n    model = KMeans(i,random_state=42)\n    model.fit(rows_to_cluster[[\"q0007_0001\", \"q0007_0002\", \"q0007_0003\", \"q0007_0004\", \"q0007_0005\", \"q0007_0008\", \"q0007_0009\"]])\n#     model.cluster_centers_\n    inertia_.append(model.inertia_)\n    \nplt.figure(figsize=[13,5])\nax1 = plt.subplot(1,2,1)\nplt.title('inertia of clusters used')\nplt.plot(list(range(min_,max_)),inertia_)\nplt.xlabel('n cluster')\nplt.ylabel('inertia')","d7467ae8":"model = KMeans(3,random_state=42)\nmodel.fit(rows_to_cluster[[\"q0007_0001\", \"q0007_0002\", \"q0007_0003\", \"q0007_0004\", \"q0007_0005\", \"q0007_0008\", \"q0007_0009\"]])\ncentroids = model.cluster_centers_\ncentroids_list = []\nfor i in range(len(centroids)):\n    centroids_list.append(list(centroids[i]))\n\nax = plt.subplot(1,1,1)\nx=range(len([\"q0007_0001\", \"q0007_0002\", \"q0007_0003\", \"q0007_0004\", \"q0007_0005\", \"q0007_0008\", \"q0007_0009\"]))\nname = ['A','B','C']\nfor i in range(len(centroids_list)):\n    y = centroids_list[i]\n    plt.plot(x,y,label='cluster'+str(name[i]))\nax.set_xticklabels([\"\",\"q0007_0001\", \"q0007_0002\", \"q0007_0003\", \"q0007_0004\", \"q0007_0005\", \"q0007_0008\", \"q0007_0009\"], rotation =90)\nplt.legend()","4d7dcd19":"print(model.labels_)\nprint('total labels',len(model.labels_))\nprint('predicted labels\\n',pd.Series(model.labels_).value_counts())\ncluster_a_indices = []\ncluster_b_indices = []\ncluster_c_indices = []\nfor i in range(len(model.labels_)):\n    if model.labels_[i] == 0: cluster_a_indices.append(i)\n    elif model.labels_[i] == 1: cluster_b_indices.append(i)\n    else: cluster_c_indices.append(i)\n\ncluster_a_df = rows_to_cluster.iloc[cluster_a_indices]\ncluster_b_df = rows_to_cluster.iloc[cluster_b_indices]\ncluster_c_df = rows_to_cluster.iloc[cluster_c_indices]","6a14ae44":"cols_compare = ['race2','racethn4', 'educ3', 'educ4', 'age3', 'kids', 'orientation']\noutcome = []\nsignificant_factors = []\n\nprint('\\nAnalyze significant factors to the clusters:')\nfor i in cols_compare:\n    x =list(zip(list(cluster_a_df[i].value_counts()),list(cluster_b_df[i].value_counts()),list(cluster_c_df[i].value_counts())))\n    print(x)\n    chi2, pval, dof, expect = chi2_contingency(x)\n    outcome.append([i,pval])\n    if pval <= 0.05: significant_factors.append([i,pval]) \n    print([i,pval])\n    \nprint('\\nThe factors that contribute to the different clusters could be:')\nfor i in range(len(significant_factors)):\n    print(significant_factors[i])","a3c34501":"column = 'orientation'\nprint('cluster A size:',len(cluster_a_df))\nprint('cluster B size:',len(cluster_b_df))\nprint('cluster C size:',len(cluster_c_df))\nprint('\\nDistribution of education:')\nprint('a\\n',cluster_a_df[column].value_counts()\/len(cluster_a_df))\nprint('b\\n',cluster_b_df[column].value_counts()\/len(cluster_b_df))\nprint('c\\n',cluster_c_df[column].value_counts()\/len(cluster_c_df))","9f51432b":"#we do a survey hypothesis testing on education\nfrom scipy.stats import chi2_contingency\nX = [ [30, 10],\n         [35, 15],\n         [28, 12] ]\nchi2, pval, dof, expect = chi2_contingency(X)\n","ff3dbb82":"* K means With 3 clusters produced the 2 clusters (cluster A, C) similiar to clusters previously seen when Kmeans with 2 clusters. \n* A third cluster (cluster B) is similar to the cluster C for some questions, but differ a lot in q3, q8, q9.\n    * (B lower) q3: Express physical affection to male friends, like hugging, rubbing shoulders\n    * (B higher) q8: Watch sports of any kind\n    * (B higher) q9: Work out","80788c45":"# Mapping the Data\n\nIn order for us to start thinking about using the KMeans algorithm with this data, we need to first figure out how to turn these responses into numerical data. Let's once again consider question 7. We can't cluster the data using the phrases `\"Often\"` or `\"Rarely\"`, but we can turn those phrases into numbers. For example, we could map the data in the following way: \n* `\"Often\"` -> `4`\n* `\"Sometimes\"` ->  `3`\n* `\"Rarely\"` -> `2` \n* `\"Never, but open to it\"` -> `1`\n* `\"Never, and not open to it\"` -> `0`.\n\nNote that it's important that these responses are somewhat linear. `\"Often\"` is at one end of the spectrum with `\"Never, and not open to it\"` at the other. The other values fall in sequence between the two. You could perform a similar mapping for the `\"educ4\"` responses (question 29), but there isn't an obvious linear progression in the `\"racethn4\"` responses (question 28).\n\nIn order to do this transformation, use the `map()` function. `map()` takes a dictionary as a parameter. For example, the following line of code would turn all the `\"A\"`s into `1`s and all the `\"B\"`s into `2`s in the column `\"col_one\"`.\n\n```py\ndf[\"col_one\"] = df[\"col_one\"].map({\"A\": 1, \"B\": 2})\n```\n\nWe've given you a list of the columns that should be mapped. Loop through the values of the list and map each column using the mapping described above.\n\nBe careful of your spelling! Punctuation and whitespace is important. Take a look at the `value_counts()` of one of these columns to see if the mapping worked.\n","bf26f9d3":"# Investigate the Cluster Members\n\nNow that we have the indices for each cluster, let's look at some stats about these two clusters. You can get the rows of the DataFrame that correspond to cluster zero by doing the following:\n\n```py\ncluster_zero_df = rows_to_cluster.iloc[cluster_zero_indices]\n```\n\nDo the same for `cluster_one_df`.\n\nFinally, let's look at some information about these two clusters. Print the `value_counts()` of the `educ4` column of each cluster. What do you notice? Try looking at different columns. For example, are the people in cluster zero significantly older than those in cluster one? You can look at the `age3` column to see.\n\nIf you divide the result of `value_counts()` by the size of the cluster, you get the percentage of people in each category rather than the total number. This will make it easier to compare the two clusters.","97e5d196":"**Cluster classification has a significiant relationship to educational background & sexual orientation!**","05c8a927":"* One cluster has a higher centroid for all questions. So it simply means we have 1 cluster that tend to do 'more' things and another cluster that does 'fewer' things.","c23b2264":"# Explore on Your Own\n\nGreat work! You've found out that by answering those 7 questions people don't fall into a \"masculine\" category or a \"feminine\" category. Instead, they seem to be divided by their level of education!\n\nNow it's time for you to explore this data on your own. In this project, we've really focused on question 7 and its sub-questions. Take a look at some of the other questions in the survey and try to ask yourself some interesting questions. Here's a list of questions you could dive into:\n\n* Which demographic features have stronger correlations with ideas of masculinity (sexual orientation, age, race, marital status, parenthood?)\n* Are certain beliefs or actions linked to more self-described masculine or feminine individuals?\n* How do insecurities change as people grow older?\n\n\nSpecial thanks to the team at FiveThirtyEight and specifically Dhrumil Mehta for giving us access to the data!\n","7cb4de66":"# Investigate the Data\n\nWelcome to the cumulative project on clustering algorithms! In this project, we will be investigating the way people think about masculinity by applying the KMeans algorithm to data from  <a href=\"https:\/\/fivethirtyeight.com\/\" target = \"_blank\">FiveThirtyEight<\/a>. FiveThirtyEight is a popular website known for their use of statistical analysis in many of their stories.\n\nTo begin, take a look at `masculinity-survey.pdf`. FiveThirtyEight and WNYC studios used this survey to get their male readers' thoughts on masculinity. After looking through some of the questions asked, take a look at FiveThirtyEight's article <a href=\"https:\/\/fivethirtyeight.com\/features\/what-do-men-think-it-means-to-be-a-man\/\" target = \"_blank\">What Do Men Think It Means To Be A Man?<\/a> to see their major takeaways. We're going to try to find more insights using machine learning.\n\nIn the code block below, we've loaded `masculinity.csv` into a DataFrame named `survey`. This file contains the raw responses to the masculinity survey. Let's start getting a sense of how this data is structured. Try to answer these questions using your Pandas knowledge:\n* What are the names of the columns? How do those columns relate to the questions in the PDF?\n* How many rows are there?\n* How is a question with multiple parts, like question 7, represented in the DataFrame?\n* How many people said they often ask a friend for professional advice? This is the first sub-question in question 7.\n\nTo answer that last question, use the `value_counts()` function. For example, `df[\"col_a\"].value_counts()` gives you a nice summary of the values found in `\"col_a\"` of the DataFrame `df`.\n\nYou may also want to print `survey.head()` to get a sense of all of the columns.\n","d65803cc":"* Notice cluster C has high percentage of Gay\/Bisexual.","c19de74d":"# What if we suspect more clusters?","3cf79a26":"# Build the KMeans Model\n\nIt's now time to start clustering! There are so many interesting questions we could ask about this data. Let's start by seeing if clusters form based on traditionally masculine concepts. \n\nTake a look at the first four sub-questions in question 7. Those four activities aren't necessarily seen as traditionally masculine. On the other hand, sub-questions 5, 8, and 9 are often seen as very masculine activities. What would happen if we found 2 clusters based on those 7 questions? Would we find clusters that represent traditionally feminine and traditionally masculine people? Let's find out.\n\nWe need to first drop all of the rows that contain a `NaN` value in any of the columns we're interested in. Create a new variable named `rows_to_cluster` and set it equal to the result of calling `dropna` on `survey`. `dropna` should have a parameter `subset` equal to a list of the 7 columns we want. If you don't include `subset`, the function will drop all rows that have an `NaN` in *any* column. This would drop almost all the rows in the dataframe!\n\nCreate a `KMeans` object named `classifier` where `n_clusters = 2`. Call `classifier`'s `.fit()` method. The parameter of `.fit()` should be the 7 columns we're interested in. For example, the following line of code will fit the model based on the columns `\"col_one\"` and `\"col_two\"` of the Dataframe `df`. \n\n```py\nclassifier.fit(df[[\"col_one\", \"col_two\"]])\n```\n\nMake sure to only include the columns that you want to train off of. Make sure to use `rows_to_cluster` rather than `survey` to avoid including those `NaN`s!\n\n\n\nAfter fitting your model, print out the model's `cluster_centers_`.\n","22e3a533":"# Use your understanding of unsupervised learning and clustering to find patterns in a survey conducted about masculinity.","9596973a":"# Separate the Cluster Members\n\nWhen we look at the two clusters, the first four numbers represent the traditionally feminine activities and the last three represent the traditionally masculine activities. If the data points separated into a feminine cluser and a masculine cluseter, we would expect to see one cluster to have high values for the first four numbers and the other cluster to have high values for the last three numbers.\n\nInstead, the first cluster has a higher value in every feature. Since a higher number means the person was more likely to \"often\" do something, the clusters seem to represent \"people who do things\" and \"people who don't do things\".\n\nWe might be able to find out more information about these clusters by looking at the specific members of each cluster. Print `classifier.labels_`. This list shows which cluster every row in the DataFrame corresponds to.\n\nFor example,  if `classifier.labels_` was `[1, 0 ,1]`, then the first row in the DataFrame would be in cluster one, the second row would be in cluster 0, and the third row would be in cluster one. A row represents one persons answers to every question in the survey.\n\nCreate two new empty lists named `cluster_zero_indices` and `cluster_one_indices`. Loop through `classifier.labels_` and whenever a label is `0` add that index to `cluster_zero_indices`. Do the same whenever a label is a `1`.\n\nPrint `cluster_zero_indices`","8db705e8":"# Plotting the Data\n\nWe now have 11 different features that we could use in our KMeans algorithm. Before we jump into clustering, let's graph some of these features on a 2D graph. Call `plt.scatter` using `survey[\"q0007_0001\"]` and `survey[\"q0007_0002\"]` as parameters. Include `alpha = 0.1`. We want to include `alpha` because many of the data points will be on top of each other. Adding `alpha` will make the points appear more solid if there are many stacked on top of each other.\n\nInclude axis labels on your graph. The x-axis corresponds with the first column you gave the `scatter()` function. So in this case, it corresponds to the question about asking a friend for professional advice.\n\nDoes it make sense that there are few points in the top left and bottom right corners of the graph? Why? Try graphing other dimensions against each other. Are there any combinations that give you surprising results?\n"}}