{"cell_type":{"b8d1de4e":"code","e24e2bdf":"code","0bfc53bb":"code","f091dd4d":"code","24fb0303":"code","d31c4137":"code","49904c3c":"code","f700ffd6":"code","caf05c2b":"code","4b383786":"code","b286fa4d":"code","adb858d0":"code","701fe3b2":"code","322c90b1":"code","99289d58":"code","5165fcd6":"code","b9d58dd4":"code","ea1de49c":"code","e91fc735":"code","b987a2ce":"code","46ed1068":"code","43548f7a":"code","963186ea":"code","a169f847":"code","ff1c0b9f":"code","d0838b89":"code","03975c94":"code","0eaf16e7":"code","268bf78f":"code","28b2ef5f":"code","82ea8891":"code","553aebe9":"code","03017c6b":"code","df2aa1bb":"code","ac191435":"code","1ad76bf3":"markdown","52e9c897":"markdown","46ab5f41":"markdown","e34b93fa":"markdown","d969e482":"markdown","ecbbb246":"markdown","fc40b512":"markdown","93acdbc7":"markdown"},"source":{"b8d1de4e":"try:\n  %tensorflow_version 2.x\nexcept Exception:\n  pass","e24e2bdf":"import tensorflow as tf","0bfc53bb":"tf.__version__","f091dd4d":"import cv2\nimport imageio\nimport matplotlib.pyplot as plt, zipfile\nimport numpy as np\nimport os\nimport PIL\nimport xml.etree.ElementTree as ET\nfrom tqdm import tqdm\nimport glob\nimport re\nfrom tensorflow.keras import layers\nimport time\nfrom PIL import Image\nfrom IPython import display\nfrom keras.preprocessing.image import img_to_array","24fb0303":"imageneslista=glob.glob('..\/input\/stanford-dogs-dataset\/images\/Images\/*')\nimageness=[]\nfor t in imageneslista:\n    imageness+=glob.glob(t+\"\/*\")\nprint(f\"numero de imagenes : {len(imageness)}\")\n\nrazas = glob.glob('..\/input\/stanford-dogs-dataset\/annotations\/Annotation\/*')\nannotation=[]\nfor i in razas:\n    annotation+=glob.glob(i+\"\/*\")\nprint(f\"numero de anotaciones : {len(annotation)}\")\n\nbreed_map={}\nfor annot in annotation:\n    razas=annot.split(\"\/\")[-2]\n    index=razas.split(\"-\")[0]\n    breed_map.setdefault(index,razas)\n    \nprint(f\"numero de razas : {len(breed_map)}\")","d31c4137":"breed_list = os.listdir('..\/input\/stanford-dogs-dataset\/annotations\/Annotation')\nprint(breed_list)","49904c3c":"plt.figure(figsize=(10, 10))\nfor i in range(9):\n    plt.subplot(331 + i) # showing 9 random images\n    breed = np.random.choice(breed_list) # random breed\n    dog = np.random.choice(os.listdir('..\/input\/stanford-dogs-dataset\/annotations\/Annotation\/' + breed)) # random image \n    img = Image.open('..\/input\/stanford-dogs-dataset\/images\/Images\/' + breed + '\/' + dog + '.jpg') \n    tree = ET.parse('..\/input\/stanford-dogs-dataset\/annotations\/Annotation\/' + breed + '\/' + dog) # init parser for file given\n    root = tree.getroot() # idk what's it but it's from documentation\n    objects = root.findall('object') # finding all dogs. An array\n    plt.imshow(img) # displays photo\n    for o in objects:\n        bndbox = o.find('bndbox') # reading border coordinates\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        #plt.plot([xmin, xmax, xmax, xmin, xmin], [ymin, ymin, ymax, ymax, ymin]) # showing border\n        #plt.text(xmin, ymin, o.find('name').text, bbox={'ec': None}) # printing breed\n    imagen1=img.crop((xmin, ymin, xmax, ymax))\n    plt.imshow(imagen1)\n    #plt.imshow(img)\n","f700ffd6":"def cropimagen(imagen):\n    mm = re.search('input\/stanford-dogs-dataset\/images\/Images\/(.+?).jpg', imagen)\n    if mm:\n        perro = mm.group(1)\n    img = Image.open('..\/input\/stanford-dogs-dataset\/images\/Images\/' + perro + '.jpg')\n    tree = ET.parse('..\/input\/stanford-dogs-dataset\/annotations\/Annotation\/' + perro) # init parser for file given\n    root = tree.getroot() # idk what's it but it's from documentation\n    objects = root.findall('object')\n    for o in objects:\n        bndbox = o.find('bndbox') # reading border coordinates\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n    imagen1=img.crop((xmin, ymin, xmax, ymax))  \n    return imagen1\n    ","caf05c2b":"width = 64\nheight = 64\nimagenescortadas=[]\nmyarray=np.zeros((20580,64,64,3))\n\n\nfor i,image in enumerate(imageness):\n    img=cropimagen(image).resize((width, height), Image.NEAREST)\n    #plt.imshow(img)\n    imgar=(img_to_array(img))\n    if imgar.shape == (64,64,3):\n      myarray[i,:,:,:]=imgar\n\n    #print(i)\n    \n    #imagenescortadas.append(imgar[:,:,0])\n    #imagenescortadas.append(imgar[:,:,1])\n    #imagenescortadas.append(imgar[:,:,2])\n    #if i==1:\n        #break\n    \n\n    \n#myarray = np.asarray(imagenescortadas)\nprint(myarray.shape)\ndel imagenescortadas","4b383786":"plt.imshow(myarray[20579, :,:,:]\/255)","b286fa4d":"a = ((myarray- 127.5)\/127.5)","adb858d0":"a = tf.cast (a, 'float32')","701fe3b2":"plt.imshow(((a[67]+1.)\/2.))","322c90b1":"plt.imshow(myarray[67]\/255)","99289d58":"BUFFER_SIZE = 20000\nBATCH_SIZE = 32","5165fcd6":"EPOCHS = 600\nnoise_dim = 100\nnum_examples_to_generate = 9\nWEIGHT_INIT = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.2)\n# We will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","b9d58dd4":"train_dataset = tf.data.Dataset.from_tensor_slices(a).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","ea1de49c":"def make_generator_model():\n    model = tf.keras.Sequential(\n        [\n          \n            tf.keras.layers.Dense(8*8*512, use_bias=False, input_shape=(100,)),\n         \n            tf.keras.layers.BatchNormalization(),\n           \n            tf.keras.layers.LeakyReLU(),\n           \n            tf.keras.layers.Reshape((8, 8, 512)),\n            \n       \n            tf.keras.layers.Conv2DTranspose(256, (5,5), strides=(2,2), padding='same', use_bias=False,\\\n                kernel_initializer=WEIGHT_INIT),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.LeakyReLU(),\n            tf.keras.layers.Dropout(0.3),\n            \n            tf.keras.layers.Conv2DTranspose(128, (5,5), strides=(2,2), padding='same', use_bias=False,\\\n                kernel_initializer=WEIGHT_INIT),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.LeakyReLU(),\n            tf.keras.layers.Dropout(0.3),\n            \n            tf.keras.layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False,\\\n                kernel_initializer=WEIGHT_INIT),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.LeakyReLU(),\n            \n            tf.keras.layers.Dense(3,activation='tanh', use_bias=False,\\\n                kernel_initializer=WEIGHT_INIT)\n        ]\n    )\n    return model","e91fc735":"generator = make_generator_model()\nprint(generator.summary())\n'''\nnoise=tf.random.uniform(\n    [1, 100],\n    minval=0,\n    maxval=1,\n    dtype=tf.dtypes.float32)\nprint(noise.shape)\n'''\nnoise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)\ngenerated_image=(generated_image)\nprint(generated_image.shape)\nplt.imshow(generated_image[0, :, :,:])","b987a2ce":"def make_discriminator_model():\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Conv2D(64, (4,4), strides=(2,2), padding='same', input_shape=[width,height,3],\\\n                kernel_initializer=WEIGHT_INIT),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.LeakyReLU(),\n\n            \n            tf.keras.layers.Conv2D(128, (4,4), strides=(2,2), padding='same',\\\n                kernel_initializer=WEIGHT_INIT),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.LeakyReLU(),\n\n            \n            tf.keras.layers.Conv2D(256, (4,4), strides=(2,2), padding='same',\\\n                kernel_initializer=WEIGHT_INIT),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.LeakyReLU(),\n\n            \n            \n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(1, activation='sigmoid')\n        ]\n    )\n    return model","46ed1068":"discriminator = make_discriminator_model()\nprint(discriminator.summary())\ndecision = discriminator(generated_image)\nprint (decision)","43548f7a":"def smooth_positive_labels(y):\n    return y - 0.3 + (np.random.random(y.shape) * 0.3)\n\ndef smooth_negative_labels(y):\n\treturn y + np.random.random(y.shape) * 0.5\n\n\ndef noisy_labels(y, p_flip):\n\n\tn_select = int(p_flip * y.shape[0].value)\n\t\n\tflip_ix = choice([i for i in range(y.shape[0].value)], size=n_select)\n\n\ty[flip_ix] = 1 - y[flip_ix]\n\treturn y","963186ea":"cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)","a169f847":"def discriminator_loss(real_output, fake_output):\n    real_output_smooth = smooth_positive_labels(tf.ones_like(real_output))\n    fake_output_smooth = smooth_negative_labels(tf.zeros_like(fake_output))\n\n    real_loss = cross_entropy(real_output_smooth, real_output)\n    fake_loss = cross_entropy(fake_output_smooth, fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","ff1c0b9f":"def generator_loss(fake_output):\n    fake_output_smooth = smooth_negative_labels(tf.ones_like(fake_output))\n    return cross_entropy(fake_output_smooth, fake_output)","d0838b89":"generator_optimizer = tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\ndiscriminator_optimizer = tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)","03975c94":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","0eaf16e7":"@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    return gen_loss,disc_loss","268bf78f":"def train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n\n    for image_batch in dataset:\n      train_step(image_batch)\n\n    # Produce images for the GIF as we go\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n\n    # Save the model every 50 epochs\n    if (epoch + 1) % 50 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n    #a,b=train_step(image_batch)\n    print(train_step(image_batch))\n\n  # Generate after the final epoch\n  display.clear_output(wait=True)\n  generate_and_save_images(generator,epochs,seed)","28b2ef5f":"def generate_and_save_images(model, epoch, test_input):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n  if epoch%25==0:\n    predictions = model(test_input, training=False)\n\n    fig = plt.figure(figsize=(10,5))\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(3, 3, i+1)\n        #plt.imshow(((predictions[i, :, :, :]+1)*127.5)\/255)\n        plt.imshow(((predictions[i, :, :, :]+1.)\/2.))\n        plt.axis('off')\n\n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n    plt.show()","82ea8891":"%%time\ntrain(train_dataset, EPOCHS)","553aebe9":"def display_image(epoch_no):\n  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))","03017c6b":"display_image(EPOCHS)","df2aa1bb":"anim_file = 'dcgan.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n  filenames = glob.glob('image*.png')\n  filenames = sorted(filenames)\n  last = -1\n  for i,filename in enumerate(filenames):\n    frame = 2*(i**0.5)\n    if round(frame) > round(last):\n      last = frame\n    else:\n      continue\n    image = imageio.imread(filename)\n    writer.append_data(image)\n  image = imageio.imread(filename)\n  writer.append_data(image)\n\nimport IPython\nif IPython.version_info > (6,2,0,''):\n  display.Image(filename=anim_file)","ac191435":"%%time\n# SAVE TO ZIP FILE NAMED IMAGES.ZIP\nz = zipfile.PyZipFile('images.zip', mode='w')\n\nfor k in range(1000):\n    generated_image = generator(tf.random.normal([1, noise_dim]), training=False)\n    f = str(k)+'.png'\n    img = ((generated_image[0,:,:,:]+1.)\/2.).numpy()\n    tf.keras.preprocessing.image.save_img(\n        f,\n        img,\n        scale=True\n    )\n    z.write(f); os.remove(f)\nz.close()","1ad76bf3":"Creando un arreglo de imagenes 64x64","52e9c897":"Modelo de discriminador","46ab5f41":"Analizando Dataset","e34b93fa":"Importando Librerias","d969e482":"Funciones loss","ecbbb246":"Modelo de generador","fc40b512":"Mostrando algunas imagenes de perros haciendo uso del bounding box","93acdbc7":"Entrenar el modelo"}}