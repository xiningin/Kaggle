{"cell_type":{"0a4090c5":"code","7a0ee2c2":"code","8490a38d":"code","2c167a36":"code","fff3d117":"code","520560ea":"code","fd87b5e7":"code","af10640f":"code","6601670d":"code","323e2e4a":"code","e368f154":"code","cedc2148":"code","12634ac0":"code","e9c2b16d":"code","be34e595":"code","160d0336":"code","49d39dbe":"code","1de73a40":"code","b82c9e50":"code","44782ada":"code","364a72e1":"code","dd9c9896":"code","f60fd253":"code","4a4e02b1":"code","695274e2":"code","45030c99":"code","fea89bf6":"code","ce660eef":"code","405dc379":"code","eae7afda":"code","232e1c01":"code","6d9a35a9":"code","80465b8e":"code","c2002728":"code","593f45bf":"code","d34f5843":"code","0db3b2b9":"code","e25433de":"code","1aa3cb79":"code","0323cc28":"code","2ff0e9cd":"code","297961ac":"code","6ff41649":"code","20af0d23":"code","c0c6f9e3":"code","07bf064f":"code","11456973":"code","4647f3d1":"code","7f8b4350":"code","7e983e0f":"code","922065e8":"code","8a5c491e":"code","dd19db9d":"code","e80478ab":"code","8f750641":"code","8b5c7ab7":"code","cba101df":"code","15d779a7":"code","e3cc291b":"code","18b9ab78":"code","d47bc41b":"code","276703da":"markdown","d6691400":"markdown","2161f9a3":"markdown","f949601f":"markdown","78ef6c22":"markdown","ff2a7c85":"markdown","c8bc3f15":"markdown"},"source":{"0a4090c5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport sklearn.metrics as metrics","7a0ee2c2":"warnings.filterwarnings(action='ignore')","8490a38d":"data_train = pd.read_csv('..\/input\/iba-ml1-mid-project\/train.csv')","2c167a36":"data_test = pd.read_csv('..\/input\/iba-ml1-mid-project\/test.csv')","fff3d117":"data_train['credit_line_utilization'].replace(',','.',regex=True, inplace=True)","520560ea":"data_train[\"credit_line_utilization\"] = pd.to_numeric(data_train[\"credit_line_utilization\"])","fd87b5e7":"data_test['credit_line_utilization'].replace(',','.',regex=True, inplace=True)\ndata_test[\"credit_line_utilization\"] = pd.to_numeric(data_test[\"credit_line_utilization\"])","af10640f":"data_train.head(20)","6601670d":"data_train.describe()","323e2e4a":"data_train.dtypes","e368f154":"data_train.info()","cedc2148":"# There is no use of 'Id' for predicting target variable.\ndata_train.drop(labels='Id', axis=1, inplace=True)","12634ac0":"# Removing outliers\n# I applied IsolationForest and manual removal of outliers, but both of them just decreased the score.","e9c2b16d":"numeric_features=data_train.columns.difference(['defaulted_on_loan']).values.tolist()\nnumeric_features","be34e595":"X = data_train[numeric_features]\ny = data_train['defaulted_on_loan']","160d0336":"X.isna().sum()","49d39dbe":"# Imputation using Simple Imputer\nX[numeric_features]=SimpleImputer(strategy='mean').fit_transform(X[numeric_features])","1de73a40":"X.isna().sum()","b82c9e50":"# Metrics\nfrom sklearn.metrics import accuracy_score, roc_auc_score,f1_score, confusion_matrix, classification_report\ndef evalua(y_pred,y_test):\n    \n    # Evaluate of predictions \n    accuracy = accuracy_score(y_test, y_pred) \n    roc = roc_auc_score(y_test, y_pred)\n    f1=f1_score(y_test, y_pred)\n\n    # Data test results\n    print('Evaluation of predictions: \\n')\n    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n    print(\"Area ROC: %.2f%%\" % (roc * 100.0))\n    print(\"F1 Score: %.2f%%\" % (f1 * 100.0))","44782ada":"X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.99,random_state=1236)","364a72e1":"rfc1=RandomForestClassifier()","dd9c9896":"rfc1.fit(X_train, y_train)","f60fd253":"y_proba=rfc1.predict_proba(X_test)\ny_proba","4a4e02b1":"y_prediction=rfc1.predict(X_test)\ny_proba=rfc1.predict_proba(X_test)\nevalua(y_prediction,y_test)","695274e2":"probabilities = rfc1.predict_proba(X_test)\npredictions = probabilities[:,1]\nf_r, t_r, threshold = metrics.roc_curve(y_test, predictions)\nroc_auc = metrics.auc(f_r, t_r)\nplt.figure(figsize=(8, 5))\nplt.title('*******ROC*******')\nplt.plot(f_r, t_r, 'g', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'b--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive')\nplt.xlabel('False Positive')\nplt.show()","45030c99":"# Using GridSearch for finding best parameters (tuning of RF)\nparams = {\n          'n_estimators': [400, 500], \n          'max_depth': [8, 9, 10], \n          'min_samples_leaf': [50, 60]\n         }\n\nrf_grid = GridSearchCV(estimator=rfc1, param_grid=params, cv=5, verbose = 1, \n                      n_jobs = -1)","fea89bf6":"# I applied the parameters obtained from GridSeachCV, as it takes time, I did not include it to kaggle notebook.\n#rf_grid.fit(X_train, y_train)","ce660eef":"#best_param = rf_grid.best_params_\n#best_param","405dc379":"rfc2=RandomForestClassifier(random_state=50, max_features='auto', n_estimators=500, max_depth=9, criterion='gini', min_samples_leaf=50)","eae7afda":"rfc2.fit(X_train, y_train)","232e1c01":"y_prediction=rfc2.predict(X_test)\ny_proba=rfc2.predict_proba(X_test)\nevalua(y_prediction,y_test)","6d9a35a9":"print(classification_report(y_test, y_prediction))","80465b8e":"weights_f=rfc2.feature_importances_\nweights_f","c2002728":"probabilities = rfc2.predict_proba(X_test)\npredictions = probabilities[:,1]\nf_r, t_r, threshold = metrics.roc_curve(y_test, predictions)\nroc_auc = metrics.auc(f_r, t_r)\nplt.figure(figsize=(8, 5))\nplt.title('*******ROC*******')\nplt.plot(f_r, t_r, 'g', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'b--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive')\nplt.xlabel('False Positive')\nplt.show()","593f45bf":"import xgboost as xgb","d34f5843":"#Train the XGboost Model for Classification\nmodelxgb=xgb.XGBClassifier(booster='gbtree', learning_rate=0.0095, max_depth=8, min_child_weight=1, n_estimators=1000, nthread=6)","0db3b2b9":"modelxgb.fit(X_train, y_train)","e25433de":"y_pred=modelxgb.predict(X_test)\nevalua(y_pred,y_test)","1aa3cb79":"print(classification_report(y_test, y_pred))","0323cc28":"probabilities = modelxgb.predict_proba(X_test)\npredictions = probabilities[:,1]\nf_r, t_r, threshold = metrics.roc_curve(y_test, predictions)\nroc_auc = metrics.auc(f_r, t_r)\nplt.figure(figsize=(8, 5))\nplt.title('*******ROC*******')\nplt.plot(f_r, t_r, 'g', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'b--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive')\nplt.xlabel('False Positive')\nplt.show()","2ff0e9cd":"roc_auc_score(y_test, predictions)","297961ac":"from catboost import CatBoostClassifier","6ff41649":"#Checking for any categorical features\ncate_features_index = np.where(X.dtypes != float)[0]","20af0d23":"cate_features_index","c0c6f9e3":"modelcb = CatBoostClassifier()","07bf064f":"modelcb.fit(X_train,y_train,cat_features=cate_features_index,eval_set=(X_test,y_test))","11456973":"pred = modelcb.predict_proba(X_test)\npred","4647f3d1":"y_pre=modelcb.predict(X_test)\nevalua(y_pre,y_test)","7f8b4350":"probabilities = modelcb.predict_proba(X_test)\npredictions = probabilities[:,1]\nf_r, t_r, threshold = metrics.roc_curve(y_test, predictions)\nroc_auc = metrics.auc(f_r, t_r)\nplt.figure(figsize=(8, 5))\nplt.title('*******ROC*******')\nplt.plot(f_r, t_r, 'g', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'b--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive')\nplt.xlabel('False Positive')\nplt.show()","7e983e0f":"roc_auc_score(y_test, predictions)","922065e8":"# Using GridSearch for finding best parameters (tuning of CatBoost)\nparams = {  \n    'depth': [8, 9, 10, 12],\n    'learning_rate': [0.0095,0.01, 0.02],\n    'iterations':[1000, 1500],\n    'l2_leaf_reg':[1, 2.5, 3,5, 5]\n         }\ncb_grid = GridSearchCV(estimator=modelcb, param_grid=params, cv=5, verbose = 1, \n                      n_jobs = -1)","8a5c491e":"# As it takes time, I completed this process on only jupyter notebook, not kaggle\n# cb_grid.fit(X_train, y_train)","dd19db9d":"#best_param = cb_grid.best_params_\n#best_param","e80478ab":"model = CatBoostClassifier(iterations=1500, learning_rate=0.01, l2_leaf_reg=3.5, depth=8, rsm=0.98, loss_function='Logloss', eval_metric='AUC',use_best_model=True,random_seed=42)","8f750641":"model.fit(X_train,y_train,cat_features=cate_features_index,eval_set=(X_test,y_test))","8b5c7ab7":"pred = model.predict_proba(X_test)\npreds= pred[:,1]\npred","cba101df":"y_pre=model.predict(X_test)\nevalua(y_pre,y_test)","15d779a7":"print(classification_report(y_test, y_pre))","e3cc291b":"probabilities = model.predict_proba(X_test)\npredictions = probabilities[:,1]\nf_r, t_r, threshold = metrics.roc_curve(y_test, predictions)\nroc_auc = metrics.auc(f_r, t_r)\nplt.figure(figsize=(8, 5))\nplt.title('*******ROC*******')\nplt.plot(f_r, t_r, 'g', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'b--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive')\nplt.xlabel('False Positive')\nplt.show()","18b9ab78":"roc_auc_score(y_test, predictions)","d47bc41b":"data_test['credit_line_utilization'].replace(',','.',regex=True, inplace=True)\ndata_test[\"credit_line_utilization\"] = pd.to_numeric(data_test[\"credit_line_utilization\"])\nX = data_test[numeric_features]\n\nX[numeric_features]=SimpleImputer(strategy='mean').fit_transform(X[numeric_features])\ntrial = model.predict_proba(X)[:,1]\npd.DataFrame(trial).to_csv('starry_file.csv')","276703da":"## Random Forest","d6691400":"### After obtaining the parameters from GridSearchCV, some parameters are also modified manually to increase the score.","2161f9a3":"# CatBoost","f949601f":"# XGBOOST","78ef6c22":"## The highest score is achieved by using CatBoosting.\n## I tried to remove outliers by Isolation Forest and manually, but it only decreased the score. Also, the use of over, under sampling and SMOTE did not affect the score and decreased it. That is why, I did not include those parts into the notebook.","ff2a7c85":"### Now, we will apply other methods to find the best one.","c8bc3f15":"### From the results, it is clear that there is a need for tuning."}}