{"cell_type":{"b3b7d09c":"code","21b9d138":"code","3e730a7e":"code","deaed226":"code","19a15cba":"code","b5acf64a":"code","e7d6c961":"code","f818539e":"code","eefe3c75":"code","95023f2b":"code","4d9a162f":"code","98c3eed5":"code","90ec587a":"code","50877f61":"code","7a7af28d":"code","a1cae507":"code","2cef71ff":"code","98a54ac4":"code","a69b66e4":"code","602ef621":"code","7847fa3b":"code","068d2662":"code","75901fe4":"code","65e6d630":"code","56f82192":"code","f4f1ba86":"code","e35cca0c":"code","c9b39146":"code","e4e81892":"code","274d7524":"code","021c3390":"code","ebf29a2c":"code","dd4cd3a7":"code","8c55acba":"code","30c0b9ce":"code","ab055680":"code","330d5bda":"code","71ccde19":"code","8f94794a":"code","05dadac7":"code","67ab3239":"code","97178a0d":"code","38769e36":"code","f9953b69":"code","0b8e8028":"code","6e7598e8":"code","04cfd3be":"code","498d98af":"code","fd18925c":"code","4bc6ac96":"code","cacc8b52":"code","3df18972":"markdown","4fd8a34e":"markdown","3eae9886":"markdown","27808f8f":"markdown","9d06cff0":"markdown","6328e650":"markdown","da41eacc":"markdown","31f774c5":"markdown","6572a3f2":"markdown"},"source":{"b3b7d09c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21b9d138":"df=pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","3e730a7e":"df.head()","deaed226":"x=sns.countplot(df['Class'])","19a15cba":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nsk=StandardScaler()\nrs=RobustScaler()\ndf['Time']=sk.fit_transform(df['Time'].values.reshape(-1,1))\ndf['Amount']=rs.fit_transform(df['Amount'].values.reshape(-1,1))","b5acf64a":"\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","e7d6c961":"x=sns.countplot(new_df['Class'])","f818539e":"X=new_df.iloc[:,:-1].values\nX.shape","eefe3c75":"Y=new_df.iloc[:,-1].values\nY.shape","95023f2b":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)","4d9a162f":"from sklearn.svm import SVC\nsvc = SVC(kernel='linear')\nsvc.fit(x_train,y_train)\npred_svc =svc.predict(x_test)","98c3eed5":"from sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, pred_svc))","90ec587a":"from sklearn.metrics import classification_report,accuracy_score\nprint(classification_report(y_test,pred_svc))","50877f61":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, pred_svc)","7a7af28d":"print(\"Precision:\",metrics.precision_score(y_test, pred_svc))\n\n# Model Recall: what percentage of positive tuples are labelled as such?\nprint(\"Recall:\",metrics.recall_score(y_test, pred_svc))","a1cae507":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(x_train,y_train)\npred_knn=knn.predict(x_test)","2cef71ff":"from sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, pred_knn))","98a54ac4":"from sklearn.metrics import classification_report,accuracy_score\nprint(classification_report(y_test,pred_knn))","a69b66e4":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, pred_knn)","602ef621":"print(\"Precision:\",metrics.precision_score(y_test, pred_knn))\n# Model Recall: what percentage of positive tuples are labelled as such?\nprint(\"Recall:\",metrics.recall_score(y_test, pred_knn))","7847fa3b":"from sklearn import naive_bayes\nNB = naive_bayes.GaussianNB()\nNB.fit(x_train,y_train)\npred_nb=NB.predict(x_test)","068d2662":"from sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, pred_nb))","75901fe4":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, pred_nb)","65e6d630":"from sklearn.metrics import classification_report,accuracy_score\nprint(classification_report(y_test,pred_nb))","56f82192":"print(\"Precision:\",metrics.precision_score(y_test, pred_nb))\n# Model Recall: what percentage of positive tuples are labelled as such?\nprint(\"Recall:\",metrics.recall_score(y_test, pred_nb))","f4f1ba86":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=250)\nrfc.fit(x_train, y_train)\npred_rfc = rfc.predict(x_test)","e35cca0c":"from sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, pred_rfc))","c9b39146":"from sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, pred_rfc))","e4e81892":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, pred_rfc)","274d7524":"from sklearn.metrics import classification_report,accuracy_score\nprint(classification_report(y_test,pred_rfc))","021c3390":"print(\"Precision:\",metrics.precision_score(y_test, pred_rfc))\n# Model Recall: what percentage of positive tuples are labelled as such?\nprint(\"Recall:\",metrics.recall_score(y_test, pred_rfc))","ebf29a2c":"# fit model no training data\nfrom xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(x_train, y_train)","dd4cd3a7":"# make predictions for test data\ny_predxg = model.predict(x_test)","8c55acba":"accuracy = accuracy_score(y_test, y_predxg)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","30c0b9ce":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val  = train_test_split(x_train, y_train, test_size = 0.10,random_state=42)","ab055680":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras.activations import relu,softmax\nfrom keras.regularizers import l2","330d5bda":"model = Sequential()\nmodel.add(Dense(16, input_dim=30,kernel_regularizer=l2(0.01), activation='relu'))\nmodel.add(Dense(32, kernel_regularizer=l2(0.01),activation='relu'))\nmodel.add(Dense(48, kernel_regularizer=l2(0.01),activation='relu'))\n# model.add(Dense(64, kernel_regularizer=l2(0.01),activation='relu',))\n# model.add(Dense(128, kernel_regularizer=l2(0.01),activation='relu',))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1, activation='sigmoid'))","71ccde19":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","8f94794a":"from keras.callbacks import ModelCheckpoint\ncheckpointer=ModelCheckpoint(filepath='Convolutional.hdf5',verbose=1,save_best_only=True)\nhistory = model.fit(x_train, y_train, epochs=50, batch_size=16,validation_data=(x_val,y_val))","05dadac7":"score=model.evaluate(x_test,y_test,verbose=1)               #evaluates the model\naccuracy=100*score[1]                                       \nprint('Test accuracy is %.4f%%' % accuracy)","67ab3239":"score=model.evaluate(x_train,y_train,verbose=1)               #evaluates the model\naccuracy=100*score[1]                                       \nprint('Test accuracy is %.4f%%' % accuracy)","97178a0d":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","38769e36":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","f9953b69":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","0b8e8028":"from sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(x_train, y_train)\n    training_score = cross_val_score(classifier, x_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","6e7598e8":"from sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(x_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(x_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(x_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(x_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_","04cfd3be":"log_reg_score = cross_val_score(log_reg, x_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n\nknears_score = cross_val_score(knears_neighbors, x_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, x_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, x_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","498d98af":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    return plt","fd18925c":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, x_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","4bc6ac96":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(log_reg, x_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nknears_pred = cross_val_predict(knears_neighbors, x_train, y_train, cv=5)\n\nsvc_pred = cross_val_predict(svc, x_train, y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, x_train, y_train, cv=5)","cacc8b52":"from sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))","3df18972":"# NEURAL NETWORKS","4fd8a34e":"# GETTING COUNT OF VALUES IN TARGET VARIABLE","3eae9886":"# XG BOOST","27808f8f":"# THE VALUES IN THE TARGET VARIBALES ARE IMBALANCED HENCE WE NEED TO BALANCE THE VALUES TAKING EQUAL SAMPLE FROM FRAUD VS NON FRAUD","9d06cff0":"# NAIVE BAYES","6328e650":"# SCALING THE VALUES ","da41eacc":"# K NEAREST NEIGHBOURS","31f774c5":"# RANDOM FOREST","6572a3f2":"# SUPPORT VECTOR MACHINE LINEAR KERNEL"}}