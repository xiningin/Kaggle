{"cell_type":{"23a613d3":"code","3d486eeb":"code","948c80a7":"code","704a666b":"code","c2cda12c":"code","c8b6ee9e":"code","6517a472":"code","c10970e4":"code","30ee0413":"code","75610ec2":"markdown","6993e5de":"markdown","a1053dbb":"markdown","6ad61c01":"markdown","318cd41c":"markdown","4aaa8c2e":"markdown","7ed465ca":"markdown","f506dbde":"markdown","d84a5a88":"markdown","b2b82494":"markdown","dbaab818":"markdown"},"source":{"23a613d3":"import statistics\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport matplotlib.colors as mcolors\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import RobustScaler\nfrom statistics import mode \nfrom sklearn.ensemble import VotingClassifier\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\ndef data_load(filename,date):\n    return pd.read_csv(filename,parse_dates = [date])\n\nFB = data_load('..\/input\/fbstock\/FB.csv','Date')\nFB_feat = FB.iloc[:, 1:-1].values\nFB_target = FB.iloc[:, -1].values\n#####################################################\n#Plot the closing price across the years\nfig, ax = plt.subplots(figsize=(9, 7))\nyears = mdates.YearLocator()   # every year\nmonths = mdates.MonthLocator()  # every month\nyears_fmt = mdates.DateFormatter('%Y')\n# Add the x-axis and the y-axis to the plot\nax.plot(FB['Date'].values,\n        FB['Close'], data = FB['Close'].values,\n        color='purple')\n\nax.xaxis.set_major_locator(years)\nax.xaxis.set_major_formatter(years_fmt)\nax.xaxis.set_minor_locator(months)\n\n# Set title and labels for axes\nax.set(xlabel=\"Date\",\n       ylabel=\"Closing Price\",\n       title=\"Closing Price for FB Stock\")\n\n# round to nearest years.\ndatemin = np.datetime64(FB['Date'][0], 'Y')\ndatemax = np.datetime64(FB['Date'].iloc[-1], 'Y') + np.timedelta64(1, 'Y')\nax.set_xlim(datemin)\n\n# format the coords message box\nax.format_xdata = mdates.DateFormatter('%Y-%m-%d')\nax.format_ydata = lambda x: '$%1.2f' % x  # format the price.\nax.grid(True)","3d486eeb":"FB.head(10)","948c80a7":"def basic_EDA(df):\n    size = df.shape\n    sum_duplicates = df.duplicated().sum()\n    sum_null = df.isnull().sum().sum()\n    return print(\"Number of Samples: %d,\\nNumber of Features: %d,\\nDuplicated Entries: %d,\\nNull Entries: %d\" %(size[0],size[1], sum_duplicates, sum_null))\nbasic_EDA(FB)","704a666b":"#Class Imbalance\ndef bar_plot(target):\n    unique, counts = np.unique(target, return_counts = True)\n    label = np.zeros(len(unique))\n    for i in range(len(unique)):\n        label[i] = (counts[i]\/target.shape[0])*100\n        plt.bar(unique,counts, color = ['burlywood', 'green'], edgecolor='black')\n        plt.text(x = unique[i]-0.15, y = counts[i]+0.01*target.shape[0], s = str(\"%.2f%%\" % label[i]), size = 15)\n    plt.ylim(0, target.shape[0])\n    plt.xticks(unique)\n    plt.xlabel(\"Target\")\n    plt.ylabel(\"Count\")\n    plt.show()\n    return unique, counts\n\n##Visualise Class Imbalance - Training Set\nnum_classes, feat_per_class = bar_plot(FB[\"Closing_Direction\"])","c2cda12c":"def feat_corr_analysis(corrmat):\n    f, ax = plt.subplots(figsize =(9, 8)) \n    #1 Heatmap\n    sns.heatmap(corrmat, vmin=0, vmax=1, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1)\n    plt.title(\"Heatmap - Correlation between data variables\")\n    \n    #2 Correlation Values and Features\n    correlations = corrmat.abs().unstack().sort_values(kind=\"quicksort\").reset_index()\n    correlations = correlations[correlations['level_0'] != correlations['level_1']]\n    Clos_dir_corr = corrmat['Closing_Direction']\n    Clos_dir_corr = FB_corr['Closing_Direction'].drop(['Closing_Direction'], axis =0)\n    Clos_dir_corr = Clos_dir_corr.sort_values(ascending = False)  \n    \n    #Plot with different colours for better visualisation\n    clist = [(0, \"red\"), (0.125, \"orange\"), (0.25, \"green\"), (0.5, \"blue\"), \n             (0.7, \"green\"), (0.75, \"orange\"), (1, \"red\")]\n    rvb = mcolors.LinearSegmentedColormap.from_list(\"\", clist)    \n    N = Clos_dir_corr.shape[0]\n    Col_range = np.arange(N).astype(float)\n    #Create Bar Plot\n    plt.figure(figsize=(15,10))\n    plt.bar(Clos_dir_corr.index, Clos_dir_corr[:],color=rvb(Col_range\/N))\n    plt.xlabel('Features')\n    plt.ylabel('Correlation')\n    plt.xticks(fontsize=8,rotation=90)\n    plt.title('Feature Correlation for Closing Direction')\n    plt.show()\n    return \n\nFB_corr = FB.corr()\nfeat_corr_analysis(FB_corr)","c8b6ee9e":"##########################################\ndef RFELogisticRegression(x_train,y_train,x_test,y_test):\n    reg_C = 50\n    classifier = LogisticRegression(solver = 'liblinear', penalty = 'l1',C = reg_C,max_iter = 5000)\n    classifier.fit(x_train,y_train)\n    accuracy_training = 100*classifier.score(x_train,y_train)\n    accuracy_testing = 100*classifier.score(x_test,y_test)\n    return accuracy_testing\n###########################################\ndef RFESVC(x_train,y_train, x_test, y_test, reg_C, gm):\n    classifier = SVC(kernel= 'rbf', C= reg_C, gamma= gm)# creates the model\n    classifier.fit(x_train,y_train)#fits the data\n    accuracy_training = 100*classifier.score(x_train,y_train)\n    accuracy_testing = 100*classifier.score(x_test,y_test)\n    return accuracy_testing\n############################################\ndef RFERandomForest(x_train,y_train,x_test,y_test):\n    classifier = RandomForestClassifier(criterion= 'gini', \n                            max_depth= 2,\n                            n_estimators= 10,\n                            random_state= 0)\n    classifier.fit(x_train,y_train)\n    accuracy_training = 100*classifier.score(x_train,y_train)\n    accuracy_testing = 100*classifier.score(x_test,y_test)\n    return accuracy_testing\n############################################\n#Ensemble Model - Does not consider Random Forest due to lower accuracy    \nLRC = LogisticRegression(solver = 'liblinear', penalty = 'l1',C = 50,max_iter = 5000)\nSVCC = SVC(kernel= 'rbf', C= 60, gamma= 0.001,probability=True)\nEnsemble_model = VotingClassifier(estimators=[('lr', LRC), ('SVC', SVCC)], voting='soft', weights=[1,1.2])\n#############################################","6517a472":"RF_Acc = []\nEM_Acc = []\nSVM_Acc = []\nLR_Acc = []\n#Number of Windows \/ Size of Training Set\/ Size of Test Set\nn_fold = 5;train_set = 459; test_set = 113;\nsc_X = RobustScaler()\nFB_index= np.asarray(list(range(0, FB_feat.shape[0]+1)))\nwindow = 0\nfor i in range(5):\n    train_index = FB_index[(FB_index >= window) & (FB_index < window + train_set)]\n    test_index = FB_index[(FB_index >train_index.max()) & (FB_index <= train_index.max()+test_set)]\n    X_train, X_test = FB_feat[train_index], FB_feat[test_index]\n    y_train, y_test = FB_target[train_index], FB_target[test_index]\n    X_train = sc_X.fit_transform(X_train)\n    X_test = sc_X.transform(X_test)\n    #Logistic Regression\n    LR = RFELogisticRegression(X_train,y_train,X_test,y_test)\n    LR_Acc.append(LR)\n    #SVM\n    SVM = RFESVC(X_train,y_train,X_test,y_test,60,0.001)\n    SVM_Acc.append(SVM)\n    #Random Forest\n    RF = RFERandomForest(X_train,y_train,X_test,y_test)\n    RF_Acc.append(RF)\n    #Ensemble Model\n    Ensemble_model.fit(X_train,y_train)\n    EM_Score = 100*Ensemble_model.score(X_test,y_test)\n    EM_Acc.append(EM_Score)\n    #Step forward\n    window += 259\n\n#Mean Final Accuracy across all Windows\nSVM_Final_Acc = np.mean(SVM_Acc)\nLR_Final_Acc = np.mean(LR_Acc)\nRF_Final_Acc = np.mean(RF_Acc)\nEM_Final_Acc = np.mean(EM_Acc)","c10970e4":"#Comparison between individual models and ensemble\nprint(\"SVM Model: %.2f%% Standard Deviation of %.2f \" % (SVM_Final_Acc,statistics.stdev(SVM_Acc)))\nprint(\"LR Model: %.2f%% Standard Deviation of %.2f \" % (LR_Final_Acc,statistics.stdev(LR_Acc)))\nprint(\"RF Model: %.2f%% Standard Deviation of %.2f \" % (RF_Final_Acc,statistics.stdev(RF_Acc)))\nprint(\"Ensemble Model: %.2f%% Standard Deviation of %.2f \" % (EM_Final_Acc,statistics.stdev(EM_Acc)))","30ee0413":"#Analysis of each Window Accuracy\nResults = [\"W1\",\"W2\",\"W3\",\"W4\",\"W5\"]\ntitle = 'Accuracy Across Windows'\nplt.plot( Results, LR_Acc, marker='o', markerfacecolor='steelblue', markersize=12, color='navy', linewidth=4, label = 'LR')\nplt.plot( Results, SVM_Acc, marker='o', markerfacecolor='yellowgreen',markersize=12, color='olivedrab', linewidth=4,label = 'SVM')\nplt.plot( Results, RF_Acc, marker='o', markerfacecolor='bisque',markersize=12, color='darkorange', linewidth=4,label = 'RF')\nplt.plot( Results, EM_Acc, marker='o', markerfacecolor='salmon',markersize=12, color='darkred', linewidth=4,label = 'Ensemble')\nplt.xlabel('Time Series Windows')\nplt.ylabel('Test Accuracy %')\nplt.grid()\nplt.legend()\nplt.show()","75610ec2":"From mid 2012 to early 2018 the tech company showed a consistent uptrend market. However, the closing price has started to consistently drop after this period, presenting a downward trend until the last day of data collection (28\/11\/2018).","6993e5de":"As the dataset was built manually the result above was expected. No null values or duplicated entries. All values are numeric. The size of the dataset is relatively small, for this reason all features are used and no PCA or dimensionality reduction is performed. \n\nNext, we analyse the class imbalance:","a1053dbb":"This plot allows the analysis of the following:\n* Window 2 was the most challenging period to predict, as most models presented their lower accuracy on this timeframe;\n* SVM was the only model to present accuracy increase between W4 and W5. The lower accuracy on W5 can be explained as it contains the end of the upward trend, which is more difficult to forecast;\n* The ensemble model (red) presents a good balance between the SVM and LR models, showing that the model weights are appropriate; \n* LR accuracy oscillates heavilly, speacially between W2 and W4, as indicated by the Standard Deviation results. A similar accuracy across the validation windows is desired to inspire more confidence in future forecasts. \n","6ad61c01":"# Conclusion\nThe results have shown that SVM outperforms logistic regression and random forest algorithms for stock movement prediction. The inherent capability of SVM to avoid overfitting contributed to this conclusion. During experimental testing, random forest  showed a high tendency to overfit to training data achieving contradictory results between training and test accuracy. Essential contributions from literature allowed the construction of a model that can forecast with similar accuracies of current published papers. As it is the case for market traders, the usage of technical indicators and global indexes have shown to be a powerful strategy to support forecast decisions. ","318cd41c":"From the correlation analysis is interesting to see how the additional features have a higher correlation with the closing price. Preliminary tests shown that these additional features significantly helped the model to predict the stock behaviour. This analysis can help to select other technical indicators or market indexes in the future.  ","4aaa8c2e":"First let's visualise the stock price variation across the years.","7ed465ca":"# Models\n\nThe approach taken to train and test the machine learning models is referred to as sliding window or walk-forward. This method consists of dividing the dataset into several time frames. The sliding window routine creates training and test set of equal size for each window; the sets go forward in time according to the step size selected. By applying this method, the model is based on more recent data.\n\nThe final model and parameters shown below are the ones that provided the best accuracy considering the average accuracy from all windows. For this project, is decided to compare the performance of SVM, Logistic Regression and Random Forest. ","f506dbde":"There is a balanced distribution between the number of times the price went up (class 1) or down (class 0), as expected for this kind of financial data. Next, the features correlation is analysed:","d84a5a88":"The block below divides the dataset into five windows, fit, train and retrieves the accuracy of each model defined above. The training and testing split ","b2b82494":"SVM has shown to be the most accurate model, followed by the ensemble containing LR and SVM. SVM also presents the lower deviation from the mean, i.e. it is the most reliable across all the cross validation windows. The figure below allows the analysis of each model across the individual windows.","dbaab818":"# EDA"}}