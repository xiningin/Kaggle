{"cell_type":{"772a5e41":"code","a320cd07":"code","b84718d6":"code","1ca08774":"code","4baecdcb":"code","d61878e0":"code","f798b8aa":"code","c0fe38bc":"code","7b844fad":"code","6e7e1d9d":"code","cad85725":"code","8dc1d44f":"code","fcd70daf":"code","7eaf7997":"code","af2dfff4":"code","c52f9d6e":"code","fff6a7b9":"code","29829739":"code","01ba714c":"code","20f9ea4d":"code","00d1ad3b":"code","0c53dd6e":"code","b73354e4":"code","c41350e9":"code","eb6c29c8":"code","1c9dccfc":"code","ad91f82e":"code","12941f68":"code","9b88ebf1":"code","8bc1bbb9":"code","b18acbe7":"code","02260261":"code","4c24489f":"code","780f470a":"code","7f67af1a":"code","b28a8626":"code","8333bdda":"code","74bd2bb3":"code","8a10b04b":"code","a8527443":"code","ed781ac6":"code","652fbd4c":"code","9fb9639a":"code","a9ae2472":"code","84be2dee":"code","7ee9817c":"code","896520bc":"code","4cec0fa8":"code","6e96dc6b":"code","2ec47cea":"code","3993c952":"code","22f28c65":"code","8ed4cc46":"code","470f5a4f":"code","478aace0":"code","aa8b9ecc":"code","084dd3f6":"code","754a7a85":"code","93a01bbf":"code","e939bfd5":"code","7765c30b":"code","46fcb46e":"code","a7146007":"code","a21e9546":"code","72f56941":"code","057b4ed6":"code","05de20d0":"code","f3681340":"code","15be35a1":"code","b9a699a4":"code","afee420c":"code","c92314c8":"code","4ef29ee3":"code","c69ca6aa":"code","e9f94868":"code","0d8b8f6c":"code","ffc364de":"code","bf793c70":"code","c2c7d803":"code","8430a3c4":"code","534aa579":"code","bc7d2589":"code","d9e8e3dd":"code","7be76090":"code","7ee73c33":"code","8036656b":"code","80878b90":"code","74f74062":"code","6604a12d":"code","2d437434":"code","76a4e0c5":"code","bfb6a295":"code","286ffaf1":"code","46359546":"code","22643c79":"code","9ec73454":"code","3e469920":"code","c303e0a7":"code","b44139d8":"code","3aabb294":"code","741ba078":"code","f674a96b":"code","dadf807a":"code","b55e9856":"code","cd541520":"code","20365274":"code","0a655da2":"code","1a609026":"code","028439c7":"code","c196920f":"code","9607855d":"code","f747f77e":"code","60dfb27d":"code","900e5747":"code","1679ccc2":"code","f661558a":"code","2114d1fd":"code","c74e2345":"code","01f8af87":"code","727f4703":"code","a9fba4dc":"code","50373cf8":"code","5219d447":"code","38ee82fd":"code","0f6e7c89":"code","0810d399":"code","f7be04c4":"code","1e820085":"code","fd7a6e08":"code","76bc08a5":"code","81817d1c":"code","3c521c10":"code","1646acba":"code","7fdc5629":"code","923b79f6":"code","f1d4c17f":"markdown","f3b0481f":"markdown","7e92b993":"markdown","2cf89351":"markdown","6c044614":"markdown","64477eb5":"markdown","67f49cd1":"markdown","7cd3336e":"markdown","7fd172a1":"markdown","26f49f2c":"markdown","d4cbea7e":"markdown","5f566b53":"markdown","103278e0":"markdown","61e098b4":"markdown","a82c2d71":"markdown","b51188b3":"markdown","549140b5":"markdown","83acc511":"markdown","4ba06d11":"markdown","d41eaa1f":"markdown","03c3c7d2":"markdown","8eff347b":"markdown","e13ddec8":"markdown","eeaf9dbd":"markdown","2da10fa2":"markdown","8e59e960":"markdown","e54f4621":"markdown","5b719f37":"markdown","eecaf7c9":"markdown","632a29d5":"markdown","8eb804ae":"markdown","a61d2868":"markdown","35fbe6a4":"markdown","92b14ac5":"markdown","b3c7dbfa":"markdown","0d81da83":"markdown","ee53f39b":"markdown","2bc56c9e":"markdown","0c19090e":"markdown","52129d90":"markdown","5c3cf827":"markdown","87799645":"markdown","186aa234":"markdown","846e72c4":"markdown","05faf5aa":"markdown","7dc256a5":"markdown","dcdecf17":"markdown","f42a4ecb":"markdown","25cc0a14":"markdown","78b9be27":"markdown","9062ad29":"markdown","9762d0fd":"markdown","992f64bd":"markdown","b33af879":"markdown","7a21694b":"markdown","64705321":"markdown","3cc213b0":"markdown","1bd9c510":"markdown","1915e108":"markdown","01fbdf21":"markdown","fa926434":"markdown","c3da8633":"markdown","ca2dd06d":"markdown","4591a471":"markdown","cf1f615d":"markdown","e651d302":"markdown","67e97918":"markdown","89883585":"markdown","8c1700b0":"markdown","84c2e323":"markdown","401c0ca4":"markdown","e1f6d86a":"markdown","62af643f":"markdown","489fc51f":"markdown","70d5c07a":"markdown","640eaee5":"markdown","c0124214":"markdown","e7ef9040":"markdown","eccd8745":"markdown","661c3b0d":"markdown","22faca92":"markdown","e28886e9":"markdown","6af8ed82":"markdown","a2222cd9":"markdown","c77a9089":"markdown","a3e8471f":"markdown","5e7bae64":"markdown","78898998":"markdown","e8dd9a84":"markdown","84afa902":"markdown","84903f08":"markdown","df249e99":"markdown","942013df":"markdown","28b0a5a1":"markdown","3455c89e":"markdown","9977432b":"markdown","868d6f74":"markdown","65c0b989":"markdown"},"source":{"772a5e41":"#To install xgboost library use - \n!pip install xgboost\n### IMPORT: ------------------------------------\nimport scipy.stats as stats \nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\n\nimport statsmodels.api as sm\n#--Sklearn library--\nfrom sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function\n\nfrom sklearn import metrics\n\nfrom sklearn.ensemble import BaggingClassifier,RandomForestClassifier,StackingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n\nfrom xgboost import XGBClassifier\nfrom sklearn import tree\nfrom sklearn.linear_model import LogisticRegression\n\n# Libtune to tune model, get different metric scores\n\nfrom sklearn.metrics import  classification_report, accuracy_score, precision_score, recall_score,f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,plot_confusion_matrix #to plot confusion matric\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_colwidth',400)\n# To supress numerical display in scientific notations\npd.set_option('display.float_format', lambda x: '%.5f' % x) \nwarnings.filterwarnings('ignore') # To supress warnings\n # set the background for the graphs\nplt.style.use('ggplot')\n# For pandas profiling\nfrom pandas_profiling import ProfileReport\nprint('Load Libraries-Done')\n#!pip install xlrd\n#!!pip install openpyxl","a320cd07":"#Reading the Excel file  used tourism.xlsx \ndata_path='..\/input\/tourpackageprediction\/tour_package.csv'\n\n#Loading the dataset - sheet_name parameter is used if there are tabs in the excel file\n#df=pd.read_excel(data_path,sheet_name='Tourism')\ndf=pd.read_csv(data_path)\n\ndf_tour=df.copy()\nprint(f'There are {df_tour.shape[0]} rows and {df_tour.shape[1]} columns') # fstring ","b84718d6":"df_tour.head()","1ca08774":"df_tour.tail()","4baecdcb":"#get the size of dataframe\nprint (\"Rows     : \" , df_tour.shape[0])  #get number of rows\/observations\nprint (\"Columns  : \" , df_tour.shape[1]) #get number of columns\nprint (\"#\"*40,\"\\n\",\"Features : \\n\\n\", df_tour.columns.tolist()) #get name of columns\/features\nprint (\"#\"*40,\"\\nMissing values :\\n\\n\", df_tour.isnull().sum().sort_values(ascending=False))\nprint( \"#\"*40,\"\\nPercent of missing :\\n\\n\", round(df_tour.isna().sum() \/ df_tour.isna().count() * 100, 2).sort_values(ascending=False)) # looking at columns with most Missing Values\n","d61878e0":"df_tour.info()","f798b8aa":"df_tour.describe().T","c0fe38bc":"df_tour.drop(['CustomerID'],axis=1,inplace=True)","7b844fad":"\ncat_cols = ['Designation','ProdTaken', 'OwnCar', 'Passport',\n            'CityTier','MaritalStatus',\n            'ProductPitched','Gender','Occupation','TypeofContact'\n            ]","6e7e1d9d":"for i in cat_cols:\n    print('Unique values in',i, 'are :')\n    print(df_tour[i].value_counts())\n    print('*'*50)","cad85725":"df_tour.Gender.value_counts()","8dc1d44f":"df_tour['Gender'] = df_tour['Gender'].apply(lambda x: 'Female' if x == 'Fe Male' else x)","fcd70daf":"df_tour.Gender.value_counts()","7eaf7997":"## Converting the data type of categorical features to 'category'\n\ncat_cols = ['Designation','ProdTaken', 'OwnCar', 'Passport',\n            'CityTier','MaritalStatus',\n            'ProductPitched','Gender','Occupation','TypeofContact'\n            ]\ndf_tour[cat_cols] = df_tour[cat_cols].astype('category')\ndf_tour.info()","af2dfff4":"df_tour.describe(include=['category']).T","c52f9d6e":"df_tour.Age.describe()","fff6a7b9":"df_tour['Agebin'] = pd.cut(df_tour['Age'], bins = [18,25, 31, 40, 50, 65], labels = ['18-25','26-30', '31-40', '41-50', '51-65'])","29829739":"df_tour.Agebin.value_counts()","01ba714c":"df_tour.MonthlyIncome.describe()","20f9ea4d":"df_tour['Incomebin'] = pd.cut(df_tour['MonthlyIncome'], bins = [0,15000,20000, 25000, 30000,35000,40000,45000,50000,100000], labels = ['<15000', '<20000', '<25000', '<30000','<35000','<40000','<45000','<50000','<100000'])","00d1ad3b":"df_tour.Incomebin.value_counts()","0c53dd6e":"def dist_box(data):\n # function plots a combined graph for univariate analysis of continous variable \n #to check spread, central tendency , dispersion and outliers  \n    Name=data.name.upper()\n    fig,(ax_box,ax_dis)  =plt.subplots(nrows=2,sharex=True,gridspec_kw = {\"height_ratios\": (.25, .75)},figsize=(8, 5))\n    mean=data.mean()\n    median=data.median()\n    mode=data.mode().tolist()[0]\n    sns.set_theme(style=\"white\")\n    fig.suptitle(\"SPREAD OF DATA FOR \"+ Name  , fontsize=18, fontweight='bold')\n    sns.boxplot(x=data,showmeans=True, orient='h',color=\"teal\",ax=ax_box)\n    ax_box.set(xlabel='')\n     # just trying to make visualisation better. This will set background to white\n    sns.despine(top=True,right=True,left=True) # to remove side line from graph\n    sns.distplot(data,kde=False,color='purple',ax=ax_dis)\n    ax_dis.axvline(mean, color='r', linestyle='--',linewidth=2)\n    ax_dis.axvline(median, color='g', linestyle='-',linewidth=2)\n    ax_dis.axvline(mode, color='y', linestyle='-',linewidth=2)\n    plt.legend({'Mean':mean,'Median':median,'Mode':mode})\n                    ","b73354e4":"#select all quantitative columns for checking the spread\nlist_col=  df_tour.select_dtypes(include='number').columns.to_list()\nfor i in range(len(list_col)):\n    dist_box(df_tour[list_col[i]])","c41350e9":"# Making a list of all categorical variables\n\nplt.figure(figsize=(14,20))\n\nsns.set_theme(style=\"white\") \ncols=['TypeofContact', 'CityTier', 'Occupation', 'Gender',\n        'NumberOfPersonVisiting', 'NumberOfFollowups', 'ProductPitched', \n        'PreferredPropertyStar', 'MaritalStatus', 'NumberOfTrips',\n        'Passport', 'PitchSatisfactionScore',\n        'OwnCar', 'NumberOfChildrenVisiting', \n        'Designation','Agebin','Incomebin']\nfor i, variable in enumerate(cols):\n                     plt.subplot(9,2,i+1)\n                     order = df_tour[variable].value_counts(ascending=False).index   \n                     #sns.set_palette(list_palette[i]) # to set the palette\n                     sns.set_palette('Set2')\n                     ax=sns.countplot(x=df_tour[variable], data=df_tour )\n                     sns.despine(top=True,right=True,left=True) # to remove side line from graph\n                     for p in ax.patches:\n                           percentage = '{:.1f}%'.format(100 * p.get_height()\/len(df_tour[variable]))\n                           x = p.get_x() + p.get_width() \/ 2 - 0.05\n                           y = p.get_y() + p.get_height()\n                           plt.annotate(percentage, (x, y),ha='center')\n                     plt.tight_layout()\n                     plt.title(cols[i].upper())\n                                     \n","eb6c29c8":"sns.set_palette(sns.color_palette(\"Set2\", 8))\nplt.figure(figsize=(15,10))\nsns.heatmap(df_tour.corr(),annot=True)\nplt.show()","1c9dccfc":"sns.set_palette(sns.color_palette(\"Set1\", 8))\nsns.pairplot(df_tour, hue=\"ProdTaken\",corner=True)\nplt.show()","ad91f82e":"### Function to plot distributions and Boxplots of customers\ndef plot(x,target='ProdTaken'):\n    fig,axs = plt.subplots(2,2,figsize=(12,10))\n    axs[0, 0].set_title(f'Distribution of {x} \\n of a customer who had Not taken Product',fontsize=12,fontweight='bold')\n    sns.distplot(df_tour[(df_tour[target] == 0)][x],ax=axs[0,0],color='teal')\n    axs[0, 1].set_title(f\"Distribution of {x}\\n of a customer who had taken Product\",fontsize=12,fontweight='bold')\n    sns.distplot(df_tour[(df_tour[target] == 1)][x],ax=axs[0,1],color='orange')\n    axs[1,0].set_title(f'Boxplot of {x} w.r.t Product taken',fontsize=12,fontweight='bold')\n    \n    line = plt.Line2D((.1,.9),(.5,.5), color='grey', linewidth=1.5,linestyle='--')\n    fig.add_artist(line)\n   \n    sns.boxplot(df_tour[target],df_tour[x],ax=axs[1,0],palette='gist_rainbow',showmeans=True)\n    axs[1,1].set_title(f'Boxplot of {x} w.r.t Product Taken - Without outliers',fontsize=12,fontweight='bold')\n    sns.boxplot(df_tour[target],df_tour[x],ax=axs[1,1],showfliers=False,palette='gist_rainbow',showmeans=True) #turning off outliers from boxplot\n    sns.despine(top=True,right=True,left=True) # to remove side line from graph\n    plt.tight_layout(pad=4)\n    plt.show()","12941f68":"#select all quantitative columns for checking the spread\n#list_col=  ['Age','DurationOfPitch','MonthlyIncome']\nlist_col=df_tour.select_dtypes(include='number').columns.to_list()\n#print(list_col)\n#plt.figure(figsize=(14,23))\nfor j in range(len(list_col)):\n   plot(list_col[j])\n   ","9b88ebf1":"sns.set_palette(sns.color_palette(\"nipy_spectral\", 8))\n\nsns.swarmplot(y='MonthlyIncome',x='Occupation',hue='ProdTaken',data=df_tour)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nplt.title('Monthly Income vs Occupation')","8bc1bbb9":"sns.set_palette(sns.color_palette(\"nipy_spectral\", 8))\nsns.scatterplot(y='MonthlyIncome',x='Age',hue='ProdTaken',data=df_tour)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nplt.title('Monthly Income vs Age')\n","b18acbe7":"df_tour.isnull().sum()","02260261":"df_tour['TypeofContact'].value_counts()","4c24489f":"df_tour['TypeofContact'].mode()\n\ndf_tour['TypeofContact']=df_tour['TypeofContact'].fillna('Self Enquiry')","780f470a":"#Verify if there are null values\ndf_tour[\"TypeofContact\"].isnull().sum()","7f67af1a":"df_tour.groupby([\"Gender\",'ProductPitched'])[\"NumberOfFollowups\"].median()\n\ndf_tour['NumberOfFollowups']=df_tour.groupby([\"Gender\",'ProductPitched'])[\"NumberOfFollowups\"].apply(lambda x:x.fillna(x.median()))","b28a8626":"#Look at few rows where values  is missing\ndf_tour[df_tour[\"PreferredPropertyStar\"].isnull()]","8333bdda":"df_tour['PreferredPropertyStar']=df_tour.groupby([\"Designation\"])[\"PreferredPropertyStar\"].apply(lambda x:x.fillna(x.median()))\n\ndf_tour[df_tour[\"PreferredPropertyStar\"].isnull()]","74bd2bb3":"df_tour.groupby(['ProductPitched','NumberOfFollowups'], as_index=False)[\"DurationOfPitch\"].median()\n\n# Impute missing Duration of pitch with median value\ndf_tour[\"DurationOfPitch\"] = df_tour.groupby([\"ProductPitched\",'NumberOfFollowups'])[\"DurationOfPitch\"].apply(\n    lambda x: x.fillna(x.median())\n)","8a10b04b":"df_tour[df_tour[\"DurationOfPitch\"].isnull()]\n\ndf_tour.isnull().sum()","a8527443":"df_tour.groupby(['MaritalStatus'])[\"NumberOfTrips\"].median()","ed781ac6":"# Impute missing NumberOfTrips of pitch with median value\ndf_tour[\"NumberOfTrips\"] = df_tour.groupby([\"MaritalStatus\"])[\"NumberOfTrips\"].apply(\n    lambda x: x.fillna(x.median())\n)","652fbd4c":"df_tour[df_tour[\"NumberOfChildrenVisiting\"].isnull()].head(10)","9fb9639a":"# Impute missing NumberOfChildrenVisited with 0\ndf_tour['NumberOfChildrenVisiting']=df_tour['NumberOfChildrenVisiting'].fillna(0)","a9ae2472":"df_tour[df_tour[\"Age\"].isnull()].head(10)","84be2dee":"df_tour.groupby([\"Designation\", \"Gender\",\"MaritalStatus\"])[\"Age\"].median()\n\n# Impute missing Age with median value\ndf_tour[\"Age\"] = df_tour.groupby([\"Designation\", \"Gender\",\"MaritalStatus\"])[\"Age\"].apply(\n    lambda x: x.fillna(x.median())\n)","7ee9817c":"df_tour.groupby([\"Occupation\",'Designation','Gender'])[\"MonthlyIncome\"].median()","896520bc":"df_tour[\"MonthlyIncome\"]=df_tour.groupby([\"Occupation\",'Designation','Gender'])[\"MonthlyIncome\"].apply(\n    lambda x: x.fillna(x.median())\n)","4cec0fa8":"df_tour.isnull().sum() #verify if all missing values have been treated","6e96dc6b":"df_tour.Age.describe()","2ec47cea":"df_tour['Agebin'] = pd.cut(df_tour['Age'], bins = [15,25, 31, 40, 50, 70], labels = ['15-25','26-30', '31-40', '41-50', '51-70'])","3993c952":"df_tour.Agebin.value_counts()","22f28c65":"df_tour.MonthlyIncome.describe()","8ed4cc46":"df_tour['Incomebin'] = pd.cut(df_tour['MonthlyIncome'], bins = [0,15000,20000, 25000, 30000,35000,40000,45000,50000,100000], labels = ['<15000', '<20000', '<25000', '<30000','<35000','<40000','<45000','<50000','<100000'])","470f5a4f":"df_tour.Incomebin.value_counts()","478aace0":"df_tour.isnull().sum()","aa8b9ecc":"cust_prof=df_tour[df_tour['ProdTaken']==1]","084dd3f6":"#plt.figure(figsize= [12,12])\n\nsns.countplot(x=\"ProductPitched\", data=cust_prof)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","754a7a85":"sns.countplot(x='Agebin',hue='ProductPitched',data=cust_prof).set_title('Agebin Product wise')\n#sns.catplot(x='Agebin',hue='ProductPitched',col='ProdTaken',data=df_tour,kind='count')\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\n","93a01bbf":"sns.countplot(x='Incomebin',hue='ProductPitched',data=cust_prof).set_title('Incomebin Product Pitched')\n#sns.catplot(x='Incomebin',hue='ProductPitched',col='ProdTaken',data=df_tour,kind='count',aspect=1.5)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\n","e939bfd5":"sns.barplot(y='Age',x='ProductPitched',data=cust_prof).set_title('Age vs Product Pitched')\n#sns.catplot(y='Age',x='ProductPitched',col='ProdTaken',data=df_tour,kind='bar',aspect=1.5)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\n","7765c30b":"sns.countplot(x=\"ProductPitched\", data=cust_prof,  hue=\"Occupation\")\n#sns.catplot(hue='Occupation',x='ProductPitched',col='ProdTaken',data=df_tour,kind='count',aspect=1.5)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","46fcb46e":"#sns.catplot(hue='Gender',x='ProductPitched',col='ProdTaken',data=df_tour,kind='count',aspect=1.5)\nsns.countplot(x=\"ProductPitched\", data=cust_prof,  hue=\"Gender\")\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","a7146007":"sns.countplot(x=\"ProductPitched\", data=cust_prof,  hue=\"MaritalStatus\")\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","a21e9546":"sns.barplot(y='MonthlyIncome',x='ProductPitched',data=cust_prof).set_title('Monthly Income vs Product Pitched')\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","72f56941":"sns.barplot(x='Designation',y='MonthlyIncome',data=cust_prof,hue='ProductPitched').set_title('Designation vs Income')\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nplt.legend(bbox_to_anchor=(1, 1))","057b4ed6":"sns.countplot(x=\"ProductPitched\", data=cust_prof,  hue=\"PreferredPropertyStar\")\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","05de20d0":"sns.countplot(x=\"ProductPitched\", data=cust_prof,  hue=\"OwnCar\")\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","f3681340":"sns.countplot(x=\"ProductPitched\", data=cust_prof,  hue=\"CityTier\")\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","15be35a1":"cust_prof.groupby(['ProductPitched']).agg({'MonthlyIncome':{'mean','min','max'},'Age':{'mean','min','max'}})","b9a699a4":"cust_prof[cust_prof['ProductPitched']=='Basic'].describe(include='all').T","afee420c":"cust_prof[cust_prof['ProductPitched']=='Deluxe'].describe(include='all').T","c92314c8":"cust_prof[cust_prof['ProductPitched']=='King'].describe(include='all').T","4ef29ee3":"cust_prof[cust_prof['ProductPitched']=='Standard'].describe(include='all').T","c69ca6aa":"cust_prof[cust_prof['ProductPitched']=='Super Deluxe'].describe(include='all').T","e9f94868":"df_tour.info()","0d8b8f6c":"## Function to plot stacked bar chart\ndef stacked_plot(x):\n    sns.set_palette(sns.color_palette(\"nipy_spectral\", 8))\n    tab1 = pd.crosstab(x,df_tour['ProdTaken'],margins=True)\n    print(tab1)\n    print('-'*120)\n    tab = pd.crosstab(x,df_tour['ProdTaken'],normalize='index')\n    tab.plot(kind='bar',stacked=True,figsize=(7,4))\n    plt.xticks(rotation=360)\n    labels=[\"No\",\"Yes\"]\n    plt.legend(loc='lower left', frameon=False,)\n    plt.legend(loc=\"upper left\", labels=labels,title=\"Product Taken\",bbox_to_anchor=(1,1))\n    sns.despine(top=True,right=True,left=True) # to remove side line from graph\n    #plt.legend(labels)\n    plt.show()","ffc364de":"list_columns=['TypeofContact','CityTier','Occupation','Gender','NumberOfPersonVisiting',\n              'ProductPitched','PreferredPropertyStar','NumberOfTrips','MaritalStatus','Passport','OwnCar','Designation','Agebin','Incomebin','PitchSatisfactionScore','NumberOfFollowups']\nfor i, variable in enumerate(list_columns):\n       stacked_plot(df_tour[variable])","bf793c70":"Q1 = df_tour.quantile(0.25)             #To find the 25th percentile and 75th percentile.\nQ3 = df_tour.quantile(0.75)\n\nIQR = Q3 - Q1                           #Inter Quantile Range (75th perentile - 25th percentile)\n\nlower=Q1-1.5*IQR                        #Finding lower and upper bounds for all values. All values outside these bounds are outliers\nupper=Q3+1.5*IQR","c2c7d803":"((df_tour.select_dtypes(include=['float64','int64'])<lower) | (df_tour.select_dtypes(include=['float64','int64'])>upper)).sum()\/len(df_tour)*100","8430a3c4":"numeric_columns = ['MonthlyIncome','NumberOfTrips']\n# outlier detection using boxplot\nplt.figure(figsize=(20,30))\n\nfor i, variable in enumerate(numeric_columns):\n                     plt.subplot(4,4,i+1)\n                     plt.boxplot(df_tour[variable],whis=1.5)\n                     plt.tight_layout()\n                     plt.title(variable)\n\nplt.show()","534aa579":"# Check MonthlyIncome extreme values\ndf_tour.sort_values(by=[\"MonthlyIncome\"],ascending = False).head(5)","bc7d2589":"# Check NumberOfTrips extreme values\ndf_tour.sort_values(by=[\"NumberOfTrips\"],ascending = False).head(5)","d9e8e3dd":"df_tour[(df_tour.MonthlyIncome>40000) | (df_tour.MonthlyIncome<12000)]","7be76090":"#Dropping observaions with duration of pitch greater than 40. There are just 2 such observations\ndf_tour.drop(index=df_tour[df_tour.DurationOfPitch>37].index,inplace=True)\n\n#Dropping observation with monthly income less than 12000 or greater than 40000. There are just 4 such observations\ndf_tour.drop(index=df_tour[(df_tour.MonthlyIncome>40000) | (df_tour.MonthlyIncome<12000)].index,inplace=True)\n\n#Dropping observations with number of trips greater than 8. There are just 4 such observations\ndf_tour.drop(index=df_tour[df_tour.NumberOfTrips>10].index,inplace=True)","7ee73c33":"df_tour.info()","8036656b":"# Separating target column\nX = df_tour.drop(['ProdTaken','PitchSatisfactionScore','ProductPitched','NumberOfFollowups','DurationOfPitch','Agebin','Incomebin'],axis=1)\nX = pd.get_dummies(X,drop_first=True)\ny = df_tour['ProdTaken']","80878b90":"# Splitting the data into train and test sets in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1,stratify=y)\nX_train.shape, X_test.shape","74f74062":"def make_confusion_matrix(y_actual,y_predict,title):\n    '''Plot confusion matrix'''\n    fig, ax = plt.subplots(1, 1)\n    \n    cm = confusion_matrix(y_actual, y_predict, labels=[0,1])\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=[\"No\",\"Yes\"])\n    disp.plot(cmap='Greens',colorbar=True,ax=ax)\n    \n    ax.set_title(title)\n    plt.tick_params(axis=u'both', which=u'both',length=0)\n    plt.grid(b=None,axis='both',which='both',visible=False)\n    plt.show()","6604a12d":"def get_metrics_score(model,X_train_df,X_test_df,y_train_pass,y_test_pass,flag=True):\n    '''\n    Function to calculate different metric scores of the model - Accuracy, Recall, Precision, and F1 score\n    model: classifier to predict values of X\n    train, test: Independent features\n    train_y,test_y: Dependent variable\n    threshold: thresold for classifiying the observation as 1\n    flag: If the flag is set to True then only the print statements showing different will be displayed. The default value is set to True.\n    roc: If the roc is set to True then only roc score will be displayed. The default value is set to False.\n    '''\n    # defining an empty list to store train and test results\n    score_list=[] \n    pred_train = model.predict(X_train_df)\n    pred_test = model.predict(X_test_df)\n    pred_train = np.round(pred_train)\n    pred_test = np.round(pred_test)\n    train_acc = accuracy_score(y_train_pass,pred_train)\n    test_acc = accuracy_score(y_test_pass,pred_test)\n    train_recall = recall_score(y_train_pass,pred_train)\n    test_recall = recall_score(y_test_pass,pred_test)\n    train_precision = precision_score(y_train_pass,pred_train)\n    test_precision = precision_score(y_test_pass,pred_test)\n    train_f1 = f1_score(y_train_pass,pred_train)\n    test_f1 = f1_score(y_test_pass,pred_test)\n    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1))\n    if flag == True: \n          metric_names = ['Train_Accuracy', 'Test_Accuracy', 'Train_Recall', 'Test_Recall','Train_Precision',\n                          'Test_Precision', 'Train_F1-Score', 'Test_F1-Score']\n          cols = ['Metric', 'Score']\n          records = [(name, score) for name, score in zip(metric_names, score_list)]\n          display(pd.DataFrame.from_records(records, columns=cols, index='Metric').T)\n          make_confusion_matrix(y_train_pass,pred_train,\"Confusion Matrix for Train\")     \n          make_confusion_matrix(y_test_pass,pred_test,\"Confusion Matrix for Test\") \n    return score_list # returning the list with train and test scores","2d437434":"# # defining empty lists to add train and test results \nacc_train = []\nacc_test = []\nrecall_train = []\nrecall_test = []\nprecision_train = []\nprecision_test = []\nf1_train = []\nf1_test = []\n\ndef add_score_model(score):\n     '''add score of modelto list'''\n     acc_train.append(score[0])\n     acc_test.append(score[1])\n     recall_train.append(score[2])\n     recall_test.append(score[3])\n     precision_train.append(score[4])\n     precision_test.append(score[5])\n     f1_train.append(score[6])\n     f1_test.append(score[7])","76a4e0c5":"dtree=DecisionTreeClassifier(random_state=1, class_weight={0:0.20, 1:0.80})\ndtree.fit(X_train,y_train)","bfb6a295":"dtree_score=get_metrics_score(dtree,X_train,X_test,y_train,y_test)\nadd_score_model(dtree_score)","286ffaf1":"#Fitting the model\nbagging_classifier = BaggingClassifier(random_state=1, verbose=1)\nbagging_classifier.fit(X_train,y_train)","46359546":"bagging_score=get_metrics_score(bagging_classifier,X_train,X_test,y_train,y_test)\nadd_score_model(bagging_score)","22643c79":"rf_estimator = RandomForestClassifier(random_state=1)\nrf_estimator.fit(X_train,y_train)","9ec73454":"score_list_rf=get_metrics_score(rf_estimator,X_train,X_test,y_train,y_test)\nadd_score_model(score_list_rf)","3e469920":"comparison_frame = pd.DataFrame({'Model':['Decision Tree',\n                                          'Bagging Classifier',\n                                          'Random Forest',\n                                          ], \n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test,\n                                          'Train_F1':f1_train,\n                                          'Test_F1':f1_test  }) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_Recall',ascending=False)","c303e0a7":"#Choose the type of classifier. \ndtree_tuned = DecisionTreeClassifier(class_weight={0:0.20,1:0.80},random_state=1)\n\n# Grid of parameters to choose from\nparameters = {'max_depth': [1,4,7,15], \n              'min_samples_leaf': [2,3,5],\n              'max_leaf_nodes' : [ 5,7,10,15]\n              \n             }\n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(dtree_tuned, parameters, scoring=scorer,n_jobs=-1)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\ndtree_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \ndtree_tuned.fit(X_train, y_train)","b44139d8":"score_tune_dt=get_metrics_score(dtree_tuned,X_train,X_test,y_train,y_test)\nadd_score_model(score_tune_dt) # add score to dataframe","3aabb294":"plt.figure(figsize=(15,10))\nfeature_names = X_train.columns\nout = tree.plot_tree(dtree_tuned,feature_names=feature_names,filled=True,fontsize=9,class_names=['1','0'])\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()","741ba078":"feature_names = X_train.columns\nimportances = dtree_tuned.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(dtree_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","f674a96b":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","dadf807a":"# Choose the type of classifier. \nrf_tuned = RandomForestClassifier(class_weight={0:0.20,1:0.80},random_state=1)\n\nparameters = { \"max_depth\":[5,9,15],\n               \"n_estimators\": [150,200,250,500],\n               \"min_samples_leaf\": np.arange(5, 10),\n                \"max_features\": ['auto'],\n                \"max_samples\": np.arange(0.3,0.5, 0.7)\n              }\n# parameters = {\"n_estimators\": [50,80,150], \n#               \"max_depth\": [1,2,3], \n#               \"min_samples_split\": [3,4,6,7],\"max_features\": ['auto'],\n#              }\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer,cv=5,n_jobs=-1)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nrf_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nrf_tuned.fit(X_train, y_train)","b55e9856":"score_tune_rt=get_metrics_score(rf_tuned,X_train,X_test,y_train,y_test)\nadd_score_model(score_tune_rt) ","cd541520":"feature_names = X_train.columns\nimportances = rf_tuned.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(rf_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","20365274":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","0a655da2":"# Choose the type of classifier. \n\nbagging_estimator_tuned = BaggingClassifier(DecisionTreeClassifier(class_weight={0:0.20,1:0.80},random_state=1),random_state=1)\n\n# Grid of parameters to choose from\nparameters = {'max_samples': [0.7,0.8,0.9,1], \n              'max_features': [0.7,0.8,0.9,1],\n              'n_estimators' : [10,20,30,40,50],\n             }\n# Type of scoring used to compare parameter combinations\nacc_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=acc_scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nbagging_estimator_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nbagging_estimator_tuned.fit(X_train, y_train)","1a609026":"bagging_tuned=get_metrics_score(bagging_estimator_tuned,X_train,X_test,y_train,y_test)\nadd_score_model(bagging_tuned)","028439c7":"comparison_frame = pd.DataFrame({'Model':['Decision Tree',\n                                          'Bagging Classifier',\n                                          'Random Forest',\n                                          'Tuned Decision Tree',\n                                          'Tuned Random Forest',\n                                          'Tuned Bagging Classifier'], \n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test,\n                                          'Train_F1':f1_train,\n                                          'Test_F1':f1_test  }) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_Recall',ascending=False)","c196920f":"adaboost = AdaBoostClassifier(random_state=1)\nadaboost.fit(X_train,y_train)\n","9607855d":"adaboost_score=get_metrics_score(adaboost,X_train,X_test,y_train,y_test)\nadd_score_model(adaboost_score)","f747f77e":"gbc = GradientBoostingClassifier(random_state=1)\ngbc.fit(X_train,y_train)","60dfb27d":"gbc_score=get_metrics_score(gbc,X_train,X_test,y_train,y_test)\nadd_score_model(gbc_score)","900e5747":"xgb = XGBClassifier(random_state=1,eval_metric='logloss')\nxgb.fit(X_train,y_train)","1679ccc2":"xgb_score=get_metrics_score(xgb,X_train,X_test,y_train,y_test)\nadd_score_model(xgb_score)","f661558a":"feature_names = X_train.columns\nimportances = xgb.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(xgb.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","2114d1fd":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","c74e2345":"comparison_frame = pd.DataFrame({'Model':['Decision Tree',\n                                          'Random Forest',\n                                          'Bagging Classifier',\n                                          'Tuned Decision Tree',\n                                          'Tuned Random Forest',\n                                          'Tuned Bagging Classifier',\n                                          'AdaBoost',\n                                          'Gradient Boost',\n                                          'XGboost',\n                                          ], \n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test,\n                                           'Train_F1':f1_train,\n                                          'Test_F1':f1_test  }) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_Recall',ascending=False)","01f8af87":"# Choose the type of classifier. \nabc_tuned = AdaBoostClassifier(random_state=1)\n\n# Grid of parameters to choose from\n## add from article\nparameters = {\n    #Let's try different max_depth for base_estimator\n    \"base_estimator\":[DecisionTreeClassifier(max_depth=1),DecisionTreeClassifier(max_depth=2)],\n    \"n_estimators\": np.arange(10,50,100),\n    \"learning_rate\":np.arange(0.1,1,0.1)\n}\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(abc_tuned, parameters, scoring=acc_scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nabc_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nabc_tuned.fit(X_train, y_train)","727f4703":"abc_tuned_score=get_metrics_score(abc_tuned,X_train,X_test,y_train,y_test)\nadd_score_model(abc_tuned_score)","a9fba4dc":"feature_names = X_train.columns\nimportances = abc_tuned.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(abc_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","50373cf8":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","5219d447":"# Choose the type of classifier. \ngbc_tuned = GradientBoostingClassifier(init=AdaBoostClassifier(random_state=1),random_state=1)\n\n# Grid of parameters to choose from\n## add from article\nparameters = {\n    \"n_estimators\": [100,150,200,250],\n    \"subsample\":[0.8,0.9,1],\n    \"max_features\":[0.7,0.8,0.9,1]\n}\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\ngbc_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\ngbc_tuned.fit(X_train, y_train)","38ee82fd":"gbc_tuned_score=get_metrics_score(gbc_tuned,X_train,X_test,y_train,y_test)\nadd_score_model(gbc_tuned_score)","0f6e7c89":"feature_names = X_train.columns\nimportances = gbc_tuned.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(gbc_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","0810d399":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","f7be04c4":"# Choose the type of classifier. \nxgb_tuned = XGBClassifier(random_state=1,eval_metric='logloss')\n\n# Grid of parameters to choose from\n## add from\n\nparameters = {\n    \"n_estimators\": np.arange(10,100,20),\n    \"scale_pos_weight\":[0,5],\n    \"colsample_bylevel\":[0.5,1],\n    \"learning_rate\":[0.001,0.01,0.1,0.5]\n}\n# Type of scoring used to compare parameter combinations\nacc_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(xgb_tuned, parameters,scoring=acc_scorer,cv=5,n_jobs=-1)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nxgb_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nxgb_tuned.fit(X_train, y_train)","1e820085":"xgb_tuned_score=get_metrics_score(xgb_tuned,X_train,X_test,y_train,y_test)\nadd_score_model(xgb_tuned_score)\n","fd7a6e08":"feature_names = X_train.columns\nimportances = xgb_tuned.feature_importances_\nindices = np.argsort(importances)\nprint (pd.DataFrame(xgb_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","76bc08a5":"plt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","81817d1c":"estimators=[('Decision Tree', dtree_tuned),('Random Forest', rf_tuned),('AdaBoost',abc_tuned),\n           ('Gradient Boosting', gbc_tuned)]\nfinal_estimator=XGBClassifier(random_state=1,eval_metric='logloss')","3c521c10":"stacking_estimator=StackingClassifier(estimators=estimators, final_estimator=final_estimator,cv=5,n_jobs=-1)\nstacking_estimator.fit(X_train,y_train)","1646acba":"stacking_estimator=get_metrics_score(stacking_estimator,X_train,X_test,y_train,y_test)","7fdc5629":"add_score_model(stacking_estimator)","923b79f6":"comparison_frame = pd.DataFrame({'Model':['Decision Tree',\n                                          'Random Forest',\n                                          'Bagging Classifier',\n                                          'Tuned Decision Tree',\n                                          'Tuned Random Forest',\n                                          'Tuned Bagging Classifier',\n                                          'AdaBoost',\n                                          'Gradient Boost',\n                                          'XGboost',\n                                          'Tuned AdaBoost',\n                                          'Tuned Gradient Boost',\n                                          'Tuned XGboost','Stacking'\n                                          ], \n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test,\n                                           'Train_F1':f1_train,\n                                          'Test_F1':f1_test  }) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_Recall',ascending=False)","f1d4c17f":"#### Missing value Treatment Age","f3b0481f":"**Observation**\n\nBagging is  still overfitting the training data , Recall score has decreased for test data","7e92b993":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\">Insights based on EDA<\/h2>\n\n- Mostly customer visting with 2,3,4 travellers purchased the product\n- Customers who were pitched basic package mostly  brought the product , followed by standard. Reason might be its less expensive.\n- Mostly Customers who had passport bought the product.\n- Most of the customers who bought the product were Executive and Senior manager\n- Customers who were followed up 6 times had purchases the product\n* Company invited customers mostly purchased packages and preferred 5 star rated properties and were mostly from city tier   2,3.\n* Customer from 18-25 age purchased the product taken, followed by  26-30.\n* Customers who learn from 15000-20000 purchased product followed by customers in income range 20000-25000.\n* FreeLancers(need more data to conclude) and Large Business owners have higher chance of purchasing the travel package\n* Single and unmarried people has higher chance of purchasing the travel package.\n* Having a passport increased chances of purchasing the package.\n* Customers who took 7\/8 trips had higher chances of purchaing the packages\n* Gender,number of children visiting, having a car seemed to be insignificant.\n* Customers mostly purchased the travel package when marketing team did high number of followups,higher duration of pitch , and pitched Basic product.\n","2cf89351":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\"> Model Building <\/h2> ","6c044614":"#### Missing value Treatment NumberOfChildrenVisiting","64477eb5":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\">Stacking Classifier<\/h2>","67f49cd1":"Highest ocurring value is `Self Inquiry`. We will impute the missing value for TypeofContact using the mode(highest occuring value) of the feature.","7cd3336e":" Imputing  age using designation,gender,Martial status would give more  granularity","7fd172a1":"### Bivariate & Multivariate Analysis","26f49f2c":"<h3 style = \"font-family:Impact;color:black;font-weight:bold\"> Data Preprocessing<\/h3>  ","d4cbea7e":"For more granularity imputing number of trips using martial status","5f566b53":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\">Table of Contents<\/h2>\n\n- [Context](#Context) \n- [Data Dictionary](#Data-Dictionary)\n- [Problem](#Problem)\n- [Libraries](#Libraries)\n- [Read and Understand Data](#Read-and-Understand-data)\n- [Data Preprocessing](#Data-Preprocessing)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis) \n    - [Univariate Analysis](#Univariate-Analysis) \n    - [Bivariate and Multivariate Analysis](#Bivariate-&-Multivariate-Analysis)\n- [Missing value Detection and Treatment](#Missing-value-Detection-and-Treatment)   \n- [Insights based on EDA](#Insights-based-on-EDA)\n- [Outlier Detection](#Outlier-Detection)\n- [Model Building Bagging](#Model-Building-Bagging)\n   - [Decision Tree](#Decsion-Tree)\n   - [Bagging classifier](#Bagging-Classifier)\n   - [Random Forest](#Random-Forest)\n- [Model Building Boosting](#Model-Building-Boosting) \n   - [Adaboost](#Adaboost)\n   - [Gradient Boost](#Gradient-Boost)\n   - [XGBoost](#XgBoost)\n   - [Stacking Classifier](#Stacking-Classifier) \n- [Conclusion](#Conclusion) \n- [Business Recommendations & Insights](#Business-Recommendations-&-Insights)\n- [Misclassfication Analysis](#Misclassification-Analysis) \n<\/p>\n\n","103278e0":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\"> Exploratory Data Analysis<\/h2>","61e098b4":"**Observations**\n- ~38 % customers are Executive, followed by 35% are managers.\n- ~18% customers accepted product offered last time.\n- ~ 62 % customer own car.\n- ~29 % customers has a passport.\n- ~ 65 % customers are from Tier 1 cities.\n- ~ 61 % customers prefer 3 star property. \n- ~ 48 % customers are married\n- Basic package was pitched to ~ 38 % of customers and 35 % were pitched Deluxe package.\n- 60 % customers are male.\n- Occupation of ~ 49 % customer is  salaried.\n- 70.5 % customer  self enquiried for the packages.\n- Most of the customers travelled along with 3 people.\n- Most of the customers take 2 trips per year but as seen previously there are some extreme values like 22.\n- Most of the customers travelled with  only one child.\n- Most of the customers were followed up 4 times.\n- Majority  of customer monthly income is in 20000-25000 range.Most customer fall in 15000-30000 monthly income range.\n- ~35% are in 31-40 Age group.  Most customer are in 26-50 age range","a82c2d71":"**Observation**\n\nDecision tree is overfitting the training data as there is lot of disparity between test and train.Recall score is also not that high\n","b51188b3":"## Tuning Decision Tree","549140b5":"**Observations**\n- There are lot of missing values.\n- 5% values are missing in DurationOfPitch, that may be cause customer was never pitched, need to analyze further\n- 4.77% values are missig from MonthlyIncome.\n- Age has 4.62 % missing values.\n- NumberOfChildrenVisiting,NumberOfFollowups,NumberOfTrips,PreferredPropertyStar,TypeofContact has less than 3% missing values \n- ProdTaken is the Target Variable\n- Designation,Martial status,Product Pitched,Gender,Owncar,Passport,CityTier,PreferredPropertyStar, Occupation,Type of Contact are categorical variables while others are numerical value.","83acc511":"### Tuned Gradient Boosting Classifier","4ba06d11":"We have been able to build a predictive model:\n\na) that the company can deploy to identify customers who will be interested in purchasing the Travel package.\n\nb) that the company  can use to find the key factors that will have an impact on a customer taking a product or not.\n\n- Most important features that have an impact on Product taken: Desgination, Passport,TierCity,Martialstatus,occupation\n\n- Customers with Designation as Executive should be the target customers for the company .Customers who have passport and are from tier 3 city and are single or unmarried, have large business such customers have higher chances of taking new package.\n- Customers monthly income in range of 15000- 25000, and age range 15-30, prefer 5 star properties also have higher chances of taking new package based on EDA.\n- Based on EDA ,Factors from customer interaction data which can help in increasing the chances of the customer buying the travel package  are:\n  - Having a higher duration of pitch by salesman to the customer.\n  - Getting a PitchSatisfactionScore of 3 or 5.\n  - Having multiple follow ups with the customers.\n- Company should help and promote customers to get a passport , as we see having a passport increases the chances of customer accepting a package.\n- Mostly Single customers are accpeting a package , reason may be married couples might has kids , provding a property , with child care services can get married couples to accept the product.","d41eaa1f":"**Observations**","03c3c7d2":"## Gradient Boost","8eff347b":"**Observations**\n- Monthlyincome and Age has weak correlation\n- Number of children visiting and number of person vsiting has correlation as expected.\n- Numberof follow up and number of person vsiting has very weak correlation.\n-","e13ddec8":"## XGBoost ","eeaf9dbd":"### Univariate Analysis","2da10fa2":"Assuming  children visited is missing because no children accompanied these customers so we will fill the missing values with 0","8e59e960":"## Comparing all models","e54f4621":"#### Age","5b719f37":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\"> Model Performance Evaluation and Improvement-Bagging<\/h2>","eecaf7c9":" We'll fit Decision Tree and BaggingClassifier and Randomforest  models on the train data and observe their performance. \n","632a29d5":"Finally all missing values have been treated. ","8eb804ae":"- `Single` and `unmarried` status are not same category . Unmarried here means customers having partners.\n- `Female` and `Fe male` are same category , Data needs to be fixed\n- 3 star property is prefered by customers\n- Most of the customers are from Tier1 cities","a61d2868":"**Now, let's build a stacking model with the tuned models - decision tree, random forest,Adaboosting and gradient boosting, then use XGBoost to get the final prediction.**","35fbe6a4":"**Observations**\n- Age is normally distrubted  and doesn't have any outliers\n- Duration of pitch is Right skewed with some outliers  greater than 120 .Need to see if this are to be treated.\n- Number of visitors is usually 3 with a outlier of 5.\n- Average number of folows up are 4 with extreme of 6 \n- Number of trips is right skewed with some outliers where trips are greater than 17.\n- Monthly income is Right skewed. and has some outlier as higher end.\n","92b14ac5":"\nMost Important features are Monthly income,Age,passport Desiginative executive, Number of trips,city tier 3.","b3c7dbfa":"**Observation**\n- Customers who purchased the product are mostly in age range of 28 -35\n- Duration of pitch suprisingly for customer who purchased product where 10- 40 min. There are some outliers in duration of pitch for customers who didn't take the product\n- Number of trips has some outliers like 17 -20.\n- Monthly income has extreme outliers.Customers who purchased product are in earning on average 18000-23000 monthly\n","0d81da83":"Let see how can we impute Duration of pitch.In my opinion an important factor for how long sale person take times to market his sales pitch depends on Product which sale person is proposing , number of followup will also decide duration of pitch. Let verify this.","ee53f39b":"### Tuned XGBoost Classifier","2bc56c9e":" \n-  Looking at feature importance\n    Designation , Passport, Tier city , martial status,occupation are most important features.Income can also be looked into, few of other models have given higher importance to Income as well.\n- Gender,number of children visiting, having a car seemed to be insignificant.\n    ","0c19090e":"### Tuning Random Forest","52129d90":"**Removing these outliers form duration of pitch, monthly income, and number of trips.**","5c3cf827":"#### Understand the  dataset.","87799645":"#### Age\n\nAge can be a vital factor in tourism, converting ages to bin to explore if there is any pattern","186aa234":"## Conclusion","846e72c4":"[Top](#Table-of-Contents)","05faf5aa":"### Ensemble Methods\n\n- Ensemble is a group of estimators that are used together for prediction in classification as well as regression problems.\n- Ensemble is the belief that a committee of experts working together are more likely to be accurate than individual experts.\n- Ensemble method uses n number of base estimators and combines their output to give a final prediction giving better performance and robustness than a single estimator.\n- For ensemble to be effective we have to ensure\n    - The base estimators are as different from each other as possible.\n    - The errors made by each estimator should be different from each other (independent errors)\n\nBagging and Boosting are two ensemble methods.\n\n\nBagging |Boosting |\n-----|-----|\nAll the weak learners are built in parallel i.e. independent of each other|Successive weak learners to improve the accuracy from the prior learners|\nEach weak learner has equal weight in the final prediction.|More weight to those weak learners with better performance|\nSamples are drawn from the original dataset with replacement to train each individual weak learner|Subsequent samples have more of those observations which had relatively higher errors in previous weak learners|\nCan help  reduce variance of the mode|Can help  reduce bias of the mode|\nExample: Bagging Classifier, Random Forest|Example: AdaBoost, Gradient Boosting Classifier\n","7dc256a5":"**Observations**\n- Minimum age  of customer is `18` and Maximum age is `61` with mean of `37`.\n- Mean Duration of pitch  is `15` mins to max of `127` mins.\n- Mean Number of trips is `3` with maximum of `22`.This needs to be verified\n- `Average monthly income` of customer is `23619` with maximum of `98678`. This needs to be verified\n- `920` customer had taken package last time.\n","dcdecf17":"#### Droping customer id","f42a4ecb":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\">Data Dictionary<\/h2>\n\nCustomer details:\n\n- CustomerID: Unique customer ID\n- ProdTaken: Whether the customer has purchased a package or not (0: No, 1: Yes)\n- Age: Age of customer\n- TypeofContact: How customer was contacted (Company Invited or Self Inquiry)\n- CityTier: City tier depends on the development of a city, population, facilities, and living standards. The categories are ordered i.e. Tier 1 > Tier 2 > Tier 3\n- Occupation: Occupation of customer\n- Gender: Gender of customer\n- NumberOfPersonVisiting: Total number of persons planning to take the trip with the customer\n- PreferredPropertyStar: Preferred hotel property rating by customer\n- MaritalStatus: Marital status of customer\n- NumberOfTrips: Average number of trips in a year by customer\n- Passport: The customer has a passport or not (0: No, 1: Yes)\n- OwnCar: Whether the customers own a car or not (0: No, 1: Yes)\n- NumberOfChildrenVisiting: Total number of children with age less than 5 planning to take the trip with the customer\n- Designation: Designation of the customer in the current organization\n- MonthlyIncome: Gross monthly income of the customer\n\nCustomer interaction data: \n\n- PitchSatisfactionScore: Sales pitch satisfaction score\n- ProductPitched: Product pitched by the salesperson\n- NumberOfFollowups: Total number of follow-ups has been done by the salesperson after the sales pitch\n- DurationOfPitch: Duration of the pitch by a salesperson to the customer","25cc0a14":"#### Missing value Treatment PreferredPropertyStar","78b9be27":"#### View the first and last 5 rows of the dataset.","9062ad29":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\">Model Building Boosting<\/h2> ","9762d0fd":"\"Visit with us\". company wants to enable and establish a viable business model to expand the customer base.\nOne of the ways to expand the customer base is to introduce a new offering of packages.\nCurrently, there are 5 types of packages the company is offering - Basic, Standard, Deluxe, Super Deluxe, King. \nLooking at the data of the last year, we observed that 18% of the customers purchased the packages.\nHowever, the marketing cost was quite high because customers were contacted at random without looking\nat the available information.The company is now planning to launch a new product i.e. Wellness Tourism Package. \nWellness Tourism is defined as Travel that allows the traveler to maintain, enhance or kick-start a healthy lifestyle,\nand support or increase one's sense of well-being.However, this time company wants to harness the available data \nof existing and potential customers to make the marketing expenditure more efficient.\n\nWe need to analyze the customers' data and information to provide recommendations to the Policy Maker and Marketing Team \nand also build a model to predict the potential customer who is going to purchase the newly introduced travel package.\n\n**Motivation of the book is to understand how to implement Bagging, Boosting and  How to tune Model using Hyperparmeters**","992f64bd":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\">Missing value Detection and Treatment<\/h2>","b33af879":"Age can be a vital factor in tourism, converting ages to bin to explore if there is any pattern","7a21694b":"\nImportant features are Passport,Monthly Income,Age, designation executive.","64705321":"<h3 style = \"font-family:Impact;color:black;font-weight:bold\">Libraries<\/h3>  ","3cc213b0":"### Tuned AdaBoost Classifier","1bd9c510":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\">Model Performance Evaluation and Improvement-Boosting<\/h2>","1915e108":" We can see that there are just four observations with number of trips 19 or greater","01fbdf21":"#### Missing value Treatment Type of contact","fa926434":"### Income\nTo understand customers segments derving new columns which will help us identify if customer in different income range","c3da8633":" Since customer interaction data will not be available for potential customers and new customers,outliers in  Number of trips and Monthly Income  needs to be handled","ca2dd06d":"[Top](#Table-of-Contents)","4591a471":"#### Missing value Treatment Duration of pitch","cf1f615d":"[Top](#Table-of-Contents)","e651d302":"## Random Forest ","67e97918":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\">Business Recommendations & Insights<\/h2>","89883585":"Let see how can impute PreferredPropertyStar using  designation  of customer for more granularity","8c1700b0":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\">Problem<\/h2> \n\n- To predict which customer is more likely to purchase the newly introduced travel package\n- Which variables are most significant.\n- Which segment of customers should be targeted more.\n\n","84c2e323":"#### Missing value Treatment MonthlyIncome","401c0ca4":"### Customer profile according to product pitched and product purchased\n**Basic package :**Most of the customer have Monthly income < 25000, Age is in range of  26-30, Designation as Executive belong to City tier 1, are salaried and single males . Customer contacted the company.Married customers also prefer this basic package.\n\n**Deluxe package:** Most of the customer have Monthly income < 25000, Age is in range of 31-40, Designation as Managers belong to city tier 3 and occupation is small business and married .Customer contacted the company. City tier 1 and divorced customers also preferred this package\n\n\n**King :** Most of the customer have  Monthly income  in range of  30000-35000, age range in 51-60,  Designation as VP. Belong to city tier 1 and are single female  and  Occupation is small business.Females buy this package more than men.\n\n**SuperDeluxe:** Most of the customer have  Monthly income < 35000, Age is in range 41-50, Designation as AVP,  belongs to tier city 3 and is Single, male and occupation is salaried. Majority of them  were company invited\n\n**Standard package:** Most of the customer have Monthly income  <30000,Age is in range of 31-40 , Designation as Senior Manager, is married , from tier city 3,and occupation is small business. majority of them had self inquired.\n","e1f6d86a":"[Top](#Table-of-Contents)","62af643f":"Random forest  is also overfitting the traning data","489fc51f":"#### Missing value Treatment number of followup.","70d5c07a":"### Income\nTo understand customers segments derving new columns which will help us identify if customer in different income range","640eaee5":"<h3 style = \"font-family:Impact;color:black;font-weight:bold\">Read and Understand data<\/h3>  ","c0124214":"Most Important features are passport , Desgination as Executive,City tier 3.","e7ef9040":"[Top](#Table-of-Contents)","eccd8745":"[Top](#Table-of-Contents)","661c3b0d":"#### Processing Gender status.\nFemale and Fe male are two category in dataset , fixing it to Female","22faca92":"* Tuned Decision Tree  gives a more generalized model.\n* XGboost seems to overfit. To get more generalized model, we can be look into tuning  XGboost with different parameters.\n* we can also tune stacking with different weak learners which can help improve the performance and get a generalized model.\n","e28886e9":"#### Summary of the dataset.","6af8ed82":"[Top](#Table-of-Contents)","a2222cd9":"## Decision Tree","c77a9089":"Based on the information provided, i assume that Customer interaction data will not be available \nfor new and potiental customers so dropping columns related to customer interaction","a3e8471f":"#### Check the data types of the columns for the dataset.","5e7bae64":"# Adaboost","78898998":"<h3 style = \"font-family:Impact;color:black;font-weight:bold\">Context<\/h3>","e8dd9a84":"[Top](#Table-of-Contents)","84afa902":"### Split the dataset","84903f08":"## Customer Profile by Product Type","df249e99":"### Tuning Bagging Classifier","942013df":"## Bagging classifier","28b0a5a1":"<h2 style = \"font-family:Impact;color:black;font-weight:bold\">Outlier Detection<\/h2>","3455c89e":"* Since we have a significant imbalance in the distribution of the target classes, we will use stratified sampling to ensure that relative class frequencies are approximately preserved in train and test sets. \n* For that we will use the `stratify` parameter in the train_test_split function.\n* Since customer interaction data will not be available for potential customers who will purchase newly introduced travel package  ,so we will drop those variables from our data for modelling.","9977432b":"#### Missing value Treatment for NumberOfTrips","868d6f74":"For more granularity imputing on occupation,Designation,Gender","65c0b989":"<center><h1 style = \"font-family:Impact;color:black;font-weight:bold\">  Travel Package Purchase Prediction -Ensemble Techniques<\/h1><\/center>"}}