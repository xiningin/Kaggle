{"cell_type":{"d733ea82":"code","2aa15d62":"code","7825db5c":"code","6a897aa0":"code","79af6ecf":"code","323f788c":"code","9d7dfd9b":"code","702d9223":"code","db200cdd":"code","06e5055e":"code","4e0d7ad4":"code","969a71bd":"code","a9f041f6":"code","5d18105a":"code","44871b7d":"code","fd8dfe68":"code","cf18bb96":"code","0e7b5a6d":"code","10aed709":"code","e2a42919":"code","ca5ba84e":"code","07edc286":"code","ca44ef4e":"code","19a5b57b":"code","e7194d73":"code","8098a5da":"code","191fa269":"code","158e9bd9":"code","81d0afa1":"code","a135d8ee":"code","ca41ce63":"code","a9d450d5":"code","6eeac574":"code","f8e06836":"code","107f5737":"code","17fd6f65":"code","882d49af":"code","975fc9ac":"code","f4432540":"code","962a7d7d":"code","d9a63b50":"code","3e9bc561":"code","ac13b187":"code","674b8700":"code","47b77f48":"code","12191763":"code","73beac7d":"code","fce74899":"markdown","1eb5c5d3":"markdown","a6c362ca":"markdown","d3fc23ed":"markdown","71c63793":"markdown","607ff8f6":"markdown","1144cf0a":"markdown","4ec27146":"markdown","037cfe6e":"markdown","328bdf0d":"markdown","3408d568":"markdown","0da30240":"markdown","0e888883":"markdown","87e904ba":"markdown","c06630cf":"markdown","e9855dc7":"markdown","5df81ed3":"markdown","663c0ea1":"markdown","d519e969":"markdown","26b12599":"markdown","6c3aaf21":"markdown","d5a40c82":"markdown"},"source":{"d733ea82":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\n\nwarnings. simplefilter(action = \"ignore\", category = Warning)","2aa15d62":"data = pd.read_csv(\"..\/input\/creditcard\/creditcard.csv\")","7825db5c":"data.head(5)","6a897aa0":"data['Class'].value_counts()","79af6ecf":"df = data.copy()","323f788c":"X = df.drop(columns = ['Class'])\nY = df[['Class']]","9d7dfd9b":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y, train_size = 0.7)","702d9223":"Y_train","db200cdd":"from sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report","06e5055e":"classifier = RandomForestClassifier()\n\nclassifier.fit(X_train,Y_train)","4e0d7ad4":"Y_pred = classifier.predict(X_test)\nprint(confusion_matrix(Y_test,Y_pred))\nprint(accuracy_score(Y_test,Y_pred))\nprint(classification_report(Y_test, Y_pred))","969a71bd":"from imblearn.under_sampling import NearMiss","a9f041f6":"ns = NearMiss(0.7)","5d18105a":"X_train_ns, Y_train_ns = ns.fit_sample(X_train,Y_train)","44871b7d":"print(Y_train['Class'].value_counts())","fd8dfe68":"print(Y_train_ns['Class'].value_counts())","cf18bb96":"classifier = RandomForestClassifier()\nclassifier.fit(X_train_ns,Y_train_ns)","0e7b5a6d":"X_test_ns, Y_test_ns = ns.fit_sample(X_test,Y_test)","10aed709":"Y_pred = classifier.predict(X_test_ns)\nprint(confusion_matrix(Y_test_ns, Y_pred))\nprint(accuracy_score(Y_test_ns,Y_pred))\nprint(classification_report(Y_test_ns,Y_pred))","e2a42919":"Y_pred = classifier.predict(X_test)","ca5ba84e":"print(confusion_matrix(Y_test, Y_pred))\nprint(accuracy_score(Y_test,Y_pred))\nprint(classification_report(Y_test,Y_pred))","07edc286":"from imblearn.over_sampling import RandomOverSampler","ca44ef4e":"os = RandomOverSampler()","19a5b57b":"X_train_os, Y_train_os = os.fit_sample(X_train,Y_train)","e7194d73":"print(Y_train['Class'].value_counts())\nprint(Y_train_os['Class'].value_counts())","8098a5da":"classifier.fit(X_train_os,Y_train_os)","191fa269":"y_pred = classifier.predict(X_test)\nprint(confusion_matrix(Y_test,y_pred))\nprint(accuracy_score(Y_test,y_pred))\nprint(classification_report(Y_test,y_pred))","158e9bd9":"#By giving sampling strategy of 0.5\nos = RandomOverSampler(0.5)\nX_train_os, Y_train_os = os.fit_sample(X_train, Y_train)","81d0afa1":"classifier.fit(X_train_os,Y_train_os)","a135d8ee":"y_pred = classifier.predict(X_test)\nprint(confusion_matrix(Y_test,y_pred))\nprint(accuracy_score(Y_test,y_pred))\nprint(classification_report(Y_test,y_pred))","ca41ce63":"X_test_os,Y_test_os = os.fit_sample(X_test, Y_test)","a9d450d5":"y_pred = classifier.predict(X_test_os)\nprint(confusion_matrix(Y_test_os,y_pred))\nprint(accuracy_score(Y_test_os,y_pred))\nprint(classification_report(Y_test_os,y_pred))","6eeac574":"from imblearn.combine import SMOTETomek","f8e06836":"sm = SMOTETomek() #first try without giving the sampling strategy","107f5737":"X_train_sm, Y_train_sm = sm.fit_sample(X_train,Y_train)","17fd6f65":"print(Y_train['Class'].value_counts())\nprint(Y_train_sm['Class'].value_counts())","882d49af":"classifier = RandomForestClassifier()\nclassifier.fit(X_train_sm,Y_train_sm)","975fc9ac":"y_pred = classifier.predict(X_test)\n\nprint(confusion_matrix(Y_test,y_pred))\nprint(accuracy_score(Y_test,y_pred))\nprint(classification_report(Y_test,y_pred))","f4432540":"#trying with giving the sampling strategy\n\nsm = SMOTETomek(0.5)","962a7d7d":"X_train_sm,Y_train_sm = sm.fit_sample(X_train,Y_train)","d9a63b50":"print(Y_train['Class'].value_counts())\nprint(Y_train_sm['Class'].value_counts())","3e9bc561":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()\nclassifier.fit(X_train_sm,Y_train_sm)","ac13b187":"y_pred = classifier.predict(X_test)\n\nprint(confusion_matrix(Y_test,y_pred))\nprint(accuracy_score(Y_test,y_pred))\nprint(classification_report(Y_test,y_pred))","674b8700":"from imblearn.ensemble import EasyEnsembleClassifier","47b77f48":"easy = EasyEnsembleClassifier()","12191763":"easy.fit(X_train,Y_train)","73beac7d":"y_pred = easy.predict(X_test)\n\nprint(confusion_matrix(Y_test,y_pred))\nprint(accuracy_score(Y_test,y_pred))\nprint(classification_report(Y_test,y_pred))","fce74899":"It gives much better for this dataset.\n\nBecause Precision and Recall is almost good.","1eb5c5d3":"# Over Sampling","a6c362ca":"# In this, We are going to Handle the Imbalanced Datasets","d3fc23ed":"I need to find the result without sampling the test data.","71c63793":"Because of the unbalanced dataset. we dont need to concentrate on the Accuracy. It biased towards one category.\n\nWe need to focus on Precision and Recall.\n\nIt looks good(bcz precision is 0.92). we will see what will happend after sampling techniques.","607ff8f6":"NearMiss of 0.7 explains that 512 * 0.7 = 361","1144cf0a":"1) Under Sampling \n\n2) Over Sampling\n\n3) Smote technique\n\n4) Ensemble technique","4ec27146":"If we over sampling the test data also, ","037cfe6e":"#### without giving any sampling strategy, it equalizes the variable","328bdf0d":"It looks very bad\n\nMost of the higher data cases, under sampling doesnt work","3408d568":"#### After the sampling strategy of giving 0.5, ","0da30240":"Performing randomforest to find the result","0e888883":"# First Find the model accuracy by keeping the variable as it is","87e904ba":"Best sampling technique for the particular dataset is only based on the results. \n\nTrail and error method.","c06630cf":"# Ensembled Techniques","e9855dc7":"# Appling SMOTE method","5df81ed3":"### By changing the parameters in the Regression models are also sometimes useful\n\n### (i.e) Cross Validation with Kfold & Tuning the Hyper parameters with the model In LOR and also in LR","663c0ea1":"#It is highly imbalanced","d519e969":"I am going to sample the test data also","26b12599":"# Under Sampling","6c3aaf21":"its also looks good(Precision and Recall Increases) if we under sampling the test data.\n\nBut we loose more informations and data's\n\nIt is good only if we have the less number of the datasets.\n\nlet see, what happend if we are not implementing the under sampling to the test data.","d5a40c82":"for more...\n\n***Handling Categorical data's (Feature Engineering)***\n\nhttps:\/\/www.kaggle.com\/ganeshbalaji1608\/handling-categorical-data-s-feature-engineering\n\n***Handling Missing Categories (Feature Engineering)***\n\nhttps:\/\/www.kaggle.comhandling-missing-categories-feature-engineering\n\n***Handling Missing Numerical Features(Feature Engineering)***\n\nhttps:\/\/www.kaggle.comhandling-missing-numerical-features-f-e\n\n***Handling Outliers***\n\nhttps:\/\/www.kaggle.comhandling-outliers-feature-engieering\n\n***Transformations (Feature Scaling)***\n\nhttps:\/\/www.kaggle.comtransformation-techniques-feature-scaling"}}