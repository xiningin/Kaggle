{"cell_type":{"1a4c7da9":"code","7f6ac6b7":"code","b926c255":"code","0c3aab5a":"code","38ef66df":"code","a3fac78f":"code","76282c00":"code","5b03c4d2":"code","2aa0996a":"code","6c8d4813":"code","3661fd0e":"code","6127a60b":"markdown","e9ef9091":"markdown","1b4cf16e":"markdown","c0d5efdd":"markdown","a3edd860":"markdown","4d6eab85":"markdown","e91b7293":"markdown","fe0d7ad4":"markdown","a6a5e084":"markdown","7b5961ab":"markdown","ddaa017b":"markdown","3157555c":"markdown","07eecb3d":"markdown","445bb682":"markdown","815b6ca0":"markdown","196a39df":"markdown","65920675":"markdown","b034cc4a":"markdown","5fa66fcc":"markdown","d95d49f1":"markdown","9600a82e":"markdown","1b1e62f9":"markdown","a4ad7de8":"markdown","c541ebb4":"markdown","a64a5f38":"markdown","0dd2552a":"markdown","38190b29":"markdown","79f14ac6":"markdown","ddc0648d":"markdown","e7c9ffac":"markdown"},"source":{"1a4c7da9":"from IPython.display import YouTubeVideo\nYouTubeVideo('B08baRr2LlY', width=800, height=450)","7f6ac6b7":"#@title Check GPU\n\nimport tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif device_name != '\/device:GPU:0':\n  print('GPU device NOT found')\nelse:\n  print('Found GPU at: {}'.format(device_name))","b926c255":"#@title Version Info\nprint('tf version: ', tf.__version__)\nprint('tf.keras version:', tf.keras.__version__)\n","0c3aab5a":"#@title Import Libraries\nfrom random import randint\nfrom numpy import array\nfrom numpy import argmax\nimport keras.backend as K\nfrom tensorflow.keras import models\nfrom numpy import array_equal\nimport numpy as np\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.layers import RepeatVector","38ef66df":"# define model\ntimesteps=40           # dimensionality of the input sequence\nfeatures=3            # dimensionality of each input representation in the sequence\nLSTMoutputDimension = 2 # dimensionality of the LSTM outputs (Hidden & Cell states)\n\ninput = Input(shape=(timesteps, features))\noutput= LSTM(LSTMoutputDimension)(input)\nmodel_LSTM = Model(inputs=input, outputs=output)\n\nmodel_LSTM.summary()\n","a3fac78f":"input = Input((None, 3))\ndense = Dense(5)(input)\noutput = Dense(2)(dense)\nmodel_dense = Model(input, output)\nmodel_dense.summary()","76282c00":"# define model\ntimesteps=40          # dimensionality of the input sequence\nfeatures=3            # dimensionality of each input representation in the sequence\nLSTMoutputDimension = 2 # dimensionality of the LSTM outputs (Hidden & Cell states)\n\n\ninput = Input(shape=(timesteps, features))\noutput= LSTM(LSTMoutputDimension)(input)\nmodel_LSTM = Model(inputs=input, outputs=output)\n\nmodel_LSTM.summary()","5b03c4d2":"W = model_LSTM.layers[1].get_weights()[0]\nU = model_LSTM.layers[1].get_weights()[1]\nb = model_LSTM.layers[1].get_weights()[2]\n\nprint(\"W\", W.size, ' calculated as 4*features*LSTMoutputDimension ', \n      4*features*LSTMoutputDimension)\nprint(\"U\", U.size, ' calculated as 4*LSTMoutputDimension*LSTMoutputDimension ', \n      4*LSTMoutputDimension*LSTMoutputDimension)\nprint(\"b\", b.size , ' calculated as 4*LSTMoutputDimension ', \n      4*LSTMoutputDimension)\nprint(\"Total Parameter Number: W+ U + b \" , \n      W.size+ U.size + b.size)\nprint(\"Total Parameter Number: 4 \u00d7 ((x + h) \u00d7 h +h) \" ,\n      4* ((features+LSTMoutputDimension)*LSTMoutputDimension+LSTMoutputDimension))\n\n","2aa0996a":"units=LSTMoutputDimension\nW_i = W[:, :units]\nW_f = W[:, units: units * 2]\nW_c = W[:, units * 2: units * 3]\nW_o = W[:, units * 3:]\n\nU_i = U[:, :units]\nU_f = U[:, units: units * 2]\nU_c = U[:, units * 2: units * 3]\nU_o = U[:, units * 3:]\n\nb_i = b[:units]\nb_f = b[units: units * 2]\nb_c = b[units * 2: units * 3]\nb_o = b[units * 3:]","6c8d4813":"# define model\ntimesteps=40\nfeatures=3\nLSTMoutputDimension = 2\n\ninput = Input(shape=(timesteps, features))\noutput= LSTM(LSTMoutputDimension)(input)\nmodel_LSTM = Model(inputs=input, outputs=output)\n\nW = model_LSTM.layers[1].get_weights()[0]\nU = model_LSTM.layers[1].get_weights()[1]\nb = model_LSTM.layers[1].get_weights()[2]","3661fd0e":"print(\"Shapes of Matrices and Vecors:\")\nprint(\"Input [batch_size, timesteps, feature] \", input.shape)\nprint(\"Input feature\/dimension (x in formulations)\", input.shape[2])\nprint(\"Number of Hidden States\/LSTM units (cells)\/dimensionality of the output space (h in formulations)\", LSTMoutputDimension)\nprint(\"W\", W.shape)\nprint(\"U\", U.shape)\nprint(\"b\", b.shape)","6127a60b":"In the above model, \n* LSTM layer has \"dimensionality of the output space\" (unit) parameter value 2 which means that Hidden and Cell states are vectors with dimension 2\n* input for each time step is represented by a vector with dimension 3 (feature)\n\nRemember:\n\n**LSTM parameter number = 4 \u00d7 (($x$ + $h$) \u00d7 $h$ + $h$)**\n\n**LSTM parameter number = 4 \u00d7 ((3 + 2) \u00d7 2 + 2)**\n\n**LSTM parameter number = 4 \u00d7 (12)**\n\n**LSTM parameter number = 48**\n\n\nSummary indicates the total number parameters of the model (actually LSTM layer) is **48** as we computed above!\n\nNow, let's get sizes of each weight matrix and bias vector from the model:\n\n","e9ef9091":"## A Sample LSTM Model\nBy the way, you can access [this Colab Notebook from the link](https:\/\/colab.research.google.com\/drive\/1Wd5340XLy-MC3YJPh3MkDnbGQ4JSnFwp?usp=sharing) in the video description","1b4cf16e":"## Let's focus on Forget Gate","c0d5efdd":"# Example","a3edd860":"* Before explaining how to calculate number of LSTM parameters,  I would like to remind you how to calculate number of a dense layer's parameters.\n*As we will see soon, LSTM has *4 dense layers* in its internal structure. So this discussion will help us a lot soon.\n*Assume that \n\n **i**= input size\n\n  **h**= size of hidden layer (number of neurons in the hidden layer)\n\n  **o**= output size (number of neurons in the output layer)\n\n* For a **single** hidden layer,\n\n  **number of parameters in the model** = connections between layers + biases in every layer (hiden + output layers!)\n  \n  = (i\u00d7h + h\u00d7o) + (h+o)\n\n* Check the below Figure [taken from here](https:\/\/towardsdatascience.com\/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889)","4d6eab85":"---\n## Prepare","e91b7293":"* Since there are **4 gates** in the LSTM unit which have  exactly the **same dense layer** architecture,  there will be \n\n   = 4 \u00d7 12\n\n   **= 48 parameters**\n\n* We can formulate the parameter numbers in a LSTM layer given that $x$ is the input dimension, $h$ is the number of LSTM units \/ cells \/ latent space \/ output dimension:\n\n#   **LSTM parameter number = 4 \u00d7 (($x$ + $h$) \u00d7 $h$ +$h$)**\n","fe0d7ad4":"**Why 8?**\n* Remember the functions? Examine Forget gate function below:\n\n ![link text](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/LSTM_functions3.png?raw=true)\n\n* W matrix multiplies input vector (x) whose dimension in the example is 1x3 to produce results as many as hidden states which is 2 in the example.\n* Thus, it is expected that W dimension should be **3x2** **but why 3x8?**  \n* Because **there are 4 gates\/functions!** and W is the matrix for ***the whole layer***! \n* Therefore, W dimension is 3 x (2 x 4) = 3 x 8!\n* Above fact also explains why U and b has 8 values\n\n = 4 * number of hidden states!\n\n* We can formulate the dimensions of parameters as:\n\n W [ x , 4 * h ]\n\n U [ h , 4 * h ]\n\n b [  4 * h ]\n\n where  \n    \n    **x** is the number of dimension\/feature of the input\n\n    **h** is the number of Hidden States\/LSTM units (cells)\/dimensionality of the output space\n\n\n","a6a5e084":"## KERAS LSTM CELL STRUCTURE\n","7b5961ab":"## SECOND EXPLANATION USING LSTM FUNCTION DEFINITIONS\n<img src=\"https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/LSTM_internal2.png?raw=true\" width=\"500\">\n\n\n* The outputs of the 4 gates in the above figure can be expressed as a function as below:\n\n<img src=\"https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/LSTM_functions.png?raw=true\" width=\"300\">\n\n\n","ddaa017b":"\n* As seen, among these functions, there are 4 functions which have \n  * 2 weight matrices: $W$ and $U$ for $h_{t-1}$ and $x_{t}$ values\n  * 1 bias vector $b$\n\n* We can work on **forget gate's function** $f_{t}$ to calculate parameter numbers:\n\n  ![link text](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/LSTM_forgetGate.png?raw=true)\n\n ![link text](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/LSTM_functions3.png?raw=true)\n\n Notice that we can guess the size (shape) of W,U and b given:\n * Input size ($h_{t-1}$ and $x_{t}$ )\n * Output size ($h_{t-1}$)\n\n Since output must equal to Hidden State (hx1) size:\n\n  * for W param =  ($h$ \u00d7 $x$)\n  * for U param =  ($h$ \u00d7 $h$)\n  * for Biases  param =   $h$\n \n * total params = W param + U param + Biases param\n  \n    =  ($h$ \u00d7 $x$) +  ($h$ \u00d7 $h$) +  $h$\n\n    =  ( ($h$ \u00d7 $x$) +  ($h$ \u00d7 $h$) +   $h$ )\n\n    =  ( ($x$ + $h$) \u00d7  $h$  +   $h$ )\n\n* there are 4 functions which are exactly defined in the same way, in the LSTM layer, there will be \n\n ##   **LSTM parameter number = 4 \u00d7 (($x$ + $h$) \u00d7 $h$ +$h$)**\n\n\n\n\n\n","3157555c":"---\n","07eecb3d":"Let's **re-formulate** the calculations:\n\n= (i \u00d7 h + h \u00d7 o) + (h + o)\n\n= **(i + o) \u00d7 h + (h + o)**\n\nWe will use above formula below :)","445bb682":"<img src=\"https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/LSTM_internal2.png?raw=true\" width=\"700\">\n\n[Image from here](https:\/\/towardsdatascience.com\/animated-rnn-lstm-and-gru-ef124d06cf45)\n","815b6ca0":"NOTICE:\n* The above W, U and B are compacted for all gates.\n* If you want, you can access the values of **each gate values** as well:","196a39df":"# BONUS: Shapes of Matrices and Vectors:","65920675":"## REMINDER:\n\n* LSTM ***expects*** **input data** to be a **3D** tensor such that:\n\n      [batch_size, timesteps, feature]\n\n* **batch_size** how many samples in each batch during training and testing\n\n* **timesteps** means how many values exist in a sequence. For example in [4, 7, 8, 4] there are 4 timesteps\n\n* **features**: how many dimensions are used to represent a data in one time step. For example, if each value in the sequence is one hot encoded with 9 zero and 1 one then feature is 10\n* Example:\n    \n    In raw format: \n\n    X=[4, 7, 8, 4]\n\n    In one hot encoded format with 10 dimensions (feature = 10):\n\n    X=[[0 0 0 0 1 0 0 0 0 0]\n\n     [0 0 0 0 0 0 0 1 0 0]\n    \n    [0 0 0 0 0 0 0 0 1 0]\n    \n    [0 0 0 0 1 0 0 0 0 0]]","b034cc4a":"## FINDING NUMBER OF PARAMETERS IN A NEURAL NETWORK WITH A SINGLE DENSE LAYER  ","5fa66fcc":"# FIRST EXPLANATION USING LSTM INTERNAL STRUCTURE\nReview the above Figure to capture the internal structure of LSTM cell\n\n* There are 3 inputs to the LSTM cell:\n  * $h_{t-1}$ previous timestep (t-1) Hidden State value\n  * $c_{t-1}$ previous timestep (t-1) Cell State value\n  * $x_{t}$   current timestep (t) Input value\n\n* There are 4 dense layers:\n  * Forget Gate\n  * Input Gates = Input + Candidate \n  * Output Gate\n\n* The input output tensor sizes (dimensions) are symbolized with circles. In the figure,\n  * **Cell and Hidden states are vectors which have a dimension = 2.** This number is defined by the programmer by setting LSTM parameter units (LSTMoutputDimension) to 2\n  * **Input is a vector which has a dimension = 3.** This number is also defined by the programmer by deciding how many dimension would be to represent an input (e.g. dimension of one-hot encoding, word embedding, etc.)  \n\n* Note that, By definition:\n  * Hidden and Cell states vector dimensions must be the same\n  * Hidden and Cell states vector dimensions at time **t-1** and **t** must be the same\n  * Each input in the sequence must have the same vector dimensions\n\n \n\n \n","d95d49f1":"---","9600a82e":"<a href=\"https:\/\/colab.research.google.com\/github\/kmkarakaya\/ML_tutorials\/blob\/master\/LSTM_Understanding_the_Number_of_Parameters.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","1b1e62f9":"<img src=\"https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/LSTM_internal2.png?raw=true\" width=\"500\">\n\n[Image from here](https:\/\/towardsdatascience.com\/animated-rnn-lstm-and-gru-ef124d06cf45)\n","a4ad7de8":"We can find the number of parameters by counting the number of connections between layers and by adding bias.\n\n* **connections** (weigths) between layers:\n  *  between **input** and **hidden** layer is \n \n   i * h = 3 * 5 = 15 \n  * between **hidden** and **output** layer is \n\n   h * o = 5 * 2 = 10\n* **biases** in every layer\n  * biases in **hidden** layer\n   \n   h = 5\n  * biases in **output** layer\n\n   o = 2\n* **Total**:\n\n  15 + 10 + 5 + 2 = 32 parameters (weights + biases)   \n\n\nLet's create a simple model and verify the calculation:\n","c541ebb4":"#LSTM: Understanding the Number of Parameters\n\nIn this tutorial, we will focus on internal structure of Keras LSTM layer in order to understand how many **learnable parameters** an LTSM layer has.\n\nWhy do we **need** to care of calculating number of parameters in LSTM layer ***since*** we can easily get this number in the model summary report?\n\nWell there are several reasons:\n* First of all, to calculate the number of learnable parameters correctly, we need to **understand how LSTM is structured** and how **LSTM operates** ***in depth***. Thus, we will delve into LSTM gates and gate functions. We will gain precious insight about how LSTM handles ***time dependent*** or ***sequence*** input data.\n\n* Secondly, in ANN models, number of parameter is a really important metric for understanding the **model capacity** and **complexity**. We need to keep an eye on the number of parameters of each layer in the model to handle ***overfitting*** or ***underfitting*** situations. One way to prevent these situations is to **adjust the number of parameters** of each layer. We need to know how number of parameters actullay affects the performance of each layer.\n\nIf you want to enhance your understanding of LSTM layer and learn how many learnable parameters it has please continue this tutorial.\n\nBy the way, I would like to mention that in [my Youtube channel](https:\/\/www.youtube.com\/channel\/UCrCxCxTFL2ytaDrDYrN4_eA) I have a dedicated playlist in English ([All About LSTM](https:\/\/www.youtube.com\/playlist?list=PLQflnv_s49v_i1OVqE0DENBk-QJt9THjE)) and in Turkish  ([LSTM Hakk\u0131nda Her\u015fey](https:\/\/www.youtube.com\/playlist?list=PLQflnv_s49v9rxWesjI1GVs8qfxwwHDR4)). You can check these playlist to learn more about LSTM.\n\nLastly, if you want to be notified for upcoming tutorials about LSTM and Deep Learning **please subscribe to** [my Youtube channel](https:\/\/www.youtube.com\/channel\/UCrCxCxTFL2ytaDrDYrN4_eA) and a**ctivate notifications**. \n\n**Thank you!**\n\n\nNow, let's get started!","a64a5f38":"![link text](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/LSTM_functions2.png?raw=true)","0dd2552a":"## CONCLUSION\n* In this tutorial, we investigate the internal structure of Keras LSTM layer to calculate the number of learnable parameters.\n* We examine several concepts: time steps, dimentionality of the output space, gates, gate functions etc.\n* We come up with the correct formulation in 2 different ways:\n  \n  **LSTM parameter number = 4 \u00d7 (($x$ + $h$) \u00d7 $h$ + $h$)**\n* We check the correctness of the formulation by creating a simple LSTM model\n\nFrom here, you can continue learning about LSTM for example with understanding the outputs of LSTM layer. I have a tutorial for it already on [Youtube](https:\/\/youtu.be\/B66760rvHA8) and [Medium](https:\/\/medium.com\/@kmkarakaya\/lstm-understanding-output-types-e93d2fb57c77).\n\nEnjoy!","38190b29":"##References:\n[tf.keras.layers.LSTM official website](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/LSTM)\n\n[Counting No. of Parameters in Deep Learning Models by Hand by Raimi Karim](https:\/\/towardsdatascience.com\/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889)\n\n[llustrated Guide to LSTM\u2019s and GRU\u2019s: A step by step explanation by\nMichael Phi](https:\/\/towardsdatascience.com\/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n\n[Animated RNN, LSTM and GRU by Raimi Karim\n](https:\/\/towardsdatascience.com\/animated-rnn-lstm-and-gru-ef124d06cf45)\n\n[Number of parameters in Keras LSTM by  Dejan Batanjac](https:\/\/dejanbatanjac.github.io\/2019\/02\/12\/Number-of-parameters-in-Keras-LSTM.html#:~:text=The%20number%20of%20U%20parameters,parameters%20based%20on%20input%20shape.)\n","79f14ac6":"![link text](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/LSTM_forgetGate.png?raw=true)\n\n* As seen in above figure, \n  * there are Hidden state values (**2  $h_{t-1}$** red circles) and\n  * Input values ( **3 $x_{t}$** green circles)\n* Total 5 numbers ( **2  $h_{t-1}$** + **3 $x_{t}$** ) are inputted to   **a dense layer**\n* Output layer has **2  values** (which **must be equal** to the dimension of  $h_{t-1}$ Hidden state vector in the LSTM Cell) \n\n* We can calculate the number of parameters in this dense layer as we did before:\n\n  = ($h_{t-1}$ + $x_{t}$) \u00d7 $h_{t-1}$ + $h_{t-1}$\n\n  = (2 + 3) \u00d7 2 + 2\n\n  = 12\n\n* **Thus Forget Gate has 12 parameters (weights + biases)**\n\n\n\n\n","ddc0648d":"**IMPORTANT**: \n* **timesteps** (or ***input_length***) of the input sequence does **NOT** affect the number of parameters! \n  \n  **WHY?** \n  \n  Because,  same \"W\", \"U\", and \"b\" are **shared** ***throughout the time-steps***\n\n  That is; instead of using new weights and biases at each time step, LSTM unit **uses the same** \"W\", \"U\", and \"b\" values for ***all time-steps***!\n\n  This **simplifies** the calculation of backpropagation and **reduce** the number of parameters (memory requirement)\n\n\n\n\n\n","e7c9ffac":"![link text](https:\/\/miro.medium.com\/max\/501\/1*sTV2UIv76WQiHt4JfCqk9g.png)"}}