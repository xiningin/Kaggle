{"cell_type":{"6ae4b58f":"code","d1e345c4":"code","6a544a43":"code","240ef3fd":"code","bb68989a":"code","719109b3":"code","d3a0733f":"code","9a30ff5b":"code","d210c653":"code","e72e0723":"code","528f9967":"code","259e784a":"code","afccf436":"code","8a0e7add":"code","454ddaf9":"code","948d9512":"code","f6dfae46":"code","bb1e13c3":"code","50cb5df3":"code","3b17f3ba":"code","85a4e5a2":"code","a7da50c6":"code","b9857209":"code","7b8b273c":"code","bd1eeb9f":"code","8367400a":"code","ac3cbdd3":"code","b3de9251":"code","4821b7c1":"code","8dac07f1":"code","8e7d9226":"code","5167bc23":"code","355af9be":"code","58d1acba":"code","6945fbbd":"code","99af13d5":"code","f7a6a797":"code","be2aae49":"code","6fe93c42":"code","ed45b1ab":"code","62659e22":"code","f971b441":"code","eff66ff3":"code","333f5da3":"code","48f7884a":"code","4c4ad841":"code","6f59f5ed":"code","127ee6c0":"code","9ad86e15":"code","3d048306":"code","36b1d072":"code","a98fabe8":"code","688183b6":"code","543345a9":"code","ba686add":"code","2b4cdfb6":"code","ce329d45":"code","536258da":"code","e115fe35":"code","ed25c952":"code","2e02c8cd":"code","6a747741":"code","b9a4e536":"code","35b6ceaa":"code","77b37b5f":"code","95c31282":"code","e3a63821":"code","d0563ba4":"code","6d83908a":"markdown","853ad481":"markdown","5f4cb665":"markdown","61374b4d":"markdown","8f67fac6":"markdown","46e72236":"markdown","06d1a35b":"markdown","92e98283":"markdown","83b65bf7":"markdown","098022ff":"markdown","302eef73":"markdown","b07f2c4d":"markdown","f366b082":"markdown","b4cc4cd6":"markdown","f675fbaf":"markdown","a3c429ca":"markdown","9389be38":"markdown","d950c9af":"markdown","1d1ee2c7":"markdown","1c4fbd4f":"markdown","01378046":"markdown","cbafd1e9":"markdown","06cfcf70":"markdown","f32d8a3b":"markdown","092ef23d":"markdown","735ad055":"markdown","3e15bda5":"markdown","26058730":"markdown","e8f58ee6":"markdown","6163c60a":"markdown","048fcdf3":"markdown","e17a5a08":"markdown","bc05fc47":"markdown","a326792f":"markdown","d51c4284":"markdown","db068353":"markdown","9c18f8b1":"markdown","0f010c67":"markdown","78b3be4e":"markdown","62f0323e":"markdown","39851910":"markdown","a65037b9":"markdown","d4238fd7":"markdown","80ba97e2":"markdown","4d70b451":"markdown","382396fb":"markdown","d232a24e":"markdown","66e0be6d":"markdown","ed554807":"markdown","7039c6fd":"markdown","144f88c3":"markdown","70bd9aee":"markdown","e61ebe4f":"markdown","2bedeebc":"markdown","a4d417db":"markdown","57b8d6c0":"markdown","bc8692d0":"markdown","a41312e3":"markdown","59162a72":"markdown","db91f965":"markdown","c3b1291c":"markdown","35876e6c":"markdown","bdaa4d29":"markdown","f252ff23":"markdown"},"source":{"6ae4b58f":"# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n#!pip install comet on the notebook\n\"\"\"\n    Libraries are reinstalled everytime when working on a remote kernel \n\"\"\"\n!pip install comet_ml","d1e345c4":"#import the experiment module from comet_ml library\nfrom comet_ml import Experiment\n#Setting the api key for comet\nexperiment = Experiment(api_key=\"NbR6YTm2rXzu4hEh9W49inxu6\",\n                        project_name=\"classification-team-ts3\", workspace=\"percymokone\")","6a544a43":"# data analysis and wrangling libraries\nimport numpy as np\nimport pandas as pd\n\n# visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# preprocesssing\nimport re\nimport string\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\n#stopwords\nimport nltk\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nfrom collections import defaultdict\n\n# modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import resample\nfrom wordcloud import WordCloud\nfrom textblob import TextBlob\n\n# model evaluation \nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nfrom sklearn import metrics\n\n#warnings\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n\n#% matplotlib inline","240ef3fd":"# load the training and test data\ndftrain = pd.read_csv('..\/input\/climate-change-belief-analysis\/train.csv')\ndftest = pd.read_csv('..\/input\/climate-change-belief-analysis\/test.csv')","bb68989a":"#visualising the first five rows of our Dataframe\ndftrain.head()","719109b3":"dftrain.info()","d3a0733f":"#A bar graph comparing the frequency of each class sentiment\nnews=dftrain[dftrain.sentiment == 2].shape[0]\npro =dftrain[dftrain.sentiment == 1].shape[0]\nneutral=dftrain[dftrain.sentiment == 0].shape[0]\nanti =dftrain[dftrain.sentiment == -1].shape[0]\n#visualising\nplt.figure(1,figsize=(14,8))\nplt.bar([\"News\", \"Pro\", \"Neutral\" , \"Anti\"],[news, pro, neutral , anti])\nplt.xlabel('Tweet_class')\nplt.ylabel('Sentiment counts')\nplt.title('Sentiment Value Counts')\nplt.show()","9a30ff5b":"def remove_RT(df):\n    \"\"\"\n        This function removes all the retweeted tweets before resampling so that we do not resample\n        from already repeating tweets\n    \"\"\"\n    df = df.copy()\n    tweets = list(df['message'])\n    New_tweets = []\n    for tweet in tweets:\n        if tweet not in New_tweets:\n            New_tweets.append(tweet)\n        else:\n            New_tweets.append(None)\n    df['message'] = New_tweets\n    for twee in df['message']:\n        if twee is None:\n            ind = list(df['message']).index(twee)\n            df.drop(df.index[ind], inplace = True)\n    return df","d210c653":"def resambling(df):\n    \"\"\"\n        The class size is a value between the size of the majority class and the size of the minority class.\n        This function resamples by downsampling classes with observations greater than the class size and\n        upsampling the classes with observations smaller than the class size.\n    \"\"\"\n    df = df.copy()\n    df = remove_RT(df)\n    class_2 = df[df['sentiment'] == 2]  #upsampling\n    class_1 = df[df['sentiment'] == 1]  #majority class\n    class_0 = df[df['sentiment'] == 0]  #upsampling\n    class_n1 = df[df['sentiment'] == -1]  #minority class\n    class_size = int(((len(class_1)-len(class_n1))\/2) + len(class_n1))\n    #downsampling class 1\n    rclass_1 = resample(class_1, replace=False, n_samples=class_size, random_state=27)\n    #upsampling class 2\n    rclass_2 = resample(class_2, replace=True, n_samples=class_size, random_state=27)\n    #upsampling class 0\n    rclass_0 = resample(class_0, replace=True, n_samples=class_size, random_state=27)\n    #upsampling class -1\n    rclass_n1 = resample(class_n1, replace=True, n_samples=class_size, random_state=27)\n    dfsampled = pd.concat([rclass_2, rclass_1, rclass_0, rclass_n1])\n    \n    return dfsampled","e72e0723":"Resampled_Train_DF = resambling(dftrain)","528f9967":"#A bar graph comparing the frequency of each class sentiment in resampled dataframe\nnews=Resampled_Train_DF[Resampled_Train_DF.sentiment == 2].shape[0]\npro =Resampled_Train_DF[Resampled_Train_DF.sentiment == 1].shape[0]\nneutral=Resampled_Train_DF[Resampled_Train_DF.sentiment == 0].shape[0]\nanti =Resampled_Train_DF[Resampled_Train_DF.sentiment == -1].shape[0]\n\n#visualization\nplt.figure(1,figsize=(14,8))\nplt.bar([\"News\", \"Pro\", \"Neutral\" , \"Anti\"],[news, pro, neutral , anti])\nplt.xlabel('Tweet_class')\nplt.ylabel('Sentiment counts')\nplt.title('Sentiment Value Counts')\nplt.show()","259e784a":"# This line of code counts the number of words in each tweet and add a column of those counts into the existing dataframe \n\nword_count = dftrain['message'].apply(lambda x: len(x.split()))\ndftrain['word_count'] = word_count\n","afccf436":"# create subplots\nplt.figure(figsize=(14,8))\nfig,axs = plt.subplots(1, 4, sharey = True)\n\n# plot title\nfig.suptitle('Boxplots for word count of each class')\n\n# class 2 plot\ny2 = dftrain[dftrain['sentiment'] == 2]['word_count']\naxs[0].boxplot(y2)\naxs[0].set_xlabel('class 2')\n\n# class 1 plot\ny1 = dftrain[dftrain['sentiment'] == 1]['word_count']\naxs[1].boxplot(y1)\naxs[1].set_xlabel('class 1')\n\n# class 0 plot\ny0 = dftrain[dftrain['sentiment'] == 0]['word_count']\naxs[2].boxplot(y0)\naxs[2].set_xlabel('class 0')\n\n# class -1 plot\ny_1 = dftrain[dftrain['sentiment'] == -1]['word_count']\naxs[3].boxplot(y_1)\naxs[3].set_xlabel('class -1')\n\nplt.show()","8a0e7add":"a1 =dftrain[dftrain.sentiment == 2]['word_count']\na2 =dftrain[dftrain.sentiment == 1]['word_count']\na3 =dftrain[dftrain.sentiment == 0]['word_count']\na4 =dftrain[dftrain.sentiment == -1]['word_count'] \nplt.figure(1,figsize=(30,8))\nplt.subplot(1, 2, 1)\nplt.ylabel('word count frequency')\nsns.distplot(a1,color='purple',bins=50)\nsns.distplot(a2,color='blue',bins=50)\nsns.distplot(a3,color='green',bins=50)\nsns.distplot(a4,color='red',bins=50)\nplt.legend(['News','Pro', 'Neutral','Anti'])\nplt.title('Word Count Distribution Graph')\n\nplt.show()","454ddaf9":"# This line pf code counts the number of punctuation in each tweet then add the column of those counts into the existing dataframe\n\ndftrain['punct_count']  = dftrain['message'].apply(lambda x: len([i for i in x if i in string.punctuation]))\n\n\n","948d9512":"# A bar graph showing the number of punctuations as per class\n# labling the classes\nnews=dftrain[dftrain.punct_count == 2].shape[0]\npro =dftrain[dftrain.punct_count == 1].shape[0]\nneutral=dftrain[dftrain.punct_count == 0].shape[0]\nanti =dftrain[dftrain.punct_count == -1].shape[0]\n\n#plotting the graph and setting the axis labels\nplt.figure(1,figsize=(14,8))\nplt.bar([\"News\", \"Pro\", \"Neutral\" , \"Anti\"],[news, pro, neutral , anti])\nplt.xlabel('Sentiment class')\nplt.ylabel('punctuation')\nplt.title('The number of punctuation in each sentiment')\nplt.show() #visualizing the graph","f6dfae46":"# A function that creates a corpus based on the target feature\ndef create_corpus(df,sentiment):\n    \"\"\"\n    create corpus based on the target feature\n    \"\"\"\n    list1 = []\n    #appends each word from the messege to the list above and return the list\n    for s in dftrain[dftrain[\"sentiment\"]== sentiment].message.str.split():\n        for i in s:\n            list1.append(i)\n    return list1    ","bb1e13c3":"#corpus created for each class\ncorpus2 = create_corpus(df=dftrain, sentiment=2)\ncorpus1 = create_corpus(df=dftrain, sentiment=1)\ncorpus0 = create_corpus(df=dftrain, sentiment=0)\ncorpus3 = create_corpus(df=dftrain, sentiment=-1)\n\n\n\nd2= defaultdict(int)\nfor word in corpus2:\n    if word in stop:\n        d2[word]+=1\n        \nd1 =defaultdict(int)\nfor word in corpus1:\n    if word in stop:\n        d1[word]+=1\n    \nd0= defaultdict(int)\nfor word in corpus0:\n    if word in stop:\n        d0[word]+=1\n        \nd3= defaultdict(int)\nfor word in corpus3:\n    if word in stop:\n        d3[word]+=1\n        \n      \n      ","50cb5df3":"#sorting the results in descending order and get the firs 10 results\nmost2 = sorted(d2.items(), key=lambda x:x[1], reverse=True)[:10]\nmost1 = sorted(d1.items(), key=lambda x:x[1], reverse=True)[:10]\nmost0 = sorted(d0.items(), key=lambda x:x[1], reverse=True)[:10]\nmost3 = sorted(d3.items(), key=lambda x:x[1], reverse=True)[:10]\nx2,y2 =zip(*most2)\nx1 ,y1=zip(*most1)\nx0 ,y0=zip(*most0)\nx3 ,y3=zip(*most3)","3b17f3ba":"#ploting the graphs\nplt.figure(1, figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.bar(x2,y2)\nplt.title(\"Stop words frequency\")\nplt.ylabel('The  stop words frequency')\nplt.xlabel('Stop words found each class')\nplt.subplot(1, 2, 2)\nplt.bar(x1, y1)\nplt.title(\"Stop words frequency\")\nplt.ylabel('The  stop words frequency')\nplt.xlabel('Stop words found each class')\nplt.subplot(1, 2 , 1)\nplt.bar(x0,y0)\nplt.legend(['News','Neutral'])\nplt.subplot(1 , 2, 2)\nplt.bar(x3,y3)\nplt.legend(['Pro','Anti'])\n\n","85a4e5a2":"#making a copy of the dataframe which allows us to make some changes without changing the original dataframe\ndf_copy = dftrain.copy()\ndf_test = dftest.copy()","a7da50c6":"#using regular expressions to remove tagy symbols\ndef remove_pattern(input_txt, pattern):\n    \"\"\"\n    this function checks for patterns in the input_txt and removes them\n    \"\"\"\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt","b9857209":"#applyng the funtion above to remove patterns on the message column\ndf_copy['message'] = np.vectorize(remove_pattern)(df_copy['message'], \"@[\\w]*\") \ndf_test['message'] = np.vectorize(remove_pattern)(df_test['message'], \"@[\\w]*\") \n\n","7b8b273c":"\ndef  clean_text(df, text_field):\n    \"\"\"\n    this function takes in a dataframe,text field and removes urls from the text field then return a dataframe with urls removed form the text field\n    \"\"\"\n    df[text_field] = df[text_field].str.lower()\n    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\", \"\", elem))  \n    return df\n#test_clean = clean_text(clean_test_df, \"message\")\ndf_clean = clean_text(df_copy,\"message\")\ndf_test = clean_text(df_test,\"message\")","bd1eeb9f":"#A function that removes punctuations\ndef remove_punctuations(text):\n    table = str.maketrans(\"\",\"\",string.punctuation)\n    txt = text.translate(table)\n    return txt\n#removing punctuationd from the message column\ndf_clean[\"message\"] = df_clean['message'].apply(remove_punctuations)\ndf_test[\"message\"] = df_test['message'].apply(remove_punctuations)\n\n","8367400a":"#A function that removes stop words \ndef remove_stopwords(text):\n    text = [word.lower() for word in text.split() if word.lower() not in stop]\n    r = \" \".join(text)\n    return r\n#removing stop words from the message column\ndf_clean[\"message\"]=df_clean[\"message\"].apply(remove_stopwords)\n#visualing the cleaned dataframe\ndf_clean.head()\n","ac3cbdd3":"df_clean['message'] = df_clean['message'].apply(lambda x: x.replace('climate change',''))\ndf_clean['message'] = df_clean['message'].apply(lambda x: x.replace('global warming',''))\ndf_test['message'] = df_clean['message'].apply(lambda x: x.replace('climate change',''))\ndf_test['message'] = df_clean['message'].apply(lambda x: x.replace('global warming',''))\n","b3de9251":"# Word cloud for the overall data checking out which words do people use more often\nall_words = ' '.join([text for text in df_clean['message']])\n\n# Word cloud for the overall data checking out which words do people use more often\nwordcloud = WordCloud(width=800, height=500,random_state=21,max_font_size=110).generate(all_words)\n\n#ploting the word cloud\nplt.figure(figsize=(16,8))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show() #visualizing the word cloud ","4821b7c1":"class2_words = ' '.join([text for text in df_clean[dftrain['sentiment']==2]['message']])\nclass1_words = ' '.join([text for text in df_clean[dftrain['sentiment']==1]['message']])\nclass0_words = ' '.join([text for text in df_clean[dftrain['sentiment']==0]['message']])\nclass_neg1_words = ' '.join([text for text in df_clean[dftrain['sentiment']==-1]['message']])\n\n#visualizing the classes\nwordcloud2 = WordCloud(width=800, height=500,random_state=21,max_font_size=110).generate(class2_words)\nwordcloud1 = WordCloud(width=800, height=500,random_state=21,max_font_size=110).generate(class1_words)\nwordcloud0 = WordCloud(width=800, height=500,random_state=21,max_font_size=110).generate(class0_words)\nwordcloudneg1 = WordCloud(width=800, height=500,random_state=21,max_font_size=110).generate(class_neg1_words)","8dac07f1":"fig = plt.figure(figsize=(1000,500))\nfig,axs = plt.subplots(2, 2)\nfig.suptitle('Boxplots for word count of each class')\n\n# word cloud plots\naxs[0,0].imshow(wordcloud2, interpolation=\"bilinear\")\naxs[1,1].imshow(wordcloud1, interpolation=\"bilinear\")\naxs[0,1].imshow(wordcloud0, interpolation=\"bilinear\")\naxs[1,0].imshow(wordcloudneg1, interpolation=\"bilinear\")\n\n# removing axes\naxs[0,0].axis('off')\naxs[1,1].axis('off')\naxs[0,1].axis('off')\naxs[1,0].axis('off')\n\n# word cloud titles\naxs[0,0].set_title('Climate Change News')\naxs[1,1].set_title('Pro Climate Change')\naxs[0,1].set_title('Neutral to Climate Change')\naxs[1,0].set_title('Anti Climate Change')\n\nplt.show() ","8e7d9226":"df_clean['sites'] = dftrain['message'].apply(lambda x: re.search(\"(?P<url>https?:\/\/[^\\s]+)\", x).group(\"url\") if 'https' in x and re.search(\"(?P<url>https?:\/\/[^\\s]+)\", x) != None else None)\n","5167bc23":"def get_subjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndef get_polarity(text):\n    return TextBlob(text).sentiment.polarity\n\ndf_clean['subjectivity'] = df_clean[\"message\"].apply(get_subjectivity)\n\ndf_clean['polarity'] = df_clean['message'].apply(get_polarity)\ndf_clean.head()\n","355af9be":"#polarity and subjectivity plot\nplt.figure(figsize=(16,8))\nfor i in range(0, df_clean.shape[0]):\n    plt.scatter(df_clean['polarity'][i],df_clean['subjectivity'][i], color = 'Blue')\n    \nplt.title('Sentiment analysis')\nplt.xlabel('Polarity')\nplt.ylabel('Subjectivity')\nplt.show()\n","58d1acba":"# splitting the training data\nX = dftrain['message']  \ny = dftrain ['sentiment']\nXt = dftest['message']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","6945fbbd":"# import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# import LinearSVC\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.pipeline import Pipeline\n# this is a pipeline for a trial model\ntext_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC()),])\n# this is pipeline for unseen data\nunseen_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC()),])\n\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train) \n# Feed the all training data through the pipeline for unseen data\nunseen_clf.fit(X,y)","99af13d5":"# Testing the model\ny_pred = text_clf.predict(X_test)\nYY = unseen_clf.predict(Xt)","f7a6a797":"s=pd.DataFrame(YY)\nf_new = s.rename(columns={0: 'sentiment'})\ntweedid=dftest['tweetid']\ndf_merge_col = pd.merge(tweedid, f_new, left_index=True, right_index=True)\ndf_merge_col.to_csv('Pipeline_Predictions.csv',index=False)","be2aae49":"conf_matrix = metrics.confusion_matrix(y_test,y_pred)\nprint(\"Confusion matrix \\n\\n {}\".format(conf_matrix))","6fe93c42":"accu_score = metrics.accuracy_score(y_test,y_pred) \nprint('Accuracy Score = {}'.format(accu_score))","ed45b1ab":"precision = precision_score(np.array(y_test), y_pred, average = 'weighted')\nprint('Precision Score = {}'.format(precision))","62659e22":"recall = recall_score(y_test, y_pred, average = 'weighted')\nprint('Recall Score = {}'.format(recall))","f971b441":"f1 = f1_score(y_test, y_pred, average = 'weighted')\nprint(\"F1_Score = {}\".format(f1))","eff66ff3":"print(metrics.classification_report(y_test,y_pred))","333f5da3":"#creating a copy of our dataframe\ndf_copy = dftrain.copy()","48f7884a":"#visualising the copy of the dataframe\ndf_copy.head()","4c4ad841":"#variable assignment\nX= np.array(dftrain['message'])\ny= np.array(dftrain['sentiment'])\n\n#splitting the dataset into training and validation set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n#standardizing the data set\n#from sklearn.preprocessing import Normalizer\n#normalise = Normalizer()\n#X_train = normalise.fit_transform(X_train)\n#X_test = normalise.fit_transform(X_test)\n\n\n#transforming the sentiments\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(X_train)\nX_test = vectorizer.transform(X_test)\n\n#selecting the logistic regression model from sklearn and selecting lbfgs as our solver\nfrom sklearn.linear_model import LogisticRegression\nlr_model = LogisticRegression(solver='lbfgs')\n\nlr_model.fit(X_train, y_train)\npredictions = lr_model.predict(X_test)\n#predictions[0]\n\n# pickle\n#model_save_path = \"lr_model.pkl\"\n#with open(model_save_path,'wb') as file:\n   # pickle.dump(lr_model, file)","6f59f5ed":"#importing the metrics library\nfrom sklearn import metrics\n\n# Printing a confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","127ee6c0":"# creating a dataframe of the confusion matrix \ndf_confusion = pd.DataFrame(metrics.confusion_matrix(y_test,predictions))\n#visualing the confusion dataframe\ndf_confusion","9ad86e15":"# Print a classification report\nprint(metrics.classification_report(y_test,predictions))","3d048306":"# Print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","36b1d072":"df_SVM_train = dftrain.copy()\ndf_SVM_test=dftest.copy()","a98fabe8":"#Removing puncuations from the messege column so that we can use the text only for the model\ndef remove_punctuations(text):\n    for punctuation in string.punctuation:\n        text = text.replace(punctuation, '')\n    return text","688183b6":"df_SVM_test[\"message\"] = df_SVM_test['message'].apply(remove_punctuations)\ndf_SVM_test[\"message\"] = df_SVM_test['message'].apply(remove_punctuations)","543345a9":"#Defining training and testing data\n#Defining training and testing data\nX_SVM = df_SVM_train['message']\n\ny_SVM = df_SVM_train['sentiment']\n\n\nx_test = df_SVM_test['message']","ba686add":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\n\nX_train_counts = count_vect.fit_transform(X_SVM)\nX_train_counts.shape","2b4cdfb6":"#Transforming also the x_test data to numerical data\nX_test_counts = count_vect.transform(x_test)\nX_test_counts.shape","ce329d45":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, Ytrain, Ytest = train_test_split(X_train_counts, y_SVM, test_size=0.2, random_state=42)","536258da":"from sklearn.svm import SVC  # to be added on the import cell\nsvm = SVC()\nsvc = SVC(kernel='linear')\nsvc.fit(x_train, Ytrain)","e115fe35":"ypred = svc.predict(x_test)","ed25c952":"#importing the metrics library\nfrom sklearn import metrics\n\n# Printing a confusion matrix\nprint(metrics.confusion_matrix(Ytest,ypred))","2e02c8cd":"# Print the overall accuracy\nprint(metrics.accuracy_score(Ytest,ypred))","6a747741":"# creating a dataframe of the confusion matrix \ndf_confusion = pd.DataFrame(metrics.confusion_matrix(Ytest,ypred))\n#visualing the confusion dataframe\ndf_confusion","b9a4e536":"y_pred = svc.predict(X_test_counts)\n","35b6ceaa":"s=pd.DataFrame(y_pred)\nf_new = s.rename(columns={0: 'sentiment'})\ntweedid=df_SVM_test['tweetid']\ndf_merge_col = pd.merge(tweedid, f_new, left_index=True, right_index=True)\ndf_merge_col.to_csv('SVM_Predictions.csv',index=False)","77b37b5f":"#Create a dictionary of model parameters\nparams = {\"random_state\": 42,\n          \"model_type\": \"Pineline\",\n          \"feature_extraction\": \"TfidfVectorizer\"\n          }","95c31282":"#Create dictionary of important metrics\nmetrics = {\"f1\": f1,\n           \"Accuracy_Score\": accu_score,\n           \"recall\": recall,\n           \"precision\": precision\n           }","e3a63821":"# Log our parameters and results\nexperiment.log_parameters(params)\nexperiment.log_metrics(metrics)","d0563ba4":"experiment.end()","6d83908a":"splitting the dataset into training and validation set","853ad481":"### Word Count Distribution plot","5f4cb665":"From this graph we can see that the tweets with a higher word count frequency come from the PRO-class (Class1), that is the the tweets that supports the belief of man-made Climate-Change .We also notice that these four classes are imbalanced, which affects the accuracy score of the model negatively. This shows that resambling is necessary before training a model with this data.\n\nWe also learned that there is a good number of repeating tweets, retweets. Since resampling requires picking data randomly from a certain class to recreate it with a fixed number of entries, removing repeating retweets will allow fair random pick. Below we build a function that goes through the whole DataFrame removing all repeating tweets before resampling.","61374b4d":"* Removing stopwords\n* Removing punctuation\n* Removing urls\n* Removing characters\n* Removing retweets\n","8f67fac6":"# **Model Matrices** <a name='Model_matrices'>\n\nwe will firstly look at the Definations of :\n*     accuracy_score\n*     precission_score\n*     recall_score\n*     f1_score.\n*     Confution matrix\n     ","46e72236":"In this Notebook, we conducted a whirlwind overview of vectorization techniques and began to consider their use cases for different kinds of data and different machine learning algorithms. we noticed some imbalance in the classes and decided to upscale the classes .\n\nWe have trained a couple of models and in conclusion we chose the tfidfvectorizer Pipeline Model because it substaintially outperformed other models for certain tasks. It also gave an accuracy score that is higher than that of theother models. This model will be able to classify whether or not a person believes in climate change, based on their novel tweet data.","06d1a35b":"these are the stop words that were used in each class","92e98283":"- Word count analysis\n- The number a punctuations \n- For Twitter Data: amount of retweets\n- Word clouds for each class\n- Sentiment analysis","83b65bf7":"# **Importing Libraries** <a name=\"Importing_libraries\"><\/a>","098022ff":"# **Feature Engeneering** <a name='Feature_engeneering'>","302eef73":"Comet_ml is used to store all the models run in this notebook under workspace: percymokone. It stores Model's parameters, matrices, run time, and other model features to simplify comparison.","b07f2c4d":"Analyse the amount of data in each class to determine if there's any imbalance.","f366b082":"# **Building our SKlearn Pipeline Model**","b4cc4cd6":"### **Word Count Boxplot**","f675fbaf":"We buld three different models\n* SKlearn Pipeline Model\n* Logistic Regression Model\n* Support Vector Marchine (SVM) Model","a3c429ca":"# **Log Model Features To Comet_ml** <a name='Log_model_features_to_Comet_ml'>","9389be38":"Build a Machine Learning Model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n\nWe are going to use the Nltk package to apply various NLP techniques, such as tokenization, POS tagging and lemmatization to build meaningful features from a raw Twitter dataset. With these features, we are going to experiment different learning algorithms to build a sentiment analysis model.","d950c9af":" # **Building the SVM Model** <a name='Building_the_SVM_model'>","1d1ee2c7":"# **Setting up Comet_ml** <a name=\"Setting_up_Comet_ml\"><\/a>","1c4fbd4f":"## Sentiment Analysis","01378046":"## **Fitting our data using SVM model**","cbafd1e9":"**Datatypes and summary statistics**","06cfcf70":"A word cloud is a good representation to show the popular words one has in their data. Below we ploted a word cloud showing the most popular words in our data.","f32d8a3b":"Acording to the sentiment analysis This tells we have majority of the tweets a clastured at the center. Which tells us that based onn the polarity a lot opf tweets are recognised as neutral tweets","092ef23d":"# **Introduction** <a name=\"introduction\"><\/a>","735ad055":"### **Dealing with Class Imbalance**","3e15bda5":"# Table of contents\n\n1. [Introduction](#introduction)\n2. [Problem Statement](#Problem_statement)\n3. [Setting up Comet_ml](#Setting_up_Comet_ml)\n4. [Importing Libraries](#Importing_libraries)\n5. [Import The Data](#Import_the_data)\n6. [Data Preprocessing](#Data_preprocessing)\n7. [Exploratory Data Analysis](#Exploratory_data_analysis)\n8. [Cleaning The Data](#Cleaning_the_data)\n9. [Feature Engeneering](#Feature_engeneering)\n10. [Model building and Selection](#Model_building_and_Selection)\n11. [Model Evaluation](#Model_Evaluation)\n12. [Model Reporting](#Model_Reporting)\n13. [Log Model Features To Comet_ml](#Log_model_features_to_Comet_ml)\n14. [Conclusion](#Conclusion)\n15. [References](#References)\n    ","26058730":"### Extracting websites from tweets <a name='Extracting_websites_from_tweets'>","e8f58ee6":"# Conclusion <a name='Conclusion'>","6163c60a":"we firstly did word cloud visualisation with stopwords,puctuations,urls and patterns removed. We noticed how words like Climate-Change , Global warming etc takes up a huge font meaning they were mostly mentioned in the tweet messages. We decided to remove those words using the function above to get a more distinct words within our tweet messages","048fcdf3":"This function resembles the data by making sure that each class has equal number of entries. This is done by recreating a class by picking at random from its entries until we have a desired class size. The class size is found by getting the midpoint from the lenth of a class with the least data and the lenth of a class with the most data. If a class has less data than the class size then random picking is done without replacement, if it is larger then random picking is done with replacement.","e17a5a08":"### **Word Clouds from Cleaned Data**","bc05fc47":"### **Word Cloud Per Class**","a326792f":"________________________________________________________________________________\n# Climate Change Tweet Classification Predict \n## Brought to you by team_3_jhb\n### Authors:\n- Itumeleng Ngoetjana\n- Noluthando Khumalo\n- Thavha Tsiwana\n- Pontsho Mokone \n- Tshokelo Mokubi\n\n![climate_pic.jpg](attachment:climate_pic.jpg)\n[[1](https:\/\/unsplash.com\/photos\/ycW4YxhrWHM)]\n________________________________________________________________________________","d51c4284":"# **Exploratory Data Analysis** <a name='Exploratory_data_analysis'>","db068353":"# References <a name='References'>","9c18f8b1":"the model seems to be perfoming better for the classes that has more messages","0f010c67":"Word clouds for each class Indeed we expected words like climate change and global warming to appear more often as these are the topics of the subjects, this gives an assurance that we obtained relevant tweets\/data to our topic","78b3be4e":"SVM (Support Vector Machines) uses the training data and plots the data as points in space, it will then find a separating gap between two clases and use this gap as a dividing line(hyperplane), the new data will be classified acording to this deviding line. we will first copy the data to use in our SVM model","62f0323e":"# Model building and Selection","39851910":"The pipeline class is one of sklearn classification estimators, it enables usage of various vectorizers such as tfidfvectorizer. This goes in combination with a fitting algorithm like support vector machine. The advantage of a pipeline estimator is that it automatically encoporates the preprocessing (vectorizing), fitting and transformation of the data in minimal lines of code.\n\n### A flow diagram of the tfidvectorizer\n\n![diagram.png](attachment:diagram.png)","a65037b9":" ### Accuracy -\nAccuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations [[4](https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/)]. \n\nUsing the formula:  $$ Accuracy = \\frac{Correct \\space predictions}{Total \\space predictions} = \\frac{TP \\space + TN}{TP \\space + TN \\space + FP \\space + FP} $$\n\n\n### Precision -\nPrecision is the ratio of correctly predicted positive observations to the total predicted positive observations [[4](https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/)]. \n\nUsing the formula: $$ Precision = \\frac{TP}{TP \\space + FP} = \\frac{TP}{Total \\space Predicted \\space Positive} $$\n\n\n### Recall (Sensitivity) - \nRecall is the ratio of correctly predicted positive observations to the all observations in actual class [[4](https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/)]. \n\nUsing the formula: $$ Recall = \\frac{TP}{TP \\space + FN} = \\frac{TP}{Total \\space Actual \\space Positive}$$\n\n### F1 score - \nF1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution [[4](https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/)].\n\nUsing the formula: $$F_1 = 2 \\times \\frac {Precision \\space \\times \\space Recall }{Precision \\space + \\space Recall }$$\n\n\n\n","d4238fd7":"###  Testing the Accuracy of the Model","80ba97e2":"# **Cleaning The Data** <a name='Cleaning_the_data'>","4d70b451":"# **Data Preprocessing**","382396fb":"The key take away from these word clouds is that each class has its own distinct predominant words(or phrases), broken down as follows:\n\n* News: \"Donald Trump\" \n* Neutral: \"People\" \n* Anti: \"Climate\"\n* Pro: \"Believe\" ","d232a24e":"### Splitting Train data and Assigning Test data","66e0be6d":"This table shows that we do not have null values in all our entries. ","ed554807":"## **Predicting the results of the provided testing set from our fitted SVM model**","7039c6fd":"This graph shows how punctuatiuation is used per class.The tweets from the News class (class 2) has the most punctuation use in the tweets, which shows how legitimate or factual take heed of puctuation the use of puntuation or grammar . We also notice how most tweets from the Anti class (class -1) make no use of punction. From this we can savely assume that most tweets that make less use of punctuation are mostly not factual.","144f88c3":"Thirty years ago, the potentially disruptive impact of global warming became front-page news world-wide[[2](https:\/\/www.nationalgeographic.com\/magazine\/2018\/07\/embark-essay-climate-change-pollution-revkin\/)]. This new reality affected the way in which people think about many large companies that contributes a lot to this phenomenon. In that reality, many large companies decided to take initiatives that would reduce their effect in global warming as well as improve their public image. These companies include Levi Strauss & Co, Siemens, Starbucks, and many other companies[[3](https:\/\/impacthub.net\/5-big-companies-reducing-their-carbon-footprints\/)]. This made companies become interested in how people perceive climate change and whether or not they believe it is a real threat.\n\nIn this project, we are building a classification machine learning model that classifies people's tweets about climate change into four different categories explained as follow:\n\n![image from actual competition page](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F2205222%2F8e4d65f2029797e0462b52022451829c%2Fdata.PNG?generation=1590752860255531&alt=media)\n\nWe aim to provide an accurate and robust model that gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n\n\n","70bd9aee":"This  graph shows the distribution of the word count frequency. It shows the relationship between the number of words in a tweet and it's class. From the graph we can see that the tweets from class News have a lenth from 5 to 25 and it's tweet lenth has normal distribution along mean 16. It also shows the same patten for the rest of the classes. With class Pro having work count lenths from 4 till 30, with more tweets around lenth 21. Class Neutral has lenths from 2 to 30, with most tweets around 20. And Class Anti has lenths from 4 to 30, with most tweets around 21. ","e61ebe4f":"from above we saw that our data have the matching shape so we will proceed to fitting the data using svm model\n","2bedeebc":"# Punctuation Count","a4d417db":"# **Import The Data** <a name=\"Import_the_data\"><\/a>","57b8d6c0":"[1] Lim, L.-A., 2019. Unsplash. [Online] \nAvailable at: https:\/\/unsplash.com\/photos\/ycW4YxhrWHM\n[Accessed 28 June 2020].\n\n[2] Revkin, A., 2018. Climate Change First Became News 30 Years Ago. Why Haven\u2019t We Fixed It?. [Online] \nAvailable at: https:\/\/www.nationalgeographic.com\/magazine\/2018\/07\/embark-essay-climate-change-pollution-revkin\/\n[Accessed 27 June 2020].\n\n[3] Hub, I., 2018. 5 Big Companies Reducing Their Carbon Footprints. [Online] \nAvailable at: https:\/\/impacthub.net\/5-big-companies-reducing-their-carbon-footprints\/\n[Accessed 27 June 2020].\n\n[4] Renuka, J 2016, Accuracy, Precision, Recall & F1 Score: Interpretation of Performance Measures ,  Exsilio Solutions, viewed 23 June 2020, https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/\n","bc8692d0":"### Analysing Stopwords\n","a41312e3":"SVM uses numerical data so the data will be converted to numerical data using CountVectorizer","59162a72":"# **Problem Statement** <a name=\"Problem_statement\"><\/a>\n","db91f965":"# **Building Our Logistic Regression Model** <a name='Building_our_logistic_regression_model'>","c3b1291c":"Saving the results to a csv file named SVM_Predictions","35876e6c":"### polarity and subjectivity","bdaa4d29":"# **Testing the Accuracy of the Model, by compering the predicted values vs tesing values**","f252ff23":"The boxplots of word count show distinct properties for each class. The presence of outliers, varying medians and range sizes imply that the word count property will add substantial value to model training. neautral class have the highest range which implies that people with neautrl beliefs either write long tweets justyfying both sides or simply refuse to take a side as opose to anti and pro classes where people will write avarge words only justfying their beliefs.\n"}}