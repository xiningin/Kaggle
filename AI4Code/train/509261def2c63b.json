{"cell_type":{"b6ed6c28":"code","61901e1c":"code","cb96cb79":"code","e1256ec8":"code","8dcc73bc":"code","ca12a10a":"code","85786a26":"code","aaca5b00":"code","abd6b746":"code","33810522":"code","e62df3e7":"code","5f32a255":"code","ee505c5e":"code","c78bc297":"code","cb5dd2b8":"code","b838f86e":"code","15e7fe13":"code","39532a00":"code","96f64046":"code","222737ca":"code","dfab4571":"code","7eecff80":"code","ec3ac6d8":"code","5ef0260d":"code","2509b2b6":"code","2abc7c49":"code","0280f6b5":"code","1bf2661d":"code","3b1071da":"code","bd9c00bb":"code","41725dfb":"code","cc14779a":"code","da366c57":"code","79a8b868":"code","c45e140a":"code","2979e754":"code","f4a2c17c":"code","9d121c4b":"code","6d91a0c2":"code","84c67fbd":"code","6536a6cc":"code","9a2cdeb7":"code","f8b32b7b":"code","345c31be":"code","8bf7f81a":"code","1418439f":"code","7430b01d":"code","ed1d286d":"code","819214c8":"code","df6439ab":"code","4cb1e60d":"code","9d569d8e":"code","7f2c267f":"code","2f1658ae":"code","48700c59":"code","c8eb8a2a":"code","1ac91922":"code","64be17e0":"markdown","fbfc6642":"markdown","4f45587a":"markdown","9848bb2a":"markdown","4c80db4f":"markdown","73d54d1a":"markdown","194b0bad":"markdown","815dcf46":"markdown","f3f95ef5":"markdown","5feedc59":"markdown","9336baa0":"markdown","b683540a":"markdown","cd7a4159":"markdown","258a5b94":"markdown","ea86278e":"markdown","f660beee":"markdown","ce9266cb":"markdown","61e47b4e":"markdown","f3d2c46b":"markdown","872cbabe":"markdown","1de0ab0f":"markdown","a085c0b2":"markdown","bde814d2":"markdown","2cd5cd84":"markdown","761da976":"markdown","ff2bbec0":"markdown","8c2f4baa":"markdown"},"source":{"b6ed6c28":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport warnings\nimport os \nwarnings.filterwarnings(\"ignore\")\nimport datetime\n","61901e1c":"data=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\n","cb96cb79":"data.head()      #displaying the head of dataset they gives the 1st to 5 rows of the data","e1256ec8":"data.describe()      #description of dataset ","8dcc73bc":"data.info()","ca12a10a":"data.shape       #569 rows and 33 columns","85786a26":"data.columns     #displaying the columns of dataset","aaca5b00":"data.value_counts","abd6b746":"data.dtypes","33810522":"data.isnull().sum()","e62df3e7":"data.drop('Unnamed: 32', axis = 1, inplace = True)\n","5f32a255":"data","ee505c5e":"data.corr()","c78bc297":"plt.figure(figsize=(18,9))\nsns.heatmap(data.corr(),annot = True, cmap =\"Accent_r\")\n\n\n\n","cb5dd2b8":"sns.barplot(x=\"id\", y=\"diagnosis\",data=data[160:190])\nplt.title(\"Id vs Diagnosis\",fontsize=15)\nplt.xlabel(\"Id\")\nplt.ylabel(\"Diagonis\")\nplt.show()\nplt.style.use(\"ggplot\")\n","b838f86e":"sns.barplot(x=\"radius_mean\", y=\"texture_mean\", data=data[170:180])\nplt.title(\"Radius Mean vs Texture Mean\",fontsize=15)\nplt.xlabel(\"Radius Mean\")\nplt.ylabel(\"Texture Mean\")\nplt.show()\nplt.style.use(\"ggplot\")\n","15e7fe13":" \nmean_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nsns.pairplot(data[mean_col],hue = 'diagnosis', palette='Accent')\n","39532a00":"sns.violinplot(x=\"smoothness_mean\",y=\"perimeter_mean\",data=data)","96f64046":"plt.figure(figsize=(14,7))\nsns.lineplot(x = \"concavity_mean\",y = \"concave points_mean\",data = data[0:400], color='green')\nplt.title(\"Concavity Mean vs Concave Mean\")\nplt.xlabel(\"Concavity Mean\")\nplt.ylabel(\"Concave Points\")\nplt.show()\n\n","222737ca":"worst_col = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\n\nsns.pairplot(data[worst_col],hue = 'diagnosis', palette=\"CMRmap\")","dfab4571":"# Getting Features\n\nx = data.drop(columns = 'diagnosis')\n\n# Getting Predicting Value\ny = data['diagnosis']\n","7eecff80":"\n#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n","ec3ac6d8":"print(len(x_train))\n","5ef0260d":"print(len(x_test))","2509b2b6":"print(len(y_train))","2abc7c49":"print(len(y_test))","0280f6b5":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)                         \n","1bf2661d":"y_pred=reg.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",reg.score(x_train,y_train)*100)\n\n\n","3b1071da":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n\n\n\n\n","bd9c00bb":"print(accuracy_score(y_test,y_pred)*100)","41725dfb":"from sklearn.model_selection import GridSearchCV\nparam = {\n         'penalty':['l1','l2'],\n         'C':[0.001, 0.01, 0.1, 1, 10, 20,100, 1000]\n}\nlr= LogisticRegression(penalty='l1')\ncv=GridSearchCV(reg,param,cv=5,n_jobs=-1)\ncv.fit(x_train,y_train)\ncv.predict(x_test)\n","cc14779a":"print(\"Best CV score\", cv.best_score_*100)","da366c57":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123)\n\ndtree.fit(x_train,y_train)\n\n#y_pred = dtree.predict(x_test)\n","79a8b868":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",dtree.score(x_train,y_train)*100)\n\n","c45e140a":"print(accuracy_score(y_test,y_pred)*100)","2979e754":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)\n\n","f4a2c17c":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",rfc.score(x_train,y_train)*100)\n","9d121c4b":"print(accuracy_score(y_test,y_pred)*100)","6d91a0c2":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train,y_train)\n","84c67fbd":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",knn.score(x_train,y_train)*100)\nprint(knn.score(x_test,y_test))\n","6536a6cc":"print(accuracy_score(y_test,y_pred)*100)\n","9a2cdeb7":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\n","f8b32b7b":"y_pred=svc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",svc.score(x_train,y_train)*100)\nprint(svc.score(x_test,y_test))\n","345c31be":"print(\"Training Score: \",svc.score(x_train,y_train)*100)","8bf7f81a":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nadb.fit(x_train,y_train)\n\n\n\n\n\n","1418439f":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",adb.score(x_train,y_train)*100)","7430b01d":"print(accuracy_score(y_test,y_pred)*100)","ed1d286d":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\n","819214c8":"y_pred=gbc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gbc.score(x_train,y_train)*100)\nprint(gbc.score(x_test,y_test))\n","df6439ab":"print(accuracy_score(y_test,y_pred)*100)","4cb1e60d":"from xgboost import XGBClassifier\n\nxgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxgb.fit(x_train, y_train)\n","9d569d8e":"y_pred=xgb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",xgb.score(x_train,y_train)*100)\nprint(xgb.score(x_test,y_test))\n","7f2c267f":"print(\"Training Score: \",xgb.score(x_train,y_train)*100)","2f1658ae":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","48700c59":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)","c8eb8a2a":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(\"Training Score: \",gnb.score(x_train,y_train)*100)\nprint(gnb.score(x_test,y_test))\n","1ac91922":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","64be17e0":"**So we get a accuracy score of 97.80 % using  XGBClassifier**","fbfc6642":"**So we get a accuracy score of 58.7 % using logistic regression**","4f45587a":"**So we get a accuracy score of 94.73 % using Decision Tree Classifier**","9848bb2a":"**So we get a accuracy score of 96.49 % using Random Forest Classifier**","4c80db4f":"**So now we conclude the accuracy of different models:**\n\n**1. AdaBoost Classifier = 98.24 %**\n\n**2. XGB Classifier= 97.84 %**\n\n**3. Random Forest Classifier =96.57 %**\n\n**4. Gradient Boosting Classifier= 95.66%**\n\n**5. Decision Tree Classifier= 94.78 %**\n\n**6. K Neighbours Classifier= 70.18 %**\n\n**7. SVC = 63.80 %**\n\n**8. Naiye Bayes= 63.30 %**\n\n**9. Logistic Regression = 58.82%**\n","73d54d1a":"**So we get a accuracy score of 63.7 % using SVC**","194b0bad":"# 9. Naive Bayes","815dcf46":"# IMPORTING THE LIBRARIES","f3f95ef5":"# If you liked this notebook, please UPVOTE it.","5feedc59":"**So we get a accuracy score of 63.29 % using Naive Bayes**","9336baa0":"# 2. DECISION TREE CLASSIFIER","b683540a":"# 8. XGBClassifier","cd7a4159":"**So we have to drop the Unnamed: 32 coulumn which contains NaN values**","258a5b94":"#  7. Gradient Boosting Classifier","ea86278e":"# MODELS","f660beee":"**So we get a accuracy score of 98.24 % using AdaBoostClassifier**","ce9266cb":"# 6. AdaBoostClassifier","61e47b4e":"# 3. Random Forest Classifier","f3d2c46b":"**So we get a accuracy score of 95.61 % using GradientBoostingClassifier**","872cbabe":"# 1. Logistic Regression","1de0ab0f":"# 5. SVC","a085c0b2":"# 4. KNeighborsClassifier\n\n","bde814d2":"**Ada Boost Classifier got the highest accuracy**","2cd5cd84":"# TRAINING AND TESTING DATA","761da976":"# VISUALIZING THE DATA","ff2bbec0":"# LOADING THE DATASET","8c2f4baa":"**So we get a accuracy score of 70.17 % using KNeighborsClassifier**"}}