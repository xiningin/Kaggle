{"cell_type":{"a884b2b8":"code","8ea002e7":"code","2fd67c56":"code","2caa47cf":"code","884e04e0":"code","8b529ba4":"code","576c40c1":"code","c7f87290":"code","b3e80775":"code","eb8cd84a":"code","63532cf5":"code","4a0e6f03":"code","c1ed2cc6":"code","d4e2ee63":"code","2b9b5063":"code","49562a13":"code","5730eabb":"code","6de16bee":"code","cadc01a5":"code","7567aa4c":"code","c38e5186":"code","080c8e03":"code","d6c68a2e":"code","a4738738":"code","1b3ae147":"code","c2bf3a79":"code","73c08e9c":"code","7ef2a8dd":"code","f9edc3e9":"code","ef64b7a6":"code","a8dd1c62":"code","f4609798":"code","857af12f":"code","aa9ddc64":"code","54752cdd":"code","e358f522":"code","db16e1c9":"code","d364cb8c":"code","ffa41823":"code","8a94991f":"code","c4aca005":"code","d288ac8e":"code","60a92205":"markdown","4005aaee":"markdown","0c83d960":"markdown","e209aa0f":"markdown","830b6c79":"markdown","d80ecc1f":"markdown","a24ab81f":"markdown","bfd41d0e":"markdown","c0ff8c1b":"markdown","037f1615":"markdown","db8fdab0":"markdown","5f12772e":"markdown","ad6f71bd":"markdown","2498ccef":"markdown","b7e451e2":"markdown","824d77d4":"markdown","bb092675":"markdown","ea47dfa7":"markdown","15d058c5":"markdown","6779b32e":"markdown","d49cd9cd":"markdown","96f5dcf9":"markdown","3e43f8a8":"markdown"},"source":{"a884b2b8":"%pylab inline\nfrom matplotlib.pyplot import style\nstyle.use('https:\/\/raw.githubusercontent.com\/JoseGuzman\/minibrain\/master\/minibrain\/paper.mplstyle')\nimport pandas as pd\n","8ea002e7":"data = pd.read_csv('..\/input\/advertising-data\/Advertising.csv', index_col=0)\n#data = pd.read_csv('Advertising.csv', index_col=0)\ndata.head()\nprint(f'The dataset contains {data.shape[0]} observations')\n","2fd67c56":"# Simple linear regression with sklearn\nfrom sklearn.linear_model import LinearRegression","2caa47cf":"LRmodel = LinearRegression() # y = b + wX\nsales = data.Sales # dependent variable","884e04e0":"# Plot the linear regression for every single independent variable \nfig, ax = plt.subplots(1,3, figsize=(12,3), sharey=True)\nfig.tight_layout()\n\n# tv is TV investment is a column vector\ntv = data.loc[:,['TV']]\n\nLRmodel.fit(tv, sales)\n\nax[0].scatter(tv,sales, s=4, c='red', alpha = 0.3)\nx = np.linspace(0, tv.max(), num = 200)\nax[0].plot(x, LRmodel.predict(x), color = 'brown')\nax[0].set_xlabel('TV Investment ($)'), ax[0].set_ylabel('Sales ($)')\nprint(f'Intercept {LRmodel.intercept_:5.2f}, slope {LRmodel.coef_[0]:5.2f}')\nax[0].text(x = 0, y =25 , s=f'y = {LRmodel.intercept_:5.2f} + {LRmodel.coef_[0]:5.2f}x', color='brown')\n\n# X is Radio investment\nradio = data.loc[:,['Radio']]\n\nLRmodel.fit(radio, sales)\n\nax[1].scatter(radio,sales, s=4, c='C0', alpha = 0.5)\nx = np.linspace(0, radio.max(), num = 200)\nax[1].plot(x, LRmodel.predict(x), color = 'C0')\nax[1].set_xlabel('Radio Investment ($)')\nprint(f'Intercept {LRmodel.intercept_:5.2f}, slope {LRmodel.coef_[0]:5.2f}')\nax[1].text(x = 0, y =25 , s=f'y = {LRmodel.intercept_:5.2f} + {LRmodel.coef_[0]:5.2f}x', color='C0');\n\n# X is Newspaper investment\nnews = data.loc[:,['Newspaper']]\n\nLRmodel.fit(news, sales)\n\nax[2].scatter(news,sales, s=4, c='green', alpha = 0.5)\nx = np.linspace(0, news.max(), num = 200)\nax[2].plot(x, LRmodel.predict(x), color = 'green')\nax[2].set_xlabel('Newspaper Investment ($)')\nprint(f'Intercept {LRmodel.intercept_:5.2f}, slope {LRmodel.coef_[0]:5.2f}')\nax[2].text(x = 0, y =25 , s=f'y = {LRmodel.intercept_:5.2f} + {LRmodel.coef_[0]:5.2f}x', color='darkgreen');\n\n\nfor myax in ax:\n    #myax.set_ylabel('Sales ($)')\n    myax.set_yticks(np.arange(0,30,5))","8b529ba4":"import statsmodels.api as sm","576c40c1":"# Simple linear regression with ordinary least squares with statmodels\nx = data.loc[:,['TV']] # DataFrame\ny = data['Sales']\n\n## fit a OLS model with y = b + w*TV\nx = sm.add_constant(x)\nest = sm.OLS(y, x).fit()\nest.summary()","c7f87290":"print(f' y = {est.params[0]:2.4f} + {est.params[1]:2.4}x')","b3e80775":"# Similarly with formula y = TV + Radio + Newspaper\nimport statsmodels.formula.api as smf\n\nest = smf.ols(formula=\"Sales ~ TV + Radio + Newspaper\", data=data).fit()\nest.summary()","eb8cd84a":"# for p-values < 0.05\nest.pvalues[est.pvalues<=0.05]","63532cf5":"# with pytorch\n\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm # progress bar\n\ntorch.manual_seed(42); # set seed for reproducibility of results","4a0e6f03":"tv = torch.tensor( data = data.TV.values, dtype = torch.float ) # x_1 \nradio = torch.tensor( data = data.Radio.values, dtype = torch.float) # x_2\nnews = torch.tensor( data = data.Newspaper.values, dtype = torch.float) # x_3\n\nsales = torch.tensor( data = data.Sales.values, dtype = torch.float ) # targets","c1ed2cc6":"a = torch.randn(1, requires_grad = True)  # start with a random number from a normal distribution\nb = torch.randn(1, requires_grad = True)\nc = torch.randn(1, requires_grad = True)\nd = torch.randn(1, requires_grad = True)","d4e2ee63":"def mylnmodel( tv:torch.Tensor, radio:torch.Tensor, news:torch.Tensor):\n    \"\"\"\n    computes f(x; a,b,c) = a + bx_1 + cx_2 + dx_3, \n    for independent variables x_1, x_2 and x_3.\n    \n    Arguments:\n    tv (tensor) with the values of tv investment (x_1)\n    radio (tensor) with the values of radio investment (x_2)\n    news (tensor) with the newspaper investment (x_3).\n    \n    Note: coefficients a, b, c and d must be previoulsy \n    defined as tensors with requires_grad = True\n    \n    Returns a tensor with the backward() method\n    \"\"\"\n    return a + b*tv + c*radio + d*news","2b9b5063":"# generate the first prediction\npredicted = mylnmodel(tv, radio, news)\npredicted.shape","49562a13":"# compare it with targets\nsales.shape","5730eabb":"plt.figure(figsize=(3,3))\nplt.scatter(sales, predicted.detach(), c='k', s=4)\nplt.xlabel('sales'), plt.ylabel('predicted');\nx = y = range(100)\nplt.plot(x,y, c='brown')\nplt.xlim(0,100), plt.ylim(0,120);\nplt.text(60,50, f'a     = {a.item():2.4f}', fontsize=10);\nplt.text(60,40, f'tv    = {b.item():2.4f}', fontsize=10);\nplt.text(60,30, f'radio = {c.item():2.4f}', fontsize=10);\nplt.text(60,20, f'news  = {d.item():2.4f}', fontsize=10);","6de16bee":"def MSE(y_predicted:torch.Tensor, y_target:torch.Tensor):\n    \"\"\"\n    Returns a single value tensor with \n    the mean of squared errors (SSE) between the predicted and target\n    values:\n    \n    \"\"\"\n    error = y_predicted - y_target # element-wise substraction\n    return torch.sum(error**2 ) \/ error.numel() # mean (sum\/n)","cadc01a5":"predicted = mylnmodel(tv,radio,news)\nloss = MSE(y_predicted = predicted, y_target=sales)\nprint(loss) # 401.0395","7567aa4c":"# initial values for the coefficients is random, gradients are not calculated\nprint(f'a = {float(a.item()):+2.4f}, df(a)\/da = {a.grad}') # 0.3367\nprint(f'b = {float(b.item()):+2.4f}, df(b)\/da = {a.grad}') # 0.1288\nprint(f'c = {float(c.item()):+2.4f}, df(c)\/dc = {c.grad}') # 0.2345\nprint(f'd = {float(d.item()):+2.4f}, df(d)\/dd = {d.grad}') # 0.2303\n\n","c38e5186":"loss.backward()","080c8e03":"# initial values for the coefficients is random, gradients are now calculated\nprint(f'a = {float(a.item()):+2.4f}, df(a)\/da = {a.grad}') # 0.3367\nprint(f'b = {float(b.item()):+2.4f}, df(b)\/da = {a.grad}') # 0.1288\nprint(f'c = {float(c.item()):+2.4f}, df(c)\/dc = {c.grad}') # 0.2345\nprint(f'd = {float(d.item()):+2.4f}, df(d)\/dd = {d.grad}') # 0.2303","d6c68a2e":"## Use gradiendt descent\nmyMSE = list()\nfor i in tqdm(range(5_000)):\n    a.grad.zero_()\n    b.grad.zero_()\n    c.grad.zero_()\n    d.grad.zero_()\n    \n    predicted = mylnmodel(tv,radio,news) # forward pass (compute results)\n    loss = MSE(y_predicted = predicted, y_target = sales) # calculate MSE\n    \n    loss.backward() # compute gradients\n    myMSE.append(loss.item()) # append loss\n    with torch.no_grad():\n        a -= a.grad * 1e-6\n        b -= b.grad * 1e-6\n        c -= c.grad * 1e-6\n        d -= d.grad * 1e-6\n        \nplt.plot(myMSE);\nplt.xlabel('Epoch (#)'), plt.ylabel('Mean squared Errors')\n    ","a4738738":"plt.figure(figsize=(3,3))\nplt.scatter(sales, predicted.detach(), c='k', s=4)\nplt.xlabel('sales'), plt.ylabel('predicted');\nx = y = range(30)\nplt.plot(x,y, c='brown')\nplt.xlim(0,35), plt.ylim(0,35);\nplt.text(25, 15, f'a     = {a.item():2.4f}', fontsize=8)\nplt.text(25, 12, f'tv    = {b.item():2.4f}', fontsize=8)\nplt.text(25, 9,  f'radio = {c.item():2.4f}', fontsize=8)\nplt.text(25, 6,  f'news  = {d.item():2.4f}', fontsize=8)\n#plt.text(25, 12, f'tv = {mymodel.w[0].item():2.4f}', fontsize=8)\n#plt.text(25, 9, f'radio = {mymodel.w[1].item():2.4f}', fontsize=8)\n#plt.text(25, 6, f'newspaper = {mymodel.w[2].item():2.4f}', fontsize=8)","1b3ae147":"torch.manual_seed(42) # set the seed againg just in case you modified previous cells.\n# independent variables (X) and dependent variable (y)\nX = torch.tensor(data.loc[:,['TV','Radio','Newspaper']].values)\ny = torch.tensor(data.Sales.values)\nX.shape","c2bf3a79":"w = torch.rand(3, dtype=torch.double, requires_grad = True)\nb = torch.rand(1, dtype=torch.double, requires_grad = True)\nw, b","73c08e9c":"def model(X:torch.Tensor):\n    \"\"\"\n    Performs the matrix vector multiplication\n    \"\"\"\n    assert len(X.shape) == 2\n    \n    return X @ w.T + b","7ef2a8dd":"predicted = model(X)\nloss = MSE(y_predicted = predicted, y_target=y)\nprint(loss) # www.1422","f9edc3e9":"## Use gradiendt descent\nmyMSE = list()\nfor i in tqdm(range(10_000)):\n\n\n    predicted = model(X) # forward pass (compute results)\n    loss = MSE(y_predicted = predicted, y_target = y) # calculate MSE\n    \n    loss.backward() # compute gradients\n    myMSE.append(loss) # append loss\n    with torch.no_grad():\n        w -= w.grad * 1e-6\n        b -= b.grad * 1e-6\n    w.grad.zero_()\n    b.grad.zero_()\n\n        \nplt.plot(myMSE);\nplt.xlabel('Epoch (#)'), plt.ylabel('Mean squared Errors')","ef64b7a6":"b, w","a8dd1c62":"myoptimizer = torch.optim.Adam(params = [b,w], lr = 0.005)\n","f4609798":"myMSE = list()\nfor epoch in tqdm(range(5_000)):\n    myoptimizer.zero_grad() # 1. Set gradients to zero\n    predicted = model(X) # 2. compute prediction\n    loss = MSE(y_predicted = predicted, y_target = y) # 3. compute MSE\n    myMSE.append(loss)\n    loss.backward() # 4. compute the gradient based on previous\n    myoptimizer.step() # 5. myoptimizer updates parameters\n    #print(f'epoch = {epoch:02d}, loss = {current_loss}, a = {w}, b = {b}') # 6.\n\nplt.plot(myMSE);\nplt.xlabel('Epoch (#)'), plt.ylabel('Mean squared Errors')","857af12f":"w,b","aa9ddc64":"torch.manual_seed(42) # set a seed\nclass MultipleRegression(nn.Module):\n    \n    def __init__(self, n_weight:int):\n        super(MultipleRegression, self).__init__()\n        \"\"\"Initializing a Multiple regression model of the form:\n        \n    \n        Args:\n          n_weight (int): number of weigths\n  \n        \"\"\"\n        assert isinstance(n_weight, int)\n        self.w = torch.randn(size =(n_weight,1), dtype=torch.double, \n                             requires_grad=True)\n        self.b = torch.randn(1, requires_grad=True)\n\n    def forward(self, X:torch.Tensor):\n        \"\"\"Forward pass for\n        y = X * w.T + b\n\n        Args:\n          X (torch.Tensor): 2D tensor of observations x features\n\n        Returns:\n          torch.Tensor: model predictions\n        \"\"\"\n        assert isinstance(X, torch.Tensor)\n        prediction = X @ self.w + self.b\n        return prediction","54752cdd":"mymodel = MultipleRegression(n_weight = 3)\nmymodel.b, mymodel.w","e358f522":"prediction = mymodel(X)\nMSE(y_predicted = prediction, y_target = y.reshape(-1,1))","db16e1c9":"# Import nn.functional\nimport torch.nn.functional as F\nF.mse_loss(prediction, y.reshape(-1,1))","d364cb8c":"#myoptimizer = torch.optim.SGD(params = [mymodel.b,mymodel.w], lr = 1e-9, momentum =0.9)\nmyoptimizer = torch.optim.Adam([mymodel.b, mymodel.w], lr = 0.005)","ffa41823":"myMSE = list()\nfor epoch in tqdm(range(5_000)):\n    myoptimizer.zero_grad() # 1. Set gradients to zero\n    predicted = mymodel(X) # 2. compute prediction (forward method)\n    loss = F.mse_loss(predicted, y.reshape(-1,1)) # 3. compute MSE\n    myMSE.append(loss)\n    loss.backward() # 4. compute the gradient based on previous\n    myoptimizer.step() # 5. myoptimizer updates parameters\n    #print(f'epoch = {epoch:02d}, loss = {current_loss}, a = {w}, b = {b}') # 6.\n\n\nplt.plot(myMSE);\nplt.xlabel('Epoch (#)'), plt.ylabel('Mean squared Errors');","8a94991f":"mymodel.w, mymodel.b","c4aca005":"tv, radio, news = mymodel.w.T.tolist()[0]","d288ac8e":"plt.figure(figsize=(3,3))\nplt.scatter(sales, mymodel(X).detach(), c='gray', s=4)\nplt.xlabel('sales'), plt.ylabel('predicted');\nx = y = range(30)\nplt.plot(x,y, c='brown')\nplt.xlim(0,35), plt.ylim(0,35);\nplt.text(25, 15, f'b = {mymodel.b.item():2.4f}', fontsize=8)\nplt.text(25, 12, f'tv = {tv:2.4f}', fontsize=8)\nplt.text(25, 9, f'radio = {radio:2.4f}', fontsize=8)\nplt.text(25, 6, f'newspaper = {news:2.4f}', fontsize=8)\n","60a92205":"Photo by <a href=\"https:\/\/unsplash.com\/@roman_lazygeek?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Roman Mager<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/mathematic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash<\/a>\n  ","4005aaee":"From these linear values, only TV and Radio are statistically significant","0c83d960":"From the previous expression:\n\n$$y= b + w_1x_1 + w_2x_2 + w_3x_3$$\n\nwhere $x_0$, $x_1$ and $x_2$ are TV, Radio and Newspaper investments, and $w_0$, $w_1$ and $w_2$  are the coefficients for TV, Radio and Newspaper investments respectively.\n\nwe define $w_1$, $w_2$, $w_3$  and $b$ are going to be PyTorch trainable variables, and x_1, x_2 and x_3, the TV, Radio and Newspaper investment respectively.","e209aa0f":"<a id=\"section5\"><\/a>\n# 5. Multi-variate linear regression with PyTorch\n\nPyTorch also allow us to perform linear regressions. We will start from scratch a multiple regression to predict sales.","830b6c79":"On-line resources\n* [Multiple regression using statsmodels](https:\/\/www.datarobot.com\/blog\/multiple-regression-using-statsmodels\/)\n* [Hypothesis testing for linear regression](https:\/\/www.kaggle.com\/reighns\/hypothesis-testing-for-linear-regression\/comments)\n* [Multi variable regression with PyTorch](https:\/\/donaldpinckney.com\/books\/pytorch\/book\/ch2-linreg\/2018-03-21-multi-variable.html)","d80ecc1f":"<a id=\"section3.1\"><\/a>\n## 3.1 Uni-variate simple linear regression\n\nWe will perform a simple linear regression for everyone of the variables independently.\n","a24ab81f":"<a id=\"section3\"><\/a>\n# 3. Simple linear regression with sklearn\n\nWe will start wit a fit to the simple equation of the form:\n\n$$f(x;b,w) = b + wx$$\n\nwhere the independent variable ($x$) is the TV investment. The parameters are $w$ - the linear coefficient for that investment -  and $b$ the bias term (or intercept). \n","bfd41d0e":"<a id=\"section4.1\"><\/a>\n## 4.1 Uni-variate simple linear regression ","c0ff8c1b":"# Detailed Multiple regression with PyTorch","037f1615":"The important thing here is that we can see if the indepedent variables are statistically significative to predict sales. If we take into account only coefficients with probability of being zero less than 5%, only TV and radio have a significant effect.","db8fdab0":"# Table of Contents\n\n* [1. Introduction and notation](#section1)\n* [2. Loading the Advertising dataset](#section2)\n* [3. Simple linear regression with sklearn ](#section3)\n    - [3.1 Uni-variate simple linear regression](#section3.1)\n* [4. Linear regression with statmodels](#section4)\n    - [4.1 Uni-variate simple linear regression](#section4.1)\n    - [4.2 Multivariate linear regression](#section4.2)\n* [5. Linear regression with PyTorch](#section5)\n    - [5.1 Redefine multi-variable linear operation in matrix form](#section5.1)\n    - [5.2 Defining with a custom network class](#section5.2)\n    ","5f12772e":"We will define the minimization function","ad6f71bd":"<a id=\"section4\"><\/a>\n# 4. Linear regression with statmodels\n\nStatsmodels allow us to evaluate the statistical significance of the parameters used in the linear regression.","2498ccef":"Before performing the minimization, the model predicts badly sales because it only starts at random values of\nthe TV, Radio and newspapers coefficients. The red line is the ideal prediction.","b7e451e2":"![math.jpeg](attachment:math.jpeg)","824d77d4":"<a id=\"section1\"><\/a>\n# 1. Introduction and notation\n\nWe can define a function as the relation between an independent variable (i.e., a feature $x$) and the dependent variable (i.e., label) through the parameters (e.g. $b$ and $w$). We write it in this form: $f(x;b,w)$. This function is linear because the response (i.e., the dependent variable) increases or decreases monotonically as it does the indepedent variable ($x$). For example, if we write a linear relation between our sales ($y$) and the number of ads we have in TV ($x$) as a function of $b$ and $w$ parameters. \n\n$$ y = b + wx$$\n\nWe assume that sales ($y$) will increase or decreases proportionally ($w$) to the value of TV ($x$) plus an minimum value ($b$) in sales that we have when no TV investment is given ($x=0$).\n\nWhen we talk about linear regression, we want to map the relationship to estimate $b$ and $w$ parameters. When knowing the parameters, it will allow us to make future predictions when we make new observations (i.e., new $x$ values for TV investments). \n\nThe regression methods can be applied between one (or more) independent variable(s) (i.e., features) and one (or more) dependent variable(s) (i.e., labels). If the regression contains more than one independent variables, we call the indepedent variables predictors, then the method is called **multiple regression**. If the regression contains more than one depedent variable (or labels), it is called **multi-variate regression**. We will test the *uni-variate multiple regression* method for simplicity. Here, the combination of m independent variables ($x_m$) will lead to one single dependent variable ($y$).\n\n$$y = b +  w_1x_1 + w_2x_2 + w_3x_3 + ...+w_m x_m $$\n\nNote that in this case, we will need a new parameter ($w_i$) for every new (i-th) indepedent variable ($x_i$). In addition, we generally take several observations for every indepedent and dependent variable. If we obtain $j$ observations, we can have an expression \n\n$$y^{(j)} = b^{(j)} +  w_1x_1^{(j)} + w_2x_2^{(j)} + w_3x_3^{(j)} + ...+w_m x_m^{(j)} $$\n\nThen we have j-observations (from 1 to n) and i- independent variables (from 1 to m). We can re-write it all in an explicit form:\n\nThus, if we have m-independent variables ($x_m$) and have n-observation per variable, we will have:\n\n$$\\begin{bmatrix} \n    y^1  \\\\ y^2 \\\\ y^3 \\\\ \\vdots \\\\y^n \\end{bmatrix}\n    _{n \\times 1} = \n\\begin{bmatrix} \n    x_1^1 & x_2^1 & \\cdots & x_m^1 \\\\\n                x_1^2 & x_2^2 & \\cdots & x_n^2 \\\\ \n                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n                x_1^n & x_2^n & \\cdots & x_m^n \\end{bmatrix}_{n \\times m} \n\\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ \\vdots \\\\ w_m\\end{bmatrix}_{ m \\times 1} + \n\\begin{bmatrix} b^1 \\\\ b^2  \\\\ b^3 \\\\ \\vdots \\\\ b^n \\end{bmatrix}_{n \\times 1}$$\n\n\n\nThan can be simply describe the function in matrix from:\n\n$$\\hat{Y} = \\sum \\limits _{i} ^{m} X_{i}{W}_{i} $$\n\n\nwhere m is the number of independent variables. \n\nTo obtain the parameters of the linear function we use the Ordinary Least Squares (OLS) regression method. Briefly explained, the OLS minimizes the the difference between  observed values ($Y$) and predicted values for a given set of parameters ($\\hat{Y}$). To keep these differences positive, they are raised to the square. For $n$-observations we would have the following minization function:\n\n$${\\mathrm{argmin}}\\sum \\limits_{i=1}^{m}(\\hat{Y^i}-Y^i)^2$$\n\nWe can also think of the OLS regression as the fitting of the independent variables with the coefficients in a n-dimensional hyperplane.\n\n\n","bb092675":"<a id=\"section5.1\"><\/a>\n## 5.1 Redefine multi-variable linear operation in matrix form\n\nWe will formulate the multivarite linear regression in matrix form as:\n$$\\hat{Y} = \\sum \\limits _{j} ^{m} X_{i}{W}_{i} $$","ea47dfa7":"This notebook contains several ways to perform regression to several indepedent variables \n(multiple regression). It uses a simple dataset to test the code and compare the parameters\nobtained with different methods.\n\nIt also contains some math to understand the notation, but also the code to perform multiple regression\nwith sklearn and PyTorch.","15d058c5":"<a id=\"section4.2\"><\/a>\n## 4.2 Multi-variate linear regression\n\nThe simple linear regression is useful, but we want to analyze the effect of all the indepenent variables together (i.e. TV, radio and newspapers) on sales. Now we will fit the folowing expression:\n\n$$y= b + w_1x_1 + w_2x_2 + w_3x_3$$\n\nwhere $w_0$, $w_1$ and $w_2$  are the coefficients for TV, Radio and Newspaper investments respectively.","6779b32e":"We need to define the linear model as a function of the independent variables and the coefficients by a simple multiplication.","d49cd9cd":"<a id=\"section2\"><\/a>\n# 2. Loading the advertising dataset\n\nThe dataset contains 200 observations. It contains investments in TV, radio and newspapers (i.e., the indepedent variables, in dollars) and the resulting sales (or dependent variable). The idea is to find the parameters to predict the sales based on different different values of TV, radio and newspaper investments.","96f5dcf9":"a, b, c and d are the coefficients to update during training. PyTorch will compute the gradients of the \nloss function with respect them. For that, we have to tell PyTorch to keep track of gradients by setting requires_grad=True. \n","3e43f8a8":"<a id=\"section5.2\"><\/a>\n## 5.2 Defining with a custom network class"}}