{"cell_type":{"c786d54f":"code","abb6c6bd":"code","1f9ee9b5":"code","5f6d409b":"code","58e6a219":"code","5997c92f":"code","2c763728":"code","6da74d3e":"code","3cdb5038":"code","43615080":"code","b0d66bf0":"code","f3c992ae":"code","6a15f48d":"code","9648916a":"code","cbc9cf18":"code","8b15b658":"code","f8f8759d":"code","f234cacf":"code","d6f083e3":"code","8ae6daca":"code","028dea3b":"code","08b47307":"code","1675b1a7":"code","36da9872":"code","3ae70416":"code","9e00ac7d":"code","f12fac9b":"code","1d2268d2":"code","623a6290":"code","77eef007":"code","c264ba68":"code","69688459":"code","0450c9bb":"code","bb17c91f":"code","f59d116f":"code","f5705791":"code","a4eb8f1c":"code","caba9b9f":"code","944a3dbf":"code","2e8d62de":"code","020d484f":"code","cca99676":"code","3da8409e":"code","922f1585":"code","b133b4d3":"code","fe6fbae4":"code","1758d0a5":"code","d042c5ca":"code","537943be":"markdown","f359c97f":"markdown","91bf9c2f":"markdown","1e4fe4d1":"markdown","cb7a0489":"markdown","615ebb53":"markdown","08c0f561":"markdown","06f67ca1":"markdown","3c52188c":"markdown","7d6fe96a":"markdown","2ab34b3b":"markdown","18297564":"markdown","188a0c44":"markdown","d62efa66":"markdown","1971ab9c":"markdown","3da25956":"markdown","99a16678":"markdown","3ddaad9d":"markdown","f93e5cd5":"markdown","f4be2750":"markdown","c129c783":"markdown","67cd2400":"markdown","586dca4f":"markdown","c3b6a538":"markdown","270bf4b1":"markdown","da444661":"markdown","23240f62":"markdown","5397d69c":"markdown","7c0072a6":"markdown"},"source":{"c786d54f":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nimport re \nimport scipy\nfrom scipy import sparse\nimport gc \n\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \n\nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n","abb6c6bd":"def timer(func):\n    def wrapper(*args, **kws):\n        st = time.time()\n        res = func(*args, **kws)\n        et = time.time()\n        tt = (et-st)\/60\n        print(f'Time taken is {tt:.2f} mins')\n        return res\n    return wrapper\n","1f9ee9b5":"df_test = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv\")\ndf_test_l = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv\").replace(-1,0)\nprint(df_test.shape)\ndf_test = pd.merge(df_test, df_test_l, how=\"left\", on = \"id\")\ndf_test.shape","5f6d409b":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\nprint(df.shape)\ndf = pd.concat([df, df_test])\nprint(df.shape)\ndel df_test\n\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(f'****** {col} *******')\n    display(df.loc[df[col]==1,['comment_text',col]].sample(5))","58e6a219":"\n# Give more weight to severe toxic \ndf['severe_toxic'] = df.severe_toxic * 2\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf['y'] = df['y']\/df['y'].max()\n\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.sample(5)","5997c92f":"df['y'].value_counts()","2c763728":"# Validation data \n\ndf_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nprint(df_val.shape)\n\n\n# Find cases already present in toxic data\n\ndf_val = pd.merge(df_val, df.loc[:,['text']], \n                  left_on = 'less_toxic', \n                  right_on = 'text', how='left')\n\ndf_val = pd.merge(df_val, df.loc[:,['text']], \n                  left_on = 'more_toxic', \n                  right_on = 'text', how='left')\n\n# Removing those cases\ndf_val = df_val[(~df_val.text_x.isna()) | (~df_val.text_y.isna())][['worker', 'less_toxic', 'more_toxic']]\ndf_val.shape","6da74d3e":"n_folds = 2\n\nfrac_1 = 0.7\nfrac_1_factor = 1.3\n","3cdb5038":"@timer\ndef create_folds():\n    for fld in range(n_folds):\n        print(f'Fold: {fld}')\n        tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                            df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                                                random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n\n        tmp_df.to_csv(f'\/kaggle\/working\/df_fld{fld}.csv', index=False)\n        print(tmp_df.shape)\n        print(tmp_df['y'].value_counts())\n\n\ncreate_folds()","43615080":"@timer\ndef clean(data, col):\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    # Remove ip address\n    data[col] = data[col].str.replace(r'(([0-9]+\\.){2,}[0-9]+)',' ')\n    \n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')\n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([!?\\']+)',r' \\1 ')    \n    \n    return data","b0d66bf0":"# Test clean function\ntest_clean_df = pd.DataFrame({\"text\":\n                              [\"heyy\\n\\nkkdsfj\",\n                               \"hi   how\/are\/you ???\",\n                               \"hey?????\",\n                               \"hey????? 18.98.333.20 18.98.\",\n                               \"noooo!!!!!!!!!   comeone !! \",\n                              \"cooooooooool     brooooooooooo  coool brooo\",\n                              \"naaaahhhhhhh\"]})\ndisplay(test_clean_df)\nclean(test_clean_df,'text')","f3c992ae":"df = clean(df,'text')","6a15f48d":"\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                        df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                                            random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n\n    tmp_df.to_csv(f'\/kaggle\/working\/df_clean_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","9648916a":"del df,tmp_df\ngc.collect()","cbc9cf18":"df_ = pd.read_csv(\"..\/input\/ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv\")\nprint(df_.shape)\n\ndf_ = df_[['txt', 'offensiveness_score']].rename(columns={'txt': 'text',\n                                                                'offensiveness_score':'y'})\n\ndf_['y'] = (df_['y'] - df_.y.min()) \/ (df_.y.max() - df_.y.min()) \ndf_.y.hist()","8b15b658":"\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = df_.sample(frac=frac_1, random_state = 10*(fld+1))\n    tmp_df.to_csv(f'\/kaggle\/working\/df2_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","f8f8759d":"del tmp_df, df_; \ngc.collect()","f234cacf":"dfm = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\nprint(dfm.shape)\n\ndfm = clean(dfm,'comment_text')\n\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(f'****** {col} *******')\n    display(dfm.loc[dfm[col]==1,['comment_text',col]].sample(5))\n    \n\n# Give more weight to severe toxic \ndfm['severe_toxic'] = dfm.severe_toxic * 2\ndfm['y'] = (dfm[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndfm['y'] = dfm['y']\/dfm['y'].max()\n\ndfm = dfm[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndfm.y.value_counts()","d6f083e3":"\nfor fld in range(n_folds):\n    print(f'Fold: {fld}')\n    tmp_df = pd.concat([dfm[dfm.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                        dfm[dfm.y==0].sample(n=int(len(dfm[dfm.y>0])*frac_1*frac_1_factor) , \n                                            random_state = 10*(fld+1))], axis=0).sample(frac=1, \n                                                                                        random_state = 10*(fld+1))\n\n    tmp_df.to_csv(f'\/kaggle\/working\/dfm_fld{fld}.csv', index=False)\n    print(tmp_df.shape)\n    print(tmp_df['y'].value_counts())","8ae6daca":"# Validation data \n\n# df_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\n# df_val.shape","028dea3b":"# gp1=df_val.copy()\n# gp1['pair'] = gp1.apply(lambda x:\" \".join(sorted((x['less_toxic'],\n#                                                   x['more_toxic']))),axis=1)\n# gp1['pair_hash'] = gp1.pair.apply(lambda x: str(abs(hash(x)) % (10 ** 8)))\n# del gp1['pair']\n# print(len(gp1), len(gp1.pair_hash.drop_duplicates()))\n\n# gp1['cnt']=gp1.groupby(['pair_hash', \n#                         'less_toxic',\n#                         'more_toxic']).transform(lambda x: x.count())\n# print(gp1[['pair_hash', 'less_toxic', 'more_toxic','cnt']].drop_duplicates().cnt.value_counts())\n\n# #gp1.head(10)\n# majority_cases = gp1.groupby('pair_hash')\\\n#                     .agg({'cnt':['count','max']})\\\n#                     .reset_index()\\\n#                     .set_axis(['pair_hash','count','max'], \n#                               axis='columns')\\\n#                     .assign(pct=lambda x: x['max']\/x['count'])\\\n#                     .query('pct>=0.5')\\\n#                     .rename(columns={'max':'cnt'})\\\n#                     [['pair_hash','cnt']]\n\n# df_val = pd.merge(gp1,majority_cases,\n#                  how=\"inner\",\n#                  on = ['pair_hash','cnt'])\n# #gp1.groupby('pair_hash').apply(lambda x: x[['less_toxic','more_toxic','cnt']].sort_values('cnt', ascending=False))\n# df_val.shape","08b47307":"# Test data\n\ndf_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ndf_sub.shape","1675b1a7":"\nclass LengthTransformer(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return sparse.csr_matrix([[(len(x)-360)\/550] for x in X])\n    def get_feature_names(self):\n        return [\"lngth\"]\n\nclass LengthUpperTransformer(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return sparse.csr_matrix([[int(sum([1 for y in x if y.isupper()])\/len(x) > 0.75) ] for x in X])\n    def get_feature_names(self):\n        return [\"lngth_uppercase\"]","36da9872":"\n# df_val['upper_1'] = np.array(LengthUpperTransformer().transform(df_val['less_toxic']).todense()).reshape(-1,1)\n# df_val['upper_2'] = np.array(LengthUpperTransformer().transform(df_val['more_toxic']).todense()).reshape(-1,1)\n\n# print(df_val['upper_1'].mean(), df_val['upper_1'].std())\n# print(df_val['upper_2'].mean(), df_val['upper_2'].std())\n\n# df_val['upper_1'].hist(bins=100)\n# df_val['upper_2'].hist(bins=100)","3ae70416":"@timer\ndef train_pipeline(pipeline, data_path_name, n_folds, pipeline_, clean_prm = False):\n    val_preds_arr1_tmp = np.zeros((df_val.shape[0], n_folds))\n    val_preds_arr2_tmp = np.zeros((df_val.shape[0], n_folds))\n    test_preds_arr_tmp = np.zeros((df_sub.shape[0], n_folds))\n\n    for fld in range(n_folds):\n        print(\"\\n\\n\")\n        print(f' ****************************** FOLD: {fld} ******************************')\n        df = pd.read_csv(f'\/kaggle\/working\/{data_path_name}_fld{fld}.csv')\n        print(df.shape)\n\n        print(\"\\nTrain:\")\n        # Train the pipeline\n        pipeline_.fit(df['text'], df['y'])\n\n        # What are the important features for toxicity\n\n        print('\\nTotal number of features:', len(pipeline_['features'].get_feature_names()) )\n\n        if pipeline_['clf'].__class__.__name__ == 'Ridge':\n            feature_wts = sorted(list(zip(pipeline_['features'].get_feature_names(), \n                                          np.round(pipeline_['clf'].coef_,2) )), \n                                 key = lambda x:x[1], \n                                 reverse=True)\n        else:\n            feature_wts = sorted(list(zip(pipeline_['features'].get_feature_names(), \n                                          np.round(pipeline_['clf'].feature_importances_,2) )), \n                                 key = lambda x:x[1], \n                                 reverse=True)\n            \n\n        display(pd.DataFrame(feature_wts[:50], columns = ['feat','val']).T)\n        display(pd.DataFrame(feature_wts[-50:], columns = ['feat','val']).T)\n        #.plot('feat','val',kind='barh',figsize = (8,8) )\n        #plt.show()\n\n        if clean_prm:\n            print(\"\\npredict validation data \")\n            val_preds_arr1_tmp[:,fld] = pipeline_.predict(clean(df_val,'less_toxic')['less_toxic'])\n            val_preds_arr2_tmp[:,fld] = pipeline_.predict(clean(df_val,'more_toxic')['more_toxic'])\n\n            print(\"\\npredict test data \")\n            test_preds_arr_tmp[:,fld] = pipeline_.predict(clean(df_sub,'text')['text'])\n        else:\n            print(\"\\npredict validation data \")\n            val_preds_arr1_tmp[:,fld] = pipeline_.predict(df_val['less_toxic'])\n            val_preds_arr2_tmp[:,fld] = pipeline_.predict(df_val['more_toxic'])\n\n            print(\"\\npredict test data \")\n            test_preds_arr_tmp[:,fld] = pipeline_.predict(df_sub['text'])\n    return val_preds_arr1_tmp, val_preds_arr2_tmp, test_preds_arr_tmp","9e00ac7d":"features = FeatureUnion([\n    #('vect1', LengthTransformer()),\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n    #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge()),\n        #(\"clf\",LinearRegression())\n    ]\n)\n\nval_preds_arr1, val_preds_arr2, test_preds_arr = train_pipeline(pipeline, \n                                                                \"df\", \n                                                                n_folds,\n                                                                pipeline,\n                                                                clean_prm=False)\n","f12fac9b":"features = FeatureUnion([\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge()),\n    ]\n)\n\nval_preds_arr1c, val_preds_arr2c, test_preds_arrc = train_pipeline(pipeline, \n                                                                   \"df_clean\", \n                                                                   n_folds,\n                                                                   pipeline,\n                                                                   clean_prm=True)\n","1d2268d2":"features = FeatureUnion([\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 4, max_df=0.4, max_features = 10000,\n                              analyzer = 'word', ngram_range = (1,2))),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        (\"clf\", RandomForestRegressor(n_estimators = 50, \n                                      min_samples_leaf=3, \n                                      max_features = 'sqrt')),\n        #(\"clf\", Ridge()),\n    ]\n)\n\nval_preds_arr1c_r, val_preds_arr2c_r, test_preds_arrc_r = train_pipeline(pipeline, \n                                                                   \"df_clean\", \n                                                                   n_folds,\n                                                                   pipeline,\n                                                                   clean_prm=True)\n","623a6290":"features = FeatureUnion([\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge()),\n    ]\n)\n\nval_preds_arr1_, val_preds_arr2_, test_preds_arr_ = train_pipeline(pipeline, \n                                                                   \"df2\", \n                                                                   n_folds,\n                                                                   pipeline,\n                                                                   clean_prm=True)\n","77eef007":"features = FeatureUnion([\n    #('vect1', LengthTransformer()),\n    #('vect2', LengthUpperTransformer()),\n    (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, \n                              analyzer = 'char_wb', ngram_range = (3,5))),\n    #(\"vect4\", CountVectorizer(min_df= 5, max_df=0.3, analyzer = 'word', ngram_range = (2,3), token_pattern=r'(?u)\\b\\w{3,}\\b', binary=True))\n])\npipeline = Pipeline(\n    [\n        (\"features\", features),\n        #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n        (\"clf\", Ridge()),\n        #(\"clf\",LinearRegression())\n    ]\n)\n\nval_preds_arr1m, val_preds_arr2m, test_preds_arrm = train_pipeline(pipeline, \n                                                                    \"dfm\", \n                                                                    n_folds,\n                                                                   pipeline,\n                                                                   clean_prm=True)\n","c264ba68":"# del df, pipeline, feature_wts\n# gc.collect()","69688459":"print(\"\\n Toxic data \")\np1 = val_preds_arr1.mean(axis=1)\np2 = val_preds_arr2.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')\n\nprint(\"\\n Ruddit data \")\np3 = val_preds_arr1_.mean(axis=1)\np4 = val_preds_arr2_.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p3 < p4).mean() * 100,2)}')\n\nprint(\"\\n Toxic CLEAN data \")\np5 = val_preds_arr1c.mean(axis=1)\np6 = val_preds_arr2c.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p5 < p6).mean() * 100,2)}')\n\nprint(\"\\n Toxic Mulitlingual data \")\np7 = val_preds_arr1m.mean(axis=1)\np8 = val_preds_arr2m.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p7 < p8).mean() * 100,2)}')\n\nprint(\"\\n Toxic CLEAN data - RF\")\np9 = val_preds_arr1c_r.mean(axis=1)\np10 = val_preds_arr2c_r.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p9 < p10).mean() * 100,2)}')\n\n#val_preds_arr1c_r, val_preds_arr2c_r, test_preds_arrc_r\n\nprint(\"\\n Simple avg of ALL \")\nprint(f'Validation Accuracy is { np.round(((p1+p3+p5+p7+p9) < (p2+p4+p6+p8+p10)).mean() * 100,2)}')\n\n#print(\"\\n Simple product of ALL \")\n#print(f'Validation Accuracy is { np.round(((p1*p3*p5*p7) < (p2*p4*p6*p8)).mean() * 100,2)}')\n","0450c9bb":"corr = np.corrcoef(np.vstack([p1,p3,p5,p7,p9]))\nprint(corr)\n\nplt.matshow(corr)\nplt.show()","bb17c91f":"\n@timer\ndef optimize_wts():\n    func = lambda x: -1*(((x[0]*p1 + x[1]*p3 + x[2]*p5 + x[3]*p9) < \\\n                          (x[0]*p2 + x[1]*p4 + x[2]*p6  + x[3]*p10)).mean())\n\n    rranges = (slice(0.20, 0.6, 0.01), \n               slice(0.05, 0.5, 0.01),\n               slice(0.05, 0.5, 0.015),\n               slice(0.05, 0.5, 0.01),\n              )\n\n    resbrute = optimize.brute(func, \n                              rranges, \n                              #args=params, \n                              full_output=True,\n                              finish=None)\n    return resbrute\nresbrute = optimize_wts()\n\nprint(resbrute[0])  # global minimum\nprint(resbrute[1]*-1)  # function value at global minimum\n    ","f59d116f":"w1,w2,w3,w4 = resbrute[0]\n#print(best_wts)\n\np1_wt = w1*p1 + w2*p3 + w3*p5 + w4*p9\np2_wt = w1*p2 + w2*p4 + w3*p6 + w4*p10\n","f5705791":"df_val['p1'] = p1_wt\ndf_val['p2'] = p2_wt\ndf_val['diff'] = np.abs(p2_wt - p1_wt)\n\ndf_val['correct'] = (p1_wt < p2_wt).astype('int')\n","a4eb8f1c":"\n### Incorrect predictions with similar scores\n\ndf_val[(df_val.correct == 0) & (df_val.p1 < 0.5*df_val.p1.max())].sort_values('diff', ascending=True).head(20)","caba9b9f":"df_val[(df_val.correct == 0) & (df_val.p1 > 0.5*df_val.p1.max())].sort_values('diff', ascending=True).head(20)","944a3dbf":"### Incorrect predictions with dis-similar scores\n\ndf_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)","2e8d62de":"df_val[(df_val.correct == 0) & (df_val['diff'] < 0.4*df_val['diff'].max())].sort_values('diff', ascending=False).head(20)\n","020d484f":"# Predict using pipeline\n\ndf_sub['score'] = w1*test_preds_arr.mean(axis=1) + \\\n                  w2*test_preds_arr_.mean(axis=1) + \\\n                  w3*test_preds_arrc.mean(axis=1) + \\\n                  w4*test_preds_arrc_r.mean(axis=1)","cca99676":"#test_preds_arr","3da8409e":"# Cases with duplicates scores\n\ndf_sub['score'].count() - df_sub['score'].nunique()","922f1585":"same_score = df_sub['score'].value_counts().reset_index()[:10]\nsame_score","b133b4d3":"df_sub[df_sub['score'].isin(same_score['index'].tolist())]","fe6fbae4":"# Same comments have same score - which is ok ","1758d0a5":"# # Rank the predictions \n\n# df_sub['score']  = scipy.stats.rankdata(df_sub['score'], method='ordinal')\n\n# print(df_sub['score'].rank().nunique())","d042c5ca":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","537943be":"#### Some of these just look incorrectly tagged \n","f359c97f":"## Load Test data  \n","91bf9c2f":"# Load validation data & filter for overlapping sentences","1e4fe4d1":"## Read toxic Ruddit data","cb7a0489":"## Optimize the model weights for ensemble","615ebb53":"# Validate the pipeline ","08c0f561":"# Imports","06f67ca1":"# Toxic clean RF","3c52188c":"# Correlation between predictions","7d6fe96a":"## Analyze bad predictions \n### Incorrect predictions with similar scores\n### Incorrect predictions with different scores","2ab34b3b":"## Train pipeline\n\n- Load folds data\n- train pipeline\n- Predict on validation data\n- Predict on test data","18297564":"# Training data \n\n## Convert the label to SUM of all toxic labels (This might help with maintaining toxicity order of comments)","188a0c44":"## Create 3 versions of the TOXIC data","d62efa66":"# Toxic Training","1971ab9c":"## Read Jigsaw multilingual data CLEANED","3da25956":"# Create Sklearn Pipeline with \n-  TFIDF - Take 'char_wb' as analyzer to capture subwords well\n-  Ridge - Ridge is a simple regression algorithm that will reduce overfitting ","99a16678":"# Toxic __clean__ Training","3ddaad9d":"### Remove contradicting cases from validation data\n- cases where contradictory evaluation is in minority (< 50%)","f93e5cd5":"# Training function","f4be2750":"#### Some cool starters notebooks : \n- https:\/\/www.kaggle.com\/julian3833\/jigsaw-incredibly-simple-naive-bayes-0-768\n- https:\/\/www.kaggle.com\/steubk\/jrsotc-ridgeregression-ensemble-of-3","c129c783":"# Predict on test data ","67cd2400":"### Does % of uppercase characters have effect on toxicity\n","586dca4f":"# Create 3 versions of __clean__ TOXIC data","c3b6a538":"# Create 3 versions of Multilingual data","270bf4b1":"## Mulitlingual data Training","da444661":"## Ruddit data Training","23240f62":"## Correct the rank ordering","5397d69c":"## 0.85+ score by ensemble of simple TF-Idf and Ridge regression\n\n### Ensemble of TfIdf - Ridge models using data from \n- Toxic competition\n- Toxic CLEANED competition\n- Ruddit toxic data\n- Toxic multilingual competition\n\n### Analysis of bad predictions\n","7c0072a6":"# Create 3 versions of RUDDIT data"}}