{"cell_type":{"fecde05a":"code","e1b45c30":"code","ec706780":"code","9ec6da7d":"code","ea583af7":"code","21497af4":"code","fbc621d7":"code","b0c27af2":"code","9bb84620":"code","880b94b2":"code","be6356c0":"code","1b6e7c46":"code","2c5ca719":"code","22772832":"code","b305193f":"code","e38598be":"code","224e24fa":"code","14009a25":"code","d0028c2a":"code","04052081":"code","bd82d022":"code","8fdd390e":"code","a95cf792":"code","271a2248":"code","4a4f21bb":"code","9cb58671":"code","bb8a49bb":"markdown","0374d97f":"markdown","5c345851":"markdown","15ccefb7":"markdown","3e35e685":"markdown","338407a3":"markdown","b2056bc2":"markdown","21484796":"markdown","888717d3":"markdown","05a16771":"markdown","071a0b32":"markdown","f75f7f3f":"markdown","02c810d3":"markdown","095153fb":"markdown","83fc0d5e":"markdown","64e88beb":"markdown","63e51b53":"markdown","24861e9c":"markdown","e4ca6704":"markdown","ff19333c":"markdown","ccf62e87":"markdown","1115f28c":"markdown","73589fd5":"markdown","4a6812d1":"markdown","964b6e67":"markdown","af4fd214":"markdown","041d1053":"markdown","4645f015":"markdown"},"source":{"fecde05a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nsns.set(style=\"whitegrid\")","e1b45c30":"data=pd.read_csv('..\/input\/student.csv')\ndata","ec706780":"data.shape","9ec6da7d":"data.info()","ea583af7":"print (\"\\nVariables : \\n\" ,data.columns.tolist())","21497af4":"X1 = data['IQ']\nX2 = data['Study hrs.']\nY = data['Marks'] ","fbc621d7":"data.describe()","b0c27af2":"hist_data = [data['IQ'], data['Study hrs.'],Y]\n\ngroup_labels = ['IQ', 'Study Hrs','Marks']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels, bin_size=5,curve_type='normal',\n                         show_hist = True, show_curve = True)\nfig['layout'].update(title='Distribution Plot ')\nfig.show()","9bb84620":"data.drop([\"Student\"], axis = 1, inplace = True)","880b94b2":"fig = ff.create_annotated_heatmap(data.corr().values.tolist(),\n                                   \n                                  y=data.columns.tolist(),\n                                  x=data.columns.tolist(), \n                                  colorscale='Inferno',\n                                  showscale=True\n                                 )\nfig.show()","be6356c0":"import seaborn as sns\nsns.set()\nsns.pairplot(data,size = 2.5, kind = \"reg\",corner=True)","1b6e7c46":"data.drop([\"GPA\"], axis = 1, inplace = True)","2c5ca719":"fig = ff.create_annotated_heatmap(data.corr().values.tolist(),\n                                   \n                                  y=data.columns.tolist(),\n                                  x=data.columns.tolist(), \n                                  colorscale='Viridis',\n                                  showscale=True\n                                 )\nfig.show()","22772832":"\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nt = data[['IQ', 'Study hrs.']]\nt['Intercept'] = 1\n\n# Compute and view VIF\nvif = pd.DataFrame()\nvif[\"Variables\"] = t.columns\nvif[\"VIF\"] = [variance_inflation_factor(t.values, i) for i in range(t.shape[1])]\n\n# View results using print\nprint(vif)","b305193f":"# y=b0+b1x1\n\nx=sm.add_constant(X2)\n\nresults=sm.OLS(data['IQ'],x).fit()\n#Contain Ordinary Least Square Regression \nresults.summary()","e38598be":"# data[\"e\"] = data[\"Study hrs.\"]\/30 , ##error bar =  ,error_y='e'\n\nfig = px.scatter(data, x=\"Study hrs.\", y=\"IQ\",title='IQ VS Study in Hrs',trendline=\"ols\" , color= \"IQ\")\n\nfig.show()","224e24fa":"plt.figure(figsize=(13,7))\nplt.title('Residual Plot for Marks and Study Hrs',size=20)\nsns.residplot(X2, X1, lowess=True, color=\"r\")\nplt.ylabel('Residuals',size=15)","14009a25":"# y=b0+b1x1\n\nx=sm.add_constant(X1)\n\nresults=sm.OLS(Y,x).fit()\n#Contain Ordinary Least Square Regression \nresults.summary()","d0028c2a":"fig = px.scatter(data, x=\"IQ\", y=\"Marks\", title= 'MARKS vs IQ',trendline=\"ols\",color=\"Marks\")\n\nfig.show()","04052081":"plt.figure(figsize=(13,7))\nplt.title('Residual Plot for Marks and IQ',size=20)\nsns.residplot(X1, Y, lowess=True, color=\"orange\")\nplt.ylabel('Residuals',size=15)","bd82d022":"# y=b0+b1x1\n\nx=sm.add_constant(X2)\n\nresults=sm.OLS(Y,x).fit()\n\n#Contain Ordinary Least Square Regression \n\nresults.summary()","8fdd390e":"fig = px.scatter(data, x=\"Study hrs.\", y=\"Marks\",title='MARKS vs Study Hrs  ', trendline=\"ols\",color=\"Study hrs.\")\n\nfig.show()","a95cf792":"plt.figure(figsize=(13,7))\nplt.title('Residual Plot for Marks and Study Hrs',size=20)\nsns.residplot(Y, X2, lowess=True, color=\"c\")\nplt.ylabel('Residuals',size=15)","271a2248":"# regression\n# data['bestfit'] = sm.OLS(Y,sm.add_constant(X1)).fit().fittedvalues\n\n# plotly figure setup\nfig=go.Figure()\n#********************#\n\n\nfig.add_trace(go.Scatter(name='Marks vs IQ', x=X1, y=Y, mode='markers'))\n\ndata['bestfit'] = sm.OLS(Y,sm.add_constant(X1)).fit().fittedvalues\n\nfig.add_trace(go.Scatter(name='line of best fit', x=X1, y=data['bestfit'], mode='lines'))\n\n#********************#\n\n\n\nfig.add_trace(go.Scatter(name='MARKS vs Study hrs', x=X2, y=Y, mode='markers'))\n\ndata['bestfit'] = sm.OLS(Y,sm.add_constant(X2)).fit().fittedvalues\n\nfig.add_trace(go.Scatter(name='line of best fit', x=X2, y=data['bestfit'], mode='lines'))\n\n#********************#\n\n\n\n# plotly figure layout\nfig.update_layout(xaxis_title = 'Independent Variables', yaxis_title = 'MARKS',title='MARKS VS IQ and Study hrs')\n\nfig.show()","4a4f21bb":"# y=b0+b1x1\nX = data[['IQ','Study hrs.']]\nx=sm.add_constant(X)\n\nresults=sm.OLS(Y,x).fit()\n\n#Contain Ordinary Least Square Regression \nresults.summary()","9cb58671":"\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nt = data[['IQ', 'Study hrs.']]\nt['Intercept'] = 1\n\n# Compute and view VIF\nvif = pd.DataFrame()\nvif[\"Variables\"] = t.columns\nvif[\"VIF\"] = [variance_inflation_factor(t.values, i) for i in range(t.shape[1])]\n\n# View results using print\nprint(vif)","bb8a49bb":"## [Coefficient of Multiple Determination](#1.1)\n<a id= '3'><\/a>\n<a id= '1.1'><\/a>\n\n   The coefficient of multiple determination **R2** measures the proportion of variance in the dependent variable that is explained by all of the independent variables.\n\n   If we ignore the dependent variable, we can compute a coefficient of multiple determination **R2k** for each of the k independent variables. We do this by regressing the kth independent variable on all of the other independent variables. That is, we treat **Xk** as the dependent variable and use the other independent variables to predict **Xk**.\n\n   How do we interpret **R2k**? If **R2k** equals zero, variable **k** is not correlated with any other independent variable; and multicollinearity is not a problem for variable **k**. As a rule of thumb, most analysts feel that multicollinearity is a potential problem when **R2k** is greater than **0.75**; and, a serious problem when **R2k** is greater than __0.9__.","0374d97f":"#### Ordinary Least Square Method \n**A Model Summary**\n* Dep. Variable means variable that we want to predict and also called target variable, which is **Marks** \n* Model : OLS (Ordinary Least Square) , OLS is the common method to estimate the linear regression , **this method will find the line which minimises the Sum of the Squared Error** ( Lower Error = better explanatory power ) ","5c345851":"# [ Multicollinearity and Regression Analysis](#1)\n<a id='1'><\/a>","15ccefb7":"### Correlation coefficient between (X1, Y), (X2, Y), (X1, X2)","3e35e685":"**std err ** shows the accuracy of prediction for each variable (Lower means better) <br>\nStudy hrs is a significant variable it has 0.009 in **P> | t |**\n","338407a3":"### Data Description\n- Student : Number of Students.  \n- Marks : Study score obtained in particular test.\n- IQ : Intelligence Quotient\n- Study Hrs.: Weekly study time(in Hrs).\n- GPA : Grade Point Average","b2056bc2":"### Attribute Information","21484796":"#### Importing Libraries and Data","888717d3":"Contents:\n\n[Multicollinearity and Regression Analysis](#1)\n- [How to Measure Multicollinearity](#2)\n- [Coefficient of Multiple Determination](#3)\n- [How to Deal with Multicollinearity](#4)","05a16771":"### How to Measure Multicollinearity\n<a id='2'><\/a>\nThere are two popular ways to measure multicollinearity: \n\n[1. compute a coefficient of multiple determination for each independent variable, or ](#1.1)\n\n[2. compute a variance inflation factor for each independent variable.](#1.2)","071a0b32":"In regression, multicollinearity refers to the extent to which independent variables are correlated. Multicollinearity exists when:\n\n- __One independent variable is correlated with another independent variable.__\n- __One independent variable is correlated with a linear combination of two or more independent variables.__","f75f7f3f":"\n**Coefficient Table** <br>\n54.5735\t is intercept <br>\n1.9564 means  b0 \n\n\n**y=b0+b1x1**<br>\n","02c810d3":"![image.png](attachment:image.png)\n\nIf the set of independent variables is characterized by a little bit of multicollinearity, the analysis of regression coefficients should be straightforward. If there is a lot of multicollinearity, the analysis will be hard to interpret and can be skipped.\n\n**Note:** Multicollinearity makes it hard to assess the relative importance of independent variables, but it does not affect the usefulness of the regression equation for prediction. Even when multicollinearity is great, the least-squares regression equation can be highly predictive. So, if you are only interested in prediction, multicollinearity is not a problem.\n\n","095153fb":"## [Variance Inflation Factor](#1.2)\n<a id = '4'><\/a>\n<a id = '1.2'><\/a>\nThe variance inflation factor is another way to express exactly the same information found in the coefficient of multiple correlation. A variance inflation factor is computed for each independent variable, using the following formula:\n\n**VIFk = 1 \/ ( 1 - R2k )**\n\nwhere **VIFk** is the variance inflation factor for variable **k**, and **R2k** is the coefficient of multiple determination for variable **k**.\n\nIn many statistical packages (e.g., SAS, SPSS, Minitab), the variance inflation factor is available as an optional regression output. In MiniTab, for example, the variance inflation factor can be displayed as part of the regression coefficient table.\n","83fc0d5e":"**Conclusion :** R- Squared measure goodness of fit , the more factore you include in regression the higher the R Squared \n\n**F-statistic :**  is important for regression as it give us some important insights (Higher mean Better), **the lower the F-Statistic the closer to an non-significant model** .","64e88beb":"## How to Deal with Multicollinearity\n<a id = '4'><\/a>\nIf you only want to predict the value of a dependent variable, you may not have to worry about multicollinearity.\n\nMultiple regression can produce a regression equation that will work for you, even when independent variables are highly correlated.\n\nThe problem arises when you want to assess the relative importance of an independent variable with a high R2k (or, equivalently, a high VIFk). \n\nIn this situation, try the following:\n\n- Redesign the study to avoid multicollinearity. If you are working on a true experiment, the experimenter controls treatment levels. Choose treatment levels to minimize or eliminate correlations between independent variables.\n- Increase sample size. Other things being equal, a bigger sample means reduced sampling error. The increased precision may overcome potential problems from multicollinearity.\n- Remove one or more of the highly-correlated independent variables. Then, define a new regression equation, based on the remaining variables. Because the removed variables were redundant, the new equation should be nearly as predictive as the old equation; and coefficients should be easier to interpret because multicolinearity is reduced.\n- Define a new variable equal to a linear combination of the highly-correlated variables. Then, define a new regression equation, using the new variable in place of the old highly-correlated variables.\n\n**Note**: Multicollinearity only affects variables that are highly correlated. \n\nIf the variable you are interested in has a small R2j, statistical analysis of its regression coefficient will be reliable and informative. That analysis will be valid, even when other variables exhibit high multicollinearity.","63e51b53":"### Coefficient Table\n-6.5706 means constant  <br>\n0.8681 means b0 <br>\n0.2295 means b1 <br>\n\n**y = b0 + b1x1 + b2x2** <br>","24861e9c":"## VIF\n\nThe interpretation of the variance inflation factor mirrors the interpretation of the coefficient of multiple determination. If **VIFk** = 1, variable **k** is not correlated with any other independent variable. \n    \n   As a rule of thumb, multicollinearity is a potential problem when **VIFk** is greater than **4**; and, a serious problem when it is greater than **10**. The output above shows a **VIF** of **2.466**, which indicates some multicollinearity but not enough to worry about.\n\nBottom line: If **R2k** is greater than **0.9** or **VIFk** is greater than **10**, it is likely that regression coefficients are poorly estimated. And significance tests on those coefficients may be misleading.","e4ca6704":"**Dependent Variable (Y):** Marks (Study score obtained in particular test.)\n\n**Independent Variables (X1,X2...):** Study hrs and IQ.","ff19333c":"                        _________________________________________________","ccf62e87":"###### The Multicollinearity Problem\nAs part of regression analysis, researchers examine regression coefficients to assess the relative influence of independent variables. They look at the magnitude of coefficients, and they test the statistical significance of coefficients.\n\n\nIf the coefficient for a particular variable is significantly greater than zero, researchers judge that the variable contributes to the predictive ability of the regression equation. In this way, it is possible to distinguish variables that are more useful for prediction from those that are less useful.\n\n\nThis kind of analysis makes sense when multicollinearity is small. But it is problematic when multicollinearity is great. Here's why:\n\n- When one independent variable is perfectly correlated with another independent variable (or with a combination of two or more other independent variables), a unique least-squares solution for regression coefficients does not exist.\n\n\n- When one independent variable is highly correlated with another independent variable (or with a combination of two or more other independent variables), the marginal contribution of that independent variable is influenced by other independent variables. As a result:\n\n\n    - Estimates for regression coefficients can be unreliable.\n    - Tests of significance for regression coefficients can be misleading.\n    \n    \n With this in mind, the analysis of regression coefficients should be contingent on the extent of multicollinearity. This means that the analysis of regression coefficients should be preceded by an analysis of multicollinearity.","1115f28c":"**R-Squared** is measured how powerful the regression \n* R-Squared = Variability explained by the regression \/ Total variability of the dataset\n* R-Squared using values ranging from 0 to 1 , if R-Squared = 0 means your regression explains NONE of the variability , if 1 means regression explains the ENTIRE variability.\n\n**Our R-Squared has 0.595 , in other words Marks explained 60% of variability of weekly Study in hrs. , but since it is far away from 90% we may conclude that we are missing some important information .<br>\nOther determinants must be consider such as Gender,GPA or maybe some other status.**","73589fd5":"## Scatter Plot Matrix of ((X1, Y), (X2, Y), (X1, X2)...","4a6812d1":"### Context\n#### Multiple Regression, Linear Models , Multicollinearity\n- Measure multicollinearity, based on IQ, Study Hours.\n- How multicollinearity affects your ability to interpret statistical tests on IQ and Study Hours.","964b6e67":"    Multicollinearity makes it hard to assess the relative importance of independent variables, but it does not affect the usefulness of the regression equation for prediction. Even when multicollinearity is great, the least-squares regression equation can be highly predictive. So, if you are only interested in prediction, multicollinearity is not a problem.","af4fd214":"####  Correlation matrix between All Variables","041d1053":"## Multicollinearity only affects variables that are highly correlated. \n    \n    If the variable you are interested in has a small R2j, statistical analysis of its regression coefficient   will be reliable and informative. \n    \n    That analysis will be valid, even when other variables exhibit high multicollinearity.","4645f015":"##  Descriptive Statistics\n"}}