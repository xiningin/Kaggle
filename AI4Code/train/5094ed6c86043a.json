{"cell_type":{"3a11233e":"code","88d9d959":"code","c8b2daa6":"code","6485d587":"code","26438fee":"code","9aa69f63":"code","5ac2b746":"code","89f9c127":"code","9cec39ee":"code","0fbd2433":"code","c6453050":"code","0c4942f5":"code","7546afb6":"code","3676a765":"code","f1ab2901":"code","f3d476e3":"code","729db73a":"code","e34afdcb":"code","b35da3db":"code","35e58725":"code","217dff0a":"code","2c5490f6":"code","96eb2832":"code","bc11037d":"code","d3e4bc11":"code","c7f1a31e":"code","5b389e39":"code","19b05d96":"code","d0ad6921":"code","0364bc66":"code","d4511a2d":"code","cb040fec":"code","dca21860":"code","09406370":"code","953c9c1d":"code","1d34cfc6":"code","7c2b9a38":"code","d575b20c":"code","40cbb494":"code","7503ccf1":"code","f0813bcd":"code","a4ae27a4":"code","421bcb16":"markdown","50f0ac4a":"markdown","06fb7231":"markdown","a796471c":"markdown","57266881":"markdown","1f4e0830":"markdown","73a9ef81":"markdown","4ed85751":"markdown","5f5b78a3":"markdown","f252a5ff":"markdown","f7ac4c9c":"markdown","45bffcf2":"markdown","1c3418a7":"markdown","15f0aec4":"markdown","cf24bbc5":"markdown","d2f84dc4":"markdown","e726f4f3":"markdown","507637af":"markdown","e131c9a2":"markdown","976d6484":"markdown","dcdb6755":"markdown","468e1172":"markdown","d19d500d":"markdown","e5851653":"markdown","3ba908ac":"markdown","2ef27093":"markdown","b009ccfc":"markdown","715d9f62":"markdown","59e9349d":"markdown","ed9f3666":"markdown","57aba6a2":"markdown","43b92a4d":"markdown","19966238":"markdown","ad2b68b2":"markdown","7263a9dd":"markdown","c2a59d6f":"markdown","1ea3e481":"markdown","23aecdaa":"markdown","16f16f33":"markdown","75c57e94":"markdown","7af963be":"markdown","09495a98":"markdown","0e023a19":"markdown","69411d15":"markdown","cfb11afb":"markdown","61db7edd":"markdown"},"source":{"3a11233e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndf = pd.read_csv('..\/input\/fifa-2018-match-statistics\/FIFA 2018 Statistics.csv')\ny = (df['Man of the Match'] == 'Yes') # convert from string \"Yes\/No\" to binary\nfeature_names = [i for i in df.columns if df[i].dtype in [np.int64]]\nX = df[feature_names]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n","88d9d959":"# Get feature importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\npermutation = PermutationImportance(my_model, random_state=1).fit(valid_X, valid_y)\neli5.show_weights(permutation, feature_names = feature_names)","c8b2daa6":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\n# Load data\ndf_taxi = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/train.csv', nrows=50000)","6485d587":"# Remove data with extreme outlier coordinates ot negative fares\ndf_taxi = df_taxi.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +\n                      'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +\n                      'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +\n                      'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +\n                      'fare_amount > 0'\n                       )\ny = df_taxi.fare_amount","26438fee":"# Model \nbase_features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n                 'dropoff_latitude', 'passenger_count']\nX = df_taxi[base_features]\n\ntrain_taxi_X, valid_taxi_X, train_taxi_y, valid_taxi_y = train_test_split(X, y, random_state=1)\nfirst_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_taxi_X, train_taxi_y)","9aa69f63":"train_taxi_X.describe()","5ac2b746":"train_taxi_y.describe()","89f9c127":"# Permutation performance\nperm = PermutationImportance(first_model, random_state=1).fit(valid_taxi_X, valid_taxi_y)\neli5.show_weights(perm, feature_names=valid_taxi_X.columns.tolist())","9cec39ee":"# Create new features\ndf_taxi['abs_lon_change'] = abs(df_taxi.dropoff_longitude - df_taxi.pickup_longitude)\ndf_taxi['abs_lat_change'] = abs(df_taxi.dropoff_latitude - df_taxi.pickup_latitude)\n\n# Add the new featues to the base features\nfeatures_2  = ['pickup_longitude',\n               'pickup_latitude',\n               'dropoff_longitude',\n               'dropoff_latitude',\n               'abs_lat_change',\n               'abs_lon_change']\n\nX = df_taxi[features_2]\nnew_train_taxi_X, new_valid_taxi_X, new_train_taxi_y, new_valid_taxi_y = train_test_split(X, y, random_state=1)\nsecond_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(new_train_taxi_X, new_train_taxi_y)","0fbd2433":"# Create a PermutationImportance object on second_model and fit it with the new valid data\nperm_2 = PermutationImportance(second_model, random_state=1).fit(new_valid_taxi_X, new_valid_taxi_y)\n# Show the weights for the permutation importance \neli5.show_weights(perm_2, feature_names=features_2)","c6453050":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz\n\n# Create tree_based model\ntree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)\ntree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names)\n# Visualize tree structure\ngraphviz.Source(tree_graph)","0c4942f5":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the dataset for plotting\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=valid_X, model_features=feature_names, feature='Goal Scored')\n\n# Plot it\npdp.pdp_plot(pdp_goals, 'Goal Scored')\nplt.show()","7546afb6":"# Another example plot\nfeature_to_plot = 'Distance Covered (Kms)'\npdp_dist = pdp.pdp_isolate(model=tree_model, dataset=valid_X, model_features=feature_names, feature=feature_to_plot)\npdp.pdp_plot(pdp_dist, feature_to_plot)\nplt.show()","3676a765":"# Build Random Forest model\nrf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n\npdp_dist_2 = pdp.pdp_isolate(model=rf_model, dataset=valid_X,\n                            model_features=feature_names, feature=feature_to_plot)\npdp.pdp_plot(pdp_dist_2, feature_to_plot)\nplt.show()","f1ab2901":"# Use pdp_interact and pdp_interact_plot instead of pdp_isolate and pdp_isolate_plot, respectively\nfeatures_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\ninter_1 = pdp.pdp_interact(model=tree_model, dataset=valid_X, \n                           model_features=feature_names, features=features_to_plot)\npdp.pdp_interact_plot(pdp_interact_out=inter_1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","f3d476e3":"# Partial dependece plot for pickup_longitude\nfeat_name = 'pickup_longitude'\npdp_dist = pdp.pdp_isolate(model=first_model, dataset=valid_taxi_X, model_features=base_features, feature=feat_name)\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","729db73a":"from plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","e34afdcb":"# Create all partial plots for NYC taxi-fare\ndef plot_pdp():\n#     fig, axes = plt.subplots(nrows=3, ncols=2, \n#                              figsize=(13, 16))\n    \n    for feat_name in base_features:\n        pdp_dist = pdp.pdp_isolate(model=first_model, dataset=valid_taxi_X, \n                                  model_features=base_features, feature=feat_name,n_jobs=3)\n        pdp.pdp_plot(pdp_dist, feat_name)\n        \n#     plt.subplots_adjust(top=0.9)\n#     plt.show()\n    return None\nplot_pdp()\n","b35da3db":"# 2D partial plot for NYC taxi fare\nfnames = ['pickup_longitude', 'dropoff_longitude']\nlongitude_pdp = pdp.pdp_interact(model = first_model, dataset=valid_taxi_X,\n                                model_features = base_features, features = fnames)\npdp.pdp_interact_plot(pdp_interact_out=longitude_pdp, feature_names=fnames, plot_type='contour')\nplt.show()","35e58725":"# PDP for pickup_longitude without absolute difference features\nfeat_name = 'pickup_longitude'\npdp_dist_original = pdp.pdp_isolate(model=first_model, dataset=valid_taxi_X, \n                                    model_features=base_features, feature=feat_name)\npdp.pdp_plot(pdp_dist_original, feat_name)\nplt.show()\n","217dff0a":"feat_name = 'pickup_longitude'\npdp_dist = pdp.pdp_isolate(model=second_model, dataset=new_valid_taxi_X, model_features=features_2, feature=feat_name)\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","2c5490f6":"from numpy.random import rand\nn_sample = 20000\n\n#  Creates two features, `X1` and `X2`, having random values in the range [-2, 2].\nX1 = 4 * rand(n_sample) - 2\nX2 = 4 * rand(n_sample) - 2\n\n# Creates a target variable `y`, which is always 1.\ny = -2 * X1 * (X1<-1) + X1 -2 * X1 * (X1 > 1) - X2\n# Trains a `RandomForestRegressor` model to predict `y` given `X1` and `X2`\nmy_df = pd.DataFrame({'X1':X1, 'X2':X2, 'y':y})\npredictors_df = my_df.drop(['y'], axis=1)\nmy_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(predictors_df, my_df.y)\n# Creates a PDP plot for `X1` and a scatter plot of `X1` vs. `y`\npdp_dist = pdp.pdp_isolate(model=my_model, dataset=my_df, model_features=['X1', 'X2'], feature='X1')\n# Visualize results\npdp.pdp_plot(pdp_dist, 'X1')\nplt.show()","96eb2832":"# Create array holding predictive feature\nX1 = 4 * rand(n_sample) - 2\nX2 = 4 * rand(n_sample) - 2\n\n# Create y\ny =  X1 * X2\n# create dataframe because pdp_isolate expects a dataFrame as an argument\nmy_df = pd.DataFrame({'X1': X1, 'X2': X2, 'y': y})\npredictors_df = my_df.drop(['y'], axis=1)\n\nmy_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(predictors_df, my_df.y)\n\npdp_dist = pdp.pdp_isolate(model=my_model, dataset=my_df, model_features=['X1', 'X2'], feature='X1')\npdp.pdp_plot(pdp_dist, 'X1')\nplt.show()\n","bc11037d":"perm = PermutationImportance(my_model).fit(predictors_df, my_df.y)\n# show the weights for the permutation importance you just calculated\neli5.show_weights(perm, feature_names = ['X1', 'X2'])","d3e4bc11":"df = pd.read_csv('..\/input\/fifa-2018-match-statistics\/FIFA 2018 Statistics.csv')\ny = (df['Man of the Match'] == 'Yes') # convert from string \"Yes\/No\" to binary\nfeature_names = [i for i in df.columns if df[i].dtype in [np.int64]]\nX = df[feature_names]\nX_fifa = X\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, random_state=1)\nmy_model_fifa = RandomForestClassifier(random_state=0).fit(train_X, train_y)","c7f1a31e":"# Package used to calculate Shap values\nimport shap\nrow_to_show = 5\ndata_for_prediction = valid_X.iloc[row_to_show]    # use 1 row of data \ndata_for_predicition_array = data_for_prediction.values.reshape(1, -1)\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model_fifa)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\n","5b389e39":"my_model_fifa.predict_proba(data_for_predicition_array)","19b05d96":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","d0ad6921":"# Example using KernelExplainer \nk_explainer = shap.KernelExplainer(my_model_fifa.predict_proba, train_X)\nk_shap_values = k_explainer.shap_values(data_for_prediction)\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","0364bc66":"hosp_re_data = pd.read_csv('..\/input\/hospital-readmissions\/train.csv')\ny = hosp_re_data.readmitted\nbase_features_hosp = [c for c in hosp_re_data.columns if c != 'readmitted']\n# Split data into training and validation set\nX = hosp_re_data[base_features_hosp]\ntrain_X_hosp, valid_X_hosp, train_y_hosp, valid_y_hosp = train_test_split(X, y, random_state=1)\n# Create model \nmodel_hosp = RandomForestClassifier(n_estimators=30, random_state=1).fit(train_X_hosp, train_y_hosp)","d4511a2d":"# Prepate the condensed exhibits for the doctor\n# Use permutation importance as a suucinct model summary\nperm = PermutationImportance(model_hosp, random_state=1).fit(valid_X_hosp, valid_y_hosp)\neli5.show_weights(perm, feature_names = valid_X_hosp.columns.tolist())","cb040fec":"# Using PDP for number_inpatient feature\nfeature_name = 'number_inpatient'\n# Create the data for ploting\nmy_pdp = pdp.pdp_isolate(model=model_hosp, dataset=valid_X_hosp, model_features=valid_X_hosp.columns, feature=feature_name)\n# plot\npdp.pdp_plot(my_pdp, feature_name)\nplt.show()","dca21860":"feature_name = 'time_in_hospital'\n# Create the data for ploting\nmy_pdp = pdp.pdp_isolate(model=model_hosp, dataset=valid_X_hosp, model_features=valid_X_hosp.columns, feature=feature_name)\n# plot\npdp.pdp_plot(my_pdp, feature_name)\nplt.show()","09406370":"# Get the average readmission rate for each time_in_hospital\n# Do concat to keep validation data separate, rather than using all original data\nall_train_hosp = pd.concat([train_X_hosp, train_y_hosp], axis=1)\nall_train_hosp.groupby(['time_in_hospital']).mean().readmitted.plot()\nplt.show()","953c9c1d":"# Use SHAP \n# Create sample data to test the function\nsample_data_for_prediction = valid_X_hosp.iloc[0].astype(float)\n\n# Create function\ndef patient_risk_factors(model, patient_data):\n    # Create object that can calculate shap values\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(patient_data)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], patient_data)","1d34cfc6":"patient_risk_factors(model_hosp, sample_data_for_prediction)","7c2b9a38":"# Summary SHAP plot for FIFA data\n\n# Create pbbject that can calculate shap values \nexplainer = shap.TreeExplainer(my_model_fifa)\n# Calculate the shap values for all validation data for plotting\nshap_values = explainer.shap_values(valid_X)\n# Make plot\nshap.summary_plot(shap_values[1], valid_X)    # shap_values[1] is for prediction of 'True'","d575b20c":"# Create pbbject that can calculate shap values \nexplainer = shap.TreeExplainer(my_model_fifa)\n\n# Calculate the shap values for all validation data for plotting\nshap_values = explainer.shap_values(X_fifa)\n\n# Make plot\nshap.dependence_plot('Ball Possession %', shap_values[1], X_fifa, interaction_index='Goal Scored')","40cbb494":"base_features = ['number_inpatient', 'num_medications', 'number_diagnoses', 'num_lab_procedures', \n                 'num_procedures', 'time_in_hospital', 'number_outpatient', 'number_emergency', \n                 'gender_Female', 'payer_code_?', 'medical_specialty_?', 'diag_1_428', 'diag_1_414', \n                 'diabetesMed_Yes', 'A1Cresult_None']\n\nX_hosp = hosp_re_data[base_features].astype(float)\ny_hosp = hosp_re_data.readmitted\ntrain_X_hosp_2, valid_X_hosp_2, train_y_hosp_2,valid_taxi_y_2 = train_test_split(X_hosp, y_hosp, random_state=1) \n# sample data for speed\nsmall_valid_X_hosp_2 = valid_X_hosp_2[:150]\nmodel_hosp_2 = RandomForestClassifier(n_estimators=30, random_state=1).fit(train_X_hosp_2, train_y_hosp_2)\n","7503ccf1":"hosp_re_data.describe()","f0813bcd":"explainer = shap.TreeExplainer(model_hosp_2)\nshap_values = explainer.shap_values(small_valid_X_hosp_2)\nshap.summary_plot(shap_values[1], small_valid_X_hosp_2)","a4ae27a4":"shap.dependence_plot('num_lab_procedures', shap_values[1], small_valid_X_hosp_2)\nshap.dependence_plot('num_medications', shap_values[1], small_valid_X_hosp_2)","421bcb16":"A good next step is to disentangle the effect of being in certain parts of the city from the effect of total distance traveled.  ","50f0ac4a":"Create Partial Dependence Plot","06fb7231":"<h2>SHAP Values<\/h2>\n- SHapely Additive exPlanations-- break down a predicition to show the impact of each featue\n- Example 1: a model says a bank shouldn't loan someone money, and the bacnk is legally required to explanin the basis for each loan rejection\n- Example 2: a healthcare provider wants to identify what factors are driving each patient's risk of some diseaces fo the van directly address those risk factors with targeted health interventions\n","a796471c":"** Interpreting Permutation Importances**\n- Values with high `Weight` are the most impotant features\n- The first number in each row shows how much model performance decreased with a random shffuling (in this case, using \"accuracy\" as the preformance metric)\n- The number after `\u00b1` measures how performance varied from one-reshuffling to the next\n- Occasionally value of permutation importance could be negative and this indicates that the prediciton on the shuffled data happened to be more accurace that the real data. This will happen when feature didn't matter, but random chance. \n","57266881":"<h2>Step 1<\/h2>\n   - A simple model is built, but the doctor said, model user, doesn't know how to evaluate a model. \n   - S\/He would like further evidence that what the model is performing is in line with thier medical intution. \n   - To address this issue, we need to create a condensed overview of result supported by graphics","1f4e0830":"**Observations:**\n- **num_lab_procedures**: The model seems to think this is a relevant feature. One potential next step would be to explore more by coloring it with different other features to search for an interaction.\n- **num_medications** clearly slopes up until a value of about 20, and then it turns back down.","73a9ef81":"Observations:\n- Adding absolute distance reduced the partial dependence plot of `pickup_longitude`\n- Accounting for the absolute distance traveled reduced the impct of `pickup_longitude` by about 1.5 (max)","4ed85751":"<h2>Step 5<\/h2>\n- Now the doctor is convinced that the data is right, and the model overview looked reasonable. To turn this into a finished product that the doctor can use, lets create a function `patient_risk_factors` that does the following\n    - Takes a single row with patient data\n    - Create a visualization showing what features of that patient increased their risk of readmission, what features decreased it, and how much those features mattered\n    ","5f5b78a3":"**Calculate Permutatio for Taxi Fare Prediction**","f252a5ff":"The shap_values is a list with two arrays\n- The first array in the list is the SHAP values for negative outcome\n- The second array is the list of SHAP values for positive outcome","f7ac4c9c":"**Q1: What is the effect of distribution for each feature?**","45bffcf2":"Distance traveled seems far more importanct than any location effect. Possible reasons latitude feature are more important than longitude features\n    - latitudinal distances in the dataset tend to be larger\n    - it is more expensive to travel a fixed latitudinal distance\n    ","1c3418a7":" For the first model, which variables seem potentially useful for prediciting taxi fares?","15f0aec4":"Note: This notebook was based on Kaggle's **Machine Learning for Insights Challenge** by Dan. I organized it in one notebook for future reference. ","cf24bbc5":"Create a dataset with 2 features and a target, such that the pdp of the first feature is flat, but its permutation importance is high.  We will use a RandomForest for the model.","d2f84dc4":"To predict whether a ream would have a player win the Man of the Game awrd, we could ask \n   - How much was a prediction driven by the fact that the team scored 3 goals? or we can restate this as\n   - How much was a predicition driven by the fact that the team scored 3 goals, **instead of some baceline number of goals**\n  If we answer the question for `number of goals`, we could repeat the process for all other features\n   - SHAP value perform this in a way that guarantees a nice property. When we make prediction\n   - `sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values`","e726f4f3":"<h2>2D Partial Dependence Plots<\/h2>\n\n- It is used for understanding about interactions between features\n","507637af":"<h2>Step 4<\/h2>\n- It appears that `time_in_hospital` doesn't matter at all. The difference between the lowest value on the partial dependence plot and the highest value is about 5%\n- If that is what your model concluded, the doctors will believe it. But it seems so low. Could  the data be wrong, or is your model doing something more complex than they expect?  \n- They'd like you to show them the raw readmission rate for each value of `time_in_hospital` to see how it compares to the partial dependence plot.\n\n    - Make that plot. \n    - Are the results similar or different?","e131c9a2":"<h2>Exercise using the hospotal-readmissions data<\/h2>\n","976d6484":"Increasing feature_of_interest has a more positive impact on predictions when other_feature is high.","dcdb6755":"<h2>SHAP Dependence Contribution Plots<\/h2>\n- Provide an alternative insight to PDP's, but they add a lot more detail\n- It shows the effect of distribution","468e1172":"**Summary plot using SHAP**\nSummary plot is made up of may dots with the following characterstics\n- Vertical location shows what feature it is depicting\n- Color shows whether that feature was high or low for that row of the dataset\n- Horizontal location shows whether the effect of that value caused a higher or lower prediction\n","d19d500d":"<h2>Step 2 <\/h2>\n- From permutation importance, it appears that the `number_inpatient` is an importanct feature. The doctor would like to know more about it. \n- Create a graphic exhibit using PDP to show how `number_inpatient` affect the model preformace","e5851653":"**Observations:**\n- The model ignores the `Red` and `Yellow & Red` features\n- Usually yellow card doesnt affect prediction, but there is an extreme case where a high value caused a much lower prediction\n- High values of `Goal scored` caused higher prediction, and low values caused low predicition ","3ba908ac":"<h1> Machine Learning for Insights<\/h1>\n\n<h2> What are the values of these insights<\/h2> \n- Model debugging\n- Feature engineering\n- Directing furure data collection\n- Informing humman decision-making, for example advertisiers use data insight to help shoppers make purchasing decision \n- Building trust by verifying basic facts about the underlying data","2ef27093":"## The Scenario\nA hospital has struggled with \"readmissions,\" where they release a patient before the patient has recovered enough, and the patient returns with health complications. \n\nThe hospital wants your help identifying patients at highest risk of being readmitted. Doctors (rather than your model) will make the final decision about when to release each patient; but they hope your model will highlight issues the doctors should consider when releasing a patient.\n","b009ccfc":"Both **num_medications** and **num_lab_procedures** share that jumbling of pink and blue dots.\n\nAside from `num_medications` having effects of greater magnitude (both more positive and more negative), it's hard to see a meaningful difference between how these two features affect readmission risk.  Create the SHAP dependence contribution plots for each variable, and describe what you think is different between how these two variables affect predictions.\n\nAs a reminder, here is the code you previously saw to create this type of plot.\n\n    shap.dependence_plot(feature_of_interest, shap_values[1], val_X)\n    \nAnd recall that your validation data is called `small_val_X`.","715d9f62":"Observation:\n-  Dropoff and pickup lat and long are important features.\n- On average, the lattitude features matter more than the longitude features","59e9349d":"<h2> Aggregating SHAP values<\/h2>\n- Aggregating many SHAP values can give more detailed alternatives to permutation importance and partial dependence plots\n- Unlike `Permutation Importance`, SHAP summary plots gives usa bird-eye view od feature importance and what is driving it. ","ed9f3666":"<h2>Permutation Importance<\/h2>\n\nSome of the questions that can be answered by analyzing feature impotance are:\n- Identify important features that will impact model predicition\n- How did each feature in the dataset affect particular prediction accuracy\n- How does each features affect the overall model's predictions\n\n**Feature importance is calculated after a model has been fitted:  **\n- We will ask, if we randomly shuffle a single column of the validation data, leaving the target and all other columns in place, how would that affect the accuracy of prediction in the shuffled data?\n- Randomly re-ordering a single column should cause less accurate predictions, since the resulting data no longer corresponds to anything observed in the real world. \n- Model accuracy especially suffers if we shuffle a column that the model relied on heavily for precition-- this indicates the column is of high importanc\n\n** Process **\n \n 1 Get a trained model\n \n2 Shuffle the values in a single columns, make predictions using the resulting dataset. Calculate the loss function using these predicitons and target values. The preformance deterioration measures the importance of the variable that is being shuffled\n\n3 Undo the shuffling and replete the process for other columns\n    ","57aba6a2":"Random Forest based model suggests that player are more likely to win `PLayer of The Game` if the player run a total of 100km over the course of the game. Running much more than that causes lower predicitions\n\nIn general, the smooth shape of this curve seems more plausible than the steep function from the Decision Tree model. ","43b92a4d":"The team is 70% likely to have a player win the award. ","19966238":"**Conclusion**: Permutation importance is useful for debugging, understanding mode, and communicating a high-level  overview from model. ","ad2b68b2":"Observation  and how to read the ppdp plot:\n- The y-axis is interpreted as **change in the prediction** from what it would be predicted as the baseline or leftmost value\n- A blue shaded area indicates level of confidence\nFrom the graph we see that scoring a goal substantially increase chance of winning `Player of The Game`. However, extra goals beyond that appears to have little impact on predicitions.","7263a9dd":"For FIFA 2018 dataset explore features using Decision Tree","c2a59d6f":"This contour plot shows predicitions for any combination of Goals Scored and Distance covered. For example, the highest predictions is when a team scores at least 1 goal and they run a total distance close to 100km. \n","1ea3e481":"The graph is simple because of the undelying model used. The sample plot with advanced model ","23aecdaa":"Add direct distance measures\n","16f16f33":"## Question \nConsider the following SHAP contribution dependence plot. \n\nThe x-axis shows `feature_of_interest` and the points are colored based on `other_feature`.\n\n![Imgur](https:\/\/i.imgur.com\/zFdHneM.png)\n\nIs there an interaction between `feature_of_interest` and `other_feature`?  \nIf so, does `feature_of_interest` have a more positive impact on predictions when `other_feature` is high or when `other_feature` is low?\n","75c57e94":"<h2>Step 3<\/h2>\n- The doctor thinkns it's a good sign that increasing the number of inpatient procedures leads to increased predicition. \n- From the plot, one can not tell whether the change in the plot is big or small. Add `time_in_hospital` to see hot it compares","7af963be":"Partial Dependence Plots\n- It shows how a feature affects predictions. \n- Useful to answer questions such as\n    - How would similart size house would priced in different areas?\n    - Could predicted difference due to one feature or another\n- It is calculated after a model has been fit-- similar to PermutationImportance, but we **repeatedly alter the value for one variable** to make a series of predictions. ","09495a98":"**Observations:**\n- Each do represent a row of data\n- Horizontal value is the actual value from the dataset, and\n- Vertival value shows what having that horizontal value did to the prediction. The fact that there is a upward slope indicates that `Ball possession` increases the model's prediction for winning the `Man of the Game` award\n- The spread suggests that other features must be interacting with `Ball possession %`\n**In general, possessing the ball increasse a team's chance of having their player win the award. **\n","0e023a19":"Observations:\n- Since we don't have distance measure, coordinate features, such as pickup_longitude, capture the effect of distance.\n- Being picked up near the center of the longitude value lowers predicted fares on average, because it means shorter trip (on average)","69411d15":"**Q2: Which of these featue `diag_1_428`, which has wider range of effect or `payer_code_`?**\nThe width of the effects range is not a reasonable approximation to permutation importance","cfb11afb":"Modify the initialization of `y` so that our PDP plot has a positive slope in the range [-1,1], and a negative slope everywhere else","61db7edd":"Observations from 2PDP:\n- Countour running along a diagonal line, since we a pair of longitudes for pickup and dropoff indicating shorter trips\n- Fare increases as we go further from the central diagonal \n- Fare also increase as we go further to the upper-right of the graph, including staying near the 45-degree line"}}