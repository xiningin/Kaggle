{"cell_type":{"0a6feea6":"code","a85a369d":"code","a157fe81":"code","db576dc4":"code","3dabd3b8":"code","228b9864":"code","782ab950":"code","a9ad0cf4":"code","f078ea55":"code","0283b034":"code","1ab9bef4":"code","107b389a":"code","ab9c48c3":"code","73dd2570":"code","7c72851f":"code","80262267":"code","de962c13":"code","e9de4275":"code","20a54ce2":"code","c53ee2fc":"code","1e4ea294":"code","34327b17":"code","5e3a79bc":"code","15c9e00d":"code","c9f6e7a2":"code","861b6598":"code","f6af4536":"code","023689ab":"code","c7ed386d":"code","5dfb9e7f":"code","625690fe":"code","da933435":"code","4609c91b":"code","2377e10d":"code","a1c6ced7":"code","7492599f":"code","5010c578":"code","21911e82":"code","722654da":"code","fd05c6e7":"code","949a19e4":"code","3f453b2e":"code","7e5ca54d":"code","c9c2471f":"code","5014cb9b":"code","8e66562f":"code","7ae1153a":"code","974a4d57":"code","d1fecb5f":"code","69d5d762":"code","6a89f0af":"code","be52aa37":"code","0f541a50":"code","15948853":"code","a86f4ab5":"code","53a0a9b3":"code","20ebf759":"code","fb569d59":"code","033fb748":"code","eb6c3486":"code","b90d2cca":"code","325b3e0e":"code","5228e221":"code","a81027f0":"code","4adea8de":"code","2df90375":"code","83ebb239":"code","459ffa3a":"code","0d3a940e":"code","0f35e56c":"code","7c54931b":"code","b8f6cfea":"code","54a77755":"code","4e437406":"code","2b4909f6":"code","0a58643a":"code","64bcfdaa":"code","534a09bd":"code","5dfc946e":"code","6d60a0e3":"code","8320cfc2":"code","fef4a026":"code","fde47282":"code","3c787610":"code","1833cba3":"code","abaea867":"code","5c4b9a55":"code","93c8771a":"code","b56eb8da":"code","8d10e737":"code","14a07167":"code","886e21d3":"code","b625e2d2":"code","971e9e23":"code","63af8e86":"code","a35704ff":"code","f94a9c05":"code","795a4499":"code","9a59373f":"markdown","672f3fc7":"markdown","38235fe7":"markdown","e9f73624":"markdown","7399ae8e":"markdown","73320384":"markdown","51183e56":"markdown","c4ec4530":"markdown","e71cd45e":"markdown","a4821fe1":"markdown","ed2c0a25":"markdown","def3b012":"markdown","749f35ab":"markdown","2dca6642":"markdown","0a16fb3a":"markdown","3785809a":"markdown","ed0bcc78":"markdown","09319600":"markdown","588cff24":"markdown","a942a913":"markdown","69129e17":"markdown","2cfa370f":"markdown","c016fc28":"markdown","c96b812a":"markdown","be08a3ef":"markdown","14bf313f":"markdown","30f74ba3":"markdown","bd23460a":"markdown","4901f691":"markdown","109792ef":"markdown","c4e80cd4":"markdown","7d73a070":"markdown","b59a0555":"markdown","aa46b39d":"markdown","d0b50a3f":"markdown","68e0e8bb":"markdown","b7eaa671":"markdown"},"source":{"0a6feea6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport shap\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nfrom itertools import combinations\n\nfrom sklearn import preprocessing\n\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_absolute_error, accuracy_score, balanced_accuracy_score, explained_variance_score\nfrom sklearn.model_selection import train_test_split\n\n# Categorical encoders\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport category_encoders as ce\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a85a369d":"sns.set(style=\"darkgrid\")\n\ntrain_full = pd.read_csv('..\/input\/titanic\/train.csv', index_col='PassengerId')\ntest_full = pd.read_csv('..\/input\/titanic\/test.csv', index_col='PassengerId')\n\ntrain_NA = train_full.isna().sum()\ntest_NA = test_full.isna().sum()\n\npd.concat([train_NA, test_NA], axis=1, sort = False, keys=['Train NA', 'Test NA']).transpose()","a157fe81":"null_fare = test_full[test_full.Fare.isnull()].index[0]\ntest_full.loc[null_fare, 'Fare'] = 0","db576dc4":"train_full.head()","3dabd3b8":"y = train_full.Survived\ntrain_X_full = train_full[train_full.columns.drop('Survived')]\ncombined = pd.concat([train_full, test_full], axis=0)","228b9864":"combined.info()","782ab950":"def alone(df):\n    if ('SibSp' in df.columns) and ('Parch' in df.columns):\n        df['Family'] = df['SibSp'] + df['Parch'] + 1\n    \n    le = LabelEncoder()\n    \n    df['le_Ticket'] = le.fit_transform(df['Ticket'])\n    df['Same_Ticket'] = df.duplicated(['le_Ticket'])\n    df['Alone'] = np.where((df['Family'] > 1) | (df['Same_Ticket']), False, True)\n        \n    plt.figure(figsize=(20, 12))\n    sns.catplot(x=\"Alone\", kind=\"count\", palette=\"ch:.25\", data=df)\n    plt.title('Number of passengers travelling alone or not')\n    \n    df = df[df.columns.drop('SibSp', errors='ignore')]\n    df = df[df.columns.drop('Parch', errors='ignore')]\n    df = df[df.columns.drop('le_Ticket', errors='ignore')]\n    df = df[df.columns.drop('Same_Ticket', errors='ignore')]\n    return df","a9ad0cf4":"combined = alone(combined)\ntrain_X_full = combined[combined.Survived.notnull()]\ntest_full = combined[combined.Survived.isnull()]","f078ea55":"# For data visualization\ntemp = train_X_full.copy()\ntemp['Survived'] = y\n\nsa = temp[(temp.Alone==1) & (temp.Survived==1)].Survived.count()\nda = temp[(temp.Alone==1) & (temp.Survived==0)].Survived.count()\nst = temp[(temp.Alone==0) & (temp.Survived==1)].Survived.count()\ndt = temp[(temp.Alone==0) & (temp.Survived==0)].Survived.count()\nda\nplotdata = pd.DataFrame({\n        'Survived': [sa, st],\n        'Died': [da, dt],\n    }, \n    index=[\"Yes\", \"No\"]\n)\nprint(plotdata)\nplotdata.plot(kind=\"bar\", stacked=True)\nplt.title(\"Survivors travelling alone (or not)\")\nplt.xlabel(\"Alone\")\nplt.ylabel(\"Number of survivors\")","0283b034":"stacked_data = plotdata.apply(lambda x: x*100\/sum(x), axis=1)\nprint(stacked_data)\nstacked_data.plot(kind=\"bar\", stacked=True)\nplt.title(\"Survivors travelling alone (or not) as a percentage\")\nplt.xlabel(\"Alone\")\nplt.ylabel(\"Number of survivors\")","1ab9bef4":"combined.head()","107b389a":"def fix_cabin(df):\n    t = df.Cabin.fillna('U')\n    df['Cabin'] = t.str.slice(0,1)\n    \n    plt.figure(figsize=(20, 15))\n    sns.catplot(x=\"Cabin\", kind=\"count\", palette=\"ch:.25\", data=df)\n    plt.title('Number of passengers per cabin')\n    \n#     le = LabelEncoder()\n#     df['Cabin'] = le.fit_transform(df['Cabin'])","ab9c48c3":"# fix_cabin(train_X_full)\n\nfix_cabin(combined)\ntrain_X_full = combined[combined.Survived.notnull()]\ntest_full = combined[combined.Survived.isnull()]","73dd2570":"temp = train_X_full.copy()\ntemp['Survived'] = y\ntemp['Cabin'].replace({8: 'U', 2: 'C', 4: 'E', 6: 'G', 3: 'D', 0: 'A', 1: 'B', 5: 'F', 7: 'T'}, inplace=True)\nplotdata = pd.DataFrame({'Yes': [], 'No': []})\nyes = []\nno = []\n\nfor i in temp.Cabin.unique():\n    yes.append(temp[(temp.Survived==1) & (temp.Cabin==i)].Cabin.count())\n    no.append(temp[(temp.Survived==0) & (temp.Cabin==i)].Cabin.count())\n\nplotdata['Yes'] = yes\nplotdata['No'] = no\nplotdata.rename(index={0: 'U', 1: 'C', 2: 'E', 3: 'G', 4: 'D', 5: 'A', 6: 'B', 7: 'F', 8: 'T'}, inplace=True)\nplotdata.transpose()","7c72851f":"plotdata.plot(kind=\"bar\", stacked=True)\nplt.title(\"Survivors by Cabin\")\nplt.xlabel(\"Cabin\")\nplt.ylabel(\"Passengers\")","80262267":"stacked_data = plotdata.apply(lambda x: x*100\/sum(x), axis=1)\nprint(stacked_data.sort_values('Yes', ascending=False))\nstacked_data.plot(kind=\"bar\", stacked=True)\nplt.title(\"Survivors by cabin (as a percentage)\")\nplt.xlabel(\"Cabin\")\nplt.ylabel(\"Percentage of survivors\")","de962c13":"temp = train_X_full.copy()\ntemp['Survived'] = y\ntemp['Cabin'].replace({8: 'U', 2: 'C', 4: 'E', 6: 'G', 3: 'D', 0: 'A', 1: 'B', 5: 'F', 7: 'T'}, inplace=True)\nplotdata = pd.DataFrame({'P1': [], 'P2': [], 'P3': []})\np1 = []\np2 = []\np3 = []\n\nfor i in temp.Cabin.unique():\n    p1.append(temp[(temp.Pclass==1) & (temp.Cabin==i)].Cabin.count())\n    p2.append(temp[(temp.Pclass==2) & (temp.Cabin==i)].Cabin.count())\n    p3.append(temp[(temp.Pclass==3) & (temp.Cabin==i)].Cabin.count())\n\nplotdata['P1'] = p1\nplotdata['P2'] = p2\nplotdata['P3'] = p3\nplotdata.rename(index={0: 'U', 1: 'C', 2: 'E', 3: 'G', 4: 'D', 5: 'A', 6: 'B', 7: 'F', 8: 'T'}, inplace=True)\nplotdata.transpose()","e9de4275":"plotdata.plot(kind=\"bar\", stacked=True)\nplt.title(\"Classes by Cabin\")\nplt.xlabel(\"Cabin\")\nplt.ylabel(\"Passenger classes\")","20a54ce2":"stacked_data = plotdata.apply(lambda x: x*100\/sum(x), axis=1)\nprint(stacked_data)\nstacked_data.plot(kind=\"bar\", stacked=True)\nplt.title(\"Classes by cabin (as a percentage)\")\nplt.xlabel(\"Cabin\")\nplt.ylabel(\"Percentage of survivors\")","c53ee2fc":"temp = train_X_full.copy()\ntemp['Survived'] = y\ntemp['Cabin'].replace({8: 'U', 2: 'C', 4: 'E', 6: 'G', 3: 'D', 0: 'A', 1: 'B', 5: 'F', 7: 'T'}, inplace=True)\nplotdata = pd.DataFrame({'S': [], 'C': [], 'Q': []})\ns = []\nc = []\nq = []\n\nfor i in temp.Cabin.unique():\n    s.append(temp[(temp.Embarked=='S') & (temp.Cabin==i)].Cabin.count())\n    c.append(temp[(temp.Embarked=='C') & (temp.Cabin==i)].Cabin.count())\n    q.append(temp[(temp.Embarked=='Q') & (temp.Cabin==i)].Cabin.count())\n\nplotdata['S'] = s\nplotdata['C'] = c\nplotdata['Q'] = q\nplotdata.rename(index={0: 'U', 1: 'C', 2: 'E', 3: 'G', 4: 'D', 5: 'A', 6: 'B', 7: 'F', 8: 'T'}, inplace=True)\nplotdata.transpose()","1e4ea294":"plotdata.plot(kind=\"bar\", stacked=True)\nplt.title(\"Embark location by Cabin\")\nplt.xlabel(\"Cabin\")\nplt.ylabel(\"Passenger embark location\")","34327b17":"stacked_data = plotdata.apply(lambda x: x*100\/sum(x), axis=1)\nprint(stacked_data)\nstacked_data.plot(kind=\"bar\", stacked=True)\nplt.title(\"Embark location by cabin (as a percentage)\")\nplt.xlabel(\"Cabin\")\nplt.ylabel(\"Percentage of survivors\")","5e3a79bc":"def fix_embark(df):\n    most_freq = df.Embarked.mode().iloc[0]\n    df['Embarked'] = df.Embarked.fillna(most_freq)\n    plt.figure(figsize=(20, 12))\n    sns.catplot(x=\"Embarked\", kind=\"count\", palette=\"ch:.25\", data=df)\n    plt.title('Number of passengers by embark location')","15c9e00d":"# fix_embark(train_X_full)\n\nfix_embark(combined)\ntrain_X_full = combined[combined.Survived.notnull()]\ntest_full = combined[combined.Survived.isnull()]","c9f6e7a2":"temp = train_X_full.copy()\ntemp['Survived'] = y\nplotdata = pd.DataFrame({'Yes': [], 'No': []})\nyes = []\nno = []\n\nfor i in temp.Embarked.unique():\n    yes.append(temp[(temp.Survived==1) & (temp.Embarked==i)].Embarked.count())\n    no.append(temp[(temp.Survived==0) & (temp.Embarked==i)].Embarked.count())\n\nplotdata['Yes'] = yes\nplotdata['No'] = no\nplotdata.rename(index={0: 'S', 1: 'C', 2: 'Q'}, inplace=True)\nplotdata.transpose()","861b6598":"plotdata.plot(kind=\"bar\", stacked=True)\nplt.title(\"Survivors by embark location\")\nplt.xlabel(\"Embark location\")\nplt.ylabel(\"Passengers\")","f6af4536":"stacked_data = plotdata.apply(lambda x: x*100\/sum(x), axis=1)\nprint(stacked_data.sort_values('Yes', ascending=False))\nstacked_data.plot(kind=\"bar\", stacked=True)\nplt.title(\"Survivors by embark location (as a percentage)\")\nplt.xlabel(\"Embark location\")\nplt.ylabel(\"Percentage of survivors\")","023689ab":"# def encode_nominal(df, cols):\n#     oh_encoder = OneHotEncoder()\n#     oh_cols = pd.DataFrame(oh_encoder.fit_transform(df[cols]).toarray())\n#     oh_cols.columns = oh_encoder.get_feature_names(cols)\n#     oh_cols.index = df.index\n#     df = df.join(oh_cols)\n#     df = df.drop(columns=cols)\n#     return df","c7ed386d":"# train_X_full = encode_nominal(train_X_full, ['Embarked', 'Sex', 'Alone'])\n# train_X_full.head()","5dfb9e7f":"# Replace 'Name' with 'Title'\ndef extract_title(df):\n    if 'Name' in df.columns:\n        df['Title'] = df['Name'].str.split(',', expand=True)[1].str.split('.', expand=True)[0].str.strip()\n        df = df[df.columns.drop('Name')]\n        \n    df['Title'] = df['Title'].replace({'Ms': 'Miss', 'Mlle': 'Miss',\n                                       'Mme': 'Mrs', 'Lady': 'Mrs', 'the Countess': 'Mrs', 'Dona': 'Mrs',\n                                       'Don': 'Other', 'Rev': 'Other', 'Jonkheer': 'Other', 'Dr': 'Other',\n                                       'Sir': 'Other', 'Col': 'Other', 'Capt': 'Other', 'Major': 'Other'})\n        \n    sns.catplot(kind=\"count\", y=\"Title\", palette=\"ch:.25\", data=df, order=df['Title'].value_counts().index)\n    plt.title('Number of passengers per Title')\n#     le = LabelEncoder()\n#     df['Title'] = le.fit_transform(df['Title'])\n    return df","625690fe":"# train_X_full = extract_title(train_X_full)\n\ncombined = extract_title(combined)\ntrain_X_full = combined[combined.Survived.notnull()]\ntest_full = combined[combined.Survived.isnull()]","da933435":"temp = train_X_full.copy()\ntemp['Survived'] = y\nprint(temp['Title'].unique())\n# titles = temp['Title'].unique()\n# print(titles)","4609c91b":"temp.Title.unique()","2377e10d":"plotdata = pd.DataFrame({'Yes': [], 'No': []})\nyes = []\nno = []\n\nfor i in temp.Title.unique():\n    yes.append(temp[(temp.Survived==1) & (temp.Title==i)].Title.count())\n    no.append(temp[(temp.Survived==0) & (temp.Title==i)].Title.count())\n\nplotdata['Yes'] = yes\nplotdata['No'] = no\nplotdata.rename(index={0: 'Mr', 1: 'Mrs', 2: 'Miss', 3: 'Master', 4: 'Other'}, inplace=True)\nplotdata.transpose()","a1c6ced7":"plotdata.plot(kind=\"bar\", stacked=True)\nplt.title(\"Survivors by title\")\nplt.xlabel(\"Title\")\nplt.ylabel(\"Passengers\")","7492599f":"stacked_data = plotdata.apply(lambda x: x*100\/sum(x), axis=1)\nprint(stacked_data.sort_values('Yes', ascending=False))\nstacked_data.plot(kind=\"bar\", stacked=True)\nplt.title(\"Survivors by title (as a percentage)\")\nplt.xlabel(\"Title\")\nplt.ylabel(\"Percentage of survivors\")","5010c578":"def extract_ticket(df):\n    df['Ticket_Letters'] = df['Ticket'].str.replace('\\d+', '')\n#     df['Ticket_Numbers'] = df['Ticket'].str.replace('[^0-9]', '')\n    df.loc[df['Ticket_Letters']=='','Ticket_Letters'] = 'NA'\n    df.drop(columns=['Ticket'], inplace=True)\n    return df","21911e82":"combined = extract_ticket(combined)\ntrain_X_full = combined[combined.Survived.notnull()]\ntest_full = combined[combined.Survived.isnull()]\ntrain_X_full.head()","722654da":"combined['FareBin'] = pd.qcut(combined['Fare'], 4, labels=['cheap', 'average', 'expensive', 'costly'])\ncombined.head()","fd05c6e7":"sns.catplot(data=combined, x='FareBin', y='Age', hue='Survived')","949a19e4":"train_X_full = combined[combined.Survived.notnull()]\ntest_full = combined[combined.Survived.isnull()]","3f453b2e":"temp = train_X_full.copy()\ntemp['Survived'] = y\nplotdata = pd.DataFrame({'Yes': [], 'No': []})\nyes = []\nno = []\n\nfor i in temp.FareBin.unique():\n    yes.append(temp[(temp.Survived==1) & (temp.FareBin==i)].FareBin.count())\n    no.append(temp[(temp.Survived==0) & (temp.FareBin==i)].FareBin.count())\n\nplotdata['Yes'] = yes\nplotdata['No'] = no\nplotdata.rename(index={0: 'cheap', 1: 'average', 2: 'expensive', 3: 'costly'}, inplace=True)\nplotdata.transpose()\n\nplotdata.plot(kind=\"bar\", stacked=True)\nplt.title(\"Survivors by fare\")\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Passengers\")","7e5ca54d":"stacked_data = plotdata.apply(lambda x: x*100\/sum(x), axis=1)\nprint(stacked_data.sort_values('Yes', ascending=False))\nstacked_data.plot(kind=\"bar\", stacked=True)\nplt.title(\"Survivors by fare (as a percentage)\")\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Percentage of survivors\")","c9c2471f":"r = combined.corr()\nplt.figure(figsize=(10, 10))\nsns.heatmap(r, annot=True, fmt='.2f', cmap=\"YlGnBu\")\nplt.title('Correlations before one-hot encoding')","5014cb9b":"temp = combined.copy()\nto_one_hot = ['Sex', 'Embarked', 'Ticket_Letters', 'Cabin', 'Title', 'Alone', 'FareBin']\n\ntemp = pd.concat([temp, pd.get_dummies(temp[to_one_hot])], axis=1)\ntemp.drop(columns=to_one_hot, axis=1, inplace=True)\ncombined = temp\ncombined.head()","8e66562f":"train_X_full = combined[combined.Survived.notnull()]\ntest_full = combined[combined.Survived.isnull()]","7ae1153a":"combined.info()","974a4d57":"# Convert categorical features to numerical features using label encoder\ndef convert_cat(df):\n    le = LabelEncoder()\n\n    le_train_X = df.copy()\n\n    # Encode categorical features\n    s = df.dtypes=='object'\n    cat_features = list(s[s].index)\n\n    for col in cat_features:\n        le_train_X[col] = le.fit_transform(df[col])\n\n    return le_train_X","d1fecb5f":"def predict_age(df):\n    age_X = df[df.Age.notnull()]\n    \n    age_y = age_X.Age.astype(int)\n    age_X = age_X[age_X.columns.drop('Age')]\n\n    ma_X, va_X, ma_y, va_y = train_test_split(age_X, age_y, random_state=1)\n#     rf_ma_model = RandomForestRegressor(random_state=0).fit(ma_X, ma_y)\n    rf_ma_model = RandomForestRegressor().fit(ma_X, ma_y)\n\n    pred_valid = rf_ma_model.predict(va_X)\n    print('Mean absolute error: %.2f' %mean_absolute_error(pred_valid, va_y))\n    sns.distplot(explained_variance_score(va_y, pred_valid))\n    sns.distplot(pred_valid, kde=False)\n    plt.title('Explained variance')\n    return rf_ma_model","69d5d762":"without_survived = combined.copy()\nwithout_survived.drop(columns=['Survived'], inplace=True)\n# without_survived.loc[null_fare]\nwithout_survived.head()","6a89f0af":"missing_age_model = predict_age(without_survived)","be52aa37":"missing_age = without_survived[without_survived.Age.isnull()]\n# missing_age = missing_age[missing_age.columns.drop('Survived')]\nmissing_age = missing_age[missing_age.columns.drop('Age')]\npred_age = missing_age_model.predict(missing_age)\noutput_age = pd.DataFrame({'Age': pred_age}, index=missing_age.index)\noutput_age.transpose()","0f541a50":"sns.distplot(output_age.Age, kde=False)\nplt.title('Predicted missing ages')","15948853":"sns.distplot(combined.Age, kde=False)\nplt.title('Non-missing ages in training data set')","a86f4ab5":"pd.DataFrame(combined.Age).transpose()","53a0a9b3":"test = combined.copy()\ntest = test.combine_first(output_age)\npd.DataFrame(test.Age).transpose()","20ebf759":"combined = test\ntrain_X_full = combined[combined.Survived.notnull()]\ntest_full = combined[combined.Survived.isnull()]","fb569d59":"y = train_X_full.Survived\ntrain_X_full = train_X_full.drop(columns=['Survived'], axis=1)\ntrain_X, valid_X, train_y, valid_y = train_test_split(train_X_full, y, random_state=5)","033fb748":"results = {}\n\nfor i in range(1,9):\n    rf_model = RandomForestClassifier(n_estimators=i*50,random_state=0).fit(train_X, train_y)\n    pred_valid = rf_model.predict(valid_X)\n    results[i*50] = accuracy_score(pred_valid, valid_y)\n    print('%d Mean absolute error: \\t%.4f' %(i*50, mean_absolute_error(pred_valid, valid_y)))\n    print('Accuracy score: \\t%.4f' %accuracy_score(valid_y, pred_valid))\n    \nplt.plot(list(results.keys()), list(results.values()))\nplt.xlabel('# of trees')\nplt.ylabel('Accuracy score')\nplt.show()","eb6c3486":"def scores(results):\n    key_min = min(results.keys(), key=(lambda k: results[k]))\n    key_max = max(results.keys(), key=(lambda k: results[k]))\n    \n    print('Highest score at %d of %.4f' %(key_max, results[key_max]))\n    print('Lowest score at %d of %.4f' %(key_min, results[key_min]))\n    return key_max, key_min","b90d2cca":"print('Number of trees: ')\nhigh, low = scores(results)","325b3e0e":"best_rf_model = RandomForestClassifier(n_estimators=high,random_state=0).fit(train_X, train_y)\npred_valid1 = best_rf_model.predict(valid_X)\nprint('Mean absolute error: \\t%.4f' %mean_absolute_error(pred_valid1, valid_y))\nprint('Accuracy score: \\t%.4f' %accuracy_score(valid_y, pred_valid1))","5228e221":"perm = PermutationImportance(best_rf_model, random_state=1).fit(valid_X, valid_y)\neli5.show_weights(perm, feature_names=valid_X.columns.tolist())","a81027f0":"row_to_show = 4\ndata_for_prediction = valid_X.iloc[row_to_show]\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\nbest_rf_model.predict_proba(data_for_prediction_array)\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(best_rf_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\n\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","4adea8de":"row_to_show = 6\ndata_for_prediction = valid_X.iloc[row_to_show]\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\nbest_rf_model.predict_proba(data_for_prediction_array)\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(best_rf_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\n\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","2df90375":"# Calculate Shap values\nshap_values_all = explainer.shap_values(valid_X)\n\nshap.summary_plot(shap_values_all[1], valid_X)","83ebb239":"feature_cols = train_X.columns\nselector = SelectKBest(f_classif, k=len(train_X.columns))\nX_new = selector.fit_transform(train_X[feature_cols], train_y)\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=train_X.index, \n                                 columns=feature_cols)\nselected_features.head()\nselected_cols = selected_features.columns[selected_features.var() != 0]\nselected_cols","459ffa3a":"results1 = {}\n\nfor i in range(1, len(train_X.columns)):\n    selector = SelectKBest(f_classif, k=i)\n    X_new = selector.fit_transform(train_X[feature_cols], train_y)\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                     index=train_X.index, \n                                     columns=feature_cols)\n    selected_cols = selected_features.columns[selected_features.var() != 0]\n    kbest_X = train_X[selected_cols]\n    kval_X = valid_X[selected_cols]\n    rf_model = RandomForestClassifier(n_estimators=high,random_state=0).fit(kbest_X, train_y)\n    pred_valid = rf_model.predict(kval_X)\n    results1[i] = accuracy_score(pred_valid, valid_y)\n    \nplt.plot(list(results1.keys()), list(results1.values()))\nplt.xlabel('# of features')\nplt.ylabel('Accuracy score')\nplt.show()","0d3a940e":"high1, low1 = scores(results1)","0f35e56c":"results2 = {}\n\nfor i in range(1, len(train_X.columns)):\n    selector = SelectKBest(f_classif, k=i)\n    X_new = selector.fit_transform(train_X[feature_cols], train_y)\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                     index=train_X.index, \n                                     columns=feature_cols)\n    selected_cols = selected_features.columns[selected_features.var() != 0]\n    kbest_X = train_X[selected_cols]\n    kval_X = valid_X[selected_cols]\n    rf_model = RandomForestClassifier(n_estimators=low,random_state=0).fit(kbest_X, train_y)\n    pred_valid = rf_model.predict(kval_X)\n    results2[i] = accuracy_score(pred_valid, valid_y)\n    \nplt.plot(list(results2.keys()), list(results2.values()))\nplt.xlabel('# of features')\nplt.ylabel('Accuracy score')\nplt.show()","7c54931b":"high2, low2 = scores(results2)","b8f6cfea":"results3 = {}\n\nfor i in range(1, len(train_X.columns)):\n    selector = SelectKBest(f_classif, k=i)\n    X_new = selector.fit_transform(train_X[feature_cols], train_y)\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                     index=train_X.index, \n                                     columns=feature_cols)\n    selected_cols = selected_features.columns[selected_features.var() != 0]\n    kbest_X = train_X[selected_cols]\n    kval_X = valid_X[selected_cols]\n    log_model = LogisticRegression(random_state=0).fit(kbest_X, train_y)\n    pred_valid = log_model.predict(kval_X)\n    results3[i] = accuracy_score(pred_valid, valid_y)\n    \nplt.plot(list(results3.keys()), list(results3.values()))\nplt.xlabel('# of features')\nplt.ylabel('Accuracy score')\nplt.show()","54a77755":"high3, low3 = scores(results3)","4e437406":"xg_model = xgb.XGBClassifier()\n\nresults4 = {}\n\nfor i in range(1, len(train_X.columns)):\n    selector = SelectKBest(f_classif, k=i)\n    X_new = selector.fit_transform(train_X[feature_cols], train_y)\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                     index=train_X.index, \n                                     columns=feature_cols)\n    selected_cols = selected_features.columns[selected_features.var() != 0]\n    kbest_X = train_X[selected_cols]\n    kval_X = valid_X[selected_cols]\n    xg_model = xgb.XGBClassifier().fit(kbest_X, train_y)\n    pred_valid = xg_model.predict(kval_X)\n    results4[i] = accuracy_score(pred_valid, valid_y)\n    \nplt.plot(list(results4.keys()), list(results4.values()))\nplt.xlabel('# of features')\nplt.ylabel('Accuracy score')\nplt.show()","2b4909f6":"high4, low4 = scores(results4)","0a58643a":"feature_cols = train_X.columns\n\nselector = SelectKBest(f_classif, k=high1)\nX_new = selector.fit_transform(train_X[feature_cols], train_y)\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=train_X.index, \n                                 columns=feature_cols)\nselected_cols = selected_features.columns[selected_features.var() != 0]\nkbest_X = train_X[selected_cols]\nkval_X = valid_X[selected_cols]\n\nbest_k_model = RandomForestClassifier(n_estimators=high,random_state=0).fit(kbest_X, train_y)\n# best_k_model = LogisticRegression(random_state=0).fit(kbest_X, train_y)\npred_valid1 = best_k_model.predict(kval_X)\nprint('Mean absolute error: \\t%.4f' %mean_absolute_error(pred_valid1, valid_y))\nprint('Accuracy score: \\t%.4f' %accuracy_score(valid_y, pred_valid1))","64bcfdaa":"feature_cols = train_X.columns\n\nselector = SelectKBest(f_classif, k=high4)\nX_new = selector.fit_transform(train_X[feature_cols], train_y)\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=train_X.index, \n                                 columns=feature_cols)\nselected_cols2 = selected_features.columns[selected_features.var() != 0]\nkbest_X = train_X[selected_cols2]\nkval_X = valid_X[selected_cols2]\n\nxg_best_model = xgb.XGBClassifier().fit(kbest_X, train_y)\n# best_k_model = LogisticRegression(random_state=0).fit(kbest_X, train_y)\npred_valid1 = xg_best_model.predict(kval_X)\nprint('Mean absolute error: \\t%.4f' %mean_absolute_error(pred_valid1, valid_y))\nprint('Accuracy score: \\t%.4f' %accuracy_score(valid_y, pred_valid1))","534a09bd":"selected_cols","5dfc946e":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\n# cv_X = pd.concat([train_X[selected_cols], valid_X[selected_cols]])\n# cv_y = pd.concat([train_y, valid_y])\ncv_X = train_X_full[selected_cols]\ncv_y = y\ncv_X.head()","6d60a0e3":"cv_results = {}\nfor i in range(2, 10):\n    cv_score = cross_val_score(xg_best_model, cv_X, cv_y, cv=i)\n#     print('Cross-validation score: %s' %(cv_score))\n    cv_results[i] = cv_score.mean()","8320cfc2":"plt.plot(list(cv_results.keys()), list(cv_results.values()))\nplt.xlabel('# of folds')\nplt.ylabel('Score (average)')\nplt.show()","fef4a026":"cv_high, cv_low = scores(cv_results)","fde47282":"cv_score = cross_val_score(best_k_model, cv_X, cv_y, cv=cv_high)\nprint('Mean cross-validation score: %.2f' %(cv_score.mean()*100))","3c787610":"feature_cols = train_X.columns\n\nselector = SelectKBest(f_classif, k=high3)\nX_new = selector.fit_transform(train_X[feature_cols], train_y)\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=train_X.index, \n                                 columns=feature_cols)\nselected_cols1 = selected_features.columns[selected_features.var() != 0]\nkbest_X1 = train_X[selected_cols1]\nkval_X1 = valid_X[selected_cols1]\n\nbest_k_model1 = LogisticRegression(random_state=0).fit(kbest_X1, train_y)\npred_valid1 = best_k_model1.predict(kval_X1)\nprint('Mean absolute error: \\t%.4f' %mean_absolute_error(pred_valid1, valid_y))\nprint('Accuracy score: \\t%.4f' %accuracy_score(valid_y, pred_valid1))\n\ncv_X = pd.concat([train_X, valid_X])\ncv_y = pd.concat([train_y, valid_y])\ncv_X.head()","1833cba3":"cv_results = {}\nfor i in range(2, 15):\n    cv_score = cross_val_score(best_k_model1, cv_X, cv_y, cv=i)\n#     print('Cross-validation score: %s' %(cv_score))\n    cv_results[i] = cv_score.mean()\n    \nplt.plot(list(cv_results.keys()), list(cv_results.values()))\nplt.xlabel('# of folds')\nplt.ylabel('Score (average)')\nplt.show()","abaea867":"cv_high, cv_low = scores(cv_results)","5c4b9a55":"cv_score = cross_val_score(best_k_model1, cv_X, cv_y, cv=cv_high)\nprint('Mean cross-validation score: %.2f' %(cv_score.mean()*100))","93c8771a":"test_full.drop(columns=['Survived'], axis=0, inplace=True)","b56eb8da":"test_full.info()","8d10e737":"len(selected_cols)","14a07167":"ktest_X = test_full[selected_cols]\nselected_cols","886e21d3":"ktest_X.head()","b625e2d2":"pred = best_k_model.predict(ktest_X).astype('int')\npred","971e9e23":"output = pd.DataFrame({'PassengerId': test_full.index,\n                       'Survived': pred})\noutput.to_csv('survived.csv', index=False)","63af8e86":"ktest_X1 = test_full[selected_cols1]\npred1 = best_k_model1.predict(ktest_X1).astype('int')\noutput = pd.DataFrame({'PassengerId': test_full.index,\n                       'Survived': pred1})\noutput.to_csv('survived1.csv', index=False)","a35704ff":"pred1","f94a9c05":"ktest_X2 = test_full[selected_cols2]\npred2 = xg_best_model.predict(ktest_X2).astype('int')\noutput = pd.DataFrame({'PassengerId': test_full.index,\n                       'Survived': pred2})\noutput.to_csv('survived2.csv', index=False)","795a4499":"pred2","9a59373f":"# Test Predictions\n\nThe model will use these features to predict whether or not a passenger survived:","672f3fc7":"**Random Forest Classifier**\n\nStarting at 50 n_estimators, increasing by 50 up to 400, evaluate the mae and accuracy score using RandomForestClassifier:","38235fe7":"# Training dataset cleanup\n\nBefore any cleanup, these are what the rows and columns in the training data set look like:","e9f73624":"**Ticket extraction**","7399ae8e":"Using SelectKBest and starting with 1 feature, increasing by 1 up to the total number of features, evaluate the mae and accuracy score using RandomForestClassifier with the best n_estimator from above:","73320384":"Separate the y and x values:","51183e56":"Below is a comparison of the 'Age' feature before and after the null ages have been replaced:","c4ec4530":"Look at SHAP values for all rows and features:","e71cd45e":"Below, for those travelling alone, only 28.8% survived. While 50.3% of those not travelling alone survived.","a4821fe1":"Fill in missing embark values with the most frequent value:","ed2c0a25":"**Number of passengers per cabin**","def3b012":"The predictions for the test data:","749f35ab":"# SHAP Values\n\nFor a specific variable in the feature, is it increasing or decreasing this person's chance of survival? And by how much? Look at SHAP value for a random person who did not survive first, then someone who did not survive.","2dca6642":"Finally, a csv is created to output the ID of the passenger and a prediction of whether or not they survived:","0a16fb3a":"# Setup","3785809a":"From the data on survivors by cabin: D, E, B, F, C, and G all had 50% or more passengers who survived in descending order. Of the passengers whose cabin deck was unknown, 69.7% were from class 3, 24.5% from class 2 and 5.8% from class 1.","ed0bcc78":"## Picking the best model","09319600":"Here are the number of survivors who survived based on whether or not they were travelling alone:","588cff24":"This function converts categorical features using label encoding.","a942a913":"# Reading Data Files\n\nThe number of missing values in each file are:","69129e17":"## XGB Classification","2cfa370f":"## Random Forest Classifier\n\nUsing SelectKBest and starting with 1 feature, increasing by 1 up to the total number of features, evaluate the mae and accuracy score using RandomForestClassifier with the worst n_estimator from above:","c016fc28":"This function will fill in the missing cabin values with 'U' for unknown and then replace the entire column's rows with the first letter of the first cabin. Finally it will label encode the 'Cabin' column.","c96b812a":"**Correlation**","be08a3ef":"Here are the predictions for the passengers with missing ages:","14bf313f":"Grab the titles from the name and drop the name column","30f74ba3":"**Logistic Regression with Cross validation**","bd23460a":"## Predicting the missing age values\n\nFor the training data set, the other features will be used to predict the missing age values using LinearRegression.","4901f691":"# SelectKBest","109792ef":"# Feature Engineering","c4e80cd4":"## Cross-validation","7d73a070":"# Selecting a model for Predictions","b59a0555":"**XGBoost**","aa46b39d":"**Logistic Regression**","d0b50a3f":"Combine the siblings\/spouses with the parent\/children into one feature called family (+1 for self) and drop both the columns. Pase the 'Ticket' feature and see if two people have the same ticket number. Then create a feature 'Alone' based on the family size feature, 'Family', and 'Same_Ticket' feature.","68e0e8bb":"## Logistic Regression","b7eaa671":"**Quick one-hot encode**"}}