{"cell_type":{"e490611d":"code","78b13870":"code","aa09541b":"code","0a7ef548":"code","03b99d02":"code","438fa3aa":"code","1211b337":"code","4c6df916":"code","2c57b7c8":"code","9872bd01":"code","b3ae4aad":"code","1b47e878":"markdown","b5a4e1c1":"markdown","7fd472c9":"markdown","d5e0c69d":"markdown","0396604b":"markdown","9620b732":"markdown","bcbd8592":"markdown","2baee72f":"markdown","58b75b71":"markdown","c58d5cd1":"markdown","fe1decec":"markdown"},"source":{"e490611d":"'''\nCustom deep learing model for Gachon Deep Learning Class Competition\nhttps:\/\/www.kaggle.com\/c\/gachon-deeplearning\nCreated by Team I: [LeeNamJun, YeoJunKu, ChoSoYeong, ChoiJiWon]\n\nThe primary logic of our approach is find out the best combination of element of model (hyper-parameters, optimizers, batch size, ...)\nwith less epoch, then train model with the combination with more epoch\n\nUsing datasets for personal purposes is strongly prohibited \n'''","78b13870":"import torch\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nimport numpy as np\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nimport random\nimport copy\nfrom itertools import product\nimport time\n\nUSE_CUDA = torch.cuda.is_available()\nDEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\nprint(DEVICE)\n\nEPOCH = 25","aa09541b":"transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.486], std=[0.229, 0.224, 0.225])\n])","0a7ef548":"# Class for load data from directory\nclass MyDataset(Dataset):\n    def __init__(self, image_dir, label, transforms=None, test=False):\n        self.image_dir = image_dir\n        self.label = label\n        self.image_list = os.listdir(self.image_dir)\n        self.transforms = transforms\n        self.test_mode = test\n    \n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self,idx):\n        image_name = os.path.join(self.image_dir, self.image_list[idx])\n        image = cv2.imread(image_name)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (224,224), cv2.INTER_AREA)\n        \n        image = transforms(image)\n       \n        if self.test_mode:\n            return (image, self.image_list[idx])\n        else:\n            return (image, self.label)","03b99d02":"# Class for load data from directory with augmentation by fliping images\nclass MyDataset_aug(Dataset):\n    def __init__(self, image_dir, label, transforms=None, test=False):\n        self.image_dir = image_dir\n        self.label = label\n        self.image_list = os.listdir(self.image_dir)\n        self.transforms = transforms\n        self.test_mode = test\n    \n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self,idx):\n        image_name = os.path.join(self.image_dir, self.image_list[idx])\n        image = cv2.imread(image_name)\n        flipLR_img = np.fliplr(image) # Flip original image\n        flipLR_img = cv2.cvtColor(flipLR_img, cv2.COLOR_BGR2RGB)\n        flipLR_img = cv2.resize(flipLR_img, (224,224), cv2.INTER_AREA)\n        \n        flipLR_img = transforms(flipLR_img)\n        \n        if self.test_mode:\n            return (flipLR_img, self.image_list[idx])\n        else:\n            return (flipLR_img, self.label)","438fa3aa":"path = \"..\/input\/yoga6classes\/\"\n\nbalancing_train = MyDataset(path + \"balancing_train\", 0, transforms)\nbalancing_train_aug = MyDataset_aug(path + \"balancing_train\", 0, transforms)\n\ninverted_train = MyDataset(path + \"inverted_train\", 1, transforms)\ninverted_train_aug = MyDataset_aug(path + \"inverted_train\", 1, transforms)\n\nreclining_train = MyDataset(path + \"reclining_train\", 2, transforms)\nreclining_train_aug = MyDataset_aug(path + \"reclining_train\", 2, transforms)\n\nsitting_train = MyDataset(path + \"sitting_train\", 3, transforms)\nsitting_train_aug = MyDataset_aug(path + \"sitting_train\", 3, transforms)\n\nstanding_train = MyDataset(path + \"standing_train\", 4, transforms)\nstanding_train_aug = MyDataset_aug(path + \"standing_train\", 4, transforms)\n\nwheel_train = MyDataset(path + \"wheel_train\", 5, transforms)\nwheel_train_aug = MyDataset_aug(path + \"wheel_train\", 5, transforms)\n\n\nbalancing_valid = MyDataset(path + \"balancing_valid\", 0, transforms)\ninverted_valid = MyDataset(path + \"inverted_valid\", 1, transforms)\nreclining_valid = MyDataset(path + \"reclining_valid\", 2, transforms)\nsitting_valid = MyDataset(path + \"sitting_valid\", 3, transforms)\nstanding_valid = MyDataset(path + \"standing_valid\", 4, transforms)\nwheel_valid = MyDataset(path + \"wheel_valid\", 5, transforms)\n\n# Concat original images and augmented images\ntrain_set = ConcatDataset([balancing_train, inverted_train, reclining_train, sitting_train, standing_train, wheel_train,\n                          balancing_train_aug, inverted_train_aug, reclining_train_aug, sitting_train_aug, standing_train_aug, wheel_train_aug])\n# Concat original images\nval_set = ConcatDataset([balancing_valid, inverted_valid, reclining_valid, sitting_valid, standing_valid, wheel_valid])\n\nprint(\"Number of Training set images : \", len(train_set))\nprint(\"Number of Validation set images : \", len(val_set))","1211b337":"model = models.resnet18(pretrained=True) # Using pretrained ResNet18\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 6) # Define fully connected layer\nmodel = model.to(DEVICE)\n\nprint(\"create model\")","4c6df916":"# Function for train model\ndef train(model, train_loader, optimizer, epoch):\n    model.train()\n    for i, (image, target) in enumerate(train_loader):\n        image, target = image.to(DEVICE), target.to(DEVICE)\n        output = model(image)\n \n        optimizer.zero_grad()\n        loss = F.cross_entropy(output, target).to(DEVICE)\n        loss.backward()\n        optimizer.step()     \n        if i % 10 == 0:\n            print('Train Epoch : {} [{}\/{} ({:.0f})%]\\tLoss: {:.6f}'\n                 .format(epoch, i*len(image), len(train_loader.dataset), 100.*i \/ len(train_loader), loss.item()))\n\n# Function for validate model\ndef evaluate(model, val_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for(image, target) in val_loader:\n            image,target = image.to(DEVICE), target.to(DEVICE)\n            output = model(image)\n            \n            test_loss += F.cross_entropy(output, target, reduction = 'sum').item()\n            pred = output.max(1, keepdim = True)[1]\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            \n    test_loss \/= len(val_loader.dataset)\n    test_accuracy = 100. * correct \/ len(val_loader.dataset)\n    return test_loss, test_accuracy\n","2c57b7c8":"# Configuration set for tuner\nconfig = {\n    \"lr\": [0.001, 0.005, 0.0005],\n    \"batch_size\": [64, 32, 16, 8],\n    \"optimizer\": ['SGD','Adagrad', 'Adam']\n}\n\n'''\nTuner to find out the best combination of hyper parameters and optimizer\nStrategy: Find the right combination with less epoch and train it back to that combination with more epoch\n'''\nclass Tuner:\n    def __init__(self, model, train_data, test_data, config, epoch):\n        self.model = model\n        self.train_data = train_data\n        self.test_data = test_data\n        self.config = config\n        self.epoch = epoch\n        self.combinations = list(product(config[\"lr\"], config[\"batch_size\"], config[\"optimizer\"]))\n        self.best_combination = self.tune_parameters()\n        \n    # Find out the best combination of elements\n    def tune_parameters(self):\n        best_combination = {\"combination\": [], \"acc\": 0.0}\n        best_loss = 1\n        best_epoch = 0\n        best_model = copy.deepcopy(self.model)\n        \n        while len(self.combinations) >= 1:\n            combi = self.combinations.pop()\n            compare_model, optimizer, train_loader, val_loader = self.make_model(combi)\n            \n            # Training model with a combination\n            print('Training combination: ', combi, ' ...')\n            best = 0\n            for epoch in range(self.epoch):\n                train(compare_model, train_loader, optimizer, epoch)\n                test_loss, test_accuracy = evaluate(compare_model, val_loader)\n                if test_accuracy > best:\n                    best = test_accuracy\n                    best_loss = test_loss\n                    best_epoch = epoch\n                print('[{}] Test Loss : {:.4f}, Accuracy : {:.4f}%'.format(epoch, test_loss, test_accuracy))\n\n                # Stop epoch if it doesn't renew more than 5 epochs based on lowest loss\n                if epoch > best_epoch+5 and test_loss > best_loss:\n                    break\n\n            # Deciding best model\n            if best > best_combination[\"acc\"]:\n                best_combination[\"acc\"] = best\n                best_combination[\"combination\"] = combi\n                best_model = compare_model\n\n            print('---------------------------------------------------------')\n            print(\"finish train a combination\")\n            print(\"current best acc: \", best_combination[\"acc\"], \"with combination: \", \"lr: {}, batch_size: {}, optimizer: {}\".format(best_combination[\"combination\"][0], best_combination[\"combination\"][1], best_combination[\"combination\"][2]))\n        \n        model = best_model\n        torch.save(model.state_dict(), \".\/best_model.pth\")\n        return best_combination[\"combination\"]\n        \n    # Make a set of model elements\n    def make_model(self, combi):\n        # Deep Copy original model to training to initialize weights at every combinations\n        compare_model = copy.deepcopy(self.model)\n        compare_model = compare_model.to(DEVICE)\n        \n        # Make Optimizer\n        if(combi[2] == 'SGD'):\n            optimizer = optim.SGD(compare_model.parameters(), lr=combi[0], momentum = 0.9, nesterov=True)\n        elif(combi[2] == 'Adagrad'):\n            optimizer = optim.Adagrad(compare_model.parameters(), lr=combi[0])\n        elif(combi[2] == 'Adam'):\n            optimizer = optim.Adam(compare_model.parameters(), lr=combi[0])\n        \n        # Make Loader\n        train_loader = DataLoader(self.train_data, batch_size=combi[1], shuffle=True)\n        val_loader = DataLoader(self.test_data, batch_size=combi[1], shuffle=False)\n        \n        return compare_model, optimizer, train_loader, val_loader\n    \n# Get the best combination of hyper paramers and optimizer\ncombi = Tuner(model, train_set, val_set, config, 10).best_combination","9872bd01":"start = time.time()\nbest = 0\nprint(combi)\n# Decide optimizer to use\nif(combi[2] == 'SGD'):\n    optimizer = optim.SGD(model.parameters(), lr=combi[0], momentum = 0.9, nesterov=True)\n\nif(combi[2] == 'Adagrad'):\n    optimizer = optim.Adagrad(model.parameters(), lr=combi[0])\n    \nif(combi[2] == 'Adam'):\n    optimizer = optim.Adam(model.parameters(), lr=combi[0])\n\nBATCH_SIZE = combi[1]\n\n# Make loaders\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Train\nfor epoch in range(EPOCH):\n    train(model, train_loader, optimizer, epoch)\n    test_loss, test_accuracy = evaluate(model, val_loader)\n    if test_accuracy > best:\n        best = test_accuracy\n        torch.save(model.state_dict(), \".\/best_model.pth\")\n    print('[{}] Test Loss : {:.4f}, Accuracy : {:.4f}%'.format(epoch, test_loss, test_accuracy))\n\nend = time.time()\ntime = end - start\n\n# Print results\nprint(\"finish train\")\nprint(\"best acc: \", best)\nprint(\"time: {}h, {}m,{}s\".format(int(time\/3600), int(time\/60), time%60))\n","b3ae4aad":"import csv\n\nload = torch.load('.\/best_model.pth')\nmodel.load_state_dict(load) # load weights from the best model\nmodel.eval()\nprint(\"load model for test set\")\n\nf = open(\".\/prediction.csv\", \"w\", newline=\"\") # open file to save prediction results\nw = csv.writer(f)\nw.writerow(['id', 'target'])\n\n# Make test data loader\ntest_set = MyDataset(\"..\/input\/yogatestdataset\", 0,transforms, test = True)\ntest_loader = DataLoader(test_set, batch_size=combi[1], shuffle=False)\n\npreds = []\nimg_ids = []\ncorrect = 0\n\n# Predict class with test data\nwith torch.no_grad():\n    for (image, image_name) in test_loader:\n        image = image.to(DEVICE)\n        output = model(image)\n        \n        pred = output.max(1, keepdim = True)[1]\n        preds.extend(pred)\n        img_ids.extend(image_name)\n        \n# Write results to .csv\nfor i in range(600):\n    w.writerow([img_ids[i][:-4], str(preds[i].item())])\n    \nf.close()\nprint(\"save prediction csv\")","1b47e878":"# Transforms","b5a4e1c1":"# Train code & Loss function & test code & evaluation code","7fd472c9":"# Custom Dataset","d5e0c69d":"# Tuner","0396604b":"# Gachon Deep Learning - Team I","9620b732":"# Main code & Save the best model","bcbd8592":"# Build model","2baee72f":"# Save the predicted value as a CSV file","58b75b71":"# ConcatDataset & DataLoader","c58d5cd1":"# Augmentation","fe1decec":"# Import requirements"}}