{"cell_type":{"21b7e442":"code","fc7a4db5":"code","eaedd5db":"code","8e9cbc51":"code","69549ae0":"code","43da8d24":"code","4441990e":"code","e3731907":"code","a3d3306d":"code","fa6e51eb":"code","e8eb137c":"code","ab2f868e":"code","6c281500":"code","9b9c6a76":"code","39610071":"code","e058ca89":"code","2857e780":"code","97b4ee03":"code","8bd8c95e":"code","f43dc311":"code","26ab9ed8":"code","91a0b735":"code","63c964ed":"code","1f3d3ed8":"markdown","e2ce4ce6":"markdown","83d585a3":"markdown","f7955075":"markdown","36f958ed":"markdown","80cc5450":"markdown","8fb823f4":"markdown","4eaebd7f":"markdown","58284a5e":"markdown","1893cbbb":"markdown","fee25cf0":"markdown"},"source":{"21b7e442":"from sklearn.utils import all_estimators\n\nestimators = all_estimators()\n\nfor name, class_ in estimators:\n    if hasattr(class_, 'predict_proba'):\n        print(name)","fc7a4db5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eaedd5db":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import f1_score, accuracy_score, log_loss\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold, cross_val_score, StratifiedShuffleSplit, KFold\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.rcParams['figure.figsize'] = [20, 6]\nplt.style.use('seaborn-darkgrid')","8e9cbc51":"train = pd.read_csv('\/kaggle\/input\/machinehack-flower-class-recognition\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/machinehack-flower-class-recognition\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/machinehack-flower-class-recognition\/sample_submission.csv')\ntrain.columns = train.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\ntest.columns = test.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n\nprint('Train Data shape: ', train.shape, 'Test Data shape: ', test.shape)\n\ntrain.head(10)","69549ae0":"cat_cols = train.columns[~(train.columns.isin(['class']))].tolist()\ntest.head()","43da8d24":"train.nunique()","4441990e":"test.nunique()","e3731907":"train.isnull().sum()","a3d3306d":"i = 1\nfor column in train.columns[~(train.columns.isin(['area_code', 'region_code', 'height', 'diameter']))].tolist():\n    plt.figure(figsize = (80, 10))\n    plt.subplot(3, 3, i)\n    sns.barplot(x = train[column].value_counts().index, y = train[column].value_counts())\n    i += 1\n    plt.show()","fa6e51eb":"# Unique areas in train and test set\n\ntrain_area = train[['area_code']].drop_duplicates(subset = None, keep = 'first', inplace = False)\nprint('Unique areas in train data: ', len(train_area))\ntest_prod = test[['area_code']].drop_duplicates(subset = None, keep = 'first', inplace = False)\nprint('Unique areas in test data: ', len(test_prod))\n\n# Unique pairs of area and locality in train and test set\n\ntrain_area_locality = train[['area_code', 'locality_code']].drop_duplicates(subset = None, keep = 'first', inplace = False)\nprint('Unique locality - area pairs in train data: ', len(train_area_locality))\n\ntest_area_locality = test[['area_code', 'locality_code']].drop_duplicates(subset = None, keep = 'first', inplace = False)\nprint('Unique locality - area pairs in test data: ', len(test_area_locality))\n\n# Unique pairs of region and locality in train and test set\n\ntrain_region_locality = train[['region_code', 'locality_code']].drop_duplicates(subset = None, keep = 'first', inplace = False)\nprint('Unique locality - region pairs in train data: ', len(train_region_locality))\n\ntest_region_locality = test[['region_code', 'locality_code']].drop_duplicates(subset = None, keep = 'first', inplace = False)\nprint('Unique locality - region pairs in test data: ', len(test_region_locality))\n\n# Unique pairs of region and area in train and test set\n\ntrain_area_region = train[['area_code', 'region_code']].drop_duplicates(subset = None, keep = 'first', inplace = False)\nprint('Unique region - area pairs in train data: ', len(train_area_region))\n\ntest_area_region = test[['area_code', 'region_code']].drop_duplicates(subset = None, keep = 'first', inplace = False)\nprint('Unique region - area pairs in test data: ', len(test_area_region))\n\n# Unique pairs of area, ocality and region in train and test set\n\ntrain_area_region_loc = train[['area_code', 'region_code', 'locality_code']].drop_duplicates(subset = None, keep = 'first', inplace = False)\nprint('Unique area - region - locality pairs in train data: ', len(train_area_region_loc))\n\ntest_area_region_loc = test[['area_code', 'region_code', 'locality_code']].drop_duplicates(subset = None, keep = 'first', inplace = False)\nprint('Unique area - region - locality pairs in test data: ', len(test_area_region_loc))","e8eb137c":"train['type'] = 'train'\ntest['type'] = 'test'\n\nmaster = pd.concat([train, test])\nmaster.head()","ab2f868e":"master['diameter'] = master['diameter'].apply(lambda x: 0.5 if x == 0 else x)\nmaster['height'] = master['height'].apply(lambda x: 0.1 if x == 0 else x)","6c281500":"i = 1\nfor column in master.columns[~(master.columns.isin(['area_code', 'region_code', 'height', 'diameter', 'type']))].tolist():\n    plt.figure(figsize = (80, 10))\n    plt.subplot(3, 3, i)\n    sns.barplot(x = master[column].value_counts().index, y = master[column].value_counts())\n    i += 1\n    plt.show()","9b9c6a76":"# grouping by frequency of species\nspecies_fq = master.groupby('species').size()\/len(master)\nmaster.loc[:, \"{}_freq\".format('species')] = master['species'].map(species_fq)\n\n# grouping by frequency of area, region, locality\narea_fq = master.groupby('area_code').size()\/len(master)\nreg_fq = master.groupby('region_code').size()\/len(master)\nloc_fq = master.groupby('locality_code').size()\/len(master)\n\n# grouping by frequency of pairs of area, region, locality\nreg_area_fq = (master.groupby(['region_code', 'area_code']).size()\/len(master)).reset_index(name = 'reg_area_freq')\nreg_loc_fq = (master.groupby(['region_code', 'locality_code']).size()\/len(master)).reset_index(name = 'reg_loc_freq')\nloc_area_fq = (master.groupby(['locality_code', 'area_code']).size()\/len(master)).reset_index(name = 'loc_area_freq')\n\n# grouping by frequency of triplets area, region, locality\nreg_area_loc_fq = (master.groupby(['locality_code', 'area_code', 'region_code']).size()\/len(master)).reset_index(name = 'reg_area_loc_freq')\n\n# grouping by frequency of pairs of species to each of area, region, locality\nspec_area_fq = (master.groupby(['species', 'area_code']).size()\/len(master)).reset_index(name = 'spec_area_freq')\nspec_reg_fq = (master.groupby(['species', 'region_code']).size()\/len(master)).reset_index(name = 'spec_reg_freq')\nspec_loc_fq = (master.groupby(['species', 'locality_code']).size()\/len(master)).reset_index(name = 'spec_loc_freq')\n\n# grouping by frequency of species & pairs of area, region, locality\nspec_area_reg_fq = (master.groupby(['species', 'area_code', 'region_code']).size()\/len(master)).reset_index(name = 'spec_area_reg_freq')\nspec_area_loc_fq = (master.groupby(['species', 'area_code', 'locality_code']).size()\/len(master)).reset_index(name = 'spec_area_loc_freq')\nspec_loc_reg_fq = (master.groupby(['species', 'locality_code', 'region_code']).size()\/len(master)).reset_index(name = 'spec_loc_reg_freq')\n\n# grouping by frequency of species & triplets of area, region, locality\nspec_area_loc_reg_fq = (master.groupby(['species', 'locality_code', 'region_code', 'area_code']).size()\/len(master)).reset_index(name = 'spec_area_loc_reg_freq')\n\n#Merging to main master\nmaster.loc[:, \"{}_freq\".format('area')] = master['area_code'].map(area_fq)\nmaster.loc[:, \"{}_freq\".format('reg')] = master['region_code'].map(reg_fq)\nmaster.loc[:, \"{}_freq\".format('loc')] = master['locality_code'].map(loc_fq)\n\nmaster = master.merge(reg_area_fq, on = ['region_code', 'area_code'], how = 'left')\nmaster = master.merge(reg_loc_fq, on = ['region_code', 'locality_code'], how = 'left')\nmaster = master.merge(loc_area_fq, on = ['locality_code', 'area_code'], how = 'left')\n\nmaster = master.merge(reg_area_loc_fq, on = ['locality_code', 'area_code', 'region_code'], how = 'left')\n\nmaster = master.merge(spec_area_fq, on = ['species', 'area_code'], how = 'left')\nmaster = master.merge(spec_reg_fq, on = ['species', 'region_code'], how = 'left')\nmaster = master.merge(spec_loc_fq, on = ['species', 'locality_code'], how = 'left')\n\nmaster = master.merge(spec_area_reg_fq, on = ['species', 'area_code', 'region_code'], how = 'left')\nmaster = master.merge(spec_area_loc_fq, on = ['species', 'area_code', 'locality_code'], how = 'left')\nmaster = master.merge(spec_loc_reg_fq, on = ['species', 'locality_code', 'region_code'], how = 'left')\n\nmaster = master.merge(spec_area_loc_reg_fq, on = ['species', 'locality_code', 'region_code', 'area_code'], how = 'left')\n\nmaster.head()","39610071":"ss = StandardScaler()\nmaster[['height', 'diameter']] = ss.fit_transform(master[['height', 'diameter']])\n\nmaster = master.drop(['area_code', 'region_code', 'locality_code', 'species'], axis = 1)\nmaster.head()","e058ca89":"train_data = master.loc[(master['type'] == 'train')]\ntest_data = master.loc[(master['type'] == 'test')]\n\ntrain_data = train_data.sort_values(by = ['class'])\n\ntrain_data = train_data.drop(['type'], axis = 1)\ntest_data = test_data.drop(['type', 'class'], axis = 1)","2857e780":"# Partitioning the features and the target\n\n#X = train[train.columns[~(train.columns.isin(['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7']))].tolist()]\n\nX = train_data[train_data.columns[~(train_data.columns.isin(['class']))].tolist()]\ny = train_data[['class']]","97b4ee03":"pca = PCA()\npca.fit_transform(X)\npca.get_covariance()\nexplained_variance = pca.explained_variance_ratio_\nexplained_variance","8bd8c95e":"with plt.style.context('seaborn-darkgrid'):\n    plt.figure(figsize=(10, 8))\n\n    plt.bar(range(17), explained_variance, alpha = 0.5, align = 'center', label = 'individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc = 'best')\n    plt.tight_layout()","f43dc311":"X = X.values\ny = y.values","26ab9ed8":"kfold, scores = KFold(n_splits = 6, shuffle = True, random_state = 22), list()\nfor train, test in kfold.split(X):\n    X_train, X_test = X[train], X[test]\n    y_train, y_test = y[train], y[test]\n    \n    model = XGBClassifier(random_state = 22, max_depth = 5, n_estimators = 200, objective = 'reg:squaredlogerror')\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = f1_score(y_test, preds, average = 'weighted')\n    scores.append(score)\n    print('Validation F1Score:', score)\nprint(\"Average Validation F1Score: \", sum(scores)\/len(scores))","91a0b735":"yPreds = model.predict(test_data.values)\nyPred_Probs = model.predict_proba(test_data.values)","63c964ed":"pd.set_option('display.float_format', lambda x: '%.20f' % x)\n\nsubmission = pd.DataFrame(yPred_Probs)\nsubmission.columns = ['Class_0', 'Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7']\nsubmission.to_csv('flower_class_3.csv', index = False)\nsubmission.head()","1f3d3ed8":"![](https:\/\/machinehack-be.s3.amazonaws.com\/flower_class_recognition_weekend_hackathon_17\/Flower%20Type%20Prediction-03-min.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAI2O7AQTB6JBT4VSA%2F20200823%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20200823T153003Z&X-Amz-Expires=172800&X-Amz-SignedHeaders=host&X-Amz-Signature=fd62b0a4fdb4a08e7988c05fb252961653d3fa319e9f5fe6023f53b4b9a0cf6d)","e2ce4ce6":"## Further XGBoost classifiers parameters that can be tuned\n\n- learning_rate: step size shrinkage used to prevent overfitting. Range is [0,1]\n- max_depth: determines how deeply each tree is allowed to grow during any boosting round.\n- subsample: percentage of samples used per tree. Low value can lead to underfitting.\n- colsample_bytree: percentage of features used per tree. High value can lead to overfitting.\n- n_estimators: number of trees you want to build.","83d585a3":"# Feature generation","f7955075":"## This is a baseline model which can be further improved!","36f958ed":"## XGBoost Classifier model\n### Using KFold Cross Validation","80cc5450":"### Feature Scaling","8fb823f4":"## Dataset Description:\n\n- Train.csv - 12666 rows x 7 columns (includes Class as target column)\n- Test.csv - 29555 rows x 6 columns\n- Sample Submission.csv - Please check the Evaluation section for more details on how to generate a valid submission.\n \n\n### Attributes Description:\n\n- Area_Code - Generic Area code, species were collected from\n- Locality_Code - Locality code, species were collected from\n- Region_Code - Region code, species were collected from\n- Height - Height collected from lab data\n- Diameter - Diameter collected from lab data\n- Species - Species of the flower\n- Class - Target Column (0-7) classes\n \n\n### Skills:\n\n- Advanced Classification Techniques, Gradient Boosting, Neural Nets, etc.\n- Feature engineering, high cardinality categorical columns\n- Optimizing log_loss to generalize well on unseen data","4eaebd7f":"# Exploratory Data Analysis","58284a5e":"### Public leaderboard score as of now stands at 0.8","1893cbbb":"## Scree Plot to view explained variance ratio using PCA","fee25cf0":"# Overview\n\nWelcome to another exciting weekend hackathon to flex your machine learning classification skills by classifying various classes of flowers into 8 different classes. To recognize the right flower you will be using 6 different attributes to classify them into the right set of classes(0-7). Using computer vision to do such recognition has reached state-of-the-art. Collecting Image data needs lots of human labor to annotate the images with the labels\/bounding-boxes for detection\/segmentation based tasks. Hence, some generic attribute which can be collected easily from various Area\/Locality\/Region were captured for over various species of flowers. \n\nIn this hackathon, we are challenging the machinehack community to use classical machine learning classification techniques to come up with a machine learning model that can generalize well on the unseen data provided explanatory attributes about the flower species instead of a picture.\n\nIn this competition, you will be learning advanced classification techniques, handling higher cardinality categorical variables, and much more. "}}