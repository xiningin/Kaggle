{"cell_type":{"617cbace":"code","3b91c38b":"code","707abea2":"code","2a428511":"code","5d57c21b":"code","4cb78359":"code","132125a7":"code","d99db8be":"code","ce74cd7c":"code","bceaf9fc":"code","8164698e":"code","65378395":"code","ba62280f":"code","874ae6f6":"markdown","2f3340c3":"markdown","692476e3":"markdown","7694375a":"markdown","d996f0fe":"markdown","3a5c41d8":"markdown","69236bce":"markdown","b56982cc":"markdown","24956378":"markdown","45120d8c":"markdown","1dd9e9ab":"markdown","a1991f9f":"markdown","495082f9":"markdown","769e7c1f":"markdown","255e888b":"markdown","c93aceda":"markdown","059b785e":"markdown","b9f7fbea":"markdown","949e67c3":"markdown","bd0eb9ab":"markdown","3d141c6a":"markdown","9972baff":"markdown","f6a2acfb":"markdown","730c9101":"markdown","f140264f":"markdown","e57d3e8e":"markdown","5fdfa50f":"markdown","27d4d555":"markdown"},"source":{"617cbace":"#standard imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\nimport pandas_datareader.data as web\nimport seaborn as sns","3b91c38b":"#resize charts to fit screen if using Jupyter Notebook\nplt.rcParams['figure.figsize']=[20,5]","707abea2":"#Daily S&P500 data from 1986==>\nurl = \"..\/input\/spx.csv\"\ndf = pd.read_csv(url,index_col='date', parse_dates=True)\n\n#view raw S&P500 data\ndf.head()","2a428511":"#To model returns we will use daily % change\ndaily_ret = df['close'].pct_change()\n#drop the 1st value - nan\ndaily_ret.dropna(inplace=True)\n#daily %change\ndaily_ret.head()","5d57c21b":"#use pandas to resample returns per month and take Standard Dev as measure of Volatility\n#then annualize by multiplying by sqrt of number of periods (12)\nmnthly_annu = daily_ret.resample('M').std()* np.sqrt(12)\n\nprint(mnthly_annu.head())\n#we can see major market events show up in the volatility\nplt.plot(mnthly_annu)\nplt.axvspan('1987','1989',color='r',alpha=.5)\nplt.axvspan('2008','2010',color='r',alpha=.5)\nplt.title('Monthly Annualized vol - Black Monday and 2008 Financial Crisis highlighted')\nlabs = mpatches.Patch(color='red',alpha=.5, label=\"Black Monday & '08 Crash\")\nplt.legend(handles=[labs])\n","4cb78359":"#for each year rank each month based on volatility lowest=1 Highest=12\nranked = mnthly_annu.groupby(mnthly_annu.index.year).rank()\n\n#average the ranks over all years for each month\nfinal = ranked.groupby(ranked.index.month).mean()\n\nfinal.describe()","132125a7":"#the final average results over 32 years \nfinal","d99db8be":"#plot results for ranked s&p 500 volatility\n#clearly October has the highest AMVR\n#and December has the lowest\n#mean of 6.45 is plotted\n\nb_plot = plt.bar(x=final.index,height=final)\nb_plot[9].set_color('g')\nb_plot[11].set_color('r')\nfor i,v in enumerate(round(final,2)):\n    plt.text(i+.8,1,str(v), color='black', fontweight='bold')\nplt.axhline(final.mean(),ls='--',color='k',label=round(final.mean(),2))\nplt.title('Average Monthly Volatility Ranking S&P500 since 1986')\n\nplt.legend()\nplt.show()\n\n\n","ce74cd7c":"#take abs value move from the mean\n#we see Dec and Oct are the biggest abs moves\n\nfin = abs(final - final.mean())\nprint(fin.sort_values())\nOct_value = fin[10]\nDec_value = fin[12]\nprint('Extreme Dec value:', Dec_value)\nprint('Extreme Oct value:', Oct_value)\n","bceaf9fc":"#as our Null is that no seasonality exists or alternatively that the month does not matter in terms of AMVR,\n#we can shuffle 'date' labels\n#for simplicity, we will shuffle the 'daily' return data, which has the same effect as shuffling 'month' labels\n\n#generate null data \n\nnew_df_sim = pd.DataFrame()\nhighest_only = []\n\ncount=0\nn=1000\nfor i in range(n):\n    #sample same size as dataset, drop timestamp\n    daily_ret_shuffle = daily_ret.sample(8191).reset_index(drop=True)\n    #add new timestamp to shuffled data\n    daily_ret_shuffle.index = (pd.bdate_range(start='1986-1-3',periods=8191))\n    \n    #then follow same data wrangling as before...\n    mnthly_annu = daily_ret_shuffle.resample('M').std()* np.sqrt(12)\n    \n    ranked = mnthly_annu.groupby(mnthly_annu.index.year).rank()\n    sim_final = ranked.groupby(ranked.index.month).mean()\n    #add each of 1000 sims into df\n    new_df_sim = pd.concat([new_df_sim,sim_final],axis=1)\n    \n    #also record just highest AMVR for each year (we will use this later for p-hacking explanation)\n    maxi_month = max(sim_final)\n    highest_only.append(maxi_month)\n    \n\n    \n#calculate absolute deviation in AMVR from the mean\nall_months = new_df_sim.values.flatten()\nmu_all_months = all_months.mean()\nabs_all_months = abs(all_months-mu_all_months)    \n\n#calculate absolute deviation in highest only AMVR from the mean\nmu_highest = np.mean(highest_only)\nabs_highest = [abs(x - mu_all_months) for x in highest_only]\n\n","8164698e":"#count number of months in sim data where ave-vol-rank is >= Dec\n#Note: we are using Dec not Oct, as Dec has highest absolute deviation from the mean\ncount=0\nfor i in abs_all_months:\n    if i> Dec_value:\n        count+=1\nans = count\/len(abs_all_months)        \nprint('p-value:', ans )","65378395":"#same again but just considering highest AMVR for each of 100 trials\ncount=0\nfor i in abs_highest:\n    if i> Dec_value:\n        count+=1\nans = count\/len(abs_highest)        \nprint('p-value:', ans )","ba62280f":"abs_all_months_95 = np.quantile(abs_all_months,.95)\nabs_highest_95 = np.quantile(abs_highest,.95)\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,sharex='col',figsize=(20,20))\n\n#need to normalize!\n\n#plot 1\nax1.hist(abs_all_months,histtype='bar',color='#42a5f5')\nax1.set_title('AMVR all months',fontsize=30)\nax1.set_ylabel('Frequency',fontsize=20)\nn,bins,patches = ax3.hist(abs_all_months,density=1,histtype='bar',cumulative=True,bins=30,color='#42a5f5')\nax3.set_ylabel('Cumulative probability',fontsize=20)\nax1.axvline(Dec_value,color='b',label='Dec Result',lw=10)\nax3.axvline(Dec_value,color='b',lw=10)\nax3.axvline(abs_all_months_95,color='r',ls='--',label='5% Sig level',lw=10)\n\n\n#plot2\nax2.hist(abs_highest,histtype='bar',color='g')\nax2.set_title('AMVR highest only',fontsize=30)\nax2.axvline(Dec_value,color='b',lw=10)\nn,bins,patches = ax4.hist(abs_highest,density=1,histtype='bar',cumulative=True,bins=30,color='g')\nax4.axvline(Dec_value,color='b',lw=10)\nax4.axvline(abs_highest_95,color='r',ls='--',lw=10)\n\nax1.legend(fontsize=15)\nax3.legend(fontsize=15)\n","874ae6f6":"The most important part of hypothesis testing is being clear what question we are trying to answer. In our case we are asking:\n\n\u201c**Could the most extreme value happen by chance?**\u201d\n\nThe most extreme value we define as the **greatest absolute AMVR deviation from the mean**. This question forms our null hypothesis.\n\nIn our data the most extreme value is the December value (1.23) not the October value (1.08), because we are looking at the biggest absolute deviation from the mean not simply the highest volatility:","2f3340c3":"So that\u2019s our data, now onto Hypothesis testing\u2026\n\n### Hypothesis Testing: What\u2019s the question?\n\nHypothesis testing is one of the most fundamental techniques of data science, yet it is one on the most intimidating and misunderstood. The basis for this fear, is the way it is taught in Stats 101, where we are told to:\n\n*\"perform a t-test, is it a one-sided or two-sided test?, choose a suitable test-statistic such as Welch\u2019s t-test, calculate degrees of freedom, calculate the t score, look up the critical value in a table, compare critical value to t statistic \u2026\u2026\"*","692476e3":"By plotting our AMVR we can clearly see the most volatile month has been October and the lowest, December.","7694375a":"                              Shuffle 'date' label to create null dataset\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*-rzHR7JrlTjcXAYp4zKCUA.gif)","d996f0fe":"To answer the 2nd question and to avoid multiplicity, instead of comparing our result to the distribution made with all 12000 AMVR deviations, we only consider the highest value from each of the absolute AMVR 1000 trials. This gives a **p-value of 23%**, very much **not significant!**","3a5c41d8":"Understandably, this is leads to confusion over what test to conduct and how to conduct it. However, all of these classical statistical techniques for performing a hypothesis test, were developed at a time when we had very little computing power and were simply closed-form analytical solutions for calculating a p-value,that's it! But with the added complication of needing to pick the right formula for the given situation, due to their restrictive and sometimes opaque assumptions.\n\nBut rejoice!\n\nThere is a better way. Simulation.\n\nTo understand how simulation can help us, lets remind ourselves what a hypothesis test is:\n\nWe wish to test ***\u201cwhether the observed effect in our data is real or whether it could happen simply by chance\u201d***\nand to perform this test we do the following:\n   *  Choose an appropriate \u2018test statistic\u2019: this is simply a number that measures the observed effect. In our case we will choose the **absolute deviation in AMVR from the mean**.\n   * Construct a Null Hypothesis: this is simply a version of the data where the observed effect in not present. In our case we will shuffle the labels of the data repeatedly **permutation**. The justification for this is detailed below.\n   * Compute a p-value: this is the probability of seeing the observed effect amongst the null data, in other words, by chance. **We do this through repeated simulation** of the null data. In our case, we shuffle the \u2018date\u2019 labels of the data many times and simply count the occurrence of our test statistic as it appears through multiple simulations.\n   \n>    That\u2019s hypothesis testing in 3 steps! No matter what phenomena we are testing, the question is always the same: \u201c**is the observed effect real, or is it due to chance**\u201d\n\n[There is only one test! This great blog by Allen Downey has more details on hypothesis testing](http:\/\/allendowney.blogspot.com\/2011\/05\/there-is-only-one-test.html)\n\n**The real power of simulation is that we have to make explicit what our model assumptions are through code**. Whereas classical techniques can be a \u2018black-box\u2019 when it comes to there assumptions.","69236bce":"### Wrangle with Pandas:\n\nTo answer the question of whether the extreme volatility seen in certain months really is significant, we need to transform our 32yrs of price data into a format that shows the phenomena we are investigating.\n\nOur format of choice will be **average monthly volatility rankings (AMVR)**.\n\nThe following code shows how we get our raw price data into this format:","b56982cc":"### p-Hacking\n\nHere\u2019s the interesting bit. We\u2019ve constructed a hypothesis to test, we\u2019ve generated simulated data by shuffling the \u2018date\u2019 labels of the data , now we need to perform our hypothesis test to find the probability of observing a result as significant as the December result given that the null hypothesis (no seasonality) is true.\n\nBefore we perform the test lets set our expectations.\n\n**Whats the probability of seeing *at least one* significant result given a 5% significance level?**\n\n= 1-p(not significant)\n\n= 1-(1\u20130.05)\u00b9\u00b2\n\n= 0.46\n\nso there\u2019s a **46%** chance of seeing **at least one** month with a significant result, given our null is true.\n\nNow lets ask, **for each individual test** (*comparing each of the 12 months absolute AMVR to the mean*) **how many significant values should we expect to see amongst our random, non-seasonal data?**\n\n12 x 0.05 = 0.6\n\nSo with a 0.05 significance level we should expect a false positive rate of **0.6**. In other words, for each test (with the null data) comparing all 12 months AMVR to the mean, 0.6 months will have show a significant result.(*obviously we cant have less than 1 month showing a result, but under repeat testing the math should tend towards this number*).\n\nAll the way through this work, we have stressed the importance of being really clear with the question we are trying to answer. The problem with the expectations we\u2019ve just calculated is **we have assumed we are testing for a significant result against all 12 months!** That\u2019s why the probability of seeing at least one false positive is so high at 46%.\n\nThis is an example of [multiple comparison bias](https:\/\/www.quantopian.com\/lectures\/p-hacking-and-multiple-comparisons-bias). Where we have expanded our search space and increased the likelihood of finding a significant result. This a problem because we could abuse this phenomena to cherry pick the parameters of our model which give us the \u2018desired\u2019 p-value.\n\n> This is the essence of p-Hacking\n\nThis xkcd nicely highlights the issue with multiple comparison bias and it\u2019s subsequent abuse, p-hacking.\n","24956378":"The left column is the data that answers question 1 and the right column, question 2. The top row are the probability distributions and the bottom row are the CDF. The red dashed line is the 5% significance level that we arbitrarily decided upon. The blue line is the original extreme December AMVR value of 1.23.","45120d8c":"To illustrate the effect of p-hacking and how to reduce multiplicity, we need to understand the subtle but significant difference between the following 2 question:\n\n*     **\u201cWhats the probability that December would appear this extreme by chance.\u201d**\n*     **\u201cWhats the probability any month would appear this extreme by chance.\"**\n\nThe beauty of simulation lies in its simplicity. The following code is all we need to compute the p-value to answer the 1st question. We simply count how many values in our dataset using all 12000 AMVR deviations (12 months x 1000 trials) are greater than the observed December value. We get a **p-value of 4.4%**, close to our somewhat arbitrary 5% cut off, but **significant** none the less.\n\n","1dd9e9ab":"### Our data:\n\nWe will be using Daily S&P500 data for this analysis, in particular we will use the raw daily closing prices from 1986 to 2018 (which is surprisingly hard to find so I\u2019ve made it [publicly available](https:\/\/www.kaggle.com\/pdquant\/sp500-daily-19862018)).\n\nThe inspiration for this post came from [Winton](https:\/\/www.winton.com\/research\/seasonal-volatility-and-the-multiplicity-effect), which we will be reproducing here, albeit with 32 years of data vs their 87 years.\n\n","a1991f9f":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*_yxe_kGGtT9m4qd6ahGohQ.png)","495082f9":"A great resource for learning about sampling is by [Julian Simon](http:\/\/www.resample.com\/intro-text-online\/).\n\nNote: *The way our test is constructed is the equivalent of a two sided test using \u2018classical\u2019 methods, such as Welch\u2019s t-test or ANOVA etc, because we are interested in the most extreme value, either above or below the mean.*\n\nThese decisions are design choices and we have this freedom because **the Null Model is just that, it\u2019s a Model!** This means we can specify its parameters as we choose, the key is to really be clear what question we are trying to answer.\n\nBelow is the code to simulate 1000 sets of 12 AMVR, permuting the date labels each time to build up the sampling distribution. The output from this code is included below in the p-hacking section\u2026","769e7c1f":"import our data and convert into daily returns using pct_change","255e888b":"A quick look at the annualized monthly vol, shows major market events clearly, such as Black Monday and the 2008 Financial Crisis.","c93aceda":"                               Whoa! Green Jelly beans are significant\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/0*zZgRnBSbwUN3b6D5.png)","059b785e":"this gives our final** Average Monthly Volatility Rankings**. Numerically we can see that **month 10 (October) is the highest and 12 (December) is the lowest**.","b9f7fbea":"A nice trick to make charts display full width in Jupyter notebooks:","949e67c3":"### Simulation\n\nNow we know what question we are asking, we need to construct our \u2018Null Model\u2019.\n\nThere are a number of options here:\n\n   * **Parametric models**. If we had a good idea of the data distribution or simply made assumption on it, we could use \u2018classical\u2019 hypothesis testing techniques, t-test, X\u00b2, one-way ANOVA etc. These models can be restrictive and something of a blackbox if their assumptions aren\u2019t fully understood by the researcher.\n   * **Direct Simulation**. We could make assumptions about the [data generating process](http:\/\/www.rimini.unibo.it\/fanelli\/econometric_models2_2012.pdf) and simulate directly. For example we could specify an ARMA time series model for the financial data we are modeling and deliberately engineer it to have no seasonality. This could be a reasonable choice for our problem, however if we knew the data generating process for the S&P500 we would be rich already!\n   * **Simulation through Resampling**. This is the approach we will take. By repeatedly sampling at random from the existing dataset and shuffling the labels, we can make the observed effect equally likely amongst all labels in our data (in our case the labels are the dates), thus giving the desired null dataset.\n\nSampling is a big topic but we will focus on one particular technique, **[permutation](https:\/\/speakerdeck.com\/jakevdp\/statistics-for-hackers)** or shuffling.\n\nTo get the desired null model, we need to construct a dataset that has **no seasonality** present. If the null is true, *that there is no seasonality in the data and the observed effect was simply by chance*, then the labels for each month (Jan,Feb etc) are meaningless and therefore we can shuffle the data repeatedly to build up what classical statistics would call the \u2018*[sampling distribution of the test statistic under the null hypothesis](https:\/\/www.youtube.com\/watch?time_continue=441&v=5Dnw46eC-0o)*\u2019. This has the desired effect of making the observed phenomena (the extreme December value) **equally likely** for all months, which is exactly what our null model requires.\n\nTo prove how powerful simulation techniques are with modern computing power, the code in this example will actually permute the daily price data, which requires lots more processing power, yet still completes in seconds on a modern CPU. \n\nNote: *shuffling either the daily or the monthly labels will give us the desired null dataset in our case.*","bd0eb9ab":"Here\u2019s where we can use the power of pandas to group our volatility by year and create a ranking for each of the 12 months over all 32 years of data","3d141c6a":"Now we use one of the more powerful tools in Pandas: [Resample](https:\/\/www.google.com\/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwiJl6ShnJXeAhXHK8AKHQBCCAsQFjAAegQIBxAB&url=https%3A%2F%2Fpandas.pydata.org%2Fpandas-docs%2Fstable%2Fgenerated%2Fpandas.DataFrame.resample.html&usg=AOvVaw1le9agxvLanaQp9zlNYG9Y). This allows us to change the frequency of our data from daily to monthly and to use standard deviation as a measure of volatility. These are the design choices we have when constructing an analysis.","9972baff":"# <center><span style=\"color:blue; font-size: 2em\"> Stocks, Significance Testing & p-Hacking: How <span style=\"color:green\"> volatile <\/span> <span style=\"color:red\"> is volatile? <\/span><\/span><\/center>\n\n\n>## ** October is historically the most volatile month for stocks, but is this a persistent signal or just noise in the data?**\n\n\n![](https:\/\/cdn-images-1.medium.com\/max\/600\/1*D0_bUZNtw_3aq_CyMqj0dw.gif)\n","f6a2acfb":"Example below: The left plot shows the true data and the observed effect with a certain probability (green). The right plot is our simulated null data with a recording of when the observed effect was seen by chance (red). This is the basis of hypothesis testing, what is the probability of seeing the observed effect in our null data.","730c9101":"![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*UW9OJHyVcl0qWWikSv0kpQ.png)","f140264f":"The left side plot shows that the original December value is significant at a 5% level, but only just! However, when we account for multiple comparison bias, in the right hand plot the threshold for significance moves up from around 1.2 (abs AMVR) up to around 1.6 (see the redline).\n\n>    **By accounting for multiple comparison bias our December value of 1.23 is no longer significant!**\n\nBy taking into consideration the specific question we are trying to answer and avoiding multiple comparison bias, we have avoided p-hacking our model and avoided showing a significant result when there isn\u2019t one.\n\nTo further explore p-hacking and how it can be abused to tell a particular story about our data, see this great interactive app from [FiveThirtyEight](https:\/\/fivethirtyeight.com\/features\/science-isnt-broken\/#part2)\n\n### Conclusions\n\n* **We have learnt that hypothesis testing is not the big scary beast we thought it was. Simply follow the 3 steps above to construct your model for any kind of data or test statistic.**\n\n* **We\u2019ve show that asking the right question is vital for scientific analysis. A slight change in the wording can lead to a very different model with very different results.**\n\n* **We discussed the importance of recognizing and correcting for multiple comparison bias and avoiding the pitfalls of p-hacking and showed how a seemingly significant result can become non-significant.**\n\n* **With more and more \u2018big data\u2019 along with academic pressure to produce a research paper with \u2018novel\u2019 findings or political pressure to show a result as being \u2018significant\u2019, the temptation for p-hacking is ever increasing. By learning to recongise when we are guilty of it and correcting for it accordingly we can become better researchers and ultimately produce more accurate and therefore actionable scientific results!**\n\nAuthors Notes: *Our results differ slightly from the original [Winton research](https:\/\/www.winton.com\/research\/seasonal-volatility-and-the-multiplicity-effect), this due in part to having a slightly different data set (32yrs vs 87yrs) and they have October being the month of interest whereas we have December. Also they used an undisclosed method for their \u2018simulated data\u2019 whereas we have made explicit, through code our methodology for creating that data. We have made certain modeling assumptions through out this work, again, these have been made explicit and can be seen in the code. These design and modeling choices are part of the scientific process, so long as they are made explicit, the analysis has merit.*\n\nHope you found this useful and interesting, follow to be notified with my latest posts!\nI have also written this in [blog form on medium](https:\/\/medium.com\/@pdquant\/stocks-significance-testing-p-hacking-how-volatile-is-volatile-1a0da3064b8a) for those who prefer it.\n\nFollow [twitter.com\/pdquant](http:\/\/www.twitter.com\/pdquant) for more!\n\n","e57d3e8e":"### Our goal:\n\n*  [**Demonstrate how to use Pandas to analyze Time Series**](#Wrangle-with-Pandas:)\n* [**Understand how to construct a hypothesis test**](#Hypothesis-Testing:-What\u2019s-the-question?)\n* [**Use simulation to perform hypothesis testing**](#Simulation)\n* [**Show the importance of accounting for multiple comparison bias**](#p-Hacking)","5fdfa50f":"### Stocks, Significance Testing & p-Hacking.\n\n <center><span style=\" font-size: 20px\"> Over the past 32 years, October has been the most volatile month on average for the S&P500 and December the least, in this article we will use simulation to assess the statistical significance of this observation and to what extent this observation could occur by chance.<\/span> <\/center>","27d4d555":"Now lets plot these distributions:"}}