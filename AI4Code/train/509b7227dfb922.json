{"cell_type":{"2c86c685":"code","4f418a90":"code","2a2d09b9":"code","ed05ca2f":"code","2e63de6f":"code","a8603625":"code","191393a2":"code","a565bd59":"code","b9caf96f":"code","11563512":"code","78a67acd":"code","cd03a8d0":"code","0da5fceb":"code","c872f650":"code","8f22c294":"code","5639da85":"code","f7acb828":"code","c96bcbf1":"code","7194a6bf":"code","854afc26":"code","37786dd1":"code","83f59957":"code","937411e8":"code","1c2948e8":"code","a9689dd3":"code","93638e93":"code","ad41213c":"code","be8b1db6":"code","6186e6b4":"code","d8c9684e":"code","825c9c20":"code","b82fd117":"markdown","4844a505":"markdown","757c88d6":"markdown","47c7926d":"markdown","c60c1d25":"markdown","7d815d89":"markdown","56ff89d5":"markdown","c73513e8":"markdown","6326952b":"markdown","f12e684b":"markdown","2d7ef872":"markdown","210ab402":"markdown","4012d319":"markdown","b75e1b5b":"markdown","824be0ed":"markdown","0b9860e6":"markdown"},"source":{"2c86c685":"\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n### CNN models ###\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D\nfrom keras.utils import np_utils\nfrom keras.regularizers import l2\nfrom keras.optimizers import SGD, RMSprop\nfrom keras.utils import to_categorical\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import models\nfrom keras.utils.vis_utils import plot_model\nfrom keras.layers import Input, GlobalAveragePooling2D,concatenate\nfrom keras.models import Model\nfrom tensorflow.keras import layers\nfrom keras.applications.inception_v3 import InceptionV3\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve, roc_auc_score","4f418a90":"\nbatch_size = 32\nnum_epochs = 50\ninput_shape = (48, 48, 1)\nvalidation_split = .2\nverbose = 1\nnum_classes = 7\nbase_path = 'models\/'\nshape_x = 48\nshape_y = 48\nimage_size=(48,48)","2a2d09b9":"data=pd.read_csv('..\/input\/fer2013\/fer2013.csv')","ed05ca2f":"data['pixels']=data['pixels'].astype(\"string\")\npixels = data['pixels'].tolist()\nwidth, height = 48, 48\nfaces = []\nfor pixel_sequence in pixels:\n    face = [int(pixel) for pixel in pixel_sequence.strip().split(' ',48*48)]\n    face = np.asarray(face).reshape(width, height)\n    face = cv2.resize(face.astype('uint8'),image_size)\n    faces.append(face.astype('float32'))\nfaces = np.asarray(faces)\nfaces = np.expand_dims(faces, -1)\nfaces \/= 127.5\nfaces -= 1.\nemotions = pd.get_dummies(data['emotion']).to_numpy()","2e63de6f":"datagen = ImageDataGenerator(\n        zoom_range=0.2,          # randomly zoom into images\n        rotation_range=10,       # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,   # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,    # randomly flip images\n        vertical_flip=False)     # randomly flip images","a8603625":"xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.3,shuffle=True)\nxval,xtest,yval,ytest=train_test_split(xtest,ytest,test_size=0.3,shuffle=True)","191393a2":"def entry_flow(inputs) :\n    \n    x = Conv2D(32, 3, strides = 2, padding='same')(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    x = Conv2D(64,3,padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    previous_block_activation = x\n    \n    for size in [64, 128, 256] :\n    \n        x = Activation('relu')(x)\n        x = SeparableConv2D(size, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n    \n        x = Activation('relu')(x)\n        x = SeparableConv2D(size, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n        \n        x = MaxPooling2D(3, strides=2, padding='same')(x)\n        \n        residual = Conv2D(size, 1, strides=2, padding='same')(previous_block_activation)\n        \n        x = keras.layers.Add()([x, residual])\n        previous_block_activation = x\n    \n    return x","a565bd59":"def middle_flow(x, num_blocks=8) :\n    \n    previous_block_activation = x\n    \n    for _ in range(num_blocks) :\n    \n        x = Activation('relu')(x)\n        x = SeparableConv2D(256, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n    \n        x = Activation('relu')(x)\n        x = SeparableConv2D(256, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n        \n        x = Activation('relu')(x)\n        x = SeparableConv2D(256, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n        \n        x = keras.layers.Add()([x, previous_block_activation])\n        previous_block_activation = x\n    \n    return x","b9caf96f":"def exit_flow(x, num_classes=7) :\n    \n    previous_block_activation = x\n    \n    x = Activation('relu')(x)\n    x = SeparableConv2D(256, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n    \n    x = Activation('relu')(x)\n    x = SeparableConv2D(1024, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n    \n    x = MaxPooling2D(3, strides=2, padding='same')(x)\n    \n    residual = Conv2D(1024, 1, strides=2, padding='same')(previous_block_activation)\n    x = keras.layers.Add()([x, residual])\n      \n    x = Activation('relu')(x)\n    x = SeparableConv2D(728, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n    \n    x = Activation('relu')(x)\n    x = SeparableConv2D(1024, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n    \n    x = GlobalAveragePooling2D()(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    \n    return x","11563512":"inputs = Input(shape=(shape_x, shape_y, 1))\noutputs = exit_flow(middle_flow(entry_flow(inputs)))","78a67acd":"xception = Model(inputs, outputs,name=\"Xception\")","cd03a8d0":"plot_model(xception, to_file='xception.png', show_shapes=True, show_layer_names=True)","0da5fceb":"xception.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nxception.summary()","c872f650":"def dexpression():\n    inputs = Input(shape=(shape_x, shape_y, 1))\n\n    x = Conv2D(64, 7, strides = 2, padding='same')(inputs)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(3, strides=2, padding='same')(x)\n    x = BatchNormalization()(x)\n\n    x_1 = Conv2D(96, 1, strides = 1, padding='same')(x)\n    x_1 = Activation('relu')(x_1)\n    x_1 = MaxPooling2D(3, strides=1, padding='same')(x_1)\n    x_1 = BatchNormalization()(x_1)\n\n    x_2 = Conv2D(208, 3, strides = 1, padding='same')(x_1)\n    x_2 = Activation('relu')(x_2)\n    x_2 = MaxPooling2D(3, strides=1, padding='same')(x_2)\n\n    x_3 = Conv2D(64, 1, strides = 1, padding='same')(x_1)\n    x_3 = Activation('relu')(x_3)\n    x_3 = MaxPooling2D(3, strides=1, padding='same')(x_3)\n\n\n    x_4=concatenate([x_2,x_3],axis=3)\n\n    x_5 = Conv2D(96, 1, strides = 1, padding='same')(x_4)\n    x_5 = Activation('relu')(x_5)\n    x_5 = Conv2D(208, 3, strides = 1, padding='same')(x_5)\n    x_5 = Activation('relu')(x_5)\n\n    x_6 = MaxPooling2D(3, strides=1, padding='same')(x_4)\n    x_6 = Activation('relu')(x_6)\n    x_6 = Conv2D(64, 1, strides = 1, padding='same')(x_6)\n    x_6 = Activation('relu')(x_6)\n    x_6 = MaxPooling2D(3, strides=1, padding='same')(x_6)\n    x_7 = concatenate([x_5,x_6],axis=3)\n\n    x_8 = Flatten()(x_7)\n    x_8 = Dropout(0.25)(x_8)\n    x_8 = Dense(7, activation='softmax')(x_8)\n    return Model(inputs, x_8,name='DeXpression')","8f22c294":"dexpression=dexpression()","5639da85":"plot_model(dexpression, to_file='dexpression.png', show_shapes=True, show_layer_names=True)","f7acb828":"dexpression.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\ndexpression.summary()","c96bcbf1":"def CNN():\n    model = Sequential(name='CNN')\n    model.add(Conv2D(64, (3, 3), padding='same', input_shape=(48,48,1)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='same'))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(128, (3, 3), padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='same'))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(256, (3, 3), padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='same'))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(512, (3, 3), padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='same'))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n\n    model.add(Dense(512))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Dense(256))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Dense(7))\n    model.add(Activation('softmax'))\n    \n    return model","7194a6bf":"CNN=CNN()","854afc26":"plot_model(CNN, to_file='CNN.png', show_shapes=True, show_layer_names=True)","37786dd1":"CNN.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nCNN.summary()","83f59957":"\nearly_stop = EarlyStopping('val_loss', patience=100)\nreduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n                                  patience=50, min_lr=0.00001,model='auto')\ntrained_models_path = base_path + '_Xception'\nmodel_names = trained_models_path + '.{epoch:02d}-{val_accuracy:.2f}.hdf5'\nmodel_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n                                                    save_best_only=True)\ncallbacks = [model_checkpoint, early_stop, reduce_lr]","937411e8":"xception_history =xception.fit(datagen.flow(xtrain, ytrain, batch_size),\n          steps_per_epoch=len(xtrain) \/ batch_size, \n          epochs=num_epochs, \n          verbose=1, \n          callbacks=callbacks,\n          validation_data=(xval,yval))","1c2948e8":"early_stop = EarlyStopping('val_loss', patience=100)\nreduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n                                  patience=25, min_lr=0.00001,model='auto')\ntrained_models_path = base_path + '_DeXpression'\nmodel_names = trained_models_path + '.{epoch:02d}-{val_accuracy:.2f}.hdf5'\nmodel_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n                                                    save_best_only=True)\ncallbacks = [model_checkpoint, early_stop, reduce_lr]","a9689dd3":"dexpression_history =dexpression.fit(datagen.flow(xtrain, ytrain, batch_size),\n          steps_per_epoch=len(xtrain) \/ batch_size, \n          epochs=num_epochs, \n          verbose=1, \n          callbacks=callbacks,\n          validation_data=(xval,yval))","93638e93":"early_stop = EarlyStopping('val_loss', patience=100)\nreduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n                                  patience=25, min_lr=0.00001,model='auto')\ntrained_models_path = base_path + 'CNN'\nmodel_names = trained_models_path + '.{epoch:02d}-{val_accuracy:.2f}.hdf5'\nmodel_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n                                                    save_best_only=True)\ncallbacks = [model_checkpoint, early_stop, reduce_lr]","ad41213c":"CNN_history =CNN.fit(datagen.flow(xtrain, ytrain, batch_size),\n          steps_per_epoch=len(xtrain) \/ batch_size, \n          epochs=num_epochs, \n          verbose=1, \n          callbacks=callbacks,\n          validation_data=(xval,yval))","be8b1db6":"\nfig,axes=plt.subplots(3,2,figsize=(20, 20))\nfor (m,history), ax in zip({'CNN':CNN_history,'Xception':xception_history,'Dexpression':dexpression_history}.items(),axes):\n    # Loss Curves\n    \n    ax[0].plot(history.history['loss'],'r',linewidth=2.0)\n    ax[0].plot(history.history['val_loss'],'b',linewidth=2.0)\n    ax[0].legend(['Training loss', 'Validation Loss'],fontsize=18)\n    ax[0].set_xlabel('Epochs ',fontsize=16)\n    ax[0].set_ylabel('Loss',fontsize=16)\n    ax[0].set_title('Loss Curves '+m,fontsize=16)\n \n    # Accuracy Curves\n    ax[1].plot(history.history['accuracy'],'r',linewidth=2.0)\n    ax[1].plot(history.history['val_accuracy'],'b',linewidth=2.0)\n    ax[1].legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n    ax[1].set_xlabel('Epochs ',fontsize=16)\n    ax[1].set_ylabel('Accuracy',fontsize=16)\n    ax[1].set_title('Accuracy Curves '+m,fontsize=16)","6186e6b4":"plt.savefig('plots.png')","d8c9684e":"\nfor model in [CNN,xception,dexpression]:\n    ypred=model.predict(xtest)\n    ypred_=np.argmax(ypred, axis=1)\n    ytest_=np.argmax(ytest, axis=1)\n    print(classification_report(ytest_, ypred_,digits=3))\n    \n","825c9c20":"import itertools\nfrom sklearn.metrics import confusion_matrix\nfrom matplotlib.pyplot import figure\n\n\nfor model,i in zip([CNN,xception,dexpression],[1,2,3]):\n    fig = figure(figsize=(10, 10))\n    \n    ypred=model.predict(xtest)\n    rounded_predections=np.argmax(ypred, axis=1)\n    rounded_labels=np.argmax(ytest, axis=1)\n    cm = confusion_matrix(rounded_labels, rounded_predections)\n    labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n    title='Confusion matrix '+model.name\n    \n\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(labels))\n    plt.xticks(tick_marks, labels, rotation=45)\n    plt.yticks(tick_marks, labels)\n    fmt = 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    plt.savefig('confusion_matrix_'+model.name+'.png')\n    plt.show()\n    ","b82fd117":"## Xception :\n[PAPER](https:\/\/arxiv.org\/pdf\/1610.02357.pdf)\n\n![](https:\/\/www.researchgate.net\/publication\/342580102\/figure\/fig3\/AS:908305815830530@1593568390179\/Schematic-diagram-of-the-Xception-model.png)","4844a505":"## CNN\n","757c88d6":"### Plotting accuracy and loss curves ","47c7926d":"## CNN inspired by Goodfellow I.J\n[PAPER](https:\/\/arxiv.org\/pdf\/1307.0414.pdf)\n\n![](https:\/\/raw.githubusercontent.com\/NJNischal\/Facial-Expression-Recognition-with-CNNs\/9999cbdaa55542e86e11a9e129bafcfb96bd0e60\/model.png)","c60c1d25":"## Preprocessing","7d815d89":"## Testing","56ff89d5":"## Parameters","c73513e8":"### Confusion Matrix","6326952b":"## Dexpression\n[PAPER](https:\/\/arxiv.org\/pdf\/1509.05371.pdf)\n\n![](https:\/\/qph.fs.quoracdn.net\/main-qimg-252e5047af30f8ea18def43bb35dbf41.webp)","f12e684b":"### Classification reports","2d7ef872":"## Imports","210ab402":"## Data ","4012d319":"# Emotion Recognition FER2013\n\n   ![](https:\/\/www.researchgate.net\/profile\/Garima-Verma-3\/publication\/343556711\/figure\/fig1\/AS:933334486114318@1599535690875\/Sample-of-the-FER2013-dataset.png)","b75e1b5b":"## Data Augmentation","824be0ed":"## Dexpression","0b9860e6":"# Training \n## Xception"}}