{"cell_type":{"dae78528":"code","097a423e":"code","02e87fa4":"code","d7378007":"code","67cd724e":"code","c28db248":"code","08149227":"code","34a308f2":"code","af68ddcc":"code","3f4f8fef":"code","3370ecfa":"code","c84e7f3c":"code","76e3cae2":"code","d75cff59":"code","8ef02306":"code","c0482f77":"code","8a653afc":"code","6891abcf":"code","6b6c734d":"code","563fa495":"code","5ea82c33":"code","92573384":"code","aba7e402":"code","5cb54f64":"code","460c42e0":"code","5b65522c":"code","a9581041":"code","168c038a":"code","8ae90092":"code","ed39c315":"code","26df7c9c":"code","b0713ead":"code","0fc27797":"code","213eabde":"code","e34b1cbd":"code","f3a07a3a":"code","b04dc822":"code","4bd8bc6f":"code","c5f895c3":"code","9753fe3d":"code","9c35239c":"code","55e313a2":"code","40641596":"code","2d9caf4a":"code","a61eb044":"code","955db3a8":"code","7b8d75eb":"code","a72c1935":"code","184e2280":"code","e681c854":"code","d96728a9":"code","74ff09f6":"code","ddf2b2aa":"code","0885c7fa":"markdown","41f3e8b8":"markdown","bb7b8ab0":"markdown","d0577a20":"markdown","e3c1b422":"markdown","c8f56252":"markdown","ca6ebece":"markdown","0cf49c66":"markdown","70a58476":"markdown","1cd99c70":"markdown","795f36e5":"markdown","c08ce497":"markdown","fb25a8e1":"markdown","ef9e2b38":"markdown","1b01d7b9":"markdown","67f78691":"markdown","53266ed2":"markdown","97329580":"markdown","04e2036b":"markdown","8f582d75":"markdown","1e99032e":"markdown","b6ea971e":"markdown","b926f664":"markdown","420a9dff":"markdown","5bb944cf":"markdown","fed83cca":"markdown","a5705999":"markdown","336196b8":"markdown","f68d84a5":"markdown","ccfd5bc2":"markdown","84725445":"markdown","e91d0f4b":"markdown","2fd41815":"markdown","9547e8c8":"markdown","1434abc4":"markdown","e8d2d54c":"markdown","93fe048a":"markdown","9aa2e49f":"markdown","e0150248":"markdown","72051c25":"markdown","f29177aa":"markdown","420bc400":"markdown","31738ba0":"markdown","99922582":"markdown","6c07ca23":"markdown","66a4daea":"markdown"},"source":{"dae78528":"# importing packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","097a423e":"# Importing the data \ndf = pd.read_csv('..\/input\/pgaTourData.csv')\n\n# Examining the first 5 data\nprint(df.head())","02e87fa4":"df.info()","d7378007":"df.shape","67cd724e":"# Replace NaN with 0 in Top 10 \ndf['Top 10'].fillna(0, inplace=True)\ndf['Top 10'] = df['Top 10'].astype(int)\n\n# Replace NaN with 0 in # of wins\ndf['Wins'].fillna(0, inplace=True)\ndf['Wins'] = df['Wins'].astype(int)\n\n# Drop NaN values \ndf.dropna(axis = 0, inplace=True)","c28db248":"# Change Rounds to int\ndf['Rounds'] = df['Rounds'].astype(int)\n\n# Change Points to int \ndf['Points'] = df['Points'].apply(lambda x: x.replace(',',''))\ndf['Points'] = df['Points'].astype(int)\n\n# Remove the $ and commas in money \ndf['Money'] = df['Money'].apply(lambda x: x.replace('$',''))\ndf['Money'] = df['Money'].apply(lambda x: x.replace(',',''))\ndf['Money'] = df['Money'].astype(float)","08149227":"df.info()","34a308f2":"df.head()","af68ddcc":"df.describe()","3f4f8fef":"# Looking at the distribution of data\nf, ax = plt.subplots(nrows = 6, ncols = 3, figsize=(20,20))\ndistribution = df.loc[:,df.columns!='Player Name'].columns\nrows = 0\ncols = 0\nfor i, column in enumerate(distribution):\n    p = sns.distplot(df[column], ax=ax[rows][cols])\n    cols += 1\n    if cols == 3:\n        cols = 0\n        rows += 1\n","3370ecfa":"# Looking at the number of players with Wins for each year \nwin = df.groupby('Year')['Wins'].value_counts()\nwin = win.unstack()\nwin.fillna(0, inplace=True)\n\n# Converting win into ints\nwin = win.astype(int)\n\nprint(win)","c84e7f3c":"# Looking at the percentage of players without a win in that year \nplayers = win.apply(lambda x: np.sum(x), axis=1)\npercent_no_win = win[0]\/players\npercent_no_win = percent_no_win*100\nprint(percent_no_win)","76e3cae2":"# Plotting percentage of players without a win each year \nfig, ax = plt.subplots()\nbar_width = 0.8\nopacity = 0.7 \nindex = np.arange(2010, 2019)\n\nplt.bar(index, percent_no_win, bar_width, alpha = opacity)\nplt.xticks(index)\nplt.xlabel('Year')\nplt.ylabel('%')\nplt.title('Percentage of Players without a Win')","d75cff59":"# Plotting the number of wins on a bar chart \nfig, ax = plt.subplots()\nindex = np.arange(2010, 2019)\nbar_width = 0.2\nopacity = 0.7 \n\ndef plot_bar(index, win, labels):\n    plt.bar(index, win, bar_width, alpha=opacity, label=labels)\n\n# Plotting the bars\nrects = plot_bar(index, win[0], labels = '0 Wins')\nrects1 = plot_bar(index + bar_width, win[1], labels = '1 Wins')\nrects2 = plot_bar(index + bar_width*2, win[2], labels = '2 Wins')\nrects3 = plot_bar(index + bar_width*3, win[3], labels = '3 Wins')\nrects4 = plot_bar(index + bar_width*4, win[4], labels = '4 Wins')\nrects5 = plot_bar(index + bar_width*5, win[5], labels = '5 Wins')\n\nplt.xticks(index + bar_width, index)\nplt.xlabel('Year')\nplt.ylabel('Number of Wins')\nplt.title('Distribution of Wins each Year')\nplt.legend()","8ef02306":"# Percentage of people who did not place in the top 10 each year\ntop10 = df.groupby('Year')['Top 10'].value_counts()\ntop10 = top10.unstack()\ntop10.fillna(0, inplace=True)\nplayers = top10.apply(lambda x: np.sum(x), axis=1)\n\nno_top10 = top10[0]\/players * 100\nprint(no_top10)","c0482f77":"# Who are some of the longest hitters \ndistance = df[['Year','Player Name','Avg Distance']].copy()\ndistance.sort_values(by='Avg Distance', inplace=True, ascending=False)\nprint(distance.head())","8a653afc":"# Who made the most money\nmoney_ranking = df[['Year','Player Name','Money']].copy()\nmoney_ranking.sort_values(by='Money', inplace=True, ascending=False)\nprint(money_ranking.head())","6891abcf":"# Who made the most money each year\nmoney_rank = money_ranking.groupby('Year')['Money'].max()\nmoney_rank = pd.DataFrame(money_rank)\nprint(money_rank.iloc[0,0])\n\nindexs = np.arange(2010, 2019)\nnames = []\nfor i in range(money_rank.shape[0]):\n    temp = df.loc[df['Money'] == money_rank.iloc[i,0],'Player Name']\n    names.append(str(temp.values[0]))\n\nmoney_rank['Player Name'] = names\nprint(money_rank)","6b6c734d":"# Looking at the changes in statistics over time \nf, ax = plt.subplots(nrows = 5, ncols = 3, figsize=(35,65))\ndistribution = df.loc[:,(df.columns!='Player Name') & (df.columns!='Wins')].columns\ndistribution = distribution[distribution != 'Year']\n\nprint(distribution)\nrows = 0\ncols = 0\nfor i, column in enumerate(distribution):\n    p = sns.boxplot(x = 'Year', y = column, data=df, ax=ax[rows][cols], showfliers=False)\n    p.set_ylabel(column,fontsize=20)\n    p.set_xlabel('Year',fontsize=20)\n    cols += 1\n    if cols == 3:\n        cols = 0\n        rows += 1\n","563fa495":"# Defining the players that had a win or more in each year \nchampion = df.loc[df['Wins'] >= 1, :]\nprint(champion.head())","5ea82c33":"f, ax = plt.subplots(nrows = 8, ncols = 2, figsize=(35,65))\ndistribution = df.loc[:,df.columns!='Player Name'].columns\ndistribution = distribution[distribution != 'Year']\n\nrows = 0\ncols = 0\nlower_better = ['Average Putts', 'Average Score']\nfor i, column in enumerate(distribution):\n    avg = df.groupby('Year')[column].mean()\n    best = champion.groupby('Year')[column].mean()\n    ax[rows,cols].plot(avg, 'o-',)\n    ax[rows,cols].plot(best, 'o-',)\n    ax[rows,cols].set_title(column, fontsize = 20)\n    \n    cols += 1\n    if cols == 2:\n        cols = 0\n        rows += 1","92573384":"# Plot the correlation matrix between variables \ncorr = df.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values,\n            cmap='coolwarm')\n","aba7e402":"df.corr()['Wins']","5cb54f64":"# Importing the Machine Learning modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVC  \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\nif __name__ == '__main__':\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', category=ImportWarning)","460c42e0":"# Adding the Winner column to determine if the player won that year or not \ndf['Winner'] = df['Wins'].apply(lambda x: 1 if x>0 else 0)\n\n# New DataFrame \nml_df = df.copy()\n\n# Y value for machine learning is the Winner column\ntarget = df['Winner']\n\n# Removing the columns Player Name, Wins, and Winner from the dataframe\nml_df.drop(['Player Name','Wins','Winner'], axis=1, inplace=True)\nprint(ml_df.head())","5b65522c":"per_no_win = target.value_counts()[0] \/ (target.value_counts()[0] + target.value_counts()[1])\nper_no_win = per_no_win.round(4)*100\nprint(str(per_no_win)+str('%'))","a9581041":"# Function for the logisitic regression \ndef log_reg(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                   random_state = 10)\n    clf = LogisticRegression().fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n         .format(clf.score(X_train, y_train)))\n    print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n         .format(clf.score(X_test, y_test)))\n    cf_mat = confusion_matrix(y_test, y_pred)\n    confusion = pd.DataFrame(data = cf_mat)\n    print(confusion)\n    \n    print(classification_report(y_test, y_pred))\n    \n    # Returning the 5 important features \n    rfe = RFE(clf, 5)\n    rfe = rfe.fit(X, y)\n    print('Feature Importance')\n    print(X.columns[rfe.ranking_ == 1].values)\n    \n    print('ROC AUC Score: {:.2f}'.format(roc_auc_score(y_test, y_pred)))","168c038a":"log_reg(ml_df, target)","8ae90092":"# Adding Domain Features \nml_d = ml_df.copy()\n# Top 10 \/ Money might give us a better understanding on how well they placed in the top 10\nml_d['Top10perMoney'] = ml_d['Top 10'] \/ ml_d['Money']\n\n# Avg Distance \/ Fairway Percentage to give us a ratio that determines how accurate and far a player hits \nml_d['DistanceperFairway'] = ml_d['Avg Distance'] \/ ml_d['Fairway Percentage']\n\n# Money \/ Rounds to see on average how much money they would make playing a round of golf \nml_d['MoneyperRound'] = ml_d['Money'] \/ ml_d['Rounds']","ed39c315":"log_reg(ml_d, target)","26df7c9c":"# Adding Polynomial Features to the ml_df \nmldf2 = ml_df.copy()\npoly = PolynomialFeatures(2)\npoly = poly.fit(mldf2)\npoly_feature = poly.transform(mldf2)\nprint(poly_feature.shape)\n\n# Creating a DataFrame with the polynomial features \npoly_feature = pd.DataFrame(poly_feature, columns = poly.get_feature_names(ml_df.columns))\nprint(poly_feature.head())","b0713ead":"log_reg(poly_feature, target)","0fc27797":"def svc_class(X,y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                   random_state = 10)\n    scaler = MinMaxScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    \n    svclassifier = SVC(kernel='rbf', C=10000)  \n    svclassifier.fit(X_train_scaled, y_train) \n    y_pred = svclassifier.predict(X_test_scaled) \n    print('Accuracy of SVM on training set: {:.2f}'\n         .format(svclassifier.score(X_train_scaled, y_train)))\n    print('Accuracy of SVM classifier on test set: {:.2f}'\n         .format(svclassifier.score(X_test_scaled, y_test)))\n\n    \n    print('ROC AUC Score: {:.2f}'.format(roc_auc_score(y_test, y_pred)))","213eabde":"svc_class(ml_df, target)","e34b1cbd":"svc_class(ml_d, target)","f3a07a3a":"svc_class(poly_feature, target)","b04dc822":"def random_forest(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                   random_state = 10)\n    clf = RandomForestClassifier(n_estimators=200).fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print('Accuracy of Random Forest classifier on training set: {:.2f}'\n         .format(clf.score(X_train, y_train)))\n    print('Accuracy of Random Forest classifier on test set: {:.2f}'\n         .format(clf.score(X_test, y_test)))\n    \n    cf_mat = confusion_matrix(y_test, y_pred)\n    confusion = pd.DataFrame(data = cf_mat)\n    print(confusion)\n    \n    print(classification_report(y_test, y_pred))\n    \n    # Returning the 5 important features \n    rfe = RFE(clf, 5)\n    rfe = rfe.fit(X, y)\n    print('Feature Importance')\n    print(X.columns[rfe.ranking_ == 1].values)\n    \n    print('ROC AUC Score: {:.2f}'.format(roc_auc_score(y_test, y_pred)))","4bd8bc6f":"random_forest(ml_df, target)","c5f895c3":"random_forest(ml_d, target)","9753fe3d":"random_forest(poly_feature, target)","9c35239c":"# New DataFrame \nearning_df = df.copy()\n\n# Y value for machine learning is the Money column\ntarget = earning_df['Money']\n\n# Removing the columns Player Name, Wins, Winner, Points, Top 10, and Money from the dataframe\nearning_df.drop(['Player Name','Wins','Winner','Points','Top 10','Money'], axis=1, inplace=True)\n\nprint(earning_df.head())","55e313a2":"# Importing the Machine Learning modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\n","40641596":"def linear_reg(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10)\n    clf = LinearRegression().fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n\n    print('R-Squared on training set: {:.3f}'\n          .format(clf.score(X_train, y_train)))\n    print('R-Squared on test set {:.3f}'\n          .format(clf.score(X_test, y_test)))\n    \n    print('linear model coeff (w):\\n{}'\n         .format(clf.coef_))\n    print('linear model intercept (b): {:.3f}'\n         .format(clf.intercept_))\n","2d9caf4a":"linear_reg(earning_df, target)","a61eb044":"# Creating a Polynomial Feature to improve R-Squared\npoly = PolynomialFeatures(2)\npoly = poly.fit(earning_df)\npoly_earning = poly.transform(earning_df)\nprint(poly_feature.shape)\n\n# Creating a DataFrame with the polynomial features \npoly_earning = pd.DataFrame(poly_feature, columns = poly.get_feature_names(earning_df.columns))","955db3a8":"linear_reg(poly_earning, target)","7b8d75eb":"# Adding a regularization penalty (Ridge)\ndef linear_reg_ridge(X, y, al):\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                   random_state = 10)\n    clf = Ridge(alpha = al).fit(X_train, y_train)\n\n    print('(poly deg 2 + ridge) R-squared score (training): {:.3f}'\n         .format(clf.score(X_train, y_train)))\n    print('(poly deg 2 + ridge) R-squared score (test): {:.3f}'\n         .format(clf.score(X_test, y_test)))\n    \n    print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n         .format(clf.coef_))\n    print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n         .format(clf.intercept_))","a72c1935":"linear_reg_ridge(poly_earning, target, al = 1)","184e2280":"linear_reg_ridge(poly_earning, target, al = 100)","e681c854":"from sklearn.model_selection import cross_val_score\n\ndef cross_val(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10)\n    clf = Ridge().fit(X_train, y_train)\n    scores = cross_val_score(clf, X, y, cv=5)\n    \n    print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    print(scores)","d96728a9":"cross_val(poly_earning, target)","74ff09f6":"# Using the Linear Regression to predict Tiger Wood's Earnings based on the Model\ndef find_earning(X,y,name,year):\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                   random_state = 10)\n    clf = Ridge().fit(X_train, y_train)\n    y_pred = clf.predict(X)\n    y_pred = pd.Series(y_pred)\n\n    pred_data = pd.concat([X, y_pred], axis=1)\n    pred_name = pd.concat([pred_data, df['Player Name']], axis=1)\n\n    return pred_name.loc[(pred_name['Player Name']==name) & (pred_name['Year']==year), 0]\n","ddf2b2aa":"print('Tiger Woods\\' Predicted Earning: ' + \n      str(find_earning(X = poly_earning, y = target, name = 'Tiger Woods', year = 2013).values[0]))\n\n# Tiger Wood's actual earnings in 2018 \ntw13 = df.loc[(df['Player Name']=='Tiger Woods') & (df['Year']==2013), 'Money']\nprint('Tiger Woods\\' Actual Earning: ' + str(tw13.values[0]))\n","0885c7fa":"### Distribution of the Data ","41f3e8b8":"With this table, we can examine the earnings of each player by year. Some of the most notable were Jordan Speith's earning of 12 million dollars and Justin Thomas earning the most money in both 2017 and 2018. ","bb7b8ab0":"### Correlation","d0577a20":"## 3. <a id='section_3'>Exploratory Data Analysis<\/a>\n<a href='#TOC'>Back to table of Contents<\/a>","e3c1b422":"We can see that Jordan Spieth has made the most amount of money in a year. Earning an outstanding total of 12 million dollars","c8f56252":"From the logisitic regression, we got an accuracy of 0.9 on the training set and an accuracy of 0.91 on the test set. This was surprisingly accurate for a first run. However, the ROC AUC Score of 0.78 could be improved. Therefore, I decided to add more features as a way of possibly improving the model.","ca6ebece":"From the correlation matrix, we can observe that Money is highly correlated to wins along with the FedExCup Points. We can also observe that the fairway percentage, year, and rounds are not correlated to Wins.","0cf49c66":"## Logistic Regression","70a58476":"### What I Learned\n\nFrom this notebook, I learned about numerous aspects of the game that differentiate the winner and the average PGA Tour player. For example, we can see that the fairway percentage and greens in regulations does not seems to contribute as much to a player's win. However, all the strokes gained statistics contribute pretty highly to wins for these players. It was interesting to see which aspects of the game that the professionals should put their time into. This also gave me the idea of track my personal golf statistics, so that I could compare it to the pros and find areas of my game that need the most improvement. \n\n### Machine Learning Model\n\nFrom this PGA Tour EDA and Machine Learning Models, I was able to examine the data of PGA Tour players, classify if a player will win that year or not, and predict their earnings. While, I believe that I can improve my prediction of their earnings, I am satisfied with my classification and regression model. With the random forest classification model, I was able to achieve an ROC AUC of 0.89 and an accuracy of 0.95 on the test set. This was a significant improvement from the ROC AUC of 0.78 and accuracy of 0.91. Because the data is skewed with approximately 80% of players not earning a win, the primary measure of the model was the ROC AUC. I was able to improve my model from ROC AUC score of 0.78 to a score of 0.89 by simply trying 3 different models, adding domain features, and polynomial features.\n\n### Moving Forward\n\nHaving done a simple regression model on predicting the earnings of golfers, I would like to come back to this project again with a deeper understanding of other regression models to attempt to better predict the earnings of golfers. As shown below, the model predicted that Tiger Woods would make 1.3 million dollars in the year 2013. But in fact, Tiger Woods made 8 million dollars. Tiger Woods is one of the best players in the world is definately an outlier. However, I would like to come up with a model that can better predict the earnings of even the best players in the world. \n\nIf you made it to the end of this notebook, I hope you learned something about the PGA Tour statistics!","1cd99c70":"## 5. <a id='section_5'>Machine Learning Model (Regression)<\/a>\n<a href='#TOC'>Back to table of Contents<\/a>\n\nCan we predict a golfer's earnings by only looking at their statistics. (Not looking at their placings in the year)","795f36e5":"From the box plot above, we can observe that the percentages of players without a win are around 80%. There wa also a negligible amount of variation in the percentage of players without a win in the past 8 years. ","c08ce497":"### Exploring Wins and Top 10 Placings by Year","fb25a8e1":"## 2. <a id='section_2'>Data Cleaning<\/a>\n<a href='#TOC'>Back to table of Contents<\/a>\n\nFrom a rough look at the initial data, I realized that the data needs to be further cleaned. \n- For the columns Top 10 and Wins, convert the NaNs to 0s. \n- Change Top 10 and Wins into an int\n- Drop NaN values for players who do not have the full statistics\n- Change the columns Rounds into int \n- Change points to int \n- Remove the dollar sign ($) and commas in the column Money","ef9e2b38":"### Cross Validation ","1b01d7b9":"### Golf Statistics over Time","67f78691":"## 4. <a id='section_4'>Machine Learning Model (Classification)<\/a>\n<a href='#TOC'>Back to table of Contents<\/a>\n\nTo predict winners, I used multiple machine learning models to explore which models could accuracy classify if a player is going to win in that year. \n\nTo measure the models, I used Receiver Operating Characterisitc Area Under the Curve. (ROC AUC) The ROC AUC tells us how capable the model is at distinguishing players with a win. In addition, as the data is skewed with 83% of players having no wins in that year, ROC AUC is a better measure than the accuracy of the model. ","53266ed2":"### Comparing the Average and Champions","97329580":"We know from the calculation above that the data for wins is skewed. Even without machine learning we know that approximately 83% of the players does not lead to a win. Therefore, we will be utilizing ROC AUC as the primary measure of these models","04e2036b":"### Preparing the Data for Regression ","8f582d75":"### Importing Packages ","1e99032e":"Out of the 3 models that I implemented, I had the most success with the ridge regression with a polynomial degree of 2 and an alpha of 1. This ridge regression had a R-squared value of 0.770 which was only slightly better than the polynomial regression. ","b6ea971e":"### Importing and Examining the Data","b926f664":"## 6. <a id='section_6'>Conclusion<\/a>\n<a href='#TOC'>Back to table of Contents<\/a>","420a9dff":"By looking at the percentage of players that did not place in the top 10 by year, We can observe that only approximately 20% of players did not place in the Top 10. In addition, the range for these player that did not place in the Top 10 is only 9.47%. This tells us that this statistic does not vary much on a yearly basis. ","5bb944cf":"From the distributions plotted, It appears that most of the graphs are normally distributed. However, we can observe that Money, Points, Wins, and Top 10s tend to are all skewed to the right. This could be explained by the separation of the best players and the average PGA Tour player. The best players have multiple placings in the Top 10 with wins that allows them to earn more from tournaments, while the average player will have no wins and only a few Top 10 placings that prevent them from earning as much. ","fed83cca":"### Feature Engineering","a5705999":"Having grown up watching golf, I have always been interested in exploring what sets the best golfers (golfers with wins) apart from the rest. Therefore, I decided to explore their statistics. To collect all the data, I scraped the data from the [PGA Tour website](https:\/\/www.pgatour.com\/stats.html) using python libraries such as beautifulsoup. (The code for the data collection is included in the repository)\n\nFrom this data, I performed an exploratory data analysis to explore the distribution of players on numerous aspects of the game, discover outliers, and further explore how the game has changed from 2010 to 2018. I also utilized numerous supervised machine learning models to predict a golfer's earnings and wins. \n\nTo predict the golfer's win, I used multiple classification methods such as logisitic regression, SVM (Support Vector Machines), and Random Forest Classification. I found that I had the best performance with the Random Forest Classification method. To predict the golfer's earnings, I used linear regression, polynomial features with linear regression, and ridge regression. ","336196b8":"By looking at the distribution of Wins each year, we can see that it is rare for most players to even win a tournament in the PGA Tour. Majority of players do not win, and a very few number of players win more than once a year.","f68d84a5":"## 1. <a id='section_1'>Description of the Data<\/a>\n<a href='#TOC'>Back to table of Contents<\/a>\n  \npgaTourData.csv contains 1674 rows and 18 columns. Each row indicates a golfer's performance for that year.\n\n- Player Name: Name of the golfer\n- Rounds: The number of games that a player played  \n- Fairway Percentage: The percentage of time a tee shot lands on the fairway\n- Year: The year in which the statistic was collected \n- Avg Distance: The average distance of the tee-shot \n- gir: (Green in Regulation) is met if any part of the ball is touching the putting surface while the number of strokes taken is at least two fewer than par\n- Average Putts: The average number of strokes taken on the green \n- Average Scrambling: Scrambling is when a player misses the green in regulation, but still makes par or better on a hole\n- Average Score: Average Score is the average of all the scores a player has played in that year \n- Points: The number of FedExCup points a player earned in that year. These points can be earned by competing in tournaments.\n- Wins: The number of competition a player has won in that year \n- Top 10: The number of competitions where a player has placed in the Top 10\n- Average SG Putts: Strokes gained: putting measures how many strokes a player gains (or loses) on the greens.\n- Average SG Total: The Off-the-tee + approach-the-green + around-the-green + putting statistics combined\n- SG:OTT: Strokes gained: off-the-tee measures  player performance off the tee on all par-4s and par-5s. \n- SG:APR: Strokes gained: approach-the-green measures player performance on approach shots. Approach shots include all shots that are not from the tee on par-4 and par-5 holes and are not included in strokes gained: around-the-green and strokes gained: putting. Approach shots include tee shots on par-3s.\n- SG:ARG: Strokes gained: around-the-green measures player performance on any shot within 30 yards of the edge of the green. This statistic does not include any shots taken on the putting green.\n- Money: The amount of prize money a player has earned from tournaments\n\nThe official explanation for strokes gained is included [here](https:\/\/www.pgatour.com\/news\/2016\/05\/31\/strokes-gained-defined.html).","ccfd5bc2":"### Exploring the Earnings of Players","84725445":"### Exploring the Longest Hitters","e91d0f4b":"From this table, we can see that most players end the year without a win. In fact it is pretty rare to find a player that has won more than once. ","2fd41815":"The Random Forest Model was scored highly on ROC AUC Score, obtaining a value of 0.89. With this, we observed that the Random Forest Model and the Support Vector Machine Models could accurately classify players with and without a win. ","9547e8c8":"With Support Vector Machines, the ROC AUC Scores were significantly better. The SVM scored a 0.89 on the data with domain features that I included compared to the score of 0.75 on the logisitic regression with polynomial features.","1434abc4":"Something that I found interesting by plotting each variable across time was majority of the statistics had little to no change for Professional Golfers in the past 8 years. However, some of the areas where there were changes were in Money, Average Score, and Rounds. \n\nThis was rather interesting as golf club manufacturers would often advertise about the huge improvements in distance for players when they switched to their latest club. But in fact, there was only an increase in the average distance of 10 yards. ","e8d2d54c":"### SVM (Support Vector Machine)","93fe048a":"## Can We Predict If a PGA Tour Player Won a Tournament in That Year and Their Earnings?","9aa2e49f":"## Preparing the Data for Classification \n","e0150248":"### Application of the Linear Regression Model ","72051c25":"# PGA Tour Machine Learning Project ","f29177aa":"From the Graphs above, we can see the average scores of the best players (players with a win) versus the PGA Tour average. This can give us an indication to which statistics help players win. \n\nWe can see that the fairway percentage and greens in regulations does not seems to contribute as much to a player's win. However, we can see that all the strokes gained statistics have a large impact on the wins of these players. In addition, we can see that the average score and average putts are lower for players with a win. ","420bc400":"### Random Forest Model\n","31738ba0":"From feature engineering, there were no improvements in the ROC AUC Score. In fact as I added more features, the accuracy and the ROC AUC Score decreased. This could signal to us that another machine learning algorithm could better predict winners.","99922582":"We can see that Rory McIlroy is one of the longest hitters in the game, setting the average driver distance to be 319.7 yards in 2018. He was also the longest hitter in 2017 with an average of 316.7 yards. There are other notable players like J.B. Holmes and Dustin Johnson who have an average of over 317 yards.","6c07ca23":"## <a id='TOC'>Table of Contents<\/a>\n<ol>\n<li><a href='#section 1'>Description of the Data<\/a><\/li>\n<li><a href='#section_2'>Data Cleaning<\/a><\/li>\n<li><a href='#section_3'>Exploratory Data Analysis<\/a><\/li>\n<li><a href='#section_4'>Machine Learning Model (Classification)<\/a><\/li>\n<li><a href='#section_5'>Machine Learning Model (Regression)<\/a><\/li>\n<li><a href='#section_6'>Conclusion<\/a><\/li>\n<\/ol>","66a4daea":"We can see that the data has 1674 rows and 18 columns."}}