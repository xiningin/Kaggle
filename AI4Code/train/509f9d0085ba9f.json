{"cell_type":{"3aa2055c":"code","9b63bea6":"code","6259800d":"code","1adea93f":"code","e85e9365":"code","4508e093":"code","e7f10b7c":"code","1ea72c65":"code","6a169028":"code","11250541":"code","f9a4eb3f":"code","063884d7":"code","5ca6327b":"code","b3f83908":"code","763d3107":"code","a3c7f57b":"code","a16c6cf5":"code","d78777d2":"code","d42c1a0d":"code","ee0fbaa6":"code","fcd0b583":"code","62cb65b7":"code","2090490e":"code","454c79da":"code","42a75abb":"code","6016d874":"code","abe6d572":"markdown","f2e133dd":"markdown","4e0389f3":"markdown","657252c7":"markdown","8b593c67":"markdown","1cdde273":"markdown","be7bfab2":"markdown","29cd518b":"markdown","ce9f3a26":"markdown","fcb68405":"markdown","34e51b93":"markdown","f6f36a67":"markdown","4cca99b0":"markdown","88d06f21":"markdown","8a15ce4e":"markdown","0d4ec8c9":"markdown","a44f440d":"markdown","117d63e6":"markdown","701c1a72":"markdown","cdee9567":"markdown","d4256249":"markdown","ecc6f887":"markdown","ab24dc8f":"markdown","c1aecc5c":"markdown","40c9b626":"markdown","44590d3c":"markdown"},"source":{"3aa2055c":"# import packages\nimport os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\n# setting up options\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nwarnings.filterwarnings('ignore')\n\n# import datasets\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')","9b63bea6":"train_df.head()","6259800d":"print(f'Number of rows: {train_df.shape[0]};  Number of columns: {train_df.shape[1]}; No of missing values: {sum(train_df.isna().sum())}')","1adea93f":"train_df.dtypes","e85e9365":"train_df.describe()","4508e093":"print('Target column basic statistics:')\ntrain_df['target'].describe()","e7f10b7c":"print('Frequency of each target classes:')\ntrain_df['target'].value_counts()","1ea72c65":"test_df.head()","6a169028":"print(f'Number of rows: {test_df.shape[0]};  Number of columns: {test_df.shape[1]}; No of missing values: {sum(test_df.isna().sum())}')","11250541":"test_df.dtypes","f9a4eb3f":"test_df.describe()","063884d7":"submission.head()","5ca6327b":"features = [feature for feature in train_df.columns if feature not in ['id', 'target']]\nunique_values_train = np.zeros(2)\nfor feature in features:\n    temp = train_df[feature].unique()\n    unique_values_train = np.concatenate([unique_values_train, temp])\nunique_values_train = np.unique(unique_values_train)\n\nunique_values_test = np.zeros(2)\nfor feature in features:\n    temp = test_df[feature].unique()\n    unique_values_test = np.concatenate([unique_values_test, temp])\nunique_values_test = np.unique(unique_values_test)\n\nunique_value_feature_train = pd.DataFrame(train_df[features].nunique())\nunique_value_feature_train = unique_value_feature_train.reset_index(drop=False)\nunique_value_feature_train.columns = ['Features', 'Count']\nunique_value_feature_test = pd.DataFrame(test_df[features].nunique())\nunique_value_feature_test = unique_value_feature_test.reset_index(drop=False)\nunique_value_feature_test.columns = ['Features', 'Count']\n\nunique_value_feature_diff = unique_value_feature_train.copy()\nunique_value_feature_diff['Count'] = unique_value_feature_train['Count'] - unique_value_feature_test['Count']\nunique_value_feature_diff = unique_value_feature_diff[unique_value_feature_diff['Count']!=0]\n\ntranspose_features_train = train_df[features]\ntranspose_features_train = transpose_features_train.apply(pd.Series.value_counts, axis=1).fillna(0)\ntranspose_features_test = test_df[features]\ntranspose_features_test = transpose_features_test.apply(pd.Series.value_counts, axis=1).fillna(0)","b3f83908":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(5, 0.8), facecolor='#f6f5f5')\ngs = fig.add_gridspec(2, 1)\ngs.update(wspace=0.4, hspace=0.8)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ffd514'])\n\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\", \"left\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0.tick_params(axis = \"y\", which = \"both\", left = False)\nax0_sns = sns.histplot(ax=ax0, x=unique_values_train, zorder=2, bins=352, linewidth=0, alpha=1)\nax0_sns.set_xlabel(\"Unique values\",fontsize=4, weight='bold')\nax0_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax0_sns.get_yaxis().set_visible(False)\nax0.text(-18, 2.3, 'Unique Values', fontsize=6, ha='left', va='top', weight='bold')\nax0.text(-18, 1.9, 'Unique value for train and test dataset', fontsize=4, ha='left', va='top')\nax0_sns.legend(['Train', 'Test'], ncol=2, facecolor=background_color, edgecolor=background_color, fontsize=3, bbox_to_anchor=(-0.01, 1.6), loc='upper left')\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ff355d'])\n\nax1 = fig.add_subplot(gs[1, 0])\nfor s in [\"right\", \"top\", \"left\"]:\n    ax1.spines[s].set_visible(False)\nax1.set_facecolor(background_color)\nax1.tick_params(axis = \"y\", which = \"both\", left = False)\nax1_sns = sns.histplot(ax=ax1, x=unique_values_test, zorder=2, bins=352, linewidth=0, alpha=1)\nax1_sns.set_xlabel(\"Unique values\",fontsize=4, weight='bold')\nax1_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax1_sns.get_yaxis().set_visible(False)\nax1_sns.legend(['Test'], ncol=2, facecolor=background_color, edgecolor=background_color, fontsize=3, bbox_to_anchor=(0.06, 3.4), loc='upper left')\n\nplt.show()","763d3107":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(4, 12), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.4, hspace=0.1)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ffd514']*75)\n\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0_sns = sns.barplot(ax=ax0, y=unique_value_feature_train['Features'], x=unique_value_feature_train['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"Unique Values\",fontsize=4, weight='bold')\nax0_sns.set_ylabel(\"Features\",fontsize=4, weight='bold')\nax0_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.text(0, -1.9, 'Unique Values - Train Dataset', fontsize=6, ha='left', va='top', weight='bold')\nax0.text(0, -1.2, 'feature_15 has the highest unique value', fontsize=4, ha='left', va='top')\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():.0f}'\n    x = p.get_x() + p.get_width() + 7\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='center', va='center', fontsize=4, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ff355d']*75)\n\nax1 = fig.add_subplot(gs[0, 1])\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1.set_facecolor(background_color)\nax1_sns = sns.barplot(ax=ax1, y=unique_value_feature_test['Features'], x=unique_value_feature_test['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax1_sns.set_xlabel(\"Unique Values\",fontsize=4, weight='bold')\nax1_sns.set_ylabel(\"Features\",fontsize=4, weight='bold')\nax1_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax1_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1.text(0, -1.9, 'Unique Values - Test Dataset', fontsize=6, ha='left', va='top', weight='bold')\nax1.text(0, -1.2, 'feature_15 also has the highest unique value in test dataset', fontsize=4, ha='left', va='top')\nfor p in ax1.patches:\n    value = f'{p.get_width():.0f}'\n    x = p.get_x() + p.get_width() + 7\n    y = p.get_y() + p.get_height() \/ 2 \n    ax1.text(x, y, value, ha='center', va='center', fontsize=4, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n\nplt.show()","a3c7f57b":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(5, 1.5), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0.4, hspace=0.1)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#00A4CCFF']*6)\n\nax = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax.spines[s].set_visible(False)\nax.set_facecolor(background_color)\nax_sns = sns.barplot(ax=ax, x=unique_value_feature_diff['Features'], \n                      y=unique_value_feature_diff['Count'], \n                      zorder=2, linewidth=0, alpha=1, saturation=1)\nax_sns.set_xlabel(\"Features\",fontsize=4, weight='bold')\nax_sns.set_ylabel(\"Unique values\",fontsize=4, weight='bold')\nax_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax.text(-0.5, 15.5, 'Unique Values Differences', fontsize=6, ha='left', va='top', weight='bold')\nax.text(-0.5, 14.5, 'Positive means that train dataset has higher unique value than test dataset', fontsize=4, ha='left', va='top')\n# data label\nfor p in ax.patches:\n    percentage = f'{p.get_height():.0f}'\n    x = p.get_x() + p.get_width() \/ 2\n    y = p.get_height() + 0.8\n    ax.text(x, y, percentage, ha='center', va='center', fontsize=4,\n           bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n\nplt.show()","a16c6cf5":"mean_unique_value_train = pd.DataFrame(transpose_features_train.mean(axis=0))\nmean_unique_value_train = mean_unique_value_train.reset_index(drop=False)\nmean_unique_value_train.columns = ['Unique', 'Mean']\nmean_unique_value_train = mean_unique_value_train.sort_values('Mean', ascending=False)[:10]\nmean_unique_value_train\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(5, 1.5), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0.4, hspace=0.1)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ffd514']*10)\n\nax = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax.spines[s].set_visible(False)\nax.set_facecolor(background_color)\nax_sns = sns.barplot(ax=ax, x=mean_unique_value_train['Unique'], \n                      y=mean_unique_value_train['Mean'], \n                      zorder=2, linewidth=0, alpha=1, saturation=1)\nax_sns.set_xlabel(\"Unique values\",fontsize=4, weight='bold')\nax_sns.set_ylabel(\"Mean occurance\",fontsize=4, weight='bold')\nax_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax.text(-0.5, 61, 'Mean Unique Values - Train Dataset', fontsize=6, ha='left', va='top', weight='bold')\nax.text(-0.5, 57, 'Number zero is dominating in every row', fontsize=4, ha='left', va='top')\n# data label\nfor p in ax.patches:\n    percentage = f'{p.get_height():.1f}'\n    x = p.get_x() + p.get_width() \/ 2\n    y = p.get_height() + 3\n    ax.text(x, y, percentage, ha='center', va='center', fontsize=4,\n           bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n\nplt.show()","d78777d2":"mean_unique_value_test = pd.DataFrame(transpose_features_test.mean(axis=0))\nmean_unique_value_test = mean_unique_value_test.reset_index(drop=False)\nmean_unique_value_test.columns = ['Unique', 'Mean']\nmean_unique_value_test = mean_unique_value_test.sort_values('Mean', ascending=False)[:10]\nmean_unique_value_test\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(5, 1.5), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0.4, hspace=0.1)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ff355d']*10)\n\nax = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax.spines[s].set_visible(False)\nax.set_facecolor(background_color)\nax_sns = sns.barplot(ax=ax, x=mean_unique_value_test['Unique'], \n                      y=mean_unique_value_test['Mean'], \n                      zorder=2, linewidth=0, alpha=1, saturation=1)\nax_sns.set_xlabel(\"Unique values\",fontsize=4, weight='bold')\nax_sns.set_ylabel(\"Mean occurance\",fontsize=4, weight='bold')\nax_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax.text(-0.5, 61, 'Mean Unique Values - Test Dataset', fontsize=6, ha='left', va='top', weight='bold')\nax.text(-0.5, 57, 'Number zero is dominating in every row', fontsize=4, ha='left', va='top')\n# data label\nfor p in ax.patches:\n    percentage = f'{p.get_height():.1f}'\n    x = p.get_x() + p.get_width() \/ 2\n    y = p.get_height() + 3\n    ax.text(x, y, percentage, ha='center', va='center', fontsize=4,\n           bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n\nplt.show()","d42c1a0d":"zero_positive_train = pd.DataFrame()\nzero_positive_train['zero'] = transpose_features_train.iloc[:, 0]\nzero_positive_train['positive'] = transpose_features_train.iloc[:,1:].sum(axis=1)\n\nzero_positive_test = pd.DataFrame()\nzero_positive_test['zero'] = transpose_features_test.iloc[:, 0]\nzero_positive_test['positive'] = transpose_features_test.iloc[:,1:].sum(axis=1)\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(5, 2), facecolor='#f6f5f5')\ngs = fig.add_gridspec(2, 1)\ngs.update(wspace=0.4, hspace=1)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#00A4CCFF'])\n\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0_sns = sns.histplot(ax=ax0, x=zero_positive_train['positive'], zorder=2, color='#ff5573', linewidth=0.4, alpha=0.9)\nax0_sns = sns.histplot(ax=ax0, x=zero_positive_train['zero'], zorder=2, linewidth=0.4, alpha=0.9)\nax0_sns.set_xlabel(\"#No of occurance\",fontsize=4, weight='bold')\nax0_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax0.text(-4, 8250, 'Train Dataset ', fontsize=6, ha='left', va='top', weight='bold')\nax0.text(-4, 7250, 'Zero has a higher occurance compare to positive numbers in train dataset', fontsize=4, ha='left', va='top')\nax0_sns.legend(['Positive', 'Zero'], ncol=2, facecolor=background_color, edgecolor=background_color, fontsize=3, bbox_to_anchor=(-0.01, 1.2), loc='upper left')\nax0_sns.set_ylabel(\"Count\",fontsize=4, weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n\nax1 = fig.add_subplot(gs[1, 0])\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1.set_facecolor(background_color)\nax1_sns = sns.histplot(ax=ax1, x=zero_positive_test['positive'], zorder=2, color='#ff5573', linewidth=0.4, alpha=0.9)\nax1_sns = sns.histplot(ax=ax1, x=zero_positive_test['zero'], zorder=2, linewidth=0.4, alpha=0.9)\nax1_sns.set_xlabel(\"#No of occurance\",fontsize=4, weight='bold')\nax1_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax1.text(-4, 4250, 'Test Dataset ', fontsize=6, ha='left', va='top', weight='bold')\nax1.text(-4, 3700, 'Zero is also have a higher occurance compare to positive numbers in test dataset', fontsize=4, ha='left', va='top')\nax1_sns.legend(['Positive', 'Zero'], ncol=2, facecolor=background_color, edgecolor=background_color, fontsize=3, bbox_to_anchor=(-0.01, 1.2), loc='upper left')\nax1_sns.set_ylabel(\"Count\",fontsize=4, weight='bold')\nax1_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n\nplt.show()","ee0fbaa6":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ffd514']*25)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.5, 92, 'Top 5 Values - Train Dataset', fontsize=10, fontweight='bold')\nax0.text(-0.5, 85, 'feature_0 - feature_24', fontsize=6, fontweight='light')        \n\nfeatures = list(train_df.columns[1:26])\n\nrun_no = 0\nfor col in features:\n    temp_df = pd.DataFrame(train_df[col].value_counts())[:5]\n    temp_df = temp_df.reset_index(drop=False)\n    temp_df.columns = ['Number', 'Count']\n    sns.barplot(ax=locals()[\"ax\"+str(run_no)],x=temp_df['Number'], y=temp_df['Count']\/len(train_df)*100, zorder=2, linewidth=0, alpha=1, saturation=1)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].yaxis.set_major_formatter(ticker.PercentFormatter())\n    # data label\n    for p in locals()[\"ax\"+str(run_no)].patches:\n        percentage = f'{p.get_height():.1f}%\\n'\n        x = p.get_x() + p.get_width() \/ 2\n        y = p.get_height()\n        locals()[\"ax\"+str(run_no)].text(x, y, percentage, ha='center', va='center', fontsize=4)\n\n    run_no += 1\n\nplt.show()","fcd0b583":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ffd514']*25)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.5, 65, 'Top 5 Values - Train Dataset', fontsize=10, fontweight='bold')\nax0.text(-0.5, 60, 'feature_25 - feature_49', fontsize=6, fontweight='light')        \n\nfeatures = list(train_df.columns[26:51])\n\nrun_no = 0\nfor col in features:\n    temp_df = pd.DataFrame(train_df[col].value_counts())[:5]\n    temp_df = temp_df.reset_index(drop=False)\n    temp_df.columns = ['Number', 'Count']\n    sns.barplot(ax=locals()[\"ax\"+str(run_no)],x=temp_df['Number'], y=temp_df['Count']\/len(train_df)*100, zorder=2, linewidth=0, alpha=1, saturation=1)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].yaxis.set_major_formatter(ticker.PercentFormatter())\n    # data label\n    for p in locals()[\"ax\"+str(run_no)].patches:\n        percentage = f'{p.get_height():.1f}%\\n'\n        x = p.get_x() + p.get_width() \/ 2\n        y = p.get_height()\n        locals()[\"ax\"+str(run_no)].text(x, y, percentage, ha='center', va='center', fontsize=4)\n\n    run_no += 1\n\nplt.show()","62cb65b7":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ffd514']*25)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.5, 52, 'Top 5 Values - Train Dataset', fontsize=10, fontweight='bold')\nax0.text(-0.5, 48, 'feature_50 - feature_74', fontsize=6, fontweight='light')        \n\nfeatures = list(train_df.columns[51:76])\n\nrun_no = 0\nfor col in features:\n    temp_df = pd.DataFrame(train_df[col].value_counts())[:5]\n    temp_df = temp_df.reset_index(drop=False)\n    temp_df.columns = ['Number', 'Count']\n    sns.barplot(ax=locals()[\"ax\"+str(run_no)],x=temp_df['Number'], y=temp_df['Count']\/len(train_df)*100, zorder=2, linewidth=0, alpha=1, saturation=1)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].yaxis.set_major_formatter(ticker.PercentFormatter())\n    # data label\n    for p in locals()[\"ax\"+str(run_no)].patches:\n        percentage = f'{p.get_height():.1f}%\\n'\n        x = p.get_x() + p.get_width() \/ 2\n        y = p.get_height()\n        locals()[\"ax\"+str(run_no)].text(x, y, percentage, ha='center', va='center', fontsize=4)\n\n    run_no += 1\n\nplt.show()","2090490e":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ff355d']*25)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.5, 92, 'Top 5 Values - Test Dataset', fontsize=10, fontweight='bold')\nax0.text(-0.5, 85, 'feature_0 - feature_24', fontsize=6, fontweight='light')        \n\nfeatures = list(test_df.columns[1:26])\n\nrun_no = 0\nfor col in features:\n    temp_df = pd.DataFrame(test_df[col].value_counts())[:5]\n    temp_df = temp_df.reset_index(drop=False)\n    temp_df.columns = ['Number', 'Count']\n    sns.barplot(ax=locals()[\"ax\"+str(run_no)],x=temp_df['Number'], y=temp_df['Count']\/len(test_df)*100, zorder=2, linewidth=0, alpha=1, saturation=1)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].yaxis.set_major_formatter(ticker.PercentFormatter())\n    # data label\n    for p in locals()[\"ax\"+str(run_no)].patches:\n        percentage = f'{p.get_height():.1f}%\\n'\n        x = p.get_x() + p.get_width() \/ 2\n        y = p.get_height()\n        locals()[\"ax\"+str(run_no)].text(x, y, percentage, ha='center', va='center', fontsize=4)\n\n    run_no += 1\n\nplt.show()","454c79da":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ff355d']*25)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.5, 66, 'Top 5 Values - Test Dataset', fontsize=10, fontweight='bold')\nax0.text(-0.5, 60, 'feature_25 - feature_49', fontsize=6, fontweight='light')        \n\nfeatures = list(test_df.columns[26:51])\n\nrun_no = 0\nfor col in features:\n    temp_df = pd.DataFrame(test_df[col].value_counts())[:5]\n    temp_df = temp_df.reset_index(drop=False)\n    temp_df.columns = ['Number', 'Count']\n    sns.barplot(ax=locals()[\"ax\"+str(run_no)],x=temp_df['Number'], y=temp_df['Count']\/len(test_df)*100, zorder=2, linewidth=0, alpha=1, saturation=1)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].yaxis.set_major_formatter(ticker.PercentFormatter())\n    # data label\n    for p in locals()[\"ax\"+str(run_no)].patches:\n        percentage = f'{p.get_height():.1f}%\\n'\n        x = p.get_x() + p.get_width() \/ 2\n        y = p.get_height()\n        locals()[\"ax\"+str(run_no)].text(x, y, percentage, ha='center', va='center', fontsize=4)\n\n    run_no += 1\n\nplt.show()","42a75abb":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ff355d']*25)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.5, 53, 'Top 5 Values - Test Dataset', fontsize=10, fontweight='bold')\nax0.text(-0.5, 48, 'feature_50 - feature_74', fontsize=6, fontweight='light')        \n\nfeatures = list(test_df.columns[51:76])\n\nrun_no = 0\nfor col in features:\n    temp_df = pd.DataFrame(test_df[col].value_counts())[:5]\n    temp_df = temp_df.reset_index(drop=False)\n    temp_df.columns = ['Number', 'Count']\n    sns.barplot(ax=locals()[\"ax\"+str(run_no)],x=temp_df['Number'], y=temp_df['Count']\/len(test_df)*100, zorder=2, linewidth=0, alpha=1, saturation=1)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].yaxis.set_major_formatter(ticker.PercentFormatter())\n    # data label\n    for p in locals()[\"ax\"+str(run_no)].patches:\n        percentage = f'{p.get_height():.1f}%\\n'\n        x = p.get_x() + p.get_width() \/ 2\n        y = p.get_height()\n        locals()[\"ax\"+str(run_no)].text(x, y, percentage, ha='center', va='center', fontsize=4)\n\n    run_no += 1\n\nplt.show()","6016d874":"temp_target = pd.DataFrame(train_df['target'].value_counts()\/len(train_df))\ntemp_target = temp_target.reset_index(drop=False)\ntemp_target = temp_target.sort_values('index')\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(5, 1.5), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0.4, hspace=0.1)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#00A4CCFF']*9)\n\nax = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax.spines[s].set_visible(False)\nax.set_facecolor(background_color)\nax_sns = sns.barplot(ax=ax, x=temp_target['index'], \n                      y=temp_target['target'] * 100, \n                      zorder=2, linewidth=0, alpha=1, saturation=1)\nax_sns.set_xlabel(\"Class\",fontsize=4, weight='bold')\nax_sns.set_ylabel(\"\",fontsize=4, weight='bold')\nax_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax.text(-0.5, 31.2, 'Target Distribution', fontsize=6, ha='left', va='top', weight='bold')\nax.text(-0.5, 29, 'Target variable is doiminated by class_6 and class_8', fontsize=4, ha='left', va='top')\nax.yaxis.set_major_formatter(ticker.PercentFormatter())\n# data label\nfor p in ax.patches:\n    percentage = f'{p.get_height():.1f}%'\n    x = p.get_x() + p.get_width() \/ 2\n    y = p.get_height() + 1.5\n    ax.text(x, y, percentage, ha='center', va='center', fontsize=4,\n           bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n\nplt.show()","abe6d572":"### 4.1.3 Individual features\n\nCount how many unique values in each features and perform differences calculation on train and test datasets to see how both datasets differ from one to another.\n\n**Observations:**\n- `feature_15` has the highest unique values that are above 100 followed by `feature_46`, `feature_59`, `feature_60` and `feature_73` with more than 80 unique values.\n- There are differences between train and test dataset unique values on `feature_15`, `feature_28`, `feature_46`, `feature_59`, `feature_60` and `feature_73`. The highest differences is from `feature_60` which is more than `10`.","f2e133dd":"# Table of Contents\n<a id=\"table-of-contents\"><\/a>\n- [1 Introduction](#1)\n- [2 Preparations](#2)\n- [3 Datasets Overview](#3)\n    - [3.1 Train dataset](#3.1)\n    - [3.2 Test dataset](#3.2)\n    - [3.3 Submission](#3.3)\n- [4 Features](#4)\n    - [4.1 Unique values](#4.1)\n    - [4.2 Distribution](#4.2)\n- [5 Target](#5)\n    - [5.1 Distribution](#5.1)","4e0389f3":"[back to top](#table-of-contents)\n<a id=\"4\"><\/a>\n# 4 Features\nNumber of features available to be used to create a prediction model are `75`. The analysis is started by looking on number of unique value on each features and progressed with a comparison between train and test dataset on each features.\n\n<a id=\"4.1\"><\/a>\n## 4.1 Unique values\nIn the structure of the train and test dataset, all of the features are in integer values. Counting number of unique value on train & test dataset wil be performed to see how feature are distributed.\n\n## 4.1.1 Preparation\nPrepare train dataset for data analysis and visualization. *(to see the details, please expand)*","657252c7":"#### 4.2.1.3 feature_50 - feature_74\n**Observations:**\n- Zero is dominating the occurance with more than 50% of each feature, except for `feature_50`, `feature_54` and `feature_56`.\n- Top 5 values in each feature has contributed more than 80% of each feature, except for `feature_54`.\n\n","8b593c67":"Similar to the train dataset, test dataset has `100,000` of rows with `76` of columns and there is `0` missing value. There is no `target` in the test dataset. ","1cdde273":"The dimension and number of missing values in the test dataset is as below:","be7bfab2":"### 3.2.3 Basic statistics\nBelow is the basic statistics for each variables which contain information on `count`, `mean`, `standard deviation`, `minimum`, `1st quartile`, `median`, `3rd quartile` and `maximum`.","29cd518b":"[back to top](#table-of-contents)\n<a id=\"4.2\"><\/a>\n## 4.2 Distribution\nShowing top 5 unique values distribution on each feature assuming the feature is classified as categorical variable. As there are 75 features, it will break down into 25 features for each sections. Top 5 distribution between train and test dataset are quite the same on each others.\n\n### 4.2.1 Train dataset\n**Observations:**\n- Zero is dominating the occurance with more than 50% of each feature, except for `feature_12`, `feature_18`, `feature_19`, `feature_39`, `feature_43`. `feature_50`, `feature_54` and `feature_56`.\n- Top 5 values in each feature has contributed more than 80% of each feature, except for `feature_19`, `feature_43` and `feature_54`.\n\n#### 4.2.1.1 feature_0 - feature_24\n**Observations:**\n- Zero is dominating the occurance with more than 50% of each feature, except for `feature_12`, `feature_18`, `feature_19` and `feature_20`.\n- Top 5 values in each feature has contributed more than 80% of each feature, except for `feature_19`.","ce9f3a26":"#### 4.2.2.2 feature_25 - feature_49\n`feature_25` - `feature_49` have a same observations as train dataset, below are the repeating observations.\n\n**Observations:**\n- Zero is dominating the occurance with more than 50% of each feature, except for `feature_31`, `feature_39` and `feature_43`.\n- Top 5 values in each feature has contributed more than 80% of each feature, except for `feature_43`.\n\n\n","fcb68405":"[back to top](#table-of-contents)\n<a id=\"3\"><\/a>\n# 3 Dataset Overview\nThe intend of the overview is to get a feel of the data and its structure in train, test and submission file. An overview on train and test datasets will include a quick analysis on missing values and basic statistics, while sample submission will be loaded to see the expected submission.\n\n<a id=\"3.1\"><\/a>\n## 3.1 Train dataset\nAs stated before, train dataset is mainly used to train predictive model as there is an available target variable in this set. This dataset is also used to explore more on the data itself including find a relation between each predictors and the target variable.\n\n**Observations:**\n- There are 9 classes to be predicted, start from `Class_1` to `Class_9` which dominated by `Class_6` followed by `Class_8` and `Class_9`.\n- All feature columns are in `int64` or integer format, which make a possibility to treat them as categorical or continuous features. A correct classfication may help to make a better prediction but it\u2019s hard to be done as there is no description and information about the features.\n- All of features have a `minimum` and `1st quartile` values of `0` which is stil the same of `median` except for median `feature_12`, `feature_18`, `feature_19`, `feature_20`, `feature_31`, `feature_39`, `feature_43`, `feature_50`, `feature_54` and `feature_56`.\n- Most of the features have a mean below 3, except for `feature_43` and `feature_54`.\n\n### 3.1.1 Quick view\nBelow is the first 5 rows of train dataset:","34e51b93":"### 3.2.2 Data types\nAll columns are in `int64` type which is consistent with the train dataset. *(to see the details, please expand)*","f6f36a67":"### 4.1.2 Range\n\nCollecting each unique values in train and test dataset and plot it into a bar chart to show the range of their unique values and to have a comparison between train and test datasets.\n\n**Observations:**\n- There are some differences of unique values between train and test dataset for value above 100. \n- Both train and test dataset has a range from 0 to 352.","4cca99b0":"The train dataset has `200,000` of rows with `77` of columns and there is `0` missing values which is good as there will be no extra step needed to take care of them.\n\n### 3.1.2 Data types\nExcept `target` column, all of the other columns are in `int64` type. *(to see the details, please expand)*","88d06f21":"[back to top](#table-of-contents)\n<a id=\"2\"><\/a>\n# 2 Preparations\nPreparing packages and data that will be used in the analysis process. Packages that will be loaded are mainly for data manipulation, data visualization and modeling. There are 2 datasets that are used in the analysis, they are train and test dataset. The main use of train dataset is to train models and use it to predict test dataset. While sample submission file is used to informed participants on the expected submission for the competition. *(to see the details, please expand)*","8a15ce4e":"[back to top](#table-of-contents)\n<a id=\"5\"><\/a>\n# 5 Target\nAnalyzing target variable by looking on how this variable are distributed and how each feature are distributed among each classes.\n\n<a id=\"5.1\"><\/a>\n## 5.1 Distribution\nTarget variable consists of `9` classes start from `class_1` and end at `class_9` on the train dataset.\n\n**Observations:**\n\n- There are 2 classes that dominated the target variable, they are `class_6` and `class_8` with `class_6` slightly higher than `class_8`. This two classes represent `52%` of total train dataset.\n- `class_2` and `class_9` are also about the same, each of them is contributing around `12%` of train dataset. Combining these 2 classes represent around `25%` of train dataset.\n- In third place, there are `class_2` and `class_7` that are contributing `7.4%`, each with total combination of `14.8%`.\n- It seems the target variable can be divided into 4 big chuncks: \n    - The biggest chunck is `class_6` and `class_8`.\n    - The second biggest chunk is`class_2` and `class_9`.\n    - The third biggest chunck is `class_2` and `class_7`.\n    - Other small chunck which is `class_1`, class_4` and `class_5`.","0d4ec8c9":"### 3.1.3 Basic statistics\nBelow is the basic statistics for each variables which contain information on `count`, `mean`, `standard deviation`, `minimum`, `1st quartile`, `median`, `3rd quartile` and `maximum`.\n\n#### 3.1.3.1 Id and features column","a44f440d":"#### 4.2.2.3 feature_50 - feature_74\n`feature_50` - `feature_74` have a same observations as train dataset, below are the repeating observations.\n\n**Observations:**\n- Zero is dominating the occurance with more than 50% of each feature, except for `feature_50`, `feature_54` and `feature_56`.\n- Top 5 values in each feature has contributed more than 80% of each feature, except for `feature_54`.\n\n\n","117d63e6":"### 4.2.2 Test dataset\nTrain and test dataset have a same observations, below are the repeating observations.\n\n**Observations:**\n- Zero is dominating the occurance with more than 50% of each feature, except for `feature_12`, `feature_18`, `feature_19`, `feature_39`, `feature_43`. `feature_50`, `feature_54` and `feature_56`.\n- Top 5 values in each feature has contributed more than 80% of each feature, except for `feature_19`, `feature_43` and `feature_54`.\n\n#### 4.2.2.1 feature_0 - feature_24\n`feature_0` - `feature_24` have a same observations as train dataset, below are the repeating observations.\n\n**Observations:**\n- Zero is dominating the occurance with more than 50% of each feature, except for `feature_12`, `feature_18`, `feature_19` and `feature_20`.\n- Top 5 values in each feature has contributed more than 80% of each feature, except for `feature_19`.\n\n\n","701c1a72":"The dimension and number of missing values in the train dataset is as below:","cdee9567":"### 4.1.5 Zero & Positive Individual Values\nIn general, a row can be divided into two categories: zero and positive numbers and calculated the number of occurance in each row and see the distribution for the whole dataset. \n\n**Observations:**\n- Zero occured more than positive numbers in every rows and have an opposite distribution, both in train and test dataset. \n- There are differences on how many zero and positive numbers occured in train and test dataset. \n\n","d4256249":"[back to top](#table-of-contents)\n<a id=\"3.3\"><\/a>\n## 3.3 Submission\nPlease be caution on the prediction submission format! It\u2019s not same as the target column in train dataset that predict each clases. The submission file is expected to have a prediction probability that the product belongs on each of the classes seen in the dataset.\n\nBelow is the first 5 rows of submission file:","ecc6f887":"### 4.1.4 Individual Values\n\nTaking top 10 of an individual value mean that occurred in a row. Example: calculate how many times the number zero is occured in a row then averaging this number occurance with another sample (row) in the dataset. The purporse is to get sense how each unique values occured in each sample (row).\n\n**Observations:**\n- Number zero is dominating the occurrance in every rows on train and test datasets, on average this number occurred 48.5 times from 75 features on train dataset and test dataset.\n- In general, the mean occurrance of each value are in descending order where the highest occurance is started from 0.\n- The top 10 figures indicated that train and test dataset have a same mean occurance starting from number 0 and ended with number 9.","ab24dc8f":"#### 3.1.3.2 Target column\nThere will be `9 classes` that will be predicted in this competition. Most frequent class in the target column is `Class_6` with a frequency of `51,811`, that contribute more than `25%` of the train dataset.","c1aecc5c":"[back to top](#table-of-contents)\n<a id=\"3.2\"><\/a>\n## 3.2 Test dataset\nTest dataset is used to make a prediction based on the model that has previously trained. Exploration in this dataset is also needed to see how the data is structured and especially on it\u2019s similiarity with the train dataset.\n\n**Observations:**\n- Observations in the `train` dataset are still consistent in the `test` dataset.\n\n### 3.2.1 Quick view\nBelow is the first 5 rows of test dataset:","40c9b626":"[back to top](#table-of-contents)\n<a id=\"1\"><\/a>\n# 1 Introduction\n\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, Kaggle have launched many Playground competitions that are more approachable than Featured competition, and thus more beginner-friendly.\n\nThe goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\n\nSubmissions are evaluated using multi-class logarithmic loss. Each row in the dataset has been labeled with one true Class. For each row, you must submit the predicted probabilities that the product belongs to each class label. The formula is:\n\n$$ \\text{log loss} = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^My_{ij}\\log(p_{ij}), $$\n\nwhere $N$ is the number of rows in the test set, $M$ is the number of class labels, $\\text{log}$ is the natural logarithm, $y_{ij}$ is 1 if observation $i$ is in class $j$ and 0 otherwise, and $p_{ij}$ is the predicted probability that observation $i$ belongs to class $j$.\n\nThe submitted probabilities for a given product are not required to sum to one; they are rescaled prior to being scored, each row is divided by the row sum. In order to avoid the extremes of the $\\text{log}$ function, predicted probabilities are replaced with $max(min(p,1-10^{-15}),10^{-15})$.","44590d3c":"#### 4.2.1.2 feature_25 - feature_49\n**Observations:**\n- Zero is dominating the occurance with more than 50% of each feature, except for `feature_31`, `feature_39` and `feature_43`.\n- Top 5 values in each feature has contributed more than 80% of each feature, except for `feature_43`."}}