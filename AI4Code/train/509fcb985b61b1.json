{"cell_type":{"7fff370f":"code","50bdfc3e":"code","2ab772dd":"code","2dbad88e":"code","c67f694e":"code","f00b602d":"code","ce673b23":"code","170842b5":"code","852c0a99":"code","e58520a4":"code","0288ca12":"code","6397fc9f":"code","11b20a76":"code","c1b01c23":"code","b62fec90":"code","a1e4085c":"code","e84652ff":"code","2647975c":"code","09818464":"code","a62b751b":"code","027c8261":"code","dc297791":"code","91b2c88f":"code","3b3188c0":"code","cf63ee32":"code","6ae381b2":"code","f29387df":"code","dbc7e6b1":"code","90b7f0ca":"code","25a3a15e":"code","dd5d1a7c":"code","f3c17565":"code","85353554":"code","25524331":"code","5ac270d9":"code","4545f668":"code","4d6bccf8":"code","c0a2090a":"code","dc82ee48":"code","aba9b9c2":"code","af09ca6d":"code","28717e1f":"code","72396d28":"code","2391f2bb":"code","91eaae34":"code","7a4a491e":"code","e72788f9":"code","46ef01eb":"code","927879d7":"code","03840701":"code","10356dc6":"code","e0e35085":"code","2ec042b3":"code","e49b2478":"code","9d566437":"code","8e4b6697":"code","84f68a42":"markdown","5503a29d":"markdown","0d6f77d1":"markdown","5eab6229":"markdown","0a112128":"markdown","aa0d24c6":"markdown"},"source":{"7fff370f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nimport datetime as dt\nimport calendar,warnings,itertools,matplotlib,keras,shutil\nimport tensorflow as tf\nimport statsmodels.api as sm\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict\nfrom sklearn import svm,metrics,tree,preprocessing,linear_model\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Ridge,LinearRegression,LogisticRegression,ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier, GradientBoostingRegressor,BaggingClassifier,ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score,mean_squared_error,recall_score,confusion_matrix,f1_score,roc_curve, auc\nfrom sklearn.datasets import load_iris,make_regression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.kernel_ridge import KernelRidge\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom IPython.core import display as ICD\nwarnings.filterwarnings('ignore') ","50bdfc3e":"#Importing Dataset using pandas\ndataset=pd.read_csv(\"..\/input\/dataco-smart-supply-chain-for-big-data-analysis\/DataCoSupplyChainDataset.csv\",header= 0,encoding= 'unicode_escape')\ndataset.head(5)","2ab772dd":"dataset.shape","2dbad88e":"#Dropping Columns that are irrelevant and repetitive\n\n## Concatenating first and last name so that one could be dropped\ndataset['Customer Full Name'] = dataset['Customer Fname'].astype(str)+dataset['Customer Lname'].astype(str) \n\ndata=dataset.drop(['Customer Email','Product Status','Customer Password','Customer Street','Customer Fname','Customer Lname',\n           'Latitude','Longitude','Product Description','Product Image','Order Zipcode','shipping date (DateOrders)','Customer Zipcode', 'order date (DateOrders)','Delivery Status' ],axis=1)","c67f694e":"data.apply(lambda x: sum(x.isnull())) #Checking missing values","f00b602d":"train_data=data.copy()","ce673b23":"fig, ax = plt.subplots(figsize=(24,12)) # figsize\nsns.heatmap(data.corr(),annot=True,linewidths=.5,fmt='.1g',cmap= 'coolwarm') # Heatmap for correlation matrix","170842b5":"data.plot(x='Product Price', y='Sales per customer',linestyle='dotted',\n     markerfacecolor='blue', markersize=12) \nplt.title('Product Price vs Sales per customer')#title\nplt.xlabel('Product Price')  # X-axis title\nplt.ylabel('Sales per customer') # Y=axis title\nplt.show()","852c0a99":"# Creating a new column with binary classification of fraud status\ntrain_data['fraud'] = np.where(train_data['Order Status'] == 'SUSPECTED_FRAUD', 1, 0)\ntrain_data.drop(['Order Status'], axis=1, inplace=True)","e58520a4":"# Checking data tyoes so that relevant encosing couldcould be done \n\ntrain_data.dtypes","0288ca12":"# Using Label encoder all the column with datatype as object is been transformed\n\nle = preprocessing.LabelEncoder()\ntrain_data['Customer Country']  = le.fit_transform(train_data['Customer Country'])\ntrain_data['Market']            = le.fit_transform(train_data['Market'])\ntrain_data['Type']              = le.fit_transform(train_data['Type'])\ntrain_data['Product Name']      = le.fit_transform(train_data['Product Name'])\ntrain_data['Customer Segment']  = le.fit_transform(train_data['Customer Segment'])\ntrain_data['Customer State']    = le.fit_transform(train_data['Customer State'])\ntrain_data['Order Region']      = le.fit_transform(train_data['Order Region'])\ntrain_data['Order City']        = le.fit_transform(train_data['Order City'])\ntrain_data['Category Name']     = le.fit_transform(train_data['Category Name'])\ntrain_data['Customer City']     = le.fit_transform(train_data['Customer City'])\ntrain_data['Department Name']   = le.fit_transform(train_data['Department Name'])\ntrain_data['Order State']       = le.fit_transform(train_data['Order State'])\ntrain_data['Shipping Mode']     = le.fit_transform(train_data['Shipping Mode'])\ntrain_data['Order Country']     = le.fit_transform(train_data['Order Country'])\ntrain_data['Customer Full Name']= le.fit_transform(train_data['Customer Full Name'])","6397fc9f":"def classifiermodel(model_f,model_l,xf_train, xf_test,yf_train,yf_test,xl_train, xl_test,yl_train,yl_test):\n    model_f=model_f.fit(xf_train,yf_train) # Fitting train data for fraud detection\n    model_l=model_l.fit(xl_train,yl_train) # Fitting train data for predection of late delivery\n    yf_pred=model_f.predict(xf_test)\n    yl_pred=model_l.predict(xl_test)  \n    accuracy_f=accuracy_score(yf_pred, yf_test) #Accuracy for fraud detection\n    accuracy_l=accuracy_score(yl_pred, yl_test) #Accuracy for predection of late delivery\n    recall_f=recall_score(yf_pred, yf_test) #Recall score for  fraud detection\n    recall_l=recall_score(yl_pred, yl_test)# Recall score for predection of late delivery\n    conf_f=confusion_matrix(yf_test, yf_pred)# fraud detection\n    conf_l=confusion_matrix(yl_test, yl_pred)#predection of late delivery\n    f1_f=f1_score(yf_test, yf_pred)#fraud detection\n    f1_l=f1_score(yl_test, yl_pred)#predection of late delivery\n    print('Model paramters used are :',model_f)\n    print('Accuracy of fraud status is        :', (accuracy_f)*100,'%')\n    print('Recall score of fraud status is        :', (recall_f)*100,'%')\n    print('Conf Matrix of fraud status is        :\\n',  (conf_f))\n    print('F1 score of fraud status is        :', (f1_f)*100,'%')\n    print('Accuracy of late delivery status is:', (accuracy_l)*100,'%')\n    print('Recall score of late delivery status is:', (recall_l)*100,'%')\n    print('Conf Matrix of late delivery status is: \\n',(conf_l))\n    print('F1 score of late delivery status is:', (f1_l)*100,'%')","11b20a76":"keras.layers.BatchNormalization()\nclassifier = Sequential()\n#First Hidden Layer\nclassifier.add(Dense(1024, activation='relu',kernel_initializer='random_normal', input_dim=38)) #Since we have 44 columns\nclassifier.add(Dropout(0.3))\n#Third Hidden Layer\nclassifier.add(Dense(512, activation='relu',kernel_initializer='random_normal'))\nclassifier.add(Dropout(0.3))\n#Fourth Hidden Layer\nclassifier.add(Dense(256, activation='relu',kernel_initializer='random_normal'))\nclassifier.add(Dropout(0.3))\n#Fifth Hidden Layer\nclassifier.add(Dense(128, activation='relu',kernel_initializer='random_normal'))\nclassifier.add(Dropout(0.3))\n#Output Layer\nclassifier.add(Dense(1, activation='sigmoid',kernel_initializer='random_normal'))\n","c1b01c23":"classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","b62fec90":"#All columns expect fraud\nxf=train_data.loc[:, train_data.columns != 'fraud']\n#Only fraud column\nyf=train_data['fraud']\n#Splitting the data into two parts in which 80% data will be used for training the model and 20% for testing\nxf_train, xf_test,yf_train,yf_test = train_test_split(xf,yf,test_size = 0.2,random_state = 42)","a1e4085c":"#Preparing vaidation set for finding the optimun number of epochs required.\n\nxf_val = xf[:28000] # reserving the first 10,000 reviews for validation\nxf_train_small = xf[28000:] # reserving the remaining 15,000 reviews for training\nyf_val = yf[:28000]\nyf_train_small = yf[28000:]\nhistory = classifier.fit(xf_train_small, \n                    yf_train_small, \n                    epochs=20, \n                    batch_size=216,\n                   validation_data=(xf_val, yf_val))","e84652ff":"history_dict = classifier.history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","2647975c":"classifier.fit(xf_train,yf_train, batch_size=216, epochs=3)","09818464":"from tensorflow.keras.utils import plot_model\nplot_model(classifier, to_file='Case1.png', show_shapes=True)","a62b751b":"train_evaluate=classifier.evaluate(xf_train, yf_train)\ntest_evaluate=classifier.evaluate(xf_test, yf_test)\nprint('accuracy for Train set is',train_evaluate)\nprint('accuracy for Test set is',test_evaluate)# evaluation of model.\nyf_pred1=classifier.predict(xf_test,batch_size=512,verbose=1)\nyf_pred=np.argmax(yf_pred1,axis=1)\nprint(f1_score(yf_test,yf_pred,average=\"weighted\"))","027c8261":"xl=train_data.loc[:, train_data.columns != 'Late_delivery_risk']\n#Only fraud column\nyl=train_data['Late_delivery_risk']\n#Splitting the data into two parts in which 80% data will be used for training the model and 20% for testing\nxl_train, xl_test,yl_train,yl_test = train_test_split(xl,yl,test_size = 0.1, random_state = 42)","dc297791":"sc = StandardScaler()\nxl_train=sc.fit_transform(xl_train)\nxl_test=sc.transform(xl_test)","91b2c88f":"xl_val = xl[:28000] # reserving the first 10,000 reviews for validation\nxl_train_small = xl[28000:] # reserving the remaining 15,000 reviews for training\nyl_val = yl[:28000]\nyl_train_small = yl[28000:]\nhistory = classifier.fit(xl_train_small, \n                    yl_train_small, \n                    epochs=20, \n                    batch_size=216,\n                   validation_data=(xl_val, yl_val))","3b3188c0":"history_dict = classifier.history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","cf63ee32":"classifier.fit(xl_train,yl_train, batch_size=216, epochs=5)","6ae381b2":"train_evaluate=classifier.evaluate(xl_train, yl_train)\ntest_evaluate=classifier.evaluate(xl_test, yl_test)\nprint('accuracy for Train set is',train_evaluate)\nprint('accuracy for Test set is',test_evaluate)# evaluation of model.\nyl_pred1=classifier.predict(xl_test,batch_size=216,verbose=1)\nyl_pred=np.argmax(yl_pred1,axis=1)\nprint(f1_score(yl_test,yl_pred,average=\"weighted\"))","f29387df":"xs=train_data.loc[:, train_data.columns != 'Sales']\nys=train_data['Sales']\nxs_train, xs_test,ys_train,ys_test = train_test_split(xs,ys,test_size = 0.3, random_state = 42)","dbc7e6b1":"scaler=MinMaxScaler()\nxs_train=scaler.fit_transform(xs_train)\nxs_test=scaler.transform(xs_test)","90b7f0ca":"def regressionmodel(model_s,model_q,xs_train, xs_test,ys_train,ys_test,xq_train, xq_test,yq_train,yq_test):\n    model_s=model_s.fit(xs_train,ys_train)#Fitting train data for sales\n    model_q=model_q.fit(xq_train,yq_train)#Fitting train data for order quantity\n    ys_pred=model_s.predict(xs_test)#predicting sales with test data\n    yq_pred=model_q.predict(xq_test)#predicting order quantity with test data\n    print('Model parameter used are:',model_s) #Printing the model to see which parameters are used\n    #Printing mean absolute error for predicting sales\n    print(\"MAE of sales is         :\", metrics.mean_absolute_error(ys_test,ys_pred))\n    #Printing Root mean squared error for predicting sales\n    print(\"RMSE of sales is        :\",np.sqrt(metrics.mean_squared_error(ys_test,ys_pred)))\n    #Printing mean absolute error for predicting order quantity\n    print(\"MAE of order quantity   :\", metrics.mean_absolute_error(yq_test,yq_pred))\n    #Printing Root mean squared error for predicting order quantity\n    print(\"RMSE of order quantity  :\",np.sqrt(metrics.mean_squared_error(yq_test,yq_pred)))","25a3a15e":"regressor = Sequential()\n\n#First Hidden Layer\nregressor.add(Dense(512, activation='relu',kernel_initializer='normal',input_dim=38))\nregressor.add(Dropout(0.3))\n#Second  Hidden Layer\nregressor.add(Dense(256, activation='relu',kernel_initializer='normal'))\nregressor.add(Dropout(0.3))\n#Third  Hidden Layer\nregressor.add(Dense(256, activation='relu',kernel_initializer='normal'))\nregressor.add(Dropout(0.3))\n#Fourth  Hidden Layer\nregressor.add(Dense(256, activation='relu',kernel_initializer='normal'))\nregressor.add(Dropout(0.3))\n#Fifth  Hidden Layer\nregressor.add(Dense(256, activation='relu',kernel_initializer='normal'))\n#Output Layer\nregressor.add(Dense(1, activation='linear'))# Linear activation is used.","dd5d1a7c":"X = array(X).reshape(20, 1, 1)","f3c17565":"#Build the LSTM network model\nmodel = Sequential()\nmodel.add(LSTM(units=512, dropout=0.5, recurrent_dropout=0.5,return_sequences=True, activation='relu',kernel_initializer='random_normal', input_shape=(38, 1)))\nmodel.add(Dense(units=1,activation='linear'))\n","85353554":"xs_val = xs[:28000] # reserving the first 10,000 reviews for validation\nxs_train_small = xs[28000:] # reserving the remaining 15,000 reviews for training\nys_val = ys[:28000]\nys_train_small = ys[28000:]\nhistory = classifier.fit(xs_train_small, \n                    ys_train_small, \n                    epochs=30, \n                    batch_size=216,\n                   validation_data=(xs_val, ys_val))","25524331":"history_dict = classifier.history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","5ac270d9":"regressor.compile(optimizer='rmsprop',loss='mean_absolute_error',metrics=['mean_absolute_error'])\nregressor.fit(xs_train,ys_train, batch_size=216, epochs=30)","4545f668":"print(xs_train.shape)\nprint(xs_test.shape)\n","4d6bccf8":"model.compile(optimizer='rmsprop',loss='mean_absolute_error',metrics=['mean_absolute_error'])\nmodel.fit(xs_train,ys_train, batch_size=216, epochs=5)","c0a2090a":"pred_train_s= regressor.predict(xs_train)\npred_s_test= regressor.predict(xs_test)\nprint('MAE Value train data:',regressor.evaluate(xs_train,ys_train))\nprint('RMSE of train data:',np.sqrt(mean_squared_error(ys_train,pred_train_s)))\nprint('MAE Value test data:',regressor.evaluate(xs_test,ys_test))\nprint('RMSE of test data:',np.sqrt(mean_squared_error(ys_test,pred_s_test)))","dc82ee48":"from keras.layers import LSTM","aba9b9c2":"xt=train_data.loc[:, train_data.columns != 'Sales']\nyt=train_data['Sales']\nxt_train, xt_test,yt_train,yt_test = train_test_split(xt,yt,test_size = 0.3, random_state = 42)","af09ca6d":"scaler=MinMaxScaler()\nxt_train=scaler.fit_transform(xt_train)\nxt_test=scaler.transform(xt_test)","28717e1f":"ytt = train_data['Sales'].values.reshape(-1, 1) #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nytt_scaled = min_max_scaler.fit_transform(ytt)\n","72396d28":"xt_train, xt_test,yt_train,yt_test = train_test_split(xt,ytt_scaled,test_size = 0.3, random_state = 42)","2391f2bb":"yt_test","91eaae34":"# reshape input to be [samples, time steps, features] which is required for LSTM\nxt_train1 =xt_train.reshape(xt_train.shape[0],xt_train.shape[1] , 1)\nxt_test1 = xt_test.reshape(xt_test.shape[0],xt_test.shape[1] , 1)","7a4a491e":"# reshape input to be [samples, time steps, features] which is required for LSTM\nyt_train1 =yt_train.reshape(yt_train.shape[0],yt_train.shape[1] , 1)\nyt_test1 = yt_test.reshape(yt_test.shape[0],yt_test.shape[1] , 1)","e72788f9":"#Build the LSTM network model\nmodel = Sequential()\nmodel.add(LSTM(units=512, dropout=0.5, recurrent_dropout=0.5,return_sequences=True, activation='relu',kernel_initializer='random_normal', input_shape=(38, 1)))\nmodel.add(Dense(units=1,activation='softmax'))","46ef01eb":"model.summary()","927879d7":"model.compile(optimizer='adam',loss='mean_squared_error',metrics=['mean_absolute_error'])","03840701":"model.fit(xt_train,yt_train, batch_size=216, epochs=5)","10356dc6":"xt_train.shape[0]","e0e35085":"train_predict=model.predict(xt_train1)\ntest_predict=model.predict(xt_test1)","2ec042b3":"modeva= model.evaluate(xt_train1,yt_train)","e49b2478":"print('MAE Value train data:',modeva)","9d566437":"modevatest=model.evaluate(xt_test1,yt_test)","8e4b6697":"print('MAE Value test data:',modevatest)","84f68a42":"# LSTM","5503a29d":"# Casestudy 2: Late Delivery Classification ","0d6f77d1":"# Heatmap for correlation matrix","5eab6229":"keras.layers.BatchNormalization()\nclassifier_l= Sequential()\n#First Hidden Layer\nclassifier_l.add(Dense(1024, activation='relu',kernel_initializer='random_normal', input_dim=38)) #Since we have 44 columns\nclassifier_l.add(Dropout(0.3))\n#Third Hidden Layer\nclassifier_l.add(Dense(512, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Fourth Hidden Layer\nclassifier_l.add(Dense(256, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Fifth Hidden Layer\nclassifier_l.add(Dense(128, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Sixth Hidden Layer\nclassifier_l.add(Dense(64, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Seventh Hidden Layer\nclassifier_l.add(Dense(32, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Eight Hidden Layer\nclassifier_l.add(Dense(16, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Ninth Hidden Layer\nclassifier_l.add(Dense(8, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Tenth Hidden Layer\nclassifier_l.add(Dense(4, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Eleventh Hidden Layer\nclassifier_l.add(Dense(2, activation='relu',kernel_initializer='random_normal'))\nclassifier_l.add(Dropout(0.3))\n#Output Layer\nclassifier_l.add(Dense(1, activation='sigmoid',kernel_initializer='random_normal'))","0a112128":"# Casestudy 3: Regression on Sales Data","aa0d24c6":"# Casestudy 1: Fraud Order Classification "}}