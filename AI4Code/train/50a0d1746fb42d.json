{"cell_type":{"eb204a6f":"code","f8275048":"code","c3ce1858":"code","41163bbc":"code","5749cb88":"code","76a64932":"code","0194df72":"code","946e3dcd":"code","b66bd87d":"code","1bafd448":"code","5b77aead":"code","94578fff":"code","51b7aa4b":"code","48d45fc2":"code","ec53b680":"code","bf2ac7c6":"code","13535f90":"code","0d7b6469":"code","6a73937f":"code","79b5e235":"code","54ded2cd":"code","fb54410e":"code","7018759d":"code","0843abf4":"code","7575d9d2":"code","69679c5c":"code","6c18d967":"code","bbcd0d52":"code","57be97c3":"markdown","7ef5ca77":"markdown","caa08807":"markdown","1ae73fb4":"markdown","ab24dacf":"markdown","2a512354":"markdown","9e1eacb1":"markdown","7f525620":"markdown","7ebf33e8":"markdown","b841c8f7":"markdown"},"source":{"eb204a6f":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport timm","f8275048":"# model = timm.create_model('eca_nfnet_l0', pretrained=True)\n","c3ce1858":"# !pip install torchsummary \n# from torchsummary import summary\n# summary(model,(3, 512,512))","41163bbc":"import os\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport timm\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors","5749cb88":"class CFG:\n    seed = 54\n    classes = 11014 \n    scale = 30 \n    margin = 0.5\n    model_name =  'tf_efficientnet_b3'\n    fc_dim = 512\n    img_size = 512\n    batch_size = 20\n    num_workers = 4\n    device = device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model_path = '..\/input\/shopee-correct-label-train\/arcface_512x512_tf_efficientnet_b3_14_epoch_correct_label.pt'\n#     model_path = ['..\/input\/shopee-match-ensemble-eff-b1-b3-focal-loss\/B1arcface_512x512_tf_efficientnet_b1_nofold_14_epo.pt',\n#                   '..\/input\/shopee-training-eff-b4\/arcface_512x512_tf_efficientnet_b2_14_epoch.pt',\n#                   '..\/input\/shopee-match-ensemble-eff-b1-b3-focal-loss\/arcface_512x512_tf_efficientnet_b3_14_epoch.pt']\n#     model_path = ['..\/input\/eff-b4-tfidf\/arcface_512x512_tf_efficientnet_b4_0_fold_15_epoch.pt',\n#                  '..\/input\/eff-b4-tfidf\/arcface_512x512_tf_efficientnet_b4_1_fold_15_epoch.pt',\n#                  '..\/input\/eff-b4-tfidf\/arcface_512x512_tf_efficientnet_b4_2_fold_15_epoch.pt']","76a64932":"def read_dataset():\n    df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    df_cu = cudf.DataFrame(df)\n    image_paths = '..\/input\/shopee-product-matching\/test_images\/' + df['image']\n        \n    return df, df_cu, image_paths\n \n     ","0194df72":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","946e3dcd":"# df = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n# df_cu = cudf.DataFrame(df)\n# image_paths = '..\/input\/shopee-product-matching\/train_images\/' + df['image']","b66bd87d":"# tmp = df.groupby('image_phash').posting_id.agg('unique').to_dict()\n# df['oof'] = df.image_phash.map(tmp)\n# df.head()","1bafd448":"# tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n# df['target'] = df.label_group.map(tmp)\n# df.head()","5b77aead":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n \/ (len(row.target)+len(row[col]))\n    return f1score\n\n# df['f1'] = df.apply(getMetric('oof'),axis=1)\n# print('CV score =',df.f1.mean())","94578fff":"# Create Model\n\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n    \n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.out_features\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output, nn.CrossEntropyLoss()(output,label)\n\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = True,\n        arc_mode = 0,\n        mode = False):\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.training = mode\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.1)\n            self.classifier = nn.Linear(in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        features = self.extract_features(image)\n        if self.training:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n\n    def extract_features(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc and self.training:\n            x = self.dropout(x)\n            x = self.classifier(x)\n            x = self.bn(x)\n        return x","51b7aa4b":"def get_image_neighbors(df, embeddings, KNN=50):\n\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n#     threshold = 0.3\n    threshold = 6.4\n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions","48d45fc2":"def get_test_transforms():\n    return albumentations.Compose([\n        albumentations.Resize(CFG.img_size, CFG.img_size, always_apply=True),\n        albumentations.Normalize(),\n        ToTensorV2(p=1.0)\n    ])","ec53b680":"class ShopeeDataset(Dataset):\n\n    def __init__(self, image_paths, transforms=None):\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']\n        \n        return image, torch.tensor(1)","bf2ac7c6":"def get_image_embeddings(image_paths):\n\n    model = ShopeeModel(pretrained=False).to(CFG.device)\n    model.load_state_dict(torch.load(CFG.model_path))\n    model.eval()\n    # Choose one of fold model to load weigths\n#     if fold == 1:\n#         checkpoint = torch.load(CFG.model_path[0])\n#         checkpoint1 = {k[7:]:v for k, v in checkpoint.items()}\n#         model.load_state_dict(checkpoint1)\n#         model.eval()\n#     elif fold == 2:\n#         checkpoint = torch.load(CFG.model_path[1])\n# #         checkpoint1 = {k[7:]:v for k, v in checkpoint.items()}\n#         model.load_state_dict(checkpoint)\n#         model.eval()\n#     elif fold == 3:\n#         checkpoint = torch.load(CFG.model_path[2])\n# #         checkpoint1 = {k[7:]:v for k, v in checkpoint.items()}\n#         model.load_state_dict(checkpoint)\n#         model.eval()\n#     elif fold == 3:\n#         model.load_state_dict(torch.load(CFG.model_path[fold]))\n#         model.eval()\n#      elif fold == 4:\n#         model.load_state_dict(torch.load(CFG.model_path[fold]))\n#         model.eval()\n#     else:\n#         model.load_state_dict(torch.load(CFG.model_path[fold]))\n#         model.eval()\n        \n    image_dataset = ShopeeDataset(image_paths=image_paths, transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        num_workers=CFG.num_workers\n    )\n\n    embeds = []\n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            features = model(img,label)\n            image_embeddings = features.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","13535f90":"df,df_cu,image_paths = read_dataset()","0d7b6469":"# Get neighbors for image_embeddings\n# image_embeddings1 = get_image_embeddings(image_paths.values, fold=1, arc_mode=0)\n# image_embeddings2 = get_image_embeddings(image_paths.values, fold=2, arc_mode=1)\n# image_embeddings3 = get_image_embeddings(image_paths.values, fold=3, arc_mode=2)\n# image_embeddings = np.concatenate([image_embeddings1, image_embeddings2, image_embeddings3],axis=1)\nimage_embeddings = get_image_embeddings(image_paths.values)\n\n# image_embeddings = get_image_embeddings(image_paths.values)\ndf, image_predictions = get_image_neighbors(df, image_embeddings, KNN=50 if len(df)>3 else 3)\ndf.head()","6a73937f":"# df,df_cu,image_paths = read_dataset()\n\n# tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n# df['target'] = df.label_group.map(tmp)\n# df.head()","79b5e235":"# df, image_predictions = get_image_neighbors(df, image_embeddings, KNN=50 if len(df)>3 else 3)\n# df['Image_predictions'] = image_predictions\n# df['F1'] = df.apply(getMetric('Image_predictions'),axis=1)\n\n# print('F1 Score:{}'.format(df.F1.mean()))\n# df.head()","54ded2cd":"# def format_convert(match):\n#     image_predictions = []\n#     for i in match:\n#         image_predictions.append( ' '.join((i)))\n    \n#     return image_predictions\n# #         print(image_predictions)","fb54410e":"# df, image_predictions = get_image_neighbors(df, image_embeddings, KNN=50 if len(df)>3 else 3)\n# image_predictions = format_convert(image_predictions)\n\n# df['matches'] = image_predictions\n# df.head()","7018759d":"def get_text_predictions(df, max_features=25_000):\n    \n    model = TfidfVectorizer(stop_words='english',\n                            binary=True,\n                            max_features=max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n\n    print('Finding similar titles...')\n    CHUNK = 1024 * 4\n    CTS = len(df) \/\/ CHUNK\n    if (len(df)%CHUNK) != 0:\n        CTS += 1\n\n    preds = []\n    for j in range( CTS ):\n        a = j * CHUNK\n        b = (j+1) * CHUNK\n        b = min(b, len(df))\n        print('chunk', a, 'to', b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.75)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n\n    del model,text_embeddings\n    gc.collect()\n    return preds","0843abf4":"text_predictions = get_text_predictions(df, max_features=25_000)","7575d9d2":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join( np.unique(x) )","69679c5c":"# df[['posting_id', 'matches']].to_csv('submission.csv', index=False)\n# a = pd.read_csv('.\/submission.csv')\n# a.head()","6c18d967":"df['image_predictions'] = image_predictions\ndf['text_predictions'] = text_predictions\ndf['matches'] = df.apply(combine_predictions, axis=1)\ndf[['posting_id', 'matches']].to_csv('submission.csv', index=False)","bbcd0d52":"# def combine_predictions(row):\n#     x = np.concatenate([row['image_predictions'], row['text_predictions']])\n#     return ' '.join( np.unique(x) )","57be97c3":"# Calcaluate the cv score of training dataset","7ef5ca77":"# Preparing Submissions","caa08807":"# Image Predictions","1ae73fb4":"# F1 score","ab24dacf":"# Verify the CV Score of image_hase","2a512354":"# Config","9e1eacb1":"# Utils","7f525620":"# Import Packages","7ebf33e8":"# Calculate the test set","b841c8f7":"# Text Predictions"}}