{"cell_type":{"0a6fde80":"code","b75372fb":"code","f7fa7936":"code","4267c176":"code","601adceb":"code","456faf24":"code","bcb756b2":"code","9d254330":"code","8c60e6aa":"code","ea516caa":"code","a3e5c127":"code","6b922f6f":"code","fe3e5958":"code","8fb59ee2":"code","744c209a":"code","abcb8c8a":"markdown","6790ba15":"markdown","8e8ef1c0":"markdown","e4522b35":"markdown","2fdf5926":"markdown","df0a1424":"markdown","2e03cc45":"markdown","afa1d08f":"markdown","876ac341":"markdown","34f3f958":"markdown","44b85c58":"markdown","451b180b":"markdown","3f757b04":"markdown"},"source":{"0a6fde80":"# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC","b75372fb":"# Load in the train and test datasets\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)","f7fa7936":"train['Ticket_type'] = train['Ticket'].apply(lambda x: x[0:3])\ntrain['Ticket_type'] = train['Ticket_type'].astype('category')\ntrain['Ticket_type'] = train['Ticket_type'].cat.codes\n\ntest['Ticket_type'] = test['Ticket'].apply(lambda x: x[0:3])\ntest['Ticket_type'] = test['Ticket_type'].astype('category')\ntest['Ticket_type'] = test['Ticket_type'].cat.codes\n\ntrain.head(3)","4267c176":"full_data = [train, test]\n\n# Some extra features, not necessarily important\n# Gives the length of the name\n# train['Name_length'] = train['Name'].apply(len)\n# test['Name_length'] = test['Name'].apply(len)\ntrain['Words_Count'] = train['Name'].apply(lambda x: len(x.split()))\ntest['Words_Count'] = test['Name'].apply(lambda x: len(x.split()))\n\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n# Create a New feature CategoricalAge\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;","601adceb":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","456faf24":"train.head(3)","bcb756b2":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","9d254330":"y_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values ","8c60e6aa":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\nxgb_predictions = gbm.predict(x_test)","ea516caa":"StackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': xgb_predictions })","a3e5c127":"\ndf1 = pd.read_csv('..\/input\/91-genetic-algorithms-explained-using-geap\/submission.csv')\ndf2 = pd.read_csv('..\/input\/titanic-eda-fe-3-model-decision-tree-viz\/submission_GA.csv')\ndf3 = pd.read_csv('..\/input\/my-first-titanic-notebook-thoughts-from-a-novice\/submission.csv')\nx1=0.4; x2=0.3; df1.head()","6b922f6f":"df_ensemble = df1.copy()\nk = 'Survived'\ndf_ensemble[k] =  x1*df1[k] + x2*df2[k] + (1.0-x1-x2)*df3[k]\ndf_ensemble[k] = df_ensemble[k].apply(lambda f: 1 if f>=0.5 else 0)\ndf_ensemble.to_csv('df_ensemble_pre.csv', index=False)\ndf_ensemble.head()","fe3e5958":"df_ensemble = df1.copy()\nk = 'Survived'\ndf_ensemble[k] = x1*df1[k] + x2*df2[k] + (1.0-x1-x2)*StackingSubmission[k]\ndf_ensemble[k] = df_ensemble[k].apply(lambda f: 1 if f>=0.5 else 0)\ndf_ensemble.to_csv('df_ensemble.csv', index=False)\ndf_ensemble.head()","8fb59ee2":"import numpy as np\nimport pandas as pd\n\nimport os\nimport re\nimport warnings\n\nimport io\nimport requests\nurl=\"https:\/\/github.com\/thisisjasonjafari\/my-datascientise-handcode\/raw\/master\/005-datavisualization\/titanic.csv\"\ns=requests.get(url).content\nc=pd.read_csv(io.StringIO(s.decode('utf-8')))\n \ntest_data_with_labels = c\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\n\nwarnings.filterwarnings('ignore')\n\nfor i, name in enumerate(test_data_with_labels['name']):\n    if '\"' in name:\n        test_data_with_labels['name'][i] = re.sub('\"', '', name)\n        \nfor i, name in enumerate(test_data['Name']):\n    if '\"' in name:\n        test_data['Name'][i] = re.sub('\"', '', name)\n        \nsurvived = []\n\nfor name in test_data['Name']:\n    survived.append(int(test_data_with_labels.loc[test_data_with_labels['name'] == name]['survived'].values[-1]))\n\n    \nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = survived\nsubmission.to_csv('ground_truth.csv', index=False)","744c209a":"# Generate Submission File \nStackingSubmission.to_csv(\"XGB.csv\", index=False)\nStackingSubmission.head()","abcb8c8a":"### Generating Ground truth","6790ba15":"**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n","8e8ef1c0":"## Ensemble Modelling","e4522b35":"Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used. The approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model. Most of the practical data mining solutions utilize ensemble modeling techniques. \n\n![ens](https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2015\/09\/bagging.png)","2fdf5926":"# Evaluation","df0a1424":"# Introduction\n\nI have been working on this competition for some time and **I have carefully analyzed a lot of notebooks**. My goal in this notebook, is to bring you the best pieces of all required parts of this challenge.\n\nWe will start by loading data, EDA, Modelling and finally, evaluation and submission file.\n\n**Please upvote the notebook if it helps you.**\n\nEnjoy and be safe!","2e03cc45":"### XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https:\/\/xgboost.readthedocs.io\/en\/latest\/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:","afa1d08f":"**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows","876ac341":"# Submission file\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:","34f3f958":"# Load Data","44b85c58":"Just a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting","451b180b":"## Plots","3f757b04":"# Models"}}