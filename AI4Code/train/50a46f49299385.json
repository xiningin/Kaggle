{"cell_type":{"37c5d57e":"code","b0727923":"code","3548dc75":"code","5f7e5b7b":"code","c4d22e70":"code","12a66360":"code","dbba3b39":"markdown","8f7ae6fc":"markdown","2c01b327":"markdown","0f593f8d":"markdown","005f1f10":"markdown","3ac125ce":"markdown","a2e65672":"markdown","91a2981e":"markdown","d42edf39":"markdown","cf5f54cb":"markdown","168ac8d6":"markdown","ec344cf6":"markdown"},"source":{"37c5d57e":"import requests\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom numpy import array\nfrom numpy import argmax\nimport re\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder \nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import mean_absolute_error\n\ndataset = pd.read_csv(\"..\/input\/malicious-and-benign-websites\/dataset.csv\")\ndataset.dropna(inplace=True)\n\n","b0727923":"\nprint(dataset.shape)\nstring_dataset = dataset.drop(['URL','URL_LENGTH', 'NUMBER_SPECIAL_CHARACTERS', 'CONTENT_LENGTH',\n       'TCP_CONVERSATION_EXCHANGE', 'DIST_REMOTE_TCP_PORT', 'REMOTE_IPS',\n       'APP_BYTES', 'SOURCE_APP_PACKETS', 'REMOTE_APP_PACKETS',\n       'SOURCE_APP_BYTES', 'REMOTE_APP_BYTES', 'APP_PACKETS',\n       'DNS_QUERY_TIMES', 'Type'],axis = \"columns\")\n\nprint(string_dataset.columns)\n\n\n#It is implausible to one hot encode dates because of how unique they are\nstring_dataset = string_dataset.drop(['WHOIS_REGDATE', 'WHOIS_UPDATED_DATE'],axis=\"columns\")\n\nfor i in string_dataset.columns:\n  arr = [\"hello\"]\n  firsttime = True\n  # Looped through each feature in string_dataset and figured out all the unique datapoints \n  for j in string_dataset[i]:\n    duplicated = False\n    for k in arr: #alternate approach for when using an empty list: if j.lower() not in arr: **indent** arr.append(j.lower())\n      if(j.lower() == k.lower()):\n        duplicated = True\n    if(duplicated == False):\n      arr.append(j)\n  arr.remove(\"hello\")\n  #loop through the unique features in the column list  and see if the dataset column has it or not\n  for j in arr:\n    encoded = [0]*(string_dataset.shape[0]) #or string.shape[0]\n    b = 0\n    for d in dataset[i]:\n      if(d == j):\n        encoded[b] = 1\n      b = b + 1\n    fencoded = array(encoded)\n    dataset[j] = fencoded.tolist()\nprint(dataset.shape)\n\n  \n","3548dc75":"#Delete all the string columns since we have encoded them already\n#If i wanted to use dates of server updates, you can use datetime() the librarys is import datetime\n#result = date1-date2 \n#use minmax scaler to help weigh the date data\nrevised_dataset = dataset.drop(['CHARSET', 'SERVER', 'WHOIS_COUNTRY', 'WHOIS_STATEPRO', 'WHOIS_REGDATE',\n       'WHOIS_UPDATED_DATE'],axis=\"columns\")\n\n","5f7e5b7b":"print(dataset.shape)\nfinished_dataset = revised_dataset.drop(['URL'],axis = \"columns\")\n","c4d22e70":"X = finished_dataset.drop('Type',axis = 'columns')\ny = finished_dataset['Type']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\nregressor = DecisionTreeClassifier(random_state = 0)\n#prints out the columns to double check that the unique features are there\n#for i in X.columns:\n  #print(i)\n\n#fitting the data to the default settings\nregressor.fit(X_train,y_train)\ny_predict = regressor.predict(X_test)\nprint(regressor.score(X_test,y_test))\n\n\n\n\n","12a66360":"from scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n\nparameters = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n\ntree =DecisionTreeClassifier()\ntree_cv =GridSearchCV(tree,parameters,cv = 5)\ntree_cv.fit(X_test,y_test)\n\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))","dbba3b39":"**Summary:**\n\nOverall, this research project has allowed me to learn a lot more about machine learning and also how different features affect the maliciousness of a website.\n\nDuring this time, I've learned about the advantages and disadvantages of machine learning models. For instance, Support Vector Machines use a lot of time to calculate results but it works well with predictions where there is a distinct separation between classes. I also learned that there is a difference between DecisionTreeClassifier and DecisionTreeRegressor and that classifiers are used for datasets where you have to predict a binary type of data. For this application, I used DecisionTreeClassifier because website maliciousness is a binary outcome. I also learned quite a lot about feature importance which I did not think was very effective before I started this project. I learned how to use cross validation and correlation graphs to see which features correlate to the feature I am trying to predict.\n\n\n I felt that I learned a lot during my time working on this project and here are some future goals that I have.\n\nShapley values are a great way of finding feature importance. Shapley values compare model performance with and without a feature to find which features hold the greatest impact on the model's predictions. By using the features that hold a greater impact on the model, the model's performance can be improved. In the future, I will explore using of Shapley values to improve the performance of maliciousness prediction models. \n\nWhen I was working on this project I continously encountered issues relating to the small size of the data set. 1,000 data points is likely a poor representation of the billion websites on the internet as of today. To fix this, I wanted to find a way to get more data. The creator of the Kaggle dataset I studied said that he used a low client honeypot to capture the data coming from each website. I will definitely attempt to set one up in the future so that I can have more data to feed to my model. \n\n","8f7ae6fc":"**Code:**","2c01b327":"Below are the modules I have imported for my Machine Learning model. It includes libraries and the dataset.\n\n\n\n\n","0f593f8d":"**Code:**","005f1f10":"**The Learning Phase Part 1 - How did I convert data to feed into a learning model?**\n\n\n\n\n\nThe code cell below separates the dataset into string and integer features and switches the string features into integer features by one-hot encoding the data. \n\nMachine learning models only take in numerical data so string type data are not able to be used by the chosen model. Sometimes, this is not a problem if the dataset is mostly made of numerical data. However, the dataset I am using has quite a lot of string type data and if it were all removed, there would not be many features left to work with. Therefore, I decided to one hot encode the data so that I have more features to work with. \n\n**Importance:**\n\nThe dataset includes many string type features that I hypothesized would be crucial in determing website maliciousness. For instance, features like server origination, server names, country origination and dates could be important factors for predicting maliciousness.\n\n\n","3ac125ce":"**The Analysis Phase: Machine Learning Model Analysis**\n\nIn the code cell below, I reached the conclusion of using decision tree classifier and train test split by doing different methods. \n\nNear the beginning of my project, I tested many ML algorithms to see which one worked the best. The algorithms that I tested were LinearRegression, DecisionTreeRegressor, DecisionTreeClassifier and SVM. When I first ran these models, I removed all the string features and just ran on the numerical data. The models performed relatively well but the SVM had a downside: it took almost 15 minutes for it to run, which made it very impractible for me to use given the time I had. Furthermore, if a SVM based model were to be implemented in a passive website detection program, it may not detect website maliciousness in time when users are surfing the web. In the end, I decided to choose DecisionTreeClassifier model because there were a lot of customizable options you could use to modify the model. This will allow me to make the model as suitable as it needs to be to detect malicious websites. I didn't consider using a neural network because the dataset I considered was too small to sufficiently train a neural network.\n\nWhen I was experimenting with my models, I found out that I was consistently getting extremely high accuracy (very close to 100%), which was suspicious, given that I was just running the model with the raw dataset. Later on, I realized that I had been training the model on the whole dataset and testing it on the whole dataset as well. This meant that the model already knew the answers beforehand, which was the reason behind the sky high accuracy. To fix this, I used train test split, a function that splits the dataset into two parts, the training part and the testing part.\n\n**Importance:**\n\nA machine learning model that detects malicious websites should be fast and accurate because users can stumble upon malware and malicious content very fast and the faster the prediction, the earlier the user can be warned. Therefore, time heavy models like SVM may not be suitable for these websites.  Tree-based models can give more room for flexibility and are relatively fast to run which is the reason why I chose the DecisionTreeClassifier model.","a2e65672":"**Code:**","91a2981e":"**Introduction**\n\nIn the vast Internet, there exist countless websites and it's often hard to tell which websites are malicious and which ones are safe to use. According to SecurityWeek,\"There are more than 1.86 billion websites on the internet. Around 1% of these -- something like 18,500,000 -- are infected with malware at a given time each week; while the average website is attacked 44 times every day\".\n\nThese websites pose a variety of risks to users, including stolen accounts and personal data and the collapse of small businesses or large corporations. However, by taking characteristics about a website, including its URL, server origination and other features, it may be possible to determine which websites are harmful to users.\n\nIn this analysis, I aimed to determine whether it is plausible leverage machine learning to predict website maliciousness from features about the website.\n\n**Overview**\n\nIn this Google Colab notebook, you will find the steps I took to analyze a Kaggle dataset containing data on websites with labels of whether those sites contain malicious content. I fit machine learning models to predict the maliciousness of websites. I focused on using tree-based classifiers as the base of my machine learning model and used Bag-Of-Words and RandomSearchCV to manipulate data and parameter hypertuning respectively.\n\n","d42edf39":"# Malicious Website Detection using DecisionTreeClassifier","cf5f54cb":"**The Learning Phase Part 2 -- Importance of URL data**\n\nURL's are a great resource in data because sometimes a URL can give quite a lot of information on the nature of a website. Although each URL is unique, it is possible to split the URL into different parts and make patterns off of each part. For instance, if the URL domain was not .com or .org or some other widely recognized domain, it may have a higher chance of being malicious. Unfortunately, I did not have the time to do this because it would have taken a lot of time, thinking and coding to find out where to split the URL. Some ideas I had was to split it by period, to find each section. For instance, colab.research.google.com would split it into colab, research, google and com.\n\n**Importance:**\n\nEven though, I did not implement the URL feature, URLs are an important part in website maliciousness. Often times, malicious websites like to imitate a well-known business' name as their URL name. For instance, ammazon or gooogle are tell-tale signs of maliciousness. Another example of URL importance is the domain name. If the domain is not a traditional domain and is along the lines of \".xyz\" or \".freemoney\", there is a high chance of the website being not legitimate.","168ac8d6":"**The Tuning Phase: Hyperparameter Tuning**\n\nDecisionTree models have many parameters that they take in and choosing which parameters the model takes in can cause some significant impact on the model performance. \n\nTo find the best parameters for my model, I decided to choose GridSearchCV because it was a good way to truly find the best parameters in the parameter list given. \n\nIn the beginning, I used RandomSearch and I wasn't able to get good results. RandomSearchCV gave me an accuracy score that was less than the default parameters. It turns out that RandomSearchCV takes random parameters from each part from the given parameter list. Given that the size of the dataset I was working with was quite small, I did not think that it would be good choice. However, if I were using a larger dataset and if the relationship between parameters and performance was non-linear, RandomSearchCV may have been a better choice.\n\nOn the other hand, GridSearchCV has a much more systematic way of choosing the best parameters, therefore I decided to use GridSearchCV rather than RandomSearchCV.\n\n**Importance:**\n\nHyperparameter tuning gives the model the accuracy needed for website maliciousness detection. By choosing which parameters impact the model the most, the model will be able to make more accurate predictions which will create a safer atmosphere for users.","ec344cf6":"**One-hot-encoding success**\n\nThe code cell below shows that there are 324 features, this lets us know that the code has successfully one hot encoded our string features by creating columns for each unique data point in the string data. This is important because with these extra features, we can have more features to use to determine whether websites are malicious. "}}