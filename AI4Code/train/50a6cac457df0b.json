{"cell_type":{"4c952e04":"code","f80b95e2":"code","8a9cffad":"code","37ff16bd":"code","b446151d":"code","8a82dd85":"code","a9211714":"code","04f62077":"code","38b15731":"code","1933082a":"code","7fd7c7ac":"code","0d99ef0f":"code","a12da7ba":"code","5b9e5c8d":"code","ef840cc1":"code","8c5a87ea":"code","b2fc079f":"code","3ad444b4":"code","3a83380f":"code","5396f90f":"code","682547bf":"code","155f447f":"code","e0ae2654":"code","d68fd27e":"code","1c9e46e3":"code","bb7b6c04":"code","7b007eb9":"code","79948b7a":"code","1e0cb3ac":"code","da25db9a":"code","3dac3bfd":"code","6c279311":"code","cacbd931":"code","3a53113b":"code","8ad691d0":"code","6d2c93df":"code","942c1354":"code","d62460d1":"code","0f1dc290":"code","8721c0b7":"code","dbe76d33":"code","28662a90":"code","9b2a28b5":"code","38287df3":"code","20f2667f":"code","5b1d266a":"markdown","f59fe2cb":"markdown","80b179f5":"markdown","14a487d0":"markdown","8b7f1d59":"markdown","bd65ec07":"markdown","62b2f1c1":"markdown","f2a2afb4":"markdown","5c581092":"markdown","2d672f9a":"markdown","c14c605f":"markdown","dde7cd82":"markdown","d55b5a3d":"markdown","ff9356ea":"markdown","ac5bc084":"markdown","1fbf9ed5":"markdown","0b7a0db5":"markdown","239e5a40":"markdown","017dfcf4":"markdown","0d9923aa":"markdown","b6b8310b":"markdown","0bf2d4db":"markdown","6b2379be":"markdown","f2ad820e":"markdown","7e558265":"markdown","69955cb6":"markdown","05be0bb1":"markdown"},"source":{"4c952e04":"#Loading library\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom nltk.stem import PorterStemmer\nimport nltk\nfrom sklearn.metrics import r2_score\n","f80b95e2":"youtube = pd.read_csv(\"..\/input\/youtube-new\/INvideos.csv\")\n","8a9cffad":"youtube.head()","37ff16bd":"print(youtube.shape)","b446151d":"print(youtube.isnull().values.any())","8a82dd85":"youtube = youtube.dropna(how='any',axis=0)","a9211714":"youtube.describe()","04f62077":"youtube.drop(['video_id','thumbnail_link'],axis=1,inplace=True)","38b15731":"youtube.apply(lambda x: len(x.unique()))","1933082a":"for x in (['comments_disabled','ratings_disabled','video_error_or_removed','category_id']):\n    count=youtube[x].value_counts()\n    print(count)\n    plt.figure(figsize=(7,7))\n    sns.barplot(count.index, count.values, alpha=0.8)\n    plt.title('{} vs No of video'.format(x))\n    plt.ylabel('No of video')\n    plt.xlabel('{}'.format(x))\n    plt.show()","7fd7c7ac":"#No of tags\ntags=[x.count(\"|\")+1 for x in youtube[\"tags\"]]\nyoutube[\"No_tags\"]=tags","0d99ef0f":"#length of desription\ndesc_len=[len(x) for x in youtube[\"description\"]]\nyoutube[\"desc_len\"]=desc_len","a12da7ba":"#length of title\ntitle_len=[len(x) for x in youtube[\"title\"]]\nyoutube[\"len_title\"]=title_len","5b9e5c8d":"publish_time = pd.to_datetime(youtube['publish_time'], format='%Y-%m-%dT%H:%M:%S.%fZ')\nyoutube['publish_time'] = publish_time.dt.time\nyoutube['publish_date'] = publish_time.dt.date\n\n#day at which video is publish\nyoutube['publish_weekday']=publish_time.dt.weekday_name","ef840cc1":"#ratio of view\/likes  upto 3 decimal\nyoutube[\"Ratio_View_likes\"]=round(youtube[\"views\"]\/youtube[\"likes\"],3)\n#ratio of view\/dislikes  upto 3 decimal\nyoutube[\"Ratio_View_dislikes\"]=round(youtube[\"views\"]\/youtube[\"dislikes\"],3)\n#ratio of view\/comment_count  upto 3 decimal\nyoutube[\"Ratio_views_comment_count\"]=round(youtube[\"views\"]\/youtube[\"comment_count\"],3)\n#ratio of likes\/dislikes  upto 3 decimal\nyoutube[\"Ratio_likes_dislikes\"]=round(youtube[\"likes\"]\/youtube[\"dislikes\"],3)","8c5a87ea":"print(max(youtube[\"Ratio_View_likes\"]))\nprint(max(youtube[\"Ratio_View_dislikes\"]))\nprint(max(youtube[\"Ratio_views_comment_count\"]))\nprint(max(youtube[\"Ratio_likes_dislikes\"]))","b2fc079f":"#removing the infinite values\nyoutube=youtube.replace([np.inf, -np.inf], np.nan)\nyoutube = youtube.dropna(how='any',axis=0)","3ad444b4":"youtube['publish_weekday'] = youtube['publish_weekday'].replace({'Monday':1,\n                                                             'Tuesday':2,\n                                                             'Wednesday':3,\n                                                             'Thursday':4,\n                                                             'Friday':5,\n                                                             'Saturday':6,\n                                                             'Sunday':7})","3a83380f":"count=youtube[\"publish_weekday\"].value_counts()\nprint(count)\nplt.figure(figsize=(7,7))\nsns.barplot(count.index, count.values, alpha=0.8)\nplt.title('No of videos vs weekdays')\nplt.ylabel('no of videos')\nplt.xlabel('weekdays')\nplt.show()","5396f90f":"data = youtube\n\ncorr = data.corr()\nplt.figure(figsize=(12, 12))\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","682547bf":"youtube.drop(['trending_date','publish_date','publish_time','tags','title','description','channel_title'],axis=1,inplace=True)","155f447f":"views=youtube['views']\nyoutube_view=youtube.drop(['views'],axis=1,inplace=False)","e0ae2654":"train,test,y_train,y_test=train_test_split(youtube_view,views, test_size=0.2,shuffle=False)","d68fd27e":"print(train.shape,test.shape,y_train.shape,y_test.shape)\n","1c9e46e3":"# REGRESSION ANALYSIS\n\n# LINEAR REGRESSION\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nmodel = LinearRegression()\nmodel.fit(train, y_train)\n\n# predicting the  test set results\ny_pred = model.predict(test)\nprint('Root means score', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('Variance score: %.2f' % r2_score(y_test, y_pred))\nprint(\"Result :\",model.score(test, y_test))\nd1 = {'True Labels': y_test, 'Predicted Labels': y_pred}\nSK = pd.DataFrame(data = d1)\nprint(SK)","bb7b6c04":"lm1 = sns.lmplot(x=\"True Labels\", y=\"Predicted Labels\", data = SK, size = 10)\nfig1 = lm1.fig \nfig1.suptitle(\"Sklearn \", fontsize=18)\nsns.set(font_scale = 1.5)","7b007eb9":"from sklearn.ensemble import RandomForestRegressor\nnEstimator = [140,160,180,200,220]\ndepth = [10,15,20,25,30]\n\nRF = RandomForestRegressor()\nhyperParam = [{'n_estimators':nEstimator,'max_depth': depth}]\ngsv = GridSearchCV(RF,hyperParam,cv=5,verbose=1,scoring='r2',n_jobs=-1)\ngsv.fit(train, y_train)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\nscores = gsv.cv_results_['mean_test_score'].reshape(len(nEstimator),len(depth))\nplt.figure(figsize=(8, 8))\nplt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\nplt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\nplt.xlabel('n_estimators')\nplt.ylabel('max_depth')\nplt.colorbar()\nplt.xticks(np.arange(len(nEstimator)), nEstimator)\nplt.yticks(np.arange(len(depth)), depth)\nplt.title('Grid Search r^2 Score')\nplt.show()\nmaxDepth=gsv.best_params_['max_depth']\nnEstimators=gsv.best_params_['n_estimators']","79948b7a":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators = nEstimators,max_depth=maxDepth)\nmodel.fit(train, y_train)\n\n\n# predicting the  test set results\ny_pred = model.predict(test)\nprint('Root means score', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('Variance score: %.2f' % r2_score(y_test, y_pred))\nprint(\"Result :\",model.score(test, y_test))\nd1 = {'True Labels': y_test, 'Predicted Labels': y_pred}\nSK = pd.DataFrame(data = d1)\nprint(SK)","1e0cb3ac":"lm1 = sns.lmplot(x=\"True Labels\", y=\"Predicted Labels\", data = SK, size = 10)\nfig1 = lm1.fig \nfig1.suptitle(\"Sklearn \", fontsize=18)\nsns.set(font_scale = 1.5)\n","da25db9a":"likes=youtube['likes']\nyoutube_like=youtube.drop(['likes'],axis=1,inplace=False)","3dac3bfd":"train,test,y_train,y_test=train_test_split(youtube_like,likes, test_size=0.2,shuffle=False)","6c279311":"print(train.shape,test.shape,y_train.shape,y_test.shape)","cacbd931":"# REGRESSION ANALYSIS\n\n# LINEAR REGRESSION\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nmodel = LinearRegression()\nmodel.fit(train, y_train)\n\n# predicting the  test set results\ny_pred = model.predict(test)\nprint('Root means score', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('Variance score: %.2f' % r2_score(y_test, y_pred))\nprint(\"Result :\",model.score(test, y_test))\nd1 = {'True Labels': y_test, 'Predicted Labels': y_pred}\nSK = pd.DataFrame(data = d1)\nprint(SK)","3a53113b":"lm1 = sns.lmplot(x=\"True Labels\", y=\"Predicted Labels\", data = SK, size = 10)\nfig1 = lm1.fig \nfig1.suptitle(\"Sklearn \", fontsize=18)\nsns.set(font_scale = 1.5)","8ad691d0":"from sklearn.ensemble import RandomForestRegressor\n\nnEstimator = [140,160,180,200,220]\ndepth = [10,15,20,25,30]\n\nRF = RandomForestRegressor()\nhyperParam = [{'n_estimators':nEstimator,'max_depth': depth}]\ngsv = GridSearchCV(RF,hyperParam,cv=5,verbose=1,scoring='r2',n_jobs=-1)\ngsv.fit(train, y_train)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\nscores = gsv.cv_results_['mean_test_score'].reshape(len(nEstimator),len(depth))\n\nplt.figure(figsize=(8, 8))\nplt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\nplt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\nplt.xlabel('n_estimators')\nplt.ylabel('max_depth')\nplt.colorbar()\nplt.xticks(np.arange(len(nEstimator)), nEstimator)\nplt.yticks(np.arange(len(depth)), depth)\nplt.title('Grid Search r^2 Score')\nplt.show()\nmaxDepth=gsv.best_params_['max_depth']\nnEstimators=gsv.best_params_['n_estimators']","6d2c93df":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators = nEstimators,max_depth=maxDepth)\nmodel.fit(train, y_train)\n\n\n# predicting the  test set results\ny_pred = model.predict(test)\nprint('Root means score', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('Variance score: %.2f' % r2_score(y_test, y_pred))\nprint(\"Result :\",model.score(test, y_test))\nd1 = {'True Labels': y_test, 'Predicted Labels': y_pred}\nSK = pd.DataFrame(data = d1)\nprint(SK)","942c1354":"lm1 = sns.lmplot(x=\"True Labels\", y=\"Predicted Labels\", data = SK, size = 10)\nfig1 = lm1.fig \nfig1.suptitle(\"Sklearn \", fontsize=18)\nsns.set(font_scale = 1.5)","d62460d1":"comment_count=youtube['comment_count']\nyoutube_comment=youtube.drop(['comment_count'],axis=1,inplace=False)","0f1dc290":"train,test,y_train,y_test=train_test_split(youtube_comment,comment_count, test_size=0.2,shuffle=False)","8721c0b7":"print(train.shape,test.shape,y_train.shape,y_test.shape)\n","dbe76d33":"# REGRESSION ANALYSIS\n\n# LINEAR REGRESSION\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nmodel = LinearRegression()\nmodel.fit(train, y_train)\n\n# predicting the  test set results\ny_pred = model.predict(test)\nprint('Root means score', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('Variance score: %.2f' % r2_score(y_test, y_pred))\nprint(\"Result :\",model.score(test, y_test))\nd1 = {'True Labels': y_test, 'Predicted Labels': y_pred}\nSK = pd.DataFrame(data = d1)\nprint(SK)","28662a90":"lm1 = sns.lmplot(x=\"True Labels\", y=\"Predicted Labels\", data = SK, size = 10)\nfig1 = lm1.fig \nfig1.suptitle(\"Sklearn \", fontsize=18)\nsns.set(font_scale = 1.5)","9b2a28b5":"nEstimator = [140,160,180,200,220]\ndepth = [10,15,20,25,30]\n\nRF = RandomForestRegressor()\nhyperParam = [{'n_estimators':nEstimator,'max_depth': depth}]\ngsv = GridSearchCV(RF,hyperParam,cv=5,verbose=1,scoring='r2',n_jobs=-1)\ngsv.fit(train, y_train)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\nscores = gsv.cv_results_['mean_test_score'].reshape(len(nEstimator),len(depth))\n\nplt.figure(figsize=(8, 8))\nplt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\nplt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\nplt.xlabel('n_estimators')\nplt.ylabel('max_depth')\nplt.colorbar()\nplt.xticks(np.arange(len(nEstimator)), nEstimator)\nplt.yticks(np.arange(len(depth)), depth)\nplt.title('Grid Search r^2 Score')\nplt.show()\nmaxDepth=gsv.best_params_['max_depth']\nnEstimators=gsv.best_params_['n_estimators']","38287df3":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators = nEstimators,max_depth=maxDepth)\nmodel.fit(train, y_train)\n\n\n# predicting the  test set results\ny_pred = model.predict(test)\nprint('Root means score', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('Variance score: %.2f' % r2_score(y_test, y_pred))\nprint(\"Result :\",model.score(test, y_test))\nd1 = {'True Labels': y_test, 'Predicted Labels': y_pred}\nSK = pd.DataFrame(data = d1)\nprint(SK)","20f2667f":"lm1 = sns.lmplot(x=\"True Labels\", y=\"Predicted Labels\", data = SK, size = 10)\nfig1 = lm1.fig \nfig1.suptitle(\"Sklearn \", fontsize=18)\nsns.set(font_scale = 1.5)\n","5b1d266a":"### 5.1.1 spliting the data into train and test in ratio of  80:20 ","f59fe2cb":"## 6.Conclusion\n### View Predicition\n\n|Model|Variance|Result|\n|-----|--------|------|\n|Linear Regression|0.73|0.734|\n|Random Forests|0.98|0.984|\n\n### Like Predicition\n\n|Model|Variance|Result|\n|-----|--------|------|\n|Linear Regression|0.41|0.411|\n|Random Forests|0.96|0.958|\n\n\n### No of Comment\n\n|Model|Variance|Result|\n|-----|--------|------|\n|Linear Regression|0.35|0.35|\n|Random Forests|0.81|0.81|\n","80b179f5":"## 3.Feature Engineering","14a487d0":"## 5.Machine Learning Models","8b7f1d59":"## 5.1.3 Random Forest","bd65ec07":"## 2.EDA","62b2f1c1":"### 5.3.3 Random Forest","f2a2afb4":"## 5.1.3.2 Random Forest using Optimal Hyperparameter","5c581092":"### 5.3.3.1 Hypermeter Turning","2d672f9a":"### Removing non Correlated coloumns","c14c605f":"### 5.2.2 Linear Regression","dde7cd82":"### 5.2.3.2 Random Forest using the optimal hypermeter","d55b5a3d":"### 5.2.1 spliting the data into train and test in ratio of  80:20 ","ff9356ea":"## 5.2 Prediciting Likes","ac5bc084":"### 5.3.3.1 RandomForest optimal Hyper-Parameter","1fbf9ed5":"## 4.Correlation  Matrix\n","0b7a0db5":"# Youtube Likes,views Prediction\n\n## Table of content\n* 1.Machine Learning Formulation\n    * 1.1 Data Overview\n    * 1.2  Attribute-information\n* 2.EDA\n* 3.Feature Engineering\n    * 3.1 publish_weekday-Day at which video is publish\n    * 3.2 No of Tags-No of tag video contain\n    * 3.3 Length of description-Length of video description\n    * 3.4 Ratio's\n         * 3.4.1 Ratio of View and likes\n         * 3.4.2 Ratio of view and dislikes\n         * 3.4.3 Ratio of view and comment_count\n         * 3.4.4 Ratio of likes and dislikes\n* 4.Correlation matrix\n* 5.Machine Learning(metric=r^2 score )\n    * 5.1-View Predicition\n         * 5.1.1-Splitting the data into train and Test(80:20)\n         * 5.1.2-Linear Regression\n         * 5.1.3-Random Forest\n              \n  * 5.2 -Like Predicition\n       * 5.2.1-Splitting the data into train and Test(80:20)\n       * 5.2.2-Linear Regression\n       * 5.2.3-Random Forest\n       \n  * 5.3-comment Count Predicition\n      * 5.3.1-Splitting the data into train and Test(80:20)\n      * 5.3.2-Linear Regression\n      * 5.3.3-Random Forest\n      \n* 6.Conclusion\n\n## 1.Machine Learning Formulation\n### 1.1 Data Overviews\nContain one file\n\n#### 1.2 Attribute-information\n* video_id-Unique video id\n* trending_date-the date at which video start trending\n* title-Title of video\n* channel_title-video posted by channel\n* category_id-there are 15 Category value\n* publish_time-at what time video is uplaoded\n* tags-tag given to video\n* views-no of views\n* likes-no of likes\n* dislikes-no of dislikes\n* comment_count-no of comment\n\n\n","239e5a40":"### 5.3.1 spliting the data into train and test in ratio of  80:20 ","017dfcf4":"## 5.3 Prediciting No of Comment","0d9923aa":"## 5.1.2 Linear Regression","b6b8310b":"### 5.2.3.1 Hypermeter Turning","0bf2d4db":"### 5.3.2 Linear Regression","6b2379be":" * 3.1 publish_weekday-Day at which video is publish\n * 3.2 No of Tags-No of tag video contain\n * 3.3 Length of description-Length of video description\n * 3.4 Ratio's\n    * 3.4.1 Ratio of View and likes\n    * 3.4.2 Ratio of view and dislikes\n    * 3.4.3 Ratio of view and comment_count\n    * 3.4.4 Ratio of likes and dislikes","f2ad820e":"### Loading Libary","7e558265":"## 5.1.3.1 Hyper-parameter Turning","69955cb6":"## 5.1 Prediciting Views","05be0bb1":"### 5.2.3 Random Forest"}}