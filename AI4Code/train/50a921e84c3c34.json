{"cell_type":{"7faed365":"code","faab72df":"code","c65fa921":"code","c8adf615":"code","17a2e6f0":"code","01f35fca":"code","4d943242":"code","ea76a523":"code","fc553621":"code","8fded205":"code","2d15b6e3":"code","3aae3a95":"code","47890f1e":"code","699cb8ab":"code","19d275bc":"code","740b463e":"code","3bf487a3":"code","31e794f2":"code","7052a3b0":"code","a62c0862":"code","8e8a4552":"code","be9a52c7":"code","7766235e":"code","e130d17c":"code","e191af63":"markdown","24d33945":"markdown","4243d243":"markdown","824a8dfe":"markdown","5344e3d2":"markdown","8fda843c":"markdown","0e119e58":"markdown","0145439d":"markdown","ed5df2ac":"markdown","5a9cf65b":"markdown","9b50b4ec":"markdown","85b98d5c":"markdown","3f378ae6":"markdown","6beaf786":"markdown","ff57a483":"markdown","c4672ff4":"markdown","0fb5e6d7":"markdown","dc870962":"markdown","9bec4b7b":"markdown"},"source":{"7faed365":"import numpy as np \nimport pandas as pd \n\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import mode, skew, kurtosis, entropy\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport dask.dataframe as dd\nfrom dask.multiprocessing import get\n\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas(tqdm_notebook)\n\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Print all rows and columns\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nIS_LOCAL = False\n\nimport os\n\nif(IS_LOCAL):\n    PATH=\"..\/input\/santander-value-prediction-challenge\/\"\nelse:\n    PATH=\"..\/input\/\"\nprint(os.listdir(PATH))","faab72df":"train = pd.read_csv(PATH+\"train.csv\")\ntest = pd.read_csv(PATH+\"test.csv\")","c65fa921":"NLAGS = 29 #number of lags for leak calculation","c8adf615":"all_cols = [f for f in train.columns if f not in [\"ID\", \"target\"]]\ny = np.log1p(train[\"target\"]).values\ncols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1',\n        '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9',\n        'd6bb78916', 'b43a7cfd5', '58232a6fb', '1702b5bf0', '324921c7b',\n        '62e59a501', '2ec5b290f', '241f0f867', 'fb49e4212', '66ace2992',\n        'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', '1931ccfdd',\n        '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a',\n        '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2', '0572565c2',\n        '190db8488', 'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98']","17a2e6f0":"def _get_leak(df, cols, lag=0):\n    \"\"\" To get leak value, we do following:\n       1. Get string of all values after removing first two time steps\n       2. For all rows we shift the row by two steps and again make a string\n       3. Just find rows where string from 2 matches string from 1\n       4. Get 1st time step of row in 3 (Currently, there is additional condition to only fetch value if we got exactly one match in step 3)\"\"\"\n    series_str = df[cols[lag+2:]].apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n    series_shifted_str = df[cols].shift(lag+2, axis=1)[cols[lag+2:]].apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n    target_rows = series_shifted_str.progress_apply(lambda x: np.where(x == series_str)[0])\n    target_vals = target_rows.apply(lambda x: df.loc[x[0], cols[lag]] if len(x)==1 else 0)\n    return target_vals\n\ndef get_all_leak(df, cols=None, nlags=15):\n    \"\"\"\n    We just recursively fetch target value for different lags\n    \"\"\"\n    df =  df.copy()\n    for i in range(nlags):\n        print(\"Processing lag {}\".format(i))\n        df[\"leaked_target_\"+str(i)] = _get_leak(df, cols, i)\n    return df","01f35fca":"test[\"target\"] = train[\"target\"].mean()","4d943242":"all_df = pd.concat([train[[\"ID\", \"target\"] + cols], test[[\"ID\", \"target\"]+ cols]]).reset_index(drop=True)\nall_df.head()","ea76a523":"all_df = get_all_leak(all_df, cols=cols, nlags=NLAGS)","fc553621":"leaky_cols = [\"leaked_target_\"+str(i) for i in range(NLAGS)]\ntrain = train.join(all_df.set_index(\"ID\")[leaky_cols], on=\"ID\", how=\"left\")\ntest = test.join(all_df.set_index(\"ID\")[leaky_cols], on=\"ID\", how=\"left\")\ntrain[[\"target\"]+leaky_cols].head(10)","8fded205":"train[\"nz_mean\"] = train[all_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\ntest[\"nz_mean\"] = test[all_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)","2d15b6e3":"train[\"compiled_leak\"] = 0\ntest[\"compiled_leak\"] = 0\nfor i in range(NLAGS):\n    train.loc[train[\"compiled_leak\"] == 0, \"compiled_leak\"] = train.loc[train[\"compiled_leak\"] == 0, \"leaked_target_\"+str(i)]\n    test.loc[test[\"compiled_leak\"] == 0, \"compiled_leak\"] = test.loc[test[\"compiled_leak\"] == 0, \"leaked_target_\"+str(i)]\n    \nprint(\"Leak values found in train and test \", sum(train[\"compiled_leak\"] > 0), sum(test[\"compiled_leak\"] > 0))\nprint(\"% of correct leaks values in train \", sum(train[\"compiled_leak\"] == train[\"target\"])\/sum(train[\"compiled_leak\"] > 0))","3aae3a95":"train.loc[train[\"compiled_leak\"] == 0, \"compiled_leak\"] = train.loc[train[\"compiled_leak\"] == 0, \"nz_mean\"]\ntest.loc[test[\"compiled_leak\"] == 0, \"compiled_leak\"] = test.loc[test[\"compiled_leak\"] == 0, \"nz_mean\"]\nnp.sqrt(mean_squared_error(y, np.log1p(train[\"compiled_leak\"]).fillna(14.49)))\n","47890f1e":"sub1 = test[[\"ID\"]]\nsub1[\"target\"] = test[\"compiled_leak\"]","699cb8ab":"NUMBER_KFOLDS  = 5\nNFOLDS = 5 #folds number for CV\nMAX_ROUNDS = 3000 #lgb iterations\nEARLY_STOP = 100 #lgb early stop \nVERBOSE_EVAL = 200 #Print out metric result","19d275bc":"train = pd.read_csv(PATH+\"train.csv\")\ntest = pd.read_csv(PATH+\"test.csv\")\nall_cols = [c for c in train.columns if c not in ['ID', 'target']]\nleak_col = []\nfor c in all_cols:\n    leak1 = np.sum((train[c]==train['target']).astype(int))\n    leak2 = np.sum((((train[c] - train['target']) \/ train['target']) < 0.05).astype(int))\n    if leak1 > 30 and leak2 > 3500:\n        leak_col.append(c)","740b463e":"print('Leak columns: ',len(leak_col))","3bf487a3":"print('Leak columns: ',leak_col)","31e794f2":"col = list(leak_col)\ntrain_lk = train[col +  ['ID', 'target']]\ntest_lk = test[col +  ['ID']]","7052a3b0":"for df in [train_lk, test_lk]:\n    df[\"nz_mean\"] = df[col].apply(lambda x: x[x!=0].mean(), axis=1)\n    df[\"nz_max\"] = df[col].apply(lambda x: x[x!=0].max(), axis=1)\n    df[\"nz_min\"] = df[col].apply(lambda x: x[x!=0].min(), axis=1)\n    df[\"ez\"] = df[col].apply(lambda x: len(x[x==0]), axis=1)\n    df[\"mean\"] = df[col].apply(lambda x: x.mean(), axis=1)\n    df[\"max\"] = df[col].apply(lambda x: x.max(), axis=1)\n    df[\"min\"] = df[col].apply(lambda x: x.min(), axis=1)\n    df[\"kurtosis\"] = df[col].apply(lambda x: x.kurtosis(), axis=1)\ncol += ['nz_mean', 'nz_max', 'nz_min', 'ez', 'mean', 'max', 'min', 'kurtosis']","a62c0862":"for i in range(2, 100):\n    train_lk['index'+str(i)] = ((train_lk.index + 2) % i == 0).astype(int)\n    test_lk['index'+str(i)] = ((test_lk.index + 2) % i == 0).astype(int)\n    col.append('index'+str(i))","8e8a4552":"test_lk = pd.merge(test_lk, sub1, how='left', on='ID',)","be9a52c7":"from scipy.sparse import csr_matrix, vstack\ntrain_lk = train_lk.replace(0, np.nan)\ntest_lk = test_lk.replace(0, np.nan)\ntrain_lk = pd.concat((train_lk, test_lk), axis=0, ignore_index=True)","7766235e":"test_lk['target'] = 0.0\nfolds = NFOLDS\nfor fold in range(folds):\n    x1, x2, y1, y2 = model_selection.train_test_split(train_lk[col], \n                                                      np.log1p(train_lk.target.values), \n                                                      test_size=0.20, \n                                                      random_state=fold)\n    params = {'learning_rate': 0.02,\n              'max_depth': 7, \n              'boosting': 'gbdt', \n              'objective': 'regression', \n              'metric': 'rmse', \n              'is_training_metric': True, \n              'feature_fraction': 0.9, \n              'bagging_fraction': 0.8, \n              'bagging_freq': 5, \n              'seed':fold}\n    model = lgb.train(params, \n                      lgb.Dataset(x1, label=y1), \n                      MAX_ROUNDS, \n                      lgb.Dataset(x2, label=y2), \n                      verbose_eval=VERBOSE_EVAL, \n                      early_stopping_rounds=EARLY_STOP)\n    test_lk['target'] += np.expm1(model.predict(test_lk[col], \n                                num_iteration=model.best_iteration))\ntest_lk['target'] \/= folds\nsub1 = test_lk[['ID', 'target']]","e130d17c":"#submission\ntest_lk[['ID', 'target']].to_csv('submission.csv', index=False)","e191af63":"Main calculation for leaks.","24d33945":"Run the lgb model.","4243d243":"Start with the first lag and recursivelly fill zeros.","824a8dfe":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>\n\n# <a id=\"4\">Exploit the leak<\/a>","5344e3d2":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>\n\n# <a id=\"6\">Average and submission<\/a>","8fda843c":"Before applying the **get_all_leaks** function, we create a unique dataframe with all selected columns in train and test sets.","0e119e58":"# <a id=\"3\">Read the data<\/a>","0145439d":"# <a id=\"2\">Load packages<\/a>","ed5df2ac":"# <a id=\"1\">Introduction<\/a>  \n\nIn this Kernel we combine the  creation of a model with selected features [1][2] with exploatation of the leak (as identified by Giba [3] and developed by Moshin [4])\n\n<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>","5a9cf65b":"Merge test_lk with prepared sub1 = test[ID, target] calculated before by exploiting the leal.","9b50b4ec":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/b8\/Banco_Santander_Logotipo.svg\" width=\"800\"><\/img>\n\n<h1><center><font size=\"6\">Combine Leak Exploit and Model with Selected Features<\/font><\/center><\/h1>\n\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Load packages<\/a>  \n- <a href='#3'>Read the data<\/a>  \n- <a href='#4'>Exploit the leak<\/a> \n- <a href='#5'>Build a model<\/a>\n- <a href='#6'>Averaging and submission<\/a> \n- <a href='#7'>References<\/a>","85b98d5c":"Replace zeros with NAs in both train_lk and test_lk and merge train_lk with test_lk in train_lk","3f378ae6":"Then we join both train and test sets with all_df leaky columns.","6beaf786":"# <a id=\"7\">References<\/a>  \n\n\n[1] <a href=\"https:\/\/www.kaggle.com\/ogrellier\">olivier<\/a>, <a href=\"https:\/\/www.kaggle.com\/ogrellier\/santander-46-features\">Santander_46_features<\/a>   \n[2] <a href=\"https:\/\/www.kaggle.com\/the1owl\">the1owl<\/a>, <a href=\"https:\/\/www.kaggle.com\/the1owl\/love-is-the-answer\">Love is the answer<\/a>   \n[3] <a href=\"https:\/\/www.kaggle.com\/titericz\">Giba<\/a>, <a href=\"https:\/\/www.kaggle.com\/titericz\/the-property-by-giba\">The Property of Giba<\/a>   \n[4] <a href=\"https:\/\/www.kaggle.com\/tezdhar\">Mohsin Hasan<\/a>, <a href=\"https:\/\/www.kaggle.com\/tezdhar\/breaking-lb-fresh-start\">Breaking LB - Fresh Start<\/a>   \n","ff57a483":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>\n\n\n# <a id=\"5\">Build a model<\/a>","c4672ff4":"## Model parameters","0fb5e6d7":"We replace with the non-zeros mean the compiled leaks equal with zero.","dc870962":"We calculate the mean for non-zero columns.","9bec4b7b":"We initialize the test **target** column with the mean of train **target** values."}}