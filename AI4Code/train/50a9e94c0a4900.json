{"cell_type":{"03aa1d13":"code","ab3820ce":"code","e2962609":"code","d5d8c97e":"code","c3969491":"code","23cd1946":"code","9373bacf":"code","20ae6274":"code","9fd84156":"code","eb2e3dec":"code","937cb329":"code","b671c9a4":"code","78d5dbbe":"code","4ff3e9f1":"code","49fcec8f":"code","18ed8813":"code","25171b61":"code","d7d0aea3":"code","49f86613":"code","b783db24":"code","23a0eb75":"code","d801fae5":"code","6cc2c928":"code","91371ee6":"code","3b4cdc7d":"code","217d6cb5":"code","5bd02b7d":"code","ff7db52e":"code","b3f14937":"code","b1fe733f":"code","5c16dc78":"code","3595123f":"code","cd988792":"code","9680846b":"code","bfb4adaf":"code","f61ef872":"code","0253c507":"code","c7b8342f":"code","840c221f":"code","71975669":"code","c7b573a5":"code","d22aa319":"code","4ab726d2":"code","69896f54":"code","5f8bc73b":"code","ca94f8d0":"code","570e562a":"code","cb8a0310":"code","cdda2349":"code","c615f8b1":"code","203f8cb5":"markdown","ec171ea1":"markdown","8f706dcf":"markdown","ed47941b":"markdown","704b0d49":"markdown","17f4ad4a":"markdown","2d86f37e":"markdown","cbd795ea":"markdown","76c7f467":"markdown","4a72f6e1":"markdown","98a18424":"markdown","b01fefe1":"markdown","d3d9b7eb":"markdown","2c7b6591":"markdown","6921e425":"markdown","56614213":"markdown","12b1c653":"markdown","a7facd30":"markdown","afb6a7d6":"markdown","e50730d3":"markdown","4d2c189b":"markdown","abc20945":"markdown","eba356eb":"markdown","5576aeea":"markdown","8827ebd6":"markdown"},"source":{"03aa1d13":"#import libraries\nfrom operator import itemgetter    \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nimport warnings\nfrom scipy import stats\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import RobustScaler, OrdinalEncoder,OneHotEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom scipy.stats import norm,skew\nfrom sklearn.base import  BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV,train_test_split\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import  GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\n\nprint(\"Modules imported \\n\")\n\nget_ipython().magic(u'matplotlib inline')\nplt.style.use('ggplot')\nwarnings.filterwarnings('ignore')\n%matplotlib inline","ab3820ce":"# Load raw data\ntrain = pd.read_csv('..\/input\/dataset\/train.csv') \ntest = pd.read_csv('..\/input\/dataset\/test.csv') ","e2962609":"set(train.columns) - set(test.columns)","d5d8c97e":"# Histograms\ntrain.hist(bins=50, figsize=(20,15))\nplt.show()","c3969491":"train.describe()","23cd1946":"log_price = np.log1p(train[\"SalePrice\"])\n \nsns.distplot(log_price, fit=norm);\n(mu, sigma) = norm.fit(log_price)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution');","9373bacf":"fig = plt.figure()\nres = stats.probplot(log_price, plot=plt)\nplt.show()","20ae6274":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.title('SalePrice vs. GrLivArea')\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('GrLivArea', fontsize=15)\nplt.show()","9fd84156":"var = 'TotalBsmtSF'\nfig, ax = plt.subplots()\nax.scatter(x = train[var], y = train['SalePrice'])\nplt.title('SalePrice vs. TotalBasement')\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('TotalBsmtSF', fontsize=15)\nplt.show();","eb2e3dec":"var = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nplt.title('SalePrice vs. OverallQuality')\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)","937cb329":"plt.title('SalePrice vs. GarageArea')\nsns.boxplot(y='SalePrice', x = 'GarageArea', data=train);","b671c9a4":"# Pearson Correlation Coefficient\ncorr_matrix = train.corr()\ncorr_matrix[\"SalePrice\"].sort_values(ascending=False)","78d5dbbe":"# Scatter_matrix\nattributes = [\"OverallQual\", \"YrSold\", \"YearBuilt\",\"SalePrice\"]\nscatter_matrix(train[attributes], figsize=(12, 8));","4ff3e9f1":"trainwprice = pd.DataFrame(train)\ntrainnoprice = trainwprice.drop(\"SalePrice\", axis=1)\n\ntrainnoprice.drop(['Id'],axis=1, inplace=True)\ntest.drop(['Id'],axis=1, inplace=True)\n\nprint(\"train \", trainnoprice.shape)\nprint(\"test \", test.shape)","49fcec8f":"def check_missing_cols(df) -> pd.DataFrame:\n    \"\"\"\n    Checks missing values in a df\n    \"\"\"\n    ColsMissingValues = df.isnull().sum()\n    print(\"There are \", len(ColsMissingValues[ColsMissingValues>0]), \" features with missing values\")\n\n    all_data_na = (df.isnull().sum() \/ len(df)) * 100\n    all_data_na = all_data_na.sort_values(ascending=False)\n    missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n    print(missing_data.head(len(ColsMissingValues[ColsMissingValues>0])))","18ed8813":"check_missing_cols(train)","25171b61":"check_missing_cols(test)","d7d0aea3":"categorical_features = [\"MSZoning\",\"Street\",\"Alley\",\"LotShape\",\"LandContour\",\"Utilities\",\"LotConfig\",\"LandSlope\",\n                        \"Neighborhood\",\"Condition1\",\"Condition2\",\"BldgType\",\"HouseStyle\",\n                        \"RoofStyle\",\"RoofMatl\",\"Exterior1st\",\"Exterior2nd\",\"MasVnrType\",\"ExterQual\",\"ExterCond\",\n                        \"Foundation\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"Heating\",\n                        \"HeatingQC\",\"CentralAir\",\"Electrical\",\"KitchenQual\",\"Functional\", \"FireplaceQu\",\"GarageType\",\n                        \"GarageFinish\",\"GarageQual\",\"GarageCond\",\"PavedDrive\",\"PoolQC\", \"Fence\",\n                        \"MiscFeature\",\"SaleType\",\"SaleCondition\"]\n            \n                        \nnumerical_features = [\"LotFrontage\",\"LotArea\",\"OverallQual\",\"OverallCond\",\"MasVnrArea\",\"BsmtFinSF1\",\"BsmtFinSF2\",\n                     \"BsmtUnfSF\",\"TotalBsmtSF\",\"1stFlrSF\",\"2ndFlrSF\",\"LowQualFinSF\",\"GrLivArea\",\n                      \"BsmtFullBath\",\"BsmtHalfBath\",\"FullBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\n                      \"TotRmsAbvGrd\",\"Fireplaces\",\"GarageCars\",\"GarageArea\",\"WoodDeckSF\",\"OpenPorchSF\",\n                      \"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\"PoolArea\",\"MiscVal\",\"MoSold\",\"YearBuilt\",\n                      \"YearRemodAdd\",\"GarageYrBlt\",\"YrSold\"]","49f86613":"class AddFeature(BaseEstimator, TransformerMixin):\n    def __init__(self,additional=1):\n        self.additional = additional\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X) -> pd.DataFrame:\n        \"\"\"\n        Transforms dataframe by adding new features\n        \"\"\"\n        if self.additional==1:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n        else:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n\n        return X","b783db24":"def grid_get(self,X,y,param_grid) -> np.array:\n    \"\"\"\n    Performs grid search\n    \"\"\"\n    grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=\"neg_mean_squared_error\")\n    grid_search.fit(X,y)\n    print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n    grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n    print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])","23a0eb75":"class DropCols(BaseEstimator, TransformerMixin):\n    def __init__(self, remove_cols = True):\n        self.remove_cols = remove_cols\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X) -> pd.DataFrame:\n        \"\"\"\n        Deletes unwanted columns\n        \"\"\"\n        if self.remove_cols:\n            X.drop(columns=['PoolQC','BsmtFinType1','LowQualFinSF','MoSold','Electrical','BldgType','SaleType',\n                           'BsmtFinSF2','Exterior2nd','ExterCond','2ndFlrSF','3SsnPorch','Exterior1st','MasVnrType',\n                           'GarageFinish','Alley','PoolArea','RoofStyle','MiscFeature','Fence','BsmtHalfBath',\n                            'Street','LotConfig','TotalBsmtSF','GarageYrBlt'])\n          \n        return X","d801fae5":"numeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(missing_values=np.nan,strategy='median')),\n    ('scaler', RobustScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n                    ('imputer', SimpleImputer(missing_values=np.nan,strategy='constant',fill_value=\"None\")),\n                    ('encode', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(remainder='passthrough',\n    transformers=[\n        ('num', numeric_transformer, numerical_features),\n        ('cat', categorical_transformer, categorical_features)],\n         sparse_threshold=0.0,)\n","6cc2c928":"data_processor = Pipeline(             \n                     steps=[('add_feature', AddFeature(additional=2)),\n                     ('drop_cols',DropCols()),\n                     ('pre_processing',preprocessor)])","91371ee6":"data_processor.fit(trainnoprice)","3b4cdc7d":"traindf = data_processor.transform(trainnoprice)","217d6cb5":"traindf = pd.DataFrame(traindf)","5bd02b7d":"testdf = data_processor.transform(test)","ff7db52e":"testdf[np.isnan(testdf)] = 0","b3f14937":"testdf = pd.DataFrame(testdf)","b1fe733f":"testdf.shape","5c16dc78":"def rmse(y, y_pred) -> float:\n    \"\"\"\n    Calculates rmse\n    \"\"\"\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef rmse_cv(model,X,y) -> np.array:\n    \"\"\"\n    Function for cross validation\n    \"\"\"\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse","3595123f":"n_train=train.shape[0]\ny= train.SalePrice\nyFinal = np.log(train.SalePrice)","cd988792":"xtrain, xval, ytrain, yval = train_test_split(traindf, yFinal, test_size=0.2,\n                                                    random_state=0)","9680846b":"lasso = Lasso(alpha=0.0005, random_state=1)","bfb4adaf":"lasso.fit(xtrain,ytrain)","f61ef872":"%time\nlasso_ypred = lasso.predict(xval)","0253c507":"lasso_rmse = rmse(yval,lasso_ypred)","c7b8342f":"lasso_rmse","840c221f":"gbr = (GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n                                     learning_rate=0.05,max_depth=5,max_features='sqrt',min_impurity_decrease=0.0,\n                                     min_samples_leaf=10, min_samples_split=10,n_estimators=300,\n                                     random_state=None))\ngbr.fit(xtrain,ytrain)","71975669":"%time\ngbr_ypred = gbr.predict(xval)\ngbr_rmse = rmse(yval,gbr_ypred)","c7b573a5":"gbr_rmse","d22aa319":"xgb=XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                        learning_rate=0.05, max_depth=3, \n                        min_child_weight=1.7817, n_estimators=1500,\n                        reg_alpha=0.4640, reg_lambda=0.8571,\n                        subsample=0.5213,\n                        random_state =7, nthread = -1)\n\nxgb.fit(xtrain,ytrain)","4ab726d2":"%time\nxgr_ypred = xgb.predict(xval)\nxgb_rmse = rmse(yval,xgr_ypred)","69896f54":"xgb_rmse","5f8bc73b":"testFinal = testdf.copy()","ca94f8d0":"# SUBMISSION\nX_test = testFinal.copy()\nsub = pd.read_csv(\"sample_submission.csv\")\n%time\npred = np.exp(XGR.predict(testFinal))\nsub.SalePrice=pred\nsub.to_csv(\"submission_xgb.csv\",index=False)","570e562a":"model_ = ['Lasso','Gradient Boost','XGBoost']\nscores = [lasso_rmse,gbr_rmse,xgb_rmse]\nrun_times = [0.00787,0.00596,0.062]","cb8a0310":"kaggle_scores = [0.130,0.129,0.127]","cdda2349":"pd.DataFrame({'model':model_,\n             'score': scores,\n             'runtime(s)':run_times,\n             'submission_score':kaggle_scores})","c615f8b1":"# LASSO\nlasso_pred = np.exp(lasso.predict(testFinal))\n\n#GBR\ngbr_pred = np.exp(gbr.predict(testFinal))\n\n# XGBOOST\nxgb_pred = np.exp(xgb.predict(testFinal))\n\n\n# merging\nfinal_pred = 0.1*lasso_pred + 0.25*gbr_pred + 0.65*xgb_pred\nsub = pd.read_csv(\"sample_submission.csv\")\nsub.SalePrice = final_pred\n\n# CSV File(Kaggle Format)\nsub.to_csv('pred_final.csv', index=False)","203f8cb5":"### Univariate analysis","ec171ea1":"- Lotfrontage, LotArea,BsmtFullBath, 1stFloorSF, GrLiveArea and SalePrice are positively skewed.\n- TotalBsmtSF,GarageYrBlt are negatively skewed.","8f706dcf":"####  Final Submission\nWe can optimize for best score by using a ratio of the models, to get best score.","ed47941b":"### GBR","704b0d49":"There is a strong linear relationship between Sales Price and total square basement\n\nLet's see SalePrice relationship with some categorical variables","17f4ad4a":"### Objectives\n- Understand the dataset\n- Build pipelines for preprocessing\n- Test performance of model inference","2d86f37e":"### Bivariate analysis","cbd795ea":"### Xtreme boost","76c7f467":"- The earliest year sold of a house is 2006 and the latest year sold is 2010.\n- The cheapest house sold at 34900 and the most expensive is 755000.\n- The latest house was also built in 2010\n- The house prices (min and max) seem realistic.","4a72f6e1":"### Model\n\n### Lasso - Baseline Model","98a18424":"Manually, grouped the variables into objects and numerical varibles, to help me understand the columns better and also avoid errors that might come up as a result of a varible seen as a numerical because it was encoded ","b01fefe1":"#### Conclusion\nAfter submission and evaluation, the best score on the leaderboard is a score of **0.124**, which was gotten by merging the 3 models.\n\nFuture improvements\n- Feature crossing can be tried to see if the model performs better\n- The model can be optimized better to reduce time \n- Iterative process can be done continuosly to improve model\n- More features can be generated from base features, such as age of building etc.\n\nLimitations\n- I had limited submissions on kaggle (10 per day) and since, I had to submit this work at the earliest time possible, I could not wait to get more submissions and improve on this. \n\nHowever, I believe with more time, I could get improved scores.","d3d9b7eb":"Since there is a positive relationship between the Overall Quality and the SalePrice, the above plot shos that the two varible grow together.","2c7b6591":"From the above plot, the Living area and Sale Price have roughly a linear relationship. The two dots on the top indicate a sudden price increase. It may be because the house is located in a hot spot.\n\nAlso the last 2 points that do no follow the linear rule could be an indication of not so good land conditions. This points can be seen as outliers and should be removed before it is being fed into the model.\n\nWhat can we say about it's relationship with the size of basement?","6921e425":"### Submission","56614213":"No difference in train and test columns,except the SalePrice, which is normal. We will now explore our data","12b1c653":"## Exploratory Data Analysis","a7facd30":"### Handle missing values","afb6a7d6":"The minimum value is more than zero, hence that shows the price is a valid.\nAlso, the graph is a skewed one, it is positively skewed, hence is not a normal distribution. We will have to transform the variable to look like a normal distribution.\n\nWe will also check its relationship with other numerical variables","e50730d3":"### Pipeline","4d2c189b":"There is a good relationship between the overall quality and sales price","abc20945":"- We can see that there is a relationship between the SalePrice and the Overall Quality of the house.\n- Year built and the Sale Price are also correlated.","eba356eb":"### Feature Engineering","5576aeea":"## Introduction\nWe will be predicting prices with ames dataset and will be performing EDA on the dataset as well as modeling the data to predict house saleprice.\n\nWe will also be using pipelines to ensure smooth flow of code","8827ebd6":"Using Pearson, we can see variables with close positive and negative relationship to the target variables."}}