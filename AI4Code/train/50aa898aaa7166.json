{"cell_type":{"95590f82":"code","52ac453c":"code","2765919a":"code","f862a63e":"code","631e1ac1":"code","810923ea":"code","c17fdf7b":"code","1d0bc8f7":"code","c9c7ca97":"code","ee4f1366":"code","5794d9b6":"code","b236a330":"code","1a672abb":"code","e8aaffdf":"code","c0cbfae5":"code","b3d6012a":"code","1561099b":"code","33c634a4":"code","4259ccb2":"code","d4b389ba":"code","02eb7421":"code","44f36b5b":"code","d0652460":"code","5df7d06c":"code","9679f8f6":"code","a26f47ff":"code","107b65d3":"code","a49d8b5e":"code","80b2ca7d":"code","f09e8354":"code","f773b8e9":"code","7adafefc":"code","1f9159b9":"code","c864aaea":"code","187a11f3":"code","76fbb484":"code","d8de7b4f":"code","528b7fff":"code","f05525e6":"code","b487049f":"code","fc990827":"code","4acca251":"code","2beee545":"code","dedfc40c":"code","9f283899":"code","a9661f76":"code","d73d5110":"code","8fa3bc8e":"code","92558c5a":"code","2ec9b5b1":"code","031895b2":"code","2442dc60":"code","e6f778d8":"code","aae0b8fa":"code","fbb96b89":"code","98f152d5":"code","bb189867":"code","fc5c1303":"code","13e2fec3":"code","71ebed3a":"code","0bfc7b96":"code","8b128f8f":"code","5f20685f":"code","2b635e31":"code","448de93a":"code","61574376":"code","cc9dd812":"code","9deebaf1":"code","65f20e37":"code","eb553378":"code","5589719c":"code","f18f6e60":"code","e0a544eb":"code","22a2f2ad":"code","a1e3df47":"code","7682e72f":"code","ca4d6e85":"code","55d814a6":"code","7ef48c18":"code","1e994d5b":"code","b2cfe28c":"code","216c2cf7":"code","844c090e":"code","282cda41":"code","4a4b71d9":"code","7ac9d22f":"code","5f2e54c8":"code","3c1dac38":"code","dcf06abc":"code","0cd34328":"code","6a107be2":"code","61da310f":"code","65960248":"code","db7bc49e":"code","eb8a7102":"code","bdc16bd2":"code","04cac0c2":"code","c877de34":"code","90b2aca4":"code","d01e3225":"code","1abf3584":"code","a2cdd05d":"code","f8068913":"code","8069e0db":"code","841e2e40":"code","b51e8af5":"code","b64fe6ec":"code","60618876":"code","72ec76c8":"code","ce0a03b7":"code","1de9f359":"code","757e64aa":"code","eca9cf47":"code","2c8b858d":"code","b3c1893f":"code","55770c73":"code","8c6605fc":"code","a4ba6a63":"code","71aaf99d":"code","374ebbd9":"code","a180c544":"code","70040d1f":"code","7f3b8a63":"code","8fbdf7ba":"code","3bc91a15":"code","0dfebbb5":"code","474711fc":"code","71da36b8":"code","df369d03":"code","4777b9f1":"code","48c91aff":"code","6248e4fd":"code","6d1c9683":"code","493c6180":"code","90e8dd4e":"code","c5dbe0d9":"markdown","aa9d45dc":"markdown","a1b74f62":"markdown","81bcb304":"markdown","6782be2c":"markdown","f930bafe":"markdown","82d2e72d":"markdown","327c36f6":"markdown","7fb75484":"markdown","1a723bc1":"markdown","0c6a3b00":"markdown","cb2b7f51":"markdown","48947cbc":"markdown","0da8eb52":"markdown","ddec8bef":"markdown","0072cf0f":"markdown","5135bbc2":"markdown","911d7395":"markdown","010bd067":"markdown","d0b01d15":"markdown","9128c219":"markdown","3c38daa3":"markdown","c1688808":"markdown","d7ac8f15":"markdown","d3ab4aa7":"markdown","911f3434":"markdown","b6f19eea":"markdown","bd5fb638":"markdown","793e8d68":"markdown","e0a21bf0":"markdown","db45cd5f":"markdown","95de9dc7":"markdown","9d85ebf5":"markdown","ffe5f3b1":"markdown","92def2bb":"markdown","03458a73":"markdown","51f2eff1":"markdown","72e25826":"markdown","92286a7d":"markdown","53c9b5c4":"markdown","432b43e8":"markdown","850ffbd1":"markdown","7d269ab6":"markdown","9973f476":"markdown","1d10fc34":"markdown","440ce59d":"markdown","4f19f627":"markdown","b0466d52":"markdown","953408b0":"markdown","c880c080":"markdown","7f283c22":"markdown","26de9e2a":"markdown","195cb067":"markdown","c98c079f":"markdown","ffceabd8":"markdown","5073faae":"markdown","0bbac362":"markdown","3db7e06b":"markdown","7b4d7857":"markdown","ed15fa2a":"markdown","265491ad":"markdown","c363d208":"markdown","752ab788":"markdown","9230c2c0":"markdown","9bc7e943":"markdown","52372d73":"markdown","607cf2d2":"markdown","1cd5c753":"markdown","3458eacd":"markdown","e4bf8c4d":"markdown","f0ce1c42":"markdown","980d75a1":"markdown","5298868d":"markdown","42150bf5":"markdown","95d1ec73":"markdown","94853483":"markdown","f0699906":"markdown","a544c961":"markdown","1a429e3f":"markdown","4a1f9e56":"markdown","97374b4e":"markdown","ae7498be":"markdown","4cab1c1f":"markdown","2cea7575":"markdown","da4b9104":"markdown","15c1166f":"markdown","51e80ff9":"markdown","8ea315e0":"markdown","98f5b205":"markdown","d150910c":"markdown","e1255876":"markdown","ad6617a5":"markdown","90a6825b":"markdown","9829a4f9":"markdown","6f898f10":"markdown","ef16d648":"markdown","6e06977d":"markdown","fd429a2c":"markdown","49dd4d91":"markdown","b50e9f00":"markdown","200b4baa":"markdown","4c7084dd":"markdown","d8603c6a":"markdown","0f519e70":"markdown","a041b7c2":"markdown","3ee5b7ab":"markdown","420b2908":"markdown","09f39b85":"markdown","55604f27":"markdown","e4b0e6d2":"markdown","1a9ccfad":"markdown","10c89f69":"markdown","18da8b3b":"markdown","337ab4d6":"markdown","bfab272e":"markdown","f37a4e08":"markdown","f6e0bc65":"markdown","27a2b713":"markdown","83ef285a":"markdown","d7764812":"markdown","f2e4dad4":"markdown","681c3b0d":"markdown","b475075e":"markdown","2b581ce8":"markdown","124b2e95":"markdown"},"source":{"95590f82":"import numpy as np\nnp.random.seed(7)","52ac453c":"import tensorflow as tf\ntf.__version__","2765919a":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\nimport random, re\nimport time\n\n# used to supress display of warnings\nimport warnings\n\nimport missingno as mno\n\n# nlp libraries\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport holoviews as hv\nfrom holoviews import opts\n\nimport os;\nfrom os import makedirs\n\n# sampling methods\nfrom sklearn.utils import resample\nfrom imblearn.over_sampling import SMOTE\n\n# import zscore for scaling the data\nfrom scipy.stats import zscore\n\nfrom scipy.stats import randint as sp_randint\n\n# save models\nimport pickle\n\n# pre-processing methods\nfrom sklearn.model_selection import train_test_split\n\n# the classification models \nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import Lasso\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# ensemble models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\n# methods and classes for evaluation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score, roc_auc_score,log_loss\n\n# cross-validation methods\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# feature selection methods\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\n\n# pre-processing methods\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Deep learning libraries\nfrom keras.utils import np_utils\nfrom keras.utils import plot_model\nfrom keras.layers import Input\nfrom keras.layers.merge import Concatenate\nfrom keras.optimizers import SGD\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import optimizers\nfrom keras.models import Model\nfrom tensorflow.keras.layers import Flatten, Activation, Dense, LSTM, BatchNormalization, Embedding, Dropout, Flatten, Bidirectional, GlobalMaxPool1D\nfrom keras.models import model_from_json\nfrom keras.regularizers import l1, l2, l1_l2\nfrom keras.constraints import maxnorm, min_max_norm\nfrom keras.constraints import unit_norm\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import model_from_json\n\nfrom keras.models import load_model\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# Keras pre-processing\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","f862a63e":"os.environ['HV_DOC_HTML'] = 'true'\n\ndef _render(self, **kw):\n  hv.extension('bokeh')\n  return hv.Store.render(self)\nhv.core.Dimensioned._repr_mimebundle_ = _render","631e1ac1":"#PYTHONHASHSEED allows you to set a fixed value for the hash seed secret.\n\n\nos.environ['PYTHONHASHSEED']=str(7)\n\n# Reproduce the results\ndef reset_random_seeds():\n   os.environ['PYTHONHASHSEED']=str(7)\n   #np.random.seed(7)\n   #random.seed(7)\n   tf.random.set_seed(7)\n\n#random_state = 42\n#np.random.seed(random_state)\n#tf.random.set_seed(random_state)\n\n!rm -R log\/","810923ea":"# suppress display of warnings\nwarnings.filterwarnings('ignore')\n\n# display all dataframe columns\npd.options.display.max_columns = None\n\n# to set the limit to 3 decimals\npd.options.display.float_format = '{:.7f}'.format\n\n# display all dataframe rows\npd.options.display.max_rows = None","c17fdf7b":"# Read IHMStefanini_industrial_safety_and_health_database_with_accidents_description.csv file, that is imported into kagge Input\nindustry_df = pd.read_csv(\"..\/input\/industrial-safety-and-health-analytics-database\/IHMStefanini_industrial_safety_and_health_database_with_accidents_description.csv\")","1d0bc8f7":"# Get the top 5 rows\ndisplay(industry_df.head())","c9c7ca97":"print(\"Number of rows = {0} and Number of Columns = {1} in the Data frame\".format(industry_df.shape[0], industry_df.shape[1]))","ee4f1366":"# Check datatypes\nindustry_df.dtypes","5794d9b6":"# Check Data frame info\nindustry_df.info()","b236a330":"# Column names of Data frame\nindustry_df.columns","1a672abb":"# Remove 'Unnamed: 0' column from Data frame\nindustry_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n\n# Rename 'Data', 'Countries', 'Genre', 'Employee or Third Party' columns in Data frame\nindustry_df.rename(columns={'Data':'Date', 'Countries':'Country', 'Genre':'Gender', 'Employee or Third Party':'Employee type'}, inplace=True)\n\n# Get the top 2 rows\nindustry_df.head(2)","e8aaffdf":"# Check duplicates in a data frame\nindustry_df.duplicated().sum()","c0cbfae5":"# View the duplicate records\nduplicates = industry_df.duplicated()\n\nindustry_df[duplicates]","b3d6012a":"# Delete duplicate rows\nindustry_df.drop_duplicates(inplace=True)","1561099b":"# Get the shape of Industry data\nindustry_df.shape","33c634a4":"print(\"Number of rows = {0} and Number of Columns = {1} in the Data frame after removing the duplicates.\".format(industry_df.shape[0], industry_df.shape[1]))","4259ccb2":"# Check unique values of all columns except 'Description' column\nfor x in industry_df.columns:\n    if x != 'Description':\n      print('--'*30); print(f'Unique values of \"{x}\" column'); print('--'*30)\n      print(industry_df[x].unique())\n      print('\\n')","d4b389ba":"# Check the presence of missing values\nindustry_df.isnull().sum()","02eb7421":"# Visualize missing values\nmno.matrix(industry_df, figsize = (20, 6));","44f36b5b":"industry_df['Date'] = pd.to_datetime(industry_df['Date'])\n\nindustry_df['Year'] = industry_df.Date.apply(lambda x : x.year)\nindustry_df['Month'] = industry_df.Date.apply(lambda x : x.month)\nindustry_df['Day'] = industry_df.Date.apply(lambda x : x.day)\nindustry_df['Weekday'] = industry_df.Date.apply(lambda x : x.day_name())\nindustry_df['WeekofYear'] = industry_df.Date.apply(lambda x : x.weekofyear)\n\nindustry_df.head()","d0652460":"# function to create month variable into seasons\ndef month2seasons(x):\n    if x in [9, 10, 11]:\n        season = 'Spring'\n    elif x in [12, 1, 2]:\n        season = 'Summer'\n    elif x in [3, 4, 5]:\n        season = 'Autumn'\n    elif x in [6, 7, 8]:\n        season = 'Winter'\n    return season","5df7d06c":"industry_df['Season'] = industry_df['Month'].apply(month2seasons)\nindustry_df.head(3)","9679f8f6":"import holidays\n\nbrazil_holidays = []\n\nprint('--'*40); print('List of Brazil holidays in 2016'); print('--'*40)\nfor date in holidays.Brazil(years = 2016).items():\n    brazil_holidays.append(str(date[0]))\n    print(date)\n\nprint('--'*40); print('List of Brazil holidays in 2017'); print('--'*40)\nfor date in holidays.Brazil(years = 2017).items():\n    brazil_holidays.append(str(date[0]))\n    print(date)","a26f47ff":"industry_df['Is_Holiday'] = [1 if str(val).split()[0] in brazil_holidays else 0 for val in industry_df['Date']]\nindustry_df.head(3)","107b65d3":"print('--'*30); print('Value Counts for `Country` label'); print('--'*30)\n\ntotal_row_cnt = industry_df.shape[0]\ncountry_01_cnt = industry_df[industry_df.Country == 'Country_01'].shape[0]\ncountry_02_cnt = industry_df[industry_df.Country == 'Country_02'].shape[0]\ncountry_03_cnt = industry_df[industry_df.Country == 'Country_03'].shape[0]\n\nprint(f'Country_01 count: {country_01_cnt} i.e. {round(country_01_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Country_02 count: {country_02_cnt} i.e. {round(country_02_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Country_03 count: {country_03_cnt} i.e. {round(country_03_cnt\/total_row_cnt*100, 0)}%')\n\nprint('--'*30); print('Distributon of `Country` label'); print('--'*30)\n_ = industry_df['Country'].value_counts().plot(kind = 'pie', autopct = '%.0f%%', labels = ['Country_01', 'Country_02', 'Country_03'], figsize = (10, 6))","a49d8b5e":"local_cnt = np.round(industry_df['Local'].value_counts(normalize=True) * 100)\n\nhv.extension('bokeh')\nhv.Bars(local_cnt).opts(title=\"Local Count\", color=\"#8888ff\", xlabel=\"Locals\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=700, height=300,tools=['hover'],show_grid=True))","80b2ca7d":"print('--'*30); print('Value Counts for `Industry Sector` label'); print('--'*30)\n\nMining_cnt = industry_df[industry_df['Industry Sector'] == 'Mining'].shape[0]\nMetals_cnt = industry_df[industry_df['Industry Sector'] == 'Metals'].shape[0]\nOthers_cnt = industry_df[industry_df['Industry Sector'] == 'Others'].shape[0]\n\nprint(f'Mining count: {Mining_cnt} i.e. {round(Mining_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Metals count: {Metals_cnt} i.e. {round(Metals_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Others count: {Others_cnt} i.e. {round(Others_cnt\/total_row_cnt*100, 0)}%')\n\nprint('--'*30); print('Distributon of `Industry Sector` label'); print('--'*30)\n\nsector_cnt = np.round(industry_df['Industry Sector'].value_counts(normalize=True) * 100)\n\nhv.Bars(sector_cnt).opts(title=\"Industry Sector Count\", color=\"#8888ff\", xlabel=\"Sectors\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))\\\n                * hv.Text('Mining', 15, f\"{int(sector_cnt.loc['Mining'])}%\")\\\n                * hv.Text('Metals', 15, f\"{int(sector_cnt.loc['Metals'])}%\")\\\n                * hv.Text('Others', 15, f\"{int(sector_cnt.loc['Others'])}%\")","f09e8354":"print('--'*30); print('Value Counts for `Accident Level` label'); print('--'*40)\n\nI_acc_cnt = industry_df[industry_df['Accident Level'] == 'I'].shape[0]\nII_acc_cnt = industry_df[industry_df['Accident Level'] == 'II'].shape[0]\nIII_acc_cnt = industry_df[industry_df['Accident Level'] == 'III'].shape[0]\nIV_acc_cnt = industry_df[industry_df['Accident Level'] == 'IV'].shape[0]\nV_acc_cnt = industry_df[industry_df['Accident Level'] == 'V'].shape[0]\nVI_acc_cnt = industry_df[industry_df['Accident Level'] == 'VI'].shape[0]\n\nprint(f'Accident Level - I count: {I_acc_cnt} i.e. {round(I_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Accident Level - II count: {II_acc_cnt} i.e. {round(II_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Accident Level - III count: {III_acc_cnt} i.e. {round(III_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Accident Level - IV count: {IV_acc_cnt} i.e. {round(IV_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Accident Level - V count: {V_acc_cnt} i.e. {round(V_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Accident Level - VI count: {VI_acc_cnt} i.e. {round(VI_acc_cnt\/total_row_cnt*100, 0)}%')\n\nprint('--'*30); print('Value Counts for `Potential Accident Level'); print('--'*40)\n\nI_pot_acc_cnt = industry_df[industry_df['Potential Accident Level'] == 'I'].shape[0]\nII_pot_acc_cnt = industry_df[industry_df['Potential Accident Level'] == 'II'].shape[0]\nIII_pot_acc_cnt = industry_df[industry_df['Potential Accident Level'] == 'III'].shape[0]\nIV_pot_acc_cnt = industry_df[industry_df['Potential Accident Level'] == 'IV'].shape[0]\nV_pot_acc_cnt = industry_df[industry_df['Potential Accident Level'] == 'V'].shape[0]\nVI_pot_acc_cnt = industry_df[industry_df['Potential Accident Level'] == 'VI'].shape[0]\n\nprint(f'Potential Accident Level - I count: {I_pot_acc_cnt} i.e. {round(I_pot_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Potential Accident Level - II count: {II_pot_acc_cnt} i.e. {round(II_pot_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Potential Accident Level - III count: {III_pot_acc_cnt} i.e. {round(III_pot_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Potential Accident Level - IV count: {IV_pot_acc_cnt} i.e. {round(IV_pot_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Potential Accident Level - V count: {V_pot_acc_cnt} i.e. {round(V_pot_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Potential Accident Level - VI count: {VI_pot_acc_cnt} i.e. {round(VI_pot_acc_cnt\/total_row_cnt*100, 0)}%')\n\nprint('--'*30); print('Distributon of `Accident Level` & `Potential Accident Level` label'); print('--'*40)\n\nac_level_cnt = np.round(industry_df['Accident Level'].value_counts(normalize=True) * 100)\npot_ac_level_cnt = np.round(industry_df['Potential Accident Level'].value_counts(normalize=True) * 100, decimals=1)\nac_pot = pd.concat([ac_level_cnt, pot_ac_level_cnt], axis=1,sort=False).fillna(0).rename(columns={'Accident Level':'Accident', 'Potential Accident Level':'Potential'})\nac_pot = pd.melt(ac_pot.reset_index(), ['index']).rename(columns={'index':'Severity', 'variable':'Levels'})\n\nhv.Bars(ac_pot, ['Severity', 'Levels'], 'value').opts(opts.Bars(title=\"Accident Levels Count\", width=700, height=300,tools=['hover'],\\\n                                                                show_grid=True,xrotation=45, ylabel=\"Percentage\", yformatter='%d%%'))","f773b8e9":"print('--'*30); print('Value Counts for `Gender` label'); print('--'*30)\n\nMale_cnt = industry_df[industry_df['Gender'] == 'Male'].shape[0]\nFemale_cnt = industry_df[industry_df['Gender'] == 'Female'].shape[0]\n\nprint(f'Male count: {Male_cnt} i.e. {round(Male_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Female count: {Female_cnt} i.e. {round(Female_cnt\/total_row_cnt*100, 0)}%')\n\nprint('--'*30); print('Distributon of `Gender` label'); print('--'*30)\n\ngender_cnt = np.round(industry_df['Gender'].value_counts(normalize=True) * 100)\n\nhv.Bars(gender_cnt).opts(title=\"Gender Count\", color=\"#8888ff\", xlabel=\"Gender\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))","7adafefc":"print('--'*30); print('Value Counts for `Employee type` label'); print('--'*30)\n\nthird_party_cnt = industry_df[industry_df['Employee type'] == 'Third Party'].shape[0]\nemp_cnt = industry_df[industry_df['Employee type'] == 'Employee'].shape[0]\nthird_rem_cnt = industry_df[industry_df['Employee type'] == 'Third Party (Remote)'].shape[0]\n\nprint(f'Third Party count: {third_party_cnt} i.e. {round(third_party_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Employee count: {emp_cnt} i.e. {round(emp_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Third Party (Remote) count: {third_rem_cnt} i.e. {round(third_rem_cnt\/total_row_cnt*100, 0)}%')\n\nprint('--'*30); print('Distributon of `Employee type` label'); print('--'*30)\n\nemp_type_cnt = np.round(industry_df['Employee type'].value_counts(normalize=True) * 100)\n\nhv.Bars(emp_type_cnt).opts(title=\"Employee type Count\", color=\"#8888ff\", xlabel=\"Employee Type\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))","1f9159b9":"cr_risk_cnt = np.round(industry_df['Critical Risk'].value_counts(normalize=True) * 100)\n\nhv.Bars(cr_risk_cnt[::-1]).opts(title=\"Critical Risk Count\", color=\"#8888ff\", xlabel=\"Critical Risks\", ylabel=\"Percentage\", xformatter='%d%%')\\\n                .opts(opts.Bars(width=600, height=600,tools=['hover'],show_grid=True,invert_axes=True))","c864aaea":"year_cnt = np.round(industry_df['Year'].value_counts(normalize=True,sort=False) * 100)\nyear = hv.Bars(year_cnt).opts(title=\"Year Count\", color=\"yellow\", xlabel=\"Years\")\n\nmonth_cnt = np.round(industry_df['Month'].value_counts(normalize=True,sort=False) * 100)\nmonth = hv.Bars(month_cnt).opts(title=\"Month Count\", color=\"#8888ff\", xlabel=\"Months\") * hv.Curve(month_cnt).opts(color='red', line_width=3)\n\n(year).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True, ylabel=\"Percentage\", yformatter='%d%%'))\n","187a11f3":"(month).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True, ylabel=\"Percentage\", yformatter='%d%%'))\n","76fbb484":"day_cnt = np.round(industry_df['Day'].value_counts(normalize=True,sort=False) * 100)\nhv.Bars(day_cnt).opts(title=\"Day Count\", color=\"#8888ff\", xlabel=\"Days\") * hv.Curve(day_cnt).opts(width=500, height=300, color='red', line_width=3)","d8de7b4f":"f = lambda x : np.round(x\/x.sum() * 100)\nem_gen = industry_df.groupby(['Gender','Industry Sector'])['Industry Sector'].count().unstack().apply(f, axis=1)\n\nhv.Bars(pd.melt(em_gen.reset_index(), ['Gender']), ['Gender','Industry Sector'], 'value').opts(opts.Bars(title=\"Industry Sector by Gender Count\", width=800, height=300,tools=['hover'],\\\n                                                                show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%'))","528b7fff":"f = lambda x : np.round(x\/x.sum() * 100)\n\nac_gen = industry_df.groupby(['Gender','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1)\nac = hv.Bars(pd.melt(ac_gen.reset_index(), ['Gender']), ['Gender','Accident Level'], 'value').opts(opts.Bars(title=\"Accident Level by Gender Count\"))\n\npot_ac_gen = industry_df.groupby(['Gender','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=1)\npot_ac = hv.Bars(pd.melt(pot_ac_gen.reset_index(), ['Gender']), ['Gender','Potential Accident Level'], 'value').opts(opts.Bars(title=\"Potential Accident Level by Gender Count\"))\n\n(ac ).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%'))","f05525e6":"( pot_ac).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%'))","b487049f":"f = lambda x : np.round(x\/x.sum() * 100)\n\nac_em = industry_df.groupby(['Employee type','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1)\nac = hv.Bars(pd.melt(ac_em.reset_index(), ['Employee type']), ['Employee type','Accident Level'], 'value').opts(opts.Bars(title=\"Accident Level by Employee type Count\"))\n\npot_ac_em = industry_df.groupby(['Employee type','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=1)\npot_ac = hv.Bars(pd.melt(pot_ac_em.reset_index(), ['Employee type']), ['Employee type','Potential Accident Level'], 'value').opts(opts.Bars(title=\"Potential Accident Level by Employee type Count\"))\n\n(ac ).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%',fontsize={'title':9}))","fc990827":"( pot_ac).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%',fontsize={'title':9}))","4acca251":"f = lambda x : np.round(x\/x.sum() * 100)\n\nac_mo = industry_df.groupby(['Month','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1).fillna(0)\nac = hv.Curve(ac_mo['I'], label='I') * hv.Curve(ac_mo['II'], label='II') * hv.Curve(ac_mo['III'], label='III') * hv.Curve(ac_mo['IV'], label='IV') * hv.Curve(ac_mo['V'], label='V')\\\n        .opts(opts.Curve(title=\"Accident Level by Month Count\"))\n\npot_ac_mo = industry_df.groupby(['Month','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=1).fillna(0)\npot_ac = hv.Curve(pot_ac_mo['I'], label='I') * hv.Curve(pot_ac_mo['II'], label='II') * hv.Curve(pot_ac_mo['III'], label='III') * hv.Curve(pot_ac_mo['IV'], label='IV')\\\n        * hv.Curve(pot_ac_mo['V'], label='V') * hv.Curve(pot_ac_mo['VI'], label='VI').opts(opts.Curve(title=\"Potential Accident Level by Month Count\"))\n        \n(ac+pot_ac).opts(opts.Curve(width=800, height=300,tools=['hover'],show_grid=True, ylabel=\"Percentage\", yformatter='%d%%')).cols(1)","2beee545":"# Summary statistics\nindustry_df.drop(columns='Description').describe(exclude=[np.number]).T","dedfc40c":"# Check the Correlation\nindustry_df.corr()","9f283899":"# Text preprocessing and stopwords\nfrom text_preprocess_py import * #(custom module)","a9661f76":"print('--'*30); print('Converting description to lower case')\nindustry_df['Cleaned_Description'] = industry_df['Description'].apply(lambda x : x.lower())\n\nprint('Replacing apostrophes to the standard lexicons')\nindustry_df['Cleaned_Description'] = industry_df['Cleaned_Description'].apply(lambda x : replace_words(x))\n\nprint('Removing punctuations')\nindustry_df['Cleaned_Description'] = industry_df['Cleaned_Description'].apply(lambda x: remove_punctuation(x))\n\nprint('Applying Lemmatizer')\nindustry_df['Cleaned_Description'] = industry_df['Cleaned_Description'].apply(lambda x: lemmatize(x))\n\nprint('Removing multiple spaces between words')\nindustry_df['Cleaned_Description'] = industry_df['Cleaned_Description'].apply(lambda x: re.sub(' +', ' ', x))\n\nprint('Removing stop words')\nindustry_df['Cleaned_Description'] = industry_df['Cleaned_Description'].apply(lambda x: remove_stopwords(x))\n\nprint('--'*30)","d73d5110":"print('--'*45); print('Get the length of each line, find the maximum length and print the maximum length line'); \nprint('Length of line ranges from 64 to 672.'); print('--'*45)\n\n# Get length of each line\nindustry_df['line_length'] = industry_df['Cleaned_Description'].str.len()\n\nprint('Minimum line length: {}'.format(industry_df['line_length'].min()))\nprint('Maximum line length: {}'.format(industry_df['line_length'].max()))\nprint('Line with maximum length: {}'.format(industry_df[industry_df['line_length'] == industry_df['line_length'].max()]['Cleaned_Description'].values[0]))","8fa3bc8e":"print('--'*45); print('Get the number of words, find the maximum number of words and print the maximum number of words'); \nprint('Number of words ranges from 10 to 98.'); print('--'*45)\n\n# Get length of each line\nindustry_df['nb_words'] = industry_df['Cleaned_Description'].apply(lambda x: len(x.split(' ')))\n\nprint('Minimum number of words: {}'.format(industry_df['nb_words'].min()))\nprint('Maximum number of words: {}'.format(industry_df['nb_words'].max()))\nprint('Line with maximum number of words: {}'.format(industry_df[industry_df['nb_words'] == industry_df['nb_words'].max()]['Cleaned_Description'].values[0]))","92558c5a":"wordcloud = WordCloud(width = 1500, height = 800, random_state=0, background_color='black', colormap='rainbow', \\\n                      min_font_size=5, max_words=300, collocations=False).generate(\" \".join(industry_df['Cleaned_Description'].values))\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","2ec9b5b1":"print('--'*30); print('Five point summary for number of words')\ndisplay(industry_df['nb_words'].describe().round(0).astype(int)); \n\nprint('99% quantilie: {}'.format(industry_df['nb_words'].quantile(0.99)));print('--'*30)","031895b2":"#To create WOrd to Vector Embeddings\nfrom gensim.models import Word2Vec\n# define training data\nsentences = industry_df['Cleaned_Description']\n\n# train model\nmodel = Word2Vec(sentences, min_count=1)\n\n# summarize the loaded model\nprint(model)\n\n# summarize vocabulary\nwords = list(model.wv.index_to_key)\nprint(words)\n\n# save model\nmodel.save('model.bin')\n\n# load model\nnew_model = Word2Vec.load('model.bin')\nprint(new_model)","2442dc60":"#Using glove embedding from 200d file, which is imported locally into kaggle.\nembeddings_index = {}\nEMBEDDING_FILE = '..\/input\/glove6b200d\/glove.6B.200d.txt'\nf = open(EMBEDDING_FILE)\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","e6f778d8":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v \/ np.sqrt((v ** 2).sum())","aae0b8fa":"# create sentence GLOVE embeddings vectors using the above function for training and validation set\nind_glove_df = [sent2vec(x) for x in tqdm(industry_df['Cleaned_Description'])]","fbb96b89":"ind_tfidf_df = pd.DataFrame()\nfor i in [1,2,3]:\n    vec_tfidf = TfidfVectorizer(max_features=10, norm='l2', stop_words='english', lowercase=True, use_idf=True, ngram_range=(i,i))\n    X = vec_tfidf.fit_transform(industry_df['Cleaned_Description']).toarray()\n    tfs = pd.DataFrame(X, columns=[\"TFIDF_\" + n for n in vec_tfidf.get_feature_names()])\n    ind_tfidf_df = pd.concat([ind_tfidf_df.reset_index(drop=True), tfs.reset_index(drop=True)], axis=1)\n\nind_tfidf_df.head(3)","98f152d5":"# To replace white space everywhere in Employee type\nindustry_df['Employee type'] = industry_df['Employee type'].str.replace(' ', '_')\nindustry_df['Employee type'].value_counts()","bb189867":"# To replace white space everywhere in Critical Risk\nindustry_df['Critical Risk'] = industry_df['Critical Risk'].str.replace('\\n', '').str.replace(' ', '_')\nindustry_df['Critical Risk'].value_counts().head()","fc5c1303":"# Create Industry DataFrame\nind_featenc_df = pd.DataFrame()\n\n# Label encoding\nindustry_df['Season'] = industry_df['Season'].replace('Summer', 'aSummer').replace('Autumn', 'bAutumn').replace('Winter', 'cWinter').replace('Spring', 'dSpring')\nind_featenc_df['Season'] = LabelEncoder().fit_transform(industry_df['Season']).astype(np.int8)\n\nindustry_df['Weekday'] = industry_df['Weekday'].replace('Monday', 'aMonday').replace('Tuesday', 'bTuesday').replace('Wednesday', 'cWednesday').replace('Thursday', 'dThursday').replace('Friday', 'eFriday').replace('Saturday', 'fSaturday').replace('Sunday', 'gSunday')\nind_featenc_df['Weekday'] = LabelEncoder().fit_transform(industry_df['Weekday']).astype(np.int8)\n\nind_featenc_df['Accident Level'] = LabelEncoder().fit_transform(industry_df['Accident Level']).astype(np.int8)\nind_featenc_df['Potential Accident Level'] = LabelEncoder().fit_transform(industry_df['Potential Accident Level']).astype(np.int8)","13e2fec3":"# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y = np_utils.to_categorical(ind_featenc_df['Accident Level'])\n","71ebed3a":"# Dummy variables encoding\nCountry_dummies = pd.get_dummies(industry_df['Country'], columns=[\"Country\"], drop_first=True)\nLocal_dummies = pd.get_dummies(industry_df['Local'], columns=[\"Local\"], drop_first=True)\nGender_dummies = pd.get_dummies(industry_df['Gender'], columns=[\"Gender\"], drop_first=True)\nIS_dummies = pd.get_dummies(industry_df['Industry Sector'], columns=['Industry Sector'], prefix='IS', drop_first=True)\nEmpType_dummies = pd.get_dummies(industry_df['Employee type'], columns=['Employee type'], prefix='EmpType', drop_first=True)\nCR_dummies = pd.get_dummies(industry_df['Critical Risk'], columns=['Critical Risk'], prefix='CR', drop_first=True)\n\n# Merge the above dataframe with the original dataframe ind_feat_df\nind_featenc_df = ind_featenc_df.join(Country_dummies.reset_index(drop=True)).join(Local_dummies.reset_index(drop=True)).join(Gender_dummies.reset_index(drop=True)).join(IS_dummies.reset_index(drop=True)).join(EmpType_dummies.reset_index(drop=True)).join(CR_dummies.reset_index(drop=True))\n\nind_featenc_df = industry_df[['Year','Month','Day','WeekofYear']].reset_index(drop=True).join(ind_featenc_df.reset_index(drop=True))\n\nind_featenc_df.head(3)","0bfc7b96":"# Check NaN values\nnp.any(np.isnan(ind_featenc_df))","8b128f8f":"# Consider only top 30 GLOVE features\nind_feat_df = ind_featenc_df.join(pd.DataFrame(ind_glove_df).iloc[:,0:30].reset_index(drop=True))","5f20685f":"ind_feat_df.head(3)","2b635e31":"# Consider only top 30 GLOVE features\nind_feat_df = ind_featenc_df.join(ind_tfidf_df.reset_index(drop=True))","448de93a":"ind_feat_df.head(3)","61574376":"X = ind_feat_df.drop(['Accident Level','Potential Accident Level'], axis = 1) # Considering all Predictors\ny = ind_feat_df['Accident Level']","cc9dd812":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 1, stratify = y)","9deebaf1":"X_train, X_test, y_train_dummy, y_test_dummy = train_test_split(X, dummy_y, test_size = 0.20, random_state = 1, stratify = y)","65f20e37":"print('X_train shape : ({0},{1})'.format(X_train.shape[0], X_train.shape[1]))\nprint('y_train shape : ({0},)'.format(y_train.shape[0]))\nprint('X_test shape : ({0},{1})'.format(X_test.shape[0], X_test.shape[1]))\nprint('y_test shape : ({0},)'.format(y_test.shape[0]))","eb553378":"sns.countplot(ind_feat_df['Accident Level'])","5589719c":"# Display old accident level counts\nind_feat_df['Accident Level'].value_counts()","f18f6e60":"# Concatenate our training data back together\nX_up = pd.concat([X_train, y_train], axis=1)\n\n# Get the majority and minority class\nacclevel_0_majority = X_up[X_up['Accident Level'] == 0]\nacclevel_1_minority = X_up[X_up['Accident Level'] == 1]\nacclevel_2_minority = X_up[X_up['Accident Level'] == 2]\nacclevel_3_minority = X_up[X_up['Accident Level'] == 3]\nacclevel_4_minority = X_up[X_up['Accident Level'] == 4]\n\n# Upsample Level1 minority class\nacclevel_1_minority_upsampled = resample(acclevel_1_minority,\n                                replace = True, # sample with replacement\n                                n_samples = len(acclevel_0_majority), # to match majority class\n                                random_state = 1)\n\n# Upsample Level2 minority class\nacclevel_2_minority_upsampled = resample(acclevel_2_minority,\n                                replace = True, # sample with replacement\n                                n_samples = len(acclevel_0_majority), # to match majority class\n                                random_state = 1)\n\n# Upsample Level3 minority class\nacclevel_3_minority_upsampled = resample(acclevel_3_minority,\n                                replace = True, # sample with replacement\n                                n_samples = len(acclevel_0_majority), # to match majority class\n                                random_state = 1)\n\n# Upsample Level4 minority class\nacclevel_4_minority_upsampled = resample(acclevel_4_minority,\n                                replace = True, # sample with replacement\n                                n_samples = len(acclevel_0_majority), # to match majority class\n                                random_state = 1)","e0a544eb":"# Combine majority class with upsampled minority classes\ndf_upsampled = pd.concat([acclevel_0_majority, acclevel_1_minority_upsampled, acclevel_2_minority_upsampled, acclevel_3_minority_upsampled, \n                          acclevel_4_minority_upsampled])","22a2f2ad":"# Display new accident level counts\ndf_upsampled['Accident Level'].value_counts()","a1e3df47":"# Separate input features and target\nX_train_up = df_upsampled.drop(['Accident Level'], axis = 1) # Considering all Predictors\ny_train_up = df_upsampled['Accident Level']","7682e72f":"sm = SMOTE(random_state=1)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\ndf_smote = pd.concat([pd.DataFrame(X_train_smote), pd.DataFrame(y_train_smote)], axis=1)\ndf_smote.columns = ['Year', 'Month', 'Day',\n        'WeekofYear', 'Season', 'Weekday',\n       'Country_02', 'Country_03', 'Local_02', 'Local_03', 'Local_04',\n       'Local_05', 'Local_06', 'Local_07', 'Local_08', 'Local_09', 'Local_10',\n       'Local_11', 'Local_12', 'Male', 'IS_Mining', 'IS_Others',\n       'EmpType_Third_Party', 'EmpType_Third_Party_(Remote)',\n       'CR_Blocking_and_isolation_of_energies', 'CR_Burn',\n       'CR_Chemical_substances', 'CR_Confined_space', 'CR_Cut',\n       'CR_Electrical_Shock', 'CR_Electrical_installation', 'CR_Fall',\n       'CR_Fall_prevention', 'CR_Fall_prevention_(same_level)',\n       'CR_Individual_protection_equipment', 'CR_Liquid_Metal',\n       'CR_Machine_Protection', 'CR_Manual_Tools', 'CR_Not_applicable',\n       'CR_Others', 'CR_Plates', 'CR_Poll', 'CR_Power_lock', 'CR_Pressed',\n       'CR_Pressurized_Systems',\n       'CR_Pressurized_Systems_\/_Chemical_Substances', 'CR_Projection',\n       'CR_Projection\/Burning', 'CR_Projection\/Choco',\n       'CR_Projection\/Manual_Tools', 'CR_Projection_of_fragments',\n       'CR_Suspended_Loads', 'CR_Traffic', 'CR_Vehicles_and_Mobile_Equipment',\n       'CR_Venomous_Animals', 'CR_remains_of_choco', 'TFIDF_activity', 'TFIDF_area',\n       'TFIDF_causing', 'TFIDF_employee', 'TFIDF_hand', 'TFIDF_injury',\n       'TFIDF_left', 'TFIDF_operator', 'TFIDF_right', 'TFIDF_time',\n       'TFIDF_causing injury', 'TFIDF_described injury',\n       'TFIDF_employee reports', 'TFIDF_finger left', 'TFIDF_injury described',\n       'TFIDF_left foot', 'TFIDF_left hand', 'TFIDF_medical center',\n       'TFIDF_right hand', 'TFIDF_time accident',\n       'TFIDF_causing injury described', 'TFIDF_described time accident',\n       'TFIDF_finger left hand', 'TFIDF_finger right hand',\n       'TFIDF_generating described injury', 'TFIDF_hand causing injury',\n       'TFIDF_injury time accident', 'TFIDF_left hand causing',\n       'TFIDF_right hand causing', 'TFIDF_time accident employee', 'Accident Level']","ca4d6e85":"# Separate input features and target\nX_train_smote = df_smote.iloc[:,:-1] # Considering all Predictors\ny_train_smote = df_smote.iloc[:,-1:]","55d814a6":"X_train_smote.head(1)","7ef48c18":"# Display new accident level counts\ny_train_smote['Accident Level'].value_counts()","1e994d5b":"# convert integers to dummy variables (i.e. one hot encoded)\ny_train_smote_dummy = np_utils.to_categorical(y_train_smote['Accident Level'])\ny_train_smote_dummy","b2cfe28c":"# Transform independent features\nscaler_X = StandardScaler()#StandardScaler()\npipeline = Pipeline(steps=[('s', scaler_X)])\nX_train.iloc[:,:6] = pipeline.fit_transform(X_train.iloc[:,:6]) # Scaling only first 6 feautres\n\nX_test.iloc[:,:6] = pipeline.fit_transform(X_test.iloc[:,:6]) # Scaling only first 6 feautres","216c2cf7":"X_train.head(3)","844c090e":"# generating the covariance matrix and the eigen values for the PCA analysis\ncov_matrix = np.cov(X_train.T) # the relevanat covariance matrix\n#print('Covariance Matrix \\n%s', cov_matrix)\n\n#generating the eigen values and the eigen vectors\ne_vals, e_vecs = np.linalg.eig(cov_matrix)\n#print('Eigenvectors \\n%s' %e_vecs)\n#print('\\nEigenvalues \\n%s' %e_vals)","282cda41":"# the \"cumulative variance explained\" analysis \ntot = sum(e_vals)\nvar_exp = [( i \/tot ) * 100 for i in sorted(e_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n#print(\"Cumulative Variance Explained\", cum_var_exp)","4a4b71d9":"# Plotting the variance expalained by the principal components and the cumulative variance explained.\nplt.figure(figsize=(20 , 5))\nplt.bar(range(1, e_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\nplt.step(range(1, e_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()","7ac9d22f":"# Capturing 90% variance of the data\npca = PCA(n_components = 0.90)\nX_train_reduced = pca.fit_transform(X_train)\nX_test_reduced = pca.transform(X_test)","5f2e54c8":"print(X_train_reduced.shape)\nprint(X_test_reduced.shape)","3c1dac38":"# DummyClassifier to predict all Accident levels\ndummy = DummyClassifier(strategy='stratified').fit(X_train, y_train)\ndummy_pred = dummy.predict(X_test)\n\n# checking unique labels\nprint('Unique predicted labels: ', (np.unique(dummy_pred)))\n\n# checking accuracy\nprint('Test score: ', accuracy_score(y_test, dummy_pred))","dcf06abc":"# Checking unique values\npredictions = pd.DataFrame(dummy_pred)\npredictions[0].value_counts()","0cd34328":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 \/ rows * vsota","6a107be2":"def train_test_model(model, method, X_train, X_test, y_train, y_test, of_type, index, scale, report, save_model):\n    \n    if report == \"yes\":\n        print (model)\n        print (\"***************************************************************************\")\n\n    if method == 'CatBoostClassifier' or method == 'LGBMClassifier':\n\n      model.fit(X_train, y_train) # Fit the model on Training set\n    else:\n      model.fit(X_train, y_train) # Fit the model on Training set\n\n    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score\n    \n    if of_type == \"coef\":\n        # Intercept and Coefficients\n        print(\"The intercept for our model is {}\".format(model.intercept_), \"\\n\")\n        \n        for idx, col_name in enumerate(X_train.columns):\n            print(\"The coefficient for {} is {}\".format(col_name, model.coef_.ravel()[idx]))\n\n    y_pred = model.predict(X_test) # Predict on Test set\n\n    # Initialise mc_logloss\n    mc_logloss = 1.00\n    if method != 'RidgeClassifier':\n      y_predictions = model.predict_proba(X_test)\n\n    train_accuracy_score = model.score(X_train, y_train)\n    test_accuracy_score = model.score(X_test, y_test)\n\n    precision_score = precision_score(y_test, y_pred, average='weighted')\n    recall_score = recall_score(y_test, y_pred, average='weighted')\n    f1_score = f1_score(y_test, y_pred, average='weighted')\n\n    if method != 'RidgeClassifier':\n      mc_logloss = multiclass_logloss(y_test, y_predictions, eps=1e-15)\n\n    if report == \"yes\":\n      # Model - Confusion matrix\n      model_cm = confusion_matrix(y_test, y_pred)\n\n      sns.heatmap(model_cm, annot=True,  fmt='.2f', xticklabels = [\"I\", \"II\", \"III\", \"IV\", \"V\"] , yticklabels = [\"I\", \"II\", \"III\", \"IV\", \"V\"] )\n      plt.ylabel('Actual')\n      plt.xlabel('Predicted')\n      plt.show()\n\n      # Model - Classification report\n      model_cr = classification_report(y_test, y_pred)\n      print(model_cr)\n\n    # Store the accuracy results for each model in a dataframe for final comparison\n    resultsDf = pd.DataFrame({'Method': method, 'Train Accuracy': train_accuracy_score, 'Test Accuracy': test_accuracy_score, \n                              'Precision': precision_score, 'Recall': recall_score, 'F1-Score': f1_score, \n                              'Multi-Class Logloss': mc_logloss}, index=[index])\n    \n    # Save the model\n    if save_model == \"yes\":\n      filename = 'finalised_model.sav'\n      pickle.dump(model, open(filename, 'wb'))\n      \n    return resultsDf  # return all the metrics along with predictions","61da310f":"def hyperparameterstune_model(name, model, X_train, y_train, param_grid):\n    \n    start = time.time()  # note the start time \n    \n    # Before starting with grid search we need to create a scoring function. This is accomplished using the make_scorer function of scikit-learn.\n    mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)\n\n    # define grid search\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    if name == 'LGBMClassifier':\n      grid_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, n_jobs=-1, cv=cv, \n                                       scoring = mll_scorer, error_score=0)\n    else:\n      grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, \n                                 scoring = mll_scorer, error_score=0)\n      \n    model_grid_result = grid_search.fit(X_train, y_train)\n\n    # summarize results\n    print(\"Best F1_Score: %f using %s\" % (model_grid_result.best_score_, model_grid_result.best_params_))\n    means = model_grid_result.cv_results_['mean_test_score']\n    stds = model_grid_result.cv_results_['std_test_score']\n    params = model_grid_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n      if param == model_grid_result.best_params_:\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n        print(\"95% Confidence interval range: ({0:.4f} %, {1:.4f} %)\".format(mean-(2*stdev), mean+(2*stdev)))\n\n    end = time.time()  # note the end time\n    duration = end - start  # calculate the total duration\n    print(\"Total duration\" , duration, \"\\n\")\n    \n    return model_grid_result.best_estimator_","65960248":"# For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss; 'liblinear' is limited to one-versus-rest schemes.\n\nresultsDf = pd.DataFrame()\n\n# Building a Linear Regression model\nlr = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state = 1)\n                                                     \n# Train and Test the model\nresultsDf = train_test_model(lr, 'Logistic Regression without Sampling', X_train, X_test, y_train, y_test, 'none', 1, 'no', 'yes', 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf","db7bc49e":"# Building a Random Forest Classifier on Training set\nrfc_model = RandomForestClassifier(n_estimators=10, random_state=1)\n\n# Train and Test the model\nrf_df = train_test_model(rfc_model, 'Random Forest with original data', X_train, X_test, y_train, y_test, 'none', 2, 'no', 'yes', 'no')\n\n#Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,rf_df])\nresultsDf","eb8a7102":"# Building a Linear Regression model\nlr = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state = 1)\n                                                     \n# Train and Test the model\nlr_df = train_test_model(lr, 'Logistic Regression with Sampling', X_train_up, X_test, y_train_up, y_test, 'none', 3, 'no', 'yes', 'no')\n\n#Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,lr_df])\nresultsDf","bdc16bd2":"# get the accuracy, precision, recall, f1 score from model\ndef get_classification_metrics(model, X_test, y_test, target_type):\n  \n  # predict probabilities for test set\n  yhat_probs = model.predict(X_test, verbose=0) # Multiclass\n\n  # predict crisp classes for test set\n  if target_type == 'multi_class':\n    yhat_classes = model.predict_classes(X_test, verbose=0) # Multiclass\n  else:\n    yhat_classes = (np.asarray(model.predict(X_test))).round() # Multilabel\n\n  # reduce to 1d array\n  yhat_probs = yhat_probs[:, 0]\n\n  # accuracy: (tp + tn) \/ (p + n)\n  accuracy = accuracy_score(y_test, yhat_classes)\n\n  # precision tp \/ (tp + fp)\n  precision = precision_score(y_test, yhat_classes, average='micro')\n\n  # recall: tp \/ (tp + fn)\n  recall = recall_score(y_test, yhat_classes, average='micro')\n\n  # f1: 2 tp \/ (2 tp + fp + fn)\n  f1 = f1_score(y_test, yhat_classes, average='micro')\n\n  return accuracy, precision, recall, f1","04cac0c2":"class Metrics(tf.keras.callbacks.Callback):\n\n    def __init__(self, validation_data=()):\n        super().__init__()\n        self.validation_data = validation_data\n\n    def on_train_begin(self, logs={}):\n        self.val_f1s = []\n        self.val_recalls = []\n        self.val_precisions = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        xVal, yVal, target_type = self.validation_data\n        if target_type == 'multi_class':\n          val_predict_classes = model.predict_classes(xVal, verbose=0) # Multiclass\n        else:\n          val_predict_classes = (np.asarray(self.model.predict(xVal))).round() # Multilabel\n        \n        \n        val_targ = yVal\n\n        _val_f1 = f1_score(val_targ, val_predict_classes, average='micro')\n        _val_recall = recall_score(val_targ, val_predict_classes, average='micro')\n        _val_precision = precision_score(val_targ, val_predict_classes, average='micro')\n        self.val_f1s.append(_val_f1)\n        self.val_recalls.append(_val_recall)\n        self.val_precisions.append(_val_precision)\n        #print(\"\u2014 train_f1: %f \u2014 train_precision: %f \u2014 train_recall %f\" % (_val_f1, _val_precision, _val_recall))\n        return","c877de34":"# fix random seed for reproducibility\nseed = 7\nnp.random.seed(seed)\ntf.random.set_seed(seed)\n\n# define the model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(150, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(40, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(1, activation='linear'))\n\n# compile the keras model\n#using SGD optimizer with learning rate of 0.001\nopt = SGD(lr=0.001, momentum=0.9)\nmodel.compile(loss='mse', optimizer=opt, metrics=['accuracy'])\n\n# Use earlystopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, min_delta=0.001)\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)\n\n# fit the keras model on the dataset\ntraining_history = model.fit(X_train, y_train, epochs=100, batch_size=8, verbose=2, validation_data=(X_test, y_test), callbacks=[rlrp])","90b2aca4":"model.summary()","d01e3225":"# evaluate the keras model\n_, train_accuracy = model.evaluate(X_train, y_train, batch_size=8, verbose=0)\n\n_, test_accuracy = model.evaluate(X_test, y_test, batch_size=8, verbose=0)\nprint('Test accuracy: %.2f' % (test_accuracy*100))","1abf3584":"accuracy, precision, recall, f1 = get_classification_metrics(model, X_test, y_test, 'non-multi-class')\nprint('Accuracy: %f' % accuracy)\nprint('Precision: %f' % precision)\nprint('Recall: %f' % recall)\nprint('F1 score: %f' % f1)\n\n","a2cdd05d":"method =\" ANN Multiclass Classification\"\nresultsDfANNC = pd.DataFrame({'Method': method, 'Train Accuracy': train_accuracy, 'Test Accuracy': accuracy, \n                              'Precision': precision, 'Recall': recall, 'F1-Score': f1}, index=[4])\nresultsDf = pd.concat([resultsDf,resultsDfANNC])\nresultsDf","f8068913":"epochs = range(len(training_history.history['loss'])) # Get number of epochs\n\n# plot loss learning curves\nplt.plot  (epochs, training_history.history['loss'], label = 'train')\nplt.plot  (epochs, training_history.history['val_loss'], label = 'val')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation loss')","8069e0db":"# fix random seed for reproducibility\nreset_random_seeds()\n#param = 1e-9\nparam = 1e-4\n\n# define the model\nmodel = Sequential()\n\nmodel.add(Dense(10, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param),\n                kernel_constraint=unit_norm()))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param), \n                kernel_constraint=unit_norm()))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(5, activation='softmax', kernel_regularizer=l2(param), \n                kernel_constraint=unit_norm())) # Multilabel\n\n# compile the keras model\n#opt = optimizers.Adamax(lr=0.01)\nopt = SGD(lr=0.001, momentum=0.9)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n\n# Use earlystopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)\n\ntarget_type = 'multi_label'\nmetrics = Metrics(validation_data=(X_train, y_train_dummy, target_type))\n\n# fit the keras model on the dataset\ntraining_history = model.fit(X_train, y_train_dummy, epochs=100, batch_size=8, verbose=2, validation_data=(X_test, y_test_dummy), callbacks=[rlrp, metrics])","841e2e40":"model.summary()","b51e8af5":"# evaluate the keras model\n_, train_accuracy = model.evaluate(X_train, y_train_dummy, batch_size=8, verbose=0)\n\n_, test_accuracy = model.evaluate(X_test, y_test_dummy, batch_size=8, verbose=0)\naccuracy, precision, recall, f1 = get_classification_metrics(model, X_test, y_test_dummy, target_type)\nprint('Accuracy: %f' % accuracy)\nprint('Precision: %f' % precision)\nprint('Recall: %f' % recall)\nprint('F1 score: %f' % f1)\n","b64fe6ec":"method =\"ANN- One hot encoded\"\nresultsDfNN = pd.DataFrame({'Method': method, 'Train Accuracy': train_accuracy, 'Test Accuracy': accuracy, \n                              'Precision': precision, 'Recall': recall, 'F1-Score': f1\n                              }, index=[5])\nresultsDf = pd.concat([resultsDf,resultsDfNN])\nresultsDf","60618876":"epochs = range(len(training_history.history['loss'])) # Get number of epochs\n\n# plot loss learning curves\nplt.plot(epochs, training_history.history['loss'], label = 'train')\nplt.plot(epochs, training_history.history['val_loss'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation loss')","72ec76c8":"# plot accuracy learning curves\nplt.plot(epochs, training_history.history['categorical_accuracy'], label = 'train')\nplt.plot(epochs, training_history.history['val_categorical_accuracy'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation accuracy')","ce0a03b7":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model weights to disk\")\n\n# Save the model in h5 format \nmodel.save(\"finalized_keras_model.h5\")\nprint(\"Saved model to disk\")","1de9f359":"# fix random seed for reproducibility\nreset_random_seeds()\n#param = 1e-9\nparam = 1e-4\n\n# define the model\nmodel = Sequential()\n\nmodel.add(Dense(10, input_dim=X_train_smote.shape[1], activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param),\n                kernel_constraint=unit_norm()))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param), \n                kernel_constraint=unit_norm()))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(5, activation='softmax', kernel_regularizer=l2(param), \n                kernel_constraint=unit_norm())) # Multilabel\n\n# compile the keras model\n#opt = optimizers.Adamax(lr=0.01)\nopt = SGD(lr=0.001, momentum=0.9)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n\n# Use earlystopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)\n\ntarget_type = 'multi_label'\nmetrics = Metrics(validation_data=(X_train_smote, y_train_smote_dummy, target_type))\n\n# fit the keras model on the dataset\ntraining_history = model.fit(X_train_smote, y_train_smote_dummy, epochs=100, batch_size=8, verbose=2, validation_data=(X_test, y_test_dummy), callbacks=[rlrp, metrics])","757e64aa":"model.summary()","eca9cf47":"# evaluate the keras model for test and train accuracy\n_, train_accuracy = model.evaluate(X_train_smote, y_train_smote_dummy, batch_size=8, verbose=0)\n_, test_accuracy = model.evaluate(X_test, y_test_dummy, batch_size=8, verbose=0)\n\nprint('Train accuracy: %.2f' % (train_accuracy*100))\nprint('Test accuracy: %.2f' % (test_accuracy*100))","2c8b858d":"accuracy, precision, recall, f1 = get_classification_metrics(model, X_test, y_test_dummy, target_type)\nprint('Accuracy: %f' % accuracy)\nprint('Precision: %f' % precision)\nprint('Recall: %f' % recall)\nprint('F1 score: %f' % f1)\ny_hat = model.predict(X_test)\nmc_logloss = multiclass_logloss(y_test_dummy, y_hat, eps=1e-15)\nprint('mc logloss: %f' % mc_logloss)","b3c1893f":"method =\"ANN- One hot encoded with SMOTE data\"\nresultsDfNN2 = pd.DataFrame({'Method': method, 'Train Accuracy': train_accuracy, 'Test Accuracy': accuracy, \n                              'Precision': precision, 'Recall': recall, 'F1-Score': f1, \n                              'Multi-Class Logloss': mc_logloss}, index=[6])\nresultsDf = pd.concat([resultsDf,resultsDfNN2])\nresultsDf","55770c73":"epochs = range(len(training_history.history['loss'])) # Get number of epochs\n\n# plot loss learning curves\nplt.plot(epochs, training_history.history['loss'], label = 'train')\nplt.plot(epochs, training_history.history['val_loss'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation loss')","8c6605fc":"# plot accuracy learning curves\nplt.plot(epochs, training_history.history['categorical_accuracy'], label = 'train')\nplt.plot(epochs, training_history.history['val_categorical_accuracy'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation accuracy')","a4ba6a63":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model weights to disk\")\n\n# Save the model in h5 format \nmodel.save(\"finalized_keras_model.h5\")\nprint(\"Saved model to disk\")","71aaf99d":"# Select input and output features\nX_text = industry_df['Cleaned_Description']\ny_text = industry_df['Accident Level']","374ebbd9":"# Encode labels in column 'Accident Level'.\ny_text = LabelEncoder().fit_transform(y_text)","a180c544":"# Divide our data into testing and training sets:\nX_text_train, X_text_test, y_text_train, y_text_test = train_test_split(X_text, y_text, test_size = 0.20, random_state = 1, stratify = y_text)\n\nprint('X_text_train shape : ({0})'.format(X_text_train.shape[0]))\nprint('y_text_train shape : ({0},)'.format(y_text_train.shape[0]))\nprint('X_text_test shape : ({0})'.format(X_text_test.shape[0]))\nprint('y_text_test shape : ({0},)'.format(y_text_test.shape[0]))","70040d1f":"# Convert both the training and test labels into one-hot encoded vectors:\ny_text_train = np_utils.to_categorical(y_text_train)\ny_text_test = np_utils.to_categorical(y_text_test)","7f3b8a63":"# The first step in word embeddings is to convert the words into thier corresponding numeric indexes.\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X_text_train)\n\nX_text_train = tokenizer.texts_to_sequences(X_text_train)\nX_text_test = tokenizer.texts_to_sequences(X_text_test)","8fbdf7ba":"# Sentences can have different lengths, and therefore the sequences returned by the Tokenizer class also consist of variable lengths.\n# We need to pad the our sequences using the max length.\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"vocab_size:\", vocab_size)\n\nmaxlen = 100\n# this takes our sentences and replaces each word with an integer\n# we then pad the sequences so they're all the same length (sequence_length)\nX_text_train = pad_sequences(X_text_train, padding='post', maxlen=maxlen)\nX_text_test = pad_sequences(X_text_test, padding='post', maxlen=maxlen)","3bc91a15":"# We need to load the built-in GloVe word embeddings\nembedding_size = 200\nembeddings_dictionary = dict()\n\nglove_file = open('..\/input\/glove6b200d\/glove.6B.200d.txt', encoding=\"utf8\")\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\n\nglove_file.close()\n\n\n#Creating embedding matrix\n# first create a matrix of zeros, this is our embedding matrix\n\nembedding_matrix = np.zeros((vocab_size, embedding_size))\n\n# for each word in out tokenizer lets try to find that work in our glove model\n# we found the word - add that words vector to the matrix\n\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n\nlen(embeddings_dictionary.values())","0dfebbb5":"reset_random_seeds()\n\n# Build a LSTM Neural Network\ndeep_inputs = Input(shape=(maxlen,))\n\n#Adding emdedding layer\n#Embedding, we need to specify the embeddings matrix (the set of vectors representing our words) I'm going to make them trainable - this means they can be modified during training if something more accurate is found\nembedding_layer = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(deep_inputs)\n#add bidirectional LSTM with 128 nodes.\nLSTM_Layer_1 = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)\n\n\n#Adding a Max Pool (Downsamples the input representation by taking the maximum value over the time dimension)\nmax_pool_layer_1 = GlobalMaxPool1D()(LSTM_Layer_1)\n#Adding Drop out layer ( Dropout is a technique used to prevent a model from overfitting. Dropout works by randomly setting the outgoing edges of hidden units (neurons that make up hidden layers) to 0 at each update of the training phase.)\ndrop_out_layer_1 = Dropout(0.5, input_shape = (256,))(max_pool_layer_1)\n\n\n#Adding a dense layer of 128 nodes with activation as relu\ndense_layer_1 = Dense(128, activation = 'relu')(drop_out_layer_1)\n#Adding Drop out layer \ndrop_out_layer_2 = Dropout(0.5, input_shape = (128,))(dense_layer_1)\n\n\n#Adding a dense layer of 64 nodes with activation as relu\ndense_layer_2 = Dense(64, activation = 'relu')(drop_out_layer_2)\n#Adding Drop out layer \ndrop_out_layer_3 = Dropout(0.5, input_shape = (64,))(dense_layer_2)\n\n#Adding a dense layer of 32 nodes with activation as relu\ndense_layer_3 = Dense(32, activation = 'relu')(drop_out_layer_3)\n#Adding Drop out layer \ndrop_out_layer_4 = Dropout(0.5, input_shape = (32,))(dense_layer_3)\n\n#Adding a dense layer of 10 nodes with activation as relu\ndense_layer_4 = Dense(10, activation = 'relu')(drop_out_layer_4)\n#Adding Drop out layer \ndrop_out_layer_5 = Dropout(0.5, input_shape = (10,))(dense_layer_4)\n\n#Adding a output layer of 5 classes with softmax as activatin function\ndense_layer_5 = Dense(5, activation='softmax')(drop_out_layer_5)\n\n#Consutruct model with SGD and learning rate as 0.01 and categorical cross entropy for this classification problem.\nmodel = Model(inputs=deep_inputs, outputs=dense_layer_5)\n\nopt = SGD(lr=0.001, momentum=0.9)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])","474711fc":"print(model.summary())","71da36b8":"plot_model(model, to_file='model_plot1.png', show_shapes=True, show_dtype=True, show_layer_names=True)","df369d03":"# Use earlystopping\n# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, min_delta=0.001)\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)\n\n#TO  Reduce learning rate when a metric has stopped improving.\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)\n\ntarget_type = 'multi_label'\nmetrics = Metrics(validation_data=(X_text_train, y_text_train, target_type))\n\n# fit the keras model on the dataset\ntraining_history = model.fit(X_text_train, y_text_train, epochs=100, batch_size=8, verbose=2, validation_data=(X_text_test, y_text_test), callbacks=[rlrp, metrics])","4777b9f1":"# evaluate the keras model\n_, train_accuracy = model.evaluate(X_text_train, y_text_train, batch_size=8, verbose=0)\n_, test_accuracy = model.evaluate(X_text_test, y_text_test, batch_size=8, verbose=0)\n\nprint('Train accuracy: %.2f' % (train_accuracy*100))\nprint('Test accuracy: %.2f' % (test_accuracy*100))","48c91aff":"accuracy, precision, recall, f1 = get_classification_metrics(model, X_text_test, y_text_test, target_type)\nprint('Accuracy: %f' % accuracy)\nprint('Precision: %f' % precision)\nprint('Recall: %f' % recall)\nprint('F1 score: %f' % f1)\ny_hat = model.predict(X_text_test)\nmc_logloss = multiclass_logloss(y_text_test, y_hat, eps=1e-15)\nprint('mc logloss: %f' % mc_logloss)\n","6248e4fd":"method =\"LSTM with Glove, TFIFD embedding\"\nresultsDfLSTM = pd.DataFrame({'Method': method, 'Train Accuracy': train_accuracy, 'Test Accuracy': accuracy, \n                              'Precision': precision, 'Recall': recall, 'F1-Score': f1, \n                              'Multi-Class Logloss': mc_logloss}, index=[7])\nresultsDf = pd.concat([resultsDf,resultsDfLSTM])\nresultsDf","6d1c9683":"epochs = range(len(training_history.history['loss'])) # Get number of epochs\n\n# plot loss learning curves\nplt.plot(epochs, training_history.history['loss'], label = 'train')\nplt.plot(epochs, training_history.history['val_loss'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation loss')","493c6180":"#Pickle the model for later use\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model weights to disk\")\n\n# Save the model in h5 format \nmodel.save(\"finalized_keras_model.h5\")\nprint(\"Saved model to disk\")","90e8dd4e":"resultsDf","c5dbe0d9":"#### Here we can use the DummyClassifier to predict all accident levels just to show how misleading accuracy can be.","aa9d45dc":"#### Remove 'Unnamed: 0' and Rename - 'Data', 'Countries', 'Genre', 'Employee or Third Party' columns","a1b74f62":"* Number of accidents are very high in particular days like 4, 8 and 16 in every month.","81bcb304":"<a id=\"data-cleansing\"><\/a>\n## <font color='violet'>Data Cleansing<\/font>","6782be2c":"SMOTE: Synthetic Minority Over-sampling Technique used to upsample a unbalanced dataset and better than random upsampling","f930bafe":"<a id=\"nlp-models-text-input\"><\/a>\n##### <font color='green'> 1. Creating a Model with Text Inputs Only<\/font>\n\nIn this section, we will create a classification model that uses accident description column alone.","82d2e72d":"**Observations**\n\n* Both of the two accident level have the tendency that non-severe levels decreased throughout the year, but severe levels did not changed much, and some of these levels increased slightly in the second half of the year.","327c36f6":"* Because most part of the Critical Risks are classified as 'Others', it is thought that there are too many risks to classify precisely.\n\n* And it is also thought that it takes so much time to analyze risks and reasons why the accidents occur.","7fb75484":"LSTM stands for Long-Short Term Memory. LSTM is a type of recurrent neural network but is better than traditional recurrent neural networks in terms of memory. Having a good hold over memorizing certain patterns LSTMs perform fairly better.LSTMs efficiently improves performance by memorizing the relevant information that is important and finds the pattern.","1a723bc1":"**Observations**\n\n* Proportion of Metals sector employees in each gender is not equal.\n* Proportion of Mining sector employees in each gender is not equal.\n* Proportion of Others sector employees in each gender is not equal.","0c6a3b00":"#### \n#### <font color='green'>2. Decision Tree - Random Forest Classifier<\/font>\n\n\n* While in every machine learning problem, it\u2019s a good rule of thumb to try a variety of algorithms, it can be especially beneficial with imbalanced datasets. Decision trees frequently perform well on imbalanced data. They work by learning a hierarchy of if\/else questions and this can force both classes to be addressed.","cb2b7f51":"<a id=\"ml-models\"><\/a>\n## \n## <font color='Red'> Design, train and test machine learning classifiers<\/font>","48947cbc":"Here we are dropping the potential Accident Level and considering only the Accident Level as the traget column.","0da8eb52":"#### \n## <font color='Majenta'> Use PCA - Extract Principal Components that capture about 95% of the variance in the data<\/font>","ddec8bef":"**Country**","0072cf0f":"* The number of accidents decreases as the Accident Level increases.\n* The number of accidents increases as the Potential Accident Level increases.","5135bbc2":"#### Check Missing Values","911d7395":"##### h. Accident Levels by Seasons - Is the distribution of accident levels and potential accident levels differ significantly in different seasons?","010bd067":"* A good fit, it is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values. \n* The loss of the model will almost always be lower on the training dataset than the validation dataset.","d0b01d15":"RidgeClassifier :\nRidge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values.\n\nCatBoostClassifier:\"CatBoost is a high-performance open source library for gradient boosting on decision trees.\"\"\nSo, CatBoost is an algorithm for gradient boosting on decision trees.\n\nLGBMClassifier: Light GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm","9128c219":"<a id=\"wordcloud\"><\/a>\n#### \n## <font color='Darkpink'> WordCloud<\/font>","3c38daa3":"**Finally bidirectional LSTM model can be considered to productionalized the model and predict the accident level****","c1688808":"#### <font color='green'> Define MultiClass-Logloss<\/font>","d7ac8f15":"TF-IDF, which stands for term frequency\u200a\u2014\u200ainverse document frequency, is a scoring measure widely used in information retrieval (IR) or summarization. TF-IDF is intended to reflect how relevant a term is in a given document.","d3ab4aa7":"## <font color='Green'> Variable Creation - Glove Word Embeddings<\/font>","911f3434":"#### Drop Duplicates","b6f19eea":"* 59% accidents occurred in Country_01\n* 31% accidents occurred in Country_02\n* 10% accidents occurred in Country_03\n\n","bd5fb638":"<a id=\"nlp-models\"><\/a>\n## \n## <font color='Red'> Design, train LSTM classifiers<\/font>","793e8d68":"\n## <font color='Red'> Conclusion<\/font>","e0a21bf0":"**Employee type**","db45cd5f":"Now combining Glove encoded data and TFIDF and label encoded data","95de9dc7":"* **From the above output, we see that except first column all other columns datatype is object.**\n\n* **Categorical columns - 'Countries', 'Local', 'Industry Sector', 'Accident Level', 'Potential Accident Level', 'Genre', 'Employee or Third Party', 'Critical Risk', 'Description'**\n\n* **Date column - 'Data'**","9d85ebf5":"## <font color='Red'> Insights<\/font>","ffe5f3b1":"**Critical Risk**","92def2bb":"**Local**","03458a73":"#### \n#### <font color='green'> Model with Hyperparameter Tuning<\/font>\n","51f2eff1":"**Observations**\n\n* WeekofYear featuer is having very high positive correlation with Month feature.","72e25826":"<a id=\"ann-models-multi-class\"><\/a>\n#### \n#### <font color='green'> 5. ANN Multiclass classification - Target variable - One hot encoded<\/font>\n\n\nIn this section, we will create a classification model that uses categorical columns and tf-idf features from accident description and one-hot encoded target variable. We can use simple densely connected neural networks to make predictions.","92286a7d":"#### \n## <font color='Majenta'> Varible Tansformation (Normalization and Scaling)<\/font>","53c9b5c4":"#### <font color='green'>4. Get ANN Multiclass Classification Metrics <\/font>\n","432b43e8":"1. There are about 425 rows and 11 columns in the dataset.\n2. We noticed that except a 'date' column all other columns are categorical columns.\n3. Column Description alone contains the text data","850ffbd1":"##### e. Accident Levels by Employee type - Is the distribution of accident levels and potential accident levels differ significantly in different employee types?","7d269ab6":"#### \n#### <font color='green'>1. Modelling - Logistic Regression<\/font>\n","9973f476":"GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\nin GloVe: Global Vectors for Word Representation. GloVe Embeddings are a type of word embedding that encode the co-occurrence probability ratio between two words as vector differences.","1d10fc34":"AdaBoostClassifier#### Variable Creation - Word2Vec Embeddings","440ce59d":"<a id=\"import-libraries\"><\/a>\n## <font color='Red'>  Import the necessary libraries<\/font>","4f19f627":"using standard scaler to standardize before modelling.","b0466d52":"# Solution Overview \nIn the problem statement, we are given the data of industrial safety database, which consists of several columns , in which there is a Description column, which we plan to use NLP processing techniques to predict the accident levels of the industry in Brazil, with the chat user interface.\n\nTarget Column: Accident Level\n","953408b0":"#### \n## <font color='Purple'> Sampling Techniques - Create Training and Test Set<\/font>","c880c080":"\n\n**Local**\n* Highest manufacturing plants are located in Local_03 city and lowest in Local_09 city.\n\n**Country**\n* Percentage(%) of accidents occurred in respective countries: 59% in Country_01, 31% in Country_02 and 10% in Country_03.\n\n**Industry Sector**\n* Percentage(%) of manufacturing plants belongs to respective sectors: 57% to Mining sector, 32% to Metals sector and 11% to Others sector.\n\n**Country + Industry Sector**\n* Metals and Mining industry sector plants are not available in Country_03.\n* Distribution of industry sector differ significantly in each country.\n\n**Accident Levels**\n* The number of accidents decreases as the Accident Level increases and increases as the Potential Accident Level increases.\n\n**Employee type**\n* 44% Third party empoyees, 43% own empoyees and 13% Third party(Remote) empoyees working in this industry.\n\n**Accident Levels + Employee type**\n* For both accident levels, the incidence of Employee is higher at low accident levels, but the incidence of Third parties seems to be \nslightly higher at high accident levels.\n\n**Accident Levels + Calendar**\n* Accidents are recorded from 1st Jan 2016 to 9th July 2017 in every month, there are high number of accidents in 2016 and less in 2017.\n* Number of accidents are high in beginning of the year and it keeps decreasing later.\n* Number of accidents are very high in particular days like 4, 8 and 16 in every month.\n* Number of accidents increased during the middle of the week and declined since the middle of th week.\n\n* Both of the two accident level have the tendency that non-severe levels decreased throughout the year, but severe levels did not changed much, \nand some of these levels increased slightly in the second half of the year.\n* Both of the two accident level is thought that non-severe levels decreased in the first and the last of the week, but severe levels did not \nchanged much.\n\n**Critical Risk**\n* Most of the critical risks are classified as Others.","7f283c22":"\n<div class=\"header\">\n  <h1>NATURAL LANGUAGE PROCESSING CAPSTONE PROJECT AIML CHATBOT INTERFACE<\/h1>\n<\/div>\n\n***DOMAIN***: \nIndustrial safety. NLP based Chatbot.\n\n\n***CONTEXT***: \n        The database comes from one of the biggest industry in Brazil and in the world. It is an urgent need for industries\/companies around the globe to understand why employees still suffer some injuries\/accidents in plants. Sometimes they also die in such environment.\n        \n***DATA DESCRIPTION***:\nThis The database is basically records of accidents from 12 different plants in 03 different countries which every line in the data is an \noccurrence of an accident.\n\n\n***Columns description***: \n* \u2023 Data: timestamp or time\/date information\n* \u2023 Countries: which country the accident occurred (anonymised)\n* \u2023 Local: the city where the manufacturing plant is located (anonymised)\n* \u2023 Industry sector: which sector the plant belongs to\n* \u2023 Accident level: from I to VI, it registers how severe was the accident (I means not severe but VI means very severe)\n* \u2023 Potential Accident Level: Depending on the Accident Level, the database also registers how severe the accident could    have been (due to other factors involved in the accident)\n* \u2023 Gender: if the person is male of female\n* \u2023 Employee or Third Party: if the injured person is an employee or a third party\n* \u2023 Critical Risk: some description of the risk involved in the accident\n* \u2023 Description: Detailed description of how the accident happened.\n\n\n\nLink to download the dataset: https:\/\/drive.google.com\/file\/d\/1_GmrRP1S2OIa02KlfOBNkYa8uxazGbfE\/view?usp=sharing,\nOriginal dataset link: https:\/\/www.kaggle.com\/ihmstefanini\/industrial-safety-and-health-analytics-database","26de9e2a":"* 44% Third party empoyees working in this industry.\n* 43% own empoyees working in this industry.\n* 13% Third party(Remote) empoyees working in this industry.","195cb067":"Above one is overfit model, it can be identified from the learning curve of the training and validation loss only.","c98c079f":"#### \n## <font color='Green'> Variable Creation - Label Encoding<\/font>","ffceabd8":"1.Able to predict the accident level with a test accuracy of 73.81% and f1-score of 73.81%\n\n2.7 duplicate values in this dataset and were dropped\n\n3.No outliers and no missing values\n\n4.Extracted the day, month and year from Date column and created new features such as weekday, weekofyear and seasons.\n\n5.Target variable \u2013 \u2018Accident Level\u2019 distribution is not equal (I: 309, II: 40, III: 31, IV: 30, V: 8).\nClass imbalance issue is handled using below methods and found out that, for this particular dataset, with original data we have achieved the better results.\n\na. Resampling techniques: Oversampling minority class\n\nb. SMOTE: Generate synthetic samples\n\n6.Feature engineering :\n\n    We have tried the below feature engineering methods\n    1. Variable Creation - Word2Vec Embeddings\n\n    2.Variable Creation - TFIDF Features\n\n    3.Variable Creation - Label Encoding\n    \n    4.Combine Glove and Encoded Features\n\n    5.Combine TFIDF and Encoded FeatureS\n    \n7.We have explored the following Models with the following F1- scores\nExplored below options in Neural Networks.\n\na. Convert Classification to Numerical problem: test accuracy of 53.57% which is a bad result.\n\nb. Multiclass classification - Target variable - One hot encoded: test accuracy of 73.81% and f1-score of 73.81% with original data + TF-IDF features from accident description column.\n\nc. Create a model with Text inputs (accident description alone) only: test accuracy of 73.81% and f1-score of 73.81% with original data.\n\n","5073faae":"## <font color='violet'>Data Cleansing Summary:<\/font>\n","0bbac362":"**Calendar**","3db7e06b":"<a id=\"univariate-analysis\"><\/a>\n#### Univariate Analysis","7b4d7857":"## <font color='violet'>Data Collection Summary:<\/font>\n","ed15fa2a":"Now combining Glove(top 30) encoded data and TFIDF and label encoded data","265491ad":"We can create holidays variable based on Brazil holidays list from 2016 and 2017.\n\nAnother national holidays are election days. There are a plenty of unofficial ethnic and religious holidays in Brazil. Octoberfest, Brazilian Carnival, Kinderfest, Fenaostra, Fenachopp, Musikfest, Schutzenfest, Kegelfest, Cavalhadas, Oberlandfest, Tirolerfest, Marejada are among them.\n\n**Note**: Considering official holidays only.","c363d208":"As we know, this database comes from one of the biggest industry in Brazil which has four climatological seasos as below.\n\nhttps:\/\/seasonsyear.com\/Brazil\n\n* Spring : September to November\n* Summer : December to February\n* Autumn : March to May\n* Winter : June to August\n\nWe can create seasonal variable based on month variable.","752ab788":"* Highest manufacturing plants are located in Local_03 city.\n* Lowest manufacturing plants are located in Local_09 city.","9230c2c0":"This tool(Word2Vec) provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words. \nThe word2vec tool takes a text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words. The resulting word vector file can be used as features in many natural language processing and machine learning applications.","9bc7e943":"Max number of words is 98, while minimum number of words is 10","52372d73":"##### Industry Sector by Gender - Is the distribution of industry sector differ significantly in different genders?","607cf2d2":"#### Check Duplicates","1cd5c753":"## <font color='Red'>EDA Summary:<\/font>","3458eacd":"74% of data where accident description > 100 is captured in low accident level.\n34% of data where accident description > 100 is captured in high medium potential accident level.\n25% of data where accident description > 100 is captured in medium potential accident level.\n23% of data where accident description > 100 is captured in low potential accident level.\nFew of the NLP pre-processing steps taken before applying model on the data\n\nConverting to lower case, avoid any capital cases\nConverting apostrophe to the standard lexicons\nRemoving punctuations\nLemmatization\nRemoving stop words\nAfter pre-processing steps:\n\nMinimum line length: 64\nMaximum line length: 672\nMinimum number of words: 10\nMaximum number of words: 98","e4bf8c4d":"# <font color='Red'>Data Collection<\/font>\n","f0ce1c42":"#### Data type of each attribute","980d75a1":"Above one is good fit, it is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values. The loss of the model will almost always be lower on the training dataset than the validation dataset.","5298868d":"* Creating Date column and creating new features such as weekday, weekofyear.","42150bf5":"**Gender**","95d1ec73":"General:\n1.Majority of accidents happen in Mining followed by Metal industry.\n2.Accident level (1) is the most common accidents reported.\n3.Slightly major accidents happen with contract workers compared to employees.\n4.The third party workers are having higher potential accident levels, \n5.In the Dataset more accidents are registered in Country 1 and in Local_03\n6.For both accident levels, the incidence of Employee is higher at low accident levels, but the incidence of Third parties seems to be higher accident levels\n\n\nTo improve\n1. The Dataset is relatively small  and hence the data is too small for the model.\n2. Description Data is not structured, some key words can be used to describe the accurate description.\n3. The Dataset is unbalanced, might be due to the problem type here. \n4. Including the non-text inputs for LSTM model couldbe done\n5. The words described(Body-related,Accident-related: )the severity of the accident is not evident with saying body parts name or accident name, there can be termed as injury level, so as to improve the model.\n6.And major critical risks are termed as others, which is not giving more clear information on risks to mitigate.\n7.The time of the day of accidents is not captured, it could help to draw conclusion on finding the root cause of the accidents.","94853483":"* 57% manufacturing plants belongs to Mining sector.\n* 32% manufacturing plants belongs to Metals sector.\n* 11% manufacturing plants belongs to Others sector.","f0699906":"1.Above one is underfit model, it can be identified from the learning curve of the training loss only.\n2.It is showing noisy values of relatively high loss, indicating that the model was unable to learn the training dataset at all and model does not have a suitable capacity for the complexity of the dataset.","a544c961":"1. Removed 'Unnamed: 0' column and renamed - 'Data', 'Countries', 'Genre', 'Employee or Third Party' columns in the dataset.\n2. We had 7 duplicate instances in the dataset and dropped those duplicates.\n3. There are no outliers in the dataset.\n4. No missing values in dataset.\n5. We are left with 418 rows and 10 columns after data cleansing.","1a429e3f":"# \n## <font color='Red'> Next Steps<\/font>","4a1f9e56":"* There are more men working in this industry as compared to women.","97374b4e":"* **Target variable:** 'Accident Level', 'Potential Accident Level'\n* **Predictors (Input varibles):** 'Date', 'Country', 'Local', 'Industry Sector', 'Gender', 'Employee type', 'Critical Risk', 'Description'","ae7498be":"**Industry Sector**","4cab1c1f":"<a id=\"eda\"><\/a>\n\n## <font color='Red'>EDA (Data Analysis and Preparation)<\/font>","2cea7575":"We can see the Accident levels 1, 2, 3, 4, are non uniformly present. Hence to avoid the biasing in the result we can upsample the dataset","da4b9104":"<a id=\"data-pre-processing\"><\/a>\n## <font color='violet'> Data Pre-processing<\/font>\n","15c1166f":"## <font color='Green'> Variable Creation - Word2Vec Embeddings<\/font>","51e80ff9":"* We observed that there are records of accidents from 1st Jan 2016 to 9th July 2017 in every month. So there are no outliers in the 'Date' column.\n\n* There are only three country types so there are no outliers in 'Country' column.\n\n* There are 12 Local cities where manufacturing plant is located and it's types are in sequence so there are no outliers in 'Local' column.\n\n* There are only three Industry Sector types which are in sequence so there are no outliers in 'Industry Sector' column.\n\n* There are only five Accident Level types which are in sequence so there are no outliers in 'Accident Level' column.\n\n* There are only six Potential Accident Level types which are in sequence so there are no outliers in 'Potential Accident Level' column.\n\n* There are only two Gender types in the provided data so there are no outliers in 'Gender' column.\n\n* There are only three Employee types in the provided data so there are no outliers in 'Gender' column.\n\n* There are quite a lot of Critical risk descriptions and we don't see any outliers but with the help of SME we can decide whether this column has outliers or not.","8ea315e0":"**Observations**\n\nThere are many body-related, employee related, movement-related, equipment-related and accident-related words.\n\n* Body-related: left, right, hand, finger, face, foot and glove\n* Employee-related: employee, operator, collaborator, assistant, worker and mechanic\n* Movement-related: fall, hit, lift and slip\n* Equipment-related: equipment, pump, meter, drill, truck and tube\n* Accident-related: accident, activity, safety, injury, causing\n","98f5b205":"##### d. Accident Levels by Gender - Is the distribution of accident levels and potential accident levels differ significantly in different genders?","d150910c":"#### Variable Identification","e1255876":"\n## <font color='Orange'> Combine Glove and Encoded Features<\/font>","ad6617a5":"#### \n## <font color='Orange'> Combine TFIDF and Encoded Features<\/font>","90a6825b":"* There is no need to worry about preserving the data; it is already a part of the industry dataset and we can merely remove or drop these rows from your cleaned data","9829a4f9":"#### Setting Options","6f898f10":"<a id=\"nlp-pre-processing\"><\/a>\n\n## <font color='Red'>NLP Pre-processing<\/font>\n\nFew of the NLP pre-processing steps taken before applying model on the data\n\n- Converting to lower case, avoid any capital cases\n- Converting apostrophe to the standard lexicons\n- Removing punctuations\n- Lemmatization\n- Removing stop words","ef16d648":"* Accidents are recorded from 1st Jan 2016 to 9th July 2017 in every month, there are high number of accidents in 2016 and less in 2017.\n* Number of accidents are high in beginning of the year and it keeps decreasing later.","6e06977d":"#### \n## <font color='Purple'> Resampling Techniques \u2014 Oversample minority class<\/font>","fd429a2c":"#### Study Summary Statistics","49dd4d91":"#### \n#### <font color='green'> Train and test model<\/font>\n","b50e9f00":"## <font color='DarkPink'> NLP Pre-processing Summary:<\/font>","200b4baa":"## <font color='DarkPink'> NLP text summary statistics<\/font>","4c7084dd":"## <font color='Purple'> Resampling Techniques \u2014 SMOTE - Generate synthetic samples - upsample smaller class<\/font>","d8603c6a":"<a id=\"feature-engineering\"><\/a>\n## <font color='Red'> Feature Engineering<\/font>","0f519e70":"Using PYTHONHASHSEED that allows to set a fixed value for the hash seed secret, this enables to have repeated same results on multiple runs","a041b7c2":"#### Study Correlation","3ee5b7ab":"**Accident Levels**","420b2908":"#### Get the Length of each line and find the maximum length\n\nAs different lines are of different length. We need to pad the our sequences using the max length.","09f39b85":"**Observations**\n\n* For both accident levels, the incidence of Employee is higher at low accident levels, but the incidence of Third parties seems to be slightly higher at high accident levels.","55604f27":"#### <font color='green'> 5. ANN  Multiclass classification - Target variable - One hot encoded with SMOTE data<\/font>\n\n\nIn this section, we will create a classification model that uses categorical columns and tf-idf features from accident description and one-hot encoded target variable. We can use simple densely connected neural networks to make predictions.","e4b0e6d2":"**Observations**\n\n* Proportion of accident levels in each gender is not equal and males have a higher accident levels than females.\n* There are many low risks at general accident level, but many high risks at potential accident level.","1a9ccfad":"# \n# <font color='Red'>Settings<\/font>\n","10c89f69":"#### \n#### <font color='green'>3. Modelling - Logistic Regression - Oversampling<\/font>\n","18da8b3b":"There are 7 duplicate records","337ab4d6":"We have the balanced dataset, with smote upsampling","bfab272e":"Now the data is balanced across 5 levels","f37a4e08":"We have train accuracy of 20% and Test accuracy of 11.9%","f6e0bc65":"Beacause the dataset is highly imbalanced, we need to balance it before we do ML","27a2b713":"To design a clickable UI in Tkinter \/ Flask \/Django Python UI which can automate tasks performed - like input the user values and generate the estimated Accident  levels.\n","83ef285a":"##### f. Accident Levels by Month - Is the distribution of accident levels and potential accident levels differ significantly in different months?","d7764812":"1.Accuracy continually riseS during training. \n2.As expected, we see the learning curves for accuracy on the test dataset plateau, indicating that the model has no longer overfit the training dataset and it is generalized model.","f2e4dad4":"#### Check Outliers\n\nAs we know, there is no concept of outliers detection in categorical variables(nominal and ordinal), as each value is count as labels. Let's check the unique and frequency(mode) of each variable.","681c3b0d":"#### Shape of the data","b475075e":"## <font color='Green'> Variable Creation - TFIDF Features<\/font>","2b581ce8":"<a id=\"ann-models-clas-to-num-problem\"><\/a>\n\n#### <font color='orange'> Convert Classification to Numeric problem<\/font>\n\nIn this section, we will create a classification model that uses categorical columns and tf-idf features from accident description and label encoded target variable. We can use simple densely connected neural networks to make predictions.\n\nSince we have ordinal relationship between each category in target variable, we have considered this one as numerical\/regression problem and try to observe the ANN behaviour.\n","124b2e95":"**Observations**\n\n* Both of the two accident level have the tendency that non-severe levels decreased throughout the year, but severe levels did not changed much, and some of these levels increased slightly in the second half of the year."}}