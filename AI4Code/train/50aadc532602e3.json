{"cell_type":{"e4477a5f":"code","73d9507d":"code","e72a7f21":"code","71edf44a":"code","3fbf0821":"code","663f94a6":"code","8f9eff9a":"code","61ea0e53":"code","ac2e2d7e":"code","12b4cff5":"code","df31409e":"code","25721dfd":"code","d9ccf709":"code","91534c5e":"code","a620b543":"code","526cb6b7":"code","1f98dc46":"code","04421970":"code","2082e247":"code","bdfc7a2f":"code","c5eb5aa6":"code","2620c19d":"code","361d05a3":"code","ee8c45e3":"code","ce0809d7":"code","234ce0f1":"code","4266b673":"code","3790e5c9":"code","f34ba7b9":"code","05e34f30":"code","179acb68":"code","866b6850":"code","b57b86c6":"code","1a6e0749":"code","235aa0fd":"code","79c9c623":"code","6fb7e570":"code","59b9f4ad":"code","f8b48388":"code","9a22f480":"code","2890de39":"code","4bc6f62b":"markdown","1f705442":"markdown","6e4aa7b3":"markdown","d534a8e2":"markdown","40bd0e81":"markdown","51561416":"markdown","81dfb729":"markdown","9767613e":"markdown","7a60b068":"markdown","06763a4d":"markdown","a488d74b":"markdown","76d0a9f7":"markdown","c0cda15a":"markdown","b7142fd2":"markdown","e0caf87d":"markdown","c37f5849":"markdown","e97fa012":"markdown","b37fc36a":"markdown","ae0ffaea":"markdown","d34a0225":"markdown","f3473481":"markdown","8b30e60f":"markdown","a9bc8845":"markdown","a075cd99":"markdown","1668d628":"markdown","c10f82ea":"markdown","322ae540":"markdown","f3420158":"markdown","c6f9ec7e":"markdown","119eb6bf":"markdown","8890da9a":"markdown","944d680e":"markdown","e1b22966":"markdown","b41aca75":"markdown","275d76d3":"markdown"},"source":{"e4477a5f":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import PowerTransformer,StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GridSearchCV\n# Import train_test_split()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport  tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom itertools import combinations\n#import smong \nfrom sklearn.linear_model import LinearRegression, RidgeCV\nimport category_encoders as ce\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nimport warnings\nwarnings.filterwarnings('ignore')","73d9507d":"# import lux\n# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\n# Preview the data\ntrain.head()","e72a7f21":"train.duplicated(subset='id', keep='first').sum()","71edf44a":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","3fbf0821":"cat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","663f94a6":"train['target'].describe()","8f9eff9a":"# Create arrays for the features and the response variable\ny = train['target']\nX = train.drop(['id','target'], axis=1)","61ea0e53":"# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))","ac2e2d7e":"# select non-numeric columns\ncat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns","12b4cff5":"# select the float columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","df31409e":"num_columns=['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\ncat_columns=['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']\nall_columns = (num_columns+cat_columns)\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","25721dfd":"if set(all_columns) == set(train.drop(['id','target'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","d9ccf709":"cat_columns2=['cat0', 'cat1', 'cat2']\ncat_columns1=['cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\nnum_columns1=[ 'cont1', 'cont2', 'cont3', 'cont4', 'cont5',  'cont7',  'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nnum_columns2=['cont0','cont6','cont8']\nall_columns1=cat_columns1+cat_columns2+num_columns1+num_columns2\nif set(all_columns) == set(train.drop(['id','target'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","91534c5e":"Encoder = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            #OneHotEncoder(handle_unknown='ignore'),\n            #LabelEncoder(),\n            OrdinalEncoder() ,\n            #SparseInteractions(degree=2)\n              )","a620b543":"Scaler  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        RobustScaler(),\n                        # PowerTransformer(),\n                        # StandardScaler(),\n                        # MinMaxScaler(),\n                        # QuantileTransformer\n)","526cb6b7":"cross_validation_design = KFold(n_splits=5,\n                                shuffle=True,\n                                random_state=77)\n\ncross_validation_design","1f98dc46":"class OutlierReplace(BaseEstimator,TransformerMixin):\n    def __init__(self,factor=1.5):\n        self.factor = factor\n\n    def outlier_removal(self,X,y=None):\n        X = pd.Series(X).copy()\n        qmin=X.quantile(0.05)\n        qmax=X.quantile(0.95)\n        q1 = X.quantile(0.25)\n        q3 = X.quantile(0.75)\n        iqr = q3 - q1\n        lower_bound = q1 - (self.factor * iqr)\n        upper_bound = q3 + (self.factor * iqr)\n        #X.loc[((X < lower_bound) | (X > upper_bound))] = np.nan \n        X.loc[X < lower_bound] = qmin\n        X.loc[X > upper_bound] = qmax\n        return pd.Series(X)\n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X,y=None):\n        return X.apply(self.outlier_removal)","04421970":"from xgboost import XGBRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.preprocessing import PolynomialFeatures\n# Random HyperParameters\nxgb_params0 ={'max_depth': 2,\n 'learning_rate': 0.07123779922425576,\n 'gamma': 0.30000000000000004,\n 'min_child_weight': 3, \n 'subsample': 0.9,\n 'colsample_bytree': 0.9,\n 'reg_alpha': 0.0013832469277755268,\n 'reg_lambda': 0.035255154051375526}\n###################\nxgb_params1 = {'n_estimators': 7000,\n            'learning_rate': 0.16,\n            'subsample': 0.96,\n            'colsample_bytree': 0.12,\n            'max_depth': 2,\n            'booster': 'gbtree', \n            'reg_lambda': 100.1,\n            'reg_alpha': 15.9,\n            'random_state':40}\n###################\nxgb_params2={'n_estimators': 5500, \n 'max_depth': 7, \n 'learning_rate': 0.04228058062291333,\n 'gamma': 0.7000000000000001, \n 'min_child_weight': 7,\n 'subsample': 0.8,\n 'colsample_bytree': 0.1,\n 'reg_alpha': 11.70236964019307,\n 'reg_lambda': 0.030505390929992562}\n###################\n\nxgb_params3 = {\n    #'tree_method':'gpu_hist',         ## parameters for gpu\n    #'gpu_id':0,                       #\n    #'predictor':'gpu_predictor',      #\n    'n_estimators': 10000,\n    'learning_rate': 0.03628302216953097,\n    'subsample': 0.7875490025178415,\n    'colsample_bytree': 0.11807135201147481,\n    'max_depth': 3,\n    'booster': 'gbtree', \n    'reg_lambda': 0.0008746338866473539,\n    'reg_alpha': 23.13181079976304,\n    'n_jobs':-1,\n    'random_state':40}\n\n\n###################\nXGBR0 = XGBRegressor(**xgb_params0,\n                    objective='reg:squarederror', \n                    #early_stopping_rounds=100 ,\n                    tree_method='gpu_hist',\n                    gpu_id=0, \n                    predictor=\"gpu_predictor\"\n                   )\n###################\nXGBR1 = XGBRegressor(**xgb_params1,\n                    objective='reg:squarederror', \n                    #early_stopping_rounds=100 ,\n                    tree_method='gpu_hist',\n                    gpu_id=0, \n                    predictor=\"gpu_predictor\"\n                   )\n###################\nXGBR2 = XGBRegressor(**xgb_params2,\n                    objective='reg:squarederror', \n                    #early_stopping_rounds=100 ,\n                    tree_method='gpu_hist',\n                    gpu_id=0, \n                    predictor=\"gpu_predictor\")\n###################\n                   \nXGBR3 = XGBRegressor(**xgb_params3,\n                    objective='reg:squarederror', \n                    #early_stopping_rounds=100 ,\n                    tree_method='gpu_hist',\n                    gpu_id=0, \n                    predictor=\"gpu_predictor\")\nXGBR31 = XGBRegressor(**xgb_params3,\n                    objective='reg:squarederror', \n                    #early_stopping_rounds=100 ,\n                    #tree_method='gpu_hist',\n                    #gpu_id=0, \n                    #predictor=\"gpu_predictor\"\n                     )\n###################\n# Cat Features  \nCatBoostEncoder = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.cat_boost.CatBoostEncoder(),\n            #SparseInteractions(degree=2)\n              )\nOrdinalencoder = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            ce.ordinal.OrdinalEncoder(),\n           # SparseInteractions(degree=2)\n              )\n###################\nRobustscaler  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        RobustScaler()\n)\n\nMaxAbsScaler1  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        MaxAbsScaler()\n)\nPowertransformer = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        PowerTransformer())\nquantiletransformer = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        #PolynomialFeatures(degree=2),\n                        QuantileTransformer())\n\n###################\n\nOrdinalEncoder_MaxAbsScaler = make_column_transformer(\n    ( OrdinalEncoder , cat_columns),\n    ( MaxAbsScaler1, num_columns))\n###################\n\nOrdinalEncoder_Powertransformer = make_column_transformer(\n    ( OrdinalEncoder , cat_columns),\n    ( Powertransformer, num_columns))\n\n##################\nOrdinalEncoder_RobustScaler = make_column_transformer(\n    ( Ordinalencoder , cat_columns),\n    ( Robustscaler, num_columns))\n\n###################\nCatBoostEncoder_RobustScaler = make_column_transformer(\n    ( CatBoostEncoder , cat_columns),\n    ( RobustScaler, num_columns) )\n################### \nOrdinalquantiletransformer = make_column_transformer(\n    ( Ordinalencoder , cat_columns),\n    ( quantiletransformer , num_columns))    \n #################### Model 0  \nXGBpipe0 = Pipeline([\n        ('preprocess', OrdinalEncoder_RobustScaler),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR0)])      \n #################### Model 1   \nXGBpipe1 = Pipeline([\n        ('preprocess', OrdinalEncoder_RobustScaler),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR1)]) \n    \n#################### Model 2 \nXGBpipe2 = Pipeline([\n        ('preprocess', CatBoostEncoder_RobustScaler),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR1)])\n    \n#################### Model 3    \nXGBRpipe3= Pipeline([\n        ('preprocess', OrdinalEncoder_MaxAbsScaler),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR1)])  \n    \n#################### Model 4   \nXGBRpipe4 = Pipeline([\n        ('preprocess', OrdinalEncoder_Powertransformer),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', XGBR1)])\n#################### Model 5  \nXGBRpipe5= Pipeline([\n    ('preprocess', Ordinalquantiletransformer),\n    # ('dim_red', SelectKBest(mutual_info_regression, k=20)),\n    # ('interactions', SparseInteractions(degree=2)),\n    ('classifier', XGBR2)])  \n#################### Model 6\nXGBRpipe6= Pipeline([\n    ('preprocess', OrdinalEncoder_RobustScaler),\n    # ('dim_red', SelectKBest(mutual_info_regression, k=20)),\n    # ('interactions', SparseInteractions(degree=2)),\n    ('classifier', XGBR3)]) \n\n#################### Model 7  \ncat_columns_reduced=[ 'cat1',  'cat3',  'cat5',  'cat7', 'cat8',\n       'cat9']\nOrdinalEncoder_RobustScaler_reduced = make_column_transformer(\n    ('drop',['cat0','cat2' ,'cat4','cat6']),\n    ( Ordinalencoder , cat_columns_reduced),\n    ( Robustscaler, num_columns)\n)   \nXGBRpipe7= Pipeline([\n    ('preprocess', OrdinalEncoder_RobustScaler_reduced),\n    # ('dim_red', SelectKBest(mutual_info_regression, k=20)),\n    # ('interactions', SparseInteractions(degree=2)),\n    ('classifier', XGBR3)]) \n###############################################\n# Define the model \nLGBM_params = {'subsample': 0.8,\n     'reg_lambda': 0.002070681206144435,\n     'reg_alpha': 23.13,\n     'n_estimators': 7000,\n     'min_child_weight': 6,\n     'max_depth': 2,\n     'learning_rate': 0.05,\n     'colsample_bytree': 0.11807135201147481}\nmodelLGBMRegressor1 = lgbm.LGBMRegressor(**LGBM_params,\n                                   metric = 'rmse', \n                                   objective= \"rmse\",\n                                   boosting_type= 'gbdt',\n                                   device_type='gpu',\n                                   # predictor=\"gpu_predictor\", \n                                   n_jobs = -1,\n                                   min_child_samples =  27,\n                                   #max_bin = 520,\n                                   bagging_seed= 42,\n                                  # predictor=\"gpu_predictor\"\n                                   #cat_l2 = 0.025083670064082797\n                                  ) \nLGBMpipe1 = Pipeline([\n        ('preprocess', OrdinalEncoder_RobustScaler),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', modelLGBMRegressor1)])\n############################\nLGBM_params = {'n_estimators': 7000,\n            'learning_rate':0.034923843361431936,\n            'subsample': 0.7000000000000001,\n            'colsample_bytree':  0.1,\n            'max_depth': 3,\n            'booster': 'gbtree', \n            'reg_lambda': 100.1,\n            'reg_alpha':0.5333109437994918,\n            'reg_lambda': 97.48121524546883,\n            'random_state':40}\nmodelLGBMRegressor2 = lgbm.LGBMRegressor(metric = 'rmse', \n                                   objective= \"rmse\",\n                                   boosting_type= 'gbdt',\n                                   device_type='gpu',\n                                   n_jobs = -1,\n                                   min_child_samples =  27,\n                                   #max_bin = 520,\n                                   bagging_seed= 42,\n                                 #  predictor=\"gpu_predictor\"\n                                   #cat_l2 = 0.025083670064082797\n                                  ) \nLGBMpipe2 = Pipeline([\n        ('preprocess', OrdinalEncoder_RobustScaler),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', modelLGBMRegressor2)])\n\n##############################\ncatmodel = CatBoostRegressor(iterations=9000,\n                              learning_rate=0.5932074045035394,\n                              loss_function=\"RMSE\",\n                              random_state=42,\n                              verbose=0,\n                              thread_count=4,\n                              depth=1,\n                              reg_lambda=72.51667229513434,\n                              #l2_leaf_reg=3.28,\n                              task_type=\"GPU\",\n                              devices='0:1' )\ncatpipe = Pipeline([\n        ('preprocess', OrdinalEncoder_RobustScaler),\n        # ('dim_red', SelectKBest(f_regression, k=20)),\n        # ('interactions', SparseInteractions(degree=2)),\n        ('classifier', catmodel)])\n# Other models to try in the futur \n# from sklearn.ensemble import HistGradientBoostingRegressor\n# https:\/\/www.kaggle.com\/carlmcbrideellis\/histogram-gradient-boosting-regression-example","2082e247":"print(cat_columns)\nprint(num_columns )","bdfc7a2f":"cat_columns2=['cat0', 'cat1', 'cat2']\ncat_columns1=['cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\nnum_columns1=[ 'cont1', 'cont2', 'cont3', 'cont4', 'cont5',  'cont7',  'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nnum_columns2=['cont0','cont6','cont8']\nall_columns1=cat_columns1+cat_columns2+num_columns1+num_columns2\nif set(all_columns) == set(train.drop(['id','target'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","c5eb5aa6":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","2620c19d":"listmodels =[XGBR0 ,XGBR1, XGBR2  ,XGBR3 ,modelLGBMRegressor1 ,\n             modelLGBMRegressor2 ,catmodel ]\n\nlistpipe =[XGBpipe0   ,XGBpipe1 ,XGBpipe2 ,XGBRpipe3,XGBRpipe4 ,\n XGBRpipe5,XGBRpipe6, XGBRpipe7,LGBMpipe1 ,\nLGBMpipe2,catpipe ]\naveraged_models = AveragingModels(models = (XGBR0 ,XGBR3 ,modelLGBMRegressor1 ,\n             modelLGBMRegressor2 ,catmodel  ))\naveraged_models_pipe = Pipeline([('data_cleaning', OrdinalEncoder_RobustScaler),\n                        ('Stack_reg1', averaged_models)\n                        ])","361d05a3":"from sklearn import set_config\nset_config(display='diagram')\naveraged_models_pipe","ee8c45e3":"#averaged_models_pipe.fit(X,y)","ce0809d7":"N_FOLD = 3\ntest_final= test.drop(['id'], axis=1)\n#Setting the kfold parameters\nkf = KFold(n_splits=N_FOLD, shuffle=True, random_state = 50)\noof_preds = np.empty((X.shape[0],))\npredictions = np.empty((test.shape[0],),float)\nmodel_fi = 0\nmean_rmse = 0\nUseful_columns= ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6',\n       'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nfor num, (train_idx, valid_idx) in enumerate(kf.split(X[0:100])):\n    # split the train data into train and validation\n    X_train = X.iloc[train_idx][Useful_columns]\n    X_valid = X.iloc[valid_idx][Useful_columns]\n    y_train = y.iloc[train_idx]\n    y_valid = y.iloc[valid_idx]\n    \n    # Define the Pipeline\/model \n    XGBR61 = XGBRegressor(**xgb_params3,\n                    objective='reg:squarederror', \n                    #early_stopping_rounds=100 ,\n                    tree_method='gpu_hist',\n                    gpu_id=0, \n                    predictor=\"gpu_predictor\"\n                            )\n    XGBRpipe61= Pipeline([\n    ('preprocess', OrdinalEncoder_RobustScaler),\n    # ('dim_red', SelectKBest(mutual_info_regression, k=20)),\n    # ('interactions', SparseInteractions(degree=2)),\n    ('classifier', XGBR61)])  \n    # Make a copy to avoid changing original data\n    X_valid_eval=X_valid.copy()\n    # Bundle preprocessing only in a temp pipeline\n    eval_set_pipe = OrdinalEncoder_RobustScaler\n    # fit transform X_valid.copy()\n    X_valid_eval = eval_set_pipe.fit(X_train).transform (X_valid_eval)\n    # Train the model\n    XGBRpipe61.fit(X_train, y_train,\n                  classifier__eval_set=[(X_valid_eval,y_valid)],\n                   classifier__early_stopping_rounds=100,\n                  classifier__verbose = 0)\n    \n    #Mean of the predictions\n    predictions += XGBRpipe61.predict(test_final) \/ N_FOLD\n    \n    #Mean of feature importance\n    #model_fi += XGBROrdinalEncoderRobustScalerwithoutreduction_features144.feature_importances_ \/ N_FOLD \n    \n    #Out of Fold predictions\n    oof_preds[valid_idx] = XGBRpipe61.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ N_FOLD\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","234ce0f1":"test_final= test.drop(['id'], axis=1)\ndef Stacking(model,eval_set_pipe,train,y,test,n_fold=3):\n    N_FOLD = n_fold\n    #test_final= test.drop(['id'], axis=1)\n    #Setting the kfold parameters\n    kf = KFold(n_splits=N_FOLD, shuffle=True, random_state = 50)\n\n     #oof_preds = np.zeros((X.shape[0],)\n    #val_pred=np.empty((0,1),float)   \n    val_pred=[]\n    #predictions = np.zeros((test.shape[0],),float)\n    #test_pred=np.empty((test.shape[0],1),float)\n    test_pred=[]\n    model_fi = 0\n    mean_rmse = 0\n    Useful_columns= ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n           'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6',\n           'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n    for num, (train_idx, valid_idx) in enumerate(kf.split(X)):\n        # split the train data into train and validation\n        X_train = X.iloc[train_idx][Useful_columns]\n        X_valid = X.iloc[valid_idx][Useful_columns]\n        y_train = y.iloc[train_idx]\n        y_valid = y.iloc[valid_idx]\n\n        \n        # Make a copy to avoid changing original data\n        X_valid_eval=X_valid.copy()\n        # Bundle preprocessing only in a temp pipeline\n        #eval_set_pipe = OrdinalEncoder_RobustScaler\n        # fit transform X_valid.copy()\n        X_valid_eval = eval_set_pipe.fit(X_train).transform (X_valid_eval)\n        # Train the model\n        model.fit(X_train, y_train,\n                      classifier__eval_set=[(X_valid_eval,y_valid)],\n                       classifier__early_stopping_rounds=100,\n                      classifier__verbose = 0)\n\n        #Mean of the predictions\n        #test_pred += pipe.predict(test_final) \/ N_FOLD\n        #test_pred=np.append(test_pred,model.predict(test) )\n        \n        #test_pred=np.column_stack([test_pred,model.predict(test)])\n        test_pred.append(model.predict(test))\n        #Mean of feature importance\n        #model_fi += pipe.feature_importances_ \/ N_FOLD \n\n        #Out of Fold predictions\n        val_pred.append(model.predict(X_valid))\n        #val_pred.append(model.predict(X_valid))\n        # val_pred=np.append(val_pred,model.predict(X_valid))\n        fold_rmse = np.sqrt(mean_squared_error(y_valid, model.predict(X_valid)))\n        print(f\"Fold {num} | RMSE: {fold_rmse}\")\n\n        mean_rmse += fold_rmse \/ N_FOLD\n    print(f\"\\nOverall RMSE: {mean_rmse}\")\n    return np.mean(np.column_stack(test_pred), axis=1),np.hstack(val_pred)","4266b673":"listofmodels1=[XGBRpipe6,LGBMpipe1]\ndef get_val_test_data(eval_set_pipe=OrdinalEncoder_RobustScaler,n_fold=3, train=X,test=test_final,y=y,listofmodels=listofmodels1):\n    #oof_preds = np.zeros((X.shape[0],)\n    val_pred_final=[]                    \n    #predictions = np.zeros((test.shape[0],),float)\n    #test_pred=np.empty((test.shape[0],1),float)\n    test_pred_final=[]\n    for model in listofmodels :\n        test_pred ,val_pred=Stacking(model=model,eval_set_pipe=OrdinalEncoder_RobustScaler,n_fold=5, train=X,test=test_final,y=y)\n        # pred :\n        val_pred_final.append(val_pred)\n        # test :\n        test_pred_final.append(test_pred)\n    return  np.column_stack(val_pred_final), np.column_stack(test_pred_final)  ","3790e5c9":"from datetime import datetime\nstart = datetime.now()\n# your code\n#val_pred_final,test_pred_final=get_val_test_data(listofmodels1)\nend = datetime.now()\ntime_taken = end - start\nprint('Time: ',time_taken) ","f34ba7b9":"#val_pred_final_df= pd.DataFrame(val_pred_final,columns=['pred1','pred2'])\n#test_pred_final_df= pd.DataFrame(test_pred_final,columns=['test1','test2'])\n#val_pred_final_df.shape","05e34f30":"#meata_model_scratch = LinearRegression()\n#meata_model_scratch.fit(val_pred_final_df,y)","179acb68":"#scratch_predictions = meata_model_scratch.predict(test_pred_final_df)","866b6850":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.empty((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X.iloc[list(train_index)], y.iloc[list(train_index)])\n                y_pred = instance.predict(X.iloc[list(holdout_index)])\n                out_of_fold_predictions[list(holdout_index), i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","b57b86c6":"listofmodels1=[XGBRpipe6,LGBMpipe1]\nlasso = Lasso(alpha =0.0005)\nstacked_averaged_models = StackingAveragedModels(base_models = (XGBRpipe6,LGBMpipe1),\n                                                 meta_model = lasso)","1a6e0749":"from sklearn import set_config\nset_config(display='diagram')\nstacked_averaged_models.fit(X,y)","235aa0fd":"preds = stacked_averaged_models.predict(X)\nrmse = np.sqrt(mean_squared_error(y,preds))\nprint(f\" | OOP_RMSE: {rmse}\")","79c9c623":"from sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.svm import LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import StackingRegressor\n\n\nestimators = [('XGBR',XGBRpipe6),('LGBM',LGBMpipe1)]\n     \nStack_Sklearn = StackingRegressor(estimators=estimators,final_estimator=lasso)\nStack_Sklearn_pipe = Pipeline([\n  #  ('data_cleaning', data_preprocess),\n                      ('Stack_Sklearn', Stack_Sklearn)\n                        ])\n\nStack_Sklearn_pipe.fit(X, y) \npreds = Stack_Sklearn_pipe.predict(X)\nrmse = np.sqrt(mean_squared_error(y,preds))\nprint(f\" | Stack_Sklearn_RMSE: {rmse}\")","6fb7e570":"from mlxtend.regressor import StackingRegressor\nfrom mlxtend.data import boston_housing_data\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\n# Initializing models\n\n\nStack_mlxtend =StackingRegressor(regressors=[XGBRpipe6,LGBMpipe1], \n                         meta_regressor=lasso)\nStack_mlxtend_pipe = Pipeline([\n  #  ('data_cleaning', data_preprocess),\n                      ('Stack_Sklearn', Stack_mlxtend)\n                        ])\n\nStack_mlxtend_pipe.fit(X, y) \npreds = Stack_mlxtend_pipe.predict(X)\nrmse = np.sqrt(mean_squared_error(y,preds))\nprint(f\" | mlxtend_RMSE: {rmse}\")","59b9f4ad":"import time \ndef plot_regression_results(ax, y_true, y_pred, title, scores, elapsed_time):\n    \"\"\"Scatter plot of the predicted vs true targets.\"\"\"\n    ax.plot([y_true.min(), y_true.max()],\n            [y_true.min(), y_true.max()],\n            '--r', linewidth=2)\n    ax.scatter(y_true, y_pred, alpha=0.2)\n\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n    ax.set_xlim([y_true.min(), y_true.max()])\n    ax.set_ylim([y_true.min(), y_true.max()])\n    ax.set_xlabel('True')\n    ax.set_ylabel('Predicted')\n    extra = plt.Rectangle((0, 0), 0, 0, fc=\"w\", fill=False,\n                          edgecolor='none', linewidth=0)\n    ax.legend([extra], [scores], loc='upper left')\n    title = title + 'n Evaluation in {:.2f} seconds'.format(elapsed_time)\n    ax.set_title(title)","f8b48388":"# to evaluate the model\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport math\nfig, axs = plt.subplots(2, 2, figsize=(10, 15))\naxs = np.ravel(axs)\nerrors_list=[]\nestimators=[('OOP_stacked',stacked_averaged_models),\n           ('Stack_Sklearn',Stack_Sklearn_pipe),\n           ('Stack_mlxtend',Stack_mlxtend_pipe)]\nfor ax, (name, est) in zip(axs, estimators ):\n    start_time = time.time()\n    model = est.fit(X_train, y_train)\n                     \n    elapsed_time = time.time() - start_time\n    \n    pred = model.predict(X_test)\n    errors = y_test - model.predict(X_test)\n    errors_list.append(errors)\n    test_r2= r2_score(np.exp(y_test), np.exp(pred))\n    #This metric measures the ratio between actual values and predicted values and takes the log of the predictions and actual values. \n    #Use this instead of RMSE if an under-prediction is worse than an over-prediction. You can also use this when you don\u2019t want to penalize large differences when both of the values are large numbers.\n    test_rmsle=math.sqrt(mean_squared_log_error(y_test,pred))\n    rmse = np.sqrt(mean_squared_error(y_test,pred))\n    plot_regression_results(ax,y_test,pred,name,(r'$R^2={:.3f}$' + '\\n' + \n                            r'$RMSLE={:.3f}$'+'\\n' + r'$RMSE={:.3f}$').format(test_r2,test_rmsle,rmse),elapsed_time)\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)\nplt.show()","9a22f480":"titles = ['OOP_stacked','Stack_Sklearn','Stack_mlxtend'] \nf,a = plt.subplots(3,1)\na = a.ravel()\nfor idx,ax in enumerate(a):\n    ax.hist(errors_list[idx])\n    ax.set_title(titles[idx])\nplt.tight_layout()","2890de39":"#preds_valid = Stack_regtype2_pipe.predict(X_test)\n#print(mean_squared_error(y_test, preds_valid, squared=False))\n#test_final= test.drop(['id'], axis=1)\n# Use the model to generate predictions\n#predictions = Stack_regtype2_pipe.predict(test_final)\n# Save the predictions to a CSV file\nStack_Sklearn_preds=Stack_Sklearn_pipe.predict(test_final)\noutput1 = pd.DataFrame({'Id': test.id,'target': Stack_Sklearn_preds})\noutput1.to_csv('Stack_Sklearn_preds.csv', index=False)\nStack_mlxtend_preds=Stack_mlxtend_pipe.predict(test_final)\noutput2 = pd.DataFrame({'Id': test.id,'target': Stack_mlxtend_preds})\noutput2.to_csv('Stack_mlxtend_preds.csv', index=False)\n\n# Save the predictions to a CSV file\nOOP_scratch_predictions=stacked_averaged_models.predict(test_final)\noutput = pd.DataFrame({'Id': test.id,'target': OOP_scratch_predictions})\noutput.to_csv('scratch_predictions.csv', index=False)","4bc6f62b":"| mlxtend_RMSE: 0.7091052678099321","1f705442":"# Ensembling : \n\n## 1- Simplest Stacking Regressor approach: Averaging Base models\n\nWe begin with this simple approach of averaging base models. Build a new class to extend scikit-learn with our model and also to leverage encapsulation and code reuse. Averaged base models class","6e4aa7b3":"# Convert Dtypes : ","d534a8e2":"# Data Modeling\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.","40bd0e81":"**Second we  define a function to make predictions on n-folds of train and test dataset. This function returns the predictions for train and test for each model.**","51561416":"**Use this meta model dor final predictions :**","81dfb729":"### Duplicates ","9767613e":"## Define the model features and target\n### Extract X and y ","7a60b068":"# 2 Stacking : Adding a Meta-model\n\nThe meta-model is used to find the pattern between the base model predictions as features and actual predictions as the target variables.\nA single model( Meta-model) is used to learn how to best combine the predictions from the contributing models \n\n\n","06763a4d":"# Predifined lib\n## Sklearn \n","a488d74b":"## Advantages and Disadvantages of Stacking\n\nLike all other methods in machine learning, stacking has advantages and disadvantages. Here are some of the advantages of stacking:\n\n    Stacking can yield improvements in model performance.\n    Stacking reduces variance and creates a more robust model by combining the predictions of multiple models.\n\nKeep in mind that stacking also has the following disadvantages:\n\n    Stacked models can take significantly longer to train than simpler models and require more memory.\n    Generating predictions using stacked models will usually be slower and more computationally expensive. This drawback is important to consider if you are planning to deploy a stacked model into production.\n\n# Summary\n\nStacking is a great way to take advantage of the strengths of different models by combining their predictions. This method has been used to win machine learning competitions and thanks to Scikit-learn, it is very easy to implement. However, the performance improvements that come from stacking do come with a price in the form of longer training and inference times.\n\nreference : \n\nbest links: \n\nhttps:\/\/towardsdatascience.com\/ensemble-learning-stacking-blending-voting-b37737c4f483\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/12\/improve-predictive-model-score-stacking-regressor\/\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\/\n\n**oop staking from scratch :**\n\nhttps:\/\/github.com\/FernandoLpz\/Stacking-Blending-Voting-Ensembles\/blob\/master\/stacking.py\n\nhttps:\/\/www.kaggle.com\/santiagovaldarrama\/30-days-of-ml-stacked-ensembles\n\nhttps:\/\/www.kaggle.com\/jeonghojae\/blending-and-stacking\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/12\/improve-predictive-model-score-stacking-regressor\/\n\n\n\n**sklearn has a prebuild stacking models implementation :**\n\n**sklearn stacking :**\n\nhttps:\/\/towardsdatascience.com\/a-practical-guide-to-stacking-using-scikit-learn-91e8d021863d\n\n**Add deep learing sklearn stackregresor  :**\n\n\nhttps:\/\/github.com\/sailajak2003\/ensemble-\/blob\/master\/ensemble_regression.ipynb\n\n\nhttps:\/\/sailajakarra.medium.com\/ensemble-scikit-learn-and-keras-be93206c54c4\n\n**stacking as many as you want  deepstack lib  :**\n\nhttps:\/\/towardsdatascience.com\/the-power-of-ensembles-in-deep-learning-a8900ff42be9\n\nhttps:\/\/github.com\/jcborges\/DeepStack\n\n**mlxtend : so beautiful representation :**\n\nhttps:\/\/ealizadeh.com\/blog\/mlxtend-library-for-data-science\n\nhttps:\/\/stackoverflow.com\/questions\/48079973\/xgboost-sample-weights-vs-scale-pos-weight\n\nhttps:\/\/stackoverflow.com\/questions\/67303447\/how-to-use-downsampling-and-configure-class-weight-parameter-when-using-xgboost\n\nhttps:\/\/towardsdatascience.com\/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205","76d0a9f7":"## Create test and train groups\n\nNow we\u2019ve got our dataframe ready we can split it up into the train and test datasets for our model to use. We\u2019ll use the Scikit-Learn train_test_split() function for this. By passing in the X dataframe of raw features, the y series containing the target, and the size of the test group (i.e. 0.1 for 10%), we get back the X_train, X_test, y_train and y_test data to use in the model.","c0cda15a":"## check that we have all column","b7142fd2":"##  Target \n###  exploring target data main statistics","e0caf87d":"##  What should we do for each colmun\n### Separate features by dtype\n\nNext we\u2019ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I\u2019ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don\u2019t want to include the target income column, so I\u2019ve dropped that.\n### Cat Features ","c37f5849":"# What is ensembling?\n\nIn general, ensembling is a technique of combining two or more algorithms of similar or dissimilar types called base learners. This is done to make a more robust system which incorporates the predictions from all the base learners. It can be understood as conference room meeting between multiple traders to make a decision on whether the price of a stock will go up or not.\n\nSince all of them have a different understanding of the stock market and thus a different mapping function from the problem statement to the desired outcome. Therefore, they are supposed to make varied predictions on the stock price based on their own understandings of the market.\n\nNow we can take all of these predictions into account while making the final decision. This will make our final decision more robust, accurate and less likely to be biased. The final decision would have been opposite if one of these traders would have made this decision alone.\n\nYou can consider another example of a candidate going through multiple rounds of job interviews. The final decision of candidate\u2019s ability is generally taken based on the feedback of all the interviewers. Although a single interviewer might not be able to test the candidate for each required skill and trait. But the combined feedback of multiple interviewers usually helps in better assessment of the candidate.\n\n## Types of ensembling\n\nSome of the basic concepts which you should be aware of before we go into further detail are:\n\n**Averaging:** It\u2019s defined as taking the average of predictions from models in case of regression problem or while predicting probabilities for the classification problem.\n\nMajority vote: It\u2019s defined as taking the prediction with maximum vote \/ recommendation from multiple models predictions while predicting the outcomes of a classification problem.\n\nWeighted average: In this, different weights are applied to predictions from multiple models then taking the average which means giving high or low importance to specific model output.\n\n\n**Boosting:**\n\nBoosting is a sequential technique in which, the first algorithm is trained on the entire dataset and the subsequent algorithms are built by fitting the residuals of the first algorithm, thus giving higher weight to those observations that were poorly predicted by the previous model.\n\nIt relies on creating a series of weak learners each of which might not be good for the entire dataset but is good for some part of the dataset. Thus, each model actually boosts the performance of the ensemble.\n\nIt\u2019s really important to note that boosting is focused on reducing the bias. This makes the boosting algorithms prone to overfitting. Thus, parameter tuning becomes a crucial part of boosting algorithms to make them avoid overfitting.\n\n\nSome examples of boosting are XGBoost, GBM, ADABOOST, etc.\n\n**Stacking:**\n\nIn stacking multiple layers of machine learning models are placed one over another where each of the models passes their predictions to the model in the layer above it and the top layer model takes decisions based on the outputs of the models in layers below i\n\nPractically speaking, there can be a countless number of ways in which you can ensemble different models. But these are some techniques that are mostly used:\n\nBagging: Bagging is also referred to as bootstrap aggregation. To understand bagging, we first need to understand bootstrapping. Bootstrapping is a sampling technique in which we choose \u2018n\u2019 observations or rows out of the original dataset of \u2018n\u2019 rows as well. But the key is that each row is selected with replacement from the original dataset so that each row is equally likely to be selected in each iteration. \n\n\n**Model Stacking**\n\nImprove your Predictive Model\u2019s Score using a Stacking Regressor\n\n**\u201cThe whole is greater than the sum of its parts.\u201d \u2013 Aristotle**\n\n![image.png](attachment:c77d97ae-75a4-4355-8ce5-8a87ba829fd9.png)!\n\nStacking is a way to ensemble multiple classifications or regression model. There are many ways to ensemble models, the widely known models are Bagging or Boosting. Bagging allows multiple similar models with high variance are averaged to decrease variance. Boosting builds multiple incremental models to decrease the bias, while keeping variance small.\n\nStacking (sometimes called Stacked Generalization) is a different paradigm. The point of stacking is to explore a space of different models for the same problem. The idea is that you can attack a learning problem with different types of models which are capable to learn some part of the problem, but not the whole space of the problem. So, you can build multiple different learners and you use them to build an intermediate prediction, one prediction for each learned model. Then you add a new model which learns from the intermediate predictions the same target. This final model is said to be stacked on the top of the others, hence the name. Thus, you might improve your overall performance, and often you end up with a model which is better than any individual intermediate model. Notice however, that it does not give you any guarantee, as is often the case with any machine learning technique.\n\n# How stacking works?\n\n    -We split the training data into K-folds just like K-fold cross-validation.\n    A base model is fitted on the K-1 parts and predictions are made for Kth part.\n\n    -We do for each part of the training data.\n    The base model is then fitted on the whole train data set to calculate its performance on the test set.\n\n    -We repeat the last 3 steps for other base models.\n\n    Predictions from the train set are used as features for the second level model.\n\n    -Second level model is used to make a prediction on the test set.\n\n![image.png](attachment:740e8c1e-19c3-47bd-8b81-3f98de2f7383.png)!\n\n\n# Blending\n\nBlending is also an ensemble technique that can help us to improve performance and increase accuracy. It follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions. In other words, unlike stacking, the predictions are made on the holdout set only. The holdout set and the predictions are used to build a model which is run on the test set. Here is a detailed explanation of the blending process:\n\n    -The train set is split into two parts, viz-training and validation sets.\n    -Model(s) are fit on the training set.\n    \n    -The predictions are made on the validation set and the test set.\n    \n    -The validation set and its predictions are used as features to build a new model.\n    \n    -This model is used to make final predictions on the test and meta-features.\n\nThe difference between stacking and blending is that Stacking uses out-of-fold predictions for the train set of the next layer (i.e meta-model), and Blending uses a validation set (let\u2019s say, 10-15% of the training set) to train the next layer.\n\ni will work in blending next notebook .","e97fa012":"## 2-1 Stacking from scratch step by step : \nWe first define a function to make predictions on n-folds of train and test dataset. This function returns the predictions for train and test for each model.\nBelow is a step-wise explanation for a simple stacked ensemble:\n\n    The train set is split into 10 parts.\n    \n![image.png](attachment:45d6c474-9fd5-41d3-b367-52fb7d4361e6.png)!\n\n    A base model (suppose a decision tree) is fitted on 9 parts and predictions are made for the 10th part. This is done for each part of the train set.\n    \n![image.png](attachment:e317febc-019e-430f-a7a9-1eaedaaf1089.png)!\n\n    The base model (in this case, decision tree) is then fitted on the whole train dataset.\n    Using this model, predictions are made on the test set.\n    \n![image.png](attachment:20882d49-d386-4163-b30e-a5abb559848d.png)\n\n    Steps 2 to 4 are repeated for another base model (say knn) resulting in another set of predictions for the train set and test set.\n    \n![image.png](attachment:926b30de-db5e-450e-888b-547dfef48411.png)\n\n    The predictions from the train set are used as features to build a new model. \n    \n![image.png](attachment:c129c7d8-65cb-453d-8485-ad4c3ed16755.png)\n\n\nThis model is used to make final predictions on the test prediction set.\n\n**Frist let's show how cv work for a single model**","b37fc36a":"**Now we\u2019ll create  base models :**","ae0ffaea":"## 2-2  OOP  Design : \nSteps:\n\n1. Split the data into 2 sets training and holdout set.\n2. Train all the base models in the training data.\n3. Test base models on the holdout dataset and store the predictions(out-of-fold predictions).\n4. Use the out-of-fold predictions made by the base models as input features, and the correct output as the target variable to train the meta-model.\n\n    The first three steps will be done iteratively for the k-folds based on the value of k. If k=5 then we will train the model on the 4 folds and predict on the holdout set (5th fold). Repeating this step for k-times (here,k=5) gives the out-of-fold predictions for the whole dataset. This will be done for all the base models.\n\n    Then meta-model will be trained using the out-of-predictions by all the models as X and the original target variable as y. Prediction of this meta-model will be considered as the final prediction.","d34a0225":"    Fold 0 | RMSE: 0.7217289573955751\n    Fold 1 | RMSE: 0.7168170011639267\n    Fold 2 | RMSE: 0.716900401110619\n    Fold 3 | RMSE: 0.7154363369590778\n    Fold 4 | RMSE: 0.7183025362169766\n\n    Overall RMSE: 0.717837046569235\n    [LightGBM] [Warning] Unknown parameter: predictor\n    Fold 0 | RMSE: 0.7226569416740861\n    [LightGBM] [Warning] Unknown parameter: predictor\n    Fold 1 | RMSE: 0.7173947596450057\n    [LightGBM] [Warning] Unknown parameter: predictor\n    Fold 2 | RMSE: 0.7173894695872729\n    [LightGBM] [Warning] Unknown parameter: predictor\n    Fold 3 | RMSE: 0.7154675440143379\n    [LightGBM] [Warning] Unknown parameter: predictor\n    Fold 4 | RMSE: 0.7189852970496661\n\n    Overall RMSE: 0.7183788023940736\n    Time:  0:15:13.279158","f3473481":"### Num\/Cat Features ","8b30e60f":"# Compare the performance ","a9bc8845":"**fit() \u2013** Cloning the base model and meta-model, Training the base model in 5fold cross-validation, and storing the out of fold predictions. Then training the meta-model using the out-of-fold-predictions.\n\n**predict() \u2013** base model predictions for X will be column stacked and then used as an input for meta-model to predict.","a075cd99":"## Mlextend Stacking ","1668d628":"# Define Baseline lgbm ","c10f82ea":"**Create a third model, LinearRegression, on the predictions of the decision tree and knn models.**","322ae540":"# Train Catboost \/ Xgboost \/ Lgbm\n## Define Baseline XGBR ","f3420158":"OOP_RMSE: 0.7109383367142464","c6f9ec7e":"### Num Features ","119eb6bf":"#  Submit to the competition\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.\n","8890da9a":"\n## Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","944d680e":"final prediction score : 0.74","e1b22966":"| Stack_Sklearn_RMSE: 0.7108556166688924","b41aca75":"## Num Features :","275d76d3":"## Compelete prerocess pipe for  Cat dara "}}