{"cell_type":{"902fbb14":"code","7f52d5a1":"code","583362e9":"code","977af2d3":"code","8896d1d9":"code","116b77ca":"code","ae2302dc":"code","09093b6a":"code","2e70dc6e":"code","cb144884":"code","301b4718":"code","8df2db1b":"code","fe7d23a8":"code","a71b26c5":"code","8be2587a":"code","68e9ae66":"code","522dab67":"code","6fc6c463":"code","8c6a91f7":"code","0e722959":"code","619d179e":"code","6a578e1b":"code","f346e740":"code","1913bf6c":"code","c6433780":"code","9fe503f3":"code","da5741fa":"code","558d2896":"code","6dab5f1e":"code","bc46ee85":"markdown","adea470b":"markdown","738a146c":"markdown","5aa65f1f":"markdown","cc9e4459":"markdown","72563dea":"markdown","b920d857":"markdown","17c85f51":"markdown","2177d774":"markdown","a3a74ff7":"markdown","cf5ffa07":"markdown","1d60508e":"markdown","54716c30":"markdown","6ec60af2":"markdown","21550622":"markdown","0c468734":"markdown","0e326522":"markdown","d3701138":"markdown","3cb73b18":"markdown","05e599c4":"markdown","b158ffb4":"markdown","8b017712":"markdown","244c4c1e":"markdown","3fe540cf":"markdown","7d381cec":"markdown","6db10c67":"markdown","db8fee11":"markdown","56c5b239":"markdown","20ec6c81":"markdown"},"source":{"902fbb14":"import pandas as pd\nimport numpy as np\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\nimport shutil\nfrom tensorflow.keras.callbacks import EarlyStopping","7f52d5a1":"with_maskImage = os.listdir('..\/input\/face-mask-dataset\/data\/with_mask')\nwithout_maskImage = os.listdir('..\/input\/face-mask-dataset\/data\/without_mask')","583362e9":"print(len(with_maskImage))\nprint(len(without_maskImage))","977af2d3":"os.mkdir('.\/train')\nos.mkdir('.\/train\/with_mask')\nos.mkdir('.\/val')\nos.mkdir('.\/val\/with_mask')\nos.mkdir('.\/test')\nos.mkdir('.\/test\/with_mask')\n\n\nos.mkdir('.\/train\/without_mask')\nos.mkdir('.\/val\/without_mask')\nos.mkdir('.\/test\/without_mask')","8896d1d9":"with_mask_train_len = int(np.round(0.6 * len(with_maskImage), 0))\nwith_mask_val_len = int(np.round(0.7 * len(with_maskImage), 0))\n\n\nfor i in range(with_mask_train_len):\n    shutil.copy(os.path.join('..\/input\/face-mask-dataset\/data\/with_mask', with_maskImage[i]), '.\/train\/with_mask')\n\nfor i in range(with_mask_train_len, with_mask_val_len):\n    shutil.copy(os.path.join('..\/input\/face-mask-dataset\/data\/with_mask', with_maskImage[i]), '.\/val\/with_mask')\n\nfor i in range(with_mask_val_len, len(with_maskImage)):\n    shutil.copy(os.path.join('..\/input\/face-mask-dataset\/data\/with_mask', with_maskImage[i]), '.\/test\/with_mask')\n    \n\nwithout_mask_train_len = int(np.round(0.6 * len(without_maskImage), 0))\nwithout_mask_val_len = int(np.round(0.7 * len(without_maskImage), 0))\n\n\n\nfor i in range(without_mask_train_len):\n    shutil.copy(os.path.join('..\/input\/face-mask-dataset\/data\/without_mask', without_maskImage[i]), '.\/train\/without_mask')\n\nfor i in range(without_mask_train_len, without_mask_val_len):\n    shutil.copy(os.path.join('..\/input\/face-mask-dataset\/data\/without_mask', without_maskImage[i]), '.\/val\/without_mask')\n\nfor i in range(without_mask_val_len, len(without_maskImage)):\n    shutil.copy(os.path.join('..\/input\/face-mask-dataset\/data\/without_mask', without_maskImage[i]), '.\/test\/without_mask')","116b77ca":"PATH_TRAIN_MASK = '.\/train\/with_mask'\nPATH_VAL_MASK = '.\/val\/with_mask'\nPATH_TEST_MASK = '.\/test\/with_mask'\n\nPATH_TRAIN_NO_MASK = '.\/train\/without_mask'\nPATH_VAL_NO_MASK = '.\/val\/without_mask'\nPATH_TEST_NO_MASK = '.\/test\/without_mask'\n\nPATH_TRAIN = '.\/train'\nPATH_VAL = '.\/val'\nPATH_TEST = '.\/test'","ae2302dc":"print(len(os.listdir(PATH_TRAIN_MASK)))\nprint(len(os.listdir(PATH_VAL_MASK)))\nprint(len(os.listdir(PATH_TEST_MASK)))","09093b6a":"print(len(os.listdir(PATH_TRAIN_NO_MASK)))\nprint(len(os.listdir(PATH_VAL_NO_MASK)))\nprint(len(os.listdir(PATH_TEST_NO_MASK)))","2e70dc6e":"train_mask_files = os.listdir(PATH_TRAIN_MASK)\ntrain_no_mask_files = os.listdir(PATH_TRAIN_NO_MASK)","cb144884":"rows=20 #rows in subplots\ncols=5 #columns in subplots\n\nfig,ax = plt.subplots(rows,cols,figsize=(12,100))\nr = 0\nc = 0\nfor i in range(rows*cols):\n    aa = plt.imread(os.path.join(PATH_TRAIN_NO_MASK,train_no_mask_files[i]))\n    ax[r,c].axis(\"off\")\n    ax[r,c].imshow(aa)\n    c+=1\n    if c == cols:\n        c=0\n        r+=1\nplt.show()","301b4718":"rows=20 #rows in subplots\ncols=5 #columns in subplots\n\nfig,ax = plt.subplots(rows,cols,figsize=(12,100))\nr = 0\nc = 0\nfor i in range(rows*cols):\n    aa = plt.imread(os.path.join(PATH_TRAIN_MASK,train_mask_files[i]))\n    ax[r,c].axis(\"off\")\n    ax[r,c].imshow(aa)\n    c+=1\n    if c == cols:\n        c=0\n        r+=1\nplt.show()","8df2db1b":"from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\n\ntrain_data_gen = ImageDataGenerator(rotation_range=30,\n                                    width_shift_range=0.02,\n                                    height_shift_range=0.02,\n                                    zoom_range=[0.8,1.2],\n                                    horizontal_flip=True,\n                                    rescale=1\/255\n                                   )\n\nval_data_gen = ImageDataGenerator(rescale=1\/255)\ntest_data_gen = ImageDataGenerator(rescale=1\/255)","fe7d23a8":"training_data = train_data_gen.flow_from_directory(PATH_TRAIN, target_size=(200, 200), color_mode='rgb', class_mode='binary', batch_size=32)\nval_data = val_data_gen.flow_from_directory(PATH_VAL, target_size=(200, 200), color_mode='rgb', class_mode='binary', batch_size=32)\ntest_data = test_data_gen.flow_from_directory(PATH_TEST, target_size=(200, 200), color_mode='rgb', class_mode='binary', batch_size=32)","a71b26c5":"training_data.class_indices","8be2587a":"training_data.image_shape","68e9ae66":"model = Sequential()\n\nmodel.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu', input_shape=training_data.image_shape))\nmodel.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n\n\nmodel.add(Conv2D(filters=128, kernel_size=3, strides=1, activation='relu'))\nmodel.add(Conv2D(filters=128, kernel_size=3, strides=1, activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n\nmodel.add(Conv2D(filters=256, kernel_size=3, strides=1, activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n\nmodel.add(Conv2D(filters=512, kernel_size=3, strides=1, activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n\n\nmodel.add(Flatten())\n\nmodel.add(Dense(units=512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=1024, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=1, activation='sigmoid'))\n\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","522dab67":"early_stop = EarlyStopping(monitor='val_loss', patience=4, mode='min')","6fc6c463":"history = model.fit(training_data, batch_size=32, epochs=5, validation_data=val_data, callbacks=early_stop)","8c6a91f7":"loss_df = pd.DataFrame(history.history)\nloss_df","0e722959":"loss_df.plot()\nplt.legend(loc=3)","619d179e":"test_data.class_indices","6a578e1b":"model.evaluate(test_data)","f346e740":"prediction = model.predict(test_data).flatten()","1913bf6c":"print(prediction)","c6433780":"prediction = np.round(prediction)","9fe503f3":"prediction","da5741fa":"from sklearn.metrics import confusion_matrix, classification_report","558d2896":"y_test = test_data.classes\nsns.heatmap(confusion_matrix(y_test, prediction), annot=True, cbar=False, fmt='d')\nplt.xlabel('Prediction')\nplt.ylabel('Actual')","6dab5f1e":"print(classification_report(y_test, prediction))","bc46ee85":"# Evaluating the test data","adea470b":"# Plotting the data","738a146c":"# Rounding the result of prediction in 0 and 1","5aa65f1f":"# Confusion Matrix","cc9e4459":"# Making a directory to split the data into train, val and test for Mask and No      Mask","72563dea":"# Importing the DataGenerator library and rescaling the images ","b920d857":"# Now to access the train data for Mask and No Mask we need to save the files in new variables","17c85f51":"# Printing the prediction","2177d774":"# Creating the model","a3a74ff7":"# Importing the libraries confusion matrix and classification report from sklearn","cf5ffa07":"# Checking the length of the files","1d60508e":"# Predicting the test data","54716c30":"# Taking the EarlyStopping to prevent the data from getting overfit ","6ec60af2":"# Passing the path of the files and saving it in the respective variables","21550622":"# Storing the result of train data in a new dataframe and visualizing it ","0c468734":"# Checking the class indices which will give us the binary value","0e326522":"# Importing the required libraries","d3701138":"# With the help of 'shutil', the source path will be copied to destination path for both Mask and No Mask","3cb73b18":"# Fitting the data","05e599c4":"# Printing the result of prediction","b158ffb4":"# Classification Report ","8b017712":"# Visualizing the images without mask","244c4c1e":"# Checking the length of the data for train, val and test for No Mask","3fe540cf":"# Checking the total images that belong to train, val and test","7d381cec":"# Getting the shape of the images of training data","6db10c67":"# Getting the binary values of the images of test data","db8fee11":"# Checking the length of the data for train, val and test for Mask","56c5b239":"# Visualizing the images with mask","20ec6c81":"# Now, the final path for train, val and test for Mask and No Mask will be saved in another variables which will be the seperate path for train, val and test"}}