{"cell_type":{"a30b6f1a":"code","c5d23ece":"code","f82d5afc":"code","48e51180":"code","e9e4b6db":"code","9c086c9b":"code","5e705288":"code","b2bee917":"code","69824799":"code","7900773b":"code","62cda2d0":"code","f329f219":"code","e1ff524c":"code","85f70b3d":"code","2c315a7a":"code","93870c6f":"code","d9418e15":"code","a72c3254":"code","79bc15c9":"code","7ea3dc61":"code","1851cfc9":"code","a49ef5ac":"code","11de6564":"code","e66438f1":"code","e7928fb4":"code","9c0d3c38":"code","453c7aec":"code","64c3d5a9":"code","65c92d23":"code","19d53f86":"code","115b6c8f":"code","1bc6dcf7":"code","d7e8eb61":"code","f2f67e8a":"code","d7b806a0":"code","e6ccd86b":"code","31444458":"code","9e3c0e4d":"code","1179b751":"code","4cf81916":"code","67ffadb7":"code","5f3ec89b":"code","5802175c":"code","3369586c":"code","1ac607cf":"code","cb46f7ee":"code","e477f2f5":"code","748036d0":"code","fe7e7494":"code","201a1102":"code","03e7e6ce":"code","6dcb6d95":"code","48c73e21":"code","0088e6a6":"code","53bf2eef":"code","2ed5cc61":"code","a25ada23":"code","233a902a":"code","605d4af3":"code","125f5abe":"code","c7f7df30":"code","6ef0d64f":"code","8c4e1547":"code","9a66598b":"code","2b010d6a":"code","4b561220":"code","ee76ecc6":"code","454acea0":"code","19b8e4db":"code","ca4eeefb":"code","5bf7db1d":"code","ff6de2fb":"code","6daf64df":"code","7df1f357":"code","013a6e49":"code","8f96dada":"code","57751c82":"code","75806f71":"code","e67ad00e":"code","0a6592d2":"code","9dc231f6":"code","796aeb46":"code","b23af29d":"code","1f821e6e":"code","aa95c923":"code","9b70894d":"code","4399e42d":"code","41b5e1bb":"code","babaecf5":"code","d3ddfe09":"code","4af3797a":"code","864338fb":"code","8055cad8":"code","ecdc53ec":"code","04a6c0f7":"code","ca18cc05":"code","aa8bb372":"code","383ce43c":"code","431aa9c3":"code","923def2b":"code","ae710ed3":"code","8c525a4c":"code","9b15fee0":"code","b9397f00":"code","78b231e5":"code","3acd9ee1":"code","830d7954":"code","3c3418d6":"code","caf12468":"code","bf5d56ac":"markdown","4cdd758c":"markdown","461e2745":"markdown","a68f5118":"markdown","ae4676cc":"markdown","3acc4230":"markdown","6d87faa3":"markdown","a5cdc205":"markdown","e3f05b93":"markdown","110f614d":"markdown","c4401305":"markdown","b752a4a3":"markdown","9983a504":"markdown","7c6d859b":"markdown","b027d40c":"markdown","acca43e0":"markdown","226df5d0":"markdown","98cb3029":"markdown","a4ab2f11":"markdown","e076f34f":"markdown","64627ba8":"markdown","1fbb5396":"markdown","5db1db7a":"markdown","d46873ed":"markdown","e6057ea2":"markdown","330ab534":"markdown","bc784085":"markdown","3a578ab8":"markdown","59ed8b6c":"markdown","ae906f88":"markdown","bf8cc510":"markdown","21061a1b":"markdown","1e771d66":"markdown","5226d5b1":"markdown","e413824f":"markdown","88137a75":"markdown","24d47c2a":"markdown","6f3a6dde":"markdown","6b4ab599":"markdown","ca4ba5cd":"markdown","1728791c":"markdown","d9799d4d":"markdown","863895c9":"markdown","845355b9":"markdown","59d0c3b7":"markdown","ffa5699b":"markdown","ba9a3947":"markdown","ce63669c":"markdown","54d325f1":"markdown","a9763915":"markdown","87832716":"markdown","b5794cd0":"markdown","a48232f6":"markdown","0a1cc998":"markdown","6407b00b":"markdown","89d54ab2":"markdown","4e957230":"markdown","289e9352":"markdown","62a27af6":"markdown","89a7c971":"markdown","30ab65c5":"markdown","4e61f7d4":"markdown","431a4645":"markdown","f0474fed":"markdown","fa1c6575":"markdown","38acaa08":"markdown","14e28c2f":"markdown","9980c0d2":"markdown"},"source":{"a30b6f1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# For data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns; sns.set()\n# Plotly for interactive graphics \nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5d23ece":"data=pd.read_csv(\"\/kaggle\/input\/diabetes\/diabetes.csv\")","f82d5afc":"df=data.copy()  #bir variable modeli etkileyen bisey degilse drop etmemiz lazim","48e51180":"df.head()","e9e4b6db":"df.info()#bazi kategorik degiskenler derece belirtir aslindas integer bisey modelden cikaramam hemen.","9c086c9b":"df.nunique() #tur sayisi","5e705288":"df[df['Glucose'] == 0]\n","b2bee917":"len(df[df['Glucose'] == 0])","69824799":"len(df[df['Insulin'] == 0])","7900773b":"df.duplicated().sum()","62cda2d0":"df.describe().T ","f329f219":"print(df.shape)\nprint(df.ndim)\nprint(df.size)","e1ff524c":"df.corr() #I can see from here which variables I can put into the model\n#For example, the variables can be removed by looking at the order of importance.\n#Onem durumuna gore istersek bazi variable lari silebiliriz.","85f70b3d":"plt.figure(figsize = (12,6)) \nsns.heatmap(df.corr(),robust=True,fmt='.1g',linewidths=1.3,linecolor = 'gold', annot=True,);","2c315a7a":"sns.pairplot(df,hue='Outcome')","93870c6f":"df.isnull().sum() #nan degerleri olan sutunlari tarama","d9418e15":"#VISUALIZATION OF NAN  VALUES\n#Nan value yok. Gerekli degil.\nimport missingno as msno \nmsno.matrix(df)\nplt.show()","a72c3254":"plt.figure(figsize=(12,12))\nplt.subplot(3,3,1)\nsns.barplot(x='Outcome',y='Glucose',data=df,hue=\"Outcome\")\nplt.subplot(3,3,2)\nsns.barplot(x='Outcome',y='BloodPressure',data=df,hue=\"Outcome\")\nplt.subplot(3,3,3)\nsns.barplot(x='Outcome',y='SkinThickness',data=df,hue=\"Outcome\")\nplt.subplot(3,3,4)\nsns.barplot(x='Outcome',y='BMI',data=df,hue=\"Outcome\")\nplt.subplot(3,3,5)\nsns.barplot(x='Outcome',y='DiabetesPedigreeFunction',data=df,hue=\"Outcome\")\nplt.subplot(3,3,6)\nsns.barplot(x='Outcome',y='Age',data=df,hue=\"Outcome\")","79bc15c9":"sns.swarmplot(x='Outcome',y='BloodPressure',data=df,hue='Outcome')","7ea3dc61":"sns.countplot(x=\"Outcome\",data=df)","1851cfc9":"\nx_dat=df.drop([\"Outcome\"],axis=1)\ny=df[\"Outcome\"]","a49ef5ac":"#If there is a outlier values, it must be done before coming here\nx=(x_dat-np.min(x_dat))\/(np.max(x_dat)-np.min(x_dat)).values  ","11de6564":"from sklearn.model_selection import train_test_split,cross_val_score,ShuffleSplit,GridSearchCV\nfrom sklearn.metrics import accuracy_score,mean_squared_error,roc_curve,roc_auc_score,classification_report,r2_score,confusion_matrix\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42) \n#train test ayrilirken kucuk veri ise % 80 yapmak lazim yani yuksek yapmak lazim ama veri buyukse % 60 bile yapilabilir.","e66438f1":"\nfrom sklearn.linear_model import LogisticRegression\nlr_model=LogisticRegression() #default olanlar gelir.C var..\nlr_model.fit(x_train,y_train)","e7928fb4":"?lr_model","9c0d3c38":"print(lr_model.intercept_)\nprint(lr_model.coef_)#Bu katsayilar diebeti etkileme katsayisi mesela insulin 4 kat etkiliyor.mesela - olanlar ters yonde etkiliyor.","453c7aec":"y_pred=lr_model.predict(x_test)","64c3d5a9":"y_pred[0:10]#tahminlerin ilk 10 degerini gorduk","65c92d23":"accuracy_score(y_test,y_pred)","19d53f86":"y_probs = lr_model.predict_proba(x_test)[:,1]\n","115b6c8f":"y_probs","1bc6dcf7":"y_pred = [1 if i >0.70 else 0 for i in y_probs]\ny_pred[:10]","d7e8eb61":"log_score = accuracy_score(y_test,y_pred)\nprint (\"log score=\",log_score)","f2f67e8a":"confusion_matrix(y_test,y_pred)","d7b806a0":"lr_model = LogisticRegression(solver = \"liblinear\")\nlr_model= lr_model.fit(x_train,y_train)\nlr_model","e6ccd86b":"lr_model.predict(x_test)","31444458":"accuracy_score(y_test, lr_model.predict(x_test))","9e3c0e4d":"#Cross validation (10 katli ) yaparsak\nlr_finalscore=cross_val_score(lr_model, x_test, y_test, cv = 5).mean()\nlr_finalscore","1179b751":"from sklearn.naive_bayes import GaussianNB\nnb_model = GaussianNB()\nnb_model = nb_model.fit(x_train, y_train)\nnb_model","4cf81916":"y_pred = nb_model.predict(x_test)\ny_pred","67ffadb7":"accuracy_score(y_test,y_pred)","5f3ec89b":"confusion_matrix(y_test,y_pred)","5802175c":"# 10 katli cross validation.\nnb_finalscore=cross_val_score(nb_model, x_test, y_test, cv = 10).mean()\nnb_finalscore","3369586c":"from sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier()\nknn_model = knn_model.fit(x_train, y_train)\nknn_model","1ac607cf":"y_pred = knn_model.predict(x_test)","cb46f7ee":"accuracy_score(y_test, y_pred)","e477f2f5":"#print(classification_report(y_test, y_pred))","748036d0":"confusion_matrix(y_test,y_pred)","fe7e7494":"knn_params = {\"n_neighbors\": np.arange(1,50)}","201a1102":"knn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, knn_params, cv=10)\nknn_cv.fit(x_train, y_train)","03e7e6ce":"print(\"The best score:\" + str(knn_cv.best_score_))\nprint(\"The best parameters: \" + str(knn_cv.best_params_))","6dcb6d95":"knn = KNeighborsClassifier(3)  #we choose 3 neigboors. I think 1 is not good \nknn_tuned = knn.fit(x_train, y_train)","48c73e21":"knn_finalscore=knn_tuned.score(x_test, y_test)\nknn_finalscore","0088e6a6":"from sklearn.svm import SVC\nsvm_model = SVC().fit(x_train,y_train)#we choose default c:1,kernel:'rbf',dagree:3...\n#?svm_model","53bf2eef":"y_pred = svm_model.predict(x_test)","2ed5cc61":"accuracy_score(y_test,y_pred)","a25ada23":"svc_params = {\"C\": np.arange(1,10)}\n\nsvc = SVC(kernel = \"rbf\")\n\nsvc_cv_model = GridSearchCV(svc,svc_params, \n                            cv = 10, \n                            n_jobs = -1,        \n                            verbose = 2 )\n\nsvc_cv_model.fit(x_train, y_train)","233a902a":"print(\"The best parameters: \" + str(svc_cv_model.best_params_))","605d4af3":"svc_tuned1 = SVC(kernel = \"rbf\", C = 8).fit(x_train, y_train)\n\ny_pred = svc_tuned1.predict(x_test)\naccuracy_score(y_test, y_pred)\n","125f5abe":"svc_params = {\"C\": np.arange(1,10)}\n\nsvc = SVC(kernel = \"linear\")\n\nsvc_cv_model = GridSearchCV(svc,svc_params, \n                            cv = 10, \n                            n_jobs = -1, \n                            verbose = 2 )\n\nsvc_cv_model.fit(x_train, y_train)","c7f7df30":"print(\"The best parameters: \" + str(svc_cv_model.best_params_))","6ef0d64f":"svc_tuned2 = SVC(kernel = \"linear\", C = 6).fit(x_train, y_train)\ny_pred = svc_tuned2.predict(x_test)\naccuracy_score(y_test, y_pred)","8c4e1547":"svc_model = SVC(kernel = \"rbf\").fit(x_train, y_train)","9a66598b":"svc_params = {\"C\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100],\n             \"gamma\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100]}","2b010d6a":"svc = SVC()\nsvc_cv_model = GridSearchCV(svc, svc_params, \n                         cv = 10, \n                         n_jobs = -1,\n                         verbose = 2)\n\nsvc_cv_model.fit(x_train, y_train)","4b561220":"print(\"The best parameters: \" + str(svc_cv_model.best_params_))","ee76ecc6":"svc_tuned3 = SVC(C = 5, gamma =50).fit(x_train, y_train)\ny_pred = svc_tuned3.predict(x_test)\nsvc_finalscore=accuracy_score(y_test, y_pred)\nsvc_finalscore\n","454acea0":"from sklearn.ensemble import RandomForestClassifier\nrf_model=RandomForestClassifier()\nrf_model.fit(x_train,y_train)\n#?rf_model  #n estimator default 100 trees","19b8e4db":"y_pred = rf_model.predict(x_test)","ca4eeefb":"accuracy_score(y_test, y_pred)","5bf7db1d":"rf_params = {\"max_depth\": [2,5,8,10],\n            \"max_features\": [2,5,8],\n            \"n_estimators\": [10,500,1000],\n            \"min_samples_split\": [2,5,10]}","ff6de2fb":"rf_model = RandomForestClassifier()\n\nrf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                           n_jobs = -1, \n                           verbose = 2) ","6daf64df":"#rf_cv_model.fit(x_train, y_train)","7df1f357":"#print(\"The best parameters: \" + str(rf_cv_model.best_params_))","013a6e49":"rf_tuned = RandomForestClassifier(max_depth = 10, \n                                  max_features = 5, \n                                  min_samples_split = 2,\n                                  n_estimators = 1000)\n\nrf_tuned.fit(x_train, y_train)","8f96dada":"y_pred = rf_tuned.predict(x_test)\nrf_finalscore=accuracy_score(y_test, y_pred)\nrf_finalscore","57751c82":"Importance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n                         index = x_train.columns)","75806f71":"Importance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\")\n\nplt.xlabel(\"Variable Importance Levels\")","e67ad00e":"from sklearn.ensemble import GradientBoostingClassifier\ngbm_model = GradientBoostingClassifier().fit(x_train, y_train)","0a6592d2":"y_pred = gbm_model.predict(x_test)","9dc231f6":"accuracy_score(y_test, y_pred)","796aeb46":"# gbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n#              \"n_estimators\": [100,500,100],\n#              \"max_depth\": [3,5,10],\n#              \"min_samples_split\": [2,5,10]}","b23af29d":"# gbm = GradientBoostingClassifier()","1f821e6e":"# gbm_cv = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)\n# gbm_cv.fit(x_train, y_train)","aa95c923":"#print(\"The best parameters: \" + str(gbm_cv.best_params_))","9b70894d":"gbm = GradientBoostingClassifier(learning_rate = 0.05, \n                                 max_depth = 10,\n                                min_samples_split = 10,\n                                n_estimators = 100)","4399e42d":"gbm_tuned =  gbm.fit(x_train,y_train)","41b5e1bb":"y_pred = gbm_tuned.predict(x_test)\ngbm_finalscore=accuracy_score(y_test,y_pred)\ngbm_finalscore","babaecf5":"!pip install xgboost","d3ddfe09":"from xgboost import XGBClassifier\nxgb_model = XGBClassifier().fit(x_train, y_train)\n#?xgb_model","4af3797a":"y_pred = xgb_model.predict(x_test)","864338fb":"accuracy_score(y_test, y_pred)","8055cad8":"# #xgb_params = {\n#         'n_estimators': [100, 500, 1000, 2000],\n#         'subsample': [0.6, 0.8, 1.0],\n#         'max_depth': [3, 4, 5,6],\n#         'learning_rate': [0.1,0.01,0.02,0.05],\n#         \"min_samples_split\": [2,5,10]}","ecdc53ec":"# xgb = XGBClassifier()\n\n# xgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 10, n_jobs = -1, verbose = 2)\n# xgb_cv_model.fit(x_train, y_train)","04a6c0f7":"#print(\"The best parameters:\"+str(xgb_cv_model.best_params_)","ca18cc05":"#?xgb_model","aa8bb372":"xgb = XGBClassifier(learning_rate = 0.01, \n                    max_depth = 6,\n                    n_estimators = 100,\n                    subsample = 0.8)\nxgb_tuned =  xgb.fit(x_train,y_train)\ny_pred = xgb_tuned.predict(x_test)\nXGBoost_finalscore=accuracy_score(y_test, y_pred)\nXGBoost_finalscore","383ce43c":"!conda install -c conda-forge lightgbm","431aa9c3":"#!conda install -c conda-forge lightgbm\nfrom lightgbm import LGBMClassifier\nlgbm_model = LGBMClassifier().fit(x_train, y_train)","923def2b":"y_pred = lgbm_model.predict(x_test)","ae710ed3":"accuracy_score(y_test, y_pred)","8c525a4c":"lgbm_params = {\n        'n_estimators': [100, 500, 1000, 2000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,6],\n        'learning_rate': [0.1,0.01,0.02,0.05],\n        \"min_child_samples\": [5,10,20]}","9b15fee0":"# lgbm = LGBMClassifier()\n\n# lgbm_cv_model = GridSearchCV(lgbm, lgbm_params, \n#                              cv = 10, \n#                              n_jobs = -1, \n#                              verbose = 2)\n# lgbm_cv_model.fit(x_train, y_train)","b9397f00":"#print(\"The best parameters:\"+str(lgbm_cv_model.best_params_))","78b231e5":"lgbm = LGBMClassifier(learning_rate = 0.1, \n                       max_depth = 4,\n                       subsample = 0.6,\n                       n_estimators = 500,\n                       min_child_samples = 10)\nlgbm_tuned = lgbm.fit(x_train,y_train)","3acd9ee1":"y_pred = lgbm_tuned.predict(x_test)\nlgbm_finalscore=accuracy_score(y_test, y_pred)\nlgbm_finalscore","830d7954":"print(lr_finalscore,rf_finalscore,knn_finalscore,svc_finalscore,nb_finalscore,gbm_finalscore,XGBoost_finalscore,lgbm_finalscore)","3c3418d6":"# indexx = [\"Log\",\"RF\",\"KNN\",\"SVM\",\"NB\",\"GBM\",\"XGBoost\"\"LightGBM\"]\n# regressions = [lr_finalscore,rf_finalscore,knn_finalscore,svc_finalscore,nb_finalscore,gbm_finalscore,XGBoost_finalscore,lgbm_finalscore]\n\n# plt.figure(figsize=(8,6))\n# sns.barplot(x=indexx,y=regressions)\n# plt.xticks()\n# plt.title('Comparision of Classification Methods',color = 'orange',fontsize=20);","caf12468":"# from plotly.plotly import iplot\n# import plotly.graph_objs as go\n# import chart_studio.plotly as py\n\n\n# indexx = [\"Log\",\"RF\",\"KNN\",\"SVM\",\"NB\",\"GBM\",\"XGBoost\"\"LightGBM\"]\n# regressions = [lr_finalscore,rf_finalscore,knn_finalscore,svc_finalscore,nb_finalscore,gbm_finalscore,XGBoost_finalscore,lgbm_finalscore]\n# # creating trace1\n# trace1 =go.Scatter(\n#                     x = indexx,\n#                     y = regressions,\n#                     mode = \"lines+markers+text\",\n#                     name = \"#\",\n#                     marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n#                     text= indexx) #uzerine geldiginde ne gorunsun.\n\n# data = [trace1]  # olusturdugumuz veriler listeye atadik\n\n# # konumlandirmayi yapar ve isimlendirir.(layout)\n# layout = dict(title = 'Model Compare',\n#               xaxis= dict(title= 'Models',ticklen= 15,zeroline= True),\n#               yaxis= dict(title= 'Scores',ticklen= 15,zeroline= True)\n#              )\n# fig = dict(data = data, layout = layout)\n# iplot(fig)","bf5d56ac":"* LOGISTIC REGRESSION MODEL:0.7575\n* RANDOM FORESTS MODEL: 0.975\n* KNN MODEL: 0.845\n* NAIVE BAYES MODEL: 0.7674\n* SVC MODEL: 0.975\n* GBM MODEL: 0.985\n* XGBoost MODEL: 0.90\n* LightGBM MODEL: 0.985","4cdd758c":"#### For kernel:rbf , C and gamma ","461e2745":"## If you find this kernel helpful, Please UPVOTES.","a68f5118":"### A ) Modeling of SVC","ae4676cc":"## Problem Definition\n* Given clinical parameters about a patient, can we predict whether or not they have diabets?","3acc4230":"## 4 )SVC","6d87faa3":"## 1) logistic regression","a5cdc205":"### D ) Model Tuning of LightGBM","e3f05b93":"* Naive Bayes s\u0131n\u0131fland\u0131r\u0131c\u0131s\u0131, bir s\u0131n\u0131ftaki belirli bir \u00f6zelli\u011fin varl\u0131\u011f\u0131n\u0131n ba\u015fka herhangi bir \u00f6zelli\u011fin varl\u0131\u011f\u0131yla ilgisi olmad\u0131\u011f\u0131n\u0131 varsayar.\n* \u00d6rne\u011fin, k\u0131rm\u0131z\u0131, yuvarlak ve \u00e7ap\u0131 yakla\u015f\u0131k 3 in\u00e7 olan bir meyve elma olarak kabul edilebilir. Bu \u00f6zellikler birbirine veya di\u011fer \u00f6zelliklerin varl\u0131\u011f\u0131na ba\u011fl\u0131 olsa bile, t\u00fcm bu \u00f6zellikler ba\u011f\u0131ms\u0131z olarak bu meyvenin bir elma olma olas\u0131l\u0131\u011f\u0131na katk\u0131da bulunur","110f614d":"* Count of not diabet is more than diabet.","c4401305":"![image.png](attachment:47512a12-dedf-4cfd-ad5e-a104c6ffc714.png)","b752a4a3":"## DATA READING AND EXPLORING","9983a504":"### C ) Accuracy score of NB ","7c6d859b":"### B ) Prediction of NB","b027d40c":"### A ) Modeling of Random Forests","acca43e0":"![image.png](attachment:7f3c4227-92cf-4fea-a749-da177d9c42d6.png)","226df5d0":"###  A ) Modeling of NB","98cb3029":"### C ) Accuracy score of KNN","a4ab2f11":"## We will use the following CLASSIFICATION METHODS for Prediction\n* LOGISTIC REGRESSION MODEL\n* RANDOM FORESTS MODEL\n* KNN MODEL\n* NAIVE BAYES MODEL\n* SVC MODEL\n* GBM MODEL\n* XGBoost MODEL\n* LightGBM MODEL","e076f34f":"### A ) Modeling of KNN","64627ba8":"### Normalization of variabales","1fbb5396":"### D ) Model Tuning of GBM","5db1db7a":"### Proba values - probability","d46873ed":"### C ) Accuracy score of SVC","e6057ea2":"### C ) Accuracy Test(for default) of Logistic regression","330ab534":"### B ) Prediction of KNN","bc784085":"### D ) Model Tuning of Random Forests","3a578ab8":"### C ) Accuracy score of LightGBM","59ed8b6c":"### A ) Modeling of XGBoost","ae906f88":"### D ) Model Tuning of NB","bf8cc510":"### A ) Modeling of GBM","21061a1b":"## LOOK AT ALL SCORE OF CLASSIFICATION METHODS\nThe most successful models are GBM=0.985 and LightGBM=0.985","1e771d66":"### D ) Model Tuning of KNN","5226d5b1":"* n_jobs=-1, t\u00fcm i\u015flemcileri kullanmak anlam\u0131na gelir.\n* verbose int: Controls the verbosity: the higher, the more messages.(Ayr\u0131nt\u0131y\u0131 kontrol eder: ne kadar y\u00fcksekse, o kadar fazla mesaj.)","e413824f":"### C ) Accuracy score of Random Forests","88137a75":"## SOME OF VISUALIZATION","24d47c2a":"* There is no Nan values","6f3a6dde":"## 2 ) Gaussian Naive Bayes","6b4ab599":"## 5 ) Random Forests\n* Average of decision trees\n* Decison is what we need, but it is not so. During the test phase, if the decision is tried, if the data is big and the score is good, we decide that it will be random forest.\n* Decision can give different results each time because it is 1 tree but not so in r.forest.\n* R.forest gets good score if the data is small but light gbm gets better score if big data","ca4ba5cd":"![image.png](attachment:df8afbbc-9685-4767-b512-a498e4f51205.png) \n","1728791c":"Parametre s\u00f6zl\u00fc\u011f\u00fc olu\u015fturulduktan sonraki ad\u0131m, GridSearchCV s\u0131n\u0131f\u0131n\u0131n bir \u00f6rne\u011fini olu\u015fturmakt\u0131r. Temel olarak y\u00fcr\u00fctmek istedi\u011finiz algoritma olan tahmin edici parametresi i\u00e7in de\u011ferler iletmeniz gerekir.\n* param_grid parametresi, parametre olarak az \u00f6nce olu\u015fturdu\u011fumuz parametre s\u00f6zl\u00fc\u011f\u00fcn\u00fc,\n* the scoring parameter performans metriklerini, \n* cv parametresi bizim durumumuzda 10 olan kat say\u0131s\u0131n\u0131 ve\n* n_jobs parametresi, parametre olarak kullan\u0131lan CPU say\u0131s\u0131n\u0131 ifade eder. y\u00fcr\u00fctmek i\u00e7in kullanmak istiyorsunuz. n_jobs parametresi i\u00e7in -1 de\u011feri, mevcut t\u00fcm bilgi i\u015flem g\u00fcc\u00fcn\u00fcn kullan\u0131ld\u0131\u011f\u0131 anlam\u0131na gelir. \u00c7ok say\u0131da veriniz varsa bu kullan\u0131\u015fl\u0131 olabilir.","d9799d4d":"## 8 ) LightGBM","863895c9":"## 3 ) KNN","845355b9":"#### For kernel:rbf","59d0c3b7":"### D ) Model Tuning of XGBoost","ffa5699b":"### B ) Prediction of Random Forests","ba9a3947":"### B ) Prediction of Logistic Regression","ce63669c":"![image.png](attachment:7693b749-a526-4789-a7c2-3e05513c6385.png)","54d325f1":"### B ) Prediction of GBM","a9763915":"KNN ile temelde yeni noktaya en yak\u0131n noktalar aran\u0131r. K, bilinmeyen noktan\u0131n en yak\u0131n kom\u015fular\u0131n\u0131n miktar\u0131n\u0131 temsil eder. Sonu\u00e7lar\u0131 tahmin etmek i\u00e7in algoritman\u0131n k miktar\u0131n\u0131 (genellikle bir tek say\u0131) se\u00e7eriz.","87832716":"### B ) Prediction of SVC","b5794cd0":"### B ) Prediction of XGBoost","a48232f6":"### Test-Train splitting","0a1cc998":"Logistic Regression ( Lojistik Regresyon ) s\u0131n\u0131fland\u0131rma i\u015flemi yapmaya yarayan bir regresyon y\u00f6ntemidir. Kategorik veya say\u0131sal verilerin s\u0131n\u0131fland\u0131r\u0131lmas\u0131nda kullan\u0131l\u0131r. Ba\u011f\u0131ml\u0131 de\u011fi\u015fkenin yani sonucun sadece 2 farkl\u0131 de\u011fer alabilmesi durumda \u00e7al\u0131\u015f\u0131r. ( Evet \/ Hay\u0131r, Erkek \/ Kad\u0131n, \u015ei\u015fman \/ Zay\u0131f vs. )","6407b00b":"#### For kernel:linear","89d54ab2":"### A ) Modeling of LightGBM","4e957230":"### C ) Accuracy score of GBM","289e9352":"## Data contains;\n* Pregnancies\n* Glucose = Glucose rate\n* BloodPressure \n* SkinThickness \n* Insulin = Inusilin rate\n* BMI = Body Mass Index\n* DiabetesPedigreeFunction \n* Age \n* Outcome = diabet:1, not diabet:0","62a27af6":"### D ) Model tuning of Logistic regression","89a7c971":"### B ) Prediction of LightGBM","30ab65c5":"* There is correlation between Outcome and Glucose","4e61f7d4":"## 7 ) XGBoost","431a4645":"## 6 ) Gradient Boosting Machines","f0474fed":"### C ) Accuracy of XGBoost","fa1c6575":"### D ) Model Tuning of SVC","38acaa08":"### Preparation dependent and independent variables","14e28c2f":"## PREDICTION WITH CLASSIFICATION METHODS","9980c0d2":"### A ) Modeling of Logistic Regression"}}