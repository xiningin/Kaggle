{"cell_type":{"126ff8cf":"code","1fbcf6d7":"code","ca91f569":"code","6c3e2988":"code","eb17992b":"code","e475620a":"code","c8d7820c":"code","a74d082a":"code","0cbf39ae":"code","7b49fc23":"code","ded230bb":"code","a643bd7a":"code","eb48ccff":"code","d7c58186":"code","4bb4919d":"code","23d91eb3":"code","0545f134":"code","6fe85e6d":"code","1344bbc8":"code","58b663ae":"code","cd205bb4":"code","37ac3793":"code","f35eb2e1":"code","bfa3b2ae":"code","1ed4fa11":"code","cd53fb03":"code","4a122930":"code","6dc641ec":"code","0ca30eb8":"code","4d173144":"code","ad974891":"markdown","37ce19d3":"markdown","2ea934ad":"markdown","cf754ee2":"markdown","59027c9f":"markdown","f8d4ee3f":"markdown","361ff3ae":"markdown","1d2e50ef":"markdown","fe1964bb":"markdown","76646c44":"markdown","fc32403b":"markdown","b32f291b":"markdown","00b8f5cd":"markdown","9dfaba43":"markdown","f18343b6":"markdown","0b50241a":"markdown","9b47b87f":"markdown","ec6397ce":"markdown","6535cc32":"markdown","fd27fd5c":"markdown","4e5dac05":"markdown","0cb03c19":"markdown","b91f2067":"markdown","907dada5":"markdown","3932c56c":"markdown"},"source":{"126ff8cf":"import copy\nimport seaborn\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.transforms as mtransforms\nfrom math import pi\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\nimport warnings\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1fbcf6d7":"# 1. Importing & pre-processing the data\ndata = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\ntarget = 'RainTomorrow'\ndata.rename(columns = {target: 'Target'}, inplace = True)\ndata[[\"Date\"]] = data[[\"Date\"]].astype('datetime64[ns]')\npeek = data.head(5)\ndisplay(peek)","ca91f569":"class_counts = pd.DataFrame(data.groupby('Target').size(), columns=[\"Observations\"])\ndisplay(class_counts)","6c3e2988":"def timeline(data,start,end,y,color=\"grey\",ascending=False):\n    data = data.sort_values(\"count\",axis=0,ascending=ascending)\n    index = data.index.tolist()    \n    fig, ax = plt.subplots(figsize=(10,data.shape[0]*.25))\n    days = data[end] - data[start]\n    \n    plt.hlines(data[y],data[start],data[end],linewidth=9, color='lightgrey')\n    for i in range(len(data)):\n        ax.text(mdates.date2num(data[start][index[i]])-200,i-.3, '%d' % (days[index[i]].days), color=color, fontweight=\"bold\") \n    ax.spines[[\"right\", \"top\", \"left\", \"bottom\"]].set_visible(False)\n    ax.xaxis.grid(True,linestyle=\":\",color='black')\n    ax.set_axisbelow(False)\n    \ndata_by_station = data.groupby('Location').size()\nstation_stats = []\nfor station in data_by_station.items():\n    station_data = data.loc[data[\"Location\"] == station[0]]\n    station_name = station_data[\"Location\"].iloc[0]\n    start_date = station_data.iloc[0][\"Date\"]\n    end_date = station_data.iloc[-1][\"Date\"]\n    datapoints_count = station[1]\n    datapoints_missing = max(station_data.isnull().sum())\n    missing_to_all_ratio = round(datapoints_missing\/datapoints_count*100)\n    station_stats.append([station_name,start_date,end_date,datapoints_count,datapoints_missing,missing_to_all_ratio])  \nstation_stats = pd.DataFrame(station_stats, columns=[\"name\",\"start\",\"end\",\"count\",\"missing\",\"missing_prct\"])\n\ntimeline(station_stats,'start','end','name')","eb17992b":"def plotBarH(data,x,y,title,color=\"grey\",ascending=False):\n    data = data.sort_values(x,axis=0,ascending=ascending)\n    index = data.index.tolist()\n    fig, ax = plt.subplots(figsize=(10,data.shape[0]*.25))\n    \n    if ascending: a = -1\n    else: a = 0\n    padding = 0.01 * data[x][index[a]]\n    \n    for i in range(len(data)):\n        ax.text(data[x][index[i]]+padding,i-.3, '%.2f %%' % (data[x][index[i]]), color=color, fontweight=\"bold\", fontsize=10) \n        \n    plt.barh(data[y],data[x], color=color)\n    ax.spines[[\"right\", \"top\", \"left\", \"bottom\"]].set_visible(False)\n    plt.title(title, size=12, weight=\"bold\")\n    \nplotBarH(station_stats,'missing_prct','name',\"Missing Values by Station\")","e475620a":"missing_data = pd.DataFrame([(\n    # Feature name\n    data.columns[i],\n    # Sum of missing values\n    data.iloc[:,i].isnull().sum(), \n    # Percentage of missing values\n    round(data.iloc[:,i].isnull().sum() \/ data.shape[0] * 100,2)) \n    for i in range(data.shape[1])],\n    columns=[\"variable\",\"missing\",\"missing_prct\"] )\n\nplotBarH(missing_data,'missing_prct','variable', \"Missing Values by Feature\")","c8d7820c":"warnings.filterwarnings('ignore')\ndef factorize(data):\n    for i in range(0,len(data.dtypes)):\n        if str(data.dtypes[i]) == 'object':\n            data.iloc[:,i], u = pd.factorize(data.iloc[:,i])\n    return data\n\nfac_data = copy.deepcopy(data)\nfac_data[[\"WindGustDir\",\"WindDir9am\",\"WindDir3pm\",\"RainToday\",\"Target\"]] = factorize(data[[\"WindGustDir\",\"WindDir9am\",\"WindDir3pm\",\"RainToday\",\"Target\"]])\n\nfig,ax = plt.subplots(figsize=(15,8))\nseaborn.heatmap(ax=ax,data=fac_data.corr().round(2),annot=True,cmap=seaborn.diverging_palette(220,20),linewidth=2)\nplt.title(\"Correlation Matrix\", size=12, weight=\"bold\")\n\"\"\"\"\"\"","a74d082a":"# Removing observations with missing missing values \nfac_data.dropna(inplace=True)\n\n# Feature importance\ndef importance(X,y,fun):\n    model = fun\n    model.fit(X, y)\n    data = pd.DataFrame([list(X.columns), list(model.feature_importances_)]).T\n    data.columns = [\"Feature Names\", \"Importance\"]\n    return data\n\nX = fac_data.drop(['Location','Date','Target'], axis=1)\ny = fac_data['Target']\n\n# Random undersampling\nundersample = RandomUnderSampler(sampling_strategy='majority')\nX, y = undersample.fit_resample(X,y)\n\ntopFeatures = importance(X,y,DecisionTreeClassifier())\nplotBarH(topFeatures,'Importance','Feature Names',\"Feature Importance Score (Decision Tree)\",ascending=True)","0cbf39ae":"topFeatures = importance(X,y,RandomForestClassifier())\nplotBarH(topFeatures,'Importance','Feature Names',\"Feature Importance Score (Random Forest)\",ascending=True)","7b49fc23":"topFeatures = importance(X,y,AdaBoostClassifier())\nplotBarH(topFeatures,'Importance','Feature Names',\"Feature Importance Score (Adaptive Boosting)\",ascending=True)","ded230bb":"def distPlot(data1,data2):\n    y,x = 5,5\n    fig, axes = plt.subplots(x,y,figsize=(12,12))\n    features = data1.columns\n    features = features.drop([\"Date\",\"Location\",\"RainToday\"])\n    for i in range(len(features)):\n        seaborn.kdeplot(data1[features[i]],fill=False, ax=axes[i\/\/y,i%x], color='red')\n        seaborn.kdeplot(data2[features[i]], fill=False, ax=axes[i\/\/y,i%x], color='gray')\n        axes[i\/\/y,i%x].title.set_text(features[i])\n        axes[i\/\/y,i%x].set_ylabel(\"\")\n        axes[i\/\/y,i%x].set_xlabel(\"\")\n    fig.tight_layout(pad=2.0)\n    \nfac_melbourne = fac_data.loc[fac_data[\"Location\"] == \"MelbourneAirport\"]\n    \ndistPlot(fac_melbourne,fac_data)","a643bd7a":"def timeSeriesPrep(data):\n    data[\"Year\"] = data.Date.dt.year\n    data[\"Month\"] = data.Date.dt.month_name()\n    data[\"Day\"] = data.Date.dt.day\n    cats = ['January', 'February', 'March', 'April','May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n    data['Month'] = pd.Categorical(data['Month'], ordered=True, categories=cats)\n    return data\n\ndef swarmPlot(data,x,y,hue,size):\n    fig, ax = plt.subplots(figsize=(20,5))\n    ax.spines[[\"right\", \"top\", \"left\", \"bottom\"]].set_visible(False)\n    ax.yaxis.grid(True,linestyle=\":\",color='black')\n    ax = seaborn.swarmplot(data=data[[x,y,hue]],x=x,y=y,hue=hue,size=size)\n    ax.get_legend().remove()\n    plt.title(y, size=12, weight=\"bold\")\n\nmelbourne = data.loc[data[\"Location\"] == \"MelbourneAirport\"]\nmelbourne = timeSeriesPrep(melbourne)\nswarmPlot(melbourne,\"Year\",\"Sunshine\",\"Target\",4.5)","eb48ccff":"swarmPlot(melbourne,\"Month\",\"Sunshine\",\"Target\",3.5)","d7c58186":"swarmPlot(melbourne,\"Month\",\"Humidity3pm\",\"Target\",3)","4bb4919d":"swarmPlot(melbourne,\"Month\",\"Pressure3pm\",\"Target\",3.5)","23d91eb3":"swarmPlot(melbourne,\"Month\",\"Temp9am\",\"Target\",3)","0545f134":"def polar_data(data,variable):\n    \n    directions = ['E', 'ENE', 'NE', 'NNE','N', 'NNW', 'NW', 'WNW', 'W', 'WSW', 'SW', 'SSW','S','SSE','SE','ESE']\n    \n    #Probability of rain per direction\n    rain_probability = data.groupby(variable).apply(lambda x: x[x=='Yes'].count()\/len(x))[[\"Target\"]]  \n    \n    #Frequency of rain per direction\n    rain_count = dict()\n    for c in data.groupby(variable).size().items():\n        rain_count[c[0]] = c[1]\n    \n    #Frequency and probability ordered\n    df = [[direction, rain_count[direction], round(rain_probability.loc[direction]*100,1)] for direction in directions]    \n    N = len(df)\n    \n    _, frequency, probability = zip(*df)\n    frequency += frequency[:1]\n    probability += probability[:1]\n    \n    #Polar plot angles\n    angles = [n \/ float(N) * 2 * pi for n in range(N)]\n    angles += angles[:1]\n    \n    return directions, frequency, probability, angles\n\ndef polar_plot(polar_d,title,color,prob=False,size=7):\n    \n    directions, frequency, probability, angles = polar_d\n\n    if prob: values = probability\n    else: values = frequency\n    \n    fig = plt.figure(figsize=(size,size))\n    ax = plt.subplot(111, polar=True)\n    plt.title(title, size=12, weight=\"bold\")\n    ax.set_yticklabels([])\n    ax.plot(angles, values,'o-',color=color, linewidth=1, linestyle='solid')\n    ax.fill(angles, values, color, alpha=0.1)\n    plt.xticks(angles[:-1], directions, color='black', size=10)\n    trans_offset = mtransforms.offset_copy(ax.transData, fig=fig,x=2, y=2, units='inches')\n        \n    for x, y in zip(angles, values):\n        if prob: plt.text(x, y, '%d %%' % (int(y)), color=color)\n        else: plt.text(x, y, '%d' % (int(y)), color=color)\n            \npolar_plot(polar_data(data,\"WindGustDir\"),\"Australia (frequency)\",\"brown\", prob=False)","6fe85e6d":"polar_plot(polar_data(data,\"WindGustDir\"),\"Australia (probability)\",\"darkblue\", prob=True)","1344bbc8":"polar_plot(polar_data(melbourne,\"WindGustDir\"),\"Melbourne (frequency)\",\"brown\", prob=False)","58b663ae":"polar_plot(polar_data(melbourne,\"WindGustDir\"),\"Melbourne (probability)\",\"darkblue\", prob=True)","cd205bb4":"def box(data,x,y,hue):\n    fig, ax = plt.subplots(figsize=(20,5))\n    ax.spines[[\"right\", \"top\", \"left\", \"bottom\"]].set_visible(False)\n    ax.yaxis.grid(True,linestyle=\":\",color='black')\n    ax = seaborn.boxplot(data=data[[x,y,hue]],x=x,y=y,hue=hue)\n    ax.get_legend().remove()\n\nbox(melbourne,\"Month\",\"WindGustSpeed\",\"Target\")","37ac3793":"box(melbourne,\"Month\",\"Cloud3pm\",\"Target\")","f35eb2e1":"fig, axes = plt.subplots(1,3,figsize=(24,8))\n\nseaborn.scatterplot(\n    x=data['Humidity3pm'], \n    y=data['Pressure3pm'], \n    hue=data['Target'], \n    palette={\"No\":'tab:orange',\"Yes\":'tab:blue'}, \n    ax=axes[0], \n    alpha=0.1,\n    size=1,\n    legend=False\n)\n\nseaborn.scatterplot(\n    x=data['Pressure3pm'], \n    y=data['Sunshine'], \n    hue=data['Target'], \n    palette={\"No\":'tab:orange',\"Yes\":'tab:blue'}, \n    ax=axes[1], \n    alpha=0.1,\n    size=1,\n    legend=False\n)\n\nseaborn.scatterplot(\n    x=data['Humidity3pm'], \n    y=data['Sunshine'], \n    hue=data['Target'], \n    palette={\"No\":'tab:orange',\"Yes\":'tab:blue'}, \n    ax=axes[2], \n    alpha=0.1,\n    size=1,\n    legend=False\n)","bfa3b2ae":"def svm(X,y):\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    \n    undersample = RandomUnderSampler(sampling_strategy='majority', random_state=0)\n    X, y = undersample.fit_resample(X,y)\n    \n    tuned_parameters = [\n        {'kernel': ['rbf'], 'gamma': [1e-3],'C': [100]},\n        #{'kernel': ['linear'], 'C': [10]}\n        ]\n    \n    classifier = GridSearchCV(SVC(), tuned_parameters, scoring='%s_macro' % 'precision')\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    classifier.fit(X_train, y_train)\n    \n    predictions = classifier.predict(X_test)\n    \n    return round(accuracy_score(y_test,predictions)*100,2)\n    \nclean_data = data.dropna(inplace=False)\n\naccuracy, title = [],[]\n\ntitle.append(\"8 Features\")\nX = clean_data[['Humidity3pm','Sunshine','Pressure3pm','WindGustSpeed','Cloud3pm','Rainfall','MinTemp','Evaporation']]\ny = clean_data['Target']\naccuracy.append(svm(X,y))\n\ntitle.append(\"3 Features\")\nX = clean_data[['Humidity3pm','Sunshine','Pressure3pm']]\ny = clean_data['Target']\naccuracy.append(svm(X,y))\n\ntitle.append(\"2 Features\")\nX = clean_data[['Humidity3pm','Pressure3pm']]\ny = clean_data['Target']\naccuracy.append(svm(X,y))\n\nscore = pd.DataFrame(np.array([accuracy]).T, columns=[\"Accuracy (%)\"], index=title)\ndisplay(score)","1ed4fa11":"X = clean_data[['Humidity3pm','Sunshine','Pressure3pm','WindGustSpeed','Cloud3pm','Rainfall','MinTemp','Evaporation']]\ny = clean_data['Target']\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X, columns=['Humidity3pm','Sunshine','Pressure3pm','WindGustSpeed','Cloud3pm','Rainfall','MinTemp','Evaporation'])\n\npca = PCA()\npca.fit(X,y)\nx_new = pca.transform(X)\npca_comp_T = pca.components_.T\n\nPC_weights = pd.DataFrame(pca_comp_T,columns=['PC'+str(i+1) for i in range(0,8)], index=list(X.columns))\ndisplay(PC_weights.iloc[:,:3])","cd53fb03":"def biplot(score, coeff , y, variables,size,colors,alpha,margins,pc1,pc2,extent=4):\n    variables = list(variables)\n    xs = score[:,0] # projection on PC1\n    ys = score[:,1] # projection on PC2\n    n = coeff.shape[0] # number of variables\n    fig, ax = plt.subplots(figsize=(15,15))\n    classes = np.unique(y)\n    colors = {0:'tab:orange',1:'tab:blue'}\n    for s,l in enumerate(classes):\n        plt.scatter(xs[y==l],ys[y==l], marker='o', s=size, alpha=alpha, c=colors[s]) \n    plt.margins(x=margins[0], y=margins[1]) \n    \n    for i in range(n):\n        #plot as arrows the variable scores (each variable has a score for PC1 and one for PC2)\n        plt.arrow(0, 0, coeff[i,pc1-1]*extent, coeff[i,pc2-1]*extent, color = 'k', alpha = 0.9,linestyle = '-',linewidth = 0.8, overhang=0.5)\n        plt.text(coeff[i,pc1-1]* extent, coeff[i,pc2-1] * extent, variables[i], color = 'k', ha = 'center', va = 'center',fontsize=12,weight='bold')\n\n    plt.xlabel(\"PC{}\".format(pc1), size=12,weight='bold')\n    plt.ylabel(\"PC{}\".format(pc2), size=12,weight='bold')\n    ax.spines[[\"right\", \"top\"]].set_visible(False)\n\nPC1 = 1\nPC2 = 2\nbiplot(\n    #PCAs, Feature vectors, labels, column names\n    x_new[:,PC1-1:PC2], pca_comp_T, y, X.columns,\n    #size, colours, alpha, margins xy, PCs \n    5,plt.cm.Set1, 0.3,[-0.3,0],PC1,PC2,\n    )","4a122930":"def screeplot(var_explained):\n    pca_no = len(var_explained)\n    fig, ax = plt.subplots(figsize=(pca_no,5))\n    \n    plt.plot(var_explained,linestyle='--',marker='o')\n    ax.spines[[\"right\", \"top\", \"left\", \"bottom\"]].set_visible(False)\n    ax.xaxis.grid(True,linestyle=\":\",color='black')\n    ax.yaxis.grid(True,linestyle=\":\",color='black')\n    plt.xticks(np.arange(0,pca_no,step=1),['PC'+str(i+1) for i in range(0,pca_no)])\n\n    for i in range(pca_no):\n        ax.text(i,var_explained[i]+0.005, '%10.2f%%' % (var_explained[i]*100), color='tab:blue')\n\n    plt.xlabel(\"Principal Components\", size=10)\n    plt.ylabel(\"Explained Variance\", size=10)\n    plt.title(\"Scree Plot\")\n\nscreeplot(pca.explained_variance_ratio_)","6dc641ec":"pca_matrix = pd.DataFrame(x_new,columns=['PC'+str(i+1) for i in range(0,8)])\n\ntitle.append(\"3 PCs\")\naccuracy.append(svm(pca_matrix.iloc[:,:3], y))\n\ntitle.append(\"2 PCs\")\naccuracy.append(svm(pca_matrix.iloc[:,:2], y))\n\nscore = pd.DataFrame(np.array([accuracy]).T, columns=[\"Accuracy (%)\"], index=title)\ndisplay(score)","0ca30eb8":"def plot3D(features,target,elev,azim,title,size=40,alpha=0.9,edgecolor=\"white\"):\n    plot = Axes3D(plt.figure(1,figsize=(18,18)), elev=elev, azim=azim)\n    classes = np.unique(target)\n    colors = {\"No\":'tab:orange',\"Yes\":'tab:blue'}\n    target = target.replace(colors)\n    plot.scatter(features[0],features[1],features[2],c=target, edgecolor=edgecolor, s=size, alpha=alpha)\n    plot.set_xlabel(features[0].name)\n    plot.set_ylabel(features[1].name)\n    plot.set_zlabel(features[2].name)\n    plt.title(title, size=12, weight=\"bold\")\n    plt.show()\n\nplot3D(\n       #features\n       (X['Sunshine'],X['Pressure3pm'],X['Humidity3pm']),\n       #target, elevation, azimuth\n       y,-170,170,\n        \"Humidity3pm, Pressure3pm, Sunshine\"\n      )","4d173144":"plot3D(\n       #features\n       (pca_matrix.iloc[:,0],pca_matrix.iloc[:,1],pca_matrix.iloc[:,2]),\n       #target, elevation, azimuth\n       y,-170,75,\n    \"PC1, PC2, PC3\"\n      )","ad974891":"The table below summarizes predictive accuracies achieved on held-out datapoints. We can see that by selecting 3 main features \u201cHumidity3pm\u201d, \u201cPressure3pm\u201d and \u201cSunshine\u201d, it is possible to maintain the most of predictive power. It can be also observed that decomposing data into PCs maintains higher accuracy than selecting the same amount of the strongest features: 78.98% compared to 78.36% for 3-dimensional datasets and 78.50% compared to 75.76% for 2-dimensional datasets.","37ce19d3":"Then, we can investigate the amount of missing information in each weather station. As it can be seen, approximately half of the weather stations has 100% of observations that miss at least one value. Melbourne Airport has the least amount of observations with missing values - only 1%.","2ea934ad":"From scatterplots of 3 selected features, we can observe a good separation of classes. The colours represent the target class, with orange indicating \u2018No Rain Tomorrow\u2019 and blue - \u2018Rain Tomorrow\u2019. The first and third scatterplot show clear separation of target classes at approximately 65% thershold of humidity level at 3pm: the humidity above that threshold significantly increases chances of rain the next day. Similarly, pressure below 1005 hpa signals higher probability of the rain the next day. Although sparsity of datapoints below 1005 hpa threshold indicates that in general pressure below that level occurs rarely in Australia. Finally, sunshine for more than 7 hours during the day reduces probability of rain the next day. Negative correlation between \u2018Sunshine\u2019 and \u2018Humidity\u2019 can be also noticed, which may be explained by the fact that the sunshine may reduce the level of humidity.\n\nTo investigate the effect that the feature selection has on predicitive accuracy, I have tested the accuracy of SVM prediction on 3 datasets containing 8 features, 3 features and 2 features. Before training SVM classifier I have standardised data to standard deviation 1 and mean 0. Then, I have balanced the data by Random Undersampling and held out 25% of test data for measuring accuracy. I have used cross-validation grid search to select the best hyperparameters. Radial kernel with C100 and Gamma 0.001 worked best for 8 and 3 dimensional dataset, while linear kernal with C10 was selected for 2 dimensional feature space. As expected, the accuracy decreased with lower amount of features. Interestingly, reducing dimensions from 8 features to 3 features lowered the accuracy by only 1.08%, while reducing dimensions from 3 features to 2 features cut the accuracy by 3.09%. This implies strong predicitive qualitities of three selected features \u2018Sunshine\u2019, \u2018Humidity3pm\u2019 and \u2018Pressure3pm\u2019 if compared with other features.","cf754ee2":"## Data Analysis of \"Rain in Australia\" dataset\n\nThe data was automatically collected between 01-12-2008 and 25-06-2017 from 49 weather stations by Australian Bureau of Meteorology. Overall, there are 23 features and 145,460 observations. The features include 17 continuous variables and 6 discrete variables. ","59027c9f":"Adaptive Boosting iteratively assigns the weights to each stump that splits on each feature. The weights are calculated by evaluating proportion of misclassified samples in each stump and assigning higher penalty on misclassified samples in the next iteration. The resulting weights of stumps are then used to give scoring for each feature.","f8d4ee3f":"3D plot showing relationship of \u2018PC1\u2019, \u2018PC2\u2019, \u2018PC3\u2019 dimensions","361ff3ae":"The dataset is imbalanced with regards to the target variable \u201cRain Tomorrow\u201d: there are 31,877 observations of \u2018Yes\u2019 and 110,316 observations of \u2018No\u2019. The class distribution is skewed towards \u201cNo rain\u201d. Assuming that the data was collected consistently and without measurement errors, the skew signifies that rainy days are approximately 3 times less frequent than days without rain in Australia. ","1d2e50ef":"### 2. Correlation and feature selection\n\nTo select the most important features, correlation matrix can be analysed. As it was expected, the same measurements taken on different times of the day were correlated between each other. The features can be broken into the following correlated groups:\n* Humidity9am and Humidity3pm\n* MinTemp, MaxTemp, Temp9a and Temp3pm\n* Pressure 9am and Pressure 3pm,\n* Cloud 9am and Cloud 3pm,\n* RainToday and Rainfall,\n* WindSpeed 9am and WindGustSpeed\n\nAs the features within each group are correlated, it is reasonable to assume that each additional feature within the group does not provide new information for the purpose of discriminating the target class.","fe1964bb":"###  1. Data quality and integrity\n\nBy grouping the data by each weather station (\u201cLocation\u201d variable), we can compare the timeframes when the data was collected. As it can be seen, most stations started collecting the data from 2009. Canberra has collected data for the longest period \u2013 3,524 days, while Katherine, Uluru and Nhil collected the data for the shortest period \u2013 1,577 days, starting only in Q1 of 2013.","76646c44":"The features that had the strongest positive correlation with the target variable are: \u201cCloud3pm\u201d, \u201cHumidity3pm\u201d, \u201cRainToday\u201d and \u201cWindGustSpeed\u201d; and the features with strongest negative correlation with the target are: \u201cSunshine\u201d, \u201cPressure9am\u201d and \u201cTemp3pm\u201d. In order to verify whether those are the most important features, I have used Decision Tree, Random Forest and Adaptive Boosting algorithms to get importance scoring. Before running the above algorithms, I have removed all observations with missing values and performed random undersampling to balance the dataset. For tree-based algorithms the scaling of variables is not required, because the trees are not sensitive to range differences between features. The classification algorithms were performed on 12,427 data points.\n\nDecision Tree is a greedy algorithm: for each subsequent branch it selects the feature that contributes to the highest reduction of entropy.","fc32403b":"Biplot (or loading plot) shows the influence each feature has on first two Principal Components. It can be seen that \u2018Humidity3pm\u2019, \u2018Cloud3pm\u2019 and \u2018Sunshine\u2019 have significant weight on PC1, while \u2018Pressure3pm\u2019, \u2018WindGustSpeed\u2019 and \u2018MinTemp\u2019 have higher weight on PC2. The weights describe the influence each feature has on each principal component. These can be also examined by looking at linear combination matrix (above). From linear combination matrix follows, for example, that \u2018Sunshine\u2019 has the highest absolute weight on PC1 (0.54) and WindGustSpeed has the lowest absolute weight on PC1 (0.03). It can be also observed that \u2018Humidity3pm\u2019, \u2018Cloud3pm\u2019 and \u2018Rainfall\u2019 have strong positive correlation between each other and \u2018Humidity3pm\u2019 and \u2018Sunshine\u2019 have strong negative correlation.","b32f291b":"#### *B. Humidity at 3pm*\n\nFrom swarmplot of \u201cHumidity3pm\u201d, it can be seen that the classes can be separated at the threshold of approximatelty 70%: the higher humidity increases likelihood of rain the next day. It can be also seen that the range of humidity is narrower in summer than in the winter, with summer being more humid. The overall range is from 0 to 100% with mean of 51.5% and median 52%. The standard deviation is 21%.","00b8f5cd":"### 5. Eigenvector Decomposition \n\nIn order to reduce number of data dimensions while preserving predictive qualities, we can look into eigenvector decomposition. In PCA. the first Principal Component is fit in a way that maximizes variance of projected data. Therefore, to avoid larger ranges distorting the fit, it is important to scale features before the decomposition. After verifying that all selected features have Gaussian distribution, we can standardise the data to mean 0 and standard deviation of 1. The linear combination matrix below shows the decomposed Principle Components and respective weight of each feature.","9dfaba43":"The plot below shows the proportion of missing values in each feature across all weather stations. \u2018Sunshine\u2019 and \u2018Evaporation\u2019 features have the largest proportion of missing values - 48% and 43% respectively.","f18343b6":"### 4. Feature selection\n\nFrom previous sections, the \u2018Sunshine\u2019, \u2018Humidity3pm\u2019 and \u2018Pressure3pm\u2019 were ranked as the strongest predictors for \u2018Rain Tomorrow\u2019 target class.","0b50241a":"#### *F. Wind Gust Speed*\n\n\u2018WindGustSpeed\u2019 is the 4th most important predictor according to Decision Tree and AdaBoost algorithms. From the boxplot, it can be concluded that the higher \u2018WindGustSpeed\u2019 approximately above 55 km\/h increases likelihood of rain the next day. The dimension has a range of 6 km\/h to 135 km\/h with mean of 40 km\/h and median of 39 km\/h. The standard deviation is 13.6 km\/h.","9b47b87f":"#### *G. Cloud cover at 3pm*\n\n\u2018Cloud3pm\u2019 scored higher in Random Tree classifier, than in the Decision Tree algorithm. From the boxplot, it can be seen that the likelihood of rain the next day increases with median cloud coverage being above approximately 7 octas. The dimension has a range of 0 to 9 octas with mean of 4.5 and median of 5 octas. The standard deviation is 2.72 octas.","ec6397ce":"Random Forest algorithm uses ensemble of uncorrelated Decision Trees. Each of those trees are built using square root of p randomly selected features (where p is the total number of features). The scoring shows the extent by which selected features reduces impurity across the trees in the ensemble. The advantage of Random Forest is that by selecting small sample of predictors in each iteration, the weaker predictors may be allowed at the higher level of tree, thus reducing the variance.","6535cc32":"### 3. Key dimensions\n\nIn order to achieve comrehensible and efficient visualisation, while showing distribution of all datapoints, representative sample can be used instead of the main dataset. With the least amount of missing values, the sample collected from Melbourne Airport is a good candidate for being representative of the whole \u2018Rain in Australia\u2019 dataset. To confirm this choice, The comparison of the distrubution of data in each feature in both Melbourn Airport subset and \u2018Rain in Australia\u2019 should be analysed. From the plot below, we can see that apart of three features representing the direction of wind (\u201cWindGustDir\u201d, \u201cWindDir9am\u201d, WindDir3pm\u201d), the distribution of both datasets are similar. ","fd27fd5c":"3D plot showing relationship of \u2018Humidity3pm\u2019, \u2018Pressure3pm\u2019, \u2018Sunshine\u2019 dimensions","4e5dac05":"#### *D. Temperature at 9am*\n\nFeature \u2018Temp9am\u2019 scored significantly lower in tree based algorithms in the previous section. As expected, from the swarmplot we cannot see clear target class separation. It can be also observed that temperature swings are significantly wider in the winter time than in the summer time. The dimension has a range of -7.2 to 40.2 hpa with mean of 17 degrees and median of 16.7 degrees celsius. The hottest temperature at 9am was recorded in Pearce on 12th Janurary 2014 and the coldest temperature was recorded in Mount Ginini on 13th July 2016. The standard deviation is 6.5 degrees celsius.","0cb03c19":"#### *A. Sunshine*\n\nGiven that the dataset is time series, it is important to include time dimension in the feature analysis. The swarmplot distributions below show number of hours of sunshine per day on yearly basis. The colours represent the target class, with orange indicating \u2018No rain tomorrow\u2019 and blue \u2013 \u2018Rain tomorrow\u2019. As it can be seen, the feature space can be split at approximatelty 7 hour threshold: if there is a less than 7 hour of sunshine on the day, there is higher likelihood of rain the next day. The dimension has a range of 0 to 14.5 hours with mean of 7.6 hours and median 8.4 hours. The standard deviation is 3.78 h. To get meaningful insight into changes throughout the year, looking at the same datapoints on the monthly basis is more useful. It can be observed that in winter time, there is considerably higher amount of sunny days and the rainy days are mainly concentrated in summer period.","b91f2067":"From the Screeplot below, it can seen that the cumulative variance explained by 2 Principal Components is 57.86%. The accuracy of SVM prediction based on first two Pricipal Components is 78.53%, which is considerably higher than the predictive accuracy of 2 selected features (75.76%), but is slightly lower than the predictive accuracy of all 8 features (79.93%). This shows that PC1 and PC2 have absorbed high level of predictive power from 8 features, which allows 2 PCs to achieve better performance than 2 best features.","907dada5":"#### *C. Pressure at 3pm*\n\nThe swarmplot \u201cPressure3pm\u201d shows that the pressure is higher in summer period than in winter period. The threhold that separates two classes can be seen at approximately 1010 hpa in winter and 1020 hpa in summer. The dimension has a range of 977 to 1039 hpa with mean and median of 1015 hpa. The standard deviation is 7 hpa.","3932c56c":"#### *E. Wind Gust Direction*\n\n\u2018WindGustDir\u2019 has high entropy across all weather stations and it scored poorly with tree-based methods. It can be observed that the frequency of wind gust directions is evenly distributed in Australia, with North-East direction being slightly less frequent and West direction being the dominant with 7.5% of all occurances. Given high entropy, it is no surprise that the probability of the rain on the following day is similar in all wind gust directions, with marginally higher probability of rain after Northwest wind gusts. As it has been noted from distribution analysis in previous section, WindGustDir in Melbourne is less equally distributed than in the whole Australia. It can be seen that the North is the most frequent wind gust direction in Melbourne and the East direction is almost absent. Accordingly, the information gain from east direction is larger and the probability of raining the day after wind gust direction from East is much higher."}}