{"cell_type":{"6a7347aa":"code","86d2d879":"code","8c0f01ab":"code","24798f7d":"code","c431367a":"code","c391e37c":"code","aa4cb4b0":"code","1bc54322":"code","f0aed49c":"code","46f5c708":"code","b3a90f4d":"code","af072a08":"code","66f51ecf":"code","0a732282":"code","9373ae0e":"code","68274f26":"code","f0716a2b":"code","5a75a7e0":"code","89dcbd5c":"code","8ecb31e4":"code","69500033":"code","a54a64c5":"code","500d35aa":"code","f3a45493":"code","45a18211":"code","b94a910f":"code","60031077":"code","34b3e33b":"code","78444bbb":"code","5a97e74f":"code","dc4486ab":"code","6601d015":"code","f67cdb27":"code","45eaee96":"code","8475532b":"code","1c4c38e3":"code","b6f5ed53":"code","dfbd103c":"code","d96c9097":"code","83c07c87":"code","cb006c04":"code","5c3a7304":"code","47ab3510":"code","a0d92153":"code","c8c11cc9":"code","b6f46279":"code","72446949":"code","e0a4f834":"markdown","ce72e43e":"markdown","9a0adc21":"markdown","2331574d":"markdown","fa1f8568":"markdown","5941e54c":"markdown","42d0725e":"markdown","ae8e47ee":"markdown","2ff17c51":"markdown","1ed8f383":"markdown","a8ee66e3":"markdown","d355afd0":"markdown","2e240357":"markdown","102b84c5":"markdown","0c1c867b":"markdown","f4ccf393":"markdown"},"source":{"6a7347aa":"!pip install pandas-profiling","86d2d879":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport json\n# import altair as alt\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# alt.renderers.enable('notebook')\n","8c0f01ab":"import pandas_profiling\nimport plotly.graph_objs as go\n# import plotly.plotly as py\nimport plotly.offline as pyo\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly_express as px\ninit_notebook_mode(connected=True)\nfrom matplotlib import cm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nimport plotly_express as px","24798f7d":"folder_path = '..\/input\/'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\n# test_identity = pd.read_csv(f'{folder_path}test_identity.csv')\n# test_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\n# let's combine the data and work with the whole dataset\n# I will save this for later\n# train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n# test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","c431367a":"print(f'Train Transaction dataset has {train_transaction.shape[0]} rows and {train_transaction.shape[1]} columns.')","c391e37c":"# print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n# print(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')","aa4cb4b0":"train_transaction = train_transaction.sample(n=100000)\ntrain_transaction.head()","1bc54322":"trx_colnames = train_transaction.columns\nfor i in range(len(trx_colnames)):\n    print(i, ': ',trx_colnames[i])","f0aed49c":"trx_colnames_main = trx_colnames[1:17]\ntrx_colnames_CDM = trx_colnames[17:55]\ntrx_colnames_V = trx_colnames[55:]","46f5c708":"# profiler1 = train_transaction[trx_colnames_main].profile_report()\n# profiler1","b3a90f4d":"# profiler2 = train_transaction[trx_colnames_CDM].profile_report()\n# profiler2","af072a08":"import sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","66f51ecf":"trx_colnames_C = [c for c in trx_colnames if c.startswith(\"C\") ]\ntrx_colnames_C","0a732282":"trxC = train_transaction[trx_colnames_C]\ntrxClog = np.log1p(trxC)\nfig, ax = plt.subplots(15,3,figsize=(10,20))\nfor i,c in enumerate(trxC.columns):\n    trxC[c].hist(ax=ax[i,0])\n    trxClog[c].hist(ax=ax[i,1])\n    trxClog[c].hist(ax=ax[i,2],cumulative=True,density=True)\nplt.tight_layout()\nplt.suptitle('Distribution of C variables - C, log(C), cumulative log(C)')","9373ae0e":"from sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans","68274f26":"X = trxClog\npca = PCA(n_components=len(trx_colnames_C)).fit(X)\n#Plotting the Cumulative Summation of the Explained Variance\nexpvar=np.cumsum(pca.explained_variance_ratio_)\ndata = [go.Scatter(y=expvar)]\nlayout = {'title': 'Review PCA Explained Variance to determine number of components'}\niplot({'data':data,'layout':layout})","f0716a2b":"pca = PCA(n_components=4)\nXPCA = pca.fit_transform(X)\nNc = range(1,10)\nkmeans = [KMeans(i) for i in Nc]\nscore = [kmeans[i].fit(XPCA).score(XPCA) for i in range(len(kmeans))]\ndata = [go.Scatter(y=score,x=list(Nc))]\nlayout = {'title':'Review Elbow Curve to determine number of clusters for KMeans'}\niplot({'data':data,'layout':layout})","5a75a7e0":"from yellowbrick.features.pca import PCADecomposition\nvisualizer = PCADecomposition(scale=True)\nvisualizer.fit_transform(trxClog)\nvisualizer.poof()","89dcbd5c":"trx_colnames_V = [c for c in trx_colnames if c.startswith(\"V\") ]\ntrx_colnames_V","8ecb31e4":"trxV = train_transaction[trx_colnames_V]\ntrxVlog = np.log1p(trxV)\nfig, ax = plt.subplots(15,3,figsize=(10,20))\nfor i,c in enumerate(trxV.columns[:15]):\n    trxV[c].hist(ax=ax[i,0],bins=50)\n    trxVlog[c].hist(ax=ax[i,1],bins=50)\n    trxVlog[c].hist(ax=ax[i,2],bins=50,cumulative=True,density=True,histtype='step')\nplt.tight_layout()\nplt.suptitle('Distribution of V variables - V, log(V), cumulative log(V)')","69500033":"X = trxV.fillna(0)\npca = PCA(n_components=10).fit(X)\n#Plotting the Cumulative Summation of the Explained Variance\nexpvar=np.cumsum(pca.explained_variance_ratio_)\ndata = [go.Scatter(y=expvar)]\nlayout = {'title': 'Review PCA Explained Variance to determine number of components'}\niplot({'data':data,'layout':layout})","a54a64c5":"from yellowbrick.features.pca import PCADecomposition\nvisualizer = PCADecomposition(scale=True)\nvisualizer.fit_transform(trxV.fillna(0))\nvisualizer.poof()","500d35aa":"def make_day_feature(df, offset=0, tname='TransactionDT'):\n    \"\"\"\n    Creates a day of the week feature, encoded as 0-6. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    offset : float (default=0)\n        offset (in days) to shift the start\/end of a day.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    # found a good offset is 0.58\n    days = df[tname] \/ (3600*24)        \n    encoded_days = np.floor(days-1+offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    hours = df[tname] \/ (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')","f3a45493":"train_transaction['TransactionDateTime'] = train_transaction['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\ntrain_transaction['TransactionDate'] = [x.date() for x in train_transaction['TransactionDateTime']]\n# train_transaction['TransactionDateHour'] = train_transaction['TransactionDateTime'].date()\ntrain_transaction['TransactionHour'] = train_transaction.TransactionDT \/\/ 3600\ntrain_transaction['TransactionHourOfDay'] = train_transaction['TransactionHour'] % 24\ntrain_transaction['TransactionDay'] = train_transaction.TransactionDT \/\/ (3600 * 24)","45a18211":"train_transaction.head(10)","b94a910f":"agg_dict = {}\nfor col in trx_colnames_C:\n    agg_dict[col] = ['mean','sum']\nprint(agg_dict)","60031077":"train_trx_hour = train_transaction.groupby(['TransactionHour']).agg(agg_dict).reset_index()\ntrain_trx_hour.head(5)","34b3e33b":"train_trx_hour.columns.values","78444bbb":"train_trx_hour.columns = ['_'.join(col).strip() for col in train_trx_hour.columns.values]\ntrain_trx_hour.head()","5a97e74f":"train_trx_hour.columns = ['TrxHourAgg_' + col for col in train_trx_hour.columns.values]\ntrain_trx_hour.head()","dc4486ab":"train_trx_hour.DShour = pd.to_datetime(train_trx_hour.TransactionHour)\ntrain_trx_hour.head()","6601d015":"train_trx_day = train_transaction.groupby(['TransactionDay'])[\"isFraud\",\"TransactionAmt\"].mean().reset_index()\ntrain_trx_day.head()","f67cdb27":"fig = px.line(train_trx_hour.iloc[:1000,:],x=\"TransactionHour\",y=\"isFraud\",title='Average Fraud Rate by Hour')\nfig.show()","45eaee96":"train_trx_date = train_transaction.groupby(['TransactionDate']).agg({'isFraud':['mean','sum'],'TransactionAmt':['count','mean','sum']}).reset_index()\ntrain_trx_date.head()","8475532b":"train_trx_date.columns = train_trx_date.columns.get_level_values(0) + '_' + train_trx_date.columns.get_level_values(1)\ntrain_trx_date.head()","1c4c38e3":"fig = px.line(train_trx_date,x=\"TransactionDate_\",y=\"isFraud_sum\",title=\"Total Frauds by Date\")\nfig.show()","b6f5ed53":"px.scatter(train_trx_date,x=\"TransactionAmt_count\",y=\"isFraud_mean\",trendline='lowess',title=\"Relationship between Transaction Amount and Fraud Rate\")","dfbd103c":"px.scatter(train_trx_date,x=\"TransactionAmt_count\",y=\"isFraud_sum\",trendline='lowess',title='Relationship between Transaction Amount and Number of Frauds (abs)')","d96c9097":"train_trx_date[\"DayOfWeek\"] = [x.weekday() for x in train_trx_date.TransactionDate_]\ntrain_trx_date[\"DayOfWeek\"] = train_trx_date[\"DayOfWeek\"].apply(str)\ntrain_trx_date.head()","83c07c87":"px.scatter(train_trx_date,x=\"TransactionAmt_count\",y=\"isFraud_mean\",color='DayOfWeek',marginal_y='box',trendline='ols',title=\"Fraud rate vs. Transaction Amount\")","cb006c04":"px.scatter(train_trx_date,x=\"TransactionAmt_count\",y=\"isFraud_sum\",color='DayOfWeek',marginal_y='box',trendline='ols',title=\"Fraud Count vs. Transaction Amount\")","5c3a7304":"train_trx_day['TransactionDay'].dtype\n","47ab3510":"datetime.date.fromordinal(2)","a0d92153":"from fbprophet import Prophet\ndf = train_trx_date[['TransactionDate_','isFraud_sum']]\ndf.columns = ['ds','y']\nprophet = Prophet()\nprophet.fit(df)\nforecast = prophet.predict(df)","c8c11cc9":"forecast.head()","b6f46279":"fig,ax = plt.subplots(1,3,figsize=(20,5))\nforecast.weekly[:7].plot(ax=ax[0],ylim=(-2,40))\nax[0].set_title(\"weekly component\")\nforecast.trend.plot(ax=ax[1],ylim=(-2,40))\nax[1].set_title(\"trend component\")\n# ax[1].xticks(forecast.ds)\nax[2].plot(forecast.yhat)\nax[2].plot(df.y)\nax[2].set_ylim(-2,40)\nax[2].set_title(\"comparing fitted vs. actual\")\nplt.suptitle('Avg Fraud Count based on: Day Of Week, Long-term Trend, Seasonality vs. Actual')","72446949":"df = train_trx_date[['TransactionDate_','isFraud_mean']]\ndf.columns = ['ds','y']\nprophet = Prophet()\nprophet.fit(df)\nforecast = prophet.predict(df)\n\nfig,ax = plt.subplots(1,3,figsize=(20,5))\nforecast.weekly[:7].plot(ax=ax[0],ylim=(-0.01,0.09))\nax[0].set_title(\"weekly component\")\nforecast.trend.plot(ax=ax[1],ylim=(-0.01,0.09))\nax[1].set_title(\"trend component\")\nax[2].plot(forecast.yhat)\nax[2].plot(df.y)\nax[2].set_ylim(-0.01,0.09)\nax[2].set_title(\"comparing fitted vs. actual\")\nplt.suptitle('Avg Fraud Rate based on: Day Of Week, Long-term Trend, Seasonality vs. Actual')","e0a4f834":"# Util to check memory consumption","ce72e43e":"Check out pandas profiling report below. This library generates a summary of dataset, including number of null, distribution of values, correlation, etc. It also suggests which variables should be dropped due to high correlation. I am commenting it out because it consumes tons of memory, so I'm running it offline one-time.","9a0adc21":"This is a useful function to check the memory consumption size of our python objects:","2331574d":"# EDA - Train_Transaction","fa1f8568":"# Loading data","5941e54c":"# This is still very early. To be continued!","42d0725e":"# Variable: Vs","ae8e47ee":"# Datetime analysis","2ff17c51":"# EDA - Transaction - Key Takeaways from Pandas Profiler\n* C: The variables here are highly correlated\n* D: A lot of zeros or missing to deal with\n* M: Half is missing! \n* A lot of them are highlyg skewed, or having a very long tail to the right\n","1ed8f383":"There is too much na. Need to strategize around na. May make more sense to convert the V into categorical, and then do embedding.","a8ee66e3":"Let's quickly check the transaction table","d355afd0":"## Fraud Rate have relative small weekday seasonality, compared to overall variability","2e240357":"# Variable: Cs","102b84c5":"# Overview\n\nFirst dip into this dataset! Will do some EDAs to understand the nature of the dataset and variables. I'm using Pandas Profiling to generate quick standard EDA before deep-diving\n\nReference notebooks:\n* https:\/\/www.kaggle.com\/artgor\/eda-and-models: Join trx and cust from the start. Histograms of variables (except V variables). LGB pred\n* https:\/\/www.kaggle.com\/jazivxt\/safe-box: Uses the1owl package for autoML\n* https:\/\/www.kaggle.com\/jesucristo\/fraud-complete-eda: time series analysis","0c1c867b":"Reference: \n* https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature\n* https:\/\/www.kaggle.com\/kevinbonnes\/transactiondt-starting-at-2017-12-01","f4ccf393":"We don't need to do log transformation"}}