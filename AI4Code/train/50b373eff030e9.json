{"cell_type":{"f665a36b":"code","443f25ed":"code","dd3934dc":"code","728bfa9a":"code","941070ae":"code","b7d8a54a":"code","4f13037c":"code","6725d55c":"code","c5c7d99a":"code","9d6a783b":"code","2e4f271f":"code","1fccbc0a":"code","2e5d26ce":"code","1530d526":"code","3f7be00e":"code","6010d89b":"code","9397f5a4":"code","69cb5650":"code","48c1b7b7":"code","7ead4965":"code","30d192a7":"code","36a4398c":"code","853f352f":"code","01a32240":"code","2f9991a8":"code","edd9cc2e":"code","291b3534":"code","2dad4e10":"code","5b2a0713":"code","ade5cf85":"code","47d078ae":"markdown","3d6337a4":"markdown","d64221fd":"markdown","28440625":"markdown","01f785dd":"markdown","190631b0":"markdown","4ed2aa48":"markdown","c4d3cd2f":"markdown","4b1f9cc9":"markdown","6b556a51":"markdown","cb944187":"markdown","2fd1abbd":"markdown","f5cc84fa":"markdown","d366632d":"markdown","28f21f19":"markdown","efab9836":"markdown","8ef79ba1":"markdown","1b4cf609":"markdown","212a84e6":"markdown","72ef7a90":"markdown","10f4ce7e":"markdown","0fcbf31d":"markdown","a7d8616d":"markdown","50c8ebcf":"markdown","1238a39e":"markdown","79e6c3ca":"markdown"},"source":{"f665a36b":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM","443f25ed":"# Tesla stock: TSLA\ncompany = 'Tesla' \nstock_data = pd.read_csv('\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/Stocks\/tsla.us.txt', parse_dates=['Date'], sep=',', index_col='Date')","dd3934dc":"stock_data","728bfa9a":"# Plot closing prices\ndf_close = stock_data['Close']\n\nplt.figure(figsize=(10,6))\nplt.grid()\nplt.plot(df_close)\nplt.xlabel('Date')\nplt.ylabel('Closing Prices')\nplt.title('Tesla stock closing price')","941070ae":"def test_stationarity(timeseries):\n    '''\n    Input: timeseries (dataframe): timeseries for which we want to study the stationarity\n    '''\n    \n    #Determing rolling statistics\n    rolmean = timeseries.rolling(20).mean()\n    rolstd = timeseries.rolling(20).std()\n\n    #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n\n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value',\\\n                                             '#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)","b7d8a54a":"plt.figure(figsize = (10,6))\ntest_stationarity(df_close.head(2000))","4f13037c":"#To plot the trend and the seasonality\n# We set the period to 28 as we have in average 7*4 = 28 days in a \n# month\n# The result below is just to have a first visualization of trend and\n# seasonality. Here we take a monthly average but to forecast a stock \n# price, it is more difficult than that to choose the right number of\n# days to average on\nresult = seasonal_decompose(df_close, model='multiplicative',period=28)\nfig = plt.figure()  \nfig = result.plot()  \nfig.set_size_inches(16, 9)","6725d55c":"df_close_log = df_close.apply(np.log)\ndf_close_tf = df_close_log.apply(np.sqrt)\n\nplt.figure(figsize = (10,6))\nplt.plot(df_close_tf)\nplt.title('Transformed data')","c5c7d99a":"df_close_shift = df_close_tf - df_close_tf.shift()\n\ndf_close_shift.dropna(inplace=True)\nplt.figure(figsize = (10,6))\ntest_stationarity(df_close_shift)","9d6a783b":"def preprocess_lstm(sequence, n_steps,n_features):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the sequence\n        if end_ix >= len(sequence):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n        \n    X = np.array(X)\n    y = np.array(y)\n\n    X = X.reshape((X.shape[0], X.shape[1], n_features))\n    return X, y","2e4f271f":"# choose the number of days on which to base our predictions \nnb_days = 60\n\nn_features = 1\n\nX, y = preprocess_lstm(df_close_shift.to_numpy(), nb_days, n_features)","1fccbc0a":"#Split the data set between the training set and the test set\ntest_days = 365 \n\nX_train, y_train = X[:-test_days], y[:-test_days]\nX_test, y_test = X[-test_days:], y[-test_days:]","2e5d26ce":"train_original = df_close.iloc[:-test_days]\ntest_original = df_close.iloc[-test_days:]\n\nplt.figure(figsize=(10,6))\nplt.grid(True)\nplt.xlabel('Dates')\nplt.ylabel('Closing Prices')\nplt.plot(train_original, 'b', label='Train data')\nplt.plot(test_original, 'g', label='Test data')\nplt.legend()","1530d526":"def vanilla_LSTM():\n    model = Sequential()    \n    model.add(LSTM(units=50, input_shape=(nb_days, n_features)))\n    model.add(Dense(1))\n    return model","3f7be00e":"model = vanilla_LSTM()\nmodel.summary()\nmodel.compile(optimizer='adam', \n              loss='mean_squared_error',\n              metrics=[tf.keras.metrics.MeanAbsoluteError()])","6010d89b":"model.fit(X_train, \n          y_train, \n          epochs=15, \n          batch_size = 32)","9397f5a4":"# Evaluate the model on the test data using\nprint(\"Evaluate on test data\")\nresults = model.evaluate(X_test, y_test, batch_size=32)\nprint(\"Test MSE:\", results[0])\nprint(\"Test MAE:\", results[1])","69cb5650":"# Prediction\ny_pred = model.predict(X_test)\n\n# We create a dataframe from y_pred to have date-time indexes.\npred_data = pd.DataFrame(y_pred[:,0], test_original.index,columns=['Close'])\n\n# Apply inverse transformation from 1.d\n\n# Add the differenciation term\npred_data['Close'] = pred_data['Close'] + df_close_tf.shift().values[-test_days:] \n\n# Take the square, and the exponent\npred_data = pred_data.apply(np.square)\npred_data = pred_data.apply(np.exp)\n\n\n# Plot actual prices vs predicted prices \nplt.figure(figsize=(10,6))\nplt.grid(True)\nplt.xlabel('Dates')\nplt.ylabel('Closing Prices')\nplt.plot(test_original,'b',label='Actual prices')\nplt.plot(pred_data, 'orange',label='Predicted prices')\nplt.title(company + ' Stock Price')\n\nplt.legend()","48c1b7b7":"\nplt.figure(figsize=(10,6))\nplt.grid(True)\nplt.xlabel('Dates')\nplt.ylabel('Closing Prices')\nplt.plot(train_original, 'b', label='Train data')\nplt.plot(test_original, 'g', label='Test data')\nplt.plot(pred_data, 'orange', label='Prediction')\nplt.title(company + ' Stock Price')\nplt.legend()","7ead4965":"def preprocess_multistep_lstm(sequence, n_steps_in, n_steps_out, features):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the sequence\n        if out_end_ix > len(sequence):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n\n    X = np.array(X)\n    y = np.array(y)\n\n    X = X.reshape((X.shape[0], X.shape[1], n_features))\n    \n    return X, y","30d192a7":"# Number of days into the future we want to predict\nn_steps_out = 10\n\n# choose the number of days on which to base our predictions \nnb_days = 60\n\nn_features = 1\n\nX, y = preprocess_multistep_lstm(df_close_shift.to_numpy(), nb_days, n_steps_out, n_features)\n","36a4398c":"#Split the data set between the training set and the test set\ntest_days = 365 \n\nX_train, y_train = X[:-test_days], y[:-test_days]\nX_test, y_test = X[-test_days:], y[-test_days:]","853f352f":"def vanilla_multistep_LSTM():\n    model = Sequential()    \n    model.add(LSTM(units=50, input_shape=(nb_days, n_features)))\n    model.add(Dense(n_steps_out))\n    return model","01a32240":"model = vanilla_multistep_LSTM()\nmodel.summary()\nmodel.compile(optimizer='adam', \n              loss='mean_squared_error',\n              metrics=[tf.keras.metrics.MeanAbsoluteError()])","2f9991a8":"model.fit(X_train, \n          y_train, \n          epochs=15, \n          batch_size = 32)","edd9cc2e":"# Evaluate the model on the test set\nprint(\"Evaluate on test data\")\nresults = model.evaluate(X_test, y_test, batch_size=32)\n\nprint(\"Test MSE:\", results[0])\nprint(\"Test MAE:\", results[1])","291b3534":"# Prediction\ny_pred = model.predict(X_test)\n\n# the_day is the day from which we will study the n_steps_out-th dayS of prediction into \n# the future. Note: The first day start at index 0\nthe_day = 0\ny_pred_days = y_pred[the_day,:]\n\nplt.figure(figsize=(10,6))\nplt.grid(True)\nplt.plot(y_test[the_day,:],label='Orginal data - transformed')\nplt.plot(y_pred_days, color='red',label='Predictions - transformed')\nplt.xlabel('Time (days)')\nplt.ylabel('Closing Prices amplitude in the transformed space')\nplt.title('Original data vs predictions in the transformed space')","2dad4e10":"# Apply inverse transformations from 3.a\n\n# Add the differenciation term\npred_diff_cumsum = y_pred_days.cumsum()\n\nbase_number = df_close_tf.values[-test_days+the_day+nb_days-1]\nidx = test_original.iloc[the_day:the_day+n_steps_out].index\n\npred_tf = pd.Series(base_number, index=idx)\npred_tf = pred_tf.add(pred_diff_cumsum,fill_value=0)\n\nprint(pred_tf)","5b2a0713":"# Take the square, and the exponent\npred_log = pred_tf.apply(np.square)\npred = pred_log.apply(np.exp)\nprint(pred)","ade5cf85":"# Plot actual prices vs predicted prices \nplt.figure(figsize=(10,6))\nplt.grid(True)\nplt.xlabel('Dates')\nplt.ylabel('Closing Prices')\nplt.plot(test_original.iloc[max(0,the_day-30):the_day+n_steps_out],'b',label='Actual prices')\nplt.plot(pred, '-o',color='orange',label='Predicted prices')\nplt.title(company + ' Stock Price')\n\nplt.legend()","47d078ae":"We obtain significally better results:\n\nThe test statistic is a lot lower than the critical value at 1%. Therefore, we can say with more than 99% confidence that this is a stationary series. Moreover, as the p-value is inferior to the 5% threshold, the null hypothesis is rejected, meaning that the Dickey-Fuller test is verified. Thus, the time series is stationary.","3d6337a4":"Although our model have difficulties predicting sudden variations. Predictions are very close to reality.","d64221fd":"## 1. Data preprocessing <a class=\"anchor\" id=\"chapter1\"><\/a>\n### a. Upload and plot data <a class=\"anchor\" id=\"section_1_1\"><\/a>","28440625":"### b. Split Data (train set and test set) <a class=\"anchor\" id=\"section_2_2\"><\/a>\nWe split the set between a training set and a test set. For the test set, we take the last year (365 days) of the dataset.\n\nThen, we visualize both set with the original data.","01f785dd":"### b. Splitting Data <a class=\"anchor\" id=\"section_3_2\"><\/a>","190631b0":"Again the MSE and the MAE are quite low. Thus, our model generalize well.","4ed2aa48":"### d. Training and evaluating the model <a class=\"anchor\" id=\"section_1_4\"><\/a>","c4d3cd2f":"### e. Predictions <a class=\"anchor\" id=\"section_3_5\"><\/a>\n","4b1f9cc9":"## 3. Multi-Step LSTM <a class=\"anchor\" id=\"chapter3\"><\/a>\n\nLSTM model can also be used to predict several step into the future.\n\nLet's modify our code above to enable multi-step predictions.\n\n### a. Preprocessing <a class=\"anchor\" id=\"section_3_1\"><\/a>","6b556a51":"Both the Mean Square Error (MSE) and the Mean Absolute Error (MAE) are very loss. Thus our model generalize well.","cb944187":"As for the one step LSTM data need to be preprocessed in order to be passed to an Multi-steps LSTM model. In this section we will predict certain number of days into the future.\n\nExample: If we want to predict 3 days into the future\n\nInput: $[1,2,3,4,5,6,7,8,9,10]$\n\nInput preprocessed: \n     \n$X: [1,2,3,4], [2,3,4,5], [3,4,5,6], ...$\n\n$y: [5,6,7], [6,7,8], [7,8,9], ...$\n\nSimilarly to part 2.a, we reshape X to match what the model expects as inputs ($[samples, timesteps, features]$), we deal with a univariate series so the number of feature is one, and we choose the number of days on which to base our predictions to be 60 days.\n                 ","2fd1abbd":"Then we take the square and the exponent to go to the original space.","f5cc84fa":"## 2. LSTM <a class=\"anchor\" id=\"chapter2\"><\/a>\n\nWe will implement the Long Short-Term Memory (LSTM) cells to predict future values of the stock price.\n\nLSTMs are a special kind of Recurrent Neural Network (RNN) introduced by Hochreiter and Schmidhuber (1997). They are widely use to learn long term dependencies.\n\n### a. Preprocessing again for LSTM <a class=\"anchor\" id=\"section_2_1\"><\/a>\nData need to be preprocessed in order to be passed to an LSTM model. In this section we will predict one day into the future. For the Multi-Step LSTM model, please take a look at part 3. \n\nExample:\n\nInput: $[1,2,3,4,5,6,7,8,9,10]$\n\nInput preprocessed: \n     \n$X: [1,2,3,4], [2,3,4,5], [3,4,5,6], ...$\n\n$y: [5,6,7,...]$\n                        \nWe also need to reshape X as the model expects as input a training data of shape $[samples, timesteps, features]$.\n\nHere, we are working with a univariate series, so the number of feature is one.\n\nFinally, we choose the number of days on which to base our predictions to be 60 days.","d366632d":"We start by estimating and eliminating trends. As the time series is constantly increasing, we apply a log transform, and a square root transform to flatten it.","28f21f19":"### c. Implementing the model <a class=\"anchor\" id=\"section_3_3\"><\/a>","efab9836":"First, on the above graph, we can see that the rolling mean is increasing. Thus, the time series is not stationary.\n\nThe p-value is greater than the $5\\%$ critical value, so the null hypothesis cannot be rejected.\n\nRegarding the test statistic used in the augmented Dickey-Tuller statistic, it is a negative number. The more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence.\n\nHere, -0.84 is greater than the -2.86 critical value at the $95\\%$ confidence bound. Therefore, we can confirm that the null hypotheis cannot be rejected.\n\nLet's apply some transformation to make our time series stationary.\n\n### c. Making our time series stationary <a class=\"anchor\" id=\"section_1_3\"><\/a>\n\n\nThere are three major time series patterns: trend, seasonality, and cycles. Trend and cycles are usually combined into a single component, leaving us with a trend-cycle component, a seasonal component, and a remainder component (containing the rest of the time series).\n\nTo make our time series stationary, the principle is to estimate trend and seasonality, and to remove those from the time series. Then, we can apply the forecasting technique (i.e here LSTM), the final step being to convert the forcasted values into the original scale by adding the estimated trend and seasonality.","8ef79ba1":"## Useful Links <a class=\"anchor\" id=\"links\"><\/a>\n\nExamples of LSTM models in Python:\n\nhttps:\/\/machinelearningmastery.com\/how-to-develop-lstm-models-for-time-series-forecasting\/ \n\n\nUseful tutorial for timeseries forecasting: \n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/time-series-forecasting-codes-python\/\n \n\nOpen source \u201cGluon-TS\u201d. You can use DeepAR model for forecasting if you want an LSTM solution: \n\nhttps:\/\/github.com\/awslabs\/gluon-ts \n \n\nBook that explains forecasting principles:\n\nhttps:\/\/otexts.com\/fpp2\/ ","1b4cf609":"### e. Predictions <a class=\"anchor\" id=\"section_1_5\"><\/a>\n\nLet's predict stock prices on the test set.\n\nAfter calling model.predict(), we need to apply the part 1.d inverse transformations to the predicted data to be able to compare our predictions with the original data.\n","212a84e6":"### d. Training and evaluating the model <a class=\"anchor\" id=\"section_3_4\"><\/a>","72ef7a90":"Now, let's first plot n_steps_out days prediction from one time stamp.\nBelow we arbitraly choose the 1st day of the test set.\n\nHence, we are going to plot predictions made on the 1st day for all the 10 days after this day.","10f4ce7e":"In this notebook, we will try to predict Tesla stock prices using LSTM models. LSTMs are a special kind of Recurrent Neural Network (RNN) introduced by Hochreiter and Schmidhuber (1997). They are widely used to learn long term dependencies.\n\nFirst, we upload and preprocess data. Then, we implement a Vanilla LSTM model to predict one day into the future. Finally, we study a multi-Step LSTM to predict several days into the future.\n\nVery useful links are provided at the end of the notebook. Those links will help you go through simple data processing and LSTMs.\n\n### Summary\n* [1. Data Preprocessing](#chapter1)\n    * [a. Upload and plot data](#section_1_1)\n    * [b. Checking stationarity](#Section_1_2)\n    * [c. Making our time series stationary](#Section_1_3)\n* [2. LSTM](#chapter2)\n    * [a. Preprocessing again for LSTM](#section_2_1)\n    * [b. Split Data (train set and test set)](#section_2_2)\n    * [c. Implementing the model](#section_2_3)\n    * [d. Training and evaluating the model](#section_2_4)\n    * [e. Predictions](#section_2_5)\n* [3. Multi-Step LSTM](#chapter2)\n    * [a. Preprocessing](#section_3_1)\n    * [b. Splitting Data](#section_3_2)\n    * [c. Implementing the model](#section_3_3)\n    * [d. Training and evaluating the model](#section_3_4)\n    * [e. Predictions](#section_3_5)\n* [Useful Links](#links)\n\n    ","0fcbf31d":"Let's now scale back predictions to the the original scale by taking inverse transformations from 3.a.\n\nThe first, to convert predictions from the differencing space to the log space, we add predictions consecutively to the base number which is the last value used to make the predictions of interest.","a7d8616d":"### c. Implementing the model <a class=\"anchor\" id=\"section_2_3\"><\/a>\n\nWe implement the most simple LSTM model, the \"Vanilla LSTM\". This model consists in a single hidden layer of LSTM unit, and one output layer used to make predictions.","50c8ebcf":"### b. Checking stationarity <a class=\"anchor\" id=\"section_1_2\"><\/a>\nA time series is said to be stationary if its statistical properties such as mean, variance remain constant over time. The reason why we verify time series stationarity is because we cannot build a time series model if the time series is not stationary. Let's verify that the time series is stationary.\n\nTo do so, we will use the rolling statistics plots along with augmented Dickey-Fuller test results.\n\nThe Dickey-Fuller tests the null hypothesis that a unit root is present in a time series model. The alternative hypothesis is the stationarity of the model.\n\n\nNote: \nThe intuition behind the test is as follows. If the series y is stationary (or trend-stationary), then it has a tendency to return to a constant (or deterministically trending) mean. Therefore, large values will tend to be followed by smaller values (negative changes), and small values by larger values (positive changes). Accordingly, the level of the series will be a significant predictor of next period's change, and will have a negative coefficient. If, on the other hand, the series is integrated, then positive changes and negative changes will occur with probabilities that do not depend on the current level of the series; in a random walk, where you are now does not affect which way you will go next. (source: https:\/\/en.wikipedia.org\/wiki\/Dickey%E2%80%93Fuller_test)","1238a39e":"Let's know use one classical method to deal with trend and seasonality: differenciating.\nWe take the difference of the observation at a particular instant with that at the previous instant.","79e6c3ca":"# Predicting Stock Prices using Long Short-Term Memory (LSTM)"}}