{"cell_type":{"2f778206":"code","3ceeeb37":"code","1194bd47":"code","048fb551":"code","b06028a6":"code","80ec056c":"code","48da6bfd":"code","e5874a6f":"code","eac8191a":"code","db7b535f":"code","81ccd3c4":"code","1f571c57":"code","36cca115":"markdown","ae68905a":"markdown","ae216d91":"markdown","3254f6bd":"markdown","013c69a4":"markdown","e24b072c":"markdown","8fab9c43":"markdown","92dd9439":"markdown","b18685de":"markdown","7b8df660":"markdown","1902fb4d":"markdown","2ede3d49":"markdown"},"source":{"2f778206":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas import read_csv\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib\nmatplotlib.use('Agg')\nfrom matplotlib import pyplot","3ceeeb37":"data = read_csv('..\/input\/train.csv')\ndataset = data.values","1194bd47":"X = dataset[:,0:94]\ny = dataset[:,94]","048fb551":"label_encoded_y = LabelEncoder().fit_transform(y)\nmodel = XGBClassifier()","b06028a6":"subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\nparam_grid = dict(subsample=subsample)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\ngrid_result = grid_search.fit(X, label_encoded_y)","80ec056c":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))","48da6bfd":"pyplot.errorbar(subsample, means, yerr=stds)\npyplot.title(\"XGBoost subsample vs Log Loss\")\npyplot.xlabel('subsample')\npyplot.ylabel('Log Loss')","e5874a6f":"colsample_bytree = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\nparam_grid = dict(colsample_bytree=colsample_bytree)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\ngrid_result = grid_search.fit(X, label_encoded_y)","eac8191a":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))","db7b535f":"pyplot.errorbar(colsample_bytree, means, yerr=stds)\npyplot.title(\"XGBoost colsample_bytree vs Log Loss\")\npyplot.xlabel('colsample_bytree')\npyplot.ylabel('Log Loss')","81ccd3c4":"# grid search\nmodel = XGBClassifier()\ncolsample_bylevel = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\nparam_grid = dict(colsample_bylevel=colsample_bylevel)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\ngrid_result = grid_search.fit(X, label_encoded_y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))","1f571c57":"pyplot.errorbar(colsample_bylevel, means, yerr=stds)\npyplot.title(\"XGBoost colsample_bylevel vs Log Loss\")\npyplot.xlabel('colsample_bylevel')\npyplot.ylabel('Log Loss')","36cca115":"We can see that the best results were achieved by setting colsample_bylevel to 70%, resulting in an (inverted) log loss of -0.001062, which is better than -0.001239 seen when setting the per-tree column sampling to 100%.\n\nThis suggest to not give up on column subsampling if per-tree results suggest using 100% of columns, and to instead try per-split column subsampling.\n\nWe can plot the performance of each colsample_bylevel variation. The results show relatively low variance and seemingly a plateau in performance after a value of 0.3 at this scale.","ae68905a":"Split the data into X and y","ae216d91":"Encode string class values as integers","3254f6bd":"We can see that the best results achieved were 0.3, or training trees using a 30% sample of the training dataset.\n\nWe can plot these mean and standard deviation log loss values to get a better understanding of how performance varies with the subsample value.","013c69a4":"Summarize results","e24b072c":"### 1.  Subsampling of rows in the dataset when creating each tree.\n\nRow subsampling involves selecting a random sample of the training dataset without replacement.\n\nRow subsampling can be specified in the scikit-learn wrapper of the XGBoost class in the subsample parameter. The default is 1.0 which is no sub-sampling.\n\nWe can use the grid search capability built into scikit-learn to evaluate the effect of different subsample values from 0.1 to 1.0 on the Otto dataset.\n\nThere are 9 variations of subsample and each model will be evaluated using 10-fold cross validation, meaning that 9\u00d710 or 90 models need to be trained and tested.\n\nPerform K Fold cross validation using the GridSearchCV ","8fab9c43":"Plotting the results, we can see the performance of the model plateau (at least at this scale) with values between 0.5 to 1.0.\n\n\n\n**3. Subsampling of columns for each split in the dataset when creating each tree.**\n\n\nRather than subsample the columns once for each tree, we can subsample them at each split in the decision tree. In principle, this is the approach used in random forest.\n\nWe can set the size of the sample of columns used at each split in the colsample_bylevel parameter in the XGBoost wrapper classes for scikit-learn.\n\nAs before, we will vary the ratio from 10% to the default of 100%.","92dd9439":"# Summary\n\nIn this kernel we have discovered what is stochastic gradient boosting with XGBoost in Python.\n\nSpecifically, we learned:\n\n- About stochastic boosting and how you can subsample your training data to improve the generalization of your model\n- How to tune row subsampling with XGBoost in Python and scikit-learn.\n- How to tune column subsampling with XGBoost both per-tree and per-split.\n\n# If you like this kernel Greatly Appreciate to UPVOTE .","b18685de":"We can see that indeed 30% has the best mean performance, but we can also see that as the ratio increased, the variance in performance grows quite markedly.\n\nIt is interesting to note that the mean performance of all subsample values outperforms the mean performance without subsampling (**subsample**=1.0).\n\n**2. Subsampling of columns in the dataset when creating each tree.**\n\nWe can also create a random sample of the features (or columns) to use prior to creating each decision tree in the boosted model.\n\nIn the XGBoost wrapper for scikit-learn, this is controlled by the **colsample_bytree** parameter.\n\nThe default value is 1.0 meaning that all columns are used in each decision tree. We can evaluate values for **colsample_bytree** between 0.1 and 1.0 incrementing by 0.1.\n\nPerform K Fold cross validation using the GridSearchCV","7b8df660":"Load data","1902fb4d":"We can see that the best performance for the model was **colsample_bytree**=1.0. This suggests that subsampling columns on this problem does not add value.","2ede3d49":"Summarize results"}}