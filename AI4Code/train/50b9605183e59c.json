{"cell_type":{"256621ff":"code","3154f4e2":"code","9ea567ae":"code","2de1c346":"code","74b57097":"code","92c6ecea":"code","fcdd14ed":"code","f8dbb9df":"code","6d69c2b5":"code","e5783293":"code","3ccd3830":"code","1b8924ad":"code","ac37f564":"code","6cc5c6c4":"code","51fbff83":"code","b83fd130":"code","946ee4a1":"code","a757cca3":"code","dca03914":"code","f46fc31a":"code","1369f266":"code","735f36ce":"code","a27a8b17":"code","40693bdb":"code","02f50ea5":"code","72ea0ea4":"code","6a635aac":"code","b50bd5da":"code","e5e9c16b":"code","d942d690":"code","adb3fab7":"code","cd4935f7":"code","85fcb815":"code","67a5e5cf":"code","230a2ab2":"code","aa932a49":"code","99a83240":"code","5d11b02f":"code","b7e85922":"code","580c93fb":"code","0b924763":"code","9f345b17":"code","483041cc":"code","73344712":"code","b9f12fd1":"code","7cd27a22":"markdown","5c7183ea":"markdown","48a9723b":"markdown","38168378":"markdown","84892f05":"markdown","aba774c6":"markdown","3af59b94":"markdown","caf239d1":"markdown","c5200996":"markdown","beb31d4a":"markdown","96063044":"markdown","c3ec9c1d":"markdown","f0fdcc15":"markdown","7f3678e9":"markdown","905ca3c8":"markdown","3bcb64f8":"markdown","f6fd1ea0":"markdown","7bb5dce5":"markdown","f047b429":"markdown","180bcebe":"markdown","86c0654b":"markdown","6ec88e2a":"markdown","7a343728":"markdown","b1b2b457":"markdown","80ced1d6":"markdown","19504c6a":"markdown","7daef28a":"markdown","5f294faa":"markdown","5255ed69":"markdown","2f89c398":"markdown","c82818f4":"markdown","1af3c7b8":"markdown","ec181fef":"markdown","e2b08c87":"markdown","a3ed35aa":"markdown","cd44c74e":"markdown","171939ae":"markdown","14d1fdf4":"markdown","09dc88dc":"markdown","131f42f4":"markdown","41df0c0a":"markdown","f63c73e2":"markdown","f12f2374":"markdown","b916234b":"markdown","14767bd5":"markdown"},"source":{"256621ff":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import PolynomialFeatures\n\nsns.set(style='white', context='notebook', palette='dark')\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3154f4e2":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","9ea567ae":"print(train.dtypes)","2de1c346":"train.describe()","74b57097":"print(train.isnull().sum()\/train.shape[0])","92c6ecea":"print(test.isnull().sum()\/test.shape[0])","fcdd14ed":"def barplots(dfMean, dfCount, title1, title2):\n    fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n    sns.barplot(x=dfMean.index, y=dfMean['Survived'], alpha=.5,ax=axis1).set_title(title1)\n    sns.barplot(x=dfCount.index, y=dfCount['Survived'], alpha=.5,ax=axis2).set_title(title2)","f8dbb9df":"df1=train.groupby(['Pclass'])[['Survived']].mean()\ndf2=train.groupby(['Pclass'])[['Survived']].count()\nbarplots(df1, df2, \"Surival Rate\", \"Count\")","6d69c2b5":"df1=train.groupby(['SibSp'])[['Survived']].mean()\ndf2=train.groupby(['SibSp'])[['Survived']].count()\nbarplots(df1, df2, \"SibSp Survival Rate\", \"SibSp Count\")","e5783293":"df1=train.groupby(['Parch'])[['Survived']].mean()\ndf2=train.groupby(['Parch'])[['Survived']].count()\nbarplots(df1, df2, \"Parch Survival Rate\", \"Parch Count\")","3ccd3830":"def family_size(data):\n    data['FamilySize'] = data['SibSp'] + data['Parch'] \n    return data\n\ntrain = family_size(train)\ntest = family_size(test)\n\ndf1=train.groupby(['FamilySize'])[['Survived']].mean()\ndf2=train.groupby(['FamilySize'])[['Survived']].count()\nbarplots(df1, df2, \"Surival Rate\", \"Count\")","1b8924ad":"def Bin_family(data):\n    data['FamilyBin'] = data['FamilySize'].map({0:0,1:1,2:1,3:1,4:2,5:2,6:2,7:2,8:2,9:2,10:2,11:2}).astype(int)\n    return data\n\ntrain=Bin_family(train)\ntest=Bin_family(test)\n\ndf1=train.groupby(['FamilyBin'])[['Survived']].mean()\ndf2=train.groupby(['FamilyBin'])[['Survived']].count()\nbarplots(df1, df2, \"Surival Rate\", \"Count\")","ac37f564":"print(train.groupby(['FamilyBin', 'Pclass'])[['Survived']].mean())\nprint(train.groupby(['FamilyBin', 'Pclass'])[['Survived']].count())","6cc5c6c4":"df1=train.groupby(['Sex'])[['Survived']].mean()\ndf2=train.groupby(['Embarked'])[['Survived']].mean()\nbarplots(df1, df2, \"Sex Survival Rate\", \"Embarked Survival Rate\")","51fbff83":"print(train.groupby(['Embarked', 'Pclass'])[['Survived']].count())\nprint(train.groupby(['Embarked', 'Pclass'])[['Survived']].mean())\nprint(train.groupby(['Embarked'])[['Pclass']].count())","b83fd130":"def titleExtract(data):\n    data['Title'] = data.Name.str.extract('([A-Za-z]+)\\.', expand=False)\n    data['Title'] = data['Title'].replace(['Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer'], 'Mr')\n    data['Title'] = data['Title'].replace(['Ms', 'Miss'], 'Miss')\n    data['Title'] = data['Title'].replace(['Mlle', 'Mme','Lady', 'Countess', 'Dona'], 'Mrs')\n    return data\n\n\ntrain=titleExtract(train)\ntest=titleExtract(test)","946ee4a1":"df1=train.groupby(['Title'])[['Survived']].mean()\ndf2=train.groupby(['Title'])[['Survived']].count()\nbarplots(df1, df2, \"Surival Rate\", \"Count\")\n\nprint(train.groupby(['Title'])[['Age']].mean())","a757cca3":"print(train.groupby(['Title', 'Sex'])[['Survived']].mean())\nprint(train.groupby(['Title', 'Sex'])[['Survived']].count())","dca03914":"print(train.groupby(['Title', 'Pclass'])[['Survived']].mean())\nprint(train.groupby(['Title', 'Pclass'])[['Survived']].count())","f46fc31a":"def mapTitle(data):\n    title_map = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4}\n    data['Title'] = data['Title'].map(title_map).astype(int)\n    return data\n   \n\ntrain=mapTitle(train)\ntest=mapTitle(test)","1369f266":"def fill_fare_nulls(data):\n    data.loc[(data.Fare.isnull())&(data.Pclass==1), 'Fare']=60.3\n    data.loc[(data.Fare.isnull())&(data.Pclass==2), 'Fare']=14.25\n    data.loc[(data.Fare.isnull())&(data.Pclass==3), 'Fare']=8.05\n    return data\n\ntest=fill_fare_nulls(test)\n","735f36ce":"train.groupby(['Pclass'])[['Fare']].median()","a27a8b17":"train['FareBand'] = pd.cut(train['Fare'], (-1, 8.05, 14.25, 60.2875, 1000), labels=['0','1','2','3'])\ntest['FareBand'] = pd.cut(test['Fare'], (-1, 8.05, 14.25, 60.2875, 1000), labels=['0','1','2','3']) \ntrain['FareBand'].astype(int)\ntest['FareBand'].astype(int)\n\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n\nd=train.groupby(['FareBand'])[['Survived']].mean()\nsns.barplot(x=d.index, y=d['Survived'], alpha=.5, ax=axis1).set_title(\"Survival Rate\")\n\nd=train.groupby(['FareBand'])[['Survived']].count()\nsns.barplot(x=d.index, y=d['Survived'], alpha=.5, ax=axis2).set_title(\"Count\")","40693bdb":"print(train.groupby('Pclass')['Fare'].median())\nprint(test.groupby('Pclass')['Fare'].median())","02f50ea5":"def splitClass_Train(data):\n    data.loc[(data.Fare>60.3)&(data.Pclass==1), 'PassengerCat']=0\n    data.loc[(data.Fare<=60.3)&(data.Pclass==1), 'PassengerCat']=1\n    \n    data.loc[(data.Fare>14.3)&(data.Pclass==2), 'PassengerCat']=2\n    data.loc[(data.Fare<=14.3)&(data.Pclass==2), 'PassengerCat']=3\n              \n    data.loc[(data.Fare>8.1)&(data.Pclass==3), 'PassengerCat']=4\n    data.loc[(data.Fare<=8.1)&(data.Pclass==3), 'PassengerCat']=5\n    data['PassengerCat']=data['PassengerCat'].astype(int)\n    return data\n\n\ntrain=splitClass_Train(train)\n\ndef splitClass_Test(data):\n    data.loc[(data.Fare>60.00)&(data.Pclass==1), 'PassengerCat']=0\n    data.loc[(data.Fare<=60.00)&(data.Pclass==1), 'PassengerCat']=1\n    \n    data.loc[(data.Fare>15.8)&(data.Pclass==2), 'PassengerCat']=2\n    data.loc[(data.Fare<=15.8)&(data.Pclass==2), 'PassengerCat']=3\n              \n    data.loc[(data.Fare>7.9)&(data.Pclass==3), 'PassengerCat']=4\n    data.loc[(data.Fare<=7.9)&(data.Pclass==3), 'PassengerCat']=5\n    data['PassengerCat']=data['PassengerCat'].astype(int)\n    return data\n\n\ntest=splitClass_Test(test)\n","72ea0ea4":"df1=train.groupby(['PassengerCat'])[['Survived']].mean()\ndf2=train.groupby(['PassengerCat'])[['Survived']].count()\nbarplots(df1, df2, \"Surival Rate\", \"Count\")","6a635aac":"avgAges=train.groupby(['Title', 'Pclass'], as_index=False)['Age'].median()\nprint(avgAges)\navgAges = avgAges['Age']\n","b50bd5da":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n#Before filling in NA values\ntrain['Age'].hist(bins=20, ax=axis1).set_title('Before Imputing') \ndef fillAgeNulls(data, avgAges):\n    data.loc[(data.Age.isnull())&(data.Title==1)&(data.Pclass==1), 'Age']=avgAges[0]\n    data.loc[(data.Age.isnull())&(data.Title==1)&(data.Pclass==2), 'Age']=avgAges[1]\n    data.loc[(data.Age.isnull())&(data.Title==1)&(data.Pclass==3), 'Age']=avgAges[2]\n    \n    data.loc[(data.Age.isnull())&(data.Title==2)&(data.Pclass==1), 'Age']=avgAges[3]\n    data.loc[(data.Age.isnull())&(data.Title==2)&(data.Pclass==2), 'Age']=avgAges[4]\n    data.loc[(data.Age.isnull())&(data.Title==2)&(data.Pclass==3), 'Age']=avgAges[5]\n    \n    data.loc[(data.Age.isnull())&(data.Title==3)&(data.Pclass==1), 'Age']=avgAges[6]\n    data.loc[(data.Age.isnull())&(data.Title==3)&(data.Pclass==2), 'Age']=avgAges[7]\n    data.loc[(data.Age.isnull())&(data.Title==3)&(data.Pclass==3), 'Age']=avgAges[8]\n    \n    data.loc[(data.Age.isnull())&(data.Title==4), 'Age']=avgAges[9]\n    \n    \n    return data\n\n\ntrain=fillAgeNulls(train, avgAges)\ntest=fillAgeNulls(test, avgAges)\n\ntrain['Age'].hist(bins=20, ax=axis2).set_title('After Imputing')","e5e9c16b":"\ntrain['AgeBand']=pd.cut(train['Age'], (0, 6, 60, 80), labels=['0','1','2'])\ntest['AgeBand']=pd.cut(test['Age'], (0, 6, 60, 80), labels=['0','1','2'])\ntrain['AgeBand'].astype(int);\ntest['AgeBand'].astype(int);\n\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n\nd=train.groupby(['AgeBand'])[['Survived']].mean()\nsns.barplot(x=d.index, y=d['Survived'], alpha=.5, ax=axis1).set_title('Surival Rate')\n\nd=train.groupby(['AgeBand'])[['Survived']].count()\nsns.barplot(x=d.index, y=d['Survived'], alpha=.5,ax=axis2).set_title('Count')","d942d690":"print(train.groupby(['AgeBand', 'Sex'])[['Survived']].mean())\nprint(train.groupby(['AgeBand', 'Sex'])[['Survived']].count())","adb3fab7":"train=train.drop(columns=['Name', 'Ticket', 'PassengerId', 'Cabin', 'SibSp','Parch', 'Embarked', 'FamilyBin','FareBand', 'Pclass', 'Sex', 'AgeBand'])\ntest=test.drop(columns=['Name', 'Ticket', 'PassengerId', 'Cabin','SibSp', 'Parch', 'Embarked', 'FamilyBin','FareBand', 'Pclass', 'Sex', 'AgeBand'])","cd4935f7":"train.head(10)","85fcb815":"sns.heatmap(train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix","67a5e5cf":"print(train.dtypes)","230a2ab2":"train=pd.get_dummies(train,columns=['Title', 'PassengerCat'])\ntest=pd.get_dummies(test,columns=['Title', 'PassengerCat'])","aa932a49":"print(train.isnull().sum())\nprint(\"\\nTest Set:\")\nprint(test.isnull().sum())","99a83240":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.ensemble import GradientBoostingClassifier","5d11b02f":"train_X=train.drop('Survived', axis=1)\ntrain_Y=train['Survived'].astype(int)\ntest_X=test\nKF=KFold(n_splits=10, random_state=1)\n\nmodels=[]\nmodelScores=[]\nmodelSTD=[]","b7e85922":"LR_model = LogisticRegression(solver = 'lbfgs', max_iter = 3000)\n\nCV=cross_val_score(LR_model,train_X,train_Y,cv=KF, scoring=\"accuracy\")\nCV.mean()\nLR_model.fit(train_X, train_Y)\nmodels.append('LogisticRegression')\nmodelScores.append(round(CV.mean(),3))\nmodelSTD.append(round(CV.std(),3))","580c93fb":"from sklearn.model_selection import GridSearchCV\n\nRFC = RandomForestClassifier()\n\n\nRF_grid = {\"max_depth\": [None],\n              \"max_features\": [4, 6, 8],\n              \"min_samples_split\": [3, 5, 7],\n              \"min_samples_leaf\": [5, 10, 15],\n              \"bootstrap\": [True],\n              \"n_estimators\" :[500],\n              \"criterion\": [\"gini\"]}\n\ngsRFC = GridSearchCV(RFC,param_grid = RF_grid, cv=10, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsRFC.fit(train_X,train_Y)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\nprint(gsRFC.best_score_)\nprint(gsRFC.best_params_)","0b924763":"#Random Forest\nRF_model = RandomForestClassifier(bootstrap= True,\n criterion = 'gini',\n max_depth = None,\n max_features=8,\n min_samples_leaf = 5,\n min_samples_split = 7,\n n_estimators = 500, random_state=1)\n\nRF_model.fit(train_X, train_Y)\n\n\nCV=cross_val_score(RF_model,train_X,train_Y,cv=KF, scoring=\"accuracy\")\nmodels.append('RandomForest')\n#modelScores.append(round(RF_model.oob_score_,3))\nmodelScores.append(round(CV.mean(),3))\nmodelSTD.append('NA')\n\nfeatureImportance = pd.concat((pd.DataFrame(train_X.columns, columns = ['Feature']), \n           pd.DataFrame(RF_model.feature_importances_, columns = ['Importance'])), \n          axis = 1).sort_values(by='Importance', ascending = False)[:20]\nplt.subplots(figsize=(20,8))\nsns.barplot(x=featureImportance['Feature'], y=featureImportance['Importance'], alpha=.5).set_title('Feature Importance')","9f345b17":"from sklearn.metrics import confusion_matrix\nimport itertools\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\n    \nplt.figure()\ncnf_matrix = confusion_matrix(train_Y, RF_model.predict(train_X))\nnp.set_printoptions(precision=2)\nclass_names = ['0', '1']\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix for RF')","483041cc":"plt.figure()\ncnf_matrix = confusion_matrix(train_Y, LR_model.predict(train_X))\nnp.set_printoptions(precision=2)\nclass_names = ['0', '1']\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix for LR')","73344712":"ModelComparison=pd.DataFrame({'CV Score':modelScores, 'Std':modelSTD}, index=models)\nModelComparison","b9f12fd1":"\ntest_ID = pd.read_csv('..\/input\/test.csv')\ntest_ID = test_ID['PassengerId']\n\nSurvival_predictions = RF_model.predict(test)\nID=np.arange(892,1310,1)\n\nsubmission=pd.DataFrame({\n        \"PassengerId\": ID,\n        \"Survived\": Survival_predictions\n    })\nsubmission.to_csv('submission.csv', index=False)","7cd27a22":"No missing values so we are ready to model.","5c7183ea":"Here we will get a view of the dataset that we will use in our predictions","48a9723b":"To make it easier to plot I'll bin the fare based on the median values of fare at each Pclass.","38168378":"We will use the paramters that gave the best cross validated result:\n","84892f05":"Interestingly, if you were a female your survival rate did not appear to depend on whether you were a child, middle aged, or old (note there are only 3 observations for female seniors). Additionally, if you were a male it only mattered if you were young - old and middle aged males have a very similar survival rate. This information however is already encoded.","aba774c6":"**Modeling**\n\nA couple things to note before starting this section.\n\n1. It's important to cross validate your results and be weary of overfitting. The prediction accuracy that your models have on your training set is usually not a very good indicator of how well it will perform on unseen data. Cross validation scores are an unbiased estimate of your models prediction accuracy that will aid in model selection.\n\n2. Most models have a wide variety of parameters that are given default values when you instantiate them. Generally want to adjust these parameters through either a grid search or a random search. A grid search exhaustively tries every combination of parameters that you add to the grid while a random search tests a random subset. I believe that random searches are generally better when you are not sure of roughly \"where\" the best parameters are. Also keep in mind that this parameter selectino can take quite a bit of time - on my laptop each gridsearch takes ~3 minutes while on my desktop with a i7 7700k it takes ~45 seconds.\n","3af59b94":"**Feature Selection**\n\nFirst we will drop the features that we used to create other features and the features that were not very useful.\n\nWe used Name, SibSp, Parch, Fare, Age, FareBand, PClass in our feature engineering so we will drop them here.\n\nTicket, PassengerId, Cabin, Embarked were either not explored or deemed to be not very useful.","caf239d1":"First, this is my understanding of whether a feature is continuous or discrete (e.i. categorical)\n1. If something like the \"average\" of a feature has some meaning, it is usually a continuous variable. For example, we can interpret the mean age of passengers on the Titanic, therefore it is a continuous feature.\n2. If the \"average\" doesn't have a reasonable interpretation, it is usually a discrete variable. For example, can we really interpret the mean passenger class? Obviously, passenger class 2.3 doesn't have much meaning, you clearly cant be in Pclass 2.3. A more reasonable way to understand this feature would be to find the proportion of passengers in each Pclass. Therefore, it is a categorical feature.\n\nWe can see the following:\n\n* Surived is a categorical response variable\n* Pclass and Embarked are categorical features with 3 levels. Pclass however is an ordinal, which means that there is an \"order\" to the feature. Obviously, Pclass 1 > Pclass 2 > Pclass 3. This is not true for Embarked since we can't infer any meaning from the ports \"Q\", \"C\", and \"S\".\n* Age is a continuous variable. The average age was ~30 years old.\n* SibSp and Parch are categorical\n* Fare is a continuous feature\n* Name, ticket, and cabin are qualitative features","c5200996":"We can see that those in Pclass=1 are generally older than those in Pclass 2 or 3. This matches with intuition since richer people are typically older. There isn't much variation between passenger classes for those with the \"Maser (4)\" title, so we wont disseminate based on passenger class for those with title 4. Next we fill in the missing age values using the information above. Admittedly, it's a little (very) ugly. I originally implemented with with loops but I found it to be pretty hard to interpret and perhaps equally as ugly.","beb31d4a":"**Definining the problem**\n\nOur goal is predict the surival of passengers on the Titanic. This is a prediction and binary classification problem, so we will need to use classification models once we have completed our analysis.","96063044":"From this we can see that there is some variation in survival rates between the different Titles. Those with Miss and Mrs title's appeared to have much higher surivival rates than other titles.  Additionally we can see that those with the \"Master\" surname are children and those with the \"Miss\" surname are typically younger than than those with \"Mr\" or \"Mrs\".\n\nIn preparation for using this feature in the future, we can assign each category a number.","c3ec9c1d":"We can see that my concerns were mostly unfounded, there is quite a large variation in survival rates between family sizes, holding Pclass constant. We can see that 53% of passengers in first class who were alone survived, compared to the 73% (!) survival rate of first class passengers in small families. It also appears that if you were in a large third class family you were almost guarunteed to perish. \n\nNow lets see if these differences are perhaps due to families having women in children in them, which would increase the average survival rate of passengers in that family.","f0fdcc15":"Next get the dummy variables for our non-ordinal categorical features. ","7f3678e9":"**Exploratory Analysis**\n\nIn this section I'll try to answer the following questions:\n\n1. How many features are there and how are they encoded? \n2. Are there missing values? Can we reasonably infer these values (e.i. impute them)?\n3. What features are correlated with survival? \n4. What features can we combine? ","905ca3c8":"As expected, the survival rate is ordered by class and most passengers were in third class. Those in higher passengers have a higher surival rate than those in lower classes. Somewhat suprisingly, there are more passengers in first class than in second class.","3bcb64f8":"**Ticket and other features**","f6fd1ea0":"Title is quite important primarly because it indicated whether a passenger was male or female.  ","7bb5dce5":"The medians are a little different here, although not by much.","f047b429":"I haven't explored cabin\/ticket in depth, but from what I have seen they are not particularly useful. I'm also worried about introducing more noise into the model. Perhaps in the future I will explore them in depth but for now I will neglect them.","180bcebe":"**Fare**\n\nThere is one missing value for fare in the test set. Instead of looking for it I'll just make an educated guess on its value based on the passenger's Pclass.","86c0654b":"So it's quite clear that those who paid more had a higher survival rate - that's not too suprising or interesting at this point. What is interesting is that perhaps those who paid above median price for their ticket may have had a higher survival rate. That is, perhaps those in the \"upper\" first class did better than those in the \"lower\" first class.\n\n**Revisiting Pclass**\n\nI think a good way to split passenger class is based on the median fare of each Pclass. We are in a sense splitting passengers based on whether they are in the \"upper\" or \"lower\" part of their Pclass.","6ec88e2a":"First start by opening up the datasets","7a343728":"Once again this is the expected result, females had a much higher survival rate than males since women and children had priority when getting off the ship. \n\nInterestingly, those who embarked from port \"C\" tended to have a much higher survival rate than those who embarked from port \"Q\" or \"S\". To me, this seems fairly unintuitive Clearly, this needs to be further investigated.","b1b2b457":"Curiously, we can see that those in the bottom half of first class were typically worse off than those in the upper half of second class. I don't really have an explanation as to why this is the case, perhaps it's just chance or a higher number of female passengers in the \"upper\" second passenger class. Overall I think this might be a better way of utilizing the fare and Pclass feature.","80ced1d6":"def mapSex(data):\n    sex_map = {\"female\": 0, \"male\": 1,}\n    data['Sex'] = data['Sex'].map(sex_map).astype(int)\n    return data\n\n\ntrain = mapSex(train)\ntest = mapSex(test)","19504c6a":"Now we can plot this since we have extracted a categorical feature from a qualitative one.","7daef28a":"**Name**\n\nBefore we can visualize this, we will have to do some feature engineering. \n\nWe'll start by first extracting the surname from the Name feature. ","5f294faa":"We can see that port C had a larger proportion of first class passengers than port Q or port S. This seems to explain why passengers who departed from port C had a higher survival rate. As a result, I don't think this predictor is very valuable - the differences seem to be explained by other factors and it doesn't match intuition. ","5255ed69":"**Trees**","2f89c398":"Now, I think this warrents some more investigation. I'm curious to see if this difference is due to smaller families having a disproportionate number of first class passengers.","c82818f4":"Submitting this results in a score of ~80. Currently, I'm working on implenting a gradient boosted tree which will hopefully produce better results. \n\nPlease leave any suggestions you have to help me improve this kernel!","1af3c7b8":"**Passenger Class**","ec181fef":"**Gender and Embarked**","e2b08c87":"**Parch and SibSp**\n","a3ed35aa":"We will do a grid search to find the best parameters. This code will take 1-3 minutes to run depending on hardware. ","cd44c74e":"It looks like most passengers were alone, and passengers who were alone typically had lower survival rates. I think the surival rate plots above are a little misleading, there are not many observations for for higher values of Parch and SibSp.\n\nFrom here it seems fairly natural to combine these features and create a new one for the family size. This will also help with increasing the number of observations for larger families.","171939ae":"Here we can see a summary of each model showing their cross validated score and the standard deviation. \n\nWe can see that SVC and the tree models performed quite well. I'm most confident with the random forest because its score is based off of an out of bag estimate. Even though there is virtually no difference in score between both decision trees, I'll assume the pruned one is a little better because we have remove reduced its variance.","14d1fdf4":"Similar results to the training set, but there is also a missing fare value. We'll deal with this later.","09dc88dc":"I would like to hear some feedback about this - I'm quite concerned with how the distribution has changed after imputing age. \n\nTo reduce the amount of noise we will next bin age. This also helps group some \"special\" age categories together - we expect children and especially young to have a higher survival rate and seniors to have a lower survival rate. \n\nThe cuts here are somewhat arbitrary and I'm not a huge fan of this implementation - I've essentially grouped being into either a young children, kids, adult, and senior category. \n\nI'd be interested in hearing feedback about this.","131f42f4":"I won't go into too much detail about the individual models here. I think there are some pretty good resources on details about them online.\n\n\n\n**Logistic Regression**","41df0c0a":"We can see that we are Age, Cabin, and Embarked all have missing values. There are roughly 900 observations in the training set and we can see that the Cabin feature is missing ~80% of its values and age is missing ~20% of its values. This corresponds to ~700 missing cabin observations and ~200 missing age features. We'll return to these later, but for now I will say that filling in the missing values for embarked is going to be quite simple and I don't believe that I will be able to extract much useful information from the cabin feature. The age feature seems the most interesting to me since I believe that it is reasonable to infer that some age groups will fare better than others. We've all heard of the phrase \"Women and children first\", so putting in some work on the age feature will probably end up paying off.\n\nNext, we will begin to explore our categorical and continuous features graphically.","f63c73e2":"It's a little more clear now that if you were alone or in a family greater than 4 your chances of survival were lower. I still have some concerns with there not being many observations for large families. In solve this, I'll bin the families into alone, small, and large.","f12f2374":"Random Forest:","b916234b":"**Age**\n\nI saved age because the goal is to use the title and passenger class features to predict age. ","14767bd5":"Before we start modeling, make sure we have no missing values on our training and test set."}}