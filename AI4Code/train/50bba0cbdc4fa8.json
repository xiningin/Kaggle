{"cell_type":{"03d53eb5":"code","b80541a7":"code","5d175790":"code","580f017e":"code","ba7ce282":"code","8dec18aa":"code","fb29d3a4":"code","da781d2f":"code","6459acdd":"code","52fd7731":"code","706a4f5c":"code","a7872c4b":"code","e68eb2e6":"code","753e0d0c":"code","be65e556":"code","848ee4d2":"code","cc646bb3":"code","4f694f27":"code","ba542c3d":"code","efe22138":"code","0fb1f5a4":"code","f7cd284c":"markdown","d5a7e09f":"markdown","6778ad99":"markdown","240f9827":"markdown","50ecb600":"markdown","7e1537b5":"markdown","10f4837a":"markdown","2f4ca6c4":"markdown","46dd64a4":"markdown","04915cc5":"markdown","c3965cb1":"markdown","62e6ab30":"markdown","18f5798f":"markdown","e3531969":"markdown","7dfd79be":"markdown","94b0cae5":"markdown","2a43a449":"markdown","c550a01a":"markdown"},"source":{"03d53eb5":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.linalg import eigh\nfrom sklearn import decomposition","b80541a7":"train_df = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")","5d175790":"train_df.head()","580f017e":"label = train_df.label\ntrain = train_df.drop('label', axis=1)","ba7ce282":"print(label.shape)\nprint(train.shape)","8dec18aa":"label[0]","fb29d3a4":"train.head()","da781d2f":"plt.figure(figsize=(2,2))\n# reshape d from 1d to 2d pixel array for given idx ( prefer 28 X 28)\ngrid_data = train.loc[0].values.reshape(28,28)\n#plot above grid image with cmap as gray and interpoltion as none\nplt.imshow(grid_data,interpolation='none',cmap='gray')\n\n#display plot\nplt.show()","6459acdd":"standardized_data = StandardScaler().fit_transform(train)\nstandardized_data.shape","52fd7731":"covar_matrix = np.matmul(standardized_data.T,standardized_data)\ncovar_matrix","706a4f5c":"# since we need to project (42000 X 784) to (42000 X 2). Therefore we need to select top 2 eigen values\nvalues, vectors = eigh(covar_matrix,eigvals=(782,783))\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\nprint(vectors)\n\n# converting the eigen vectors into (2d) shape \nvectors = vectors.T\nprint(\"Updated shape of eigen vectors = \",vectors.shape)\nprint(vectors)\n# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector","a7872c4b":"new_coord =  np.matmul(vectors, standardized_data.T)\nprint(new_coord)\nprint(new_coord.shape)","e68eb2e6":"pca_data = pd.DataFrame({\"1st_principal\" : new_coord[1]\n                         ,\"2nd_principal\" : new_coord[0], \"label\" : label})","753e0d0c":"pca_data","be65e556":"sns.FacetGrid(pca_data, hue='label', height=8).map(plt.scatter, \"1st_principal\", \"2nd_principal\", 'label').add_legend()\nplt.show()","848ee4d2":"pca = decomposition.PCA()\npca.n_components = 2\npca_data_sci = pca.fit_transform(standardized_data)\npca_data_sci.shape","cc646bb3":"pca_data_sci_new = pd.DataFrame({\"1st_principal\" : pca_data_sci.T[0]\n                         , \"2nd_principal\" : pca_data_sci.T[1], \"label\" : label})","4f694f27":"pca_data_sci_new","ba542c3d":"sns.FacetGrid(pca_data_sci_new, hue='label',height=8).map(plt.scatter, \"1st_principal\", \"2nd_principal\", 'label').add_legend()\nplt.show()","efe22138":"pca.n_components = 784\n\npca_data = pca.fit_transform(standardized_data)\n\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_)\n\n#cumulative sum of the percentage_var_explained\ncumulative_explained_variance = np.cumsum(percentage_var_explained)","0fb1f5a4":"plt.figure(figsize=(6,4))\nplt.plot(cumulative_explained_variance,linewidth=3)\nplt.grid()\n\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","f7cd284c":"## what is dimenionality reduction ?","d5a7e09f":"### 2. Eigen Vectors and Eigen Values \n   \n   The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude","6778ad99":"- High Dimensional data with too many features is hard and longer to process\n- Most of the time many feature are co-related eg - humidity and rainfall , therefore processing them independently is redundant\n- Many Machine Learning simply breaks down when working with high dimentional data. This Phenomenon is commonly refered to as **Curse of dimensionality**","240f9827":"### 1. Co-variance Matrix\n\nCovariance Matrix basically describe the variance of the data","50ecb600":"## why do we need to dimensionality reduction ?","7e1537b5":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.linalg import eigh\nfrom sklearn import decomposition\n\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls","10f4837a":"# PCA Implementation using Scikit-Learn","2f4ca6c4":"Equation : \n\n$\\mathbf{S} = \\mathbf{A}^\\intercal  \\mathbf{A}$\n","46dd64a4":"### Projecting onto 2D Plane","04915cc5":"# Data Preprocessing ","c3965cb1":"# PCA Implementation ","62e6ab30":"Basic Equation is :               \n\n$\\mathbf{S}\\mu = \\lambda \\mu $\n\nwhere $\\lambda$ is eigen value, $\\mathbf{S}$ is Co-variance matrix and $\\mu $ is eigen vectors","18f5798f":"the above grpah shows that by choosing 200 n_components or principals we can get a variance of around 90% \nthus instead of working with all 784 dimensions we can work with around 200 dimensions without any major loss in information","e3531969":"# PCA","7dfd79be":"to implement pca we need two things\n1. Co-variance Matrix\n2. Eigen Vectors and Eigen Values\n3. Projection onto 2D Plane","94b0cae5":"# Variance Explained by PCA","2a43a449":"PCA or Pricipal component Analysis is a technique used for dimensionality reduction , It projects each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. It does so by creating new uncorrelated variables that successively maximize variance. ","c550a01a":"Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data"}}