{"cell_type":{"8f8b4140":"code","4dd4f25c":"code","bab0462a":"code","98291bc6":"code","cdf65be8":"code","c9e966e6":"code","73376f5f":"code","4ded76b5":"code","54c884c6":"code","75b3fff3":"code","6a77812c":"code","03191eb0":"code","c1cdb91a":"code","50e48117":"code","30eb2c4d":"code","40db3a9f":"code","bf3c4250":"code","82a54a01":"code","faf314ca":"code","5b06d5b2":"code","84b0c5f1":"code","9e2d93b7":"code","612e1574":"code","b7410fbb":"code","6a69e68a":"code","f9b54513":"code","c87883dc":"code","d95f6fec":"code","0ff9c003":"code","ef7e5d0c":"code","44f7bdf3":"code","056efb2f":"code","f95c2e59":"code","cc60a1a5":"code","564e1c24":"code","478913ee":"code","2936031f":"code","b121fa84":"code","a1f6e0b1":"code","e3f8ca24":"code","d5edad8c":"code","d739c07a":"code","77152ee9":"code","2be39509":"code","d5cd8f21":"code","fe0f0dd8":"code","22bc6303":"code","9de5825e":"code","7859f0db":"code","34762e1d":"code","3444f6bf":"code","8a604d67":"markdown","8a4603eb":"markdown","fbbbe5ef":"markdown","da3ae69e":"markdown","a89c2409":"markdown","fd5fe8b5":"markdown","8f918ff6":"markdown","8b3a0bfe":"markdown","70b31cf7":"markdown","53c2500e":"markdown","fbaad01d":"markdown","4f053dec":"markdown","fa6a98ff":"markdown","528ce6ab":"markdown","c7488b1a":"markdown","a83e7b57":"markdown","3b65d321":"markdown","3277b983":"markdown","6b553dc2":"markdown","3e6c66e6":"markdown","01f8162c":"markdown","210345a2":"markdown","2c312b91":"markdown","c9c6f7b4":"markdown","5cc04397":"markdown","1f8c9032":"markdown","374357c1":"markdown","992ae28a":"markdown","25e171e2":"markdown","adee009b":"markdown","d89be4ff":"markdown","895cf32a":"markdown","33c5082f":"markdown","fc2beb1a":"markdown","c7f342ad":"markdown","6550f0f2":"markdown","16aabcb4":"markdown","fd8ec833":"markdown","ebdb2337":"markdown"},"source":{"8f8b4140":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom imblearn.over_sampling import ADASYN\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4dd4f25c":"credit_card=pd.read_csv(\"..\/input\/creditcard.csv\")\ncredit_card.head()\n#Take a look of our dataset and variables to get brief understanding of the data we'll be working with","bab0462a":"#Check the size of our dataset\ncredit_card.shape","98291bc6":"credit_card[\"Class\"].value_counts()","cdf65be8":"#Check if there is missing value\nnull_data=pd.isnull(credit_card).sum()\nprint(null_data)","c9e966e6":"f, (fraud, normal) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nbins = 50\n\nfraud.hist(credit_card.Time[credit_card.Class == 1], bins = bins)\nfraud.set_title('Fraud')\n\nnormal.hist(credit_card.Time[credit_card.Class == 0], bins = bins)\nnormal.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Number of Transactions')\nplt.show()","73376f5f":"f, (fraud, normal) = plt.subplots(2, 1, sharex=False, figsize=(12,7))\n\nbins = 30\n\nfraud.hist(credit_card.Amount[credit_card.Class == 1], bins = bins)\nfraud.set_title('Fraud')\n\nnormal.hist(credit_card.Amount[credit_card.Class == 0], bins = bins)\nnormal.set_title('Normal')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.yscale('log')\nplt.show()","4ded76b5":"f, (fraud, normal) = plt.subplots(2, 1, sharex=False, figsize=(12,8))\n\nfraud.scatter(credit_card.Time[credit_card.Class == 1], credit_card.Amount[credit_card.Class == 1])\nfraud.set_title('Fraud')\n\nnormal.scatter(credit_card.Time[credit_card.Class == 0], credit_card.Amount[credit_card.Class == 0])\nnormal.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","54c884c6":"v_variable=credit_card.iloc[:,1:29].columns","75b3fff3":"plt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, cn in enumerate(credit_card[v_variable]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(credit_card[cn][credit_card.Class == 1], bins=50)\n    sns.distplot(credit_card[cn][credit_card.Class == 0], bins=50)\n    ax.set_xlabel('')\n    plt.legend(credit_card[\"Class\"])\n    ax.set_title('histogram of feature: ' + str(cn))\n    \nplt.show()","6a77812c":"#select the values of \"V1\" to \"Amount\"\ncreditcard_v=credit_card.iloc[:,1:30].values","03191eb0":"print(creditcard_v)","c1cdb91a":"creditcard_v.shape","50e48117":"#Set 30 variables as \"X\" and \"Class\" as y\nX=creditcard_v\ny=credit_card[\"Class\"].values\n\nsss=StratifiedShuffleSplit(n_splits=5, test_size=0.3,random_state=0)\nsss.get_n_splits(X,y)","30eb2c4d":"#So this is the cross-validator that we are using\nprint(sss)","40db3a9f":"#Split train\/test sets of X and y\nfor train_index, test_index in sss.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train,X_test=X[train_index], X[test_index]\n    y_train,y_test=y[train_index], y[test_index]","bf3c4250":"#Let's see the number of sets in each class of training and testing datasets\nprint(pd.Series(y_train).value_counts())\nprint(pd.Series(y_test).value_counts())","82a54a01":"pca=PCA(n_components=2)\ndata_2d=pd.DataFrame(pca.fit_transform(X_train))","faf314ca":"data_2d","5b06d5b2":"data_2d=pd.concat([data_2d, pd.DataFrame(y_train)], axis=1)\ndata_2d.columns=[\"x\",\"y\",\"fraud\"]","84b0c5f1":"data_2d","9e2d93b7":"#visualise the 2D training data\nsns.set_style(\"darkgrid\")\nsns.lmplot(x=\"x\", y=\"y\", data=data_2d, fit_reg=False, hue=\"fraud\",height=5, aspect=2)\nplt.title(\"Scatter Plot of imbalanced Training Data\")","612e1574":"ada= ADASYN()\nx_resample,y_resample=ada.fit_sample(X_train,y_train)","b7410fbb":"#concat oversampled \"x\" and \"y\" into one DataFrame\ndata_oversampled=pd.concat([pd.DataFrame(x_resample),pd.DataFrame(y_resample)],axis=1)\n#replace column labels using the labels of original datasets\ndata_oversampled.columns=credit_card.columns[1:31]","6a69e68a":"#while the label of column 30 is \"Class\",we can rename it to \"fraud\"\ndata_oversampled.rename(columns={\"Class\":\"fraud\"},inplace=True)","f9b54513":"data_oversampled[\"fraud\"].value_counts()","c87883dc":"#reduce dimensionality to 2 dimensions\noversampled_train2d=pd.DataFrame(pca.fit_transform(data_oversampled.iloc[:,0:29]))\noversampled_train2d=pd.concat([oversampled_train2d,data_oversampled[\"fraud\"]],axis=1)\noversampled_train2d.columns= [\"x\",\"y\",\"fraud\"]","d95f6fec":"#visualise data\nsns.set_style(\"darkgrid\")\nsns.lmplot(x=\"x\", y=\"y\", data=oversampled_train2d, fit_reg=False, hue=\"fraud\",height=5, aspect=2)\nplt.title(\"Scatter Plot of balanced Training Data\")","0ff9c003":"y_resample","ef7e5d0c":"#use one-hot encoding to reformat\nY_resample=keras.utils.to_categorical(y_resample,num_classes=None)","44f7bdf3":"print(Y_resample)","056efb2f":"ANN=keras.Sequential()\n\nANN.add(keras.layers.Dense(4, input_shape=(29,),activation=\"relu\"))\nANN.add(keras.layers.Dense(2, activation=\"softmax\"))","f95c2e59":"ANN.compile(keras.optimizers.Adam(lr=0.04), \"categorical_crossentropy\",metrics=['accuracy'])","cc60a1a5":"ANN.summary()","564e1c24":"ANN.fit(x_resample,Y_resample, epochs=30, verbose=0)","478913ee":"#Evaluate the model based on accuracy\nY_test=keras.utils.to_categorical(y_test,num_classes=None)\ncontrol_accuracy=ANN.evaluate(X_test,Y_test)[1]\nprint(\"Accuracy: {}\".format(control_accuracy))\nprint(\"\\n\")\n\n#Use sklearn to calculate precision and recall of the model\n#Create a classification report\nprediction_control=ANN.predict(X_test, verbose=1)\nlabels=[\"Normal\",\"Fraud\"]\ny_pred=np.argmax(prediction_control,axis=1)\nprecision=precision_score(y_test,y_pred,labels=labels)\nrecall=recall_score(y_test,y_pred,labels=labels)\nprint(\"Fraud Precision:{}\".format(precision))\nprint(\"Fraud Recall:{}\".format(recall))\n\nprint(\"\\n\")\nprint(classification_report(y_test,y_pred, target_names=labels,digits=8))","2936031f":"ANN_exp=keras.Sequential()\nANN_exp.add(keras.layers.Dense(4, input_shape=(29,),activation=\"relu\"))\nANN_exp.add(keras.layers.Dense(4, activation=\"relu\"))\nANN_exp.add(keras.layers.Dense(2, activation=\"softmax\"))\n    \nANN_exp.compile(keras.optimizers.Adam(lr=0.04), \"categorical_crossentropy\",metrics=[\"accuracy\"])\n    \nANN_exp.fit(x_resample,Y_resample, epochs=30, verbose=0)","b121fa84":"#Evaluate the model based on accuracy\nY_test=keras.utils.to_categorical(y_test,num_classes=None)\nexp_accuracy=ANN_exp.evaluate(X_test,Y_test)[1]\nprint(\"Accuracy: {}\".format(exp_accuracy))\nprint(\"\\n\")\n\n#Use sklearn to calculate precision and recall of the model\n#Create a classification report\nprediction_exp=ANN_exp.predict(X_test, verbose=1)\nlabels=[\"Normal\",\"Fraud\"]\ny_pred_exp=np.argmax(prediction_exp,axis=1)\nprecision=precision_score(y_test,y_pred_exp,labels=labels)\nrecall=recall_score(y_test,y_pred_exp,labels=labels)\nprint(\"Fraud Precision:{}\".format(precision))\nprint(\"Fraud Recall:{}\".format(recall))\n\nprint(\"\\n\")\nprint(classification_report(y_test,y_pred_exp, target_names=labels,digits=8))","a1f6e0b1":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n        \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","e3f8ca24":"cnf_matrix_control = confusion_matrix(y_test,y_pred)\n\nclass_names = [\"Normal\",\"Fraud\"]\nplt.figure(figsize=(6,6))\nplot_confusion_matrix(cnf_matrix_control\n                      , classes=class_names\n                      , title='Confusion matrix of control arm')\nplt.show()\n\ncnf_matrix_exp = confusion_matrix(y_test,y_pred_exp)\n\nclass_names = [\"Normal\",\"Fraud\"]\nplt.figure(figsize=(6,6))\nplot_confusion_matrix(cnf_matrix_exp\n                      , classes=class_names\n                      , title='Confusion matrix of experimental arm')\nplt.show()","d5edad8c":"results_control_evaluation= []\nfor i in range(0,30):\n    model=keras.Sequential()\n    model.add(keras.layers.Dense(4, input_shape=(29,),activation=\"relu\"))\n    model.add(keras.layers.Dense(2, activation=\"softmax\"))\n    model.compile(keras.optimizers.Adam(lr=0.04), \"categorical_crossentropy\",metrics=[\"accuracy\"])\n    \n    model.fit(x_resample,Y_resample, epochs=30, verbose=0)\n    \n    Y_test=keras.utils.to_categorical(y_test,num_classes=None)\n    accuracy=model.evaluate(X_test,Y_test)[1]\n    \n    prediction_control=model.predict(X_test, verbose=0)\n    labels=[\"Normal\",\"Fraud\"]\n    y_pred=np.argmax(prediction_control,axis=1)\n    precision=precision_score(y_test,y_pred)\n    recall=recall_score(y_test,y_pred)\n    evaluation=pd.DataFrame([accuracy,precision,recall])\n    results_control_evaluation.append(evaluation)\nresult_control=pd.concat(results_control_evaluation,axis=1)\nprint(result_control)","d739c07a":"results_experimental_evaluation= []\nfor i in range(0,30):\n    model_exp=keras.Sequential()\n    model_exp.add(keras.layers.Dense(4, input_shape=(29,),activation=\"relu\"))\n    model_exp.add(keras.layers.Dense(4, activation=\"relu\"))\n    model_exp.add(keras.layers.Dense(2, activation=\"softmax\"))\n    \n    model_exp.compile(keras.optimizers.Adam(lr=0.04), \"categorical_crossentropy\",metrics=[\"accuracy\"])\n    \n    model_exp.fit(x_resample,Y_resample, epochs=30, verbose=0)\n    \n    Y_test=keras.utils.to_categorical(y_test,num_classes=None)\n    accuracy=model_exp.evaluate(X_test,Y_test)[1]\n\n    prediction_experimental=model_exp.predict(X_test, verbose=0)\n    labels=[\"Normal\",\"Fraud\"]\n    y_exp_pred=np.argmax(prediction_control,axis=1)\n    precision=precision_score(y_test,y_exp_pred)\n    recall=recall_score(y_test,y_exp_pred)\n    evaluation=pd.DataFrame([accuracy,precision,recall])\n    results_experimental_evaluation.append(evaluation)\nresult_experimental=pd.concat(results_experimental_evaluation,axis=1)\nprint(result_experimental)","77152ee9":"from scipy import stats\n\nalpha = 0.05;\n\ns, p = stats.normaltest(result_control.iloc[0,:])\nif p < alpha:\n  print('Control data is not normal')\nelse:\n  print('Control data is normal')\nprint(\"p-value:{}\".format(p))\nprint(\"\\n\")\ns, p = stats.normaltest(result_experimental.iloc[0,:])\nif p < alpha:\n  print('Experimental data is not normal')\nelse:\n  print('Experimental data is normal')\n\nprint (\"p-value:{}\".format(p))","2be39509":"median_control_acc=result_control.iloc[0,:].median()\nmedian_control_preci=result_control.iloc[1,:].median()\nmedian_control_recall=result_control.iloc[2,:].median()\n\nmedian_exp_acc=result_experimental.iloc[0,:].median()\nmedian_exp_preci=result_experimental.iloc[1,:].median()\nmedian_exp_recall=result_experimental.iloc[2,:].median()","d5cd8f21":"print(\"Median control accuracy:{}\".format(median_control_acc))\nprint(\"Median experim accuracy:{}\".format(median_exp_acc))\nprint(\"\\n\")\n\n#Significance test of accuracy\ns, p = stats.wilcoxon(result_control.iloc[0,:], result_experimental.iloc[0,:])\nprint(\"p-value:{}\".format(p))\nif p < 0.05:\n  print('null hypothesis rejected, significant difference between the data-sets')\nelse:\n  print('null hypothesis accepted, no significant difference between the data-sets')","fe0f0dd8":"print(\"Median control precision:{}\".format(median_control_preci))\nprint(\"Median experim precision:{}\".format(median_exp_preci))\nprint(\"\\n\")\n\n#Significance test of Precision\ns, p = stats.wilcoxon(result_control.iloc[1,:], result_experimental.iloc[1,:])\nprint(\"p-value:{}\".format(p))\nif p < 0.05:\n  print('null hypothesis rejected, significant difference between the data-sets')\nelse:\n  print('null hypothesis accepted, no significant difference between the data-sets')","22bc6303":"print(\"Median control recall:{}\".format(median_control_recall))\nprint(\"Median experim recall:{}\".format(median_exp_recall))\nprint(\"\\n\")\n\n#Significance test of Precision\ns, p = stats.wilcoxon(result_control.iloc[2,:], result_experimental.iloc[2,:])\nprint(\"p-value:{}\".format(p))\nif p < 0.05:\n  print('null hypothesis rejected, significant difference between the data-sets')\nelse:\n  print('null hypothesis accepted, no significant difference between the data-sets')","9de5825e":"result_accuracy=pd.concat([result_control.iloc[0,:],result_experimental.iloc[0,:]],axis=1)\nresult_precision=pd.concat([result_control.iloc[1,:],result_experimental.iloc[1,:]],axis=1)\nresult_recall=pd.concat([result_control.iloc[2,:],result_experimental.iloc[2,:]],axis=1)\n\nresult_accuracy.columns=[\"control\",\"experimental\"]\nresult_precision.columns=[\"control\",\"experimental\"]\nresult_recall.columns=[\"control\",\"experimental\"]","7859f0db":"result_accuracy.boxplot()\nplt.title(\"Boxplot of Accuracy\")","34762e1d":"result_precision.boxplot()\nplt.title(\"Boxplot of Precision\")","3444f6bf":"result_recall.boxplot()\nplt.title(\"Boxplot of Recall\")","8a604d67":"It can hardly see a uniform distribution in fraudulent transactions, while a cyclical distribution is found in normal transactions.","8a4603eb":"# Designing and Configuring BPNN algorithms","fbbbe5ef":"Next, train the model using training set.","da3ae69e":"The same approach is used for experimental arm.","a89c2409":"Since we have two classes which are already integers of y_resample. 1 represents fraud, 0 represents normal.\n\nNow we need to apply one-hot encoding to reformat y_resample to be a 2-dimensional vector in terms of [1. 0.] or [0. 1.].","fd5fe8b5":"Here we are using **Stratified ShuffleSplit cross_validator**, which is a merge of StratifiedKFold and ShuffleSplit. We split data into **5 folds**, each fold has **30% test** sets and **70% train** sets. \n\n*Note: Each fold is made by the preserving percentage of samples for each class, in this case is Class 0: Class 1 = 284315 : 492*\n\n> Stratified ShuffleSplit cross-validator\nProvides train\/test indices to split data in train\/test sets.\nThis cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.\nNote: like the ShuffleSplit strategy, stratified random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.\n\n- Scikit-learn organization (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit)","8f918ff6":"Fraudulent transactions are mostly small amount. Only one fraud is over $1000, most of them are less than $100. Normal transactions have maximum amount at approximately $25000.","8b3a0bfe":"Each class has nearly the same amount of samples, the training dataset now is balanced.\n\nNow we can use the same approach as earlier to reduce the dimensionality of our balanced training samples and visualise it using scatterplot.","70b31cf7":"Then we design experimental arm.","53c2500e":"Next, test the model and provide evaluations","fbaad01d":"# Import useful APIs for Machine Learning #","4f053dec":"It seems there is **no missing value** within our dataset. Great!\n\nNext, we need to select valuable variables that would be used for later classification. ","fa6a98ff":"Now we design a ANN model using Kera from TensorFlow to train the training dataset as a **control arm**. The model consists of one hidden layer with four neurons on it. ","528ce6ab":"In order to maximize machine learning outcomes, we have to oversample the data, which is to create new synthetic samples that simulate the minority class to balance the dataset. Now we can use SMOTE( ) to oversample the training data.\n\n> ADASYN:  ADAptive SYNthetic (ADASYN) is based on the idea of adaptively generating minority data samples according to their distributions using K nearest neighbor. The algorithm adaptively updates the distribution and there are no assumptions made for the underlying distribution of the data.  The algorithm uses Euclidean distance for KNN Algorithm. The key difference between ADASYN and SMOTE is that the former uses a density distribution, as a criterion to automatically decide the number of synthetic samples that must be generated for each minority sample by adaptively changing the weights of the different minority samples to compensate for the skewed distributions. The latter generates the same number of synthetic samples for each original minority sample.\n\n-- Rohit Walimbe, Data Science Central (https:\/\/www.datasciencecentral.com\/profiles\/blogs\/handling-imbalanced-data-sets-in-supervised-learning-using-family)","c7488b1a":"Next we need to split the samples to training and testing data. Due to highly imbalanced nature of our dataset, the model-validation technique we will use is **Stratified cross-validation**. \n\n> Stratified cross-validation is a good technique in the case of highly imbalanced classes. For binary classification with a training\/test split rather than cross-validation, this involves the training set having the same proportion of positive-labeled points as the test set (and hence the same as the overall training set). Such a split is easily accomplished by splitting your points by label resulting in two sets, shuffling each of these sets, and then placing the first x% of each set into the training set and the last x% of each set into the test set.\n\n   - StackExchange (https:\/\/stats.stackexchange.com\/questions\/91922\/splitting-an-imbalanced-dataset-for-training-and-testing)","a83e7b57":"According to the values of \"Class\", we can see the dataset is highly imbalanced. Class \"1\" consist of 284315 samples while Class \"0\" consist of 492 samples. Therefore later we need to deal with the problem.","3b65d321":"No clear ralationship between Amount and Time in both fraud and normal classes.","3277b983":"Firstly, we can check the distribution of the amount of transactions using different transaction time.","6b553dc2":"The samples of \"Time\" are integers rather than float, which are in a different form from other variable samples. Here I will drop it to avoid noise.","3e6c66e6":"# One-Hot Encoding for Class labels","01f8162c":"# Generating 30 predictions of experimental arm","210345a2":"Next we need to verify sample size sufficiency to make sure the sample size of training and testing datasets are sufficient to demonstrate the hypothese test.","2c312b91":"# Stratified Cross-Validation - Split training\/testing datasets","c9c6f7b4":"**H0**: the null hypothesis: insufficient evidence to support hypothesis. \n\n**H1**: the alternate hypothesis: evidence suggests the hypothesis is likely true.","5cc04397":"# Generating 30 predictions of control arm","1f8c9032":"# Data Preprocessing #","374357c1":"# Data Visualization","992ae28a":"# Non-parametric test and significance test","25e171e2":"Let's see how many samples in each class after oversampling.","adee009b":"# Oversampling Training Data","d89be4ff":"Now we got a plot distribution of imbalanced training data. The figure shows some complex non-linear relationship between x and y. \n\nAs our training data is still highly imbalanced, which we can see the majority points are in blue and minority are in yellow. Thus we need to resample the training sets to create balanced training data for later machine learning.","895cf32a":"According to definition:\n> scipy.stats.normaltest(a, axis=0, nan_policy='propagate')\nTest whether a sample differs from a normal distribution. This function tests the null hypothesis that a sample comes from a normal distribution.\n- https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.normaltest.html\n\n\nFor this test,\n\nNull hypothesis: H0: Samples come from a normal distribution\n\nH1: Samples do not come from a normal distribution\n","33c5082f":"Based on the model we designed above, we can receive evaluation results thirty times using the loop below.","fc2beb1a":"Since the control and experimental arm are both not normal distribution, non-parametric test in terms of median and Wilcoxon signed-rank test are selected to test the hypothesis.","c7f342ad":"Here, we can see that there are **284807 rows** and **31 columns** in the dataset.","6550f0f2":"\nBefore starting the experiment, a testable hypothesis is defined. Since the aim of this project is to implement a ANN algorithmn for credit card fraud detection. To demonstrate whether the number of hidden layers can influence the classification performance of ANN, a hypothesis here is defined:\n\n**Hypothesis**: The classification performance of a neural network classifier on the Credit Card Fraud Detection dataset is affected by the number of hidden layer neurons.","16aabcb4":"# 2-D visualisation of imbalanced training data","fd8ec833":"# Statistic Normal Test","ebdb2337":"Before resampling, let's reduce the dimensionality of our input variables and visualise the imbalanced training dataset first. Here we reduce variables to 2 dimensions."}}