{"cell_type":{"75e96fc8":"code","a378fe40":"code","de87a76a":"code","a59f3049":"code","8fe7dce6":"code","00ecbb37":"code","4054faac":"code","3beeb424":"code","cb02d9d5":"code","a76c2688":"code","95104542":"code","5286192e":"code","ea1972a8":"code","5139214b":"code","32c4cce4":"code","c42be617":"code","963c93f7":"code","a97a3287":"code","4fd98b10":"code","46fad0ab":"code","7215a843":"code","ccd4b3d5":"code","f015e493":"code","409e7b3a":"code","54d3274c":"code","3214e387":"code","4927caa4":"code","528c7ecf":"code","b8012b3e":"code","d12bd31a":"code","43fd5752":"code","30b578d4":"code","020b63e9":"code","298edf8c":"code","81771456":"code","75d345ff":"code","20dffd19":"code","3420bea1":"code","51d60fa5":"code","4a885ff7":"code","d1906b55":"code","2fd26a79":"code","d453fbb7":"code","646c6438":"code","aaabbe39":"code","63f92255":"code","9dd15429":"code","7793cee5":"code","09ca284b":"code","6e606bc8":"code","6481dfb6":"code","b5da4b25":"code","c6bcecd3":"code","453e6740":"code","c2854dfe":"code","4a98d8ce":"code","8ae5269f":"code","45219949":"code","602de343":"code","a11e46a7":"code","ae6fe1b4":"code","23477e09":"code","975dbf19":"code","90d5f02c":"code","496a27cd":"markdown","baabc095":"markdown","83b8fe64":"markdown","91fa9228":"markdown","9eb5f251":"markdown","0c4bdcc5":"markdown","0c89304a":"markdown","7edeab44":"markdown","28da80ca":"markdown","8a13fedf":"markdown","e5693889":"markdown","1c80ea7a":"markdown","5bab38d4":"markdown","c554e548":"markdown","4eeaa509":"markdown","82dfcb02":"markdown","1671260e":"markdown","40dd3929":"markdown","d354cd30":"markdown","98df62b2":"markdown","c8a6a179":"markdown","d1e9f044":"markdown","3005e43f":"markdown","dae524b5":"markdown","dd90c65c":"markdown","e871ced2":"markdown","c34fe5f2":"markdown","815f1b96":"markdown","5364c67a":"markdown","146fd3a5":"markdown","5cef2542":"markdown","f59bb624":"markdown","55cf9675":"markdown","507f731b":"markdown","a67855e9":"markdown","b2b10d4e":"markdown","d0af27b5":"markdown","8ab1b72a":"markdown","b6477ba5":"markdown","23caa649":"markdown","6ec9f69b":"markdown","6e92f2fd":"markdown","62ebd7ec":"markdown","87c8dad4":"markdown","2ee45289":"markdown","7222edc4":"markdown","9fe370c3":"markdown","aded6fc9":"markdown","03fc6dd6":"markdown","7bf73fcb":"markdown","fa8d0967":"markdown","bb472d8a":"markdown","d4840497":"markdown","542b96a9":"markdown","fd30d69f":"markdown","b1678813":"markdown","4e51dc7a":"markdown","16283567":"markdown","15485d7e":"markdown","6c78825d":"markdown","8135a588":"markdown","09215db9":"markdown","ef63035b":"markdown","b24ec41d":"markdown","c74a2509":"markdown"},"source":{"75e96fc8":"# Data Processing and Cleaning\nimport numpy as np\nimport pandas as pd\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nimport seaborn as sns\n\n# Sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.tree import export_graphviz\n\n# Modeling\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\n#Miscellaneous\nfrom tqdm import tqdm_notebook\n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","a378fe40":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')","de87a76a":"train.head(3).T","a59f3049":"train.shape","8fe7dce6":"test.shape","00ecbb37":"train.loc[train['id'] == 16,'revenue'] = 192864         \ntrain.loc[train['id'] == 90,'budget'] = 30000000                  \ntrain.loc[train['id'] == 118,'budget'] = 60000000       \ntrain.loc[train['id'] == 149,'budget'] = 18000000       \ntrain.loc[train['id'] == 313,'revenue'] = 12000000       \ntrain.loc[train['id'] == 451,'revenue'] = 12000000      \ntrain.loc[train['id'] == 464,'budget'] = 20000000       \ntrain.loc[train['id'] == 470,'budget'] = 13000000       \ntrain.loc[train['id'] == 513,'budget'] = 930000         \ntrain.loc[train['id'] == 797,'budget'] = 8000000        \ntrain.loc[train['id'] == 819,'budget'] = 90000000       \ntrain.loc[train['id'] == 850,'budget'] = 90000000       \ntrain.loc[train['id'] == 1007,'budget'] = 2              \ntrain.loc[train['id'] == 1112,'budget'] = 7500000       \ntrain.loc[train['id'] == 1131,'budget'] = 4300000        \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       \ntrain.loc[train['id'] == 1542,'budget'] = 1             \ntrain.loc[train['id'] == 1570,'budget'] = 15800000       \ntrain.loc[train['id'] == 1571,'budget'] = 4000000        \ntrain.loc[train['id'] == 1714,'budget'] = 46000000       \ntrain.loc[train['id'] == 1721,'budget'] = 17500000       \ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      \ntrain.loc[train['id'] == 1885,'budget'] = 12             \ntrain.loc[train['id'] == 2091,'budget'] = 10             \ntrain.loc[train['id'] == 2268,'budget'] = 17500000       \ntrain.loc[train['id'] == 2491,'budget'] = 6              \ntrain.loc[train['id'] == 2602,'budget'] = 31000000       \ntrain.loc[train['id'] == 2612,'budget'] = 15000000       \ntrain.loc[train['id'] == 2696,'budget'] = 10000000      \ntrain.loc[train['id'] == 2801,'budget'] = 10000000       \ntrain.loc[train['id'] == 335,'budget'] = 2 \ntrain.loc[train['id'] == 348,'budget'] = 12\ntrain.loc[train['id'] == 470,'budget'] = 13000000 \ntrain.loc[train['id'] == 513,'budget'] = 1100000\ntrain.loc[train['id'] == 640,'budget'] = 6 \ntrain.loc[train['id'] == 696,'budget'] = 1\ntrain.loc[train['id'] == 797,'budget'] = 8000000 \ntrain.loc[train['id'] == 850,'budget'] = 1500000\ntrain.loc[train['id'] == 1199,'budget'] = 5 \ntrain.loc[train['id'] == 1282,'budget'] = 9              \ntrain.loc[train['id'] == 1347,'budget'] = 1\ntrain.loc[train['id'] == 1755,'budget'] = 2\ntrain.loc[train['id'] == 1801,'budget'] = 5\ntrain.loc[train['id'] == 1918,'budget'] = 592 \ntrain.loc[train['id'] == 2033,'budget'] = 4\ntrain.loc[train['id'] == 2118,'budget'] = 344 \ntrain.loc[train['id'] == 2252,'budget'] = 130\ntrain.loc[train['id'] == 2256,'budget'] = 1 \ntrain.loc[train['id'] == 2696,'budget'] = 10000000","4054faac":"test.loc[test['id'] == 3033,'budget'] = 250 \ntest.loc[test['id'] == 3051,'budget'] = 50\ntest.loc[test['id'] == 3084,'budget'] = 337\ntest.loc[test['id'] == 3224,'budget'] = 4  \ntest.loc[test['id'] == 3594,'budget'] = 25  \ntest.loc[test['id'] == 3619,'budget'] = 500  \ntest.loc[test['id'] == 3831,'budget'] = 3  \ntest.loc[test['id'] == 3935,'budget'] = 500  \ntest.loc[test['id'] == 4049,'budget'] = 995946 \ntest.loc[test['id'] == 4424,'budget'] = 3  \ntest.loc[test['id'] == 4460,'budget'] = 8  \ntest.loc[test['id'] == 4555,'budget'] = 1200000 \ntest.loc[test['id'] == 4624,'budget'] = 30 \ntest.loc[test['id'] == 4645,'budget'] = 500 \ntest.loc[test['id'] == 4709,'budget'] = 450 \ntest.loc[test['id'] == 4839,'budget'] = 7\ntest.loc[test['id'] == 3125,'budget'] = 25 \ntest.loc[test['id'] == 3142,'budget'] = 1\ntest.loc[test['id'] == 3201,'budget'] = 450\ntest.loc[test['id'] == 3222,'budget'] = 6\ntest.loc[test['id'] == 3545,'budget'] = 38\ntest.loc[test['id'] == 3670,'budget'] = 18\ntest.loc[test['id'] == 3792,'budget'] = 19\ntest.loc[test['id'] == 3881,'budget'] = 7\ntest.loc[test['id'] == 3969,'budget'] = 400\ntest.loc[test['id'] == 4196,'budget'] = 6\ntest.loc[test['id'] == 4221,'budget'] = 11\ntest.loc[test['id'] == 4222,'budget'] = 500\ntest.loc[test['id'] == 4285,'budget'] = 11\ntest.loc[test['id'] == 4319,'budget'] = 1\ntest.loc[test['id'] == 4639,'budget'] = 10\ntest.loc[test['id'] == 4719,'budget'] = 45\ntest.loc[test['id'] == 4822,'budget'] = 22\ntest.loc[test['id'] == 4829,'budget'] = 20\ntest.loc[test['id'] == 4969,'budget'] = 20\ntest.loc[test['id'] == 5021,'budget'] = 40 \ntest.loc[test['id'] == 5035,'budget'] = 1 \ntest.loc[test['id'] == 5063,'budget'] = 14 \ntest.loc[test['id'] == 5119,'budget'] = 2 \ntest.loc[test['id'] == 5214,'budget'] = 30 \ntest.loc[test['id'] == 5221,'budget'] = 50 \ntest.loc[test['id'] == 4903,'budget'] = 15\ntest.loc[test['id'] == 4983,'budget'] = 3\ntest.loc[test['id'] == 5102,'budget'] = 28\ntest.loc[test['id'] == 5217,'budget'] = 75\ntest.loc[test['id'] == 5224,'budget'] = 3 \ntest.loc[test['id'] == 5469,'budget'] = 20 \ntest.loc[test['id'] == 5840,'budget'] = 1 \ntest.loc[test['id'] == 5960,'budget'] = 30\ntest.loc[test['id'] == 6506,'budget'] = 11 \ntest.loc[test['id'] == 6553,'budget'] = 280\ntest.loc[test['id'] == 6561,'budget'] = 7\ntest.loc[test['id'] == 6582,'budget'] = 218\ntest.loc[test['id'] == 6638,'budget'] = 5\ntest.loc[test['id'] == 6749,'budget'] = 8 \ntest.loc[test['id'] == 6759,'budget'] = 50 \ntest.loc[test['id'] == 6856,'budget'] = 10\ntest.loc[test['id'] == 6858,'budget'] =  100\ntest.loc[test['id'] == 6876,'budget'] =  250\ntest.loc[test['id'] == 6972,'budget'] = 1\ntest.loc[test['id'] == 7079,'budget'] = 8000000\ntest.loc[test['id'] == 7150,'budget'] = 118\ntest.loc[test['id'] == 6506,'budget'] = 118\ntest.loc[test['id'] == 7225,'budget'] = 6\ntest.loc[test['id'] == 7231,'budget'] = 85\ntest.loc[test['id'] == 5222,'budget'] = 5\ntest.loc[test['id'] == 5322,'budget'] = 90\ntest.loc[test['id'] == 5350,'budget'] = 70\ntest.loc[test['id'] == 5378,'budget'] = 10\ntest.loc[test['id'] == 5545,'budget'] = 80\ntest.loc[test['id'] == 5810,'budget'] = 8\ntest.loc[test['id'] == 5926,'budget'] = 300\ntest.loc[test['id'] == 5927,'budget'] = 4\ntest.loc[test['id'] == 5986,'budget'] = 1\ntest.loc[test['id'] == 6053,'budget'] = 20\ntest.loc[test['id'] == 6104,'budget'] = 1\ntest.loc[test['id'] == 6130,'budget'] = 30\ntest.loc[test['id'] == 6301,'budget'] = 150\ntest.loc[test['id'] == 6276,'budget'] = 100\ntest.loc[test['id'] == 6473,'budget'] = 100\ntest.loc[test['id'] == 6842,'budget'] = 30","3beeb424":"pd.DataFrame(train.isnull().sum()).T","cb02d9d5":"((pd.DataFrame(train.isnull().sum()).T)\/len(train))*100","a76c2688":"((pd.DataFrame(test.isnull().sum()).T)\/len(test))*100","95104542":"pd.DataFrame(train.budget.value_counts()).T","5286192e":"train[train['revenue'] < 10][['imdb_id', 'title']].T","ea1972a8":"fig = plt.figure(figsize=(18,15))\nplt.subplots_adjust(hspace=0.5)\n\n# Plot 1: Target Variable Distribution\nplt.subplot2grid((4,2), (0,0))\n\nsns.distplot(np.log1p(train['revenue']), kde=False, bins=40)\n\nplt.title('Distribution for Target Variable', fontsize = 15)\nplt.xlabel('Revenue - log(1+x)', fontsize=12)\n\n# Plot 2: Revenue and Budget\nplt.subplot2grid((4,2), (0,1))\n\nsns.scatterplot(x = 'budget', y = 'revenue', data=train)\n\nplt.title('Revenue vs. Budget', fontsize = 15)\nplt.xlabel('Budget', fontsize=12)\nplt.ylabel('Revenue', fontsize=12)\n\n# Plot 3: Revenue, Runtime, and Popularity\nax = plt.subplot2grid((4,2), (1,0), projection='3d', rowspan = 2, colspan = 2)\n\nx3d = np.array(train['runtime'])\ny3d = np.array(train['popularity'])\nz3d = np.array(train['revenue'])\n\n# Unique category labels\ncolor_labels = train['original_language'].unique()\n\n# List of RGB triplets\nrgb_values = sns.color_palette(\"Set2\", len(color_labels))\n\n# Map label to RGB\ncolor_map = dict(zip(color_labels, rgb_values))\ncolors = train['original_language'].map(color_map)\ncolors = np.random.rand(len(train))\n\nax.scatter(\n    x3d, y3d, z3d,\n    c = colors,\n    alpha = 0.8,\n    )\n\nax.set_xlabel('Runtime')\nax.set_ylabel('Popularity')\nax.set_zlabel('Revenue')\nax.set_title('3D plot for Runtime, Popularity and Revenue', fontsize = 15)\n\n# Plot 4: Correlation matrix\nplt.subplot2grid((4,2), (3,0))\n\ncorr = train.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, mask = mask, cmap = 'PiYG', annot = True, fmt=\".2f\")\n\nplt.yticks(rotation=0) \nplt.xticks(rotation=0)\nplt.title('Correlation Matrix for Train Data', fontsize = 15)\n\n# Plot 5: Distribution of Films by Language\nplt.subplot2grid((4,2), (3,1), colspan = 2)\n\ntop5_languages = train['original_language'].value_counts()[:5]\ntop5_languages.plot(kind = 'bar')\n\nplt.yticks(rotation=0) \nplt.xticks(rotation=0)\nplt.title('Distribution of Films by Language', fontsize = 15)\n\n# Display Plot\nplt.show()","5139214b":"train['revenue'] = np.log1p(train['revenue'])","32c4cce4":"x = train.budget.values.reshape(-1,1)\ny = train.revenue\nreg = LinearRegression().fit(x, y)","c42be617":"print(f'Regression Score: {reg.score(x, y)}')\nprint(f'Regression Coefficient: {reg.coef_[0]}')\nprint(f'Regression Intercept: {reg.intercept_}')","963c93f7":"predictions = reg.predict(test['budget'].values.reshape(-1,1))","a97a3287":"submission['revenue'] = np.round(np.expm1(predictions))","4fd98b10":"submission.to_csv('submission_budget_linreg.csv', index = False)","46fad0ab":"rf_cols = ['budget', 'original_language', 'popularity', 'release_date', 'runtime', 'status', 'homepage', 'overview', 'revenue']\nrf_train = train[rf_cols].copy()\nrf_cols.remove('revenue')\nrf_test = test[rf_cols].copy()","7215a843":"median_budget = rf_train[rf_train['budget'] > 0]['budget'].median()\nmedian_budget","ccd4b3d5":"def fill_budget(df, median_budget):\n    df['budget_is_median'] = 0\n    df.loc[df.budget == 0, 'budget_is_median'] = 1\n    df.loc[df.budget == 0, 'budget'] = median_budget\n    return df","f015e493":"rf_train = fill_budget(rf_train, median_budget)\nrf_test = fill_budget(rf_test, median_budget)","409e7b3a":"rf_combined = pd.concat([rf_train, rf_test], sort=False)","54d3274c":"le = LabelEncoder()\nle.fit(rf_combined['original_language'])\nrf_train['original_language'] = le.transform(rf_train['original_language'])\nrf_test['original_language'] = le.transform(rf_test['original_language'])","3214e387":"rf_test.loc[rf_test['status'].isnull() == True, 'status'] = 'Released'\nrf_combined.loc[rf_combined['status'].isnull() == True, 'status'] = 'Released'","4927caa4":"le = LabelEncoder()\nle.fit(rf_combined['status'])\nrf_train['status'] = le.transform(rf_train['status'])\nrf_test['status'] = le.transform(rf_test['status'])","528c7ecf":"rf_train.loc[rf_train['homepage'].isnull() == True, 'homepage'] = 0\nrf_train.loc[rf_train['homepage'].isnull() == False, 'homepage'] = 1\n\nrf_test.loc[rf_test['homepage'].isnull() == True, 'homepage'] = 0\nrf_test.loc[rf_test['homepage'].isnull() == False, 'homepage'] = 1","b8012b3e":"median_runtime = rf_train['runtime'].median()\nmedian_runtime","d12bd31a":"def fill_runtime(df, median_runtime):\n    df['runtime_is_median'] = 0\n    df.loc[df.runtime == 0, 'runtime_is_median'] = 1\n    df.loc[df.runtime.isnull() == True, 'runtime_is_median'] = 1\n    df.loc[df.runtime == 0, 'runtime'] = median_runtime\n    df.loc[df.runtime.isnull() == True, 'runtime'] = median_runtime\n    return df","43fd5752":"rf_train = fill_runtime(rf_train, median_runtime)\nrf_test = fill_runtime(rf_test, median_runtime)","30b578d4":"from datetime import timedelta, date","020b63e9":"rf_test.loc[rf_test['release_date'].isnull() == True, 'release_date'] = '10\/19\/2001'\ntest.loc[test['release_date'].isnull() == True, 'release_date'] = '10\/19\/2001'","298edf8c":"def add_date_features(df, col, prefix):\n    df[col] = pd.to_datetime(df[col])\n    future = df[col] > pd.Timestamp(year=2017,month=12,day=31)\n    df.loc[future, col] -= timedelta(days=365.25*100)\n    \n    df[prefix+'_day_of_week'] = df[col].dt.dayofweek\n    df[prefix+'_day_of_year'] = df[col].dt.dayofyear\n    df[prefix+'_month'] = df[col].dt.month\n    df[prefix+'_year'] = df[col].dt.year\n    df[prefix+'_day'] = df[col].dt.day\n    df[prefix+'_is_year_end'] = df[col].dt.is_year_end\n    df[prefix+'_is_year_start'] = df[col].dt.is_year_start\n    df[prefix+'_week'] = df[col].dt.week\n    df[prefix+'_quarter'] = df[col].dt.quarter    \n    \n    df.drop(col, axis = 1, inplace = True)\n\n    return df","81771456":"rf_train = add_date_features(rf_train, 'release_date', 'release')\nrf_test = add_date_features(rf_test, 'release_date', 'release')","75d345ff":"def get_dictionary(s):\n    try:\n        d = eval(s)\n    except:\n        d = {}\n    return d","20dffd19":"json_cols = ['production_companies', 'production_countries', 'cast', 'crew', 'Keywords', 'belongs_to_collection']\nfor col in json_cols:\n    rf_train[col] = train[col]\n    rf_train[col] = rf_train[col].apply(lambda x: get_dictionary(x))\n    rf_test[col] = test[col]\n    rf_test[col] = rf_test[col].apply(lambda x: get_dictionary(x))","3420bea1":"for col in json_cols:\n    # Get name of collection movie belongs to\n    if col == 'belongs_to_collection':\n        rf_train['collection_name'] = rf_train[col].apply(lambda row: row[0]['name'] if row != {} else '0')\n        rf_test['collection_name'] = rf_test[col].apply(lambda row: row[0]['name'] if row != {} else '0')\n        rf_combined = pd.concat([rf_train, rf_test], sort=False)\n        le = LabelEncoder()\n        le.fit(rf_combined['collection_name'])\n        rf_train['collection_name'] = le.transform(rf_train['collection_name'])\n        rf_test['collection_name'] = le.transform(rf_test['collection_name'])    \n    \n    # Size of feature\n    rf_train[col] = rf_train[col].apply(lambda row: 0 if row is None else len(row))\n    rf_test[col] = rf_test[col].apply(lambda row: 0 if row is None else len(row))\n\n# Word count for overview\nrf_train['overview_wordcount'] = rf_train['overview'].str.split().str.len()\nrf_train.drop('overview', axis = 1, inplace = True)\nrf_train.loc[rf_train['overview_wordcount'].isnull() == True, 'overview_wordcount'] = 0\n\nrf_test['overview_wordcount'] = rf_test['overview'].str.split().str.len()\nrf_test.drop('overview', axis = 1, inplace = True)\nrf_test.loc[rf_test['overview_wordcount'].isnull() == True, 'overview_wordcount'] = 0\n\n# Feature Interactions\nrf_train['_budget_runtime_ratio'] = np.round(rf_train['budget']\/rf_train['runtime'], 2)\nrf_train['_budget_year_ratio'] = np.round(rf_train['budget']\/(rf_train['release_year']*rf_train['release_year']), 2)\nrf_train['_releaseYear_popularity_ratio'] = np.round(rf_train['release_year']\/rf_train['popularity'], 2)\n\nrf_test['_budget_runtime_ratio'] = np.round(rf_test['budget']\/rf_test['runtime'], 2)\nrf_test['_budget_year_ratio'] = np.round(rf_test['budget']\/(rf_test['release_year']*rf_test['release_year']), 2)\nrf_test['_releaseYear_popularity_ratio'] = np.round(rf_test['release_year']\/rf_test['popularity'], 2)\n","51d60fa5":"rf_train.head()","4a885ff7":"X_train, X_valid, y_train, y_valid = train_test_split(\n    rf_train.drop('revenue', axis = 1), rf_train['revenue'], \n    test_size=0.1, \n    random_state=42\n)","d1906b55":"def rmse(y_pred, y_true):\n    return np.sqrt(mean_squared_error(y_pred, y_true))\n\ndef print_rf_score(model):\n    print(f'Train R2:   {model.score(X_train, y_train)}')\n    print(f'Valid R2:   {model.score(X_valid, y_valid)}')\n    print(f'Train RMSE: {rmse(model.predict(X_train), y_train)}')\n    print(f'Valid RMSE: {rmse(model.predict(X_valid), y_valid)}')","2fd26a79":"rf = RandomForestRegressor(n_jobs = -1, random_state = 42)\nrf.fit(X_train, y_train)\nprint_rf_score(rf)","d453fbb7":"rf = RandomForestRegressor(\n    n_estimators = 1, \n    max_depth = 3, \n    bootstrap = False, \n    n_jobs = -1, \n    random_state = 42\n)\nrf.fit(X_train, y_train)\nprint_rf_score(rf)","646c6438":"# Export as dot file\nexport_graphviz(\n    rf.estimators_[0], \n    out_file='tree.dot', \n    feature_names = X_train.columns,\n    rounded = True, \n    proportion = False, \n    precision = 2, \n    filled = True,\n    rotate = True\n)\n\n# Convert to png using system command (requires Graphviz)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png', height = 600, width = 800)","aaabbe39":"rf = RandomForestRegressor(\n    n_estimators = 1, \n    bootstrap = False, \n    n_jobs = -1, \n    random_state = 42\n)\nrf.fit(X_train, y_train)\nprint_rf_score(rf)","63f92255":"rf = RandomForestRegressor(n_estimators = 10, n_jobs = -1, random_state = 42)\nrf.fit(X_train, y_train)\nprint_rf_score(rf)","9dd15429":"tree_preds = np.stack([tree.predict(X_valid) for tree in rf.estimators_])\nprint(f' Individual Tree Predictions: {[np.around(tree_preds[:,0], 1)]}')\nprint(f' Mean of Tree Predictions:    {np.mean(tree_preds[:,0])}')\nprint(f' Ground truth for sample:     {y_valid[0]}')","7793cee5":"%%time\ntrain_scores_r2 = []\nvalid_scores_r2 = []\ntrain_scores_rmse = []\nvalid_scores_rmse = []\nfor trees in tqdm_notebook(range(1, 100)):\n    rf = RandomForestRegressor(n_estimators = trees, n_jobs = -1, random_state = 42)\n    rf.fit(X_train, y_train)\n    train_scores_r2.append(rf.score(X_train, y_train))\n    valid_scores_r2.append(rf.score(X_valid, y_valid))    \n    train_scores_rmse.append(rmse(rf.predict(X_train), y_train))\n    valid_scores_rmse.append(rmse(rf.predict(X_valid), y_valid))\n","09ca284b":"fig = plt.figure(figsize=(10,8))\nplt.subplots_adjust(hspace=0.5)\n\n# Plot 1: Train R2\nplt.subplot2grid((2,2), (0,0))\n\nplt.plot(train_scores_r2)\nplt.title('Training Data R Squared', fontsize = 15)\nplt.xlabel('Estimators', fontsize=12)\n\n# Plot 2: Valid R2\nplt.subplot2grid((2,2), (0,1))\n\nplt.plot(valid_scores_r2, color='r')\nplt.title('Validation Set R Squared', fontsize = 15)\nplt.xlabel('Estimators', fontsize=12)\n\n# Plot 1: Train RMSE\nplt.subplot2grid((2,2), (1,0))\n\nplt.plot(train_scores_rmse)\nplt.title('Training Data RMSE', fontsize = 15)\nplt.xlabel('Estimators', fontsize=12)\n\n# Plot 2: Valid RMSE\nplt.subplot2grid((2,2), (1,1))\n\nplt.plot(valid_scores_rmse, color='r')\nplt.title('Validation Set RMSE', fontsize = 15)\nplt.xlabel('Estimators', fontsize=12)\n\nplt.show()","6e606bc8":"rf = RandomForestRegressor(\n    n_estimators = 20, \n    n_jobs = -1, \n    oob_score = True, \n    random_state = 42\n)\nrf.fit(X_train, y_train)\nprint_rf_score(rf)\nprint(f'OOB Score:  {rf.oob_score_}')","6481dfb6":"rf = RandomForestRegressor(\n    n_estimators = 20, \n    min_samples_leaf = 4, \n    n_jobs = -1, \n    oob_score = True, \n    random_state = 42\n)\nrf.fit(X_train, y_train)\nprint_rf_score(rf)\nprint(f'OOB Score:  {rf.oob_score_}')","b5da4b25":"rf = RandomForestRegressor(\n    n_estimators = 20, \n    min_samples_leaf = 4, \n    max_features = 0.3, \n    n_jobs = -1, \n    oob_score = True, \n    random_state = 42\n)\nrf.fit(X_train, y_train)\nprint_rf_score(rf)\nprint(f'OOB Score:  {rf.oob_score_}')","c6bcecd3":"rf = RandomForestRegressor(\n    n_estimators = 20, \n    min_samples_leaf = 4, \n    max_features = 0.3, \n    n_jobs = -1,\n    oob_score = True, \n    random_state = 42,\n)\nrf.fit(X_train, y_train)\nprint_rf_score(rf)\nprint(f'OOB Score:  {rf.oob_score_}')","453e6740":"predictions = np.expm1(rf.predict(rf_test))\nsubmission['revenue'] = np.round(predictions)\nsubmission.to_csv('submission_simple_rf.csv', index = False)","c2854dfe":"def xgtrain(X_train, X_valid, y_train, y_valid):\n    regressor = XGBRegressor(\n        n_estimators = 50000, \n        learning_rate = 0.001,\n        max_depth = 6, \n        subsample = 0.3, \n        colsample_bytree = 0.2\n        )\n    \n    regressor_ = regressor.fit(\n        X_train.values, y_train.values, \n        eval_metric = 'rmse', \n        eval_set = [\n            (X_train.values, y_train.values), \n            (X_valid.values, y_valid.values)\n        ],\n        verbose = 1000,\n        early_stopping_rounds = 500,\n        )\n    \n    return regressor_","4a98d8ce":"%%time\nregressor_ = xgtrain(X_train, X_valid, y_train, y_valid)","8ae5269f":"def lgbtrain(X_train, y_train, X_valid, y_valid):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': {'l2_root'},\n        'max_depth': 4,\n        'learning_rate': 0.001,\n        'feature_fraction': 0.3,\n        'bagging_fraction': 1,\n        'bagging_freq': 1,\n    }\n    \n    gbm = lgb.train(\n        params,\n        lgb_train,\n        valid_sets = lgb_eval,\n        num_boost_round=50000,\n        early_stopping_rounds=500,\n        verbose_eval = 1000\n    )\n    \n    return gbm","45219949":"%%time\ngbm = lgbtrain(X_train, y_train, X_valid, y_valid)","602de343":"feature_importances = pd.DataFrame(rf.feature_importances_, index = X_train.columns, columns=['importance'])\nfeature_importances = feature_importances.sort_values('importance', ascending=True)\nfeature_importances.plot(kind = 'barh', figsize = (10,8))\nplt.show()","a11e46a7":"predictions = np.expm1(rf.predict(rf_test)) + np.expm1(regressor_.predict(rf_test.values)) + np.expm1(gbm.predict(rf_test.values))\npredictions \/= 3","ae6fe1b4":"submission['revenue'] = np.round(predictions)","23477e09":"submission.head()","975dbf19":"submission.to_csv('submission_ensemble.csv', index = False)","90d5f02c":"from IPython.display import FileLinks\nFileLinks('.')","496a27cd":"Parse release date and extract features such as day, month, year!","baabc095":"Before using external data or performing extensive cleaning and parsing of features, let us fit the simplest linear regression and Random Forest model to get a baseline score. Before we do that though, we need to take the log of the target variable, as the competition metric is RMSLE (Root Mean Squared Log Error)","83b8fe64":"#### Bagging Trees","91fa9228":"The value of *n_estimators* is 10, which is the default. Let's grab predictions for each of these 10 trees and look at how they perform on the first validation sample.","9eb5f251":"#### Out of Bag Evaluation","0c4bdcc5":"## Feature Engineering","0c89304a":"## Training","7edeab44":"Let's plot different variables in the dataset to potentially gain interesting insights about distributions and target variables.","28da80ca":"#### Drawing a Single Decision Tree","8a13fedf":"### Preparing Submission","e5693889":"The Out of Bag R2 score is in the same range as the validation R2 score, which is good news.","1c80ea7a":"As we saw, our deep single tree massively overfit. Bagging, an important concept in ensembling, suggests that if we create a large number of such trees built on random samples of data and average their errors, we will get a good model. To illustrate this process better, let's return to our default random forest with no hyperparameter tuning.","5bab38d4":"## Ensembling","c554e548":"So this is what the training data looks like after performing all the required cleaning","4eeaa509":"A note on the *eval* function being used.  The *eval()* method parses the expression passed to this method and runs python expression (code) within the program. In simple terms, the *eval()* method runs the python code (which is passed as an argument) within the program.","82dfcb02":"From a first peek at the data, we can spot missing values and features that are JSON objects. Let's dig a bit more into the dataset and its features","1671260e":"So far, we haven't parsed any JSON features. Let's go through the *production companies*, *production countries*, *cast*, *crew*, *keywords*, and *belongs to collection* feature and try and extract information that might help our model","40dd3929":"### Random Forest","d354cd30":"Let's perform a simple linear regression Model using just the budget feature.","98df62b2":"This feature will store 0 for movies that don't have a homepage, and 1 for movies that do.","c8a6a179":"Another issue we will have to take care of is the units of the budget and target variable, revenue. Let's deal with this in the data cleaning section later.","d1e9f044":"1. For features such as cast, crew, production companies, keywords, and production countries, get the length\/size of the feature. \n2. If the movie belongs to a collection, extract the name of the collection, and label encode it.\n3. For movie overview, count the number of words in overview\n4. Feature interactions","3005e43f":"This gives us a Kaggle score of *2.71*, and would place you around the top 75th percenticle of this competition. It is a significant improvement over the default predictions, which led to a Kaggle score of *3.79*. We have barely used any brainpower so this is not bad at all.","dae524b5":"#### Min_Samples_Leaf","dd90c65c":"#### Original Language Feature\nWe will label encode this categorical feature using sklearn. For now, let's do this in the simplest manner and not worry about any smart encoding. We will have to concatenate the train and test set in order before fitting the label encodings","e871ced2":"#### Budget Feature\nFill zero values for the budget feature in train and test data with median of the feature in the train set. We shall use only the train set to calculate median budget. This will avoid data leakage. We will also create another binary feature, *'budget_is_median'* which will hold the value 1 for indices that have been filled with median budget","c34fe5f2":"### Linear Regression","815f1b96":"## Simple Models","5364c67a":"#### Release Data Feature","146fd3a5":"As is visible, the predictions for individual trees are all over the place but their average is fairly reasonable.","5cef2542":"Previously, we set our *max_depth* to 3 to make the tree easy to visualize. Now, let's remove this hyperparameter and observe the difference in evaluation.","f59bb624":"#### Max_Features","55cf9675":"## Reading Data","507f731b":"The train data has less observations than the test data! Challenge accepted","a67855e9":"Here, we will draw a single decision tree that is builds our Random Forest ensemble. We will make a small tree which is easy to visualize. To achieve this, we set *max_depth* to 3. We will also turn *bootstrap* to False in order to sample all of the data and build a deterministic tree.","b2b10d4e":"## Parsing JSON Features","d0af27b5":"### Null and Missing Values","8ab1b72a":"#### Visualizing the imapct of additional Trees","b6477ba5":"### Light GBM","23caa649":"### XGBoost","6ec9f69b":"#### Predictions for Random Forest model","6e92f2fd":"#### Homepage Feature","62ebd7ec":"#### Random Forest with Hyperparameter Tuning","87c8dad4":"## Feature Importances","2ee45289":"Our validation R squared is very low compared to the training R squared, suggesting we are overfitting. Although the validation RMSE will place us in the top 40% of the competition, much better than our naive linear regression model.","7222edc4":"### Visualizations","9fe370c3":"#### Creating a deeper Single Tree","aded6fc9":"#### Functions to evaluate our Random Forest","03fc6dd6":"To prevent overfitting, we will tune min_samples_leaf. This will reduce the depth of our trees by a couple of levels","7bf73fcb":"Get Output files without committing","fa8d0967":"Filling missing data with external ground truth","bb472d8a":"### Target Variable Problems","d4840497":"1. **Target Variable Distribution:** It's never a bad idea to start by plotting the distribution of the target variable. Using np.log1p allows us to plot the several values between 0 and 1 million USD and get a much more uniform distribution.\n2. **Revenue vs. Budget:** A scatterplot to explore the relationship between these two numerical features. We have plenty of zero values for budget\n3. **3D Plot:** A 3D plot to explore the relationship between *Runtime, Popularity, and Revenue.*\n4. **Correlation Matrix:** So revenue and budget are highly correlated. Let's try to predict the target variable with just with this one variable and get a baseline score and leaderboard rank!\n5. **Distribution by Language** The data is dominated by English language films. French, Russian, Spanish, and Hindi films follow.","542b96a9":"So the *Budget* feature has 812 values as 0. The likely cause is that information wasn't collected or available for those observations. It is unlikely that some of the movies had an extremely low budget, close to zero. \n\nWe are immediately faced with a decision. How to deal with these missing values? Dropping more than 25% of the training set is not the best idea, and the *test* set is likely to have the same problem. Therefore, we should should look to fill values for these observations from external sources or the median value for budget. Let's deal with this later.","fd30d69f":"Editing erronous Data in train and test set (based on Discussion forums)","b1678813":"#### Runtime Feature\nWe have some null values for *Runtime* feature. There are also some values that have the value 0. Let's fill them with the median value for runtime from the train set.","4e51dc7a":"1. The distribution of null values across both the train and test set is roughly the same. This is good news!\n2. *Budget* feature has 0 null values. But this doesn't mean that all values of this feature are meaningful. It could simply be due to the common practive of replacing null values with a dummy value like -1, 0, or 999\n\nLet's look at the frequency of unique values held by the *Budget* feature to see if anything out of the ordinary pops up","16283567":"To know if our validation score is worse because our model is overfitting or because the validation set is from a different distribution, or both, we can leverage a hyperparameter called *oob_score*, or Out of Bag score. The idea behind it is to calculate error on the training set while only including those trees in the calculation of a row's error where that row was not included in the training the tree. \n\nIf you have a lot of trees, all of the rows in the dataset should appear a few times in the out of bag samples. This approach is beneficial as we can see if our model generalizes, even if we have a small amount of data. We can avoid creating a separate validation set and lose valuable training data.","15485d7e":"#### Status Feature\nWe will deal with the *Status* categorical feature in the same manner. From our EDA section, we know that there are 2 missing values in the test set. Let's replace these values with the most common occurence of the variable, *'Released'*","6c78825d":"## EDA","8135a588":"#### Peform Train and Validation Split","09215db9":"#### Random Forest with default hyperparameters","ef63035b":"#### Preparing Submission for Linear Regression","b24ec41d":"*Belongs to collection* and *homepage* have a lot of null values. Let's put a **percentage** to these missing values for both the train and test set","c74a2509":"As expected, we have a training R squared of 1. This is because each observation in the training data is a leaf node and can be accounted for easily. Our R squared and validation score on test data, however, has decreased tremendously."}}