{"cell_type":{"f1b6872c":"code","d5b8aadc":"code","186cbf8f":"code","4b77e892":"code","1b280607":"code","db386afc":"code","b4113ba1":"code","9eb325b4":"code","b0b7d936":"code","60e1fbce":"code","61afe1d5":"code","33222fe9":"code","2faad6b2":"code","763839af":"code","79d106fb":"code","b922f7de":"code","4909a19f":"code","679112e6":"code","853f7576":"code","cff254df":"code","f99801bf":"code","0f0b5dc5":"code","6ffeab8c":"code","d8c5aa59":"code","eff9f140":"code","51fe79c5":"code","062ada53":"code","f4e05fdc":"code","c6599a39":"code","95ef3f4e":"markdown","3af4c08e":"markdown","303f7bd1":"markdown","f0b5971a":"markdown","05b7f4e3":"markdown","944e86c1":"markdown","bf0c7be0":"markdown","904f1749":"markdown","772e6f80":"markdown","060e4d2e":"markdown","8068b9b9":"markdown","b4ab6f4d":"markdown","783635bc":"markdown","7d1246f1":"markdown","8351fc24":"markdown","196c5bcc":"markdown","0e5ade4d":"markdown","e0a4ccdd":"markdown","761b241e":"markdown","79accc82":"markdown","2149ac93":"markdown","4b5393b7":"markdown","d3903b4e":"markdown","f884e27d":"markdown","1be1d152":"markdown","da2d25a3":"markdown","f408be00":"markdown","8865e929":"markdown","85c1adea":"markdown","23b47932":"markdown","034f53bf":"markdown"},"source":{"f1b6872c":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras_preprocessing.image import ImageDataGenerator\n\nimport zipfile \n\nimport cv2\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom keras import models\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model","d5b8aadc":"path = '\/kaggle\/input\/challenges-in-representation-learning-facial-expression-recognition-challenge\/'\nos.listdir(path)","186cbf8f":"data = pd.read_csv(path+'icml_face_data.csv')\ndata.columns = ['emotion', 'Usage', 'pixels']      \ntrain = pd.read_csv(path+'train.csv')\ntest = pd.read_csv(path+'test.csv')","4b77e892":"data.head()","1b280607":"train.head()","db386afc":"test.head()","b4113ba1":"data['Usage'].value_counts()","9eb325b4":"def prepare_data(data):\n    image_array = np.zeros(shape=(len(data), 48, 48, 1))\n    image_label = np.array(list(map(int, data['emotion'])))\n\n    for i, row in enumerate(data.index):\n        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n        image = np.reshape(image, (48, 48)) \n        image_array[i, :, :, 0] = image \/ 255\n\n    return image_array, image_label\n\ndef vis_training(hlist, start=1):\n    \n    loss = np.concatenate([h.history['loss'] for h in hlist])\n    val_loss = np.concatenate([h.history['val_loss'] for h in hlist])\n    acc = np.concatenate([h.history['accuracy'] for h in hlist])\n    val_acc = np.concatenate([h.history['val_accuracy'] for h in hlist])\n    \n    epoch_range = range(1,len(loss)+1)\n\n    plt.figure(figsize=[12,6])\n    plt.subplot(1,2,1)\n    plt.plot(epoch_range[start-1:], loss[start-1:], label='Training Loss')\n    plt.plot(epoch_range[start-1:], val_loss[start-1:], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.legend()\n\n    plt.subplot(1,2,2)\n    plt.plot(epoch_range[start-1:], acc[start-1:], label='Training Accuracy')\n    plt.plot(epoch_range[start-1:], val_acc[start-1:], label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend()\n\n    plt.show()","b0b7d936":"emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}","60e1fbce":"full_train_images, full_train_labels = prepare_data(data[data['Usage']=='Training'])\ntest_images, test_labels = prepare_data(data[data['Usage']!='Training'])\n\nprint(full_train_images.shape)\nprint(full_train_labels.shape)\nprint(test_images.shape)\nprint(test_labels.shape)","61afe1d5":"train_images, valid_images, train_labels, valid_labels =\\\n    train_test_split(full_train_images, full_train_labels, test_size=0.2, random_state=1)\n\nprint(train_images.shape)\nprint(valid_images.shape)\nprint(train_labels.shape)\nprint(valid_labels.shape)","33222fe9":"N_train = train_labels.shape[0]\n\nsel = np.random.choice(range(N_train), replace=False, size=16)\n\nX_sel = train_images[sel, :, :, :]\ny_sel = train_labels[sel]\n\nplt.figure(figsize=[12,12])\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow(X_sel[i,:,:,0], cmap='binary_r')\n    plt.title(emotions[y_sel[i]])\n    plt.axis('off')\nplt.show()","2faad6b2":"%%time \n\ncnn = Sequential()\n\ncnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(48,48,1)))\ncnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\ncnn.add(Dropout(0.25))\ncnn.add(BatchNormalization())\n\ncnn.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\ncnn.add(Dropout(0.25))\ncnn.add(BatchNormalization())\n\ncnn.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\ncnn.add(Dropout(0.5))\ncnn.add(BatchNormalization())\n\ncnn.add(Flatten())\n\ncnn.add(Dense(512, activation='relu'))\ncnn.add(Dropout(0.5))\ncnn.add(Dense(512, activation='relu'))\ncnn.add(Dropout(0.5))\ncnn.add(Dense(512, activation='relu'))\ncnn.add(Dropout(0.5))\n\ncnn.add(Dense(7, activation='softmax'))\n\ncnn.summary()","763839af":"%%time \n\nopt = keras.optimizers.Adam(lr=0.001)\ncnn.compile(loss='sparse_categorical_crossentropy',\n                  optimizer=opt, metrics=['accuracy'])","79d106fb":"%%time \n\nh1 = cnn.fit(train_images, train_labels, batch_size=256, epochs=30, verbose=1, \n                   validation_data =(valid_images, valid_labels)) ","b922f7de":"vis_training([h1])","4909a19f":"%%time \nkeras.backend.set_value(cnn.optimizer.learning_rate, 0.00001)\n\nh2 = cnn.fit(train_images, train_labels, batch_size=256, epochs=30, verbose=1, \n                   validation_data =(valid_images, valid_labels)) ","679112e6":"vis_training([h1, h2])","853f7576":"cnn.save('final_model.h5')","cff254df":"test_prob = cnn.predict(test_images)\ntest_pred = np.argmax(test_prob, axis=1)\ntest_accuracy = np.mean(test_pred == test_labels)\n\nprint(test_accuracy)","f99801bf":"conf_mat = confusion_matrix(test_labels, test_pred)\n\npd.DataFrame(conf_mat, columns=emotions.values(), index=emotions.values())","0f0b5dc5":"fig, ax = plot_confusion_matrix(conf_mat=conf_mat,\n                                show_normed=True,\n                                show_absolute=False,\n                                class_names=emotions.values(),\n                                figsize=(8, 8))\nfig.show()","6ffeab8c":"print(classification_report(test_labels, test_pred, target_names=emotions.values()))","d8c5aa59":"class GradCAM:\n    def __init__(self, model, classIdx, layerName=None):\n        self.model = model\n        self.classIdx = classIdx\n        self.layerName = layerName\n        if self.layerName is None:\n            self.layerName = self.find_target_layer()\n            \n    def find_target_layer(self):\n        for layer in reversed(self.model.layers):\n            if len(layer.output_shape) == 4:\n                return layer.name\n        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n        \n    def compute_heatmap(self, image, eps=1e-8):\n        gradModel = Model(\n            inputs=[self.model.inputs],\n            outputs=[self.model.get_layer(self.layerName).output,self.model.output]\n       )\n           \n        with tf.GradientTape() as tape:\n            inputs = tf.cast(image, tf.float32)\n            (convOutputs, predictions) = gradModel(inputs)\n            loss = predictions[:, self.classIdx]\n            grads = tape.gradient(loss, convOutputs)\n\n            castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n            castGrads = tf.cast(grads > 0, \"float32\")\n            guidedGrads = castConvOutputs * castGrads * grads\n            convOutputs = convOutputs[0]\n            guidedGrads = guidedGrads[0]\n\n            weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n            cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n\n            (w, h) = (image.shape[2], image.shape[1])\n            heatmap = cv2.resize(cam.numpy(), (w, h))\n            numer = heatmap - np.min(heatmap)\n            denom = (heatmap.max() - heatmap.min()) + eps\n            heatmap = numer \/ denom\n            heatmap = (heatmap * 255).astype(\"uint8\")\n        return heatmap\n\n    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n        colormap = cv2.COLORMAP_VIRIDIS):\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n        return (heatmap, output)","eff9f140":"plt.figure(figsize=[16,16])\nfor i in range(36):\n    img = test_images[i,:,:,0]\n    p_dist = cnn.predict(img.reshape(1,48,48,1))\n    k = np.argmax(p_dist)\n    p = np.max(p_dist)\n\n    cam = GradCAM(cnn, k)\n    heatmap = cam.compute_heatmap(img.reshape(1,48,48,1))\n\n    plt.subplot(6,6,i+1)\n    plt.imshow(img, cmap='binary_r')\n    plt.imshow(heatmap, alpha=0.5, cmap='coolwarm')\n    plt.title(f'{emotions[test_labels[i]]} - ({emotions[k]} - {p:.4f})')\n    plt.axis('off')\nplt.tight_layout()\nplt.show()","51fe79c5":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nimport cv2\n\nclass GradCAM:\n    def __init__(self, model, classIdx, layerName=None):\n        self.model = model\n        self.classIdx = classIdx\n        self.layerName = layerName\n        if self.layerName is None:\n            self.layerName = self.find_target_layer()\n            \n    def find_target_layer(self):\n        for layer in reversed(self.model.layers):\n            if len(layer.output_shape) == 4:\n                return layer.name\n        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n        \n    def compute_heatmap(self, image, eps=1e-8):\n        gradModel = Model(\n            inputs=[self.model.inputs],\n            outputs=[self.model.get_layer(self.layerName).output,self.model.output]\n       )\n           \n        with tf.GradientTape() as tape:\n            inputs = tf.cast(image, tf.float32)\n            (convOutputs, predictions) = gradModel(inputs)\n            loss = predictions[:, self.classIdx]\n            grads = tape.gradient(loss, convOutputs)\n\n            castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n            castGrads = tf.cast(grads > 0, \"float32\")\n            guidedGrads = castConvOutputs * castGrads * grads\n            convOutputs = convOutputs[0]\n            guidedGrads = guidedGrads[0]\n\n            weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n            cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n\n            (w, h) = (image.shape[2], image.shape[1])\n            heatmap = cv2.resize(cam.numpy(), (w, h))\n            numer = heatmap - np.min(heatmap)\n            denom = (heatmap.max() - heatmap.min()) + eps\n            heatmap = numer \/ denom\n            heatmap = (heatmap * 255).astype(\"uint8\")\n        return heatmap\n\n    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n        colormap = cv2.COLORMAP_VIRIDIS):\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n        return (heatmap, output)","062ada53":"n = 30\nimg = test_images[n,:,:,0]\ncam = GradCAM(cnn, 3)\nheatmap = cam.compute_heatmap(img.reshape(1,48,48,1))\nplt.imshow(img, cmap='binary_r')\nplt.imshow(heatmap, alpha=0.5, cmap='RdBu_r')\nplt.show()","f4e05fdc":"test_prob = cnn.predict(test_images)\ntest_pred = np.argmax(test_prob, axis=1)\n\nsel_imgs = [10, 15, 26, 12, 64, 14, 9]\n\nfor n in sel_imgs:\n    img = test_images[n,:,:,0]\n    \n    plt.figure(figsize=[10,3])\n    plt.subplot(1, 3, 1)\n    plt.imshow(img, cmap='binary_r')\n    plt.title(f'True Label: {emotions[test_labels[n]]}')\n    plt.axis('off')\n    \n    cam = GradCAM(cnn, test_pred[n])\n    heatmap = cam.compute_heatmap(img.reshape(1,48,48,1))\n    \n    plt.subplot(1, 3, 2)\n    plt.imshow(img, cmap='binary_r')\n    plt.imshow(heatmap, alpha=0.5, cmap='RdBu_r')\n    plt.title(f'Predicted Label: {emotions[test_pred[n]]}')\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 3)\n    plt.bar(emotions.values(), test_prob[n, :], color='steelblue', edgecolor='k')\n    plt.xticks(rotation=45)\n    plt.ylim([0,1])\n    plt.title('Distribution of Predictions')\n    plt.show()","c6599a39":"ranked_pred = np.argsort(test_prob, axis=1)\nfor k in range(7):\n    correct = test_labels.reshape(-1,1) == ranked_pred[:, -(k+1):]\n    top_k_acc = np.sum(correct) \/ len(test_labels)\n    print(f'Top {k} Accuracy: {top_k_acc}')","95ef3f4e":"We save our model so that it will be easier for us to access it if we need it.","3af4c08e":"This is the first way we will use to evaluate our model. The confusion matrix is a table where every column represents teh predicted label and the rows represent the true label. ","303f7bd1":"## 4.2 Classification report ","f0b5971a":"## 4.4 Top \"K\" accuracy","05b7f4e3":"# 4. Evaluating the model ","944e86c1":"Now that our model is build and trained we will generate test prediction. \nWe see that our best accuracy is 0.66 or 66%. ","bf0c7be0":"The first we do is load all the libraries we will need for this project.","904f1749":"## 4.1 - Generate Test Predictions and Calculating Accuracy","772e6f80":"In the next cells we will see the data we are going to work with and will work on formating it. ","060e4d2e":"## 4.3 Class Activation Maps","8068b9b9":"# 3.CNN Model","b4ab6f4d":"## 4.2 Confusion matrix","783635bc":" # 2. Data","7d1246f1":"Defining train and test data. ","8351fc24":"## 3.3 Save Model","196c5bcc":"## 2.2 Prepare the data","0e5ade4d":"Our data is composed by the training set and the test set. The data consists of 28700  images of faces.  On the training set we are given the emotion it represents with the according number and the picture in the format of 48x48 pixel grayscale image. \nThe test set on the other hand contains only the pixel value. Our job will be to predict what emotions they present. ","e0a4ccdd":"This metric computes the number of times where the correct label is among the top k labels predicted (ranked by predicted scores). So by Top 0 accuracy we understand how many times the model was succesful in predicting the right label in the first try. Top 1 accuracy shows us the number of times the correct label was  one of the two predection the model made, end so on. ","761b241e":"## 2.1 Load the Data","79accc82":"## 2.3 Display Sample of Images ","2149ac93":"The task will be to categorize each of the images into one the the below categories. ","4b5393b7":"After we write our model, we will need to train it. Below are two training run. After each run we also include a visualization of their accuracy that helps us see how our model is doing. We notice that in the second run our accuracy is higher and overfiting gets lower. ","d3903b4e":"The faces have been automatically registered so that the face  is more or less centered and occupies about the same amount of space in each image. When we run the code below it presents us with different images every time. ","f884e27d":"## 3.1 Training Run 1","1be1d152":"So we see that data is the sum of all the Train set, test set (private test and public test). ","da2d25a3":"The second method we use to evaluate our model is the class activation map. This is a simple technique that shows the image region used by the CNN to identify the specific class. In this case the region is shown in a red color to highlight the area the model used ot predict the feeling. ","f408be00":"> **Helping functions**","8865e929":"After our dataset is loaded and we have used the helping functions to clean it and formatted it, the next step is to build our model. ","85c1adea":"# Facial expression recognition \n## Aliaj, Marsela\nLearning the facial expression from an image using a CNN(convolutional neural network) model.","23b47932":"## 3.2 Training Run 2","034f53bf":"# 1. Load Libraries"}}