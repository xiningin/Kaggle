{"cell_type":{"0bcb49a2":"code","487681b5":"code","d332268a":"code","64653679":"code","d9282cbb":"code","6dd9078b":"code","a7775494":"code","a2781b47":"code","d658098b":"code","fc8b2ac7":"code","65f56d69":"code","23dbeee3":"code","cd74eb18":"code","e40877ea":"code","d6d0a6d8":"code","cc374fc1":"code","7500cfa9":"code","8b893e45":"code","fdf56f01":"code","37c31250":"code","f6272791":"code","c915bc61":"code","055bb2fc":"code","2ec89647":"code","9b38acf3":"code","bdb93fff":"code","fceef518":"code","5e3c3f8f":"code","36bf8578":"code","4bc6e287":"code","eb51497b":"code","1b93d28b":"code","bf12366a":"code","7c6142f6":"code","cac5540d":"code","0eddcb6a":"code","3ff860fa":"code","55fe73ef":"code","0c536f90":"code","c4ffda29":"code","a8fdb69d":"code","1f8645a9":"code","a4932a60":"code","fc0f6d4e":"code","75835f46":"code","bcf572f5":"code","1c81fb04":"code","11a99898":"code","24a8cdd0":"code","ba641263":"code","1f5c2f2d":"code","43aaca68":"code","aa37000a":"code","75047b4d":"code","f29b07e3":"code","854cdd4d":"code","328a3922":"code","b9a1ed96":"code","159e2d8a":"code","717940c7":"code","8fa2ff32":"code","242bb507":"code","3ac541e1":"code","264f7f52":"code","9630170d":"code","71db600f":"code","62d029b7":"markdown","8a9f9ee5":"markdown"},"source":{"0bcb49a2":"import csv\n#from apiclient.discovery import build\n#DEVELOPER_KEY = 'API KEY'\n#youtube = build('youtube', 'v3', developerKey=DEVELOPER_KEY)","487681b5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","d332268a":"import re \nimport nltk \nfrom nltk.corpus import stopwords \nfrom nltk.stem import WordNetLemmatizer ","64653679":"def clean_data(col):\n    try:\n        col = re.sub('[^a-zA-Z]', ' ',col) \n        col = col.lower()            \n        col = col.split()            \n        lemmatizer = WordNetLemmatizer()             \n        col = [lemmatizer.lemmatize(word) for word in col if not word in set(stopwords.words('english'))]            \n        col = ' '.join(col)\n    except Exception as e:\n        print('col- ',col)\n        print(e)\n        col = 'Error'\n    return(col)\n    \n#data['Title'] = data['Title'].apply(clean_data)","d9282cbb":"list_title = []\nlist_description = []\ndef get_list(cols):\n    list_title.append(cols[0])\n    list_description.append(cols[1])","6dd9078b":"#below is the code to scrap youtube videos\n'''\n#using youtube api to get to the list of videos\nfrom apiclient.discovery import build\nfrom apiclient.errors import HttpError\n\n\nDEVELOPER_KEY = \"API KEY\"\nYOUTUBE_API_SERVICE_NAME = \"youtube\"\nYOUTUBE_API_VERSION = \"v3\"\n\n\n\n\ndef youtube_search(q, max_results=50,order=\"relevance\", token=None, location=None, location_radius=None):\n    \n    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n                            developerKey=DEVELOPER_KEY)\n\n    search_response = youtube.search().list(\n        q=q,\n        type=\"video\",\n        pageToken=token,\n        order = order,\n        part=\"id,snippet\",\n        maxResults=max_results,\n        location=location,\n        locationRadius=location_radius\n        ).execute()\n\n\n\n    videos = []\n\n    for search_result in search_response.get(\"items\", []):\n        \n        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n                        \n            videos.append(search_result)\n    \n    try:\n        #token is used to jump to the next page on youtube to search for more videos\n        nexttok = search_response[\"nextPageToken\"]\n        return(nexttok, videos)\n    except Exception as e:\n        nexttok = \"last_page\"\n        return(nexttok, videos)\n'''","a7775494":"#test = youtube_search(\"spinners\")","a2781b47":"#print(test[1][2])","d658098b":"#token = test[0]\n#test_2 = youtube_search(\"spinners\", token=token)","fc8b2ac7":"#making a csv file and writing a header\n'''csv_file = open('YouTube Titles and description using youtube api.csv','w',encoding=\"utf-8\",newline='')\ncsv_writer = csv.writer(csv_file)\ncsv_writer.writerow(['Video Id','Title', 'Description','Category'])'''","65f56d69":"'''\n#the idea was to get 1700 videos but because there is limit to free youtube api queries, I was bound to stop at 200 videos\ndef get_1700vid(list_of_strings):\n    for string in list_of_strings:\n        counter=0\n        category = string\n        token = None\n        while((token!=\"last_page\") & (counter<=200)):\n            result = youtube_search(string,token=token)\n            token = result[0]\n            for i in range(0,len(result[1])):\n                counter+=1\n                title = result[1][i]['snippet']['title']\n                vid_id = result[1][i]['id']['videoId']\n                videos = youtube.videos().list(id=vid_id, part='snippet').execute()\n                for video in videos.get('items', []):\n                    description = video['snippet']['description']\n                print(vid_id,'-',title,'-',category,'-',counter)\n                csv_writer.writerow([vid_id,title, description,category])\n    return    \n'''","23dbeee3":"#list_of_topics=['travel','science','food','manufacturing','history','music']\n#get_1700vid(list_of_topics)","cd74eb18":"#csv_file.close()","e40877ea":"#this is the csv file which was populated with the scraped youtuble videos\ndf = pd.read_csv('..\/input\/YouTube Titles and description using youtube api.csv')","d6d0a6d8":"df.info()","cc374fc1":"df.head()","7500cfa9":"df.isnull().values.any()","8b893e45":"null_columns=df.columns[df.isnull().any()]\ndf[null_columns].isnull().sum()","fdf56f01":"#wherever there is null description, I've used the title to replace it. This was the most appropriate way I could think of\ndf['Description'] = df.apply(lambda row: row['Title'] if pd.isnull(row['Description']) else row['Description'],axis=1)","37c31250":"df.isnull().values.any()","f6272791":"df['Title'] = df['Title'].apply(clean_data)","c915bc61":"df.head()","055bb2fc":"df['Description'] = df['Description'].apply(clean_data)","2ec89647":"df.head()","9b38acf3":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","bdb93fff":"le_cat = le.fit_transform(df['Category'])","fceef518":"series = pd.Series(le_cat)","5e3c3f8f":"df = pd.concat([df,series],axis=1)","36bf8578":"df.head()","4bc6e287":"df.columns=['Video_id','Title','Description','Category','category']","eb51497b":"df = df.drop('Category',axis=1)","1b93d28b":"df.head()","bf12366a":"#to get a list of title and description to be used for bag of words later\ndf[['Title','Description']].apply(get_list,axis=1)","7c6142f6":"len(list_description)","cac5540d":"from sklearn.feature_extraction.text import CountVectorizer   \ncv = CountVectorizer(max_features = 1500) \nX = cv.fit_transform(list_description, list_title).toarray() \ny = df['category'].values","0eddcb6a":"X.shape","3ff860fa":"from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer().fit_transform(X)","55fe73ef":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(tfidf_transformer, y, test_size = 0.20, random_state = 0)","0c536f90":"#Used RFC\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 1000, criterion = 'entropy')\nclassifier.fit(X_train, y_train)","c4ffda29":"y_pred = classifier.predict(X_test)\nclassifier.score(X_test, y_test)","a8fdb69d":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","1f8645a9":"#sing SVC\nfrom sklearn.svm import SVC","a4932a60":"param_grid = {'C':[0.1,1,10,100,1000],'gamma':[1,0.1,0.01,0.001,0.0001]}","fc0f6d4e":"from sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(SVC(),param_grid,verbose=3)","75835f46":"grid.fit(X_train,y_train)","bcf572f5":"grid.best_params_","1c81fb04":"grid.best_estimator_","11a99898":"grid_predictions = grid.predict(X_test)","24a8cdd0":"print(classification_report(y_test,grid_predictions))","ba641263":"#using logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression()","1f5c2f2d":"log_model.fit(X_train,y_train)","43aaca68":"predictions_log = log_model.predict(X_test)","aa37000a":"print(classification_report(y_test,predictions_log))","75047b4d":"X_train.shape","f29b07e3":"#using neural network\nimport tensorflow as tf\nfrom tensorflow import keras","854cdd4d":"#building the model\nmodel = keras.Sequential([\n    keras.layers.Dense(128,input_shape=(1500,),activation=tf.nn.relu),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(128,activation=tf.nn.relu),\n    keras.layers.Dense(6, activation=tf.nn.softmax)\n])\n\nmodel.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","328a3922":"model.fit(X_train,y_train,epochs=10)","b9a1ed96":"test_loss, test_acc = model.evaluate(X_test, y_test)\n\nprint('Test accuracy:', test_acc)","159e2d8a":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn import tree\nmodel = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\nmodel.fit(X_train, y_train)\nbagging_pred = model.predict(X_test)","717940c7":"print(classification_report(y_test,bagging_pred))","8fa2ff32":"from sklearn.ensemble import AdaBoostClassifier\nmodel = AdaBoostClassifier(random_state=1)\nmodel.fit(X_train, y_train)\nmodel.score(X_test,y_test)","242bb507":"boost_pred = model.predict(X_test)","3ac541e1":"print(classification_report(y_test,boost_pred))","264f7f52":"from sklearn.ensemble import GradientBoostingClassifier\nmodel= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\nmodel.fit(X_train, y_train)\nmodel.score(X_test,y_test)","9630170d":"grad_boost_pred = model.predict(X_test)","71db600f":"print(classification_report(y_test,grad_boost_pred))","62d029b7":"SVM proves to be the most superior of them. ","8a9f9ee5":"Bagging algo proves to be better than Boosting. But overall, SVM dominates all of them."}}