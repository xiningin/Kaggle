{"cell_type":{"8af8e682":"code","211830e6":"code","13f1f892":"code","72a1ae28":"code","254a5fa3":"code","a7df925d":"code","c82fd607":"code","afc88243":"code","c4be09e4":"code","8d79ebdd":"code","ed3fa340":"code","5836d833":"code","af88d6cc":"code","225a4303":"code","100b740c":"code","f5e43f21":"code","a50186b8":"code","b47dfe87":"code","080aece0":"code","82223355":"markdown","c1e8680e":"markdown","4ce65bcf":"markdown","2c60de62":"markdown","0aa3bda2":"markdown","4137f42b":"markdown","30ae243d":"markdown"},"source":{"8af8e682":"# Install sentence-transformers\n!pip install sentence-transformers","211830e6":"import os\nimport re \nimport gc\nimport glob\nimport json\nimport scipy\nimport torch\nimport pickle\nimport string\nimport numpy as np\nimport pandas as pd\nfrom time import time  \nfrom tqdm import tqdm\nfrom PIL import Image\nfrom sklearn import cluster\nfrom typing import List, Dict\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom sklearn.manifold import TSNE\nfrom transformers import pipeline\nfrom sklearn.decomposition import PCA\nfrom transformers import AutoTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoModelForQuestionAnswering\n\n\n# Constanst defenition\nPCA_NUM_COMPONENT             = 20\nNUMPY_SEED                    = 42\nMAX_WORDS                     = 3000 \nSAMPLES_TNSE                  = 10000\nCORONVAVIRUS_PATTERN          = 'CORONA'\nNUM_CLUSTERS                  = [5,10,15]\nCORRECT_CLASS                 = 'correct'\nINCORRECT_CLASS               = 'incorrect'\nCOVID19_PICTURE_PATH          = r'\/kaggle\/input\/images\/covid19_pic.png'\nCLUSTRING_INFO_PATH           = r'\/kaggle\/working\/clusters_info.pkl'\nCLUSTERS_DICT_PATH            = r'\/kaggle\/working\/clusters_dict.pkl'\nROOT_PATH                     = r'\/kaggle\/input\/CORD-19-research-challenge'\nMATRIX_ABSTRACT_PATH          = r'\/kaggle\/working\/abstract_emb_arr.npy'\nARTICLES_ABS_PATH             = r'\/kaggle\/working\/articles_abstract.pkl'\nIMAGE_CLUD_PATH_REGULAR       = r'\/kaggle\/working\/image_cloud.png'\nANNOT_ANSWER1_PATH            = r'\/kaggle\/input\/manually-annotated-q\/Q1_annotated.pkl'\nANNOT_ANSWER2_PATH            = r'\/kaggle\/input\/manually-annotated-q\/Q2_annotated.pkl'\nANNOT_ANSWER3_PATH            = r'\/kaggle\/input\/manually-annotated-q\/Q3_annotated.pkl'\nANNOT_ANSWER4_PATH            = r'\/kaggle\/input\/manually-annotated-q\/Q4_annotated.pkl'\nMETADATA_PATH                 = r'\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv'\nARTICLES_ABS_PLUS_ENCODE_PATH = r'\/kaggle\/working\/articles_abstract_plus_encode.pkl'\nIMAGE_CLUD_PATH_CORONA_MASK   = r'\/kaggle\/working\/wc_cluster_{}_class_{}.png'\nANSWER2_PATH                  = r'\/kaggle\/working\/2_What_works_have_been_done_on_infection_spreading.csv'\nANSWER1_PATH                  = r'\/kaggle\/working\/1_Are_theregeographic_variations_in_the_rate_of_COVID-19_spread.csv'\nANSWER3_PATH                  = r'\/kaggle\/working\/3_Are_there_geographic_variations_in_the_mortality_rate_of_COVID.CSV'\nANSWER4_PATH                  = r'\/kaggle\/working\/4_Is_there_any_evidence_to_suggest_geographic_based_virus_mutations.csv'\n\n\n\n# Remove uninformative words\nREMOVE_TERMS                  = ['covid19','corona','coronavirus','covid','covid','results','result','result','one','two','use','used','using','well', \\\n                                 'including','found','although','method','conclusion','case','observed','model','showed','due','three', 'cases','system',\\\n                                 'among','known','case','type','time','total','different','activity','based','level','effect','thu','within','may','compared',\\\n                                 'show','many','few','et al','following','similar','data','methods','detected','approach','change','multiple','several',\\\n                                 'detected','first','specific','increase','response','include','need','report','suggest','identified','finding','addition']\n\n\n\n%matplotlib inline","13f1f892":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            if 'abstract' in content:\n                for entry in content['abstract']:\n                    self.abstract.append(entry['text'])\n            else:\n                self.abstract.append('Not provided.')\n            # Body text\n            if 'body_text' in content:\n                for entry in content['body_text']:\n                    self.body_text.append(entry['text'])\n            else:\n                self.body_text.append('Not provided.')\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n\n\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n    \ndef get_date_dt(all_json, meta_df):    \n    dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [],\n             'abstract_summary': []}\n    for idx, entry in tqdm(enumerate(all_json), desc=\"Parsing the articles Json's content\", total=len(all_json)):\n        try:\n            content = FileReader(entry)\n        except:\n            continue\n\n        # get metadata information\n        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n        # no metadata, skip this paper\n        if len(meta_data) == 0:\n            continue\n\n        dict_['paper_id'].append(content.paper_id)\n        dict_['abstract'].append(content.abstract)\n        dict_['body_text'].append(content.body_text)\n        \n        # get metadata information\n        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n\n        try:\n            # if more than one author\n            authors = meta_data['authors'].values[0].split(';')\n            if len(authors) > 2:\n                # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n                dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n            else:\n                # authors will fit in plot\n                dict_['authors'].append(\". \".join(authors))\n        except Exception as e:\n            # if only one author - or Null valie\n            dict_['authors'].append(meta_data['authors'].values[0])\n\n        # add the title information, add breaks when needed\n        dict_['title'].append(meta_data['title'].values[0])\n\n        # add the journal information\n        dict_['journal'].append(meta_data['journal'].values[0])\n        \n\n    return pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal'])","72a1ae28":"# The following code needed to be run only once(!)\n# Get all the data of json articles\nmeta_df = pd.read_csv(METADATA_PATH, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str,\n    'doi': str\n})\n\nall_json = glob.glob(f'{ROOT_PATH}\/**\/*.json', recursive=True)\ndata_dt = get_date_dt(all_json, meta_df)\n\n\n# Encode the articles accorfing to their abstract\nencoder = SentenceTransformer(\"roberta-large-nli-stsb-mean-tokens\")\narticles_abstract = data_dt\nencoded_articles_abstract = encoder.encode(articles_abstract['abstract'].tolist())\narticles_abstract['encoded_articles_abstract'] = encoded_articles_abstract   \nwith open(ARTICLES_ABS_PLUS_ENCODE_PATH, 'wb') as handle:\n    pickle.dump(articles_abstract, handle)","254a5fa3":"def query_questions(query, encoded_query, articles_abstract):    \n    articles_abstract['distances'] = scipy.spatial.distance.cdist([encoded_query], articles_abstract['encoded_articles_abstract'].tolist(), \"cosine\")[0]\n    articles_abstract = articles_abstract.sort_values('distances').reset_index()[:70]\n    \n    articles_abstract['sentence_list'] = [body.split(\". \") for body in articles_abstract['body_text'].to_list()] \n    paragraphs = []\n    for index, ra in articles_abstract.iterrows():\n        para_to_add = [\". \".join(ra['sentence_list'][n:n+7]) for n in range(0, len(ra['sentence_list']), 7)]        \n        para_to_add.append(ra['abstract'])\n        paragraphs.append(para_to_add)\n    articles_abstract['paragraphs'] = paragraphs\n    answers = answer_question(query, articles_abstract)\n    return answers\n\ndef get_QA_bert_model():\n    torch_device = 0 if torch.cuda.is_available() else -1\n    tokenizer = AutoTokenizer.from_pretrained(\"ahotrod\/albert_xxlargev1_squad2_512\")\n    model = AutoModelForQuestionAnswering.from_pretrained(\"ahotrod\/albert_xxlargev1_squad2_512\")\n    return pipeline('question-answering', model=model, tokenizer=tokenizer,device=torch_device)\n\ndef answer_question(question: str, context_list):\n    # anser question given question and context\n    answers =[]\n    all_para = [item for sublist in context_list['paragraphs'].to_list() for item in sublist] \n    print(f\"paragraph to scan: {len(all_para)}\")\n    for _, article in context_list.iterrows():\n        for context in article['paragraphs']:\n            if len(context) < 10:\n                continue\n            with torch.no_grad():\n                answer = nlp_qa(question=question, context=context)\n            answer['paragraph'] = context\n            answer['paper_id'] = article['paper_id']\n            answers.append(answer)            \n    df = pd.DataFrame(answers)\n    df = df.sort_values(by='score', ascending=False)\n    return df","a7df925d":"nlp_qa = get_QA_bert_model()\nquestions = [\"Are there geographic variations in the rate of COVID-19 spread?\",\\\n                                \"What works have been done on infection spreading?\",\\\n                                \"Are there geographic variations in the mortality rate of COVID-19?\",\\\n                                \"Is there any evidence to suggest geographic based virus mutations?\"]\nencoded_query = encoder.encode(questions)\n\ndel encoder\ngc.collect()","c82fd607":"answer1 = query_questions(questions[0],encoded_query[0], articles_abstract)\nanswer1.to_csv(ANSWER1_PATH)\nanswer1.head()","afc88243":"answer2 = query_questions(questions[1], encoded_query[1], articles_abstract)\nanswer2.to_csv(ANSWER2_PATH)\nanswer2.head()","c4be09e4":"answer3 =  query_questions(questions[2], encoded_query[2], articles_abstract)\nanswer3.to_csv(ANSWER3_PATH)\nanswer3.head()","8d79ebdd":"answer4 =  query_questions(questions[3], encoded_query[3], articles_abstract)\nanswer4.to_csv(ANSWER4_PATH)\nanswer4.head()","ed3fa340":"del articles_abstract\ngc.collect()","5836d833":"# Load the annotations\ndef load_annotations(path):\n    with open(path, 'rb') as handle:\n        annotations = pickle.load(handle)\n    \n    return annotations\n\n# Only the correct an incorrect classification account in the measurement\ndef get_only_correct_incorrect_data(annotations):\n    clean_annotations   = []\n    \n    for annot in annotations:\n        if annot[-1] in [CORRECT_CLASS,INCORRECT_CLASS]:\n            clean_annotations.append(annot[-1])\n            \n    return clean_annotations\n\n# Calculate the mean-average-precision\n# Wilipedia: https:\/\/en.wikipedia.org\/wiki\/Evaluation_measures_(information_retrieval)#Mean_average_precision\ndef calc_average_precision(annot_list):\n    \n    average_precision = 0\n    correct_annot = 0\n    total_relevat = len(annot_list)\n    \n    for ii, annot in enumerate(annot_list,1):\n        if annot == CORRECT_CLASS:\n            correct_annot += 1.0\n            average_precision += correct_annot\/ii\n    \n    return average_precision\/total_relevat\n\n\ndef calc_precision_among_1(q_list):\n    correct_annot = 0\n    q_checked_list = q_list[:1]\n\n    for q_i in q_checked_list:\n        if q_i == CORRECT_CLASS:\n            correct_annot += 1.0\n    \n    return correct_annot\/1\n\ndef calc_precision_among_3(q_list):\n    correct_annot = 0\n    q_checked_list = q_list[:3]\n\n    for q_i in q_checked_list:\n        if q_i == CORRECT_CLASS:\n            correct_annot += 1.0\n    \n    return correct_annot\/3\n\ndef calc_precision_among_15(q_list):\n    correct_annot = 0\n    q_checked_list = q_list[:15]\n\n    for q_i in q_checked_list:\n        if q_i == CORRECT_CLASS:\n            correct_annot += 1.0\n    \n    return correct_annot\/15\n\ndef At_least_one_of_3_correct(q_list):\n    correct_annot = 0.0\n    q_checked_list = q_list[:3]\n\n    for q_i in q_checked_list:\n        if q_i == CORRECT_CLASS:\n            correct_annot = 1.0\n    \n    return correct_annot","af88d6cc":"# Load annotated results\nq1_annot = load_annotations(ANNOT_ANSWER1_PATH)\nq2_annot = load_annotations(ANNOT_ANSWER2_PATH)\nq3_annot = load_annotations(ANNOT_ANSWER3_PATH)\nq4_annot = load_annotations(ANNOT_ANSWER4_PATH)\n\n# Clean the data\nq1_annot_clean = get_only_correct_incorrect_data(q1_annot)\nq2_annot_clean = get_only_correct_incorrect_data(q2_annot)\nq3_annot_clean = get_only_correct_incorrect_data(q3_annot)\nq4_annot_clean = get_only_correct_incorrect_data(q4_annot)\n\n# Calculate average precision\nap1 = calc_average_precision(q1_annot_clean)\nap2 = calc_average_precision(q2_annot_clean)\nap3 = calc_average_precision(q3_annot_clean)\nap4 = calc_average_precision(q4_annot_clean)\n\n# Calculate precison among the first results\np1_1 = calc_precision_among_1(q1_annot_clean)\np1_2 = calc_precision_among_1(q2_annot_clean)\np1_3 = calc_precision_among_1(q3_annot_clean)\np1_4 = calc_precision_among_1(q4_annot_clean)\n\n# Calculate precison among the 3 first results\np3_1 = calc_precision_among_3(q1_annot_clean)\np3_2 = calc_precision_among_3(q2_annot_clean)\np3_3 = calc_precision_among_3(q3_annot_clean)\np3_4 = calc_precision_among_3(q4_annot_clean)\n\n# Calculate precison among the 3 first results\np15_1 = calc_precision_among_15(q1_annot_clean)\np15_2 = calc_precision_among_15(q2_annot_clean)\np15_3 = calc_precision_among_15(q3_annot_clean)\np15_4 = calc_precision_among_15(q4_annot_clean)\n\n# At least one of the first three was correct\np_any3_1 = At_least_one_of_3_correct(q1_annot_clean)\np_any3_2 = At_least_one_of_3_correct(q2_annot_clean)\np_any3_3 = At_least_one_of_3_correct(q3_annot_clean)\np_any3_4 = At_least_one_of_3_correct(q4_annot_clean)\n\nprint(f'Mean average precision: {(ap1+ap2+ap3+ap4)\/4}')\nprint(f'Precision of the first results: {(p1_1+p1_2+p1_3+p1_4)\/4}')\nprint(f'Precision of the first 3 results: {(p3_1+p3_2+p3_3+p3_4)\/4}')\nprint(f'Precision that at least one of the first three was correct: {(p_any3_1+p_any3_2+p_any3_3+p_any3_4)\/4}')","225a4303":"# Load the annotations\ndef load_annotations(path):\n    with open(path, 'rb') as handle:\n        annotations = pickle.load(handle)\n    \n    return annotations\n\n# Cleaning the data\ndef data_clean(text, remove_terms_list):\n    \n    # split into words\n    tokens = word_tokenize(text)\n\n    # convert to lower case\n    tokens = [w.lower() for w in tokens]\n\n    # remove punctuation from each word\n    table = str.maketrans('', '', string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n\n    # remove remaining tokens that are not alphabetic\n    words = [word for word in stripped if word.isalpha()]\n\n    # filter stop words\n    stop_words = set(stopwords.words('english'))\n    words = [w for w in words if not w in stop_words]\n    \n    # Remove the terms that choosen to remove\n    words = [w for w in words if not w in remove_terms_list]\n    \n    # Glue it back to text\n    text = \" \".join(word for word in words)\n\n    return(text)\n\ndef draw_word_cloud(text, save_cloud_loc, pattern = None, mask_path=None):\n    if pattern == CORONVAVIRUS_PATTERN:\n        # Load mask\n        covid19_mask = np.array(Image.open(mask_path))\n\n        # Make the word cloud\n        wc = WordCloud(background_color=\"white\", max_words=MAX_WORDS, mask=covid19_mask, contour_width=1).generate(text)\n        \n        # Save image\n#         wc.to_file(save_cloud_loc)\n    else:\n        \n        # Make the word cloud\n        wc = WordCloud(background_color=\"white\", max_words=MAX_WORDS).generate(text)\n        \n        # Save image\n#         wc.to_file(save_cloud_loc)\n\n\ndef create_matrix_encoded_abstract():\n\n    with open(ARTICLES_ABS_PLUS_ENCODE_PATH, 'rb') as fp:\n        articles_abstract_plus_encode = pickle.load(fp)\n        \n    filtered_abstract = only_exists_abstract(articles_abstract_plus_encode)\n    X= filtered_abstract['encoded_articles_abstract'].values\n\n    tmp = np.expand_dims(np.asarray(X[0]),axis =1).T\n    for ii,i in enumerate(X[1:]):\n\n        if len(i.shape) == 1:\n            t = np.expand_dims(i,axis =1).T\n        else:\n            t = i.T\n        tmp = np.append(tmp,t,axis=0)\n    return (tmp)\n#     # Save the output\n#     with open(MATRIX_ABSTRACT_PATH, 'wb') as f:\n#         np.save(f, tmp)\n        \n        \ndef clustering(clusters_number, data_for_clustering):\n    clusters_info = {}\n    for cluster_num in clusters_number:\n        kmeans = cluster.KMeans(n_clusters=cluster_num)\n        kmeans.fit(data_for_clustering)\n        \n        # Record result\n        clusters_info[cluster_num] = {'labels': kmeans.labels_, 'cluster_centers': kmeans.cluster_centers_}\n        print (f'Number of clusters: {cluster_num}. Score of negative square distances: {kmeans.score(data_for_clustering)}')\n    \n    return clusters_info\n\ndef only_exists_abstract(df):\n    return(df.loc[df['abstract'] != ''])","100b740c":"## Create matrix out of the embedded values in the encoded abstract(needed to be run only once)\ndata_for_clustering = create_matrix_encoded_abstract()\n\n\n# # Load the matrix\n# with open(MATRIX_ABSTRACT_PATH, 'rb') as f:\n#     data_for_clustering = np.load(f)\n    \n## Create the clustring data, that we want to examine(should be run only once)\nclusters_info = clustering(NUM_CLUSTERS, data_for_clustering)\n# with open(CLUSTRING_INFO_PATH, 'wb') as handle:\n#     pickle.dump(clusters_info, handle)\n    \n# # Load calculated clustring data\n# with open(CLUSTRING_INFO_PATH, 'rb') as handle:\n#     clusters_info = pickle.load(handle)\n    \n# create datafram,e from the matrix\nfeat_cols = [ 'emb'+str(i) for i in range(data_for_clustering.shape[1]) ]\ndf = pd.DataFrame(data_for_clustering,columns=feat_cols)\nnp.random.seed(NUMPY_SEED)\n\n# create premutation for the selected data\nrndperm = np.random.permutation(df.shape[0])","f5e43f21":"# Calculate PCA and T-SNE\n# pca\npca = PCA(n_components=3)\npca_result = pca.fit_transform(df[feat_cols].values)\ndf['pca-one'] = pca_result[:,0]\ndf['pca-two'] = pca_result[:,1] \ndf['pca-three'] = pca_result[:,2]\nprint('Principal components toatl explained variations: {}'.format(sum(pca.explained_variance_ratio_)))\n\n# pca reduction and then t-sne(because of costly time computation)\ndf_subset = df.loc[rndperm[:SAMPLES_TNSE],:].copy()\ndata_subset = df_subset[feat_cols].values\n\npca_reduced = PCA(n_components=PCA_NUM_COMPONENT)\npca_result_reduced = pca_reduced.fit_transform(data_subset)\nprint('Cumulative explained variation for 50 principal components: {}'.format(sum(pca_reduced.explained_variance_ratio_)))\n\ntsne = TSNE(n_components=3, verbose=3, perplexity=40, n_iter=300)\ntsne_pca_results = tsne.fit_transform(pca_result_reduced)","a50186b8":"# Plot results\nfig = plt.figure(figsize=(16, 10))\nfig.suptitle('K-means clustring, PCA & T-NSE display')\n\nfor ii,(cluster_num, cluster_val) in enumerate(clusters_info.items()):\n    df['y'] = cluster_val['labels']\n    \n    for pic_type in range(2):\n        if pic_type == 0:\n            ax = fig.add_subplot(2, 3, ii+1, projection='3d')\n\n            ax.scatter(\n                            xs=df.loc[rndperm,:][\"pca-one\"], \n                            ys=df.loc[rndperm,:][\"pca-two\"], \n                            zs=df.loc[rndperm,:][\"pca-three\"], \n                            c=df.loc[rndperm,:][\"y\"], \n                            cmap='tab10'\n                        )\n            ax.title.set_text(f'PCA, {str(cluster_num)} clusters')\n            ax.set_xlabel('pca-one')\n            ax.set_ylabel('pca-two')\n            ax.set_zlabel('pca-three')\n        else:\n            ax = fig.add_subplot(2, 3, ii+1+3, projection='3d')\n\n            ax.scatter(\n                            xs=tsne_pca_results[:,0], \n                            ys=tsne_pca_results[:,1], \n                            zs=tsne_pca_results[:,2], \n                            c=df.loc[rndperm[:SAMPLES_TNSE],:][\"y\"], \n                            cmap='tab10'\n                        )\n            ax.title.set_text(f'T-NSE, {str(cluster_num)} clusters')\n            ax.set_xlabel('t-nse-one')\n            ax.set_ylabel('t-nse-two')\n            ax.set_zlabel('t-nse-three')\n\n","b47dfe87":"## prepare word cloud data\n# Load abstract dataframe and clean it(takes only abstract)\nabstract_df = load_annotations(ARTICLES_ABS_PLUS_ENCODE_PATH)\nfiltered_abstract = only_exists_abstract(abstract_df)\n\n\n# Extract text from the abstract and put it to dictionary with the matched class \nclusters_dict = {}\nfor k1,v1 in clusters_info.items():\n    filtered_abstract['y'] = v1['labels']\n\n    for class_num in range(k1):\n        class_abstract_text = filtered_abstract.loc[filtered_abstract['y'] == class_num, 'abstract']\n        if k1 in clusters_dict:\n            clusters_dict[k1][class_num] =  ''\n        else:\n            clusters_dict[k1] = {class_num: ''}\n        \n        text_list = class_abstract_text.values.tolist()\n        long_text = ' '.join(text_list)\n        \n        clean_data = data_clean(long_text, REMOVE_TERMS)\n        clusters_dict[k1][class_num] = clean_data\n             \n# Save results \n# with open(CLUSTERS_DICT_PATH, 'wb') as handle:\n#     pickle.dump(clusters_dict, handle)","080aece0":"## Make the word cloud\n# Save coronavirus mask word cloud image\nfor k1, v1 in clusters_dict.items():\n    for k2, text in v1.items():\n        if text == '':\n            print(f'Number of clusters: {k1}, in cluster number: {k2}, There is no text')\n            continue\n            \n        image_saved_name = IMAGE_CLUD_PATH_CORONA_MASK.format(k1,k2)\n        draw_word_cloud(text, image_saved_name, CORONVAVIRUS_PATTERN, COVID19_PICTURE_PATH)","82223355":"# 1. NLP Method\n","c1e8680e":"# 3. Clustering Insights\nIn order to give additional insight, we examine K-means clustering the abstracts\u2019 representations.  \nAfter inspections of few clustering options (In this notebook presented only 5\/10 and 15 clusters), the cluster of 10 was selected, See word cloud below. \n\nIt can be seen that the division to clusters are indeed meaningful(with a little bit of noise), e.g. in cluster number 2, it can be seen that the topic is 'livestock'.\nIn cluster 10, there theme is the biological mechanism of the covid19 virus.","4ce65bcf":"# 2. Evaluation & Results\nA small team (not involved in developing the method) evaluated the answers provided by our engine for the four questions posed in the sample task.  We manually evaluated the highest confidence 50 answers for each question and measured against that small dataset in means of information retrieval and precision. Note: for regular search engines, fewer answers may be needed; however, we are trying to extract as many insights as possible for the question, beyond the first few. In addition, while our logic tried to find the most fitting answer, the original task definition is to compose the best answers. Such composition is best formed out of several answers.\n\nResults\nOut of the 3 highest confidence answers, at least one was correct in  42% of the cases. The Mean Average Precision over the ~50 answers is 22%. Note: for the latter calculation we did not consider the score confidence (rather \u2013 we considered all 50 answers as if the engine regarded them as correct).\n\nExamples of insightful answers:\n\n**QUESTION1**:\n\nAre there geographic variations in the rate of COVID-19 spread?\n\n**ANSWERS**:\n1. \tCOVID-19 growth rate exponents are greater in larger cities\n1. \tDecision-makers get serious benefits from using better and more flexible models as they can avoid of nuanced lock-downs, better plan the exit strategy based on local population data, different stages of the epidemic in different areas, making specific recommendations to specific groups of people; all this resulting in a lesser impact on economy, improved forecasts of regional demand upon NHS allowing for intelligent resource allocation.\n1. \tOf course, the model can be applied to any other country or region. In the paper the results are presented in detail for Bulgaria, Italy and globally.\n\n\n**QUESTION2**:\n\nWhat works have been done on infection spreading?\n\n**ANSWERS**:\n1. \tIn this outbreak, genotyping helped characterize the transmission of TB among homeless shelter residents and suggested that multiple chains of transmission occurred simultaneously')\n1. For example, advances in virology have been instrumental in improving the usefulness of surveillance data to understand the epidemiology of viral hepatitis.\n1. In this chapter, some basic ideas of modelling the spread of infectious diseases, the main concepts of epidemic dynamics, and some developing tendencies in the study of epidemic dynamics are introduced, and some results with respect to the spread of SARS in China are given.\n\n**QUESTION3**:\n\nAre there geographic variations in the mortality rate of COVID-19?\n\n**ANSWERS**:\n1. For the recorded number of deaths from COVID-19, Italy has the highest number of deaths from the disease followed by Spain, USA, UK, Sweden, Ghana and Nigeria (least) over the 45 days period (Fig. 2)\n1. When faced with the same COVID-19 during the fight against the epidemic, medical health workers in rural areas might worry about being infected due to a different working place involving different medical skills and medical conditions. In contrast, the medical conditions in urban areas were often much better'\n1. Highly sophisticated epidemiological models involving, for instance, spatial resolution and networks of social connections, can be of great use in making detailed predictions, provided that the relevant parameters are adequately known. This is not yet the case for COVID-19, given that estimates of the basic reproduction number itself differ substantially\n\n**QUESTION4**:\n\nIs there any evidence to suggest geographic based virus mutations?\n\n**ANSWERS**:\n\n1. Additionally, model results showed two geographical clusters with high predicted host diversity: one extending across central sub-Saharan Africa to the southeastern coast of Africa, and the second encompassing the Neotropic region and Central Americas\n1. Several important limitations must be considered in our extrapolations, including (i) the assumption that a mean of 58 viruses per species is a reasonable estimate and that host populations are panmictic with respect to viral transmission (such that expanded geographic sampling would not influence viral detec-tions)\n1. Mean probabilities for other Orders were not significantly different from each other (Mann-Whitney-Wilcoxon Bonferroni-adjusted P > 0.05, Supplementary Figure 12 ). The geographical distribution of these species showed high species diversity clusters across Europe and Russia\n","2c60de62":"# Helping fight Covid-19 with State Of The Art NLP methods\nWe developed a preliminary method for addressing one of the Kaggle Covid 19 challenges. Specifically we looked at the geographical influences sample task.\n\nIn this task 4 questions were posed, and we managed to get some relevant answers from the papers. for example for the question: \u201cWhat works have been done on infection spreading?\u201d the engine finds answers such as: \u201cIn this outbreak, genotyping helped characterize the transmission of TB among homeless shelter residents and suggested that multiple chains of transmission occurred simultaneously\u201d and \u201cIn this chapter, some basic ideas of modelling the spread of infectious diseases, the main concepts of epidemic dynamics, and some developing tendencies in the study of epidemic dynamics are introduced, and some results with respect to the spread of SARS in China are given\u201d.\n\nFor additional insight, we examined clustering the documents using our method\u2019s document representation. The clusters appear to be meaningful e.g. one of the clusters focuses on 'livestock' and another on the biological mechanism of covid19...\n\nThe notebook seperated into 4 parts:\n1.\tNLP Process\n2.\tEvaluation\n3.\tInsights\n4.\tConclusions\n\n\n\nAuthors: \n* Shaked Greenfeld (Apps Dev Intermed Prgmr Anlyst at CITI innovation labs, TLV)\n* Or Levitas(Apps Dev Programmer Analyst at CITI innovation labs, TLV)\n* Dvir Levy(Apps Dev Sr. Programmer Anlyst at CITI innovation labs, TLV)\n* Gila Halperin(Apps Dev Sr. Programmer Anlyst at CITI innovation labs, TLV)\n* Karni Gilon(Apps Development Sr Manager at CITI innovation labs, TLV)","0aa3bda2":"Results of the word cloud, 10 clusters\n\n![corona_clusters.jpg](attachment:corona_clusters.jpg)","4137f42b":"# 4. Conclusion\n\nWe presented straightforward unsupervised approach while using State-of-The-Art tools. \n\nWe manually evaluated our results for a small set of questions, and got some interesting insights. \n\nBoth method and evaluation represent solutions ideas. For fully developed solutions additional work needs to be done.\n\n\n**Points for improvement**:\n1.\tThe state-of-the-art tools used 'as is', a better approach is train these models on the corpos for knowledge domain shift.\n2.\tAdd a \u201ccorrectness\u201d threshold: do not return answers below that threshold.\n3.\tError analysis: check precision errors,  i.e., when the engine provided high confidence errors for wrong answers; and recall errors: when potentially good answers get low scores. \n4.\tEvaluation: should be done on a large set of questions with a large team of annotators, and with Subject Matter Experts (people with biological background).\n\n\nWe hope that the notebook gave you more insights.\n","30ae243d":"![Logic.jpg](attachment:Logic.jpg)\n\nThe method includes two parts, as follows\n1.\tRetrieve the most relevant papers - retrieve the most related papers to the questions. We first calculate a RoBERTa representation for the question. We then calculate the cosine similarity between the question RoBERT representation and the abstracts representations, We retrieve the 70 most similar papers to be used in the next stage.\n2.\tWe split the papers into paragraphs(batch of 7 lines). We run \"ALBERT Question Answering\" on the question and each of the paragraphs, This process finds the best question-fitting span for each paragraph, along with a confidence score. Spans pertaining to paragraphs with no good answer are expected to have a low confidence score. We sort the span responses according to their confidence score in descending orders. Finally the answers we provide are the paragraphs corresponding to those spans...\n\nPre-processing: We pre-preprocess the papers, calculating RoBERTa representation for their respective abstracts.\nOther methods: prior to choosing RoBERTa and ALBERT we tested several other approached (not reported here) which yielded worse results."}}