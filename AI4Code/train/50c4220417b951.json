{"cell_type":{"d62e86bd":"code","5c86602d":"code","08a024a1":"code","f37cdb82":"code","57fc42da":"code","cbbbefeb":"code","372d4d09":"code","907761c1":"code","873130e8":"code","e22ac26e":"code","4f27b446":"code","4c32819b":"code","01734b17":"code","d7a13b4b":"markdown","fd5974c8":"markdown"},"source":{"d62e86bd":"#library import\n\nimport pandas as pd\nimport librosa \nimport librosa.display\nimport IPython.display as ipd \nimport numpy as np\nfrom tqdm import tqdm\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n#Read the csv file\n\naudio_dataset_path = '..\/input\/coughwavcovid\/'\nmetadata_path = pd.read_csv('..\/input\/coughwavcovid\/cough_covid.csv') \n\n\n#Show data\nmetadata_path.head(10) ","5c86602d":"#we extract 100 characteristics from each sound file\ndef audio_feature_extraction(file):\n    audio, sample_rate = librosa.load(audio_dataset_path+file_name, res_type='kaiser_fast')\n    mfcc_feat = librosa.feature.mfcc(y=audio, sr= sample_rate, n_mfcc=100)\n    mfcc_scaled_features = np.mean(mfcc_feat.T, axis =0)\n    return mfcc_scaled_features\n\nimport numpy as np\nextracted_features = []\nfor index_num, row in tqdm(metadata_path.iterrows()):\n    file_name = str(row['file_name'])\n    final_class_labels = str(row['class'])\n    data = audio_feature_extraction(file_name)\n    extracted_features.append([data, final_class_labels])","08a024a1":"### converting extracted_features to Pandas dataframe\nextracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\nextracted_features_df.head()","f37cdb82":"# Split the dataset into independent and dependent dataset\nX=np.array(extracted_features_df['feature'].tolist())\ny=np.array(extracted_features_df['class'].tolist())\nX.shape\ny","57fc42da":"#Select training and test data from the model\nlabelencoder=LabelEncoder()\ny=to_categorical(labelencoder.fit_transform(y))\n### Train Test Split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)","cbbbefeb":"#Show the data\nprint(\"Training data:\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"Test data:\")\nprint(X_test.shape)\nprint(y_test.shape)","372d4d09":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout, Dense,Activation, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn import metrics\n## No of classes\nnum_labels=y.shape[1]\nprint(num_labels)","907761c1":"#Building the layers\nmodel=Sequential()\n###first layer\nmodel.add(Dense(100,input_shape=(100,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n###second layer\nmodel.add(Dense(200))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n###third layer\nmodel.add(Dense(100))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\n###final layer\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))\nmodel.summary()","873130e8":"#Compile the models\nmodel.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')","e22ac26e":"## Training \nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime \n\nnum_epochs = 400\nnum_batch_size = 10\n#save the training\ncheckpointer = ModelCheckpoint(filepath='cought_covid.h5', \n                               verbose=1, save_best_only=True)\nstart = datetime.now()\nprint(\"Start Training...\")\nhistory=model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","4f27b446":"#Show the grahp accuray\ntest_accuracy=model.evaluate(X_test,y_test,verbose=0)\nprint(test_accuracy[1])\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=[14,10])\nplt.subplot(211)\nplt.plot(history.history['loss'],'r',linewidth=3.0)\nplt.plot(history.history['val_loss'],'b',linewidth=3.0)\nplt.legend(['Training loss', 'Validation Loss'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves',fontsize=16)\n \n# Accuracy Curves\nplt.figure(figsize=[14,10])\nplt.subplot(212)\nplt.plot(history.history['accuracy'],'r',linewidth=3.0)\nplt.plot(history.history['val_accuracy'],'b',linewidth=3.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves',fontsize=16)","4c32819b":"# function to extract features from the audion file\n\n# transform each category with it's respected label\n\ndef extract_feature(file_name):\n    # load the audio file\n    audio_data, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n    \n    # get the feature \n    feature = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=100)\n    \n    # scale the features\n    feature_scaled = np.mean(feature.T,axis=0)\n    \n    # return the array of features\n    return np.array([feature_scaled])\n\n\n\n\n# function to predict the feature\n\ndef print_prediction(file_name):\n    \n    # extract feature from the function defined above\n    prediction_feature = extract_feature(file_name) \n    \n    # get the id of label using argmax\n    predicted_vector = np.argmax(model.predict(prediction_feature), axis=-1)\n    \n    # get the class label from class id\n    predicted_class = labelencoder.inverse_transform(predicted_vector)\n    \n    # display the result\n    print(\"The predicted class is:\", predicted_class[0], '\\n')\n        ","01734b17":"# File name=file to predict\n#upload 3 files that have not been used in the model\n#sample35Normal.wav\n#sample-41C.wav\n#sample-42C.wav\nfile_name ='..\/input\/modeltest\/sample-41C.wav'\n\n# get the output\nprint_prediction(file_name)\n\n# play the file\n\ndat1, sampling_rate1 = librosa.load(file_name)\nplt.figure(figsize=(20, 10))\nD = librosa.amplitude_to_db(np.abs(librosa.stft(dat1)), ref=np.max)\nplt.subplot(4, 2, 1)\nlibrosa.display.specshow(D, y_axis='linear')\nlibrosa\nplt.colorbar(format='%+2.0f dB')\nplt.title(file_name)\n\nipd.Audio(file_name)","d7a13b4b":"The model shows a 100% success rate.\nIt should be noted that the amount of training dataset is very small.\n\nLet's try new files after training ... ","fd5974c8":"\n\nWe will use as a basis a neural network designed for the classification of environmental sounds.\nIdentified tasks:\n*    1.- Preparation of the data set.\n*    2.- Adaptation of the neural network.\n*    3.- Training.\n*    4.- Results.\n\nThe exercise is available in colabs:\nhttps:\/\/colab.research.google.com\/drive\/1CdvU5DA8Cq25i2q7G3T9IbMOOiLJdnf0?usp=sharing\n\nWe will use a conversion program to convert the files to wav format.\nThe libraries that we will use will extract the characteristics of each sound file.\nhttps:\/\/es.wikipedia.org\/wiki\/MFCC\nThese characteristics will be the inputs of our neural network.\n\nWe convert the \"Covid\" files and the \"Normal\" files to wav format:\n* Covid:\n* sample-1C.wav\n* sample-XC.wav\n* .....\n* Normal:\n* sample-1Normal.wav\n* sample-Xnormal.wav\n\nAnd we generate a csv file (cough_covid.csv) with the classification of each file in format:\n\n* file_name, classID, class\n* sample-1C.wav, 1, Positive\n* sample-XC.wav, 1, Positive\n* ..............\n* sample-1Normal.wav, 0, Neative\n* sample-XC.Normal, 0, Negative\n\nCopy the wav files and csv file in the work path:\n\\cough_covid\\..\n\n* cough_covid.csv\n* \n* sample-1C.wav\n* sample-XC.wav\n* sample-1Normal.wav\n* sample-Xnormal.wav\n"}}