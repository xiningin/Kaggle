{"cell_type":{"a54ec480":"code","e0101b8e":"code","ef76a070":"code","7ce625b7":"code","125efb1d":"code","a9e945b0":"code","7ab57f56":"code","2edb1988":"code","f0d14476":"code","da0c322f":"code","2a687c36":"code","71b2c50f":"code","e4101c20":"code","8833444d":"code","6fac658c":"code","aba2b468":"code","3811c164":"code","26da26c6":"code","9103c154":"code","ccea674e":"code","d0796713":"code","5037d6a0":"code","05527e90":"markdown","f46fff2a":"markdown","7a2185ef":"markdown","108b0b47":"markdown","6b0a9f8b":"markdown","018f7038":"markdown","279b940d":"markdown","97ac3794":"markdown","6d84e103":"markdown"},"source":{"a54ec480":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Plotting labelled data\nfrom nltk.corpus import stopwords # dealing with stop words\nfrom textblob import TextBlob # dealing with spelling correction\nfrom textblob import Word # dealing with lemmatization\nfrom sklearn.feature_extraction.text import TfidfVectorizer # leading with term frequency\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom scipy.sparse import hstack\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e0101b8e":"train = pd.read_csv('..\/input\/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('..\/input\/test.tsv', sep=\"\\t\")\nsubmission = pd.read_csv('..\/input\/sampleSubmission.csv', sep=\"\\t\")","ef76a070":"train.shape","7ce625b7":"test.shape","125efb1d":"train['Phrase'] = train['Phrase'].apply(lambda x: \" \".join(x.lower() for x in x.split()))","a9e945b0":"train['Phrase'] = train['Phrase'].str.replace('[^\\w\\s]','')","7ab57f56":"stop = stopwords.words('english')\ntrain['Phrase'] = train['Phrase'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))","2edb1988":"TextBlob(train['Phrase'][1]).words\nTextBlob(test['Phrase'][1]).words","f0d14476":"train['Phrase'] = train['Phrase'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))","da0c322f":"test['Phrase'] = test['Phrase'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))","2a687c36":"tfidf = TfidfVectorizer(ngram_range=(1,2))\ntrain_vect = tfidf.fit_transform(train['Phrase'])\ntest_vect = tfidf.transform(test['Phrase'])","71b2c50f":"train_vect.shape","e4101c20":"test_vect.shape","8833444d":"#train_features = hstack([train_vect])\n#test_features = hstack([test_vect])","6fac658c":"le=LabelEncoder()\ny=le.fit_transform(train.Sentiment.values)\n#y = pd.get_dummies(train.Sentiment)\n#y.head()","aba2b468":"lsv = LinearSVC()\nnb = MultinomialNB()\nlr = LogisticRegression(random_state=0)","3811c164":"X_train_vect, X_test_vect, y_train_vect, y_test_vect = train_test_split(train_vect, y, train_size=0.75)\nnb.fit(X_train_vect, y_train_vect)\npredictions_nb = nb.predict(X_test_vect)\naccuracy = accuracy_score(y_test_vect, predictions_nb)\nprint(accuracy)","26da26c6":"lr.fit(X_train_vect, y_train_vect)\npredictions_lr = lr.predict(X_test_vect)\naccuracy = accuracy_score(y_test_vect, predictions_lr)\nprint(accuracy)","9103c154":"lsv.fit(X_train_vect, y_train_vect)\npredictions_lsv = lsv.predict(X_test_vect)\naccuracy = accuracy_score(y_test_vect, predictions_lsv)\nprint(accuracy)","ccea674e":"lsv.fit(train_vect, y)\npredictions_lsv = lsv.predict(test_vect)","d0796713":"test['Sentiment'] = predictions_lsv\nsubmission = test[[\"PhraseId\",\"Sentiment\"]]\nsubmission.to_csv(\"Finalsubmission.csv\", index = False)","5037d6a0":"submission.head()","05527e90":"**Text data pre processing **","f46fff2a":"Tokenization\nTokenization refers to dividing the text into a sequence of words or sentences. In this example, we have used the textblob library to first transform phrases into a blob and then converted them into a series of words.","7a2185ef":"Removal of Stop Words","108b0b47":"Inverse Document Frequency\nThe intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it\u2019s appearing in all the documents.\n\nTherefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n\nIDF = log(N\/n), where, N is the total number of rows and n is the number of rows in which the word was present.\n\nTfidfVectorizer can lowercase letters, disregard punctuation and stopwords.","6b0a9f8b":"Lower case","018f7038":"Removing Punctuation","279b940d":"**Model selection**\n\n* Logistic Regression\n* Linear Support Vector Machine\n* Multinomial Naive Bayes","97ac3794":"**Predict and submission**","6d84e103":"Lemmatization\nLemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, we usually prefer using lemmatization over stemming."}}