{"cell_type":{"6a58adfa":"code","63bfd601":"code","f789528d":"code","41709a9b":"code","d0f2686c":"code","ca27dcd5":"code","5fd3fc3c":"code","063237b0":"code","db036770":"code","67601552":"code","73c6a47c":"code","c5d1ce33":"code","fc22563e":"code","14a8acbf":"code","1369ed93":"code","241fbcfe":"code","afb05abf":"markdown","9630872f":"markdown","5ed73bce":"markdown","3819def7":"markdown","f1f676ba":"markdown","0b030521":"markdown","748a88aa":"markdown","4346bc01":"markdown","540fd047":"markdown","c82b1c62":"markdown","53e3c9f4":"markdown","880967cb":"markdown","b961a108":"markdown"},"source":{"6a58adfa":"# set up environment\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport time","63bfd601":"# a function for preparing our training and testing data\ndef prep_data(phenotype) :\n    pheno = pd.read_csv('..\/input\/gono-unitigs\/metadata.csv', index_col=0)\n    pheno = pheno.dropna(subset=[phenotype]) # drop samples that don't have a value for our chosen resistance profile\n    pheno = pheno[phenotype]\n    \n    # read in unitig data\n    X = pd.read_csv('..\/input\/gono-unitigs\/' + phenotype + '_gwas_filtered_unitigs.Rtab', sep=\" \", index_col=0, low_memory=False)\n    X = X.transpose()\n    X = X[X.index.isin(pheno.index)] # only keep rows with a resistance measure\n    pheno = pheno[pheno.index.isin(X.index)]\n    return X, pheno\n","f789528d":"# prepare our data for predicting ciprofloxacin resistance\nphenotype = 'cip_sr'\nX, pheno = prep_data(phenotype)\n\n# create an array for storing performance metrics\nperformance = []\nmethod = []\ntimes = []","41709a9b":"# look at the length distribution of the unitigs in our dataset\nunitigs = X.columns\nmylen = np.vectorize(len)\nuni_len = mylen(unitigs)\nsb.distplot(uni_len)","d0f2686c":"# function for fitting a model\ndef fitmodel(X, pheno, estimator, parameters, modelname, method, performance, times) :\n    kfold = KFold(n_splits=5)\n    for train_index, test_index in kfold.split(X, pheno):\n        # time how long it takes to train each model type\n        start = time.process_time()\n        \n        # split data into train\/test sets\n        X_train = X.iloc[train_index]\n        y_train = pheno[train_index]\n        X_test = X.iloc[test_index]\n        y_test = pheno[test_index]\n        \n        # perform grid search to identify best hyper-parameters\n        gs_clf = GridSearchCV(estimator=estimator, param_grid=parameters, cv=3, n_jobs=-1, scoring='balanced_accuracy')\n        gs_clf.fit(X_train, y_train)\n        \n        # predict resistance in test set\n        y_pred = gs_clf.predict(X_test)\n        y_pred[y_pred<0.5] = 0\n        y_pred[y_pred>0.5] = 1\n\n        score = balanced_accuracy_score(y_test, y_pred)\n        performance = np.append(performance, score)\n        method = np.append(method, modelname)\n        times = np.append(times, (time.process_time() - start))\n\n        print(\"Best hyperparameters for this fold\")\n        print(gs_clf.best_params_)\n        print(\"Confusion matrix for this fold\")\n        print(confusion_matrix(y_test, y_pred))\n    return gs_clf, method, performance, times\n","ca27dcd5":"enet = SGDClassifier(loss=\"log\", penalty=\"elasticnet\")\nenet_params = {\n    'l1_ratio': [0.1, 0.2, 0.5]\n}\n\nenet_model, method, performance, times = fitmodel(X, pheno, enet, enet_params, \"Elastic net\", method, performance, times)","5fd3fc3c":"svm = SVC(class_weight='balanced')\nsvm_params = {\n    'C': [0.01],\n    'gamma': [1e-06, 1e-05],\n    'kernel': ['linear']\n}\n\nsvm_model, method, performance, times = fitmodel(X, pheno, svm, svm_params, \"Support vector machine\", method, performance, times)","063237b0":"xgb_mod = xgb.XGBClassifier(random_state=0)\nxgb_params = {\n    'alpha': [1e-5, 1e-4], \n    'colsample_bytree': [0.6],\n    'gamma': [0.05, 0.1], \n    'learning_rate': [0.01, 0.1], \n    'max_depth': [2], \n    'objective': ['binary:hinge'], \n    'subsample': [0.2, 0.4, 0.6]\n}\n\nxgb_model, method, performance, times = fitmodel(X, pheno, xgb_mod, xgb_params, \"XGBoost\", method, performance, times)","db036770":"rf = RandomForestClassifier(random_state=0, n_jobs=-1, class_weight=\"balanced\")\nrf_params = {\n    'max_features': [round(X.shape[1]*0.1), round(X.shape[1]*0.5), round(X.shape[1]*0.8)],\n    'max_depth': [3],\n    'n_estimators': [50]\n}\n\nrf_model, method, performance, times = fitmodel(X, pheno, rf, rf_params, \"Random forest\", method, performance, times)","67601552":"# compare results from the different predictors\nsb.set_context(\"talk\")\nplt.title(\"Model Performance - Ciprofloxacin Resistance\", y=1.08)\nsb.swarmplot(x=method, y=performance, palette=\"YlGnBu_d\", size=10)\nsb.despine()\nplt.ylabel(\"Balanced accuracy\")\nplt.xticks(rotation=30, ha=\"right\")","73c6a47c":"# took at the time taken to train the different models\nsb.set_context(\"talk\")\nplt.title(\"Model Training Times - Ciprofloxacin Resistance\", y=1.08)\nsb.swarmplot(x=method, y=times, palette=\"YlGnBu_d\", size=10)\nsb.despine()\nplt.ylabel(\"Time taken for training\")\nplt.xticks(rotation=30, ha=\"right\")","c5d1ce33":"# function for looking at SVM feature importance\nsb.set_context(\"talk\")\ndef plot_coefficients(classifier, feature_names, top_features=5):\n    coef = classifier.best_estimator_.coef_.ravel()\n    top_positive_coefficients = np.argsort(coef)[-top_features:]\n    top_negative_coefficients = np.argsort(coef)[:top_features]\n    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n    # create plot\n    plt.figure(figsize=(10, 5))\n    plt.title(\"Feature Importances (Support Vector Machine) - Ciprofloxacin Resistance\", y=1.08)\n    colors = ['crimson' if c < 0 else 'cornflowerblue' for c in coef[top_coefficients]]\n    plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n    feature_names = np.array(feature_names)\n    plt.xticks(np.arange(0, 1 + 2 * top_features), feature_names[top_coefficients], rotation=60, ha='right')\n    plt.show()\n    np.asarray(feature_names)[top_positive_coefficients]\n\nplot_coefficients(svm_model, list(X.columns))\n\n# if we print the unitigs, we can then look at what genes they relate to\ncoef = svm_model.best_estimator_.coef_.ravel()\nfeature_names = list(X.columns)\ntop_negative_coefficients = np.argsort(coef)[:5]\nprint(\"Top negative predictors: \", np.asarray(feature_names)[top_negative_coefficients])\n\ntop_positive_coefficients = np.argsort(coef)[-5:]\nprint(\"Top positive predictors: \", np.asarray(feature_names)[top_positive_coefficients])","fc22563e":"importances = rf_model.best_estimator_.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf_model.best_estimator_.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1][:10]\n\n# Plot the feature importances of the forest\nsb.set_context(\"talk\")\nplt.figure(figsize=(10, 5))\nplt.title(\"Feature Importances (Random Forest) - Ciprofloxacin Resistance\")\nplt.bar(range(10), importances[indices],\n       color=\"crimson\", yerr=std[indices], align=\"center\")\nplt.xticks(range(10), np.asarray(feature_names)[indices], rotation=60, ha='right')\nplt.xlim([-1, 10])\nplt.show()\n\nfeature_names = list(X.columns)\nprint(\"Top predictors: \", np.asarray(feature_names)[indices])","14a8acbf":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    sb.set_context(\"talk\")\n    plt.figure(figsize=(7, 5))\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Balanced accuracy\")\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=\"balanced_accuracy\")\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid(color='gainsboro')\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"crimson\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"cornflowerblue\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"crimson\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"cornflowerblue\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n","1369ed93":"title = \"Learning Curve (Random forest) - Ciprofloxacin resistance\"\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n\nestimator = RandomForestClassifier(max_features=round(X.shape[1]*0.5), n_jobs=-1, max_depth=3, n_estimators=50)\nplot_learning_curve(estimator, title, X, pheno, ylim=(0.7, 1.01), cv=cv, n_jobs=-1)\n\nplt.show()","241fbcfe":"title = \"Learning Curve (Support vector machine) - Ciprofloxacin resistance\"\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n\nestimator = SVC(C=0.01, gamma=1e-06, kernel='linear')\nplot_learning_curve(estimator, title, X, pheno, ylim=(0.7, 1.01), cv=cv, n_jobs=-1)\n\nplt.show()","afb05abf":"## Model training\n\nWe will start will building some basic models for ciprofloxacin resistance. This resistance pattern can mostly be explained by a single mutation, so is likely to be impacted by the amount of noise each method incorporates. ","9630872f":"## Should we collect more samples?\n\nHere, we will construct a learning curve to see how much performance improves as we include more of our sample set, and whether performance gains have levelled off, suggesting that collecting more samples won't help, or whether it's still improving. ","5ed73bce":"Now we will try XGBoost. ","3819def7":"For this exercise, we have genome sequence and antibiotic resistance data gathered from different publicly available sources. If you'd like to learn more about the collection, an interactive view of the data can be accessed [here](https:\/\/microreact.org\/project\/N_gonorrhoeae). \n\nFor this analysis, we're using **unitigs**, stretches of DNA shared by a subset of the strains in our study. Unitigs are an efficient but flexible way of representing DNA variation in bacteria. If you'd like to learn more about unitigs, and how this dataset was constructed, have a look at [this paper](https:\/\/journals.plos.org\/plosgenetics\/article?id=10.1371\/journal.pgen.1007758).\n\nThe full dataset consists of 584,362 unitigs, which takes a long time to train models on, so for this exercise we will be using a set that has been filtered for unitigs associated with resistance. ","f1f676ba":"You can take the unitigs from above, and input one into this search algorithm: https:\/\/www.uniprot.org\/blast or https:\/\/card.mcmaster.ca\/analyze\/blast to see if it comes from a known protein or resistance mechanism. The search has to be formatted like this for CARD:\n\n```\n>sequence\n[unitig]\n```\n\nand you will need to choose the BLASTN option for the query to be processed correctly.\n\nIf you want to look into the biology more, try [this link](https:\/\/blast.ncbi.nlm.nih.gov\/Blast.cgi?PAGE_TYPE=BlastSearch&USER_FORMAT_DEFAULTS=on&SET_SAVED_SEARCH=true&PAGE=MegaBlast&PROGRAM=blastn&GAPCOSTS=0%200&MATCH_SCORES=1,-2&DATABASE=nt_v5&BLAST_PROGRAMS=megaBlast&MAX_NUM_SEQ=100&SHORT_QUERY_ADJUST=on&EXPECT=10&WORD_SIZE=28&REPEATS=repeat_9606&TEMPLATE_TYPE=0&TEMPLATE_LENGTH=0&FILTER=L&FILTER=m&EQ_MENU=Neisseria%20gonorrhoeae%20%28taxid%3A485%29&PROG_DEFAULTS=on&SHOW_OVERVIEW=on&SHOW_LINKOUT=on&ALIGNMENT_VIEW=Pairwise&MASK_CHAR=2&MASK_COLOR=1&GET_SEQUENCE=on&NUM_OVERVIEW=100&DESCRIPTIONS=100&ALIGNMENTS=100&FORMAT_OBJECT=Alignment&FORMAT_TYPE=HTML), to see where the unitigs can be found in a publicly available collection of genomes. ","0b030521":"Here, we read in the data for our analysis. We will be starting with models to predict ciprofloxacin resistance. ","748a88aa":"Now let's do the same for the random forest. ","4346bc01":"Now that we have our data organised, we can start fitting models. First we will try an elastic net logistic regression. ","540fd047":"## Welcome to the Day Two ML tutorial\n\n### Machine learning algorithms for identifying antibiotic resistant bacteria\n\n**Instructor:** [Nicole Wheeler](https:\/\/twitter.com\/nwheeler443) - Data Scientist at the [Centre for Genomic Pathogen Surveillance](https:\/\/www.pathogensurveillance.net\/)\n\n---\n\nIn this module, you will learn how to train machine learning models for predicting antibiotic resistance in bacteria. \n\nWe will be focussing on a species called _Neisseria gonorrhoeae_, the bacteria which cause gonorrhoea. Gonorrhoea is the second most common sexually transmitted infection (STI) in Europe, after chlamydia. Rates of gonorrhoea infection are on the rise, with a 26% increase reported from 2017-2018 in the UK. \n\nMany people who are infected (especially women) experience no symptoms, helping the disease to spread. If the infection is left untreated, it can lead to infertility in women, and can occasionally spread to other parts of the body such as your joints, heart valves, brain or spinal cord.\n\nResistance of these bacteria to antibiotics is rising over time, making infections hard to treat. Below, you can see rates of resistance to different antibiotics. Image is from this paper: https:\/\/www.mdpi.com\/2079-6382\/7\/3\/60.\n![image.png](attachment:image.png)\nIn the past, patients were treated with an antibiotic called ciprofloxaxcin. Doctors had to stop using this antibiotic because resistance to the drug became too common, causing treatments of infections to fail. Until very recently, the recommended treatment was two drugs - ceftriaxone and azithromycin. Azithromycin was removed from recommendations because of concern over rising resistance to the antibiotic. In February 2018, the first ever reported case of resistance to treatment with ceftriaxone and azithromycin, as well as resistance to the last-resort treatment spectinomycin, was reported. Currently in the UK, patients are only treated with ceftriaxone.  \n\nIn this notebook, we will look at machine learning algorithms for predicting resistance to __ciprofloxacin__. ","c82b1c62":"## Exploring what the model has learned","53e3c9f4":"\nThis is a basic workflow for building predictive models. Now you have some options for next steps. The other things you can do in the following sections are:\n* Look into what your model has learned, and whether this fits with our existing knowledge of antibiotic resistance\n* Examine how much we'd benefit from collecting more samples\n* Explore the impact of genetic relatedness on accuracy measures\n\nAlternatively, you can go back and try the following:\n* Explore more hyper-parameters and try to build more accurate models\n* Try some other model types\n* Try using these unitigs to predict resistance to another antibiotic included in metadata.csv\n\nSelect one that sounds interesting to you.\n\nOr, you can read more about antibiotic resistance:\n* [The impact of antibiotic resistance on modern medicine](https:\/\/medium.com\/@nwheeler443\/the-unmeasured-cost-of-antibiotic-resistance-7de8dc41ea41)\n* [Phage therapy, a possible solution to the antibiotic resistance crisis](https:\/\/medium.com\/@nwheeler443\/phage-therapy-a-solution-to-the-antibiotic-resistance-crisis-e0ec34309a5)\n* [Should machine learning algorithms guide antibiotic prescribing?](https:\/\/towardsdatascience.com\/should-machine-learning-algorithms-guide-antibiotic-prescribing-f74753e28472)","880967cb":"Next, we will try a support vector machine.","b961a108":"And finally, a random forest."}}