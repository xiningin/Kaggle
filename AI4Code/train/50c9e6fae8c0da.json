{"cell_type":{"aaa094bb":"code","f29bdf06":"code","a96a4356":"code","3cdea8b2":"code","a7f5c4af":"code","e131b847":"code","fd3ea007":"code","352baafa":"code","ae277129":"code","3381ca6f":"code","aee223cf":"code","8c7eadad":"code","eadbab36":"code","514ea712":"code","13c5b046":"code","90b814c0":"code","cc325247":"code","6fcc0c27":"code","ca8e128f":"code","b00d11e1":"code","86a1a6ab":"code","a3fecabf":"code","db075456":"code","fa5b80cc":"code","971fc87d":"code","e656912a":"code","c3c30399":"code","a49cef1f":"code","2f5ec7e3":"code","27ea2292":"code","1486c9b4":"code","d1141415":"code","dec57c6e":"code","f8c124b8":"code","d30de455":"code","5fa83216":"code","e8fb7580":"code","bc210a5b":"code","ed0e7607":"code","e3b97615":"code","fbf316fd":"code","8f8fd60c":"code","d0faf45c":"code","3c7d098e":"code","3c25e55e":"code","876f9dff":"code","04fb6776":"code","342e3fed":"code","f89001a0":"code","559a449f":"code","3731d696":"code","9eadfdd5":"code","2739a481":"code","e3a50077":"code","922ea365":"code","423c0b78":"code","0fec8c53":"code","6ac5c57a":"code","05cf42e9":"code","6f7e97a4":"code","f90b8a8b":"code","085b8594":"code","467529c5":"code","5c96d3b6":"code","b19db1ac":"code","52862e29":"code","cd8ae586":"code","7c78eedf":"code","a99d1dc5":"code","5510dd0a":"markdown","ac60f68a":"markdown","9e6ff721":"markdown","9d9bc380":"markdown","5fe73db2":"markdown","918edf29":"markdown","93c8df9e":"markdown","39ae2989":"markdown","44e2307b":"markdown","72b210e2":"markdown","6250c9f3":"markdown","7a648b85":"markdown","92b729cc":"markdown","5a233e19":"markdown","c3b468ee":"markdown","950caa7c":"markdown","85e28c45":"markdown","46848d94":"markdown","9fbffa6d":"markdown","cc733dbf":"markdown","ae4c03ba":"markdown","d930f120":"markdown","dd58936c":"markdown","6261756d":"markdown","69b9713e":"markdown","773afbb1":"markdown","97c7e86c":"markdown","cd4823ef":"markdown","83bf9cda":"markdown","15df614d":"markdown","756b8834":"markdown","cca41f03":"markdown","6ac54ad1":"markdown","035a6969":"markdown","c3a80c78":"markdown","d294c85a":"markdown","bed63daf":"markdown","1d438477":"markdown","a9372ecc":"markdown","eec42d16":"markdown","663fe31e":"markdown","f5695724":"markdown","bf06e587":"markdown","211ae747":"markdown","cb4595f5":"markdown","7772c68d":"markdown","134b721c":"markdown","262020b2":"markdown","629bd1ec":"markdown","c1361b79":"markdown","6a66b8c5":"markdown","4d6073ba":"markdown","2ade9afa":"markdown","f4b03ed9":"markdown","9323df73":"markdown","ec787e01":"markdown"},"source":{"aaa094bb":"import torch","f29bdf06":"a = torch.Tensor([0, 1, 2])\n\nB = torch.Tensor([[ 0,  1,  2,  3],\n              [ 4,  5,  6,  7],\n              [ 8,  9, 10, 11]])\n\nprint(f\"a:\\n{a}\\n a.shape:{a.shape}\")\nprint(f\"B:\\n{B}\\n B.shape:{B.shape}\")","a96a4356":"C = torch.einsum('i,ij->i', a, B)\nprint(f\"\\nC:\\n{C}\\n C.shape: {C.shape}\")","3cdea8b2":"C = (a[:, None] * B).sum(axis=1) # product + sum\nprint(f\"\\nC:\\n{C}\\n C.shape:\\n{C.shape}\")","a7f5c4af":"u = torch.Tensor([0, 1, 2, 3])","e131b847":"torch.einsum('i', u)","fd3ea007":"v = torch.Tensor([4, 5, 6, 7])","352baafa":"torch.einsum('i,i', u, v)","ae277129":"(u*v).sum() #\u00a0it comes from this","3381ca6f":"u@v","aee223cf":"torch.einsum('i,i->i', u, v)","8c7eadad":"u*v","eadbab36":"torch.einsum('i,j -> ij', u, v)","514ea712":"u[:, None]@v[None,:] # = u(i,1) x v(1,j)","13c5b046":"A = torch.Tensor([\n        [ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]\n])\nprint(f\"A.shape:{A.shape}\")","90b814c0":"torch.einsum('ij', A)","cc325247":"torch.einsum('ij -> ij', A)","6fcc0c27":"torch.einsum('ij-> ', A) #\u00a0a scalar output is returned","ca8e128f":"A.sum()","b00d11e1":"torch.einsum('ij->j', A) #\u00a0contraction along i: j (columns) survives","86a1a6ab":"A.sum(axis=0) # sum along columns; a row will result: <<4 columns>>","a3fecabf":"A.sum(axis=0).shape","db075456":"torch.einsum('ij->i', A)","fa5b80cc":"A.sum(axis=1)","971fc87d":"torch.einsum('ji', A)","e656912a":"torch.einsum('ij->ji', A)","c3c30399":"try: torch.einsum('ii->i', A)\nexcept Exception as e: print(e)","a49cef1f":"torch.einsum('ii->i', A[:, :3])","2f5ec7e3":"torch.diag(A)","27ea2292":"torch.einsum('jj->j', A[:, :3])","1486c9b4":"torch.einsum('ii', A[:, :3]) # sum of the diagonal","d1141415":"torch.trace(A)","dec57c6e":"B = torch.Tensor([\n        [ 12,  13,  14,  15],\n        [ 16,  17, 18, 19],\n        [ 20, 21, 22, 23]\n])\nprint(f\"B.shape:{B.shape}\")","f8c124b8":"A.shape, B.shape","d30de455":"torch.einsum('ij, ij -> ij', A, B)","5fa83216":"torch.einsum('ij, ij -> ij', A[0,None], B[0,None])","e8fb7580":"A[0] * B[0]","bc210a5b":"torch.einsum('ij, ij -> ji', A, B) # just transpose","ed0e7607":"print(f\"A.shape:{tuple(A.shape)}, B.shape:{tuple(B.shape)}\")","e3b97615":"torch.einsum('ij, jk -> ik', A, B.T)","fbf316fd":"torch.matmul(A,B.T) # same as above","8f8fd60c":"torch.einsum('ij, kl -> ik', A, B).shape","d0faf45c":"torch.einsum('ij, kl -> il', A, B).shape","3c7d098e":"torch.einsum('ij, kl -> ij', A, B).shape","3c25e55e":"a = torch.einsum('ij, kl -> ijkl', A, B) # expansion\ntorch.einsum('ijkl -> ij', a) # sum along k and l","876f9dff":"torch.einsum('ij, kl -> ij', A, B)","04fb6776":"torch.einsum('ij, kl -> ijk', A, B)","342e3fed":"print(torch.einsum('ij, kl -> ijk', A, B).shape)","f89001a0":"# 1. contraction of B along l\nB_k = torch.einsum('kl -> k', B)\nB_k","559a449f":"# 2. element-wise product between A and B_k\ntorch.einsum('ij, k -> ijk', A, B_k)","3731d696":"torch.einsum('ij, jk -> ij', A, B.T)","9eadfdd5":"torch.einsum('jk', B.T) # original matrix B.T","2739a481":"torch.einsum('jk -> j', B.T) # contraction on j","e3a50077":"B_T_j = torch.einsum('jk -> j', B.T)\ntorch.einsum('ij, j -> ij', A, B_T_j) #\u00a0simple product","922ea365":"A = torch.Tensor([\n    [\n        [ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]\n    ],\n    [\n        [ 12,  13,  14,  15],\n        [ 16,  17, 18, 19],\n        [ 20, 21, 22, 23]\n    ]\n\n])\nprint(f\"A.shape:{A.shape}\")","423c0b78":"torch.einsum('ijk', A)","0fec8c53":"torch.einsum('ijk -> ijk', A)","6ac5c57a":"torch.einsum('i kj', A)","05cf42e9":"torch.einsum('ijk->i', A)","6f7e97a4":"torch.einsum('ijk ->i', A)","f90b8a8b":"B = torch.Tensor([\n    [\n        [ 10,  11,  12,  13],\n        [ 14,  15,  16,  17],\n        [ 18,  19, 20, 21]\n    ],\n    [\n        [ 32,  33,  34,  35],\n        [ 36,  37, 38, 39],\n        [ 40, 41, 42, 43]\n    ]\n\n])\nprint(f\"B.shape:{B.shape}\")","085b8594":"print(f\"A.shape:{list(A.shape)}, B.shape:{list(B.shape)}\")","467529c5":"torch.einsum('ijk, ijl->kl', A, B)","5c96d3b6":"A = torch.arange(8*5).reshape(8,5)\nprint(f\"A.shape:{list(A.shape)}\")\nA","b19db1ac":"B = torch.arange(5*3).reshape(5,3)\nprint(f\"B.shape:{list(B.shape)}\")\nB","52862e29":"A@B","cd8ae586":"A_batched = A.reshape(4,2,5) # cut in four\nA_batched","7c78eedf":"print(f\"A_batched.shape:{list(A_batched.shape)}, B.shape:{list(B.shape)}\")","a99d1dc5":"torch.einsum('ijk, ku->iju', A_batched,B)","5510dd0a":"### <div id='3d'>3D Tensor<\/div>","ac60f68a":"Let's see how it is generated","9e6ff721":"##### <div id='2d_expansion'>Expansion. *('ij, kl -> ijk')*<\/div>","9d9bc380":"### <div id='intro'>Brief intro<\/div>","5fe73db2":"#### <div id='1d_binary'>Binary operations<\/div>","918edf29":"### <div id='1d'>1D Tensor: Vector<\/div>","93c8df9e":"Let's see a normal matrix multiplication between A, (8,5) and B, (5,3).","39ae2989":"#### <div id='2d_binary'>Binary operations<\/div>","44e2307b":"##### <div id='3d_identity'>Identity<\/div>","72b210e2":"##### Transpositions","6250c9f3":"It distributes (broadcast) the product between A and the contraction of B along an axis.","7a648b85":"##### <div id='2d_trace'>Trace. *('ii')*<\/div>","92b729cc":"## <div id='exercises'>Practical notes<\/div>","5a233e19":"```torch.einsum(equation, *operands) \u2192 Tensor```","c3b468ee":"Generic product *(ij, kl)*","950caa7c":"The above results can be seen as different sums on the dims [expansion](#2d_expansion) (broadcasting) of the two matrices. Below an example","85e28c45":"## Contents\n\n* [Brief Intro](#intro)\n* [Definition](#definition)\n* [Practical notes](#exercises)\n    * [1D: Vectors](#1d)\n        * [Identity: *(i)*](#1d_identity)\n        * [Binary operations](#1d_binary)\n            * [Element-wise product, *(i,i -> i)*](#vec_hadamard)\n            * [Inner product, *(i,i->i)*](#vec_inner)\n            * [Outer product, *(i,j->ij)*](#vec_outer)\n    * [2D: Matrices](#2d)\n        * [Unary operations](#2d_unary)\n            * [Identity: *(ij)*](#2d_identity)\n            * [Total sum *('ij -> ')*](#2d_sum)\n            * [Sum along axis. *('ij -> i')*](#2d_axis_sum)\n            * [Transposition. *('ij -> ji')*](#2d_transpose)\n            * [Diagonal. *('ii -> i')*](#2d_diagonal)\n            * [Trace. *('ii')*](#2d_trace)\n        * [Binary operations](#2d_unary)\n            * [Element-wise product. *('ij, ij -> ij')*](#2d_hadamard)\n            * [Matrix product, *(ij,jk)*](#2d_inner)\n            * [Expansion. *('ij, kl -> ijk')*](#2d_expansion)\n            * [Other composite operations. ('ij, jk -> ij')](#2d_other)\n\n    * [3D](#3d)\n        * [Tensor Contraction](#3d_contraction)\n        * [Batch Matrix multiplication](#3d_batch_mat_prod)","46848d94":"We can say *i* and *j* being rows and columns axes","9fbffa6d":"##### <div id='2d_axis_sum'>Sum along axis. *('ij -> i')*<\/div>","cc733dbf":"##### <div id='2d_identity'>Identity. *('ij')* <\/div>","ae4c03ba":"##### <div id='vec_outer'>Outer product<\/div> *(i,j)*, assumes two vectors not necessarily of same length","d930f120":"### <div id='definition'>Definition<\/div>","dd58936c":"Note the same axis in the declaration, ```(ii)```, different from ```(ij)```, being the identity for 2 rank tensors.","6261756d":"##### <div id='3d_contraction'>Tensor contraction<\/div>","69b9713e":"##### <div id='1d_identity'>Identity: *(i)*<\/div>","773afbb1":"This notebook contains some practical notes about einsum function. Here it's implemented in pytorch, but is is available on numpy and other languages as well.\nFeel free to contribute, leave a feedback or suggesting corrections \ud83d\ude0a","97c7e86c":"Quick example","cd4823ef":"##### <div id='2d_diagonal'>Diagonal. *('ii -> i')*<\/div>","83bf9cda":"Einsum is basically a product + sum","15df614d":"#### <div id='refs'>References:<\/div>\n* [Torch einsum](https:\/\/pytorch.org\/docs\/stable\/torch.html#torch.einsum)\n* [Numpy einsum](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.einsum.html)\n* [basic guide to einsum](https:\/\/ajcr.net\/Basic-guide-to-einsum\/)\n* [Einstein Summation in Numpy](https:\/\/obilaniu6266h16.wordpress.com\/page\/3\/)\n* [Einstein Summation, WolframMathWorld](https:\/\/mathworld.wolfram.com\/EinsteinSummation.html)","756b8834":"##### <div id='2d_hadamard'>Element-wise product. *('ij, ij -> ij')*<\/div>","cca41f03":"##### <div id='2d_sum'>Total sum *('ij -> ')*<\/div>","6ac54ad1":"Just another product against some axes","035a6969":"##### <div id='2d_transpose'>Transposition. *('ij -> ji')*<\/div>","c3a80c78":"##### <div id='vec_hadamard'>dot Element-wise product<\/div> *(i,i -> i)*, assumes two vectors of same length","d294c85a":"Explaination.  \nLet's take the same operation between their first rows and compare with the previous output. The result will be equal to the first row.","bed63daf":"The previous assumptions can be extended to higher rank tensors.","1d438477":"This happens because *i* by *j* matrix A is multiplied against a vector resulting by the contraction of B.T on its first index.","a9372ecc":"Swap axes *j* and *k*: axes transposition","eec42d16":"Remind that alphabetical ordering leads the priority of the indices. \"ijk\" is different from \"jki\".","663fe31e":">This function provides a way of computing multilinear expressions (i.e. sums of products) using the Einstein summation convention.\n\n>The equation is given in terms of lower case letters (indices) to be associated with each dimension of the operands and result. The left hand side lists the operands dimensions, separated by commas.\n* There should be one index letter per tensor dimension. The right hand side follows after -> and gives the indices for the output. If the -> and right hand side are omitted, it implicitly defined as the alphabetically sorted list of all indices appearing exactly once in the left hand side. * The indices not apprearing in the output are summed over after multiplying the operands entries. \n* If an index appears several times for the same operand, a diagonal is taken.\n* Ellipses \u2026 represent a fixed number of dimensions. If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output.","f5695724":"_____","bf06e587":"#### <div id='2d_unary'>Unary Operations<\/div>","211ae747":"Element-wise multiplication of A and B. row vs row","cb4595f5":"If we switch output axes in the einsum we can have the transpose result.","7772c68d":"##### <div id='vec_inner'>Inner product<\/div> *(i,i->i)*, assumes two vectors of same length","134b721c":"*Einstein notation*, AKA *Enstein summation* or *Einsum* is a concise way to express matrix operations. It is particularly suited for tensors operations and, thus, mainly used in Physics. In particular Einstein invented such notation in order to easily and cleary indicate the indexes of the tensors over which to perform matrix products.  \n[Numpy einsum](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.einsum.html) was the first python implementation of such notation, as much as I know. You can know more from [here](https:\/\/ajcr.net\/Basic-guide-to-einsum\/#historical-notes-and-links).  \nIt is important to notice that **Numpy einsum is not a perfect copy of the Einstein notation**, it has its pros and cons.\n[Torch einsum](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.einsum.html#torch-einsum) is the Pytorch version of the numpy implementation. It came with perks like parallelization on GPU, so it was faster than the Numpy one at the beginning. As much as I know, Numpy recently optimized einsum. Pytorch einsum is not the perfect copy of Numpy einsum. For example I noticed it lacks the broadcasting of a scalar over a vector. \n\nFor more info you can have a look at the [References](#refs) or Google will help you.\n","262020b2":"Needs a trunkated tensor, namely a square one","629bd1ec":"##### Diagonals","c1361b79":"##### <div id='3d_batch_mat_prod'>Batch matrix multiplication<\/div>","6a66b8c5":"##### <div id=''>Matrix product, dot, ('ij,jk -> ik')<\/div>","4d6073ba":"Now the other axis","2ade9afa":"##### <div id='2d_other'>Other composite operations. *('ij, jk -> ij')*<\/div>","f4b03ed9":"Now split A in 4 chunks and compute the same multiplication as above, but batched.","9323df73":"Please notice the surviving axes: the result will have same dimensions as A.","ec787e01":"### <div id='2d'>2D Tensor: Matrix<\/div>"}}