{"cell_type":{"79b2380d":"code","eb5f3358":"code","1fea1bf8":"code","cf3a62f3":"code","065c71d5":"code","b52af5ee":"code","9869a10e":"code","1e823feb":"code","3b3c4186":"code","05cfd1d7":"code","43ca5a67":"markdown","dc238a17":"markdown","eb1338b9":"markdown","bea2de58":"markdown","13275627":"markdown","2f90e006":"markdown","3fefd7b4":"markdown","88f6d704":"markdown"},"source":{"79b2380d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import model_selection\nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport optuna\nimport tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","eb5f3358":"train_data = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ny = train_data.loss\n\nuseful_features = [col for col in train_data.columns if col not in ('id', 'loss')]\nX = train_data[useful_features]\nX.head()\n\n#scaler = StandardScaler()\n#X[useful_features] = scaler.fit_transform(X[useful_features])\n\n#pca = PCA(n_components = 40)\n#X = pca.fit_transform(X)\n\n#train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","1fea1bf8":"test_data = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\n\ntest_X = test_data[useful_features]\ntest_X.head()","cf3a62f3":"X.shape, y.shape, test_X.shape","065c71d5":"#Setting the kfold parameters\nkf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\noof_preds = np.zeros((X.shape[0],))\npreds_cat = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(kf.split(X)):\n    X_train, X_valid = X.loc[train_id], X.loc[valid_id]\n    y_train, y_valid = y.loc[train_id], y.loc[valid_id]\n    \n    cat_model = CatBoostRegressor(random_state=42)\n    cat_model.fit(X_train, y_train,\n                  verbose = False,\n                 eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                 early_stopping_rounds = 100)\n    \n    #Mean of the predictions\n    preds_cat += cat_model.predict(test_X).reshape(-1,) \/ 10 # Splits\n    \n    #Mean of feature importance\n    model_fi += cat_model.feature_importances_ \/ 10 #splits\n    \n    #Out of Fold predictions\n    oof_preds[valid_id] = cat_model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_id]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ 10\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","b52af5ee":"xgb_params = {\n    'lambda': 67.79737006663706,\n    'alpha': 40.12405005448161,\n    'colsample_bytree': 0.061613774851329205,\n    'subsample': 0.9556736521337416,\n    'learning_rate': 0.17024722721525629,\n    'n_estimators': 9489,\n    'objective': 'reg:squarederror',\n    'max_depth': 3,\n    'gamma': 2,\n    'booster': 'gbtree',\n    'min_child_weight': 155,\n    'random_state': 42,\n    'n_jobs': 4,\n    'sampling_method': 'uniform'\n}","9869a10e":"#Setting the kfold parameters\nkf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\noof_preds = np.zeros((X.shape[0],))\npreds_xgb = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(kf.split(X)):\n    X_train, X_valid = X.loc[train_id], X.loc[valid_id]\n    y_train, y_valid = y.loc[train_id], y.loc[valid_id]\n    \n    xgb_model = XGBRegressor(**xgb_params, tree_method='gpu_hist')\n    xgb_model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 100)\n    \n    #Mean of the predictions\n    preds_xgb += xgb_model.predict(test_X).reshape(-1,) \/ 10 # Splits\n    \n    #Mean of feature importance\n    model_fi += xgb_model.feature_importances_ \/ 10 #splits\n    \n    #Out of Fold predictions\n    oof_preds[valid_id] = xgb_model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_id]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ 10\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","1e823feb":"test_preds = (0.7 * preds_xgb) + (0.3 * preds_cat)","3b3c4186":"output = pd.DataFrame({'id': test_data.id,\n                       'loss': test_preds})\noutput.to_csv('submission_ensemble.csv', index=False)","05cfd1d7":"# 7.88397: 0.7*XGB + 0.3CAT","43ca5a67":"# **Reading the Test Data**","dc238a17":"# **Importing Libraries**","eb1338b9":"# **K Fold CV and Implementation of Catboost Regression Algorithm**","bea2de58":"# **Reading the Training Data and Extracting Useful Features**","13275627":"# **K Fold CV and Implementation of XGBoost Regression Algorithm** ","2f90e006":"# **Ensembling of XGBoost and CatBoost Regression Predictions**","3fefd7b4":"# **Setting the XGBoost Hyperparameters**","88f6d704":"# **Saving the Final Submission File**"}}