{"cell_type":{"6c3708cf":"code","0f0e4f59":"code","54056ecc":"code","e48fc36b":"code","b99748ce":"code","0972a847":"code","af7d26d1":"code","6864e614":"code","4e5ba43d":"code","36a13485":"code","1b052470":"code","227f10e8":"code","b11fd647":"code","3a2ec4e6":"code","fbf6dfce":"code","d494fb2d":"code","f56962dc":"code","21c5e123":"code","70f97fe7":"code","014f4ba2":"code","3b1e227c":"code","ee338cff":"code","2632759f":"code","e8ad2130":"code","b523d07d":"code","69166651":"code","191a0313":"code","142717a2":"code","d8ae1221":"code","47a8cd64":"code","3a70dd8a":"code","bfb32b31":"code","864e8ef4":"code","d924df9b":"code","53690a70":"code","5aaef6bd":"code","da3ea6e3":"code","814f4714":"code","c25a946b":"code","56e0f48c":"code","aede757a":"code","5bd5cc9f":"code","3b7f759c":"code","f9497734":"code","11a66241":"code","d97545a0":"markdown","da1cdb9a":"markdown","acf5d6c6":"markdown","2a5c7e79":"markdown","fbe53587":"markdown","d913f4df":"markdown","98cbc572":"markdown","3dbcac1d":"markdown","71d611c9":"markdown","41686582":"markdown","7cb3f0db":"markdown"},"source":{"6c3708cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0f0e4f59":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plot\n%matplotlib inline\nfrom numba import jit, cuda","54056ecc":"#pip install numba","e48fc36b":"os.getcwd()","b99748ce":"train = train=pd.read_csv(\"\/kaggle\/input\/liverpool-ion-switching\/train.csv\")","0972a847":"train.shape","af7d26d1":"train.head()","6864e614":"test = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/test.csv')","4e5ba43d":"test.shape","36a13485":"test.head()","1b052470":"test['time'] = (test['time'] - 500).round(4)","227f10e8":"test.head()","b11fd647":"train['signal'].describe()","3a2ec4e6":"def add_batch(data, batch_size):\n    c = 'batch_' + str(batch_size)\n    data[c] = 0\n    ci = data.columns.get_loc(c)\n    n = int(data.shape[0] \/ batch_size)\n    print('Batch size:', batch_size, 'Column name:', c, 'Number of batches:', n)\n    for i in range(0, n):\n        data.iloc[i * batch_size: batch_size * (i + 1), ci] = i","fbf6dfce":"for batch_size in [500000, 400000, 200000,100000]:\n    add_batch(train, batch_size)\n    add_batch(test, batch_size)","d494fb2d":"train.head()","f56962dc":"original_batch_column = 'batch_500000'\n\nbatch_columns = [c for c in train.columns if c.startswith('batch')]\nbatch_columns","21c5e123":"batch_6 = train[train[original_batch_column] == 6]","70f97fe7":"# From https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#        else:\n#            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","014f4ba2":"def add_stats(data, batch_column, column):\n    \n    # mean,std: one value per batch\n    stats = {}\n    group = data.groupby(batch_column)[column]\n    stats['mean']   = group.mean()\n    stats['median'] = group.median()\n    stats['max']    = group.max()\n    stats['min']    = group.min()\n    stats['std']    = group.std()\n    \n    c = column + '_' + batch_column\n    \n    # apply it to batches\n    for key in stats:\n        data[c + '_' + key] = data[batch_column].map(stats[key].to_dict())\n    \n    # range\n    data[c + '_range'] = data[c + '_max'] - data[c + '_min']\n    data[c + '_max_to_min_ratio'] = data[c + '_max'] \/ data[c + '_min']","3b1e227c":"for batch_column in batch_columns:\n    if batch_column == original_batch_column:\n        continue\n    \n    add_stats(train, batch_column, 'signal')\n    # add_stats(train, batch_column, 'open_channels')\n    \n    add_stats(test, batch_column, 'signal')","ee338cff":"train.head()","2632759f":"def add_shifted_signal(data, shift):\n    for batch in data[original_batch_column].unique():\n        m = data[original_batch_column] == batch\n        new_feature = 'shifted_signal_'\n        if shift > 0:\n            shifted_signal = np.concatenate((np.zeros(shift), data.loc[m, 'signal'].values[:-shift]))\n            new_feature += str(shift)\n        else:\n            t = -shift\n            shifted_signal = np.concatenate((data.loc[m, 'signal'].values[t:], np.zeros(t)))\n            new_feature += 'minus_' + str(t)\n        data.loc[m, new_feature] = shifted_signal","e8ad2130":"add_shifted_signal(train, -1)\nadd_shifted_signal(test, -1)","b523d07d":"add_shifted_signal(train, 1)\nadd_shifted_signal(test, 1)","69166651":"train.head()","191a0313":"exclude_columns = ['time', 'signal', 'open_channels'] + batch_columns","142717a2":"def add_signal_minus(data, exclude_columns):\n    for column in [c for c in data.columns if c not in exclude_columns]:\n        data['signal_minus_' + column] = data['signal'] - data[column]","d8ae1221":"add_signal_minus(train, exclude_columns)\nadd_signal_minus(test, exclude_columns)","47a8cd64":"train.head()\n#batch_columns","3a70dd8a":"# groups = train['batch'].copy()\n\ny_train = train['open_channels'].copy()\nx_train = train.drop(['time', 'open_channels'] + batch_columns, axis=1)\n\nx_test = test.drop(['time'] + batch_columns, axis=1)","bfb32b31":"list(x_train.columns)","864e8ef4":"del train\ndel test","d924df9b":"set(x_train.columns) ^ set(x_test.columns)","53690a70":"from sklearn.preprocessing import StandardScaler\nx_train = x_train.values\nx_test = x_test.values","5aaef6bd":"from sklearn.model_selection import train_test_split","da3ea6e3":"X_train, X_test, Y_train, Y_test = train_test_split(x_train,y_train,test_size=0.20)","814f4714":"from sklearn.neighbors import KNeighborsRegressor","c25a946b":"error_rate = []\n# Will take some time\nfor i in range(1,20):\n    print('training for k=',i)\n    knn = KNeighborsRegressor(n_neighbors=i,weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski')\n    knn.fit(X_train,Y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i-Y_test)**2)","56e0f48c":"import matplotlib.pyplot as plt\n%matplotlib inline","aede757a":"plt.figure(figsize=(10,6))\nplt.plot(range(1,20),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","5bd5cc9f":"np.round(pred_i)","3b7f759c":"knn = KNeighborsRegressor(n_neighbors=4,weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski')\nknn.fit(X_train,Y_train)\npred_i = knn.predict(x_test)","f9497734":"y_pred = np.round(pred_i)","11a66241":"submission = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv')\nsubmission['open_channels'] = pd.Series(y_pred, dtype='int32')\nsubmission['open_channels']=submission['open_channels'].astype('int')\nsubmission.to_csv('submission_knn04.csv', index=False, float_format='%.4f')\nsubmission.head()","d97545a0":"## Extract target variable","da1cdb9a":"## KNN","acf5d6c6":"## Add copies of the signal with time shift","2a5c7e79":"## Model building and predictions","fbe53587":"## Create 'batch' feature","d913f4df":"## Add stats","98cbc572":"## Free memory","3dbcac1d":"## Add signal minus other features","71d611c9":"## Submit predictions","41686582":"## Standard scaling","7cb3f0db":"## Shift time in test data"}}