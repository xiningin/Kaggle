{"cell_type":{"3612c634":"code","86eaa18d":"code","070e6bfd":"code","74b25372":"code","22ae4127":"code","57be108d":"code","253611b5":"code","435c087f":"markdown","80392a81":"markdown","c69c4441":"markdown","781d00bf":"markdown","47b8ea97":"markdown","7a0da902":"markdown","7be8a5df":"markdown","23d6777c":"markdown","e87bfa64":"markdown","139e15b1":"markdown","2e86499e":"markdown","e0208a2b":"markdown","0098221e":"markdown","5873b4ad":"markdown","1323b9f3":"markdown","e5eebaa5":"markdown","119a48c3":"markdown","af556ed8":"markdown","040d3206":"markdown","a6128b2e":"markdown","a2613af2":"markdown","31374b1a":"markdown","ac93ab11":"markdown","f6c60521":"markdown","a11392ed":"markdown","84855cc6":"markdown","6a8980ab":"markdown","6ed84730":"markdown","330584fe":"markdown"},"source":{"3612c634":"from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\n# Generate dataset with 1000 samples, 100 features and 2 classes\n\ndef gen_dataset(n_samples=1000, n_features=100, n_classes=2, random_state=123):  \n    X, y = datasets.make_classification(\n        n_features=n_features,\n        n_samples=n_samples,  \n        n_informative=int(0.6 * n_features),    # the number of informative features\n        n_redundant=int(0.1 * n_features),      # the number of redundant features\n        n_classes=n_classes, \n        random_state=random_state)\n    return (X, y)\n\nX, y = gen_dataset(n_samples=1000, n_features=100, n_classes=2)\n","86eaa18d":"import pandas as pd\n\n\n# convert X and y to dataframe\nX = pd.DataFrame(X)\ny = pd.DataFrame(y)","070e6bfd":"# ignore warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","74b25372":"# train \/ test split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","22ae4127":"from sklearn.model_selection import cross_val_score\nfrom hyperopt import STATUS_OK\nimport lightgbm as lgb\n\ndef objective_function(params):\n    clf = lgb.LGBMClassifier(**params)\n    score = cross_val_score(clf, X_train, y_train, cv=5).mean()\n    return {'loss': -score, 'status': STATUS_OK}    \n    ","57be108d":"from hyperopt import hp\nimport numpy as np\n\nspace= {\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1)),\n    'max_depth': hp.quniform('max_depth', 5, 15, 1),\n    'n_estimators': hp.quniform('n_estimators', 5, 35, 1),\n    'num_leaves': hp.quniform('num_leaves', 5, 50, 1),\n    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart']),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n}","253611b5":"import csv\n\n# File to save first results\nout_file = 'gbm_trials.csv'\nof_connection = open(out_file, 'w')\nwriter = csv.writer(of_connection)\n\n# Write the headers to the file\nwriter.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\nof_connection.close()","435c087f":"from hyperopt import fmin, tpe, Trials\n\nimport numpy as np\n\nnum_eval = 500\n\ntrials = Trials()\n\nbest_param = fmin(objective_function, space, algo=tpe.suggest, max_evals=num_eval, trials=trials, rstate= np.random.RandomState(1))\n\n\n","80392a81":"Alternatively, we can monitor the progress of a long training process by writing a line to a csv file with each search iteration. This also saves all the results to disk. We can do this using the `csv` library. Before training we should open a new csv file and write the headers. Within the objective function we can add lines to write to the csv on every iteration. The complete code is as follows:-","c69c4441":"Thank you.","781d00bf":"<a class=\"anchor\" id=\"0\"><\/a>\n# Bayesian Optimization using Hyperopt","47b8ea97":"Here we use a number of different domain distribution types which are as follows:-\n\n- choice : categorical variables\n- quniform : discrete uniform (integers spaced evenly)\n- uniform: continuous uniform (floats spaced evenly)\n- loguniform: continuous log uniform (floats spaced evenly on a log scale)","7a0da902":"## 5. Bayesian Optimization Implementation <a class=\"anchor\" id=\"5\"><\/a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nEnough of theory, let's get to the implementation. In this implementation, I will walk through :-\n\n        - the workings of Bayesian Optimization.\n        - its application by means of Hyperopt.\n        \n        \nFirst of all, I will generate a synthetic dataset by using the Scikit-learn's `make_classification` library. I will generate a random binary classification dataset with 1000 samples, 100 features with 2 classes and split it in a train and test set.","7be8a5df":"### 2. Domain space\n\nThe domain space is the range of values that we want to evaluate for each hyperparameter.In each iteration of the search, the Bayesian optimization algorithm will choose one value for each hyperparameter from the domain space. In Bayesian optimization this space has probability distributions for each hyperparameter value rather than discrete values. When first tuning a model, we should create a wide domain space centered around the default values and then refine it in subsequent searches.\n\nNow, I will define the domain space as follows:-","23d6777c":"In the objective-function, I implement cross-validation. Once the cross validation is complete, we get the mean score. We want a value to minimize. So, we take negative of score. This value is then returned as the loss key in the return dictionary.\n\nThe objective function returns a dictionary of values - loss and status.\n\nNext, I will define the domain space.","e87bfa64":"### Optimization\n\nOnce we have the four parts in place, optimization is run with fmin as follows:-","139e15b1":"### Gradient Boosting Model\n\nFor implementation purpose, I will use the Gradient Boosting Model (GBM). GBM is an ensemble boosting method based on using weak learners which are trained sequentially to form a strong learner. Mostly, weak learners are decision-trees. I choose GBM because it has vast set of parameters to tune. The parameters are related to the entire ensemble and individual decision-trees. \n\n","2e86499e":"## 1. Introduction to Bayesian Optimization <a class=\"anchor\" id=\"1\"><\/a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nWe have found the best machine learning model for a kaggle competition. Now, we try to improve our model by tuning  its hyperparameters. There are standard hyperparameter optimization techniques like GridSearch and RandomSearch. But these techniques search the full space of available parameter values. If we have small set of parameter values, then this is Ok. But with large parameter spaces, this is quite time-consuming and obviously frustrating.\n\nA popular alternative to tune the model hyperparameters is **Bayesian Optimization**. Bayesian Optimization is a probabilistic model-based technique used to find minimum of any function. This approach can yield better performance on the test set while it requires fewer iterations than random search. It takes into account\npast evaluations when choosing the optimal set of hyperparameters. Thus it chooses its parameter combinations in an informed way. In doing so, it focus on those parameters that yield the best possible scores. Thus, this technique requires less number of iterations to find the optimal set of parameter values. It ignores those areas of the parameter space that are useless. Hence, it is less time-consuming and not frustrating at all. \n\nPlease visit the following link for more information on Bayesian Optimization.\n\nhttps:\/\/github.com\/fmfn\/BayesianOptimization\n\n\n","e0208a2b":"## 6. Important points to note  <a class=\"anchor\" id=\"6\"><\/a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n- The optimal hyperparameters are those that do best in cross validation and not necessarily those that do best on the test data. When we use cross validation, we hope that these results generalize to the test data.\n\n- Even using 10-fold cross-validation, the hyperparameter tuning overfits to the training data. The best score from cross-validation is significantly higher than that on the test data.\n\n- Random search may return better hyperparameters just by luck. Bayesian optimization is not guaranteed to find better hyperparameters and can get stuck in a local minimum of the objective function.","0098221e":"### 1. Objective function\n\nOur aim is to minimize the objective function. It takes in a set of values as input - in this case hyperparameters of GBM model - and outputs a real value to minimize - the cross validation loss. \n\nI will write the objective function for the GBM model with 5-fold cross validation.","5873b4ad":"## 2. Bayesian Optimization Method <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\nBayesian optimization is also called **Sequential Model-Based Optimization (SMBO)**. It finds the value that minimizes an objective function by building a surrogate function. A surrogate function is nothing but a probability model based on past evaluation results of the objective. In the surrogate function, the input values to be evaluated are selected based on the criteria of expected improvement. Bayesian methods use past evaluation results to choose the next input values. So, this method excludes the poor input values and limit the evaluation of the objective function by choosing the next input values which have done well in the past.\n\nNowadays, there are a number of Python libraries that enable us to implement Bayesian Optimization for machine learning models. The examples of libraries are Spearmint, Hyperopt or SMAC. Scikit-learn also provides a library named **Scikit-optimize** for Bayesian optimization.\n\nBayesian Optimization methods differ in how they construct the surrogate function. Spearmint uses Gaussian Process surrogate while SMAC uses Random Forest Regression. Hyperopt uses the Tree Parzen Estimator (TPE) for optimization. \n\n\n","1323b9f3":"Hello friends,\n\nIn this kernel, I will walk through the **Bayesian Optimization process using Hyperopt**. Bayesian Optimization process is used to find optimal set of parameters for any machine learning model. ","e5eebaa5":"Next, I will define the optimization algorithm.","119a48c3":"## 4. 4 parts of Optimization Problem <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\nImplementing an optimization problem in Hyperopt requires 4 parts as follows:-\n\n\n**1. Objective function**\n\nThe objective function can be any function which returns a real value that we want to minimize. In this case, we want to minimize the validation error of a machine learning model with respect to the hyperparameters. If the real value is accuracy, then we want to maximize it. Then the function should return the negative of that metric. \n\n**2. Domain space**\n\nThe domain space is the input values over which we want to search.\n\n**3. Optimization algorithm**\n\nIt is the method used to construct the surrogate objective function and choose the next values to evaluate.\n\n**4. Results**\n\nResults are score or value pairs that the algorithm uses to build the model.\n","af556ed8":"## 7. Conclusion <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n\n- In this kernel, I demonstrated the idea of Bayesian Optimisation by using Hyperopt.\n\n- Bayesian optimisation chooses the next hyperparameters in an informed way, and as such spends more time evaluating areas of the parameter distribution it believes have the highest chance of bringing a cross-validation score improvement versus previous iterations.\n\n- This can result in fewer evaluations of the objective function and better generalisation performance on the test set compared to random or grid search.\n\n- The relative benefits of Bayesian Optimisation differ with the number of dimensions of the dataset and the size of the parameter grid. The larger the dataset and or the parameter grid, the higher the potential for efficacy gains.\n\n- Random Search can still outperform Bayesian Optimisation as it could bump onto the optimal set of hyperparameters right at the start \u2014 just by luck.","040d3206":"### Results\n\nWithin each iteration, the algorithm chooses new hyperparameter values from the surrogate function which is constructed based on the previous results and evaluates these values in the objective function. This continues for num_eval evaluations of the objective function with the surrogate function continually updated with each new result. \n\nThe `best_param` object yield the results. The `best_param` object that is returned from fmin contains the hyperparameters that yielded the lowest loss on the objective function. Once we have these hyperparameters, we can use them to train a model on the full training data and then evaluate on the test data.\n","a6128b2e":"Your comments and feedback are most welcome.","a2613af2":"[Go to Top](#0)","31374b1a":"Now, I will write the 4 parts of the optimization problem as described earlier. The first is the objective function:-\n\n","ac93ab11":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Introduction to Bayesian Optimization](#1)\n1. [Bayesian Optimization method](#2)\n1. [Introduction to Hyperopt](#3)\n1. [4 parts of optimization problem](#4)\n   1. [Objective function](#4.1)\n   1. [Domain space](#4.2)\n   1. [Optimization algorithm](#4.3)\n   1. [Results](#4.4)\n1. [Bayesian Optimization implementation](#5)\n   1. [Optimization](#5.1)\n   1. [Results](#5.2)\n1. [Important points to note](#6)\n1. [Conclusion](#7)\n\n","f6c60521":"**I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated**","a11392ed":"Writing to a csv file means we can check the progress by opening the file while training. Every time the objective function is called, it will write one line to this file.","84855cc6":"### 3. Optimization algorithm\n\nWriting the optimization algorithm in hyperopt is very simple. It just involves a single line of code. We should use the `Tree Parzen Estimator (tpe)`. The code snippet is as follows:-\n\n`from hyperopt import tpe`\n\n`tpe_algorithm = tpe.suggest`\n\n\nHyperopt  has the TPE option along with random search. During optimization, the TPE algorithm constructs the probability model from the past results and decides the next set of hyperparameters to evaluate in the objective function by maximizing the expected improvement.\n\n\nNext, we should document the results history.","6a8980ab":"## 3. Introduction to Hyperopt  <a class=\"anchor\" id=\"3\"><\/a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nBayesian Optimization technique uses **Hyperopt** to tune the model hyperparameters. **Hyperopt** is a Python library which is used to tune model hyperparameters. \n\n\nMore information on Hyperopt can be found at the following link:-\n\n\nhttps:\/\/hyperopt.github.io\/hyperopt\/?source=post_page\n\n","6ed84730":"I hope you find this kernel useful and enjoyable.\n\n","330584fe":"### 4. Results history\n\nHyperopt will track the results internally for the algorithm. To take a look into the results, we can use a `Trials` object which will store basic training information and also the dictionary returned from the objective function (which includes the loss and params). All we need is one line of code to make a `Trials` object. The code snippet is as follows:-\n\n`from hyperopt import Trials`\n\n`trials = Trials()`\n"}}