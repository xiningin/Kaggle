{"cell_type":{"2813b992":"code","66a6ee04":"code","43a93d36":"code","3b341999":"code","ec831afc":"code","158724b2":"code","9714c4bd":"code","f40660a9":"code","06022d65":"code","0c92b07f":"code","58ab3bbc":"markdown","3c629960":"markdown","6f183c6b":"markdown","275e9e95":"markdown","04264af8":"markdown","aa708bb8":"markdown","031e923f":"markdown","1bea6480":"markdown","99dfa98c":"markdown","7a94cc73":"markdown","de4c5e23":"markdown","6c884305":"markdown","d5fd0aab":"markdown","dcf5a00a":"markdown","64188d97":"markdown","197bd324":"markdown","49c9aa32":"markdown","b85c44c4":"markdown","d79977b1":"markdown","00ff3c48":"markdown","58c4539e":"markdown","b0742e8a":"markdown"},"source":{"2813b992":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","66a6ee04":"from tensorflow.keras.preprocessing.image  import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        rescale=1.\/255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        validation_split=0.2\n        )\n\ntest_datagen = ImageDataGenerator()","43a93d36":"train_gen = train_datagen.flow_from_directory(\"..\/input\/intel-image-classification\/seg_train\/seg_train\", \n                                              shuffle = True, \n                                              batch_size = 1, \n                                              target_size = (64,64),\n                                              class_mode = \"categorical\",\n                                              subset = \"training\"\n                                             )\n\nval_gen = train_datagen.flow_from_directory(\"..\/input\/intel-image-classification\/seg_train\/seg_train\", \n                                              shuffle = True, \n                                              batch_size = 1, \n                                              target_size = (64,64),\n                                              class_mode = \"categorical\",\n                                              subset = \"validation\"\n                                             )\n\ntest_gen = test_datagen.flow_from_directory(\"..\/input\/intel-image-classification\/seg_test\/seg_test\", \n                                              shuffle = True, \n                                              batch_size = 1, \n                                              target_size = (64,64),\n                                              class_mode = \"categorical\")","3b341999":"labels = train_gen.class_indices\nlabels","ec831afc":"x_train = np.concatenate([train_gen.next()[0] for i in range(len(train_gen))])\ny_train = np.concatenate([train_gen.next()[1] for i in range(len(train_gen))])\n\nprint(\"X_Train Shape : \", x_train.shape)\nprint(\"Y_Train Shape : \", y_train.shape)","158724b2":"x_test = np.concatenate([test_gen.next()[0] for i in range(len(test_gen))])\ny_test = np.concatenate([test_gen.next()[1] for i in range(len(test_gen))])\n\nprint(\"X_Test Shape : \", x_test.shape)\nprint(\"Y_Test Shape : \", y_test.shape)","9714c4bd":"x_val = np.concatenate([val_gen.next()[0] for i in range(len(val_gen))])\ny_val = np.concatenate([val_gen.next()[1] for i in range(len(val_gen))])\n\nprint(\"X_Val Shape : \", x_val.shape)\nprint(\"Y_Val Shape : \", y_val.shape)","f40660a9":"from keras import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\nfrom keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV","06022d65":"def get_cnn_model(activation = \"linear\", optimizer = \"Adam\"):\n    model = Sequential()\n    model.add(Conv2D(input_shape = (64,64,1), filters = 20, kernel_size = (5,5), activation = \"relu\", padding = \"same\"))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Dropout(0.2))\n    model.add(Conv2D(filters = 40, kernel_size = (3,3), activation = \"relu\", padding = \"same\"))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n\n    model.add(Conv2D(filters=80, kernel_size=(3,3), activation=\"relu\", padding=\"same\"))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(units=512, activation = \"relu\"))\n    model.add(Dense(units=len(labels), activation = \"softmax\"))\n    \n    model.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])\n    \n    return model","0c92b07f":"activation = ['softmax', 'relu', 'tanh', 'sigmoid', 'linear']\noptimizer = ['SGD', 'Adam', 'Adamax']\nparam_grid = dict(activation = activation, optimizer = optimizer)\n\nclf = KerasClassifier(build_fn = get_cnn_model, epochs = 50, batch_size = 32, verbose = 0)\n\nmodel = GridSearchCV(estimator= clf, param_grid=param_grid, n_jobs=-1)\n#model.fit(x_train, y_train, validation_data=(x_val, y_val))","58ab3bbc":"![image.png](attachment:0d4c6610-7a5c-4d94-a6da-f7b047e1fae7.png)","3c629960":"# Data Preprocessing","6f183c6b":"**Using ImageDataGenerator is too slow. Therefore we have to convert this to numpy array.**","275e9e95":"# Hyper Parameter Tuning","04264af8":"**Momentum**\n\nMomentum helps to know the direction of the next step with the knowledge of the previous steps. It helps to prevent oscillations. A typical choice of momentum is between 0.9 to 0.9999.","aa708bb8":"# HyperParameters","031e923f":"**Learning Rate**\n\nLearning rate defines how quickly a network updates it's parameters\nLow learning rate slows down the learning process but converges smoothly. Larger learning rate speeds up the learning but may not converge.\nUsually a decaying Learning rate is preferred.","1bea6480":"**Random Search**\n\nIn Random Search, we create a grid of hyperparameters and train\/test our model on just some random combination of these hyperparameters.","99dfa98c":"**Number of epochs**\n\nNumber of epochs is the number of times the whole training data is shown to the network while training.\nIncrease the number of epochs until the validation accuracy starts decreasing even when training accuracy is increasing(overfitting).","7a94cc73":"# Create Models","de4c5e23":"**Manual Search**\n\nWhen using Manual Search, we choose some model hyperparameters based on our judgment. We then train the model, evaluate its accuracy and start the process again. This loop is repeated until a satisfactory accuracy is scored. Briefly, this search is not actually search :)","6c884305":"![image.png](attachment:9d7bbf1f-26ed-47f0-8cb5-d063458911fa.png)","d5fd0aab":"In this notebook I'll create a model for Intel Image Dataset. After that I am going to try w\u015fth various hyperparameters.\nLet's start with import libraries for basic data preprocessing.","dcf5a00a":"Using Cross-Validation when implementing Hyperparameters optimization can be really important. In this way, we might avoid using some Hyperparameters which works really good on the training data but not so good with the test data.","64188d97":"![image.png](attachment:e10b9c51-cf4e-48b1-ad28-b130999042a9.png)","197bd324":"**Batch size**\n\nMini batch size is the number of sub samples given to the network after which parameter update happens.\nA good default for batch size might be 32. Also try 32, 64, 128, 256, and so on.","49c9aa32":"# Methods used to find out Hyperparameters","b85c44c4":"And also \u0131 am going to try use multiple callbacks and explanation of this.","d79977b1":"**Bayesian optimization**\n\nThe Bayesian optimization builds a probabilistic model to map hyperparmeters to the objective fuction. It computes the posterior predictive distribution. It iteratively evaluates a promising hyperparameter configuration, and updates the priors based on the data, to form the posterior distribution of the objective function and tries to find the location of the optimum. The Bayesian optmization balances exploration and exploitation so is suited for functions that are expensive to evaluate.","00ff3c48":"**Grid Search**\n\nThe grid search is an exhaustive search through a set of manually specified set of values of hyperparameters. It means you have a set of models (which differ from each other in their parameter values, which lie on a grid). What you do is you then train each of the models and evaluate it using cross-validation. You then select the one that performed best.","58c4539e":"One of the most common used Cross-Validation methods is K-Fold Validation. In K-Fold, we divide our training set into N partitions and then iteratively train our model using N-1 partitions and test it with the left-over partition (at each iteration we change the left-over partition). Once having trained N times the model we then average the training results obtained in each iteration to obtain our overall training performance results","b0742e8a":"*When I trying the train a model always I have doubt which can I train better?\nTherefore we have to tune HyperParameter. There are many ways to do it. For example Regularization, Gradient Checking, Batch and Mini-Batch Gradient Descent e.g.*"}}