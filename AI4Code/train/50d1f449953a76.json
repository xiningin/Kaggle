{"cell_type":{"f54536b2":"code","0f720d63":"code","374622ea":"code","0a7d1d86":"code","6c1d1d66":"code","7102c105":"code","d28310e0":"code","a6328a95":"code","40c3078c":"code","9313a027":"code","17096da8":"code","1ecaadab":"code","2e0db745":"code","37bc73c1":"code","61a8db4f":"code","ab141f08":"code","8d19cff8":"code","b1ba70c6":"code","e908f584":"code","5e14c616":"code","9d184e6a":"code","3e3eb057":"code","574b8a31":"code","f5d87c1f":"code","75078351":"code","cc656c46":"code","933dc43c":"code","de87f8d0":"code","38352cf0":"code","e54406ba":"code","4cd6c19b":"code","491fa574":"code","6eed3b43":"code","2b173fec":"code","46ae208f":"code","94ea2707":"code","98cb8020":"code","ec5d6eb1":"code","4b8c551b":"code","b78c40cd":"code","14e4f632":"code","abc8c947":"code","43e44d3f":"code","922f345f":"code","dbb0a53d":"code","6cfa3cb2":"code","50879418":"code","70c317b2":"code","64938cbb":"code","909c89a4":"code","3a3dad41":"code","db0b69b4":"code","30b6014c":"code","d73206cc":"code","bd50852c":"code","b4f45010":"code","28bcfc1f":"code","035a11ca":"code","786f1163":"code","bc42b2c8":"code","fb97d81d":"code","dd1b15c7":"code","b7fd2d79":"code","38b52070":"code","d98f3ddc":"code","716921be":"code","d2e91cbe":"code","4213363f":"code","6639e3a6":"code","9ebfe846":"code","b4c79495":"code","e3d83d21":"markdown","8f440a60":"markdown","888d2691":"markdown","254253ea":"markdown","0d701459":"markdown","9d76823f":"markdown","02cee9e6":"markdown","95f58446":"markdown","5ef7af12":"markdown","f3f83873":"markdown","0039473c":"markdown","820e1605":"markdown","d0a401ba":"markdown","6d9f1b7d":"markdown","b5e0678e":"markdown","93cb6da3":"markdown"},"source":{"f54536b2":"#Importing Libraries\nimport os\nimport pandas as pd # data processing\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt # plotting library\nimport seaborn as sns # plotting library","0f720d63":"# Load files\n\ntrain = pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\")\ntest = pd.read_csv(\"..\/input\/home-data-for-ml-course\/test.csv\")\n\n# train = pd.read_csv('train.csv') # training dataset\n# test = pd.read_csv('test.csv')   # same dataset, without target `SalePrice`.\n\ntrain_id = train['Id']\ntest_id = test['Id']\n\ntrain.shape, test.shape #Just checking the shape","374622ea":"corrmat = train.corr()\nplt.figure(figsize = (12, 10))\nsns.heatmap(corrmat, square=True)\nplt.title('Correlation between features');\nplt.show()","0a7d1d86":"#Plot of the second Heatmap with less features\ncorrMatrix=train[[\"SalePrice\",\"OverallQual\",\"GrLivArea\",\"GarageCars\",\n                  \"GarageArea\",\"GarageYrBlt\",\"TotalBsmtSF\",\"1stFlrSF\",\"FullBath\",\n                  \"TotRmsAbvGrd\",\"YearBuilt\",\"YearRemodAdd\"]].corr()\n\ncorrmat = train.corr()\nplt.figure(figsize = (12, 10))\nsns.heatmap(corrMatrix, square=True)\nplt.title('Correlation between less features');\nplt.show()","6c1d1d66":"numeric_features = train.select_dtypes(include=[np.number])\nnumeric_features.dtypes","7102c105":"#The first five features are the most positively correlated with SalePrice,\n#while the next five are the most negatively correlated.\ncorr = numeric_features.corr()\nprint (corr['SalePrice'].sort_values(ascending=False)[:5], '\\n')\nprint (corr['SalePrice'].sort_values(ascending=False)[-5:])","d28310e0":"#Plot of numerical values (Tried to visualise here but didn't really help out)\n%matplotlib inline  \ncorrmat.hist(bins = 20 , figsize = (20,20))\nplt.show()","a6328a95":"#Plot of the features that have the greatest impact on the price (Proud of this one)\nplt.figure(figsize = (12, 10))\ncorrmat['SalePrice'].sort_values(ascending = False).plot(kind = 'bar')\nplt.show()","40c3078c":"#Summarizing the data of OverallQual and SalePrice by using pivot_table, \nOverQuality_pivot = train.pivot_table(index='OverallQual',\n                  values='SalePrice', aggfunc=np.median)","9313a027":"OverQuality_pivot","17096da8":"#Plot of the relationship between OverallQual and SalePrice,\nOverQuality_pivot.plot(kind='bar', figsize = (12, 10))\nplt.xlabel('Overall Quality')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()","1ecaadab":"#Plot of GrLivArea feature to remove any outliers\nplt.figure(figsize = (12, 10))\nplt.scatter(train['GrLivArea'], train['SalePrice'])\nplt.show()","2e0db745":"#Plot of GrLivArea removing the outliers\ntrain = train.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 700000)].index)\nplt.figure(figsize = (12, 10))\nplt.scatter(train['GrLivArea'], train['SalePrice'])\nplt.show()","37bc73c1":"#Joining train and test datasets\ntrain_shape = train.shape\ntest_shape = test.shape\ntrain_salePrice = train['SalePrice']\ndataset = pd.concat((train, test)).reset_index(drop = True)","61a8db4f":"from scipy import stats\nfrom scipy.stats import norm, skew","ab141f08":"#Plot of the price distribution\nplt.figure(figsize = (12, 10))\nsns.distplot(train_salePrice , fit = norm)\nplt.show()","8d19cff8":"#Using the numpy fuction log1p which  applies log(1+x) to all elements of the column\nplt.figure(figsize = (12, 10))\nsns.distplot(np.log1p(train_salePrice) , fit = norm)\nplt.show()","b1ba70c6":"#Plot of columns that have NaN values\n#Cleaning the data set to have a stronger analysis by removing NaN values\n\nnan_columns = [column for column in dataset.columns if dataset[column].isnull().values.any()]\nplt.figure(figsize = (12, 10))\ndataset[nan_columns].isnull().sum().sort_values(ascending = False).plot(kind = 'bar')\nplt.show()","e908f584":"def dummy_variable(dataset, column, variable):\n    tmp = pd.get_dummies(dataset[column])\n    tmp = tmp.drop(variable, axis = 1)\n    for col in tmp.columns:\n        dataset[column , '_' , col] = tmp[col]\n    return dataset.drop(column, axis = 1)","5e14c616":"#Categorical variable (PoolQC)\ndataset['PoolQC'] = dataset['PoolQC'].fillna(0)\nd = {'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\ndataset['PoolQC'] = dataset['PoolQC'].replace(d)","9d184e6a":"#The pd.dummy_variable() for MiscFeature and removing NA column\ndataset['MiscFeature'] = dataset['MiscFeature'].fillna('NA')\ndataset = dummy_variable(dataset, 'MiscFeature', 'NA')","3e3eb057":"#The dummy_variable() for Alley and removing NA column\ndataset['Alley'] = dataset['Alley'].fillna('NA')\ndataset = dummy_variable(dataset, 'Alley', 'NA')","574b8a31":"#The dummy_variable() for Fence and removing NA column\ndataset['Fence'] = dataset['Fence'].fillna('NA')\ndataset = dummy_variable(dataset, 'Fence', 'NA')","f5d87c1f":"#Categorical variable (FireplaceQu)\ndataset['FireplaceQu'] = dataset['FireplaceQu'].fillna(0)\nd = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\ndataset['FireplaceQu'] = dataset['FireplaceQu'].replace(d)","75078351":"dataset['LotFrontage'] = dataset.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","cc656c46":"#Categorical variable (GarageQual & GarageCond)\ndataset['GarageQual'] = dataset['GarageQual'].fillna(0)\ndataset['GarageCond'] = dataset['GarageCond'].fillna(0)\nd = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\ndataset['GarageQual'] = dataset['GarageQual'].replace(d)\ndataset['GarageCond'] = dataset['GarageCond'].replace(d)","933dc43c":"#Categorical variable (GarageFinish)\ndataset['GarageFinish'] = dataset['GarageFinish'].fillna(0)\nd = {'Unf': 1, 'RFn': 2, 'Fin': 3}\ndataset['GarageFinish'] = dataset['GarageFinish'].replace(d)","de87f8d0":"#Replacing NaN values with 0\ndataset['GarageYrBlt'] = dataset['GarageYrBlt'].fillna(0)","38352cf0":"#The dummy_variable() for GarageType and removing NA column\ndataset['GarageType'] = dataset['GarageType'].fillna('NA')\ndataset = dummy_variable(dataset, 'GarageType', 'NA')","e54406ba":"#Categorical variable (BsmtExposure)\ndataset['BsmtExposure'] = dataset['BsmtExposure'].fillna(0)\nd = {'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\ndataset['BsmtExposure'] = dataset['BsmtExposure'].replace(d)","4cd6c19b":"#Categorical variable (BsmtCond & BsmtQual)\ndataset['BsmtCond'] = dataset['BsmtCond'].fillna(0)\ndataset['BsmtQual'] = dataset['BsmtQual'].fillna(0)\nd = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\ndataset['BsmtCond'] = dataset['BsmtCond'].replace(d)\ndataset['BsmtQual'] = dataset['BsmtQual'].replace(d)","491fa574":"#Categorical variable (BsmtFinType2 & BsmtFinType1)\ndataset['BsmtFinType2'] = dataset['BsmtFinType2'].fillna(0)\ndataset['BsmtFinType1'] = dataset['BsmtFinType1'].fillna(0)\nd = {'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\ndataset['BsmtFinType2'] = dataset['BsmtFinType2'].replace(d)\ndataset['BsmtFinType1'] = dataset['BsmtFinType1'].replace(d)","6eed3b43":"#The dummy_variable() for MasVnrType and removing NA column\ndataset['MasVnrType'] = dataset['MasVnrType'].fillna('None')\ndataset = dummy_variable(dataset, 'MasVnrType', 'None')","2b173fec":"#Replacing NaN values with 0 for MasVnrArea\ndataset['MasVnrArea'] = dataset['MasVnrArea'].fillna(0)","46ae208f":"#Replacing NaN values with the most frequent element because there are only 4 NaN values and then\n#using pd.get_dummies() and remove column\ndataset['MSZoning'] = dataset['MSZoning'].fillna(dataset['MSZoning'].value_counts().index[0])\ndataset = dummy_variable(dataset, 'MSZoning', 'C (all)')","94ea2707":"#Replacing NaN values with 0 for BsmtFullBath & BsmtHalfBath\ndataset['BsmtFullBath'] = dataset['BsmtFullBath'].fillna(0)\ndataset['BsmtHalfBath'] = dataset['BsmtHalfBath'].fillna(0)","98cb8020":"#Removing Utilities column because all the elements are 'AllPub' and there are only 2 NaN values\ndataset = dataset.drop('Utilities', axis = 1)","ec5d6eb1":"#Replacing NaN values with the most frequent element because there are only 2 NaN values and then\n#using pd.get_dummies() and remove column\ndataset['Functional'] = dataset['Functional'].fillna(dataset['Functional'].value_counts().index[0])\ndataset['Electrical'] = dataset['Electrical'].fillna(dataset['Electrical'].value_counts().index[0])\ndataset = dummy_variable(dataset, 'Functional', 'Sev')\ndataset = dummy_variable(dataset, 'Electrical', 'Mix')","4b8c551b":"#Replacing NaN values with 0 for BsmtFullBath & BsmtHalfBath\ndataset['BsmtUnfSF'] = dataset['BsmtUnfSF'].fillna(0)","b78c40cd":"#Replacing NaN values with the most frequent element because there are only 2 NaN values and then\n#using pd.get_dummies() and remove column\ndataset['Exterior1st'] = dataset['Exterior1st'].fillna(dataset['Exterior1st'].value_counts().index[0])\ndataset['Exterior2nd'] = dataset['Exterior2nd'].fillna(dataset['Exterior2nd'].value_counts().index[0])\ndataset = dummy_variable(dataset, 'Exterior1st', 'AsbShng')\ndataset = dummy_variable(dataset, 'Exterior2nd', 'AsphShn')","14e4f632":"#Replacing NaN values with 0 for BsmtFullBath & BsmtHalfBath\ndataset['TotalBsmtSF'] = dataset['TotalBsmtSF'].fillna(0)\ndataset['GarageCars'] = dataset['GarageCars'].fillna(0)\ndataset['BsmtFinSF2'] = dataset['BsmtFinSF2'].fillna(0)\ndataset['BsmtFinSF1'] = dataset['BsmtFinSF1'].fillna(0)","abc8c947":"#categorical variable (convertible in ordinal variable)\ndataset['KitchenQual'] = dataset['KitchenQual'].fillna(dataset['KitchenQual'].value_counts().index[0])\nd = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\ndataset['KitchenQual'] = dataset['KitchenQual'].replace(d)","43e44d3f":"#Replacing NaN values with the most frequent element because there are only 1 NaN values and then\n#using pd.get_dummies() and remove Oth column\ndataset['SaleType'] = dataset['SaleType'].fillna(dataset['SaleType'].value_counts().index[0])\ndataset = dummy_variable(dataset, 'SaleType', 'Oth')","922f345f":"#Replacing NaN values with 0 for GarageArea\ndataset['GarageArea'] = dataset['GarageArea'].fillna(0)","dbb0a53d":"#Creating a new column which contains the sum of TotalBsmtSF, 1stFlrSF, 2ndFlrSF\ndataset['TotalSF'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']","6cfa3cb2":"#Having only the non-numeric columns\ndataset.select_dtypes(exclude = [np.number]).columns","50879418":"#And converting each non-numeric column to a column that contains values of 0 or 1\nfor column in dataset.select_dtypes(exclude = [np.number]).columns:  \n    dataset = dummy_variable(dataset, column, dataset[column].value_counts().index[-1])","70c317b2":"#deleting Id and SalePrice columns \ndataset = dataset.drop(['Id', 'SalePrice'], axis = 1)","64938cbb":"#Now we can re-split the dataset\ntrain = dataset[ : train_shape[0]]\ntest = dataset[train_shape[0] : ]","909c89a4":"#check shapes of train and test (all good !)\ntrain.shape, test.shape","3a3dad41":"#Tools \nfrom sklearn.metrics import mean_squared_error # Mean squared error regression loss\nfrom sklearn.model_selection import GridSearchCV # GridSearchCV implements a \u201cfit\u201d and a \u201cscore\u201d method\nfrom sklearn.model_selection import KFold # statistical method used to estimate the skill of machine learning models.\nfrom sklearn.model_selection import cross_val_score # cross_val_score estimates the expected accuracy of your model on out-of-training data\nfrom sklearn.preprocessing import RobustScaler # This Scaler removes the median and scales the data according to the quantile range, standardization of a dataset is a common requirement for many machine learning estimators.\nfrom sklearn.pipeline import Pipeline # Sequentially apply a list of transforms and a final estimator\n\n\n# Regression model\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.linear_model import Lasso \nfrom sklearn.linear_model import Ridge \nfrom sklearn.linear_model import RidgeCV \nfrom sklearn.linear_model import ElasticNet \nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.svm import SVR \n\n# Boosting\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor \nfrom sklearn.ensemble import ExtraTreesRegressor \nfrom sklearn.ensemble import AdaBoostRegressor \nfrom xgboost import XGBRegressor","db0b69b4":"X = train\ny = np.log1p(train_salePrice.values)","30b6014c":"scoring = 'neg_mean_squared_error'","d73206cc":"# Testing various regression algorithms.\n# Creating an empty list and appending each models\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('RIDGE', Ridge()))\nmodels.append(('RIDGECV', RidgeCV()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('SVR', SVR()))","bd50852c":"#Testing various regression algorithms\n\nresults = []\nnames = []\nm, model_name = float('-inf'), ''\nfor name, model in models:\n    kfold = KFold()\n    cross_val_res = cross_val_score(model, X, y, cv = kfold, scoring = scoring)\n    print('Model:', name, '\\tMean:', cross_val_res.mean(), '\\tStd:', cross_val_res.std())\n    results.append(cross_val_res)\n    names.append(name)\n    if cross_val_res.mean() > m:\n        m = cross_val_res.mean()\n        model_name = name\nprint('\\nBest model:', '\\t' + model_name)","b4f45010":"#Plot of the statistical distribution of the chosen algorithms\n# We need to scale, we can't compare the algorithms here\nfig = plt.figure(figsize = (12, 10))\nfig.suptitle('Algorithm comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","28bcfc1f":"#testing various regression algorithms with the feature scaling\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', RobustScaler()), ('LR', LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', RobustScaler()), ('LASSO', Lasso())])))\npipelines.append(('ScaledRIDGE', Pipeline([('Scaler', RobustScaler()), ('RIDGE', Ridge())])))\npipelines.append(('ScaledRIDGECV', Pipeline([('Scaler', RobustScaler()), ('RIDGECV', RidgeCV())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', RobustScaler()), ('EN', ElasticNet())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', RobustScaler()), ('CART', DecisionTreeRegressor())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', RobustScaler()), ('KNN', KNeighborsRegressor())])))\npipelines.append(('ScaledSVR', Pipeline([('Scaler', RobustScaler()), ('SVR', SVR())])))","035a11ca":"results = []\nnames = []\nm, model_name = float('-inf'), ''\nfor name, model in pipelines:\n    kfold = KFold()\n    cross_val_res = cross_val_score(model, X, y, cv = kfold, scoring = scoring)\n    print('Model:', name, '\\tMean:', cross_val_res.mean(), '\\tStd:',cross_val_res.std())\n    results.append(cross_val_res)\n    names.append(name)\n    if cross_val_res.mean() > m:\n        m = cross_val_res.mean()\n        model_name = name\nprint('\\nBest model:', '\\t' + model_name)","786f1163":"#Plot of the statistical distribution of the chosen algorithms\nfig = plt.figure(figsize = (12, 10))\nfig.suptitle('Algorithm comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","bc42b2c8":"#RidgeCV tuning (finding the best alphas parameter) The number of sequential trees to be modeled\nalpha_values = []\nvalue = 27.0\nwhile value <= 30.0:\n    alpha_values.append([value])\n    value += 0.1\nparam_grid = dict(alphas = alpha_values)\nkfold = KFold()\ngrid = GridSearchCV(estimator = RidgeCV(), param_grid = param_grid, scoring = scoring, cv = kfold)\ngrid_result = grid.fit(X, y)","fb97d81d":"print('Best score: %f using %s\\n' % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('%f (%f) with: %r' % (mean, stdev, param))\n    \nbest_alpha = grid_result.best_params_['alphas']","dd1b15c7":"#testing various boosting algorithms\n# create an empty list and appending each models\nmodels = []\nmodels.append(('AB', AdaBoostRegressor()))\nmodels.append(('GBM', GradientBoostingRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('XGB', XGBRegressor()))\nmodels.append(('ET', ExtraTreesRegressor()))","b7fd2d79":"results = []\nnames = []\nm, model_name = float('-inf'), ''\nfor name, model in models:\n    kfold = KFold()\n    cross_val_res = cross_val_score(model, X, y, cv = kfold, scoring = scoring)\n    print('Model:', name, '\\tMean:', cross_val_res.mean(), '\\tStd:',cross_val_res.std())\n    results.append(cross_val_res)\n    names.append(name)\n    if cross_val_res.mean() > m:\n        m = cross_val_res.mean()\n        model_name = name\nprint('\\nBest model:', '\\t' + model_name)","38b52070":"#Plot of the statistical distribution of the chosen algorithms\nfig = plt.figure(figsize = (12, 10))\nfig.suptitle('Algorithm comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","d98f3ddc":"#Testing various boosting algorithms with the feature scaling\nensembles = []\nensembles.append(('ScaledAB', Pipeline([('Scaler', RobustScaler()),('AB', AdaBoostRegressor())])))\nensembles.append(('ScaledGBM', Pipeline([('Scaler', RobustScaler()),('GBM', GradientBoostingRegressor())])))\nensembles.append(('ScaledRF', Pipeline([('Scaler', RobustScaler()),('RF', RandomForestRegressor())])))\nensembles.append(('ScaledXGB', Pipeline([('Scaler', RobustScaler()),('XGB', XGBRegressor())])))\nensembles.append(('ScaledET', Pipeline([('Scaler', RobustScaler()),('ET', ExtraTreesRegressor())])))","716921be":"results = []\nnames = []\nm, model_name = float('-inf'), ''\nfor name, model in ensembles:\n    kfold = KFold()\n    cross_val_res = cross_val_score(model, X, y, cv = kfold, scoring = scoring)\n    print('Model:', name, '\\tMean:', cross_val_res.mean(), '\\tStd:',cross_val_res.std())\n    results.append(cross_val_res)\n    names.append(name)\n    if cross_val_res.mean() > m:\n        m = cross_val_res.mean()\n        model_name = name\nprint('\\nBest model:', '\\t' + model_name)","d2e91cbe":"# Plot of the statistical distribution of the chosen algorithms \n# (We're trying to have the mean that is closer to 0)\nfig = plt.figure(figsize = (12, 10))\nfig.suptitle('Algorithm comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","4213363f":"#GBM tuning (finding the best n_estimators parameter) The number of sequential trees to be modeled\n\nparam_grid = dict(n_estimators = np.array([350, 355, 360, 365, 370, 375, 380, 385, 390, 395, 400]))\nmodel = GradientBoostingRegressor()\nkfold = KFold()\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, scoring = scoring, cv = kfold)\ngrid_result = grid.fit(X, y)","6639e3a6":"print('Best score: %f using %s\\n' % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('%f (%f) with: %r' % (mean, stdev, param))\n    \nbest_n_estimators = grid_result.best_params_['n_estimators']","9ebfe846":"#Ensembling RidgeCV (70%) and Gradient Boosting (30%) without feature scaling\nprint('Predicting house prices with:\\n')\nprint('RidgeCV(alphas = ' + str(best_alpha) + ')')\nprint('GradientBoostingRegressor(n_estimators = '+ str(best_n_estimators) + ')')\n\nridge_cv = RidgeCV(alphas = best_alpha)\nridge_cv_predictions = ridge_cv.fit(X, y).predict(test)\n\ngbm = GradientBoostingRegressor(n_estimators = best_n_estimators)\ngbm_predictions = gbm.fit(X, y).predict(test)\n\npredictions = np.expm1(ridge_cv_predictions) * 0.7 + np.expm1(gbm_predictions) * 0.3","b4c79495":"# Prepare submission CSV to send to Kaggle\nsubmission = pd.DataFrame()\nsubmission['Id'] = test_id\nsubmission['SalePrice'] = predictions\nsubmission.to_csv('submission.csv', index = False)\nsubmission","e3d83d21":"# Kaggle Project\n\n\nMy first project in Kaggle.","8f440a60":"\n> # Nader Narcisse\n>\n> # Best Rank : 346 \/ 28,719\n>\n> ### Best Score : 12788.64722\n\n> ![Boosting](https:\/\/scontent-cdg2-1.xx.fbcdn.net\/v\/t1.15752-9\/93683533_2615263418692205_427952645907415040_n.png?_nc_cat=111&_nc_sid=b96e70&_nc_ohc=dRVJIQpVwfQAX966cBv&_nc_ht=scontent-cdg2-1.xx&oh=26004d728382374317ff6835e0dee49c&oe=5EC35E86)\n\nI'm still submitting to see if I can go up the leaderboard (the screen shot might not be relevant anymore)\n\nLink to my Kaggle:\n> https:\/\/www.kaggle.com\/nadernarcisse\/competitions","888d2691":"# Choosing the best regression model\n\nMy objective here is to see what is the best model to apply on my sets \n\nI choosed here a couple of regression model, I saw what model other people used and selected the models that seemed to be doing great on this competition but also the tools that i will be using to make sure that my model and my datasets are pre-tested accurately.\n\nDocumentation : https:\/\/scikit-learn.org\/stable\/supervised_learning.html\n\nOn kaggle the Boosting algorithm are pretty popular amongst competitions and they are well reputated\n\"Boosting\" is an ensemble method for improving the model predictions of any given learning algorithm. I choose to add some in this notebook.\n\nGreat sources that helped me understand the concept :\n\n- https:\/\/towardsdatascience.com\/boosting-algorithms-explained-d38f56ef3f30\n\n- https:\/\/towardsdatascience.com\/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f\n\n- https:\/\/www.kaggle.com\/kashnitsky\/topic-10-gradient-boosting\n\n![Boosting](https:\/\/miro.medium.com\/max\/1400\/0*qCcM7uCOqIw6npnJ.png)\n\n\n\nI'm basically doing a competition for the best models out of the one I selected. :)","254253ea":"# Outliers","0d701459":"### KDE\n\nSimple look on the price distribution using scipy, simple and useful.\n\n> Documentation : https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html","9d76823f":"# Saving the predictions","02cee9e6":"## Missing Data\n\nThe problem with missing data is that there is no fixed way of dealing with them, and the problem is universal. Missing values affect our performance and predictive capacity. They have the potential to change all our statistical parameters. The way they interact with outliers once again affects our statistics.\n\nI'm choosing to use the median to replace the NaN values. Median is the middle score of data-points when arranged in order. And unlike the mean, the median is not influenced by outliers of the data set.\n\n> Sources : https:\/\/www.freecodecamp.org\/news\/the-penalty-of-missing-values-in-data-science-91b756f95a32\/","95f58446":"<b> n_estimators <\/b> : This is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower.\n\n> Sources :\n>\n> https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n\nAbout <i>\"Grid Search with Cross Validation\"<\/i>","5ef7af12":"We notice that the median sales price strictly increases as Overall Quality increases.\nWhich is not a suprise.","f3f83873":"## XGBoost what's wrong ?\n\nNow I'm suprised ! \nHow could XGBoost didn't do a better performance than GBM ? \n\n\"Performance depends on dataset and its properties. Boosting is not a silver bullet solution. Simpler models like logistic regression for binary classification has higher generalization ability over boosting.\"\n\n> Great discussion about it : https:\/\/www.kaggle.com\/questions-and-answers\/110226\n\n\nThough GBM is fairly robust at higher number of trees but it can still overfit at a point. Hence, this should be tuned using <b> Cross Validation (CV) <\/b> for a particular learning rate.\n\n> Great source : https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/\n","0039473c":"# Ensembling Models\n\n> <b> Two is better than one: Ensembling Models <\/b>\n\nJust as diversity in nature contributes to more robust biological systems, ensembles of ML models produce stronger results by combining the strengths (and compensating for the weaknesses) of multiple submodels.\n\n>Sources :\n>\n>https:\/\/towardsdatascience.com\/two-is-better-than-one-ensembling-models-611ee4fa9bd8\n>\n>https:\/\/www.kdnuggets.com\/2019\/09\/ensemble-learning.html\n>\n>https:\/\/towardsdatascience.com\/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\n\nUsing 70\/30 method\n\n70% for training and 30% for validation.","820e1605":"# Loading & Visualising Data\n\nTo start this competition I need to visualise the data so I can spot the features that have the greatest impact on the price.\n\nI want to use a heatmap in this case.\n\nA heat map is a two-dimensional representation of information with the help of colors. \nIt is very useful in visualizing the concentration of values between two dimensions of a matrix.\n- Heatmaps can help the user visualize simple or complex information.\n- Heatmaps are used in many areas such as defense, marketing and understanding <b> consumer behavior<\/b>.\n\nThis helps in finding patterns and gives a perspective of depth.\n","d0a401ba":"## Merging Datasets\nOne important reason to combine datasets is to maintain consistency between the sets.\n\nGenerally, machine learning models aren't great at <i> extrapolation <\/i>, so if there's data in the test set that isn't well represented in the training set (whether the result of noise or not), the prediction probably won't be reliable.","6d9f1b7d":"## Avoiding the Dummy Variable Trap\n\nThis function <i>dummy_variable() <\/i> takes in input a dataset, a column, a variable and converts the columns values in 0 or 1 and finally removes the column of the variable given in input to avoid the so called \"Dummy Variable Trap\"\n\nGreat sources that helped me understand the concept :\n\n>https:\/\/www.youtube.com\/watch?v=QER-3u0YN5Q\n>\n>https:\/\/www.youtube.com\/watch?v=g9aLvY8BfRM","b5e0678e":"I conclude that <b> OverallQual <\/b> (Overall material and finish quality) and <b> GrLivArea <\/b>  (Above grade (ground) living area square feet) features have the greatest impact on the price","93cb6da3":"## How can we rank the algorithms you ask?\n\nIn classification problems when multiples algorithms are applied to different benchmarks a difficult issue arises, \n><b> How can we rank the algorithms? <b>\n    \nIn machine learning it is common run the algorithms several times and then a statistic is calculated in terms of <b> means <\/b> and <b> standard deviations (std) <\/b>. In order to compare the performance of the algorithms, it is very common to employ statistical tests. \n\n- <b> std <\/b> : Using this metric to calculate the variability of a population or sample is a crucial test of a machine learning model\u2019s accuracy against real world data. In addition, standard deviation can be used to measure confidence in a model\u2019s statistical conclusions.\n\n- <b> mean <\/b> : The statistical mean refers to the mean or average that is used to derive the central tendency of the data in question. It is determined by adding all the data points in a population and then dividing the total by the number of points. The resulting number is known as the mean or the average.\n\nI'm using the XGBoost, because this algorithm re-implements the tree boosting and gained popularity by winning Kaggle and other data science competition wondering if it will be efficient in this competition.\n\n> Great source : https:\/\/medium.com\/syncedreview\/tree-boosting-with-xgboost-why-does-xgboost-win-every-machine-learning-competition-ca8034c0b283\n\nIn this competition we're having datasets that have house <b> prices <\/b>. It's important to make sure that we scale the features.\n\nGreat example that helped me understand the concept:\n\n- \"For example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\"\n\n> https:\/\/www.kaggle.com\/rtatman\/data-cleaning-challenge-scale-and-normalize-data\n> https:\/\/medium.com\/greyatom\/why-how-and-when-to-scale-your-features-4b30ab09db5e\n"}}