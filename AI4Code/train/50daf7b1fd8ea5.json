{"cell_type":{"c12aec36":"code","a4ed8874":"code","586772ec":"code","524bc880":"code","2464ecb9":"code","1a7f6187":"code","db6473bd":"code","8447f911":"code","07bffabe":"code","8edf0726":"code","98d4a979":"markdown","77d36973":"markdown","f275ecb0":"markdown","16213a56":"markdown","dd0f5b60":"markdown","b0da6de3":"markdown","9914a245":"markdown","cdf508dc":"markdown","6dee92ee":"markdown","29a0b4ae":"markdown"},"source":{"c12aec36":"MODEL_NAMES = [\n    'bert-large-cased',\n    'openai-gpt',\n    #'gpt2-large',\n    #'xlnet-large-cased',\n    #'xlm-mlm-en-2048',\n    'roberta-large',\n    'distilbert-base-cased',\n    #'ctrl',\n    'camembert-base',\n    'albert-large-v2',\n    #'t5-large',\n    'flaubert\/flaubert_large_cased',\n    'facebook\/bart-large-cnn',\n    'moussaKam\/mbarthez',\n    #'DialoGPT-large',\n    #'facebook\/m2m100_418M',\n    'allenai\/longformer-large-4096',\n    #'lxmert-base-uncased',\n    #'funnel-transformer\/small',\n    #'funnel-transformer\/xlarge',\n    'microsoft\/layoutlm-large-uncased',\n    'microsoft\/deberta-xlarge-v2',\n    'squeezebert\/squeezebert-mnli-headless',\n    'camembert\/camembert-base-wikipedia-4gb',\n    'chkla\/roberta-argument',\n]","a4ed8874":"import torch\nfrom transformers import AutoTokenizer, AutoModel\n\nassert torch.cuda.is_available(), 'No CUDA!'\ndevice = torch.device('cuda')\n\ndef get_model_info(model_name: str):    \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(model_name).to(device)\n    return model, tokenizer,","586772ec":"from torch.nn import Sequential\n\ndef get_last_two_layers_in_sequence(seq: Sequential):   \n    seq_size = len(seq)\n    if seq_size == 0:\n        return None, None,\n    elif seq_size == 1:\n        return None, seq[seq_size - 1],\n    else:\n        return seq[seq_size - 2], seq[seq_size - 1],\n\n    \ndef get_last_two_layers(model):\n    prev_layer = None\n    last_layer = None\n    module_dict = model._modules\n    for key in module_dict:\n        module = module_dict[key]\n        if isinstance(module, Sequential):\n            m_prev, m_last = get_last_two_layers_in_sequence(module)                        \n        else:\n            m_prev, m_last = get_last_two_layers(module)\n        if m_last is None:\n            m_last = module\n        prev_layer = m_prev if m_prev is not None else last_layer\n        last_layer = m_last\n    return prev_layer, last_layer,\n\n\ndef get_last_hidden_layer(model):\n    prev, _ = get_last_two_layers(model)\n    return prev","524bc880":"import torch\n\ndef get_features(model, tokenizer, layer, out_shape, text: str):\n    vector_buffer = torch.zeros(out_shape)\n    def _local_hook(_, _input, _output):\n        nonlocal vector_buffer\n        vector_buffer.copy_(_output.data)\n    tensors = tokenizer(text, return_tensors='pt')\n    fh = layer.register_forward_hook(_local_hook)\n    model(**tensors.to(device))\n    fh.remove()\n    return torch.flatten(vector_buffer)","2464ecb9":"def get_features_shape_mn(model_name: str, test_text='hello', for_input=False):\n    model, tokenizer = get_model_info(model_name)\n    layer = get_last_hidden_layer(model)\n    return get_features_shape(model, tokenizer, layer, test_text=test_text, for_input=for_input)\n\n\ndef get_features_shape(model, tokenizer, layer, test_text='hello', for_input=False):\n    t_dims = None\n    def _local_hook(_, _input, _output):\n        nonlocal t_dims\n        t_dims = _input[0].size() if for_input else _output.size()\n        return _output \n    tensors = tokenizer(test_text, return_tensors='pt')\n    fh = layer.register_forward_hook(_local_hook)\n    model(**tensors.to(device))\n    fh.remove()\n    return t_dims","1a7f6187":"import sys\nimport gc\nimport pandas as pd\n\ndef flattened_size(size):\n    p = 1\n    for i in range(len(size)):\n        p *= size[i]\n    return p\n\n\nfeature_counts = []\nfor model_name in MODEL_NAMES:\n    try:\n        model, tokenizer = get_model_info(model_name)\n        layer = get_last_hidden_layer(model)\n        shape1 = get_features_shape(model, tokenizer, layer, test_text='hello')\n        num_features = flattened_size(shape1)\n        shape2 = get_features_shape(model, tokenizer, layer, test_text='hello world')\n        tokens_dependent = flattened_size(shape2) != num_features\n        feature_counts.append((model_name, num_features, tokens_dependent))\n        del model\n        del tokenizer\n        gc.collect()\n    except:\n        print('Model failed: %s |' % model_name, sys.exc_info()[0])\nfeature_count_frame = pd.DataFrame(feature_counts, columns=['model_name', 'num_features', 'tokens_dependent'])\nfeature_count_frame","db6473bd":"import numpy as np\nimport pandas as pd\n\ndef transform_dataset(data_frame: pd.DataFrame, model, tokenizer, layer, features_shape):\n    y = []\n    x = []\n    for index, row in data_frame.iterrows():\n        text = row['excerpt']\n        label = row['target']\n        features = get_features(model, tokenizer, layer, features_shape, text)\n        x.append(features.detach().numpy())\n        y.append(label)\n    return np.array(x), np.array(y),","8447f911":"from sklearn.neighbors import BallTree\n\ndef leave_one_out_knn_predictions(x, y: np.ndarray, k: int):\n    ball_tree = BallTree(x, leaf_size=10)\n    idx_matrix = ball_tree.query(x, k=k+1, return_distance=False)\n    loo_idx_matrix = idx_matrix[:,1:]\n    return np.array([np.mean(y[idx_row]) for idx_row in loo_idx_matrix])","07bffabe":"%%time\n\nfrom sklearn.metrics import mean_squared_error\n\n\ndef evaluate_models(model_names):\n    train_data = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\n    results_matrix = []\n    for index, row in feature_count_frame.iterrows():\n        model_name = row['model_name']\n        if row['tokens_dependent']:\n            print('Skipping %s' % model_name)\n            continue\n        print('Evaluating %s ...' % model_name)\n        model, tokenizer = get_model_info(model_name)\n        layer = get_last_hidden_layer(model)\n        shape = get_features_shape(model, tokenizer, layer)\n        x, y = transform_dataset(train_data, model, tokenizer, layer, shape)\n        pred_y = leave_one_out_knn_predictions(x, y, k=10)\n        rmse = mean_squared_error(pred_y, y, squared=False)\n        results_matrix.append([model_name, rmse])\n        del model\n        del tokenizer\n        del x\n        gc.collect()\n    return pd.DataFrame(results_matrix, columns=['model_name','rmse'])\n\n\nmodel_eval_frame = evaluate_models(MODEL_NAMES)\nmodel_eval_frame = model_eval_frame.sort_values('rmse')","8edf0726":"import numpy as np\nimport plotly.express as px\n\ndef plot_bar_chart(data, x_var: str, y_var: str, title='', x_label='', y_label=''):\n    fig = px.bar(data, x=x_var, y=y_var, title=title,\n                 labels={x_var: x_label, y_var: y_label})\n    fig.show()\n    \n\nplot_bar_chart(model_eval_frame, 'model_name', 'rmse',\n               title='Text model comparison',\n               x_label='Model name', y_label='k-NN RMSE')","98d4a979":"## Number of features\nTo find the shape of the feature tensor, we use a layer hook that captures the shape of the layer's output.","77d36973":"## Evaluation\nNot we put everything together and run an evaluation routine. Models are skipped if the number of features depends on the number of tokens. Cross-validation RMSE of remaining model feature sets is shown in a bar chart below.","f275ecb0":"## Using k-NN to evaluate feature sets\nIf we were to use a linear regressor or a neural network to evaluate feature sets, we'd have to consider that larger feature sets are at a disadvantage, statistically. (The competition's training dataset isn't very large.) Plus regularization parameters depend on the number of features. Even with a random forest or a GBM, the number of trees would need to be adjusted according to the number of features. The k-NN algorithm does not have these issues.\n\nAdditionally, we can quickly do leave-one-out cross-validation with k-NN, and we're defining a custom function that does this, with the help of *sklearn*'s BallTree implementation:","16213a56":"The following table captures the number of features produced by the last hidden layer of each model, \nassuming the input contains only one token. If two tokens produce a different tensor shape, then the\n*tokens_dependent* column will be *True*.","dd0f5b60":"## Last hidden layer\nPyTorch models may consist of sub-models or sequences of modules. The *get_last_hidden_layer*() function makes a best guess as to which layer is the one that feeds the output layer.","b0da6de3":"## Transformation\nThe *transform_dataset*() function takes competition data frame and produces a feature matrix in place of excerpts.","9914a245":"## Feature extraction\nWe'll now define a function that can extract features from an arbitrary model and layer, assuming the shape of the feature tensor is known.","cdf508dc":"## Selected models\nThe following models were picked from the [Huggingface pre-trained models page](https:\/\/huggingface.co\/transformers\/pretrained_models.html). Some were commented out because they produce an error of one type or another (e.g. notebook runs out of memory, or there's no tokenizer.)","6dee92ee":"## Overview\nThis notebook compares features extracted from the last hidden layer of 9 [pre-trained text models from Huggingface](https:\/\/huggingface.co\/transformers\/pretrained_models.html). Models are evaluated according to prediction accuracy in the [CommonLit Readability Prize competition](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize) training data. Model *roberta-large* appears to be the best model, followed by *camembert-base*.","29a0b4ae":"The following function is used to load a model and tokenizer."}}