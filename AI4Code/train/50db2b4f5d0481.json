{"cell_type":{"fb92027b":"code","3f50de57":"code","f6b659f7":"code","92639509":"code","064891c4":"code","9d67c126":"code","b19d872f":"code","cee1a0a7":"code","c0705529":"code","029887e5":"code","babd391b":"code","a6b02588":"code","dba0a27b":"code","7614296a":"code","91f5567a":"code","15f137e3":"code","922a4264":"code","2ff66fa2":"code","11b7f75d":"code","513aaf01":"code","0bd8eb03":"code","c6c51bc7":"code","756b2504":"code","35c13925":"code","a91f9323":"code","6d333081":"code","84c0d5d5":"code","2328baed":"code","7868c209":"code","3e2d2d75":"code","e3f8a821":"code","3558a194":"code","e3057636":"code","f8b4500a":"code","a58ec33b":"code","a325ab65":"code","ad472888":"code","c03bdcc2":"code","b881b3e3":"code","8c8ffae3":"code","a9e49790":"code","66df546d":"code","a9b142b5":"code","c0ec22b7":"code","08bee5fe":"code","7d0a38f7":"code","f25caa79":"code","14f506e8":"code","af778615":"code","7936285b":"code","add69742":"code","3fc78a97":"code","b25da6ae":"code","a4f699a4":"code","ed72f9cc":"code","97c69d6b":"code","13b1166c":"code","7f9723de":"code","7161b56b":"code","380c9cd4":"code","208db0b5":"code","78169672":"code","2d32ee12":"code","9b3f9259":"code","4d5803e0":"code","056340a2":"code","e46fafec":"code","3434888f":"code","5cf09e29":"code","fa202dee":"code","c9b2ca77":"code","217de11f":"code","3faed5cf":"code","1f4f1c4e":"code","b0d30b75":"code","8a6781d1":"code","80d5cb95":"code","20b62e45":"code","e1918fde":"code","8161620b":"code","6eaeaa0f":"code","b3dacb7b":"code","81f2c059":"code","2d3a4468":"code","c74bcfb1":"code","44db1712":"code","a054847c":"code","88b6a6d1":"code","3cff8cba":"code","23080754":"code","139fe358":"code","e5c87c06":"code","dd412baf":"code","35d166e1":"code","9e73f7b0":"code","d416df7c":"code","440a989c":"code","0816d330":"code","019a58b6":"code","5ab3e96c":"code","4da2123a":"code","49f902af":"code","1a3eecf2":"code","6477360e":"code","56994e33":"code","ef1363f1":"code","13de6f2c":"code","ac0f1def":"code","b5352e04":"code","8e320773":"code","bd887f94":"code","d3027fa2":"code","520f17a3":"code","57ac132a":"code","d824d9c2":"code","4396f64b":"code","0a0ff376":"code","f2f73e89":"code","d6dcd71c":"code","3dedbc7c":"code","e65ad65c":"code","a8217739":"code","b3935617":"code","879f05af":"code","7566d792":"code","472ac6f4":"code","b80eab92":"code","91e83f2d":"code","85f21637":"markdown","4c35426b":"markdown","f7f951b4":"markdown","48a38722":"markdown","16d747f8":"markdown","3f052ddf":"markdown","9e25950c":"markdown","72359382":"markdown","671febf2":"markdown","ede97d6f":"markdown","02234933":"markdown","5e2a53ab":"markdown","f60bb8de":"markdown","d54952f5":"markdown","e1d90f12":"markdown","a1613b15":"markdown","2721e9ff":"markdown","65fad1d8":"markdown","98d38c27":"markdown","87e53749":"markdown","36055c0c":"markdown","b276bdc7":"markdown","afd395a5":"markdown","480ebe46":"markdown","03eb1077":"markdown","2d652b0e":"markdown","39846689":"markdown","bd1fd7b6":"markdown","cc79f221":"markdown","f2931ce5":"markdown","e991f32a":"markdown","db97e387":"markdown","25632400":"markdown","f69a1c17":"markdown","2b924153":"markdown","b08197ae":"markdown","ee220152":"markdown","a5ffa394":"markdown","5631a5f9":"markdown","22488097":"markdown","5750dfe4":"markdown","fbe79482":"markdown","38006239":"markdown"},"source":{"fb92027b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.autonotebook import tqdm","3f50de57":"df=pd.read_csv('\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/quora_train.csv');\n","f6b659f7":"df.head()","92639509":"df.info()\n","064891c4":"si=len(df)","9d67c126":"si","b19d872f":"ans=df.groupby('is_duplicate')['id'].count()","cee1a0a7":"print(\"percentage of question pairs that are similar\",(round(ans[0]\/si,2))*100,\"%\")","c0705529":"print(\"percentage of question pairs that are similar\",(round(ans[1]\/si,2))*100,\"%\")","029887e5":"bo=df[df.duplicated()]","babd391b":"len(bo)","a6b02588":"qid=pd.Series(df['qid1'].tolist()+df['qid2'].tolist())\n","dba0a27b":"print(\"Total no of unique questions:\",len(np.unique(qid)))","7614296a":"uni=qid.value_counts()\nprint(uni)","91f5567a":"len(uni)","15f137e3":"plt.figure(figsize=(17, 8))\nplt.hist(uni,bins=160)\nplt.yscale('log', nonposy='clip')\nplt.plot()","922a4264":"row=df[df.isnull().any(1)]   ","2ff66fa2":"row","11b7f75d":"df = df.fillna('')\nro=df[df.isnull().any(1)]","513aaf01":"ro","0bd8eb03":"df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count') ","c6c51bc7":"df['freq_qid2']=df.groupby('qid2')['qid2'].transform('count')","756b2504":"df['q1_len']=df['question1'].transform(func =lambda s: len(s) )","35c13925":"df['q2_len']=df['question2'].transform(func=lambda s: len(s))","a91f9323":"df['q1_wordscount']=df['question1'].transform(func=lambda s:len(s.split(\" \")))","6d333081":"df['q2_wordscount']=df['question2'].transform(func=lambda s:len(s.split(\" \")))","84c0d5d5":"def word_common(row):\n  q1=row['question1'].lower()\n  q2=row['question2'].lower()\n  q1=set(q1.split(\" \"))\n  q2=set(q2.split(\" \"))\n  return len(q1&q2)\ndf['common_words_count']=df.apply(word_common,axis=1)\n","2328baed":"def cou(row):\n  return len(set(row['question1'].split()))+len(set(row['question2'].split()))\ndf['tot_words']=df.apply(cou,axis=1)","7868c209":"def rati(row):\n  return row['common_words_count']\/row['tot_words']\ndf['words_ratio']=df.apply(rati,axis=1)","3e2d2d75":"def addfreq(row):\n  return (row['freq_qid1']+row['freq_qid2'])\ndf['add_freq']=df.apply(addfreq,axis=1)\n","e3f8a821":"df['sub_freq']=df.apply(lambda row:abs(row['freq_qid1']-row['freq_qid2']),axis=1)\n","3558a194":"df","e3057636":"import seaborn as sns\n\n","f8b4500a":"\ndef plo(var1,var2):\n  plt.figure(figsize=(12,8))\n  plt.subplot(1,2,1)\n  sns.violinplot(x=var1,y=var2,data=df)\n  plt.subplot(1,2,2)\n  sns.distplot(df[df[var1]==1][var2],label=1,color='red')\n  sns.distplot(df[df[var1]==0][var2],label=0,color='blue')\n  plt.legend()\n  plt.plot()\n\n","a58ec33b":"plo('is_duplicate','freq_qid1')","a325ab65":"plo('is_duplicate','freq_qid2')","ad472888":"plo('is_duplicate','q1_len')","c03bdcc2":"plo('is_duplicate','q2_len')","b881b3e3":"plo('is_duplicate','q1_wordscount')","8c8ffae3":"plo('is_duplicate','q2_wordscount')","a9e49790":"plo('is_duplicate','common_words_count')","66df546d":"plo('is_duplicate','tot_words')","a9b142b5":"plo('is_duplicate','words_ratio')","c0ec22b7":"plo('is_duplicate','add_freq')","08bee5fe":"plo('is_duplicate','sub_freq')","7d0a38f7":"df[df.isnull().any(1)]","f25caa79":"df","14f506e8":"df.to_pickle(\"\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/basic_features.pkl\")","af778615":"df = pd.read_pickle(\"\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/basic_features.pkl\")","7936285b":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nfrom tqdm.autonotebook import tqdm\n\nporter=PorterStemmer()","add69742":"def replac(x):\n  x=x.replace(\"$\",\"dollar\")\n  x=x.replace(\"\u20ac\",\"euro\")\n  x=x.replace(\"\u20b9\",\"rupee\")\n  x=x.replace(\"\u00a5\",\"yen\")\n  x=x.replace(\"\u00a3\",\"pound\")\n  x=x.replace(\"%\",\"percent\")\n  x=x.replace(\"&\",\"and\")\n  x=x.replace(\"'ll\",\" will\")\n  x=x.replace(\",\",\" \")\n  x=x.replace(\"?\",\" \")\n  x=x.replace(\"i'm\",\"i am\")\n  x=x.replace(\"'s\",\" is\")\n  x=x.replace(\"'ve\",\" have\")\n  x=x.replace(\"won't\",\"will not\")\n  x=x.replace(\"can't\",\"can not\")\n  x=x.replace(\"n't\",\" not\")\n  x=x.replace(\",000\",\"k\")\n  x=x.replace(\"000,000\",\"m\")\n  x=x.replace(\"000,000,000\",\"b\")\n  x=x.replace(\".\",\" \")\n  return x;\n\ndef preprocessing(x):\n  x=x.lower()\n  x=replac(x)\n  x=BeautifulSoup(x)\n  x=x.get_text()\n  lis=x.split()\n  \n  st=\"\"\n  for word in lis:\n    st+=porter.stem(word)\n    st+=\" \"\n  x=st;\n  return x;","3fc78a97":"tqdm.pandas()\ndf['question1']=df['question1'].progress_apply(preprocessing)\ndf['question2']=df['question2'].progress_apply(preprocessing)\n","b25da6ae":"df","a4f699a4":"df.to_pickle(\"\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/preprocessing.pkl\")","ed72f9cc":"df=pd.read_pickle(\"\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/preprocessing.pkl\")\n","97c69d6b":"from nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')","13b1166c":"stop_words=stopwords.words(\"english\")\nprint(stop_words)","7f9723de":"tqdm.pandas()\ndef cwc_min_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  que1_words=set(que1).difference(set(stop_words))\n  que2_words=set(que2).difference(set(stop_words))\n  common_words=que1_words.intersection(que2_words)\n  return len(common_words)\/(min(len(que1_words),len(que2_words))+0.0001)\n\ndf['cwc_min']=df.progress_apply(cwc_min_cal,axis=1)","7161b56b":"tqdm.pandas()\ndef cwc_max_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  que1_words=set(que1).difference(set(stop_words))\n  que2_words=set(que2).difference(set(stop_words))\n  common_words=que1_words.intersection(que2_words)\n  return len(common_words)\/(max(len(que1_words),len(que2_words))+0.0001)\n\ndf['cwc_max']=df.progress_apply(cwc_max_cal,axis=1)","380c9cd4":"tqdm.pandas()\ndef csc_min_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  que1_stop_words=set(que1).intersection(set(stop_words))\n  que2_stop_words=set(que2).intersection(set(stop_words))\n  common_stop_words=que1_stop_words.intersection(que2_stop_words)\n  return len(common_stop_words)\/(min(len(que1_stop_words),len(que2_stop_words))+0.0001)\ndf['csc_min']=df.progress_apply(csc_min_cal,axis=1)","208db0b5":"tqdm.pandas()\ndef csc_max_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  que1_stop_words=set(que1).intersection(set(stop_words))\n  que2_stop_words=set(que2).intersection(set(stop_words))\n  common_stop_words=que1_stop_words.intersection(que2_stop_words)\n  return len(common_stop_words)\/(max(len(que1_stop_words),len(que2_stop_words))+0.0001)\ndf['csc_max']=df.progress_apply(csc_max_cal,axis=1)","78169672":"tqdm.pandas()\ndef ctc_min_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  common_token=set(que1).intersection(set(que2))\n  return len(common_token)\/(min(len(que1),len(que2))+0.0001)\ndf['ctc_min']=df.progress_apply(ctc_min_cal,axis=1)","2d32ee12":"tqdm.pandas()\ndef ctc_max_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  common_token=set(que1).intersection(set(que2))\n  return len(common_token)\/(max(len(que1),len(que2))+0.0001)\ndf['ctc_max']=df.progress_apply(ctc_max_cal,axis=1)","9b3f9259":"tqdm.pandas()\ndef fir_word_eq_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  if(len(que1)==0 or len(que2)==0):\n    return 0.0\n  if(que1[0]==que2[0]):\n    return 1.0;\n  else:\n    return 0.0;\ndf['fir_word_eq']=df.progress_apply(fir_word_eq_cal,axis=1)","4d5803e0":"tqdm.pandas()\ndef las_word_eq_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  if(len(que1)==0 or len(que2)==0):\n    return 0.0\n  if(que1[len(que1)-1]==que2[len(que2)-1]):\n    return 1.0;\n  else:\n    return 0.0;\ndf['las_word_eq']=df.progress_apply(las_word_eq_cal,axis=1)","056340a2":"tqdm.pandas()\ndef absdiff_token_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  return abs(len(que1)-len(que2))\ndf['absdiff_token']=df.progress_apply(absdiff_token_cal,axis=1)","e46fafec":"tqdm.pandas()\ndef mean_token_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  return (len(que1)+len(que2))\/2\n\ndf['mean_token']=df.progress_apply(mean_token_cal,axis=1)","3434888f":"!pip install fuzzywuzzy","5cf09e29":"from fuzzywuzzy import fuzz","fa202dee":"tqdm.pandas()\ndef fuzz_ratio_cal(row):\n  return fuzz.ratio(row['question1'],row['question2'])\n\ndf['fuzz_ratio']=df.progress_apply(fuzz_ratio_cal,axis=1)","c9b2ca77":"tqdm.pandas()\ndef fuzz_partial_ratio_cal(row):\n  return fuzz.partial_ratio(row['question1'],row['question2'])\ndf['fuzz_partial_ratio']=df.progress_apply(fuzz_partial_ratio_cal,axis=1)\n","217de11f":"tqdm.pandas()\ndef fuzz_sort_ratio_cal(row):\n  return fuzz.token_sort_ratio(row['question1'],row['question2'])\ndf['fuzz_sort_ratio']=df.progress_apply(fuzz_sort_ratio_cal,axis=1)\n","3faed5cf":"tqdm.pandas()\ndef fuzz_set_ratio_cal(row):\n  return fuzz.token_set_ratio(row['question1'],row['question2'])\ndf['fuzz_set_ratio']=df.progress_apply(fuzz_set_ratio_cal,axis=1)\n","1f4f1c4e":"!pip install distance","b0d30b75":"import distance\ntqdm.pandas()\ndef get_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) \/ (min(len(a), len(b)) + 1)\ndf[\"lcs_ratio\"]  = df.progress_apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n","8a6781d1":"df.head(1)","80d5cb95":"df.to_pickle(\"\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/advanced_features.pkl\")","20b62e45":"df=pd.read_pickle(\"\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/advanced_features.pkl\")","e1918fde":"plo('is_duplicate','fuzz_ratio')","8161620b":"plo('is_duplicate','fuzz_partial_ratio')","6eaeaa0f":"plo('is_duplicate','fuzz_sort_ratio')","b3dacb7b":"plo('is_duplicate','fuzz_set_ratio')","81f2c059":"fea=['ctc_min','ctc_max','cwc_min','cwc_max']\nsns.pairplot(df[['ctc_min','ctc_max','cwc_min','cwc_max','is_duplicate']][:],hue='is_duplicate',vars=fea)\nplt.show()","2d3a4468":"from wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\nstop_words=stopwords.words(\"english\")","c74bcfb1":"df_dup=df[df['is_duplicate']==1]\ndf_ndup =df[df['is_duplicate'] == 0]\n","44db1712":"du = np.dstack([df_dup[\"question1\"], df_dup[\"question2\"]]).flatten()\nndu = np.dstack([df_ndup[\"question1\"], df_ndup[\"question2\"]]).flatten()\n","a054847c":"strdu=\" \".join(list(du))\nstrndu=\" \".join(list(ndu))\n","88b6a6d1":"wc=WordCloud(stopwords=stop_words)\nwc.generate(strdu)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show() ","3cff8cba":"wc=WordCloud(stopwords=stop_words )\nwc.generate(strndu)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","23080754":"df_sample=df[0:10000]\ndf_sample.head(1)","139fe358":"from sklearn.manifold import TSNE\ntsne=TSNE(n_components=2,perplexity=30,n_iter=1000)\ntsne_df=pd.DataFrame(tsne.fit_transform(df_sample[['cwc_min','cwc_max','csc_min','csc_max','ctc_min','ctc_max','las_word_eq','fir_word_eq','absdiff_token','mean_token','fuzz_ratio','fuzz_partial_ratio','fuzz_sort_ratio','fuzz_set_ratio','lcs_ratio']]),columns=['fea1','fea2'])\ntsne_df['is_duplicate']=df_sample['is_duplicate']\ntsne_df\n","e5c87c06":"sns.FacetGrid(tsne_df, hue='is_duplicate',height=10).map(plt.scatter,\"fea1\",\"fea2\").add_legend()\nplt.show()","dd412baf":"import spacy\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer","35d166e1":"ques=list(df['question1'])+list(df['question2'])","9e73f7b0":"tfidf=TfidfVectorizer()\nspar_mat=tfidf.fit_transform(ques)","d416df7c":"idf_val=dict(zip(tfidf.get_feature_names(),tfidf.idf_))","440a989c":"len(idf_val)","0816d330":"nlp = spacy.load('en_core_web_sm')\nvectors = []\nfor question in tqdm(list(df['question1'])):\n  doc = nlp(question)\n  try:\n    vec=np.zeros([len(doc[0].vector)])\n  except:\n    vec=np.zeros(96)\n  for word in doc:\n    word_vec= word.vector\n    try:\n      idf =idf_val[str(word)]\n    except:\n      idf = 0\n    vec += (word_vec* idf)\n  vectors.append(vec)\ndf['q1_w2v_idf'] = list(vectors)","019a58b6":"df","5ab3e96c":"df.to_pickle(\"\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/tfidfw2v_q1.pkl\")","4da2123a":"df=pd.read_pickle('\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/tfidfw2v_q1.pkl')","49f902af":"df","1a3eecf2":"nlp = spacy.load('en_core_web_sm')\nvectors = []\nfor question in tqdm(list(df['question2'])):\n  doc = nlp(question)\n  try:\n    vec=np.zeros([len(doc[0].vector)])\n  except:\n    vec=np.zeros(96)\n  for word in doc:\n    word_vec= word.vector\n    try:\n      idf =idf_val[str(word)]\n    except:\n      idf = 0\n    vec += (word_vec* idf)\n  vectors.append(vec)\ndf['q2_w2v_idf'] = list(vectors)","6477360e":"df.to_pickle(\"\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/tfidfw2v.pkl\")","56994e33":"df=pd.read_pickle(\"\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/tfidfw2v.pkl\")","ef1363f1":"df","13de6f2c":"dfq1=pd.DataFrame((df['q1_w2v_idf'].values).tolist(),index=df.index,columns=[\"q1_\"+str(i) for i in range(len(df['q1_w2v_idf'][0]))])\ndfq2=pd.DataFrame((df['q2_w2v_idf'].values).tolist(),index=df.index,columns=[\"q2_\"+str(i) for i in range(len(df['q2_w2v_idf'][0]))])\ndfq1['id']=df.id\ndfq2['id']=df.id\ndfq1q2  = dfq1.merge(dfq2, on='id',how='left')\ndf=df.merge(dfq1q2,on='id',how='left')\ndf.drop(['id','q1_w2v_idf','q2_w2v_idf','qid1','qid2','question1','question2'],axis=1,inplace=True)","ac0f1def":"df.to_pickle(\"\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/final_df.pkl\")","b5352e04":"df=pd.read_pickle(\"\/content\/drive\/My Drive\/Colab Notebooks\/project Question Pair Similarity\/final_df.pkl\")","8e320773":"# df=df[0:400000]","bd887f94":"df.head()","d3027fa2":"x_label=df.iloc[:,1:]\ny_label=df.iloc[:,0:1]","520f17a3":"y_val=list(map(int,y_label.values))","57ac132a":"from sklearn.model_selection import train_test_split","d824d9c2":"x_train,x_test,y_train,y_test=train_test_split(x_label,y_val,stratify=y_val,test_size=0.3) ","4396f64b":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import log_loss","0a0ff376":"y_predict=np.zeros((len(y_test),2))\nfor i in range(len(y_test)):\n    rand_val= np.random.rand(1,2)\n    y_predict[i] = ((rand_val\/sum(sum(rand_val)))[0])\n#print(Y_predict)\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, y_predict, eps=1e-15))","f2f73e89":"y_predict=np.argmax(y_predict,axis=1)\nconfusion_matrix(y_test,y_predict)","d6dcd71c":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV","3dedbc7c":"mi=100000\nbest_alpha=-1;\nalpha=[10**i for i in range(-5,2)]\nfor i in tqdm(alpha):\n  model=SGDClassifier(alpha=i,random_state=42 ,loss='log') \n  #model.fit(x_train,y_train)\n  sig_model=CalibratedClassifierCV(model,method='sigmoid')\n  sig_model.fit(x_train,y_train)\n  y_predict=sig_model.predict_proba(x_test)\n  logloss=log_loss(y_test,y_predict)\n  if(mi>logloss):\n    mi=logloss\n    best_alpha=i\n  print(\"alpha->\",i,\"\\t\\tlog-loss->\",logloss)","e65ad65c":"best_alpha=0.001","a8217739":"best_model=SGDClassifier(alpha=best_alpha,random_state=42,loss=\"log\")\nbest_model=CalibratedClassifierCV(best_model,method=\"sigmoid\")\nbest_model.fit(x_train,y_train)\ny_predict=best_model.predict_proba(x_train)\nprint(\"Train Log Loss -->\",log_loss(y_train,y_predict))\ny_predict=best_model.predict_proba(x_test)\nprint(\"Test Log Loss -->\",log_loss(y_test,y_predict))\ny_predict=np.argmax(y_predict,axis=1)\nconfusion_matrix(y_test,y_predict)\n","b3935617":"to_best_model","879f05af":"mi=100000\nbest_alpha=-1;\nalpha=[10**i for i in range(-5,2)]\nfor i in tqdm(alpha):\n  model=SGDClassifier(alpha=i,random_state=42,loss=\"hinge\")\n  #model.fit(x_train,y_train)\n  sig_model=CalibratedClassifierCV(model,method='sigmoid')\n  sig_model.fit(x_train,y_train)\n  y_predict=sig_model.predict_proba(x_test)\n  logloss=log_loss(y_test,y_predict)\n  if(mi>logloss):\n    mi=logloss\n    best_alpha=i\n  print(\"alpha->\",i,\"\\t\\tlog-loss->\",logloss)","7566d792":"best_model=SGDClassifier(alpha=best_alpha,random_state=42,loss=\"hinge\")\nbest_model=CalibratedClassifierCV(best_model,method=\"sigmoid\")\nbest_model.fit(x_train,y_train)\ny_predict=best_model.predict_proba(x_train)\nprint(\"Train Log Loss -->\",log_loss(y_train,y_predict))\ny_predict=best_model.predict_proba(x_test)\nprint(\"Test Log Loss -->\",log_loss(y_test,y_predict))\ny_predict=np.argmax(y_predict,axis=1)\nconfusion_matrix(y_test,y_predict)\n","472ac6f4":"import xgboost as xgb   ","b80eab92":"params = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\nd_train=xgb.DMatrix(x_train,label=y_train)\nd_test=xgb.DMatrix(x_test,label=y_test)\nwatchlist = [(d_train, 'train'), (d_test, 'valid')]\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\nxgdmat = xgb.DMatrix(x_train,y_train)\ny_predict = bst.predict(d_test)\nprint(\"The test log loss is:\",log_loss(y_test, y_predict, eps=1e-15))","91e83f2d":"y_predict=np.array(y_predict>0.5,dtype=int)\nconfusion_matrix(y_test,y_predict)","85f21637":"<h1> Tfidf-word vector <h1>","4c35426b":"**<h1>preprocessing of text data<\/h1>**","f7f951b4":"- **total frequency of question 1 and question 2 add_freq=freq_qid1+freq_qid2**","48a38722":"- cscmin= common stop word count\/min(no of stop words in q1,no of stop words in q2)","16d747f8":"- fuzz_sort_ratio","3f052ddf":"- ctcmax=common token count\/max(no of tokens in q1,no of tokens in q2)","9e25950c":"<h1> Pair Plot of Natural language Processing features <\/h1>\n","72359382":"- **subraction of frequency of question 1 and question 2 sub_freq=abs(freq_qid1-freq_qid2)**","671febf2":"- ctcmin=common token count\/min(no of tokens in q1,no of tokens in q2)","ede97d6f":"<h1> <b>Adding some Basic Features<\/b> <\/h1>","02234933":"- **total words count**","5e2a53ab":"<h1> Logistic Regression <\/h1>\n","f60bb8de":"- **length of question 2**","d54952f5":"- fuzz_set_ratio","e1d90f12":"- **no of words in question 2**\n","a1613b15":"- **words ratio =(common_words_count\/tot_words)**","2721e9ff":"<h1> Training a Random Model","65fad1d8":"Checking for the null values\n","98d38c27":"<h1> Fuzzy Feature Analysis <\/h1>","87e53749":"- mean_token = average (tokens of q1,tokens of q2)","36055c0c":"<h1> visualization (TSNE) <\/h1>","b276bdc7":"- last_word_equal= 1 if last word of both questions are equal else 0","afd395a5":"- **length of question 1**","480ebe46":"<h1> SVM (Support Vector Machine) <\/h1>","03eb1077":"<h1> XGBoost <\/h1>","2d652b0e":"- **frequency of questino 1 occurs**","39846689":"- cwcmax=common word count\/max(no of words in q1,no of words in q2)","bd1fd7b6":"- cscmax= common stop word count\/max(no of stop words in q1,no of stop words in q2)","cc79f221":"- cwcmin=common word count\/min(no of words in q1,no of words in q2)","f2931ce5":"- **no of common words between queston 1 and question 2**","e991f32a":"- first_word_equal= 1 if first word of both questions are equal else 0","db97e387":"- **no of words in question 1**","25632400":"<h1>Train Test Splitting in stratified Way<\/h1>","f69a1c17":"<h1> Adding NLP and Fuzzy Based Features <\/h1>\n","2b924153":"<h1> Plotting Word Cloud <\/h1>","b08197ae":"**<h1> Feature Analysis <\/h1>**","ee220152":"- Longest common subsequence ratio\n\n","a5ffa394":"- absdiff_tokens=abs(no of tokens in q1- no of tokens in q2)","5631a5f9":"- Stemming words in sentence \n- removing stop words\n- Expanding Contractions\n- lowering the casese\n","22488097":"<p> There are no duplicates <\/p> ","5750dfe4":"<h3> fuzzy Logic feature <\/h3>\n\n- fuzz_ratio","fbe79482":"- fuzz_partial_ratio\n","38006239":"- **frequency of question 2 occurs**"}}