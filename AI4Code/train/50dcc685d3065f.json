{"cell_type":{"2466a757":"code","08528f96":"code","51cc067e":"code","43d79169":"code","26cd1cab":"code","ee73d7ce":"code","f4d977ae":"code","7f770925":"code","7360e006":"code","88e3e37f":"code","858dfdc9":"code","f1e51561":"code","f9d7f303":"code","87b83fec":"code","3bc1a6ed":"code","88c6c803":"code","dd94ca28":"code","de0b43a8":"code","8755d951":"code","7c553995":"code","e9bfe328":"code","67d2f35a":"code","795441bc":"code","c31c8d60":"code","2c7e37a6":"code","2db02afd":"code","d72d45d5":"code","ec960d7c":"code","cfd1200d":"code","49f83c3f":"code","d395bbff":"code","07449211":"code","734189f5":"code","dc643e8b":"code","d398e5d3":"code","12c979a5":"code","888db438":"code","555f9056":"code","f9e289d2":"code","5a5968b5":"code","0b3e366a":"code","8c522a7b":"code","746c3b71":"code","3503d347":"code","1f661f6a":"code","a3435153":"code","d462376a":"code","e2dbc862":"code","bd1bad16":"code","2b2b07e5":"code","914292f2":"code","91a183f1":"code","fa77f326":"code","5656f839":"code","85b93475":"code","7c600920":"code","2939a44b":"code","0b93bc7c":"code","949e886f":"code","d459e8a4":"code","75f4e2e8":"code","7e5b58e3":"code","755704f9":"code","c58a9eca":"code","a1df9918":"code","7926d2a4":"code","96535b7d":"code","84c3d879":"code","496f1ab6":"markdown","27f658d1":"markdown","96b4f011":"markdown","f0f7201d":"markdown","516e37fb":"markdown","384ea48e":"markdown","ca91326a":"markdown","b113da21":"markdown","75c97853":"markdown","8ebbc618":"markdown","e5624e9c":"markdown","fb709de9":"markdown","b3bb56b5":"markdown","1bd5b42b":"markdown","1c4b0e1f":"markdown","30707dbd":"markdown","6f1de20f":"markdown","57dbe77b":"markdown","77c37ed3":"markdown","0d0d72bb":"markdown","87c6456c":"markdown","be96e906":"markdown","06464f72":"markdown","ae2af6b4":"markdown","6085317e":"markdown","275266f3":"markdown","77bb032b":"markdown","3e389403":"markdown","71c965c8":"markdown","7667ad03":"markdown","beb24bad":"markdown","169f9bf4":"markdown","b335fe65":"markdown","f6a02961":"markdown","14495e61":"markdown","7c4218e0":"markdown","d260025b":"markdown","bf2d1e89":"markdown","95080e3d":"markdown","43a4f9fb":"markdown","f6d17902":"markdown","bc880854":"markdown","73dd13bb":"markdown","fb3bccce":"markdown","c71bd663":"markdown","8a85d833":"markdown","1a3fe5b8":"markdown","831b9c7f":"markdown","2393d464":"markdown","59652e44":"markdown","6db97eab":"markdown","c21d83e6":"markdown","582b0a54":"markdown","d5036340":"markdown","fb7237df":"markdown","babf8676":"markdown","23ab3651":"markdown","2d46ea3d":"markdown","6765974a":"markdown","0bfac1de":"markdown","2b63cd7a":"markdown","0bebec26":"markdown","09b016ac":"markdown","beda259a":"markdown","e7d8f54b":"markdown","f7b914a0":"markdown","feb2c21a":"markdown","3fa31d2b":"markdown","c9602540":"markdown","fe11296f":"markdown","87cc85f7":"markdown","3ba1e71c":"markdown","dc4cee31":"markdown","27f19891":"markdown","f3fb530b":"markdown","929d5afc":"markdown","f84701d2":"markdown","0fe59974":"markdown","20373517":"markdown","2ac77f50":"markdown","69488635":"markdown","8f190b91":"markdown","cfd89c2f":"markdown","9a1a11a0":"markdown","91f10a3b":"markdown","2a4cc352":"markdown"},"source":{"2466a757":"from wand.image import Image as Img\nImg(filename='..\/input\/cityofla\/CityofLA\/Additional data\/PDFs\/2017\/july 2017\/July 21\/ARTS ASSOCIATE 2454 072117 REV 072817.pdf', resolution=300)","08528f96":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport os\nimport numpy as np\nfrom datetime import datetime\nfrom collections  import Counter\nfrom nltk import word_tokenize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport calendar\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nprint(os.listdir(\"..\/input\"))\nfrom gensim.models import word2vec\nfrom sklearn.manifold import TSNE\nfrom nltk import pos_tag\nfrom nltk.help import upenn_tagset\nimport gensim\nimport matplotlib.colors as mcolors\nfrom nltk import jaccard_distance\nfrom nltk import ngrams\n#import textstat\nplt.style.use('ggplot')","51cc067e":"files=[dir for dir in os.walk('..\/input\/cityofla')]\nfor file in files:\n    print(os.listdir(file[0]))\n    print(\"\\n\")","43d79169":"bulletins=os.listdir(\"..\/input\/cityofla\/CityofLA\/Job Bulletins\/\")\nadditional=os.listdir(\"..\/input\/cityofla\/CityofLA\/Additional data\/\")","26cd1cab":"csvfiles=[]\nfor file in additional:\n    if file.endswith('.csv'):\n        print(file)\n        csvfiles.append(\"..\/input\/cityofla\/CityofLA\/Additional data\/\"+file)\n        ","ee73d7ce":"job_title=pd.read_csv(csvfiles[0])\nsample_job=pd.read_csv(csvfiles[1])\nkaggle_data=pd.read_csv(csvfiles[2])","f4d977ae":"job_title.head()","7f770925":"print(\"The are %d rows and %d cols in job_title file\" %(job_title.shape))","7360e006":"    sample_job[sample_job['Field Name']=='SCHOOL_TYPE']['Description']","88e3e37f":"print(\"The are %d rows and %d cols in sample_job file\" %(sample_job.shape))\n","858dfdc9":"kaggle_data.head()","f1e51561":"print(\"The are %d rows and %d cols in kaggle_data file\" %(kaggle_data.shape))","f9d7f303":"print(\"There are %d text files in bulletin directory\" %len(bulletins))","87b83fec":"def get_headings(bulletin):       \n    \n    \"\"\"\"function to get the headings from text file\n        takes a single argument\n        1.takes single argument list of bulletin files\"\"\"\n    \n    with open(\"..\/input\/cityofla\/CityofLA\/Job Bulletins\/\"+bulletins[bulletin]) as f:    ##reading text files \n        data=f.read().replace('\\t','').split('\\n')\n        data=[head for head in data if head.isupper()]\n        return data\n        \ndef clean_text(bulletin):      \n    \n    \n    \"\"\"function to do basic data cleaning\n        takes a single argument\n        1.takes single argument list of bulletin files\"\"\"\n                                            \n    \n    with open(\"..\/input\/cityofla\/CityofLA\/Job Bulletins\/\"+bulletins[bulletin]) as f:\n        data=f.read().replace('\\t','').replace('\\n','')\n        return data","3bc1a6ed":"get_headings(1)\n","88c6c803":"get_headings(2)","dd94ca28":"\n\n\ndef to_dataframe(num,df):\n    \"\"\"\"function to extract features from job bulletin text files and convert to\n    pandas dataframe.\n    function take two arguments \n                        1.the number of files to be read\n                        2.dataframe object                                      \"\"\"\n    \n\n    \n    opendate=re.compile(r'(Open [D,d]ate:)(\\s+)(\\d\\d-\\d\\d-\\d\\d)')       #match open date\n    \n    salary=re.compile(r'\\$(\\d+,\\d+)((\\s(to|and)\\s)(\\$\\d+,\\d+))?')       #match salary\n    \n    requirements=re.compile(r'(REQUIREMENTS?\/\\s?MINIMUM QUALIFICATIONS?)(.*)(PROCESS NOTE)')      #match requirements\n    \n    for no in range(0,num):\n        with open(\"..\/input\/cityofla\/CityofLA\/Job Bulletins\/\"+bulletins[no],encoding=\"ISO-8859-1\") as f:         #reading files \n                try:\n                    file=f.read().replace('\\t','')\n                    data=file.replace('\\n','')\n                    headings=[heading for heading in file.split('\\n') if heading.isupper()]             ##getting heading from job bulletin\n\n                    sal=re.search(salary,data)\n                    date=datetime.strptime(re.search(opendate,data).group(3),'%m-%d-%y')\n                    try:\n                        req=re.search(requirements,data).group(2)\n                    except Exception as e:\n                        req=re.search('(.*)NOTES?',re.findall(r'(REQUIREMENTS?)(.*)(NOTES?)',\n                                                              data)[0][1][:1200]).group(1)\n                    \n                    duties=re.search(r'(DUTIES)(.*)(REQ[A-Z])',data).group(2)\n                    try:\n                        enddate=re.search(\n                                r'(JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)\\s(\\d{1,2},\\s\\d{4})'\n                                ,data).group()\n                    except Exception as e:\n                        enddate=np.nan\n                    \n                    selection= [z[0] for z in re.findall('([A-Z][a-z]+)((\\s\\.\\s)+)',data)]     ##match selection criteria\n                    \n                    df=df.append({'File Name':bulletins[no],'Position':headings[0].lower(),'salary_start':sal.group(1),\n                               'salary_end':sal.group(5),\"opendate\":date,\"requirements\":req,'duties':duties,\n                                'deadline':enddate,'selection':selection},ignore_index=True)\n                    \n                    \n                    reg=re.compile(r'(One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|one|two|three|four)\\s(years?)\\s(of\\sfull(-|\\s)time)')\n                    df['EXPERIENCE_LENGTH']=df['requirements'].apply(lambda x :  re.search(reg,x).group(1) if re.search(reg,x) is not None  else np.nan)\n                    df['FULL_TIME_PART_TIME']=df['EXPERIENCE_LENGTH'].apply(lambda x:  'FULL_TIME' if x is not np.nan else np.nan )\n                    \n                    reg=re.compile(r'(One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|one|two|three|four)(\\s|-)(years?)\\s(college)')\n                    df['EDUCATION_YEARS']=df['requirements'].apply(lambda x :  re.search(reg,x).group(1) if re.search(reg,x) is not None  else np.nan)\n                    df['SCHOOL_TYPE']=df['EDUCATION_YEARS'].apply(lambda x : 'College or University' if x is not np.nan else np.nan)\n                    \n                except Exception as e:\n                    print('umatched sequence')\n                    \n                    \n                \n                \n        \n           \n    return df\n\n            \n            \n            \n            ","de0b43a8":"df=pd.DataFrame(columns=['File Name','Position','salary_start','salary_end','opendate','requirements','duties','deadline'])\ndf=to_dataframe(len(bulletins),df)\ndf.to_csv('job class output.csv')","8755d951":"df.shape","7c553995":"df.head()","e9bfe328":"data_dictionary=pd.DataFrame({'Field Name':['File Name','Position','salary_start','salary_end','opendate',\n                                            'requirements','duties','deadline','selection','EXPERIENCE_LENGTH','FULL_TIME_PART_TIME','EDUCATION_YEARS','SCHOOL_TYPE'],\n                             })\n\ndata_dictionary['Description']=['The file name of the job bulletin from which each record came','The title of the particular class (e.g., Systems Analyst, Carpenter)',\n                              'The overall salary start','The overall maximum salary','The date the job bulletin opened','Overall requirement that has to be filled',\n                              'A summary of what someone does in the particular job\\n','The date the job bulletin closed','list of selection criterias','Years required in a particular job class or external role.',\n                              'Whether the required experience is full-time, part','Years required in a particular education program',\n                               'School Type: School type required (e.g. college or university, high school)']\n\ndata_dictionary['Data Type']=['string']*13\n\ndata_dictionary['Accepts Null Values?']=['Yes']*13","67d2f35a":"data_dictionary","795441bc":"data_dictionary.to_csv('data dictionary.csv')","c31c8d60":"print('There are %d different jobs available' %df['Position'].nunique())","2c7e37a6":"plt.figure(figsize=(8,5))\ntext=''.join(job for job in df['Position'])                                ##joining  data to form text\ntext=word_tokenize(text)\njobs=Counter(text)                                                         ##counting number of occurences\njobs_class=[job for job in jobs.most_common(12) if len(job[0])>3]          ##selecting most common words\n#offers=[job[1] for job in jobs.most_common(12) if len(job[0]>3)]\na,b=map(list, zip(*jobs_class))\nsns.barplot(b,a,palette='rocket')                                           ##creating barplot\nplt.title('Job sectors')\nplt.xlabel(\"count\")\nplt.ylabel('sector')\n","2db02afd":"\"\"\"\"\n    convert salary to proper  form \n    by removing '$' and ',' symbols.\n                                    \"\"\"\n\ndf['salary_start']=[int(sal.split(',')[0]+sal.split(',')[1] ) for sal in df['salary_start']]   \ndf['salary_end']=[sal.replace('$','')  if sal!= None else 0 for sal in df['salary_end']  ]\ndf['salary_end']=[int(sal.split(',')[0]+sal.split(',')[1] ) if type(sal)!=int else 0 for sal in df['salary_end']]","d72d45d5":"plt.figure(figsize=(7,5))\nsns.distplot(df['salary_start'])\nplt.title('salary distribution')\nplt.show()","ec960d7c":"'''finding the most paid 10 jobs at LA'''\n\nmost_paid=df[['Position','salary_start']].sort_values(by='salary_start',ascending=False)[:10]\nplt.figure(figsize=(7,5))\nsns.barplot(y=most_paid['Position'],x=most_paid['salary_start'],palette='rocket')\nplt.title('Best paid jobs in LA')","cfd1200d":"''''calculating salary start - salary end '''\n\ndf['salary_diff']=abs(df['salary_start']-df['salary_end'])\n\nranges=df[['Position','salary_diff']].sort_values(by='salary_diff',ascending=False)[:10]","49f83c3f":"plt.figure(figsize=(7,5))\nsns.barplot(y=ranges['Position'],x=ranges['salary_diff'],palette='RdBu')   ##plotting\n\n","d395bbff":"ranges","07449211":"'''Extracting year out of opendate timestamp object and counting\n    the number of each occurence of each year using count_values() '''\n\ndf['year_of_open']=[date.year for date in df['opendate']]\n\ncount=df['year_of_open'].value_counts(ascending=True)\nyears=['2020','2019','2018', '2017', '2016', '2015', '2014', '2013', '2012', '2008', '2006',\n           '2005', '2002', '1999']\nplt.figure(figsize=(7,5))\nplt.plot([z for z in reversed(years)],count.values,color='blue')\n\nplt.title('Oppurtunities over years')\nplt.xlabel('years')\nplt.ylabel('count')\nplt.gca().set_xticklabels([z for z in reversed(years)],rotation='45')\nplt.show()","734189f5":"experience=df['EXPERIENCE_LENGTH'].value_counts().reset_index()\nexperience['index']=experience['index'].apply(lambda x : x.lower())\nexperience=experience.groupby('index',as_index=False).agg('sum')\nlabels=experience['index']\nsizes=experience['EXPERIENCE_LENGTH']\nplt.figure(figsize=(5,7))\nplt.pie(sizes,explode=(0, 0.1, 0, 0,0,0,0),labels=labels)\nplt.gca().axis('equal')\nplt.title('Experience value count')\nplt.show()","dc643e8b":"x1=df['SCHOOL_TYPE'].value_counts()[0]\nx2=df['FULL_TIME_PART_TIME'].value_counts()[0]\nplt.figure(figsize=(5,5))\nplt.bar(height=[x1,x2],x=['College Degree','Experience'])\n","d398e5d3":"'''Extracting month out of opendate timestamp object and counting\n    the number of each occurence of each months using count_values() '''\n\n\nplt.figure(figsize=(7,5))\ndf['open_month']=[z.month for z in df['opendate']]\ncount=df['open_month'].value_counts(sort=False)\nsns.barplot(y=count.values,x=count.index,palette='rocket')\nplt.gca().set_xticklabels([calendar.month_name[x] for x in count.index],rotation='45')\nplt.show()","12c979a5":"'''Extracting weekday out of opendate timestamp object and counting\n    the number of each occurence of each weekday using count_values() '''\n\n\nplt.figure(figsize=(7,5))\n\ndf['open_day']=[z.weekday() for z in df['opendate']]\ncount=df['open_day'].value_counts(sort=False)\nsns.barplot(y=count.values,x=count.index,palette='rocket')\nplt.gca().set_xticklabels([calendar.day_name[x] for x in count.index],rotation='45')\nplt.show()","888db438":"print('%d job applications may close without prior notice' %df['deadline'].isna().sum())\n","555f9056":"#df['dealine']=df['deadline'].fillna(method='backfill',inplace=True\n#deadline=[datetime.strptime(x,'%B %d, %Y')  for x in df['deadline'] ]\n","f9e289d2":"req=' '.join(text for text in df['requirements'])\n","5a5968b5":"\ndef show_wordcloud(data, title = None):\n    \n    \n    '''funtion to produce and display wordcloud\n        taken 2 arguments\n        1.data to produce wordcloud\n        2.title of wordcloud'''\n    \n    \n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=set(STOPWORDS),\n        max_words=250,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\nshow_wordcloud(text,'REQUIREMENTS')","0b3e366a":"lem=WordNetLemmatizer()\ntext=[lem.lemmatize(w) for w in word_tokenize(req)]\nvect=TfidfVectorizer(ngram_range=(1,3),max_features=100)\nvectorized_data=vect.fit_transform(text)\n#id_map=dict((v,k) for k,v in vect.vocabulary_.items())\nvect.vocabulary_.keys()","8c522a7b":"def build_corpus(df,col):\n    \n    '''function to build corpus from dataframe'''\n    lem=WordNetLemmatizer()\n    corpus= []\n    for x in df[col]:\n        \n        \n        words=word_tokenize(x)\n        corpus.append([lem.lemmatize(w) for w in words])\n    return corpus\n","746c3b71":"corpus=build_corpus(df,'requirements')\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=30, workers=4)\n","3503d347":"def tsne_plot(model,title='None'):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=80, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(12, 12)) \n    plt.title(title)\n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","1f661f6a":"tsne_plot(model,'Requirements')","a3435153":"token=word_tokenize(req)\ncounter=Counter(token)\ncount=[x[0] for x in counter.most_common(40) if len(x[0])>3]\nprint(\"Most common words in Requirement\")\nprint(count)","d462376a":"duties= ' '.join(d for d in df['duties'])\nshow_wordcloud(duties,'Duties')","e2dbc862":"lem=WordNetLemmatizer()\ntext=[lem.lemmatize(w) for w in word_tokenize(duties)]\nvect=TfidfVectorizer(ngram_range=(1,3),max_features=200)\nvectorized_data=vect.fit_transform(text)\n#id_map=dict((v,k) for k,v in vect.vocabulary_.items())\nvect.vocabulary_.keys()","bd1bad16":"token=word_tokenize(duties)\ncounter=Counter(token)\ncount=[x[0] for x in counter.most_common(40) if len(x[0])>3]\nprint(\"Most common words in Duties\")\nprint(count)","2b2b07e5":"corpus=build_corpus(df,'duties')\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=40, workers=4)\n","914292f2":"tsne_plot(model,'Duties')","91a183f1":"lem=WordNetLemmatizer()\ntext=[lem.lemmatize(w) for w in word_tokenize(duties)]\nvect=TfidfVectorizer(ngram_range=(1,3),max_features=200)\nvectorized_data=vect.fit_transform(text)\nid2word=dict((v,k) for k,v in vect.vocabulary_.items())\n\n","fa77f326":"corpus=gensim.matutils.Sparse2Corpus(vectorized_data,documents_columns=False)\nldamodel = gensim.models.ldamodel.LdaModel(corpus,id2word=id2word,num_topics=8,random_state=34,passes=25,per_word_topics=True)\n","5656f839":"ldamodel.show_topic(1)","85b93475":"def format_topics_sentences(ldamodel, corpus, texts):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodel, corpus=corpus, texts=build_corpus(df,'duties'))\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\ndf_dominant_topic.dropna(inplace=True)\ndf_dominant_topic.head(5)","7c600920":"data=build_corpus(df,'duties')","2939a44b":"topics = ldamodel.show_topics(formatted=False)\ndata_flat = [w for w_list in build_corpus(df,'duties') for w in w_list]\ncounter = Counter(data_flat)\n\nout = []\nfor i, topic in topics:\n    for word, weight in topic:\n        out.append([word, i , weight, counter[word]])\n\ndf_plot= pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(4, 2, figsize=(10,12), sharey=True, dpi=160)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=df_plot.loc[df_plot.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=df_plot.loc[df_plot.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    #ax_twin.set_ylim(0, 0.040); ax.set_ylim(0, 4000)\n    ax.set_title('Topic: ' + str(i), color=cols[i])\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(df_plot.loc[df_plot.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords',y=1)    \nplt.show()","0b93bc7c":"\nplt.figure(figsize=(7,7))\ncount=df['selection'].astype(str).value_counts()[:10]\nsns.barplot(y=count.index,x=count,palette='rocket')\nplt.gca().set_yticklabels(count.index,rotation='45')\nplt.show()\n","949e886f":"def pronoun(data):\n    \n    '''function to tokenize data and perform pos_tagging.Returns tokens having \"PRP\" tag'''\n    \n    prn=[]\n    vrb=[]\n    token=word_tokenize(data)\n    pos=pos_tag(token)\n   \n    vrb=Counter([x[0] for x in pos if x[1]=='PRP'])\n    \n    return vrb\n    \n\n\nreq_prn=pronoun(req)\nduties_prn=pronoun(duties)\nprint('pronouns used in requirement section are')\nprint(req_prn.keys())\nprint('\\npronouns used in duties section are')\nprint(duties_prn.keys())\n","d459e8a4":"names=['senior waterman','policeman']\nfor name in names:\n    z=re.match(r'\\w+?\\s?\\w+(man|women)$',name)\n    print(z)","75f4e2e8":"for name in df['Position']:\n    z=re.match(r'\\w+?\\s?\\w+(man|women)$',name)\n    if z is not None:\n        print(z)\n    ","7e5b58e3":"\ndef similar_jobs(job):\n    \n    ''' function to find and return jobs with similar job title.take a single argument\n            - job title\n            returns\n                -list of similar jobs '''\n    \n    word1=word_tokenize(job)\n    jobs=[]\n    for i,name in enumerate(df['Position']):\n        word2=word_tokenize(name)\n        distance=jaccard_distance(set(ngrams(word1,n=1)),set(ngrams(word2,n=1)))\n        if(distance<.55):\n            jobs.append((name,i))\n    return jobs","755704f9":"similar_jobs(df['Position'][10])","c58a9eca":"def similar_req(job):\n    \n    ''' function to find and return jobs with similar job title.take a single argument\n            - job title\n            returns\n                -list of similar jobs '''\n    \n    word1=word_tokenize(job)\n    jobs=[]\n    for i,name in enumerate(df['requirements']):\n        word2=word_tokenize(name)\n        distance=jaccard_distance(set(ngrams(word1,n=1)),set(ngrams(word2,n=1)))\n        if(distance<.5):\n            jobs.append((name,df.iloc[i]['Position']))\n    return jobs","a1df9918":"similar_req(df['requirements'][10])","7926d2a4":"df['requirements'][312]","96535b7d":"reading=[]\nfor file in df['File Name']:\n    text=open(\"..\/input\/cityofla\/CityofLA\/Job Bulletins\/\"+file,'r',encoding=\"ISO-8859-1\").read()\n    sentence = text.count('.') + text.count('!') + text.count(';') + text.count(':') + text.count('?')\n    words = len(text.split())\n    syllable = 0\n    for word in text.split():\n        for vowel in ['a','e','i','o','u']:\n            syllable += word.count(vowel)\n        for ending in ['es','ed','e']:\n            if word.endswith(ending):\n                   syllable -= 1\n        if word.endswith('le'):\n            syllable += 1\n            \n    G = round((0.39*words)\/sentence+ (11.8*syllable)\/words-15.59)\n    reading.append(G)","84c3d879":"plt.hist(reading)\nplt.xlabel('Flesch Index')\nplt.title('Flesch index distribution')\nplt.show()","496f1ab6":"## Finding and removing gender biases[^](#4)<a id=\"3\"><\/a><br>","27f658d1":"1. Surprisingly, i couldn't find any gender biased or racist pronouns in **Requirement ** or **Duties section**\n2. you can see all the pronouns used are neutral.","96b4f011":"Here in this section I am trying to find similar jobs given a job title.\n- **Assumption** : I assume that similar jobs have similar job title.    \n               \nI have used jaccard distance to find the text similarity and compare it with a threashold value to obtain the jobs.     \nThis method can output the jobs the same domain so that candidates who posses the right skills can apply for similar jobs in the domain.I believe clustering jobs can improve the pool of applicants to great extend.\n","f0f7201d":"- To improve the diversity employers are encourged to give chance to **Freshers** and people willing to work **part time**.\n     This can attract greater pool of applicants.","516e37fb":"### Word Counts of Topic Keywords","384ea48e":"## What is in this kernel?","ca91326a":"We can see that there is more job opportunities created in the months of **March,October and December**","b113da21":"**Flesch Index  -------         Text file reading Grade**\n\n   0-30      ---------              College                 \n\n   50-60     ---------               High School                          \n\t\n   90-100    ---------                Fourth Grade\n\n\t\n1. From above the flesch-kincaid Grade level formula is used to compute the equivalent Grade level G \u2212","75c97853":"#### Extracting the headings from job bulletins","8ebbc618":"- We can see that **service sector** dominates in creating opputunities.","e5624e9c":"- In this section we will try and find out if there is any gender bias in job titles.\n    A **gender-specific job title** is a name of a job that also specifies or implies the gender of the person performing that job. \n    the job title **policeman** implies that the person is male. A gender-neutral job title, on the other hand, is one that does not specify or imply       gender, such as firefighter or a lawyer.","fb709de9":"There are 3 comma seperated files inside the folder,\n1. job titles : contains the title given to different jobs available.\n2. sample job class export template.csv : contains sample job bulletin to csv export details.\n3. kaggle_data_dictionary : contains name and description of each column that is in sample job class export template.","b3bb56b5":"### Finding similar jobs","1bd5b42b":"- we can see clusters of words used in **Requirement** section.","1c4b0e1f":"- you can clearly observe that the jobs outputted are from the same domain and are moreover similar.","30707dbd":" Now we have a proper comma seperataed file containing most of the information we need.we will now start exploring it. ","6f1de20f":"### what about deadlines ?","57dbe77b":"- we can observe that there is some specefic pattern or format which is kept while writing job bulletins.\n- The order of the headings almost coincides with each other,which will be beneficial for our task.","77c37ed3":"### Experience or College degree, which is more preferred ?","0d0d72bb":"### Which month of the year offers most opportunities?","87c6456c":"- I think it is clear that 50 percent of the job requires atleat two years experience in the field.\n- This has a negative effect of keeping freshers away from entering the job sector.","be96e906":"### TSNE","06464f72":"We will print headings from first two job bulletins file.","ae2af6b4":"In the follow section i am trying investigate if there is any gender biased terms used in **Requirement** and **Duties** section of the job bulletin.   \nFor that i will pos tag all the text data in the requirement field and then,\n- Extract the words having pronoun tag.\n- check if any gender biased terms like he\/she is used in the field.\n","6085317e":"###  Latent Dirichlet Allocation (LDA)","275266f3":"Wow ! All the postings are open from **friday** ! This is pretty interesting.\n- Is there any specific reason behind this?\nWe will try to find out.\nanyway its good to post all opening in a day  of week so  that candidates can refer the bulletins and apply without missing any openings.","77bb032b":"- It is evident from the above graph that job oppurtunities is constantly increasing after 2012 or so. \n- job oppurtunities has never decreased.","3e389403":"### Topic modeling visualization","71c965c8":"## Data preparation[^](#1)<a id=\"1\" ><\/a><br>","7667ad03":"![](https:\/\/media.giphy.com\/media\/3XAU2dw8fjghZmsRZd\/giphy.gif)","beb24bad":"### Preparing the data dictionary","169f9bf4":"In the below section we will take a look at the three csv files which was just loaded to get basic understanding of the data.","b335fe65":"- I think employers should post a deadline for job application in the job bulletins to avoid any unwanted confusions between candidates.","f6a02961":"### checking all subdirectories","14495e61":"- we can see that index is always between 0-30,which comes under college grade.May be city of LA officials should look in to this and moderate their bulletins to make it simple to read !","7c4218e0":"- City of LA can use any of the similarity functions available to find jobs having similar requirements,for example jaccard distace,cosine similary etc..I have implemented jaccard similarity the in above section.","d260025b":"- FleschKincaid Grade Level       \nDesigned to indicate how difficult a reading passage is to understand. The result is a number that corresponds with a U.S grade level.\n> FKGL = 0.39 * (total words\/ total \nsentences) + 11.8 (total syllables\/ \ntotal words) -15.59\n\n\n","bf2d1e89":"### What are the common requirements for any post?","95080e3d":"### What about the salary distribution?","43a4f9fb":"## Getting  basic ideas[^](#0)<a id=\"0\"><\/a><br>","f6d17902":"## Exploratory data analysis[^](#2)<a id=\"2\"><\/a><br>","bc880854":"### Which day of the week ?","73dd13bb":"### Suggestions[](#3)<a id='3'><\/a><br>","fb3bccce":"- job bulletins :This directory contains the job bulletins in text format.\n- additional data :This directory contains additional data in pdf and csv format. ","c71bd663":"### which are the common job sectors in LA?","8a85d833":"- we can observe that thet salaries typically varies from $50k to $150k.\n- Most jobs salary start from $80000.","1a3fe5b8":"- Nothing ! I think the authorities hava done a good job by removing possible gender biases from job titles and renamed it suitably.","831b9c7f":"- Plotting Grade level distribution","2393d464":"- It is evident that  **interview** ,**Essay** and **Questionnaire** are the most common selection criterias.","59652e44":"## Data Science for Good: City of Los Angeles","6db97eab":"1. It might be a good idea to have **different display boards** for service and product sectors,so that candidates can easily find job of their preference.\n2. 50 percent of the job requires atleat two years experience in the field.This has a negative effect of keeping freshers away from entering the job sector.\n3. It  might be a good idea to declare March,October,December months as **hiring months** and this could even attract a lot of potential candidates.\n4. Define a deadline to avoid confusions and delaying.(2weeks is ideal)\n","c21d83e6":"### Finding jobs with similar requirements","582b0a54":"We will find and print all the files inside **Additional data** that has csv format.","d5036340":"- It can be observed that companies prefer  \n- **experienced** \n- **educated professionals**  having **degree from an accredicted university**\n- also willing to work **full-time**","fb7237df":"### Reading the required csv files","babf8676":"### What about the full time experience?","23ab3651":"### Word Cloud of Requirements","2d46ea3d":"- So you can see that the code works well.We will try in our dataset.","6765974a":"### Problem statement","0bfac1de":"In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output.","2b63cd7a":"- Almost all jobs bulletins specify the experience needed by candidate to qualify for the job.\n  But only some post the required educational baground.This might be in the assumption that an experienced candidate will surely posses College or university degree.","0bebec26":"### Most influential and common words in duties","09b016ac":"0. [Getting basic idea](#0)\n1. [Data Preparation](#1)\n2. [Exploratory data analysis](#2)\n    1. [Suggestions ](#3)\n3. [Finding and removing word biases](#4)\n4. [Clustering jobs ](#5)\n5. [Readability Index](#6)\n\n","beda259a":"### Word 2 Vec","e7d8f54b":"### What are the most common selection criterias?","f7b914a0":"Topic modeling is a type of statistical modeling for discovering the abstract \u201ctopics\u201d that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.","feb2c21a":"### If you like my kernel please consider upvoting.Thank you :)","3fa31d2b":"### Word 2 Vec","c9602540":"When it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the documents is also interesting to look.\n\nLet\u2019s plot the word counts and the weights of each keyword in the same chart.\n\nYou want to keep an eye out on the words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important. The chart I\u2019ve drawn below is a result of adding several such words to the stop words list in the beginning and re-running the training process.","fe11296f":"*** - It  might be a good idea to declare these months as **hiring months** and this could even attract a lot of potential candidates.","87cc85f7":"###  Is there any gender bias in job titles?","3ba1e71c":"Here,\n- **Position** implies JOB_CLASS_TITLE\n- **Duties** implies JOB_DUTIES\n- **Requirements** : All of the job requirements posted under requirements section\n- **Salary** has been broken down to **Salary_start** and **Salary_end** which signifies the salary range.\n    for example $ 5000 to $ 6000 \n        salary_start = 5000\n        salary_end = 6000\n- **Opendate** : The job application open date\n- **deadline** : deadline for applying for the job.\n- **selection** : The list of  selection criterias.","dc4cee31":"### Importing Libraries","27f19891":"### TSNE","f3fb530b":"### Duties","929d5afc":"## Readability Index[^](#6)<a id=\"5\"><\/a><br>","f84701d2":"### Has job opportunities really increased recently?","0fe59974":"### Most influential words in requirements","20373517":"### Which are the best paid jobs in LA?","2ac77f50":"### Which the jobs with highest salary deviation?","69488635":"The goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to:\n1.  identify language that can negatively bias the pool of applicants; \n2.  improve the diversity and quality of the applicant pool; and\/or \n3.  make it easier to determine which promotions are available to employees in each job class.","8f190b91":"## Clustering Jobs [^](#5)<a id=\"4\"><\/a><br>","cfd89c2f":"###  What is the Dominant topic and its percentage contribution in each document","9a1a11a0":"- It might be a good idea to have **different display boards** for service and product sectors,so that candidates can easily find job of their preference.","91f10a3b":"#### Headings","2a4cc352":"### Is there any Gender bias in job bulletins?"}}