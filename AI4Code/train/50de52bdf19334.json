{"cell_type":{"1dc2901c":"code","0756f52a":"code","1475125b":"code","4a5803cb":"code","0a224aed":"code","e600d187":"code","2c73cc5c":"code","855e490d":"code","a74c2738":"code","7e0977ff":"code","fe5d2802":"code","6da74ac1":"code","77d09ba0":"code","2018030f":"code","f5701de3":"code","b9aed4f6":"code","04798f6a":"code","ecb6e2d0":"code","136ee262":"code","ad969473":"code","037c1853":"code","a70c0d62":"code","2c964e27":"code","77069982":"code","3bddfdf3":"code","afd77a06":"code","b74d44c2":"code","52417d84":"code","386dfed4":"code","ff1c4096":"code","d053e98f":"code","37b3540a":"code","c3c5527f":"code","7ef7378c":"code","82b34658":"code","b661ceba":"markdown","8e02600e":"markdown","cece98fb":"markdown","b8a8cf5b":"markdown","35ec018c":"markdown","eee281fd":"markdown","0982a44e":"markdown","c6e9065c":"markdown","9871f4d1":"markdown","e447ee20":"markdown","0c36692b":"markdown","9714dadb":"markdown","dba4e266":"markdown","ec07a6e0":"markdown","a5e87ca1":"markdown","caf48e2d":"markdown","6c562f2c":"markdown","3ccb7201":"markdown"},"source":{"1dc2901c":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","0756f52a":"import fastai\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nimport cv2\n\nimport pandas as pd\nimport matplotlib.pyplot as plt","1475125b":"# Making pretrained weights work without needing to find the default filename\nif not os.path.exists('\/tmp\/.cache\/torch\/checkpoints\/'):\n        os.makedirs('\/tmp\/.cache\/torch\/checkpoints\/')\n!cp '..\/input\/resnet50\/resnet50.pth' '\/tmp\/.cache\/torch\/checkpoints\/resnet50-19c8e357.pth'\n!cp '..\/input\/densenet161\/densenet161.pth' '\/tmp\/.cache\/torch\/checkpoints\/densenet161-8d451a50.pth'","4a5803cb":"import os\nos.listdir('..\/input\/')","0a224aed":"print('Make sure cudnn is enabled:', torch.backends.cudnn.enabled)","e600d187":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 1667\nseed_everything(SEED)","2c73cc5c":"base_image_dir = os.path.join('..', 'input\/aptos2019-blindness-detection\/')\ntrain_dir = os.path.join(base_image_dir,'train_images\/')\ndf = pd.read_csv(os.path.join(base_image_dir, 'train.csv'))\ndf['path'] = df['id_code'].map(lambda x: os.path.join(train_dir,'{}.png'.format(x)))\ndf = df.drop(columns=['id_code'])\ndf = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\ndf.head(10)","855e490d":"len_df = len(df)\nprint(f\"There are {len_df} images\")","a74c2738":"df['diagnosis'].hist(figsize = (10, 5))","7e0977ff":"IMG_SIZE = 256 #512\n\ndef crop_image1(img,tol=7):\n    # img is image data\n    # tol  is tolerance\n        \n    mask = img>tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n    #         print(img1.shape,img2.shape,img3.shape)\n            img = np.stack([img1,img2,img3],axis=-1)\n    #         print(img.shape)\n        return img\n\ndef load_ben_color(path, sigmaX=10):\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = crop_image_from_gray(image)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n        \n    return image\n\ndef _load_format(path, convert_mode, after_open)->Image:\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = crop_image_from_gray(image)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0), 10) ,-4 ,128)\n                    \n    return Image(pil2tensor(image, np.float32).div_(255)) #return fastai Image format\n\nvision.data.open_image = _load_format\n","fe5d2802":"# create Stratified validation split (20%)\nfrom sklearn.model_selection import StratifiedKFold\ncv = StratifiedKFold(n_splits=5, random_state=42)\ntr_ids, val_ids = next(cv.split(df.path, df.diagnosis))\nprint(len(tr_ids), len(val_ids))\n_ = df.loc[val_ids].hist()","6da74ac1":"bs=32   \nsrc = (ImageList.from_df(df=df,path='.\/',cols='path') #get dataset from dataset\n        #.split_by_rand_pct(0.2, seed=42) #Splitting the dataset\n       .split_by_idx(val_ids)\n        .label_from_df(cols='diagnosis',label_cls=FloatList) #obtain labels from the level column\n      )\nsrc","77d09ba0":"tfms = get_transforms(do_flip=True, flip_vert=True, max_rotate=0.10, max_zoom=1.3, max_warp=0.0, max_lighting=0.2)\ndata = (\n    src.transform(tfms,size=128)\n    .databunch(bs=bs)\n    .normalize(imagenet_stats)\n)","2018030f":"from sklearn.metrics import cohen_kappa_score\ndef quadratic_kappa(y_hat, y):\n    return torch.tensor(cohen_kappa_score(torch.round(y_hat), y, weights='quadratic'),device='cuda:0')","f5701de3":"#update1 - kappa from fastai\n#kappa = KappaScore()\n#kappa.weights = \"quadratic\"\n\n#update2 - kappa from fastai did not work, I get errors during training, switching to sklearn, need to debug later","b9aed4f6":"learn = cnn_learner(data, base_arch=models.densenet161 ,metrics=[quadratic_kappa], #densenet161\n                    callback_fns=[partial(EarlyStoppingCallback, monitor='quadratic_kappa',\n                                          min_delta=0.01, patience=3)],\n                    model_dir='\/kaggle',pretrained=True)\n#todo: callback best kappa score or mse for regression\n#partial(SaveModel, monitor='quadratic_kappa')","04798f6a":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","ecb6e2d0":"learn.fit_one_cycle(5, 2e-2)\n","136ee262":"learn.recorder.plot_losses()\nlearn.recorder.plot_metrics()","ad969473":"#learn.save('stage-1_dn161')\n","037c1853":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()","a70c0d62":"learn.fit_one_cycle(10, max_lr=slice(1e-5,1e-3))","2c964e27":"learn.export()\nlearn.save('stage2')\nlearn.recorder.plot_losses()","77069982":"learn.show_results(ds_type=DatasetType.Train, rows=4, figsize=(8,10))","3bddfdf3":"valid_preds = learn.get_preds(ds_type=DatasetType.Valid)","afd77a06":"import numpy as np\nimport pandas as pd\nimport os\nimport scipy as sp\nfrom functools import partial\nfrom sklearn import metrics\nfrom collections import Counter\nimport json","b74d44c2":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = metrics.cohen_kappa_score(y, X_p, weights='quadratic')\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n        print(-loss_partial(self.coef_['x']))\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","52417d84":"optR = OptimizedRounder()\noptR.fit(valid_preds[0],valid_preds[1])","386dfed4":"coefficients = optR.coefficients()\nprint(coefficients)","ff1c4096":"from fastai.core import *\nfrom fastai.basic_data import *\nfrom fastai.basic_train import *\nfrom fastai.torch_core import *\ndef _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, num_pred:int=10) -> Iterator[List[Tensor]]:\n    \"Computes the outputs for several augmented inputs for TTA\"\n    dl = learn.dl(ds_type)\n    ds = dl.dataset\n    old = ds.tfms\n    aug_tfms = [o for o in learn.data.train_ds.tfms]\n    try:\n        pbar = master_bar(range(num_pred))\n        for i in pbar:\n            ds.tfms = aug_tfms\n            yield get_preds(learn.model, dl, pbar=pbar)[0]\n    finally: ds.tfms = old\n\nLearner.tta_only = _tta_only\n\ndef _TTA(learn:Learner, beta:float=0, ds_type:DatasetType=DatasetType.Valid, num_pred:int=10, with_loss:bool=False) -> Tensors:\n    \"Applies TTA to predict on `ds_type` dataset.\"\n    preds,y = learn.get_preds(ds_type)\n    all_preds = list(learn.tta_only(ds_type=ds_type, num_pred=num_pred))\n    avg_preds = torch.stack(all_preds).mean(0)\n    if beta is None: return preds,avg_preds,y\n    else:            \n        final_preds = preds*beta + avg_preds*(1-beta)\n        if with_loss: \n            with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n            return final_preds, y, loss\n        return final_preds, y\n\nLearner.TTA = _TTA","d053e98f":"# remove zoom from FastAI TTA\n#tta_params = {'beta':0.12, 'scale':1.0}","37b3540a":"sample_df = pd.read_csv('..\/input\/aptos2019-blindness-detection\/sample_submission.csv')\nsample_df.head()","c3c5527f":"learn.data.add_test(ImageList.from_df(sample_df,'..\/input\/aptos2019-blindness-detection',folder='test_images',suffix='.png'))\n#preds,y = learn.get_preds(ds_type=DatasetType.Test)\n","7ef7378c":"\npreds,y = learn.TTA(ds_type=DatasetType.Test)\n#, **tta_params)","82b34658":"test_predictions = optR.predict(preds, coefficients)\nsample_df.diagnosis = test_predictions.astype(int)\nsample_df.head()\nsample_df.to_csv('submission.csv',index=False)","b661ceba":"## Training (Transfer learning)","8e02600e":"## Inference","cece98fb":"\nTTA wraps get_preds with augmentation\nTTA parameters I need to research","b8a8cf5b":"For regression use Floatlist as label class, default CategoryList for classification","35ec018c":"## EDA","eee281fd":"The Kaggle competition used the Cohen's quadratically weighted kappa so I have that here to compare. This is a better metric when dealing with imbalanced datasets like this one, and for measuring inter-rater agreement for categorical classification (the raters being the human-labeled dataset and the neural network predictions). Here is an implementation based on the scikit-learn's implementation, but converted to a pytorch tensor, as that is what fastai uses.","0982a44e":"we will be using a variable learning rate for the various layers. Using 'slice'  takes a start value and a stop value and train the very first layers at a learning rate of 1e-6, and the very last layers at a rate of 1e-4, and distribute all the other layers across that (i.e. between those two values equally).","c6e9065c":"## Ensemble - Todo","9871f4d1":"Version history - \n* V7-8 -  Metrics for understanding loss and QWK\n* V8-10 - Tried using Kappascore from fastai but it did not work. \n* V 11-12 - split train\/test using Stratified KFold. Changing Batch size\n* V13 - Using TTA\n\nTodo - Kfold CV ","e447ee20":"default loss is cross entropy, we can try some custom loss function \n\nlearn.loss_func = NewLoss()\n\nWeight decay during training is another thing needs to be researched","0c36692b":"## Optimize the Metric\n\nOptimizing the quadratic kappa metric was an important part of the top solutions in the previous competition. Thankfully, @abhishek has already provided code to do this for us. We will use this to improve the score.","9714dadb":"Here we are training the last layer, save the weights and next we unfreeze all layers and train","dba4e266":"Checking the distribution of labels and basic EDA..\n\nTodo: Training with old competition data, data imbalance etc.","ec07a6e0":"## Submission\nLet's now create a submission","a5e87ca1":"## TTA\n\nTest-time augmentation, or TTA, is a commonly-used technique to provide a boost in your score, and is very simple to implement. \n* Below is custom TTA implementation from one of the public kernels. \n","caf48e2d":"## Fast AI Data Block API for Regression","6c562f2c":"##References\nFASTAI Course Videos and notes\nhttps:\/\/course.fast.ai\n\n\nStarter kernels - \nhttps:\/\/www.kaggle.com\/demonplus\/fast-ai-starter-with-resnet-50\nhttps:\/\/www.kaggle.com\/lhohoz\/aptos-fastai-resnet50-with-previous-data\n\nAll custom functions are from public kernels, if I missed any credits please remind me. Will also welcome any suggestion as I am still newbie to this field.\n\nPlease upvote if you find useful.","3ccb7201":"**Training:**\n\nWe use transfer learning, where we retrain the last layers of a pretrained neural network. I use the ResNet and Densenet architectures trained on the ImageNet dataset, which has been commonly used for pre-training applications in computer vision. Fastai makes it quite simple to create a model and train.\n* Pretrained Weights have to be from publicly available datasets from Kaggle and not from internet.\n* resnet slightly better than densenet here"}}