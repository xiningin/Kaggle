{"cell_type":{"84cc07a5":"code","7d64068d":"code","a34ccb29":"code","46892575":"code","fc38db78":"code","aba15d5f":"code","6869503e":"code","9a3913e4":"code","f3aa84f0":"code","626dbbfe":"code","b082132d":"code","6d561ff8":"code","49096686":"code","78188c49":"code","6f4c0c16":"code","96197155":"code","5cc4e8fb":"code","dbc33e72":"code","e3207339":"code","6ff0fabf":"code","3d74a940":"code","29ce5f1b":"code","820f130a":"code","4b227e08":"code","34cea5f2":"code","28b78cc2":"code","79faf693":"code","8ba29b7f":"code","55eaa221":"code","6afe2705":"code","5a99f7b7":"code","09652263":"code","1bebb276":"code","9cb1d34a":"code","ff510be8":"code","ab5c3437":"code","f79d4ac4":"code","1c4bca68":"code","3161e77c":"code","c2723502":"code","da358ead":"code","97210cf5":"code","2456104e":"code","29d51638":"code","515f1b6d":"code","fe5266f7":"code","3a5e71aa":"code","4f90e585":"code","58a8d8eb":"code","5b6c7cc0":"code","bc6e4729":"code","2a0bdb85":"code","504a8eea":"code","0340b2d3":"code","bc3a91bc":"code","b70e3921":"code","309ab9e2":"code","88e8d026":"code","fcbaccf2":"code","b2388b42":"code","e1521dfe":"code","e60fe07e":"markdown","ca10599a":"markdown","cfa97b1e":"markdown","64c77f1c":"markdown","ee1be44b":"markdown","74228785":"markdown","481da49a":"markdown","4af01180":"markdown","80269204":"markdown","8d1526e5":"markdown"},"source":{"84cc07a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7d64068d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport warnings\n\n#sklearn model\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nwarnings.filterwarnings('ignore')","a34ccb29":"train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","46892575":"train.head()","fc38db78":"FEATURES = [col for col in train.columns if col not in ['id', 'target']]","aba15d5f":"print(f'train shape: {train.shape}')\nprint(f'test shape: {test.shape}')","6869503e":"train.drop(['id', 'target'], axis=1).info()","9a3913e4":"train.describe()","f3aa84f0":"x_train = train.drop(['id', 'target'], axis=1)\ny_train = train.target\n\nx_test = test.drop('id', axis=1)","626dbbfe":"feature_std = x_train.std(axis=0)","b082132d":"feature_std = feature_std \/ (x_train.max(axis=0) - x_train.min(axis=0))","6d561ff8":"sns.histplot(feature_std)","49096686":"index = feature_std[feature_std< 0.1].index","78188c49":"x_train.drop(index, axis=1).shape","6f4c0c16":"from sklearn.decomposition import PCA\n\npca = PCA(2)\npca.fit(x_train)\nx_train_pca = pca.transform(x_train)","96197155":"plt.scatter(x_train_pca[:, 0], x_train_pca[:,1], c = y_train)","5cc4e8fb":"del train, test\ngc.collect()","dbc33e72":"# target visualization\nplt.pie(y_train.value_counts(), labels=['One', 'Zero'], autopct='%1.1f%%')\nplt.axis('equal') ","e3207339":"# #feature visualization\n# ncols = 3\n# nrows = x_train.shape[1] \/\/ ncols + (x_train.shape[1] % ncols != 0)\n# fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 5*nrows))\n\n# for row in range(nrows):\n#     for col in range(ncols):\n#         index = row * ncols + col\n#         if index >= x_train.shape[1] :\n#             break\n#         sns.kdeplot(x_train.iloc[:, index], ax=axes[row, col])\n#         sns.kdeplot(x_test.iloc[:, index], ax=axes[row, col])","6ff0fabf":"# #feature visualization\n# ncols = 3\n# nrows = x_train.shape[1] \/\/ ncols + (x_train.shape[1] % ncols != 0)\n# fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 5*nrows))\n\n# for row in range(nrows):\n#     for col in range(ncols):\n#         index = row * ncols + col\n#         if index >= x_train.shape[1] :\n#             break\n#         sns.kdeplot(x_train[y_train==1].iloc[:, index], ax=axes[row, col], label='One')\n#         sns.kdeplot(x_train[y_train==0].iloc[:, index], ax=axes[row, col], label='Zero')\n#         plt.legend()\n","3d74a940":"# np.sum((x_train[:, 96] > 2.5) == y_train) \/ y_train.shape[0]","29ce5f1b":"value = x_train.mean(axis=1)","820f130a":"value = x_train.sum(axis=1)","4b227e08":"sns.kdeplot(value[y_train == 1])\nsns.kdeplot(value[y_train == 0])","34cea5f2":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\ny_train = y_train.values\ngc.collect()","28b78cc2":"feature_std = x_train.std(axis=0)","79faf693":"# def objective(trial):\n\n#     param_grid = {'objective': 'binary:logistic',\n#               'use_label_encoder': False,\n#               'n_estimators': trial.suggest_int('n_estimators', 500, 5000),\n#               'learning_rate': trial.suggest_discrete_uniform('learning_rate',0.01,0.1,0.01),\n#               'subsample': trial.suggest_categorical ('subsample', [0.2,0.3,0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n#               'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1.0, 0.1),\n#               'max_depth': trial.suggest_int('max_depth', 2, 20),\n#               'booster': 'gbtree',\n#               'gamma': trial.suggest_uniform('gamma',1.0,10.0),\n#               'reg_alpha': trial.suggest_int('reg_alpha',50,100),\n#               'reg_lambda': trial.suggest_int('reg_lambda',50,100),\n#               'random_state': 42,\n#                  }\n\n#     x_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train, test_size=0.3, random_state=50)\n#     xgb_model = XGBClassifier(**param_grid, tree_method='gpu_hist', predictor='gpu_predictor',\n#                             eval_metric=['logloss'])\n\n#     xgb_model.fit(x_train_, y_train_, verbose=False)\n#     y_pred = xgb_model.predict_proba(x_val)[:, 1]\n#     return roc_auc_score(y_val, y_pred)","8ba29b7f":"# train_time = 1 * 30 * 60 # h * m * s\n# study = optuna.create_study(direction='maximize', sampler=TPESampler(), study_name='XGBClassifier')\n# study.optimize(objective, timeout=train_time)\n\n# print('Number of finished trials: ', len(study.trials))\n# print('Best trial:')\n# trial = study.best_trial\n\n# print('\\tValue: {}'.format(trial.value))\n# print('\\tParams: ')\n# for key, value in trial.params.items():\n#     print('\\t\\t{}: {}'.format(key, value))","55eaa221":"# # xgb_params = trial.params\n# xgb_params = {\n#                 'n_estimators': 4492,\n#                 'learning_rate': 0.01,\n#                 'subsample': 1.0,\n#                 'colsample_bytree': 0.2,\n#                 'max_depth': 15,\n#                 'gamma': 1.0328829988080024,\n#                 'reg_alpha': 100,\n#                 'reg_lambda': 93 }\n\n# xgb_params['tree_method'] = 'gpu_hist'\n# xgb_params['predictor'] = 'gpu_predictor'\n# xgb_params['use_label_encoder'] = False","6afe2705":"# from sklearn.model_selection import KFold\n\n# n_split = 10\n# kfold = KFold(n_split)\n\n# val_pred = np.zeros(y_train.shape)\n# y_test = np.zeros((x_test.shape[0],))\n\n# for i, (train_index, val_index) in enumerate(kfold.split(x_train)):\n#     # train model\n#     print(\"fold {} training\".format(i))\n#     model = XGBClassifier(**xgb_params, eval_metric=['logloss'])\n#     model.fit(x_train[train_index], y_train[train_index])\n    \n#     # predict val and test\n#     val_pred[val_index] = model.predict_proba(x_train[val_index])[:, 1]\n#     vla_score = roc_auc_score(y_train[val_index], val_pred[val_index])\n#     print(\"fold {} validation auc score {}\".format(i, vla_score))\n    \n#     y_test += model.predict_proba(x_test)[:, 1] \/ n_split\n    \n","5a99f7b7":"# from sklearn.linear_model import LogisticRegression\n\n# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train)\n# model = LogisticRegression()\n# model.fit(x_train, y_train)\n# y_pred = model.predict_proba(x_val)[:, 1]\n# roc_auc_score(y_val, y_pred)","09652263":"from sklearn.svm import LinearSVC","1bebb276":"# def objective(trial):\n\n#     param_grid = {\n#               'tol': trial.suggest_categorical('tol', [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10]),\n#               'max_iter': trial.suggest_int('max_iter',1000, 5000)\n#                  }\n\n#     x_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train, test_size=0.3, random_state=50)\n#     svc = LinearSVC(**param_grid, penalty='l2', dual=False)\n    \n#     svc.fit(x_train_, y_train_)\n#     val_pred_svc = svc.decision_function(x_val)\n#     return roc_auc_score(y_val, val_pred_svc)","9cb1d34a":"# train_time = 1 * 30 * 60 # h * m * s\n# study = optuna.create_study(direction='maximize', sampler=TPESampler(), study_name='svc')\n# study.optimize(objective, timeout=train_time)\n\n# print('Number of finished trials: ', len(study.trials))\n# print('Best trial:')\n# trial = study.best_trial\n\n# print('\\tValue: {}'.format(trial.value))\n# print('\\tParams: ')\n# for key, value in trial.params.items():\n#     print('\\t\\t{}: {}'.format(key, value))","ff510be8":"# svc_params = trial.params","ab5c3437":"# from sklearn.model_selection import KFold\n\n# n_split = 10\n# kfold = KFold(n_split)\n\n# val_pred = np.zeros(y_train.shape)\n# y_test = np.zeros((x_test.shape[0],))\n\n# for i, (train_index, val_index) in enumerate(kfold.split(x_train)):\n#     # train model\n#     print(\"fold {} training\".format(i))\n    \n#     svc = LinearSVC(**svc_params, penalty='l2', dual=False)\n#     svc.fit(x_train[train_index], y_train[train_index])\n    \n#     # predict val and test\n#     val_pred[val_index] = svc.decision_function(x_train[val_index])\n#     vla_score = roc_auc_score(y_train[val_index], val_pred[val_index])\n#     print(\"fold {} validation auc score {}\".format(i, vla_score))\n    \n#     y_test += svc.decision_function(x_test) \/ n_split\n    ","f79d4ac4":"x_data = x_train\ny_data = y_train","1c4bca68":"x_train, x_val, y_train, y_val = train_test_split(x_train, y_train)\n\nsvc = LinearSVC(tol=1e-6, penalty='l2', dual=False, max_iter=1900)\nsvc.fit(x_train, y_train)\ny_pred = svc.decision_function(x_val)\nscore = roc_auc_score(y_val, y_pred)\n\n# print('val auc score :', score)\n# y_test = svc.decision_function(x_test)[:, 1]\n# print(y_test[:10])","3161e77c":"print(score)","c2723502":"y_pred_binary = svc.predict(x_data)","da358ead":"np.sum(y_pred_binary == y_data) \/ y_data.shape[0]","97210cf5":"y_pred_data = svc.decision_function(x_data)","2456104e":"sns.kdeplot(y_pred_data[y_data == 1], label='One')\nsns.kdeplot(y_pred_data[y_data == 0], label='Zero')\nplt.legend()","29d51638":"np.sum((y_pred_data > 0).astype('int') == y_data) \/ y_data.shape[0]","515f1b6d":"np.sum(y_pred_data > 0) \/ y_pred_data.shape[0]","fe5266f7":"1 - (np.sum(y_pred_data[y_data == 1] < 0) + np.sum(y_pred_data[y_data == 0] > 0)) \/ y_pred_data.shape[0]","3a5e71aa":"(np.sum(y_pred_data[y_data == 0] < 0)) \/ np.sum(y_pred_data < 0)","4f90e585":"print(score)","58a8d8eb":"y_data_pred = svc.predict(x_data)\ny_data_proba = svc.decision_function(x_data)\n\ny_data_proba_right = y_data_proba[y_data == y_data_pred]\ny_data_proba_wrong = y_data_proba[y_data != y_data_pred]\n\nimport seaborn as sns\n\nsns.histplot(y_data_proba_right, label='right')\nplt.legend()\nplt.figure()\nsns.histplot(y_data_proba_wrong, label='wrong')\nplt.legend()","5b6c7cc0":"plt.scatter(y_pred_data, np.zeros(y_pred_data.shape), c=y_data)\nplt.legend()","bc6e4729":"from sklearn.tree import DecisionTreeClassifier\n\nx_train, x_val, y_train, y_val = train_test_split(y_pred_data.reshape((-1, 1)), y_data, test_size=0.1)\ndecision_tree = DecisionTreeClassifier(max_depth=20)\n\ndecision_tree.fit(y_pred_data.reshape((-1, 1)), y_data)\ny_pred_train = decision_tree.predict_proba(x_train)[:, 1]\ntrain_score = roc_auc_score(y_train, y_pred_train)\nprint(f'roc_auc_score train score {train_score}')\n\ny_pred = decision_tree.predict_proba(x_val)[:, 1]\nval_score = roc_auc_score(y_val, y_pred)\nprint(f'roc_auc_score val score {val_score}')\n","2a0bdb85":"y_pred = svc.decision_function(x_test)","504a8eea":"y_test = decision_tree.predict_proba(y_pred.reshape((-1, 1)))[:, 1]","0340b2d3":"y_test.shape","bc3a91bc":"# x_data_1 = x_data[np.abs(y_data_proba) < 0.1]\n# y_data_1 = y_data[np.abs(y_data_proba) < 0.1]","b70e3921":"# y_val_proba = svc.decision_function(x_val)\n\n# y_pred = svc.decision_function(x_val[np.abs(y_val_proba) > 0.05])\n# score = roc_auc_score(y_val[np.abs(y_val_proba) > 0.05], y_pred)\n# print(score)","309ab9e2":"# x_train, x_val, y_train, y_val = train_test_split(x_data_1, y_data_1)\n\n# svc1 = LinearSVC(tol=1e-6, penalty='l2', dual=False, max_iter=1900)\n# svc1.fit(x_train, y_train)\n# y_pred = svc1.decision_function(x_val)\n# score = roc_auc_score(y_val, y_pred)\n# print(score)","88e8d026":"# from sklearn.tree import DecisionTreeClassifier\n\n# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train)\n\n# svc1 = DecisionTreeClassifier()\n# svc1.fit(x_train, y_train)\n# y_pred = svc1.predict_proba(x_val)[:,1]\n# score = roc_auc_score(y_val, y_pred)\n\n# print('val auc score :', score)\n# y_test = svc.predict_proba(x_test)[:, 1]\n# print(y_test[:10])","fcbaccf2":"# print(\"val auc score :\", roc_auc_score(y_train, val_pred))","b2388b42":"sub_mission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\nsub_mission.target = y_test\nsub_mission.to_csv('submission.csv', index=False)\n","e1521dfe":"plt.figure()\nsns.kdeplot(y_test)","e60fe07e":"## Submission","ca10599a":"## EDA","cfa97b1e":"### data visualization","64c77f1c":"## Read Data","ee1be44b":"### tree","74228785":"### feature engineering","481da49a":"### svm","4af01180":"## Validation Score","80269204":"## Train Model","8d1526e5":"### logic regretion "}}