{"cell_type":{"ff47fac9":"code","59585ece":"code","cdec19b7":"code","7f3d090a":"code","7f1fe65a":"code","d9973e5d":"code","50c66650":"code","b5cd54c9":"code","555c3a9b":"code","a1eab39a":"code","8a414d5f":"code","ff2fea9f":"code","02548974":"code","34a93ed2":"code","259731e3":"code","469cb27d":"markdown","24f15ad3":"markdown","bf26b8b3":"markdown","f4c091ca":"markdown","353085e0":"markdown","0e9cb0ae":"markdown","3d282d8f":"markdown","0bc61fd3":"markdown","b38c5a73":"markdown","8e5fae95":"markdown","39990e2d":"markdown","0d4ebc9f":"markdown","303dc319":"markdown","926701c7":"markdown"},"source":{"ff47fac9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","59585ece":"df = pd.read_csv('..\/input\/train.csv',nrows=2_000_000, usecols=[1,2,3,4,5,6,7])","cdec19b7":"df['pickup_datetime'] = df['pickup_datetime'].str.slice(0, 16)\ndf['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')","7f3d090a":"# Remove observations with missing values\n# Since there are only a few of these, i'm not concerned with imputation\ndf.dropna(how='any', axis='rows', inplace=True)\n\n# Removing observations with erroneous values\nmask = df['pickup_longitude'].between(-75, -73)\nmask &= df['dropoff_longitude'].between(-75, -73)\nmask &= df['pickup_latitude'].between(40, 42)\nmask &= df['dropoff_latitude'].between(40, 42)\nmask &= df['passenger_count'].between(0, 8)\nmask &= df['fare_amount'].between(0, 250)\n\ndf = df[mask]","7f1fe65a":"def dist(pickup_lat, pickup_long, dropoff_lat, dropoff_long):  \n    distance = np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n    \n    return distance","d9973e5d":"def transform(data):\n    # Extract date attributes and then drop the pickup_datetime column\n    data['hour'] = data['pickup_datetime'].dt.hour\n    data['day'] = data['pickup_datetime'].dt.day\n    data['month'] = data['pickup_datetime'].dt.month\n    data['year'] = data['pickup_datetime'].dt.year\n    data = data.drop('pickup_datetime', axis=1)\n\n    # Distances to nearby airports, and city center\n    # By reporting distances to these points, the model can somewhat triangulate other locations of interest\n    nyc = (-74.0063889, 40.7141667)\n    jfk = (-73.7822222222, 40.6441666667)\n    ewr = (-74.175, 40.69)\n    lgr = (-73.87, 40.77)\n    data['distance_to_center'] = dist(nyc[1], nyc[0],\n                                      data['pickup_latitude'], data['pickup_longitude'])\n    data['pickup_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                         data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    data['pickup_distance_to_ewr'] = dist(ewr[1], ewr[0], \n                                          data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_ewr'] = dist(ewr[1], ewr[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    data['pickup_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                          data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    \n    data['long_dist'] = data['pickup_longitude'] - data['dropoff_longitude']\n    data['lat_dist'] = data['pickup_latitude'] - data['dropoff_latitude']\n    \n    data['dist'] = dist(data['pickup_latitude'], data['pickup_longitude'],\n                        data['dropoff_latitude'], data['dropoff_longitude'])\n    \n    return data\n\n\ndf = transform(df)","50c66650":"import xgboost as xgb\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import mean_squared_error","b5cd54c9":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop('fare_amount', axis=1),\n                                                    df['fare_amount'], test_size=0.25)\n#del(df)\ndtrain = xgb.DMatrix(X_train, label=y_train)\n#del(X_train)\ndtest = xgb.DMatrix(X_test)\n#del(X_test)","555c3a9b":"# def xgb_evaluate(max_depth, gamma,min_child_weight,max_delta_step,subsample,colsample_bytree):\n#     params = {'eval_metric': 'rmse',\n#               'max_depth': int(max_depth),\n#               'subsample': subsample,\n#               'eta': 0.1,\n#               'gamma': gamma,\n#               'colsample_bytree': colsample_bytree,   \n#               'min_child_weight': min_child_weight ,\n#               'max_delta_step':max_delta_step\n#              }\n#     # Used around 1000 boosting rounds in the full model\n#     cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n    \n#     # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n#     return -1.0 * cv_result['test-rmse-mean'].iloc[-1]","a1eab39a":"# xgb_bo = BayesianOptimization(xgb_evaluate, {\n#                                     'max_depth': (2, 12),\n#                                      'gamma': (0.001, 10.0),\n#                                      'min_child_weight': (0, 20),\n#                                      'max_delta_step': (0, 10),\n#                                      'subsample': (0.4, 1.0),\n#                                      'colsample_bytree' :(0.4, 1.0)})\n# # Use the expected improvement acquisition function to handle negative numbers\n# # Optimally needs quite a few more initiation points and number of iterations\n# xgb_bo.maximize(init_points=3, n_iter=5, acq='ei')","8a414d5f":"# params = xgb_bo.res['max']['max_params']\n# print(params)\nparams = {'max_depth': 12.0, 'gamma': 0.001, 'min_child_weight': 8.740952582296343, 'max_delta_step': 10.0, 'subsample': 0.4, 'colsample_bytree': 1.0}\nparams['max_depth'] = int(params['max_depth'])","ff2fea9f":"# Train a new model with the best parameters from the search\nmodel2 = xgb.train(params, dtrain, num_boost_round=250)\n\n# Predict on testing and training set\ny_pred = model2.predict(dtest)\ny_train_pred = model2.predict(dtrain)\n\n# Report testing and training RMSE\nprint(np.sqrt(mean_squared_error(y_test, y_pred)))\nprint(np.sqrt(mean_squared_error(y_train, y_train_pred)))","02548974":"import matplotlib.pyplot as plt\nfscores = pd.DataFrame({'X': list(model2.get_fscore().keys()), 'Y': list(model2.get_fscore().values())})\nfscores.sort_values(by='Y').plot.bar(x='X')","34a93ed2":"test = pd.read_csv('..\/input\/test.csv').set_index('key')\ntest['pickup_datetime'] = test['pickup_datetime'].str.slice(0, 16)\ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n\n# Predict on holdout set\ntest = transform(test)\ndtest = xgb.DMatrix(test)\ny_pred_test = model2.predict(dtest)","259731e3":"holdout = pd.DataFrame({'key': test.index, 'fare_amount': y_pred_test})\nholdout.to_csv('submission.csv', index=False)","469cb27d":"See __[NYC Taxi Fare - Data Exploration](https:\/\/www.kaggle.com\/breemen\/nyc-taxi-fare-data-exploration)__ for an excellent EDA on this dataset and the intuition for including airports.","24f15ad3":"Extract the parameters of the best model.","bf26b8b3":"## Submit predictions","f4c091ca":"## Read Data\nUse all data for a better score. However i've not been able to properly configure the K80 gpu available on kaggle to work with xgboost, so I've had to severly limit both the amount of data, and size of model.\n\nThe data appears to be randomized, so reading in the beginning rows is acceptable.\n\nUsing the entire dataset will use around 32gb of memory throughout this notebook, So primarily for this reason I achieved first place on the leaderboard as of July 31,2018 using an AWS EC2 p3.2xlarge instance.","353085e0":"## Feature Engineering\nManhattan distance provides a better approximation of actual travelled distance than haversine for most trips.","0e9cb0ae":"## Train\/Test split","3d282d8f":"## Clean","0bc61fd3":"Slicing off unecessary components of the datetime and specifying the date format results in a MUCH more efficiecnt conversion to a datetime object.","b38c5a73":"## Feature Importance","8e5fae95":"Being careful about memory management, which is critical when running the entire dataset.","39990e2d":"## Predict on Holdout Set","0d4ebc9f":"## Training\nOptimizing hyperparameters with bayesian optimization. I've tried to limit the scope of the search as much\nas possible since the search space grows exponentially when considering aditional hyperparameters.\n\nGPU acceleration with a few pre tuned hyperparameters speeds up the search a lot.","303dc319":"# Bayesian Optimization with XGBoost","926701c7":"## Testing"}}