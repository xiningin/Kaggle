{"cell_type":{"0d9a606e":"code","f78df0c7":"code","4a3e2f51":"code","f4207cd7":"code","d9816853":"code","7e5519f3":"code","c9a24e29":"code","ab4dc53e":"markdown","27441c19":"markdown","e1ae9ddf":"markdown","390461ed":"markdown","a5a71142":"markdown","c3da8fb7":"markdown","a0b0490a":"markdown","49d00863":"markdown"},"source":{"0d9a606e":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","f78df0c7":"seed = 7\nnp.random.seed(seed)\n\n# load data\ndataset = np.loadtxt('https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/pima-indians-diabetes.data.csv', delimiter = ',')\n\nX = dataset[:, 0:8]\ny = dataset[:, 8]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y , test_size = np.int(X.shape[0]\/3), random_state = 123, stratify = y)\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(12, input_dim = 8, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(8, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# summary model\nmodel.summary()\n\n# input shape\nprint('X train shape: ', X_train.shape)\nprint('y train shape: ', y_train.shape)\n\nprint('X test shape: ', X_test.shape)\nprint('y test shape: ', y_test.shape)","4a3e2f51":"# compile model\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\n# create checkpoint\nfilepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, \n                             monitor = 'val_acc', \n                             verbose = 1, \n                             save_best_only = True, \n                             mode = 'max')\n\ncallbacks_list = [checkpoint]","f4207cd7":"# fit the model\nmodel.fit(X_train, y_train, \n          validation_split = 0.33, \n          epochs = 150, \n          batch_size = 10,\n          callbacks = callbacks_list,\n          verbose = 0)","d9816853":"# compile model\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\n# create checkpoint\nfilepath=\"weights-best-file.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, \n                             monitor = 'val_acc', \n                             verbose = 1, \n                             save_best_only = True, \n                             mode = 'max')\n\ncallbacks_list = [checkpoint]\n\n# fit the model\nmodel.fit(X_train, y_train, \n          validation_split = 0.33, \n          epochs = 150, \n          batch_size = 10,\n          callbacks = callbacks_list,\n          verbose = 0)","7e5519f3":"# create model\nmodel1 = Sequential()\nmodel1.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\nmodel1.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel1.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n# load weights\nmodel1.load_weights(\"weights-best-file.hdf5\", by_name = True)\n# Compile model (required to make predictions)\nmodel1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(\"Created model and loaded weights from file\")\n\n# estimate accuracy on whole dataset using loaded weights\nscores = model1.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (model1.metrics_names[1], scores[1]*100))","c9a24e29":"# create model\nmodel2 = Sequential()\nmodel2.add(Dense(24, input_dim=8, kernel_initializer='uniform', activation='relu'))\nmodel2.add(Dense(12, kernel_initializer='uniform', activation='relu'))\nmodel2.add(Dense(6, kernel_initializer='uniform', activation='relu'))\nmodel2.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n# load weights\nmodel2.load_weights(\"weights-best-file.hdf5\", by_name = True)\n# Compile model (required to make predictions)\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(\"Created model and loaded weights from file\")\n\n# estimate accuracy on whole dataset using loaded weights\nscores = model2.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (model2.metrics_names[1], scores[1]*100))","ab4dc53e":"\n## 3.2. Load tr\u1ea1ng th\u00e1i c\u1ee7a checkpoint\n\nNh\u01b0 v\u1eady ch\u00fang ta \u0111\u00e3 bi\u1ebft c\u00e1ch l\u00e0m th\u1ebf n\u00e0o \u0111\u1ec3 l\u01b0u tr\u1ea1ng th\u00e1i c\u1ee7a checkpoint trong h\u01b0\u1edbng d\u1eabn tr\u00ean. Sau khi \u0111\u00e3 l\u01b0u checkpoint ch\u00fang ta c\u0169ng c\u1ea7n bi\u1ebft c\u00e1ch l\u00e0m th\u1ec3 n\u00e0o \u0111\u1ec3 load c\u00e1c h\u1ec7 s\u1ed1 c\u1ee7a model t\u1eeb c\u00e1c checkpoint \u0111\u1ec3 t\u00e1i s\u1eed d\u1ee5ng cho nh\u1eefng l\u1ea7n training sau m\u00e0 kh\u00f4ng c\u1ea7n ph\u1ea3i th\u1ef1c hi\u1ec7n qu\u00e1 tr\u00ecnh training l\u1ea1i t\u1eeb \u0111\u1ea7u. V\u1eeba ti\u1ebft ki\u1ec7m \u0111\u01b0\u1ee3c chi ph\u00ed th\u1eddi gian, v\u1eeba d\u1ec5 k\u1ebf th\u1eeba ki\u1ebfn th\u00fac model cho nh\u1eefng t\u00e1c v\u1ee5 training kh\u00e1c.\n\nV\u1ec1 b\u1ea3n ch\u1ea5t `checkpoint` s\u1ebd ch\u1ee9a c\u00e1c h\u1ec7 s\u1ed1 t\u01b0\u01a1ng \u1ee9ng v\u1edbi m\u1ed7i layer. Checkpoint c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c \u0111\u1ed3ng b\u1ed9 h\u00f3a d\u01b0\u1edbi d\u1ea1ng c\u00e1c file JSON ho\u1eb7c YAML.\n\nTrong file v\u00ed d\u1ee5 b\u00ean d\u01b0\u1edbi ta s\u1ebd s\u1eed d\u1ee5ng checkpoint \u0111\u1ec3 load model v\u00e0 \u0111\u00e1nh gi\u00e1 model tr\u00ean t\u1eadp test th\u00f4ng qua h\u00e0m `load_weights` c\u1ee7a keras. Ngo\u00e0i ra h\u00e0m n\u00e0y c\u0169ng \u0111\u01b0\u1ee3c d\u00f9ng \u0111\u1ec3 load c\u00e1c pretrain model \u1edf m\u1ed9t s\u1ed1 model zoo trong keras.\n","27441c19":"H\u00e0m m\u1ea5t m\u00e1t \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong m\u00f4 h\u00ecnh l\u00e0 h\u00e0m entropy \u0111\u1ed1i v\u1edbi 2 l\u1edbp. H\u00e0m entropy s\u1ebd t\u00ecm c\u00e1ch gi\u1ea3m thi\u1ec3u s\u1ef1 kh\u00e1c bi\u1ec7t l\u1edbn nh\u1ea5t v\u1ec1 kho\u1ea3ng c\u00e1ch gi\u1eefa 2 c\u1ee7a x\u00e1c xu\u1ea5t \u0111\u01b0\u1ee3c d\u1ef1 b\u00e1o v\u00e0 nh\u00e3n th\u1ef1c t\u1ebf. Thu\u1eadt to\u00e1n optimizer \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u00e0 'adam' v\u00e0 metric \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh sau m\u1ed7i l\u01b0\u1ee3t hu\u1ea5n luy\u1ec7n \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u00e0 'accuracy'. Sau khi \u0111\u00e3 compile model, ch\u00fang ta th\u1ef1c hi\u1ec7n qu\u00e1 tr\u00ecnh training.","e1ae9ddf":"# 4. T\u00e0i li\u1ec7u tham kh\u1ea3o.\n\n1. [Checkpoint with keras - machinelearningmastery](https:\/\/machinelearningmastery.com\/check-point-deep-learning-models-keras\/)\n2. [Checkpoint - keras official homepage](https:\/\/keras.io\/callbacks\/#ModelCheckpoint)\n3. [fine tuning - keras - learnopencv](https:\/\/www.learnopencv.com\/keras-tutorial-fine-tuning-using-pre-trained-models\/)\n4. [fine tuning - keras - blog marubon](http:\/\/marubon-ds.blogspot.com\/2018\/03\/some-fine-tuning-models-with-keras.html)","390461ed":"Nh\u01b0 v\u1eady ch\u00fang ta c\u00f3 th\u1ec3 th\u1ea5y trong metrics \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u00e0m ti\u00eau chu\u1ea9n \u0111o l\u01b0\u1eddng trong model l\u00e0 `validation accuracy` . Ch\u1ec9 khi validation accuracy \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n th\u00ec model m\u1edbi \u0111\u01b0\u1ee3c l\u01b0u v\u00e0o checkpoint. Do ch\u00fang ta \u0111\u00e3 c\u1ea5u h\u00ecnh checkpoint = `weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5` n\u00ean checkpoint s\u1ebd \u0111\u01b0\u1ee3c l\u01b0u v\u1edbi t\u00ean file th\u1ec3 hi\u1ec7n epoch v\u00e0 gi\u00e1 tr\u1ecb validation accuracy.  Vi\u1ec7c l\u01b0u tr\u1eef n\u00e0y gi\u00fap cho ch\u00fang ta bi\u1ebft m\u1ee9c \u0111\u1ed9 c\u1ea3i thi\u1ec7n accuracy qua m\u1ed7i epoch v\u00e0 \u0111\u1ed3ng th\u1eddi c\u00f3 nhi\u1ec1u v\u1ecb tr\u00ed c\u00f3 th\u1ec3 backup model h\u01a1n v\u00e0 d\u0129 nhi\u00ean file cu\u1ed1i c\u00f9ng s\u1ebd c\u00f3 `validation accuracy` l\u00e0 l\u1edbn nh\u1ea5t.\n\nDo m\u1ed7i l\u1ea7n ch\u1ea1y model s\u1ebd chia t\u1eadp validation kh\u00e1c nhau n\u00ean t\u00ean c\u00e1c gi\u00e1 tr\u1ecb validation accuracy s\u1ebd kh\u00e1c nhau, \u0111i\u1ec1u n\u00e0y d\u1eabn \u0111\u1ebfn c\u00e1c file checkpoint s\u1ebd kh\u00e1c nhau sau m\u1ed7i l\u1ea7n ch\u1ea1y. Nh\u01b0 v\u1eady \u0111\u1ec3 l\u01b0u tr\u1ea1ng th\u00e1i model v\u00e0o m\u1ed9t file checkpoint duy nh\u1ea5t c\u00f3 h\u1ec7 s\u1ed1 gi\u00fap c\u1ea3i thi\u1ec7n validation accuracy l\u00e0 t\u1ed1t nh\u1ea5t ta s\u1ebd \u0111\u1eb7t t\u00ean duy nh\u1ea5t cho filepath l\u00e0 `weights-best-file.hdf5`","a5a71142":"# 1. Gi\u1edbi thi\u1ec7u v\u1ec1 checkpoint\n\nTraining c\u00e1c model deep learning l\u00e0 m\u1ed9t qu\u00e1 tr\u00ecnh d\u00e0i. Ph\u1ee5 thu\u1ed9c v\u00e0o \u0111\u1ed9 l\u1edbn c\u1ee7a d\u1eef li\u1ec7u v\u00e0 s\u1ed1 l\u01b0\u1ee3ng tham s\u1ed1, ph\u01b0\u01a1ng th\u1ee9c optimization m\u00e0 c\u00f3 th\u1ec3 k\u00e9o d\u00e0i t\u1eeb m\u1ed9t v\u00e0i ti\u1ebfng \u0111\u1ebfn m\u1ed9t v\u00e0i ng\u00e0y. Trong qu\u00e1 tr\u00ecnh \u0111\u00f3 c\u00f3 th\u1ec3 ph\u00e1t sinh l\u1ed7i b\u1ea5t c\u1ee9 khi n\u00e0o. L\u1ed7i c\u00f3 th\u1ec3 xu\u1ea5t ph\u00e1t t\u1eeb nhi\u1ec1u ph\u00eda ch\u1eb3ng h\u1ea1n nh\u01b0: \n\n1. S\u1ef1 c\u1ed1 v\u1eadt l\u00fd: m\u1ea5t \u0111i\u1ec7n, out of memory, ... \n\n2. S\u1ef1 c\u1ed1 l\u1eadp tr\u00ecnh: sai \u0111\u1ecbnh d\u1ea1ng d\u1eef li\u1ec7u, k\u00edch th\u01b0\u1edbc c\u00e1c chi\u1ec1u d\u1eef li\u1ec7u, ki\u1ebfn tr\u00fac thi\u1ebft k\u1ebf model,....\n\n3. L\u1ed7i v\u1eadn h\u00e0nh model: tri\u1ec7t ti\u00eau v\u00e0 b\u00f9ng n\u1ed5 gradient, c\u00e1c s\u1ed1 b\u1ecb tr\u00e0n lu\u1ed3ng v\u00e0 th\u1ea5p lu\u1ed3ng (overflow and underflow),...\n\nV\u00ec chi ph\u00ed c\u1ee7a qu\u00e1 vi\u1ec7c t\u00ednh to\u00e1n l\u00e0 r\u1ea5t l\u1edbn n\u00ean vi\u1ec7c training l\u1ea1i t\u1eeb \u0111\u1ea7u c\u00e1c m\u00f4 h\u00ecnh v\u00ec c\u00e1c s\u1ef1 c\u1ed1 v\u1eadt l\u00fd l\u00e0 \u0111i\u1ec1u kh\u00f4ng mong mu\u1ed1n. Ch\u00ednh v\u00ec v\u1eady qu\u00e1 tr\u00ecnh training n\u00ean \u0111i k\u00e8m c\u00f9ng backup model th\u00f4ng qua c\u00e1c checkpoints \u0111\u1ec1 l\u01b0u tr\u1eef tr\u1ea1ng th\u00e1i v\u1ec1 c\u00e1c h\u1ec7 s\u1ed1, ki\u1ebfn tr\u00fac m\u1ea1ng, m\u1ee9c \u0111\u1ed9 ch\u00ednh x\u00e1c,....  c\u1ee7a model qua t\u1eebng l\u01b0\u1ee3t hu\u1ea5n luy\u1ec7n. Nh\u01b0 v\u1eady khi training l\u1ea1i model s\u1ebd kh\u00f4ng c\u1ea7n ph\u1ea3i th\u1ef1c hi\u1ec7n l\u1ea1i t\u1eeb \u0111\u1ea7u m\u00e0 qu\u00e1 tr\u00ecnh training c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c ti\u1ebfp t\u1ee5c t\u1ea1i checkpoint \u0111\u00e3 d\u1eebng \u1edf l\u1ea7n training tr\u01b0\u1edbc.\n\n# 2. Checkpoint trong m\u1ea1ng neural network\n\nCh\u00ednh v\u00ec vai tr\u00f2 quan tr\u1ecdng c\u1ee7a l\u01b0u d\u1eef tr\u1ea1ng th\u00e1i model n\u00ean h\u1ea7u h\u1ebft c\u00e1c th\u01b0 vi\u1ec7c v\u00e0 framework deep learning \u0111\u1ec1u h\u1ed7 tr\u1ee3 backup model th\u00f4ng qua checkpoint. B\u00e0i vi\u1ebft n\u00e0y s\u1ebd h\u01b0\u1edbng d\u1eabn m\u1ed9t s\u1ed1 checkpoint \u0111\u01a1n gi\u1ea3n nh\u1ea5t trong th\u01b0 vi\u1ec7n keras b\u1edfi keras kh\u00e1 d\u1ec5 d\u00f9ng, d\u1ec5 hi\u1ec3u v\u00e0 ph\u1ed5 bi\u1ebfn trong c\u1ed9ng \u0111\u1ed3ng.\n\nTrong keras, m\u1ed9t checkpoint c\u00f3 th\u1ec3 l\u01b0u tr\u1eef c\u00e1c h\u1ec7 s\u1ed1 th\u00f4ng qua [callback API](https:\/\/keras.io\/callbacks\/#modelcheckpoint).\n\nC\u00f3 m\u1ed9t v\u00e0i d\u1ea1ng callback kh\u00e1c nhau \u0111\u01b0\u1ee3c h\u1ed7 tr\u1ee3 trong keras nh\u01b0:\n\n[BaseLogger](https:\/\/keras.io\/callbacks\/#BaseLogger): BaseLogger s\u1ebd l\u01b0u tr\u1eef trung b\u00ecnh c\u1ee7a c\u00e1c metrics sau m\u1ed7i m\u1ed9t epoch ho\u1eb7c l\u1ea5y tr\u1ea1ng th\u00e1i cu\u1ed1i c\u00f9ng c\u1ee7a metrics t\u1ea1i epoch.\n\n[TerminateOnNaN](https:\/\/keras.io\/callbacks\/#TerminateOnNaN): Th\u00f4ng th\u01b0\u1eddng gi\u00e1 tr\u1ecb NaN c\u1ee7a h\u00e0m loss \u0111\u01b0\u1ee3c g\u00e2y ra b\u1edfi l\u1ed7i t\u00ednh to\u00e1n. TerminateOnNaN s\u1ebd d\u1eebng training n\u1ebfu gi\u00e1 tr\u1ecb c\u1ee7a h\u00e0m loss l\u00e0 NaN. \n\n[ProgbarLogger](https:\/\/keras.io\/callbacks\/#ProgbarLogger): In gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c metrics ra m\u00e0n h\u00ecnh.\n\n[History](https:\/\/keras.io\/callbacks\/#History): Ghi c\u00e1c gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c s\u1ef1 ki\u1ec7n v\u00e0o m\u1ed9t object l\u00e0 History. Callback history s\u1ebd \u0111\u01b0\u1ee3c t\u1ef1 \u0111\u1ed9ng \u00e1p d\u1ee5ng trong m\u1ecdi model c\u1ee7a keras v\u00e0 object history \u0111\u01b0\u1ee3c tr\u1ea3 v\u1ec1 th\u00f4ng qua h\u00e0m fit.\n\n[Modelcheckpoint](https:\/\/keras.io\/callbacks\/#ModelCheckpoint): L\u01b0u tr\u1eef m\u00f4 h\u00ecnh qua m\u1ed7i epochs. Tr\u1ea1ng th\u00e1i c\u1ee7a model \u0111\u01b0\u1ee3c l\u01b0u th\u00f4ng qua m\u1ed9t filepath. Th\u00f4ng th\u01b0\u1eddng filepath s\u1ebd \u0111\u01b0\u1ee3c \u0111\u1eb7t t\u00ean theo th\u1ee9 t\u1ef1 c\u1ee7a epoch v\u00e0 gi\u00e1 tr\u1ecb c\u1ee7a metrics.\n\n[EarlyStopping](https:\/\/keras.io\/callbacks\/#EarlyStopping): D\u1eebng model khi c\u00e1c ph\u1ea9m ch\u1ea5t c\u1ee7a model kh\u00f4ng c\u1ea3i thi\u1ec7n.\n\n[RemoteMonitor](https:\/\/keras.io\/callbacks\/#RemoteMonitor): Callback th\u00f4ng qua m\u1ed9t stream event t\u1edbi server.\n\nNgo\u00e0i c\u00e1c ki\u1ec3u callback tr\u00ean c\u00f2n m\u1ed9t s\u1ed1 c\u00e1c callback kh\u00e1c nh\u01b0: LearningRateSchedule, TensorBoard, ReduceLROnPlateau,...\n\nTrong tr\u01b0\u1eddng h\u1ee3p ch\u00fang ta ch\u1ec9 quan t\u00e2m \u0111\u1ebfn tr\u1ea1ng th\u00e1i c\u1ee7a model sau m\u1ed7i v\u00f2ng l\u1eb7p m\u00e0 kh\u00f4ng quan t\u00e2m \u0111\u1ebfn chi\u1ebfn l\u01b0\u1ee3c h\u1ecdc, c\u00e1c l\u1ed7i NaN, c\u1ea5u h\u00ecnh remote server,.... th\u00ec s\u1eed d\u1ee5ng ModelCheckpoint l\u00e0 gi\u1ea3i ph\u00e1p \u0111\u01a1n gi\u1ea3n v\u00e0 th\u00f4ng d\u1ee5ng nh\u1ea5t.\n\nC\u1ea5u h\u00ecnh c\u1ee7a ModelCheckpoint nh\u01b0 sau:\n\n`keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', \n                                 verbose=0, save_best_only=False, \n                                 save_weights_only=False, mode='auto', \n                                 period=1)`\n                                 \ntrong \u0111\u00f3 `filepath` l\u00e0 \u0111\u01b0\u1eddng link file l\u01b0u tr\u1eef checkpoint. Th\u00f4ng th\u01b0\u1eddng khi \u0111\u1eb7t t\u00ean cho m\u1ed9t checkpoint ta s\u1ebd \u0111\u1ec3 t\u00ean c\u00f3 \u0111\u1ecbnh d\u1ea1ng gi\u00fap nh\u1eadn bi\u1ebft th\u1ee9 t\u1ef1 c\u1ee7a epoch v\u00e0 gi\u00e1 tr\u1ecb c\u1ee7a metric, ch\u1eb3ng h\u1ea1n `weights.{epoch:02d}-{val_loss:.2f}.hdf5`; `monitor` l\u00e0 m\u1ed9t list ch\u1ee9a c\u00e1c metrics c\u1ea7n l\u01b0u tr\u1eef; `save_best_only` s\u1ebd l\u01b0u model t\u1ea1i epochs m\u00e0 gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c metrics \u0111o l\u01b0\u1eddng l\u00e0 t\u1ed1t nh\u1ea5t. Trong tr\u01b0\u1eddng h\u1ee3p t\u1ea1i epoch m\u00e0 metrics kh\u00f4ng \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n th\u00ec model s\u1ebd kh\u00f4ng \u0111\u01b0\u1ee3c l\u01b0u tr\u1eef; `save_weights_only` s\u1ebd qui \u0111\u1ecbnh l\u01b0u ch\u1ec9 h\u1ec7 s\u1ed1 c\u1ee7a model ho\u1eb7c l\u01b0u to\u00e0n b\u1ed9 model bao g\u1ed3m c\u00e1c tham s\u1ed1 kh\u00e1c ngo\u00e0i h\u1ec7 s\u1ed1 c\u1ee7a model nh\u01b0 learning_rate, batch_size,....\n\u0110\u1ecbnh d\u1ea1ng l\u01b0u tr\u1eef model th\u01b0\u1eddng l\u00e0 `hdf5`. \u0110\u1ec3 l\u01b0u tr\u1eef \u0111\u01b0\u1ee3c \u0111\u1ecbnh d\u1ea1ng `hdf5` ch\u00fang ta c\u1ea7n c\u00e0i \u0111\u1eb7t package `hdf5`.\n\nC\u00e1c checkpoint c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c truy\u1ec1n tr\u1ef1c ti\u1ebfp v\u00e0o model ngay sau khi g\u1ecdi h\u00e0m `fit()` \u0111\u1ec3 th\u1ef1c hi\u1ec7n training t\u1ea1i tr\u1ea1ng th\u00e1i c\u1ee7a checkpoints.\n\n# 3. Th\u1ef1c h\u00e0nh checkpoint model\n\nB\u00ean d\u01b0\u1edbi ta s\u1ebd ti\u1ebfn h\u00e0nh x\u00e2y d\u1ef1ng model v\u00e0 l\u01b0u v\u00e0o c\u00e1c checkpoint, model m\u00e0 ch\u00fang ta x\u00e2y d\u1ef1ng s\u1ebd d\u1ef1a tr\u00ean b\u1ed9 d\u1eef li\u1ec7u ph\u00e2n lo\u1ea1i b\u1ec7nh ti\u1ec3u \u0111\u01b0\u1eddng [pima-indians-diabetes](https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/pima-indians-diabetes.data.csv) d\u1ef1a tr\u00ean c\u00e1c ch\u1ec9 s\u1ed1 li\u00ean quan \u0111\u1ebfn ng\u01b0\u1eddi b\u1ec7nh.  \u0110\u00e2y l\u00e0 m\u1ed9t t\u1eadp d\u1eef li\u1ec7u nh\u1ecf n\u00ean kh\u00f4ng t\u1ed1n nhi\u1ec1u th\u1eddi gian training, do \u0111\u00f3 \u0111\u01b0\u1ee3c m\u00ecnh l\u1ef1a ch\u1ecdn \u0111\u1ec3 l\u00e0m h\u01b0\u1edbng d\u1eabn cho nhanh. 33% m\u1eabu s\u1ebd \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng cho validation model v\u00e0 66% s\u1ebd \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 training. Model s\u1ebd \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng d\u1ef1a tr\u00ean vi\u1ec7c c\u1ea3i thi\u1ec7n accuracy c\u1ee7a t\u1eadp validation th\u00f4ng qua khai b\u00e1o tham s\u1ed1 `monitor = val_loss` v\u00e0 `mode = max`. T\u00ean file \u0111\u01b0\u1ee3c l\u01b0u tr\u1eef checkpoint s\u1ebd bao g\u1ed3m \u0111i\u1ec3m accuracy c\u1ee7a model th\u00f4ng qua c\u00e1ch c\u1ea5u h\u00ecnh t\u00ean `weights-improvement-{val_acc=.2f}.hdf5`.\n\n## 3.1. X\u00e2y d\u1ef1ng model\n","c3da8fb7":"Nh\u01b0 v\u1eady qua b\u00e0i h\u01b0\u1edbng d\u1eabn n\u00e0y ch\u00fang ta \u0111\u00e3 n\u1eafm \u0111\u01b0\u1ee3c:\n\n1. \u00dd ngh\u0129a c\u1ee7a checkpoint.\n2. C\u00e1c lo\u1ea1i checkpoint th\u00f4ng d\u1ee5ng trong keras.\n3. C\u00e1ch th\u1ee9c kh\u1edfi t\u1ea1o m\u1ed9t checkpoint.\n4. C\u00e1ch th\u1ee9c load c\u00e1c checkpoint v\u00e0o m\u1ed9t c\u1ea5u tr\u00fac model v\u00e0 s\u1eed d\u1ee5ng model \u0111\u1ec3 d\u1ef1 b\u00e1o.","a0b0490a":"Ch\u00fang ta \u0111\u1ec3 \u00fd th\u1ea5y r\u1eb1ng model1 v\u00e0 model c\u00f3 c\u00f9ng c\u1ea5u tr\u00fac. Ch\u00fang ta c\u0169ng ho\u00e0n to\u00e0n c\u00f3 th\u1ec3 t\u1ea3i c\u00e1c tr\u1ecdng s\u1ed1 c\u1ee7a c\u00e1c model v\u00e0o c\u00e1c model c\u00f3 c\u1ea5u tr\u00fac kh\u00e1c nhau th\u00f4ng qua vi\u1ec7c khai b\u00e1o `by_name = True` trong h\u00e0m `load_weights`.  Khi \u0111\u00f3 h\u1ec7 s\u1ed1 s\u1ebd \u0111\u01b0\u1ee3c t\u1ea3i v\u00e0o nh\u1eefng layer n\u1ebfu ch\u00fang c\u00f3 c\u00f9ng t\u00ean. Vi\u1ec7c n\u00e0y nh\u1eb1m h\u1ed7 tr\u1ee3 c\u00e1c qu\u00e1 tr\u00ecnh `fine-tuning` v\u00e0 `transfer- learning` gi\u1eefa nh\u1eefng model c\u00f3 c\u1ea5u tr\u00fac kh\u00e1c bi\u1ec7t.","49d00863":"M\u00f4 h\u00ecnh c\u00f3 ki\u1ebfn tr\u00fac g\u1ed3m 3 layer n\u1ed1i ti\u1ebfp, fully connected v\u1edbi s\u1ed1 units l\u1ea7n l\u01b0\u1ee3t l\u00e0 12, 8, 1. T\u1ea1i c\u00e1c t\u1ea7ng chuy\u1ec3n giao gi\u1eefa c\u00e1c layer \u0111\u1ec1u \u00e1p d\u1ee5ng m\u1ed9t h\u00e0m activation bi\u1ebfn \u0111\u1ed5i phi tuy\u1ebfn. Ch\u1eb3ng h\u1ea1n gi\u1eefa layer 1 v\u00e0 2, 2 v\u00e0 3 \u0111\u1ec1u l\u00e0 relu, gi\u1eefa layer 3 v\u00e0 output l\u00e0 h\u00e0m sigmoid \u0111o x\u00e1c xu\u1ea5t \u0111\u1ea7u ra t\u01b0\u01a1ng \u1ee9ng v\u1edbi m\u1ed7i class c\u1ee7a m\u1ed9t quan s\u00e1t."}}