{"cell_type":{"a1608027":"code","a04b530e":"code","a9bbc755":"code","984f667b":"code","138c88b2":"code","1b6b3712":"code","c40e7ce3":"code","b1510aa9":"code","e561c612":"code","f57d5360":"code","a0a4d060":"code","130be9eb":"code","750c610b":"code","4a375fe8":"markdown","a4f0483d":"markdown","af80ebed":"markdown","a31a906d":"markdown","96248b6b":"markdown","6426f7fb":"markdown"},"source":{"a1608027":"import pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom IPython import display\n\nimport os,sys,random,cv2\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms as T\nfrom torch.optim.optimizer import Optimizer \n\nfrom skimage.restoration import denoise_wavelet \n\n\nsys.path.append('..\/input\/evabkk\/')\n\nfrom eva_model import *\nfrom data      import *\n\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = True  # type: ignore\n    torch.backends.cudnn.benchmark = True  # type: ignore\n  \n\n    \nclass cfg  :\n    batch_size     = 175\n    test_batch_size= 96\n    run_batch_size   = 175\n    \n    num_epochs          = 7 \n    stop_num_epochs     = 2 \n    num_workers         = 2\n    \n    Num_StockSample = 48\n    Windows         = 120\n    W_factor        = 0.16\n    W_step          = 2\n   \n    img_size       = 224\n    cwt_window     =  64\n    \n    delta_a         = 0.2\n    delta_b         = 0.2\n    \n    \n    warmup_epochs= 2\n    warmup_lr    = 0\n    base_lr      = 0.0075\n    final_lr     = 1e-7\n    \n    knn_monitor= True # knn monitor will take more time\n    knn_interval= 1\n    knn_k= 500\n\n    csvpath        = '..\/input\/evabkk\/BK.csv'\n    randcsv_path   = \"..\/working\/randomstock.csv\"\n    model_path     = \"..\/working\/model_weigth.pth\"\n    \n    seed           = 15 \n\nset_seed(cfg.seed)        \n","a04b530e":"%%time\n#load All Stocks from pandas \nData_Dict= Create_StockDictionary(cfg)","a9bbc755":"def plot_Sample_StockPandas(df_random) :\n    COLS = 4\n    ROWS =  int(np.ceil(cfg.Num_StockSample\/COLS)) \n    index = 0\n\n    for k in range(ROWS):\n            plt.figure(figsize=(20,5))\n            for j in range(COLS):\n                 plt.subplot(1,COLS,j+1)\n                 if index <  len(df_random) :  \n                     stock_name = df_random.loc[index,'stock']\n                     i          = df_random.loc[index,'i']\n                     signal      = Data_Dict[stock_name]['data'][ range(i,i+cfg.Windows) ]\n                     x           = Data_Dict[stock_name]['Dates'][ range(i,i+cfg.Windows) ]\n                     plt.plot(signal)\n                     plt.title('('+str(index)+') '+stock_name+' : '+x[0]+' to '+x[-1])  \n                     index+=1   \n            plt.show()   \n\n\ndef Plot_scheduler(Datalength = 100)  : \n    model = nn.Sequential(\n              nn.Linear(10,10)\n            )\n\n    optimizer           = torch.optim.Adam(model.parameters(), lr=0.00001)\n    scheduler           = LR_Scheduler(\n            optimizer,\n            cfg.warmup_epochs, \n            cfg.warmup_lr, \n            cfg.num_epochs, \n            cfg.base_lr, \n            cfg.final_lr, \n            Datalength\n        )\n\n    lrs = []\n    plt.figure(figsize=(8,5))\n    \n    for epoch in range(cfg.num_epochs):\n        for it in range(Datalength):\n            lr = scheduler.step()\n        lrs.append(lr)\n    plt.plot(lrs)\n    plt.title('Scheduler lr')\n    plt.xlabel('Epoch')\n    plt.show()\n\n\ndf_random        = RandomSample_2_Pandas(Data_Dict,cfg) \nRandom_DataDict  = RandomSample_Pandas2_Dict(df_random,Data_Dict, cfg)\n\ndf_random.to_csv(cfg.randcsv_path,index=False )\n\nplot_Sample_StockPandas(df_random)\nPlot_scheduler()\n\n\ndf_random","984f667b":"%%time\n\nX_train ,y_train,X_test ,y_test,X_memory,y_memory = Train_test_Split(Random_DataDict,test_size = 0.32)\n\nprint(f'X_train = {X_train.shape} X_test = { X_test.shape} y_train = {y_train.shape} y_test = {y_test.shape} , y_mem = {y_memory.shape} , X_memory  = {X_memory.shape }  ')","138c88b2":"%%time\n# knn monitor as in InstDisc https:\/\/arxiv.org\/abs\/1805.01978\n# implementation follows http:\/\/github.com\/zhirongw\/lemniscate.pytorch and https:\/\/github.com\/leftthomas\/SimCLR\ndef knn_predict(feature, feature_bank, feature_labels, classes, knn_k, knn_t):\n    # compute cos similarity between each feature vector and feature bank ---> [B, N]\n    sim_matrix = torch.mm(feature, feature_bank)\n    # [B, K]\n    B, K       =  sim_matrix.shape   \n    knn_k      =  min(knn_k,K)\n    \n    sim_weight, sim_indices = sim_matrix.topk(k= knn_k, dim=-1)\n    # [B, K]\n    sim_labels = torch.gather(feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices)\n    sim_weight = (sim_weight \/ knn_t).exp()\n\n    sim_labels = sim_labels.type(torch.int64)\n    \n    # counts for each class\n    one_hot_label = torch.zeros(feature.size(0) * knn_k, classes, device=sim_labels.device)\n    # [B*K, C]\n    one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n    # weighted score ---> [B, C]\n    pred_scores = torch.sum(one_hot_label.view(feature.size(0), -1, classes) * sim_weight.unsqueeze(dim=-1), dim=1)\n\n    pred_labels = pred_scores.argsort(dim=-1, descending=True)\n    return pred_labels\n\n\ndef knn_monitor(net, memory_data_loader, test_data_loader, epoch, k=200, t=0.1, classes = 10):\n    net.eval()\n    total_top1, total_top5, total_num, feature_bank,feature_labels = 0.0, 0.0, 0, [],[]\n    with torch.no_grad():\n        # generate feature bank\n        for sample_batched in tqdm(memory_data_loader, desc='Feature extracting', leave=False, disable= False):\n             data  = sample_batched['image'].to(device)\n             label = sample_batched['label'].to(device)\n             feature = net(data)\n             feature = F.normalize(feature, dim=1)\n             feature_bank.append(feature)\n             feature_labels.append(label)            \n         # [D, N]        \n        feature_bank   = torch.cat(feature_bank, dim=0).t().contiguous()  \n        # [N]   \n        feature_labels = torch.cat(feature_labels,dim=0) \n\n        test_bar = tqdm(test_data_loader , desc='kNN', disable= False)\n\n        for sample_batched in test_bar:\n             data,target  = sample_batched['image'].to(device),sample_batched['label'].to(device)\n             feature      = net(data)\n             feature      = F.normalize(feature, dim=1)\n             \n             pred_labels = knn_predict(feature, feature_bank, feature_labels, classes, k, t)\n                \n             total_num += data.size(0)\n             total_top1 += (pred_labels[:, 0] == target).float().sum().item()  \n             test_bar.set_postfix({'Accuracy':total_top1 \/ total_num * 100})  \n\n        return (total_top1 \/ total_num) * 100\n\n\n\n\ndef train(model,optimizer ,X_train ,y_train,X_test ,y_test,X_memory,y_memory):\n   \n\n    device              = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    training_dataset    = Random_Dataset(X_train ,y_train,Random_DataDict,Data_Dict, cfg , mode = 'train')\n    train_loader        = DataLoader(training_dataset, batch_size=cfg.batch_size, shuffle=True,num_workers = cfg.num_workers)\n    \n    mem_dataset         = Random_Dataset(X_memory,y_memory,Random_DataDict,Data_Dict, cfg , mode = 'test')\n    memory_loader       = DataLoader(mem_dataset, batch_size=cfg.batch_size, shuffle=False,num_workers = cfg.num_workers)\n\n    \n    testing_dataset     = Random_Dataset(X_test ,y_test,Random_DataDict,Data_Dict, cfg , mode = 'test')\n    test_loader         = DataLoader(testing_dataset, batch_size=cfg.test_batch_size, shuffle=False,num_workers =  cfg.num_workers)\n\n    \n    device              = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    lr_scheduler        = LR_Scheduler(\n            optimizer,\n            cfg.warmup_epochs, \n            cfg.warmup_lr, \n            cfg.num_epochs, \n            cfg.base_lr, \n            cfg.final_lr, \n            len(train_loader),\n        )\n\n\n\n    global_progress = tqdm(range(0, cfg.stop_num_epochs), desc=f'Training')\n    best_accuracy = -1\n\n    for epoch in global_progress:\n        model.train()\n        local_progress = tqdm(train_loader, desc=f'Epoch {epoch}\/{ cfg.num_epochs }', disable= False)\n        for sample_batched in local_progress:\n              x1 = sample_batched['image1'].to(device, non_blocking=True)\n              x2 = sample_batched['image2'].to(device, non_blocking=True)\n              model.zero_grad()\n              data_dict = model.forward(x1, x2)\n              del x1 ,x2  \n\n              loss = data_dict['loss'].mean() # ddp\n              loss.backward()\n              optimizer.step()\n              lr_scheduler.step()\n              data_dict.update({'lr':lr_scheduler.get_lr()})\n              local_progress.set_postfix(data_dict)\n                   \n        \n        accuracy = knn_monitor(model.backbone, memory_loader, test_loader, device, k=cfg.knn_k, classes = cfg.Num_StockSample) \n\n        if accuracy > best_accuracy :\n           model_path = cfg.model_path\n           torch.save(model.state_dict(), model_path)\n           print(f\"Model accuracy {accuracy:.3f} saved to {model_path} \\n\")\n           best_accuracy = accuracy \n    \n       \n    return \n\n\ndevice              = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbackbone            = Net(use_pretrain = True)\nmodel               = SimSiam(backbone).to(device)\noptimizer           = LARS_simclr(model.named_modules(), weight_decay= 0, lr=1e-4)\ntrain(model,optimizer ,X_train ,y_train,X_test ,y_test,X_memory,y_memory)","1b6b3712":"import pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nfrom IPython import display\n\nimport os,sys,random,cv2\n\nfrom time import sleep\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms as T\nfrom torch.optim.optimizer import Optimizer \n\nfrom skimage.restoration import denoise_wavelet \n\n\nsys.path.append('..\/input\/evabkk\/')\n\nfrom eva_model import *\nfrom data      import *\n\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = True  # type: ignore\n    torch.backends.cudnn.benchmark = True  # type: ignore\n  \n    \nclass cfg  :\n    batch_size     = 145\n    test_batch_size= 96\n    run_batch_size   = 175\n    \n    num_epochs          = 7 \n    stop_num_epochs     = 1 \n    num_workers         = 2\n    \n    Num_StockSample = 40\n    Windows         = 120\n    W_factor        = 0.16\n    W_step          = 2\n   \n    img_size       = 224\n    cwt_window     =  64\n    \n    delta_a         = 0.2\n    delta_b         = 0.2\n    \n    \n    warmup_epochs= 2\n    warmup_lr    = 0\n    base_lr      = 0.0075\n    final_lr     = 1e-7\n    \n    knn_monitor= True # knn monitor will take more time\n    knn_interval= 1\n    knn_k= 500\n\n    csvpath        = '..\/input\/evabkk\/BK.csv'\n    randcsv_path   = \"..\/working\/randomstock.csv\"\n    model_path     = \"..\/working\/model_weigth.pth\"\n    \n    seed           = 19 \n\nset_seed(cfg.seed)        \n","c40e7ce3":"device              = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbackbone            = Net(use_pretrain = False)\nmodel               = SimSiam(backbone).to(device)\n\ntry :\n    Data_Dict \nexcept :    \n    Data_Dict= Create_StockDictionary(cfg)\n\nif os.path.exists(cfg.model_path) and os.path.exists(cfg.randcsv_path):\n    model.load_state_dict(torch.load( cfg.model_path    ))\n    df_random        = pd.read_csv(cfg.randcsv_path)\n    Random_DataDict  = RandomSample_Pandas2_Dict(df_random,Data_Dict, cfg)\n    print('load model sucess')\nelse:    \n       df_random        = RandomSample_2_Pandas(Data_Dict,cfg) \n       Random_DataDict  = RandomSample_Pandas2_Dict(df_random,Data_Dict, cfg) \n       print('using random weight model')\n\n\n","b1510aa9":"def Plot_Compare_Performance(Nlarge, df_best_weight,df_best_match, df_best_index) :\n    \n    display.clear_output(wait=True)\n    xNlarge = min(Nlarge,len(df_best_weight.columns))\n    \n    \n    for index, row in df_random.iterrows() :\n            fig = plt.figure(figsize=( (Nlarge+2)*3,3)) \n            ax  = fig.add_subplot(1, xNlarge+1, 1)\n\n            ref_stock = row['stock']\n            ref_i     = row['i']  \n            signal    = Data_Dict[ref_stock]['data'][ range(ref_i,ref_i+cfg.Windows) ]\n            x         = Data_Dict[ref_stock]['Dates'][ range(ref_i,ref_i+cfg.Windows) ]\n            ax.plot(signal)\n            ax.set_title('Target '+ref_stock+' : '+x[0]+' to '+x[-1], fontsize=8)\n\n            for j in range(xNlarge) :\n               match_stock_name =  df_best_match.loc[ref_stock,j]\n               match_stock_i    =  df_best_index.loc[ref_stock,j]\n               match_weight_i   =  df_best_weight.loc[ref_stock,j] \n               signal           = Data_Dict[match_stock_name]['data'][ range(match_stock_i,match_stock_i+cfg.Windows) ]\n               x                = Data_Dict[match_stock_name]['Dates'][ range(match_stock_i,match_stock_i+cfg.Windows) ]\n               ax  = fig.add_subplot(1, xNlarge+1, j+2)\n               ax.plot(signal)\n               ax.set_title(match_stock_name+' : '+x[0]+' to '+x[-1], fontsize=8)  \n               ax.text(0.97, 0.01, f'Similarity Score = {match_weight_i:.3f}', verticalalignment='bottom', horizontalalignment='right', transform=ax.transAxes,color='green')\n    plt.show()\n    \n\n\ndef find_best(xdf_weight, xdf_index , nbest = 2) :\n    \n    df_weight[df_weight > 0.995] = -1\n\n    df_best_weight              = df_weight.apply(lambda s,n : pd.Series(s.nlargest(n).tolist()) ,axis=1, n=nbest)\n    df_best_match               = df_weight.apply(lambda s, n: pd.Series(s.nlargest(n).index), axis=1, n=nbest)\n    df_best_index               = df_best_match.copy()\n    \n    for index, data in df_best_match.iterrows() :\n        l                          = data.tolist()\n        df_best_index.loc[index,:] = df_index.loc[index,l].tolist()    \n\n    return df_best_weight,df_best_match, df_best_index\n\n\n\ndef Features_Dataset(model, df_random) :\n\n    net = model.backbone.eval()\n\n    device              = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    index_List        = df_random['i'].values.astype(int).tolist()\n    stock_List        = df_random['label'].values.tolist()  \n\n    run_dataset  = Random_Dataset(index_List  , stock_List,Random_DataDict,Data_Dict, cfg , mode = 'test')\n    run_loader   = DataLoader(run_dataset, batch_size=cfg.run_batch_size, shuffle=False,num_workers= cfg.num_workers)\n\n    feature_bank,feature_labels  = [],[]\n\n\n    for sample_batched in tqdm(run_loader, disable= True, leave=True):\n          data = sample_batched['image'].to(device)\n          label =sample_batched['label']\n          feature      = net(data)\n          feature      = F.normalize(feature, dim=1)\n          feature_bank.append(feature)\n          feature_labels.append(label)\n\n    # [D, N]        \n    feature_bank   = torch.cat(feature_bank, dim=0)  \n    # [N]   \n    feature_labels = torch.cat(feature_labels,dim=0)     \n    return feature_bank , feature_labels\n\ndef Features_StockName(model,stock_name) :\n    \n    net = model.backbone.eval()\n    \n    stock_name_dataset = Stock_name_Dataset(stock_name ,Data_Dict,cfg)\n    stock_name_loader  = DataLoader(stock_name_dataset, batch_size=cfg.run_batch_size, shuffle=False,num_workers= cfg.num_workers)\n    \n    stock_feature = []\n    \n    for sample_batched in tqdm(stock_name_loader , desc='Stock name = '+ stock_name, leave=False):\n      data = sample_batched['image'].to(device)\n      feature      = net(data)\n      feature      = F.normalize(feature, dim=1)\n      stock_feature.append(feature)\n      sleep(0.01)  \n          \n    stock_feature   = torch.cat(stock_feature, dim=0)  \n    \n    return stock_feature\n\n\n","e561c612":"%%time\n\nNumber_StockSearch = 200\nNlarge             = 7\n\n\n\n\nAll_StockNames = list(Data_Dict.keys()) \nrandom.shuffle(All_StockNames)\n\nfeature_bank , feature_labels = Features_Dataset(model, df_random) \n\ndf_weight = pd.DataFrame(index = df_random.stock.tolist())\ndf_index  = pd.DataFrame(index = df_random.stock.tolist())\n\n\nfor iprogress,search_stock_name in enumerate(All_StockNames[0:Number_StockSearch]) :\n    \n    print(f\"Operate {iprogress+1} of Totals { len(All_StockNames[0:Number_StockSearch]) } \\n\")\n\n    stock_feature            = Features_StockName(model,stock_name= search_stock_name)\n\n    max_k                    = min(stock_feature.shape[0],30)\n    sim_matrix               = torch.mm(feature_bank, stock_feature.t().contiguous())\n    sim_weight, sim_indices  = sim_matrix.topk(k= max_k, dim=-1)\n\n    best_index               = sim_indices[:,0].detach().cpu().numpy().tolist()\n    best_weight              = sim_weight[:,0].detach().cpu().numpy().tolist()\n\n    df_weight[search_stock_name] , df_index[search_stock_name]                                 = best_weight , best_index\n    df_best_weight,df_best_match, df_best_index                                   = find_best(df_weight, df_index, nbest = Nlarge )  \n    \n    Plot_Compare_Performance(Nlarge, df_best_weight,df_best_match, df_best_index) \n    \n    \n               \n\n       \n       \n\n\n","f57d5360":"df_random","a0a4d060":"df_best_weight","130be9eb":"df_best_match","750c610b":"df_best_index","4a375fe8":"# Inference Part","a4f0483d":"# Train test Split","af80ebed":"# Load All Stock Dataset","a31a906d":"# Search for Similarity by each Stocks","96248b6b":"# Train Model","6426f7fb":"# Random 48 Targets of Adj Close price Sotck over 120 day period for finding Similarity matching over All Thailand Stocks from 1980-2021"}}