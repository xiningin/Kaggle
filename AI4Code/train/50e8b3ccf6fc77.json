{"cell_type":{"ffb29eac":"code","01d39153":"code","e11520ad":"code","722b13ba":"code","6c116f25":"code","02f1e23a":"code","ca3f0d43":"code","162ffcc5":"code","99af351a":"code","81697b5b":"code","30446a9f":"code","ee87cced":"code","185632f0":"code","d87e5fc2":"code","fd39429c":"code","27a05e7a":"code","41aea8fc":"code","39304a1f":"code","259b4dc0":"code","e7362a6b":"code","df3de7e5":"code","9bf4bb7a":"code","3e28766c":"code","3a3e3e9d":"code","d9171193":"code","eec286c1":"code","9ed3da9a":"code","4eaaeaf6":"code","2bd87b4b":"code","4b2af71d":"markdown","22cc5ac5":"markdown","2ecc0131":"markdown","abacd59c":"markdown","49470c6d":"markdown","5bbc0d5c":"markdown"},"source":{"ffb29eac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01d39153":"train_df=pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\nsubmission_df=pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\nprint('The train dataset contains {} rows and {} columns.'.format( len(train_df.index), len(train_df.columns)))\nprint('The test dataset contains {} rows and {} columns.'.format( len(test_df.index), len(test_df.columns)))","e11520ad":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n%matplotlib inline\nimport matplotlib\nsns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'","722b13ba":"input_cols =list(train_df.columns)[1:-1]\ntarget_cols='target'\ninputs_df = train_df[input_cols].copy()\ntargets = train_df[target_cols]\ninputs_df","6c116f25":"inputs_df.info()","02f1e23a":"from sklearn.preprocessing import OneHotEncoder\n# Extract all categorical cols \ncategorical_cols = inputs_df.select_dtypes('object').columns.tolist()\n#  Create the encoder\nenc=OneHotEncoder().fit(inputs_df[categorical_cols])\n# Extract all numeric cols\nnumeric_cols = inputs_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n# Generate column names for each category\nencoded_cols = list(enc.get_feature_names(categorical_cols))\nprint('encoded columns are:'+str(encoded_cols))\nprint()\nprint('categorical columns are:'+str(categorical_cols))\nprint()\nprint('These are the categories which we are going to encode and then add it to our data'+str(enc.categories_))","ca3f0d43":"# Transform and add new one-hot category columns\ninputs_df[encoded_cols]=enc.transform(inputs_df[categorical_cols]).toarray()\ntest_df[encoded_cols]=enc.transform(test_df[categorical_cols]).toarray()\ninputs_df=inputs_df[encoded_cols+numeric_cols]\ntest_df=test_df[encoded_cols+numeric_cols]","162ffcc5":"inputs_df","99af351a":"# Lets find some statistical measures\ninputs_df.describe().T","81697b5b":"from sklearn.model_selection import train_test_split\ntrain_inputs, val_inputs, train_targets, val_targets = train_test_split(inputs_df, \n                                                                        targets, \n                                                                        test_size=0.25, \n                                                                        random_state=42)","30446a9f":"print('The train dataset contains {} rows and {} columns.'.format( len(train_inputs.index), len(train_inputs.columns)))\nprint('The val dataset contains {} rows and {} columns.'.format( len(val_inputs.index), len(val_inputs.columns)))","ee87cced":"#We can use DecisionTreeRegressor from sklearn.tree to train a decision tree.\nfrom sklearn.tree import DecisionTreeRegressor\nmodel = DecisionTreeRegressor(random_state=42)  ","185632f0":"%%time\nmodel.fit(train_inputs, train_targets)","d87e5fc2":"# Let's evaluate the decision tree regressor using the RootMeanSquaredError.\n#Generate predictions on the training and validation sets using the trained decision tree, and compute the RMSE loss.\ntrain_preds = model.predict(train_inputs)\nval_preds=model.predict(val_inputs)\nfrom sklearn.metrics import mean_squared_error\ntrain_rmse=mean_squared_error(train_targets,train_preds,squared=False)\nval_rmse=mean_squared_error(val_targets,val_preds,squared=False)\nprint('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))","fd39429c":"print('train accuracy is {:.2f}%:'.format((1-train_rmse)*100))\nprint('val accuracy is :{:.2f}%'.format((1-val_rmse)*100))","27a05e7a":"# We can visualize the decision tree learned from the training data.\nfrom sklearn.tree import plot_tree, export_text\nplt.figure(figsize=(80,20))\nplot_tree(model, feature_names=train_inputs.columns, max_depth=2, filled=True);","41aea8fc":"# Visualize the tree textually using export_text\ntree_text = export_text(model ,max_depth=5,feature_names=list(train_inputs.columns))\n# Display the first few lines\nprint(tree_text[:2000])","39304a1f":"# Check max depth\nprint(model.tree_.max_depth)\n# Check feature importance\nprint(model.feature_importances_)","259b4dc0":"importance_df = pd.DataFrame({\n    'feature': train_inputs.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)","e7362a6b":"importance_df.head(10)","df3de7e5":"# Lets visualize 10 most important features\nplt.title('Feature Importance')\nsns.barplot(data=importance_df.head(10), x='importance', y='feature');","9bf4bb7a":"def test_params(**params):\n    model = DecisionTreeRegressor(random_state=42, **params).fit(train_inputs, train_targets)\n    train_rmse = mean_squared_error(model.predict(train_inputs), train_targets, squared=False)\n    val_rmse = mean_squared_error(model.predict(val_inputs), val_targets, squared=False)\n    return train_rmse, val_rmse","3e28766c":"def test_param_and_plot(param_name, param_values):\n    train_errors, val_errors = [], [] \n    for value in param_values:\n        params = {param_name: value}\n        train_rmse, val_rmse = test_params(**params)\n        train_errors.append(train_rmse)\n        val_errors.append(val_rmse)\n    plt.figure(figsize=(10,6))\n    plt.title('Overfitting curve: ' + param_name)\n    plt.plot(param_values, train_errors, 'b-o')\n    plt.plot(param_values, val_errors, 'r-o')\n    plt.xlabel(param_name)\n    plt.ylabel('RMSE')\n    plt.legend(['Training', 'Validation'])","3a3e3e9d":"%%time\ntest_param_and_plot('max_depth', [2,4,6,8])\n","d9171193":"print('Training error and Validation error for max_depth=6 are:'+str(test_params(max_depth=6)))\nprint('Training error and Validation error for min_samples_split=200 are:'+str(test_params(min_samples_split=200)))\nprint('Training error and Validation error for min_samples_leaf=50 are:'+str(test_params(min_samples_leaf=50)))","eec286c1":"%%time\ntest_params(max_depth=6,min_samples_leaf=50,min_samples_split=200)","9ed3da9a":"#Training the Best Model\ndt= DecisionTreeRegressor(random_state=42,max_depth=6,min_samples_leaf=50,min_samples_split=200).fit(train_inputs, train_targets)\n#Making Predictions on the Test Set\ntest_preds=dt.predict(test_df)\n","4eaaeaf6":"submission_df=pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\nsubmission_df","2bd87b4b":"submission_df['target'] = test_preds\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df","4b2af71d":"## Training and Visualizing Decision Trees\n\nA decision tree in general parlance represents a hierarchical series of binary decisions we let the computer figure out the optimal structure & hierarchy of decisions, instead of coming up with criteria manually. \n\n<img src=\"https:\/\/i.imgur.com\/qSH4lqz.png\" width=\"480\">","22cc5ac5":"It appears that the model has learned the training examples perfect, and doesn't generalize well to previously unseen examples. This phenomenon is called \"**overfitting**\", and reducing overfitting is one of the most important parts of any machine learning project.","2ecc0131":"### Training and Validation Set\n\nFinally, let's split the dataset into a training and validation set. We'll use a randomly select 25% subset of the data for validation. Also, we'll use just the numeric and encoded columns, since the inputs to our model must be numbers. ","abacd59c":"* Lets create a helper function `test_params` which will calculate train rmse and val rmse for us.\n* Lets create one more function `test_param_and_plot` which will plot values train rmse and val rmse. ","49470c6d":"## Hyperparameter Tuning and Overfitting\n\nAs we saw in the previous section, our decision tree classifier memorized all training examples, leading to a 100% training accuracy, while the validation accuracy is 5%. This phenomenon is called **overfitting**, and in this section, we'll look at some strategies for reducing overfitting. *The process of reducing overfitting is known as **_regularlization_**.*\n\n\nThe `DecisionTreeClassifier` accepts several arguments, some of which can be modified to reduce overfitting.","5bbc0d5c":"### Feature Importance\n\nBased on the gini index computations, a decision tree assigns an \"importance\" value to each feature. These values can be used to interpret the results given by a decision tree."}}