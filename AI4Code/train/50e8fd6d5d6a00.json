{"cell_type":{"0063dedd":"code","e5f56ea4":"code","1984d1bc":"code","e532cf26":"code","b9755cd2":"code","a16515b2":"code","a48f0541":"code","fb6c6cd8":"code","2fa61a9e":"code","3b1b74b1":"code","1c374500":"code","6eaefe96":"code","35e2098e":"code","fd18be55":"code","9b0278ec":"code","b031b1fe":"code","f03cb958":"code","a422a976":"code","08717b2d":"code","9548b047":"code","c8b0851c":"code","f8e9027e":"code","782c5c87":"code","4f6ea5b8":"code","851f5978":"code","7cee1248":"code","943b58ed":"code","561096a3":"code","f33fc092":"code","b7fc9c12":"code","0512a556":"code","82b0f45b":"code","67f28da9":"code","adc972c3":"code","0f4924dd":"code","5f15b7ca":"code","a6c2adcf":"code","020a2ab8":"code","21bfba3f":"code","02c82bb2":"code","53d22660":"code","9b333d86":"code","8cc9df17":"code","5b2292d0":"code","35e8dba2":"markdown","54d4fab3":"markdown","f9ce4825":"markdown","ebe5ccce":"markdown","b88a2e8a":"markdown","2feb3207":"markdown","b5ba3d5e":"markdown","161726c6":"markdown","5781ba26":"markdown","4169b232":"markdown","0133442e":"markdown","c8a73c69":"markdown","ed5ea776":"markdown"},"source":{"0063dedd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","e5f56ea4":"data = pd.read_excel('\/kaggle\/input\/employee-absenteeism\/Absenteeism_at_work_Project.xls')\ndata.head()","1984d1bc":"data.info()","e532cf26":"pd.set_option(\"display.float_format\", \"{:.2f}\".format)\ndata.describe()","b9755cd2":"for column in data.columns:\n    print(f\"===============Column: {column}==============\")\n    print(f\"Number of unique values: {data[column].nunique()}\")\n    print(f\"Max: {data[column].max()}\")\n    print(f\"Min: {data[column].min()}\")","a16515b2":"data.columns","a48f0541":"data.ID.value_counts().hist(bins=data.ID.nunique())","fb6c6cd8":"data.drop('ID', axis=1, inplace=True)","2fa61a9e":"# Visulazing the distibution of the data for every feature\ndata.hist(edgecolor='black', linewidth=1.2, figsize=(20, 20));","3b1b74b1":"print(f\"{data['Absenteeism time in hours'].value_counts()}\")\nprint(f\"{data['Absenteeism time in hours'].value_counts()[0] \/ data['Absenteeism time in hours'].value_counts()[1]}\")","1c374500":"data[\"Reason for absence\"].value_counts()","6eaefe96":"data[\"Reason for absence\"] = data[\"Reason for absence\"].map({0: \"Group_1\", 1: \"Group_1\", 2: \"Group_1\", 3: \"Group_1\", \n                                                             4: \"Group_1\", 5: \"Group_1\", 6: \"Group_1\", 7: \"Group_1\", \n                                                             8: \"Group_1\", 9: \"Group_1\", 10: \"Group_1\", 11: \"Group_1\", \n                                                             12: \"Group_1\", 13: \"Group_1\", 14: \"Group_1\", 15: \"Group_2\", \n                                                             16: \"Group_2\", 17: \"Group_2\", 17: \"Group_2\", 18: \"Group_3\", \n                                                             19: \"Group_3\", 20: \"Group_3\", 21: \"Group_3\", 22: \"Group_4\", \n                                                             23: \"Group_4\", 24: \"Group_4\", 25: \"Group_4\", 26: \"Group_4\", \n                                                             27: \"Group_4\", 28: \"Group_4\"})\n# data[\"Reason for Absence\"] = data[\"Reason for Absence\"].astype(\"category\").cat.codes\ndata[\"Reason for absence\"].value_counts()","35e2098e":"data_1 = pd.get_dummies(data, columns=['Reason for absence'])","fd18be55":"data_1.head()","9b0278ec":"data_1.dtypes","b031b1fe":"data_1.dropna(inplace=True)\ndata_1.isna().sum()","f03cb958":"data_1[\"Education\"] = data_1.Education.map({1: 0, 2: 1, 3: 1, 4: 1})","a422a976":"data_1.Education.value_counts()","08717b2d":"data_1.Education.isna().sum()","9548b047":"data_2 = pd.get_dummies(data_1, columns=[\"Education\"], drop_first=True)\ndata_2.columns","c8b0851c":"plt.figure(figsize=(20, 15))\nsns.heatmap(data_2.corr(), annot=True)","f8e9027e":"data_2['Absenteeism time in hours'].hist(bins=data_2['Absenteeism time in hours'].nunique())","782c5c87":"# data_2.drop([\"Distance to Work\", \"Month\", \"Weekday\", \"Age\", \"Daily Work Load Average\"], \n#             axis=1, inplace=True)","4f6ea5b8":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\nX = data_2.drop('Absenteeism time in hours', axis=1)\ny = np.where(data_2[\"Absenteeism time in hours\"] > data_2[\"Absenteeism time in hours\"].median(), 1, 0)\n\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\npipe = Pipeline([\n    ('min_max_scaler', MinMaxScaler()),\n    ('std_scaler', StandardScaler())\n])\n\nX_train = pipe.fit_transform(X_train)\nX_test = pipe.transform(X_test)","851f5978":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\ndef evaluate(model, X_train, X_test, y_train, y_test):\n    y_test_pred = model.predict(X_test)\n    y_train_pred = model.predict(X_train)\n\n    print(\"TRAINIG RESULTS: \\n===============================\")\n    clf_report = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))\n    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_train, y_train_pred)}\")\n    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_train, y_train_pred):.4f}\")\n    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n\n    print(\"TESTING RESULTS: \\n===============================\")\n    clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))\n    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_test_pred)}\")\n    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_test_pred):.4f}\")\n    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")","7cee1248":"from sklearn.linear_model import LogisticRegression\n\nprint(\"\\n=========LOGISTIC REGRESSION============\")\nlr_clf = LogisticRegression(solver='liblinear', penalty='l1')\nlr_clf.fit(X_train, y_train)\nevaluate(lr_clf, X_train, X_test, y_train, y_test)","943b58ed":"from sklearn.metrics import precision_recall_curve, roc_curve\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.title(\"Precision\/Recall Tradeoff\")\n    \n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], \"k--\")\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    \n    \nprecisions, recalls, thresholds = precision_recall_curve(y_test, lr_clf.predict(X_test))\nplt.figure(figsize=(14, 25))\nplt.subplot(4, 2, 1)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\nplt.subplot(4, 2, 2)\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions\/recalls tradeoff\");\n\nplt.subplot(4, 2, 3)\nfpr, tpr, thresholds = roc_curve(y_test, lr_clf.predict(X_test))\nplot_roc_curve(fpr, tpr)","561096a3":"scores_dict = {\n    'Logistic Regression': {\n        'Train': accuracy_score(y_train, lr_clf.predict(X_train)),\n        'Test': accuracy_score(y_test, lr_clf.predict(X_test)),\n    },\n}","f33fc092":"scores = cross_val_score(lr_clf, X_train, y_train, cv=10)\nprint(f\"Cross-Validation score mean: {scores.mean() * 100:.2f}% (+\/-{scores.std() * 100:.2f})\")","b7fc9c12":"feature_name = X.columns\nsummary_table = pd.DataFrame(columns=[\"Features_name\"], data=feature_name)\nsummary_table[\"Coefficients\"] = np.transpose(lr_clf.coef_)\nsummary_table","0512a556":"summary_table.index = summary_table.index + 1\nsummary_table.loc[0] = ['Intercept', lr_clf.intercept_[0]]\nsummary_table.sort_index(inplace=True)","82b0f45b":"summary_table[\"Odds_ratio\"] = np.exp(summary_table.Coefficients)\nsummary_table.sort_values(by=\"Odds_ratio\", ascending=False)","67f28da9":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n\nprint(\"\\n=========RANDOM FOREST============\")\nn_estimators = [100, 500, 1000, 1500]\nmax_features = ['auto', 'sqrt']\nmax_depth = [2, 5, 10, 15, None]\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\ncriterion = ['gini', 'entropy']\n\nparams_grid = {\n    'n_estimators': n_estimators, \n    'max_features': max_features,\n    'max_depth': max_depth, \n    'min_samples_split': min_samples_split,\n    'min_samples_leaf': min_samples_leaf, \n    'bootstrap': bootstrap\n}\n\nrf_clf = RandomForestClassifier(random_state=42)\n\nrf_cv = GridSearchCV(rf_clf, params_grid, scoring=\"f1\", cv=3, verbose=2, n_jobs=-1)\n\n\nrf_cv.fit(X_train, y_train)\nbest_params = rf_cv.best_params_\nprint(f\"Best parameters: {best_params}\")\n\nrf_clf = RandomForestClassifier(**best_params)\nrf_clf.fit(X_train, y_train)\n\nevaluate(rf_clf, X_train, X_test, y_train, y_test)","adc972c3":"precisions, recalls, thresholds = precision_recall_curve(y_test, rf_clf.predict(X_test))\nplt.figure(figsize=(14, 25))\nplt.subplot(4, 2, 1)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\nplt.subplot(4, 2, 2)\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions\/recalls tradeoff\");\n\nplt.subplot(4, 2, 3)\nfpr, tpr, thresholds = roc_curve(y_test, rf_clf.predict(X_test))\nplot_roc_curve(fpr, tpr)","0f4924dd":"scores_dict['Random Forest'] = {\n        'Train': accuracy_score(y_train, rf_clf.predict(X_train)),\n        'Test': accuracy_score(y_test, rf_clf.predict(X_test)),\n    }","5f15b7ca":"import xgboost as xgb\n\nn_estimators = [50, 100, 250]\nmax_depth = [2, 3, 5, 10, 15]\n# booster = ['gbtree', 'gblinear']\nbase_score = [0.2, 0.25, 0.5, 0.75, 0.99]\nlearning_rate = [0.05, 0.1, 0.5, 0.9, 1]\nmin_child_weight = [1, 2, 3, 4]\nsubsample = [0.5, 0.75, 0.85, 0.9, 1]\ncolsample_bytree = [0.5, 0.75, 0.85, 0.9, 1]\ncolsample_bynode = [0.5, 0.75, 0.85, 0.9, 1]\ncolsample_bylevel = [0.5, 0.75, 0.85, 0.9, 1]\n\nparams_grid = {\n    'n_estimators': n_estimators, \n    'max_depth': max_depth,\n    'learning_rate' : learning_rate, \n    'min_child_weight' : min_child_weight, \n#     'booster' : booster, \n    'base_score' : base_score,\n    'subsample': subsample,\n#     'colsample_bytree': colsample_bytree,\n#     'colsample_bynode': colsample_bynode,\n#     'colsample_bylevel': colsample_bylevel,\n}\n\nxgb_clf = xgb.XGBClassifier()\n\nxgb_cv = GridSearchCV(xgb_clf, params_grid, cv=3, scoring = 'f1',n_jobs =-1, verbose=1)\n\n\nxgb_cv.fit(X_train, y_train)\nbest_params = xgb_cv.best_params_\nprint(f\"Best paramters: {best_params}\")\n\nxgb_clf = xgb.XGBClassifier(**best_params)\nxgb_clf.fit(X_train, y_train)\n\nevaluate(xgb_clf, X_train, X_test, y_train, y_test)","a6c2adcf":"precisions, recalls, thresholds = precision_recall_curve(y_test, xgb_clf.predict(X_test))\nplt.figure(figsize=(14, 25))\nplt.subplot(4, 2, 1)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\nplt.subplot(4, 2, 2)\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions\/recalls tradeoff\");\n\nplt.subplot(4, 2, 3)\nfpr, tpr, thresholds = roc_curve(y_test, xgb_clf.predict(X_test))\nplot_roc_curve(fpr, tpr)","020a2ab8":"scores_dict['XGBoost'] = {\n        'Train': accuracy_score(y_train, xgb_clf.predict(X_train)),\n        'Test': accuracy_score(y_test, xgb_clf.predict(X_test)),\n    }","21bfba3f":"from sklearn.svm import SVC\n\nparam_grid = {'C': [60, 70, 75, 65 ], \n              'gamma': [0.002, 0.001, 0.0009, 0.0008, 0.0007], \n              'kernel': ['rbf', 'poly', 'linear']} \n\nsvm_cv = GridSearchCV(SVC(), param_grid, scoring='accuracy', verbose=1, cv=3)\nsvm_cv.fit(X_train, y_train)\n\nbest_params = svm_cv.best_params_\nprint(f\"Best params: {best_params}\")\n\nsvm_clf = SVC(**best_params)\nsvm_clf.fit(X_train, y_train)\nevaluate(svm_clf, X_train, X_test, y_train, y_test)","02c82bb2":"precisions, recalls, thresholds = precision_recall_curve(y_test, svm_clf.predict(X_test))\nplt.figure(figsize=(14, 25))\nplt.subplot(4, 2, 1)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\nplt.subplot(4, 2, 2)\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions\/recalls tradeoff\");\n\nplt.subplot(4, 2, 3)\nfpr, tpr, thresholds = roc_curve(y_test, xgb_clf.predict(X_test))\nplot_roc_curve(fpr, tpr)","53d22660":"scores_dict['Support Vector Machine'] = {\n        'Train': accuracy_score(y_train, svm_clf.predict(X_train)),\n        'Test': accuracy_score(y_test, svm_clf.predict(X_test)),\n    }","9b333d86":"from sklearn.metrics import roc_auc_score\n\nml_models = {\n    'Logistic Regression': lr_clf, \n    'Random Forest': rf_clf, \n    'XGboost': xgb_clf, \n    'Support Vector Machine': svm_clf\n}\nfor model in ml_models:\n    print(f\"{model.upper()} roc_auc_score: {roc_auc_score(y_test, ml_models[model].predict(X_test)):.3f}\")","8cc9df17":"scores_df = pd.DataFrame(scores_dict)\n\nscores_df.plot(kind='barh', figsize=(15, 8))","5b2292d0":"import pickle\n\nwith open('Xgb_clf', 'wb') as file:\n    pickle.dump(xgb_clf, file)\n\nwith open('Rf_clf', 'wb') as file:\n    pickle.dump(rf_clf, file)","35e8dba2":"Now you can simply select the threshold value that gives you the best precision\/recall tradeoff for your task. In our case we need to predict if an employee will absent accuratly. so we need to increase ","54d4fab3":"# \u2714\ufe0f Logistic Regression","f9ce4825":"# \ud83d\udcbc Business Case Study: Employee Absenteeism\n\n> \ud83d\udcbe The database was created with records of absenteeism at work from July 2007 to July 2010 at a courier company in Brazil.\n\n----\n\n# \ud83d\udccc The business Task\n\n> The exercise will address `Absenteeism` at a company during work time.\n\n---\n\n## \ud83d\udd12 Problem: \n> The problem is that the work environment of today is more:\n> - Competitive\n> - Managers set unachievable business goals\n> - have an elevated risk of becoming unemployed\n\n> This can be lead to an increase in pressure and stress of the employee. Those factors influence employee health, which is of course indesirable.\n\n---\n\n## \ud83c\udfc3\u200d What is Absenteeism?\n> Absence from work during normal working hours resulting in temporary incapacity to execute a regular working activity.\n> - Based on what information should we predict whether an employee is expected to be absent or not?\n> - How should we measure absenteeism?\n\n---\n\n## \ud83c\udfaf Purpose of the business exercise:\n\n> Explore whether a person presenting certain characteristics is expected to be away from work at some point in time or not.\n\n> We want to know for how many working hours any employee could be away from work based on information like:\n> - How far they live from their workplace.\n> - How many children and pets they have.\n> - Do they have higher education?","ebe5ccce":"# \ud83e\udd16 Applying machine learning algorthims","b88a2e8a":"The data doesn't have any missing values.","2feb3207":"# \u2714\ufe0f XGBoost","b5ba3d5e":"# \u2714\ufe0f Random Forest","161726c6":"# \ud83d\udcca  Exploratory Data Analysis (EDA)","5781ba26":"`Reason for Absence`: We have `28` reason of absence from `0` to `28`.","4169b232":"`ID`: individual identification (in this case we have 34 employees) indicates precisely who has been away during working hours.  Will this information improve our analysis in any way? No, because it's only a label variable (a number that is there to distinguish the individuals from one another, not to carry any numeric information).\n\nSo we are going to drop this column","0133442e":"# \ud83c\udfaf Comparing Machine Learning models \ud83d\udcca\n\nArea Under the Curve score (AUC) is good way to compare classifiers. A perfect classifier AUC will have a ROC AUC equal to 1.","c8a73c69":"# \ud83d\udcbd Save the models","ed5ea776":"# \u2714\ufe0f Support Vector Machine"}}