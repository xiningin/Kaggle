{"cell_type":{"01efcc85":"code","05542d4e":"code","125c728f":"code","e9e786e5":"code","fb48bc2f":"code","03a655de":"code","42c336e5":"code","baea869f":"code","093dcd30":"code","76780c27":"code","14845243":"code","a5df2afd":"code","ddf70366":"code","f996362d":"code","251bc75f":"code","fb35ded3":"code","1041815e":"code","05f1c343":"code","b9b5dccd":"code","04c200f1":"code","aae1b59e":"code","bf35bb2c":"code","22c3f508":"code","cca00b06":"code","5486e057":"code","2e0522c4":"code","937f104c":"code","5f273757":"code","d8464649":"code","7cdeda26":"code","194d9552":"code","e42f7e7b":"code","794e8379":"code","c709af29":"code","b1c36509":"code","0f10bd72":"code","9238c159":"code","515b8bfb":"code","8b914fbd":"code","9cceb842":"code","9ec5b1b9":"code","ab978617":"code","a1dea608":"code","271f0800":"code","e56e41dc":"code","ba78e7a4":"code","5d726ad5":"code","03c77c34":"code","14cbba87":"code","f1b2f1dc":"code","079e42cf":"code","1f586e08":"code","dbf426f1":"code","eb8b3c60":"code","e3ea5eea":"code","bd95f0b6":"code","d50488cc":"code","b38c5884":"code","1ab7b39e":"code","20a599db":"code","04e35120":"code","5f297ba4":"code","d5d555e1":"code","2b56e484":"code","a6c91f6b":"code","9a6c0350":"code","2ea1d7cb":"code","91231923":"code","183622d3":"code","7638eada":"code","e890b3a3":"code","2d1aa371":"code","083bd7d0":"code","6e23022d":"code","129f6e8d":"code","29757397":"code","110f9b7c":"code","b3bcf0e6":"code","2ee768a0":"code","c0dd7726":"code","33ea229b":"code","54974a9d":"code","aee4f6ca":"code","67de4cdc":"code","e83cc5fc":"code","96a6908f":"code","9eddac27":"code","48a9190d":"code","36efc7f8":"code","4070032c":"code","f2e1f7a0":"code","73ab05e7":"code","c0969a40":"code","2197dae6":"code","d8f08d42":"code","24c911ea":"code","77ea66dd":"markdown","346e664d":"markdown","c7338832":"markdown","ccf54634":"markdown","36002177":"markdown","95496913":"markdown","e038e9bd":"markdown","6393ecf3":"markdown","1883287c":"markdown","eb99da33":"markdown","75aa37e8":"markdown","6eba8391":"markdown","e3748978":"markdown","c8bbbe9d":"markdown","ea27802e":"markdown"},"source":{"01efcc85":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","05542d4e":"import numpy as np\nimport scipy as sp\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom category_encoders import OrdinalEncoder, OneHotEncoder, TargetEncoder\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.preprocessing import quantile_transform\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\n\nimport gc\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)","125c728f":"df_train = pd.read_csv('\/kaggle\/input\/homework-for-students4plus\/train.csv', index_col=0)\ndf_test = pd.read_csv('\/kaggle\/input\/homework-for-students4plus\/test.csv', index_col=0)\n\nsample_submission = pd.read_csv('\/kaggle\/input\/homework-for-students4plus\/sample_submission.csv')\nfree_zipcde_database = pd.read_csv('\/kaggle\/input\/homework-for-students4plus\/free-zipcode-database.csv')\nstatelatlong = pd.read_csv('\/kaggle\/input\/homework-for-students4plus\/statelatlong.csv')\nUS_GDP_by_State = pd.read_csv('\/kaggle\/input\/homework-for-students4plus\/US_GDP_by_State.csv')","e9e786e5":"df_train.shape, df_test.shape","fb48bc2f":"# statelatlong \u3068\u7d50\u5408\ndf_train = pd.merge(df_train, statelatlong, how='left', left_on='addr_state', right_on='State').set_index(df_train.index)\ndf_test = pd.merge(df_test, statelatlong, how='left', left_on='addr_state', right_on='State').set_index(df_test.index)","03a655de":"df_train.shape, df_test.shape","42c336e5":"US_GDP_by_State_grpby = US_GDP_by_State.groupby(\"State\")[['State & Local Spending','Gross State Product','Real State Growth %','Population (million)']].mean()\nUS_GDP_by_State_grpby.head()","baea869f":"US_GDP_by_State_grpby['State & Local Spending\/Population'] = US_GDP_by_State_grpby['State & Local Spending']\/US_GDP_by_State_grpby['Population (million)']\nUS_GDP_by_State_grpby['Gross State Product\/Population'] = US_GDP_by_State_grpby['Gross State Product']\/US_GDP_by_State_grpby['Population (million)']\nUS_GDP_by_State_grpby = US_GDP_by_State_grpby.drop(['State & Local Spending'], axis=1)\nUS_GDP_by_State_grpby = US_GDP_by_State_grpby.drop(['Gross State Product'], axis=1)\nUS_GDP_by_State_grpby = US_GDP_by_State_grpby.drop(['Population (million)'], axis=1)\nUS_GDP_by_State_grpby.head()","093dcd30":"# US_GDP_by_State \u3068\u7d50\u5408\ndf_train = pd.merge(df_train, US_GDP_by_State_grpby, how='left', left_on='City', right_on='State').set_index(df_train.index)\ndf_test = pd.merge(df_test, US_GDP_by_State_grpby, how='left', left_on='City', right_on='State').set_index(df_test.index)","76780c27":"df_train.shape, df_test.shape","14845243":"for col in df_train.columns:\n        print(col, df_train[col].nunique(), df_train[col].dtype)","a5df2afd":"# dtype\u304cobject\uff08\u6570\u5024\u3067\u306a\u3044\u3082\u306e\uff09\u306e\u30ab\u30e9\u30e0\u540d\u3068\u30e6\u30cb\u30fc\u30af\u6570\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\ncats = []\nnums = []\nfor col in df_train.columns:\n    if df_train[col].dtype == 'object':\n        cats.append(col)\n        print('cats:', col, df_train[col].nunique(), df_train[col].dtype)\n    else:\n        nums.append(col)\n        print('nums:',col, df_train[col].nunique(), df_train[col].dtype)","ddf70366":"df_train.isnull().sum()","f996362d":"#\u5404\u5217\u306e\u69cb\u6210\u8981\u7d20\u306e\u78ba\u8a8d\nfor i in df_train.columns:\n    print(df_train[i].value_counts())\n    print('\\n')","251bc75f":"# df_train['installment'] = df_train['installment'].apply(np.log1p)\n# df_train['annual_inc'] = df_train['annual_inc'].apply(np.log1p)\n# df_train['dti'] = df_train['dti'].apply(np.log1p)\n\n# df_test['installment'] = df_test['installment'].apply(np.log1p)\n# df_test['annual_inc'] = df_test['annual_inc'].apply(np.log1p)\n# df_test['dti'] = df_test['dti'].apply(np.log1p)","fb35ded3":"fig=plt.figure(figsize=[20,60])\n\ntarget = 'loan_condition'\n\ni = 0\nfor col in nums:\n    i=i+1\n    if col == target:\n        continue\n\n    else:\n        plt.subplots_adjust(wspace=0.2, hspace=0.8)\n        ax_name = fig.add_subplot(40,2,i)\n        ax_name.hist(df_train[col],bins=30,density=True, alpha=0.5,color = 'r')\n        ax_name.hist(df_test[col],bins=30,density=True, alpha=0.5, color = 'b')\n        ax_name.set_title(col)","1041815e":"import datetime\ndf_train['issue_d'] = pd.to_datetime(df_train['issue_d'], format='%b-%Y')\ndf_test['issue_d'] = pd.to_datetime(df_test['issue_d'], format='%b-%Y')","05f1c343":"# issue_d \u3067\u96c6\u8a08\u3057\u3066\u3001\u6642\u7cfb\u5217\u306e\u30c7\u30fc\u30bf\u306e\u63a8\u79fb\u3092\u78ba\u8a8d\u3057\u3066\u307f\u308b\ndf_train_m = df_train.groupby('issue_d').agg(np.sum).copy()\ndf_test_m = df_test.groupby('issue_d').agg(np.sum).copy()","b9b5dccd":"plt.plot(df_train_m['loan_condition'])\nplt.title('loan_condition')\nplt.xlabel('issue_d')\nplt.ylabel('loan_condition')\nplt.show()","04c200f1":"fig=plt.figure(figsize=[20,60])\n\ntarget = 'loan_condition'\n\ni = 0\nfor col in nums:\n    i=i+1\n    if col == target:\n        continue\n\n    else:\n        plt.subplots_adjust(wspace=0.2, hspace=0.8)\n        ax_name = fig.add_subplot(40,2,i)\n\n        plt.plot(df_train_m[col])\n        plt.plot(df_test_m[col])\n        plt.title(col)\n        plt.xlabel('issue_d')\n        plt.ylabel(col)\n        plt.show()","aae1b59e":"# 2014\u5e74\u4ee5\u964d\u306e\u30c7\u30fc\u30bf\u306b\u9650\u5b9a\ndf_train['year'] = df_train['issue_d'].dt.year\n\n#df_train.drop(df_train.index[df_train['year'] <= 2013], inplace=True) # 2014-2015\u5e74\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u3059\u308b\ndf_train.drop(df_train.index[df_train['year'] <= 2014], inplace=True) # 2015\u5e74\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u3059\u308b","bf35bb2c":"df_train.shape, df_test.shape","22c3f508":"# drop \u5f8c\u306e\u518d\u78ba\u8a8d\ndf_train_m = df_train.groupby('issue_d').agg(np.sum).copy()\ndf_test_m = df_test.groupby('issue_d').agg(np.sum).copy()","cca00b06":"fig=plt.figure(figsize=[20,60])\n\ntarget = 'loan_condition'\n\ni = 0\nfor col in nums:\n    i=i+1\n    if col == target:\n        plt.subplots_adjust(wspace=0.2, hspace=0.8)\n        ax_name = fig.add_subplot(40,2,i)\n\n        plt.plot(df_train_m[col])\n        plt.title(col)\n        plt.xlabel('issue_d')\n        plt.ylabel(col)\n        plt.xticks(rotation=45)\n        plt.show()\n\n    else:\n        plt.subplots_adjust(wspace=0.2, hspace=0.8)\n        ax_name = fig.add_subplot(40,2,i)\n\n        plt.plot(df_train_m[col])\n        plt.plot(df_test_m[col])\n        plt.title(col)\n        plt.xlabel('issue_d')\n        plt.ylabel(col)\n        plt.xticks(rotation=45)\n        plt.show()","5486e057":"#df_train[df_train.emp_title.isnull()==True]\ndf_train['emp_title'].isnull().sum()","2e0522c4":"# emp_title\u306enull\u30d5\u30e9\u30b0\u3092\u4f5c\u6210(\u3053\u306e\u30d5\u30e9\u30b0\u306f\u91cd\u8981)\ndf_train['emp_title_null_flg'] = np.where(df_train['emp_title'].isnull()==True, 1, 0)\ndf_test['emp_title_null_flg'] = np.where(df_test['emp_title'].isnull()==True, 1, 0)","937f104c":"df_train['emp_title_null_flg'].value_counts()","5f273757":"# df_train.query('emp_title == [\"president\", \"CEO\"]')","d8464649":"# df_train['emp_title_president_flg'] = np.where((df_train['emp_title'] == 'president') | (df_train['emp_title'] == 'CEO'), 1, 0)\n# df_test['emp_title_president_flg'] = np.where((df_test['emp_title'] == 'president') | (df_test['emp_title'] == 'CEO'), 1, 0)","7cdeda26":"# df_train['emp_title_president_flg'].value_counts()","194d9552":"# # debt_consolidation_flg\u306e\u4f5c\u6210\n# df_train['purpose'].value_counts()","e42f7e7b":"# df_train['debt_consolidation_flg'] = np.where((df_train['purpose'] == 'debt_consolidation'), 1, 0)\n# df_test['debt_consolidation_flg'] = np.where((df_test['purpose'] == 'debt_consolidation'), 1, 0)","794e8379":"# df_train['debt_consolidation_flg'].value_counts()","c709af29":"# Text\u3092\u3088\u3051\u3066\u304a\u304f\nTXT_train = df_train.emp_title.copy()\nTXT_test = df_test.emp_title.copy()\n\n#df_train = df_train.drop(['emp_title'], axis=1)\n#df_test = df_test.drop(['emp_title'], axis=1)","b1c36509":"df_train = df_train.drop(['year'], axis=1)\ndf_train = df_train.drop(['issue_d'], axis=1)\n#df_train = df_train.drop(['title'], axis=1) \ndf_train = df_train.drop(['earliest_cr_line'], axis=1)\n#df_train = df_train.drop(['State'], axis=1) # adder_state\u3068\u304b\u3076\u3063\u3066\u3044\u308b\u3002\u304b\u3064\u3001adder_state\u306e\u65b9\u304cfeature importance\u304c\u4e0a\u3002\n#df_train = df_train.drop(['City'], axis=1) # adder_state\u3068\u304b\u3076\u3063\u3066\u3044\u308b\u3002\u304b\u3064\u3001adder_state\u306e\u65b9\u304cfeature importance\u304c\u4e0a\u3002\n#df_train = df_train.drop(['addr_state'], axis=1) # train\u3068test \u3067\u8aa4\u5dee\u304c\u5927\u304d\u3044\u3002zip_code\u3067\u5341\u5206\n\ndf_test = df_test.drop(['issue_d'], axis=1)\n#df_test = df_test.drop(['title'], axis=1)\ndf_test = df_test.drop(['earliest_cr_line'], axis=1)\n#df_test = df_test.drop(['State'], axis=1)\n#df_test = df_test.drop(['City'], axis=1)\n#df_test = df_test.drop(['addr_state'], axis=1) # train\u3068test \u3067\u8aa4\u5dee\u304c\u5927\u304d\u3044\u3002zip_code\u3067\u5341\u5206\n","0f10bd72":"df_train.shape, df_test.shape","9238c159":"X_train = df_train.drop(['loan_condition'], axis=1)\ny_train = df_train.loan_condition\n\nX_test = df_test","515b8bfb":"X_train.isnull().sum()","8b914fbd":"X_test.isnull().sum()","9cceb842":"#emp_length\u304c\u6b20\u640d\u3057\u3066\u3044\u308b\u4eba\u306egrade\u3002emp_length\u306e\u6b20\u640d\u5024\u88dc\u5b8c\u306f\u3001grade\u3067group by\u3057\u305f\u969b\u306e\u5e73\u5747\u5024\u304c\u3044\u3044\u304b\u3082\uff1f\nX_train[X_train['emp_length'].isnull() == True].grade.value_counts()\n","9ec5b1b9":"def mapping(map_col, mapping):\n    X_train[map_col] = X_train[map_col].map(mapping)\n    X_test[map_col] = X_test[map_col].map(mapping)","ab978617":"grade_mapping = { \"A\": 1,\"B\": 2,\"C\": 3,\"D\": 4,\"E\": 5,\"F\": 6,\"G\": 7 }\n\nsubgrade_mapping = {\"A1\": 1,\"A2\": 2,\"A3\": 3,\"A4\": 4,\"A5\": 5,\"B1\": 6,\"B2\": 7,\"B3\": 8,\"B4\": 9,\"B5\": 10,\n                    \"C1\": 11,\"C2\": 12,\"C3\": 13,\"C4\": 14,\"C5\": 15,\"D1\": 16,\"D2\": 17,\"D3\": 18,\"D4\": 19,\"D5\": 20,\n                    \"E1\": 21,\"E2\": 22,\"E3\": 23,\"E4\": 24,\"E5\": 25,\"F1\": 26,\"F2\": 27,\"F3\": 28,\"F4\": 29,\"F5\": 30,\n                    \"G1\": 31,\"G2\": 32,\"G3\": 33,\"G4\": 34,\"G5\": 35\n                   }\n\nemp_length_mapping = {\"< 1 year\": 0.5, \"1 year\": 1, \"2 year\": 2,\"3 year\": 3,\"4 year\": 4,\"5 year\": 5,\n                      \"6 year\": 6, \"7 year\": 7, \"8 year\": 8,\"9 year\": 9,\"10+ years\": 10\n                    }","a1dea608":"mapping('grade', grade_mapping)\nmapping('sub_grade', subgrade_mapping)\nmapping('emp_length', emp_length_mapping)","271f0800":"#\u8abf\u67fb","e56e41dc":"#emp_length\u304c0\u306e\u4eba\u306f\u3069\u3093\u306a\u4eba\uff1f\nX_train[X_train['emp_length'].isnull() == True].head(100)","ba78e7a4":"#emp_length\u304c\u6b20\u640d\u3057\u3066\u3044\u308b\u4eba\u306egrade\u3002emp_length\u306e\u6b20\u640d\u5024\u88dc\u5b8c\u306f\u3001grade\u3067group by\u3057\u305f\u969b\u306e\u5e73\u5747\u5024\u304c\u3044\u3044\u304b\u3082\uff1f\nX_train[X_train['emp_length'].isnull() == True].grade.value_counts()","5d726ad5":"X_train[X_train['dti'].isnull()==True]","03c77c34":"X_train[X_train['revol_util'].isnull()==True]","14cbba87":"X_train[X_train['mths_since_last_delinq'].isnull()==True]","f1b2f1dc":"X_train[X_train['mths_since_last_major_derog'].isnull()==True]","079e42cf":"# \u6b20\u640d\u5024\u306e\u88dc\u5b8c\u51e6\u7406","1f586e08":"# # null\u30d5\u30e9\u30b0\u3092\u4f5c\u6210\u3000#\u3053\u306e\u30d5\u30e9\u30b0\u306f\u306a\u3044\u307b\u3046\u304c\u3044\u3044\n# X_train['mths_since_last_delinq_null_flg'] = np.where(X_train['mths_since_last_delinq'].isnull()==True, 1, 0)\n# X_test['mths_since_last_delinq_null_flg'] = np.where(X_test['mths_since_last_delinq'].isnull()==True, 1, 0)\n\n# X_train['mths_since_last_record_null_flg'] = np.where(X_train['mths_since_last_record'].isnull()==True, 1, 0)\n# X_test['mths_since_last_record_null_flg'] = np.where(X_test['mths_since_last_record'].isnull()==True, 1, 0)\n\n# X_train['mths_since_last_major_derog_null_flg'] = np.where(X_train['mths_since_last_major_derog'].isnull()==True, 1, 0)\n# X_test['mths_since_last_major_derog_null_flg'] = np.where(X_test['mths_since_last_major_derog'].isnull()==True, 1, 0)","dbf426f1":"\nX_train['mths_since_last_delinq'].fillna(0, inplace=True)\nX_train['mths_since_last_record'].fillna(0, inplace=True)\nX_train['mths_since_last_major_derog'].fillna(0, inplace=True)\nX_train['dti'].fillna(0, inplace=True)\n\nX_test['mths_since_last_delinq'].fillna(0, inplace=True)\nX_test['mths_since_last_record'].fillna(0, inplace=True)\nX_test['mths_since_last_major_derog'].fillna(0, inplace=True)\nX_test['inq_last_6mths'].fillna(0, inplace=True)\nX_test['dti'].fillna(0, inplace=True)","eb8b3c60":"#sub_grade\u306egroupby\u3057\u305f\u5e73\u5747\u5024\u3067\u6b20\u640d\u5024\u3092\u88dc\u5b8c\u3059\u308b\nX_train['revol_util'] = X_train.groupby(['sub_grade'])['revol_util'].apply(lambda d: d.fillna(d.mean()))\nX_train['emp_length'] = X_train.groupby(['sub_grade'])['emp_length'].apply(lambda d: d.fillna(d.mean()))\n\nX_test['revol_util'] = X_test.groupby(['sub_grade'])['revol_util'].apply(lambda d: d.fillna(d.mean()))\nX_test['emp_length'] = X_test.groupby(['sub_grade'])['emp_length'].apply(lambda d: d.fillna(d.mean()))","e3ea5eea":"# \u56db\u5247\u6f14\u7b97\u306b\u3088\u308b\u7279\u5fb4\u91cf\u306e\u4f5c\u6210(1)\nX_train['grade+subg'] = X_train['grade'] + X_train['sub_grade']\nX_test['grade+subg'] = X_test['grade'] + X_test['sub_grade']","bd95f0b6":"# \u56db\u5247\u6f14\u7b97\u306b\u3088\u308b\u7279\u5fb4\u91cf\u306e\u8ffd\u52a0(2)\nX_train['loan_amnt\/installment'] = X_train['loan_amnt'] \/ X_train['installment']\nX_train['revol_bal\/installment'] = X_train['revol_bal'] \/ X_train['installment']\n#X_train['tot_cur_bal\/loan_amnt'] = X_train['tot_cur_bal'] \/ X_train['loan_amnt']\n\nX_test['loan_amnt\/installment'] = X_test['loan_amnt'] \/ X_test['installment']\nX_test['revol_bal\/installment'] = X_test['revol_bal'] \/ X_test['installment']\n#X_test['tot_cur_bal\/loan_amnt'] = X_test['tot_cur_bal'] \/ X_test['loan_amnt']\n","d50488cc":"# \u56db\u5247\u6f14\u7b97\u306b\u3088\u308b\u7279\u5fb4\u91cf\u306e\u8ffd\u52a0(3)\ncols=['annual_inc','dti', 'emp_length', 'loan_amnt', 'open_acc','emp_title_null_flg', \n      'inq_last_6mths', 'mths_since_last_record', 'revol_bal', 'revol_util', 'tot_cur_bal']\n\nfor col in cols:\n    X_train['*sub_grade_' + col] = X_train[col] * X_train['sub_grade']\n    X_train['*grade_' + col] = X_train[col] * X_train['grade'] \n    \n    X_test['*sub_grade_' + col] = X_test[col] * X_test['sub_grade']\n    X_test['*grade_' + col] = X_test[col] * X_test['grade']","b38c5884":"cats = []\nnums = []\nfor col in X_train.columns:\n    if X_train[col].dtype == 'object':\n        cats.append(col)\n        print('cats:', col, X_train[col].nunique(), X_train[col].dtype)\n    else:\n        nums.append(col)\n        print('nums:',col, X_train[col].nunique(), X_train[col].dtype)","1ab7b39e":"# \u6b63\u898f\u5206\u5e03\u306b\u5f93\u3063\u3066\u3044\u306a\u3044\u306e\u3067\u30e9\u30f3\u30af\u30ac\u30a6\u30b9\u3092\u5b9f\u65bd\u3059\u308b\n\nX_all = pd.concat([X_train, X_test], axis=0)\nX_all[nums] = quantile_transform(X_all[nums], n_quantiles=100, random_state=0, output_distribution='normal')","20a599db":"X_train = X_all.iloc[:X_train.shape[0], :]\nX_test = X_all.iloc[X_train.shape[0]:, :]","04e35120":"# \u30ab\u30a6\u30f3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n#ce_cols = ['grade','sub_grade','home_ownership','purpose','zip_code','initial_list_status','application_type']\nce_cols = ['purpose']\n\nfor col in ce_cols:\n    train_summary = X_train[col].value_counts()\n    test_summary = X_test[col].value_counts()\n\n    # map\u3059\u308b\u3002\n    X_train['ce_' + col] = X_train[col].map(train_summary)\n    X_test['ce_' + col] = X_test[col].map(test_summary)","5f297ba4":"# Ordinal Encoder \u306f\u3001\u767b\u5834\u9806\u5e8f\u306e\u9806\u756a\u3002label encoder \u306f\u3001\u8f9e\u66f8\u306e\u9806\u306b\u6574\u6570\u5024\u3092\u5272\u308a\u5f53\u3066\u308b\u3002\n#oe_cols = ['home_ownership', 'purpose','zip_code','initial_list_status','application_type']\noe_cols = ['home_ownership', 'purpose','zip_code','initial_list_status','application_type', 'addr_state', 'State', 'City', 'emp_title', 'title']\nencoder = OrdinalEncoder(cols=oe_cols)\nX_train[oe_cols] = encoder.fit_transform(X_train[oe_cols])\nX_test[oe_cols] = encoder.transform(X_test[oe_cols])","d5d555e1":"# Target Encoding\ntarget = 'loan_condition'\nX_temp = pd.concat([X_train, y_train], axis=1)\n\n#te_cols = ['grade','sub_grade','home_ownership','purpose','zip_code','initial_list_status','application_type', 'emp_title_null_flg', 'emp_title_president_flg']\n#te_cols = ['home_ownership','purpose','zip_code','initial_list_status','application_type', 'Latitude', 'Longitude']\n#te_cols = ['grade','sub_grade','emp_length', 'home_ownership','purpose','zip_code','initial_list_status','application_type', 'Latitude', 'Longitude']\nte_cols = ['home_ownership','purpose','zip_code','initial_list_status','Latitude', 'Longitude', 'addr_state', 'State', 'City', 'emp_title', 'emp_length', 'title']\n\n\n#for col in cats: #te_cols:\nfor col in te_cols:    \n\n    # X_test\u306fX_train\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    summary = X_temp.groupby([col])[target].mean()\n    X_test['te_' + col] = X_test[col].map(summary) \n\n\n    # X_train\u306e\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092oof\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n    enc_train = Series(np.zeros(len(X_train)), index=X_train.index)\n\n    for i, (train_ix, val_ix) in enumerate((skf.split(X_train, y_train))):\n        X_train_, _ = X_temp.iloc[train_ix], y_train.iloc[train_ix]\n        X_val, _ = X_temp.iloc[val_ix], y_train.iloc[val_ix]\n\n        summary = X_train_.groupby([col])[target].mean()\n        enc_train.iloc[val_ix] = X_val[col].map(summary)\n        \n    X_train['te_' + col]  = enc_train","2b56e484":"# \u91cd\u8981\u5ea6\u306e\u9ad8\u3044\u7279\u5fb4\u91cf\u3067\u30a8\u30f3\u30b3\u30fc\u30c9\u3092\u5b9f\u65bd(1)\ntarget = 'sub_grade'\nX_temp = pd.concat([X_train, y_train], axis=1)\n\n#fe_cols = ['grade','sub_grade','home_ownership','purpose','zip_code','initial_list_status','application_type', 'emp_title_null_flg', 'emp_title_president_flg']\nfe_cols = ['home_ownership','purpose','zip_code','initial_list_status','application_type']\n#fe_cols = ['home_ownership','purpose','zip_code']\n\nfor col in fe_cols:\n\n    # X_test\u306fX_train\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    summary = X_temp.groupby([col])[target].mean()\n    X_test['fe_' + col] = X_test[col].map(summary) \n\n\n    # X_train\u306e\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092oof\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n    enc_train = Series(np.zeros(len(X_train)), index=X_train.index)\n\n    for i, (train_ix, val_ix) in enumerate((skf.split(X_train, y_train))):\n        X_train_, _ = X_temp.iloc[train_ix], y_train.iloc[train_ix]\n        X_val, _ = X_temp.iloc[val_ix], y_train.iloc[val_ix]\n\n        summary = X_train_.groupby([col])[target].mean()\n        enc_train.iloc[val_ix] = X_val[col].map(summary)\n        \n    X_train['fe_' + col]  = enc_train","a6c91f6b":"# \u91cd\u8981\u5ea6\u306e\u9ad8\u3044\u7279\u5fb4\u91cf\u3067\u30a8\u30f3\u30b3\u30fc\u30c9\u3092\u5b9f\u65bd(2)\ntarget = 'grade'\nX_temp = pd.concat([X_train, y_train], axis=1)\n\nfe_cols = ['home_ownership','purpose','zip_code','initial_list_status','application_type']\n#fe_cols = ['home_ownership','purpose','zip_code']\n\nfor col in fe_cols:\n\n    # X_test\u306fX_train\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    summary = X_temp.groupby([col])[target].mean()\n    X_test['fe2_' + col] = X_test[col].map(summary) \n\n\n    # X_train\u306e\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092oof\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n    enc_train = Series(np.zeros(len(X_train)), index=X_train.index)\n\n    for i, (train_ix, val_ix) in enumerate((skf.split(X_train, y_train))):\n        X_train_, _ = X_temp.iloc[train_ix], y_train.iloc[train_ix]\n        X_val, _ = X_temp.iloc[val_ix], y_train.iloc[val_ix]\n\n        summary = X_train_.groupby([col])[target].mean()\n        enc_train.iloc[val_ix] = X_val[col].map(summary)\n        \n    X_train['fe2_' + col]  = enc_train","9a6c0350":"# \u91cd\u8981\u5ea6\u306e\u9ad8\u3044\u7279\u5fb4\u91cf\u3067\u30a8\u30f3\u30b3\u30fc\u30c9\u3092\u5b9f\u65bd(3)\ntarget = 'grade+subg'\nX_temp = pd.concat([X_train, y_train], axis=1)\n\nfe_cols = ['home_ownership','purpose','zip_code','initial_list_status','application_type']\n#fe_cols = ['home_ownership','purpose','zip_code']\n\nfor col in fe_cols:\n\n    # X_test\u306fX_train\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    summary = X_temp.groupby([col])[target].mean()\n    X_test['fe3_' + col] = X_test[col].map(summary) \n\n\n    # X_train\u306e\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092oof\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n    enc_train = Series(np.zeros(len(X_train)), index=X_train.index)\n\n    for i, (train_ix, val_ix) in enumerate((skf.split(X_train, y_train))):\n        X_train_, _ = X_temp.iloc[train_ix], y_train.iloc[train_ix]\n        X_val, _ = X_temp.iloc[val_ix], y_train.iloc[val_ix]\n\n        summary = X_train_.groupby([col])[target].mean()\n        enc_train.iloc[val_ix] = X_val[col].map(summary)\n        \n    X_train['fe3_' + col]  = enc_train","2ea1d7cb":"X_train","91231923":"X_train.shape, X_test.shape","183622d3":"fig=plt.figure(figsize=[20,60])\n\ni = 0\n#for col in nums:\nfor col in X_train.columns:\n    i=i+1\n    \n    plt.subplots_adjust(wspace=0.2, hspace=0.8)\n    ax_name = fig.add_subplot(90,2,i)\n    ax_name.hist(X_train[col],bins=30,density=True, alpha=0.5,color = 'r')\n    ax_name.hist(X_test[col],bins=30,density=True, alpha=0.5, color = 'b')\n    ax_name.set_title(col)","7638eada":"X_train.isnull().sum()","e890b3a3":"X_test.isnull().sum()","2d1aa371":"# LightGBM GroupKFold\ngroups = X_train.zip_code.values\n\ny_pred_dbdt_avg = np.zeros(len(X_test))\nnum_split = 5\n\ngkf = GroupKFold(n_splits = num_split)\nscores = []\nscores2 = []\n\niter_num = 6 # seed averaging\u3067\u7e70\u308a\u8fd4\u3059\u56de\u6570\u3092\u8a2d\u5b9a\u3002\u5b9f\u969b\u306b\u306f\u3001\uff08iter_num-1\uff09\u56de\n\nfor random_state in range(1, iter_num): # iter_num \u306e\u56de\u6570\u5206\u3001\u7e70\u308a\u8fd4\u3059\n    for i, (train_ix, test_ix) in enumerate(tqdm(gkf.split(X_train, y_train, groups))):\n\n        X_train_, y_train_, groups_train_ = X_train.iloc[train_ix], y_train.iloc[train_ix], groups[train_ix]\n        X_val, y_val, groups_val = X_train.iloc[test_ix], y_train.iloc[test_ix], groups[test_ix]\n\n        lgbm_clf = LGBMClassifier(objective='binary',\n                             n_estimators=10000, \n                             boosting_type='gbdt',\n                             importance_type='gain',\n                             class_weight='balanced',\n                             learning_rate=0.05,\n                             max_depth=10,\n                             num_leaves=31,\n                             subsample=0.99,\n                             colsample_bytree=0.70,\n                             min_child_samples=20,\n                             lambda_l1=0.91,\n                             lambda_l2=0.61,\n                             random_state=random_state, \n                             silent = 1,\n                             n_jobs=-1)\n\n        lgbm_clf.fit(X_train_, y_train_, early_stopping_rounds=20, eval_metric='auc', verbose=300, eval_set=[(X_val, y_val)])\n    \n        y_pred = lgbm_clf.predict_proba(X_val)[:,1]\n        score2 = roc_auc_score(y_train_, lgbm_clf.predict_proba(X_train_)[:,1])\n        scores2.append(score2)\n        score = roc_auc_score(y_val, y_pred)\n        scores.append(score)\n\n        print('train CV Score of Fold_%d is %f' % (i, score2))\n        print('---------------------------------------------------')\n        print('val CV Score of Fold_%d is %f' % (i, score))\n        print('---------------------------------------------------')\n        print('diff', score - score2)\n        print('===================================================')\n    \n        # \u4e88\u6e2c\u78ba\u7387\u306e\u5e73\u5747\u5024\u3092\u6c42\u3081\u308b\n        y_pred_dbdt_avg += lgbm_clf.predict_proba(X_test)[:,1]\n\ny_pred_dbdt_avg = y_pred_dbdt_avg \/ (num_split * (iter_num - 1))\n\nprint('Average')\nprint('val avg:', np.mean(scores))\nprint(scores)\nprint('=============================')\nprint('train avg:', np.mean(scores2))\nprint(scores2)\nprint('=============================')\nprint('diff',np.mean(scores) - np.mean(scores2))\n","083bd7d0":"# # LightGBM SKF\n\n# y_pred_dbdt_avg2 = np.zeros(len(X_test))\n# num_split = 5\n\n# skf = StratifiedKFold(n_splits=num_split, random_state=71, shuffle=True)\n# scores = []\n# scores2 = []\n\n# iter_num = 6 # seed averaging\u3067\u7e70\u308a\u8fd4\u3059\u56de\u6570\u3092\u8a2d\u5b9a\u3002\u5b9f\u969b\u306b\u306f\u3001\uff08iter_num-1\uff09\u56de\n\n# for random_state in range(1, iter_num): # iter_num \u306e\u56de\u6570\u5206\u3001\u7e70\u308a\u8fd4\u3059\n#     for i, (train_ix, test_ix) in enumerate(tqdm(skf.split(X_train, y_train))):\n\n#         X_train_, y_train_ = X_train.values[train_ix], y_train.values[train_ix]\n#         X_val, y_val = X_train.values[test_ix], y_train.values[test_ix]\n\n#         lgbm_clf = LGBMClassifier(objective='binary',\n#                              n_estimators=10000, \n#                              boosting_type='gbdt',\n#                              importance_type='gain',\n#                              class_weight='balanced',\n#                              learning_rate=0.05,\n#                              max_depth=10,\n#                              num_leaves=31,\n#                              subsample=0.99,\n#                              colsample_bytree=0.70,\n#                              min_child_samples=20,\n#                              lambda_l1=0.91,\n#                              lambda_l2=0.61,\n#                              random_state=random_state, \n#                              silent = 1,\n#                              n_jobs=-1)\n\n#         lgbm_clf.fit(X_train_, y_train_, early_stopping_rounds=20, eval_metric='auc', verbose=300, eval_set=[(X_val, y_val)])\n    \n#         y_pred = lgbm_clf.predict_proba(X_val)[:,1]\n#         score2 = roc_auc_score(y_train_, lgbm_clf.predict_proba(X_train_)[:,1])\n#         scores2.append(score2)\n#         score = roc_auc_score(y_val, y_pred)\n#         scores.append(score)\n\n#         print('train CV Score of Fold_%d is %f' % (i, score2))\n#         print('---------------------------------------------------')\n#         print('val CV Score of Fold_%d is %f' % (i, score))\n#         print('---------------------------------------------------')\n#         print('diff', score - score2)\n#         print('===================================================')\n    \n#         # \u4e88\u6e2c\u78ba\u7387\u306e\u5e73\u5747\u5024\u3092\u6c42\u3081\u308b\n#         y_pred_dbdt_avg2 += lgbm_clf.predict_proba(X_test)[:,1]\n\n# y_pred_dbdt_avg2 = y_pred_dbdt_avg2 \/ (num_split * (iter_num - 1))\n\n# print('Average')\n# print('val avg:', np.mean(scores))\n# print(scores)\n# print('=============================')\n# print('train avg:', np.mean(scores2))\n# print(scores2)\n# print('=============================')\n# print('diff',np.mean(scores) - np.mean(scores2))\n","6e23022d":"# Plot feature importance\n# Initialize an empty array to hold feature importances\nfeature_importances = np.zeros(X_train.shape[1])\n\nimportances = lgbm_clf.feature_importances_\n\nindices = np.argsort(importances)[::-1]\n\nfeat_labels = X_train.columns[0:]\nfor f in range(X_train.shape[1]):\n    IMPORTANCES_LIST = pd.DataFrame([[\"%2d) %-*s %f\" % (f +1, 30, feat_labels[indices[f]], importances[indices[f]])]] )\n#    IMPORTANCES_LIST.to_csv('.\/data\/feature_importances.csv', mode='a', header=False, index=False)\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))","129f6e8d":"import lightgbm as lgb\nfig, ax = plt.subplots(figsize=(5, 8))\nlgb.plot_importance(lgbm_clf, max_num_features=100, ax=ax, importance_type='gain')","29757397":"# TargetEncording\u5f8c\u306e\u6b20\u640d\u5024\u88dc\u5b8c\nX_train.fillna(0, axis=0, inplace=True)\nX_test.fillna(0, axis=0, inplace=True)","110f9b7c":"# X_train = X_train.drop(['emp_title'], axis=1)\n# X_test = X_test.drop(['emp_title'], axis=1)","b3bcf0e6":"fig=plt.figure(figsize=[20,60])\n\ntarget = 'loan_condition'\n\ni = 0\nfor col in X_train.columns:\n    i=i+1\n\n    plt.subplots_adjust(wspace=0.2, hspace=0.8)\n    ax_name = fig.add_subplot(90,2,i)\n    ax_name.hist(X_train[col],bins=30,density=True, alpha=0.5,color = 'r')\n    ax_name.hist(X_test[col],bins=30,density=True, alpha=0.5, color = 'b')\n    ax_name.set_title(col)","2ee768a0":"col = X_train.columns\n\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train[col] = scaler.transform(X_train[col])\nX_test[col] = scaler.transform(X_test[col])","c0dd7726":"X_train","33ea229b":"# # GradientBoost\n# from sklearn.model_selection import train_test_split\n\n# X_train_, X_val, y_train_, y_val = train_test_split(X_train,\n#                                                     y_train,\n#                                                     test_size=0.25,\n#                                                     shuffle=False,\n#                                                     random_state=1)\n\n# gbc_clf = GradientBoostingClassifier(n_estimators=90, learning_rate=0.25, random_state=1)\n\n# gbc_clf.fit(X_train_, y_train_)\n# y_pred_gbc = gbc_clf.predict_proba(X_val)[:,1]\n# score2 = roc_auc_score(y_train_, gbc_clf.predict_proba(X_train_)[:,1])\n# score = roc_auc_score(y_val, y_pred_gbc)\n\n# y_pred_gbc_avg = gbc_clf.predict_proba(X_test)[:,1]\n\n# print('train Score:', score2)\n# print('---------------------------------------------------')\n# print('val Score:', score)\n# print('---------------------------------------------------')\n# print('diff', score - score2)\n","54974a9d":"from tensorflow.keras.layers import Dense ,Dropout, BatchNormalization, Input, Embedding, SpatialDropout1D, Reshape, Concatenate, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom keras import optimizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.metrics import AUC\nfrom sklearn.preprocessing import StandardScaler\n\n# Import Keras packages\nfrom keras.models import Sequential, load_model\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras import regularizers\n","aee4f6ca":"# from sklearn.model_selection import train_test_split\n# X_train_, X_val, y_train_, y_val = train_test_split(X_train,\n#                                                     y_train,\n#                                                     test_size=0.3,\n#                                                     shuffle=False,\n#                                                     random_state=1)","67de4cdc":"# create regression model\nfrom keras import regularizers\nweight_decay = 0.01\n\nmodel = Sequential()\n    \nmodel.add(Dense(64, input_dim=X_train.shape[1], kernel_initializer='he_normal'))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(32, kernel_initializer='he_normal'))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(16, kernel_initializer='he_normal'))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(8, kernel_initializer='he_normal'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(1, activation='sigmoid'))\n    \n\n# compile model\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=[AUC()])\n ","e83cc5fc":"# Learning\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n#callback_es = EarlyStopping(monitor='val_loss', patience=10)\ncallback_es = EarlyStopping(monitor='val_loss', patience=20)\n#callback_op = ModelCheckpoint(filepath='weights.{epoch:02d}.hdf5')\n\nhistory = model.fit(X_train, y_train, epochs=150, batch_size=128, validation_split=0.50, verbose=1, callbacks=[callback_es])\n","96a6908f":"y_pred_keras = model.predict(X_test).reshape(-1,)","9eddac27":"## [test]\n# y_pred_keras\n# (y_pred_keras + y_pred_keras)\/2","48a9190d":"# LogisticRegression SKF\n\ny_pred_lr_avg = np.zeros(len(X_test))\nnum_split = 5\n\nskf = StratifiedKFold(n_splits=num_split, random_state=71, shuffle=True)\nscores = []\nscores2 = []\n\niter_num = 6 # seed averaging\u3067\u7e70\u308a\u8fd4\u3059\u56de\u6570\u3092\u8a2d\u5b9a\u3002\u5b9f\u969b\u306b\u306f\u3001\uff08iter_num-1\uff09\u56de\n\nfor random_state in range(1, iter_num): # iter_num \u306e\u56de\u6570\u5206\u3001\u7e70\u308a\u8fd4\u3059\n    for i, (train_ix, test_ix) in enumerate(tqdm(skf.split(X_train, y_train))):\n\n        X_train_, y_train_ = X_train.values[train_ix], y_train.values[train_ix]\n        X_val, y_val = X_train.values[test_ix], y_train.values[test_ix]\n\n        lr_clf = LogisticRegression(l1_ratio=0.9, class_weight='balanced', random_state=random_state)\n\n        lr_clf.fit(X_train_, y_train_)\n    \n        y_pred = lr_clf.predict_proba(X_val)[:,1]\n        score2 = roc_auc_score(y_train_, lr_clf.predict_proba(X_train_)[:,1])\n        scores2.append(score2)\n        score = roc_auc_score(y_val, y_pred)\n        scores.append(score)\n\n        print('train CV Score of Fold_%d is %f' % (i, score2))\n        print('---------------------------------------------------')\n        print('val CV Score of Fold_%d is %f' % (i, score))\n        print('---------------------------------------------------')\n        print('diff', score - score2)\n        print('===================================================')\n    \n        # \u4e88\u6e2c\u78ba\u7387\u306e\u5e73\u5747\u5024\u3092\u6c42\u3081\u308b\n        y_pred_lr_avg += lr_clf.predict_proba(X_test)[:,1]\n\ny_pred_lr_avg = y_pred_lr_avg \/ (num_split * (iter_num - 1))\n\nprint('Average')\nprint('val avg:', np.mean(scores))\nprint(scores)\nprint('=============================')\nprint('train avg:', np.mean(scores2))\nprint(scores2)\nprint('=============================')\nprint('diff',np.mean(scores) - np.mean(scores2))\n","36efc7f8":"# TEXT \u7279\u5fb4\u91cf\u306e\u8ffd\u52a0\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=10000, analyzer='word', ngram_range=(1, 2))\n\ntrain = tfidf.fit_transform(TXT_train.fillna('#'))\ntest = tfidf.transform(TXT_test.fillna('#'))\n\n#X_train = sp.sparse.hstack([X_train, train]).todense() # \u7d50\u5408\n#X_test = sp.sparse.hstack([X_test, test]).todense()\n\nX_train = sp.sparse.hstack([X_train, train])# \u7d50\u5408\nX_test = sp.sparse.hstack([X_test, test])\n\nX_train = X_train.tocsr()# \u884c\u65b9\u5411\u306e\u30b9\u30e9\u30a4\u30b9\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u5909\u63db\u3059\u308b\nX_test = X_test.tocsr()\n\n#del TXT_train, TXT_test\ngc.collect()\n","4070032c":"X_train.shape, X_test.shape","f2e1f7a0":"num_train = int(X_train.shape[0]*0.7)\n\nX_train_ = X_train[:num_train, :]\ny_train_ = y_train[:num_train]\n\nX_val = X_train[num_train:, :]\ny_val = y_train[num_train:]","73ab05e7":"# Text\u3092\u8ffd\u52a0\u3057\u305f\u30e2\u30c7\u30eb\nlgbm_clf = LGBMClassifier(objective='binary',\n                          n_estimators=10000, \n                          boosting_type='gbdt',\n                          importance_type='gain',\n                          class_weight='balanced',\n                          learning_rate=0.05,\n                          max_depth=10,\n                          num_leaves=31,\n                          subsample=0.99,\n                          colsample_bytree=0.70,\n                          min_child_samples=20,\n                          lambda_l1=0.91,\n                          lambda_l2=0.61,\n                          random_state=71, \n                          silent = 1,\n                          n_jobs=-1)\n\nlgbm_clf.fit(X_train_, y_train_, early_stopping_rounds=20, eval_metric='auc', verbose=300, eval_set=[(X_val, y_val)])\n\ny_pred_txt = lgbm_clf.predict_proba(X_val)[:,1]\n\nprint(roc_auc_score(y_val, y_pred_txt)) # \u691c\u5b9a\u30b9\u30b3\u30a2\n\ny_pred_dbdt_txt = lgbm_clf.predict_proba(X_test)[:,1]","c0969a40":"# # \u5168\u30c7\u30fc\u30bf\u3067\u518d\u5b66\u7fd2\u3057\u3001test\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3059\u308b: average\u3092\u3068\u308b\u306e\u3067\u4e0d\u8981\n# clf.fit(X_train, y_train)\n\n# y_pred = clf.predict_proba(X_test)[:,1]\n\n\n# # sample submission\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001\u4e88\u6e2c\u5024\u3092\u4ee3\u5165\u306e\u5f8c\u3001\u4fdd\u5b58\u3059\u308b\n# submission = pd.read_csv('\/kaggle\/input\/homework-for-students4plus\/sample_submission.csv', index_col=0)\n\n# submission.loan_condition = pred_test\n# submission.to_csv('.\/submission.csv')\n\n# submission.head()","2197dae6":"#pred_test = (1*y_pred_dbdt_avg + 0*y_pred_dbdt_txt+ 0*y_pred_lr_avg+ 0*y_pred_keras) #0.70088\npred_test = (0.50*y_pred_dbdt_avg + 0.10*y_pred_dbdt_txt+ 0.20*y_pred_lr_avg+ 0.20*y_pred_keras)\n","d8f08d42":"pred_test","24c911ea":"submission = pd.read_csv('\/kaggle\/input\/homework-for-students4plus\/sample_submission.csv', index_col=0)\n\nsubmission.loan_condition = pred_test\nsubmission.to_csv('submission.csv')","77ea66dd":"# Ordinal Encoding","346e664d":"# \u4e0d\u8981\u3068\u601d\u308f\u308c\u308b\u5217\u306e\u524a\u9664","c7338832":"# LogisticRegression","ccf54634":"# Keras","36002177":"# Feature Importance","95496913":"# \u6b20\u640d\u5024\u306e\u88dc\u5b8c","e038e9bd":"# \u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","6393ecf3":"# LightGBM GroupKFold","1883287c":"# X \u3068 y \u306e\u5206\u96e2","eb99da33":"# \u30ab\u30a6\u30f3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","75aa37e8":"# \u4e88\u6e2c\u5024\u306e\u7b97\u51fa","6eba8391":"# \u30e9\u30f3\u30af\u30ac\u30a6\u30b9","e3748978":"# \u30c6\u30ad\u30b9\u30c8\u306e\u8ffd\u52a0","c8bbbe9d":"# \u91cd\u8981\u5ea6\u306e\u9ad8\u3044\u7279\u5fb4\u91cf\u3092\u5229\u7528\u3057\u3066\u306e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","ea27802e":"# \u30e2\u30c7\u30ea\u30f3\u30b0"}}