{"cell_type":{"aa1c23f1":"code","e963573a":"code","69b6a17f":"code","25c05885":"code","cdd346b6":"code","b28f13c0":"code","f34f071f":"code","1c7d9a7b":"code","812c92ce":"code","04821a2d":"markdown","35616b18":"markdown"},"source":{"aa1c23f1":"# Load libraries\nimport numpy as np\nimport pandas as pd\nimport gc\nimport datetime\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn import preprocessing\n# Params\nNFOLD = 5\nDATA_PATH = '..\/input\/'","e963573a":"%%time\ntrain_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\ndel train_transaction, train_identity, test_transaction, test_identity\n\n\n# Label Encoding\nfor f in test.columns:\n    if train[f].dtype=='object' or test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))\n        \nprint(train.shape)\nprint(test.shape)","69b6a17f":"%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","25c05885":"train.drop('isFraud', axis=1,inplace=True)\n# Mark train as 1, test as 0\ntrain['target'] = 1\ntest['target'] = 0\n\n# Concat dataframes\nn_train = train.shape[0]\ndf = pd.concat([train, test], axis = 0)\ndel train, test\ngc.collect()","cdd346b6":"df.head()","b28f13c0":"# Remove columns with only one value in our training set\npredictors = list(df.columns.difference(['target']))\ndf_train = df.iloc[:n_train].copy()\ncols_to_remove = [c for c in predictors if df_train[c].nunique() == 1]\ndf.drop(cols_to_remove, axis=1, inplace=True)\n\n# Update column names\npredictors = list(df.columns.difference(['target']))\n\n# Get some basic meta features\ndf['cols_mean'] = df[predictors].replace(0, np.NaN).mean(axis=1)\ndf['cols_count'] = df[predictors].replace(0, np.NaN).count(axis=1)\ndf['cols_sum'] = df[predictors].replace(0, np.NaN).sum(axis=1)\ndf['cols_std'] = df[predictors].replace(0, np.NaN).std(axis=1)","f34f071f":"# Prepare for training\n\n# Shuffle dataset\ndf = df.iloc[np.random.permutation(len(df))]\ndf.reset_index(drop = True, inplace = True)\n\n# Get target column name\ntarget = 'target'\n\n# lgb params\nlgb_params = {\n        'boosting': 'gbdt',\n        'application': 'binary',\n        'metric': 'auc', \n        'learning_rate': 0.1,\n        'num_leaves': 32,\n        'max_depth': 8,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 5,\n        'feature_fraction': 0.7,\n}\n\n# Get folds for k-fold CV\nfolds = KFold(n_splits = NFOLD, shuffle = True, random_state = 0)\nfold = folds.split(df)\n    \neval_score = 0\nn_estimators = 0\neval_preds = np.zeros(df.shape[0])","1c7d9a7b":"# Run LightGBM for each fold\nfor i, (train_index, test_index) in enumerate(fold):\n    print( \"\\n[{}] Fold {} of {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), i+1, NFOLD))\n    train_X, valid_X = df[predictors].values[train_index], df[predictors].values[test_index]\n    train_y, valid_y = df[target].values[train_index], df[target].values[test_index]\n\n    dtrain = lgb.Dataset(train_X, label = train_y,\n                          feature_name = list(predictors)\n                          )\n    dvalid = lgb.Dataset(valid_X, label = valid_y,\n                          feature_name = list(predictors)\n                          )\n        \n    eval_results = {}\n    \n    bst = lgb.train(lgb_params, \n                         dtrain, \n                         valid_sets = [dtrain, dvalid], \n                         valid_names = ['train', 'valid'], \n                         evals_result = eval_results, \n                         num_boost_round = 5000,\n                         early_stopping_rounds = 100,\n                         verbose_eval = 100)\n    \n    print(\"\\nRounds:\", bst.best_iteration)\n    print(\"AUC: \", eval_results['valid']['auc'][bst.best_iteration-1])\n\n    n_estimators += bst.best_iteration\n    eval_score += eval_results['valid']['auc'][bst.best_iteration-1]\n   \n    eval_preds[test_index] += bst.predict(valid_X, num_iteration = bst.best_iteration)\n    \nn_estimators = int(round(n_estimators\/NFOLD,0))\neval_score = round(eval_score\/NFOLD,6)\n\nprint(\"\\nModel Report\")\nprint(\"Rounds: \", n_estimators)\nprint(\"AUC: \", eval_score)    ","812c92ce":"# Feature importance\nlgb.plot_importance(bst, max_num_features = 20)","04821a2d":"As we can see, the separation is almost perfect - which strongly suggests that the train \/ test rows are very easy to distinguish. **Meaning the distribution of Train and Test is not the same**","35616b18":"For more details :\nhttp:\/\/fastml.com\/adversarial-validation-part-one\/\nhttps:\/\/www.kaggle.com\/konradb\/adversarial-validation-and-other-scary-terms\nhttps:\/\/www.kaggle.com\/ogrellier\/adversarial-validation-and-lb-shakeup\n\nBasic data processing from:\nhttps:\/\/www.kaggle.com\/artkulak\/ieee-fraud-simple-baseline-0-9383-lb\n\n**Some code snippet below is from other past competition kernels , but now I don't remember the owner, if you are the one please mention in comment and I will add your credit here :)**"}}