{"cell_type":{"3c24c2f4":"code","4bc0760b":"code","f1164730":"code","63828883":"code","f1623696":"code","daeb6d39":"code","4261f942":"code","7ff8374e":"code","4e88393e":"code","3e8b83fe":"code","526ed03d":"code","8f2accee":"code","c421654d":"code","4ab3fa06":"code","2022a17e":"code","9d9f0f9d":"code","f1d7e261":"code","be58517e":"code","8f6bc205":"code","e9b9e70f":"code","8dd3ef39":"code","62b7c474":"code","11c8e8aa":"code","52b9242f":"code","69cf30da":"code","87a29e90":"code","8bc0c936":"code","f0e5ba75":"code","77652304":"code","d664ce12":"code","967178d7":"code","efa56cd9":"code","f9a510f1":"code","1fd2ff8e":"code","9b298620":"code","574b0971":"code","f932774c":"code","6190156b":"code","5218bfd1":"code","ab80b766":"code","21f3bec6":"code","e0f679bf":"code","c62bef68":"code","f2f047eb":"code","f12afb3d":"code","51630614":"code","dc3e46df":"code","441a6f2c":"code","1f694719":"code","bdcd495f":"code","056c164d":"code","13200ca9":"code","2b1793e2":"code","cf3368e5":"code","4dc8f4c4":"code","f7f2f119":"code","92f9d287":"code","6d05849b":"code","983da1a4":"code","ba38f4b6":"code","3991c7d0":"code","3051053e":"markdown","984c3a00":"markdown","8e48011c":"markdown","c9b005c6":"markdown","13f44e83":"markdown","36dd580b":"markdown","1c1862c5":"markdown","ada204d2":"markdown","bb15254c":"markdown","997fa7e8":"markdown","37ce868f":"markdown","85c56f63":"markdown","c21cd271":"markdown","bc17b150":"markdown","f02da487":"markdown","2a467746":"markdown","a975b0bd":"markdown","e7e0d83b":"markdown","d6f82513":"markdown","491388f7":"markdown","60b2909a":"markdown","6f93b12d":"markdown","703dbe6e":"markdown","bcb5f26b":"markdown","da8f5500":"markdown","b133c149":"markdown","a6ac02fd":"markdown","7c6f052f":"markdown","0a03a5fb":"markdown","50dbf7f8":"markdown","64ff7d7a":"markdown","2efe64a2":"markdown","d3acec25":"markdown","8a6018a1":"markdown","1cec84f8":"markdown","696c97aa":"markdown","069ca873":"markdown","e4569adc":"markdown","962be201":"markdown","dd561df9":"markdown","ee42602e":"markdown","14b25fa4":"markdown","92b221b5":"markdown","d63779f0":"markdown","a4bd232d":"markdown","b330a0a5":"markdown","bc5b522e":"markdown","59fb311f":"markdown","e815c3f4":"markdown","6fb37031":"markdown","a6b13628":"markdown","6be993a7":"markdown","71bfae67":"markdown","91f8d7eb":"markdown","abd29f72":"markdown","347ddf1f":"markdown","913128b8":"markdown","162bc114":"markdown","1de25243":"markdown","e32e0bc1":"markdown","e674cdaf":"markdown","9c6b1403":"markdown","d2bdbe91":"markdown","30539fae":"markdown"},"source":{"3c24c2f4":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\nimport os","4bc0760b":"# set working directory to input folder\npath = \"\/kaggle\/input\/m5-forecasting-accuracy\/\"\ntry:\n    os.chdir(path)\n    print(\"Directory changed to:\", path)\nexcept OSError:\n    print(\"Can't change the Current Working Directory\")\n    \n# import data\nsell_prices = pd.read_csv(\"sell_prices.csv\")\ncalendar = pd.read_csv('calendar.csv')\nsales = pd.read_csv('sales_train_validation.csv')\nsample_submission = pd.read_csv('sample_submission.csv')\n\nprint(\"Data loaded\")","f1164730":"sample_submission.head()","63828883":"validation_prop = len(sample_submission[sample_submission['id'].str.contains('evaluation')]) \/ len(sample_submission)\n\nprint(\"Proportion of validation rows in sample_submission.csv:\", validation_prop)","f1623696":"# gather a bunch of dates for timeline\nsell_prices_cal = pd.merge(sell_prices, calendar, how = 'left', on = 'wm_yr_wk')\n\n\ncalendar_startdate = calendar.date.min()\ncalendar_enddate = calendar.date.max()\nsales_train_validation_startdate = calendar[calendar.d == 'd_1'].date.item()\nsales_train_validation_enddate = calendar[calendar.d == 'd_1913'].date.item()\nsubmission_validation_startdate = calendar[calendar.d == 'd_1914'].date.item()\nsubmission_validation_enddate = calendar[calendar.d == 'd_1941'].date.item()\nsubmission_evaluation_startdate = calendar[calendar.d == 'd_1942'].date.item()\nsubmission_evaluation_enddate = calendar[calendar.d == 'd_1969'].date.item()\nsell_price_startdate = sell_prices_cal.date.min()\nsell_price_enddate = sell_prices_cal.date.max()\n\ndel sell_prices_cal","daeb6d39":"import plotly.figure_factory as ff\n\ndf = [dict(Task=\"Sell Prices\", Start = sell_price_startdate, Finish = sell_price_enddate),\n      dict(Task=\"Calendar\", Start = calendar_startdate, Finish = calendar_enddate),\n      dict(Task=\"Sales train validation\", Start = sales_train_validation_startdate, Finish = sales_train_validation_enddate),\n      dict(Task=\"Submission validation\", Start = submission_validation_startdate, Finish = submission_validation_enddate),\n      dict(Task=\"Submission evaluation\", Start = submission_evaluation_startdate, Finish = submission_evaluation_enddate)]\n\nfig = ff.create_gantt(df)\nfig.show()","4261f942":"sell_prices.head()","7ff8374e":"sell_prices['store_id'].value_counts()","4e88393e":"# using vectorized str.split will be much faster than using apply here\nsell_prices['state'] =  sell_prices['store_id'].str.split('_').str[0]","3e8b83fe":"sell_prices['state'].unique().tolist()","526ed03d":"sell_prices['product_cat'] =  sell_prices['item_id'].str.split('_').str[0]\nsell_prices['product_cat'].unique().tolist()","8f2accee":"from statsmodels.graphics.mosaicplot import mosaic\n\nmosaic(sell_prices, ['state', 'product_cat'])\nplt.show()","c421654d":"sell_prices['state'].value_counts().plot(kind = 'bar')\nplt.title('Number of rows by State')\nplt.show()","4ab3fa06":"sell_prices['store_id'].value_counts()\n","2022a17e":"sell_prices.groupby(['state'])['sell_price'].mean()","9d9f0f9d":"sell_prices.groupby(['product_cat'])['sell_price'].mean()","f1d7e261":"plt.figure(figsize=(18,9)) # ah.. the sweet 18 by 9 ratio\n\nsns.lineplot(x = 'wm_yr_wk', y = 'sell_price', hue = 'product_cat', data = sell_prices)\nplt.title(\"Sell Price of Product Categories over time\")\nplt.show()","be58517e":"plt.figure(figsize=(18,9))\n\nsns.lineplot(x = 'wm_yr_wk', y = 'sell_price', hue = 'state', data = sell_prices)\nplt.title(\"Sell Price of Product Categories over time\")\nplt.show()","8f6bc205":"calendar.head()","e9b9e70f":"# duplicate check\ncalendar['d'].duplicated().sum()","8dd3ef39":"# convert date column (str) to date_time object\ncalendar['date'] = pd.to_datetime(calendar['date'])\n\n# date range\ndisplay(max(calendar['date']) - min(calendar['date']))\n\n# last d value\ndisplay(calendar['d'].tail(1))","62b7c474":"display(calendar['event_name_1'].value_counts())\ndisplay(calendar['event_name_2'].value_counts())","11c8e8aa":"plt.figure(figsize=(18,9))\nsns.heatmap(calendar.isnull(), cbar = False)\nplt.show()","52b9242f":"# before filling NaNs, better check if other columns contains any NaNs\ncalendar.isnull().sum(axis = 0)","69cf30da":"calendar = calendar.fillna(\"Normal\")\ncalendar.head()","87a29e90":"events_weekly = calendar[calendar['event_type_1'] != 'Normal'][['wm_yr_wk', 'event_name_1', 'event_type_1']]\ndisplay(events_weekly)\nprint(\"Number of week duplicates:\", events_weekly['wm_yr_wk'].duplicated().sum())","8bc0c936":"# \"Some of You May Die, but that is a Sacrifice I'm Willing to Make\"\nevents_weekly = events_weekly.drop_duplicates(subset = 'wm_yr_wk', keep = 'first')","f0e5ba75":"# merge + fill NaNs\nprice_with_event = pd.merge(sell_prices, events_weekly, how = 'left', on = ['wm_yr_wk']).fillna('Normal')\n\n# new column denotes event\nprice_with_event['event'] = np.where(price_with_event['event_type_1'] == 'Normal', 'Normal', 'Event')\nprice_with_event.head()","77652304":"plt.figure(figsize=(18,9))\n\nsns.relplot(x = 'wm_yr_wk', y = 'sell_price', \n            hue = 'event_type_1', style = 'event_type_1', row = 'product_cat', \n            height = 4, # make the plot 4 units high\n            aspect = 3, # height should be three times width\n            kind = 'line',data = price_with_event, ci = None)  # remove confident interval for better clarity\n\nplt.title(\"Sell Price of Product Categories over time\")\nplt.show()","d664ce12":"sns.boxplot(x = 'event_type_1', y = 'sell_price', data = price_with_event)\nplt.show()","967178d7":"temp_df = price_with_event.groupby(['item_id', 'event'])['sell_price'].min().unstack()\ntemp_df.head()","efa56cd9":"temp_df['event_delta'] = temp_df['Event'] - temp_df['Normal']\ntemp_df.head()","f9a510f1":"sum(temp_df['event_delta'] < 0) \/ len(temp_df)","1fd2ff8e":"temp_df['discount_prop'] = temp_df['event_delta'] \/ temp_df['Normal']\ntemp_df","9b298620":"temp_df.sort_values(by=['discount_prop']).head(10)","574b0971":"# reset the index to get the column item_id back\ntemp_df.reset_index(inplace = True)\n\n# get item department from item_id\ntemp_df['department'] = temp_df['item_id'].str.split('_').str[:2].apply(lambda x: '_'.join(x))\ntemp_df.head()\n","f932774c":"temp_df[temp_df['discount_prop'] < 0].groupby(['department'])['discount_prop'].count().sort_values(ascending = False).plot(kind = 'bar')\nplt.show()","6190156b":"sales.head()","5218bfd1":"sales_long = pd.melt(sales, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'd', value_name = 'unit_sold')\nsales_long.head()","ab80b766":"sales_cat = sales_long.groupby(['d', 'cat_id'])['unit_sold'].sum().unstack()\nsales_cat","21f3bec6":"# reset index\nsales_cat.reset_index(inplace = True)\n\n# melt the cat\nsales_cat = pd.melt(sales_cat, id_vars =['d'], var_name = 'product_cat', value_name = 'unit_sold')\n\n# merge with calendar to get datetime\nsales_cat = pd.merge(sales_cat, calendar[['date', 'd']], how = 'left', on = 'd')\n\nsales_cat","e0f679bf":"plt.figure(figsize=(18,9))\n\nsns.lineplot(x = 'date', y = 'unit_sold', hue = 'product_cat', data = sales_cat)\nplt.title(\"Units Sold for each Product Category\")\nplt.show()","c62bef68":"sales_cat[sales_cat['unit_sold'] < 500]","f2f047eb":"sales_univariate = sales_long.groupby(['d'])['unit_sold'].sum().reset_index()\n\n# merge with calendar to get datetime\nsales_univariate = pd.merge(sales_univariate, calendar[['date', 'd', 'weekday', 'month', 'year']], how = 'left', on = 'd')\n\nsales_univariate.head()","f12afb3d":"sales_univariate.groupby(['weekday'])['unit_sold'].sum().plot(kind = 'bar')\nplt.title(\"Number of Unit Sold by Weekday\")\nplt.show()","51630614":"sales_univariate.groupby(['month'])['unit_sold'].sum().plot(kind = 'line')\nplt.title(\"Number of Unit Sold by Month\")\nplt.show()","dc3e46df":"sales_univariate.groupby(['year'])['unit_sold'].sum().plot(kind = 'line')\nplt.title(\"Number of Unit Sold by Year\")\nplt.show()","441a6f2c":"from statsmodels.tsa.seasonal import seasonal_decompose\n\n# set date as index\nsales_univariate = sales_univariate.set_index('date')\n\n# get the unit_sold series\nunit_sold_series = sales_univariate['unit_sold'].sort_index()","1f694719":"plt.figure(figsize = (16,10))\n# period is the number of observations in a series if you consider the natural time interval of measurement (weekly, monthly, yearly)\ndecomposition = seasonal_decompose(unit_sold_series, model='multiplicative',period= 365) \ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(unit_sold_series, label = 'Original')\nplt.legend(loc = 'best')\nplt.subplot(412)\nplt.plot(trend, label = 'Trend')\nplt.legend(loc = 'best')\nplt.subplot(413)\nplt.plot(seasonal, label = 'Seasonal')\nplt.legend(loc = 'best')\nplt.subplot(414)\nplt.plot(residual, label = 'Residuals')\nplt.legend(loc = 'best')\nplt.tight_layout()","bdcd495f":"holiday_lists = calendar[(calendar['date'].dt.year == 2013) & (calendar['event_name_1'] != 'Normal')][['date', 'event_name_1']]\nholiday_lists = pd.merge(holiday_lists, seasonal.reset_index())\nholiday_lists['dayofyear'] = holiday_lists.date.dt.dayofyear\n\nplt.figure(figsize = (16,10))\nsns.lineplot(x = seasonal.index.dayofyear, y = seasonal)\np1 = sns.scatterplot(x = holiday_lists.date.dt.dayofyear, y = holiday_lists.seasonal, color = 'red')\n\n# add holoday labels\nfor line in range(0,holiday_lists.shape[0]):\n     p1.text(holiday_lists.dayofyear[line]+0.2, holiday_lists.seasonal[line], holiday_lists.event_name_1[line], horizontalalignment='left', size='medium', color='black')\nplt.title('Average Impact of Seasonal and Events on Sales')","056c164d":"# Prepare Data\nx = holiday_lists.loc[:, ['seasonal']]\nholiday_lists['z'] = x - 1 \nholiday_lists['colors'] = ['red' if x < 0 else 'green' for x in holiday_lists['z']]\nholiday_lists.sort_values('z', inplace=True)\nholiday_lists.reset_index(inplace=True)\n\n# Draw plot\nplt.figure(figsize=(20,10), dpi= 80)\nplt.hlines(y=holiday_lists.event_name_1, xmin=0, xmax=holiday_lists.z, color = holiday_lists.colors, alpha=0.4, linewidth=5)\n\nfor x, y, tex in zip(holiday_lists.z, holiday_lists.event_name_1, holiday_lists.z):\n    t = plt.text(x, y, round(tex, 2), horizontalalignment='right' if x < 0 else 'left', \n                 verticalalignment='center', fontdict={'color':'red' if x < 0 else 'green', 'size':14})\n\n# Decorations\nplt.gca().set(ylabel='Event', xlabel='Impact on Sale')\nplt.title('Diverging Bars of Event Impact', fontdict={'size':20})\nplt.grid(linestyle='--', alpha=0.5)","13200ca9":"def item_visualizer(item_name):\n    specific_item = sales_long[sales_long['item_id'] == item_name][['item_id', 'd', 'unit_sold']]\n\n    # merge with calendar to get datetime\n    specific_item = pd.merge(specific_item, calendar[['date', 'd', 'wm_yr_wk']], how = 'left', on = 'd')\n\n    # merge with sell_prices to get pricing\n    specific_item = pd.merge(specific_item, sell_prices[['item_id', 'sell_price', 'wm_yr_wk', ]], how = 'left', on = ['item_id', 'wm_yr_wk'])\n    \n#     # random sampling to reduce plot time\n#     specific_item = specific_item.sample(frac = 0.1, random_state = 42)\n\n    plt.figure(figsize = (14,7))\n    ax1 = sns.lineplot(x = 'date', y = 'sell_price', data = specific_item, label = 'Sell Price', color = 'red', alpha = 0.8)\n    ax1.legend(loc=\"upper right\")\n\n    ax2 = plt.twinx()\n    sns.lineplot(x = 'date', y = 'unit_sold', data = specific_item, label = 'Units Sold', alpha = 0.5, ax=ax2)\n    ax2.legend(loc=\"upper left\")\n\n    plt.title('Sell Price and Sales of item: ' + item_name)\n    plt.show()","2b1793e2":"item_visualizer('FOODS_1_005')","cf3368e5":"# split into train and test sets\nunit_sold = unit_sold_series.reset_index()\n\nn_train = round(365 * 4.5) # train on 4.5 year worth of data\ntrain = unit_sold[:n_train]\ntest = unit_sold[n_train:]","4dc8f4c4":"plt.figure(figsize = (16,8))\nsns.lineplot(x = train.date, y = train.unit_sold, label = 'Train')\nsns.lineplot(x = test.date, y = test.unit_sold, label = 'Test')\nplt.title('Total Units Sold from All Malmarts')\nplt.show()","f7f2f119":"from statsmodels.tsa.api import ExponentialSmoothing, Holt\n\nprediction_holt = test.copy()\nlinear_fit = Holt(np.asarray(train['unit_sold'])).fit()\nprediction_holt['Holt_linear'] = linear_fit.forecast(len(test))\n\nplt.figure(figsize = (16,8))\nplt.plot(train.unit_sold, label = 'Train')\nplt.plot(test.unit_sold, label = 'Test')\nplt.plot(prediction_holt['Holt_linear'], label = 'Holt Linear Prediction')\nplt.legend(loc = 'best')\nplt.title('Holt Linear Trend Forcast')\nplt.show()","92f9d287":"# calculate RMSE to check the accuracy of the model:\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms = sqrt(mean_squared_error(test.unit_sold, prediction_holt.Holt_linear))\n\nprint(rms)\n","6d05849b":"prediction_holtwinter = test.copy()\nfit1 = ExponentialSmoothing(np.asarray(train['unit_sold']), seasonal_periods= 365, trend = 'add', seasonal= 'add', damped = True).fit()\nprediction_holtwinter['Holt_Winter'] = fit1.forecast(len(test))\n\nplt.figure(figsize = (16,8))\nplt.plot(train['unit_sold'], label = 'Train')\nplt.plot(test['unit_sold'], label = 'Test', alpha = 0.6)\nplt.plot(prediction_holtwinter.Holt_Winter, label = 'Holt Winters Prediction', alpha = 0.6)\nplt.legend(loc = 'best')\nplt.show()","983da1a4":"rms = sqrt(mean_squared_error(test.unit_sold, prediction_holtwinter.Holt_Winter))\n\nprint(rms)","ba38f4b6":"import statsmodels.api as sm\n\n\ntrain = unit_sold_series[:n_train]\ntest = unit_sold_series[n_train:]\n\ny_hat_avg = test.copy()\nfit1 = sm.tsa.statespace.SARIMAX(train, order=(2, 1, 4),seasonal_order=(0,1,1,12)).fit()\n\ny_hat_avg['SARIMA'] = fit1.predict(start=\"2015-07-29\", end=\"2016-04-24\", dynamic=True)\n\nplt.figure(figsize=(16,8))\nplt.plot(train, label='Train')\nplt.plot(test, label='Test')\nplt.plot(y_hat_avg['SARIMA'], label='SARIMA')\nplt.legend(loc='best')\nplt.show()","3991c7d0":"rms = sqrt(mean_squared_error(test, y_hat_avg['SARIMA']))\n\nprint(rms)","3051053e":"**Observation**: Food price is on the rise yo, my guess: inflation. Meanwhile, household items are getting cheaper. I blame China for this (just kidding).\n\nNotice the confident interval around the lines. This is because we have multiple data points for any given x. We can conclude that the pricing variation is greatest for hobby products.\n\nThere is a sudden jump in hobbies category. Perhaps a historic event? I might investigate this if week_id can be mapped to datetime. ","984c3a00":"**Do location affects the sale of different categories? (Consumer behavior: Location - Product)**\n\nSince we are dealing with cat-cat variables, a mosaic plot would be appropriate here. A mosaic plot allows visualizing multivariate categorical data in a way that allow us to compare the proportion of different categories with one another:","8e48011c":"### Seasonal ARIMA Model","c9b005c6":"**Observation**: This is a format where we can create a graph using `day` as x-axis and `unit_sold` as y-axis. Let's do some aggregation:","13f44e83":"### Sell_prices","36dd580b":"**Which items are most heavily discounted during events?**","1c1862c5":"**Interpretation**: Top 10 discounted items are food and household items. We can look at these item specifically to examine the discount effect on the sales.\n\nWe can slice the item_id string to get the product department. Let's check which department would offer most discounted items during events:\n\n","ada204d2":"** Which states were included in the dataset? **","bb15254c":"**Observations**:\n\n* `wm_yr_wk` can be mapped with `sell_price` for price\/event analysis\n\n* `d`? What is d? \n\n* `event` variables contains lots of NaNs, which make sense since events are periodical. We will need to perform missing values analysis here.\n\n* `snap`? SNAP stands for the [Supplemental Nutrition Assistance Program](https:\/\/www.feedingamerica.org\/take-action\/advocate\/federal-hunger-relief-programs\/snap). Which is food stamp program for the lower-income Americans. According to this website, \"SNAP benefits cannot be used to buy any kind of alcohol or tobacco products or any nonfood items like household supplies and vitamins and medicines.\". So this program only waives food purchases.\n","997fa7e8":"**Interpretation**: White represents the missing. `event_name` and `event_type` contain missing values in pair. We can fill NaNs with \"Normal\" date to avoid NaN values.","37ce868f":"## Visualize the sale data:","85c56f63":"## Relationship between Sell Price and Sales\n\nAs we learn previously that the item pricing is sensitive the type of item. To see pricing affect sale, I think it's best to look at a single item and observe the correlation between sell price and sales:","c21cd271":"In the submission table, the `id` variable contains \"validation\" rows and \"evaluation\" rows. Validation corresponds to the Public leaderboard, while evaluation corresponds to the Private leaderboard (the prize pool). ","bc17b150":"**Observation**: Week duplicates, yikes! Some weeks may contain more than 1 event. This may become important later on (multiple events might affect sale number), but to make my life easier, I will remove the duplicates, keeping at 1 event\/week. ","f02da487":"**Observations**:\n\nThis table is fairly simple with only 4 variables\n\n* `store_id` looks like it may contains state location, assuming CA stands for California\n\n* `item_id` may contains category of the item\n\n* `wm_yr_wk`'s meaning is unclear. So I looked it up using the [provided data descriptions](https:\/\/mofc.unic.ac.cy\/wp-content\/uploads\/2020\/03\/M5-Competitors-Guide-Final-10-March-2020.docx). This variable denotes the week id. Not sure if the number is chronological, we will examine this later.\n\n* `sell_price` This is the price is provided per week (average across seven days).","2a467746":"**Interpretation**: Food_3, Household_2 and Household_1 are departments that discount many items.","a975b0bd":"## Calendar","e7e0d83b":"**Interpretation**: About 1 in every 5 products. However, from a glance, most price decrease is pretty small compared to the total cost of the item (cents to dollars). Therefore, we need to calculate the proportion of the discount relative to the price of the item:","d6f82513":"### Holt's Linear Trend Model","491388f7":"## Sale Train Validation","60b2909a":"**Interpretation**: Boxplot also confirms that events don't directly influence the sell price. \n\nI guess lowering the price is a viable strategy to attract more customer to buy. But why would Walmart lower the price of ALL items on average during any specific event? I was foolish to even hope for any price action during events. \n\nBut perhaps they would specifically lower a small number of products that Walmart knows would attract more sale during the event? \n\nWe need to calculate the price difference between normal days and event days:","6f93b12d":"**Observation**: 3 product categories: Hobbies, Foods, and Household. Walmart is being very general with their data.","703dbe6e":"**Observation**: After some eye-balling, I would say Walmart pricing is fairly consistent across the states.","bcb5f26b":"**Which months have the highest sales?**","da8f5500":"** Does store_id contain location info? **\n\nWe will use count to see the classes frequency:","b133c149":"## Exploratory data analysis:\n\nIn order to capture my learning process in an organic manner, I would like to present the findings based on a series of questions and answers that arise from examining the data. Some questions may seem naive and some answers might seems contrived, but such is the nature of exploration. Therefore, this kernel is suitable for absolute newcomer, such as myself.\n\nSo just sit back and enjoy reading me fumbling through data. Let me know if you find something interests you.\n\n### Understand the Objective and Timelines\n\n**What are we trying to predict?**\n\nThe competition wants 28 days ahead point forecasts. Here's what the submission looks like:","a6ac02fd":"**Explaination**: This plot is the condensation of how sesonality affect the units sold through the years (2011-2016). The blue line is the average of the total unit sold at any given day of the year. The blue shadow is the area that represent the 95% confident interval of sales since we have multiple y-values for any given x-value.\n\nThe y axis represent the multiplication of seasonality effect. For example, y = 1.2 suggests a 20% increase from the baseline sale (the \"trend\" in the decomposition plot).\n\n**Observation**:\nWe can see that certain events can strongly influence the sales. Some holidays such as Christmas, New Year,Thanksgiving, and Halloween associate with trough sales, while other holidays such as SuperBowl, Labor Day, Veterans Days, etc., associate with peak sales.\n\nLet's see how each event influences the sale number:","7c6f052f":"**What is the proportion of items would go on discount during events?**","0a03a5fb":"**Observations**: This is an example item that exhibit a strong association between units sold and discount actions.\n\nFor this particular item:\n\n* Walmart drops the price when the sales is consistently low over a period of time. \n\n* As soon as the sales pick up, Walmart returns to normal or higher pricing.\n\n* The confident intervals, especially during discount periods, indicate the differences in discount amount among different Walmart locations. This results in the high variation in discounted prices.\n\n","50dbf7f8":"**Interpretation**: There is a increasing trend of sales from 2011 to 2015. The drop-off in 2016 is most likely due to the lack of data for this year.","64ff7d7a":"**Interpretation**: 0 duplicate found, suggesting this column may contains identification property, perhaps date?  ","2efe64a2":"Notive that I used min() for group aggregation. This is because I want to target the items with most significant reduction in price during events. Now we can subtract min price of event by normal price to see how much item would decrease in price:","d3acec25":"**Interpretation**: There it is! CA has 1 extra store \"CA_4\". One more compared to TX and WI.","8a6018a1":"**Interpretation**: The sales fall off the cliff on December 25th. This suggests Wamart closed during Christmas day. After some googling, this website confirms indeed that [\"Walmart will be closed on Christmas Day\"](https:\/\/heavy.com\/entertainment\/2019\/12\/walmart-hours-open-closed-christmas-eve-day-2019\/).\n\nI wonder what are the units got sold during Christmas day? Employee's last minute purchases? Foods come to life and scan themselves out the door?\n\n## Time series analysis\n\n### Univariate time series\n\nBefore adding more variables into the predictive model, we would gain a lot of information from just a single time-dependent dependency: units sold over time.","1cec84f8":"**Does item_id contain product categories info?**","696c97aa":"**Does item pricing fluctuate with time (weekly) - Grouped by product categories?**","069ca873":"### Holt Winter's Model","e4569adc":"**Interpretation**: D is the number of day elapsed since 2011-01-28.","962be201":"### Timelines","dd561df9":"**Observation**: Events are holidays, sports events, and religious significants. No special event hosted by Walmart.","ee42602e":"**Intepretation**: Only three states were included. Fortunately, these states were spreaded out in the US. Due to geographical differences, there may be differences in consumer behavior as well. We will need to keep location effect in mind.","14b25fa4":"## Relationship between Sell Price and Events\n\nLet's visualize how events affect item pricing. We will need to join sell_prices with calendar, but...\n\n* Pricing is set weekly, while events are daily. We will need to extract daily weeks that contain events before joining.\n","92b221b5":"**Explaination**: Plotting the seasonal components this way reveals the impact of event on sale. As we learn previously, the closure of Chrismas will result in reduction of ~100% in sale. However, Chismas event in this plot only suggest a 76% reduction in sale. This is because some of these losses in sale was accounted in the residuals of the decomposition. \n","d63779f0":"**Answer**: Weekends are days with highest sales.","a4bd232d":"**How about pricing trend grouped by states?**","b330a0a5":"**Answer**: The first quarter of the year has higher sales.","bc5b522e":"**Observation**: Not much new information here expect the number of unit sold. `dept_id`, `cat_id`, and `state_id` has been conveniently separated for us.\n\n* The last day is 1913, which is less than 1969 days worth of data.\n\n* This table is in the wide format, which is not optimal for plotting. I will use the melt() function, check out [this guide on wide to long guide](http:\/\/www.datasciencemadesimple.com\/reshape-wide-long-pandas-python-melt-function\/). Let's reshape this data to long:\n\nWarning: The reshaped data is pretty big (>3.5 Gb).","59fb311f":"**Observations:**\n\n* Foods category accounted for most sales; hobbies accounted for least sales\n\n* There are some troughs that are close to 0. Did they closed on those days?\n\n* There might be seasonality and trend in the time-series plot. \n\n**Did Walmart close their door?**","e815c3f4":"Something is not adding up here. There are way more CA rows than the other.","6fb37031":"### Time series decomposition:\n\nThere are two kind of decomposition model: additive and multiplicative. \n\n* Additive model is describe as:\n\ny(t) = Level + Trend + Seasonality + Noise\n\n* Multiplicative model is describe as:\n\ny(t) = Level * Trend * Seasonality * Noise\n\nIn a multiplicative time series, the components multiply together to make the time series. If you have an increasing trend, the amplitude of seasonal activity increases. This is appropriate for our sale data since the increase in number of Walmart sales would also increase the seasonal sales.","a6b13628":"## Univariate Time Series Prediction\n\n**Note**: This section my personal review on simple time series forcast with single variable (total units sold). Therefore, you can skip this section as it may not be relevant to the topic of this competition.","6be993a7":"**Is there a trend for sale throughout the years?**","71bfae67":"Interpretation: TX Walmarts have the lowest sell price on average, while WI has the highest average sell price.","91f8d7eb":"**Intepretation**: It looks like the first 2 characters denote the state. We can create another variable called `state` to explore further:","abd29f72":"**Which days of the week have the highest sales?**","347ddf1f":"**So what is d?**","913128b8":"**Interpretation**: Food is cheaper than hobby and household item, makes sense.","162bc114":"**Interpretation**: Using the primitive power of my eye-balls, I can see that sale count of each product category does not differ across different locations. An average Walmart shopper in California would spend money on Hobbies, Foods, and Household roughly the same way a Texan does.\n\n**Observation**: The area for CA is relatively larger, indicating higher number of items. Why is that?","1de25243":"**How do sale prices differ across different states?**","e32e0bc1":"**Interpretation**: The three components are shown separately in the bottom three panels of the figure. These components can be added together to reconstruct the data shown in the top panel (the original series).\n\nNow that we have extracted the seasonal component from sales, we can examine how events might affect the seasonal behaviors of sale.\n\n## Relationship between Sales and Events","e674cdaf":"**Interpretation**: I can't really tell if price would decrease when events occured. We need to confirm this with another visualization:","9c6b1403":"**Which product category has the highest average sell price?**","d2bdbe91":"Cool bean! Now let's draw:","30539fae":"**Handling missing data:**\n"}}