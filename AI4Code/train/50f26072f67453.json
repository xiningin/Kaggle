{"cell_type":{"7449d406":"code","e6e5ccf1":"code","ab4b01bd":"code","3b159e9d":"code","e398c8ed":"code","d928393e":"code","e9267d70":"code","3259ae90":"code","bd32bd67":"code","fff4222c":"code","a5550909":"code","7794fc26":"code","30d5f1ba":"code","7eca3bbe":"code","268f5868":"code","c34e279d":"code","d2a4de25":"code","339acca0":"code","eb9d04e5":"code","8c444232":"code","cd94ef24":"code","879cd4f6":"code","949bb8ab":"code","2d06d0fc":"code","1460d740":"code","0005e2a0":"code","3ebb71f8":"code","2e51fa5c":"code","eb106ec6":"code","8feff31e":"code","ce2c96e6":"code","87915591":"code","0eabf2ae":"code","aa506014":"code","ec3d78b2":"code","e3711b14":"code","a561bfcc":"code","ddf7190e":"markdown","7aa56e08":"markdown","53739e7e":"markdown","bcbe565d":"markdown","4b7f9b61":"markdown","3129cf94":"markdown","8d8f9669":"markdown","4e9ac56d":"markdown","c519a77c":"markdown","6b0f557e":"markdown","27990a39":"markdown","7842bc70":"markdown"},"source":{"7449d406":"!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm","e6e5ccf1":"!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM","ab4b01bd":"!apt-get install -y -qq libboost-all-dev","3b159e9d":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","e398c8ed":"!cd LightGBM\/python-package\/;python3 setup.py install --precompile","d928393e":"!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","e9267d70":"# Latest Pandas version\n!pip install -q 'pandas==0.25' --force-reinstall","3259ae90":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import KFold, StratifiedKFold,TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nimport os\nprint(os.listdir(\"..\/input\"))\nimport warnings\nimport datetime\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')","bd32bd67":"print(\"Pandas version:\", pd.__version__)","fff4222c":"import warnings\nwarnings.filterwarnings(\"ignore\")","a5550909":"import gc\ngc.enable()","7794fc26":"import lightgbm as lgb\nprint(\"LightGBM version:\", lgb.__version__)","30d5f1ba":"from sklearn.preprocessing import LabelEncoder","7eca3bbe":"train_transaction = pd.read_csv('..\/input\/train_transaction.csv')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv')\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')","268f5868":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)","c34e279d":"train.head()","d2a4de25":"predictions = test[['TransactionID_x']]","339acca0":"#Based on this great kernel https:\/\/www.kaggle.com\/arjanso\/reducing-dataframe-memory-size-by-65\n#credits : https:\/\/www.kaggle.com\/mjbahmani\/reducing-memory-size-for-ieee\ndef reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings            \n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()\n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n                NAlist.append(col)\n                df[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)    \n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return df, NAlist","eb9d04e5":"train, NAlist = reduce_mem_usage(train)\nprint(\"_________________\")\nprint(\"\")\nprint(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\nprint(\"_________________\")\nprint(\"\")\nprint(NAlist)","8c444232":"test, NAlist = reduce_mem_usage(test)\nprint(\"_________________\")\nprint(\"\")\nprint(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\nprint(\"_________________\")\nprint(\"\")\nprint(NAlist)","cd94ef24":"train = train.sort_values('TransactionID_x')","879cd4f6":"train = train.set_index('TransactionID_x')\ntest = test.set_index('TransactionID_x')","949bb8ab":"train.head()","2d06d0fc":"y_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\ngc.collect()","1460d740":"test.head()","0005e2a0":"# Drop target, fill in NaNs\nX_train = train.drop('isFraud', axis=1)\nX_test = test.copy()\ndel train, test\ngc.collect()","3ebb71f8":"X_train = X_train.fillna(-999)\nX_test = X_test.fillna(-999)","2e51fa5c":"# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))","eb106ec6":"X_tr = X_train[:475000]\ny_tr = y_train[:475000]","8feff31e":"def augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","ce2c96e6":"params = {'max_bin' :63,\n    'num_leaves' : 255,\n    'num_iterations' : 500,\n    'learning_rate' : 0.05,\n    'tree_learner' :'serial',\n    'is_training_metric' : False,\n    'min_data_in_leaf' : 1,\n    'min_sum_hessian_in_leaf' : 100,\n    'sparse_threshold':1.0,\n    'device' : 'gpu',\n    'num_thread' : -1,\n    'save_binary': True,\n    'seed': 42,\n    'feature_fraction_seed' : 42,\n    'bagging_seed' : 42,\n    'drop_seed' : 42,\n    'data_random_seed' : 42,\n    'objective' : 'binary',\n    'boosting_type' : 'gbdt',\n    'verbose' : 1,\n    'metric' : 'auc',\n    'is_unbalance' : True,\n    'boost_from_average' : False}","87915591":"feature_importance_df = pd.DataFrame()\nval_aucs = []\nskf = KFold(n_splits=5)\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_tr, y_tr)):\n    X_tra, y_tra = X_tr.iloc[trn_idx], y_tr.iloc[trn_idx]\n    X_valid, y_valid = X_tr.iloc[val_idx], y_tr.iloc[val_idx]\n    print('Fold:',fold+1)\n    \n    N = 3\n    p_valid,yp = 0,0\n    for i in range(N):\n        X_t, y_t = augment(X_tra.values, y_tra.values)\n        X_t = pd.DataFrame(X_t)\n        X_t = X_t.add_prefix('var_')\n    \n        trn_data = lgb.Dataset(X_t, label=y_t)\n        val_data = lgb.Dataset(X_valid, label=y_valid)\n        evals_result = {}\n        lgb_clf = lgb.train(params,\n                        trn_data,\n                        100000,\n                        valid_sets = [trn_data, val_data],\n                        early_stopping_rounds=3000,\n                        verbose_eval = 1000,\n                        evals_result=evals_result\n                       )\n        p_valid += lgb_clf.predict(X_valid)\n        yp += lgb_clf.predict(X_test)\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = X_train.columns\n    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    val_score = roc_auc_score(y_valid, p_valid)\n    val_aucs.append(val_score)\n    predictions['fold{}'.format(fold+1)] = yp\/N","0eabf2ae":"print(\"Distribution of 1s in original data : {} \/ {} \".format(np.sum(y_train) , len(y_train)))\nprint(\"Percentage of 1s in original data  : {}\".format(np.sum(y_train)*100.0\/len(y_train)))\nprint('--'*30)\nprint(\"Distribution of 1s in subset of data(450000 rows) : {} \/ {} \".format(np.sum(y_tr) , len(y_tr)))\nprint(\"Percentage of 1s in subset of data(450000 rows)  : {}\".format(np.sum(y_tr)*100.0\/len(y_tr)))\nprint('--'*30)\nprint(\"Percentage of 1s in augmented data : {}\".format(np.sum(y_t)*100.0\/len(y_t)))\nprint(\"Distribution of 1s in augmented data : {} \/ {} \".format(np.sum(y_t) , len(y_t)))","aa506014":"mean_auc = np.mean(val_aucs)\nstd_auc = np.std(val_aucs)\nprint('Mean auc is {:2f} and standard deviation of auc is {:2f}'.format(mean_auc,std_auc))","ec3d78b2":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:30].index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\nplt.figure(figsize=(14,10))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Feature importance top 50(averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","e3711b14":"predictions['isFraud'] = np.mean(predictions[[col for col in predictions.columns if col not in ['TransactionID_x', 'isFraud']]].values, axis=1)\npredictions.to_csv('lgb_all_predictions.csv', index=None)","a561bfcc":"sub_df = pd.DataFrame({\"TransactionID\":predictions[\"TransactionID_x\"].values})\nsub_df[\"isFraud\"] = predictions['isFraud']\nsub_df.to_csv(\"lgb_submission.csv\", index=False)\nsub_df.head()","ddf7190e":"## Modeling","7aa56e08":"## IEEE Fraud: LightGBM GPU+Augmentation\n### Hi welcome to this kernel in this kernel i am using lightgbm and augmentation technique. <br>\n### Reference and credits: <br>\n1. https:\/\/www.kaggle.com\/kirankunapuli\/ieee-fraud-lightgbm-with-gpu <br>\n2. https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment <br>\n3. https:\/\/www.kaggle.com\/niteshx2\/beginner-explained-lgb-2-leaves-augment <br>\nThanks to authors of the above kernels!\n","53739e7e":"### Build and re-install LightGBM with GPU support","bcbe565d":"* We can see that after augmentation the percentage of 1s was increased by approximatly 1.5%.This technique is more like oversampling but,here we oversample both classes,rather than just one. ","4b7f9b61":"## Let's see what this augmentation does to our dataframe","3129cf94":"## Feature Importances","8d8f9669":"## LightGBM GPU Installation","4e9ac56d":"### That's all for now.Suggestions are welcome for further improvement of kernel,Upvote if you like this kernel.You can achieve better score with using full dataset for training, hyperparameter tuning,feature engineering,building different models and stacking the predictions.Thankyou! <br>\n### Happy learning\n","c519a77c":"Note : Here i am taking only 475000 rows in the data because of memory issues in kernel.If you have good hardware you can take full data frame for training.","6b0f557e":"## Submission","27990a39":"## Preprocessing","7842bc70":"## Imports"}}