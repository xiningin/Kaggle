{"cell_type":{"57322c86":"code","224e838f":"code","999a5f61":"code","a5dd4df7":"code","c71c6dcc":"code","5845b61a":"code","b75b89cc":"code","ef81db50":"code","01601088":"code","6d5f1d67":"code","6703c403":"code","ceed7ea1":"code","4af01937":"code","3ca16ae4":"code","dea1a919":"code","8ba1da68":"code","81230976":"code","ede7d529":"code","a2902daf":"code","62de0cb3":"code","a8aa7c61":"code","5971bed5":"code","7bfe5d59":"code","4e4962fa":"code","36c23a44":"code","ed38b91a":"code","d71047cb":"code","ae242da4":"code","a54f4316":"code","2a107bf1":"code","ce5e966d":"code","bb5d7261":"code","d3a0d2a7":"code","52e07125":"code","c58c1879":"code","0dd09e91":"code","7e70e49b":"code","725a91e5":"code","98c7f68c":"code","f6734f05":"code","b2918889":"code","551cf206":"code","d4418ef0":"code","85a75baf":"code","1b318cd9":"code","56d9ebc6":"code","656b02f0":"code","521d035b":"code","90e39372":"code","5892a4f0":"code","9011b428":"code","88f6e5a9":"code","516b3522":"code","06eded29":"code","464a0502":"code","f0d2e9bf":"code","0e5d9c39":"code","81189264":"code","b02f61f1":"code","a1a7cc56":"code","7166883f":"code","eb98b076":"code","0372e50e":"code","51a310e8":"code","bfb071ae":"code","147ab5ce":"code","c1656740":"code","6994bbac":"code","b9b31610":"code","fdc6087c":"code","84cf15d8":"code","9f0f65e6":"code","367310f5":"code","41f5d976":"markdown","bce31a3a":"markdown","22c8a7fb":"markdown","25a3f2ba":"markdown","0befb9e8":"markdown","bf7cdc2a":"markdown","9c960fe1":"markdown","a7151e87":"markdown","813f592c":"markdown","4570e47d":"markdown","8e904aa6":"markdown","f097f3c1":"markdown","4029e1ad":"markdown","603ea994":"markdown","9a983645":"markdown","1fa877c9":"markdown","670ef60d":"markdown","0eec9feb":"markdown","1d3b5dc9":"markdown","7458a1bd":"markdown","daa61cb7":"markdown","90c21b90":"markdown","3a61af54":"markdown","ddaace9b":"markdown","3fa557ed":"markdown","0be36c92":"markdown","1c6f1efd":"markdown","e2f08f98":"markdown","272033f6":"markdown","d0c61b89":"markdown","efb932d3":"markdown","717147fc":"markdown","eafc741d":"markdown","8979bd0d":"markdown","5f96f1dd":"markdown","34f202ff":"markdown","acf68cc2":"markdown","23c57c34":"markdown","3ed2d88e":"markdown","66f5054c":"markdown","365a6a82":"markdown","be4a19db":"markdown","04bf5c0d":"markdown","b993464d":"markdown","8ba11ee6":"markdown","79d0f8fe":"markdown","84699c16":"markdown","c7074832":"markdown","6d56c27f":"markdown"},"source":{"57322c86":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport time\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nstartall = time.time()\n\npd.options.display.max_columns = None\npd.options.display.max_rows = None\npd.options.display.float_format = '{:.2f}'.format # to set the displayed data as in the two decimal format","224e838f":"start = time.time()\n\ndf = pd.read_csv('\/kaggle\/input\/lending-club-loan-data\/loan.csv', low_memory = False)\n\nstop = time.time()\nduration = stop-start\nprint('It took {:.2f} seconds to read the entire csv file.'.format(duration))\n\ndf.head()","999a5f61":"df.describe()","a5dd4df7":"df.info()","c71c6dcc":"# Analyzing the missing value in each columns\ndf_null = pd.DataFrame({'Count': df.isnull().sum(), 'Percent': round(100*df.isnull().sum()\/len(df),2)})\ndf_null[df_null['Count'] != 0] ","5845b61a":"# Visualize the percentage of missing values in columns that have more than 70% missing values\ndf_null_70up = df_null[df_null['Percent'] >= 70]\ndf_null_70up = df_null_70up.sort_values(\n    by=['Percent'], \n    ascending=False\n)\n\nplt.figure(figsize=(15,8))\nbarchart = sns.barplot(\n    df_null_70up.index, \n    df_null_70up['Percent'],\n    palette='Set2'\n)\n\nbarchart.set_xticklabels(barchart.get_xticklabels(), rotation=45, horizontalalignment='right')","b75b89cc":"# Remove columns which missing values > 70%\ndf_1 = df.dropna(axis=1, thresh=int(0.70*len(df)))\ndf_1.head()","ef81db50":"print(\n    'The number of columns has reduced from {} to {} columns by removing columns with 70% missing values'.\n    format(len(df.columns), len(df_1.columns))\n)","01601088":"plt.figure(figsize = (15,5))\nplot1 = sns.barplot(df.loan_status.value_counts().index, df.loan_status.value_counts(), palette = 'Set1')\nplt.xticks(rotation = 45, horizontalalignment='right')\nplt.yticks(fontsize = 12)\nplt.title(\"Loan Status Distribution\", fontsize = 20, weight='bold')\nplt.ylabel(\"Count\", fontsize = 15)\n\ntotal = len(df_1)\nsizes = []\nfor p in plot1.patches:\n    height = p.get_height()\n    sizes.append(height)\n    plot1.text(p.get_x() + p.get_width()\/2.,\n            height + 10000,\n            '{:1.3f}%'.format(height\/total*100),\n            ha = \"center\", \n            fontsize = 10) ","6d5f1d67":"selected_loan_status = ['Fully Paid', 'Charged Off', 'Default']\ndf_2 = df_1[df_1.loan_status.isin(selected_loan_status)]\ndf_2.loan_status = df_2.loan_status.replace({'Fully Paid' : 'Good Loan'})\ndf_2.loan_status = df_2.loan_status.replace({'Charged Off' : 'Bad Loan'})\ndf_2.loan_status = df_2.loan_status.replace({'Default' : 'Bad Loan'})","6703c403":"print(\n    'The number of rows has been reduced from {:,.0f} to {:,.0f} by filtering the data with the correlated loan status'.\n    format(len(df_1), len(df_2))     \n)","ceed7ea1":"plt.figure(figsize=(8, 5))\nplot2 = sns.countplot(df_2.term, hue = df_2.loan_status)\nplt.title(\"Loan's Term Distribution\", fontsize = 20, weight='bold')\nplt.ylabel(\"Count\", fontsize = 15)\nplt.xlabel(\"Term\", fontsize = 15)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\n\ntotal = len(df_2)\nsizes = []\nfor p in plot2.patches:\n    height = p.get_height()\n    sizes.append(height)\n    plot2.text(p.get_x() + p.get_width()\/2.,\n            height + 10000,\n            '{:1.0f}%'.format(height\/total*100),\n            ha = \"center\", \n            fontsize = 12) ","4af01937":"plt.figure(figsize = (10,7))\nsns.distplot(df.loan_amnt, bins=20)\nplt.title('Loan Amount Distribution', fontsize = 20, weight='bold')\nplt.xlabel('Loan Amount', fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)","3ca16ae4":"plt.figure(figsize = (10,7))\nsns.countplot(round(df.int_rate, 0).astype(int))\nplt.title('Interest Rate Distribution', fontsize = 20, weight='bold')\nplt.xlabel('Interest Rate (Rounded)', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)","dea1a919":"plt.figure(figsize = (16,5))\nplot3 = sns.countplot(df_2.sort_values(by='grade').grade, hue = df_2.loan_status)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\nplt.title(\"Grade Distribution\", fontsize = 20, weight='bold')\nplt.xlabel(\"Grade\", fontsize = 15)\nplt.ylabel(\"Count\", fontsize = 15)\n\ntotal = len(df_2)\nsizes = []\nfor p in plot3.patches:\n    height = p.get_height()\n    sizes.append(height)\n    plot3.text(p.get_x() + p.get_width()\/2.,\n            height + 3000,\n            '{:1.2f}%'.format(height\/total*100),\n            ha = \"center\", \n            fontsize = 10) ","8ba1da68":"plt.figure(figsize = (15,5))\nplot4 = sns.barplot(df.purpose.value_counts().index, df.purpose.value_counts(), palette = 'Set1')\nplt.xticks(rotation = 30, fontsize = 12, horizontalalignment='right')\nplt.yticks(fontsize = 12)\nplt.title(\"Loan Purpose Distribution\", fontsize = 20, weight='bold')\nplt.ylabel(\"Count\", fontsize = 15)\n\ntotal = len(df_1)\nsizes = []\nfor p in plot4.patches:\n    height = p.get_height()\n    sizes.append(height)\n    plot4.text(p.get_x() + p.get_width()\/2.,\n            height + 10000,\n            '{:1.2f}%'.format(height\/total*100),\n            ha = \"center\", \n            fontsize = 10) ","81230976":"plt.figure(figsize = (20,11))\nsns.boxplot(df_2.loan_status, df_2.loan_amnt, hue = df_2.term, palette = 'Paired')\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\nplt.xlabel(\"Loan Status Categories\", fontsize = 15)\nplt.ylabel(\"Loan Amount Distribution\", fontsize = 15)\nplt.title(\"Loan Status by Loan Amount\", fontsize = 20, weight='bold')","ede7d529":"plt.figure(figsize = (20,11))\nsns.boxplot(df_2.loan_status, round(df_2.int_rate, 0).astype(int), hue = df_2.term, palette = 'Paired')\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\nplt.xlabel(\"Loan Status Categories\", fontsize = 15)\nplt.ylabel(\"Interest Rate Distribution\", fontsize = 15)\nplt.title(\"Loan Status by Interest Rate\", fontsize = 20, weight='bold')","a2902daf":"plt.figure(figsize=(12, 7))\nplot5 = sns.countplot(df_2.verification_status, hue = df_2.loan_status, palette = 'inferno')\nplt.title(\"Verification Status Distribution\", fontsize = 20)\nplt.xlabel(\"Verification Status\", fontsize = 15)\nplt.ylabel(\"Count\", fontsize = 15)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\n\ntotal = len(df_2)\nsizes = []\nfor p in plot5.patches:\n    height = p.get_height()\n    sizes.append(height)\n    plot5.text(p.get_x() + p.get_width()\/2.,\n            height + 5000,\n            '{:1.0f}%'.format(height\/total*100),\n            ha = \"center\", \n            fontsize = 12)","62de0cb3":"most_emp_title = df_2.emp_title.value_counts()[:20].index.values  # get the top 20 most frequent employee job title\ncm = sns.light_palette(\"orange\", as_cmap=True)\n\nround(pd.crosstab(df_2[df_2['emp_title'].isin(most_emp_title)]['emp_title'], \n                  df_2[df_2['emp_title'].isin(most_emp_title)]['grade'], \n                  normalize='index') * 100,2).style.background_gradient(cmap = cm)","a8aa7c61":"df_3 = df_2[[\n    'loan_status', 'term','int_rate',\n    'installment','grade', 'annual_inc',\n    'verification_status','dti'  # These features are just initial guess, you can try to choose any other combination\n]]\ndf_3.head()","5971bed5":"# Find missing values in the chosen columns\ndf_null = pd.DataFrame({'Count': df_3.isnull().sum(), 'Percent': round(100*df_3.isnull().sum()\/len(df_3),2)})\ndf_null[df_null['Count'] != 0] ","7bfe5d59":"# Dropping rows with null values\ndf_clean = df_3.dropna(axis = 0)","4e4962fa":"print('Number of dropped rows: {} rows'.format(len(df_3)-len(df_clean)))","36c23a44":"# The next step is to transform categorical target variable into integer\ndf_clean.loan_status = df_clean.loan_status.replace({'Good Loan' : 1})\ndf_clean.loan_status = df_clean.loan_status.replace({'Bad Loan' : 0})\ndf_clean.loan_status.unique()","ed38b91a":"from sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\n\ndf_clean['term'] = label.fit_transform(df_clean['term'])\ndf_clean['grade'] = label.fit_transform(df_clean['grade'])\ndf_clean['verification_status'] = label.fit_transform(df_clean['verification_status'])","d71047cb":"x = df_clean.drop(['loan_status'], axis=1)\ny = df_clean['loan_status']","ae242da4":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nimport numpy as np \n\ncoltrans = ColumnTransformer(\n    [('one_hot_encoder', OneHotEncoder(categories='auto'), [0,3,5])],      # 0,3,5 refers to the column indexes that need to be transformed      \n    remainder = 'passthrough'                               \n)                                                         \n\nx = np.array(coltrans.fit_transform(x))","a54f4316":"from sklearn.model_selection import train_test_split\nxtr, xts, ytr, yts = train_test_split(\n    x,\n    y,\n    test_size = .2\n)","2a107bf1":"print(ytr.value_counts())\nprint(yts.value_counts())","ce5e966d":"from imblearn.over_sampling import SMOTE\n\nsmt = SMOTE()\nxtr_2, ytr_2 = smt.fit_sample(xtr, ytr)","bb5d7261":"np.bincount(ytr_2)","d3a0d2a7":"from sklearn.ensemble import RandomForestClassifier\n\nstart = time.time()\n\nmodel = RandomForestClassifier()\nmodel.fit(xtr_2, ytr_2)\n\nstop = time.time()\nduration = stop-start\nprint('The training took {:.2f} seconds.'.format(duration))","52e07125":"print(round(model.score(xts, yts) * 100, 2), '%')","c58c1879":"y_pred = model.predict(xts)","0dd09e91":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(yts, y_pred)","7e70e49b":"pd.crosstab(yts, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)","725a91e5":"from sklearn.metrics import classification_report\n\ntarget_names = ['Bad Loan', 'Good Loan']\nprint(classification_report(yts, model.predict(xts), target_names=target_names))","98c7f68c":"from sklearn.ensemble import RandomForestClassifier\n\nstart = time.time()\n\nmodel2 = RandomForestClassifier()\nmodel2.fit(xtr, ytr)\n\nstop = time.time()\nduration = stop-start\nprint('The training took {:.2f} seconds.'.format(duration))","f6734f05":"print(round(model2.score(xts, yts) * 100, 2), '%')","b2918889":"y_pred2 = model2.predict(xts)","551cf206":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(yts, y_pred2)","d4418ef0":"pd.crosstab(yts, y_pred2, rownames=['Actual'], colnames=['Predicted'], margins=True)","85a75baf":"from sklearn.metrics import classification_report\n\ntarget_names = ['Bad Loan', 'Good Loan']\nprint(classification_report(yts, y_pred2, target_names=target_names))","1b318cd9":"from imblearn.under_sampling import NearMiss\n\nnr = NearMiss()\nxtr_3, ytr_3 = nr.fit_sample(xtr, ytr)","56d9ebc6":"np.bincount(ytr_3)","656b02f0":"from sklearn.ensemble import RandomForestClassifier\n\nstart = time.time()\n\nmodel3 = RandomForestClassifier()\nmodel3.fit(xtr_3, ytr_3)\n\nstop = time.time()\nduration = stop-start\nprint('The training took {:.2f} seconds.'.format(duration))","521d035b":"print(round(model3.score(xts, yts) * 100, 2), '%')","90e39372":"y_pred3 = model3.predict(xts)","5892a4f0":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(yts, y_pred3)","9011b428":"pd.crosstab(yts, y_pred3, rownames=['Actual'], colnames=['Predicted'], margins=True)","88f6e5a9":"from sklearn.metrics import classification_report\n\ntarget_names = ['Bad Loan', 'Good Loan']\nprint(classification_report(yts, y_pred3, target_names=target_names))","516b3522":"# First, by knowing what are the features available in the dataframe\ndf_4 = df_2","06eded29":"# The next step is to transform categorical target variable into integer\ndf_4.loan_status = df_4.loan_status.replace({'Good Loan' : 1})\ndf_4.loan_status = df_4.loan_status.replace({'Bad Loan' : 0})","464a0502":"df_4.columns.to_series().groupby(df_clean.dtypes).groups","f0d2e9bf":"# First, dropping categorical features (object type) which have too many options available\ndf_4 = df_4.drop(['emp_title', 'sub_grade', 'issue_d', 'last_pymnt_d', 'last_credit_pull_d', 'hardship_flag', 'debt_settlement_flag'], axis=1)","0e5d9c39":"# Second, to filter numerical features, we can use .corr() function to select only features with high correlation to the target variable\ndf_4.corr()['loan_status']","81189264":"df_clean = df_4[[\n    'loan_status', # target variable\n    # features (object):\n    'term', 'grade','home_ownership', 'verification_status', 'pymnt_plan', 'purpose', \n    'initial_list_status', 'application_type', 'disbursement_method',\n    # features (int\/float):\n    'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'recoveries',                   \n    'collection_recovery_fee', 'last_pymnt_amnt', 'int_rate'\n]]","b02f61f1":"df_null = pd.DataFrame({'Count': df_clean.isnull().sum(), 'Percent': round(100*df_clean.isnull().sum()\/len(df_clean),2)})\ndf_null[df_null['Count'] != 0] ","a1a7cc56":"from sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\n\ndf_clean['term'] = label.fit_transform(df_clean['term'])\ndf_clean['grade'] = label.fit_transform(df_clean['grade'])\n# df_clean['emp_length'] = label.fit_transform(df_clean['emp_length'])\ndf_clean['home_ownership'] = label.fit_transform(df_clean['home_ownership'])\ndf_clean['verification_status'] = label.fit_transform(df_clean['verification_status'])\ndf_clean['pymnt_plan'] = label.fit_transform(df_clean['pymnt_plan'])\ndf_clean['purpose'] = label.fit_transform(df_clean['purpose'])\ndf_clean['initial_list_status'] = label.fit_transform(df_clean['initial_list_status'])\ndf_clean['application_type'] = label.fit_transform(df_clean['application_type'])\ndf_clean['disbursement_method'] = label.fit_transform(df_clean['disbursement_method'])","7166883f":"df_clean.head()","eb98b076":"x = df_clean.drop(['loan_status'], axis=1)\ny = df_clean['loan_status']","0372e50e":"x.head()","51a310e8":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nimport numpy as np \n\ncoltrans = ColumnTransformer(\n    [('one_hot_encoder', OneHotEncoder(categories='auto'), [0,1,2,3,4,5,6,7,8])],        \n    remainder = 'passthrough'                               \n)                                                         \n\nx = np.array(coltrans.fit_transform(x))","bfb071ae":"from sklearn.model_selection import train_test_split\nxtr, xts, ytr, yts = train_test_split(\n    x,\n    y,\n    test_size = .2\n)","147ab5ce":"from sklearn.ensemble import RandomForestClassifier\nimport time\n\nstart = time.time()\n\nmodel = RandomForestClassifier()\nmodel.fit(xtr, ytr)\n\nstop = time.time()\nduration = stop-start\nprint('The training took {:.2f} seconds.'.format(duration))","c1656740":"print(round(model.score(xts, yts) * 100, 2), '%')","6994bbac":"y_pred = model.predict(xts)","b9b31610":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(yts, y_pred)","fdc6087c":"pd.crosstab(yts, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)","84cf15d8":"from sklearn.metrics import classification_report\n\ntarget_names = ['Bad Loan', 'Good Loan']\nprint(classification_report(yts, model.predict(xts), target_names=target_names))","9f0f65e6":"import sklearn.metrics as metrics\n\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = model.predict_proba(xts)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(yts, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\n\n# Plotting the ROC curve\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","367310f5":"import math\n\nstopall = time.time()\ndurationall = stopall-startall\nduration_mins = math.floor(durationall\/60)\nduration_secs = durationall - (duration_mins*60)\n\nprint('The whole notebook runs for {} minutes {:.2f} seconds.'.format(duration_mins, duration_secs))","41f5d976":"We also have to transform categorical feature columns using *one hot encoding*","bce31a3a":"# Lending Club Loan Data\n## Loan Category Classifier Machine Learning\n\n#### by: Albertus Rianto Wibisono - 28 Nov 2019\n\ndata source: https:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data","22c8a7fb":"#### 4.C.) Third Trial","25a3f2ba":"Columns with total misisng values more than 70% would be unnecessary for further analysis and might eventually lead to a inaccurate result in the final model","0befb9e8":"Here we can see that after SMOTE function runs, the number of 'good' and 'bad' loan has been balanced.","bf7cdc2a":"If we look to the loan status distribution above, it is clearly noted that the data set is unbalanced, where the amount of *bad* loan is far fewer than than the *good* one.\n\nAn imbalanced data can result in inaccurate \/ biased classifications of the final output. Therefore, before fitting the data into the machine learning model, we need to rebalance the data with a method called SMOTE (Synthetic Minority Oversampling Technique).","9c960fe1":"For the first trial, I'm gonna select data with only potentially related features. To filter the dataframe with this method, one has to really understand the business knowledge behind the data very well. \n\nThe fewer number of column is, means that the model running time will also be faster. ","a7151e87":"### 1.) Data Import","813f592c":"A nice thing to note here is that either *bad loan* and *good loan* has relatively the same loan amount (for both loan's terms). Except for good loan with 60 months terms, the amount average is slightly higher compared to the bad loan's average number. This means that the amount of loan alone can't necessarily predict the category of the loan.","4570e47d":"**3.8. Verification Status**","8e904aa6":"Features can be in the form of number (float\/integer) or string (object). For categorical features, we also have to one-hot-encode it first before fitting into the dataset. Choosing a categorical feature which have too many options will only causing the matrix becoming too large and difficult to handle by the computer.","f097f3c1":"From the graph above, it can be noted that more borrowers in Lending Club are using 36 months loan's term. It is also important to see that while fewer people using 60 months loan's term, but one third of them is categorised as *bad loan* while only 16% of the total borrowers who use 36 months loan's term is considered as *bad*.","4029e1ad":"For the classification report generated from this first trial, it seems that undersampling method increase the performance of the ML model to predcit bad loan, but it decreases the performance to predict good loan significantly. As a result, the whole accuracy score is just as bad.\n\nSo far, we haven't been able to increase the performance of the ML model.","603ea994":"The crosstab functions builds a cross-tabulation table that can show the frequency with which certain groups of data appear.\nFrom the visualization above, we can see that for loans with grade **'A'**, the most frequent borrowers are employee with job titles = ('Director', 'Engineer', 'President', 'Vice President') and other high-paying job titles. This means that these job titles have a lower risk of defaulting so that their loans is graded higher, which is reasonable. ","9a983645":"Another method to deal with unbalanced dataset is by applying NearMiss to perform undersampling in order to also get a balanced dataset.","1fa877c9":"Split data into target column (x) and features (y)","670ef60d":"### 2.) Exploratory Data Analysis","0eec9feb":"Based on the existing loan_status, I will choose only rows which loan status = ('fully paid', 'default', 'charged off') in order to easily categorize them into *good* or *bad* loans.","1d3b5dc9":"**3.7. Loan Status Distribution based by Loan Amount and Interest Rate**","7458a1bd":"**Fitting into Machine Learning Model**","daa61cb7":"#### 4.D.) Fourth Trial","90c21b90":"### 3.) Data Visualization ","3a61af54":"Another way to increase ML model performance is by doing hyperparameter tuning, but this is not what I'm gonna do in this notebook.\n\nTherefore, we could also re-consider the features we have filtered earlier. Beside using domain knowledge, it is also better to choose the features **objectively** by looking at the correlation for each features toward the target variable (loan_status).","ddaace9b":"Using *OneHotEncoder* to transform categorical columns: loan_term, loan_grade, verification_status","3fa557ed":"From the confusion matrix and classification report above, we can really see that the model is biased toward *good loan*. It runs pretty good at predicting *good loan*, but it performs really bad at predicting the *bad loan* (from all 52k actual bad loans, the model predicted only less than half of them right --> 13k)","0be36c92":"Finally, the fourth trial shows the best result from the ML model in predicting loan's classification. Beside generating a good accuracy score, the classification report also shows a magnificent result for both classification.","1c6f1efd":"Random Forest is the most common method used for classification algorithm. Although it has a high risk to overfit the data, but it has been a prominent method to solve cases with classification output.","e2f08f98":"The next step is, splitting data into training and testing data","272033f6":"It seems that the accuracy score gives a slightly higher score compared to the first trial with balanced dataset. But, it also has to be noted that the model shows a worse performance at predicting *good loan* (look at the decreasing classification report scores).","d0c61b89":"**3.6. Loan Purpose**","efb932d3":"**3.2. Loan's Term**","717147fc":"This notebook is utilizing data obtained from the link provided above. This is a good notebook for ML amateur pracitioner to learn:\n1. Visualize data using matplotlib and seaborn\n2. Creating machine learning model and gradually find ways to increase the ML performance that generates the most accurate classification\n\nA thing to note here is that this notebook runs for a long time (almost 12 minutes on my laptop) because the size of the dataset itself and the complexity of the ML model.","eafc741d":"#### 4.A) First Trial","8979bd0d":"It even shows a good AUC ROC score!","5f96f1dd":"**3.1. Loan Status**","34f202ff":"By comparing the height of the bar of good and bad loan for each verification status, we can imply that **'verified'** status has the biggest percentage of bad loan (almost 30%). Somehow, this doesn't make sense in the first place. This might create a misleading conclusion that '*verification status'* might not be the best consideration to predict the loan's quality.","acf68cc2":"#### 4.B.) Second Trial","23c57c34":"While the model gives a good accuracy score, it's better to see the **classification report** and **confusion matrix** to really see how good the model performance is in classifying both classes.","3ed2d88e":"In this second trial, I want to try fitting the ML model using the unbalanced dataset (without applying SMOTE) to see if it really impactful to the final performance of the model. Using the same steps from the first trial...","66f5054c":"From the chart above we see that as the grade is degrading, more loans are categorised as *bad* rather than *good*, which is reasonable because a lower grade means that the risk of defaulting is also increasing","365a6a82":"**3.5. Grade**","be4a19db":"It can be inferred that almost 80% of borrower in Lending Club has the purpose to re-pay their previous debt","04bf5c0d":"It can be inferred that many loan done in Lending Club has interest rate lies between 7-18 %","b993464d":"It's good that there's no missing values","8ba11ee6":"### 4.) Machine Learning","79d0f8fe":"**3.4. Interest Rate**","84699c16":"**3.3. Loan Amount**","c7074832":"The graphic above shows that *bad loan* has relatively higher interest rate compared to *good loan*. This means that with higher interest rate, borrowers are more unlikely to repay their debt, and this is totally reasonable.","6d56c27f":"**3.9. Top 20 Job Titles for Each Grades**"}}