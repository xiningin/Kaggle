{"cell_type":{"990f16fb":"code","6cbbc172":"code","90de1793":"code","d5c24831":"code","2746d6ad":"code","eae8a86d":"code","15fd87e7":"code","226c2883":"code","828ef1b0":"code","00c27712":"code","24b144ac":"code","b4a12598":"code","70e55e30":"code","53389cca":"code","5cbc3873":"code","57e6c5a6":"code","f1d8bc18":"code","3e14ad66":"code","9310795d":"code","e268f6ef":"code","48b962e7":"code","b55c9044":"code","b2089b1e":"code","5722d6e1":"code","d19a9455":"code","e2de7fe6":"code","83e5f3f5":"code","1e02e637":"code","7e7c3527":"code","63599a75":"code","e77a52ae":"code","641e4415":"code","59d84690":"code","b6e91fc1":"code","6e638b80":"code","977bf14d":"code","bc523784":"code","94f01141":"code","adf8cf95":"code","bf36e099":"code","a7280ab3":"code","2450bf77":"code","3b704135":"code","ecd2a633":"code","926924da":"code","d2b4831c":"code","bda97014":"code","746d2871":"code","9261675d":"code","6ec69ee7":"code","5c714e4a":"code","8d82fdb8":"code","cd170624":"code","03a7d396":"code","a70a493d":"code","dce36d2f":"markdown","561fa527":"markdown","f4dc42dc":"markdown","8203919a":"markdown","5070544d":"markdown","f87abb8b":"markdown","316ba1e2":"markdown","8ff651b3":"markdown","3ee2cc91":"markdown","84ca31f2":"markdown","22d25e68":"markdown","74f13dbe":"markdown","7a0b9014":"markdown","c2eca96b":"markdown","01461bea":"markdown","c0b71d1d":"markdown","6c87f748":"markdown","7371c224":"markdown","6eebde5b":"markdown","c3c8343b":"markdown","1eafe90c":"markdown","10207918":"markdown","762608aa":"markdown","07b6061d":"markdown","a21b4387":"markdown","76755de5":"markdown"},"source":{"990f16fb":"import pandas as pd # used to load, manipulate the data and for one-hot encoding\nimport numpy as np # data manipulation\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\nfrom sklearn.utils import resample # for downsample the dataset\nfrom sklearn.model_selection import train_test_split # for splitting the dataset into train and test split\nfrom sklearn.preprocessing import scale # scale and center the data\nfrom sklearn.svm import SVC # will make a SVM for classification\nfrom sklearn.model_selection import GridSearchCV # will do the cross validation\nfrom sklearn.metrics import plot_confusion_matrix # will draw the confusion matrix\nfrom sklearn.decomposition import PCA # to perform PCA to plot the data\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score, roc_curve, auc\nimport seaborn as sns","6cbbc172":"data = pd.read_csv(\"..\/input\/mri-and-alzheimers\/oasis_longitudinal.csv\")","90de1793":"pd.set_option('display.max_columns', None) # will show the all columns with pandas dataframe\npd.set_option('display.max_rows', None) # will show the all rows with pandas dataframe","d5c24831":"data.head()\n# data.tail()\n# data.size","2746d6ad":"data.shape","eae8a86d":"data.info()","15fd87e7":"data['M\/F'] = [1 if each == \"M\" else 0 for each in data['M\/F']]\ndata['Group'] = [1 if each == \"Demented\" or each == \"Converted\" else 0 for each in data['Group']]\n# data['Group'] = data['Group'].replace(['Converted'], ['Demented']) # Target variable\n# data['Group'] = data['Group'].replace(['Demented', 'Nondemented'], [1,0]) # Target variable\ndata.info()","226c2883":"correlation_matrix = data.corr()\ndata_corr = correlation_matrix['Group'].sort_values(ascending=False)\ndata_corr","828ef1b0":"from pandas.plotting import scatter_matrix\n\nattributes = [\"Group\", \"CDR\", \"M\/F\", \"SES\", \"ASF\"]\n\nscatter_matrix(data[attributes], figsize=(15, 11), alpha=0.3)","00c27712":"import plotly.express as px\n\nfig = px.scatter(data, x='Group', y='SES', color='Group')\nfig.show()","24b144ac":"import plotly.express as px\n\nfig = px.scatter(data, x='Group', y='Age', color='Group')\nfig.show()","b4a12598":"import plotly.express as px\n\nfig = px.scatter(data, x='Group', y='ASF', color='Group')\nfig.show()","70e55e30":"data.isnull().sum()","53389cca":"median = data['MMSE'].median()\ndata['MMSE'].fillna(median, inplace=True)\ndata.isnull().sum()","5cbc3873":"median = data['SES'].median()\ndata['SES'].fillna(median, inplace=True)\ndata.isnull().sum()","57e6c5a6":"y = data['Group'].values\nX = data[['M\/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']]","f1d8bc18":"# by default test_size= 0.25\nX_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size= 0.20, random_state=42)\n\ndf_ytrain = pd.DataFrame(y_trainval)\ndf_ytest = pd.DataFrame(y_test)\n\nprint('In Training Split:')\nprint(df_ytrain[0].value_counts())\n\nprint('\\nIn Testing Split:')\nprint(df_ytest[0].value_counts())","3e14ad66":"# by default test_size= 0.25\nX_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size= 0.20, random_state=42, stratify=y)\n\n\ndf_ytrain = pd.DataFrame(y_trainval)\ndf_ytest = pd.DataFrame(y_test)\n\nprint('In Training Split:')\nprint(df_ytrain[0].value_counts())\n\nprint('\\nIn Testing Split:')\nprint(df_ytest[0].value_counts())","9310795d":"# here StandardScaler() means z = (x - u) \/ s\nscaler = StandardScaler().fit(X_trainval)\n#scaler = MinMaxScaler().fit(X_trainval)\nX_trainval_scaled = scaler.transform(X_trainval)\nX_test_scaled = scaler.transform(X_test)","e268f6ef":"X_trainval_scaled","48b962e7":"X_trainval.describe()","b55c9044":"X_trainval.hist(bins=30, figsize=(20,15))\nplt.show()","b2089b1e":"import plotly.express as px\n\nx = ['M\/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']\n\nfig = px.histogram(X_trainval, x='eTIV', nbins=50)\nfig.show()","5722d6e1":"import plotly.express as px\n\nx = ['M\/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']\n\nfig = px.scatter(X_trainval, x='eTIV')\nfig.show()","d19a9455":"clf_svm = SVC(random_state=42)\nclf_svm.fit(X_trainval_scaled, y_trainval)\n\n# for test there are 94 cases\nplot_confusion_matrix(clf_svm, \n                      X_test_scaled, \n                      y_test, \n                      values_format='d', \n                      display_labels=['Nondemented', 'Demented'])","e2de7fe6":"train_score = 0\ntest_score = 0\ntest_recall = 0\ntest_auc = 0\n\ntrain_score = clf_svm.score(X_trainval_scaled, y_trainval)\ntest_score = clf_svm.score(X_test_scaled, y_test)\ny_predict = clf_svm.predict(X_test_scaled)\n\ntest_recall = recall_score(y_test, y_predict)\nfpr, tpr, thresholds = roc_curve(y_test, y_predict)\ntest_auc = auc(fpr, tpr)\n\n\nprint(\"Train accuracy \", train_score)\nprint(\"Test accuracy \", test_score)\nprint(\"Test recall\", test_recall)\nprint(\"Test AUC\", test_auc)","83e5f3f5":"# Normally, C = 1 and gamma = 'scale' are default values\n# C controls how wide the margin will be with respect to how many misclassification we are allowing\n# C is increasing --> reduce the size of the margin and fewer misclassification and vice versa\nparam_grid = [\n    {'C': [0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 30, 50, 80, 100],\n    'gamma': ['scale', 0.5, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001],\n    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']},\n]\n\noptimal_params = GridSearchCV(SVC(),\n                             param_grid,\n                             cv=5, # we are taking 5-fold as in k-fold cross validation\n                             scoring='accuracy', # try the other scoring if have time\n                             verbose=0,\n                             n_jobs=-1)\n\noptimal_params.fit(X_trainval_scaled, y_trainval)\nprint(optimal_params.best_params_)","1e02e637":"C = optimal_params.best_params_['C']\ngamma = optimal_params.best_params_['gamma']\nkernel = optimal_params.best_params_['kernel']","7e7c3527":"clf_svm = SVC(random_state=42, C=C, gamma=gamma, kernel=kernel)\nclf_svm.fit(X_trainval_scaled, y_trainval)\n\nplot_confusion_matrix(clf_svm, \n                      X_test_scaled, \n                      y_test, \n                      values_format='d', \n                      display_labels=['Nondemented', 'Demented'])","63599a75":"train_score = 0\ntest_score = 0\ntest_recall = 0\ntest_auc = 0\n\ntrain_score = clf_svm.score(X_trainval_scaled, y_trainval)\ntest_score = clf_svm.score(X_test_scaled, y_test)\ny_predict = clf_svm.predict(X_test_scaled)\n\ntest_recall = recall_score(y_test, y_predict)\nsvm_fpr, svm_tpr, thresholds = roc_curve(y_test, y_predict)\ntest_auc = auc(svm_fpr, svm_tpr)\n\n\nprint(\"Train accuracy \", train_score)\nprint(\"Test accuracy \", test_score)\nprint(\"Test recall\", test_recall)\nprint(\"Test AUC\", test_auc)","e77a52ae":"from sklearn.ensemble import RandomForestClassifier","641e4415":"# n_estimators(M) --> the number of trees in the forest\n# max_features(d) --> the number of features to consider when looking for the best split\n# max_depth(m) --> the maximum depth of the tree.\n\nrfc = RandomForestClassifier(random_state=42)\nrfc.fit(X_trainval_scaled, y_trainval)\n\n# for test there are 94 cases\nplot_confusion_matrix(rfc, \n                      X_test_scaled, \n                      y_test, \n                      values_format='d', \n                      display_labels=['Nondemented', 'Demented'])","59d84690":"train_score = 0\ntest_score = 0\ntest_recall = 0\ntest_auc = 0\n\ntrain_score = rfc.score(X_trainval_scaled, y_trainval)\ntest_score = rfc.score(X_test_scaled, y_test)\ny_predict = rfc.predict(X_test_scaled)\ntest_recall = recall_score(y_test, y_predict)\nfpr, tpr, thresholds = roc_curve(y_test, y_predict)\ntest_auc = auc(fpr, tpr)\n\nprint(\"Train accuracy \", train_score)\nprint(\"Test accuracy \", test_score)\nprint(\"Test recall\", test_recall)\nprint(\"Test AUC\", test_auc)","b6e91fc1":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt', 'log2']\n\n# Maximum number of levels in tree\nmax_depth = range(1,10)\n\n# measure the quality of a split\ncriterion = ['gini']\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]","6e638b80":"# Create the param grid\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'criterion': criterion,\n               'bootstrap': bootstrap}","977bf14d":"optimal_params = GridSearchCV(RandomForestClassifier(),\n                             param_grid,\n                             cv=5, # we are taking 5-fold as in k-fold cross validation\n                             scoring='accuracy', # try the other scoring if have time\n                             verbose=0,\n                             n_jobs=-1)\n\noptimal_params.fit(X_trainval_scaled, y_trainval)\nprint(optimal_params.best_params_)","bc523784":"bootstrap = optimal_params.best_params_['bootstrap']\ncriterion = optimal_params.best_params_['criterion']\nmax_depth = optimal_params.best_params_['max_depth']\nmax_features = optimal_params.best_params_['max_features']\nn_estimators = optimal_params.best_params_['n_estimators']","94f01141":"rfc = RandomForestClassifier(n_estimators=n_estimators, \n                             max_features=max_features, \n                             max_depth=max_depth, \n                             criterion=criterion,\n                             bootstrap=bootstrap,\n                             random_state=42)\n\nrfc.fit(X_trainval_scaled, y_trainval)\n\n# for test there are 94 cases\nplot_confusion_matrix(rfc, \n                      X_test_scaled, \n                      y_test, \n                      values_format='d', \n                      display_labels=['Nondemented', 'Demented'])","adf8cf95":"train_score = 0\ntest_score = 0\ntest_recall = 0\ntest_auc = 0\n\ntrain_score = rfc.score(X_trainval_scaled, y_trainval)\ntest_score = rfc.score(X_test_scaled, y_test)\ny_predict = rfc.predict(X_test_scaled)\ntest_recall = recall_score(y_test, y_predict)\nrfc_fpr, rfc_tpr, thresholds = roc_curve(y_test, y_predict)\ntest_auc = auc(rfc_fpr, rfc_tpr)\n\nprint(\"Train accuracy \", train_score)\nprint(\"Test accuracy \", test_score)\nprint(\"Test recall\", test_recall)\nprint(\"Test AUC\", test_auc)","bf36e099":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score, roc_curve, auc","a7280ab3":"log_reg_model = LogisticRegression().fit(X_trainval_scaled, y_trainval)\n\n# for test there are 94 cases\nplot_confusion_matrix(log_reg_model, \n                      X_test_scaled, \n                      y_test, \n                      values_format='d', \n                      display_labels=['Nondemented', 'Demented'])","2450bf77":"train_score = 0\ntest_score = 0\ntest_recall = 0\ntest_auc = 0\n\nlog_reg_model = LogisticRegression().fit(X_trainval_scaled, y_trainval)\ntrain_score = log_reg_model.score(X_trainval_scaled, y_trainval)\ntest_score = log_reg_model.score(X_test_scaled, y_test)\nscores = log_reg_model.score(X_test_scaled, y_test)\ny_predict = log_reg_model.predict(X_test_scaled)\ntest_recall = recall_score(y_test, y_predict)\nfpr, tpr, thresholds = roc_curve(y_test, y_predict)\ntest_auc = auc(fpr, tpr)\n\n\nprint(\"Train accuracy \", train_score)\nprint(\"Test accuracy \", test_score)\nprint(\"Test recall\", test_recall)\nprint(\"Test AUC\", test_auc)","3b704135":"param_grid = {'penalty': ['l1','l2'], \n               'C': [0.001,0.01,0.1,1, 2, 3, 5, 10,100,1000]}\n\noptimal_params = GridSearchCV(LogisticRegression(),\n                             param_grid,\n                             cv=5, # we are taking 5-fold as in k-fold cross validation\n                             scoring='accuracy', # try the other scoring if have time\n                             verbose=0,\n                             n_jobs=-1)\n\noptimal_params.fit(X_trainval_scaled, y_trainval)\nprint(optimal_params.best_params_)","ecd2a633":"# best_score = -10\n# for c in range(1, 20):       \n#         log_reg_model = LogisticRegression(C=c)\n#         scores = cross_val_score(log_reg_model, X_trainval_scaled, y_trainval, cv=5, scoring='accuracy')\n        \n#         mean_score = scores.mean()\n        \n#         if mean_score > best_score:\n#             best_score = mean_score\n#             best_c = c\n# print(best_c)","926924da":"best_C = optimal_params.best_params_['C']\nbest_penalty = optimal_params.best_params_['penalty']","d2b4831c":"log_reg_model = LogisticRegression(C=best_C, penalty=best_penalty).fit(X_trainval_scaled, y_trainval)\n\n# for test there are 94 cases\nplot_confusion_matrix(log_reg_model, \n                      X_test_scaled, \n                      y_test, \n                      values_format='d', \n                      display_labels=['Nondemented', 'Demented'])","bda97014":"train_score = 0\ntest_score = 0\ntest_recall = 0\ntest_auc = 0\n\nbest_log_reg_model = LogisticRegression(C=best_C, penalty=best_penalty).fit(X_trainval_scaled, y_trainval)\ntrain_score = best_log_reg_model.score(X_trainval_scaled, y_trainval)\ntest_score = best_log_reg_model.score(X_test_scaled, y_test)\ny_predict = best_log_reg_model.predict(X_test_scaled)\ntest_recall = recall_score(y_test, y_predict)\nlgr_fpr, lgr_tpr, thresholds = roc_curve(y_test, y_predict)\ntest_auc = auc(lgr_fpr, lgr_tpr)\n\nprint(\"Train accuracy with Logistec regression:\", train_score)\nprint(\"Test accuracy with Logistec regression:\", test_score)\nprint(\"Test recall with Logistec regression:\", test_recall)\nprint(\"Test AUC with Logistec regression:\", test_auc)","746d2871":"dt_model = DecisionTreeClassifier().fit(X_trainval_scaled, y_trainval)\n\n# for test there are 94 cases\nplot_confusion_matrix(dt_model, \n                      X_test_scaled, \n                      y_test, \n                      values_format='d', \n                      display_labels=['Nondemented', 'Demented'])","9261675d":"train_score = 0\ntest_score = 0\ntest_recall = 0\ntest_auc = 0\n\ndt_model = DecisionTreeClassifier().fit(X_trainval_scaled, y_trainval)\ntrain_score = dt_model.score(X_trainval_scaled, y_trainval)\ntest_score = dt_model.score(X_test_scaled, y_test)\ny_predict = dt_model.predict(X_test_scaled)\ntest_recall = recall_score(y_test, y_predict)\nfpr, tpr, thresholds = roc_curve(y_test, y_predict)\ntest_auc = auc(fpr, tpr)\n\nprint(\"Train accuracy with DecisionTreeClassifier:\", train_score)\nprint(\"Test accuracy with DecisionTreeClassifier:\", test_score)\nprint(\"Test recall with DecisionTreeClassifier:\", test_recall)\nprint(\"Test AUC with DecisionTreeClassifier:\", test_auc)","6ec69ee7":"param_grid = {'criterion': ['gini'], \n              'max_depth': range(1,10)}\n\noptimal_params = GridSearchCV(DecisionTreeClassifier(),\n                             param_grid,\n                             cv=5, # we are taking 5-fold as in k-fold cross validation\n                             scoring='accuracy', # try the other scoring if have time\n                             verbose=0,\n                             n_jobs=-1)\n\noptimal_params.fit(X_trainval_scaled, y_trainval)\nprint(optimal_params.best_params_)","5c714e4a":"criterion = optimal_params.best_params_['criterion']\nmax_depth = optimal_params.best_params_['max_depth']","8d82fdb8":"# best_score = -1\n# for d in range(1, 25):       \n#         dt_model = DecisionTreeClassifier(max_depth = d)\n#         scores = cross_val_score(dt_model, X_trainval_scaled, y_trainval, cv=5, scoring='accuracy')\n        \n#         mean_score = scores.mean()\n        \n#         if mean_score > best_score:\n#             best_score = mean_score\n#             best_d = d\n# print(best_d)","cd170624":"dt_model = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth).fit(X_trainval_scaled, y_trainval)\n\n# for test there are 94 cases\nplot_confusion_matrix(dt_model, \n                      X_test_scaled, \n                      y_test, \n                      values_format='d', \n                      display_labels=['Nondemented', 'Demented'])","03a7d396":"train_score = 0\ntest_score = 0\ntest_recall = 0\ntest_auc = 0\n\ndt_model = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth).fit(X_trainval_scaled, y_trainval)\ntrain_score = dt_model.score(X_trainval_scaled, y_trainval)\ntest_score = dt_model.score(X_test_scaled, y_test)\ny_predict = dt_model.predict(X_test_scaled)\ntest_recall = recall_score(y_test, y_predict)\ndt_fpr, dt_tpr, thresholds = roc_curve(y_test, y_predict)\ntest_auc = auc(dt_fpr, dt_tpr)\n\nprint(\"Train accuracy with DecisionTreeClassifier:\", train_score)\nprint(\"Test accuracy with DecisionTreeClassifier:\", test_score)\nprint(\"Test recall with DecisionTreeClassifier:\", test_recall)\nprint(\"Test AUC with DecisionTreeClassifier:\", test_auc)","a70a493d":"plt.figure(figsize=(5, 5), dpi=100)\nplt.plot(svm_fpr, svm_tpr, linestyle='-', label='SVM')\nplt.plot(lgr_fpr, lgr_tpr, marker='.', label='Logistic')\nplt.plot(rfc_fpr, rfc_tpr, linestyle=':', label='Random Forest')\nplt.plot(dt_fpr, dt_tpr, linestyle='-.', label='Decision Tree')\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nplt.legend()\n\nplt.show()","dce36d2f":"# SVM","561fa527":"### Optimize parameters(Finetuning) --> GridSearchCV() for SVM","f4dc42dc":"When **inplace = True** , the data is modified in place, which means it will return nothing and the dataframe is now updated. \nWhen **inplace = False** , which is the *default*, then the operation is performed and it returns a copy of the object. You then need to save it to something.","8203919a":"## Load the data","5070544d":"# Decision Tree","f87abb8b":"## Explore the data","316ba1e2":"### With Stratified Sampling","8ff651b3":"### Scale the dataset","3ee2cc91":"set axis=0 for rows or, just put axis='rows' to access the rows\n\nset axis=1 for columns or, just put axis='columns' to access the columns","84ca31f2":"## Prepare the data for X and y where, \n\n1. X = The columns\/features for **making the prediction**\n2. y = The **predicted value**","22d25e68":"### Optimize parameters(Finetuning) --> GridSearchCV()","74f13dbe":"## Correlation Between Attributes","7a0b9014":"## Train-Test Split","c2eca96b":"# Random Forest","01461bea":"## Adding Imports","c0b71d1d":"### Optimize parameters(Finetuning) --> GridSearchCV()","6c87f748":"### Taking median values for the missing values of MMSE","7371c224":"# Logistic Regression","6eebde5b":"### Taking median values for the missing values of SES","c3c8343b":"### Optimize parameters(Finetuning) --> GridSearchCV()","1eafe90c":"## Plot ROC and compare AUC","10207918":"## Data Visualization","762608aa":"## Checking For Missig\/Null Values","07b6061d":"### Train-Test distribution Without Stratified Sampling","a21b4387":"Note: Based on the given data **CDR** is used to tell what the condition of the patient meaning, does the patient has any dementia or, not.\n\nCDR Value Meaning:\n\n* 0 ---> Normal\n* 0.5 ---> Very Mild Dementia\n* 1 ---> Mild Dementia\n* 2 ---> Moderate Dementia\n* 3 ---> Severe Dementia","76755de5":"## Converting Categorical Data to Numerical Data"}}