{"cell_type":{"a97e2a5b":"code","529dad2b":"code","3e4f7759":"code","aa87d6a1":"code","f7eb4680":"code","3bca701b":"code","cd39219b":"code","38a24922":"code","b3c09702":"code","69ea0af0":"code","b7cb9792":"code","dc026385":"code","f862a776":"code","8a6327c6":"code","44f40bda":"code","71cba566":"code","32f32166":"code","8931f1c0":"code","e3a4e14b":"code","54eeec39":"code","b34733f9":"code","c9b1f815":"code","37749109":"code","5bbac2e9":"code","425fd75d":"code","acbf5fb0":"code","e4458e7e":"code","4a81b916":"code","318f4343":"code","1d7300f0":"code","72cda2c0":"code","8c7b316f":"code","9b4df6cf":"code","ba66343d":"code","c9a85149":"code","ff3fa1ae":"code","feeb749d":"code","76ea6748":"code","649d849e":"code","50289a83":"code","7676f129":"code","f1e497e6":"code","f18e28f5":"code","9cc18388":"code","00ab1461":"code","7d1fa969":"code","359953f6":"markdown","84b74cef":"markdown","f691bfb4":"markdown","b853ad8d":"markdown","ca116163":"markdown","1541acc2":"markdown","405be3a2":"markdown","bb7db00a":"markdown","e7178d4d":"markdown","db0e88e1":"markdown","34ee958e":"markdown","77007c1b":"markdown","92b9a2b2":"markdown","8016ab97":"markdown","824586bc":"markdown","816062cc":"markdown","18a54457":"markdown"},"source":{"a97e2a5b":"import pandas as pd\nimport numpy as np\n\npd.set_option('display.max_columns', 60)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom IPython.core.pylabtools import figsize\n# Set default font size\nplt.rcParams['font.size'] = 24\n\nimport seaborn as sns\nsns.set(font_scale = 2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer","529dad2b":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.shape, test.shape","3e4f7759":"data = pd.concat([train, test], sort=False)\ndata.shape","aa87d6a1":"data.head()","f7eb4680":"data.tail()","3bca701b":"data.info()","cd39219b":"for col in list(data.columns):   \n    if ('Class' in col):\n        data[col] = data[col].astype(str)    ","38a24922":"# Statistics for each column\ndata.describe()","b3c09702":"# Function to calculate missing values by column\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","69ea0af0":"missing_values_table(data)","b7cb9792":"missing_df = missing_values_table(data)\nmissing_columns = list(missing_df[missing_df['% of Total Values'] > 50].index)\nprint('We will remove %d columns.' % len(missing_columns))","dc026385":"data = data.drop(columns=missing_columns)","f862a776":"figsize(8, 8)\nplt.style.use('fivethirtyeight')\nplt.hist(data['SalePrice'].dropna(), bins = 100, edgecolor = 'k')\nplt.xlabel('Sale Price'); plt.ylabel('Number of houses')\nplt.title('Sale Price Distribution')","8a6327c6":"# from the figure, we see outliers at the right end\ndata['SalePrice'].describe()","44f40bda":"data['SalePrice'].dropna().sort_values().tail(15)","71cba566":"data.loc[data['SalePrice'] > 700000]","32f32166":"# calculate the first and third quartile\nfirst_quartile = data['SalePrice'].describe()['25%']\nthird_quartile = data['SalePrice'].describe()['75%']\nInterquartile = third_quartile - first_quartile\n\nupper_outlier = data.loc[data['SalePrice'] > third_quartile + 3 * Interquartile].index\nlower_outlier = data.loc[data['SalePrice'] < first_quartile - 3 * Interquartile].index\nlen(list(upper_outlier) + list(lower_outlier))","8931f1c0":"# calculate the first and third quartile\nfirst_quartile = data['GrLivArea'].describe()['25%']\nthird_quartile = data['GrLivArea'].describe()['75%']\nInterquartile = third_quartile - first_quartile\n\nupper_outlier = data.loc[data['GrLivArea'] > third_quartile + 3 * Interquartile].index\nlower_outlier = data.loc[data['GrLivArea'] < first_quartile - 3 * Interquartile].index\nlen(list(upper_outlier) + list(lower_outlier))","e3a4e14b":"data['GrLivArea'].dropna().sort_values().tail(15)","54eeec39":"# Remove the two outliers\ndata.loc[data['GrLivArea'] > 4500, ['GrLivArea', 'SalePrice']]","b34733f9":"data = data.loc[(data['GrLivArea'] < 4500) | (data['SalePrice'].isna())]","c9b1f815":"Zones = data.dropna(subset=['SalePrice'])\nZones = Zones['MSZoning'].value_counts()\nZones = Zones[Zones>50].index","37749109":"'''\nPlot of distribution of Sale Price for zone class \n       A\tAgriculture\n       C\tCommercial\n       FV\tFloating Village Residential\n       I\tIndustrial\n       RH\tResidential High Density\n       RL\tResidential Low Density\n       RP\tResidential Low Density Park \n       RM\tResidential Medium Density\n'''\nfigsize(12, 10)\n\n# Plot each building\nfor zone in Zones:\n    # Select the zone type\n    subset = data[data['MSZoning'] == zone]\n    \n    # Density plot of Sale Price\n    sns.kdeplot(subset['SalePrice'].dropna(),\n               label = zone, shade = False, alpha = 0.8);\n    \n# label the plot\nplt.xlabel('Sale Price', size = 20); plt.ylabel('Density', size = 20); \nplt.title('Density Plot of Sale Price by general zone classification', size = 28);","5bbac2e9":"Nbos = data.dropna(subset=['SalePrice'])\nNbos = Nbos['Neighborhood'].value_counts()\nNbos = Nbos[Nbos>50].index","425fd75d":"# Plot of distribution of Sale Price for neighborhoods\nfigsize(12, 10)\n\nfor Nbo in Nbos:    \n    subset = data[data['Neighborhood'] == Nbo]\n    \n    # Density plot of Sale Price\n    sns.kdeplot(subset['SalePrice'].dropna(),\n               label = Nbo, shade = False, alpha = 0.8);\n    \n# label the plot\nplt.xlabel('Sale Price', size = 20); plt.ylabel('Density', size = 20); \nplt.title('Density Plot of Sale Price by Neighborhoods', size = 28);","acbf5fb0":"# Find all correlations and sort \ncorrelations_data = data.corr()['SalePrice'].sort_values()\n\n# Print the most negative correlations\nprint(correlations_data.head(15), '\\n')\n\n# Print the most positive correlations\nprint(correlations_data.tail(15))","e4458e7e":"# Select the numeric columns\nnum_features = data.select_dtypes('number')\n\n# Create columns with square root and log of numeric columns\nfor col in num_features.columns:\n    # Skip the target column\n    if col == 'SalePrice':\n        next\n    else:\n        num_features['sqrt_' + col] = np.sqrt(num_features[col])\n        num_features['log_' + col] = np.log(num_features[col])\n\n# Drop houses without sale price\nnum_features = num_features.dropna(subset = ['SalePrice'])\n\n# Find correlations with the sale price \ncorrelations = num_features.corr()['SalePrice'].dropna().sort_values()","4a81b916":"# Display most negative correlations\ncorrelations.head(15)","318f4343":"correlations.tail(15)","1d7300f0":"# Select the categorical columns\ncategorical_subset = data.select_dtypes('object')\n\n# One hot encode\ncat_features = pd.get_dummies(categorical_subset)\n\ncat_features['SalePrice'] = data['SalePrice']\n\n# Find correlations with the sale price \ncat_features = cat_features.dropna(subset=['SalePrice'])\ncorrelations = cat_features.corr()['SalePrice'].dropna().sort_values()","72cda2c0":"correlations.head(10)","8c7b316f":"correlations.tail(20)","9b4df6cf":"'''\na rule of thumb for correlation\n0.00-0.19: very weak\n0.20-0.39: weak\n0.40-0.59: moderate \n0.60-0.79: strong\n0.80-1.00: very strong\nhere I choose cutoff values ca. > 0.3 or < -0.3\n'''\ncat_features = list(list(correlations.head(10).index)+list(correlations.tail(20).index))\ncat_features_names = list()\nfor feature in cat_features:\n    cat_features_names.append(feature.split('_')[0])\n    \ncat_features_names = list(set(cat_features_names))\ncat_features_names ","ba66343d":"cat_features = data[cat_features_names]\ncat_features = cat_features.dropna(subset=['SalePrice'])","c9a85149":"cat_features = cat_features.drop(columns=['SalePrice'])\ncat_features_names.remove('SalePrice')\nfeatures = pd.concat([cat_features, num_features],axis=1)\nfeatures.columns","ff3fa1ae":"figsize(12, 10)\n\n# Limit to zone types with more than 50 observations (from previous code)\nfeatures = features[features['MSZoning'].isin(Zones)]\n\n# Use seaborn to plot a scatterplot of Sale Price vs. Overall Quality\nsns.lmplot('OverallQual', 'SalePrice', \n          hue = 'MSZoning', data = features,\n          scatter_kws = {'alpha': 0.8, 's': 60}, fit_reg = False,\n          size = 12, aspect = 1.2);\n\n# Plot labeling\nplt.xlabel(\"Overall Quality\", size = 28)\nplt.ylabel('Sale Price', size = 28)\nplt.title('Overall Quality vs. Sale Price', size = 36);","feeb749d":"figsize(12, 10)\n# Limit to neighborhood with more than 50 observations (from previous code)\nfeatures = features[features['Neighborhood'].isin(Nbos)]\n\n# Use seaborn to plot a scatterplot of sale price vs overall quality\nsns.lmplot('OverallQual', 'SalePrice', \n          hue = 'Neighborhood', data = features,\n          scatter_kws = {'alpha': 0.8, 's': 60}, fit_reg = False,\n          size = 12, aspect = 1.2);\n\n# Plot labeling\nplt.xlabel(\"Overall Quality\", size = 28)\nplt.ylabel('Sale Price', size = 28)\nplt.title('Overall Quality vs. Sale Price', size = 36);","76ea6748":"# Extract the columns to  plot\nplot_data = features[['SalePrice', 'OverallQual', \n                      'GrLivArea', \n                      'TotalBsmtSF']]\n\n# Replace the inf with nan\nplot_data = plot_data.replace({np.inf: np.nan, -np.inf: np.nan})\n\n# Rename columns \nplot_data = plot_data.rename(columns = {'SalePrice': 'Sale Price', \n                                        'OverallQual': 'Overall Quality',\n                                        'GrLivArea': 'living area SF',\n                                        'TotalBsmtSF':'Basement SF'})\n\n# Drop na values\nplot_data = plot_data.dropna()\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3)\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, color = 'red', alpha = 0.6)\n\n# Diagonal is a histogram\ngrid.map_diag(plt.hist, color = 'red', edgecolor = 'black')\n\n# Bottom is correlation and density plot\ngrid.map_lower(corr_func);\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.Reds)\n\n# Title for entire plot\nplt.suptitle('Pairs Plot of Sale Price', size = 36, y = 1.02);","649d849e":"# Copy the original data\nfeatures = data.copy()\n\n# Select the numeric columns\nnumeric_subset = data.select_dtypes('number')\n\n# Create columns with log of numeric columns\nfor col in numeric_subset.columns:\n    # Skip the Sale Price column\n    if col == 'SalePrice':\n        next\n    else:\n        numeric_subset['log_' + col] = np.log(numeric_subset[col]+0.0001)\n        \n# Select the categorical columns\ncategorical_subset = data[cat_features_names]\n\n# fill missing category data before one hot encoding\ncategorical_subset = categorical_subset.fillna(categorical_subset.mode().iloc[0])\n\n# One hot encode\ncategorical_subset = pd.get_dummies(categorical_subset)\n\n# Join the two dataframes using concat\n# Make sure to use axis = 1 to perform a column bind\nfeatures = pd.concat([numeric_subset, categorical_subset], axis = 1)\n\nfeatures.shape","50289a83":"def remove_collinear_features(x, threshold):\n    '''\n    Objective:\n        Remove collinear features in a dataframe with a correlation coefficient\n        greater than the threshold. Removing collinear features can help a model\n        to generalize and improves the interpretability of the model.\n        \n    Inputs: \n        threshold: any features with correlations greater than this value are removed\n    \n    Output: \n        dataframe that contains only the non-highly-collinear features\n    '''\n    \n    # Dont want to remove correlations between Sale Price\n    y = x['SalePrice']\n    x = x.drop(columns = ['SalePrice'])\n    \n    # Calculate the correlation matrix\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i):\n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n            \n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n                # print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])\n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    x = x.drop(columns = drops)\n    \n    \n    # Add the target back in to the data\n    x['SalePrice'] = y\n               \n    return x","7676f129":"# Remove the collinear features above a specified correlation coefficient\nfeatures = remove_collinear_features(features, 0.6);","f1e497e6":"# Remove any columns with all na values\nfeatures  = features.dropna(axis=1, how = 'all')\nfeatures.shape","f18e28f5":"no_price = features[features['SalePrice'].isna()]\nprice = features[features['SalePrice'].notnull()]\n\nprint(no_price.shape)\nprint(price.shape)","9cc18388":"# Separate out the features and targets\nfeatures = price.drop(columns='SalePrice')\ntargets = pd.DataFrame(price['SalePrice'])\n\n# Replace the inf and -inf with nan (required for later imputation)\nfeatures = features.replace({np.inf: np.nan, -np.inf: np.nan})\n\n# Split into 80% training and 20% testing set\nX, X_test, y, y_test = train_test_split(features, targets, test_size = 0.2, random_state = 42)\n\nprint(X.shape)\nprint(X_test.shape)\nprint(y.shape)\nprint(y_test.shape)","00ab1461":"# Function to calculate mean absolute error\ndef mae(y_true, y_pred):\n    return np.mean(abs(y_true - y_pred))","7d1fa969":"baseline_guess = np.median(y)\n\nprint('The baseline guess of median price %0.2f' % baseline_guess)\nprint(\"Baseline Performance on the test set: MAE = %0.4f\" % mae(y_test, baseline_guess))","359953f6":"## Imports\n\nWe will use the standard data science and machine learning libraries: numpy, pandas, and scikit-learn. We also use matplotlib and seaborn for visualization.\n","84b74cef":"##### select categorical features, which has good correlations with the predictant","f691bfb4":"#### Removing Outliers\nThe definition of an extreme outlier:\n\nOn the low end, an extreme outlier is below First Quartile\u22123\u2217Interquartile Range\n\nOn the high end, an extreme outlier is above Third Quartile+3\u2217Interquartile Range\n\nAfter checking the relation between SalePrice and OverallQual (in the next section) as well as with GrLivArea, I decide to keep outliers in SalePrice but remove two over the upper boundary outliers in GrLivArea. ","b853ad8d":"##    Housing Price - 1 \n### In this notebook, we will carry out the first three steps of a machine learning problem:\n\n-    Cleaned and formatted the raw data\n-    Performed an exploratory data analysis\n-    Developed a set of features to train our model using feature engineering and feature selection\n","ca116163":"#### Correlations between Features and Target","1541acc2":"### Data Cleaning and Formatting\n##### Load in the Data and Examine","405be3a2":"# Feature Engineering and Selection\n\n- Feature Engineering: to extract or create new features from the raw data. This might mean taking transformations of variables, such as the log and square root, or one-hot encoding categorical variables. \n- Feature Selection: to choose the most relevant features. It might be something as simple as the highest correlation with the target, or the features with the most variance. \n\nIn this project, I will\n- Select all the numerical variables and categorical variables which significantly correlated with the predictant\n- Add in the log transformation of the numerical variables, One-hot encode the categorical variables\n\nFor feature selection, I will:\n- Remove collinear features","bb7db00a":"### Remove Collinear Features","e7178d4d":"#### Pairs Plot","db0e88e1":"#### Looking for Relationships","34ee958e":"## Missing Values","77007c1b":"# Establish a Baseline","92b9a2b2":"## Convert Data to Correct Types","8016ab97":"## Data Types and Missing Values","824586bc":"## Exploratory Data Analysis\n\n#### Single variable plots","816062cc":"#### Two-Variable Plots","18a54457":"##### select all numerical variables"}}