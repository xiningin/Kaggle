{"cell_type":{"7a470d59":"code","96a342e5":"code","5023a725":"code","f7f49bbe":"code","94588c16":"code","9f6364ed":"code","ded24444":"code","867ad6c4":"code","cfa22bb9":"code","ab1a1427":"code","c1a2eb24":"code","3d95826e":"code","b3fb80a0":"code","833ac3f6":"code","c757b237":"code","3e49f769":"code","fc05b567":"code","3963192c":"code","306f2571":"code","67f9f38a":"code","accdbcef":"code","d5f3c087":"code","c2fc038f":"code","3568b8d0":"code","059739a6":"code","c64c5527":"code","6b485c38":"code","67d9f0b4":"code","849ebca2":"markdown","22ccf3a5":"markdown","bfca1fca":"markdown","9f780bc7":"markdown","133acd08":"markdown","d35cf271":"markdown","9f5a695b":"markdown","0b1b18f6":"markdown","ee603cc3":"markdown","1fbed5c3":"markdown","bca5261a":"markdown","0b9c53b2":"markdown","84826543":"markdown","2fc43ca9":"markdown","eed55851":"markdown","29f6318f":"markdown","69b7dfb4":"markdown","9a2596dd":"markdown","70810328":"markdown","e445d24a":"markdown","dc566972":"markdown","350ac78e":"markdown","321693ce":"markdown","297372d9":"markdown","d5441b44":"markdown","3bff11e4":"markdown","e9513326":"markdown","1a58ca8d":"markdown"},"source":{"7a470d59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","96a342e5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.cluster import KMeans\nfrom numpy import sqrt, random, array, argsort\nfrom sklearn.preprocessing import scale\n\ndata=pd.read_csv(\"\/kaggle\/input\/abalone-dataset\/abalone.csv\")\ndata\n","5023a725":"data.columns","f7f49bbe":"data.info()","94588c16":"df=data.drop(columns=['Sex'],axis=1)\ndf","9f6364ed":"from collections import Counter\ndef detection(df,features):\n    outlier_indices=[]\n    \n    for c in features:\n        #1st quartile\n        Q1 = np.percentile(df[c],25)\n        \n        #3rd quartile\n        Q3 = np.percentile(df[c],75)\n        \n        #IQR calculation\n        IQR = Q3 - Q1\n        outlier_step = IQR * 1.5\n        lower_range = Q1 - (outlier_step)\n        upper_range = Q3 + (outlier_step)\n        \n        #Outlier detection                                    #Outlier indexes\n        outlier_list_col=df[  (df[c] < lower_range) | (df[c] > upper_range)  ].index\n       \n        #Store indexes\n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices=Counter(outlier_indices)\n    # number of outliers\n    # If we have more then 2 outliers in a sample, this sample ll be drop\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2 )\n    #we are taking indexes\n    \n    return multiple_outliers","ded24444":"df.columns=['length', 'diameter', 'height', 'weight.w', 'weight.s',\n       'weight.v', 'weight.sh', 'rings']\ndf.info()","867ad6c4":"outliers=detection(df,[\"length\",\"weight.w\",\"height\",\"diameter\"])\n                       \ndf.loc[outliers]","cfa22bb9":"df=df.drop(outliers,axis=0).reset_index(drop = True)\ndf","ab1a1427":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.cluster import KMeans\nfrom numpy import sqrt, random, array, argsort\nfrom sklearn.preprocessing import scale\n","c1a2eb24":"df.info()\n","3d95826e":"sns.pairplot(df)\n","b3fb80a0":"from sklearn import preprocessing\n\nscaled_preprocessing=preprocessing.scale(df)\nscaled_preprocessing","833ac3f6":"from scipy.stats import zscore\nscaled = df.apply(zscore)\nscaled.head()","c757b237":"from sklearn.cluster import KMeans\nwcss=[] #liste olu\u015ftur\ncluster_range=range(1,10)\nfor k  in cluster_range:\n    kmeans=KMeans(n_clusters=k)\n    kmeans.fit(scaled)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot(cluster_range,wcss,marker='x')\nplt.xlabel(\"number of k(cluster)\")\nplt.ylabel(\"WCSS\")\nplt.title(\"Elbow Method for optimal number of clusters\")\nplt.show()","3e49f769":"clusters_df = pd.DataFrame({'clusters':cluster_range,\n                            'inertia': wcss})\nclusters_df\n","fc05b567":"kmeans=KMeans(n_clusters = 3)\nkmeans.fit(scaled)","3963192c":"centroids = kmeans.cluster_centers_\ncentroids\n","306f2571":"centroid_df = pd.DataFrame(centroids,columns = list(scaled.columns))\ncentroid_df","67f9f38a":"clusters=scaled.copy()\nclusters['cluster_pred']=kmeans.fit_predict(scaled)\nscaled[\"labels\"]=clusters['cluster_pred']\nscaled","accdbcef":"sns.pairplot(scaled,hue = 'labels')\n","d5f3c087":"#\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(10,8))\nax = Axes3D(fig,rect = [0,0,1,1],elev = 10,azim = 120)\nlabels = kmeans.labels_\nax.scatter(scaled.iloc[:,0],scaled.iloc[:,2],scaled.iloc[:,3],c = labels.astype(np.float),edgecolor = 'k')\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel('Length')\nax.set_ylabel('Weight sh')\nax.set_zlabel('Height')\nax.set_title('3D plot for KMeans Clustering')\n\n","c2fc038f":"from sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","3568b8d0":"data=pd.read_csv(\"\/kaggle\/input\/abalone-dataset\/abalone.csv\")\ndf=data.drop(columns=['Sex'],axis=1)\ndf.columns=['length', 'diameter', 'height', 'weight.w', 'weight.s',\n       'weight.v', 'weight.sh', 'rings']\ndf.info()","059739a6":"lenWeightSh=df[[\"length\",\"weight.sh\"]]\nlenWeightSh","c64c5527":"plt.scatter(df['length'],df['weight.sh'],marker=\"o\")\nplt.xlabel(\"length\",fontsize=12)\nplt.ylabel(\"Shell Weight\",fontsize=12)\nplt.title('Length & Shell Weight', fontsize=16)\nplt.show()","6b485c38":"#  lenWeightSh\nfrom sklearn.cluster import DBSCAN\noutlier_detection = DBSCAN(\n  eps = 0.05,\n  metric=\"euclidean\",\n  min_samples = 15,\n  n_jobs = -1)\nclusters = outlier_detection.fit_predict(lenWeightSh)\n\nclusters","67d9f0b4":"from matplotlib import cm\ncmap = cm.get_cmap('Accent')\nlenWeightSh.plot.scatter(\n  x = \"length\",\n  y = \"weight.sh\",\n  c = clusters,\n  cmap = cmap,\n  colorbar = False,\n  marker=\"o\"\n)\n","849ebca2":"<a id=\"4\"><\/a><\/br>\n### Detection Method\nWe apply the formulas. Our parameters df and features. You can write your dataset name instead of \"df\", and \"features\" section which properties you wanna detect outliers.","22ccf3a5":"I ll add other techniques about detection of outliers.","bfca1fca":"Load the data:","9f780bc7":"#### Elbow Rule\n\nThe Elbow Rule is one of the most popular methods, when determining the optimal number of clusters.\n\n- **Distortion :** It is calculated as the average of the squared distances from the cluster centers of the respective clusters. Typically, the Euclidean distance metric is used.\n- **Inertia(WCSS):** It is the sum of squared distances of samples to their closest cluster center.\n![resim.png](attachment:resim.png)","133acd08":"We get rid of outliers. Now we have 4137 rows in our dataset.<br>\nYou can also look:\n[ https:\/\/medium.com\/@coimer\/outlier-5f4cf2ba681e](http:\/\/)\n","d35cf271":"We find the centroids, show them in list","9f5a695b":"Change the column names. Some blanks may cause trouble","0b1b18f6":"Let's learn,general info about our dataset","ee603cc3":"If you wanna find more outliers you can decrease the value of epsilon.(it means your samples should be closer to each other.)\n","1fbed5c3":"You can do same thing with zscore","bca5261a":"#### What is KMeans Clustering?\n**Clustering** is identifying subgroups in the data such that data points in the same subgroup (cluster) are very similar while data points in different clusters are very different.<br> \nKMeans assigns data points to a cluster such that the sum of the squared distance between the data points(Euclidean distance) and the cluster\u2019s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.<br>\nThe logic of finding centroids:\n![kmeansViz.png](attachment:kmeansViz.png)\nhttps:\/\/medium.com\/@coimer\/k-means-algoritmas%C4%B1-330caca20e\n\n\n","0b9c53b2":"Add libraries, and load the data","84826543":"Our method is ready, now search outliers ","2fc43ca9":"Pairplot allows us to see the distribution of a single variable while the scatter plots on the upper and lower triangles show the relationship (or lack thereof) between two variables.\nFor instance, the left-most plot in the seventh row shows the scatter plot of length versus diameter.","eed55851":"# Introduction\nWe re goin to find the outliers with three methods.\n(IQR, KMeans, DBSCAN)\n\n<font color = 'blue'>\nContent:\n    \n1. [What is Outlier?](#1)\n2. [Outlier Detection](#2)\n    - 2.1. [Interquartile Range Method](#3)\n    -      [Detection Method](#4)\n    - 2.2. [KMeans Clustering Method](#5)\n    - 2.3. [DBSCAN Method](#6)    \n","29f6318f":"<a id=\"4\"><\/a><\/br>\n**Scaling** <br>\n- Distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.<br>\n- Since both the features have different scales, there is a chance that higher weightage is given to features with higher magnitude. This will impact the performance of the machine learning algorithm and obviously, we dont want our algorithm to be biassed towards one feature.<br>\n- **Therefore, we scale our data before employing a distance based algorithm so that all the features contribute equally to the result.**<br>","69b7dfb4":"<a id=\"6\"><\/a><\/br>\n## 2.3. DBSCAN Method:\n\nLets try another method for clustering,**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm commonly used for outlier detection.\n\n> Unlike K-Means, DBSCAN does not require the number of clusters as a parameter. Rather it infers the number of clusters based on the data, and it can discover clusters of arbitrary shape.\n\n\n**Algorithm**<br>\n\n* Pick a point at random that has not been assigned to a cluster or been designated as an outlier. Compute its neighborhood to determine if it\u2019s a core point. If yes, start a cluster around this point. If no, label the point as an outlier.\n* Once we find a core point and thus a cluster, expand the cluster by adding all directly-reachable points to the cluster. Perform \u201cneighborhood jumps\u201d to find all density-reachable points and add them to the cluster. If an outlier is added, change that point\u2019s status from outlier to border point.\n* Repeat these two steps until all points are either assigned to a cluster or designated as an outlier.\n\n![400px-DBSCAN-Illustration.svg.png](attachment:400px-DBSCAN-Illustration.svg.png)\n\n> More info: https:\/\/www.youtube.com\/watch?v=C3r7tGRe2eI","9a2596dd":"Here is a exact number of inertias","70810328":"As the slope starts becoming constant after k = 3, we will choose value of k as 3 for our kmeans modelling.\n","e445d24a":"Looking at the pairplot above we can see that we have 3 different classes, where class 1 represents INFANT, class 0 represents FEMALES and class 2 represents the MALES.\n\n\n\n0=FEMALES (blue)<br>\n1=INFANT (orange)<br>\n2=MALES (green)<br>","dc566972":"DBSCAN Algorithm requires 2 parameters:<br>\n- Epsilon :how close points should be to each other to be considered a part of a cluster\n- MinPts :how many neighbors a point should have to be included into a cluster.<br><br>\nIn this example, DBSCAN object that requires a minimum of 15 data points in a neighborhood of radius 0.05 to be considered a core point.","350ac78e":"<a id=\"3\"><\/a><\/br>\n## 2.1. Interquartile Range Method\nThe interquartile range rule is useful in detecting the presence of outliers. Outliers are individual values that fall outside of the overall pattern of a data set.\n\n![iqr.webp](attachment:iqr.webp)\n\nFirst we are gonna find the first quartile Q1, which represents a quarter of the way through the list of all data. Then find the third quartile Q3, which represents three-quarters of the way through the list of all data<br>\n\nWe need to find the median of the data set, which represents the midpoint of the whole list of data.<br>\n\nFinally we can find the upper and lower bond of our data with these formulas:<br>\n- IQR = Q3 - Q1\n- **Q1 -(1.5 x IQR)**  (lower range) \n- **Q3 +(1.5 x IQR)**  (upper range)<br>\n\nWhen you reach the knowledge of upper and lower range, you can detect the outliers. Clean the data from outliers, to sum up you can do right analysis about ur data.<br>\n\nNow, we ll apply interquartile range method on abalone dataset.","321693ce":"We are dealing with numerical data, we should drop the gender section.","297372d9":"<a id=\"2\"><\/a><\/br>\n# 2. Outlier Detection\nThere are several methods that used for detection of outliers. First of all, we gonna talk about Interquartile Range Method.\n","d5441b44":"We labeled data, add labels row in our dataset","3bff11e4":"Drop the outliers and reset the indexes","e9513326":"<a id=\"5\"><\/a>\n## 2.2. KMeans Clustering Method\n","1a58ca8d":"<a id=\"1\"><\/a><\/br>\n# 1. What is Outlier?\nOutliers are data points that are far from other data points. In other words, they\u2019re extreme values that are outside the range of what is expected and unlike the other data. Outliers are problematic for many statistical analyses because they can cause tests to either miss significant findings or distort real results.\n![r.jpeg](attachment:r.jpeg)\n"}}