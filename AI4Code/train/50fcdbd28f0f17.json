{"cell_type":{"1184e4ff":"code","5a5a6320":"code","70ccff56":"code","ceb83561":"code","6e91b9ba":"code","7aea6dc8":"code","e035365d":"code","229e3829":"code","fb781399":"code","1b56ffd2":"code","10f30b6f":"code","e90f0de2":"code","b964b736":"code","01bfa645":"code","d1cd2de3":"code","de8dc6cb":"code","fc63d4cc":"code","c96e603f":"code","87ca2169":"code","b8489d0b":"code","22177953":"code","38bb908e":"code","5ce19f8b":"code","f35d8f9f":"code","376488d0":"code","7b03a614":"code","f131bdf7":"code","9e6f4aee":"code","545515e9":"code","dcabeba8":"code","51b1c959":"code","1f1cc134":"code","fd26da6f":"code","dff16687":"code","f5d46d33":"code","033f0363":"code","7aec93a6":"code","d94cd043":"code","a0486e39":"code","9b0ab329":"code","11fa8831":"code","f1108994":"code","d2dd5a77":"code","db89c1a5":"markdown","51921a54":"markdown","5aa7ee35":"markdown","b0bc8e7f":"markdown","6dcd89c8":"markdown","251f249e":"markdown","ffb26634":"markdown","74f730cc":"markdown","81437779":"markdown","685b79c5":"markdown","1d35771f":"markdown","256389fb":"markdown","04d704b7":"markdown","227fa042":"markdown","78aaf1fb":"markdown","1bc89f98":"markdown","13adeb6e":"markdown","d122aa27":"markdown","28281b45":"markdown","c58e0524":"markdown","e8507afc":"markdown","68f123fa":"markdown","0564ec62":"markdown","c8e06750":"markdown","7aa49969":"markdown","cf2de2ba":"markdown"},"source":{"1184e4ff":"# Lets import some packages\n\nimport numpy as np\nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport random\nimport spacy\nimport nltk\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom catboost import CatBoostClassifier\n\n\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom tqdm.notebook import tqdm\nfrom transformers import BertTokenizer, AutoTokenizer\nfrom torch.utils.data import TensorDataset\nfrom transformers import BertForSequenceClassification, AutoModel, AutoModelForSequenceClassification\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import TextClassificationPipeline\n\n\n","5a5a6320":"# Define paths for train, test, and sample solution. \n\ntrain_path = os.path.join(\"..\/input\/learn-ai-bbc\/\", \"BBC News Train.csv\")\ntest_path = os.path.join(\"..\/input\/learn-ai-bbc\/\", \"BBC News Test.csv\")\nsample_sol_path = os.path.join(\"..\/input\/learn-ai-bbc\/\", \"BBC News Sample Solution.csv\")","70ccff56":"train_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\nsample_sol_df = pd.read_csv(sample_sol_path)","ceb83561":"train_df.info()","6e91b9ba":"test_df.info()","7aea6dc8":"train_df.Category.value_counts().to_frame()","e035365d":"# Get the percentage of every class in the whole training data. \ncategory_percentage = (100*train_df.Category.value_counts()\/len(train_df)).to_frame().reset_index()\ncategory_percentage.columns = ['category', 'percentage']\n\n\n# Visualize the output\nplt.figure(figsize=(15,8))\nchart = sns.barplot(x='category', y='percentage', data=category_percentage)\nchart.set_title('Distrbution per class')\nchart.set_xticklabels(chart.get_xticklabels(), rotation=30)\n\nfor p in chart.patches:\n             chart.annotate(\"%0.2f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                 ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                 textcoords='offset points')","229e3829":"train_df['#tokens'] = train_df['Text'].apply(lambda x : len(x.split()))\ntest_df['#tokens'] = test_df['Text'].apply(lambda x : len(x.split()))","fb781399":"plt.figure(figsize=(11, 8))\nplt.title('#tokens dist. in train', fontdict={'fontsize':14})\nsns.boxplot(data=train_df, x='#tokens')\n","1b56ffd2":"plt.figure(figsize=(11, 8))\nplt.title('#tokens dist. in test', fontdict={'fontsize':14})\nsns.boxplot(data=test_df, x='#tokens')","10f30b6f":"# Generate the summarized dataframe\navg_tokens_per_cat = train_df.groupby('Category')['#tokens'].mean().to_frame().reset_index()\n# Plot results\nplt.figure(figsize=(15,8))\nchart = sns.barplot(x='Category', y='#tokens', data=avg_tokens_per_cat)\nchart.set_title('Average #tokens per Category')\nchart.set_xticklabels(chart.get_xticklabels(), rotation=30)\n\nfor p in chart.patches:\n             chart.annotate(\"%0.2f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                 ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                 textcoords='offset points')","e90f0de2":"# Generate the summarized dataframe\nmax_tokens_per_cat = train_df.groupby('Category')['#tokens'].max().to_frame().reset_index()\n# Plot the results\nplt.figure(figsize=(15,8))\nchart = sns.barplot(x='Category', y='#tokens', data=max_tokens_per_cat)\nchart.set_title('Max. #tokens per Category')\nchart.set_xticklabels(chart.get_xticklabels(), rotation=30)\n\nfor p in chart.patches:\n             chart.annotate(\"%0.2f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                 ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n                 textcoords='offset points')","b964b736":"# Get all uninque classes\npossible_labels = train_df['Category'].unique()\n# Generate a dict were every class map to an id\nlabel_dict = {}\nfor index, possible_label in enumerate(possible_labels):\n    label_dict[possible_label] = index\n# Get a new encoded category column (category_id)\ntrain_df['category_id'] = train_df.Category.map(label_dict)","01bfa645":"N_FOLDS = 5\ntrain_df[\"kfold\"] = -1\ny = train_df.category_id.values\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(skf.split(X=train_df, y=y)):\n    train_df.loc[valid_indicies, \"kfold\"] = fold\n    \ntrain_df.kfold.value_counts()","d1cd2de3":"def run_estimator_on_folds(estimator,model_name, folds):\n    \"\"\"\n    Run any sklearn estimatior on our data \n    - params\n        - param estimator : sklearn estimator\n        - param model_name : str \n        - param folds : int -> number of folds\n    - return\n        - valid_df : dataframe with predictions to all training data\n        - test_df : dataframe with predictions to all test data. \n    \"\"\"\n    \n    acc_list = []\n    valid_dfs = []\n    test_preds = []\n    \n    for fold in range(folds):\n        df_train =  train_df[train_df.kfold != fold].reset_index(drop=True)\n        df_valid = train_df[train_df.kfold == fold].reset_index(drop=True)\n        df_test = test_df.copy()\n\n        ytrain = df_train.category_id\n        yvalid = df_valid.category_id\n\n        xtrain = df_train.Text\n        xvalid = df_valid.Text\n        \n        xtest = df_test.Text \n\n        tfv = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 1), stop_words='english')\n        tfv.fit(xtrain)\n\n        xtrain = tfv.transform(xtrain)\n        xvalid = tfv.transform(xvalid)\n        xtest = tfv.transform(xtest)\n        \n        estimator.fit(xtrain, ytrain)\n\n        ypred = estimator.predict(xvalid)\n        test_ypred = estimator.predict(xtest)\n        df_valid.loc[:, str(model_name)+'_pred'] = ypred\n        test_preds.append(test_ypred)\n        valid_dfs.append(df_valid)\n        acc = accuracy_score(yvalid, ypred)\n        acc_list.append(acc)\n        print(\"fold: {}, Accuracy: {}\".format(fold, acc))\n\n    print()\n    print(\"Mean Acc: {}, std Acc: {}\".format(np.mean(acc_list), np.std(acc_list)))\n    print()\n    df_valid_final = pd.concat(valid_dfs)\n    test_ypred = np.round(np.mean(np.column_stack(test_preds), axis=1))\n    df_test.loc[:, str(model_name)+'_test_pred'] = test_ypred\n    return df_valid_final, df_test\n    ","de8dc6cb":"models = [('LR', LogisticRegression(n_jobs=-1)),\n           ('SGD', SGDClassifier(n_jobs=-1)),\n           ('Ridge', RidgeClassifier()), \n           ('MultinomialNB', MultinomialNB()), \n           ('SVM', SVC()),\n           ('RandomForest', RandomForestClassifier(n_estimators=200)), \n           ('GradientBoosting', GradientBoostingClassifier()), \n           ('CatBoost', CatBoostClassifier(iterations=1000, depth=5, subsample=.2,\n                                   l2_leaf_reg=1,\n                                   verbose = False,\n                                   bootstrap_type=\"Bernoulli\",\n                                   learning_rate=0.02,\n                                   task_type=\"GPU\" ))]","fc63d4cc":"for (model_name, model) in models:\n    print(30*'='+'  '+model_name+'  '+'='*30)\n    run_estimator_on_folds(model_name=model_name, estimator=model, folds = N_FOLDS)","c96e603f":"\ndef preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n    '''\n    Preprocess a string.\n    :parameter\n        :param text: string - name of column containing text\n        :param lst_stopwords: list - list of stopwords to remove\n        :param flg_stemm: bool - whether stemming is to be applied\n        :param flg_lemm: bool - whether lemmitisation is to be applied\n    :return\n        cleaned text\n    '''\n    ## clean (convert to lowercase and remove punctuations and   \n    # characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    ## Tokenize (convert from string to list)\n    lst_text = text.split()\n    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text","87ca2169":"en = spacy.load('en_core_web_sm')\nstopwords = en.Defaults.stop_words\nlst_stopwords = list(stopwords)","b8489d0b":"train_df['Text'] = train_df.Text.apply(lambda x: \n          preprocess_text(x, flg_stemm=False, flg_lemm=True, \n          lst_stopwords=lst_stopwords))\n\ntest_df['Text'] = test_df.Text.apply(lambda x: \n          preprocess_text(x, flg_stemm=False, flg_lemm=True, \n          lst_stopwords=lst_stopwords))","22177953":"train_dfs, test_dfs = [], []\nfor (model_name, model) in models:\n    print(30*'='+'  '+model_name+'  '+'='*30)\n    train_temp, test_temp = run_estimator_on_folds(model_name=model_name, estimator=model, folds = N_FOLDS)\n    train_dfs.append(train_temp)\n    test_dfs.append(test_temp)\n\n    \ntrain = pd.concat(train_dfs, axis = 1)\ntest = pd.concat(test_dfs, axis = 1)\ntrain = train.loc[:,~train.columns.duplicated()]\ntest = test.loc[:,~test.columns.duplicated()]\n","38bb908e":"acc_list = []\ntest_preds = []\n\n\nfor fold in range(N_FOLDS):\n\n    df_train =  train[train.kfold != fold].reset_index(drop=True)\n    df_valid = train[train.kfold == fold].reset_index(drop=True)\n    df_test = test.copy()\n\n    ytrain = df_train.category_id\n    yvalid = df_valid.category_id\n\n    xtrain = df_train[[col for col in df_train.columns if 'pred' in col]].values\n    xvalid = df_valid[[col for col in df_valid.columns if 'pred' in col]].values\n\n    \n    ypreds = np.round((1*df_valid['Ridge_pred']+\n                      1*df_valid['SGD_pred'])\/2)\n    \n    test_ypred = np.round((1*df_test['Ridge_test_pred']+\n                          1*df_test['SGD_test_pred'])\/2)\n    test_preds.append(test_ypred)\n    acc = accuracy_score(yvalid, ypreds)\n    acc_list.append(acc)\n    print(\"fold: {}, Accuracy: {}\".format(fold, acc))\n\nprint()\nprint(\"Mean Acc: {}, std Acc: {}\".format(np.mean(acc_list), np.std(acc_list)))\nprint()\ntest_ypred = np.round(np.mean(np.column_stack(test_preds), axis=1))\ndf_test.loc[:, 'overall_test_pred'] = test_ypred","5ce19f8b":"df_test = df_test[['ArticleId', 'overall_test_pred']]\nid_to_cat = {value:key for (key,value) in label_dict.items()}\ndf_test['Category'] = df_test.overall_test_pred.map(id_to_cat)\ndf_test = df_test[['ArticleId', 'Category']]\ndf_test.to_csv('submission_blended.csv', index=False)","f35d8f9f":"train_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\nsample_sol_df = pd.read_csv(sample_sol_path)","376488d0":"train_df['label'] = train_df['Category'].replace(label_dict)","7b03a614":"# Splite the data\nX_train, X_val, y_train, y_val = train_test_split(train_df.index.values, \n                                                  train_df.label.values, \n                                                  test_size=0.1, \n                                                  random_state=42, \n                                                  stratify=train_df.label.values)\n\n# Specify the data_type (train and val)\ntrain_df['data_type'] = ['not_set']*train_df.shape[0]\n\ntrain_df.loc[X_train, 'data_type'] = 'train'\ntrain_df.loc[X_val, 'data_type'] = 'val'\n\ntrain_df.groupby(['Category', 'label', 'data_type']).count()","f131bdf7":"# Intiate a bert tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n                                          do_lower_case=True)\n\n# Encode the train dataq\n# Truncation is a must scince we know that we have sequences larger than 512\nencoded_data_train = tokenizer.batch_encode_plus(\n    train_df[train_df.data_type=='train'].Text.values, \n    add_special_tokens=True, \n    return_attention_mask=True, \n    padding='max_length', \n    truncation = True,\n    max_length=512, \n    return_tensors='pt'\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    train_df[train_df.data_type=='val'].Text.values, \n    add_special_tokens=True, \n    return_attention_mask=True, \n    padding='max_length', \n    truncation = True,\n    max_length=512, \n    return_tensors='pt'\n)\n\n\n\ninput_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(train_df[train_df.data_type=='train'].label.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(train_df[train_df.data_type=='val'].label.values)\n\n\n\n","9e6f4aee":"# instantiate a bert model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                      num_labels=len(label_dict),\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)","545515e9":"# Prepare tensor datasets\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)","dcabeba8":"len(dataset_train), len(dataset_val)\n","51b1c959":"batch_size = 3\n\ndataloader_train = DataLoader(dataset_train, \n                              sampler=RandomSampler(dataset_train), \n                              batch_size=batch_size)\n\ndataloader_validation = DataLoader(dataset_val, \n                                   sampler=SequentialSampler(dataset_val), \n                                   batch_size=batch_size)","1f1cc134":"optimizer = AdamW(model.parameters(),\n                  lr=1e-5, \n                  eps=1e-8)","fd26da6f":"epochs = 5\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,\n                                            num_training_steps=len(dataloader_train)*epochs)","dff16687":"def f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')\n\ndef accuracy_per_class(preds, labels):\n    label_dict_inverse = {v: k for k, v in label_dict.items()}\n    \n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}\/{len(y_true)}\\n')","f5d46d33":"# Seeding all\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","033f0363":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nprint(device)","7aec93a6":"def evaluate(dataloader_val):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_val:\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total\/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals","d94cd043":"for epoch in tqdm(range(1, epochs+1)):\n    \n    model.train()\n    \n    loss_train_total = 0\n\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n\n        model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }       \n\n        outputs = model(**inputs)\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()\/len(batch))})\n         \n        \n    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total\/len(dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (Weighted): {val_f1}')","a0486e39":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                      num_labels=len(label_dict),\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)\n\nmodel.to(device)\n\nmodel.load_state_dict(torch.load('finetuned_BERT_epoch_5.model', map_location=torch.device('cpu')))\n\n\n\nmodel.save_pretrained(\"finetuned_bbc_news_model\")\ntokenizer.save_pretrained(\"finetuned_bbc_news_model_tokenizer\")\n\n_, predictions, true_vals = evaluate(dataloader_validation)\naccuracy_per_class(predictions, true_vals)","9b0ab329":"model = AutoModelForSequenceClassification.from_pretrained('.\/finetuned_bbc_news_model')\ntokenizer = AutoTokenizer.from_pretrained('.\/finetuned_bbc_news_model_tokenizer', return_tensors = 'pt')","11fa8831":"pipe = TextClassificationPipeline(model = model, \n                                  tokenizer = tokenizer, \n                                  framework = 'pt', \n                                  function_to_apply = 'softmax')","f1108994":"preds = []\nfor row in test_df.itertuples():\n    article_txt = row.Text.split()\n    article_txt = article_txt[:300]\n    article_txt = ' '.join([w for w in article_txt])\n    pred = pipe(article_txt)\n    preds.append(pred[0]['label'].split('_')[1])","d2dd5a77":"test_df['cat_id'] = preds\ntest_df['Category'] = test_df.cat_id.apply(lambda x : id_to_cat[int(x)])\n# Prepare submittion \ntest_df[['ArticleId', 'Category']].to_csv('submission_bert.csv', index = False)","db89c1a5":"This submittion scored `98.12%` on test set which is a huge imporvement","51921a54":"### EDA\n\nIn the following cells, I will try to do some explanatory data analysis to investigate the datasets more!","5aa7ee35":"It's obvious that linear models perform better than tree-based models and even boosted trees models. \n\nNow, we will try to do some preprocessing and see if it enhance the performance. ","b0bc8e7f":"The mean accuracy of the blended model in `97.78%`","6dcd89c8":"We have almost equal distrbution classes!","251f249e":"Now, lets build a function to evalutate our model.","ffb26634":"This submittion scored `97.5%` for the test set","74f730cc":"Also, the max number of tokens in every class can be useful","81437779":"It seems like preprocessing helped a lot!\nLet's have to make a submittion\n\n\nAs you can see, the `SGDClassifier` and `RidgeClassifier` have provided above `98%` so, I will try to make a `Model Blending` between them and submit. ","685b79c5":"Now, lets create a pipeline for prediction on test data","1d35771f":"To reduce the effect on any possible overfitting (espcially that the data size is very small), I will create a `StratifiedKFols` will traing and make sure to use the folds correctly to avoid any possible `data leakage` ","256389fb":"### Introduction\n\nIn this notebook, I will try to address the problem of sentiment classification using different approachs that will be diversified between conventional machine learning and deep learning models.\n\nLets start!","04d704b7":"Let's see the average number of tokens for every class. ","227fa042":"Here, we got 5 equally distributed folds","78aaf1fb":"Articles, in general, in both train and test have the same distrbution of the number of tokens. However, we can expect longer sequences in future data because the test set has some longes sequences.","1bc89f98":"We have 5 clases in our problem.","13adeb6e":"`tech` articles tend to have the largest number of average words in article. ","d122aa27":"Now, let's evaluate our model.","28281b45":"Now, I will create a list of tubles that contain a list of models diversified between linear models, tree-based, and boosted tree-based models. \n\n\nNOTE: No fine tunining is done","c58e0524":"`plotics` tend to have the longest articles.","e8507afc":"Now, I will think more about the number of words (tokens) in every article. number of tokens will really matter espcially in deep learning approaches.","68f123fa":"Now, lets train all these models and see. ","0564ec62":"### BERT\n\n\nLets read a fresh copy of the datasets","c8e06750":"Now, I will create a function to run any estimator on our data and get back the accuracy for every folds and the final validation and test prediction. ","7aa49969":"### Preprocessing and Modelling\n\nIn this part, I will go through different aproaches such as encoding, verctorizing text, and text preprocessing. In addition to training different models","cf2de2ba":"#### Traning Loop"}}