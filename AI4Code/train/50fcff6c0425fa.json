{"cell_type":{"f6be5b83":"code","fec9e6a2":"code","e5afbdc7":"code","fd8ec936":"code","e1b2535e":"code","a01a7b9d":"code","188f92eb":"code","f238249d":"code","326d7143":"code","11d7450d":"code","fa163383":"code","735ca7d7":"code","f64f4d33":"code","8f949ff1":"code","6bfe99ae":"code","e5c79cfe":"code","3658860d":"code","66eb6f7c":"code","fe8afdc8":"code","1bf56393":"code","badba5ce":"code","2219e921":"code","09e4e4f2":"code","133faef8":"code","67b4d18d":"code","eff7665f":"code","1bd4294c":"code","b1da16fd":"code","03d05d66":"code","b591b7ae":"code","3c9c3d38":"code","888134c5":"markdown","d9fb993c":"markdown","ce03bd57":"markdown"},"source":{"f6be5b83":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import date, timedelta","fec9e6a2":"train = pd.read_parquet('\/kaggle\/input\/medallion-monthly-test\/train.parquet')\ntest = pd.read_parquet('\/kaggle\/input\/medallion-monthly-test\/test.parquet')\nss = pd.read_csv('\/kaggle\/input\/medallion-monthly-test\/sample_submission.csv')\n\ntrain.shape, test.shape, ss.shape","e5afbdc7":"train.head(2)","fd8ec936":"# cnt = 0\n# for i in range(len(train)):\n#     print(train.iloc[i])\n#     cnt +=1\n#     if cnt==10:\n#         break","e1b2535e":"train.info()","a01a7b9d":"test.info()","188f92eb":"# con_df[92320:-1].head()","f238249d":"# df[\"ratings_disabled\"], train[\"comments_disabled\"], train[\"comment_count\"]","326d7143":"for i, c in train.iterrows():\n    train.loc[i,'age'] = (date(2022, 1, 1)-c['publishedAt'].date()).days\n    train.loc[i,'daystotrend'] = (c['trending_date']-c['publishedAt'].date()).days\nfor i, c in test.iterrows():\n    test.loc[i,'age'] = (date(2022, 1, 1)-c['publishedAt'].date()).days\n    test.loc[i,'daystotrend'] = (c['trending_date']-c['publishedAt'].date()).days","11d7450d":"train.head() #cut","fa163383":"from tqdm import tqdm\nimport nltk\nimport re\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n \n# Create WordNetLemmatizer object\nwnl = WordNetLemmatizer()\n\ndef deEmojify(text):\n    \"function to remove emojis from text\"\n    regrex_pattern = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"  # chinese char\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\" \n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                      \"]+\", re.UNICODE)\n    return regrex_pattern.sub(r'',text)\n\na_set1 = []\nfor i in tqdm(train[\"tags\"]):\n    tags = [deEmojify(wnl.lemmatize(word.strip().lower())) for word in i.split(\"|\")]\n    a_set1.extend(tags)   \n\na_set_of_tags = filter(None, a_set1)\na_set = set(a_set_of_tags)\nprint(len(a_set)) \nprint(\"\\n\")\n# print(a_set)","735ca7d7":"# pd.set_option(\"display.max_rows\", None)\nst_df = pd.DataFrame({\"tags\":a_set1})\ncnt = st_df[\"tags\"].value_counts().to_frame()\n# cnt.to_csv(\"tags_cnt.csv\",index=False)\n# tag_cnt_df = pd.read_csv(\"tags_cnt.csv\")\n# tag_cnt_df\ncnt.head()","f64f4d33":"# a_set1[10]","8f949ff1":"# import sys\n# np.set_printoptions(threshold=sys.maxsize)\n# sorted(np.unique(a_set1, return_counts=True)[0],reverse=True)","6bfe99ae":"\ndef sep_tag(row):\n    tag_list = row.split(\"|\")\n    ","e5c79cfe":"con_df = pd.concat([train, test], axis=0)\nprint(con_df.shape)\ndisplay(con_df)","3658860d":"# con_df[92320:98120]","66eb6f7c":"df = con_df[[\"categoryId\",\"channelId\", \"comments_disabled\",\"ratings_disabled\",\"age\",\"daystotrend\",\"view_count\"]]\ndf.head()","fe8afdc8":"# df.info()","1bf56393":"# Import label encoder\nfrom sklearn import preprocessing\n \n# label_encoder object knows how to understand word labels.\nlabel_encoder = preprocessing.LabelEncoder()\n# cat_list = ['ratings_disabled','comments_disabled','comments_disabled', 'ratings_disabled','channelTitle','channelId','categoryId']\ncat_list = [\"categoryId\",\"channelId\", \"comments_disabled\",\"ratings_disabled\"]\n# Encode labels in column 'species'.\nfor i in cat_list:\n    df[i] = label_encoder.fit_transform(df[i])\n\ndf.head()","badba5ce":"# con_df[92320:-1].head()\ntrain_df = df[0:92319]\ntest_df = df[92320:98120]\n\ndata = train_df.copy()\ny = train_df[\"view_count\"]\nx = data.drop(['view_count'],axis=1)\n\nSEED = 42\n\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_val,y_train,y_val = train_test_split(x,y,test_size = 0.2,random_state = SEED)","2219e921":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n# from sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.ensemble import RandomForestRegressor\n\nregr = RandomForestRegressor(max_depth=5, random_state=SEED)\nmodel = regr.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nscore =  mean_squared_error(y_val, y_pred)\n# y_val.size\nprint(score)\nprint(np.sqrt(score))","09e4e4f2":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import (AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor)\n\n\nbase_learners = [\n                 ('rf_1', RandomForestRegressor(n_estimators=100, random_state=SEED)),\n                 ('adb',AdaBoostRegressor(n_estimators=100, random_state=SEED)),\n                 ('ext',ExtraTreesRegressor(n_estimators=100, random_state=SEED)),\n                 ('gbc',GradientBoostingRegressor(n_estimators=100,random_state=SEED)),\n                 ('svc', SVR())\n    \n    \n                ]\n\n# Initialize Stacking Classifier with the Meta Learner\nstk_reg = StackingRegressor(estimators=base_learners, final_estimator=LinearRegression())\n\nstk_reg.fit(x_train, y_train)","133faef8":"y_pred1 = stk_reg.predict(x_val)\nscore1 =  mean_squared_error(y_val, y_pred1)\n# y_val.size\nprint(score1)\nprint(np.sqrt(score1))","67b4d18d":"# round(model.score(train_df[xs1], train_df['view_count']),4)","eff7665f":"test_df[[\"categoryId\",'channelId','comments_disabled','ratings_disabled','age','daystotrend']]","1bd4294c":"test_df[\"view_count\"] = stk_reg.predict(test_df[[\"categoryId\",'channelId','comments_disabled','ratings_disabled','age','daystotrend']])","b1da16fd":"# test_df.head()","03d05d66":"sub_df = pd.concat([test['id'],test_df[\"view_count\"]],axis=1)\nsub_df['view_count'] = sub_df['view_count'].astype('int64')\n# sub_df.head()\n\nsub_df.tail()","b591b7ae":"sub_df.to_csv(\"submission.csv\",index=False)","3c9c3d38":"sub = pd.read_csv(\".\/submission.csv\")\nsub.info()","888134c5":"# Using tags as one hot encoded feature: [incomplete]","d9fb993c":"# features I will be using:\n### Basic features:\n1. categoryId\n2. channelId\n3. comments_disabled \n4. ratings_disabled   \n5. difference of days between getting published and getting trended\n6. age of the video\n\n### Future feature engineering:\n1. Add tags as a categorical features","ce03bd57":"# Ideas with the data:\n1. analyze the trending videos of every year.\n2. analyze what is the sweat spot timing to upload a video."}}