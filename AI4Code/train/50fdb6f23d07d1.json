{"cell_type":{"4f8fff5c":"code","cc5b75bf":"code","af45eaf6":"code","73919fb4":"code","940577fc":"code","4054decf":"code","eb72e79d":"code","4e9c519f":"code","0946111f":"code","14f64bcb":"markdown","1509f37d":"markdown","e0d1b44c":"markdown","fc3867c0":"markdown","a029e66a":"markdown","90f949f0":"markdown"},"source":{"4f8fff5c":"import random\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\n\nimport glob\nimport os\nimport gc\nfrom joblib import Parallel, delayed","cc5b75bf":"path_submissions = '\/'\ntarget_name = 'target'\nscores_folds = {}","af45eaf6":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","73919fb4":"book_example = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0')\ntrade_example =  pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=0')\n\nstock_id = '0'\ntime_id = book_example.time_id.unique()\n\nbook_example = book_example[book_example['time_id'].isin(time_id)]\nbook_example.loc[:,'stock_id'] = stock_id\ntrade_example = trade_example[trade_example['time_id'].isin(time_id)]\ntrade_example.loc[:,'stock_id'] = stock_id\n\nbook_example['wap'] = calc_wap(book_example)\n\nbook_example.loc[:,'log_return'] = log_return(book_example['wap'])\nbook_example = book_example[~book_example['log_return'].isnull()]\n\nbook_example = book_example.merge(trade_example, on=['seconds_in_bucket','time_id'],how='left', suffixes=('', '_y'))\nbook_example = book_example.loc[:, ~book_example.columns.str.endswith('_y')]\n\nbook_example = book_example.fillna(0)\n\nrv = pd.DataFrame(book_example[['log_return','time_id']].groupby(['time_id']).agg(realized_volatility)).reset_index()\ntrain = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv', dtype = {'stock_id': np.int32, 'time_id': np.int32, 'target': np.float64})\ntrain.head()\n\ntrain_0 = train[train['stock_id']==0]\ndf_rv_train = train_0.merge(rv, on = ['time_id'], how = 'right')\ndf_rv_train['error'] = (df_rv_train['target'] - df_rv_train['log_return'])\ndf_rv_train['percentage_error'] = (df_rv_train['target'] - df_rv_train['log_return'])\/df_rv_train['target']","940577fc":"# bid ask price difference aka spread\n# statistical feature engineering\n\ndf = book_example\n\n\ndefault_indices = [1,2,3]\ndefault_indices_diff = [[1,2],[1,3],[2,3]]\ndefault_col = ['size','price','volume']\n\ndef calc3(df):\n    df['ask_price3'] = (df['ask_price1']*df['ask_size2'] + df['ask_price2']*df['ask_size1'])\/(df['ask_size1'] + df['ask_size2'])\n    df['ask_size3'] = df['ask_size1'] + df['ask_size2']\n    df['bid_price3'] = (df['bid_price1']*df['bid_size2'] + df['bid_price2']*df['bid_size1'])\/(df['bid_size1'] + df['bid_size2'])\n    df['bid_size3'] = df['bid_size1'] + df['bid_size2']\n    return df\n\ndef calc_mid_prices(df, indices = default_indices):\n    for i in indices:\n        df['mid_price'+str(i)] = (df['ask_price'+str(i)] + df['bid_price'+str(i)])\/2\n    return df\n\ndef calc_market_depth(df, indices = default_indices):\n    for i in indices:\n        df['market_depth'+str(i)] = (df['bid_size'+str(i)] + df['ask_size'+str(i)])\n    return df\n\ndef calc_price_impact(df, indices = default_indices):\n    for i in indices:\n        df['price_impact'+str(i)] = 1\/(df['bid_size'+str(i)] + df['ask_size'+str(i)])\n    return df\n\ndef calc_waps(df, indices = default_indices):\n    for i in indices:\n        df['wap'+str(i)] = (df['bid_price'+str(i)] * df['ask_size'+str(i)] + df['ask_price'+str(i)] * df['bid_size'+str(i)])\/(df['bid_size'+str(i)] + df['ask_size'+str(i)])\n    return df\n\ndef calc_wrong_waps(df, indices = default_indices):\n    for i in indices:\n        df['wap'+str(i+3)] = (df['bid_price'+str(i)] * df['bid_size'+str(i)] + df['ask_price'+str(i)] * df['ask_size'+str(i)])\/(df['bid_size'+str(i)] + df['ask_size'+str(i)])\n    return df\n\ndef calc_volumes(df, sides =['bid','ask'] ,indices = default_indices):\n    for s in sides:\n        for i in indices:\n            df[s+'_volume'+str(i)] = df[s+'_price'+str(i)]*df[s+'_size'+str(i)]\n    return df\n\n# bid ask difference - price, size, volume\ndef calc_imbalance(df, col=default_col, indices = default_indices):\n    for c in col:\n        for i in indices:\n            df[c+'_imbalance'+str(i)] = df['ask_'+c+str(i)] - df['bid_'+c+str(i)] \n    return df\n\n# sides differences - price, size, volume\ndef calc_difference(df, sides=['bid','ask'], col=default_col, indices = default_indices_diff):\n    for s in sides:\n        for c in col:\n            for i in indices:\n                df[s+'_'+c+'_difference'+str(i[0])+str(i[1])] = df[s+'_'+c+str(i[0])] - df[s+'_'+c+str(i[1])]\n    return df\n\n\n# Accumulated Features\ndef accumulate_base(df, sides = ['bid','ask'], col = default_col, ind = default_indices):\n    df_group = df.groupby('time_id').cumsum()\n    for s in sides:\n        for c in col:\n            for i in ind:\n                df['acc_'+s+'_'+c+str(i)] = df_group[s+'_'+c+str(i)]\n    return df\n\ndef accumulate_imbalance(df,col = default_col, ind = default_indices):\n    df_group = df.groupby('time_id').cumsum()\n    for c in col:\n        for i in ind:\n            df['acc_'+c+'_imbalance'+str(i)] = df_group[c+'_imbalance'+str(i)]\n            \n    return df           \n        \ndef accumulate_difference(df, sides = ['bid','ask'], col = default_col, ind = default_indices_diff):\n    df_group = df.groupby('time_id').cumsum()\n    for s in sides:\n        for c in col:\n            for i in ind:\n                df['acc_'+s+'_'+c+'_difference'+str(i[0])+str(i[1])] = df_group[s+'_'+c+'_difference'+str(i[0])+str(i[1])]\n    return df\n\ndef calc_log_returns(df, indices = default_indices):\n    for i  in default_indices:\n        df['log_return'+str(i)] = df.groupby(['time_id'])['wap'+str(i)].apply(log_return)\n    return df","4054decf":"def Build_OB_Features(df):\n    \n    indices = [1,2,3]\n    indices_diff = [[1,2],[1,3],[2,3]]\n    col = ['size','price','volume']\n    \n    df = calc3(df)\n    df = calc_mid_prices(df)\n    df = calc_market_depth(df)\n    df = calc_price_impact(df)\n    df = calc_waps(df)\n    df = calc_wrong_waps(df, indices = [1])\n    df = calc_volumes(df,indices = indices)\n    df = calc_imbalance(df)\n    df = calc_difference(df, col = col)\n    df = accumulate_base(df, col = col)\n    df = accumulate_imbalance(df,col = col, ind = indices)\n    df = accumulate_difference(df, col = col, ind = indices_diff)\n                \n    return df","eb72e79d":"book_example = Build_OB_Features(book_example)","4e9c519f":"df_train_stock_0 = book_example.groupby('time_id').agg([np.mean,np.sum,np.std])\ndf_train_stock_0.columns = ['_'.join(col) for col in df_train_stock_0.columns]\n\nsns.set(rc={'figure.figsize':(24,8)})\nsns.set_style(style='white')\n\ncolumns = [columns for columns in book_example.columns if columns not in ['time_id','stock_id']]\n\nfor col in columns:\n    color = (random.random(), random.random(), random.random())\n    \n    fig, axs = plt.subplots(ncols=3)\n    sns.regplot(x=df_train_stock_0[col+'_mean'], y=df_rv_train['target'], color=color, order = 2, line_kws={\"color\": 'black'}, ax=axs[0]).set(ylim=(0, None),title= 'Mean')\n    sns.regplot(x=df_train_stock_0[col+'_sum'], y=df_rv_train['target'], color=color, order = 2, line_kws={\"color\": 'black'}, ax=axs[1]).set(ylim=(0, None),title= 'Sum')\n    sns.regplot(x=df_train_stock_0[col+'_std'], y=df_rv_train['target'], color=color, order = 2, line_kws={\"color\": 'black'}, ax=axs[2]).set(ylim=(0, None),title= 'Std')\n    fig.suptitle(col+' v.s. target',size=30) \n    \n    plt.show()","0946111f":"for col in columns:\n    \n    color = (random.random(), random.random(), random.random())\n    \n    fig, axs = plt.subplots(ncols=3)\n    \n    sns.regplot(x=df_train_stock_0[col+'_mean'], y=df_rv_train['percentage_error'], color=color, order = 2, line_kws={\"color\": 'black'}, ax=axs[0]).set(ylim=(-20, 2),title= 'Mean')\n    sns.regplot(x=df_train_stock_0[col+'_sum'], y=df_rv_train['percentage_error'], color=color, order = 2, line_kws={\"color\": 'black'}, ax=axs[1]).set(ylim=(-20, 2),title= 'Sum')\n    sns.regplot(x=df_train_stock_0[col+'_std'], y=df_rv_train['percentage_error'], color=color, order = 2, line_kws={\"color\": 'black'}, ax=axs[2]).set(ylim=(-20, 2),title= 'Std')\n    fig.suptitle(col+' v.s. Baseline Relative Error',size=30) \n    \n    plt.show()","14f64bcb":"# Base Feature engineering notebook","1509f37d":"Notebook that regroup most of features engineering on book \/ trade data and test them against target and percentage error for the stock 0. \n\nThe features that do not appears in currently available notebooks are mostly from publicly available code from a previous orderbook challenge (XTX challenge). (See for example https:\/\/github.com\/alexbotsula\/XTX_Challenge\/blob\/master\/Research\/Order_book_vars.py)\n\n# Other Feature Engineering Notebooks: \n\nThis notebook is part of a serie on basic Feature Engineering \/ visual variable selection notebooks:\n\n1) Base Features: https:\/\/www.kaggle.com\/lucasmorin\/feature-engineering-1-base-features\n\n2) Aggregation Functions: https:\/\/www.kaggle.com\/lucasmorin\/feature-engineering-2-aggregation-functions\n\n3) RV aggregation: https:\/\/www.kaggle.com\/lucasmorin\/feature-engineering-3-rv-aggregation\/","e0d1b44c":"# Features v.s. baseline error","fc3867c0":"# Base Features","a029e66a":"# Features versus Target","90f949f0":"# Tools"}}