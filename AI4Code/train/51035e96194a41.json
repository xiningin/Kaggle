{"cell_type":{"936a68f3":"code","ba5c4003":"code","34ca26f9":"code","e5d7f811":"code","aac0eefb":"code","95ee3a10":"code","eb14d180":"code","849144a4":"code","b20dde73":"code","743187f5":"code","71dd4fc4":"code","416e6086":"code","d8703e72":"code","6db7ff9d":"code","5d7fbe56":"code","d28c642c":"code","89f088be":"code","532b213f":"code","b7c14557":"code","26ce6a6a":"code","ee51ab70":"code","8edc7227":"code","e1515277":"code","72ffdd62":"code","165fb065":"code","d5b86b45":"code","75e8b0a1":"code","fcd490d7":"code","98acf973":"code","6ae37c61":"code","c0fa8509":"code","fe5d6b00":"code","8a9d1b53":"code","0a59306c":"code","1a435814":"code","cf876ced":"code","95499a7d":"code","0238b985":"code","0f7b705e":"code","77a9dd23":"code","8c37a81c":"code","3cdae04c":"markdown","ada84fcd":"markdown","20247021":"markdown","ad0be2f8":"markdown","e91ccaf1":"markdown","a8ffa1e1":"markdown","6621e297":"markdown","6076e97f":"markdown","556cb536":"markdown","d1dcc6a2":"markdown","2ef8c1f8":"markdown","fb2e523a":"markdown","9ad7ba4e":"markdown","ef2b60db":"markdown","8a5a01a7":"markdown","bf94d083":"markdown"},"source":{"936a68f3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport gc,os,sys\n\nsns.set_style('darkgrid')\npd.options.display.float_format = '{:,.3f}'.format\n\nprint(os.listdir(\"..\/input\"))","ba5c4003":"%%time\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nprint(train.shape, test.shape)","34ca26f9":"for c in train.columns:\n    if c not in test.columns: print(c)","e5d7f811":"train.head()","aac0eefb":"null_cnt = train.isnull().sum().sort_values()\nprint('null count:', null_cnt[null_cnt > 0])","95ee3a10":"numcols = train.drop('target',axis=1).select_dtypes(include='number').columns.values","eb14d180":"train['target'].value_counts().to_frame().plot.bar()","849144a4":"all_data = train.append(test, sort=False).reset_index(drop=True)\ndel train, test\ngc.collect()\n\nall_data.head()","b20dde73":"# drop constant column\nconstant_column = [col for col in all_data.columns if all_data[col].nunique() == 1]\nprint('drop columns:', constant_column)\nall_data.drop(constant_column, axis=1, inplace=True)","743187f5":"#method='pearson','kendall','spearman'\ncorr_matrix = all_data.corr(method='pearson').abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [c for c in upper.columns if any(upper[c] > 0.95)]\ndel upper\n\ndrop_column = all_data.columns[to_drop]\nprint('drop columns:', drop_column)\n#all_data.drop(drop_column, axis=1, inplace=True)","71dd4fc4":"cols = [col for col in all_data.columns if col not in ['ID_code']]\nfor i, t in all_data.loc[:, cols].dtypes.iteritems():\n    if t == object:\n        print(i)\n        all_data[i] = pd.factorize(all_data[i])[0]","416e6086":"from sklearn import preprocessing\n\n#scaler = preprocessing.StandardScaler()\n#scaler = preprocessing.MaxAbsScaler()\nscaler = preprocessing.RobustScaler()\nall_data.loc[:,numcols] = scaler.fit_transform(all_data[numcols])","d8703e72":"_='''noneffective\nfeats = [\"var_{}\".format(i) for i in range(200)]\nfor f in feats:\n    all_data[f] = pd.cut(all_data[f], 100, labels=range(100))\n'''","6db7ff9d":"all_data.head()","5d7fbe56":"mean1 = all_data[all_data['target']==1].mean()\nmean0 = all_data[all_data['target']==0].mean()\npd.concat([mean1, mean0, np.abs(mean1-mean0)], axis=1).sort_values(by=2, ascending=False)[1:10]","d28c642c":"X_train = all_data[all_data['target'].notnull()].reset_index(drop=True)\nX_test = all_data[all_data['target'].isnull()].drop(['target'], axis=1).reset_index(drop=True)\ndel all_data\ngc.collect()\n\n# drop ID_code\nX_train.drop(['ID_code'], axis=1, inplace=True)\nX_test_ID = X_test.pop('ID_code')\n\nY_train = X_train.pop('target')\n\nprint(X_train.shape, X_test.shape)","89f088be":"_='''\ndef get_redundant_pairs(df):\n    # Get diagonal and lower triangular pairs of correlation matrix\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(X_train))\n'''","532b213f":"_='''noneffective\nfrom imblearn.over_sampling import SMOTE,ADASYN\n#from imblearn.combine import SMOTETomek\n\nsm = SMOTE(random_state=42)\n#sm = SMOTE(kind='svm',random_state=42)\n#sm = SMOTE(kind='borderline1',random_state=42)\n#sm = ADASYN(random_state=42)\n#sm = SMOTETomek(random_state=42)\nX_train, Y_train = sm.fit_sample(X_train, Y_train)\nX_train = pd.DataFrame(X_train, columns=X_test.columns)\nprint(X_train.shape)\n'''","b7c14557":"from sklearn.neighbors import KNeighborsClassifier\n\n'''\nfor k in range(2, 20):\n    knc = KNeighborsClassifier(n_neighbors=k)\n    knc.fit(X_train, Y_train)\n    score = knc.score(X_train, Y_train)\n    print(\"[%d] score: {:.2f}\".format(score) % k)\n'''\nknc = KNeighborsClassifier(n_neighbors=3)\nknc.fit(X_train, Y_train)\n#X_train_knc = knc.predict(X_train)\n#X_test_knc = knc.predict(X_test)\n#knc_data = pd.DataFrame({'KNC':X_train_knc, 'target':Y_train})\n#sns.countplot(x='KNC', hue='target', palette='Set1', data=knc_data)\n\nX_train['_knc'] = knc.predict_proba(X_train)[:,1]\nX_test['_knc'] = knc.predict_proba(X_test)[:,1]","26ce6a6a":"from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=2, init='k-means++', max_iter=3000, random_state=42)\nX_train_km = km.fit_predict(X_train)\nX_test_km = km.predict(X_test)\n\nkm_data = pd.DataFrame({'KMeans':X_train_km, 'target':Y_train})\nsns.countplot(x='KMeans', hue='target', palette='Set1', data=km_data)","ee51ab70":"from sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.model_selection import cross_validate, cross_val_score, StratifiedKFold\nfrom sklearn import metrics\n\npca = PCA()\npca.fit(X_train)\nev_ratio = pca.explained_variance_ratio_\nev_ratio = np.hstack([0,ev_ratio.cumsum()])\n\nplt.xlabel('components')\nplt.plot(ev_ratio)\nplt.show()","8edc7227":"_='''\ndef select_n_components(var_ratio, goal_var: float) -> int:\n    total_variance = 0.0\n    n_components = 0\n    for explained_variance in var_ratio:\n        total_variance += explained_variance\n        n_components += 1\n        if total_variance >= goal_var:\n            break\n            \n    return n_components\n\nlda = LDA(n_components=None)\nlda.fit(X_train, Y_train)\nprint(select_n_components(lda.explained_variance_ratio_, 0.95))\n'''","e1515277":"fig, ax = plt.subplots(2, 2, figsize=(16, 6))\nax = ax.ravel()\n\npca = PCA(n_components=1)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.fit_transform(X_test)\npca_1 = X_train_pca[Y_train > 0].reshape(-1)\npca_0 = X_train_pca[Y_train == 0].reshape(-1)\nax[0].hist([pca_1, pca_0], color=['b','r'], bins=30, alpha=0.5, histtype='barstacked')\nax[0].set_title('PCA visualization')\n\nlda = LDA(n_components=1)\nlda.fit(X_train, Y_train)\nX_train_lda = lda.transform(X_train)\nX_test_lda = lda.transform(X_test)\nlda_1 = X_train_lda[Y_train > 0].reshape(-1)\nlda_0 = X_train_lda[Y_train == 0].reshape(-1)\nax[1].hist([lda_1, lda_0], color=['b','r'], bins=30, alpha=0.5, histtype='barstacked')\nax[1].set_title('LDA visualization')\n\nax[2].hist(X_test_pca, color='g', bins=30, alpha=0.5, histtype='barstacked')\nax[3].hist(X_test_lda, color='g', bins=30, alpha=0.5, histtype='barstacked')\n\nplt.show()","72ffdd62":"fig, ax = plt.subplots(2, 2, figsize=(16, 6))\nax = ax.ravel()\n\ngnb = GaussianNB()\ngnb.fit(X_train, Y_train)\nX_train_gnb = gnb.predict_log_proba(X_train)[:,1]\nX_test_gnb = gnb.predict_log_proba(X_test)[:,1]\ngnb_1 = X_train_gnb[Y_train > 0].reshape(-1)\ngnb_0 = X_train_gnb[Y_train == 0].reshape(-1)\nax[0].hist([gnb_1, gnb_0], color=['b','r'], bins=30, alpha=0.5, histtype='barstacked')\nax[0].set_title('GaussianNB visualization')\n\nbnb = BernoulliNB(fit_prior=True)\nbnb.fit(X_train, Y_train)\nX_train_bnb = bnb.predict_log_proba(X_train)[:,1]\nX_test_bnb = bnb.predict_log_proba(X_test)[:,1]\nbnb_1 = X_train_bnb[Y_train > 0].reshape(-1)\nbnb_0 = X_train_bnb[Y_train == 0].reshape(-1)\nax[1].hist([bnb_1, bnb_0], color=['b','r'], bins=30, alpha=0.5, histtype='barstacked')\nax[1].set_title('BernoulliNB visualization')\n\nax[2].hist(X_test_gnb, color='g', bins=30, alpha=0.5, histtype='barstacked')\nax[3].hist(X_test_bnb, color='g', bins=30, alpha=0.5, histtype='barstacked')\n\nplt.show()","165fb065":"fig, ax = plt.subplots(2, 2, figsize=(16, 6))\nax = ax.ravel()\n\nlgr = LogisticRegression(C=0.1, class_weight='balanced', penalty='l1', solver='liblinear')\nlgr.fit(X_train, Y_train)\nX_train_lgr = lgr.predict_log_proba(X_train)[:,1]\nX_test_lgr = lgr.predict_log_proba(X_test)[:,1]\nlgr_1 = X_train_lgr[Y_train > 0].reshape(-1)\nlgr_0 = X_train_lgr[Y_train == 0].reshape(-1)\nax[0].hist([lgr_1, lgr_0], color=['b','r'], bins=30, alpha=0.5, histtype='barstacked')\nax[0].set_title('LogisticRegression visualization')\nax[2].hist(X_test_lgr, color='g', bins=30, alpha=0.5, histtype='barstacked')\n\nsgd = SGDClassifier(max_iter=10000, loss='log', tol=1e-5)\nsgd.fit(X_train, Y_train)\nX_train_sgd = sgd.predict_log_proba(X_train)[:,1]\nX_test_sgd = sgd.predict_log_proba(X_test)[:,1]\nsgd_1 = X_train_sgd[Y_train > 0].reshape(-1)\nsgd_0 = X_train_sgd[Y_train == 0].reshape(-1)\nax[1].hist([sgd_1, sgd_0], color=['b','r'], bins=30, alpha=0.5, histtype='barstacked')\nax[1].set_title('SGDClassifier visualization')\nax[3].hist(X_test_sgd, color='g', bins=30, alpha=0.5, histtype='barstacked')\nplt.show()","d5b86b45":"fpr, tpr, thresholds = metrics.roc_curve(Y_train, X_train_gnb)\nauc = metrics.auc(fpr, tpr)\n\nplt.plot(fpr, tpr, label='ROC curve (area = %.2f)'%auc)\nplt.legend()\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","75e8b0a1":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nscores = cross_val_score(pca, X_train, Y_train, cv=skf)\nprint(\"PCA, Accuracy: %0.4f (+\/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n    \nscores = cross_val_score(lda, X_train, Y_train, cv=skf) # max components is (classes - 1)\nprint(\"LDA, Accuracy: %0.4f (+\/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n    \nscores = cross_val_score(gnb, X_train, Y_train, cv=skf)\nprint(\"GNB, Accuracy: %0.4f (+\/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n\nscores = cross_val_score(bnb, X_train, Y_train, cv=skf)\nprint(\"BNB, Accuracy: %0.4f (+\/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n\nscores = cross_val_score(lgr, X_train, Y_train, cv=skf)\nprint(\"LGR, Accuracy: %0.4f (+\/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n\nscores = cross_val_score(sgd, X_train, Y_train, cv=skf)\nprint(\"SGD, Accuracy: %0.4f (+\/- %0.4f)\" % (scores.mean(), scores.std() * 2))","fcd490d7":"_='''noneffective\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, verbose=0, perplexity=30, n_iter=1000)\nX_all_tsne = tsne.fit_transform(pd.concat([n_train,n_test]))\nX_train_tsne = X_all_tsne[:n_train.shape[0]]\nX_test_tsne = X_all_tsne[n_train.shape[0]:]\nprint(X_train_tsne.shape)\n\nplt.figure(figsize=(6,6))\nplt.scatter(X_train_tsne[:,0], X_train_tsne[:,1], c=color, alpha=0.3)\nplt.title(\"t-SNE visualization\")\nplt.show()\n\nn_train['tnse1'] = X_train_tsne[:,0]\nn_test['tnse1'] = X_test_tsne[:,0]\nn_train['tnse2'] = X_train_tsne[:,1]\nn_test['tnse2'] = X_test_tsne[:,1]\n'''","98acf973":"for df in [X_train, X_test]:\n    df['sum'] = df.sum(axis=1)  \n    df['min'] = df.min(axis=1)\n    df['max'] = df.max(axis=1)\n    df['mean'] = df.mean(axis=1)\n    df['std'] = df.std(axis=1)\n    df['skew'] = df.skew(axis=1)\n    df['kurt'] = df.kurtosis(axis=1)\n    df['med'] = df.median(axis=1)\n    df['var'] = df.var(axis=1)\n    df['negval'] = df.apply(lambda x: (x < 0).astype(int).sum(), axis=1)","6ae37c61":"n_train = pd.DataFrame()\nn_test = pd.DataFrame()\n\n#n_train['_km'] = X_train_km\n#n_test['_km'] = X_test_km\n#n_train['_lda'] = X_train_lda\n#n_test['_lda'] = X_test_lda\n#n_train['_pca'] = X_train_pca\n#n_test['_pca'] = X_test_pca\nn_train['_lgr'] = X_train_lgr\nn_test['_lgr'] = X_test_lgr\nn_train['_sgd'] = X_train_sgd\nn_test['_sgd'] = X_test_sgd\nn_train['_gnb'] = X_train_gnb\nn_test['_gnb'] = X_test_gnb\n#n_train['_bnb'] = X_train_bnb\n#n_test['_bnb'] = X_test_bnb","c0fa8509":"_='''\nfor c in X_train.columns:\n    X_train[c +'_perc'] = X_train[c].rank()\n    X_test[c + '_perc'] = X_test[c].rank()\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(2)\nn_train = poly.fit_transform(n_train)\nn_test = poly.transform(n_test)\n\nfor c in n_train.columns:\n    n_train[c+'_r'] = rankdata(n_train[c]).astype('float32')\n    n_train[c+'_n'] = norm.cdf(n_train[c]).astype('float32')\n    n_test[c+'_r'] = rankdata(n_test[c]).astype('float32')\n    n_test[c+'_n'] = norm.cdf(n_test[c]).astype('float32')\n'''","fe5d6b00":"X_train = pd.concat([X_train, n_train], axis=1)\nX_test = pd.concat([X_test, n_test], axis=1)","8a9d1b53":"_='''noneffective\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nfeat = SelectKBest(f_classif, k=50)\nfeat.fit(X_train, Y_train)\n\nX_train = X_train.loc[:,feat.get_support()]\nX_test = X_test.loc[:,feat.get_support()]\n'''","0a59306c":"print(pd.DataFrame([[val for val in dir()], [sys.getsizeof(eval(val)) for val in dir()]],\n                   index=['name','size']).T.sort_values('size', ascending=False).reset_index(drop=True)[:5])","1a435814":"from sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\n\nparams = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : -1,\n    \"num_leaves\" : 13,\n    \"learning_rate\" : 0.01,\n    \"bagging_freq\": 5,\n    \"bagging_fraction\" : 0.4,\n    \"feature_fraction\" : 0.05,\n    \"min_data_in_leaf\": 80,\n    \"min_sum_heassian_in_leaf\": 10,\n    \"tree_learner\": \"serial\",\n    \"boost_from_average\": \"false\",\n    \"bagging_seed\" : 42,\n    \"verbosity\" : 1,\n    \"seed\": 42\n}\n\nfolds = StratifiedKFold(n_splits=10)\noof_preds = np.zeros(X_train.shape[0])\nsub_preds = np.zeros(X_test.shape[0])\nfor fold_, (trn_, val_) in enumerate(folds.split(X_train, Y_train)):\n    trn_x, trn_y = X_train.iloc[trn_], Y_train[trn_]\n    val_x, val_y = X_train.iloc[val_], Y_train[val_]\n\n    model = lgb.LGBMRegressor(**params, n_estimators=100000)\n    model.fit(trn_x, trn_y, eval_set=[(val_x, val_y)], early_stopping_rounds=3000, verbose=1000)\n\n    oof_preds[val_] = model.predict(val_x, num_iteration=model.best_iteration_)\n    sub_preds += model.predict(X_test, num_iteration=model.best_iteration_) \/ folds.n_splits","cf876ced":"# Plot feature importance\nfeature_importance = model.feature_importances_\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\nsorted_idx = sorted_idx[len(feature_importance) - 20:]\npos = np.arange(sorted_idx.shape[0]) + .5\n\nplt.figure(figsize=(12,6))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","95499a7d":"fpr, tpr, thresholds = metrics.roc_curve(Y_train, oof_preds)\nauc = metrics.auc(fpr, tpr)\n\nplt.plot(fpr, tpr, label='ROC curve (area = %.2f)'%auc)\nplt.legend()\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)","0238b985":"_='''\nfrom catboost import CatBoostClassifier\n\nfolds = StratifiedKFold(n_splits=5)\noof2_preds = np.zeros(X_train.shape[0])\nsub2_preds = np.zeros(X_test.shape[0])\nfor fold_, (trn_, val_) in enumerate(folds.split(X_train, Y_train)):\n    trn_x, trn_y = X_train.iloc[trn_], Y_train[trn_]\n    val_x, val_y = X_train.iloc[val_], Y_train[val_]\n    \n    model = CatBoostClassifier(iterations=100000, learning_rate=0.01, objective=\"Logloss\", eval_metric='AUC')\n    model.fit(trn_x, trn_y, eval_set=[(val_x, val_y)], early_stopping_rounds=3000, verbose=1000)\n\n    oof2_preds[val_] = model.predict_proba(val_x)[:,1]\n    sub2_preds += model.predict_proba(X_test)[:,1] \/ folds.n_splits\n'''","0f7b705e":"#preds = sub_preds * 0.7 + sub2_preds * 0.3\npreds = sub_preds","77a9dd23":"submission = pd.DataFrame({\n    'ID_code': X_test_ID,\n    'target': preds\n})\nsubmission.to_csv(\"submission.csv\", index=False)","8c37a81c":"submission['target'].sum()","3cdae04c":"## KMeans","ada84fcd":"- CatBoostClassifier","20247021":"- scaling","ad0be2f8":"# Predict","e91ccaf1":"- drop high correlation columns","a8ffa1e1":"- factorize","6621e297":"# Feature engineering","6076e97f":"# Data analysis","556cb536":"- drop constant columns","d1dcc6a2":"- mean diff","2ef8c1f8":"- LGBMClassifier","fb2e523a":"# Submit","9ad7ba4e":"- PCA is noneffective","ef2b60db":"## PCA, LDA, NB","8a5a01a7":"## Preparation","bf94d083":"# Load data"}}