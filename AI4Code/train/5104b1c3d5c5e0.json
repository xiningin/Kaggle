{"cell_type":{"ffb1f75a":"code","406e9ace":"code","432a8905":"code","1f51d6e4":"code","806da97c":"code","5d86e544":"code","db0798fa":"code","1a2c37af":"code","20e1f9a5":"code","63c2696d":"code","ade4089b":"code","155c9b28":"markdown","ac2b191f":"markdown","0e56345e":"markdown","186ea96f":"markdown","bec62546":"markdown","39a70960":"markdown","8b1c6b81":"markdown","3c6bc624":"markdown","e19f6cfa":"markdown","688ebf93":"markdown","51de303f":"markdown","22b0bcb4":"markdown"},"source":{"ffb1f75a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec","406e9ace":"from sklearn.model_selection import train_test_split\n# Reading Data\nX = pd.read_csv('..\/input\/titanic\/train.csv')\nX_test_full = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Putting target variable in 'y' and dropping that column from X_train\ny = X['Survived']\nX = X.drop(['Survived'], axis=1)\n\n# Breaking off X in train and validation datasets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size = 0.2, random_state = 0)\n\n","432a8905":"X.head()","1f51d6e4":"fig = plt.figure(figsize=(15,8))\ngs = fig.add_gridspec(2,2)","806da97c":"sns.set(style='white')\ncorr = X.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(11, 9))\n                             \nsns.heatmap(corr, mask=mask, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","5d86e544":"# Number of missing values in each column of X_train_full\nX_train_full.isnull().sum()","db0798fa":"# Number of missing values in each column of X (train + valid)\nX.isnull().sum()","1a2c37af":"from sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Droping columns from train and validation sets\ndrop_columns = ['Cabin', 'Name', 'Ticket']\nX_train = X_train_full.drop(drop_columns, axis=1)\nX_valid = X_valid_full.drop(drop_columns, axis=1)\n\n# Creating the transformers\nnumeric_transformer = SimpleImputer(strategy = 'mean')\ncategoric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy = 'most_frequent')),\n                                        ('onehot', OneHotEncoder())])\ncategoric_cols = ['Sex', 'Embarked']\n\n# Creating the pipeline with ColumnTransformer\npreprocessor = ColumnTransformer(transformers=[('imputer_numeric',\n                                               numeric_transformer,\n                                               ['Age']),\n                                              ('imputer_categoric',\n                                              categoric_transformer,\n                                              categoric_cols)])","20e1f9a5":"from sklearn.ensemble import RandomForestClassifier\n#from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Defining Model\nmodel = RandomForestClassifier(n_estimators=300)\n#model2 = LogisticRegression()\n\n# Bundle preprocessing and modeling code in a Pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\nmy_pipeline.fit(X_train, y_train)\npreds_val = my_pipeline.predict(X_valid)\n\n# Accuracy of the prediction\naccuracy_score(y_valid,preds_val)","63c2696d":"X_test = X_test_full.drop(['Name', 'Cabin', 'Ticket'], axis=1)\npreds_test = my_pipeline.predict(X_test)","ade4089b":"# Save test predictions to file\noutput = pd.DataFrame({'PassengerID': X_test['PassengerId'],\n    'Survived': preds_test})\noutput.set_index(['PassengerID'])\noutput.to_csv('submission.csv', index=False)","155c9b28":"### Strategy to deal with categorical data","ac2b191f":"## Exploratory Analysis","0e56345e":"## Preprocessing","186ea96f":"## Output","bec62546":"On a first try, I will use the Random Forest Classifier model.","39a70960":"## Reading and separating Data","8b1c6b81":"Now we know that only 3 features have missing values: 'Age' (Age of the passengers), Cabin (Cabin number of the passenger) and Embarked (port of embarkation).\n\nWe will treat these 3 columns differently according to the type of data they represent (numerical or categorical) and to the number of missing values they have.\n\n1. The **Cabin** data could be useful finding a correlation between the cabins and survival rate since people in the cabins nearest to the rescue boats could have a higher chance of survive (assuming that they were in the cabin when the ship started to sink). Although, as a start point, I chose to drop the cabin column, since it has more than 77% of the data missing.\n\n1. The NA values **Embarked** column will be treated with an imputer puting the most frequent value (which is S for Southampton, with 72% of the entries). Since it's only two values, the treatment will not have a huge impact on the model. Another possibility could be erase those two entries from the dataset. Besides that, a further investigation of the data of those two entries could possibly let us infer these missing values.\n\n1. The NA values in the **Age** column are the most problematic, since it has large number of missing values (177), but not in a proportion huge enough for us to discard the column (177 out of 891 accounts for approximately 20%). Moreover, basing in common sense, age could play a huge role in survival rate, since children and elders could have more difficulty to acess the rescue boats, or, on the other hand, have preference in the embark. Having said that, the first method of treatment of the missing values will be an imputer by the mean, bearing in mind that a more refined method will certainly bring much better results.","3c6bc624":"### Preprocessor code","e19f6cfa":"The dataset has 5 categorical columns: 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'. The Cabin column was already ruled out by the amount of missing values it had. The question here is what strategy will I use to work with the other categorical columns? In a first glance, and for sake of simplicity in this first try, I will drop 'Name' and 'Ticket' columns and use One Hot Encoding for the rest.\n* The 'Name' column, based on common sense, probably wouldn't have a big impact on survival rate (although the title before the names could indicate some sort of socio-economic differentiation or even titles inside the ship).\n* The 'Ticket' comlumn have a very high cardinality (681 unique entries out of 891 samples) and couldn't be treated by an One Hot Encoding, so I ruled out as well in this first try.","688ebf93":"## Machine Learning Model","51de303f":"### Exploring and treating missing data\n\nHere I check if there's any missing values in the dataset, counting them by columns to choose how to treat them.","22b0bcb4":"# Titanic Competition\n\nThis is the notebook of my first all alone, no tutorials, Kaggle competition.\n\nIt will be used for the study of data cleaning, exploratory analysis, data visualization, and machine learning models.\n\nFeel free to make any comments, suggestions and critics about the code, the writing and formatting of the notebook, or anything you find relevant.\n\nThank you!"}}