{"cell_type":{"46ef70b2":"code","ccd57748":"code","85604277":"code","a994092f":"code","5633835e":"code","019e35ea":"code","d0016f96":"code","09db3800":"code","48a9004a":"code","082edf20":"code","81885ff7":"code","19260d0a":"code","09199697":"code","46f621e7":"code","63c0a913":"code","4833fc86":"code","d2ae565e":"code","5f573ce1":"code","78841393":"code","39ea04ae":"code","cc5d9732":"code","89218ad1":"code","7e111a82":"code","f367f56c":"code","a2cf1ed9":"code","c802e63a":"code","083f99b0":"code","b511c8d4":"code","d349df89":"code","9c2f621a":"code","0121b86c":"code","bb863a2b":"code","b0ee0592":"code","fc965f93":"code","4c6f422e":"code","b6c9188b":"code","f8e2ff46":"code","534c102b":"code","9308fa1a":"code","08acd6bd":"code","fe5812c2":"code","dc339cb1":"code","c81c8ff8":"code","37f612c8":"code","524a57ce":"code","3ad64139":"code","e59cfa99":"code","4760ad91":"code","0ed7fabe":"code","f6a29833":"code","ae65f070":"code","72274a4e":"markdown","406dc06a":"markdown","4e8553de":"markdown","8792dbda":"markdown","2c148c67":"markdown","05158722":"markdown","40e21803":"markdown","141a7561":"markdown","67cd18e8":"markdown","9d2c3a09":"markdown","8e1220cf":"markdown","a9a89470":"markdown","0496498e":"markdown","84d3e285":"markdown","8fc86b64":"markdown"},"source":{"46ef70b2":"import numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import RandomOverSampler\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler","ccd57748":"train = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/sample_submission.csv')","85604277":"train.head()","a994092f":"train.isnull().sum()","5633835e":"train['Response'].value_counts()","019e35ea":"sns.countplot(train.Response)","d0016f96":"#converting Text Object to int type\ntrain['Vehicle_Age']=train['Vehicle_Age'].replace({'< 1 Year':0,'1-2 Year':1,'> 2 Years':2})\ntrain['Gender']=train['Gender'].replace({'Male':1,'Female':0})\ntrain['Vehicle_Damage']=train['Vehicle_Damage'].replace({'Yes':1,'No':0})\ntest['Vehicle_Age']=test['Vehicle_Age'].replace({'< 1 Year':0,'1-2 Year':1,'> 2 Years':2})\ntest['Gender']=test['Gender'].replace({'Male':1,'Female':0})\ntest['Vehicle_Damage']=test['Vehicle_Damage'].replace({'Yes':1,'No':0})","09db3800":"train.head()","48a9004a":"test.head()","082edf20":"x_train = train.drop(['id', 'Response'], axis = 1)\ny_train = train['Response']\ntest = test.drop(['id'], axis = 1)","81885ff7":"corrmat = x_train.corr()\nfig, ax = plt.subplots()\nfig.set_size_inches(13,13)\nsns.heatmap(corrmat, annot = True)","19260d0a":"from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile","09199697":"mi = mutual_info_classif(x_train.fillna(0), y_train)\nmi","46f621e7":"# let's add the variable names and order the features\n# according to the MI for clearer visualisation\nmi = pd.Series(mi)\nmi.index = x_train.columns\nmi.sort_values(ascending=False)","63c0a913":"# and now let's plot the ordered MI values per feature\nmi.sort_values(ascending=False).plot.bar(figsize=(15, 8))","4833fc86":"x_train['Vintage_m'] = x_train['Vintage']\/30.25\ntest['Vintage_m'] = test['Vintage']\/30.25","d2ae565e":"x_train = x_train.apply(np.ceil)\ntest = test.apply(np.ceil)","5f573ce1":"x_train.head()","78841393":"mi = mutual_info_classif(x_train.fillna(0), y_train)\nmi","39ea04ae":"# let's add the variable names and order the features\n# according to the MI for clearer visualisation\nmi = pd.Series(mi)\nmi.index = x_train.columns\nmi.sort_values(ascending=False)","cc5d9732":"# and now let's plot the ordered MI values per feature\nmi.sort_values(ascending=False).plot.bar(figsize=(15, 8))","89218ad1":"x_train_2 = x_train.drop(['Vintage'], axis = True)\ntest_2 = test.drop(['Vintage'], axis = True)","7e111a82":"x_train_2.head()","f367f56c":"x_train_2 = x_train_2.astype(int)\ntest_2 = test_2.astype(int)","a2cf1ed9":"x_train_2.head()","c802e63a":"test_2.head()","083f99b0":"# categorical column \ncat_col=['Gender', 'Vintage_m', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage','Policy_Sales_Channel']","b511c8d4":"x_train_2['type'] = 'train'\ntest_2['type'] = 'test'","d349df89":"all_data = pd.concat([x_train_2, test_2])","9c2f621a":"all_data.head()","0121b86c":"# Numerical Column\nnumerical_cols = ['Annual_Premium', 'Age']\n\nscaler = MinMaxScaler()\nall_data[numerical_cols] = scaler.fit_transform(all_data[numerical_cols])\n\nall_data.head()\n","bb863a2b":"df_train = all_data[all_data['type'] == 'train']\ndf_test = all_data[all_data['type'] == 'test']","b0ee0592":"df_train = df_train.drop(['type'], axis = 1)\ndf_test = df_test.drop(['type'], axis = 1)","fc965f93":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(df_train, y_train, test_size=.25, random_state=42,stratify=y_train,shuffle=True)","4c6f422e":"from catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\ncatb = CatBoostClassifier(eval_metric = 'AUC')\ncatb= catb.fit(X_train, Y_train,cat_features=cat_col,eval_set=(X_test, Y_test),early_stopping_rounds=50,verbose=1000)\ny_cat = catb.predict(X_test)\nprobs_cat_train = catb.predict_proba(X_train)[:, 1]\nprobs_cat_test = catb.predict_proba(X_test)[:, 1]\nroc_auc_score(Y_train, probs_cat_train)\nroc_auc_score(Y_test, probs_cat_test)","b6c9188b":"col=['Gender', 'Age', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage_m']","f8e2ff46":"submmission = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/sample_submission.csv')","534c102b":"cat_pred= catb.predict_proba(df_test[col])[:, 1]\nsubmmission['Response']=cat_pred\nsubmmission.to_csv(\"cat_after_FE.csv\", index = False)","9308fa1a":"submmission.head()","08acd6bd":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\n\ndef bayes_parameter_opt_lgb(X_train, Y_train, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=10000, learning_rate=0.05, output_process=False):\n    train_data = lgb.Dataset(data=X_train, label=Y_train, categorical_feature = cat_col, free_raw_data=False)\n    # parameters\n    def lgb_eval(num_leaves,min_child_samples, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n        params = {'application':'binary','num_iterations': n_estimators, 'learning_rate':learning_rate, 'early_stopping_round':100, 'metric':'auc'}\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params[\"min_child_samples\"] = int(round(min_child_samples))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['lambda_l1'] = max(lambda_l1, 0)\n        params['lambda_l2'] = max(lambda_l2, 0)\n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, shuffle=True, verbose_eval =200, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n    # range \n    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n                                        'min_child_samples': (10,500),\n                                        'feature_fraction': (0.1, 0.9),\n                                        'bagging_fraction': (0.5, 1),\n                                        'max_depth': (5, 9),\n                                        'lambda_l1': (0, 5),\n                                        'lambda_l2': (0, 3),\n                                        'min_split_gain': (0.001, 0.1),                                        \n                                        'min_child_weight': (5, 50)}, random_state=0)\n# optimize\n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n\n    model_auc=[]\n    for model in range(len( lgbBO.res)):\n        model_auc.append(lgbBO.res[model]['target'])\n\n    # return best parameters\n    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n","fe5812c2":"opt_params = bayes_parameter_opt_lgb(X_train, Y_train, init_round=20, opt_round=50, n_folds=3, random_seed=6, n_estimators=1000, learning_rate=0.05)","dc339cb1":"(opt_params)","c81c8ff8":"opt_parameters = opt_params[1]","37f612c8":"opt_parameters[\"bagging_fraction\"]","524a57ce":"import lightgbm as lgb\nopt_model_lgb = lgb.LGBMClassifier(bagging_fraction = opt_parameters[\"bagging_fraction\"],\n                                   feature_fraction = opt_parameters[\"feature_fraction\"],\n                                   lambda_l1 = opt_parameters[\"lambda_l1\"],\n                                   lambda_l2 = opt_parameters[\"lambda_l2\"],\n                                   max_depth = int(round(opt_parameters[\"max_depth\"])),\n                                   min_child_weight = opt_parameters[\"min_child_weight\"],\n                                   min_child_samples = int(round(opt_parameters[\"min_child_samples\"])),\n                                   min_split_gain = opt_parameters[\"min_split_gain\"],\n                                   num_leaves = int(round(opt_parameters[\"num_leaves\"]))\n                                    )","3ad64139":"opt_model_lgb.fit(X_train,Y_train)","e59cfa99":"from sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nprobs_lgb_train = opt_model_lgb.predict_proba(X_train)[:, 1]\nprobs_lgb_test = opt_model_lgb.predict_proba(X_test)[:, 1]\nroc_auc_score(Y_train, probs_lgb_train)\nroc_auc_score(Y_test, probs_lgb_test)\npred_lgb = opt_model_lgb.predict_proba(X_test)\nroc_auc_score(Y_test, probs_lgb_test)","4760ad91":"lgb_pred= opt_model_lgb.predict_proba(df_test[col])[:, 1]\nsubmmission['Response']=lgb_pred\nsubmmission.to_csv(\"lgb_after_FE.csv\", index = False)","0ed7fabe":"average_pred = (cat_pred + lgb_pred)\/2","f6a29833":"average_pred[:5]","ae65f070":"submmission['Response']=average_pred\nsubmmission.to_csv(\"Stacking_of_CAT_LGB_after_FE.csv\", index = False)","72274a4e":"Thanks to Analytics Vidhya for this competition. This is my first competition and it gave me amazing experience and amazing learning and thanks to amazing other competitors and their suggestions and comments which helped me to improve my Score. My Public Rank was **61** and my private rank is **52**\n\nBelow is my modelling procedure for my final submission.\n\n* Used combination of **CATboosting** and **LightGBM** models.\n* Used **Bayes Optimization** for the first time for Hyperparameter Tuning of LightGBM models. (This was a great learning).\n* **Feature Engineering -** Tried various feature engineering techniques on Policy Channel and Premium columns but could not improve the model much. But changing Vintage into Months has helped me to improve the model. (May be a great scope for me to learn more on this aspect. I wish I become better and better as I move forward in this journey). \n* **Imbalanced Classification -** I tried to use SMOTE for the first time which was given 95% score on balanced data. But when I tested the model on the original Imbalanced data, the score was really bad. So balancing the data hasn't worked much for me in this competition. So finally had gone with **Stratified** technique which always kept the score good.\n* **Trust your CV score -** My final submission is performing less accurate on the Public Score. But it's cross validation score was higher during the modelling process. Thanks that I submitted this, it might have helped to improve few ranks on private board :).\n\nFinally, congratulations to Gaurav for staying ahead all through the competition buddy :).","406dc06a":"## Label Encoding","4e8553de":"As we observe converting the Vintage from days into Month is helping us to get good relationship between the Vintage and Target Variable.","8792dbda":"## Correlation","2c148c67":"## Mutual Information Test","05158722":"## Average of all Two Models","40e21803":"**Vintage** Lets try to understand more about Vintage column and why it there is not much relation between the Target variable and Vintage.","141a7561":"<div align='center'><font size=\"6\" color=\"#075b8c\"> Cross Sell My final Submission <\/font><\/div>","67cd18e8":"## Importing Data","9d2c3a09":"Here we can observe a strong negative correlation between **Previously Insured** and **Vehicle Damage**.","8e1220cf":"**id -** Unique ID for the customer \n\n**Gender -** Gender of the customer - Binary Variable\n\n**Age -** Age of the customer - \n\n**Driving_License -**\t0 : Customer does not have DL, 1 : Customer already has DL - Categorical Variable\n\n**Region_Code -**\tUnique code for the region of the customer - Nominal Variable\n\n**Previously_Insured -**\t1 : Customer already has Vehicle Insurance, 0 : Customer doesn't have Vehicle Insurance - Categorical Variable\n\n**Vehicle_Age -**\tAge of the Vehicle - Ordinal Variable\n\n**Vehicle_Damage -**\n1 : Customer got his\/her vehicle damaged in the past.\n0 : Customer didn't get his\/her vehicle damaged in the past.\n\n**Annual_Premium -**\tThe amount customer needs to pay as premium in the year\n\n**Policy_Sales_Channel -**\tAnonymised Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n\n**Vintage -**\tNumber of Days, Customer has been associated with the company\n\n**Response -**\t1 :  Customer is interested, 0 : Customer is not interested\n","a9a89470":"<div align='center'><font size=\"6\" color=\"#075b8c\"> Thank You All <\/font><\/div>","0496498e":"## LIGHT GBM","84d3e285":"## Some Feature Engineering","8fc86b64":"## Importing the Libraries"}}