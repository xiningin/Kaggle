{"cell_type":{"a8d53cfc":"code","ae3895f6":"code","5b0360c7":"code","1a317237":"code","702e3b99":"code","ee27c8b0":"code","57a31e7c":"code","064781e7":"code","3ab32e93":"code","3696ff34":"code","960cb56b":"code","41bbbacc":"code","5c2e91ab":"code","19d0699c":"code","a0b9ac1f":"code","6b0c238b":"code","8fa40728":"code","2e8551b3":"code","45890780":"code","85d93ff7":"code","2e971cfd":"code","9599fc9b":"code","8b4b37c5":"code","30ac97a6":"code","a0cbec41":"code","83b23e5b":"code","e1d5c217":"code","092a7dbf":"code","64edfd14":"code","2a2ed33c":"code","f32282b1":"code","103f316c":"code","cff12774":"code","4bf44113":"code","0a05e84a":"code","cd94ff5b":"code","1ca68dac":"code","06cad43b":"code","11db1c06":"code","b07c5082":"code","036f8953":"code","136b9166":"code","ac2a59ba":"code","73b8f85f":"code","f8cf9a62":"code","1ba5a59a":"code","8445b27c":"code","d99624d6":"code","b8b318e2":"code","e99e4b8f":"code","a16e77f9":"code","21aa2dab":"code","1ed95d16":"code","95c3d9bd":"code","f790208a":"code","82b037fe":"code","d074b359":"code","424f98dc":"code","1d9e5088":"code","0aeb4452":"code","cbfb5b4e":"markdown","5ae6d350":"markdown","25e04f64":"markdown","e7c9a6ab":"markdown","a1147477":"markdown","ac63b803":"markdown","6f18f947":"markdown","3e828574":"markdown","e447d64d":"markdown","68a1fefd":"markdown","d246c095":"markdown","302fb188":"markdown","9c7368ab":"markdown","58fd887d":"markdown","30a60e61":"markdown","7d5d711b":"markdown","463cce5a":"markdown","c2e63b6a":"markdown","ce4f07cd":"markdown","cf05f0c0":"markdown","ba1f0c30":"markdown","5403d9e5":"markdown","c8cebcd3":"markdown","e3ea4b29":"markdown","56ad7755":"markdown","d711fb40":"markdown","fcaf80ef":"markdown","242d54ba":"markdown","c19bc66d":"markdown","64582287":"markdown","7396d24f":"markdown","80447bf9":"markdown","1d208f11":"markdown","2bba5b2b":"markdown","0bd9abb2":"markdown","7da8a349":"markdown","69f3ac34":"markdown","190935ff":"markdown","27998304":"markdown","93200333":"markdown","0256b4cc":"markdown","a9655351":"markdown","f0c435ea":"markdown","85367488":"markdown","2c37d5ac":"markdown","413bcdf8":"markdown","40cfc1e9":"markdown","fbff5827":"markdown","5f391e92":"markdown"},"source":{"a8d53cfc":"import numpy as np\nimport pandas as pd","ae3895f6":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","5b0360c7":"movies=pd.read_csv('\/kaggle\/input\/movieratings\/movies_data.csv')\nratings=pd.read_csv('\/kaggle\/input\/movieratings\/ratings.csv')","1a317237":"movies.head()","702e3b99":"movies['movieId']=movies['id']\nmovies = movies.drop(\"id\", axis=1)","ee27c8b0":"movies.tail()","57a31e7c":"temp = movies[movies['overview'] == 'Released'].index\n# Removing these row indexes from dataFrame\nmovies.drop(temp, inplace=True)","064781e7":"ratings.head()","3ab32e93":"movies.shape","3696ff34":"ratings.shape","960cb56b":"movies.isnull().sum()","41bbbacc":"ratings.isnull().sum()","5c2e91ab":"movies['movieId']=movies['movieId'].astype(int)\nratings['movieId']=ratings['movieId'].astype(int)\ndf=movies.merge(ratings,on='movieId')\ndf.head()","19d0699c":"df=df.drop(\"userId\",axis=1)","a0b9ac1f":"#grouping data by title and count of ratings\nratings_count=df.groupby(['title'], as_index = False)[['rating']].count()\nratings_count['ratings_count']=ratings_count['rating']\nratings_count=ratings_count.drop(\"rating\", axis=1)\nratings_count.head()","6b0c238b":"ratings_count.describe()\n","8fa40728":"final_df=pd.merge(df,ratings_count,on='title')\nfinal_df.head()","2e8551b3":"final_df_=final_df.groupby(['movieId','title','genres','overview','ratings_count'], as_index = False)[['rating']].mean()\nfinal_df_['ratings_mean']=final_df_['rating']\nfinal_df_=final_df_.drop(\"rating\",axis=1)\nfinal_df_.head()","45890780":"final_df_.shape","85d93ff7":"c=final_df_['ratings_mean'].mean()\nc","2e971cfd":"m=final_df_['ratings_count'].quantile(0.85)\nm","9599fc9b":"final_df_.shape","8b4b37c5":"q_movies=final_df_.copy().loc[final_df_['ratings_count'] >= m]\nq_movies.shape","30ac97a6":"def weighted_rating(x, m=m, c=c):\n    v = x['ratings_count']\n    R = x['ratings_mean']\n    # Calculation based on the IMDB formula\n    return (v\/(v+m) * R) + (m\/(m+v) * c)","a0cbec41":"# Define a new feature 'score' and calculate its value with `weighted_rating()`\nq_movies['score'] = q_movies.apply(weighted_rating, axis=1)","83b23e5b":"#Sort movies based on score calculated above\nq_movies = q_movies.sort_values('score', ascending=False)\n\n#Print the top 15 movies\nq_movies[['title', 'ratings_count', 'ratings_mean', 'score']].head(10)","e1d5c217":"import matplotlib.pyplot as plt\nplt.hist(q_movies['score'], bins = 20)\nplt.xlabel(\"Number\")\nplt.ylabel(\"Frequency\")\nplt.show()","092a7dbf":"Most_rated=ratings_count.nlargest(20,'ratings_count')","64edfd14":"plt.figure(figsize=(30,10))\nplt.title('Top 20 movies with highest ratings count',fontsize=40)\nplt.ylabel('ratings_count',fontsize=30)\nplt.xticks(fontsize=25,rotation=90)\nplt.xlabel('movies title',fontsize=30)\nplt.yticks(fontsize=25)\nplt.bar(Most_rated['title'],Most_rated['ratings_count'],linewidth=3,edgecolor='red',color='purple')","2a2ed33c":"ratings_sum=df.groupby(['title'])[['rating']].sum()\nratings_sum=ratings_sum.nlargest(20,'rating')","f32282b1":"plt.figure(figsize=(30,10))\nplt.title('Top 20 movies with highest rating sum',fontsize=40)\nplt.ylabel('ratings_sum',fontsize=30)\nplt.xticks(fontsize=25,rotation=90)\nplt.xlabel('movies title',fontsize=30)\nplt.yticks(fontsize=25)\nplt.bar(ratings_sum.index,ratings_sum['rating'],linewidth=3,edgecolor='red',color='green')","103f316c":"final_df_['overview'].head(5)","cff12774":"#Import TfIdfVectorizer from scikit-learn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\ntfidf = TfidfVectorizer(stop_words='english')\n\n#Replace NaN with an empty string\nfinal_df_['overview'] = final_df_['overview'].fillna('')\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(final_df_['overview'])\n\n#Output the shape of tfidf_matrix\ntfidf_matrix.shape\n","4bf44113":"# Import linear_kernel\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","0a05e84a":"#Construct a reverse map of indices and movie titles\nindices = pd.Series(final_df_.index, index=final_df_['title']).drop_duplicates()","cd94ff5b":"# Function that takes in movie title as input and outputs most similar movies\ndef get_recommendations(title, cosine_sim=cosine_sim):\n    # Get the index of the movie that matches the title\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar movies\n    return final_df_['title'].iloc[movie_indices]","1ca68dac":"get_recommendations('Shadows in Paradise')","06cad43b":"get_recommendations('Star Wars')","11db1c06":"get_recommendations('Four Rooms')","b07c5082":"# Parse the stringified features into their corresponding python objects\nfrom ast import literal_eval\nfinal_df_['genres'] = final_df_['genres'].apply(literal_eval)","036f8953":"# Returns the list top 3 elements or entire list; whichever is more.\ndef get_list(x):\n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n        if len(names) > 3:\n            names = names[:3]\n        return names\n\n    #Return empty list in case of missing\/malformed data\n    return []","136b9166":"final_df_['genres'] = final_df_['genres'].apply(get_list)","ac2a59ba":"final_df_[['genres']].head(10)","73b8f85f":"# Function to convert all strings to lower case and strip names of spaces\ndef clean_data(x):\n    if isinstance(x, list):\n        return [str.lower(i.replace(\" \", \"\")) for i in x]\n    else:\n        #Check if director exists. If not, return empty string\n        if isinstance(x, str):\n            return str.lower(x.replace(\" \", \"\"))\n        else:\n            return ''","f8cf9a62":"final_df['genres']=final_df['genres'].apply(clean_data)","1ba5a59a":"def create_soup(x):\n  return ''.join(x['genres'])\n\nfinal_df_['soup']=final_df_.apply(create_soup, axis=1)\n","8445b27c":"# Import CountVectorizer and create the count matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(final_df_['soup'])\n\n","d99624d6":"# Compute the Cosine Similarity matrix based on the count_matrix\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_sim2 = cosine_similarity(count_matrix, count_matrix)","b8b318e2":"# Reset index of our main DataFrame and construct reverse mapping as before\nfinal_df_ = final_df_.reset_index()\nindices = pd.Series(final_df_.index, index=final_df_['title'])","e99e4b8f":"get_recommendations('Shadows in Paradise', cosine_sim2)","a16e77f9":"get_recommendations('Star Wars', cosine_sim2)","21aa2dab":"get_recommendations('Four Rooms', cosine_sim2)","1ed95d16":"# Reset index of our main DataFrame and construct reverse mapping as before\nfinal_df_ = final_df_.reset_index()\nindices = pd.Series(final_df_.index, index=final_df_['title'])","95c3d9bd":"ratings.head()","f790208a":"pip install scikit-surprise","82b037fe":"from surprise import SVD\nfrom surprise import Dataset\nfrom surprise import accuracy\nfrom surprise import Reader\nfrom surprise.model_selection import train_test_split\n\nreader = Reader()\ndata = data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n# sample random trainset and testset\n# test set is made of 25% of the ratings.\ntrainset, testset = train_test_split(data, test_size=.25)\n\n# We'll use the famous SVD algorithm.\nalgo = SVD()\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nalgo.fit(trainset)\npredictions = algo.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(predictions)","d074b359":"predictions","424f98dc":"prediction = pd.DataFrame(predictions, columns=['user_id','movie_id','actual_ratings','pred_ratings','details']).to_csv('Predictions.csv')","1d9e5088":"prediction = pd.DataFrame(predictions, columns=['user_id','movie_id','actual_ratings','pred_ratings','details'])","0aeb4452":"prediction.head()","cbfb5b4e":"# **Recommendation based on Demographic Filtering**","5ae6d350":"latent factor is a broad idea which describes a property or concept that a user or an item have. SVD decreases the dimension of the utility matrix by extracting its latent factors. Essentially, we map each user and each item into a latent space with dimension r. Therefore, it helps us better understand the relationship between users and items as they become directly comparable","25e04f64":"Here, the minimun number of ratings count is 1 and max is 324.","e7c9a6ab":"**Task**:\n\nTo recommend optimal movies to user and predict user ratings. ","a1147477":"We'll be using the Surprise library to implement SVD.","ac63b803":"**Mean of all the ratings**","6f18f947":"# **Genres based-recommender**","3e828574":"**Most_rated movies based on ratings count**","e447d64d":"**Repalcing column name \"id\" with \"movieId\" to merge with ratings.**","68a1fefd":"# **Loading Data**","d246c095":"It goes without saying that the quality of our recommender would be increased with the usage of better metadata. ","302fb188":"# **Importing Libraries**","9c7368ab":"In this recommender system the content of the movie (genres) is used to find its similarity with other movies. Then the movies that are most likely to be similar are recommended.","58fd887d":"We will be using the cosine similarity to calculate a numeric quantity that denotes the similarity between two movies.\n\nSince we have used the TF-IDF vectorizer, calculating the dot product will directly give us the cosine similarity score. Therefore, we will use sklearn's linear_kernel() instead of cosine_similarities() since it is faster","30a60e61":"We will compute pairwise similarity scores for all movies based on their plot descriptions and recommend movies based on that similarity score. The plot description is given in the overview feature of our dataset. Let's take a look at the data.","7d5d711b":"We see that our recommender has been successful in capturing more information due to more metadata and has given us (arguably) better recommendations.","463cce5a":"One way to handle the scalability and sparsity issue created by CF is to leverage a latent factor model to capture the similarity between users and items. Essentially, we want to turn the recommendation problem into an optimization problem. We can view it as how good we are in predicting the rating for items given a user. One common metric is Root Mean Square Error (RMSE). The lower the RMSE, the better the performance.","c2e63b6a":"**About Dataset:**\n\n\u25cf movies_data.csv - The data contains -\nId - which is movie id\nGenres - it can contain multiple genres for a single movie.\nOverview - A textual description about the movie\n\n\u25cf ratings.csv -\nUserid - id for user\nMovieid - id for movie\nRating - given by user to that movie (out of 5)","ce4f07cd":"**Approach:**\n\nDemographic, Content-Based and collaborative filtering using SVD.\n\n","cf05f0c0":"We are now in a good position to define our recommendation function. These are the following steps we'll follow :-\n\nGet the index of the movie given its title.\n\nGet the list of cosine similarity scores for that particular movie with all movies. Convert it into a list of tuples where the first element is its position and the second is the similarity score.\n\nSort the aforementioned list of tuples based on the similarity scores; that is, the second element.\n\nGet the top 10 elements of this list. Ignore the first element as it refers to self (the movie most similar to a particular movie is the movie itself).\nReturn the titles corresponding to the indices of the top elements","ba1f0c30":"**Score Distribution**","5403d9e5":"We are going to define a function that takes in a movie title as an input and outputs a list of the 10 most similar movies. Firstly, for this, we need a reverse mapping of movie titles and DataFrame indices. In other words, we need a mechanism to identify the index of a movie in our metadata DataFrame, given its title.","c8cebcd3":"We see that over 17,000 different words were used to describe the 2800 movies in our dataset.","e3ea4b29":"While our system has done a decent job of finding movies with similar plot descriptions, the quality of recommendations is not that great","56ad7755":"**Final Data frame**","d711fb40":"**Single Value Decomposition**","fcaf80ef":"# **Collaborative Filtering**","242d54ba":"**Merging df and ratings_count**","c19bc66d":"Next, we'll write functions that will help us to extract the required information from genres.","64582287":"Now, we can filter out the movies that qualify for the chart","7396d24f":"We see that there are 427 movies which qualify to be in this list. Now, we need to calculate our metric for each qualified movie. To do this, we will define a function, weighted_rating() and define a new feature score, of which we'll calculate the value by applying this function to our DataFrame of qualified movies:","80447bf9":"***Here, few movieIds have unsuitable format. So we'll remove the rows with overview='Released'. Also id has been replaced with movieId. ***","1d208f11":"Here we have all the predictions of Test set","2bba5b2b":"# **Movie Recommendation System and user ratings prediction**","0bd9abb2":"# **Exploratory Data analysis and Feature engineering**","7da8a349":"# **Content Based Filtering**","69f3ac34":"# **Movie Recommendation Engine**","190935ff":"**Quantile**","27998304":"So, the mean rating for all the movies is approx 3 on a scale of 5.The next step is to determine an appropriate value for m, the minimum votes required to be listed in the chart. We will use 80th percentile as our cutoff. In other words, for a movie to feature in the charts, it must have more votes than at least 85% of the movies in the list.","93200333":"# **overviews-based Recommender**","0256b4cc":"We are now in a position to create our \"metadata soup\", which is a string that contains all the metadata that we want to feed to our vectorizer (genres).","a9655351":"# **Data Visualisation**","f0c435ea":"**Higest rated movies based on ratings sum**","85367488":"We can see the RMSE value is 0.9.","2c37d5ac":"The engine that we built is not really personal in that it doesn't capture the personal tastes and biases of a user. Anyone querying our engine for recommendations based on a movie will receive the same recommendations for that movie, regardless of who she\/he is.","413bcdf8":"The next step would be to convert the names and keyword instances into lowercase and strip all the spaces between them. ","40cfc1e9":"converting the word vector of each overview.Now we'll compute Term Frequency-Inverse Document Frequency (TF-IDF) vectors for each overview.","fbff5827":"content based engine suffers from some severe limitations. It is only capable of suggesting movies which are close to a certain movie. That is, it is not capable of capturing tastes and providing recommendations across genres","5f391e92":"# **Merging Data for Demographic Filtering**"}}