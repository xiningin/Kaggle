{"cell_type":{"a91047ba":"code","f8ebb7d9":"code","aa1e1b58":"code","74812f90":"code","39236e94":"code","24f6654f":"code","e45f33c2":"code","47a5203a":"code","58a0cc96":"code","3215c6dd":"code","32b9a810":"code","fa412d43":"code","9aab3bb1":"code","7cd79f74":"code","820d3597":"code","05a56bf3":"code","24128ade":"code","bab88bca":"code","b8c692a4":"code","00317ffd":"code","0010886e":"code","c36d6359":"code","8b616cb7":"code","28e3df7c":"code","93efda54":"code","d30f9738":"code","dc2f7c0f":"code","6538b737":"code","35cc254e":"code","b45de864":"code","ec99c8cb":"code","889d16c9":"code","cc047b89":"code","f42f44c1":"code","3fadfdac":"code","7161ee03":"code","2b366d3a":"code","70bf9c59":"code","84ce98f6":"code","f1c9e41d":"code","86894519":"code","fd6f0a37":"code","d688e9bb":"code","9e9420fb":"markdown","cbb9dabc":"markdown","c59f3e88":"markdown","4883649d":"markdown","992e3f37":"markdown","5b53010b":"markdown","6c7e465a":"markdown","48bac840":"markdown","1ddb1e8e":"markdown","b784d77c":"markdown","3ce0bedb":"markdown","ba3cbb51":"markdown","efb72906":"markdown","3e6bb7fd":"markdown","bf11640e":"markdown","44a94c8b":"markdown","0e3a5f9f":"markdown","f0def5b6":"markdown","a0ad3c43":"markdown","da0c98fc":"markdown","1ddd4bf0":"markdown","2c951d20":"markdown","c0f61c89":"markdown","325df879":"markdown"},"source":{"a91047ba":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\ninit_notebook_mode(connected=True)\npd.set_option('display.max_columns', 5000)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#os.mkdir('\/kaggle\/working\/individual_charts\/')\nimport matplotlib.pyplot as plt\n# Load the data\n#Will come in handy to wrap the lengthy texts\nimport textwrap\n#useful libraries and functions\nfrom itertools import repeat\n#Libraries that give a different visual possibilities\nfrom pandas import option_context \nfrom plotly.subplots import make_subplots\n\ndef long_sentences_seperate(sentence, width=30):\n    try:\n        splittext = textwrap.wrap(sentence,width)\n        text = '<br>'.join(splittext)#whitespace is removed, and the sentence is joined\n        return text\n    except:\n        return sentence\n\ndef load_csv(base_dir,file_name):\n    \"\"\"Loads a CSV file into a Pandas DataFrame\"\"\"\n    file_path = os.path.join(base_dir,file_name)\n    df = pd.read_csv(file_path,low_memory=False)\n    return df    \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","f8ebb7d9":"base_dir = '..\/input\/widsdatathon2022\/'\nfile_name = 'train.csv'\ndataset_main = load_csv(base_dir,file_name)","aa1e1b58":"#Single Factor Bar Graphs\ndef uni_factor(factor):\n    grp_factor = dataset_main.groupby(factor)['floor_area'].count().reset_index()\n    grp_factor.sort_values(by='floor_area',inplace=True,ascending = False)\n    grp_factor[factor] = grp_factor[factor].astype('category')\n    vis = px.bar(data_frame=grp_factor,y = factor,x ='floor_area',color= factor)\n    vis.update_layout(yaxis = {'categoryorder' : 'total ascending'},\n                      title = 'Number of Buildings based on ' + factor,\n                     height = 800)\n    vis.show()\n\n#Single Factor Histogram Graphs    \ndef uni_hist_plot(independent,dependent):\n    vis = px.histogram(data_frame=dataset_main,x=dependent,color=independent)\n    vis.update_layout(title='Distribution of '+ dependent + ' based on '+ independent)\n    vis.show()\n\n#Single Factor Box Plot Graphs    \ndef uni_box_plot(independent,dependent):\n    vis = px.box(data_frame=dataset_main,x=dependent,color=independent)\n    vis.update_layout(title='Box plot of '+ dependent + ' based on '+ independent)\n    vis.show()\n\n#Two Factor Scatter Plot Graphs \ndef two_factor(factor1, factor2,independent):\n    vis = px.scatter(data_frame=dataset_main,y = factor1,x =factor2,color=independent,\n                     facet_col=independent,facet_col_wrap=3)\n    vis.update_layout(title = 'Relation between ' + factor1 + ' and ' +factor2 + ' in ' + independent + 'condition',\n                     height = 1000)\n    vis.show()\n\n#Single factor target average Bar Graph    \ndef avg_on_factor(factor,target):\n    grp_factor = dataset_main.groupby(factor)[target].mean().reset_index()\n    grp_factor.sort_values(by=target,inplace=True,ascending = False)\n    grp_factor[factor] = grp_factor[factor].astype('category')\n    vis = px.bar(data_frame=grp_factor,y = factor,x =target,color= factor)\n    vis.update_layout(yaxis = {'categoryorder' : 'total ascending'},\n                      title = 'Average of '+target +' on basis of ' + factor,\n                     height = 800)\n    vis.show()\n    \n\ndef corr_heat_map(df,title):\n\n    #Building the dataset with column that are numerical\n    df = df[df.columns[df.dtypes != 'object']]\n    df_corr_mat = df.corr() #building the correlation matrix\n    #Building the lower triagle of the correlation matrix\n    df_corr_mat_lt = df_corr_mat.where(np.tril(np.ones(df_corr_mat.shape)).astype(np.bool))\n    vis = px.imshow(df_corr_mat_lt,aspect=\"auto\",\n                    height=1000,color_continuous_scale='spectral',width=900)\n    vis.update_layout(title=title)\n    vis.show()","74812f90":"#Collecting garbage memory and deleting unwanted Dataframes, that have served their purpose earlier\nimport gc\ngc.collect()","39236e94":"#https:\/\/stackoverflow.com\/a\/46581125\/16388185\ndef clean_dataset(df):\n    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n    df.dropna(inplace=True)\n    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n    return df[indices_to_keep].astype(np.float64)","24f6654f":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\n\n#Import Model Packages \nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom numpy.linalg import inv, eig, svd\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import KernelPCA\n# Error Metrics\nfrom sklearn.metrics import mean_squared_error","e45f33c2":"dataset_main.direction_max_wind_speed = dataset_main.direction_max_wind_speed.fillna(1)\ndataset_main.direction_peak_wind_speed = dataset_main.direction_peak_wind_speed.fillna(1)\ndataset_main.max_wind_speed = dataset_main.max_wind_speed.fillna(1)\ndataset_main.days_with_fog = dataset_main.days_with_fog.fillna(dataset_main.days_with_fog.median())\ndataset_main.loc[dataset_main.energy_star_rating.isna(),'energy_star_rating'] = np.median(dataset_main.loc[~dataset_main.energy_star_rating.isna(),'energy_star_rating'])\ndataset_main.loc[dataset_main.year_built.isna(),'year_built'] = np.median(dataset_main.loc[~dataset_main.year_built.isna(),'year_built'])\n\n#That leaves the Energy star rating. We need to check the test set provided for deciding\n\n# Segregating the columns with categorical value\ndata_cat = dataset_main[dataset_main.columns[dataset_main.dtypes == 'object']]\n\ndata_num = dataset_main[dataset_main.columns[dataset_main.dtypes != 'object']]\ndata_num.drop('Year_Factor',axis=1,inplace=True)\n#A simple and straight_forward one-hot encoding using Pandas' Get_Dummies\ndata_cat = pd.get_dummies(data_cat,columns=data_cat.columns)\n\ndata_model = pd.merge(left=data_cat,right=data_num,left_index=True,right_index=True)","47a5203a":"#Creating the X the Independent variables and Y the Target or Dependent variables\ndata_model = clean_dataset(data_model)\n#We lost another 2500 entries to the big number and infinity issues\nX = data_model.iloc[:,:-2]\nY = data_model.site_eui","58a0cc96":"# split out validation dataset for the end\n\nvalidation_size = 0.2\n\n#In case the data is not dependent on the time series, then train and test split randomly\nseed = 7\n# X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n\n#In case the data is not dependent on the time series, then train and test split should be done based on sequential sample\n#This can be done by selecting an arbitrary split point in the ordered list of observations and creating two new datasets.\n\ntrain_size = int(len(X) * (1-validation_size))\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\nY_train, Y_test = Y[0:train_size], Y[train_size:len(X)]","3215c6dd":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X_train)\nrescaledDataset = pd.DataFrame(scaler.fit_transform(X_train),\n                               columns = X_train.columns, index = X_train.index)\n# summarize transformed data\nX_train.dropna(how='any', inplace=True)\nrescaledDataset.dropna(how='any', inplace=True)","32b9a810":"scaler = StandardScaler().fit(X_test)\nrescaledtestset = pd.DataFrame(scaler.fit_transform(X_test),\n                               columns = X_test.columns, index = X_test.index)\n# summarize transformed data\nX_test.dropna(how='any', inplace=True)\nrescaledtestset.dropna(how='any', inplace=True)","fa412d43":"#Function to calculate the explained variance\ndef svd_cal(data, components):\n    ncomps = components\n\n    #Instantiating SVD\n    svd = TruncatedSVD(n_components=ncomps)\n    svd_fit = svd.fit(data)\n\n    #predicting the target values using the rescaled data\n    Y_pred = svd.fit_transform(data)\n    \n    #calculating the cumulative variance preserved\n    return round(svd_fit.explained_variance_ratio_.cumsum()[-1]*100,2)","9aab3bb1":"# plotting the retained variance based on the parameters\n    \nexp_var = []\nfor para in range(5, 70):\n    exp_var.append(svd_cal(data= rescaledDataset, components= para))\n\nvis1 = go.Figure()\nvis1.add_trace(go.Scatter(x=np.arange(1, len(exp_var) + 1),y=exp_var,mode='lines',name='explained_variance'))\n\nvis1.update_xaxes(title = \"Cumulative Sum\")\nvis1.update_yaxes(title = \"Number of Parameters\")\n\nvis1.show()","7cd79f74":"#Function to reduce the dimensions\ndef svd_red(dataset,comps):\n    svd = TruncatedSVD(n_components=comps)\n    svd_fit = svd.fit(dataset)\n    Y_pred = svd.fit_transform(dataset)\n    dfsvd = pd.DataFrame(Y_pred, columns=['c{}'.format(c) for c in range(comps)], index=dataset.index)\n    return dfsvd","820d3597":"# Based on the number of components, the final X_train svd is executed\ndfsvd = svd_red(dataset=rescaledDataset,comps=60)\ndfsvd_test = svd_red(dataset=rescaledtestset,comps=60)\nprint(dfsvd_test.shape)\nprint(dfsvd.shape)","05a56bf3":"svdcol = dfsvd.columns\ndfsvd['signal'] = Y_train\ncorr_heat_map(dfsvd,'Visualising the Truncated Parameters')","24128ade":"#Two Factor tsne Scatter Plot Graphs \ndef tsne_scatter(data):\n    vis = px.scatter(data_frame=data,y = 'y',x = 'x',color='signal')\n    vis.update_layout(title = 'Scatterplot of a Multiple dimension dataset reduced to 2D using t-SNE',\n                     height = 1000)\n    vis.show()","bab88bca":"#Reducing the SVD dataframe further down to 2 dimensions using tSNE\ntsne = TSNE(n_components=2, random_state=0)\n\nZ = tsne.fit_transform(dfsvd[svdcol])\ndftsne = pd.DataFrame(Z, columns=['x','y'], index=dfsvd.index)\n\ndftsne['signal'] = Y_train\n\n#Reducing the SVD test dataframe further down to 2 dimensions using tSNE\ntsne = TSNE(n_components=2, random_state=0)\n\nZ = tsne.fit_transform(dfsvd_test)\ndftsne_test = pd.DataFrame(Z, columns=['x','y'], index=dfsvd_test.index)\n\ndftsne_test['signal'] = Y_test","b8c692a4":"tsne_scatter(dftsne)\ntsne_scatter(dftsne_test)","00317ffd":"num_folds = 10\nscoring = 'neg_mean_squared_error'\n#scoring ='neg_mean_absolute_error'\n#scoring = 'r2'\n\nmodels = []\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))","0010886e":"def machine_learn(X_train,Y_train,X_test,Y_test,models):\n    names = []\n    kfold_results = []\n    test_results = []\n    train_results = []\n    for name, model in models:\n        names.append(name)\n\n        ## K Fold analysis:\n\n        kfold = KFold(n_splits=num_folds, random_state=seed)\n        #converted mean square error to positive. The lower the beter\n        cv_results = -1* cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n        kfold_results.append(cv_results)\n\n\n        # Full Training period\n        print('{} model fit Started'.format(model))\n        res = model.fit(X_train, Y_train)\n        print('{} model fit Completed'.format(model))\n        #The error function is root of mean_squared_error\n        train_result = np.sqrt(mean_squared_error(res.predict(X_train), Y_train))\n        train_results.append(train_result)\n\n        # Test results\n        #The error function is root of mean_squared_error\n        test_result = np.sqrt(mean_squared_error(res.predict(X_test), Y_test))\n        test_results.append(test_result)\n        print('{} test result'.format(model))\n\n    return kfold_results, train_result, test_result\n        ","c36d6359":"#Learning using the SVD dataframe with 60 parameters\ndfsvd = dfsvd[svdcol] #the trainin data has to be removed\nkfold_results, train_result, test_result = machine_learn(X_train=dfsvd,Y_train=Y_train,\n                                                         X_test=dfsvd_test,Y_test = Y_test,\n                                                         models=models)\n","8b616cb7":"kfold = pd.DataFrame(kfold_results,columns=range(1,11)).T\nkfold.columns = [x[0] for x in models]","28e3df7c":"visB = go.Figure()\nfor kf in kfold.columns:\n    visB.add_trace(go.Box(x=kfold[kf],name=kf))\nvisB.update_xaxes(type='log')\nvisB.update_layout(title='Kfold Error Results')\nvisB.show()","93efda54":"# compare algorithms\nfig = plt.figure()\nnames = [x[0] for x in models]\n\nind = np.arange(len(names))  # the x locations for the groups\nwidth = 0.35  # the width of the bars\n\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.bar(ind - width\/2, train_result,  width=width, label='Train Error')\nplt.bar(ind + width\/2, test_result, width=width, label='Test Error')\nfig.set_size_inches(15,8)\nplt.legend()\nax.set_xticks(ind)\nax.set_xticklabels(names)\nplt.show()","d30f9738":"#Learning using the SVD dataframe with 60 parameters\nkfold_results, train_result, test_result = machine_learn(X_train=dftsne,Y_train=Y_train,\n                                                         X_test=dftsne_test,Y_test = Y_test,\n                                                         models=models)","dc2f7c0f":"kfold = pd.DataFrame(kfold_results,columns=range(1,11)).T\nkfold.columns = [x[0] for x in models]\n\nvisT = go.Figure()\nfor kf in kfold.columns:\n    visT.add_trace(go.Box(x=kfold[kf],name=kf))\nvisT.update_xaxes(type='log')\nvisT.update_layout(title='tSNE Kfold Error Results')\nvisT.show()","6538b737":"# compare algorithms\nfig = plt.figure()\nnames = [x[0] for x in models]\n\nind = np.arange(len(names))  # the x locations for the groups\nwidth = 0.35  # the width of the bars\n\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.bar(ind - width\/2, train_result,  width=width, label='Train Error')\nplt.bar(ind + width\/2, test_result, width=width, label='Test Error')\nfig.set_size_inches(15,8)\nplt.legend()\nax.set_xticks(ind)\nax.set_xticklabels(names)\nplt.show()","35cc254e":"#loading test_set\ntest_main = load_csv(base_dir='..\/input\/widsdatathon2022',file_name='test.csv')\n\n#Need to find an effective way to deal with all the columns, so lets try describe\ntest_main.describe()\n\ntest_main.direction_max_wind_speed = test_main.direction_max_wind_speed.fillna(1)\ntest_main.direction_peak_wind_speed = test_main.direction_peak_wind_speed.fillna(1)\ntest_main.max_wind_speed = test_main.max_wind_speed.fillna(1)\n#direction_max_wind_speed,  direction_peak_wind_speed,  max_wind_speed all can be safely filled with 1\ntest_main.days_with_fog = test_main.days_with_fog.fillna(0)\ntest_main.loc[test_main.energy_star_rating.isna(),'energy_star_rating'] = np.median(test_main.loc[~test_main.energy_star_rating.isna(),'energy_star_rating'])\ntest_main.loc[test_main.year_built.isna(),'year_built'] = np.median(test_main.loc[~test_main.year_built.isna(),'year_built'])\n\n# Segregating the columns with categorical value\ntest_cat = test_main[test_main.columns[test_main.dtypes == 'object']]\n\ntest_num = test_main[test_main.columns[test_main.dtypes != 'object']]\ntest_num.drop('Year_Factor',axis=1,inplace=True)\n#A simple and straight_forward one-hot encoding using Pandas' Get_Dummies\ntest_cat = pd.get_dummies(test_cat,columns=test_cat.columns)\n\ntest_model = pd.merge(left=test_cat,right=test_num,left_index=True,right_index=True)","b45de864":"scaler = StandardScaler().fit(test_model)\nscaledtest_act = pd.DataFrame(scaler.fit_transform(test_model),\n                               columns = test_model.columns, index = test_model.index)\n# summarize transformed data\nscaledtest_act.dropna(how='any', inplace=True)","ec99c8cb":"#Reducing the original re-scaled dataframe  to 2 dimensions using tSNE\n# Based on the number of components, the final X_train svd is executed\ntest_svd = svd_red(dataset=test_model,comps=60)\n\ntsne = TSNE(n_components=2, random_state=0)\n\nZ = tsne.fit_transform(test_svd)\ntsne_test = pd.DataFrame(Z, columns=['x','y'], index=test_svd.index)","889d16c9":"#Use the entire train and test set for training the lasso model for final result.\ntotal_data_tsne = pd.concat([dftsne,dftsne_test])\n#Seperate the Signal from the Independent variables.\nY = total_data_tsne.signal\nX = total_data_tsne[['x','y']]\n\nlasso = Lasso()\nlasso.fit(X, Y)\n\n#After 3 notebooks, the results arrive by this seemingly naive command\ntest_result = lasso.predict(tsne_test)","cc047b89":"my_submission_1 = pd.DataFrame({'id': test_main.id, 'site_eui': test_result})\n# you could use any filename. We choose submission here\nmy_submission_1.to_csv('submission.csv', index=False)","f42f44c1":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras","3fadfdac":"def preprocess(X, y):\n    print(X)\n    print(y)\n    return X, y\ndef make_dataset(feature, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices((feature, y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(256)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","7161ee03":"#In the model, the integerlook up model tree is not required\ndef get_model():\n    features_inputs = tf.keras.Input((127, ), dtype=tf.float16) # feature input of the WIDS\n    feature_x = layers.Dense(64, activation='relu')(features_inputs)\n    feature_x = layers.Dense(64, activation='relu')(feature_x)\n    feature_x = layers.Dense(64, activation='relu')(feature_x)\n    feature_x = layers.Dense(32, activation='relu')(feature_x)\n    feature_x = layers.Dense(16, activation='relu')(feature_x)\n    output = layers.Dense(1)(feature_x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=features_inputs, outputs=output)\n    model.compile(optimizer=tf.optimizers.Adam(0.0001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model","2b366d3a":"model = get_model()\nmodel.summary()","70bf9c59":"keras.utils.plot_model(model, show_shapes=True)","84ce98f6":"X = data_model.iloc[:,:-2]\nY = data_model.site_eui\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X)\nrescaledData = pd.DataFrame(scaler.fit_transform(X),\n                               columns = X.columns, index = X.index)\n# summarize transformed data\nX.dropna(how='any', inplace=True)\nrescaledData.dropna(how='any', inplace=True)","f1c9e41d":"%%time\nfrom sklearn.model_selection import KFold\nkfold = KFold(5, shuffle=True, random_state=42)\nmodels = []\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(rescaledData)):\n    X_train, X_val = rescaledData.iloc[train_indices], rescaledData.iloc[valid_indices]\n    y_train, y_val = Y.iloc[train_indices], Y.iloc[valid_indices]\n    train_ds = make_dataset(X_train, y_train)\n    valid_ds = make_dataset(X_val, y_val, mode=\"valid\")\n    \n    model = get_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}.tf\", save_best_only=True, save_weights_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=5)\n    history = model.fit(train_ds, epochs=100, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    \n    models.append(model)\n    pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n    plt.title(\"MSE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n    plt.title(\"MAE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    plt.title(\"RMSE\")\n    plt.show()\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    gc.collect()\n    break","86894519":"def preprocess_test(feature):\n    return (feature), 0\ndef make_test_dataset(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices((feature))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","fd6f0a37":"test_ds = make_test_dataset(scaledtest_act)\ny_pred = inference(models,test_ds)","d688e9bb":"my_submission= pd.DataFrame({'id': test_main.id, 'site_eui': y_pred.flatten()})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)\nmy_submission.head(2)","9e9420fb":"#### [Back to Contents](#contents)","cbb9dabc":"#### <a id='dmod'> DNN Model Building   <\/a>","c59f3e88":"#### [Back to Contents](#contents)","4883649d":"##### <a id='tsnelea'> Machine learning on Features  <\/a>","992e3f37":"#### <a id='finres'> Dimension reduction results  <\/a>","5b53010b":"#### <a id='dres'> Results and checking <\/a>","6c7e465a":"<a id='error'> error in filling null values  <\/a>\n\nIn the below cell, the commands to fill null values for the Energy_star_rating and Year_built created considerable head ache, still I realised the mistake. Always use the proper iloc or loc indexing, is the lesson","48bac840":"## <a id='contents'>  Contents <\/a>\n\n### [tSNE with Truncated SVD](#tsne)\n\n   #### [Transformation](#tsnemod)\n\n   #### [Visualisating features](#tsnevis)\n   \n   #### [Machine Learning on Features](#tsnelea)\n   \n   #### [Dimension reduction results](#finres)\n   \n   #### [Lessons of the Exercise ](#FinCon)\n\n### [DNN with Encoded Dataset ](#dnn)\n\n   #### [Model Building](#dmod)\n\n   #### [Fitting model ](#dfit)\n   \n   #### [Result & Understanding](#dres)\n","1ddb1e8e":"<a id='peek3'> tSNE ML test and train results  <\/a>","b784d77c":"##### <a id='peek1'> tSNE Visualisation linking SVD Train & Test variables to Target <\/a>","3ce0bedb":"#### <a id='tsne'> tSNE with Truncated SVD <\/a>\n\nDataset prepared by seperating the categorical and the numerical values, followed by one hot encoding the categorical columns. Intention is to understand how tSNE creates the features from so many parameters that make the dataset. Then use that features to model.  \n\n[1) Transformation](#tsnemod)\n\n[2) Visualisating Features](#tsnevis)\n\n[3) Machine learning on Features](#tsnelea)","ba3cbb51":"##### <a id='FinCon'> Lessons of the Exercise <\/a>\n\n The [results](#peek3) are downright strange. The test and training error are almost 0. Is this even possible? Public score was 62!!!!. Not any better. But there are lessons learnt \n\n\nThe [simple errors](#error) in filling null values can take away precious time. In the below cells there is place where I fill the null values of the energy_star_rating with median value. Initially, I had it all wrong. Due to which, I was unable to execute the tSNE algorithms.\n\n1) The important point learnt is using the tSNE and SVD for transforming the data in order to improve the final results\n\n2) Visualising tSNE of the data gives a different perspective about the kind of data, and the space in which the data is present. \n\n3) Challenge of working with the correct data, and creating functions and loops to automate the process. \n\nI came across the Ubiquant Competition. The dataset in that competition has 300 features, like the WIDS. https:\/\/www.kaggle.com\/lonnieqin\/ubiquant-market-prediction-with-dnn?kernelSessionId=85994078\n\nNext I am planning to try the DNN from this notebook. There are some modifications done to architecture. But the dense layers working on the main features have been kept same","efb72906":"#### [Back to Contents](#contents)","3e6bb7fd":"##### <a id='tsnemod'> Transformation<\/a>","bf11640e":"##### <a id='tsnevis'> Visualising the Parameters <\/a>\n\nThe parameters themselves don't seem to have even slightest correlation. However they all have some negative or positive correlation with the target value","44a94c8b":"<a id='peek2'> Machine learning function  <\/a>","0e3a5f9f":"#### [Back to Contents](#contents)","f0def5b6":"#### [Back to Contents](#contents)","a0ad3c43":"#### <a id='#dnn'> DNN with Encoded Dataset <\/a>\n\n   #### [Model Building](#dmod)\n\n   #### [Fitting model ](#dfit)\n   \n   #### [Result & Understanding](#dres)","da0c98fc":"#### [Back to Contents](#contents)","1ddd4bf0":"The SVD dataset has not provided any good result, the test error is still above 56, the value that was achieved in the earlier notebooks. Next step is to use the tSNE reduced dataset","2c951d20":"#### <a id='dfit'> DNN Model Fitting <\/a>","c0f61c89":"#### [Back to Contents](#contents)","325df879":"### Purpose of the Notebook:\n\nEDA was done, which gave good understanding of the building, environment conditions\nhttps:\/\/www.kaggle.com\/kamaljp\/building-energy-usage-edanmodeling?kernelSessionId=85754348\n\nModeling was tried with multiple Machine learning and introductory Neural Nets\nhttps:\/\/www.kaggle.com\/kamaljp\/modeling-site-eui-wids\/notebook?kernelSessionId=85945920\n\nThe results were still not better than first Lasso Regresor model score along with the EDA. The data is holding its secrets so strong. This notebook explores the Feature Engineering using dimensionality reduction methods, and use that reduced dataset to model using lasso regressor. Here goes nothing...\n\n### What to Expect\n\nTruncated SVD is used to reduce the dimensions from 127 to 60. You can see the impact it has had [here](#tsnevis). Following in the same path, tSNE further reduces the entire components to just 2 parameters. \nThe visuals are as usual insightful,but this time they are having depth to it. \n\nThe model training, and results generation has been converted into functions. The entire model, instantiate, cross_validate, train and test has been wrapped into a [function](#peek2). This is done after using the process multiple times in earlier notebooks. Similarly, function for svd calculation, svd variance explanation are also made into functions. \n\n\n### Sneek Peek\n\ntSNE is about inter-dimensional data transformation. The tSNE visual of the [training and testing](#peek1) data shows the power of this algorithm. Here were [errors](#error) that caused challenges in executing functions. The final [results](#finres) is sufficiently strange, but the values seems to make sense.\n\nPS: Use the blue links to go the exact location of the code and related activity"}}