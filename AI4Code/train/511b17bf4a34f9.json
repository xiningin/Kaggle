{"cell_type":{"d8a74bee":"code","5e965bae":"code","81a13581":"code","ba43e4b3":"code","3baecc45":"code","7a29edd4":"code","bb40a7f6":"code","32014f3b":"code","e8d7249b":"code","a73c52cf":"code","eedc44b4":"code","a732af9d":"code","dbc3a508":"markdown","ec147a2d":"markdown","5a28a24b":"markdown","eaef9ffb":"markdown","f173548a":"markdown","bdd05736":"markdown"},"source":{"d8a74bee":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re, spacy,markovify\nfrom collections import Counter\nimport seaborn as sns","5e965bae":"sw = set(STOPWORDS)\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef show_word_cloud(title,content):\n    wordcloud = WordCloud(stopwords=sw,background_color = 'white',width=1500,height=1000).generate(content)\n    \n    plt.figure(figsize=(15,10))\n    plt.imshow(wordcloud,interpolation='bilinear')\n    plt.axis('off')\n    print(title)\n    plt.show()\n\ndef find_adjectives(content):\n    document = nlp(content)\n    adjectives=[]\n    for token in document:\n        if(token.pos_==\"ADJ\" and token.lemma_ != \"-\"):\n              adjectives.append(token.lemma_)\n    return adjectives","81a13581":"#reading content of all the files\nbase_dir_of_subtitles=\"..\/input\/sub-titles-of-presentations\/\"\nfile_names = [\"Google-2018\",\"Google-2019\",\"Apple-2018\",\"Apple-2019\"]\n\nfile_content_array=[]\nfor file_name in file_names:\n    file_content=open (base_dir_of_subtitles+file_name+\".txt\").read()\n\n    #remove speaker names from content\n    words_to_remove = re.findall(r'(\\w+:)',file_content)\n\n    for word in words_to_remove:\n        file_content=file_content.replace(word,\"\")\n        \n    file_content_array.append(file_content)","ba43e4b3":"#word cloud of google speech\nshow_word_cloud(file_names[0],file_content_array[0])\nshow_word_cloud(file_names[1],file_content_array[1])","3baecc45":"#word cloud of apple speech\nshow_word_cloud(file_names[2],file_content_array[2])\nshow_word_cloud(file_names[3],file_content_array[3])","7a29edd4":"#Finding all adjectives in the speech using spaCy\nadjectives_array=[]\nfor file_name in file_names:\n    file_content=\"\"\n    file_content=open (base_dir_of_subtitles+file_name+\".txt\").read()\n    adjectives_array.append(find_adjectives(file_content))","bb40a7f6":"#word cloud of adjectives in google speech\nadjective_google_2018=' '.join([str(elem) for elem in adjectives_array[0]])\nshow_word_cloud(file_names[0],adjective_google_2018)\n\nadjective_google_2019=' '.join([str(elem) for elem in adjectives_array[1]])\nshow_word_cloud(file_names[1],adjective_google_2019)","32014f3b":"#word cloud of adjectives in apple speech\nadjective_apple_2018=' '.join([str(elem) for elem in adjectives_array[2]])\nshow_word_cloud(file_names[2],adjective_apple_2018)\n\nadjective_apple_2019=' '.join([str(elem) for elem in adjectives_array[3]])\nshow_word_cloud(file_names[3],adjective_apple_2019)","e8d7249b":"#Plotting most used adjectives in each of the speeches\ntop_adjective_count=20\n\nfreq=Counter(adjectives_array[0])\ndf=pd.DataFrame(freq.most_common(top_adjective_count),columns=['adjective','count'])\ndf=df.apply(lambda x: x.astype(str).str.lower())\ndf['count'] = pd.to_numeric(df['count'])\ndf['company']='Google-2018'\n\nfreq1=Counter(adjectives_array[1])\ndf1=pd.DataFrame(freq1.most_common(top_adjective_count),columns=['adjective','count'])\ndf1['count'] = pd.to_numeric(df1['count'])\ndf1['company']='Google-2019'\n\nfreq2=Counter(adjectives_array[2])\ndf2=pd.DataFrame(freq2.most_common(top_adjective_count),columns=['adjective','count'])\ndf2['count'] = pd.to_numeric(df2['count'])\ndf2['company']='Apple-2018'\n\nfreq3=Counter(adjectives_array[3])\ndf3=pd.DataFrame(freq3.most_common(top_adjective_count),columns=['adjective','count'])\ndf3['count'] = pd.to_numeric(df3['count'])\ndf3['company']='Apple-2019'\n\nfinal_df=pd.concat([df,df1,df2,df3])\n\nplot_data = final_df.pivot(\"adjective\", \"company\", \"count\")\n\nchart_title=\"Top \"+str(top_adjective_count)+\" Adjectives in each of the Speeches\"\nf, ax = plt.subplots(figsize=(15, 12))\nax.set_title(chart_title)\nsns.heatmap(plot_data, annot=True, fmt=\".0f\", linewidths=.2,linecolor='grey', ax=ax,cmap=\"YlGnBu\")","a73c52cf":"#Plotting most used words in each of the speeches\nfile_content_array[0]=re.sub('\\n',' ',file_content_array[0])\nfile_content_array[1]=re.sub('\\n',' ',file_content_array[1])\nfile_content_array[2]=re.sub('\\n',' ',file_content_array[2])\nfile_content_array[3]=re.sub('\\n',' ',file_content_array[3])\n\nfile_content_array[0]=file_content_array[0].lower()\nfile_content_array[1]=file_content_array[1].lower()\nfile_content_array[2]=file_content_array[2].lower()\nfile_content_array[3]=file_content_array[3].lower()\n\nadded_stopwords=['.',',','-','3','!','4','_','\"','--','?',\"we're\",\"we've\",\"you're\",\"yeah\",\"tim\"]\nfor w in added_stopwords:\n    nlp.vocab[w].is_stop = True\n    lex = nlp.vocab[w]\n    lex.is_stop = True\n\ndoc = nlp(file_content_array[0])\ntokens = [token.text for token in doc if not token.is_stop and token.text.strip()!='']\n\ntop_words_count=20\nfreq=Counter(tokens)\ndf=pd.DataFrame(freq.most_common(top_words_count),columns=['words','count'])\ndf=df.apply(lambda x: x.astype(str).str.lower())\ndf['count'] = pd.to_numeric(df['count'])\ndf['company']='Google-2018'\n\ndoc = nlp(file_content_array[1])\ntokens = [token.text.strip() for token in doc if not token.is_stop and token.text.strip()!='']\n\nfreq=Counter(tokens)\ndf1=pd.DataFrame(freq.most_common(top_words_count),columns=['words','count'])\ndf1=df1.apply(lambda x: x.astype(str).str.lower())\ndf1['count'] = pd.to_numeric(df1['count'])\ndf1['company']='Google-2019'\n\ndoc = nlp(file_content_array[2])\ntokens = [token.text for token in doc if not token.is_stop and token.text.strip()!='']\n\nfreq=Counter(tokens)\ndf2=pd.DataFrame(freq.most_common(top_words_count),columns=['words','count'])\ndf2=df2.apply(lambda x: x.astype(str).str.lower())\ndf2['count'] = pd.to_numeric(df2['count'])\ndf2['company']='Apple-2018'\n\ndoc = nlp(file_content_array[3])\ntokens = [token.text for token in doc if not token.is_stop and token.text.strip()!='']\n\nfreq=Counter(tokens)\ndf3=pd.DataFrame(freq.most_common(top_words_count),columns=['words','count'])\ndf3=df3.apply(lambda x: x.astype(str).str.lower())\ndf3['count'] = pd.to_numeric(df3['count'])\ndf3['company']='Apple-2019'\n\nfinal_df=pd.concat([df,df1,df2,df3])\n\nplot_data=pd.pivot_table(final_df,values='count',index='words',columns='company')\n\nchart_title=\"Top \"+str(top_words_count)+\" word in each of the Speeches\"\nf, ax = plt.subplots(figsize=(15, 12))\nax.set_title(chart_title)\nsns.heatmap(plot_data, annot=True, fmt=\".0f\", linewidths=.2,linecolor='grey', ax=ax,cmap=\"YlGnBu\")","eedc44b4":"#most frequent bi-grams - pair of words appearing most number of time in the speeches\ndoc = nlp(file_content_array[0])\ntokens = [token.text for token in doc if not token.is_stop and token.text.strip()!='']\n\ntop_bigram_count=10\nbigrams = zip(tokens, tokens[1:])\nfreq=Counter(bigrams)\ndf=pd.DataFrame(freq.most_common(top_words_count),columns=['words','count'])\ndf=df.apply(lambda x: x.astype(str).str.lower())\ndf['count'] = pd.to_numeric(df['count'])\ndf['company']='Google-2018'\n\ndoc = nlp(file_content_array[1])\ntokens = [token.text.strip() for token in doc if not token.is_stop and token.text.strip()!='']\n\nbigrams = zip(tokens, tokens[1:])\nfreq=Counter(bigrams)\ndf1=pd.DataFrame(freq.most_common(top_words_count),columns=['words','count'])\ndf1=df1.apply(lambda x: x.astype(str).str.lower())\ndf1['count'] = pd.to_numeric(df1['count'])\ndf1['company']='Google-2019'\n\ndoc = nlp(file_content_array[2])\ntokens = [token.text for token in doc if not token.is_stop and token.text.strip()!='']\n\nbigrams = zip(tokens, tokens[1:])\nfreq=Counter(bigrams)\ndf2=pd.DataFrame(freq.most_common(top_words_count),columns=['words','count'])\ndf2=df2.apply(lambda x: x.astype(str).str.lower())\ndf2['count'] = pd.to_numeric(df2['count'])\ndf2['company']='Apple-2018'\n\ndoc = nlp(file_content_array[3])\ntokens = [token.text for token in doc if not token.is_stop and token.text.strip()!='']\n\nbigrams = zip(tokens, tokens[1:])\nfreq=Counter(bigrams)\ndf3=pd.DataFrame(freq.most_common(top_words_count),columns=['words','count'])\ndf3=df3.apply(lambda x: x.astype(str).str.lower())\ndf3['count'] = pd.to_numeric(df3['count'])\ndf3['company']='Apple-2019'\n\nfinal_df=pd.concat([df,df1,df2,df3])\n\nplot_data=pd.pivot_table(final_df,values='count',index='words',columns='company')\n\nchart_title=\"Top \"+str(top_words_count)+\" bi-grams in each of the Speeches\"\nf, ax = plt.subplots(figsize=(10, 20))\nax.set_title(chart_title)\nsns.heatmap(plot_data, annot=True, fmt=\".0f\", linewidths=.2,linecolor='grey', ax=ax,cmap=\"YlGnBu\")","a732af9d":"#Now, with all the 4 speeches, lets try to generate random sentences using Markovify\n#https:\/\/github.com\/jsvine\/markovify\nwith open(base_dir_of_subtitles+\"Google-2018.txt\") as f:\n    text = f.read()\n\nwith open(base_dir_of_subtitles+\"Apple-2018.txt\") as f:\n    text2 = f.read()\n\nwith open(base_dir_of_subtitles+\"Google-2019.txt\") as f:\n    text3 = f.read()\n\nwith open(base_dir_of_subtitles+\"Apple-2019.txt\") as f:\n    text4 = f.read()\n    \ntext_model = markovify.Text(text)\ntext_model2 = markovify.Text(text2)\ntext_model3 = markovify.Text(text3)\ntext_model4 = markovify.Text(text4)\n\nmodel_combo = markovify.combine([ text_model, text_model2,text_model3,text_model4 ], [1,1,1,1])\n\n# Print five randomly-generated sentences\nfor i in range(5):\n    print(str(i+1)+\" : \"+model_combo.make_sentence())","dbc3a508":"* **I am always amazed how sometimes these random generated sentences make total sense. **\n* **To test it yourself, download this piece of code and try on your local machine, I am sure you will be surprised with results**\n\n***EOF***","ec147a2d":"**Observation**\n* Looking at all of the 4 word clouds, one can easyily guess the number of products announced by Apple and Google in their respective speeches like Pixel, iPhone, iPad, Pixel Slate, Google Home and Apple Watch etc..","5a28a24b":"**Observations**\n* 11 and Pro appeared in Apple-2019 speech because, well iPhone 11 and iPhone 11 Pro were launched in 2019 only;)\n* The word \"bionic\" refers to Apple's processors thats why it does not appear in Google' speeches\n* The word \"watch\" is only Apple' speeches because they release Apple Watch each year and Google does not have any Watch in their product line-up.\n* Similarly, words \"home\",\"hub\",\"slate\" and \"nest\" are only in Google's speeches because these all are Google products","eaef9ffb":"**Observations**\n* With bi-grams it is very easy to find out the products launched by each company and also the most talked about features\n* Low light photography is named \"Night Mode\" in Apple and Google calls it \"Night Sight\"\n* Apple mentioned \"Portrait Mode\" more than Google\n* HDR is called as \"Smart HDR\" by Apple and same is named as \"HDR Plus\"  by Google and both appeared exactly 9 times\n* Apple mentioned \"A13 Bionic\" less number of times than \"A12 Bionic\"\n* \"Motion Sense\" appeared in Google's 2019 speech because they launched Pixel 4 with that technology\n* Apple mentioned \"Machine Learning\" 9 times in 2019 where as, Google mentioned it only 5 and 7 times in 2018 and 2019 respectively","f173548a":"**Observations**\n* Its evident that Apple uses the word \"new\" more frequently in its presentation\n* Occurence of word \"ultra\" and \"wide\" is more in Apple's 2019 speech due to new wide and ultra wide lenses introduced in iPhone 11 Pro\n* Google is not using the words \"incredible\" and \"advanced\" as frequent as Apple does\n* Surprisingly the word \"smart\" is not much used in speeches (with exception of Google's 2018 speech) ","bdd05736":"## Analyzing speeches of Apple's iPhone and Google's Pixel launch event from 2018 and 2019\n\nFollowing are the video links -\n* Made by Google 2018 - https:\/\/www.youtube.com\/watch?v=EsoQGTA1SxY\n* Made by Google 2019 - https:\/\/www.youtube.com\/watch?v=XKmsYB54zBk\n* September Event 2018 \u2014 Apple - https:\/\/www.youtube.com\/watch?v=wFTmQ27S7OQ\n* September Event 2019 \u2014 Apple - https:\/\/www.youtube.com\/watch?v=-rAeqN-Q7x4\n\nI will use Markov chain generator in the end to generate some random sentences out of the 4 speeches.<br\/>Markov chain generator - https:\/\/github.com\/jsvine\/markovify"}}