{"cell_type":{"4fcf4251":"code","42e3edb4":"code","808cc36c":"code","17230f38":"code","df6089f7":"code","9da9ea13":"code","f603c3cd":"code","a1639e81":"code","4dc12177":"code","fd84d83e":"code","948a4ef2":"code","b078eea0":"code","c7821a35":"code","ca30e703":"code","44c3566a":"code","8110d27c":"code","33001868":"code","fc9cabf4":"code","54ddfb9e":"code","fa9dd930":"code","74682d64":"code","cd1c3a92":"code","5ee62f16":"code","13f1ed59":"code","097a583a":"code","d85cff7c":"code","2c36a8c6":"code","3dbdac91":"code","9c0b60e8":"code","d0299e25":"code","56f977a8":"code","28f21ff6":"code","564458c3":"code","ed77ec74":"code","e03528bb":"code","8bdde8be":"code","b4b56c0c":"code","eb55397e":"code","bf1bdd58":"code","5fbb591c":"code","0ce09fe7":"code","58b86d6e":"code","9293f371":"code","bdc27d2c":"code","0c0e1e5e":"code","b9d0fc56":"code","b96a0bf9":"code","b2a03d15":"code","955c1bfe":"code","d67b3657":"code","af820046":"code","60afeada":"code","2dbd02e0":"code","b048bf36":"code","617eb917":"code","19f13155":"code","0170c282":"code","57fc4d9a":"code","9ae4d28f":"code","ac41c173":"code","2b298503":"code","2bbd5464":"code","e4f643ff":"markdown","b8dab63a":"markdown","abf71cea":"markdown","cd088544":"markdown","8774b00e":"markdown","f4f9e3ee":"markdown","c6955585":"markdown","644e7806":"markdown","c5f5269b":"markdown","60f6303c":"markdown","128d18bc":"markdown","ab5dce75":"markdown","ead8927d":"markdown","1acf5948":"markdown","f462a31b":"markdown","3af4f005":"markdown","0e52aa5d":"markdown","5ebb7cd7":"markdown","e7ba11b5":"markdown","f1cd674b":"markdown","69e531f5":"markdown","22d85a00":"markdown","e69e855f":"markdown","cf68a316":"markdown","3a37452a":"markdown","969a7970":"markdown","48d3b712":"markdown","35dd372a":"markdown","5fad04de":"markdown","9e6b7bc6":"markdown","2a3eba4d":"markdown","ea408dc3":"markdown","3477e79c":"markdown","85645c23":"markdown","ec37b92d":"markdown","61ecbb3e":"markdown","15004d33":"markdown","050cb2ec":"markdown"},"source":{"4fcf4251":"\nfrom datetime import datetime\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nimport matplotlib.style as style\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\nimport category_encoders as ce\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport itertools\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns","42e3edb4":"import os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/house-prices-advanced-regression-techniques\"))","808cc36c":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint (\"Data is loaded!\")","17230f38":"pd.set_option('display.max_columns',1000)\ntrain.head()","df6089f7":"print(train.shape)\nprint(test.shape)","9da9ea13":"test.head()","f603c3cd":"train.get_dtype_counts()","a1639e81":"train.describe()","4dc12177":"train.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","fd84d83e":"# Removing all the rows with the target variable is missing.\ntrain.dropna(axis=0, subset=['SalePrice'], inplace=True)\n#this will remove the outliers in the column GrLivArea which we will discuss later.\n#train = train[train.GrLivArea < 4500]\n#train.reset_index(drop=True, inplace=True)\n\n#store target in y and all other indepenent variables in X\ny = train.SalePrice\n#train.drop(['SalePrice'], axis=1, inplace=True)","948a4ef2":"#Find correlation with independent variables and target variable\n(train.corr()**2)['SalePrice'].sort_values(ascending = False)[1:]\n#This will display correlation with target variable and independent variables from most correlated to least.","b078eea0":"train1=train.copy()\ntrain.drop(['SalePrice'], axis=1, inplace=True)","c7821a35":"def customized_scatterplot(y, x):\n        ## Sizing the plot. \n    style.use('fivethirtyeight')\n    plt.subplots(figsize = (15,10))\n    ## Plotting target variable with predictor variable(OverallQual)\n    sns.scatterplot(y = y, x = x);","ca30e703":"customized_scatterplot(y, train.GrLivArea)","44c3566a":"#this will remove the outliers in the column GrLivArea.\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)","8110d27c":"train[train.GrLivArea > 4500]","33001868":"customized_scatterplot(y, train.LotFrontage  )","fc9cabf4":"train[train.LotFrontage>300]","54ddfb9e":"customized_scatterplot(y, train.OpenPorchSF )","fa9dd930":"train[train.OpenPorchSF>500]","74682d64":"customized_scatterplot(y, train.LowQualFinSF);","cd1c3a92":"train[train.LowQualFinSF>500]","5ee62f16":"customized_scatterplot(y, train.WoodDeckSF)","13f1ed59":"train[train.WoodDeckSF>800]","097a583a":"customized_scatterplot(y, train.BsmtFinSF2)","d85cff7c":"train[train.BsmtFinSF2>1400]","2c36a8c6":"## Plot sizing. \nfig, (ax1, ax2) = plt.subplots(figsize = (20,10), ncols=2,sharey=False)\n## Scatter plotting for SalePrice and GrLivArea.\nsns.scatterplot(x = train.GrLivArea,y = y, ax=ax1)\n## regression line for GrLivArea and SalePrice. \nsns.regplot(x=train.GrLivArea, y=y, ax=ax1);\n\n### Scatter plotting for SalePrice and MasVnrArea. \nsns.scatterplot(x = train.MasVnrArea,y = y, ax=ax2)\n## regression line for MasVnrArea and SalePrice. \nsns.regplot(x=train.MasVnrArea, y=y, ax=ax2);\n","3dbdac91":"fig, (ax1, ax2) = plt.subplots(figsize = (20,10), ncols=2,sharey=False)\n#plt.subplots(figsize = (15,10))\nsns.residplot(train.GrLivArea, y, ax=ax1);\nsns.residplot(train.MasVnrArea, y,ax=ax2);","9c0b60e8":"#y = train['SalePrice']\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y, kde=False, fit=stats.johnsonsu)\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=stats.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=stats.lognorm)","d0299e25":"print(\"Skewness: \" + str(y.skew()))\nprint(\"Kurtosis: \" + str(y.kurt()))","56f977a8":"y = np.log1p(y)\ny=y.reset_index(drop=True)","28f21ff6":"#y = train['SalePrice']\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y, kde=False, fit=stats.johnsonsu)\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=stats.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=stats.lognorm)","564458c3":"fig, (ax1, ax2) = plt.subplots(figsize = (20,10), ncols=2,sharey=False)\n#plt.subplots(figsize = (15,10))\nsns.residplot(train.GrLivArea, y, ax=ax1);\nsns.residplot(train.MasVnrArea, y,ax=ax2);","ed77ec74":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","e03528bb":"#saleprice correlation matrix\ncorrmat = train1.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k,'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train1[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","8bdde8be":"def missing_percentage(df):\n    \n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n\nmissing_percentage(train)","b4b56c0c":"sns.set_style(\"whitegrid\")\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()","eb55397e":"#concatenate train and test data \ntrain_test=pd.concat((train,test)).reset_index(drop=True)\n","bf1bdd58":"#Separate Numerical and categorical columns.\ntrain_test_num = train_test.select_dtypes(exclude=['object'])\ntrain_test_obj= train_test.select_dtypes(exclude=['int','float'])\n\nmy_imputer=SimpleImputer()\nmy_obj_imputer=SimpleImputer(strategy=\"most_frequent\")\n#Impute numerical data\nimputed_train_test_num=pd.DataFrame(my_imputer.fit_transform(train_test_num))\n#Impute categorical data\nimputed_train_test_obj = pd.DataFrame(my_obj_imputer.fit_transform(train_test_obj))\n#imputation remove indexes so put column indexes back\nimputed_train_test_num.columns = train_test_num.columns\nimputed_train_test_obj.columns = train_test_obj.columns\n#Concatenate imputed categorical and numerical columns\nimputed_train_test=pd.concat([imputed_train_test_obj,imputed_train_test_num],axis=1)","5fbb591c":"pd.set_option('display.max_columns',1000)\nimputed_train_test","0ce09fe7":"label_encoder = LabelEncoder()\nfor col in set(train_test_obj):\n    imputed_train_test[col]=label_encoder.fit_transform (imputed_train_test[col])\n","58b86d6e":"pd.set_option('display.max_columns',1000)\nimputed_train_test","9293f371":"for col in set(train_test_obj): \n    count_enc = ce.CountEncoder(cols=col)\n    imputed_train_test[col +'_count']=count_enc.fit_transform(imputed_train_test[col])\n    #imputed_train_test[col]=count_enc.fit_transform(imputed_train_test[col])\n    ","bdc27d2c":"pd.set_option('display.max_columns',1000)\nimputed_train_test","0c0e1e5e":"imputed_train_test.shape","b9d0fc56":"X_train = imputed_train_test.iloc[:len(y), :]\nX_test = imputed_train_test.iloc[len(y):, :]","b96a0bf9":"for col in set(train_test_obj):\n    target_enc = ce.TargetEncoder(cols=col)\n    target_enc.fit(X_train[col],y)\n    X_train[col+'_target' ]=target_enc.transform (X_train[col])\n    X_test[col+'_target' ]=target_enc.transform(X_test[col])","b2a03d15":"X_train","955c1bfe":"outliers = [30, 88, 462, 631,1322,691,934,297,322,185,53,495]\nX_train = X_train.drop(X_train.index[outliers])\ny = y.drop(y.index[outliers])\n\noverfit = []\nfor i in X_train.columns:\n    counts = X_train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X_train) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\nX_train = X_train.drop(overfit, axis=1)","d67b3657":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y, train_size=0.8, test_size=0.2,\nrandom_state=0)","af820046":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","60afeada":"alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","2dbd02e0":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","b048bf36":"gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)","617eb917":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )","19f13155":"xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","0170c282":"stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","57fc4d9a":"print('START Fit')\n\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X_train), np.array(y_train))\n\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(X_train, y_train)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X_train, y_train)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(X_train, y_train)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X_train, y_train)\n\nprint('GradientBoosting')\ngbr_model_full_data = gbr.fit(X_train, y_train)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(X_train, y_train)\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X_train, y_train)","9ae4d28f":"def blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.05 * lasso_model_full_data.predict(X)) + \\\n            (0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))\n","ac41c173":"print('RMSLE score on train data:')\nprint(rmsle(y_valid, blend_models_predict(X_valid)))","2b298503":"print('Predict submission')\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_test)))  ","2bbd5464":"#my_model.fit(X_train, y_train)\n#preds = my_model.predict(X_test)\n\n#output = pd.DataFrame({'Id': X_test.Id,\n                       #'SalePrice': preds})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.to_csv","e4f643ff":"Now we will plot the missing data using seaborn bar plot for easy understanding. total 19 columns having missing values with 5 columns having over 50% of mising data. By using several methds we can fill the missing value locations. For example if the values of a particular column is numerical we fill the missing values with mean,median or mode, and with categorical columns fill with most repeated data or use other techniques.","b8dab63a":"#### Kurtosis ####\nkurtosis findsout outliers in a dataset. it finds out how long is the tail of the curve is. ","abf71cea":"# House prices using advanced regression techniques #\n\nThis is a practical machine learning tutorial based on a kaggle competition named house prices,with 79 explanatory variables describing almost every aspects of residential homes in ames iowa, this competition challenges you to predict the Sale price of each home. \n\n## Goal ##\nIt is our job to predict the saleprice of each house in the test set, each row in the test set describes a house and we must predict the sale price of each house.\n\n### Metric used for evaluation ###\n\nSubmisssions are evaluated on root-mean squared error(RMSE), between the logarithm of the predicted value and the logarithm of the observed sale price (Taking log, means that errors in predicting expensive houses and cheap houses will affect the result equal.)\n\nTo train the model we have the details of 1460 houses in the train data set. After training , the model will get an idea of what features and qualities of a house decides the sale price. The model will then go through the features of each house in the test set and predict the sale price. \nWe calculate the difference(error) of predicted price from actual price by using a metric called root mean squared error(rmse).","cd088544":"From theese plots  it is apparent that our target vriable not normally distributed. Johnsonsu is the close match, but even lognormal is also good. Linear regression need target variable to be multivariate normally distributed against independent variables.","8774b00e":"\n ### Let us check which independent variables are most correlated with target variable.","f4f9e3ee":"here we use simple imputer to fill the missing values. simple imputer will decide whether to use mean, median, mode or some other techniques to fill missing data. when we use simple imputer for categorical columns use a strategy like 'most_frequent', for numerical, no strategy is required, imputing is done automatically.  ","c6955585":"#### Missing Values\nThis function will help us to count the total no of missing values and its percentage, Machine learning models cannot handle mssing values in feature columns. Following code will give us percentage of missing values in each columns.","644e7806":"Zero skewness means bell shaped curve not skewed to right or left side. Here the value shows clear positive skewness. Default value of kurtosis is 3, here the value shows so many outliers in the target variable.","c5f5269b":"To check each regression assumptions, let us draw a regression line and check how the error of the dependent variable increase when the independent variable increases (Heteroscedatisity).","60f6303c":"Let us examine the data files, train data set has 81 columns including dependent variable the SalePrice, and 1460 rows. 1 row describes details of one house. we later separate and store dependent vriable in y and all 79 independent variables in X one column is id and we will drop that later. ","128d18bc":"### Assumptons of Liear regression\n\nLinear regression analyses if one or more independent variable explains the dependent variable.when multiple independent variable involved we use multi linear regresion.\nWhen we have one target variable and multiple independent variables in the dataset then we have to follow multi linear regression.Target variable must follow a linear relationship with each independent variables for multi linear regression.\n5 assumptons of linear regression are\n### 1.   Lenear relationship \n\n   Relationship between dependent and independent variables should be linear, and error should be almost same always.\n   \n   \n### 2.  Homoscedasticity vs Heteroscedasticity\nwhen the value of the independent variable increases, the error or residual of the target must not increase(It should remain same).It is called  **Homoscedaticity** .if        the residual increases with increase of independent variable is clled **heteroscedasticity**. Target variable should keep homoscedasticity with ecah independent variable,        else it would be a problem for multi linear regression.\n   \n      \n### 3.   Autocorrelation\n\n\n### 4.   Multivariate normality\n\n   Target variable must be normally distributed, even multiple independent variables are involved. Transformation of target variable using techniques like log                transformation transform distribution of target variable close to normal distribution. we will visualise this in a moment\n   \n   \n### 5.   Multicolinearity  \n   Correlation between independent variables are called **Multicollinearity**. multicolinear variables give same information to the model so we      might remove one of them, or use regression models like Lasso or Ridge, theese are good at dealing with multicolinearity \n\n","ab5dce75":"let us apply log transformation on our target variable","ead8927d":"Now we are importing necessary packages for this competition.","1acf5948":"### Categorical encoding\nMachine learning models cannot handle categorical values, so that we have to use any one of the encoding technique to convert the categorical columns to numerical columns.Some of the encoding techniques are label encoding, target encoding, count encoding and one hot encoding. Here we use label encoder, count encoder and target ecoder together, and our model produced more accuracy than when onehot encoding used.label encoding will give an unique value to each categorical variables","f462a31b":"Below the residualplot will clearly show us the phenmenon heteroscedasticity and linearity.","3af4f005":"After transformation of the target variable, the error variance is reduced significantly this can be seen from the above plots.","0e52aa5d":"### Count encoding ###\nEach categorical value will be replaced with that value's repeating counts. for example in this dataset feature column Neighborhood, 'OldTown' repeats 113 times, whenever OldTown appears count encoding will replace OldTown with 113.\n### Target encoding ###\nTarget encoding replaces each categorical variable with it's target variable's average value. For example in our train dataset, Neighborhood column's OldTown value will be replaced with average of it's target variable's value. Before doing target encoding we have to split our concatenated data in to train and test set. Target variable is mandatory for target encoding which makes the train test split necessary.","5ebb7cd7":"The scatter plot below shows all datapoints around a straight line(This line is caled regression line or bestfit line).The regression line represents the predicted value and the dots represents the actual values.the difference between the actual value and the predicted value is the error, or the distance between each data point and the line is called the error.When the error decreases the predicted value will come close to the actual value.when the error becomes zero then the predicted value will be same as the actual value and the model is considered as 100% accurate, but that is not possible in the real world, because a straight line can never pass through every data points especially when there are thousands of points. But we can find out a best fit line with least sum of squared error. In the coming sections we will explore our data, try to reduce the error and make our model more accurate.","e7ba11b5":"Here from the scatterplot (LotFrontage against SalePrice)we remove row no 934. In all other plots below,if there are more than one outlier, we decided not to delete some rows(even if it appear as an outlier) because it increses the error value, it means while deleting those rows may purging valid information from our dataset.","f1cd674b":"### Let us map 10 most correlated variables here. ","69e531f5":"We use pandas to read train and test csv files and stores the contents in train and test variables.  ","22d85a00":"From the above scatter plots it is aparent that SalePrice is not maintaining a good linear relationship with Independent variables. ","e69e855f":"![image.png](attachment:image.png)","cf68a316":"#### Skewness ####\nWhen the target variable follow a bell shaped curve it is considered normally distributed here the curve is skewed to the right side(curve has tail to the right side) means positive skewness. positive skewness of the data can be normalise to a certain extent with log transformation.","3a37452a":"#### Linear regression","969a7970":"We have to split imputed_train_test dataset into X_train and X_test, because TargetEncoder needs target variable for encoding , and without splitting the dataset X will be 2917 rows and y will be 1460 rows that will through rows mismatch error. ","48d3b712":"[](http:\/\/)os is the module used to list the directory, we can list the files for this competition from the ..\/input directory,which is located in the kaggle cloud       \nthe sample_submission.csv tells us in which format the predicted saleprice to be submitted. ","35dd372a":"### Multicolinearity or Correlation between  independent variables ###\nNow it is time to check for multicollenearity it is the correlation between independent variables. Heatmap is best plot to undersrstand multicolinearity. ","5fad04de":"Here we can see that price of some houses are too less compared to their Ground living area, we can consider these are outliers.Let us plot each numerical independent variable against target variable and findout some outliers which we will remove from our dataset later. here we will plot few independent variables against 'SalePrice' and findout the rows(Outliers) which we want to remove. we are soon going to findout that whether removing thees rows(Outliers) decreases error(RMSE or MAE) of our model, decreasing error makes our model more accurate.","9e6b7bc6":"from the above plots it is clear that both masvener area and grlivarea against SalePrice are not maintaining a good linear relationship.","2a3eba4d":"Now let us check Multivariate normality. this checks whether our target variable follow a normal distibution. ","ea408dc3":"## Exploratory data analysis ##\n","3477e79c":"Let us plot numerical independent variables against target variable.","85645c23":"#### Split back concatenated train test data into X_train and X_test","ec37b92d":"from this heat map it is clear that Garagearea and garage cars are highly corelated.so corellated variables give same nformation to the model so we may remove one of them to eliminate redunduncy, but here we use Lasso or ridge regression both are good at dealing wih multicolinearity.","61ecbb3e":"### Let us check Linear relation ship between target variable and numerical independent variables","15004d33":"* Here the datapoints across the straight line is not distributed uniformly around the line. the dots are more widely scattered away from the line or the dots form a cone shape,  The residual or error of the dependent variable increases with respect to the increase in independent variable. ","050cb2ec":"I used Kaggle course and theese kernels to prepare this solution, thees kernels helped me understand a lot about the basic concepts and many thanks for publishing theese kernels.       \nhttps:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python    \nhttps:\/\/www.kaggle.com\/jesucristo\/1-house-prices-solution-top-1    \nhttps:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing   \n"}}