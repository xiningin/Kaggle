{"cell_type":{"cc847cc3":"code","0e620055":"code","7be15280":"code","29d51a52":"code","75905532":"code","39c0ec75":"code","49ecda53":"code","195faf78":"code","b5fe18ba":"code","fad9f7b9":"code","3e479c33":"code","12d44ecc":"code","064ada85":"code","25d7f212":"code","4abcab7d":"code","90ea10e1":"code","ffca19a0":"code","ab984eaf":"code","bb87b555":"code","12dcbefa":"code","8855a0aa":"code","b5a71d9e":"code","5c1ab6c9":"code","b2c42098":"code","d9301fc0":"code","3e5efede":"code","85687205":"code","fa7c3397":"code","7daed83d":"markdown","7218fa4b":"markdown","3da61bd6":"markdown","068b9c24":"markdown","3a32afdd":"markdown","0e500a98":"markdown","44436ad7":"markdown","0c74130f":"markdown","875b6aa0":"markdown","d20f2bca":"markdown","743ad136":"markdown","67a5591d":"markdown","075a9b24":"markdown","d3d28882":"markdown","dc3f1f82":"markdown","3937e9fb":"markdown","0b38d051":"markdown","f6663313":"markdown","e0638089":"markdown","1033797a":"markdown","ffa198b5":"markdown","d5476cae":"markdown","08306948":"markdown","6d1ba4d3":"markdown","7e754000":"markdown","42461de6":"markdown","0a4281a7":"markdown","7be5dbaa":"markdown"},"source":{"cc847cc3":"!pip install seaborn --upgrade","0e620055":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport os\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set()\n%matplotlib inline\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7be15280":"data = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv').drop(['id'], axis=1)\ndata.head()","29d51a52":"data.info()","75905532":"# Create new columns for visualization\ndata['Stroke?'] = data['stroke']==1\ndata['Hypertension?'] = data['hypertension']==1\ndata['Heart Disease'] = data['heart_disease']==1\n\n# Declare size of figures\nmy_size = {'width':800, 'height':500}","39c0ec75":"data['gender'].value_counts()","49ecda53":"fig = px.histogram(data, x='age',\n                   nbins=20, \n                   title='Age distribution', \n                   color_discrete_sequence=px.colors.qualitative.Antique,\n                   marginal='box', \n                   color='Stroke?',\n                   **my_size,)\n\nfig.update_layout(bargap=0.1)","195faf78":"temp = pd.pivot_table(\n            data,\n            values = ['stroke'],\n            index = ['hypertension'],\n            columns = ['heart_disease'],\n            aggfunc = {'stroke':['count','mean']}\n        )\n\ntemp.columns = temp.columns.set_levels(['No', 'Yes'], level=2)\ntemp.index = pd.Index(['No','Yes'], name='Hypertension')\n\ntemp.style.set_properties(**{'background-color': 'khaki','border-color': 'white'},subset=[('stroke','mean','No'),('stroke','mean','Yes')])","b5fe18ba":"px.imshow(\n    temp.loc[:,('stroke','mean')],\n    labels = dict(color='Stroke'),\n    title = 'Stroke probabilities',\n    color_continuous_scale = px.colors.sequential.Redor,\n    **my_size\n)","fad9f7b9":"def get_quick_report(feature):\n    temp = pd.pivot_table(\n                    data,\n                    values = 'stroke',\n                    index = feature,\n                    aggfunc = ['sum','count','mean']\n                )\n    temp.columns = pd.MultiIndex.from_arrays([['Stroke','Stroke','Stroke'],['sum','count','mean']])\n    \n    return temp\n\n    \ntemp_married = get_quick_report('ever_married')\ntemp_work = get_quick_report('work_type')\ntemp_residence = get_quick_report('Residence_type')","3e479c33":"def one_to_many(index):\n    out = []\n    for i in index.values:\n        out.append((index.name, i))\n    return out","12d44ecc":"temp = pd.concat([temp_married, temp_work, temp_residence], axis=0)\n\narr = one_to_many(temp_married.index) + one_to_many(temp_work.index) + one_to_many(temp_residence.index)\n\ntemp.index = pd.MultiIndex.from_tuples(arr)\ntemp.style.background_gradient(sns.light_palette('darkorange',as_cmap=True), subset=[('Stroke','mean')])","064ada85":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(\n    shared_yaxes =True,\n    rows=1, cols=2,\n    horizontal_spacing = 0.02,\n    subplot_titles = (\"Average Glucose level\", \"Body mass index\")\n)\n\nfor i in [0,1]:\n    if i == 0:\n        name = 'No'\n        color = 'rgb(217,175,107)'\n        group = 'g_No'\n    else:\n        name = 'Yes'\n        color = 'rgb(204,80,62)'\n        group = 'g_Yes'\n        \n    fig.add_trace(\n        go.Histogram(\n            x = data[data['stroke']==i]['avg_glucose_level'],\n            nbinsx  = 50,\n            legendgroup = group,\n            name = name,\n            marker = dict(color=color),\n            showlegend = False\n        ),\n        row=1, col=1,\n    )\n    \n    fig.add_trace(\n        go.Histogram(\n            x = data[data['stroke']==i]['bmi'],\n            nbinsx  = 50,\n            legendgroup = group,\n            name = name,\n            marker = dict(color=color)\n        ),\n        row=1, col=2\n    )\n\nfig.update_layout(barmode='overlay', bargap=0)\nfig.update_xaxes(row=1, col=1, title_text='Glucose level')\nfig.update_xaxes(row=1, col=2, title_text='BMI')\nfig.update_yaxes(row=1, col=1, title_text='count')\nfig.update_layout(legend_title_text='Stroke')\n\nfig.show()","25d7f212":"temp = data.groupby(by='smoking_status')['Stroke?'].agg('mean')*100\n\nfig = make_subplots(\n    subplot_titles = [\"Smoke and stroke\"],\n    specs=[[{\"secondary_y\": True}]]\n)\n\nfor i in [0,1]:\n    \n    if i == 0:\n        name = 'No'\n        color = 'rgb(217,175,107)'\n    else:\n        name = 'Yes'\n        color = 'rgb(204,80,62)'\n        \n    \n    fig.add_trace(\n        go.Histogram(x=data[data['stroke']==i]['smoking_status'], \n                     name=name, \n                     marker = dict(color=color)),\n        secondary_y=False,\n    )\n\nfig.add_trace(\n    go.Scatter(x=temp.index, \n               y=temp.values, \n               name=\"Average\", \n               mode='markers', \n               marker=dict(size=20, color='royalblue')),\n    secondary_y=True,\n)\n\nfig.update_layout(legend_title_text='Stroke', **my_size)\nfig.update_yaxes(title_text='count', secondary_y=False)\nfig.update_yaxes(title_text='% stroke', secondary_y=True)","4abcab7d":"px.histogram(\n    data[data['gender']!='Other'],  # Because it has only 1 observation\n    x = 'gender',\n    color = 'Stroke?',\n    barmode = 'group',\n    color_discrete_sequence = px.colors.qualitative.Antique,\n    title = 'Gender and stroke',\n    **my_size\n)","90ea10e1":"data = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv').drop(['id'], axis=1)\ndata.head(3)","ffca19a0":"sns.countplot(data=data, x='stroke')\nplt.show()","ab984eaf":"data.isnull().sum()","bb87b555":"# Preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost\n\n# Evaluation\nfrom sklearn.metrics import classification_report, confusion_matrix","12dcbefa":"X = data.drop(['stroke'],axis=1)\nY = data['stroke']\n\nX_category = X.select_dtypes(include='object')\nX_numeric = X.select_dtypes(exclude='object')\n\nX = pd.concat([X_category, X_numeric], axis=1)","8855a0aa":"# Building the preprocessing pipeline\nimp_std = Pipeline(\n    steps=[\n        ('impute', SimpleImputer(strategy='median')),\n        ('scale', StandardScaler()),\n    ]\n)\n\nct = ColumnTransformer(\n    remainder='passthrough',\n    transformers = [\n        (\"Encoding\",OneHotEncoder(),[0,1,2,3,4]),\n        (\"Scaler\", imp_std,[5,6,7,8,9])\n    ]\n)\n\n\n# Split the data\nX_train_idle, X_test_idle, y_train, y_test = train_test_split(X, Y, \n                                                              test_size=0.2, \n                                                              stratify=Y)\n\n# Fit our transformers to train set\nct.fit(X_train_idle)\n\n# Transform both train and test set\nX_train = ct.transform(X_train_idle)\nX_test = ct.transform(X_test_idle)","b5a71d9e":"from imblearn.over_sampling import SMOTE\n\nX_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train)","5c1ab6c9":"models = dict()\nmodels['Dicision Tree'] = DecisionTreeClassifier(class_weight={0:1,1:2})\nmodels['Random Forest'] = RandomForestClassifier(class_weight={0:1,1:2})\nmodels['Logreg'] = LogisticRegression()\nmodels['GradientBoost'] = GradientBoostingClassifier()\nmodels['AdaBoost'] = AdaBoostClassifier()\nmodels['XGBoost'] = xgboost.XGBClassifier()","b2c42098":"for model in models:\n    models[model].fit(X_train_resampled, y_train_resampled)\n    print(model + ' : fit')","d9301fc0":"print(\"Train set prediction\")\nfor x in models:\n        \n    print('------------------------'+x+'------------------------')\n    model = models[x]\n    y_train_pred = model.predict(X_train_resampled)\n    arg_train = {'y_true':y_train_resampled, 'y_pred':y_train_pred}\n    print(confusion_matrix(**arg_train))\n    print(classification_report(**arg_train))","3e5efede":"print(\"Test set prediction\")\nfor x in models:\n        \n    print('------------------------'+x+'------------------------')\n    model = models[x]\n    y_test_pred = model.predict(X_test)\n    arg_test = {'y_true':y_test, 'y_pred':y_test_pred}\n    print(confusion_matrix(**arg_test))\n    print(classification_report(**arg_test))","85687205":"from sklearn.metrics import roc_curve, roc_auc_score\n\nfig, ax = plt.subplots()\nfig.set_size_inches(13,6)\n\nfor m in models:\n    y_pred = models[m].predict_proba(X_test)\n    fpr, tpr, _ = roc_curve(y_test, y_pred[:,1].ravel())\n    plt.plot(fpr,tpr, label=m)\nplt.xlabel('False-Positive rate')\nplt.ylabel('True-Positive rate')\nplt.legend()\nplt.show()","fa7c3397":"print('roc_auc_score')\nfor i in models:\n    model = models[i]\n    print(i + ' : ',roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]).round(4))","7daed83d":"## 1.4) Health information","7218fa4b":"## 1.1) Age","3da61bd6":"In this section, we'll look into the effect of smoke level, BMI, ,gender, and Glucose level on a stroke.","068b9c24":"According to the highly imbalance of this dataset, at my first run, the models perform very well in predicting major class (0: not having stroke) but very poorly for minor class. So, I'll try applying SMOTE to oversample the dataset in hope that the models can learn more efficiently.","3a32afdd":"See the performance on train set.","0e500a98":"Building models with their default parameters.","44436ad7":"We see that people that have ever had both heart disease and hypertension are most likely to have a stroke. On the other hand, people that never have those diseases tend to not having a stroke too. ","0c74130f":"We see that people that are ever smoke(both formerly and presently) have a relatively high chance to have a stroke.","875b6aa0":"## 1.3) Personal information","d20f2bca":"# 3) Building models","743ad136":"Lastly, let's see the roc curve to compare the performance of different models.","67a5591d":"We don't see any clear relation between Glucose level, BMI to stroke. It seems like people can have a stroke at every level of Glucose and BMI.","075a9b24":"# 2) Preprocess the data","d3d28882":"The metric I give more interest is **\"Recall\"** rather than accuracy because I don't want the situation like the following: <br>\n    - \"A person is very likely to have a stroke but the model tells he\/she doesn't\"\n\nWhich is a very bad situation. The model will tell us like that when it has low recall (high False Negative rate). <br>\nThe True Negative situation (model tells that this a person will have a stroke but he\/she actually doesn't) is not that bad compared to the first one. In the second case, a person will have to take a good care of his health.","dc3f1f82":"Let's import the fundamental modules.","3937e9fb":"# 1) Explore Data Analysis","0b38d051":"## 1.2) Disease record : Hypertension, heart disease","f6663313":"I'll rearrange the columns so that we can easily track the index of columns for ColumnTransformer in next step.","e0638089":"I'll use sklearn.pipeline.Pipeline to sequentially transform the numerical columns by imputting followed by scaling. Then, pass this pipeline along with OneHotEncoder to ColumnsTransformer to do the Preprocessing stuff. \n\nOf course, we have to split the data into train set and test set. Then we fit the ColumnsTransformer to the train set and transform it to both of them.","1033797a":"In this notebook, I do:\n   - Visualize the effect of variables on the stroke\n   - Building the models to predict a stroke disease given the predictors\n\nThe main problem of this dataset is that it's highly imbalanced in target class (stroke). But the methods like SMOTE and adjucting the decision threshold can help us deal with this problem.","ffa198b5":"Inspecting from models' classification report, I would say that Logistic regression model has done the best job here. <br>\n**Note:** Furthermore, We can try **tuning models' hyperparameters** to get the better result or **adjusting the probablity threshold** to improve their performance. (*I'll do that in the next update*)","d5476cae":"Fit the models to the resampled train set.","08306948":"There're only missing values in bmi column. I'll fill the mean to them. <br>\nNext step, I'll do preprocessing the data and fitting it to models. Let's import the relevant classes.","6d1ba4d3":"## - Missing values, Standardize, Encoding <br>\nLet's import the dataset again.","7e754000":"The performance on train set is (too) good. That's because we use SMOTE. It makes model learn very well because of having a perfect balance dataset. <br>\nNext, see the performance in test set.","42461de6":"We see that the dataset is very imbalanced. I'll do my best to deal with it later. <br>\nBut first let's see how many missing values.","0a4281a7":"In this section, we'll look into the effect of married status, working type, and residence type on a stroke.","7be5dbaa":"We clearly see that most of the people that have a stroke are elderly."}}