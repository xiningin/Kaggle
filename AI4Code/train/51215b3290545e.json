{"cell_type":{"ea186b08":"code","a4369bb3":"code","628dd4d9":"code","aab646de":"code","40cf2ed0":"code","73c79dc4":"code","7a84a3fb":"code","68dda0a9":"code","733b816e":"code","6496da54":"code","50d37ab0":"code","9edee395":"code","994a8be7":"code","f7f896ba":"code","0814db32":"code","6a709b0c":"code","c21cfb6e":"code","42cdcfda":"code","a2bb333f":"code","eacf2b6d":"code","b4f95f17":"code","20c4b9a1":"code","9d8a12ef":"code","8a7ca8aa":"code","18704d98":"code","1d953caa":"code","e0f59803":"code","8909a8ae":"code","7342630c":"code","70845887":"code","3c2e57af":"code","40f1b387":"code","b95e5c0e":"code","623ff0db":"code","685ba76a":"code","0edcb0e1":"code","b9b565cb":"code","386eaf3f":"code","73e831bf":"code","859cee96":"code","4dc67446":"code","4bffd6b3":"code","aaeae7e5":"code","45d00cf3":"code","2d53a3ce":"code","b387e917":"code","713bfbf3":"code","1b18fa09":"code","52f5c26d":"code","df568427":"code","53c83905":"code","21788ae8":"code","3012836e":"code","452feb6c":"code","99b78e11":"code","12aacaab":"code","15e72f16":"code","371f9afa":"code","fb499768":"code","f435f6b6":"code","502dce01":"code","a90c4180":"code","d650f3f2":"code","f0c72233":"code","da447c40":"code","f9e9196d":"code","511a42f8":"code","3c3b0602":"code","c2965cfc":"code","478db633":"code","3d68bb11":"code","73ac9d13":"code","c7053d4b":"code","a17ca3b5":"code","c2ef5454":"code","05fdd02c":"code","50f96bb5":"code","520d23a4":"code","0e7803c3":"code","0f4acd97":"code","869d3a01":"code","18b12fa4":"code","cbdf3d4d":"code","9dda075c":"code","4e8e12ab":"markdown","5cbb5dd8":"markdown","7210cb39":"markdown","2a608812":"markdown","6699a6ea":"markdown","e13286bc":"markdown","d1f629da":"markdown","8b013d6e":"markdown","b17f9c7f":"markdown","fd95f19b":"markdown","a1f5b396":"markdown","fea10731":"markdown","f1100583":"markdown","0bb22444":"markdown","3ac5078d":"markdown","f9ac1a73":"markdown","046ae014":"markdown","e5295f1e":"markdown","4339e519":"markdown","d4bcca0d":"markdown","18c826f4":"markdown","2595f765":"markdown","d9b922a9":"markdown","4580aecd":"markdown","9bb4dfdb":"markdown","a7854218":"markdown","acb6b525":"markdown","ed021a93":"markdown","35824fb1":"markdown","75ee8e98":"markdown","029b58fa":"markdown","b93456bc":"markdown","10fe1912":"markdown","063a79b6":"markdown","4581fffb":"markdown","5ba306f0":"markdown","5f9163f0":"markdown","ca8d0d15":"markdown","20c44db6":"markdown","34b0b713":"markdown","d184d338":"markdown","3c61db3c":"markdown","8d504127":"markdown","9a9d3cb2":"markdown","70c32d72":"markdown","9ee6b505":"markdown","eb70abdf":"markdown","6e2a0b58":"markdown","b65c3240":"markdown","b0be81d0":"markdown","358b730f":"markdown","6b52da69":"markdown","246b7e01":"markdown","d929ef1b":"markdown","72134dba":"markdown","b1fa1058":"markdown","e5ce7ee1":"markdown","d7797dc3":"markdown","ef29ade0":"markdown","55231a87":"markdown","559a827b":"markdown","64a36b1f":"markdown","68cecb30":"markdown","3e5f3eeb":"markdown","5f21d888":"markdown","d8e5e5b5":"markdown","2f8df21a":"markdown","365cabb8":"markdown","15789753":"markdown","ad416a8d":"markdown"},"source":{"ea186b08":"#Import the different libraries \nimport os\nprint(os.listdir(\"..\/input\")) #display the available files for analysis\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport seaborn as sns\nfrom collections import defaultdict\nimport re\nfrom bs4 import BeautifulSoup\n","a4369bb3":"#Code for wordcloud (adapted for removal of stop words)\n\n#Code adpted from : https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc\n\n#import the wordcloud package\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n#Define the word cloud function with a max of 200 words\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(10,10), \n                   title = None, title_size=20, image_color=False):\n    stopwords = set(STOPWORDS)\n    #define additional stop words that are not contained in the dictionary\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n    #Generate the word cloud\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    #set the plot parameters\n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  ","628dd4d9":"#ngram function\ndef ngram_extractor(text, n_gram):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n# Function to generate a dataframe with n_gram and top max_row frequencies\ndef generate_ngrams(df, n_gram, max_row):\n    temp_dict = defaultdict(int)\n    for question in df:\n        for word in ngram_extractor(question, n_gram):\n            temp_dict[word] += 1\n    temp_df = pd.DataFrame(sorted(temp_dict.items(), key=lambda x: x[1])[::-1]).head(max_row)\n    temp_df.columns = [\"word\", \"wordcount\"]\n    return temp_df\n\n#Function to construct side by side comparison plots\ndef comparison_plot(df_1,df_2,col_1,col_2, space):\n    fig, ax = plt.subplots(1, 2, figsize=(20,10))\n    \n    sns.barplot(x=col_2, y=col_1, data=df_1, ax=ax[0], color=\"royalblue\")\n    sns.barplot(x=col_2, y=col_1, data=df_2, ax=ax[1], color=\"royalblue\")\n\n    ax[0].set_xlabel('Word count', size=14)\n    ax[0].set_ylabel('Words', size=14)\n    ax[0].set_title('Top words in sincere questions', size=18)\n\n    ax[1].set_xlabel('Word count', size=14)\n    ax[1].set_ylabel('Words', size=14)\n    ax[1].set_title('Top words in insincere questions', size=18)\n\n    fig.subplots_adjust(wspace=space)\n    \n    plt.show()","aab646de":"#Import and view information of the Professionals Dataset\nprof = pd.read_csv('..\/input\/professionals.csv')\nprof.info()","40cf2ed0":"#Print the start and end dates\nprint('Oldest Professional join date:',prof.professionals_date_joined.min(), '\\n' +'Most Recent Professional join date:',prof.professionals_date_joined.max()) ","73c79dc4":"#Tabulate the count of missing values in the dataset\nprof.isnull().sum()\n\n#The location (~11% of missing values), industry (~9% of missing values) and headline (~7% of missing values)columns have missing values \n#missing value % has been calculated as [missing value count\/ 28152]","7a84a3fb":"#Select headlines from professionals dataset\nprof_headlines = prof[\"professionals_headline\"]\nprof_headlines.replace('--', np.nan, inplace=True) \nprof_headlines_na = prof_headlines.dropna()\n#run the function on the professional headlines and Remove NA values for clarity of visualisation\nplot_wordcloud(prof_headlines_na, title=\"Word Cloud of Professionals Headlines\")","68dda0a9":"#Define a barplot for each\n#Below code adapted from: https:\/\/www.kaggle.com\/anu0012\/quick-start-eda-careervillage-org\n\nheadlines = prof_headlines_na.value_counts().head(20)\nplt.figure(figsize=(12,8))\nsns.barplot(headlines.values, headlines.index)\nplt.xlabel(\"Count\", fontsize=15)\nplt.ylabel(\"Unique Headlines\", fontsize=15)\nplt.title(\"Top 20 Unique Professionals Headlines\")\nplt.show()","733b816e":"prof_industry_na = prof[\"professionals_industry\"].dropna()       \nindustries = prof_industry_na.value_counts().head(20)\nplt.figure(figsize=(12,8))\nsns.barplot(industries.values, industries.index)\nplt.xlabel(\"Count\", fontsize=15)\nplt.ylabel(\"Unique Industries\", fontsize=15)\nplt.title(\"Top 20 Unique Professionals Headlines\")\nplt.show()","6496da54":"prof_loc_na = prof[\"professionals_location\"].dropna()       \ntop_loc = prof_loc_na.value_counts().head(20)\nprint(top_loc)","50d37ab0":"#Import and view information of the School membership Dataset\nschool = pd.read_csv('..\/input\/school_memberships.csv')\nschool.info()\n#Check for missing values\nschool.isnull().sum()","9edee395":"#Identify the top school IDs\nschool_id_na = school[\"school_memberships_school_id\"] \ntop_school_ids = school_id_na.value_counts().head(20)\nprint(top_school_ids)","994a8be7":"#Import and view information of the Matches Dataset\nmatch = pd.read_csv('..\/input\/matches.csv')\nmatch.info()\n#Check for missing values\nmatch.isnull().sum()","f7f896ba":"#Identify which email ids contained the most questions\ntop_match_emails = match[\"matches_email_id\"] .value_counts().head(5)\nprint(top_match_emails)","0814db32":"#Import and view information of the Tags Dataset\ntag_ques = pd.read_csv('..\/input\/tag_questions.csv')\ntag_ques.info()\n#Check for missing values\ntag_ques.isnull().sum()","6a709b0c":"#Identify which tag ids were the most used\ntag_ques_pair = tag_ques[\"tag_questions_tag_id\"] .value_counts().head(5)\nprint(tag_ques_pair)","c21cfb6e":"#Import and view information of the Tags Dataset\ntags = pd.read_csv('..\/input\/tags.csv')\ntags.info()\n#Check for missing values\ntags.isnull().sum()","42cdcfda":"#Identify which tags were the most used\ntags_names = tags[\"tags_tag_name\"] .value_counts().head(5)\nprint(tags_names)","a2bb333f":"#Import and view information of the Tags Dataset\nans = pd.read_csv('..\/input\/answers.csv')\nans.info()\n#Check for missing values\nans.isnull().sum()","eacf2b6d":"#Print the oldest and most recent start date\nprint('Oldest answer date:',ans.answers_date_added.min(), '\\n' +'Most recent answer date:',ans.answers_date_added.max()) ","b4f95f17":"ans_author_id = ans[\"answers_author_id\"] .value_counts().tail(5) #switch to .head for the top n responses\nprint(ans_author_id)","20c4b9a1":"#Obtain the unique counts of author ids \nans_author = ans[\"answers_author_id\"].value_counts().head(20)\n#print(ans_author.tail(5))\nplt.figure(figsize=(12,8))\nsns.barplot(ans_author.values, ans_author.index)\nplt.xlabel(\"Count\", fontsize=15)\nplt.ylabel(\"Unique author_id\", fontsize=15)\nplt.title(\"Top 20 Unique author_id\")\nplt.show()","9d8a12ef":"#Select headlines from professionals dataset\nans_body = ans[\"answers_body\"]\nans_body_na = ans_body.dropna()\n#run the function on the professional headlines and Remove NA values for clarity of visualisation\nplot_wordcloud(ans_body_na, title=\"Word Cloud for Answer body\")","8a7ca8aa":"#Define empty list\nans_bod_cleaned = []\nres = []\n#Define for loop to iterate through the elements of the answer_body\nfor l in ans_body_na:\n    #Parse the contents of the cell\n    soup = BeautifulSoup(l, 'html.parser')\n    #Find all instances of the text within the <\/p> tag\n    for el in soup.find_all('p'):\n        res.append(el.get_text())\n    #concatenate the strings from the list    \n    endstring = ' '.join(map(str, res))\n    #reset list\n    res = []\n    #Append the concatenated string to the main list\n    ans_bod_cleaned.append(endstring)","18704d98":"#convert list elements to lower case\nans_body_na_cleaned = [item.lower() for item in ans_bod_cleaned]\n#remove html links from list \nans_body_na_cleaned =  [re.sub(r\"http\\S+\", \"\", item) for item in ans_body_na_cleaned]\n#remove special characters left\nans_body_na_cleaned = [re.sub(r\"[-()\\\"#\/@;:<>{}`+=~|.!?,]\", \"\", item) for item in ans_body_na_cleaned]\n#convert to dataframe and rename the column of the ans_body_na_cleaned list\nans_body_na_clean = pd.DataFrame(np.array(ans_body_na_cleaned).reshape(-1))\nans_body_na_clean.columns = [\"ans\"]\n#Squeeze dataframe to obtain series\nanswers_cleaned = ans_body_na_clean.squeeze()","1d953caa":"#generate unigram\nans_unigram = generate_ngrams(answers_cleaned, 1, 20)","e0f59803":"#generate barplot for unigram\nplt.figure(figsize=(12,8))\nsns.barplot(ans_unigram[\"wordcount\"],ans_unigram[\"word\"])\nplt.xlabel(\"Word Count\", fontsize=15)\nplt.ylabel(\"Unigrams\", fontsize=15)\nplt.title(\"Top 20 Unigrams for Answer body\")\nplt.show()","8909a8ae":"#generate bigram\nans_bigram = generate_ngrams(answers_cleaned, 2, 20)","7342630c":"#generate barplot for bigram\nplt.figure(figsize=(12,8))\nsns.barplot(ans_bigram[\"wordcount\"],ans_bigram[\"word\"])\nplt.xlabel(\"Word Count\", fontsize=15)\nplt.ylabel(\"Bigrams\", fontsize=15)\nplt.title(\"Top 20 Bigrams for Answer body\")\nplt.show()","70845887":"#generate trigram\nans_trigram = generate_ngrams(answers_cleaned, 3, 20)","3c2e57af":"#generate barplot for bigram\nplt.figure(figsize=(12,8))\nsns.barplot(ans_trigram[\"wordcount\"],ans_trigram[\"word\"])\nplt.xlabel(\"Word Count\", fontsize=15)\nplt.ylabel(\"Trigrams\", fontsize=15)\nplt.title(\"Top 20 Trigrams for Answer body\")\nplt.show()","40f1b387":"# Number of words in the answers\nanswers_cleaned[\"word_count\"] = answers_cleaned.apply(lambda x: len(str(x).split()))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"word_count\", data=answers_cleaned, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Word Count', size=10, color=\"#0D47A1\")\nax.set_ylabel('Answer body', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Word Count distribution for Answer body', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","b95e5c0e":"# Number of stopwords in answers\nanswers_cleaned[\"stop_words_count\"] = answers_cleaned.apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"stop_words_count\", data=answers_cleaned, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Stop Word Count', size=10, color=\"#0D47A1\")\nax.set_ylabel('Answer body', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Number of Stop Words distribution for Answer body', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","623ff0db":"#Import and view information of the Tags Dataset\nemails = pd.read_csv('..\/input\/emails.csv')\nemails.info()\n#Check for missing values\nemails.isnull().sum()","685ba76a":"#Print the oldest and most recent start date\nprint('Oldest email date:',emails.emails_date_sent.min(), '\\n' +'Most recent email date:',emails.emails_date_sent.max()) ","0edcb0e1":"#The different frequencies at which the emails were received\nemails[\"emails_frequency_level\"].unique()    ","b9b565cb":"#Identify which recipients were the most solicited\nrecipient_emails = emails[\"emails_recipient_id\"] .value_counts().head(5)\nprint(recipient_emails)","386eaf3f":"#Import and view information of the Tags Dataset\nstudents = pd.read_csv('..\/input\/students.csv')\nstudents.info()\n#Check for missing values\nstudents.isnull().sum()","73e831bf":"#Identify where most students are from\nstudents_loc = students[\"students_location\"].dropna()\nprint(students_loc.value_counts().head(5))","859cee96":"#Print the oldest and most recent start date\nprint('Oldest student registration date:',students.students_date_joined.min(), '\\n' +'Most recent student registration date:',students.students_date_joined.max()) ","4dc67446":"#Import and view information of the Tags Dataset\nques = pd.read_csv('..\/input\/questions.csv')\nques.info()\n#Check for missing values\nques.isnull().sum()","4bffd6b3":"#Print the oldest and most recent start date\nprint('Oldest question date:',ques.questions_date_added.min(), '\\n' +'Most recent question date:',ques.questions_date_added.max()) ","aaeae7e5":"#Identify who asked the most questions\nques_auth = ques[\"questions_author_id\"].dropna()\nprint(ques_auth.value_counts().head(5))","45d00cf3":"#Select headlines from professionals dataset\nques_title = ques[\"questions_title\"]\nques_title_na = ques_title.dropna()\n#run the function on the professional headlines and Remove NA values for clarity of visualisation\nplot_wordcloud(ques_title_na, title=\"Word Cloud for Question title\")","2d53a3ce":"#convert list elements to lower case\nquest_title_na_cleaned = [item.lower() for item in ques_title_na]\n#remove html links from list \nquest_title_na_cleaned =  [re.sub(r\"http\\S+\", \"\", item) for item in quest_title_na_cleaned]\n#remove special characters left\nquest_title_na_cleaned = [re.sub(r\"[-()\\\"#\/@;:<>{}`+=~|.!?,]\", \"\", item) for item in quest_title_na_cleaned]","b387e917":"#generate unigram\nques_title_unigram = generate_ngrams(quest_title_na_cleaned, 1, 20)","713bfbf3":"#generate barplot for unigram\nplt.figure(figsize=(12,8))\nsns.barplot(ques_title_unigram[\"wordcount\"],ques_title_unigram[\"word\"])\nplt.xlabel(\"Word Count\", fontsize=15)\nplt.ylabel(\"Unigrams\", fontsize=15)\nplt.title(\"Top 20 Unigrams for Question title\")\nplt.show()","1b18fa09":"#generate bigram\nques_title_bigram = generate_ngrams(quest_title_na_cleaned, 2, 20)","52f5c26d":"#generate barplot for bigram\nplt.figure(figsize=(12,8))\nsns.barplot(ques_title_bigram[\"wordcount\"],ques_title_bigram[\"word\"])\nplt.xlabel(\"Word Count\", fontsize=15)\nplt.ylabel(\"Bigrams\", fontsize=15)\nplt.title(\"Top 20 Bigrams for Question title\")\nplt.show()","df568427":"#generate trigram\nques_title_trigram = generate_ngrams(quest_title_na_cleaned, 3, 20)","53c83905":"#generate barplot for trigram\nplt.figure(figsize=(12,8))\nsns.barplot(ques_title_trigram[\"wordcount\"],ques_title_trigram[\"word\"])\nplt.xlabel(\"Word Count\", fontsize=15)\nplt.ylabel(\"Bigrams\", fontsize=15)\nplt.title(\"Top 20 Trigrams for Question title\")\nplt.show()","21788ae8":"#convert to dataframe and rename the column of the ans_body_na_cleaned list\nques_title_na_clean = pd.DataFrame(np.array(quest_title_na_cleaned).reshape(-1))\nques_title_na_clean.columns = [\"ques_title\"]\nques_title_na_clean = ques_title_na_clean.squeeze()\n\n# Number of words in the question_title\nques_title_na_clean[\"word_count\"] = ques_title_na_clean.apply(lambda x: len(str(x).split()))\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"word_count\", data= ques_title_na_clean, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Word Count', size=10, color=\"#0D47A1\")\nax.set_ylabel('Question title text', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Word Count distribution for Question title', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","3012836e":"# Number of stopwords in question_title\nques_title_na_clean[\"stop_words_count\"] = ques_title_na_clean.apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"stop_words_count\", data=answers_cleaned, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Stop Word Count', size=10, color=\"#0D47A1\")\nax.set_ylabel('Question title text', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Number of Stop Words distribution for Question title', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","452feb6c":"#Select headlines from professionals dataset\nques_bod = ques[\"questions_body\"]\nques_bod_na = ques_bod.dropna()\n#run the function on the professional headlines and Remove NA values for clarity of visualisation\nplot_wordcloud(ques_bod_na, title=\"Word Cloud for Question Body\")","99b78e11":"#convert list elements to lower case\nquest_bod_na_cleaned = [item.lower() for item in ques_bod_na]\n#remove special characters left\nquest_bod_na_cleaned = [re.sub(r\"[-()\\\"#\/@;:<>{}`+=~|.!?,]\", \"\", item) for item in quest_bod_na_cleaned]","12aacaab":"#generate unigram\nques_bod_unigram = generate_ngrams(quest_bod_na_cleaned, 1, 20)","15e72f16":"#generate barplot for unigram for question_body\nplt.figure(figsize=(12,8))\nsns.barplot(ques_bod_unigram[\"wordcount\"],ques_bod_unigram[\"word\"])\nplt.xlabel(\"Word Count\", fontsize=15)\nplt.ylabel(\"Unigrams\", fontsize=15)\nplt.title(\"Top 20 Unigrams for Question Body\")\nplt.show()","371f9afa":"#generate bigram\nques_bod_bigram = generate_ngrams(quest_bod_na_cleaned, 2, 20)","fb499768":"#generate barplot for bigram question_body\nplt.figure(figsize=(12,8))\nsns.barplot(ques_bod_bigram[\"wordcount\"],ques_bod_bigram[\"word\"])\nplt.xlabel(\"Word Count\", fontsize=15)\nplt.ylabel(\"Bigrams\", fontsize=15)\nplt.title(\"Top 20 Bigrams for Question Body\")\nplt.show()","f435f6b6":"#generate trigram\nques_bod_trigram = generate_ngrams(quest_bod_na_cleaned, 3, 20)","502dce01":"#generate barplot for bigram question_body\nplt.figure(figsize=(12,8))\nsns.barplot(ques_bod_trigram[\"wordcount\"],ques_bod_trigram[\"word\"])\nplt.xlabel(\"Word Count\", fontsize=15)\nplt.ylabel(\"Trigrams\", fontsize=15)\nplt.title(\"Top 20 Trigrams for Question Body\")\nplt.show()","a90c4180":"#convert to dataframe and rename the column of the ans_body_na_cleaned list\nques_bod_na_clean = pd.DataFrame(np.array(quest_bod_na_cleaned).reshape(-1))\nques_bod_na_clean.columns = [\"ques_body\"]\nques_bod_na_clean = ques_bod_na_clean.squeeze()\n\n# Number of words in the question_body\nques_bod_na_clean[\"word_count\"] = ques_bod_na_clean.apply(lambda x: len(str(x).split()))\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"word_count\", data= ques_bod_na_clean, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Word Count', size=10, color=\"#0D47A1\")\nax.set_ylabel('Question Body text', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Word Count distribution for Question Body', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","d650f3f2":"# Number of stopwords in question_body\nques_bod_na_clean[\"stop_words_count\"] = ques_bod_na_clean.apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"stop_words_count\", data=ques_bod_na_clean, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Stop Word Count', size=10, color=\"#0D47A1\")\nax.set_ylabel('Question Body text', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Number of Stop Words distribution for Question Body', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","f0c72233":"#Import and view information of the Groups Dataset\ngroups = pd.read_csv('..\/input\/groups.csv')\ngroups.info()\n#Check for missing values\ngroups.isnull().sum()","da447c40":"#Identify who asked the most questions\ngroup_type = groups[\"groups_group_type\"].dropna()\nprint(group_type .value_counts().tail(5)) #switch to .head for the top results ","f9e9196d":"#Import and view information of the Groups membership Dataset\ngroups_mem = pd.read_csv('..\/input\/group_memberships.csv')\ngroups_mem.info()\n#Check for missing values\ngroups_mem.isnull().sum()","511a42f8":"#Identify the most popular group_memberships_user_id     \ngroups_mem_id = groups_mem[\"group_memberships_user_id\"].dropna()\nprint(groups_mem_id .value_counts().head(5))","3c3b0602":"#Identify the most popular group_memberships_group_id\ngroups_mem_g_id = groups_mem[\"group_memberships_group_id\"].dropna()\nprint(groups_mem_g_id .value_counts().head(5))","c2965cfc":"#Import and view information of the comments Dataset\ncomms = pd.read_csv('..\/input\/comments.csv')\ncomms.info()\n#Check for missing values\ncomms.isnull().sum()","478db633":"#Identify the most popular commenters (comments_author_id)\ncomms_authors = comms[\"comments_author_id\"].dropna()\nprint(comms_authors .value_counts().head(5))","3d68bb11":"#Print the oldest and most recent comment dates\nprint('Oldest comment date:',comms.comments_date_added.min(), '\\n' +'Most comment date:',comms.comments_date_added.max()) ","73ac9d13":"#select comments text from comments dataste\ncomms_bod = comms[\"comments_body\"]\ncomms_bod_na = comms_bod.dropna()\n#run the function on the professional headlines and Remove NA values for clarity of visualisation\nplot_wordcloud(comms_bod_na, title=\"Word Cloud of Comments body\")","c7053d4b":"#convert list elements to lower case\ncomms_bod_na_cleaned = [item.lower() for item in comms_bod_na]\n#remove html links from list \ncomms_bod_na_cleaned =  [re.sub(r\"http\\S+\", \"\", item) for item in comms_bod_na_cleaned]\n#remove special characters left\ncomms_bod_na_cleaned = [re.sub(r\"[-()\\\"#\/@;:<>{}`+=~|.!?,]\", \"\", item) for item in comms_bod_na_cleaned]","a17ca3b5":"#generate unigram from comments body\ncomms_bod_unigram = generate_ngrams(comms_bod_na_cleaned, 1, 20)","c2ef5454":"#generate barplot for unigram from comments body\nplt.figure(figsize=(12,8))\nsns.barplot(comms_bod_unigram[\"wordcount\"],comms_bod_unigram[\"word\"])\nplt.xlabel(\"Word Count\", fontsize=15)\nplt.ylabel(\"Unigrams\", fontsize=15)\nplt.title(\"Top 20 unigrams from Comments Body\")\nplt.show()","05fdd02c":"#generate bigram from comments body\ncomms_bod_bigram = generate_ngrams(comms_bod_na_cleaned, 2, 20)","50f96bb5":"#generate barplot for bigram\nplt.figure(figsize=(12,8))\nsns.barplot(comms_bod_bigram[\"wordcount\"],comms_bod_bigram[\"word\"])\nplt.xlabel(\"Word Count\", fontsize=15)\nplt.ylabel(\"Bigrams\", fontsize=15)\nplt.title(\"Top 20 Bigrams from Comments Body\")\nplt.show()","520d23a4":"#generate trigram from comments body\ncomms_bod_trigram = generate_ngrams(comms_bod_na_cleaned, 3, 20)","0e7803c3":"#generate barplot for bigram\nplt.figure(figsize=(12,8))\nsns.barplot(comms_bod_trigram[\"wordcount\"],comms_bod_trigram[\"word\"])\nplt.xlabel(\"Word Count\", fontsize=15)\nplt.ylabel(\"Bigrams\", fontsize=15)\nplt.title(\"Top 20 Trigrams from Comments Body\")\nplt.show()","0f4acd97":"#convert to dataframe and rename the column of the ans_body_na_cleaned list\ncomms_bod_na_clean = pd.DataFrame(np.array(comms_bod_na_cleaned).reshape(-1))\ncomms_bod_na_clean.columns = [\"ques_body\"]\ncomms_bod_na_clean = comms_bod_na_clean.squeeze()\n\n# Number of words in the question_body\ncomms_bod_na_clean[\"word_count\"] = comms_bod_na_clean.apply(lambda x: len(str(x).split()))\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"word_count\", data= comms_bod_na_clean, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Word Count', size=10, color=\"#0D47A1\")\nax.set_ylabel('Comment Body text', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Word Count distribution for Comments Body', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","869d3a01":"# Number of stopwords in question_body\ncomms_bod_na_clean[\"stop_words_count\"] = comms_bod_na_clean.apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"stop_words_count\", data=comms_bod_na_clean, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Stop Word Count', size=10, color=\"#0D47A1\")\nax.set_ylabel('Comments Body text', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Number of Stop Words distribution for Comments Body', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","18b12fa4":"#Import and view information of the comments Dataset\ntags = pd.read_csv('..\/input\/tag_users.csv')\ntags.info()\n#Check for missing values\ntags.isnull().sum()","cbdf3d4d":"#Identify the most popular tag_user_tag_id\ntag_user = tags[\"tag_users_tag_id\"].dropna()\nprint(tag_user.value_counts().head(5))","9dda075c":"#Identify the most popular tag_user_tag_id\ntag_user_id = tags[\"tag_users_user_id\"].dropna()\nprint(tag_user_id.value_counts().head(5))","4e8e12ab":"# Kernel Start\n\n<div id=\"start\"> \n\n<\/div>","5cbb5dd8":"### **Frequency counts of email ids from Matches Dataset**\n","7210cb39":"## *Students dataset*\n\n<div id=\"Stud\"> \n\n<\/div>","2a608812":"[Go back to start of kernel](#start)","6699a6ea":"###  **Word cloud of questions_body from Questions Dataset**","e13286bc":"### **Frequency counts of school ids from School memberships dataset**\n","d1f629da":"**Groups Dataset summary**\n\nIt is noted based on the below that there are 49  entries in the dataset with no missing . It is also seen that the youth program is the most popular group type with 33 mentions as compared to competition (1 mention) which is the least popular.","8b013d6e":"[Go back to start of kernel](#start)","b17f9c7f":"### **Barplots for unique counts of Industries from Professionals Dataset**\n","fd95f19b":"### **Barplots for unique counts of headlines from Professionals Dataset**\n","a1f5b396":"Thanks a lot for having gone through this attempt at conducting an EDA for the careeradvice.org dataset.\n\nAny comments, recommendations, upvotes and insights would be very appreciated!","fea10731":"###  **Box plots for Word count distribution and Stop words of question_body from questions dataset**","f1100583":"[Go back to start of kernel](#start)","0bb22444":"## *Tagged User Dataset*\n\n<div id=\"Tagu\"> \n\n<\/div>","3ac5078d":"###  **Frequency counts of most used tags from tags dataset**","f9ac1a73":" ### **Tag Questions Dataset summary**\nIt is noted based on the below that there are 76,552  entries in the dataset, with no missing values.  The top 5 occuring hastag to question id pairings are 27490, 129, 89, 54 and 27292.\n","046ae014":"###  **Word cloud of comments_body from comments dataset**\n","e5295f1e":"###  **n-gram analysis of question_title from Questions Dataset**","4339e519":"# Methodology\n\nGiven the NLP nature of the datasets, similar exploration methods will be applied to each dataset to unveil their distinct characteristics.  This will include using visualisations such as:\n\n1) **Frequency Tables**\n\nA frequency table is a method of organizing raw data in a compact form by displaying a series of scores in ascending or descending order, together with their frequencies\u2014the number of times each score occurs in the respective data set.\n\n2) **Barplots **\n\nA Barplot indicates the relationship between a categorical variable and a numerical variable. \n\n3) ** Word Clouds**\n\nWord clouds can identify trends and patterns that would otherwise be unclear or difficult to see in a tabular format. Frequently used keywords stand out better in a word cloud. Common words that might be overlooked in tabular form are highlighted in larger text making them pop out when displayed in a word cloud.\n\n4) ** N-Gram (Unigram, Bigram and Trigram)**\n\nAn n-gram is a contiguous sequence of n items from a given sample of text or speech. Different definitions of n-grams will allow for the identification of the most prevalent words\/sentences in the training data and thus help distinguish what comprises insincere and sincere questions.\n\nIt should be noted that prior to displaying individual words or sentences, the text will first be tokenized (based on a desired integer) and then put into a dataframe which will be used to construct side by side plots. Tokenization is, generally, an early step in the NLP process, a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc.\n\n5) **Box plots for Word count distribution and Stop words.**\n\nA boxplot is a standardized way of displaying the distribution of data based on a five number summary (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d). It indicates how the outliying values in a dataset are distributed and what their values are. It can also be used to examine the distribution of the data and its skewness.\n","d4bcca0d":"## *Matches Dataset*\n\n<div id=\"Mtc\"> \n\n<\/div>","18c826f4":"## *End of Kernel*\n\n<div id=\"End\"> \n\n<\/div>","2595f765":"###  **Word cloud of answers_text from Answers Dataset**","d9b922a9":" ### **Professionals Dataset Summary**\n \nIt is noted based on the below that there are 28,152 entries in the dataset, with a number of missing values (ranging from 7 to 11% of missing value per field, in the below section).  The oldest professional joined in 2011 while the most recent joiner dates to 2019. Across the top 5 headlines, it is noted that the majority of professionals operate as Solutions consultants, Assurance associates, General Managers, Software Engineers and Project Managers. It is also observed that a number of employees work at PWC across the Professionals dataset. Additionally, the top 5 industries in which the professionals work are Telcommunications, IT, Healthcare, Education and Finance. Finally, the majority of the professionals operate out of the US and India, based on the top 20 locations obtained. ","4580aecd":"## *Professionals Dataset*\n\n<div id=\"Prof\"> \n\n<\/div>","9bb4dfdb":"** Group Memberships Dataset summary**\n\nIt is noted based on the below that there are 1,038 entries in the dataset with no missing values. It is also seen that the most recurring groups_mem_id has 14 occurances as compared to 1 mention for the least popular one. Additionally, the most reoccuring group_memberships_group_id has 117 mentions and the least reoccuring one has 1 mention.","a7854218":" ### **Tag Dataset summary**\nIt is noted based on the below that there are 16,269  entries in the dataset, with 1 missing value in the tag name column.  It is noted that there is no single reoccuring tag, implying that all tags are unique.","acb6b525":"[Go back to start of kernel](#start)","ed021a93":"[Go back to start of kernel](#start)","35824fb1":"###  **n-gram analysis of answers_text from Answers Dataset**","75ee8e98":"Prior to computing the n-gram analysis, the data will first be preprocessed to remove any html tags, upper case characters, urls and special characters.","029b58fa":"###  **n-gram analysis of question_title from questions dataset**","b93456bc":"[Go back to start of kernel](#start)","10fe1912":"###  **Box plots for Word count distribution and Stop words for answer_body from Answers Dataset**","063a79b6":"###  **Frequency counts of tag ids from Tag question Dataset**","4581fffb":"**Tagged User Dataset summary**\n\nIt is noted based on the below that there are 136,663   entries in the dataset with no missing values. It is also seen that the most popular tag_users_tag_id has 3135 instances as compared to the least occuring one with 1 mention. Additionally, the least popular tag_users_user_id  stands at 1 mention and the most popular one with 82 mentions. ","5ba306f0":"[Go back to start of kernel](#start)","5f9163f0":"**Emails Dataset summary**\n\nIt is noted based on the below that there are 1,850,101 entries in the dataset with no missing values. It is noted that the oldest email dates back to 2013 and the most recent answer dates to  2019. The different frequencies at which the emails were received are daily, immediate and weekly. Additionally, the most recurring recipient id is 0079e89bf1544926b98310e81315b9f1 with 3496 unique mentions in the dataset. ","ca8d0d15":"## *Tag dataset*\n\n<div id=\"Tag\"> \n\n<\/div>","20c44db6":"**Questions Dataset summary**\n\nIt is noted based on the below that there are 23,931 entries in the dataset, with no missing values. It is also seen that the oldest question dates back to 2011 and the most recent question dates to 2019. The maximal and minimal number of answers in the dataset by an author are respectively 93 and 1, as shown in the frequency table (commented out). In line with the answers dataset, the word cloud indicates that some major themes of interest relate to success, work, repsonses, college and technology. This is in line with the n-gram analysis which outlines that Human Resources, recruitment, personality, career and university are the main themes of the answers. Finally, the distribution of the stop words and word count appears to be heavily right skewed, similar to the answers dataset.","34b0b713":"[Go back to start of kernel](#start)","d184d338":"[Go back to start of kernel](#start)","3c61db3c":"## *Comments dataset*\n\n<div id=\"comms\"> \n\n<\/div>","8d504127":"**Students Dataset summary**\n\nIt is noted based on the below that there are 30,971  entries in the dataset with 2033 missing values in the students_location column. It is noted that the oldest registered student dates back to 2011 and the most recent answer dates to  2019. Most of the students are spread across US based and Indian based locations.","9a9d3cb2":"[Go back to start of kernel](#start)","70c32d72":"## *Answers dataset*\n\n<div id=\"Ans\"> \n\n<\/div>","9ee6b505":"## *Emails dataset*\n\n<div id=\"Ems\"> \n\n<\/div>","eb70abdf":"**Comments Dataset summary**\n\nIt is noted based on the below that there are 14,966   entries in the dataset with 4 missing values in the comments_body column. It is also seen that the oldest comment dates back to 2011 and the most recent one dates back to 2019. The word clould indicates that the themes of the comments revolve around thanking users for their helpful advice. This is supported by the n-gram analysis which outlines that encouragements and thanks are the most prevalent themes in the comments. Similar to the above datasets, the comments_body has a right skewed distribution, albeit wider than the others.","6e2a0b58":"## *Tag questions dataset*\n\n<div id=\"TagQ\"> \n\n<\/div>","b65c3240":"[Go back to start of kernel](#start)","b0be81d0":"### **Frequency counts of locations for Professionals from Professionals Dataset**\n","358b730f":"### **Frequency Count of missing values in Professionals Dataset**","6b52da69":"###  **Bar plot of unique author ids from Answers Dataset **","246b7e01":" ### **Matches Dataset summary**\nIt is noted based on the below that there are 4,316,275  entries in the dataset, with no missing values.  The top 5 email ids which contained the most questions are 569938, 569892, 569829, 569941 and 508675.","d929ef1b":"[Go back to start of kernel](#start)","72134dba":"## *Questions dataset*\n\n<div id=\"Ques\"> \n\n<\/div>","b1fa1058":"Prior to computing the n-gram analysis, the data will first be preprocessed to remove any upper case and special characters.","e5ce7ee1":"## *Groups dataset*\n\n<div id=\"Grou\"> \n\n<\/div>","d7797dc3":"## *Group memberships dataset*\n\n<div id=\"Groum\"> \n\n<\/div>","ef29ade0":"Prior to computing the n-gram analysis, the data will first be preprocessed to remove any upper case and special characters.","55231a87":"[Go back to start of kernel](#start)","559a827b":"# Overview \n\nThe objective of this notebook is to investigate the nature of the provided datasets and provide a simple dedicated EDA for each of the different datasets.\n\nEach subsection will start with a summary of all the findings from the EDA for the particular dataset followed by all the visualisations and computations that have led to those observations.\n\nThe hyperlinks to the EDAs for the different datasets (13 different datasets) are as follows:\n\n* [Professionals](#Prof)\n* [School Memberships](#Sch)\n* [Matches](#Mtc)\n* [Tag Questions](#TagQ)\n* [Tags](#Tag)\n* [Answers](#Ans)\n* [Emails](#Ems)\n* [Students](#Stud)\n* [Questions](#Ques)\n* [Groups](#Grou)\n* [Group memberships](#Groum)\n* [Comments](#comms)\n* [Tagged User](#Tagu)\n\nAny comments, recommendations, upvote or suggestions are much appreciated!\n\n*Credits*: The main python code for the EDA of the different datasets has been adapted from my [kernel on the Quora Insincere Questions Classification Challenge](https:\/\/www.kaggle.com\/spurryag\/beginner-attempt-at-nlp-workflow), which has been in turn inspired by various kernels for that kaggle competition. The linked kernel contains the original references to the authors of the adapted code for the Quora Insincere Questions Classification challenge.\n\nOther kernels that have been leveraged in this notebook are:\n\nhttps:\/\/www.kaggle.com\/anu0012\/quick-start-eda-careervillage-org *\n\n**Hyperlink to end of kernel:**\n\n[End of Kernel](#End)\n","64a36b1f":" ### **School Memberships Dataset summary**\nIt is noted based on the below that there are 5,638 entries in the dataset, with no missing values.  The top 5 most frequent school ids are 196700, 200003, 196665, 196883 and 200261.","68cecb30":"## *School Memberships Dataset*\n\n<div id=\"Sch\"> \n\n<\/div>","3e5f3eeb":"### **Wordclouds for Headlines of Professionals Dataset **","5f21d888":"[Go back to start of kernel](#start)","d8e5e5b5":"###  **Box plots for Word count distribution and Stop words of question_body from comments dataset**","2f8df21a":"**Answers Dataset summary**\n\nIt is noted based on the below that there are 51,123  entries in the dataset, with 1 missing value in the answers_body column. It is also seen that the oldest answer dates back to 2011 and the most recent answer dates to 2019. The maximal and minimal number of answers in the dataset by contributor are respectively 1710 and 1, as shown in the frequency table. The word cloud indicates that some major themes of interest relate to success, work, repsonses, college and computing. The n-gram analysis outlines that caeer path, personality, career and university are the main themes of the answers. Finally, the distribution of the stop words and word count appears to be heavily right skewed.","365cabb8":"###  **Box plots for Word count distribution and Stop words of question_title from Questions dataset**","15789753":"###  **Word cloud of questions_title from Questions Dataset **","ad416a8d":"###  **n-gram analysis of comments_body from comments dataset**"}}