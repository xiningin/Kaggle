{"cell_type":{"b389dc0e":"code","69562982":"code","ec30a0de":"code","d079a1f1":"code","9de65372":"code","de43342d":"code","4d7a318c":"code","f835a8d4":"code","cf0aee2c":"code","5acc95e9":"code","1ebb7430":"code","496f2e66":"code","b6a77315":"code","bdf503de":"code","282aeada":"code","f49e51f8":"code","e4d78589":"code","cecda021":"code","5923de9b":"code","3dd58d6a":"code","525cb54b":"markdown","8d3b9399":"markdown","75576844":"markdown","5ccad610":"markdown","9bafc2fd":"markdown","8104680d":"markdown","0eee63c9":"markdown","21c06753":"markdown","f7af959d":"markdown"},"source":{"b389dc0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","69562982":"import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ec30a0de":"test_text = pd.read_csv(\"..\/input\/ag-news-classification-dataset\/test.csv\")\ntrain_text = pd.read_csv(\"..\/input\/ag-news-classification-dataset\/train.csv\")\n\nprint(f\"shape of train dataset >> {train_text.shape}\")\nprint(f\"shape of test dataset >> {test_text.shape}\")\n\ntest_text.sample(5)\ntrain_text.sample(5)","d079a1f1":"index_to_label ={\n    1:'World',\n    2:'Sports',\n    3:'Business',\n    4:'Sci\/Tech'\n}\n\ndef return_dataset(dataset):\n    label = dataset[\"Class Index\"]\n    label = pd.get_dummies(label)\n    data = dataset[\"Title\"] + \" \" + dataset[\"Description\"]\n    print(f\"shape of data >> {data.shape}\")\n    print(f\"shape of label >> {label.shape}\\n\")\n    return data,label\n\ntrain_data,train_label = return_dataset(train_text)\ntest_data,test_label = return_dataset(test_text)","9de65372":"sns.set_style(\"whitegrid\")\n\nfig = plt.figure(figsize=(8,8))\naxe1 = fig.add_subplot(1,2,1)\nsns.countplot(data=train_label)\naxe1.set_title(\"train dataset\")\naxe1.set_xlabel([index_to_label[i] for i in range(1,5)])\n\naxe2 = fig.add_subplot(1,2,2)\nsns.countplot(data=test_label)\naxe2.set_title(\"test dataset\")\naxe2.set_xlabel([index_to_label[i] for i in range(1,5)])\n\n\nplt.tight_layout()\nplt.show()","de43342d":"from keras.preprocessing.text import Tokenizer\n\ntok = Tokenizer()\ntok.fit_on_texts(train_data)\nprint(f\"numbers of words used >> {len(tok.word_index)}\")\n\nword_size = 999\nvocab_size = word_size+1 #1000\n\ntok = Tokenizer(num_words=word_size)\ntok.fit_on_texts(train_data)\n\nword_index = tok.word_index\nindex_word = tok.index_word\n\ntrain_data = tok.texts_to_sequences(train_data)\ntest_data = tok.texts_to_sequences(test_data)\n\nprint(\"First two samples\")\nprint(train_data[0])\nprint(train_data[1])","4d7a318c":"from keras.preprocessing.sequence import pad_sequences\n\nprint(f\"maximum >> {np.max([len(s)for s in train_data])}\")\nprint(f\"minimum >> {np.min([len(s)for s in train_data])}\")\nprint(f\"average >> {np.mean([len(s)for s in train_data])}\")\nprint(f\"median >> {np.median([len(s)for s in train_data])}\\n\")\n\nlens = [len(s) for s in train_data]\nplt.hist(lens,bins=50)\nplt.show()\n\nsequence_len = 50\n\ntrain_data = pad_sequences(train_data,maxlen=sequence_len,padding='post',truncating='post')\ntest_data = pad_sequences(test_data,maxlen=sequence_len,padding='post',truncating='post')\n\nprint(\"print first two samples\")\nprint(train_data[0])\nprint(train_data[1])\n\nprint(\"\\ntrain data shape >>\",train_data.shape)\nprint(\"test data shape >>\",test_data.shape)","f835a8d4":"from keras.layers import Input,Embedding,GlobalAveragePooling1D,Dense,LSTM,Bidirectional,TimeDistributed\n\ndef create_simple_model(word_vec_size=64):\n    X = Input(shape=[sequence_len])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_len)(X)\n    H = GlobalAveragePooling1D()(H)\n    Y = Dense(4,activation='softmax')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","cf0aee2c":"from keras.utils import plot_model\nfrom keras.callbacks import ReduceLROnPlateau\n\nreduceLR = ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.5,min_lr=0.0001,verbose=1)\n\ndef fit_test(model,n):\n    hist = model.fit(train_data,train_label,batch_size=64,validation_split=0.2,epochs=n,verbose=0,callbacks=[reduceLR])\n    result = model.evaluate(test_data,test_label)\n    \n    return hist,result","5acc95e9":"simple1 = create_simple_model(64)\nfit_test(simple1,7)\nplot_model(simple1)\n","1ebb7430":"simple2 = create_simple_model(128)\nfit_test(simple2,7)\nplot_model(simple2)","496f2e66":"from keras.layers import Dropout\n\ndef create_LSTM(word_vec_size=64,hidden_size=64):\n    X = Input(shape=[sequence_len])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_len)(X)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Bidirectional(LSTM(int(hidden_size\/2),return_sequences=True))(H)\n    H = GlobalAveragePooling1D()(H)\n    \n    H = Dropout(0.2)(H)\n    H = Dense(1024)(H)\n    H = Dropout(0.2)(H)\n    H = Dense(256)(H)\n    H = Dropout(0.2)(H)\n    H = Dense(32)(H)\n    H = Dropout(0.1)(H)\n    \n    Y = Dense(4,activation='softmax')(H)\n    # \uc120\ud0dd\uc9c0 2\uac1c : (1)GlobalAveragePooling\uc73c\ub85c timestep slice\ub4e4 \ud558\ub098\ub85c \ubaa8\uc544\uc8fc\uac70\ub098\n    #             (2)\uc774\uc804 LSTM\uc5d0\uc11c return_sequences=False \ud558\uac70\ub098\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","b6a77315":"lstm1 = create_LSTM(256,256)\nhist = lstm1.fit(train_data,train_label,batch_size=256,validation_split=0.2,epochs=10,verbose=1,callbacks=[reduceLR])\nev = lstm1.evaluate(test_data,test_label)","bdf503de":"# from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n\n# X_train,y_train = return_dataset(train_text)\n# X_test,y_test = return_dataset(test_text)\n\n# print(f\"Train dataset shape before preprocessing >> {train_data.shape}\")\n# print(X_train[22])\n\n# print(\"\\n\\nPreprocessing!!\\n\")\n# vectorizer = CountVectorizer()\n# transformer = TfidfTransformer()\n\n# train_data_dtm = vectorizer.fit_transform(X_train)\n# train_data_tfidf = transformer.fit_transform(train_data_dtm)\n# train_data_tfidf = train_data_tfidf.toarray()\n\n# def preprocess(data):\n#     result = vectorizer.transform(data)\n#     result = transformer.transform(result)\n#     result = result.toarray()\n#     return result\n# print(\"Preprocessing done!\\n\\n\")\n\n# test_data_tfidf = preprocess(X_test)\n\n# print(f\"Train dataset shape after preprocessing >> {train_data_tfidf.shape}\")\n# print(train_data_tfidf[22])","282aeada":"# y_train = train_text['Class Index']\n# y_test = test_text['Class Index']","f49e51f8":"# # memory \uc6a9\ub7c9 \ub54c\ubb38\uc5d0 \uc2e4\ud589 \uc548\ub428\n# from sklearn.model_selection import train_test_split\n\n# train_data_tfidf,_,y_train,_ = train_test_split(train_data_tfidf,y_train,test_size=0.8,stratify=y_train)","e4d78589":"# from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\n# from sklearn.metrics import accuracy_score\n\n# gaussian = GaussianNB()\n# bernoulli = BernoulliNB()\n# multinomial = MultinomialNB()\n\n# def test(model):\n#     print(model)\n#     model.fit(train_data_tfidf,y_train)\n    \n#     train_pred = model.predict(train_data_tfidf)\n#     train_acc = accuracy_score(y_train,train_pred)\n#     print(\"train accuracy >>\", train_acc)\n    \n#     test_pred = model.predict(test_data_tfidf)\n#     test_acc = accuracy_score(y_test,test_pred)\n#     print(\"test accuracy >>\", test_acc)\n#     print(\"\\n\")","cecda021":"# test(gaussian)","5923de9b":"from keras.layers import Concatenate,Dropout\n\ndef create_new_lstm(word_vec_size=64,hidden_size=64):\n    X = Input(shape=[sequence_len])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_len,mask_zero=True)(X)\n    \n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    all_hidden = Bidirectional(LSTM(int(hidden_size\/2),return_sequences=True))(H)\n    average = GlobalAveragePooling1D()(all_hidden)\n    last_hidden = all_hidden[:,-1,:]\n    \n    H = Concatenate()([average,last_hidden])\n    \n    H = Dropout(0.2)(H)\n    H = Dense(1024)(H)\n    H = Dropout(0.2)(H)\n    H = Dense(256)(H)\n    H = Dropout(0.2)(H)\n    H = Dense(32)(H)\n    H = Dropout(0.1)(H)\n    \n    Y = Dense(4,activation='softmax')(H)\n    # \uc120\ud0dd\uc9c0 2\uac1c : (1)GlobalAveragePooling\uc73c\ub85c timestep slice\ub4e4 \ud558\ub098\ub85c \ubaa8\uc544\uc8fc\uac70\ub098\n    #             (2)\uc774\uc804 LSTM\uc5d0\uc11c return_sequences=False \ud558\uac70\ub098\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\n\nlstm2 = create_new_lstm(256,256)\nhist = lstm2.fit(train_data,train_label,batch_size=256,validation_split=0.2,epochs=10,verbose=1,callbacks=[reduceLR])\nev = lstm2.evaluate(test_data,test_label)","3dd58d6a":"from keras.layers import GlobalMaxPooling1D\n\ndef create_new_lstm(word_vec_size=64,hidden_size=64):\n    X = Input(shape=[sequence_len])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_len,mask_zero=True)(X)\n    \n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    all_hidden = Bidirectional(LSTM(int(hidden_size\/2),return_sequences=True))(H)\n    average = GlobalMaxPooling1D()(all_hidden)\n    last_hidden = all_hidden[:,-1,:]\n    \n    H = Concatenate()([average,last_hidden])\n    \n    H = Dropout(0.2)(H)\n    H = Dense(1024)(H)\n    H = Dropout(0.2)(H)\n    H = Dense(256)(H)\n    H = Dropout(0.2)(H)\n    H = Dense(32)(H)\n    H = Dropout(0.1)(H)\n    \n    Y = Dense(4,activation='softmax')(H)\n    # \uc120\ud0dd\uc9c0 2\uac1c : (1)GlobalAveragePooling\uc73c\ub85c timestep slice\ub4e4 \ud558\ub098\ub85c \ubaa8\uc544\uc8fc\uac70\ub098\n    #             (2)\uc774\uc804 LSTM\uc5d0\uc11c return_sequences=False \ud558\uac70\ub098\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\n\nlstm3 = create_new_lstm(256,256)\nhist = lstm3.fit(train_data,train_label,batch_size=256,validation_split=0.2,epochs=10,verbose=1,callbacks=[reduceLR])\nev = lstm3.evaluate(test_data,test_label)","525cb54b":"<4-2> Model with LSTM(RNN) : bidirectional,many-to-one,stacked","8d3b9399":"<4-3> Naive Bayes Classifiers","75576844":"<2> split contents and label","5ccad610":"<1> Load train, test dataset","9bafc2fd":"<4-1> Model without RNNs","8104680d":"(4-5) LSTM (last hidden cell +(concatenate) maximum of all hidden cells)\n<4-2 acc 87% --> let's compare which is better>","0eee63c9":"<3> Preprocess text data (tokenize then apply padding) ","21c06753":"(4-4) LSTM (last hidden cell +(concatenate) average of all hidden cells)\n<4-2 acc 87% --> let's compare which is better>","f7af959d":"<4> Make models and train and test"}}