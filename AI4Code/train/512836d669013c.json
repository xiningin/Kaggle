{"cell_type":{"684cad3a":"code","ae9d4abd":"code","df775f67":"code","086129cc":"code","d42d85f8":"code","e79ed702":"code","c6f4a52a":"code","54b7074e":"code","f646cbae":"code","d35465a9":"code","21923666":"code","7f5d7a33":"code","086ce51a":"code","08aa8bd4":"code","d2594507":"code","c22f1061":"code","f527927a":"code","8c24a7ad":"code","982dd385":"code","44c54748":"code","b20bf502":"code","2869f4f9":"code","4074abad":"code","47a1bbe4":"code","66edcd14":"code","c7ceee36":"code","07e7d167":"code","b972b4ba":"code","8422f8fb":"markdown","063c1c0f":"markdown","aea57381":"markdown","edb26251":"markdown"},"source":{"684cad3a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport time\nimport csv\n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold,KFold\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\nfrom sklearn import preprocessing\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport itertools\nfrom scipy import interp\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams\n#import ggridges\n\nfolder_path = '..\/input\/'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\n# combine the data and work with the entire dataset\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n","ae9d4abd":"print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n# Shows our first file that loaded to dataframe.","df775f67":"train_transaction.head()","086129cc":"train_identity.head()","d42d85f8":"train.head()","e79ed702":"#Change the index to allow for more versatile slicing and labeling.\ntrain['TransactionID'].is_unique","c6f4a52a":"#Checking to see what data formats are in our dataframe.\ntrain.get_dtype_counts()","54b7074e":"# Select first set of columns and exclude the synthetic \"v\" features and other very sparse categoricals like deviceinfo and deviceid\ntrain_transaction = train_transaction.iloc[:, :55]\ntrain_identity = train_identity.iloc[:, :54]\n\ndel train_transaction, train_identity","f646cbae":"# We want to look for missing values in addr1 column.\nprint(train['addr1'])\n# We sum the amount of missing values.\nprint(sum(train['addr1'].isnull()))","d35465a9":"# We want to look for missing values in addr1 column.\nprint(train['addr2'])\n# We sum the amount of missing values.\nprint(sum(train['addr2'].isnull()))","21923666":"# In this section of code, we are looking for missing values in the rest of the dataframe.\n\nprint(train['card1'])\nprint(sum(train['card1'].isnull()))","7f5d7a33":"print(train['card2'])\nprint(sum(train['card2'].isnull()))","086ce51a":"print(train['card3'])\nprint(sum(train['card3'].isnull()))","08aa8bd4":"print(train['card4'])\nprint(sum(train['card4'].isnull()))","d2594507":"print(train['card5'])\nprint(sum(train['card5'].isnull()))\n","c22f1061":"print(train['card6'])\nprint(sum(train['card6'].isnull()))","f527927a":"from random import seed\nfrom random import random\n\n# Initialize a network\ndef initialize_network(n_inputs, n_hidden, n_outputs):\n    network = list()\n    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n    network.append(hidden_layer)\n    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n    network.append(output_layer)\n    return network\n\nseed(1)\nnetwork = initialize_network(2,1,2)\nfor layer in network:\n    print(layer)","8c24a7ad":"from math import exp\n\n# Calculate neuron activation for an input\ndef activate(weights, inputs):\n\tactivation = weights[-1]\n\tfor i in range(len(weights)-1):\n\t\tactivation += weights[i] * inputs[i]\n\treturn activation\n\n# Transfer neuron activation\ndef transfer(activation):\n\treturn 1.0 \/ (1.0 + exp(-activation))\n\n# Forward propagate input to a network output\ndef forward_propagate(network, row):\n\tinputs = row\n\tfor layer in network:\n\t\tnew_inputs = []\n\t\tfor neuron in layer:\n\t\t\tactivation = activate(neuron['weights'], inputs)\n\t\t\tneuron['output'] = transfer(activation)\n\t\t\tnew_inputs.append(neuron['output'])\n\t\tinputs = new_inputs\n\treturn inputs\n\n# test forward propagation\nnetwork = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n\t\t[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]]\nrow = [1, 0, None]\noutput = forward_propagate(network, row)\nprint(output)","982dd385":"# Calculate the derivative of an neuron output\ndef transfer_derivative(output):\n\treturn output * (1.0 - output)","44c54748":"error = (expected - output) * transfer_derivative(output)\n\nerror = (weight_k * error_j) * transfer_derivative(output)","b20bf502":"# Backpropagate error and store in neurons\ndef backward_propagate_error(network, expected):\n\tfor i in reversed(range(len(network))):\n\t\tlayer = network[i]\n\t\terrors = list()\n\t\tif i != len(network)-1:\n\t\t\tfor j in range(len(layer)):\n\t\t\t\terror = 0.0\n\t\t\t\tfor neuron in network[i + 1]:\n\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n\t\t\t\terrors.append(error)\n\t\telse:\n\t\t\tfor j in range(len(layer)):\n\t\t\t\tneuron = layer[j]\n\t\t\t\terrors.append(expected[j] - neuron['output'])\n\t\tfor j in range(len(layer)):\n\t\t\tneuron = layer[j]\n\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])","2869f4f9":"# Calculate the derivative of an neuron output\ndef transfer_derivative(output):\n\treturn output * (1.0 - output)\n\n# Backpropagate error and store in neurons\ndef backward_propagate_error(network, expected):\n\tfor i in reversed(range(len(network))):\n\t\tlayer = network[i]\n\t\terrors = list()\n\t\tif i != len(network)-1:\n\t\t\tfor j in range(len(layer)):\n\t\t\t\terror = 0.0\n\t\t\t\tfor neuron in network[i + 1]:\n\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n\t\t\t\terrors.append(error)\n\t\telse:\n\t\t\tfor j in range(len(layer)):\n\t\t\t\tneuron = layer[j]\n\t\t\t\terrors.append(expected[j] - neuron['output'])\n\t\tfor j in range(len(layer)):\n\t\t\tneuron = layer[j]\n\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n\n# test backpropagation of error\nnetwork = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n\t\t[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\nexpected = [0, 1]\nbackward_propagate_error(network, expected)\nfor layer in network:\n\tprint(layer)","4074abad":"weight = weight + learning_rate * error * input","47a1bbe4":"# Update network weights with error\ndef update_weights(network, row, l_rate):\n\tfor i in range(len(network)):\n\t\tinputs = row[:-1]\n\t\tif i != 0:\n\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n\t\tfor neuron in network[i]:\n\t\t\tfor j in range(len(inputs)):\n\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']","66edcd14":"# Train a network for a fixed number of epochs\ndef train_network(network, train, l_rate, n_epoch, n_outputs):\n\tfor epoch in range(n_epoch):\n\t\tsum_error = 0\n\t\tfor row in train:\n\t\t\toutputs = forward_propagate(network, row)\n\t\t\texpected = [0 for i in range(n_outputs)]\n\t\t\texpected[row[-1]] = 1\n\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n\t\t\tbackward_propagate_error(network, expected)\n\t\t\tupdate_weights(network, row, l_rate)\n\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))","c7ceee36":"\n# Test training backprop algorithm\nseed(1)\ndataset = [[2.7810836,2.550537003,0],\n\t[1.465489372,2.362125076,0],\n\t[3.396561688,4.400293529,0],\n\t[1.38807019,1.850220317,0],\n\t[3.06407232,3.005305973,0],\n\t[7.627531214,2.759262235,1],\n\t[5.332441248,2.088626775,1],\n\t[6.922596716,1.77106367,1],\n\t[8.675418651,-0.242068655,1],\n\t[7.673756466,3.508563011,1]]\nn_inputs = len(dataset[0]) - 1\nn_outputs = len(set([row[-1] for row in dataset]))\nnetwork = initialize_network(n_inputs, 2, n_outputs)\ntrain_network(network, dataset, 0.5, 20, n_outputs)\nfor layer in network:\n\tprint(layer)","07e7d167":"\n# Test training backprop algorithm\nseed(1)\ndataset = [[2.7810836,2.550537003,0],\n\t[1.465489372,2.362125076,0],\n\t[3.396561688,4.400293529,0],\n\t[1.38807019,1.850220317,0],\n\t[3.06407232,3.005305973,0],\n\t[7.627531214,2.759262235,1],\n\t[5.332441248,2.088626775,1],\n\t[6.922596716,1.77106367,1],\n\t[8.675418651,-0.242068655,1],\n\t[7.673756466,3.508563011,1]]\nn_inputs = len(dataset[0]) - 1\nn_outputs = len(set([row[-1] for row in dataset]))\nnetwork = initialize_network(n_inputs, 2, n_outputs)\ntrain_network(network, dataset, 0.5, 20, n_outputs)\nfor layer in network:\n\tprint(layer)","b972b4ba":"from math import exp\n\n# Calculate neuron activation for an input\ndef activate(weights, inputs):\n\tactivation = weights[-1]\n\tfor i in range(len(weights)-1):\n\t\tactivation += weights[i] * inputs[i]\n\treturn activation\n\n# Transfer neuron activation\ndef transfer(activation):\n\treturn 1.0 \/ (1.0 + exp(-activation))\n\n# Forward propagate input to a network output\ndef forward_propagate(network, row):\n\tinputs = row\n\tfor layer in network:\n\t\tnew_inputs = []\n\t\tfor neuron in layer:\n\t\t\tactivation = activate(neuron['weights'], inputs)\n\t\t\tneuron['output'] = transfer(activation)\n\t\t\tnew_inputs.append(neuron['output'])\n\t\tinputs = new_inputs\n\treturn inputs\n\n# Make a prediction with a network\ndef predict(network, row):\n\toutputs = forward_propagate(network, row)\n\treturn outputs.index(max(outputs))\n\n# Test making predictions with the network\ndataset = [[2.7810836,2.550537003,0],\n\t[1.465489372,2.362125076,0],\n\t[3.396561688,4.400293529,0],\n\t[1.38807019,1.850220317,0],\n\t[3.06407232,3.005305973,0],\n\t[7.627531214,2.759262235,1],\n\t[5.332441248,2.088626775,1],\n\t[6.922596716,1.77106367,1],\n\t[8.675418651,-0.242068655,1],\n\t[7.673756466,3.508563011,1]]\nnetwork = [[{'weights': [-1.482313569067226, 1.8308790073202204, 1.078381922048799]}, {'weights': [0.23244990332399884, 0.3621998343835864, 0.40289821191094327]}],\n\t[{'weights': [2.5001872433501404, 0.7887233511355132, -1.1026649757805829]}, {'weights': [-2.429350576245497, 0.8357651039198697, 1.0699217181280656]}]]\nfor row in dataset:\n\tprediction = predict(network, row)\n\tprint('Expected=%d, Got=%d' % (row[-1], prediction))","8422f8fb":"**Data Cleaning**\n\nWhen looking at the columns that we are given we can see that there are already binary columns that were in use by a prior computational method for data analysis. (Remove these columns?)\n\n","063c1c0f":"**Data Quality & Preprocessing**\n\nWe would like our data to fulfill the requirements of its' intended use, this would infer that our data is of the highest quality. We are making sure that the data is accurate, consistent, complete, reliable, and interpretable. During preprocessing we will look for noise or inaccuracy within the data. Looking through the data descriptions that were given:\n\n**Transaction Table**\n\n* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n\n* TransactionAMT: transaction payment amount in USD\n\n* ProductCD: product code, the product for each transaction\n\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n\n* addr: address\n\n* dist: distance\n\n* P_ and (R__) emaildomain: purchaser and recipient email domain\n\n* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n\n* D1-D15: timedelta, such as days between previous transaction, etc.\n\n* M1-M9: match, such as names on card and address, etc.\n\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\nCategorical Features:\n\n* ProductCD\n\n* card1 - card6\n\n* addr1, addr2\n\n* Pemaildomain Remaildomain\n\n* M1 - M9\n\n\n**Identity Table**\n\n*Variables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions. \nThey're collected by Vesta\u2019s fraud protection system and digital security partners.\n(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)*\n\n\nCategorical Features:\n* DeviceType\n\n* DeviceInfo\n\n* id12 - id38","aea57381":"**Introduction**\n\nThis kernel seeks to analyze and interpret the IEEE-CIS Fraud Detection Competition Data.\n\n**Mission Statement**\n\nAdvancing nature-inspired computational paradigms in science and engineering. \n\n**Vision**\n\nBe the international leader in innovative interdisciplinary research, technology transfer, real-world applications, and education in Computational Intelligence.\n\n**Field of Interest (FOI)**\n\nThe Field of Interest of the Computational Intelligence Society (CIS) shall be the theory, design, application, and development of biologically and linguistically motivated computational paradigms emphasizing neural networks, connectionist systems, genetic algorithms, evolutionary programming, fuzzy systems, and hybrid intelligent systems in which these paradigms are contained. \n\n*From IEEE-CIS*\n![](https:\/\/cdn.appenresources.com\/wp-content\/uploads\/2018\/03\/Recent-Developments-in-Neural-Networks.png)","edb26251":"After looking at both address columns we can see that there are 65,706 transactions with missing addresses.\n\n\nWe will now want to look at the other columns in the data to see if there are other types of missing values. (It will be interesting to see if there is a correlation between address fields and other columns.)"}}