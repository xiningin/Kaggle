{"cell_type":{"89af0601":"code","c3b00f51":"code","35f96d7e":"code","d1765a21":"code","94bda053":"code","94d78ef1":"code","06109ccc":"code","701b9636":"code","3146bb5e":"code","3b1e5481":"code","f5c0feb5":"code","ba5753b1":"code","5a1eb9ef":"code","1d164830":"code","c02b3228":"code","0b6868d6":"code","91629cd7":"code","ec8ed488":"code","2cb3a0a7":"code","ee7b8f4d":"code","e9a296b8":"code","03348860":"code","f26046bb":"code","73887a25":"code","84e9bb0f":"code","413c37d8":"code","f6e510e7":"code","075ce431":"code","4c8a0015":"code","2f29f9af":"code","740bcbb7":"code","311ed81a":"code","b6bf12af":"code","922fb587":"markdown","6dbaf614":"markdown","8d15a5b9":"markdown","e2809a2b":"markdown","97817442":"markdown","17130644":"markdown","df2f693b":"markdown","cf505a73":"markdown","07027297":"markdown","96957f04":"markdown","43e00e62":"markdown","b1c764d0":"markdown","bcafd8b1":"markdown","830e9b68":"markdown","0068af23":"markdown","fffdad24":"markdown","b0ea2840":"markdown","0c0b1fc9":"markdown","2e36bfcf":"markdown","924ef986":"markdown","1814f8a9":"markdown","4e469d7e":"markdown","ba614d8e":"markdown","e8add3e4":"markdown","5516c56d":"markdown","929f23f4":"markdown","6eede3db":"markdown","8a21795b":"markdown","11aa9aac":"markdown","527bfe0e":"markdown","4155f607":"markdown","45a42442":"markdown","6f2a59fa":"markdown","08060245":"markdown","5f688e4c":"markdown","20ebe96b":"markdown","8d4be0ae":"markdown","42aa3688":"markdown","9f8b220a":"markdown"},"source":{"89af0601":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats","c3b00f51":"df_Vehicle=pd.read_csv('..\/input\/vehicle\/vehicle.csv').dropna()","35f96d7e":"df_Vehicle.tail(5)","d1765a21":"df_Vehicle.isnull().sum()","94bda053":"sns.pairplot(df_Vehicle,diag_kind='kde',hue='class')","94d78ef1":"df_Vehicle['class'].value_counts()","06109ccc":" sns.countplot(x=\"class\", data=df_Vehicle)","701b9636":"df_Vehicle[df_Vehicle.isna()].count()","3146bb5e":"df_Vehicle_numeric_cols=df_Vehicle.select_dtypes(include=[np.number])\nfrom scipy.stats import zscore\ndf_scale=df_Vehicle_numeric_cols.apply(zscore)\ndf_scale","3b1e5481":"cluster_errors = []\nX=np.array(df_scale)","f5c0feb5":"from sklearn.cluster import KMeans\n# Let us check optimal number of clusters-\ncluster_range = range( 1, 10)\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 5,max_iter=100)\n  clusters.fit(X)\n  labels = clusters.labels_                     # capture the cluster lables\n  centroids = clusters.cluster_centers_         # capture the centroids\n  cluster_errors.append( clusters.inertia_ )    # capture the intertia\n# combine the cluster_range and cluster_errors into a dataframe by combining them\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors} )\nclusters_df[0:10]","ba5753b1":"# Elbow plot\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","5a1eb9ef":"#computing of the slope using code \n#slope=error\/cluster\nerrors = clusters_df['cluster_errors']\nfor i in range(8):\n    print(errors[i+1]-errors[i])","1d164830":"# Number of clusters\nkmeans = KMeans(n_clusters=4)\n# Fitting the input data\nkmeans = kmeans.fit(X)\n# Getting the cluster labels\nlabels = kmeans.predict(X)\n# Centroid values\ncentroids = kmeans.cluster_centers_\n# Comparing with scikit-learn centroids\nprint(\"Centroid values\")\nprint(\"sklearn\")\nprint(centroids) # From sci-kit learn","c02b3228":"colnames = df_scale.columns","0b6868d6":"df_centroids=pd.DataFrame(centroids,columns=colnames)","91629cd7":"df_centroids","ec8ed488":"kmeans.labels_","2cb3a0a7":"prediction= kmeans.predict(X)\n#X[\"clusters\"] = prediction\nX_df = pd.DataFrame(X, columns= colnames)\nX_df[\"group\"] = prediction","ee7b8f4d":"X_df.head()","e9a296b8":"sns.pairplot(X_df,diag_kind='kde',hue='group')","03348860":"np.random.seed(101)  # for repeatability of this dataset\na = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])\nb = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,])\nc = np.random.multivariate_normal([10, 20], [[3, 1], [1, 4]], size=[100,])","f26046bb":"a=np.concatenate([a, b, c])\ndf=pd.DataFrame(a)\ndf.head()","73887a25":"df.info()","84e9bb0f":"sns.pairplot(df,diag_kind='kde')","413c37d8":"#observation:\n#1.range=4\n#max peaks for 0's = 2\n#max peaks for 1's = 2","f6e510e7":"from scipy.cluster.hierarchy import dendrogram, linkage","075ce431":"Z = linkage(df, method='ward', metric='euclidean')","4c8a0015":"from scipy.spatial.distance import pdist\nplt.figure(figsize=(18, 16))\nplt.title('Agglomerative Hierarchical Clustering Dendogram')\nplt.xlabel('sample index')\nplt.ylabel('Distance')\ndendrogram(Z,leaf_rotation=90.0,p=25,color_threshold=12,leaf_font_size=10,truncate_mode='level')\nplt.tight_layout()","2f29f9af":"from scipy.spatial.distance import pdist\nplt.figure(figsize=(18, 16))\nplt.title('Agglomerative Hierarchical Clustering Dendogram')\nplt.xlabel('sample index')\nplt.ylabel('Distance')\ndendrogram(Z,leaf_rotation=90.0,p=12,color_threshold=12,leaf_font_size=10,truncate_mode='lastp')\nplt.tight_layout()","740bcbb7":"from scipy.cluster.hierarchy import fcluster\nz=fcluster(Z,t=50,criterion='distance')","311ed81a":"z","b6bf12af":"plt.scatter(df.iloc[:,0],df.iloc[:,1],c=z)","922fb587":"Since the dimensions of the data are not really known to us, it would be wise to standardize the data using z scores before we go for any clustering methods.\nYou can use zscore function to do this","6dbaf614":"Since the variable is categorical, you can use value_counts function","8d15a5b9":"## Hierarchical Clustering ","e2809a2b":"### 10. Use scatter matrix to print all the 3 distributions","97817442":"Use Matplotlib to plot the scree plot - Note: Scree plot plots Errors vs the no of clusters","17130644":"### K - Means Clustering","df2f693b":"### 1. Read the dataset using function .dropna() - to avoid dealing with NAs as of now","cf505a73":"Use ward as linkage metric and distance as Eucledian","07027297":"### Find out the optimal value of K","96957f04":"### 12. Plot the dendrogram for the consolidated dataframe","43e00e62":"### 11. Find out the linkage matrix","b1c764d0":"### 8. Variable creation","bcafd8b1":"https:\/\/docs.scipy.org\/doc\/scipy-0.14.0\/reference\/generated\/scipy.cluster.hierarchy.linkage.html","830e9b68":"### 9. Combine all three arrays a,b,c into a dataframe","0068af23":"### Use matplotlib to visually observe the clusters in 2D space ","fffdad24":"### 13. Recreate the dendrogram for last 12 merged clusters ","b0ea2840":"a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])\nb = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,])\nc = np.random.multivariate_normal([10, 20], [[3, 1], [1, 4]], size=[100,])","0c0b1fc9":"### 14. From the truncated dendrogram, find out the optimal distance between clusters which u want to use an input for clustering data","2e36bfcf":"Hint: Use truncate_mode='lastp' attribute in dendrogram function to arrive at dendrogram ","924ef986":"Iterating values of k from 1 to 10 fit K means model\nUsing inertia","1814f8a9":"### 2. Print\/ Plot the dependent (categorical variable) - Class column","4e469d7e":"optimal value = 4","ba614d8e":"### Use kmeans.labels_ function to print out the labels of the classes","e8add3e4":"### 3. Standardize the data ","5516c56d":"Hint: Use pd.Dataframe function ","929f23f4":"### 5. Calculate errorrs for each K","6eede3db":"### 15. Using this distance measure and fcluster function to cluster the data into 3 different groups","8a21795b":"Optimal distance > 50 for 3 clusters","11aa9aac":"### Check for any missing values in the data ","527bfe0e":"https:\/\/docs.scipy.org\/doc\/numpy-1.15.1\/reference\/generated\/numpy.random.multivariate_normal.html","4155f607":"https:\/\/docs.scipy.org\/doc\/scipy-0.14.0\/reference\/generated\/scipy.cluster.hierarchy.dendrogram.html","45a42442":"For Hierarchical clustering, we will create datasets using multivariate normal distribution to visually observe how the clusters are formed at the end","6f2a59fa":"You can use kmeans.cluster_centers_ function to pull the centroid information from the instance","08060245":"### The data set has information about features of silhouette extracted from the images of different cars\n\nFour \"Corgie\" model vehicles were used for the experiment: a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400 cars. This particular combination of vehicles was chosen with the expectation that the bus, van and either one of the cars would be readily distinguishable, but it would be more difficult to distinguish between the cars.\n\n","5f688e4c":"https:\/\/docs.scipy.org\/doc\/scipy-0.15.1\/reference\/generated\/scipy.cluster.hierarchy.fcluster.html","20ebe96b":"### 7. Store the centroids in a dataframe with column names from the original dataset given ","8d4be0ae":"### 6. Plotting Elbow\/ Scree Plot","42aa3688":"### Assign a dummy array called Cluster_error","9f8b220a":"### Using optimal value of K - Cluster the data. \nNote: Since the data has more than 2 dimension we cannot visualize the data. As an alternative, we can observe the centroids and note how they are distributed across different dimensions"}}