{"cell_type":{"e5d25735":"code","855d8319":"code","72252ffe":"code","5f50523f":"code","2b0e3ccd":"code","4a6fac42":"code","2270c16d":"code","b036491e":"code","1f245a0b":"code","23057682":"code","b43a41b4":"code","72352b00":"code","7f4c9ec2":"code","b3a4fbe7":"code","cb85a5d0":"code","dd741703":"markdown","8f646e41":"markdown","58120d25":"markdown","2079159a":"markdown","c5a4ab13":"markdown","bbe8a85c":"markdown","054edeec":"markdown","81062068":"markdown","7fcf75e3":"markdown","b7301e05":"markdown","021be702":"markdown","6a29ac7b":"markdown","4800efd2":"markdown"},"source":{"e5d25735":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","855d8319":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport itertools\nfrom sklearn.linear_model import PassiveAggressiveClassifier\n\ndf = pd.read_csv('..\/input\/fake_or_real_news.csv') # Load data into DataFrame\ny = df.label\nX_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33,random_state=53)\n","72252ffe":"from sklearn.feature_extraction.text import CountVectorizer\n# list of text documents\ntext = [\"The quick brown fox jumped over the lazy dog.\"]\n# create the transform\nvectorizer = CountVectorizer()\n# tokenize and build vocab\nvectorizer.fit(text)\n# summarize\nprint(vectorizer.vocabulary_)\n# encode document\nvector = vectorizer.transform(text)\n# summarize encoded vector\nprint(vector.shape)\nprint(type(vector))\nprint(vector.toarray())","5f50523f":"# encode another document\ntext2 = [\"the puppy\"]\nvector = vectorizer.transform(text2)\nprint(vector.toarray())","2b0e3ccd":"corpus = [\n'This is the first document.',\n'This document is the second document.',\n'And this is the third one.',\n'Is this the first document?',\n]\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray())  ","4a6fac42":"count_vectorizer = CountVectorizer(stop_words='english')\ncount_train = count_vectorizer.fit_transform(X_train.values)\ncount_test = count_vectorizer.transform(X_test.values)","2270c16d":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# list of text documents\ntext = [\"The quick brown fox jumped over the lazy dog.\",\"The dog.\", \"The fox\"]\n\n# create the transform\nvectorizer = TfidfVectorizer()\n\n# tokenize and build vocab\nvectorizer.fit(text)\n\n# summarize\nprint(\"Vocabulary :- \",vectorizer.vocabulary_)\nprint(\"IDF :- \",vectorizer.idf_)\n\n# encode document\nvector = vectorizer.transform([text[0]])\n\n# summarize encoded vector\nprint(\"Text : \", text[0])\nprint(\"Shape : \",vector.shape)\n\nprint(\"Representation : \", vector.toarray())","b036491e":"# Initialize the `tfidf_vectorizer` \ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7) \n\n# Fit and transform the training data \ntfidf_train = tfidf_vectorizer.fit_transform(X_train) \n\n# Transform the test set \ntfidf_test = tfidf_vectorizer.transform(X_test)\n\nprint(tfidf_test)","1f245a0b":"# Get the feature names of `tfidf_vectorizer` \nprint(tfidf_vectorizer.get_feature_names()[-10:])\n# Get the feature names of `count_vectorizer` \nprint(count_vectorizer.get_feature_names()[0:10])","23057682":"import matplotlib.pyplot as plt\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    See full source and example: \n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","b43a41b4":"clf = MultinomialNB() \nclf.fit(count_train, y_train)\npred = clf.predict(count_test)\nscore = accuracy_score(y_test, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\nprint(cm)\nplot_confusion_matrix(cm, classes=['FAKE', 'REAL'])","72352b00":"clf = MultinomialNB() \nclf.fit(tfidf_train, y_train)\npred = clf.predict(tfidf_test)\nscore = accuracy_score(y_test, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\nprint(cm)\nplot_confusion_matrix(cm, classes=['FAKE', 'REAL'])","7f4c9ec2":"from sklearn.feature_extraction.text import HashingVectorizer\n# list of text documents\ntext = [\"The quick brown fox jumped over the lazy dog.\"]\n# create the transform\nvectorizer = HashingVectorizer(n_features=20)\n# encode document\nvector = vectorizer.transform(text)\n# summarize encoded vector\nprint(vector.shape)\nprint(vector.toarray())","b3a4fbe7":"\nfrom sklearn.feature_extraction.text import HashingVectorizer\n# Initialize the hashing vectorizer\nhashing_vectorizer = HashingVectorizer(stop_words='english',n_features=5000,non_negative=True)\n\n# Fit and transform the training data \nhashing_train = hashing_vectorizer.fit_transform(X_train)\n\n# Transform the test set \nhashing_test = hashing_vectorizer.transform(X_test)\n\nprint(hashing_test)\n\n","cb85a5d0":"clf = MultinomialNB() \nclf.fit(hashing_train, y_train)\npred = clf.predict(hashing_test)\nscore = accuracy_score(y_test, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\nprint(cm)\nplot_confusion_matrix(cm, classes=['FAKE', 'REAL'])","dd741703":"Importantly, the same vectorizer can be used on documents that contain words not included in the vocabulary. These words are ignored and no count is given in the resulting vector.\n\nFor example, below is an example of using the vectorizer above to encode a document with one word in the vocab and one word that is not.","8f646e41":"# Classification using Count Vectorizers Word Vectors","58120d25":"# Using Count Vectorizer on Fake News Dataset","2079159a":"# Drawing Confusion Matrix With TfIdf Vectorizer","c5a4ab13":"# Classification using Hashing Word Vectors","bbe8a85c":"# Understanding CountVectorizer\n\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n\nYou can use it as follows:\n\n* Create an instance of the CountVectorizer class.\n* Call the fit() function in order to learn a vocabulary from one or more documents.\n* Call the transform() function on one or more documents as needed to encode each as a vector.\n\nAn encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.\n\nBecause these vectors will contain a lot of zeros, we call them sparse. Python provides an efficient way of handling sparse vectors in the scipy.sparse package.\n\n","054edeec":"# Using Hashing on Fake News Dataset","81062068":"# Introduction\n\nFake news has become one of the biggest problems of our age. It has serious impact on our online as well as offline discourse. One can even go as far as saying that, to date, fake news poses a clear and present danger to western democracy and stability of the society.\n\nHere, we develop our NLP skills by building word vectors using different approaches and testing classification performance against those.\n","7fcf75e3":"# Classification using TF-IDF Word Vectors","b7301e05":"# Conclusion\n\nThis tutorial compares different NLP feature extraction approaches based on bag of words(word vectors). I hope this becomes a good starting point for those exploring NLP techniques in data science. \n\nNote : Don't forget to upvote the tutorial, if you like it.\n","021be702":"# Hashing\n\nCounts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large.\n\nThis, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms.\n\nA clever work around is to use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks).\n\nThe HashingVectorizer class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed.","6a29ac7b":"# Understanding TfidfVectorizer Using a Simple Example\n\nThe TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents.\n\nThe same create, fit, and transform process is used as with the CountVectorizer.","4800efd2":"# Using Tf-IDF Vectorizer on Fake News Dataset"}}