{"cell_type":{"6230a50c":"code","3d6f5586":"code","e12ff625":"code","e34b04b3":"code","8272840a":"code","acbb2380":"code","c608b14c":"code","3021589e":"code","04f5e9c9":"code","d24467f3":"code","2f5a61cb":"code","cab16a9b":"markdown","354f9f3b":"markdown","c200d7e3":"markdown","508c8c67":"markdown","ddc6abfb":"markdown","4c30c28a":"markdown","b8d22cc3":"markdown","f367b911":"markdown","a71ff01d":"markdown"},"source":{"6230a50c":"!pip install mtcnn                               ","3d6f5586":"from keras.models import load_model\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom mtcnn.mtcnn import MTCNN\nfrom numpy import * \nimport pandas as pd ","e12ff625":"model = load_model('..\/input\/facenet-keras\/facenet_keras.h5')      \n\nprint(model.inputs)                                                \nprint(model.outputs)","e34b04b3":"def get_embedding(model, face_pixels):               \n    # scale pixel values\n    face_pixels = face_pixels.astype('float32')\n    # standardize pixel values across channels (global)\n    mean, std = face_pixels.mean(), face_pixels.std()\n    face_pixels = (face_pixels - mean) \/ std\n    # transform face into one sample\n    samples = expand_dims(face_pixels, axis=0)\n    # make prediction to get embedding\n    yhat = model.predict(samples)\n    return yhat[0]","8272840a":"def extract_face(filename, required_size=(160, 160)): \n    # load image from file\n    image = Image.open(filename)\n    # convert to RGB, if needed\n    image = image.convert('RGB')\n    # convert to array\n    pixels = asarray(image)\n    # create the detector, using default weights\n    detector = MTCNN()\n    # detect faces in the image\n    results = detector.detect_faces(pixels)\n    # extract the bounding box from the first face\n    x1, y1, width, height = results[0]['box']\n    # bug fix\n    x1, y1 = abs(x1), abs(y1)\n    x2, y2 = x1 + width, y1 + height\n    # extract the face\n    face = pixels[y1:y2, x1:x2]\n    # resize pixels to the model size\n    image = Image.fromarray(face)\n    image = image.resize(required_size)\n    face_array = asarray(image)\n    return face_array","acbb2380":"originalface = extract_face('..\/input\/original-image\/billgates2.jpg')\ntestface     = extract_face('..\/input\/sample-image\/billgates1.jpeg')","c608b14c":"plt.imshow(originalface)   #cropped face of original image","3021589e":"plt.imshow(testface)  #cropped face of test image","04f5e9c9":"originalembedding = get_embedding(model,originalface)    \ntestembedding = get_embedding(model,testface)","d24467f3":"dist = linalg.norm(testembedding-originalembedding)    ","2f5a61cb":"print(dist)","cab16a9b":"# **get_embedding()** function will return the 128 bit embeddings. ","354f9f3b":"# Load the model & summarize Input and Output shape.","c200d7e3":"Euclidean Distance between the two embeddings of same person's is relatively small & Big for differnt person's.\n\nBy this idea we can verify that the given images are of same person or different.\n\nthis is called the one shot learning, which means we don't have to train model for every one.   ","508c8c67":"# Exract the faces from Original & test Images.","ddc6abfb":"# Forward propagation to predict embeddings.  ","4c30c28a":"# Welcome to my kernel !!\n> Please give your feedback & **upvote** this kernel if you like it.","b8d22cc3":"#  **extract_face()** function will return the detected cropped face. ","f367b911":" # Install mtcnn for Face Detection","a71ff01d":"# Calculate the Euclidean Distance "}}