{"cell_type":{"5ae0cee5":"code","7bdd0acc":"code","87b5901d":"code","65d319a1":"code","d0cdada2":"code","45c0cb3b":"code","59e3c165":"code","f46506da":"code","3c67d6d8":"code","71c0c243":"code","4a67b594":"code","7b5f8fcc":"code","fa061583":"code","d803d700":"code","0326d912":"code","b9e19132":"code","826ee522":"code","8a6c1911":"code","44acb140":"code","1143f453":"code","8c59b37e":"code","0f4d416a":"code","6f3759ac":"code","bc9572e6":"code","26e2c34c":"markdown","787b1608":"markdown","dc90b8a1":"markdown","eba29c45":"markdown","9b29ceae":"markdown","f63c49bb":"markdown","d5572e49":"markdown","ef875db6":"markdown","7078b4e9":"markdown","0f5c79bb":"markdown","317ae760":"markdown","1a177bbd":"markdown","ed109fad":"markdown","9ce571f4":"markdown","185de116":"markdown","25670e64":"markdown","ecead844":"markdown","521e2617":"markdown","0353d806":"markdown","004ce3a2":"markdown","b3803c62":"markdown","b122af72":"markdown","2fca5845":"markdown"},"source":{"5ae0cee5":"import pandas as pd\n\nmain_file_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv' # this is the path to the Iowa data that you will use\ndata = pd.read_csv(main_file_path)\n\n# Run this code block with the control-enter keys on your keyboard. Or click the blue botton on the left\nprint('Some output from running this cell')","7bdd0acc":"## Print a summary of data\ndata.describe()","87b5901d":"##To check the dimensionality of data\ndata.shape","65d319a1":"## To check all the columns\ndata.columns","d0cdada2":"data.SalePrice.head()","45c0cb3b":"cols_of_interest =[\"SalePrice\",\"SaleCondition\"]\ntwo_cols_of_data = data[cols_of_interest]","59e3c165":"## Displaying 2 cols of data \ntwo_cols_of_data","f46506da":"two_cols_of_data.describe()","3c67d6d8":"y = data.SalePrice","71c0c243":"## FInd all the numerical columns in data\ndata_predictors = list(data.dtypes[data.dtypes == \"int64\"].index)\ndata_predictors = data_predictors[:-1]","4a67b594":"X = data[data_predictors]","7b5f8fcc":"from sklearn.tree import DecisionTreeRegressor","fa061583":"# Define model\nmodel = DecisionTreeRegressor()\n\n#Fit the model\nmodel.fit(X,y)","d803d700":"print(\"Making predictions for the following 5 houses\")\nprint(X.head())\nprint(\"The predicted values are\")\nprint(model.predict(X.head()))\n","0326d912":"from sklearn.metrics import mean_absolute_error","b9e19132":"predicted_prices = model.predict(X)\nmean_absolute_error(y,predicted_prices)","826ee522":"from sklearn.model_selection import train_test_split","8a6c1911":"train_X,test_X,train_y,test_y = train_test_split(X,y,random_state= 564)","44acb140":"#Define Model\nmodel_1 = DecisionTreeRegressor()\n# Fit the model\nmodel_1.fit(train_X,train_y)\n","1143f453":"## Get predicted values\npredicted_prices_1 = model_1.predict(test_X)","8c59b37e":"print(mean_absolute_error(test_y,predicted_prices_1))","0f4d416a":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor","6f3759ac":"## Defined a function to calculate mae\ndef get_mae(max_leaf_nodes, train_X, train_y,test_X, test_y):\n    model_2 = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes,random_state=454)\n    model_2.fit(train_X,train_y)\n    pred_y = model_2.predict(test_X)\n    mae = mean_absolute_error(test_y,pred_y)\n    return mae","bc9572e6":"for max_leaves in [2,3,5,50,150,200,250,300,500,1000,3000,5000,18000,50000]:\n    my_mae = get_mae(max_leaves,train_X,train_y,test_X,test_y)\n    print(\"max leaf nodes : %d \\t and Mean Absolute error : %d\"%(max_leaves,my_mae))","26e2c34c":"The output describes some parameters about the type of model you've built. Don't worry about it for now.\n\nIn practice, you'll want to make predictions for new houses coming on the market rather than the houses we already have prices for. But we'll make predictions for the first rows of the training data to see how the predict function works.","787b1608":"\n**If you have any questions or hit any problems, come to the [Learn Discussion](https:\/\/www.kaggle.com\/learn-forum) for help. **\n\n**Return to [ML Course Index](https:\/\/www.kaggle.com\/learn\/machine-learning)**","dc90b8a1":"# Experimenting With Different Models\nNow that you have a trustworthy way to measure model accuracy, you can experiment with alternative models and see which gives the best predictions. But what alternatives do you have for models?\n\nYou can see in scikit-learn's documentation that the decision tree model has many options (more than you'll want or need for a long time). The most important options determine the tree's depth. Recall from page 2 that a tree's depth is a measure of how many splits it makes before coming to a prediction. This is a relatively shallow tree\n\n","eba29c45":"![image.png](attachment:image.png)\n","9b29ceae":"### Conclusion\nHere's the takeaway: Models can suffer from either:\n\n#### Overfitting:\ncapturing spurious patterns that won't recur in the future, leading to less accurate predictions, or\n#### Underfitting:\nfailing to capture relevant patterns, again leading to less accurate predictions.\nWe use validation data, which isn't used in model training, to measure a candidate model's accuracy. This lets us try many candidate models and keep the best one.\n\nBut we're still using Decision Tree models, which are not very sophisticated by modern machine learning standards.\n\n\n","f63c49bb":"## Example\nThere are a few alternatives for controlling the tree depth, and many allow for some routes through the tree to have greater depth than other routes. But the max_leaf_nodes argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.\n\nWe can use a utility function to help compare MAE scores from different values for max_leaf_nodes:","d5572e49":"## Selecting Multiple Columns\nYou can select multiple columns from a DataFrame by providing a list of column names inside brackets. Remember, each item in that list should be a string (with quotes).","ef875db6":"The prediction error for each house is: \nerror=actual\u2212predicted\n\nSo, if a house cost $150,000 and you predicted it would cost $100,000 the error is $50,000.\n\nWith the MAE metric, we take the absolute value of each error. This converts each error to a positive number. We then take the average of those absolute errors. This is our measure of model quality. In plain English, it can be said as\n\nOn average, our predictions are off by about X\n\nWe first load the Melbourne data and create X and y. That code isn't shown here, since you've already seen it a couple times.\n\nWe then create the Decision tree model with this code:","7078b4e9":"## Of the options listed above , 250 is the optimal number of leaves\n\n","0f5c79bb":"The calculation of mean absolute error in the Melbourne data is","317ae760":"The data is loaded into train_X, val_X, train_y and val_y using the code you've already seen (and which you've already written).\n\nWe can use a for-loop to compare the accuracy of models built with different values for max_leaf_nodes.","1a177bbd":"![image.png](attachment:image.png)","ed109fad":"# Introduction","9ce571f4":"# The Problem with \"In-Sample\" Scores\nThe measure we just computed can be called an \"in-sample\" score. We used a single set of houses (called a data sample) for both building the model and for calculating it's MAE score. This is bad.\n\nImagine that, in the large real estate market, door color is unrelated to home price. However, in the sample of data you used to build the model, it may be that all homes with green doors were very expensive. The model's job is to find patterns that predict home prices, so it will see this pattern, and it will always predict high prices for homes with green doors.\n\nSince this pattern was originally derived from the training data, the model will appear accurate in the training data.\n\nBut this pattern likely won't hold when the model sees new data, and the model would be very inaccurate (and cost us lots of money) when we applied it to our real estate business.\n\nEven a model capturing only happenstance relationships in the data, relationships that will not be repeated when new data, can appear to be very accurate on in-sample accuracy measurements.\n","185de116":"# What is Model Validation\nYou've built a model. But how good is it?\n\nYou'll need to answer this question for almost every model you ever build. In most (though not necessarily all) applications, the relevant measure of model quality is predictive accuracy. In other words, will the model's predictions be close to what actually happens.\n\nSome people try answering this problem by making predictions with their training data. They compare those predictions to the actual target values in the training data. This approach has a critical shortcoming, which you will see in a moment (and which you'll subsequently see how to solve).\n\nEven with this simple approach, you'll need to summarize the model quality into a form that someone can understand. If you have predicted and actual home values for 10000 houses, you will inevitably end up with a mix of good and bad predictions. Looking through such a long list would be pointless.\n\nThere are many metrics for summarizing model quality, but we'll start with one called Mean Absolute Error (also called MAE). Let's break down this metric starting with the last word, error.","25670e64":"There are many ways to select a subset of your data. We'll start with two main approaches:\n\n## Selecting a Single Column\nYou can pull out any variable (or column) with dot-notation. This single column is stored in a Series, which is broadly like a DataFrame with only a single column of data. Here's an example:\n","ecead844":"By convention, this data is called X.","521e2617":"\n## Building Your Model\nYou will use the scikit-learn library to create your models. When coding, this library is written as sklearn, as you will see in the sample code. Scikit-learn is easily the most popular library for modeling the types of data typically stored in DataFrames.\n\n### The steps to building and using a model are:\n\nDefine: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.\nFit: Capture patterns from provided data. This is the heart of modeling.\nPredict: Just what it sounds like\nEvaluate: Determine how accurate the model's predictions are.\n\n\nHere is the example for defining and fitting the model.","0353d806":"## Choosing the Prediction Target\nYou have the code to load your data, and you know how to index it. You are ready to choose which column you want to predict. This column is called the prediction target. There is a convention that the prediction target is referred to as y. Here is an example doing that with the example data.","004ce3a2":"In practice, it's not uncommon for a tree to have 10 splits between the top level (all houses and a leaf). As the tree gets deeper, the dataset gets sliced up into leaves with fewer houses. If a tree only had 1 split, it divides the data into 2 groups. If each group is split again, we would get 4 groups of houses. Splitting each of those again would create 8 groups. If we keep doubling the number of groups by adding more splits at each level, we'll have  210  groups of houses by the time we get to the 10th level. That's 1024 leaves.\n\nWhen we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are quite close to those homes' actual values, but they may make very unreliable predictions for new data (because each prediction is based on only a few houses).\n\nThis is a phenomenon called OVERFITTING, where a model matches the training data almost perfectly, but does poorly in validation and other new data. On the flip side, if we make our tree very shallow, it doesn't divide up the houses into very distinct groups.\n\nAt an extreme, if a tree divides houses into only 2 or 4, each group still has a wide variety of houses. Resulting predictions may be far off for most houses, even in the training data (and it will be bad in validation too for the same reason). When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called UNDERFITTING.\n\nSince we care about accuracy on new data, which we estimate from our validation data, we want to find the sweet spot between underfitting and overfitting. Visually, we want the low point of the (red) validation curve in","b3803c62":"We can verify that we got the columns we need with the describe command.\n","b122af72":"### Continue\nYou've built a decision tree model that can predict the prices of houses based on their characteristics. It's natural to ask how accurate the model's predictions will be, and measuring accuracy is necessary for us to see whether or not other approaches improve our model.\n\nMove on to the next page to see how we measure model accuracy.","2fca5845":"## Example\u00b6\nModels' practical value come from making predictions on new data, so we should measure performance on data that wasn't used to build the model. The most straightforward way to do this is to exclude some data from the model-building process, and then use those to test the model's accuracy on data it hasn't seen before. This data is called validation data.\n\nThe scikit-learn library has a function train_test_split to break up the data into two pieces, so the code to get a validation score looks like this:m"}}