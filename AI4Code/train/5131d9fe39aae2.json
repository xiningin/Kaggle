{"cell_type":{"9046d9d4":"code","9c9b0d4b":"code","1450396f":"code","02d01705":"code","91f94731":"code","d2780c42":"code","119b8e16":"code","8dee1fef":"code","92863b50":"markdown","269a3ac6":"markdown","8541057b":"markdown","7911d2a0":"markdown","5a137dcd":"markdown","c8830983":"markdown","85494c59":"markdown","9e096586":"markdown"},"source":{"9046d9d4":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split ,GridSearchCV,StratifiedKFold,RandomizedSearchCV\nfrom sklearn.metrics import mean_absolute_error , accuracy_score ,confusion_matrix\nprint(\"XGBoost version:\", xgb.__version__)\n","9c9b0d4b":"#get the data\ntest = pd.read_csv('..\/input\/learn-together\/test.csv')\ntrain = pd.read_csv('..\/input\/learn-together\/train.csv')\n\n#prepare the data fir training and Validation\n\nY = train['Cover_Type']\nX = train.copy()\nX.drop(columns = ['Cover_Type'] , inplace=True )\nx_train , x_val , y_train , y_val = train_test_split(X ,Y ,  train_size=0.8, test_size=0.2, \n                                                      random_state=0)\n\n","1450396f":"\n%%time\nmodel = xgb.XGBClassifier(\nlearning_rate =0.1,\nn_estimators=500,\nmax_depth=5,\nmin_child_weight=1,\ngamma=0,\nsubsample=0.8,\ncolsample_bytree=0.8,\nobjective= 'multi:softmax',\nnthread=9,\nscale_pos_weight=1,\nseed=27,\ntree_method='gpu_hist' )\n\n#training\ntrain_model = model.fit(x_train, y_train)\npred = train_model.predict(x_val)\nprint(\"Accuracy for model 3: %.2f\" % (accuracy_score(y_val, pred) * 100))","02d01705":"#Get classification_report ,   \nfrom sklearn.metrics import classification_report\ntarget_names = ['Spruce\/Fir' , 'Lodgepole Pine','Ponderosa Pine','Cottonwood\/Willow','Aspen','Douglas-fir','Krummholz']\nprint( (classification_report(y_val, pred,target_names=target_names)))","91f94731":"#Confusion matrix\nconfusion_matrix(y_val, pred)","d2780c42":"%%time \n\n\nfolds = 2\nparam_comb = 20\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 5, 7, 10],\n        'learning_rate': [0.01, 0.02, 0.05]    \n        }\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nxgbb = XGBClassifier(learning_rate=0.01, n_estimators=250, objective='binary:logistic',\n                    silent=True, nthread=6, tree_method='gpu_hist', eval_metric='auc')\n\nrandom_search = RandomizedSearchCV(xgbb, param_distributions=params, n_iter=param_comb, n_jobs=4, cv=skf.split(x_train,y_train), verbose=3, random_state=1001 )\n\ntrain_model =random_search.fit(x_train,y_train)\npred = train_model.predict(x_val)\n\nprint(\"Accuracy for model is: %.2f\" % (accuracy_score(y_val, pred) * 100))\nprint(train_model.best_params_)\nprint(train_model.best_estimator_)\ncvres = train_model.cv_results_\n\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(mean_score, params)\n","119b8e16":"model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1.0, eval_metric='auc',\n              gamma=2, learning_rate=0.05, max_delta_step=0, max_depth=10,\n              min_child_weight=1, missing=None, n_estimators=250, n_jobs=1,\n              nthread=6, objective='multi:softprob', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=True, subsample=0.8, tree_method='gpu_hist', verbosity=1)\n\n#training\ntrain_model = model.fit(x_train, y_train)\npred = train_model.predict(x_val)\nprint(\"Accuracy for model 3: %.2f\" % (accuracy_score(y_val, pred) * 100))","8dee1fef":"\nsubmit = pd.DataFrame(train_model.predict(test) , columns = [ 'Cover_Type'])\nsubmit['ID']=test['Id']\nsubmit.to_csv('submission.csv' , index=False)\n\n","92863b50":"Model  has a pretty decent performance. However, model seem to struggle predicting Spruce\/Fir and Lodgepole Pine. Models do great on Cottonwood\/Willow and Krummholz. Let's look at the confusion matrix:","269a3ac6":"Let's do a little  Hyperparameter Tunning. ","8541057b":"Preprocessing the data (reading and splitting for training and validation)","7911d2a0":"# Training\n\n#### To activate GPU usage, simply use `tree_method='gpu_hist'. We set a standard model up first. ","5a137dcd":"For Spruce\/Fir and Lodgepole Pine, there seem to be many false predictions. An attempt needs to be made for imporvements on these two categories.","c8830983":"Now that we have good hyperparameters, we train a new model:","85494c59":"# About this kernel\n\n#### I go through various standard techniques using XGBoost using GPU processor, which naturally is much faster than CPU.\n\nThis Kernel uses much material from other kernels, namely \nhttps:\/\/www.kaggle.com\/xhlulu\/ieee-fraud-xgboost-with-gpu-fit-in-40s by @xhlulu \nhttps:\/\/www.kaggle.com\/babatee\/intro-xgboost-classification  by @babatee\nhttps:\/\/www.kaggle.com\/vinhnguyen\/accelerating-hyper-parameter-searching-with-gpu by Vinh Nguyen\n\n### Please enjoy and upvote!\n\n","9e096586":"#### Model was only slightly improved to achieve an accuracy score of 87.53. This is about the extent that XGboost can be improved without making adjustments to the data\/attributes (scaling, encoding, etc)."}}