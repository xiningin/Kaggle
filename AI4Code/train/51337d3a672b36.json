{"cell_type":{"e7a9a72c":"code","4885963a":"code","73d8bb2d":"code","9d5eaf03":"code","29f9467e":"code","36b048cf":"code","8bac6283":"code","1bc24e16":"code","55e72a24":"code","c1abc9b4":"code","34eb410a":"code","96887f00":"code","d013b728":"code","be0fedaf":"code","ab95eb01":"code","302c14d6":"code","e4afd493":"code","3b6d36d3":"code","7db7bfcb":"code","ab5df62f":"code","5c5d01cd":"code","5be4c763":"markdown","6ccf60b5":"markdown","a668f5dd":"markdown","6b2cfd24":"markdown","a598338b":"markdown","7f764c68":"markdown","6a156778":"markdown","6efa16a4":"markdown","6e3a4229":"markdown","9aabf2ed":"markdown","8128e035":"markdown","d7ac0905":"markdown","39869353":"markdown","7736f867":"markdown","66910e20":"markdown"},"source":{"e7a9a72c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nfrom scipy.stats import skew\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4885963a":"#Load data\ndataset = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","73d8bb2d":"dataset.info()","9d5eaf03":"#check for missing data\ndataset.isnull().values.any()","29f9467e":"dataset.nunique().sort_values()","36b048cf":"from sklearn.preprocessing import minmax_scale\n","8bac6283":"features=dataset.columns[:-1]\nbin_features=['anaemia','diabetes',\n        'high_blood_pressure',\n      'sex', 'smoking']\nnum_features=['age', 'creatinine_phosphokinase',\n       'ejection_fraction', 'platelets',\n       'serum_creatinine', 'serum_sodium', 'time']\nlabel='DEATH_EVENT'\nstd_features=[]\nsc_features=[]\nfor feature in num_features:\n    dataset['std_' + feature] = (dataset[feature]-np.mean(dataset[feature])) \/ np.std(dataset[feature])\n    feature_min=dataset[feature].min(axis=0)\n    feature_range=(dataset[feature] - feature_min).max(axis=0)\n    dataset['sc_'+feature] = (dataset[feature] - feature_min)\/feature_range\n    std_features.append('std_' + feature)\n    sc_features.append('sc_' + feature)\ndataset.head(5)","1bc24e16":"fig=plt.figure()\ndata=dataset[label].value_counts()\nx1=data[0]\/(data[0]+data[1])*100\nx2=100-x1\ndata.index=['Survived ('+str(round(x1,2))+'%)','Deceased ('+str(round(x2,2))+'%)']\nax=data.plot.bar(color=['green','red'], rot=0)\nax.set_title('Lethality rate')\nplt.show()\nprint (f'Ower data has a skew of {skew (dataset[label]):0.2f}')","55e72a24":"for feature in std_features:\n    print (feature)\n    dataset_pos=dataset[dataset[label]==1]\n    dataset_neg=dataset[dataset[label]==0]\n    fg,ax = plt.subplots(1)\n    plt.boxplot (dataset_pos[feature],positions = [1], widths = 0.6)\n    plt.boxplot(dataset_neg[feature],positions = [2], widths = 0.6)\n    ax.set_xticklabels(['Deceased', 'Survived'])\n    ax.set_xticks([1, 2])\n    plt.show()\n    ","c1abc9b4":"fig, axes = plt.subplots(len(num_features)\/\/2+1,2,figsize=(20,20))\nax=axes.ravel() # flatten the array\nfor i in range (len(num_features)):\n    _,bins = np.histogram(dataset[num_features].iloc[:,i],bins=30)\n    ax[i].hist(dataset_pos[num_features].iloc[:,i], bins=bins, color='red', alpha=0.5)\n    ax[i].hist(dataset_neg[num_features].iloc[:,i], bins=bins, color='green', alpha=0.5)\n    ax[i].set_title(dataset[num_features].columns[i])\n","34eb410a":"dataset[std_features].iloc[:,1]","96887f00":"pd.plotting.scatter_matrix(dataset[std_features], c=dataset[label],figsize=(20,20), cmap='RdYlGn')\nplt.show()","d013b728":"dataset[std_features+[label]].corr().abs().style.background_gradient()\n","be0fedaf":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, classification_report\nfrom sklearn.svm import SVC\n\n\n","ab95eb01":"my_models=[LinearRegression,LogisticRegression,LinearSVC, DecisionTreeClassifier, RandomForestClassifier,SVC,GradientBoostingClassifier]\nmy_parameters=[{},# Linear Regression\n               {'C':1000,# LogisticRegression\n                'class_weight': {0:33, 1:67},\n               'random_state':0},# that is kind of wierd, I had to swap classes to get better result...\n               {'random_state':0},#LinearSVC\n               {'max_depth':12,#DecisionTreeClassifier\n               'min_samples_split':4,\n               'min_samples_leaf':1,\n                'random_state':0,\n               },\n               {'n_estimators':500,#RandomForestClassifier\n                'min_samples_split':4,\n                'min_samples_leaf':2,\n                'max_features':5,\n                'max_leaf_nodes':None,\n                'min_impurity_decrease':0,\n                'random_state':0,\n                \n               },\n               {'kernel':'sigmoid',#SVC\n               'C':0.5,\n               'random_state':0},\n               {'random_state':0, #GradientBoostingClassifier\n               'learning_rate':0.05}\n              ]","302c14d6":"#First with minimum features, only numeric with higher corr\nmy_features=[]\nfor feature in std_features:\n    if dataset[label].corr(dataset[feature].abs()) > 0.1:\n        my_features.append(feature)\nprint (my_features)\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.loc[:, dataset.columns != label], dataset[label], test_size=0.2, random_state=0)\n\nfor model,parameters in zip (my_models, my_parameters):\n    test_model = model(**parameters).fit(X_train[my_features], y_train)\n    train_prediction = np.round(test_model.predict(X_train[my_features])).astype('int')\n    predictions = np.round(test_model.predict(X_test[my_features])).astype('int')\n    print ('*'*10, test_model, '*'*10)\n    #print(' predicted labels: ', predictions[:30])\n    #print('real labels:       ', y_test[:30].values)\n    mse = mean_squared_error(y_test, predictions)\n    print(\"MSE:\", mse)\n    rmse = np.sqrt(mse)\n    print(\"RMSE:\", rmse)\n    r2 = r2_score(y_test, predictions)\n    print(\"R2:\", r2)\n    acc = accuracy_score(y_train, train_prediction)\n    print (f'Accuracy on train data: {acc:.2f}' )\n    acc = accuracy_score(y_test, predictions)\n    print (f'Accuracy on test data: {acc:.2f}')\n    auc=None\n    try:\n        auc=roc_auc_score(y_test, test_model.predict_proba(X_test[my_features])[:, 1])\n    except Exception as e:\n        print (e)\n        try:\n            auc=roc_auc_score(y_test, test_model.decision_function(X_test[my_features]))\n        except Exception as e:\n            print (e)\n        pass\n        \n    print ('auc score:',auc)\n    print ('-'*20)\n    print (classification_report(y_test,predictions))","e4afd493":"#add binary features\nmy_ext_features = my_features+bin_features\nmy_ext_features","3b6d36d3":"for model,parameters in zip (my_models, my_parameters):\n    test_model = model(**parameters).fit(X_train[my_ext_features], y_train)\n    train_prediction = np.round(test_model.predict(X_train[my_ext_features])).astype('int')\n    predictions = np.round(test_model.predict(X_test[my_ext_features])).astype('int')\n    print ('*'*10, test_model, '*'*10)\n    #print(' predicted labels: ', predictions[:30])\n    #print('real labels:       ', y_test[:30].values)\n    mse = mean_squared_error(y_test, predictions)\n    print(\"MSE:\", mse)\n    rmse = np.sqrt(mse)\n    print(\"RMSE:\", rmse)\n    r2 = r2_score(y_test, predictions)\n    print(\"R2:\", r2)\n    acc = accuracy_score(y_train, train_prediction)\n    print (f'Accuracy on train data: {acc:.2f}' )\n    acc = accuracy_score(y_test, predictions)\n    print (f'Accuracy on test data: {acc:.2f}')\n    #print (roc_auc_score(y_test, model.decision_function(X)))\n    auc=None\n    try:\n        auc=roc_auc_score(y_test, test_model.predict_proba(X_test[my_ext_features])[:, 1])\n    except Exception as e:\n        print (e)\n        try:\n            auc=roc_auc_score(y_test, test_model.decision_function(X_test[my_ext_features]))\n        except Exception as e:\n            print (e)\n        pass\n        \n    print ('auc score:',auc)\n    print ('-'*20)\n    print (classification_report(y_test,predictions))\n","7db7bfcb":"for model,parameters in zip (my_models, my_parameters):\n    test_model = model(**parameters).fit(X_train[features], y_train)\n    train_prediction = np.round(test_model.predict(X_train[features])).astype('int')\n    predictions = np.round(test_model.predict(X_test[features])).astype('int')\n    print ('*'*10, test_model, '*'*10)\n    #print(' predicted labels: ', predictions[:30])\n    #print('real labels:       ', y_test[:30].values)\n    mse = mean_squared_error(y_test, predictions)\n    print(\"MSE:\", mse)\n    rmse = np.sqrt(mse)\n    print(\"RMSE:\", rmse)\n    r2 = r2_score(y_test, predictions)\n    print(\"R2:\", r2)\n    acc = accuracy_score(y_train, train_prediction)\n    print (f'Accuracy on train data: {acc:.2f}' )\n    acc = accuracy_score(y_test, predictions)\n    print (f'Accuracy on test data: {acc:.2f}')\n    #print (roc_auc_score(y_test, model.decision_function(X)))\n    auc=None\n    try:\n        auc=roc_auc_score(y_test, test_model.predict_proba(X_test[features])[:, 1])\n    except Exception as e:\n        print (e)\n        try:\n            auc=roc_auc_score(y_test, test_model.decision_function(X_test[features]))\n        except Exception as e:\n            print (e)\n        pass\n        \n    print ('auc score:',auc)\n    print ('-'*20)\n    print (classification_report(y_test,predictions))","ab5df62f":"# still good results with ensemble models (they can choose the best features), other models clearly needed feature manipulations","5c5d01cd":"X_train, X_test, y_train, y_test = train_test_split(dataset[std_features], dataset[label], test_size=0.2, random_state=1)\nmy_models1=[SVC,RandomForestClassifier,GradientBoostingClassifier]\nmy_parameters1=[{'C':30,\n                'gamma':'scale',\n                'random_state':1},\n                {'n_estimators':500,#RandomForestClassifier\n                'min_samples_split':4,\n                'min_samples_leaf':2,\n                'max_features':5,\n                'max_leaf_nodes':None,\n                'min_impurity_decrease':0,\n                'random_state':1,\n                \n               },\n                {'random_state':1, #GradientBoostingClassifier\n               'learning_rate':0.05}\n              ]\nprint (my_features)\nfor model,parameters in zip (my_models1, my_parameters1):\n    test_model = model(**parameters).fit(X_train[my_features], y_train)\n    train_prediction = np.round(test_model.predict(X_train[my_features])).astype('int')\n    predictions = np.round(test_model.predict(X_test[my_features])).astype('int')\n    print ('*'*10, test_model, '*'*10)\n    #print(' predicted labels: ', predictions[:30])\n    #print('real labels:       ', y_test[:30].values)\n    mse = mean_squared_error(y_test, predictions)\n    print(\"MSE:\", mse)\n    rmse = np.sqrt(mse)\n    print(\"RMSE:\", rmse)\n    r2 = r2_score(y_test, predictions)\n    print(\"R2:\", r2)\n    acc = accuracy_score(y_train, train_prediction)\n    print (f'Accuracy on train data: {acc:.2f}' )\n    acc = accuracy_score(y_test, predictions)\n    print (f'Accuracy on test data: {acc:.2f}')\n    #print (roc_auc_score(y_test, model.decision_function(X)))\n    auc=None\n    try:\n        auc=roc_auc_score(y_test, test_model.predict_proba(X_test[my_features])[:, 1])\n    except Exception as e:\n        print (e)\n        try:\n            auc=roc_auc_score(y_test, test_model.decision_function(X_test[my_features]))\n        except Exception as e:\n            print (e)\n        pass\n        \n    print ('auc score:',auc)\n    print ('-'*20)\n    print (classification_report(y_test,predictions))","5be4c763":"Let's try using all the original features","6ccf60b5":"The dataset data is heavily skewed and the number of observations is quite small. ","a668f5dd":"Let's add more features","6b2cfd24":"Random forest and Gradient Boosting gives higher accuracy and better models according to AUR.","a598338b":"Let's see the distributions of standardized features","7f764c68":"Here are the 3 best models","6a156778":"There is no missing data","6efa16a4":"SVC works pretty well, but requies preprocessing. Random forest works best on the numerical features that have higher correlation with label (others should be excluded ?). Gradient Boosting shows the best result. I could use grid search to improve it even better","6e3a4229":"We can see that we have 5 binary features and 7 numerical. Numerical fields require standardization. ","9aabf2ed":"Let's try some magic","8128e035":"Let's choose for now just features with abs(correlation) > 0.1","d7ac0905":"And repeat with the same models","39869353":"Let's see how evenly distributed dataset we have. Start with the label. What is the proportion between lethal and non-lethal cases?","7736f867":"In creatinine_phosphokinase and serum_creatinine we have lots of outliers.\n","66910e20":"We have 11 features and 1 label"}}