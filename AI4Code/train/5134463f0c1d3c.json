{"cell_type":{"b56f2ab0":"code","a2910108":"code","e84b2d70":"code","4ff89921":"code","537faa71":"code","0e248177":"code","bd8159ce":"code","1e81e64b":"code","2f7f1162":"code","ad745cd2":"code","90372764":"code","430e39e6":"markdown"},"source":{"b56f2ab0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2910108":"import torch\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nimport torch.nn.functional as F\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import preprocessing\n\nrandom.seed(777)\ntorch.manual_seed(777)\ntorch.cuda.is_available()\n\ndevice=torch.device('cuda')\ntorch.cuda.manual_seed_all(777)\nScaler=preprocessing.StandardScaler()","e84b2d70":"train=pd.read_csv(\"train.csv\")\nprint(train[\"PM10\"].value_counts(normalize=True))\n\ntrain['\uc2dc\uac04'] = train['\uc2dc\uac04'].astype(str)\ntrain['\uc2dc\uac04'] = pd.to_datetime(train['\uc2dc\uac04'],format=\"%Y-%m-%d:%H\", errors='ignore')\ntrain=train.set_index('\uc2dc\uac04')\n\nwind=pd.DataFrame({'\ud48d\ud5a5': np.cos(np.pi*train[\"\ud48d\ud5a5\"]\/360)})\ntrain=train.drop(\"\ud48d\ud5a5\",axis=1)\ntrain=pd.concat((train,wind),axis=1)\n\ntrain_x=train[['\uc2b5\ub3c4','\uac15\uc218','\uae30\uc628','\ud48d\uc18d','\ud48d\ud5a5']]\ntrain_y=train[\"PM10\"].replace([\"\uc88b\uc74c\",\"\ubcf4\ud1b5\",\"\ub098\uc068\",\"\ub9e4\uc6b0\ub098\uc068\"], [0,1,2,3])\nscaler=MinMaxScaler()\ntrain_x=Scaler.fit_transform(train_x)\ntrain_x=torch.FloatTensor(train_x).to(device)\ntrain_y=torch.LongTensor(np.array(train_y)).to(device)\nprint(train_x.shape)","4ff89921":"train_dataset=torch.utils.data.TensorDataset(train_x,train_y)\nlearning_rate =0.01\ntraining_epochs = 200\nbatch_size = 200\ndata_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=True,\n                                          drop_last=True)","537faa71":"linear1=torch.nn.Linear(5,4,bias=True)\nlinear2=torch.nn.Linear(4,4,bias=True)\nlinear3=torch.nn.Linear(4,4,bias=True)\nlinear4=torch.nn.Linear(4,4,bias=True)\nlinear5=torch.nn.Linear(4,4,bias=True)\n\nrelu=torch.nn.ReLU()\n","0e248177":"torch.nn.init.xavier_normal_(linear1.weight)\ntorch.nn.init.xavier_normal_(linear2.weight)\ntorch.nn.init.xavier_normal_(linear3.weight)\ntorch.nn.init.xavier_normal_(linear4.weight)\ntorch.nn.init.xavier_normal_(linear5.weight)","bd8159ce":"model=torch.nn.Sequential(linear1,relu,\n                          linear2,relu,\n                          linear3,relu,\n                          linear4,relu,\n                          linear5\n                          ).to(device)","1e81e64b":"loss = torch.nn.CrossEntropyLoss().to(device) # softmax \ub0b4\ubd80\uc801\uc73c\ub85c \uacc4\uc0b0\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) ","2f7f1162":"total_batch = len(data_loader)\nfor epoch in range(training_epochs):\n    avg_cost = 0\n\n    for X, Y in data_loader:\n\n        X = X.to(device)\n        Y = Y.to(device)\n      \n        optimizer.zero_grad()\n        hypothesis = model(X)\n        cost = loss(hypothesis, Y)\n        cost.backward()\n        optimizer.step()\n\n        avg_cost += cost \/ total_batch\n\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n\nprint('Learning finished')","ad745cd2":"with torch.no_grad():\n  \n  test=pd.read_csv(\"test.csv\",index_col = False).drop(\"\uc2dc\uac04\", axis = 1) \n  wind=pd.DataFrame({'\ud48d\ud5a5': np.cos(np.pi*test[\"\ud48d\ud5a5\"]\/360)})\n  test=test.drop(\"\ud48d\ud5a5\",axis=1)\n  test=pd.concat((test,wind),axis=1)\n  test=np.array(test)\n\n  test=Scaler.transform(test)\n  test=torch.FloatTensor(test).to(device)\n\n  hypothesis = model(test)\n  predict1=torch.argmax(hypothesis,dim=1)\n  print(predict1)\n  submission=pd.read_csv('submission.csv', index_col=False,encoding='utf-8-sig')\n  for i in range(len(predict1)):\n    submission['PM10'][i]=int(predict1[i])\n\n  submission['PM10']=submission['PM10'].astype(int)\n  submission[\"PM10\"]=submission[\"PM10\"].replace([0,1,2,3],[\"\uc88b\uc74c\",\"\ubcf4\ud1b5\",\"\ub098\uc068\",\"\ub9e4\uc6b0\ub098\uc068\"])\n\n#Adam \uc0ac\uc6a9test\ub370\uc774\ud130\uc5d0\ub3c4 \uc2a4\ucf00\uc77c\ub7ec \uc0ac\uc6a9#Layer \uc313\uae30","90372764":"#\uc81c\ucd9c\nsubmission.to_csv('baseline.csv',index=False)\n!kaggle competitions submit -c aqiprediction -f baseline.csv -m \"Message\"","430e39e6":"* Layer \uc313\uae30(5\uac1c)\n* \ud65c\uc131\ud568\uc218(relu)\uc0ac\uc6a9\/\ucd5c\uc801\ud654\ud568\uc218 SGD(\uae30\uc874\ubca0\uc774\uc2a4\ub77c\uc778\ucf54\ub4dc)->Adam \uc0ac\uc6a9\n* test data\ub3c4 \uc815\uaddc\ud654"}}