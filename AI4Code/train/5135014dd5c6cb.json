{"cell_type":{"c8247a36":"code","48665f69":"code","74aa0933":"code","20e4f134":"code","3fe9e11d":"code","938c9e00":"code","da6b105a":"code","817f252f":"code","d966705d":"code","91cf1886":"code","8c913f9d":"code","63000aa8":"code","38921467":"code","8b902822":"code","a4b1780d":"code","428ebd12":"code","a5b13b2c":"code","fbd36b76":"code","06b76c42":"code","3683fbfd":"code","b657a4dc":"code","495bb7e2":"code","73100486":"markdown","9ff6df58":"markdown","4afcffcc":"markdown","4e1d836c":"markdown","4af179f4":"markdown","ed928af1":"markdown","178c44d1":"markdown","452d5cdb":"markdown","c1674177":"markdown","d6f049be":"markdown","fcf438d9":"markdown","8658f874":"markdown","8634a5ee":"markdown","48c43c71":"markdown","9aa2e771":"markdown","058ff76c":"markdown","7ba4f6ab":"markdown","4c61c795":"markdown","9dc57de5":"markdown","b9d9ff9e":"markdown","00b32297":"markdown"},"source":{"c8247a36":"%%bash\npip install pytorch-pfn-extras\npip install timm","48665f69":"import os\nimport gc\nimport copy\nimport yaml\nimport random\nimport shutil\nimport typing as tp\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom torch.cuda import amp\n\nimport timm\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport pytorch_pfn_extras as ppe\nfrom pytorch_pfn_extras.config import Config\nfrom pytorch_pfn_extras.training import extensions as ppe_exts, triggers as ppe_triggers\n\nimport cuml\nfrom matplotlib import pyplot as plt\n%matplotlib inline","74aa0933":"ROOT = Path.cwd().parent\nINPUT = ROOT \/ \"input\"\nOUTPUT = ROOT \/ \"output\"\nDATA = INPUT \/ \"seti-breakthrough-listen\"\nTRAIN = DATA \/ \"train\"\nTEST = DATA \/ \"test\"\n\nTRAIN_OUTPUT = INPUT \/ \"seti-e-t-resnet18d-baseline\"\n\nRANDAM_SEED = 1086\nCLASSES = [\"target\",]\nN_CLASSES = len(CLASSES)\nFOLDS = [0, 1, 2, 3, 4]\nN_FOLDS = len(FOLDS)","20e4f134":"train_all = pd.read_csv(DATA \/ \"train_labels.csv\")\ntest = pd.read_csv(DATA \/ \"sample_submission.csv\")","3fe9e11d":"class BasicImageModel(nn.Module):\n    \n    def __init__(\n        self, base_name: str, dims_head: tp.List[int],\n        pretrained=False, in_channels: int=3\n    ):\n        \"\"\"Initialize\"\"\"\n        self.base_name = base_name\n        super(BasicImageModel, self).__init__()\n        \n        # # prepare backbone\n        if hasattr(timm.models, base_name):\n            base_model = timm.create_model(\n                base_name, num_classes=0, pretrained=pretrained, in_chans=in_channels)\n            in_features = base_model.num_features\n            print(\"load imagenet pretrained:\", pretrained)\n        else:\n            raise NotImplementedError\n\n        self.backbone = base_model\n        print(f\"{base_name}: {in_features}\")\n        \n        # # prepare head clasifier\n        if dims_head[0] is None:\n            dims_head[0] = in_features\n\n        layers_list = []\n        for i in range(len(dims_head) - 2):\n            in_dim, out_dim = dims_head[i: i + 2]\n            layers_list.extend([\n                nn.Linear(in_dim, out_dim),\n                nn.ReLU(), nn.Dropout(0.5),])\n        layers_list.append(\n            nn.Linear(dims_head[-2], dims_head[-1]))\n        self.head_cls = nn.Sequential(*layers_list)\n\n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        h = self.backbone(x)\n        h = self.head_cls(h)\n        return h","938c9e00":"FilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]\n\n\nclass SetiSimpleDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset using 6 channels by stacking them along time-axis\n\n    Attributes\n    ----------\n    paths : tp.Sequence[FilePath]\n        Sequence of path to cadence snippet file\n    labels : tp.Sequence[Label]\n        Sequence of label for cadence snippet file\n    transform: albumentations.Compose\n        composed data augmentations for data\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform: A.Compose,\n    ):\n        \"\"\"Initialize\"\"\"\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"Return num of cadence snippets\"\"\"\n        return len(self.paths)\n\n    def __getitem__(self, index: int):\n        \"\"\"Return transformed image and label for given index.\"\"\"\n        path, label = self.paths[index], self.labels[index]\n        img = self._read_cadence_array(path)\n        img = self.transform(image=img)[\"image\"]\n        return {\"image\": img, \"target\": label}\n\n    def _read_cadence_array(self, path: Path):\n        \"\"\"Read cadence file and reshape\"\"\"\n        img = np.load(path)  # shape: (6, 273, 256)\n        img = np.vstack(img)  # shape: (1638, 256)\n        img = img.transpose(1, 0)  # shape: (256, 1638)\n        img = img.astype(\"f\")[..., np.newaxis]  # shape: (256, 1638, 1)\n        return img\n\n    def lazy_init(self, paths=None, labels=None, transform=None):\n        \"\"\"Reset Members\"\"\"\n        if paths is not None:\n            self.paths = paths\n        if labels is not None:\n            self.labels = labels\n        if transform is not None:\n            self.transform = transform\n\n\nclass SetiAObsDataset(SetiSimpleDataset):\n    \"\"\"Use only on-target observation\"\"\"\n\n    def _read_cadence_array(self, path: Path):\n        \"\"\"Read cadence file and reshape\"\"\"\n        img = np.load(path)[[0, 2, 4]]  # shape: (3, 273, 256)\n        img = np.vstack(img)  # shape: (819, 256)\n        img = img.transpose(1, 0)  # shape: (256, 819)\n        img = img.astype(\"f\")[..., np.newaxis]  # shape: (256, 819, 1)\n        return img","da6b105a":"def set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n\n\ndef to_device(\n    tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]],\n    device: torch.device, *args, **kwargs\n):\n    if isinstance(tensors, tuple):\n        return (t.to(device, *args, **kwargs) for t in tensors)\n    elif isinstance(tensors, dict):\n        return {\n            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n    else:\n        return tensors.to(device, *args, **kwargs)","817f252f":"CONFIG_TYPES = {\n    # # utils\n    \"__len__\": lambda obj: len(obj),\n    \"method_call\": lambda obj, method: getattr(obj, method)(),\n\n    # # Dataset, DataLoader\n    \"SetiSimpleDataset\": SetiSimpleDataset,\n    \"SetiAObsDataset\": SetiAObsDataset,\n    \"DataLoader\": torch.utils.data.DataLoader,\n\n    # # Data Augmentation\n    \"Compose\": A.Compose, \"OneOf\": A.OneOf,\n    \"Resize\": A.Resize,\n    \"HorizontalFlip\": A.HorizontalFlip, \"VerticalFlip\": A.VerticalFlip,\n    \"ShiftScaleRotate\": A.ShiftScaleRotate,\n    \"RandomResizedCrop\": A.RandomResizedCrop,\n    \"Cutout\": A.Cutout,\n    \"ToTensorV2\": ToTensorV2,\n\n    # # Model\n    \"BasicImageModel\": BasicImageModel,\n\n    # # Optimizer\n    \"AdamW\": optim.AdamW,\n\n    # # Scheduler\n    \"OneCycleLR\": lr_scheduler.OneCycleLR,\n\n#     # # Loss,Metric\n#     \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss,\n#     \"ROCAUC\": ROCAUC,\n\n#     # # Metric Wrapper\n#     \"micro_average\": micro_average,\n#     \"calc_across_all_batchs\": calc_across_all_batchs,\n\n    # # PPE Extensions\n    \"ExtensionsManager\": ppe.training.ExtensionsManager,\n\n    \"observe_lr\": ppe_exts.observe_lr,\n    \"LogReport\": ppe_exts.LogReport,\n    \"PlotReport\": ppe_exts.PlotReport,\n    \"PrintReport\": ppe_exts.PrintReport,\n    \"PrintReportNotebook\": ppe_exts.PrintReportNotebook,\n    \"ProgressBar\": ppe_exts.ProgressBar,\n    \"ProgressBarNotebook\": ppe_exts.ProgressBarNotebook,\n    \"snapshot\": ppe_exts.snapshot,\n    \"LRScheduler\": ppe_exts.LRScheduler, \n\n    \"MinValueTrigger\": ppe_triggers.MinValueTrigger,\n    \"MaxValueTrigger\": ppe_triggers.MaxValueTrigger,\n    \"EarlyStoppingTrigger\": ppe_triggers.EarlyStoppingTrigger,\n}","d966705d":"cfg_path = TRAIN_OUTPUT \/ \"fold0\" \/\"config.yml\"\nmodel_path = TRAIN_OUTPUT \/ \"best_metric_model_fold0.pth\"\n\nwith open(cfg_path, \"r\") as fr:\n    pre_eval_cfg = yaml.safe_load(fr)\n    \ncfg = Config(pre_eval_cfg, types=CONFIG_TYPES)","91cf1886":"train_all_path_label = {\n    \"paths\": [DATA \/ \"train\" \/ f\"{img_id[0]}\/{img_id}.npy\" for img_id in train_all[\"id\"].values],\n    \"labels\": train_all[CLASSES].values.astype(\"f\")}\ntest_path_label = {\n    \"paths\": [DATA \/ \"test\" \/ f\"{img_id[0]}\/{img_id}.npy\" for img_id in test[\"id\"].values],\n    \"labels\": test[CLASSES].values.astype(\"f\")}\n\ncfg[\"\/dataset\/val\"].lazy_init(**train_all_path_label)\ncfg[\"\/dataset\/test\"].lazy_init(**test_path_label)\n\ntrain_all_loader = cfg[\"\/loader\/val\"]\ntest_loader = cfg[\"\/loader\/test\"]","8c913f9d":"device = torch.device(cfg[\"\/globals\/device\"])\nmodel = cfg[\"\/model\"]\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel = model.to(device)","63000aa8":"def extract_features(model, loader, device):\n    model.eval()\n    emb_list = []\n    pred_list = []\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            x = to_device(batch[\"image\"], device)\n            h = model.backbone(x)  # shape: (bs, 512)\n            y = model.head_cls(h)   # shape: (bs, 1)\n            emb_list.append(h.detach().cpu().numpy())\n            pred_list.append(y.detach().cpu().numpy())\n        \n        emb_arr = np.concatenate(emb_list)\n        pred_arr = np.concatenate(pred_list)\n        del emb_list\n        del pred_list\n    return emb_arr, pred_arr","38921467":"train_emb, train_pred = extract_features(model, train_all_loader, device)","8b902822":"test_emb, test_pred = extract_features(model, test_loader, device)","a4b1780d":"print(train_emb.shape, train_pred.shape)\nprint(test_emb.shape, test_pred.shape)","428ebd12":"del model, train_all_loader, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","a5b13b2c":"all_emb = np.concatenate([train_emb, test_emb], axis=0)\nall_pred = np.concatenate([train_pred, test_pred], axis=0)\nprint(all_emb.shape, all_pred.shape)","fbd36b76":"all_df = pd.concat([train_all, test], axis=0, ignore_index=True)\nall_df[\"target\"].value_counts()","06b76c42":"all_df = pd.concat([train_all, test], axis=0, ignore_index=True)\nall_df[\"data_type\"] = \"\"\nall_df.loc[all_df.target == 1.0, \"data_type\"] = \"train_pos\"\nall_df.loc[all_df.target == 0.0, \"data_type\"] = \"train_neg\"\nall_df.loc[all_df.target == 0.5, \"data_type\"] = \"test\"\nall_df[\"data_type\"].value_counts()","3683fbfd":"tsne = cuml.TSNE(n_components=2, perplexity=10.0)\nall_emb_2d = tsne.fit_transform(all_emb)\n\nneg_emb_2d = all_emb_2d[all_df.query(\"data_type == 'train_neg'\").index.values]\npos_emb_2d = all_emb_2d[all_df.query(\"data_type == 'train_pos'\").index.values]\ntest_emb_2d = all_emb_2d[all_df.query(\"data_type == 'test'\").index.values]","b657a4dc":"fig = plt.figure(figsize=(20,20))\nax_neg = fig.add_subplot(2,2,1)\nax_pos = fig.add_subplot(2,2,2)\nax_posneg = fig.add_subplot(2,2,3)\n\nax_neg.scatter(neg_emb_2d[:, 0],neg_emb_2d[:, 1],color='red',s=10,label='train_non-needles', alpha=0.3)\nax_neg.legend(fontsize=13)\nax_neg.set_title('non-\"needles\" in Train', fontsize=18)\nax_pos.scatter(pos_emb_2d[:, 0],pos_emb_2d[:, 1],color='blue',s=10,label='train_needles', alpha=0.3)\nax_pos.legend(fontsize=13)\nax_pos.set_title('\"needles\" in Train', fontsize=18)\n\nax_posneg.scatter(neg_emb_2d[:, 0],neg_emb_2d[:, 1],color='red',s=10,label='train_non-needles', alpha=0.3)\nax_posneg.scatter(pos_emb_2d[:, 0],pos_emb_2d[:, 1],color='blue',s=10,label='train_needles', alpha=0.3)\nax_posneg.legend(fontsize=13)\nax_posneg.set_title('\"needles\" v.s. non-\"needles\" in Train', fontsize=18)","495bb7e2":"fig = plt.figure(figsize=(20,25))\n\nax_posneg = fig.add_subplot(3,2,1)\nax_test = fig.add_subplot(3,2,2)\nax_negtest = fig.add_subplot(3,2,3)\nax_postest = fig.add_subplot(3,2,4)\nax_all = fig.add_subplot(3,2,5)\n\nax_posneg.scatter(neg_emb_2d[:, 0],neg_emb_2d[:, 1],color='red',s=10, label='train_non-needles', alpha=0.3)\nax_posneg.scatter(pos_emb_2d[:, 0],pos_emb_2d[:, 1],color='blue',s=10, label='train_needles', alpha=0.3)\nax_posneg.legend(fontsize=13)\nax_posneg.set_title('\"needles\" v.s. non-\"needles\" in Train', fontsize=18)\n\nax_test.scatter(test_emb_2d[:, 0],test_emb_2d[:, 1],color='limegreen',s=10, label='test_examples', alpha=0.3)\nax_test.legend(fontsize=13)\nax_test.set_title('examples in Test', fontsize=18)\n\nax_negtest.scatter(test_emb_2d[:, 0],test_emb_2d[:, 1],color='limegreen',s=10, label='test_examples', alpha=0.3)\nax_negtest.scatter(neg_emb_2d[:, 0],neg_emb_2d[:, 1],color='red',s=10, label='train_non-needles', alpha=0.3)\nax_negtest.legend(fontsize=13)\nax_negtest.set_title('non-\"needles\" in Train  v.s. examples in Test', fontsize=18)\n\nax_postest.scatter(test_emb_2d[:, 0],test_emb_2d[:, 1],color='limegreen',s=10, label='test_examples', alpha=0.3)\nax_postest.scatter(pos_emb_2d[:, 0],pos_emb_2d[:, 1],color='blue',s=10, label='train_needles', alpha=0.3)\nax_postest.legend(fontsize=13)\nax_postest.set_title('\"needles\" in Train  v.s. examples in Test', fontsize=18)\n\nax_all.scatter(test_emb_2d[:, 0],test_emb_2d[:, 1],color='limegreen',s=10, label='test_examples', alpha=0.3)\nax_all.scatter(neg_emb_2d[:, 0],neg_emb_2d[:, 1],color='red',s=10, label='train_non-needles', alpha=0.3)\nax_all.scatter(pos_emb_2d[:, 0],pos_emb_2d[:, 1],color='blue',s=10, label='train_needles', alpha=0.3)\nax_all.legend(fontsize=13)\nax_all.set_title('Train v.s. Test', fontsize=18)","73100486":"## Read Data","9ff6df58":"# About\n\nHere I compare distributions of train and test.  \nI use my resnet18d baseline as a feature extractor. Training and inference notebook is here:  \nhttps:\/\/www.kaggle.com\/ttahara\/seti-e-t-resnet18d-baseline\n\nI'm very interested in if the test contains \"unknown messages\".","4afcffcc":"## Define Class, Function","4e1d836c":"## Train vs Test\n\nNext, I compare train examples and test examples. I plot test examples by green.","4af179f4":"# Visualization\n\nNow each image is represented by a point in 512 dimension space. But it is difficult for me to check them directly because I live in 3-dimensional world.\n\nLet's map this 512 dimensional space to 2 dimensions. I use [RAPIDS cuML TSNE]() for dimensionality reduction.","ed928af1":"# Prapere","178c44d1":"### Dataset","452d5cdb":"## Prepare Model, Loader","c1674177":"## Extarct","d6f049be":"## Import","fcf438d9":"To be honest, I was expecting a much different result. \nThis is because I thought that at least part of TestSet would have a completely different distribution than TrainSet, assuming that aliens send \"unknown\" messages.  \nThe plots of CNN embeddings show us that distributions of TrainSet and TestSet are almost the same.\n  \n\nHowever, it should be noted that what I've shown in those plot is distributions of **image embeddings**. \nThere may be **\"unknown messages(needles)\"** hidden in the TestSet.","8658f874":"### Utils","8634a5ee":"## Mapping 512 dim to 2 dim by TSNE","48c43c71":"## Install","9aa2e771":"As you see on a left-bottom plot, most of \"needles\" are neatly separated on the 2D-space. This is why participants can achieve more than 0.95 AUC easily.\n\nFor higher AUC, we need to find a few \"needles\" in the haystack of negative examples.","058ff76c":"# EOF","7ba4f6ab":"# Conclusion\n\nIn this notebook, I visualized cadence snippets on 2D-space utilizing CNN embeddings and TSNE algorithm.\n  \nThere are two main things I've learned: \n1. As you know from CV and LB, positive examples(\"needles\") and negative examples in TrainSet are almost separable in visual analysis.\n2. Distribution of **image embeddings** for TrainSet and TestSet are almost identical.\n\nAlthough it is possible that there are unknown messages hidden in the TestSet, I believe that improving the CV score properly will lead to improving the Private Score.","4c61c795":"## config_types for evaluating configuration\n\nI use [pytorch-pfn-extras](https:\/\/github.com\/pfnet\/pytorch-pfn-extras) for training NNs. This library has useful config systems but requires some preparation.\n\nFor more details, see [docs](https:\/\/github.com\/pfnet\/pytorch-pfn-extras\/blob\/master\/docs\/config.md).","9dc57de5":"# Extract Embeddings (and Prediction)","b9d9ff9e":"## \"needles\" v.s. non-\"needles\" in Train\n\nI'll start from comparing positive examples (called \"needles\") and negative examples.\n\nBellow I plot \"needles\" by blue points and non-\"needles\" by red.","00b32297":"### Model"}}