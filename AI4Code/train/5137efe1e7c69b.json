{"cell_type":{"bc1043f8":"code","f6548a72":"code","80c96a47":"code","627b3df7":"code","6120a574":"code","5fb23701":"code","7766efbd":"code","5541c069":"code","377e5401":"code","30a12755":"code","11a58b8c":"code","efe443a2":"code","5eacb7b6":"code","ece01ef2":"code","9b6d28f9":"code","3adfdf00":"code","d6011321":"code","8d19e4f0":"code","78b87912":"code","59941da0":"code","12869f81":"code","240354b4":"code","35edd67a":"code","09ef2cbc":"code","3cfa7a9b":"code","320befd1":"code","7a1e9b8f":"code","16d51fea":"code","f21b31a1":"code","84155680":"code","f3f9b374":"code","778e3663":"code","071d4e92":"markdown","b5215c0f":"markdown","72bf1b82":"markdown","c4c7b885":"markdown","9a889bd9":"markdown","305b6588":"markdown","f092b3ac":"markdown","4ee2b901":"markdown","1657bea4":"markdown","bb667657":"markdown","e28ac186":"markdown","fc7477a0":"markdown","7d4c02f9":"markdown","7ef7c755":"markdown","e6060c62":"markdown","508b9768":"markdown","2725b4c2":"markdown"},"source":{"bc1043f8":"#sklearn\nfrom sklearn.metrics import roc_curve,auc,classification_report,precision_score,recall_score,f1_score,plot_confusion_matrix,confusion_matrix,make_scorer\nfrom sklearn.pipeline import make_pipeline,Pipeline\nfrom sklearn.preprocessing import FunctionTransformer,LabelEncoder,OneHotEncoder,StandardScaler,QuantileTransformer\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV,GridSearchCV\nfrom sklearn.utils import shuffle,resample\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import NearestNeighbors\n\n#plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\n\n\nimport  numpy as np\nimport pandas as pd\nimport scipy\nfrom copy import deepcopy\nimport itertools\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f6548a72":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","80c96a47":"df=df.drop(['id'],axis=1)\n\ndf=df.drop([df[df.gender=='Other'].index][0],axis=0)","627b3df7":"df.stroke.value_counts()","6120a574":"df.bmi.isna().value_counts()","5fb23701":"missing=df[df.bmi.isna()==True]\nmissing_no_con=missing.drop(['age','bmi','avg_glucose_level'],axis=1)\n\n\nno_missing=df[df.bmi.isna()==False]\nno_missing_no_con=no_missing.drop(['age','bmi','avg_glucose_level'],axis=1)","7766efbd":"comb=[]\n\ngroups={}\n\nfor f in missing_no_con.columns:\n  comb.append(tuple(np.unique(df[f])))\n\ncomb=list(itertools.product(comb[0],comb[1],comb[2],comb[3],comb[4],comb[5],comb[6],comb[7]))\n\n\nfor idx,c in enumerate(comb):\n  get=missing_no_con[(missing_no_con[missing_no_con.columns[0]]==c[0])&\n               (missing_no_con[missing_no_con.columns[1]]==c[1])&\n               (missing_no_con[missing_no_con.columns[2]]==c[2])&\n               (missing_no_con[missing_no_con.columns[3]]==c[3])&\n               (missing_no_con[missing_no_con.columns[4]]==c[4])&\n               (missing_no_con[missing_no_con.columns[5]]==c[5])&\n               (missing_no_con[missing_no_con.columns[6]]==c[6])&\n               (missing_no_con[missing_no_con.columns[7]]==c[7])]\n  \n  if len(get.index)!=0:\n    groups[c]=list(get.index)","5541c069":"std=StandardScaler()\nnn=NearestNeighbors(n_neighbors=1)\nfor k,v in groups.items():\n  get=no_missing_no_con[(no_missing_no_con[no_missing_no_con.columns[0]]==k[0])&\n               (no_missing_no_con[no_missing_no_con.columns[1]]==k[1])&\n               (no_missing_no_con[no_missing_no_con.columns[2]]==k[2])&\n               (no_missing_no_con[no_missing_no_con.columns[3]]==k[3])&\n               (no_missing_no_con[no_missing_no_con.columns[4]]==k[4])&\n               (no_missing_no_con[no_missing_no_con.columns[5]]==k[5])&\n               (no_missing_no_con[no_missing_no_con.columns[6]]==k[6])&\n               (no_missing_no_con[no_missing_no_con.columns[7]]==k[7])]\n  index=list(get.index)  \n  if len(index)!=0: \n    neighbors=df.loc[index][['age','avg_glucose_level']]\n    neighbors=pd.DataFrame(std.fit_transform(neighbors),columns=neighbors.columns,index=neighbors.index)\n    nn.fit(neighbors)\n    for idx in v:\n      target=missing.loc[[idx]]\n      target=target[['age','avg_glucose_level']] \n      nbrs_idx=nn.kneighbors(std.fit_transform(target),1,return_distance=False)\n      nbrs_idx=neighbors.index[nbrs_idx[0][0]]\n      df.at[idx,'bmi']=np.array(df.bmi.loc[[nbrs_idx]])[0]","377e5401":"df.bmi.isna().value_counts()","30a12755":"df['bmi']=df.bmi.fillna(df.bmi.median())","11a58b8c":"df.bmi.isna().value_counts()","efe443a2":"fig,ax=plt.subplots(ncols=3,figsize=(40,10))\n\nfor i,f in enumerate(df[['age','avg_glucose_level','bmi']].columns):\n  sns.histplot(data=df,x=f,hue='stroke',ax=ax[i])","5eacb7b6":"fig,ax=plt.subplots(ncols=3,figsize=(40,10))\ndf_=deepcopy(df)\n\ndf_=np.log(df_[['age','avg_glucose_level','bmi']])\n\ndf_['stroke']=df.stroke\nfor i,f in enumerate(df[['age','avg_glucose_level','bmi']].columns):\n  sns.histplot(data=df_,x=f,hue='stroke',ax=ax[i])","ece01ef2":"\nfig,ax=plt.subplots(ncols=len(df.columns)-4,figsize=(40,10))\nfor i,f in enumerate(df.drop(['stroke','bmi','age','avg_glucose_level'],axis=1).columns):\n  sns.boxplot(data=df,x=f,y='bmi',hue='stroke',ax=ax[i])","9b6d28f9":"fig,ax=plt.subplots(ncols=len(df.columns)-4,figsize=(40,10))\nfor i,f in enumerate(df.drop(['stroke','bmi','age','avg_glucose_level'],axis=1).columns):\n  sns.boxplot(data=df,x=f,y='avg_glucose_level',hue='stroke',ax=ax[i])","3adfdf00":"from scipy.stats import chi2_contingency,f,ttest_ind","d6011321":"p_vals=[]\nfor f in df.drop(['age','bmi','avg_glucose_level','stroke'],axis=1).columns:\n  obs=pd.crosstab(df[f],df.stroke)\n  chi2, p, dof, ex=chi2_contingency(obs)\n  if p<=0.05:\n    print(f'({f} and Stroke)--- Reject--- independence Assumption , p-val :{p}')\n  else:\n    print(f'({f} and Stroke)---Accept--- independence Assumption , p-val :{p}')\n\n  p_vals.append(p)","8d19e4f0":"plt.barh(df.drop(['age','bmi','avg_glucose_level','stroke'],axis=1).columns,-np.log(p_vals))\nplt.title('-log(p value)')\nplt.show()","78b87912":"df_=deepcopy(df)\n\ndf_=np.log(df_[['age','avg_glucose_level','bmi']])\n\ndf_['stroke']=df.stroke","59941da0":"#avg glucose level\n\n\nstroke=df_[df_.stroke==1]['avg_glucose_level']\n\nno_stroke=df_[df_.stroke==0]['avg_glucose_level']\n\n\n\nF=np.var(stroke)\/np.var(no_stroke)\n\ndf1=len(stroke)\ndf2=len(no_stroke)\n\n\nalpha = 0.05 \np_value = scipy.stats.f.sf(F, df1, df2)\nprint('p:',p_value)\nif p_value > alpha:\n\n  print('accept VarX=VarY')\n  t,p=ttest_ind(stroke,no_stroke,equal_var=True)\nelse:\n  print('reject VarX=VarY')\n  t,p=ttest_ind(stroke,no_stroke,equal_var=False)\n\n\nif p\/2<alpha:\n    print(f't-test p :{p\/2} , Reject H0')\nelse:\n    print(f't-test p :{p\/2} , Accept H0')","12869f81":"#bmi\n\n\nstroke=df_[df_.stroke==1]['bmi']\n\nno_stroke=df_[df_.stroke==0]['bmi']\n\n\nF=np.var(stroke)\/np.var(no_stroke)\n\ndf1=len(stroke)\ndf2=len(no_stroke)\n\n\nalpha = 0.05 \np_value = scipy.stats.f.sf(F, df1, df2)\nprint('p:',p_value)\nif p_value > alpha:\n\n  print('accept VarX=VarY')\n  t,p=ttest_ind(stroke,no_stroke,equal_var=True)\nelse:\n  print('reject VarX=VarY')\n  t,p=ttest_ind(stroke,no_stroke,equal_var=False)\n    \n\nif p\/2<alpha:\n    print(f't-test p :{p\/2} , Reject H0')\nelse:\n    print(f't-test p :{p\/2} , Accept H0')","240354b4":"class Augmentor:\n  def __init__(self,degree=2,\n        interaction_only=False,\n        use_ohe=True,\n        assign_drop=None):\n    self.Label_Enc={}\n    self.OneHot_Enc={}\n    self.Poly=None\n    self.degree=degree\n    self.interaction_only=interaction_only\n    self.use_ohe=use_ohe\n    self.assign_drop=assign_drop\n    self.continuous=['age','avg_glucose_level','bmi']\n\n    try:\n      for x in self.continuous:\n        if x in self.assign_drop:\n          self.continuous.remove(x)\n    except:\n      None\n\n  def __call__(self,X,y=None):\n    X_=deepcopy(X) \n\n    if self.use_ohe:\n      X_=self.one_hot_encoder(X_)\n\n    #call by address, no return \n    self.label_encoder(X_)\n    if self.degree>1:\n      X_=self.polynomial(X_)\n\n    if self.assign_drop!=None:\n      X_=X_.drop(self.assign_drop,axis=1)\n    return X_\n\n  def one_hot_encoder(self,X):\n    for f in X.columns:\n      if f in ['work_type','smoking_status']:\n        try:\n          col_names=self.OneHot_Enc[f].get_feature_names()\n        except:\n          self.OneHot_Enc[f]=OneHotEncoder()\n          self.OneHot_Enc[f].fit(X[f].values.reshape(-1,1))\n          col_names=self.OneHot_Enc[f].get_feature_names()\n        X[col_names]=self.OneHot_Enc[f].transform(X[f].values.reshape(-1,1)).toarray()\n\n    try:\n      X=X.drop(['work_type','smoking_status'],axis=1)\n    except:\n      pass\n    return X\n    \n\n  def label_encoder(self,X):\n    '''\n    input : Dataframe\n    '''\n    for f in X.columns:\n      if f not in self.continuous:\n        try:\n          self.Label_Enc[f].classes_\n          X[f]=self.Label_Enc[f].transform(X[f])\n        except:\n          self.Label_Enc[f]=LabelEncoder()\n          X[f]=self.Label_Enc[f].fit_transform(X[f])\n\n\n  def polynomial(self,X):\n    if self.Poly==None:\n      self.Poly=PolynomialFeatures(degree=self.degree,interaction_only=self.interaction_only,include_bias=False)\n      self.Poly.fit(X[self.continuous])\n    col_names=self.Poly.get_feature_names()\n    X[col_names]=self.Poly.transform(X[self.continuous])\n    try:\n      for x in self.continuous:\n        X=X.drop([x],axis=1)\n    except:\n      pass\n    return X\n\n","35edd67a":"def scaled_f1(y_true,y_pred):\n    C=confusion_matrix(y_true,y_pred).astype('float32')\n    C[0,:]=C[0,:]\/C[0,:].sum()\n    C[1,:]=C[1,:]\/C[1,:].sum()\n    TP=C[1][1]\n    FP=C[0][1]\n    TN=C[0][0]\n    FN=C[1][0]\n    precision=TP\/(TP+FP)\n    recall=TP\/(TP+FN)\n    f1=2*precision*recall\/(precision+recall)\n    return f1","09ef2cbc":"X_train,X_test,y_train,y_test=train_test_split(df.drop(['stroke'],axis=1),df.stroke,test_size=0.3,random_state=0,shuffle=True)","3cfa7a9b":"lr=make_pipeline(FunctionTransformer(Augmentor(degree=3,interaction_only=True)),\n                 StandardScaler(),SGDClassifier(loss='log',average=True,\n                 max_iter=200,\n                 early_stopping=True,random_state=0))\n\n\nparams={'sgdclassifier__alpha':[1e-4,1e-3,0.01],\n    'sgdclassifier__class_weight':[{0:1,1:10},{0:1,1:15}],\n     'sgdclassifier__penalty':['elasticnet'],\n      'sgdclassifier__l1_ratio':[0,0.15,0.5,1]}\n\nscorer=make_scorer(scaled_f1)\n\nclf=GridSearchCV(lr,params,scoring=scorer,n_jobs=-1,cv=30,verbose=0)\n\nclf=clf.fit(X_train,y_train)","320befd1":"clf.best_params_","7a1e9b8f":"y_pred=clf.predict(X_train)\n\nprint(classification_report(y_train,y_pred))","16d51fea":"y_pred=clf.predict(X_test)\n\nprint(classification_report(y_test,y_pred))","f21b31a1":"plot_confusion_matrix(clf,X_test,y_test)\nplt.show()","84155680":"C=confusion_matrix(y_test,y_pred).astype('float32')\n\nC[0,:]=C[0,:]\/C[0,:].sum()\nC[1,:]=C[1,:]\/C[1,:].sum()\n\nC","f3f9b374":"TP=C[1][1]\nFP=C[0][1]\n\nTN=C[0][0]\n\nFN=C[1][0]\n\nprecision=TP\/(TP+FP)\n\nrecall=TP\/(TP+FN)\n\nf1=2*precision*recall\/(precision+recall)\n\nprint('test precision:',precision)\nprint('test recall',recall)\nprint('test f1-score :' ,f1)","778e3663":"plt.figure(figsize=(12,12))\nplt.barh(clf.best_estimator_.steps[0][1].transform(df.drop(['stroke'],axis=1)).columns,\n         clf.best_estimator_.steps[2][1].coef_[0])\nplt.title('Regression Weight')\nplt.show()","071d4e92":"* Custom Scorer\n\n* Since data is imbalance , we need to measure by percentage rather than sample number\n\n  * How much percentage Negative I predict correstly ","b5215c0f":"* score\n  ","72bf1b82":"* Chi-squared independence test\n  * Test whether categorical variable indepent to whether stroke","c4c7b885":"* BMI","9a889bd9":"* Model interpretation (fix other variable)\n\n* For Logistic Regression\n\n  * if age +1 , log odds ratio +13\n  \n  * if childen +1 , log odss ratio -8","305b6588":"* Testing Set Score","f092b3ac":"* Training Set score","4ee2b901":"### Modeling","1657bea4":"### Augment Function","bb667657":"* glucose level","e28ac186":"### Missing Data","fc7477a0":"### Statistical Testing","7d4c02f9":"* Encode Categorical \n * If more than 2 categorical then use one-hot encoding\n * else use label encoding\n\n* Continuous\n * Polynomial Feature with only interaction","7ef7c755":"* Confusion Matrix","e6060c62":"* 70% Training , 30% Testing\n\n* Don't use resampling before because the evaluation has no interpretation \n  * 95% precision recall is fake because model has seen these \"copied\" data in training set \n  \n  \n * Regularzation\n \n   * elastic net for model selection purpose\n     * L1+L2 term : since I use polynomial feature and expand categorical by one hot encoding , feature number is large , so I use L1 term for feature selection and L2 term for feature weighting\n     \n   \n   * class weight for imbalance data \n   \n   \n* Tuning \n\n  * $\\alpha$ : streghth of regularzation term\n  \n  * l1-ratio $\\lambda$ : $\\frac{(1-\\lambda)}{2}*L2+\\frac{\\lambda}{2}*L1$ \n    * If $\\lambda$=0 , only L2 term\n    \n    * If $\\lambda$=1 , only L1 term\n    \n  * class weight\n  \n  * 30-Fold Cross Validation","508b9768":"* two sample t-test\n  * Test whether stroke guy's bmi or glucose level higher than the no stroke ","2725b4c2":"### Visualize "}}