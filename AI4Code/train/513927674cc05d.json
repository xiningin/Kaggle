{"cell_type":{"5c3bf946":"code","49cf7951":"code","28b8372b":"code","3e34e8a7":"code","667c31f7":"code","925dcb42":"code","9f0aba7f":"code","cdc938f7":"code","ea9f88a3":"code","3f8d1649":"code","fb11425f":"code","9404064b":"code","510a17e2":"code","601615ec":"code","452ea6f2":"code","3fa89548":"code","01a52268":"code","09706410":"code","95dd4ab7":"code","4942900c":"code","f1bc002b":"code","afbe911c":"code","6ae2dca8":"code","8ac83281":"code","538d94a9":"code","22a7ee87":"code","412379fe":"code","9583fa3f":"code","c22db4ab":"code","d8187661":"code","690b5224":"code","8857454b":"code","7dbd4f06":"code","8cbfd5dc":"code","c64f02d4":"code","d834c713":"code","2452197b":"code","2a6af774":"code","47d3bf26":"code","ad5e9d38":"code","baa0f8aa":"code","dbb7e7ca":"code","ca9bcd91":"code","926b991e":"code","c27cc3cc":"markdown","5c88684e":"markdown","bd004f8e":"markdown","7b567295":"markdown","9abed361":"markdown","4aa7f66f":"markdown","c87e5309":"markdown","89031559":"markdown","8fb66740":"markdown","11c3445a":"markdown","e2d7a4e3":"markdown","00158062":"markdown","95589fce":"markdown","8149b8d8":"markdown","36e71b19":"markdown","e0079934":"markdown","2b4ddc57":"markdown","bf59e8e5":"markdown","d203196d":"markdown","d0ab032e":"markdown","21421e0c":"markdown","35bff39b":"markdown","824f1665":"markdown"},"source":{"5c3bf946":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb","49cf7951":"df = pd.read_csv('..\/input\/breast-cancer-csv\/breastCancer.csv')\ndf.head(5)","28b8372b":"df.shape","3e34e8a7":"df.info()","667c31f7":"df = df.drop(columns = ['id'])","925dcb42":"df['bare_nucleoli'].value_counts()","9f0aba7f":"df_absent = df[df['bare_nucleoli']=='?']\ndf_absent = df_absent.reset_index()\ndf_absent = df_absent.drop(columns=['index'])\ndf_absent","cdc938f7":"df_present = df[df['bare_nucleoli']!='?']\ndf_present = df_present.reset_index()\ndf_present = df_present.drop(columns=[\"index\"])\ndf_present = df_present.astype(np.float64)\ndf_present.head()","ea9f88a3":"df_present_temp = df_present.drop(columns=['bare_nucleoli'])\nxm = df_present_temp.values\nym = df_present['bare_nucleoli'].values\n\nfrom sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(xm, ym, test_size=0.2, random_state=4)\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport matplotlib.pyplot as plt\n\nk_min = 2\ntest_MAE_array = []\nk_array = []\nMAE = 2\n\nfor k in range(2, 20):\n    model = KNeighborsRegressor(n_neighbors=k).fit(train_x, train_y)\n    \n    y_predict = model.predict(test_x)\n    y_true = test_y\n\n    test_MAE = mean_absolute_error(y_true, y_predict)\n    if test_MAE < MAE:\n        MAE = test_MAE\n        k_min = k\n\n    test_MAE_array.append(test_MAE)\n    k_array.append(k)\n\nplt.plot(k_array, test_MAE_array,'r')\nplt.show()\nprint(\"Best k parameter is \",k_min )","3f8d1649":"final_model = KNeighborsRegressor(n_neighbors=16).fit(xm,ym)\n\ndf_absent_temp = df_absent.drop(columns=['bare_nucleoli'])\ndf_absent_temp = df_absent_temp.astype(np.float64)\ndf_absent_temp.head()","fb11425f":"x_am = df_absent_temp.values\ny_am = final_model.predict(x_am)\ny_am","9404064b":"y_am = np.round(y_am)\ny_am = y_am.astype(np.int64)\ny_am","510a17e2":"df_pred = pd.DataFrame({'bare_nucleoli':y_am})\ndf_pred","601615ec":"data_frame1 =  df_absent_temp.join(df_pred)\ndata_frame1 = data_frame1.astype(np.int64)\ndata_frame1","452ea6f2":"df_join_2 = df_present['bare_nucleoli']\ndata_frame_2 = df_present_temp.join(df_join_2)\ndata_frame_2 = data_frame_2.astype(np.int64)\ndata_frame_2.head()","3fa89548":"dataset = [data_frame1, data_frame_2]\ndataset = pd.concat(dataset)\ndataset.head()","01a52268":"dataset.columns","09706410":"cols = ['clump_thickness', 'size_uniformity', 'shape_uniformity',\n       'marginal_adhesion', 'epithelial_size', 'bland_chromatin',\n       'normal_nucleoli', 'mitoses', 'bare_nucleoli', 'class']\ndataset = dataset[cols]\ndataset.head()","95dd4ab7":"from sklearn.preprocessing import LabelEncoder\nlabelencoder_Y = LabelEncoder()\ndataset.iloc[:,9] = labelencoder_Y.fit_transform(dataset.iloc[:,9].values)\ndataset.head()","4942900c":"dataset['class'].value_counts()","f1bc002b":"count_plot = dataset.iloc[:,9]\nsb.countplot(count_plot)\nplt.show()","afbe911c":"fig, ax = plt.subplots(1,1)\nax.pie(dataset['class'].value_counts(),autopct='%1.1f%%', labels=['Class = 0','Class = 1'])\nplt.axis = 'equal'","6ae2dca8":"for i in range(9):\n    column = dataset.iloc[:,i]\n    graph = pd.crosstab(column,dataset['class'])\n    graph.plot.bar(stacked=True)\n    plt.show()","8ac83281":"for i in range(9):\n    distribution = dataset.iloc[:,i]\n    graph = sb.displot(distribution)\n    plt.show()","538d94a9":"for i in range(9):\n    x = dataset.iloc[:,i]\n    for j in range(i+1,9):\n        y = dataset.iloc[:,j]\n        hue_parameter = dataset['class']\n        ax = sb.scatterplot(x=x, y=y, hue=hue_parameter)\n        plt.show()","22a7ee87":"dataset.iloc[:,0:10].corr()","412379fe":"plt.figure(figsize = (10,10))\nsb.heatmap(dataset.iloc[:,0:10].corr(), annot = True,fmt = '.0%')\nplt.show()","9583fa3f":"data = pd.melt(dataset,id_vars=\"class\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,5))\nsb.boxplot(x=\"features\", y=\"value\", hue=\"class\", data=data)\nplt.xticks(rotation=90)","c22db4ab":"X = dataset.drop(columns='class').values\nY = dataset['class'].values\nfrom sklearn.model_selection import train_test_split\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=0.2, random_state=0)","d8187661":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnb_classifier = GaussianNB()\nnum_folds = 10\nkfold = KFold(n_splits=num_folds)\ncv_results = cross_val_score(nb_classifier, X_Train, Y_Train, cv=kfold, scoring='accuracy')\nprint('Naive Bayes Accuracy on Training Data after 10 Fold Cross Validation is :',cv_results.mean())\nprint()\nnb_classifier.fit(X_Train, Y_Train)\nY_Pred_nb = nb_classifier.predict(X_Test)\ncm_nb = confusion_matrix(Y_Test, Y_Pred_nb)\nprint(cm_nb)\nprint()\nTP_nb = cm_nb[0][0]\nFP_nb = cm_nb[0][1]\nTN_nb = cm_nb[1][1]\nFN_nb = cm_nb[1][0]\n    \nprint('Success Rate = ',(TP_nb+TN_nb)\/(TP_nb+TN_nb+FN_nb+FP_nb))\nprint('Misclassificate Rate = ',(FP_nb+FN_nb)\/(TP_nb+TN_nb+FN_nb+FP_nb))\nprint('Sensitivity\/tp_rate = ', TP_nb\/(TP_nb+FN_nb))\nprint('Specificity\/tn_rate = ', TN_nb\/(TN_nb+FP_nb))\nprint('fp rate = ',FP_nb\/(TN_nb+FP_nb))\nprint('fn rate = ',FN_nb\/(TP_nb+FN_nb))","690b5224":"k_min = 2\ntest_MAE_array = []\nk_array = []\nMAE = 1\n\nfor k in range(2, 20):\n    model = KNeighborsRegressor(n_neighbors=k, metric = 'minkowski').fit(X_Train, Y_Train)\n    Predict_Y = model.predict(X_Test)\n    True_Y = Y_Test\n    test_MAE = mean_absolute_error(True_Y, Predict_Y)\n    if test_MAE < MAE:\n        MAE = test_MAE\n        k_min = k\n    test_MAE_array.append(test_MAE)\n    k_array.append(k)\nplt.plot(k_array, test_MAE_array,'r')\nplt.show()\nprint(\"Best k parameter is \",k_min )","8857454b":"from sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier(n_neighbors = 5,metric='minkowski')\nnum_folds = 10\nkfold = KFold(n_splits=num_folds)\ncv_results = cross_val_score(knn_classifier, X_Train, Y_Train, cv=kfold, scoring='accuracy')\nprint('K-Nearest Neighbors Accuracy on Training Data after 10 Fold Cross Validation is :',cv_results.mean())\nknn_classifier.fit(X_Train, Y_Train)\nY_Pred_knn = knn_classifier.predict(X_Test)\nprint()\ncm_knn = confusion_matrix(Y_Test, Y_Pred_knn)\nprint(cm_knn)\nprint()\nTP_knn = cm_knn[0][0]\nFP_knn = cm_knn[0][1]\nTN_knn = cm_knn[1][1]\nFN_knn = cm_knn[1][0]\n    \nprint('Success Rate = ',(TP_knn+TN_knn)\/(TP_knn+TN_knn+FN_knn+FP_knn))\nprint('Misclassificate Rate = ',(FP_knn+FN_knn)\/(TP_knn+TN_knn+FN_knn+FP_knn))\nprint('Sensitivity\/tp_rate = ', TP_knn\/(TP_knn+FN_knn))\nprint('Specificity\/tn_rate = ', TN_knn\/(TN_knn+FP_knn))\nprint('fp rate = ',FP_knn\/(TN_knn+FP_knn))\nprint('fn rate = ',FN_knn\/(TP_knn+FN_knn))","7dbd4f06":"import sys\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nsc = StandardScaler()\nX_Train_Scaled = sc.fit_transform(X_Train)\nX_Test_Scaled = sc.fit_transform(X_Test)\nlr_classifier = LogisticRegression()\nparam_grid = {\n            'penalty' : ['l2','l1'],  \n            'C' : [0.001, 0.01, 0.1, 1, 10, 100]\n            }\n\nCV_lr_grid = GridSearchCV(estimator = lr_classifier, param_grid = param_grid , scoring = 'accuracy', verbose = 1, n_jobs = -1, cv=10)\nCV_lr_grid.fit(X_Train_Scaled, Y_Train)\nbest_parameters = CV_lr_grid.best_params_\nprint('The best parameters for using this model is', best_parameters)\nlogistic_classifier = LogisticRegression(C = best_parameters['C'], \n                                penalty = best_parameters['penalty'], \n                                random_state = 0)\nnum_folds = 10\nkfold = KFold(n_splits=num_folds)\ncv_results = cross_val_score(logistic_classifier, X_Train_Scaled, Y_Train, cv=kfold, scoring='accuracy')\nprint('Logistic Regression Accuracy on Training Data with best parameters after 10 Fold Cross Validation is :',cv_results.mean())\nlogistic_classifier.fit(X_Train_Scaled, Y_Train)\nY_Pred_lr = logistic_classifier.predict(X_Test_Scaled)\nprint()\ncm_lr = confusion_matrix(Y_Test, Y_Pred_lr)\nprint(cm_lr)\nprint()\nTP_lr = cm_lr[0][0]\nFP_lr = cm_lr[0][1]\nTN_lr = cm_lr[1][1]\nFN_lr = cm_lr[1][0]\n    \nprint('Success Rate = ',(TP_lr+TN_lr)\/(TP_lr+TN_lr+FN_lr+FP_lr))\nprint('Misclassificate Rate = ',(FP_lr+FN_lr)\/(TP_lr+TN_lr+FN_lr+FP_lr))\nprint('Sensitivity\/tp_rate = ', TP_lr\/(TP_lr+FN_lr))\nprint('Specificity\/tn_rate = ', TN_lr\/(TN_lr+FP_lr))\nprint('fp rate = ',FP_lr\/(TN_lr+FP_lr))\nprint('fn rate = ',FN_lr\/(TP_lr+FN_lr))","8cbfd5dc":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nsvm = SVC()\nparam_grid = {'C': [0.001,0.01, 0.1, 1, 10,20,30,40,50,100], \n              'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.02, 0.03, 0.001], \n              'kernel': ['linear']} \ngrid = GridSearchCV(svm, param_grid, refit=True, verbose=1, cv=10)\ngrid.fit(X_Train_Scaled, Y_Train)\nbest_parameters = grid.best_params_\nprint('The best parameters for using this model is', best_parameters)\nsvm_classifier = SVC(C = best_parameters['C'], \n                                gamma = best_parameters['gamma'],\n                                kernel = 'linear',    \n                                random_state = 0,\n                                probability = True)\nnum_folds = 10\nkfold = KFold(n_splits=num_folds)\ncv_results = cross_val_score(svm_classifier, X_Train_Scaled, Y_Train, cv=kfold, scoring='accuracy')\nprint('Support Vector Machines Accuracy on Training Data with best parameters after 10 Fold Cross Validation is :',cv_results.mean())\nprint()\nsvm_classifier.fit(X_Train_Scaled, Y_Train)\nY_Pred_svm = svm_classifier.predict(X_Test_Scaled)\ncm_svm = confusion_matrix(Y_Test, Y_Pred_svm)\nprint(cm_svm)\nprint()\nTP_svm = cm_svm[0][0]\nFP_svm = cm_svm[0][1]\nTN_svm = cm_svm[1][1]\nFN_svm = cm_svm[1][0]\n    \nprint('Success Rate = ',(TP_svm+TN_svm)\/(TP_svm+TN_svm+FN_svm+FP_svm))\nprint('Misclassificate Rate = ',(FP_svm+FN_svm)\/(TP_svm+TN_svm+FN_svm+FP_svm))\nprint('Sensitivity\/tp_rate = ', TP_svm\/(TP_svm+FN_svm))\nprint('Specificity\/tn_rate = ', TN_svm\/(TN_svm+FP_svm))\nprint('fp rate = ',FP_svm\/(TN_svm+FP_svm))\nprint('fn rate = ',FN_svm\/(TP_svm+FN_svm))","c64f02d4":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn import tree\ny_entropy = []\ny_gini = []\nfor depth in range(len(dataset.columns)):\n    classifier_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=depth+1,random_state=0)\n    classifier_gini = DecisionTreeClassifier(criterion='gini', max_depth=depth+1,random_state=0)\n    classifier_entropy.fit(X_Train_Scaled, Y_Train)\n    classifier_gini.fit(X_Train_Scaled, Y_Train)\n    y_entropy.append(classifier_entropy.score(X_Test_Scaled, Y_Test)*100)\n    y_gini.append(classifier_gini.score(X_Test_Scaled, Y_Test)*100)\n    \nplt.figure(figsize=(12,4))\nplt.plot(range(1, len(dataset.columns)+1), y_entropy)\nplt.plot(range(1, len(dataset.columns)+1), y_gini)\nplt.title('Gini Vs Entropy')\nplt.xlabel('Depth')\nplt.ylabel('Accuracy')\nplt.legend(['Entropy', 'Gini'])\nplt.show()\n\nbest_accuracy = np.amax(y_entropy), np.amax(y_gini)\nbest_criterion = ['Entropy', 'Gini']\n\nprint('Best Criterion: {}, Accuracy {:.2f}% at depth = {}'.format(best_criterion[np.argmax(best_accuracy)], \n                                                                  np.amax(best_accuracy), \n                                                                  np.argmax(y_gini)+1 if np.amax(y_gini) > np.amax(y_entropy) else np.argmax(y_entropy)+1))\ndt_classifier = DecisionTreeClassifier(criterion = 'entropy', max_depth = 7, random_state= 0)\nnum_folds = 10\nkfold = KFold(n_splits=num_folds)\ncv_results = cross_val_score(dt_classifier, X_Train_Scaled, Y_Train, cv=kfold, scoring='accuracy')\nprint('Decision Trees Accuracy on Training Data with best parameters after 10 Fold Cross Validation is :',cv_results.mean())\nprint()\ndt_classifier.fit(X_Train_Scaled, Y_Train)\nY_Pred_dt = dt_classifier.predict(X_Test_Scaled)\ncm_dt = confusion_matrix(Y_Test, Y_Pred_dt)\nprint(cm_dt)\nprint()\nTP_dt = cm_dt[0][0]\nFP_dt = cm_dt[0][1]\nTN_dt = cm_dt[1][1]\nFN_dt = cm_dt[1][0]\nprint('Success Rate = ',(TP_dt+TN_dt)\/(TP_dt+TN_dt+FN_dt+FP_dt))\nprint('Misclassificate Rate = ',(FP_dt+FN_dt)\/(TP_dt+TN_dt+FN_dt+FP_dt))\nprint('Sensitivity\/tp_rate = ', TP_dt\/(TP_dt+FN_dt))\nprint('Specificity\/tn_rate = ', TN_dt\/(TN_dt+FP_dt))\nprint('fp rate = ',FP_dt\/(TN_dt+FP_dt))\nprint('fn rate = ',FN_dt\/(TP_dt+FN_dt))\n\nplt.figure(figsize = (10,10))\ntree.plot_tree(classifier_gini.fit(X_Test, Y_Test))","d834c713":"from sklearn.ensemble import RandomForestClassifier\nnum_folds = 10\nkfold = KFold(n_splits=num_folds)\nfor i in range(1, 21):\n    for j in range(1,10):\n        rf = RandomForestClassifier(n_estimators = i, random_state=0, max_depth = j)\n        score = cross_val_score(rf, X_Train_Scaled, Y_Train, scoring='accuracy' ,cv=kfold).mean()\n        print(\"N_Estimators = \" + str(i) + \" : Depth = \"+ str(j) + \" : Accuracy = \" + str(score))","2452197b":"# Training the algorithm for best parameters.\n# There are 4 cases in which the same maximum training accuracy.\n# The 4 cases are - N = 12,14,15,17 and Depth = 5.\nN_Estimators = [12,14,15,17]\n# Calculating the testing accuracy for the best obtained N_Estimators and Depth and then finding the best testing accuracy.\nfor i in N_Estimators:\n    rfc_classifier = RandomForestClassifier(n_estimators = i, max_depth = 5, random_state = 0)                      \n    rfc_classifier.fit(X_Train_Scaled, Y_Train)\n    Y_Pred_rfc = rfc_classifier.predict(X_Test_Scaled)\n    cm_rfc = confusion_matrix(Y_Test, Y_Pred_rfc)\n    TP_rfc = cm_rfc[0][0]\n    FP_rfc = cm_rfc[0][1]\n    TN_rfc = cm_rfc[1][1]\n    FN_rfc = cm_rfc[1][0]\n    print('Success Rate = ',(TP_rfc+TN_rfc)\/(TP_rfc+TN_rfc+FN_rfc+FP_rfc))","2a6af774":"rfc_classifier_final = RandomForestClassifier(n_estimators = 15, max_depth = 5, random_state = 0)       \nnum_folds = 10\nkfold = KFold(n_splits=num_folds)\ncv_results = cross_val_score(rfc_classifier_final, X_Train_Scaled, Y_Train, cv=kfold, scoring='accuracy')\nprint('Random Forest Accuracy on Training Data with best parameters after 10 Fold Cross Validation is :',cv_results.mean())\nprint()\nrfc_classifier_final.fit(X_Train_Scaled, Y_Train)\nY_Pred_rfc = rfc_classifier_final.predict(X_Test_Scaled)\ncm_rfc = confusion_matrix(Y_Test, Y_Pred_rfc)\nprint(cm_rfc)\nTP_rfc = cm_rfc[0][0]\nFP_rfc = cm_rfc[0][1]\nTN_rfc = cm_rfc[1][1]\nFN_rfc = cm_rfc[1][0]\nprint()    \nprint('Success Rate = ',(TP_rfc+TN_rfc)\/(TP_rfc+TN_rfc+FN_rfc+FP_rfc))\nprint('Misclassificate Rate = ',(FP_rfc+FN_rfc)\/(TP_rfc+TN_rfc+FN_rfc+FP_rfc))\nprint('Sensitivity\/tp_rate = ', TP_rfc\/(TP_rfc+FN_rfc))\nprint('Specificity\/tn_rate = ', TN_rfc\/(TN_rfc+FP_rfc))\nprint('fp rate = ',FP_rfc\/(TN_rfc+FP_rfc))\nprint('fn rate = ',FN_rfc\/(TP_rfc+FN_rfc))\nprint()","47d3bf26":"from sklearn.neural_network import MLPClassifier\nacc = []\nlearning_rate = [1e-5,1e-4,1e-3,1e-2,1e-1,1]\nfor i in range(10,90,10):\n    for j in learning_rate:\n        nn = MLPClassifier(hidden_layer_sizes=(9,2),\n                      learning_rate_init = j,\n                      max_iter = i,\n                      random_state = 33)\n        score = cross_val_score(nn, X_Train_Scaled, Y_Train, scoring='accuracy' ,cv=kfold).mean()\n        print(\"Epochs\/Number_Of_Iterations = \" + str(i) + \" : Learning_Rate = \"+ str(j) + \" : Accuracy = \" + str(score))  ","ad5e9d38":"# The best accuracy on training data is achieved when learning rate is 1e-2.\n# Keeping the learning rate as 1e-2, we iterate through number of epochs\/iterations to find the best testing accuracy and \n# minimum overfitting of training data. \n\ntraining_acc = []\ntesting_acc = []\nfor i in range(1,50):\n    model = MLPClassifier(hidden_layer_sizes=(9,2),\n                      learning_rate_init = 1e-2,\n                      max_iter = i,\n                      random_state = 33)\n    model.fit(X_Train_Scaled,Y_Train)\n    prd_r = model.predict(X_Test_Scaled)\n    test_acc = metrics.accuracy_score(Y_Test, prd_r) * 100.\n    train_acc = model.score(X_Train_Scaled,Y_Train) *100\n    testing_acc.append(test_acc)\n    training_acc.append(train_acc)\ntesting_error = [100-x for x in testing_acc]  \ntraining_error = [100-x for x in training_acc]\nplt.title('Model Loss')\nplt.ylabel('Error')\nplt.xlabel('Epochs')\nplt.plot(training_error,label = 'Train')\nplt.plot(testing_error, label = 'Test')  \nplt.legend(loc='upper right')\nplt.show()    \nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.plot(training_acc,label = 'Train')\nplt.plot(testing_acc, label = 'Test')  \nplt.legend(loc='lower right')\nplt.show() ","baa0f8aa":"# Optimum number of epochs as seen from the graph can be chosen as 20 (Same testing accuracy for epochs between 14 and 29)\nMLP_classifier = MLPClassifier(hidden_layer_sizes=(9,2),\n                      learning_rate_init = 1e-2,\n                      max_iter = 20,\n                      random_state = 33)\nnum_folds = 10\nkfold = KFold(n_splits=num_folds)\ncv_results = cross_val_score(MLP_classifier, X_Train_Scaled, Y_Train, cv=kfold, scoring='accuracy')\nprint('Neural Networks Accuracy on Training Data with best parameters after 10 Fold Cross Validation is :',cv_results.mean())\nprint()\nMLP_classifier.fit(X_Train_Scaled, Y_Train)\nY_Pred_mlp = MLP_classifier.predict(X_Test_Scaled)\ncm_mlp = confusion_matrix(Y_Test, Y_Pred_mlp)\nprint(cm_mlp)\nTP_mlp = cm_mlp[0][0]\nFP_mlp = cm_mlp[0][1]\nTN_mlp = cm_mlp[1][1]\nFN_mlp = cm_mlp[1][0]\nprint()    \nprint('Success Rate = ',(TP_mlp+TN_mlp)\/(TP_mlp+TN_mlp+FN_mlp+FP_mlp))\nprint('Misclassificate Rate = ',(FP_mlp+FN_mlp)\/(TP_mlp+TN_mlp+FN_mlp+FP_mlp))\nprint('Sensitivity\/tp_rate = ', TP_mlp\/(TP_mlp+FN_mlp))\nprint('Specificity\/tn_rate = ', TN_mlp\/(TN_mlp+FP_mlp))\nprint('fp rate = ',FP_mlp\/(TN_mlp+FP_mlp))\nprint('fn rate = ',FN_mlp\/(TP_mlp+FN_mlp))\nprint()","dbb7e7ca":"from sklearn import metrics\nfig = plt.figure(figsize=(10,10))\nax1 = fig.add_subplot(111)\nax1.plot([0, 1], [0, 1], ls=\"--\")\ncolors = (\"blue\",\"green\",\"orange\",\"red\",\"purple\",\"brown\",\"pink\")\ntp_array = [TP_nb\/(TP_nb+FN_nb),TP_lr\/(TP_lr+FN_lr),TP_knn\/(TP_knn+FN_knn), TP_svm\/(TP_svm+FN_svm),TP_dt\/(TP_dt+FN_dt),TP_rfc\/(TP_rfc+FN_rfc),TP_mlp\/(TP_mlp+FN_mlp)]\nfp_array = [FP_nb\/(TN_nb+FP_nb),FP_lr\/(TN_lr+FP_lr),FP_knn\/(TN_knn+FP_knn), FP_svm\/(TN_svm+FP_svm), FP_dt\/(TN_dt+FP_dt),FP_rfc\/(FP_rfc+TN_rfc),FP_mlp\/(FP_mlp+TN_mlp)]\nfor x,y,color in zip(tp_array,fp_array,colors):\n    plt.scatter(y,x,c=color)\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')\nax1.set_title('Classifiers on ROC curve')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nax1.plot(tp_array[0], fp_array[0] , label= \"Naive_Bayes\" , color = \"blue\")\nax1.plot(tp_array[2], fp_array[2] , label= \"K Nearest Neighbors\", color = \"orange\")\nax1.plot(tp_array[1], fp_array[1] , label= \"Logistic Regression\", color = \"green\")\nax1.plot(tp_array[3], fp_array[3] , label= \"Support Vector Machine\", color = \"red\")\nax1.plot(tp_array[4], fp_array[4] , label= \"Decision Trees\", color = \"purple\")\nax1.plot(tp_array[5], fp_array[5] , label= \"Random Forest\", color = \"brown\")\nax1.plot(tp_array[6], fp_array[6] , label= \"MLP\", color = \"white\")\nax1.legend(loc=\"lower right\")\nplt.show()\n# Pink and Red Overlap","ca9bcd91":"from numpy import sqrt\nfrom numpy import argmax\n# calculate the fpr and tpr for all thresholds of the classification\nnb_probs = nb_classifier.predict_proba(X_Test)\nknn_probs = knn_classifier.predict_proba(X_Test)\nlr_probs = logistic_classifier.predict_proba(X_Test_Scaled)\nsvm_probs = svm_classifier.predict_proba(X_Test_Scaled)\ndt_probs = dt_classifier.predict_proba(X_Test_Scaled)\nrfc_probs = rfc_classifier_final.predict_proba(X_Test_Scaled)\nmlp_probs = MLP_classifier.predict_proba(X_Test_Scaled)\n\npreds_nb = nb_probs[:,1]\npreds_knn = knn_probs[:,1]\npreds_lr = lr_probs[:,1]\npreds_svm = svm_probs[:,1]\npreds_dt = dt_probs[:,1]\npreds_rfc = rfc_probs[:,1]\npreds_mlp = mlp_probs[:,1]\n\nfpr_nb, tpr_nb, threshold_nb = metrics.roc_curve(Y_Test, preds_nb)\nroc_auc_nb = metrics.auc(fpr_nb, tpr_nb)\nfpr_knn, tpr_knn, threshold_knn = metrics.roc_curve(Y_Test, preds_knn)\nroc_auc_knn = metrics.auc(fpr_knn, tpr_knn)\nfpr_lr, tpr_lr, threshold_lr = metrics.roc_curve(Y_Test, preds_lr)\nroc_auc_lr = metrics.auc(fpr_lr, tpr_lr)\nfpr_svm, tpr_svm, threshold_svm = metrics.roc_curve(Y_Test, preds_svm)\nroc_auc_svm = metrics.auc(fpr_svm, tpr_svm)\nfpr_dt, tpr_dt, threshold_dt = metrics.roc_curve(Y_Test, preds_dt)\nroc_auc_dt = metrics.auc(fpr_dt, tpr_dt)\nfpr_rfc, tpr_rfc, threshold_rfc = metrics.roc_curve(Y_Test, preds_rfc)\nroc_auc_rfc = metrics.auc(fpr_rfc, tpr_rfc)\nfpr_mlp, tpr_mlp, threshold_mlp = metrics.roc_curve(Y_Test, preds_mlp)\nroc_auc_mlp = metrics.auc(fpr_mlp, tpr_mlp)\n\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characterstics')\nplt.plot(fpr_nb, tpr_nb, label='Naive Bayes (AUC = %0.4f)' % (roc_auc_nb))\nplt.plot(fpr_knn, tpr_knn, label='K Nearest Neighbors (AUC = %0.4f)' % (roc_auc_knn))\nplt.plot(fpr_lr, tpr_lr, label='Logistic Regression (AUC = %0.4f)' % (roc_auc_lr))\nplt.plot(fpr_svm, tpr_svm, label='Support Vector Machines (AUC = %0.4f)' % (roc_auc_svm))\nplt.plot(fpr_dt, tpr_dt, label='Decision Trees (AUC = %0.4f)' % (roc_auc_dt))\nplt.plot(fpr_rfc, tpr_rfc, label='Random Forest Classifier (AUC = %0.4f)' % (roc_auc_rfc))\nplt.plot(fpr_mlp, tpr_mlp, label='MLP (AUC = %0.4f)' % (roc_auc_mlp))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")","926b991e":"roc_auc_nb = metrics.auc(fpr_nb, tpr_nb)\nroc_auc_knn = metrics.auc(fpr_knn, tpr_knn)\nroc_auc_lr = metrics.auc(fpr_lr, tpr_lr)\nroc_auc_svm = metrics.auc(fpr_svm, tpr_svm)\nroc_auc_dt = metrics.auc(fpr_dt, tpr_dt)\nroc_auc_rfc = metrics.auc(fpr_rfc, tpr_rfc)\nroc_auc_mlp = metrics.auc(fpr_mlp, tpr_mlp)\nprint(\"Naive Bayes AUC = \",roc_auc_nb)\nprint(\"K Nearest Neighbors AUC = \",roc_auc_knn)\nprint(\"Logistic Regression AUC = \",roc_auc_lr)\nprint(\"Support Vector Machine AUC = \",roc_auc_svm)\nprint(\"Decision Trees AUC = \",roc_auc_dt)\nprint(\"Random Forest AUC = \",roc_auc_rfc)\nprint(\"MLP AUC = \",roc_auc_mlp)","c27cc3cc":"**Exploring the Dataset**","5c88684e":"**Neural Networks**","bd004f8e":"**ROC-AUC for different Classifiers**","7b567295":"**Exploratory Data Analysis**","9abed361":"**Logistic Regression Algorithm**","4aa7f66f":"**Support Vector Machine Algorithm**","c87e5309":"The column \"id\" plays no role in training\/testing, hence it is dropped.","89031559":"**Importing Libraries**","8fb66740":"**Handling the missing data**","11c3445a":"**Decision Trees Algorithm**","e2d7a4e3":"After predicting the missing values, they are merged with the subset of the datatset comprising of rows that did not have any missing values. ","00158062":"Since, the dataset is too small (comprising of only 699 samples), the missing values cannot be removed from the dataset. Instead, the missing vales are predicted using the K-Nearest Neighbors algorithm. The subsets created previously are used as the training and testing data for predicting the mising values. For choosing the optimum value of K, Mean Absolute Error is used as the metric. The value of K is varied, and the K value giving the least MAE is chosen as the optimum value. ","95589fce":"**Random Forest Algorithm**","8149b8d8":"In the above output, it can be clearly seen that all the features except \"bare_nucleoli\" have int64 as their datatype, whereas the feature \"bare_nucleoli\" has an object datatype, indicating that it has data other than numerical values.","36e71b19":"Upon exploring the column \"bare_nucleoli\", it was observed that it has 16 missing values, indicated by a \"?\", which explains why it had an object datatype and not int64.  ","e0079934":"**Classifiers as points on Receiver Operating Characterstics Curve**","2b4ddc57":"**Splitting the data into train and test data**","bf59e8e5":"The data is now divided into two subsets, one subset comprising of the rows which have missing values in them, and the other subset comprising of all the remaining rows in it. ","d203196d":"**Training the machine using various algorithms -**","d0ab032e":"**K-Nearest Neighbors Algorithm**","21421e0c":"In the given dataset, the classes are labelled as 2 and 4, representing 0 and 1 respectively, where 0 states that the given sample represents a cell which is benign i.e. not cancerous, and 1 states that the given sample is cancerous i.e. malignant. Class labels having a value 2 are encoded to 0 annd class labels having a value 4 are encoded to 1.  ","35bff39b":"**Naive Bayes Algorithm**","824f1665":"**Importing the Dataset **"}}