{"cell_type":{"81171b27":"code","8ece2737":"code","a8f1c2e1":"code","02c32ce1":"code","1795eb2a":"code","16dc41ff":"code","6a73263e":"code","4fbda39b":"code","8c83e948":"code","96f0d256":"code","8f30acfa":"code","c86cf4ac":"code","ad7c4d77":"code","ba6bf365":"code","1836ffe6":"code","2622e50a":"code","ac380b3e":"code","3a51c564":"code","7ed6eec5":"code","633db217":"code","341bace3":"code","e5877fab":"code","adb6a45a":"code","ec5b3313":"code","8276013a":"code","3c5c5350":"code","87e3d01c":"code","87002b19":"code","70d27fba":"code","b74beaf5":"code","67c9b75a":"code","dd8af0fb":"code","4b989d08":"code","db972b62":"code","bf533ce3":"code","d9836310":"code","46750cbf":"code","d8ca7ae5":"code","fe3f9716":"code","e28e81f3":"code","567f1e8b":"code","aaeef257":"code","71c1f369":"code","7f7d235a":"code","773af085":"code","dfff89b3":"code","50287f73":"code","cb28c082":"code","6df2ea63":"code","23b2dabb":"code","2933fe4b":"code","ea36064e":"code","aae73374":"code","4aff325e":"code","a46544eb":"code","0bb1c817":"code","37a186e6":"code","b0abe2f5":"code","59c803c1":"code","a1f95bf9":"markdown","9b58ae79":"markdown","59f80398":"markdown","07cb9cb0":"markdown","7d14b09c":"markdown","1f7b0883":"markdown","22e30499":"markdown","bae97915":"markdown","858c936d":"markdown","fa8f4a5c":"markdown","0f86aa37":"markdown","b83e5ec7":"markdown","5817d3ad":"markdown","4840e42a":"markdown","32718a9c":"markdown","2d1babca":"markdown","52e3f552":"markdown","a09659ee":"markdown","b259c49d":"markdown","586dcc9b":"markdown","1b01e1ba":"markdown","9723deb6":"markdown","564a06da":"markdown","47298e46":"markdown","94d5cbea":"markdown","61f8072b":"markdown","c3d23bd6":"markdown","caab1593":"markdown","1416c7d7":"markdown","84a75db8":"markdown","307bfd9b":"markdown","1e254af3":"markdown","e34edd84":"markdown","0028374f":"markdown","5ccff201":"markdown","c6987b6f":"markdown","29739ba5":"markdown","a2541a15":"markdown","73abac35":"markdown","ed917485":"markdown","1821fa6f":"markdown","07bf6b97":"markdown","0f56c7d9":"markdown","86b402f2":"markdown","de377055":"markdown","a4d9bc8d":"markdown","a3788b8b":"markdown","77bff15f":"markdown","6a6c013b":"markdown","9fe71192":"markdown","5b18fe97":"markdown","acbd33c6":"markdown","f376ff38":"markdown","48951783":"markdown","e2778a01":"markdown","0734af0e":"markdown","7995ce64":"markdown","18f07bfb":"markdown","3b9e1b7b":"markdown","e2df73b3":"markdown","ef1da35b":"markdown","da0c1f9b":"markdown","6bd416de":"markdown","15d498a4":"markdown"},"source":{"81171b27":"!curl -O https:\/\/download.java.net\/java\/GA\/jdk11\/9\/GPL\/openjdk-11.0.2_linux-x64_bin.tar.gz\n\n!mv openjdk-11.0.2_linux-x64_bin.tar.gz \/usr\/lib\/jvm\/; cd \/usr\/lib\/jvm\/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n!update-alternatives --install \/usr\/bin\/java java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java 1\n!update-alternatives --set java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java","8ece2737":"import os\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/jdk-11.0.2\"","a8f1c2e1":"!pip install pyserini==0.8.1.0\nfrom pyserini.search import pysearch","02c32ce1":"COVID_INDEX = '..\/input\/luceneindexcovidparagraph20200410\/lucene-index-covid-paragraph-2020-04-10'","1795eb2a":"searcher = pysearch.SimpleSearcher(COVID_INDEX)","16dc41ff":"def get_articles(query):\n    hits = searcher.search(query)\n    #print(len(hits))\n    # Prints the first 10 hits\n    return hits","6a73263e":"query = 'range of incubation periods for COVID-19'\nhits = get_articles(query)\nfor i in range(0, 10):\n    #print some relevant fields\n    print(f'{i+1} {hits[i].docid} {hits[i].score} {hits[i].lucene_document.get(\"title\")} {hits[i].lucene_document.get(\"doi\")}')","4fbda39b":"hits[0].contents.split('\\n')","8c83e948":"import json\ndef get_para_results(query):\n    hits = searcher.search(query,10) \n    temp = {} # to store the doi of the articles being returned so we know if the article is repeated\n    i = 0\n    output = []\n    while i<len(hits) and i<10:\n        outJson = {}\n        outJson['rank'] = i+1\n        # check if the current article has a paragraph returned or not ('has_full_text' in the dataset)\n        if '.' in hits[i].docid:\n            doc_id = hits[i].docid.split('.')[0]\n            para_id = hits[i].docid.split('.')[1]\n            doi = hits[i].lucene_document.get('doi')\n            paragraph = {}\n            paragraph['score'] = hits[i].score\n            paragraph['text'] = hits[i].contents.split('\\n')[-1] # get the last element, since the contents are sorted as [title, abstract, paragraph]\n            paragraph['id'] = para_id\n            # check if the doi (same article) has not appeared before in the list\n            if doi not in temp:\n                outJson['abstract'] = hits[i].lucene_document.get('abstract') # include abstract if new article\n                article_data = json.loads(searcher.doc(doc_id).lucene_document().get('raw')) # get all the relevant data from the dataset \n                if 'body_text' in article_data:\n                    outJson['body_text'] = article_data['body_text'] # include 'body_text' in case needed later\n                temp[doi] = i\n            outJson['paragraphs'] = []\n            outJson['paragraphs'].append(paragraph)\n        else:\n            # no paragraph present, which means article does not have full text available\n            outJson['abstract'] = hits[i].lucene_document.get('abstract')\n            outJson['score'] = hits[i].score\n        outJson['title'] = hits[i].lucene_document.get('title')\n        outJson['sha'] = hits[i].lucene_document.get('sha')\n        outJson['doi'] = hits[i].lucene_document.get('doi')\n        output.append(outJson)\n        i+=1\n    return output","96f0d256":"query = 'range of incubation periods for COVID-19'\ni = 1\nfor item in get_para_results(query):\n    if i>10:\n        break\n    print(item)\n    i+=1","8f30acfa":"def information_retrieval(file_name, topk = 10):\n\n    with open(file_name) as f:\n        json_file = json.load(f)\n    subtasks = json_file[\"sub_task\"]\n    \n    all_results = []\n    data_for_qa = []\n    for item in subtasks:\n        questions = item[\"questions\"]\n        for query in questions:\n            result_item = {\"question\" : query}\n            retri_result = get_para_results(query)\n            result_item[\"data\"] = retri_result\n\n            qa_item = {\"question\": query}\n            context = []\n            titles = []\n            doi = []\n            count = 1\n            for item in retri_result:\n                if count>topk:\n                    break\n                if 'abstract' in item and len(item['abstract']) > 0:\n                    context.append(item['abstract'])\n                    doi.append(item[\"doi\"])\n                    titles.append(item[\"title\"])\n                    count+=1\n                if 'paragraphs' in item:\n                    context.append(item['paragraphs'][0]['text'])   \n                    doi.append(item[\"doi\"])\n                    titles.append(item[\"title\"])\n                    count+=1\n\n            qa_item[\"data\"] = {\"answer\": \"\", \"context\": context, \"doi\": doi, \"titles\": titles}\n\n            all_results.append(result_item)\n            data_for_qa.append(qa_item)\n\n    return data_for_qa\n\ndef parse_ir_results(query, retri_result, topk = 10):\n    all_results = []\n    data_for_qa = []\n    qa_item = {\"question\": query}\n    result_item = {\"question\" : query}\n    result_item[\"data\"] = retri_result\n    context = []\n    titles = []\n    doi = []\n    count = 1\n    for item in retri_result:\n        if count>topk:\n            break\n        if 'abstract' in item and len(item['abstract']) > 0:\n            context.append(item['abstract'])\n            doi.append(item[\"doi\"])\n            titles.append(item[\"title\"])\n            count+=1\n        if 'paragraphs' in item:\n            context.append(item['paragraphs'][0]['text'])   \n            doi.append(item[\"doi\"])\n            titles.append(item[\"title\"])\n            count+=1\n    qa_item[\"data\"] = {\"answer\": \"\", \"context\": context, \"doi\": doi, \"titles\": titles}\n\n    all_results.append(result_item)\n    data_for_qa.append(qa_item)    \n\n    return all_results, data_for_qa\n\n    \ndef information_retrieval_query(query):\n\n    retri_result = get_para_results(query)\n    all_results, data_for_qa = parse_ir_results(query, retri_result ,topk = 20)\n    \n    return all_results, data_for_qa","c86cf4ac":"### 3.1 install the prerequisite\nimport os\nimport sys\nimport json\n\n!pip uninstall tensorflow -y\n!pip uninstall tensorflow-gpu -y\n!pip install tensorflow==1.13.1\n!pip install caireCovid==0.1.8","ad7c4d77":"import tensorflow as tf\nimport caireCovid\nfrom caireCovid import QaModule\nfrom caireCovid.qa_utils import stop_words\nimport math","ba6bf365":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","1836ffe6":"### 3.2 Check all version\nprint(tf.__version__)","2622e50a":"# QA System\nclass QA_System():\n    def _init_(self):\n        # Load the QA models. Please refer to [Github](https:\/\/github.com\/yana-xuyan\/caireCovid) for details.\n        self.model = QaModule(['mrqa', 'biobert'], [\"\/kaggle\/input\/pretrained-qa-models\/mrqa\/1564469515\", \"\/kaggle\/input\/pretrained-qa-models\/biobert\/1585470591\"], \\\n                              \"\/kaggle\/input\/xlnetlargecased\/xlnet_cased_L-24_H-1024_A-16\/spiece.model\", \"\/kaggle\/input\/pretrained-qa-models\/bert_config.json\", \\\n                              \"\/kaggle\/input\/bert-base-cased\/vocab.txt\")\n    def getAnswer(self, query):\n        _, data_for_qa = information_retrieval_query(query)\n        answers =  self.model.getAnswers(data_for_qa)\n        return answers\n    def getAnswers(self, filename):\n        _, data_for_qa = information_retrieval(query)\n        answers = self.model.getAnswers(data_for_qa)\n        return answers\n    def makeFormatAnswers(self, answers):\n        format_answers = []\n        for i in range(len(answers[0]['data']['answer'])):\n                format_answer = {}\n                format_answer['question'] = answers[0]['question']\n                format_answer['answer'] = answers[0]['data']['answer'][i]\n                format_answer['context'] = answers[0]['data']['context'][i]\n                format_answer['doi'] = answers[0]['data']['doi'][i]\n                format_answer['title'] = answers[0]['data']['title'][i]\n                format_answer[\"confidence\"] = answers[0]['data']['confidence'][i]\n                format_answer[\"raw\"] = answers[0]['data']['raw'][i]\n                format_answers.append(format_answer)\n        return format_answers\n\ndef get_QA_answer_api(query):\n    url = \"http:\/\/eez114.ece.ust.hk:5000\/query_qa\"\n    payload = \"{\\n\\t\\\"text\\\": \\\"\"+query+\"\\\"\\n}\"\n    headers = {\n        'Content-Type': \"application\/json\",\n        'cache-control': \"no-cache\",\n        'Postman-Token': \"696fa512-5fed-45ca-bbe7-b7a1b4d19fe4\"\n    }\n    response = requests.request(\"POST\", url, data=payload, headers=headers)\n    response = response.json()\n    return response","ac380b3e":"import argparse\nimport sys\nimport pandas as pd\nimport csv\nimport requests\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.tokenize import sent_tokenize # use sentence tokenize\nfrom IPython.core.display import display, HTML","3a51c564":"from nltk import word_tokenize, pos_tag, sent_tokenize\nfrom caireCovid.qa_utils import stop_words\nstop_words.append('including')\n\ndef rankAnswers(answers):\n    for item in answers:\n        query = item[\"question\"]\n        context = item['context']\n        # make new query with only n. and adj.\n        tokens = word_tokenize(query.lower())\n        tokens = [word for word in tokens if word not in stop_words]\n        tagged = pos_tag(tokens)\n        query_token = [tag[0] for tag in tagged if 'NN' in tag[1] or 'JJ' in tag[1] or 'VB' in tag[1]]\n\n        text = context.lower()\n        count = 0\n        text_words = word_tokenize(text)\n        for word in text_words:\n            if word in query_token:\n                count += 1\n            \n        match_number = 0\n        for word in query_token:\n            if word == 'covid-19':\n                continue\n            if word in text_words:\n                match_number += 1\n        matching_score = count \/ (1 + math.exp(-len(text_words)+50))\/ 5 + match_number*10\n        item['matching_score'] = matching_score\n        item['rerank_score'] = matching_score + 0.5 * item['confidence']\n    \n    # sort QA results\n    answers.sort(key=lambda k: k[\"rerank_score\"], reverse=True)\n#     print([item['rerank_score'] for item in answers])\n    return answers\n\ndef highlight_qaresult(qaresult):\n    if qaresult == []:\n        print('API broken')\n        return 1\n    ## tokenize query\n    query = qaresult[0]['question']\n    query_tokens = word_tokenize(query.lower())\n    query_tokens = [word for word in query_tokens if word not in stop_words]\n    tagged = pos_tag(query_tokens)\n    query_tokens = [tag[0] for tag in tagged if 'NN' in tag[1] or 'JJ' in tag[1] or 'VB' in tag[1]]\n\n    ## highlihgt answer\n    for i in range(len(qaresult)):\n        context_1 = \"<style type='text\/css'>mark { background-color:yellow; color:black; } <\/style>\"\n        golden = qaresult[i]['answer']\n        context = qaresult[i]['context']\n        context_sents = sent_tokenize(context)\n        golden_sents = sent_tokenize(golden)\n        for sent in context_sents:\n            if sent not in golden:\n                context_1 += sent\n            else:\n                context_1 += \"<mark>\"\n                for word in sent.split():\n                    word_tokens = word_tokenize(word)\n                    if len(word_tokens) > 1:\n                        for j in word_tokens:\n                            if j.lower() in query_tokens:\n                                context_1 = context_1 + \"<b>\" + j + \"<\/b>\"\n                            else:\n                                context_1 = context_1 + j\n                        context_1 = context_1 + \" \"\n                    else:\n                        for j in word_tokens:\n                            if j.lower() in query_tokens:\n                                context_1 = context_1 + \"<b>\" + j + \" <\/b>\"\n                            else:\n                                context_1 = context_1 + j + \" \"\n                context_1 += \" <\/mark>\"\n        qaresult[i]['context'] = context_1\n    return qaresult\n\ndef display_QA(result):\n    result = highlight_qaresult(result)\n    pdata = []\n    count = 0\n    for i in range(len(result)):\n        count += 1\n        line = []\n        context_1 = \"<div> \"\n        context = result[i]['context']\n        context_1 = context_1 + context\n        context_1 += \" <\/div>\"\n        line.append(context_1)\n        context_2 = '<a href= \"https:\/\/doi.org\/'\n        context_2 += result[i]['doi']\n        context_2 += '\">'\n        context_2 += result[i]['title']\n        context_2 += '<\/a>'\n        line.append(context_2)\n        pdata.append(line)\n        if count > 5:\n            break\n    df = pd.DataFrame(pdata, columns = ['QA results', 'title'])\n    df = df.style.set_properties(**{'text-align': 'left','mark-color': 'red'})\n    display(df)","7ed6eec5":"!pip install easydict\n!pip install covidSumm==0.1.4\n!pip install fairseq","633db217":"import covidSumm\nimport requests\nimport json\nimport os\nimport argparse","341bace3":"from covidSumm.abstractive_utils import get_ir_result, result_to_json, get_qa_result\nfrom covidSumm.abstractive_model import abstractive_summary_model\nfrom covidSumm.abstractive_config import set_config\nfrom covidSumm.abstractive_bart_model import *","e5877fab":"def get_summary_list(article_list, abstractive_model):\n    summary_list = []\n    for i in range(len(article_list)):\n        article = article_list[i]\n        summary_results = abstractive_model.generate_summary(article)\n        result = \"\"\n        for item in summary_results:\n            result += item.replace('\\n', ' ')\n        summary_list.append(result)\n    return summary_list\n\ndef get_answer_summary(query, abstractive_model):\n    paragraphs_list = get_qa_result(query, topk = 3)\n    answer_summary_list = abstractive_model.generate_summary(paragraphs_list)\n    answer_summary = \"\"\n    for item in answer_summary_list:\n        answer_summary += item.replace('\\n', ' ')\n    answer_summary_json = {}\n    answer_summary_json['summary'] = answer_summary\n    answer_summary_json['question'] = query\n    return answer_summary_json\n\ndef get_article_summary(query, abstractive_summary_model):\n    article_list, meta_info_list = get_ir_result(query, topk = 10)  \n    summary_list = get_summary_list(article_list, abstractive_summary_model)\n    summary_list_json = []\n    \n    for i in range(len(summary_list)):\n        json_summary = {}\n        json_summary = result_to_json(meta_info_list[i], summary_list[i])\n        summary_list_json.append(json_summary)\n\n    return summary_list_json\n\ndef get_bart_answer_summary_from_qa(query, qa_result, bart_model):\n    # we select top3\n    paragraphs_list = []\n    topk = 3\n\n    for i in range(topk):\n        if 'context' in qa_result[i].keys():\n            one_line = {}\n            one_line['src'] = qa_result[i]['context']\n            one_line['tgt'] = \"\"\n            paragraphs_list.append(one_line)\n    \n    answer_summary_list = bart_model.bart_generate_summary(paragraphs_list)\n    answer_summary_result = \"\"\n    for item in answer_summary_list:\n        answer_summary_result += item.replace('\\n', ' ')\n    \n    answer_summary_json = {}\n    answer_summary_json['summary'] = answer_summary_result\n    answer_summary_json['question'] = query\n    return answer_summary_json","adb6a45a":"args = set_config()\nargs['model_path'] = '\/kaggle\/input\/carieabssummmodel\/'\nsummary_model_1 = abstractive_summary_model(config = args)","ec5b3313":"model_path = \"\/kaggle\/input\/bartsumm\/bart.large.cnn\"\nsummary_model_2 = Bart_model(model_path)","8276013a":"from IPython.core.display import display, HTML\nimport pandas as pd\n\ndef display_summary(ans_summary_json, model_type):\n    question = ans_summary_json['question']\n    text = ans_summary_json['summary']\n    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query<\/b>: '+question+'<\/div>'\n    display(HTML(question_HTML))\n\n    execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>' + model_type + ' Abstractive Summary:<\/b>: '+text+'<\/div>'\n    display(HTML(execSum_HTML))\n\ndef display_article_summary(result, query):\n    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query<\/b>: '+query+'<\/div>'\n    pdata = []\n    abstract = \"\"\n    summary = \"\"\n    for i in range(len(result)):\n        if 'abstract' in result[i].keys():\n            line = []\n            context_2 = '<a href= \"https:\/\/doi.org\/'\n            context_2 += result[i]['doi']\n            context_2 += ' target=\"_blank\">'\n            context_2 += result[i]['title']\n            context_2 += '<\/a>'\n            line.append(context_2)\n            \n            abstract = \"<div> \" \n            abstract += result[i]['abstract']\n            abstract += \" <\/div>\"\n            line.append(abstract)\n            summary = \"<div> \" + result[i]['summary'] + \" <\/div>\"\n            line.append(summary)\n\n\n            pdata.append(line)\n    display(HTML(question_HTML))\n    df = pd.DataFrame(pdata, columns = ['Title','Abstract','Summary'])\n    HTML(df.to_html(render_links=True, escape=False))\n#     display(HTML(df.to_html(render_links=True, escape=False)))\n    df = df.style.set_properties(**{'text-align': 'left'})\n    display(df)\n","3c5c5350":"query = \"How incubation period for COVID-19 varies across age?\"","87e3d01c":"def run_example(query):\n    # Given one query, we retrieve the relevant paragraphs and feed the (paragraph, query) pairs into the QA system \n    qa_result = get_QA_answer_api(query)\n    # Answer Reranking\n    qa_result = rankAnswers(qa_result)\n    \n    # Input \"summary_model_2\" is the BART summarization model.\n    # Function \"get_bart_answer_summary\" is loaded from covidSumm.abstractive_bart_model\n    # Given one query, we take top-3 reranked paragraphs from the QA module and summarize them into one paragraph\n    answer_summary_2 = get_bart_answer_summary_from_qa(query, qa_result, summary_model_2)\n    display_summary(answer_summary_2, 'BART')\n    display_QA(qa_result)","87002b19":"run_example(query)","70d27fba":"query = \"What is the range of incubation periods for COVID-19 in humans?\"\nrun_example(query)","b74beaf5":"query = \"How incubation period for COVID-19 varies across age?\"\nrun_example(query)","67c9b75a":"query = \"How incubation period for COVID-19 varies across health status?\"\nrun_example(query)","dd8af0fb":"query = \"How is prevalence of asymptomatic shedding and transmission?\"\nrun_example(query)","4b989d08":"query = \"How is prevalence of asymptomatic shedding and transmission for children?\"\nrun_example(query)","db972b62":"query = \"How Seasonality affects COVID-19 transmission?\"\nrun_example(query)","bf533ce3":"query = \"What do we know about Physical science of the COVID-19, including charge distribution, adhesion to hydrophilic \/ phobic surfaces, environmental survival and viral shedding?\"\nrun_example(query)","d9836310":"query = \"What do we know about persistence and stability of COVID-19 on different substrates and sources, including nasal discharge, sputum, urine, fecal matter, blood?\"\nrun_example(query)","46750cbf":"query = \"What do we know about persistence of COVID-19 on surfaces of different materials including copper, stainless steel, plastic?\"\nrun_example(query)","d8ca7ae5":"query = \"What do we know about Natural history of COVID-19 and shedding of it from an infected person?\"\nrun_example(query)","fe3f9716":"query = \"What do we know about implementation of diagnostics and products to improve clinical processes for COVID-19?\"\nrun_example(query)","e28e81f3":"query = \"what do we know about COVID-19 models, including animal models for infection, disease and transmission?\"\nrun_example(query)","567f1e8b":"query = \"What are tools and studies to monitor phenotypic change and potential adaptation of COVID-19?\"\nrun_example(query)","aaeef257":"query = \"What do we know about immune response and immunity for COVID-19?\"\nrun_example(query)","71c1f369":"query = \"How movement control strategies help to prevent COVID-19 secondary transmission in health care and community settings?\"\nrun_example(query)","7f7d235a":"query = \"How personal protective equipment (PPE) helps to reduce risk of COVID-19 transmission in health care and community settings?\"\nrun_example(query)","773af085":"query = \"What is role of the environment in COVID-19 transmission?\"\nrun_example(query)","dfff89b3":"query = \"Is smoking a risk factor for COVID-19?\"\nrun_example(query)","50287f73":"query = \"Is pre-existing pulmonary disease a risk factor for COVID-19?\"\nrun_example(query)","cb28c082":"query = \"Does co-morbidities make COVID-19 more transmissible or virulent?\"\nrun_example(query)","6df2ea63":"query = \"Are Neonates more potential for COVID-19?\"\nrun_example(query)","23b2dabb":"query = \"What are Socio-economic factors and economic impact of COVID-19?\"\nrun_example(query)","2933fe4b":"query = \"What are behavioral factors for COVID-19?\"\nrun_example(query)","ea36064e":"query = \"What are the differences of Socio-economic and behavioral factors for COVID-19?\"\nrun_example(query)","aae73374":"query = \"What is the basic reproductive number of COVID-19?\"\nrun_example(query)","4aff325e":"query = \"What is the incubation period of COVID-19?\"\nrun_example(query)","a46544eb":"query = \"What is the serial interval  of COVID-19?\"\nrun_example(query)","0bb1c817":"query = \"What are the modes of transmission of COVID-19?\"\nrun_example(query)","37a186e6":"query = \"What are the environmental factors of COVID-19?\"\nrun_example(query)","b0abe2f5":"query = \"What do we know about susceptibility of populations for COVID-19?\"\nrun_example(query)","59c803c1":"query = \"What are public health mitigation measures effective for controlling COVID-19?\"\nrun_example(query)","a1f95bf9":"We can see some repititions in the results above. This can either be due to multiple paragraphs in the same article being matched with the query, or one article appearing more than once in the CORD-19 dataset, due to different sources. Let's now try printing out the actual paragraph that is being matched with each of the returned hits. First we print out contents of the indexing of the first hit for example:","9b58ae79":"* Now we initiate our **Summerization model UniLM**.","59f80398":"# How is prevalence of asymptomatic shedding and transmission for children?","07cb9cb0":"We now make another function to get the best matched paragraphs results from the dataset to our given query. If the article in concern does not have full text available then only the abstract is indexed. Since we know the doi field is unique to each article, we check if the article has already appeared before in the list returned. To avoid repetitions, we only include the 'abstract' and the 'body_text' fields form the dataset if the article is new and not a repeated article from before. The function is shown below:","7d14b09c":"## 1.2 Search Engine","1f7b0883":"# What are tools and studies to monitor phenotypic change and potential adaptation of COVID-19?","22e30499":"# How is prevalence of asymptomatic shedding and transmission?","bae97915":"In this part, we introduce the word matching highlight strategy. There are two main components in this part: (1) word matching score calculate. (2) rerank and display. The input is the Question Answering result and for the output we display the most relevant paragraph with the highlighted answer.\n+ **POS-tag based scoring:** \nWe calculate a similarity score between a QA result and a given query based on keyword matching. To obtain this score, we first select important keywords based on POS-tagging - we consider words with {NN(noun), VB (verb), JJ (adjective)} POS-tags to be important keywords. Based on the set of filtered keywords, we count the word-match between QA-result keywords and query keywords. Higher the count is, more similar the QA-result and the query are. To penalize the matching scores of short retrieved paragraphs, we normalize them with sigmoid value computed from paragraph length. Moreover, we reward the paragraphs with more diverse keywords from query, which is the major factor of matching scores.\n\n+ **rerank and display:**\nThe re-ranking score is based on both the word matching score above and the confidence score from the QA system. The QA results are again ranked and displayed. ","858c936d":"# 1. Document retrieval\n## 1.1 Query Paraphrasing","fa8f4a5c":"# How movement control strategies help to prevent COVID-19 secondary transmission in health care and community settings?","0f86aa37":"The objective of this sub-module is to break down a user\u2019s query and rephrase complex query sentences into several shorter and simpler questions that convey the same meaning.  In this way,  the search engine and the question answering modules will be able to find more relevant and less redundant results. ","b83e5ec7":"* We initiate our **Summerization model BART**.","5817d3ad":"In this project, for sake of efficiency, ***we build an API for the Search Engine module to integrate it with the Question Answering module into this system***.","4840e42a":"# What do we know about Natural history of COVID-19 and shedding of it from an infected person?","32718a9c":"# What do we know about persistence and stability of COVID-19 on different substrates and sources, including nasal discharge, sputum, urine, fecal matter, blood?","2d1babca":"# What is the incubation period of COVID-19?","52e3f552":"We use [Anserini](https:\/\/github.com\/castorini\/anserini) to create the search engine to retrieve a preliminary candidate set of documents. Anserini is an information retrieval module wrapped around the open source search engine **Lucene**. Although Lucene has been used widely to build industry search engine applications, its complex indexing and lack of documentation for ad hoc experimentation and testing on standard test sets, has made it less popular in the information retrieval community. Anserini uses the Lucene indexing to create an easy-to-understand module. Standard ranking algorithms(e.g. bag of words, BM25) have been implemented in the module, which enables us to use Lucene for our application. Thanks to Jimmy Lin, we make this platform based on his [notebook](https:\/\/github.com\/castorini\/anserini-notebooks\/blob\/master\/pyserini_covid19_paragraph.ipynb). Since the disk is not large enough for saving the whole dataset with index and other models, we use his API to get the information retrieval results.","a09659ee":"# Are Neonates more potential for COVID-19?","b259c49d":"Automatic text summarization is a common problem in machine learning and natural language processing (NLP). Basically there are two main types of how to summarize text in NLP:\n* **Extraction-based summarization**, which involves pulling key phrases from the source document and combining them to make a summary, and;\n* **Abstraction-based summarization**, which creates new phrases and sentences that relay the most useful information from the original text \u2014 just like humans do. \nIn general, the abstractive method is a much harder task but performs better than an extractive method.\n\nIn our project, considering the requirements that people may still want to further read each paragraph containing the predicted QA answer spans, we summarize the top-k  (top-3) paragraphs that QA module passes, to generate a **paragraph-level abstractive summary**. \n\nOur model is based on two different abstractive summarization models: [Unilm](https:\/\/github.com\/microsoft\/unilm\/tree\/master\/s2s-ft) and [BART](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/bart), both of which have obtained SOTA results on the summarization tasks ([CNN\/DM datasets](https:\/\/cs.nyu.edu\/~kcho\/DMQA\/), and [XSUM](https:\/\/github.com\/EdinburghNLP\/XSum\/tree\/master\/XSum-Dataset) data). UniLM model is a unified pre-trained model for language understanding and generation. BART is a sequence-to-sequence model trained with denoising as a pre-training objective for language generation, translation, and comprehension.\n\nWe fine-tuned the UniLM model using [SumOnGraph](https:\/\/github.com\/coshiang\/SumOnGraph) biology dataset which includes literature for 5 types of diseases including Cancer, Cardiovascular Disease, Diabetes, Allergy, and Obesity. Original data is from PubMed which is a free resource supporting the search and retrieval of biomedical and life sciences literature with the aim of improving health\u2013both globally and personally. We used the BART model fine-tuned on CNN\/DailMail dataset. \n\nWe generate a summary for each answer-related paragraph from the QA module, then concatenate them directly to form our final **paragraph-level** answer summary.\n\n*(Actually we also implement **article-level summary**, even though in this Kaggle task, for the simplicity and legibility, we only display **the paragraph-level results** of the summarization models. It takes the whole article as input, and generate a summary for each sections (eg. Introductions section, Methodologies section)  of the articles, and then concatenate them together as a more fine-grained article-level summary, as complementary to the abstracts. You can refer to this [git](https:\/\/github.com\/Iamfinethanksu\/covidSumm) or [notebook] (https:\/\/www.kaggle.com\/sudansudan\/covid19-summarization)  for more details.))*\n\nIf anyone is interested in our article-level results, please utilize ```summary = get_article_summary(query, abstractive_summary_model)``` to obtain the results from our system. ","586dcc9b":"We can build the lucene index of the COVID-19 dataset from scratch, or get one of the pre-built indexes. Using the paragraph indexing which indexes each paragraph of an article (already uploaded the index as a dataset to use), can be downloaded from: https:\/\/www.dropbox.com\/s\/ivk87journyajw3\/lucene-index-covid-paragraph-2020-04-10.tar.gz","1b01e1ba":"# System Architecture Overview","9723deb6":"# How incubation period for COVID-19 varies across health status?","564a06da":"# What are the differences of Socio-economic and behavioral factors for COVID-19?","47298e46":"# Does co-morbidities make COVID-19 more transmissible or virulent?","94d5cbea":"The first two are title and abstract, and the last element is the matched paragraph. We can see that the paragraph matched is actually quite good at answering the query. The next step would be to get an answer in a concised form by passing the matched paragraphs to a QA model.","61f8072b":"# How personal protective equipment (PPE) helps to reduce risk of COVID-19 transmission in health care and community settings?","c3d23bd6":"# Is pre-existing pulmonary disease a risk factor for COVID-19?","caab1593":"We can try running the function for the previous query, and check if the results are what we want.","1416c7d7":"# What are the modes of transmission of COVID-19?","84a75db8":"# What are Socio-economic factors and economic impact of COVID-19?","307bfd9b":"# What do we know about susceptibility of populations for COVID-19?","1e254af3":"# How incubation period for COVID-19 varies across age?","e34edd84":"# What is the basic reproductive number of COVID-19?","0028374f":"# What is the serial interval  of COVID-19?","5ccff201":"# What are the environmental factors of COVID-19?","c6987b6f":"# How Seasonality affects COVID-19 transmission?","29739ba5":"Write down whatever question you are interested in below. For example:","a2541a15":"### Please try it!","73abac35":"# 3. Abstractive Summerization","ed917485":"# What are public health mitigation measures effective for controlling COVID-19?","1821fa6f":"# 2. Relevant Snippet Selector\n## 2.1 Question Answering","07bf6b97":"The indexing is done based on each paragraph merged with the title and abstract. Given an article with id *doc_id*, the index will be as follows:\n* *doc_id* : title + abstract\n* *doc_id.00001* : title + abstract + 1st paragraph\n* *docid.00002*: title + abstract + 2nd paragraph\n* *docid.00003*: title + abstract + 3rd paragraph","0f56c7d9":"### 1.2.1 install Python dependencies and pre-built index","86b402f2":"# What is the range of incubation periods for COVID-19 in humans?","de377055":"# Is smoking a risk factor for COVID-19?","a4d9bc8d":"In response to the COVID-19 pandemic, a lot of scholarly articles have been published recently and made freely available. At the same time, there are emerging requests from both the medical research community and the broader society to find answers to various questions regarding COVID-19. A system that can provide reliable answers to the COVID-19 related questions from the latest academic resources is crucial, especially for the medical community in the current time-critical race to treat patients and to find a cure for the virus. \n \nTo address the aforementioned requests by the medical community, we propose a machine learning-based system that uses state-of-the-art natural language processing(NLP) question answering(QA) techniques combined with summarization for mining the available scientific literature. The system is an end-to-end neural network-based open-domain QA system that can answer COVID-19 related questions, such as those questions proposed in the COVID-19 Kaggle task. Through our system, users can get two versions of the outcome:\n1. A ranked list of relevant snippets from the literature given a query;\n1. A fluent summary of the relevant results. We provide the paragraph-level summaries, which takes the paragraphs where the top three relevant snippets are located as input, to enable a more efficient way of understanding of the content.\n \nOur system consists of three different modules: **1) Document Retriever, 2) Relevant Snippet Selector, and 3) Multi-Document Summarizer**. The first module pre-processes a user\u2019s query and retrieves the most relevant k number of academic publications. The second module outputs a list of the most relevant answer snippets from the retrieved documents. It also highlights the relevant keywords. The last module is for generating the second output, namely a concise summary of the top-ranked retrieved relevant paragraphs in the two previous modules.\n\nWe have launched our [CAiRE-Covid website](https:\/\/caire.ust.hk\/covid), which showcases our results for each user query in real-time, so people can further experiment with our system.\n","a3788b8b":"# what do we know about COVID-19 models, including animal models for infection, disease and transmission?","77bff15f":"To accelerate our QA system, instead of running corresponding process in notebook, we leverage an API to make better use of the local GPUs. We also make our QA system public to the community by making python package for downloading with the following command:\n> pip install caireCovid\n\nFor anyone who may be interested in the implementation details, lease refer to our [Github repository](https:\/\/github.com\/yana-xuyan\/caireCovid). Some examples and other useful resources are also available there.","6a6c013b":"![image.png](attachment:image.png)\nIn this section, we will elaborate on the building blocks of each module in our system.\n \n1) Document Retriever\n+ **Query Paraphrasing**: It converts a long\/complicated query from a user to several shorter and simpler questions for search;\n+ **Search Engine**: We use Anserini with Lucene to retrieve publications from the candidate pool with high coverage. \n \n2) Relevant Snippet Selector: \n+ **Question Answering(QA)**: This sub-module looks for and integrates evidence from one or multiple paragraphs. We leverage an ensemble of two neural-based QA models which are pre-trained on SQuAD style QA datasets. Here we consider the QA module as a supporting fact selector to provide relevant snippets from the retrieved documents.\n+ **Answer Re-ranking & Highlight Generation**:  We rerank the retrieved result by a word matching score based on part-of-speech tagging as well as the QA system confidence score. We also highlight the answer span in order to enable easier reading of the QA results.\n\n3) Multi-document Summarizer:\n+ **Abstractive Summarization**: Another output of our system is an abstractive summary that synthesizes the answer from multiple retrieved snippets. This step aims to generate short pieces of fluent summaries based on the top relevant results. Using the neural-based summarizer, we generate summaries to improve the legibility of the results and help the user to have an overview of the relevant snippets in a short time.","9fe71192":"# Let's try now","5b18fe97":"Getting the pyserini library, which is anserini wrapped with python:","acbd33c6":"Following the lucene+answerini information retrieval as described in: https:\/\/github.com\/castorini\/anserini\/blob\/master\/docs\/experiments-covid.md\n\nSetting up JAVA sdk 11 first:","f376ff38":"# What is role of the environment in COVID-19 transmission?","48951783":"Let's write some code to print the results, which are top 10 articles matching a given query, along with the best matched paragraph. We are printing some of the fields corresponding to each article, a complete list of fields can be found [here](https:\/\/github.com\/castorini\/anserini\/blob\/master\/src\/main\/java\/io\/anserini\/index\/generator\/CovidGenerator.java#L46).","e2778a01":"# What do we know about implementation of diagnostics and products to improve clinical processes for COVID-19?","0734af0e":"# What do we know about immune response and immunity for COVID-19?","7995ce64":"# What do we know about persistence of COVID-19 on surfaces of different materials including copper, stainless steel, plastic?","18f07bfb":"# What are behavioral factors for COVID-19?","3b9e1b7b":"# What do we know about Physical science of the COVID-19, including charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival and viral shedding?","e2df73b3":"## 2.2 Highlights Generation","ef1da35b":"* install the prerequisite packages\n(The covidSumm packages are from [here](https:\/\/github.com\/Iamfinethanksu\/covidSumm).","da0c1f9b":"# Project Description","6bd416de":"For the question answering (QA) module, we have leveraged the BioBERT QA model which is finetuned on the SQuAD dataset and [our generalized QA model](http:\/\/https:\/\/github.com\/yana-xuyan\/caireCovid) for MRQA@EMNLP 2019 Shared Task[1]. Instead of fine-tuning the QA models on COVID-19 related datasets, we focus more on maintaining the generalization ability of our system so it can be easily applied to other similar tasks. For the MRQA model, we utilized six datasets, which vary from each other in terms of data source, context lengths, whether multi-hop reasoning is needed, strategies for data augmentation to reduce overfitting to the training data in order to enable generalization to out-of-domain data. Multi-task learning over six datasets is used to fine-tune large pre-trained language model XLNet[2] and it helped achieve promising results. To make the answers more readable, instead of providing small spans of answers, we provide the whole sentences and the surrounding context.  \n\nTo better evaluate the question answering results, we leverage the prediction probability of the QA models as the confidence score. The final answers of our system are re-ranked using this score as one of the factors, which will be talked about later in Section 2.2.\n \n\n[1]Su, Dan, et al. \"Generalizing Question Answering System with Pre-trained Language Model Fine-tuning.\" Proceedings of the 2nd Workshop on Machine Reading for Question Answering. 2019.\n[2]Yang, Zhilin, et al. \"Xlnet: Generalized autoregressive pretraining for language understanding.\" Advances in neural information processing systems. 2019.","15d498a4":"### 1.2.2 example for using Anserini\nHere we type query 'range of incubation periods for COVID-19' in the search engine and it will return the top10 revelant items Anserini gets in the dataset."}}