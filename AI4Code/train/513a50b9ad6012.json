{"cell_type":{"6d260ad1":"code","e8427906":"code","8e756b3b":"code","25d0d186":"code","84321c89":"code","961106bd":"code","3de45134":"code","46325705":"code","8f776f77":"code","bbfa381b":"markdown","18a17295":"markdown","bbd8b4c5":"markdown","e3794329":"markdown","7b962aa6":"markdown"},"source":{"6d260ad1":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.autograd as autograd\nimport numpy as np\nimport random\n\ntorch.manual_seed(1)","e8427906":"def argmax(vec):\n    _, idx = torch.max(vec, 1) #returns max value of each row together with index\n    return idx.item()","8e756b3b":"def prepare_seq(seq, word_to_ix):\n    return torch.tensor( [word_to_ix[w] for w in seq], dtype= torch.long)","25d0d186":"def log_sum_exp( vec ):\n    max_score = vec[0, argmax(vec)]\n    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1] )\n    V = vec - max_score_broadcast\n    return max_score + torch.log( torch.sum( torch.exp(V) ) )\n#this calculates the log sum of exponents of scores. The reason why we don't use torch.log(torch.sum(torch.exp(vec))) is that it can easily lead to value errors. exp(max_score) is very large (can reach INF), so we omit the error by carrying it out and performing the rest of computations on reasonable values\n","84321c89":"#The immediate output of the network is the tensor of emission scores\nclass BiLSTM_CRF( nn.Module ):\n    def __init__(self, vocab_size, hidden_size, tag_to_ix, embedding_dim):\n        super(BiLSTM_CRF, self).__init__()\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.tag_to_ix = tag_to_ix\n        self.tagset_size = len(self.tag_to_ix)\n        \n        self.word_embeds = nn.Embedding( vocab_size, embedding_dim)\n        self.lstm = nn.LSTM( embedding_dim, hidden_size \/\/ 2, num_layers= 1, bidirectional= True )\n        self.hidden2tag = nn.Linear( hidden_size, self.tagset_size)\n        \n        #entry i,j is trnsition score to i from j\n        self.transitions = nn.Parameter( torch.randn(self.tagset_size, self.tagset_size) )\n        \n        #We never transition to the start tag and never transition from the stop tag\n        self.transitions.data[ tag_to_ix[START_TAG], :] = -10000.\n        self.transitions.data[ :, tag_to_ix[ STOP_TAG] ] = -10000.\n        \n        self.hidden = self.init_hidden()\n        \n    def init_hidden(self):\n        return ( torch.randn( 2, 1, self.hidden_size \/\/ 2), torch.randn(2, 1, self.hidden_size\/\/2) )\n    \n    def _forward_alg( self, feats ):\n        #Do the forward algorithm to compute the partition function (the denominator of softmax probability)\n        #feats are emission scores, so that feats[i][t] is the emission score of the tag t at the i_th word\n        \n        init_alphas = torch.full( (1, self.tagset_size), -10000. ) #tensor of values equal to -10000\n        #START_TAG has all of the score\n        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n        \n        #Wrap in the variable so that we get an automatic backpropagation\n        forward_var = init_alphas #In forward_var store log of sum of all(already computed) possible scores, where each score is proportional to the exponents of their emissions and transitions\n        #Iterate through the sentence\n        for feat in feats:\n\n            t = self.tagset_size\n            log_forward = self.transitions + forward_var.view(1, -1).expand(t, t) + feat.view(-1, 1).expand(t, t)\n            max_rows = torch.max(log_forward, 1)[0].view(-1, 1)\n            max_broad = max_rows.expand(t, t)\n            log_forward = log_forward - max_broad\n            forward_var = max_rows.view(1, -1) + torch.log( torch.sum( torch.exp(log_forward), 1) ) \n            \n            #ALTERNATIVE APPROACH (POORER COMPUATIONALLY)\n            '''\n            alphas_t = []\n            for next_tag in range( self.tagset_size):\n                #Say, next_tag = y.t\n                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n                #[ EMIT[y.t|x.f.t], EMIT[y.t|x.f.t], EMIT[y.t|x.f.t] , ..., EMIT[y.t|x.f.t] ]\n                #the ith entry of trans_score is the transitions score from i to next_tag\n                trans_score = self.transitions[next_tag].view(1, -1)\n                #[ TR[y.t|y.0], TR[y.t|y.1], ..., TR[y.t|y.t] ]\n                next_tag_var = forward_var + trans_score + emit_score #Each route ending (temporalily) at y.t has its sum multiplied by exp(trans_score+emit_score)\n                #so now, in the ith index, we have the sum of log_exps of routes ending at i\n                \n                #[ EMIT[y.t|x.f.t]+TR[y.t|y.0] +ForVar[y.0], ..., EMIT[y.t|x.f.t]+TR[y.t|y.t] + ForVar[t]]\n                alphas_t.append( log_sum_exp(next_tag_var).view(1)) #exponentiate to obtain routes and take logarithm to maintain the invariant\n            forward_var =torch.cat(alphas_t).view(1, -1) #Now the old values of forward_var disappear. Forward_var is a new row tensor.\n            '''\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        alpha = log_sum_exp( terminal_var)\n        return alpha #alpha is the logarithm of sums of exponentials of scores of all possible routes\n    \n    def _get_lstm_features(self, sentence):\n        self.hidden = self.init_hidden( )\n        embeds = self.word_embeds(sentence).view( len(sentence), 1, -1)\n        lstm_out, self.hidden = self.lstm( embeds, self.hidden)\n        lstm_out = lstm_out.view( len(sentence), self.hidden_size)\n        lstm_feats = self.hidden2tag(lstm_out)\n        return lstm_feats\n    \n    def _score_sentence(self, feats, tags):\n        #Give a score of a provided tag sequence\n        #Recall that len( feats ) = len( tags )\n        score = torch.zeros(1)\n        #Concatenate the start_tag, as all tags begin in the abstract start_tag\n        tags = torch.cat( [ torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags ] )\n        for i, feat in enumerate(feats):\n            score = score + self.transitions[ tags[i+1], tags[i] ] + feat[ tags[i+1] ]  #i+1 as 0 is the start_tag\n        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1] ] #All tags end in an abtract stop_tag\n        return score\n\n    def _viterbi_decode(self, feats):\n        backpointers = []\n\n        #initialize the viterbi variables in log space\n        init_vvars = torch.full( (1, self.tagset_size), -10000.)\n        init_vvars[0][self.tag_to_ix[START_TAG] ] = 0\n\n        forward_var = init_vvars #Column\/row tensor of the best paths ending in each node of a layer\n        for feat in feats:\n            bptrs_t = [] #holds the backpointers for this step\n            viterbivars_t = [] #holds the viterbi variables for this step\n\n            for next_tag in range( self.tagset_size ):\n                next_tag_var = forward_var + self.transitions[next_tag] #column\/row tensor of all potentailly the best routes ending in this node in this layer\n                best_tag_id = argmax( next_tag_var) #choose the best one (don't care about emission, as it is the same for all of them ending in this node)\n                bptrs_t.append(best_tag_id) #remember the tag which gives the best route\n                viterbivars_t.append( next_tag_var[0][best_tag_id].view(1) ) #remember the score of the best path\n\n            #Now add in the emission scores, and assign forward_var to the set of viterbi variable we just computed\n            forward_var = ( torch.cat(viterbivars_t) + feat).view(1, -1)\n            backpointers.append( bptrs_t)\n\n        #Transition to STOP_TAG\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG] ] #all of the paths end in the stop_tag which is connected to all of them with one particular edge value\n        best_tag_id = argmax(terminal_var)#choose the shortest path\n        path_score = terminal_var[0][best_tag_id]\n        \n\n        #Follow the backpointers to decode the best path\n        best_path = [best_tag_id]\n        for bptrs_t in reversed(backpointers):\n            best_tag_id = bptrs_t[best_tag_id]\n            best_path.append(best_tag_id) #create a list of indexes of the tags of the best path in reversed order\n            \n        #Pop off the start tag\n        start = best_path.pop()\n        assert start == self.tag_to_ix[START_TAG] #Sanity check\n        best_path.reverse() #reverse the list, so that the indexes are now in correct order\n        return path_score, best_path\n    \n    def structured_perceptron(self, sentence, tags): #A cost function we will use. \n        feats = self._get_lstm_features(sentence) #Compute the features\n        denominator = self._forward_alg(feats)#The log of denominator of our probability, given that the sentence has features feats\n        standard = self._score_sentence(feats, tags)#The log enumerator of our probability, -\/\/-\n        viterbi_score = self._viterbi_decode(feats)[0]\n        return viterbi_score - standard #viterbi_score is the highest score, so this is positive. we want standard to be high, so we punish the algorithm with standard's distnce to viterbi_score\n    \n    def forward(self, sentence):\n        #Get the emission score from BiLSTM\n        lstm_feats = self._get_lstm_features(sentence)\n\n        #Find the best path given the features\n        score, tag_seq = self._viterbi_decode(lstm_feats)\n        return score, tag_seq  ","961106bd":"START_TAG = \"<START>\"\nSTOP_TAG = \"<STOP>\"\nEMBEDDING_DIM = 5\nHIDDEN_DIM = 4\n\n\n# Make up some training data\ntraining_data = [(\n    \"the wall street journal reported today that apple corporation made money\".split(),\n    \"B I I I O O O B I O O\".split()\n), (\n    \"georgia tech is a university in georgia\".split(),\n    \"B I O O O O B\".split()\n)]\n\nword_to_ix = {}\nfor sentence, tag in training_data:\n    for word in sentence:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\n            \ntag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n\nmodel = BiLSTM_CRF(len(word_to_ix), HIDDEN_DIM, tag_to_ix, EMBEDDING_DIM)\noptimizer = optim.SGD( model.parameters(), lr = 0.01, weight_decay = 1e-4)\n\n#Check predictions before training \nwith torch.no_grad():\n    sent = training_data[0][0]\n    precheck_sent = prepare_seq(sent, word_to_ix)\n    tags = training_data[0][1]\n    precheck_tags = torch.tensor( [tag_to_ix[t] for t in tags], dtype=torch.long)\n    predictions = model(precheck_sent)\n    print( predictions, [tag_to_ix[t] for t in training_data[0][1] ])\n    \nepochs = 400\nlosses = []\nfor epoch in range(epochs):\n    model.zero_grad()\n    for sentence, tags in training_data:\n        model.zero_grad()\n        sent = prepare_seq(sentence, word_to_ix)\n        targets = torch.tensor( [tag_to_ix[t] for t in tags], dtype=torch.long)\n        loss = model.structured_perceptron(sent, targets)\n        \n        loss.backward()\n        optimizer.step()\n        \n        losses.append( loss.item() )\n        if epoch % 50 == 0:\n            print( loss.item() )\n        #Surprised that there is no forward in the learning process? And where are we using that long _viterbi_decode function? Look at the explainations below the graph!\n        \nwith torch.no_grad():\n    predictions = model( prepare_seq(training_data[0][0], word_to_ix ) )\n    print( predictions, [tag_to_ix[t] for t in training_data[0][1] ])\n\n        \n        ","3de45134":"import matplotlib.pyplot as plt","46325705":"plt.plot( losses)","8f776f77":"#This work is an implementation based on the code from https:\/\/pytorch.org\/tutorials\/beginner\/nlp\/advanced_tutorial.html\n#My explaination provided in comments are my interpretation of reasoning provided on the website.","bbfa381b":"In the learning process, we take the sentence \"sent\" , and target tags \"targets\", and we immidiatel compute the loss. Even though it seems weird, the deep structure of the function neg_log_likelihood explains this move.\nIn neg_log_likelihood, we are given a sentence \"sent\", and tags \"targets\". Here's what it does step by step.\n1. We compute feats, which are the emission scores (let's keep calling them feats) of this sentence.\n2. We calculate the \"denominator\" value. As I mentioned, it is the logarithm of the denominator of the softmax probabilitiy\/ies for sentences which have these particular feats. I.E, each sentence  with features feats, has a probability of having tags t of a form:  exp( score( feats ,t) )\/exp(denominator )\n3. We calculate the \"enumerator\" value. This is the logarithm of the enumerator of the softmax probability that the sentence s with features feats, has tags targets. This is simply score( feats, targets). Thus the probability that the sentence sent has tags targets i exp(enumerator)\/exp(denominator). As this input of the data set suggssts, thid probability should be high, as sentence sent has tags targets. Thus tha value denominator-enumerator, should be low (it is always positive, as exp(enumerator) contributes to exp(denominator) )\n4. We return the value denominator - enumerator, which as I explained is a good loss. Optimized parametes of the network will learn to assign high probability to features and appropriate targets.\n\n5. Now, we can use viterbi_decode to make predictions. As our network, for every sentence,  assigns the probabilities of it having certain tags sequences, we can find the optimal one with Viterbi algorithm. ","18a17295":"Helper functions","bbd8b4c5":"~Create model","e3794329":"**Training**","7b962aa6":"**BI-LSTM CONDITIONAL RANDOM FIELD**"}}