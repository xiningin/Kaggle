{"cell_type":{"0e349d0b":"code","1564ec11":"code","c64db3b9":"code","871b1c2e":"code","0bb2894e":"code","f206c281":"code","33a03969":"code","5f3322c7":"code","d989943d":"code","2c52f378":"code","5eb960c0":"code","78bdc12f":"code","f0e64656":"code","07cd08bc":"code","59e9aae4":"code","0cc55e4f":"code","365414b2":"code","77c54c0b":"code","bf9a182b":"code","549ec760":"code","465cedb6":"code","ef6b9daa":"code","f9795b8c":"code","3d1bb290":"markdown","ee173035":"markdown","7e075889":"markdown","2b82c75e":"markdown","c821a0db":"markdown","7584574e":"markdown","e792c73d":"markdown","6455d14c":"markdown","d5fed26e":"markdown","30e2c205":"markdown","04ec7ad9":"markdown","c31798d8":"markdown"},"source":{"0e349d0b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Test data is not needed, because it's explanatory analysis. \ntitanic = pd.read_csv(\"..\/input\/train.csv\")\ntitanic.head()","1564ec11":"titanic.info()","c64db3b9":"# Drop columns : only left basic features\ntitanic.drop('PassengerId', axis=1, inplace=True)\ntitanic.drop(\"Name\", axis=1, inplace=True)\ntitanic.drop(\"Ticket\", axis=1, inplace=True)\ntitanic.drop(\"Cabin\", axis=1, inplace=True)","871b1c2e":"# Type change\ntitanic['Pclass'] = titanic['Pclass'].astype(object)","0bb2894e":"# Define categorical and numerical features]\ncategorical_features = ['Pclass', 'Sex', 'Embarked']\nnumerical_feature = ['Age', 'Fare']\nfeatures = categorical_features + numerical_feature\noutcome = 'Survived'","f206c281":"# Plot setting\nsns.set_context(\"paper\")\nsns.set(font='serif')\nsns.set_style(\"white\", {\n    \"font.family\": \"serif\",\n    \"font.serif\": [\"Times\", \"Palatino\", \"serif\"]\n})","33a03969":"total = titanic[features].isnull().sum().sort_values(ascending = False)\npercent = (titanic[features].isnull().sum()\/titanic[features].isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data","5f3322c7":"column_means = titanic['Age'].mean(axis=0)\ntitanic.loc[np.isnan(titanic['Age']), 'Age'] = column_means","d989943d":"for feature in categorical_features : \n    df = titanic.groupby([feature,outcome])[outcome].count().unstack(outcome)\n    df.plot(kind='bar', figsize=(10,5))\n    plt.title(feature)\n    plt.show()","2c52f378":"for feature in numerical_feature : \n    for value in ['0', '1'] : \n        subset = titanic[titanic[outcome] == int(value)]\n        sns.distplot(subset.loc[subset[feature].notnull(), feature], label=value)\n        plt.legend()\n    plt.show()","5eb960c0":"# Make dummy variables\ntitanic_logistic = titanic.copy()\ntitanic_logistic = titanic_logistic.drop(categorical_features, axis=1)\nfor feature in categorical_features : \n    dummy = pd.get_dummies(titanic[feature], columns=feature, prefix=feature)\n    titanic_logistic = pd.concat([titanic_logistic, dummy.iloc[:,1:]], axis=1)      ","78bdc12f":"logistic_features = list(titanic_logistic.columns)\nlogistic_features = list(set(logistic_features) - set(['Survived']))","f0e64656":"import statsmodels.api as sm\ntitanic_logistic['intercept'] = 1\nlogit = sm.Logit(titanic_logistic[outcome].astype(int), titanic_logistic[logistic_features]) \nresult = logit.fit()\n\n# Scipy error fixing..\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n\nprint(result.summary())\nprint(\"Odds ratio\")\nprint(np.exp(result.params))","07cd08bc":"oddsratio = np.exp(result.params)\nplt.figure(figsize=(8,6))\nn_features = len(logistic_features)\nplt.barh(range(n_features), oddsratio, align='center')\nplt.yticks(np.arange(n_features), logistic_features)\nplt.xlabel(\"Odds ratio\")\nplt.ylabel(\"Feature\")\nplt.ylim(-1, n_features)\nplt.show()","59e9aae4":"#  One hot encoding\ntitanic_onehot = titanic.copy()\ntitanic_onehot = titanic_onehot.drop(categorical_features, axis=1)\nfor feature in categorical_features : \n    dummy = pd.get_dummies(titanic[feature], columns=feature, prefix=feature)\n    titanic_onehot= pd.concat([titanic_onehot, dummy], axis=1)  ","0cc55e4f":"onehot_features = list(titanic_onehot.columns)\nonehot_features = list(set(titanic_onehot) - set(['Survived']))","365414b2":"def plot_feature_importances(model, features):\n    plt.figure(figsize=(8,6))\n    n_features = len(features)\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), features)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)","77c54c0b":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth=5, random_state=0)\ntree.fit(titanic_onehot[onehot_features], titanic_onehot[outcome])\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(titanic_onehot[onehot_features], titanic_onehot[outcome])))\nplot_feature_importances(tree, onehot_features)","bf9a182b":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth=3, n_estimators=100, random_state=0)\nrf.fit(titanic_onehot[onehot_features], titanic_onehot[outcome])\nprint('Random Forest - Max Depth = 3')\nprint(\"Accuracy on training set: {:.3f}\".format(rf.score(titanic_onehot[onehot_features], titanic_onehot[outcome].astype('int'))))\nprint('Random Forest Feature Importance')\nplot_feature_importances(rf, onehot_features)","549ec760":"from pandas import read_csv, DataFrame\nimport numpy as np\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\nfrom sklearn.ensemble.partial_dependence import partial_dependence\n\nmodel = GradientBoostingClassifier(n_estimators=20, max_depth=4,learning_rate=0.1, loss='deviance',random_state=1)\nmodel.fit(titanic_onehot[onehot_features], titanic_onehot[outcome])\nimportances = model.feature_importances_\nplot_feature_importances(model, onehot_features)","465cedb6":"import matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (20,20)\n\nfeatures_index = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] # For 1, 5 th features, draw interaction plot\nfig, axs = plot_partial_dependence(model, titanic_onehot[onehot_features], features_index,feature_names=onehot_features,n_jobs=3, grid_resolution=50)\nplt.figure()\nplt.subplots_adjust(bottom=0.1, right=1.1, top=1.4) ","ef6b9daa":"fig = plt.figure()\ntarget_feature = (onehot_features.index(\"Age\"), onehot_features.index(\"Sex_female\"))\npdp, axes = partial_dependence(model, target_feature, X = titanic_onehot[onehot_features], grid_resolution=50)\n\nXX, YY = np.meshgrid(axes[0], axes[1])\nZ = pdp[0].reshape(list(map(np.size, axes))).T\nax = Axes3D(fig)\nsurf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1,cmap=plt.cm.BuPu, edgecolor='k')\n\nax.set_xlabel(onehot_features[target_feature[0]])\nax.set_ylabel(onehot_features[target_feature[1]])\nax.set_zlabel('Partial dependence')\n\nplt.colorbar(surf)\nplt.suptitle('Partial dependence of predictors')\n                 \nplt.subplots_adjust(right=1,top=.9)\nplt.show()","f9795b8c":"from pdpbox import pdp\npdp_inter = pdp.pdp_interact(model,titanic_onehot[onehot_features],features=['Sex_female','Age'], model_features=onehot_features) \npdp.pdp_interact_plot(pdp_inter, ['Sex_female','Age'], plot_type='grid', x_quantile=True, plot_pdp=True) ","3d1bb290":"### GradientBoostingTree Feature importance","ee173035":"### Variable\tDefinition\tKey\n- survival:\tSurvival\t0 = No, 1 = Yes\n- pclass:\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n- sex:\tSex\t\n- Age:\tAge in years\t\n- sibsp:\t# of siblings \/ spouses aboard the Titanic\t\n- parch:\t# of parents \/ children aboard the Titanic\t\n- ticket:\tTicket number\t\n- fare:\tPassenger fare\t\n- cabin:\tCabin number\t\n- embarked:\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton","7e075889":"### Decision Tree Feature importance","2b82c75e":"### Missing values","c821a0db":"### Simple visualization","7584574e":"### Partial Dependence Plot\n- When using black box machine learning algorithms like random forest and boosting, it is hard to understand the relations between predictors and model outcome.\n- Partial dependence plot (PDP) is one way to invenstigate relationship between variable and outcome.\n- The partial dependence plot (PDP or PD plot) shows the marginal effect of a feature on the predicted outcome of a previously fit model (J. H. Friedman 2001). \n- What is partial dependence plot? (https:\/\/www.kaggle.com\/dansbecker\/partial-dependence-plots)\n- python partial dependence plot toolbox (https:\/\/github.com\/SauceCat\/PDPbox)\n\n_Reference_ \n- _interpretable machine learning_ (https:\/\/christophm.github.io\/interpretable-ml-book\/pdp.html)","e792c73d":"### Imputation (Age)\n> Compute column mean except zero and impute the missing values.","6455d14c":"### Logistic regression","d5fed26e":"### Random forest feature importance","30e2c205":"## Feature importance and partial dependence plot in titanic\nIn this kernel, I builded both statistical model and machine learning model and explored relationship between features and survival. <br>","04ec7ad9":"### 2D Interaction plot","c31798d8":"### Interaction plot : Age * Sex"}}