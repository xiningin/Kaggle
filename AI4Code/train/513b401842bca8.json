{"cell_type":{"3b2f03d3":"code","1006edab":"code","bda7dd07":"code","fbabea11":"code","6142192f":"code","d826442d":"code","c1440337":"code","77281cca":"code","5086d2e0":"code","e3e29ffc":"code","02b9226d":"code","f3cdf850":"code","545c81bd":"code","47e56421":"code","ccc7cea5":"code","de891955":"code","9f51bbaf":"code","88b2835f":"code","02d4320e":"code","4c9b382d":"code","5559de34":"code","44c3d20a":"code","b3f816c3":"code","ba3ace31":"code","74e3ec2e":"code","d30a2b60":"code","cc5dd909":"code","13d4e7d2":"code","18b828a4":"code","e27111f7":"code","71ec8d5d":"code","f67bc530":"code","62ed3643":"code","788c388d":"code","631a37cd":"code","7a167163":"code","ae695965":"code","d2bf9201":"code","7fb7bfcc":"code","50b392a8":"code","ee1ad288":"code","0aae8b8c":"code","3c2035e6":"code","4b3c58c1":"code","5dca9bf1":"code","11dd008f":"code","18f4c437":"code","14029277":"code","721ccf58":"code","24480e0f":"code","24a7fa14":"code","93bf8e88":"code","1a8b118d":"code","d7ec964d":"code","0f2b5747":"code","a33c4411":"code","7b1b5118":"code","c197211f":"code","af6f23d5":"code","61963736":"code","eac77d48":"code","dd5fb4b7":"code","013ac47f":"code","30a46dd8":"code","b32bb63b":"code","c13b4b0f":"code","807989f6":"code","2d6b6aa5":"code","5fbda1dd":"code","710c8fe3":"code","faf8b2c8":"code","32440bcc":"code","7957e8aa":"code","bb2e0add":"code","610a61d2":"code","4bf37fa2":"code","e38c896b":"code","183f32bc":"code","8b0470a5":"code","ae878b25":"code","2a54db9f":"code","e26979b5":"code","cb2cebdc":"code","2370cbd1":"code","5fbda7b5":"code","1d2f8ddf":"code","39f81b0a":"code","9b6a6201":"code","a41732aa":"code","48cf151f":"code","ee0deedc":"code","9bc66cab":"code","be6fb656":"code","430f8fc1":"code","6cccdc14":"code","5fce2ebe":"code","a437235a":"markdown","f2bb68c8":"markdown","6e4161fe":"markdown","12328fb0":"markdown","06ba0c27":"markdown","b8721906":"markdown","66bcbf50":"markdown","8bbec74f":"markdown","1e779a6d":"markdown","667d8658":"markdown","5f611b3c":"markdown","53a5d208":"markdown","6cd16a94":"markdown","e072b778":"markdown","82336a45":"markdown","befae419":"markdown","fd29c7a8":"markdown","2dcc6a90":"markdown","1b887239":"markdown","0fc82569":"markdown","bdb98868":"markdown","48dd92a0":"markdown","20957f06":"markdown","970100f2":"markdown","62a8fbcf":"markdown","1cf6e323":"markdown","cdac4c8e":"markdown","56da18b3":"markdown","5817c004":"markdown","a4149565":"markdown","8f595b1f":"markdown","a3d1123f":"markdown","b44ea804":"markdown","190e8986":"markdown","170679a5":"markdown","d53dfaf5":"markdown","7f6844c9":"markdown","f7c1753b":"markdown","29213968":"markdown","5b464b49":"markdown","fccd324a":"markdown","0a8317df":"markdown","687c0530":"markdown","32542842":"markdown","18075e48":"markdown","65cca7cb":"markdown","7000ddca":"markdown","c62d1a70":"markdown","63bcf0c4":"markdown","980b6330":"markdown","7a240d6d":"markdown","85db5fc9":"markdown","ad691e1c":"markdown","566b00b2":"markdown","de1de86d":"markdown","e79c99ff":"markdown","9b1471ea":"markdown","9d9f9822":"markdown","8c967c21":"markdown","c3fcacfb":"markdown"},"source":{"3b2f03d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport gc\nimport pickle\nfrom numpy.fft import *\n\nfrom IPython.display import FileLink\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\ndirname = None\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nprint(\"Diret\u00f3rio: \" + dirname)\n# Any results you write to the current directory are saved as output.","1006edab":"from matplotlib import pyplot as plt\nimport matplotlib.style as style \nimport seaborn as sns\n\nstyle.use('ggplot')\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\nimport plotly.offline as py \nfrom plotly.offline import init_notebook_mode, iplot\npy.init_notebook_mode(connected=True) # this code, allow us to work with offline plotly version\nimport plotly.graph_objs as go # it's like \"plt\" of matplot\n\nsns.set()\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","bda7dd07":"import itertools\nimport pandasql as ps\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport multiprocessing as mp\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import Ridge, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import Imputer, MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import r2_score, make_scorer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nfrom hyperopt import hp\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials","fbabea11":"models = []\nclasses = []\ncpus = mp.cpu_count()\ncategorie = LabelEncoder()\nresults = pd.DataFrame(columns=['Name', 'Model', 'Predict', 'Accuracy', 'Score', 'File', 'Value'])","6142192f":"models.append((\"LogisticRegression\",LogisticRegression(solver='lbfgs', multi_class='multinomial')))\nmodels.append((\"Ridge\",Ridge()))\nmodels.append((\"SVC\",SVC()))\nmodels.append((\"LinearSVC\",LinearSVC()))\nmodels.append((\"KNeighbors\",KNeighborsClassifier()))\nmodels.append((\"DecisionTree\",DecisionTreeClassifier()))\nmodels.append((\"ExtraTrees\",ExtraTreesClassifier()))\nmodels.append((\"RandomForest\",RandomForestClassifier()))\nmodels.append((\"RandomForest_entropy\",RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=10, random_state=0, max_features=None)))\nmodels.append((\"RandomForest_gini\",RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=10, random_state=0, max_features=None)))\nmodels.append((\"MLPClassifier\",MLPClassifier(solver='lbfgs', random_state=0)))\n\nmodels.append((\"XGBoost\",XGBClassifier()))\nmodels.append((\"XGBoost_XGB\",xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, subsample=0.5)))\nmodels.append((\"XGBoost_Pipeline\",Pipeline(steps=[('xgboost', xgb.XGBClassifier(objective='multi:softmax',num_class=3))])))","d826442d":"# Visualiza\u00e7\u00e3o da matr\u00edz de correla\u00e7\u00e3o entre os dados - Simples\ndef correlation(df):\n    corr = df.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(corr, cmap = cmap, square=True, cbar_kws={ 'shrink' : .9 }, ax=ax, annot = True, annot_kws = {'fontsize' : 8 })\n    \n# Visualiza\u00e7\u00e3o da matr\u00edz de correla\u00e7\u00e3o entre os dados - Completa\ndef heatmap(df, title, col):\n    fig, ax = plt.subplots(1,1, figsize = (15,6))\n\n    hm = sns.heatmap(df.iloc[:,col:].corr(),\n                     ax = ax, cmap = 'coolwarm',\n                     annot = True, fmt = '.2f',\n                     linewidths = 0.05)\n    fig.subplots_adjust(top=0.93)\n    fig.suptitle(title, fontsize=12, fontweight='bold')\n    \n# Histograma com a compara\u00e7\u00e3o da vari\u00e1veis de teste e treino\ndef comparefeatures(dtrain, dtest, col):\n    plt.figure(figsize=(26, 16))\n    \n    for i, col in enumerate(dtrain.columns[col:]):\n        ax = plt.subplot(3, 4, i + 1)\n        sns.distplot(dtrain[col], bins=100, label='train')\n        sns.distplot(dtest[col], bins=100, label='test')\n        ax.legend()  \n\n# Detectando valores incompletos, nulos e erros\ndef missing_values_table(df, clean = False):\n    mis_val = df.isnull().sum()        \n    mis_val_percent = 100 * df.isnull().sum() \/ len(df)        \n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)        \n    mis_val_table_ren_columns = mis_val_table.rename(\n    columns = {0 : 'Missing Values', 1 : '% of Total Values'})        \n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n    '% of Total Values', ascending=False).round(1)\n\n    print (\"O dataframe tem \" + str(df.shape[1]) + \" colunas.\\n\"      \n        \"H\u00e1 \" + str(mis_val_table_ren_columns.shape[0]) + \" colunas que possuem valores ausentes.\")\n\n    if clean and mis_val_table_ren_columns.shape[0] > 0:\n        df.fillna(0, inplace = True)\n        df.replace(-np.inf, 0, inplace = True)\n        df.replace(np.inf, 0, inplace = True)\n        mis_val_table_ren_columns = 0\n        print(\"O dataframe foi limpo.\")\n            \n    return mis_val_table_ren_columns\n    \n# Apresenando subgr\u00e1ficos para compara\u00e7\u00e3o das colunas de um dataset\ndef plot_subplots(df, columns):\n    plt.subplots(figsize=(18,15))\n    length=len(columns)\n\n    for i,j in zip(columns,range(length)):\n        plt.subplot((length\/2),3,j+1)\n        plt.subplots_adjust(wspace=0.2,hspace=0.5)\n        df[i].hist(bins=20,edgecolor='black')\n        plt.title(i)\n    plt.show()\n\n# Apresentando subgr\u00e1ficos de boxplot para verifi\u00e7\u00e3o do outliers nos dados\ndef plot_boxplots(df, layout):\n    df.plot(kind='box', subplots=True, layout=layout, figsize=(18,15), sharex=False, sharey=False)\n    plt.show()\n    \n# Apresentando a correla\u00e7\u00e3o\ndef correlation(df):\n    corr = df.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(corr, cmap = cmap, square=True, cbar_kws={ 'shrink' : .9 }, ax=ax, annot = True, annot_kws = {'fontsize' : 8 })\n\n# From: Code Snippet For Visualizing Series Id by @shaz13\ndef plotSeries(series_id, X, y, col = 3):\n    style.use('ggplot')\n    plt.figure(figsize=(28, 16))\n    print(y[y['series_id'] == series_id]['surface'].values[0].title())\n    \n    series_dict = {}\n    for series in (X['series_id'].unique()):\n        series_dict[series] = X[X['series_id'] == series] \n    \n    for i, col in enumerate(series_dict[series_id].columns[col:]):\n        if col.startswith(\"o\"):\n            color = 'red'\n        elif col.startswith(\"a\"):\n            color = 'green'\n        else:\n            color = 'blue'\n        if i >= 7:\n            i+=1\n        plt.subplot(3, 4, i + 1)\n        plt.plot(series_dict[series_id][col], color=color, linewidth=3)\n        plt.title(col)    \n\n# Complementando com a s\u00e9rie de Fourier os datasets de densidade\ndef data_denoised(target, data):\n    denoised = pd.DataFrame()\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            denoised[col] = data.groupby([target])[col]\n        else:\n            # Apply filter_signal function to the data in each series\n            denoised_data = data.groupby([target])[col].apply(lambda x: filter_signal(x))\n\n            # Assign the denoised data back to X_denoised\n            list_denoised_data = []\n            for arr in denoised_data:\n                for val in arr:\n                    list_denoised_data.append(val)\n\n            denoised[col] = list_denoised_data\n\n    return denoised   \n\n#https:\/\/en.wikipedia.org\/wiki\/Conversion_between_quaternions_and_Euler_angles\n#quaternion to eular\ndef quaternion_to_euler(qx,qy,qz,qw):\n    import math\n    # roll (x-axis rotation)\n    sinr_cosp = +2.0 * (qw * qx + qy + qz)\n    cosr_cosp = +1.0 - 2.0 * (qx * qx + qy * qy)\n    roll = math.atan2(sinr_cosp, cosr_cosp)\n    \n    # pitch (y-axis rotation)\n    sinp = +2.0 * (qw * qy - qz * qx)\n    if(math.fabs(sinp) >= 1):\n        pitch = copysign(M_PI\/2, sinp)\n    else:\n        pitch = math.asin(sinp)\n        \n    # yaw (z-axis rotation)\n    siny_cosp = +2.0 * (qw * qz + qx * qy)\n    cosy_cosp = +1.0 - 2.0 * (qy * qy + qz * qz)\n    yaw = math.atan2(siny_cosp, cosy_cosp)\n    \n    return roll, pitch, yaw            \n\n#eular angle\ndef eular_angle(data):\n    x, y, z, w = data['orientation_X'].tolist(), data['orientation_Y'].tolist(), data['orientation_Z'].tolist(), data['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    data['euler_x'] = nx\n    data['euler_y'] = ny\n    data['euler_z'] = nz\n    \n    return data\n\n# from @theoviel at https:\/\/www.kaggle.com\/theoviel\/fast-fourier-transform-denoising\ndef filter_signal(signal, threshold=1e3):\n    fourier = rfft(signal)\n    frequencies = rfftfreq(signal.size, d=20e-3\/signal.size)\n    fourier[frequencies > threshold] = 0\n    return irfft(fourier)\n\n# Exclu\u00edndo valores duplicados\ndef drop_duplicate(df, columns = None):\n    \n    if columns:\n        df.drop(columns = columns, inplace = True) \n    df.drop_duplicates()\n    df.info()\n    return df\n\n# Convertendo uma coluna em valor categ\u00f3rico\ndef dummie_transform(df, column):\n    df[column] = categorie.fit_transform(df[column].astype(str))\n    return df\n\n# Revertendo uma coluna de valor categ\u00f3rico\ndef dummie_untransform(array):\n    #df = categorie.inverse_transform(array)\n    df = categorie.inverse_transform(array.argmax(axis=1))\n    return df\n\n# Transforma\u00e7\u00e3o e ajuste das escalas dos dados\ndef scaler_transform(df, col, default = True):\n   \n    for col in df.columns[col:]:\n        if default:\n            scaler = StandardScaler()\n        else:\n            scaler = MinMaxScaler(feature_range=(0, 1))\n\n        df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n        \n    return df\n\n# Feature Engineer\ndef feature_engineer(data):\n    df = pd.DataFrame()\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue\n            \n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] \/ df[col + '_min']\n        df[col + '_mad'] = data.groupby(['series_id'])[col].apply(lambda x: np.median(np.abs(np.diff(x))))\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])\/2\n    return df    \n\n# Salvando o modelo treinado\ndef write_pickle(model, file):\n    file = file + \".pkl\"\n    with open(file, mode='wb') as f:\n        pickle.dump(model, f)\n    return file\n\n# Recuperando um valor treinado\ndef read_pickle(file):\n    file = file + \".pkl\"\n    with open(file, mode='rb') as f:\n        model = pickle.load(f)    \n    return model\n\n# Executando o modelo para compara\u00e7\u00e3o\ndef prediction(models, Xtrain, ytrain, iterable = 10):\n    # make the model for train data\n    print(\"In\u00edcio do treinamento do modelo: \" + name + \"\\n\")\n    \n    X_train, X_test, y_train, y_test = train_test_split(Xtrain, ytrain, train_size=0.70, test_size=0.30, random_state=2011, shuffle=True)\n    \n    for name, model in models:\n        model.fit(X_train, y_train)\n\n        print(\"....Writing model in pickle\")\n        file = write_pickle(model, name)        \n\n        # make predictions for test data\n        predict = model.predict(X_test)\n\n        # evaluate predictions\n        score = model.score(X_train, y_train)  \n        print(\"....Model Score: %.2f%%\" % (score * 100.0))\n        \n        # evaluate predictions\n        accuracy = accuracy_score(y_test, predict)    \n        print(\"....Accuracy Score: %.2f%%\" % (accuracy * 100.0))\n\n        # Implementing your own scoring\n        scores = cross_val_score(model, X_train, y_train, cv=iterable)\n        print(\"....Cross Validation Score: %.2f%%\" % (scores.mean() * 100.0))\n\n        results.append({'Name': name, 'Model': model, 'Predict': predict, 'Accuracy': accuracy, 'Score': scores, 'File': file, 'Value': scores.mean()}, ignore_index=True)\n        print(\"Modelo treinado!\")\n    \n    print(\"\\n T\u00e9rmino do treinamento dos modelos\")\n\n    # Limpeza da Mem\u00f3ria\n    gc.collect()    \n    \n    # Apresentando o resultado\n    results\n    return predict\n\n# Executando a lista de modelos para encontra o melhor resultado\ndef all_prediction(models, Xtrain, ytrain, iterable = 10):\n    # make the model for train data    \n    print(\"In\u00edcio do treinamento dos modelos \\n\")\n    \n    X_train, X_test, y_train, y_test = train_test_split(Xtrain, ytrain, train_size=0.70, test_size=0.30, random_state=2011, shuffle=True)\n\n    for name, model in models:\n        print(\"....Treinando o Modelo \" + name)\n        \n        model.fit(X_train, y_train)\n        \n        print(\"....Writing model in pickle\")\n        file = write_pickle(model, name)        \n        \n        # make predictions for test data\n        predict = model.predict(X_test)\n\n        # evaluate predictions\n        score = model.score(X_train, y_train)  \n        print(\"....Model Score: %.2f%%\" % (score * 100.0))\n        \n        # evaluate predictions\n        accuracy = accuracy_score(y_test, predict)\n        print(\"....Accuracy Score: %.2f%%\" % (accuracy * 100.0))\n        \n        # Implementing your own scoring\n        scores = cross_val_score(model, X_train, y_train['surface'], cv=iterable)\n        print(\"....Cross Validation Score: %.2f%%\" % (scores.mean() * 100.0))\n       \n        results.append({'Name:': name, 'Model': model, 'Predict': predict, 'Accuracy': accuracy, 'Score': scores, 'File': file, 'Value': scores.mean()}, ignore_index=True)\n        print(\"....Modelo \" + name + \" treinado!\\n\")\n\n        # Limpeza da Mem\u00f3ria\n        gc.collect()\n        \n    print(\"\\n T\u00e9rmino do treinamento dos modelos\")\n\n    # Apresentando o resultado\n    results\n    return results\n    \ndef split_prediction(models, X_train, y_train, X_test, iterable = 10):\n    # make the model for train data    \n    print(\"In\u00edcio do treinamento dos modelos \\n\")\n\n    folds = StratifiedKFold(n_splits=iterable, shuffle=True, random_state=2011)\n    predicted = np.zeros((X_test.shape[0],9))\n    measured = np.zeros((X_train.shape[0]))\n    score = 0\n    \n    for name, model in models:\n        \n        for times, (trn_idx, val_idx) in enumerate(folds.split(X_data.values, y_train.values)):\n            model.fit(X_data.iloc[trn_idx], y_train[trn_idx])\n            measured[val_idx] = model.predict(X_data.iloc[val_idx])\n            predict += model.predict_proba(X_test) \/ folds.n_splits\n            score += model.score(X_data.iloc[val_idx], y_train[val_idx])\n\n            print(\"....Fold: {} score: {}\".format(times, model.score(X_data.iloc[val_idx], y_train[val_idx])))\n\n            # Implementing your own scoring\n            scores += cross_val_score(model, X_train, y_train, cv=iterable)\n            print(\"....Cross Validation Score: %.2f%%\" % (scores.mean() * 100.0))\n\n        results.append({'Name:': name, 'Model': model, 'Predict': predict, 'Accuracy': None, 'Score': (scores \/ folds.n_splits), 'File': file, 'Value': scores.mean()}, ignore_index=True)\n        print(\"....Modelo \" + name + \" treinado!\\n\")\n\n        print('\\n Average score', score \/ folds.n_splits)\n        \n        # Limpeza da Mem\u00f3ria\n        gc.collect()\n\n    print(\"\\n T\u00e9rmino do treinamento dos modelos\")\n    # Apresentando o resultado\n    results\n    return results\n","c1440337":"# Carregando os dados de treino e qualificando as vari\u00e1veis por tipo para padronizar\n\nfX_train = pd.read_csv(dirname + '\/X_treino.csv', low_memory=False\n                    ,dtype = {'series_id': np.int16,'measurement_number': np.int16\n                              ,'orientation_X': np.float32,'orientation_X': np.float32\n                              ,'orientation_Y': np.float32,'orientation_Z': np.float32\n                              ,'orientation_W': np.float32,'angular_velocity_X': np.float32\n                              ,'angular_velocity_Y': np.float32,'angular_velocity_Z': np.float32\n                              ,'linear_acceleration_X': np.float32,'linear_acceleration_Y': np.float32\n                              ,'linear_acceleration_Z': np.float32})\n\nfy_train = pd.read_csv(dirname + '\/y_treino.csv', low_memory=False\n                      ,dtype = {'series_id': np.int16,'group_id': np.int16, 'surface': np.str})\n\nfX_test = pd.read_csv(dirname + '\/X_teste.csv', low_memory=False\n                    ,dtype = {'series_id': np.int16,'measurement_number': np.int16\n                              ,'orientation_X': np.float32,'orientation_X': np.float32\n                              ,'orientation_Y': np.float32,'orientation_Z': np.float32\n                              ,'orientation_W': np.float32,'angular_velocity_X': np.float32\n                              ,'angular_velocity_Y': np.float32,'angular_velocity_Z': np.float32\n                              ,'linear_acceleration_X': np.float32,'linear_acceleration_Y': np.float32\n                              ,'linear_acceleration_Z': np.float32})\n\nf_submission = pd.read_csv(dirname + '\/sample_submission.csv', low_memory=False\n                      ,dtype = {'series_id': np.int16, 'surface': np.str})","77281cca":"print('\\nCaracter\u00edsticas do dado de teste')\nprint('    Quantidade de dados: {0}\\n    Quantidade de caracter\u00edsticas: {1}'.format(fX_test.shape[0], fX_test.shape[1]))\n\nprint('Caracter\u00edsticas do dado de treino')\nprint('    Quantidade de dados: {0}\\n    Quantidade de caracter\u00edsticas: {1}'.format(fX_train.shape[0], fX_train.shape[1]))\n\nprint('\\nCaracter\u00edsticas do dado alvo')\nprint('    Quantidade de dados: {0}\\n    Quantidade de caracter\u00edsticas: {1}'.format(fy_train.shape[0], fy_train.shape[1]))\n\nprint('\\nCaracter\u00edsticas do dado de submiss\u00e3o')\nprint('    Quantidade de dados: {0}\\n    Quantidade de caracter\u00edsticas: {1}'.format(f_submission.shape[0], f_submission.shape[1]))","5086d2e0":"fX_test.describe(include='all').T","e3e29ffc":"fX_train.describe(include='all').T","02b9226d":"fy_train.describe(include='all').T","f3cdf850":"f_submission.describe(include='all').T","545c81bd":"#sns.countplot(y='surface',data = y_train)\ntrace0 = go.Pie(\n    labels = fy_train['surface'].value_counts().index,\n    values = fy_train['surface'].value_counts().values,\n    domain = {'x':[0.55,1]})\n\ntrace1 = go.Bar(\n    x = fy_train['surface'].value_counts().index,\n    y = fy_train['surface'].value_counts().values\n    )\n\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title = 'Distribui\u00e7\u00e3o da frequ\u00eancia por tipo de superf\u00edcie',\n    xaxis = dict(domain = [0,.50]))\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","47e56421":"heatmap(fX_test, 'Correla\u00e7\u00e3o entre as vari\u00e1veis independentes de teste', 3)","ccc7cea5":"heatmap(fX_train, 'Correla\u00e7\u00e3o entre as vari\u00e1veis independentes de treino', 3)","de891955":"X_train","9f51bbaf":"fig = plt.figure(figsize=(15,15))\n\nax = fig.add_subplot(1)\nax.set_title('Distribution of Orientation_X,Y,Z,W',\n             fontsize=14, \n             fontweight='bold')\nX_train.iloc[:,1:5].boxplot()\n\nax = fig.add_subplot(2)\nax.set_title('Distribution of Angular_Velocity_X,Y,Z',\n             fontsize=14, \n             fontweight='bold')\nX_train.iloc[:,5:8].boxplot()\n\nax = fig.add_subplot(3)\nax.set_title('Distribution of linear_accelaration_X,Y,Z',\n             fontsize=14, \n             fontweight='bold')\nX_train.iloc[:,8:11].boxplot()","88b2835f":"comparefeatures(fX_train, fX_test, 3)","02d4320e":"# Checando valores missing do dataset de teste\nmissing_values_table(fX_test)","4c9b382d":"# Checando valores missing do dataset de treino\nmissing_values_table(fX_train)","5559de34":"# Checando valores missing do dataset de resultado\nmissing_values_table(fy_train)","44c3d20a":"X_test = drop_duplicate(fX_test)","b3f816c3":"X_train = drop_duplicate(fX_train)","ba3ace31":"y_train = drop_duplicate(fy_train)","74e3ec2e":"# Juntando os dados Treino\ndf_merge = pd.merge(fX_train, fy_train, how=\"inner\", on=\"series_id\")\ndf_merge.describe(include='all').T","d30a2b60":"classes = fy_train.surface.value_counts()\nclasses","cc5dd909":"query = \"\"\" SELECT t.series_id, \n                   t.orientation_X, t.orientation_Y, t.orientation_Z, t.orientation_W, \n                   t.angular_velocity_X, t.angular_velocity_Y, t.angular_velocity_Z, \n                   t.linear_acceleration_X, t.linear_acceleration_Y, t.linear_acceleration_Z\n            FROM X_test as t\n            INNER JOIN (SELECT series_id, MIN(measurement_number) as measurement_number, COUNT() as qtde\n                        FROM X_test\n                        GROUP BY series_id) x\n                ON t.series_id = x.series_id and t.measurement_number = x.measurement_number\"\"\"\n\nX_test_min = ps.sqldf(query, locals())\nX_test_min","13d4e7d2":"query = \"\"\" SELECT t.series_id, \n                   t.orientation_X, t.orientation_Y, t.orientation_Z, t.orientation_W, \n                   t.angular_velocity_X, t.angular_velocity_Y, t.angular_velocity_Z, \n                   t.linear_acceleration_X, t.linear_acceleration_Y, t.linear_acceleration_Z\n            FROM X_test as t\n            INNER JOIN (SELECT series_id, MAX(measurement_number) as measurement_number, COUNT() as qtde\n                        FROM X_test\n                        GROUP BY series_id) x\n                ON t.series_id = x.series_id and t.measurement_number = x.measurement_number\"\"\"\n\nX_test_max = ps.sqldf(query, locals())\nX_test_max","18b828a4":"query = \"\"\" SELECT t.series_id, \n                   t.surface\n            FROM df_merge as t\n            INNER JOIN (SELECT series_id, MIN(measurement_number) as measurement_number\n                        FROM df_merge\n                        GROUP BY series_id) x\n                ON t.series_id = x.series_id and t.measurement_number = x.measurement_number\"\"\"\n\ny_train_min = ps.sqldf(query, locals())\ny_train_min","e27111f7":"query = \"\"\" SELECT t.series_id, \n                   t.surface\n            FROM df_merge as t\n            INNER JOIN (SELECT series_id, MAX(measurement_number) as measurement_number\n                        FROM df_merge\n                        GROUP BY series_id) x\n                ON t.series_id = x.series_id and t.measurement_number = x.measurement_number\"\"\"\n\ny_train_max = ps.sqldf(query, locals())\ny_train_max","71ec8d5d":"X_test = X_test.drop([\"row_id\", \"measurement_number\"], axis=1)\nX_test","f67bc530":"y_train = df_merge[[\"series_id\", \"surface\"]].copy()\ny_train","62ed3643":"X_train = df_merge.copy().drop([\"row_id\", \"measurement_number\", \"group_id\", \"surface\"], axis=1)\nX_train","788c388d":"X_test_scaler = scaler_transform(X_test, 3, default = True)\nX_test_scaler_min = scaler_transform(X_test_min, 3, default = True)\nX_test_scaler_max = scaler_transform(X_test_max, 3, default = True)","631a37cd":"X_train_scaler = scaler_transform(X_train, 3, default = True)","7a167163":"plotSeries(10, X_train_scaler, y_train)","ae695965":"comparefeatures(X_test, X_test_scaler, 3)","d2bf9201":"comparefeatures(X_train, X_train_scaler, 3)","7fb7bfcc":"#X_test_denoised = data_denoised1(\"series_id\", X_test_scaler)\n#X_test_denoised","50b392a8":"#X_test_denoised = data_denoised(\"series_id\", X_test_scaler)\n#X_test_denoised_min = data_denoised(\"series_id\", X_test_scaler_min)\n#X_test_denoised_max = data_denoised(\"series_id\", X_test_scaler_max)","ee1ad288":"#X_train_denoised = data_denoised(\"series_id\", X_train_scaler)","0aae8b8c":"#plotSeries(10, X_train_denoised, y_train)","3c2035e6":"#comparefeatures(X_test, X_test_denoised, 3)","4b3c58c1":"#comparefeatures(X_train, X_train_denoised, 3)","5dca9bf1":"X_test_eular = eular_angle(X_test_scaler)\nX_test_eular_min = eular_angle(X_test_scaler_min)\nX_test_eular_max = eular_angle(X_test_scaler_max)","11dd008f":"X_train_eular = eular_angle(X_train_scaler)","18f4c437":"plotSeries(10, X_train_eular, y_train)","14029277":"comparefeatures(X_test, X_test_eular, 3)","721ccf58":"comparefeatures(X_train, X_train_eular, 3)","24480e0f":"X_test_engineer = feature_engineer(X_test)\nX_test_engineer","24a7fa14":"X_train_engineer = feature_engineer(X_train)\nX_train_engineer","93bf8e88":"y_train = dummie_transform(y_train, \"surface\")\ny_train.head(5)","1a8b118d":"def prediction1(models, Xtrain, ytrain, iterable = 10):\n    print(\"Iniciando o modelo\")\n    \n    X_train, X_test, y_train, y_test = train_test_split(Xtrain, ytrain, train_size=0.70, test_size=0.30, random_state=2011, shuffle=True)\n    \n    for name, model in models:\n        model.fit(X_train, y_train)\n\n        print(\"....Writing model in pickle\")\n        file = write_pickle(model, name)        \n\n        # make predictions for test data\n        predict = model.predict(X_test)\n\n        # evaluate predictions\n        score = model.score(X_train, y_train)  \n        print(\"....Model Score: %.2f%%\" % (score * 100.0))\n        \n        # evaluate predictions\n        accuracy = accuracy_score(y_test, predict)    \n        print(\"....Accuracy Score: %.2f%%\" % (accuracy * 100.0))\n\n        # Implementing your own scoring\n        scores = cross_val_score(model, X_train, y_train, cv=iterable)\n        print(\"....Cross Validation Score: %.2f%%\" % (scores.mean() * 100.0))\n\n        results.append({'Name': name, 'Model': model, 'Predict': predict, 'Accuracy': accuracy, 'Score': scores, 'File': file, 'Value': scores.mean()}, ignore_index=True)\n        print(\"Modelo treinado!\")\n    \n    print(\"\\n T\u00e9rmino do treinamento dos modelos\")\n\n    # Limpeza da Mem\u00f3ria\n    gc.collect()    \n    \n    # Apresentando o resultado\n    results\n    return predict","d7ec964d":"models1 = []\nmodels1.append((\"RandomForest_entropy\",RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=10, random_state=0, max_features=None)))\n\npool = mp.Pool(mp.cpu_count())\n\nresults = pool.map(prediction1, [models1, X_train, y_train['surface'], 10])\n\npool.close()\n","0f2b5747":"importances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis = 0)\nindices = np.argsort(importances)[::-1]","a33c4411":"feature_importances = pd.DataFrame(importances, index = X_data.columns, columns = ['importance'])\nfeature_importances.sort_values('importance', ascending = False)\nfeature_importances.head(20)","7b1b5118":"less_important_features = feature_importances.loc[feature_importances['importance'] < 0.0025]\nprint('There are {0} features their importance value is less then 0.0025'.format(less_important_features.shape[0]))","c197211f":"#Remove less important features from train and test set.\nfor i, col in enumerate(less_important_features.index):\n    X_data = X_data.drop(columns = [col], axis = 1)\n    X_test = X_test.drop(columns = [col], axis = 1)\n    \nX_data.shape, X_test.shape","af6f23d5":"predicted = np.zeros((X_test.shape[0],9))\nmeasured= np.zeros((X_data.shape[0]))\nscore = 0\n\nfor times, (trn_idx, val_idx) in enumerate(folds.split(X_data.values, y_train['surface'].values)):\n    model = RandomForestClassifier(n_estimators=700, n_jobs = -1)\n    model.fit(X_data.iloc[trn_idx], y_train['surface'][trn_idx])\n    measured[val_idx] = model.predict(X_data.iloc[val_idx])\n    predicted += model.predict_proba(X_test) \/ folds.n_splits\n    score += model.score(X_data.iloc[val_idx], y_train['surface'][val_idx])\n    \n    print(\"Fold: {} score: {}\".format(times,model.score(X_data.iloc[val_idx], y_train['surface'][val_idx])))\n    gc.collect()\n    \nprint('\\n Average score', score \/ folds.n_splits)","61963736":"# Treinando o modelo com base em uma lista de modelos para compara\u00e7\u00e3o\ndef prediction1(model, X_train, y_train, X_test, iterable = 10):\n    # make the model for train data\n    name = type(model).__name__\n\n    print(\"In\u00edcio do treinamento do modelo: \" + name + \"\\n\")\n    X_train, X_test, y_train, y_test = train_test_split(sX_train, sy_train, train_size=0.70, test_size=0.30, random_state=2011, shuffle=True)\n    \n    for name, model in models:\n        model.fit(X_train, y_train)\n\n        print(\"....Writing model in pickle\")\n        file = write_pickle(model, name)        \n\n        # make predictions for test data\n        predict = model.predict(X_test)\n\n        # evaluate predictions\n        score = model.score(X_train, y_train)  \n        print(\"....Model Score: %.2f%%\" % (score * 100.0))\n        \n        # evaluate predictions\n        accuracy = accuracy_score(y_test, predict)    \n        print(\"....Accuracy Score: %.2f%%\" % (accuracy * 100.0))\n\n        # Implementing your own scoring\n        scores = cross_val_score(model, X_train, y_train, cv=iterable)\n        print(\"....Cross Validation Score: %.2f%%\" % (scores.mean() * 100.0))\n\n        results.append({'Name': name, 'Model': model, 'Predict': predict, 'Accuracy': accuracy, 'Score': scores, 'File': file, 'Value': scores.mean()}, ignore_index=True)\n        print(\"Modelo treinado!\")\n    \n    print(\"\\n T\u00e9rmino do treinamento dos modelos\")\n\n    # Limpeza da Mem\u00f3ria\n    gc.collect()    \n    \n    # Apresentando o resultado\n    results\n    return predict","eac77d48":"# make the model for train data\nmodel = RandomForestClassifier(criterion='entropy')\npredicted = prediction1(model, X_data, y_train['surface'], X_test, iterable = 10)","dd5fb4b7":"_submission = submission.copy()\nfilename = r'submission_2.csv'","013ac47f":"_submission['surface'] = dummie_untransform(predicted)\n_submission.to_csv(filename, index=False)\n_submission.head(10)\n\nprint('Saved file: ' + filename)\n\nFileLink(filename)","30a46dd8":"columns = ['row_id', 'series_id']\ndf_test = drop_duplicate(df_test, columns)","b32bb63b":"columns = ['series_id', 'row_id', 'group_id']\ndf_train = drop_duplicate(df_train, columns)","c13b4b0f":"# An\u00e1lise dos dados - Variaveis independentes de teste\ncolumns=df_test.columns[1:10]\nplot_subplots(df_test, columns)","807989f6":"# Gr\u00e1fico Boxplot - Teste\nplot_boxplots(df_test, (3,4))","2d6b6aa5":"# An\u00e1lise dos dados - Variaveis independentes de treino\ncolumns=df_train.columns[1:10]\nplot_subplots(df_train, columns)","5fbda1dd":"# Gr\u00e1fico Boxplot - Teste\nplot_boxplots(df_train, (3,4))","710c8fe3":"df_train['surface'].value_counts(normalize=True).plot(kind='bar', figsize=(12,8))\nplt.title('Superficies - Surface')\nplt.xlabel('Superf\u00edcie')\nplt.ylabel('Frequ\u00eancia')\nplt.show()","faf8b2c8":"## Limpeza da Mem\u00f3ria\ngc.collect()","32440bcc":"correlation(df_train)","7957e8aa":"df_train = dummie_transform(df_train, \"surface\")\ndf_train.head(5)","bb2e0add":"# Dataset de treino\nX_train = df_train.drop(['surface', 'surface_'],axis=1)\nX_train = scaler_transform(X_train)\ny_train = df_train['surface_']","610a61d2":"# Dataset de teste\nX_test = scaler_transform(df_test)","4bf37fa2":"param = {\n        'num_leaves': 7,\n        'max_bin': 119,\n        'min_data_in_leaf': 6,\n        'learning_rate': 0.03,\n        'min_sum_hessian_in_leaf': 0.00245,\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'feature_fraction': 0.05,\n        'lambda_l1': 4.972,\n        'lambda_l2': 2.276,\n        'min_gain_to_split': 0.65,\n        'max_depth': 14,\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0\n    }\n\nmodels.append((\"LogisticRegression\",LogisticRegression(solver='lbfgs', multi_class='multinomial')))\nmodels.append((\"Ridge\",Ridge()))\nmodels.append((\"SVC\",SVC()))\nmodels.append((\"LinearSVC\",LinearSVC()))\nmodels.append((\"KNeighbors\",KNeighborsClassifier()))\nmodels.append((\"DecisionTree\",DecisionTreeClassifier()))\nmodels.append((\"ExtraTrees\",ExtraTreesClassifier()))\nmodels.append((\"RandomForest\",RandomForestClassifier()))\nmodels.append((\"RandomForest_entropy\",RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=10, random_state=0, max_features=None)))\nmodels.append((\"RandomForest_gini\",RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=10, random_state=0, max_features=None)))\nmodels.append((\"MLPClassifier\",MLPClassifier(solver='lbfgs', random_state=0)))\n\n#models.append((\"LightGBM\", lgb.train(param, xg_train, 5000, valid_sets = [xg_valid], verbose_eval=500, early_stopping_rounds = 250)))\n\nmodels.append((\"XGBoost\",XGBClassifier()))\nmodels.append((\"XGBoost_XGB\",xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, subsample=0.5)))\nmodels.append((\"XGBoost_Pipeline\",Pipeline(steps=[('xgboost', xgb.XGBClassifier(objective='multi:softmax',num_class=3))])))","e38c896b":"# X_train, X_test, y_train, y_test = train_test_split(sX_train, sy_train, train_size=0.75, test_size=0.25, random_state=2011, shuffle=True)","183f32bc":"## Limpeza da Mem\u00f3ria\ngc.collect()","8b0470a5":"all_prediction(models)","ae878b25":"# make the model for train data\nmodel = LogisticRegression(multi_class='multinomial')\nprediction(model, iterable = 10)","2a54db9f":"results","e26979b5":"# make the model for train data\nmodel = RandomForestClassifier(criterion='entropy')\nprediction(model, iterable = 10)","cb2cebdc":"# make the model for train data\nmodel = Ridge()\ndf_results = prediction(model, df_results, iterable = 10)","2370cbd1":"# make the model for train data\nmodel = KNeighborsClassifier()\ndf_results = prediction(model, df_results, iterable = 10)","5fbda7b5":"xg_train = xgb.DMatrix(X_train, label=y_train)\nxg_test = xgb.DMatrix(X_test, label=y_test)\nxg_train.save_binary('train.buffer')\nxg_test.save_binary('train.buffer')\n\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\nparam['silent'] = 1 # cleans up the output\nparam['num_class'] = 3 # number of classes in target label\n\nxg_list = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 30","1d2f8ddf":"# make the model for train data\nmodel = XGBClassifier()\ndf_results = prediction(model, df_results, iterable = 10)","39f81b0a":"# creating a confusion matrix \ncm = confusion_matrix(y_test, pred5) ","9b6a6201":"#Train the XGboost Model for Classification\nmodel = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, subsample=0.5)\ndf_results = prediction(model, df_results, iterable = 10)","a41732aa":"#Train the XGboost Model for Classification\nname = \"XGB_v3\"\nmodel = Pipeline(steps=[('xgboost', xgb.XGBClassifier(objective='multi:softmax',num_class=3))])\nprediction(model, name, iterable = 10)","48cf151f":"#Train the XGboost Model for Classification\nname = \"XGB_v4\"\nmodel = xgb.train(param, xg_train, num_round, watchlist)\nprediction(model, name, iterable = 10)","ee0deedc":"# make predictions for test data\ny_pred81 = model8.predict(xg_train)\npred81 = [round(value) for value in y_pred81]\nprint('Train accuracy score:',accuracy_score(y_train, y_pred81))\n\n# evaluate predictions\naccuracy8 = accuracy_score(y_train, pred8)\nprint(\"Accuracy: %.2f%%\" % (accuracy8 * 100.0))\n\n# Implementing your own scoring\nscores8 = cross_val_score(model8, X_train, y_train, cv=10)\nprint('Accuracy for XGB v4 Classifier : ', scores8.mean())\n\ny_pred82 = model8.predict(xg_test)\npred82 = [round(value) for value in y_pred82]\nprint('Test accuracy score:',accuracy_score(y_test,y_pred82))","9bc66cab":"# read the file pickle to model\n_model = pickle.load(open(\"RandomForestClassifier.pkl\",\"rb\"))\n\n# Fazendo as previsoes de surface no dataset de teste\nprediction = _model.predict(X_test) \n\n# Voltando a transformacao da variavel target em formato texto\nresult = dummie_untransform(prediction)","be6fb656":"#Gerando Arquivo de Submissao\nsubmission = pd.DataFrame({\n    \"series_id\": dfx_test.series_id, \n    \"surface\": result\n})\nsubmission = submission.drop_duplicates()\nsubmission","430f8fc1":"submission.groupby(['series_id','surface'])['series_id'].sum()","6cccdc14":"# Usei essa excelente ideia do Rodrigo Lima de Oliveira\n# Executando query para identificar as superficies com maiores quantidade, para fazer o submit\nquery = \"\"\" SELECT x.series_id, x.surface, MAX(x.qtde) maior\n           FROM (SELECT series_id, surface, count() as qtde\n                   FROM submission\n                  GROUP BY series_id, surface) x\n          GROUP BY x.series_id\"\"\"\n\nsubmission = ps.sqldf(query, locals())\nsubmission = submission.drop(['maior'],axis=1)\n\nsubmission","5fce2ebe":"#Convert DataFrame to a csv file that can be uploaded\nfilename = r'submission.csv'\nsubmission.to_csv(filename, index=False)\n\nprint('Saved file: ' + filename)\n\nFileLink(filename)","a437235a":"# Categoriza\u00e7\u00e3o da vari\u00e1vel alvo","f2bb68c8":"## Compara\u00e7\u00e3o dos atributos entre o dataset de teste e treino","6e4161fe":"2. Random Forest","12328fb0":"### An\u00e1lise Explorat\u00f3ria","06ba0c27":"# Modelo - RandomForester (sem split)","b8721906":"## Criando os dados de Teste e Treino","66bcbf50":"# Link e arquivos e diret\u00f3rios","8bbec74f":"### Verificando a convers\u00e3o antes e depois","1e779a6d":"## Juntando os dados de de treino: vari\u00e1veis independentes e dependente","667d8658":"## Vari\u00e1veis","5f611b3c":"### Verificando a convers\u00e3o","53a5d208":"## Correla\u00e7\u00e3o","6cd16a94":"## Tratando Valores Missing","e072b778":"## \u00c2ngulos de Euler (Euler angles)\nOs \u00e2ngulos de Euler s\u00e3o tr\u00eas \u00e2ngulos introduzidos por Leonhard Euler para descrever a orienta\u00e7\u00e3o de um corpo r\u00edgido em rela\u00e7\u00e3o a um sistema de coordenadas fixo.","82336a45":"### Verificando a convers\u00e3o","befae419":"## Correla\u00e7\u00f5es","fd29c7a8":"## Feature Engineering","2dcc6a90":"1. Logistic Regression","1b887239":"### Verificando a convers\u00e3o","0fc82569":"## Fun\u00e7\u00f5es de Apoio","bdb98868":"# Modelagem Preditiva","48dd92a0":"### Verificando a convers\u00e3o antes e depois","20957f06":"## Mantendo a \u00faltima medi\u00e7\u00e3o realizada para os dados alvo","970100f2":"6. XGBoot - V2","62a8fbcf":"3. Ridge Regression","1cf6e323":"## Removendo registros duplicados","cdac4c8e":"7. XGBoost - v3","56da18b3":"# Avaliando e prevendo o resultado final","5817c004":"4. K-Nearest Neighbors","a4149565":"## Resultado da importa\u00e7\u00e3o dos dados","8f595b1f":"## Analise dos dados das vari\u00e1veis dependentes","a3d1123f":"## Visualiza\u00e7\u00e3o da frequ\u00eancia de distribui\u00e7\u00e3o dos dados sobre a vari\u00e1vel alvo","b44ea804":"## Variaveis independentes de teste","190e8986":"## Variaveis independentes de treino","170679a5":"## Definindo os modelos","d53dfaf5":"## Importando as bibliotecas para vis\u00e3o dos Modelos","7f6844c9":"# Boxplot","f7c1753b":"8. XGBoot - v4","29213968":"## Gerando o arquivo de envio (submission)","5b464b49":"## Colocando os dados em Escala","fccd324a":"## Criando a Vari\u00e1vel categ\u00f3rica para o Superfice","0a8317df":"### Modelagem Preditiva","687c0530":"### Verificando a convers\u00e3o antes e depois","32542842":"XGBoot - Parameters","18075e48":"## Criando os labels unicos da vari\u00e1vel target","65cca7cb":"# Carregando as bibliotecas gr\u00e1ficas para visualiza\u00e7\u00e3o dos dados","7000ddca":"# Modelos","c62d1a70":"5. XGBoot - v1","63bcf0c4":"## Separando os dados por vari\u00e1veis independentes e dependentes do dataset consolidado","980b6330":"## Execu\u00e7\u00e3o do modelo - Usando as vari\u00e1veis mais import\u00e2ntes","7a240d6d":"# Defini\u00e7\u00e3o do Problema\n\nNesta competi\u00e7\u00e3o, voc\u00ea ajudar\u00e1 rob\u00f4s (especificamente ve\u00edculos aut\u00f4nomos terrestres) a reconhecer a superf\u00edcie do piso em que est\u00e3o, usando os dados coletados por sensores IMU (Inertial Measurement Units). Ser\u00e1 uma oportunidade \u00fanica de aprendizado. ","85db5fc9":"# Engenharia de recursos (Feature Enginnering)\nA Engenharia de recursos \u00e9 o processo de usar o conhecimento de dom\u00ednio dos dados para criar recursos que fazem os algoritmos de aprendizado de m\u00e1quina funcionarem. A engenharia de recursos \u00e9 fundamental para a aplica\u00e7\u00e3o do aprendizado de m\u00e1quina e \u00e9 dif\u00edcil e cara. Os recursos de seus dados s\u00e3o importantes para os modelos preditivos usados e influenciar\u00e3o os resultados que voc\u00ea obter\u00e1. A qualidade e a quantidade dos recursos ter\u00e3o grande influ\u00eancia sobre se o modelo \u00e9 bom ou n\u00e3o.","ad691e1c":"## Transforma\u00e7\u00e3o dos dados em scala padronizada","566b00b2":"## Convers\u00e3o dos datasets - Fast Fourier Transform Denoising","de1de86d":"# Carregando os arquivos de dados","e79c99ff":"# Prepra\u00e7\u00e3o dos dados","9b1471ea":"# Modelo - RandomForester (sem split)","9d9f9822":"# Leitura adicional e refer\u00eancias\n\n[sensor-eda-fe-and-prediction-improvement](https:\/\/www.kaggle.com\/hiralmshah\/robot-sensor-eda-fe-and-prediction-improvement)\n\n[standardize-or-normalize-examples-in-python](https:\/\/medium.com\/@rrfd\/standardize-or-normalize-examples-in-python-e3f174b65dfc)\n\n[where-do-the-robots-drive](https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive)\n\n[scale-standardize-or-normalize-with-scikit-learn](https:\/\/towardsdatascience.com\/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)\n\n[feature importance](https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html)\n\n[Fast Fourier Transformation](https:\/\/pt.wikipedia.org\/wiki\/Transformada_r%C3%A1pida_de_Fourier)\n\n[Eular Angles](https:\/\/en.wikipedia.org\/wiki\/Conversion_between_quaternions_and_Euler_angles)","8c967c21":"# Definindo as vari\u00e1veis mais importantes para o modelo\n","c3fcacfb":"# Removendo as medi\u00e7\u00f5es com o mesmo Id\n## Mantendo a \u00faltima medi\u00e7\u00e3o realizada para os dados de teste"}}