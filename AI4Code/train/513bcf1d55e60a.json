{"cell_type":{"18188fff":"code","42c1c521":"code","a10382ad":"code","11b45af2":"code","4db040fb":"code","ba4a2c3e":"code","668e4b43":"code","f3b27869":"code","88782c31":"code","307e496e":"markdown","d25bb85c":"markdown","774a2c39":"markdown","8c3c877a":"markdown"},"source":{"18188fff":"from gensim.models import Word2Vec\nfrom multiprocessing import Pool\nimport sqlite3 as sql\nimport numpy as np\nimport logging\nimport time\nimport re\n\ndb = '''..\/input\/english-wikipedia-articles-20170820-sqlite\/enwiki-20170820.db'''\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","42c1c521":"def get_query(select, db=db):\n    '''\n    1. Connects to SQLite database (db)\n    2. Executes select statement\n    3. Return results and column names\n    \n    Input: 'select * from analytics limit 2'\n    Output: ([(1, 2, 3)], ['col_1', 'col_2', 'col_3'])\n    '''\n    with sql.connect(db) as conn:\n        c = conn.cursor()\n        c.execute(select)\n        col_names = [str(name[0]).lower() for name in c.description]\n    return c.fetchall(), col_names\n\ndef tokenize(text, lower=True):\n    '''\n    1. Strips apostrophes\n    2. Searches for all alpha tokens (exception for underscore)\n    3. Return list of tokens\n\n    Input: 'The 3 dogs jumped over Scott's tent!'\n    Output: ['the', 'dogs', 'jumped', 'over', 'scotts', 'tent']\n    '''\n    text = re.sub(\"'\", \"\", text)\n    if lower:\n        tokens = re.findall('''[a-z_]+''', text.lower())\n    else:\n        tokens = re.findall('''[A-Za-z_]''', text)\n    return tokens\n    \ndef get_section(rowid):\n    '''\n    1. Construct select statement\n    2. Retrieves section_text\n    3. Tokenizes section_text\n    4. Returns list of tokens\n\n    Input: 100\n    Output: ['the','austroasiatic','languages','in',...]\n    '''\n    select = '''select section_text from articles where rowid=%d''' % rowid\n    doc, _ = get_query(select)\n    tokens = tokenize(doc[0][0])\n    return tokens\n       \nclass Corpus():\n    def __init__(self, rowids):\n        self.rowids = rowids\n        self.len = len(rowids)\n\n    def __iter__(self):\n        rowids = np.random.choice(self.rowids, self.len, replace=False)\n        with Pool(processes=4) as pool:\n            docs = pool.imap_unordered(get_section, rowids)\n            for doc in docs:\n                yield doc\n\n    def __len__(self):\n        return self.len","a10382ad":"select = '''select distinct rowid from articles'''\nrowids, _ = get_query(select)\nrowids = [rowid[0] for rowid in rowids]","11b45af2":"start = time.time()\n# To keep training time reasonable, let's just look at a random 10K section text sample.\nsample_rowids = np.random.choice(rowids, 10000, replace=False)\ndocs = Corpus(sample_rowids)\nword2vec = Word2Vec(docs, min_count=100, size=100)\nend = time.time()\nprint('Time to train word2vec from generator: %0.2fs' % (end - start))","4db040fb":"word2vec = Word2Vec.load('..\/input\/english-wikipedia-articles-20170820-models\/enwiki_2017_08_20_word2vec.model')","ba4a2c3e":"# has it seen the word anarchy?\nword2vec.wv['anarchy']","668e4b43":"word2vec.wv['king']","f3b27869":"# find similar words\nword2vec.wv.most_similar(positive=['king'])","88782c31":"vect = word2vec.wv['france'] - word2vec.wv['paris'] + word2vec.wv['london']\nword2vec.wv.similar_by_vector(vect)","307e496e":"Now let's train a Word2Vec. Ideally, we'd split the section text into sentences, but feeding section text as a block performs well. ","d25bb85c":"For now, let's load a pre-trained model and explore how to use it.","774a2c39":"# Tutorial: Word2Vec\nThis is a basic guide to efficiently training a Word2Vec model on the English Wikipedia dump using Gensim.","8c3c877a":"First step, grab the index we'll be iterating over. In this case, we want to use section text, so let's use the implicit column: **rowid**."}}