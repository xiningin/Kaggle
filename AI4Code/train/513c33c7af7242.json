{"cell_type":{"08aa98ac":"code","6ca2ab87":"code","f726b4e8":"code","2acc0031":"code","30947da8":"code","3d1c0e9c":"code","f1d82278":"code","2bd4cbe6":"code","64255627":"code","9805dc09":"code","990b134f":"code","08fa9620":"code","481310c2":"code","b354d4da":"code","1b611553":"code","c7e450eb":"code","2fd14c50":"code","e15ddef6":"code","7e0289af":"code","d5e3fffd":"code","dec97f62":"code","12fd6d46":"code","9ef07ed7":"code","b2a6f72f":"code","63daa978":"code","33b3710b":"code","c0e0a1f0":"code","4a501047":"code","594fcfc1":"code","dbb31286":"code","fe594fa5":"code","221a887d":"code","68532675":"code","a9ac0cd9":"code","b21a00e4":"code","171e3be9":"code","6d4ac862":"code","85e80a44":"code","a6e9640d":"code","eeccf2d8":"code","daebae0d":"code","9bfa8b8e":"code","0144b167":"code","70dc1e4b":"code","ddd3bbde":"code","799eafa2":"code","4b23a537":"code","fe48a8bf":"code","22c4bd98":"code","0cd5f9f5":"code","3873395f":"code","6c830502":"code","bb228375":"code","05081044":"markdown","99082031":"markdown","c739153f":"markdown","972affff":"markdown","dca2bb3f":"markdown","28b0b129":"markdown","a0bbf30e":"markdown","f3252a2d":"markdown","58337035":"markdown","26802146":"markdown","0dec6f5b":"markdown","81652b04":"markdown","9d9166f7":"markdown","09c9fd8f":"markdown","cac69ece":"markdown","22e350b4":"markdown","f2851d21":"markdown","73b1ab68":"markdown","f53f7f55":"markdown","b2f8bf32":"markdown","f1497c68":"markdown","fa6842ba":"markdown","b099364a":"markdown","71c56c9d":"markdown","e399e58f":"markdown","f35718c4":"markdown","28e460dc":"markdown","7530aa2c":"markdown"},"source":{"08aa98ac":"%matplotlib inline\nimport keras\nimport numpy as np\nfrom keras import backend as K\nfrom keras.utils.data_utils import get_file\nfrom keras.utils import np_utils\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\nfrom keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\nfrom keras.layers.core import Flatten, Dense, Dropout, Lambda\nfrom keras.regularizers import l2, l1\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import *\nfrom keras.optimizers import SGD, RMSprop, Adam\nfrom keras.metrics import categorical_crossentropy, categorical_accuracy\nfrom keras.layers.convolutional import *\nfrom keras.preprocessing import image, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras import applications","6ca2ab87":"import numpy as np\nfrom keras.datasets import imdb as keras_imdb","f726b4e8":"#imdb = np.load(open(\"..\/input\/imdb.npz\", \"rb\"), allow_pickle=True)\n#x_train = imdb[\"x_train\"]\n#y_train = imdb[\"y_train\"]\n#x_test = imdb[\"x_test\"]\n#y_test = imdb[\"y_test\"]","2acc0031":"import numpy as np\n# save np.load\nnp_load_old = np.load\n\n# modify the default parameters of np.load\nnp.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n\n# call load_data with allow_pickle implicitly set to true\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=None,\n                                                       skip_top=0,\n                                                       maxlen=None,\n                                                       seed=113,\n                                                       start_char=1,\n                                                       oov_char=2,\n                                                       index_from=3)\n\n# restore np.load for future normal usage\nnp.load = np_load_old","30947da8":"import nltk","3d1c0e9c":"sentence = \"            Hello,  my name's Chien #test           \"","f1d82278":"l = sentence.split()","2bd4cbe6":"\" \".join(l)","64255627":"tokenizer = nltk.tokenize.TweetTokenizer() ","9805dc09":"tokenizer.tokenize(sentence)","990b134f":"print(x_train[1][:10])","08fa9620":"print(x_train.shape)","481310c2":"print(len(x_train[0]))","b354d4da":"word_to_id = keras.datasets.imdb.get_word_index()","1b611553":"word_to_id[\"horrible\"]","c7e450eb":"list(word_to_id.keys())[:10]","2fd14c50":"# Add terms to our vocabulary\nword_to_id = {word : (word_id + 3) for word, word_id in keras.datasets.imdb.get_word_index().items()}  # Add an offset of 3 to leave room for the new terms\nword_to_id[\"<PAD>\"] = 0\nword_to_id[\"<START>\"] = 1\nword_to_id[\"<UNK>\"] = 2\n\n# Also define the opposite mapping\nid_to_word = {word_id: word for word, word_id in word_to_id.items()}","e15ddef6":"def get_review(x, i):\n    return \" \".join(id_to_word[id_] for id_ in x[i])\n\ndef get_label(y, i):\n    if y[i] == 1:\n        return \"Positive\"\n    else:\n        return \"Negative\"","7e0289af":"for review_index in range(10):\n    review = get_review(x_train, review_index)\n    label = get_label(y_train, review_index)\n    print(\"*\" * 50)\n    print(f\"Review index: {review_index}\")\n    print(f\"Label: {label}\")\n    print(\"-\" * 50)\n    print(review)\n    print(\"*\" * 50)","d5e3fffd":"vocab_size = 5000\ntrn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\ntest = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]","dec97f62":"lens = np.array(list(map(len, trn)))\nprint('Maximum text length:', lens.max(),' -- Minimum length:', lens.min(), '-- Mean length of text:',lens.mean())","12fd6d46":"# we'll pad all inputs to obtain homogeneous inputs of dim 500\nseq_len = 500\n\ntrn = sequence.pad_sequences(trn, maxlen = seq_len,value=0, padding=\"post\", truncating=\"post\")\ntest = sequence.pad_sequences(test, maxlen = seq_len,value=0, padding=\"post\", truncating=\"post\")","9ef07ed7":"trn.shape","b2a6f72f":"get_review(trn, 1)","63daa978":"def MLP():\n    model = Sequential()\n    model.add(Embedding(vocab_size,32,input_length=seq_len))\n    \n    model.add(Flatten())\n    model.add(Dense(100,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(100,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1,activation='sigmoid'))\n\n    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n    print(model.summary())\n    \n    return model\n\nmodel = MLP()  ","33b3710b":"model.fit(trn, y_train, validation_data=(test, y_test), epochs=4, batch_size=512)","c0e0a1f0":"scores = model.evaluate(test,y_test,verbose=0)\nprint('loss: ', scores[0],'- accuracy: ', scores[1])","4a501047":"def CNN():\n    model = Sequential()\n    model.add(Embedding(vocab_size, 32, input_length=seq_len))\n    \n    #model.add(Dropout(0.2))\n    model.add(Conv1D(64, 5, padding='same', activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(MaxPooling1D())\n    \n    model.add(Flatten())\n    \n    model.add(Dense(100, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n    print(model.summary())\n    \n    return model\n\nmodel = CNN()","594fcfc1":"model.fit(trn, y_train, validation_data=(test, y_test), epochs=2)","dbb31286":"scores = model.evaluate(test,y_test,verbose=0)\nprint('loss: ', scores[0],'- accuracy: ', scores[1])","fe594fa5":"from keras.layers import concatenate","221a887d":"graph_in = Input ((vocab_size, 32))\nconvs = [ ] \nfor fsz in range (3, 6): \n    x = Conv1D(64, fsz, padding='same', activation=\"relu\")(graph_in)\n    x = MaxPooling1D()(x) \n    x = Flatten()(x) \n    convs.append(x)\nout = concatenate(convs)\ngraph = Model(graph_in, out)\ngraph.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])","68532675":"graph.summary()","a9ac0cd9":"def multisize_CNN():\n    model = Sequential()\n    model.add(Embedding(vocab_size, 32,input_length=seq_len))\n    model.add(Dropout (0.2))\n    model.add(graph)\n    model.add(Dropout(0.5))\n    model.add(Dense(100, activation=\"relu\"))\n    model.add(Dropout(0.7))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n    print(model.summary())\n    \n    return(model)\n\nmodel = multisize_CNN()","b21a00e4":"model.fit(trn, y_train, validation_data=(test, y_test), epochs=5, batch_size=512)","171e3be9":"scores = model.evaluate(test,y_test,verbose=0, batch_size=512)\nprint('loss: ', scores[0],'- accuracy: ', scores[1])","6d4ac862":"predictions = model.predict(test[:10],1)\npredictions = np.round(predictions).astype('int')","85e80a44":"for review_index in range(10):\n    review = get_review(x_train, review_index)\n    prediction = get_label(predictions, review_index)\n    truth = get_label(y_test, review_index)\n    \n    print(\"*\" * 50)\n    print(f\"Review index: {review_index}\")\n    print(f\"Prediction: {label}  --- Truth: {truth}\")\n    print(\"-\" * 50)\n    print(review)\n    print(\"*\" * 50)","a6e9640d":"text = get_text(x_test, 10)\npreds = get_label_txt(predictions[:10])\ntrue = get_label_txt(y_test[0:10])","eeccf2d8":"for i in range(40):\n    print('*******************************************************************************')\n    print('TEXT n\u00b0', i + 1, ' -- TRUE label:', true[i], ' -- PREDICTED label:', preds[i])\n    print('-------------------------------------------------------------------------------')\n    print(text[i])\n    print('*******************************************************************************')","daebae0d":"def RNN_LSTM():\n    model = Sequential()\n    model.add(Embedding(vocab_size,5,input_length=seq_len))\n    model.add(LSTM(50))\n    model.add(Dropout(0.25))\n    model.add(Dense(1,activation='sigmoid'))\n\n    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n    print(model.summary())\n    \n    return model\n\nmodel = RNN_LSTM()       ","9bfa8b8e":"model.fit(trn, y_train, validation_data=(test, y_test), epochs=4, batch_size=512)","0144b167":"scores = model.evaluate(test,y_test,verbose=0, batch_size=512)\nprint('loss: ', scores[0],'- accuracy: ', scores[1])","70dc1e4b":"predictions = model.predict(test[:10],1)\npredictions = np.round(predictions).astype('int')","ddd3bbde":"n_texts = 20","799eafa2":"text = get_text(x_test, n_texts)\npreds = get_label_txt(predictions[:n_texts])\ntrue = get_label_txt(y_test[:n_texts])","4b23a537":"for i in range(n_texts):\n    print('*******************************************************************************')\n    print('TEXT n\u00b0', i + 1, ' -- TRUE label:',  true[i], ' -- PREDICTED label:', preds[i])\n    print('-------------------------------------------------------------------------------')\n    print(text[i])\n    print('*******************************************************************************')","fe48a8bf":"def RNN_GRU():\n    model = Sequential()\n    model.add(Embedding(vocab_size,5,input_length=seq_len))\n    model.add(GRU(50))\n    model.add(Dropout(0.25))\n    model.add(Dense(1,activation='sigmoid'))\n\n    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n    print(model.summary())\n    \n    return model\n\nmodel = RNN_GRU()  ","22c4bd98":"model.fit(trn, y_train, validation_data=(test, y_test), epochs=4, batch_size=512)","0cd5f9f5":"scores = model.evaluate(test,y_test,verbose=0)\nprint('loss: ', scores[0],'- accuracy: ', scores[1])","3873395f":"predictions = model.predict(test[:10],1)\npredictions = np.round(predictions).astype('int')","6c830502":"text = get_text(x_test, 10)\npreds = get_label_txt(predictions[:10])\ntrue = get_label_txt(y_test[:10])","bb228375":"for i in range(10):\n    print('*******************************************************************************')\n    print('TEXT n\u00b0', i + 1, ' -- TRUE label:', true[i], ' -- PREDICTED label:', preds[i])\n    print('-------------------------------------------------------------------------------')\n    print(text[i])\n    print('*******************************************************************************')","05081044":"# B - Using CNN\nA CNN is likely to work better, since it's designed to take advantage of ordered data. We'll need to use a 1D CNN, since a sequence of words is 1D.","99082031":"How many samples do we have? ","c739153f":"#### Evaluating","972affff":"# E. Using GRU","dca2bb3f":"# Visual Checking","28b0b129":"#### Evaluating","a0bbf30e":"# A. Using MLP to classify","f3252a2d":"#### Evaluating","58337035":"#### Fitting","26802146":"# D. Using LSTM","0dec6f5b":"# C - Using multi-size CNN","81652b04":"We decide to add three terms to our vocabulary:\n- start of reviews with word `<START>`\n- words we may remove (and merge) because not frequent enough with `<UNK>`\n- pad: just like for images, we need constant input size and that may imply to pad reviews with `<PAD>`","9d9166f7":"One sample seems to consist of a list of integers","09c9fd8f":"#### Fitting","cac69ece":"## Visual Checking","22e350b4":"## Some NLP?","f2851d21":"## Visual Checking","73b1ab68":"# Preparing Data\nWe'll consider only the 5000 most frequent words in the text. We'll then try to generate embeddings for these text","f53f7f55":"#### Fitting","b2f8bf32":"## Models - Training own Embeddings\n### It's possible, nothing unfeasible, just a lot of patience !","f1497c68":"#### Fitting","fa6842ba":"# NLP: Sentiment Analysis: Is the critic positive or negative ?!\nWe are doing sentiment analysis on the imdb critics labelled data set. The data is labelled; either positive of negative.The data set is composed of words ids. ","b099364a":"And how long is a single sample?","71c56c9d":"## Explore our data!","e399e58f":"We need to define a mapping between integers and words","f35718c4":"#### Evaluating","28e460dc":"# IMDB movie critics\n\nWe'll see how to train RNNs in order to do sentiment analysis on movie critics. We'll see how to train embeddings, LSTM, GRU models.","7530aa2c":"#### Fitting"}}