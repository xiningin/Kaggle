{"cell_type":{"db1c48b5":"code","a9c45760":"code","907ddea4":"code","cd42061b":"code","d92cd53b":"code","4ecf5f70":"code","5d5c99af":"code","16c63d8a":"code","4b7f58ca":"code","53fd4165":"code","86d5477a":"code","c394a21a":"code","573ba6bd":"code","df295125":"code","bcdc18f3":"code","6a3ec14d":"markdown","97b74a79":"markdown","80b023e4":"markdown"},"source":{"db1c48b5":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a9c45760":"data = pd.read_csv(\"\/kaggle\/input\/gamingaddiction\/db.csv\")","907ddea4":"data.head(1)","cd42061b":"data_row = data.iloc[:, 3:-1]\ndata_row['\u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0414\u043e\u0442\u0435'] = data_row['\u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0414\u043e\u0442\u0435'].str.extract('(\\d+)', expand=False)\ndata_row['\u0420\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0434\u043e\u0442\u0435 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442'] = data_row['\u0420\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0434\u043e\u0442\u0435 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442'].str.extract('(\\d+)', expand=False)\ndata_row['\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0447\u0430\u0441\u043e\u0432 \u0432 \u0438\u0433\u0440\u0435 (\u0445\u043e\u0442\u044f \u0431\u044b \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e\u0435)'] = data_row['\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0447\u0430\u0441\u043e\u0432 \u0432 \u0438\u0433\u0440\u0435 (\u0445\u043e\u0442\u044f \u0431\u044b \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e\u0435)'].str.extract('(\\d+)', expand=False)\ndata_row['\u041a\u0430\u043a\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0433\u0440 \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c \u0442\u044b \u0438\u0433\u0440\u0430\u0435\u0448\u044c \u0432 \u043d\u0435\u0434\u0435\u043b\u044e? '] = data_row['\u041a\u0430\u043a\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0433\u0440 \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c \u0442\u044b \u0438\u0433\u0440\u0430\u0435\u0448\u044c \u0432 \u043d\u0435\u0434\u0435\u043b\u044e? '].str.extract('(\\d+)', expand=False)\ndata_row['\u0412\u043e\u0437\u0440\u0430\u0441\u0442'] = data_row['\u0412\u043e\u0437\u0440\u0430\u0441\u0442'].str.extract('(\\d+)', expand=False)\ndf = data_row.fillna(0)\ndf = df.replace(to_replace = 0, value = 0)\ndf = df.dropna()\ndf = df.astype('float32')\ndf = df.drop(np.where(df['\u0420\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0434\u043e\u0442\u0435 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442'] > 10000)[0])\ndf = df.drop(np.where(df['\u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0414\u043e\u0442\u0435'] > 10000)[0])\ndf = df.loc[df['\u0420\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0434\u043e\u0442\u0435 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442'] > 200]\ndf = df.loc[df['\u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0414\u043e\u0442\u0435'] > 200]\ndf = df.loc[df['\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0447\u0430\u0441\u043e\u0432 \u0432 \u0438\u0433\u0440\u0435 (\u0445\u043e\u0442\u044f \u0431\u044b \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e\u0435)'] > 200]\ndf = df.loc[df['\u0420\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0434\u043e\u0442\u0435 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442'] <= df['\u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0414\u043e\u0442\u0435']]\ndf = df.loc[df['\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0447\u0430\u0441\u043e\u0432 \u0432 \u0438\u0433\u0440\u0435 (\u0445\u043e\u0442\u044f \u0431\u044b \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e\u0435)'] < 30000]\ndf = df.astype('float32')\ndf = df.drop('\u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0414\u043e\u0442\u0435',axis=1)","d92cd53b":"df['target'] = df['\u0420\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0434\u043e\u0442\u0435 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442']\ndf = df.drop(columns=['\u0420\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u0434\u043e\u0442\u0435 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442'])","4ecf5f70":"train, val, test = np.split(df.sample(frac=1), [int(0.8*len(df)), int(0.9*len(df))])","5d5c99af":"def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n    df = dataframe.copy()\n    labels = df.pop('target')\n    df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(batch_size)\n    return ds","16c63d8a":"batch_size = 5\ntrain_ds = df_to_dataset(train, batch_size=batch_size)","4b7f58ca":"[(train_features, label_batch)] = train_ds.take(1)\nprint('Every feature:', list(train_features.keys()))\nprint('A batch of ages:', train_features['\u0412\u043e\u0437\u0440\u0430\u0441\u0442'])\nprint('A batch of targets:', label_batch )","53fd4165":"def get_normalization_layer(name, dataset):\n  # Create a Normalization layer for the feature.\n    normalizer = layers.Normalization(axis=None)\n\n  # Prepare a Dataset that only yields the feature.\n    feature_ds = dataset.map(lambda x, y: x[name])\n\n  # Learn the statistics of the data.\n    normalizer.adapt(feature_ds)\n\n    return normalizer\ndef get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n  # Create a layer that turns strings into integer indices.\n    if dtype == 'string':\n        index = layers.StringLookup(max_tokens=max_tokens)\n  # Otherwise, create a layer that turns integer values into integer indices.\n    else:\n        index = layers.IntegerLookup(max_tokens=max_tokens)\n\n  # Prepare a `tf.data.Dataset` that only yields the feature.\n    feature_ds = dataset.map(lambda x, y: x[name])\n\n  # Learn the set of possible values and assign them a fixed integer index.\n    index.adapt(feature_ds)\n\n  # Encode the integer indices.\n    encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n\n  # Apply multi-hot encoding to the indices. The lambda function captures the\n  # layer, so you can use them, or include them in the Keras Functional model later.\n    return lambda feature: encoder(index(feature))\ndef plot_loss(history):\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.ylim([0, 1000])\n    plt.xlabel('Epoch')\n    plt.ylabel('Error [rating]')\n    plt.legend()\n    plt.grid(True)","86d5477a":"batch_size = 256\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)","c394a21a":"all_inputs = []\nencoded_features = []\n\n# Numerical features.\nfor header in ['\u0412\u043e\u0437\u0440\u0430\u0441\u0442', '\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0447\u0430\u0441\u043e\u0432 \u0432 \u0438\u0433\u0440\u0435 (\u0445\u043e\u0442\u044f \u0431\u044b \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e\u0435)']:\n    numeric_col = tf.keras.Input(shape=(1,), name=header)\n    normalization_layer = get_normalization_layer(header, train_ds)\n    encoded_numeric_col = normalization_layer(numeric_col)\n    all_inputs.append(numeric_col)\n    encoded_features.append(encoded_numeric_col)","573ba6bd":"categorical_cols = df.columns[2:-1]\n\nfor header in categorical_cols:\n    categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='float')\n    encoding_layer = get_category_encoding_layer(name=header,\n                                               dataset=train_ds,\n                                               dtype='float',\n                                               max_tokens=5)\n    encoded_categorical_col = encoding_layer(categorical_col)\n    all_inputs.append(categorical_col)\n    encoded_features.append(encoded_categorical_col)","df295125":"all_features = tf.keras.layers.concatenate(encoded_features)\nx = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer='l2')(all_features)\nx = tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer='l2')(x)\nx = tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer='l2')(x)\nx = tf.keras.layers.Dense(16, activation=\"relu\")(x)\noutput = tf.keras.layers.Dense(1)(x)\n\nmodel = tf.keras.Model(all_inputs, output)\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\ncheckpoint_filepath = '\/tmp\/checkpoint'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True)\nmodel.compile(optimizer=tf.keras.optimizers.Adam(\n                learning_rate=0.001),\n              loss='mean_absolute_error',\n              )\n\nhistory = model.fit(train_ds, epochs=500, verbose=2, validation_data=val_ds, callbacks=[callback, model_checkpoint_callback])\nplot_loss(history)\nmodel.load_weights(checkpoint_filepath)","bcdc18f3":"def otvet(df, model):\n    X_test = {}\n    for i in range(0,len(df.columns[:-1])):\n        print(df.columns[i], str(i+1))\n        a = float(input())\n        X_test[df.columns[i]] = a\n    input_dict = {name: tf.convert_to_tensor([value]) for name, value in X_test.items()}\n    predictions = model.predict(input_dict)\n    return predictions","6a3ec14d":"\u0427\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0432\u0432\u0435\u0434\u0438\u0442\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u044e otvet(df, model) \u0412\u043e\u043f\u0440\u043e\u0441\u044b 1-3 - \u0447\u0438\u0441\u043b\u043e, \u0412\u043e\u043f\u0440\u043e\u0441\u044b \u0441 3-\u0434\u043e \u043a\u043e\u043d\u0446\u0430 - \u0432 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0435 \u043e\u0442 1 \u0434\u043e 5 (\u0421\u043e\u0432\u0441\u0435\u043c \u043d\u0435 \u0441\u043e\u0433\u043b\u0430\u0441\u0435\u043d - \u041f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u0441\u043e\u0433\u043b\u0430\u0441\u0435\u043d)","97b74a79":"\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u0442 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 \u0447\u0435\u043c \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439","80b023e4":"\u0420\u0430\u0437\u0431\u0435\u0440\u0435\u043c\u0441\u044f \u0432 \u043d\u0435\u0439\u0440\u043e\u043d\u043a\u0430\u0445 \u043d\u0435\u043c\u043d\u043e\u0433\u043e, \u0430 \u0441 \u0432\u0430\u043c\u0438 \u0421\u0435\u0440\u0435\u0433\u0433\u0430. \u041d\u0443 \u0432\u043e\u043e\u0431\u0449\u0435-\u0442\u043e \u041a\u043e\u0442\u0447\u0430, \u043d\u043e \u043d\u0435 \u0432\u0430\u0436\u043d\u043e. \u0414\u0430\u043d\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0431\u044b\u043b \u0441\u043e\u0431\u0440\u0430\u043d \u043f\u043e\u0434\u043f\u0438\u0441\u0447\u0438\u043a\u0430\u043c\u0438 \u0441\u0435\u0440\u0435\u0433\u0438\u0438, \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u044f \u043d\u0435\u0441\u043b\u043e\u0436\u043d\u0443\u044e \u0433\u0443\u0433\u043b \u0444\u043e\u0440\u043c\u0443.\n\u0412\u043e\u0442 \u043a\u0441\u0442\u0430\u0442\u0438 \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0435: https:\/\/forms.gle\/DWRXR67RFvatE25k9\n\u0418 \u043c\u043d\u0435 \u043f\u0440\u0438\u0448\u043b\u0430 \u0438\u0434\u0435\u044f:\"\u0410 \u0447\u0442\u043e \u0435\u0441\u043b\u0438 \u043f\u043e\u043f\u044b\u0442\u0430\u0442\u044c\u0441\u044f \u043e\u0441\u043d\u043e\u0432\u044b\u0432\u0430\u044f\u0441\u044c \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u0445 \u043e\u043f\u0440\u043e\u0441\u043d\u0438\u043a\u0430 - \u0443\u0437\u043d\u0430\u0442\u044c \u043a\u0430\u043a\u043e\u0439 \u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u041c\u041c\u0420\"\n\u0418 \u043d\u0430\u0437\u0432\u0430\u043b \u044f \u044d\u0442\u0443 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u0443 - *\u043f\u0441\u0438\u0445\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u041c\u041c\u0420*"}}