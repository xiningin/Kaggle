{"cell_type":{"32ef1789":"code","0949ed5a":"code","a709f28c":"code","f18c43e3":"code","e22c91b0":"code","2a37ba2b":"code","7ae67fb6":"code","c0e84e43":"code","4db865b1":"code","18eb3546":"code","222d0bf6":"code","bec5e25d":"code","ef862938":"code","a3fe010b":"code","8792fd9a":"code","ee45de1e":"code","8e4ac010":"code","68c01d21":"code","d6b0cfca":"code","d7598a95":"code","b7f0ac29":"code","27093ef2":"code","19aaa0d0":"code","1eb573a6":"code","152f8783":"code","4a2ae73d":"code","39acba87":"code","28b75960":"code","9f3ebd7f":"code","be361259":"code","1974c866":"code","2a62090f":"code","0e2a854a":"code","ee36d555":"code","6e053b0a":"code","5575af3c":"code","5eed0d2b":"code","e42048b9":"code","77e79560":"code","5f70bbd0":"code","454eff78":"code","9e4f664d":"code","6cb7268b":"code","6a389621":"code","2cc193c6":"code","8e4a1a32":"code","17bab667":"code","6e359d05":"code","ea51b949":"code","24d652a1":"markdown","f00b38e3":"markdown","e60574a5":"markdown","6a951e9d":"markdown","08718953":"markdown","6b0898c7":"markdown","65f13faf":"markdown","c9ce128f":"markdown","c4d8eec4":"markdown","30594b80":"markdown"},"source":{"32ef1789":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0949ed5a":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","a709f28c":"data=pd.read_csv('..\/input\/marketing-data\/marketing_data.csv',index_col=0)\ndata.head()","f18c43e3":"data.columns","e22c91b0":"data.nunique()","2a37ba2b":"data.info()","7ae67fb6":"data.describe()","c0e84e43":"data['Total_Children']=data['Kidhome']+data['Teenhome']","4db865b1":"data = data.drop(['Kidhome','Teenhome'],axis=1)","18eb3546":"data.head()","222d0bf6":"data.nunique()","bec5e25d":"data[' Income '].dtype","ef862938":"data['Income'] = data[' Income '].str.replace(' ','').str.replace('$','').str.replace(',','').astype(float)\ndata = data.drop([' Income '],axis=1)","a3fe010b":"data.isnull().sum()","8792fd9a":"data['Income'] = data['Income'].fillna(data['Income'].mean())","ee45de1e":"data.isnull().sum()","8e4ac010":"data['Dt_Customer']=pd.to_datetime(data['Dt_Customer'])","68c01d21":"data.loc[data['Year_Birth']<1910]\n","d6b0cfca":"data.drop(index=[11004,1150,7829],inplace=True)","d7598a95":"sns.boxplot(x='Education',y='Income',data=data)","b7f0ac29":"data.loc[data.Income>600000]","27093ef2":"data.drop(index=9432,inplace=True)","19aaa0d0":"sns.boxplot(x='Education',y='Income',data=data)","1eb573a6":"data.loc[data.Income>140000]","152f8783":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nfig,axis=plt.subplots(1,2,figsize=(12,6))\nsns.boxplot(x='Education',y='Income',data=data,ax=axis[0])\n\n\nsns.boxplot(x='Country',y='Income',data=data,ax=axis[1])","4a2ae73d":"data['Total_purchases'] = data['NumDealsPurchases']+data['NumWebPurchases']+data['NumCatalogPurchases']+data['NumStorePurchases']\ndata = data.drop(['NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases'],axis=1)","39acba87":"data['veg_products'] = data['MntFruits']\ndata['nonveg_products'] = data['MntMeatProducts']+data['MntFishProducts']\ndata = data.drop(['MntFruits','MntMeatProducts','MntFishProducts'],axis=1)","28b75960":"data.head()","9f3ebd7f":"sns.barplot(x=data.Marital_Status,y=data.NumWebVisitsMonth)","be361259":"data.dtypes","1974c866":"#Import library:\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nvar_mod =['Education','Marital_Status','Country','Dt_Customer']\nfor i in var_mod:\n    data[i] = le.fit_transform(data[i])","2a62090f":"data.dtypes","0e2a854a":"X= data.drop('Response',axis=1)\ny = data['Response']","ee36d555":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,stratify=data['Response'],test_size=0.3)","6e053b0a":"from sklearn.feature_selection import mutual_info_classif\n# determine the mutual information\nmutual_info = mutual_info_classif(X_train, y_train)\nmutual_info","5575af3c":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","5eed0d2b":"#let's plot the ordered mutual_info values per feature\nmutual_info.sort_values(ascending=False).plot.bar(figsize=(14, 7))\nplt.show()","e42048b9":"from sklearn.feature_selection import SelectPercentile\n#we Will select the  top 40% important features\nsel_five_cols = SelectPercentile(mutual_info_classif, percentile=50)\nsel_five_cols.fit(X_train, y_train)\nX_train.columns[sel_five_cols.get_support()]","77e79560":"X_train.drop(['AcceptedCmp3','veg_products','NumWebVisitsMonth','Total_purchases','Marital_Status','Complain','MntSweetProducts','Education','Year_Birth','AcceptedCmp2','Country'],axis=1)\nX_test.drop(['AcceptedCmp3','veg_products','NumWebVisitsMonth','Total_purchases','Marital_Status','Complain','MntSweetProducts','Education','Year_Birth','AcceptedCmp2','Country'],axis=1)","5f70bbd0":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","454eff78":"from keras.models import Sequential\nfrom keras.layers import Dense","9e4f664d":"model = Sequential()\n# Adding the input layer and the first hidden layer\nmodel.add(Dense(units = 15, kernel_initializer='he_uniform', activation='relu', input_dim=X_train.shape[1]))\n# Adding the second hidden layer\nmodel.add(Dense(units = 10, kernel_initializer='he_uniform',activation='relu'))\n# Adding the third hidden layer\nmodel.add(Dense(units = 5, kernel_initializer='he_uniform',activation='relu'))\n# Adding the output layer\nmodel.add(Dense(units=1, kernel_initializer='glorot_uniform',activation='sigmoid'))","6cb7268b":"model.compile(optimizer='Adam',loss='binary_crossentropy',metrics = ['accuracy'])","6a389621":"model.summary()","2cc193c6":"model_history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=25,batch_size=10)","8e4a1a32":"pred = model.predict(X_test)","17bab667":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,pred.round())","6e359d05":"# summarize history for loss\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","ea51b949":"# summarize history for accuracy\nplt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","24d652a1":"### 2. Removing Outliners","f00b38e3":"let's check if any older then 1910:","e60574a5":"let's check columns names:","6a951e9d":"Cleaning Income column:","08718953":"### 4. Feature Selection\n\nMutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.","6b0898c7":"### 3. Feature Engineering","65f13faf":"Task Details You're a marketing analyst and you've been told by the Chief Marketing Officer that recent marketing campaigns have not been as effective as they were expected to be. You need to analyze the data set to understand this problem and propose data-driven solutions.\n\nExpected Submission Submit a well documented notebook with these three sections:\n\nSection 01: Data Exploration Are there any null values or outliers? How will you wrangle\/handle them? Are there any variables that warrant transformations? Are there any useful variables that you can engineer with the given data? Do you notice any patterns or anomalies in the data? Can you plot them?","c9ce128f":"### 1. Cleaning Data","c4d8eec4":"filling null values with \"Income\" column mean():","30594b80":"### 5. ANN training"}}