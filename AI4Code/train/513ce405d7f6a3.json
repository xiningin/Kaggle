{"cell_type":{"94474f93":"code","a80356c5":"code","68f645b2":"code","b55e139e":"code","7fb299cd":"code","c64c7433":"code","f8d4e3f8":"code","6303987c":"code","2dca91d8":"code","72d07f22":"code","470339c1":"code","a0a10bd6":"code","bf3eac7c":"code","209cd77b":"code","efba9587":"code","1cfa52db":"code","6df4356d":"code","b6e1b7c3":"code","7acc6a63":"code","cb5b60dd":"code","0d1714ac":"code","8f82206a":"code","4b2cd817":"code","e2529288":"code","881d5cd4":"code","bc7c3d04":"code","48597fcd":"code","e9174cc3":"code","9fc95e6a":"code","d808a68e":"code","914ba14a":"code","ff079424":"code","fafc8337":"code","980f3014":"code","5ec2ee92":"code","28d71889":"code","14ab1cf7":"code","467558d1":"code","241ab6d4":"code","12a00c55":"code","af536e57":"code","671a7b13":"code","6d332ec9":"code","be365dd1":"code","1e8a9e22":"code","70c7de57":"code","2968dcfe":"code","dbac6887":"code","2475b4fb":"code","e947f556":"code","9c2cd8ea":"code","ec3c078f":"code","b23f071a":"code","7f8e8a38":"code","9f596182":"code","6d8c6af1":"code","c0f15274":"code","39baa59b":"code","f7b9662b":"code","69966219":"code","059a4985":"code","bcf3a7e1":"markdown","ad5c498a":"markdown","09621699":"markdown","5cec3ef1":"markdown","0c72712a":"markdown","5951e51d":"markdown","f635770b":"markdown","6573da8f":"markdown","3958b294":"markdown","4c3c1179":"markdown","189720bc":"markdown","a560cf09":"markdown","b72c7449":"markdown","e5408a82":"markdown","65a6213d":"markdown","849df27e":"markdown","8de596a3":"markdown","3cb605ba":"markdown","af2c66dc":"markdown","1ef8bf25":"markdown","1096a8a8":"markdown","caf37aaa":"markdown","57627be9":"markdown","dbc9c3b0":"markdown"},"source":{"94474f93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a80356c5":"train_df = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv.zip', delimiter='\\t')\ntest_df = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv.zip', delimiter='\\t')","68f645b2":"train_df.info()","b55e139e":"train_df.shape","7fb299cd":"test_df.info()","c64c7433":"train_df.head()\n","f8d4e3f8":"train_df.Phrase[0]","6303987c":"train_df.Phrase[1]","2dca91d8":"train_df['SentenceId'].unique()\n","72d07f22":"test_df.head()","470339c1":"test_df.Phrase[0]","a0a10bd6":"test_df.Phrase[1]","bf3eac7c":"train_df.isnull().sum()","209cd77b":"from collections import Counter\nimport matplotlib.pyplot as plt \n\ntarget_cnt = Counter(train_df.Sentiment)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_cnt.keys(), target_cnt.values())\nplt.title(\"Dataset labels distribuition\")\nplt.show()","efba9587":"target_Sen = Counter(train_df.SentenceId)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_Sen.keys(), target_Sen.values())\nplt.title(\"Dataset SentenceId distribuition\")\nplt.show()","1cfa52db":"target_Sen = Counter(train_df.PhraseId)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_Sen.keys(), target_Sen.values())\nplt.title(\"Dataset PhraseId distribuition\")\nplt.show()","6df4356d":"df_EDA1 = train_df[train_df['Sentiment']==0]","b6e1b7c3":"df_EDA1","7acc6a63":"target_Sen = Counter(df_EDA1.SentenceId)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_Sen.keys(), target_Sen.values())\nplt.title(\"Dataset SentenceId distribuition\")\nplt.show()","cb5b60dd":"print(df_EDA1.SentenceId.value_counts().shape)\nprint(train_df.SentenceId.value_counts().shape)\nprint(df_EDA1.SentenceId.value_counts().shape[0]\/train_df.SentenceId.value_counts().shape[0])","0d1714ac":"df_EDA1.SentenceId.value_counts()","8f82206a":"df_EDA2 = train_df[train_df['SentenceId']==3189]\ndf_EDA2","4b2cd817":"target_cnt = Counter(df_EDA2.Sentiment)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_cnt.keys(), target_cnt.values())\nplt.title(\"Dataset labels distribuition of SentenceId 3189\")\nplt.show()","e2529288":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')","881d5cd4":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words= 500, width= 1600, height= 800, stopwords= stop_words).generate(\" \".join(train_df[train_df.Sentiment==0].Phrase))\nplt.imshow(wc, interpolation= 'bilinear')\nplt.show()","bc7c3d04":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words= 500, width= 1600, height= 800, stopwords= stop_words).generate(\" \".join(train_df[train_df.Sentiment==1].Phrase))\nplt.imshow(wc, interpolation= 'bilinear')\nplt.show()","48597fcd":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words= 500, width= 1600, height= 800, stopwords= stop_words).generate(\" \".join(train_df[train_df.Sentiment==2].Phrase))\nplt.imshow(wc, interpolation= 'bilinear')\nplt.show()","e9174cc3":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words= 500, width= 1600, height= 800, stopwords= stop_words).generate(\" \".join(train_df[train_df.Sentiment==3].Phrase))\nplt.imshow(wc, interpolation= 'bilinear')\nplt.show()","9fc95e6a":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words= 500, width= 1600, height= 800, stopwords= stop_words).generate(\" \".join(train_df[train_df.Sentiment==4].Phrase))\nplt.imshow(wc, interpolation= 'bilinear')\nplt.show()","d808a68e":"base_line = len(train_df[train_df['Sentiment']==2])\/len(train_df.Sentiment)\nbase_line","914ba14a":"#importing \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n#split\nx_train1,x_test1,y_train1,y_test1=train_test_split(train_df['Phrase'],train_df['Sentiment'],test_size=0.2,random_state=42)\n#vectorizing\ncv=CountVectorizer()\nX_train1=cv.fit_transform(x_train1.values)\n#model \nmodel1=MultinomialNB()\nmodel1.fit(X_train1,y_train1)\ntest_count1=cv.transform(x_test1.values)\nModel1_score =model1.score(test_count1,y_test1)\npred1 = model1.predict(test_count1)\n\nprint('Model1 score:' ,Model1_score) ","ff079424":"from sklearn.metrics import confusion_matrix, accuracy_score\nfrom matplotlib import pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\nconf = confusion_matrix(y_test1, pred1)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in ['0', '1', '2', '3', '4']],\n    columns = [i for i in ['0', '1', '2', '3', '4']]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","fafc8337":"from sklearn.ensemble import GradientBoostingClassifier\ngbrt = GradientBoostingClassifier(random_state =0)\ngbrt.fit(X_train1,y_train1)\nModel2_score =gbrt.score(test_count1,y_test1)","980f3014":"Model2_Accscore =gbrt.score(X_train1,y_train1)","5ec2ee92":"Model2_Accscore","28d71889":"Model2_score","14ab1cf7":"Model2_score\npred2 = gbrt.predict(test_count1)","467558d1":"conf = confusion_matrix(y_test1, pred2)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in ['0', '1', '2', '3', '4']],\n    columns = [i for i in ['0', '1', '2', '3', '4']]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","241ab6d4":"from sklearn.svm import LinearSVC\nlinear_svm = LinearSVC(C=100).fit(X_train1,y_train1)\nModel3_score =linear_svm.score(test_count1,y_test1)\npred3 = linear_svm.predict(test_count1)\n\nprint('Model13 score:' ,Model3_score)\n","12a00c55":"Model3_Accscore =linear_svm.score(X_train1,y_train1)\nModel3_Accscore","af536e57":"conf = confusion_matrix(y_test1, pred3)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in ['0', '1', '2', '3', '4']],\n    columns = [i for i in ['0', '1', '2', '3', '4']]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","671a7b13":"from sklearn.svm import SVC\nsvm = SVC().fit(X_train1,y_train1)\nModel4_score =svm.score(test_count1,y_test1)\npred4 = linear_svm.predict(test_count1)\n\nprint('Model14 score:' ,Model4_score)","6d332ec9":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds ","be365dd1":"indices = y_train1\ndepth = 5\ny_train = tf.one_hot(indices, depth)","1e8a9e22":"hub_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\", input_shape=[], output_shape=[512,16], \n  dtype=tf.string,trainable= True)","70c7de57":"model = tf.keras.models.Sequential([\n  hub_layer,\n    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n    \n  tf.keras.layers.Dense(256, activation='relu'),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(5, activation='softmax')\n  ])\nmodel.summary()\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics= ['acc','MAE'])\n#model.fit(x_train1, y_train, batch_size= 64, validation_split = 0.2, epochs= 10)","2968dcfe":"#tf.config.experimental_run_functions_eagerly(True)","dbac6887":"model.fit(x_train1, y_train, batch_size= 64, validation_split = 0.2, epochs= 10)","2475b4fb":"pred4 = model.predict_classes(x_test1)","e947f556":"conf = confusion_matrix(y_test1, pred4)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in ['0', '1', '2', '3', '4']],\n    columns = [i for i in ['0', '1', '2', '3', '4']]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","9c2cd8ea":"from nltk.corpus import stopwords\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS","ec3c078f":"stop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])","b23f071a":"def preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >2 and token not in stop_words:\n            result.append(token)\n    return result","7f8e8a38":"train_df['Phrase_lower'] = train_df['Phrase'].str.lower()\ntrain_df['Phrase_text_new'] = train_df['Phrase_lower'].str.replace(r'[^A-Za-z0-9]+', ' ')\ntrain_df['cleen_Phrase'] =train_df['Phrase_text_new'].apply(preprocess)\ntrain_df['cleen_Phrase']= train_df['cleen_Phrase'].apply(lambda x: \" \".join(x))\n\ntrain_df.head()","9f596182":"train_df['cleen_Phrase'][2]","6d8c6af1":"x_train2,x_test2,y_train2,y_test2=train_test_split(train_df['cleen_Phrase'],train_df['Sentiment'],test_size=0.2,random_state=42)","c0f15274":"model1 = tf.keras.models.Sequential([\n  hub_layer,\n    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n    \n  tf.keras.layers.Dense(256, activation = tf.keras.layers.LeakyReLU(alpha=0.3)),\n    tf.keras.layers.Dropout(0.3),\n    \n  tf.keras.layers.Dense(64, activation = tf.keras.layers.LeakyReLU(alpha=0.3)),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(8, activation='relu'),\n  tf.keras.layers.Dense(5, activation='softmax')\n  ])\nmodel1.summary()\nmodel1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics= ['acc','MAE'])\n#model.fit(x_train1, y_train, batch_size= 64, validation_split = 0.2, epochs= 10)","39baa59b":"indices = y_train2\ndepth = 5\nY_train = tf.one_hot(indices, depth)","f7b9662b":"model1.fit(x_train2, Y_train, batch_size= 64, validation_split = 0.2, epochs= 5)","69966219":"pred5 = model1.predict_classes(x_test2)","059a4985":"conf = confusion_matrix(y_test2, pred5)\n\ncm = pd.DataFrame(\n    conf, index = [i for i in ['0', '1', '2', '3', '4']],\n    columns = [i for i in ['0', '1', '2', '3', '4']]\n)\n\nplt.figure(figsize = (12,7))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","bcf3a7e1":"# SVM","ad5c498a":"Model traing Acc","09621699":"# wordcloud of negative review","5cec3ef1":"# wordcloud of Very very postive review","0c72712a":"# Please Upvote this notebook if you find it useful","5951e51d":"# This notebook shows how to implement different Machine learning algorithms to do sentiment analyses.","f635770b":"Naive Bayes is the simplest Machine learning model that can use to do classifications. Even without pre-processing the text, Naive Bayes got an accuracy of 61%","6573da8f":"# wordcloud of Very natural review","3958b294":"# Data Analysis","4c3c1179":"# Base line for a model","189720bc":"# Wordcloud of Very postive review ","a560cf09":"# EDA","b72c7449":"# EDA","e5408a82":"# Let's see model performance after cleaning the data","65a6213d":"# Naive bayes moldes without data preprocesing","849df27e":"# Data visualization ","8de596a3":"# TensorFlow Model","3cb605ba":"# I hope you find this notebook useful please upvote the notebook and give your comment.","af2c66dc":"# Let's combined SentenceId and Phrase data.","1ef8bf25":"# **wordcloud of Very negative review**","1096a8a8":"# Gradient Boosting Classifier","caf37aaa":"# Data loading","57627be9":"**This notebook shows how to implement Machine learning and deep learning models to do sentiment analysis even without any data preprocessing and hyperparameter tuning the accuracies of the models are reasonably good.**\n","dbc9c3b0":"# The graph shows that most of the sentiments in SentenceId 3189 are negative but most of the sentiment in the full data set is natural.\nso the SentenceId can be used as a feature to improve the results."}}