{"cell_type":{"3a1377c2":"code","352abb8a":"code","711e6064":"code","09dd4033":"code","16323ee7":"code","796a4e63":"code","6e3db7c1":"code","3a6d8798":"code","dc22e58f":"code","fab937e1":"code","9daf4342":"code","cd9d4be2":"code","f6ce264a":"code","145749c9":"code","68a5df36":"code","99c8b992":"code","5f11db5e":"code","dddece1b":"code","af04ead7":"code","4a711355":"code","a6fc80a7":"code","c6f941c3":"code","4e81aaec":"code","7e0ff802":"code","a317cdb0":"code","e1f08da2":"code","c2f9c9dd":"code","769c6e09":"code","473a07a3":"code","90e42c9b":"code","69009fae":"code","7e0d82a6":"code","257420a0":"code","143463d8":"code","cd2e8e55":"code","80517a59":"code","6cf2da94":"code","f2ebc36b":"code","825cf4b7":"code","451adeb7":"code","f8c2c194":"code","921c7bc2":"code","58b3e724":"markdown","52db55dd":"markdown","88e6dfa6":"markdown","f27c7e6d":"markdown","b902e8fb":"markdown","815a5745":"markdown","9bc35142":"markdown","e20106e9":"markdown","e9cc1f32":"markdown","18f95af7":"markdown","c50fd897":"markdown","b5cacd0e":"markdown","211e27d7":"markdown","44872d51":"markdown","aadffd93":"markdown","b2ab0e01":"markdown","51605657":"markdown","67e1abb9":"markdown","f7d02c23":"markdown","a8b2ff3f":"markdown","2f0ea8f9":"markdown","ebadfa5d":"markdown","5a0691a4":"markdown","87fe4877":"markdown","e27d6e5c":"markdown","f2e02e68":"markdown","1e0769de":"markdown","4da31c13":"markdown","057673c0":"markdown","2ebb4eb9":"markdown","2cdcd3a2":"markdown","5632025d":"markdown","3b8dd7eb":"markdown","c1e03fd3":"markdown","c1197de8":"markdown","af9ff5d8":"markdown","105a3730":"markdown","2324db3d":"markdown","6d7e7c8f":"markdown","68fba223":"markdown"},"source":{"3a1377c2":"import pandas as pd\nimport numpy as np\nimport warnings\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","352abb8a":"warnings.simplefilter(action = 'ignore', category = FutureWarning)","711e6064":"sns.set_theme(rc = {'grid.linewidth': 0.6, 'grid.color': 'white',\n                    'axes.linewidth': 2, 'axes.facecolor': '#ECECEC', \n                    'axes.labelcolor': '#000000',\n                    'figure.facecolor': 'white',\n                    'xtick.color': '#000000', 'ytick.color': '#000000'})","09dd4033":"df = pd.read_csv('..\/input\/car-price-prediction\/CarPrice_Assignment.csv')","16323ee7":"Cat_vars = []\nNum_vars = []\n\nfor col in list(df.columns):\n    \n    if ((df[col].dtype == 'float64') | (df[col].dtype == 'int64')) & (df[col].nunique() > 10):\n        \n        Num_vars.append(col)\n    \n    else: Cat_vars.append(col)","796a4e63":"Num_vars.remove('car_ID')\nNum_vars.remove('price')\n\nCat_vars.remove('CarName')","6e3db7c1":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7}): \n\n    fig_1, ax_1 = plt.subplots(5, 3, figsize = (9, 11))\n\n    for idx, (column, axes) in list(enumerate(zip(Num_vars, ax_1.flatten()))):\n    \n        sns.regplot(ax = axes, x = df[column], y = np.log(df['price']),\n                    scatter_kws = {'s': 1, 'color': 'k'}, \n                    line_kws = {'lw': 0.5, 'color': 'r'})\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax_1.flatten()[idx + 1:]]\n\n    fig_1.suptitle('Numeric variables', fontsize = 11)\n    plt.tight_layout(pad = 1)\n    plt.show()","3a6d8798":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 8.5, \n                          'xtick.labelsize': 7.5, 'ytick.labelsize': 7.5}): \n\n    fig_2, ax_2 = plt.subplots(4, 3, figsize = (9.5, 11.5))\n\n    for idx, (column, axes) in list(enumerate(zip(Cat_vars, ax_2.flatten()))):\n    \n        plt.setp(axes.collections, alpha = 0.1)\n    \n        sns.stripplot(ax = axes, x = df[column], \n                      y = np.log(df['price']),\n                      color = 'k', s = 1.5, alpha = 1,\n                      jitter = 0.15)\n        \n        sns.pointplot(ax = axes, x = df[column],\n                      y = np.log(df['price']),\n                      color = 'r', scale = 0.25,\n                      estimator = np.median, # ci = 'sd', capsize = 0.05,\n                      join = True, errwidth = 0)\n    \n        plt.setp(axes.lines, zorder = 100)\n        plt.setp(axes.collections, zorder = 100)\n        \n        if (df[column].dtypes == 'O') & (df[column].nunique() > 4):\n        \n            plt.setp(axes.get_xticklabels(), rotation = 90)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax_2.flatten()[idx + 1:]]\n\n    fig_2.suptitle('Categorical variables', fontsize = 11)\n    plt.tight_layout()\n    plt.show()","dc22e58f":"X = pd.concat([pd.get_dummies(df[['drivewheel', 'aspiration']].astype(object), drop_first = True), \n              df[['horsepower', 'wheelbase']]],\n              axis = 1, join = 'outer', ignore_index = False)\n\nY = df['price']","fab937e1":"import statsmodels.api as sm\nfrom statsmodels.iolib.summary2 import summary_col","9daf4342":"model_1 = sm.OLS(endog = np.log(Y), exog = sm.add_constant(X))\nmodel_1_fit = model_1.fit()\nmodel_1_pred = model_1_fit.predict()\n\nprint(model_1_fit.summary())","cd9d4be2":"model_1_a = sm.OLS(endog = np.log(Y), exog = sm.add_constant(X.drop('horsepower', axis = 1)))\nmodel_1_a_fit = model_1_a.fit()\n\nmodel_1_b = sm.OLS(endog = np.log(Y), exog = sm.add_constant(X.drop('wheelbase', axis = 1)))\nmodel_1_b_fit = model_1_b.fit()","f6ce264a":"pd.DataFrame([model_1_fit.bse, model_1_fit.params, (model_1_fit.bse \/ model_1_fit.params)*100,\n      model_1_a_fit.bse, model_1_a_fit.params, (model_1_a_fit.bse \/ model_1_a_fit.params)*100,\n      model_1_b_fit.bse, model_1_b_fit.params, (model_1_b_fit.bse \/ model_1_b_fit.params)*100],\n      index = ['SE', 'Coeff', 'SE \/ Coeff, %', 'SE', 'Coeff', 'SE \/ Coeff, %', \n               'SE', 'Coeff', 'SE \/ Coeff, %']).iloc[0:, 4:]","145749c9":"from statsmodels.stats.outliers_influence import variance_inflation_factor","68a5df36":"X_VIF = model_1_fit.model.exog\nVIF = list((variance_inflation_factor(X_VIF, i) for i in range(X_VIF.shape[1])))\n\ndf_VIF = pd.DataFrame(VIF, columns = ['VIF'], index = list(model_1_fit.params.index))\n\ndf_VIF.iloc[4:, 0:].round(4)","99c8b992":"X_2 = df[['carwidth', 'carlength', 'citympg', 'boreratio', 'stroke']]\n\nmodel_2 = sm.OLS(endog = np.log(Y), exog = sm.add_constant(X_2))\nmodel_2_fit = model_2.fit()\n\nprint(model_2_fit.summary())","5f11db5e":"from sklearn.preprocessing import StandardScaler","dddece1b":"def VV_(df, Vars_list):\n    \n    ### Scaling data\n    \n    scaler = StandardScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(df.filter(Vars_list)), \n                               columns = df.filter(Vars_list).columns)\n    \n    ### Calculating the correlation matrix\n    \n    Corr_m = data_scaled.corr()\n    \n    ### Getting eigenvalues and eigenvectors\n    \n    values, vectors = np.linalg.eig(Corr_m)\n    \n    return(values, vectors)","af04ead7":"Results = VV_(X_2, list(X_2.columns))","4a711355":"from IPython.core.display import HTML\n\ndef multi_table(table_list):\n    ''' Acceps a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n    '''\n    return HTML(\n        '<table><tr style=\"background-color:white;\">' + \n        ''.join(['<td>' + table._repr_html_() + '<\/td>' for table in table_list]) +\n        '<\/tr><\/table>')","a6fc80a7":"A = pd.DataFrame({'Eigenvalue': Results[0]},\n                 index = ['PC ' + str(X) for X in range(1, len(Results[0]) + 1)]).round(3)\n\nB = pd.DataFrame({'Variance, %': Results[0] \/ sum(Results[0])*100},\n                 index = ['PC ' + str(X) for X in range(1, len(Results[0]) + 1)]).round(1)\n\nC = pd.DataFrame(Results[1], index = list(X_2.columns),\n                 columns = ['PC ' + str(X) for X in range(1, len(Results[0]) + 1)]).round(2)","c6f941c3":"multi_table([A, B, C])","4e81aaec":"print(model_1_fit.summary())","7e0ff802":"det_1 = np.linalg.det(X.iloc[0:, 0:2].corr())\ndet_2 = np.linalg.det(X.iloc[0:, 2:].corr())\ndet_3 = np.linalg.det(X.corr())\n\nGVIF = det_1 * det_2 \/ det_3\n\nGVIF_all = pd.DataFrame({'GVIF': [GVIF],\n                        'GVIF^(1\/2Df)': [np.power(GVIF, 1 \/ (2 * len(X.iloc[0:, 0:2].columns)))]},\n                        index = ['drivewheel'])\n\nGVIF_all.round(3)","a317cdb0":"df['log price'] = np.log(df['price'])\nmodel_1_results = pd.DataFrame({'log price fitted': model_1_pred, 'residuals': model_1_fit.resid})","e1f08da2":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 10, \n                          'xtick.labelsize': 9, 'ytick.labelsize': 9}): \n\n    fig_3, ax_3 = plt.subplots(3, 3, figsize = (10.5, 9))\n    ax_flat = ax_3.flatten()\n    \n    ### Plots \u2116 1 - 3\n    \n    sns.scatterplot(ax = ax_flat[0], y = model_1_results['residuals'], x = df['horsepower'], \n                    s = 9.5, color = 'k')\n    \n    ax_flat[0].set_title('a) Residuals vs $X_5$', fontsize = 11, color = 'black')\n    ax_flat[0].axvspan(xmin = 250, xmax = 300, \n                       ymin = 0.02, ymax = 0.27, \n                       alpha = 0.2, color = 'red')\n    \n    sns.scatterplot(ax = ax_flat[1], y = model_1_results['residuals'], x = df['wheelbase'],\n                    s = 9.5, color = 'k')\n    \n    ax_flat[1].set_title('b) Residuals vs $X_6$', fontsize = 11, color = 'black')\n\n    sns.scatterplot(ax = ax_flat[2], y = model_1_results['residuals'], \n                    x = model_1_results['log price fitted'], s = 9.5, color = 'k')\n    \n    ax_flat[2].set_title('c) Residuals vs $\\hatY$', fontsize = 11, color = 'black')\n    ax_flat[2].axvspan(xmin = 10.5, xmax = 11, \n                       ymin = 0.02, ymax = 0.27, \n                       alpha = 0.2, color = 'red')\n    \n    ### Plots \u2116 3 - 6\n    \n    sns.scatterplot(ax = ax_flat[3], y = model_1_results['residuals'], x = Y.index,\n                    s = 9.5, color = 'k')\n    \n    ax_flat[3].set_title('d) Residual scatterplot', fontsize = 11, color = 'black')\n    \n    sns.histplot(ax = ax_flat[4], x = model_1_results['residuals'], kde = True,\n                 color = 'k')\n    \n    ax_flat[4].set_title('e) Residual histogram', fontsize = 11, color = 'black')\n    \n    sm.qqplot(model_1_results['residuals'], line = 'q', markerfacecolor = 'k', \n              markeredgecolor = 'k', markersize = 2, ax = ax_flat[5])\n    \n    ax_flat[5].set_title('f) Residual qq-plot', fontsize = 11, color = 'black')\n    \n    ### Plot \u2116 7\n    \n    sns.scatterplot(ax = ax_flat[6], y = model_1_results['residuals'], x = df['log price'],\n                    s = 9.5, color = 'k')\n    \n    ax_flat[6].set_title('g) Residuals vs $Y$', fontsize = 11, color = 'black')\n    x_1 = np.linspace(8.5, 10.5, 100)\n    ax_flat[6].plot(x_1, -0.01*(x_1\/2)**2 - 0.2, color = 'r', linestyle = '--')\n    ax_flat[6].plot(x_1, +0.05*(x_1\/2)**2 - 0.75, color = 'r', linestyle = '--')\n    \n    [axes.set_visible(False) for axes in ax_flat[7:]]\n    \n    fig_3.suptitle('Residual analysis', fontsize = 12)\n    plt.tight_layout(pad = 1)\n    plt.show()","c2f9c9dd":"with plt.rc_context(rc = {'figure.dpi': 300, 'axes.labelsize': 9.5, \n                          'xtick.labelsize': 8.5, 'ytick.labelsize': 8.5}): \n\n    fig_4, ax_4 = plt.subplots(2, 1, figsize = (9, 6))\n    ax_flat = ax_4.flatten()\n    \n    sns.scatterplot(ax = ax_flat[0], y = df['log price'], x = df['horsepower'], \n                    s = 11, color = 'k')\n    \n    ax_flat[0].axvspan(xmin = 257, xmax = 294, \n                       ymin = 0.75, ymax = 0.9, \n                       alpha = 0.2, color = 'red')\n    \n    [axes.set_visible(False) for axes in ax_flat[1:]]\n    \n    plt.tight_layout(pad = 1)\n    plt.show()","769c6e09":"from statsmodels.stats.diagnostic import normal_ad\nfrom scipy.stats import shapiro","473a07a3":"round(shapiro(model_1_results['residuals'])[1], 3)","90e42c9b":"round(normal_ad(model_1_results['residuals'])[1], 3)","69009fae":"from statsmodels.stats.diagnostic import het_breuschpagan\nfrom statsmodels.stats.diagnostic import het_white","7e0d82a6":"C = pd.DataFrame(het_breuschpagan(model_1_results['residuals'], model_1.exog),\n                 index = ['Lm', 'Lm p-value', 'F', 'F p-value'],\n                 columns = ['Value']).round(3)\n\nD = pd.DataFrame(het_white(model_1_results['residuals'], model_1.exog),\n                 index = ['Lm', 'Lm p-value', 'F', 'F p-value'],\n                 columns = ['Value']).round(3)","257420a0":"multi_table([C, D])","143463d8":"from statsmodels.graphics import tsaplots\nimport matplotlib.collections","cd2e8e55":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 11, \n                          'xtick.labelsize': 10, 'ytick.labelsize': 10}): \n\n    fig_6, ax_6 = plt.subplots(2, 2, figsize = (11, 7))\n    ax_flat = ax_6.flatten()\n\n    tsaplots.plot_acf(model_1_results['residuals'], lags = 36, ax = ax_flat[0],\n                      markerfacecolor = 'k', markeredgecolor = 'k', markersize = 3,\n                      vlines_kwargs={\"colors\": 'k'})\n    \n    ax_flat[0].collections[1].set_facecolor('r')\n    ax_flat[0].lines[0].set_color('k')\n    \n    tsaplots.plot_pacf(model_1_results['residuals'], lags = 36, ax = ax_flat[1],\n                       markerfacecolor = 'k', markeredgecolor = 'k', markersize = 3,\n                       vlines_kwargs={\"colors\": 'k'})\n    \n    ax_flat[1].collections[1].set_facecolor('r')\n    ax_flat[1].lines[0].set_color('k')\n\n    [axes.set_visible(False) for axes in ax_flat[2:]]\n    \n    fig_6.suptitle('ACF and PACF', fontsize = 13)\n    plt.tight_layout(pad = 1)\n    plt.show()","80517a59":"from statsmodels.stats.diagnostic  import acorr_ljungbox","6cf2da94":"acorr_ljungbox(model_1_results['residuals'], lags = 12, model_df = 6, return_df = True)","f2ebc36b":"print(model_1_fit.summary())","825cf4b7":"from statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm","451adeb7":"df['log_price'] = np.log(df['price'])","f8c2c194":"reg_model_drivewheel = ols('log_price' + '~' + 'drivewheel', data = df).fit()\n\nsm.stats.anova_lm(reg_model_drivewheel)[0:1]","921c7bc2":"reg_model_aspiration = ols('log_price' + '~' + 'aspiration', data = df).fit()\n\nsm.stats.anova_lm(reg_model_aspiration)[0:1]","58b3e724":"<p><div style = \"text-align: justify; font-size: 16px\">In this model, both <span style=\"color:#003ba8\">drivewheel<\/span> and <span style=\"color:#003ba8\">aspiration<\/span> were significant (assuming $\u03b1 = 0.05$).<\/div><\/p>","52db55dd":"<p><div style = \"text-align: justify; font-size: 16px\">Values were fairly low, supporting the idea of an insignificant multicollinearity effect.<\/div><\/p>","88e6dfa6":"### 1.1 Standard errors and $VIF$","f27c7e6d":"```\nSeries: df$price \nRegression with ARIMA(1,1,1) errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n         ar1      ma1  drivewheel_fwd  drivewheel_rwd  aspiration_turbo\n      0.5849  -0.9847         -0.0857          0.1233            0.0425\ns.e.  0.0745   0.0223          0.0620          0.0736            0.0318\n      horsepower  wheelbase\n          0.0062     0.0207\ns.e.      0.0005     0.0034\n\nsigma^2 estimated as 0.03313:  log likelihood=60.54\nAIC=-105.08   AICc=-104.34   BIC=-78.54\n```","b902e8fb":"<h1><center> I. Abstract <\/center><\/h1>","815a5745":"## 1. Multicollinearity <a class=\"anchor\" id = \"III_1\"><\/a>","9bc35142":"## Thanks for reading","e20106e9":"<p><div style = \"text-align: justify; font-size: 16px\"> <b>Source<\/b>: [<a href=\"http:\/\/web.vu.lt\/mif\/a.buteikis\/wp-content\/uploads\/PE_Book\/4-5-Multiple-collinearity.html\">1<\/a>].<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\">In $R$ this procedure can be done with ease. These are the results I got for all the regressors:<\/div><\/p>","e9cc1f32":"<p><div style = \"text-align: justify; font-size: 16px\">When the residuals of your model are highly correlated, it makes sense to opt for regression with $ARIMA$ errors because you will not loose interpretability. I have a <a href=\"https:\/\/www.kaggle.com\/suprematism\/time-series-analysing-and-forecasting\">notebook<\/a> (in $R$) dedciated to the topic of time series where such models were used.<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\">To make this example more complete, I built a model with the same parameters but $ARIMA$ errors:<\/div><\/p>","18f95af7":"### I. Abstract\n\n### II. Quick EDA\n\n### III. Building models\n\n* [1. Multicollinearity](#III_1)\n\n\n* [2. Residual analysis](#III_2)\n\n\n* [3. Some notes on how to interpret coefficients of dummy variables](#III_3)","c50fd897":"<h1><center> II. Quick EDA <\/center><\/h1>","b5cacd0e":"<p><div style = \"text-align: justify; font-size: 16px\">In both cases, $\\widehat{se}(\\hat{\u03b2}_i)$ and $\\hat{\u03b2}_i$ did not change drastically, which was definitely a good sign.<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\">One can also calculate $VIF$ so as to get a better idea of how collinear predictors affect a model. Note, you should analyse $VIF$ values only for continuous variables.<\/div><\/p>","211e27d7":"## 3. Some notes on how to interpret coefficients of dummy variables <a class=\"anchor\" id = \"III_3\"><\/a>","44872d51":"<p><div style = \"text-align: justify; font-size: 16px\">Before starting hectically deleting collinear variables, analyse their standard errors and explore what happens with coefficients if you temporarily remove one of such regressors from the model:<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\">For instance, both <span style=\"color:#003ba8\">horsepower<\/span> and <span style=\"color:#003ba8\">wheelbase<\/span> were positively correlated with the target variable and each other. Nonetheless, their standard errors were not inflated. One can see it by comparing $\\widehat{se}(\\hat{\u03b2}_i)$ to $\\hat{\u03b2}_i$:<\/div><\/p>\n\n$$Horsepower: \\frac{\\widehat{se}(\\hat{\u03b2}_5)}{\\hat{\u03b2}_5} = \\frac{0.000468}{0.007775}\u00d7100 = 6.016\\text{%}$$\n\n$$Wheelbase: \\frac{\\widehat{se}(\\hat{\u03b2}_6)}{\\hat{\u03b2}_6} = \\frac{0.002908}{0.027494}\u00d7100 = 10.576\\text{%}$$\n\n<p><div style = \"text-align: justify; font-size: 16px\">Following that, I created and compared 2 models: with <span style=\"color:#003ba8\">horsepower<\/span> but without <span style=\"color:#003ba8\">wheelbase<\/span> and vice versa.<\/div><\/p>","aadffd93":"<p><div style = \"text-align: justify; font-size: 16px\">After the first model was built, the following warning appeared:<\/div><\/p>\n\n```\n[2] The condition number is large, 2.89e+03. This might indicate that there are strong multicollinearity or other numerical problems.\n```\n\n<p><div style = \"text-align: justify; font-size: 16px\">When we use regression for interpreting results, we need to address multicollinearity, otherwise our coefficients may become unstable, despite the fact that the efficacy of the model itself is not affected. However, it is important to remember that not every case of multicollinearity results in the aforementioned issues.<\/div><\/p>","b2ab0e01":"## 2. Residual analysis <a class=\"anchor\" id = \"III_2\"><\/a>","51605657":"![6) Model.png](attachment:63da3cce-539b-41aa-bffe-6f1a4d7fcd79.png)","67e1abb9":"<p><div style = \"text-align: justify; font-size: 16px\">In this notebook I tried covering the topic of linear regression, addressing, to varying degrees, questions of multicollinearity, autocorrelation, normality etc.<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\"><b>Important note<\/b>: should you want to maximise predictive accuracy, you have to opt for more sophisticated algorithms. Nevertheless, linear regression is still valuable when it comes to exploring relationships, making statistical inferences and interpreting results of your research.<\/div><\/p>","f7d02c23":"<h1><center> Table of contents <\/center><\/h1>","a8b2ff3f":"### 1.2 Eigenvalues and eigenvectors","2f0ea8f9":"### 2.3 Autocorrelation. Practice","ebadfa5d":"### 1.3 Using $GVIF$ for mixed data","5a0691a4":"<p><div style = \"text-align: justify; font-size: 16px\">According to both tests, we did not have enough evidence to conclude that our residuals were not distributed normally (assuming $\u03b1 = 0.05$).<\/div><\/p>","87fe4877":"<p><div style = \"text-align: justify; font-size: 16px\">If your model contains both continuous and categorical (dummy-encoded) regressors, you can be interested in exploring whether your categorical variables cause multicollinearity. In this case, $GVIF$ can help you out.<\/div><\/p>\n    \n<p><div style = \"text-align: justify; font-size: 16px\">To make $GVIF$ comparable, it incorporates degrees of freedom. For instance, if your categorical variable can be encoded via 4 dummy variables, it has 4 \u2013 1 = 3 degrees of freedom: $GVIF^{(0.5Df)}$. Thus, $GVIF$ for continuous predictors is the same as $VIF$ (since each of them has only 1 degree of freedom). Finally, should you want to make $GVIF^{(0.5Df)}$ comparable to $VIF$ in order to apply some frequently used rules of thumb, you will need to square $GVIF^{(0.5Df)}$.<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\"> <b>Source<\/b>: [<a href=\"https:\/\/www.jstor.org\/stable\/2290467\">1<\/a>, <a href=\"https:\/\/stats.stackexchange.com\/questions\/70679\/which-variance-inflation-factor-should-i-be-using-textgvif-or-textgvif\/96584#96584\">2<\/a>].<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\">Unfortunately, $GVIF$ has not been implemented in Python, so I calcuated it manually (for <span style=\"color:#003ba8\">drivewheel<\/span>).<\/div><\/p>","e27d6e5c":"<p><div style = \"text-align: justify; font-size: 16px\">It is evident that there were multiple significant correlation coefficients. Thus, the presence of autocorrelation was confirmed. Based on $PACF$ plot it can be seen that the second coefficient (lag 2) became barely significant. So, $\\hat{\u03b5}_{t-1}$ could explain the autocorrelation structure of the residuals well enough.<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\">I have to admit, having such a strong autocorrelation in panel data is a little odd. Lastly, I performed the Ljung-Box test so as to one more time confirm my findings:<\/div><\/p>","f2e02e68":"<p><div style = \"text-align: justify; font-size: 16px\">We can use eigenvalues and eigenvectors to determine the presence of multicollinearity:<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\">First, we should explore eigenvalues, paying attention to low values as they show us which principal components explain the least amount of variance and, as a result, show collinearity. After that, we need to analyse elements of eigenvectors, figuring out what variables are collinear.<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\"> <b>Source<\/b>: [<a href=\"https:\/\/stackoverflow.com\/questions\/25676145\/capturing-high-multi-collinearity-in-statsmodels\">1<\/a>].<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\">To make it more evident, I built a second model containing only continuous predictors (again, this method is used for identifying what continuous variables are collinear):<\/div><\/p>","1e0769de":"### 2.2 Heteroskedasticity and normality. Practice","4da31c13":"<p><div style = \"text-align: justify; font-size: 16px\">Since the purpose of this notebook was not to determine the most important variables but rather showcase some hopefully useful techniques necessary for making your analysis efficient, I picked several regressors (almost) randomly.<\/div><\/p>","057673c0":"### 2.1 Assumptions overview. Theory","2ebb4eb9":"<p><div style = \"text-align: justify; font-size: 16px\">To my mind, the best way to make sure that these assumptions hold is to produce some insightful graphs:<\/div><\/p>","2cdcd3a2":"<h1><center> III. Building models <\/center><\/h1>","5632025d":"<p><div style = \"text-align: justify; font-size: 16px\">First, I want to note that the problem of autocorrelation was fully addressed. Second, I believe it is important to mention that the coefficients of our parameters did not change much.<\/div><\/p>","3b8dd7eb":"<p><div style = \"text-align: justify; font-size: 16px\">According to both tests, we did not have enough evidence to conclude that our residuals were homoskedastic (assuming $\u03b1 = 0.05$).<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\"><b>Summary<\/b>:<\/div><\/p>\n\n* <div style = \"text-align: justify; font-size: 16px\">Heteroskedasticity was present;<\/div>\n\n* <div style = \"text-align: justify; font-size: 16px\">Autocorrelation was potentially present. Further analysis was needed;<\/div>\n\n* <div style = \"text-align: justify; font-size: 16px\">Residuals were approximately normal.<\/div>\n\n<p><div style = \"text-align: justify; font-size: 16px\">Since regressors were picked just for the sake of argument, it was not surpirsing that plenty of problems occurred. Nonetheless, it just showed that visual analysis worked as intended.<\/div><\/p>","c1e03fd3":"<p><div style = \"text-align: justify; font-size: 16px\">Since significance of dummy variables is interpreted relative to the excluded category, we need additionally test whether included categorical variables are significant at all. To do so, we can use $ANOVA$.<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\"> <b>Source<\/b>: [<a href=\"https:\/\/stats.stackexchange.com\/questions\/160026\/why-does-an-insignificant-regressor-become-significant-if-i-add-some-significant\">1<\/a>, <a href=\"https:\/\/towardsdatascience.com\/statistics-in-python-using-anova-for-feature-selection-b4dc876ef4f0\">2<\/a>].<\/div><\/p>","c1197de8":"<div style = \"text-align: justify; font-size: 16px\">Spotting autocorrelation based on the previous plots might be quite challenging. That is why $ACF$ and $PACF$ plots should be drawn as well:<\/div>","af9ff5d8":"<p><div style = \"text-align: justify; font-size: 16px\">PC 3 had the lowest eigenvalue, explaining around 3% of the total variance in data. Unsurprisingly, the largest absolute values of the eigenvector's elements were 0.68 and 0.73. These two values represented <span style=\"color:#003ba8\">carwidth<\/span> and <span style=\"color:#003ba8\">carlength<\/span> respectively.<\/div><\/p>","105a3730":"```\nvif(lm_model)\n\n               GVIF Df  GVIF^(1\/(2*Df))\ndrivewheel 1.805168  2        1.159123\naspiration 1.127569  1        1.061871\nhorsepower 1.575096  1        1.255028\nwheelbase  1.411371  1        1.188012\n```","2324db3d":"<p><div style = \"text-align: justify; font-size: 16px\"><b>Residuals conditioning on $X$<\/b>:<\/div><\/p>\n\n* <div style = \"text-align: justify; font-size: 16px\">Plot \u2116 3 (c): it can be seen that, overall, there were no obvious patterns. That is exactly what we normally aim to achieve. Nonetheless, it is important to mention that the 2 observations highlighted in red were outliers, probably not the most influential ones, but still;<\/div>\n\n\n* <div style = \"text-align: justify; font-size: 16px\">Plot \u2116 1 (a) and plot \u2116 2 (b) can be helpful in identifying some unusual relationships specific to a particular variable. For instance, the outliers that were spotted on the 3rd graph (c) could be explained through the analysis of the 1st plot (a). After figuring out which variable makes your model worse, you can then explore it further. For example, I decided to check how <span style=\"color:#003ba8\">horsepower<\/span> was related to <span style=\"color:#003ba8\">log price<\/span>:<\/div>","6d7e7c8f":"<p><div style = \"text-align: justify; font-size: 16px\">We consider the population regression model: $Y = f(x) + \u03b5$, where $f(x)$ is $E(Y|x)$, and it depends on parameters $\u03b2_1,\u03b2_2,\u2026,\u03b2_k$. Thus, a sample regression model is $Y = \\hat{f}(x) + \\hat{\u03b5}$, and it depends on $\\hat{\u03b2}_1,\\hat{\u03b2}_2,\u2026,\\hat{\u03b2}_k$.<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\"><b>Required assumptions (fixed $X$ \u2013 experimental studies)<\/b>:<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\">1) $\\hat{\u03b2}_i$ is unbiased if:<\/div><\/p>\n\n* <div style = \"font-size: 16px\">$E(\u03b5) = 0$.<\/div>\n\n<p><div style = \"text-align: justify; font-size: 16px\">2) $\\hat{\u03b2}_i$ is the BLUE (best linear unbiased estimator) if:<\/div><\/p>\n    \n* <div style = \"font-size: 16px\">$E(\u03b5) = 0$;<\/div>\n* <div style = \"font-size: 16px\">$Var(\u03b5) = \u03c3^2 < \u221e$;<\/div> \n* <div style = \"font-size: 16px\">$Cov(\u03b5_i,\u03b5_j) = 0, i \u2260 j$.<\/div> \n    \n<p><div style = \"text-align: justify; font-size: 16px\">3) We can test hypothesis and create confidence intervals if:<\/div><\/p>\n\n* <div style = \"font-size: 16px\">$\u03b5$ ~ $N(0,\u03c3^2)$. In other words, $Y$ ~ $N(f(x),\u03c3^2)$.<\/div>\n\n<p><div style = \"text-align: justify; font-size: 16px\"><b>Required assumptions (random $X$ \u2013 observational studies)<\/b>:<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\">1) $\\hat{\u03b2}_i$ is unbiased if:<\/div><\/p>\n\n* <div style = \"font-size: 16px\">$E(\u03b5|X) = 0$.<\/div>\n\n<p><div style = \"text-align: justify; font-size: 16px\">2) $\\hat{\u03b2}_i$ is the BLUE (best linear unbiased estimator) if:<\/div><\/p>\n    \n* <div style = \"font-size: 16px\">$E(\u03b5|X) = 0$;<\/div>\n* <div style = \"font-size: 16px\">$Var(\u03b5|X) = \u03c3^2 < \u221e$;<\/div> \n* <div style = \"font-size: 16px\">$Cov(\u03b5_i,\u03b5_j|X) = 0, i \u2260 j$.<\/div> \n    \n<p><div style = \"text-align: justify; font-size: 16px\">3) We can test hypothesis and create confidence intervals if:<\/div><\/p>\n\n* <div style = \"font-size: 16px\">$\u03b5$ ~ $N(0,\u03c3^2)$. In other words, $Y$ ~ $N(f(x),\u03c3^2)$.<\/div>\n\n<p><div style = \"text-align: justify; font-size: 16px\"> <b>Source<\/b>: [<a href=\"https:\/\/stats.stackexchange.com\/questions\/246047\/independent-variable-random-variable\">1<\/a>, <a href=\"https:\/\/towardsdatascience.com\/ols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5\">2<\/a>].<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\"><b>Some other important requirements (in practice)<\/b>:<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\">1) While exploring $Cov(\\hat{\u03b5}_i,X)$ can allow us to understand better how a particular regressor behaves in our model, $Cov(\\hat{\u03b5}_i,\\hat{f}(x))$ reveals whether a meaningful relationship between $\\hat{\u03b5}_i$ and $\\hat{f}(x)$ exists. The latter is frequently caused by a lack of important regressor(s):<\/div> <\/p>\n\n* <div style = \"font-size: 16px\">$Cov(\\hat{\u03b5}_i,X) = 0$ and $Cov(\\hat{\u03b5}_i,\\hat{f}(x)) = 0$.<\/div>\n\n<p><div style = \"text-align: justify; font-size: 16px\">2) This property is true by definition. However, a strong correlation is not a good sign because it means that your model does not include important independent variables:<\/div><\/p>\n\n* <div style = \"font-size: 16px\">$Cov(\\hat{\u03b5}_i,y_i) \u2260 0$ but not too strong.<\/div>\n\n<p><div style = \"text-align: justify; font-size: 16px\"> <b>Source<\/b>: [<a href=\"https:\/\/stats.stackexchange.com\/questions\/155587\/residual-plots-why-plot-versus-fitted-values-not-observed-y-values\">1<\/a>, <a href=\"https:\/\/stats.stackexchange.com\/questions\/476372\/plotting-residuals-vs-fitted-values-or-vs-independent-variables\">2<\/a>, <a href=\"https:\/\/stats.stackexchange.com\/questions\/406478\/high-correlation-between-residuals-and-dependent-variable\">3<\/a>].<\/div><\/p>","68fba223":"<p><div style = \"text-align: justify; font-size: 16px\">One can fairly argue that the highlighted observations our model failed to capture well enough were not at all outliers, and, to some extent, I can agree. Nevertheless, we cannot be sure whether those values were representative of a nonlinear relationship (since we don\u2019t have a sufficient number of similar observations) or they were simply unusual. That is why domain knowledge can be absolutely crucial.<\/div><\/p>\n\n<p><div style = \"text-align: justify; font-size: 16px\"><b>Residual distribution<\/b>:<\/div><\/p>\n\n* <div style = \"text-align: justify; font-size: 16px\">Plot \u2116 5 (e) and plot \u2116 6 (f) should help us identify what distribution our residuals have. If we see that they are far from being normal, we can opt for GLM or use bootstrap to construct confidence intervals. In this case, residuals were close to be normal; a bit heavy on the right tail, which was the case because of the two \"outliers\". Note, if your data has some significant outliers, you are better off working with either quantile regression or robust regression;<\/div>\n\n\n* <div style = \"text-align: justify; font-size: 16px\">Plot \u2116 7 (g): as expected, plotting $Y$ against residuals showed some positive correlation, but it was not too strong;<\/div>\n\n\n* <div style = \"text-align: justify; font-size: 16px\">Finally, plot \u2116 4 (d) should look meaningless: no trends, no heteroskedasticity. This model failed to achieve it \u2013 we can clearly see some spikes and non-constant variance. Also, when the summary of our model was printed, the value of Durbin-Watson statistic was low (1.094), suggesting autocorrelation. This was discussed more thoroughly a bit later.<\/div>\n\n<p><div style = \"text-align: justify; font-size: 16px\">In addition to visual analysis, we can also perform some tests:<\/div><\/p>"}}