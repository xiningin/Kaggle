{"cell_type":{"bce6039d":"code","a1d87cdd":"code","d7b15c5c":"code","d9c6999c":"code","2f197485":"code","ec9f21e5":"code","0109d595":"code","09c08167":"code","7c5b169e":"code","eeee2382":"code","6f84b749":"code","0626c4d4":"code","336665f8":"code","89d4e984":"code","08ca4b3c":"code","11e08001":"code","52346d19":"code","e5b7566d":"code","8f139c07":"code","7cdceb5d":"markdown","2d4899e2":"markdown","d7a1b431":"markdown","b1c2597c":"markdown","70d0d672":"markdown","420103ce":"markdown","8e837cee":"markdown","1cd92182":"markdown","34aa3a7c":"markdown"},"source":{"bce6039d":"\n\nfrom IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\n\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}\n\nsns.set_style(\"darkgrid\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") \n        \npd.options.display.max_columns = 1000\npd.options.display.max_rows = 1000\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a1d87cdd":"train=pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv\")","d7b15c5c":"print(\"train shape is:\",train.shape)\nprint(\"test shape is:\",test.shape)","d9c6999c":"train.head()","2f197485":"test.head()","ec9f21e5":"print(train[train['target']==1]['target'].value_counts())\nprint(train[train['target']==0]['target'].value_counts())","0109d595":"train.drop('id',axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)","09c08167":"# cont_features","7c5b169e":"# here i have just plotted down first  6 features you can try all of them it will take a lot of time though due to large amount of features\ncont_features=list(test.columns)\nprint(\"Feature distribution of continous features: \")\nncols = 3\nnrows = 2\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(40,10), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = cont_features[r*ncols+c]\n        sns.kdeplot(x=train[col], ax=axes[r, c], color='#58D68D', label='Train data')\n        sns.kdeplot(x=test[col], ax=axes[r, c], color='#DE3163', label='Test data')\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=15, fontweight='bold')\n        axes[r, c].tick_params(labelsize=10, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(20)\n        axes[r, c].yaxis.offsetText.set_fontsize(20)\nplt.show()","eeee2382":"#import libraries\nfrom numpy.random import seed\nseed(42)\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=5, verbose=0,\n    mode='min')","6f84b749":"train.var()","0626c4d4":"colNames=[col for col in test.columns if col not in 'id','f2','f35','f44']\n# colNames.remove('id')\ntrain.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\ntrain_nn=train[colNames].copy()\ntest_nn=test[colNames].copy()\ntrain_nn=pd.concat([train_nn,test_nn])\n# for col in colNames:\n#     #print(col)\n#     qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n#     train_nn[col] = qt.fit_transform(train_nn[[col]])\n#     test_nn[col] = qt.transform(test_nn[[col]])    \n# test_nn=train_nn[-test_nn.shape[0]:]\n# train_nn=train_nn[:-test_nn.shape[0]]","336665f8":"train_nn['target']=train['target']","89d4e984":"#https:\/\/bignerdranch.com\/blog\/implementing-swish-activation-function-in-keras\/\nfrom keras.backend import sigmoid\nfrom sklearn.metrics import roc_auc_score\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nget_custom_objects().update({'swish': Activation(swish)})\n\nhidden_units = (128,64,32)#initialized number of neurons in each hidden layer\n\ndef base_model():\n\n    num_input = keras.Input(shape=(test.shape[1],), name='num_data')#input layer\n\n\n    out = keras.layers.Concatenate()([num_input])\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n\n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted target value probability\n    out = keras.layers.Dense(1, activation='sigmoid', name='prediction')(out)\n    \n    model = keras.Model(\n    inputs = [num_input],\n    outputs = out,\n    )\n    \n    return model","08ca4b3c":"target_name='target'\nscores_folds = {}\nmodel_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 5\nkf = model_selection.StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train_nn)\n\nfeatures_to_consider.remove('target')\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\n\ntrain_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\ntest_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n\ntrain_nn[pred_name] = 0\ntest_nn[target_name] = 0\ntest_predictions_nn = np.zeros(test_nn.shape[0])\n\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(train,train['target'])):\n    print(f'Training fold {fold + 1}')\n    X_train, X_test = train_nn.iloc[trn_ind][features_to_consider], train_nn.iloc[val_ind][features_to_consider]\n    y_train, y_test = train_nn.iloc[trn_ind]['target'], train_nn.iloc[val_ind]['target']\n    print('CV {}\/{}'.format(counter, n_folds)) \n    #############################################################################################\n    # NN\n    #############################################################################################\n    \n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.001),\n#          keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n        loss='binary_crossentropy',\n        metrics = ['AUC']\n    )\n    \n    num_data = X_train[features_to_consider]\n    \n    scaler = MinMaxScaler(feature_range=(0, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n      \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n\n    model.fit([num_data], \n              target,               \n              batch_size=2048,\n              epochs=1000,\n              validation_data=([num_data_test], y_test),\n              callbacks=[es, plateau],\n              validation_batch_size=len(y_test),\n              shuffle=True,\n             verbose = 1)\n\n    preds = model.predict([num_data_test]).reshape(1,-1)[0]\n    score = round(roc_auc_score(y_test,preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    \n    tt =scaler.transform(test_nn[features_to_consider].values)\n    \n    test_predictions_nn += model.predict([tt]).reshape(1,-1)[0].clip(0,1e10)\/n_folds       \n    counter += 1","11e08001":"print(scores_folds)","52346d19":"sub=pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\nsub['target']=test_predictions_nn","e5b7566d":"sub","8f139c07":"sub.to_csv(\"submission.csv\",index=False)","7cdceb5d":"**INFERENCE**\n*  **curves of both train and test look similar (good point) our cv will correlate with LB better.**","2d4899e2":"# **IMPORT TENSORFLOW KERAS LIBRARIES**\n\nHere i have define 2 things:\n\n* early stopping:if the model auc doesn't improve for 20 iterations training of that particular model will be stop\n\n* Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 0.2 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.","d7a1b431":"# **<font color='red'>NN BASELINE<\/font>**\n**<font color='blue'>In this notebook i have implemented a baseline model of Neural Network (do let me know if you have any doubts or find any bugs)<\/font>**\n\n\n* **SCOPE OF IMPROVEMENTS**\n1. **i haven't tested effect of batch size**\n2. **adding layers**\n3. **feature engineering on combined train and test data.**\n4. **analize train and test distribution carefully.**\n5. **try different optimizers (Nadam,AdamW,Adagrad etc.)**\n6. **try different scheduling techniques.**\n","b1c2597c":"# OBSERVATIONS\n1. **No null value is present in the dataset**\n2. **data has 102 columns\/features including target**\n3. **all 100 features are float variables**\n4. **The given problem is of binary classification.**\n\n","70d0d672":"# **<font color='green'>IMPORT LIBRARIES<\/font>**","420103ce":"# **KEY POINTS:**\n* **Swish Activation**:The choice of activation functions in Deep Neural Networks has a significant impact on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU), which is f(x)=max(0,x). Although various alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. So Google Brain Team has proposed a new activation function, named Swish, which is simply f(x) = x \u00b7 sigmoid(x). Their experiments show that Swish tends to work better than ReLU on deeper models across a number of challenging datasets. \n\n![image.png](attachment:aa68cdce-9fee-422e-a1e0-ead3785b705e.png)\n\n* **Input Layer** : every neural network has an input layer, we have passed shape=(100,) to it since there are 100 features in our data.\n\n* **Hidden Layers** :In neural networks, a hidden layer is located between the input and output of the algorithm, in which the function applies weights to the inputs and directs them through an activation function as the output. In short, the hidden layers perform nonlinear transformations of the inputs entered into the network. here we have three hidden layers of units (100,64,32) units.\n\n* **Output Layers** :There must always be one output layer in a neural network. The output layer takes in the inputs which are passed in from the layers before it, performs the calculations via its neurons and then the output is computed. here we have used sigmoid activation.Image below is the diagram of sigmid activation.\n\n![image.png](attachment:7e2ce760-5f4a-432a-ae88-1fe400f68eab.png)","8e837cee":"# **Key points:**\n\n*  **Validation strategy**: here i have used Kfold strategy, the data is divided into 5 folds , 4 of them are used for training and 1 of them is used for validation this will happen 5 times.\n* **Loss=binary cross entropy**: for understanding this you can refer this link :https:\/\/towardsdatascience.com\/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n![image.png](attachment:a780c9e1-1c64-4292-ba2e-7493ee12d7bf.png)\n* **Metrics=AUC**:This is the metrics we have to use in this competition.\n\n* **Batch size**:Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration. so our training examples are going into our network in batches here specifically they are going in the batches of 2048, you can try tweaking it and check what happens if we change it.","1cd92182":"**Uncomment the last 6 lines if you want to try exploring Quantile transformer**","34aa3a7c":"# **Quantile Transformation**\nTransform features using quantiles information.\n\nThis method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.\n\nThe transformation is applied on each feature independently. First an estimate of the cumulative distribution function of a feature is used to map the original values to a uniform distribution. The obtained values are then mapped to the desired output distribution using the associated quantile function. Features values of new\/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable."}}