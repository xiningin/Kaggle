{"cell_type":{"d8425b34":"code","7a64c74b":"code","3e369698":"code","dbb89b5c":"code","cdaf53f7":"code","dc43da7c":"code","cbc96032":"code","7849951a":"code","08bd3e01":"code","e059e3a0":"code","bf3e1e2c":"code","8adbf873":"code","cac7da29":"code","36d966d0":"code","0cf7a1d9":"code","43abedd0":"code","468b25cc":"code","2c084e41":"code","222914db":"code","dd6496fa":"code","40603002":"code","eb3bee67":"code","e1773a32":"code","37372d1f":"code","9ba5fa22":"code","8fa353a4":"code","3c3b580b":"code","9b2d68b4":"code","ba6ea1cb":"code","cf588201":"code","b9a357ab":"code","5ca897a5":"code","7af28ae2":"code","8de92295":"code","b6cccb41":"code","9807a1d1":"code","88a7f3f9":"code","36cf7df6":"code","cc5fba99":"code","c6d84630":"code","eda787f7":"code","b98c9b6d":"code","7cd502be":"code","2b4f3307":"code","e762ae29":"code","1438aace":"code","1bfaafec":"code","27cf46df":"code","03dc47ae":"code","8d543d2d":"code","370f3c5a":"code","f76e6c1d":"code","e757b438":"code","25a3948c":"code","87f95cb8":"markdown","7003bc03":"markdown"},"source":{"d8425b34":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport string\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport random\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\", encoding='latin-1')\ndf.head()","7a64c74b":"# Drop unwanted columns and rename remaining columns \ndf = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1)\ndf = df.rename(columns={'v1': 'label', 'v2': 'text'})\ndf.head()","3e369698":"#lets make another column i.e the length of the text\nlen_text=[]\nfor i in df['text']:\n    len_text.append(len(i))","dbb89b5c":"#adding length column in dataframe\ndf['text_length']=len_text","cdaf53f7":"df.head(5)","dc43da7c":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,5))\ndf[df['label']=='spam']['text_length'].plot(bins=35,kind='hist',color='blue',label='spam',alpha=0.5)\nplt.legend()\nplt.xlabel('message length')\nplt.show()","cbc96032":"plt.figure(figsize=(12,5))\ndf[df['label']=='ham']['text_length'].plot(bins=35,kind='hist',color='red',label='spam',alpha=0.5)\nplt.legend()\nplt.xlabel('message length')\nplt.show()","7849951a":"#from the above two histograms we can conclude that spam messages are mostly of length bw 150-200\n#and ham messages are of shorter length","08bd3e01":"plt.figure(figsize=(12,5))\ndf['label'].value_counts().plot(kind='bar',color='green',label='spam-vs-nonspam')\nplt.legend()\nplt.show()","e059e3a0":"# Count observations in each label\ndf.label.value_counts()\n#data.info()","bf3e1e2c":"#lets convert the target variable into numerical from for classification\ndf['label']=np.where(df['label']=='spam',1,0)","8adbf873":"df.head(5)","cac7da29":"# Extract every text \ntexts = []\nfor index, row in df.iterrows():\n    texts.append((row['text'], row['label']))\ntexts[:5]","36d966d0":"# * * * PREPROCESSING * * * \n# Remove whitespace and punctutation \ntokenized = []\nfor t in texts:\n    m = t[0]\n    text = re.sub('[' + string.punctuation + ']', ' ', m)\n    text = re.sub('[\\n\\t\\r]', '', text)\n    words = text.split()\n    tokenized.append((words, t[1]))\ntokenized[0] # First element","0cf7a1d9":"# Remove stopwords\nstopwords = []\ntry:\n    f = open('..\/input\/stopword-lists-for-19-languages\/englishST.txt', 'r')\n    stopwords = f.read().split('\\n')\nexcept IOError:\n    print('Problem opening file')\nfinally:\n    f.close()\nprint('Sentence before stopwrods removed: \\n', tokenized[51])\nfiltered = []\nfor t in tokenized:\n    text = t[0]\n    f_text = []\n    for word in text:\n        if word not in stopwords and len(word) > 2:\n            f_text.append(word)\n    filtered.append((f_text, t[1]))\n\nprint('\\nSentence after stopwords removed: \\n', filtered[51])","43abedd0":"# Stem the words\nstemmer = PorterStemmer()\nstemmed = []\nfor t in filtered:\n    text = t[0]\n    stemmed_text = []\n    for word in text:\n        stemmed_word = stemmer.stem(word.lower())\n        stemmed_text.append(stemmed_word)\n    stemmed.append((stemmed_text, t[1]))\n\nstemmed[51]","468b25cc":"# Counting number of texts each word occurs\nword_count = {}\nfor t in stemmed:\n    text = t[0]\n    already_counted = []\n    for word in text:\n        if word not in word_count:\n            word_count[word] = 1\n        elif word not in already_counted:\n            word_count[word] += 1\n            already_counted.append(word)\n\n#  Removing the words that only occurs once\nfor i in range(len(stemmed)):\n    stemmed[i] = (list(filter(lambda x: word_count[x] > 4, stemmed[i][0])), stemmed[i][1])","2c084e41":"# Splitting data in trainingdata and testdata (80-20 ratio)\ntotaltexts = df.label.value_counts()\ntotal = totaltexts[0] + totaltexts[1] # Total number of texts\ntest_number = int(0.20 * total) # Number of testing mails\n# Picking randomly\ntest_set = []\ntaken = {}\nwhile len(test_set) < test_number:\n    #print(len(train_texts))\n    num = random.randint(0, test_number - 1)\n    if num not in taken.keys():\n        test_set.append(stemmed.pop(num))\n        taken[num] = 1\n\ntrain_set = stemmed # Trainset is the remaining texts\n        \n# Total number of hams and spams\nnumber_of_hams = df.label.value_counts()[0]\nnumber_of_spams = df.label.value_counts()[1]\n\nlen(train_set)\/total, len(test_set)\/total","222914db":"# * * * TRAINING THE MODEL * * * \n\n# meaning: Computing probabilities needed for P(Spam|Word)\n\n# Need to train these 4 possibilities:\n# 1) Probability that a word appears in spam messages\n# 2) Probability that a word appears in ham messages\n# 3) Overall probability that any given message is spam\n# 4) Overall probability that any given message is not spam (is ham)\n\ndef p_appears_in_spam(word):\n    count = 0\n    total_spams = 0\n    for t in train_set:\n        text = t[0]\n        if t[1] == 1:\n            total_spams += 1\n            if word in text:\n                count += 1\n    return count\/total_spams\n             \n\ndef p_appears_in_ham(word):\n    count = 0\n    total_hams = 0\n    for t in train_set:\n        text = t[0]\n        if t[1] == 0:\n            total_hams += 1\n            if word in text:\n                count += 1\n    return count\/total_hams\n\ndef total_spams_and_hams(tset):\n    spams = 0\n    hams = 0\n    for t in tset:\n        spams += 1 if t[1] == 1 else 0\n        hams += 1 if t[1] == 0 else 0\n    return spams, hams\n\n\np_spam = total_spams_and_hams(train_set)[0]\/len(train_set) # Probability that a message is spam\np_ham = total_spams_and_hams(train_set)[1]\/len(train_set) # Probability that a message is ham\n\n# Finally we can compute P(Spam | Word)\ndef p_is_spam_given_word(word):\n    return (p_appears_in_spam(word)*p_spam)\/((p_appears_in_spam(word)*p_spam + p_appears_in_ham(word)*p_ham))\n\nword = 'free'\nprint('Probability that a message is spam given the word \"{}\" is: {}'.format(word, p_is_spam_given_word(word)))","dd6496fa":"# Collecting the probabilities in a dictionary\nprobabilities = {}\nfor t in train_set:\n    text = t[0]\n    for word in text:\n        if word not in probabilities:\n            p = p_is_spam_given_word(word)\n            if p == 0:\n                probabilities[word] = 0.2 # To deal with the zero probability problem. Tweaking this value\n            elif p == 1:\n                probabilities[word] = 0.98 # Tweaking this value\n            else:\n                probabilities[word] = p","40603002":"# * * * TESTING THE MODEL * * * \n# Training is done\n# This function will be used to classify new messages, using the trained probabilities \n\nfrom functools import reduce\ndef p_is_spam(words):\n    probs = []\n    for word in words:\n        if word in probabilities:\n            probs.append(probabilities[word])\n        # 'else' is for unseen word, a value to tweak\n        # Assumes it is somewhat higher probability that an unseen word belongs to a ham message than a spam message\n        # as \n        else:\n            probs.append(0.4) \n    probs_not = list(map(lambda prob: 1-prob, probs))\n    product = reduce(lambda x, y: x * y, probs, 1) \n    product_not = reduce(lambda x, y: x * y, probs_not, 1)\n    return product\/(product + product_not)","eb3bee67":"total_correct = 0\ntrue_spam_as_spam = 0\ntrue_spam_as_ham = 0\ntrue_ham_as_ham = 0\ntrue_ham_as_spam = 0\n\n# Care most about minimizing false positives, that is: labeling non-spam messages as spam\nfalse_positives = []\n\n\nfor t in test_set:\n    guess = -1\n    words = t[0]\n    answer = t[1]\n    p_spam = p_is_spam(words)\n    # If p > 0.95, predict 'yes' (is spam)\n    guess = 1 if p_spam > 0.95 else 0\n    if guess == answer:\n        total_correct += 1\n        if answer == 0: # true negative\n            true_ham_as_ham += 1\n        else: # true positive\n            true_spam_as_spam += 1 \n    else:\n        if answer == 0: # false positive\n            true_ham_as_spam += 1\n            false_positives.append((words, p_spam))\n        else: # true negative\n            true_spam_as_ham += 1\n\n            \ntrue_spams = total_spams_and_hams(test_set)[0]\ntrue_hams = total_spams_and_hams(test_set)[1]\n\nprint('Total test texts: ', len(test_set))\nprint('Number of correct: ', total_correct)\nprint('Accuracy: ', total_correct*100\/(true_spams+true_hams))\nprint('-------------------------------')\nprint('Ham precision: ', true_ham_as_ham\/(true_ham_as_ham + true_spam_as_ham))\nprint('Ham recall: ', true_ham_as_ham\/(true_ham_as_ham + true_ham_as_spam))\nprint('Spam precision: ', true_spam_as_spam\/(true_spam_as_spam + true_ham_as_spam)) # Most important \nprint('Spam recall: ', true_spam_as_spam\/(true_spam_as_spam + true_spam_as_ham))\nprint('-------------------------------')\nprint('False Positives (hams that got labeled as spam):')\nfor i, (text, p) in enumerate(false_positives):\n    print('{}: Words in text: {} | Degree of certainty: {}'.format(i+1, text, p))","e1773a32":"# * * * VISUALISATIONS * * * \nfrom wordcloud import WordCloud\n\nspam_words = \"\"\nham_words = \"\"\n\nall = train_set + test_set\n\nfor t in all:\n    text = t[0]\n    s = \"\"\n    for word in text:\n        s += word + ' '\n    if t[1] == 0:\n        ham_words += s\n    else:\n        spam_words += s + ' '\n\n# # Generate a word cloud image\nspam_wordcloud = WordCloud(width=600, height=400).generate(spam_words)\nham_wordcloud = WordCloud(width=600, height=400).generate(ham_words)\n","37372d1f":"#Spam Word cloud\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(spam_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)","9ba5fa22":"# Ham Word cloud\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(ham_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)","8fa353a4":"#another approach for the naive bayes implementation","3c3b580b":"df.head()","9b2d68b4":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords","ba6ea1cb":"spam=[]\nham=[]\nspam_class=df[df['label']==1]['text']\nham_class=df[df['label']==0]['text']","cf588201":"def extract_ham(ham_class):\n    global ham\n    words = [word.lower() for word in word_tokenize(ham_class) if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n    ham=ham+words","b9a357ab":"def extract_spam(spam_class):\n    global spam\n    words = [word.lower() for word in word_tokenize(spam_class) if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n    spam=spam+words","5ca897a5":"spam_class.apply(extract_spam)\nham_class.apply(extract_ham )","7af28ae2":"\nfrom wordcloud import WordCloud\nspam_wordcloud = WordCloud(width=600, height=400).generate(\" \".join(spam))\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(spam_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","8de92295":"ham_cloud=WordCloud(width=600,height=400,background_color='black').generate(\" \".join(ham))\nplt.figure(figsize=(10,8),facecolor='k')\nplt.imshow(ham_cloud)\nplt.tight_layout(pad=0)\nplt.show()","b6cccb41":"#top 10 spam words=\nspam_words=np.array(spam)\npd.Series(spam_words).value_counts().head(n=10)","9807a1d1":"#top 10 ham words\nham_words=np.array(ham)\npd.Series(ham_words).value_counts().head(n=10)","88a7f3f9":"#now  we are done with visualizations task,next move into text ceaning\nfrom nltk.stem import SnowballStemmer\nimport string\nstemmer = SnowballStemmer(\"english\")\n\ndef cleanText(message):\n    \n    message = message.translate(str.maketrans('', '', string.punctuation))\n    words = [stemmer.stem(word) for word in message.split() if word.lower() not in stopwords.words(\"english\")]\n    \n    return \" \".join(words)\n\ndf[\"text\"] = df[\"text\"].apply(cleanText)\ndf.head(n = 10)    ","36cf7df6":"#we have done with the text cleaning and analysis,now next step is the model building","cc5fba99":"#here x is the independent variable and y is the dependent variable\nx=df['text']\ny=df['label']","c6d84630":"#to train and test the model we have to split into training and testing sets\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)","eda787f7":"\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer()","b98c9b6d":"#as our feature variable x is in the form of string or texts ,for the algorithm we have to convert into vectors with the help of countvectorizer","7cd502be":"x_train=cv.fit_transform(x_train)","2b4f3307":"#fit method is used to train the model with features and labels\nfrom sklearn.naive_bayes import MultinomialNB\nnb=MultinomialNB()\nnb.fit(x_train,y_train)\npredictions=nb.predict(cv.transform(x_test))\n","e762ae29":"#evaluate the model on various metrics\naccuracy=accuracy_score(y_test,predictions)","1438aace":"accuracy","1bfaafec":"confusion_matrix=confusion_matrix(y_test,predictions)","27cf46df":"confusion_matrix","03dc47ae":"true_negatives=confusion_matrix[0][0]\nfalse_positives=confusion_matrix[0][1]\nfalse_negatives=confusion_matrix[1][0]\ntrue_positives=confusion_matrix[1][1]","8d543d2d":"print(\"true negative predcitons:\",true_negatives)\nprint(\"false positive predictions:\",false_positives)\nprint(\"false negative predictions:\",false_negatives)\nprint(\"true postive predictions:\",true_positives)","370f3c5a":"import seaborn as sns\nplt.figure(figsize=(12,5))\ngroup_names =['True Neg','False Pos','False Neg','True Pos']\ngroup_counts =['{0:0.0f}'.format(value) for value in\n                confusion_matrix.flatten()]\ngroup_percentages =['{0:.2%}'.format(value) for value in\n                     confusion_matrix.flatten()\/np.sum(confusion_matrix)]\nlabels=[f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels=np.asarray(labels).reshape(2,2)\nsns.heatmap(confusion_matrix, annot=labels, fmt='', cmap='Blues')","f76e6c1d":"#lets also calculate the recall and precision score\nfrom sklearn.metrics import recall_score,precision_score\nrecall=recall_score(y_test,predictions)\nprecision=precision_score(y_test,predictions)","e757b438":"precision","25a3948c":"recall","87f95cb8":"### Combining Individual Probabilities\nDetermining whether a message is spam or ham based only on the presence of one word is error-prone, must try to consider all the words (or the most interesting) in the message\n###### Probability that a text is spam: $P(Spam) =  \\frac{p_1p_2...p_n}{p_1p_2...p_n + (1-p_1)(1-p_2)...(1-p_n)} $\n\n$p_1$: The probability $P(S|W_1)$, that it is spam knowing it contains a first word (for example \"free\")","7003bc03":"###### Probability that a text containing a given word is spam (Bayes' theorem):\n$P(Spam|Word) =  \\frac{P(Word|Spam)P(Spam)}{P(Word|Spam)P(Spam) + P(Word|Ham)P(Ham)} $"}}