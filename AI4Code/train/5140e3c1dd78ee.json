{"cell_type":{"42de9386":"code","2b1241eb":"code","8efa5083":"code","74fd4d75":"code","f44080db":"code","61f24063":"code","a47053e0":"code","c414a796":"code","7f277f25":"code","00739645":"code","9c032ca2":"code","fbe57fcf":"code","02255da2":"code","e6b05613":"code","e13faf9a":"code","11a63a73":"code","386f1b6c":"code","fef803de":"code","26e285a4":"code","34e99e87":"code","e4aaf51e":"code","20fef43a":"code","3032e16f":"code","a8f8817a":"code","0706f765":"code","b1b339fb":"code","a1f87162":"code","9e8a23f5":"code","d3c3b578":"code","e705b0e4":"code","50ba75aa":"code","08ef86cb":"code","71b4a9fb":"code","d9644408":"code","cd718aa7":"code","a54d0dd9":"code","55cdaf58":"code","083dc887":"code","87b3b472":"code","d1892ce0":"code","7516cb59":"code","fbe1ccb0":"code","603c6e25":"markdown","9287aea2":"markdown","28bcf113":"markdown","6fae4fb2":"markdown","8fc37b1d":"markdown","32cfed0f":"markdown","36714ce8":"markdown","f40edbd1":"markdown","94370164":"markdown","1e6808b3":"markdown","bd56ec44":"markdown","0317a5dc":"markdown","fa2fd5c3":"markdown","ac5f549d":"markdown","17351742":"markdown","5b20cd38":"markdown","5166c41a":"markdown","8df916e9":"markdown","d6c703ab":"markdown","bb413cf5":"markdown","26219a48":"markdown","65bc79af":"markdown","0d55d692":"markdown","caac16e8":"markdown"},"source":{"42de9386":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b1241eb":"import numpy as np #using numpy for linear algebra\nimport pandas as pd #using pandas for data processing, CSV I\/O \n\n#Using matplotlib for images - keeps the plots in one place. Calls image as static pngs\n%matplotlib inline \nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec # subplots\nimport mpld3 as mpl\n\nimport seaborn as sns\nimport scipy as sp\nimport warnings \nimport os\nwarnings.simplefilter(action='ignore', category=Warning)\nimport datetime\n\n#Modules for each model will be loaded with the subsequent model sections\n","8efa5083":"data=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv', header = 0) #reading the data\ndata.head() #displaying the header of the dataset\ndata.describe() #displaying the description of the dataset in the rows","74fd4d75":"data.info() #displaying the content of the data, data types, etc.","f44080db":"data.shape #what is the dimensions of the dataset","61f24063":"data.columns #displaying the column names","a47053e0":"data.value_counts #getting an idea on the data included in the columns","c414a796":"#it looks like there is a column with NaNs\ndata.isnull().sum() #searching for NaN values, if any","7f277f25":"data.drop('Unnamed: 32', axis = 1, inplace = True)\n#note this line might give an error if you run it - that's okay.  Continue with the next line...","00739645":"#let's make sure the column with NaNs was removed....\ndata","9c032ca2":"data.shape #let's make sure the dimensions show that the column with NaNs was removed","fbe57fcf":"#We want to see how much data is benign and how much is malignant\nsns.set_style('whitegrid')\nplt.figure(figsize=(12,6))\nsns.countplot(x=\"diagnosis\", data=data, palette='rocket');","02255da2":"pd.value_counts(data['diagnosis'])","e6b05613":"#Now that we defined the data, we will now visualize the malignant and benign cases with the other features combined\n\n#Radius Mean\nplt.hist(data[\"radius_mean\"][data[\"diagnosis\"]==\"M\"],color=\"red\",alpha=0.5)\nplt.hist(data[\"radius_mean\"][data[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"RADIUS MEAN\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()\n\n#Texture Mean\nplt.hist(data[\"texture_mean\"][data[\"diagnosis\"]==\"M\"],color=\"red\",alpha=0.5)\nplt.hist(data[\"texture_mean\"][data[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"TEXTURE MEAN\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()\n\n#Perimeter Mean\nplt.hist(data[\"perimeter_mean\"][data[\"diagnosis\"]==\"M\"],color=\"red\",alpha=0.5)\nplt.hist(data[\"perimeter_mean\"][data[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"PERIMETER MEAN\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()\n\n#Area Mean\nplt.hist(data[\"area_mean\"][data[\"diagnosis\"]==\"M\"],color=\"red\",alpha=0.5)\nplt.hist(data[\"area_mean\"][data[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"AREA MEAN\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()\n\n#Smoothness Mean\nplt.hist(data[\"smoothness_mean\"][data[\"diagnosis\"]==\"M\"],color=\"red\",alpha=0.5)\nplt.hist(data[\"smoothness_mean\"][data[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"SMOOTHNESS MEAN\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()\n\n#Compactness Mean\nplt.hist(data[\"compactness_mean\"][data[\"diagnosis\"]==\"M\"],color=\"red\",alpha=0.5)\nplt.hist(data[\"compactness_mean\"][data[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"CONCAVITY MEAN\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()\n\n#Concavity Mean\nplt.hist(data[\"concavity_mean\"][data[\"diagnosis\"]==\"M\"],color=\"red\",alpha=0.5)\nplt.hist(data[\"concavity_mean\"][data[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"CONCAVITY MEAN\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()\n\n#Concave Points Mean\nplt.hist(data[\"concave points_mean\"][data[\"diagnosis\"]==\"M\"],color=\"red\",alpha=0.5)\nplt.hist(data[\"concave points_mean\"][data[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"CONCAVE POINTS MEAN\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()\n\n#Symmetry Mean\nplt.hist(data[\"symmetry_mean\"][data[\"diagnosis\"]==\"M\"],color=\"red\",alpha=0.5)\nplt.hist(data[\"symmetry_mean\"][data[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"SYMMETRY MEAN\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()\n\n#Fractal Dimension Mean\nplt.hist(data[\"fractal_dimension_mean\"][data[\"diagnosis\"]==\"M\"],color=\"red\",alpha=0.5)\nplt.hist(data[\"fractal_dimension_mean\"][data[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"FRACTAL DIMENSION MEAN\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()","e13faf9a":"data.corr","11a63a73":"#making a correlation table\ndata.corr()","386f1b6c":"#we can better visualize the correlation in color with. The lighter the color, the greater the correlation value.\nplt.figure(figsize=(20, 17))\nmatrix = np.triu(data.corr())\nsns.heatmap(data.corr(), annot=True, linewidth=.8, mask=matrix, cmap=\"rocket\");","fef803de":"#here's another set of correlation plots. We are plotting the diagnosis (maroon = malignant; orange = benign)\nmean_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nsns.pairplot(data[mean_col],hue = 'diagnosis', palette='rocket')","26e285a4":"#get features\nx = data.drop(columns = 'diagnosis')\n\n#get predictive value\ny = data['diagnosis']","34e99e87":"#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=0) #defining the test data as 30% of the data\n\n#Check to see if it's a 70:30 split\nprint(len(x_train))\nprint(len(x_test))\nprint(len(y_train))\nprint(len(y_test))","e4aaf51e":"from sklearn.linear_model import LogisticRegression\nlr_model = LogisticRegression()\nlr_model.fit(x_train, y_train)","20fef43a":"y_pred=lr_model.predict(x_test)\nfrom sklearn.metrics import accuracy_score, classification_report,confusion_matrix, mean_squared_error\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(\"Training Score:\", lr_model.score(x_train, y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Logistic Regression Model is: \", accuracy_score(y_test, y_pred)*100 , \"%\")\nlr_model_results=accuracy_score(y_test, y_pred)*100","3032e16f":"#Checking out the actual vs predicted values with the \ndata = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","a8f8817a":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123)\ndtree.fit(x_train, y_train)","0706f765":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",dtree.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Decision Tree Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\ndtree_results=accuracy_score(y_test, y_pred)*100","b1b339fb":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train, y_train)","a1f87162":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",rfc.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Random Forest Classifier Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\nrfc_results=accuracy_score(y_test, y_pred)*100","9e8a23f5":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train, y_train)","d3c3b578":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",knn.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the K Neareset Neighbors Classifier Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\nknn_results=accuracy_score(y_test, y_pred)*100","e705b0e4":"from sklearn.svm import SVC\nsvc=SVC()\n\nsvc.fit(x_train, y_train)","50ba75aa":"y_pred=svc.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",svc.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Support Vector Classification Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\nsvc_results=accuracy_score(y_test, y_pred)*100","08ef86cb":"from sklearn.ensemble import AdaBoostClassifier\nadb=AdaBoostClassifier(base_estimator = None)\n\nadb.fit(x_train, y_train)","71b4a9fb":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",adb.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Ada Boost Classifier Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\nadb_results=accuracy_score(y_test, y_pred)*100","d9644408":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\n\ngbc.fit(x_train, y_train)","cd718aa7":"y_pred=gbc.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gbc.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Gradient Boost Classifier Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\ngbc_results=accuracy_score(y_test, y_pred)*100","a54d0dd9":"from xgboost import XGBClassifier\nxgb=XGBClassifier(objective ='reg:linear', colsample_bytree= 0.3, learning_rate=0.1,\n                  max_depth=5, alpha=10, n_estimators=10)\n\nxgb.fit(x_train, y_train)","55cdaf58":"y_pred=xgb.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",xgb.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Extreme Gradient Boosting Classifier Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\nxgb_results=accuracy_score(y_test, y_pred)*100","083dc887":"from sklearn.naive_bayes import GaussianNB\ngnb=GaussianNB()\n\ngnb.fit(x_train, y_train)","87b3b472":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gnb.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Naive Bayes Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\ngnb_results=accuracy_score(y_test, y_pred)*100","d1892ce0":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# Importing data\ndata = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndel data['Unnamed: 32']\n\nX = data.iloc[:, 2:].values\ny = data.iloc[:, 1].values\n\n# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_X_1 = LabelEncoder()\ny = labelencoder_X_1.fit_transform(y)\n\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu', input_dim=30))\n# Adding dropout to prevent overfitting\nclassifier.add(Dropout(rate=0.1))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))\n# Adding dropout to prevent overfitting\nclassifier.add(Dropout(rate=0.1))\n\n# Adding the output layer\nclassifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n#Optimizer is chosen as adam for gradient descent and Binary_crossentropy is the loss function used.\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size=75, epochs=75)\n# Long scroll ahead but worth\n# The batch size and number of epochs have been set using trial and error. Still looking for more efficient ways. Open to suggestions.\n","7516cb59":"\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error, f1_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nscore = f1_score(y_test, y_pred, average='binary')\nprint(\"Training Score: \",score*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Deep Learning Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\n\ndl_results=accuracy_score(y_test, y_pred)*100","fbe1ccb0":"print(\"Logistic Regression: \",lr_model_results)\nprint(\"Decision Tree: \",dtree_results)\nprint(\"Random Forest: \",rfc_results)\nprint(\"K Nearest Neighbors: \",knn_results)\nprint(\"Support Vector: \",svc_results)\nprint(\"Ada Boost: \",adb_results)\nprint(\"Gradient Boost: \",gbc_results)\nprint(\"Extreme Gradient Boost: \",xgb_results)\nprint(\"Naive Bayes: \",gnb_results)\nprint(\"Deep Learning: \",dl_results)","603c6e25":"# **7. GRADIENT BOOST CLASSIFIER**","9287aea2":"# **Breast Cancer Prediction - Comparing Models**\n\nNotebook inspired by Buddhini Waidyawansa (https:\/\/www.kaggle.com\/buddhiniw\/breast-cancer-prediction), Aryan Tiwari (https:\/\/www.kaggle.com\/aryantiwari123\/breast-cancer-eda-and-prediction-98), and Siddharth Yadav (https:\/\/www.kaggle.com\/thebrownviking20\/intro-to-keras-with-breast-cancer-data-ann).\n\nData used from Wisconsin breast cancer diagnostic data (https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data)","28bcf113":"# **8. EXTREME GRADIENT BOOSTING (XGB) CLASSIFIER**","6fae4fb2":"# **2. DECISION TREE CLASSIFIER**","8fc37b1d":"# **5. SUPPORT VECTOR CLASSIFICATION (SVC)**","32cfed0f":"# **IMPORTING AND LOADING LIBRARIES**\n","36714ce8":"# **CREATING A TEST AND TRAIN SET**\n\nThe data is not ordered, so we will do a simle 70:30 split to create a training and test data set. ","f40edbd1":"# **EXPLORING THE DATA**\n\nWe want to perform exploratory data analysis. We want to learn what is in the data and clean the data from NaN values.","94370164":"Let's take a look at another way of correlating the data:","1e6808b3":"**Task Description:** To predict whether the cancer is benign or malignant\n\n**Data Description:** Features are computed from a digitized image of a fine need aspirate (FNA) of a breast mass. The features describe characteristics of the cell nuclei present in the image. \n\nAttribute information:\n* 1) ID number\n* 2) Diagnosis (M = malignant, B = benign)\n3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\n* a) radius (mean of distances from center to points on the perimeter)\n* b) texture (standard deviation of gray-scale values)\n* c) perimeter\n* d) area\n* e) smoothness (local variation in radius lengths)\n* f) compactness (perimeter^2 \/ area - 1.0)\n* g) concavity (severity of concave portions of the contour)\n* h) concave points (number of concave portions of the contour)\n* i) symmetry\n* j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant\n","bd56ec44":"# **LOADING THE DATA**","0317a5dc":"# **9. NAIVE BAYES**","fa2fd5c3":"The data shown here appears to reflect the amount detailed in the data details. Great! Let's see if we can start making correlations with the nucleaus features and diagnosis.","ac5f549d":"# **3. RANDOM FOREST CLASSIFIER**","17351742":"# **MODELS**\n\nIn this section, we explore several models and evaluate the accuracy. \n\nTo start, we will build a generic function for a classification model.","5b20cd38":"# **6. ADA BOOST CLASSIFIER**","5166c41a":"# **10. DEEP LEARNING MODEL**","8df916e9":"# **VISUALIZING THE DATA**\n\nWe want to visualize the data, make correlations on related features.","d6c703ab":"It looks like Unnamed: 32 colum has 569 NaN values, so we will drop it.","bb413cf5":" **OBSERVATIONS**\n\n* From the images above, since there are clear distinctions among larger values of certain parameters for malignant cancer, some values may be useful in classification of cancer. Values that can be used in classificaiton of cancer are mean values of:\n   * Cell radius\n   * Perimeter\n   * Area \n   * Compactness\n   * Concavity\n   * Concave Points \n   \n\n* From the images above, values that may not be great indicators of malignant cancers by not showing a preference in one diagnosis or the other are mean valuess of:\n   * Texture\n   * Smoothness\n   * Symmetry\n   * Fractual Dimension ","26219a48":"We can see that radius, area, and perimeter are strongly correlated. \n\nAlso, compactness, concavity, and concave point are also strongly correlated. ","65bc79af":"# **1. LOGISTIC REGRESSION**\n\nThis model can be used for classification of discrete data. ","0d55d692":"The column with NaNs was removed! Great! We can now move on to making sense of the data with visualizations and correlations.","caac16e8":"# **4. K NEAREST NEIGHBORS CLASSIFIER (KNN)**"}}