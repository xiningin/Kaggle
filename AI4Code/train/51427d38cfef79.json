{"cell_type":{"0979752d":"code","eeb7407d":"code","12736a42":"code","fac8c7bb":"code","599e2d25":"code","7e4ea2d4":"code","b6306745":"code","2afba1b5":"code","9d1a97da":"code","f129c0b3":"code","b2486c4e":"code","e27a7086":"code","bf3fb4b0":"code","c24c8c9f":"code","104cdf71":"code","d66319b0":"code","a4513aa1":"code","3d485dc8":"code","911bf74e":"code","d24e2ebc":"code","638f1a6e":"code","f2ae3b7e":"markdown","2cf0ff75":"markdown","e95c063d":"markdown","4810ac61":"markdown","28e6c65a":"markdown"},"source":{"0979752d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport itertools\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\n\nprint(tf.__version__)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","eeb7407d":"STRING_LENGTH_MAX =256","12736a42":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","fac8c7bb":"train_df['question_text'].apply(len).max()","599e2d25":"train_df.head(3)","7e4ea2d4":"# train = train_df.sample(10000)\ntrain = train_df.sample(1000000)\nx_train = train['question_text']\ny_train = train['target']","b6306745":"import re\n\nx_list_words = [re.findall(r'\\w+',x) for x in x_train.values]\nvocab = set( itertools.chain.from_iterable(x_list_words) )\nword_to_index = {w:(i+3) for i,w in enumerate(vocab)}\nword_to_index[\"<PAD>\"] = 0\nword_to_index[\"<START>\"] = 1\nword_to_index[\"<UNK>\"] = 2  # unknown\nword_to_index[\"<UNUSED>\"] = 3\nindex_to_word = {v:k for k,v in word_to_index.items()}\nlen(index_to_word)","2afba1b5":"x_train_int = [\n    [word_to_index[w] for w in words]\n    for words in x_list_words\n]","9d1a97da":"validate = train_df.loc[~train_df.index.isin(train.index)]\n# validate = train_df.loc[~train_df.index.isin(train.index)].sample(10000)\nx_val = validate['question_text']\ny_val = validate['target']\n\ndef encode_x(x):\n    x_list_words = [re.findall(r'\\w+',x) for x in x.values]\n    x_list_int = [\n        [word_to_index.get(w,2) for w in words]\n        for words in x_list_words\n    ]\n    return x_list_int\n\nx_val_int = encode_x(x_val)","f129c0b3":"def decode_ints(list_ints):\n    return ' '.join([index_to_word.get(i, '?') for i in list_ints])","b2486c4e":"decode_ints(x_val_int[0])","e27a7086":"x_train.apply(len).max()","bf3fb4b0":"x_val.apply(len).max()","c24c8c9f":"train_data = keras.preprocessing.sequence.pad_sequences(x_train_int,\n                                                        value=word_to_index[\"<PAD>\"],\n                                                        padding='post',\n                                                        maxlen=STRING_LENGTH_MAX)\n\ntest_data = keras.preprocessing.sequence.pad_sequences(x_val_int,\n                                                       value=word_to_index[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=STRING_LENGTH_MAX)","104cdf71":"test_data.shape","d66319b0":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nvocab_size = len(word_to_index)+1\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, 64, input_length=STRING_LENGTH_MAX))\nmodel.add(keras.layers.Bidirectional(keras.layers.CuDNNLSTM(64)))\nmodel.add(keras.layers.Dropout(0.3))\n# model.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\nmodel.summary()","a4513aa1":"model.compile(optimizer=tf.train.AdamOptimizer(),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","3d485dc8":"early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=0,\n                              verbose=0, mode='auto')\n\nhistory = model.fit(train_data,\n                y_train,\n                epochs=100,\n                batch_size=1024,\n                validation_data=(test_data, y_val),\n                verbose=1,\n                callbacks=[early_stop,])","911bf74e":"from sklearn import metrics\ny_pred = model.predict(test_data, batch_size=2048, verbose=1)\nfor thresh in np.arange(0.1, 0.9, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(y_val, (y_pred>thresh).astype(int))))","d24e2ebc":"x_test = test_df['question_text']\nx_test_int = encode_x(x_test)\nsub_data = keras.preprocessing.sequence.pad_sequences(x_test_int,\n                                                       value=word_to_index[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=STRING_LENGTH_MAX)\npred_val_y = model.predict([sub_data], batch_size=1024, verbose=0)","638f1a6e":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub.prediction = pred_val_y > 0.245\nsub.to_csv(\"submission.csv\", index=False)","f2ae3b7e":"Simple text classification, following example https:\/\/www.tensorflow.org\/tutorials\/keras\/basic_text_classification\nThe idea is to train an embedding layer, and use average1D to average the latent embedding vectors of the words in each sentence. \n\nAlso borrowed code for checking f1 score from https:\/\/www.kaggle.com\/sudalairajkumar\/a-look-at-different-embeddings\n\nPotential improving point.\n\n1. Dropout\n2. Regularization\n3. More complex structure\n4. GRU\n5. Capsule\n6. Attention\n7. Stacking","2cf0ff75":"Make predictions","e95c063d":"To save time on commit, only took a small sample. If using all training data, the best f1 score is 0.626. ","4810ac61":"| | f1 (validation) | f1 (public LB) |\n|---|---|---|\n|without any RNN layer |0.626|0.613|\n|one Bidirectional LSTM |0.633|0.61|\n|one Bi-LSTM w dropout |0.62|0.61|\n|one Bi-LSTM w dropout, regularization  |0.62|0.61|","28e6c65a":"Simple text classification, following example https:\/\/www.tensorflow.org\/tutorials\/keras\/basic_text_classification\nThe idea is to train an embedding layer, and use average1D"}}