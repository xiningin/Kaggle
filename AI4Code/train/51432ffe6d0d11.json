{"cell_type":{"f986bdac":"code","0e36a9a2":"code","274738da":"code","4e465726":"code","1a720d67":"code","152cfc2e":"code","72012b3c":"code","aedf81b9":"code","b39b88d5":"code","45a8eacb":"code","77b90520":"code","58793288":"code","a00b78d1":"code","f3e0f13f":"code","e8dbb1fa":"code","cf1e8d5f":"code","9671354a":"code","f445bafc":"code","828e1369":"code","4480a550":"code","ea2f6922":"code","9189ce25":"code","9330f9aa":"code","c532eed4":"code","2b2d1a0a":"code","d163ffe9":"code","6c2f6b26":"code","b3b9076e":"code","42441b55":"code","9d44a4bb":"code","86a9ec56":"code","9c7b42c0":"code","8e6ce731":"code","885796fd":"code","bee28b59":"code","7e703e4a":"code","ce56c826":"code","98e99043":"code","a6369925":"code","568776c9":"code","8ee7e327":"code","ab0f60a0":"code","3cbbf92a":"code","6f0f80ad":"code","a130d057":"code","4f97181f":"code","eace6bbc":"code","186dd493":"code","62aa5e78":"code","ceca7feb":"code","ccae2b8d":"markdown","b5f420e7":"markdown","33e53288":"markdown","a1b394c9":"markdown","3b0ff41e":"markdown","dd9afea7":"markdown","d31cb27f":"markdown","47a86603":"markdown","1e8bfcf9":"markdown","06ea11ca":"markdown","0b372170":"markdown","255bfca4":"markdown","82d51afa":"markdown","d2d7f5a8":"markdown","34a4472b":"markdown","9a2f5f64":"markdown","15708e04":"markdown","b7be43f2":"markdown","6ce41d15":"markdown","eeb0a6b0":"markdown","a9f8eb24":"markdown","1ddbd359":"markdown","95ef64eb":"markdown","38cccc33":"markdown","7139d81e":"markdown","356faf88":"markdown","a7ccbd2b":"markdown","d5d140d6":"markdown","ddd0ee6d":"markdown","0ed760e8":"markdown","f31e1573":"markdown","c74840e5":"markdown","e78c17a3":"markdown","b8583cb9":"markdown","34e98f6e":"markdown","79c172c8":"markdown","e80f1a77":"markdown","5e9ac329":"markdown","87e0b083":"markdown","57262cc8":"markdown","f23a37fb":"markdown","939135aa":"markdown","7e14c276":"markdown"},"source":{"f986bdac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nos.cpu_count()\n# Any results you write to the current directory are saved as output.","0e36a9a2":"train = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip\",parse_dates=[\"Date\"])\ntest = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip\",parse_dates=[\"Date\"])","274738da":"train.head()","4e465726":"add = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip\")\nadd.head()","1a720d67":"store = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv\")\nstore","152cfc2e":"import seaborn as sns\nimport matplotlib.pyplot as plt\nax = sns.barplot(x=\"Type\", y=\"Size\", hue=\"Type\", data=store)\nsns.catplot(x=\"Type\", kind=\"count\", palette=\"ch:.25\", data=store)","72012b3c":"train = pd.merge(train,store,on = \"Store\",how=\"left\")\ntest = pd.merge(test,store,on = \"Store\",how=\"left\")","aedf81b9":"train.head()","b39b88d5":"test.head()","45a8eacb":"train.info()","77b90520":"train[\"year\"] = train[\"Date\"].dt.year\ntrain[\"month\"] = train[\"Date\"].dt.month\ntrain[\"day\"] = train[\"Date\"].dt.day\ntrain[\"week\"] = train[\"Date\"].dt.week\ntest[\"year\"] = test[\"Date\"].dt.year\ntest[\"month\"] = test[\"Date\"].dt.month\ntest[\"day\"] = test[\"Date\"].dt.day\ntest[\"week\"] = test[\"Date\"].dt.week","58793288":"train[\"Type\"] = train[\"Type\"].replace({\"A\":0,\"B\":1,\"C\":2})\ntest[\"Type\"] = test[\"Type\"].replace({\"A\":0,\"B\":1,\"C\":2})","a00b78d1":"train.head()","f3e0f13f":"def wmae(y_pred, targ, holiday_week):\n    sumOfWeights = 0\n    sumofCol_B_X_Col_E = 0\n    \n    for i in range(0, len(y_pred)):\n        weight = 0\n        if holiday_week[i]: \n            weight = 5\n        else:\n            weight = 1\n        \n        Col_B_X_Col_E = abs(targ[i] - y_pred[i])*weight\n        sumOfWeights += weight \n        sumofCol_B_X_Col_E += Col_B_X_Col_E\n    WMAE = sumofCol_B_X_Col_E\/sumOfWeights\n    return WMAE","e8dbb1fa":"train.groupby(\"IsHoliday\")[\"Weekly_Sales\"].median()","cf1e8d5f":"plt.figure(figsize=(10,6))\nsns.barplot(train[\"IsHoliday\"],train[\"Weekly_Sales\"])","9671354a":"train.groupby(\"Type\")[\"Weekly_Sales\"].mean()","f445bafc":"plt.figure(figsize=(10,6))\nsns.barplot(train[\"Type\"],train[\"Weekly_Sales\"])","828e1369":"plt.figure(figsize=(10,6))\nsns.boxplot(train[\"year\"],train[\"Weekly_Sales\"],showfliers=False)","4480a550":"plt.figure(figsize=(10,6))\nsns.boxplot(train[\"Store\"],train[\"Weekly_Sales\"],showfliers=False)","ea2f6922":"plt.figure(figsize=(10,6))\nsns.boxplot(train[\"Dept\"],train[\"Weekly_Sales\"],showfliers=False)","9189ce25":"feature = [\"Store\",\"Dept\",\"year\",\"month\",\"day\",\"week\",\"IsHoliday\", \"Size\"]","9330f9aa":"X_train = train[feature]\nX_test = test[feature]","c532eed4":"X_train","2b2d1a0a":"y_train = train[\"Weekly_Sales\"]","d163ffe9":"from sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train,y_train, test_size=0.2)","6c2f6b26":"%%time\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=10,n_jobs=4)\n","b3b9076e":"model.fit(X_train1,y_train1)","42441b55":"(pd.DataFrame([X_train.columns,model.feature_importances_],columns=feature).T).plot.bar()","9d44a4bb":"holidays = X_test1['IsHoliday'].to_numpy()\ny_test1 = y_test1.to_numpy()","86a9ec56":"result = model.predict(X_test1)","9c7b42c0":"WMAE = wmae(result, y_test1, holidays)\nprint(WMAE)","8e6ce731":"'''from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [10, 100, 200]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 2, n_jobs = 3, verbose = 2)'''\n\n","885796fd":"#grid_search.fit(X_train, y_train)","bee28b59":"grid_search.best_params_","7e703e4a":"#!pip install xgboost","ce56c826":"import xgboost as xgb","98e99043":"data_dmatrix = xgb.DMatrix(data=X_train,label=y_train)","a6369925":"xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n #               max_depth = 5, alpha = 10, n_estimators = 10)","568776c9":"xg_reg.fit(X_train1,y_train1)\nresult = xg_reg.predict(X_test1)","8ee7e327":"WMAE = wmae(result, y_test1, holidays)\nprint(WMAE)","ab0f60a0":"#from sklearn.model_selection import GridSearchCV\n#from sklearn.model_selection import ShuffleSplit","3cbbf92a":"'''\nxgb1 = xg_reg\nparameters = {'nthread':[3], #when use hyperthread, xgboost may become slower\n              'objective':['reg:linear'],\n              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'silent': [1],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500]}\n\nxgb_grid = GridSearchCV(xgb1,\n                        parameters,\n                        cv = 2,\n                        n_jobs = 3,\n                        verbose=True)\n\nxgb_grid.fit(X_train,\n         y_train)\n\nprint(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)\n'''","6f0f80ad":"'''xg_reg = xgb.XGBRegressor(colsample_bytree= 0.7, learning_rate= 0.07, max_depth= 5, min_child_weight= 4, n_estimators= 300, nthread= 4, objective= 'reg:linear', silent= 1, subsample=0.7)'''","a130d057":"'''xg_reg.fit(X_train,y_train)\n\nresult = xg_reg.predict(X_test)'''","4f97181f":"\n%%time\nfrom sklearn.ensemble import RandomForestRegressor\n#model = RandomForestRegressor(bootstrap= True, max_depth= 110, max_features= 3, min_samples_leaf= 3, min_samples_split= 8, n_estimators= 200)\n\nmodel = RandomForestRegressor(n_estimators= 500)\n\nmodel.fit(X_train,y_train)\n\nresult = model.predict(X_test)","eace6bbc":"sub = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip\")","186dd493":"sub.head()","62aa5e78":"sub[\"Weekly_Sales\"] = result\nsub.head()","ceca7feb":"sub.to_csv(\"walmart_predict_sub.csv\",index=False)","ccae2b8d":"Lad os kigge p\u00e5 stores.csv ogs\u00e5","b5f420e7":"Det vi kan se ud fra evalueringen er, at holiday v\u00e6gtes 5 gange s\u00e5 h\u00f8jt som almindelige dage, hvilket betyder, at jo bedre vi er til at predicte rigtigt p\u00e5 holidays desto bedre score f\u00e5r vi","33e53288":"Allerede nu t\u00e6nker jeg, at vi b\u00f8r spiltte datoen i \u00e5r, m\u00e5ned og dag og m\u00e5ske ogs\u00e5 i uge i og med at salgstallet er per uge","a1b394c9":"Lad os pr\u00f8ve at unders\u00f8ge, om der er en sammenh\u00e6ng mellem st\u00f8rrelsen p\u00e5 butikken og A,B og C\nVi ved ikke pr\u00e6cis hvad size betyder, men det er sikkert butikkens st\u00f8rrelse i square feet","3b0ff41e":"Det er som vi ogs\u00e5 s\u00e5 i box-plottet helt klart dept, store og type der er de vigtigste, mens holiday ikke spiller nogen rolle. Men vi er n\u00f8d til at have den med for at kunne lave vores evaluering. Vores evaluering har brug for en liste af predictions, en liste af ground truth og en liste af holidays for samme datoer, s\u00e5 lad os lave en liste af holidays","dd9afea7":"Lad os ogs\u00e5 lige tage et kig p\u00e5, hvilke typer af data vi nu har at g\u00f8re med. Det dur jo ikke med catagorical data, s\u00e5 alt skal laves om til numerical data","d31cb27f":"Lad os l\u00e6gge os fast p\u00e5 den model vi helst vil bruge. Tr\u00e6ne p\u00e5 hele s\u00e6ttet, predicte p\u00e5 tests\u00e6ttet og indsende til kaggle","47a86603":"Her er der tydeligt meget store individuelle forskelle, sikkert en god kandidat for salget. Hvad med afdelingerne imellem?","1e8bfcf9":"Lad os pr\u00f8ve med en anden algoritme nemlig XGBoost. Den er forklaret her: https:\/\/www.youtube.com\/watch?v=OtD8wVaFm6E\nMen ligger ikke i scikit learn, s\u00e5 den skal installeres med pip","06ea11ca":"S\u00e5 ops\u00e6tter vi den med en r\u00e6kke hyperparametre. Vi kan pr\u00f8ve GridSearch om lidt for at finde ud af, hvad de bedste parametre er","0b372170":"**Kig p\u00e5 indholdet:**","255bfca4":"Der er tydeligvis en sammenh\u00e6ng mellem st\u00f8rrelse og A,B og C. Det f\u00f8rste plot viser at A-butikker altid er st\u00f8rst, og det andet plot viser, at der er flest A-butikker. Vi har jo et store_id i tr\u00e6ningsdataene, s\u00e5 vi kan koble stores sammen med tr\u00e6ning og se, om det kunne give noget v\u00e6rdi (og g\u00f8re det samme for test, s\u00e5 vi kan bruge tests\u00e6ttet, n\u00e5r vi har tr\u00e6net vores model)","82d51afa":"Lad os kigge p\u00e5, hvordan de enkelte butikker p\u00e5virker resultatet. Her vil et box-plot v\u00e6re godt, fordi det viser minimum, maksimum, 25 percentilen, median og 75 percentilen samtidig ","d2d7f5a8":"# ** 3. Feature engineering**","34a4472b":"S\u00e5 fik vi styr p\u00e5 data. IsHoliday vil automatisk blive fortolket som 0 og 1, s\u00e5 den beh\u00f8ver ikke encodes","9a2f5f64":"Det var et ret skidt resultat, lad os pr\u00f8ve at optimere den lidt","15708e04":"Lad os starte med at s\u00e6tte antal tr\u00e6er til 10 og dybden til 4 niveauer, bagefter kan vi pr\u00f8ve med GridSearch for at finde det optimale antal","b7be43f2":"# ** 2. Indl\u00e6s datas\u00e6ttene**","6ce41d15":"Vi kunne godt one-hot-encode Type, men da der kun er 3 forskellige typer, giver det mere mening at lave en find and replace","eeb0a6b0":"Ovenst\u00e5ende datas\u00e6t (add) virker umiddelbart rimelig ubrugeligt, m\u00e5ske lige med undtagelse af temperatur, som kunne p\u00e5virke folks k\u00f8belyst, hvis de g\u00e5r indenfor i d\u00e5rligt vejr eller handler mere, n\u00e5r vejret er godt","a9f8eb24":"# ** 7.\tIndsend resultatet **","1ddbd359":"# ** 4. Udforsk data**","95ef64eb":"![image.png](attachment:image.png)","38cccc33":"Modellen blev desv\u00e6rre ikke bedre efter tuning, s\u00e5 jeg m\u00e5 have truffet nogle d\u00e5rlige valg som input til modellen","7139d81e":"Forskellen er ikke k\u00e6mpe, men der er da lidt forskel","356faf88":"En god model til at forudsige regression er Random Forest Regressor, som er forklaret her: https:\/\/www.youtube.com\/watch?v=g9c66TUylZ4 ","a7ccbd2b":"Desv\u00e6rre ved vi endnu ikke helt, hvad der vil v\u00e6re de bedste features og hyperparametre, men det finder vi ud af","d5d140d6":"S\u00e5 lad os ops\u00e6tte GridSearchCV","ddd0ee6d":"Vi kan pr\u00f8ve at kigge p\u00e5, hvor stor forskel der er i salget p\u00e5 en helligdag i forhold til en almindelig dag","0ed760e8":"Ja der er en tydelig sammenh\u00e6ng mellem butikstype og salg, s\u00e5 dette m\u00e5 v\u00e6re en vigtig feature","f31e1573":"Vi har som sagt en dato, som vi kunne splitte op. I og med at data handler om udvikling over tid vil datoen altid v\u00e6re meget relevant, og hvis vi one-hot-encoder den, ender vi med en eksplosion i antal kolonner, s\u00e5 det er bedre at opdele den","c74840e5":"Lad os kigge lidt p\u00e5, hvordan data overhovedet skal evalueres. Det skal evalueres p\u00e5 WMAE, hvilket ikke umiddelbart er tilg\u00e6ngeligt for os i noget bibliotek, s\u00e5 vi bliver n\u00f8dt til selv at oms\u00e6tte formlen vi er blevet givet til kode","e78c17a3":"Nu skal vi s\u00e5 have udvalgt de kolonner vi tror mest p\u00e5. Ud fra vores analyse, m\u00e5 det v\u00e6re: Store, Dept, Type, year, month, day, week, IsHoliday. S\u00e5 vi laver en feature liste, som vi kan bruge til at udv\u00e6lge kolonnerne","b8583cb9":"Er det et godt resultat? Hvis vi kigger p\u00e5 leaderboardet, s\u00e5 er det faktisk en rigtig god score. Faktisk bedre end nummer 1. Men nu har vi jo kun testet p\u00e5 det data vi havde tilg\u00e6ngeligt, s\u00e5 det kan \u00e6ndre sig, n\u00e5r vi indsender vores resultat. Men der burde v\u00e6re en rimelig god chance. Men vi b\u00f8r nok afpr\u00f8ve noget cross validation for at v\u00e6re sikker p\u00e5, at vi ikke bare har v\u00e6ret heldige","34e98f6e":"Ogs\u00e5 store individuelle forskelle, ikke underligt, men godt at vide","79c172c8":"Lad os pr\u00f8ve at se, om typen af butik kunne have indflydelse p\u00e5 salget. I og med nogle butikker er st\u00f8rre end andre, vil der nok v\u00e6re en stor forskel","e80f1a77":"xgboost har brug for noget der kaldes en DataMatrix, s\u00e5 vi m\u00e5 lave vores data om til den type","5e9ac329":"Det ligger godt nok overraskende stabilt! Ud over at vi skal bruge \u00e5ret i forhold til predicte, ser det faktisk ikke ud til at g\u00f8re nogen som helst forskel","87e0b083":"N\u00e5r vi har fitted vores model, kan vi sp\u00f8rge modellen, hvilke features der var de v\u00e6sentligste i forhold til at opn\u00e5 et godt resultat","57262cc8":"Gad vide, hvordan salget har \u00e6ndret sig gennem \u00e5rene?","f23a37fb":"og s\u00e5 predicte","939135aa":"Det vi selvf\u00f8lgelig skal v\u00e6re opm\u00e6rksomme p\u00e5, er at den store korrelation mellem type og st\u00f8rrelse begge indikerer det samme, og det vil derfor v\u00e6re bedre for modellen, at vi fjerner en af dem p\u00e5 sigt","7e14c276":"# ** 5. L\u00f8s opgaven**"}}