{"cell_type":{"e236d54d":"code","91aa808f":"code","ad850d42":"code","53ee8e41":"code","80eeb90e":"code","5c1a18df":"code","45582962":"code","260d9643":"code","69b5c095":"code","ce24634a":"code","9e219818":"markdown","331d2873":"markdown","3d61d728":"markdown","2b7ceecc":"markdown","a151354e":"markdown"},"source":{"e236d54d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#function definitions\ndef MahalanobisDist(obs, mu, covMat):\n    meanDiff = obs - mu;\n    #print(meanDiff.shape)\n    invCovMat = np.linalg.inv(covMat)\n    #print(invCovMat.shape)\n    p1 = np.matmul(invCovMat, meanDiff)\n    #print(p1)\n    p2 = np.matmul(np.transpose(meanDiff), p1)\n    d = np.mean(np.sqrt(p2))\n    #print(d)\n    return d \n\ndef RemoveOutliersByMD(df):\n    onlyMags = df.iloc[:,1:4]\n    #print(onlyMags)\n    covMat = onlyMags.cov()\n    colMeans = onlyMags.mean(axis=0)\n    print(colMeans)\n    mDists = []\n    for i, obs in onlyMags.iterrows():    \n        mDists.append(MahalanobisDist(obs, colMeans, covMat))\n    #print(np.array(mDists).shape)\n    # spDataClean_ just for display purposes\n    # spDataClean_ = spDataClean;\n    # spDataClean_['MahaDists'] = mDists\n    # spDataClean_.sort_values(\"MahaDists\", ascending=False).head(30)\n    # remove observations that are 60% of the max Mahalanobis distance\n    maxMDist = np.max(mDists)\n    dfOR = df.drop(df.index[mDists > (0.6 * maxMDist)], axis=0)\n    #spDataCleanOR.sort_values(\"MahaDists\", ascending=False).head(30)\n    return dfOR;\n\ndef EucDist(x1, y1, x2, y2):\n    return np.sqrt( np.power((x1-x2),2) + np.power((y1-y2), 2) )\n\ndef DoGridSearchTuning(X, y, numFolds, params, model):\n    gsRes = GridSearchCV(model, param_grid=params, cv=numFolds, n_jobs=-1, scoring='neg_mean_absolute_error')\n    gsRes.fit(X,y)\n    return gsRes\n\ndef PlotResultsOfGridSearch(gsResult):\n    meanScores = np.absolute(gsResult.cv_results_['mean_test_score'])\n    #print(meanScores)\n    T = gsResult.cv_results_['params']\n    axes = T[0].keys()\n    cnt = 0\n    var = []\n    for ax in axes:\n        var.append(np.unique([ t[ax] for t in T ]))\n        cnt = cnt+1\n    x = np.reshape(meanScores, newshape=(len(var[0]),len(var[1])))\n    xDf = pd.DataFrame(x, columns = var[1], index=var[0])\n    print(xDf)\n    plt.figure(figsize=(9,6))\n    sb.heatmap(xDf, annot=True, cbar_kws={'label': 'MAE'}, fmt='.6g')\n    plt.xlabel(list(axes)[1])\n    plt.ylabel(list(axes)[0])\n    plt.show()\n\ndef EvaluateModel(dataTrain, dataTest, xTrain, yTrain, xTest, yTest, modelX, modelY, tagline):\n    modelX.fit(dataTrain, xTrain)\n    modelY.fit(dataTrain, yTrain)\n    predX = modelX.predict(dataTest)\n    predY = modelY.predict(dataTest)\n    dists = EucDist(predX, predY, xTest, yTest);\n    meanED = np.mean(dists)\n    maxED = np.max(dists)\n    minED = np.min(dists)\n    print(tagline)\n    print(\"meanED = \" + str(meanED) + \" m\")\n    print(\"maxED = \" + str(maxED) + \" m\")\n    print(\"minED = \" + str(minED) + \" m\")\n    plt.figure(figsize=(9,6))\n    sb.distplot(dists, hist=True, kde=False, \n             bins=100, color = 'blue',\n             hist_kws={'edgecolor':'black'})\n    \n    plt.title(tagline)\n    plt.ylabel('Frequency of Error')\n    plt.xlabel('Euclidean Distance (m)')\n    \ndef EvaluateModelNN(dataTrain, dataTest, xyTrain, xyTest, model, tagline):\n    model.fit(dataTrain, xyTrain)\n    pred = model.predict(dataTest)\n    dists = EucDist(pred[:,0], pred[:,1], xyTest[:,0], xyTest[:,1]);\n    meanED = np.mean(dists)\n    maxED = np.max(dists)\n    minED = np.min(dists)\n    print(tagline)\n    print(\"meanED = \" + str(meanED) + \" m\")\n    print(\"maxED = \" + str(maxED) + \" m\")\n    print(\"minED = \" + str(minED) + \" m\")\n    \ndef DoModelAnalysis(Tr, Te, yTr, yTe, scaler, stdParams, expParams, numFolds, model, modelName, dataName):\n    # do scaling\n    Tr = scaler.transform(Tr)\n    Te = scaler.transform(Te)\n    \n    # do grid search\n    gsX = DoGridSearchTuning(Tr, yTr[:,0], numCV, expParams, model(**stdParams))\n    gsY = DoGridSearchTuning(Tr, yTr[:,1], numCV, expParams, model(**stdParams))\n    print('Best parameters for X regressor ' + str(modelName) + ' on ' + str(dataName) + \":\")\n    print(gsX.best_params_)\n    PlotResultsOfGridSearch(gsX)\n    \n    print('Best parameters for Y regressor ' + str(modelName) + ' on ' + str(dataName) + \":\")\n    print(gsY.best_params_)\n    PlotResultsOfGridSearch(gsY)\n  \n    gsX.best_params_.update(stdParams)\n    gsY.best_params_.update(stdParams)\n    #modelXEval = model.set_params(**gsX.best_params_)\n    modelXEval = model(**gsX.best_params_)\n    \n    #modelXEval = MLPRegressor(**gsX.best_params_)\n    #print(modelXEval)\n    #modelYEval = model.set_params(**gsY.best_params_)\n    modelYEval = model(**gsY.best_params_)\n    \n    #modelYEval = MLPRegressor(**gsY.best_params_)\n    #print(modelYEval)\n    \n#     if modelName == 'Neural Network':\n#         modelXEval = MLPRegressor(**gsX.best_params_)\n#         modelXEval = MLPRegressor(**gsX.best_params_)\n        \n    \n    modelXEval.fit(Tr, yTr[:,0])\n    modelYEval.fit(Tr, yTr[:,1])\n    \n    EvaluateModel(Tr, Te, yTr[:,0], yTr[:,1], yTe[:,0], yTe[:,1], modelXEval, modelYEval, '-- Metrics for ' + str(modelName) + ' on ' + str(dataName) + ' --')\n\n    ","91aa808f":"# smartphone data\nspData = pd.read_csv('..\/input\/m1SmartPhoneDataWithPosition.csv')\nspData = pd.concat([spData, pd.read_csv('..\/input\/m2SmartPhoneDataWithPosition.csv')])\nspData = spData.drop(spData.columns[0], axis=1)\n#spData = spData.drop(spData.columns[0], axis=1)\nspData = spData.reset_index()\n#spData.head(10)\nspDataClean = spData.drop(spData.index[spData['posId'] == -1])\nprint(spDataClean.shape)\nspDataClean.head(10)\n\n","ad850d42":"# smartwatch data\nswData = pd.read_csv('..\/input\/m1SmartWatchDataWithPosition.csv')\nswData = pd.concat([swData, pd.read_csv('..\/input\/m2SmartWatchDataWithPosition.csv')])\nswData = swData.drop(swData.columns[0], axis=1)\n#spData = spData.drop(spData.columns[0], axis=1)\nswData = swData.reset_index()\n#spData.head(10)\nswDataClean = swData.drop(swData.index[swData['posId'] == -1])\nprint(swDataClean.shape)\nswDataClean.head(10)","53ee8e41":"# do outlier removal\nswDataCleanOR = RemoveOutliersByMD(swDataClean)\nspDataCleanOR = RemoveOutliersByMD(spDataClean)\n#swDataCleanOR = swDataClean\n#spDataCleanOR = spDataClean\nprint(swDataCleanOR.shape)\nprint(spDataCleanOR.shape)","80eeb90e":"# do data splitting\ndata = spDataCleanOR\nx = data['x']\ny = data['y']\ndata = data.iloc[:,[1,2,3,5,6]]\nprint(data.columns)\n\nspTr, spTe, spOutXTr, spOutXTe = train_test_split(data, x, test_size=0.3, random_state=2)\nspTr, spTe, spOutYTr, spOutYTe = train_test_split(data, y, test_size=0.3, random_state=2)\nspScaler = StandardScaler()\nspScaler.fit(spTr)\n#spTr = spScaler.transform(spTr)\n\ndata = swDataCleanOR\nx = data['x']\ny = data['y']\ndata = data.iloc[:,[1,2,3,4,5]]\nprint(data.columns)\n\nswTr, swTe, swOutXTr, swOutXTe = train_test_split(data, x, test_size=0.3, random_state=2)\nswTr, swTe, swOutYTr, swOutYTe = train_test_split(data, y, test_size=0.3, random_state=2)\nswScaler = StandardScaler()\nswScaler.fit(swTr)\n#swTr = swScaler.transform(swTr)","5c1a18df":"# do grid search for neural net (with x and y simultaneously)\nnumCV=5\nstdParamsNN = {'solver': 'lbfgs', 'random_state': 4}\nparamsToTestNN = {'hidden_layer_sizes': [(3,),(5,3),(5,3,3)], 'activation': ['logistic', 'relu','tanh']}\nxySW = np.stack((swOutXTr, swOutYTr), axis=1)\n# print(xySW.shape)\n# nnSWGSRes = DoGridSearchTuning(swTr, xySW, numCV, paramsToTest, MLPRegressor(**stdParams))\nxySP = np.stack((spOutXTr, spOutYTr), axis=1)\n# print(xySP[:,1])\n# print(xySP.shape)\n# nnSPGSRes = DoGridSearchTuning(spTr, xySP, numCV, paramsToTest, MLPRegressor(**stdParams))\n\n# print(nnSWGSRes.best_params_)\n# PlotResultsOfGridSearch(nnSWGSRes)\n\n# print(nnSPGSRes.best_params_)\n# PlotResultsOfGridSearch(nnSPGSRes)\n\nactuXYSP = np.stack((spOutXTe, spOutYTe), axis=1)\nactuXYSW = np.stack((swOutXTe, swOutYTe), axis=1)\n\n# Do model analysis for neural network\n# DoModelAnalysis(spTr, spTe, xySP, actuXYSP, spScaler, stdParamsNN, paramsToTestNN, numCV, \n#                 MLPRegressor(**stdParamsNN), 'Neural Network', 'Smartphone Data')\n\nDoModelAnalysis(spTr, spTe, xySP, actuXYSP, spScaler, stdParamsNN, paramsToTestNN, numCV, \n                MLPRegressor, 'Neural Network', 'Smartphone Data')\n\n\nDoModelAnalysis(swTr, swTe, xySW, actuXYSW, swScaler, stdParamsNN, paramsToTestNN, numCV, \n                MLPRegressor, 'Neural Network', 'Smartwatch Data')\n\n\n","45582962":"# # do fitting and evaluation for NN\n# spTe = spScaler.transform(spTe)\n# swTe = swScaler.transform(swTe)\n\n# nnSWGSRes.best_params_.update(stdParams)\n# nnSPGSRes.best_params_.update(stdParams)\n# nnSP = MLPRegressor(**nnSPGSRes.best_params_)\n# nnSW = MLPRegressor(**nnSWGSRes.best_params_)\n# # nnSP.fit(spTr, xySP)\n# # nnSW.fit(swTr, xySW)\n\n# actuXYSW = np.stack((swOutXTe, swOutYTe), axis=1)\n# actuXYSP = np.stack((spOutXTe, spOutYTe), axis=1)\n\n# EvaluateModel(swTr, swTe, xySW, actuXYSW, nnSW, '-- Metrics for NN smartwatch data --')\n# EvaluateModel(spTr, spTe, xySP, actuXYSP, nnSP, '-- Metrics for NN smartphone data --')\n","260d9643":"stdParamsRF = {'max_features': 'sqrt', 'n_jobs': -1, 'random_state': 4}\nparamsToTestRF = {'n_estimators': [150, 300, 450], 'min_samples_leaf': [5, 10, 15, 20]}\n\nDoModelAnalysis(spTr, spTe, xySP, actuXYSP, spScaler, stdParamsRF, paramsToTestRF, numCV, \n                RandomForestRegressor, 'Random Forest', 'Smartphone Data')\n\nDoModelAnalysis(swTr, swTe, xySW, actuXYSW, swScaler, stdParamsRF, paramsToTestRF, numCV, \n                RandomForestRegressor, 'Random Forest', 'Smartwatch Data')","69b5c095":"stdParamsGBR = {'random_state': 4}\nparamsToTestGBR = {'n_estimators': [150, 300, 450], 'min_samples_leaf': [5, 10, 15, 20]}\n\nDoModelAnalysis(spTr, spTe, xySP, actuXYSP, spScaler, stdParamsGBR, paramsToTestGBR, numCV, \n                GradientBoostingRegressor, 'Boosting Trees', 'Smartphone Data')\n\nDoModelAnalysis(swTr, swTe, xySW, actuXYSW, swScaler, stdParamsGBR, paramsToTestGBR, numCV, \n                GradientBoostingRegressor, 'Boosting Trees', 'Smartwatch Data')","ce24634a":"stdParamsSVR = {'max_iter': 1000, 'kernel': 'poly'}\nparamsToTestSVR = {'degree': [2,3,4], 'C': [0.001, 0.01, 1]}\n\nDoModelAnalysis(spTr, spTe, xySP, actuXYSP, spScaler, stdParamsSVR, paramsToTestSVR, numCV, \n                SVR, 'Support Vector Machine', 'Smartphone Data')\n\nDoModelAnalysis(swTr, swTe, xySW, actuXYSW, swScaler, stdParamsSVR, paramsToTestSVR, numCV, \n                SVR, 'Support Vector Machine', 'Smartwatch Data')","9e219818":"** Neural Network Analysis **","331d2873":"** Random Forest Analysis **","3d61d728":"** DATA PREPARATION ** \n1. Omit -1 posId.\n2. Outlier removal with Mahalanobis distance.\n3. Scale data with mean 0 and scale 1.\n4. Only use magnetic data fields (angles don't seem to work with decorrelating the data)","2b7ceecc":"** Gradient Boosting Trees Analysis **","a151354e":"** SVM Regression **"}}