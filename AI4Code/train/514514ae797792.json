{"cell_type":{"94244add":"code","adc5a497":"code","b7a77c70":"code","06298e15":"code","faf041dd":"code","e8e636c4":"code","d2c47d52":"code","aacf88f8":"code","207253ec":"code","c8425f66":"code","0d1c1837":"code","ce23a51f":"code","9a529dd7":"code","5cd746b2":"code","d9d28135":"code","6a1e4625":"code","907c72cc":"code","f0855289":"code","7fa0d320":"code","8443505c":"code","b2973c2d":"code","891b6e9a":"code","b4b33d98":"code","64024841":"code","b7671213":"code","e279bd5e":"code","206b9eb0":"code","b7bd88ab":"code","493c69a7":"code","ac121d00":"code","e37de267":"code","527f85b3":"code","d5035aec":"code","046981b1":"code","37aaeda6":"code","cda79b2a":"code","b888d308":"code","3adcbbc4":"code","a3149948":"code","e06568c9":"code","b2899161":"code","a2bb6cf9":"code","8d67c2e0":"code","da03d10c":"code","ee11c0f6":"code","0e7cc662":"code","c5ad5332":"code","d11acba5":"code","d9b0f731":"code","c7733f6e":"markdown","168785e8":"markdown","56c4eea1":"markdown","9b9c5058":"markdown","190f6c44":"markdown","c7e04b67":"markdown","a4c145aa":"markdown","2fbc4261":"markdown","08ead422":"markdown","95441bb1":"markdown","0baa7296":"markdown","6030625e":"markdown","131a8b9f":"markdown","85f5ad5c":"markdown","e6fa03bf":"markdown","9c36da26":"markdown","e8a6f893":"markdown","6b6d2197":"markdown","35db72d3":"markdown","92be3f15":"markdown","8847b401":"markdown","f2c3be59":"markdown","b575f12f":"markdown","30f219af":"markdown","635818c1":"markdown","d7993944":"markdown","65680deb":"markdown","53793234":"markdown","acab082b":"markdown","1de61218":"markdown","33a345aa":"markdown","36fdf3a5":"markdown","ad13035e":"markdown","2a6f3115":"markdown","e44f8542":"markdown","d86291f1":"markdown","3e364633":"markdown","a06ae90e":"markdown","2855b46b":"markdown","1ac3db85":"markdown","827a8512":"markdown","cc714cfe":"markdown"},"source":{"94244add":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport numpy as np\n\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom sklearn.svm import SVR\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom xgboost import plot_importance,plot_tree\nfrom mlxtend.regressor import StackingCVRegressor\n","adc5a497":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\nprint('Size of the training dataset',df_train.shape)\nprint('Size of the testing dataset',df_test.shape)","b7a77c70":"plt.style.use('default')\n\nmissing_train = df_train.isnull().sum()\nmissing_train = missing_train[missing_train > 0].sort_values(ascending = False)\nplt.figure(figsize = (12, 3))\nplt.subplot(1, 2, 1)\ng = sns.barplot(missing_train.index, missing_train.values,color = 'seagreen', alpha = 0.7)\ng.set_xticklabels(missing_train.index, rotation = 90, fontsize = 8)\nplt.yticks(fontsize = 10)\nplt.title('Missing values in the training set', fontsize = 10)\nplt.ylabel('', fontsize = 10)\n\nmissing_test = df_test.isnull().sum()\nmissing_test = missing_test[missing_test > 0].sort_values(ascending = False)\nplt.subplot(1, 2, 2)\ng = sns.barplot(missing_test.index, missing_test.values, color='seagreen', alpha = 0.7)\ng.set_xticklabels(missing_test.index, rotation = 90, fontsize = 8)\nplt.yticks(fontsize = 10)\nplt.title('Missing values in the test set', fontsize = 10)\nplt.ylabel('', fontsize = 10)\n\nplt.show()","06298e15":"def eda(df_train, df_test):\n    df_train_explore = df_train.copy()#create copies so that the original sets remain unaltered after the EDA \n    df_test_explore = df_test.copy()\n    col = df_test_explore.columns # I will look at the 'Sale Price' column later\n    f = 16 #font size\n    for i in range(1, len(col)): # start from 1 because I am not interested in the 'Id' column\n        print(col[i])\n        t = df_train_explore[col[i]].dtypes\n        \n        plt.figure(figsize = (28, 4))\n        df_train_explore[col[i]].dropna(inplace = True) # for the first EDA I just remove the NaN values because \n                                                       #some Seaborn plots cannot handle them  \n        df_test_explore[col[i]].dropna(inplace = True)\n        \n        if t != 'object': #numerical features\n            plt.subplot(1, 4, 1)\n            sns.distplot(df_train_explore[col[i]], color = 'seagreen', hist_kws = {\"alpha\": 0.5})\n            plt.ylabel('Frequency', fontsize = f)\n            plt.xlabel(col[i], fontsize = f)\n            plt.title('Training set')\n            plt.subplot(1, 4, 2)\n            sns.distplot(df_test_explore[col[i]], color = 'seagreen', hist_kws = {\"alpha\": 0.5})\n            plt.ylabel('Frequency', fontsize = f)\n            plt.xlabel(col[i], fontsize = f)\n            plt.title('Test set')\n            plt.subplot(1, 4, 3)\n            sns.scatterplot(x = col[i], y = 'SalePrice', data = df_train_explore, color = 'seagreen', alpha = 0.8)\n            plt.ylabel('Sale Price',fontsize = f)\n            plt.xlabel(col[i], fontsize = f)\n            \n            if len(df_train_explore[col[i]].unique()) > 20: #bin the continuous features\n                df_train_explore[col[i]], bins=pd.cut(df_train_explore[col[i]], 10, retbins = True, duplicates = 'drop')\n                \n            grouped_train = df_train_explore.groupby(col[i])\n            grouped_mean_train = grouped_train['SalePrice'].mean()   \n            if grouped_mean_train.index.values.dtype == 'int64':\n                x = grouped_mean_train.index.values\n            else:\n                x = (bins[0:len(bins)-1] + bins[1:len(bins)])\/2\n            plt.subplot(1, 4, 4)\n            g = sns.lineplot(x, grouped_mean_train.values, color = 'seagreen',alpha = 0.8, marker = 'o')\n            plt.ylabel('Mean Sale Price',fontsize = f)\n            plt.xlabel(col[i], fontsize = f)\n           \n        else:#categorical features\n            grouped_train = df_train_explore.groupby(col[i])\n            grouped_test = df_test_explore.groupby(col[i])\n            grouped_train_count = grouped_train.count()\n            grouped_test_count = grouped_test.count()\n            grouped_mean_train = grouped_train['SalePrice'].mean()   \n            plt.subplot(1, 4, 1)\n            g = sns.barplot(grouped_train_count.index.values, grouped_train_count['Id'].values, color = 'seagreen', alpha = 0.7)\n            plt.ylabel('Frequency', fontsize = f)\n            g.set_xticklabels(grouped_train_count.index.values, rotation = 90, fontsize = f)\n            plt.title('Training set')\n            plt.subplot(1, 4, 2)\n            g = sns.barplot(grouped_test_count.index.values, grouped_test_count['Id'].values, color = 'seagreen', alpha = 0.7)\n            g.set_xticklabels(grouped_test_count.index.values, rotation = 90, fontsize = f)\n            plt.ylabel('Frequency',fontsize = f)\n            g.set_xticklabels(grouped_test_count.index.values, rotation = 90, fontsize = f)\n            plt.title('Test set')\n\n            plt.subplot(1, 4, 3)\n            g = sns.scatterplot(x = col[i], y = 'SalePrice', data = df_train_explore, color = 'seagreen', alpha = 0.8)\n            plt.ylabel('Sale Price', fontsize = f)\n            g.set_xticklabels(grouped_train_count.index.values, rotation = 90, fontsize = f)\n            plt.xlabel('', fontsize = f)\n            \n            plt.subplot(1, 4, 4)\n            g = sns.lineplot(grouped_mean_train.index.values, grouped_mean_train.values, color = 'seagreen', alpha = 0.8, marker = 'o')\n            g.set_xticklabels(grouped_mean_train.index.values, rotation = 90, fontsize = f)\n            plt.ylabel('Mean Sale Price', fontsize = f)\n        plt.show()","faf041dd":"eda(df_train, df_test)","e8e636c4":"plt.figure(figsize = (8, 3))\nplt.subplot(1, 2, 1)\nsns.distplot(df_train['SalePrice'], color = 'seagreen', hist_kws = {\"alpha\": 0.5})\nplt.title('SalePrice')\n\nplt.subplot(1, 2, 2)\nsns.distplot(np.log1p(df_train['SalePrice']), color = 'seagreen', hist_kws = {\"alpha\": 0.5})\nplt.title('Log(SalePrice)')\nplt.show()","d2c47d52":"def clean_data(df):\n    df = df.replace({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1})\n    df = df.replace({'Grvl': 1, 'Pave': 2})\n    df['Alley'].fillna(0, inplace = True)\n    df['GarageYrBlt'].fillna(0, inplace = True)\n    df['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n    df = df.replace({'LotShape':{'IR3': 1, 'IR2': 2, 'IR1': 3, 'Reg': 4}})\n    df['LotShape'].fillna(df['LotShape'].value_counts().idxmax(), inplace=True) #if missing fill with the most common value\n    df['MSZoning'].fillna(df['MSZoning'].value_counts().idxmax(), inplace=True)\n    df['Utilities'].fillna(df['Utilities'].value_counts().idxmax(), inplace=True)\n    df = df.replace({'Utilities':{'ELO': 1, 'NoSeWa': 2, 'NoSewr': 3, 'AllPub': 4}})\n    df['LandSlope'].fillna(df['LandSlope'].value_counts().idxmax(), inplace = True) \n    df = df.replace({'LandSlope':{'Gtl': 1, 'Mod': 2, 'Sev': 3}} )\n    df['Exterior1st'].fillna(df['Exterior1st'].value_counts().idxmax(), inplace = True)\n    df['Exterior2nd'].fillna(df['Exterior2nd'].value_counts().idxmax(), inplace = True)\n    df['MasVnrType'].fillna(df['MasVnrType'].value_counts().idxmax(), inplace = True)\n    df['MasVnrArea'].fillna(0, inplace = True)\n    df['BsmtFinType1'].fillna('NoBsmt', inplace = True)\n    df['BsmtFinType2'].fillna('NoBsmt', inplace = True)\n    df['BsmtCond'].fillna(0, inplace = True)\n    df['BsmtQual'].fillna(0, inplace = True)\n    df['BsmtExposure'].fillna(0, inplace = True)\n    df = df.replace({'BsmtExposure':{'Av': 3, 'Mn': 2, 'No': 1}})\n    df['BsmtFinSF1'].fillna(0, inplace = True)\n    df['BsmtFinSF2'].fillna(0, inplace = True)\n    df['BsmtUnfSF'].fillna(0, inplace = True)\n    df['TotalBsmtSF'].fillna(0, inplace = True)\n    df = df.replace({'CentralAir':{'Y': 1, 'N': 0}})\n    df['1stFlrSF'].fillna(0, inplace = True)\n    df['2ndFlrSF'].fillna(0, inplace = True)\n    df['LowQualFinSF'].fillna(0, inplace = True)\n    df['BsmtFullBath'].fillna(0, inplace = True)\n    df['BsmtHalfBath'].fillna(0, inplace = True)\n    df['FullBath'].fillna(0, inplace = True)\n    df['HalfBath'].fillna(0, inplace = True)\n    df['KitchenQual'].fillna(df['KitchenQual'].value_counts().idxmax(),inplace = True)\n    df['Functional'].fillna('Typ',inplace = True)\n    df = df.replace({'Functional':{'Typ': 7, 'Min1': 6, 'Maj1': 5, 'Min2': 4, 'Mod': 3, 'Maj2': 2, 'Sev': 1, 'Sal': 0}})\n    df['FireplaceQu'].fillna(0, inplace = True)\n    df['GarageType'].fillna('NoGarage', inplace = True)\n    df['GarageFinish'].fillna(0, inplace = True)\n    df = df.replace({'GarageFinish':{'Fin': 3, 'RFn': 2, 'Unf': 1}})\n    df['GarageArea'].fillna(0, inplace = True)\n    df['GarageCars'].fillna(0, inplace = True)\n    df['GarageQual'].fillna(0, inplace = True)\n    df['GarageCond'].fillna(0, inplace = True)\n    df = df.replace({'PavedDrive':{'Y': 2, 'P': 1, 'N': 0}})\n    df['Electrical'].fillna(df['Electrical'].value_counts().idxmax(), inplace = True)\n    df['PoolQC'].fillna(0, inplace = True)\n    df['Fence'].fillna('NoFence', inplace = True)\n    df['MiscFeature'].fillna('NoMiF', inplace = True)\n    df['SaleType'].fillna(df['SaleType'].value_counts().idxmax(), inplace = True)\n    return df","aacf88f8":"def num_to_cat(df):\n    #df['MSSubClass'] = df['MSSubClass'].astype(str)\n    df['YrSold'] = df['YrSold'].astype(str)\n    #df['MoSold'] = df['MoSold'].astype(str)\n    return df","207253ec":"def remove_columns(df):\n    col = ['Id', 'Utilities', 'Street', 'PoolArea' ,'LowQualFinSF', 'Alley', 'EnclosedPorch', '3SsnPorch', 'PoolQC',\n           'KitchenAbvGr']\n    df.drop(columns = col, inplace = True)\n    return df","c8425f66":"def add_features(df):\n    df['YrBltAndRemod'] = df['YearBuilt'] + df['YearRemodAdd']\n    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n    df['Total_sqr_footage'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF'])\n    df['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) + df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n    df['Total_porch_sf'] = (df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF'])\n    \n    df['haspool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['hasgarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['hasbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['hasfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    return df","0d1c1837":"def find_outliers(df):\n    outliers = df[df['GrLivArea'] > 4000].index\n    return outliers","ce23a51f":"def add_polynomial(df):\n    col = ['OverallQual', 'ExterQual', 'BsmtQual', 'TotalBsmtSF', 'GrLivArea', 'GarageArea', 'GarageQual', 'KitchenQual',\n           'TotalSF']\n    for c in col:\n        c_2 = c + '-2'\n        c_3 = c + '-3'\n        c_sqrt = c + '-sqrt'\n        df[c_2] = df[c]**2\n        df[c_3] = df[c]**3\n        df[c_sqrt] = np.sqrt(df[c])\n    return df   ","9a529dd7":"def transf_skewed(df):\n    numerical = [var for var in df.columns if df[var].dtype != 'object' and var not in ['Id', 'SalePrice']]\n    skew_features = df[numerical].apply(lambda x: skew(x)).sort_values(ascending=False)\n    high_skew = skew_features[skew_features > 0.5]\n    skew_index = high_skew.index\n    for i in skew_index:\n        df[i] = boxcox1p(df[i],  boxcox_normmax(df[i] + 1))       \n    return df","5cd746b2":"def one_hot_encoding(df):\n    df = pd.get_dummies(df)\n    return df","d9d28135":"def label_encoding(train):\n    obj = [var for var in train.columns if train[var].dtype == 'object']\n    for c in obj:\n        le = LabelEncoder()\n        train[c] = le.fit_transform(train[c])\n    return train","6a1e4625":"s = df_train.shape[0]\ny = df_train['SalePrice']\ny_log = np.log1p(y)\ndf_train = df_train.drop(columns = 'SalePrice')\n\noutliers=find_outliers(df_train)\ny_log.drop(outliers, inplace = True)\n\nX = pd.concat([df_train, df_test])","907c72cc":"score = pd.DataFrame(columns = ['model', 'score'])","f0855289":"cleaned_X = clean_data(X)\nremove_columns(cleaned_X)\nlabel_encoding(cleaned_X)\n\nX_train_TM = cleaned_X.iloc[:s, :]\nX_test_TM = cleaned_X.iloc[s:, :]\n\nX_train_TM.drop(outliers, inplace = True)\n\nprint('Missing values in the cleaned training set:', X_train_TM.isnull().sum().sum())\nprint('Missing values in the cleaned testing set:', X_test_TM.isnull().sum().sum())","7fa0d320":"xgb_model = xgb.XGBRegressor(learning_rate = 0.01, n_estimators = 5800,\n                                     max_depth = 2, min_child_weight = 1,\n                                     gamma = 0, subsample = 0.5,\n                                     colsample_bytree = 0.5,\n                                     objective = 'reg:linear', nthread = -1,\n                                     scale_pos_weight = 1, seed = 27,\n                                     reg_alpha = 0.2, random_state = 42)\nsc = cross_val_score(xgb_model, X_train_TM, y_log, scoring = \"neg_mean_squared_error\", cv = 10)\nnp.sqrt(-sc.mean())\nscore = score.append({'model': 'XGBoost', 'score': np.sqrt(-sc.mean())}, ignore_index = True)\nprint('The score of the XGBoost model:', np.sqrt(-sc.mean()))","8443505c":"xgb_model.fit(X_train_TM, y_log)\n\nplt.style.use('seaborn') \nfig, ax1 = plt.subplots(figsize = (6, 5))\nplot_importance(xgb_model, importance_type = 'weight', grid = 'on', height = 0.8, max_num_features = 25, color = 'seagreen',\n                ax = ax1, alpha = 0.6)\nplt.xticks(fontsize = 8)\nplt.yticks(fontsize = 8)\nplt.xlabel('F score', fontsize = 8)\nplt.ylabel('Features', fontsize = 8)\nplt.title('Feature importance', fontsize = 12)\nplt.show()","b2973c2d":"rforest_model = RandomForestRegressor(criterion = 'mse', max_depth = 18,\n           max_features = 0.4, min_samples_leaf = 1, min_samples_split = 2,\n           min_weight_fraction_leaf = 0, n_estimators = 5500, random_state = 0)\nsc = cross_val_score(rforest_model, X_train_TM, y_log, scoring = \"neg_mean_squared_error\", cv = 10)\nnp.sqrt(-sc.mean())\nscore = score.append({'model': 'RandomForest', 'score': np.sqrt(-sc.mean())}, ignore_index = True)\n\nprint('The score of the RandomForest model:', np.sqrt(-sc.mean()))","891b6e9a":"gbr_model = GradientBoostingRegressor(loss = 'lad', learning_rate = 0.025, n_estimators = 5500,\n                                    subsample = 0.5, criterion = 'friedman_mse',\n                                    min_samples_split = 7, min_samples_leaf = 1, min_weight_fraction_leaf = 0.0,\n                                    max_depth = 2, min_impurity_decrease = 0.0, min_impurity_split = None, init = None, \n                                    random_state = 42, max_features = 'sqrt', alpha = 0.9, verbose = 0, \n                                    max_leaf_nodes = None, warm_start = False, presort = 'auto', validation_fraction = 0.3, \n                                    n_iter_no_change = None, tol = 0.0001)\nsc = cross_val_score(gbr_model, X_train_TM, y_log, scoring = \"neg_mean_squared_error\", cv = 10)\nnp.sqrt(-sc.mean())\nscore = score.append({'model': 'GradientBoosting', 'score': np.sqrt(-sc.mean())}, ignore_index = True)\n\nprint('The score of the GradientBoosting model:', np.sqrt(-sc.mean()))","b4b33d98":"lgb_model = lgb.LGBMRegressor(objective = 'regression', num_leaves = 5,\n                              learning_rate = 0.009, n_estimators = 5500,\n                              max_bin = 200, bagging_fraction = 0.705,\n                              bagging_freq = 5, feature_fraction = 0.2,\n                              feature_fraction_seed = 9, bagging_seed = 9,\n                              min_data_in_leaf = 2)\nsc = cross_val_score(lgb_model, X_train_TM, y_log, scoring = \"neg_mean_squared_error\", cv = 10)\nnp.sqrt(-sc.mean())\nscore = score.append({'model': 'LightGBM', 'score': np.sqrt(-sc.mean())}, ignore_index = True)\nprint('The score of the LightGBM model:', np.sqrt(-sc.mean()))","64024841":"cleaned_X = clean_data(X)\ncleaned_X = add_features(cleaned_X)\ncleaned_X = add_polynomial(cleaned_X)\ncleaned_X = num_to_cat(cleaned_X)\ncleaned_X = transf_skewed(cleaned_X)\n\nnumerical = [var for var in cleaned_X if cleaned_X[var].dtype != 'object']\ncolumns_scaling = (cleaned_X[numerical].max() > 3).index.values\nrobust_scaler = RobustScaler()\ncleaned_X[columns_scaling] = robust_scaler.fit_transform(cleaned_X[columns_scaling])\n\nremove_columns(cleaned_X)\ncleaned_X = one_hot_encoding(cleaned_X)\n\nX_train_LM = cleaned_X.iloc[:s, :]\nX_test_LM = cleaned_X.iloc[s:, :]\n\nX_train_LM.drop(outliers, inplace = True)\n\nprint('Missing values in the cleaned training set:', X_train_LM.isnull().sum().sum())\nprint('Missing values in the cleaned testing set:', X_test_LM.isnull().sum().sum())","b7671213":"lasso_model = linear_model.Lasso(alpha = 0.00046, max_iter = 500, fit_intercept = True)\nsc = cross_val_score(lasso_model, X_train_LM, y_log, scoring = \"neg_mean_squared_error\", cv = 10)\nscore = score.append({'model': 'Lasso', 'score': np.sqrt(-sc.mean())}, ignore_index = True)\nprint('The score of the Lasso model:', np.sqrt(-sc.mean()))","e279bd5e":"elastic_model = linear_model.ElasticNet(alpha = 0.00046, l1_ratio = 1, fit_intercept = True, normalize = False, \n                                        precompute = False, max_iter = 500, copy_X = True, tol = 0.001, warm_start = False,\n                                        positive = False, random_state = 42, selection = 'cyclic')\nsc = cross_val_score(elastic_model, X_train_LM, y_log, scoring = \"neg_mean_squared_error\", cv = 10)\nscore = score.append({'model': 'ElasticNet', 'score': np.sqrt(-sc.mean())}, ignore_index = True)\nprint('The score of the ElasticNet model:', np.sqrt(-sc.mean()))","206b9eb0":"lassolars_model = LassoLarsIC(criterion = 'aic', eps = 0.3)\nsc = cross_val_score(lassolars_model, X_train_LM, y_log, scoring = \"neg_mean_squared_error\", cv = 10)\nscore = score.append({'model': 'LassoLars', 'score': np.sqrt(-sc.mean())}, ignore_index = True)\nprint('The score of the LassoLars model:', np.sqrt(-sc.mean()))","b7bd88ab":"svr_model = SVR(C= 20, epsilon = 0.0117, gamma = 0.00041)\nsc = cross_val_score(svr_model, X_train_LM, y_log, scoring = \"neg_mean_squared_error\", cv = 10)\nscore = score.append({'model': 'SVR', 'score': np.sqrt(-sc.mean())}, ignore_index = True)\nprint('The score of the SVR model:', np.sqrt(-sc.mean()))","493c69a7":"bridge_model = BayesianRidge(alpha_1 = 45, alpha_2 = 0, compute_score = False,\n                            copy_X = True, fit_intercept = True, lambda_1 = 0, lambda_2 = 0,\n                            n_iter = 500, normalize = False, tol = 0.001, verbose = False)\nsc = cross_val_score(bridge_model, X_train_LM, y_log, scoring = \"neg_mean_squared_error\", cv = 10)\nscore = score.append({'model': 'BayesianRidge', 'score': np.sqrt(-sc.mean())}, ignore_index = True)\nprint('The score of the BayesianRidge model:', np.sqrt(-sc.mean()))","ac121d00":"cleaned_X = clean_data(X)\ncleaned_X = add_features(cleaned_X)\ncleaned_X = add_polynomial(cleaned_X)\ncleaned_X = num_to_cat(cleaned_X)\ncleaned_X = transf_skewed(cleaned_X)\n\nremove_columns(cleaned_X)\ncleaned_X = one_hot_encoding(cleaned_X)\n\nX_train_LM2 = cleaned_X.iloc[:s, :]\nX_test_LM2 = cleaned_X.iloc[s:, :]\n\nX_train_LM2.drop(outliers, inplace = True)\n\nprint('Missing values in the cleaned training set:', X_train_LM2.isnull().sum().sum())\nprint('Missing values in the cleaned testing set:', X_test_LM2.isnull().sum().sum())","e37de267":"kridge_model = KernelRidge(alpha = 7.5)\nsc = cross_val_score(kridge_model,  X_train_LM2, y_log, scoring = \"neg_mean_squared_error\", cv = 10)\nscore = score.append({'model': 'KernelRidge', 'score': np.sqrt(-sc.mean())}, ignore_index = True)\nprint('The score of the KernelRidge model:', np.sqrt(-sc.mean()))                                             ","527f85b3":"score","d5035aec":"#Tree based models\nX_train, X_val, y_train, y_val = train_test_split(X_train_TM, y_log, test_size = 0.3,random_state = 0)\n\nxgb_model.fit(X_train, y_train)\ny_val_pred_xgb = np.expm1(xgb_model.predict(X_val))\nprint('XGB:', np.sqrt(mean_squared_log_error(np.expm1(y_val), y_val_pred_xgb)))\n\ngbr_model.fit(X_train, y_train)\ny_val_pred_gbr = np.expm1(gbr_model.predict(X_val))\ngbr_score = np.sqrt(mean_squared_log_error(np.expm1(y_val), y_val_pred_gbr))\nprint('GradientBoosting:', gbr_score)\n\nlgb_model.fit(X_train, y_train)\ny_val_pred_lgb = np.expm1(lgb_model.predict(X_val))\nlgb_score = np.sqrt(mean_squared_log_error(np.expm1(y_val), y_val_pred_lgb))\nprint('LightGBM:', lgb_score)\n\n#Linear models\nX_train, X_val, y_train, y_val=train_test_split(X_train_LM,  y_log, test_size = 0.3, random_state = 0)\n\nlasso_model.fit(X_train, y_train)\ny_val_pred_lasso = np.expm1(lasso_model.predict(X_val))\nprint('Lasso:', np.sqrt(mean_squared_log_error(np.expm1(y_val), y_val_pred_lasso)))\n\nelastic_model.fit(X_train, y_train)\ny_val_pred_elastic = np.expm1(elastic_model.predict(X_val))\nprint(\"ElasticNet:\",np.sqrt(mean_squared_log_error(np.expm1(y_val), y_val_pred_elastic)))\n\nbridge_model.fit(X_train, y_train)\ny_val_pred_bridge = np.expm1(bridge_model.predict(X_val))\nprint('BRidge:',np.sqrt(mean_squared_log_error(np.expm1(y_val), y_val_pred_bridge)))\n\nsvr_model.fit(X_train, y_train)\ny_val_pred_svr = np.expm1(svr_model.predict(X_val))\nprint('SVR:',np.sqrt(mean_squared_log_error(np.expm1(y_val), y_val_pred_svr)))\n\n#KernelRidge\nX_train, X_val, y_train, y_val = train_test_split(X_train_LM2,  y_log, test_size=0.3,random_state = 0)\n\nkridge_model.fit(X_train, y_train)\ny_val_pred_kridge = np.expm1(kridge_model.predict(X_val))\nprint('Kridge:', np.sqrt(mean_squared_log_error(np.expm1(y_val), y_val_pred_kridge)))","046981b1":"d = {'y': np.expm1(y_val), 'XGB': y_val_pred_xgb, 'GBR': y_val_pred_gbr, 'LGB':  y_val_pred_lgb,\n     'Lasso': y_val_pred_lasso, 'ElasticNet': y_val_pred_elastic, 'BRidge': y_val_pred_bridge,\n     'SVR':  y_val_pred_svr, 'KRidge': y_val_pred_kridge}\n\nprediction_val = pd.DataFrame(data = d)\n\nprediction_val_x = prediction_val[['ElasticNet','BRidge','GBR','SVR']]\nprediction_val_y = prediction_val['y']\nprediction_val.head(10)","37aaeda6":"reg = LinearRegression()\nreg.fit(prediction_val_x, prediction_val_y)\nstacked_prediction = reg.predict(prediction_val_x)\nc = prediction_val_x.columns.values\nfor i in range(0, len(reg.coef_)):\n    print(np.round(reg.coef_[i], decimals = 3).astype(str) + ' x ' + c[i])\nprint('Score:' + np.sqrt(mean_squared_log_error(np.expm1(y_val), stacked_prediction)).astype(str))","cda79b2a":"plt.style.use('default') \nl = range(min(np.expm1(y_val).astype(int)), max(np.expm1(y_val).astype(int)))\nplt.figure(figsize = (10, 6))\nplt.plot(np.expm1(y_val), y_val_pred_elastic, color = 'aqua', marker = 'o', linestyle = 'None', alpha = 0.5, label = 'ElasticNet')\nplt.plot(np.expm1(y_val), y_val_pred_bridge, color = 'mediumpurple', marker = 'o', linestyle = 'None', alpha = 0.5, label = 'BRidge')\nplt.plot(np.expm1(y_val), y_val_pred_gbr, color = 'pink', marker = 'o', linestyle = 'None', alpha = 0.5, label = 'GBR')\nplt.plot(np.expm1(y_val), y_val_pred_svr, color = 'seagreen', marker = 'o', linestyle = 'None', alpha = 0.5, label = 'SVR')\nplt.plot(np.expm1(y_val), pd.Series(stacked_prediction), color = 'gray', marker = 'o', linestyle = 'None', markerfacecolor = 'white', label = 'Stack')\nplt.plot(l, l, color = 'gray', linestyle = 'dashed')\nplt.xlabel('SalePrice', fontsize = 16)\nplt.ylabel('Prediction', fontsize = 16)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\nplt.legend(fontsize = 12)\nplt.show()","b888d308":"xgb_model.fit(X_train_TM, y_log)\ny_test_pred_xgb = np.expm1(xgb_model.predict(X_test_TM))","3adcbbc4":"gbr_model.fit(X_train_TM, y_log)\ny_test_pred_gbr = np.expm1(gbr_model.predict(X_test_TM))","a3149948":"lgb_model.fit(X_train_TM, y_log)\ny_test_pred_lgb = np.expm1(lgb_model.predict(X_test_TM))","e06568c9":"lasso_model.fit(X_train_LM, y_log)\ny_test_pred_lasso = np.expm1(lasso_model.predict(X_test_LM))","b2899161":"elastic_model.fit(X_train_LM, y_log)\ny_test_pred_elastic = np.expm1(elastic_model.predict(X_test_LM))\n","a2bb6cf9":"bridge_model.fit(X_train_LM, y_log)\ny_test_pred_bridge = np.expm1(bridge_model.predict(X_test_LM))","8d67c2e0":"svr_model.fit(X_train_LM, y_log)\ny_test_pred_svr = np.expm1(svr_model.predict(X_test_LM))","da03d10c":"kridge_model.fit(X_train_LM2, y_log)\ny_test_pred_kridge = np.expm1(kridge_model.predict(X_test_LM2))","ee11c0f6":"av =(y_test_pred_xgb + y_test_pred_elastic + y_test_pred_kridge + y_test_pred_bridge + y_test_pred_gbr + y_test_pred_lgb +\\\n    y_test_pred_svr)\/7\npred_av = {'Id': df_test['Id'], 'SalePrice': av}\nprediction_av = pd.DataFrame(data = pred_av)\nprediction_av.head(5)","0e7cc662":"d = {'XGB': y_test_pred_xgb, 'GBR': y_test_pred_gbr, 'LGB':  y_test_pred_lgb, 'Lasso': y_test_pred_lasso,\n     'ElasticNet': y_test_pred_elastic, 'BRidge': y_test_pred_bridge, 'SVR':  y_test_pred_svr, 'KRidge': y_test_pred_kridge}\nprediction_test = pd.DataFrame(data = d)\nprediction_test.head()","c5ad5332":"prediction_test_x = prediction_test[['ElasticNet','BRidge','GBR','SVR']]\nprediction_test_x.head()","d11acba5":"stacked_prediction_test = reg.predict(prediction_test_x)\ndf_stacked_prediction_test = pd.DataFrame(data = {'Id': df_test['Id'], 'SalePrice': stacked_prediction_test})\ndf_stacked_prediction_test.head()","d9b0f731":"df_stacked_prediction_test.to_csv('mysubmission.csv', index = False)","c7733f6e":"There are a couple of features that have a numerical data type but might work better for some models as categorical features. The function num_to_cat performs these transformations. ","168785e8":"### Stack the models","56c4eea1":"The *score* dataframe will hold the score of the different models.","9b9c5058":"The scores for the RandomForest and LassoLarsIC models are considerably higher than the other models, which are all in the same range. Hence, I have removed these two models from the stacked prediction. ","190f6c44":"### Tree-Based Models","c7e04b67":"## Predict the target value for the test set","a4c145aa":"## Exploratory data analysis","2fbc4261":"I have built a dataframe with all the predictions. After some experimentation, I got the best score on the test set using the ElasticNet, Bridge, GBR and SVR models.","08ead422":"## Training","95441bb1":"This is my first Kaggel competition and my first data science project. My solution was inspired by what I have learned in the courses I took and other kernels:\n\nhttps:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n\nhttps:\/\/www.kaggle.com\/agehsbarg\/top-10-0-10943-stacking-mice-and-brutal-force\n\nMy goal was to learn and understand the process as much as possible, not necessarily to obtain a very high score.  \n\nWhat I have done differently (at least I have not seen this approach in the kernels that I have read) is to have model based feature engineering. \n\nI am looking forward to you suggestions on how to further improve this solution. ","0baa7296":"To see how the predictions of the different models work I used 70% of the training set to train the models and 30% to validate the predictions. I built a dataframe using all the predictions from the different models and used them as features to predict the target variable. Here a simple linear regression worked the best for me.  ","6030625e":"### Tree based models","131a8b9f":"Next, I have defined a function to plot for each feature the distribution of values in the training and test sets, the the sale price and the mean sale price as a function of each feature. For the categorical features I have just calculated the mean sale price for each category. For the continuous values I have created bins and calculated the mean sale price for each bin.   ","85f5ad5c":"After a lot of experimentation I tried a model based data transformation and that gave the best score for me. To be able to mix and match different transformations, I have defined a couple of functions that I could combine later. ","e6fa03bf":"Now I have all the necessary tools to start training the models. \n\nI improved my score by having  model based feature engineering:\n- using the Label Encoder instead of One Hot Encoder (OHE) for the tree based models (XGBoost, LGB, Random Forrest, GBR) made them much faster. However the linear models need the dummy features given by the OHE. \n- adding features improved the linear models but made the score of the tree based models worst.\n- scaling the features is beneficial for the linear models except the Kernel Ridge.\n\nBased on the above observations and additional experimentation I have grouped the models in three groups and transformed the datasets accordingly. ","9c36da26":"The add_features function creates additional features. This function was inspired by the kernels mentioned in the introduction.  ","e8a6f893":"The first function, clean_data fills the missing values and encodes the categorical feature. I did not use the label encoder because it would have used the alphabetical order for encoding and not the logical order (e.g. from Poor to Excellent). I think this is the way that most of the kernels I read cleaned the data sets. I chose not to concatenate the train and test data to avoid data leakage.  ","6b6d2197":"### Linear models","35db72d3":"### Score","92be3f15":"The function remove_columns removes unwanted features. Removing the features listed below gave the best score. ","8847b401":"#### Hyperparameter tuning","f2c3be59":"### Stack the models\n\nI have also tried to just average the 7 best performing models. This give a score of 0.11744 on the public leaderbord. ","b575f12f":"For some records the prediction overestimates the target value, for others it underestimates it. So it is reasonable to expect that stacking the models will give a better score than the score of the individual models. The LAsso and ElasticNet models give the same predictions so it does not make sense to add both of them to the stack.  The BayesianRidge and KernelRidge models also are very similar, so I will add  only one of them to the stack. ","30f219af":"I have tried to remove a different amount of outliers. The following gave the best score to me.  ","635818c1":"The add_polynomial function adds polynomial features to the datasets.","d7993944":"#### Feature processing","65680deb":"\n### Missing values","53793234":"### Kernel Ridge model","acab082b":"Conclusions from the EDA:\n\n- The good news is that the training and test sets have basically the same distribution for each of the features (some distributions seam to be narrower for the training set but that is just the effect of a different scale of the x axis). This means that a good model could predict the sale prices in the test set nicely. \n\n- Most of the features have a strong correlation with the sale price, however a few features seam to have no influence. \nOne feature, the 'Utilities' is clearly completely useless as it has only one unique value in the entire test set. \n\n- There are some clear outliers, houses with large living area that have sold for very little \n\n- The distribution of many of the continuous features are skewed so some of the models might benefit from a boxcox transformation. ","1de61218":"#### KernelRidge","33a345aa":"First I have removed the target variable from the training set, and transformed it. I removed the outliers from the target variable and concatenated the train and test sets. ","36fdf3a5":"## Stack the models","ad13035e":"The distribution of the SalePrice is skewed, it benefits from a log transformation. ","2a6f3115":"#### Hyperparameter tuning","e44f8542":"#### Linear Models","d86291f1":"I have also defined a function to transform skewed distributions. This function is also inspired by the kernels I have mentioned in the introduction.","3e364633":"I have used two types of encoding: One_hot_encoding for the linear models and label_encoding for the tree based models.","a06ae90e":"#### Feature processing","2855b46b":"## Functions for data cleaning and transformations","1ac3db85":" # House Prices: Advanced Regression Techniques","827a8512":"First I have checked the number of missing values in both datasets. As described in most of the kernels, there are features with many missing values, and they can be filled based on the description file. Although the test set has more features with missing values than the training set, the features with the most missing values are the same for both sets. ","cc714cfe":"#### Feature processing"}}