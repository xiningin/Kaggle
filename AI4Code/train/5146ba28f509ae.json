{"cell_type":{"89b066a7":"code","2b77f382":"code","d64272f4":"code","98657824":"code","8ae631b1":"code","493d80f3":"code","d9206107":"code","57bf01a5":"code","65df672e":"code","3a1febfc":"code","a009dc92":"code","564d5dff":"code","563ce042":"code","1722b56d":"code","f952d016":"code","47bc460c":"code","0f76e3ef":"code","174bd84c":"code","329a6e14":"code","6a8dd9d0":"code","91ad1bcd":"code","9bdf5935":"code","6da597b0":"code","8cb11355":"code","1dfcd610":"code","b259ab38":"code","92ac40eb":"code","b8cd55be":"code","58a22fcd":"code","9ed8613d":"code","227f2342":"code","484ef103":"code","469a7a5e":"code","3771303d":"code","9b6d6c09":"code","3c568380":"code","97974cd9":"code","5ba0f9eb":"code","f227d502":"code","e97476e0":"code","55b9b2d2":"code","25f97a54":"code","3c7aa12c":"code","76cf18b5":"code","07c1f538":"code","7dd32ade":"code","be6b88fc":"code","1f15c2ae":"code","542804fe":"code","ce202305":"code","5d5840fc":"code","728a3495":"code","469f8d73":"code","468eccf6":"code","8d665adf":"code","2751df25":"code","e2ce56d2":"code","78b383d9":"code","0a4c3768":"code","4b5db2ef":"code","6b9fdef9":"code","ed9c1452":"code","2b44e592":"code","683916dc":"code","aa902801":"code","bc0c8022":"code","76cb2194":"code","71a3e44c":"code","8264991e":"code","27bc2149":"code","6f3a10e0":"code","18af266c":"code","c825d783":"code","df4d1840":"code","88178721":"code","431271fb":"code","5db2f543":"code","120150b3":"code","5b0cbe3b":"code","97593dcd":"code","c03fb72c":"code","8363ce13":"code","75d51930":"code","77260265":"code","c98f0298":"code","bf21a245":"code","346818ea":"code","d586d683":"code","851e2b2a":"code","7e6b87af":"code","0d536bf4":"code","e100ebb3":"code","8cc3938a":"code","eb79a504":"code","99b7eb5a":"code","6a5fe844":"code","5f6d4a8f":"code","d1635175":"code","72361846":"code","e0342eb0":"code","1f6c2791":"code","dd419505":"code","6fa00c09":"code","915a7865":"code","814d7e4b":"code","6c2bc761":"markdown","9d628cad":"markdown","76c1c01d":"markdown","84585202":"markdown","74787446":"markdown","0dac0e5d":"markdown","d525c7b7":"markdown","905886ef":"markdown","7814639a":"markdown","6dfe14c9":"markdown","1690e874":"markdown","450d39fc":"markdown","7bc59ef2":"markdown","c8379364":"markdown","383badb2":"markdown","62703ef2":"markdown","cd43dc27":"markdown","b6c30d69":"markdown","3c882cc3":"markdown","04129a8e":"markdown","833fd957":"markdown","27340a1f":"markdown","844aa40a":"markdown","e1ab1ac1":"markdown","bfe33b7d":"markdown","58c7ba18":"markdown","0edbdd14":"markdown","23ace53f":"markdown","cc45cf02":"markdown","a2ff1f0e":"markdown","397c72f2":"markdown","1fa21a16":"markdown","f0d734dd":"markdown","de4b35bf":"markdown","42384f3a":"markdown","6a8bdc0f":"markdown","e93921ed":"markdown","8e6c9a2a":"markdown","506d47ba":"markdown","0d373730":"markdown","c77cc9c1":"markdown","46e2da1a":"markdown","a81135cb":"markdown","bfc8675f":"markdown","b2d68074":"markdown","20c3a302":"markdown","3e047bbf":"markdown","ea3fb601":"markdown","8881e9b8":"markdown","85011167":"markdown","e76bd477":"markdown","f7c313ed":"markdown","cce6be3b":"markdown"},"source":{"89b066a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2b77f382":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d64272f4":"data_train = pd.read_csv('..\/input\/train.csv')","98657824":"data_train.head()","8ae631b1":"numeric_var_names=[key for key in dict(data_train.dtypes) if dict(data_train.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\ncat_var_names=[key for key in dict(data_train.dtypes) if dict(data_train.dtypes)[key] in ['object']]\nprint(numeric_var_names)\nprint(cat_var_names)","493d80f3":"data_num = data_train[numeric_var_names]","d9206107":"data_num.info()","57bf01a5":"print(data_num.Age.mean())\nprint(data_num.Age.median())","65df672e":"data_num.Age = data_num.Age.fillna(data_num.Age.mean())","3a1febfc":"data_num.info()","a009dc92":"data_num.quantile(0.99)","564d5dff":"data_num.max()","563ce042":"def outlier_capping(x):\n    x = x.clip_upper(x.quantile(0.99))\n    x = x.clip_lower(x.quantile(0.01))\n    return x\n\ndata_num[['Age' ,'Fare']]=data_num[['Age' ,'Fare']].apply(lambda x: outlier_capping(x))","1722b56d":"# Use a general function that returns multiple values\ndef var_summary(x):\n    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  x.std(), x.var(), x.min(), x.dropna().quantile(0.01), x.dropna().quantile(0.05),x.dropna().quantile(0.10),x.dropna().quantile(0.25),x.dropna().quantile(0.50),x.dropna().quantile(0.75), x.dropna().quantile(0.90),x.dropna().quantile(0.95), x.dropna().quantile(0.99),x.max()], \n                  index=['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1' , 'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX'])\n\nnum_summary=data_num.apply(lambda x: var_summary(x)).T\nnum_summary","f952d016":"data_cat = data_train[cat_var_names]","47bc460c":"data_cat.head()","0f76e3ef":"data_cat = data_cat.drop(columns= ['Name'])","174bd84c":"data_cat.head()","329a6e14":"data_cat.info()","6a8dd9d0":"data_cat = data_cat.drop(columns= ['Cabin'])","91ad1bcd":"data_cat.Ticket = data_cat.Ticket.apply(lambda x: x.split(' ')[1] if x.find(' ')!= -1 else x )","9bdf5935":"data_cat.Ticket.head()","6da597b0":"data_cat.Ticket = data_cat.Ticket[data_cat.Ticket != 'LINE']\ndata_cat.Ticket = data_cat.Ticket[data_cat.Ticket != 'Basle']","8cb11355":"data_cat.Ticket = data_cat.Ticket[data_cat.Ticket != 'Basle']","1dfcd610":"data_cat.Ticket = pd.to_numeric(data_cat.Ticket)","b259ab38":"data_cat.head()","92ac40eb":"def create_dummies( df, colname ):\n    col_dummies = pd.get_dummies(df[colname], prefix=colname)\n    col_dummies.drop(col_dummies.columns[0], axis=1, inplace=True)\n    df = pd.concat([df, col_dummies], axis=1)\n    df.drop( colname, axis = 1, inplace = True )\n    return df","b8cd55be":"for c_feature in ['Sex', 'Embarked']:\n    data_cat[c_feature] = data_cat[c_feature].astype('category')\n    data_cat = create_dummies(data_cat , c_feature )","58a22fcd":"data_cat.head()","9ed8613d":"data_new = pd.concat([data_num, data_cat], axis =1)","227f2342":"data_new.head()","484ef103":"sns.boxplot?","469a7a5e":"fig,axes = plt.subplots(2,2,figsize=(16,10))\nsns.boxplot(x = data_new.Survived, y = data_new.Age,ax=axes[0,0] )\nsns.boxplot(x = data_new.Survived, y = data_new.Fare,ax=axes[0,1])\nsns.boxplot(x = data_new.Survived, y = data_new.Ticket,ax=axes[1,0])\nsns.boxplot(x = data_new.Survived, y = data_new.PassengerId,ax=axes[1,1])","3771303d":"cat_var_names = ['Pclass','SibSp','Parch','Sex_male','Embarked_Q','Embarked_S']","9b6d6c09":"for cat_variable in cat_var_names:\n    t =sns.catplot(data=data_new,kind='count',x='Survived',col=cat_variable)    ","3c568380":"X = data_new","97974cd9":"from sklearn.model_selection import train_test_split\nimport statsmodels.formula.api as sm\ntrain_features = X.columns.difference(['Survived'])\ntrain_X, test_X = train_test_split(X, test_size=0.3, random_state=42)\ntrain_X.columns","5ba0f9eb":"train_X.Survived.shape","f227d502":"train_features","e97476e0":"logreg = sm.logit(formula='Survived ~ ' + \"+\".join(train_features), data=train_X)\nresult = logreg.fit()","55b9b2d2":"result.summary2()","25f97a54":"my_formula='Survived ~ ' + \"+\".join(['Age', 'Embarked_S',\n       'Pclass', 'Sex_male', 'SibSp'])","3c7aa12c":"logreg = sm.logit(formula=my_formula, data=train_X)\nresult = logreg.fit()\nresult.summary2()","76cf18b5":"train_X.head(2)","07c1f538":"from sklearn import metrics\ntrain_gini = 2*metrics.roc_auc_score(train_X['Survived'], result.predict(train_X)) - 1\nprint(\"The Gini Index for the model built on the Train Data is : \", train_gini)\n\ntest_gini = 2*metrics.roc_auc_score(test_X['Survived'], result.predict(test_X)) - 1\nprint(\"The Gini Index for the model built on the Test Data is : \", test_gini)","7dd32ade":"#for train data\n## Intuition behind ROC curve - predicted probability as a tool for separating the '1's and '0's\ntrain_predicted_prob = pd.DataFrame(result.predict(train_X))\ntrain_predicted_prob.columns = ['prob']\ntrain_actual = train_X['Survived']\n# making a DataFrame with actual and prob columns\ntrain_predict = pd.concat([train_actual, train_predicted_prob], axis=1)\ntrain_predict.columns = ['actual','prob']\ntrain_predict.head()","be6b88fc":"#for test data\n## Intuition behind ROC curve - predicted probability as a tool for separating the '1's and '0's\ntest_predicted_prob = pd.DataFrame(result.predict(test_X))\ntest_predicted_prob.columns = ['prob']\ntest_actual = test_X['Survived']\n# making a DataFrame with actual and prob columns\ntest_predict = pd.concat([test_actual, test_predicted_prob], axis=1)\ntest_predict.columns = ['actual','prob']\ntest_predict.head()","1f15c2ae":"## Intuition behind ROC curve - confusion matrix for each different cut-off shows trade off in sensitivity and specificity\nroc_like_df = pd.DataFrame()\ntrain_temp = train_predict.copy()\n\nfor cut_off in np.linspace(0,1,50):\n    train_temp['cut_off'] = cut_off\n    train_temp['predicted'] = train_temp['prob'].apply(lambda x: 0.0 if x < cut_off else 1.0)\n    train_temp['tp'] = train_temp.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==1 else 0.0, axis=1)\n    train_temp['fp'] = train_temp.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==1 else 0.0, axis=1)\n    train_temp['tn'] = train_temp.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==0 else 0.0, axis=1)\n    train_temp['fn'] = train_temp.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==0 else 0.0, axis=1)\n    sensitivity = train_temp['tp'].sum() \/ (train_temp['tp'].sum() + train_temp['fn'].sum())\n    specificity = train_temp['tn'].sum() \/ (train_temp['tn'].sum() + train_temp['fp'].sum())\n    roc_like_table = pd.DataFrame([cut_off, sensitivity, specificity]).T\n    roc_like_table.columns = ['cutoff', 'sensitivity', 'specificity']\n    roc_like_df = pd.concat([roc_like_df, roc_like_table], axis=0)","542804fe":"roc_like_df.head()","ce202305":"## Finding ideal cut-off for checking if this remains same in OOS validation\nroc_like_df['total'] = roc_like_df['sensitivity'] + roc_like_df['specificity']","5d5840fc":"roc_like_df[roc_like_df['total']==roc_like_df['total'].max()]","728a3495":"test_predict['predicted'] = test_predict['prob'].apply(lambda x: 1 if x > 0.306122 else 0)\ntrain_predict['predicted'] = train_predict['prob'].apply(lambda x: 1 if x > 0.306122 else 0)\nsns.heatmap(pd.crosstab(train_predict['actual'], train_predict['predicted']), annot=True, fmt='.0f')\nplt.title('Train Data Confusion Matrix')\nplt.show()\nsns.heatmap(pd.crosstab(test_predict['actual'], test_predict['predicted']), annot=True, fmt='.0f')\nplt.title('Test Data Confusion Matrix')\nplt.show()","469f8d73":"print(\"The overall accuracy score for the Train Data is : \", metrics.accuracy_score(train_predict.actual, train_predict.predicted))\nprint(\"The overall accuracy score for the Test Data  is : \", metrics.accuracy_score(test_predict.actual, test_predict.predicted))","468eccf6":"train_predict.head()","8d665adf":"data_test = pd.read_csv('..\/input\/test.csv')","2751df25":"data_test.head()","e2ce56d2":"data_test.info()","78b383d9":"data_test.Age = data_num.Age.fillna(data_num.Age.mean())","0a4c3768":"data_test.Fare = data_num.Fare.fillna(data_num.Age.mean())","4b5db2ef":"def outlier_capping(x):\n    x = x.clip_upper(x.quantile(0.99))\n    x = x.clip_lower(x.quantile(0.01))\n    return x\n\ndata_test[['Age' ,'Fare']]=data_num[['Age' ,'Fare']].apply(lambda x: outlier_capping(x))","6b9fdef9":"data_test = data_test.drop(columns= ['Name'])","ed9c1452":"data_test = data_test.drop(columns= ['Cabin'])","2b44e592":"data_test.Ticket = data_test.Ticket.apply(lambda x: x.split(' ')[1] if x.find(' ')!= -1 else x )","683916dc":"data_test.Ticket = data_test.Ticket[data_test.Ticket != 'LINE']\ndata_test.Ticket = data_test.Ticket[data_test.Ticket != 'Basle']","aa902801":"def create_dummies( df, colname ):\n    col_dummies = pd.get_dummies(df[colname], prefix=colname)\n    col_dummies.drop(col_dummies.columns[0], axis=1, inplace=True)\n    df = pd.concat([df, col_dummies], axis=1)\n    df.drop( colname, axis = 1, inplace = True )\n    return df","bc0c8022":"for c_feature in ['Sex', 'Embarked']:\n    data_test[c_feature] = data_test[c_feature].astype('category')\n    data_test = create_dummies(data_test , c_feature )","76cb2194":"data_test.head()","71a3e44c":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier","8264991e":"data_new = data_new.dropna()","27bc2149":"X = data_new","6f3a10e0":"train_features = X.columns.difference(['Survived'])\ntrain_X, test_X = train_test_split(X, test_size=0.3, random_state=42)\ntrain_X.columns","18af266c":"train_y = train_X.Survived\ntest_y = test_X.Survived","c825d783":"train_X = train_X.drop(columns= ['Survived'])\ntest_X = test_X.drop(columns= ['Survived'])","df4d1840":"data_new.head()","88178721":"param_grid = {'min_samples_split':[5,6,7],\n              'n_estimators': [100,200,300],\n              'min_samples_leaf':[2,3,4,5],\n              'max_depth':[3,4,5,]\n              }","431271fb":"tree3 = GridSearchCV(RandomForestClassifier(), param_grid, cv = 5)\ntree3.fit( train_X,train_y )\nprint(tree3.best_score_)\nprint(tree3.best_params_)","5db2f543":"model_rf = RandomForestClassifier(min_samples_leaf=2, min_samples_split= 6, n_estimators= 300,max_depth = 5,oob_score=True)  #fine tuning the model\nmodel_rf.fit(train_X, train_y)","120150b3":"print('---------Accuracy score')\nprint ('Train Accuracy score:' , metrics.accuracy_score(model_rf.predict(train_X), train_y))\nprint ('Test Accuracy score:' , metrics.accuracy_score(model_rf.predict(test_X), test_y))\ntrain_gini = 2*metrics.roc_auc_score(train_y, model_rf.predict(train_X)) - 1\nprint('---------Gini score')\nprint(\"Train Gini score:\", train_gini)\n\ntest_gini = 2*metrics.roc_auc_score(test_y, model_rf.predict(test_X)) - 1\nprint(\"Test Gini score:\", test_gini)","5b0cbe3b":"from sklearn.ensemble import GradientBoostingClassifier","97593dcd":"data_new.head()","c03fb72c":"param_grid = {'learning_rate':[0.3,0.4],\n              'n_estimators':[50,60,70],\n              'min_samples_split':[4,5],\n              'max_depth':[2,3,4,5]\n    \n                }","8363ce13":"tree2 = GridSearchCV(GradientBoostingClassifier(), param_grid, cv = 5)\ntree2.fit( train_X, train_y )\nprint(tree2.best_score_)\nprint(tree2.best_params_)","75d51930":"model_gbm = GradientBoostingClassifier( learning_rate =0.3,n_estimators= 70,min_samples_split =4,max_depth = 3)  #fine tuning the model\n\nmodel_gbm.fit(train_X, train_y)","77260265":"print('---------Accuracy score')\nprint ('Train Accuracy score:' , metrics.accuracy_score(model_gbm.predict(train_X), train_y))\nprint ('Test Accuracy score:' , metrics.accuracy_score(model_gbm.predict(test_X), test_y))\ntrain_gini = 2*metrics.roc_auc_score(train_y, model_gbm.predict(train_X)) - 1\nprint('---------Gini score')\nprint(\"Train Gini score:\", train_gini)\n\ntest_gini = 2*metrics.roc_auc_score(test_y, model_gbm.predict(test_X)) - 1\nprint(\"Test Gini score:\", test_gini)","c98f0298":"from sklearn.neighbors import KNeighborsClassifier","bf21a245":"param_grid = {'n_neighbors': np.arange(1,99,2),\n              'p':[1,2]\n            }","346818ea":"tree2 = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\ntree2.fit( train_X, train_y )","d586d683":"print(tree2.best_score_)\nprint(tree2.best_params_)","851e2b2a":"model_knn = KNeighborsClassifier(n_neighbors=81,p=1)  #fine tuning the model\n\nmodel_knn.fit(train_X, train_y)","7e6b87af":"print('---------Accuracy score')\nprint ('Train Accuracy score:' , metrics.accuracy_score(model_knn.predict(train_X), train_y))\nprint ('Test Accuracy score:' , metrics.accuracy_score(model_knn.predict(test_X), test_y))\ntrain_gini = 2*metrics.roc_auc_score(train_y, model_knn.predict(train_X)) - 1\nprint('---------Gini score')\nprint(\"Train Gini score:\", train_gini)\n\ntest_gini = 2*metrics.roc_auc_score(test_y, model_knn.predict(test_X)) - 1\nprint(\"Test Gini score:\", test_gini)","0d536bf4":"import sklearn.naive_bayes as nb\nfrom sklearn.naive_bayes import MultinomialNB","e100ebb3":"model_nb = MultinomialNB(alpha= 0)\nmodel_nb.fit( train_X, train_y )","8cc3938a":"print('---------Accuracy score')\nprint ('Train Accuracy score:' , metrics.accuracy_score(model_nb.predict(train_X), train_y))\nprint ('Test Accuracy score:' , metrics.accuracy_score(model_nb.predict(test_X), test_y))\ntrain_gini = 2*metrics.roc_auc_score(train_y, model_nb.predict(train_X)) - 1\nprint('---------Gini score')\nprint(\"Train Gini score:\", train_gini)\n\ntest_gini = 2*metrics.roc_auc_score(test_y, model_nb.predict(test_X)) - 1\nprint(\"Test Gini score:\", test_gini)","eb79a504":"from sklearn.svm import SVC","99b7eb5a":"svc = SVC()\nparam_grid = {\n    'kernel': ['rbf'],\n    'C': [0.01,0.1, 10, 100, 1000,10000]}","6a5fe844":"svc= GridSearchCV(svc, param_grid, cv=5)\nsvc.fit(train_X, train_y)","5f6d4a8f":"print( svc.best_params_)\nprint(svc.best_score_)","d1635175":"model = SVC(C = 0.01, kernel= 'rbf')\nmodel.fit(train_X,train_y)","72361846":"print('---------Accuracy score')\nprint ('Train Accuracy score:' , metrics.accuracy_score(model.predict(train_X), train_y))\nprint ('Test Accuracy score:' , metrics.accuracy_score(model.predict(test_X), test_y))\ntrain_gini = 2*metrics.roc_auc_score(train_y, model.predict(train_X)) - 1\nprint('---------Gini score')\nprint(\"Train Gini score:\", train_gini)\n\ntest_gini = 2*metrics.roc_auc_score(test_y, model.predict(test_X)) - 1\nprint(\"Test Gini score:\", test_gini)","e0342eb0":"models = pd.DataFrame({'Model' :['LOGISTIC REGRESSION','RANDOM FOREST','GBM','KNN','Naive Bayes','SVM'],\n                       'Score':[ metrics.accuracy_score(test_predict.actual, test_predict.predicted),\n                                 metrics.accuracy_score(model_rf.predict(test_X), test_y),\n                                 metrics.accuracy_score(model_gbm.predict(test_X), test_y),\n                                 metrics.accuracy_score(model_knn.predict(test_X), test_y),\n                                 metrics.accuracy_score(model_nb.predict(test_X), test_y),\n                                 metrics.accuracy_score(model.predict(test_X), test_y),\n                                 ]})\nmodels.sort_values(by = 'Score',ascending=True)","1f6c2791":"data_test.head()","dd419505":"data_test['Survived'] = model_rf.predict(data_test)","6fa00c09":"data_test.head(10)","915a7865":"ids = data_test['PassengerId']\npredictions = model_rf.predict(data_test)\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","814d7e4b":"output","6c2bc761":"#### checking actual vs predicted values","9d628cad":"#### Parameters tuning","76c1c01d":"#### dropping variable CABIN as it has 75 % missing data","84585202":"#### Fine Tuning","74787446":"#### Testing Model Performance","0dac0e5d":"#### Importing training data","d525c7b7":"##### factor plots  to see if categorical variables can differ Survived and not surviving people","905886ef":"### Testing Model accuracy","7814639a":"#### final output","6dfe14c9":"##### Testing Model Performance","1690e874":"#### RANDOM FOREST","450d39fc":"### Predicting for test data now using RF model","7bc59ef2":"#### Removing unwanted characters from ticket variable to make it better usage for analysis","c8379364":"### Converting categorical variables to Dummy variables","383badb2":"#### train test split","62703ef2":"#### Based on the Testing Accuracy we can select Random Forest ( 81.2 % test accuracy) model as the best model for predicting Whether the passengers survived or not","cd43dc27":"### Now implementing  with ML algorithms","b6c30d69":"### GBM Model","3c882cc3":"#### Model fit","04129a8e":"#### removing errornous entries","833fd957":"### Getting data from test file for predicting later based on best Model","27340a1f":"### Now handling categorical data","844aa40a":"### Naive bayes","e1ab1ac1":"#### Converting categorical data to dummy variables","bfe33b7d":"### Splitting data for train and test","58c7ba18":"#### Checking model accuracy","0edbdd14":"#### Now finding the cut off based on ROC curve ( using just Logistic regression takes cut off default by 0.5 so this method is much better)","23ace53f":"##### cut off = 0.306","cc45cf02":"### Using KNN","a2ff1f0e":"### SVM","397c72f2":"##### Scatter plots to see if numerical variables can differ Survived and not survived people","1fa21a16":"#### Paramter Tuning","f0d734dd":"#### replacing missing data for Age with mean values","de4b35bf":"#### Missing value treatment","42384f3a":"#### capping data to 99 percentile","6a8bdc0f":"### Calculating Sensitivity and specificity for various Cut offs","e93921ed":"#### reading data from file","8e6c9a2a":"#### outlier data treatment","506d47ba":"### Testing Model Accuracy","0d373730":"#### Removing less important variables","c77cc9c1":"### Seprating continuous and categorical data","46e2da1a":"#### Exploratory Analysis on Data","a81135cb":"#### Creating Data SUmmary to better understand data","bfc8675f":"#### prediction using random forest model","b2d68074":"### Logistic regression","20c3a302":"#### missing value treatment","3e047bbf":"### Testing Model Accuracy","ea3fb601":"Submission File","8881e9b8":"#### Outliers treatment","85011167":"#### dropping ir relevant column NAME as passenger id is a more better alternative to it","e76bd477":"#### reading data from file","f7c313ed":"#### Now combining numerical and categorical data","cce6be3b":"### For Continuous Data"}}