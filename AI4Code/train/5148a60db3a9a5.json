{"cell_type":{"c7539d3f":"code","cd3e7a78":"code","cba97738":"code","7cdbeb49":"code","efd617c7":"code","3bd6b678":"code","3c1a96ac":"code","0a5b999b":"code","873c0dfc":"code","58259b6c":"code","d5dfc502":"code","322d83ab":"code","823453d3":"code","d38fe25c":"code","4a4cf080":"code","cb6e1bf2":"code","87d0d651":"code","1cc71726":"code","6a0acbb3":"code","3c832942":"code","d73d5629":"code","45b1579b":"markdown","f8a61d81":"markdown","a81f5c91":"markdown","b0cdf544":"markdown","15204531":"markdown","948ad5ab":"markdown","54a1ead3":"markdown","90b76664":"markdown","19893c9f":"markdown","13b36a86":"markdown","8455d314":"markdown","6c17bada":"markdown","a86920aa":"markdown","7b7e7cb5":"markdown","d9af9873":"markdown","f7f80140":"markdown","014f4553":"markdown","c7ec89d8":"markdown","5aa320f2":"markdown","091f0ceb":"markdown","e2c6b084":"markdown"},"source":{"c7539d3f":"import numpy as np\nimport matplotlib.pyplot as plt\nimport  pandas as pd","cd3e7a78":"dataset = pd.read_csv(\"..\/input\/soildataset\/soildataset.csv\")","cba97738":"dataset.shape","7cdbeb49":"dataset.head()","efd617c7":"dataset.info()","3bd6b678":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder\nNitrogen=pd.DataFrame()\nNitrogen['Value']=dataset['Nitrogen']\nNitrogen['Label_Nitrogen']=LabelEncoder().fit_transform(dataset['Content of Nitrogen'])\n\nPhosphorous=pd.DataFrame()\nPhosphorous['Value']=dataset['Phosphorous']\nPhosphorous['Label_Phosphorous']=LabelEncoder().fit_transform(dataset['Content of Phosphorous'])\n\nPotassium=pd.DataFrame()\nPotassium['Value']=dataset['Potassium']\nPotassium['Label_Potassium']=LabelEncoder().fit_transform(dataset['Content of Potassium'])\n","3c1a96ac":"X = dataset.iloc[:,:-3].values\ny = Nitrogen.iloc[:,-1].values","0a5b999b":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=0)","873c0dfc":"from sklearn.naive_bayes import MultinomialNB  #classifier is suitable for classification with discrete features\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)","58259b6c":"y_naive = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nac = accuracy_score(y_test, y_naive)\ncm = confusion_matrix(y_test, y_naive)\nprint('Accuracy for Naive Bayes is: ',ac)","d5dfc502":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=1)\n# Train the model on training data\nrf.fit(X_train, y_train)","322d83ab":"predictions = rf.predict(X_test)","823453d3":"rf.score(X_test,predictions)","d38fe25c":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std =sc.fit_transform(X_test)","4a4cf080":"from sklearn.tree import DecisionTreeClassifier\ny_train_std=y_train\nclassifier = DecisionTreeClassifier(criterion = 'entropy',random_state= 0)\nclassifier.fit(X_train_std,y_train_std)","cb6e1bf2":"y_pred = classifier.predict(X_test_std)","87d0d651":"from sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.multioutput import MultiOutputClassifier\n\ncm = confusion_matrix(y_test, y_pred)\n\nprint(\"Accuracy for Decision Tree:\",accuracy_score(y_test, y_pred))","1cc71726":"from sklearn import tree\ntext_representation = tree.export_text(classifier)\nprint(text_representation)","6a0acbb3":"from sklearn.svm import SVC\n\nsvm = SVC(kernel='rbf')\nsvm.fit(X_train_std, y_train)\nsvm_pred=svm.predict(X_test_std)","3c832942":"print('Accuracy for SVM is : {:.2f}'.format(svm.score(X_test_std, y_test)))","d73d5629":"from sklearn.metrics import classification_report\nprint(\"Decision Tree:\")\nprint(classification_report(y_test,y_pred))\nprint(\"Naive Bayes:\")\nprint(classification_report(y_test,y_naive))\nprint(\"Random Forest:\")\nprint(classification_report(y_test, predictions))\nprint(\"Support Vector Machine:\")\nprint(classification_report(y_test, svm_pred))\n","45b1579b":"# Classification Report","f8a61d81":"## Feature Scaling","a81f5c91":"## Tree Representation","b0cdf544":"## Data analysis","15204531":"## Preparing Data into Input Feature and Output Target Class","948ad5ab":"## Making the Confusion Matrix","54a1ead3":"## Importing the libraries","90b76664":"## Importing the dataset","19893c9f":"# Naive Bayes\n<font size=4>Based on the famous Bayes theorem for conditional probability, Na\u00efve bayes is used as a\nsupervised learning algorithm. Na\u00efve bayes assumes that the occurrence of other features and\nhence it is called Na\u00efve.<\/font>","13b36a86":"## Splitting the dataset into the Training set and Test set","8455d314":"<center><h1 style=\"font-family: cursive\"> Analysis of ML models for classifying NPK values<\/center>\n<center><img src=\"https:\/\/i.pinimg.com\/564x\/33\/16\/7e\/33167eefe42bb80940480a49e70aa4dc.jpg\" width='80%' height='1%'><\/center>","6c17bada":"<font size=4>Random Forest is meta estimator that fits a number of decision tree classifiers on various sub-\nsamples of the dataset and uses averaging to improve the predictive accuracy and control over-\nfitting<\/font>","a86920aa":"# Support Vector Machine","7b7e7cb5":"## Predicting the Test set results","d9af9873":"# Random Forest","f7f80140":"<font size=4>The dataset is split into training and testing data in the ratio of 8:2. The NPK values in the dataset were categorized as High, Medium, and Low. Therefore, we need to perform label encoding in which High has been labelled as 2, Medium as 1 and Low as 0.<\/font>","014f4553":"<font size=4>This consist of a set of supervised learning methods which are widely used for classification\noutlier detection and regression. In scikit.learn, SVM supports both dense and sparse sample\nvectors as input<\/font>","c7ec89d8":"**Highest**","5aa320f2":"# **Applying ML ALgorithms**","091f0ceb":"## Training the Decision Tree Classification model on the Training set","e2c6b084":"<font size=4> From the classification report above, it is concluded that Random Forest is the most suitable method for correctly classifying NPK values.<\/font>"}}