{"cell_type":{"264f7d95":"code","f6b3ce65":"code","f2429614":"code","033d4865":"code","1ac51a9d":"code","e737eee0":"code","462ce9af":"code","f11c965b":"code","4a8b6c79":"code","b9f2fd16":"code","5921e82a":"code","624879ab":"code","8b51998e":"code","74dff1c5":"code","9cec3d43":"code","c1b8ddae":"code","ec4238bd":"code","a633e580":"code","53dad36a":"code","5799ee61":"code","5c85063b":"code","81a69b16":"code","3e3ba72c":"code","f97a3c0f":"code","9555aad4":"code","c7ea0279":"code","80218c47":"code","e983789a":"code","e5faaf24":"code","eee6786f":"code","518aa882":"code","b7a1b2c4":"code","d6c5fc5f":"code","1a702d92":"code","cb9a03a6":"code","0e936809":"code","26a3e278":"code","b8bc750b":"code","f4de15ac":"code","91dcc11c":"code","0be2fc3f":"code","b76822e8":"code","cf0909d7":"code","6feaff67":"code","04006743":"code","64679038":"code","c312987a":"code","81b24123":"markdown","700756e3":"markdown","bc64202e":"markdown"},"source":{"264f7d95":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f6b3ce65":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf = pd.read_csv('..\/input\/california-housing-prices\/housing.csv')","f2429614":"df.head()","033d4865":"from pandas_profiling import ProfileReport as pp\nprofile = pp(\n    df, title=\"California House \", html={\"style\": {\"full_width\": True}}, sort=\"None\"\n)\n# The Notebook Widgets Interface\nprofile.to_widgets()\n# Or use the HTML report in an iframe\nprofile","1ac51a9d":"df.info()","e737eee0":"import matplotlib.pyplot as plt\ndf.hist(bins=50, figsize=(15, 12))\nplt.show()","462ce9af":"df['income_cat'] = pd.cut(df['median_income'], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])\ndf['income_cat'].hist()","f11c965b":"sns.distplot((df['median_income']))","4a8b6c79":"sns.distplot(df['median_house_value'])","b9f2fd16":"q = df[\"median_house_value\"].quantile(0.96)\ndf = df[df[\"median_house_value\"]<q]","5921e82a":"sns.distplot((df['housing_median_age']))","624879ab":"f, axes = plt.subplots(1, 2,figsize=(14,6))\nsns.distplot(np.sqrt(df['total_rooms']),ax=axes[0])\nsns.boxplot(df['total_rooms'],ax=axes[1])","8b51998e":"df['total_rooms'] = np.sqrt(df['total_rooms'])\nq = df[\"total_rooms\"].quantile(0.98)\ndf = df[df[\"total_rooms\"]<q]","74dff1c5":"f, axes = plt.subplots(1, 2,figsize=(14,6))\nsns.distplot(np.sqrt(df['total_bedrooms']),ax=axes[0])\nsns.boxplot(df['total_bedrooms'],ax=axes[1])","9cec3d43":"df['total_bedrooms'] = np.sqrt(df['total_bedrooms'])\nq = df[\"total_bedrooms\"].quantile(0.95)\ndf = df[df[\"total_bedrooms\"]<q]","c1b8ddae":"sns.distplot(df['median_income'])","ec4238bd":"q = df[\"median_income\"].quantile(0.98)\ndf = df[df[\"median_income\"]<q]","a633e580":"f, axes = plt.subplots(1, 2,figsize=(14,6))\nsns.distplot(np.sqrt(df['population']),ax=axes[0])\nsns.boxplot(df['population'],ax=axes[1])","53dad36a":"df['population'] = np.sqrt(df['population'])\nq = df[\"population\"].quantile(0.98)\ndf = df[df[\"population\"]<q]","5799ee61":"f, axes = plt.subplots(1, 2,figsize=(14,6))\nsns.distplot(np.sqrt(df['households']),ax=axes[0])\nsns.boxplot(df['households'],ax=axes[1])","5c85063b":"df['households'] = np.sqrt(df['households'])\nq = df[\"households\"].quantile(0.98)\ndf = df[df[\"households\"]<q]","81a69b16":"df.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4, s=df['population']\/100, label='population',\nfigsize=(12, 8), c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)\nplt.title(\"Population Density\")\nplt.legend()\nplt.show()","3e3ba72c":"print(df['ocean_proximity'].value_counts())\nsns.countplot(df['ocean_proximity'])","f97a3c0f":"ocean_ohe = pd.get_dummies(df['ocean_proximity'],drop_first=True)\nocean_ohe","9555aad4":"data = pd.concat([df,ocean_ohe],axis=1)\ndata.head()","c7ea0279":"data = data.drop(['longitude','latitude','ocean_proximity'],1)\ndata.head()","80218c47":"plt.figure(figsize=(12,12))\nsns.heatmap(data.corr(),annot=True,cbar=False)","e983789a":"from sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\ncol = ['housing_median_age','total_rooms','total_bedrooms','population','households','median_income','median_house_value']\nfor i in col:\n    data[[i]] = std.fit_transform(data[[i]]) ","e5faaf24":"data[\"rooms_per_household\"] = data[\"total_rooms\"]\/data[\"households\"]\ndata[\"bedrooms_per_room\"] = data[\"total_bedrooms\"]\/data[\"total_rooms\"]\ndata[\"population_per_household\"] = data[\"population\"]\/data[\"households\"]","eee6786f":"data.head()","518aa882":"data['income_cat'] = data['income_cat'].astype('int')","b7a1b2c4":"X = data.drop(['median_house_value'],1)\ny = data['median_house_value']","d6c5fc5f":"print(X.shape)\nprint(y.shape)","1a702d92":"from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\ndef training_model(model):\n    X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n    model.fit(X_train,y_train)\n    print(\"RMSE ==> \",np.sqrt(mean_squared_error(y_train,model.predict(X_train))))\n    print(\"RMSE ==> \",np.sqrt(mean_squared_error(y_test,model.predict(X_test))))\n        \n        \n    \n    ","cb9a03a6":"#LINEAR REGRESSION\nfrom sklearn.linear_model import LinearRegression\nln = LinearRegression()\ntraining_model(ln)","0e936809":"#RANDOM FOREST REGRESSOR\nfrom sklearn.ensemble import RandomForestRegressor\nalgo_reg = RandomForestRegressor(n_estimators=5, random_state=42)\ntraining_model(algo_reg)","26a3e278":"## ADABOOST REGRESSOR\nfrom sklearn.ensemble import AdaBoostRegressor\nalgo_ada = AdaBoostRegressor(n_estimators=5, random_state=42)\ntraining_model(algo_ada)","b8bc750b":"#GRADIENT BOOSTING REGRESSOR\nfrom sklearn.ensemble import GradientBoostingRegressor\nalgo_gb = GradientBoostingRegressor(n_estimators=5, random_state=42)\ntraining_model(algo_gb)","f4de15ac":"## DECISION TREE REGRESSOR\nfrom sklearn.tree import DecisionTreeRegressor\n\nalgo_dec = DecisionTreeRegressor(random_state=42)\ntraining_model(algo_dec)","91dcc11c":"X.info()","0be2fc3f":"## XGBOOST\nimport xgboost\nxgb = xgboost.XGBRFRegressor()\ntraining_model(xgb)","b76822e8":"## LIGHTGBM\nimport lightgbm\nlgb = lightgbm.LGBMRegressor()\ntraining_model(lgb)","cf0909d7":"## CATBOOST\nimport catboost\ncb = catboost.CatBoostRegressor()\ntraining_model(cb)","6feaff67":"## USING NEURAL NETWORK\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom math import sqrt\nfrom sklearn import preprocessing\n\nmodel = keras.Sequential([keras.layers.Dense(512, input_dim = X.shape[1], kernel_initializer=\"normal\", activation=\"relu\"),\n                          keras.layers.Dense(512, kernel_initializer=\"normal\", activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l1(0.1)),\n                          keras.layers.Dense(256, kernel_initializer=\"normal\", activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l1(0.1)),\n                          keras.layers.Dense(128, kernel_initializer=\"normal\", activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l1(0.1)),\n                          keras.layers.Dense(64, kernel_initializer=\"normal\", activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l1(0.1)),\n                          keras.layers.Dense(32, kernel_initializer=\"normal\", activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l1(0.1)),\n                          keras.layers.Dense(1, kernel_initializer=\"normal\", activation=\"linear\")])\n\nmodel.compile(loss=\"mean_absolute_error\", optimizer=\"adam\", metrics=[\"mean_absolute_error\"])","04006743":"history = model.fit(X,y, epochs=30, validation_split=0.25, shuffle=True)","64679038":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n","c312987a":"## WITH SLIGHTLY MOR HYPERPARAMETER TUNING OF CATBOOST MODEL , WE CAN SCORE MORE.","81b24123":"We see that most of data is left-skewed.We will try to make it non-skewed.","700756e3":"FEATURE ENGINEERING","bc64202e":"The Random Forest is overfitting, hence we need to do certain parameter tuning."}}