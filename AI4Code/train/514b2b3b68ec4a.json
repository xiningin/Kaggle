{"cell_type":{"f3d05c12":"code","440c0060":"code","a11fb0d7":"code","45729a46":"code","d925aab4":"code","9883bf94":"code","9c1ac568":"code","58015550":"code","18073c1c":"code","882dea6b":"code","f2b404d6":"code","a3f49993":"code","0f7b3f42":"code","c2fd71be":"code","1893526f":"code","19c6ef94":"code","d35f26bf":"code","b4792ced":"code","30b14dae":"code","9591e6a1":"code","910947a6":"code","39bb66f6":"code","51695032":"code","8e121c32":"code","c1073e09":"code","b3287d8a":"code","9a9fdc9a":"code","a899550a":"code","907cad38":"code","3c197fce":"code","56a8100f":"code","dedb80a0":"code","6f47c1a0":"code","1bb4c49d":"code","43ef163f":"code","55aab083":"code","9084ed87":"markdown","5ae78cc1":"markdown","28213677":"markdown","a0c3ae59":"markdown","3c59046e":"markdown","c532edf5":"markdown","927bde59":"markdown","b0cf33cc":"markdown","26a8c6cc":"markdown","69ca709f":"markdown","e2de09b1":"markdown","64a8c170":"markdown","9e1db2ba":"markdown","05dfb826":"markdown","383061d7":"markdown","a161c786":"markdown","c0338212":"markdown","72eb07bc":"markdown","1caa0518":"markdown","9189d161":"markdown","a6f1e101":"markdown","aea40fa6":"markdown","fec2c55f":"markdown","ca084f56":"markdown","ac092062":"markdown","ffd6c766":"markdown","6a71dd78":"markdown","3106f56b":"markdown","7cc98715":"markdown","2e388988":"markdown","062f5cae":"markdown","0732e0bd":"markdown","00a6d9a4":"markdown","6f198f2d":"markdown","cac6e115":"markdown"},"source":{"f3d05c12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap #Sonu\u00e7 g\u00f6rselle\u015ftirme\n#Libraries for ML\nfrom sklearn.preprocessing import RobustScaler #Standardizasyon i\u00e7in\nfrom sklearn.model_selection import train_test_split, GridSearchCV #GridSearchCV: KNN ile ilgili en iyi parametreleri belirlemek\nfrom sklearn.metrics import accuracy_score ,confusion_matrix #Sonu\u00e7 de\u011ferlendirme\nfrom sklearn.neighbors import KNeighborsClassifier,  LocalOutlierFactor #Trainin algoritmas\u0131 ve NCA ve Outlier de\u011ferler i\u00e7in\nfrom sklearn.decomposition import PCA #PCA i\u00e7in\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n\n#Others\nimport warnings\nwarnings.filterwarnings('ignore') #Uyar\u0131lar\u0131 kapatmak\n\n","440c0060":"#read data\ndata = pd.read_csv(\"..\/input\/heart.csv\")","a11fb0d7":"data.info()","45729a46":"print(len(data)) #data uzunlu\u011fu \nprint('Data Shape',data.shape)","d925aab4":"describe = data.describe() #Statsitiksel de\u011ferlerin tespiti\nprint(describe.T)","9883bf94":"data.tail()\n","9c1ac568":"sns.countplot(data['target']) \nprint(data.target.value_counts())","58015550":"corr_data = data.corr() # Datada bulunan numerik de\u011ferler aras\u0131ndaki korelasyona bakar.\nsns.clustermap(corr_data,annot= True,fmt = '.2f')\n#annot grafik \u00fcst\u00fcndeki say\u0131sal de\u011ferleri g\u00f6steririken fmt ise virg\u00fclden sonra ka\u00e7 basamak g\u00f6sterilecek bunu belirler.\nplt.title('Correlation Between Features')\nplt.show();","18073c1c":"threshold = 0.28 #Bu e\u015fik de\u011feri ile sadece bu de\u011ferin \u00fcst\u00fcndeki korelasyonlar\u0131 de\u011ferlendirece\u011fiz yeni grafikte\n\nfiltre = np.abs(corr_data['target']) > threshold # Burada corelasyon de\u011ferleri negatifde olaca\u011f\u0131ndan mutlak de\u011ferini al\u0131p tresholddan b\u00fcy\u00fcklar\u0131 filtreledik.\ncorr_feature = corr_data.columns[filtre].tolist()\n#Bu de\u011fi\u015fkene ise korelasyon matrisi s\u00fctunlar\u0131na filtrenin uygulanmas\u0131yla \u00e7\u0131kan \u00e7\u0131kt\u0131lar\u0131 listeye \u00e7evirip atad\u0131k.\n\nsns.clustermap(data[corr_feature].corr(),annot= True,fmt = '.2f')\n#Buradan e\u015fik de\u011ferine uygun olarak elde edilen featurelar\u0131n uyguland\u0131\u011f\u0131 corr matr. olu\u015fturduk.\n\nplt.title('Correlation Between Features with threshold 0.28')\nplt.show();","882dea6b":"#Box p. \u00f6ncesi bir melted i\u015flemi gerekitor.\ndata_melted = pd.melt(data,id_vars='target',\n                      var_name='Features',\n                      value_name='Value')\n\nplt.figure()\nsns.boxplot(x='Features',y='Value',hue='target',data=data_melted) #Featureslar target'a g\u00f6re ayr\u0131ld\u0131.\nplt.xticks(rotation=75) #Feature isimleri 90 derece dik g\u00f6r\u00fclecek.\nplt.show()","f2b404d6":"\nsns.pairplot(data[corr_feature],diag_kind='kde',markers='+',hue='target')\nplt.show()\n","a3f49993":"x = data.drop(['target'],axis=1) \ny = data.target\ncolumns = x.columns.tolist() # Featurelar\u0131n isimlerini bir listede toplad\u0131k.","0f7b3f42":"test_size = 0.2\nx = RobustScaler().fit_transform(x)\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=test_size,random_state=42)","c2fd71be":"\nx_train_df = pd.DataFrame(x_train,columns=columns)\nx_train_df_describe = x_train_df.describe()\nx_train_df['target'] = y_train\n\n#Box p. \u00f6ncesi bir melted i\u015flemi gerekitor.\ndata_melted = pd.melt(x_train_df,id_vars='target',\n                      var_name='Features',\n                      value_name='Value')\n\nplt.figure()\nsns.boxplot(x='Features',y='Value',hue='target',data=data_melted) #Featureslar target'a g\u00f6re ayr\u0131ld\u0131.\nplt.xticks(rotation=75) #Feature isimleri 90 derece dik g\u00f6r\u00fclecek.\nplt.show()","1893526f":"\nsns.pairplot(x_train_df[corr_feature],diag_kind='kde',markers='+',hue='target')\nplt.show()","19c6ef94":"random_state = 42\nknn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(x_train, y_train) #Calculation (In the supervise learning this section = training)\ny_predict = knn.predict(x_test) #Test Section\ncm = confusion_matrix(y_test, y_predict) #Plooting\nacc = accuracy_score(y_test, y_predict) #Accuracy Score\nscore = knn.score(x_test, y_test) #acc ile buras\u0131n\u0131n sonucu ayn\u0131 \u00e7\u0131kacak. Do\u011frulama ama\u00e7l\u0131 yap\u0131l\u0131yor.\n\nprint(\"Score:\",score)\nprint(\"CM:\",cm)\nprint(\"Basic KNN Acc:\",acc)","d35f26bf":"def KNN_best_parameters(x_train,x_test,y_train,y_test):\n    \n    k_range = list(range(1,51)) #En uygun k de\u011feri buluma\n    weight_options = ['uniform','distance'] #En uygun weighti buluma\n    #manhattan_distance = 1\n    #euclidean_distance = 2\n    distance_options = [1,2] #En uygun distance type buluma\n    print()\n    param_grid = dict(n_neighbors=k_range,weights=weight_options,p=distance_options) #Aranacak parametreleri bir s\u00f6zl\u00fckte toplad\u0131k.\n\n    knn =KNeighborsClassifier() #Parametrelerin denenece\u011fi knn olu\u015fturuldu.\n    grid = GridSearchCV(knn,param_grid,cv=10,scoring='accuracy') #Parametrelerin aranmas\u0131 i\u00e7in method\n    grid.fit(x_train, y_train) #fitting ile best parm. elde edildi\n    \n    print('Best training score: {} with parametres: {}'.format(grid.best_score_,grid.best_params_))\n    print()\n    \n    knn = KNeighborsClassifier(**grid.best_params_) #Test setinde deneme i\u015flemi i\u00e7in\n    knn.fit(x_train, y_train)\n    \n    y_predict_test = knn.predict(x_test)\n    y_predict_train = knn.predict(x_train)\n\n    cm_test = confusion_matrix(y_test,y_predict_test)\n    cm_train = confusion_matrix(y_train,y_predict_train)\n\n    acc_test = accuracy_score(y_test,y_predict_test)  \n    acc_train = accuracy_score(y_train,y_predict_train)\n\n    print('Test Score: {}, Train Score: {}'.format(acc_test,acc_train))\n    print()\n    print('CM Test:',cm_test)\n    print('CM Train:',cm_train)\n    \n    return grid\n\ngrid_knn_bestparam = KNN_best_parameters(x_train,x_test,y_train,y_test)","b4792ced":"knn_score=grid_knn_bestparam.score(x_test,y_test)","30b14dae":"scaler = RobustScaler()\nx_scaled = scaler.fit_transform(x) #x verisi b\u00f6l\u00fcnmeden tam bir \u015fekilde PCA i\u00e7in scale edildi. \n\npca = PCA(n_components=2) #2 componentli bir PCA olu\u015fturduk.\npca.fit(x_scaled)\nx_reduce_pca = pca.transform(x_scaled) #2feature'a yani boyuta d\u00fc\u015f\u00fcr\u00fclm\u00fc\u015f x \npca_data =pd.DataFrame(x_reduce_pca,columns=['p1','p2']) #reduce datadan incelemek i\u00e7in bir dataframe olu\u015fturuldu\npca_data['target'] = y #buna target eklendi. G\u00f6rselle\u015ftirmek i\u00e7in gerekli.\n\nsns.scatterplot(x='p1',y='p2',hue='target',data=pca_data) # targeta g\u00f6re renklendirilmi\u015f grafik\nplt.title('PCA: P1 Vs P2')","9591e6a1":"x_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(x_reduce_pca,y,test_size=test_size,random_state=42)\n\ngrid_pca = KNN_best_parameters(x_train_pca, x_test_pca, y_train_pca, y_test_pca)\n#en iyi parametreleri elde etti\u011fimiz metodu PCA i\u00e7in \u00e7al\u0131\u015ft\u0131r\u0131yorum.","910947a6":"cmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .05 # step size in the mesh\nX = x_reduce_pca\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights = '%s')\"\n          % (len(np.unique(y)),grid_pca.best_estimator_.n_neighbors, grid_pca.best_estimator_.weights))\n","39bb66f6":"svc = SVC()\nsvc.fit(x_train,y_train)  #learning \n#SVM Test \n\nSVMscore_test = svc.score(x_test,y_test)\nSVMscore_train = svc.score(x_train,y_train)\nprint (\"SVM Test Accuracy:\", svc.score(x_test,y_test))\n\nprint (\"SVM Train Accuracy:\", svc.score(x_train,y_train))","51695032":"yprediciton2= svc.predict(x_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton2)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()","8e121c32":"dt = DecisionTreeClassifier(random_state=random_state,max_depth=4)\n\ndt.fit(x_train,y_train) #learning\n#prediciton\nDTCscore_test = dt.score(x_test,y_test)\nDTCscore_train = dt.score(x_train,y_train)\nprint(\"Decision Tree Test Score: \",dt.score(x_test,y_test))\nprint(\"Decision Tree Train Score: \",dt.score(x_train,y_train))","c1073e09":"yprediciton2= dt.predict(x_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton2)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()","b3287d8a":"scores = []\nfor each in range(1,50):\n    rf = RandomForestClassifier(n_estimators = each,random_state=random_state,max_depth=3)\n    rf.fit(x_train,y_train)\n    scores.append(rf.score(x_test,y_test))\n    \nplt.figure(1, figsize=(10, 5))\nplt.plot(range(1,50),scores,color=\"black\",linewidth=2)\nplt.title(\"Optimum N Estimator Value\")\nplt.xlabel(\"N Estimators\")\nplt.ylabel(\"Score(Accuracy)\")\nplt.grid(True)\nplt.show()","9a9fdc9a":"rf= RandomForestClassifier(n_estimators = 19, random_state=random_state,max_depth=3) #n_estimator = DT\nrf.fit(x_train,y_train) # learning\nRFCscore=rf.score(x_test,y_test)\n\nprint(\"Random Forest Test Score: \",rf.score(x_test,y_test))\nprint(\"Random Forest Train Score: \",rf.score(x_train,y_train))","a899550a":"yprediciton2= rf.predict(x_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton2)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()","907cad38":"scores = []\nfor each in range(1,10):\n    ada = AdaBoostClassifier(n_estimators = each,random_state=random_state)\n    ada.fit(x_train,y_train)\n    scores.append(ada.score(x_test,y_test))\n    \nplt.figure(1, figsize=(10, 5))\nplt.plot(range(1,10),scores,color=\"black\",linewidth=2)\nplt.title(\"Optimum N Estimator Value\")\nplt.xlabel(\"N Estimators\")\nplt.ylabel(\"Score(Accuracy)\")\nplt.grid(True)\nplt.show()","3c197fce":"ada = AdaBoostClassifier(base_estimator=dt,n_estimators=3,random_state=random_state,learning_rate=0.001)\nada.fit(x_train,y_train) # learning\nAdascore=ada.score(x_test,y_test)\n\nprint(\"AdaBoost Test Score: \",ada.score(x_test,y_test))\nprint(\"AdaBoost Train Score: \",ada.score(x_train,y_train))","56a8100f":"yprediciton2= ada.predict(x_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton2)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()","dedb80a0":"#LR with sklearn\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(x_train,y_train)\nprint(\"LR Test Accuracy {}\".format(LR.score(x_test,y_test)))\nprint(\"LR Train Accuracy {}\".format(LR.score(x_train,y_train)))\nLRscore =LR.score(x_test,y_test)","6f47c1a0":"yprediciton2= LR.predict(x_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton2)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()","1bb4c49d":"v1 = VotingClassifier(estimators = [('svc',svc),('knn',knn),('dt',dt),('rf',rf),('ada',ada)])\nv1.fit(x_train,y_train) # learning\nvooting1=v1.score(x_test,y_test)\n\nprint(\"Voting C. Test Score: \",v1.score(x_test,y_test))\nprint(\"Voting C. Train Score: \",v1.score(x_train,y_train))","43ef163f":"v2= VotingClassifier(estimators = [('svc',svc),('knn',knn),('rf',rf)])\nv2.fit(x_train,y_train) # learning\nvooting2=v2.score(x_test,y_test)\n\nprint(\"Voting C. Test Score: \",v2.score(x_test,y_test))\nprint(\"Voting C. Train Score: \",v2.score(x_train,y_train))","55aab083":"scores=[SVMscore_test,DTCscore_test,knn_score,RFCscore,Adascore,LRscore,vooting1,vooting2]\nAlgorthmsName=[\"SVM\",\"Decision Tree\",\"K-NN\",\"Random Forest\",\"Adaboost\",\"Logistic Regression\", 'Voting Classifier -1-','Voting Classifier-2-']\n\n#create traces\n\ntrace1 = go.Scatter(\n    x = AlgorthmsName,\n    y= scores,\n    name='Algortms Name',\n    marker =dict(color='rgba(0,255,0,0.5)',\n               line =dict(color='rgb(0,0,0)',width=2)),\n                text=AlgorthmsName\n)\ndata = [trace1]\n\nlayout = go.Layout(barmode = \"group\",\n                  xaxis= dict(title= 'ML Algorithms',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Prediction Scores',ticklen= 5,zeroline= False))\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","9084ed87":"### Random Forest Classifier","5ae78cc1":"### Support vector classifier","28213677":"### Confusion Matrix","a0c3ae59":"### KNN fit towards PCA outputs","3c59046e":"## Standardization & Train - Tespt Split ","c532edf5":"### Optimum N Estimator Value for Random Forest","927bde59":"### Box Plotting before Standardization","b0cf33cc":"### Optimum N Estimator Value for Adaboost","26a8c6cc":"### Voting Classifiers","69ca709f":"### Principal component analysis","e2de09b1":"### Pair plotting after standardization","64a8c170":"\nHello, I'm going to preciton with Logistic regression KNN Classification SVM Decision Tree,adaboost, voting classifier and Random Forest. I will use Heart Disease UCI dataset. If you wonder anything about dateset,you can read here. (https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci)","9e1db2ba":"### Confusion Matrix","05dfb826":"### Confusion Matrix","383061d7":"## Data Reading","a161c786":"### Pair Plotting before Standardization","c0338212":"## K-nearest neighbors classifier ","72eb07bc":"# ML Classification\n\n----Content\n\n1-Import Dataset\n\n2-Investigation Dataset\n\n3-Visualizaition Dataset\n\n4-ML Algoritms importation and predicition\n\n5-Visualizaition of results\n\n6-Conclusion","1caa0518":"### Confusion Matrix","9189d161":"### Confusion Matrix","a6f1e101":"### Pre-processing before standardization","aea40fa6":"# Conclusion\n\n1- Thank you for investigation my kernel.\n\n2- I compared ML algorithms with Heart Disease Dataset.\n\n3- I found optimum value by aid of for loop.\n\n4- Finally, I obtained the same score values.\n\n5- I expect your opinion and criticism.\n# If you like this kernel, Please Upvote :) Thanks\n\n<img src=\"https:\/\/media.giphy.com\/media\/1oF1KAEYvmXBMo6uTS\/giphy.gif\" width=\"500px\">","fec2c55f":"### Wrong classification in PCA_KNN","ca084f56":"### Correlation Matrix of Features","ac092062":"## Exploratory Data Analysis","ffd6c766":"### Box plotting after standardization","6a71dd78":"### Choose Best Parameters with GridSearchCrossValidation ","3106f56b":"### Decision Tree Classifier","7cc98715":"## Evalutaion of Results","2e388988":"### Logistic Regression","062f5cae":"## Target Visualization\n\n* The \"goal\" field refers to the presence of heart disease in the patient. Target = 1 => presence of heart disease Target = 0 => no of heart disease","0732e0bd":"### Correlation Matrix of Features (with Threshold)","00a6d9a4":"# Libraries","6f198f2d":"### AdaBoost Classifier","cac6e115":"# Introduction\n"}}