{"cell_type":{"ac8e5375":"code","795f23b2":"code","679073ba":"code","a33c1ef6":"code","318e02ce":"code","c6b5801e":"code","630ccfc8":"code","6ecb3d42":"code","9b3c4e59":"code","16d9e5fe":"code","11aec5fd":"code","e69591d2":"code","c656cad9":"code","c8c2fb4a":"code","6bb4065c":"code","ea7b8364":"code","6d049396":"code","fe932e8c":"code","ffa3d188":"code","69613e18":"code","7cd27732":"code","a1c5aff8":"code","f575f0b6":"markdown","ede157af":"markdown","d8dbc474":"markdown","41d9528f":"markdown","336120ee":"markdown","9e5bd779":"markdown","f06bbd2c":"markdown","5c307ee6":"markdown","ad62f64f":"markdown","f059486c":"markdown","aa613d0e":"markdown","9c3a48e8":"markdown","de6fc943":"markdown","1852f1ac":"markdown","8e5044c4":"markdown","23854ddf":"markdown","060c7f7e":"markdown","11f59cf5":"markdown"},"source":{"ac8e5375":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, KFold, cross_val_score\nfrom sklearn.metrics import classification_report, accuracy_score\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","795f23b2":"train_df = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/learn-together\/test.csv\")","679073ba":"train_df.shape, test_df.shape","a33c1ef6":"train_df.head()","318e02ce":"train_df.dtypes.unique()","c6b5801e":"target_name = \"Cover_Type\"\nfeature_names = [n for n in train_df.columns.values if n != target_name and n != \"Id\"]","630ccfc8":"print(\"Missing values in training data, target column?\", train_df[target_name].isnull().any().sum())\nprint(\"Missing values in training data, feature columns?\", train_df[feature_names].isnull().any().sum())\nprint(\"Missing values in test data?\", test_df.isnull().any().sum())","6ecb3d42":"len(train_df.select_dtypes(include=[\"object\"]).columns)","9b3c4e59":"w = train_df[[\"Wilderness_Area1\", \"Wilderness_Area2\", \"Wilderness_Area3\", \"Wilderness_Area4\", \"Cover_Type\"]].groupby([\"Cover_Type\"]).sum()\nw_percentages = w\/w.sum()\nw_percentages","16d9e5fe":"labels = [\"Wilderness_Area1\", \"Wilderness_Area2\", \"Wilderness_Area3\", \"Wilderness_Area4\"]\ncolors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\nx = np.arange(len(w_percentages.columns.values))\n\nplt.figure(figsize=(16,8))\n\nfor i in range(len(w_percentages.index.values)):\n    if i == 0:\n        running_sum = 0\n    else:\n        running_sum += w_percentages.iloc[i-1,:]\n    plt.bar(x, w_percentages.iloc[i,:], color=colors[i], bottom=running_sum)\n\nplt.xticks(x, labels);\n\nplt.legend([\"1 - Spruce\/Fir\",\n\"2 - Lodgepole Pine\",\n\"3 - Ponderosa Pine\",\n\"4 - Cottonwood\/Willow\",\n\"5 - Aspen\",\n\"6 - Douglas-fir\",\n\"7 - Krummholz\"], loc=\"center\", bbox_to_anchor=(0.5,-0.2), ncol=2);","11aec5fd":"plt.figure(figsize=(16,8))\nfor i in range(1, 7):\n    sns.distplot(train_df[\"Elevation\"][train_df[\"Cover_Type\"] == i]);\n\nplt.legend([\"1 - Spruce\/Fir\",\n\"2 - Lodgepole Pine\",\n\"3 - Ponderosa Pine\",\n\"4 - Cottonwood\/Willow\",\n\"5 - Aspen\",\n\"6 - Douglas-fir\",\n\"7 - Krummholz\"], loc=\"center\", bbox_to_anchor=(0.5,-0.2), ncol=2);\nplt.title(\"Elevation Density by Cover Type\");\nplt.ylabel(\"Density\");","e69591d2":"train_df[\"Cover_Type\"].value_counts()","c656cad9":"train = train_df.copy()\ntest = test_df.copy()\n\ny = train[\"Cover_Type\"]\ntrain.drop(columns=[\"Id\", \"Cover_Type\"], inplace=True, axis=1)\ntest_ids = test[\"Id\"]\ntest.drop(columns=[\"Id\"], inplace=True, axis=1)","c8c2fb4a":"seed = 0","6bb4065c":"X_train, X_val, y_train, y_val = train_test_split(train,\n                                                  y,\n                                                  test_size = 0.2,\n                                                  random_state = seed)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","ea7b8364":"rf = RandomForestClassifier(n_estimators = 100,\n                                            random_state = seed)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nfeature_importances = pd.DataFrame({\"Feature\": X_train.columns,\n                                   \"Importance\": importances}).sort_values(\"Importance\", ascending=False)\n\nrf_predictions = rf.predict(X_val)","6d049396":"accuracy_score(y_val, rf_predictions)","fe932e8c":"plt.figure(figsize=(16,8))\nsns.barplot(x=\"Feature\", y=\"Importance\", data=feature_importances)\nplt.title(\"Feature Importances for Random Forest Model\")\nplt.xticks(rotation=\"vertical\")\nplt.show()","ffa3d188":"def generate_test_predictions(model):\n    predictions = model.predict(X_test)\n    output = pd.DataFrame({\"ID\": test_ids, \"Cover_Type\": predictions})\n    \n    return output\n    \noutput = generate_test_predictions(rf)\noutput.to_csv(\"submission_rf100.csv\", index=False)\noutput.head()","69613e18":"rf.get_params()","7cd27732":"n_estimators = [int(x) for x in np.linspace(100, 2000, num=20)]\n\nrandom_param_grid = {\"n_estimators\": n_estimators}\n\nmodel = RandomForestClassifier()\n\nrf_random_search = RandomizedSearchCV(estimator=model, param_distributions=random_param_grid,\n                                     n_iter=20, cv=3, verbose=2, random_state=seed, n_jobs=-1)\n\nrf_random_search.fit(train, y)","a1c5aff8":"rf_random_search.best_params_","f575f0b6":"We can see significant class separation in the above elevation plot.","ede157af":"### Tune Hyperparameters of Random Forest\n\nFirst, let's see what parameters we have to play with:","d8dbc474":"### Explore Data","41d9528f":"### Identify Feature & Target Column Names","336120ee":"### Fit Random Forest Model\n\nStart with a simple RF model with 100 trees.","9e5bd779":"The data has already been processed as there are no missing values.","f06bbd2c":"### Prepare Training and Test Data","5c307ee6":"### Check for Missing Values","ad62f64f":"We are only dealing with numerical data.","f059486c":"### Examine the Sample Size of the Different Cover Type Responses\n\nThere are equal numbers of each Cover Type target:","aa613d0e":"At this point, decided to move into a local Jupyter instance to bypass the Kaggle session limitations. Be back with more soon!","9c3a48e8":"We can see that cover type does indeed vary based on wilderness area. For example, Wilderness Area 4 is dominated by Cover Types 3, 4 and 6.","de6fc943":"### EDA Summary\n\n**Type of problem**: Multiclass Classification\n\n**Summary of EDA findings**: There are 54 columns of features. The training set has 15,120 rows while the test set is significantly larger with 565,892 rows. The dataset is balanced, with an equal split of response variables: 2,160 observations for each of the seven cover types.\n\n**Target**: Cover_Type","1852f1ac":"### Check for Non-Numeric Features\n\nFeatures Soil_TypeX and Wilderness_AreaX have already been one-hot encoded.","8e5044c4":"The simple random forest model is producing > 84% accuracy on the validation set:","23854ddf":"### Visualize Data","060c7f7e":"### Split into Training\/Validation Sets\n\nLet's use 80% for training, 20% for validation.","11f59cf5":"### Load Data from CSV"}}