{"cell_type":{"0ea0824e":"code","09e4ef6a":"code","d29a3610":"code","18e297cb":"code","0d74c8f6":"code","e680fcf4":"code","a7f46a14":"code","ac5e145c":"code","2f02514e":"code","06162047":"code","7eb60d25":"code","75b1e252":"code","0403edce":"code","bcafd5a0":"code","e341bf2c":"code","6e27ca2d":"code","00961fa0":"code","9e930a91":"code","ee2964a8":"code","a2656754":"code","f421c310":"code","db4d2d7b":"code","c662ceda":"code","efa5fee6":"code","5b652ddd":"markdown","d851594c":"markdown","8d7edf0e":"markdown","82aa7d18":"markdown","eed83edd":"markdown","5f6c33cf":"markdown","e3f18576":"markdown","cf03d14c":"markdown","4b1af3f7":"markdown","11238682":"markdown","a2246ab2":"markdown","e8668438":"markdown","2ad2443e":"markdown","be781ba0":"markdown","1fdac9ef":"markdown","c4645d9e":"markdown","81f9137e":"markdown"},"source":{"0ea0824e":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport nltk\n# Importing Natural language Processing toolkit.\nfrom PIL import Image\n# from python imaging library\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","09e4ef6a":"wines = pd.read_csv(\"..\/input\/winemag-data-130k-v2.csv\",index_col = 0)\nwines = wines.dropna(subset = ['points' , 'price'])\ndf_wines = wines[['country', 'description', 'points', 'price', 'variety']]\ndf_wines = df_wines.sample(frac = 0.05)\nprint(df_wines.shape)\ndf_wines.head()","d29a3610":"df_wines.loc[(df_wines['points'] > 80) & (df_wines['points'] <=85), 'wine_quality'] = 'Average'\ndf_wines.loc[(df_wines['points'] > 85) & (df_wines['points'] <=90), 'wine_quality'] = 'Good'\ndf_wines.loc[(df_wines['points'] > 90) & (df_wines['points'] <=95), 'wine_quality'] = 'Great'\ndf_wines.loc[(df_wines['points'] > 95) & (df_wines['points'] <=100), 'wine_quality'] = 'Perfect'\ndf_wines = df_wines[df_wines['wine_quality'].apply(lambda x: type(x) == str)]","18e297cb":"df_wines.loc[(df_wines['price'] > 0) & (df_wines['price'] <=10) , 'price_val'] = '0-10'\ndf_wines.loc[(df_wines['price'] > 10) & (df_wines['price'] <=20) , 'price_val'] = '10-20'\ndf_wines.loc[(df_wines['price'] > 20) & (df_wines['price'] <=30) , 'price_val'] = '20-30'\ndf_wines.loc[(df_wines['price'] > 30) & (df_wines['price'] <=50) , 'price_val'] = '30-50'\ndf_wines.loc[(df_wines['price'] > 50) & (df_wines['price'] <=100) , 'price_val'] = '50-100'\ndf_wines.loc[df_wines['price'] > 100  , 'price_val'] = 'Above 100'","0d74c8f6":"df_wines = df_wines.drop(columns = ['price', 'points'])\ndf_wines.head(5)","e680fcf4":"sample_data = df_wines\nfor i in sample_data.description:\n    # Importing tokenize library\n    from nltk.tokenize import word_tokenize\n    # Tokenizing the words not using the treebankTokenizer as it was changing the text and using it with punctuation marks \n    tokens = word_tokenize(i)\n    \n    # Changing all the letters to lowercase \n    tokens_low = [w.lower() for w in tokens]\n    \n    \n    # Removing all non-alphabetics from the descriptions \n    words = [word for word in tokens_low if word.isalpha()]\n    \n    # Removing stopwords.\n    stopwords = set(STOPWORDS)\n    stopwords.update([\"drink\" , 'now', 'wine' ,'flavour'])\n    filter_sen = [w for w in words if not w in stopwords]\n    \n    #Using stemming for normalisation \n    from nltk.stem.porter import PorterStemmer\n    porter  = PorterStemmer()\n    stemmed = [porter.stem(word) for word in filter_sen]\n    \n    sentence = \" \".join(w for w in stemmed)\n    sample_data = sample_data.replace(i , sentence)","a7f46a14":"from sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(sample_data, test_size = 0.3)\nprint(\"The total training data is {}\".format(X_train.shape))\nprint(\"The total test data is {}\".format(X_test.shape))","ac5e145c":"print(\"The sample training data has {} tweets\".format(X_train.shape))\nX_train.head(4)","2f02514e":"quality_w = pd.crosstab(index = X_train['wine_quality'], columns = 'count' )\nprice = pd.crosstab(index = X_train['price_val'], columns = 'count' )\nplt.rcParams['figure.figsize'][0] = 14\nplt.rcParams['figure.figsize'][1] = 6\nfig, axs = plt.subplots(1,2)\nquality_w.plot(kind = 'pie', subplots = True, ax=axs[0],autopct='%1.1f%%',shadow=True)\nprice.plot(kind = 'pie',subplots= True,ax=axs[1],  autopct='%1.1f%%',shadow=True)\nplt.suptitle(\" A.% Distribution of Qualities of Wines    B.% Distribution of the Price Ranges found\",fontsize = 16)\nplt.show()","06162047":"from sklearn.feature_extraction.text import CountVectorizer\ncount = CountVectorizer()\ntraining_counts = count.fit_transform(X_train.description)\nprint(\"The shape of the data is {}\".format(training_counts.shape))\n#count.vocabulary_","7eb60d25":"from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(training_counts)\nX_train_tfidf.shape","75b1e252":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_model = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\nfeatures = tfidf_model.fit_transform(X_train.description)\ntfidf_df= pd.DataFrame(features.todense(),columns=tfidf_model.get_feature_names())\ntfidf_df.head()","0403edce":"from sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\ntext_clf_multi = Pipeline([('vect' , CountVectorizer()), ('tfidf' , TfidfTransformer()), ('clf', MultinomialNB())])","bcafd5a0":"train1 = text_clf_multi.fit(X_train.description, X_train.wine_quality)\npredicted1 = train1.predict(X_test.description)\nscore1_qual = 100 * text_clf_multi.score(X_test['description'], X_test['wine_quality'])\nprint(\"The score of Mulitnomial Naive Bayes for the quality of wine is {} %\".format(score1_qual))","e341bf2c":"train1 = text_clf_multi.fit(X_train.description, X_train.price_val)\npredicted1 = train1.predict(X_test.description)\nscore1_price = 100 * text_clf_multi.score(X_test['description'], X_test['price_val'])\nprint(\"The score of Mulitnomial Naive Bayes for the price value is {} %\".format(score1_price))","6e27ca2d":"from sklearn.pipeline import Pipeline \nfrom sklearn.svm import SVC\ntext_clf_svm_linear = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-svm', SVC(kernel = 'linear'))])\ntext_clf_svm_rbf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-svm', SVC(kernel = 'rbf'))])","00961fa0":"text_clf_svm_linear.fit(X_train.description, X_train.wine_quality)\ntext_clf_svm_rbf.fit(X_train.description, X_train.wine_quality)\npredict_svm = text_clf_svm_linear.predict(X_test.description)\npredict_svm = text_clf_svm_rbf.predict(X_test.description)\nlinear_svm_qual = 100 * text_clf_svm_linear.score(X_test['description'] , X_test['wine_quality'])\nrbf_svm_qual  = 100  * text_clf_svm_rbf.score(X_test['description'] , X_test['wine_quality'])\nprint(\"The score of Support Vector Machine Linear Kernel for the quality of wine is {} %\".format(linear_svm_qual))\nprint(\"The score of Support Vector Machine RBF Kernel for the quality of wine is {} %\".format(rbf_svm_qual))","9e930a91":"text_clf_svm_linear.fit(X_train.description, X_train.price_val)\ntext_clf_svm_rbf.fit(X_train.description, X_train.price_val)\npredict_svm = text_clf_svm_linear.predict(X_test.description)\npredict_svm_price = text_clf_svm_rbf.predict(X_test.description)\nlinear_svm_price = 100 * text_clf_svm_linear.score(X_test['description'] , X_test['price_val'])\nrbf_svm_price  = 100  * text_clf_svm_rbf.score(X_test['description'] , X_test['price_val'])\nprint(\"The score of Support Vector Machine Linear Kernel for the price value is {} %\".format(linear_svm_price))\nprint(\"The score of Support Vector Machine RBF Kernel for the price value is {} %\".format(rbf_svm_price))","ee2964a8":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\ntext_clf_knn_3 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-svm',KNeighborsClassifier(n_neighbors=3))])\ntext_clf_knn_7 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-svm',KNeighborsClassifier(n_neighbors=7))])","a2656754":"text_clf_knn_3.fit(X_train.description, X_train.wine_quality)\ntext_clf_knn_7.fit(X_train.description, X_train.wine_quality)\npredict_knn = text_clf_knn_3.predict(X_test.description)\npredict_knn = text_clf_knn_7.predict(X_test.description)\nknn_3_qual = 100 * text_clf_knn_3.score(X_test['description'] , X_test['wine_quality'])\nknn_7_qual = 100 * text_clf_knn_7.score(X_test['description'] , X_test['wine_quality'])\nprint(\"The score of  3 Neighbor KNN for the quality of wine is {} %\".format(knn_3_qual))\nprint(\"The score of 7 Neighbor KNN for the quality of wine is {} %\".format(knn_7_qual))","f421c310":"text_clf_knn_3.fit(X_train.description, X_train.price_val)\ntext_clf_knn_7.fit(X_train.description, X_train.price_val)\npredict_knn = text_clf_knn_3.predict(X_test.description)\npredict_knn = text_clf_knn_7.predict(X_test.description)\nknn_3_price = 100 * text_clf_knn_3.score(X_test['description'] , X_test['price_val'])\nknn_7_price = 100 * text_clf_knn_7.score(X_test['description'] , X_test['price_val'])\nprint(\"The score of  3 Neighbor KNN for the price values of wine is {} %\".format(knn_3_price))\nprint(\"The score of 7 Neighbor KNN for the price values of wine is {} %\".format(knn_7_price))","db4d2d7b":"data = [['Naive Bayes Multi', score1_qual] , ['SVM Linear' , linear_svm_qual] , ['SVM RBF', rbf_svm_qual] , \n        ['KNN_3Neighbor' , knn_3_qual] , ['KNN_7Neighbor', knn_7_qual]]\nquality_pred = pd.DataFrame(data)\ndata2 = [['Naive Bayes Multi', score1_price] , ['SVM Linear' , linear_svm_price] , ['SVM RBF', rbf_svm_price] , \n        ['KNN_3Neighbor' , knn_3_price] , ['KNN_7Neighbor', knn_7_price]]\nprice_pred = pd.DataFrame(data2)","c662ceda":"plt.rcParams['figure.figsize'][0] = 8\nplt.rcParams['figure.figsize'][1] = 6\nindex = np.arange(len(quality_pred))\nwidth = 0.4\nplt.bar(index, quality_pred[1], width = width)\nplt.xlabel('Algorithm Used', fontsize=12)\nplt.ylabel('Percentage Accuracy', fontsize=12)\nplt.xticks(index, quality_pred[0], fontsize=10)\nplt.yticks(np.arange(0,100, step = 10))\nplt.title('% Accuracy of Various ML Algos in predicting the quality of wine', fontsize = 15)\nplt.show()","efa5fee6":"index2 = np.arange(len(price_pred))\nwidth = 0.4\nplt.bar(index2, price_pred[1], width = width, color = 'red')\nplt.xlabel(\"Various ML Algos used\",fontsize = 12)\nplt.ylabel(\"Percentage Accuracy for Algos\", fontsize = 12)\nplt.xticks(index2, price_pred[0])\nplt.yticks(np.arange(0,100, step = 10))\nplt.title('% Accuracy of Various ML Algos in predicting the quality of wine', fontsize = 15)\nplt.show()","5b652ddd":"## Comparing all these methods and finding the most suitable for both parameters","d851594c":"### Splitting the data into training and test data","8d7edf0e":"### Checking the number of quality types of wine in training data","82aa7d18":"### Using Bag of Words","eed83edd":">  ## Result Analysis ","5f6c33cf":"<p> For information related to the basics of dataset and the data analysis part of it refer to this link  \n    <a href = \"https:\/\/www.kaggle.com\/bhaargavi\/wine-classification-analysis-of-data\"> <h3>Link to Wine Classification Analysis of Data <\/h3> <\/a>  <\/p> \n<p> For information related to the word cloud formations and various basic concepts of NLP  refer to this link  \n   <a href = \"https:\/\/www.kaggle.com\/bhaargavi\/wine-review-classification-making-word-clouds \"> <h3>Link to Wine Review Classification -- Making Word Clouds<\/h3> <\/a> <\/p>    ","e3f18576":"### B. Suppot Vector Machines","cf03d14c":" ### C. K Nearest Neighbors ","4b1af3f7":"<p> Now we need to divide the points and price of the wines in specific groups so that we can classify the data easily. So here we classify the points in 4 groups -- Average, Good, Great and Perfect and the prices in the six categories 0-10, 10-20, 20-30, 30-50, 50-100, Above 100. <\/p>","11238682":"### Using TF-IDF Values","a2246ab2":"### A. Naive Bayes Algorithm","e8668438":"<p> The output of this analysis is that when we are predicting quality the best out of all is SVM Linear Approach with more than 65% accuracy.  KNN with 7 Neighbors is closer to it and can be our second bet. And then we have th 3 Neighbours KNN with accuracy of 62%. But The others including SVM Gaussian and Linear Naive Bayes have lesser accuracy of around 55-58%.\n    So the preferred approach will be SVM Linear Approach<\/p>\n   \n  \n  <p>The output of the 2nd  analysis is that when we are predicting price the best out of all is Naive Bayes Multinomial with accuracy going to 40%. And then there is SVM Linear Approach around 39% accuracy and third choice among these will be KNN with  7 Neighbors. But others have the accuracy of around30-32%. But the problem here is the accuracy for the best approach is also 40% around which is way too less to be reliable enough and we can't support data on these methods. So there can be various other techniques also that can be applied to it for better results like Neural Networks. and its various Forms  But amongst these for price  prediction our best bet will be Naive Bayes Multinomial Apporach<\/p>","2ad2443e":"<p> Now the basic things done while removing these less important words were \n   <ol>\n    <li> Tokenizing the words <\/li>\n    <li> Converting all of them to lower cases <\/li>\n    <li> Removing all stopwords <\/li>\n    <li> Performing Normalisation only stemming on them. <\/li>\n    <li>Joining them back to sentences and replacing them with their orignials in the dataframe <\/li>\n   <\/ol>\n<\/p>","be781ba0":"## Text Classification using NLP for Various types of Wines -- Part 3","1fdac9ef":"### Machine Learning Algorithms ","c4645d9e":"### Removal of less important words for classifcation \n<p> NOw after the wordclouds we actually need to do some real text analysis and for that the first step will be removal of less important words from the descriptions using function which is as follows <\/p>","81f9137e":"### Basic Concepts of NLP \n<ol>\n    <li> Bag of Words -- It is an approach used in document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier<\/li>\n <li> TFIDF Values -- Term frequency and Inverse document frequency helps us to find the importance of each word in the document\n    <\/li>\n<\/ol>"}}