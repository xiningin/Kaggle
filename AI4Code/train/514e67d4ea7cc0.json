{"cell_type":{"535595d7":"code","ab34b2f3":"code","d0ef8973":"code","2dce7c48":"code","a7cd49e7":"code","2ec62cd5":"code","f90076f6":"code","46f88467":"code","fe6402e8":"code","43254ed8":"code","03d7f5f5":"code","853a9004":"code","d7b0dd0f":"code","68e5b4f5":"code","04dbde9e":"code","f02c205f":"code","d8172ec3":"code","fd71c928":"code","3046158b":"code","35caf657":"code","9f23831e":"code","9a26f90a":"code","2a77f504":"code","6a02be9c":"code","f6747630":"code","aff4bc5e":"code","16953417":"code","e4fdc655":"code","c7393a63":"code","23ecb6b0":"code","20242249":"code","87dc37eb":"code","fd0ebe11":"code","138d9c2e":"code","12ccfca6":"code","e26e26b2":"code","427297a0":"code","5f5fb303":"code","c8938cac":"code","0ad95f32":"code","a59900c1":"code","2ff7d690":"code","e3b71e9e":"code","1dddcc16":"code","cecdd60b":"code","90f4def2":"code","a9e574c0":"code","b6c90ad1":"code","41e2ef65":"code","4600b6c6":"code","f4aeb04a":"code","830545e9":"code","3b304572":"code","79753fa8":"code","2ea170cd":"code","c054b44f":"code","113a781d":"code","20c715bc":"code","e53a4ba5":"code","9795dee0":"code","a90893ea":"code","eeb116b6":"code","840d1212":"code","215e2d1e":"code","fc7bffd2":"code","e1145109":"code","6467c48d":"code","c61366d5":"code","821d5435":"code","87b0f63c":"code","9857b347":"code","6c44317a":"code","5b18049f":"code","472e4e89":"code","11dbc8d6":"code","82ab6e5e":"code","d6fba94d":"markdown","47171e2a":"markdown","a4d98913":"markdown","65282593":"markdown","500b5d07":"markdown","54fe835b":"markdown","825bf211":"markdown","e405220f":"markdown","5503c774":"markdown","3fac761b":"markdown","235b3ea8":"markdown","5bac1925":"markdown","5b4029f9":"markdown","cf752702":"markdown","a1813379":"markdown","36bfc404":"markdown","d94e608a":"markdown","4edfe7a9":"markdown"},"source":{"535595d7":"!wget https:\/\/www.dropbox.com\/sh\/ld6fx87zdvlwxiz\/AACbD2hgIL5CCzEY19nvXbpDa?dl=0 # Import the data from dropbox","ab34b2f3":"!unzip \/content\/AACbD2hgIL5CCzEY19nvXbpDa?dl=0 # unzip the data","d0ef8973":"# Import basic modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns","2dce7c48":"train = pd.read_csv(\"\/content\/train_s3TEQDk.csv\") # Read the train file\ntest = pd.read_csv(\"\/content\/test_mSzZ8RL.csv\") # Read the test file","a7cd49e7":"# Show first 10 rows of training set\ntrain.head(10)","2ec62cd5":"train.info() #Extract the basic information from the data like dtypes of features, number of non-null values","f90076f6":"test.info()","46f88467":"train.isnull().sum() # Check the count for null values in feature","fe6402e8":"test.isnull().sum()","43254ed8":"# Extract the null columns from training and testing set\n\nnull_columns_train = [col for col in train.columns if train[col].isnull().sum() !=0]\nnull_columns_test = [col for col in test.columns if test[col].isnull().sum() !=0]\n\n# Print the percentage of null values in each column\ndef perc_null_vals(null_columns, data, dataset_type = None):\n  total_val_counts = data.shape[0] # Total values in the dataset\n  for col in null_columns:\n    null_val_counts = data[col].isnull().sum() # number of null values in the dataset\n    perc_null_vals = float(null_val_counts)*100\/total_val_counts\n    print(\"The percentage of null values in {} in the {} set is {:.3f}%\".format(col, dataset_type, perc_null_vals))\n\n# Call the above function\nperc_null_vals(null_columns_train, train, \"train\")\nperc_null_vals(null_columns_test, test, \"test\")","03d7f5f5":"train.head()","853a9004":"train['Occupation'].value_counts()","d7b0dd0f":"# Check for unique values in the categorical columns\ncat_cols = [cols for cols in train.select_dtypes('O').columns]\n\ndef print_unique_vals(data, columns):\n  # iterate over each column and print the unique values in each categorical column as well as their counts\n  for col in columns:\n    print(\"Unqiue values in {} are\".format(col))\n    print(data[col].value_counts())\n    print()\n\n# Call the above method\nprint_unique_vals(train, cat_cols)","68e5b4f5":"# Check similarly for test set\ncat_cols_test = [col for col in test.select_dtypes('O').columns]\nprint_unique_vals(test, cat_cols_test)","04dbde9e":"# Create a new column called is_lead_text for plotting a pairplot\ntrain['Is_Lead_text'] = train['Is_Lead'].apply(lambda x : \"Yes\" if x==1 else \"No\")\n\n# Create a copy of train set\ntrain_copy = train.copy()\ntrain_copy.drop('Is_Lead',axis = 1, inplace=True)\n\n# Plot the pairplot\nsns.pairplot(train_copy, hue = \"Is_Lead_text\")\nplt.show()","f02c205f":"# extract numerical columns and plot scatter plots between each\nnum_cols = [col for col in train_copy.select_dtypes('int')]\n\n# Check the distribution of the numerical features\ntrain_copy.hist(figsize=(12,8))\nplt.show()\n\n# Check for class imbalance\nsns.countplot(x = train_copy['Is_Lead_text'])\nplt.grid()\nplt.show()\n\n# Check for class count w.r.t every other categorical_column\nsns.countplot(x = train_copy['Is_Lead_text'], hue=train_copy['Gender'])\nplt.grid()\nplt.show()\n\nsns.countplot(x = train_copy['Is_Lead_text'], hue=train_copy['Is_Active'])\nplt.grid()\nplt.show()\n\nsns.countplot(x = train_copy['Is_Lead_text'], hue=train_copy['Occupation'])\nplt.grid()\nplt.show()\n\nsns.countplot(x = train_copy['Is_Lead_text'], hue=train_copy['Channel_Code'])\nplt.grid()\nplt.show()\n\nsns.countplot(x = train_copy['Is_Lead_text'], hue=train_copy['Credit_Product'])\nplt.grid()\nplt.show()","d8172ec3":"train_copy.head()","fd71c928":"# plot boxplots w.r.t Is_Lead_text\nsns.boxplot(x=train_copy['Credit_Product'], y=train_copy['Avg_Account_Balance'])\nplt.show()\n\nsns.boxplot(x=train_copy['Credit_Product'], y=train_copy['Age'])\nplt.show()\n\nsns.boxplot(x=train_copy['Credit_Product'], y=train_copy['Vintage'])\nplt.show()","3046158b":"# Fill na in credit_product\ndef fill_na(data, null_indices):\n  for i in null_indices:\n    if  (55 <= train['Age'].iloc[i] <=60)  and (60 <= train['Vintage'].iloc[i] <= 40):\n      train['Credit_Product'].iloc[i] = \"Yes\"\n    elif 30 <= train['Age'].iloc[i] <= 40:\n      train['Credit_Product'].iloc[i] = \"No\"\n\n\n# Find null indices\nnull_indices = train[train['Credit_Product'].isnull() == True].index\n\n# Call the above method and fill the null values\nfill_na(train, null_indices=null_indices)","35caf657":"train.isnull().sum()","9f23831e":"train['Credit_Product'].value_counts()","9a26f90a":"# For rest of the null values introduce a new category of \"Unknown\"\ntrain.fillna('Unkown', inplace=True)","2a77f504":"# Similarly use fill_na(user defined function on test set)\nnull_indices_test = test[test['Credit_Product'].isnull()==True].index\nfill_na(test, null_indices_test)","6a02be9c":"test.isnull().sum() # this logic did not work on test set, fill the null vlaues with unknown brand","f6747630":"test.fillna('Unknown', inplace=True)","aff4bc5e":"# Check for null values\ntrain.isnull().sum()","16953417":"test.isnull().sum()","e4fdc655":"train.head(10)","c7393a63":"# Drop the is_lead_text column, ID and Region_Code\ntrain.drop(['ID', 'Region_Code', 'Is_Lead_text'], axis=1, inplace=True)","23ecb6b0":"# Encode the columns\ncat_cols_updated = [cols for cols in train.select_dtypes('O').columns]\ncat_cols_updated","20242249":"from sklearn.preprocessing import LabelEncoder\n\ndef encoder(data, cat_cols):\n\n  for col in cat_cols:\n    le = LabelEncoder()\n    data[col] = le.fit_transform(data[col].values.reshape((-1,1)))\n  return data\n","87dc37eb":"encoded_data = encoder(train, cat_cols_updated) # use the encoder function","fd0ebe11":"test.head()","138d9c2e":"# Store the Sample ID of the test set\ntest_ID = test['ID']","12ccfca6":"test.drop(['ID', 'Region_Code'],axis=1, inplace=True)\ncat_cols_test_updated = [cols for cols in test.select_dtypes('O').columns]","e26e26b2":"cat_cols_test_updated","427297a0":"# Call the encoder funciton\nencoded_data_test = encoder(test, cat_cols_test_updated)","5f5fb303":"encoded_data_test.head()","c8938cac":"plt.figure(figsize=(12,8))\nsns.heatmap(encoded_data.corr(), annot=True)\nplt.show()","0ad95f32":"# Scale the training and testing data\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef scaled_data(data):\n  for col in data.columns:\n    # Min Max Scaler object\n    mms = MinMaxScaler()\n    data[col] = mms.fit_transform(data[col].values.reshape((-1,1)))\n  return data","a59900c1":"final_training_data = scaled_data(encoded_data.drop('Is_Lead', axis=1))\nfinal_testing_data = scaled_data(encoded_data_test)","2ff7d690":"final_training_data.head()","e3b71e9e":"len(final_training_data.columns)","1dddcc16":"len(final_testing_data.columns)","cecdd60b":"final_training_data.head()","90f4def2":"target_variable = encoded_data['Is_Lead'] # Store the target variable","a9e574c0":"# Model selection \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score\n\n# Split the training data\nX_train, X_test, y_train, y_test = train_test_split(final_training_data, target_variable, test_size=0.2, random_state=42)","b6c90ad1":"# Define a function to compute all the metrics\ndef compute_metrics(y_true, y_pred, y_score, model):\n\n  # 1. Accuracy Score\n  acc_score = accuracy_score(y_true=y_true, y_pred=y_pred)\n\n  # 2. ROC_AUC_Score\n  roc_score = roc_auc_score(y_true=y_true, y_score=y_score)\n\n  # 3. Precision Score\n  prec = precision_score(y_true=y_true, y_pred=y_pred)\n\n  # Recall Score\n  rec = recall_score(y_true=y_true, y_pred=y_pred)\n\n  # Create a df of all the metrics\n  df_metrics = pd.DataFrame(np.array([acc_score, roc_score, prec, rec]).reshape((1,4)), columns=[\"Accuracy\", \"ROC_AUC_Score\", \"Precision\", \"Recall\"], index=[model])\n  return df_metrics","41e2ef65":"from sklearn.linear_model import LogisticRegressionCV # Logistic Regression Model\n\n# Model object\nlg_clf = LogisticRegressionCV(cv=3, verbose=1, random_state=42, n_jobs=-1)\n\n# fit the model\nlg_clf.fit(X_train, y_train)","4600b6c6":"# Make predicitons on the test set and compute the metrics\npredictions_1 = lg_clf.predict(X_test)\nprediction_prob1 = lg_clf.predict_proba(X_test)\nprediction_prob1 = prediction_prob1[ : ,1]","f4aeb04a":"log_reg_results = compute_metrics(y_true = y_test, y_pred = predictions_1, y_score = prediction_prob1, model = \"Logistic Regression\")","830545e9":"log_reg_results","3b304572":"from sklearn.tree import DecisionTreeClassifier\n\n# Model object\ndt_clf_default = DecisionTreeClassifier() # Default Model\n\n# Fit the model\ndt_clf_default.fit(X_train, y_train)","79753fa8":"# Make predictions and compute metrics\npredictions_2 = dt_clf_default.predict(X_test)\nprediction_prob2 = dt_clf_default.predict_proba(X_test)\nprediction_prob2 = prediction_prob2[ : ,1]\n\n# Compute the metrics\ndt_clf_default_metrics = compute_metrics(y_true=y_test, y_pred=predictions_2, y_score=prediction_prob2, model = \"Decision Tree Default\")\ndt_clf_default_metrics","2ea170cd":"# Hyperparameter Tuning\n\n# set parameters\ncriterion = [\"ginin\", \"entropy\"]\nsplitter = [\"best\", \"random\"]\nmax_depth = [None, 10, 20, 30]\nmin_samples_split = [2,3,4,5,6,8,9,10]\nmin_samples_leaf = [1,2,3,4,5]\nmin_weight_fraction_leaf = [ 0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\nmax_features = [\"auto\", \"sqrt\", \"log2\", None]\nclass_weight = [None, \"balanced\"]\n\n\n# Create a parameter grid\nparams = {\"criterion\" : criterion,\n          \"splitter\" : splitter,\n          \"max_depth\" : max_depth,\n          \"min_samples_split\" : min_samples_split,\n          \"min_samples_leaf\" : min_samples_leaf,\n          \"min_weight_fraction_leaf\" : min_weight_fraction_leaf,\n          \"max_features\" : max_features,\n          \"class_weight\" : class_weight\n          }\n\n# Model Object\ndt_clf = DecisionTreeClassifier(random_state=42)\n\n# Randomised Search CV\nrscv_dt_clf = RandomizedSearchCV(dt_clf, params, n_iter=20, n_jobs=-1, cv=3, verbose=1, random_state=42)\n\n# Fit the model\nrscv_dt_clf.fit(X_train, y_train)","c054b44f":"dt_best_estimator = rscv_dt_clf.best_estimator_","113a781d":"dt_best_estimator.fit(X_train, y_train)","20c715bc":"# Make predicitons and compute metrics\npredictions_3 = dt_best_estimator.predict(X_test)\nprediction_prob3 = dt_best_estimator.predict_proba(X_test)\nprediction_prob3 = prediction_prob3[ : , 1]\n\n# Compute the metrics\ndt_clf_best_est_results = compute_metrics(y_test, predictions_3, prediction_prob3, \"Decision Tree Best Estimator\")\ndt_clf_best_est_results","e53a4ba5":"from sklearn.ensemble import RandomForestClassifier\n\n# Model Object\nrf_clf_default = RandomForestClassifier() # Default Model\n\n# Fit the model\nrf_clf_default.fit(X_train, y_train)\n\n# Make Predicitons and Compute metrics\npredictions_4 = rf_clf_default.predict(X_test)\nprediction_prob4 = rf_clf_default.predict_proba(X_test)[ : , 1]\n\n# Compute the metrics\nrf_clf_default_results = compute_metrics(y_test, predictions_4, prediction_prob4 , model = \"Random Forest Default\")","9795dee0":"rf_clf_default_results","a90893ea":"# Hyperparameter Tuninig\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'criterion' : [\"gini\", \"entropy\"],\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 500]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\n# Fit the model\ngrid_search.fit(X_train, y_train)","eeb116b6":"# Make Predicitons and Compute metrics\npredictions_8 = grid_search.predict(X_test)\nprediction_prob8 = grid_search.predict_proba(X_test)[ : , 1]\n\n# Compute the metrics\nrf_clf_best_results = compute_metrics(y_test, predictions_8, prediction_prob8 , model = \"Random Forest Best Estimator\")","840d1212":"from xgboost import XGBClassifier\n\n# Model object\nxgb_clf = XGBClassifier()\n\n# Fit the object\nxgb_clf.fit(X_train, y_train)","215e2d1e":"# Make predictions and compute metrics\npredictions_5 = xgb_clf.predict(X_test)\nprediction_prob5 = xgb_clf.predict_proba(X_test)[ : , 1]\n\n# compute the metrics\nxgb_clf_results = compute_metrics(y_test, predictions_5, prediction_prob5, model = \"XGB Classifier\")\nxgb_clf_results","fc7bffd2":"from sklearn.ensemble import AdaBoostClassifier\n\n# default model\nadb_clf = AdaBoostClassifier()\n\n# Fit the model\nadb_clf.fit(X_train, y_train)\n\n# Make predictions and compute metrics\npredictions_5 = adb_clf.predict(X_test)\nprediction_prob5 = adb_clf.predict_proba(X_test)[ : ,1]","e1145109":"adb_clf_results = compute_metrics(y_test, predictions_5, prediction_prob5, \"AdaBoost Classifier\")","6467c48d":"adb_clf_results","c61366d5":"from sklearn.svm import SVC\n\n# Model object\nsvm_clf = SVC()\n\n# Fit the model\nsvm_clf.fit(X_train, y_train)\n\n# Make predictions\npredictions_6 = svm_clf.predict(X_test)\n#prediction_prob6 = svm_clf.predict_proba(X_test)[ : ,1]","821d5435":"# Compute metrics\nsvm_clf.decision_function(X_test)","87b0f63c":"import tensorflow as tf\nimport keras\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\n\n\n# Build a sequential model\nmodel = Sequential()\n\n# Add layers\nmodel.add(Dense(50, activation='relu', input_shape = (X_train.shape[-1],)))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.50))\nmodel.add(Dense(150, activation='relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\n# buld the model\nprint(model.summary())\n\n# Compile the model\nmetric = tf.metrics.AUC(from_logits=True)\nmodel.compile(optimizer='sgd', loss='binary_crossentropy', metrics=metric)\nhist = model.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=EarlyStopping(patience=10))","9857b347":"# make predictions and compute metrics\npredictions_7 = model.predict_classes(X_test)\nprediction_prob7 = model.predict_proba(X_test)\n\n# Compute metrics\nnn_model_metrics = compute_metrics(y_test, predictions_7, prediction_prob7, \"Neural Network\")\nnn_model_metrics","6c44317a":"final_testing_data.rename(columns={'Credit_Product_Unknown' : 'Credit_Product_Unkown'}, inplace=True)","5b18049f":"# Compare the models\napproach_1_result_metrics = pd.concat([log_reg_results, \n                                       dt_clf_default_metrics, \n                                       dt_clf_best_est_results, \n                                       rf_clf_default_results,\n                                       xgb_clf_results,\n                                       adb_clf_results,\n                                       nn_model_metrics]).sort_values(['ROC_AUC_Score'], ascending=False)\napproach_1_result_metrics","472e4e89":"def make_predictions(model, model_object, data, id_column, path=\"\/content\/\"):\n\n  if model != \"NeuralNetwork\":\n    # make predictions\n    predictions = model_object.predict(data)\n  else:\n    predictions = model_object.predict_classes(data).reshape((-1,))\n\n  # Concatenate the predictions and the ID\n  prediction_df = pd.DataFrame({\"ID\" : test_ID, \"Is_Lead\" : predictions})\n\n  # store into .csv\n  dest = path + model + \"_\" + \"submissions.csv\"\n  prediction_df.to_csv(dest, index=False)","11dbc8d6":"models = {\"LogisticRegression\" : lg_clf, \n          \"DecisionTree\" : dt_best_estimator, \n          \"RandomForest\" : rf_clf_default,\n          \"XGBoost\" : xgb_clf,\n          \"AdaBoost\" : adb_clf,\n          \"NeuralNetwork\" : model\n          }\n\n# Iterate over the itemrs and Call the above method\nfor mod, model_obj in models.items():\n  make_predictions(model = mod, model_object=model_obj, data=final_testing_data, id_column = test_ID)","82ab6e5e":"final_testing_data.shape","d6fba94d":"# Machine Learninig Modelling","47171e2a":"# Predictions","a4d98913":"## Neural Networks","65282593":"## Adaboost Classifier","500b5d07":"## Decision Tree Classifier","54fe835b":"# Data preprocessing","825bf211":"## Approach 1\n\n\n*   Logistic Regression\n*   Decision Tree\n*   Random Forest Classifier\n*   XGBoost Classifier\n*   AdaBoost Classifier\n*   Neural Network\n\n\n\n\n\n","e405220f":"## Random Forest Classifier","5503c774":"**Every Categorical column other than ID and Region_Code have same subcategories in both train and test set**","3fac761b":"**Credit_Product feature have some null values.**","235b3ea8":"\n\n1.   Those between age 55-60 or Vintage between 60-80 have a Credit_Product.\n2.   Those between age 30-40 do not have a Credit_Product.\n\n","5bac1925":"# Import the data","5b4029f9":"# EDA on trainset","cf752702":"## Logistic Regression","a1813379":"## XGBoost","36bfc404":"# Load the Data","d94e608a":"**Almost equal percentage of null values are present in both the dataset.**","4edfe7a9":"## SVM"}}