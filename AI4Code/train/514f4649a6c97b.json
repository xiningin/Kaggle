{"cell_type":{"90757469":"code","1989d329":"code","7c029245":"code","98268703":"code","a02b1be9":"code","4fd8b3e9":"code","5c0cf03e":"code","9a62fff6":"code","93f532f4":"code","4b3af247":"code","fc91d4de":"code","765a6549":"code","d0e86657":"code","919fc2aa":"code","44498002":"code","b1f1fb1f":"code","8804052f":"code","f0d4cf78":"code","2eba749a":"code","5eb3a13e":"code","ce083c83":"code","8b0e14db":"code","4bddee79":"code","58dd12bf":"code","7869c647":"code","9580e6e9":"code","d8122c9a":"code","8ec204d4":"code","5ddb987d":"code","45f43cde":"code","a74a74a8":"code","e46501af":"code","6d03c21d":"code","c2d45bac":"code","01933743":"code","9a2a6b90":"code","a100d642":"code","d80de6e9":"code","2b61ef9d":"code","1933a50e":"code","75bfce9b":"code","92a13b6b":"code","78bc44ac":"code","9f0557f7":"code","1cb53f5d":"code","ad597bec":"code","2114c8d3":"code","75560d52":"code","2d67e3c3":"code","2632f0fb":"code","36eb031d":"code","21e429d0":"code","b76356d8":"code","f3f0d88f":"code","4e017998":"code","b9b023a9":"markdown","975c1073":"markdown","485bc758":"markdown","46ab74ae":"markdown","e17ebcf1":"markdown","f16a1dc5":"markdown","c4967a25":"markdown","6b574d9f":"markdown","3824eaae":"markdown","96744ce6":"markdown","5ed7bb9c":"markdown","53d2a88c":"markdown","3c17f1e2":"markdown","c1f57ffa":"markdown","366bfef0":"markdown","957e5aa0":"markdown","f81ff2d8":"markdown","adcfa8b6":"markdown","28f3363f":"markdown"},"source":{"90757469":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1989d329":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# statistics tools\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# ML\nimport h2o\nfrom h2o.estimators import H2ORandomForestEstimator\nfrom h2o.estimators import H2OGradientBoostingEstimator","7c029245":"# load data\ndf = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\n","98268703":"# dimensions of data\ndf.shape","a02b1be9":"# column names\nprint(df.columns.tolist())","4fd8b3e9":"\ndf.info()","5c0cf03e":"# impute with -99\ndf.bmi = df.bmi.fillna(-99)","9a62fff6":"# rename columns\ndf.rename(columns = {'Residence_type':'residence_type'}, inplace = True)","93f532f4":"# define target variable\ndf['target'] = df.stroke\ndf = df.drop(['stroke'], axis=1) # remove stroke column","4b3af247":"# select numerical features\nfeatures_num = ['age', 'avg_glucose_level','bmi']","fc91d4de":"# basic stats\ndf[features_num].describe(percentiles=[0.1,0.25,0.5,0.75,0.9])","765a6549":"# plot distribution of numerical features\nfor f in features_num:\n    df[f].plot(kind='hist', bins=50)\n    plt.title(f)\n    plt.grid()\n    plt.show()","d0e86657":"# pairwise scatter plot\nsns.pairplot(df[features_num], \n             kind='reg', \n             plot_kws={'line_kws':{'color':'magenta'}, 'scatter_kws': {'alpha': 0.1}})\nplt.show()","919fc2aa":"# Spearman (Rank) correlation\ncorr_spearman = df[features_num].corr(method='spearman')\n\nfig = plt.figure(figsize = (6,5))\nsns.heatmap(corr_spearman, annot=True, cmap=\"RdYlGn\", vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","44498002":"features_cat = ['gender','hypertension','heart_disease','ever_married',\n                'work_type','residence_type','smoking_status']","b1f1fb1f":"for f in features_cat:\n    df[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","8804052f":"# calc frequencies\ntarget_count = df.target.value_counts()\nprint(target_count)\nprint()\nprint('Percentage of strokes [1]:', np.round(100*target_count[1] \/ target_count.sum(),2), '%')","f0d4cf78":"# plot target distribution\ntarget_count.plot(kind='bar')\nplt.title('Target = Stroke')\nplt.grid()\nplt.show()","2eba749a":"# add binned version of numerical features\n\n# quantile based:\ndf['age_bin'] = pd.qcut(df['age'], q=10, precision=1)\ndf['avg_glucose_level_bin'] = pd.qcut(df['avg_glucose_level'], q=10, precision=1)\n\n# explicitly defined bins:\ndf['bmi_bin'] = pd.cut(df['bmi'], [-100,10,20,25,30,35,40,50,100])","5eb3a13e":"# plot target vs features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in features_num:\n    f_bin = f+'_bin'\n    plt.rcParams[\"figure.figsize\"] = (16,7) # increase plot size for mosaics\n    mosaic(df, [f_bin, 'target'], title='Target vs ' + f + ' [binned]')\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","ce083c83":"# BMI - check cross table\nctab = pd.crosstab(df.bmi_bin, df.target)\nctab","8b0e14db":"# normalize each row to get row-wise target percentages\n(ctab.transpose() \/ ctab.sum(axis=1)).transpose()\n","4bddee79":"# plot target vs features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in features_cat:\n    plt.rcParams[\"figure.figsize\"] = (8,7) # increase plot size for mosaics\n    mosaic(df, [f, 'target'], title='Target vs ' + f)\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","58dd12bf":"# \"ever married\" - check cross table\nctab = pd.crosstab(df.ever_married, df.target)\nctab","7869c647":"# normalize each row\n(ctab.transpose() \/ ctab.sum(axis=1)).transpose()","9580e6e9":"# select predictors\npredictors = features_num + features_cat\nprint('Number of predictors: ', len(predictors))\nprint(predictors)","d8122c9a":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","8ec204d4":"# upload data frame in H2O environment\ndf_hex = h2o.H2OFrame(df)\n\ndf_hex['target'] = df_hex['target'].asfactor()\n\n# train \/ test split (70\/30)\ntrain_hex, test_hex = df_hex.split_frame(ratios=[0.7], seed=999)\n\n# pandas versions of train\/test\ndf_train = train_hex.as_data_frame()\ndf_test = test_hex.as_data_frame()","5ddb987d":"# export for potential external processing\ndf_train.to_csv('df_train.csv')\ndf_test.to_csv('df_test.csv')","45f43cde":"# define Gradient Boosting model\nfit_1 = H2OGradientBoostingEstimator(ntrees = 100,\n                                     max_depth=4,\n                                     min_rows=10,\n                                     learn_rate=0.01, # default: 0.1\n                                     sample_rate=1,\n                                     col_sample_rate=0.7,\n                                     nfolds=5,\n                                     score_each_iteration=True,\n                                     stopping_metric='auto',\n                                     stopping_rounds=10,\n                                     seed=999)","a74a74a8":"# train model\nt1 = time.time()\nfit_1.train(x=predictors,\n            y='target',\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","e46501af":"# show training scoring history\nplt.rcParams['figure.figsize']=(7,4)\nfit_1.plot()","6d03c21d":"# show cross validation metrics\nfit_1.cross_validation_metrics_summary()","c2d45bac":"# show scoring history - training vs cross validations\nfor i in range(5):\n    cv_model_temp = fit_1.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History [AUC]'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_auc, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_auc, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.ylabel('AUC')\n    plt.ylim(0.8,1)\n    plt.legend()\n    plt.grid()\n    plt.show()","01933743":"# training performance\nperf_train = fit_1.model_performance(train=True)\nperf_train.plot()\n","9a2a6b90":"# cross validation performance\nperf_cv = fit_1.model_performance(xval=True)\nperf_cv.plot()","a100d642":"# on training data - automatic threshold (optimal F1 score)\nconf_train = fit_1.confusion_matrix(train=True)\nconf_train.show()","d80de6e9":"# corresponding accuracy for this threshold:\nconf_list_temp = conf_train.to_list()\nn_matrix = sum(conf_list_temp[0]) + sum(conf_list_temp[1])\nacc_t0 = (conf_list_temp[0][0]+conf_list_temp[1][1]) \/ n_matrix\nprint('Accuracy:', np.round(acc_t0,6))","2b61ef9d":"# alternatively specify threshold manually - here we try to achieve a symmetric outcome\ntt = 0.148\nconf_train_man = fit_1.confusion_matrix(train=True, thresholds=tt)\nconf_train_man.show()","1933a50e":"# corresponding accuracy for manual threshold:\nconf_list_temp = conf_train_man.to_list()\nn_matrix = sum(conf_list_temp[0]) + sum(conf_list_temp[1]) \nacc_t1 = (conf_list_temp[0][0]+conf_list_temp[1][1]) \/ n_matrix\nprint('Accuracy:', np.round(acc_t1,6))","75bfce9b":"# check on cross validation\nconf_cv_man = fit_1.confusion_matrix(xval=True, thresholds=tt)\nconf_cv_man.show()","92a13b6b":"# corresponding accuracy for our manual threshold:\nconf_list_temp = conf_cv_man.to_list()\nn_matrix = sum(conf_list_temp[0]) + sum(conf_list_temp[1])\nacc_t1_CV = (conf_list_temp[0][0]+conf_list_temp[1][1]) \/ n_matrix\nprint('Accuracy:', np.round(acc_t1_CV,6))","78bc44ac":"# basic version\nfit_1.varimp_plot()","9f0557f7":"# variable importance using shap values => see direction as well as severity of feature impact\nt1 = time.time()\nfit_1.shap_summary_plot(train_hex);\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","1cb53f5d":"# predict on train set (extract probabilities only)\npred_train = fit_1.predict(train_hex)['p1']\npred_train = pred_train.as_data_frame().p1\n\n# and plot\nplt.hist(pred_train, bins=50)\nplt.title('Predictions on Train Set')\nplt.grid()\nplt.show()","ad597bec":"# check calibration\nfrequency_pred = sum(pred_train)\nfrequency_act = df_train.target.sum()\nprint('Predicted Frequency:', frequency_pred)\nprint('Actual Frequency   :', frequency_act)","2114c8d3":"# calc performance on test test\nperf_test = fit_1.model_performance(test_hex)\n\n# ROC Curve - Test Set\nperf_test.plot()","75560d52":"# confusion matrix using our manual threshold\nconf_test_man = perf_test.confusion_matrix(thresholds=tt)\nconf_test_man.show()","2d67e3c3":"# calc accuracy for manual threshold:\nconf_list_temp = conf_test_man.to_list()\nn_matrix = sum(conf_list_temp[0]) + sum(conf_list_temp[1]) \nacc_t1_test = (conf_list_temp[0][0]+conf_list_temp[1][1]) \/ n_matrix\nprint('Accuracy:', np.round(acc_t1_test,6))","2632f0fb":"# predict on test set (extract probabilities only)\npred_test = fit_1.predict(test_hex)['p1']\npred_test = pred_test.as_data_frame().p1\n\n# and plot\nplt.hist(pred_test, bins=50)\nplt.title('Predictions on Test Set')\nplt.grid()\nplt.show()","36eb031d":"# connect prediction with data frame\ndf_test['prediction'] = pred_test","21e429d0":"# show most endangered patients (according to our model) in test set\ndf_high_20 = df_test.nlargest(20, columns='prediction')\ndf_high_20","b76356d8":"print('Actual cases in highest 20    :', df_high_20.target.sum())\nprint('Predicted cases in highest 20 :', np.round(df_high_20.prediction.sum(),2))","f3f0d88f":"# show least endangered patients (according to our model) in test set\ndf_low_20 = df_test.nsmallest(20, columns='prediction')\ndf_low_20","4e017998":"print('Actual cases in lowest 20    :', df_low_20.target.sum())\nprint('Predicted cases in lowest 20 :', np.round(df_low_20.prediction.sum(),2))","b9b023a9":"Much better: 184 actual positives vs. 185 predicted positives!","975c1073":"Quite good: 65 actual positives vs 69 predicted positives.","485bc758":"Target vs Categorical Features\n","46ab74ae":"Stroke Prediction Model (Binary Classification)\n","e17ebcf1":"Confusion Matrix","f16a1dc5":"\"Naive\" Interpretations based on those univariate plots:\nInfluence of gender seems surprisingly low\nHypertension and heart disease massively increase risk of stroke\n\"Ever married\" too!?\nWork type: Higher risk for self-employed (more stress?)\nResidence type: Slightly higher risk for urban vs rural\nSmoking: Highest risk for former smokers. Not much difference between \"smokes\" and \"never smoked\"?","c4967a25":"Predictions on training data\n","6b574d9f":"Variable Importance","3824eaae":"Build Model","96744ce6":"Data cleansing","5ed7bb9c":"Import and first glance","53d2a88c":"Almost 20% of the missing BMIs had a stroke! This is way higher than for the other bins.","3c17f1e2":"Show examples","c1f57ffa":"\"Naive\" Interpretations based on those univariate plots:\nRisk increases with age and glucose level (diabetes).\nHigh BMI levels are also indicating higher risk.\nA missing value for BMI (the leftmost column) seems to indicate a massively increased risk!?\n","366bfef0":"Target vs Numerical Features","957e5aa0":"ROC Curve - Training Data","f81ff2d8":"Numerical Features","adcfa8b6":"ROC Curve - Cross Validation","28f3363f":"Evaluate on Test Set"}}