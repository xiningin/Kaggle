{"cell_type":{"2a1462b5":"code","4cada514":"code","c97258d9":"code","0be8a1ef":"code","943a2bdb":"code","d74335d9":"code","8037e3b3":"code","043eec00":"code","3b6e97c6":"code","c6bec84e":"code","373ac23d":"code","e1e31116":"code","20e2eaf3":"code","44ac98f7":"code","7fe1feff":"code","c05cb8e1":"code","db721626":"code","2a5992b7":"code","016785a8":"code","c8df01ea":"code","6a127ac5":"code","9b0a0148":"code","f410f3cd":"code","4faa2ed5":"code","4b29a663":"code","0b900037":"code","f0905529":"code","3453d7e0":"code","5935b1e3":"code","2084289f":"code","ebf33caa":"code","d47b2728":"code","fbbca441":"code","cdd6d75e":"code","1d7ffe05":"code","cd1e973c":"code","3d7b26c9":"code","739a7ab8":"code","c0b9a9a9":"code","59072bc6":"code","d25c02ca":"code","cff75446":"code","d0825421":"code","90a8aecc":"code","88a41ce1":"code","620ebc37":"code","65b8a302":"code","ef8f87bb":"code","e4666c8f":"code","554d5d0c":"code","ad2e073b":"code","adfdc9dd":"code","e83802ee":"code","e568afbe":"code","b2a51d8f":"code","5df601a6":"code","4184cd96":"code","704211b7":"code","cfa58cf0":"code","558ff3fe":"code","e90283fa":"code","2a09b713":"code","66b69ed4":"code","4e5d6c71":"code","85a68b56":"code","77a52c73":"code","63729501":"code","5926a865":"code","d9fcf5ec":"code","d9342a15":"code","09a4140c":"code","88089daf":"code","d00b2044":"code","73c855ef":"code","3382828e":"code","62101df1":"code","80bca964":"code","584507cb":"code","b5b25e9c":"code","b1af6d3d":"code","1cca6366":"code","7cada736":"code","7abd7282":"code","5017b630":"markdown","d4e84317":"markdown","a5709ce6":"markdown","8dc955f6":"markdown","4c34a7cb":"markdown","b3e98b3c":"markdown","96f46b34":"markdown"},"source":{"2a1462b5":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport pickle\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\nimport random\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nimport gc\nimport json\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, RobustScaler\n# from category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multioutput import MultiOutputClassifier\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings('ignore')","4cada514":"!pip install ..\/input\/iterative-stratification\/iterative_stratification-0.1.6-py3-none-any.whl","c97258d9":"import sys\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","0be8a1ef":"import os\nimport copy\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom sklearn.preprocessing import QuantileTransformer\n\nimport warnings\nwarnings.filterwarnings('ignore')","943a2bdb":"%%time\ntrain_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_drug = pd.read_csv('..\/input\/lish-moa\/train_drug.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsub_rankgauss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","d74335d9":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","8037e3b3":"#RankGauss\n\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","043eec00":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","3b6e97c6":"# GENES\nn_comp = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","c6bec84e":"#CELLS\nn_comp = 50  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","373ac23d":"from sklearn.feature_selection import VarianceThreshold\n\n\nvar_thresh = VarianceThreshold(0.8)  #<-- Update\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape","e1e31116":"%%time\nfrom sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 35, n_clusters_c = 5, SEED = 123):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features ,test_features=fe_cluster(train_features,test_features)","20e2eaf3":"%%time\ndef fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\ntrain_features,test_features=fe_stats(train_features,test_features)","44ac98f7":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","7fe1feff":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","c05cb8e1":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","db721626":"def assign_folds(seed,fold):\n    scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n    targets = scored.columns[1:]\n    scored = scored.merge(train_drug, on='sig_id', how='left') \n    scored = scored.iloc[non_ctl_idx]\n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc<=18].index.sort_values()\n    vc2 = vc.loc[vc>18].index.sort_values()\n\n    # STRATIFY DRUGS 18X OR LESS\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits=fold, shuffle=True, \n              random_state=seed)\n    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 18X\n    skf = MultilabelStratifiedKFold(n_splits=fold, shuffle=True, \n              random_state=seed)\n    tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    # ASSIGN FOLDS\n    scored['fold'] = scored.drug_id.map(dct1)\n    scored.loc[scored.fold.isna(),'fold'] =\\\n        scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n    scored.fold = scored.fold.astype('int8')\n\n    return scored['fold'].values","2a5992b7":"folds = train.copy()\nseed_folds = train.copy()\n\nval_folds = assign_folds(34,5)\n\nfolds['kfold'] = val_folds\nfolds","016785a8":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","c8df01ea":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","6a127ac5":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","9b0a0148":"class Model(nn.Module):      \n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.4)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.4)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","f410f3cd":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","4faa2ed5":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","4b29a663":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5            \nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048","0b900037":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    asn_folds = assign_folds(seed,5)\n    seed_folds['kfold'] = asn_folds\n    \n    trn_idx = train[seed_folds['kfold'] != fold].index\n    val_idx = train[seed_folds['kfold'] == fold].index\n    \n    train_df = train[seed_folds['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[seed_folds['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"SEED: {seed} ,FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"RankGauss_{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"RankGauss_{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","f0905529":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","3453d7e0":"%%time\n\nSEED = [34, 14, 75]  \noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","5935b1e3":"torch.cuda.empty_cache()","2084289f":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)","ebf33caa":"sub_gauss = sub_rankgauss.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub_gauss","d47b2728":"!pip install --no-index --find-links \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","fbbca441":"import copy\nimport tqdm\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n\nfrom scipy import stats\n\n### Machine Learning ###\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom pickle import load,dump\n\n### Deep Learning ###\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n# Tabnet \nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor","cdd6d75e":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ndf = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","1d7ffe05":"train_features2=train_features.copy()\ntest_features2=test_features.copy()","cd1e973c":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","3d7b26c9":"qt = QuantileTransformer(n_quantiles=100,random_state=42,output_distribution='normal')\ntrain_features[GENES+CELLS] = qt.fit_transform(train_features[GENES+CELLS])\ntest_features[GENES+CELLS] = qt.transform(test_features[GENES+CELLS])","739a7ab8":"seed = 42\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nset_seed(seed)","c0b9a9a9":"# GENES\nn_comp = 600  \ngpca= load(open('..\/input\/moa-tabnet-train-inference\/gpca.pkl', 'rb'))\ntrain2= (gpca.transform(train_features[GENES]))\ntest2 = (gpca.transform(test_features[GENES]))\n\ntrain_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train_gpca), axis=1)\ntest_features = pd.concat((test_features, test_gpca), axis=1)","59072bc6":"#CELLS\nn_comp = 50 \n\ncpca= load(open('..\/input\/moa-tabnet-train-inference\/cpca.pkl', 'rb'))\ntrain2= (cpca.transform(train_features[CELLS]))\ntest2 = (cpca.transform(test_features[CELLS]))\n\ntrain_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train_cpca), axis=1)\ntest_features = pd.concat((test_features, test_cpca), axis=1)","d25c02ca":"from sklearn.feature_selection import VarianceThreshold\n\nc_n = [f for f in list(train_features.columns) if f not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\nmask = (train_features[c_n].var() >= 0.85).values\ntmp = train_features[c_n].loc[:, mask]\ntrain_features = pd.concat([train_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\ntmp = test_features[c_n].loc[:, mask]\ntest_features = pd.concat([test_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)","cff75446":"from sklearn.cluster import KMeans\ndef fe_cluster_genes(train, test, n_clusters_g = 22, SEED = 42):\n    \n    features_g = GENES\n    #features_c = CELLS\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        kmeans_genes = load(open('..\/input\/moa-tabnet-train-inference\/kmeans_genes.pkl', 'rb'))\n        train[f'clusters_{kind}'] = kmeans_genes.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans_genes.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n   # train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster_genes(train_features2,test_features2)","d0825421":"def fe_cluster_cells(train, test, n_clusters_c = 4, SEED = 42):\n    \n    #features_g = GENES\n    features_c = CELLS\n    \n    def create_cluster(train, test, features, kind = 'c', n_clusters = n_clusters_c):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        kmeans_cells = load(open('..\/input\/moa-tabnet-train-inference\/kmeans_cells.pkl', 'rb'))\n        train[f'clusters_{kind}'] = kmeans_cells.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans_cells.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster_cells(train_features2,test_features2)","90a8aecc":"train_pca=pd.concat((train_gpca,train_cpca),axis=1)\ntest_pca=pd.concat((test_gpca,test_cpca),axis=1)","88a41ce1":"def fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n        kmeans_pca = load(open('..\/input\/moa-tabnet-train-inference\/kmeans_pca.pkl', 'rb'))\n        train[f'clusters_pca'] = kmeans_pca.predict(train.values)\n        test[f'clusters_pca'] = kmeans_pca.predict(test.values)\n        train = pd.get_dummies(train, columns = [f'clusters_pca'])\n        test = pd.get_dummies(test, columns = [f'clusters_pca'])\n        return train, test\ntrain_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)","620ebc37":"train_cluster_pca = train_cluster_pca.iloc[:,650:]\ntest_cluster_pca = test_cluster_pca.iloc[:,650:]","65b8a302":"train_features_cluster=train_features2.iloc[:,876:]\ntest_features_cluster=test_features2.iloc[:,876:]","ef8f87bb":"gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']","e4666c8f":"def fe_stats(train, test):\n    \n    features_g = GENES\n    features_c = CELLS\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n        df['c52_c42'] = df['c-52'] * df['c-42']\n        df['c13_c73'] = df['c-13'] * df['c-73']\n        df['c26_c13'] = df['c-23'] * df['c-13']\n        df['c33_c6'] = df['c-33'] * df['c-6']\n        df['c11_c55'] = df['c-11'] * df['c-55']\n        df['c38_c63'] = df['c-38'] * df['c-63']\n        df['c38_c94'] = df['c-38'] * df['c-94']\n        df['c13_c94'] = df['c-13'] * df['c-94']\n        df['c4_c52'] = df['c-4'] * df['c-52']\n        df['c4_c42'] = df['c-4'] * df['c-42']\n        df['c13_c38'] = df['c-13'] * df['c-38']\n        df['c55_c2'] = df['c-55'] * df['c-2']\n        df['c55_c4'] = df['c-55'] * df['c-4']\n        df['c4_c13'] = df['c-4'] * df['c-13']\n        df['c82_c42'] = df['c-82'] * df['c-42']\n        df['c66_c42'] = df['c-66'] * df['c-42']\n        df['c6_c38'] = df['c-6'] * df['c-38']\n        df['c2_c13'] = df['c-2'] * df['c-13']\n        df['c62_c42'] = df['c-62'] * df['c-42']\n        df['c90_c55'] = df['c-90'] * df['c-55']\n        \n        \n        for feature in features_c:\n             df[f'{feature}_squared'] = df[feature] ** 2     \n                \n        for feature in gsquarecols:\n            df[f'{feature}_squared'] = df[feature] ** 2        \n        \n    return train, test\n\ntrain_features2,test_features2=fe_stats(train_features2,test_features2)","554d5d0c":"train_features_stats=train_features2.iloc[:,902:]\ntest_features_stats=test_features2.iloc[:,902:]","ad2e073b":"train_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\ntest_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)","adfdc9dd":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","e83802ee":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","e568afbe":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","b2a51d8f":"target=target[target_cols]","5df601a6":"train = pd.get_dummies(train, columns=['cp_time','cp_dose'])\ntest_ = pd.get_dummies(test, columns=['cp_time','cp_dose'])","4184cd96":"feature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id']]","704211b7":"train = train[feature_cols]\ntest = test_[feature_cols]","cfa58cf0":"X_test = test.values","558ff3fe":"class LogitsLogLoss(Metric):\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        logits = 1 \/ (1 + np.exp(-y_pred))\n        aux = (1 - y_true) * np.log(1 - logits + 5e-5) + y_true * np.log(logits + 5e-5)\n        return np.mean(-aux)","e90283fa":"MAX_EPOCH = 200\n\ntabnet_params = dict(\n    n_d = 32,\n    n_a = 32,\n    n_steps = 1,\n    gamma = 1.3,\n    lambda_sparse = 0,\n    optimizer_fn = optim.Adam,\n    optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n    mask_type = \"entmax\",\n    scheduler_params = dict(mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n    scheduler_fn = ReduceLROnPlateau,\n    seed = seed,\n    verbose = 10\n)","2a09b713":"%%time\ntest_cv_preds = []\n\nNB_SPLITS = 10\nmskf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = 0, shuffle = True)\nSEED = [20,21,22]\nfor s in SEED:\n    tabnet_params['seed'] = s\n    for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, target)):\n        \n        model = TabNetRegressor()\n        ### Predict on test ###\n        model.load_model(f\"..\/input\/moa-tabnet-train-inference\/TabNet_seed_{s}_fold_{fold_nb+1}.zip\")\n        preds_test = model.predict(X_test)\n        test_cv_preds.append(1 \/ (1 + np.exp(-preds_test)))\n\ntest_preds_all = np.stack(test_cv_preds)","66b69ed4":"all_feat = [col for col in df.columns if col not in [\"sig_id\"]]\n# To obtain the same lenght of test_preds_all and submission\ntest = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\nsig_id = test[test[\"cp_type\"] != \"ctl_vehicle\"].sig_id.reset_index(drop = True)\ntmp = pd.DataFrame(test_preds_all.mean(axis = 0), columns = all_feat)\ntmp[\"sig_id\"] = sig_id\n\nsub_tabnet = pd.merge(test[[\"sig_id\"]], tmp, on = \"sig_id\", how = \"left\")\nsub_tabnet.fillna(0, inplace = True)\nsub_tabnet","4e5d6c71":"from sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras import layers,regularizers,Sequential,Model,backend,callbacks,optimizers,metrics,losses","85a68b56":"# Import train data, drop sig_id, cp_type\n\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_scored = train_targets_scored.drop('sig_id',axis=1)\nlabels_train = train_targets_scored.values\n\n# Drop training data with ctl vehicle\n\ntrain_features = train_features.iloc[non_ctl_idx]\nlabels_train = labels_train[non_ctl_idx]\n\n# Import test data\n\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntest_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\n\n# Import predictors from public kernel\n\njson_file_path = '..\/input\/t-test-pca-rfe-logistic-regression\/main_predictors.json'\n\nwith open(json_file_path, 'r') as j:\n    predictors = json.loads(j.read())\n    predictors = predictors['start_predictors']","77a52c73":"# Create g-mean, c-mean, genes_pca (2 components), cells_pca (all components)\n\ncs = train_features.columns.str.contains('c-')\ngs = train_features.columns.str.contains('g-')\n\ndef preprocessor(train,test):\n    # PCA\n    n_gs = 2 # No of PCA comps to include\n    n_cs = 100 # No of PCA comps to include\n    \n    pca_cs = PCA(n_components = n_cs)\n    pca_gs = PCA(n_components = n_gs)\n\n    train_pca_gs = pca_gs.fit_transform(train[:,gs])\n    train_pca_cs = pca_cs.fit_transform(train[:,cs])\n    test_pca_gs = pca_gs.transform(test[:,gs])\n    test_pca_cs = pca_cs.transform(test[:,cs])\n    \n    # c-mean, g-mean\n    \n    train_c_mean = train[:,cs].mean(axis=1)\n    test_c_mean = test[:,cs].mean(axis=1)\n    train_g_mean = train[:,gs].mean(axis=1)\n    test_g_mean = test[:,gs].mean(axis=1)\n    \n    # Append Features\n    \n    train = np.concatenate((train,train_pca_gs,train_pca_cs,train_c_mean[:,np.newaxis]\n                            ,train_g_mean[:,np.newaxis]),axis=1)\n    test = np.concatenate((test,test_pca_gs,test_pca_cs,test_c_mean[:,np.newaxis],\n                           test_g_mean[:,np.newaxis]),axis=1)\n    \n    # Scaler for numerical values\n\n    # Scale train data\n    scaler = preprocessing.StandardScaler()\n\n    train = scaler.fit_transform(train)\n\n    # Scale Test data\n    test = scaler.transform(test)\n    \n    return train, test","63729501":"n_labels = train_targets_scored.shape[1]\nn_train = train_features.shape[0]\nn_test = test_features.shape[0]\n\n\n# Prediction Clipping Thresholds\n\np_min = 0.0005\np_max = 0.9995\n\n# OOF Evaluation Metric with clipping and no label smoothing\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))","5926a865":"def build_model(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(512, activation=\"elu\"), \n        layers.BatchNormalization(),\n        layers.Dense(256, activation = \"elu\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = layers.Concatenate()([input_2, input_3])\n\n    head_2 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(512, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(512, \"elu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"elu\")\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = layers.Average()([input_3, input_4]) \n\n    head_3 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_avg)\n\n\n    model = Model(inputs = [input_1, input_2], outputs = output)\n    model.compile(optimizer='adam', loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n    \n    return model","d9fcf5ec":"%%time\n\nn_seeds = 5\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\n# Training Loop\n\nn_folds = 10\ny_pred = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nfor seed in seeds:\n    fold = 0\n    kf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in kf.split(train_features):\n        print(f'FOLD: {fold}, SEED: {seed}')\n        X_train, X_test = preprocessor(train_features.iloc[train].values,\n                                       train_features.iloc[test].values)\n        _,data_test = preprocessor(train_features.iloc[train].values,\n                                   test_features.drop('cp_type',axis=1).values)\n        X_train_2 = train_features.iloc[train][predictors].values\n        X_test_2 = train_features.iloc[test][predictors].values\n        data_test_2 = test_features[predictors].values\n        y_train = labels_train[train]\n        y_test = labels_train[test]\n        n_features = X_train.shape[1]\n        n_features_2 = X_train_2.shape[1]\n\n        model = build_model(n_features, n_features_2, n_labels)\n        \n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=2, mode='min', min_lr=1E-5)\n        early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=10, mode='min',restore_best_weights=True)\n\n        hist = model.fit([X_train,X_train_2],y_train, batch_size=128, epochs=182,verbose=0,validation_data = ([X_test,X_test_2],y_test),\n                         callbacks=[reduce_lr, early_stopping])\n        hists.append(hist)\n        \n        # Save Model\n        model.save('ResNet_seed_'+str(seed)+'_fold_'+str(fold))\n\n        # OOF Score\n        y_val = model.predict([X_test,X_test_2])\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))\/(n_folds*n_seeds)\n\n        # Run prediction\n        y_pred += model.predict([data_test,data_test_2])\/(n_folds*n_seeds)\n\n        fold += 1\n    tf.keras.backend.clear_session()\n    del model, X_train, X_test, X_train_2, X_test_2","d9342a15":"tf.print('\\nOOF score is ',oof)","09a4140c":"sub_resnet = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nsub_resnet.iloc[:,1:] = np.clip(y_pred,p_min,p_max)\n# sub_resnet.iloc[:,1:] = y_pred[0]\n\n# Set ctl_vehicle to 0\nsub_resnet.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\nsub_resnet","88089daf":"%%time\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_features = train_features.drop(['sig_id','cp_type'],axis=1)\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_scored = train_targets_scored.drop('sig_id',axis=1)\nlabels_train = train_targets_scored.values\n\n# Drop training data with ctl vehicle\n\ntrain_features = train_features.iloc[non_ctl_idx]\nlabels_train = labels_train[non_ctl_idx]\n\n# Import test data\n\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntest_features = test_features.drop('sig_id',axis=1)","d00b2044":"# Label Encoder for categorical cp_dose\n\ncat = 'cp_dose'\nle = preprocessing.LabelEncoder()\nle.fit(train_features[cat])\ntrain_features[cat] = le.transform(train_features[cat])\n\n# Transform categorical\n\ntest_features[cat] = le.transform(test_features[cat])","73c855ef":"# Fit scaler to joint train and test data\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(train_features.append(test_features.drop('cp_type',axis=1)))\n\n# Scale train data\ndata_train = scaler.transform(train_features)\n\n# Scale test data\ndata_test = scaler.transform(test_features.drop('cp_type',axis=1))","3382828e":"%%time\nn_labels = train_targets_scored.shape[1]\nn_features = data_train.shape[1]\nn_train = data_train.shape[0]\nn_test = data_test.shape[0]\n\n\n# Prediction Clipping Thresholds\n\np_min = 0.001\np_max = 0.999\n\n# Evaluation Metric with clipping and no label smoothing\n\n@tf.autograph.experimental.do_not_convert\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))\n\n\n# Generate Seeds\n\nn_seeds = 6\n\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\n# Training Loop\n\nn_folds = 5\ny_pred = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nfor seed in seeds:\n    fold = 0\n    mskf = MultilabelStratifiedKFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in mskf.split(data_train,labels_train):\n        \n        X_train = data_train[train]\n        X_test = data_train[test]\n        y_train = labels_train[train]\n        y_test = labels_train[test]\n\n        # Define NN Model\n\n        model = Sequential()\n        model.add(layers.Dense(2048, input_shape=(n_features,)))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.3))\n        model.add(layers.Dense(1024))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.3))\n        model.add(layers.Dense(512))\n        model.add(layers.Activation('relu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.3))        \n        model.add(layers.Dense(n_labels))\n        model.add(layers.Activation('sigmoid')) \n        model.compile(optimizer='adam', loss=losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.3, patience=5, mode='min', min_lr=1E-5)\n        early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True)\n\n        hist = model.fit(X_train,y_train, batch_size=128, epochs=182,verbose=0,validation_data = (X_test,y_test),callbacks=[reduce_lr, early_stopping])\n        hists.append(hist)\n        \n        # Save Model\n        model.save('LabelSmooth_seed_'+str(seed)+'_fold_'+str(fold))\n\n        # OOF Score\n        y_val = model.predict(X_test)\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))\/(n_folds*n_seeds)\n\n        # Run prediction\n        y_pred += model.predict(data_test)\/(n_folds*n_seeds)\n\n        fold += 1\n\ntf.print('OOF score is ',oof)","62101df1":"sub_smooth = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nsub_smooth.iloc[:,1:] = np.clip(y_pred,p_min,p_max)\n\nsub_smooth.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\nsub_smooth","80bca964":"from xgboost import XGBClassifier\nfrom category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss\nfrom sklearn.multioutput import MultiOutputClassifier\n\nSEED = 34\nNFOLDS = 5\nDATA_DIR = '\/kaggle\/input\/lish-moa\/'\nnp.random.seed(SEED)\n\ntrain = pd.read_csv(DATA_DIR + 'train_features.csv')\nnon_ctl_idx = train.loc[train['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_drug = pd.read_csv(DATA_DIR + 'train_drug.csv')\ntargets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\ntest = pd.read_csv(DATA_DIR + 'test_features.csv')\nsub_xgb = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n\n# drop where cp_type==ctl_vehicle (baseline)\nctl_mask = train.cp_type=='ctl_vehicle'\ntrain = train[~ctl_mask]\ntargets = targets[~ctl_mask]\n\n# drop id col\nX = train.iloc[:,1:].to_numpy()\nX_test = test.iloc[:,1:].to_numpy()\ny = targets.iloc[:,1:].to_numpy() \n\nclf = Pipeline([('encode', CountEncoder(cols=[0, 2])),\n                ('classify', MultiOutputClassifier(\n                                 XGBClassifier(tree_method='gpu_hist')))\n               ])\n\nparams = {'classify__estimator__colsample_bytree': 0.6522,\n          'classify__estimator__gamma': 3.6975,\n          'classify__estimator__learning_rate': 0.0503,\n          'classify__estimator__max_delta_step': 2.0706,\n          'classify__estimator__max_depth': 10,\n          'classify__estimator__min_child_weight': 31.5800,\n          'classify__estimator__n_estimators': 166,\n          'classify__estimator__subsample': 0.8639\n         }\n\n_ = clf.set_params(**params)","584507cb":"def assign_folds(seed,fold):\n    scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n    targets = scored.columns[1:]\n    scored = scored.merge(train_drug, on='sig_id', how='left') \n    scored = scored.iloc[non_ctl_idx]\n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc<=18].index.sort_values()\n    vc2 = vc.loc[vc>18].index.sort_values()\n\n    # STRATIFY DRUGS 18X OR LESS\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits=fold, shuffle=True, \n              random_state=seed)\n    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 18X\n    skf = MultilabelStratifiedKFold(n_splits=fold, shuffle=True, \n              random_state=seed)\n    tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    # ASSIGN FOLDS\n    scored['fold'] = scored.drug_id.map(dct1)\n    scored.loc[scored.fold.isna(),'fold'] =\\\n        scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n    scored.fold = scored.fold.astype('int8')\n\n    return scored['fold'].values","b5b25e9c":"use_xgb = True","b1af6d3d":"%%time\nif use_xgb:\n    oof_preds = np.zeros(y.shape)\n    test_preds = np.zeros((test.shape[0], y.shape[1]))\n    oof_losses = []\n\n    x = assign_folds(SEED,NFOLDS)\n\n    for n in range(NFOLDS):\n        print(f'Inferencing FOLD: {n}')\n        trn_idx = np.where(x != n)[0]\n        val_idx = np.where(x == n)[0]\n\n        X_train, X_val = X[trn_idx], X[val_idx]\n        y_train, y_val = y[trn_idx], y[val_idx]\n\n        # drop where cp_type==ctl_vehicle (baseline)\n        ctl_mask = X_train[:,0]=='ctl_vehicle'\n        X_train = X_train[~ctl_mask,:]\n        y_train = y_train[~ctl_mask]\n\n        clf = pickle.load(open(f'..\/input\/moa-xgb-weights\/xgb_cv_fold_{n}.dat', \"rb\"))\n\n        val_preds = clf.predict_proba(X_val) # list of preds per class\n        val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n        oof_preds[val_idx] = val_preds\n\n        loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n        oof_losses.append(loss)\n        preds = clf.predict_proba(X_test)\n        preds = np.array(preds)[:,:,1].T # take the positive class\n        test_preds += preds \/ NFOLDS\n\n    print(oof_losses)\n    print('Mean OOF loss across folds', np.mean(oof_losses))\n    print('STD OOF loss across folds', np.std(oof_losses))","1cca6366":"control_mask = train['cp_type']=='ctl_vehicle'\noof_preds[control_mask] = 0\n\nprint('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))","7cada736":"control_mask = test['cp_type']=='ctl_vehicle'\ntest_preds[control_mask] = 0\n\nsub_xgb.iloc[:,1:] = test_preds\nsub_xgb","7abd7282":"%%time\nids = sub_tabnet[['sig_id']]\n\nsub_tabnet.drop('sig_id',inplace=True,axis=1)\nsub_resnet.drop('sig_id',inplace=True,axis=1)\nsub_smooth.drop('sig_id',inplace=True,axis=1)\nsub_xgb.drop('sig_id',inplace=True,axis=1)\nsub_gauss.drop('sig_id',inplace=True,axis=1) \n\nsub_en = 0.3*sub_gauss + 0.3*sub_tabnet + 0.25*sub_resnet +  0.1*sub_smooth +0.05*sub_xgb\nsub_en['sig_id'] = ids[['sig_id']]\ncols = sub_en.columns.tolist()\ncols.insert(0, cols.pop(cols.index('sig_id')))\nsub_en = sub_en.reindex(columns=cols)\nsub_en.to_csv('submission.csv',index=False)\nsub_en","5017b630":"# Ensemble","d4e84317":"# Label Smoothing","a5709ce6":"# ResNet","8dc955f6":"# Credits to the orginal Authors\nRankGauss: https:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-feature-engineering-0-01846  \nTabNet: https:\/\/www.kaggle.com\/ludovick\/introduction-to-tabnet-kfold-10-training  \nResNet: https:\/\/www.kaggle.com\/rahulsd91\/moa-multi-input-resnet-model  \nLabelSmooth: https:\/\/www.kaggle.com\/rahulsd91\/moa-label-smoothing   \nXGB: https:\/\/www.kaggle.com\/fchmiel\/xgboost-baseline-multilabel-classification  ","4c34a7cb":"# Rank Gauss","b3e98b3c":"# XGB","96f46b34":"# TabNet"}}