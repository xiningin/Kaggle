{"cell_type":{"41a9581f":"code","38c0fe9b":"code","c8684595":"code","5fef11fa":"code","272964aa":"code","1f052f7c":"code","070a8376":"code","37d600df":"code","449a11c8":"code","201da1ad":"code","c5ec67e6":"code","6053e8e3":"code","282978e0":"code","bc2c4f0f":"code","e11cbc1e":"code","513a5d18":"code","b0804139":"code","06f49ac4":"code","1ee5d1fa":"code","3c1fc9d4":"code","c0fc1442":"code","89408544":"code","2acd885b":"code","83380a68":"code","77c9c20c":"code","33f79caa":"code","9f13a3b3":"code","4f2f62ab":"code","74416a05":"code","786dd718":"code","e9142ed4":"code","ee0fd31b":"code","adce348c":"code","9a37ea70":"code","d121ee81":"code","2ea7743c":"code","98d142d5":"code","dd5ec043":"code","c4689bac":"code","8a76cc29":"code","f1295391":"code","d2ef8782":"code","a52321dc":"code","404b44cb":"code","5ed8e7ac":"code","5e6b38c6":"code","59c5e8be":"code","9d2fa3fc":"code","52c6e9a4":"code","d923b643":"code","f5c0b278":"code","7e2cb17c":"code","70ae376f":"code","cc85bbc5":"code","2ad3bb08":"code","af4933e7":"code","4e15eb7e":"code","1cdacb44":"code","bf3cc68b":"code","61053114":"code","be465b8a":"code","db362ea6":"code","2d2779d9":"code","05cfe679":"code","7c421cec":"code","3bb3d926":"code","831e0fd6":"code","ad6a8544":"code","f4ddc10d":"code","20b63f87":"code","1f5e4a04":"code","04a8a6ee":"code","f832b887":"code","198ceb34":"code","5cd10b3d":"code","ac1473ce":"code","0b684b84":"markdown","90c32aa4":"markdown","1cca68f8":"markdown","679dada2":"markdown","eb49daeb":"markdown","f15a08e1":"markdown","646e7998":"markdown","7106fa74":"markdown","22ed2495":"markdown","32cfa77d":"markdown","8757fe5e":"markdown","0c3a8244":"markdown","64343cb7":"markdown","9255a2b1":"markdown","37279418":"markdown","817f31fa":"markdown","de29494e":"markdown","e847ca89":"markdown","645bc73f":"markdown","a6981fc0":"markdown","553d4ad0":"markdown","7a17bd68":"markdown","3aafaba0":"markdown","e7d3ccdd":"markdown","11ec66c2":"markdown","531a43e0":"markdown","d2cdb759":"markdown","3c4a56a8":"markdown","4de47518":"markdown","9ee63cd6":"markdown","0376a146":"markdown","85721a32":"markdown","503b2863":"markdown","4a779bfd":"markdown","9d52c3ba":"markdown","d77a9226":"markdown","401ba93e":"markdown","4f77fbc9":"markdown","cfde3fac":"markdown","d4f17fd0":"markdown","67bfe020":"markdown","b467a04d":"markdown","9d548502":"markdown","e0ec26d7":"markdown","777d5eca":"markdown","71e9ca7c":"markdown","2b45b8b6":"markdown","d01bb5b2":"markdown","3a4b82a5":"markdown","7f18b850":"markdown","38509683":"markdown","af846699":"markdown","f2b002cf":"markdown","7d3d2551":"markdown","f3b0780a":"markdown","7738009b":"markdown","1e48b24f":"markdown","3410b574":"markdown"},"source":{"41a9581f":"# !pip install catboost\n# !pip install featuretools\n\n# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Importing necessary libraries\n\nimport os\nimport chardet\n\nimport numpy as np\nimport pandas as pd\n\n# visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# Model analysis and building libraries\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.impute import KNNImputer\nimport featuretools as ft\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostClassifier\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\nfrom imblearn.over_sampling import SMOTE, SMOTENC\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom sklearn.metrics import roc_auc_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Sequential, optimizers","38c0fe9b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c8684595":"# Encoding is in standard ascii format\n\ntrain = pd.read_csv('\/kaggle\/input\/jobathon-may-2021-credit-card-lead-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/jobathon-may-2021-credit-card-lead-prediction\/test.csv')\n\n# Copy of train set for EDA\ndf = train.copy()\ntest_df = test.copy()","5fef11fa":"# Number of features and corresponding records\n\nprint('Training data shape: ',train.shape)\nprint('Testing data shape: ',test.shape)","272964aa":"df.head(5)","1f052f7c":"# Distribution of features data types\n\ndf.dtypes","070a8376":"# Feature Analysis\n\ndf.describe(include='all').T","37d600df":"# Checking the Null values\n\ndf.info()","449a11c8":"# Percentage of Null values in each column\n\nround(df.isnull().sum()\/len(df)*100, 2)","201da1ad":"print('Null values: ', df.Credit_Product.isnull().sum())","c5ec67e6":"ax = sns.countplot(df.Credit_Product, hue=df.Is_Lead)\nax.set_title('Credit Product Distribution')\nplt.show()","6053e8e3":"# Dropping Null values\n\ndf = df[~df['Credit_Product'].isna()]","282978e0":"print('Total Number of features: ', len(df.ID))\nprint('Total Number of Unique features: ', len(df.ID.unique()))","bc2c4f0f":"# Dropping the ID feature\n\ndf = df.drop(['ID'], axis=1)\ntest_df = test_df.drop(['ID'], axis=1)","e11cbc1e":"test_df.head(3)","513a5d18":"df.Credit_Product = df.Credit_Product.map({'Yes':1,'No':0})\ntest_df.Credit_Product = test_df.Credit_Product.map({'Yes':1,'No':0})","b0804139":"df.Gender.value_counts()","06f49ac4":"ax = sns.countplot(x=df.Gender, hue=df.Is_Lead)\nax.set_title('Gender Distribution in Dataset')\nplt.show()","1ee5d1fa":"dummy_encoding = pd.get_dummies(df['Gender'], drop_first=True)\n\n# Concatinating with existing dataframe\ndf = pd.concat([df, dummy_encoding], axis=1)\n\n# Drop parent category column which are encoded \ndf = df.drop(['Gender'] , axis=1)\n\ndummy_encoding = pd.get_dummies(test_df['Gender'], drop_first=True)\n\n# Concatinating with existing dataframe\ntest_df = pd.concat([test_df, dummy_encoding], axis=1)\n\n# Drop parent category column which are encoded \ntest_df = test_df.drop(['Gender'] , axis=1)","3c1fc9d4":"df.head()","c0fc1442":"df.Occupation.value_counts()","89408544":"f,ax = plt.subplots(nrows=1,ncols=2,figsize=(15,6))\nsns.countplot(df.Occupation, hue=df.Is_Lead, ax=ax[0])\nax[0].set_title('Occupation Distribution')\nsns.countplot(df[df.Occupation=='Entrepreneur'].Occupation, hue=df.Is_Lead, ax=ax[1])\nax[1].set_title('Entrepreneur Distribution')\nplt.tight_layout()","2acd885b":"dummy_encoding = pd.get_dummies(df['Occupation'], drop_first=True)\n\n# Concatinating with existing dataframe\ndf = pd.concat([df, dummy_encoding], axis=1)\n\n# Drop parent category column which are encoded \ndf = df.drop(['Occupation'] , axis=1)\n\ndummy_encoding = pd.get_dummies(test_df['Occupation'], drop_first=True)\n\n# Concatinating with existing dataframe\ntest_df = pd.concat([test_df, dummy_encoding], axis=1)\n\n# Drop parent category column which are encoded \ntest_df = test_df.drop(['Occupation'] , axis=1)","83380a68":"df.head()","77c9c20c":"df.Channel_Code.value_counts()","33f79caa":"ax = sns.countplot(df.Channel_Code)\nax.set_title('Channel Distribution')\nplt.show()","9f13a3b3":"dummy_encoding = pd.get_dummies(df['Channel_Code'], drop_first=True)\n\n# Concatinating with existing dataframe\ndf = pd.concat([df, dummy_encoding], axis=1)\n\n# Drop parent category column which are encoded \ndf = df.drop(['Channel_Code'] , axis=1)\n\ndummy_encoding = pd.get_dummies(test_df['Channel_Code'], drop_first=True)\n\n# Concatinating with existing dataframe\ntest_df = pd.concat([test_df, dummy_encoding], axis=1)\n\n# Drop parent category column which are encoded \ntest_df = test_df.drop(['Channel_Code'] , axis=1)","4f2f62ab":"ax = sns.countplot(df.Is_Active, hue=df.Is_Lead)\nax.set_title('Distribution of Is_Active')\nplt.show()","74416a05":"# Binary Encoding to convert categorical values to numerical values\n\ndf.Is_Active = df.Is_Active.map({'Yes': 1, 'No': 0})\ntest_df.Is_Active = test_df.Is_Active.map({'Yes': 1, 'No': 0})","786dd718":"df.head()","e9142ed4":"plt.figure(figsize=(15,6))\nax = sns.countplot(df.Region_Code, hue=df.Is_Lead)\nax.set_title('Distribution of Region Code')\nplt.xticks(rotation=45)\nplt.show()","ee0fd31b":"rc_encoding = df.groupby('Region_Code')['Is_Lead'].mean().reset_index()\n\nplt.figure(figsize=(15,6))\nax = sns.barplot(x='Region_Code', y='Is_Lead', data=rc_encoding.sort_values(by=['Is_Lead'], ascending=False));\nax.set_title('Lead Probability Distribution of Region Code')\nplt.xticks(rotation=45)\nplt.show()","adce348c":"# Will convert categorical variables to region lead probability\n\n# Dictionary to map\nrc_enc_dict = {rc[0]:rc[1]for rc in rc_encoding.values.tolist()}\n\ndf.Region_Code = df.Region_Code.map(rc_enc_dict)\ntest_df.Region_Code = test_df.Region_Code.map(rc_enc_dict)","9a37ea70":"df.head()","d121ee81":"plt.figure(figsize=(12,6))\nax = sns.distplot(df.Avg_Account_Balance\/10000)\nax.set_title('Distribution of Average Account Balance (10k scale)')\nplt.show()","2ea7743c":"plt.figure(figsize=(10,6))\nax = sns.boxplot(df.Avg_Account_Balance)\nax.set_title('Distribution of Average Account Balance ')\nplt.show()","98d142d5":"f,ax = plt.subplots(nrows=2,ncols=1,figsize=(12,6))\nsns.distplot(df.Vintage, ax=ax[0])\nax[0].set_title('Distribution of Vintage')\nsns.boxplot(df.Vintage, ax=ax[1])\nax[1].set_title('Distribution of Vintage')\nplt.tight_layout()","dd5ec043":"plt.figure(figsize=(12, 6))\nax = sns.distplot(df.Age)\nax.set_title('Distribution of Age')\nplt.show()","c4689bac":"tree_model = DecisionTreeClassifier(max_depth=2)\ntree_model.fit(df.Age.to_frame(), df.Is_Lead)\ndf['Age_tree']=tree_model.predict_proba(df.Age.to_frame())[:,1] ","8a76cc29":"fig = plt.figure()\nfig = df.groupby(['Age_tree'])['Is_Lead'].mean().plot()\nfig.set_title('Monotonic relationship between discretised Age and Lead')\nfig.set_ylabel('Lead')\nplt.show()\n\n# Monotonic Relationnship is a good predictor indication","f1295391":"df.groupby(['Age_tree'])['Is_Lead'].count()","d2ef8782":"# Binning the Age into 4 differnt categories\n\ndef bin_age(x):\n  if x in range(0,34): return 'Age_23_33'\n  if x in range(34,36): return 'Age_34_35'\n  if x in range(36,42): return 'Age_36_41'\n  if x in range(42,100): return 'Age_42_85'\n\ndf.Age = df.Age.apply(lambda x : bin_age(x))\ntest_df.Age = test_df.Age.apply(lambda x : bin_age(x))","a52321dc":"dummy_encoding = pd.get_dummies(df['Age'], drop_first=True)\n\n# Concatinating with existing dataframe\ndf = pd.concat([df, dummy_encoding], axis=1)\n\n# Drop parent category column which are encoded \ndf = df.drop(['Age','Age_tree'] , axis=1)\n\ndummy_encoding = pd.get_dummies(test_df['Age'], drop_first=True)\n\n# Concatinating with existing dataframe\ntest_df = pd.concat([test_df, dummy_encoding], axis=1)\n\n# Drop parent category column which are encoded \ntest_df = test_df.drop(['Age'] , axis=1)","404b44cb":"plt.figure(figsize=(6,4))\nax = sns.countplot(df.Is_Lead)\nax.set_title('Distribution of Is Lead')\nplt.show()","5ed8e7ac":"# Get percentage of Lead\n\ndf.Is_Lead.mean()*100","5e6b38c6":"y = df.pop('Is_Lead')","59c5e8be":"# SMOTE should be applied for Trainset or else both test and train will overfit\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.20, random_state=42)","9d2fa3fc":"# For Oversampling\nsm = SMOTE(random_state=5)\n\n# For Categorical encoder\n#sm = SMOTENC(random_state=42, categorical_features=[2,4,5,6,7,8,9,10,11,12,13,14])\n\n# For Undersampling\n#sm = RandomUnderSampler(random_state=42)\n\nSX_train, Sy_train = sm.fit_resample(X_train, y_train)\nSX_train = pd.DataFrame(SX_train, columns=X_test.columns)","52c6e9a4":"dff = SX_train.copy()\ndft = test_df.copy()\n\ndff[['Vintage','Avg_Account_Balance']] = MinMaxScaler().fit_transform(dff[['Vintage','Avg_Account_Balance']]) \ndft[['Vintage','Avg_Account_Balance']] = MinMaxScaler().fit_transform(dft[['Vintage','Avg_Account_Balance']]) ","d923b643":"imputer = KNNImputer(n_neighbors=2)\ntestset = imputer.fit_transform(dft)","f5c0b278":"dff[['Credit_Product','Is_Active','Male','Other','Salaried','Self_Employed','X2','X3','X4','Age_34_35','Age_36_41','Age_42_85']] = dff[['Credit_Product','Is_Active','Male','Other','Salaried','Self_Employed','X2','X3','X4','Age_34_35','Age_36_41','Age_42_85']].astype(int)","7e2cb17c":"cat = CatBoostClassifier(learning_rate=0.05, \n                         l2_leaf_reg=1, \n                         iterations= 500, \n                         depth= 9, \n                         border_count= 20, \n                         eval_metric = 'AUC')\n\ncat= cat.fit(X_train, y_train,cat_features=['X2','X3','X4','Age_34_35','Age_36_41','Age_42_85','Credit_Product', 'Other', 'Salaried', 'Self_Employed', 'Is_Active'],eval_set=(X_test, y_test),early_stopping_rounds=70,verbose=50)\n","70ae376f":"# cat = CatBoostClassifier(eval_metric = 'AUC')\n# param = { 'depth':[3,1,2,6,4,8,9,10,20,30,50],\n#          'iterations':[250,100,500,1000],\n#          'learning_rate':[0.03,0.001,0.01,0.1,0.13,0.2,0.3],\n#          'l2_leaf_reg':[3,1,5,10,100],\n#          'border_count':[32,5,10,20,100,200]\n#         }\n\n# randm = RandomizedSearchCV(cat, param_distributions = param, cv=5,refit = True, n_iter = 10, n_jobs=-1)\n# randm.fit(X_train, y_train, cat_features=['Gender',\t'Age', 'Region_Code',\t'Occupation', 'Channel_Code',\t'Vintage', 'Credit_Product', 'Is_Active'])\n\n# randm.best_params_","cc85bbc5":"cat_y_pred = cat.predict_proba(X_train)[:, 1]\ncat_y_pred2 = cat.predict_proba(X_test)[:, 1]\n\nprint('Train ROC:',roc_auc_score(y_train,cat_y_pred))\nprint('Test ROC:',roc_auc_score(y_test,cat_y_pred2))","2ad3bb08":"cft = testset.copy()\ncft = pd.DataFrame(cft,columns=dft.columns)\ncft[['Credit_Product','Is_Active','Male','Other','Salaried','Self_Employed','X2','X3','X4','Age_34_35','Age_36_41','Age_42_85']] = cft[['Credit_Product','Is_Active','Male','Other','Salaried','Self_Employed','X2','X3','X4','Age_34_35','Age_36_41','Age_42_85']].astype(int)","af4933e7":"cat_pred = cat.predict_proba(cft)[:, 1]","4e15eb7e":"lgb = LGBMClassifier(boosting_type='gbdt',n_estimators=500,depth=10,learning_rate=0.04,\n                     objective='binary',metric='auc',is_unbalance=True,\n                     colsample_bytree=0.5,reg_lambda=2,reg_alpha=2,random_state=42)\n\nlgb= lgb.fit(X_train, y_train,eval_metric='auc',eval_set=(X_test , y_test),verbose=50,categorical_feature=[2,4,5,6,7,8,9,10,11,12,13,14],early_stopping_rounds= 50)\n","1cdacb44":"# lgb = LGBMClassifier(objective='binary',metric='auc',is_unbalance=True)\n# param = { 'depth':[3,1,2,6,4,8,9,10,20,30,50],\n#          'n_estimators':[250,100,500,1000],\n#          'learning_rate':[0.03,0.04,0.1,0.13,0.2,0.3]\n#         }\n\n# grid = GridSearchCV(lgb, param_grid = param, cv=5,refit = True, n_jobs=-1)\n# grid.fit(X_train, y_train, cat_features=categorical_feature=[2,4,5,6,7,8,9,10,11,12,13,14])\n\n# grid.best_params_","bf3cc68b":"lgb_y_pred = lgb.predict_proba(X_train)[:, 1]\nlgb_y_pred2 = lgb.predict_proba(X_test)[:, 1]\n\nprint('Train AUC:',roc_auc_score(y_train,lgb_y_pred))\nprint('Test AUC:',roc_auc_score(y_test,lgb_y_pred2))","61053114":"lgb_pred = lgb.predict_proba(cft)[:, 1]","be465b8a":"xg = XGBClassifier(n_estimators=200, max_depth=3,gamma=1)\nxg.fit(X_train,y_train)\n\nxg_y_pred = xg.predict_proba(X_train)[:, 1]\nxg_y_pred2 = xg.predict_proba(X_test)[:, 1]\n\nprint('Train AUC:',roc_auc_score(y_train,xg_y_pred))\nprint('Test AUC:',roc_auc_score(y_test,xg_y_pred2))","db362ea6":"# xg = XGBClassifier(objective='binary',metric='auc',is_unbalance=True)\n# param = { 'max_depth':[3,1,2,6,4,8,9,10,20,30,50],\n#          'n_estimators':[250,100,500,1000]\n#         }\n\n# grid = GridSearchCV(xg, param_grid = param, cv=5,refit = True, n_jobs=-1)\n# grid.fit(X_train, y_train)\n\n# grid.best_params_","2d2779d9":"xg_pred = xg.predict_proba(cft)[:, 1]","05cfe679":"rf = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=5)\nrf.fit(X_train,y_train)\n\nrf_y_pred = rf.predict_proba(X_train)[:, 1]\nrf_y_pred2 = rf.predict_proba(X_test.values)[:, 1]\n\nprint('ROC Train:',roc_auc_score(y_train,rf_y_pred))\nprint('ROC Test:',roc_auc_score(y_test,rf_y_pred2))","7c421cec":"# rf = RandomForestClassifier(objective='binary',metric='auc',is_unbalance=True)\n# param = { 'max_depth':[3,1,2,6,4,8,9,10,20,30,50],\n#          'n_estimators':[250,100,500,1000],\n#          'min_samples_split': [3,4,5,6,7]\n#         }\n\n# grid = GridSearchCV(rf, param_grid = param, cv=5,refit = True, n_jobs=-1)\n# grid.fit(X_train, y_train)\n\n# grid.best_params_","3bb3d926":"rf_pred = rf.predict_proba(testset)[:, 1]","831e0fd6":"validation_test = pd.DataFrame([y_test.values, rf_y_pred2, xg_y_pred2, cat_y_pred2, lgb_y_pred2]).T\n\nvalidation_test = validation_test.rename(columns={0: 'y_test', 1: 'rf_y_pred2', 2:'xg_y_pred2', 3: 'cat_y_pred2', 4: 'lgb_y_pred2'})\n\nvalidation_test['Mean'] = validation_test.apply(lambda x : pd.Series([x['rf_y_pred2'],x['xg_y_pred2'],x['cat_y_pred2'],x['lgb_y_pred2']]).mean(), axis=1)","ad6a8544":"\nprint('Test Mean',roc_auc_score(validation_test['y_test'],validation_test['Mean']))","f4ddc10d":"# We add features to both train and test set\nfet = testset.copy()\nfef = df.copy()\nfe_y = y.copy()\n\nfet = pd.DataFrame(fet, columns=X_train.columns)\n\nenc = pd.concat([fef,fet],axis=0)\n\nes = ft.EntitySet(id = 'Lead')\nes.entity_from_dataframe(entity_id='ID', dataframe = enc,index='index')\n\nfeature_matrix , feature_defs = ft.dfs(entityset = es, target_entity='ID', trans_primitives = ['add_numeric', 'multiply_numeric'], verbose=True)","20b63f87":"fef = feature_matrix[:len(fef)]\nfet = feature_matrix[len(fef):]","1f5e4a04":"# Stratified Split\n\nX_train, X_test, y_train, y_test = train_test_split(fef, fe_y, stratify=y,test_size=0.20, random_state=42)","04a8a6ee":"model = Sequential()\nmodel.add(layers.Dense(500, activation = 'relu', input_shape = (225,))) \nmodel.add(layers.Dense(450, activation = 'relu')) \nmodel.add(layers.Dense(380, activation = 'relu'))\nmodel.add(layers.Dense(260, activation = 'relu')) \nmodel.add(layers.Dense(150, activation = 'relu'))\nmodel.add(layers.Dense(80, activation = 'relu'))\nmodel.add(layers.Dense(30, activation = 'relu'))\nmodel.add(layers.Dense(15, activation = 'relu'))\nmodel.add(layers.Dense(4, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n\nmodel.compile(optimizer=optimizers.Adam(lr=0.001),  loss='binary_crossentropy',  metrics=['AUC'])","f832b887":"model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test),batch_size=2000, verbose=1)","198ceb34":"model.evaluate(X_test,  y_test, verbose=2)","5cd10b3d":"nn_pred = model.predict(fet)","ac1473ce":"sub = pd.read_csv('sample_submission_eyYijxG.csv')\n\nsub.Is_Lead = xb_pred\n\nsub.to_csv('MySubmission.csv', index=False)","0b684b84":"## Hyperparameter Tuning","90c32aa4":"## Region Code\n\n`Region_Code` : Code of the Region for the customers","1cca68f8":"### SMOTE","679dada2":"* We have outliers in the Average Account Balance feature","eb49daeb":"# Neural Network\n","f15a08e1":"# XG","646e7998":"## Hyperparameter Tuning","7106fa74":"## Is Lead\n\n`Is_Lead`: This is a target variable","22ed2495":"### Normalizing the Data","32cfa77d":"Categorical Features in dataset: 7\n\nNumerical features in dataset: 4","8757fe5e":"# Credit Card Lead Prediction\n\nHappy Customer Bank is a mid-sized private bank that deals in all kinds of banking products, like Savings accounts, Current accounts, investment products, credit products, among other offerings.\n\n\n\nThe bank also cross-sells products to its existing customers and to do so they use different kinds of communication like tele-calling, e-mails, recommendations on net banking, mobile banking, etc. \n\n\n\nIn this case, the Happy Customer Bank wants to cross sell its credit cards to its existing customers. The bank has identified a set of customers that are eligible for taking these credit cards.\n\n\n\nNow, the bank is looking for your help in identifying customers that could show higher intent towards a recommended credit card, given:\n\n* Customer details (gender, age, region etc.)\n* Details of his\/her relationship with the bank (Channel_Code,Vintage, 'Avg_Asset_Value etc.)","0c3a8244":"## Vintage\n\n`Vintage` : Vintage for the Customer (In Months)","64343cb7":"> X4 channel has less weightage in the dataset","9255a2b1":"> Dummy Encoding Categorical variables","37279418":"# Random Forest","817f31fa":"## Is Active\n\n`Is_Active` : If the Customer is Active in last 3 Months","de29494e":"### Train Test Split","e847ca89":"# CAT Boost","645bc73f":"# Conclusion","a6981fc0":"### Imputing Null Values in Test Set\n\n> Using KNN Imputer ","553d4ad0":"# Modelling","7a17bd68":"# Ensemble","3aafaba0":"> There is no bias in the Gender dataset. we do have equal weightage of data of both Male and Female","e7d3ccdd":"> Active Customers have high Probabiity of taking a lead","11ec66c2":"## Data Dictionary \n| Variable      | Definition |\n| ----------- | ----------- |\n| ID      | Unique Identifier for a row |\n| Gender      | Gender of the Customer |\n| Age      | Age of the Customer (in Years) |\n| Region_Code      | Code of the Region for the customers |\n| Occupation      | Occupation Type for the customer |\n| Channel_Code      | Acquisition Channel Code for the Customer  (Encoded) |\n| Vintage      | Vintage for the Customer (In Months) |\n| Credit_Product      | If the Customer has any active credit product (Home loan,Personal loan, Credit Card etc.) |\n| Avg_Account_Balance      | Average Account Balance for the Customer in last 12 Months |\n| Is_Active      | If the Customer is Active in last 3 Months |\n| Is_Lead(Target)      | If the Customer is interested for the Credit Card  {0 : Customer is not interested, 1 : Customer is interested} |","531a43e0":"> we have Imbalanced Dataset\n\n> There is a ratio of 6:1 between lead and non lead","d2cdb759":"> Entrepreneur Occupation has less weightage in the dataset. The reason is Happy Bank being a mid sized bank\n\n> But Being a Entrepreneur has a high probability in taking a lead","3c4a56a8":"**Dummy Encoding**\n\nCategorical variables are dummy encoded by dropping the first column to avoid collinearlity among them","4de47518":"1. Each region has differnt trend \n\n2. Dummy Encoding of Region Code will increase complexity of Model and also preformance will be lowered\n\n3. Will use Lead Probabilty Score of each region instead of categories. \n\n> $probability\\_score = \\frac{no\\_of\\_leads\\_in\\_region}{ no\\_of\\_customers\\_in\\_region}$\n","9ee63cd6":"> As this is a binary feature we need to assign 1\/0 instead of Yes\/No as a Binary Encoding","0376a146":"## Average Account Balance\n\n`Avg_Account_Balance` : Average Account Balance for the Customer in last 12 Months","85721a32":"<center><h1>Credit Card Lead Prediction \ud83d\udcb3\ud83d\udcb3<\/h1>\n    <h2> XGBoost, LBGM, CATBoost, NN Review <\/h2>\n<img class='center' height=\"600\" width=\"800\" src=\"https:\/\/cdn.britannica.com\/02\/160902-050-B58BAD84\/Credit-cards.jpg\">\n<\/center>\n    \n","503b2863":"# Feature Engineering and Preprocessing","4a779bfd":"> Datasplit of 1:4 ratio for modelling and testing","9d52c3ba":"* Started with Vanila SVM, Logistic and KNN Classifiers which performed on the lower side.\n\n* CATboost are selected and hypertuned to optimize the roc_auc score as data is mostly categorical\n\n* Used XGBoost, Random Forest and LightGBM hypertuned to optimize the score but the performance is similar to CATboost\n\n* All three CATBoost, XGBoost, LightGBM and Random Forest showed similar Train and Validation Scores \n\n* This Indicated the the model is not verfitting but suffering from bias.\n\n* Tried Oversampling, Undersampling the data to check if imbalance in data is the cause. But Performance didnt have significant affect\n\n* Tried an ensemble method by Averging all four models the model performnce didn't improve much\n\n* Tried to combine features using Automtic Feature generator and trained the model on Neural Nets but there was no significant affect\n\n* XGBoost seems to have high ROC_AUC on both validation and test set\n\n* XGBoost is selected as final model for predicting on Test Set\n\n\n","d77a9226":"# LGBM","401ba93e":"## Age\n\n`Age`: Age of the Customer (in Years)","4f77fbc9":"## Credit Product\n\n`Credit_Product`: If the Customer has any active credit product (Home loan, Personal loan, Credit Card etc.)\n\n* Need to perform Binary Encoding","cfde3fac":"**Dummy Encoding**\n\nCategorical variables are dummy encoded by dropping the first column to avoid collinearlity among them","d4f17fd0":"## Hyperparameter Tuning","67bfe020":"> Will Select Bins for Age based on Decision Tree","b467a04d":"**Handling Imbalanced Data**\n\nA technique similar to upsampling is to create synthetic samples.\n\nWe will use imblearn\u2019s SMOTE or Synthetic Minority Oversampling Technique.\n\nSMOTE uses a nearest neighbors algorithm to generate new and synthetic data we can use for training our model.\n\nSMOTE will synthesise samples to oversample the minority class by handling overfitting\n\n","9d548502":"* There is a trend in output. Most of the Users with Credit Product has a high probability of taking a lead\n\n* Hence rather than imputing with mode we will discard the records with null values as we have huge data","e0ec26d7":"## Credit Product\n\n`Credit_Product` : If the Customer has any active credit product (Home loan, Personal loan, Credit Card etc.)\n\nWe have `12%` of Null values in credit product\n\nWays to handle Null Values\n\n* Drop the null value rows\n* Impute the Null values\n","777d5eca":"# Data Understanding","71e9ca7c":"## Occupation\n\n`Occupation`: Occupation Type for the customer\n","2b45b8b6":"* We will Try Aggregation of Features to see the improvement in the metrics","d01bb5b2":"## Channel Code\n\n`Channel_Code`: Acquisition Channel Code for the Customer  (Encoded)\n","3a4b82a5":"## Hyperparameter Tuning","7f18b850":"## Gender\n\n`Gender`: Gender of the Customer\n","38509683":"> Checking if Age Tree is a good predictor","af846699":"# Feature Engineering","f2b002cf":"## ID\n\n`ID` : \tUnique Identifier for a row\n\n* This feature will be dropped as there is no trend in the data as every instance is a unique datapoint","7d3d2551":"# Test Dataset","f3b0780a":"> Will use stratified split instead of SMOTE","7738009b":"# Data Cleaning\n","1e48b24f":"* We have `246725` and `105312` rows in Train and Test dataset respectively.\n* We have around `10` independent features in the dataset","3410b574":"We have 12% of null values in `Credit_product` feature"}}