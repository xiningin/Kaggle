{"cell_type":{"a918ead7":"code","688dc290":"code","430e9ff5":"code","d65943b8":"code","51951860":"code","08e8c081":"code","167bf1ed":"code","1a403017":"code","522c165a":"code","370c8aa4":"code","0257b9f7":"code","b3c5a73e":"code","0478a2ca":"code","9d0001d1":"markdown","033606cb":"markdown","1a64ddd2":"markdown","dba32c64":"markdown","948a686d":"markdown","40f634d5":"markdown","36961df3":"markdown"},"source":{"a918ead7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","688dc290":"aapl_df = pd.read_csv(\"..\/input\/nasdaq-and-nyse-stocks-histories\/full_history\/AAPL.csv\")\naapl_df = aapl_df.sort_index(axis=0, ascending=False)\naapl_df[\"date\"] = pd.to_datetime(aapl_df[\"date\"])\naapl_df.head()","430e9ff5":"# previously used way to transfer strings into ints\nfrom datetime import datetime as dt\ndates=aapl_df[\"date\"]\n\n# The year as an int\naapl_df[\"date_year\"] = dates.dt.year\n\n# Month values (1-12)\naapl_df[\"date_month\"] = dates.dt.month\n\n# Regular Series.dt.week is deprecated, thats why this one is slightly different.  (1-53)\naapl_df[\"date_week\"] = dates.dt.isocalendar().week\n\n# Day values for each month (1-31)\naapl_df[\"date_day\"] = dates.dt.day\n\n# Day values from 1 to 366 (if it's a leap year)\naapl_df[\"date_day_of_year\"] = dates.dt.dayofyear\n\n# Assigns days values of 0-6 for Monday-Sunday\naapl_df[\"date_day_of_week\"] = dates.dt.dayofweek\n\naapl_df.describe()","d65943b8":"# I will skip normalization for now, I haven't gotten it to work with a model yet.\naapl_data = aapl_df.copy()\n#aapl_data[\"volume\"] \/= 100000000\n#aapl_data[\"open\"] \/= 100\n#aapl_data[\"close\"] \/= 100\n\n\n# delete columns we don't need (high,low is look ahead bias, adjclose is redundant, date is already represented with various int variables)\ndel(aapl_data[\"high\"])\ndel(aapl_data[\"low\"])\ndel(aapl_data[\"adjclose\"])\ndel(aapl_data[\"date\"])\naapl_data.describe()","51951860":"# moving average of volume, and open price at 5 and 20 days\n# iloc[: is whole column, int is column number]\n\n#.rolling takes the last 'int' values as a range to calculate a stat\naapl_data['volume_5MA'] = aapl_data.iloc[:,0].rolling(window=5).mean()\naapl_data['volume_20MA'] = aapl_data.iloc[:,0].rolling(window=20).mean()\n\naapl_data['open_5MA'] = aapl_data.iloc[:,1].rolling(window=5).mean()\naapl_data['open_20MA'] = aapl_data.iloc[:,1].rolling(window=20).mean()\naapl_data.head()","08e8c081":"# Now we have NaN values at the beginning.  Luckily its only the first 19 rows, so we can safely drop them\nprint(aapl_data.isna().sum())\naapl_data.dropna(inplace=True)\nprint(aapl_data.isna().sum())\naapl_data.head()","167bf1ed":"#split the data at 70%, 20%, and 10%\nn = len(aapl_data)\n# try model with less data\naapl_data = aapl_data[int(n*.4):]\nn = len(aapl_data)\n\ntrain_df = aapl_data[0:int(n*0.7)]\nval_df = aapl_data[int(n*0.7):int(n*0.9)]\ntest_df = aapl_data[int(n*0.9):]\nval_df.head()","1a403017":"#get labels for each data split\ntrain_y = train_df[\"close\"]\nval_y = val_df[\"close\"]\ntest_y = test_df[\"close\"]\n\n\n#get features for each data split, everything except for open and close price for now\ntrain_X = train_df.drop(columns=['close', 'open'])\nval_X = val_df.drop(columns=['close', 'open'])\ntest_X = test_df.drop(columns=['close', 'open'])\n\n\n'''\ntrain_X = train_df.drop(columns=['close'])\nval_X = val_df.drop(columns=['close'])\ntest_X = test_df.drop(columns=['close'])'''\nval_X.head()","522c165a":"from sklearn import linear_model\n\n#Intializes the model and fits it to the training data\nmodel = linear_model.Lasso(alpha=.1)\nmodel.fit(train_X, train_y)\ny_val_pred = model.predict(val_X)\ny_test_pred = model.predict(test_X)\n\n#function for evaluation metrics, I no longer use r or r^2 after last time\nimport math\nfrom sklearn.metrics import mean_squared_error, r2_score\ndef eval_metrics(y_test, y_pred, dataset):\n    print(\"\\nEvaluation metrics for \" + dataset + \":\\n\")\n    print(\"Mean Squared Error: %.3f\" % mean_squared_error(y_test, y_pred))\n    print(\"Root Mean Squared Error: %.3f\" % math.sqrt(mean_squared_error(y_test, y_pred)))\n    print(\"R Score: %.4f\" % math.sqrt(abs(r2_score(y_test, y_pred))))\n    print(\"-----------------------------------------\")\n\neval_metrics(val_y,y_val_pred, \"Validation Data\")\neval_metrics(test_y,y_test_pred, \"Test Data\")\n","370c8aa4":"import matplotlib.pyplot as plt\n\n#converts back into datetime value for graphing\naapl_X_val_graph_date = pd.to_datetime(val_X['date_year'] * 1000 + val_X['date_day_of_year'], format='%Y%j')\naapl_X_test_graph_date = pd.to_datetime(test_X['date_year'] * 1000 + test_X['date_day_of_year'], format='%Y%j')\n\n#Plot of the results\nplt.figure(figsize=(20,12))\n\n\ngraph = plt.plot(aapl_X_val_graph_date, val_y, label = \"Acutal Validation Values\")\ngraph = plt.plot(aapl_X_val_graph_date, y_val_pred, label = \"Predicted Validation Values\")\n\ngraph = plt.plot(aapl_X_test_graph_date, test_y, label = \"Acutal Test Values\")\ngraph = plt.plot(aapl_X_test_graph_date, y_test_pred, label = \"Predicted Test Values\")\n\n\nplt.legend()\n#plt.margins(0,0)\nplt.show()","0257b9f7":"# To better understand the MSE and RMSE, I calculated MSE by hand (RMSE is just the square root of MSE)\n# Essentially RMSE is the mean of the absolute value of the differences between the predicted and actual stock price\n\ndf = pd.DataFrame(aapl_X_val_graph_date, columns=['date'])\ndf[\"acutal_close\"] = val_y\ndf[\"pred_close\"] = y_val_pred\ndf[\"residual\"] = val_y-y_val_pred\ndf[\"residual square\"] = df[\"residual\"] * df[\"residual\"]\nprint(\"This is the mean squared error for validation data: %.3f\" % df[\"residual square\"].mean())","b3c5a73e":"df = pd.DataFrame(aapl_X_test_graph_date, columns=['date'])\ndf[\"acutal_close\"] = test_y\ndf[\"pred_close\"] = y_test_pred\ndf[\"residual\"] = test_y-y_test_pred\ndf[\"residual square\"] = df[\"residual\"] * df[\"residual\"]\nprint(\"This is the mean squared error for test data: %.3f\" % df[\"residual square\"].mean())","0478a2ca":"\ndf = pd.DataFrame(val_df, columns=['open', 'close'])\ndf['residual'] = df['close'] - df['open']\ndf[\"residual square\"] = df[\"residual\"] * df[\"residual\"]\nprint(\"This is the MSE for open vs. close stock price in val data: %.3f\" % df[\"residual square\"].mean())\nprint(\"This is the RMSE for open vs. close stock price in val data: %.3f\" % math.sqrt(df[\"residual square\"].mean()))\n\ndf = pd.DataFrame(test_df, columns=['open', 'close'])\ndf['residual'] = df['close'] - df['open']\ndf[\"residual square\"] = df[\"residual\"] * df[\"residual\"]\nprint(\"\\nThis is the MSE for open vs. close stock price in test data: %.3f\" % df[\"residual square\"].mean())\nprint(\"This is the RMSE for open vs. close stock price in test data: %.3f\" % math.sqrt(df[\"residual square\"].mean()))\n","9d0001d1":"# Notes\nThis is the best performing model so far, and the only inputs related to price were the two moving averages\n\n### Future analysis needed on:\n1. Why normalization throws off the model\n2. Why cutting data at the beginning works, to a point\n3. How to improve this model\n4. What other types of models could better deal with the heavily date depenednedt data (LSTM, NN, etc.)","033606cb":"# Model Analysis\nThe model performs better on the validation data than the test data, this probably means the model is overfitting the data.  (probably since the training data is very flat, and low compared to the val or test data)","1a64ddd2":"Super interesting comment I saw:\n> When you perform the normalization on the data, by virtue of normalizing before we split the train \/ test \/ validate sets \u2026 isn't that effectively taking some \"future\" information and incorporating it into the training set? (i.e. if the test set had some extreme data moves, calculating the minmax scaler would 'inform' the training session about the range of future outcomes?) \u2026. would doing normalize_data(df) on pre-split datasets make sense? \u2026 or does that pose a problem because we don't have enough data to get normalized ranges ?\n\nI think it does.  So I don't think using something like .zscore works, because it compares the stock price to other prices.  Instead I need to normalize it a different way\n\nPerhaps multiply or divide every value by some scalar or don't normalize.","dba32c64":"I now understand RMSE and MSE, so what are good values in the context of this model.  Lets find the RMSE between open and closing data.","948a686d":"# Model performance for: all data, first 30% of data cut, first 40% cut:\n**Evaluation metrics for Validation Data:**\n\n\nRoot Mean Squared Error: 1.904, 1.522, 2.004\n\n**Evaluation metrics for Test Data:**\n\n\nRoot Mean Squared Error: 4.479, 3.318, 2.919\n\nThe model performs much better with less of the data from the beginning","40f634d5":"Essentially, just predicting the open price for the close price would result in the above MSE and RMSE.\n\nSo yeah, it would currently be better to use the open price as the closing price instead of our model.\n\nAt least we have a much better understanding of the eval metrics","36961df3":"# Eval Metric Further Analysis"}}