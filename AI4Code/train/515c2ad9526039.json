{"cell_type":{"858cfcb0":"code","a812e8bb":"code","d9c69cfa":"code","99b24d9c":"code","f97dee67":"code","28340364":"code","220d2eb4":"code","6ff94601":"code","e0379bf2":"markdown","96ce187e":"markdown"},"source":{"858cfcb0":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_val_score,LeaveOneOut\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np","a812e8bb":"iris = load_iris()","d9c69cfa":"iris.data[:3]","99b24d9c":"iris.target","f97dee67":"model = LogisticRegression(max_iter=1000)","28340364":"loo = LeaveOneOut()\nscores = cross_val_score(model,iris.data,iris.target,cv=loo)","220d2eb4":"print(\"cross valation count=\",len(scores))","6ff94601":"print(\"scroes mean=\",np.round(scores.mean(),2))","e0379bf2":"# LOOCV (LEAVE ONE OUT CROSS VALIDATION)","96ce187e":"Leave-One-Out cross-validator\n\nProvides train\/test indices to split data in train\/test sets. \n\nEach sample is used once as a test set (singleton) while the remaining samples form the training set.\n\nNote: **LeaveOneOut() is equivalent to KFold(n_splits=n) and LeavePOut(p=1)** \nwhere n is the number of samples.\n\nDue to the high number of test sets (which is the same as the number of samples) this cross-validation method can be very costly. \n\nFor large datasets one should favor KFold, ShuffleSplit or StratifiedKFold."}}