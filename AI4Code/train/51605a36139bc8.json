{"cell_type":{"bf4cac95":"code","87a76530":"code","47c961b8":"code","e0531333":"code","9779c616":"code","84d911e4":"code","1e61afda":"code","7e2a28f1":"code","aab1ea22":"code","2972cbed":"code","4013cc96":"code","a20da9ef":"code","c1bb06a4":"code","73bb0de2":"code","63c0f634":"code","b3762cf4":"code","467d0631":"code","4bf52133":"code","a41f9d49":"code","3c63f6b9":"code","d35dd09c":"code","7fa73d79":"code","ccb56b56":"code","472ecee2":"code","ef484888":"code","9483caab":"code","582b48a2":"code","f5489437":"code","f57c9280":"markdown","f6a02364":"markdown","8540049e":"markdown","7d593b9c":"markdown","78fff654":"markdown","99a93ad3":"markdown","2313deea":"markdown","39b977c0":"markdown","96be7eeb":"markdown","2176b2d1":"markdown"},"source":{"bf4cac95":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom bs4 import BeautifulSoup\nimport re\nimport os\nimport codecs\nfrom sklearn import feature_extraction\nimport mpld3","87a76530":"#import three lists: titles, links and wikipedia synopses\ntitles = open('..\/input\/document-clustering\/document Clustering\/title_list.txt').read().split('\\n')\n#ensures that only the first 100 are read in\ntitles = titles[:100]\n\nlinks = open('..\/input\/document-clustering\/document Clustering\/link_list_imdb.txt').read().split('\\n')\nlinks = links[:100]\n\nsynopses_wiki = open('..\/input\/document-clustering\/document Clustering\/synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\nsynopses_wiki = synopses_wiki[:100]\n\nsynopses_clean_wiki = []\nfor text in synopses_wiki:\n    text = BeautifulSoup(text, 'html.parser').getText()\n    #strips html formatting and converts to unicode\n    synopses_clean_wiki.append(text)\n\nsynopses_wiki = synopses_clean_wiki\n    \n    \ngenres = open('..\/input\/document-clustering\/document Clustering\/genres_list.txt').read().split('\\n')\ngenres = genres[:100]\n\nprint(str(len(titles)) + ' titles')\nprint(str(len(links)) + ' links')\nprint(str(len(synopses_wiki)) + ' synopses')\nprint(str(len(genres)) + ' genres')","47c961b8":"synopses_imdb = open('..\/input\/document-clustering\/document Clustering\/synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\nsynopses_imdb = synopses_imdb[:100]\n\nsynopses_clean_imdb = []\n\nfor text in synopses_imdb:\n    text = BeautifulSoup(text, 'html.parser').getText()\n    #strips html formatting and converts to unicode\n    synopses_clean_imdb.append(text)\n\nsynopses_imdb = synopses_clean_imdb","e0531333":"synopses = []\n\nfor i in range(len(synopses_wiki)):\n    item = synopses_wiki[i] + synopses_imdb[i]\n    synopses.append(item)","9779c616":"ranks = []\n\nfor i in range(0,len(titles)):\n    ranks.append(i)","84d911e4":"stopwords = nltk.corpus.stopwords.words('english')","1e61afda":"from nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")","7e2a28f1":"def tokenize_and_stem(text):\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens = []\n    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n    stems = [stemmer.stem(t) for t in filtered_tokens]\n    return stems\n\n\ndef tokenize_only(text):\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens = []\n    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n    return filtered_tokens","aab1ea22":"totalvocab_stemmed = []\ntotalvocab_tokenized = []\nfor i in synopses:\n    allwords_stemmed = tokenize_and_stem(i)\n    totalvocab_stemmed.extend(allwords_stemmed)\n    \n    allwords_tokenized = tokenize_only(i)\n    totalvocab_tokenized.extend(allwords_tokenized)","2972cbed":"vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\nvocab_frame","4013cc96":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=0.2, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n\n%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)\n\nprint(tfidf_matrix.shape)","a20da9ef":"terms = tfidf_vectorizer.get_feature_names()","c1bb06a4":"from sklearn.metrics.pairwise import cosine_similarity\ndist = 1 - cosine_similarity(tfidf_matrix)","73bb0de2":"from sklearn.cluster import KMeans\n\nnum_clusters = 5\n\nkm = KMeans(n_clusters=num_clusters)\n\n%time km.fit(tfidf_matrix)\n\nclusters = km.labels_.tolist()","63c0f634":"import joblib\n\n#joblib.dump(km,  'doc_cluster.pkl')\nclusters = km.labels_.tolist()","b3762cf4":"import pandas as pd\n\nfilms = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n\nframe = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])","467d0631":"frame","4bf52133":"frame['cluster'].value_counts()","a41f9d49":"grouped = frame['rank'].groupby(frame['cluster'])\n\ngrouped.mean()","3c63f6b9":"from __future__ import print_function\n\nprint(\"Top terms per cluster:\")\nprint()\norder_centroids = km.cluster_centers_.argsort()[:, ::-1]\nfor i in range(num_clusters):\n    print(\"Cluster %d words:\" % i, end='')\n    for ind in order_centroids[i, :6]:\n        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n    print()\n    print()\n    print(\"Cluster %d titles:\" % i, end='')\n    for title in frame.loc[i]['title'].values.tolist():\n        print(' %s,' % title, end='')\n    print()\n    print()","d35dd09c":"frame['Rank'] = frame['rank'] + 1\nframe['Title'] = frame['title']","7fa73d79":"import os  # for os.path.basename\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn.manifold import MDS\n\nMDS()\n\n# two components as we're plotting points in a two-dimensional plane\n# \"precomputed\" because we provide a distance matrix\n# we will also specify `random_state` so the plot is reproducible.\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n\npos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n\nxs, ys = pos[:, 0], pos[:, 1]","ccb56b56":"#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\nfrom nltk.tag import pos_tag\n\ndef strip_proppers_POS(text):\n    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n    return non_propernouns","472ecee2":"#set up colors per clusters using a dict\ncluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n\n#set up cluster names using a dict\ncluster_names = {0: 'Killed, soldiers, captain', \n                 1: 'Father, New York, brothers', \n                 2: 'Police, killed, murders', \n                 3: 'Marries, family, friends', \n                 4: 'News, army, hours, execution'}","ef484888":"%matplotlib inline","9483caab":"#create data frame that has the result of the MDS plus the cluster numbers and titles\ndf = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n\n#group by cluster\ngroups = df.groupby('label')\n\n\n# set up plot\nfig, ax = plt.subplots(figsize=(17, 9)) # set size\nax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n\n#iterate through groups to layer the plot\n#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color\/label\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n    ax.set_aspect('auto')\n    ax.tick_params(\\\n        axis= 'x',          # changes apply to the x-axis\n        which='both',      # both major and minor ticks are affected\n        bottom='off',      # ticks along the bottom edge are off\n        top='off',         # ticks along the top edge are off\n        labelbottom='off')\n    ax.tick_params(\\\n        axis= 'y',         # changes apply to the y-axis\n        which='both',      # both major and minor ticks are affected\n        left='off',      # ticks along the bottom edge are off\n        top='off',         # ticks along the top edge are off\n        labelleft='off')\n    \nax.legend(numpoints=1)  #show legend with only 1 point\n\n#add label in x,y position with the label as the film title\nfor i in range(len(df)):\n    ax.text(df.loc[i]['x'], df.loc[i]['y'], df.loc[i]['title'], size=8)  \n\nplt.show()\n","582b48a2":"from scipy.cluster.hierarchy import ward, dendrogram\n\nlinkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n\nfig, ax = plt.subplots(figsize=(15, 20)) # set size\nax = dendrogram(linkage_matrix, orientation=\"right\", labels=titles);\n\nplt.tick_params(\\axis= 'x',which='both',bottom='off',top='off',labelbottom='off')\n\nplt.tight_layout() #show plot with tight layout\n\nplt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters","f5489437":"plt.close()","f57c9280":"The Objective here is to identify the latent structures within the synopses of the top 100 films of all time (per an IMDB list).\n\nThe steps of the excution are as follows: \n1. tokenizing and stemming each word\n2. transforming the corpus into vector using tf-idf\n3. calculating cosine distance between each documents as a measure of similarity\n4. clustering the documents using k-means\n5. plotting the clustering output using matplotlib\n6. conducting a Hierarchical clustering on the corpus using Ward clustering\n7. plotting a dandrogram","f6a02364":"## Tf-idf and document similarity\n\n\nHere, we will define term frequency-inverse document frequency (tf-idf) vectorizer parameters and then convert the synopses list into a tf-idf matrix.\n\nTo get a Tf-idf matrix, first count word occurrences by document. This is transformed into a document-term matrix (dtm). This is also just called a term frequency matrix. An example of a dtm is here at right.\n\nThen apply the term frequency-inverse document frequency weighting: words that occur frequently within a document but not frequently within the corpus receive a higher weighting as these words are assumed to contain more meaning in relation to the document.\n![image.png](attachment:3c181faf-d1a2-4e2a-91d1-c361b5d614bd.png)\n\nA couple things to note about the parameters we define below:\n\n*max_df*: this is the maximum frequency within the documents a given feature can have to be used in the tfi-idf matrix. If the term is in greater than 80% of the documents it probably cares little meanining (in the context of film synopses)\n\n*min_idf*: this could be an integer (e.g. 5) and the term would have to be in at least 5 of the documents to be considered. Here I pass 0.2; the term must be in at least 20% of the document. I found that if I allowed a lower min_df I ended up basing clustering on names--for example \"Michael\" or \"Tom\" are names found in several of the movies and the synopses use these names frequently, but the names carry no real meaning.\n*ngram_range*: this just means I'll look at unigrams, bigrams and trigrams. ","8540049e":"# Document Clustering using Kmeans","7d593b9c":"## Visualizing cluster documents","78fff654":"## Multidimensional Scaling","99a93ad3":"## K-means clustering\n\nUsing the tf-idf matrix, you can run a slew of clustering algorithms to better understand the hidden structure within the synopses. K-means initializes with a pre-determined number of clusters (here 5). Each observation is assigned to a cluster (cluster assignment) so as to minimize the within cluster sum of squares. Next, the mean of the clustered observations is calculated and used as the new cluster centroid. Then, observations are reassigned to clusters and centroids recalculated in an iterative process until the algorithm reaches convergence.\n","2313deea":"## Stopwords, stemming and tokenizing","39b977c0":"Below we will define two functions:\n\n*tokenize_and_stem*: tokenizes (splits the synopsis into a list of its respective words (or tokens) and also stems each token\n\n*tokenize_only*: tokenizes the synopsis only\n\nwe will use both these functions to create a dictionary which becomes important in case we want to use stems for an algorithm, but later convert stems back to their full words for presentation purposes.","96be7eeb":"Using these two lists,we will create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column. The benefit of this is it provides an efficient way to look up a stem and return a full token. The downside here is that stems to tokens are one to many: the stem 'run' could be associated with 'ran', 'runs', 'running', etc. ","2176b2d1":"## Hierarchical document clustering"}}