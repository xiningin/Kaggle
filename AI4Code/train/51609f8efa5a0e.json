{"cell_type":{"c428bf35":"code","59681554":"code","d3d540a8":"code","4abbb474":"code","44a83966":"code","32e85b5e":"code","dac9c49b":"code","9193333d":"code","dbfd0e01":"code","59ab3abf":"code","637c2b65":"code","3507aeca":"code","7d1f8eb2":"code","a9e80425":"code","dab44433":"code","f1e07128":"code","760f00eb":"code","13ea02ca":"code","be70412a":"code","a05f2e06":"code","9c54334e":"code","cac536a4":"code","6bb2b60b":"code","e57872fa":"code","8214a8c9":"code","fa97252a":"code","5cf53375":"code","14627a67":"code","1296bafc":"code","33b341a7":"code","e8cca01f":"code","aeedc730":"code","13b40a05":"code","3f0ebf05":"code","4d606f65":"code","1cc83365":"code","4a3fbadc":"code","417b6751":"code","38d475b1":"code","da603d5b":"code","95264c8d":"code","ccbf6913":"code","abe5cdcd":"code","b5393d70":"code","ff34bed1":"code","f0e9ecc1":"code","c4939aaf":"code","bfd45f18":"code","76e78f6f":"code","c6378772":"code","70505163":"code","3043f1c0":"code","483570a9":"code","b8c39101":"code","f7811638":"code","cd4a18b3":"code","c87f5383":"code","071d9571":"code","73d8865b":"code","800c9fbd":"code","056586b8":"code","9ec1bb49":"code","b03bf054":"code","a8384155":"code","725eb63b":"code","af952561":"code","fd562dbe":"code","0a109fa3":"code","61327f6c":"code","80eb4be5":"code","17b25dbb":"code","7fd64f80":"code","4f53252a":"code","dc024f6c":"code","8c409714":"code","b5d1f503":"code","60bcdf4f":"code","1584a5b7":"code","fc16cee3":"code","7bfe4ec3":"code","2f37e8a4":"code","2a6f0e54":"code","e01e29a8":"code","d0b8ddb6":"code","2ad059c7":"code","cb4e4719":"code","d103152d":"code","2181bd7d":"code","84678012":"code","0a0b8f73":"code","df115742":"code","4a281e6c":"code","c0bdaa95":"code","8f7fd956":"code","66a2114d":"code","34597c9a":"code","a2cfc030":"code","570ccc6a":"code","87008510":"code","d6c49167":"code","d0466b0f":"code","9019849a":"code","dea6a30e":"code","fbcba1f6":"code","3373dd45":"code","a5ceac64":"code","f3273658":"code","e0aa64d9":"code","8115bcc1":"code","495b9883":"code","82228f94":"code","d587d6dc":"code","345484ae":"code","c6e00762":"code","69ef1335":"code","4adfb137":"code","dd133434":"code","c4380fbd":"code","60d9c6b1":"code","44f068dd":"code","6fdb2313":"markdown","90837b4e":"markdown","43e21364":"markdown","fe868077":"markdown","88965091":"markdown","d02788ac":"markdown","1d829333":"markdown","75a821bc":"markdown","c810291b":"markdown","c1fad18f":"markdown","0b611cd9":"markdown","384e5b23":"markdown","8d321d49":"markdown","0a076d6c":"markdown","0885b0d7":"markdown","0a22d400":"markdown","e1351748":"markdown","3905fe4d":"markdown","63eaaee9":"markdown","cfa2352b":"markdown","b70ef86a":"markdown","0a42a6f3":"markdown","d868cc48":"markdown","04a1679d":"markdown","ddea5b51":"markdown","fefc335b":"markdown","04df37e6":"markdown","ccc50690":"markdown","a16b4d7f":"markdown","d24049b2":"markdown","48d486a8":"markdown","3fd96b0f":"markdown","c65939ac":"markdown","b93d88e4":"markdown","dfa06ac5":"markdown","caf3bcae":"markdown","0faa97f7":"markdown","d2144ec9":"markdown","26e87d92":"markdown"},"source":{"c428bf35":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\n\n# to make the model\nfrom sklearn.model_selection import train_test_split, GridSearchCV","59681554":"pwd","d3d540a8":"# Read train and test data with pd.read_csv():\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic-traintest-data\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic-traintest-data\/test.csv\")","4abbb474":"# copy data in order to avoid any change in the original:\ntrain = train_data.copy()\ntest = test_data.copy()","44a83966":"train.head()","32e85b5e":"test.head()","dac9c49b":"train.info()","9193333d":"train.describe().T","dbfd0e01":"train['Pclass'].value_counts()","59ab3abf":"train['Sex'].value_counts()","637c2b65":"train['SibSp'].value_counts()","3507aeca":"train['Parch'].value_counts()","7d1f8eb2":"train['Ticket'].value_counts()","a9e80425":"train['Cabin'].value_counts()","dab44433":"train['Embarked'].value_counts()","f1e07128":"train['Age'].value_counts()","760f00eb":"sns.barplot(x = 'Pclass', y = 'Survived', data = train);","13ea02ca":"sns.barplot(x = 'SibSp', y = 'Survived', data = train);","be70412a":"sns.barplot(x = 'Parch', y = 'Survived', data = train);","a05f2e06":"sns.barplot(x = 'Sex', y = 'Survived', data = train);","9c54334e":"sns.barplot(x = 'Sex', y = 'Survived', hue = \"Pclass\" , data = train);","cac536a4":"train.head()","6bb2b60b":"# Since we classified the ticket and the cabin variables as useless \n# we can seperate them from the data for probable further use\ntrain_Ticket = train[\"Ticket\"]\ntest_Ticket = test[\"Ticket\"]\ntrain_Cabin = train[\"Cabin\"]\ntest_Cabin = test[\"Cabin\"]","e57872fa":"train_Ticket.head()","8214a8c9":"# Now we drop the Ticket and Cabin feature\ntrain = train.drop(['Ticket' , 'Cabin'], axis = 1)\ntest = test.drop(['Ticket' , 'Cabin'], axis = 1)\n\ntrain.head()","fa97252a":"train.isnull().sum()","5cf53375":"train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].mean())","14627a67":"test[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].mean())","1296bafc":"train.isnull().sum()","33b341a7":"test.isnull().sum()","e8cca01f":"train[\"Embarked\"].value_counts()","aeedc730":"# Fill NA with the most frequent value:\ntrain[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")","13b40a05":"test[test[\"Fare\"].isnull()]","3f0ebf05":"test[[\"Pclass\",\"Fare\"]].groupby(\"Pclass\").mean()","4d606f65":"test[\"Fare\"] = test[\"Fare\"].fillna(12.46)","1cc83365":"test[\"Fare\"].isnull().sum()","4a3fbadc":"train.isnull().sum()","417b6751":"test.isnull().sum()","38d475b1":"# There is no missing data now!","da603d5b":"train.describe().T","95264c8d":"# Let's try to catch any outlier data of the variables:\nsns.boxplot(x = train['Age']);","ccbf6913":"sns.boxplot(x = train['Fare']);","abe5cdcd":"# There are outliers in Fare and Age data. We want to keep all data in interquartile range IQR","b5393d70":"# For Age Data\nQ1 = train['Age'].quantile(0.25)\nQ3 = train['Age'].quantile(0.75)\nIQR = Q3 - Q1\n\nage_lower_limit = Q1- 1.5*IQR\nprint(age_lower_limit)\n\nage_upper_limit = Q3 + 1.5*IQR\nprint(age_upper_limit)","ff34bed1":"Q1 = train['Fare'].quantile(0.25)\nQ3 = train['Fare'].quantile(0.75)\nIQR = Q3 - Q1\n\nfare_lower_limit = Q1- 1.5*IQR\nprint(fare_lower_limit)\n\nfare_upper_limit = Q3 + 1.5*IQR\nprint(fare_upper_limit)","f0e9ecc1":"# We will not use lower_limit information --> force data not exceed upper_limit","c4939aaf":"# observations with Age data higher than the upper limit:\n\ndf_a = train['Age'] > (age_upper_limit)\ndf_a.value_counts()","bfd45f18":"# There is only 42 data in 891 outlying so it is better to fix them to upper_limit\n# Almost 5 % go deeper:\ntrain.sort_values(\"Age\", ascending=False).head(42)\n# distribution is nice but the only value 80 --> nearest 76","76e78f6f":"train['Age'] = train['Age'].replace(80, 74)\ntrain.sort_values(\"Age\", ascending=False).head()","c6378772":"test.sort_values(\"Age\", ascending=False).head()","70505163":"test['Age'] = test['Age'].replace(76, 67)\ntest.sort_values(\"Age\", ascending=False).head()","3043f1c0":"# observations with Fare data higher than the upper limit:\n\ndf_f = train['Fare'] > (fare_upper_limit)\ndf_f.value_counts()","483570a9":"# More than 10 % --> only manuel touch to the max 3 values\ntrain.sort_values(\"Fare\", ascending=False).head()","b8c39101":"train['Fare'] = train['Fare'].replace(512.3292, 263)\ntrain.sort_values(\"Fare\", ascending=False).head()","f7811638":"test.sort_values(\"Fare\", ascending=False).head()","cd4a18b3":"test['Fare'] = test['Fare'].replace(512.3292, 263)\ntest.sort_values(\"Fare\", ascending=False).head()","c87f5383":"sns.heatmap(train.corr(), annot = True);","071d9571":"# Considerable correlation exists only between Survived VS Sex and Pclass (parch vs SibSp ignorable)","73d8865b":"# Convert Sex values into 1-0:\n# (Male:1 Female:0)\n\nfrom sklearn import preprocessing\n\nlbe = preprocessing.LabelEncoder()\ntrain[\"Sex\"] = lbe.fit_transform(train[\"Sex\"])\ntest[\"Sex\"] = lbe.fit_transform(test[\"Sex\"])","800c9fbd":"train.head()","056586b8":"bins = [0, 5, 12, 18, 24, 35, 60, np.inf]\nmylabels = ['Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = mylabels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = mylabels)","9ec1bb49":"# Map each Age value to a numerical value:\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)","b03bf054":"train.head()","a8384155":"#drop the Age feature:\ntrain = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","725eb63b":"train[\"Fare\"].head()","af952561":"train[\"Fare\"]","fd562dbe":"# Map Fare values into groups of numerical values:\ntrain['FareBand'] = pd.qcut(train['Fare'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ntest['FareBand'] = pd.qcut(test['Fare'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])","0a109fa3":"# Drop Fare values\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","61327f6c":"train[\"FareBand\"]","80eb4be5":"# Drop also name values  --> useless:\ntrain = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)\n","17b25dbb":"train.head()","7fd64f80":"# Map each Embarked value to a numerical value:\n\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)","4f53252a":"train.head()","dc024f6c":"?sns.heatmap","8c409714":"sns.heatmap(train.corr(), annot = True , cbar = True , square=True );","b5d1f503":"sns.heatmap(test.corr(), annot = True );","60bcdf4f":"train.head()","1584a5b7":"train[\"FamilySize\"] = train_data[\"SibSp\"] + train_data[\"Parch\"] + 1","fc16cee3":"test[\"FamilySize\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1","7bfe4ec3":"# Create new feature of family size:\n\ntrain['Single'] = train['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntrain['SmallFam'] = train['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ntrain['MedFam'] = train['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ntrain['LargeFam'] = train['FamilySize'].map(lambda s: 1 if s >= 5 else 0)","2f37e8a4":"train.head()","2a6f0e54":"# Create new feature of family size:\n\ntest['Single'] = test['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntest['SmallFam'] = test['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ntest['MedFam'] = test['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ntest['LargeFam'] = test['FamilySize'].map(lambda s: 1 if s >= 5 else 0)","e01e29a8":"test.head()","d0b8ddb6":"# Convert Embarked into dummy variables:\n\ntrain = pd.get_dummies(train, columns = [\"Embarked\"], prefix=\"Em\")","2ad059c7":"train.head()","cb4e4719":"test = pd.get_dummies(test, columns = [\"Title\"])\ntest = pd.get_dummies(test, columns = [\"Embarked\"], prefix=\"Em\")","d103152d":"test.head()","2181bd7d":"# Create categorical values for Pclass:\ntrain[\"Pclass\"] = train[\"Pclass\"].astype(\"category\")\ntrain = pd.get_dummies(train, columns = [\"Pclass\"],prefix=\"Pc\")","84678012":"test[\"Pclass\"] = test[\"Pclass\"].astype(\"category\")\ntest = pd.get_dummies(test, columns = [\"Pclass\"],prefix=\"Pc\")","0a0b8f73":"train.head()","df115742":"test.head()","4a281e6c":"train['personnum']=train.SibSp + train.Parch\ntrain.head()","c0bdaa95":"Alone = []\nfor i in train[\"personnum\"]:\n    if i ==0:\n        Alone.append(1)\n    else:\n        Alone.append(0)\n","8f7fd956":"Alone = pd.DataFrame(Alone)\n#Alone.head()\nAlone.describe().T","66a2114d":"#train = train.drop([\"0\"], axis = 1)\n#train = train.drop([\"Alone\"] , axis=1)","34597c9a":"Alone.columns = [\"Alone\"]\n#Al = Alone.rename(columns={'0': 'Alone'}\n#Alone.column_name = \"Alone\"\nAlone.head()","a2cfc030":"train = pd.concat((train,Alone) , axis=1)\n#train = train.drop([\"personnum\"] , axis = 1)\ntrain.head()","570ccc6a":"train = train.drop([0] , axis = 1)","87008510":"train.head()","d6c49167":"#Test Data\ntest['personnum']=test.SibSp + test.Parch\n\nAlone_t = []\nfor i in test[\"personnum\"]:\n    if i ==0:\n        Alone_t.append(1)\n    else:\n        Alone_t.append(0)\nAlone_t = pd.DataFrame(Alone_t)\nAlone_t.describe().T\nAlone_t.columns = [\"Alone\"]\ntest = pd.concat((test,Alone_t) , axis=1)","d0466b0f":"test.head()","9019849a":"test = test.drop([\"personnum\"] , axis = 1)","dea6a30e":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_test, y_train, y_test = train_test_split(predictors, target, test_size = 0.20, random_state = 0)","fbcba1f6":"x_train.shape","3373dd45":"x_test.shape","a5ceac64":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\nacc_logreg = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_logreg)","f3273658":"from sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_test)\nacc_randomforest = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_randomforest)","e0aa64d9":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)","8115bcc1":"xgb_params = {\n        'n_estimators': [200, 500],\n        'subsample': [0.6, 1.0],\n        'max_depth': [2,5,8],\n        'learning_rate': [0.1,0.01,0.02],\n        \"min_samples_split\": [2,5,10]}","495b9883":"xgb = GradientBoostingClassifier()\n\nxgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 10, n_jobs = -1, verbose = 2)","82228f94":"xgb_cv_model.fit(x_train, y_train)","d587d6dc":"xgb_cv_model.best_params_","345484ae":"xgb = GradientBoostingClassifier(learning_rate = xgb_cv_model.best_params_[\"learning_rate\"], \n                    max_depth = xgb_cv_model.best_params_[\"max_depth\"],\n                    min_samples_split = xgb_cv_model.best_params_[\"min_samples_split\"],\n                    n_estimators = xgb_cv_model.best_params_[\"n_estimators\"],\n                    subsample = xgb_cv_model.best_params_[\"subsample\"])","c6e00762":"xgb_tuned =  xgb.fit(x_train,y_train)","69ef1335":"y_pred = xgb_tuned.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)","4adfb137":"models = pd.DataFrame({\n    'Model': ['Logistic Regression','Random Forest', 'Gradient Boosting Classifier'],\n    'Score': [acc_logreg, acc_randomforest,  acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","dd133434":"train","c4380fbd":"test","60d9c6b1":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = xgb_tuned.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('MHsubmission.csv', index=False)","44f068dd":"output.head()","6fdb2313":"# TITANIC SURVIVAL PREDICTION PROJECT\n## Business Understanding \/ Problem Definition","90837b4e":"### Classes of some categorical variables","43e21364":"## Gradient Boosting Classifier","fe868077":"# Data Understanding (Exploratory Data Analysis)","88965091":"## Outlier Treatment","d02788ac":"### Visualization","1d829333":"**Goal:**\n\nPredict the survival of the passangers in Titanic by using the most efficient machine learning model. ","75a821bc":"### Ticket and Cabin","c810291b":"### Family Size","c1fad18f":"#### SibSp vs survived:","0b611cd9":"In general, barplot is used for categorical variables while histogram, density and boxplot are used for numerical data.","384e5b23":"## Spliting the train data","8d321d49":"#### Sex vs survived:","0a076d6c":"### AgeGroup","0885b0d7":"##### Sex vs survived (including Plcass):","0a22d400":"#### Pclass vs survived:","e1351748":"### Sex","3905fe4d":"### Fare","63eaaee9":"# Submission","cfa2352b":"**Variables:**\n\ni)  ***Categorical:***\n\n    a) Nominal:\n\nSurvival: Survived = 1 , Dead = 0\n\nSex     : Female = 1   , Male = 0\n\nEmbarked: Port of Embarkation -> C = Cherbourg, Q = Queenstown, S = Southampton\n\n    b) Ordinal:\n\nPclass  : Ticket class -> 1 = 1st, 2 = 2nd, 3 = 3rd ( 1st class indicates the richest people)\n\n    c) Useless:\n    \nTicket: Ticket number (However value is numerical ticket number does't count any measurable quantity)\n\nCabin: Cabin number (This value may contain information about resposibility of the crew and may be categorical)\n\nii)  ***Numerical:***\n\nAge: Age in years\n\nSibSp: # of siblings \/ spouses aboard the Titanic\n\nParch: # of parents \/ children aboard the Titanic\n\nFare: Passenger fare (this value should be classified into groups but there exists 248 different type)\n\n","b70ef86a":"## Logistic Regression","0a42a6f3":"### Basic summary statistics about the numerical data","d868cc48":"# Modeling, Evaluation and Model Tuning","04a1679d":"**Variable Notes:**\n\nPclass: A proxy for socio-economic status (SES)\n- 1st = Upper\n- 2nd = Middle\n- 3rd = Lower\n\nAge: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nSibSp: The dataset defines family relations in this way...\n- Sibling = brother, sister, stepbrother, stepsister\n- Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nParch: The dataset defines family relations in this way...\n- Parent = mother, father\n- Child = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","ddea5b51":"## Missing Value Treatment","fefc335b":"## Analysis and Visualization of Numeric and Categorical Variables","04df37e6":"## Importing Librarires","ccc50690":"#### Parch vs survived:","a16b4d7f":"### Age","d24049b2":"## Variable Transformation","48d486a8":"# Data Preparation","3fd96b0f":"## Random Forest","c65939ac":"#### 2 Embarked missing in train data , 1 Fare missing in test data","b93d88e4":"### Alone Feature","dfa06ac5":"## Loading Data","caf3bcae":"### Pclass","0faa97f7":"## Deleting Unnecessary Variables","d2144ec9":"### Embarked","26e87d92":"## Feature Engineering"}}