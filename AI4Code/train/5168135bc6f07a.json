{"cell_type":{"7f8ee7be":"code","88179dec":"code","175e1309":"code","35afe7c5":"code","0eb26193":"code","905469ec":"code","231423a0":"code","712b20d8":"code","a5f48c2b":"code","32593a9f":"code","eb0c9c5b":"code","2e2c784f":"code","d9f4de39":"code","e138e7e5":"code","ec9b957d":"code","85c8e0eb":"code","024e546b":"code","78131a49":"code","151843b3":"code","1eead275":"code","233702f0":"code","0603140b":"code","1338b39f":"code","19fb1449":"code","c3c69944":"code","acce6e6a":"code","7a6a4b22":"code","cd84534e":"code","79a7b743":"code","6455e183":"code","5137df48":"code","3f0e7f7c":"code","ef1d6179":"code","401c0772":"code","536abd09":"code","fe2b01af":"code","7823f106":"code","7036b9b0":"code","06f39ba0":"code","89f17e24":"code","546bf424":"code","c3b06593":"code","6ce23e95":"code","9d94be5b":"code","392d2da5":"code","f290e750":"code","0d7a6a75":"code","8d59f4bf":"code","6adbd555":"code","026e4641":"code","17985d90":"code","c62ce4eb":"code","fa6e2c13":"code","f8193c42":"code","b8a1137c":"code","0df72d5a":"code","02def2c9":"code","60feeac2":"code","e719a572":"code","53042910":"code","cfc6a82a":"code","c613aa68":"code","052ca5a3":"code","07713847":"code","fa2ae689":"code","f7a0ddb9":"code","bee612db":"code","db4790e7":"code","3856cb86":"code","d09a726d":"code","e56cdf60":"code","dedd4121":"code","de85701c":"code","47afd983":"code","241ce933":"code","5a4b3fff":"code","fdb5a7da":"code","f2d63e87":"code","7f8b9cf7":"code","37febb34":"code","4c362272":"code","5245f07f":"code","8140f27f":"markdown","e46fa7f3":"markdown","7254afef":"markdown","68a2fd56":"markdown","360e2aa2":"markdown","b4eaa3e7":"markdown","d8f6f671":"markdown","ce3a41de":"markdown","934e02ff":"markdown","b99f30db":"markdown","e9bedfef":"markdown","2ed136d7":"markdown","0ff7ffd0":"markdown","df43aea9":"markdown","8e1e3310":"markdown","55217f5e":"markdown","1af4f633":"markdown","a151d01a":"markdown","f7019bcc":"markdown","48759fd7":"markdown","67380289":"markdown","04667b5a":"markdown","6c8e88c8":"markdown","8bcaf432":"markdown","c65b2c46":"markdown","e1dea47e":"markdown","6fcb8496":"markdown","3f545da1":"markdown","75dcdf86":"markdown","95300a6a":"markdown","e3c99bb8":"markdown","b820e52a":"markdown","62f6f0f8":"markdown","55885cdd":"markdown","dc95a2e3":"markdown","d6306af4":"markdown","15501828":"markdown","dd304ba7":"markdown","d17e1bc1":"markdown","69b1685f":"markdown","e147c169":"markdown","f66fe182":"markdown","a31203ca":"markdown","2156ed21":"markdown","7cd27d6e":"markdown","55d0b587":"markdown","9f9aeeee":"markdown","ac0b0cfb":"markdown","e2539019":"markdown","e549c4fa":"markdown","615a6e03":"markdown","7fbcfd57":"markdown","affb01e7":"markdown","73983b01":"markdown","1abe567e":"markdown","56681180":"markdown","4fa37051":"markdown","8ca1fa58":"markdown","83845c63":"markdown","5f516af3":"markdown","5146d10b":"markdown","774549cf":"markdown","8dd4d578":"markdown","8fc2bdec":"markdown","e4604267":"markdown","50de452e":"markdown","8d4ff3ef":"markdown","e74f5b7d":"markdown","f9fbfe7a":"markdown","5c99e67a":"markdown","d830d7b0":"markdown","95dde596":"markdown","7768bd5f":"markdown","dc518d02":"markdown","2c4881d0":"markdown","bbeca123":"markdown","9782c749":"markdown","382dc3eb":"markdown","295ca36a":"markdown","a436a368":"markdown","34e50db7":"markdown","97292985":"markdown","e9a359b3":"markdown","65010e0b":"markdown","1df818c0":"markdown","e7fe374a":"markdown","680435c1":"markdown","530b948a":"markdown"},"source":{"7f8ee7be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","88179dec":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport sys\nimport os\nimport warnings\nwarnings.filterwarnings('ignore') \n#plt.style.use('seaborn')\nplt.style.use('fivethirtyeight')","175e1309":"admt=pd.read_csv('..\/input\/Admission_Predict.csv')","35afe7c5":"admt.head()","0eb26193":"admt.isnull().sum()","905469ec":"print('Rows     :',admt.shape[0])\nprint('Columns  :',admt.shape[1])\nprint('\\nFeatures :\\n     :',admt.columns.tolist())\nprint('\\nMissing values    :',admt.isnull().values.sum())\nprint('\\nUnique values :  \\n',admt.nunique())","231423a0":"admt.columns.to_frame().T","712b20d8":"admt.count().to_frame().T","a5f48c2b":"print(\"There are\",len(admt.columns),\"columns:\")\nfor x in admt.columns:\n    sys.stdout.write(str(x)+\", \")                                                      #admt.columns also works ","32593a9f":"admt.rename(columns={'Serial No.':'Srno','GRE Score':'GRE','TOEFL Score':'TOEFL','University Rating':'UnivRating','Chance of Admit ':'Chance'},inplace=True)","eb0c9c5b":"admt.head()","2e2c784f":"admt.columns","d9f4de39":"admt.drop('Srno', axis=1, inplace=True)\nadmt.head()","e138e7e5":"admt.describe().plot(kind = \"area\",fontsize=27, figsize = (20,8), table = True,colormap=\"rainbow\")\nplt.xlabel('Statistics',)\nplt.ylabel('Value')\nplt.title(\"General Statistics of Admissions\")","ec9b957d":"plt.figure(1, figsize=(10,6))\nplt.subplot(1,4, 1)\nplt.boxplot(admt['GRE'])\nplt.title('GRE Score')\n\nplt.subplot(1,4,2)\nplt.boxplot(admt['TOEFL'])\nplt.title('TOEFL Score')\n\nplt.subplot(1,4,3)\nplt.boxplot(admt['UnivRating'])\nplt.title('University Rating')\n\nplt.subplot(1,4,4)\nplt.boxplot(admt['CGPA'])\nplt.title('CGPA')\n\nplt.show()\n","85c8e0eb":"fig=plt.gcf()\nfig.set_size_inches(10,10)\nfig=sns.heatmap(admt.corr(),annot=True,cmap='inferno',linewidths=1,linecolor='k',square=True,mask=False, vmin=-1, vmax=1,cbar_kws={\"orientation\": \"vertical\"},cbar=True)","024e546b":"#correlations_data = admt.corr()['Chance'].sort_values(ascending=False)\ncor=admt.corr()['Chance']\n# Print the correlations\nprint(cor)","78131a49":"admt[['GRE','TOEFL','UnivRating','CGPA']].hist(figsize=(10,8),bins=10,color='#ffd700',linewidth='1',edgecolor='k')\nplt.tight_layout()\nplt.show()","151843b3":"category = ['GRE','TOEFL','UnivRating','SOP','LOR ','CGPA','Research','Chance']\ncolor = ['yellowgreen','gold','lightskyblue','pink','red','purple','orange','gray']\nstart = True\nfor i in np.arange(4):\n    \n    if start == True:\n        fig = plt.figure(figsize=(14,8))\n        start = False\n        \n    plt.subplot2grid((4,2),(i,0))\n    admt[category[2*i]].hist(color=color[2*i],bins=10)\n    plt.title(category[2*i])\n    plt.subplot2grid((4,2),(i,1))\n    admt[category[2*i+1]].hist(color=color[2*i+1],bins=10)\n    plt.title(category[2*i+1])\n    \nplt.subplots_adjust(hspace = 0.7, wspace = 0.2)    \nplt.show()","1eead275":"print('Mean CGPA Score is :',int(admt[admt['CGPA']<=500].CGPA.mean()))\nprint('Mean GRE Score is :',int(admt[admt['GRE']<=500].GRE.mean()))\nprint('Mean TOEFL Score is :',int(admt[admt['TOEFL']<=500].TOEFL.mean()))\nprint('Mean University rating is :',int(admt[admt['UnivRating']<=500].UnivRating.mean()))","233702f0":"a=len(admt[admt.Research==1])\nb=len(admt[admt.Research==0])\nprint('Total number of students',a+b)\nprint('Students having Research:',len(admt[admt.Research==1]))\nprint('Students not having Research:',len(admt[admt.Research==0]))","0603140b":"y=np.array([len(admt[admt.Research==1]),len(admt[admt.Research==0])])\nx=['Having Research','Not having Research']\nax=plt.bar(x,y,width=0.5,color='red',edgecolor='k',align='center',linewidth=2)\n#plt.xlabel('',fontsize=20)\nplt.ylabel('Student Count',fontsize=20)\n#ax.tick_params(labelsize=20)\nplt.title('Student Research',fontsize=25)\nplt.grid()\nplt.ioff()","1338b39f":"f,ax=plt.subplots(1,2,figsize=(18,8))\nadmt['Research'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Students Research')\nax[0].set_ylabel('Student Count')\nsns.countplot('Research',data=admt,ax=ax[1])\nax[1].set_title('Students Research')\nplt.show()","19fb1449":"sns.scatterplot(data=admt,x='GRE',y='TOEFL',hue='Research')","c3c69944":"def modiffy(row):\n    if row['Chance'] >0.7 :\n        return 1\n    else :\n        return 0\nadmt['Admit'] = admt.apply(modiffy,axis=1)\nadmttemp = admt.drop(['Chance'], axis=1)\n#sns.pairplot(admttemp,hue='Admit')\nsns.scatterplot(data=admttemp,x='GRE',y='TOEFL',hue='Admit')\ndel admttemp","acce6e6a":"sns.factorplot('Research','Admit',data=admt)\nplt.show()","7a6a4b22":"admt_sort=admt.sort_values(by=admt.columns[-1],ascending=False)\nadmt_sort.head()\n#admt.head()\n#admttemp.head()","cd84534e":"admt_sort[(admt_sort['Chance']>0.90)].mean().reset_index()","79a7b743":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.violinplot(\"Research\",\"GRE\",hue=\"Admit\", data=admt,split=True)\nplt.subplot(2,2,2)\nsns.violinplot(\"Research\",\"TOEFL\",hue=\"Admit\", data=admt,split=True)\nplt.subplot(2,2,3)\nsns.violinplot(\"Research\",\"CGPA\",hue=\"Admit\", data=admt,split=True)\nplt.subplot(2,2,4)\nsns.violinplot(\"Research\",\"UnivRating\",hue=\"Admit\", data=admt,split=True)\n#ax[0].set_title('Pclass and Age vs Survived')\n#ax[0].set_yticks(range(0,110,10))\n#sns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[1])\n#ax[1].set_title('Sex and Age vs Survived')\n#ax[1].set_yticks(range(0,110,10))\nplt.ioff()\nplt.show()\n","6455e183":"f,ax=plt.subplots(1,2,figsize=(18,8))\nadmt['Admit'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Admitted to University')\nax[0].set_ylabel('')\nsns.countplot('Admit',data=admt,ax=ax[1])\nax[1].set_title('Admitted to University')\nplt.show()","5137df48":"from sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\n","3f0e7f7c":"admt.head()","ef1d6179":"X=admt.iloc[:,:-2].values\nX[0]","401c0772":"y=admt.iloc[:,-2].values # or we can use y=data.iloc[:,3].values\ny[0]","536abd09":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.05,random_state=0)","fe2b01af":"from sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nreg=linear_model.LinearRegression()\nreg.fit(X_train,y_train)\npred1=reg.predict(X_test)\nprint(\"Mean Squared Error: \",mean_squared_error(y_test,pred1))\n#('Accuracy for Linear Regression is ',metrics.accuracy_score(y_pred,y_test))","7823f106":"y_test","7036b9b0":"pred1","06f39ba0":"Score=['337','118','4','4.5','4.5','9.65','1']\nScore=pd.DataFrame(Score).T\nchance=reg.predict(Score)\nchance","89f17e24":"plt.figure(figsize=(12,8))\ny=pred1\ny1=y_test\nx=np.arange(1, 21, 1)\nx1=np.arange(0,21,2)\nplt.plot(x,y,color='r',marker='o',label='Predicted')\nplt.plot(x,y1,color='g',label='Actual')\nplt.xticks(x1)\nplt.gca().legend(('Predicted','Test'))\nplt.xlabel('Cases',fontsize=20)\nplt.ylabel('Chance of Admission',fontsize=20)\nplt.title('Chance Predicted Vs Actual Values',fontsize=25)\nplt.grid()\nplt.ioff()","546bf424":"admt.head()","c3b06593":"from sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(n_estimators = 1000,random_state = 123)\ncolumns = ['Admit']\nadmt.drop(columns, inplace=True, axis=1)\nX = admt.drop('Chance',axis = 1)\ny = admt['Chance']\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size = .25,random_state = 123)\nrf_model = RandomForestRegressor(n_estimators = 1000,random_state = 123)\nrf_model.fit(X_train,y_train)\nfeature_importance = pd.DataFrame(sorted(zip(rf_model.feature_importances_, X.columns)), columns=['Value','Feature'])\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_importance.sort_values(by=\"Value\", ascending=False))\nplt.xlabel('Value',fontsize=20)\nplt.ylabel('Feature',fontsize=20)\nplt.title('Random Forest Feature Importance',fontsize=25)\nplt.grid()\nplt.ioff()\nplt.tight_layout()","6ce23e95":"admt.head()","9d94be5b":"X=admt_sort.iloc[:,[0,5]].values    # O represents GRE Score and 5 represnts CGPA \ny=admt_sort.iloc[:,8].values        # 8 tells us if the Candidate got Admission or not \n","392d2da5":"from sklearn import preprocessing\nlab_enc = preprocessing.LabelEncoder()\nY = lab_enc.fit_transform(y)","f290e750":"from sklearn.model_selection import train_test_split   #cross_validation doesnt work any more\nX_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=0) \n#y_train","0d7a6a75":"from sklearn.preprocessing import StandardScaler \nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.fit_transform(X_test)\n#X_train","8d59f4bf":"from sklearn.linear_model import LogisticRegression\nclassifier=LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)","6adbd555":"y_pred=classifier.predict(X_test)","026e4641":"from sklearn.metrics import confusion_matrix  #Class has capital at the begining function starts with small letters \ncm=confusion_matrix(y_test,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","17985d90":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_train,y_train\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('yellow','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('Predicting University Admission')\nplt.xlabel('GRE Score')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()","c62ce4eb":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_test,y_test\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('yellow','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('Predicting University Admission')\nplt.xlabel('GRE Score')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()","fa6e2c13":"from sklearn.neighbors import KNeighborsClassifier\nclassifier_4=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nclassifier_4.fit(X_train,y_train)","f8193c42":"y_pred_4=classifier_4.predict(X_test)","b8a1137c":"from sklearn.metrics import confusion_matrix  #Class has capital at the begining function starts with small letters \ncm=confusion_matrix(y_test,y_pred_4)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","0df72d5a":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_train,y_train\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,classifier_4.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('red','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('K-NN (Training set)')\nplt.xlabel('GRE Score')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()","02def2c9":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_test,y_test\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,classifier_4.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('red','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('K-NN (Test set)')\nplt.xlabel('GRE Score')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()","60feeac2":"X=admt_sort.iloc[:,[0,5]].values \n#X","e719a572":"from sklearn.cluster import KMeans\nwcss=[]\nfor i in range(1,11):\n    kmeans=KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,11),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","53042910":"kmeans=KMeans(n_clusters=4,init='k-means++',max_iter=300,n_init=10,random_state=0)\ny_kmeans=kmeans.fit_predict(X)","cfc6a82a":"plt.scatter(X[y_kmeans==0,0],X[y_kmeans==0,1],s=100,c='red',label='Must Improve') \nplt.scatter(X[y_kmeans==1,0],X[y_kmeans==1,1],s=100,c='greenyellow',label='Excellent')  \nplt.scatter(X[y_kmeans==2,0],X[y_kmeans==2,1],s=100,c='yellow',label='Good')   \nplt.scatter(X[y_kmeans==3,0],X[y_kmeans==3,1],s=100,c='green',label='Outstanding')  #cyan\n#plt.scatter(X[y_kmeans==4,0],X[y_kmeans==4,1],s=100,c='burlywood',label='Sensible')\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c='magenta',label='Centroids')\nplt.title('Cluster of Students')\nplt.xlabel('GRE Score')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show","c613aa68":"admt_sort.head()","052ca5a3":"X=admt_sort.iloc[:,0:8].values\n\ny=admt_sort.iloc[:,8].values\n#X\ny","07713847":"from sklearn.model_selection import train_test_split   #cross_validation doesnt work any more\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0) \n#X_train","fa2ae689":"from sklearn.preprocessing import StandardScaler \nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.fit_transform(X_test)\n#X_train","f7a0ddb9":"import keras \nfrom keras.models import Sequential \nfrom keras.layers import Dense ","bee612db":"classifier_6=Sequential()","db4790e7":"classifier_6.add(Dense(output_dim=6,init='uniform',activation='relu',input_dim=8))","3856cb86":"classifier_6.add(Dense(output_dim=5,init='uniform',activation='relu'))","d09a726d":"classifier_6.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))","e56cdf60":"classifier_6.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","dedd4121":"classifier_6.fit(X_train, y_train,batch_size=10,nb_epoch=100)","de85701c":"y_pred=classifier_6.predict(X_test)\ny_pred=(y_pred>0.7)\ny_pred","47afd983":"from sklearn.metrics import confusion_matrix  #Class has capital at the begining function starts with small letters \ncm=confusion_matrix(y_test,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","241ce933":"admt.loc[admt['Chance']>=0.5,['Chance']]=1\nadmt.loc[admt['Chance']<0.5,['Chance']]=0","5a4b3fff":"admt['GRE']=admt['GRE']\/admt['GRE'].max()\nadmt['TOEFL']=admt['TOEFL']\/admt['TOEFL'].max()\nadmt['UnivRating']=admt['UnivRating']\/admt['UnivRating'].max()\nadmt['SOP']=admt['SOP']\/admt['SOP'].max()\nadmt['LOR ']=admt['LOR ']\/admt['LOR '].max()\nadmt['CGPA']=admt['CGPA']\/admt['CGPA'].max()","fdb5a7da":"import keras\n\nX=admt[['GRE','TOEFL','UnivRating','SOP','LOR ','CGPA','Research']]\n# labels y are one-hot encoded, so it appears as two classes \ny = keras.utils.to_categorical(np.array(admt[\"Chance\"]))","f2d63e87":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.7,random_state=100)","7f8b9cf7":"from keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\n\n\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=7))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(32))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","37febb34":"model.fit(X_train, y_train, epochs=1000, batch_size=100, verbose=0)","4c362272":"score = model.evaluate(X_test, y_test)\nprint(score)","5245f07f":"y_pred=model.predict(X_test)\ny_pred","8140f27f":"**1.Linear regression **","e46fa7f3":"**What should be your Scores for 0.9 % Chance of Admission?**","7254afef":"**6.9 Compliling the ANN**","68a2fd56":"**Chance of admission**","360e2aa2":"**3.3 Feature Scaling**","b4eaa3e7":"**3.6 Making the confusion matrix**","d8f6f671":"We can see that the name of the columns have be changed as per our convience.We can see that first  column is serial number it will not have any effect on the chance of admission to the University.We better drop the column of serial number from the data set.","ce3a41de":"For having a 90% Chance to get admission one should have GRE=333.61,TOEFL=116.28,CGPA=9.53 .If you get scores more than this then your chances of admission are very good.","934e02ff":"## Training the model ","b99f30db":"**3.2 Splitting the dataset to Train and Test Set**","e9bedfef":"**2.Decision Tree **","2ed136d7":"**Method to display the columns in the data set **","0ff7ffd0":"**Plotting the distribution of the data**","df43aea9":"We can see that 55% Students have done Research.It possible only the better student could get a chance for doing research.Doing research does add practical knowledge and increases the student skill of working with groups or teams.","8e1e3310":"We can display describe dunction in pictorial way.In most cases the describle table is sufficient for us the get valuable information about the data.","55217f5e":"This data set has the information on the GRE,TOEFL,CGPA and other details of students seeking Post graduation admission at Universities.We will try to exprole the data and see what we can understand from it.The deeper question would be are college degrees revalent in the era of Nano degrees?This is a work in process and I will be updating the kernel in coming days.If you like my work please do vote.","1af4f633":"Heat map gives a good pictorial representation of the correlation of features to our target value chance of admit to university.Looking at the heat map to get the correlation can sometimes be condusing.One way out would be the to get the correlation values against target(Chance) as shown below.","a151d01a":"**6.8 Adding the output layer**","f7019bcc":"**3.5 Predicting the test set results**","48759fd7":"**Lets start with machine learning **","67380289":"**6.12 Making the confusion matrix**","04667b5a":"**5. K Means Clustering: **\nK means is an unsupervised clustering algorithm.We use it here to see how the students will be getting clustered based on their GRE and CGPA Scores.","6c8e88c8":"We can see that the column for serial number is droped or removed from the dataset.","8bcaf432":"**6.3 Feature Scaling**","c65b2c46":"**Importing the modules needed for the analysis **","e1dea47e":"**Importing the data into kernel**","6fcb8496":"## Splitting the data set ","3f545da1":"Splitting the data into training and test data using test size of 0.05","75dcdf86":"**6.1 Generating the Array of Features and Target Values **","95300a6a":"**6.11 Predicting the test set results**","e3c99bb8":"We are assuming here that students with 0.7 chance of admission have secured admission.We create another column in oour dataset named Admit.The value of Admit=1 if Chance>0.7 and Admit=0 if Chance<0.7.","b820e52a":"4. K Means Classification ","62f6f0f8":"**6.4 Importing the Keras Library**","55885cdd":"Based on the cluster I have catogerised the students into four catogeries.\n\n1.Outstanding - GRE> 327 + and CGPA > 8.5\n\n2.Ecxcellent -GRE> 317 + and CGPA > 7.7\n\n3.Good -GRE> 306 + and CGPA > 7.3\n\n4.Must Improve -GRE> 290 + and CGPA > 6.7","dc95a2e3":"We can see that CGPA,GRE,TOEFL,University Ranking has the highest correlation with the chance of admission to the university.The other parameters like SOP,LOR and Research has less impact on the chance of admission.We can dropt he column Srno from our dataframe as it doesnt have any impact on the chance of admission.","d6306af4":"Correct predictions =24+41=65\n\nWrong predictions =6+9=15\n\nAccuracy = (65\/180)*100 =81. 25 %","15501828":"## Defining a model ","dd304ba7":"**3.8 Visualizing the Test Set Results**","d17e1bc1":"**Lets Predict the Chance of Admission **","69b1685f":"Correct predictions =28+39=64 \n\nWrong predictions =6+7=13\n\nAccuracy =(64\/77)*100 =83.11 %","e147c169":"We can clearly see that students with higher GRE and TOEFL scores have very high chance of getting an university admission.","f66fe182":"**Violin plots to reinforce our earlier learning**","a31203ca":"We can see that the maximum Chance of admission is 0.97.Lets find out the scores needed for 90 % chance of admission.","2156ed21":"Correct prediction=30+42=72\n\nWrong predictions =8+0=8\n\nAccuracy of Prediction =(72\/80)*100 =90 %","7cd27d6e":"**Renaming the columns to make our lives easy**","55d0b587":"**3.7 Visualizing the Training Set Results**","9f9aeeee":"**How important is Research to get an Admission?**","ac0b0cfb":"**6.7 Adding the second hidden Layer**","e2539019":"We can see that stutents who have done research do have good TOEFL and GRE Score.","e549c4fa":"**3.1 Generating Array of Features and Target Values**","615a6e03":"**6.10 Fitting the ANN to training set**","7fbcfd57":"Yes your chance of Admission increases if you do Research.","affb01e7":"The Yellow region is the area of people who failed to get admission.Red dots represent the students who failed to get admission.\nGreen Dotd and Green Area represent the people who Managed to get admission.\n\n0-Not Admitted \n\n1-Admitted \n","73983b01":"**6.2 Splitting the dataset to Train and Test Set**","1abe567e":"So the algrothim predicts the value as 0.95 against the actual value 0.92","56681180":"Above box plot shows us the min,median and max values for GRE,TOEFL,University rating and CGPA for the dataset.","4fa37051":"**6.5 Initialising the ANN**","8ca1fa58":"We can see that 59% of the student have high chance of Admission.","83845c63":"**4.4Visualizing the Training Set Results**","5f516af3":"**4.1 Fitting K Nearest  Neighbor to Training set**","5146d10b":"The Values predicted by Linear regression are :","774549cf":"**5.2 Using Elbow method to find the optiminal cluster number**","8dd4d578":"The test values from the dataset are :","8fc2bdec":"**3.Logistic Regression: **\nIt is used to predict binary results.In this case we have crerated the column Admit which tells us detail of Whether the candidate has got admission(1) or not (0).We have seen from the decision tree algorithm that CGPA and the GRE Score has the highest influence on the chance of admission.So while making a Logistic Regression we will use the values of CGPA and GRE score to predict the Admission to the University ","e4604267":"We can clearly see that the student with research have higher chance of admission and their overall all GRE,TOEFL and CPGA scores are also high.","50de452e":"We can see that there are no missing values in the data set.","8d4ff3ef":"We can see from the above plot that we have fairly good correlation.","e74f5b7d":"**6. Artificial Neural Network (ANN)**","f9fbfe7a":"## Feature scaling of the data ","5c99e67a":"**5.3. Applying K means to the Dataset**","d830d7b0":"We can see CGPA,GRE,TOEFL and SOP arte most important features in the data set.","95dde596":"From the Elbow plot we can see that four could be the optiminal number of cluster for this analysis.","7768bd5f":"# Predicting the chance of Admission using ANN","dc518d02":"** Finding out correlations between the features and the chance of admission to the university**","2c4881d0":"Target of an aspirant would be get more than the mean scores displayed above.","bbeca123":"**Lets explore the data **","9782c749":"**3.4 Fitting Logistic Regression into Training set**","382dc3eb":"**5.1 Generating the Array of Features **","295ca36a":"Above method can be used to find out the rows of values in the data set.","a436a368":"Looking at the column names we can see that we can make the names of the colums shorter.","34e50db7":"Predicting the chance for a use case.We give the input to the algorithm in the form of a list as shown below.","97292985":"### Pictorial representation of correlation between the actual and predicted values","e9a359b3":"**Summary Of Dataset**","65010e0b":"**6.6 Adding the input layer and the first hidden layer**","1df818c0":"**4.3 Making the confusion matrix**","e7fe374a":"**4.2 Predicting the test set results**","680435c1":"**5.4 Visualizing the clusters**","530b948a":"**4.5.Visualizing the Test Set Results**"}}