{"cell_type":{"1f8573f5":"code","913f6dc3":"code","e0878b2f":"code","947061e5":"code","22309801":"code","e78e1c1d":"code","704d72d6":"code","4575e9fc":"code","c5012334":"code","109866f7":"code","f7500ef9":"code","7530999a":"code","6f27898e":"code","ff609507":"code","360493f3":"code","14e21737":"code","cc5db9dc":"code","a68ef1e2":"code","33dfb515":"code","e01edddb":"code","7ab653ef":"code","3d0329a2":"code","7f035517":"code","bd756824":"code","bead10fc":"code","5d25d2a7":"code","1f4ea7b1":"code","7f52bfae":"code","585dcdd0":"code","7263b5e6":"code","63e30bb2":"code","6194a6c5":"code","477476d6":"code","bb0261cb":"code","0aec1909":"code","c602414d":"code","5e3d0a9c":"code","32adcd9f":"code","a07b9465":"code","87667c79":"code","3e628ea0":"code","5a8fe5cb":"code","99bcfbb4":"code","0ed2751b":"code","bfd4ffde":"code","9eda9076":"code","d24931cc":"code","9533b565":"code","cab3626f":"code","8f432ce6":"code","87b95ece":"code","220ffae0":"code","9583c8a7":"code","b9b4a873":"code","056706f6":"code","04dc3eb9":"code","2dac48b2":"code","03a8e3d5":"code","963adbe0":"code","b533bc08":"code","562e8202":"markdown","11e654f7":"markdown","9a9aecfd":"markdown","446c3b8d":"markdown","2e771446":"markdown","907490f0":"markdown","8d3836c9":"markdown","2173d996":"markdown","a2b7c7dd":"markdown","8cc61516":"markdown","ffa284eb":"markdown","8a82c80f":"markdown","8095b9f6":"markdown","6d2a3bcf":"markdown","80678f04":"markdown","fadafead":"markdown","6ee5f85c":"markdown","c398d602":"markdown","d916356a":"markdown","a30b6c79":"markdown","a3e9d3df":"markdown","b37ca19e":"markdown","fa196ad7":"markdown","5a318c90":"markdown","4daaf9eb":"markdown","c22d977d":"markdown","e5694ad7":"markdown","f57f3d95":"markdown","1147c319":"markdown","4aa57a4f":"markdown","5419723f":"markdown","77972d5c":"markdown","fee2c16e":"markdown","fc687146":"markdown","cb147d02":"markdown","0542e0e0":"markdown","a668b697":"markdown","a4473b64":"markdown","f570435a":"markdown","76cf501a":"markdown","ae6159fc":"markdown","085b1145":"markdown","4dfa8e87":"markdown","2abf7feb":"markdown"},"source":{"1f8573f5":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","913f6dc3":"# Read the labeled training and test data\n# Header = 0 indicates that the first line of the file contains column names, \n# delimiter = \\t indicates that the fields are seperated by tabs, and \n# quoting = 3 tells python to ignore doubled quotes\n\ntrain = pd.read_csv('..\/input\/labeledTrainData.tsv', header = 0, delimiter = '\\t', quoting = 3)\ntest = pd.read_csv('..\/input\/testData.tsv', header = 0, delimiter = '\\t', quoting = 3)\nunlabel_train = pd.read_csv('..\/input\/unlabeledTrainData.tsv', header = 0, delimiter = '\\t', quoting = 3)","e0878b2f":"'train dim:{}, unlabeled train dim:{}, test dim:{}'.format(train.shape, unlabel_train.shape, test.shape)","947061e5":"# Import the libraries for data cleaning.\n\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk","22309801":"def preprocess_wordlist(data, stopwords = False):\n    \n    # Remove HTML tag\n    review = BeautifulSoup(data,'html.parser').get_text()\n    \n    # Remove non-letters\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    \n    # Convert to lower case\n    review = review.lower()\n    \n    # Tokenize\n    word = nltk.word_tokenize(review)\n    \n    # Optional: Remove stop words (false by default)\n    if stopwords:\n        stops = set(nltk.corpus.stopwords.words(\"english\"))\n        \n        words = [w for w in word if not w in stops]\n    \n    return word\n","e78e1c1d":"def preprocess_sent(data, stopwords = False):\n    \n    # Split the paragraph into sentences\n    \n    #raw = tokenizer.tokenize(data.strip())\n    raw = nltk.sent_tokenize(data.strip())\n    \n    # If the length of the sentence is greater than 0, plug the sentence in the function preprocess_wordlist (clean the sentence)\n    sentences = [preprocess_wordlist(sent, stopwords) for sent in raw if len(sent) > 0]\n    \n    return sentences","704d72d6":"sentence = []\n\n# Append labeled reviews first\nfor review in train['review']:\n    sentence += preprocess_sent(review)\n    \n# Append unlabeled reviews\nfor review in unlabel_train['review']:\n    sentence += preprocess_sent(review)\n","4575e9fc":"print(len(sentence))\nprint()\nprint(sentence[:2])","c5012334":"train['review'][0]","109866f7":"from gensim.models import word2vec","f7500ef9":"num_features = 250 #400\nmin_count = 40\nnum_processor = 4\ncontext = 10\ndownsampling = 0.001","7530999a":"# Plug in the sentence variable first.\n\nmodel = word2vec.Word2Vec(sentence, workers = num_processor, \n                         size = num_features, min_count = min_count,\n                         window = context, sample = downsampling)","6f27898e":"# Unload unneccessary memory once the learning process is done.\n\nmodel.init_sims(replace = True)","ff609507":"model_name = \"250features_40minwords_20context\"\nmodel.save(model_name)","360493f3":"model.most_similar(\"king\")","14e21737":"# Import libraries\nfrom sklearn.manifold import TSNE\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","cc5db9dc":"# List of vocabulary.\n\nvocab = list(model.wv.vocab)\n\n# index vector values by corresponding vocab list\n\nX = model[vocab]\n\nprint(\"Total Number of Vocab:\", len(X))\nprint()\nprint(X[0][:10])\n\n# Visualize only 100 words.\n\ntsne = TSNE(n_components = 2)\nX_tsne = tsne.fit_transform(X[:100,:])","a68ef1e2":"df = pd.DataFrame(X_tsne, index = vocab[:100], columns = ['X','Y'])\ndf.head()","33dfb515":"fig = plt.figure()\nfig.set_size_inches(30,20)\n\nax = fig.add_subplot(1,1,1)\nax.scatter(df['X'], df['Y'])\n\n# Put the label on each point.\nfor word, pos in df.iterrows():\n    ax.annotate(word, pos, fontsize = 30)\nplt.show()","e01edddb":"''' \n\nThe purpose of this function is to combine all the word2vec vector values of each word in each review\nif each review is given as input and divide by the total number of words.\n\nEach word can be represented as number of feature dimension space vector. ex) model['flower'] = array([0.1, 0.2, ...]).\n(You can think of it as extended xy coordinate.) Therefore, it enables vectorizing each review by \ncombining all the words' vector values.\n\nIllustration example:\n\n'I' = [0.1, 0.2, ...]\n'have' = [0.2, 0.3, ...]\n'a car' = [0.1, 0.2, ...]\n'I have a car' = [0.1 + 0.2 + 0.1, 0.2 + 0.3 + 0.2,  ...]\n\n\nex) review1 = ['he', 'has', 'a', 'cat']\n\nFirst word : If the word 'he' is in the word2vec, index the vector values from word2vec model by model['he']\n(the dimension of the matrix would be, in this case, (400,) ) and add them to predefined zero matrix.\n\nSecond word: Check if there is the word 'has' in the word2vec model and if there is, index the vector values and \nadd them on top of the added vector values from the first word.\n\nThe rest: Iterate the above steps for the rest of words and lastly, divide by the total number of words. \n\nIllustration example: \n\nzero : [0,    0,   0,   ....]\nword1: [0.2,  0.3, 0.4, ....]\nword2: [0.1,  0.2, 0.3, ....]\n\nword(1~2): [0.3, 0.5, 0.7, ....]\n\n'''\n\ndef makeFeatureVec(review, model, num_features):\n    \n    featureVec = np.zeros((num_features,), dtype = \"float32\")\n    \n    # Unique word set\n    word_index = set(model.wv.index2word)\n    \n    # For division we need to count the number of words\n    nword = 0\n    \n    # Iterate words in a review and if the word is in the unique wordset, add the vector values for each word.\n    for word in review:\n        if word in word_index:\n            nword += 1\n            featureVec = np.add(featureVec, model[word])\n    \n    # Divide the sum of vector values by total number of word in a review.\n    featureVec = np.divide(featureVec, nword)        \n    \n    return featureVec","7ab653ef":"''' \n\nWhile iterating over reviews, add the vector sums of each review from the function \"makeFeatureVec\" to \nthe predefined vector whose size is the number of total reviews and the number of features in word2vec.\nThe working principle is basically same with \"makeFeatureVec\" but this is a review basis and \nmakeFeatureVec is word basis (or each word's vector basis)\n\n\nreturn matrix:\n\n            'V1'    'V2'    'V3'     'V4'\nreview 1    0.1      0.2     0.1     0.5\nreview 2    0.5      0.4     0.05    0.05\n\n'''\n\ndef getAvgFeatureVec(clean_reviews, model, num_features):\n    \n    # Keep track of the sequence of reviews, create the number \"th\" variable.\n    review_th = 0\n    \n    # Row: number of total reviews, Column: number of vector spaces (num_features = 250 we set this in Word2Vec step).\n    reviewFeatureVecs = np.zeros((len(clean_reviews), num_features), dtype = \"float32\")\n    \n    # Iterate over reviews and add the result of makeFeatureVec.\n    for review in clean_reviews:\n        reviewFeatureVecs[int(review_th)] = makeFeatureVec(review, model, num_features)\n        \n        # Once the vector values are added, increase the one for the review_th variable.\n        review_th += 1\n    \n    return reviewFeatureVecs","3d0329a2":"clean_train_reviews = []\n\n# Clean the reviews by preprocessing function with stopwords option \"on\".\nfor review in train[\"review\"]:\n    clean_train_reviews.append(preprocess_wordlist(review, stopwords = True))\n\n# Apply \"getAvgFeatureVec\" function.\ntrainDataAvg = getAvgFeatureVec(clean_train_reviews, model, num_features)\n    \n    \n# Same steps repeats as we did for train_set.    \nclean_test_reviews = []\n\nfor review in test[\"review\"]:\n    clean_test_reviews.append(preprocess_wordlist(review, stopwords = True))\n\ntestDataAvg = getAvgFeatureVec(clean_test_reviews, model, num_features)","7f035517":"from sklearn.cluster import KMeans\nimport time","bd756824":"print(model.wv.syn0.shape)","bead10fc":"num_clusters = model.wv.syn0.shape[0] \/\/ 5","5d25d2a7":"start = time.time()\n\nkmean = KMeans(n_clusters = num_clusters)\nindex = kmean.fit_predict(model.wv.syn0)\n\nend = time.time()\nprint(\"Time taken for K-Means clustering: \", end - start, \"seconds.\")","1f4ea7b1":"index = list(index)\nvoca_list = model.wv.index2word\n\n# dictionary format -  word : the cluster where the key word belongs.\nvoca_cluster = {voca_list[i]: index[i] for i in range(len(voca_list))}","7f52bfae":"# Check the first 10 clusters in voca_cluster we created.\n\n# Loop from 0 to 9th cluster \nfor cluster in range(10):\n    \n    word = []\n    \n    # Iterate over the number of total words. \n    for i in range(len(voca_cluster.values())):\n        \n        # If the cluster (0~10) corresponds to iterating i th voca_cluster values (cluster),\n        if(list(voca_cluster.values())[i] == cluster):\n            \n            # Append the words.\n            word.append(list(voca_cluster.keys())[i])\n    \n    print(word)","585dcdd0":"# Preprocess data for input as before\n\ntrain_review = []\n\nfor review in train['review']:\n    train_review.append(preprocess_wordlist(review, stopwords= True))\n\ntest_review = []\n\nfor review in test['review']:\n    test_review.append(preprocess_wordlist(review, stopwords = True))","7263b5e6":"train_centroid = np.zeros((len(train['review']), num_clusters), dtype = 'float32')\ntest_centroid = np.zeros((len(test['review']), num_clusters), dtype = 'float32')","63e30bb2":"'''\nThe array that we are going to create looks like this:\n\ncl1 cl2 cl3 cl4 ....\n 3   10  5   30 ...\n\nAs usual we will be creating the empty array having the number of clusters dimension space.\nWhile Iterating over words, if there is any word found in the voca_cluster, find the cluster where the word belongs to\nand add one to the feature corresponding to the cluster.\n\n( ex) if 'cat' assigned to cluster 10 then add one to 10th feature in the empty array. )  \n\n'''\n\ndef create_boc(wordlist, voca_cluster):\n    \n    # The number of cluster == the maximum number of values in voca_cluster\n    boc = np.zeros(max(voca_cluster.values()) + 1, dtype='float32')\n    \n    # Iterate over words and increase by one to the cluster if any word in the voca_cluster we created\n    for word in wordlist:\n        if word in voca_cluster:\n            index = voca_cluster[word]\n            boc[index] += 1\n            \n    return boc\n    ","6194a6c5":"# Transform the training and test set reviews into bags of centroid.\n\ncount = 0\n\nfor review in train_review:\n    train_centroid[count] = create_boc(review, voca_cluster)\n    count += 1\n    \ncount = 0\n\nfor review in test_review:\n    test_centroid[count] = create_boc(review, voca_cluster)\n    count += 1\n    ","477476d6":"print(\"Train Dimension (avg):\",trainDataAvg.shape,\",\", \"Train Dimension (centroid):\",train_centroid.shape)","bb0261cb":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, learning_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neural_network import MLPClassifier","0aec1909":"kfold = StratifiedKFold(n_splits=5, random_state = 2018)","c602414d":"# LinearSVC\n\nsv = LinearSVC(random_state=2018)\n\nparam_grid1 = {\n    'loss':['squared_hinge'],\n    'class_weight':[{1:2}],\n    'C': [20],\n    'penalty':['l2']\n}\n\ngs_sv = GridSearchCV(sv, param_grid = [param_grid1], verbose = 1, cv = kfold, n_jobs = 1, scoring = 'roc_auc' )\ngs_sv.fit(trainDataAvg, train['sentiment'])\ngs_sv_best = gs_sv.best_estimator_\nprint(gs_sv.best_params_)\n\n# {'C': 20, 'class_weight': {1: 2}, 'loss': 'squared_hinge', 'penalty': 'l2'} - 86.30","5e3d0a9c":"y_submission1 = gs_sv.predict(testDataAvg)","32adcd9f":"print(gs_sv.best_score_)","a07b9465":"bnb = BernoulliNB()\ngs_bnb = GridSearchCV(bnb, param_grid = {'alpha': [0.002],\n                                         'binarize': [0.001]}, verbose = 1, cv = kfold, n_jobs = 1, scoring = 'roc_auc')\ngs_bnb.fit(trainDataAvg, train['sentiment'])\ngs_bnb_best = gs_bnb.best_estimator_\nprint(gs_bnb.best_params_)\n\n# {'alpha': 0.002, 'binarize': 0.001} - 68.348","87667c79":"y_submission2 = gs_bnb.predict(testDataAvg)","3e628ea0":"print(gs_bnb.best_score_)","5a8fe5cb":"MLP = MLPClassifier(random_state = 2018)\n\nmlp_param_grid = {\n    'hidden_layer_sizes':[(10,10)],\n    'activation':['tanh'],\n    'solver':['adam'],\n    'alpha':[0.01],\n    'learning_rate':['constant'],\n    'max_iter':[1000]\n}\n\ngsMLP = GridSearchCV(MLP, param_grid = mlp_param_grid, cv = kfold, scoring = 'roc_auc', n_jobs= 1, verbose = 1)\ngsMLP.fit(trainDataAvg,train['sentiment'])\nprint(gsMLP.best_params_)\nmlp_best0 = gsMLP.best_estimator_\n\n# {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (1,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'} - 87.012\n# {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (2,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'} - 86.960\n# {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'} - 87.020\n# {'activation': 'tanh', 'alpha': 0.009, 'hidden_layer_sizes': (5,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'} - 87.004\n# {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'} - 87.108","99bcfbb4":"y_submission3 = gsMLP.predict(testDataAvg)","0ed2751b":"print(gsMLP.best_score_)","bfd4ffde":"lr = LogisticRegression(random_state = 2018)\n\n\nlr_param2 = {\n    'penalty':['l1'],\n    'dual':[False],\n    'C':[40],\n    'class_weight':['balanced'],\n    'solver':['saga']\n    \n}\n\nlr_CV = GridSearchCV(lr, param_grid = [lr_param2], cv = kfold, scoring = 'roc_auc', n_jobs = 1, verbose = 1)\nlr_CV.fit(trainDataAvg,train['sentiment'])\nprint(lr_CV.best_params_)\nlogi_best = lr_CV.best_estimator_\n\n\n# {'C': 100, 'class_weight': 'balanced', 'dual': False, 'penalty': 'l1', 'solver': 'saga'} - 87.376\n# {'C': 50, 'class_weight': 'balanced', 'dual': False, 'penalty': 'l1', 'solver': 'saga'} - 87.380\n# {'C': 40, 'class_weight': 'balanced', 'dual': False, 'penalty': 'l1', 'solver': 'saga'} - 87.424","9eda9076":"y_submission4 = lr_CV.predict(testDataAvg)","d24931cc":"print(lr_CV.best_score_)","9533b565":"# LinearSVC\n\nsv = LinearSVC(random_state=2018)\n\nparam_grid1 = {\n    'loss':['squared_hinge'],\n    'class_weight':['balanced'],\n    'C': [0.001],\n    'penalty':['l2']\n}\n\ngs_sv = GridSearchCV(sv, param_grid = [param_grid1], verbose = 1, cv = kfold, n_jobs = 1, scoring = 'roc_auc' )\ngs_sv.fit(train_centroid, train['sentiment'])\ngs_sv_best = gs_sv.best_estimator_\nprint(gs_sv.best_params_)\n\n# {'C': 0.001, 'class_weight': 'balanced', 'loss': 'squared_hinge', 'penalty': 'l2'} - 87.256 ","cab3626f":"y_submission11 = gs_sv.predict(test_centroid)","8f432ce6":"print(gs_sv.best_score_)","87b95ece":"bnb = BernoulliNB()\ngs_bnb = GridSearchCV(bnb, param_grid = {'alpha': [0.01],\n                                         'binarize': [0.001]}, verbose = 1, cv = kfold, n_jobs = 1, scoring = 'roc_auc')\ngs_bnb.fit(train_centroid, train['sentiment'])\ngs_bnb_best = gs_bnb.best_estimator_\nprint(gs_bnb.best_params_)\n\n# {'alpha': 0.01, 'binarize': 0.001} - 81.87","220ffae0":"y_submission22 = gs_bnb.predict(test_centroid)","9583c8a7":"print(gs_bnb.best_score_)","b9b4a873":"MLP = MLPClassifier(random_state = 2018)\n\nmlp_param_grid = {\n    'hidden_layer_sizes':[(5,)],\n    'activation':['tanh'],\n    'solver':['sgd'],\n    'alpha':[1],\n    'learning_rate':['adaptive'],\n    'max_iter':[1000]\n}\n\n\ngsMLP = GridSearchCV(MLP, param_grid = mlp_param_grid, cv = kfold, scoring = 'roc_auc', n_jobs= 1, verbose = 1)\ngsMLP.fit(train_centroid, train['sentiment'])\nprint(gsMLP.best_params_)\nmlp_best0 = gsMLP.best_estimator_\n\n# {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (1,), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'sgd'} - 87.092\n# {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (5,), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'sgd'} - 87.068\n# {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (5,), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'sgd'} - 87.068","056706f6":"y_submission33 = gsMLP.predict(test_centroid)","04dc3eb9":"print(gsMLP.best_score_)","2dac48b2":"lr = LogisticRegression(random_state = 2018)\n\nlr_param2 = {\n    'penalty':['l1'],\n    'dual':[False],\n    'class_weight':[{1:2}],\n    'C': [100],\n    'solver':['saga']\n    \n}\n\nlr_CV = GridSearchCV(lr, param_grid = [lr_param2], cv = kfold, scoring = 'roc_auc', n_jobs = 1, verbose = 1)\nlr_CV.fit(train_centroid,train['sentiment'])\nprint(lr_CV.best_params_)\nlogi_best = lr_CV.best_estimator_\n\n\n# {'C': 100, 'class_weight': {1: 2}, 'dual': False, 'penalty': 'l1', 'solver': 'saga'} - 85.968","03a8e3d5":"y_submission44 = lr_CV.predict(test_centroid)","963adbe0":"print(lr_CV.best_score_)","b533bc08":"result = pd.DataFrame(data = {'id':test['id'], 'sentiment': y_submission4})\nresult.to_csv('submission_29.csv', index = False, quoting = 3)","562e8202":"## 9. Conclusion <a id='conclusion'><\/a>","11e654f7":"### 3.3 Combining Labeled and Unlabeled Reviews <a id='clur'><\/a>","9a9aecfd":"#### 7.24 Logistic Regression <a id='logi2'><\/a>","446c3b8d":"#### 6.2.2 Mapping Vocabulary to Cluster <a id='mvc'><\/a>","2e771446":"### 7.2 Modeling for Clustering <a id='mc'><\/a>","907490f0":"For creating a model, unlike Bag Of Words method, it is not necessary to remove the stopwords such as \"the\", \"a\", \"and\", etc as the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors. Another thing to mention is that it may improve the accuracy to keep numbers from the data but for the simplicity, I decide to delete everything except alphabet characters.","8d3836c9":"This is the third notebook for IMDb sentiment analysis (First notebook: [Bag of Words Meets Bags of Popcorn: CountVectorizer](url_here), Second notebook: [Bag of Words Meets Bags of Popcorn: TF-IDF](url_here)). In this notebook, for a bit deeper sentiment analysis, we will be using Word2Vec to train a model. Word2Vec is a neural network based algorithm that creates distributed word vectors.\nOne of the advantages of Word2Vec over Bag Of Words, it understands meaning and semantic relationships among words. Also, it does not require labels in order to create meaningful representations. In addition that in a aspect of speed, it learns quickly compared to other methods. Once the Word2Vec is created, we will visualize the vectorized data by t-SNE. Before we dive into modeling, there is one more step to prepare two different matrices created by using Word2Vec by vector averaging method and clustering method. We will test them independently and choose the best one among the results predicted by different methods including 'bag of words' and 'TF-IDF.","2173d996":"word2vec's each row represents each word appeared in data after cleaning process. The column represents each variable of vector value. In other word, words * features matrix.","a2b7c7dd":"### 3.2 Splitting a Paragraph to Sentences <a id='sps'><\/a>","8cc61516":"## 6. From Words to Paragraph <a id='fwp'><\/a>","ffa284eb":"## 8. Submission <a id='submission'><\/a>","8a82c80f":"We will be evaluating below algorithms' cross validation means and errors. \n\n- Linear Support Vector Machine\n- Bernoulli Naive Bayes\n- Perceptron\n- Logistic Regression","8095b9f6":"#### 7.14 Logistic Regression <a id='logi1'><\/a>","6d2a3bcf":"### 7.1 Modeling for Vector Averaging <a id='mva'><\/a>","80678f04":"Worker threads : Number of parallel processes to run. One thing to remember here is that unlike sklearn, it does not accept -1 option to use all the processors. <br>\n\nDownsampling of frequent words : According to the Google documentation, values between 0.00001 and 0.001 would suffice. <br>\n\nContext : How many words around the target word will be used? <br>\n\nMinimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. The reason why I chose 40 is that there are 30 reviews in each movie and repeat the title 30 times; therefore in order to avoid attaching too much importance to individual movie titles, I set to 40. <br>\n\nWord vector dimensionality: Self-explanatory. \n","fadafead":"### 6.1 Vector Averaging <a id='vaveraging'><\/a>","6ee5f85c":"# 2. Bag of Words Meets Bags of Popcorn : Word2Vec","c398d602":"Based on what I have experimented, small clusters, with an average of only 5 words or so per cluster, gave better results than large clusters with many words. Feel free to change it to improve the score.","d916356a":"The most important thing to notice is splitting sentences by sentences __prior to__ splitting them to single word because you will lose the information which words have formed a sentence if you split into words first. Some may question that simple split function might be enough to split up a paragraph to sentences; however, English sentences can end with many punctuations and it is not strange that anything can be placed at the end of the sentences. For this reason, when we split text into sentences, we need to use NLTK's punkt tokenizer (sent_tokenize function uses an instance of a punkt tokenizer). The punkt tokenizer is an unsupervised trainable model ,which enables that the tokenizer uses to decide where to cut or learn abbreviations in text.","a30b6c79":"Due to the constraint of memory and CPU power, num_features option was reduced from what I first intend to use 400 to 250. If your system is sustainable enough to run K-Means clustering with 400 features or even more, feel free to change the number and try out.","a3e9d3df":"The above plot indicates a lot of information about the movie. First, some groups consist of very similar words. \"michael\", \"jackson\" and \"mj\" are located very closely and \"his\", \"he\", and \"guy\" are also very close to each other or \"bad\" and \"boring\". On the other hand, some words are grouped with totally opposite words such as \"innocent\" and \"guilty\" or \"want\" and \"hate\". From this fact, without even reading the actual reviews, we can presume that many positive reviews came from Michael Jackson fans and many negative reviews came from non-Micael Jackson fans because as mentioned above, the totally opposite meaning of words are clustered very closely like \"hate\" and \"like\".<br> \n\nWe can find some very informative words describing the movie including \"eighties\", \"music\", \"documentary\", \"biography\", \"remember\", \"unless\". By these words, we can easily infer that the first review is about Michael Jackson biographical documentary. Also, this move would be bad or boring unless you are a fan of Michael Jackson.<br>\n\nLastly, one more pattern to notice is stopword groups: \"the\", \"a\", \"an\" and \"is\", \"was\", \"are\", \"s\", \"ve\", \"has\". Even though they do not give any information about the review, the fact that the stopwords are clustered together closely shows how well the t-sne performed.<br>\n\nCompared to the previous methods' (CountVectorizer, TF-IDF) WordCloud, it appears more informative and based on the above plot, we can capture the general shape of the first review.","b37ca19e":"The best result in this notebook is 87.424% performed by Logistic Regression in Vector Averaging. You may wonder why the result of the bag of words method (CountVectorizer\/TF-IDF) performs better. The main reason is that while making the average vector and centroid matrices, the order of words were lost, which makes it very similar to the concept of bag of words. You may try Paragraph Vectors instead of Word Vector to fix the problem of word order since the paragraph vector preserves word order information. Another reason is that as mentioned earlier, due to constraint of my laptop's memory and CPU power, using more than 250 features throws a memory error. If your system is sustainable enough, you can improve the result by increasing the number of features or use more complex models such as XG Boost.<br>\n\nAmong the all methods that I attempted, the best result is 90.36% performed by Logistic Regression in TF-IDF. As you notice, all the best results come from Logistic Regression and the worst come from Naive Bayes.<br>","fa196ad7":"#### 7.22 Bernoulli Naive Bayes <a id='bnb2'><\/a>","5a318c90":"Notice that we use stop word removal, which would just add noise.<br>\nWe will compare the performance of vector averaging method and the next method. ","4daaf9eb":"Check whether the clustering model that we created works by printing words belongs to the first ten clusters","c22d977d":"## 5. Visualization <a id='viz'><\/a>\n\nAs the Word2Vec has more than two features, it is hard to directly plot the whole data. We will be using the technique called t-SNE so that we can reduce the dimensions from 250 to 2, which enables to visualize more human-friendly and see the relationship among points (words).","e5694ad7":"#### 7.23 Perceptron <a id='perceptron2'><\/a>","f57f3d95":"#### 7.11 Support Vector Machine <a id='svm1'><\/a>","1147c319":"#### 7.21 Support Vector Machine <a id='svm2'><\/a>","4aa57a4f":"### 3.1 Splitting a Sentence to Words <a id='ssw'><\/a>","5419723f":"## Table of Contents","77972d5c":"## 7. Modeling <a id='modeling'><\/a>","fee2c16e":"#### 7.12 Bernoulli Naive Bayes <a id='bnb1'><\/a>","fc687146":"#### 7.13 Perceptron <a id='perceptron1'><\/a>","cb147d02":"## 4. Word2Vec <a id='word2vec'><\/a>","0542e0e0":"One thing to notice is that unlike the previous two notebook, now we can use a unlabeled data set since as mentioned to the introduction, word2vec can learn from unlabeled data.","a668b697":"#### 6.2.1 KMeans Clustering <a id='kmc'><\/a>","a4473b64":"In general, there are two types of architecture options: skip-gram (default) and CBOW (continuous bag of words). Most of time, skip-gram is little bit slower but has more accuracy than CBOW. CBOW is the method to predict one word by whole text; therefore, small set of data is more favorable. On the other hand, skip-gram is totally opposite to CBOW. With the target word, skip-gram is the method to predict the words around the target words. The more data we have, the better it performs. As the architecture, there are two training algorithms for Word2Vec: Hierarchical softmax (default) and negative sampling. We will be using the default.","f570435a":"1. [Introduction](#intro)<br><br>\n2. [Reading the Data](#reading)<br><br>\n3. [Data Cleaning and Text Preprocessing](#dctp)<br> - [3.1. Splitting a Sentence to Words](#ssw)<br> - [3.2. Splitting a Paragraph to Sentences](#sps)<br> - [3.3. Combining Labeled and Unlabeled Reviews](#clur)<br><br>\n4. [Word2Vec](#word2vec)<br><br>\n5. [Visualization](#viz)<br><br>\n6. [From Words to Paragraph](#fwp)<br> - [6.1. Vector Averaging](#vaveraging)<br> - [6.2. Clustering](#clustering)<br> - [6.2.1 KMeans Clustering](#kmc)<br> - [6.2.2. Mapping Vocabulary to Cluster](#mvc)<br> - [6.2.3. Bag of Centroids](#boc)<br><br>\n7. [Modeling](#modeling)<br>- [7.1. Modeling for Vector Averaging](#mva)<br> - [7.1.1. Linear Support Vector Machine](#svm1)<br>- [7.1.2. Bernoulli Naive Bayes Classifier](#bnb1)<br>- [7.1.3. Perceptron](#perceptron1)<br> - [7.1.4. Logistic Regression](#logi1)<br> - [7.2. Modeling for Clustering](#mc)<br> - [7.2.1. Linear Support Vector Machine](#svm2)<br> - [7.2.2. Bernoulli Naive Bayes Classifier](#bnb2)<br> - [7.2.3. Perceptron](#perceptron2)<br> - [7.2.4. Logistic Regression](#logi2)<br><br>\n8. [Submission](#submission)<br><br>\n9. [Conclusion](#conclusion)<br><br>\n\nFirst notebook: [Bag of Words Meets Bags of Popcorn: CountVectorizer](https:\/\/www.kaggle.com\/kyen89\/0-sentiment-analysis-countvectorizer\/)<br>\nSecond notebook: [Bag of Words Meets Bags of Popcorn: TF-IDF](https:\/\/www.kaggle.com\/kyen89\/1-sentiment-analysis-tf-idf)","76cf501a":"## 1. Introduction <a id=\"intro\"><\/a>","ae6159fc":"### 6.2 Clustering <a id='clustering'><\/a>\n\nWord2Vec creates clusters of semantically related words, so we can try to use the similarity of words within a cluster. To achieve this, we will be using one of the most commonly used clustering technique called K-Means.","085b1145":"## 2. Reading the Data <a id=\"reading\"><\/a>","4dfa8e87":"#### 6.2.3 Bag of Centroids <a id='boc'><\/a>","2abf7feb":"## 3. Data Cleaning and Text Preprocessing <a id='dctp'><\/a>\n"}}