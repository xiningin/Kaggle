{"cell_type":{"3d4e7e97":"code","63865c01":"code","2f03af23":"code","0e733942":"code","18ff4627":"code","14d437c3":"code","1b98c85d":"code","88c4c88c":"code","cbc759c8":"code","0100dbf9":"code","98975055":"code","d7ec8718":"code","7b383fb8":"code","335dc3eb":"code","ade9eb88":"code","984d3fe0":"code","395456be":"code","8a0ea0f7":"code","c7c366ca":"code","a6fa276b":"code","4536aaf0":"code","72117360":"code","3efdcf67":"code","47770e73":"code","ca5ce275":"code","1444053e":"code","674a61fb":"code","caa613ce":"code","a3d04ebc":"code","8914c842":"code","353d7605":"code","e534aa51":"code","bb5ced9e":"code","60ba5ca5":"code","2b3019f7":"code","9f445bf0":"code","54dff27b":"code","87430371":"code","e941d8ff":"code","c5910619":"code","c0048528":"code","78ff0d11":"code","59cbe628":"code","a9968b3c":"code","203fb209":"code","da3745b7":"code","d2e4fd75":"code","5056fe6b":"code","bc992410":"markdown"},"source":{"3d4e7e97":"\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","63865c01":"df=pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","2f03af23":"df.shape","0e733942":"df.isnull().sum()","18ff4627":"df.head()","14d437c3":"import seaborn as sns","1b98c85d":"sns.countplot('target',data=df)","88c4c88c":"df.head()","cbc759c8":"a = pd.get_dummies(df['sex'], prefix = \"sex\")\nb = pd.get_dummies(df['cp'], prefix = \"cp\")\nc = pd.get_dummies(df['fbs'], prefix = \"fbs\")\nd = pd.get_dummies(df['restecg'],prefix='restecg')\ne = pd.get_dummies(df['exang'],prefix='exang')\nf = pd.get_dummies(df['slope'],prefix='slope')\ng = pd.get_dummies(df['thal'],prefix='thal')\nh = pd.get_dummies(df['ca'],prefix='ca')","0100dbf9":"df2 = [df, a, b, c,d,e,f,g,h]\ndf = pd.concat(df2, axis = 1)\ndf.head()","98975055":"pd.set_option('display.max_columns', None)\n\ndf = df.drop(columns = ['cp', 'thal', 'slope','sex','fbs','restecg','ca','exang'])\ndf.head()","d7ec8718":"from sklearn.ensemble import RandomForestClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz \nfrom sklearn.metrics import roc_curve, auc \nfrom sklearn.metrics import classification_report \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.model_selection import train_test_split ","7b383fb8":"x=df.drop('target',axis=1)","335dc3eb":"y=df['target']","ade9eb88":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n","984d3fe0":"\nx_train = x_train.values\nx_test = x_test.values\ny_train = y_train.values\ny_test = y_test.values","395456be":"x_train","8a0ea0f7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"RandomForestClassifier\":RandomForestClassifier()\n}","c7c366ca":"import warnings\nwarnings.filterwarnings(\"ignore\")\n","a6fa276b":"from sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(x_train, y_train)\n    training_score = cross_val_score(classifier, x_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","4536aaf0":"from sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(x_train, y_train)\n#We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(x_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# # Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(x_train, y_train)\n\n# # SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# # DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(x_train, y_train)\n\n# # tree best estimator\ntree_clf = grid_tree.best_estimator_\n\n##random forest classifier\nrfc=RandomForestClassifier(random_state=42)\n\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid)\nCV_rfc.fit(x_train, y_train)\nrandom_tree_clf=CV_rfc.best_estimator_","72117360":"# random_tree_clf=CV_rfc.best_params_","3efdcf67":"log_reg_score = cross_val_score(log_reg, x_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n\nknears_score = cross_val_score(knears_neighbors, x_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\nsvc_score = cross_val_score(svc, x_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, x_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')\nrandom_forest_score = cross_val_score(random_tree_clf, x_train, y_train, cv=5)\nprint('Random forest Classifier Cross Validation Score', round(random_forest_score.mean() * 100, 2).astype(str) + '%')","47770e73":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n\nlog_reg_pred = cross_val_predict(log_reg, x_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nknears_pred = cross_val_predict(knears_neighbors, x_train, y_train, cv=5)\n\nsvc_pred = cross_val_predict(svc, x_train, y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, x_train, y_train, cv=5)\nrandom_forest_pred = cross_val_predict(random_tree_clf, x_train, y_train, cv=5)\n","ca5ce275":"from sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))\nprint('Random Forest Classifier: ', roc_auc_score(y_train, random_forest_pred))","1444053e":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\ny_pred_log_reg = log_reg.predict(x_test)\n\ny_pred_knear = knears_neighbors.predict(x_test)\ny_pred_svc = svc.predict(x_test)\ny_pred_tree = tree_clf.predict(x_test)\ny_pred_forest = random_tree_clf.predict(x_test)\n\nlog_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\nkneighbors_cf = confusion_matrix(y_test, y_pred_knear)\nsvc_cf = confusion_matrix(y_test, y_pred_svc)\ntree_cf = confusion_matrix(y_test, y_pred_tree)\nforest_cf = confusion_matrix(y_test, y_pred_forest)\n\n\nfig, ax = plt.subplots(2, 2,figsize=(22,12))\n\n\nsns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.copper)\nax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=14)\nax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.copper)\nax[0][1].set_title(\"KNearsNeighbors \\n Confusion Matrix\", fontsize=14)\nax[0][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.copper)\nax[1][0].set_title(\"Suppor Vector Classifier \\n Confusion Matrix\", fontsize=14)\nax[1][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(tree_cf, ax=ax[1][1], annot=True, cmap=plt.cm.copper)\nax[1][1].set_title(\"DecisionTree Classifier \\n Confusion Matrix\", fontsize=14)\nax[1][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\n# sns.heatmap(forest_cf, ax=ax[2][0], annot=True, cmap=plt.cm.copper)\n# ax[2][0].set_title(\"Random forest Classifier \\n Confusion Matrix\", fontsize=14)\n# ax[2][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\n# ax[2][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\n\nplt.show()","674a61fb":"sns.heatmap(forest_cf,annot=True, cmap=plt.cm.copper)","caa613ce":"from sklearn.metrics import classification_report\n\n\nprint('Logistic Regression:')\nprint(classification_report(y_test, y_pred_log_reg))\n\nprint('KNears Neighbors:')\nprint(classification_report(y_test, y_pred_knear))\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_svc))\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_tree))\nprint('random forest Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_forest))","a3d04ebc":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\nrandom_forest_fpr, random_forest_tpr, random_forest_threshold = roc_curve(y_train, random_forest_pred)\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr,random_forest_fpr,random_forest_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot(random_forest_fpr, random_forest_tpr, label='Random forest Classifier Score: {:.4f}'.format(roc_auc_score(y_train, random_forest_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr,random_forest_fpr,random_forest_tpr)\nplt.show()","8914c842":"\nlog_reg.predict([x_test[1]])[0]","353d7605":"y_test[1]","e534aa51":"data = [{'age': 63, 'sex': 1, 'cp':3,'trestbps':145, 'chol':233, 'fbs':1, 'restecg':0, 'thalach':150, 'exang':0, 'oldpeak':2.3, 'slope':0, 'ca':0, 'thal':1}]","bb5ced9e":"df1 = pd.DataFrame(data)\ndf1","60ba5ca5":"# df1 = df1.reindex(columns = df.columns, fill_value=0)","2b3019f7":"df2 = pd.DataFrame(data)\ndf2","9f445bf0":"df3=pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","54dff27b":"train_objs_num = len(df3)\ndataset = pd.concat(objs=[df3, df2], axis=0)\na = pd.get_dummies(dataset['sex'], prefix = \"sex\")\nb = pd.get_dummies(dataset['cp'], prefix = \"cp\")\nc = pd.get_dummies(dataset['fbs'], prefix = \"fbs\")\nd = pd.get_dummies(dataset['restecg'],prefix='restecg')\ne = pd.get_dummies(dataset['exang'],prefix='exang')\nf = pd.get_dummies(dataset['slope'],prefix='slope')\ng = pd.get_dummies(dataset['thal'],prefix='thal')\nh = pd.get_dummies(dataset['ca'],prefix='ca')\ndf2 = [dataset, a, b, c,d,e,f,g,h]\ndataset = pd.concat(df2, axis = 1)\ndataset = dataset.drop(columns = ['cp', 'thal', 'slope','sex','fbs','restecg','ca','exang'])\n\n# dataset = pd.get_dummies(dataset)\ntrain = dataset[:train_objs_num].copy()\ntest = dataset[train_objs_num:].copy()","87430371":"test=test.drop('target',axis=1)\ntest.head()","e941d8ff":"test.shape","c5910619":"log_reg.predict(test)[0]","c0048528":"df.head(1)","78ff0d11":"grid_log_reg.best_params_","59cbe628":"from sklearn.pipeline import Pipeline\n","a9968b3c":"model_pipeline = Pipeline(steps=[('log_reg',LogisticRegression(C=1,penalty='l2'))])","203fb209":"model_pipeline.fit(x_train,y_train)","da3745b7":"from pickle import dump\ndump(model_pipeline, open('heart_model.pkl', 'wb'))","d2e4fd75":"from pickle import load\nmodel = load(open('.\/heart_model.pkl', 'rb'))\npredictions = model.predict(x_test)\n","5056fe6b":"print('Logistic Regression:')\nprint(classification_report(y_test, predictions))","bc992410":"As logistic regression is working best among the given model. So, we are using logistic regression as our model"}}