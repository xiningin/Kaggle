{"cell_type":{"7bd13aba":"code","621fcaa6":"code","8f1e7130":"code","40a17973":"code","34a27527":"code","858065c7":"code","741b765e":"code","bef74d8d":"code","fc54dbe0":"code","e192a477":"code","e59c2c4b":"code","3d44856a":"code","6acc697f":"code","be0c022e":"code","61adcc61":"code","b0b8ce2b":"code","4b59ec99":"code","22a9399b":"code","0ef40fb8":"code","7467cb5e":"code","bc7b8724":"markdown","39d529fe":"markdown","5c712f33":"markdown","b953ffc6":"markdown","ce85d8d2":"markdown","52bfb516":"markdown","a57100ff":"markdown","8e2700ff":"markdown","67836fc0":"markdown","8cb616ec":"markdown","b2d82a66":"markdown","dcb92478":"markdown","5ada32a8":"markdown","1232ad1f":"markdown","daa69af8":"markdown","5bb9429d":"markdown","7ddd5f71":"markdown","eb874b94":"markdown","dd995cd8":"markdown","4bd1939f":"markdown"},"source":{"7bd13aba":"from pandas import read_csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression","621fcaa6":"url = \"..\/input\/online-shoppers-intention\/online_shoppers_intention.csv\"\ndataset = read_csv(url)\ndataset[\"Administrative_Duration\"] = dataset[\"Administrative_Duration\"].replace([-1],0)\ndataset[\"Informational_Duration\"] = dataset[\"Informational_Duration\"].replace([-1],0)\ndataset[\"ProductRelated_Duration\"] = dataset[\"ProductRelated_Duration\"].replace([-1],0)\ndataset[\"Administrative\"] = dataset[\"Administrative\"].replace([-1],0)\ndataset[\"Informational\"] = dataset[\"Informational\"].replace([-1],0)\ndataset[\"ProductRelated\"] = dataset[\"ProductRelated\"].replace([-1],0)\ndataset = dataset.dropna()        ","8f1e7130":"#Shape : Dimensions de l'ensemble de donn\u00e9es\nprint(dataset.shape)\n# Head\nprint(dataset.head(dataset.size))\n# descriptions\nprint('description')\nprint(dataset.describe())\n# class distribution\nprint('Values distributions')\nprint(dataset.groupby('Revenue').size()\/12316)\nprint(dataset.groupby('Browser').size()\/12316)\nprint(dataset.groupby('Region').size()\/12316)\nprint(dataset.groupby('OperatingSystems').size()\/12316)\nprint(dataset.groupby('Weekend').size()\/12316)\nprint(dataset.groupby('Month').size()\/12316)\nprint(\"type des colones\")\nprint(dataset.dtypes)\nprint(processed_dataset.isnull())","40a17973":"initial_dataset = dataset\ndataset[\"Revenue\"] = dataset[\"Revenue\"].astype(int)\ndataset[\"Weekend\"] = dataset[\"Weekend\"].astype(int)\ny = pd.get_dummies(dataset.Month, prefix='')\n#print(y)\ndataset = dataset.join(y)\ny = pd.get_dummies(dataset.VisitorType, prefix='')\ndataset = dataset.join(y)\ndel dataset[\"Month\"]\ndel dataset[\"VisitorType\"]\nprocessed_dataset = dataset\nprint(dataset)","34a27527":"Revenue = [0,1]\nobjects = Revenue\nRevenue_values = (initial_dataset.groupby('Revenue').size()\/12316).tolist()\ny_pos = np.arange(len(objects))\nperformance = Revenue_values\n\nplt.bar(y_pos, performance, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Distribution % ')\nplt.title(\"Distribution of sessions that ended up with a purchase (1),(0) sessions that didn't\")","858065c7":"Browser = [1,2,3,4,5,6,7,8,9,10,11,12,13]\nobjects = Browser\nBrowser_values = (initial_dataset.groupby('Browser').size()\/12316).tolist()\ny_pos = np.arange(len(objects))\nperformance = Browser_values\n\nplt.bar(y_pos, performance, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Browsers Distribution % ')\nplt.title(\"Distribution of sessions that were conducted in multiple browsers, browsers names are encoded using integers\")","741b765e":"Region = [1,2,3,4,5,6,7,8,9]\nobjects = Region\nRegion_values = (initial_dataset.groupby('Region').size()\/12316).tolist()\ny_pos = np.arange(len(objects))\nperformance = Region_values\n\nplt.bar(y_pos, performance, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('regions % distribution')\nplt.title(\"Distribution of sessions that were conducted from multiple regions, regions names are encoded using integers\")","bef74d8d":"OperatingSystem = [1,2,3,4,5,6,7,8]\nobjects = OperatingSystem\nOperatingSystem_values = (initial_dataset.groupby('OperatingSystems').size()\/12316).tolist()\ny_pos = np.arange(len(objects))\nperformance = OperatingSystem_values\n\nplt.bar(y_pos, performance, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel(\"Distribution % \")\nplt.title(\"Distribution of sessions that were conducted in multiple Operating systems, Operating systems names are encoded using integers\")","fc54dbe0":"weekend = [0,1]\nobjects = weekend\nweekend_values = (initial_dataset.groupby('Weekend').size()\/12316).tolist()\ny_pos = np.arange(len(objects))\nperformance = weekend_values\n\nplt.bar(y_pos, performance, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel(\"Distribution % \")\nplt.title(\"Distribution of sessions that were conducted during the weekend (1), (0) sessions that weren't conducted during the weekend\")","e192a477":"VisitorType = [\"New visitor\",\"Other\",\"Returning visitor\"]\nobjects = VisitorType\nVisitorType_values = (initial_dataset.groupby('VisitorType').size()\/12316).tolist()\ny_pos = np.arange(len(objects))\nperformance = VisitorType_values\n\nplt.bar(y_pos, performance, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel(\"Distribution % \")\nplt.title(\"Distribution of sessions that were conducted by new visitors,sessions that were conducted by returning visitors (visitors who had already visited the website at least once in the paste),other\")","e59c2c4b":"Month = [\"Aug\",\"Dec\",\"Feb\",\"Jul\",\"June\",\"Mar\",\"May\",\"Nov\",\"Oct\",\"Sep\"]\nobjects = Month\nMonth_values = (initial_dataset.groupby('Month').size()\/12316).tolist()\ny_pos = np.arange(len(objects))\nperformance = Month_values\n\nplt.bar(y_pos, performance, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel(\"Distribution % \")\nplt.title(\"Distribution of sessions that were conducted during specific months Aug,Dec,Feb,Jul,June,Mar,May,Nov,Oct,Sep\")","3d44856a":"print(processed_dataset.corr())","6acc697f":"processed_dataset[\"Avgadministrative\"] = np.where(processed_dataset[\"Administrative\"] == 0,processed_dataset[\"Administrative\"],processed_dataset[\"Administrative_Duration\"]\/processed_dataset[\"Administrative\"])\nprocessed_dataset[\"AvgproductRelated\"] = np.where(processed_dataset[\"ProductRelated\"] == 0,processed_dataset[\"ProductRelated\"],processed_dataset[\"ProductRelated_Duration\"]\/processed_dataset[\"ProductRelated\"])\nprocessed_dataset[\"AvgInformational\"] = np.where(processed_dataset[\"Informational\"] == 0,processed_dataset[\"Informational\"],processed_dataset[\"Informational_Duration\"]\/processed_dataset[\"Informational\"])\ndel processed_dataset[\"Administrative\"]\ndel processed_dataset[\"Administrative_Duration\"]\ndel processed_dataset[\"ProductRelated\"]\ndel processed_dataset[\"ProductRelated_Duration\"]\ndel processed_dataset[\"Informational\"]\ndel processed_dataset[\"Informational_Duration\"]\nprint(processed_dataset)   ","be0c022e":"from sklearn.decomposition import PCA\npca = PCA(n_components=1)\npca.fit(np.array(processed_dataset[[\"ExitRates\",\"BounceRates\"]]))\nx_pca = pca.transform(np.array(processed_dataset[[\"BounceRates\",\"ExitRates\"]]))\nprint(pca.explained_variance_ratio_)\nNewexitrates = pd.DataFrame(x_pca,columns=[\"New_exit_rate\"])\nprocessed_dataset[\"New_exit_rate\"] = Newexitrates\ndel processed_dataset[\"ExitRates\"]\ndel processed_dataset[\"BounceRates\"]\nprint(processed_dataset)","61adcc61":"processed_dataset = processed_dataset.dropna()\nfeatures = np.array(processed_dataset[[\"PageValues\",\"SpecialDay\",\"OperatingSystems\",\"Browser\",\"Region\",\"TrafficType\",\"Weekend\",\"Revenue\",\"_Aug\",\"_Nov\",\"_Oct\",\"_Sep\",\"_Feb\",\"_June\",\"_Jul\",\"_May\",\"_Dec\",\"_Mar\",\"_New_Visitor\",\"_Other\",\"_Returning_Visitor\",\"Avgadministrative\",\"AvgproductRelated\",\"AvgInformational\",\"New_exit_rate\"]])\nlabel = np.array(processed_dataset[\"Revenue\"])","b0b8ce2b":"x_train, x_test, y_train, y_test = train_test_split(features,label,train_size=0.8,test_size=0.2)\ntrain_0 = 0\ntrain_1 = 0\ntest_0 = 0\ntest_1 = 0\nfor i in range(0,len(y_train)):\n    if y_train[i] == 0:\n        train_0 = train_0 + 1 \n    if y_train[i] == 1:\n        train_1 = train_1 + 1 \n\nfor i in range(0,len(y_test)):\n    if y_test[i] == 0:\n        test_0 = test_0 + 1 \n    if y_test[i] == 1:\n        test_1 = test_1 + 1 \n        \nprint(\"train set proportion:\\n\")\nprint(\"train set revenue 0 :\\n\",train_0\/len(y_train))\nprint(\"train set revenue 1 :\\n\",train_1\/len(y_train))\nprint(\"test set proportion:\\n\")\nprint(\"test set revenue 0 :\\n\",test_0\/len(y_test))\nprint(\"test set revenue 1 :\\n\",test_1\/len(y_test))\n\n","4b59ec99":"print(x_train.dtype)","22a9399b":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0,C=10**32).fit(x_train,y_train)\nprint(\"accuracy :\",clf.score(x_test,y_test))\ny_predicted = clf.predict(x_test)\nscore = 0\ny_test = np.array(y_test)\nfor k in range(0,len(y_predicted)):\n    if y_predicted[k] == y_test[k]:\n        score = score + 1\nprint(\"accuracy verified manually\",score\/len(y_predicted))\nprint(\"estimated probabilities\",clf.predict_proba(x_test))","0ef40fb8":"from sklearn.linear_model import SGDClassifier\n\nclf = SGDClassifier(max_iter=1000, tol=1e-3,learning_rate=\"constant\",eta0=1\/len(x_train))\nprint(clf.fit(x_train,y_train))\nprint(\"accuracy :\",clf.score(x_test,y_test))","7467cb5e":"clf = SGDClassifier(max_iter=1000,tol=1e-3,learning_rate=\"constant\",eta0=1\/len(x_train),loss=\"perceptron\")\nprint(clf.fit(x_train,y_train))\nprint(\"accuracy :\",clf.score(x_test,y_test))","bc7b8724":"The next model is linear SVM","39d529fe":"We can notice that a large portion of sessions were conducted in the browser encoded by 2, 64%.","5c712f33":"Next, we noticed some colinearity patterns between the variables : Administrative\/Adminitrative_Duration, ProductRelated\/ProductRelated_Duration, Informational\/Informational_Duration, (which was expected from the description of these columns, the duration spent on each type of pages would be proportional to the number of these types of pages visited) and finally  ExitRates\/BounceRates.","b953ffc6":"Almost 53% of the sessions are dones using the operating system \"2\",this could be a strong argument to secure deals,partnership deals that requires the target clients to use the operating system \"2\"","ce85d8d2":"We dealt with each one of them by scaling the total duration by the number of visited pages which will give us as result the average duration spent on each type of pages (or the duration spent on one page of each type). We though that the initial duration column isn't scaled since it depends on the type of pages visited. By introducing the new columns : \"Avgadminitrative\",\"AvgproductRelated\",\"AvgInformational\", we keep only a duration independent of the number of pages that really tells \"how much time a user spends time on this type of page\".","52bfb516":"Split the data 80% training data, 20% test data, we also check the label 0 and 1 distribution, in both the training and testing. We can then conclude that they are equally distributed and the training set represents well the population. We are now ready to move on to modeling.","a57100ff":"**CONCLUSION**:\nWe can deduce that we got an acceptable accuracy eventhough it can be improved, these models are trained using stoachastic gradient (gradient estimation) I got a better accuracy using these model with the dataset on https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Shoppers+Purchasing+Intention+Dataset#. I hope this work can help and would love to hear some feedback\/idea on how it could be improved. Thank you for visiting my notebook.","8e2700ff":"Most of the sessions are users revisiting the site, the growth rate of the business is relatively low.","67836fc0":"Let's now visualize the class variable distribution. I think it was expected to see that only few sessions (15%) commited to a purchase while the other the remaining sessions almost 84% didn't.","8cb616ec":"This one is perceptron ","b2d82a66":"Let's first have some intel about the columns data types, distributions.. We will have to reencode the variables : \"Visitortypes\",\"Month\",\"Revenue\",\"Weekend\"","dcb92478":"As far as Weekend and Revenue columns,we convert their boolean values into 0 (for False) and 1 (for True). We use One-hot encoding technique to encode the values of Month and VisitorType columns (new columns are added to the dataframe which will be labeled by the values of Visitortype and Month, the values of the new columns will be 1 (if the sessions was done under the said circumstance) or 0 (otherwise)","5ada32a8":"We gather all the necessary data in arrays","1232ad1f":"a large portion of sessions consists of sessions that we conducted during the may and november months, this could lead us to believe that during this period of the year users are mostly interested and the best opportunity to get some product buys could be during these months.  ","daa69af8":"We start out first using Logistic regression, by setting the parameter C to a very high value to eliminate regularization since we don't need at first. I actually get this error, it worked well with the correct dataset that I found on https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Shoppers+Purchasing+Intention+Dataset#. It would be good if I can get some help on how I can solve this.","5bb9429d":"There are some wrong informations in the dataset (negative values (-1) in the columns that are related some specific pages),to my knowledge the correct values are 0 instead of -1. The correct dataset is here : https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Shoppers+Purchasing+Intention+Dataset#. I will publish the csv file in the forum so everyone can have download directly without having to correct negative values.","7ddd5f71":"We can notice that 38% of the sessions were conducted from the region encoded by 1, this could lead the buisness to try marketing techniques to target for example more people from this region (while supposing the people from this specific region are the most interested ones by the product of the business)","eb874b94":"**INTRODUCTION**\nIn this notebook we predict the user potential purchase intention, using differents models Logistic regression,Perceptron and linear SVM.\n\nFirst we import all the necessary modules and import the data","dd995cd8":"Next we deal with the columns BounceRates and ExitRates by applying the PCA technique that allow us to eliminate the dependency and generate a new column to replace BounceRates and ExitRates. The explained variance (95%) is signficant enough to consider that it represents well the variables BounceRates and ExitRates, so we took this principal component, named it \"New_exit_rate\" and moved forward with it","4bd1939f":"Most of the sessions were conducted during the week, while only 24% are conducted during the weekend. This could lead us to avoid maintaining the web site during the week as it's more likely that customers will be browsing during the week and maybe purchases could be made. Therefore reporting the web site maintenance during the weekend would be good."}}