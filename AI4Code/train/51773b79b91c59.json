{"cell_type":{"4c7df964":"code","fd4026f9":"code","c0df7744":"code","636a83dd":"code","b750d2cd":"code","18d673f5":"code","d6f1c228":"code","094479c7":"code","599cb203":"code","b2776f4d":"code","600a63a7":"code","823fe191":"code","67cbcafa":"code","8f3cbba3":"code","1ff4d832":"code","26f179fa":"code","69a4d995":"code","853cab1a":"code","34bcf3d6":"code","ba7a1394":"code","39ccfbdc":"code","56f78484":"code","4dc92c80":"code","9630a53f":"code","e7e21b47":"code","c4de6a47":"code","bb31decd":"code","bf7a518c":"code","57be44ae":"code","4a2c0ad8":"markdown","70529784":"markdown","7c3c4434":"markdown","7fb4ba19":"markdown","fbeaac92":"markdown","ce0fc424":"markdown","312a923a":"markdown","9ac504d1":"markdown","c450b094":"markdown","7319177d":"markdown","2ccf2d9b":"markdown","1e939bb0":"markdown","ea1b1099":"markdown","a56e6722":"markdown","2c5f8b31":"markdown","962d9037":"markdown","ef6db52f":"markdown","7d86bb28":"markdown","821c57a7":"markdown","b85d1d20":"markdown","93a17bba":"markdown","3939f6d9":"markdown","96e2c729":"markdown","542942c3":"markdown","0f6d0b39":"markdown","9ec8bad5":"markdown","66d7991b":"markdown","f332ac5a":"markdown","e9175843":"markdown","27cc7b3a":"markdown","f6557ec9":"markdown","980ef8ae":"markdown","ebe5bce7":"markdown","0d896397":"markdown","f9c369a3":"markdown","a4af7eac":"markdown"},"source":{"4c7df964":"import itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nimport os","fd4026f9":"item_categories = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\ntrain = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\ntrain = train.merge(items, on='item_id', how='left')\ntrain = train.merge(item_categories, on='item_category_id', how='left')\ntest = test.merge(items, on=\"item_id\", how=\"left\")\ntest = test.merge(item_categories, on=\"item_category_id\", how=\"left\")\n# Convert date field to datetime dtype\ntrain[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%d.%m.%Y\")","c0df7744":"train.head()","636a83dd":"print(f\"There are {(train['item_cnt_day']==0).sum()} training items with zero sales\")","b750d2cd":"cols_with_na = [col for col in train if train[col].isna().sum()>0]\nprint(f\"The following columns have missing values: {cols_with_na}\")\nprint(f\"There are {train.duplicated().sum()} duplicated rows out of a total of {train.shape[0]} rows.\")","18d673f5":"train = train.drop_duplicates()","d6f1c228":"print(f'Mean and 99.9th quantile of \"item_cnt_day\" are {train.item_cnt_day.mean():.2f} and {train.item_cnt_day.quantile(0.999):.2f}')\nprint(f'Mean and 99.9th quantile of \"item_price\" are {train.item_price.mean():.2f} and {train.item_price.quantile(0.999):.2f}')\nfig, axes = plt.subplots(1,2, figsize = (15,2))\n_ = sns.boxplot(x=train.item_cnt_day, ax=axes[0])\n_ = sns.boxplot(x=train.item_price, ax=axes[1])","094479c7":"train.sort_values('item_cnt_day', ascending=False).head(5)","599cb203":"train[train.item_price<0]\ntrain = train[(train.item_price < train.item_price.quantile(0.99999)) & (train.item_price > 0)]\ntrain = train[train.item_cnt_day < train.item_cnt_day.quantile(0.99999)]","b2776f4d":"test.head(5)","600a63a7":"print(f\"There are {len(test['shop_id'].unique())} unique shop_id's and {len(test['item_id'].unique())} unique item_id's in the test set.\")\nprint(f\"There are {len(test)} test items in total.\")","823fe191":"unique_items_date_block = [train.loc[train.date_block_num==x,:].item_id.unique().size for x in range(34)]\nunique_items_date_block.append(test.item_id.unique().size)\nax = sns.lineplot(x = range(len(unique_items_date_block)), y = unique_items_date_block, marker='.', markersize=12)\n_ = ax.set(xlabel = \"Date block\", ylabel = \"Unique items sold\", title = \"Unique items per month (test set=date block 34)\")","67cbcafa":"intersection = [len(set(train.loc[train.date_block_num==x,:].item_id).intersection(set(test.item_id))) for x in range(33)]\nintersection.append(len(set(test.item_id)))\nax = sns.lineplot(x = range(len(intersection)), y = intersection)\n_ = ax.set(xlabel = \"Date block\", ylabel = \"Intersection with date block 34\")\n_ = ax.set_title(\"Sold items also found in test set\")","8f3cbba3":"intersection = [len(set(train.loc[train.date_block_num==i+1,:].item_id) - \n    set(train.loc[train.date_block_num==i,:].item_id)) for i in range(33)]\nintersection.append(len(set(train.loc[train.date_block_num==33].item_id) - set(test.item_id)))\nax = sns.lineplot(x = range(1, len(intersection)+1), y = intersection)\n_ = ax.set(xlabel = \"Date block\", ylabel = \"New items\")\n_ = ax.set_title(\"Items not seen in the preceding month\")","1ff4d832":"def create_testlike_train(sales_train, test=None):\n    # Create a date_block_num \/ item_id \/ shop_id index using all combinations of item_id and shop_id occurring within each date_block\n    # Optionally concatenate the test items to the end\n    indexlist = []\n    for i in sales_train.date_block_num.unique():\n        x = itertools.product(\n            [i],\n            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n        )\n        indexlist.append(np.array(list(x)))\n    df = pd.DataFrame(\n        data=np.concatenate(indexlist, axis=0),\n        columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    # Add revenue column to sales_train\n    sales_train[\"item_revenue_day\"] = (\n        sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n    )\n\n    # Aggregate item_id \/ shop_id item_cnts and revenue at the month level\n    sales_train_grouped = sales_train.groupby(\n        [\"date_block_num\", \"shop_id\", \"item_id\"]\n    ).agg(\n        item_cnt_month=pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n        item_revenue_month=pd.NamedAgg(\n            column=\"item_revenue_day\", aggfunc=\"sum\"\n        ),\n    )\n\n    # Merge the grouped data with the index\n    df = df.merge(\n        sales_train_grouped,\n        how=\"left\",\n        on=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    if test is not None:\n        test[\"date_block_num\"] = 34\n        test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n        test[\"shop_id\"] = test.shop_id.astype(np.int8)\n        test[\"item_id\"] = test.item_id.astype(np.int16)\n        test = test.drop(columns=\"ID\")\n\n        df = pd.concat([df, test])\n\n    # Fill empty item_cnt entries with 0\n    df.item_cnt_month = df.item_cnt_month.fillna(0)\n    df.item_revenue_month = df.item_revenue_month.fillna(0)\n\n    return df","26f179fa":"matrix = create_testlike_train(train)\n# Adding item category column to allow sorting by category later\nmatrix = matrix.merge(items[['item_id', 'item_category_id']], on='item_id', how='left')\nmatrix.head(5)","69a4d995":"sales_by_month = matrix.groupby('date_block_num').item_cnt_month.sum()\nfig, axes = plt.subplots(1,2, figsize = (15,2))\nax = sns.lineplot(data=sales_by_month, marker='.', markersize=12, ax=axes[0])\n_ = ax.set(xlabel = \"Date block\", ylabel = \"item sales\", title = \"Total sales per month\")\nsales_by_month = matrix.groupby('date_block_num').item_cnt_month.mean()\nax = sns.lineplot(data=sales_by_month, marker='.', markersize=12, ax=axes[1])\n_ = ax.set(xlabel = \"Date block\", ylabel = \"item sales\", title = \"Mean sales per item per month\")","853cab1a":"from statsmodels.tsa.seasonal import STL\nresult = STL(sales_by_month, period=12).fit()\nfig = result.plot()\nfig.set_size_inches((7,5))\nfig.tight_layout()","34bcf3d6":"def item_shop_age_months(matrix):\n    matrix[\"item_age\"] = matrix.groupby(\"item_id\")[\"date_block_num\"].transform(\n    lambda x: x - x.min()\n    )\n    matrix[\"new_item\"] = matrix[\"item_age\"] == 0\n    matrix[\"new_item\"] = matrix[\"new_item\"].astype(\"int8\")\n    matrix[\"shop_age\"] = (\n        matrix.groupby(\"shop_id\")[\"date_block_num\"]\n        .transform(lambda x: x - x.min())\n        .astype(\"int8\")\n    )\n    matrix[\"new_shop\"] = matrix.shop_age == 0\n    matrix[\"new_shop\"] = matrix[\"new_shop\"].astype(\"int8\")\n    return matrix\n\nmatrix = item_shop_age_months(matrix)\n\nfig, axes = plt.subplots(1,2, figsize = (15,4))\nax = matrix.groupby('item_age').item_cnt_month.mean().plot(\n    ylabel='mean items sold', title='Sales by item age', ax=axes[0])\nax = matrix.groupby('shop_age').item_cnt_month.mean().plot(\n    ylabel='mean items sold', title='Sales by shop age', ax=axes[1])","ba7a1394":"shops = pd.read_csv(\"..\/input\/predict-future-sales-extra\/shops_extra.csv\")\nshops['shop_name_trans'] = shops['shop_name'] + \" \\n\" + shops['shop_name_en']\nshopnamedict = { id : f\"{id:02}: {name}\" for name, id in zip(list(shops['shop_name_trans']), list(shops['shop_id']))}\n\nts = matrix.groupby(['date_block_num','shop_id'])['item_cnt_month'].mean()\nts = ts.reset_index()\nts['shop_name_trans'] = ts['shop_id'].map(shopnamedict)\ntsp = ts.pivot(index='date_block_num',columns='shop_name_trans',values='item_cnt_month')\n\nfig, axes = plt.subplots(20,3, figsize=(15,30))\nfor i, shop_name in enumerate(tsp.columns):\n    idx = divmod(i,axes.shape[1])\n    axes[idx[0]][idx[1]].plot(tsp.index,tsp.loc[:,shop_name], marker='.')\n    axes[idx[0]][idx[1]].set_xlim(0,34)\n    axes[idx[0]][idx[1]].set_title(shop_name, fontsize=10)\nfig.tight_layout()","39ccfbdc":"# Correct shop labels\ntrain.loc[train.shop_id == 0, \"shop_id\"] = 57\ntrain.loc[train.shop_id == 1, \"shop_id\"] = 58\ntrain.loc[train.shop_id == 11, \"shop_id\"] = 10\n# Drop special categories\ntrain = train.loc[(train.shop_id != 9) & (train.shop_id != 20), :]\n# Remove data from short-lived shops\nshops_to_remove = [8, 23, 32, 33, 40]\ntrain = train.loc[~train[\"shop_id\"].isin(shops_to_remove), :]","56f78484":"print(f\"There are {len(set(test.shop_id) - set(train.shop_id))} shops in the test set that are not in the train set\")\nprint(f\"There are {len(set(train.shop_id) - set(test.shop_id))} shops in the train set that are not in the test set\")","4dc92c80":"from sklearn.decomposition import PCA\n\ndef plot_pca(data, n_components):\n    pca = PCA(n_components=n_components)\n    components = pca.fit_transform(data)\n    components = pd.DataFrame(components)\n    # Plot the explained variances\n    features = range(pca.n_components_)\n    fig, axes = plt.subplots(1,2, figsize = (15,4))\n    _ = axes[0].bar(features, pca.explained_variance_ratio_, color='black')\n    _ = axes[0].set_xlabel('PCA features')\n    _ = axes[0].set_ylabel('variance %')\n    _ = axes[0].set_xticks(features)\n    _ = axes[0].set_title(\"Explained variance by principle component\")\n    # Plot the shops according to their projection\n    x = components[0]\n    y = components[1]\n    sns.scatterplot(x=x, y=y, ax=axes[1])\n    axes[1].set_xlabel('PC 1')\n    axes[1].set_ylabel('PC 2')\n    axes[1].set_title('Shop scores on the first two components')\n    for i, txt in enumerate(data.index.to_list()):\n        axes[1].annotate(str(txt), (x[i], y[i]))\n\nstart_month=21\nend_month=33\npt = matrix[matrix.shop_id.isin(test.shop_id.unique())].query(f\"date_block_num>{start_month} & date_block_num<={end_month}\")\npt = pt.pivot_table(values='item_cnt_month', columns='item_category_id', index='shop_id', fill_value=0, aggfunc='mean')\nplot_pca(pt, 10)     ","9630a53f":"pt = matrix[matrix.shop_id.isin(test.shop_id.unique())].query(f\"date_block_num>{start_month} & date_block_num<={end_month} & shop_id!=55 & shop_id!=12\")\npt = pt.pivot_table(values='item_cnt_month', columns='item_category_id', index='shop_id', fill_value=0, aggfunc='mean')\nplot_pca(pt, 10)     ","e7e21b47":"item_categories = pd.read_csv(\"..\/input\/predict-future-sales-extra\/item_categories_enhanced.csv\")\nunique_id_cat = (\n    train.groupby([\"date_block_num\", \"item_category_id\"])\n    .item_id.nunique()\n    .rename(\"unique_items_cat\")\n    .reset_index()\n)\nunique_id_cat = unique_id_cat.merge(item_categories[['item_category_id', 'category_name']],\n                                    on='item_category_id', how='left')\n\nunique_items_cat_ts = unique_id_cat.pivot(\n    index=\"date_block_num\", columns=\"category_name\", values=\"unique_items_cat\"\n)\n\n# Create a time series of mean sales per item by category\nm = matrix.merge(items.drop(columns='item_category_id'), on='item_id', how='left')\nm = m.merge(item_categories, on='item_category_id', how='left')\nts = m.groupby([\"date_block_num\", \"category_name\"])[\"item_cnt_month\"].mean().reset_index()\n\nsales_cat = ts.pivot(\n    index=\"date_block_num\", columns=\"category_name\", values=\"item_cnt_month\"\n)\n\n# Plot both series for all categories\nfig, axes = plt.subplots(28, 3, figsize=(15, 45))\nfor i, cat_name in enumerate(sales_cat.columns):\n    idx = divmod(i, axes.shape[1])\n    axes[idx[0]][idx[1]].plot(sales_cat.index, sales_cat.loc[:, cat_name], marker=\".\")\n    axes[idx[0]][idx[1]].set_xlim(0, 34)\n    axes[idx[0]][idx[1]].set_title(cat_name, fontsize=10)\n    ax2 = axes[idx[0]][idx[1]].twinx()\n    ax2.plot(\n        unique_items_cat_ts.index,\n        unique_items_cat_ts.loc[:, cat_name],\n        marker=\".\",\n        color=\"orange\",\n    )\n    ax2.set_xlim(0, 34)\n    ax2.set_title(cat_name, fontsize=10)\nfig.tight_layout()","c4de6a47":"t = test.merge(item_categories[['item_category_id', 'supercategory']], on='item_category_id', how='left')\ncat_proportions_train = m.groupby('supercategory').item_category_id.count()\ncat_proportions_train = cat_proportions_train.rename('train')\ncat_proportions_train = pd.DataFrame(cat_proportions_train)\n\ncat_proportions_test = t.groupby('supercategory').item_category_id.count()\ncat_proportions_test = cat_proportions_test.rename('test')\ncat_proportions = cat_proportions_train.merge(cat_proportions_test, left_index=True, right_index=True)\ncat_proportions = cat_proportions.sort_values('train', ascending=True)\ncat_proportions = cat_proportions.transform(lambda x: x \/ x.sum())\nax = cat_proportions.plot.barh(figsize=(10,7))\n_ = plt.title('General categories in train and test sets')\n_ = plt.xlabel('proportion')","bb31decd":"itemsales = matrix.groupby([\"date_block_num\", \"item_id\"]).agg(\n    {\"item_cnt_month\": \"sum\", \"item_revenue_month\": \"sum\"}\n)\nitemsales[\"price\"] = itemsales.item_revenue_month \/ itemsales.item_cnt_month\nitemsales = itemsales.drop(columns=[\"item_cnt_month\", \"item_revenue_month\"])\nitemsales[\"item_cnt_month_mean\"] = matrix.groupby([\"date_block_num\", \"item_id\"]).item_cnt_month.mean()\nitemsales = itemsales.loc[slice(0, 33), slice(None), :]\n\n# Calculating prices in this way produces some negative and infinite values, these can be removed\nitemsales = itemsales.drop(itemsales['price'][(itemsales['price']==np.inf) | (itemsales['price']<=0)].index)\nitemsales = itemsales.drop(itemsales['item_cnt_month_mean'][(itemsales['item_cnt_month_mean']<=0)].index)\n\n# Plot mean sales\nfig, ax = plt.subplots(1,1, figsize=(6,6))\nax = sns.scatterplot(data=itemsales, x='price', y='item_cnt_month_mean', marker='.')\n_ = ax.set_title('Sales by price')","bf7a518c":"itemsales = itemsales.reset_index()\nitemsales[\"item_age\"] = itemsales.groupby(\"item_id\")[\"date_block_num\"].transform(\nlambda x: x - x.min()\n)\nitemsales = itemsales.merge(items[['item_id','item_category_id']], how='left', on='item_id')\nitemsales = itemsales.merge(item_categories, how='left', on='item_category_id')\n\nitemsales['price_relative_to_category'] = itemsales.groupby(['date_block_num','item_category_id'])['price'].transform(lambda x: x \/ x.mean())\nitemsales['price_relative_to_self'] = itemsales.groupby(['item_id'])['price'].transform(lambda x: x \/ x.mean())\n\ndef calc_bin_means(feature_name):\n    feature = itemsales[feature_name]\n    mybins=np.logspace(np.log10(feature.quantile(0.01)), np.log10(feature.quantile(0.99)),20)\n    bin_means, bin_edges, binnumber = stats.binned_statistic(feature, itemsales['item_cnt_month_mean'],\n        statistic='mean', bins=mybins)\n    bin_widths = np.diff(bin_edges)\n    return (bin_means, bin_edges, binnumber)\n\nfeats = ['price', 'price_relative_to_category', 'price_relative_to_self']\nfig, axes = plt.subplots(2,2, figsize=(10,10))\naxes = axes.flatten()\nfor i, feature_name in enumerate(feats):\n    bin_means, bin_edges, binnumber = calc_bin_means(feature_name)\n    sns.lineplot(x=bin_edges[:-1], y=bin_means, ax=axes[i])\n    axes[i].set_xlabel(feature_name)\n    axes[i].set_ylabel('mean item_cnt_month')\n    axes[i].set_xscale('log')\n_ = sns.lineplot(data=itemsales.groupby(\"item_age\").price.mean(), ax=axes[3])","57be44ae":"def calc_bin_means(feature_name):\n    feature = itemsales[feature_name]\n    mybins=np.logspace(np.log10(feature.min()), np.log10(feature.max()),20)\n    bin_means, bin_edges, binnumber = stats.binned_statistic(feature, itemsales['item_cnt_month_mean'],\n        statistic='mean', bins=mybins)\n    bin_widths = np.diff(bin_edges)\n    return (bin_means, bin_edges, binnumber)\n\nfeats = ['price', 'price_relative_to_category', 'price_relative_to_self']\nfig, axes = plt.subplots(1,3, figsize=(16,5))\nfor i, feature_name in enumerate(feats):\n    bin_means, bin_edges, binnumber = calc_bin_means(feature_name)\n    sns.lineplot(x=bin_edges[:-1], y=bin_means, ax=axes[i])\n    axes[i].set_xlabel(feature_name)\n    axes[i].set_ylabel('mean item_cnt_month')\n    axes[i].set_xscale('log')","4a2c0ad8":"If we perform the same operations again with the digital shops removed we see that almost all the differences between the shops are on a single dimension, and there are a small number of shops with much higher values in this dimensions. These groups of shops could potentially be marked as some kind of cluster feature.","70529784":"# 1. Introduction\nThis notebook covers some basic EDA such as checking the training data for outliers and missing values, determining the structure of the test items and how they were generated, and plotting some sales trends over time and separated according to shops and item categories.  \n\nIt also describes a kind of data leakage in the test items that I haven't seen described before. Nothing game-changing, but kind of interesting.","7c3c4434":"# 4. Exploring the training data","7fb4ba19":"# 3. Determining the structure of the test dataset","fbeaac92":"Load the data, convert the date field to the datetime datatype to enable extra features, merge the item categories frame with the items and sales frames to enable sorting by item category.\n","ce0fc424":"## 4.4 Proportions of categories in train and test sets\n\nIt is fairly easy to manually group the 83 categories into more general supercategories like games, music etc. These have been saved in a csv table and can be used to clarify the makeup of items in the train and test sets.","312a923a":"* From this we can see that most of the items are cinema, music, games or toys. Efforts to improve prediction on these categories of item are likely to be the most profitable.\n\n* From this chart it is evident that the distribution of item types is different in the train and test sets. Compensating for this in some way may improve performance.\n","9ac504d1":"Let's have a quick look at the train data","c450b094":"The following function reformats the training data to the same format as the test table, i.e. the Cartesian product of all items and shops that record at least one sale in each monthly date block.","7319177d":"The train and test data as provided are mismatched, as the train data consists of per-day entries, while the test items are aggregated to the month level.  \n\nA little exploration finds that each of the shop_ids is paired with the exact same set of item_ids. This suggests that the entries are the Cartesian product (i.e. all possible combinations) of a set of shops and items, which can be validated by checking the value counts of the shop_id and item_id's.","2ccf2d9b":"Mean sales per item can also be decomposed into seasonal and continuous trends, which shows the clear seasonal trend (i.e. a peak around the winter holidays) and a mild overall downwards trend that levels out.","1e939bb0":"## 4.3 Sales by category and possible data leakage  \n\nMean item sales can also be separated out according to category. As with the shop names, category names were translated to English.  \n\nFor comparison, the number of unique items in each category is plotted on the same axis (in orange), to explore the possibility that this has a meaningful relationship to sales. As can been seen from the plots, there is a clear relationship, although this varies by item category.","ea1b1099":"## PCA decomposition and clustering of shops  \nPrinciple component analysis lets us simplify high dimensional data into low-dimensional features and so make it easier to understand. We can do this with the sales of items in shops to try to understand their similarities and differences.  \n\nThe next cell calculates the number of items sold in each category for each shop and performs PCA on this data. Plotted is the amount of variation between shops explained by each component and the shops projections onto the first two components.  \n\nThe explained variance plot shows that over 80% of the differences between the shops correspond to a general component representing the overall volume of item sales. The scatterplot indicates that shop 55 and possibly shop 12 have a different sales profile to the other shops - this makes sense because these shops are online.","a56e6722":"* **Lower priced items tend to sell less, which can be related to price reductions for older, less popular items**  \n\nThe trends are clearer if items are binned by price and mean values are calculated for bins, as below. Also plotted below are trends for item prices relative to other items in the same category, and relative to the mean price of the same item over all months.  \n\nAlso shown is a plot of mean price by item age, which shows that items tend to fall in price with time.","2c5f8b31":"A reasonable assumption would be that the shops and items in the test set are those items for which there were at least one sale in the test month. A quick check of the validity of this is to the compare the number of unique items in the test month with the unique items in each month of the train set and see if they are similar. We can plot this.","962d9037":"There appear to be a few outliers which might distort the model. We can have a quick look at the highest-selling items.","ef6db52f":"For clarity: mean item sales are in blue with the y axis on the left, and the orange line is the number of unique items in that category in that monthly date block.\n\nThere is a lot of information here : \n* From the names of the categories it looks like they could be grouped together into supercategories to create extra features, e.g. \"games\"\n* Some categories contain very few items or sales and could potentially be removed or combined with other categories.\n* Some categories become obsolete over time and could potentially be removed from the training data.","7d86bb28":"## 4.3 Sales as a function of item age\nPlotting items as a function of their age indicates that they tend to sell most when they are new and then decline to a plateau about 1 year later. Item age is calculated here as the number of months since the month of their first appearence in the data. The slightly lower sales in the first compared to the second month is attributable to items not always being available for the whole month during their first calendar month of availability.  \n\nThe corresponding plot of sales according to shop age shows no such trend","821c57a7":"Boxplots of item counts and prices","b85d1d20":"So the test set is average in this respect. The hypothesis that the set of item_id's in the test set is those that recorded a sale seems sound.","93a17bba":"# 2. Data validity checks, outliers and cleaning","3939f6d9":"## 4.1 Overall sales trends\nPlotting total sales counts per month shows clear downwards and seasonal trends. However the mean number of each item sold per month (which is what is to be predicted) shows a less pronounced downwards trend.\n","96e2c729":"Some shops closed during the period covered by the training data and are included in the test set. These could be removed to potentially improve test performance.","542942c3":"So there are 42 shop_ids and 5100 item_ids, 42 x 5100 = 214,200, the number of rows in the test table.","0f6d0b39":"Check for duplicated rows and missing values.","9ec8bad5":"No missing values, good! The duplicated rows are no big deal, but we can remove them anyway.","66d7991b":"## 4.5 Relationship between price and sales\n* **Prices and sales counts are highly skewed**  \n\nThe price of items is included in the training data and might be expected to be related to sales. We can plot the price and sales numbers of items in training matrix in a scatter plot, but the sales counts and prices are heavily skewed towards the lower end and are too numerous to see clearly.","f332ac5a":"Perform the imports, load the data and merge it with the item, category and shop tables","e9175843":"The top 5 items appear to include a generic item for delivery charges, a promotional t-shirt, tickets for a gaming exhibition, and Grand Theft Auto V, presumably on its release date. Deciding what to do with these items is tricky, as they may distort the model if not removed from the training data, but if similar high-selling items are in the test set then there will be a large penalty for failing to correctly predict them.","27cc7b3a":"### 4.5.1 Effect of outliers  \n* **Price trends are obscured by outliers**  \n\nFinally, the above plots were made excluding the highest and lowest 1% of items by price. When these are included, the plots look very diffeent and the central trend is not so obvious.  \n\nThis seems to reinforce the point that outliers could distort predictions if not removed from the data.","f6557ec9":"The number of unique test item_id's is close to those in preceding months. A trend of decreasing numbers of unique items in each month is clear. If this trend of decreasing unique items is related to sales trends in general, this implies a kind of data leak in the test table.  \n\nIt might be interesting to check how many of the items sold in previous months correspond to the items in the test data. We can check the intersection of the sets of these items.","980ef8ae":"The training data seems to be date-specific sales figures for specific items in specific shops, only containing entries for date-item-shop combinations for which sales occurred.","ebe5bce7":"## 4.2 Sales by shop\nWe can plot mean sales per item over time separately for each of the shops. The names of these are in Russian, for clarity I added translations into English from google translate.","0d896397":"There are several important things to note here.\n\n* Several shops have been recorded under 2 different IDs and should be combined (0-57, 1-58 and 10-11). Other shops are only open for a short time and may be better removed.\n* Some shops appear to be online only (e.g. 12) or occasional special categories rather than physical shops (e.g. 9 and 20).\n* Finally, it can be noted that almost all shop names start with the name of the city in which the shop is located - this could be used as a feature.","f9c369a3":"Remove the outliers and items with negative prices (there is only one, but it might have strange effects when engineering features).","a4af7eac":"This plot shows that the items sold in each month become more similar to those in the test set the closer the month is in time (as should be expected), but also that there are quite a lot of items in the test set which did not appear in the preceding month.  \n\nWe can check if this is some kind of anomaly by calculating how much the set of sold items changes each month."}}