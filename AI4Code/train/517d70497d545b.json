{"cell_type":{"bf482b51":"code","f3e4da6f":"code","7c69013c":"code","7098aec9":"code","1e04e555":"code","fbb97cef":"code","8c082b6e":"code","ae6f5f57":"code","ae92a1fe":"code","b90e000b":"code","a62c3977":"code","b79ab0cf":"code","3255e9db":"code","18fd3226":"markdown","fdca79aa":"markdown","29910282":"markdown","ffff9bb6":"markdown","fbfc3377":"markdown","6ca98903":"markdown","fb48fb97":"markdown","acd838f1":"markdown","65c97a14":"markdown","27808cff":"markdown","930d558b":"markdown"},"source":{"bf482b51":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n## load data\ntrain_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntarget = 'SalePrice'\ntask = 'regression'\nreward_metric = 'rmse'\n","f3e4da6f":"train_data['dollor_value'] = train_data['YrSold'].apply(lambda k: [1.0,1.03,1.07,1.06,1.08][int(k)-2006])\ntest_data['dollor_value'] = test_data['YrSold'].apply(lambda k: [1.0,1.03,1.07,1.06,1.08][int(k)-2006])\ntrain_data['inflation_rate'] = train_data['YrSold'].apply(lambda k: [0.0323,0.0285,0.0384,-0.0036,0.0164][int(k)-2006])\ntest_data['inflation_rate'] = test_data['YrSold'].apply(lambda k: [0.0323,0.0285,0.0384,-0.0036,0.0164][int(k)-2006])\n\n\ntrain_data[target] = np.log1p(train_data[target])","7c69013c":"## install hypergbm\n!pip3 install -U hypergbm\n!pip3 install -U scikit-learn==0.23.2","7098aec9":"from hypergbm import make_experiment\nfrom hypernets.core.trial import TrialHistory\nfrom hypernets.searchers import PlaybackSearcher\nfrom hypergbm.search_space import GeneralSearchSpaceGenerator\nfrom hypernets.searchers import EvolutionSearcher\nfrom hypernets.experiment.cfg import ExperimentCfg as cfg\ncfg.experiment_discriminator=None","1e04e555":"experiment = make_experiment(train_data.copy(), target=target, reward_metric=reward_metric,max_trials=1,callbacks=[], search_callbacks=[])\nestimator = experiment.run()\nestimator","fbb97cef":"##difine search_space,only use lightgbm\nsearch_space_ = GeneralSearchSpaceGenerator(n_estimators=2000,enable_xgb = False,enable_catboost=False) \n## define search_algorithm\nrs = EvolutionSearcher(search_space_,optimize_direction='min',population_size=50, sample_size=6, candidates_size=5) \n\n##run make_experiment\nexp = make_experiment(\n                      train_data.copy(),\n                      target=target,\n                      num_folds= 5, ## 5 folds for Cross-validate\n                      max_trials= 3, ## search for 10 trials\n                      searcher = rs,\n                      random_state=7,\n                      callbacks=[], search_callbacks=[] #only for removing the trainning info, idea from issue#51\n                      )\nestimator = exp.run()","8c082b6e":"search_history = exp.hyper_model_.history\nfor trial in search_history.trials:\n    print(trial.trial_no,trial.reward)","ae6f5f57":"# ## how to search and save history in hypergbm\n# history = 'history.txt'\n# search_space_ = GeneralSearchSpaceGenerator(n_estimators=2000,\n#                                             enable_xgb = False,\n#                                             enable_lightgbm=False,\n#                                             catboost_init_kwargs={'random_state':8},\n#                                             catboost_fit_kwargs = {'early_stopping_rounds':100}\n#                                            ) \n# rs = EvolutionSearcher(search_space_,optimize_direction='min',population_size=50, sample_size=6, candidates_size=5) \n\n# exp = make_experiment(\n#                       train_data.copy(),\n#                       target=target,\n#                       cv = True,\n#                       num_folds= 5, \n#                       max_trials= 200, ## search for 200 trials\n#                       early_stopping_time_limit= 7200,## search for 2h\n#                       searcher = rs,\n#                       random_state=7,\n#                       callbacks=[], search_callbacks=[] #only for removing the trainning info, idea from issue#51\n#                       )\n# estimator = exp.run()\n# exp.hyper_model_.history.save(history_file)","ae92a1fe":"##playback history\n# history_file = f\"\/kaggle\/input\/resource\/history_for_house_prices.txt\"\nhistory_file = f\"\/kaggle\/input\/resource\/history_for_house_prices_v2.txt\"\nsearch_space_ = GeneralSearchSpaceGenerator(n_estimators=2000,\n                                            enable_xgb = False,\n                                            enable_lightgbm=False,\n#                                             catboost_init_kwargs={'random_state':8}, ## cannot fix the random_state\n                                            catboost_fit_kwargs = {'early_stopping_rounds':100}\n                                           ) \nhistory = TrialHistory.load_history(search_space_, history_file)\nplayback = PlaybackSearcher(history, top_n=20, optimize_direction='min')\n\nexp = make_experiment(\n                      train_data = train_data.copy(),\n                      target=target,\n                      num_folds = 5,\n                      max_trials=20,\n                      searcher = playback,\n                      callbacks=[], search_callbacks=[] #only for removing the trainning info, idea from issue#51\n                      )\nestimator = exp.run()\n","b90e000b":"search_history = exp.hyper_model_.history\nfor trial in search_history.trials:\n    print(trial.trial_no,trial.reward)\n\n","a62c3977":"preds = estimator.predict(test_data)\npreds = np.expm1(preds)\nsubmission[target] = preds\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head(5)","b79ab0cf":"exp = make_experiment(\n                      train_data.copy(),\n                      target=target,\n                      num_folds= 5, ## 5 folds for Cross-validate\n                      max_trials= 10, ## search for 10 trials\n                      random_state=7,\n                      callbacks=[], search_callbacks=[] #only for removing the trainning info, idea from issue#51\n                      )\nestimator = exp.run()","3255e9db":"preds = estimator.predict(test_data)\npreds = np.expm1(preds)\nsubmission[target] = preds\nsubmission.head(5)\n# submission.to_csv(\"submission_PLB_0.12528.csv\", index=False)","18fd3226":"### Example","fdca79aa":"# Background\n#### I learned  lots of excellent data preprocessing methods in this competition, such as [*Comprehensive data exploration with Python,etc...*], but here I'm trying to hand it over to autoML.\n#### Let's check the performance of automl. ","29910282":"# Learn some base info about hypergbm","ffff9bb6":"#### By printing estimator, we can see that estimator is a pipeline, including over three steps of data preprocessing, single model trainning and ensemble. The work flow is shown as below:\n![image.png](attachment:5c577f16-ad1e-4ffb-a3fe-171279ed5e89.png)\n#### u can visit https:\/\/github.com\/DataCanvasIO\/HyperGBM for more details.","fbfc3377":"# House Prices - Advanced Regression Techniques\n#### **Hi all, i try to introduce how to use autoML tools for getting quit nice score in this code, and  make it easier to understand.**\n\n#### **Hope you like it.**\n#### **All comments are more than welcome.:)**","6ca98903":"# Check the correlation between local score and PLB\n\nSometimes the local score gets better but the PLB gets worse, which we call overfitting, and sometimes you'll find Lightgbm and XGBoost have similar local scores but the PLB scores are very different.\u00a0So I think it's very important to make sure that the local score and PLB are aligned before doing HPO.","fb48fb97":"# Conclusion\n* **Make sure the local score is aligned with the PLB before doing HPO**.\n* **It's a good trying to ensemble the same models with different random_states**\n* **You can get a baseline score with autoML tool.**\n* **If you want to learn more, this is a good way to read the code of open source projects**  \n* **Automl is a nice way to increase your interest in ML**","acd838f1":" We can choose trial_1(0.1256) and trial_1(0.1375) to submit.\n \n For checking that *cv_score up,PLB up?*  *the diff between cv_score and PLB*.\n \n U can do same things with catboost,xgboost(skip here)\n \n The result is as below:\n \n ![image.png](attachment:68c6956d-dcfc-4176-b26d-16f20cba22c9.png)\n \n### Conclusion\n* **Single model performance: catboost > lightgbm > xgboost**\n* **It's hard to break 0.14 for xgboost**\n* **Catboost cv_score have the highest similarity to PLB.**\n* **Lightgbm's cv_score changed from 0.124 to 0.122, but the PLB remained the same.**\n\n\n \n ","65c97a14":"# Add new features for this data\n\n#### I'm inspired by Vadim Titko,it's really a interesting idea.\n\n![image.png](attachment:7ae9438b-a1b9-4c61-b976-7890653d5a9d.png)\n![image.png](attachment:e1925a3f-c31c-4e6f-bb17-2766907422b5.png)","27808cff":"# Ensemble one model with different random_state.\n\nI  search best catboost model for 200 trials locally, and choose the best one.\n\nI copy it 5 times, and try to reload them to ensemble.\n\n\nPS: In hypergbm, same trial will skip to train, so i change the method of Scaler.\n\nSo that i could make 5 trials with different random_state, u can try it locally.\n\nrandom_state is random,result maybe a little different with me.\n\n**Using this way, i improve my score from 0.1218 to 0.1197**\uff0c\n","930d558b":"# Do a simple submit by hypergbm\nwe choose all GBM algorithm including lightgbm,xgboost,catboost, and search for 10 trials, and ensemble.\n\nlet's check the performance of this way."}}