{"cell_type":{"94ae272f":"code","d20b7a55":"code","f78715be":"code","e0bf2d46":"code","2c3b159a":"code","5e02c541":"code","42c9b09f":"code","a4a734e7":"code","58eab26c":"code","deebc85a":"code","cf79bf7f":"code","7aa85064":"code","376d9c5e":"code","f0689521":"code","d33c18d6":"code","7c35883e":"code","bad6fa90":"code","7b980f2d":"code","ad83986e":"code","95c6a4de":"code","3ad9c93e":"code","643d837f":"code","5bd3b944":"code","c8a5fc3e":"code","53640fa9":"code","5557baec":"code","f7d48019":"code","0a1a6d9c":"code","0741394b":"code","d169c835":"code","f3223ffb":"code","fffd1b7a":"code","710b898a":"code","d5eb5679":"code","f684ddbc":"code","7f398102":"code","547aa77e":"code","66091fe0":"code","e55cd566":"code","7f1856f4":"code","0168ce80":"code","0ac2011e":"code","aee5985f":"code","02688798":"code","783bfe40":"code","7d3ddea3":"code","40ab4cda":"code","f3277d27":"code","8a17821f":"code","ee7c2ab9":"code","931af9d2":"code","9a134b84":"code","69fe453d":"code","80923587":"code","0b4f7bd1":"code","5f866546":"code","981362ec":"code","d718b84e":"code","4bf476d1":"markdown","fa761a37":"markdown","32742108":"markdown","d8c7f58d":"markdown","5eefd10a":"markdown","eb9bd0ca":"markdown","22c38f13":"markdown","35b8cd8a":"markdown","483e714b":"markdown","d270b7f1":"markdown","bc938cdb":"markdown","571408fd":"markdown","04723423":"markdown","2d39238e":"markdown","0f91edea":"markdown","b423aeb1":"markdown","52c3e209":"markdown","a314609a":"markdown","bdb0289c":"markdown","292739ec":"markdown","edd4d60e":"markdown","7a6f79ea":"markdown","dd08f2b1":"markdown","91284b84":"markdown","fc66bebc":"markdown","0e252744":"markdown","c48d4b0e":"markdown","56b7208b":"markdown","3c58282d":"markdown","14fb58b4":"markdown","184b38f6":"markdown","0219c465":"markdown","6a5f16ba":"markdown","d3dde82c":"markdown","204586b6":"markdown","2c7fc99d":"markdown","a2cdd4ef":"markdown","9612088a":"markdown","244140b3":"markdown","c1ad538f":"markdown","7e76c0d1":"markdown","f5491ac6":"markdown","93f6c2d4":"markdown","93011e02":"markdown","89920de0":"markdown","8a9c393e":"markdown","93151de7":"markdown","e33510bd":"markdown","b9638766":"markdown","1d32b315":"markdown","0b74e2a8":"markdown","509bdcc5":"markdown","98860292":"markdown"},"source":{"94ae272f":"import numpy as np\nimport pandas as pd\n\nnp.random.seed(0) \nimport random\n\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nfrom keras.utils.vis_utils import plot_model\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","d20b7a55":"# (X_train, y_train), (X_test, y_test) = mnist.load_data()\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\n\nX_train = train.drop(labels = [\"label\"], axis = 1)\ny_train = train['label']\n\nX_test = test\n\nprint(X_train.shape, X_test.shape)","f78715be":"X_train_plot = X_train.values.reshape(-1, 28, 28)","e0bf2d46":"def Show_example_digits(mono = 'gray'):\n    fig = plt.figure(figsize = (16, 16))\n    for idx in range(15):\n        plt.subplot(5, 5,idx+1)\n        plt.imshow(X_train_plot[idx], cmap = mono)\n        plt.title(\"Digit {}\".format(y_train[idx]))\n        \n    plt.tight_layout()\n    \nShow_example_digits()","2c3b159a":"# Function return digit in grayscale\ndef plot_digit(digit, dem = 28, font_size = 12):\n    max_ax = font_size * dem\n    \n    fig = plt.figure(figsize=(13, 13))\n    plt.xlim([0, max_ax])\n    plt.ylim([0, max_ax])\n    plt.axis('off')\n    black = '#000000'\n    \n    for idx in range(dem):\n        for jdx in range(dem):\n\n            t = plt.text(idx * font_size, max_ax - jdx*font_size, digit[jdx][idx], fontsize = font_size, color = black)\n            c = digit[jdx][idx] \/ 255.\n            t.set_bbox(dict(facecolor=(c, c, c), alpha = 0.5, edgecolor = 'black'))\n            \n    plt.show()","5e02c541":"rand_number = random.randint(0, len(y_train))\nprint(y_train[rand_number])\nplot_digit(X_train_plot[rand_number])","42c9b09f":"digit_range = np.arange(10)\n\nval = y_train.value_counts().index\ncnt = y_train.value_counts().values\nmycolors = ['red', 'blue', 'green', 'orange', 'brown', 'grey', 'pink', 'olive', 'deeppink', 'steelblue']\n\nplt.figure(figsize = (15, 7))\nplt.title(\"The number of digits in the data\", fontsize = 20)\nplt.xticks(range(10))\nplt.bar(val, cnt, color = mycolors);","a4a734e7":"img_rows, img_cols = 28, 28\n\nnum_pixels = X_train.shape[1] \n\ninput_shape = (img_rows, img_cols)","58eab26c":"# Data Normalization [0, 1]\nX_train \/= 255\nX_test \/= 255\n\n# one-hot encoding for target column\ny_train = to_categorical(y_train)\n\n# | [0, 1, 2, ... , 9] | = 10\nnum_classes = y_train.shape[1]\n\n# Number of objects, vector size (28 * 28)\nprint(X_train.shape, X_test.shape)","deebc85a":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 2, stratify=y_train)","cf79bf7f":"def f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    \n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","7aa85064":"def draw_learning_curve(history, keys=['f1', 'loss']):\n    plt.figure(figsize=(20,8))\n    for i, key in enumerate(keys):\n        plt.subplot(1, 2, i + 1)\n        sns.lineplot(x = history.epoch, y = history.history[key])\n        sns.lineplot(x = history.epoch, y = history.history['val_' + key])\n        plt.title('Learning Curve')\n        plt.ylabel(key.title())\n        plt.xlabel('Epoch')\n#         plt.ylim(ylim)\n        plt.legend(['train', 'test'], loc='best')\n    plt.show()","376d9c5e":"def callbacks(name): \n    return [ \n        EarlyStopping(monitor = 'loss', patience = 6), \n        ReduceLROnPlateau(monitor = 'loss', patience = 3), \n        ModelCheckpoint(f'..\/working\/{name}.hdf5', save_best_only=True) # saving the best model\n    ]","f0689521":"def get_mlp():\n    \n    return Sequential([\n        #input layer is automatic generation by keras\n        \n        #hidden layer\n        Dense(512, input_dim = num_pixels, activation='relu'),\n        \n        #output layer\n        Dense(num_classes, activation='softmax')\n    ])","d33c18d6":"model = get_mlp()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1])\nmodel.summary()","7c35883e":"learning_history = model.fit(X_train, y_train,\n          batch_size = 1024, epochs = 40, verbose = 2, callbacks = callbacks('simple_mlp'),\n          validation_data=(X_val, y_val));","bad6fa90":"score = model.evaluate(X_val, y_val, verbose = 0)\nprint('Test loss: {}%'.format(score[0] * 100))\nprint('Test score: {}%'.format(score[1] * 100))\n\nprint(\"MLP Error: %.2f%%\" % (100 - score[1] * 100))","7b980f2d":"draw_learning_curve(learning_history)","ad83986e":"def get_mlpv2():\n    \n    return Sequential([\n        Dense(512, input_dim=num_pixels, activation='relu'),\n        Dropout(0.3),\n        Dense(256, activation='relu'),\n        Dropout(0.2),\n        Dense(128, activation='relu'),\n        Dense(num_classes, activation='softmax')\n    ])","95c6a4de":"model = get_mlpv2()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1])\nmodel.summary()","3ad9c93e":"learning_history = model.fit(X_train, y_train,\n          batch_size = 1024, epochs = 40, verbose = 2, callbacks = callbacks('mlp_reg'),\n          validation_data=(X_val, y_val));","643d837f":"draw_learning_curve(learning_history)","5bd3b944":"score = model.evaluate(X_val, y_val, verbose = 0)\nprint('Test loss: {}%'.format(score[0] * 100))\nprint('Test score: {}%'.format(score[1] * 100))\n\nprint(\"MLP Error: %.2f%%\" % (100 - score[1] * 100))","c8a5fc3e":"X_train.shape","53640fa9":"X_train = X_train.values.reshape(-1, 28, 28, 1)\nX_val = X_val.values.reshape(-1, 28, 28, 1)\nX_test = X_test.values.reshape(-1, 28, 28, 1)\ninput_shape = (28, 28, 1)","5557baec":"def get_cnn():\n    return Sequential([\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape = input_shape),\n        Conv2D(32, kernel_size=(3, 3), activation='relu' ),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n        Conv2D(64, kernel_size=(3, 3), activation='relu' ),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        Conv2D(128, kernel_size=(3, 3), activation='relu' ),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        \n        Flatten(),\n        \n        Dense(256, activation='relu'),\n        Dropout(0.5),\n        Dense(num_classes, activation = \"softmax\")\n        \n    ])","f7d48019":"model = get_cnn()\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=[f1])\nmodel.summary()","0a1a6d9c":"learning_history = model.fit(X_train, y_train,\n          batch_size = 128,\n          epochs = 50,\n          verbose = 1,\n          callbacks = callbacks('cnn_v1'),\n          validation_data = (X_val, y_val))","0741394b":"score = model.evaluate(X_val, y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test score:', score[1])\n\nprint(\"CNN Error: %.2f%%\" % (100-score[1]*100))","d169c835":"draw_learning_curve(learning_history)","f3223ffb":"y_pred = model.predict(X_val)","fffd1b7a":"def draw_output(idx_nums):\n    plt.figure(figsize = (20, 20))\n    plt.xticks( range(10) )\n    x = np.ceil(np.sqrt(len(idx_nums)))\n    cnt = 1\n    for ph in idx_nums:\n        plt.subplot(x, x, cnt)\n        curr_photo = y_val[ph]\n        \n        plt.xlim(0, 10)\n        plt.title(\"Digit: {0}\\n idx: {1} \".format(np.argmax(y_val[ph]), ph), fontsize = 10) \n        plt.bar(range(10), y_pred[ph])\n        \n        cnt += 1","710b898a":"cnt_error = []\nfor idx, (a, b) in enumerate(zip(y_val, y_pred)):\n    if np.argmax(a) == np.argmax(b): continue\n    cnt_error.append( (np.argmax(a)) )\n\ncnt_error = np.unique(cnt_error, return_counts = True)\nsns.set_style(\"darkgrid\")\nplt.figure(figsize = (15, 7))\nbar_plot = sns.barplot(cnt_error[0], cnt_error[1], palette=\"muted\")\nplt.show()","d5eb5679":"cnt_ind = 1\nlist_idx = []\nX_val_plot = X_val.reshape( X_val.shape[:-1] )\nfig = plt.figure(figsize=(14, 14))\n\nfor idx, (a, b) in enumerate(zip(y_val, y_pred)):\n    if np.argmax(a) == np.argmax(b): continue\n    if (np.argmax(a) == 2 or np.argmax(a) == 9):    \n        plt.subplot(5, 5, cnt_ind)\n        plt.imshow(X_val_plot[idx], cmap='gray', interpolation='none')\n        plt.title('y_true={0}\\ny_pred={1}\\n ind = {2}'.format(np.argmax(a), np.argmax(b), idx))\n        plt.tight_layout()\n        list_idx.append(idx)\n        cnt_ind += 1","f684ddbc":"draw_output(list_idx)","7f398102":"train_aug = ImageDataGenerator(\n        featurewise_center = False,\n        samplewise_center = False,\n        featurewise_std_normalization = False, \n        samplewise_std_normalization = False,\n        zca_whitening = False,\n        horizontal_flip = False,\n        vertical_flip = False,\n        fill_mode = 'nearest',\n        rotation_range = 10,  \n        zoom_range = 0.1, \n        width_shift_range = 0.1, \n        height_shift_range = 0.1)\n        \n\ntrain_aug.fit(X_train)\ntrain_gen = train_aug.flow(X_train, y_train, batch_size=64)","547aa77e":"def get_cnn_v2():\n    return Sequential([\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape = input_shape),\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        \n        Flatten(),\n          \n        Dense(512, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.5),\n        \n        Dense(256, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.4),\n        \n        Dense(64, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.3),\n        \n        Dense(num_classes, activation = \"softmax\")\n        \n    ])","66091fe0":"model = get_cnn_v2()\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=[f1])\nmodel.summary()","e55cd566":"learning_history = model.fit_generator(train_gen, epochs = 100, \n                               steps_per_epoch = X_train.shape[0] \/\/ 64,\n                               validation_data = (X_val, y_val),\n                               callbacks = callbacks('best_cnn'),\n                             )","7f1856f4":"model = load_model('..\/working\/best_cnn.hdf5', custom_objects={\"f1\": f1})","0168ce80":"score = model.evaluate(X_val, y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test score:', score[1])\n\nprint(\"CNN Error: %.2f%%\" % (100-score[1]*100))","0ac2011e":"draw_learning_curve(learning_history)","aee5985f":"output = model.predict(X_test)\n\noutput = np.argmax(output, axis = 1)\n\noutput = pd.Series(output, name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001), name = \"ImageId\"), output], axis = 1)\n\nsubmission.to_csv(\"submission.csv\", index=False)","02688798":"def load_data(path):\n    with np.load(path) as f:\n        x_train, y_train = f['x_train'], f['y_train']\n        x_test, y_test = f['x_test'], f['y_test']\n        return (x_train, y_train), (x_test, y_test)\n\n(x_train1, y_train1), (x_test1, y_test1) = load_data('..\/input\/mnist-numpy\/mnist.npz')","783bfe40":"x_train1 = x_train1 \/ 255\nx_test1 = x_test1 \/ 255\n\nx_train1 = x_train1.reshape(-1, 28, 28, 1)\nx_test1 = x_test1.reshape(-1, 28, 28, 1)\n\ny_train1 = y_train1.reshape(y_train1.shape[0], 1)\ny_test1 = y_test1.reshape(y_test1.shape[0], 1)","7d3ddea3":"Add_X = np.vstack((x_train1, x_test1))\n\nAdd_y = np.vstack((y_train1, y_test1))\n\nAdd_y = to_categorical(Add_y)","40ab4cda":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n\nX_train = train.drop(labels = [\"label\"], axis = 1)\ny_train = train['label']\ny_train = to_categorical(y_train)\n\nX_train \/= 255\nX_train = X_train.values.reshape(-1, 28, 28, 1)","f3277d27":"add_train_aug = ImageDataGenerator(\n        featurewise_center = False,\n        samplewise_center = False,\n        featurewise_std_normalization = False, \n        samplewise_std_normalization = False,\n        zca_whitening = False,\n        horizontal_flip = False,\n        vertical_flip = False,\n        fill_mode = 'nearest',\n        rotation_range = 10,  \n        zoom_range = 0.1, \n        width_shift_range = 0.1, \n        height_shift_range = 0.1)\n        \n\nadd_train_aug.fit(Add_X)\nadd_train_gen = add_train_aug.flow(Add_X, Add_y, batch_size=64)","8a17821f":"model = get_cnn_v2()\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=[f1])\nmodel.summary()","ee7c2ab9":"learning_history = model.fit_generator((add_train_gen), epochs = 100, \n                               steps_per_epoch = x_train1.shape[0] \/\/ 64,\n                               validation_data = (X_val, y_val),\n                               callbacks = callbacks('cnn_bonus'),\n                             )","931af9d2":"model = load_model('cnn_bonus.hdf5', custom_objects={\"f1\": f1})","9a134b84":"score = model.evaluate(X_val, y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test score:', score[1])\n\nprint(\"CNN Error: %.2f%%\" % (100-score[1]*100))","69fe453d":"draw_learning_curve(learning_history)","80923587":"plot_model(model)","0b4f7bd1":"plt.imshow(X_train[0].reshape(28, 28), cmap='gray');","5f866546":"def apply_grey_patch(image, top_left_x, top_left_y, patch_size):\n    patched_image = np.array(image, copy=True)\n    patched_image[top_left_y:top_left_y + patch_size, top_left_x:top_left_x + patch_size, :] = 0\n\n    return patched_image\n\n\nimg = X_train[0]\n\nPATCH_SIZE = 4\nsensitivity_map = np.zeros((img.shape[0], img.shape[0]))\n\nfor top_left_x in range(0, img.shape[0], PATCH_SIZE):\n    for top_left_y in range(0, img.shape[1], PATCH_SIZE):\n        patched_image = apply_grey_patch(img, top_left_x, top_left_y, PATCH_SIZE)\n        \n        \n        predicted_classes = model.predict(np.array([patched_image]))[0]\n        confidence = predicted_classes[1]\n        \n        sensitivity_map[\n            top_left_y:top_left_y + PATCH_SIZE,\n            top_left_x:top_left_x + PATCH_SIZE,\n        ] = confidence\n        ","981362ec":"plt.imshow(sensitivity_map, cmap='gray');","d718b84e":"output = model.predict(X_test)\n\noutput = np.argmax(output, axis = 1)\n\noutput = pd.Series(output, name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001), name = \"ImageId\"), output], axis = 1)\n\nsubmission.to_csv(\"bonus_submission.csv\", index=False)","4bf476d1":"#### Simple MLP with one hidden layer","fa761a37":"#### Split data\n\nadded stratified folds","32742108":"Input layer has 28 * 28 pixels reshape to vector\n\nHidden layer has a lot of neurons\n\nOutput layer has 10 neurons ","d8c7f58d":"# Generate bonus output","5eefd10a":"CNN consists with:\n- convolution layer\n- in the past MLP","eb9bd0ca":"# Experimenting on MNIST","22c38f13":"## Data Preparing","35b8cd8a":"### Importing libs","483e714b":"#### Run the `draw_output` function to see the probability of each value occurring","d270b7f1":"<font size=\"3\">\n    <div style=\"text-align: right\"> <b> Author <\/b> <\/div>\n<\/font>\n<div style=\"text-align: right\"> J\u0119drzej <\/div>\n<div style=\"text-align: right\"> Dudzicz <\/div>","bc938cdb":"# Occlusion sensitivity","571408fd":" The **MNIST** database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems\n \n The **MNIST** database contains 60,000 training images and 10,000 testing images.\n Photo size: **28x28 p**.","04723423":"#### As you can see, even with such a large data set, we do not get 100% accuracy","2d39238e":"## Visualize Model","0f91edea":"## Data Augmentation","b423aeb1":"Our network has 407,050 params (weights)","52c3e209":"#### I slightly improved the model, so we keep working.","a314609a":"Cool! I get ~98% accuracy with easy model MLP and i didn't work too much.\n\nAccording to Pareto principle 80\/20 in this case 20% work generate ~80%~ 98% accuracy :)","bdb0289c":"#### Chart of the number of digits in the data","292739ec":"#### As you can see, the model is wrong in cases where the common person would also have trouble finding the correct answer.","edd4d60e":"![MLP](https:\/\/miro.medium.com\/max\/700\/1*-IPQlOd46dlsutIbUq1Zcw.png)","7a6f79ea":"**Update:**\n- I made this notebook 7 months ago, and it is one of my first data science projects. During this time I learned a lot of interesting things and put these things in here.","dd08f2b1":"#### I made function to visual output","91284b84":"### Let's see in which cases the model is invalid.","fc66bebc":"Firstly, let's think how the network should look like. It will have three layers:\n\n1 Input Layer\n\n2 Hidden Layer\n\n3 Output Layer\n","0e252744":"## Exploratory Data Analysis","c48d4b0e":"#### Building new model and using batch normalization","56b7208b":"##  Reading data","3c58282d":"#### Function to drawing learning curve history learning neural network\n\n","14fb58b4":"#### Adding new layer and Dropout to avoid overfitting","184b38f6":"### The Purpose of notebook","0219c465":"#### I will try to improve the result by expanding the data. You have to be careful when rotating your photos not to misclassify numbers such as 9 and 6.\n\n#### Data augmentation:\n\nRandomly shift images horizontally by 10% of the width\n\nRandomly shift images vertically by 10% of the height\n\nRandomly rotate images by 10 degrees\n\nRandomly Zoom by 10% some images\n","6a5f16ba":"#### We need to reshape data.","d3dde82c":"### Final step: Conclusions\n\n#### I achieved the following results:\n- MLP: ~98% f1\n- CNN: ~100% f1\n\nI have created a model that recognizes handwritten numbers. You can try to get more data to make the model even better.\n\n\n#### This is my first notebook.\n\n#### I would love to know your comments and note about this.","204586b6":"#### Let's see these photos (2, 9)","2c7fc99d":"Let's see, using additional data, how this will affect the final result.","a2cdd4ef":"# Generate output","9612088a":"It will use 3 convolutional layers: (Conv2D, Conv2D, pool)","244140b3":"## CNN ([Convolutional_neural_network](https:\/\/en.wikipedia.org\/wiki\/Convolutional_neural_network))","c1ad538f":"### Structure","7e76c0d1":"Run new model ","f5491ac6":"I will design and try two neural networks to get ~100% accuracy\n- MLP (Multilayer perceptron)\n\n- CNN (Convolutional neural network)","93f6c2d4":"Design network with using keras \n\nThe metrics we use are f1_score","93011e02":"#### We achieved a great result of 99.6% accuracy","89920de0":"#### You can use **GPU** to accelerate training","8a9c393e":"#### Let's assign the values provided by the model","93151de7":"#### The number of errors for the each digit","e33510bd":"#### Loading the best model","b9638766":"# Adding Callbacks\n- EarlyStopping (Stop training when a monitored metric has stopped improving)\n- ReduceLROnPlateau (Reduce learning rate when a metric has stopped improving)\n- ModelCheckpoint (Callback to save the Keras model or model weights at some frequency)","1d32b315":"# Bonus","0b74e2a8":"### About MNIST Dataset","509bdcc5":"![CNN](https:\/\/miro.medium.com\/max\/1872\/1*SGPGG7oeSvVlV5sOSQ2iZw.png)","98860292":"## MLP ([Multilayer perceptron](https:\/\/en.wikipedia.org\/wiki\/Multilayer_perceptron))"}}