{"cell_type":{"5d8733e0":"code","d65a33f9":"code","c05652cc":"code","ddb194ab":"code","1cf8f966":"code","bde868cf":"code","40fc73ac":"code","167a5551":"code","786073b9":"code","7e438ed8":"code","c1c00de8":"code","52235d1a":"code","5a8ec70c":"code","ec34ed5a":"code","d1dcc901":"code","4ea06c69":"code","8734b391":"code","7c609bfa":"code","24e78166":"code","9094644a":"code","78d4200a":"code","d7e537e0":"code","10728998":"code","5abff7e6":"code","e70a5db6":"code","13e83e9c":"code","0dcebb64":"code","389aa0e3":"code","fb1fb515":"code","16253966":"code","66179fbf":"code","04a8b9db":"code","c22f840e":"code","ccbb0f98":"code","2a75af51":"code","3dfa5dbb":"code","022db08c":"code","d49ffb0e":"code","b1978611":"code","bc8445b0":"code","da7be16f":"code","165ca059":"code","89cb8a48":"code","529e960c":"code","213dd4d2":"code","e8f72dad":"code","2167b34e":"code","8d432e84":"code","833c97f1":"code","1a142d0f":"code","8dffe1e3":"code","133cf2f5":"code","74df67a7":"code","85495ae9":"code","3be41e59":"code","4c0a9077":"code","6146737f":"code","c49d47a0":"code","62d00e01":"code","49d245b8":"code","d53fc439":"code","b687dc49":"code","93398b1d":"code","ed5b0fbd":"code","fc352bcc":"code","4f4c0088":"code","04cd6fcc":"code","2e46b8ff":"markdown","7e85093b":"markdown","dc1c013f":"markdown","f4575e3c":"markdown","be75c2f5":"markdown","23c4e6d5":"markdown","391148b4":"markdown","900158cd":"markdown","3e21eda5":"markdown","a8103e86":"markdown","c4c6ca55":"markdown","f21d5684":"markdown","a599a59c":"markdown","b6adda17":"markdown","ab0949ef":"markdown","3d34b5ca":"markdown","389b4b92":"markdown","59d7c7b7":"markdown","4ce55a17":"markdown","0f9fc6a3":"markdown","9b2cdd34":"markdown","8ca8a68f":"markdown","7f13adb5":"markdown"},"source":{"5d8733e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d65a33f9":"#data manipulation\nimport pandas as pd\nimport numpy as np\n#system operations and python version\nimport os\nimport sys\nassert sys.version_info >= (3,5)\n#visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n#display all columns\npd.options.display.max_columns = None\n#consistent sized plots\nfrom pylab import rcParams\nrcParams['figure.figsize']=12,5\nrcParams['axes.labelsize']= 12\nrcParams['xtick.labelsize']= 12\nrcParams['ytick.labelsize']= 12\n#handle unwanted warnings\nimport warnings\nwarnings.filterwarnings(action='ignore',message='')","c05652cc":"#load training and test data\ntrain =  pd.read_csv('\/kaggle\/input\/soccer-dataset\/train.csv',delimiter=',',engine='python')\ntest =  pd.read_csv('\/kaggle\/input\/soccer-dataset\/test.csv',delimiter=',',engine='python')","ddb194ab":"#view the top rows\ntrain.head()","1cf8f966":"train.tail()","bde868cf":"#check the shape of train and test data\ntrain.shape, test.shape","40fc73ac":"#check info \ntrain.info()","167a5551":"#check basic stats of the numerical features\ntrain.describe().transpose()","786073b9":"#check the value counts of all categorical features\ncat_features = train.select_dtypes(include='object').columns.to_list()\nprint('Categorical features \\n {}'.format(cat_features))","7e438ed8":"#number of seasons involved\ntrain['season'].nunique()","c1c00de8":"#seasons and their counts\ntrain['season'].value_counts()","52235d1a":"'''\nPrint the unique feature names with counts for categorical features\n'''\nfor feature in cat_features:\n    print('Feature Name {}'.format(feature))\n    print('--------------------------------')\n    print('Number of unique categories {}'.format(train[feature].nunique()))\n    print('Unique categories & their counts')\n    print(train[feature].value_counts().sort_values(ascending=False))\n    print('\\n')","5a8ec70c":"#check for data balance\nsns.countplot(train['Outcome'])\nplt.title('Countplot of Outcome')\nplt.show()","ec34ed5a":"train['Outcome'].value_counts()","d1dcc901":"'''\nNull values in the dataset\n'''\ntrain.isna().sum().sort_values(ascending=False)","4ea06c69":"#visualize null values\nplt.figure(figsize=(15,7))\nsns.heatmap(train.isna(),cbar=False,yticklabels=False,cmap='summer')\nplt.title('Null Values Visualization')\nplt.show()","8734b391":"#convert to the right data type\ntrain['date'] = pd.to_datetime(train['date'],dayfirst=True)\ntrain['season'] = train['season'].astype(str)","7c609bfa":"train.info()","24e78166":"#check a few rows\ntrain.head(2)","9094644a":"'''\nFirst & foremost, create a copy of the train data\n'''\nsoccer = train.copy()","78d4200a":"'''\nCheck histogram of the integer features\n'''\nnum_features = soccer.select_dtypes(include=[float,int]).columns.to_list()\n\nsoccer_num = soccer[num_features]\nsoccer_num.hist(figsize=(20,15),bins=30)\nplt.show()","d7e537e0":"#check the correlation heatmap\nsns.heatmap(soccer.corr())\nplt.title('Correlation Heatmap')\nplt.show()","10728998":"'''\nPlot the correlations between numerical features which do not have null values. \n'''\n# plot scatter matrix\nfrom pandas.plotting import scatter_matrix\n\nattributes = ['SPI1', 'SPI2', 'proj_score1', 'proj_score2']\nscatter_matrix(soccer[attributes], figsize=(12, 8))\nplt.show()","5abff7e6":"#scatter plot of correlated feature SPI1 and SPI2\nsns.lmplot(x='SPI1',y='SPI2',data=soccer,hue='Outcome',palette='Set1',markers=['x','o'])\nplt.title('SPI1 vs SPI2 Scatter Plot')\nplt.grid()\nplt.xlabel('SPI1')\nplt.ylabel('SPI2')\nplt.show()","e70a5db6":"#scatter plot of correlated feature proj_score2 and SPI2\nplt.figure(figsize=(15,7))\nsns.lmplot(x='proj_score2',y='SPI2',data=soccer,hue='Outcome',palette='Set1',markers=['x','o'])\nplt.title('proj_score2 vs SPI2 Scatter Plot')\nplt.grid()\nplt.xlabel('proj_score2')\nplt.ylabel('SPI2')\nplt.show()","13e83e9c":"#regression plot of correlated feature proj_score2 and SPI2\nsns.regplot(x='proj_score2',y='SPI2',data=soccer,x_jitter=0.2)\nplt.title('proj_score2 vs SPI2 Regression Plot')\nplt.grid()\nplt.xlabel('proj_score2')\nplt.ylabel('SPI2')\nplt.show()","0dcebb64":"# Plot the distribution of SPI1\nsns.displot(\n    data=soccer,\n    x=\"SPI1\", hue=\"Outcome\",\n    kind=\"kde\", height=6,\n    multiple=\"fill\", clip=(0, None),\n    palette=\"ch:rot=-.25,hue=1,light=.75\")\nplt.title('Plot of Distribution of SPI1')\nplt.show()   ","389aa0e3":"# Plot the distribution of SPI2\nsns.displot(\n    data=soccer,\n    x=\"SPI2\", hue=\"Outcome\",\n    kind=\"kde\", height=6,\n    multiple=\"fill\", clip=(0, None),\n    palette=\"ch:rot=-.25,hue=1,light=.75\")\nplt.title('Plot of Distribution of SPI2')\nplt.show() ","fb1fb515":"# Plot the distribution of SPI1\nsns.displot(\n    data=soccer,\n    x=\"proj_score1\", hue=\"Outcome\",\n    kind=\"kde\", height=6,\n    multiple=\"fill\", clip=(0, None),\n    palette=\"ch:rot=-.25,hue=1,light=.75\")\nplt.title('Plot of Distribution of proj_score1')\nplt.show()  ","16253966":"# Plot the distribution of SPI1\nsns.displot(\n    data=soccer,\n    x=\"proj_score2\", hue=\"Outcome\",\n    kind=\"kde\", height=6,\n    multiple=\"fill\", clip=(0, None),\n    palette=\"ch:rot=-.25,hue=1,light=.75\")\nplt.title('Plot of Distribution of proj_score2')\nplt.show()  ","66179fbf":"'''\nNormality tests of SPI1 and SPI2 features\n'''\nfrom scipy.stats import normaltest\n\nfeatures = ['SPI1','SPI2','proj_score1','proj_score2']\nfor feature in features:\n    data = soccer[feature]\n    stat,p = normaltest(data)\n    if p >0.05:\n        print('Feature {} is normally distributed'.format(feature))\n    else:\n        print('Feature {} is NOT normally distributed'.format(feature))","04a8b9db":"'''\ntest for correlation\n'''\nfrom scipy.stats import pearsonr\n\ndata1 = soccer['SPI1']\ndata2 = soccer['SPI2']\n\nstat, p = pearsonr(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print(f'Feature SPI1 and SPI2 are independent and have no correlation')\nelse:\n    print('Feature SPI1 and SPI2 are dependent and are correlated')","c22f840e":"'''\nDrop the fatures which have null values, these all have more than 64% missing values\n'''\nsoccer.drop(['importance1', 'importance2','score1', 'score2', 'xg1', 'xg2', 'nsxg1', 'nsxg2', 'adj_score1','adj_score2'],axis=1,inplace=True)\nsoccer.head()\n             ","ccbb0f98":"#check if any missing values\nsoccer.isna().sum()","2a75af51":"'''\nCreate a new feature as the linear sum of SPI1 and SPI2\n'''\nsoccer['SPI'] = soccer['SPI1'] + soccer['SPI2']\n#soccer['proj_score'] = soccer['proj_score1'] + soccer['proj_score2']","3dfa5dbb":"'''\nDrop the redundant feature columns\n'''\n#drop the redundant columns \nsoccer.drop(['SPI1','SPI2','proj_score1','proj_score2'],axis=1,inplace=True)\nsoccer.head(3)","022db08c":"#replace date with month, the year will be taken care by the season feature \nimport datetime as dt\nsoccer['month'] = soccer['date'].dt.month\nsoccer['weekday'] = soccer['date'].dt.weekday\nsoccer.drop('date',axis=1,inplace=True)\nsoccer.head(2)","d49ffb0e":"soccer.info()","b1978611":"plt.figure(figsize=(25,5))\nax = sns.countplot(x=\"league_id\", data=soccer,\n                   facecolor=(0, 0, 0, 0),\n                   linewidth=2,\n                   edgecolor=sns.color_palette(\"dark\", 3),hue='Outcome')\nplt.title('Countplot of league_id')\nplt.show()","bc8445b0":"'''\nSplit the data into train and dev set\n'''\nfrom sklearn.model_selection import train_test_split\nX = soccer.drop('Outcome',axis=1)\ny = soccer['Outcome']\nseed =  41\ntest_size = 0.2\n\n#split the dataset\nX_train,X_dev,y_train,y_dev = train_test_split(X,y,test_size=test_size,random_state=seed,stratify=soccer['Outcome'])","da7be16f":"#check the shape post split \nX_train.shape,X_dev.shape,y_train.shape,y_dev.shape","165ca059":"# split into numerical and categorical data\nX_train_num = X_train.select_dtypes(exclude='object')\nX_train_cat = X_train.select_dtypes(include='object')\n\nX_dev_num = X_dev.select_dtypes(exclude='object')\nX_dev_cat = X_dev.select_dtypes(include='object')","89cb8a48":"#transform the numerical features for modeling\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n\nnum_pipeline = Pipeline([('pow_transform',PowerTransformer()),                         \n                         ('std_scaler',StandardScaler())])\n\nX_train_num_tr = num_pipeline.fit_transform(X_train_num)\n\n","529e960c":"#transformed numerical values\nX_train_num_tr","213dd4d2":"#transform the categorical features for modeling\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_pipeline = Pipeline([('cat_encoder',OneHotEncoder(handle_unknown='ignore',sparse=False))])\nX_train_cat_tr = cat_pipeline.fit_transform(X_train_cat)","e8f72dad":"X_train_num_tr.shape","2167b34e":"#concatenate the transformed array to create the input array\nX_train_tr = np.hstack((X_train_num_tr,X_train_cat_tr))","8d432e84":"'''\nApply the transformation pipeline on the dev set\n'''\nX_dev_num_tr = num_pipeline.transform(X_dev_num)\nX_dev_cat_tr = cat_pipeline.transform(X_dev_cat)\nX_dev_tr = np.hstack((X_dev_num_tr,X_dev_cat_tr))","833c97f1":"X_dev_tr.shape","1a142d0f":"#there is no transformation here --> converting to numpy array\ny_train_tr = np.array(y_train)\ny_dev_tr = np.array(y_dev)","8dffe1e3":"#import predictive models\nfrom sklearn.ensemble import RandomForestClassifier","133cf2f5":"'''\nTrain the baseline model\n'''\nweights = {0:5.0,1:1.0}\nclf = RandomForestClassifier(n_estimators=200,random_state=42,max_features=0.2,class_weight=weights)\nclf.fit(X_train_tr,y_train_tr)","74df67a7":"#predict on the train and dev set \npredictions_train = clf.predict(X_train_tr)\npredictions_dev = clf.predict(X_dev_tr)","85495ae9":"## evaluate the models\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report","3be41e59":"print('Accuracy on the train set {}'.format(accuracy_score(y_train_tr,predictions_train)))\nprint('Accuracy on the dev set {}'.format(accuracy_score(y_dev_tr,predictions_dev)))","4c0a9077":"'''\nClassification report on the dev set\n'''\nprint(classification_report(y_dev_tr,predictions_dev))","6146737f":"'''\nPrepare the test data from predictions. This is being done linearly here. A better approach is to have a data preparation function\n'''\n#convert to the right data type \ntest['date'] = pd.to_datetime(test['date'],dayfirst=True)\ntest['season'] = test['season'].astype(str)\n\n#drop the features with more than 65% null values \ntest.drop(['importance1', 'importance2','score1', 'score2', 'xg1', 'xg2', 'nsxg1', 'nsxg2', 'adj_score1','adj_score2'],axis=1,inplace=True)\n\n#create the new features & drop the redundant features\ntest['SPI'] = test['SPI1'] + test['SPI2']\n#test['proj_score'] = test['proj_score1'] + test['proj_score2']\ntest.drop(['SPI1','SPI2','proj_score1','proj_score2'],axis=1,inplace=True)\n\n\ntest['month'] = test['date'].dt.month\ntest['weekday'] = test['date'].dt.weekday\ntest.drop('date',axis=1,inplace=True)\n\n#apply the data transformation using defined pipeline\nX_test_num = test.select_dtypes(exclude='object')\nX_test_cat = test.select_dtypes(include='object')\n\nX_test_num_tr = num_pipeline.transform(X_test_num)\nX_test_cat_tr = cat_pipeline.transform(X_test_cat)\nX_test_tr = np.hstack((X_test_num_tr,X_test_cat_tr))","c49d47a0":"'''\nFit the model on the entire training set before proceeding with final predictions on the test set\n'''\nX_train_full = np.vstack((X_train_tr,X_dev_tr))\ny_train_full = np.hstack((y_train_tr,y_dev_tr))\nclf.fit(X_train_full,y_train_full)\n\npredictions_train_full = clf.predict(X_train_full)\nprint('Accuracy = {}'.format(accuracy_score(y_train_full,predictions_train_full)))\n\npredictions_train_full_proba = clf.predict_proba(X_train_full)\nprint('log loss = {}'.format(log_loss(y_train_full,predictions_train_full_proba[:,1])))","62d00e01":"#perform predictions on the prepared test data\npredictions_test = clf.predict(X_test_tr)\n#get the prediction probability\npredictions_test_proba = clf.predict_proba(X_test_tr)","49d245b8":"#create and submit the submission file\nsubmission_df =  pd.DataFrame(predictions_test_proba[:,1],columns=['Outcome'])\nsubmission_df.to_csv('soccer_26Aug_1.csv',index=False)","d53fc439":"#import tensorflow\nimport tensorflow as tf","b687dc49":"#instantiate the model\nmodel = tf.keras.models.Sequential()\n#define activation\nactivation = tf.keras.layers.Activation(tf.nn.relu)\n#add the dense layers\nmodel.add(tf.keras.layers.Dense(units=1024,input_dim=X_train_full.shape[1],activation='relu'))\nmodel.add(tf.keras.layers.Dense(units=512,activation=activation))\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(tf.keras.layers.Dense(units=256,activation=activation))\nmodel.add(tf.keras.layers.Dense(units=128,activation=activation))\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(tf.keras.layers.Dense(units=64,activation=activation))\nmodel.add(tf.keras.layers.Dense(units=32,activation=activation))\n\nmodel.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n\n#compile the model\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","93398b1d":"#define the early stop criteria\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=50,restore_best_weights=True)","ed5b0fbd":"#fit the model\nhistory = model.fit(X_train_full,y_train_full,epochs=500,validation_split=0.2,callbacks=[early_stop],verbose=2,\n                   batch_size=512)","fc352bcc":"from sklearn.metrics import log_loss\npredictions_train = model.predict(X_train_full)","4f4c0088":"predictions_train","04cd6fcc":"predictions_test_dnn = model.predict(X_test_tr)\n#create and submit the submission file\nsubmission_df =  pd.DataFrame(predictions_test_dnn,columns=['Outcome'])\nsubmission_df.to_csv('soccer_26Aug_2_dnn.csv',index=False)","2e46b8ff":"- _As per the data description, Outcome is the target label_\n","7e85093b":"## _Exploratory Data Analysis_","dc1c013f":"- _This shows that proj_score2 and SPI2 have significant linear correlation_","f4575e3c":"- _As the value of SPI2 increases, there appears to be more of outomces 0. However, over the distribution of the SPI2, the outcomes 0 and 1 appear to be almost equal_","be75c2f5":"- _Similar to SPI1 feature, there are more outcomes of value 1 when the value of proj_score1 increases. Again to remind there is no feature description or metadata to understand the associate any logic of this relationship._","23c4e6d5":"## _Further Improvements_\n- _Performance can be improved using hyper-parameter search and cross validation. The metric of interest is log_loss_\n\n","391148b4":"## _Statistical Correlations_","900158cd":"- _There are 3 seasons from 2019 to 2021_\n- _There are 657 unique teams in both Team1 and Team2_\n- _There are 39 unique leagues and United Soccer League and Major League Soccer have the highest counts_ ","3e21eda5":"## _Summary of the EDA_\n- _There are numerous values with null in some of the features. Instead of imputing without understanding their significance and role, it is better to simply drop them from further analysis and from building a predictive model._\n- _SPI1 SPI2, proj score1 and proj score2 are some of the features which depict multi-collinearity and also influence the outcome label in a big way as indicated by the displots. The data is imbalanced with more 1's than 0 outcomes and this gives us a clue of which feature would be more suitable to have a better predictability of one of these two outcomes_\n\n\n","a8103e86":"#### _Analysis from the scatter plots_\n- _SPI1 and SPI2 are highly correlated with each other. Both these features are positively correlated_\n- _proj_score1 and proj_score2 are highly negatively correlated_\n- _There is also visible correlation between SPI1 and proj_score1 as well as SPI2 and proj_score2_\n- _SPI2 and proj_score2 seems to have a non-linear relationship as the scatter plot is denser till the SPI2 values of 50 and then becomes lighter and higher as SPI2 values increases_\n\n_One way to address the multi-collinearity would be by dimensionality reduction using PCA or even t-SNE for non linear relationship_","c4c6ca55":"- _Similar to SPI2 feature, there are more outcomes of value 0 when the value of proj_score1 increases. Again to remind there is no feature description or metadata to understand the associate any logic of this relationship._","f21d5684":"## _Convert features to right data type_\n- _There are date features as object_\n- _season should be a categorical feature which can be later encoded for model training_\n- _league id is numerical which can be converted to categorical. However, in all likelihood this feature would eventually will not be seen important to feed into the model training. The model might learn the unique pattern from these id's and may not generalize well to the unseen data_ \n- _league id however, is a unique identifier and in production it would be a helpful feature to determine that the same data remains in the train and dev set even when the dataset is updated with new data. With the help of these unique league ids, hash codes can be generated using which the same data observation can be ensured to be in the train and the dev set for multiple iterations in the future. For the current problem, this feature is not useful_","a599a59c":"# _Soccer Fever_\n\n### _Introduction_\nSoccer aka Football is the most popular game in the world. It\u2019s a religion of its own. If groups of 10 people can stop time and make people watch them in awe and reverence, it\u2019s this beautiful game. Also, anybody can play soccer- all it needs is 4 poles, a ground and a ball. You can just get started with the play.\n\nIn fact, Nelson Mandela very effectively used Football as the unifying factor when he was elected President of South Africa post the Apartheid era. The sport just cuts across all discriminating factors.\n\n### _Relevance_\nAn entire ecosystem revolves around this beautiful sport. Clubs, Merchandise, listed football clubs, fan clubs and a group of rivals who can just get into a fight based on the outcome of the game.  The amount of currency involved in this game is just phenomenal. It impacts millions of people who depend on it for their livelihood and recreation.\n\n### _Criticality_\nWe live in ambiguity and always need some information to just make a decision. Decisions are made based on possible outcomes. Win\/ Loss\/ Pass \/ Fail etc.\n\nThe below problem statement is a classic study for decision-making and understanding the odds stacked against a particular situation.\n\n<b> Train <\/b>\nDataset: 7443*21\nColumns: 21\nTarget Column: Outcome\nEvaluation Metric: Log Loss\n\n<b> Test <\/b>\nDataset: 4008*20\nColumns: 20\nSubmission Format :\n\nDataset: 4008*1( Column Name - \u2018Outcome\u2019)\nSkills\nMulti-Class Classification\nOptimizing Log Loss","b6adda17":"## _Statistical Tests_","ab0949ef":"## _Feature Engineering and Data Preparation_\n### _Ways to handle the multi-collinearity_\n- _Drop one of the multi-collinear feature_\n- _Create  new feature combining the multi-collinear features_\n- _Let the features be present and handle them as part of dimensionality reduction techniques_\n","3d34b5ca":"- _The yellow patches reflect the null values. There are way too many null values in some of the features_\n- _Imputing these features with more than 50% of their values as null would be a huge assumption and risk to the model. Secondly, there is no metadata provided with the dataset to determine what these features actually reflect and how they can influence the Outcome. This is a pity but a hard reality of this hackathon._","389b4b92":"- _Similarly, we can statistically check the correlations between the other features. Infact they are all correlated_","59d7c7b7":"- _Clearly the data is not balanced. This consideration will have to be adjusted during model training. In logistic regression this can be addressed via weights hyper-parameter. Other approach would be to split the dataset for model training using stratified approach_","4ce55a17":"## _Load Packages and Data_","0f9fc6a3":"## _Modeling_","9b2cdd34":"- _As the value of SPI1 increases, there are more outcomes of 1 versus 0_","8ca8a68f":"- _A few of the multi-collinearity between the features is visible. Example - score1 with adj_score1, score2 with adj_score2_","7f13adb5":"## _Deep Learning Model_"}}