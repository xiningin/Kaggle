{"cell_type":{"c952b0c8":"code","523aabe0":"code","3fb085b4":"code","b24734c0":"code","77cf0b0b":"code","102e03fb":"code","52c4ee49":"code","6313d3f0":"code","5809a20b":"code","44b4acd0":"code","1022d880":"code","48632895":"code","3f470c9e":"markdown","b42ac3d4":"markdown","f5f44d2c":"markdown"},"source":{"c952b0c8":"import numpy as np\nimport pandas as pd\n\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport random\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score,accuracy_score, confusion_matrix\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport gc\nimport pickle","523aabe0":"sample = pd.DataFrame()\nsample['target'] = [random.randint(0,1) for i in range(10000)]\nsample['onehot1'] = [random.randint(0,1) for i in range(10000)]\nsample['onehot2'] = [random.randint(0,1) for i in range(10000)]\nsample['onehot3'] = [random.randint(0,1) for i in range(10000)]\nsample['Continuous'] = [np.random.randn() for i in range(10000)]\n#target\u304c1\u306e\u6642\u3001\u6b63\u898f\u4e71\u6570\u306b1\u3092\u8db3\u3059\nsample.loc[sample[sample['target']==1].index,'Continuous'] = sample.loc[sample[sample['target']==1].index,'Continuous']+1\n#sample['category1'] = [random.randint(0,3) for i in range(10000)]\n#sample['category2'] = [random.randint(0,3) for i in range(10000)]","3fb085b4":"X = sample.drop(['target'], axis=1)\ny = sample['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8, random_state=123)","b24734c0":"#lightgbm\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u63a2\u7d22\u7bc4\u56f2\u6307\u5b9a\nparams_test ={'num_leaves': sp_randint(6, 100), \n             #'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             #'subsample': sp_uniform(loc=0.2, scale=0.8), \n             #'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n             'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7],\n             'n_estimators':sp_randint(100,1000),\n             'learning_rate':[0.0001,0.0005,0.001,0.005,0.01,0.05,0.1],\n             'feature_fraction': [0.1,0.5], #set=, subsample= will be ignored\n             'bagging_fraction': [0.1,0.5], #set=,subsample= will be ignored\n             'min_data_in_leaf': sp_randint(100, 500), #set=, min_child_samples=\n            }\n\n\n#\u6700\u9069\u5316\u5168\u4f53\u306e\u8a2d\u5b9a\nopt_params = {\n    'seed' : None, #314\n    'nfolds' : 5,\n    'early_stopping_rounds':100, #for 1 search\n    'searchpoints':10, #Number of Hyperparameter search\n}","77cf0b0b":"def lgbm_hp_opt(X_train,y_train,params_test,opt_params):\n    \n    #train\u3068valid\u306b\u5206\u3051\u308b\n    X_train_opt, X_valid, y_train_opt, y_valid = train_test_split(X_train, \n                                                                  y_train,train_size=0.8, \n                                                                  random_state=opt_params['seed'])\n    \n    clf = lgb.LGBMClassifier(objective='binary', random_state=opt_params['seed'],\n                             silent=True, metric='logloss', n_jobs=None)\n    \n    #\u30d5\u30a3\u30c3\u30c6\u30f3\u30b0\u95a2\u9023\u306e\u6307\u5b9a\n    fit_params={\"early_stopping_rounds\":opt_params[\"early_stopping_rounds\"], \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_valid,y_valid)],\n            'eval_names': ['valid'],\n            #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n            'verbose': 100,\n            'categorical_feature': 'auto'}\n    \n    gs = RandomizedSearchCV(estimator=clf,\n                            param_distributions=params_test, \n                            n_iter=opt_params['searchpoints'],\n                            scoring='roc_auc',\n                            cv=opt_params['nfolds'],\n                            refit=True,\n                            random_state=opt_params['seed'],\n                            verbose=True)\n\n    gs.fit(X_train_opt, y_train_opt, **fit_params)\n    print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))\n    \n    return gs","102e03fb":"gs = lgbm_hp_opt(X_train,y_train,params_test,opt_params)","52c4ee49":"#predict_train\u306b\u5408\u3046\u3088\u3046\u306bdict\u3092\u7de8\u96c6\u3059\u308b\nparams = gs.best_params_.copy()\nparams['objective'] = 'binary'\nparams['metric'] = 'binary_logloss'\nparams['verbosity'] = -1\ndisplay(params)","6313d3f0":"#with open('lgbm_best_params.pickle',mode=\"wb\") as f:\n#    pickle.dump(params, f)\n\npd.to_pickle(params,'..\/output\/lgbm_params.pickle')\n#train\u306enotebook\u306b\u304a\u3044\u3066\u3001params\u3092\u8aad\u307f\u8fbc\u3093\u3060\u3089\u4ee5\u4e0b\u3092\u5b9f\u884c\u3059\u308b\n#n_estimators = params['n_estimators']\n#del params['n_estimators']","5809a20b":"def lightgbm_train(X_train, y_train, X_test, y_test,params, n_estimators=10000,NFOLDS=5):\n    folds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=6785)\n    columns = X_train.columns\n    splits = folds.split(X_train, y_train)\n    y_preds_test = np.zeros(X_test.shape[0]) #\u30c6\u30b9\u30c8\u306e\u4e88\u6e2c\u5024\u3092\u683c\u7d0d\u3059\u308b\u5909\u6570\n    y_oof = np.zeros(X_train.shape[0]) #out of folds \u7528\u306earray\n    #y_oof_proba = np.zeros(X_train.shape[0])\n    score = 0\n\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = columns\n\n    for fold_n, (train_index, valid_index) in enumerate(splits):\n        print(f'Fold{fold_n+1}:')\n        X_train_fold, X_valid = X_train[columns].iloc[train_index], X_train[columns].iloc[valid_index]\n        y_train_fold, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n\n        dtrain = lgb.Dataset(X_train_fold, label=y_train_fold)\n        dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n        clf = lgb.train(params, dtrain, n_estimators, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500) #valid_sets\u306e\u4f7f\u3044\u65b9\b\u304c\u6b63\u3057\u3044\u304b\uff1f\n        # https:\/\/qiita.com\/d_desuyon\/items\/807e01311ad08570ee78 valid_sets\u304c\u3053\u308c\u306e\u4f7f\u3044\u65b9\u3068\u9055\u3046\n        feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n\n        y_pred_valid = clf.predict(X_valid)\n        #y_pred_valid_proba = clf.predict_proba(X_valid)\n        y_oof[valid_index] = y_pred_valid #\u5404fold\u306evalid\u30c7\u30fc\u30bf\u306e\u4e88\u6e2c\u5024\u306e\u78ba\u7387\u3092\u683c\u7d0d\u3059\u308b\n        #y_oof_proba[valid_index] = y_pred_valid_proba\n\n        print(' ')\n        print('-------------------------')\n        print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n        print('-------------------------')\n        print(' ')\n        score += roc_auc_score(y_valid, y_pred_valid) \/ NFOLDS\n        y_preds_test += clf.predict(X_test) \/ NFOLDS #\u30c6\u30b9\u30c8\u306e\u4e88\u6e2c\u5024\u3092\u4ee3\u5165\n\n        del X_train_fold, X_valid, y_train_fold, y_valid\n        gc.collect()\n\n    test_auc = roc_auc_score(y_test, y_preds_test)\n    print('-------------------------')\n    print(f'test_auc:{test_auc}')\n    print(f\"Mean AUC = {score}\") #valid\u306e\n    print(f\"Out of folds AUC = {roc_auc_score(y_train, y_oof)}\") #\u5e73\u5747\u3067\u306f\u306a\u3044\n\n    #precision, recall\n    valid_true = y_train \n    valid_pred = y_oof\n    test_true = y_test\n    test_pred = y_preds_test\n    print('\\nPrecision:')\n    print(f'Valid Presicion:{precision(valid_true,valid_pred)}')\n    print(f'Test Presicion:{precision(test_true,test_pred)}')\n    print('\\nRecall:')\n    print(f'Valid Recall:{recall(valid_true,valid_pred)}')\n    print(f'Test Recall:{recall(test_true,test_pred)}')\n\n\n    \"\"\"\u4e0d\u6b63\u89e3\u30c7\u30fc\u30bf\u306e\u8aa4\u5dee\uff08\u6b8b\u5dee\uff09\u3092\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3067\u30d7\u30ed\u30c3\u30c8\u3059\u308b\"\"\"\n    Falsedataplot(valid_true,valid_pred,bins=20)\n    \n    \"\"\" \u6df7\u540c\u884c\u5217(confusion matrix)\u3092\u30d7\u30ed\u30c3\u30c8\u3059\u308b\"\"\"\n    conf_m = Confusion_matrix_plot(valid_true, valid_pred)\n    print(conf_m)\n\n\n    return feature_importances, score, y_preds_test, test_auc,conf_m\n\ndef precision(y_true,y_preds):\n    \"\"\"\u9069\u5408\u7387 (Precision) \u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\"\"\"\n    y_pred_conv = np.where(y_preds > 0.5, 1, 0)\n    metric = precision_score(y_true, y_pred_conv)\n    return metric\n\ndef recall(y_true,y_preds):\n    \"\"\"\u518d\u73fe\u7387 (Recall) \u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\"\"\"\n    y_pred_conv = np.where(y_preds > 0.5, 1, 0)\n    metric = recall_score(y_true, y_pred_conv)\n    return metric\n\ndef Falsedataplot(y_true,y_preds,bins=20):\n    \"\"\"\u4e0d\u6b63\u89e3\u30c7\u30fc\u30bf\u306e\u8aa4\u5dee\uff08\u6b8b\u5dee\uff09\u3092\u30d7\u30ed\u30c3\u30c8\u3059\u308b\u95a2\u6570\"\"\"\n    df = pd.DataFrame()\n    df['true_label'] = y_true\n    df['pred'] = y_preds\n    df['pred_label'] = np.where(y_preds>0.5,1,0)\n    df['residual'] = y_true - y_preds\n    df = df.reset_index(drop=True)\n    tmp = df.query('true_label != pred_label')['residual']\n\n    #\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u30d7\u30ed\u30c3\u30c8\n    #\u30de\u30a4\u30ca\u30b9\u5074\u306f0\u30921\u3068\u9593\u9055\u3048\u305f\u30c7\u30fc\u30bf\u3067\u3001\u30d7\u30e9\u30b9\u5074\u306f1\u30920\u3068\u9593\u9055\u3048\u305f\u30c7\u30fc\u30bf\n    plt.hist(tmp,bins=bins)\n    plt.show()\n\ndef Confusion_matrix_plot(y_true, y_preds):\n    y_preds_label = np.where(y_preds>0.5,1,0)\n    conf_m = pd.DataFrame(confusion_matrix(y_true,y_preds_label),\n             index=['Actual_False','Actual_True'],columns=['Predicted_False','Predicted_True'])\n    return conf_m","44b4acd0":"n_estimators = params['n_estimators']\ndel params['n_estimators']","1022d880":"NFOLDS = 5\nfeature_importances, score, y_preds_test,test_auc,y_oof = lightgbm_train(X_train,y_train, \n                                                                         X_test, y_test, params, NFOLDS)","48632895":"feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(NFOLDS)]].mean(axis=1)\n#feature_importances.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(10, 10))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance average');","3f470c9e":"## Run","b42ac3d4":"## importance_plot","f5f44d2c":"## Preprocessing"}}