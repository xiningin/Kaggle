{"cell_type":{"ed2c1345":"code","d4595e3b":"code","5316af3e":"code","30872620":"code","fd51272f":"code","244872c3":"code","789d6751":"code","292bfa8d":"code","1c09e39a":"code","95a6ffa7":"code","8d50e49c":"code","f8406199":"code","b78d0127":"code","2cfb65a7":"code","25db05f1":"code","76597116":"code","a6440c73":"code","ffe5e467":"code","d7c4556e":"code","18d87178":"code","ec39276b":"code","4b738134":"code","749a9836":"code","48151bc9":"code","35bb98da":"code","87a9ee3d":"code","facbb326":"code","561ec8b7":"code","b004f500":"code","eb4046bb":"code","f00b6352":"code","68d004c6":"code","f90b8d87":"code","5d578c68":"code","2a919671":"code","c8d97b88":"code","c144ef65":"code","4aa63f03":"code","76e0e649":"code","6c1cdf98":"code","fcf5dd07":"code","982dba19":"code","45daf121":"code","6ededb35":"code","951b4cfb":"code","a2173c88":"code","7305d2b9":"code","3efc416e":"code","09c6288a":"markdown","617171f5":"markdown","69202817":"markdown","9684c5d5":"markdown","d39164b1":"markdown","bf14bd93":"markdown","03ae9a5d":"markdown","6f9e1081":"markdown","b61c4cee":"markdown","4becb528":"markdown","68c69ad6":"markdown","bed51320":"markdown","6f72ea45":"markdown","8b5e9bde":"markdown","7a8feff6":"markdown","1bd9b8d6":"markdown","8d272d2b":"markdown"},"source":{"ed2c1345":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4595e3b":"#Data collection\ngold_data = pd.read_csv('\/kaggle\/input\/gold-price-data\/gld_price_data.csv')","5316af3e":"gold_data.head()","30872620":"gold_data['Next_GLD'] = gold_data['GLD'].shift(-1)","fd51272f":"gold_data.head()","244872c3":"gold_data = gold_data.drop('Date', axis =1).pct_change()","789d6751":"gold_data.head()","292bfa8d":"gold_data.dropna(axis = 0, inplace=True)","1c09e39a":"gold_data.head()","95a6ffa7":"gold_data.shape","8d50e49c":"correlation = gold_data.corr()","f8406199":"#a heatmap of the correlation\nplt.figure(figsize=(8,8))\nsns.heatmap(correlation, cbar = True, square=True, fmt='.1f', annot = True, annot_kws = {'size': 8}, cmap='Blues' )","b78d0127":"# correlation values\nprint(correlation['Next_GLD'])","2cfb65a7":"X = gold_data.drop(['Next_GLD'], axis = 1)\nY = gold_data['Next_GLD']","25db05f1":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1 , shuffle=False)","76597116":"print(X.shape, X_test.shape, X_train.shape)","a6440c73":"X_train.tail()","ffe5e467":"X_test.tail()","d7c4556e":"regressor = RandomForestRegressor(n_estimators=100)","18d87178":"regressor.fit(X_train, Y_train)","ec39276b":"Y_train_prediction = regressor.predict(X_train)","4b738134":"print('r2 score:', metrics.r2_score (Y_train, Y_train_prediction))","749a9836":"Y_test_prediction = regressor.predict(X_test)\nprint('r2 score:', metrics.r2_score (Y_test, Y_test_prediction))","48151bc9":"sns.scatterplot(Y_test, Y_test_prediction)","35bb98da":"X","87a9ee3d":"column_names = X.columns\nfor i in range(1,8):\n  for col in column_names:\n    col_name = col + str(i)\n    X[col_name] = X[col].shift(i)","facbb326":"X.head()","561ec8b7":"X.isnull().sum()","b004f500":"X.dropna(axis = 0, inplace=True)","eb4046bb":"Y = Y.iloc[7:]","f00b6352":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, shuffle = False, test_size = 0.2)","68d004c6":"print(X_train.shape, X_test.shape, X.shape)","f90b8d87":"regressor2 = RandomForestRegressor(n_estimators=300)","5d578c68":"regressor2.fit(X_train, Y_train)","2a919671":"Y_test_predict = regressor2.predict(X_test)","c8d97b88":"#r2 evaluation\nmetrics.r2_score(Y_test, Y_test_predict)","c144ef65":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.preprocessing import StandardScaler\nfrom datetime import datetime","4aa63f03":"gold_data = pd.read_csv('\/kaggle\/input\/gold-price-data\/gld_price_data.csv')\ngold_data.head()","76e0e649":"cols = list(gold_data)[1:] #Variables for training","6c1cdf98":"df_for_training = gold_data[cols].astype(float)","fcf5dd07":"df_for_plot=df_for_training.tail(5000)\ndf_for_plot.plot.line()","982dba19":"# normalize the dataset\nscaler = StandardScaler()\nscaler = scaler.fit(df_for_training)\ndf_for_training_scaled = scaler.transform(df_for_training)","45daf121":"pd.DataFrame(df_for_training_scaled).plot.line()","6ededb35":"trainX = []\ntrainY = []\nn_future = 1 #predicting the gold price in the next day\nn_past = 21 #using the last 21 days to predict\n\n#Reformat input data into a shape: (samples x timesteps x features)\nfor i in range(n_past, len(df_for_training_scaled)):\n    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n    trainY.append(df_for_training_scaled[i:i + n_future, 1]) #Gold price is in the second column","951b4cfb":"trainX, trainY = np.array(trainX), np.array(trainY)\n\nprint(trainX.shape)\nprint(trainY.shape)","a2173c88":"#training the model\nmodel = Sequential()\nmodel.add(LSTM(64, activation = 'relu', input_shape = (trainX.shape[1], trainX.shape[2]), return_sequences = True))\nmodel.add(LSTM(32, activation = 'relu', return_sequences = False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(trainY.shape[1]))\nmodel.compile(optimizer = 'adam', loss = 'mse')\nmodel.summary()","7305d2b9":"history = model.fit(trainX, trainY, epochs=40, batch_size=23, validation_split=0.1, verbose=1)\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.legend()","3efc416e":"from pandas.tseries.holiday import USFederalHolidayCalendar\nfrom pandas.tseries.offsets import CustomBusinessDay\nus_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n#Remember that we can only predict one day in future as our model needs 5 variables\n#as inputs for prediction. We only have all 5 variables until the last day in our dataset.\nn_past = 16\nn_days_for_prediction=15  #let us predict past 15 days\ntrain_dates = pd.to_datetime(gold_data['Date'])\n\npredict_period_dates = pd.date_range(list(train_dates)[-n_past], periods=n_days_for_prediction, freq=us_bd).tolist()\nprint(predict_period_dates)\n\n#Make prediction\nprediction = model.predict(trainX[-n_days_for_prediction:]) #shape = (n, 1) where n is the n_days_for_prediction\n\n#Perform inverse transformation to rescale back to original range\n#Since we used 5 variables for transform, the inverse expects same dimensions\n#Therefore, let us copy our values 5 times and discard them after inverse transform\nprediction_copies = np.repeat(prediction, df_for_training.shape[1], axis=-1)\ny_pred_future = scaler.inverse_transform(prediction_copies)[:,0]\n\n\n# Convert timestamp to date\nforecast_dates = []\nfor time_i in predict_period_dates:\n    forecast_dates.append(time_i.date())\n    \ndf_forecast = pd.DataFrame({'Date':np.array(forecast_dates), 'GLD':y_pred_future})\ndf_forecast['Date']=pd.to_datetime(df_forecast['Date'])\n\n\noriginal =gold_data[['Date', 'GLD']]\noriginal['Date']=pd.to_datetime(original['Date'])\n\n\nsns.lineplot(original['Date'], original['GLD'])\nsns.lineplot(df_forecast['Date'], df_forecast['GLD'])","09c6288a":"Since there are NaN values in the first 7 rows, let's remove the first 7 rows from X and Y and then train the model on the new dataset.","617171f5":"In this markdown we want to see if the next day price change in gold can be predicted from today's price change in gold and some other assets. ","69202817":"We barely see any correlation between Next_GLD percentage change and other percentage changes. ","9684c5d5":"In order to train an LSTM network, the input data should be in number of samples X timesteps X number of features format. For this predictive system let's use the past 21 days to predict the next day gold price.","d39164b1":"# Model training\n\nRandom forest","bf14bd93":"# Splitting the features and labels","03ae9a5d":"Let's see if the next day gold price change can be predicted with the change in gold price and other asset today.","6f9e1081":"Let's Evaluate the model","b61c4cee":"The closer the value of R square to 1, the better the performance of the model. However, it should be noted that the prediction is on the data that is used for training as well. We should see how this model does with the test dataset.","4becb528":"Let's see if having a history of price changes as features helps with the prediction. Let's prepare the dataset first.","68c69ad6":"One of the common mistakes in developing predictive systems for financial data is to use random split into train and test datasets to train the model. This should be avoided as for financial data time is of importance. So in using the train_test_split function we should assign the shuffle option as False. ","bed51320":"Performanc has improved but further investigation is needed. further. ","6f72ea45":"For each asset, let's have the past 7 values as features.","8b5e9bde":"Some other notebooks that have looked into the correlation between the price of these assests have observed strong correlation between silver and gold price for the same day price movement. Let's see if there are any correlations between the next day price change and today's price change for the assets.","7a8feff6":"Let's use multivariate LSTM to see if gold price prediction is possible.","1bd9b8d6":"# Multivariate LSTM","8d272d2b":"R2 compares the fit of the chosen model with that of a horizontal straight line (the null hypothesis). If the chosen model fits worse than a horizontal line, then R2 is negative. Note that R2 is not always the square of anything, so it can have a negative value without violating any rules of math. R2 is negative only when the chosen model does not follow the trend of the data, so fits worse than a horizontal line."}}