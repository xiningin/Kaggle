{"cell_type":{"5570a29e":"code","2236b0bb":"code","11806566":"code","88f930f4":"code","46434808":"code","d155e6d2":"code","295dffae":"code","29f2d07b":"code","df131d66":"code","fbc25687":"code","f21f9732":"code","f348bb8f":"code","5c435df8":"code","e13b476c":"code","efc10d33":"code","6c6cdc3b":"code","fb61699f":"code","e79f1abb":"code","c1beae28":"code","171700b9":"code","1555ca39":"code","c0caf3ed":"code","1d12fbf0":"code","1aeada85":"markdown","99a51caf":"markdown","efd04f74":"markdown","3a66e37a":"markdown","61dd0044":"markdown","68b823b9":"markdown","78437173":"markdown","33f97cc4":"markdown","425c7a95":"markdown","d5b7d05b":"markdown","9fbf3c7a":"markdown","144c9689":"markdown","495b5703":"markdown","c71f2cbf":"markdown","c2a77c24":"markdown","e8186a0f":"markdown"},"source":{"5570a29e":"import numpy as np\nimport sklearn\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","2236b0bb":"data = pd.read_csv(\"..\/input\/heart.csv\")","11806566":"# Preprocess the data\ndata.replace('?',-99999, inplace=True)\nprint(data.axes)\n\nprint(data.columns)","88f930f4":"#visualize and explore the data\nprint(data.loc[10])\n\n# Print the shape of the dataset\nprint(data.shape)","46434808":"#describing the data\nprint(data.describe())","d155e6d2":"#Plotting the data\ndata.hist(figsize=(15,15))\nplt.show()","295dffae":"#scattering the plot\nscatter_matrix(data, figsize=(20,20))\nplt.show()","29f2d07b":"# Correlation matrix\ncorrmat = data.corr()\nplt.figure(figsize=(15,15))\nsns.heatmap(corrmat,cmap='viridis',annot=True,linewidths=0.5,)","df131d66":"# Get all the columns from the dataFrame\ncolumns = data.columns.tolist()\n\n# Filter the columns to remove data we do not want\ncolumns = [c for c in columns if c not in [\"target\", \"chol\", \"fbs\", \"restecg\", \"testbps\"]]\n\n# Store the variable we'll be predicting on\ntarget = \"target\"\n\nX = data[columns]\ny = data[target]\n\n# Print shapes\nprint(X.shape)\nprint(y.shape)","fbc25687":"print(X.loc[26])\nprint(y.loc[26])","f21f9732":"#Creating X and y datasets for training\n#from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X,y, test_size = 0.2)","f348bb8f":"#Specify the testing option\n\nscoring = 'accuracy'\n\nprint(X_train.shape, X_test.shape)\nprint(y_train.shape, y_test.shape)","5c435df8":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score","e13b476c":"# Define models to train\n\n\nmodels = []\nmodels.append(('KNN', KNeighborsClassifier(n_neighbors = 3)))\nmodels.append(('NaiveB', GaussianNB()))\nmodels.append(('CART', DecisionTreeClassifier(max_depth=5)))\nmodels.append(('ADA', AdaBoostClassifier()))\nmodels.append(('RFC', RandomForestClassifier(max_depth=10, n_estimators=40)))","efc10d33":"# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=5)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","6c6cdc3b":"from sklearn.model_selection import StratifiedKFold\nfrom yellowbrick.model_selection import CVScores\n\n_, ax = plt.subplots()\n\ncv = StratifiedKFold(5)\n\noz = CVScores(RandomForestClassifier(max_depth=10, n_estimators = 40), ax=ax, cv=cv, scoring= 'accuracy')\noz.fit(X,y)\noz.poof()","fb61699f":"# Compare Algorithms\nfig = plt.figure(figsize = (18,18))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results, widths = 0.6)\nax.set_xticklabels(names)\nplt.show()","e79f1abb":"from sklearn.ensemble import VotingClassifier\n\nensemble = VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1)\n\nensemble.fit(X_train, y_train)\n\npredictions = ensemble.score(X_test, y_test)*100\n\n\nprint(\"The Voting Classifier Accuracy is: \", predictions)","c1beae28":"for name, model in models:\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    print(name)\n    print(accuracy_score(y_test, predictions)*100)\n    print(classification_report(y_test, predictions))","171700b9":"from sklearn.metrics import  confusion_matrix\npredict = model.predict(X_test)\nprint(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(y_test, predict))\nprint('\\n')\n\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, predict)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","1555ca39":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(max_depth=5)\nmodel.fit(X_train, y_train)\naccuracy = model.score(X_test, y_test)","c0caf3ed":"estimator = model\nfeature_names = [i for i in X_train.columns]\n\ny_train_str = y_train.astype('str')\ny_train_str[y_train_str == '0'] = 'no Disease'\ny_train_str[y_train_str == '1'] = 'Disease'\ny_train_str = y_train_str.values","1d12fbf0":"from sklearn.tree import export_graphviz #plot tree\nimport graphviz\nexport_graphviz(estimator, out_file='tree2.dot', \n                feature_names = feature_names,\n                class_names = y_train_str,\n                rounded = True, proportion = True, \n                label='root',\n                precision = 2, filled = True)\nwith open(\"tree2.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)","1aeada85":"Visualize a decision tree using Python export_graphviz.","99a51caf":"Check the missing value and replace with (-99999), so that python will ignore","efd04f74":"Plot the Histogram of each attribute","3a66e37a":"Plot the scatter matrix to look more deep for the relationship between attributes","61dd0044":"This step is very important as we will spit our data into the training and testing to check the accuracy and for this we will use (model selection) library. When you\u2019re working on a model and want to train it, you obviously have a dataset. But after training, we have to test the model on some test dataset. To do this we will split the dataset into two sets, one for training and the other for testing; and you do this before you start training your model.","68b823b9":"In this step we will import all the algorithms we want to compare and then check the result of each algo for 5 Fold cross validation.","78437173":"In this step we will do the Algorithms comparison using box plot visualization","33f97cc4":"Assign the data in two variables X and y, and from correlation matrix we got the idea of all the necessary attributes.","425c7a95":"Now we will look at the confusion matrix to evaluate the accuracy of a classification.","d5b7d05b":"Import the csv file using pandas","9fbf3c7a":"Visualize the random row and shape of dataset","144c9689":"Now we will make predictions on validation sheet, we will look at the accuracy score and classification report which is consisting of many important parameters","495b5703":"Let's see the result of all 5 KFolds for Random Forest","c71f2cbf":"In this step we will plot the correlation matrix to see the correlation between attributes. This also help us in determining that which attributes have high correlation and then we can decide which attribute is important for us. In Python the correlation values lie between (-1 and 1). There are two key components of a correlation value:\n\nmagnitude \u2013 The larger the magnitude (closer to 1 or -1), the stronger the correlation.\nsign \u2013 If negative, there is an inverse correlation. If positive, there is a regular correlation.","c2a77c24":"1.\tVoting Classifier -: The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses. ","e8186a0f":"Now look at the stastical values of Dataset"}}