{"cell_type":{"ce936f23":"code","6bcc5f54":"code","ee7a0eaa":"code","4734f841":"code","e3ee8fa6":"code","b073b77a":"code","7911a579":"code","cd9a4b6e":"code","6f923427":"code","81c5bc19":"code","8e1ada8a":"code","a9999b3c":"code","5ce763ef":"code","945f5a05":"code","c6bb58dc":"code","f22fcab5":"code","96d346ef":"code","4af4b512":"code","7d3c945c":"code","44bc72f1":"code","d70751ff":"code","815a93e7":"code","4c182cc3":"code","7841cee4":"code","c4cd645c":"code","224963cd":"code","f5c2bd7f":"code","5c49a480":"code","4f4ebde9":"code","75129b49":"code","ba1257d3":"code","74ca002f":"code","46df3837":"code","4851e19e":"markdown","cc62b534":"markdown","e384a5d6":"markdown","74e5b850":"markdown","0624d010":"markdown","514edc7f":"markdown","02cadebe":"markdown","7e3844cd":"markdown","995b8668":"markdown","b9a2a838":"markdown","704ed7ed":"markdown"},"source":{"ce936f23":"!cp ..\/input\/lightautoml-framework-lama\/PyMeeus-0.5.11.tar.gz.txt PyMeeus-0.5.11.tar.gz && pip install PyMeeus-0.5.11.tar.gz \n!cp ..\/input\/lightautoml-framework-lama\/efficientnet_pytorch-0.7.1.tar.gz.txt efficientnet_pytorch-0.7.1.tar.gz && pip install efficientnet_pytorch-0.7.1.tar.gz\n!cp ..\/input\/lightautoml-framework-lama\/json2html-1.3.0.tar.gz.txt json2html-1.3.0.tar.gz && pip install json2html-1.3.0.tar.gz\n!cp ..\/input\/lightautoml-framework-lama\/log_calls-0.3.2.tar.gz.txt log_calls-0.3.2.tar.gz && pip install log_calls-0.3.2.tar.gz\n!cp ..\/input\/lightautoml-framework-lama\/pyperclip-1.8.2.tar.gz.txt pyperclip-1.8.2.tar.gz && pip install pyperclip-1.8.2.tar.gz\n!rm -rf *.tar.gz\n!pip install --no-index --find-links=..\/input\/lightautoml-framework-lama lightautoml","6bcc5f54":"import numpy as np\nimport pandas as pd\npd.set_option('max_rows', 300)\npd.set_option('max_columns', 300)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfrom collections import ChainMap\nfrom joblib import Parallel, delayed\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import KFold, train_test_split\n\n# LightAutoML presets, task and report generation\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom lightautoml.report.report_deco import ReportDeco\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt, rcParams\nrcParams.update({'font.size': 22})","ee7a0eaa":"INPUT_PATH = '..\/input\/optiver-realized-volatility-prediction\/'\nN_THREADS = 4","4734f841":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])\/(df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_mean_price(df):\n    mp = (df['bid_price1'] + df['ask_price1']) \/ 2\n    return mp\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))","e3ee8fa6":"def calc_array_feats(arr, prefix, q_cnt = 10, diff = True):\n    if diff:\n        arr = np.diff(np.array(arr))\n    percs = np.linspace(0, 100, q_cnt + 1).astype(int)\n    cols = [prefix + '__P' + str(p)  for p in percs]\n    if len(arr) > 0:\n        vals = np.percentile(arr, percs)\n    else:\n        vals = [np.nan] * len(cols)\n    res = dict(zip(cols,vals))\n    return res","b073b77a":"def rmspe(y_true, y_pred, **kwargs):\n    return (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","7911a579":"def create_targ_enc_feature(col, targ_col, transform_func, tr_data, te_data, n_folds = 20, n_runs = 1):\n    # Test col is transformed by the whole train set aggregation\n    stock_id_target_trans = tr_data.groupby(col)[targ_col].agg(transform_func) \n    te_col_transformed = te_data[col].map(stock_id_target_trans)\n\n    # Train col can be transformed only inside CV not to overfit\n    # New values imputed with global train values\n    glob_val = transform_func(tr_data[col].values)\n    tr_col_transformed = np.repeat(0.0, tr_data.shape[0])\n    for i in range(n_runs):\n        kf = KFold(n_splits = n_folds, shuffle = True, random_state = 13)\n        for idx_train, idx_val in kf.split(tr_data):\n            target_trans = tr_data.iloc[idx_train].groupby(col)[targ_col].agg(transform_func) \n            tr_col_transformed[idx_val] += tr_data[col].iloc[idx_val].map(target_trans).fillna(glob_val) \/ n_runs\n        \n    return tr_col_transformed, te_col_transformed","cd9a4b6e":"def create_plot(stock_id, time_id, book_train, trade_train):\n    # Select time_id\n    bt = book_train.query('time_id == {}'.format(time_id))\n    bt['wap'] = calc_wap(bt)\n\n    trades = trade_train.query('time_id == {}'.format(time_id))\n    trades['seconds_in_bucket'] = np.maximum(trades['seconds_in_bucket'] - 1, 0)\n\n    # Combine trades prices\/timestamps with book prices\/timestamps\n    diffs = []\n    times = set(bt['seconds_in_bucket'].values)\n    for t in trades['seconds_in_bucket'].values:\n        d = 0\n        while t >= 0:\n            if t in times:\n                diffs.append(d)\n                break\n            else:\n                t -= 1\n                d += 1\n        if t == -1:\n            print('Negative!')\n\n    trades['seconds_in_bucket'] -= diffs\n\n    # Merged and calc the color (buy\/sell)\n    merged = pd.merge(bt, trades[['seconds_in_bucket', 'price', 'size']], on = 'seconds_in_bucket', how = 'left').dropna()\n    merged['diff_with_bid'] = merged['price'] - merged['bid_price1']\n    merged['diff_with_ask'] = merged['ask_price1'] - merged['price']\n    merged['color'] = (merged['diff_with_bid'] < merged['diff_with_ask']).astype(int).map({0: 'green', 1: 'red'})\n\n    fig = plt.figure(figsize = (60, 20))\n    plt.plot(bt['seconds_in_bucket'].values, bt['ask_price2'].values, 'b--', linewidth = 1, label = 'Ask price 2')\n    plt.plot(bt['seconds_in_bucket'].values, bt['ask_price1'].values, 'b', linewidth = 2, label = 'Ask price 1')\n    plt.plot(bt['seconds_in_bucket'].values, bt['wap'].values, 'm', linewidth = 1, label = 'WAP')\n    plt.plot(bt['seconds_in_bucket'].values, bt['bid_price1'].values, 'g', linewidth = 2, label = 'Bid price 1')\n    plt.plot(bt['seconds_in_bucket'].values, bt['bid_price2'].values, 'g--', linewidth = 1, label = 'Bid price 2')\n\n    fig.axes[0].fill_between(bt['seconds_in_bucket'].values, \n                             bt['bid_price1'].values, \n                             bt['ask_price1'].values, \n                             color = 'orange', \n                             alpha = 0.15)\n\n    fig.axes[0].fill_between(bt['seconds_in_bucket'].values, \n                             bt['bid_price2'].values, \n                             bt['bid_price1'].values, \n                             color = 'green', \n                             alpha = 0.25)\n\n    fig.axes[0].fill_between(bt['seconds_in_bucket'].values, \n                             bt['ask_price1'].values, \n                             bt['ask_price2'].values, \n                             color = 'blue', \n                             alpha = 0.15)\n\n\n    mask = (merged['color'] == 'green').values\n    plt.scatter(merged['seconds_in_bucket'].values[mask], \n                merged['price'].values[mask], \n                marker = '*', color = merged['color'].values[mask], \n                s = 500, label = 'Buy trades')\n    \n    mask = (merged['color'] == 'red').values\n    plt.scatter(merged['seconds_in_bucket'].values[mask], \n                merged['price'].values[mask], \n                marker = '*', color = merged['color'].values[mask], \n                s = 500, label = 'Sell trades')\n\n    plt.grid(True)\n    plt.legend()\n    plt.title('Stock_id = {}, time_id = {}'.format(stock_id, time_id))\n    plt.xlabel('seconds_in_bucket')\n    plt.ylabel('Price')\n    plt.show()","6f923427":"for stock_id in [0,1,2]:\n    # Read data\n    bt = pd.read_parquet(INPUT_PATH + \"book_train.parquet\/stock_id={}\".format(stock_id))\n    tt = pd.read_parquet(INPUT_PATH + \"trade_train.parquet\/stock_id={}\".format(stock_id))\n    \n    time_ids = bt['time_id'].value_counts().index.values[[0, -1]]\n    for time_id in time_ids:\n        create_plot(stock_id, time_id, bt, tt)","81c5bc19":"def create_features(stock_id, time_id, bt, trades, last_s):\n    q_cnt = 5\n    \n    bt = bt[bt['seconds_in_bucket'] >= last_s]\n    trades = trades[trades['seconds_in_bucket'] > bt['seconds_in_bucket'].min() + 1]\n    \n    # BOOK PART    \n    features_arr = [{\n        'rv_1': realized_volatility(bt['log_return']),\n        'rv_2': realized_volatility(bt['log_return2']),\n        'rv_mp': realized_volatility(bt['log_return_mean_price'])\n    }]\n    \n    for col in ['abs_wap_balance', 'wap_balance', 'price_spread', 'bid_spread', 'ask_spread',\n               'total_volume', 'abs_volume_imbalance', 'volume_imbalance']:\n        features_arr.append(calc_array_feats(bt[col].values, 'B_' + col, q_cnt, False))\n        \n    for col in ['seconds_in_bucket', 'bid_volume', 'ask_volume']:\n        features_arr.append(calc_array_feats(bt[col].values, 'B_' + col, q_cnt, True))\n    # ==========================================\n    \n    # TRADES PART ==========================================\n    trades['seconds_in_bucket'] = np.maximum(trades['seconds_in_bucket'] - 1, 0)\n\n    # Combine trades prices\/timestamps with book prices\/timestamps\n    diffs = []\n    times = set(bt['seconds_in_bucket'].values)\n    for t in trades['seconds_in_bucket'].values:\n        d = 0\n        while t >= 0:\n            if t in times:\n                diffs.append(d)\n                break\n            else:\n                t -= 1\n                d += 1\n\n    trades['seconds_in_bucket'] -= diffs\n    \n    features_arr.append(calc_array_feats(np.array(diffs), 'T_diffs', q_cnt, False))\n    vc_diffs = pd.Series(diffs).value_counts()\n    features_arr.append(calc_array_feats(vc_diffs.values, 'T_vc_diffs_values', q_cnt, False))\n    features_arr.append(calc_array_feats(vc_diffs.index.values, 'T_vc_diffs_index', q_cnt, False))\n    features_arr.append({'T_len_vc_diffs': len(vc_diffs)})\n    \n    for col in ['size', 'order_count']:\n        features_arr.append(calc_array_feats(trades[col].values, 'T_' + col, q_cnt, False))\n        \n    for col in ['seconds_in_bucket', 'price']:\n        features_arr.append(calc_array_feats(trades[col].values, 'T_' + col, q_cnt, True))\n    # ==========================================\n\n    # MERGED PART\n    merged = pd.merge(bt, trades[['seconds_in_bucket', 'price', 'size']], on = 'seconds_in_bucket', how = 'left').dropna()\n    merged['diff_with_bid'] = merged['price'] - merged['bid_price1']\n    merged['diff_with_ask'] = merged['ask_price1'] - merged['price']\n    merged['side'] = (merged['diff_with_bid'] < merged['diff_with_ask']).astype(int)\n    side = merged['side'].values\n    merged['diff_with_side_volume1'] = np.array([row[s] for row, s in zip(merged[['ask_size1', 'bid_size1']].values, side)]) - merged['size']\n    merged['diff_with_side_full_volume'] = np.array([row[s] for row, s in zip(merged[['ask_volume', 'bid_volume']].values, side)]) - merged['size']\n    \n    features_arr.append({\n        'M_cnt': len(merged),\n        'M_mean_side': np.mean(side)\n    })\n    \n    cside = np.cumsum(2 * side - 1)\n    features_arr.append(calc_array_feats(cside, 'M_cside', q_cnt, False))\n    \n    for col in ['diff_with_side_volume1', 'diff_with_side_full_volume', 'diff_with_bid', 'diff_with_ask']:\n        features_arr.append(calc_array_feats(merged[col].values, 'M_' + col, q_cnt, False)) \n      \n    # ==========================================\n    features = dict(ChainMap(*features_arr))\n    features = {str(last_s) + '_' + k: features[k] for k in features}\n    \n    features['#stock_id'] = stock_id\n    features['#time_id'] = time_id\n    features['#row_id'] = '{}-{}'.format(stock_id, time_id)\n\n    return features","8e1ada8a":"def calc_features_dataset_for_stock(stock_id, test_flg = False, debug = False):\n    part = 'train'\n    if test_flg:\n        part = 'test'\n        \n    bt = pd.read_parquet(INPUT_PATH + \"book_{}.parquet\/stock_id={}\".format(part, stock_id))\n    bt['wap'] = calc_wap(bt)\n    bt['log_return'] = log_return(bt['wap'])\n    bt['wap2'] = calc_wap2(bt)\n    bt['log_return2'] = log_return(bt['wap2'])\n    bt['mean_price'] = calc_mean_price(bt)\n    bt['log_return_mean_price'] = log_return(bt['mean_price'])\n    \n    bt['abs_wap_balance'] = abs(bt['wap'] - bt['wap2'])\n    bt['wap_balance'] = bt['wap'] - bt['wap2']\n    \n    bt['price_spread'] = 2 * (bt['ask_price1'] - bt['bid_price1']) \/ (bt['ask_price1'] + bt['bid_price1'])\n    bt['bid_spread'] = (bt['bid_price1'] - bt['bid_price2']) \/ bt['bid_price1']\n    bt['ask_spread'] = (bt['ask_price1'] - bt['ask_price2']) \/ bt['ask_price1']\n    \n    bt['total_volume'] = (bt['ask_size1'] + bt['ask_size2']) + (bt['bid_size1'] + bt['bid_size2'])\n    bt['bid_volume'] = bt['bid_size1'] + bt['bid_size2']\n    bt['ask_volume'] = bt['ask_size1'] + bt['ask_size2']\n    bt['abs_volume_imbalance'] = abs(bt['bid_volume'] - bt['ask_volume'])\n    bt['volume_imbalance'] = bt['bid_volume'] - bt['ask_volume']\n    \n    book_groups = bt.groupby('time_id')\n    trade_groups = pd.read_parquet(INPUT_PATH + \"trade_{}.parquet\/stock_id={}\".format(part, stock_id)).groupby('time_id')\n\n    feats_arr = []\n    bg_keys = book_groups.groups.keys()\n    tr_keys = set(trade_groups.groups.keys())\n    sample_trades_df = pd.DataFrame(columns = ['time_id', 'seconds_in_bucket', 'price', 'size', 'order_count'])\n    for time_id in tqdm(bg_keys):\n        arr = []\n        b_gr = book_groups.get_group(time_id)\n        for last_s in [600, 300]:\n            if time_id in tr_keys:\n                t_gr = trade_groups.get_group(time_id)\n            else:\n                t_gr = sample_trades_df.copy()\n            arr.append(create_features(stock_id, time_id, b_gr, t_gr, 600 - last_s))\n        feats_arr.append(dict(ChainMap(*arr)))\n        if debug:\n            break\n       \n    df = pd.DataFrame(feats_arr)\n    df = df[sorted(df.columns)].rename({'#stock_id': 'stock_id', '#time_id': 'time_id', '#row_id': 'row_id'}, axis = 1)\n    print('Stock {} ready'.format(stock_id))\n    return df","a9999b3c":"%%time\ndf = calc_features_dataset_for_stock(stock_id = 0, test_flg = False, debug = True)\ndf","5ce763ef":"def multiprocessed_df_creation(stock_ids, n_jobs = 4, test_flg = False, debug = False):\n    res_df = Parallel(n_jobs=n_jobs, verbose=1)(\n        delayed(calc_features_dataset_for_stock)(stock_id, test_flg, debug) for stock_id in stock_ids\n    )\n\n    res_df = pd.concat(res_df).reset_index(drop = True)\n    return res_df\n","945f5a05":"stock_ids = [0,1,2,3,4,5]\nmultiprocessed_df_creation(stock_ids = stock_ids, \n                           n_jobs = N_THREADS, \n                           test_flg = False, \n                           debug = True)","c6bb58dc":"train = pd.read_csv(INPUT_PATH + 'train.csv')\ntrain.head()","f22fcab5":"train_stock_ids = train.stock_id.unique()\ntrain_stock_ids","96d346ef":"%%time\ntrain_data = multiprocessed_df_creation(train_stock_ids, \n                                        n_jobs = N_THREADS, \n                                        test_flg = False, \n                                        debug = False)","4af4b512":"train_data = pd.merge(train, train_data, on = ['stock_id', 'time_id'], how = 'left')","7d3c945c":"train_data","44bc72f1":"train_data.shape","d70751ff":"test = pd.read_csv(INPUT_PATH + 'test.csv')\ntest.head()","815a93e7":"DEBUG = (test.shape[0] == 3)\nDEBUG","4c182cc3":"test_stock_ids = test.stock_id.unique()\ntest_stock_ids","7841cee4":"%%time\ntest_data = multiprocessed_df_creation(test_stock_ids, \n                                       n_jobs = N_THREADS, \n                                       test_flg = True, \n                                       debug = False)","c4cd645c":"test_data = pd.merge(test, test_data, on = ['stock_id', 'time_id', 'row_id'], how = 'left')\ntest_data","224963cd":"N_FOLDS = 10\nTIMEOUT = 24 * 3600\n\n# Default params for LGBM models\nlgbm_params = {\n      \"objective\": \"rmse\", \n      \"metric\": \"rmse\", \n      \"boosting_type\": \"gbdt\",\n      'early_stopping_rounds': 30,\n      'learning_rate': 0.01,\n      'lambda_l1': 1.0,\n      'lambda_l2': 1.0,\n      'feature_fraction': 0.8,\n      'bagging_fraction': 0.8,\n}","f5c2bd7f":"def create_additional_feats(tr_data, te_data):\n    for t_col in ['target', '0_rv_1', '300_rv_1']:\n        print(t_col)\n        for name, func in [('mean', np.mean), ('min', np.min), ('max', np.max)]:\n            print('\\t', name)\n            tr_col, te_col = create_targ_enc_feature('stock_id', t_col, func, tr_data, te_data, 20, 3)\n            tr_data['stock_id_enc_{}_{}'.format(name, t_col)] = tr_col\n            te_data['stock_id_enc_{}_{}'.format(name, t_col)] = te_col\n            \n    for d in [tr_data, te_data]:\n        d['0rv1_diff_300rv1'] = d['0_rv_1'] - d['300_rv_1']\n        d['0rv1_del_300rv1'] = d['0_rv_1'] \/ d['300_rv_1']\n    ","5c49a480":"if DEBUG:\n    tr_data, te_data = train_test_split(train_data, \n                                         test_size = 0.2, \n                                         random_state = 42)\n    print('Data splitted. Parts sizes: tr_data = {}, te_data = {}'\n                  .format(tr_data.shape, te_data.shape))","4f4ebde9":"def create_and_train_model(tr_data, te_data):\n    # Task setup - mse loss and mse metric. To optimize rmspe we use object weights for the loss (weight column)\n    task = Task('reg',)\n    tr_data['weight'] = 1 \/ tr_data['target'] ** 2\n    \n    # Columns roles setup\n    roles = {\n        'target': 'target',\n        'drop': ['row_id', 'time_id'],\n        'category': 'stock_id',\n        'weights': 'weight'\n    }\n    \n    # Train LightAutoML model\n    automl = TabularAutoML(task = task, \n                           timeout = TIMEOUT,\n                           cpu_limit = N_THREADS,\n                           general_params = {'use_algos': [['lgb', 'lgb_tuned', 'cb_tuned']]},\n                           reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS},\n                           tuning_params = {'max_tuning_time': 600},\n                           lgb_params = {'default_params': lgbm_params, 'freeze_defaults': True},\n                           verbose = 3\n                           )\n\n    oof_pred = automl.fit_predict(tr_data, roles = roles)\n    print('OOF prediction for tr_data:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))\n    \n    # Fast feature importances calculation\n    fast_fi = automl.get_feature_scores('fast')\n    fast_fi.set_index('Feature')['Importance'].head(100).plot.bar(figsize = (50, 10), grid = True)\n    \n    # Let's see how the final model looks like\n    print(automl.create_model_str_desc())\n    \n    # Test data prediction\n    te_pred = automl.predict(te_data)\n    print('Prediction for te_data:\\n{}\\nShape = {}'.format(te_pred, te_pred.shape))\n    \n    return oof_pred.data[:, 0], te_pred.data[:, 0], automl","75129b49":"if DEBUG:\n    create_additional_feats(tr_data, te_data)\n    \n    oof_pred, valid_pred, automl = create_and_train_model(tr_data, te_data)\n    \n    # Check scores\n    print('OOF RMSPE score = {:.5f}'.format(rmspe(tr_data['target'], oof_pred)))\n    print('TEST RMSPE score = {:.5f}'.format(rmspe(te_data['target'], valid_pred)))\n    \n    create_additional_feats(tr_data, test_data)\n    \n    test_pred = automl.predict(test_data)\n    submission = test_data[['row_id']]\n    submission['target'] = test_pred.data[:, 0]\n    submission.to_csv('submission.csv', index = False)","ba1257d3":"if not DEBUG:\n    create_additional_feats(train_data, test_data)\n    \n    oof_pred, test_pred, automl = create_and_train_model(train_data, test_data)\n    \n    # Check scores\n    print('OOF RMSPE score = {:.5f}'.format(rmspe(train_data['target'], oof_pred)))\n    \n    submission = test_data[['row_id']]\n    submission['target'] = test_pred\n    submission.to_csv('submission.csv', index = False)","74ca002f":"# Fast feature importances calculation\nfast_fi = automl.get_feature_scores('fast')\nfast_fi.set_index('Feature')['Importance'].head(100).plot.bar(figsize = (50, 10), grid = True)","46df3837":"# Let's see how the final model looks like\nprint(automl.create_model_str_desc())","4851e19e":"# Multiprocessed preprocessor wrapper","cc62b534":"# Generate full train","e384a5d6":"# Libraries imports","74e5b850":"# Functions for preprocess","0624d010":"# EDA and data visualization ","514edc7f":"# Global constants ","02cadebe":"# Offline LightAutoML installation ","7e3844cd":"# Bonus. Feature importances and model structure ","995b8668":"# Create LightAutoML model","b9a2a838":"# Feature engineering ","704ed7ed":"# Generate full test"}}