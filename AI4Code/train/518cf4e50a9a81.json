{"cell_type":{"161c46ee":"code","5d274a36":"code","d9f8b903":"code","0cbf7d46":"code","38bd21ff":"code","e83c3b9a":"code","37a77eb7":"code","2271c5ab":"code","1e864b26":"code","1b686993":"code","55ac7f1f":"code","4ea8b14f":"code","70c7b5ef":"code","8ce172b6":"code","922ba2d2":"code","a9cb0602":"code","b94b5afa":"code","dc6d6b51":"code","15ea8160":"code","f69f8960":"code","ecde0851":"code","a8416380":"code","4a569385":"code","9f2e4a28":"code","8c72e7e3":"code","5c17b976":"code","7e946dcc":"code","bfa759fe":"code","7e8b3d79":"code","f02218c7":"code","6bf8df12":"code","45bbc1a8":"code","c59bab7d":"code","363651c1":"code","c147b0d5":"code","2215ed1a":"code","a559a6dc":"code","75179320":"code","5994eede":"code","9a80c372":"code","bf9d5c90":"code","ee66596a":"code","a9b89e99":"code","080f0542":"code","38066c15":"markdown","19ba9092":"markdown","de424989":"markdown","82fc825a":"markdown","e9ef9501":"markdown","fb351cc4":"markdown","9a9a8fab":"markdown","d8c21ddc":"markdown","4dd88ce8":"markdown"},"source":{"161c46ee":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas import DataFrame, read_csv, concat, Series\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport datatable as dt\nfrom scipy import stats\nfrom cesium.featurize import featurize_time_series as ft\nfrom pandas.plotting import lag_plot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss as LL\nfrom sklearn.metrics import mean_squared_error as MSE, mean_absolute_error as MAE","5d274a36":"import sys\n!cp ..\/input\/rapids\/rapids.21.06 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","d9f8b903":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0cbf7d46":"%%time\n# load the data\ndata = pd.read_csv('..\/input\/ubiquant-market-prediction\/train.csv', \n                   chunksize=100000)","38bd21ff":"# using a chunk to analyze data patterns\nchunk_1 = data.read(200000)","e83c3b9a":"chunk_1.head(4)","37a77eb7":"print('Number of Rows: %s' % str((chunk_1.shape[0])))\nprint('Number of Columns: %s' % str((chunk_1.shape[1])))","2271c5ab":"chunk_1['target'].value_counts()","1e864b26":"chunk_1.info()","1b686993":"chunk_1.time_id.nunique()","55ac7f1f":"sns.set(rc={'figure.figsize':(11.7, 8.27)})\nsns.set_theme(style='darkgrid')\nsns.displot(x=chunk_1['target'][:40000], kde=True, color='grey')","4ea8b14f":"sns.displot(x=chunk_1['f_0'][:40000], kde=True, color='grey')","70c7b5ef":"# make a function to plot histograms\ndef plot_hist(x):\n    return chunk_1.hist(x)","8ce172b6":"plot_hist('f_66')","922ba2d2":"plot_hist('f_55')","a9cb0602":"plt.title('Scatterplot of two features')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.scatter(x=chunk_1['f_4'][:5000], y=chunk_1['f_5'][:5000], color='grey')","b94b5afa":"chunk_1.isnull().sum().to_numpy()","dc6d6b51":"print('Number of unique investment IDs: %s' % str(chunk_1['investment_id'].nunique()))","15ea8160":"print('Number of unique investment Time Stamps: %s' % str(chunk_1['time_id'].nunique()))","f69f8960":"plt.boxplot(x=chunk_1['target'][:10000])","ecde0851":"plt.title('KDE Density of Target Feature in Data')\nsns.kdeplot(x=chunk_1['target'], color='grey')","a8416380":"series = pd.read_csv('..\/input\/ubiquant-market-prediction\/train.csv', chunksize=100000, header=0, index_col=0,\nparse_dates=True, squeeze=True)\n\nlag_plot(series.read(5000))","4a569385":"from pandas.plotting import autocorrelation_plot\nautocorrelation_plot(series.read(10000))","9f2e4a28":"sns.lineplot(x=chunk_1.groupby('time_id')['investment_id'].nunique().index, \n             y=chunk_1.groupby('time_id')['investment_id'].nunique(), \n             color='blue')","8c72e7e3":"plt.subplot(4, 1, 1)\nplt.plot(chunk_1[\"f_4\"][:5000], label='feature 4')\nplt.legend()\nplt.subplot(4, 1, 2)\nplt.plot(chunk_1[\"f_5\"][:5000], label='feature 4')\nplt.legend()\nplt.subplot(4, 1, 3)\nplt.plot(chunk_1[\"f_6\"][:5000], label='feature 4')\nplt.legend()\nplt.subplot(4, 1, 4)\nplt.plot(chunk_1[\"f_7\"][:5000], label='feature 4')\nplt.legend()","5c17b976":"features = chunk_1.iloc[:, 4:].columns.tolist()\nlen(features)","7e946dcc":"features.target","bfa759fe":"%%time\n# make a pipleine\npipeline = Pipeline([('Scaler', StandardScaler()), ('PCA', PCA(n_components=5))])\npipeline.fit(chunk_1[features])\npca_components = pipeline.transform(chunk_1[features])","7e8b3d79":"var = pipeline.named_steps['PCA'].explained_variance_ratio_.sum() * 100","f02218c7":"labels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pipeline.named_steps['PCA'].explained_variance_ratio_ * 100)\n}","6bf8df12":"labels","45bbc1a8":"labels['color'] = 'Cluster'","c59bab7d":"# make a plot for PCA\nfig = px.scatter_matrix(\n    pca_components,\n    labels=labels,\n    dimensions=range(4),\n    color=chunk_1[\"target\"], \n    opacity=0.5\n)\n\nfig","363651c1":"X = chunk_1[['f_1', 'f_2', 'f_3', 'f_4', 'f_5', 'f_6', 'f_7', 'f_8', 'f_9', 'f_10']]\ny = chunk_1['target']","c147b0d5":"np.mean(chunk_1['f_2'])","2215ed1a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, \n                                                    random_state=42)","a559a6dc":"%%time\ngb = GradientBoostingRegressor(n_estimators=200)\ngb.fit(X_train, y_train)","75179320":"pred = gb.predict(X_test)\npred[:4]","5994eede":"# calculate the mean squared error\nMSE(y_test, pred)","9a80c372":"gb.score(X_train, y_train)","bf9d5c90":"#X = chunk_1.drop('target', axis=1)\n#y = chunk_1.target\n\n#X.shape, y.shape","ee66596a":"#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, \n #                                                   random_state=42)","a9b89e99":"# rf = GradientBoostingRegressor(n_estimators=200)\n# rf.fit(X_train, y_train)","080f0542":"# calculate loss\n#y_pred_rf = rf.predict(X_test)\n#MSE(y_test, y_pred_rf)","38066c15":"### Principcal Component Analysis\nWe will perform PCA on N number of dataset features for Dimensionality Reduction.","19ba9092":"## Load the Data\nThe dataset is much huge. For successfull loading of data in our memory, we will load a chunk of data that will contain first **100,000** samples.","de424989":"This was a starter....Work in progres...","82fc825a":"### Make a Lag Plot of Features","e9ef9501":"## EDA (Exploratory Data Analysis)","fb351cc4":"# Integrate RAPIDS With Kaggle","9a9a8fab":"## Applying Machine Learning Models\nThe dataset is a regression-based dataset, in which we have to predict values in future time. For time series, following models can be used:\n* Random Forest Regressor\n* Gradient Boosting Machine\n* Adaptive Boosting Regressor","d8c21ddc":"# Ubiquent Market Prediction\nIn this notebook, we will aim to make a **Time Series Forecasting Model** that will predict future returns, concered with **Ubiquent Market** ","4dd88ce8":"### Making Line Plots of N features\nWe randomly select 4 features from market data, and make line plots of those features. Each feature comprises of a different time series measurment."}}