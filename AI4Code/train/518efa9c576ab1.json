{"cell_type":{"7dadd942":"code","8aae9313":"code","f835bcdd":"code","44ffd275":"code","984e2304":"code","5dbd7a5d":"code","65b1f013":"code","f290df4a":"code","1bca12c4":"code","520f0bb4":"code","a273edad":"code","1e559c8e":"code","f3867221":"code","a55241f4":"code","39ea6d13":"code","96e5416e":"code","b6ea0b8e":"code","079e0db4":"code","84363625":"code","7ce1eb53":"code","85330cd4":"code","db925bfb":"code","1b2db103":"code","1f3e4991":"code","8d8b228d":"code","dbe8e298":"code","c7c559c3":"code","f7a44980":"code","06789898":"code","1a7621f6":"code","ca46eff3":"code","ed053a90":"code","aaf06279":"code","f8681875":"code","2cb2d4af":"code","087031a3":"code","5ff43110":"code","38882088":"code","97bb514e":"code","dd4704c1":"code","394c30d1":"code","4b4ddc33":"code","fbb68c3a":"code","80669e2d":"code","16ee8976":"code","98f945d1":"code","145d1cc5":"code","24f85df6":"code","89b6f999":"code","ac7f5cef":"code","56ca2ee2":"code","5582bb11":"code","067a3233":"code","5d79116f":"code","88117304":"code","3a214016":"code","a0b2070a":"code","ccc2c122":"code","82174216":"code","3dd84621":"code","778c084f":"code","b243ec2c":"code","b76ea8ce":"code","baf7abd1":"code","7bb8234e":"code","35956945":"code","2b9d0cb8":"code","978da924":"code","89092b6c":"code","8f7a2f63":"code","a294878e":"code","5c47a08b":"code","35684b5d":"code","db671c3f":"code","6cdd813f":"markdown","7a554eaa":"markdown","cdf0b682":"markdown","17c16a5d":"markdown","2c494131":"markdown","7bb8a4fa":"markdown","d906d698":"markdown","84efacf1":"markdown","2d92212e":"markdown","d3a83e02":"markdown","fbeccd30":"markdown","5505a9f8":"markdown","321a2291":"markdown","27a4e8dc":"markdown","e9337706":"markdown","fff7894f":"markdown","2183d5b0":"markdown","3705e2f8":"markdown","27bd9d01":"markdown","0639ce4e":"markdown","08c430cf":"markdown","c2dce4d2":"markdown","27fe7f27":"markdown","67743503":"markdown","12b8327b":"markdown","d2dbab2e":"markdown","eab9f9ee":"markdown","b33ade95":"markdown","82f3efc8":"markdown","baa2e2f7":"markdown","e9cb6c03":"markdown","1fd20163":"markdown","e67ccfed":"markdown","1c128a00":"markdown","b44dce94":"markdown","e2e9212a":"markdown","a8c5dc35":"markdown","87e9ed7a":"markdown","1396a22d":"markdown","22d03215":"markdown","f11eccb5":"markdown","de27ce9a":"markdown","b94bca5e":"markdown","0797a42a":"markdown","5070e730":"markdown","587af121":"markdown"},"source":{"7dadd942":"'''Importing Data Manipulattion Moduls'''\nimport numpy as np\nimport pandas as pd\n\n'''Seaborn and Matplotlib Visualization'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('bmh')                    \nsns.set_style({'axes.grid':False}) \n%matplotlib inline\n\n'''plotly Visualization'''\nimport plotly.offline as py\nfrom plotly.offline import iplot, init_notebook_mode\nimport plotly.graph_objs as go\ninit_notebook_mode(connected = True)\n\n'''Ignore deprecation and future, and user warnings.'''\nimport warnings as wrn\nwrn.filterwarnings('ignore', category = DeprecationWarning) \nwrn.filterwarnings('ignore', category = FutureWarning) \nwrn.filterwarnings('ignore', category = UserWarning) ","8aae9313":"'''Read in train and test data from csv files'''\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","f835bcdd":"'''Train and test data at a glance'''\ntrain.head()","44ffd275":"test.head()","984e2304":"'''Dimensions of train and test data'''\nprint('Dimensions of train data:', train.shape)\nprint('Dimensions of test data:', test.shape)","5dbd7a5d":"\"\"\"Let's check the columns names\"\"\"\ntrain.columns.values","65b1f013":"\"\"\"Let's merge the train and test data and inspect the data type\"\"\"\nmerged = pd.concat([train, test], axis=0, sort=True)\ndisplay(merged.dtypes.value_counts())\nprint('Dimensions of data:', merged.shape)","f290df4a":"'''Extracting numerical variables first'''\nnum_merged = merged.select_dtypes(include = ['int64', 'float64'])\ndisplay(num_merged.head(3))\nprint('\\n')\ndisplay(num_merged.columns.values)","1bca12c4":"'''Plot histogram of numerical variables to validate pandas intuition.'''\ndef draw_histograms(df, variables, n_rows, n_cols):\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=40,ax=ax,color = 'green',alpha=0.5, figsize = (40, 200))\n        ax.set_title(var_name, fontsize = 43)\n        ax.tick_params(axis = 'both', which = 'major', labelsize = 35)\n        ax.tick_params(axis = 'both', which = 'minor', labelsize = 35)\n        ax.set_xlabel('')\n    fig.tight_layout(rect = [0, 0.03, 1, 0.95])  # Improves appearance a bit.\n    plt.show()\n    \ndraw_histograms(num_merged, num_merged.columns, 19, 2)","520f0bb4":"'''Convert MSSubClass, OverallQual, OverallCond, MoSold, YrSold into categorical variables.'''\nmerged.loc[:,['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold']] = merged.loc[:,['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold']].astype('object')","a273edad":"'''Check out the data type after correction'''\nmerged.dtypes.value_counts()","1e559c8e":"'''Function to plot scatter plot'''\ndef scatter_plot(x, y, title, xaxis, yaxis, size, c_scale):\n    trace = go.Scatter(x = x,\n                        y = y,\n                        mode = 'markers',\n                        marker = dict(color = y, size=size, showscale = True, colorscale = c_scale))\n    layout = go.Layout(hovermode = 'closest', title = title, xaxis = dict(title = xaxis), yaxis = dict(title = yaxis))\n    fig = go.Figure(data = [trace], layout = layout)\n    return iplot(fig)\n\n'''Function to plot bar chart'''\ndef bar_plot(x, y, title, yaxis, c_scale):\n    trace = go.Bar(x = x,\n                   y = y,\n                   marker = dict(color = y, colorscale = c_scale))\n    layout = go.Layout(hovermode= 'closest', title = title, yaxis = dict(title = yaxis))\n    fig = go.Figure(data = [trace], layout = layout)\n    return iplot(fig)\n\n'''Function to plot histogram'''\ndef histogram_plot(x, title, yaxis, color):\n    trace = go.Histogram(x = x,\n                        marker = dict(color = color))\n    layout = go.Layout(hovermode = 'closest', title = title, yaxis = dict(title = yaxis))\n    fig = go.Figure(data = [trace], layout = layout)\n    return iplot(fig)","f3867221":"corr = train.corr()\nf, ax = plt.subplots(figsize=(15, 12))\nsns.heatmap(corr, linewidths=.5, vmin=0, vmax=1, square=True)","a55241f4":"k = 10 #number of variables for heatmap\ncols = corr.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","39ea6d13":"'''Sactter plot of GrLivArea vs SalePrice.'''\nscatter_plot(train.GrLivArea, train.SalePrice, 'GrLivArea vs SalePrice', 'GrLivArea', 'SalePrice', 10, 'Rainbow')","96e5416e":"'''Drop observations where GrLivArea is greater than 4000 sq.ft'''\ntrain.drop(train[train.GrLivArea>4000].index, inplace = True)\ntrain.reset_index(drop = True, inplace = True)","b6ea0b8e":"'''Sactter plot of GrLivArea vs SalePrice.'''\nscatter_plot(train.GrLivArea, train.SalePrice, 'GrLivArea vs SalePrice', 'GrLivArea', 'SalePrice', 10, 'Rainbow')","079e0db4":"'''Scatter plot of TotalBsmtSF Vs SalePrice'''\nscatter_plot(train.TotalBsmtSF, train.SalePrice, 'TotalBsmtSF Vs SalePrice', 'TotalBsmtSF', 'SalePrice', 10, 'Cividis')","84363625":"'''Drop observations where TotlaBsmtSF is greater than 3000 sq.ft'''\ntrain.drop(train[train.TotalBsmtSF>3000].index, inplace = True)\ntrain.reset_index(drop = True, inplace = True)","7ce1eb53":"'''Scatter plot of TotalBsmtSF Vs SalePrice'''\nscatter_plot(train.TotalBsmtSF, train.SalePrice, 'TotalBsmtSF Vs SalePrice', 'TotalBsmtSF', 'SalePrice', 10, 'Cividis')","85330cd4":"'''Scatter plot of YearBuilt Vs SalePrice'''\nscatter_plot(train.YearBuilt, np.log1p(train.SalePrice), 'YearBuilt Vs SalePrice', 'YearBuilt', 'SalePrice', 10, 'viridis')","db925bfb":"'''Drop observations where YearBulit is less than 1893 sq.ft'''\ntrain.drop(train[train.YearBuilt<1900].index, inplace = True)\ntrain.reset_index(drop = True, inplace = True)","1b2db103":"'''Scatter plot of YearBuilt Vs SalePrice'''\nscatter_plot(train.YearBuilt, np.log1p(train.SalePrice), 'YearBuilt Vs SalePrice', 'YearBuilt', 'SalePrice', 10, 'viridis')","1f3e4991":"'''Scatter plot of GarageCars Vs SalePrice'''\nscatter_plot(train.GarageCars, np.log(train.SalePrice), 'GarageCars Vs SalePrice', 'GarageCars', 'SalePrice', 10, 'Electric')","8d8b228d":"'''Scatter plot of GarageCars Vs SalePrice'''\nscatter_plot(train.OverallQual, np.log(train.SalePrice), 'OverallQual Vs SalePrice', 'OverallQual', 'SalePrice', 10, 'Bluered')","dbe8e298":"'''Scatter plot of FullBath Vs SalePrice'''\nscatter_plot(train.FullBath, np.log(train.SalePrice), 'FullBath Vs SalePrice', 'FullBath', 'SalePrice', 10, 'RdBu')","c7c559c3":"'''separate our target variable first'''\ny_train = train.SalePrice\n\n'''Drop SalePrice from train data.'''\ntrain.drop('SalePrice', axis = 1, inplace = True)\n\n'''Now combine train and test data frame together.'''\ndf_merged = pd.concat([train, test], axis = 0)\n\n'''Dimensions of new data frame'''\ndf_merged.shape","f7a44980":"'''Again convert MSSubClass, OverallQual, OverallCond, MoSold, YrSold into categorical variables.'''\ndf_merged.loc[:,['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold']] = df_merged.loc[:,['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold']].astype('object')\ndf_merged.dtypes.value_counts()","06789898":"'''columns with missing observation'''\nmissing_columns = df_merged.columns[df_merged.isnull().any()].values\n'''Number of columns with missing obervation'''\ntotal_missing_columns = np.count_nonzero(df_merged.isnull().sum())\nprint('We have ' , total_missing_columns ,  'features with missing values and those features (with missing values) are: \\n\\n' , missing_columns)","1a7621f6":"'''Simple visualization of missing variables'''\nplt.figure(figsize=(20,8))\nsns.heatmap(df_merged.isnull(), yticklabels=False, cbar=False, cmap = 'summer')","ca46eff3":"'''Get and plot only the features (with missing values) and their corresponding missing values.'''\nmissing_columns = len(df_merged) - df_merged.loc[:, np.sum(df_merged.isnull())>0].count()\nx = missing_columns.index\ny = missing_columns\ntitle = 'Variables with Missing Values'\nscatter_plot(x, y, title, 'Features Having Missing Observations','Missing Values', 20, 'Viridis')","ed053a90":"missing_columns","aaf06279":"'''Impute by None where NaN means something.'''\nto_impute_by_none = df_merged.loc[:, ['PoolQC','MiscFeature','Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageCond','GarageFinish','GarageQual','BsmtFinType2','BsmtExposure','BsmtQual','BsmtCond','BsmtFinType1','MasVnrType']]\nfor i in to_impute_by_none.columns:\n    df_merged[i].fillna('None', inplace = True)","f8681875":"'''These are categorical variables and will be imputed by mode.'''\nto_impute_by_mode =  df_merged.loc[:, ['Electrical', 'MSZoning','Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional', 'SaleType']]\nfor i in to_impute_by_mode.columns:\n    df_merged[i].fillna(df_merged[i].mode()[0], inplace = True)","2cb2d4af":"'''The following variables are either discrete numerical or continuous numerical variables.So the will be imputed by median.'''\nto_impute_by_median = df_merged.loc[:, ['BsmtFullBath','BsmtHalfBath', 'GarageCars', 'MasVnrArea', 'GarageYrBlt', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea']]\nfor i in to_impute_by_median.columns:\n    df_merged[i].fillna(df_merged[i].median(), inplace = True)","087031a3":"'''We need to convert categorical variable into numerical to plot correlation heatmap. So convert categorical variables into numerical.'''\ndf = df_merged.drop(columns=['Id','LotFrontage'], axis=1)\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf = df.apply(le.fit_transform) # data is converted.\ndf.head(2)","5ff43110":" # Inserting Age in variable correlation.\ndf['LotFrontage'] = df_merged['LotFrontage']\n# Move Age at index 0.\ndf = df.set_index('LotFrontage').reset_index()\ndf.head(2)","38882088":"'''correlation of df'''\ncorr = df.corr()\ndisplay(corr['LotFrontage'].sort_values(ascending = False)[:5])\ndisplay(corr['LotFrontage'].sort_values(ascending = False)[-5:])","97bb514e":"'''Impute LotFrontage with median of respective columns (i.e., BldgType)'''\ndf_merged['LotFrontage'] = df_merged.groupby(['BldgType'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))","dd4704c1":"'''Is there any missing values left untreated??'''\nprint('Missing variables left untreated: ', df_merged.columns[df_merged.isna().any()].values)","394c30d1":"'''Skewness and Kurtosis of SalePrice'''\nprint(\"Skewness: %f\" % y_train.skew())\nprint(\"Kurtosis: %f\" % y_train.kurt())","4b4ddc33":"'''Plot the distribution of SalePrice with skewness.'''\nhistogram_plot(y_train, 'SalePrice without Transformation', 'Abs Frequency', 'deepskyblue')","fbb68c3a":"'''Plot the distribution of SalePrice with skewness'''\ny_train = np.log1p(y_train)\ntitle = 'SalePrice after Transformation (skewness: {:0.4f})'.format(y_train.skew())\nhistogram_plot(y_train, title, 'Abs Frequency', ' darksalmon')","80669e2d":"'''Now calculate the rest of the explanetory variables'''\nskew_num = pd.DataFrame(data = df_merged.select_dtypes(include = ['int64', 'float64']).skew(), columns=['Skewness'])\nskew_num_sorted = skew_num.sort_values(ascending = False, by = 'Skewness')\nskew_num_sorted","16ee8976":"''' plot the skewness for rest of the explanetory variables'''\nbar_plot(skew_num_sorted.index, skew_num_sorted.Skewness, 'Skewness in Explanetory Variables', 'Skewness', 'Blackbody')","98f945d1":"'''Extract numeric variables merged data.'''\ndf_merged_num = df_merged.select_dtypes(include = ['int64', 'float64'])","145d1cc5":"'''Make the tranformation of the explanetory variables'''\ndf_merged_skewed = np.log1p(df_merged_num[df_merged_num.skew()[df_merged_num.skew() > 0.5].index])\n\n\n#Normal variables\ndf_merged_normal = df_merged_num[df_merged_num.skew()[df_merged_num.skew() < 0.5].index]\n    \n#Merging\ndf_merged_num_all = pd.concat([df_merged_skewed, df_merged_normal], axis = 1)","24f85df6":"'''Update numerical variables with transformed variables.'''\ndf_merged_num.update(df_merged_num_all)","89b6f999":"'''Standarize numeric features with RobustScaler'''\nfrom sklearn.preprocessing import RobustScaler\n\n'''Creating scaler object.'''\nscaler = RobustScaler()\n\n'''Fit scaler object on train data.'''\nscaler.fit(df_merged_num)\n\n'''Apply scaler object to both train and test data.'''\ndf_merged_num_scaled = scaler.transform(df_merged_num)","ac7f5cef":"'''Retrive column names'''\ndf_merged_num_scaled = pd.DataFrame(data = df_merged_num_scaled, columns = df_merged_num.columns, index = df_merged_num.index)\n# Pass the index of index df_merged_num, otherwise it will sum up the index.\n","56ca2ee2":"\"\"\"Let's extract categorical variables first and convert them into category.\"\"\"\ndf_merged_cat = df_merged.select_dtypes(include = ['object']).astype('category')\n\n\"\"\"let's begin the tedious process of label encoding of ordinal variable\"\"\"\ndf_merged_cat.LotShape.replace(to_replace = ['IR3', 'IR2', 'IR1', 'Reg'], value = [0, 1, 2, 3], inplace = True)\ndf_merged_cat.LandContour.replace(to_replace = ['Low', 'Bnk', 'HLS', 'Lvl'], value = [0, 1, 2, 3], inplace = True)\ndf_merged_cat.Utilities.replace(to_replace = ['NoSeWa', 'AllPub'], value = [0, 1], inplace = True)\ndf_merged_cat.LandSlope.replace(to_replace = ['Sev', 'Mod', 'Gtl'], value = [0, 1, 2], inplace = True)\ndf_merged_cat.ExterQual.replace(to_replace = ['Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3], inplace = True)\ndf_merged_cat.ExterCond.replace(to_replace = ['Po', 'Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3, 4], inplace = True)\ndf_merged_cat.BsmtQual.replace(to_replace = ['None', 'Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3, 4], inplace = True)\ndf_merged_cat.BsmtCond.replace(to_replace = ['None', 'Po', 'Fa', 'TA', 'Gd'], value = [0, 1, 2, 3, 4], inplace = True)\ndf_merged_cat.BsmtExposure.replace(to_replace = ['None', 'No', 'Mn', 'Av', 'Gd'], value = [0, 1, 2, 3, 4], inplace = True)\ndf_merged_cat.BsmtFinType1.replace(to_replace = ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], value = [0, 1, 2, 3, 4, 5, 6], inplace = True)\ndf_merged_cat.BsmtFinType2.replace(to_replace = ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], value = [0, 1, 2, 3, 4, 5, 6], inplace = True)\ndf_merged_cat.HeatingQC.replace(to_replace = ['Po', 'Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3, 4], inplace = True)\ndf_merged_cat.Electrical.replace(to_replace = ['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr'], value = [0, 1, 2, 3, 4], inplace = True)\ndf_merged_cat.KitchenQual.replace(to_replace = ['Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3], inplace = True)\ndf_merged_cat.Functional.replace(to_replace = ['Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'], value = [0, 1, 2, 3, 4, 5, 6], inplace = True)\ndf_merged_cat.FireplaceQu.replace(to_replace =  ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3, 4, 5], inplace = True)\ndf_merged_cat.GarageFinish.replace(to_replace =  ['None', 'Unf', 'RFn', 'Fin'], value = [0, 1, 2, 3], inplace = True)\ndf_merged_cat.GarageQual.replace(to_replace =  ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3, 4, 5], inplace = True)\ndf_merged_cat.GarageCond.replace(to_replace =  ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3, 4, 5], inplace = True)\ndf_merged_cat.PavedDrive.replace(to_replace =  ['N', 'P', 'Y'], value = [0, 1, 2], inplace = True)\ndf_merged_cat.PoolQC.replace(to_replace =  ['None', 'Fa', 'Gd', 'Ex'], value = [0, 1, 2, 3], inplace = True)\ndf_merged_cat.Fence.replace(to_replace =  ['None', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'], value = [0, 1, 2, 3, 4], inplace = True)","5582bb11":"'''All the encodeded variables have int64 dtype except OverallQual and OverallCond. So convert them back into int64.'''\ndf_merged_cat.loc[:, ['OverallQual', 'OverallCond']] = df_merged_cat.loc[:, ['OverallQual', 'OverallCond']].astype('int64')\n\n'''Extract label encoded variables'''\ndf_merged_label_encoded = df_merged_cat.select_dtypes(include = ['int64'])","067a3233":"'''Now selecting the nominal vaiables for one hot encording'''\ndf_merged_one_hot = df_merged_cat.select_dtypes(include=['category'])\n\n\"\"\"Let's get the dummies variable\"\"\"\ndf_merged_one_hot = pd.get_dummies(df_merged_one_hot, drop_first=True)","5d79116f":"\"\"\"Let's concat one hot encoded and label encoded variable together\"\"\"\ndf_merged_encoded = pd.concat([df_merged_one_hot, df_merged_label_encoded], axis=1)\n\n'''Finally join processed categorical and numerical variables'''\ndf_merged_processed = pd.concat([df_merged_num_scaled, df_merged_encoded], axis=1)\n\n'''Dimensions of new data frame'''\ndf_merged_processed.shape","88117304":"'''Now retrive train and test data for modelling.'''\ndf_train_final = df_merged_processed.iloc[0:1438, :]\ndf_test_final = df_merged_processed.iloc[1438:, :]\n\n'''And we have our target variable as y_train.'''\ny_train = y_train","3a214016":"'''Updated train data'''\ndf_train_final.head()","a0b2070a":"'''Updated test data'''\ndf_train_final.head()","ccc2c122":"\"\"\"Let's have a final look at data dimension\"\"\"\nprint('Input matrix dimension:', df_train_final.shape)\nprint('Output vector dimension:', y_train.shape)\nprint('Test data dimension:', df_test_final.shape)","82174216":"'''set a seed for reproducibility'''\nseed = 44\n\n'''Initialize all the regesssion models object we are interested in'''\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n''''We are interested in the following 14 regression models.\nAll initialized with default parameters except random_state and n_jobs.'''\nlr = LinearRegression(n_jobs = -1)\nlasso = Lasso(random_state = seed)\nridge = Ridge(random_state = seed)\nelnt = ElasticNet(random_state = seed)\nkr = KernelRidge()\ndt = DecisionTreeRegressor(random_state = seed)\nsvr = SVR()\nknn = KNeighborsRegressor(n_jobs= -1)\npls = PLSRegression()\nrf = RandomForestRegressor(n_jobs = -1, random_state = seed)\net = ExtraTreesRegressor(n_jobs = -1, random_state = seed)\nab = AdaBoostRegressor(random_state = seed)\ngb = GradientBoostingRegressor(random_state = seed)\nxgb = XGBRegressor(n_jobs = -1, random_state = seed)\nlgb = LGBMRegressor(n_jobs = -1, random_state = seed)","3dd84621":"'''Training accuracy of our regression models. By default score method returns coefficient of determination (r_squared).'''\ndef train_r2(model):\n    model.fit(df_train_final, y_train)\n    return model.score(df_train_final, y_train)\n\n'''Calculate and plot the training accuracy.'''\nmodels = [lr, lasso, ridge, elnt, kr, dt, svr, knn, pls, rf, et, ab, gb, xgb, lgb]\ntraining_score = []\nfor model in models:\n    training_score.append(train_r2(model))\n\n'''Plot dataframe of training accuracy.'''\ntrain_score = pd.DataFrame(data = training_score, columns = ['Training_R2'])\ntrain_score.index = ['LR', 'LASSO',  'RIDGE', 'ELNT', 'KR', 'DT', 'SVR', 'KNN', 'PLS', 'RF', 'ET', 'AB', 'GB', 'XGB', 'LGB']\ntrain_score = (train_score*100).round(4)\nscatter_plot(train_score.index, train_score['Training_R2'], 'Training Score (R_Squared)', 'Models', '% Training Score', 30, 'Rainbow')","778c084f":"'''Evaluate model on the hold set'''\ndef train_test_split(model):\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error\n    X_train, X_test, Y_train, Y_test = train_test_split(df_train_final, y_train, test_size = 0.3, random_state = seed)\n    model.fit(X_train, Y_train)\n    prediction = model.predict(X_test)\n    mse = mean_squared_error(prediction, Y_test)\n    rmse = np.sqrt(mse) #non-negative square-root\n    return rmse\n\n'''Calculate train_test_split score of differnt models and plot them.'''\nmodels = [lasso, ridge, elnt, kr, dt, svr, knn, pls, rf, et, ab, gb, xgb, lgb]\ntrain_test_split_rmse = []\nfor model in models:\n    train_test_split_rmse.append(train_test_split(model))\n    \n'''Plot data frame of train test rmse'''\ntrain_test_score = pd.DataFrame(data = train_test_split_rmse, columns = ['Train_Test_RMSE'])\ntrain_test_score.index = ['LASSO',  'RIDGE', 'ELNT', 'KR', 'DT', 'SVR', 'KNN', 'PLS', 'RF', 'ET', 'AB', 'GB', 'XGB', 'LGB']\nscatter_plot(train_test_score.index, train_test_score['Train_Test_RMSE'], \"Models' Test Score (RMSE) on Holdout(30%) Set\", 'Models', 'RMSE', 30, 'plotly3')","b243ec2c":"'''Function to compute cross validation scores.'''\ndef cross_validation(model):\n    from sklearn.model_selection import cross_val_score\n    val_score = cross_val_score(model, df_train_final, y_train, cv=10, n_jobs= -1, scoring = 'neg_mean_squared_error')\n    sq_val_score = np.sqrt(-1*val_score)\n    r_val_score = np.round(sq_val_score, 5)\n    return r_val_score.mean()\n\n'''Calculate cross validation score of differnt models and plot them.'''\nmodels = [lasso, ridge, elnt, kr, dt, svr, knn, pls, rf, et, ab, gb, xgb, lgb]\ncross_val_scores = []\nfor model in models:\n    cross_val_scores.append(cross_validation(model))\n\n'''Plot data frame of cross validation scores.'''\nx_val_score = pd.DataFrame(data = cross_val_scores, columns=['Cross_Val_Score(RMSE)'])\nx_val_score.index = ['LASSO',  'RIDGE', 'ELNT', 'KR', 'DT', 'SVR', 'KNN', 'PLS', 'RF', 'ET', 'AB', 'GB', 'XGB', 'LGB']\nscatter_plot(x_val_score.index, x_val_score['Cross_Val_Score(RMSE)'], \"Models' 10-fold Cross Validation Scores (RMSE)\", 'Models', 'RMSE', 30, 'cividis')","b76ea8ce":"'''Create a function to tune hyperparameters of the selected models.'''\ndef tune_hyperparameters(model, param_grid):\n    from sklearn.model_selection import GridSearchCV\n    global best_params, best_score #if you want to know best parametes and best score\n    \n    '''Construct grid search object with 10 fold cross validation.'''\n    grid = GridSearchCV(model, param_grid, cv = 10, verbose = 1, scoring = 'neg_mean_squared_error', n_jobs = -1)\n    grid.fit(df_train_final, y_train)\n    best_params = grid.best_params_ \n    best_score = np.round(np.sqrt(-1 * grid.best_score_), 5)\n    return best_params, best_score","baf7abd1":"'''Difine hyperparameters of ridge'''\nridge_param_grid = {'alpha':[0.5, 2.5, 3.3, 5, 5.5, 7, 9, 9.5, 9.52, 9.64, 9.7, 9.8, 9.9, 10, 10.5,10.62,10.85, 20, 30],\n                    'random_state':[seed]}\ntune_hyperparameters(ridge, ridge_param_grid)\nridge_best_params, ridge_best_score = best_params, best_score\nprint('Ridge best params:{} & best_score:{:0.5f}' .format(ridge_best_params, ridge_best_score))","7bb8234e":"alpha = [0.0001, 0.0002, 0.00025, 0.0003, 0.00031, 0.00032, 0.00033, 0.00034, 0.00035, 0.00036, 0.00037, 0.00038, \n         0.0004, 0.00045, 0.0005, 0.00055, 0.0006, 0.0008,  0.001, 0.002, 0.005, 0.007, 0.008, 0.01]\n\nlasso_params = {'alpha': alpha,\n               'random_state':[seed]}\n\ntune_hyperparameters(lasso, lasso_params)\nlasso_best_params, lasso_best_score = best_params, best_score\nprint('Lasso best params:{} & best_score:{:0.5f}' .format(lasso_best_params, lasso_best_score))","35956945":"KernelRidge_param_grid = {'alpha':[0.1, 0.15, 0.23, 0.25, 0.3,1],\n                          'kernel': ['linear', 'polynomial'],\n                          'degree': [2,3],\n                          'coef0': [1.5,2,3]}\ntune_hyperparameters(kr, KernelRidge_param_grid)\nkr_best_params, kr_best_score = best_params, best_score\nprint('Kernel Ridge best params:{} & best_score: {:0.5f}'. format(kr_best_params, kr_best_score))","2b9d0cb8":"elastic_params_grid = {'alpha': [0.0001,0.0002, 0.0003, 0.01,0.1,2], \n                 'l1_ratio': [0.2, 0.85, 0.95,0.98,10],\n                 'random_state':[seed]}\ntune_hyperparameters(elnt, elastic_params_grid)\nelastic_best_params, elastic_best_score = best_params, best_score\nprint('Elastic Net best params:{} & best_score:{:0.5f}' .format(elastic_best_params, elastic_best_score))","978da924":"svr_params_grid = {'kernel':['linear', 'poly', 'rbf'],\n                   'C':[2,4,5],\n                   'gamma':[0.01,0.001,0.0001]}\ntune_hyperparameters(svr, svr_params_grid)\nsvr_best_params, svr_best_score = best_params, best_score\nprint('SVR best params:{} & best_score:{:0.5f}' .format(svr_best_params, svr_best_score))","89092b6c":"rf_params_grid = {'n_estimators':[1,5,50,100],\n                   'max_depth':[1,2],\n                   'min_samples_split':[3,4],\n                   'min_samples_leaf':[2,4],\n                   'random_state':[seed]}\ntune_hyperparameters(rf, rf_params_grid)\nrf_best_params, rf_best_score = best_params, best_score\nprint('RF best params:{} & best_score:{:0.5f}' .format(rf_best_params, rf_best_score))","8f7a2f63":"xgb_params_grid = {'min_child_weight': [5, 10],\n                   'gamma': [0.04, 0.1, 1.5],\n                   'subsample': [0.6, 0.8, 1.0],\n                   'colsample_bytree': [0.46, 1.0],\n                   'max_depth': [3, 4]}\nxgb_opt = XGBRegressor(learning_rate = 0.03, reg_alpha = 0.4640, reg_lambda = 0.8571, n_estimators = 1000, \n                       silent = 1, nthread = -1, random_state = 101)\n\ntune_hyperparameters(xgb_opt, xgb_params_grid)\nxgb_best_params, xgb_best_score = best_params, best_score\nprint('XGB best params:{} & best_score:{:0.5f}' .format(xgb_best_params, xgb_best_score))","a294878e":"'''Not Optimize Randomly choosen parameters'''\n'''Hyperparameters of gb'''\ngb_opt = GradientBoostingRegressor(n_estimators = 3000, learning_rate = 0.05,\n                                   max_depth = 4, max_features = 'sqrt',\n                                   min_samples_leaf = 15, min_samples_split = 10, \n                                   loss = 'huber', random_state = seed)\n'''Hyperparameters of lgb'''\nlgb_opt = LGBMRegressor(objective = 'regression', num_leaves = 5,\n                              learning_rate=0.05, n_estimators = 660,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed = 9, bagging_seed = 9,\n                              min_data_in_leaf = 6, min_sum_hessian_in_leaf = 11)\n'''We can assume these 2 model best score is equal to cross validation scores.\nThought it might not be precise, but I will take it'''\ngb_best_score = cross_validation(gb_opt)\nlgb_best_score = cross_validation(lgb_opt)","5c47a08b":"\"\"\"Let's plot the models' rmse after optimization.\"\"\"\noptimized_scores = pd.DataFrame({'Optimized Scores':np.round([lasso_best_score, ridge_best_score, kr_best_score, \n                  elastic_best_score, svr_best_score, rf_best_score, xgb_best_score, gb_best_score, lgb_best_score], 5)})\noptimized_scores.index = ['Lasso', 'Ridge', 'Kernel_ridge', 'E_net', 'SVM', 'RF', 'XGB', 'GB', 'LGB']\noptimized_scores.sort_values(by = 'Optimized Scores')\nscatter_plot(optimized_scores.index, optimized_scores['Optimized Scores'], \"Models' Scores after Optimization\", 'Models','Optimized Scores', 40, 'Rainbow')","35684b5d":"'''Initialize 9 object models with best hyperparameters'''\nlasso_opt = Lasso(**lasso_best_params)\nridge_opt = Ridge(**ridge_best_params)\nkernel_ridge_opt = KernelRidge(**kr_best_params)\nelastic_net_opt = ElasticNet(**elastic_best_params)\nrf_opt = RandomForestRegressor(**rf_best_params)\nsvm_opt = SVR(**svr_best_params)\nxgb_opt = XGBRegressor(**xgb_best_params)\ngb_opt = gb_opt\nlgb_opt = lgb_opt","db671c3f":"'''Now train and predict with optimized models'''\ndef predict_with_optimized_models(model):\n    model.fit(df_train_final, y_train)\n    y_pred = np.expm1(model.predict(df_test_final))\n    submission = pd.DataFrame()\n    submission['Id']= test.Id\n    submission['SalePrice'] = y_pred\n    return submission\n\n'''Make submission with optimized lasso, ridge, kernel_ridge, elastic_net and svm, xgb, gb, and lgb.'''\npredict_with_optimized_models(lasso_opt).to_csv('lasso_optimized.csv', index = False)\npredict_with_optimized_models(ridge_opt).to_csv('ridge_optimized.csv', index = False)\npredict_with_optimized_models(kernel_ridge_opt).to_csv('kernel_ridge_optimized.csv', index = False)\npredict_with_optimized_models(elastic_net_opt).to_csv('elastic_net_optimized.csv', index = False)\npredict_with_optimized_models(rf_opt).to_csv('rf_opt_optimized.csv', index = False)\npredict_with_optimized_models(svm_opt).to_csv('svm_opt_optimized.csv', index = False)\npredict_with_optimized_models(xgb_opt).to_csv('xgb_optimized.csv', index = False)\npredict_with_optimized_models(gb_opt).to_csv('gb_optimized.csv', index = False)\npredict_with_optimized_models(lgb_opt).to_csv('lgb_optimized.csv', index = False)","6cdd813f":"**For delailed variable description please check out [here](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data)**\n\n#### So what can we see till now.\n* we have total 81 variables for train and 80 for test variable\n* we don't have *SalePrice* variable for test variable because this will be our task to infer *SalePrice* for test set by learning from train set.\n* So *SalePrice* is our target variable and rest of the variables are our predictor variables.","7a554eaa":"***Linear relationship. Nice!!...***\n\n**In summary**\n\n* 'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.\n* 'OverallQual', FullBath and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual'.","cdf0b682":"## 3.3 Normality And Transformation of Distributions <a id=\"3.3\"><\/a>\nNormal distribution (bell-shaped) of variables is not only one of the assumptions of regression problems but also a assumption of parametric test (like one-way-anova, t-test etc) and pearson correlation. But in practice, this can not be met perfectly and hence some deviation off this assumption is acceptable. In this section, we would try to make the skewed distribution as normal as possible. Since most of the variables are positively skewed, we would apply log transformation on them. **Let's observe our target variable separately:**\n\nIf skewness is 0, the data are perfectly symmetrical, although it is quite unlikely for real-world data. As a general rule of thumb:\n\nIf skewness is less than -1 or greater than 1, the distribution is highly skewed.\nIf skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\nIf skewness is between -0.5 and 0.5, the distribution is approximately symmetric.","17c16a5d":"#### 5.2.2.2 Optimize Lasso\nModel parameter desription [Here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Lasso.html)","2c494131":"## 3.1 Linearity and Outliers Treatment <a id=\"3.1\"><\/a>\nThe best way to check linear relationship is to plot scatter diagram.\nAs well as we also treat the outliers.","7bb8a4fa":"# About the Kernel\nHi Everyone!!, **This kernel is easily understandable to the beginner like me.** This verbosity tries to explain everything I could possibly know. Once you get through the notebook, you can find this useful and straightforward. I attempted to explain things as simple as possible.\n\nIn this notebook, I extensively use plotly along with seaborn and matplotlib for data visualization and Machine learning Algorithms to predict house prices in Ames. So it's a regression problem.\n\nKeep Learning,\n\nvikas singh","d906d698":"### 5.2.3 Retrain and Predict Using Best Hyperparameters <a id=\"5.2.3\"><\/a>\nNow we would like to retrain our models using the best parameters responsible for best rmse after optimization. Then we would predict on test data to see how different models perform on leaderboard.","84efacf1":"# CONTEXT\n\n* [1. Importing Packages and Collecting Data](#1)\n* [2.Variable Description, Identification, and Correction](#2)\n* [3. Checking the Assumption](#3)\n  * [3.1 Linearity and Outliers Treatment](#3.1)\n  * [3.2 Imputing Missing Variables](#3.2)\n  * [3.3 Normality And Transformation of Distributions](#3.3)\n* [4. Feature Engineering](#4)\n  * [4.1 Feature Scaling](#4.1)\n  * [4.2 Encoding Categorical Variables](#4.2)\n    * [4.2.1 Manually Label Encoding](#4.2.1)\n    * [4.2.2 One Hot Encoding](#4.2.2)\n * [5. Model Building and Evaluation](#5)\n   * [5.1 Model Training](#5.1)\n   * [5.2 Model Evaluation](#5.2)\n     * [5.2.1 K-Fold Cross Validation](#5.2.1)\n     * [5.2.2 Optimization of Hyperparameters](#5.2.2)\n     * [5.2.3 Retrain and Predict Using Best Hyperparameters](#5.2.3)","2d92212e":"***Being a regression problem, score method returns r_squared(coefficients of determination) and hence bigger is better. Looks like DT and ET have exactly r2_score of 100%. Usually higher r2_score is better but r2_score very close to 1 might indicate overfitting. But train accuracy of a model is not enough to tell if a model can be able to generalize the unseen data or not. Because training data is something our model has been trained with, i.e., data our model has already seen it. We all know that, the purpose of building a machine learning model is to generalize the unseen data, i.e., data our model has not yet seen. Hence we can't use training accuracy for our model evaluation rather we must know how our model will perform on the data our model is yet to see.***","d3a83e02":"**These are the variables most correlated with 'SalePrice'. My thoughts on this:**\n\n * 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!\n \n* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n\n***<font color=blue>we concluded that the following variables can play an important role in this problem:***<\/font>\n\n* OverallQual\n* GarageCars\n* YearBuilt\n* FullBath\n* TotalBsmtSF.\n* GrLivArea.","fbeccd30":"***Ahh!!!...Showing linear relationship***","5505a9f8":"**Again Ridge (0.114) has managed to beat SVM(0.116) as the best regression model on 10-fold cross validation. And rmse of GB, XGB, and LGB have also dropped from previous holdout set's rmse.**","321a2291":"* ## 5.2 Model Evaluation <a id=\"5.2\"><\/a>\nSo basically, to evaluate a model's performance, we need some data (input) for which we know the ground truth(label). For this problem, we don't know the ground truth for the test set but we do know for the train set. So the idea is to train and evaluate the model performance on different data. One thing we can do is to split the train set in two groups, usually in 80:20 ratio. That means we would train our model on 80% of the training data and we reserve the rest 20% for evaluating the model since we know the ground truth for this 20% data. Then we can compare our model prediction with this ground truth (for 20% data). That's how we can tell how our model would perform on unseen data. This is the first model evaluation technique. In sklearn we have a train_test_split method for that. Let's evaluate our model using train_test_split method. **Note: From now on, we will be using root mean squared error (that is, the average squared difference between the estimated values and the actual value) as the evaluation metric for this problem. So smaller is better.**","27a4e8dc":"#### 5.2.2.5 Optimize Support Vector Regression\nModel parameters desciption [Here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVR.html)","e9337706":"# 2.Variable Description, Identification, and Correction <a id=\"2\"><\/a>","fff7894f":"**Now we can clearly see distribution so numerical variable, we find that:**\n* There are two type of numerical  variable in this data continuous (like LotFrontage, LotArea, and YearBuilt) \n* and some are discrete (like MSSubClass, OverallQual, OverallCond, BsmtFullBath, and HalfBath etc.)\n* some variables are actually categorical (like like MSSubClass, OverallQual, and OverallCond). *For detailed data documentation see data_description.txt","2183d5b0":"## 4.2 Encoding Categorical Variables <a id=\"4.2\"><\/a>\nWe have to encode categorical variables for our machine learning algorithms to interpret them. We would use label encoding and then one hot encoding.\n\n### 4.2.1 Manually Label Encoding <a id=\"4.2.1\"><\/a>\nWe would like to encode some categorical (ordinal) variables to preserve their ordinality. If we use sklearn's label encoder, it will randomly encode these ordinal variables and therefore ordinality would be lost. To overcome this, we will use pandas replace method to manually encode orninal variables. ***Variables like LotShape, LandContour, Utilities, LandSlope, OverallQual (already encoded), OverallCond (already encoded), ExterQual, ExterCond, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, HeatingQC, BsmtFinType2, Electrical, KitchenQual, Functional, FireplaceQu, GarageFinish, GarageQual, GarageCond, PavedDrive, PoolQC, Fence have inherent orders. Let's encode them. Don't get bored if you fell exhausted in the process.***\n\n**Manuall Work - go to data_depciption.txt > Ctrl+f > find the variable > start labeling**","3705e2f8":"# 5. Model Building and Evaluation <a id=\"5\"><\/a>\nWith all the preprocessings done and dusted, we're ready to train our regression models with the processed data.","27bd9d01":"*Great!!... It seems that 'SalePrice' and 'GrLivArea' showing the* **linear relationship.**\n\n***Outliers Treatment*** ","0639ce4e":"**Being root mean squared error, smaller is better. Looks like, RIDGE (0.114) is the best regression model followed by SVR(0.118), GB(0.123) and XGB(0.121). Unfortunately, LR(11.02) can't find any linear pattern, hence it performs worst and hence discarded.**\n\nHowever, train_test split has its drawbacks. Because this approach introduces bias as we are not using all of our observations for testing and also we're reducing the train data size. To overcome this we can use a technique called **cross validation** where all the data is used for training and testing periodically. Thus we may reduce the bias introduced by train_test_split. From different cross validation methods, we would use **k-fold cross validation.** In sklearn we have a method cross_val_score for calculating k-fold cross validation score.\n\n### 5.2.1 K-Fold Cross Validation <a id=\"5.2.1\"><\/a>\nK-Fold is a popular and easy to understand, it generally results in a less biased model compare to other methods. Because it ensures that every observation from the original dataset has the chance of appearing in training and test set. This is one among the best approach if we have a limited input data.\n![](http:\/\/analytics.georgetown.edu\/sites\/gradanalytics\/files\/styles\/georgetown_thumbnail\/public\/osvald_0.jpg)\n\nLet's say we will use 10-fold cross validation. So k = 10 and we have total 1438 observations. Each fold would have 1438\/10 = 143.8 observations. So basically k-fold cross validation uses fold-1 (143.8 samples) as the testing set and k-1 (9 folds) as the training sets and calculates test accuracy.This procedure is repeated k times (if k = 10, then 10 times); each time, a different group of observations is treated as a validation or test set. This process results in k estimates of the test accuracy which are then averaged out.\n\n\n\n\n","08c430cf":"# 3. Checking the Assumption <a id=\"3\"><\/a>\n\nAccording to Hair et al. (2013), four assumptions should be tested:\n\n* **Normality** - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.\n\n* **Homoscedasticity** - I just hope I wrote it right. Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n\n* **Linearity**- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n\n* **Absence of correlated errors(Multicollineraty)** - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.","c2dce4d2":"*Okay!!!...we observed that as year is passing, sale price  of the house also increase, showing* ***linear relationship.***\n\n**Note:** we don't know if SalePrice is affected by inflation or not.\n\n***Outliers Treatment***","27fe7f27":"## 4.1 Feature Scaling <a id=\"4.1\"><\/a>\nIn sklearn we have various method like from MinMaxScaler, minmax_scale, MaxAbsScaler, StandardScaler, RobustScaler,Normalizer, QuantileTransformer, PowerTransformer. **For more see the [documentation](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html)**\n\nUsually well known for rescaling data, i.e., normalization and standarization. Normalization scales all numeric variables in the range [0,1]. So outliers might be lost. On the other hand, standarization transforms data to have zero mean and unit variance.\n\nFeature scaling helps gradient descent converge faster, thus reducing training time. Its not necessary to standarize the target variable. However, due to the presence of outliers, we would use sklearn's RobustScaler since it is not affected by outliers.","67743503":"#### 'SalePrice' correlation matrix","12b8327b":"## 3.2 Imputing Missing Variables <a id=\"3.2\"><\/a>\nThe simpliest way to impute missing values of a variable is to impute its missing values with its mean, median or mode depending on its distribution and variable type(categorical or numerical). By now, we should have a idea about the distribution of the variables and the presence of outliers in those variables. For categorical variables mode-imputation is performed and for numerical variable mean-impuation is performed if its distribution is symmetric(or almost symmetric or normal like Age). On the other hand, for a variable with skewed distribution and outliers, meadian-imputation is recommended as median is more immune to outliers than mean.\n\nHowever, one clear disadvantage of using mean, median or mode to impute missing values is the addition of bias if the amount of missing values is significant. So simply replacing missing values with the mean or the median might not be the best solution since missing values may differ by groups and categories. To solve this, we can group our data by some variables that have no missing values and for each subset compute the median to impute the missing values of a variable.","d2dbab2e":"**If you find my kernel useful, some upvotes will be appreciated.**\n\n**Check out my other kernels :**\n\nsimple Titanic EDA [HERE](https:\/\/www.kaggle.com\/vikassingh1996\/titanic-exploratory-data-analysis-the-beginning)\n\nTitanic - Modeling [HERE](https:\/\/www.kaggle.com\/vikassingh1996\/data-pre-processing-and-modeling)\n                              ","eab9f9ee":"#### 5.2.2.6 Optimize Random Forest Regressor\nModel parameters desciption [Here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html)","b33ade95":"#### 5.2.2.1 Optimize Ridge\nModel parameter description [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html)","82f3efc8":"**Variables like PoolQC, MiscFeature, Alley, Fence, and FirePlaceQu have most missing variables**. Usually we drop a variable if at least 40% of its values are missing. Hence, one might tempt to drop variables like PoolQC, MiscFeature, Alley, Fence, and FirePlaceQu. Deleting these variables would be a blunder because data description tells these 'NaN' has some purpose for those variables. Like 'NaN' in PoolQC refers to 'No Pool', 'NaN' in MiscFeature refers to 'None', and 'NaN' in Alley means 'No alley access' etc. More generally NaN means the absent of that variable. Hence we gonna replace NaN with 'None' in those variable. Please do read data [description](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/download\/data_description.txt) carefully.","baa2e2f7":"***We transform the Saleprice in log transformation for more clear linear relationship.***","e9cb6c03":"# 1. Importing Packages and Collecting Data <a id=\"1\"><\/a>","1fd20163":"![](http:\/\/)## 5.1 Model Traing <a id=\"5.1\"><\/a>","e67ccfed":"### 5.2.2 Optimization of Hyperparameters <a id=\"5.2.2\"><\/a>\nLet's start optimizing Hyperparameters of the models.\n\nFor optimization we used Grid Search to all the models with the hopes of optimizing their hyperparameters and thus improving their accuracy. Are the default model parameters the best bet? Let's find out.\n\nI choose only 7 models because **optimizing hyperparameters is time consuming** and but I like to would encourage you to optimize all the models.","1c128a00":"* *<b>Deviate from the normal distribution.<\/b>*\n* *<b>Have appreciable positive skewness.<\/b>*","b44dce94":"#### 5.2.2.3 Optimize KernelRidge\n Model Parameters description [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.kernel_ridge.KernelRidge.html)","e2e9212a":"### Create 3 functions for different Plotly plots.\nwe are going use plotly plots broadly in the notebook therefore  I would like to create functions for different Plotly plots.","a8c5dc35":"#### 5.2.2.7 Optimize XGboost Regressor\nModel parameters description [Here](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html)","87e9ed7a":"At the first sight light orange colored squares that get my attention, showing strong correlation between 'SalePrice' (depended variable) and  'TotalBsmtSF', 'OverallQual', 'GrLivArea', 'TotalBsmtSF', '1stFlrSF' 'GarageArea' etc( Independed variable).","1396a22d":"* ### 4.2.2 One Hot Encoding <a id=\"4.2.2\"><\/a>\nCategorical variables without any inherent order will be converted into numerical for our model using pandas get_dummies method.","22d03215":"***Only 'BldgType' categorical variable has the highest correlation with LotFrontage***","f11eccb5":"***NOTE: For computational restrictions and time limit, I won't optimize gb and lgb models. I also reckon some models like DT, KNN won't do any better after optimization since they have poor cross validation scores. If you have resources and time, I would encourage you to try to optimize these models yourself and see how they perform.***","de27ce9a":"*'TotalBsmt' and 'SalePrice' also showing **linear relationship**.It seems that at zero Total square feet of basement area there is the some sale price of the house.*\n\n***Outliers treatment***","b94bca5e":"# 4. Feature Engineering <a id=\"4\"><\/a>","0797a42a":"#### 5.2.2.4 Optimize Elastic Net\nModel parameter description [Here]()","5070e730":"## Correlation Matrix ","587af121":"**'LotFrontage' is remain to impute becuase:**\nAlmost 17% observations of LotFrontage are missing in. Hence, simply imputing LotFrontage by mean or median might introduce bias since the amount of missing values is significant. Again LotFrontage may differ by different categories of house. To solve this, we can group our data by some variables that have no missing values and for each subset compute the median LotFrontage to impute the missing values of it. This method may result in better accuracy without high bias, unless a missing value is expected to have a very high variance."}}