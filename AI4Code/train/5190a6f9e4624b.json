{"cell_type":{"9714dd0f":"code","a9eab9c9":"code","14be18d8":"code","bb8f990a":"code","16af63b7":"code","b740a79d":"code","8680bc4a":"code","141f040f":"code","b23ec458":"code","d92333a1":"code","44315840":"code","7241b49a":"code","2b944380":"code","8179830e":"code","3835db81":"code","b670f12a":"code","17d250ba":"code","17464350":"code","815df8af":"code","ab6ed00e":"code","09abcf1d":"code","f5e7b78a":"code","29dc2d4f":"code","6e34ef22":"code","d520d738":"code","e266f89a":"code","883e2bc8":"code","4914cafe":"code","bdcf1fc3":"code","b8294f6e":"code","f1a55aae":"code","fe3dbdb3":"code","d727ea98":"markdown","94a749da":"markdown","fcbe858d":"markdown","c70ea53d":"markdown","d3b274b9":"markdown","a50df0cf":"markdown","9df1c081":"markdown","13f26647":"markdown","c5fb9068":"markdown","cd2894b7":"markdown"},"source":{"9714dd0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nimport re\nimport time\nfrom string import punctuation\n\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom collections import namedtuple, defaultdict\n\nfrom sklearn.feature_extraction import stop_words\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, auc\n\nfrom sklearn.linear_model import LogisticRegression\nimport gc\n\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nfrom pprint import pprint","a9eab9c9":"def get_stats(original_function):\n    def wraps(*args, **kwargs):\n        df = original_function(*args, **kwargs)\n        print(\"nrows : %d\" % df.shape[0])\n        print(\"ncolumns : %d\" % df.shape[1])\n        return df\n    return wraps\n        \ndef log_time(original_function):\n    def wraps(*args, **kwargs):\n        begin = time.time()\n        results = original_function(*args, **kwargs)\n        print(\"Elapsed time %fs\" % (begin - time.time()))\n        return results\n    return wraps\n\n@get_stats\ndef read_csv(path):\n    return pd.read_csv(path)","14be18d8":"def sort_vocab(func):\n    def wraps(*args, **kwargs):\n        vectorizer, X = func(*args, **kwargs)\n        tfs = np.asarray(X.sum(axis=0)).ravel()\n        multiplier = -1\n        mask_inds = (multiplier * tfs).argsort()\n        \n        terms = list(vectorizer.vocabulary_.keys())\n        indices = list(vectorizer.vocabulary_.values())\n        labels = list()\n        for i, index in enumerate(mask_inds):\n            labels.append(terms[indices.index(index)])\n            \n        return labels, list(tfs[mask_inds])\n    return wraps\n\ndef fit_transform(original_function):\n    def wraps(*args, **kwargs):\n        corpus = original_function(*args, **kwargs)\n        X_train = corpus.apply(lambda doc: \" \".join(doc))\n        vect = CountVectorizer(lowercase=False).fit(X_train)\n        X = vect.transform(X_train)\n        return vect, X\n    return wraps\n\ndef limit_features(func):\n    def wraps(*args, **kwargs):\n        matches = func(*args, **kwargs)\n        pos = np.where(matches.apply(len) > 0)[0]\n        return matches[pos]\n    return wraps\n\n@sort_vocab\n@fit_transform\n@limit_features\ndef search_pattern(raw_documents, use_vect=True, token_pattern=None):\n    if use_vect:\n        pat = re.compile(token_pattern)\n        return raw_documents.apply(lambda doc: pat.findall(doc))\n    \n    return raw_documents.apply(lambda doc: \" \".join(doc))\n\ndef plot_barh(response, title='', color='m', N=5, xlim=None, figsize=(9, 8)):\n    '''Plots a horizontal bar chart'''\n    labels, values = response\n    fig, ax = plt.subplots(figsize=figsize)\n    y_pos = np.arange(N)\n    rects = ax.barh(y_pos, values[:N], color=color, align='center')\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(labels[:N], fontdict={'size': 15, 'color': 'b'})\n    ax.set_title(title, fontdict={'size': 20})\n    if xlim is not None:\n        ax.set_xlim(xlim)\n        \n    for rect in rects:\n        w = rect.get_width()\n        xloc = w - 10\n        yloc = rect.get_y() + (rect.get_height()\/2.0)\n        ax.text(xloc, yloc, \"%.5f\" % w if w < 1 else w, horizontalalignment='center', verticalalignment='center', color='white', weight='bold', fontdict={'size': 14}, clip_on=True)\n        \n    plt.show()","bb8f990a":"CloudParams = namedtuple('CloudParams', ['title', 'size', 'color'])\n\ndef prepare_cloud_data(corpus):\n    return \" \".join(corpus)\n\ndef prepare_cloud_data_from_tokens(corpus):\n    wordcloud = ' '\n    for item in np.hstack(corpus):\n        wordcloud = wordcloud + ' ' + item\n    return wordcloud\n\ndef plot_wordcloud(text, stops, params, figsize=(8, 6)):\n    stopwords = stop_words.ENGLISH_STOP_WORDS\n    if stops is not None:\n        stopwords = stopwords.union(stops)\n        \n    wordcloud = WordCloud(background_color ='white',\n                    stopwords = stopwords,\n                    random_state = 42).generate(text)\n\n    plt.figure(figsize=figsize)\n    plt.imshow(wordcloud)\n    plt.title(params.title, fontdict={\n        'size': params.size,\n        'color': params.color\n    })\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()","16af63b7":"def split_data(func):\n    def wrapper(*args, **kwargs):\n        X = func(*args, **kwargs)\n        X_train, X_test, y_train, y_test = train_test_split(X, args[1], test_size=0.2, random_state=42)\n        print(X_train.shape, y_train.shape)\n        return X_train, X_test, y_train, y_test\n    return wrapper\n\ndef fit_model(func):\n    def wrapper(*args, **kwargs):\n        clf = args[3]\n        X_train, X_test, y_train, y_test= func(*args, **kwargs)\n        \n        clf.fit(X_train, y_train)\n        return X_test, y_test, clf\n    return wrapper\n\ndef fit_vectorizer(func):\n    def wrapper(*args, **kwargs):\n        corpus = args[0]\n        vectorizer = args[2]\n        X = vectorizer.fit_transform(corpus)\n        print(\"Fitting vectorizer...\", X.shape)\n        display_stats(X, vectorizer, 10)\n        return X\n    return wrapper\n\ndef display_stats(X, vectorizer, N):\n    tfs = np.asarray(X.sum(axis=0)).ravel()\n    mask_inds = (-tfs).argsort()[:N]\n\n    vocab_values = list(vectorizer.vocabulary_.values())\n    terms = list(vectorizer.vocabulary_.keys())\n\n    labels = []\n    for i, j in enumerate(mask_inds):\n        labels.append(terms[vocab_values.index(j)])\n\n    plot_barh((labels, tfs[mask_inds]), title='Most occurred words', N=N, figsize=(8, 6))\n\ndef score_model(func):\n    def wrapper(*args, **kwargs):\n        X_test, y_true, clf = func(*args, **kwargs)\n        y_pred = clf.predict(X_test)\n        \n        TP = np.sum(y_pred[np.where(y_true.ravel() == 1)[0]])\n        TN = len(np.where(y_true.ravel() == 0)[0]) - np.sum(y_pred[np.where(y_true.ravel() == 0)[0]])\n        FP = np.sum(y_pred[np.where(y_true.ravel() == 0)[0]])\n        FN = len(np.where(y_true.ravel() == 1)[0]) - np.sum(y_pred[np.where(y_true.ravel() == 1)[0]])\n        \n        accuracy = (TP + TN) \/ len(y_true.ravel())\n        precision = TP \/ (TP + FP)\n        recall = TP \/ (TP + FN)\n        f1_score = 2 * ((precision * recall) \/ (precision + recall))\n        \n        response = (accuracy, precision, recall, f1_score)\n        plot_barh((['accuracy', 'precision', 'recall', 'F1'], response), N=4, figsize=(8, 4))\n        return response\n    return wrapper\n\n@score_model\n@fit_model\n@split_data\n@fit_vectorizer\ndef run_pipeline(raw_documents, target, vectorizer, model):\n    pass ","b740a79d":"def create_vocab(docs):\n    vocab = dict()\n    for tokens in docs:\n        for token in tokens:\n            if token not in vocab:\n                vocab[token] = 1\n            else:\n                vocab[token] += 1\n    return vocab\n\ndef find_docs_matching_pattern(docs, search):\n    occurrences = docs[docs.apply(len) > 0]\n    occurrences = occurrences.apply(lambda x: np.sum([search in x]))\n    pos = occurrences.index.values\n    return train_df.loc[pos[np.where(occurrences.values > 0)[0]], 'question_text']","8680bc4a":"!ls \"..\/input\"","141f040f":"train_df = read_csv(\"..\/input\/train.csv\")\nprint()\ntest_df = read_csv(\"..\/input\/test.csv\")\nprint()\nsubmission_df = read_csv(\"..\/input\/sample_submission.csv\")","b23ec458":"fig, ax = plt.subplots(figsize=(6, 4))\nlabels = ['Train', 'Test']\nsizes = [len(train_df), len(test_df)]\ncolors = ['#BB1AF7', \"#20DAFF\"]\n\npatches, texts, autotexts = ax.pie(sizes, explode=(0, 0.1), labels=labels, colors=colors, autopct='%1.1f%%')\nfor text in texts:\n    text.set_fontsize(20)\nax.axis('equal')\nplt.show()","d92333a1":"cloud_data = prepare_cloud_data(train_df.loc[train_df.target == 1, 'question_text'])\n\nplot_wordcloud(cloud_data, set(), CloudParams('Insincere', 30, 'b'))","44315840":"cloud_data = prepare_cloud_data(train_df.loc[train_df.target == 0, 'question_text'][:10000])\n\nplot_wordcloud(cloud_data, set(), CloudParams('Sincere', 30, 'b'))","7241b49a":"puncts = train_df.question_text.apply(lambda doc: [item for item in doc.split() if item in punctuation])\npunct_vocabulary = create_vocab(puncts)\npunct_vocabulary = sorted(punct_vocabulary.items(), key=lambda x: x[1], reverse=True)\n\nplot_barh(([i[0] for i in punct_vocabulary], [i[1] for i in punct_vocabulary]), title='Puncts', N=10, figsize=(8, 6))","2b944380":"ALL_PUNCTS = list(set([i[0] for i in punct_vocabulary]))\nnp.asarray(ALL_PUNCTS)","8179830e":"caps = train_df.question_text.apply(lambda row: [token for token in row.split() if re.match(r\"\\b[A-Z]{3,}\\b\", token) is not None])","3835db81":"fig, ax = plt.subplots(figsize=(5, 4))\nindex = np.arange(2)\nrects = ax.bar(index, [np.sum(caps.apply(len) == 0), np.sum(caps.apply(len) == 1)], width=0.35, color='m')\nax.set_xticks(index)\nax.set_xticklabels(['NO CAPS', 'HAS CAPS'])\nax.set_title('# CAPS', fontdict={'size': 15})\nplt.show()","b670f12a":"caps.apply(len).value_counts().sort_index()[::-1][:5]","17d250ba":"for i in range(3):\n    print(train_df.loc[np.random.choice(np.where(caps.apply(len) >= 10)[0]), 'question_text'])","17464350":"digits = train_df.question_text.apply(lambda row: [token for token in row.split() if token.isdigit()])\n\ndlens = train_df.question_text.apply(lambda row: np.sum([len(token) for token in row.split() if token.isdigit()]))","815df8af":"dlens.value_counts().sort_index()[:5]","ab6ed00e":"digits.apply(len).value_counts().sort_index()[::-1][:5]","09abcf1d":"years = train_df.question_text.apply(lambda row: [token for token in row.split() if re.match(r\"\\b20\\d{2}$\\b\", token) is not None])\n\nyears = years[years.apply(len) > 0]","f5e7b78a":"row = []\ncol = []\ndata = []\n\nvocabulary = defaultdict(int)\nvocabulary.default_factory = vocabulary.__len__\n\nfor i, row in enumerate(years):\n    feature_counter = {}\n    for token in row:\n        feature_indx = vocabulary[token]\n        if feature_indx not in feature_counter:\n            feature_counter[feature_indx] = 1\n        else:\n            feature_counter[feature_indx] += 1\n","29dc2d4f":"vocabulary","6e34ef22":"def replace_year(doc):\n    doc = re.sub(r\"19[0-9][0-9]\", r\"19_yy\", doc)\n    return doc\n\ndef replace_number_abbrv(doc):\n    doc = re.sub(r\"\\b(\\d{1,2})000\\b\", r\"\\1k\", doc)\n    return doc\n\ndef clean_text(doc):\n    doc = replace_year(doc)\n#     doc = replace_number_abbrv(doc)\n    return doc","d520d738":"filtered = train_df.copy()\nfiltered['cleaned_text'] = filtered.question_text.apply(lambda doc: clean_text(doc))","e266f89a":"stoplist = set('and'.split())\nvectorizer = CountVectorizer(max_features=10000, min_df=2, stop_words=stoplist)\nlr = LogisticRegression()","883e2bc8":"stoplist","4914cafe":"indx = np.random.choice(filtered.index.values)\nprint(filtered.loc[indx, 'question_text'])\nprint()\nprint(vectorizer.build_tokenizer()(filtered.loc[indx, 'cleaned_text']))","bdcf1fc3":"response = run_pipeline(filtered.cleaned_text, filtered.target, vectorizer, lr)","b8294f6e":"response","f1a55aae":"#(0.954238683127572, 0.6929009294047854, 0.4419778002018164, 0.5396996534462841)","fe3dbdb3":"print(vectorizer.get_feature_names()[:200])","d727ea98":"## Loading functions","94a749da":"### Numbers","fcbe858d":"## Modeling","c70ea53d":" # 1. Has data imbalance?","d3b274b9":"## Load the data","a50df0cf":"From the above example we can infer there are docs where the number of uppercase words are more than 20.","9df1c081":"> ### Punctuations","13f26647":"# 2. Wordcloud","c5fb9068":"> ### UpperCaps","cd2894b7":"# 3. Generate features"}}