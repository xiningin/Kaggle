{"cell_type":{"d2b970ef":"code","af217dd8":"code","7e4b9dcc":"code","656752bc":"code","bc0258d5":"code","768b9d2e":"code","88db7b3b":"code","341ea5a0":"code","e66a7612":"code","cf588432":"code","d80a4280":"code","674abc0e":"code","bedcc3a2":"code","2b6695ae":"code","c3bb11ae":"code","0ad6c297":"code","78d6420c":"markdown","8b0c4a3e":"markdown","836cd630":"markdown","7eef2cee":"markdown","8668bfbc":"markdown","055287a4":"markdown","c007c994":"markdown","13cf55e8":"markdown","b93d6dc8":"markdown","e4e3fdff":"markdown","5e282cbd":"markdown","b49bf680":"markdown","0a8e933c":"markdown","d3e6b53d":"markdown","cea3c7f7":"markdown","afa5c061":"markdown","71b720e4":"markdown"},"source":{"d2b970ef":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly.tools import FigureFactory as ff\n\nwarnings.filterwarnings('ignore')","af217dd8":"data = pd.read_csv('..\/input\/data.csv')","7e4b9dcc":"data.head()","656752bc":"data = data[['Video Uploads', 'Subscribers', 'Video views']]","bc0258d5":"def plot_line(X, y, theta):\n    p1 = go.Scatter(x = X, y = y,mode='markers', marker=dict(color='black'))\n    p2 = go.Scatter(x=np.array([0, max(X)]), \n                    y=np.array([[1,0],[1,max(X)]]).dot(theta),\n                    mode='lines', line=dict(color='blue', width=3))\n    fig = go.Figure(data=[p1, p2])\n    py.iplot(fig)\n    \ndef plot(X, y, mode='markers', title='Plot title', x_axis='X axis', y_axis='Y axis'):\n    p1 = go.Scatter(x = X, y = y,mode=mode, marker=dict(color='black'))\n    layout = go.Layout(\n                    title=title,\n                    xaxis=dict(title=x_axis),\n                    yaxis=dict(title=y_axis)\n                )\n    fig = go.Figure(data=[p1], layout=layout)\n    py.iplot(fig)\n    \ndef plot_scatter_matrix(data):\n    fig = ff.create_scatterplotmatrix(data, height=1000, width=1000, title='Scatterplot Matrix')\n    py.iplot(fig)","768b9d2e":"plot_scatter_matrix(data)","88db7b3b":"data = data[['Subscribers','Video views']].apply(pd.to_numeric, errors='coerce').dropna()\nX_train, y_train= data['Subscribers'], data['Video views']","341ea5a0":"def normalize(matrix):\n    means = matrix.mean(axis=0)\n    maxs = matrix.max(axis=0)\n    mins = matrix.min(axis=0)\n    return ((matrix - means) \/ (maxs - mins),means, maxs,mins)\n\ndef normalize_data(matrix, means, maxs, mins):\n    return (matrix - means) \/ (maxs - mins)","e66a7612":"matrix_train = np.column_stack((X_train,y_train))\nmatrix_train_norm, means, maxs, mins = normalize(matrix_train)\nX_train = matrix_train_norm[:,0]\ny_train = matrix_train_norm[:,1]","cf588432":"X_train.shape, y_train.shape ","d80a4280":"def cost_function(X, y, theta):\n    m = len(y)\n    return 1\/(2*m) * sum(np.square((np.dot(X,theta) - y)))    ","674abc0e":"def gradient_descent(X, y, alpha=0.1, num_iters=100):\n    theta = np.zeros(2)\n    X = np.column_stack((np.ones(X.shape[0]),X))\n    m = len(y)\n    J_hist = np.zeros(num_iters)\n    theta_hist = np.zeros((num_iters,2))\n    for i in range(0,num_iters):\n        prediction = X.dot(theta)\n        \n        theta = theta - alpha*1\/m * X.T.dot((prediction-y))\n        J_hist[i] = cost_function(X, y, theta)\n        theta_hist[i] = theta.T\n        diff = 1 if i == 0 else J_hist[i-1] - J_hist[i]\n    return (theta, J_hist, theta_hist)","bedcc3a2":"theta_grad, J_hist, theta_hist = gradient_descent(X_train, y_train, 0.2,3000)\ntheta_grad_large, J_hist_large, theta_hist_large = gradient_descent(X_train, y_train,10,19)\ntheta_grad_small, J_hist_small, theta_hist_small = gradient_descent(X_train, y_train, 0.02,5000)\n\nprint(theta_grad)\nplot(np.arange(len(J_hist)),J_hist,mode='lines'\n     , title='Cost historic with learning rate = 0.2'\n     , x_axis='number of iteration'\n     , y_axis='Cost')\nplot(np.arange(len(J_hist_large)),J_hist_large,mode='lines'\n     , title='Cost historic with learning rate = 10'\n     , x_axis='number of iteration'\n     , y_axis='Cost')\nplot(np.arange(len(J_hist_small)),J_hist_small,mode='lines'\n     , title='Cost historic with learning rate = 0.02'\n     , x_axis='number of iteration'\n     , y_axis='Cost')","2b6695ae":"def normal_equation(X, y):\n    X = np.column_stack((np.ones(X.shape[0]),X))\n    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)","c3bb11ae":"theta_norm = normal_equation(X_train,y_train)\nprint(theta_norm)","0ad6c297":"from sklearn import linear_model\nregr = linear_model.LinearRegression(normalize=True)\n\nregr.fit(X_train.reshape(-1, 1), y_train)\n\ntheta_sklearn = np.array([regr.intercept_,regr.coef_])\nprint('Theta for gradient descent : ',theta_grad)\nprint('Theta for normal equation  : ',theta_norm)\nprint('Theta for sklearn          : ',theta_sklearn)\nplot_line(X_train,y_train,theta_grad)\nplot_line(X_train,y_train,theta_norm)\nplot_line(X_train,y_train,theta_sklearn)","78d6420c":"### <a id=\"3.2\">3.2 Gradient Descent<\/a>","8b0c4a3e":"Now let's dive into a famous algorithm : **Gradient Descent**. Its goal is to minimize the cost function $J(\\theta)$ . Since the only parameter of this function is $\\theta$ we have to find the optimal values of $\\theta$.\n\nThe algorithm is the following :\n\n$Reapeat\\space until\\space converge : \\{$\n$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$$\n$\\}$\n\nHere $\\alpha$ is the learning rate, we use it to go quicker or slower to the optimal value (be carefull if $\\alpha$ is to big it will not converge, and if it is to small it will take a lot of time to converge so you have to take a value that is not too big and not to small). \n\nThe derivative term bring us a great information : which \"direction\" to take to converge. The partial derivative of $J(\\theta)$ is $\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)}) x_j^{(i)}$ but we can simplify this with linear algebra so that the partial derivative is $\\frac{1}{m} X^T(h_\\theta(X)-y)$\n\nSo now we can rewrite our formula like that : \n$$\\theta_j := \\theta_j - \\alpha  \\frac{1}{m} X^T(h_\\theta(X)-y) $$\n","836cd630":"### <a id=\"3.4\">3.4 Compare with sklearn<\/a>\n\nNow to be sure I didn't make anything stupid, I prefer to check my value with the value that returns sklearn library. \ud83d\ude09","7eef2cee":"I define **3 plot functions** : one for the linear regression, one to plot the cost function's historic and one for the scatter matrix","8668bfbc":"Honestly, I choose this dataset just because it have some numerical column easy to use, but let's see if the data can show us some correlation for the linear regression.","055287a4":"As you can tell by this part's title, it's time to convert our dataset into a matrix. But with this dataset we have a little problem if we want to use linear regression with gradient descent (it will be explained in the 3.2 part) : the value are too big ! So if we want to the model to work we need to normalize the data.\n\nTo do that we need to use [Feature scaling](https:\/\/en.wikipedia.org\/wiki\/Feature_scaling) with this formula :\n\\begin{align}\nX'= \\frac{X - X_{min}} {X_{max} - X_{min}}\n\\end{align}\n\nWhere $X'$ is the new value, $X_{min}$ and $X_{max}$ the minimum and maximum value for the feature.\n\nThis will center the values around 0.\n","c007c994":"So let's run our first algorithm with our values, I will also train 2 others models with large and small alpha values so we can see the evolution of the cost by iteration. ","13cf55e8":"So we can see that the feature of *Video Uploads* is not really useful if we want to do a linear regression.\n\nFor the model that I will try to make, the goal is to predict the number of *Video Views* by getting the number of *Subscribers*.\n\nBecause the purpose of this kernel is to know the maths behind Linear Regression I will take **the entire dataset to train my models**.","b93d6dc8":"### <a id=\"1.2\">1.2 Read the data<\/a>","e4e3fdff":"## <a id=\"3\"> 3. Linear Regression<\/a>\n\nBefore introducing the model we have to know how the matrix look like and what is our hypothesis : the function we will use to predict the wanted value.\n\nWe have a matrix $X$ with $m$ rows and $n$ columns that is : $X = \\begin{bmatrix} x^1_1 & x^2_1 & ... & x^n_1 \\\\ x^1_2 & x^2_2 & ... & x^n_2 \\\\ ... & ... & ... & ... \\\\ x^1_m & x^2_m & ... & x^n_m \\end{bmatrix}$ \n and $Y$ have $m$ rows and 1 column : $Y = \\begin{bmatrix} y^1 \\\\ y^2 \\\\... \\\\ y^n \\end{bmatrix}$\n \n Linear regression hypothesis is the next : $h_\\theta(X) = \\theta_0x_0 + \\theta_1x_1 + ... + + \\theta_nx_n = \\theta^TX$\n \nWhere $x = \\begin{bmatrix} x^0 \\\\x^1 \\\\ ... \\\\ x^n \\end{bmatrix}$  and $\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ ... \\\\ \\theta_n \\end{bmatrix}$\n\nYou can see that $X$ have a new feature : $x^0$ this is a vector that have only 1 value because with that we can get the intercept : $\\theta_0$\n\n\n\n\nGreat ! Know we can make our model ! \ud83d\udc4d\n\n### <a id=\"3.1\">3.1 Cost function<\/a>\n\nThe cost function is the function that calculate the error so we need the result to be small at the end. Here it's the sum of squared error that we will use. The formula is : \n$$J(\\theta) = \\frac{1}{2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2$$\n","5e282cbd":"As we can see for the first model, with an alpha value of 0,02 it converges after aproximatly 2000 iterations. The second model start to increase after 17 iterations (it's not good) and for the last one even after 5000 iterations it don't converge.\n\n\n### <a id=\"3.3\">3.3 Normal Equation<\/a>\n\nI discover in the class from Coursera that for linear regression it exists an equation that can directly give the optimal $\\theta$ : the normal equation.\n\nHere is the formula :\n$$\\theta = (X^TX)^{-1}X^Ty$$\n\nSo I code it to compare with my other theta found with gradient descent.","b49bf680":"# The mathematics of Linear Regression\n*October 2018*\n***\n\nHi everyone ! \n\nI just finished the [Coursera Machine Learning course from Andrew Ng](https:\/\/www.coursera.org\/learn\/machine-learning) and what I liked a lot is that by coding the model in Octave just with the math formula I could **understand how it works** ! \n\nSo I will do the same with different models and to start, let's begin with the classic Linear Regression !\n\nThe goals of this kernel for me is :\n* to learn how to write a model in python just with numpy linear algebra library\n* to improve my writing skills (in English and with Markdown)\n* to have a better sense of the notebooks\n\nI hope you will enjoy it and if you have any suggestions : I will be glad to hear them to improve my skills !\n\n![](http:\/\/www.bridgeport.edu\/wp-content\/uploads\/2016\/04\/UB-Mathematics-720x390.jpg)\n\n\n## Table of content\n\n* [1. Load libraries and read data](#1)\n    * [1.1 Load libraries](#1.1)\n    * [1.2 Read the data](#1.2)\n    * [1.3 A quick look on the data](#1.3)\n    \n    \n* [2. Prepare data to linear algebra (normalization)](#2)\n\n\n* [3. Linear Regression](#3)\n    * [3.1 Cost function](#3.1)\n    * [3.2 Gradient Descent](#3.2)\n    * [3.3 Normal Equation](#3.3)\n    * [3.4 Compare with sklearn](#3.4)\n    \n    \n## <a id=\"1\">1. Load libraries and read data<\/a>\n\n### <a id=\"1.1\">1.1 Load libraries<\/a>","0a8e933c":"## <a id=\"2\">2. Prepare data to linear algebra (normalization)<\/a>","d3e6b53d":"Now we have 2 vectors : *X_train* and *y_train* with a size of $m\\times 1$. Where $m$ is the number of rows so here it's 3090 and $n$ is the number of columns so 1 here.\n\nIt's important to know that *X_train* could have many features, we denote this number with $n$ so *X_train* is a $m \\times n$ matrix.","cea3c7f7":"Yes, all the $\\theta$ are similar ! \ud83d\udc4d\n\nSo I hope that you enjoyed this kernel, please feel free to say what you though about my work ! \n\n\nThanks for reading,\n\n$Nathan.$\n\n\n\nSources :\n* [Coursera Machine Learning course from Andrew Ng](https:\/\/www.coursera.org\/learn\/machine-learning)\n* [CS229 Lecture notes](http:\/\/cs229.stanford.edu\/notes\/cs229-notes1.pdf)\n* [Normalization (statistics) Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Normalization_(statistics))\n","afa5c061":"Let's only take the last 3 columns with numerical values : *Video Uploads, Subscribers and Video views.*","71b720e4":"### <a id=\"1.3\">1.3 A quick look on the data<\/a>"}}