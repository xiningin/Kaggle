{"cell_type":{"e024b658":"code","0e54235f":"code","096911eb":"code","6a35dbd5":"code","14ed4e29":"code","93a45b26":"code","e2941552":"code","c0ff1c76":"code","51b0c3da":"code","6670899b":"code","d5eef44b":"code","05f329ad":"code","3b87b274":"code","3d9db13b":"code","536269a8":"code","d0783915":"code","707b51d4":"code","3a37b604":"code","c6c67b56":"code","a9aa01bf":"code","276c6d7f":"code","97412417":"code","c80f557a":"code","a541e37b":"code","f0379fd6":"code","0f71eaad":"code","d3250c2f":"code","b0e32059":"code","db8f356c":"code","92c57436":"code","49697258":"code","a8ebb0d1":"code","3c1a92c1":"code","6833b4f5":"code","80369a80":"code","989bb8f5":"code","cffa987a":"code","edfaa5d3":"code","e0aafc8f":"code","7020a34e":"code","5f9cde96":"code","6214f70d":"code","402931ad":"code","2d95f7c0":"code","6253a461":"code","406eaed7":"code","f80d59a4":"code","d149fede":"code","37ab2be3":"code","ed381c89":"code","d456a816":"code","4d4a55c4":"code","d091e8bb":"code","452f914f":"code","4d7fd866":"code","22717b8f":"code","b5b16b41":"code","a9971fc7":"code","f56efade":"code","db5c0203":"code","235db1a5":"code","9b0d7808":"code","48420274":"code","e50c1c98":"code","6f02c6b7":"code","91047a98":"code","8e9ca19a":"code","96f2da68":"code","b3b19da3":"code","0259ae5e":"code","42595824":"code","08a42f7c":"code","7783a09a":"code","9e95617b":"code","9b6540ca":"code","2d2e89f1":"code","4fc58ad2":"code","6fe6879b":"code","2795793b":"code","01658975":"code","0cc5602c":"code","a91d9d0b":"code","3e14adbf":"code","186b0a9b":"code","ac4fb140":"code","77488fde":"code","00c153ab":"code","8d70ac41":"code","3b4852d0":"code","75bfa340":"code","703b1b3c":"code","d9b58a29":"code","eb18ab5d":"code","85a0426f":"code","b0485e26":"code","4fa43fbb":"code","895b2047":"code","58a82758":"code","eac521ec":"code","6186e71b":"code","cfa8a210":"code","9d1f01ff":"code","0adc788a":"code","134f726e":"code","8aefd903":"code","75245210":"code","d2530085":"code","ffed30b0":"code","1ce00392":"code","27c92f0f":"code","32ca732b":"code","a66ba62a":"markdown","85a7564b":"markdown","038a9a4d":"markdown","44158f72":"markdown"},"source":{"e024b658":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\n# for the model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\n\n# for feature engineering\nfrom feature_engine import missing_data_imputers as mdi\nfrom feature_engine import discretisers as dsc\nfrom feature_engine import categorical_encoders as ce\nfrom imblearn.over_sampling import SMOTE","0e54235f":"# Data has 2260668 observations so choosing a sample. Common home computers may give memory error with 2260668 rows\n# Taking a small representative random sample. If you have better machines, you can increase the sample size\n\n\n\nimport random\n\n# Count the lines in origenal data 'loan.csv'\nnum_lines = 2260668\n\n# Sample size \nsize = int(num_lines \/110)\n\n# The row indices to skip - make sure 0 is not included to keep the header!\nskip_idx = random.sample(range(1, num_lines), num_lines - size)\n\n# Read the data\ndata = pd.read_csv(\"loan.csv\", skiprows=skip_idx)\n\ndf = data.copy()\nprint(df.shape)","096911eb":"print(df.columns)","6a35dbd5":"df.dtypes\n# We will print the first few of df little later to get a feel of the dataset","14ed4e29":"# Descrete variables\ndiscrete = [var for var in df.columns if df[var].dtype!='O' and var!='loan_status' and df[var].nunique()<10]","93a45b26":"# Continuous\ncontinuous = [var for var in df.columns if df[var].dtype!='O' and var!='loan_status' and var not in discrete]\n","e2941552":"discrete","c0ff1c76":"continuous","51b0c3da":"# Categorical variables. Date included \ncategorical = [var for var in df.columns if df[var].dtype=='O' ]","6670899b":"categorical","d5eef44b":"# Print first few rows\ndf.head()","05f329ad":"# Shape of the dataset\ndf.shape\n","3b87b274":"# Date variables are seperated \ndates = []\nfor i in categorical:\n    if (re.search(\"_d$\", i) != None) or (re.search(\"_date$\", i)):\n        dates.append(i)\nprint (dates)\n   ","3d9db13b":"dates","536269a8":"# Pure categorical variables without date variables  \npure_categorical = [var for var in categorical if var not in dates]","d0783915":"pure_categorical","707b51d4":"# Number of each variable types in the origenal loan.csv\nprint('There are {} discrete variables'.format(len(discrete)))\nprint('There are {} continuous variables'.format(len(continuous)))\nprint('There are {} categorical variables'.format(len(pure_categorical)))\nprint('There are {} date variables'.format(len(dates)))","3a37b604":"# Missing Variables\n# Number of missing variables\n\ndf.isnull().sum()","c6c67b56":"# % of missing variables\nNull_Percent = df.isnull().mean()\ndf_null_percent = pd.DataFrame(Null_Percent, columns=['percent_null'])\ndf_null_percent['percent_null'][df_null_percent['percent_null']>0]","a9aa01bf":"# Percentage of null in pure_categorical columns. Columns above 90% null values \npure_cat_null_majo = pd.DataFrame(df[pure_categorical].isnull().mean()*100, columns=['cat_null_90'])\npure_cat_null_majo_clms = pure_cat_null_majo['cat_null_90'][pure_cat_null_majo['cat_null_90']>90]\nlst_cat_90 = pure_cat_null_majo_clms.index\nprint(pure_cat_null_majo_clms)\nprint(lst_cat_90)","276c6d7f":"# Percentage of null in discrete columns. Columns above 90% null values \npure_dis_null_majo = pd.DataFrame(df[discrete].isnull().mean()*100, columns=['dis_null_90'])\npure_dis_null_majo_clms = pure_dis_null_majo['dis_null_90'][pure_dis_null_majo['dis_null_90']>90]\nlst_dis_90 = pure_dis_null_majo_clms.index\nprint(pure_dis_null_majo_clms)\nprint(lst_dis_90)","97412417":"# Percentage of null in continuous columns. Columns above 90% null values \npure_con_null_majo = pd.DataFrame(df[continuous].isnull().mean()*100, columns=['con_null_90'])\npure_con_null_majo_clms = pure_con_null_majo['con_null_90'][pure_con_null_majo['con_null_90']>90]\npure_con_null_majo_clms\nlst_con_90 = pure_con_null_majo_clms.index\nprint(pure_con_null_majo_clms)\nprint(lst_con_90)","c80f557a":"# Percentage of null in continuous columns. Columns above 90% null values \npure_date_null_majo = pd.DataFrame(df[dates].isnull().mean()*100, columns=['date_null_90'])\npure_date_null_majo_clms = pure_date_null_majo['date_null_90'][pure_date_null_majo['date_null_90']>90]\npure_date_null_majo_clms\nlst_date_90 = pure_date_null_majo_clms.index\nprint(pure_con_null_majo_clms)\nprint(lst_date_90)","a541e37b":"# Drop all the columns from the origenal dataframe, which are null fo > 90% of the rows\ndf.drop(list(lst_dis_90), axis=1, inplace=True)\ndf.drop(list(lst_date_90), axis=1, inplace=True)\ndf.drop(list(lst_cat_90), axis=1, inplace=True)\ndf.drop(list(lst_con_90), axis=1, inplace=True)","f0379fd6":"# Shape of df after dropping all columns with above 90% null values \ndf.shape\n# After dropping all columns > 90% null, we are left with 107 columns. Origenally we had 145 columns","0f71eaad":"# Category types and their % in y. In this attempt we are converting the problem into a simple 0 ot 1 type\n# cllassification problem\ndf['loan_status'].value_counts()\/len(df['loan_status'])","d3250c2f":"# Preparing y for categorical predictions\nloan_status_mapping_dict = {'Fully Paid' : 0, \n                            'Current' : 0,\n                            'Charged Off': 1,\n                            'Late (31-120 days)': 0,\n                            'In Grace Period'  : 0,\n                            'Late (16-30 days)': 0,\n                            'Does not meet the credit policy. Status:Fully Paid':0,\n                            'Does not meet the credit policy. Status:Charged Off':0,\n                            'Default':1\n                            }\n\nloan_status_mapping_dict","b0e32059":"# Map load_status\ndf['loan_status_mapped'] = df['loan_status'].map(loan_status_mapping_dict)","db8f356c":"# Percentage of default and no default\ndf['loan_status_mapped'].value_counts()\/len(df['loan_status_mapped'])","92c57436":"# Length of y\nprint(len(df['loan_status_mapped']))","49697258":"# 88% are 0s, so it's a highly unblalanced y. We have to apply special techniques to balance these two classes in y\n# We will do sampling only after treating missing values and categorical encoding\n\n\n# Dividing the dataset into train and test with 30% as test\nX_train, X_test, y_train, y_test = train_test_split(df.drop(['loan_status','loan_status_mapped'], axis=1), df['loan_status_mapped'], test_size = 0.3, random_state = 101) \n  \n# describes info about train and test set \nprint(\"Number transactions X_train dataset: \", X_train.shape) \nprint(\"Number transactions y_train dataset: \", y_train.shape) \nprint(\"Number transactions X_test dataset: \", X_test.shape) \nprint(\"Number transactions y_test dataset: \", y_test.shape) \nprint('\\n')\nprint(\"Before OverSampling, counts of label '1' in train: {}\".format(sum(y_train == 1))) \nprint(\"Before OverSampling, counts of label '0' in train: {} \\n\".format(sum(y_train == 0))) \n  ","a8ebb0d1":"# Seperate variable types in X_train\ndis_train = [var for var in discrete if var not in lst_dis_90]\ncon_train = [var for var in continuous if var not in lst_con_90]\npure_categorical_train = [var for var in pure_categorical if var not in lst_cat_90]\ndates_train = [var for var in dates if var not in lst_date_90]\n","3c1a92c1":"print('Discrete variables in train dataset', len(dis_train),'   ' , dis_train)\nprint('\\n')\nprint('Continous variables in train dataset', len(con_train),'   ' ,con_train)\nprint('\\n')\nprint('Categorical variables in train dataset', len(pure_categorical_train), '   ' ,pure_categorical_train)\nprint('\\n')\nprint('Date variables in train dataset', len(dates_train),'   ' ,dates_train)","6833b4f5":"# Finding rare categories in pure categorical variables. Continued in the following cells\nX_train.shape\nX_train.columns\nX_train[['term', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', 'verification_status', 'pymnt_plan', 'purpose', 'title', 'zip_code', 'addr_state', 'earliest_cr_line', 'initial_list_status', 'application_type', 'hardship_flag', 'disbursement_method', 'debt_settlement_flag']].head()\n","80369a80":"X_train[['term', 'grade', 'sub_grade', 'emp_title', 'emp_length', \n         'home_ownership', 'verification_status', 'pymnt_plan', \n         'purpose', 'title', 'zip_code', 'addr_state', 'earliest_cr_line', \n         'initial_list_status', 'application_type', 'hardship_flag', \n         'disbursement_method', 'debt_settlement_flag']].nunique()\n","989bb8f5":"# There are very high number of ctegories in some of the variables \n#sub_grade, emp_title, purpose, title, zip_code, addr_state, earliest_cr_line\n\n# later we will classify strings occuring <4% as rare category\n\n# Finding category percentages of each category in each of these features\ndef FindCategoryFractions(var):\n     var_fraction = ((X_train[var].value_counts() \/ len(X_train)).sort_values())*100\n\n     return var_fraction\n     \n\nFindCategoryFractions('sub_grade')","cffa987a":"FindCategoryFractions('emp_title')","edfaa5d3":"FindCategoryFractions('purpose')","e0aafc8f":"FindCategoryFractions('title')","7020a34e":"# zip_code, addr_state, earliest_cr_line are the remaining tiles\nFindCategoryFractions('zip_code')","5f9cde96":"FindCategoryFractions('addr_state')","6214f70d":"FindCategoryFractions('earliest_cr_line')","402931ad":"# Dropping the columns with very high cardinality from train and test. Total three columns are being dropped \nX_train.drop(['earliest_cr_line','zip_code','emp_title'], axis=1, inplace=True)\nX_test.drop(['earliest_cr_line','zip_code','emp_title'], axis=1, inplace=True)","2d95f7c0":"# Reminding train and test shape\nprint('Train Shape', X_train.shape)\nprint('Test Shape', X_test.shape)","6253a461":"# Cardinality is still high in the features sub_grade, purpose, title, addr_state but still managiable\n# We will engineer these features by labelling <4% occuring categories as rare\n#Before that we do some more necessary steps","406eaed7":"# Let's have a look at the descrete variables \n# Discrete variables in train dataset 4     ['policy_code', 'acc_now_delinq', 'num_tl_120dpd_2m', 'num_tl_30dpd']\nX_train[ ['policy_code', 'acc_now_delinq', 'num_tl_120dpd_2m', 'num_tl_30dpd']].head()","f80d59a4":"X_train[ ['policy_code', 'acc_now_delinq', 'num_tl_120dpd_2m', 'num_tl_30dpd']].tail()","d149fede":"# Looks like all the descrete variables contain numerics and some null values.\n# After imputing the null values, they should be ready for ML models","37ab2be3":"# Let's look at the null value treatment now for X_Train\nX_Train_null_features = pd.DataFrame(X_train.isnull().mean(), columns = ['X_train_null'])\nX_Train_null_features\n#X_Train_null_features['X_train_null'][X_Train_null_features['X_train_null']>0]","ed381c89":"X_Train_null_features_lst = list(X_Train_null_features['X_train_null'][X_Train_null_features['X_train_null']>0].index)\nX_Train_null_features_lst\n# The following columns have some null values in it","d456a816":"# Seperating features with null values as per their data type\nX_Train_null_cat = [var for var in X_Train_null_features_lst if var in pure_categorical_train]\nX_Train_null_dis = [var for var in X_Train_null_features_lst if var in dis_train]\nX_Train_null_con = [var for var in X_Train_null_features_lst if var in con_train]\nX_Train_null_dates = [var for var in X_Train_null_features_lst if var in dates_train]","4d4a55c4":"# Category columns we are left with that have some null values\nX_Train_null_cat","d091e8bb":"# Treating date variables. Continued in the following cells","452f914f":"X_train[dates_train].info()","4d7fd866":"# Converting all date objects into date-time nvariables \nX_train['issue_d'] = pd.to_datetime(X_train['issue_d'])\nX_train['last_pymnt_d'] = pd.to_datetime(X_train['last_pymnt_d'])\nX_train['next_pymnt_d'] = pd.to_datetime(X_train['next_pymnt_d'])\nX_train['last_credit_pull_d'] = pd.to_datetime(X_train['last_credit_pull_d'])\n\nX_test['issue_d'] = pd.to_datetime(X_test['issue_d'])\nX_test['last_pymnt_d'] = pd.to_datetime(X_test['last_pymnt_d'])\nX_test['next_pymnt_d'] = pd.to_datetime(X_test['next_pymnt_d'])\nX_test['last_credit_pull_d'] = pd.to_datetime(X_test['last_credit_pull_d'])","22717b8f":"X_test[dates_train].info()","b5b16b41":"X_train[dates_train].head()","a9971fc7":"X_train['last_credit_pull_d'].value_counts()","f56efade":"# Create new features by extracting quarter and year from the date time variables. \n\nX_train['issue_d_qtr']= X_train['issue_d'].dt.quarter\nX_train['issue_d_year']= X_train['issue_d'].dt.year\n\nX_train['last_pymnt_d_qtr']= X_train['last_pymnt_d'].dt.quarter\nX_train['last_pymnt_d_year']= X_train['last_pymnt_d'].dt.year\n\nX_train['next_pymnt_d_qtr']= X_train['next_pymnt_d'].dt.quarter\nX_train['next_pymnt_d_year']= X_train['next_pymnt_d'].dt.year\n\nX_train['last_credit_pull_d_qtr']= X_train['last_credit_pull_d'].dt.quarter\nX_train['last_credit_pull_d_year']= X_train['last_credit_pull_d'].dt.year\n\n# Date variables in train and test ['issue_d','last_pymnt_d','next_pymnt_d','last_credit_pull_d']","db5c0203":"X_test['issue_d_qtr']= X_test['issue_d'].dt.quarter\nX_test['issue_d_year']= X_test['issue_d'].dt.year\n\nX_test['last_pymnt_d_qtr']= X_test['last_pymnt_d'].dt.quarter\nX_test['last_pymnt_d_year']= X_test['last_pymnt_d'].dt.year\n\nX_test['next_pymnt_d_qtr']= X_test['next_pymnt_d'].dt.quarter\nX_test['next_pymnt_d_year']= X_train['next_pymnt_d'].dt.year\n\nX_test['last_credit_pull_d_qtr']= X_test['last_credit_pull_d'].dt.quarter\nX_test['last_credit_pull_d_year']= X_test['last_credit_pull_d'].dt.year\n\n# Drop the origenal date variables from X_Train and X_test\ndate_var_drop =  ['issue_d','last_pymnt_d','next_pymnt_d','last_credit_pull_d']\nX_train.drop(date_var_drop, axis=1, inplace=True)\nX_test.drop(date_var_drop, axis=1, inplace=True)\n# New date deriaved variables - ['issue_d_qtr','issue_d_year','last_pymnt_d_qtr','last_pymnt_d_year','next_pymnt_d_qtr','next_pymnt_d_year','last_credit_pull_d_qtr','last_credit_pull_d_year']","235db1a5":"# Random check\nX_test['last_credit_pull_d_qtr'].head()\n","9b0d7808":"X_test['last_credit_pull_d_year'].tail()","48420274":"X_train.head()","e50c1c98":"# Newly derieved variables from original train and test date objects\ndate_derived_car = ['issue_d_qtr','issue_d_year','last_pymnt_d_qtr','last_pymnt_d_year','next_pymnt_d_qtr','next_pymnt_d_year','last_credit_pull_d_qtr','last_credit_pull_d_year']\nX_train[date_derived_car].head()","6f02c6b7":"# Imputing null places in date_derived_car \nX_train[date_derived_car].isnull().sum()","91047a98":"date_derived_car_with_null =  ['last_pymnt_d_qtr','last_pymnt_d_year', 'next_pymnt_d_qtr','next_pymnt_d_year']\n# These are the variables with some null values in it","8e9ca19a":"# Transforming imputations for missing data. Let's have a relook at the shates of train and test\nprint(X_train.shape)\nprint(X_test.shape)","96f2da68":"# Preparing to inpute with Feature-Engine library.\n# If you have not done already, you need to install it  \npipe = Pipeline([('imuter_discrete', mdi.ArbitraryNumberImputer(arbitrary_number=-1, variables=X_Train_null_dis)),\n                 ('imuter_continuous', mdi.ArbitraryNumberImputer(arbitrary_number=-1, variables=X_Train_null_con)),\n                 ('imuter_categorical',mdi.CategoricalVariableImputer(variables=X_Train_null_cat)),\n                 ('imputer_dates', mdi.ArbitraryNumberImputer(arbitrary_number=-1, variables=date_derived_car_with_null))\n                ])","b3b19da3":"#Fit X_train to this newly created imputing pipe\npipe.fit(X_train)","0259ae5e":"#Now we can thansform X_train using this pipe object. It will fill up the missing values for us\ntemp = pipe.transform(X_train)","42595824":"temp.shape","08a42f7c":"# Check for null values in tnanformed train dataset\ntemp.isnull().sum()","7783a09a":"########## CHEERS IT LOOKS LIKE WE HAVE NO NULL VALUES LEFT IN ANY OF COLUMNS. EVERYTHING WENT AS PLANNED ############\n# WE CAN PERFORM ADDITIONAL STEPS TO CHECK IF THERE ARE STILL ANY COLUMNS WITH NULL VALUES\n# BUT WE WILL STOP HERE AS ABOVE OUTPUT SHOWS NO APPARENT NULL VALUES","9e95617b":"# Imputing for missing variables in X_test\ntemp_test = pipe.transform(X_test)","9b6540ca":"temp_test.shape","2d2e89f1":"# Check for null values in tnanformed test dataset\ntemp_test_null_df = pd.DataFrame(temp_test.isnull().sum(), columns=['testnull'])","4fc58ad2":"temp_test_null_df['testnull'][temp_test_null_df['testnull']>0]\n# It will show columns with stiil having null values. There is one but with null vaules still","6fe6879b":"# Looks like strange. Only two columns are left with one null value each\n# Let's investigate\nX_test[['last_credit_pull_d_qtr','last_credit_pull_d_year']][X_test['last_credit_pull_d_qtr'].isnull()]\n# the output is not shown below as I had rerum after deliting index with nulll row\n# So when you will run from the beginnning it will definitely show up","2795793b":"# The single observation number 18553 has both the null values.\n# It might be because of some case that is there in test and not come in train. It's just a speculation however\n# For now I am going to delete the observation number 18553 from the test\nX_test.drop(18553, inplace = True)","01658975":"# Now looks like there are no null values left. Now let's have a relook at train \n\ntemp_train_null_df = pd.DataFrame(temp.isnull().sum(), columns=['testnull'])\ntemp_train_null_df['testnull'][temp_train_null_df['testnull']>0]","0cc5602c":"# Now there are no null values left in test. Now let's have a relook at test\n#temp.isnull().sum()\ntemp_test_null_df = pd.DataFrame(temp_test.isnull().sum(), columns=['testnull'])\ntemp_test_null_df['testnull'][temp_test_null_df['testnull']>0]","a91d9d0b":"# X_train after transformation is represented by temp\n# X_test after transformation is represented by temp_test\n# In the above two cells, we have varified both do not have any null values left.\n####### Both train and test imputing transformations are successfull ###### ","3e14adbf":"# Recheck shape after transformation\nprint(temp.shape)\nprint(temp_test.shape)","186b0a9b":"# Giving meaningful manes\nX_train_imputed = temp\nX_test_imputed = temp_test","ac4fb140":"# Recheck shape after transformation\nprint(X_train_imputed.shape)\nprint(X_test_imputed.shape)","77488fde":"#Now let's trun towards categorical variables for rare labels and encoding \n# [sub_grade, emp_length, purpose, addr_state] stiil have high cardinality \n# Let's have a relook at their cardinality again\nX_train_imputed[['sub_grade', 'emp_length', 'purpose', 'addr_state']].nunique()","00c153ab":"X_train_dropped = ['loan_status', 'earliest_cr_line', 'zip_code', 'emp_title']\nX_train_remaining_categorical = [var for var in pure_categorical_train if var not in X_train_dropped]\n\n# we will use X_train_remaining_categorical for categorical encoding. \n# These are the categorical variables remaining in X_train after dropping several columns in earliee spets","8d70ac41":"# let's create a imputer pipe using feature engine library\nencoding_pipe = Pipeline([('encoder_rare_label',\n                                               ce.RareLabelCategoricalEncoder(tol=0.004,\n                                               n_categories=6,\n                                               variables=['sub_grade', 'emp_length', 'purpose', 'addr_state'])),\n                           ('categorical_encoder',\n                                               ce.OrdinalCategoricalEncoder(encoding_method='ordered',\n                                               variables=X_train_remaining_categorical))\n                        ])\n","3b4852d0":"encoding_pipe.fit(X_train_imputed, y_train)\n# print(X_train_imputed.shape)\n# print(X_test_imputed.shape)","75bfa340":"# Transfor with this encoding pipe\n# X_train = ordinal_enc.transform(X_train)","703b1b3c":"#pipe.fit(X_test_imputed)\nX_train_imputed_encoded = encoding_pipe.transform(X_train_imputed)\n#temp_test = pipe.transform(X_test)","d9b58a29":"# transform test\nX_test_imputed_encoded = encoding_pipe.transform(X_test_imputed)","eb18ab5d":"# Now date engineering and tranformation steps are complete\n# Time to do some random sanity checks\nprint('Train final shape',X_train_imputed_encoded.shape)\n\nprint('Test final shape',X_test_imputed_encoded.shape)","85a0426f":"X_train_imputed_encoded.head()","b0485e26":"X_test_imputed_encoded.head()","4fa43fbb":"#### NEXT STEPS ####\n\n# Apparantly, on the surface, everything is numeric and there are no null values. Let's proceed with ML algorithms\n# We will mainly use here Ramdom Forest & XGBoost, which are are not sensitive to outliers and do not need a normal -\n# - distribution for every variable. XGBoost had given us the best results many times in the past when compared to some linear models\n","895b2047":"(y_train.value_counts())\/len(y_train)","58a82758":"# in y_train 0 is coming 88% and 1 is coming only 11%\n# Do the oversample to balance 1 in the train datasets\n#from imblearn.over_sampling import SMOTE \n\nsm = SMOTE(random_state = 2) \nX_train_smote, y_train_smote= sm.fit_sample(X_train_imputed_encoded, y_train.ravel())\n\n\nprint('\\n')\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_smote.shape)) \nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape)) \nprint('\\n')\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_smote == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_smote == 0))) \n","eac521ec":"# Origenal X_train and y_train side\nprint('Train size before oversampling,',X_train_imputed_encoded.shape)\n# Origenal X_train and y_train side\nprint('Test size before oversampling,',X_test_imputed_encoded.shape)\n","6186e71b":"import matplotlib.pyplot as plt\nfrom sklearn import  metrics, model_selection\nfrom xgboost.sklearn import XGBClassifier\nfrom numpy import loadtxt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report","cfa8a210":"# XGBOost parameters for tuning the algorithm to get optimum performance\n# Let's work with some ramdomly chosen hyper parameters \nXGBmodel = XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, subsample = 0.1)\n#XGBmodel = XGBClassifier()\n# grid_search.fit(X, label_encoded_y)\n# y_pred = model.predict(X_test)\n# lr1.fit(X_train_res, y_train_res.ravel())\n#predictions = lr1.predict(X_test) \n  \n# print classification report \n#print(classification_report(y_test, predictions)) ","9d1f01ff":"# Fit the model \nXGBmodel.fit(X_train_smote, y_train_smote.ravel())","0adc788a":"# Make the predictions \ny_pred = XGBmodel.predict(X_test_imputed_encoded)","134f726e":"# Model performance\nprint(classification_report(y_test, y_pred)) \nprint('\\n')\nprint(confusion_matrix(y_test, y_pred )) ","8aefd903":"# Test the accuracy on train data\ny_pred_train = XGBmodel.predict(X_train_imputed_encoded)","75245210":"print(classification_report(y_train, y_pred_train )) \nprint('\\n')\n# onfusion_matrix(y_true, y_pred, labels=['Cat', 'Dog', 'Rabbit']) # confusion_matrix\nprint(confusion_matrix(y_train, y_pred_train )) ","d2530085":"# Link to learn one more about SMOTE based sampling\n## https:\/\/www.geeksforgeeks.org\/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python\/","ffed30b0":"# Import necessary packages \nfrom numpy import loadtxt\nfrom xgboost import plot_importance\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot","1ce00392":"# Plot feature importance\n# XGBClassifier.get_booster().get_score(importance_type= f)\nf = 'gain'\n#XGBClassifier.get_booster().get_score(importance_type= 'gain')\n\nimportance_dict = XGBmodel.get_booster().get_score(importance_type='gain')\nfrom collections import OrderedDict \n\nimportance_dict_sorted_values_rev = {k: v for k, v in sorted(importance_dict.items(), key=lambda item: item[1], reverse=True)}\n#importance_dict_sorted_values_acen = {k: v for k, v in sorted(importance_dict.items(), key=lambda item: item[1])}\nimportance_dict_sorted_values_rev\n","27c92f0f":"# Initialize limit  \nN = 30\n    \n# Using items() + list slicing  \n# Get first K items in dictionary  \nsorted_features_by_gain = dict(list(importance_dict_sorted_values_rev.items())[0: N])  \nsorted_features_by_gain","32ca732b":"import matplotlib.pyplot as plt\nfig= plt.figure(figsize=(40,40))\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 35}\n\nplt.rc('font', **font)\n#data = {'apple': 67, 'mango': 60, 'lichi': 58}\nnames = list(sorted_features_by_gain.keys())\nvalues = list(sorted_features_by_gain.values())\n\n#tick_label does the some work as plt.xticks()\nplt.barh(range(len(sorted_features_by_gain)),values,tick_label=names)\nplt.gca().invert_yaxis()\n#plt.savefig('bar.png')\n\nplt.show()","a66ba62a":"## Compare the file sizes of origenal data and after SMOTE sampling. 0 & 1 are balaned in y_train. Let's proceed XGBoost","85a7564b":"# In the following exercise wre are attempting first cut feature engineering and feature selection using XGBoost algorithm. \n##This exercise should be taken as a feature selection and feature engineering exercide only","038a9a4d":"# Feature Importance. ","44158f72":"# Conclusion: we did feature engineering and feature selection using XGBoost algorithm. This exercise should be taken as a feature selection and feature engineering exercide only\n## I thing some baking expert should be able to explain the validity of this feature importance. If not explainable, we need to remodel. Also we had dropped some columns with more than 90% null value. In the next iteration, all of them may be added as ## they can also have some hidden information. For now we stop here"}}