{"cell_type":{"033c715d":"code","9d7f91ec":"code","19d0975a":"code","116b1953":"code","89a284f7":"code","747660d0":"code","5da6e115":"code","59552d4d":"code","ee7ca15b":"code","5ffd57b7":"code","79cc1c64":"code","44e04f7e":"code","109556a5":"code","029ec165":"code","3f703479":"code","5f78eb03":"code","c1025990":"code","66021426":"code","c36e99d2":"code","eff1ecc1":"code","949add4d":"code","3df2c0c1":"code","96f20417":"code","d6dd0986":"code","892be6bf":"code","ebce25dd":"code","74addca6":"code","39dd5495":"code","112945d3":"code","85783edf":"code","019e86b2":"code","5b9791c0":"code","fdea7857":"code","1d8decde":"code","ce85f850":"code","57529a0c":"markdown","a2c8baa6":"markdown","13b27066":"markdown","a1a888e7":"markdown","e2c17b86":"markdown","418b81ee":"markdown","a300f419":"markdown","78e9901d":"markdown","83b3cac8":"markdown","1d576aab":"markdown","a0e12f51":"markdown","c71daa39":"markdown","d3030a8d":"markdown","acbad19e":"markdown","c531e52c":"markdown","ec350f30":"markdown","1a8a4018":"markdown","4db69883":"markdown","b625c1c0":"markdown","9fd9846a":"markdown","d380cecb":"markdown","c0e61961":"markdown","1bfb0d6b":"markdown","f7abd180":"markdown","4a7eaa64":"markdown","15b0c604":"markdown","9238a1db":"markdown","d183253b":"markdown","da73bd82":"markdown","38cc799b":"markdown","c96a8852":"markdown","8cb257b7":"markdown","dfe9a377":"markdown","57eed776":"markdown","bab119b1":"markdown","2694c526":"markdown","5339d090":"markdown","afbcc70c":"markdown","a658d408":"markdown","320cf3e6":"markdown","7f5b536c":"markdown","c8a730b5":"markdown","b71c5a4c":"markdown","98f2ec8b":"markdown","f6a6f6fb":"markdown","f729f425":"markdown","8d4a9d63":"markdown","e6c8d62b":"markdown","6a0ad44e":"markdown","3cd523d5":"markdown","89e9fda1":"markdown","df79b675":"markdown","966788da":"markdown","5bfb1193":"markdown"},"source":{"033c715d":"# Data analysis libraries\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import display\npd.options.display.max_columns = None\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"Set1\")\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\nimport cufflinks\ncufflinks.go_offline(connected=True)\n\n# Initiate plotly\ninit_notebook_mode(connected=True) #do not miss this line","9d7f91ec":"# Load flat file (Kaggle)\nreviews_raw = pd.read_csv('..\/input\/beer_reviews.csv')\n\n# Load flat file from GDrive\n#reviews_raw = pd.read_csv('\/content\/gdrive\/My Drive\/Python\/Projects\/Beer Reviews\/beer_reviews.csv')\n\n#reviews_raw = pd.read_csv('https:\/\/query.data.world\/s\/gib6aa6n3tmtuvrqpbdlhcbgdg7tk4')\n\n# Best Practice: Make a copy of the raw data to work on\nreviews = reviews_raw.copy()\n\n# Peep it\nreviews.head()","19d0975a":"# Convert 'object' to 'category' \nreviews[reviews.select_dtypes(['object']).columns] = reviews.select_dtypes(['object']).\\\n                                                         apply(lambda x: x.astype('category'))\n\n# Examine structure \nreviews.info()","116b1953":"# Check for null values\nprint(reviews.isnull().sum())","89a284f7":"# Percent of data missing `beer_abv` values\nprint(\"Percent Null Values of `beer_abv` column:\", round(67785 \/ 1586614 * 100, 2),\"%\")","747660d0":"# Drop null row values\nreviews = reviews.dropna()\nreviews.info()","5da6e115":"# Check for null values\nprint(reviews.isnull().sum())","59552d4d":"# Sort by user overall rating first\nreviews = reviews.sort_values('review_overall', ascending=False)\n\n# Keep the highest rating from each user and drop the rest \nreviews = reviews.drop_duplicates(subset= ['review_profilename','beer_name'], keep='first')\n\n# Peep structure\nreviews.info()\n\n# Percent of data that are duplicates\nprint(\"Percent of Duplicate Values:\", round((1518478 - 1496263)\/ 1518478 * 100, 2),\"%\")","ee7ca15b":"# Histogram of all numeric features\nreviews.hist(figsize=(12,12))\nplt.show()","5ffd57b7":"# Review scores of >= 1\nreviews = reviews[(reviews['review_overall'] >= 1) | \\\n        (reviews['review_appearance'] >= 1)]\n\n# Check it out\nreviews.info()","79cc1c64":"# Split after \/ & keep only first string\nreviews['brewery_name'] = reviews['brewery_name'].str.split(' \/ ').str[0]","44e04f7e":"# Top 10 Beers by ABV\ntop_10_abv = reviews[['beer_name','brewery_name','beer_abv']].\\\n    sort_values('beer_abv', ascending=False). \\\n    drop_duplicates('beer_name').\\\n    head(10).\\\n    sort_values('beer_abv', ascending=True)\n\n# Combine brewery and beer name for readability\ntop_10_abv['combined_name'] = top_10_abv['brewery_name'].str.\\\n  cat(top_10_abv['beer_name'], sep=' : ')\n\n# Plot it\np = [go.Bar(x = top_10_abv['beer_abv'] \/ 100,\n            y = top_10_abv['beer_name'],\n            hoverinfo = 'x',\n            text=top_10_abv['brewery_name'],\n            textposition = 'inside',\n            orientation = 'h',\n            opacity=0.7, \n            marker=dict(\n                color='rgb(1, 87, 155)'))]\n\n# Pieces of Flair\nlayout = go.Layout(title='Top 10 Strongest Beers by ABV',\n                   xaxis=dict(title=\"ABV\",\n                              tickformat = \"%\",\n                              hoverformat = '.2%'),\n                   margin = dict(l = 220),\n                   font=dict(family='Courier New, monospace',\n                            color='dark gray'))\n\nfig = go.Figure(data=p, layout=layout)\n\n# Plot it\npy.offline.iplot(fig)","109556a5":"# Use only unique beers and breweries for aggregate stats\nabv = reviews[['beer_name','brewery_name','beer_abv']]\\\n    .drop_duplicates(['beer_name','brewery_name'])\\\n    .pivot_table(index=\"brewery_name\", aggfunc=(\"count\",'mean','median'))\n\n# Adjust column names\nabv.columns = abv.columns.to_series().str.join('_')\n\n# Reset index\nabv.reset_index(inplace=True)\n\n# Rename column\nabv.rename(columns={'beer_abv_count':'beer_count'}, inplace=True)\n\n# Filter for breweries with at least 4 unique beers\nabv_filtered_mean = abv.query('beer_count >= 4')\\\n    .sort_values('beer_abv_mean', ascending=False)\\\n    .head(10)\\\n    .sort_values('beer_abv_mean', ascending=True)\n\n# Plot it\np1 = [go.Bar(x = abv_filtered_mean['beer_abv_mean'] \/ 100,\n             y = abv_filtered_mean['brewery_name'],\n             text=abv_filtered_mean['beer_abv_mean'].round(1),\n             hoverinfo='x',\n             textposition = 'inside',\n             orientation = 'h',\n             opacity=0.7, \n             marker=dict(\n                color='rgb(1, 87, 155)'))]\n\n# Pieces of Flair\nlayout = go.Layout(title='Top 10 Breweries by Highest Mean ABV',\n                   xaxis=dict(title=\"Mean ABV\",\n                              tickformat='%',\n                              hoverformat = '.2%'),\n                   margin = dict(l = 230),\n                   font=dict(family='Courier New, monospace',\n                            color='dark gray'))\n\nfig = go.Figure(data=p1, layout=layout)\n\n# Plot it\npy.offline.iplot(fig)","029ec165":"# Filter for breweries with at least 4 unique beers\nabv_filtered_median = abv.query('beer_count >= 4')\\\n    .sort_values('beer_abv_median', ascending=False)\\\n    .head(10)\\\n    .sort_values('beer_abv_median', ascending=True)\n\n# Plot it\np2 = [go.Bar(x = abv_filtered_median['beer_abv_median'] \/ 100,\n            y = abv_filtered_median['brewery_name'],\n            hoverinfo='x',\n            text=abv_filtered_median['beer_abv_median'].round(1),\n            textposition = 'inside',\n            orientation = 'h',\n            opacity=0.7, \n            marker=dict(\n                color='rgb(1, 87, 155)'))]\n\n# Pieces of Flair\nlayout = go.Layout(title='Top 10 Breweries by Highest Median ABV',\n                   xaxis=dict(title=\"Median ABV\",\n                              tickformat=\"%\",\n                              hoverformat = '.2%'),\n                   margin = dict(l = 230),\n                   font=dict(family='Courier New, monospace',\n                            color='dark gray'))\n\nfig = go.Figure(data=p2, layout=layout)\n\n# Plot it\npy.offline.iplot(fig)","3f703479":"# Drop duplicate breweries & beer names\ndf = reviews\\\n    .drop_duplicates(['beer_name','brewery_name'])\n\n# Create column with boolean based on ABV\ndf['above_10'] = np.where(df['beer_abv'] >= 10, 1, 0)\n\n# Apply function to create percent above threshold of 10\nabove_10 = pd.DataFrame(df.groupby('brewery_name')['above_10']\\\n                        .apply(lambda x: sum(x)\/len(x)))\n\n# Join percent back with abv \nabv = pd.merge(abv, above_10, on='brewery_name')\\\n    .sort_values('above_10', ascending=False)\n\n# Filter for breweries with at least 4 unique beers\nabv_filtered_above_10 = abv.query('beer_count >= 4')\\\n    .sort_values('above_10', ascending=False)\\\n    .head(10)\\\n    .sort_values('above_10', ascending=True)\n\n# Plot it\np3 = [go.Bar(x = abv_filtered_above_10['above_10'],\n            y = abv_filtered_above_10['brewery_name'],\n            hoverinfo='x',\n            text=abv_filtered_above_10['above_10'].round(2),\n            textposition = 'inside',\n            orientation = 'h',\n            opacity=0.7, \n            marker=dict(\n                color='rgb(1, 87, 155)'))]\n\n# Pieces of Flair\nlayout = go.Layout(title='Top 10 Breweries by Percent of Beers over 10% ABV',\n                   xaxis=dict(title=\"ABV\",\n                              tickformat=\"%\",\n                              hoverformat = '.1%'),\n                   margin = dict(l = 230),\n                   font=dict(family='Courier New, monospace',\n                            color='dark gray'))\n\nfig = go.Figure(data=p3, layout=layout)\n\n# Plot it\npy.offline.iplot(fig)","5f78eb03":"# Pivot table with overall rating\ntop_3_rec = reviews[['beer_name','review_overall']]\\\n    .pivot_table(index=\"beer_name\", aggfunc=(\"count\",'mean','median'))\\\n    .dropna()\n\n# Rename columns and flatten pivot table\ntop_3_rec.columns = top_3_rec.columns.to_series().str.join('_')\ntop_3_rec.reset_index(inplace=True)\n\n# Filter for highest rated beers\ntop_3_rec = top_3_rec.query('review_overall_count >= 1000')\\\n  .sort_values('review_overall_mean', ascending=False)\\\n  .head(3)\n\n# Check it out\ntop_3_rec","c1025990":"# Initiate PySpark \n!pip install pyspark\n!pip install -q findspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n# PySpark function\nfrom pyspark.sql.functions import monotonically_increasing_id","66021426":"# Create Pandas DF of ratings by user and item\nratings = reviews[['review_profilename', 'beer_name', 'review_overall']]\n\n# Pivot table of user review counts\nuser_pivot = reviews[['review_profilename', 'beer_name']]\\\n  .pivot_table(index=\"review_profilename\", aggfunc=(\"count\"))\\\n  .reset_index()\\\n  .rename(columns={'beer_name':'user_review_count'})\n\n# Join with ratings \nuser_ct = user_pivot.merge(ratings, on='review_profilename', how='inner')\n\n# Pivot table of beer review counts\nbeer_pivot = user_ct[['beer_name','review_overall']]\\\n  .pivot_table(index=\"beer_name\", aggfunc=(\"count\"))\\\n  .reset_index()\\\n  .rename(columns={'review_overall':'beer_review_count'})\n\n# Join with merged user review counts \/ ratings\nuser_beer_ct = user_ct.merge(beer_pivot, on='beer_name', how='inner')\n\n# Filter for user_review_count >= 50 & beer_review_count >= 100 \nfilt_user_beer_ct = user_beer_ct[(user_beer_ct['user_review_count'] >= 50) & \\\n                   (user_beer_ct['beer_review_count'] >= 100)]\n\n# Remove unwanted variables\nratings = filt_user_beer_ct.drop(['user_review_count', 'beer_review_count'], axis=1)\n\n# Convert Pandas mixed data into strings\nratings[['review_profilename', 'beer_name']] = ratings[['review_profilename', \n                                                        'beer_name']].astype(str)\n\n# Rename columns\nratings = ratings.rename(columns = {'review_profilename':'user', \n                                    'review_overall':'rating'})\n\n# Convert Pandas DF to Spark DF\nratings = spark.createDataFrame(ratings)\n\n# Get unique users and repartition to 1 partition\nusers = ratings.select(\"user\").distinct().coalesce(1)\n\n# Create a new column of unique integers called \"user_id\" in the users dataframe.\nusers = users.withColumn(\"user_id\", monotonically_increasing_id()).persist()\n\n# Extract the distinct beer IDs\nbeers = ratings.select('beer_name').distinct()\n\n# Repartition the data to only one partition\nbeers = beers.coalesce(1)\n\n# Create a new column of beer_id integers\nbeers = beers.withColumn(\"beer_id\", monotonically_increasing_id()).persist()\n\n# Join ratings, users, and beers dataframes\nbeer_ratings = ratings.join(users, \"user\", \"left\").join(beers, \"beer_name\", \"left\")\n\n# Check it out\nbeer_ratings.show()","c36e99d2":"# Select relevant columns & convert the columns to the proper data types\nbeer_tbl = beer_ratings.select(beer_ratings.user_id.cast(\"integer\"), \n                         beer_ratings.beer_id.cast(\"integer\"), \n                         beer_ratings.rating.cast(\"double\"))\n\n# Use the .printSchema() method to see the datatypes of the ratings dataset.\nbeer_tbl.printSchema()","eff1ecc1":"# Count the total number of ratings in the dataset\nnumerator = beer_tbl.select(\"rating\").count()\n\n# Count the number of distinct user_ids and distinct beer_ids\nnum_users = beer_tbl.select(\"user_id\").distinct().count()\nnum_beers = beer_tbl.select(\"beer_id\").distinct().count()\n\n# Set the denominator equal to the number of users multiplied by the number of beers\ndenominator = num_users * num_beers\n\n# Divide the numerator by the denominator\nsparsity = (1.0 - (numerator *1.0)\/denominator)*100\nprint (\"The beer_tbl dataframe is\", \"%.2f\" % sparsity + \"% empty.\")","949add4d":"import pyspark.sql.functions as f\n\n# Min num ratings for beers\nprint(\"Beer with the fewest ratings: \")\nbeer_tbl.groupBy(\"beer_id\").count().select(f.min(\"count\")).show()\n\n# Max num ratings for beers\nprint(\"Beer with the most ratings: \")\nbeer_tbl.groupBy(\"beer_id\").count().select(f.max(\"count\")).show()\n\n# Avg num ratings per beer\nprint(\"Avg num ratings per beer: \")\nbeer_tbl.groupBy(\"beer_id\").count().select(f.avg(\"count\")).show()\n\n# Min num ratings for users\nprint(\"User with the fewest ratings: \")\nbeer_tbl.groupBy(\"user_id\").count().select(f.min(\"count\")).show()\n\n# Max num ratings for users\nprint(\"User with the most ratings: \")\nbeer_tbl.groupBy(\"user_id\").count().select(f.max(\"count\")).show()\n\n# Avg num ratings per users\nprint(\"Avg num ratings per user: \")\nbeer_tbl.groupBy(\"user_id\").count().select(f.avg(\"count\")).show()","3df2c0c1":"# Import libraries\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Split the ratings dataframe into training and test data\n(training_data, test_data) = beer_tbl.randomSplit([0.8, 0.2], seed=1234)","96f20417":"# Create ALS model: for CV\nals_cv = ALS(userCol=\"user_id\", \n          itemCol=\"beer_id\", \n          ratingCol=\"rating\", \n          nonnegative = True, \n          implicitPrefs = False,\n          coldStartStrategy=\"drop\") \n\n# Create ALS model: no CV\nals = ALS(userCol=\"user_id\", \n          itemCol=\"beer_id\", \n          ratingCol=\"rating\", \n          rank =10, \n          maxIter =15, \n          regParam = 0.1,\n          coldStartStrategy=\"drop\", \n          nonnegative =True, \n          implicitPrefs = False)\n\n# Fit the model to the training_data\nmodel = als.fit(training_data)\n\n# Generate predictions on the test_data\ntest_predictions = model.transform(test_data)\n           \n# Define evaluator as RMSE and print length of evaluator\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\") \n\n# Calculate and print the RMSE of the test_predictions\nRMSE = evaluator.evaluate(test_predictions)\nprint(RMSE)","d6dd0986":"#### Cross Validation \n# Import the requisite items\n#from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Add hyperparameters and their respective values to param_grid\n# Let Spark know which combos of hyperparameters to try and how to evaluate them.\n#param_grid = ParamGridBuilder() \\\n          #  .addGrid(als.rank, [10, 20, 50]) \\\n          #  .addGrid(als.maxIter, [5, 20, 50]) \\\n          #  .addGrid(als.regParam, [0.01, 0.05, 0.1,]) \\\n          #  .build()\n\n#print(\"Num models to be tested: \", len(param_grid))\n\n# Build cross validator\n#cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator= evaluator, numFolds=3)\n\n# Fit cross validator to the 'training_data' \n#model_cv = cv.fit(training_data)\n\n# Extract best model from the cv model above\n#best_model = model_cv.bestModel\n\n# Create test predictions\n#test_predictions = best_model.transform(test_data)\n\n# Calculate and print the RMSE of the test_predictions\n#RMSE = evaluator.evaluate(test_predictions)","892be6bf":"# Generate n recommendations for all users\nALS_recommendations = model.recommendForAllUsers(10)\n\n# Create temporary table\nALS_recommendations.registerTempTable(\"ALS_recs_temp\")\n\n# Clean up output, explode it, extract lateral view as separate columns\nclean_recs = spark.sql(\"SELECT user_id, beer_ids_and_ratings.beer_id AS beer_id,\\\n                                beer_ids_and_ratings.rating AS prediction \\\n                       FROM ALS_recs_temp\\\n                       LATERAL VIEW explode(recommendations) \\\n                       exploded_table AS beer_ids_and_ratings\")\n\n# Filter for beers that each user has not consumed before\nrecommendations = clean_recs.join(beer_ratings.select('user_id', 'beer_id', 'rating'), \n                ['user_id','beer_id'], \"left\")\\\n                .filter(beer_ratings.rating.isNull()).drop('rating')\\\n                .join(beer_ratings.select('beer_id','beer_name'), ['beer_id'], \"left\")\\\n                .distinct()\n\n# Check it out\nrecommendations.show()","ebce25dd":"# Import col function to filtering\nfrom pyspark.sql.functions import col\n\n# Look at user 62's actual beer ratings\nprint (\"User 62's Ratings:\")\nbeer_ratings.filter(col(\"user_id\") == 62).sort(\"rating\", ascending = False).show()","74addca6":"# Look at the beers recommended to user 62\nprint (\"User 62's Recommendations:\")\nrecommendations.filter(col(\"user_id\") == 62).show()","39dd5495":"# Method 1 - Heatmap: Pandas\n# List of numeric factors to include\nfactors = [\"review_appearance\",\"review_aroma\",\"review_palate\",\"review_taste\", \"review_overall\"]\n\n# Create a correlation matrix \ncorr_metrics = reviews[factors].corr()\ncorr_metrics.style.background_gradient(cmap='Blues')","112945d3":"# Method 2 - Heatmap: Plotly\nheat_map_plotly = ff.create_annotated_heatmap(\n    z=corr_metrics.values,\n    x=list(corr_metrics.columns),\n    y=list(corr_metrics.index),\n    annotation_text=corr_metrics.round(2).values,\n    showscale=True,\n    colorscale='Portland',\n    font_colors=['white','white'])\n\nlayout = go.Layout(title=\"Correlation Heatmap\",\n                   margin = dict(l = 330))\n\nfig = go.Figure(data=heat_map_plotly, layout=layout)\npy.offline.iplot(fig)","85783edf":"# Method 3 - Bar Chart: Plotly\n# Correlation with target review_overall\nfactor_corr = corr_metrics[['review_overall']]\\\n    .drop(['review_overall'])\\\n    .reset_index()\\\n    .sort_values('review_overall', ascending=True)\n\n# Plot it with Plotly\np4 = [go.Bar(x = factor_corr['review_overall'],\n            y = factor_corr['index'],\n            hoverinfo='x',\n            text=factor_corr['review_overall'].round(2),\n            textposition = 'inside',\n            orientation = 'h',\n            opacity=0.7, \n            marker=dict(\n                color='rgb(1, 87, 155)'))]\n\n# Pieces of Flair\nlayout = go.Layout(title='Correlation with Target: review_overall',\n                   xaxis=dict(title=\"Correlation\",\n                              hoverformat = '.2f'),\n                   margin = dict(l = 130),\n                   font=dict(family='Courier New, monospace',\n                            color='dark gray'))\n\nfig = go.Figure(data=p4, layout=layout)\n\n# Plot it\npy.offline.iplot(fig)","019e86b2":"# Method 4 - Bar Chart: Seaborn\n# Plot it with Seaborn just for fun\nsns.barplot(data=factor_corr.sort_values('review_overall', ascending=False), y=\"index\",x=\"review_overall\")\nplt.ylabel(\"\"); plt.xlabel(\"Correlation\")\nplt.title(\"Correlation with Target: review_overall\")\nplt.figure(figsize=(12,8))\nplt.show()","5b9791c0":"# Normalization\n# Define our features \nfactors = [\"review_appearance\",\"review_aroma\",\"review_palate\",\"review_taste\"]\nfeatures = reviews[factors]\n\n# Define our labels\nlabels = reviews['review_overall']\n\n# Import the StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the features and set the values to a new variable\nscaler = StandardScaler()\nscaled_train_features = scaler.fit_transform(features)\n\n# Note the five features\n#print(features.head())\n\n# Note that scaled_train_features are arrays\n#print(scaled_train_features)\n\n# Import PCA class\nfrom sklearn.decomposition import PCA\n\n# Get our explained variance ratios from PCA using all features\npca = PCA()\npca.fit(scaled_train_features)\nexp_variance = pca.explained_variance_ratio_\n\n# plot the explained variance using a barplot\nfig, ax = plt.subplots()\nax.bar(range(1, 5), exp_variance) # Use the number of features \nax.set_xlabel('Principal Components')\nax.set_title('Explained Variance by # of Principal Components')\nax.set_ylabel('Explained Variance')\nplt.show()\nprint(pca.explained_variance_ratio_)","fdea7857":"# Calculate the cumulative explained variance\ncum_exp_variance = np.cumsum(exp_variance)\n\n# Plot the cumulative explained variance and draw a dashed line at 0.90.\nfig, ax = plt.subplots()\nax.plot(range(1, 5), cum_exp_variance)\nax.axhline(y=0.9, linestyle='--')\nplt.title('Cumulative Explained Variance by # of Principal Components')\nplt.show()","1d8decde":"# Bar plot \np1 = dict(\n    type='bar',\n    x=['PC %s' %i for i in range(1,6)],\n    y=exp_variance,\n    opacity=0.7, \n    marker=dict(color='rgb(1, 87, 155)'),\n    name='Individual'\n)\n\n# Scatter plot\np2 = dict(\n    type='scatter',\n    x=['PC %s' %i for i in range(1,6)], \n    y=cum_exp_variance,\n    marker=dict(color='rgb(255, 82, 82)'),\n    name='Cumulative'\n)\n\ndata = [p1, p2]\n\nlayout=dict(\n    title='Explained Variance by Number of Principal Components',\n    yaxis=dict(\n        title='Explained Variance',\n        tickformat=\"%\",\n        hoverformat = '.1%'),\n    margin = dict(r = 200),\n    font=dict(family='Courier New, monospace',\n                            color='dark gray'),\n    annotations=list([\n        dict(x=1.16,\n             y=1.05,\n             xref='paper',\n             yref='paper',\n             text='Explained Variance',\n             showarrow=False,\n        )\n    ])\n)\n\nfig = dict(data=data, layout=layout)\n\n# Plot it\npy.offline.iplot(fig)","ce85f850":"# Aggregate count & mean for relevant variables\naro_appear = reviews[['beer_style','review_aroma', 'review_appearance','review_overall']]\\\n    .pivot_table(index=\"beer_style\", aggfunc=('count','mean'))\n\n# Flatten pivot table\naro_appear.columns = aro_appear.columns.to_series().str.join('_')\naro_appear.reset_index(inplace=True)\n\n# Remove redundant '_count' columns\naro_appear.drop(list(aro_appear.filter(regex = 'count')), \n                      axis = 1, \n                      inplace = True)\n\n#aro_appear = aro_appear.drop(['review_appearance_count', \n                                         # 'review_aroma_count'], axis=1)\n\n# Add average of combined aroma and appearance \naro_appear['aroma_appear_mean'] = (aro_appear['review_appearance_mean'] + \n                              aro_appear['review_aroma_mean']) \/ 2\n\n# Add absolute average distance from mean \naro_appear['diff_from_mean'] = abs(aro_appear['review_appearance_mean'] - aro_appear['review_aroma_mean'])\n\n# Sort for plotly \nsort_aro_appear = aro_appear.sort_values('aroma_appear_mean', ascending=False)\\\n                    .head(10)\\\n                    .sort_values('aroma_appear_mean',ascending=True)\n\n# Aroma-Appearance Dumbbell Plot Workaround\np1 = go.Scatter(\n    x = sort_aro_appear['aroma_appear_mean'],\n    y = sort_aro_appear['beer_style'],\n    error_x = dict(\n        type='data',\n        array=sort_aro_appear['diff_from_mean']\/2,\n        thickness=1.5),\n    mode = 'markers',\n    name = 'Combined Average',\n    marker=dict(\n        color='rgba(84, 110, 122, 0.95)',\n        line=dict(\n            color='rgba(84, 110, 122, 1.0)',\n            width=1,\n        ),\n        symbol='circle',\n        size=8\n    )\n)\n\n# Aroma Rating Dot Plot \np2 = go.Scatter(\n    x = sort_aro_appear['review_aroma_mean'],\n    y = sort_aro_appear['beer_style'],\n    mode = 'markers',\n    name = 'Aroma Average',\n    marker=dict(\n        color='rgba(229, 57, 53, 0.95)',\n        line=dict(\n            color='rgba(229, 57, 53, 1.0)',\n            width=1,\n        ),\n        symbol='circle',\n        size=8\n    )\n)\n\n# Appearance Rating Dot Plot\np3 = go.Scatter(\n    x = sort_aro_appear['review_appearance_mean'],\n    y = sort_aro_appear['beer_style'],\n    mode = 'markers',\n    name = 'Appearance Average',\n    marker=dict(\n        color='rgb(1, 87, 155, 0.95)',\n        line=dict(\n            color='rgba(1, 87, 155, 1.0)',\n            width=1,\n        ),\n        symbol='circle',\n        size=8\n    )\n)\n\n\n# Set layout\nlayout=dict(\n    title='Top 10 Beer Styles by Combined Appearance\/Aroma Average',\n    xaxis=dict(\n        showline=True,\n        showticklabels=True,\n        ticks='outside',\n        title='Rating',\n        hoverformat = '.2f',\n        autorange=True,\n        showgrid=False,),\n    margin = dict(l = 250),\n    font=dict(family='Courier New, monospace', color='dark gray'),\n    legend=dict(\n        font=dict(\n            size=10,\n        ),\n        yanchor='bottom',\n        xanchor='right',\n    ),\n    hovermode='closest'\n)\n\n\n# Plot it\nfig = go.Figure(data=[p1,p2,p3], layout=layout)\npy.offline.iplot(fig)","57529a0c":"### 4.1.4 Top 10 Breweries by Percent of Beers over 10% ABV","a2c8baa6":"## 4.2 If you had to pick 3 beers to recommend using only this data, which would you pick?","13b27066":"## 4.1 Which brewery produces the strongest beers by alcohol beverage percent?","a1a888e7":"#### 4.2.2.4 Sparsity\n\nHow much of the matrix is empty?","e2c17b86":"|Numeric Variable| Distribution|Interpretation|\n|---|---|---|\n|`beer_abv`| Skewed right | Most beers are less than 20% ABV|\n|`beer_beerid`| Skewed right | Lower IDs have more reviews than higher IDs|\n|`brewery_id`| Skewed right| Lower IDs have more reviews than higher IDs|\n|`review_appearance`|Skewed left| Most beers rated 4 and higher|\n|`review_aroma`| Normal | Most beers rated between 3 and 4|\n|`review_overall`|Skewed left| Most beers rated 4 and higher|\n|`review_palate`|Normal| Most beers rated between 3 and 4|\n|`review_taste`|Normal| Most beers rated between 3.5 and 4.5 |\n|`review_time`|Skewed left| Num of beers rated vs time suggests linearity|\n\n> *Also note the existence of ratings which have a value of 0 in the* `review_overall` *&* `review_appearance` *columns.*","418b81ee":"If I were to recommend 3 beers to somebody who is new to beer (i.e. does not have a style preference), then I'd first narrow down to the most popular beers and subsequently find the highest rated beers amongst that subset. \n\nThe idea is that finding a widely-known beer with a high rating increases the chance that your recommendations will be taken more favorably from your audience as opposed to you offering an obscure beer with a high rating. Most people tend to like things that other people like. ","a300f419":"> *There were only seven reviews with ratings of less than 1 here which is basically negligible. However, it's always a good idea to check for numbers which don't make sense in your dataset.*","78e9901d":"## 2.2 Load Data","83b3cac8":"### 3.2.2 Percent Null Values","1d576aab":"## 3.4 Overview of Numeric Features","a0e12f51":"## 3.1 Types of Data","c71daa39":"> *The dataset now contains a mix of numeric and categorical variables.*","d3030a8d":"## 2.1 Load Libraries","acbad19e":"### 3.2.1 Count Null Values","c531e52c":"## 3.3 Duplicate Data\n\nIf a user has rated the same beer more than once, then only keep their highest rating.","ec350f30":"### 3.2.3 Drop Null Values","1a8a4018":"> *It takes about 3 principal components to explain 90.2% of the variance.*","4db69883":"#### 4.2.2.6 Train-Test-Split","b625c1c0":"#### 4.2.2.5 Summary Statistics","9fd9846a":"## 4.4 If I enjoy a beer due to its aroma and appearance, which beer style should I try?","d380cecb":"## 3.6 Remove Strings\n\nSplit `brewery_name` using a delimiter and only keep strings before the split. ","c0e61961":"#### 4.2.2.8 Cross Validation\n\nCross validation is too computationally expensive and will not be utilized.","1bfb0d6b":"#### 4.2.2.7 Train ALS Model","f7abd180":"### 4.1.1. Top 10 Strongest Beers by ABV","4a7eaa64":"> *On average, our model predicts 0.58 above or below values of the original ratings matrix.*","15b0c604":"### 4.3.2 PCA","9238a1db":"### 4.1.3 Top 10 Breweries by Highest Median ABV","d183253b":"> *The matrix is sparse as expected.*","da73bd82":"### 4.2.2 Recommendation Engine with PySpark","38cc799b":"> *Removing all null values only impacts the size of the dataset by a little over 4%.*","c96a8852":"#### 4.2.2.10 Test Recommendations","8cb257b7":"#### 4.2.2.1 Initiate PySpark","dfe9a377":"## 3.2 Missing Data","57eed776":"## 1.2 The Take-Home Test","bab119b1":"> *Removing all duplicate values drops about 1.5% of the remaining data.*","2694c526":"# 1. Introduction","5339d090":"> *Although PCA is used mainly for dimensionality reduction, it can also be used to see how many variables explain a given amount of variance.*","afbcc70c":"> *The strongest beer produced is 57.7% ABV produced by Schorschbrau. We can\u2019t rule out the extremely slight possibility that the actual strongest beer is one with a null value in column `beer_abv`.*","a658d408":"## 3.5 Ratings of 0\n\nSince ratings are on a scale of 1-5, any values in review variables that are less than 1 are not suitable for analysis. ","320cf3e6":"> *There is high correlation between* `review_taste` *and the target variable* `review_overall`.","7f5b536c":"### 4.3.1 Correlation with Target","c8a730b5":"The main idea here is to use a *not-too-huge* dataset that allows for different approaches towards solving the same problem. Interviewers want to know if you can do the following: \n\n- Get the data\n- Explore the data\n- Wrangle the data\n- Visualize the data\n- Explain the data ","b71c5a4c":"# 3. Exploratory Analysis","98f2ec8b":"# 2. Set Up","f6a6f6fb":"#### 4.2.2.9 Make Recommendations","f729f425":"## 4.3 Which of the factors (aroma, taste, appearance, palette) are most important in determining the overall quality of a beer?","8d4a9d63":"Things to Look For: \n\n- Types of data\n- Missing data\n- Duplicate data \n- Unsuitable data","e6c8d62b":"# 4. Questions","6a0ad44e":"## 1.1 The Data Science Interview","3cd523d5":"### 4.1.2 Top 10 Breweries by Highest Mean ABV\n\n","89e9fda1":"### 4.2.1 Simplest Recommendation","df79b675":"A few months ago, Tanya Cashorali from TCB Analytics sat down with Datacamp for a very interesting [webinar](https:\/\/support.datacamp.com\/hc\/en-us\/articles\/360019081553-Feb-2019-Webinar-How-to-hire-and-test-for-data-skills-A-one-size-fits-all-interview-kit) discussing the ins-and-outs of data science interviews. There were a few key insights: \n\n1. Demand for data scientists has led to some unrealistic expectations regarding the necessary qualifications of candidates. \n2. Data science is a highly integrated discipline. This means candidates will have diverse professional backgrounds. \n3. Interviewing candidates with diverse backgrounds is challenging and requires a way of \"quantifying technical and cultural fit\". Old-school interview techniques such as *whiteboarding* are apparently not effective. \n4. A data analysis exercise can quantify company fit by revealing how candidates with different backgrounds and abilities approach the same problem.","966788da":"#### 4.2.2.2 Create & Join DataFrames","5bfb1193":"#### 4.2.2.3 Review Schema\n\nIf a dataset includes ratings from users, then these ratings are considered *explicit*, not *implicit*. "}}