{"cell_type":{"2adf33d5":"code","5a1687a4":"code","12e35ca7":"code","782aff36":"code","ef28c703":"code","9756eabe":"code","02b1698c":"code","6e6e5ae7":"code","534b053a":"code","767c339a":"code","89e3f39b":"code","2860b535":"code","88407e5a":"code","b5316c62":"code","7f390dad":"code","ebdbe3c2":"code","852ec046":"code","bad711a1":"code","e8270784":"code","beda5f06":"code","163aca20":"code","1cc25f3c":"code","9fc4bce6":"markdown","a2e76b63":"markdown","38582493":"markdown","b1e7c4c3":"markdown","d18764f0":"markdown"},"source":{"2adf33d5":"import os \nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import OneHotEncoder  \nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.preprocessing import StandardScaler  \nfrom sklearn.model_selection import train_test_split\nfrom scipy import sparse  ","5a1687a4":"os.listdir(\"..\/input\/cat-in-the-dat\")","12e35ca7":"train=pd.read_csv(\"..\/input\/cat-in-the-dat\/train.csv\")\ntrain.head()","782aff36":"test=pd.read_csv(\"..\/input\/cat-in-the-dat\/test.csv\")\ntest.head()","ef28c703":"train=train.drop(index=train[~train.nom_7.isin(test.nom_7)].index)\ntrain=train.drop(index=train[~train.nom_8.isin(test.nom_8)].index)\ntrain=train.drop(index=train[~train.nom_9.isin(test.nom_9)].index)","9756eabe":"new_test=test.drop(columns=['id'])\nnew_test.head()","02b1698c":"plt.figure(figsize=(8,8))\ntrain.target.value_counts().plot(kind='bar',color=['red','plum'])","6e6e5ae7":"plt.figure(figsize=(8,8))\ncorr = train.corr()\nsns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,annot=True)\nplt.title(\"correlation plot for train data\",size=28)","534b053a":"plt.figure(figsize=(8,8))\ncorr = test.corr()\nsns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,annot=True)\nplt.title(\"correlation plot for test data\",size=28)","767c339a":"new_train=train.drop(columns=['id','target'])\nnew_train.head()","89e3f39b":"fig,ax=plt.subplots(5,2,figsize=(15,30))\nj=0\nfor i in new_train.columns[:10]:\n    sns.barplot(y=new_train[i].value_counts()[:10],x=new_train[i].value_counts()[:10].index,ax=ax[int(j\/2),round(j%2)])\n    ax[int(j\/2),round(j%2)].set_title(\"bar chart for \"+i)\n    ax[int(j\/2),round(j%2)].set_ylabel(\"counts\")\n    j+=1\n","2860b535":"fig,ax=plt.subplots(5,2,figsize=(30,30))\nj=0\nfor i in new_train.columns[10:20]:\n    sns.barplot(y=new_train[i].value_counts()[:10],x=new_train[i].value_counts()[:10].index,ax=ax[int(j\/2),round(j%2)])\n    ax[int(j\/2),round(j%2)].set_title(\"bar chart for \"+i)\n    ax[int(j\/2),round(j%2)].set_ylabel(\"counts\")\n    j+=1","88407e5a":"fig,ax=plt.subplots(2,2,figsize=(15,15))\nj=0\nfor i in new_train.columns[20:23]:\n    sns.barplot(y=new_train[i].value_counts()[:10],x=new_train[i].value_counts()[:10].index,ax=ax[int(j\/2),round(j%2)])\n    ax[int(j\/2),round(j%2)].set_title(\"bar chart for \"+i)\n    ax[int(j\/2),round(j%2)].set_ylabel(\"counts\")\n    j+=1","b5316c62":"new_train=new_train.drop(columns=['bin_0'])\nnew_test=new_test.drop(columns=['bin_0'])","7f390dad":"data = pd.concat([new_train, new_test])\n\ndummies = pd.get_dummies(data, columns=data.columns, drop_first=True,sparse=True)\nnew_train = dummies.iloc[:new_train.shape[0], :]\nnew_test = dummies.iloc[new_train.shape[0]:, :]\ndel data\ndel dummies\nnew_train = new_train.sparse.to_coo().tocsr()\nnew_test = new_test.sparse.to_coo().tocsr()\n\n","ebdbe3c2":"X_train,X_test,y_train,y_test=train_test_split(new_train,train['target'],test_size=0.001,random_state=0)\n\nlr = LogisticRegression(C=0.095,solver='lbfgs',class_weight='balanced')  \nlr.fit(X_train, y_train)  \nproba_test = lr.predict_proba(X_test)[:, 1]\nLR_result=pd.DataFrame({'pred':proba_test,'real':y_test})\nLR_result['pred_0_1']=LR_result.pred.apply(lambda x:1 if x>=0.5 else 0)\n","852ec046":"print('LR_acc: ',sum(LR_result.real==LR_result.pred_0_1)\/len(LR_result))","bad711a1":"lr.predict_proba(new_test)[:, 1]","e8270784":"import lightgbm as lgb  \nimport pickle  \n\nX_train=X_train.astype(float)\nX_test=X_test.astype(float)\nlgb_train = lgb.Dataset(X_train, y_train)  \nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train) \nparams = {  \n    'boosting_type': 'gbdt',  \n    'objective': 'binary',  \n    'metric': {'binary_logloss', 'auc'},  \n    'num_leaves':450,  \n    'max_depth': 25,  \n    'min_data_in_leaf': 150,  \n    'learning_rate': 0.1,  \n    'feature_fraction': 0.95,  \n    'bagging_fraction': 0.95,  \n    'bagging_freq': 10,  \n    'lambda_l1': 0,    \n    'lambda_l2': 0, \n    'min_gain_to_split': 0.1,  \n    'verbose': 0,  \n    'is_unbalance': True  \n}  ","beda5f06":"gbm = lgb.train(params,  \n                lgb_train,  \n                num_boost_round=10000,  \n                valid_sets=lgb_eval,  \n                early_stopping_rounds=500)  ","163aca20":"lr.fit(new_train, train['target'])  \nLR_TEST=lr.predict_proba(new_test)[:, 1]\nnew_test=new_test.astype(float)\nLGBM_TEST= gbm.predict(new_test, num_iteration=gbm.best_iteration) \n","1cc25f3c":"prediction=pd.DataFrame({'id':test.id,'LR_TEST':LR_TEST,'LGBM_TEST':LGBM_TEST})\nsubmit=pd.DataFrame({'id':test.id,'target':LR_TEST})\nprediction.to_csv('prediction.csv',index=False)\nsubmit.to_csv('submission.csv',index=False)","9fc4bce6":"# LightGBM","a2e76b63":"Here I want to sum up my work from version 1 to version 44.In this data,I have tried to remove variable, set parameters for model,balance the number of sample for target.Actually,most of scores are \nconcentrated on 0.805 to 0.80.\n\nNow I get the highest score is 0.80678,I remove the variable 'bin_0' and use logistic regression.\n\nActually,I could not get the high score from LGBM,even I tried to set the parameter.","38582493":"# Summary ","b1e7c4c3":"# Logistic Regression","d18764f0":"# testdata"}}