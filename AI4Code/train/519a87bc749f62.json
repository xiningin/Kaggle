{"cell_type":{"f20450af":"code","269f1535":"code","a3f3b840":"code","e332ab3a":"code","a64be86b":"code","3ec41d4d":"code","6ee48d12":"code","2b3209f3":"code","5a5c7175":"code","c4b8aff0":"code","ffc41cdf":"code","05a941d2":"code","118c5cc5":"code","0ff77a50":"code","0e58f1c5":"code","850f4051":"code","b02ddd20":"code","24f33faa":"code","0754a981":"code","f9237005":"code","a3729b58":"code","61f534ec":"code","98aa93a0":"code","f5cf3d9f":"code","f956f5a3":"code","591be706":"code","8b4f9d84":"code","e8b5bff4":"code","7f4aff56":"code","7dc315ef":"code","8c3c1946":"markdown","a9c2765d":"markdown","27e77be0":"markdown","14131fd0":"markdown","8bb77396":"markdown","4e5a984f":"markdown","7c7f06fd":"markdown","964ba3db":"markdown","4891d9b3":"markdown","10fad530":"markdown","f79de74a":"markdown","794fe95f":"markdown","fe8eca3d":"markdown","c45cbe50":"markdown"},"source":{"f20450af":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom IPython.display import clear_output\nimport numpy as np\nimport matplotlib.pyplot as plt\ndevice= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nimport random\nfrom torch.nn.utils.rnn import pack_sequence, pad_packed_sequence","269f1535":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a3f3b840":"df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\ndate_features = pd.read_csv('\/kaggle\/input\/m5features\/date_features.csv')\nsales_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nann_features = pd.read_csv('\/kaggle\/input\/m5features\/ann_features.csv')\ndf['id'] = df['id'].str[:-11]\nsales_prices['sell_price'] = MinMaxScaler().fit_transform(sales_prices['sell_price'].to_numpy().reshape(-1,1))\nsales_prices['id'] = sales_prices['item_id'] + \"_\"  + sales_prices['store_id'] \nsales_prices.drop(['item_id','store_id'], axis =1, inplace = True)","e332ab3a":"date_features.head()","a64be86b":"ann_features.head()\n#std_x , mean_x ....skew_x are at the item level\n#std_y , mean_y and skew_y are at the category level we can add more such encodings if needed at the store level and state level\n#Help : Suggest me more aggregate properties of the time series","3ec41d4d":"class MinMaxtransformer():\n    ''' A class to scale the time series data for each item_id'''\n    def __init__(self,d_x,d_y, info = None):\n        self.d_x = d_x\n        self.d_y = d_y\n        if info is None :\n            self.info = pd.DataFrame({'id': [],'min':[],'max':[]})\n        else :\n            self.info = info\n    \n    def fit(self, df):\n        '''Will store in min and max values of the rows in a info dataframe'''\n        self.info['id'] = df['id']\n        self.info['max']= df.loc[:,self.d_x:self.d_y].max(axis=1)\n        self.info['min']= df.loc[:,self.d_x:self.d_y].min(axis=1)\n        self.info['maxdiffmin'] = self.info['max'] - self.info['min']\n    \n    def transform(self , df, d_x = None ,d_y = None):\n        if d_x == None or d_y == None :\n            d_x = self.d_x\n            d_y = self.d_y\n        df = pd.merge(df,self.info, on ='id', how ='left')\n        for col in df.loc[:,d_x:d_y].columns:\n            df[col] = (df[col] - df['min'])\/(df['maxdiffmin'])\n        df.drop(['min','max', 'maxdiffmin'],axis =1, inplace = True)\n        return df\n    \n    def reverse_transform(self, df, d_x =None,d_y = None, round_ = False):\n        df = pd.merge(df,self.info, on ='id', how ='left')\n        if d_x == None or d_y == None :\n            d_x = self.d_x\n            d_y = self.d_y\n        for col in df.loc[:,d_x:d_y].columns:\n            df[col] = df[col] * df['maxdiffmin'] + df['min']\n            if round_ :\n                df[col] = round(df[col])\n        df.drop(['min','max', 'maxdiffmin'],axis =1, inplace = True)\n        return df\n    ","6ee48d12":"mmt  = MinMaxtransformer('d_1','d_1913')\nmmt.fit(df)","2b3209f3":"from sklearn.model_selection import train_test_split\ntrainids, testids = train_test_split(df.loc[:,['id']], train_size = 0.8, random_state = 1234)\ntrainids = trainids['id'].to_list()\ntestids = testids['id'].to_list()","5a5c7175":"len(trainids)","c4b8aff0":"len(testids)","ffc41cdf":"class M5dataloader_v2():\n    def __init__(self, ids, batch_size, estart = 'd_1', eend ='d_1913', dstart = 'd_1914', dend = 'd_1941'):\n        '''IDs to be passed in list format'''\n        self.ids = ids\n        self.iteration = 0\n        self.batch_size = batch_size\n        self.estart = estart\n        self.eend = eend\n        self.dstart = dstart\n        self.dend = dend\n    \n    def get_data(self, df,date_features,sales_prices, ann_features, device = device):\n        start = (self.iteration * self.batch_size)% len(self.ids)\n        end = start + self.batch_size\n        self.iteration +=1\n        if( end < len(self.ids)):\n            batchidlist = self.ids[start:end]\n        else :\n            end = end%len(self.ids)\n            batchidlist = [id for id in self.ids[start:]] + [ id for id in self.ids[:end]]\n        filt = df['id'].isin(batchidlist)\n        batch_data = df.loc[filt,:].drop(['item_id','dept_id','cat_id', 'store_id','state_id'], axis = 1 )\n        batch_data  =  mmt.transform(batch_data,'d_1', 'd_1941')\n        batch_data.set_index('id', inplace = True)\n        \n        \n        estart = self.estart\n        eend = self.eend\n        dstart = self.dstart\n        dend = self.dend\n        # Encoder_input tensor generation\n        \n        encoder_data = batch_data.loc[:,estart:eend]\n        encoder_data.reset_index(inplace = True)\n        encoder_data = encoder_data.melt(id_vars =['id'], value_vars = encoder_data.columns.to_list()[1:],var_name ='d', value_name ='count')\n        encoder_data = pd.merge(encoder_data,date_features,how = 'left',on ='d').drop(['Unnamed: 0'], axis =1)\n        encoder_data = pd.merge(encoder_data,sales_prices,how = 'left',on =['id','wm_yr_wk']).drop(['date','wm_yr_wk'], axis =1)\n        encoder_data['d']= encoder_data['d'].str[2:].apply(int)\n        encoder_data = encoder_data.dropna(axis = 0)\n        encoder_data = pd.DataFrame( { 'id': encoder_data['id'], 'd': encoder_data['d'],'Encoder_input_vector' : encoder_data.iloc[:,2:].to_numpy().tolist()})\\\n        .pivot(index ='id' , columns= 'd', values= 'Encoder_input_vector')\n        index_vals = encoder_data.isnull().sum(axis =1).sort_values(ascending = True).index\n        encoder_data = encoder_data.reindex(index_vals)\n        l =[]\n        for i in range(self.batch_size):\n            t = torch.tensor(encoder_data.iloc[i,:].dropna().tolist()).float()\n            l.append(t)\n        \n        encoder_data = pack_sequence(l) # pack sequence from torch.nn.utils.rnn returns a packed sequence object\n        \n        \n        #Decoder input tensor generation\n        \n        decoder_data = batch_data.loc[:,dstart:dend]\n        decoder_data.reset_index(inplace = True)\n        decoder_data = decoder_data.melt(id_vars =['id'], value_vars = decoder_data.columns.to_list()[1:],var_name ='d', value_name ='count')\n        decoder_data = pd.merge(decoder_data, date_features, how = 'left' , on = 'd').drop(['Unnamed: 0'], axis =1)\n        decoder_data = pd.merge(decoder_data,sales_prices,how = 'left',on =['id','wm_yr_wk']).drop(['date','wm_yr_wk'], axis =1)\n        decoder_data['d'] = decoder_data['d'].str[2:].apply(int)\n        ground_truth =  decoder_data.loc[:,['id','d','count']]\n        decoder_data.drop(['count'], axis = 1, inplace = True)\n        decoder_data = pd.DataFrame( { 'id': decoder_data['id'], 'd': decoder_data['d'],'Decoder_input_vector' : decoder_data.iloc[:,2:].to_numpy().tolist()})\\\n        .pivot(index ='id' , columns= 'd', values= 'Decoder_input_vector')\n        decoder_data = decoder_data.reindex(index_vals)\n        decoder_data = torch.tensor(np.array(decoder_data.values.tolist())).transpose(0,1)\n        decoder_data = decoder_data.float()\n        \n    \n        ground_truth = ground_truth.pivot(index = 'id',   columns ='d', values = 'count')\n        ground_truth = ground_truth.reindex(index_vals)\n        ground_truth = torch.tensor(ground_truth.to_numpy().reshape(self.batch_size ,28,1)).transpose(0,1)\n        ground_truth = ground_truth.float()\n      \n    \n        \n        # ann_data\n        filt2 = ann_features['id'].isin(batchidlist)\n        ann_data = ann_features.loc[filt2,:]\n        ann_data.set_index('id', inplace = True)\n        ann_data = ann_data.reindex(index_vals)\n        ann_data = torch.tensor(ann_data.to_numpy().reshape(self.batch_size, ann_data.shape[1]))\n        ann_data = ann_data.float()\n        \n        return (encoder_data.to(device), decoder_data.to(device),ground_truth.to(device), ann_data.to(device))","05a941d2":"dataloader = M5dataloader_v2(trainids,10) # initialize it","118c5cc5":"encoder_data, decoder_data, ground_truth,ann_data = dataloader.get_data(df,date_features,sales_prices,ann_features, device ='cpu') \n# getting input dimensions of encoder , decoder and ann and testing the data loader\nann_input_size = ann_data.shape[1] \nenc_input_size= encoder_data.data.shape[1]\ndec_input_size= decoder_data.shape[2]+1 # we add one to include output from previous state as input","0ff77a50":"print(ann_input_size , \" \", enc_input_size, \" \", dec_input_size)","0e58f1c5":"#Lets look at the shapes of the tensor\nprint('\\n ANN tensor shape :', ann_data.shape)\nprint('\\n encoder tensor shape :', encoder_data.data.shape)\nprint('\\n decoder tensor shape :', decoder_data.shape)\nprint('\\n ground_truth tensor shape :', ground_truth.shape)\n\n# Note Encoder tensor is a packed tensor","850f4051":"class EncoderRNN(nn.Module):\n    def __init__(self, ann_input_size,enc_input_size, hidden_size, n_layers=1, dropout=0.1):\n        super(EncoderRNN, self).__init__()\n        \n        self.ann_input_size = ann_input_size\n        self.enc_input_size = enc_input_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        \n        \n        \n        self.ann_to_hidden =nn.Sequential(nn.Linear(ann_input_size,96), \n                                 nn.ReLU(),\n                                 nn.Linear(96,hidden_size))\n        self.gru = nn.GRU(enc_input_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n        \n    def forward(self, ann_data, encoder_data):\n        # Note: we run this all at once (over multiple batches of multiple sequences)\n        hidden = self.ann_to_hidden(ann_data)\n        hidden = hidden.reshape(1, hidden.shape[0],hidden.shape[1]).repeat(self.n_layers*2,1,1)\n        \n        outputs, hidden = self.gru(encoder_data, hidden)\n        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n        return outputs, hidden\n    \nclass M5_EncoderDecoder_v2(nn.Module):\n    \n    def __init__(self, ann_input_size,enc_input_size, dec_input_size, hidden_size, output_size = 1, verbose=False):\n        super(M5_EncoderDecoder_v2, self).__init__()\n        self.ann_input_size = ann_input_size\n        self.enc_input_size = enc_input_size\n        self.dec_input_size = dec_input_size + hidden_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        \n        self.encoder_rnn_cell = EncoderRNN(ann_input_size,enc_input_size, hidden_size, n_layers=2, dropout=0.1)\n        self.decoder_rnn_cell = nn.GRU(dec_input_size+hidden_size, hidden_size)\n        \n        self.h2o = nn.Linear(hidden_size, output_size)\n        self.verbose = verbose\n        self.U = nn.Linear(self.hidden_size, self.hidden_size)\n        self.W = nn.Linear(self.hidden_size, self.hidden_size)\n        self.V = nn.Linear(self.hidden_size,1)\n        self.final = nn.ReLU()\n    \n        \n    def forward(self, ann_data, encoder_data, decoder_data, ground_truth = None, steps = 28 , device = device):\n    \n        \n        batch_size = decoder_data.shape[1]\n        encoder_outputs,hidden = self.encoder_rnn_cell.forward(ann_data,encoder_data)\n        \n        U = self.U(encoder_outputs)\n        initial_ground = torch.zeros(1,decoder_data.shape[1],1).to(device)\n        flag = 1\n        if ground_truth is None:\n            ground_truth = initial_ground\n            flag = 0\n        else :\n            ground_truth = torch.cat((initial_ground,ground_truth),0)\n        \n        hidden = torch.sum(hidden,dim =0).view(1,batch_size,-1)\n        \n        outputs = []\n        for i in range(steps) :\n            W  = net.W(hidden.repeat(encoder_outputs.shape[0],1,1))\n            V= net.V(torch.tanh(U+W))\n            alpha = F.softmax(V, dim=0)\n            attn_applied = torch.bmm(alpha.T.transpose(0,1),encoder_outputs.transpose(0,1))\n            if (i == 0):\n                decoder_input = torch.cat((ground_truth[i,:,:].float(),decoder_data[i,:,:].float(),attn_applied.transpose(0,1)[0,:,:].float()),1)\n            \n            if(i > 0):\n                if flag != 0 :\n                    decoder_input = torch.cat((ground_truth[i,:,:].float(),decoder_data[i,:,:].float(),attn_applied.transpose(0,1)[0,:,:].float()),1)\n                    \n                else:\n                    decoder_input = torch.cat((out[:,0].view(batch_size, 1).float(),decoder_data[i,:,:].float(),attn_applied.transpose(0,1)[0,:,:].float()),1) \n                    # no need to use i-1 as we have added a timestep inthe form of initial ground\n            \n            _ ,hidden = self.decoder_rnn_cell(decoder_input.view(1,decoder_input.shape[0],decoder_input.shape[1]).float(), hidden)\n            out = self.h2o(hidden) \n            out = self.final(out)\n            out = out.view(out.shape[1],self.output_size)\n            outputs.append(out) # verify dimensions\n        return outputs","b02ddd20":"def criterion_binomial_log_likehood(mu, alpha, truth):\n    m = mu.reshape(-1)\n    a = torch.sigmoid(alpha.reshape(-1))\n    t = truth.reshape(-1)\n\n    am = torch.clamp_min(a*m ,1e-8)\n\n    likelihood = \\\n          torch.lgamma(t + 1. \/a) \\\n        - torch.lgamma(t + 1) \\\n        - torch.lgamma(1. \/ a) \\\n        - 1. \/ a * torch.log(1 + am) \\\n        + t * torch.log(am\/ (1 + am))\n\n    loss = -likelihood.mean()\n    return loss","24f33faa":"def train_batch(net, batch,batch_size,opt,criterion,device, teacher_force = False):\n    net.train().to(device)\n    opt.zero_grad()\n    encoder_data, decoder_data, ground_truth,ann_data = batch\n    ground_truth = ground_truth.to(device)\n    if teacher_force :\n        outputs = net.forward(ann_data, encoder_data, decoder_data, ground_truth)\n    else :\n        outputs = net.forward(ann_data,encoder_data,decoder_data)\n    \n    loss = torch.zeros(1,1).float().to(device)\n    for i, output in enumerate(outputs):\n        loss += criterion(output[:,0], output[:,1],ground_truth[i,:,:]).float()\n    loss.backward()\n    opt.step()\n    \n    return loss\/batch_size","0754a981":"def prediction(testids, net, df, sales_prices, date_features, ann_features,round_ = False, steps = 28,idstart = 1914):\n    testloader = M5dataloader_v2(testids,len(testids))\n    encoder_data, decoder_data, ground_truth,ann_data = testloader.get_data(df,date_features,sales_prices,ann_features)\n    outputs = net.forward(ann_data,encoder_data,decoder_data)\n    pred= pd.DataFrame({'id' : testids})\n    for i, output in enumerate(outputs):\n        pred['d_' + str(idstart + i)] = output[:,0].cpu().data.numpy()\n    \n    start = 'd_' + str(idstart)\n    end = 'd_' + str(idstart + 27) \n    pred = mmt.reverse_transform(pred,start,end, round_ = round_)\n    pred.set_index('id', inplace = True)\n    return pred","f9237005":"def actual_values(testids,df,steps = 28, idstart = 1914):\n    df.set_index('id',inplace = True)\n    start = 'd_' + str(idstart)\n    end = 'd_' + str(idstart + steps -1) \n    act = df.loc[testids,start:end]\n    df.reset_index(inplace = True)\n    return act","a3729b58":"def validation_error(pred,act):\n    return np.square(pred.to_numpy() - act.to_numpy()).sum()\/act.to_numpy().size","61f534ec":"def get_plots(ids, net):\n    if len(ids) > 25:\n        return \"the number of ids in the list exceeds the limit of 25\"\n    \n    trainhead = actual_values(ids, df,steps = 1000 , idstart = 886).T\n    testvalues = actual_values(ids, df,steps = 28 , idstart = 1914).T\n    predictions = prediction(ids, net, df, sales_prices, date_features, ann_features, round_ = False).T\n    b = ['_encoder','_actual', '_pred']\n    for i , x in enumerate([trainhead, testvalues, predictions]):\n        x.columns = [x for x in map(lambda x: x + b[i],x.columns.to_list())]\n        x.reset_index(inplace = True)\n        x.rename(columns={'index' : 'Days'}, inplace = True)\n        x['Days'] = x['Days'].str[2:].apply(int)\n        \n    fig, ax = plt.subplots(nrows = len(ids), ncols = 1,figsize=(25,4 * len(ids)))\n    for i in range(len(ids)):\n        if len(ids) == 1:\n            trainhead.plot(x = 'Days',y=[i+1],ax=ax);\n            testvalues.plot(x = 'Days',y=[i+1],ax=ax);\n            predictions.plot(x = 'Days',y=[i+1],ax=ax);\n        else :\n            trainhead.plot(x = 'Days',y=[i+1],ax=ax[i]);\n            testvalues.plot(x = 'Days',y=[i+1],ax=ax[i]);\n            predictions.plot(x = 'Days',y=[i+1],ax=ax[i]);","98aa93a0":"def get_submission(idlist,net, batch = 100):\n    \n    submission = []\n    for i in range(len(idlist)\/\/batch + int(len(idlist)%batch !=0)):\n        print(\"Iteration \", i, \"\/\", len(idlist)\/\/batch + int(len(idlist)%batch !=0))\n        start = i * batch\n        end = start + batch\n        if(end > len(idlist)): \n            end = len(idlist)\n        batchidlist = idlist[start:end] \n        pred = prediction(batchidlist, net, df, sales_prices, date_features, ann_features, round_ = False)\n        submission.append(pred) \n    return pd.concat(submission, axis =0)","f5cf3d9f":"def train_setup(net,trainids,testids,validation = False,plots = False, lr = 0.01, n_batches = 5, batch_size = 140, momentum = 0.9, display_freq=1, device = device,test_batch_size = 100, teacher_upto =0 ):\n    \n    net = net.to(device)\n    criterion = criterion_binomial_log_likehood # changed from nn.MSELoss\n    opt = optim.Adam(net.parameters(), lr=lr)\n    teacher_force_upto = n_batches\/\/3\n    trainloader = M5dataloader_v2(trainids,batch_size)\n    loss_arr = np.zeros(n_batches + 1)\n    if validation :\n        valid_error = []\n        valid_xaxis =[]\n    for i in range(n_batches):\n        batch = trainloader.get_data(df, date_features, sales_prices, ann_features) \n        loss_arr[i+1] = (loss_arr[i]*i + train_batch(net,batch,batch_size, opt, criterion, device = device, teacher_force = (i\/n_batches < teacher_upto) ))\/(i + 1)\n        \n        if i%display_freq == display_freq-1:\n            if plots :\n                clear_output(wait=True)\n            \n            if validation :\n                ids = random.sample(testids,test_batch_size)\n                pred = prediction(ids, net, df, sales_prices, date_features, ann_features, round_ = False)\n                act = actual_values(ids, df,steps = 28 , idstart = 1914)\n                valid_xaxis.append(i)\n                v = validation_error(pred,act)\n                valid_error.append(v)\n                print('Validation error  ', v, \" per observation as tested on \", test_batch_size, \" random samples from test set\")\n            print('Epoch ',(i*batch_size)\/len(trainids),' Iteration', i, 'Loss ', loss_arr[i])\n            \n            if plots:\n                fig, axes = plt.subplots(nrows =1, ncols=2 , figsize=(20,6))\n                axes[0].set_title('Train Error')\n                axes[0].plot(loss_arr[2:i], '-*')\n                axes[0].set_xlabel('Iteration')\n                axes[0].set_ylabel('Loss')\n\n                if validation :\n                    axes[1].set_title('Validation error')\n                    axes[1].plot(valid_xaxis, valid_error,'-*')\n                    axes[1].set_xlabel('Iteration')\n                    axes[1].set_ylabel('validation error')\n                plt.show()\n                print('\\n\\n')\n        if(i%100 ==0):    \n            torch.save(net.state_dict(), 'model.pt')\n        if(i%500 ==0):\n            filename = str(i) + 'model.pt'\n            torch.save(net.state_dict(), filename)\n    filename = str(i) + 'model.pt'\n    torch.save(net.state_dict(), filename)    \n    return (loss_arr,net)","f956f5a3":"validation = True ; plots = True ; lr = 0.01 ; n_batches = 1000 ; batch_size = 140; momentum = 0.9; display_freq=5 ; device = device ; test_batch_size = 100 ; teacher_upto = 0.5","591be706":"#Training cell\n\nnet = M5_EncoderDecoder_v2(ann_input_size = 25 ,enc_input_size= 30, dec_input_size= 30, hidden_size=126,output_size= 2)\nlosses,net = train_setup(net,trainids,testids,validation,plots, lr, n_batches, batch_size, momentum , display_freq, device,test_batch_size, teacher_upto)\n\n\n# I have already trained the model batch_size = 140 , iterations = 2000, learning_rate = 0.01 , forgot teacher enforcing guess would have learned faster\n# No more GPU quota :(  sad life!! \n# the train_setup function supports plots to monitor loss and also validation feel free to play with it","8b4f9d84":"#We will load my pretrained model\n#net = M5_EncoderDecoder_v2(ann_input_size = 25 ,enc_input_size= 30, dec_input_size= 30, hidden_size=256,output_size = 2 )\n#net.load_state_dict(torch.load('..\/input\/m5trained-models\/1500model.pt', map_location = device))\n#net.eval()\n#net.to(device)","e8b5bff4":"#get_plots(random.sample(trainids,20),net)","7f4aff56":"#idlist = df['id'].to_list()\n#submission_validation = get_submission(idlist,net)\n#submission_validation.to_csv(\"1500submission.csv\")\n#cant submit this submission file coz its not in submission format I appended evaluation with 0's to it and submitted to get an WRMSE of 0.603","7dc315ef":"device","8c3c1946":"Fitting the transformer to the input sequence\/timeseries d_1 - d_1913. It will be later used in the batchloader and prediction modules to transform the inputs and outputs.","a9c2765d":"# A look at the Neural Network(ANN) features and date features","27e77be0":"# The Dataloader :\nThe dataloader has been modified. It now outputs a packed tensor for encoder_data. \nI will also cover the internal workings of the loader on a seperate notebook. Just let me know in the comments if anyone is interested\n","14131fd0":"# Training Parameters","8bb77396":"# The Code\n\nComponents:\n* The Model Class : M5_Encoder_Decoder\n* Transformer Class : MinMaxTransformer\n* Dataloader Class : M5dataloader ( Given a batch of ids loads the required tensors for the ANN , encoder and decoder respectively)\n(*Yup it's a batched implementation!! Vectorization for the win!! I also wanted to include padding but couldnt due to Pytorch not supporting forward padding.\nI could manually do it though. Lets see*)\n* Batch trainer function : train_batch\n* Training helper function : train_setup\n* Plotting function : get_plots\n* Prediction function : Helps in postprocessing of decoder outputs to a usable dataframe format\n* Validation score fucntion\n\nI'll add more details but thats a brief on the content of the code\nPeople interested can through the code I would also be happy to provide any clarifications\n\n*Lets jump down to the interesting part for now!!!*\n**Go to model training those who dont want to see copious amounts of code!!! XD**\n\n\n","4e5a984f":"Given a set of ids it will load the tensors needed for the neural network training or evaluation\nEverytime the get_data function is called it loads a different batch from the givenids till it goes through the entire dataset","7c7f06fd":" ## The Encoder Decoder Design\n*People not familiar with it can think of it as models that take in a sequence\/pattern and based on it generates a sequence\/pattern and hence they are also know as seq2seq models*\n\nIn our approach we will use a simple neural network to capture these static features and feed them in the hidden state of the encoder. The encoder will then go across the timeseries (1913 time steps) taking the demand concatenated with the selling price and date features as the input. Finally the decoder will make predictions step by step the input will be the ouput from previous state ( or ground truth in case of teacher enforcing) , concatenated with the date features and sale prices. We will also concat attention to the hidden state before we make the output predictions ( 28 time steps).","964ba3db":"# **The Model**","4891d9b3":"# Data Transformer :\nThe purpose of the below transformer is to scale **each** of the time series based on the max and min values from d_1 to d_1913 we also later use this transformer to reverse transform the output values. We could have used a minmaxtransformer from sklearn but then I would have to individually apply it on each of the time series and also store the objects in a list fo future use.\n\nNote : Though a min max transformer scales from 0-1 there can be greater than 1 values between dates d_1914 and d_1941 as the demand may increase with time.\nThis is important to note as we cannot apply sigmoidal function to the output.","10fad530":"# **A Gentle Introduction**\nTime Series forecasting has always been a challenge it has evolved from simple statistical techniques like exponential smoothening, ARIMA , SARIMA, SARIMAX to using machine learning regression models based on algorithms like Xgboost , LGBM , linear regression, RandomForest etc.\n\nThe earlier statistical techniques like exponential smoothening,ARIMA and SARIMA had a limitation as it was designed on a single feature that is the time series itself as the input. To put it simply we could not feed it explicit features like the weather , holidays, or for example in our M5 forecasting dataset the selling price of the item. This limitation was overcome by machine learning models which could also include these explicit features to train the model.\nBut as the features started increasing the Machine learning models started overfitting also they faced a problem of autocorrelation causing them to act funnily in some cases.\n\nNeural networks try to address these limitations of machine learning solutions but at the cost of the difficulty of training such networks.\n\nIn the below solution we adopted a novel approach by using an encoder decoder model to solve the M5 Forecasting problem\nThe solution below assumes that in the rawest form times series is just a pattern and when we do forecasting we look at the input pattern which is a sequence of numbers and accordingly predict the future pattern\/sequence.\nWhile making the predictions we also use additional inputs such as date features ( is it a holiday? which day is  it? which month is it) we also keep in mind the amount to variation in the time series ( is it fluctuating a lot? is there seasonality? is it stagnant?) and we also ask questions about the underlying item ( Which store is it kept? what type of item is it etc)\n\n**Static Features** : If you give it a thought properties of the underlying item like the store in which its kept , the type of the item, are static features that dont change with each timestep.( we will call them static inputs)\n\n**Dynamic features** :The other inputs like selling price and date features change with each time step. ( We will call them dynamic inputs)\n\nThe ability of a encoder decoder model to capture both these static and dynamic features and treat them accordingly gives it the edge over machine learning solutions","f79de74a":"# Train Validation Split\n\nIts important to understand for our model our model is predicting an output sequence given an input sequence hence the below split","794fe95f":"# M5 Forecasting Encoder Decoder Model V2 with Attention using Pytorch\nThis is a modification of my previous notebook.\n\nI preferred creating an entirely new notebook as it was more than just some minor tweaks\n\nChanges in Model V2:\n1. Introduced padding of encoder inputs so that we can ignore the leading zeros\n2. Revamped the dataloader to support timeseries padding also gave flexibility to select encoder data range and decoder data range (Yeah!!Flexible training sets)\n3. Improved the model with bidirectional 2 layered GRU's ( Was amazingly dificult! Code requires refinement will do it soon)\n\nNotebook run status :\n* Bidirectional GRU\n* hidden_size = 128\n* Batch_size =\n* Iterations =\n* final model state store =\n\nUpcoming Developments stay tuned :\n1. Implementing custom loss based on negative binomial distributions ( Lets see if we can get uncertainity)\n2. Visualizing attention mechanism ( going to be difficult)\n3. Refining the input features\n4. Any of your suggestions ( Top priority!!! :D)\n5. Refining the code to be more userfriendly and reproducible\n\nDisclaimer : I am a simple guy trying complex things forgive me if I get some of the technicalities wrong.\nI am a mechanical engineer with and MBA in analytics tryin to get a grip on coding","fe8eca3d":"# Model Performance on Test Data\nLets look at 20 random plots of the 1500 iteration model\n\nThe plot is of 128 days","c45cbe50":"# Model Training :"}}