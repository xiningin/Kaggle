{"cell_type":{"0736b593":"code","1a08825b":"code","b850c6d0":"code","e519afed":"code","f9e601df":"code","1db2bc4d":"code","264db1d3":"code","60b9d780":"code","a07bab3f":"code","83679387":"code","3fe29ef0":"code","086f0e9a":"code","1d2f1b62":"code","5ca0c4d4":"code","debfce08":"code","c0e9f1ad":"code","85f9143b":"code","5841b516":"code","669cb958":"code","5ee54ca2":"code","ca3ec318":"code","ea2e268e":"code","5a78b0de":"code","1efe99a0":"code","ad102e67":"code","50588f1e":"markdown","b1205a5b":"markdown","00547014":"markdown","29c33c75":"markdown","145e6c83":"markdown","1b40ccf4":"markdown","cd70dc64":"markdown","2e1408b8":"markdown","100f30c7":"markdown","67180950":"markdown","31c361a4":"markdown","4fcd536d":"markdown","1757835c":"markdown","d2ef5f59":"markdown","29c3b3ab":"markdown","f696e8c0":"markdown","6fcf0f4b":"markdown","77466a30":"markdown","00ebd99a":"markdown"},"source":{"0736b593":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.datasets import load_boston\nfrom sklearn import preprocessing\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom prettytable import PrettyTable\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\nprint(\"DONE\")","1a08825b":"boston_data=pd.DataFrame(load_boston().data,columns=load_boston().feature_names)\nY=load_boston().target\nX=load_boston().data\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3)","b850c6d0":"# data overview\nboston_data.head(3)","e519afed":"print(X.shape)\nprint(Y.shape)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","f9e601df":"## Before standardizing data\nx_train","1db2bc4d":"# standardizing data\nscaler = preprocessing.StandardScaler().fit(x_train)\nx_train = scaler.transform(x_train)\nx_test=scaler.transform(x_test)","264db1d3":"## After standardizing data\nx_train","60b9d780":"x_test","a07bab3f":"## Adding the PRIZE Column in the data\ntrain_data=pd.DataFrame(x_train)\ntrain_data['price']=y_train\ntrain_data.head(3)","83679387":"x_test=np.array(x_test)\ny_test=np.array(y_test)","3fe29ef0":"type(x_test)","086f0e9a":"n_iter=100","1d2f1b62":"# SkLearn SGD classifier\nclf_ = SGDRegressor(max_iter=n_iter)\nclf_.fit(x_train, y_train)\ny_pred_sksgd=clf_.predict(x_test)\nplt.scatter(y_test,y_pred_sksgd)\nplt.grid()\nplt.xlabel('Actual y')\nplt.ylabel('Predicted y')\nplt.title('Scatter plot from actual y and predicted y')\nplt.show()","5ca0c4d4":"print('Mean Squared Error :',mean_squared_error(y_test, y_pred_sksgd))","debfce08":"# SkLearn SGD classifier predicted weight matrix\nsklearn_w=clf_.coef_\nsklearn_w","c0e9f1ad":"type(sklearn_w)","85f9143b":"def My2CustomSGD(train_data,learning_rate,n_iter,k,divideby):\n    w=np.zeros(shape=(1,train_data.shape[1]-1))\n    b=0\n    cur_iter=1\n    while(cur_iter<=n_iter): \n#         print(\"LR: \",learning_rate)\n        temp=train_data.sample(k)\n        #print(temp.head(3))\n        y=np.array(temp['price'])\n        x=np.array(temp.drop('price',axis=1))\n        w_gradient=np.zeros(shape=(1,train_data.shape[1]-1))\n        b_gradient=0\n        for i in range(k):\n            prediction=np.dot(w,x[i])+b\n#             w_gradient=w_gradient+(-2\/k)*x[i]*(y[i]-(prediction))\n#             b_gradient=b_gradient+(-2\/k)*(y[i]-(prediction))\n            w_gradient=w_gradient+(-2)*x[i]*(y[i]-(prediction))\n            b_gradient=b_gradient+(-2)*(y[i]-(prediction))\n        w=w-learning_rate*(w_gradient\/k)\n        b=b-learning_rate*(b_gradient\/k)\n        \n        cur_iter=cur_iter+1\n        learning_rate=learning_rate\/divideby\n    return w,b","5841b516":"def predict(x,w,b):\n    y_pred=[]\n    for i in range(len(x)):\n        y=np.asscalar(np.dot(w,x[i])+b)\n        y_pred.append(y)\n    return np.array(y_pred)","669cb958":"w,b=My2CustomSGD(train_data,learning_rate=1,n_iter=100,divideby=2,k=10)\ny_pred_customsgd=predict(x_test,w,b)\n\nplt.scatter(y_test,y_pred_customsgd)\nplt.grid()\nplt.xlabel('Actual y')\nplt.ylabel('Predicted y')\nplt.title('Scatter plot from actual y and predicted y')\nplt.show()\nprint('Mean Squared Error :',mean_squared_error(y_test, y_pred_customsgd))","5ee54ca2":"print('Mean Squared Error :',mean_squared_error(y_test, y_pred_customsgd))","ca3ec318":"# weight vector obtained from impemented SGD Classifier\ncustom_w=w\nprint(custom_w)\nprint(type(custom_w))","ea2e268e":"w,b=My2CustomSGD(train_data,learning_rate=0.01,n_iter=1000,divideby=1,k=10)\ny_pred_customsgd_improved=predict(x_test,w,b)\n\nplt.scatter(y_test,y_pred_customsgd_improved)\nplt.grid()\nplt.xlabel('Actual y')\nplt.ylabel('Predicted y')\nplt.title('Scatter plot from actual y and predicted y')\nplt.show()\nprint('Mean Squared Error :',mean_squared_error(y_test, y_pred_customsgd_improved))","5a78b0de":"print('Mean Squared Error :',mean_squared_error(y_test, y_pred_customsgd_improved))","1efe99a0":"# weight vector obtained from impemented SGD Classifier\ncustom_w_improved=w\nprint(custom_w_improved)\nprint(type(custom_w_improved))","ad102e67":"###\nfrom prettytable import PrettyTable\nx=PrettyTable()\nx.field_names=['Model','Weight Vector','MSE']\nx.add_row(['SKLearn SGD',sklearn_w,mean_squared_error(y_test, y_pred_sksgd)])\nx.add_row(['Custom SGD',custom_w,mean_squared_error(y_test,y_pred_customsgd)])\nx.add_row(['Custom SGD Improved',custom_w_improved,mean_squared_error(y_test,y_pred_customsgd_improved)])\nprint(x)","50588f1e":"## 3.2 Obtaining Weights from Custom SGD Improved","b1205a5b":"## Reference","00547014":"## 2.2 Obtaining Weights from Custom SGD","29c33c75":"## Importing libraries","145e6c83":"## Changes made in the following parameters to improve the result\n1. Learning Rate=0.01 initially and will not be divided by any number over the Iterations\n2. size of K is kept as k=10\n3. Iterations = 1000","1b40ccf4":"## Conclusion\n1. We can see the our plain custom SGD performed very poor as compared to the SKLearn SGD.\n1. When we changed the learning rate and  Batch size, the our custom SGD performed as good as the SKLearn SGD","cd70dc64":"## Fixing Total Number of Iterations for 1. SKLearn SGD and 2. Custom SGD","2e1408b8":"## 1.2 Obtaining Weights from SKLearn SGD","100f30c7":"# 3. Improved SGD","67180950":"#  Comparision ","31c361a4":"## 3.1 MSE for the Custom SGD Improved","4fcd536d":"## Setting custom parameters: As mentioned in the assignment video, the following parameters are set\n1. As mentioned Learning Rate=1 initially and will be divided by 2 over the Iterations\n2. As mentioned size of K is kept as k=10","1757835c":"## Data Loading and Preprocessing:","d2ef5f59":"# Custom SGD implementation for Linear Regression on Boston House dataset\n   ### Importing libraries\n   ### Data Loading and Preprocessing\n   ### Fixing Total Number of Iterations for 1. SKLearn SGD and 2. Custom SGD\n    \n   ## 1. SKLearn Implementation of SGD\n       1.1 Plot and MSE for the SK Learn SGD\n       1.2 Obtaining Weights from SKLearn SGD\n   ## 2. Custom Implementation Of SGD\n       . Setting custom parameters\n       2.1 Plot and MSE for the Custom SGD\n       2.2 Obtaining Weights from Custom SGD\n   ## 3. Improved Custom SGD\n       . Setting new custom parameters\n       3.1 Plot and MSE for the Custom SGD Improved\n       3.2 Obtaining Weights from Custom SGD Improved\n   ## Comparison\n   ## Conlusion\n   ## References\n   ----------------------------------------------","29c3b3ab":"# 2. Custom Implementation Of SGD","f696e8c0":"## 2.1 MSE for the Custom SGD","6fcf0f4b":"# 1. SKLearn Implementation of SGD","77466a30":"## 1.1 MSE for the SK Learn SGD","00ebd99a":"[1] My Medium Blog: https:\/\/medium.com\/@nikhilparmar9\/simple-sgd-implementation-in-python-for-linear-regression-on-boston-housing-data-f63fcaaecfb1<br>\n[2]https:\/\/machinelearningmastery.com\/implement-linear-regression-stochastic-gradient-descent-scratch-python\/ <br>\n[3]https:\/\/www.kaggle.com\/premvardhan\/stocasticgradientdescent-implementation-lr-python <br>\n[4]https:\/\/www.kaggle.com\/arpandas65\/simple-sgd-implementation-of-linear-regression\/notebook <br>\n[5]https:\/\/www.kaggle.com\/tentotheminus9\/linear-regression-from-scratch-gradient-descent<br>"}}