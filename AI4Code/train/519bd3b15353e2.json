{"cell_type":{"b8e04f9a":"code","354df45b":"code","b23ff133":"code","65cf0b89":"code","f84f17a4":"code","dc4aeac0":"code","952c3541":"code","58590d13":"code","5770cb74":"code","da84cc92":"code","c7e07d1d":"code","93c06f06":"code","fa1a3ba2":"code","257324f3":"code","7dba7018":"code","124f804c":"code","4baa19f7":"code","dfb10e44":"code","b109f6f2":"code","9ee79d28":"code","c14d52ce":"code","aa2f6605":"code","8cd2127c":"code","8b996a80":"code","ac096485":"code","eebba826":"markdown","b13c3b63":"markdown","f01b8c8f":"markdown","a9ab7ca6":"markdown","c64c6607":"markdown","b156aae3":"markdown","73c5b60a":"markdown","bf93b3a1":"markdown","f5596723":"markdown","152889ca":"markdown","0d6eb1f7":"markdown","fbb0a1bf":"markdown","a085c37e":"markdown","753a3bf9":"markdown","b2c01e6c":"markdown","e2f2799b":"markdown","e06bd3ef":"markdown","bf36bdb0":"markdown","1448b305":"markdown","162d7b46":"markdown","877cbff9":"markdown","66e68976":"markdown","2b15004c":"markdown","f9c60361":"markdown","4ddb0f5e":"markdown","9d6adadc":"markdown","f367a071":"markdown","b55dad0e":"markdown"},"source":{"b8e04f9a":"#data process\nimport pandas as pd\nimport numpy as np\n\n#pandas setting\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_info_columns\", 300)","354df45b":"#data visualiztion\nimport seaborn as sns\nimport matplotlib.pyplot as plt","b23ff133":"#machine learning model\nfrom sklearn.preprocessing import RobustScaler\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","65cf0b89":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv', nrows=10000)\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-oct-2021\/test.csv', nrows=10000)","f84f17a4":"#print(train_df.head())\n#print(train_df.info())\n#print(train_df.columns)","dc4aeac0":"features = [col for col in train_df.columns if 'f' in col]","952c3541":"binary_feature = []\ncontinuous_feature = [] \nfor feature in features:\n    if \"int\" in str(train_df[feature].dtype):   #int64, int32... belong to int.\n        binary_feature.append(feature)\n    else:\n        continuous_feature.append(feature)","58590d13":"print(\"*\"*50)\nprint(\"the number of continuous feature:  {}\".format(len(continuous_feature)))\nprint(\"*\"*50)\nprint(\"\\n\")\nprint(\"*\"*50)\nprint(\"the number of binary feature:  {}\".format(len(binary_feature)))\nprint(\"*\"*50)\nprint(\"\\n\")","5770cb74":"train_df[features].describe().T.style.bar(subset = [\"mean\"]).background_gradient(subset = [\"std\"]).background_gradient(subset = [\"50%\"])","da84cc92":"print(\"*\"*50)\nprint(\"the number of missing value in train sample is {0}\".format(train_df[features].isnull().sum().sum()))\nprint(\"*\"*50)\nprint(\"\\n\")\nprint(\"*\"*50)\nprint(\"the number of missing value in test sample is {0}\".format(test_df[features].isnull().sum().sum()))\nprint(\"*\"*50)\nprint(\"\\n\")","c7e07d1d":"train_df[\"target\"].value_counts().plot(kind = \"pie\", figsize = (8,8)).legend()","93c06f06":"corr_matrix = train_df[features + [\"target\"]].corr()","fa1a3ba2":"feature_corr_target = corr_matrix[\"target\"].apply(lambda x:abs(x)).sort_values(ascending = False)[0:30]\nfeature_corr_target.drop(\"target\", inplace = True)\nplt.figure(figsize = (15,7))\nsns.barplot(x = feature_corr_target.index, y = feature_corr_target)","257324f3":"fig, axes = plt.subplots(24,10,figsize=(17, 40))\naxes = axes.flatten()\n\nindex = 0 \n\nfor idx, ax in enumerate(axes):\n    if len(continuous_feature) == index:\n        break\n    sns.kdeplot(data=train_df, x=continuous_feature[index],hue = 'target', ax=ax)\n    ax.set_title(continuous_feature[index], loc='right', weight='bold', fontsize=10)\n    index = index + 1","7dba7018":"fig, axes = plt.subplots(12,4,figsize=(12,30))\naxes = axes.flatten()\n\nindex = 0\nfor idx,ax in enumerate(axes):\n    if index == len(binary_feature):\n        break\n    sns.histplot(data = train_df, x= binary_feature[index], hue = \"target\", ax = ax, multiple=\"dodge\")\n    index = index + 1","124f804c":"train_dataset = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv\")\ntest_dataset = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-oct-2021\/test.csv\")","4baa19f7":"scaler = RobustScaler()\ntrain_dataset[features] = scaler.fit_transform(train_dataset[features])\ntest_dataset[features] = scaler.transform(test_dataset[features])","dfb10e44":"N_SPLITS = 5\nN_ESTIMATORS = 20000  \nEARLY_STOPPING_ROUNDS = 1000 \nVERBOSE = 1000\nSEED = 200","b109f6f2":"lgb_params = {\n    'objective': 'binary',\n    'n_estimators': N_ESTIMATORS,\n    'random_state': SEED,\n    'learning_rate': 5e-3,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n}","9ee79d28":"scale = [0.1, 0.1, 0.2, 0.2, 0.4]","c14d52ce":"kdf = StratifiedKFold(n_splits = N_SPLITS, shuffle = True, random_state = SEED)\ny_pred = np.zeros(test_dataset.shape[0])\nlgb_importance = pd.DataFrame()\n\npred_df = pd.DataFrame()\nvalid_accuracy = pd.Series()\n\nfor fold, (train_idx, valid_idx) in enumerate(kdf.split(train_dataset,train_dataset[\"target\"])):\n    print(f'====== Fold {fold} ======')\n    X_train = train_dataset[features].iloc[train_idx]\n    y_train = train_dataset[\"target\"].iloc[train_idx]\n    X_valid = train_dataset[features].iloc[valid_idx]\n    y_valid = train_dataset[\"target\"].iloc[valid_idx]\n    model = LGBMClassifier(**lgb_params).fit(X_train, y_train,\n                                            eval_set = [(X_valid, y_valid)],\n                                            eval_metric = \"auc\",\n                                            early_stopping_rounds = EARLY_STOPPING_ROUNDS,\n                                            verbose = VERBOSE)\n    pred_df[str(fold)] = model.predict_proba(test_dataset[features])[:,-1] \/ N_SPLITS\n    valid_accuracy[str(fold)] = roc_auc_score(y_valid, model.predict(X_valid))\n    lgb_importance[\"fold_\"+str(fold)] = model.feature_importances_\nlgb_importance[\"name\"] = model.feature_name_\n","aa2f6605":"ensemble_sort = valid_accuracy.argsort()\nfor i in range(5):\n    y_pred += pred_df[str(ensemble_sort[i])] * scale[i] ","8cd2127c":"lgb_importance[\"mean\"] = lgb_importance[[\"fold_0\", \"fold_1\"]].mean(axis = 1)","8b996a80":"importance_result = lgb_importance[[\"name\", \"mean\"]].sort_values(by = \"mean\", ascending = False)[:30]\n\nplt.figure(figsize = (15,7))\n\nsns.barplot(data = importance_result, x = \"name\", y = \"mean\")\n","ac096485":"submission = pd.DataFrame({\"id\": test_dataset[\"id\"], \"target\": y_pred})\nsubmission.to_csv(\"submission.csv\", index = False)","eebba826":"### Univariate analysis\nEmmm, we have plot about each feature and target. In fact, most of the feature influence target slightly.","b13c3b63":"### Dataset conclusion\nAccording to the above result, we can find that:\n1. **missing value.** the trainset and testset all don't have the missing data. so we don't need to complete.\n2. **dataset size.** the dataset have 284 features. train dataset has 1000k rows and test dataset has 500k rows. it's large and maybe we should convert or delete some features to accelerate the training process\n3. **feature type.** type of feature can be classified to continuous feature and binary feature.\n4. **target.** The value of target is 0 or 1. And the distribution of target is normal","f01b8c8f":"### Data visualization\n","a9ab7ca6":"**scale** is used to weight the model.","c64c6607":"### kdeplot - find the contribution of continuous feature","b156aae3":"<div class=\"alert alert-block alert-info\">  \n<strong>Hello! I am a new Kaggler. I start to study the machine learning. As a result, I look at other notebook about this problem and conclude it according to my understanding.\n","73c5b60a":"<div class=\"alert alert-block alert-info\">\n    <h2><strong><center>Data Analysis<\/center><strong><\/h2>","bf93b3a1":"### Dataset","f5596723":"### Check the feature type\nUsually, \"feature.dtype = int\" means this feature is discrete and in this problem it represent \"binary\". Meanwhile, \"object\" means string, \"float\" means continous.","152889ca":"Raw data is too large. We had better sample 10k rows to analyze.","0d6eb1f7":"<div class=\"alert alert-block alert-info\">\n    <h2><strong><center>Training and Predicting<\/center><strong><\/h2> ","fbb0a1bf":"### Parameters  - LightGBM Hyperparameters","a085c37e":"### Loading the data","753a3bf9":"### histplot - find the contribution of binary feature","b2c01e6c":"<div class=\"alert alert-block alert-info\">\n    <h2><strong><center>Overview<\/center><strong><\/h2>","e2f2799b":"Too many features! I learnt some method about data analysis in GetStarted competition such as Titanic. Howeve , these methods are used to analyze the relation between one feature and target. if I analyze every feature using these methods, the work is hard.","e06bd3ef":"<div class=\"alert alert-block alert-info\">\n    <h2><strong><center>Experiment<\/center><strong><\/h2>","bf36bdb0":"### Target","1448b305":"<div class=\"alert alert-block alert-info\">\n    <h2><strong><center>Import Library<\/center><strong><\/h2>","162d7b46":"### Observe the corvariance \nlet us observe the corvariance between the feature and target. it is important to find the main feature quickly. Maybe this dataset have one feature that have non-linear relationship such as Y=X^2 and its corvariance equals zero so we may overlook this feature using this data analyis method. But this kind of method can find the linear relationship.","877cbff9":"<div class=\"alert alert-block alert-info\">\n    <h2><strong><center>Model Analysis<\/center><strong><\/h2>","66e68976":"<div class=\"alert alert-block alert-info\">\n    <h2><strong><center>Submission<\/center><strong><\/h2>","2b15004c":"According to these histplot, we find that most of the binary feature have a little influence on target independently except \"f22\". Also, the correlation coefficient of f22 is -0.5139, this influence is obvious.\n","f9c60361":"1. lightgbm without params: valid 0.849536 score 0.84863\n2. lightgbm with KFold and params: ","4ddb0f5e":"<div class=\"alert alert-block alert-info\">\n    <h2><strong><center>Reference<\/center><strong><\/h2>","9d6adadc":"1. [Simple EDA](https:\/\/www.kaggle.com\/subinium\/tps-oct-simple-eda\/comments)\n2. [\ud83d\udd25TensorFlow Decision Forests \ud83c\udf35 \ud83c\udf84 \ud83c\udf32 \ud83c\udf33 + W&B](https:\/\/www.kaggle.com\/usharengaraju\/tensorflow-decision-forests-w-b)\n3. [TPS Oct 2021: single LightGBM](https:\/\/www.kaggle.com\/hiro5299834\/tps-oct-2021-single-lightgbm)\n4. [LightGBM Baseline](https:\/\/www.kaggle.com\/yukiyamamoto\/lightgbm-baseline)\n5. [TPS Sep 2021 single LGBM](https:\/\/www.kaggle.com\/hiro5299834\/tps-sep-2021-single-lgbm)","f367a071":"### Training Process","b55dad0e":"### Missing value"}}