{"cell_type":{"38023e3b":"code","ecc9f29b":"code","be81ef79":"code","8e30349c":"code","d4fa1c92":"code","aa594548":"code","caae5d5f":"code","afd0cf55":"code","b84983ab":"markdown","3421b8ba":"markdown"},"source":{"38023e3b":"!pip install --upgrade pip > \/dev\/null\n!pip install --upgrade transformers > \/dev\/null\n!pip install nlp > \/dev\/null","ecc9f29b":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.layers import Dense, Input, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\nfrom transformers import TFAutoModel, AutoTokenizer\nimport nlp\n\nimport plotly.express as px","be81ef79":"def init_strategy():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Init TPU strategy\")\n    except ValueError:\n        strategy = tf.distribute.get_strategy() # for CPU and single GPU\n        print(\"Init CPU\/GPU strategy\")\n    return strategy\n\ndef build_model(model_name, maxlen, head=\"avg_pooling\"):\n    # model encoding\n    input_ids = Input(shape=(maxlen,), dtype=tf.int32, name=\"input_ids\")\n    encoder = TFAutoModel.from_pretrained(model_name)\n    encoder_output = encoder(input_ids)[0]\n    \n    # convert transformer encodings to 1d-vector\n    if head == \"cls\":\n        features = encoder_output[:, 0, :] # using first token as encoder feature map\n    elif head == \"avg_pooling\":\n        features = GlobalAveragePooling1D()(encoder_output)\n    elif head == \"max_pooling\":\n        features = GlobalMaxPooling1D()(encoder_output)\n    else:\n        raise NotImplementedError\n    \n    # 3-class softmax\n    out = Dense(3, activation='softmax')(features)\n    \n    # define model\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=1e-5), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n    return model\n\ndef tokenize_dataframe(df, tokenizer, max_length):\n    # tokenize\n    text = df[['premise', 'hypothesis']].values.tolist()\n    encoded = tokenizer.batch_encode_plus(text, padding=True, max_length=max_length, truncation=True)\n    # features\n    x = encoded['input_ids']\n    # labels\n    y = None\n    if 'label' in df.columns:\n        y = df.label.values\n    return x, y\n\ndef load_mnli(use_validation=True):\n    result = []\n    dataset = nlp.load_dataset(path='glue', name='mnli')\n    keys = ['train', 'validation_matched','validation_mismatched'] if use_validation else ['train']\n    for k in keys:\n        for record in dataset[k]:\n            c1, c2, c3 = record['premise'], record['hypothesis'], record['label']\n            if c1 and c2 and c3 in {0,1,2}:\n                result.append((c1,c2,c3,'en'))\n    result = pd.DataFrame(result, columns=['premise','hypothesis','label','lang_abv'])\n    return result\n\ndef load_snli(use_validation=True):\n    result = []\n    dataset = nlp.load_dataset(path='snli')\n    keys = ['train', 'validation'] if use_validation else ['train']\n    for k in keys:\n        for record in dataset[k]:\n            c1, c2, c3 = record['premise'], record['hypothesis'], record['label']\n            if c1 and c2 and c3 in {0,1,2}:\n                result.append((c1,c2,c3,'en'))\n    result = pd.DataFrame(result, columns=['premise','hypothesis','label','lang_abv'])\n    return result\n\ndef load_xnli():\n    result = []\n    dataset = nlp.load_dataset(path='xnli')\n    for k in dataset.keys():\n        for record in dataset[k]:\n            hp, pr, lb = record['hypothesis'], record['premise'], record['label']\n            if hp and pr and lb in {0,1,2}:\n                for lang, translation in zip(hp['language'], hp['translation']):\n                    pr_lang = pr.get(lang, None)\n                    if pr_lang is None:\n                        continue\n                    result.append((pr_lang, translation, lb,lang))\n    result = pd.DataFrame(result, columns=['premise','hypothesis','label','lang_abv'])\n    return result\n","8e30349c":"MODEL = 'jplu\/tf-xlm-roberta-large'\nEPOCHS = 6\nMAXLEN = 120\nVALIDATION = \"mnli+xnli\"\n\nstrategy = init_strategy()\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nauto = tf.data.experimental.AUTOTUNE\n\ndef preprocess(df):\n    return tokenize_dataframe(df, tokenizer, MAXLEN)","d4fa1c92":"%%time \n\n# load data\ntrain = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/sample_submission.csv')\n\nmnli = load_mnli()\nxnli = load_xnli()\n\n# tokenize\nx, y = preprocess(train)\nx_test, _ = preprocess(test)\n\n# project dataset validation \nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=2020)\n\n# nli datasets\nx_mnli, y_mnli = preprocess(mnli)\nx_xnli, y_xnli = preprocess(xnli)\n\ndel mnli, xnli\ngc.collect()","aa594548":"%%time\n\n# datasets\ndef build_dataset(x, y, mode, batch_size):\n    if mode == \"train\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((x, y))\n            .repeat()\n            .shuffle(2048)\n            .batch(batch_size)\n            .prefetch(auto)\n        )\n    elif mode == \"valid\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((x, y))\n            .batch(BATCH_SIZE)\n            .cache()\n            .prefetch(auto)\n        )\n    elif mode == \"test\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices(x)\n            .batch(BATCH_SIZE)\n        )\n    else:\n        raise NotImplementedError\n    return dataset\n\ndataset = build_dataset(x, y, \"train\", BATCH_SIZE)\ntrain_dataset = build_dataset(x_train, y_train, \"train\", BATCH_SIZE)\nvalid_dataset = build_dataset(x_valid, y_valid, \"valid\", BATCH_SIZE)\ntest_dataset = build_dataset(x_test, None, \"test\", BATCH_SIZE)\n\n# merge XNLI & MNLI \nx_mnli += x_xnli\ndel x_xnli; gc.collect()\nnli_dataset = build_dataset(x_mnli, np.concatenate([y_mnli, y_xnli]), \"train\", BATCH_SIZE)","caae5d5f":"# fit parameters\nfit_params = dict(epochs=EPOCHS, verbose=2)\nvalidation = VALIDATION\n\n# create TPU context\n# it's significant to make tpu context every training to free up memory\nstrategy = init_strategy()\nwith strategy.scope():\n    model = build_model(MODEL, MAXLEN)\n\nif validation == \"dataset\":\n    steps_per_epoch = len(x_train) \/\/ BATCH_SIZE\n    history = model.fit(\n        train_dataset,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=valid_dataset,\n        **fit_params\n    )\nelif validation == \"mnli\":\n    steps_per_epoch = len(x_mnli) \/\/ BATCH_SIZE\n    history = model.fit(\n        mnli_dataset,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=valid_dataset,\n        **fit_params\n    )\nelif validation == \"xnli\":\n    steps_per_epoch = len(x_xnli) \/\/ BATCH_SIZE\n    history = model.fit(\n        xnli_dataset,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=valid_dataset,\n        **fit_params\n    )\nelif validation == \"mnli+xnli\":\n    steps_per_epoch = len(x_mnli) \/\/ BATCH_SIZE\n    history = model.fit(\n        nli_dataset,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=valid_dataset,\n        **fit_params\n    )\n","afd0cf55":"# save weights\nmodel.save_weights(f\"XLMR_{VALIDATION}_ep{EPOCHS}.h5\")\n\nhist = history.history\nprint(max(hist['val_accuracy']))\npx.line(\n    hist, x=range(1, len(hist['loss'])+1), y=['accuracy', 'val_accuracy'], \n    title='Model Accuracy', labels={'x': 'Epoch', 'value': 'Accuracy'}\n)","b84983ab":"References:\n\n[XLM-Roberta pretrained on NLI](https:\/\/www.kaggle.com\/qinhui1999\/more-nli-datasets-xmlr-large)\n\n[Tensorflow TPU starter](https:\/\/www.kaggle.com\/xhlulu\/contradictory-watson-concise-keras-xlm-r-on-tpu)\n\n[Sentence BERT paper](https:\/\/arxiv.org\/abs\/1908.10084])","3421b8ba":"Hello! \n\nIn this notebook, I made an attempt to compile the ideas of other contestants.\n\nThe main differences of my work are as follows:\n\n\n* Using full MNLI & XLNI datasets for XLM training\n* I skipped SNLI dataset because it showed poor validation results\n* A greater number of training periods without taking into account the competition limit of 120 minutes (6 epochs), so the model weights are dumped at the end\n* The training dataset from the competition is ignored and partially used only for validation\n* Average pooling is used to extract features from transformer. It's preferable instead of using CLS token, as recommended in many works, for example, in the `Sentence-BERT` paper\n* I tried to make the code as modular, clean, and self-commented as possible\n\nAlso, I have little experience in the use of tensorflow Datasets and tools and would appreciate comments about speed up and memory improvements."}}