{"cell_type":{"50404588":"code","284875ae":"code","c4d9f8b6":"code","f3500679":"code","6fadfafd":"code","47584ec6":"code","57c11a72":"code","89580acc":"code","8f4ad7b1":"code","a9e00c9e":"code","6fc78d3a":"code","612243c2":"code","75b15cc8":"code","9f4d31eb":"code","cef2e4b8":"code","0f672803":"code","ce53eb4f":"code","a6a22884":"code","1ad2b63d":"code","44d9e334":"code","21760251":"code","cd329d7c":"code","e7d8d9a3":"code","fa873b16":"code","4d78eb74":"code","bd077728":"code","a9be1e25":"code","414b0d16":"code","806e0708":"code","e75c859b":"code","896e41e1":"code","541a4939":"code","98a11bcb":"code","45dafeb0":"code","c3c02ef7":"code","5ce9273a":"code","584905f6":"code","f9455e69":"code","102a6e48":"code","83d7cbcb":"code","f991f9ef":"code","f4853dd3":"code","93f95fef":"code","0f5cbdbd":"code","fbb20d29":"code","635128e8":"code","0db44f2e":"code","5e67d539":"code","1c0332f5":"code","edff35b3":"code","ea3994ca":"code","43d03046":"code","bd173154":"code","944162b3":"code","5644684f":"code","8e410901":"code","de832458":"code","5eb9c888":"code","900ea449":"code","9eabf183":"code","6320fbf2":"code","b236f034":"code","df7ff45c":"code","9a10b6fb":"code","59d1d3f3":"code","fccfa61a":"code","3495b157":"code","75830027":"code","d18d221b":"code","add2bdc3":"code","6f440fc9":"code","b2d8de75":"code","3e9e140e":"code","256e0911":"code","8f0f8ec7":"code","56a9acfe":"code","323325bc":"code","2bb8d3c2":"code","cf481336":"code","dd553859":"code","eca36ece":"markdown","4f8a0a3f":"markdown","799deefb":"markdown","99656f54":"markdown","49f49a72":"markdown","14af27b5":"markdown","96cb8307":"markdown","61a256b5":"markdown","33d65a37":"markdown","c4354b5c":"markdown","1d3f0e9c":"markdown","7f78e9e1":"markdown","78770cea":"markdown","807b0e64":"markdown","3db982ec":"markdown","d8a629f3":"markdown","70cc39d5":"markdown","f9c06250":"markdown","f0550ba4":"markdown","7353a87b":"markdown","3366b87b":"markdown","09925138":"markdown","db740db6":"markdown","1c26db0e":"markdown","e010c7af":"markdown","20b1d0d5":"markdown","9a537ab4":"markdown","c70b313d":"markdown","610c455f":"markdown","372c1234":"markdown","20012ba1":"markdown","97dfcf01":"markdown","acdf578d":"markdown","7f6e6d87":"markdown","0a1c8bf9":"markdown","1c5cfabf":"markdown","cfc6f28c":"markdown","d1640863":"markdown","0392fba5":"markdown","0135bb81":"markdown","2ca6c307":"markdown","6fea286c":"markdown","486d71fa":"markdown"},"source":{"50404588":"# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","284875ae":"housing = pd.read_csv('\/kaggle\/input\/hands-on-machine-learning-housing-dataset\/housing.csv')","c4d9f8b6":"housing.head()","f3500679":"housing.sample(5) #get random sample","6fadfafd":"housing.info()","47584ec6":"housing[\"ocean_proximity\"].value_counts()","57c11a72":"housing.describe()","89580acc":"housing.hist(bins=50, figsize=(20,15));","8f4ad7b1":"# to make this notebook's output identical at every run\nnp.random.seed(42)","a9e00c9e":"# For illustration only. Sklearn has train_test_split()\ndef split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]","6fc78d3a":"train_set, test_set = split_train_test(housing, 0.2)\nlen(train_set)","612243c2":"len(test_set)","75b15cc8":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","9f4d31eb":"test_set.head()","cef2e4b8":"housing = train_set.copy()","0f672803":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\");","ce53eb4f":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1);","a6a22884":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n             s=housing[\"population\"]\/100, label=\"population\", figsize=(10,7),\n             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n             sharex=False)\nplt.legend();","1ad2b63d":"corr_matrix = housing.corr()","44d9e334":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)","21760251":"# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8));","cd329d7c":"plt.figure(figsize=(12, 8))\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nsns.pairplot(housing[attributes]);","e7d8d9a3":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1)\nplt.axis([0, 16, 0, 550000]);","fa873b16":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]","4d78eb74":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","bd077728":"housing = train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = train_set[\"median_house_value\"].copy()","a9be1e25":"housing.dropna(subset=[\"total_bedrooms\"])    # option 1","414b0d16":"housing.drop(\"total_bedrooms\", axis=1)       # option 2","806e0708":"median = housing[\"total_bedrooms\"].median()\nhousing[\"total_bedrooms\"].fillna(median, inplace=True) # option 3","e75c859b":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","896e41e1":"housing_num = housing.drop(\"ocean_proximity\", axis=1)\n# alternatively: housing_num = housing.select_dtypes(include=[np.number])","541a4939":"imputer.fit(housing_num)","98a11bcb":"imputer.statistics_","45dafeb0":"housing_num.median().values","c3c02ef7":"X = imputer.transform(housing_num)","5ce9273a":"housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing.index)","584905f6":"imputer.strategy","f9455e69":"housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing_num.index)","102a6e48":"housing_tr.head()","83d7cbcb":"housing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)","f991f9ef":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]","f4853dd3":"ordinal_encoder.categories_","93f95fef":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","0f5cbdbd":"housing_cat_1hot.toarray()","fbb20d29":"cat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","635128e8":"cat_encoder.categories_","0db44f2e":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","5e67d539":"housing_num_tr","1c0332f5":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","edff35b3":"housing_prepared","ea3994ca":"housing_prepared.shape","43d03046":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","bd173154":"# let's try the full preprocessing pipeline on a few training instances\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))","944162b3":"print(\"Labels:\", list(some_labels))","5644684f":"some_data_prepared","8e410901":"from sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","de832458":"from sklearn.metrics import mean_absolute_error\n\nlin_mae = mean_absolute_error(housing_labels, housing_predictions)\nlin_mae","5eb9c888":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(housing_prepared, housing_labels)","900ea449":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","9eabf183":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","6320fbf2":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","b236f034":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","df7ff45c":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","9a10b6fb":"housing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","59d1d3f3":"from sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","fccfa61a":"scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\npd.Series(np.sqrt(-scores)).describe()","3495b157":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = svm_reg.predict(housing_prepared)\nsvm_mse = mean_squared_error(housing_labels, housing_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse","75830027":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","d18d221b":"grid_search.best_params_","add2bdc3":"grid_search.best_estimator_","6f440fc9":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","b2d8de75":"pd.DataFrame(grid_search.cv_results_)","3e9e140e":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)","256e0911":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","8f0f8ec7":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","56a9acfe":"extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","323325bc":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","2bb8d3c2":"final_rmse","cf481336":"my_model = full_pipeline_with_predictor","dd553859":"import joblib\njoblib.dump(my_model, \"my_model.pkl\") # DIFF\n#...\nmy_model_loaded = joblib.load(\"my_model.pkl\") # DIFF","eca36ece":"Check that this is the same as manually computing the median of each attribute:","4f8a0a3f":"# Extra material","799deefb":"Now let's preprocess the categorical input feature, `ocean_proximity`:","99656f54":"Remove the text attribute because median can only be calculated on numerical attributes:","49f49a72":"## Get the Data","14af27b5":"The best hyperparameter combination found:","96cb8307":"The argument `sharex=False` fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https:\/\/github.com\/pandas-dev\/pandas\/issues\/10611 ). Thanks to Wilmer Arellano for pointing it out.","61a256b5":"First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn \u22650.20.","33d65a37":"Note that I hard coded the indices (3, 4, 5, 6) for concision and clarity in the book, but it would be much cleaner to get them dynamically, like this:","c4354b5c":"## Handling Text and Categorical Attributes","1d3f0e9c":"# End-to-end Machine Learning project\n\nNote: I am using [Aur\u00e9lien Geron's notebook](https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/02_end_to_end_machine_learning_project.ipynb). I just dropped some code and added some notes to use in teaching.\n\n**I highly recommend reading [his book](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646), especially the second chapter.**","7f78e9e1":"## Discover and Visualize the Data to Gain Insights","78770cea":"## Analyze the Best Models and Their Errors","807b0e64":"Let's look at the score of each hyperparameter combination tested during the grid search:","3db982ec":"Congratulations! You already know quite a lot about Machine Learning. :)","d8a629f3":"## Transformation Pipelines","70cc39d5":"In this notebook, we will go through an example project end to end.\n \nHere are the main steps you will go through:\n<ol>\n<li>Look at the big picture.<\/li>\n    <ul>\n    <li>Frame the Problem<\/li>\n    <li>Select a Performance Measure<\/li>\n    <\/ul>  \n        \n<li>Get the data.<\/li>\n    <ul>\n    <li>Take a Quick Look at the Data<\/li>\n    <li>Create a Test Set<\/li>\n    <\/ul>  \n<li>Discover and visualize the data<\/li>\n    <ul>\n    <li>Visualizing Geographical Data<\/li>\n    <li>Looking for Correlations<\/li>\n    <li>Experimenting with Attribute Combinations<\/li>\n    <\/ul>  \n<li>Prepare the data for ML algorithms.<\/li>\n    <ul>\n    <li>Data Cleaning<\/li>\n    <li>Handling Text and Categorical Attributes<\/li>\n    <li>Transformation Pipelines<\/li>\n    <\/ul>  \n<li>Select a model and train it<\/li>\n<li>Fine-tune your model.<\/li>\n<li>Present your solution.<\/li>\n<li>Launch, monitor, and maintain your system.<\/li>\n<\/ol>","f9c06250":"Transform the training set:","f0550ba4":"# Select and Train a Model","7353a87b":"## Training and Evaluating on the Training Set","3366b87b":"### Take a Quick Look at the Data Structure","09925138":"## Better Evaluation Using Cross-Validation","db740db6":"## Grid Search","1c26db0e":"### Create a Test Set","e010c7af":"Compare against the actual values:","20b1d0d5":"Now let's join all these components into a big pipeline that will preprocess both the numerical and the categorical features:","9a537ab4":"Alternatively, you can set `sparse=False` when creating the `OneHotEncoder`:","c70b313d":"By default, the `OneHotEncoder` class returns a sparse array, but we can convert it to a dense array if needed by calling the `toarray()` method:","610c455f":"### Visualizing Geographical Data","372c1234":"## Evaluate Your System on the Test Set","20012ba1":"# Prepare the Data for Machine Learning Algorithms","97dfcf01":"# Fine-Tune Your Model","acdf578d":"## Randomized Search","7f6e6d87":"## Data Cleaning","0a1c8bf9":"In the book 3 options are listed:\n\n```python\nhousing.dropna(subset=[\"total_bedrooms\"])    # option 1\nhousing.drop(\"total_bedrooms\", axis=1)       # option 2\nmedian = housing[\"total_bedrooms\"].median()  # option 3\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)\n```\n\nTo demonstrate each of them, let's create a copy of the housing dataset, but keeping only the rows that contain at least one null. Then it will be easier to visualize exactly what each option does:","1c5cfabf":"## Looking for Correlations","cfc6f28c":"<a id='intro'><\/a>\n## Introduction","d1640863":"## Model persistence using joblib","0392fba5":"**Note**: we specify `n_estimators=100` to be future-proof since the default value is going to change to 100 in Scikit-Learn 0.22 (for simplicity, this is not shown in the book).","0135bb81":"## Experimenting with Attribute Combinations","2ca6c307":"Now let's build a pipeline for preprocessing the numerical attributes:","6fea286c":"Same plot with seaborn","486d71fa":"**Note**: since Scikit-Learn 0.22, you can get the RMSE directly by calling the `mean_squared_error()` function with `squared=False`."}}