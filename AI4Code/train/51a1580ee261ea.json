{"cell_type":{"7c6d5bfc":"code","ff3f7d7e":"code","6d818094":"code","83e50c06":"code","3026847a":"code","5768a425":"code","afba466a":"code","57cd97eb":"code","66bcd0b3":"code","8d658879":"code","42e23503":"code","0662d0a2":"code","510158ae":"code","91eeae94":"code","5466ee89":"code","51c8ceb5":"code","4b74ece4":"code","338937bc":"code","fe12d927":"code","0bdd7116":"code","b2dbc4e3":"code","bc0d1373":"code","9a2ab93e":"code","1a764ff0":"code","94e888c0":"code","a86f857c":"code","3ac3865b":"code","dc8626d6":"code","ccd2b8df":"code","3826f654":"code","61527060":"code","65868b3f":"code","7f4cbdd1":"code","fc19885a":"code","a43617d4":"code","e337d1c3":"code","c0fc9ef2":"code","cee53ced":"code","3c40780a":"code","b96e3417":"code","f0562ea6":"code","8ed5fdca":"code","a022a8c8":"code","9db3fae1":"code","03e4f2f4":"code","eb5f46b4":"code","8d7ff607":"code","9d8c5aa0":"code","160f5a1b":"code","0eb94c8a":"code","07bd077b":"markdown","3f06b197":"markdown","eeda2b2a":"markdown","de54a587":"markdown","d45b96d3":"markdown","9753de70":"markdown","3607f60e":"markdown","46ef4b79":"markdown","6a6c93d7":"markdown","58fd98bc":"markdown","668be9ba":"markdown","6292b18f":"markdown","df03fa32":"markdown","d417ceda":"markdown","21aa86b6":"markdown","624a2f2c":"markdown","d80155fb":"markdown","9cc4a542":"markdown","24b96ab2":"markdown","1ef64ee2":"markdown","bb298536":"markdown","2b0fa75f":"markdown","282038f0":"markdown","88ece46f":"markdown","6e270f07":"markdown","ea362226":"markdown","fd38a772":"markdown","cf7b90e9":"markdown","2410dc04":"markdown"},"source":{"7c6d5bfc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff3f7d7e":"import seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib.pyplot as plt\n\nimport cufflinks as cf\nimport plotly.offline as py\ncolor = sns.color_palette()\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\nimport plotly.tools as tls\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","6d818094":"from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier","83e50c06":"df = pd.read_csv('..\/input\/abalone-dataset\/abalone.csv')","3026847a":"df.head(3)","5768a425":"df.shape","afba466a":"df['age'] = df['Rings']+1.5\ndf.drop('Rings', axis = 1, inplace = True)","57cd97eb":"df.info()","66bcd0b3":"df.describe()","8d658879":"df.isnull().sum()","42e23503":"df.var()","0662d0a2":"df_sex = df['Sex'].value_counts()\nprint(df_sex.head())\ntrace = go.Bar(x = df_sex.index[: : -1] ,y = df_sex.values[: : -1], marker = dict(color = 'lightseagreen'))\ndata = [trace]\nlayout = go.Layout(height = 400, width = 500, title='Sex Distribution')\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","510158ae":"fig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, ax8)) = plt.subplots(nrows= 2, ncols = 4, figsize = (24,10))\nsns.boxplot(ax = ax1, y = 'Length', data = df, color = 'green')\nsns.boxplot(ax = ax2, y = 'Diameter', data = df, color = 'red')\nsns.boxplot(ax = ax3, y = 'Height', data = df, color = 'limegreen')\nsns.boxplot(ax = ax4, y = 'Whole weight', data = df, color = 'cyan')\nsns.boxplot(ax = ax5, y = 'Shucked weight', data = df, color = 'salmon')\nsns.boxplot(ax = ax6, y = 'Viscera weight', data = df, color = 'mediumorchid')\nsns.boxplot(ax = ax7, y = 'Shell weight', data = df, color = 'lime')\nsns.boxplot(ax = ax8, y = 'age', data = df, color = 'plum')","91eeae94":"df1 = df.copy()\ndf2 = df.copy()\ndf3 = df.copy()\n\ndf_m = df1[df1['Sex'] == 'M']\ndf_m.drop('Sex', axis = 1, inplace= True)\ndf_f = df2[df2['Sex'] == 'F']\ndf_f.drop('Sex', axis = 1, inplace= True)\ndf_i = df3[df3['Sex'] == 'I']\ndf_i.drop('Sex', axis = 1, inplace= True)\ndf_m.drop(['age'], axis=1, inplace = True)\ndf_f.drop(['age'], axis=1, inplace = True)\ndf_i.drop(['age'], axis=1, inplace = True)\n\ndf_m = df_m.mean()\ndf_f = df_f.mean()\ndf_i = df_i.mean()\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'M', marker = dict(color = 'cyan'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'F', marker = dict(color = 'violet'))\ntrace3 = go.Bar(x = df_i.index[::-1], y = df_i.values[::-1], name = 'I', marker = dict(color = 'lightsteelblue'))\ndata = [trace1, trace2, trace3]\nlayout = go.Layout(title = 'Feature Distribution', width = 800)\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","5466ee89":"df4 = df.copy()\ndf5 = df.copy()\ndf6 = df.copy()\ndf_m1 = df4[df4['Sex'] == 'M']\ndf_m1.drop('Sex', axis = 1, inplace= True)\ndf_f1 = df5[df5['Sex'] == 'F']\ndf_f1.drop('Sex', axis = 1, inplace= True)\ndf_i1 = df6[df6['Sex'] == 'I']\ndf_i1.drop('Sex', axis = 1, inplace= True)\ndf_m1.drop(['Length','Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight'], axis=1, inplace = True)\ndf_f1.drop(['Length','Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight'], axis=1, inplace = True)\ndf_i1.drop(['Length','Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight'], axis=1, inplace = True)\n\ndf_m1 = df_m1.mean()\ndf_f1 = df_f1.mean()\ndf_i1 = df_i1.mean()\ntrace1 = go.Bar(x = df_m1.index[::-1], y = df_m1.values[::-1], name = 'M', marker = dict(color = 'limegreen'))\ntrace2 = go.Bar(x = df_f1.index[::-1], y = df_f1.values[::-1], name = 'F', marker = dict(color = 'olive'))\ntrace3 = go.Bar(x = df_i1.index[::-1], y = df_i1.values[::-1], name = 'I', marker = dict(color = 'seagreen'))\ndata = [trace1, trace2, trace3]\nlayout = go.Layout(title = 'Feature Distribution', width = 750)\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","51c8ceb5":"sns.pairplot(data = df, hue = 'Sex', palette = 'Dark2')","4b74ece4":"sns.set_style('darkgrid')\nsns.lmplot(x = 'Length', y = 'age', data = df, hue = 'Sex', palette = 'Set1', scatter_kws={'edgecolor':'white', 'alpha':0.7, 'linewidth':0.5})","338937bc":"df.drop(df[(df['Length']<0.1) & (df['age'] < 5)].index, inplace=True)\ndf.drop(df[(df['Length']<0.8) & (df['age'] > 25)].index, inplace=True)\ndf.drop(df[(df['Length']>=0.8) & (df['age']< 25)].index, inplace=True)","fe12d927":"sns.lmplot(x = 'Diameter', y = 'age', data = df, hue = 'Sex', palette = 'Dark2', scatter_kws={'edgecolor':'white', 'alpha':0.7, 'linewidth':0.5})","0bdd7116":"df.drop(df[(df['Diameter']<0.1) & (df['age'] < 5)].index, inplace=True)\ndf.drop(df[(df['Diameter']<0.6) & (df['age'] > 25)].index, inplace=True)\ndf.drop(df[(df['Diameter']>=0.6) & (df['age']< 25)].index, inplace=True)","b2dbc4e3":"sns.lmplot(x = 'Height', y = 'age', data = df, hue = 'Sex', palette = 'viridis', scatter_kws={'edgecolor':'white', 'alpha':0.7, 'linewidth':0.5})","bc0d1373":"df.drop(df[(df['Height']>0.4) & (df['age'] < 15)].index, inplace=True)\ndf.drop(df[(df['Height']<0.4) & (df['age'] > 25)].index, inplace=True)","9a2ab93e":"sns.lmplot(x = 'Whole weight', y = 'age', data = df, hue = 'Sex', palette = 'magma', scatter_kws={'edgecolor':'white', 'alpha':0.7, 'linewidth':0.5})","1a764ff0":"df.drop(df[(df['Whole weight']>= 2.5) & (df['age'] < 25)].index, inplace=True)\ndf.drop(df[(df['Whole weight']<2.5) & (df['age'] > 25)].index, inplace=True)","94e888c0":"sns.lmplot(x = 'Shucked weight', y = 'age', data = df, hue = 'Sex', palette = 'gist_heat', scatter_kws={'edgecolor':'white', 'alpha':0.7, 'linewidth':0.5})","a86f857c":"df.drop(df[(df['Shucked weight']>= 1) & (df['age'] < 20)].index, inplace=True)\ndf.drop(df[(df['Shucked weight']<1) & (df['age'] > 20)].index, inplace=True)","3ac3865b":"sns.lmplot(x = 'Viscera weight', y = 'age', data = df, hue = 'Sex', palette = 'gnuplot', scatter_kws={'edgecolor':'white', 'alpha':0.7, 'linewidth':0.5})","dc8626d6":"df.drop(df[(df['Viscera weight']> 0.5) & (df['age'] < 20)].index, inplace=True)\ndf.drop(df[(df['Viscera weight']<0.5) & (df['age'] > 25)].index, inplace=True)","ccd2b8df":"sns.lmplot(x = 'Shell weight', y = 'age', data = df, hue = 'Sex', palette = 'twilight_r', scatter_kws={'edgecolor':'white', 'alpha':0.7, 'linewidth':0.5})","3826f654":"df.drop(df[(df['Shell weight']> 0.6) & (df['age'] < 25)].index, inplace=True)\ndf.drop(df[(df['Shell weight']<0.8) & (df['age'] > 25)].index, inplace=True)","61527060":"plt.figure(figsize = (8,6))\ncorr = df.corr()\nsns.heatmap(corr, annot = True)","65868b3f":"df.drop('Sex', axis = 1, inplace = True)\ndf.head()","7f4cbdd1":"df['age'].value_counts()","fc19885a":"df['age'].mean()","a43617d4":"df_1 = df.copy()","e337d1c3":"Age = []\nfor i in df_1['age']:\n    if i > 11.12:\n        Age.append('1')\n    else:\n        Age.append('0')\ndf_1['Age'] = Age\ndf_1.drop('age', axis = 1, inplace = True)\ndf_1.head()","c0fc9ef2":"df_1['Age'].value_counts()","cee53ced":"X = df_1.drop('Age', axis = 1).values\ny = df_1['Age'].values","3c40780a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)","b96e3417":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f0562ea6":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_test, y_pred))\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))\nlr_train_acc = lr.score(X_train, y_train)\nprint('Training Score: ', lr_train_acc)\nlr_test_acc = lr.score(X_test, y_test)\nprint('Testing Score: ', lr_test_acc)","8ed5fdca":"svc = SVC(C = 1, gamma= 1)\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_test, y_pred))\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))\nsvc_train_acc = svc.score(X_train, y_train) \nprint('Training Score: ', svc_train_acc)\nsvc_test_acc = svc.score(X_test, y_test)\nprint('Testing Score: ', svc_test_acc)","a022a8c8":"error_rate = []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors= i)\n    knn.fit(X_train, y_train)\n    y_predi = knn.predict(X_test)\n    error_rate.append(np.mean(y_test != y_predi))\n    \nplt.figure(figsize = (10,8))\nplt.plot(range(1,40), error_rate, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)","9db3fae1":"knn = KNeighborsClassifier(n_neighbors= 31)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_test, y_pred))\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))\nknn_train_acc = knn.score(X_train, y_train) \nprint('Training Score: ', knn_train_acc)\nknn_test_acc = knn.score(X_test, y_test)\nprint('Testing Score: ', knn_test_acc)","03e4f2f4":"dt = DecisionTreeClassifier(max_depth = 5)\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_test, y_pred))\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))\ndt_train_acc = dt.score(X_train, y_train) \nprint('Training Score: ', dt_train_acc)\ndt_test_acc = dt.score(X_test, y_test)\nprint('Testing Score: ', dt_test_acc)","eb5f46b4":"rf = RandomForestClassifier(n_estimators= 150, max_depth= 5)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_test, y_pred))\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))\nrf_train_acc = rf.score(X_train, y_train) \nprint('Training Score: ', rf_train_acc)\nrf_test_acc = rf.score(X_test, y_test)\nprint('Testing Score: ', rf_test_acc)","8d7ff607":"adb = AdaBoostClassifier(n_estimators= 100)\nadb.fit(X_train, y_train)\ny_pred = adb.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_test, y_pred))\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))\nadb_train_acc = adb.score(X_train, y_train) \nprint('Training Score: ', adb_train_acc)\nadb_test_acc = adb.score(X_test, y_test)\nprint('Testing Score: ', adb_test_acc)","9d8c5aa0":"gdb = GradientBoostingClassifier(n_estimators= 200, max_depth = 2, min_samples_leaf= 2)\ngdb.fit(X_train, y_train)\ny_pred = gdb.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_test, y_pred))\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))\ngdb_train_acc = gdb.score(X_train, y_train) \nprint('Training Score: ', gdb_train_acc)\ngdb_test_acc = gdb.score(X_test, y_test)\nprint('Testing Score: ', gdb_test_acc)","160f5a1b":"xgb = XGBClassifier(objective = \"binary:logistic\", n_estimators = 100, max_depth = 3, subsample = 0.8, colsample_bytree = 0.6, learning_rate = 0.1)\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_test, y_pred))\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))\nxgb_train_acc = xgb.score(X_train, y_train) \nprint('Training Score: ', xgb_train_acc)\nxgb_test_acc = xgb.score(X_test, y_test)\nprint('Testing Score: ', xgb_test_acc)","0eb94c8a":"x = ['Logistic Regression','SVC', 'KNN', 'Decision Tree','Random Forest','AdaBoost','Gradient Boosting','XGBoost']\ny1 = [lr_train_acc, svc_train_acc, knn_train_acc, dt_train_acc, rf_train_acc, adb_train_acc, gdb_train_acc, xgb_train_acc]\ny2 = [lr_test_acc, svc_test_acc, knn_test_acc, dt_test_acc, rf_test_acc, adb_test_acc, gdb_test_acc, xgb_test_acc]\n\ntrace1 = go.Bar(x = x, y = y1, name = 'Training Accuracy', marker = dict(color = 'cyan'))\ntrace2 = go.Bar(x = x, y = y2, name = 'Testing Accuracy', marker = dict(color = 'violet'))\ndata = [trace1,trace2]\nlayout = go.Layout(title = 'Accuracy Plot', width = 750)\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","07bd077b":"# Data Visualization","3f06b197":"Feature Distribution using plotly","eeda2b2a":"If you like it then please upvote and any suggetions are welcome.","de54a587":"In this notebook we'll be performing analysis on **ABALONE** dataset. Abalones are marine snails.\nAbalones can be found along coasts of almost every continent. Usually, abalones are consumed as food all around the world, by different cultures.","d45b96d3":"From problem statement and feature discription, let's first compute the target varible of the problem ' Age' and assign it to the dataset. Age = 1.5+Rings","9753de70":"Now we will use seaborn lmplot to look for outliers and removing them.","3607f60e":"Random Forest Classifier","46ef4b79":"# Feature Engineering","6a6c93d7":"Anticipating the age of abalone from physical estimations. The age of abalone is dictated by carving the shell through the cone, recoloring it, and tallying the number of rings through a magnifying instrument - an exhausting and tedious task. Different estimations, which are simpler to acquire, are utilized to anticipate the age. Additional data, for example, climate examples and area (thus food accessibility) might be required to take care of the issue.","58fd98bc":"Now looking for correlation in data using seaborn heatmap.","668be9ba":"# Plotting Accuracy graph using plotly","6292b18f":"# Machine Learning Methods Used:","df03fa32":"We will drop categorical column(Sex).","d417ceda":"Boxplots to look for outliers","21aa86b6":"1. Logistic Regression\n2. Support Vector Machines\n3. K Nearest Neighbors\n4. Decision Tree\n5. Ensemble Methods (Random Forest, AdaBoost, Gradient Boost)\n6. XGBoost","624a2f2c":"Distribution of Sex","d80155fb":"Train Test Split","9cc4a542":"AdaBoost Classifier","24b96ab2":"Logistic Regression","1ef64ee2":"Decision Tree Classifier","bb298536":"# Dataset Description","2b0fa75f":"Preprocessing","282038f0":"XGBoost Classifier","88ece46f":"We will choose number of neighbors.","6e270f07":"Support Vector Classifiers","ea362226":"K Nearest Neighbour Classifier","fd38a772":"# Importing Libraries and Reading Data","cf7b90e9":"# Time to train model","2410dc04":"Gradient Boosting "}}