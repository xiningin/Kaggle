{"cell_type":{"0d088a63":"code","a8a9801c":"code","5c380a83":"code","ec5fb3ce":"code","56e169b0":"code","741caf24":"code","f020d23e":"code","62bf5c8b":"code","a04c1471":"code","5e07b439":"code","1ebd84de":"code","c86eb7e0":"code","9ba0aaea":"code","ffe7ebec":"code","e2434690":"code","f7e0456c":"code","8d868d62":"code","08f32791":"code","14859ec4":"code","75aa1225":"code","db85a0af":"markdown","c20d04f0":"markdown","d1fc71e3":"markdown","82b7597f":"markdown","f9a70e33":"markdown","4ec4427e":"markdown","310353e5":"markdown","f53e9ebd":"markdown"},"source":{"0d088a63":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport cv2\nfrom IPython.display import Image","a8a9801c":"def show(img):\n    return (Image(cv2.imencode(\".png\",img)[1].tobytes()))","5c380a83":"base_path = '\/kaggle\/input\/intel-image-classification\/'\nfolders = os.listdir(base_path)\nprint(folders)","ec5fb3ce":"base_path+folders[0]+'\/'","56e169b0":"train_folders = os.listdir(os.path.join(base_path,folders[0],folders[0]))\nprint(f'train_folders: {train_folders}')\nprint('----Train Data Distribution-----')\nprint(f'Folder Name : No. of Images')\nfor folder in train_folders:\n    print(f'{folder:11} : {len(os.listdir(os.path.join(base_path,folders[0],folders[0],folder)))}')","741caf24":"val_folders = os.listdir(os.path.join(base_path,folders[2],folders[2]))\nprint(f'val_folders: {val_folders}')\nprint('----Val Data Distribution-----')\nprint(f'Folder Name : No. of Images')\nfor folder in val_folders:\n    print(f'{folder:11} : {len(os.listdir(os.path.join(base_path,folders[2],folders[2],folder)))}')","f020d23e":"test_images = os.listdir(os.path.join(base_path,folders[1],folders[1]))\nprint(f'test_images: {len(test_images)}')\nprint('First ten Images')\nprint(test_images[:10])","62bf5c8b":"!pip install -U efficientnet","a04c1471":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n# from keras.applications.inception_v3 import InceptionV3\n# from tensorflow.keras.applications import EfficientNetB2\nimport efficientnet.keras as efn \n# model = efn.EfficientNetB0(weights='imagenet') \nfrom keras import Model, layers\nfrom keras.layers import GlobalAveragePooling2D, Dropout, Dense, Input\n\nprint(\"Libraries Imported!\")","5e07b439":"train_DIR = \"\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train\/\"\n\ntrain_datagen = ImageDataGenerator( rescale = 1.0\/255,\n                                          width_shift_range=0.2,\n                                          height_shift_range=0.2,\n                                          zoom_range=0.2,\n                                          vertical_flip=True,\n                                          fill_mode='nearest')\n\n\ntrain_generator = train_datagen.flow_from_directory(train_DIR,\n                                                    batch_size=32,\n                                                    class_mode='categorical',\n                                                    target_size=(250, 250))\n\ntest_DIR = \"\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test\/\"\nvalidation_datagen = ImageDataGenerator(rescale = 1.0\/255)\n\n\nvalidation_generator = validation_datagen.flow_from_directory(test_DIR,\n                                                    batch_size=128,\n                                                    class_mode='categorical',\n                                                    target_size=(250, 250))","1ebd84de":"print(validation_generator.class_indices)\nclass2index = validation_generator.class_indices\n\nindex2class = {v: k for k, v in class2index.items()}\nprint(index2class)","c86eb7e0":"efficientNet = efn.EfficientNetB1(weights='imagenet')\n\n# for layer in efficientNet.layers:\n#     layer.trainable = False","9ba0aaea":"\nlast_output = efficientNet.layers[-1].output","ffe7ebec":"# x = tf.keras.layers.Flatten()(last_output)\nx = tf.keras.layers.Dense(units = 128, activation = tf.nn.relu)(last_output)\n# x = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense  (6, activation = tf.nn.softmax)(x)\n\nmodel = tf.keras.Model( efficientNet.input, x)\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n                                            patience=1,\n                                            verbose=1,\n                                            factor=0.25,\n                                            min_lr=0.000003)\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer= tf.keras.optimizers.Adam(), metrics=['acc'])\n\n# model.summary()","e2434690":"history = model.fit(train_generator,\n                    epochs = 15,\n                    verbose = 1,\n                   validation_data = validation_generator,\n                   callbacks=[learning_rate_reduction])","f7e0456c":"%matplotlib inline\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","8d868d62":"print(validation_generator.class_indices)","08f32791":"path = os.path.join(base_path,folders[1],folders[1],test_images[0])\nimg = cv2.resize(cv2.imread(path),(250,250))\nimg.shape\n","14859ec4":"show(img)\n","75aa1225":"image_prediction = np.argmax(model.predict(np.array([img])))\nprint(index2class[image_prediction])","db85a0af":"# About EfficientNet","c20d04f0":"# Train","d1fc71e3":"# Visualize Model Perfomance","82b7597f":"**EfficientNet** is a lightweight convolutional neural network architecture achieving the state-of-the-art accuracy with an order of magnitude fewer parameters and FLOPS, on both ImageNet and five other commonly used transfer learning datasets.\n\n**EfficientNets** rely on AutoML and compound scaling to achieve superior performance without compromising resource efficiency. The AutoML Mobile framework has helped develop a mobile-size baseline network, EfficientNet-B0, which is then improved by the compound scaling method to obtain EfficientNet-B1 to B7.\n\n<img src='https:\/\/raw.githubusercontent.com\/tensorflow\/tpu\/master\/models\/official\/efficientnet\/g3doc\/flops.png' width=\"500\" height=\"600\">\n\nEfficientNets achieve state-of-the-art accuracy on ImageNet with an order of magnitude better efficiency:\n\n* In high-accuracy regime, EfficientNet-B7 achieves the state-of-the-art 84.4% top-1 \/ 97.1% top-5 accuracy on ImageNet with 66M parameters and 37B FLOPS. At the same time, the model is 8.4x smaller and 6.1x faster on CPU inference than the former leader, Gpipe.\n\n* In middle-accuracy regime, EfficientNet-B1 is 7.6x smaller and 5.7x faster on CPU inference than ResNet-152, with similar ImageNet accuracy.\n\n* Compared to the widely used ResNet-50, EfficientNet-B4 improves the top-1 accuracy from 76.3% of ResNet-50 to 82.6% (+6.3%), under similar FLOPS constraints.","f9a70e33":"# EDA","4ec4427e":"# Read Data","310353e5":"# Get Model","f53e9ebd":"# Import Libraries"}}