{"cell_type":{"edceb270":"code","28d7280c":"code","909fc8b0":"code","27c61a2e":"code","ff6d9811":"code","1c6b2d37":"code","629c9a10":"code","f4197a84":"code","a15e75d2":"code","333242bc":"code","b1a161e4":"markdown"},"source":{"edceb270":"%%time\nimport sys\n!cp -f ..\/input\/rapids\/rapids.21.06 \/opt\/conda\/envs\/rapids.tar.gz\n!cd -f \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\n!cp -f \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","28d7280c":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport os\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Regressors\nimport lightgbm as lgb\n\n# Model selection\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\n\n# Data processing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# \nimport cudf, cuml\nimport cupy as cp\nfrom cuml.manifold import TSNE, UMAP\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import ylim, xlim\n%matplotlib inline","909fc8b0":"# Loading data \nX = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")","27c61a2e":"# Preparing data as a tabular matrix\ny = X.claim\nX = X.set_index('id').drop('claim', axis='columns')\nX_test = X_test.set_index('id')","ff6d9811":"# Dealing with missing values\nmeans = X.mean()\nX = X.fillna(means)\nX_test = X_test.fillna(means)","1c6b2d37":"tsne = TSNE(n_components=2, perplexity=10, n_neighbors=100)\nprojection_2D = tsne.fit_transform(X)","629c9a10":"plt.figure(figsize=(15, 15))\nplt.scatter(projection_2D[:,0], projection_2D[:,1],\n            c=y.values, \n            edgecolor='none', \n            alpha=0.80, \n            s=10)\nplt.axis('off')\nplt.show();","f4197a84":"umap = UMAP(n_components=2, n_neighbors=100)\nprojection_2D = umap.fit_transform(X)","a15e75d2":"valid_0 = (projection_2D[:,0] < 20) & (projection_2D[:,0] >-20)\nvalid_1 = (projection_2D[:,1] < 20) & (projection_2D[:,1] >-20)\nvalid = valid_0 & valid_1","333242bc":"plt.figure(figsize=(15, 15))\nplt.scatter(projection_2D[valid, 0], projection_2D[valid, 1],\n            c=y.values[valid], \n            edgecolor='none', \n            alpha=0.80, \n            s=10)\nplt.axis('off')\nplt.show();","b1a161e4":"# T-SNE & UMAP\nT-SNE (https:\/\/lvdmaaten.github.io\/tsne\/) and UMAP (https:\/\/github.com\/lmcinnes\/umap) are two technicalities, often used by data scientists, that allow to project multivariate data into lower dimensions. They are often used to find clusters in data. I used the fast t-SNE and UMAP implementations offered by Rapids (they require GPU access). \n\nAs stated by the article \"How to t-SNE Effectively\" (https:\/\/distill.pub\/2016\/misread-tsne\/), it is easy to see clusters where there are not, and in our data there are no clear macro clusters, probably. What can be inferedded by t-SNE and UMAP projects, though, is that there are many local clusters, just like in mixture of (this is what can be read from t-SNE). Also there are probably a few outliers, given the noise injected into the artificial data (and this can be read from UMAP, where a few central clusters are surronded by very small scattered ones).\n\nI wonder if, given such evidence, gaussian mixtures may prove a good feature engineering approach."}}