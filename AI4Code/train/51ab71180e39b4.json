{"cell_type":{"265cf487":"code","344b22da":"code","af6e4d81":"code","e5f524f1":"code","4d8595bc":"code","c9493a7d":"code","d194c734":"code","2fa1d6bb":"code","96babcdd":"code","0d007a19":"code","e61013fe":"code","bb0d83e7":"code","57c43b79":"code","b54defdf":"code","c4068e0f":"code","72347378":"code","5e95a9ac":"code","e879b42a":"code","03d3e448":"code","08b16109":"code","6dd2e7ae":"code","8523878c":"code","498c545b":"code","26576262":"code","60d79e2e":"code","e1162546":"code","8bf3ec33":"code","715f98a3":"code","fe6edb9d":"code","1100b5a8":"code","f389bb47":"code","a5985cd0":"code","f13586ab":"code","110c2a10":"code","10f90b23":"code","0283bdf7":"code","11ac0b34":"code","98bacfc4":"code","59ee0795":"code","6319e1be":"code","a5be7f97":"code","0e154cb3":"code","aaf00445":"code","e3d47dfc":"code","ac70ad59":"code","7c3ab20f":"code","0527ba0f":"code","8c99018c":"code","90598e14":"code","7b55421c":"code","b63f145f":"code","4b597ab2":"code","f66ca6c1":"code","7933dedd":"code","6e228d70":"code","c9c1cbdc":"code","2078f99c":"code","b8301baf":"code","d149805f":"code","a8eb5613":"code","f7366c04":"code","fe86f8ed":"code","52db4d65":"code","840f8b86":"code","20428451":"code","9938cdf4":"code","dc28dffe":"code","58ad084a":"code","cbdafcb3":"code","be60ee31":"code","ff36e855":"code","e6c9b3a0":"code","82644120":"code","3b2bcfe4":"code","b6ad26d8":"code","5500bfeb":"code","da8df0a2":"code","409dcb3d":"code","d4689a96":"code","cd4d4e0e":"markdown","e12d26a1":"markdown","62e7a6c5":"markdown","2c55943d":"markdown","80f8a892":"markdown","b180f34c":"markdown","0d56676b":"markdown","c3e59c0f":"markdown","64591cc3":"markdown","c0f82ee1":"markdown","a8c96957":"markdown","3fcd80cb":"markdown","14381b1b":"markdown","39b8e36e":"markdown","de706903":"markdown","0b8c8600":"markdown","23ae7e53":"markdown","907bb808":"markdown","410c44d2":"markdown","b57b9aee":"markdown","f7ce4679":"markdown","4d8a8c36":"markdown","3f7e480b":"markdown","53531938":"markdown","140a8f8d":"markdown","e7206445":"markdown","1ec06323":"markdown","017db58a":"markdown","ed207f8b":"markdown","c2f703bb":"markdown","347d5f36":"markdown","aad4ec93":"markdown","1ded2aeb":"markdown","a0b25961":"markdown","dbab55f8":"markdown","01bbbfb0":"markdown","30c868fc":"markdown","ad1e694a":"markdown","94bd7220":"markdown","fdad91b3":"markdown","cb49e89b":"markdown","c07ea20e":"markdown","4212c550":"markdown","80f00906":"markdown","05c410c7":"markdown"},"source":{"265cf487":"#importing requests to get url content from online storage\n# import requests\n#importing os to download the content on our physical storage\nimport os\n#importing pandas to store data in dataframes\nimport pandas as pd\n# import tweepy\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","344b22da":"# #storing source urls\n# twitter_achived_basic_source_url = \"https:\/\/d17h27t6h515a5.cloudfront.net\/topher\/2017\/August\/59a4e958_twitter-archive-enhanced\/twitter-archive-enhanced.csv\"\n# twitter_dogs_bread_source_url = \"https:\/\/d17h27t6h515a5.cloudfront.net\/topher\/2017\/August\/599fd2ad_image-predictions\/image-predictions.tsv\"\n# source_urls = [{\n#             \"url\" : twitter_achived_basic_source_url,\n#             \"file_name\" : \"twitter_archive_enhanced.csv\"\n#         },{\n#             \"url\": twitter_dogs_bread_source_url,\n#             \"file_name\":\"image_predictions.tsv\"\n#         }]\n# #loop and download files in root folder with the file_names\n# for source in source_urls:\n#     response = requests.get(source[\"url\"])\n#     with open(os.path.join(\"\", source[\"file_name\"]), mode='wb') as file:\n#         file.write(response.content)\n","af6e4d81":"tweets_df = pd.read_csv('..\/input\/tweet-csv\/twitter_archive_enhanced.csv', dtype={\"tweet_id\": str})\ntweets_df.head()","e5f524f1":"#importing tsv file as csv with tab delimeter\ndog_bread_df = pd.read_csv('..\/input\/tweet-csv\/image_predictions.tsv', sep='\\t', dtype={\"tweet_id\": str})\ndog_bread_df.head()","4d8595bc":"# consumer_key = '*****'\n# consumer_secret = '****'\n# access_token = '****'\n# access_secret = '****'\n\n# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n# auth.set_access_token(access_token, access_secret)\n\n# api = tweepy.API(auth)","c9493a7d":"# #Testing the api by using one tweet id.\n# tweet = api.get_status('666447344410484738',tweet_mode='extended')\n# tweet._json","d194c734":"# #just to test how to access the parameters i need\n# tweet.retweet_count, tweet.favorite_count","2fa1d6bb":"unique_tweet_ids = np.union1d(tweets_df.tweet_id.unique(), dog_bread_df.tweet_id.unique())\nunique_tweet_ids.size","96babcdd":"# api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify =True)\n# failed_process_tweets = {}\n# with open('tweet_json.txt', 'w') as file:\n#     file.write('[')\n#     for tweet_id  in unique_tweet_ids:\n#         try:\n#             print(f\"processing tweet_id: {tweet_id}\")\n#             tweet = api.get_status(tweet_id,tweet_mode='extended')\n#             json.dump(tweet._json, file)\n#             file.write(',')\n#         except tweepy.TweepError as e:\n#             print(f\"failed to process tweet_id:{tweet_id}\")\n#             failed_process_tweets[tweet_id] = e\n#             pass            \n#     file.write('{}]')","0d007a19":"tweet_extended_df = pd.read_json(\"..\/input\/tweet-json\/tweet_json.txt\",dtype ={\"id_str\": str})[:-1]\ntweet_extended_df.head()","e61013fe":"all_df = tweets_df.set_index('tweet_id').join(tweet_extended_df.set_index('id_str'),lsuffix='_basic', rsuffix='_extended')\nall_df = all_df.join(dog_bread_df.set_index('tweet_id'),lsuffix='', rsuffix='_breed')\nall_df.info()","bb0d83e7":"all_df.index.duplicated().any()","57c43b79":"all_df.replace('None', np.nan, inplace=True)\nall_df.info()","b54defdf":"all_df[['timestamp', 'created_at']].sample(5)","c4068e0f":"all_df[['source_basic', 'source_extended']].sample(5)","72347378":"all_df.source_basic.unique(), all_df.source_extended.unique()","5e95a9ac":"all_df.id.map(lambda x: '{:.0f}'.format(x)).sample(5)","e879b42a":"all_df.drop(columns=['contributors', 'coordinates', 'geo', 'place', 'in_reply_to_status_id_basic', \n                     'in_reply_to_user_id_basic', 'in_reply_to_screen_name', 'in_reply_to_status_id_extended', \n                     'in_reply_to_status_id_str', 'in_reply_to_user_id_extended', 'in_reply_to_user_id_str', \n                     'quoted_status', 'quoted_status_id','is_quote_status', 'quoted_status_id_str', 'quoted_status_permalink', \n                     'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_id', 'retweeted_status',\n                     'retweeted_status_timestamp'], inplace=True)","03d3e448":"all_df.drop(columns=['created_at','source_extended','id','expanded_urls'], inplace=True)\nall_df.sample(5)","08b16109":"all_df.info()","6dd2e7ae":"all_df[['doggo', 'floofer', 'pupper', 'puppo']].sample(10)","8523878c":"all_df.doggo.unique(), all_df.floofer.unique(), all_df.pupper.unique(),all_df.puppo.unique()","498c545b":"all_df.source_basic.replace('<a href=\"http:\/\/twitter.com\/download\/iphone\" rel=\"nofollow\">Twitter for iPhone<\/a>','iPhone', inplace=True)\nall_df.source_basic.replace('<a href=\"http:\/\/twitter.com\" rel=\"nofollow\">Twitter Web Client<\/a>','Twitter', inplace=True)\nall_df.source_basic.replace('<a href=\"http:\/\/vine.co\" rel=\"nofollow\">Vine - Make a Scene<\/a>','Vine', inplace=True)\nall_df.source_basic.replace('<a href=\"https:\/\/about.twitter.com\/products\/tweetdeck\" rel=\"nofollow\">TweetDeck<\/a>','Tweetdeck', inplace=True)\nall_df.rename(columns={\"source_basic\": \"source\"}, inplace=True)","26576262":"all_df.timestamp = pd.to_datetime(all_df[\"timestamp\"])\nall_df.rename(columns={'timestamp':\"tweet_date\"}, inplace=True)","60d79e2e":"all_df.doggo.replace(np.nan, False,inplace=True)\nall_df.floofer.replace(np.nan, False,inplace=True)\nall_df.pupper.replace(np.nan, False,inplace=True)\nall_df.puppo.replace(np.nan, False,inplace=True)\n\nall_df.doggo.replace('doggo', True,inplace=True)\nall_df.floofer.replace('floofer', True,inplace=True)\nall_df.pupper.replace('pupper', True,inplace=True)\nall_df.puppo.replace('puppo', True,inplace=True)","e1162546":"all_df[['doggo','floofer','pupper','puppo']].sum(axis=1).unique()","8bf3ec33":"all_df['multiple_age_stage'] = all_df[['doggo','floofer','pupper','puppo']].sum(axis=1) == 2\nmultiple_age_stage = all_df.query('multiple_age_stage == True')[['text', 'doggo','floofer','pupper','puppo']]\nmultiple_age_stage.shape","715f98a3":"multiple_age_stage.to_csv('multiple_age_stage.csv')","fe6edb9d":"all_df.rating_numerator.unique(), all_df.rating_denominator.unique()","1100b5a8":"rating_denominator_no_10 = all_df.query('rating_denominator != 10')[['text','rating_numerator', 'rating_denominator']]\nrating_denominator_no_10","f389bb47":"rating_numerator_above_15 = all_df.query('rating_denominator == 10 and rating_numerator > 15')[['text','rating_numerator', 'rating_denominator']]\nrating_numerator_above_15","a5985cd0":"visual_ratings_checkup_df = pd.concat([rating_denominator_no_10,rating_numerator_above_15])\nvisual_ratings_checkup_df.to_csv('visual_ratings_checkup.csv')","f13586ab":"rating_fixed = pd.read_csv('..\/input\/visual-fixing\/visual_ratings_checkup_fix.csv', dtype={\"tweet_id\": str})\nnot_dogs = rating_fixed.query('number_of_dogs != number_of_dogs').tweet_id\nall_df = all_df.query(f'tweet_id not in {not_dogs.tolist()}')","110c2a10":"dogs = rating_fixed.query('number_of_dogs == number_of_dogs')\ndogs = dogs.set_index('tweet_id')\nall_df_fixed = all_df.join(dogs[['rating_numerator', 'rating_denominator', 'number_of_dogs']], rsuffix='_fixed')\nall_df_fixed['rating_numerator'] = all_df_fixed[\"rating_numerator_fixed\"].fillna(all_df_fixed[\"rating_numerator\"]).astype(int)\nall_df_fixed['rating_denominator'] = all_df_fixed[\"rating_denominator_fixed\"].fillna(all_df_fixed[\"rating_denominator\"]).astype(int)\n","10f90b23":"all_df_fixed.rating_denominator.unique()","0283bdf7":"all_df_fixed.rating_numerator.unique()","11ac0b34":"all_df = all_df_fixed.drop(columns=['rating_numerator_fixed','rating_denominator_fixed'])","98bacfc4":"all_df['rating'] = (all_df.rating_numerator \/ all_df.rating_denominator).astype(float) ","59ee0795":"all_df.rating.describe()","6319e1be":"all_df.query('rating < 177').rating.describe()","a5be7f97":"all_df = all_df.query('rating < 177').drop(columns=['rating_numerator','rating_denominator'])","0e154cb3":"age_stage = pd.read_csv('..\/input\/visual-fixing\/multiple_age_stage_fix.csv',dtype={\"tweet_id\": str})\nnot_dogs = age_stage.query('is_dog==False').tweet_id\nall_df = all_df.query(f'tweet_id not in {not_dogs.tolist()}')","aaf00445":"dogs = age_stage.query('is_dog == True')\ndogs = dogs.set_index('tweet_id')\nall_df_fixed = all_df.join(dogs, rsuffix='_fixed')\nall_df_fixed['doggo'] = all_df_fixed[\"doggo_fixed\"].fillna(all_df_fixed[\"doggo\"])\nall_df_fixed['floofer'] = all_df_fixed[\"floofer_fixed\"].fillna(all_df_fixed[\"floofer\"])\nall_df_fixed['pupper'] = all_df_fixed[\"pupper_fixed\"].fillna(all_df_fixed[\"pupper\"])\nall_df_fixed['puppo'] = all_df_fixed[\"puppo_fixed\"].fillna(all_df_fixed[\"puppo\"])\nall_df = all_df_fixed.drop(columns=['doggo_fixed', 'floofer_fixed','pupper_fixed','puppo_fixed','is_dog','text_fixed'])","e3d47dfc":"all_df = all_df[all_df[['doggo','floofer','pupper','puppo']].sum(axis=1) != 2]","ac70ad59":"def age_stage_process(row):\n    if row.doggo:\n        return 'doggo'\n    elif row.floofer:\n        return 'floofer'\n    elif row.pupper:\n        return 'pupper'\n    elif row.puppo:\n        return 'puppo'\n    else:\n        return np.NaN\n    \nall_df['age_stage'] = all_df.apply(age_stage_process,axis=1)","7c3ab20f":"all_df.drop(columns=['doggo','floofer','pupper','puppo'], inplace=True)","0527ba0f":"all_df['favorite_count'].unique(), all_df['retweet_count'].unique()","8c99018c":"all_df.sample(1).entities.tolist()","90598e14":"all_df.sample(1).extended_entities.tolist()","7b55421c":"all_df.favorited.unique(), all_df.retweeted.unique(), all_df.possibly_sensitive.unique(), all_df.possibly_sensitive_appealable.unique()","b63f145f":"all_df.query('favorite_count != favorite_count')","4b597ab2":"all_df.query('favorite_count != favorite_count and p1 != p1')","f66ca6c1":"all_df.drop(columns = [\"multiple_age_stage\", \"number_of_dogs\", \"display_text_range\", \"entities\", \"extended_entities\", \n                       \"favorited\", \"full_text\", \"lang\",\"retweeted\", \"truncated\", \"user\", \"jpg_url\", \"img_num\", \n                       \"possibly_sensitive\", \"possibly_sensitive_appealable\"], inplace=True)","7933dedd":"missing_values = all_df.query('favorite_count != favorite_count and p1 != p1').index.tolist()\nmissing_values","6e228d70":"all_df = all_df.query(f'tweet_id not in {missing_values}')","c9c1cbdc":"missing_values = all_df.query('favorite_count != favorite_count').index.tolist()\nmissing_values","2078f99c":"all_df = all_df.query(f'tweet_id not in {missing_values}')","b8301baf":"all_df.info()","d149805f":"def bread_extraction(row):\n    bread_conf = 0\n    bread_name = ''\n    is_bread = False\n    if(row.p1_dog == True):\n        if(row.p1_conf >= bread_conf):\n            bread_conf = row.p1_conf\n            bread_name = row.p1\n            is_bread = True\n    if(row.p2_dog == True):\n        if(row.p2_conf >= bread_conf):\n            bread_conf = row.p2_conf\n            bread_name = row.p2\n            is_bread = True\n    if(row.p3_dog == True):\n        if(row.p2_conf >= bread_conf):\n            bread_conf = row.p3_conf\n            bread_name = row.p3\n            is_bread = True\n    if is_bread == False:\n        return np.nan\n    else:\n        return bread_name\n        \n        \n    \nall_df['bread'] = all_df.apply(bread_extraction,axis=1)","a8eb5613":"all_df.drop(columns=['p1','p2','p3','p1_dog','p2_dog','p3_dog','p1_conf', 'p2_conf', 'p3_conf'],inplace=True)","f7366c04":"all_df.info()","fe86f8ed":"all_df.bread.unique()","52db4d65":"all_df[all_df.name.str.lower() == all_df.name].name.unique()","840f8b86":"all_df[all_df.name.str.lower() == all_df.name].sample(10).text.tolist()","20428451":"all_df['name'] = all_df['name'].apply(lambda x: x if str(x).lower() != x else np.nan)","9938cdf4":"all_df[all_df.name.str.lower() == all_df.name].name.unique()","dc28dffe":"all_df.name.unique()","58ad084a":"all_df.info()","cbdafcb3":"all_df.to_csv('twitter_archive_master.csv')","be60ee31":"df = pd.read_csv('twitter_archive_master.csv')","ff36e855":"named_dogs = df.query('name == name')\nnamed_dogs_grouped = named_dogs.groupby('name').count()[['tweet_id']]\nnamed_dogs_grouped.rename(columns={'tweet_id':'name_count'}, inplace=True)\nnamed_dogs_grouped.query('name_count >= 8').sort_values(by=['name_count']).plot.bar()\nplt.ylim(top=12)\nplt.title(\"Most Popular Dog Names\",{'fontsize': 20},pad=20)\nplt.xlabel(\"Dogs Names\")\nplt.legend([\"Dogs Names Frequencey\"])","e6c9b3a0":"df[df.favorite_count == df.favorite_count.max()]","82644120":"df[df.retweet_count == df.retweet_count.max()]","3b2bcfe4":"df.query('tweet_id == \"744234799360020481\"').text.tolist()","b6ad26d8":"tweet_sources = df.groupby('source').count()[['tweet_id']]\ntweet_sources.rename(columns={'tweet_id': 'source_count'}, inplace=True)\ntweet_sources['source_percentage'] = tweet_sources.source_count \/ tweet_sources.source_count.sum() * 100\ntweet_sources['source_percentage'].plot.pie(figsize=(10,8), autopct='%1.1f%%',\n        explode=(0,0,0,0.1))\nplt.title(\"Source of Tweets\", {'fontsize': 20})\nplt.legend([\"Tweetdeck\", \"Twitter\", \"Vine\", \"iPhone\"])\nplt.ylabel(\"\")","5500bfeb":"bread_ratings = df.query('bread == bread')[['rating', 'bread']].groupby('bread').mean() * 10\nbread_ratings.hist()","da8df0a2":"bread_ratings.sort_values(by=['rating']).tail(5).plot.bar(figsize=(10,5))\nplt.ylim(top=14)\nplt.title(\"Top 5 Rated Breads\",{'fontsize': 20},pad=20)\nplt.xlabel(\"Breads\")\nplt.legend([\"Average Ratings\"])","409dcb3d":"bread_ratings.sort_values(by=['rating']).head(5).plot.bar(figsize=(10,5))\nplt.ylim(top=14)\nplt.title(\"Least 5 Rated Breads\",{'fontsize': 20},pad=20)\nplt.xlabel(\"Breads\")\nplt.legend([\"Average Ratings\"])","d4689a96":"from subprocess import call\ncall(['python', '-m', 'nbconvert', 'wrangle_act.ipynb'])","cd4d4e0e":"***Round 3: Cleaning***","e12d26a1":"**STEP 1:** File downloading","62e7a6c5":"***Round 3 : Assessment***\n* Quality\n    - convert `source_basic` feature into these values `iPhone`, `Twitter`, `Vine`, `TweetDeck`, as well rename it as `source` (noted in Round 1)\n    - convert `timestamp` to be datetime and rename the column into `tweet_date` (Noted in Round 1)\n    - convert columns `doggo`, `floofer`, `pupper`, `puppo` values into true\/false\n","2c55943d":"***Round 1: Cleaning***\n* Unify Null value to be NaN.\n","80f8a892":"***Round 5: Cleaning***","b180f34c":"* Uploading dog age stages after visual assessment and merge it back to orginal data","0d56676b":"* extract dog breed from predection data","c3e59c0f":"**STEP2:** Importing Files","64591cc3":"Second, I do the same with `image_predictions.tsv`, I will import it and have a quick glance on it.","c0f82ee1":"* remove null values for `favorite_count`,`retweet_count` and bread predection columns","a8c96957":"* rename `timestamp` to `tweet_date` and convert date to datetime","3fcd80cb":"***Round 1: Assessment***\n* Quality:\n    - Null values recorded as None and NaN (was noted while gathering data)\n","14381b1b":"* quality:\n    - some names are wrongs, should be fixed","39b8e36e":"***Most and least rated bread***\n* The distribution of average ratings of the breads is normally distributed with one outlayer with a rating value of 0.5, and max average rating is 1.3\n* The top average rating breads are Bouvier Des Flandres with average ratings of 1.3.\n* the lowest average rating breads are Japanese Spaniel with rating of 0.5","de706903":"* Drop columns with no\/low volume of data","0b8c8600":"**STEP 3:** Gathering Data using API","23ae7e53":"### Data Gathering\n\nLet us first start by gathering the two data sources \"Twitter Basic Details\" as `twitter_archive_enhanced.csv` and \"Dogs Image Classification\" as `image_predictions.tsv`.","907bb808":"# Project 4 - Data Wrangling (WeRateDogs)\n\n## Context\n\nWeRateDogs is a twitter account which share dog images and write a brief panegyric about the dog, then they let their followers to rate it by favoriting it. By asking WeRateDogs to share with us some of their tweets, they did. They have shared 5000+ of their tweets which contains some basic data. Sometimes in their brief panegyric they mention the breed of the dog, and some others they don\u2019t. But thanks to Udacity, they have performed some neutral network procedures to classify the dogs based on their images which are shared with the tweets.","410c44d2":"### Data Assessment and Cleaning\n\nCurrently we have gathered our data from 3 different sources, basic acrhived data from WeRateDogs, dogs breeds provided by some volunteer who applied neutral network classification process on images attached with the tweets, and downloaded live data for those tweets using twitter API. Now let us do data assessment to detect any quality or tidy issues in the data.\n\n_For this project, it is enough to find 8 quality issues, and 2 tidy issue. And as the data might contain more than this, I am going to the follow the project requirements._\n\nData Assessment and Cleaning can be done in iteration. I am going to mark every iteration of assessment followed by cleaning as a `Round`. and I am going to list list of quality and tidiness issues for each round.","b57b9aee":"* rename `source_basic` to `source` and rename values to `iPhone`, `Twitter`, `Vine`, `TweetDeck`","f7ce4679":"The classification data has 3 breeds predictions with different propability. One good feature about this data, that these predictions are linked with `tweet_id` this will make joining these data easy.","4d8a8c36":"***Round 2: Cleaning***","3f7e480b":"## Data Wrangling\n\nThe objective of this project is to gather data from different sources, to be assessed and then to perform cleansing techniques to raise the quality and tidiness of the data. Hence, it can be used in any later analysis.","53531938":"***Most dog tweet got favorited and retweeted***\n* the most dog was favorited and retweeted, it was from `Labrador Retriever` Bread, it was favorited by 159787 users and retweeted 79782 times. \n","140a8f8d":"* Drop duplicated columns","e7206445":"* group dogs age stage into one feature `age_stage`","1ec06323":"***Round 6: Assessment***","017db58a":"***Round 4: Assessment***\n* Quality:\n    - validate and correct rating numerator and denominator, and normalize denominator if possible. (Requires Visual Assessment)\n    - fix dogs with more than 1 age stage of(`doggo`, `floofer`, `pupper`, `puppo`). (Requires Visual Assessment)\n* Tideness:\n    - group age stages (`doggo`, `floofer`, `pupper`, `puppo`) into one column as `age_stage`","ed207f8b":"First, I will import `twitter_archive_enhanced.csv`and have a quick look of the data structure and values","c2f703bb":"Because the basic tweet data did not contain any information on number of likes\/favorits occured on the tweets, let us try to get this information using twitter API.\n\n_To use Twitter API, you have to request a developer app from Twitter, and they might ask you for some details in regards of What data you need, Who will have access to it, Why do you need it for ...etc._\n\nI am going to use `Tweepy` python library to connect to Twitter API.","347d5f36":"The basic tweets data contains some good information as `tweet_id`, `timestamp`, `rating_numerator`, `rating_denominator`,`name`.Also, there are some column headers which has unclear meaning as `doggo`, `floofer`, `pupper`, `puppo`. As well, I was expecting some more information in the data related to users who liked\/favorited the tweets and it is not part of the data frame.\n","aad4ec93":"* uploading fixed ratings after visual assessment, then merge the data and correct main data frame","1ded2aeb":"***Most source of the tweets***\n* 94% of the tweets was posted using iPhone twitter app, and that because of the tweets are from @WeRateDogs account.","a0b25961":"***Round 5: Assessment***","dbab55f8":"***Round 4: Cleaning***","01bbbfb0":"***Popular dog names***\n* Charlie, Cooper and Oliver are most popular names.","30c868fc":"## Storing and Acting on Wrangled Data","ad1e694a":"The only information are missing in the basic data frame shared with me is `retweet_count` and `favorit_count`. I am going to union unique tweet ids from both basic data  and dog breeds data. I will use those unique tweet ids to loop on and request twitter API one by one then store them in `tweet_extended.csv` file.\n\n_Twitter API has some usage rate limit, it is only allowed to make 450 calls per 15 minutes._","94bd7220":"* Convert wrong names into nan values.","fdad91b3":"***Round 6: Cleaning***","cb49e89b":"## Insights and Visualization","c07ea20e":"* Quality: \n    - remove the following features `multiple_age_stage`, `number_of_dogs`, `display_text_range`, `entities`, `extended_entities`, `favorited`, `full_text`, `lang`,`retweeted`, `truncated`, `user`, `jpg_url`, `img_num`, \n    - handle null values of `favorite_count`, `retweet_count` and bread predection columns\n* Tideness:\n    - extract dog breed from columns `p#`, `p#_conf` and `p#_dog` into `breed`","4212c550":"* Convert `doggo`, `floofer`, `pupper`, `puppo` values into true\/false","80f00906":"***Round 2: Assessment***\n* Quality:\n    - Remove columns which holds no data, or very low amount of data as `contributors`, `coordinates`, `geo`, `place`, `in_reply_to_status_id_basic`, `in_reply_to_user_id_basic`, `in_reply_to_screen_name`, `in_reply_to_status_id_extended`, `in_reply_to_status_id_str`, `in_reply_to_user_id_extended`, `in_reply_to_user_id_str`, `quoted_status`, `quoted_status_id`, `quoted_status_id_str`, `quoted_status_permalink`, `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_id`, `retweeted_status`, `retweeted_status_timestamp`,`is_quote_status`\n    - Drop duplicated columns as `created_at`, `source_extended`, `id`, `expanded_urls`","05c410c7":"* drop list of columns `multiple_age_stage`, `number_of_dogs`, `display_text_range`, `entities`, `extended_entities`, `favorited`, `full_text`, `lang`,`retweeted`, `truncated`, `user`, `jpg_url`, `img_num`,`possibly_sensitive`, `possibly_sensitive_appealable`"}}