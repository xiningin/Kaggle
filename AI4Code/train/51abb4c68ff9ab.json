{"cell_type":{"9fd38149":"code","1792f8a4":"code","4ff834d3":"code","676eda9c":"code","0b4e6b50":"code","ebb85937":"code","2cc49264":"code","7ac8efaa":"code","2e961350":"code","74d03379":"code","57d4af28":"code","213f6638":"code","56e55761":"code","41d26e22":"code","e68a3eaf":"code","9a5bdc94":"code","e9502f48":"code","62c939c8":"code","5c39fba2":"code","c6fdfe25":"code","d4917150":"code","0b9555fa":"code","52e971ec":"code","01d4b30a":"code","1b313186":"code","1b58e5a3":"code","6add4090":"code","69c0c25f":"code","93d2f6cd":"code","4a188c34":"markdown","e87bc956":"markdown","3f691d3c":"markdown","80c2808d":"markdown","8b473f11":"markdown","04bcf543":"markdown"},"source":{"9fd38149":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1792f8a4":"import numpy as np\nimport pandas as pd\nimport json\nimport lightgbm as lgb\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport copy\nimport datetime\n\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\n%matplotlib inline","4ff834d3":"np.random.seed(127)","676eda9c":"def load_data(filename, nrows=None, raw=False):\n    if raw == True:\n        json_columns = [\"device\", \"geoNetwork\", \"totals\", \"trafficSource\"]\n\n        df = pd.read_csv(\"..\/input\/gstore-homework\/\" + filename,\n                        converters={column: json.loads for column in json_columns},\n                        nrows=nrows)\n        \n    else:\n        df = pd.read_csv(\"..\/input\/gstore-homework\/\" + filename, nrows=nrows, index_col=0,\n                         dtype={'fullVisitorId': 'str'}, parse_dates=[\"date\"])\n        \n    return df","0b4e6b50":"X_train = load_data(filename=\"train_full_clear_corrected.csv\", nrows=None, raw=False)\ny_train = copy.deepcopy(X_train[\"transactionRevenue\"].values)\ny_train_user = X_train[['fullVisitorId', 'transactionRevenue']].groupby('fullVisitorId').sum() # !!!!!\nX_train.drop(columns=[\"transactionRevenue\"], inplace=True)\n\nX_test = load_data(filename=\"test_full_clear_corrected.csv\", nrows=None, raw=False)\n\nbounces_train = pd.read_csv(\"..\/input\/gstore-homework\/bounces_train.csv\")\nbounces_test = pd.read_csv(\"..\/input\/gstore-homework\/bounces_test.csv\")\n\ntmp = pd.DataFrame(X_test[\"fullVisitorId\"]).merge(bounces_test, left_index=True, right_index=True)\nbounced_users_mask = (tmp[[\"fullVisitorId\", \"bounces\"]].groupby(\"fullVisitorId\").bounces.value_counts().unstack(fill_value=0).loc[:, 0] == 0)\n\nbounced_users_idxs = tmp.set_index(\"fullVisitorId\")[bounced_users_mask].reset_index()[\"fullVisitorId\"]","ebb85937":"def get_folds(df=None, n_splits=5):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n\n    return fold_ids","2cc49264":"cat_cols = [\"channelGrouping\", \"browser\", \n            \"deviceCategory\", \"operatingSystem\", \n            \"city\", \"continent\", \n            \"country\", \"metro\",\n            \"networkDomain\", \"region\", \n            \"subContinent\", \"campaign\", \"medium\", \"isMobile\", \"source\"]\n\ncat_cols_without_time = [\"channelGrouping\", \"browser\", \n            \"deviceCategory\", \"operatingSystem\", \n            \"city\", \"continent\", \n            \"country\", \"metro\",\n            \"networkDomain\", \"region\", \n            \"subContinent\", \"campaign\", \"medium\", \"isMobile\", \"source\"]\n\nnum_cols = [\"hits\", \"pageviews\", \"visitNumber\", \"newVisits\"] \n\nnum_cols_without_time = [\"hits\", \"pageviews\", \"visitNumber\", \"newVisits\"]\n\ntrain_features = cat_cols + num_cols","7ac8efaa":"for col in cat_cols:\n    print(col)\n    lbl = sklearn.preprocessing.LabelEncoder()\n    lbl.fit(list(X_train[col].values.astype('str')) + list(X_test[col].values.astype('str')))\n    X_train[col] = lbl.transform(list(X_train[col].values.astype('str')))\n    X_test[col] = lbl.transform(list(X_test[col].values.astype('str')))\n\n\nfor col in num_cols:\n    X_train[col] = X_train[col].astype(float)\n    X_test[col] = X_test[col].astype(float)","2e961350":"params = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\", \n    \"num_leaves\" : 20,\n    \"min_child_samples\" : 100,\n    \"learning_rate\" : 0.1, # 0.05,\n    \"bagging_fraction\" : 0.7, # 0.5,\n    \"feature_fraction\" : 0.5, # 0.7,\n    \"bagging_frequency\" : 5,\n    \"bagging_seed\" : 127 # 2018\n}\n\n\nxgb_params = {\n        'objective': 'reg:linear',\n        'booster': 'gbtree',\n        'learning_rate': 0.1,\n        'max_depth': 22,\n        'min_child_weight': 57,\n        'gamma' : 1.45,\n        'alpha': 0.0,\n        'lambda': 0.0,\n        'subsample': 0.67,\n        'colsample_bytree': 0.054,\n        'colsample_bylevel': 0.50,\n        'n_jobs': -1,\n        'random_state': 456\n    }\n\ncat_params = {\n    'learning_rate' :0.1,\n    'depth' :10,\n    'eval_metric' :'RMSE',\n    'od_type' :'Iter',\n#     'metric_period ' : 50,\n    'od_wait' : 20,\n    'random_seed' : 42\n    \n}\n\nforest_params = {\n    \"n_estimators\": 100,\n    \"min_samples_split\": 100,\n    \"min_samples_leaf\": 100,\n    \"n_jobs\": -1,\n    \"verbose\": 1\n}","74d03379":"# Cross-validation!\nfolds = get_folds(df=X_train, n_splits=5)\n\n\nimportances = pd.DataFrame()\noof_lgb_preds = np.zeros(X_train.shape[0])\noof_xgb_preds = np.zeros(X_train.shape[0])\noof_cat_preds = np.zeros(X_train.shape[0])\noof_forest_preds = np.zeros(X_train.shape[0])\nlgb_preds = np.zeros((X_test.shape[0], len(folds)))\nxgb_preds = np.zeros((X_test.shape[0], len(folds)))\ncat_preds = np.zeros((X_test.shape[0], len(folds)))\nforest_preds = np.zeros((X_test.shape[0], len(folds)))\n\nmerge_preds = np.zeros(X_train.shape[0])\nsub_preds = np.zeros((X_test.shape[0], len(folds)))\n\n\nfor fold_, (trn_, val_) in enumerate(folds):\n    print(\"Fold:\",fold_)\n    \n    trn_x, trn_y = X_train[train_features].iloc[trn_], y_train[trn_]\n    val_x, val_y = X_train[train_features].iloc[val_], y_train[val_]\n    \n    \n    reg = lgb.LGBMRegressor(**params, n_estimators=1100)\n    xgb = XGBRegressor(**xgb_params, n_estimators=1000)\n    cat = CatBoostRegressor(**cat_params, iterations=1000)\n    forest = RandomForestRegressor(**forest_params)\n    \n    print(\"-\"* 20 + \"LightGBM Training\" + \"-\"* 20)\n    reg.fit(trn_x, np.log1p(trn_y),eval_set=[(val_x, np.log1p(val_y))],early_stopping_rounds=50,verbose=100,eval_metric='rmse')\n    print(\"-\"* 20 + \"XGboost Training\" + \"-\"* 20)\n    xgb.fit(trn_x, np.log1p(trn_y),eval_set=[(val_x, np.log1p(val_y))],early_stopping_rounds=50,eval_metric='rmse',verbose=100)\n    print(\"-\"* 20 + \"Catboost Training\" + \"-\"* 20)\n    cat.fit(trn_x, np.log1p(trn_y), eval_set=[(val_x, np.log1p(val_y))],early_stopping_rounds=50,use_best_model=True,verbose=100)\n    print(\"-\"* 20 + \"Forest Training\" + \"-\"* 20)\n    forest.fit(trn_x, np.log1p(trn_y))\n\n\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain_lgb'] = reg.booster_.feature_importance(importance_type='gain')\n    imp_df['fold'] = fold_ + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    # LightGBM\n    oof_lgb_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_lgb_preds[oof_lgb_preds < 0] = 0\n    lgb_preds_cur = reg.predict(X_test[train_features], num_iteration=reg.best_iteration_)\n    lgb_preds_cur[lgb_preds_cur < 0] = 0\n    \n    # Xgboost\n    oof_xgb_preds[val_] = xgb.predict(val_x)\n    oof_xgb_preds[oof_xgb_preds < 0] = 0\n    xgb_preds_cur = xgb.predict(X_test[train_features])\n    xgb_preds_cur[xgb_preds_cur < 0] = 0\n    \n    # Catboost\n    oof_cat_preds[val_] = cat.predict(val_x)\n    oof_cat_preds[oof_cat_preds < 0] = 0\n    cat_preds_cur = cat.predict(X_test[train_features])\n    cat_preds_cur[cat_preds_cur < 0] = 0\n\n    # Forest\n    oof_forest_preds[val_] = forest.predict(val_x)\n    oof_forest_preds[oof_forest_preds < 0] = 0\n    forest_preds_cur = forest.predict(X_test[train_features])\n    forest_preds_cur[forest_preds_cur < 0] = 0\n    \n    \n    # merge all prediction\n    merge_preds[val_] = oof_lgb_preds[val_] * 0.6 + oof_xgb_preds[val_] * 0.2 +\\\n    oof_cat_preds[val_] * 0.1 + oof_forest_preds[val_] * 0.1\n    \n    \n    \n    lgb_preds[:, fold_] = np.expm1(lgb_preds_cur)\n    xgb_preds[:, fold_] = np.expm1(xgb_preds_cur)\n    cat_preds[:, fold_] = np.expm1(cat_preds_cur)\n    forest_preds[:, fold_] = np.expm1(forest_preds_cur)\n                     \n    \n    \n# Without postprocessing (bounces)\n    \noof_lgb_preds_user = pd.DataFrame(X_train[\"fullVisitorId\"]).merge(pd.DataFrame(np.expm1(oof_lgb_preds),\n                                            columns=[\"prediction\"]), left_index=True, right_index=True) # Expm1 !!!!!\noof_lgb_preds_user = oof_lgb_preds_user.groupby(\"fullVisitorId\").sum()\ncomparison = pd.merge(y_train_user, oof_lgb_preds_user,\n                      on=[\"fullVisitorId\"], how=\"inner\")[[\"transactionRevenue\", \"prediction\"]]\ncomparison = np.log1p(comparison.values)\nprint(\"Total crossval LGBM rmse: \", mean_squared_error(comparison[:, 0], comparison[:, 1]) ** .5)\n\n\noof_xgb_preds_user = pd.DataFrame(X_train[\"fullVisitorId\"]).merge(pd.DataFrame(np.expm1(oof_xgb_preds),\n                                            columns=[\"prediction\"]), left_index=True, right_index=True) # Expm1 !!!!!\noof_xgb_preds_user = oof_xgb_preds_user.groupby(\"fullVisitorId\").sum()\ncomparison = pd.merge(y_train_user, oof_xgb_preds_user,\n                      on=[\"fullVisitorId\"], how=\"inner\")[[\"transactionRevenue\", \"prediction\"]]\ncomparison = np.log1p(comparison.values)\nprint(\"Total crossval XGB rmse: \", mean_squared_error(comparison[:, 0], comparison[:, 1]) ** .5)\n                        \n    \noof_cat_preds_user = pd.DataFrame(X_train[\"fullVisitorId\"]).merge(pd.DataFrame(np.expm1(oof_cat_preds),\n                                            columns=[\"prediction\"]), left_index=True, right_index=True) # Expm1 !!!!!\noof_cat_preds_user = oof_cat_preds_user.groupby(\"fullVisitorId\").sum()\ncomparison = pd.merge(y_train_user, oof_cat_preds_user,\n                      on=[\"fullVisitorId\"], how=\"inner\")[[\"transactionRevenue\", \"prediction\"]]\ncomparison = np.log1p(comparison.values)\nprint(\"Total crossval CatBoost rmse: \", mean_squared_error(comparison[:, 0], comparison[:, 1]) ** .5)\n                        \n    \noof_forest_preds_user = pd.DataFrame(X_train[\"fullVisitorId\"]).merge(pd.DataFrame(np.expm1(oof_forest_preds),\n                                            columns=[\"prediction\"]), left_index=True, right_index=True) # Expm1 !!!!!\noof_forest_preds_user = oof_forest_preds_user.groupby(\"fullVisitorId\").sum()\ncomparison = pd.merge(y_train_user, oof_forest_preds_user,\n                      on=[\"fullVisitorId\"], how=\"inner\")[[\"transactionRevenue\", \"prediction\"]]\ncomparison = np.log1p(comparison.values)\nprint(\"Total crossval RF rmse: \", mean_squared_error(comparison[:, 0], comparison[:, 1]) ** .5)","57d4af28":"for num in range(len(folds)):\n    X_train['predictions_' + \"lgb\"] = np.expm1(oof_lgb_preds)\n    X_train['predictions_' + \"xgb\"] = np.expm1(oof_xgb_preds)\n    X_train['predictions_' + \"cat\"] = np.expm1(oof_cat_preds)\n    X_train['predictions_' + \"forest\"] = np.expm1(oof_forest_preds)\n    \n    X_test['predictions_' + \"lgb\"] = np.mean(lgb_preds, axis=1)\n    X_test['predictions_' + \"xgb\"] = np.mean(xgb_preds, axis=1)\n    X_test['predictions_' + \"cat\"] = np.mean(cat_preds, axis=1)\n    X_test['predictions_' + \"forest\"] = np.mean(forest_preds, axis=1)","213f6638":"# Aggregate data at User level\ntrn_data = X_train[num_cols_without_time + ['fullVisitorId']].groupby('fullVisitorId').mean()\ntrn_data[cat_cols_without_time] = X_train[cat_cols_without_time + ['fullVisitorId']].groupby('fullVisitorId').median()","56e55761":"full_list = []\nfor model in [\"lgb\", \"xgb\", \"cat\", \"forest\"]:\n    trn_pred_list = X_train[['fullVisitorId', 'predictions_' + model]].groupby('fullVisitorId')\\\n        .apply(lambda df: list(df[\"predictions_\" + model]))\\\n        .apply(lambda x: {'pred_' + str(i) + \"_\" + model: pred for i, pred in enumerate(x) if i <= 43})\n    \n    full_list.append(trn_pred_list)","41d26e22":"for num, model in enumerate([\"lgb\", \"xgb\", \"cat\", \"forest\"]):\n    if num == 0:\n        trn_all_predictions = pd.DataFrame(list(full_list[num].values), index=trn_data.index)\n    else:\n        trn_all_predictions = trn_all_predictions.merge(pd.DataFrame(list(full_list[num].values), index=trn_data.index),\n                                                        left_index=True, right_index=True)","e68a3eaf":"del trn_pred_list, full_list","9a5bdc94":"# Create a DataFrame with VisitorId as index\n\nfor model in [\"lgb\", \"xgb\", \"cat\", \"forest\"]:\n    a = X_train[['fullVisitorId', 'predictions_' + model]].groupby('fullVisitorId')\n    tmp = copy.deepcopy(X_train[['fullVisitorId', 'predictions_' + model]])\n    tmp[\"predictions_\" + model] = np.log1p(tmp[\"predictions_\" + model])\n    tmp = tmp.groupby(\"fullVisitorId\")\n\n    trn_data['t_mean_' + model] = np.log1p(a.mean())\n    trn_data['t_median_' + model] = np.log1p(a.median())\n    trn_data['t_sum_log_' + model] = tmp.sum()\n    trn_data['t_sum_act_' + model] = np.log1p(a.sum())\n    \ntrn_data['t_nb_sess'] = a.count()\n# full_data = trn_data\nfull_data = pd.concat([trn_data, trn_all_predictions], axis=1).fillna(0)\n\ntrain_2level_features = trn_all_predictions.columns","e9502f48":"del trn_data, trn_all_predictions, X_train","62c939c8":"# Aggregate data at User level\n\nsub_data = X_test[num_cols_without_time + ['fullVisitorId']].groupby('fullVisitorId').mean()\nsub_data[cat_cols_without_time] = X_test[cat_cols_without_time + ['fullVisitorId']].groupby('fullVisitorId').median()","5c39fba2":"full_list_test = []\nfor model in [\"lgb\", \"xgb\", \"cat\", \"forest\"]:\n    test_pred_list = X_test[['fullVisitorId', 'predictions_' + model]].groupby('fullVisitorId')\\\n        .apply(lambda df: list(df[\"predictions_\" + model]))\\\n        .apply(lambda x: {'pred_' + str(i) + \"_\" + model: pred for i, pred in enumerate(x) if i <= 43})\n    \n    full_list_test.append(test_pred_list)","c6fdfe25":"for num, model in enumerate([\"lgb\", \"xgb\", \"cat\", \"forest\"]):\n    if num == 0:\n        test_all_predictions = pd.DataFrame(list(full_list_test[num].values), index=sub_data.index)\n    else:\n        test_all_predictions = test_all_predictions.merge(pd.DataFrame(list(full_list_test[num].values),\n                                                                       index=sub_data.index),\n                                                        left_index=True, right_index=True)\n        \nfor f in train_2level_features:\n    if f not in test_all_predictions.columns:\n        test_all_predictions[f] = np.nan","d4917150":"del test_pred_list, full_list_test","0b9555fa":"# Create a DataFrame with VisitorId as index\n\nfor model in [\"lgb\", \"xgb\", \"cat\", \"forest\"]:\n    b = X_test[['fullVisitorId', 'predictions_' + model]].groupby('fullVisitorId')\n    tmp2 = copy.deepcopy(X_test[['fullVisitorId', 'predictions_' + model]])\n    tmp2[\"predictions_\" + model] = np.log1p(tmp2[\"predictions_\" + model])\n    tmp2 = tmp2.groupby(\"fullVisitorId\")\n\n    sub_data['t_mean_' + model] = np.log1p(b.mean())\n    sub_data['t_median_' + model] = np.log1p(b.median())\n    sub_data['t_sum_log_' + model] = tmp2.sum()\n    sub_data['t_sum_act_' + model] = np.log1p(b.sum())\n    \nsub_data['t_nb_sess'] = b.count()\nsub_full_data = pd.concat([sub_data, test_all_predictions], axis=1).fillna(0)","52e971ec":"del sub_data, test_all_predictions","01d4b30a":"params = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\", \n    \"num_leaves\" : 20,\n    \"min_child_samples\" : 100,\n    \"learning_rate\" : 0.01, # 0.05,\n    \"bagging_fraction\" : 0.7, # 0.5,\n    \"feature_fraction\" : 0.5, # 0.7,\n    \"bagging_frequency\" : 5,\n    \"bagging_seed\" : 127 # 2018\n}\n\nxgb_params = {\n        'objective': 'reg:linear',\n        'booster': 'gbtree',\n        'learning_rate': 0.01,\n        'max_depth': 22,\n        'min_child_weight': 57,\n        'gamma' : 1.45,\n        'alpha': 0.0,\n        'lambda': 0.0,\n        'subsample': 0.67,\n        'colsample_bytree': 0.054,\n        'colsample_bylevel': 0.50,\n        'n_jobs': -1,\n        'random_state': 456\n    }\n\ncat_params = {\n    'learning_rate' :0.01,\n    'depth' :10,\n    'eval_metric' :'RMSE',\n    'od_type' :'Iter',\n    'od_wait' : 20,\n    'random_seed' : 42\n    \n}","1b313186":"folds = get_folds(df=full_data[\"pageviews\"].reset_index(), n_splits=2)\n\noof_reg_preds = np.zeros(full_data.shape[0])\noof_reg_preds1 = np.zeros(full_data.shape[0])\noof_reg_preds2 = np.zeros(full_data.shape[0])\nsub_preds = np.zeros(sub_full_data.shape[0])\nvis_importances = pd.DataFrame()\n\nfor fold_, (trn_, val_) in enumerate(folds):\n    print(\"Fold:\",fold_)\n    \n    trn_x, trn_y = full_data.iloc[trn_], y_train_user['transactionRevenue'].iloc[trn_]\n    val_x, val_y = full_data.iloc[val_], y_train_user['transactionRevenue'].iloc[val_]\n    \n    reg = lgb.LGBMRegressor(**params, n_estimators=1100)\n    xgb = XGBRegressor(**xgb_params, n_estimators=1000)\n    cat = CatBoostRegressor(**cat_params, iterations=1000)\n    print(\"-\"* 20 + \"LightGBM Training\" + \"-\"* 20)\n    reg.fit(trn_x, np.log1p(trn_y),eval_set=[(val_x, np.log1p(val_y))],early_stopping_rounds=50,verbose=100,eval_metric='rmse')\n    print(\"-\"* 20 + \"XGboost Training\" + \"-\"* 20)\n    xgb.fit(trn_x, np.log1p(trn_y),eval_set=[(val_x, np.log1p(val_y))],early_stopping_rounds=50,eval_metric='rmse',verbose=100)\n    print(\"-\"* 20 + \"Catboost Training\" + \"-\"* 20)\n    cat.fit(trn_x, np.log1p(trn_y), eval_set=[(val_x, np.log1p(val_y))],early_stopping_rounds=50,use_best_model=True,verbose=100)\n    \n    imp_df = pd.DataFrame()\n    imp_df['feature'] = trn_x.columns\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    \n    imp_df['fold'] = fold_ + 1\n    vis_importances = pd.concat([vis_importances, imp_df], axis=0, sort=False)\n    \n    # LightGBM\n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    lgb_preds = reg.predict(sub_full_data, num_iteration=reg.best_iteration_)\n    lgb_preds[lgb_preds < 0] = 0\n    \n    \n    # Xgboost\n    oof_reg_preds1[val_] = xgb.predict(val_x)\n    oof_reg_preds1[oof_reg_preds1 < 0] = 0\n    xgb_preds = xgb.predict(sub_full_data)\n    xgb_preds[xgb_preds < 0] = 0\n    \n    # catboost\n    oof_reg_preds2[val_] = cat.predict(val_x)\n    oof_reg_preds2[oof_reg_preds2 < 0] = 0\n    cat_preds = cat.predict(sub_full_data)\n    cat_preds[cat_preds < 0] = 0\n    \n    sub_preds += (lgb_preds \/ len(folds)) * 0.6 + (xgb_preds \/ len(folds)) * 0.3 + (cat_preds \/ len(folds)) * 0.1\n\n    \nprint(\"LGBM Result \", mean_squared_error(np.log1p(y_train_user['transactionRevenue']), oof_reg_preds) ** .5)\nprint(\"XGBoost Result\", mean_squared_error(np.log1p(y_train_user['transactionRevenue']), oof_reg_preds1) ** .5)\nprint(\"CatBoost Result\", mean_squared_error(np.log1p(y_train_user['transactionRevenue']), oof_reg_preds2) ** .5)","1b58e5a3":"# Transaction level!\n\nimportances['gain_log'] = np.log1p(importances['gain_lgb'])\nmean_gain = importances[['gain_lgb', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain_lgb'])\n\nplt.figure(figsize=(8, 10))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False).iloc[:300])","6add4090":"# User level!\n\nvis_importances['gain_log'] = np.log1p(vis_importances['gain'])\nmean_gain = vis_importances[['gain', 'feature']].groupby('feature').mean()\nvis_importances['mean_gain'] = vis_importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 10))\nsns.barplot(x='gain_log', y='feature', data=vis_importances.sort_values('mean_gain', ascending=False).iloc[:300])","69c0c25f":"sub_df = pd.DataFrame({\"fullVisitorId\": sub_full_data.reset_index()[\"fullVisitorId\"].values})\nsub_df[\"PredictedLogRevenue\"] = sub_preds\nidxs = sub_df[\"fullVisitorId\"].isin(bounced_users_idxs)\nsub_df.loc[idxs, \"PredictedLogRevenue\"] = 0","93d2f6cd":"sub_df.to_csv('two_levels_ensemble.csv', index=False)","4a188c34":"> ## 5. Visualize importances","e87bc956":"## 6. Making submission","3f691d3c":"## 3. Transaction level","80c2808d":"## 2. Cross-validation","8b473f11":"## 1. Loading data","04bcf543":"## 4. User level"}}