{"cell_type":{"4393fc33":"code","5935537e":"code","c69fa148":"code","d5a09761":"code","7c304499":"code","758d050d":"code","5614677c":"code","9a960dbd":"code","a4be2caf":"code","347d4ef6":"markdown","f626f359":"markdown","7d45fa14":"markdown","f484d21c":"markdown","e27583f5":"markdown","0356a875":"markdown","a18e3ded":"markdown"},"source":{"4393fc33":"import torch","5935537e":"def quadratic_kappa_coefficient(output, target):\n    n_classes = target.shape[-1]\n    weights = torch.arange(0, n_classes, dtype=torch.float32, device=output.device) \/ (n_classes - 1)\n    weights = (weights - torch.unsqueeze(weights, -1)) ** 2\n\n    C = (output.t() @ target).t()  # confusion matrix\n\n    hist_true = torch.sum(target, dim=0).unsqueeze(-1)\n    hist_pred = torch.sum(output, dim=0).unsqueeze(-1)\n\n    E = hist_true @ hist_pred.t()  # Outer product of histograms\n    E = E \/ C.sum() # Normalize to the sum of C.\n\n    num = weights * C\n    den = weights * E\n\n    QWK = 1 - torch.sum(num) \/ torch.sum(den)\n    return QWK","c69fa148":"target = torch.tensor([2,2,2,3,4,5,5,5,5,5]) - 1\noutput = torch.tensor([2,2,2,3,2,1,1,1,1,3]) - 1\n\noutput.shape, target.shape","d5a09761":"import torch.nn.functional as F\n\ntarget_onehot = F.one_hot(target, 5)\noutput_onehot = F.one_hot(output, 5)\n\noutput_onehot.shape, target_onehot.shape","7c304499":"quadratic_kappa_coefficient(output_onehot.type(torch.float32), target_onehot.type(torch.float32))","758d050d":"def quadratic_kappa_loss(output, target, scale=2.0):\n    QWK = quadratic_kappa_coefficient(output, target)\n    loss = -torch.log(torch.sigmoid(scale * QWK))\n    return loss\n\nclass QWKLoss(torch.nn.Module):\n    def __init__(self, scale=2.0):\n        super().__init__()\n        self.scale = scale\n\n    def forward(self, output, target):\n        # Keep trace of output dtype for half precision training\n        target = F.one_hot(target.squeeze(), num_classes=6).to(target.device).type(output.dtype)\n        output = torch.softmax(output, dim=1)\n        return quadratic_kappa_loss(output, target, self.scale)","5614677c":"class QWKMetric(torch.nn.Module):\n    def __init__(self, binned=False):\n        super().__init__()\n        self.binned = binned\n\n    def forward(self, output, target):\n        # Keep trace of dtype for half precision training\n        dtype = output.dtype\n        target = F.one_hot(target.squeeze(), num_classes=6).to(target.device).type(dtype)\n        if self.binned:\n            output = torch.sigmoid(output).sum(1).round().long()\n            output = F.one_hot(output.squeeze(), num_classes=6).to(output.device).type(dtype)\n        else:\n            output = torch.softmax(output, dim=1)\n        return quadratic_kappa_coefficient(output, target)","9a960dbd":"target = torch.randint(0, 6, (10, 1)).squeeze()\nprint(\"target: \", target)  # target class coming directly from the isup grades\n\noutput = torch.rand(10, 6)  # Logits from network, trained with not binned target\nprint(\"output: \", output)","a4be2caf":"nb_loss = QWKLoss()\nb_metric = QWKMetric(binned=True)\nnbl = nb_loss(output, target)\nbl = b_metric(output, target)\nprint(\"not binned loss: \", nbl.item())\nprint(\"binned metric: \", bl.item())","347d4ef6":"## Binned metric and not binned loss examples","f626f359":"## Use example from notebook\nReferring to the aforementioned notebook, each tensor is less by 1 because in that case there were 5 classes and it started from 1. To use the `torch` functions we must start from 0.\n\nIn this case, there are 5 classes and 10 items.  (i.e., batch size = 10).","7d45fa14":"## Implementation of metric function\nMoreover, it can be useful to define the metric function, even with binned labels for network training.","f484d21c":"Show same result from the aforementioned notebook. I have already checked the values and dimensions of the `C`, `E` and `weights` matrices, but feel free to check by yourself.","e27583f5":"Transform both arrays to `onehot` encoding.","0356a875":"# QWK metric in PyTorch\nSince the metric is not implemented natively in PyTorch, I decided to implement it by myself. I followed the very good explanation by @reigHns in his [notebook](https:\/\/www.kaggle.com\/reighns\/understanding-the-quadratic-weighted-kappa) and realized it using only pure `torch` functions.\n\nThe function takes as input a `onehot` encoding of the target, while it can take a list of probabilities (apply `softmax` to the network output) or, in case of `binned` labels, the sum of the sigmoid logits from the network (apply `sigmoid` and `sum` to the network output).\n\n## Drawbacks of QWK metric (and loss)\nSince the coefficient is calculated with the outer product of the `outputs` and `targets` histograms, if there is no guess in any class (in any item of batch), a division by `0` occurs, leading the metric (and loss) to `nan`. It is clear that, the more the batch size, the less is the probability of such an event. Higher batch sizes (10, maybe) should be considered; in addition, the last batch of `Dataloader` should be dropped to avoid this problem (flag `drop_last` in the PyTorch `Dataloader` API). In the end, this metric is quite unstable for little batch sizes, so it should be used carefully.","a18e3ded":"## Implementation of loss function\nThis method can be easily integrated inside a `torch.nn.Module` to build, for example, the correlated loss."}}