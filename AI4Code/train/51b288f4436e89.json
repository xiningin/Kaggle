{"cell_type":{"f4e2a9ed":"code","69e16398":"code","a290fc16":"code","ffbbda07":"code","3c4f9c23":"code","19e3e137":"code","7ae4c116":"code","b009f27f":"code","9191b014":"code","d1b15852":"code","b9e18eed":"code","26b9589d":"code","c3800b66":"code","07af3ebb":"code","f7bf94bb":"code","ae71127c":"code","9be8e5b1":"code","225fbb74":"code","a14c7f51":"code","dc74e0b2":"code","b898bb90":"code","f2335725":"code","77a7ac8f":"code","34203616":"code","84231915":"code","0ded947e":"code","b96f514d":"code","d8ba9341":"code","5b302a73":"markdown","ce253ee8":"markdown","1570d865":"markdown"},"source":{"f4e2a9ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","69e16398":"# Importing scikit-learn tools\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n# Importing standard ML set - numpy, pandas, matplotlib\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\n# Importing keras and its deep learning tools - neural network model, layers, contraints, optimizers, callbacks and utilities\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Activation, Dense, Dropout, Flatten\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.constraints import maxnorm\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.utils import np_utils\nfrom keras.regularizers import l2\nfrom keras.initializers import RandomNormal, VarianceScaling","a290fc16":"# path = \"..\/input\/testingdata\/TestingData\/\"\n# images = []\n# labels= []\n# brands = os.listdir(path)\n# for brand in brands:\n#     img = os.listdir(path+brand)\n#     for i in img:\n#         images.append(i)\n#         labels.append(brands.index(brand))\n    \n# dataset, labelset = shuffle(images, labels, random_state=42)\n# test_data = [dataset, labelset] ","ffbbda07":"img_x=img_y = 70\npath = \"..\/input\/trainingdata\/TrainingData\/\"\nimgs = []\nlabels= []\nbrands = os.listdir(path)\nprint(brands)\nfor idcar, brand in enumerate(brands):\n    img = os.listdir(path+brand)\n    for i, value in enumerate(img):\n        imgs.append(value)\n        labels.append(idcar)","3c4f9c23":"from PIL import Image\n# loading all images\nimages = np.array([ np.array( Image.open(path+brands[labels[i]]+'\/'+value).convert(\"RGB\") ).flatten() for i, value in enumerate(imgs)], order='F', dtype='uint8')\n# M\u1ed7i \u1ea3nh c\u00f3 k\u00edch th\u01b0\u1edbc 70x70 = 2500 pixel v\u00e0 3 k\u00eanh m\u00e0u = 14700 pixel\nprint('total images: ', np.shape(images) )","19e3e137":"dataset, labelset = shuffle(images, labels, random_state=42)\ntrain_data = [dataset, labelset] ","7ae4c116":"# an example image\nr=2434\nplt.imshow(images[r].reshape(img_x, img_y, 3))\nplt.title(brands[labels[r]])\nplt.show()","b009f27f":"# Training and preparing dataset\nX_train, X_val, y_train, y_val = train_test_split( train_data[0], train_data[1], test_size=0.2)","9191b014":"# bring images back size (20778, 50, 50,3)\ndef ImageConvert(n, i):\n    im_ex = i.reshape(n, img_x, img_y, 3)\n    im_ex = im_ex.astype('float32') \/ 255\n    # zero center data\n    im_ex = np.subtract(im_ex, 0.5)\n    # ...and to scale it to (-1, 1)\n    im_ex = np.multiply(im_ex, 2.0)\n    return im_ex\nX_train = ImageConvert(X_train.shape[0], X_train)\nX_val = ImageConvert(X_val.shape[0], X_val)","d1b15852":"# Labels have to be transformed to categorical\nY_train = np_utils.to_categorical(y_train, num_classes=len(brands))\nY_val = np_utils.to_categorical(y_val, num_classes=len(brands))","b9e18eed":"# Four Conv\/MaxPool blocks, a flattening layer and two dense layers at the end\ndef contruction(n_channels):\n    model = Sequential()\n    model.add(Conv2D(32, (3,3),\n                     input_shape=(img_x,img_y,n_channels),\n                     padding='valid',\n                     bias_initializer='glorot_uniform',\n                     kernel_regularizer=l2(0.00004),\n                     kernel_initializer=VarianceScaling(scale=2.0, mode='fan_in', distribution='normal', seed=None),\n                     activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(Conv2D(64, (3,3),\n                     padding='valid',\n                     bias_initializer='glorot_uniform',\n                     kernel_regularizer=l2(0.00004),\n                     kernel_initializer=VarianceScaling(scale=2.0, mode='fan_in', distribution='normal', seed=None),\n                     activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(Conv2D(128, (3,3),\n                     padding='valid',\n                     bias_initializer='glorot_uniform',\n                     kernel_regularizer=l2(0.00004),\n                     kernel_initializer=VarianceScaling(scale=2.0, mode='fan_in', distribution='normal', seed=None),\n                     activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(Conv2D(256, (3,3),\n                     padding='valid',\n                     bias_initializer='glorot_uniform',\n                     kernel_regularizer=l2(0.00004),\n                     kernel_initializer=VarianceScaling(scale=2.0, mode='fan_in', distribution='normal', seed=None),\n                     activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(Flatten())\n    \n    model.add(Dense(4096, activation='relu', bias_initializer='glorot_uniform'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(4096, activation='relu', bias_initializer='glorot_uniform'))\n    model.add(Dropout(0.5))\n    \n    # final activation is softmax, tuned to the number of classes\/labels possible\n    model.add(Dense(len(brands), activation='softmax'))\n    \n    # optimizer will be a stochastic gradient descent, learning rate set at 0.005\n    sgd = SGD(lr=0.005, decay=1e-6, momentum=0.95, nesterov=True)\n    adam = Adam()\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['categorical_accuracy'])\n    return model\nmodel = contruction(3)\n# Let's look at the summary\nmodel.summary()","26b9589d":"# Some callbacks have to be provided to choose the best trained model\n# patience set at 4 as 3 was too greedy - I observed better results after the third-worse epoch\nearly_stopping = EarlyStopping(patience=4, monitor='val_loss')\nCNN_file = '10car_1CNN_CMCMCMCMF.h5py'\n\ntake_best_model = ModelCheckpoint(CNN_file, save_best_only=True)\n\n# Finally for some CNN construction!\nbatch = 128\n# there are 40 brands altogether\nn_classes = len(brands)\nn_epochs = 100\n# images are RGB\nn_channels = 3","c3800b66":"history = model.fit(X_train, Y_train, batch_size=batch, shuffle=True, epochs=n_epochs, verbose=1, validation_data=(X_val, Y_val), callbacks=[early_stopping, take_best_model])","07af3ebb":"pd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1) # set the vertical range to [0-1]","f7bf94bb":"import os\nprint(os.listdir())","ae71127c":"# Save the weights\nmodel.save_weights('model_weights.h5')\n\n# Save the model architecture\nwith open('model_architecture.json', 'w') as f:\n    f.write(model.to_json())\n\n#model.load_weights(CNN_file)\nscores = model.evaluate(X_val, Y_val) # let's look at the accuracy on the test set\nprint(\"Accuracy test: %.2f%%\" % (scores[1]*100))","9be8e5b1":"from sklearn.metrics import precision_recall_fscore_support as prfs\n\n# Preparing for metrics check-up on the test set, may take a while...\nY_pred = model.predict_classes(X_val)\n\nprecision, recall, f1, support = prfs(y_val, Y_pred, average='weighted')\nprint(\"Precision: {:.2%}\\nRecall: {:.2%}\\nF1 score: {:.2%}\\nAccuracy: {:.2%}\".format(precision, recall, f1, scores[1]))","225fbb74":"\ndef ShowCase(cols, rows):\n    fdict = {'fontsize': 24,\n            'fontweight' : 'normal',\n            'verticalalignment': 'baseline'}\n    plt.figure(figsize=(cols * 5, rows * 4))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    c = 0\n    for i in range(rows * cols):\n        plt.subplot(rows, cols, i + 1)\n        \n        # r - randomly picked from the whole dataset\n        r = np.random.randint(np.shape(images)[0])\n        \n        # j - predicted class for the image of index r (weird syntax, but works :)\n        j = int(model.predict_classes(ImageConvert(1, images[r:r+1]), verbose=0))\n        \n        # increase success if predicted well\n        if labels[r] == j:\n            c += 1\n        \n        # image needs reshaping back to a 50px*50px*RGB\n        plt.imshow(images[r].reshape(img_x, img_y, 3))\n        \n        # plt.title will show the true brand and the predicted brand\n        plt.title('True brand: '+brands[labels[r]]+'\\nPredicted: '+brands[j],\n                  color= 'Green' if brands[labels[r]] == brands[j] else 'Red', fontdict=fdict) # Green for right, Red for wrong\n        \n        # no ticks\n        plt.xticks(())\n        plt.yticks(())\n        \n    # print out the success rate\n    print('Success rate: {}\/{} ({:.2%})'.format(c, rows*cols, c\/(rows*cols)))\n    \n    plt.show()","a14c7f51":"# That is strictly for the showcasing, how the CNN works - ain't that bad, after all :)\nShowCase(10, 5)","dc74e0b2":"img_x=img_y = 70\npath = \"..\/input\/testingdata\/TestingData\/\"\nimgs_test = []\nlabels_test = []\nbrands = os.listdir(path)\nfor idcar, brand in enumerate(brands):\n    img = os.listdir(path+brand)\n    for i, value in enumerate(img):\n        imgs_test.append(value)\n        labels_test.append(idcar)","b898bb90":"from PIL import Image\n# loading all images\nimages = np.array([ np.array( Image.open(path+brands[labels_test[i]]+'\/'+value).convert(\"RGB\") ).flatten() for i, value in enumerate(imgs_test)], order='F', dtype='uint8')\n# M\u1ed7i \u1ea3nh c\u00f3 k\u00edch th\u01b0\u1edbc 70x70 = 2500 pixel v\u00e0 3 k\u00eanh m\u00e0u = 14700 pixel\nprint('total images: ', np.shape(images) )","f2335725":"# preparation data\ndataset, labelset = shuffle(images, labels_test, random_state=42)\ntest_data = [dataset, labelset]\nX_test = ImageConvert(test_data[0].shape[0], test_data[0])\nY_test = np_utils.to_categorical(test_data[1], num_classes=len(brands))","77a7ac8f":"scores = model.evaluate(X_test, Y_test) # let's look at the accuracy on the test set\nprint(\"Accuracy test: %.2f%%\" % (scores[1]*100))","34203616":"\n\nfrom sklearn.metrics import precision_recall_fscore_support as prfs\n\n# Preparing for metrics check-up on the test set, may take a while...\nY_pred = model.predict_classes(X_test)\n\nprecision, recall, f1, support = prfs(test_data[1], Y_pred, average='weighted')\nprint(\"Precision: {:.2%}\\nRecall: {:.2%}\\nF1 score: {:.2%}\\nAccuracy: {:.2%}\".format(precision, recall, f1, scores[1]))","84231915":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport seaborn as sns #for better and easier plots\n\ndef report_and_confusion_matrix(label, prediction):\n    print(\"Model Report\")\n    print(classification_report(label, prediction))\n    score = accuracy_score(label, prediction)\n    print(\"Accuracy : \"+ str(score))\n    \n    ####################\n    fig, ax = plt.subplots(figsize=(8,8)) #setting the figure size and ax\n    mtx = confusion_matrix(label, prediction)\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  cbar=True, ax=ax) #create a heatmap with the values of our confusion matrix\n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","0ded947e":"report_and_confusion_matrix(test_data[1], Y_pred)","b96f514d":"# But let's check per class, too - assuming that larger datasets will be having higher metrics\nprecision_, recall_, f1_, support_ = prfs(test_data[1], Y_pred, average=None)","d8ba9341":"# We see that smaller sets (Lexus, Jaguar, Hyundai) have generally worse precision and recall\nplt.subplots(figsize=(18,30))\nx = range(len(brands))\nplt.subplot(311)\nplt.title('Precision per class')\nplt.ylim(0.8, 1.00)\nplt.bar(x, precision_, color='Red')\nplt.xticks(x, brands, rotation = 90)\nplt.subplot(312)\nplt.title('Recall per class')\nplt.ylim(0.8, 1.00)\nplt.bar(x, recall_, color='Green')\nplt.xticks(x, brands, rotation = 90)\nplt.subplot(313)\nplt.title('F1 score per class')\nplt.ylim(0.8, 1.00)\nplt.bar(x, f1_, color='Blue')\nplt.xticks(x, brands, rotation = 90)\nplt.show()","5b302a73":"**Metric of success**","ce253ee8":"**\nPlotting the accuracy, loss in both training set and validation set**","1570d865":"Evaluates the prediction on Test data"}}