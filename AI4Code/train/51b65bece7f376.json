{"cell_type":{"1b8b0a18":"code","b5f9c162":"code","ce886d88":"code","83312939":"code","bb6a60d1":"code","081b2558":"code","766273e8":"code","4b80dbab":"code","220ff9b4":"code","02290577":"code","374a5c10":"code","c5b2c845":"code","6caf225b":"code","e14972ec":"code","c705790b":"code","01555add":"code","7e4f794d":"code","82b7a3d7":"code","4fbbf35b":"markdown","52fb3eee":"markdown","200ece1e":"markdown","dfec7094":"markdown","bbdf74c4":"markdown","fb8f5d6f":"markdown","965d3d2e":"markdown","65f576fd":"markdown","ee944bb8":"markdown","0fd8ba74":"markdown","f085eede":"markdown"},"source":{"1b8b0a18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.tree import DecisionTreeClassifier #our algorithm library.\nfrom sklearn.model_selection import train_test_split #Separates for learning and prediction \n\nfrom sklearn.metrics import accuracy_score #Accuracy Score, what is our success rate ?\nfrom sklearn import tree\nimport numpy\nfrom sklearn.preprocessing import StandardScaler #for normalization\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5f9c162":"data = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\n\n#we defined our dataset\n","ce886d88":"X = data.iloc[:, 0:13]\nY = data.iloc[:, 13]\n\n# We specified our class(or target) column.","83312939":"data.head()\n# We printed our first 5 lines.","bb6a60d1":"print(X)","081b2558":"print(Y)\n#Target Column","766273e8":"nd = StandardScaler()\nnd.fit(X)\nX =nd.transform(X)\n","4b80dbab":"print(X)","220ff9b4":"X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3)","02290577":"clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,\n                               max_depth=6, min_samples_leaf=5)\n","374a5c10":"clf_gini.fit(X_train, y_train)","c5b2c845":"y_pred = clf_gini.predict(X_test)\nprint(y_pred)","6caf225b":"print(clf_gini.predict([[45,1,2,120,245,0,1,168,0,2.2,2,0,1]]))\n#Algorithm made its prediction by looking at the values I entered.\n","e14972ec":"print (\"Accuracy is \", accuracy_score(y_test,y_pred)*100)","c705790b":"clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,\n max_depth=3, min_samples_leaf=10)\n\nclf_entropy.fit(X_train, y_train)","01555add":"y_pred_en = clf_entropy.predict(X_test)\nprint(y_pred_en)","7e4f794d":"print(clf_entropy.predict([[45,1,2,120,245,0,1,168,0,2.2,2,0,1]]))","82b7a3d7":"print (\"Accuracy is \", accuracy_score(y_test,y_pred_en)*100)","4fbbf35b":"We need to divide our data into two, **Prediction and Learn**. I divided it as **70 percent** learning, \n**30 percent** as prediction. I used the code below for this.","52fb3eee":"Let's run the algorithm","200ece1e":"# **In this study, Gini and Entropy algorithms will be used.**","dfec7094":"Let's **normalize** our data.","bbdf74c4":"Let's print out the algorithm's **predictions** now.","fb8f5d6f":"Let's see the new state of our data.","965d3d2e":"Let's measure the accuracy of our algorithm.","65f576fd":"Our algorithm worked.Now we will enter random numbers ourselves for prediction. And the algorithm will predict.","ee944bb8":"Now I call the **Gini** algorithm for decision tree study.","0fd8ba74":"Our algorithm worked.Now we will enter random numbers ourselves for prediction. And the algorithm will predict.","f085eede":"Now let's do the same for **Entropy** algorithm."}}