{"cell_type":{"3edae7a0":"code","81fa05d1":"code","f9e3a2e5":"code","6df86388":"code","b7d92680":"code","b561eb73":"code","59dbfb34":"code","2e73ed75":"code","9f413599":"code","e3c5a9c3":"code","4ae29e95":"code","060aabe5":"code","e0c874b9":"code","9156f6a3":"code","58a74979":"code","8a19ffdf":"code","b34c92c4":"code","ef705a46":"code","01080591":"code","3f49e9eb":"markdown","f994c35b":"markdown","3e841863":"markdown","3f01e2d6":"markdown","e93fe6fb":"markdown","7c381dab":"markdown","b29ae8e3":"markdown","3ebe1747":"markdown","31021f01":"markdown","3ce8ff6d":"markdown","e7cb79d1":"markdown","e9b93af1":"markdown","d9856166":"markdown","24454349":"markdown","3695fd94":"markdown","5776de44":"markdown"},"source":{"3edae7a0":"# Kaggles seaborn version is still 0.10\n# 0.11 required for plots in this notebook\n!pip install seaborn --upgrade\nimport seaborn\nprint(f\"Seaborn version on this notebook = {seaborn.__version__}\")","81fa05d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math as m\nimport tensorflow as tf\nfrom sklearn import preprocessing\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9e3a2e5":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv') # when in kaggle!\n# df = pd.read_csv('heart.csv') # when at home!\ndf.describe()","6df86388":"# Useful functions\ndef make_bins_simple(min_bin, max_bin, size):\n    '''Create bins for histogram plots. Set min and max bin value (bin range) and bin size (step)'''\n    return list(range(min_bin, max_bin + size, size))\n\ndef make_bins_max(df_col, size, round_val):\n    '''Create bins for histogram plots. Min and max values rounded to nearest 10 from data'''\n    min_val = m.floor(min(df_col)\/round_val)*round_val\n    max_val = m.ceil(max(df_col)\/round_val)*round_val\n    return list(range(min_val, max_val + size, size))","b7d92680":"bins = make_bins_max(df.age, 2, 10)\nmale_pct = round(100 * sum(df.sex) \/ len(df.sex), 1)\nfemale_pct = round(100 - male_pct, 1)\n\nsns.histplot(data = df, x = df.age, hue = df.sex, kde = True, bins = bins)\nlabel = ['Male', 'Female']\nplt.legend(label)\nplt.title(f\"\"\"Age distribution by sex\n{female_pct}% female  {male_pct}% male\"\"\")\nplt.show()","b561eb73":"bins = make_bins_max(df.trestbps, 10, 10)\nmean_bps = df.groupby('target')['trestbps'].mean().round(1)\n\nkde_plot = sns.histplot(data = df, x = df.trestbps, hue = df.target, kde = True, bins = bins)\nplt.xlabel('Resting blood pressure (mmHg)')\nlabel = ['No presence', 'Presence']\nplt.legend(label)\nplt.title(f'''Resting blood pressure mean:\nNo heart disease {mean_bps[0]}\nHeart disease {mean_bps[1]}''')\nplt.show()","59dbfb34":"kde_data = [kde_plot.get_lines()[i].get_data() for i in range (len(kde_plot.get_lines()))]","2e73ed75":"presence_f = round(100*sum((df.sex == 0) & (df.target == 1))\/ sum(df.sex == 0),1)\npresence_m = round(100*sum((df.sex == 1) & (df.target == 1))\/ sum(df.sex == 1),1)\nprint(f'''Rates of heart disease by sex:\nFemales {presence_f}% \nMales {presence_m}%''')","9f413599":"bins = make_bins_max(df.trestbps, 10, 10)\ng = sns.FacetGrid(df, col=\"sex\", hue=\"target\")\ng.map_dataframe(sns.histplot, \"trestbps\", bins = bins, kde = True)\ng.set_axis_labels(\"Resting blood pressure (mmHg)\", \"Count\")\ng.add_legend()\nplt.show()","e3c5a9c3":"sns.kdeplot(x = df.cp, y = df.fbs, fill=True, hue = df.target, alpha = 0.7, thresh=.2)\nplt.xlabel('Chest pain')\nplt.ylabel('Fasting blood sugar > 120 mg\/dl')\nplt.xticks([0,1,2,3])\nplt.yticks([0,1])\nplt.xlim(-1, 3.5)\nplt.ylim(-0.5,1.5)\nplt.title(\"KDE plot\")\nplt.show()","4ae29e95":"sum(df.target)\/len(df.target), len(df.target)\n# There are more 1s than 0s but not too many.","060aabe5":"# Separate features and targets into numpy arrays\n# scale features using the sklearn preprocessing package\ntargets = np.array(df.target)\nunscaled_features = np.array(df.loc[:, df.columns != 'target'])\nscaled_features = preprocessing.scale(unscaled_features)","e0c874b9":"shuffled_indices = np.arange(scaled_features.shape[0])\nnp.random.shuffle(shuffled_indices)\n# Shuffle by assigninf random index\nshuffled_features = scaled_features[shuffled_indices]\nshuffled_targets = targets[shuffled_indices]","9156f6a3":"sample_counts = shuffled_features.shape[0]\ntrain_counts = int(0.8 * sample_counts)\nvalidation_counts = int(0.1 * sample_counts)\ntest_counts = sample_counts - train_counts - validation_counts\n\ntrain_inputs = shuffled_features[:train_counts]\ntrain_targets = shuffled_targets[:train_counts]\n\nvalidation_inputs = shuffled_features[train_counts:train_counts+validation_counts]\nvalidation_targets = shuffled_targets[train_counts:train_counts+validation_counts]\n\ntest_inputs = shuffled_features[train_counts + validation_counts:]\ntest_targets = shuffled_targets[train_counts + validation_counts:]","58a74979":"np.savez('heart_train', inputs=train_inputs, targets=train_targets)\nnp.savez('heart_validation', inputs=validation_inputs, targets=validation_targets)\nnp.savez('heart_test', inputs=test_inputs, targets=test_targets)","8a19ffdf":"# Class that will do the batching for the algorithm\nclass Heart_Data_Reader():\n    # Dataset is a mandatory arugment, while the batch_size is optional\n    # If you don't input batch_size, it will automatically take the value: None\n    def __init__(self, dataset, batch_size = None):\n    \n        # The dataset that loads is one of \"train\", \"validation\", \"test\".\n        # e.g. if I call this class with x('train',5), it will load 'heart_train.npz' with a batch size of 5.\n        npz = np.load('heart_{0}.npz'.format(dataset))\n        \n        # Two variables that take the values of the inputs and the targets. Inputs are floats, targets are integers\n        self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n        \n        # Counts the batch number, given the size you feed it later\n        # If the batch size is None, we are either validating or testing, so we want to take the data in a single batch\n        if batch_size is None:\n            self.batch_size = self.inputs.shape[0]\n        else:\n            self.batch_size = batch_size\n        self.curr_batch = 0\n        self.batch_count = self.inputs.shape[0] \/\/ self.batch_size\n    \n    # A method to load the next batch\n    def __next__(self):\n        if self.curr_batch >= self.batch_count:\n            self.curr_batch = 0\n            raise StopIteration()\n            \n        # You slice the dataset in batches and then the \"next\" function loads them one after the other\n        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n        inputs_batch = self.inputs[batch_slice]\n        targets_batch = self.targets[batch_slice]\n        self.curr_batch += 1\n        \n        # One-hot encode the targets.\n        classes_num = 2\n        targets_one_hot = np.zeros((targets_batch.shape[0], classes_num))\n        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n        \n        # The function will return the inputs batch and the one-hot encoded targets\n        return inputs_batch, targets_one_hot\n    \n        \n    # A method needed for iterating over the batches\n    def __iter__(self):\n        return self","b34c92c4":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\ndef run_NN(hidden_layer_size = 200,\n           batch_size = 140,\n           max_epochs = 1000,\n           verbose = False):\n\n    # 13 features\n    input_size = 13\n\n    # Output size is 2, (one hot encoded) no-presence or presence of heart disease.\n    output_size = 2\n\n    # Reset the default graph, so you can fiddle with the hyperparameters and then rerun the code.\n    tf.reset_default_graph()\n\n    # Create the placeholders\n    inputs = tf.placeholder(tf.float32, [None, input_size])\n    targets = tf.placeholder(tf.int32, [None, output_size])\n    \n    # Neural network structure, number of hidden layers = 2\n    weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n    biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n    outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n\n    weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n    biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n    outputs_2 = tf.nn.sigmoid(tf.matmul(outputs_1, weights_2) + biases_2)\n\n    weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, output_size])\n    biases_3 = tf.get_variable(\"biases_3\", [output_size])\n\n    # We will incorporate the softmax activation into the loss, as in the previous example\n    outputs = tf.matmul(outputs_2, weights_3) + biases_3\n\n    # Use the softmax cross entropy loss with logits\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n    mean_loss = tf.reduce_mean(loss)\n\n    # Get a 0 or 1 for every input indicating whether it output the correct answer\n    out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n    accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n\n    # Optimize with Adam\n    optimize = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(mean_loss)\n\n    # Create a session\n    sess = tf.InteractiveSession()\n\n    # Initialize the variables\n    initializer = tf.global_variables_initializer()\n    sess.run(initializer)\n\n    # Early stopping mechanism not used\n    prev_validation_loss = 9999999.\n\n    # Load the first batch of training and validation, using the class we created. \n    train_data = Heart_Data_Reader('train', batch_size)\n    validation_data = Heart_Data_Reader('validation')\n\n    validation_loss_list = []\n    training_loss_list = []\n    val_acc_list = []\n\n    # Create the loop for epochs \n    for epoch_counter in range(max_epochs):\n        \n        # Initialise epoch loss as 0 float\n        curr_epoch_loss = 0.\n\n        # Iterate over the training data \n        for input_batch, target_batch in train_data:\n            _, batch_loss = sess.run([optimize, mean_loss], \n                feed_dict={inputs: input_batch, targets: target_batch})\n\n            #Record the batch loss into the current epoch loss\n            curr_epoch_loss += batch_loss\n\n        # Find the mean curr_epoch_loss\n        # batch_count is a variable, defined in the Heart_Data_Reader class\n        curr_epoch_loss \/= train_data.batch_count\n\n        # Set validation loss and accuracy for the epoch to zero\n        validation_loss = 0.\n        validation_accuracy = 0.\n\n        # Use the same logic of the code to forward propagate the validation set\n        # There will be a single batch, as the class was created in this way\n        for input_batch, target_batch in validation_data:\n            validation_loss, validation_accuracy = sess.run([mean_loss, accuracy],\n                feed_dict={inputs: input_batch, targets: target_batch})\n\n        # Print statistics for the current epoch\n        if verbose == True:\n            print('Epoch '+str(epoch_counter+1)+\n                  '. Training loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n                  '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n                  '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n        \n        validation_loss_list.append(validation_loss)\n        training_loss_list.append(curr_epoch_loss)\n        val_acc_list.append(validation_accuracy)\n\n        # Trigger early stopping if validation loss begins increasing.\n        if validation_loss > prev_validation_loss:\n            #break\n            pass\n\n        # Store this epoch's validation loss to be used as previous in the next iteration.\n        prev_validation_loss = validation_loss\n\n    print(f'End of training with {batch_size} batches')\n    \n    plot_NN_results(hidden_layer_size, batch_size)\n    \n    return validation_loss_list, training_loss_list, val_acc_list","ef705a46":"def plot_NN_results(hidden_layer_size, batch_size):\n    max_val_acc = round(max(val_acc_list)*100,2)\n    min_val_loss_index = np.argmin(validation_loss_list)\n    \n    val_x = range(len(validation_loss_list))\n    val_y = validation_loss_list\n\n    train_x = range(len(training_loss_list))\n    train_y = training_loss_list\n\n    plt.plot(val_x, val_y, label = 'Validation')\n    plt.plot(train_x, train_y, label = 'Training')\n\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"validation loss\")\n    plt.legend()\n    plt.title(f\"\"\"Hidden layer size = {hidden_layer_size}\n    Batch no. at val loss min = {min_val_loss_index}\n    Batch size = {batch_size}\n    Max validation accuracy = {max_val_acc}%\"\"\")\n    plt.show()  ","01080591":"validation_loss_list, training_loss_list, val_acc_list = run_NN(hidden_layer_size = 200,\n                                                                batch_size = 140,\n                                                                max_epochs = 1000,\n                                                                verbose = False)\ntf.compat.v1.InteractiveSession().close()","3f49e9eb":"## c) Kernel Density Estimation (KDE) plot of fasting blood sugar, and chest pain reports\n","f994c35b":"### Distribution split by sex\n* Rates for female patient are significantly higher than for males\n* Female patients with heart disease also appear to contribute more to higher rbps","3e841863":"## b) Distribution of resting blood presure w.r.t presence of heart disease\n### Simple distribution\n* We associate high blood pressure with heart disease, therefore it is sensible to look at the differences its distribution against the target\n* Looking solely at the mean would indicate that on average patients without heart disease have a higher rbps\n* Further investigation is required due to the clear positive skewness of the distributions \n* Above 150 rbps patients with heart disease begin to contribute more to the distribution ","3f01e2d6":"#### Define target and scaled feature arrays","e93fe6fb":"#### Shuffle the data (targets are currently ordered, 1s first, 0s second)","7c381dab":"# Studying heart disease with Neural Networks","b29ae8e3":"### First look at the data: 13 feature columns and 1 target column at the end with presence (1) or no presence (0) of heart disease","3ebe1747":"#### Split data into train, test and validation sets (80:10:10)","31021f01":"## 2.  Applying a neural network model \n## a) Preprocessing data\n#### Check that the data is balanced (same number of target categories 0s and 1s)","3ce8ff6d":"### Features description:\n* age - age in years\n* sex - (0 = female, 1 = male)\n* cp - chest pain type\n* trestbps - resting blood pressure (mmHg)\n* chol - serum cholesterol (mg\/dl)\n* fbs - fasting blood sugar > 120 mg\/dl (0 = false, 1 = true)\n* restecg - resting electrocardiographic results (values 0, 1 , 2)\n* thalach - maximum heart rate achieved (bpm)\n* exang - exercise induced angina (0 = no, 1 = yes)\n* oldpeak - ST depression induced by exercise relative to rest\n* slope - the slope of the peak exercise ST segment \n* ca - number of major vessels colored by flourosopy (values 0 , 1 , 2, 3)\n* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n\n### Target\n* target - presence of heart disease (0 = no, 1 = yes)\n\n","e7cb79d1":"##### Kernel density data can be extracted from KDE plot and used for further analysis","e9b93af1":"### Functions to make bin lists for histograms","d9856166":"## b) Create ML algorithm\n### Class to handle batching","24454349":"#### Save the three datasets in .npz format for TensorFlow","3695fd94":"# 1. Exploratory data analysis: \n## a) Age distribution by sex\n* There are more than twice the number of males than females. \n* The age distribution is similar ","5776de44":"### Functions to run neural network and plot results"}}