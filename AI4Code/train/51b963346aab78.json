{"cell_type":{"fdedde52":"code","b26328ea":"code","1fe26234":"code","f9d79968":"code","5d23afe6":"code","56d0198f":"code","56acbc4f":"code","676ea1c5":"code","dc6d9a45":"code","25cadf43":"code","c6d78d39":"code","31096bcb":"code","b7a13a78":"code","c864a4eb":"code","edd8a721":"code","87153e5b":"code","cdf601d2":"code","13b0f9f8":"code","7366143a":"code","f7cafe42":"code","e6c92785":"code","e70d13e5":"code","d8683ded":"code","d4fcacfc":"code","614f2cd5":"code","b1482337":"code","74147102":"code","3f90934c":"code","e3583a64":"code","8a8fed10":"code","75be6fb6":"code","07acb37f":"code","0229207e":"code","c4d0b6b4":"code","2729a469":"code","9fb10b88":"code","eb547e89":"code","6c15eb26":"code","3ea2fc9f":"code","24b99e06":"code","a450d98d":"code","faba6be0":"code","5dadd230":"markdown","01efae36":"markdown","5127a52d":"markdown","e5872f37":"markdown","f540035c":"markdown","bcbe3cd9":"markdown","678ddd79":"markdown","039908cb":"markdown","f7008203":"markdown","648311bd":"markdown","ebd585d2":"markdown","9d86f964":"markdown","861b375e":"markdown","5686d8d2":"markdown","8c4187f9":"markdown","ff2f2075":"markdown","1175d927":"markdown","427c50b5":"markdown","e2f7c181":"markdown","12da0f3d":"markdown","88eb4551":"markdown","8fae0d3e":"markdown","88b47371":"markdown","7d701570":"markdown","50cb7918":"markdown","483f452b":"markdown"},"source":{"fdedde52":"import time\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nimport seaborn as sns\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix\n# from sklearn.preprocessing import StandardScaler","b26328ea":"dict_title = {\n    'Capt': 'Dr\/Clerc\/Mil',\n    'Col': 'Dr\/Clerc\/Mil',\n    'Major': 'Dr\/Clerc\/Mil',\n    'Jonkheer': 'Honor',\n    'Don': 'Honor',\n    'Dona': 'Honor',\n    'Sir': 'Honor',\n    'Dr': 'Dr\/Clerc\/Mil',\n    'Rev': 'Dr\/Clerc\/Mil',\n    'the Countess': 'Honor',\n    'Mme': 'Mrs',\n    'Mlle': 'Miss',\n    'Ms': 'Mrs',\n    'Mr': 'Mr',\n    'Mrs': 'Mrs',\n    'Miss': 'Miss',\n    'Master': 'Master',\n    'Lady': 'Honor'\n}\n\ndef extractTitle(df, nameCol, dictTitle):\n    '''\n    extractTitle(df, nameCol, dictTitle)\n    Input : df : dataframe, will be copied.\n            nameCol : name of the columns where to extract titles.\n            dictTitle : dictionary of title and their conversion.\n    This fonction extract title from a specific column with a custom dict and remove nameCol.\n    '''\n    \n    df_new = df.copy()\n    df_new[\"Title\"] = \"\"\n    for row in range(df_new.shape[0]):\n        name = df_new.loc[row][nameCol]\n        for title in dictTitle:\n            if title in name:\n                df_new[\"Title\"][row] = dictTitle[title]\n    return df_new.drop([nameCol], axis=1)\n\ndef getDummiesTitanic(df, dummies):\n    '''\n    getDummiesTitanic(df, dummies)\n    Input : df : dataframe, will be copied.\n            dummies : list of dummies to transform.\n            dictTitle : dictionary of title and their conversion\n    This fonction get dummies for a given list and drop the original column.\n    '''\n    df_new = df.copy()\n    for dummy in dummies:\n        try :\n            df_new = df_new.join(pd.get_dummies(df_new[dummy], prefix = dummy))\n            df_new = df_new.drop([dummy], axis=1)\n        except KeyError:\n            print(\"Warning : column {} is missing\".format(dummy))\n        \n    return df_new\n\ndef drawConfusionMatrix(y_test, y_pred):\n    '''\n    drawConfusionMatrix(y_test, y_pred)\n    Input : y_test : list of real target.\n            y_pred : list of predicted target.\n\n    This fonction draw a confusion matrix from y_test and y_pred.\n    '''\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    cm_sum = np.sum(cf_matrix, axis=1, keepdims=True)\n    cm_perc = cf_matrix \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cf_matrix).astype(str)\n    nrows, ncols = cf_matrix.shape\n    labels = [\"Died\", \"Survived\"]\n    sns.heatmap(cf_matrix\/np.sum(cf_matrix), \n                xticklabels=labels, \n                yticklabels=labels, \n                annot=True)\n    plt.yticks(rotation=0)\n    plt.ylabel('Predicted values', rotation=0)\n    plt.xlabel('Actual values')\n    plt.show()","1fe26234":"df_train_org = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test_org = pd.read_csv(\"..\/input\/titanic\/test.csv\")","f9d79968":"df_train_org.dtypes","5d23afe6":"print(\"In the train data we have {} rows and {} columns\".format(df_train_org.shape[0], df_train_org.shape[1]))","56d0198f":"df_test_org.dtypes","56acbc4f":"print(\"In the test data we have {} rows and {} columns\".format(df_test_org.shape[0], df_test_org.shape[1]))","676ea1c5":"df_train_org[\"Sex\"].value_counts()","dc6d9a45":"df_train_org[\"Embarked\"].value_counts()","25cadf43":"df_train = df_train_org.copy()\ndf_train = df_train.drop([\"PassengerId\", \"Ticket\"],axis=1) # Remove unique ID\ndf_train[\"SexNum\"] = df_train[\"Sex\"]\ndf_train[\"SexNum\"].loc[df_train[\"SexNum\"] == \"male\"] = 1\ndf_train[\"SexNum\"].loc[df_train[\"SexNum\"] == \"female\"] = 0\n\ndf_train[\"EmbarkedNum\"] = df_train[\"Embarked\"]\ndf_train[\"EmbarkedNum\"] = df_train[\"EmbarkedNum\"].fillna(0)\ndf_train[\"EmbarkedNum\"].loc[df_train[\"EmbarkedNum\"] == \"S\"] = 2\ndf_train[\"EmbarkedNum\"].loc[df_train[\"EmbarkedNum\"] == \"C\"] = 1\ndf_train[\"EmbarkedNum\"].loc[df_train[\"EmbarkedNum\"] == \"Q\"] = 0\ndf_train[\"EmbarkedNum\"] = df_train[\"EmbarkedNum\"].astype(int)\n\ndf_test= df_test_org.copy()\ndf_test= df_test.drop([\"PassengerId\", \"Ticket\"],axis=1) # Remove unique ID\ndf_test[\"SexNum\"] = df_test[\"Sex\"]\ndf_test[\"SexNum\"].loc[df_test[\"SexNum\"] == \"male\"] = 1\ndf_test[\"SexNum\"].loc[df_test[\"SexNum\"] == \"female\"] = 0\n\ndf_test[\"EmbarkedNum\"] = df_test[\"Embarked\"]\ndf_test[\"EmbarkedNum\"] = df_test[\"EmbarkedNum\"].fillna(0)\ndf_test[\"EmbarkedNum\"].loc[df_test[\"EmbarkedNum\"] == \"S\"] = 2\ndf_test[\"EmbarkedNum\"].loc[df_test[\"EmbarkedNum\"] == \"C\"] = 1\ndf_test[\"EmbarkedNum\"].loc[df_test[\"EmbarkedNum\"] == \"Q\"] = 0\ndf_test[\"EmbarkedNum\"] = df_test[\"EmbarkedNum\"].astype(int)","c6d78d39":"start_time = time.time()\nplt.figure(figsize=(8,8))\nsns.heatmap(df_train.corr(), annot=True, linewidths=.5, annot_kws={\"size\":10})\nplt.show()\nelapsed_time = time.time() - start_time\nprint(\"This graphic took me : {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))","31096bcb":"df_train.isna().mean()","b7a13a78":"df_test.isna().mean()","c864a4eb":"df_train_remove = df_train.drop([\"Cabin\", \"Age\", \"Embarked\", \"Fare\"], axis=1)\ndf_test_remove = df_test.drop([\"Cabin\", \"Age\", \"Embarked\", \"Fare\"], axis=1)","edd8a721":"df_train[\"Embarked\"].value_counts()","87153e5b":"start_time = time.time()\ndf_train_mean = df_train.drop([\"Cabin\"], axis=1).copy()\ndf_train_mean[\"Age\"] = df_train_mean[\"Age\"].fillna(df_train_mean[\"Age\"].mean())\ndf_train_mean[\"Embarked\"] = df_train_mean[\"Embarked\"].fillna(\"S\")\ndf_test_mean = df_test.drop([\"Cabin\"], axis=1).copy()\ndf_test_mean[\"Age\"] = df_test_mean[\"Age\"].fillna(df_test_mean[\"Age\"].mean())\ndf_test_mean[\"Fare\"] = df_test_mean[\"Fare\"].fillna(df_test_mean[\"Fare\"].mean())\nelapsed_time = time.time() - start_time\nprint(\"This calculations took me : {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))","cdf601d2":"df_train_mean.isna().sum()","13b0f9f8":"df_train_median = df_train.drop([\"Cabin\"], axis=1).copy()\ndf_train_median[\"Age\"] = df_train_median[\"Age\"].fillna(df_train_median[\"Age\"].median())\ndf_train_median[\"Embarked\"] = df_train_median[\"Embarked\"].fillna(\"S\")\ndf_test_median = df_test.drop([\"Cabin\"], axis=1).copy()\ndf_test_median[\"Age\"] = df_test_median[\"Age\"].fillna(df_test_median[\"Age\"].median())\ndf_test_median[\"Fare\"] = df_test_median[\"Fare\"].fillna(df_test_median[\"Fare\"].median())","7366143a":"start_time = time.time()\ndf_train_remove = extractTitle(df_train_remove, \"Name\", dict_title)\ndf_test_remove = extractTitle(df_test_remove, \"Name\", dict_title)\ndf_train_mean = extractTitle(df_train_mean, \"Name\", dict_title)\ndf_test_mean = extractTitle(df_test_mean, \"Name\", dict_title)\ndf_train_median = extractTitle(df_train_median, \"Name\", dict_title)\ndf_test_median = extractTitle(df_test_median, \"Name\", dict_title)\nelapsed_time = time.time() - start_time\nprint(\"This calculations took me : {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))","f7cafe42":"df_train_remove.head()","e6c92785":"start_time = time.time()\nlist_dummies = [\"Sex\", \"Embarked\", \"Title\"]\n\ndf_train_remove = getDummiesTitanic(df_train_remove, list_dummies)\ndf_test_remove = getDummiesTitanic(df_test_remove, list_dummies)\ndf_train_mean = getDummiesTitanic(df_train_mean, list_dummies)\ndf_test_mean = getDummiesTitanic(df_test_mean, list_dummies)\ndf_train_median = getDummiesTitanic(df_train_median, list_dummies)\ndf_test_median = getDummiesTitanic(df_test_median, list_dummies)\nelapsed_time = time.time() - start_time\nprint(\"This calculations took me : {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))","e70d13e5":"df_train_remove.head()","d8683ded":"X_train_remove, X_test_remove, y_train_remove, y_test_remove = train_test_split(df_train_remove.drop([\"Survived\"], axis=1), \n                                                               df_train_remove[\"Survived\"], \n                                                               test_size=0.2,\n                                                               random_state=0)\n\nX_train_mean, X_test_mean, y_train_mean, y_test_mean = train_test_split(df_train_mean.drop([\"Survived\"], axis=1), \n                                                       df_train_mean[\"Survived\"], \n                                                       test_size=0.2,\n                                                       random_state=0)\nX_train_median, X_test_median, y_train_median, y_test_median = train_test_split(df_train_median.drop([\"Survived\"], axis=1), \n                                                               df_train_median[\"Survived\"], \n                                                               test_size=0.2,\n                                                               random_state=0)\n\n\nSCORES = {\"Remove\":{},\"Mean\":{},\"Median\":{}}","d4fcacfc":"start_time = time.time()\nclf = svm.SVC(kernel='linear', C = 1.0) #Check other models\nclf.fit(X_train_remove, y_train_remove)\ny_pred_remove = clf.predict(X_test_remove)\n\ndrawConfusionMatrix(y_test_remove, y_pred_remove)\n\nscore = (((y_pred_remove == y_test_remove).sum())\/y_test_remove.shape[0])\nscore = round(score*100,2)\nSCORES[\"Remove\"][\"SVM\"] = score\nprint(\"Perfomace is : {}% for SVM_Remove\".format(score))\nelapsed_time = time.time() - start_time\nprint(\"This calculations took me : {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))","614f2cd5":"start_time = time.time()\nclf.fit(X_train_mean, y_train_mean)\ny_pred_mean = clf.predict(X_test_mean)\n\ndrawConfusionMatrix(y_test_mean, y_pred_mean)\n\nscore = (((y_pred_mean == y_test_mean).sum())\/y_test_mean.shape[0])\nscore = round(score*100,2)\nSCORES[\"Mean\"][\"SVM\"] = score\nprint(\"Perfomace is : {}% for SVM_Mean\".format(score))\nelapsed_time = time.time() - start_time\nprint(\"This calculations took me : {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))","b1482337":"start_time = time.time()\nclf.fit(X_train_median, y_train_median)\ny_pred_median = clf.predict(X_test_median)\n\ndrawConfusionMatrix(y_test_median, y_pred_median)\n\nscore = (((y_pred_median == y_test_median).sum())\/y_test_median.shape[0])\nscore = round(score*100,2)\nSCORES[\"Median\"][\"SVM\"] = score\nprint(\"Perfomace is : {}% for SVM_Median\".format(score))\nelapsed_time = time.time() - start_time\nprint(\"This calculations took me : {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))","74147102":"SCORES","3f90934c":"# Dict to save best parameters\nBEST_PARAMS = {\"Remove\":{},\"Mean\":{},\"Median\":{}}\n\n# defining parameter range \nparam_grid = [{\"C\": [0.1, 1, 10, 100, 1000],  \n              \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001],\n              \"kernel\": [\"rbf\", \"sigmoid\"]},\n              {\"C\": [0.1, 1, 10, 100, 1000],\n              \"kernel\": [\"linear\"]},\n             {\"C\": [0.1, 1, 10, 100, 1000],  \n              \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001],\n              \"kernel\": [\"poly\"],\n              \"degree\" : [1,2,3,4,5,6,7,8,9,10]}]\n# tol and max_iter because It's taking too long to train\ngrid = GridSearchCV(svm.SVC(max_iter=1000000), param_grid, refit = True, verbose=3, n_jobs=-1, cv=5)\ngrid.fit(X_train_remove, y_train_remove) \nprint(\"The best parameters are : {} with removed features and the score is {}\".format(grid.best_params_, grid.best_score_))\nscore = round((grid.best_score_*100),2)\nSCORES[\"Remove\"][\"SVM_BestParam\"] = score\nBEST_PARAMS[\"Remove\"][\"SVM\"] = grid.best_params_","e3583a64":"# defining parameter range \nparam_grid = [{\"C\": [0.1, 1, 10, 100, 1000],  \n              \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001],\n              \"kernel\": [\"rbf\", \"sigmoid\"]},\n              {\"C\": [0.1, 1, 10, 100, 1000],\n              \"kernel\": [\"linear\"]},\n             {\"C\": [0.1, 1, 10, 100, 1000],  \n              \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001],\n              \"kernel\": [\"poly\"],\n              \"degree\" : [1,2,3,4,5,6,7,8,9,10]}]\n# tol and max_iter because It's taking too long to train\ngrid = GridSearchCV(svm.SVC(max_iter=1000000), param_grid, refit = True, verbose=3, n_jobs=-1, cv=5)\ngrid.fit(X_train_mean, y_train_mean)\nprint(\"The best parameters are : {} with mean features and the score is {}\".format(grid.best_params_, grid.best_score_))\nscore = round((grid.best_score_*100),2)\nSCORES[\"Mean\"][\"SVM_BestParam\"] = score\nBEST_PARAMS[\"Mean\"][\"SVM\"] = grid.best_params_","8a8fed10":"# defining parameter range \nparam_grid = [{\"C\": [0.1, 1, 10, 100, 1000],  \n              \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001],\n              \"kernel\": [\"rbf\", \"sigmoid\"]},\n              {\"C\": [0.1, 1, 10, 100, 1000],\n              \"kernel\": [\"linear\"]},\n             {\"C\": [0.1, 1, 10, 100, 1000],  \n              \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001],\n              \"kernel\": [\"poly\"],\n              \"degree\" : [1,2,3,4,5,6,7,8,9,10]}]\n# tol and max_iter because It's taking too long to train\ngrid = GridSearchCV(svm.SVC(max_iter=1000000), param_grid, refit = True, verbose=3, n_jobs=-1, cv=5)\ngrid.fit(X_train_median, y_train_median)\nprint(\"The best parameters are : {} with median featuresand the score is {}\".format(grid.best_params_, grid.best_score_))\nscore = round((grid.best_score_*100),2)\nSCORES[\"Median\"][\"SVM_BestParam\"] = score\nBEST_PARAMS[\"Median\"][\"SVM\"] = grid.best_params_","75be6fb6":"SCORES","07acb37f":"BEST_PARAMS","0229207e":"pd.DataFrame(SCORES).plot(kind='bar')\nplt.ylim(0, 120)\nplt.ylabel('Precision')\nplt.show()","c4d0b6b4":"knn = KNeighborsClassifier(n_neighbors=3)\nprint(\"Train\/Test\/Record for df_train_remove\")\nknn.fit(X_train_remove, y_train_remove)\ny_pred_remove = knn.predict(X_test_remove)\nprint(confusion_matrix(y_test_remove, y_pred_remove))\nscore = (((y_pred_remove == y_test_remove).sum())\/y_test_remove.shape[0])\nscore = round(score*100,2)\nSCORES[\"Remove\"][\"KNN_3\"] = score\nprint(\"Performace is : {}% for KNN_3_Remove\".format(score))\nprint(\"Train\/Test\/Record for df_train_mean\")\nknn.fit(X_train_mean, y_train_mean)\ny_pred_mean = knn.predict(X_test_mean)\nprint(confusion_matrix(y_test_mean, y_pred_mean))\nscore = (((y_pred_mean == y_test_mean).sum())\/y_test_mean.shape[0])\nscore = round(score*100,2)\nSCORES[\"Mean\"][\"KNN_3\"] = score\nprint(\"Performace is : {}% for KNN_3_Mean\".format(score))\nprint(\"Train\/Test\/Record for df_train_median\")\nknn.fit(X_train_median, y_train_median)\ny_pred_median = knn.predict(X_test_median)\nprint(confusion_matrix(y_test_median, y_pred_median))\nscore = (((y_pred_median == y_test_median).sum())\/y_test_median.shape[0])\nscore = round(score*100,2)\nSCORES[\"Median\"][\"KNN_3\"] = score\nprint(\"Performace is : {}% for KNN_3_Median\".format(score))","2729a469":"SCORES","9fb10b88":"# defining parameter range \nparam_grid = [{\"n_neighbors\": range(1,101),  \n              \"weights\": [\"uniform\", \"distance\"],\n              \"algorithm\": [\"auto\", \"brute\"],\n              \"p\" : [1,2]},\n             {\"n_neighbors\": range(1,101),  \n              \"weights\": [\"uniform\", \"distance\"],\n              \"algorithm\": [\"ball_tree\", \"kd_tree\"],\n              \"p\" : [1,2],\n             \"leaf_size\": [1,2,3,4,5,10,15,20,25,30]}]\n\n# With Remove features\ngrid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True, verbose=3, n_jobs=-1, cv=5)\ngrid.fit(X_train_remove, y_train_remove) \nprint(\"The best parameters are : {} with remove features and the score is {}\".format(grid.best_params_, grid.best_score_))\nscore = round((grid.best_score_*100),2)\nSCORES[\"Remove\"][\"KNN_BestParam\"] = score\nBEST_PARAMS[\"Remove\"][\"KNN\"] = grid.best_params_\n\n# With Mean features\ngrid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True, verbose=3, n_jobs=-1, cv=5)\ngrid.fit(X_train_mean, y_train_mean)\nprint(\"The best parameters are : {} with mean features and the score is {}\".format(grid.best_params_, grid.best_score_))\nscore = round((grid.best_score_*100),2)\nSCORES[\"Mean\"][\"KNN_BestParam\"] = score\nBEST_PARAMS[\"Mean\"][\"KNN\"] = grid.best_params_\n\n# With Median features\ngrid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True, verbose=3, n_jobs=-1, cv=5)\ngrid.fit(X_train_median, y_train_median)\nprint(\"The best parameters are : {} with mean features and the score is {}\".format(grid.best_params_, grid.best_score_))\nscore = round((grid.best_score_*100),2)\nSCORES[\"Median\"][\"KNN_BestParam\"] = score\nBEST_PARAMS[\"Median\"][\"KNN\"] = grid.best_params_","eb547e89":"pd.DataFrame(SCORES).plot(kind='bar')\nplt.ylim(0, 120)\nplt.ylabel('Precision')\nplt.show()","6c15eb26":"SCORES","3ea2fc9f":"X_train = df_train_remove.drop([\"Survived\"], axis=1)\nY_train = df_train_remove[\"Survived\"]\nX_test = df_test_remove","24b99e06":"BEST_PARAMS[\"Remove\"][\"SVM\"]","a450d98d":"start_time = time.time()\nclf = svm.SVC(C = 1, gamma = 0.1, kernel = 'rbf') #Use best params\nclf.fit(X_train, Y_train)\ny_pred = clf.predict(X_test)","faba6be0":"# Creation of the submission file :\nDF_Fin = pd.DataFrame(columns=[\"PassengerId\",\"Survived\"])\nDF_Fin[\"PassengerId\"] = df_test_org[\"PassengerId\"]\nDF_Fin[\"Survived\"] = y_pred\n\nDF_Fin.head()","5dadd230":"## Data import","01efae36":"If we plot the results","5127a52d":"* Compute MEDIAN for numerical and most occurent for categorical","e5872f37":"From all these tests we can conclude that the best model is the SVM with best_param with 83.15% and remove features.\n\nSo if we do it with the real test data :","f540035c":"## Test and train a model\n\nFirst we need to split the datasets in train and test. The mesure the preformance of our models.\n\nWe also create a dict to record all the scores.","bcbe3cd9":"We can use three method to take care of the missing data points\n* Remove columns","678ddd79":"As we can see with the best scores we almost gain 2% with optimized hyperparameters.\n\nIf we plot them :","039908cb":"## Personalized functions and tools\n\nSince the Titanic dataset contains a name with a title, I used the idea of Ertu\u011frul Demir to create a dictionary to replace them (https:\/\/www.kaggle.com\/datafan07\/titanic-eda-and-several-modelling-approaches)","f7008203":"We can se that we have :\n* a weak negative correclation between __PClass__ and __Survived__ : 1st class survived more.\n* a weak positive correlation between __Survived__ and __Fare__ : if you have more money you wuold have survived.\n* a weak negative correlation between __Survived__ and __SexNum__ : more women survived.\n* a weak negative correlation between __Pclass__ and __Fare__ : 1st class have more money than 3rd.\n* a weak positive correlation between __SibSp__ and __Parch__ : since it's the number of family onboard it's logical.\n\n<a id=\"MISSING\"><\/a>\n### Handeling missing data\n\nAfter checking the type of the data we use, we need to see where missing values are, to do so we can simply calculated them this way :","648311bd":"# Data cleaning and analysis\n\n## Data analysis\n\nFrom Kaggle we know how the data is ordered and it define our goal :\n\nThe data has been split into two groups:\n\n    training set (train.csv)\n    test set (test.csv)\n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n\n### Data Dictionary\n\n|Variable\t    |Definition\t        |Key\n| ------------- |:-----------------:| -----:|\n|survival \t    |Survival \t        |0 = No, 1 = Yes\n|pclass \t    |Ticket class \t    |1 = 1st, 2 = 2nd, 3 = 3rd\n|sex \t        |Sex \t\n|Age \t        |Age in years \t\n|sibsp \t        |# of siblings \/ spouses aboard the Titanic \t\n|parch \t        |# of parents \/ children aboard the Titanic \t\n|ticket         |Ticket number \t\n|fare \t        |Passenger fare \t\n|cabin \t        |Cabin number \t\n|embarked \t    |Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton\n\n### Variable Notes\n\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.\n\nIf we watch how the data is in our ile, wa saw that the information is correct.\n\n* First the train data\n\n","ebd585d2":"From this step we know :\n* __Cabin__ information is missing in more than 75% of our data, so we'll not use it.\n* __Age__ information is missing at 20% of the time, we'll try to remplace the missing values\n* __Embarked__ information is missing several values in the trainset\n* __Fare__ information is missing serveral values in the testset","9d86f964":"## GridSearch to optimized Hyperparameters [see documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html)\nThe results with basic parameters for SVM are not bad, but we can find better hyperparameters with GridSearch.","861b375e":"### With the SVM (Support Vector Machines [see documentation](https:\/\/scikit-learn.org\/stable\/modules\/svm.html)) :\n\n* If we use the df_train_remove :","5686d8d2":"One of the most current way to deal with categorical data is to [get dummies](https:\/\/towardsdatascience.com\/the-dummys-guide-to-creating-dummy-variables-f21faddb1d40)","8c4187f9":"* Then the test data","ff2f2075":"As we see the two set have the same topology :\n* int and float for numerical values.\n* object for text data.\n\nWe'll need to transform the text data in numerical values in order to feed our machine learning model.","1175d927":"### Converting all data in number\n\nWe'll first take care of the name column. We'll extract the title from the names. (REF)\n\nThe steps are :\n* Create a new column\n* Read each names\n* Check if any title exist in the name\n* Add the title in the new column\n\nWe'll need to do it on all our datasets, so we can use the fonction defined before.","427c50b5":"## Conclusion and trial over the test sample","e2f7c181":"* If we use the df_train_median","12da0f3d":"Then we compute the correlation","88eb4551":"## Trying differents hyperparameters (again)","8fae0d3e":"# Titanic's data analysis and machine learning\n## By J\u00e9r\u00e9my P. Schneider\n\nAs someone new in this field, I decided to take my first challenge with the Titanic dataset\n\n\nThis notebook will be split in different part :\n\n## Setting the environnement :\n\n### Libraries Import\n### Personnalized functions and tools\n### Data import\n\n## Data cleaning and analysis :\n### Data analysis\n### Handeling missing data\n### Converting all data in number\n\n\n## Test and train a model\n\n## Conclusion and trial over the test sample\n\n# Setting the environnement\n\n## Libraries Import","88b47371":"## With the KNeighborsClassifier [see documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)","7d701570":"* If we use the df_train_mean","50cb7918":"### Checking correlation\n\nWe first remplace __Sex__ and __Embarked__ with numerical values based on the occurencies on the train data. \n\nSince we don't want to have biais we won't look too deep in the test data.","483f452b":"* Compute MEAN for numerical, and most occurent for categorical\n\nFor Embarked we take at look at the data"}}