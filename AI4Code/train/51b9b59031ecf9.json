{"cell_type":{"f75ba1bc":"code","fc063acf":"code","e1278de5":"code","60f40459":"code","02b9145b":"code","12accd9e":"code","b9d518f7":"code","af7daf54":"code","1695a0e8":"code","37ef6aa9":"code","41312d16":"code","c720d691":"code","90832681":"code","6a976f1f":"code","cbe083ea":"code","c49b7027":"code","77561b7b":"code","723b69a3":"code","0dd74666":"code","f317ea4e":"code","f4200962":"code","0514ca96":"code","72793884":"code","f2958de0":"code","52a2fe3e":"code","c64ec6fa":"code","8aea0f93":"code","a666fd2b":"code","212f21c7":"code","54db4fdb":"code","804516ba":"code","f68c4414":"code","0dd98e53":"code","abc22334":"code","69ddddc4":"code","0c9ab6d8":"code","7f626717":"code","f3ffdafb":"code","a54cf9dc":"code","41a229ee":"code","105982c6":"code","0f5c24fb":"code","1fd6ab38":"code","adf2ebeb":"code","513905fe":"code","235c7c44":"code","3b69d38e":"code","5f10904e":"code","391c34ce":"code","e4153b3c":"code","c2e886c0":"code","ee6596b5":"code","d0c8f04c":"code","650b6edd":"code","9a5020d1":"code","7e6edd61":"code","114c70d4":"code","202a151f":"code","da6e6e0a":"code","244834aa":"code","2ee313a9":"code","ca1dfc6b":"code","bc4c70de":"code","be791689":"code","95176c91":"code","6b8c48bb":"code","b713fbf8":"code","faec45f5":"code","ef3fcf1f":"code","dcb908f9":"code","f460d473":"code","edd91a09":"code","46f55b86":"code","45dd22cf":"code","6dbf9a5c":"code","b791cd1d":"code","50e16904":"code","61d0da29":"code","d0b4058d":"code","b534d01d":"code","f36b92ba":"code","d633b92c":"code","4787d117":"code","11bc621f":"code","18ed8d37":"code","d88577bd":"code","b9b7c138":"code","47116100":"code","af8a0ee9":"code","205a7e4a":"code","375a4504":"code","3d1f4f51":"code","b89e8bdd":"code","b2d880fb":"code","bba169bd":"code","f3e216a5":"code","83df5b4f":"code","b0782810":"code","6ff8fb02":"code","6bc9ba70":"code","376714a1":"markdown","6935917a":"markdown","6b7880e1":"markdown","66750b06":"markdown","843ab39b":"markdown","72f82c20":"markdown","698d11f7":"markdown","72c1c3bb":"markdown","4528777b":"markdown","1bf77b73":"markdown","173ea94f":"markdown","0979b2e1":"markdown","09dfa618":"markdown","c7b874a1":"markdown","dc89f53f":"markdown","0f41b7d9":"markdown","09a7f1d7":"markdown","7be98dc4":"markdown","ec20bcc6":"markdown","de59c713":"markdown","8a65306f":"markdown","31e7933a":"markdown","57421754":"markdown","c7fb99cb":"markdown","7febc0be":"markdown","f3c32d54":"markdown","f615f094":"markdown","b5d5023b":"markdown"},"source":{"f75ba1bc":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\nimport re\nimport tensorflow_addons as tfa # For LAMB Optimizer\nimport transformers\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom transformers import (AutoTokenizer, BertTokenizer, TFBertForSequenceClassification, BertModel)\nfrom scipy.spatial.distance import cosine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics.pairwise import cosine_similarity as cos","fc063acf":"train = pd.read_csv('..\/input\/muis-challenge\/train.csv')\ntest = pd.read_csv('..\/input\/muis-challenge\/test.csv')\nsample_sub = pd.read_csv('..\/input\/muis-challenge\/submission.csv')\nsynset = pd.read_csv('..\/input\/muis-challenge\/synset_meaning.csv')\ndf = pd.read_csv('..\/input\/muis-challenge\/wordlist.csv')","e1278de5":"class TrieNode():\n    def __init__(self):\n        self.children = defaultdict()\n        self.terminating = False\n\n\nclass Trie():\n    def __init__(self):\n        self.root = self.get_node()\n\n    def get_node(self):\n        return TrieNode()\n\n    def get_index(self, ch):\n        return ord(ch) - ord('a')\n\n    def insert(self, word):\n\n        root = self.root\n        len1 = len(word)\n\n        for i in range(len1):\n            index = self.get_index(word[i])\n\n            if index not in root.children:\n                root.children[index] = self.get_node()\n            root = root.children.get(index)\n\n        root.terminating = True\n\n    def search(self, word):\n        root = self.root\n        len1 = len(word)\n\n        for i in range(len1):\n            index = self.get_index(word[i])\n            if not root:\n                return False\n            root = root.children.get(index)\n\n        return True if root and root.terminating else False\n\n    def has_child(self, word, ch):\n        root = self.root\n        len1 = len(word)\n        assert len(ch) == 1\n        for i in range(len1):\n            index = self.get_index(word[i])\n            if not root:\n                return False\n            root = root.children.get(index)\n        if not root:\n            return False\n        else:\n            index = self.get_index(ch)\n            root = root.children.get(index)\n            if not root:\n                return False\n            else:\n                return True\n    def delete(self, word):\n\n        root = self.root\n        len1 = len(word)\n\n        for i in range(len1):\n            index = self.get_index(word[i])\n\n            if not root:\n                print (\"Word not found\")\n                return -1\n            root = root.children.get(index)\n\n        if not root:\n            print (\"Word not found\")\n            return -1\n        else:\n            root.terminating = False\n            return 0\n\n    def update(self, old_word, new_word):\n        val = self.delete(old_word)\n        if val == 0:\n            self.insert(new_word)","60f40459":"class Processor():\n    def __init__(self, vocabulary, ending_vocab):\n        ftrie = Trie()\n        etrie = Trie()\n        for word in vocabulary:\n            ftrie.insert(word)\n        self.ftrie = ftrie\n        for ending in ending_vocab:\n            etrie.insert(ending[::-1])\n        self.etrie = etrie\n\n    def extract_root(self, word):\n        valid_ending = False\n        reverse = word[::-1]\n        for ending in [reverse[:1], reverse[:2], reverse[:3]]:\n            if self.etrie.search(ending):\n                valid_ending = True\n        if not valid_ending:\n            return word\n        else:\n            i = 0\n            while i < len(word) - 1 and self.ftrie.has_child(word[:i], word[i]):\n                i+=1\n            if self.ftrie.search(word[:i]) and i != 0:\n                return word[:i]\n            else:\n                return word","02b9145b":"nuhtsul = ['\u044b','\u044b\u043d','\u0438\u0439\u043d','\u0434','\u0442','\u044b\u0433','\u0438\u0439\u0433','\u0433','\u0430\u0430\u0441','\u044d\u044d\u0441','\u043e\u043e\u0441','\u04e9\u04e9\u0441','\u0430\u0430\u0440','\u044d\u044d\u0440','\u04e9\u04e9\u0440','\u043e\u043e\u0440','\u0442\u0430\u0439','\u0442\u044d\u0439',\n'\u0442\u043e\u0439','\u0440\u04af\u04af','\u0440\u0443\u0443','\u0430\u0430','\u044d\u044d','\u04e9\u04e9','\u043e\u043e']","12accd9e":"df = pd.read_csv('\/kaggle\/input\/muis-challenge\/wordlist.csv')\nvocabulary = df['word']\nvocabulary = [word.strip() for word in vocabulary if word != '']","b9d518f7":"p = Processor(vocabulary, nuhtsul)","af7daf54":"train_merged = pd.merge(train,synset,how='left',on='synset_id')","1695a0e8":"def get_train_words(x):\n    x = re.sub(\",:;\",\" \",x)\n    words_list = x.split(\" \")\n    for idx,word in enumerate(words_list):\n        if \"#\" in word:\n            if word.split(\"#\")[0] == \" \" or word.split(\"#\")[0] == \"\":\n                target_word = words_list[idx-1]\n            else:\n                target_word = word.split(\"#\")[0]\n\n    #target_word = target_word.lower()\n    target_word = target_word.strip()\n    target_word = re.sub(\"[!@#$%^&*;:,.\/<>?\\|`_+]\",\"\",target_word)\n    target_word = re.sub(r\"[\\([{})\\]]\",\"\",target_word)\n    \n    return target_word","37ef6aa9":"train_merged['train_word'] = train_merged['text'].apply(lambda x: get_train_words(x))\ntrain_merged['train_word_mod'] = train_merged['train_word']\ntrain_merged['len'] = train_merged['train_word_mod'].apply(lambda x: len(x))","41312d16":"tokenizer = transformers.AutoTokenizer.from_pretrained(\n            \"tugstugi\/bert-base-mongolian-cased\", do_lower_case=False)","c720d691":"clause_list = ['\u044b','\u044b\u043d','\u0438\u0439\u043d','\u044b\u0433','\u0438\u0439\u0433','\u0430\u0430\u0441','\u044d\u044d\u0441','\u043e\u043e\u0441','\u04e9\u04e9\u0441','\u0430\u0430\u0440','\u044d\u044d\u0440','\u04e9\u04e9\u0440','\u043e\u043e\u0440','\u0442\u0430\u0439','\u0442\u044d\u0439',\n'\u0442\u043e\u0439','\u0440\u04af\u04af','\u0440\u0443\u0443','\u0430\u0430','\u044d\u044d','\u04e9\u04e9','\u043e\u043e']","90832681":"def clean_word(x,tokenizer,clause_list:list):\n    tokenized = tokenizer.tokenize(x['train_word_mod'])\n    word = ''\n    word1 = ''\n    if len(tokenized) > 1:\n        #tmp_list = []\n        for idx, subword in enumerate(tokenized):\n            if idx == 0:\n                word += subword[1:]\n            elif idx ==1:\n                if subword.find('\u044c') == 0 or subword.find('\u044a') == 0:\n                        word+=subword\n                elif len(subword) == 1:\n                        word+=subword\n                elif subword == '\u0430\u043c':\n                        word1+=subword\n                else:\n                        word1+=subword\n            else:\n                word1 += subword\n    else:\n        try:\n            return tokenized[0][1:]\n        except:\n            return 0\n    if word1 != '':\n        return word + ' ' + word1\n    else:\n        return word\n                            \n                        \ndef modify_sentence(x):\n    try:\n        return re.sub(x['train_word'],x['train_word_mod'],x['text'])\n    except:\n        return 0\n    \n    \ndef modify_word(x):\n    if x.find(\" \") != -1:\n        try:\n            return x.split()[1]\n        except IndexError:\n            return x.split()\n    else:\n        return  x\n    \n                \n        \n","6a976f1f":"train_merged0 = train_merged.query(\"len <= 8\")\ntrain_merged1 = train_merged.query(\"len > 8\")","cbe083ea":"train_merged1['train_word_mod'] = train_merged1.apply(lambda x: clean_word(x,tokenizer,clause_list),axis=1)\ntrain_merged1['text'] = train_merged1.apply(lambda x: modify_sentence(x),axis=1)\n","c49b7027":"train_merged1.query(\"word == '\u0442\u04e9\u0440'\")","77561b7b":"train_merged = train_merged0.append(train_merged1,ignore_index=True)","723b69a3":"train_merged['train_word_mod'] = train_merged['train_word_mod'].apply(lambda x:modify_word(x))\ntrain_merged['train_word_mod'] = train_merged['train_word_mod'].apply(lambda x: p.extract_root(x))\n","0dd74666":"train_merged.query(\"train_word_mod == 0\")","f317ea4e":"train_merged","f4200962":"def get_remove_htag(x):\n    new_list = []\n    for word in x.split(\" \"):\n        if len(re.findall(\"[#]\",word)) >0:\n            pass\n            #new_list.append(word.split(\"#\")[0])\n        else:\n            new_list.append(word)\n    return \" \".join(new_list)\n\n\ndef get_sentence_bert(x):\n    new_list = []\n    for word in x.split(\" \"):\n        if len(re.findall(\"[#]\",word)) >0:\n            #pass\n            new_list.append(\"[TARGET]\")\n            new_list.append(word.split(\"#\")[0])\n            new_list.append(\"[TARGET]\")\n        else:\n            word = re.sub(\"[!@#$%^&*;:,.\/<>?\\|`_+]\",\"\",word)\n            new_list.append(word)\n    return \" \".join(new_list)    \n\ndef modify_meaning(x):\n    tmp_list = []\n    for idx,char in enumerate(x['word']):\n        if idx == 0:\n            tmp_list.append(char)\n        else:\n            tmp_list.append(char)\n    \n    target_word = \"\".join(tmp_list)\n        \n    return f\"{target_word}: \" + x['meaning']","0514ca96":"train_merged['train_sentence'] = train_merged['text'].apply(lambda x: get_remove_htag(x))","72793884":"train_merged['train_sentence'] = train_merged['text'].apply(lambda x: get_remove_htag(x))\ntrain_merged['bert_sentence'] = train_merged['text'].apply(lambda x: get_sentence_bert(x))\ntrain_merged['mod_meaning'] = train_merged.apply(lambda x: modify_meaning(x),axis=1)","f2958de0":"train_merged.head(2)","52a2fe3e":"def lesk_array(x,synset_meaning:pd.DataFrame,extract_root_processor):\n\n    \"\"\" This is simple implementation of lesk algorithm.\n        \n        You can extend this algortihm by including nearest word weights and POS tags.\n        \n        Args: sentence - str \n              target_word - str\n              synsets - list\n    \"\"\"\n    \n    overlap_synset_dic ={}\n    keys = synset_meaning.query(f\" word == '{x['word']}'\")['synset_id'].values\n    \n    for key in keys:\n        \n        tmp_meaning = []\n        overlap_cnt = 0\n\n        for word in synset_meaning.query(f\"synset_id == {key}\")['meaning']:\n            \n            word = word.lower()\n            word = re.sub(\"[!@#$%^&*;:,.\/<>?\\|`_+]\",\"\",word)\n            word = extract_root_processor.extract_root(word)\n            tmp_meaning.append(word)\n\n        for t_word in x['train_sentence']:\n\n            t_word = t_word.lower()\n            t_word = re.sub(\"[!@#$%^&*;:,.\/<>?\\|`_+]\",\"\",t_word)\n            t_word = extract_root_processor.extract_root(t_word)\n            overlap_cnt += tmp_meaning.count(t_word)\n        \n        overlap_synset_dic[key] = overlap_cnt\n        \n    return list(overlap_synset_dic.keys())[list(overlap_synset_dic.values()).index(max(overlap_synset_dic.values()))]\n        ","c64ec6fa":"train_merged['lesk'] = train_merged.apply(lambda x: lesk_array(x,synset,p),axis=1)","8aea0f93":"train_merged['len'] = train_merged['train_word_mod'].apply(lambda x: len(x))\ntrain_merged = train_merged.query(\"len<=9\")\n#train_merged0 = train_merged.query(\"len <= 8\")\n#train_merged1 = train_merged.query(\"len > 8\")","a666fd2b":"oov_token = '<UNK>'\npad_type = 'post'\ntrunc_type = 'post'\nt = tf.keras.preprocessing.text.Tokenizer(num_words=35,char_level=True,oov_token=oov_token)\n#t.fit_on_texts(list(texts['text'].values))\n#t.fit_on_texts((train['content'].append(val['content'])).values)","212f21c7":"t.fit_on_texts(list(train_merged['train_word_mod'].values))","54db4fdb":"train_merged['word_len'] = train_merged['word'].apply(lambda x: len(x))\ntrain_merged['train_word_len'] = train_merged['train_word_mod'].apply(lambda x: len(x))","804516ba":"target_dic = {word : num for num, word in enumerate(synset.drop_duplicates(subset='word')['word'].values)}\ntrain_merged['target'] = train_merged['word'].apply(lambda x: target_dic[x])","f68c4414":"X_train, X_test, y_train, y_test = train_test_split(train_merged['train_word_mod'], train_merged['target'],random_state=42,test_size=0.1)","0dd98e53":"encoded_train = t.texts_to_sequences(X_train.values)\nencoded_test = t.texts_to_sequences(X_test.values)\n\nx_train1 = tf.keras.preprocessing.sequence.pad_sequences(encoded_train,padding=pad_type,truncating=trunc_type, maxlen=max(max(train_merged['word_len']),max(train_merged['train_word_len'])))\nx_test1 = tf.keras.preprocessing.sequence.pad_sequences(encoded_test, padding=pad_type,truncating=trunc_type, maxlen=max(max(train_merged['word_len']),max(train_merged['train_word_len'])))","abc22334":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n    \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'att': self.att,\n            'ffn': self.ffn,\n            'layernorm1' : self.layernorm1,\n            'layernorm2' : self.layernorm2,\n            'dropout1' : self.dropout1,\n            'dropout2' : self.dropout2\n        })\n        return config\n\n\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions\n    \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'token_emb': self.token_emb,\n            'pos_emb': self.pos_emb\n        })\n        return config\n\n","69ddddc4":"vocab_size = len(t.word_index)\nmax_len = max(max(train_merged['word_len']),max(train_merged['train_word_len']))","0c9ab6d8":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy(tpu) \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","7f626717":"with strategy.scope():\n#with tf.device('\/gpu:0'):\n  vocab_size=vocab_size+1\n  maxlen = max_len\n  embed_dim = 50\n  num_heads = 12 # Number of attention heads\n  ff_dim = embed_dim *4  # Hidden layer size in feed forward network inside transformer\n\n  inputs = layers.Input(shape=(maxlen,))\n  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n  embedding_layer1 = keras.layers.Embedding(vocab_size,embed_dim)\n  x = embedding_layer1(inputs)\n  transformer_block =  TransformerBlock(embed_dim, num_heads, ff_dim,rate=0.2)\n  x = transformer_block(x)\n  transformer_block1 = TransformerBlock(embed_dim,num_heads,ff_dim,rate=0.2)\n  x = transformer_block1(x)\n  #transformer_block2 = TransformerBlock(embed_dim,num_heads,ff_dim,rate=0.3)\n  #x = transformer_block2(x)\n  x = layers.SpatialDropout1D(0.2)(x)\n  x = layers.Bidirectional(layers.LSTM(256,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))(x)\n  x = layers.SpatialDropout1D(0.2)(x)\n  x = layers.Bidirectional(layers.LSTM(256,dropout=0.2,recurrent_dropout=0.2,return_sequences=False))(x)\n  #x = layers.SpatialDropout1D(0.2)(x)\n  #x = layers.Bidirectional(layers.LSTM(256,dropout=0.2,recurrent_dropout=0.2))(x)\n  #x = layers.SpatialDropout1D(0.2)(x)\n  #x = layers.Bidirectional(layers.LSTM(256,dropout=0.2,recurrent_dropout=0.2))(x)\n  #x = layers.GlobalAveragePooling1D()(x)\n  x = layers.Dropout(0.2)(x)\n  x = layers.Dense(256, activation=\"relu\")(x)\n  x = layers.Dropout(0.2)(x)\n  #x = layers.Dense(128,activation='relu')(x)\n  #x = layers.Dropout(0.2)(x)\n  #x = layers.Dense(64,activation='relu')(x)\n  #x = layers.Dropout(0.2)(x)\n  #x = layers.Dense(32,activation='relu')(x)\n  #x = layers.Dropout(0.2)(x)\n  outputs = layers.Dense(len(train_merged['target'].unique()), activation=\"softmax\")(x)\n\n  adam = keras.optimizers.Adam(learning_rate=2e-5,\n                                epsilon=1e-08,\n                                beta_1 = 0.9,\n                                beta_2 = 0.98)\n  lamb = tfa.optimizers.LAMB(learning_rate=0.00176)\n  word_model = keras.Model(inputs=inputs, outputs=outputs)\n  word_model.compile(optimizer=lamb, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])","f3ffdafb":"word_model.summary()","a54cf9dc":"EPOCHS = 100\ncallback = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\ncustom_early_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    patience=int(5), \n    min_delta=0.01, \n    mode='min',\n    restore_best_weights=True\n)\nrlr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=20, min_lr=0.00176)\n#logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n#tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\ncheckpoint =  keras.callbacks.ModelCheckpoint(filepath = \".\/{word_epoch:02d}.h5\",\n                save_best_only = True,\n                save_weights_only = True,\n                save_freq =  \"epoch\",\n                monitor = 'val_loss',\n                mode='min')\n\nhistory = word_model.fit(\n    x_train1, y_train, batch_size=32, epochs=EPOCHS, validation_data=(x_test1, y_test),callbacks=[custom_early_stopping,rlr]\n)","41a229ee":"word_model.evaluate(x_test1,y_test)","105982c6":"word_model.evaluate(x_train1,y_train)","0f5c24fb":"encoded_pred = t.texts_to_sequences(train_merged['train_word_mod'].values)\n\nx_pred = tf.keras.preprocessing.sequence.pad_sequences(encoded_pred,padding=pad_type,truncating=trunc_type, maxlen=max(max(train_merged['word_len']),max(train_merged['train_word_len'])))\n\n\npredictions = word_model.predict(x_pred)\nlist1 = []\nfor pred in predictions:\n    list1.append(np.argmax(pred))","1fd6ab38":"train_merged['preds'] = list1\ntrain_merged['pred_word'] = train_merged['preds'].apply(lambda x: list(target_dic.keys())[x])\n#test['lesk'] = test.apply(lambda x: lesk_pred(x,synset,p),axis=1)","adf2ebeb":"print(classification_report(train_merged['word'],train_merged['pred_word']))","513905fe":"tokenizer.tokenize(\"\u0442\u04e9\u0440\u04af\u04af\u043b\u044d\u0445\u04af\u0439\u0446\")","235c7c44":"train_merged.query(\"word != '\u0445\u044d\u043b\u0431\u044d\u0440' and pred_word == '\u0445\u044d\u043b\u0431\u044d\u0440'\")","3b69d38e":"tokenizer.tokenize('\u044d\u043c\u0441\u0438\u0439\u043d\u043d\u04af\u0434')","5f10904e":"def lesk_pred(x,synset_meaning:pd.DataFrame,extract_root_processor):\n\n    \"\"\" This is simple implementation of lesk algorithm \n        \n        Args: sentence - str \n              target_word - str\n              synsets - list\n    \"\"\"\n    \n    overlap_synset_dic ={}\n    keys = synset_meaning.query(f\" word == '{x['pred_word']}'\")['synset_id'].values\n    \n    for key in keys:\n        \n        tmp_meaning = []\n        overlap_cnt = 0\n\n        for word in synset_meaning.query(f\"synset_id == {key}\")['meaning']:\n            \n            word = word.lower()\n            word = re.sub(\"[!@#$%^&*;:,.\/<>?\\|`_+]\",\"\",word)\n            word = extract_root_processor.extract_root(word)\n            tmp_meaning.append(word)\n\n        for t_word in x['train_sentence']:\n\n            t_word = t_word.lower()\n            t_word = re.sub(\"[!@#$%^&*;:,.\/<>?\\|`_+]\",\"\",t_word)\n            t_word = extract_root_processor.extract_root(t_word)\n            overlap_cnt += tmp_meaning.count(t_word)\n        \n        overlap_synset_dic[key] = overlap_cnt\n        \n    return list(overlap_synset_dic.keys())[list(overlap_synset_dic.values()).index(max(overlap_synset_dic.values()))]\n        ","391c34ce":"test['preds'] = list1\ntest['pred_word'] = test['preds'].apply(lambda x: list(target_dic.keys())[x])\ntest['lesk'] = test.apply(lambda x: lesk_pred(x,synset,p),axis=1)","e4153b3c":"test['synset_id'] = test['lesk']","c2e886c0":"test[['text_id','synset_id']].to_csv('.\/submission_0824.csv',index=False)","ee6596b5":"def make_data(main_df:pd.DataFrame,synset:pd.DataFrame):\n    data=[]\n    for index,row in tqdm(main_df.iterrows()):\n        synsets = list(synset.query(f\"word == '{row['word']}'\")['synset_id'].values)\n        for key in synsets:\n            if key == row['synset_id']:\n                data.append([row['text_id'],row['synset_id'],key,row['word'],\n                               row['bert_sentence'],synset.query(f\"synset_id=={key}\")['meaning'].values[0],\n                               1])\n            else:\n                data.append([row['text_id'],row['synset_id'],key,row['word'],\n                               row['bert_sentence'],synset.query(f\"synset_id=={key}\")['meaning'].values[0],\n                               0])\n    new_df = pd.DataFrame(data,columns=['text_id','synset_id','other_synset_id',\n                                        'word','bert_sentence','meaning','similarity_y_n'])\n    return new_df\n        ","d0c8f04c":"new_df = make_data(train_merged,synset)","650b6edd":"new_df['similarity_y_n'].value_counts()","9a5020d1":"#new_df1 = new_df.query(\"similarity_y_n == 0\").sample(n=12000,random_state=42).append(new_df.query(\"similarity_y_n==1\"))","7e6edd61":"def get_len(x):\n    words = x.strip(\" \")\n    return len(words)","114c70d4":"new_df['word_len'] = new_df['bert_sentence'].apply(lambda x: get_len(x))","202a151f":"new_df['word_len'].describe()","da6e6e0a":"max_length = 284  # Maximum length of input sentence to the model.\nbatch_size = 64","244834aa":"tokenizer = transformers.AutoTokenizer.from_pretrained(\n            \"tugstugi\/bert-base-mongolian-cased\", do_lower_case=False\n        )\n\n# Adding [TARGET] token for supervising training data\n\ntokenizer.add_tokens('[TARGET]')\n","2ee313a9":"def BertSiameseData(sentence,sentence1,labels,include_targets:bool,tokenizer):\n    \n    encoded = tokenizer.batch_encode_plus(\n            sentence.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )\n    encoded1 = tokenizer.batch_encode_plus(\n            sentence1.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )\n\n\n    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n    input_ids1 = np.array(encoded1[\"input_ids\"], dtype=\"int32\")\n    attention_masks1 = np.array(encoded1[\"attention_mask\"], dtype=\"int32\")\n    token_type_ids1 = np.array(encoded1[\"token_type_ids\"], dtype=\"int32\")\n\n   \n    if include_targets:\n\n        labels = np.array(labels, dtype=\"int32\")\n        return [input_ids, attention_masks, token_type_ids,\n                input_ids1, attention_masks1, token_type_ids1], labels\n    else:\n        return [input_ids, attention_masks, token_type_ids,\n                input_ids1, attention_masks1, token_type_ids1]\n","ca1dfc6b":"new_df['mod_meaning'] = new_df.apply(lambda x: modify_meaning(x),axis=1)","bc4c70de":"X_train, X_test, y_train, y_test = train_test_split(new_df, new_df['similarity_y_n'],random_state=42,test_size=0.1)","be791689":"with strategy.scope():\n#with tf.device('\/gpu:0'):\n\n    # First Input:\n    input_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n    )\n\n    attention_masks = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n    )\n    token_type_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n    )\n\n    # Second Input:\n    input_ids1 = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"input_ids1\"\n    )\n\n    attention_masks1 = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"attention_masks1\"\n    )\n    token_type_ids1 = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids1\"\n    )\n\n    \n    # Loading pretrained BERT model.\n    bert_model = transformers.TFBertModel.from_pretrained(\"tugstugi\/bert-base-mongolian-uncased\")\n    \n    # Freeze the BERT model and resize embedding shape (because we added)\n    bert_model.resize_token_embeddings(len(tokenizer))\n    bert_model.trainable = False\n\n    bert_output = bert_model(\n        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n    )\n    sequence_output = bert_output.last_hidden_state\n    pooled_output = bert_output.pooler_output\n    \n    \n    bert_output1 = bert_model(\n        input_ids1, attention_mask=attention_masks1, token_type_ids=token_type_ids1\n    )\n    sequence_output1 = bert_output1.last_hidden_state\n    pooled_output1 = bert_output1.pooler_output\n    \n \n    bi_lstm = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(256,return_sequences=True))\n    #spt_dropout = tf.keras.layers.SpatialDropout2D(0.2)(bi_lstm)\n    #bi_lstm1 = tf.keras.layers.Bidirectional(\n    #    tf.keras.layers.LSTM(256, activation='tanh' ,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\n    \n    out_bi_lstm = bi_lstm(sequence_output)\n    out1_bi_lstm = bi_lstm(sequence_output1)\n    \n    #out_bi_lstm1 = bi_lstm1(out_bi_lstm)\n    #out1_bi_lstm1 = bi_lstm1(out1_bi_lstm)\n    \n    # Applying hybrid pooling approach to bi_lstm sequence output.\n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(out_bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(out_bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    \n    # Applying hybrid pooling approach to bi_lstm sequence output\n    avg_pool1 = tf.keras.layers.GlobalAveragePooling1D()(out1_bi_lstm)\n    max_pool1 = tf.keras.layers.GlobalMaxPooling1D()(out1_bi_lstm)\n    concat1 = tf.keras.layers.concatenate([avg_pool1, max_pool1])\n    \n    #dense = tf.keras.layers.Dense(128,activation='tanh')\n    #dense1 = tf.keras.layers.Dense(128,activation='tanh')\n    \n    #dense_out = dense(concat)\n    #dense_out1 = dense1(concat1)\n    \n    # You can apply custom distance layer such as (Cosine Similarity, Eucladian Distance, Manhattan Distance)\n    cos_layer = tf.keras.layers.Dot(axes=1,normalize=True,name='cosine_layer')\n    output = cos_layer([concat,concat1])\n    \n    model = tf.keras.models.Model(\n        inputs=[input_ids, attention_masks, token_type_ids,\n                input_ids1, attention_masks1, token_type_ids1], outputs=output\n    )\n\n    adam = tf.keras.optimizers.Adam()\n    model.compile(\n        optimizer=adam,\n        loss=\"mean_squared_error\",\n        metrics=[\"accuracy\"],\n    )\n\n\n#print(f\"Strategy: {strategy}\")\nmodel.summary()","95176c91":"train_data = BertSiameseData(\n    sentence=X_train[\"bert_sentence\"].values.astype(\"str\"),\n    sentence1=X_train['mod_meaning'].values.astype('str'),\n    labels=y_train,\n    include_targets=True,\n    tokenizer=tokenizer\n)\nvalid_data = BertSiameseData(\n    sentence=X_test[\"bert_sentence\"].values.astype(\"str\"),\n    sentence1=X_test['mod_meaning'].values.astype('str'),\n    labels=y_test,\n    include_targets=True,\n    tokenizer=tokenizer\n)","6b8c48bb":"EPOCHS = 15\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\ncustom_early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    patience=int(2), \n    min_delta=0.008, \n    mode='min',\n    restore_best_weights=True\n)\n\nhistory = model.fit(\n    train_data[0],\n    train_data[1],\n    epochs=EPOCHS, \n    batch_size=32,\n    validation_data=(valid_data[0],valid_data[1]),\n    use_multiprocessing=True,\n    workers=-1,\n    callbacks=[custom_early_stopping]\n)","b713fbf8":"model.evaluate(valid_data[0],valid_data[1])","faec45f5":"bert_model.trainable = True\n# Recompile the model to make the change effective.\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(0.001),\n    loss=\"mean_squared_error\",\n    metrics=[\"accuracy\"],\n)\nmodel.summary()","ef3fcf1f":"EPOCHS = 6\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\ncustom_early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    patience=int(3), \n    min_delta=0.008, \n    mode='min',\n    restore_best_weights=True\n)\nrlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=20, min_lr=0.1e-5)\n#logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n#tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\ncheckpoint =  tf.keras.callbacks.ModelCheckpoint(filepath = \".\/{epoch:02d}.h5\",\n                save_best_only = True,\n                save_weights_only = True,\n                save_freq =  \"epoch\",\n                monitor = 'val_loss',\n                mode='min')\n\nhistory = model.fit(\n    train_data[0],\n    train_data[1],\n    epochs=EPOCHS, \n    batch_size=32,\n    validation_data=(valid_data[0],valid_data[1]),\n    use_multiprocessing=True,\n    workers=-1,\n    callbacks=[rlr]\n)","dcb908f9":"model.evaluate(valid_data[0],valid_data[1])","f460d473":"def BertSemanticData(sentence_pairs,labels,include_targets:bool,tokenizer):\n    \"\"\"Generates batches of data.\n\n    Args:\n        sentence_pairs: Array of premise and hypothesis input sentences.\n        labels: Array of labels.\n        batch_size: Integer batch size.\n        shuffle: boolean, whether to shuffle the data.\n        include_targets: boolean, whether to incude the labels.\n\n    Returns:\n        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n        (or just `[input_ids, attention_mask, `token_type_ids]`\n         if `include_targets=False`)\n    \"\"\"\n\n\n\n        \n        # Retrieves the batch of index.\n        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token.\n    encoded = tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )\n\n        # Convert batch of encoded features to numpy array.\n    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n        # Set to true if data generator is used for training\/validation.\n    if include_targets:\n\n        labels = np.array(labels, dtype=\"int32\")\n        return [input_ids, attention_masks, token_type_ids], labels\n    else:\n        return [input_ids, attention_masks, token_type_ids]\n","edd91a09":"X_train, X_test, y_train, y_test = train_test_split(new_df, new_df['similarity_y_n'],random_state=42,test_size=0.2)","46f55b86":"y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\ny_test = tf.keras.utils.to_categorical(y_test, num_classes=2)","45dd22cf":"#strategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n#with tf.device('\/gpu:0'):\n    # Encoded token ids from BERT tokenizer.\n    input_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n    )\n    # Attention masks indicates to the model which tokens should be attended to.\n    attention_masks = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n    )\n    # Token type ids are binary masks identifying different sequences in the model.\n    token_type_ids = tf.keras.layers.Input(\n        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n    )\n    # Loading pretrained BERT model.\n    bert_model_class = transformers.TFBertModel.from_pretrained(\"tugstugi\/bert-base-mongolian-cased\")\n    # Freeze the BERT model to reuse the pretrained features without modifying them.\n    bert_model_class.resize_token_embeddings(len(tokenizer))\n    bert_model_class.trainable = False\n\n    bert_output = bert_model_class(\n        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n    )\n    sequence_output = bert_output.last_hidden_state\n    pooled_output = bert_output.pooler_output\n    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n    bi_lstm = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(256, return_sequences=True))(sequence_output)\n    spt_dropout = tf.keras.layers.SpatialDropout1D(0.3)(bi_lstm)\n    bi_lstm1 = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(256, return_sequences=True))(spt_dropout)\n    # Applying hybrid pooling approach to bi_lstm sequence output.\n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm1)\n    #max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm1)\n    #concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(avg_pool)\n    dense2 = tf.keras.layers.Dense(256,activation='gelu')(dropout)\n    dropout2 = tf.keras.layers.Dropout(0.2)(dense2)\n    output = tf.keras.layers.Dense(2, activation=\"softmax\")(dropout2)\n    class_model = tf.keras.models.Model(\n        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n    )\n\n    adam = tf.keras.optimizers.Adam()\n    class_model.compile(\n        optimizer=adam,\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n\n\n#print(f\"Strategy: {strategy}\")\nclass_model.summary()","6dbf9a5c":"train_data = BertSemanticData(\n    sentence_pairs=X_train[[\"bert_sentence\", \"mod_meaning\"]].values.astype(\"str\"),\n    labels=y_train,\n    include_targets=True,\n    tokenizer=tokenizer\n)\nvalid_data = BertSemanticData(\n    sentence_pairs=X_test[[\"bert_sentence\", \"mod_meaning\"]].values.astype(\"str\"),\n    labels=y_test,\n    include_targets=True,\n    tokenizer=tokenizer\n)","b791cd1d":"EPOCHS = 25\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\ncustom_early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    patience=int(15), \n    min_delta=0.007, \n    mode='min',\n    restore_best_weights=True\n)\nrlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=20, min_lr=0.1e-5)\n#logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n#tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\ncheckpoint =  tf.keras.callbacks.ModelCheckpoint(filepath = \".\/{epoch:02d}.h5\",\n                save_best_only = True,\n                save_weights_only = True,\n                save_freq =  \"epoch\",\n                monitor = 'val_loss',\n                mode='min')\n\nhistory = class_model.fit(\n    train_data[0],\n    train_data[1],\n    epochs=EPOCHS, \n    batch_size=32,\n    validation_data=(valid_data[0],valid_data[1]),\n    use_multiprocessing=True,\n    workers=-1,\n    callbacks=[custom_early_stopping,rlr]\n)","50e16904":"bert_model_class.trainable = True\n# Recompile the model to make the change effective.\nclass_model.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nclass_model.summary()","61d0da29":"EPOCHS = 15\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n\ncustom_early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    patience=int(6), \n    min_delta=0.01, \n    mode='min',\n    restore_best_weights=True\n)\nrlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=3, min_lr=0.1e-5)\n#logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n#tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\ncheckpoint =  tf.keras.callbacks.ModelCheckpoint(filepath = \".\/{epoch:02d}.h5\",\n                save_best_only = True,\n                save_weights_only = True,\n                save_freq =  \"epoch\",\n                monitor = 'val_loss',\n                mode='min')\n\nhistory = class_model.fit(\n    train_data[0],\n    train_data[1],\n    epochs=EPOCHS, \n    batch_size=32,\n    validation_data=(valid_data[0],valid_data[1]),\n    use_multiprocessing=True,\n    workers=1,\n    callbacks=[rlr]\n)","d0b4058d":"class_model.evaluate(valid_data[0],valid_data[1])","b534d01d":"tokenizer = transformers.AutoTokenizer.from_pretrained(\n            \"tugstugi\/bert-base-mongolian-uncased\", do_lower_case=False)","f36b92ba":"def get_cosine_sim(x):\n    \n    encoded = tokenizer(x['bert_sentence'],\n                        add_special_tokens=True,\n                        return_attention_mask=True,\n                        return_token_type_ids=True,\n                        pad_to_max_length=True,\n                        return_tensors=\"tf\",\n                        )\n    \n    encoded1 = tokenizer(x['meaning'],\n                        add_special_tokens=True,\n                        return_attention_mask=True,\n                        return_token_type_ids=True,\n                        pad_to_max_length=True,\n                        return_tensors=\"tf\",\n                        )\n    \n    output = bert_model(encoded)\n    output1 = bert_model(encoded1)\n    \n    return cos(np.array(output[0][0][0]).reshape(1,-1),np.array(output1[0][0][0]).reshape(1,-1))[0][0]","d633b92c":"get_cosine_sim(2,2)","4787d117":"get_cosine_sim(0,0)","11bc621f":"test = pd.read_csv('..\/input\/muis-challenge\/test.csv')","18ed8d37":"def fix_text(x):\n    word_list = x.split(\" \")\n    tmp_list = []\n    for idx,word in enumerate(word_list):\n        if \"#\" in word:\n            if word.split(\"#\")[0] == \"\" or word.split(\"#\")[0] == \" \":\n                tmp_list.pop(word_list.index(word_list[idx-1]))\n                tmp_list.append(word_list[idx-1]+word)\n            else:\n                tmp_list.append(word)\n        else:\n            tmp_list.append(word)\n    return \" \".join(tmp_list)\n     ","d88577bd":"test['text'] = test['text'].apply(lambda x: fix_text(x))\ntest['train_word'] = test['text'].apply(lambda x: get_train_words(x))\ntest['train_word_mod'] = test['train_word']\ntest['len'] = test['train_word_mod'].apply(lambda x: len(x))\ntest0 = test.query(\"len <=8\")\ntest1 = test.query(\"len>8\")\ntest1['train_word_mod'] = test1.apply(lambda x: clean_word(x,tokenizer,clause_list),axis=1)\ntest1['text'] = test1.apply(lambda x: modify_sentence(x),axis=1)\n#test['train_sentence'] = test['text'].apply(lambda x: get_remove_htag(x))\ntest = test0.append(test1,ignore_index=True)\ntest['train_word_mod'] = test['train_word_mod'].apply(lambda x:modify_word(x))\ntest['train_word_mod'] = test['train_word_mod'].apply(lambda x: p.extract_root(x))\n","b9b7c138":"test['bert_sentence'] = test['text'].apply(lambda x: get_sentence_bert(x))","47116100":"test.head(3)","af8a0ee9":"encoded_pred = t.texts_to_sequences(test['train_word_mod'].values)\n\nx_pred = tf.keras.preprocessing.sequence.pad_sequences(encoded_pred,padding=pad_type,truncating=trunc_type, maxlen=max(max(train_merged['word_len']),max(train_merged['train_word_len'])))\n","205a7e4a":"predictions = word_model.predict(x_pred)\nlist1 = []\nfor pred in predictions:\n    list1.append(np.argmax(pred))\n    \ntest['preds'] = list1\ntest['pred_word'] = test['preds'].apply(lambda x: list(target_dic.keys())[x])\n","375a4504":"def make_data_pred(main_df:pd.DataFrame,synset:pd.DataFrame):\n    data=[]\n    for index,row in tqdm(main_df.iterrows()):\n        synsets = list(synset.query(f\"word == '{row['pred_word']}'\")['synset_id'].values)\n        for key in synsets:\n                data.append([row['text_id'],key,row['pred_word'],\n                               row['bert_sentence'],synset.query(f\"synset_id=={key}\")['meaning'].values[0]])\n    new_df = pd.DataFrame(data,columns=['text_id','synset_id',\n                                        'pred_word','bert_sentence','meaning'])\n    return new_df\n        ","3d1f4f51":"test_df = make_data_pred(test,synset)","b89e8bdd":"def modify_meaning(x):\n    return f\"{x['pred_word']}: \" + x['meaning']","b2d880fb":"test_df['mod_meaning'] = test_df.apply(lambda x: modify_meaning(x),axis=1)","bba169bd":"test_data = BertSiameseData(\n    sentence=test_df[\"bert_sentence\"].values.astype(\"str\"),\n    sentence1=test_df['mod_meaning'].values.astype('str'),\n    labels=None,\n    tokenizer=tokenizer,\n    include_targets=False\n)","f3e216a5":"test_data = BertSemanticData(\n    test_df[[\"bert_sentence\", \"mod_meaning\"]].values.astype(\"str\"),\n    labels=None,  include_targets=False,tokenizer=tokenizer)\n","83df5b4f":"preds = class_model.predict(test_data,batch_size=64)","b0782810":"test_df['pred']=list(preds)\ntest_df['pred_value'] = test_df['pred'].apply(lambda x: max(x))\ntest_df['pred_men'] = test_df['pred'].apply(lambda x: np.argmax(x))","6ff8fb02":"def make_df(main_df:pd.DataFrame,test_main_df:pd.DataFrame):\n    data=pd.DataFrame()\n    for index,row in tqdm(test_main_df.iterrows()):\n        text_id = row['text_id']\n        le = main_df.query(f\"text_id =='{text_id}' and pred_men ==1\")\n        if len(le) > 1:\n            max1 = max(le['pred_value'].values)\n            app = le.query(f\"pred_value=={max1}\")\n            data = data.append(app)\n        elif len(le) == 1:\n            data = data.append(le)\n        elif len(le) == 0:\n            le = main_df.query(f\"text_id == '{text_id}' and pred_men == 0\")\n            min1 = min(le['pred_value'].values)\n            app = le.query(f\"pred_value=={min1}\")\n            data = data.append(app)\n    return data","6bc9ba70":"predicted = make_df(test_df,test)","376714a1":"## Get Sentences for Sentence Similarity","6935917a":"## Unuseful section","6b7880e1":"## Get training word for Char-Level Word Model","66750b06":"# First guessing the word model (Character Level - Attention Model)","843ab39b":"# Using Extract Root of Word","72f82c20":"\u041c\u0438\u043d\u0438\u0439 \u0445\u0443\u0432\u044c\u0434 \u0448\u0438\u0439\u0434\u043b\u0438\u0439\u0433 2 \u0448\u0430\u0442\u0442\u0430\u0439\u0433\u0430\u0430\u0440 \u0445\u0438\u0439\u0436 \u0431\u043e\u043b\u043d\u043e \u0433\u044d\u0436 \u0445\u0430\u0440\u0441\u0430\u043d. 1: \u0413\u043e\u043b \u04af\u0433\u0438\u0439\u0433 \u0442\u0430\u0430\u0445 (\u0430\u043c, \u0445\u044d\u043b\u0431\u044d\u0440 \u0433\u044d\u0445 \u043c\u044d\u0442) 2: \u0422\u0430\u0430\u0441\u0430\u043d \u04af\u0433\u043d\u04af\u04af\u0434 \u0434\u0443\u043d\u0434\u0430\u0430\u0441 \u0430\u0434\u0438\u043b\u0445\u0430\u043d \u0443\u0442\u0433\u0430\u0442\u0430\u0439\u0433 \u043d\u044c \u044f\u043b\u0433\u0430\u0445. \n\n\u0428\u0438\u0439\u0434\u043b\u04af\u04af\u0434\u0438\u0439\u043d \u0445\u0443\u0432\u044c\u0434:\n\n1. Character-Level \u043c\u043e\u0434\u0435\u043b \u0441\u0443\u0440\u0433\u0430\u043d \u0437\u04e9\u0432 \u04af\u0433\u0438\u0439\u0433 \u0442\u0430\u0430\u0445 \u0433\u044d\u0436 \u043e\u0440\u043e\u0434\u0441\u043e\u043d. (\u041c\u043e\u0434\u0435\u043b \u043d\u044c \u044d\u043d\u0433\u0438\u0439\u043d Transformer Block \u0431\u043e\u043b\u043e\u043d BiDirectional LSTM \u0430\u0448\u0438\u0433\u043b\u0430\u043d 28 \u04af\u0433 \u0434\u0443\u043d\u0434\u0430\u0430\u0441 classification \u0445\u0438\u0439\u0445 \u0437\u043e\u0440\u0438\u0443\u043b\u0430\u043b\u0442\u0442\u0430\u0439)\n2. BERT embeddings with Cosine Similarity. (\u04e8\u043c\u043d\u04e9 \u043d\u044c \u0441\u0443\u0440\u0433\u0430\u0441\u0430\u043d \u041c\u043e\u043d\u0433\u043e\u043b \u0445\u044d\u043b \u0434\u044d\u044d\u0440\u0445 \u043d\u044d\u044d\u043b\u0442\u0442\u044d\u0439 BERT \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u0441\u0430\u043d. \u0423\u0447\u0438\u0440 \u043d\u044c \u043c\u0438\u043d\u0438\u0439 \u0445\u0443\u0432\u044c\u0434 \u043e\u0434\u043e\u043e\u0433\u043e\u043e\u0440 \u041c\u043e\u043d\u0433\u043e\u043b \u0445\u044d\u043b\u043d\u0438\u0439 \u043d\u044d\u044d\u043b\u0442\u0442\u044d\u0439 \u0434\u0430\u0442\u0430 \u0434\u044d\u044d\u0440 \u0441\u0443\u0440\u0433\u0430\u0433\u0434\u0441\u0430\u043d \u0430\u0448\u0438\u0433\u043b\u0430\u0436 \u04af\u0440 \u0434\u04af\u043d\u0434 \u0445\u04af\u0440\u0447 \u0431\u043e\u043b\u043e\u0445\u0443\u0439\u0446 \u0437\u0430\u0433\u0432\u0430\u0440 \u043d\u044c BERT \u0431\u0430\u0439\u0441\u0430\u043d \u0433\u044d\u0436 \u0431\u043e\u0434\u043e\u0436 \u0431\u0430\u0439\u043d\u0430) \u0417\u0430\u0433\u0432\u0430\u0440\u044b\u043d \u0445\u0443\u0432\u044c\u0434 \u04e9\u0433\u04af\u04af\u043b\u0431\u044d\u0440 \u0442\u0443\u0441 \u0431\u04af\u0440 \u043d\u044c \u04e9\u0433\u04e9\u0433\u0434\u0441\u04e9\u043d target_word-\u043d \u043e\u043b\u043e\u043d\u043b\u043e\u0433 \u0434\u043e\u0442\u043e\u0440 \u043d\u044d\u0433 \u0443\u0442\u0433\u0430\u0442\u0430\u0439 \u043d\u044c \u0438\u0436\u0438\u043b \u0431\u0443\u0441\u0430\u0434\u0442\u0430\u0439 \u0438\u0436\u0438\u043b \u0431\u0443\u0441, \u0442\u0438\u0439\u043c \u0443\u0447\u0440\u0430\u0430\u0441 BERT embedding-\u0443\u0443\u0434\u044b\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u043d \u04e9\u0433\u04af\u04af\u043b\u0431\u044d\u0440\u04af\u04af\u0434\u0438\u0439\u043d \u0445\u043e\u043e\u0440\u043e\u043d\u0434\u044b\u043d \u0438\u0436\u0438\u043b \u0442\u04e9\u0441\u04e9\u04e9\u0442\u044d\u0439 \u0431\u0430\u0439\u0434\u043b\u044b\u0433 \u0442\u043e\u0433\u0442\u043e\u043e\u0436 \u0431\u043e\u043b\u043d\u043e \u0433\u044d\u0436 \u0431\u043e\u0434\u0441\u043e\u043d. \u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b\u043d \u0445\u0443\u0432\u044c\u0434 \u04e9\u0433\u04af\u04af\u043b\u0431\u044d\u04401, \u04e9\u0433\u04af\u04af\u043b\u0431\u044d\u04402 \u043d\u044c \u0442\u0443\u0441 \u0442\u0443\u0441\u0434\u0430\u0430 \u043e\u0440\u0436 \u0438\u0440\u044d\u044d\u0434 embedding \u0432\u0435\u043a\u0442\u043e\u0440\u0443\u0443\u0434\u044b\u043d \u0445\u043e\u043e\u0440\u043e\u043d\u0434\u044b\u043d \u0437\u0430\u0439\u0433 Cosine Similarity \u0430\u0448\u0438\u0433\u043b\u0430\u043d \u0433\u0430\u0440\u0433\u0430\u0445 \u0437\u043e\u0440\u0438\u043b\u0433\u043e\u0442\u043e\u0439 \u0431\u0430\u0439\u0441\u0430\u043d.   \n\n\u0411\u0443\u0441\u0430\u0434: \n\n1. Lesk Algorithm \u043d\u044c Word-Sense Disambiguation \u0442\u04e9\u0440\u043b\u0438\u0439\u043d \u0448\u0438\u0439\u0434\u043b\u04af\u04af\u0434 \u0434\u044d\u044d\u0440 \u0438\u0445 \u0445\u044d\u0440\u044d\u0433\u043b\u044d\u0433\u0434\u044d\u0436 \u0431\u0430\u0439\u0441\u0430\u043d \u0443\u0447\u0438\u0440 implementation \u0445\u0438\u0439\u0445 \u0433\u044d\u0436 \u043e\u0440\u043e\u043b\u0434\u0441\u043e\u043d. (Lesk Algorithm \u0445\u044d\u0441\u044d\u0433 \u0434\u044d\u044d\u0440 \u0431\u0430\u0439\u0433\u0430\u0430, Lesk-\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u043d submit \u0445\u0438\u0439\u0445\u044d\u0434 82%-\u0442\u043e\u0439 \u0431\u0430\u0439\u0441\u0430\u043d) \n2. \u0414\u0430\u0442\u0430 \u0446\u044d\u0432\u044d\u0440\u043b\u044d\u0433\u044d\u044d \u0431\u043e\u043b\u043e\u043d \u0431\u0443\u0441\u0430\u0434 \u0436\u0438\u0436\u0438\u0433 \u0437\u04af\u0439\u043b\u04af\u04af\u0434 \u0434\u044d\u044d\u0440 \u0431\u0443\u0440\u0443\u0443 \u0430\u0440\u0433\u0443\u0443\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u0436 \u0431\u0430\u0439\u0441\u0430\u043d.\n\n\u042d\u0446\u044d\u0441\u0442 \u043d\u044c \u044d\u043d\u044d \u043d\u04e9\u044d\u0442\u0431\u04af\u04af\u043a\u043d\u044d\u044d\u0441 \u0430\u0441\u0443\u0443\u0445 \u044d\u0441\u0432\u044d\u043b \u0437\u0430\u0441\u0432\u0430\u0440\u043b\u0430\u0436 \u0445\u044d\u043b\u044d\u0445 \u0437\u04af\u0439\u043b \u0431\u0430\u0439\u0432\u0430\u043b \u043d\u044d\u044d\u043b\u0442\u0442\u044d\u0439 \u0445\u043e\u043b\u0431\u043e\u0433\u0434\u043e\u043d\u043e \u0443\u0443 :) ","698d11f7":"# Read Data","72c1c3bb":"# Make Bert Compatible Input","4528777b":"# Lesk Algorithm Implementation","1bf77b73":"## Character Level Tokenizer","173ea94f":"## Wrote by Competition Owners","0979b2e1":"# Sentence Similarity Bert","09dfa618":"## Cleaning Anomaly Words","c7b874a1":"# Pred with Similarity","dc89f53f":"## 8-\u0441 \u0434\u044d\u044d\u0448 \u0443\u0440\u0442\u0442\u0430\u0439 \u04af\u0433\u043d\u04af\u04af\u0434 \u043d\u044c \u0438\u0445\u044d\u0432\u0447\u043b\u044d\u043d 2 \u04af\u0433 \u043d\u0438\u0439\u043b\u0441\u044d\u043d \u0443\u0447\u0438\u0440 \u0441\u0430\u043b\u0433\u0430\u0445 \u0433\u044d\u0436 \u043e\u0440\u043e\u043b\u0434\u0441\u043e\u043d. ","0f41b7d9":"# Pred with Lesk ","09a7f1d7":"# Cosine Similarity","7be98dc4":"# Fine-tune","ec20bcc6":"## Initialize TPU","de59c713":"# Fine-Tuning Cosine Similar BERT","8a65306f":"# Data Cleaning and Synset Merge","31e7933a":"# Simple Classification","57421754":"## Meta Data","c7fb99cb":"## Simple Transformer and Positional Embedding","7febc0be":"# Siamese Predict","f3c32d54":"# Simple Classification With Bert","f615f094":"# Cosine Similarity","b5d5023b":"## Calling Model"}}