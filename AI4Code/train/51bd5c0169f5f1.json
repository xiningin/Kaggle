{"cell_type":{"c4bc3be4":"code","f20bf447":"code","cfc7277f":"code","80d1f954":"code","b6be5d47":"code","49f3f9fb":"code","0a1437d0":"code","8f87b4aa":"code","e0b52e9d":"code","89d0ffe9":"code","f2749229":"code","2a360a64":"markdown","dcc654ee":"markdown","c502676d":"markdown","ca9afaa1":"markdown","7d449741":"markdown"},"source":{"c4bc3be4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f20bf447":"df_train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntrain_y = df_train['label']\ntrain_x = df_train.drop('label', axis = 1)\nx = np.array(train_x, dtype = 'float')\nx_reshape = [x_.reshape(28,28) for x_ in tqdm(x)]","cfc7277f":"#EDA\n#Image Visualization\nprint('Image Visualization')\nimport matplotlib.pyplot as plt\nrnum = np.random.randint(0, len(x_reshape) -1 , 10)\nplt.figure(figsize = (10,10))\nfor n, i in enumerate(rnum):\n    plt.subplot(1,10,n+1)\n    plt.tick_params(left = False, bottom = False, labelleft = False, labelbottom = False)\n    plt.imshow(x_reshape[i])\nplt.show()\n#label balance\nprint('label size')\nlabel_balance = df_train.groupby('label').size().to_frame().rename(columns = {0 : 'count'})\nplt.title('label balance')\nplt.bar(label_balance.index, label_balance['count'])\nplt.show()\nprint('****************************************************************************\\n')\n#Null value detection\ntrain_x.isnull().sum().loc[train_x.isnull().sum() != 0]","80d1f954":"#Custom Dataset\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as T\n\ntransforms = T.Compose([T.ToTensor(), T.Resize((224, 224))])\n\nclass CustomDataset(Dataset):\n    def __init__(self, x_data, y_data, transforms = transforms):\n        self.x_data = x_data\n        self.y_data = y_data\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.x_data)\n    def __getitem__(self, idx):\n        image = self.x_data[idx]\n        label = self.y_data.iloc[idx]\n        image = image \/ 255.0\n        sample = {'image' : image, 'label' : label}\n        if self.transforms:\n            sample['image'] = self.transforms(sample['image'])        \n        return sample","b6be5d47":"\n\n#MobileNet Define\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MobileNet(nn.Module):\n    def __init__(self, num_classes = 10):\n        super(MobileNet, self).__init__()\n        #Define \n        \n        def convdw(in_channels, out_channels, kernel_size, stride):\n            layers = []\n            #Depthwise Convolution\n            layers += [nn.Conv2d(in_channels, out_channels,kernel_size = 3,padding = 1, stride = stride, groups = in_channels)]\n            layers += [nn.BatchNorm2d(out_channels)]\n            layers += [nn.ReLU()]\n            \n            #Pointwise Convolution\n            layers += [nn.Conv2d(in_channels, out_channels,kernel_size =1,padding = 1, stride = stride)]\n            layers += [nn.BatchNorm2d(out_channels)]\n            layers += [nn.ReLU()]\n            return nn.Sequential(*layers)\n        \n        def convd(in_channels, out_channels, kernel_size, stride):\n            layers = []\n            layers += [nn.Conv2d(in_channels, out_channels, kernel_size, stride = stride)]\n            layers += [nn.BatchNorm2d(out_channels)]\n            layers += [nn.ReLU()]\n            return nn.Sequential(*layers)\n        \n        self.conv1 = convd(1,32,3, stride = 2)\n        self.convdw1 = convdw(32, 32, 3, 1)\n        self.conv2 = convd(32, 64, 1, stride = 1)\n        self.convdw2 = convdw(64, 64, 3, 2)\n        self.conv3 = convd(64, 128, 1, stride = 1)\n        self.convdw3 = convdw(128, 128, 3, 1)\n        self.conv4 = convd(128, 128, 1, stride = 1)\n        self.convdw4 = convdw(128, 128, 3, 2)\n        self.conv5 = convd(128, 256, 1, stride = 1)\n        self.convdw5 = convdw(256, 256, 3, 1)\n        self.conv6 = convd(256, 256, 1, stride = 1)\n        self.convdw6 = convdw(256, 256, 3, 2)\n        self.conv7 = convd(256, 512, 1, stride = 1)\n        #--------------------------------------------\n        #need x5\n        self.convdw = convdw(512, 512, 3, 1)\n        self.conv = convd(512, 512, 1, stride = 1)\n        #--------------------------------------------\n        self.convdw7 = convdw(512, 512, 3, 2)\n        self.conv8 = convd(512, 1024, 1, stride = 1)\n        self.convdw8 = convdw(1024, 1024, 3, 2)\n        self.conv9 = convd(1024, 1024, 1, stride = 1)\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(1024, num_classes)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.convdw1(x)\n        x = self.conv2(x)\n        x = self.convdw2(x)\n        x = self.conv3(x)\n        x = self.convdw3(x)\n        x = self.conv4(x)\n        x = self.convdw4(x)\n        x = self.conv5(x)\n        x = self.convdw5(x)\n        x = self.conv6(x)\n        x = self.convdw6(x)\n        x = self.conv7(x)\n            \n        for i in range(0,5):\n            x = self.convdw(x)\n            x = self.conv(x)\n                \n        x = self.convdw7(x)\n        x = self.conv8(x)\n        x = self.convdw8(x)\n        x = self.conv9(x)\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        output = self.fc(x)\n\n        return output","49f3f9fb":"#Training configure\n#basic options\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n# set seed\nimport random\nrandom.seed(777)\ntorch.manual_seed(777)\nif device == 'cuda':\n    torch.cuda.manual_seed_all(777)\n\n#Initialized weight -> not used\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n    elif isinstance(m, nn.Conv2d):\n        nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n\nmodel = MobileNet()\n\nlr = 1e-3\nsetting_patience = 7\nn_epoch = 50\nbatch_size = 100\n#optimizer & criterion function\ncriterion = nn.CrossEntropyLoss().cuda()\noptimizer = torch.optim.Adam(params = model.parameters(), lr = lr)\n#Define dataset\n#data split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_reshape, train_y, test_size = 0.33, random_state = 26)\ntrain_dataset = CustomDataset(X_train, y_train)\ntest_dataset = CustomDataset(X_test, y_test)\ntrain_data_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = False)\ntest_data_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\ntrain_total_batch = len(train_data_loader)\ntest_total_batch = len(test_data_loader)\ntrn_acc_list = []\ntrn_loss_list = []\ntest_acc_list = []\ntest_loss_list = []","0a1437d0":"#Training & Evaluation\ntorch.cuda.empty_cache()\nbest_accuracy = 0\ntotal_patience = 0\nfor epoch in range(n_epoch):\n    model.train()    \n    model.cuda()\n    optimizer.zero_grad()\n    trn_avg_loss = 0\n    trn_total = 0\n    trn_correct = 0\n    test_avg_loss = 0\n    test_total = 0\n    test_correct = 0\n    with tqdm(train_data_loader, unit = 'batch') as train_bar:\n        for sample in train_bar:  \n            image = sample['image'].float().cuda()\n            label = sample['label'].cuda()\n            trn_probs = model(image)\n            loss = criterion(trn_probs, label)\n            loss.backward()\n            optimizer.step()\n            trn_avg_loss += loss \/ train_total_batch\n            _, trn_predict = torch.max(trn_probs.data, 1)\n            trn_total += label.size(0)\n            trn_correct += (trn_predict == label).sum()\n            trn_accuracy = 100 * trn_correct \/ trn_total\n            train_bar.set_postfix(epoch = epoch+1, loss = loss.item(), accuracy = trn_accuracy.item())\n    model.eval()\n    with torch.no_grad():\n        with tqdm(test_data_loader, unit = 'batch') as test_bar:\n            for sample in test_bar:  \n                image = sample['image'].float().cuda()\n                label = sample['label'].cuda()\n                test_probs = model(image)\n                loss = criterion(test_probs, label)\n                test_avg_loss += loss \/ test_total_batch\n                _, test_predict = torch.max(test_probs.data, 1)\n                test_total += label.size(0)\n                test_correct += (test_predict == label).sum()\n                test_accuracy = 100 * test_correct \/ test_total\n                test_bar.set_postfix(epoch = epoch+1, loss = loss.item(), accuracy = test_accuracy.item())\n    trn_acc_list.append(trn_accuracy)\n    trn_loss_list.append(trn_avg_loss)\n    test_acc_list.append(test_accuracy)\n    test_loss_list.append(test_avg_loss)\n    \n    if total_patience == setting_patience:\n        break\n    else:\n        if best_accuracy < test_accuracy:\n            total_patience = 0\n            best_accuracy = test_accuracy\n            print('Model Improving')\n            print('Epoch : {}, Loss : {:.4f}, Accuracy : {} model save.....'.format(epoch+1, test_avg_loss, test_accuracy))\n            torch.save(model.state_dict(), '.\/checkpoint.pt')\n        else:\n            print('early stop counter : {}\/{}'.format(total_patience+1, setting_patience))\n            total_patience += 1","8f87b4aa":"import matplotlib.pyplot as plt\nplt.figure(figsize = (20,5))\nplt.subplot(1,2,1)\nplt.title('Loss')\nx = range(len(trn_loss_list))\nplt.plot(x, trn_loss_list, label = 'Train Loss')\nplt.plot(x, test_loss_list, label = 'Valid Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.title('Accuracy')\nx = range(len(trn_loss_list))\nplt.plot(x, trn_acc_list, label = 'Train Accuracy')\nplt.plot(x, test_acc_list, label = 'Valid Accuracy')\nplt.legend()\nplt.show()","e0b52e9d":"transforms = T.Compose([T.ToTensor(), T.Resize((224, 224))])\n\nclass CustomDataset_test(Dataset):\n    def __init__(self, x_data, transforms = transforms):\n        self.x_data = x_data\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.x_data)\n    def __getitem__(self, idx):\n        image = self.x_data[idx]\n        image = image \/ 255.0\n        if self.transforms:\n            image = self.transforms(image)        \n        return image","89d0ffe9":"import torch\n# Normal distribution initializing\ntorch.cuda.empty_cache()\nmodel.load_state_dict(torch.load('.\/checkpoint.pt'))\nmodel.cuda()\nmodel.eval()\ntest_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nx = np.array(test_df, dtype = 'float')\nx_reshape = [x_.reshape(28,28) for x_ in tqdm(x)]\ntest_dataset = CustomDataset_test(x_reshape)\ntest_data_loader = DataLoader(test_dataset, batch_size = 1, shuffle = False)\ny_hat_list = []\nfor image in tqdm(test_data_loader):\n    image = image.float().cuda()\n    y_hat = model(image)\n    _, predict = torch.max(y_hat.data, 1)\n    y_hat_list.append(predict.item())","f2749229":"submission = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\nsubmission['Label'] = y_hat_list\nsubmission.to_csv('submission.csv', index = False)","2a360a64":"#  **MobileNetV1 Structure**\n ![figure1](https:\/\/static-01.hindawi.com\/articles\/cin\/volume-2020\/8817849\/figures\/8817849.fig.002.svgz)\n","dcc654ee":"> **On network..**\n* **Define Depthwise Convolution Network (blue background on the picture)**\n* **Define Basic convolution**","c502676d":"# **Load Dataset**\n\nDue to given dataset shape (Dataframe & 1 dimension shape), it needs to be reshaped to (28 X 28 array).","ca9afaa1":"#  **Train Configure**\n","7d449741":"> **For training..**\n* **Setting simple early_stopping (number of patience = 5)**\n* **Loss function : Crossentropyloss (softmax)**\n* **Data split : Randomly getting dataset from train_df -> due to random extraction, 'shuffle = False' on dataloader.**\n* **Initialize network weight : xavier method uniform distribution (makes model's performance worse -> why?)**"}}