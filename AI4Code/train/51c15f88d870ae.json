{"cell_type":{"ef5ff886":"code","192d726f":"code","3742c8b3":"code","60acf4e2":"code","3aaf598f":"code","cd3b5611":"code","f8f44246":"code","22cde9bf":"code","cd1f1d02":"code","4b252663":"code","b0fd5651":"code","1c2fd4c6":"code","f889b3d3":"code","1363788f":"code","6efa6eb4":"code","68214dd7":"code","ade8d46e":"code","715e9f17":"code","2b21546e":"code","3d419770":"code","94669b2e":"code","02ebbdd9":"code","bcf8342c":"code","142d593c":"code","da32005e":"code","5e7b1e8d":"code","464188a3":"code","389e8b65":"code","fe5fb2e4":"code","ba14c218":"code","04eeb319":"code","4407c4de":"code","9c21e73c":"code","b18aae55":"code","c783b0c0":"code","4d2273c4":"code","c9efb9a4":"code","5a680743":"code","e20f98e8":"code","1f83f914":"code","ef13414d":"code","bb5dce5c":"code","3cd2d6b7":"code","f00ae026":"code","69a59de4":"code","5926f08b":"code","4d31eb3b":"code","a3dcf283":"code","1c3b34cb":"code","ca180047":"code","409f5808":"code","109e2829":"code","37cbf607":"markdown","6a3cbb19":"markdown","dd52b70c":"markdown","420389d2":"markdown","ad6d74af":"markdown","a223ee88":"markdown","0dc6e706":"markdown","70ce4e44":"markdown","81047ee3":"markdown","3396f048":"markdown","7251e355":"markdown","44326cf4":"markdown","2965de4e":"markdown","9b28d4be":"markdown","7a2002bb":"markdown","23f0d41a":"markdown","ca2aa7ea":"markdown","4218a37c":"markdown","3dab4bfb":"markdown","799ed241":"markdown"},"source":{"ef5ff886":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","192d726f":"\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\n\n\nimport pandas as pd \npd.options.display.max_columns = 100\n\nimport numpy as np \n\n\nfrom PIL import Image\nfrom scipy import ndimage\n\nimport seaborn as sns\n\nimport pylab as plot\nparams = { \n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'x-large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n}\nplot.rcParams.update(params)\n\n\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".95\"})\n%config InlineBackend.figure_format = 'svg'\n\nimport cv2\nimport glob\nimport keras\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD, Adam, RMSprop, Adagrad, Nadam, Adadelta, Adamax\nfrom keras.utils import to_categorical\nfrom matplotlib import pyplot\n\n\n","3742c8b3":"images1 = []\ntargets1 = []\n\n\n\npath1 = \"..\/input\/kvasir-dataset-v2\/kvasir-dataset-v2\/dyed-lifted-polyps\"\npath2 = \"..\/input\/kvasir-dataset-v2\/kvasir-dataset-v2\/dyed-resection-margins\"\npath3 = \"..\/input\/kvasir-dataset-v2\/kvasir-dataset-v2\/esophagitis\"\npath4 = \"..\/input\/kvasir-dataset-v2\/kvasir-dataset-v2\/normal-cecum\"\npath5 = \"..\/input\/kvasir-dataset-v2\/kvasir-dataset-v2\/normal-pylorus\"\npath6 = \"..\/input\/kvasir-dataset-v2\/kvasir-dataset-v2\/normal-z-line\"\npath7 = \"..\/input\/kvasir-dataset-v2\/kvasir-dataset-v2\/polyps\"\npath8 = \"..\/input\/kvasir-dataset-v2\/kvasir-dataset-v2\/ulcerative-colitis\"\n\n\n\nfor i in glob.glob(os.path.join(path1,'*jpg')):\n    img = cv2.imread(i)\n    #img  = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # convert to grayscale\n    img = cv2.resize(img,(50,50))  #resize\n    images1.append(np.array(img))\n    targets1.append(0)\n    \nfor j in glob.glob(os.path.join(path2,'*jpg')):\n    img = cv2.imread(j)\n    #img  = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # convert to grayscale\n    img = cv2.resize(img,(50,50))  #resize\n    images1.append(np.array(img))\n    targets1.append(1)    \n    \n    \nfor k in glob.glob(os.path.join(path3,'*jpg')):\n    img = cv2.imread(k)\n    #img  = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # convert to grayscale\n    img = cv2.resize(img,(50,50))  #resize\n    images1.append(np.array(img))\n    targets1.append(2)\n    \nfor l in glob.glob(os.path.join(path4,'*jpg')):\n    img = cv2.imread(l)\n    #img  = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # convert to grayscale\n    img = cv2.resize(img,(50,50))  #resize\n    images1.append(np.array(img))\n    targets1.append(3)    \n    \n    \nfor m in glob.glob(os.path.join(path5,'*jpg')):\n    img = cv2.imread(m)\n    #img  = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # convert to grayscale\n    img = cv2.resize(img,(50,50))  #resize\n    images1.append(np.array(img))\n    targets1.append(4)\n    \nfor n in glob.glob(os.path.join(path6,'*jpg')):\n    img = cv2.imread(n)\n    #img  = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # convert to grayscale\n    img = cv2.resize(img,(50,50))  #resize\n    images1.append(np.array(img))\n    targets1.append(5)    \n    \nfor o in glob.glob(os.path.join(path7,'*jpg')):\n    img = cv2.imread(o)\n    #img  = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # convert to grayscale\n    img = cv2.resize(img,(50,50))  #resize\n    images1.append(np.array(img))\n    targets1.append(6)\n    \nfor p in glob.glob(os.path.join(path8,'*jpg')):\n    img = cv2.imread(p)\n    #img  = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # convert to grayscale\n    img = cv2.resize(img,(50,50))  #resize\n    images1.append(np.array(img))\n    targets1.append(7)       \n    \n    \n\n\n","60acf4e2":"endoscope_images1 , endoscope_labels1 = np.array(images1), np.array(targets1)    \nnp.save(\"endoscope_images1\",endoscope_images1)\nnp.save(\"endoscope_labels1\",endoscope_labels1)","3aaf598f":"the_endoscope_images1, the_endoscope_labels1 = np.load(\"endoscope_images1.npy\") , np.load(\"endoscope_labels1.npy\")\n","cd3b5611":"fig=plt.figure(figsize=(15, 11))\nn = 36\nfor i in range(n):\n    img = np.random.randint(0, the_endoscope_images1.shape[0] , 1)\n    fig.add_subplot(n**(.5), n**(.5), i+1)\n    plt.imshow(the_endoscope_images1[img[0]])\n\n    plt.title('{}:{}' .format('dyed-lifted-polyps' if the_endoscope_labels1[img[0]]==0 else 'dyed-resection-margins' if the_endoscope_labels1[img[0]]==1 else 'esophagitis' if the_endoscope_labels1[img[0]]==2 else 'normal-cecum' if the_endoscope_labels1[img[0]]==3 else 'normal-pylorus' if the_endoscope_labels1[img[0]]==4 else 'normal-z-line' if the_endoscope_labels1[img[0]]==5 else 'polyps' if the_endoscope_labels1[img[0]]==6  else 'ulcerative-colitis' ,the_endoscope_labels1[img[0]]) )\n\n    plt.xticks([]) , plt.yticks([])\n        \nplt.show()\n","f8f44246":"X_train, X_test, Y_train, Y_test = train_test_split(the_endoscope_images1, the_endoscope_labels1, test_size=0.2)\n    \n    #converting to float and normalizing\nX_train = X_train.astype('float32')\/255 \nX_test = X_test.astype('float32')\/255\n    \n    #getting the numbr of unique classes in the labels\n#num_classes=len(np.unique(the_endoscope_labels1))\n\nnum_classes = 8\n    \n    \n#     #One hot encoding as classifier since we  has multiple classes\nY_train=keras.utils.to_categorical(Y_train,num_classes)\nY_test=keras.utils.to_categorical(Y_test,num_classes )\n","22cde9bf":"from keras.layers import Dense, Activation\n\n\nnnmodel = Sequential()\nnnmodel.add(Dense(32, input_shape=(50,50,3)))\nnnmodel.add(Activation('relu'))\nnnmodel.add(Flatten())\nnnmodel.add(Dense(8,activation=\"softmax\")) \nnnmodel.summary()","cd1f1d02":"nnmodel.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['mae', 'acc'])\n\nnnhistory =nnmodel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1)\n","4b252663":"plt.figure(figsize=(10, 6))  \n\n# summarize history for accuracy\nplt.plot(nnhistory.history['acc'])\nplt.plot(nnhistory.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test' ], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.figure(figsize=(10,6))\nplt.plot(nnhistory.history['loss'])\nplt.plot(nnhistory.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show() ","b0fd5651":"plt.figure(figsize=(10, 6))  \n\nplt.plot(nnhistory.history['mean_absolute_error'])\nplt.title('Model training mean_absolute_error')\nplt.ylabel('mean_absolute_error')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\nplt.figure(figsize=(10, 6))  \n\nplt.plot(nnhistory.history['val_mean_absolute_error'])\nplt.title('Model validation mean_absolute_error')\nplt.ylabel('mean_absolute_error')\nplt.xlabel('epoch')\nplt.legend(['test' ], loc='upper left')\nplt.show()","1c2fd4c6":"from keras.layers import Dense, Activation\n# import regularizer\nfrom keras.regularizers import l1\n# instantiate regularizer\nreg = l1(0.001)","f889b3d3":"model = Sequential()\nmodel.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(50,activation=\"relu\"))\nmodel.add(Dense(8,activation=\"softmax\",activity_regularizer=l1(0.001)))#2 represent output layer neurons \nmodel.summary()","1363788f":"model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['mae', 'acc'])\n\nhistory = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1)","6efa6eb4":"# summarize history for accuracy\nplt.figure(figsize=(10, 6))  \n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.figure(figsize=(10, 6))  \n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show() ","68214dd7":"plt.figure(figsize=(10, 6))  \n\nplt.plot(nnhistory.history['mean_absolute_error'])\nplt.title('Model training mean_absolute_error')\nplt.ylabel('mean_absolute_error')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\nplt.figure(figsize=(10, 6))  \n\nplt.plot(nnhistory.history['val_mean_absolute_error'])\nplt.title('Model validation mean_absolute_error')\nplt.ylabel('mean_absolute_error')\nplt.xlabel('epoch')\nplt.legend(['test' ], loc='upper left')\nplt.show()","ade8d46e":"from sklearn.metrics import classification_report\n\ny_pred=model.predict(X_test) \ny_pred=np.argmax(y_pred, axis=1) \ny_true = Y_test\ny_true=np.argmax(y_true, axis=1) \n\n\nprint(classification_report(y_true, y_pred))\n\n","715e9f17":"#Making confusion matrix that checks accuracy of the model\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true, y_pred)\ncm","2b21546e":"import scikitplot \n%matplotlib inline\n\n#y_true = # ground truth labels\n#y_probas = # predicted probabilities generated by sklearn classifier\ny_probas = model.predict(X_test)\nscikitplot.metrics.plot_roc(y_true, y_probas)\nplt.figure(figsize=(50, 5)) \n#plt.legend(fontsize = 'xx-small')\nplt.show()","3d419770":"plt.savefig('KvasirCnnROCcurve.png')","94669b2e":"# import regularizer\nfrom keras.layers import  Dropout, BatchNormalization\n\nfrom keras.regularizers import l1,l2\n# instantiate regularizer\nreg = l1(0.001)\nreg2 = l2(0.001)","02ebbdd9":"nn1model = Sequential()\nnn1model.add(Dense(32, input_shape=(50,50,3)))\nnn1model.add(Activation('relu'))\n# nn1model.add(Dense(64, activation=\"relu\"))\n# nn1model.add(Dense(128, activation=\"relu\"))\n# nn1model.add(Dense(512, activation=\"relu\"))\nnn1model.add(Flatten())\nnn1model.add(Dense(8,activation=\"softmax\", activity_regularizer=l1(0.001))) \n#nn1model.summary()\n\nnn1model.compile(loss='categorical_crossentropy',optimizer=SGD(lr=0.01),metrics=['accuracy'])\n\nnn1history =nn1model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=300, verbose=1)","bcf8342c":"nn1model.summary()\n","142d593c":"plt.figure(figsize=(10, 6))  \n\n\n# summarize history for accuracy\nplt.plot(nn1history.history['acc'])\nplt.plot(nn1history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test' ], loc='upper left')\nplt.show()\nplt.figure(figsize=(10, 6))  \n\n# summarize history for loss\nplt.plot(nn1history.history['loss'])\nplt.plot(nn1history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","da32005e":"score = nn1model.evaluate(X_test, Y_test, verbose=1)\nprint('\\n', 'Test_Loss:-', score[0])\nprint('\\n', 'Test_Accuracy:-', score[1])","5e7b1e8d":"from keras.layers import  Dropout, BatchNormalization","464188a3":"cnn1model = Sequential()\ncnn1model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n#cnn1model.add(BatchNormalization())\ncnn1model.add(MaxPooling2D(pool_size=2))\n#cnn1model.add(Dropout(0.5))      \ncnn1model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n#cnn1model.add(BatchNormalization())\ncnn1model.add(MaxPooling2D(pool_size=2))\n#cnn1model.add(Dropout(0.5))      \ncnn1model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnn1model.add(MaxPooling2D(pool_size=2))\n#cnn1model.add(BatchNormalization())\n#cnn1model.add(Dropout(0.25))      \ncnn1model.add(Flatten())\ncnn1model.add(Dense(50,activation=\"relu\"))\ncnn1model.add(Dense(8,activation=\"softmax\" ,kernel_regularizer=l2(0.01))) #,activity_regularizer=l1(0.01)))\n#cnn1model.summary()\ncnn1model.compile(optimizer=SGD(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\ncnn1history = cnn1model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=300, verbose=1)","389e8b65":"cnn1model.summary()\n","fe5fb2e4":"score1 = cnn1model.evaluate(X_test, Y_test, verbose=1)\nprint('\\n', 'Test_Loss:-', score1[0])\nprint('\\n', 'Test_Accuracy:-', score1[1])","ba14c218":"plt.figure(figsize=(10, 6))  \n\n# summarize history for accuracy\nplt.plot(cnn1history.history['acc'])\nplt.plot(cnn1history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test' ], loc='upper left')\nplt.show()\n\nplt.figure(figsize=(10, 6))  \n\n# summarize history for loss\nplt.plot(cnn1history.history['loss'])\nplt.plot(cnn1history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show() ","04eeb319":"from keras.callbacks import TensorBoard, LearningRateScheduler, ReduceLROnPlateau\n\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)","4407c4de":"cnnmodel_rmsprop = Sequential()\ncnnmodel_rmsprop.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_rmsprop.add(MaxPooling2D(pool_size=2))\ncnnmodel_rmsprop.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_rmsprop.add(MaxPooling2D(pool_size=2))\ncnnmodel_rmsprop.add(Flatten())\ncnnmodel_rmsprop.add(Dense(50,activation=\"relu\"))\ncnnmodel_rmsprop.add(Dense(8,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_rmsprop.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\nhist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, verbose=1, callbacks=[reduce_lr])\n","9c21e73c":"cnnmodel_adam = Sequential()\ncnnmodel_adam.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_adam.add(MaxPooling2D(pool_size=2))\ncnnmodel_adam.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_adam.add(MaxPooling2D(pool_size=2))\ncnnmodel_adam.add(Flatten())\ncnnmodel_adam.add(Dense(50,activation=\"relu\"))\ncnnmodel_adam.add(Dense(8,activation=\"softmax\"))#2 represent output layer neurons \ncnnmodel_adam.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\nhist_adam = cnnmodel_adam.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, verbose=1, callbacks=[reduce_lr])\n","b18aae55":"cnnmodel_nadam = Sequential()\ncnnmodel_nadam.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_nadam.add(MaxPooling2D(pool_size=2))\ncnnmodel_nadam.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_nadam.add(MaxPooling2D(pool_size=2))\ncnnmodel_nadam.add(Flatten())\ncnnmodel_nadam.add(Dense(50,activation=\"relu\"))\ncnnmodel_nadam.add(Dense(8,activation=\"softmax\"))#2 represent output layer neurons \ncnnmodel_nadam.compile(optimizer=Nadam(), loss='binary_crossentropy', metrics=['accuracy'])\nhist_nadam = cnnmodel_nadam.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, verbose=1, callbacks=[reduce_lr])\n","c783b0c0":"cnnmodel_sgd= Sequential()\ncnnmodel_sgd.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_sgd.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgd.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_sgd.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgd.add(Flatten())\ncnnmodel_sgd.add(Dense(50,activation=\"relu\"))\ncnnmodel_sgd.add(Dense(8,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_sgd.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_sgd = cnnmodel_sgd.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, verbose=1, callbacks=[reduce_lr])\n\n","4d2273c4":"cnnmodel_sgdnesterov = Sequential()\ncnnmodel_sgdnesterov.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_sgdnesterov.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgdnesterov.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_sgdnesterov.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgdnesterov.add(Flatten())\ncnnmodel_sgdnesterov.add(Dense(50,activation=\"relu\"))\ncnnmodel_sgdnesterov.add(Dense(8,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_sgdnesterov.compile(optimizer=SGD(nesterov=True), loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_sgdnesterov = cnnmodel_sgdnesterov.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, verbose=1, callbacks=[reduce_lr])\n","c9efb9a4":"cnnmodel_sgdmomentum = Sequential()\ncnnmodel_sgdmomentum.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_sgdmomentum.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgdmomentum.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_sgdmomentum.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgdmomentum.add(Flatten())\ncnnmodel_sgdmomentum.add(Dense(50,activation=\"relu\"))\ncnnmodel_sgdmomentum.add(Dense(8,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_sgdmomentum.compile(optimizer=SGD(momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_sgdmomentum = cnnmodel_sgdmomentum.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, verbose=1, callbacks=[reduce_lr])\n\n","5a680743":"cnnmodel_sgdnestmomentum = Sequential()\ncnnmodel_sgdnestmomentum.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_sgdnestmomentum.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgdnestmomentum.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_sgdnestmomentum.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgdnestmomentum.add(Flatten())\ncnnmodel_sgdnestmomentum.add(Dense(50,activation=\"relu\"))\ncnnmodel_sgdnestmomentum.add(Dense(8,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_sgdnestmomentum.compile(optimizer=SGD(momentum=0.9, nesterov=True), loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_sgdnestmomentum = cnnmodel_sgdnestmomentum.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, verbose=1, callbacks=[reduce_lr])\n","e20f98e8":"cnnmodel_adagrad = Sequential()\ncnnmodel_adagrad.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_adagrad.add(MaxPooling2D(pool_size=2))\ncnnmodel_adagrad.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_adagrad.add(MaxPooling2D(pool_size=2))\ncnnmodel_adagrad.add(Flatten())\ncnnmodel_adagrad.add(Dense(50,activation=\"relu\"))\ncnnmodel_adagrad.add(Dense(8,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_adagrad.compile(optimizer='adagrad', loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_adagrad = cnnmodel_adagrad.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, verbose=1, callbacks=[reduce_lr])\n\n","1f83f914":"cnnmodel_adadelta = Sequential()\ncnnmodel_adadelta.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_adadelta.add(MaxPooling2D(pool_size=2))\ncnnmodel_adadelta.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_adadelta.add(MaxPooling2D(pool_size=2))\ncnnmodel_adadelta.add(Flatten())\ncnnmodel_adadelta.add(Dense(50,activation=\"relu\"))\ncnnmodel_adadelta.add(Dense(8,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_adadelta.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_adadelta = cnnmodel_adadelta.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, verbose=1, callbacks=[reduce_lr])\n","ef13414d":"cnnmodel_adamax = Sequential()\ncnnmodel_adamax.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_adamax.add(MaxPooling2D(pool_size=2))\ncnnmodel_adamax.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_adamax.add(MaxPooling2D(pool_size=2))\ncnnmodel_adamax.add(Flatten())\ncnnmodel_adamax.add(Dense(50,activation=\"relu\"))\ncnnmodel_adamax.add(Dense(8,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_adamax.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_adamax = cnnmodel_adamax.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, verbose=1, callbacks=[reduce_lr])\n\n","bb5dce5c":"plt.figure(figsize=(10, 6))  \n#plt.axis((-1,14,0.4, 1))\n\nplt.plot(hist_rmsprop.history['val_acc'])\nplt.plot(hist_adam.history['val_acc'])\nplt.plot(hist_nadam.history['val_acc'])\nplt.plot(hist_sgd.history['val_acc'])\nplt.plot(hist_sgdnesterov.history['val_acc'])\nplt.plot(hist_sgdmomentum.history['val_acc'])\nplt.plot(hist_sgdnestmomentum.history['val_acc'])\nplt.plot(hist_adagrad.history['val_acc'])\nplt.plot(hist_adadelta.history['val_acc'])\nplt.plot(hist_adamax.history['val_acc'])\nplt.title('val. accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.legend(['rmsprop', 'adam', 'nadam', 'sgd', 'sgd_w\/nesterov', 'sgd_w\/momentum', 'sgd_w\/nesterov+momentum', 'adagrad', 'adadelta', 'adamax'], loc='lower right', fontsize = 'x-small')  \n\nplt.show()","3cd2d6b7":"plt.figure(figsize=(10, 6))  \n#plt.axis((-1,14,0.4, 1))\n\n\nplt.plot(hist_rmsprop.history['acc'])  \nplt.plot(hist_adam.history['acc'])  \nplt.plot(hist_nadam.history['acc']) \nplt.plot(hist_sgd.history['acc']) \nplt.plot(hist_sgdnesterov.history['acc']) \nplt.plot(hist_sgdmomentum.history['acc'])\nplt.plot(hist_sgdnestmomentum.history['acc'])\nplt.plot(hist_adagrad.history['acc'])\nplt.plot(hist_adadelta.history['acc'])\nplt.plot(hist_adamax.history['acc'])\nplt.title('train accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.legend(['rmsprop', 'adam', 'nadam', 'sgd', 'sgd_w\/nesterov', 'sgd_w\/momentum', 'sgd_w\/nesterov+momentum', 'adagrad', 'adadelta', 'adamax'], loc='lower right', fontsize = 'x-small')  \n\nplt.show()","f00ae026":"plt.figure(figsize=(10, 6))  \n#plt.axis((-1,14,0, 1))\n\n\nplt.plot(hist_rmsprop.history['val_loss'])  \nplt.plot(hist_adam.history['val_loss'])  \nplt.plot(hist_nadam.history['val_loss']) \nplt.plot(hist_sgd.history['val_loss']) \nplt.plot(hist_sgdnesterov.history['val_loss']) \nplt.plot(hist_sgdmomentum.history['val_loss'])\nplt.plot(hist_sgdnestmomentum.history['val_loss'])\nplt.plot(hist_adagrad.history['val_loss'])\nplt.plot(hist_adadelta.history['val_loss'])\nplt.plot(hist_adamax.history['val_loss'])\nplt.title('val loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['rmsprop', 'adam', 'nadam', 'sgd', 'sgd_w\/nesterov', 'sgd_w\/momentum', 'sgd_w\/nesterov+momentum', 'adagrad', 'adadelta', 'adamax'], loc='upper right', fontsize = 'x-small')  \n\nplt.show()","69a59de4":"plt.figure(figsize=(10, 6))  \n#plt.axis((-1,14,0, 1))\n\n\nplt.plot(hist_rmsprop.history['loss'])  \nplt.plot(hist_adam.history['loss'])  \nplt.plot(hist_nadam.history['loss']) \nplt.plot(hist_sgd.history['loss']) \nplt.plot(hist_sgdnesterov.history['loss']) \nplt.plot(hist_sgdmomentum.history['loss'])\nplt.plot(hist_sgdnestmomentum.history['loss'])\nplt.plot(hist_adagrad.history['loss'])\nplt.plot(hist_adadelta.history['loss'])\nplt.plot(hist_adamax.history['loss'])\nplt.title('train loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['rmsprop', 'adam', 'nadam', 'sgd', 'sgd_w\/nesterov', 'sgd_w\/momentum', 'sgd_w\/nesterov+momentum', 'adagrad', 'adadelta', 'adamax'], loc='upper right',fontsize = 'x-small')  \n\nplt.show()","5926f08b":"plt.figure(figsize=(10, 6))  \n#plt.axis((-1,14,0.4, 1))\n\nplt.plot(hist_rmsprop.history['val_acc'])\nplt.plot(hist_nadam.history['val_acc'])\nplt.plot(hist_sgd.history['val_acc'])\nplt.plot(hist_sgdnesterov.history['val_acc'])\n\nplt.title('test. accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.legend(['rmsprop', 'nadam', 'sgd', 'sgd_w\/nesterov'], loc='lower right', fontsize = 'x-small')  \n\nplt.show()","4d31eb3b":"plt.figure(figsize=(10, 6))  \n#plt.axis((-1,14,0.4, 1))\n\n\nplt.plot(hist_rmsprop.history['acc'])  \nplt.plot(hist_nadam.history['acc']) \nplt.plot(hist_sgd.history['acc']) \nplt.plot(hist_sgdnesterov.history['acc']) \nplt.title('train accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.legend(['rmsprop', 'nadam', 'sgd', 'sgd_w\/nesterov'], loc='lower right', fontsize = 'x-small')  \n\nplt.show()","a3dcf283":"plt.figure(figsize=(10, 6))  \n\n\nplt.plot(hist_rmsprop.history['val_loss'])  \nplt.plot(hist_nadam.history['val_loss']) \nplt.plot(hist_sgd.history['val_loss']) \nplt.plot(hist_sgdnesterov.history['val_loss']) \n\nplt.title('test loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['rmsprop', 'nadam', 'sgd', 'sgd_w\/nesterov'], loc='upper right', fontsize = 'x-small')  \n\nplt.show()","1c3b34cb":"plt.figure(figsize=(10, 6))  \n#plt.axis((-1,14,0, 1))\n\n\nplt.plot(hist_rmsprop.history['loss'])  \nplt.plot(hist_nadam.history['loss']) \nplt.plot(hist_sgd.history['loss']) \nplt.plot(hist_sgdnesterov.history['loss']) \n\nplt.title('train loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['rmsprop', 'nadam', 'sgd', 'sgd_w\/nesterov'], loc='upper right',fontsize = 'x-small')  \n\nplt.show()","ca180047":"cnndeepmodel = Sequential()\n# first convolution layer\ncnndeepmodel.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3))) \ncnndeepmodel.add(BatchNormalization())\ncnndeepmodel.add(MaxPooling2D(pool_size=2))\ncnndeepmodel.add(Dropout(0.25))\n\n#second convolution  layer\ncnndeepmodel.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnndeepmodel.add(BatchNormalization())\ncnndeepmodel.add(MaxPooling2D(pool_size=2))\ncnndeepmodel.add(Dropout(0.5))\n\n#Third convolution  layer\ncnndeepmodel.add(Conv2D(64, kernel_size=2 ,padding=\"same\",activation='relu'))\ncnndeepmodel.add(BatchNormalization())\ncnndeepmodel.add(MaxPooling2D(pool_size=2))\ncnndeepmodel.add(Dropout(0.5))\n\n#first Fully connected layer\ncnndeepmodel.add(Flatten()) \ncnndeepmodel.add(Dense(256,kernel_regularizer=l2(0.001)))#activity_regularizer=l1(0.001)))\ncnndeepmodel.add(BatchNormalization())\ncnndeepmodel.add(Activation('relu')) \ncnndeepmodel.add(Dropout(0.5))      \n\n#Final Fully connected layer\ncnndeepmodel.add(Dense(8)) \ncnndeepmodel.add(Activation('softmax')) \n\ncnndeepmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n\ncnndeepmodel.summary()\n\ncnndeephistory = cnndeepmodel.fit(X_train, Y_train, epochs=100,verbose=1,shuffle=True,callbacks=[reduce_lr])\n\ncnnscore = cnndeepmodel.evaluate(X_test, Y_test, verbose=0)\n\n#loss and accuracy\nprint('Test loss:', cnnscore[0])\nprint('Test accuracy:', cnnscore[1])","409f5808":"# from keras.models import load_model\n\n# cnndeepmodel.save('my_cnndeepmodel.h5')","109e2829":"from keras.callbacks import ModelCheckpoint\n\n#modelcheckpoint = ModelCheckpoint(\"keras.model\", savebest_only=True, verbose=1)\ncheckpoint=ModelCheckpoint(filepath='\/input\/cnndeepmodel.h5',save_best_only=True )\n#print(checkpoint)","37cbf607":"## Optimizer 5: SGD + Nesterov","6a3cbb19":"## Optimizer 6: SGD with momentum=0.9","dd52b70c":"## Optimizer 1: RMSprop","420389d2":"# Variations of the models for Hyperparameter Optimization","ad6d74af":"## Confusion matrix and classification report","a223ee88":"## Optimizer 2: Adam","0dc6e706":"# Comparing Several Keras Optimizers","70ce4e44":"## Optimizer 3: Nadam","81047ee3":"## Optimizer 9: Adadelta","3396f048":"# A deeper model","7251e355":"### Fully connected neural network","44326cf4":"## Optimizer 8:Adagrad","2965de4e":"# Showing just the best and the worst performing after some observations","9b28d4be":"# Neural Network Models","7a2002bb":"## Optimizer 7: SGD + Nesterov with momentum=0.9","23f0d41a":"## Optimizer 10: Adamax","ca2aa7ea":"## Optimizer : SGD","4218a37c":"view how the models loss and accuracy and loss changes\n1. fcn2layer, lr = 0.01, beta = 0,reg=0, epochs = 10\n2. fcn2layer+reg, lr = 0.01, beta = 0,reg=0.001, epochs = 10\n3. fcn2layer+lr1, lr = 0.001, beta = 0,reg=0, epochs = 10\n4. fcn2layer+lr1+reg, lr = 0.001, beta = 0,reg=0.001, epochs = 10\n5. fcn3layer+lr1+reg, lr = 0.001, beta = 0,reg=0.001, epochs = 10\n6. fcn3layer+lr1+reg, lr = 0.001, beta = 0,reg=0.001, epochs = 10\n7. fcn4layer+lr1+reg, lr = 0.001, beta = 0,reg=0.001, epochs = 10\n\nand also when the nodes changes\n1. fcn2layer, nodes = 32\n2. fcn2layer, nodes = 64\n3. fcn2layer, nodes = 128\n","3dab4bfb":"# Plots of the optimizers","799ed241":"# Plotting the Loss and Accuracy"}}