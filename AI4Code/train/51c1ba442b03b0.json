{"cell_type":{"7c075b04":"code","6b295c75":"code","374abcaa":"code","d9e5ddf4":"code","5f5a7a85":"code","258a0c6a":"code","fa389fee":"code","8eb7e2e3":"code","eedfc0f8":"code","8fde9757":"code","4e0ec9ae":"code","1864d8a7":"code","f807c638":"code","fb8531c9":"code","e6d3e796":"code","d0484393":"code","748d4120":"code","b99acbd6":"code","1a6fbb10":"markdown","fb012682":"markdown","27d46151":"markdown","19d638a8":"markdown","72e9e1ad":"markdown","491670a2":"markdown","3768458b":"markdown","73f0dbe1":"markdown"},"source":{"7c075b04":"import librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport glob \nfrom sklearn.metrics import confusion_matrix\nimport IPython.display as ipd  # To play sound in the notebook\nimport os\nimport sys","6b295c75":"RAV = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/\"\ndir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        path.append(RAV + i + '\/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\n\nRAV_df = RAV_df.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df.columns = ['emotion']\nRAV_df['labels'] = RAV_df.emotion \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['emotion'], axis=1)\nRAV_df.labels.value_counts()\n","374abcaa":"# fear\nfname = RAV + 'Actor_14\/03-01-06-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname, sr=None)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\nipd.Audio(fname)","d9e5ddf4":"# happy\nfname = RAV + 'Actor_14\/03-01-03-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname, sr = None)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\nipd.Audio(fname)","5f5a7a85":"# Source - RAVDESS; Gender - Male; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_09\/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, sr=None)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=16)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title(f'Audio sampled at {sample_rate} hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","258a0c6a":"import numpy as np \nimport pandas as pd \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset, DataLoader\nimport librosa\nimport os\nimport random\n\n\n\ndef seed_everything(seed):\n    \"\"\"Function to enable reproducibility\"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nclass RAVDataset(Dataset):\n    \"\"\"Dataset for wav files from RAVDESS with their mel frequency\n    cepstrum coefs\"\"\"\n    def __init__(self, data_root):\n        self.samples = []\n        for actor in sorted(os.listdir(data_root)):\n            actor_folder = os.path.join(data_root, actor)\n            \n            for actor_wav in sorted(os.listdir(actor_folder)):\n                codes = actor_wav.split('.')[0].split('-')\n                emotion = int(codes[2]) - 1\n                intensity = codes[3]\n                actor_id = codes[6]\n                wav_path = os.path.join(actor_folder, actor_wav)\n                # create image\n                wav, sample_rate = librosa.load(wav_path, sr=None,duration=2,offset=0.75)\n                mfcc = librosa.feature.mfcc(y=wav, sr=sample_rate, n_mfcc=32)\n                self.samples.append((torch.from_numpy(mfcc[np.newaxis,...]), emotion))\n                \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        return self.samples[idx]\n    \ndef train_val_dataset(dataset, val_split=0.20, test_split = 0.10):\n    \"\"\"Get train, val, test datasets\"\"\"\n    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split+test_split)\n    datasets = {}\n    split = int((test_split\/(val_split+test_split))*len(val_idx))\n    test_idx = val_idx[:split]\n    val_idx = val_idx[split:]\n    #print(len(train_idx),len(val_idx),len(test_idx))\n    datasets['train'] = Subset(dataset, train_idx)\n    datasets['val'] = Subset(dataset, val_idx)\n    datasets['test'] = Subset(dataset, test_idx)\n    return datasets","fa389fee":"SEED = 42\nseed_everything(SEED)\nRAV = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/\"\ndata = RAVDataset(RAV)\ndatasets = train_val_dataset(data)\ndataloaders = {x:DataLoader(datasets[x],8, shuffle=True) for x in ['train','val','test']}","8eb7e2e3":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 128, kernel_size=13, stride=1, padding=6)\n        self.conv2 = nn.Conv2d(128, 128, kernel_size=11, stride=1, padding=5)\n        self.conv2_bn = nn.BatchNorm2d(128)\n        self.conv3 = nn.Conv2d(128, 64, kernel_size=9, stride=1, padding=4)\n        self.conv4 = nn.Conv2d(64, 32, kernel_size=7, stride=1, padding=3)\n        self.conv4_bn = nn.BatchNorm2d(32)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.drop1 = nn.Dropout(0.2)\n        self.drop2 = nn.Dropout(0.1)\n        self.fc1 = nn.Linear(32*2*11, 400)\n        self.dense1_bn = nn.BatchNorm1d(400)\n        self.fc2 = nn.Linear(400, 8)\n        \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2_bn(self.conv2(x))))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = self.pool(F.relu(self.conv4_bn(self.conv4(x))))\n\n\n        x = x.view(-1, 32 * 2 * 11)\n        \n        # classifier\n        x = self.drop1(x)\n        x = F.relu(self.dense1_bn(self.fc1(x)))\n        x = self.drop2(x)\n        x = self.fc2(x)\n        return x","eedfc0f8":"model = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()","8fde9757":"def train(epoch):\n    model.train()\n    tr_loss = 0\n    correct = 0\n    total = 0\n    train_loader = dataloaders['train']\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n\n        optimizer.zero_grad()\n        output = model(data)\n        pred = torch.max(output.data, 1)[1]\n        correct += (pred == target).sum()\n        total += len(data)\n        \n        loss = criterion(output, target)\n        # print(loss)\n        \n        loss.backward()\n        optimizer.step()\n        \n        \n        tr_loss = loss.item()\n        if (batch_idx + 1)% 100 == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f} \\t Accuracy: {} %'.format(\n                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n                100. * (batch_idx + 1) \/ len(train_loader), loss.item(),100 * correct \/ total))\n            torch.save(model.state_dict(), '.\/model.pth')\n            torch.save(model.state_dict(), '.\/optimizer.pth')\n    train_loss.append(tr_loss \/ len(train_loader))\n    train_accuracy.append(100 * correct \/ total)","4e0ec9ae":"def evaluate(data_loader):\n    model.eval()\n    loss = 0\n    correct = 0\n    total = 0\n    for data, target in data_loader:\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        \n        output = model(data)\n        loss += F.cross_entropy(output, target, size_average=False).item()\n        pred = torch.max(output.data, 1)[1]\n        total += len(data)\n        correct += (pred == target).sum()\n    loss \/= len(data_loader.dataset)\n    valid_loss.append(loss)    \n    valid_accuracy.append(100 * correct \/ total)\n    print('\\nAverage Validation loss: {:.5f}\\tAccuracy: {} %'.format(loss, 100 * correct \/ total))","1864d8a7":"n_epochs = 200\ntrain_loss = []\ntrain_accuracy = []\nvalid_loss = []\nvalid_accuracy = []\nfor epoch in range(n_epochs):\n    train(epoch)\n    evaluate(dataloaders['val'])","f807c638":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\ndef plot_graph(epochs):\n    fig = plt.figure(figsize=(20,4))\n    ax = fig.add_subplot(1, 2, 1)\n    plt.title(\"Train - Validation Loss\")\n    plt.plot(list(np.arange(epochs) + 1) , train_loss, label='train')\n    plt.plot(list(np.arange(epochs) + 1), valid_loss, label='validation')\n    plt.xlabel('num_epochs', fontsize=12)\n    plt.ylabel('loss', fontsize=12)\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.legend(loc='best')\n    \n    ax = fig.add_subplot(1, 2, 2)\n    plt.title(\"Train - Validation Accuracy\")\n    plt.plot(list(np.arange(epochs) + 1) , train_accuracy, label='train')\n    plt.plot(list(np.arange(epochs) + 1), valid_accuracy, label='validation')\n    plt.xlabel('num_epochs', fontsize=12)\n    plt.ylabel('accuracy', fontsize=12)\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.legend(loc='best')\n\nplot_graph(n_epochs)","fb8531c9":"test_loader = dataloaders['test']","e6d3e796":"dataiter = iter(test_loader)\nimages, labels = dataiter.next()\nclasses = ('neutral', 'calm', 'happy', 'sad', 'angry', 'fearfull', 'disgust', 'surprised')\nprint('GroundTruth: ', ' '.join('%9s' % classes[labels[j]] for j in range(8)))\noutputs = model(images.cuda())\n\n_, predicted = torch.max(outputs, 1)\nprint('Predicted   : ', ' '.join('%9s' % classes[predicted[j]]\n                              for j in range(8)))","d0484393":"correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data[0].cuda(), data[1].cuda()\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the 144 test images: %d %%' % (\n    100 * correct \/ total))","748d4120":"class_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data[0].cuda(), data[1].cuda()\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(8):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(8):\n    print('Accuracy of %5s : %2d %%' % (\n        classes[i], 100 * class_correct[i] \/ class_total[i]))","b99acbd6":"import seaborn as sns\n\ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    df_cm = pd.DataFrame(\n            confusion_matrix, index=classes, columns=classes, \n        )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True)\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nnb_classes = 8\n\nconfusion_matrix = torch.zeros(nb_classes, nb_classes)\nwith torch.no_grad():\n    for i, (inputs, pclasses) in enumerate(dataloaders['test']):\n        inputs = inputs.cuda()\n        pclasses = pclasses.cuda()\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        for t, p in zip(pclasses.view(-1), preds.view(-1)):\n                confusion_matrix[t.long(), p.long()] += 1\n\nprint_confusion_matrix(confusion_matrix.numpy(), class_names = classes)","1a6fbb10":"#### Prikazujem rezultate na jednom batchu, zatim performanse na celom test setu, kako performanse zavise od klase emocija, i na kraju konfuzionu matricu ","fb012682":"#### U imenu signala koji su dostupni u setu podataka je enkodirano sve sto je relevantno za nacin koji je snimljen. Detalji su ovde:https:\/\/journals.plos.org\/plosone\/article?id=10.1371\/journal.pone.0196391 pod Filename convention","27d46151":"### Emocije su jednako rasporedjene po klasama, neutral klase ima duplo manje jer ne postoji neutral strong.\n#### Imena klasa objasnjavaju emocije, jedino pojasnjenje je da je 'calm' klasa ukljucena sa neznatno pozitivnom valencom jer su u drugim skupovima podataka 'neutral' klase dozivljavane negativno.\n\n#### Primeri zvucnih signala:","19d638a8":"### Neuralna mreza\n#### Arihtektura do koje sam dosao je nastala iterativno od mnogo jednostavnije i mogu detaljno objasniti postupak i rezonovanje kako sam do nje dosao. Moze se posmatrati kao dvodelna, deo do izravnjivanja (konvolucioni) i kasnije klasifikacija. Eksperimentisao sam sa sledecim hiperparametrima:\n* arhitektura: broj slojeva mreze, inicijalno samo conv2d i klasifikacioni deo\n* sam broj kepstralnih koeficijenata u preprocesiranju 16,32,64 sto kasnije menja i druge slojeve\n* razlicite velicine filtera: U literaturi su najrasprostranjeniji 3x3 filtri zbog kompjuterske vizije. U ovom slucaju, eksperimentalno, veci 5x5, 7x7 filteri stabilizuju konvergiranje.\n* broj konvolucionih slojeva, sto je donekle uslovljeno ulaznim oblikom podataka ako se uvek primenjuje maxpool\n* broj neurona u klasifikacionom delu, sto nije imalo veliki uticaj","72e9e1ad":"### Reprezentacija signala u domenu pogodnom za primenu klasicnih konvolucionih mreza. \n\n#### U literaturi se konvolucione mreze primenjuju na razne vrste spektograma i njihove kombinacije. Glavna idjea je da posmatramo signal iz druge perspektive, u najpoznatijem slucaju frekvencijskog. Medjutim, kako je signal nestacionaran, postoje metode koje zadrzavaju i vremenske i frekvencijske osobine.\n\n#### MFCC: [Mel skala](https:\/\/en.wikipedia.org\/wiki\/Mel_scale) pokusava da oslika kako ljudi percipiraju zvuk, dok su kepstralni koeficijenti prihva\u0107eni u literaturi za ozna\u010davanje inverzne  Fourier-ove  transformacije  logaritma  spektra  snage  signala. Drugo C su koeficijenti koji cine kepstar.","491670a2":"### Inicijalizovan seed, dataset i dataloaderi","3768458b":"### Eksploracija prateci seriju notebooka sa kaggle dataseta RAVDESS: https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-1-explore-data\n##### Napomena:RAVDESS dataset je semplovan 48kHz, iz nekog razloga mnogi re-sampluju podatke na 44100Hz. Ne vidim odmah problem sa time, medjutim zadrzavam originalan sample rate sa sr=None kada se signal ucitava librosom.\n\n##### Takodje u navednim notebookovima 2 klase 'neutral' i 'calm' su spojene u istu, iako je na sajtu RAVDESS dataseta navedena razlika i neophodnost postojanja 'calm' klase koja po njihovom misljenju nedostaje u drugim setovima podataka","73f0dbe1":"### Kreiran je dataset pomocu torch Dataseta kako bi se lakse snabdevali batchevi tokom treniranja\n\n#### Neke odluke prilikom kreiranja dataseta. \n#### Iako mozda ne standardan nacin, omogucio mi je da lako pristupam transformisanim signalima direktno u petlji. Inicajalno sam zadrzavao jos neke odlike zbog potencijalne neefikasnosti modela ukoliko se eksplicitno ne koriste (male\/female, strong), medjutim, model je funkcionisao zadovoljavajuce bez toga. Ostaju kao ideje za unapredjeje"}}