{"cell_type":{"20c4c150":"code","0a2fbe38":"code","4f56d812":"code","7baebcf5":"code","97466904":"code","fa12a0f4":"code","97247d82":"code","c314ef8a":"code","0f711c2a":"code","8fd8234a":"code","01533dbe":"code","05871c83":"code","2be41031":"code","e466e585":"code","cb32193e":"code","6781d54a":"code","d93719d5":"code","e02ed899":"code","8629590c":"code","804f8b84":"code","c5cc836e":"markdown","a6271893":"markdown","a33504da":"markdown","8e8700dd":"markdown","51f5042a":"markdown","fc8519a2":"markdown"},"source":{"20c4c150":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport time\nimport os\nfrom IPython.display import clear_output","0a2fbe38":"import tensorflow as tf\n\nfrom tensorflow.keras import datasets, layers, models","4f56d812":"from sklearn.preprocessing import LabelEncoder","7baebcf5":"sample_size = 200\nwidth = 50\nheight = 50","97466904":"files = []\nadress = '\/kaggle\/input\/fruits\/fruits-360\/Training\/{}'\nos.chdir('\/kaggle\/input\/fruits\/fruits-360\/Training')\nfor i in os.listdir(os.getcwd()):\n    files.append(i)","fa12a0f4":"data = {}\nfor f in files:\n    data[f]=[]\nfor col in files:\n    os.chdir(adress.format(col))\n    for i in os.listdir(os.getcwd()):\n        if i.endswith('.jpg'):\n            data[col].append(i)","97247d82":"start = time.time()\nimage_data = []\nimage_target = []\n\nfor title in files:\n    os.chdir(adress.format(title))\n    counter = 0\n    for i in data[title]:\n        img = cv2.imread(i)\n        image_data.append(cv2.resize(img,(width, height)))\n        image_target.append(title)\n        counter += 1\n        if counter == sample_size:\n            break\n    clear_output(wait=True)\n    print(\"Compiled Class\",title)\ncalculate_time = time.time() - start    \nprint(\"Calculate Time\",round(calculate_time,5))","c314ef8a":"start = time.time()\nimage_data_test = []\nimage_target_test = []\n\nfor title in files:\n    os.chdir(adress.format(title))\n    sayac = 0\n    for i in data[title][sample_size:]:\n        img = cv2.imread(i)\n        image_data_test.append(cv2.resize(img,(width, height)))\n        image_target_test.append(title)\n        sayac += 1\n        if sayac == 50:\n            break\n    clear_output(wait=True)\n    print(\"Compiled Class\",title)\ncalculate_time = time.time() - start    \nprint(\"Calculate Time\",round(calculate_time,5))","0f711c2a":"image_data = np.array(image_data)\nsize = image_data.shape[0]\nimage_data.shape","8fd8234a":"image_data_test = np.array(image_data_test)\nsize = image_data_test.shape[0]\nimage_data_test.shape","01533dbe":"plt.figure(figsize=(15,15))\nfor i in range(1,17):\n    fig = np.random.choice(np.arange(1,size+1))\n    plt.subplot(4,4,i)\n    plt.imshow(image_data[fig], cmap=\"gray\", origin='upper', interpolation = 'bicubic')\n    plt.title(image_target[fig])\n    plt.xticks([]), plt.yticks([])\nplt.show()","05871c83":"labels = LabelEncoder()\nlabels.fit(image_target)","2be41031":"train_images = image_data \/ 255.0\ntrain_labels = labels.transform(image_target)","e466e585":"test_images = image_data_test \/ 255.0\ntest_labels = labels.transform(image_target_test)","cb32193e":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(width,height,3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))","6781d54a":"model.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(131))","d93719d5":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory = model.fit(train_images, train_labels, epochs=8, \n                    validation_data=(test_images, test_labels))","e02ed899":"plt.style.use('ggplot')\nplt.figure(figsize=(10, 5))\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\n\ntest_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)","8629590c":"def Prediction(image):\n    \n    global width, height, files, labels\n    \n    #img = cv2.resize(image,(width,height))\n    \n    test = img \/ 255.0\n    \n    pred = model.predict(np.array([image])).argmax()\n    \n    return labels.inverse_transform([pred])[0]","804f8b84":"plt.figure(figsize=(15,15))\nfor i in range(1,17):\n    fig = np.random.choice(np.arange(1,size+1))\n    plt.subplot(4,4,i)\n    plt.imshow(image_data[fig], cmap=\"gray\", origin='upper', interpolation = 'bicubic')\n    plt.title(image_target[fig])\n    plt.ylabel(\"| Pred:{} |\".format(Prediction(image_data[fig])),fontsize=17)\n    plt.xticks([]), plt.yticks([])\nplt.show()","c5cc836e":"# **Contents**\n\n1. [Import data and python packages](#t1.)\n\n\n2. [Data visualization](#t2.)\n\n\n3. [Classification (CNN)](#t3.)\n\n\n4. [Prediction](#t4.)","a6271893":"<a id=\"t1.\"><\/a>\n# 1. Import data and python packages","a33504da":"<h1><center>Fruit 360 Dataset<\/center><\/h1>\n\n<center><img src=\"https:\/\/www.researchgate.net\/publication\/342916129\/figure\/fig2\/AS:913043131207680@1594697854025\/Illustration-of-few-images-from-Fruits-360-dataset.ppm\"><\/center>","8e8700dd":"<a id=\"t2.\"><\/a>\n# 2. Data visualization","51f5042a":"<a id=\"t3.\"><\/a>\n# 3. Classification (CNN)","fc8519a2":"<a id=\"t4.\"><\/a>\n# 4. Prediction"}}