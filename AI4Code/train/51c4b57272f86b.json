{"cell_type":{"7acb7e5d":"code","c418976c":"code","a5761a41":"code","fecb6741":"code","b73b73d5":"code","b7b6f03d":"code","83e45d57":"code","9d92687d":"code","48f156a1":"code","2d9fd9ea":"code","4cf82a21":"code","4f1f0d6a":"code","934498dc":"code","6b742f6c":"code","7663dc67":"code","7b9f1962":"code","09a44149":"code","f3f57fa2":"code","dcd18ca7":"code","085585d0":"code","5523fe60":"code","ce10536e":"code","e5920ae5":"code","a826c37d":"markdown","2bbf3edb":"markdown","77ef1aaf":"markdown","208fb5e7":"markdown","37a1f873":"markdown","21983b2e":"markdown","c7bab877":"markdown","9c588a31":"markdown","33d5930f":"markdown","991b5730":"markdown","e989bbe3":"markdown","3a178644":"markdown"},"source":{"7acb7e5d":"#Libraries Imported\nimport pandas as pd\nimport warnings\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nimport keras\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","c418976c":"df_train = pd.read_csv('\/kaggle\/input\/carsales\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/carsales\/test.csv')","a5761a41":"display(df_train.info())\ndisplay(df_train.head())\ndisplay(df_train.describe())","fecb6741":"df_train['date'] = pd.to_datetime(df_train['date'], format = '%d\/%m\/%Y')\ndf_test['date'] = pd.to_datetime(df_test['date'], format = '%d\/%m\/%Y')","b73b73d5":"print('Minimum date from training set: {}'.format(df_train['date'].min()))\nprint('Maximum date from training set: {}'.format(df_train['date'].max()))","b7b6f03d":"print('Minimum date from test set: {}'.format(df_test['date'].min()))\nprint('Maximum date from test set: {}'.format(df_test['date'].max()))","83e45d57":"plt.figure(figsize=(12,6))\nax = sns.lineplot(x=\"date\", y=\"private_orders\", data=df_train)\nax.set_title('Private Orders per Day', fontsize=30)\nax.set_xlabel('Date', fontsize=20)\nax.set_ylabel('Number', fontsize=20)\nplt.tight_layout()\nplt.show()","9d92687d":"plt.figure(figsize=(12,6))\nax = sns.lineplot(x=\"week_id\", y=\"private_orders\", data=df_train)\nax.set_title('Private Orders per Day', fontsize=30)\nax.set_xlabel('Week', fontsize=20)\nax.set_ylabel('Number', fontsize=20)\nplt.tight_layout()\nplt.show()","48f156a1":"sns.heatmap(df_train.corr(), cmap='YlOrRd', annot_kws={'size': 20});","2d9fd9ea":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","4cf82a21":"#convert to an array\nvalues = df_train.iloc[:,2:].values\n#convert all columns to float\n#values.astype('float32')\n#normalize featues\nscaler = MinMaxScaler(feature_range=(0,1))\nscaled = scaler.fit_transform(values)\n# frame as supervised learning\nreframed = series_to_supervised(scaled, 7, 1)\n# drop columns we don't want to predict\nreframed.drop(reframed.columns[range(56,63)], axis=1, inplace=True)\nprint(reframed.head())","4f1f0d6a":"# split into train and test sets\nvalues = reframed.values\nn_train_days = 7*64 \ntrain = values[:n_train_days, :]\ndev = values[n_train_days:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ndev_X, dev_y = dev[:, :-1], dev[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 7, 8))\ndev_X = dev_X.reshape((dev_X.shape[0], 7, 8))\nprint(train_X.shape, train_y.shape, dev_X.shape, dev_y.shape)","934498dc":"# design network\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(keras.layers.Dense(1))\nmodel.compile(loss='mae', optimizer='adam')\n# fit network\nhistory = model.fit(train_X, train_y, epochs=20, batch_size=128, validation_data=(dev_X, dev_y), verbose=2, shuffle=False)\n# plot history\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='dev')\nplt.legend()\nplt.show()","6b742f6c":"#take a backup for the submission file\nsubmit = df_test.copy()\n#Insert all zeroes for the values to be predicted.\ndf_test.insert(loc=9, column='private_orders', value=0)\ndf = df_test.append(df_train[-7:], ignore_index=True)\ndf.sort_values('week_id',inplace=True)\ndf = df.reset_index(drop=True)","7663dc67":"#convert to an array\nvalues_test = df.iloc[:,2:].values\n#convert all columns to float\n#values_test.astype('float32')\n#normalize featues\nscaler_test = MinMaxScaler(feature_range=(0,1))\nscaled_test = scaler_test.fit_transform(values_test)\n# frame as supervised learning\nreframed_test = series_to_supervised(scaled_test, 7, 1)\n# drop columns we don't want to predict\nreframed_test.drop(reframed_test.columns[range(56,63)], axis=1, inplace=True)\nprint(reframed_test.head())","7b9f1962":"values_test = reframed_test.values\n# split into input and outputs\ntest_X, test_y = values_test[:, :-1], values_test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntest_X = test_X.reshape((test_X.shape[0], 7, 8))\nprint(test_X.shape, test_y.shape)","09a44149":"# make a prediction\nyhat = model.predict(test_X)","f3f57fa2":"test_X = test_X.reshape((test_X.shape[0], 7*8))\ntest_X.shape","dcd18ca7":"# invert scaling for forecast\ninv_yhat = np.concatenate((test_X[:, -8:-1],yhat), axis=1)\ninv_test = scaler_test.inverse_transform(inv_yhat)\ninv_yhat = inv_test[:,-1]","085585d0":"np.around(inv_yhat)","5523fe60":"#map all the values to integers\ny = list(map(int,inv_yhat))","ce10536e":"#Insert the predicted values\nsubmit.insert(loc=9, column='private_orders', value=y)","e5920ae5":"#download the csv\nsubmit.to_csv('csv_to_submit.csv', index = False)","a826c37d":"For my model, I'll be an LSTM with 100 neurons in the first layer and a single neuron in the output layer for predicting the number of private orders per day. The model will be fit for 100 training epochs with a batch size of 128.\n\n>All these values for epochs, batch_size and mae loss function are not taken randomly. I trained my model on multiple values and after hit and trial these are the best values i could get.","2bbf3edb":"# Exploratory Data Analysis","77ef1aaf":"# Preparing the data","208fb5e7":"# Defining and Fitting the model","37a1f873":">On studying the variation of number of private orders on both granularities: daily and weekly, It can be concluded that it has seasonality and noise. It doen't seem to have some sort of trend.","21983b2e":"Since, We are using the past day to predict the next day private orders, I'll add the last record from training set to help in predicting the values in the test set.","c7bab877":"Apply the same transformations to the test data as applied to the training data.","9c588a31":"# A glance at the data","33d5930f":"# Predicting the values","991b5730":"Since, we have the training data for 79 weeks. Lets, take 64 weeks of data for training the model and remaining 15 weeks for finding the best parameters(Cross-validation).","e989bbe3":"> The private_orders seems to be most positively correlated with sessions, total_configs_finished and dealer_locator. private_orders don't seem to be negatively correlated with none of the features.","3a178644":"Since, date is a string, better to convert into datetime format for better analysis."}}