{"cell_type":{"4284340c":"code","09390fc3":"code","f4b8e622":"code","30a3d5ec":"code","58f81928":"code","4831edcc":"code","88a3a39f":"code","3f0ee5a7":"code","dabbbe7b":"markdown","1fb5690c":"markdown"},"source":{"4284340c":"import warnings\nwarnings.filterwarnings('ignore')","09390fc3":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom typing import List\nimport numpy as np","f4b8e622":"tokenizer = AutoTokenizer.from_pretrained(\"microsoft\/DialoGPT-large\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft\/DialoGPT-large\")","30a3d5ec":"from transformers import AutoModelForSequenceClassification\n\n\nmodel_score = \"microsoft\/DialogRPT-updown\"   \ntokenizer_rpt = AutoTokenizer.from_pretrained(model_score)\nmodel_rpt = AutoModelForSequenceClassification.from_pretrained(model_score)","58f81928":"# generating appropriate response to the given query as candidate\ndef get_candidates(user_input:str,\n                   context: List[str] = []) -> List[str]:\n    \"\"\"Your code here\"\"\"\n    \n    user_input = tokenizer.encode(user_input, tokenizer.eos_token, return_tensors='pt')\n    \n    beam_outputs = model.generate(\n        user_input, \n        max_length=60, \n        num_beams=5, \n        no_repeat_ngram_size=2, \n        num_return_sequences=5, \n        early_stopping=True\n    )\n    beam_output = []\n    for step in range(5):\n        m = format(tokenizer.decode(beam_outputs[:, user_input.shape[-1]:][step], skip_special_tokens=True))\n        beam_output.append(m)\n    \n    return beam_output, beam_outputs","4831edcc":"# fetching the best response that has the highest score\ndef get_scores(context:List[str],\n               user_input:str,\n               candidates: List[str]) -> List[float]:\n    \n    \"\"\"Your code here\"\"\"\n    \n    result = model_rpt(context, return_dict=True)\n    candidate_scores = torch.sigmoid(result.logits)\n\n    \n    return candidate_scores","88a3a39f":"# simple chat function with the bot\ndef chat():\n    #init the chat with 0 context\n    context = []\n    \n    while True:\n        \n        #get user input\n        user_input = input('user > ')\n        if 'bye' in user_input.lower():\n            break \n        \n        candidates, output = get_candidates(user_input)\n        scores = get_scores(output, user_input, candidates)\n        \n        response = candidates[torch.argmax(scores)]\n\n        print('Bot >', response)\n        \n        context.extend([user_input,response])","3f0ee5a7":"chat()","dabbbe7b":"# Basic Chatbot Using Huggingface hugs Transformer Model\n","1fb5690c":"### Here's a quick look at what this kernel aims at making:\n![](https:\/\/raw.githubusercontent.com\/EsratMaria\/All-about-Natural-Language-and-Speech-Processing\/master\/huggingface-transformers\/DialoGPT_DialogRPT\/visuals\/recording.gif)\n\n## How it works\n- I am using Hugging face transformer models to make interactive chat experience. More about huggingface \ud83e\udd17 can be found ![here](https:\/\/github.com\/huggingface\/transformers)\n- I am using DialoGPT model to retrieve possible candidates based on user input\n- Once the candidate are retrieved I score them using DialogRPT model. \n- The one with the best score is chosen by the bot to reply with\n- I have broken down the candidate generation and scoring into separate notebooks [here](https:\/\/github.com\/EsratMaria\/All-about-Natural-Language-and-Speech-Processing\/blob\/master\/huggingface-transformers\/DialoGPT_DialogRPT\/QueryCandidate_Retrieval_DialoGPT.ipynb) and [here](https:\/\/github.com\/EsratMaria\/All-about-Natural-Language-and-Speech-Processing\/blob\/master\/huggingface-transformers\/DialoGPT_DialogRPT\/Candidate_Scoring_DialogRPT.ipynb)\n\n\n## Extra\nIf you want to score a list of candidate given by the user then you can check the notebook [here](https:\/\/github.com\/EsratMaria\/All-about-Natural-Language-and-Speech-Processing\/blob\/master\/huggingface-transformers\/DialoGPT_DialogRPT\/Transformer_Scoring_List_of_Candidates.ipynb) which guides you through the process and is a bit different from the above. \n"}}