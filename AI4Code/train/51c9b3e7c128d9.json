{"cell_type":{"88521ed8":"code","bc2e7ec1":"code","e0af1767":"code","fc3bdcf4":"code","f9487e6f":"code","0e8e9857":"code","c637f79a":"code","b958f5f3":"code","71187d88":"code","bdf26bb5":"code","9c5bf601":"code","d1bcc413":"code","2827c97b":"code","8e8ff93a":"code","6e731f8d":"code","23f7d0c5":"code","ea40a0ba":"code","e6b9241c":"code","7dd1b7f6":"code","ba27ed08":"code","f2eb9477":"code","3bea90ed":"code","6c896b9d":"code","16473dec":"code","f6c58ffb":"code","1013b143":"markdown","57ad40cb":"markdown","4a08a1a3":"markdown","94acec6e":"markdown","edaa74a4":"markdown","bc3865a4":"markdown","ea6ddee0":"markdown","7ba91e08":"markdown","080804bb":"markdown","57589e09":"markdown","a006f519":"markdown"},"source":{"88521ed8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import auc, roc_auc_score, f1_score, confusion_matrix\n!pip install -q scikit-plot\nimport scikitplot as skplot\nfrom sklearn.model_selection import GridSearchCV\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nimport warnings\nimport os\nwarnings.filterwarnings(action='ignore')","bc2e7ec1":"data = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","e0af1767":"data.head()","fc3bdcf4":"print(f\"Training data shape : {data.shape}\")","f9487e6f":"data.Class.value_counts()","0e8e9857":"plt.figure(figsize=(5,5))\nax = sns.countplot(x= data.Class, data=data, hue=\"Class\")\nfor p in ax.patches:\n  percentage = '{:.1f}%'.format(100 * p.get_height()\/data.shape[0])\n  x = p.get_x() + p.get_width() \/ 2 - 0.15\n  y = p.get_y() + p.get_height()\n  ax.annotate(percentage, (x, y), size = 12)\nplt.show();","c637f79a":"data.isnull().sum()","b958f5f3":"data.Time.describe()","71187d88":"std_sclar = StandardScaler()\nstd_sclar.fit(data.Time.values.reshape(-1,1))\ndata[\"std_time\"] = std_sclar.transform(data.Time.values.reshape(-1,1))\nstd_sclar.fit(data.Amount.values.reshape(-1,1))\ndata[\"std_amount\"] = std_sclar.transform(data.Amount.values.reshape(-1,1))","bdf26bb5":"data.drop(labels=['Time','Amount'], axis=1, inplace=True)","9c5bf601":"data.head()","d1bcc413":"y = data.Class\nX = data.drop(labels=['Class'], axis=1)","2827c97b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)","8e8ff93a":"print(f\"Train Data 0's {round(len(y_train[y_train == 0])\/len(y_train) * 100, 2)} % and 1's {round(len(y_train[y_train == 1])\/len(y_train) * 100, 2)} %\")\nprint(f\"Test Data 0's {round(len(y_test[y_test == 0])\/len(y_test) * 100, 2)} % and 1's {round(len(y_test[y_test == 1])\/len(y_test) * 100, 2)} %\")","6e731f8d":"lrg = LogisticRegression(class_weight='balanced')\ngridcv = GridSearchCV(estimator=lrg, param_grid={'C':[0.01,0.1,1,10,100]}, n_jobs=-1, scoring='roc_auc', verbose=1, cv=3)\ngridcv.fit(X_train, y_train)\nprint(gridcv.best_score_)\nprint(gridcv.best_estimator_)","23f7d0c5":"model = LogisticRegression(C=0.01, class_weight='balanced', dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\nprint(f\"Train AUC Score : {roc_auc_score(y_train, y_pred)}\")\nprint(f\"Test AUC Score : {roc_auc_score(y_test, y_test_pred)}\")","ea40a0ba":"skplot.metrics.plot_confusion_matrix(y_test, y_test_pred, figsize=(7,5))","e6b9241c":"over_samples = SMOTE(random_state=42, n_jobs=-1)\nX_train_over, y_train_over = over_samples.fit_sample(X_train, y_train)","7dd1b7f6":"print(f\"After over sampling Positive Samples Count : {len(y_train_over[y_train_over == 1])}\")\nprint(f\"After over sampling Negitive Samples Count : {len(y_train_over[y_train_over == 0])}\")","ba27ed08":"model = LogisticRegression(C=0.01, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\nmodel.fit(X_train_over, y_train_over)\ny_pred = model.predict(X_train_over)\ny_test_pred = model.predict(X_test)\nprint(f\"Train AUC Score : {roc_auc_score(y_train_over, y_pred)}\")\nprint(f\"Test AUC Score : {roc_auc_score(y_test, y_test_pred)}\")","f2eb9477":"skplot.metrics.plot_confusion_matrix(y_test, y_test_pred)","3bea90ed":"rus = RandomUnderSampler(random_state=42)\nX_train_under, y_train_under = rus.fit_sample(X_train, y_train)","6c896b9d":"print(f\"After under sampling Positive Samples Count : {len(y_train_under[y_train_under == 1])}\")\nprint(f\"After under sampling Negitive Samples Count : {len(y_train_under[y_train_under == 0])}\")","16473dec":"model = LogisticRegression(C=0.01, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\nmodel.fit(X_train_under, y_train_under)\ny_pred = model.predict(X_train_under)\ny_test_pred = model.predict(X_test)\nprint(f\"Train AUC Score : {roc_auc_score(y_train_under, y_pred)}\")\nprint(f\"Test AUC Score : {roc_auc_score(y_test, y_test_pred)}\")","f6c58ffb":"skplot.metrics.plot_confusion_matrix(y_test, y_test_pred)","1013b143":"### When we using under sampling we do loose lots of data, in this example we lost approximately 98% of Non Fraud samples data. ","57ad40cb":"## Dataset is highly imbalanced data \n* we have approximately 99.8% negitive samples and only 0.2% of positive samples. We have to use some techniques to handle imbalanced data, otherwise model will favor to majority class","4a08a1a3":"### UNDER SAMPLING","94acec6e":"## We have 284807 samples and 31 features in given data set. \n* Except Time and Amount features remaining all independent features are names are hided and seems data normalized.\n\n* Bussiness Problem is By using independent features we do predict is Transaction is fraud or not.","edaa74a4":"* After oversampling data we can see exactly same number of samples in both positive and negitive Classes. Now we do use these data to train very simple Logistic Regression.","bc3865a4":"### Train and Test samples data validation based on Class value.","ea6ddee0":"#### We got around 92% AUC score on very simple Logistic Regression, but we didn't do any oversampling and undersampling on imbalanced data. Logistic Regression model has a \"class_weight\" attribute, if we set a attribute value as 'balanced', model will take care of imbalanced data.","7ba91e08":"### Data set we don't have any null values, so no need to handling null values based on different techniques.\n\n### Normalizing Amout and Time Features by using StandardScaler.","080804bb":"### Perfect! Train and Test data has distributed positive and negitive samples equally.","57589e09":"### We will use some Sampling techniques to handle imbalanced Data.\n\n* Undersampling : By using undersampling data we do decrease majority call samples and match both classes as same number[Randomly Choosing same number of minority class samples in majority samples]. This technique we do loss much information about original data, mostly not recomended if we have limited data.\n\n* Oversampling : It is exactly opposite to Undersampling technique, here we will generate more minority class samples which will equal to majority class samples. This technique mostly preferable because we don't loose any data here.\n\n\n### OVER SAMPLING","a006f519":"### Splitting Data into Train and Test Sets, here I am using Stratified Technique because we have very high imbalanced data. Random sampling technique might not be work properly if we have highly imbalanced data."}}