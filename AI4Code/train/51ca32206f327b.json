{"cell_type":{"7cda3855":"code","501111ca":"code","c938ff56":"code","41122150":"code","a79be677":"code","1c3334b2":"code","5888ec70":"code","3be48e00":"code","2c0d6e0a":"code","5da6acdf":"code","fbe9a3ce":"code","ee4f2c09":"markdown","6331ea32":"markdown","6642e04c":"markdown","7c6f4b7f":"markdown","3b6bcf3d":"markdown","4d78ed0a":"markdown","e9a74ca7":"markdown","b6db967d":"markdown","7baab608":"markdown","3797df4e":"markdown","ec5302fe":"markdown","80bc3bb7":"markdown","e7595c4f":"markdown","c9804354":"markdown"},"source":{"7cda3855":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nLABELS = [\"Normal\", \"Fraud\"]\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","501111ca":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv',sep=',')\ndata.head()","c938ff56":"data.info()","41122150":"count_class=pd.value_counts(data['Class'])\ncount_class.plot(kind='bar',rot=0)\nplt.title(\"Transaction Class Distribution\")\nplt.xticks(range(2), LABELS)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\") ","a79be677":"fraud = data[data['Class']==1]\nnormal = data[data['Class']==0]\nprint(fraud.shape,normal.shape)","1c3334b2":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(fraud.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(normal.Amount, bins = bins)\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","5888ec70":"import seaborn as sns\n#get correlations   of each features in dataset\ncorrmat = data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","3be48e00":"from sklearn.preprocessing import RobustScaler\n\nrob_scaler = RobustScaler()\ndata['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))\ndata.drop(['Time','Amount'], axis=1, inplace=True)\nscaled_amount = data['scaled_amount']\nscaled_time = data['scaled_time']\ndata.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndata.insert(0, 'scaled_amount', scaled_amount)\ndata.insert(1, 'scaled_time', scaled_time)\ndata.head()\n","2c0d6e0a":"Fraud = data[data['Class']==1]\nValid = data[data['Class']==0]\noutlier_fraction = len(Fraud)\/float(len(Valid))\nprint(outlier_fraction)\nprint(\"Fraud Cases : {}\".format(len(Fraud)))\nprint(\"Valid Cases : {}\".format(len(Valid)))","5da6acdf":"columns = data.columns.tolist()\n# Filter the columns to remove data we do not want \ncolumns = [c for c in columns if c not in [\"Class\"]]\n# Store the variable we are predicting \ntarget = \"Class\"\n# Define a random state \nstate = np.random.RandomState(42)\nX = data[columns]\nY = data[target]\nX_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\n# Print the shapes of X & Y\nprint(X.shape)\nprint(Y.shape)","fbe9a3ce":"classifier=IsolationForest(n_estimators=100, max_samples=len(X),contamination=outlier_fraction,random_state=state, verbose=0)\nn_outliers = len(Fraud)\nclassifier.fit(X)\nscores_prediction = classifier.decision_function(X)\ny_pred = classifier.predict(X)\n#Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\ny_pred[y_pred == 1] = 0\ny_pred[y_pred == -1] = 1\nn_errors = (y_pred != Y).sum()\n# Run Classification Metrics\nprint(\"{}: {}\".format('ISOLATION FOREST',n_errors))\nprint(\"Accuracy Score :\")\nprint(accuracy_score(Y,y_pred))\nprint(\"Classification Report :\")\nprint(classification_report(Y,y_pred))","ee4f2c09":"# Loading the dataset","6331ea32":"# Basic info on dataset","6642e04c":"# Correlation insight","7c6f4b7f":"# Calculating outlier fraction","3b6bcf3d":"# Getting target class insight","4d78ed0a":"# Modelling:Isolation forest algorithm","e9a74ca7":"In this notebook i have tried to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase using algorithm - **Isolation Forest Algorithm** which performs much far better than algo like SVM etc.I have implemented Exploratory Data Analysis with feature scaling to enhance the model.","b6db967d":"# Creating independent and Dependent Features","7baab608":"At the basis of the Isolation Forest algorithm there is the tendency of anomalous instances in a dataset to be easier to separate from the rest of the sample (isolate), compared to normal points. In order to isolate a data point the algorithm recursively generates partitions on the sample by randomly selecting an attribute and then randomly selecting a split value for the attribute, between the minimum and maximum values allowed for that attribute.\n\nIn simple language: The algorithm puts weighs on the leaf nodes according to the depth of the tree.So the points which are densely populated have higher weight as the depth is higher and on the other hand the outliers points as less in number weighs less thus seperating them in the algorithm.\nThis is how the isolation forest algo does the screening process of outliers and help in anamoly detection.","3797df4e":"# **EDA:**","ec5302fe":"# **Credit card fraud detection**","80bc3bb7":"If you liked my work, please upvote this kernel as it will  motivate me to perform more in-depth reserach towards this subject.","e7595c4f":"# Scaling Time and Amount cloumns","c9804354":"so we get that the fraud transactions are generally of far less amount as compared to the non-fraud ones."}}