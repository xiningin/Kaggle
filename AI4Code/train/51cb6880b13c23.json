{"cell_type":{"eba0fffc":"code","33e35a04":"code","7320ef70":"code","1b0a8c68":"code","2d6a8512":"code","2a61b291":"code","e4049ae6":"code","31fb7442":"code","f374a7bc":"code","54751352":"code","ecf58f16":"code","6a623370":"code","39b79ec7":"code","b19a9865":"code","cd1a674c":"code","ddcacb4e":"code","d28a3ce2":"code","d18e3d1e":"code","6e840c5f":"code","206b6873":"code","78193d2e":"code","8fa9f8a4":"code","f0249b0e":"code","a4d88b5e":"code","f4896fa6":"code","d3ca8b96":"code","79e8cf9c":"code","f56a6769":"code","fbc12dda":"code","03323dfd":"code","b9a6573c":"code","771cec57":"code","4da10bb1":"code","b31b29a7":"code","18700dd3":"code","0fc7d09a":"code","206884e3":"code","3b2625b6":"code","a43417ad":"code","d48da54e":"code","6d64cf15":"code","73f74796":"code","29d8def4":"code","42da8ff1":"code","08bd6ea2":"code","aab2ac89":"code","dc86fbf0":"code","cea969ee":"code","20cd08e7":"code","2adc122c":"code","0b8e2466":"code","2ee5f884":"code","6d606c53":"code","7c8d449a":"code","bf5482a7":"code","9371c439":"code","2ccd5637":"code","dd71f0ac":"code","d2077758":"code","0a368d60":"markdown","2aa5f9cb":"markdown","cfc26ea2":"markdown","6a14ec7f":"markdown","2f499796":"markdown","b8069b15":"markdown","1e413a64":"markdown","d1d83340":"markdown","a97d85cf":"markdown","eae9031f":"markdown","e1bc4b07":"markdown","5bd9146c":"markdown","4551ff27":"markdown","67c7f37f":"markdown","2b606b9c":"markdown","cae6c4ec":"markdown","80b7c839":"markdown","9c44721e":"markdown","d11f5a2c":"markdown","5786910c":"markdown","120ffae9":"markdown","a5aac05a":"markdown","333fa7a1":"markdown","e1445f5e":"markdown","e2774c46":"markdown","895fe8eb":"markdown","2f5c4783":"markdown","651c0640":"markdown","f9dd819d":"markdown","a41eef75":"markdown","16a054c5":"markdown","8df2751e":"markdown","6e721a94":"markdown","6f795d49":"markdown","4d0424ce":"markdown","5f164c5a":"markdown","83288d51":"markdown","2f703b1f":"markdown","d540e19f":"markdown","64935a37":"markdown","6e05d5c7":"markdown","39ed7868":"markdown","e977f234":"markdown","10050978":"markdown","ae9838a6":"markdown","5d40c225":"markdown"},"source":{"eba0fffc":"import os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport re\nimport plotly.express as px\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import LSTM\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional\nfrom keras.layers import Embedding\nfrom keras.layers import GlobalAvgPool1D\nimport tensorflow as tf\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","33e35a04":"data = pd.read_csv(\"..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\" , index_col=0)\ndata.head()","7320ef70":"data = data.drop(['Title', 'Clothing ID', 'Positive Feedback Count'], axis=1)\ndata.head()","1b0a8c68":"# Checking for the missing values\ncount_NaN = data.isna().sum()\ncount_NaN","2d6a8512":"# Dropping the missing values in the rows\ndata = data.dropna(subset=['Review Text', 'Division Name', 'Department Name', 'Class Name'], axis=0)\ndata = data.reset_index(drop=True)\n\n# Checking for the missing values after the drops\ncount_NaN_updated = data.isna().sum()\ncount_NaN_updated","2a61b291":"fig = px.histogram(data['Rating'],\n                   labels={'value': 'Rating',\n                           'count': 'Frequency',\n                           'color': 'Rating'}, color=data['Rating'])\nfig.update_layout(bargap=0.2)\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Distribution of the Ratings',\n                  title_x=0.5, title_font=dict(size=20))\nfig.show()","e4049ae6":"fig = px.histogram(data['Age'], marginal='box',\n                   labels={'value': 'Age'})\n\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Distribution of the Age of the Customers',\n                  title_x=0.5, title_font=dict(size=20))\nfig.show()","31fb7442":"labels = ['Recommended', 'Not Recommended']\nvalues = [data[data['Recommended IND'] == 1]['Recommended IND'].value_counts()[1],\n          data[data['Recommended IND'] == 0]['Recommended IND'].value_counts()[0]]\ncolors = ['green', 'red']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, opacity=0.8)])\nfig.update_traces(textinfo='percent+label', marker=dict(line=dict(color='#000000', width=2), colors=colors))\nfig.update_layout(title_text='Distribution of the Recommendations', title_x=0.5, title_font=dict(size=20))\nfig.show()","f374a7bc":"fig = px.histogram(data['Age'], color=data['Recommended IND'],\n                   labels={'value': 'Age',\n                           'color': 'Recommended'}, marginal='box')\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Distribution of the Age and Recommendation',\n                  title_x=0.5, title_font=dict(size=20))\nfig.update_layout(barmode='overlay')\nfig.show()","54751352":"fig = px.histogram(data['Rating'], color=data['Recommended IND'],\n                   labels={'value': 'Rating',\n                           'color': 'Recommended?'})\nfig.update_layout(bargap=0.2)\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Relationship between Ratings and Recommendation',\n                  title_x=0.5, title_font=dict(size=20))\nfig.update_layout(barmode='group')\nfig.show()","ecf58f16":"fig = px.histogram(data['Rating'], color=data['Department Name'],\n                   labels={'value': 'Rating',\n                           'color': 'Department Name'})\nfig.update_layout(bargap=0.2)\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Relationship between Ratings and Departments',\n                  title_x=0.5, title_font=dict(size=20))\nfig.update_layout(barmode='group')\nfig.show()","6a623370":"fig = px.histogram(data['Department Name'], color=data['Recommended IND'],\n                   labels={'value': 'Department Name',\n                           'color': 'Recommended?'})\nfig.update_layout(bargap=0.2)\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Department Name and Recommendation Distribution',\n                  title_x=0.5, title_font=dict(size=20))\nfig.update_layout(barmode='group')\nfig.show()","39b79ec7":"fig = px.histogram(data['Division Name'], color=data['Recommended IND'],\n                   labels={'value': 'Division Name',\n                           'color': 'Recommended?'})\nfig.update_layout(bargap=0.2)\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Division Name and Recommendation Distribution',\n                  title_x=0.5, title_font=dict(size=20))\nfig.update_layout(barmode='group')\nfig.show()","b19a9865":"data['length_of_text'] = [len(i.split(' ')) for i in data['Review Text']]\nfig = px.histogram(data['length_of_text'], marginal='box',\n                   labels={\"value\": \"Length of the Text\",\n                           \"color\": 'Recommended'},\n                   color=data['Recommended IND'])\n\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Distribution of the Length of the Texts',\n                  title_x=0.5, title_font=dict(size=20))\nfig.update_layout(barmode='overlay')\nfig.show()","cd1a674c":"FreqOfWords = data['Review Text'].str.split(expand=True).stack().value_counts()\nFreqOfWords_top200 = FreqOfWords[:200]\n\nfig = px.treemap(FreqOfWords_top200, path=[FreqOfWords_top200.index], values=0)\nfig.update_layout(title_text='Top Frequent 200 Words in the Dataset (Before Cleaning)',\n                  title_x=0.5, title_font=dict(size=20)\n                  )\nfig.update_traces(textinfo=\"label+value\")\nfig.show()","ddcacb4e":"# Lower Character all the Texts\ndata['Review Text'] = data['Review Text'].str.lower()\ndata['Review Text'].head()","d28a3ce2":"# Removing Punctuations and Numbers from the Text\ndef remove_punctuations_numbers(inputs):\n    return re.sub(r'[^a-zA-Z]', ' ', inputs)\n\n\ndata['Review Text'] = data['Review Text'].apply(remove_punctuations_numbers)","d18e3d1e":"def tokenization(inputs):  # Ref.1\n    return word_tokenize(inputs)\n\n\ndata['text_tokenized'] = data['Review Text'].apply(tokenization)\ndata['text_tokenized'].head()","6e840c5f":"stop_words = set(stopwords.words('english'))\nstop_words.remove('not')\n\n\ndef stopwords_remove(inputs):  # Ref.2\n    return [k for k in inputs if k not in stop_words]\n\n\ndata['text_stop'] = data['text_tokenized'].apply(stopwords_remove)\ndata['text_stop'].head()","206b6873":"lemmatizer = WordNetLemmatizer()\n\n\ndef lemmatization(inputs):  # Ref.1\n    return [lemmatizer.lemmatize(word=kk, pos='v') for kk in inputs]\n\n\ndata['text_lemmatized'] = data['text_stop'].apply(lemmatization)\ndata['text_lemmatized'].head()","78193d2e":"# Removing Words less than length 2\ndef remove_less_than_2(inputs):  # Ref.1\n    return [j for j in inputs if len(j) > 2]\n\n\ndata['final'] = data['text_lemmatized'].apply(remove_less_than_2)","8fa9f8a4":"# Joining Tokens into Sentences\ndata['final'] = data['final'].str.join(' ')\ndata['final'].head()","f0249b0e":"FreqOfWords = data['final'].str.split(expand=True).stack().value_counts()\nFreqOfWords_top200 = FreqOfWords[:200]\n\nfig = px.treemap(FreqOfWords_top200, path=[FreqOfWords_top200.index], values=0)\nfig.update_layout(title_text='Top Frequent 200 Words in the Dataset (After Cleaning)',\n                  title_x=0.5, title_font=dict(size=20)\n                  )\nfig.update_traces(textinfo=\"label+value\")\nfig.show()","a4d88b5e":"data_recommended = data[data['Recommended IND'] == 1]  # Dataframe that only includes recommended reviews\ndata_not_recommended = data[data['Recommended IND'] == 0]  # # Dataframe that only includes not recommended reviews\n\nWordCloud_recommended = WordCloud(max_words=500,\n                                  random_state=30,\n                                  collocations=True).generate(str((data_recommended['final'])))\n\nplt.figure(figsize=(15, 8))\nplt.imshow(WordCloud_recommended, interpolation='bilinear')\nplt.title('WordCloud of the Recommended Reviews', fontsize=20)\nplt.axis(\"off\")\nplt.show()","f4896fa6":"FreqOfWords = data_recommended['final'].str.split(expand=True).stack().value_counts()\nFreqOfWords_top200 = FreqOfWords[:200]\n\nfig = px.treemap(FreqOfWords_top200, path=[FreqOfWords_top200.index], values=0)\nfig.update_layout(title_text='Top Frequent 200 Words in the Recommended Reviews',\n                  title_x=0.5, title_font=dict(size=20)\n                  )\nfig.update_traces(textinfo=\"label+value\")\nfig.show()","d3ca8b96":"WordCloud_not_recommended = WordCloud(max_words=500,\n                                      random_state=30,\n                                      collocations=True).generate(str((data_not_recommended['final'])))\n\nplt.figure(figsize=(15, 8))\nplt.imshow(WordCloud_not_recommended, interpolation='bilinear')\nplt.title('WordCloud of the Not Recommended Reviews', fontsize=20)\nplt.axis(\"off\")\nplt.show()","79e8cf9c":"FreqOfWords = data_not_recommended['final'].str.split(expand=True).stack().value_counts()\nFreqOfWords_top200 = FreqOfWords[:200]\n\nfig = px.treemap(FreqOfWords_top200, path=[FreqOfWords_top200.index], values=0)\nfig.update_layout(title_text='Top Frequent 200 Words in the Not Recommended Reviews',\n                  title_x=0.5, title_font=dict(size=20)\n                  )\nfig.update_traces(textinfo=\"label+value\")\nfig.show()","f56a6769":"data['length_of_text'] = [len(i.split(' ')) for i in data['final']]\nfig = px.histogram(data['length_of_text'], marginal='box',\n                   labels={\"value\": \"Length of the Text\",\n                           \"color\": 'Recommended?'},\n                   color=data['Recommended IND'])\n\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Distribution of the Length of the Texts after Cleaning',\n                  title_x=0.5, title_font=dict(size=20))\nfig.update_layout(barmode='overlay')\nfig.show()","fbc12dda":"# I will only use Text data to predict Recommendation\ny = data['Recommended IND']\nX = data['final']\n\nX.head()","03323dfd":"# Train-Test-Validation Split\nx, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=13)  # Test: %20\n\nX_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.25, random_state=13)  # Val: %20\n\nprint('Shape of the X_train:', X_train.shape)\nprint('Shape of the X_test:', X_test.shape)\nprint('Shape of the X_val:', X_val.shape)\nprint('--'*20)\nprint('Shape of the y_train:', y_train.shape)\nprint('Shape of the y_test:', y_test.shape)\nprint('Shape of the y_val:', y_val.shape)","b9a6573c":"num_words = 10000\ntokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(X_train)\n\nTokenized_train = tokenizer.texts_to_sequences(X_train)\nTokenized_val = tokenizer.texts_to_sequences(X_val)\n\nprint('Non-tokenized Version: ', X_train[0])\nprint('Tokenized Version: ', tokenizer.texts_to_sequences([X_train[0]]))\nprint('--'*20)\nprint('Non-tokenized Version: ', X_train[80])\nprint('Tokenized Version: ', tokenizer.texts_to_sequences([X_train[80]]))","771cec57":"maxlen = 50\nPadded_train = pad_sequences(Tokenized_train, maxlen=maxlen, padding='pre')\nPadded_val = pad_sequences(Tokenized_val, maxlen=maxlen, padding='pre')","4da10bb1":"# Creating the Model\nmodel = Sequential()\n\nmodel.add(Embedding(num_words, 16, input_length=maxlen))\nmodel.add(Dropout(0.2))\n\nmodel.add(GlobalAvgPool1D())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nopt = tf.optimizers.Adam(lr=0.55e-3)  # Learning Rate\n\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nmodel.summary()","b31b29a7":"# Training the Model\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='auto', patience=5,\n                                                  restore_best_weights=True)\n\nepochs = 100\nhist = model.fit(Padded_train, y_train, epochs=epochs,\n                 validation_data=(Padded_val, y_val),\n                 callbacks=[early_stopping], batch_size=32)","18700dd3":"plt.figure(figsize=(15, 8))\nplt.plot(hist.history['loss'], label='Train Loss')\nplt.plot(hist.history['val_loss'], label='Validation Loss')\nplt.title('Train and Validation Loss Graphs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()","0fc7d09a":"plt.figure(figsize=(15, 8))\nplt.plot(hist.history['accuracy'], label='Train Accuracy')\nplt.plot(hist.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Train and Validation Accuracy Graphs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()","206884e3":"X_test = X_test.apply(tokenization)\nX_test = X_test.apply(stopwords_remove)\nX_test = X_test.apply(lemmatization)\nX_test = X_test.str.join(' ')\n\nX_test.head()","3b2625b6":"Tokenized_test = tokenizer.texts_to_sequences(X_test)\nPadded_test = pad_sequences(Tokenized_test, maxlen=maxlen, padding='pre')\n\ntest_evaluate = model.evaluate(Padded_test, y_test)","a43417ad":"pred_train_lstm = model.predict(Padded_train)\npred_test_lstm = model.predict(Padded_test)","d48da54e":"for i, x in enumerate(pred_test_lstm):\n    if 0 <= x < 0.49:\n        pred_test_lstm[i] = 0\n    else:\n        pred_test_lstm[i] = 1\n\nfor i, x in enumerate(pred_train_lstm):\n    if 0 <= x < 0.49:\n        pred_train_lstm[i] = 0\n    else:\n        pred_train_lstm[i] = 1\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=pred_test_lstm)\nplt.figure(figsize=(15, 8))\nsns.heatmap(conf_mat, annot=True, fmt='g')\nplt.title('Confusion Matrix of the Test Data', fontsize=14)\nplt.ylabel('Real Class', fontsize=12)\nplt.xlabel('Predicted Class', fontsize=12)\nplt.show()","6d64cf15":"# Accuracy\ntrain_acc_lstm = round(accuracy_score(y_train, pred_train_lstm) * 100, 2)\nprint('Train Accuracy of the LSTM: %', train_acc_lstm)\ntest_acc_lstm = round(accuracy_score(y_test, pred_test_lstm) * 100, 2)\nprint('Test Accuracy of the LSTM: %', test_acc_lstm)\nprint('--' * 20)\n\n# Precision\ntrain_precision_lstm = round(precision_score(y_train, pred_train_lstm) * 100, 2)\nprint('Train Precision of the LSTM: %', train_precision_lstm)\nprecision_lstm = round(precision_score(y_test, pred_test_lstm) * 100, 2)\nprint('Test Precision of the LSTM: %', precision_lstm)\nprint('--' * 20)\n\n# Recall\ntrain_recall_lstm = round(recall_score(y_train, pred_train_lstm) * 100, 2)\nprint('Train Recall of the LSTM: %', train_recall_lstm)\nrecall_lstm = round(recall_score(y_test, pred_test_lstm) * 100, 2)\nprint('Test Recall of the LSTM: %', recall_lstm)","73f74796":"def predict_recommendation(input_text):  # The function for doing all the previous steps\n    input_text = input_text.lower()\n    input_text = re.sub(r'[^a-zA-Z]', ' ', input_text)\n    input_text = tokenization(input_text)\n    input_text = stopwords_remove(input_text)\n    input_text = lemmatization(input_text)\n    input_text = ' '.join(input_text)\n    input_text = tokenizer.texts_to_sequences([input_text])\n    input_text = pad_sequences(input_text, maxlen=maxlen, padding='pre')\n    input_text = model.predict(input_text)\n    if input_text >= 0.5:\n        input_text = f'Recommended with %{round(float(input_text*100), 2)}'\n    else:\n        input_text = f'Not Recommended with %{round(float(input_text*100), 2)}'\n\n    return print(input_text)","29d8def4":"# This reviews above are taken from several websites for testing the model with real world data. You can find these websites in the Ref.5\npredict_recommendation(\"The clothes are such poor quality and look nothing like they do on the website. I order 2 packages of fast fashion a year just as a treat, and I sorely regret buying from here. Fabrics are cheaper than what they charge, their seems to be no thought of sizing consitency and so on\")","42da8ff1":"predict_recommendation(\"Beautiful colour of lemon great fit and length here in three days all l need is some fine weather to show if at it's best!!!!!\")","08bd6ea2":"predict_recommendation(\"As usual the clothes I ordered arrived quickly and were all a good fit, except for yoga pants , had to cit 4 ins off them but they are lovely pants. I shall wait for my next pay day!!\")","aab2ac89":"predict_recommendation(\"I should've checked reviews before ordering... each item they sent was much worse quality in person than how it appeared online, and one of the dresses looked NOTHING in person what they said it was! I even double-checked to make sure they didn't accidentally send me the wrong item. see photos below.\")","dc86fbf0":"predict_recommendation(\"cheap material that falls apart in seconds. Clothes look nothing like the pictures. I bought the chunky heeled shoes they broke after two times of wearing them.\")","cea969ee":"predict_recommendation(\"Very fast dispatch and delivery. Clothes are always a consistent fit, good quality and well priced. Couldn't ask for more! Will be using again and would happily recommend.\")","20cd08e7":"predict_recommendation(\"I have no complaints whatsoever, from ordering to getting my goods were excellent , down to the garments themselves, was as good as you see them on the website, I have shopped on here a few times and not disappointed at all, the only problem I have is that some trousers are a bit slim on leg and because I am a below knee amputee I have difficulty getting the right fit, otherwise very happy indeed.\")","2adc122c":"predict_recommendation(\"My dress had blue ink and biro stains on which was a real shame. I needed it for an outfit so had to put up with it but I'm not sure I'll order online again.\")","0b8e2466":"# Ref. 6 from now on\npredict_recommendation(\"Sizes varied despite allegedly being the same size. Some of the quality was poor. I had to send most of my order back (including the jeans I didn't order) Usually I get good stuff from Yours, so I was disappointed\")","2ee5f884":"predict_recommendation(\"I do really like yours clothing, just find the sizing is slightly off, a normal standard tshirt always seems too long and everything has dipped hems, doesn't look good when you're only short.\")","6d606c53":"predict_recommendation(\"I really love this dress. I ordered a large and it fits perfectly. There\u2019s about 1\/2\u201d that touches the ground, which could easily be fixed with a pair of wedges. The cut is flattering, flowing and hides my mom belly. I am bottom heavy and this dress accommodated everything just fine. It shows just a bit of cleavage and just a bit of knee. The fabric doesn\u2019t seem to need a slip. It gets a good breeze and looks pretty when you walk because of the ruffles. The color matched the picture exactly. If you\u2019re considering this dress, I say, yes, buy it!\")","7c8d449a":"predict_recommendation(\"I don't like writing negative reviews but this one pissed me off the second I pulled it out of the package. The fabric is the thinnest, cheapest crap I've ever seen used on a garment. None of the seams are finished properly with the seams visible on all the edges (see pictures for reference.) I just can't believe they're charging nearly $40 for this crap! I promptly returned it.\")","bf5482a7":"predict_recommendation(\"The cheapest material I've ever seen. It was like someone wove paper napkins from the dollar store together to make a dress. It felt like if the fabric ever got wet it would disintegrate. Ordered the royal blue color but the dress was actually purple. The cut was flattering. Shame that was the only good thing.\")","9371c439":"# Ref. 7 from now on\npredict_recommendation(\"I was so excited to receive this dress in the mail! The first day I wore it, I received so many compliments. The fit was true to size. I ordered an XS. Height 5'5, Weight 118, Bust 34B with an overall athletic build. I had no issues with the top of the dress as most of the review stated. If anything, I would suggest a pin. I would have rated this dress 5 stars however, I did have the side strap that holds the belt, break on the first day. I sowed it back into place but with the straps were a little more durable. Other than that, I love it!\")","2ccd5637":"predict_recommendation(\"The dress does not look like the dress pictures. The material seems cheaper and this does not fit well. It\u2019s way too big. I requested to return for a refund and now I have to pay to ship it back as well. If I could give zero stars I would. Would not recommend.\")","dd71f0ac":"predict_recommendation(\"I love this item it's was not dark blue like the picture but i love i it's very comfortable always wanted one I taken pics in side out to show the cord you can adjust also colour i would recommend this to anyone can we're all around the house also great to sleep in can not wait for my other two to arrive\")","d2077758":"predict_recommendation(\"This kaftan is NOT a silky material at all, it is a slightly transparent and dull cotton material. It is NOT as bright nor pretty as in the picture, it's more of a navy colour. When it arrived it looked worn, not ironed and had 2 small patches of damage, like someone had tried it on and maybe some jewellery had gotten stuck upon the material which then had been pulled off. I have included a picture of the damage that I took on my phone.\")","0a368d60":"**As you can see from the Treemap above, all of the words are unique words and there are no stopwords in this set. Most words are 'dress', 'fit' and 'size'. Due to we are dealing with the clothing review dataset, this is pretty reasonable.**","2aa5f9cb":"# Distribution of the Ratings","cfc26ea2":"# Lemmatization","6a14ec7f":"**According to the graph above, frequency of the Rating 5 is pretty high compared to the others.**","2f499796":"# References\n\n[1] https:\/\/medium.com\/analytics-vidhya\/text-preprocessing-for-nlp-natural-language-processing-beginners-to-master-fd82dfecf95\n\n[2] https:\/\/stackabuse.com\/removing-stop-words-from-strings-in-python\/\n\n[3] https:\/\/www.geeksforgeeks.org\/python-lemmatization-with-nltk\/\n\n[4] https:\/\/www.kaggle.com\/rmisra\/news-headlines-dataset-for-sarcasm-detection\n\n[5] https:\/\/uk.trustpilot.com\/review\/www.yoursclothing.co.uk?stars=4\n\n[6] https:\/\/www.amazon.co.uk\/product-reviews\/B08FYWYMVL\/ref=cm_cr_unknown?ie=UTF8&filterByStar=four_star&reviewerType=all_reviews&pageNumber=1#reviews-filter-bar\n\n[7] https:\/\/www.amazon.co.uk\/product-reviews\/B014R7EZT8\/ref=cm_cr_getr_d_paging_btm_next_5?ie=UTF8&filterByStar=one_star&reviewerType=all_reviews&pageNumber=5#reviews-filter-bar","b8069b15":"# Stopwords Removal","1e413a64":"**I would love to have your feedbacks and suggestions. Please leave your comments about data preprocessing steps, learning curves and final evaluation resuls!**","d1d83340":"**It is impossible to replace the Review Text features. Therefore, I will drop the missing rows from the dataset.**","a97d85cf":"**According to this graph above, almost all the Rating 5 and Rating 4 data points are recommended.**\n\n**In addition, Rating 1 and Rating 2 data points have almost no recommendations.**\n\n**For the further steps, I would create a common rating point with the Rating 4 and Rating 5 as well as Rating 1 and Rating 2. In this way, I would shrink the labels therefore, the model would perform better.**","eae9031f":"# Relationship between Ratings and Recommendation","e1bc4b07":"# Having Fun with the LSTM Model","5bd9146c":"**Tokenizing with NLTK will help me to clean the dataset for better model training.**","4551ff27":"# <center> ANN Model Creation\n","67c7f37f":"# Train and Validation Loss Graphs\n","2b606b9c":"# Division and Recommendation Distribution\n","cae6c4ec":"# Tokenizing with NLTK","80b7c839":"# Top Frequent 200 Words in the Dataset (After Cleaning)","9c44721e":"# Distribution of the Age and Recommendation","d11f5a2c":"**As you can see from the head of the dataset, we have some unnecessary features such as Clothing ID, Title. First of all, I will drop this features.**","5786910c":"**As you can see from the figure above, Recommended and not Recommended products almost have the same distribution length of text.**","120ffae9":"# Preparing the Test Data\n","a5aac05a":"# Top Frequent 200 Words in the Dataset (Before Cleaning)","333fa7a1":"# Distribution of the Recommendations","e1445f5e":"**As you can see from the 'Distribution of the Age of the Customers' graph, the age of the customers is usually distributed between 34 and 52. We have outliers that customers older than 80.**","e2774c46":"# Aim of the Notebook\n\n**Hello, welcome to this Notebook!**\n\n**In this notebook, I will be working on the Women's E-Commerce Clothing Review dataset.**\n\n**First of all, I will make some exploratory data analysis for the features and the texts as well. And then, I will clean the text data using by NLTK library.**\n\n**For the next step, I will create an ANN model to achieve my aim.**\n\n**I am open to feedback and suggestions, feel free to comment your feedback and suggestions on the comment section or contact me.**\n\n**So, let's get started!**","895fe8eb":"# Train and Validation Accuracy Graphs","2f5c4783":"**According to this Treemap above, the top frequent 200 words usually include a stopword. For the further step of this notebook, I will remove them from the text.**","651c0640":"# WordCloud of the Recommended Reviews","f9dd819d":"# WordCloud of the Not Recommended Reviews","a41eef75":"# Relationship between Ratings and Departments","16a054c5":"# Importing the Libraries","8df2751e":"**According to the graph above, Tops and Dresses have the most of the rating points. Trend and Jackets have the least.**","6e721a94":"# Distribution of the Length of the Texts after Cleaning","6f795d49":"# Tokenizing with Tensorflow\n","4d0424ce":"# <center> Data Preprocessing\n\n**In this step I will be dealing with the cleaning text data.**","5f164c5a":"# Department and Recommendation Distribution","83288d51":"**According to this pie chart, the most of the sales are Recommended.**","2f703b1f":"# <center> E-Commerce Clothing Review Classification with TF","d540e19f":"# Confusion Matrix of the Test Data","64935a37":"**In this section, I will remove all punctuations and numbers from the all dataframe. They will be not usefull for my model training.**","6e05d5c7":"# Evaluation Metrics of the LSTM Model","39ed7868":"**It looks better now!**","e977f234":"# Train-Test-Validation Split\n","10050978":"# Distribution of the Length of the Texts","ae9838a6":"# Distribution of the Age of the Customers","5d40c225":"# Padding the Datasets\n"}}