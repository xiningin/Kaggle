{"cell_type":{"95a58f09":"code","df16c0e6":"code","fba30ce2":"code","a47f9279":"code","56b35933":"code","5529aa54":"code","a29b840a":"code","e2d179e1":"code","5fb78d53":"code","16b6e1bf":"code","0b5cb873":"code","1e8c795a":"code","341ea4c1":"code","35552ade":"code","773b4479":"code","75ab76c4":"code","3ee2cab2":"code","cdc68d50":"code","d5af12a6":"code","11a28aa8":"code","b834ef2f":"code","4f45ccf6":"code","63c4ac3d":"code","9081c56d":"code","a94a6545":"code","a6b23253":"code","e6cef90c":"code","3e0bdfd6":"code","ed6021c6":"code","1e62f2b8":"code","93399f4a":"code","e5394233":"code","9a398e27":"code","f27f3016":"code","68a78eb7":"code","bfcebbd3":"code","0460da00":"code","b7342605":"code","2fd52b5d":"code","a2a341d1":"code","de2690f9":"code","c5abde40":"code","4599efd1":"code","f0a78c5c":"code","58a13c92":"code","bfc28400":"code","7640cc96":"code","36ee3332":"code","55e1a0f4":"code","99602e1e":"code","c91c70e0":"code","4686a8d6":"markdown","f179989f":"markdown","62d60f4c":"markdown","b5c30aa1":"markdown","d6ba0d5e":"markdown","60cd51a8":"markdown","d4a35f11":"markdown","ce49af22":"markdown","9f566a56":"markdown","2bb5f33e":"markdown","66a9dffc":"markdown","20f90999":"markdown","7a54d93e":"markdown","141924eb":"markdown","5616e16d":"markdown","59682799":"markdown"},"source":{"95a58f09":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","df16c0e6":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')","fba30ce2":"df=pd.read_csv('..\/input\/income-classification\/income_evaluation.csv')\ndf.columns.tolist()\nprint(df.shape)\ndf.head()","a47f9279":"df.info()","56b35933":"# In columns names there is space at the start of columns so, remove this\ndf.columns=df.columns.str.replace(' ','')\ndf.columns","5529aa54":"df.workclass.unique()","a29b840a":"df.education.unique().tolist()","e2d179e1":"df['education-num'].unique()","5fb78d53":"df['marital-status'].unique()","16b6e1bf":"df.occupation.unique()","0b5cb873":"income_data=df['income'].value_counts().reset_index()\nincome_data.columns=['income','frequency']\n\nplt.bar(income_data['income'],income_data['frequency'],color=['red','pink']);\n# imblance classes, huge difference","1e8c795a":"df.isnull().sum()","341ea4c1":"df.describe()","35552ade":"sns.boxplot(x=df['income'],y=df['fnlwgt']);\n# There are outliers","773b4479":"# X=df[['age','fnlwgt','education-num','sex','capital-gain','capital-loss','hours-per-week']]\n# # x.info()\n# X['sex']=X['sex'].astype(\"category\")\n\n# here is the 'sex' column which is categorical so, we have to label this column...","75ab76c4":"##### x.info()\n##### x['sex'] = x['sex'].cat.codes\n##### x['sex']","3ee2cab2":"# from sklearn.preprocessing import LabelEncoder\n# labelencoder=LabelEncoder()\n# x['sex'] = labelencoder.fit_transform(x['sex'])\n# x['sex'][1:30]","cdc68d50":"# OneHotEncoder from SciKit library only takes numerical categorical values\n\n# x['sex'] = x['sex'].cat.codes\n# from sklearn.preprocessing import OneHotEncoder\n# enc = OneHotEncoder(handle_unknown='ignore')\n# enc_df=pd.DataFrame(enc.fit_transform(df[['sex']]).toarray())\n# df = df.join(enc_df)\n# df.head()","d5af12a6":"# x1 = pd.DataFrame(x['sex'], columns=['sex'])\n# # generate binary values using get_dummies\n# dum_df = pd.get_dummies(x1, columns=[\"sex\"], prefix=[\"Type_is\"] )\n# # merge with main df bridge_df on key values\n# x = x.join(dum_df)\n# x","11a28aa8":"# prepare data for model\n\nX=df[['age','fnlwgt','education-num','sex','capital-gain','capital-loss','hours-per-week']]\nX['sex']=X['sex'].astype(\"category\")\nY=df['income']\nX_train,X_test,y_train,y_test=train_test_split(X,Y,random_state=1,train_size=0.75)\n\nlabelencoder=LabelEncoder()\n\nX_train['sex'] = labelencoder.fit_transform(X_train['sex'])\nX_test['sex'] = labelencoder.transform(X_test['sex'])\n\ny_train=labelencoder.fit_transform(y_train)\ny_test=labelencoder.transform(y_test)","b834ef2f":"fig,ax=plt.subplots(1,2,figsize=(10, 3))\nsns.countplot(X_train['sex'],label='count',ax=ax[0])\nax[0].set_title('Train data')\n\nsns.countplot(X_test['sex'],label='count',ax=ax[1])\nax[1].set_title('test data')\nplt.show()","4f45ccf6":"fig,ax=plt.subplots(1,2,figsize=(10, 3))\nsns.countplot(y_train,label='count',ax=ax[0])\nax[0].set_title('y_train')\nsns.countplot(y_test,label='count',ax=ax[1])\nax[1].set_title('y_test')\nplt.show()","63c4ac3d":"lr=LogisticRegression()\nlr.fit(X_train, y_train)","9081c56d":"y_pred=lr.predict(X_test)","a94a6545":"cm=confusion_matrix(y_test,y_pred)\nprint('Confusion Matrix : \\n',cm)\nsns.heatmap(cm,annot=True);\nprint('\\n')\nprint('classification_report : \\n',classification_report(y_test,y_pred))","a6b23253":"print('Accuracy: ',round(accuracy_score(y_test,y_pred)*100,2)) # TP+TN\/total is accuracy\n# print((cm[0,0]+cm[1,1])\/cm.sum())\n\n# Missclassification Rate: Overall how often is it wrong (overall incorrect prediction)\n# FP+FN\/total\nprint('Missclassification Rate: ',round((cm[0,1]+cm[1,0])\/cm.sum()*100,2))","e6cef90c":"print('Recall: ',round((cm[1,1]\/(cm[1,1]+cm[1,0]))*100,2)) # Recall=TP\/TP+FN(actual)\n\nprint('Precision: ',round((cm[1,1]\/(cm[1,1]+cm[0,1]))*100,2)) # TP\/TP+FP(predicted)","3e0bdfd6":"pd.Series(y_pred).value_counts() # predicted by the model 0 and 1 sample","ed6021c6":"pd.Series(y_test).value_counts() # test 0 and 1 sample","1e62f2b8":"pd.Series(y_test).value_counts().sum() # total sample","93399f4a":"svclassifier=SVC(kernel='rbf')","e5394233":"svclassifier.fit(X_train, y_train)","9a398e27":"y_pred=svclassifier.predict(X_test)","f27f3016":"cm=confusion_matrix(y_test,y_pred)\nprint('confusion_matrix:\\n',cm)","68a78eb7":"print('Accuracy:',round(accuracy_score(y_test,y_pred)*100,2))","bfcebbd3":"X=df[['age','fnlwgt','education-num','sex','capital-gain','capital-loss','hours-per-week']]\nlabelencoder=LabelEncoder()\nX['sex']=labelencoder.fit_transform(X['sex'])\nY=df['income']\nY=labelencoder.fit_transform(Y)","0460da00":"kfold_val=KFold(10)\nsvclassifier=SVC(kernel='rbf')\ncross_val_result=cross_val_score(svclassifier,X,Y,cv=kfold_val)\ncross_val_result","b7342605":"print('average accuracy : ',np.mean(cross_val_result))\nprint('min. accuracy : ',cross_val_result.min())\nprint('max. accuracy : ',cross_val_result.max())","2fd52b5d":"stratified=StratifiedKFold(n_splits=5) # it is use when data contain imbalanced classes\ncross_val_result=cross_val_score(svclassifier,X,Y,cv=stratified)\ncross_val_result","a2a341d1":"print('average accuracy : ',np.mean(cross_val_result))\nprint('min. accuracy : ',cross_val_result.min())\nprint('max. accuracy : ',cross_val_result.max())","de2690f9":"scaler=MinMaxScaler()\nScaled_X_train=scaler.fit_transform(X_train)\nScaled_X_test=scaler.transform(X_test)\n\ngrid_param={'C':[0.5,1.0,10.0,100.0],'kernel':['rbf','sigmoid']}\ngridsvclassifier=GridSearchCV(SVC(),grid_param)\ngridsvclassifier.fit(Scaled_X_train, y_train)\ny_pred=gridsvclassifier.predict(Scaled_X_test)\ncm=confusion_matrix(y_test,y_pred)","c5abde40":"print('confusion_matrix:\\n',cm)\nprint('\\nAccuracy:',round(accuracy_score(y_test,y_pred)*100,2))\nprint('\\nBest Parameters:',gridsvclassifier.best_params_)\nprint('\\nBest Estimator:',gridsvclassifier.best_estimator_)\nprint('\\nBest Score:',gridsvclassifier.best_score_)\nprint('\\nBest Index:',gridsvclassifier.best_index_)","4599efd1":"treeclr=DecisionTreeClassifier()\ntreeclr.fit(X_train, y_train)","f0a78c5c":"y_pred=treeclr.predict(X_test)\ntree_cm=confusion_matrix(y_test,y_pred)\nprint('confusion_matrix:\\n',tree_cm)\nprint('\\naccuracy:',round(accuracy_score(y_test,y_pred)*100,2))","58a13c92":"treeclr_scaled=DecisionTreeClassifier()\ntreeclr_scaled.fit(Scaled_X_train, y_train)","bfc28400":"y_pred1=treeclr_scaled.predict(Scaled_X_test)\ntree_cm1=confusion_matrix(y_test,y_pred1)\nprint('confusion_matrix:\\n',tree_cm1)\nprint('\\nAccuracy:',round(accuracy_score(y_test,y_pred1)*100,2))","7640cc96":"model=RandomForestClassifier()\nmodel.fit(X_train,y_train)\ny_pred1=model.predict(X_test)","36ee3332":"cm2=confusion_matrix(y_test,y_pred1)\nprint('confusion_matrix:\\n',cm2)\nprint('\\naccuracy:',round(accuracy_score(y_test,y_pred1)*100,2))","55e1a0f4":"importance_value=np.round((model.feature_importances_)*100,2)\n# print(importance_value)\n# print('\\n')\n# for c,i in zip(X_train.columns,importance_features):\n#     print('Feature is {0} and importance is {1}'.format(c,i))\nimportance_features=[(feature,importance1) for feature,importance1 in zip(X_train.columns,importance_value)]\nimportance_features=sorted(importance_features,key=lambda x: x[1],reverse=True) \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in importance_features];","99602e1e":"# use magic command for Jupyter Notebooks\n%matplotlib inline\n# Set the style\nplt.style.use('fivethirtyeight')\n# list of x locations for plotting\nx_values = list(range(len(importance_value)))\n# Make a bar chart\nplt.bar(x_values, importance_value, orientation = 'vertical')\n# Tick labels for x axis\nplt.xticks(x_values, X_train.columns, rotation='vertical')\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');","c91c70e0":"# it can also be done\n# converting into DataFrame\nimportance_data=pd.DataFrame(importance_features,columns=['features','importance'])\nimportance_data\n\n# plotting\nplt.figure(figsize=(8,5))\nplt.barh(importance_data['features'],importance_data['importance'])\nplt.gca().invert_yaxis()\nplt.show()","4686a8d6":"### label-encoding method # 01","f179989f":"### Random Forest","62d60f4c":"some algorithm preform well on categorical variable like decision tree but some \nrequired to convert categorical to numerical form","b5c30aa1":"### Let's prepare the data for model","d6ba0d5e":"### LogisticRegression","60cd51a8":"So, In different algorithms Random Forest and Support Vector Machine with rbf kernel and C=100.0 perform well.","d4a35f11":"### Recall or True Positive Rate (TPR) or Sensitivity or Probability of detection\n##### When it's actually yes, how often does it predict yes?\n##### Recall=TP\/TP+FN(actual)\n\n### precision\n##### When it predicts yes, how often is it correct? or what fraction of positive prediction \n##### are correct\n##### TP\/TP+FP(predicted)\n\n### True Negative Rate (TNR) or Specificity\n##### When it's actually no, how often does it predict no?\n##### TNR=TN\/TN+FP\n\n### False Positive Rate (FPR) or 1 - specificity\n##### When it's actually no, how often does it predict yes? or what fraction of all negetive\n##### instance does the classifier incorrectly indentify as positive\n##### FPR=FP\/TN+FP\n\n### Missclassification Rate\n##### Overall how often is it wrong (overall incorrect prediction)\n##### FP+FN\/total\n\n### F1-score\n##### It is difficult to compare two models with low precision and high recall or vice versa. \n##### Diffcult to distiguish between precision and recall for the project.\n##### So to make them comparable, we use F1-Score. F1-score helps to measure Recall and Precision \n##### at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme \n##### values more.It is maximum when Precision is equal to Recall.\n##### F1=2*((precision*recall)\/(precision+recall))","ce49af22":"### Cross Validation","9f566a56":"### Decision Tree with Normalize data","2bb5f33e":"### Decision Tree","66a9dffc":"### Normalization and GridSearch ","20f90999":"### label-encoding method # 02 (using sklearn)","7a54d93e":"### one-hot-encoding method # 02 (using get_dummies methods)","141924eb":"There is two main labelling concepts\n\none is **label-encoding** and other is **one-hot-encoding**\n\n**Remember perform encoding after train test split**\n\n##### If you have some kind of order and have more categories then you use label-encoding\n##### If you don't have any order and have comparitively less categories then you should \n##### choose one-hot-encoding\n**There is different methods for both encodings in python**\n","5616e16d":"### one-hot-encoding method # 01 (using sklearn)","59682799":"### SVM"}}