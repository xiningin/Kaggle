{"cell_type":{"f4b0eee4":"code","1aaac60a":"code","8efcad43":"code","5cac412a":"code","ba5a33c4":"code","02cbdda6":"code","1f1f3edb":"code","44707e12":"code","916f67f3":"code","5a4100c4":"code","695f02c9":"code","0d883a64":"code","c1b2e189":"code","493e42a4":"code","304cc2ce":"code","eec778c1":"code","3572dc3b":"markdown","8ec5c680":"markdown","72d58e23":"markdown","fb93ed80":"markdown","a3a6c334":"markdown","e49ca96b":"markdown","b7992e19":"markdown","7618279b":"markdown","1bb181c5":"markdown","bb89531f":"markdown","c9d8ecf3":"markdown","e724648d":"markdown","d3db92cb":"markdown","84f793ab":"markdown","fe50b990":"markdown","a2027444":"markdown","b47ad7f8":"markdown","138f0505":"markdown","b64b5609":"markdown","741d86b1":"markdown","5bcd9431":"markdown","68da5647":"markdown","0ca575b1":"markdown","342a9e68":"markdown","af1b7ce2":"markdown","1d6f62ff":"markdown","74f0aee6":"markdown"},"source":{"f4b0eee4":"text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n# Split text by whitespace\ntokens = text.split()\nprint(tokens)","1aaac60a":"# Lets split the given text by full stop (.)\ntext = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\ntext.split(\". \") # Note the space after the full stop makes sure that we dont get empty element at the end of list.","8efcad43":"import re\n\ntext = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\ntokens = re.findall(\"[\\w]+\", text)\nprint(tokens)","5cac412a":"import re\ntext = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n#text = \"My name is Surabh. What is your name?\"\ntokens_sent = re.compile('[.!?] ').split(text) # Using compile method to combine RegEx patterns\ntokens_sent","ba5a33c4":"!pip install --user -U nltk","02cbdda6":"from nltk.tokenize import word_tokenize\n\ntext = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\ntokens = word_tokenize(text)\nprint(tokens)","1f1f3edb":"from nltk.tokenize import sent_tokenize\n\ntext = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\nsent_tokenize(text)","44707e12":"!pip install spacy\n!python -m spacy download en","916f67f3":"# Load English model from spacy\nfrom spacy.lang.en import English\n\n# Load English tokenizer. \n# nlp object will be used to create 'doc' object which uses preprecoessing pipeline's components such as tagger, parser, NER and word vectors\nnlp = English()\n\ntext = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n\n# Now we will process above text using 'nlp' object. Which is use to create documents with linguistic annotations and various nlp properties\nmy_doc = nlp(text)\n\n# Above step has already tokenized our text but its in doc format, so lets write fo loop to create list of it\ntoken_list = []\nfor token in my_doc:\n    token_list.append(token.text)\n\nprint(token_list)","5a4100c4":"# Load English tokenizer, tager, parser, NER and word vectors\nnlp = English()\n\n# Create the pipeline 'sentencizer' component\nsbd = nlp.create_pipe('sentencizer')\n\n# Add component to the pipeline\nnlp.add_pipe(sbd)\n\ntext = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n\n# nlp object is used to create documents with linguistic annotations\ndoc = nlp(text)\n\n# Create list of sentence tokens\n\nsentence_list =[]\nfor sentence in doc.sents:\n    sentence_list.append(sentence.text)\nprint(sentence_list)","695f02c9":"!pip install Keras","0d883a64":"from keras.preprocessing.text import text_to_word_sequence\n\ntext = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n\ntokens = text_to_word_sequence(text)\nprint(tokens)","c1b2e189":"from keras.preprocessing.text import text_to_word_sequence\n\ntext = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n\ntext_to_word_sequence(text, split= \".\", filters=\"!.\\n\")","493e42a4":"!pip install gensim","304cc2ce":"from gensim.utils import tokenize\n\ntext = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n\ntokens = list(tokenize(text))\nprint(tokens)","eec778c1":"#from gensim.summarization.textcleaner import split_sentences\n\n#text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n\n#list(split_sentences(text))","3572dc3b":"Notice that NLTK word tokenization also consider the punctuation as token. During text cleaning process we have to account for this.","8ec5c680":"Based on RegEx pattern we are able to generate the list of words. Details about each character in our RegEx pattern is as below.\n```\n[] :\tA set of characters.\n\\w :    Returns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character).\n+  :\tOne or more occurrences.\n```\n\nSo our RegEx pattern signifies that the code should find all the alphanumeric characters until any other character is encountered.","72d58e23":"### Word Tokenization","fb93ed80":"Observe in above list, words like 'language,' and  'modeling.' are containing punctuation at the end of them. **Python split method do not consider punctuation as separate token.**","a3a6c334":"## Tokenization Using spaCy <a id =\"7\"><\/a>\n* spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython\n* in spaCy we create language model object, which then used for word and sentence tokenization\n* Syntax to install spaCy library and English model is as below\n```\n!pip install spacy\n!python -m spacy download en\n```\n* Note that we are going use \"!\" before the command to let notebook know that, it should read as commandline command\n\n### Word Tokenization","e49ca96b":"As you can notice, all words are also converted to lowercase. This is default behavior we can change it by changing the arguments e.g. text_to_word_sequence(text,lower=False)\n\n### Sentence Tokenization\nFor sentence tokenization we can use filters like \"!.\\n\" to split the text into sentences.","b7992e19":"As you can see, split() since we can't use multiple separator split() method failed to split the last sentence from separator (!). We can overcome this drawback by applying split method multiple times with different separator but there are better ways to do it.","7618279b":"## Tokenization using Keras <a id =\"8\"><\/a>\n* Keras is opensource neural network library written in python. It is easy to use and it is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML\n* To perform word tokenization we use the **text_to_word_sequence()** method from the **keras.preprocessing.text class**\n* By default, this function automatically does 3 things:\n    * Splits words by space (split=\u201d \u201c).\n    * Filters out punctuation (filters=\u2019!\u201d#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u2019).\n    * Converts text to lowercase (lower=True).\n* Syntx to install Keras\n```\n!pip install Keras\n```","1bb181c5":"![NLP_Header_Tokenization](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/NLP_Header_Tokenization.png)","bb89531f":"As you can see from above result, we are able to split sentence using multiple separators.","c9d8ecf3":"# Conclusion <a id =\"10\"><\/a>\nThere are multiple ways to do the tokenization. We can use any library depending on our requirement and features supported by the library. Feel free to try above code with different text snippet to get hold of how tokenization work.","e724648d":"### Word Tokenization","d3db92cb":"# Tokenization Techniques <a id =\"3\"><\/a>\nThere are multiple ways we can perform tokenization on given text data. We can choose any method based on language, library and purpose of modeling.\n\n## Tokenization Using Python's Inbuilt Method <a id =\"4\"><\/a>\n\n![NLP_Tokenization](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/python_split_syntax.png)\n\n* We can use **split()** method to split a string into a list where each word is a list item.\n* By default split() use whitespace as separater, but we can change it to anything.\n\n### Word Tokenization","84f793ab":"The gensim.summarization module has been removed in versions Gensim 4.x because it was an unmaintained third-party module.\nFor using the below code your Gemsim version should be less than 4.","fe50b990":"## Tokenization Using Regular Expressions(RegEx) <a id =\"5\"><\/a>\n\n![python_regex_syntax](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/python_regex_syntax.png)\n\n* A regular expression is a sequence of characters that define a search pattern.\n* Using RegEx we can match character combinations in string and perform word\/sentence tokenization.\n* Please refer [regex101](https:\/\/regex101.com\/) for testing your regular expression syntax.\n* We can use Python's **re** library for RegeEx related operations.\n","a2027444":"# Why Tokenization is Required? <a id =\"2\"><\/a>\nEvery sentence gets its meaning by the words present in it. So by analyzing the words present in the text we can easily interpret the meaning of the text. Once we have a list of words we can also use statistical tools and methods to get more insights into the text. For example, we can use word count and word frequency to find out important of word in that sentence or document.","b47ad7f8":"### Sentence Tokenization","138f0505":"## Tokenization using Gensim <a id =\"9\"><\/a>\n* Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.\n* We are going to use **tokenize()** from **gensim.utility** class for word tokenization.\n* Unlike other libraries Gensim has separate method **split_sentences()** from class **gensim.summarization.textcleaner** for sentence tokenization. \n* Syntx to install Gensim\n```\n!pip install gensim\n```","b64b5609":"### Sentence Tokenization","741d86b1":"### Sentence Tokenization","5bcd9431":"# Index\n\n* [Introduction](#1)\n* [Why Tokenization is Required?](#2)\n* [Tokenization Techniques](#3)\n  - [Tokenization Using Python's Inbuilt Method](#4)\n  - [Tokenization Using Regular Expressions(RegEx)](#5)\n  - [Tokenization Using NLTK](#6)\n  - [Tokenization Using spaCy](#7)\n  - [Tokenization using Keras](#8)\n  - [Tokenization using Gensim](#9)\n* [Conclusion](#10)\n* [References](#11)\n\n**Tutorial contains friendly description of multiple tokenization methods and python code.**","68da5647":"# References <a id =\"11\"><\/a>\n* https:\/\/keras.io\/api\/preprocessing\/text\/#text_to_word_sequence\n* https:\/\/www.nltk.org\/\n* https:\/\/machinelearningmastery.com\/prepare-text-data-deep-learning-keras\/\n* https:\/\/towardsdatascience.com\/tokenization-for-natural-language-processing-a179a891bad4\n* https:\/\/www.analyticsvidhya.com\/blog\/2019\/07\/how-get-started-nlp-6-unique-ways-perform-tokenization\/\n","0ca575b1":"# Introduction <a id =\"1\"><\/a>\n\nTokenization is one of the first step in any NLP pipeline. Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens. If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'. Generally 'space' is used to perform the word tokenization and characters like 'periods, exclamation point and newline char are used for Sentence Tokenization.  We have to choose the appropriate method as per the task in hand. While performing the tokenization few characters like spaces, punctuations are ignored and will not be the part of final list of tokens.\n\n![NLP_Tokenization](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/NLP_Tokenization.png)","342a9e68":"## Tokenization Using NLTK <a id =\"6\"><\/a>\n* Natural Language Toolkit (NLTK) is library written in python for natural language processing.\n* NLTK has module **word_tokenize()** for word tokenization and **sent_tokenize()** for sentence tokenization.\n* Syntax to install NLTK is as below\n```\n!pip install --user -U nltk\n```\n* Note that we are going use \"!\" before the command to let notebook know that, it should read as commandline command\n\n### Word Tokenization","af1b7ce2":"### Sentence Tokenization","1d6f62ff":"### Word Tokenization","74f0aee6":"### Sentence Tokenization"}}