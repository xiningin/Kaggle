{"cell_type":{"b9e17ac3":"code","8e54311f":"code","c9b21959":"code","d189960f":"code","a8b28502":"code","744f4c25":"code","6090931f":"code","872fd0fb":"code","f1cc0dba":"code","7149f094":"code","cca98b88":"code","a5da176f":"code","11c53579":"code","00582795":"code","94eb790e":"code","2f0d3767":"code","53089adc":"code","0f3b1b8e":"code","db535323":"code","88d9301a":"code","397b1b47":"code","48ecc6e4":"code","58f027f1":"code","88adcc82":"code","77130817":"code","1f054d1e":"code","f72b1623":"code","052b865d":"code","71ee9eb2":"code","46960070":"code","0c348abb":"code","d210de62":"code","029614d0":"code","b6f5f7f5":"code","3d92bc74":"code","2bc69126":"code","9e0b51a0":"code","38db7ea2":"code","c5135368":"code","7852dfaf":"code","2bfe8484":"code","9573a4b1":"code","3a9a0868":"code","2e1d1ba5":"code","1f4ca53f":"code","e505a34a":"code","fc3d6064":"code","25149100":"code","062a1ceb":"code","2f4bc503":"code","07e92ef8":"code","57abccbe":"code","a4d66025":"code","50a1dbd8":"code","40720aa1":"code","3178b030":"code","10334348":"code","59e69393":"code","f16dded2":"code","d939ba1c":"code","388096d1":"code","ba64778f":"code","1669916d":"code","edf72140":"code","093e0436":"code","d1e3f269":"code","67a2dab4":"code","682d9475":"code","f593dc57":"code","71d4a76d":"code","f307e7d1":"code","a28c7646":"code","e57b3219":"code","f9105cd6":"code","12346632":"code","518b9e8f":"code","91eaaae3":"code","769cf9dc":"code","71fb9892":"code","6fac3953":"code","236a8521":"code","509cd245":"code","d0bd686e":"code","279f9f2a":"code","b65065f2":"code","11f305f4":"code","4bbe78e8":"code","8d0b78e9":"markdown","21c3da0d":"markdown","17396934":"markdown","2262ce48":"markdown","0fff2973":"markdown","eee3bc21":"markdown","4036edac":"markdown","68d46f33":"markdown","5a2933d5":"markdown","45082d4b":"markdown","1a20d5c8":"markdown","136a7994":"markdown","2d74c3a4":"markdown","ebb3f461":"markdown","cc711433":"markdown","f2151768":"markdown","d20f9f3f":"markdown","d38526dd":"markdown","881ef4ac":"markdown","6ee62888":"markdown","979841d0":"markdown","0b4bdea3":"markdown","6c23d6c4":"markdown","e0b8ce02":"markdown","30e60ec3":"markdown","80b28b57":"markdown"},"source":{"b9e17ac3":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","8e54311f":"train = '..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv'\ntest = '..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv'","c9b21959":"# A copy of the files is preserved inorder to retain an original structure which won't be affected by\n# the data processings.","d189960f":"trainOriginal = pd.read_csv(train, encoding='latin-1')\ntestOriginal = pd.read_csv(test, encoding='latin-1')","a8b28502":"train = trainOriginal.copy()\ntest = testOriginal.copy()","744f4c25":"# Earlier attempts to open the files with a utf-8 encoding lead to a unicode error as it couldn't\n# parse certain parts of the file, hence, utf-8 was introduced as a solution.","6090931f":"train.head()","872fd0fb":"test.head()","f1cc0dba":"# The datasets contain 7 columns housing the data info. The UserName and ScreenName has being\n# encrypted due to privacy concerns. The tweets contains mentions and hashtags which must be cleaned\n# in order to help the models better understand the statistical relationship between the relevant\n# details. The sentiment column contains 5 different classes which can be remapped into 3 for better\n# statistical understanding. The other columns are the timeframe of the tweets and the location from\n# where the tweets where twitted.","7149f094":"train.info()","cca98b88":"# The datas that will have the major effects on how the models determine the classes are non integers\n# which means data preprocessing steps needs to be don before feeding them into the models.","a5da176f":"train.isnull().sum()","11c53579":"# The location column contains a whooping 8590 missing rows. Filling the blanks with the most common\n# location won't really make sense as the missing details are too much.","00582795":"train['Location'].value_counts()[:60]","94eb790e":"# The location info was splitted and merged because lots of the locations are in the same geolocation,\n# The datasets also shows a great reprentative bias. For a global distortion, the bulk of the data \n# collected are within few geopolitical zones with Africa having a little representation. Different \n# ideals, govermental polices, religious beliefs etc are factors that likely influenced the sentiment\n# class of individual tweets.","2f0d3767":"#splitting location into word pairs\ntrain['Location'] = train['Location'].str.split(\",\").str[0]\ntest['Location'] = test['Location'].str.split(\",\").str[0]","53089adc":"train['Location'].value_counts()[:60]","0f3b1b8e":"train['TweetAt'].value_counts()","db535323":"# The data collected was tweeted between 16th March, 2020 to 14th April, 2020. Any model built and \n# deployed at this time may likely not be relevant for present use due to new findings, researches, \n# tresnd that have emerged which will influence every recent covid19 related tweets. Any model built\n# using this data will be a decayed model and further decay will happen at a rapid pace.","88d9301a":"train['Sentiment'].value_counts()","397b1b47":"# Positive > Negative > Neutral and the categories will be remapped to fully represent this position.","48ecc6e4":"plt.figure(figsize=(10,10))\nsns.countplot(y='Location',data=train,order=train.Location.value_counts().iloc[\n    0:19].index).set_title(\"Twitted locations\")","58f027f1":"sns.set_style(\"whitegrid\")\nsns.set(rc={'figure.figsize':(11,8)})\nsns.countplot(train['Sentiment'])","88adcc82":"labels = ['Positve', 'Negative', 'Neutral', 'Extremely Positive', 'Extremely Negative']\ncolors = ['#ff9999','#66b3ff','#99ff99','#ffcc99', '#ff5645']\nexplode = (0.05,0.05,0.05,0.05,0.05) \nplt.pie(train.Sentiment.value_counts(), colors = colors, labels=labels,\n        autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)\ncentreCircle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centreCircle)\nplt.tight_layout()\nplt.show()","77130817":"plotDf = train.iloc[:,[2,5]] #[:,[2,5]] is the location and sentiment columns\nplotDf","1f054d1e":"sns.set(rc={'figure.figsize':(15,9)})\ngg = train.Location.value_counts()[:5].index\nplt.title('Sentiment Categories of the First 5 Top Locations', fontsize=16, fontweight='bold')\nsns.countplot(x = 'Location', hue = 'Sentiment', data = plotDf, order = gg)","f72b1623":"# Reflecting the insight from train['Sentiment'].value_counts(), positive sentiment dominates the \n# kind of tweets across the locations.","052b865d":"train['Identity'] = 0\ntest['Identity'] = 1 \ncovid = pd.concat([train, test])\ncovid.reset_index(drop=True, inplace=True)","71ee9eb2":"covid.head()","46960070":"covid['Sentiment'] = covid['Sentiment'].str.replace('Extremely Positive', 'Positive')\ncovid['Sentiment'] = covid['Sentiment'].str.replace('Extremely Negative', 'Negative')","0c348abb":"covid = covid.drop('ScreenName', axis=1)\ncovid = covid.drop('UserName', axis=1)\ncovid","d210de62":"# The blank rows in the Location column would have being filled with Unknown if it would have had any\n# significant impact on the objective of the project\n# covid['Location'].fillna('Unknown', inplace=True)\n\n# covid.isnull().sum() would have being used to check and confirm","029614d0":"sns.set_style(\"whitegrid\")\nsns.set(rc={'figure.figsize':(11,8)})\nsns.countplot(covid['Sentiment'])","b6f5f7f5":"labels = ['Positve', 'Negative', 'Neutral']\ncolors = ['lightblue','lightsteelblue','silver']\nexplode = (0.1, 0.1, 0.1)\nplt.pie(covid.Sentiment.value_counts(), colors = colors, labels=labels,\n        shadow=300, autopct='%1.1f%%', startangle=90, explode = explode)\nplt.show()","3d92bc74":"plt.figure(figsize=(10,10))\nsns.countplot(y='Location',data=train,order=train.Location.value_counts().iloc[\n    0:19].index).set_title(\"Twitted locations\")","2bc69126":"covid['Sentiment'] = covid['Sentiment'].map({'Neutral':0, 'Positive':1, 'Negative':2})","9e0b51a0":"hashTags=covid['OriginalTweet'].str.extractall(r\"(#\\S+)\")\nhashTags = hashTags[0].value_counts()\nhashTags[:50]","38db7ea2":"# As expected, the bulk of the tweets centres around covid19, it's other generic names, safety\n# protocols as well as the different materials needed to weather through the tough times.","c5135368":"mentions = train['OriginalTweet'].str.extractall(r\"(@\\S+)\")\nmentions = mentions[0].value_counts()\nmentions[:50]","7852dfaf":"import re\n\ndef clean(text):\n    text = re.sub(r'http\\S+', \" \", text)\n    text = re.sub(r'@\\w+',' ',text)\n    text = re.sub(r'#\\w+', ' ', text)\n    text = re.sub(r'\\d+', ' ', text)\n    text = re.sub('r<.*?>',' ', text)\n    text = text.split()\n    text = \" \".join([word for word in text if not word in stopWord])\n    \n    return text","2bfe8484":"import nltk\nfrom nltk.corpus import stopwords","9573a4b1":"# Stop words are high-frequency words like a, an, the, to and also that we sometimes want to filter\n# out of a document before further processing. Stop words usually have little lexical content and\n# do not hold much of a meaning.\n\n# Below is a list of 25 example of semantically non-selective stop words: a, an, and, are, as, at,\n# be, by, for, from, has, he, in, is, it, its, of, on, that, the, to, was, were, will, with.","3a9a0868":"stopWord = stopwords.words('english')","2e1d1ba5":"covid['OriginalTweet'] = covid['OriginalTweet'].apply(lambda x: clean(x))","1f4ca53f":"covid.head()","e505a34a":"covid = covid[['OriginalTweet','Sentiment','Identity']]\ncovid.head()","fc3d6064":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV","25149100":"# Lemmatization is the algorithmic process of determining the lemma of a word based on its intended\n# meaning. For example, in English, the verb \u201cto walk\u201d may appear as \u201cwalk,\u201d \u201cwalked,\u201d \u201cwalks,\u201d or\n# \u201cwalking.\u201d The base form, \u201cwalk,\u201d that one might look up in a dictionary, is called the lemma for\n# the word.\n\n# Tokenization is one of the simple yet basic concepts of natural language processing where texts are\n# splitted into meaningful segments.\n\n# Data vectorization deals with the turning of data into tensors. All inputs and targets in a machine\n# learning model must be tensors of floating-point data (or, in specific cases, tensors of integers).","062a1ceb":"covid['Corpus'] = [nltk.word_tokenize(text) for text in covid.OriginalTweet]\nlemma = nltk.WordNetLemmatizer()\ncovid.Corpus = covid.apply(lambda x: [lemma.lemmatize(word) for word in x.Corpus], axis=1)\ncovid.Corpus = covid.apply(lambda x: \" \".join(x.Corpus),axis=1)","2f4bc503":"covid.head()","07e92ef8":"train = covid[covid.Identity==0]\ntest = covid[covid.Identity==1]\ntrain.drop('Identity',axis=1, inplace=True)\ntest.drop('Identity',axis=1, inplace=True)\ntest.reset_index(drop=True,inplace=True)","57abccbe":"train.head()","a4d66025":"test.head()","50a1dbd8":"XTrain = train.Corpus\nyTrain = train.Sentiment\n\nXTest = test.Corpus\nyTest = test.Sentiment\n\nXTrain, XVal, yTrain, yVal = train_test_split(XTrain, yTrain, test_size=0.2,random_state=42)","40720aa1":"XTrain.shape, XVal.shape, yTrain.shape, yVal.shape, XTest.shape, yTest.shape","3178b030":"vectorizer = CountVectorizer(stop_words='english',ngram_range=(1,2),min_df=5).fit(covid.Corpus)\n\nXTrainVec = vectorizer.transform(XTrain)\nXValVec = vectorizer.transform(XVal)\nXTestVec = vectorizer.transform(XTest)","10334348":"# Logistic Regression (also called Logit Regression) is commonly used to estimate the probability\n# that an instance belongs to a particular class (e.g., what is the probability that this email is\n# spam?). If the estimated probability is greater than 50%, then the model predicts that the instance\n# belongs to that class (called the positive class, labeled \u201c1\u201d), or else it predicts that it does\n# not (i.e., it belongs to the negative class, labeled \u201c0\u201d).\n\n# A Logistic Regression model computes a weighted sum of the input features (plus a bias term), but\n# instead of outputting the result directly like the Linear Regression model does, it outputs the\n# logistic of this result.\n# p = h\u03b8 x = \u03c3 xT\u03b8\n# The logistic\u2014noted \u03c3(\u00b7)\u2014is a sigmoid function (i.e., S-shaped) that outputs a number\n# between 0 and 1.","59e69393":"logReg = LogisticRegression(random_state=42)","f16dded2":"# Cross-validation makes it possible to get not only an estimate of the performance of models,\n# but also a measure of how precise this estimate is (i.e., its standard deviation). But cross-\n# validation comes at the cost of training models several times, so it is not always possible.","d939ba1c":"cross_val_score(LogisticRegression(random_state=42),\n                XTrainVec, yTrain, cv=10, verbose=1, n_jobs=-1).mean()","388096d1":"model = logReg.fit(XTrainVec, yTrain)","ba64778f":"print(classification_report(yVal, model.predict(XValVec)))","1669916d":"penalty = ['l2']\nC = np.logspace(0, 4, 10)\nhyperparameters = dict(C=C, penalty=penalty)\n\nlogRegGrid = GridSearchCV(logReg, hyperparameters, cv=5, verbose=0)","edf72140":"bestModel = logRegGrid.fit(XTrainVec, yTrain)","093e0436":"# Best hyperparameters combination\n\nprint('Best Penalty:', bestModel.best_estimator_.get_params()['penalty'])\nprint('Best C:', bestModel.best_estimator_.get_params()['C'])","d1e3f269":"# Final Logistic Regression model performance\n\nyPred = bestModel.predict(XTestVec)","67a2dab4":"print(classification_report(yTest, bestModel.predict(XTestVec)))","682d9475":"# Precision deals with the accuracy of the positive predictions.\n# precision = TP \/ TP + FP\n# TP is the number of true positives, and FP is the number of false positives.\n\n# Recall, also called sensitivity or true positive rate (TPR) is the ratio of positive instances that\n# are correctly detected by the classifier.\n# recall = TP \/ TP + FN\n# TP is the number of true positives FP is the number of false positives and FN is the number of\n# false negatives.\n\n# But the metric of choice to measure the performance of the logistic regression model in this\n# project is the F1-score.The F1 score is the harmonic mean of precision and recall.\n# Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low\n# values. As a result, the classifier will only get a high F1 score if both recall and precision are\n# high.","f593dc57":"# A less concise metric also available is the confusion matrix. The general idea involves counting\n# the number of times instances of class A are classified as class B.\n\n#  Implementation:\n\n# from sklearn.metrics import confusion_matrix\n# from sklearn.model_selection import cross_val_predict\n\n# yPred = bestModel.predict(XTestVec)\n# print(confusion_matrix(yTest, yPred))\n\n# NB: it's possible that classification metrics wont't be able to handle a mix of multilabel-indicator\n# and multiclass targets.","71d4a76d":"from tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, Dropout\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical","f307e7d1":"lines = []\nfor line in train['Corpus']:\n    lines.append(line)\n    \nlen(lines)","a28c7646":"tokenizer = Tokenizer(num_words=5000, lower=True) # Number of words to consider as features\ntokenizer.fit_on_texts(train['Corpus'].values)\nwordIndex = len(tokenizer.word_index) + 1\nprint('Found %s unique tokens.' % (wordIndex))","e57b3219":"# Turns the lists of integers into a 2D integer tensor of shape (numWords, maxlen)\nXTrain = tokenizer.texts_to_sequences(train['Corpus'].values)\nXTrain = pad_sequences(XTrain, maxlen=30) # Cuts off the texts after this number of words\n\nXTest = tokenizer.texts_to_sequences(test['Corpus'].values)\nXTest = pad_sequences(XTest, maxlen=30)","f9105cd6":"# The tokenizer selects the most common 5000 words. The sequences are padded so that they all have\n# a uniform length of 30.","12346632":"XTrain.shape, XTest.shape","518b9e8f":"yTrain = to_categorical(train['Sentiment'], 3)\nyTest = to_categorical(test['Sentiment'], 3)","91eaaae3":"model= models.Sequential()\nmodel.add(layers.Embedding(wordIndex, 128, input_length=1000))\nmodel.add(layers.LSTM(200))\nmodel.add(Dropout(0.2))\nmodel.add(layers.Dense(3, activation='softmax'))","769cf9dc":"model.summary()","71fb9892":"model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.01),\n              metrics=['accuracy'])","6fac3953":"# The callbacks parameter implemented monitors the validation loss and stops the training process\n# once there is no apparent improvement for 10 epochs. It will also restore the best version of the\n# model recorded during training.","236a8521":"history = model.fit(XTrain ,yTrain, batch_size=250, epochs=100, validation_split=0.2,\n         callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)])","509cd245":"accuracy = history.history['accuracy']\nvalAccuracy = history.history['val_accuracy']\nloss = history.history['loss']\nvalLoss = history.history['val_loss']\nepochs = range(1, len(accuracy) + 1)\n\nplt.style.use('ggplot')\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,10))\nplot = ax1.plot(epochs, accuracy, 'bo', label='Training Accuracy')\nax1.plot(epochs, valAccuracy, 'b', label='Validation Accuracy')\nax1.set(title='Training\/Validation Accuracy', ylabel='Accuracy')\nax1.legend()\n\nplot = ax2.plot(epochs, loss, 'bo', label='Training Loss')\nax2.plot(epochs, valLoss, 'b', label='Validation Loss')\nax2.set(title='Training\/Validation Loss', ylabel='Loss', xlabel='Epochs')\nax2.legend()\n\nfig.suptitle('Loss\/Accuracy of the LSTM Sentiment Classifier', fontsize=16, fontweight = 'bold')","d0bd686e":"# The model still overfits: In the absence of more data, the overfitting being experienced can probably be\n# minimized by reducing the number of layers or by reducing the number of units used in the neural\n# architecture. The Dropout can also be increased. Weight regularization via the keras\n# kernel_regularizer can also be implemented.","279f9f2a":"results = model.evaluate(XTest, yTest)","b65065f2":"print(classification_report(np.argmax(yTest, 1), model.predict_classes(XTest)))","11f305f4":"# A test accuracy score of 0.84 gives a much improved performance compared to the Logistic Regression\n# algorithm. A normal slight drop from the 0.87 recorded during validation evaluation.","4bbe78e8":"model.save('.\/LSTM classifier.h5')\nkeras.models.load_model('.\/LSTM classifier.h5')","8d0b78e9":"The underlying Long Short-Term Memory (LSTM) algorithm was developed by Hochreiter and Schmidhuber\nin 1997; it was the culmination of their research on the vanishing gradient problem. This layer is a variant of the SimpleRNN layer; it adds a way to carry information across many timesteps. Imagine a conveyor belt running parallel to the sequence you\u2019re processing. Information from the sequence can jump onto the conveyor belt at any point, be transported to a later timestep, and jump off, intact,\nwhen you need it. This is essentially what LSTM does: it saves information for later, thus preventing older signals from gradually vanishing during processing (Francois Chollet, Deep Learning with Python).","21c3da0d":"LONG SHORT TERM MEMORY (LSTM) MODEL","17396934":"A python regex function to clean the tweets by removing hashtags, mentions, urls, digits and stop words.","2262ce48":"Further Data Processing and Analysis - top mentions and hashtags in the tweets are extracted and analyzed, after which they will be removed as well as the stop words just to make it easier for the models to discover the statistical relationship between the words.","0fff2973":"A prominent or distinguishing feature in this neural construct is the Embedding layer. The Embedding layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, it looks up these integers in an internal dictionary, and it returns the associated vectors. It\u2019s effectively a dictionary lookup.","eee3bc21":"Logistic Regression Performance","4036edac":"The sentiment categories are remapped into three so that the classifiers will be more accurate.\n\nNeutral: 0, Positive: 1, Negative: 2","68d46f33":"The screen and username columns are dropped since they'll have no effect on the accuracy of the model.","5a2933d5":"The train set is splitted to get a validation set.","45082d4b":"Visual Representation of the Training Set","1a20d5c8":"Both the test and train set are concatenated together to easily preprocess both together.\n\nTraining set will have an identity of 0 while the test set will have 1","136a7994":"Libraries and Frame Works Needed for Further Data Processing, Building a LSTM Model and it's Evaluation","2d74c3a4":"Classification is the process of predicting the class of given data points. Classes are sometimes called targets\/ labels or categories. Classification predictive modeling is the task of approximating a mapping function (f) from input variables (X) to discrete output variables (y). Classification belongs to the category of supervised learning where the targets also provided with the input data.\n\nA classifier utilizes some training data to understand how given input variables relate to the class.","ebb3f461":"Machine learning models finds statistical relations, therefore the data is tokenized and vectorized as part of the data preprocessing step.","cc711433":"The data sets are splitted back into training and test set","f2151768":"Visualizing the Concanated Data Set","d20f9f3f":"Fine Tuning the Logistic Regression Model\n\nA great way to do this is by 'Grid Searching' which involves the fiddling with the hyperparameters until a great combination of hyperparameter values is discovered. It can be done simply by using Scikit-Learn\u2019s GridSearchCV. All that's needed is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation.","d38526dd":"Features not needed for the predictions are dropped","881ef4ac":"Python Libraries Needed for File Opening, Data Analysis, Data Visualization, Data Exploration and Data Cleaning","6ee62888":"This end goal of this project is to build models - Logistic Regression, a classical traditional machine learning algorithm and Long Short Term Memory, a deep learning algorithm that can predict the sentiment category or class of texts based on covid19 related tweets\n\nThis problem is an instance of multiclass classification; and because each data point should be classified into only one category, the problem is more specifically an instance of single-label, multiclass classification.\n\nThe dataset used for this project can be found at https:\/\/www.kaggle.com\/datatattle\/covid-19-nlp-text-classification?rvi=1","979841d0":"The neural network consists of one embedding layer followed by one LSTM layer with 200 units. A Dropout layer is added for regularizatin to prevent overfitting of the model. The neural architecture ends with a Dense layer having three units to generate the output or prediction classes. The activation used is softmax since it is a single label, multi class problem.","0b4bdea3":"Data Processing for Machine Learning Algorithms\n\nData processing deals with preparing the input data and targets before feeding them into a machine learning model. Many data-preprocessing and feature-engineering techniques are domain specific (for example, specific to text data).\n\nData preprocessing aims at making the raw data at hand more amenable to machine learning algorithms. This includes vectorization, normalization, handling missing values, and feature extraction.\n\nParticular to this project is the need to convert the tweets into vector arrays and padded sequences before feeding it into the logistic regression and LSTM models respectively.","6c23d6c4":"Visualizing the Loss\/Accuracy of the Model In-Between Epochs","e0b8ce02":"The 5 sentiment categories are regrouped into 3 for easy data analysis","30e60ec3":"Libraries and Frame Works Needed for Further Data Processing, Building a Logistic Regression Model and it's Evaluation","80b28b57":"NEURAL NETWORK"}}