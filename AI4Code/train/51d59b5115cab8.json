{"cell_type":{"05abae57":"code","455a33cf":"code","50943b1c":"code","eddaebad":"code","56df8e1e":"code","bed49af7":"code","de0d68e7":"code","e19ebb1b":"code","da8a3a23":"code","df2fa730":"code","d224071f":"code","bccb81e1":"code","d2b3f5f3":"code","dd4fd326":"code","c3827325":"code","c18b62b4":"code","110d370b":"code","f8cde827":"code","192e3e3b":"code","2a7c510b":"code","0cb5730a":"code","9322f5a2":"code","957124bc":"code","66eb9c3a":"code","e35bf716":"markdown","186017cc":"markdown","9acd3247":"markdown","d7c17fd2":"markdown","8dd9135a":"markdown","1f355366":"markdown","6a74ad39":"markdown","5547860f":"markdown","facdc3cc":"markdown","59b70f25":"markdown","39530c55":"markdown","a4d46bd0":"markdown","c1c7dcf2":"markdown","6382dc68":"markdown","860a507e":"markdown","939fdab2":"markdown","396d99f2":"markdown","b9db5808":"markdown","fc3b65ad":"markdown","25b2ba54":"markdown","7250e5fe":"markdown","439fa987":"markdown","02e6f38f":"markdown","ca6c669b":"markdown","9ef36ecf":"markdown","c4dcc2a6":"markdown","2cf1113b":"markdown","4c4520e6":"markdown","9d78a53f":"markdown","28b3917a":"markdown","4c295be0":"markdown","22027add":"markdown","68ca38b3":"markdown"},"source":{"05abae57":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom mlxtend.preprocessing import minmax_scaling\nimport scipy.cluster.hierarchy as sch\nimport mpl_toolkits.mplot3d.axes3d as p3\nimport seaborn as sns\nfrom sklearn import metrics, mixture, cluster, datasets\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\nfrom sklearn.neighbors import kneighbors_graph\nfrom itertools import cycle, islice\nimport time\nimport warnings\nprint('Libraries correctly loaded')","455a33cf":"summer_products_path = \"..\/input\/summer-products-and-sales-in-ecommerce-wish\/summer-products-with-rating-and-performance_2020-08.csv\"\nunique_categories_path = \"..\/input\/summer-products-and-sales-in-ecommerce-wish\/unique-categories.csv\"\nunique_categories_sort_path = \"..\/input\/summer-products-and-sales-in-ecommerce-wish\/unique-categories.sorted-by-count.csv\"\n\nsummer_products = pd.read_csv(summer_products_path)\nunique_categories = pd.read_csv(unique_categories_path)\nunique_categories_sort = pd.read_csv(unique_categories_sort_path)\n\ndf = summer_products\nprint('Number of rows: '+ format(df.shape[0]) +', number of features: '+ format(df.shape[1]))","50943b1c":"C = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\nMissing_Percentage = (df.isnull().sum()).sum()\/np.product(df.shape)*100\nprint(\"The number of missing entries before cleaning: \" + str(round(Missing_Percentage,2)) + \" %\")\n\ndf[NumericVariables]=df[NumericVariables].fillna(0)\ndf=df.drop('has_urgency_banner', axis=1) # 70 % NA's\n\ndf[CategoricalVariables]=df[CategoricalVariables].fillna('Unknown')\ndf=df.drop('urgency_text', axis=1) # 70 % NA's\ndf=df.drop('merchant_profile_picture', axis=1) # 86 % NA's\n\nC = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\nMissing_Percentage = (df.isnull().sum()).sum()\/np.product(df.shape)*100\nprint(\"The number of missing entries after cleaning: \" + str(round(Missing_Percentage,2)) + \" %\")\nprint(\"The number of categorical variables:\" + str(len(CategoricalVariables)))\nprint(\"The number of numeric variables:\" + str(len(NumericVariables)))","eddaebad":"Size_map  = {'NaN':1, 'XXXS':2,'Size-XXXS':2,'SIZE XXXS':2,'XXS':3,'Size-XXS':3,'SIZE XXS':3,\n            'XS':4,'Size-XS':4,'SIZE XS':4,'s':5,'S':5,'Size-S':5,'SIZE S':5,\n            'M':6,'Size-M':6,'SIZE M':6,'32\/L':7,'L.':7,'L':7,'SizeL':7,'SIZE L':7,\n            'XL':8,'Size-XL':8,'SIZE XL':8,'XXL':9,'SizeXXL':9,'SIZE XXL':9,'2XL':9,\n            'XXXL':10,'Size-XXXL':10,'SIZE XXXL':10,'3XL':10,'4XL':10,'5XL':10}\n\ndf['product_variation_size_id'] = df['product_variation_size_id'].map(Size_map)\ndf['product_variation_size_id']=df['product_variation_size_id'].fillna(1)\nOrdinalVariables = ['product_variation_size_id']\nprint(OrdinalVariables)","56df8e1e":"Color_map  = {'NaN':'Unknown','Black':'black','black':'black','White':'white','white':'white','navyblue':'blue',\n             'lightblue':'blue','blue':'blue','skyblue':'blue','darkblue':'blue','navy':'blue','winered':'red',\n             'red':'red','rosered':'red','rose':'red','orange-red':'red','lightpink':'pink','pink':'pink',\n              'armygreen':'green','green':'green','khaki':'green','lightgreen':'green','fluorescentgreen':'green',\n             'gray':'grey','grey':'grey','brown':'brown','coffee':'brown','yellow':'yellow','purple':'purple',\n             'orange':'orange','beige':'beige'}\n\ndf['product_color'] = df['product_color'].map(Color_map)\ndf['product_color']=df['product_color'].fillna('Unknown')\ndf['product_color'].unique()","bed49af7":"NominalVariables = [x for x in CategoricalVariables if x not in OrdinalVariables]\nLvl = df[NominalVariables].nunique()\n\nToDrop=['title','title_orig','currency_buyer', 'theme', 'crawl_month', 'tags', 'merchant_title','merchant_name',\n              'merchant_info_subtitle','merchant_id','product_url','product_picture','product_id']\ndf = df.drop(ToDrop, axis = 1)\nFinalNominalVariables = [x for x in NominalVariables if x not in ToDrop]\n\ndf_dummy = pd.get_dummies(df[FinalNominalVariables], columns=FinalNominalVariables)\nprint(ToDrop)","de0d68e7":"df_clean = df.drop(FinalNominalVariables, axis = 1)\ndf_clean = pd.concat([df_clean, df_dummy], axis=1)\n\nNumericVariablesNoTarget = [x for x in NumericVariables if x not in ['units_sold']]\ndf_scale=df_clean\ndf_scale[NumericVariables] = minmax_scaling(df_clean, columns=NumericVariables)\nprint(FinalNominalVariables + NumericVariables + OrdinalVariables)","e19ebb1b":"SpearmanCorr = df.corr(method=\"spearman\")\nplt.figure(figsize=(10,10))\nsns.heatmap(SpearmanCorr, vmax=.9, square=True)","da8a3a23":"def plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([model.children_, model.distances_,\n                                      counts]).astype(float)\n\n    # Plot the corresponding dendrogram\n    dendrogram(linkage_matrix, **kwargs)","df2fa730":"X = df_scale.values\ndendrogram = sch.dendrogram(sch.linkage(X, method='ward'), truncate_mode='level', p=4)","d224071f":"AWcluster = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')\nAWcluster.fit(df_scale)\nAWlabels = AWcluster.labels_\nAWcluster_fit = AWcluster.fit(X)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), facecolor='w', edgecolor='k')\nax = p3.Axes3D(fig)\nax.view_init(15, 30)\nfor l in np.unique(AWlabels):\n    ax.scatter(X[AWlabels == l, 0], X[AWlabels == l, 1], X[AWlabels == l, 2],color=plt.cm.jet(float(l) \/ np.max(AWlabels + 1)),s=20, edgecolor='k')\nplt.title('Hierarchical: Agglomerative Clustering with ward link' )\n\nplt.show()","bccb81e1":"AAcluster = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='average')\nAAcluster.fit(df_scale)\nAAlabels = AAcluster.labels_\nAAcluster_fit = AAcluster.fit(X)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), facecolor='w', edgecolor='k')\nax = p3.Axes3D(fig)\nax.view_init(15, 30)\nfor l in np.unique(AAlabels):\n    ax.scatter(X[AAlabels == l, 0], X[AAlabels == l, 1], X[AAlabels == l, 2],color=plt.cm.jet(float(l) \/ np.max(AAlabels + 1)),s=20, edgecolor='k')\nplt.title('Hierarchical: Agglomerative Clustering with ward average' )\n\nplt.show()","d2b3f5f3":"kmeans_kwargs = {\"init\": \"random\",\"n_init\": 10,\"max_iter\": 300,\"random_state\": 42,}\n\n# A list holds the SSE values for each k\nsse = []\nfor k in range(1, 11):\n   kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n   kmeans.fit(df_scale)\n   sse.append(kmeans.inertia_)\n\nplt.style.use(\"fivethirtyeight\")\nplt.plot(range(1, 11), sse)\nplt.xticks(range(1, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()","dd4fd326":"KMcluster = KMeans(init=\"random\",n_clusters=4,n_init=10,max_iter=300,random_state=42)\nKMcluster.fit(df_scale)\nprint('The inertia equals to:' + format(KMcluster.inertia_))\n#print('The center of clusters are:' + format(KMcluster.cluster_centers_))","c3827325":"KMcluster_fit = KMcluster.fit(df_scale)\nKMlabels = KMcluster.labels_\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), facecolor='w', edgecolor='k')\nax = p3.Axes3D(fig)\nax.view_init(15, 30)\nfor l in np.unique(KMlabels):\n    ax.scatter(X[KMlabels == l, 0], X[KMlabels == l, 1], X[KMlabels == l, 2],color=plt.cm.jet(float(l) \/ np.max(KMlabels + 1)),s=20, edgecolor='k')\nplt.title('Centroid: k-means clustering' )\n\nplt.show()","c18b62b4":"Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)\n\nGM_n_components = np.arange(1, 21)\nGM_models = [mixture.GaussianMixture(n, covariance_type='full', random_state=0).fit(Xmoon) for n in GM_n_components]\n\nplt.plot(GM_n_components, [m.bic(Xmoon) for m in GM_models], label='BIC')\nplt.plot(GM_n_components, [m.aic(Xmoon) for m in GM_models], label='AIC')\nplt.legend(loc='best')\nplt.xlabel('n_components');","110d370b":"GM_n_classes = 6 #len(np.unique(df_scale))\n\nGMcluster = mixture.GaussianMixture(n_components=GM_n_classes, covariance_type='full')\nGMcluster_fit = GMcluster.fit(df_scale)\nGMlabels = GMcluster_fit.predict(df_scale)\nprint('Number of clusters: ' + format(len(np.unique(GMlabels))))","f8cde827":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), facecolor='w', edgecolor='k')\nax = p3.Axes3D(fig)\nax.view_init(15, 30)\nfor l in np.unique(GMlabels):\n    ax.scatter(X[GMlabels == l, 0], X[GMlabels == l, 1], X[GMlabels == l, 2],color=plt.cm.jet(float(l) \/ np.max(GMlabels + 1)),s=20, edgecolor='k')\nplt.title('Expectation-maximization algorithm for clustering' )\n\nplt.show()","192e3e3b":"DBcluster= DBSCAN(eps=0.3, min_samples=10)\nDBcluster_fit = DBcluster.fit(df_scale)\ncore_samples_mask = np.zeros_like(DBcluster_fit.labels_, dtype=bool)\ncore_samples_mask[DBcluster_fit.core_sample_indices_] = True\nDBlabels = DBcluster_fit.labels_ ","2a7c510b":"DB_n_clusters_ = len(set(DBlabels)) \nDB_n_noise_ = list(DBlabels).count(-1)\nprint('Estimated number of clusters: %d' % DB_n_clusters_)\nprint('Estimated number of noise points: %d' % DB_n_noise_)","0cb5730a":"# Black removed and is used for noise instead.\nunique_labels = set(DBlabels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [1, 0, 0, 0]\n\n    class_member_mask = (DBlabels == k)\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=20)\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=2)\n\nplt.title('Density-based spatial clustering of applications with noise')\nplt.show()","9322f5a2":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), facecolor='w', edgecolor='k')\nax = p3.Axes3D(fig)\nax.view_init(15, 30)\nfor l in range(0,4):\n    ax.scatter(X[DBlabels == l, 0], X[DBlabels == l, 1], X[DBlabels == l, 2],color=plt.cm.jet(float(l) \/ np.max(DBlabels + 1)),s=50, edgecolor='k')\nplt.title('Density-based spatial clustering with noise removed' )\n\nplt.show()","957124bc":"clustering_algorithms = (\n        ('K-means', KMcluster),\n        ('Ward', AWcluster),\n        ('Agglomerative', AAcluster),\n        ('DBSCAN', DBcluster),\n        ('Gaussian Mixture', GMcluster)\n    )\n\nplt.figure(figsize=(9 * 2 + 3, 12.5))\nplt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,hspace=.01)\n\nplot_num = 1\n\nfor name, algorithm in clustering_algorithms:\n    \n    t0 = time.time()   \n    algorithm.fit(X)\n        \n    t1 = time.time()\n    \n    if hasattr(algorithm, 'labels_'):\n        y_pred = algorithm.labels_.astype(np.int)\n    else:\n        y_pred = algorithm.predict(df_scale)\n    \n    plt.subplot(3, len(clustering_algorithms), plot_num)\n    plt.title(name + ', Clusters = ' + format(len(set(y_pred))), size=18)    \n    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                             '#f781bf', '#a65628', '#984ea3',\n                                             '#999999', '#e41a1c', '#dede00']),\n                                      int(max(y_pred) + 1))))      \n    colors = np.append(colors, [\"#000000\"])\n    plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n    plt.xlim(0, 0.7)\n    plt.ylim(0, 0.7)\n    plt.xticks(())\n    plt.yticks(())\n    plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n            transform=plt.gca().transAxes, size=15,\n            horizontalalignment='right')\n    plot_num += 1\n    \nplt.show()","66eb9c3a":"KMunique, KMcounts = np.unique(KMlabels, return_counts=True)\nAWunique, AWcounts = np.unique(AWlabels, return_counts=True)\nAAunique, AAcounts = np.unique(AAlabels, return_counts=True)\nDBunique, DBcounts = np.unique(DBlabels, return_counts=True)\nGMunique, GMcounts = np.unique(GMlabels, return_counts=True)\nprint('K-means distributed points: ' + format(dict(zip(KMunique, KMcounts))))\nprint('Hierarchical ward link distributed points: ' + format(dict(zip(AWunique, AWcounts))))\nprint('Hierarchical average link distributed points: ' + format(dict(zip(AAunique, AAcounts))))\nprint('DBSCAN distributed points: ' + format(dict(zip(DBunique, DBcounts))))\nprint('Gaussian Mixture distributed points: ' + format(dict(zip(GMunique, GMcounts))))","e35bf716":"Looking how records are grouped, we observe:\n* for k-means vast concentration in cluster 1, clusters 2nd and 3rd are also relevant but 0 seems to contain only absolutely extreme values\n* hierarchical ward link also shows concentration in one cluster with 3 another relevant clusters and 2 extreme ones\n* hierarchical average link is very similar to previous one but leads to move between cluster 1 and 2\n* DBSCAN should be further calibrated, almost all points classified as noise\n* Gaussian mixture gave me the same results as hierarchical ward link ! Remember that order is not relevant","186017cc":"Paradoxiacally, similar to elbow rule, but at least with use of statistic like AIC or BIC. For the simplicity, we will go for BIC min, so 6 clusters.","9acd3247":"Wrap all and prepare. Get rid of useless variables - all of them are perfectly unique like ID or remain in one level like currency.","d7c17fd2":"The method seems to be more-less consistent with hierarchical one. Only difference is that hierarchical one proposed 6 clusters, k-means offered 4 of them. K-means seem to be more'stable' in certain way in this case.","8dd9135a":"Read in libraries.","1f355366":"A lot of noise around the density points. Remaining density centers are presented on second graph.","6a74ad39":"# Clustering summary","5547860f":"# Density-based spatial clustering of applications with noise","facdc3cc":"Correction for color variable. All color levels listed.","59b70f25":"Loading and naming the data.","39530c55":"Oke, the optimal number of clusters is between k-means and hierarchical methods.","a4d46bd0":"The Gaussian Mixture Models (GMM) algorithm is an unsupervised learning algorithm since we do not know any values of a target feature. This method is slightly rarer and not so obvious but in many cases powerful. For more interested ones, the basis for this topic is Jensen's inequality, which is persistenly used by math professors to test students at first tutorials, nice description [here](https:\/\/www.python-course.eu\/expectation_maximization_and_gaussian_mixture_models.php).","c1c7dcf2":"I remove noise to look at density centers. Majority of points identified as noise by the algorithm.","6382dc68":"Difference is not big but I would go for ward link anyway as it seems to be clearer spearated. Number 6 seems to be correct value for this method.","860a507e":"# Data Analysis","939fdab2":"First, it;s improtant to say that GMM is ultimately generative model and it was not created for clustering but for the pdf definition. As the model is generative, it means that defines the probability distribution. Hence, we can mathematically find the optimal number of clusters for example by use of likelihood. ","396d99f2":"# Hierarchical clustering","b9db5808":"# Intro to clustering methods\n\nThe goal of the notebook is to explore 4 different clustering methods by use of clothes sales data. The notebook contains 7 chapters and introduction as listed below.\n* Data preparation: regular cleaning\n* Data analysis: fast look at Spearman dependencies\n* Connectivity models, example: \"hierarchical clustering\" builds models based on distance connectivity.\n* Centroid models, example: \"k-means algorithm\" represents each cluster by a single mean vector.\n* Distribution models, example: \"expectation-maximization algorithm\" for modeling by use of statistical distributions.\n* Density models: for example: \"DBSCAN\" defines clusters as connected dense regions in the data space.\n* Clustering summary: I will group the results and compare\n\nBelow the list shows different clustering methods, data types in rows, methods in columns coming from [scikit](https:\/\/scikit-learn.org\/stable\/modules\/clustering.htm) . I analyse 1st one which is centroid k-means, 5th is connectivity hierarchical with ward link, 6th connectivity hierarchical with average link, 7th density DBSCAN and last one disitrubtion EMA.\n\n**Remark: relevant part of code hidden for clarity of notebook. Please click 'unhide' to see it.**","fc3b65ad":"As the name, hierarchical clustering aims on building hierarchy of clusters. There are two basic approaches:\n* bottom-up (agglomerative): each observations starts in its own clusters and pairs of clusters are merged as one moves up the hierarchy\n* top-down (divisive): all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy\n\nHierarchical clustering struggles quite a lot with complexity. The most popular one is hierarchical agglomerative clustering (HAC).\n\nI will explore the first one as it is far the most popular. And in this case it is all about two topics: how to choose crrect number of clusters and how to get perfect link. The method issimple: first explore denrogram to find out optimal clusters number. Second just check all possible links. The choice is highly arbitrary and it depends what you really want from the given data set.","25b2ba54":"I make very regular cleaning:\n* detect the type of variable\n    * categorical (ordinal)\n    * categorical (nominal)\n    * numeric\n* Apply correction for NA's\n* Apply scaling\n* Transform nominal variables into dummy\n* Analyse the existence of interesting interactions\n* Simple features engineering\n\nThe data preparation is not granularly commented as it is not a point of this notebook. I focus on it in the different [job](https:\/\/www.kaggle.com\/jjmewtw\/prices-cleaning-analysis-estimation-in-stages) .","7250e5fe":"Scale and apply dumym modification for nominal variables. List all our variables.","439fa987":"We can easily settle the treshold line at the level of 50.000 and receive just 6 clusters.","02e6f38f":"I plot 4 different clustering methods (hierarchical with two links: ward and average) using the format used by scikit team.","ca6c669b":"According to 'elbow method', just 4 clusters are enough. Fit it and check inertia.","9ef36ecf":"What so-called DBSCAN exactly does is to find core samples of high density and expand clusters from them. Hence, the underlying concept is simple: find the high density areas and build cluster around, sounds logic.","c4dcc2a6":"Identify ordinal variables. Only one ordinal: size of the cloth.","2cf1113b":"Simple look at Spearman correlation. Huge correlation between rating variable - what makes sense.","4c4520e6":"# K-means clustering","9d78a53f":"# Expectation-maximization algorithm","28b3917a":"Identifying data type and checking NA's. Correction for NA's.","4c295be0":"![Smaller.jpg](attachment:Smaller.jpg)","22027add":"# Data preparation","68ca38b3":"The most popular clustering method which aims on searating groups by division of their variance. The K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum-of-squares criterion. The assumption about inertia takes into account convexity what is not always a case, especially considering irregularly shaped manifolds. It is not normalized, so I usually run PCA before using k-means method."}}