{"cell_type":{"dcc56fae":"code","51cfa843":"code","68e4a723":"code","4057420a":"code","b5ab9484":"code","4c134c92":"code","e94a04e0":"code","5e26c3fc":"code","9283bb16":"code","7deccd06":"code","249276e4":"code","d1bdc844":"code","ff47bcb0":"code","5b2ddad7":"code","314c4323":"code","252523bd":"code","0b652a01":"code","5a9dc2c7":"code","c23fcb95":"code","b19bf7da":"code","8ea2dc7d":"code","83e0e5b7":"code","9605b6e3":"code","fe8c4dd7":"code","7b4ccfe9":"code","c4fc8c7a":"code","2273f5f7":"code","29cc2a61":"code","78ff110c":"code","56ea2374":"code","b4f149ca":"code","e750fce2":"code","3b35f9f8":"code","41fb041d":"code","3303639a":"code","757fba14":"code","d4ad774e":"code","e793c7aa":"code","5ffa06fb":"code","85d47ccf":"code","b6588238":"code","f0693e66":"code","5b93a2ef":"code","1f623ad2":"code","d02a219c":"code","643a8afe":"code","45836499":"code","a3a2f1ea":"code","dcb38456":"code","7eded9dd":"code","c6b1fe61":"code","aecf70d6":"code","0b7d3fd9":"code","a48458f3":"code","09bfeb66":"code","8685a510":"code","dbad6c23":"code","5d3bd273":"code","24b06af9":"code","afdba152":"code","d041c1bd":"code","7838994f":"code","d5026466":"code","72bbaead":"code","f2e85baa":"code","849fffb8":"code","ae594d1e":"code","33faf27b":"code","d784b05b":"code","bb3bbd3a":"markdown","4f9eb28f":"markdown","fbd611f1":"markdown","81680afc":"markdown","3d33e868":"markdown","7f69fc1d":"markdown","e0962c4e":"markdown","28c099f6":"markdown","4aac8362":"markdown","37141134":"markdown","bad14f32":"markdown","06987e9d":"markdown","10edd6ba":"markdown","0d611a2a":"markdown","c50dbf6a":"markdown","a1d6f57d":"markdown","ced3fa85":"markdown","3e852e7c":"markdown","07f48653":"markdown","8876718d":"markdown","8f8837d2":"markdown","dcdee44a":"markdown","84aba5df":"markdown","a126b23c":"markdown","94da34e0":"markdown","9548d9e6":"markdown","8d14de49":"markdown","29ed9ab1":"markdown","cb6e3f25":"markdown","dcf1f8de":"markdown","135d4865":"markdown","e5bbdcf1":"markdown","0edc9aa8":"markdown","c8e5e1b1":"markdown","f57c9c10":"markdown","9866516e":"markdown","9495e668":"markdown","41716f73":"markdown","fda530e9":"markdown","0d1c1f0b":"markdown"},"source":{"dcc56fae":"# Imports and Libraries \nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sklearn\nimport string\nimport tensorflow as tf\nimport time\nimport warnings\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RationalQuadratic, Exponentiation, RBF\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tensorflow import feature_column","51cfa843":"# Load data\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain.head()","68e4a723":"train.head()","4057420a":"train.isnull().sum()","b5ab9484":"test.isnull().sum()","4c134c92":"train.dtypes","e94a04e0":"test.dtypes","5e26c3fc":"def removeNans(data):\n    embark_label = data[\"Embarked\"].mode()[0]\n    data[\"Cabin\"] = data[\"Cabin\"].replace(np.NAN,  \"Unknown\")\n    data[\"Embarked\"] = data[\"Embarked\"].replace(np.NAN, embark_label)\n    data[\"Age\"] = data[\"Age\"].replace(np.NAN, data[\"Age\"].mean())\n    data[\"Fare\"] = data[\"Fare\"].replace(np.NAN, data[\"Fare\"].mean())\n    return data","9283bb16":"# From source 1 \n# function: to return the substring desired\ndef substrings_in_string(big_string, substrings):\n    for substring in substrings:\n        if big_string.find(substring) != -1:\n            return substring\n    # print(big_string) # For debug check\n    return np.nan\n\n# Replacing all titles with mr, mrs, miss, master\ndef replace_titles(x):\n    title=x['Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n        return 'Mr'\n    elif title in ['Countess', 'Mme']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\n\n# function: to apply the above fuctions and return the title column for the data\ndef make_titles(data):\n    data = copy.copy(data)\n    title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev', 'Dr', 'Ms', 'Mlle',\n            'Col', 'Capt', 'Mme', 'Countess', 'Don', 'Jonkheer']\n    data['Title']=data['Name'].apply(lambda x: substrings_in_string(x, title_list))\n    data['Title']=data.apply(replace_titles, axis=1)\n    data = data.drop('Name', axis = 1)\n    return data","7deccd06":"# cabin type is just the first letter of the cabin\n# from source 2\ndef make_cabin_type(data):\n    data = copy.copy(data)\n    data['Cabin_type'] = data[\"Cabin\"].apply(lambda cabin: cabin[0])\n    data = data.drop('Cabin',axis=1)\n    return data","249276e4":"# From source 2\ndef convert_faimly_member_size(size):\n    if size == 1:\n        return \"single\"\n    elif size < 5:\n        return \"medium\"\n    else:\n        return \"large\"\n\n# not source 2\ndef make_family_columns(data):\n    data = copy.copy(data)\n    data[\"family_member_size\"] = 1 + data[\"SibSp\"] + data[\"Parch\"]\n    data[\"family_size_type\"] = data[\"family_member_size\"].apply(convert_faimly_member_size)\n    return data\n","d1bdc844":"# get feature encoded data for the categories\ndef make_label_encode(data):\n    df = copy.copy(data)\n    if 'Name' in df.columns:\n        df = df.drop('Name', axis = 1)\n    if 'Ticket' in df.columns:\n        df = df.drop('Ticket', axis = 1)\n    df[df.select_dtypes(['object']).columns] = df.select_dtypes(['object']).apply(lambda x: x.astype('category'))\n    df[list(df.select_dtypes(['category']).columns+\"_val\")] = df.select_dtypes(['category']).apply(lambda x : x.cat.codes.astype(int))\n    return df","ff47bcb0":"# get one-hot encoded features for categories\ndef make_one_hot(data):\n    df = copy.copy(data)\n    if 'Name' in df.columns:\n        df = df.drop('Name', axis = 1)\n    if 'Ticket' in df.columns:\n        df = df.drop('Ticket', axis = 1)\n    df[df.select_dtypes(['object']).columns] = df.select_dtypes(['object']).apply(lambda x: x.astype('category'))\n    df = pd.get_dummies(df,columns = list(df.select_dtypes(['category']).columns), dtype=int)\n    return df","5b2ddad7":"# Scales data\n# It is assumed that all data being put into this function are a float or int64\ndef make_scaled(data):\n    df = copy.copy(data)\n    min_max_scaler = preprocessing.MinMaxScaler()\n    dfScaled = min_max_scaler.fit_transform(df)\n    df = pd.DataFrame(dfScaled, columns= df.columns)\n    return df","314c4323":"train.info()","252523bd":"def remove_useless(data):\n    df = copy.copy(data)\n    df = df.drop(list(list(df.select_dtypes(['category']).columns)), axis = 1)\n    df = df.drop('PassengerId', axis = 1)    \n    return df","0b652a01":"# data: dataset\n# encoder 0= Label Encode,  1 = one-hot\n# isScaled true or false\ndef make_data(data, encoder = 0, isScaled = True):\n    df = copy.copy(data)\n    # remove Nans\n    df = removeNans(df)\n    # Make titles\n    df = make_titles(df)\n    #print(df.columns) # debug\n    # Make Cabin Type\n    df = make_cabin_type(df)\n    #print(df.columns) # debug\n    # Make Family Columns\n    df = make_family_columns(df)\n    #print(df.columns) # debug\n    if encoder == 0:\n        df = make_label_encode(df)\n    else:\n        df = make_one_hot(df)\n    if isScaled:\n        df = remove_useless(df)\n        df = make_scaled(df)\n    return df","5a9dc2c7":"ohData = make_data(train, encoder = 1, isScaled = True)\nohData.corr()[\"Survived\"].sort_values(key=lambda x: abs(x), ascending=False)","c23fcb95":"edaTrain = make_data(train, 0, False)\nedaTrain.describe()","b19bf7da":"edaTrain.corr()[\"Survived\"].sort_values(key=lambda x: abs(x), ascending=False)","8ea2dc7d":"edaTrain.groupby('Cabin_type')['Survived'].mean()","83e0e5b7":"sns.countplot(x=\"Cabin_type\",hue=\"Survived\", data=edaTrain)\nplt.title(\"Survival of Cabin Type\")\nplt.show()","9605b6e3":"cabin_type_plot= sns.catplot(x=\"Cabin_type\", hue=\"Survived\", col=\"Survived\", data=edaTrain, kind=\"count\", sharey=False)\ncabin_type_plot.fig.subplots_adjust(top=0.7)\ncabin_type_plot.fig.suptitle(\"Survived by Cabin Type\")\ncabin_type_plot.set_xlabels(\"Cabin Types\")\ncabin_type_plot.set_ylabels(\"Count\")\nplt.show()","fe8c4dd7":"edaTrain.groupby('Pclass')['Survived'].mean()","7b4ccfe9":"sns.countplot(x=\"Pclass\",hue=\"Survived\", data=edaTrain)\nplt.title(\"Survival of Pclass\")\nplt.show()","c4fc8c7a":"edaTrain.groupby(\"Pclass\")[\"Survived\"].mean()","2273f5f7":"sns.countplot(x=\"Sex\",hue=\"Survived\", data=edaTrain)\nplt.title(\"Survival of Sex\")\nplt.show()","29cc2a61":"sns.FacetGrid(edaTrain, col=\"Survived\").map(sns.histplot, \"Age\", bins=25)\nplt.show()","78ff110c":"edaTrain.groupby(\"SibSp\")[\"Survived\"].mean()","56ea2374":"sns.countplot(x=\"SibSp\",hue=\"Survived\", data=edaTrain)\nplt.title(\"Survival of SibSp\")\nplt.show()","b4f149ca":"edaTrain.groupby(\"Parch\")[\"Survived\"].mean()","e750fce2":"sns.countplot(x=\"Parch\",hue=\"Survived\", data=train)\nplt.title(\"Survival of Parch\")\nplt.show()","3b35f9f8":"Fare_plot=sns.FacetGrid(train, col=\"Survived\", sharey=False)\nFare_plot.map(sns.histplot, \"Fare\",bins=20)\nFare_plot.fig.subplots_adjust(top=0.7)\nFare_plot.fig.suptitle(\"Survived by Fare\")\nFare_plot.set_xlabels(\"Fare\")\nFare_plot.set_ylabels(\"Count\")\nplt.show()","41fb041d":"edaTrain.groupby(\"Embarked\")[\"Survived\"].mean()","3303639a":"sns.countplot(x=\"Embarked\",hue=\"Survived\",data=edaTrain)\nplt.title(\"Survival of Embarked\")\nplt.show()","757fba14":"edaTrain.groupby(\"Title\")[\"Survived\"].mean()","d4ad774e":"title_plot= sns.catplot(x=\"Title\", hue=\"Survived\", col=\"Survived\", data=edaTrain, kind=\"count\", sharey=False)\ntitle_plot.fig.subplots_adjust(top=0.7)\ntitle_plot.fig.suptitle(\"Survived by Title\")\ntitle_plot.set_xlabels(\"Titles\")\ntitle_plot.set_ylabels(\"Count\")\nplt.show()","e793c7aa":"# data: dataset used\n# isCC: is used in CNN model\n# isValUsed: is validation data used, if so test data is needed to get accuracy on unseen data\ndef get_train_test(data, isCC = False, isValUsed = False):\n  X = data.drop(\"Survived\", axis=1)\n  y = data[\"Survived\"]\n  #print(\"X shape: \", X.shape)\n  #print(X.values.reshape(891, 27,1))\n  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n  if isCC==True:\n    X_train=X_train.values\n    y_train= y_train.values\n    X_val= X_val.values\n    y_val=y_val.values\n    X_train=np.expand_dims(X_train,axis=2)\n    X_val=np.expand_dims(X_val,axis=2)\n  if isValUsed:\n    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n    #print(X_test.shape)\n  else: \n    X_test = np.zeros(X_val.shape[0])\n    y_test = np.zeros(y_val.shape[0])\n  return (X_train, X_val, X_test, y_train, y_val, y_test)","5ffa06fb":"leData = make_data(train, encoder = 0, isScaled = True)\nohData = make_data(train, encoder = 1, isScaled = True)\ntrainDatasets = {\n    \"Label Encoded\": leData,\n    \"One-Hot Encoded\": ohData\n}","85d47ccf":"# Check data\nleData.info()","b6588238":"# check data\nohData.info()","f0693e66":"def make_CNN(inputShape):\n    model = tf.keras.Sequential([\n        #tf.keras.layers.Input(27,1),\n        tf.keras.layers.Conv1D(128, 2, activation='relu',input_shape=inputShape),\n        tf.keras.layers.MaxPooling1D(pool_size=2),\n        tf.keras.layers.Conv1D(128, 4, activation='relu'),\n        tf.keras.layers.GlobalAveragePooling1D(),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(100, activation=\"relu\"),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","5b93a2ef":"def train_CNN(data, isValUsed = True, epochs = 4000):\n    X_train, X_val, X_test, y_train, y_val, y_test = get_train_test(data, True, isValUsed)\n    x, y, z = X_train.shape\n    model = make_CNN((y,z))\n    early_stop = tf.keras.callbacks.EarlyStopping(patience=10, monitor='accuracy')\n    if isValUsed:\n        history = model.fit(X_train, y_train,epochs=epochs, \n          validation_data=(X_val,y_val), \n          callbacks=[early_stop],\n          verbose=0)\n    else:\n        history = model.fit(X_train, y_train,epochs=epochs, \n          #validation_data=(X_val,y_val), \n          callbacks=[early_stop],\n          verbose=0)\n    return model, history","1f623ad2":"#isValUsed = True\noutcome =[]\nprettyTable = {\n    \"Label Encoded\": [0,0,0,0],\n    \"One-Hot Encoded\": [0,0,0,0]\n} \nfor i in range(2):\n    if i == 0:\n        isVal = True\n    else:\n        isVal =False\n    for dataset in trainDatasets.keys():\n        X_train, X_val, X_test, y_train, y_val, y_test = get_train_test(trainDatasets[dataset], True, isVal)\n        model, history = train_CNN(trainDatasets[dataset], isVal)\n        if isVal:\n            score = model.evaluate(X_test,y_test, verbose=0)\n            prettyTable[dataset][0] = round(score[1],4)\n            prettyTable[dataset][2] = round(score[0],4)\n        else:\n            score = model.evaluate(X_val,y_val, verbose=0)\n            prettyTable[dataset][1] = round(score[1],4)\n            prettyTable[dataset][3] = round(score[0],4)\n        text = (\"With Val: \" + str(isVal)+\"\\nDataset: \"+ str(dataset)+\"\\nAccuracy: \"+str(round(score[1],4))+\"\\nLoss: \"+\n            str(round(score[0],4))+\"\\n\")\n        outcome.append(text)\n        print(text)\n        pd.DataFrame(history.history).plot()\n        plt.show()\nfor text in outcome:\n    print(text)\n\npretty = pd.DataFrame(prettyTable, index = [\"W\/ Val Accuracy\",\"W\/O Val Accuracy\", \"W\/ Val Loss\", \"W\/O Val Loss\"])\npretty","d02a219c":"def make_DNN(inputShape):\n    model = tf.keras.Sequential([\n      tf.keras.layers.Input(shape=(inputShape)),\n      tf.keras.layers.Dense(1000, activation='relu'), \n      tf.keras.layers.Dense(750, activation='relu'),  \n      tf.keras.layers.Dense(500, activation='relu'),  \n      tf.keras.layers.Dense(250, activation='relu'),  \n      tf.keras.layers.Dense(50, activation='relu'),\n      tf.keras.layers.Dense(1, activation='sigmoid')                          \n   ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","643a8afe":"def train_DNN(data, isValUsed = True, epochs = 4000):\n    X_train, X_val, X_test, y_train, y_val, y_test = get_train_test(data, False, isValUsed)\n    xShape = X_train.shape[1]\n    model = make_DNN(xShape)\n    early_stop = tf.keras.callbacks.EarlyStopping(patience=10, monitor='accuracy')\n    if isValUsed:\n        history = model.fit(X_train, y_train,epochs=epochs, \n          validation_data=(X_val,y_val), \n          callbacks=[early_stop],\n          verbose=0)\n    else:\n        history = model.fit(X_train, y_train,epochs=epochs, \n          #validation_data=(X_val,y_val), \n          callbacks=[early_stop],\n          verbose=0)\n    return model, history","45836499":"#isVal = False\noutcome =[]\nprettyTableDnn = {\n    \"Label Encoded\": [0,0,0,0],\n    \"One-Hot Encoded\": [0,0,0,0]\n} \nfor i in range(2):\n    if i == 0:\n        isVal = True\n    else:\n        isVal =False\n    for dataset in trainDatasets.keys():\n        X_train, X_val, X_test, y_train, y_val, y_test = get_train_test(trainDatasets[dataset], False, isVal)\n        model, history = train_DNN(trainDatasets[dataset], isVal)\n        if isVal:\n            score = model.evaluate(X_test,y_test, verbose=0)\n            prettyTableDnn[dataset][0] = round(score[1],4)\n            prettyTableDnn[dataset][2] = round(score[0],4)\n        else:\n            score = model.evaluate(X_val,y_val, verbose=0)\n            prettyTableDnn[dataset][1] = round(score[1],4)\n            prettyTableDnn[dataset][3] = round(score[0],4)\n        text = (\"With Val: \" + str(isVal)+\"\\nDataset: \"+ str(dataset)+\"\\nAccuracy: \"+str(round(score[1],4))+\"\\nLoss: \"+\n            str(round(score[0],4))+\"\\n\")\n        outcome.append(text)\n        print(text)\n        pd.DataFrame(history.history).plot()\n        plt.show()\nfor text in outcome:\n    print(text)\n\nprettyDNN = pd.DataFrame(prettyTableDnn, index = [\"W\/ Val Accuracy\",\"W\/O Val Accuracy\", \"W\/ Val Loss\", \"W\/O Val Loss\"])\nprettyDNN","a3a2f1ea":"lcd = list(leData.corr()[leData.corr()[\"Survived\"].abs() > 0.2].index) \nocd = list(ohData.corr()[ohData.corr()[\"Survived\"].abs() > 0.2].index) \ntrainDatasets['Label Encoded Correlated'] = leData[lcd]\ntrainDatasets['One-Hot Encoded Correlated'] = ohData[ocd]","dcb38456":"# set up dicts\nsgd = SGDClassifier(random_state = 42)\nlr = LogisticRegression(random_state = 42)\nsvm = SVC(random_state = 42)\nknn = KNeighborsClassifier()\ngpc = GaussianProcessClassifier(random_state = 42)\ndtc = DecisionTreeClassifier(random_state = 42)\n# Ensembles\nabc = AdaBoostClassifier(random_state = 42)\nrfc = RandomForestClassifier(random_state = 42)\ngbc = GradientBoostingClassifier(random_state = 42)\n\n# had to make custom copy function due to python being python\ndef copyDict(stuff):\n  newDict = {}\n  for i in stuff.keys():\n    info = copy.copy(stuff[i])\n    newDict[i] = info\n  return newDict\n\n# Set up skmodel for finding best accuracy\nskmodel = {\n    'Stochastic Gradient Descent':[sgd, dict(learning_rate = ['adaptive', 'optimal', 'constant', 'invscaling'],\n               eta0 = list(np.arange(0.001, 0.15, 0.001)),\n               max_iter = list(np.arange(1000.0,10000.0,10.0)),\n               random_state= [42],\n               early_stopping =[True])],\n    'Logistic Regression':[lr, dict(solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n              C = list(np.arange(1,20)),\n              max_iter = list(np.arange(100,1000,10))\n              )],\n    'Support Vector Classification':[svm,dict(kernel=['linear', 'poly', 'rbf', 'sigmoid'],\n               gamma = ['scale', 'auto'],\n               max_iter = [-1])],\n    'K Nearest Neighbors Classification':[knn, dict(n_neighbors = list(np.arange(5,50,5)),\n               algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute'],\n               leaf_size = list(np.arange(30,100, 10))\n              )],\n    'Gaussian Process Classifier':[gpc, dict(kernel = [(1.0 * RBF(1.0))],#, RationalQuadratic(length_scale=1.0, alpha=1.5)]#, Exponentiation(RationalQuadratic(), exponent=2), Exponentiation(RBF(), exponent=2)],\n              max_iter_predict = list(np.arange(100,1000,10))\n               )],\n    'Decision Tree Classifie':[dtc, dict(criterion = ['gini', 'entropy'],\n               max_depth = [None]+list(np.arange(10,100,10)),\n               min_samples_split = list(np.arange(2,10,2)),\n               min_samples_leaf =  list(np.arange(1,10)),\n               max_features = ['auto', 'sqrt', 'log2'] ,\n               max_leaf_nodes = [None]+list(np.arange(100,1000,10))       \n               )],\n# Ensembles\n#####################################################################################    \n#     'AdaBoost Classifier':[abc, dict(#n_estimators = list(np.arange(50,100,10)),\n#               learning_rate = list(np.arange(1,20,1)),\n#               algorithm = ['SAMME', 'SAMME.R']             \n#                )],\n#     'Random Forest Classifier':[rfc, dict(n_estimators = list(np.arange(100,500,10)),\n#               criterion = ['gini', 'entropy'],\n#               max_depth = [None]+list(np.arange(10,100,10)),\n#               min_samples_split = list(np.arange(2,10,2)),\n#               min_samples_leaf =  list(np.arange(1,10)),\n#               max_features = ['auto', 'sqrt', 'log2'] ,\n#               max_leaf_nodes = [None]+list(np.arange(100,1000,10))        \n#                )],\n#     'Gradient Boosting Classifier':[gbc, dict(loss=['deviance', 'exponential'],\n#                learning_rate = list(np.arange(0.05,0.5,0.05, dtype = float)),\n#                n_estimators = list(np.arange(100,500,10)),\n#                criterion=['friedman_mse', 'mse', 'mae'],\n#                min_samples_split = list(np.arange(2,10,2)),\n#                min_samples_leaf =  list(np.arange(1,10)),\n#                max_depth = list(np.arange(3,50,1)),\n#                max_features = ['auto', 'sqrt', 'log2'],\n#                max_leaf_nodes = [None]+list(np.arange(100,1000,10)),\n#                n_iter_no_change =[None]+list(np.arange(3,10,1)))] \n###################################################################################\n    }\nsk1 = copyDict(skmodel)\nsk2 = copyDict(skmodel)\nsk3 = copyDict(skmodel)\nsk4 = copyDict(skmodel)\n# Set up models dict for finding best accurcy for each model and dataset\nmodels = {\n    'Label Encoded': [trainDatasets['Label Encoded'], sk1],\n    'One-Hot Encoded': [trainDatasets['One-Hot Encoded'], sk2],\n    'Label Encoded Correlated': [trainDatasets['Label Encoded Correlated'], sk3] ,\n    'One-Hot Encoded Correlated': [trainDatasets['One-Hot Encoded Correlated'], sk4]\n}\n# make dict for pretty display of acc values\n# modelAcc = {\n#     'Label Encoded': [],\n#     'One-Hot Encoded': [],\n#     'Label Encoded Correlated': [] ,\n#     'One-Hot Encoded Correlated': []\n# }\nrowLabels =[\"Stochastic Gradient Descent\",\n\"Logistic Regression\",\n\"Support Vector Classification\",\n\"K Nearest Neighbors Classification\",\n\"Gaussian Process Classifier\",\n\"Decision Tree Classifier\",\n#\"AdaBoost Classifier\",\n#\"Random Forest Classifier\",\n#\"Gradient Boosting Classifier\"\n           ]\n","7eded9dd":"# import warnings\n# warnings.filterwarnings(\"ignore\")\nisValUsed = False\n####################################################\n# dataset = models[datasetName][0]\n# skmodelDict = model[datasetName][1]\n# skmModel = models[datasetName][1][modelName][0]\n# skmParam = models[datasetName][1][modelName][1]\n####################################################\n\n# Find the best Accuracy for Model\ndef makeAccDict(modelDict):\n  modelAcc = {\n    'Label Encoded': [],\n    'One-Hot Encoded': [],\n    'Label Encoded Correlated': [] ,\n    'One-Hot Encoded Correlated': []\n  }\n  for data in modelDict.keys():\n    X_train, X_val, X_test, y_train, y_val, y_test = get_train_test(modelDict[data][0], False, isValUsed)\n    (xP, yP) = (X_test, y_test) if isValUsed else (X_val, y_val)\n    for modelName in modelDict[data][1].keys():\n      # Find best model from random search cv \n      random = RandomizedSearchCV(modelDict[data][1][modelName][0], modelDict[data][1][modelName][1],\n                          n_iter=100, scoring='accuracy', n_jobs=-1, \n                          cv=3, verbose=0, random_state=42)\n      # Fit Model and save the best estimator\n      random.fit(X_train, y_train)\n\n      modelDict[data][1][modelName][0] = random.best_estimator_\n      predFit = random.predict(xP)\n      predAcc = accuracy_score(predFit,yP)\n      modelAcc[data].append(predAcc)\n  return modelAcc\nmodelAcc = makeAccDict(models)","c6b1fe61":"rowLabels =[\"Stochastic Gradient Descent\",\n\"Logistic Regression\",\n\"Support Vector Classification\",\n\"K Nearest Neighbors Classification\",\n\"Gaussian Process Classifier\",\n\"Decision Tree Classifier\"\n]\ndisplayModel = pd.DataFrame(modelAcc, index = rowLabels)\ndisplayModel","aecf70d6":"displayModel.idxmax()","0b7d3fd9":"displayModel.max()","a48458f3":"def makeEnsembleDict(dataset):\n  skDict = models[dataset][1]\n  skModels = []\n  for skName in skDict.keys():\n    # append models with weights\n    if skName != 'K Nearest Neighbors Classification' and skName != 'Gaussian Process Classifier':\n      #print(skName)\n      skModels.append(models[dataset][1][skName][0])\n  esModel ={\n      'AdaBoost Classifier':[abc, dict(#n_estimators = list(np.arange(50,100,10)),\n        base_estimator = skModels, learning_rate = list(np.arange(1,20,1)),\n        algorithm = ['SAMME']#, 'SAMME.R']             \n          )],\n      'Random Forest Classifier':[rfc, dict(n_estimators = list(np.arange(100,500,10)),\n              criterion = ['gini', 'entropy'],\n              max_depth = [None]+list(np.arange(10,100,10)),\n              min_samples_split = list(np.arange(2,10,2)),\n              min_samples_leaf =  list(np.arange(1,10)),\n              max_features = ['auto', 'sqrt', 'log2'] ,\n              max_leaf_nodes = [None]+list(np.arange(100,1000,10))        \n                )],\n      'Gradient Boosting Classifier':[gbc, dict(loss=['deviance', 'exponential'],\n                learning_rate = list(np.arange(0.05,0.5,0.05, dtype = float)),\n                n_estimators = list(np.arange(100,500,10)),\n                criterion=['friedman_mse', 'mse', 'mae'],\n                min_samples_split = list(np.arange(2,10,2)),\n                min_samples_leaf =  list(np.arange(1,10)),\n                max_depth = list(np.arange(3,50,1)),\n                max_features = ['auto', 'sqrt', 'log2'],\n                max_leaf_nodes = [None]+list(np.arange(100,1000,10)),\n                n_iter_no_change =[None]+list(np.arange(3,10,1)))] \n      }\n  return esModel","09bfeb66":"esModels = {\n    'Label Encoded': [trainDatasets['Label Encoded'], makeEnsembleDict('Label Encoded')],\n    'One-Hot Encoded': [trainDatasets['One-Hot Encoded'], makeEnsembleDict('One-Hot Encoded')],\n    'Label Encoded Correlated': [trainDatasets['Label Encoded Correlated'], makeEnsembleDict('Label Encoded Correlated')] ,\n    'One-Hot Encoded Correlated': [trainDatasets['One-Hot Encoded Correlated'], makeEnsembleDict('Label Encoded Correlated')]\n}","8685a510":"esAcc = makeAccDict(esModels)","dbad6c23":"rowLabels =[\n\"AdaBoost Classifier\",\n\"Random Forest Classifier\",\n\"Gradient Boosting Classifier\"\n]\nesDisplayModel = pd.DataFrame(esAcc, index = rowLabels)\nesDisplayModel","5d3bd273":"esDisplayModel.idxmax()","24b06af9":"esDisplayModel.max()","afdba152":"# make dict for easy access\n# pvd: pretty votting display \npvd = {\n    'Label Encoded': [],\n    'One-Hot Encoded': [],\n    'Label Encoded Correlated': [] ,\n    'One-Hot Encoded Correlated': []\n}\nisValUsed = False\n\n# contains combined values of predictions per dataset\nmodelPred = {\n}\n# make a sum prediction for each dataset useing the recorded best estimators for each\nfor data in models.keys():\n    X_train, X_val, X_test, y_train, y_val, y_test = get_train_test(models[data][0], False, isValUsed)\n    (xP, yP) = (X_test, y_test) if isValUsed else (X_val, y_val)\n    pred = np.zeros(yP.shape[0])\n    for modelName in models[data][1].keys():\n        model = models[data][1][modelName][0]\n        model.fit(X_train, y_train)\n        model.predict(xP)\n        pred += model.predict(xP)\n    modelPred[data] = copy.copy(pred)","d041c1bd":"# From model predictions look at accuracy when a number of models agree on classification \n# Sort through summed predictions per dataset\nfor i in modelPred.keys():\n    dataset = models[i][0]\n    X_train, X_val, X_test, y_train, y_val, y_test = get_train_test(models[i][0], False, isValUsed)\n    (xP, yP) = (X_test, y_test) if isValUsed else (X_val, y_val) # gets correct pred output value for acc\n    for j in range(1,len(models[i][1].keys())+1):\n        pred_arry = copy.copy(modelPred[i])\n        for x in range(len(pred_arry)):\n            pred_arry[x]=1 if pred_arry[x]>=j else 0\n        local_acc = accuracy_score(yP,pred_arry)\n        pvd[i].append(local_acc)","7838994f":"pvdTable = pd.DataFrame(pvd)\npvdTable","d5026466":"pvdTable.idxmax()","72bbaead":"pvdTable.max()","f2e85baa":"# stacking accuracy table \nsat = {\n    'Label Encoded': [],\n    'One-Hot Encoded': [],\n    'Label Encoded Correlated': [] ,\n    'One-Hot Encoded Correlated': []\n}\n# stacking classifiers by dataset\nsc = {}\n# Make stackModelEstimator dict for stack classifier\nfor dataset in models.keys():\n    stackModelEstimators = []\n    X_train, X_val, X_test, y_train, y_val, y_test = get_train_test(models[dataset][0], False, isValUsed)\n    (xP, yP) = (X_test, y_test) if isValUsed else (X_val, y_val)\n    for item in models[dataset][1].items():\n        i, j = item\n        stackModelEstimators.append((i,j[0]))\n    clf = StackingClassifier(\n            estimators=stackModelEstimators, verbose = 0, cv =3, n_jobs=-1)\n    clf.fit(X_train,y_train)\n    sc[dataset] = copy.copy(clf)\n    #print(xP.shape)\n    clf.predict(xP)\n    sat[dataset].append(clf.score(xP,yP))     ","849fffb8":"X_train, X_val, X_test, y_train, y_val, y_test = get_train_test(trainDatasets['One-Hot Encoded'], False, isValUsed)\nprint(X_train.shape)\nfor i in trainDatasets.keys():\n    print(i)\n    print(trainDatasets[i].columns)","ae594d1e":"satTable = pd.DataFrame(sat)\nsatTable","33faf27b":"trainTest = pd.concat([train,test])\ntrainTest = make_data(trainTest, encoder = 1)\ntrainTest = trainTest.drop('Survived',axis=1)\ntestData = trainTest.iloc[len(train):]\npredictions = sc['One-Hot Encoded'].predict(testData)\npredictions = [int(x) for x in predictions]\nsubmission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':predictions})\n# submission","d784b05b":"submission.to_csv('submission.csv',index = False)","bb3bbd3a":"# Titanic Data ","4f9eb28f":"# Voting Experiment","fbd611f1":"# Ensemble Methods","81680afc":"## 6. Make Encoders\nWe will be using two different encoders for this. The label encoder and the one hot encoder both of which are conveniently provided by Pandas. <br>\n- a good guide for encoding with pandas is [here](https:\/\/pbpython.com\/categorical-encoding.html)\n","3d33e868":"Make data for training<br>\nfor isCC an extra dimension is used to make a CNN possible on this 1D data","7f69fc1d":"# Models ","e0962c4e":"Dict note guide for models\n```\ndataset = models[datasetName][0]\nskmodelDict = model[datasetName][1]\nskmModel = models[datasetName][1][modelName][0]\nskmParam = models[datasetName][1][modelName][1]\n```","28c099f6":"Sex has the highest correlation with survival","4aac8362":"# #Stack_Life\nTo see sklearn stack classifier documentation click [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingClassifier.html)<br>\nDict note guide for models\n```\ndataset = models[datasetName][0]\nskmodelDict = model[datasetName][1]\nskmModel = models[datasetName][1][modelName][0]\nskmParam = models[datasetName][1][modelName][1]\n```","37141134":"## 9. Survival by Title","bad14f32":"Comparing the accuracies of all models the one with the highest accuracy is the One-hot Encoded stacking classifier model.","06987e9d":"## 8. Survival by Embarked","10edd6ba":"## 1. Check for Nulls and Types","0d611a2a":"# DNN Model ","c50dbf6a":"## 2. Survival by Pclass","a1d6f57d":"## 1. Survival by Cabin Type","ced3fa85":"### 6.1 Label Encoder","3e852e7c":"This type of voting should be similar to stacking. Lets find out.","07f48653":"# Making Submission","8876718d":"# Exploratory Data Analysis\nFor my plots I used seaborn. You can see the api for the plots [here](https:\/\/seaborn.pydata.org\/api.html).\n","8f8837d2":"## 4. Making Cabin Type Column\nEach cabin has a room number attached to it. We want to know the the deck level not the specific room number. This is also to generalize the data.","dcdee44a":"## 5. Survival by Sibsp","84aba5df":"## 7. Scaling Data\nScale the data with minmax sklearn funtion. Documentation is found [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html). This is to make all values between zero and 1. ","a126b23c":"## 8. Remove Useless Columns","94da34e0":"### 6.2 One-Hot Encoder","9548d9e6":"# Sklearn Fun 6 Models 3 possible Ensembles \nFor those interested in learning about the different supervised models that sklearn offers you can visit [here](https:\/\/scikit-learn.org\/stable\/supervised_learning.html). Listed bellow are the links to the different model documentation.<br>\n**Classification Models**\n- [Stochastic Gradient Descent](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html)\n- [Logistic Regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)\n- [Support Vector Classification](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC)\n- [K Nearest Neighbors Classification](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n- [Gaussian Process Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier)\n- [Decision Tree Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n<br>\n\n**Ensemble Models**\n- [AdaBoost Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html)\n- [Random Forest Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n- [Gradient Boosting Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html)\n","8d14de49":"Max accuracy was with label encoded DNN w\/o val.\n","29ed9ab1":"## 7. Survival by Fare\nPlease make note that the graphs have two y-scales","cb6e3f25":"## 3. Survival by Sex","dcf1f8de":"## 9. Make Data","135d4865":"## 2. Replace NANs function","e5bbdcf1":"A majority of the people where in an unknown deck. The plots show the number of people per cabin type that survived not the correlation.","0edc9aa8":"## 4. Survival by Age","c8e5e1b1":"The loss score for the w\/o val models were better than the ones with val. The accuracy for the non val was better than the val data. ","f57c9c10":"# Titanic Challenge\nSome of this is based off other sources. The sources are listed bellow.\n1. [Basic Feature Engineering](https:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/)  \n1. [Titanic Prediction with Different Models](https:\/\/www.kaggle.com\/lonnieqin\/classification-with-sklearn-and-tensorflow)\n1. [Titanic Survival Prediction : How I got to top 3%](https:\/\/www.kaggle.com\/shreyashgupta88\/titanic-survival-prediction-how-i-got-to-top-3)\n<br>\nMost of the work done in this notebook will be explained as it goes. I am no expert so take my methods with a grain of salt however I hope that this will serve as a good example to others for the use of sklearn and tensorflow. ","9866516e":"## 5. Make Family Size and Family Size Type\nFamily size type was sugested from source 2. It was decided upon the categories from EDA graphs.","9495e668":"## 6. Survival by Parch","41716f73":"## 3. Making Title Column\nThe purpose of making the title column is so that the data is more generalized so that the models can find trends easier.<br> Note: this generally works well with supervised classification.","fda530e9":"# CNN Model","0d1c1f0b":"# Data Wrangling "}}