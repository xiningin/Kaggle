{"cell_type":{"286be2e4":"code","e56de290":"code","8459f65a":"code","5ca25b2a":"code","ebd95d7f":"code","1e39c627":"code","cd057522":"code","3e83a277":"code","d89147e2":"code","8bd00aed":"code","1d3c18da":"code","0f422d15":"code","368f68e4":"code","10f1c083":"code","44ae45e2":"code","61d69e83":"code","43979a78":"code","576c1f58":"code","14860e80":"code","b2ca10c3":"code","a25c188b":"code","0a2d05e4":"code","7350390c":"code","db615868":"code","2163dadf":"code","7b877702":"code","56d3b891":"code","adf355b1":"code","3f1c2f30":"code","f13d8970":"code","84fb674e":"code","844d9a75":"code","e767875d":"code","d44d7a90":"code","3e9f6b1c":"code","c99af14b":"code","9641bc4d":"code","d3a07e90":"code","620431ff":"code","704ed119":"code","5865fbc5":"code","bb63fbb1":"code","c33a584c":"code","bcd29cd5":"markdown","7b86c44f":"markdown","0c29fd60":"markdown","10f5c10d":"markdown","9756f077":"markdown","e7958ac4":"markdown","2dd47487":"markdown","90feabba":"markdown","e53814c8":"markdown","ee06ea4e":"markdown","3acbe9fd":"markdown","0410448a":"markdown","a603c094":"markdown","0b800d40":"markdown","0b3b948b":"markdown","af104215":"markdown","f33e2e07":"markdown","c4b38e35":"markdown","19a74d0f":"markdown","4ca4fc85":"markdown","99934170":"markdown","60cf9e13":"markdown","9c9785ba":"markdown","5279d61b":"markdown","41cade53":"markdown","04f2685d":"markdown","92798a2e":"markdown","26d65970":"markdown","1603651c":"markdown","0d3fe43b":"markdown","18adb248":"markdown","0cbb075b":"markdown","d1323b12":"markdown","091bbb16":"markdown","198fe911":"markdown","07f2d985":"markdown","c0e4b698":"markdown","d8a30c52":"markdown","c48e6917":"markdown","38275cd5":"markdown","fd7fc6f7":"markdown","35254b96":"markdown","f7e748da":"markdown"},"source":{"286be2e4":"# Module import\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline","e56de290":"data = pd.read_csv('..\/input\/heart.csv')\ndata.head(5)","8459f65a":"data.info()","5ca25b2a":"data.isnull().sum()","ebd95d7f":"plot = data[data.target == 1].age.value_counts().sort_index().plot(kind = \"bar\", figsize=(15,4), fontsize = 15)\nplot.set_title(\"Age distribution\", fontsize = 20)","1e39c627":"male = len(data[data.sex == 1])\nfemale = len(data[data.sex == 0])\nplt.pie(x=[male, female], explode=(0, 0), labels=['Male', 'Female'], autopct='%1.2f%%', shadow=True, startangle=90)\nplt.show()","cd057522":"x = [len(data[data['cp'] == 0]),len(data[data['cp'] == 1]), len(data[data['cp'] == 2]), len(data[data['cp'] == 3])]\nplt.pie(x, data=data, labels=['CP(1) typical angina', 'CP(2) atypical angina', 'CP(3) non-anginal pain', 'CP(4) asymptomatic'], autopct='%1.2f%%', shadow=True,startangle=90)\nplt.show()","3e83a277":"plot = data[data.target == 1].trestbps.value_counts().sort_index().plot(kind = \"bar\", figsize=(15,4), fontsize = 15)\nplot.set_title(\"Resting blood pressure\", fontsize = 20)","d89147e2":"plt.hist([data.chol[data.target==0], data.chol[data.target==1]], bins=20,color=['blue', 'red'], stacked=True)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.title('Heart Disease Frequency for cholestoral ')\nplt.ylabel('Frequency')\nplt.xlabel('Chol in mg\/dl')\nplt.plot()","8bd00aed":"sizes = [len(data[data.fbs == 0]), len(data[data.fbs==1])]\nlabels = ['No', 'Yes']\nplt.pie(x=sizes, labels=labels, explode=(0.1, 0), autopct=\"%1.2f%%\", startangle=90,shadow=True)\nplt.show()","1d3c18da":"sizes = [len(data[data.restecg == 0]), len(data[data.restecg==1]), len(data[data.restecg==2])]\nlabels = ['Normal', 'ST-T wave abnormality', 'definite left ventricular hypertrophy by Estes criteria']\nplt.pie(x=sizes, labels=labels, explode=(0, 0, 0), autopct=\"%1.2f%%\", startangle=90,shadow=True)\nplt.show()","0f422d15":"plt.hist([data.thalach[data.target==0], data.thalach[data.target==1]], bins=20,color=['blue', 'red'], stacked=True)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.title('Heart Disease Frequency for maximum heart rate achieved')\nplt.ylabel('Frequency')\nplt.xlabel('Heart rate')\nplt.plot()","368f68e4":"sizes = [len(data[data.exang == 0]), len(data[data.exang==1])]\nlabels = ['No', 'Yes']\nplt.pie(x=sizes, labels=labels, explode=(0.1, 0), autopct=\"%1.2f%%\", startangle=90,shadow=True)\nplt.show()","10f1c083":"sizes = [len(data[data.slope == 0]), len(data[data.slope==1]), len(data[data.slope==2])]\nlabels = ['Upsloping', 'Flat', 'Downssloping']\nplt.pie(x=sizes, labels=labels, explode=(0, 0, 0), autopct=\"%1.2f%%\", startangle=90,shadow=True)\nplt.show()","44ae45e2":"sns.countplot('thal', data=data)\nplt.title('Frequency for thal')\nplt.ylabel('Frequency')\nplt.show()","61d69e83":"cp = pd.get_dummies(data['cp'], prefix = \"cp\", drop_first=True)\nthal = pd.get_dummies(data['thal'], prefix = \"thal\" , drop_first=True)\nslope = pd.get_dummies(data['slope'], prefix = \"slope\", drop_first=True)","43979a78":"new_data = pd.concat([data, cp, thal, slope], axis=1)\nnew_data.head(3)","576c1f58":"new_data.drop(['cp', 'thal', 'slope'], axis=1, inplace=True)\nnew_data.head(3)","14860e80":"X = new_data.drop(['target'], axis=1)\ny = new_data.target","b2ca10c3":"print(X.shape)","a25c188b":"X = (X - X.min())\/(X.max()-X.min())\nX.head(3)","0a2d05e4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)","7350390c":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)","db615868":"from sklearn.model_selection import GridSearchCV","2163dadf":"params = {'penalty':['l1','l2'],\n         'C':[0.01,0.1,1,10,100],\n         'class_weight':['balanced',None]}\nlr_model = GridSearchCV(lr,param_grid=params,cv=10)","7b877702":"lr_model.fit(X_train,y_train)\nlr_model.best_params_","56d3b891":"lr = LogisticRegression(C=1, penalty='l2')\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)","adf355b1":"#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, lr.predict(X_test))\nsns.heatmap(cm, annot=True)\nplt.plot()","3f1c2f30":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)","f13d8970":"for i in range(1,11):\n    knn = KNeighborsClassifier()\n    knn.fit(X_train, y_train)\n    print(\"k : \",i ,\"score : \",knn.score(X_test, y_test), end=\"\\n\" )","84fb674e":"#Confusion Matrix\ncm = confusion_matrix(y_test, knn.predict(X_test))\nsns.heatmap(cm, annot=True)\nplt.plot()","844d9a75":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state=1)\ndt.fit(X_train, y_train)\ndt.score(X_test, y_test)","e767875d":"#Confusion Matrix\ncm = confusion_matrix(y_test, dt.predict(X_test))\nsns.heatmap(cm, annot=True)\nplt.plot()","d44d7a90":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\ngbc.score(X_test, y_test)","3e9f6b1c":"#Confusion Matrix\ncm = confusion_matrix(y_test, gbc.predict(X_test))\nsns.heatmap(cm, annot=True)\nplt.plot()","c99af14b":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\nnb.score(X_test, y_test)","9641bc4d":"#Confusion Matrix\ncm = confusion_matrix(y_test, nb.predict(X_test))\nsns.heatmap(cm, annot=True)\nplt.plot()","d3a07e90":"from sklearn.ensemble import RandomForestClassifier\nfor i in range(1, 20):\n    rfc = RandomForestClassifier(n_estimators=i)\n    rfc.fit(X_train, y_train)\n    print('estimators : ', i, \"score : \", rfc.score(X_test, y_test), end=\"\\n\")","620431ff":"for i in range(1, 10):\n    rfc = RandomForestClassifier(n_estimators=100, max_depth=i)\n    rfc.fit(X_train, y_train)\n    print('max_depth : ', i, \"score : \", rfc.score(X_test, y_test), end=\"\\n\")","704ed119":"rfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)\nrfc.score(X_test, y_test)","5865fbc5":"#Confusion Matrix\ncm = confusion_matrix(y_test, rfc.predict(X_test))\nsns.heatmap(cm, annot=True)\nplt.plot()","bb63fbb1":"from sklearn.svm import SVC\nsvc = SVC(kernel='linear')\nsvc.fit(X_train, y_train)\nsvc.score(X_test, y_test)","c33a584c":"#Confusion Matrix\ncm = confusion_matrix(y_test, svc.predict(X_test))\nsns.heatmap(cm, annot=True)\nplt.plot()","bcd29cd5":"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set","7b86c44f":"Setting parameters for GridSearchCV","0c29fd60":"- Age (age in years)\n- Sex (1 = male; 0 = female)\n- CP (chest pain type)\n- TRESTBPS (resting blood pressure (in mm Hg on admission to the hospital))\n- CHOL (serum cholestoral in mg\/dl)\n- FPS (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n- RESTECH (resting electrocardiographic results)\n- THALACH (maximum heart rate achieved)\n- EXANG (exercise induced angina (1 = yes; 0 = no))\n- OLDPEAK (ST depression induced by exercise relative to rest)\n- SLOPE (the slope of the peak exercise ST segment)\n- CA (number of major vessels (0-3) colored by flourosopy)\n- THAL (3 = normal; 6 = fixed defect; 7 = reversable defect)\n- TARGET (1 or 0)","10f5c10d":"**Age**","9756f077":"**Restecg (resting electrocardiographic results)**","e7958ac4":"**Gradient Boosting Classifier**","2dd47487":"Split our Data. 80% - train, 20% - test data","90feabba":"## Dataset Columns (Features)\n","e53814c8":"### Concise summary of a Data","ee06ea4e":"**Normalize the data**","3acbe9fd":"In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\n* In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n* In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n\nk-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.","0410448a":"<br>\n## <center>  <b> **Visualization** <\/b> <\/center>\n<br>","a603c094":"**K Nearest Neighbor**","0b800d40":"### Missing values detection","0b3b948b":"**Slope**","af104215":"**Thal (thalassemia)**","f33e2e07":"**Chol (serum cholestoral in mg\/dl)**","c4b38e35":"**Data Preprocessing**\n\n'cp', 'thal' and 'slope' are categorical variables","19a74d0f":"**THALACH (maximum heart rate achieved)**","4ca4fc85":"**Sex**","99934170":"* Logistic Regression - 0.9016393442622951\n* K Nearest Neighbor - 0.8688524590163934\n* Decision Tree Classifier - 0.8032786885245902\n* Gradient Boosting Classifies - 0.8852459016393442\n* Gaussian NB - 0.9344262295081968\n* Random Forest Classifier - 0.9180327868852459\n* Support Vector Machine - 0.9016393442622951\n\nThe best option shows the Gaussian NB model","60cf9e13":"**Random Forest Classifier**","9c9785ba":"Result (K Nearest Neighbor) - 0.8688524590163934.","5279d61b":"**Gaussian NB**","41cade53":"**Support Vector Machine**","04f2685d":"<br>\n<center> <b> ******Data Visualization****** <\/b> <\/center>\n<br>","92798a2e":"Result (Logistic Regression) - 0.9016393442622951.\n\nOur model (Logistic Regression) is giving good result.","26d65970":"**All Score**","1603651c":"removing target columns from dataset","0d3fe43b":"Result (Decision Tree Classifier) - 0.8032786885245902.","18adb248":"We don't need 'cp', 'thal', 'slope' columns so we will drop them","0cbb075b":"**Decision Tree Classifier**","d1323b12":"Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.","091bbb16":"**TRESTBPS (resting blood pressure (in mm Hg on admission to the hospital))**","198fe911":"Removing the first level.","07f2d985":"**CP (chest pain type)**","c0e4b698":"**Exang**","d8a30c52":"In statistics, the logistic model (or logit model) is a widely used statistical model that, in its basic form, uses a logistic function to model a binary dependent variable; many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model; it is a form of binomial regression. ","c48e6917":"**Logistic Regression**","38275cd5":"<br>\n## <center>  <b> **Train** <\/b> <\/center>\n<br>","fd7fc6f7":"**FPS (fasting blood sugar > 120 mg\/dl)**","35254b96":"In computer science, Decision tree learning uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.","f7e748da":"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). A SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall."}}