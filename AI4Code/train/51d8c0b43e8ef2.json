{"cell_type":{"f0147b3a":"code","3f3cd194":"code","a520a082":"code","f2796171":"code","17fc2f94":"code","16807129":"code","be85646a":"code","70b9f53c":"code","0aa38b0b":"code","57b2b73f":"code","0c224ffe":"code","59e31acc":"code","cfacc6ec":"code","e2bf26c6":"code","5b3bab1b":"markdown","df16bc6a":"markdown","4e6380a4":"markdown","6dc38112":"markdown"},"source":{"f0147b3a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#   for filename in filenames:\n#      print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3f3cd194":"train = pd.read_csv('..\/input\/random-linear-regression\/train.csv')\ntest = pd.read_csv('..\/input\/random-linear-regression\/test.csv')","a520a082":"train.shape","f2796171":"test.shape","17fc2f94":"train.info()","16807129":"train.head()","be85646a":"train = train.dropna()","70b9f53c":"train.corr()","0aa38b0b":"train.describe()","57b2b73f":"import matplotlib.pyplot as plt\n%matplotlib inline","0c224ffe":"ax = plt.axes()\nplt.scatter(train['x'], train['y'], s = 2)\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.show()","59e31acc":"class LinearRegression:\n\n    def __init__(self, lr = 0.0001, n_iters = 10000):\t#lr is learning rate\n\n        self.lr = lr\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def fit(self, x_train, y_train):\n\n        n_samples, n_features = x_train.shape #samples and features are type int\n        self.weights = np.zeros(n_features) #Return a new <numpy array> of given shape and type, filled with zeros.\n        self.bias = 0\n\n        #gradient descent\n        for _ in range(self.n_iters):\n\n            #This method computes the matrix product between the DataFrame and the values of an other Series, DataFrame or a numpy array.\n            y_predicted = np.dot(x_train, self.weights) + self.bias\n            y_predicted = y_predicted.reshape(n_samples ,1)\n\n            dw = (1\/n_samples) * np.dot(x_train.T[0].reshape(1, n_samples), (y_predicted-y_train))\n            db = (1\/n_samples) * np.sum(y_predicted - y_train)\n            #print(dw[0][0]) -> converges to 0.0016354747054689244\n            #print(db)       -> converges to -0.10937704774137864\n\n            #if dw[0][0]!='nan' or dw[0][0]!='inf' or dw[0][0]!='-inf':\n            self.weights = self.weights - (self.lr * dw[0][0])\n\n            #if db!='nan' or db!='inf' or db!='-inf':\n            self.bias = self.bias - (self.lr * db)\n\n    #calculating mse, rmse and mae\n    def predict(self, x_test, y_test, bias, weights):\n\n        mse = 0.0\n        mae = 0.0\n\n        for i in x_test['x']:\n\n            diff = abs((i*weights[0]+bias) - y_test['y'][i])\n            mse = mse + diff**2\n            mae = mae + diff\n\n        mse = mse\/len(y_test)\n        rmse = math.sqrt(mse)\n        mae = mae\/len(y_test)\n\n        print(\"RMSE: \", rmse)\n        print(\"MSE: \", mse)\n        print(\"MAE: \", mae)","cfacc6ec":"#creating an instance of the class\nlr = LinearRegression()\n\n#testing the fit method\nx_train = (train.drop(['x'], axis = 1)).to_numpy() #shape: (699, 1) <class 'numpy.ndarray'>\ny_train = (train.drop(['y'], axis = 1)).to_numpy() #shape: (699, 1) <class 'numpy.ndarray'>\n\nlr.fit(x_train, y_train)\nlr.predict(test.drop(['y'], axis = 1), test.drop(['x'], axis = 1), lr.bias, lr.weights)","e2bf26c6":"ax = plt.axes()\nplt.scatter(train['x'], train['y'], s = 2) #train data is blue\nplt.scatter(test['x'], test['y'], s = 2, c = \"orange\") #test data is orange\nplt.figure(figsize = (4, 4))\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.plot([0, 100],[lr.bias, lr.weights[0]*100+lr.bias], c = 'red')\nax.text(50, 10, \"weight: \" + str(lr.weights[0]))\nax.text(50, 3, \"bias: \" + str(lr.bias))\n\nplt.show()","5b3bab1b":"# Data Visualization","df16bc6a":"# Linear Regression Using Gradient Descent","4e6380a4":"# Using basic EDA(Exploratory data analysis)","6dc38112":"Visualizing the best fit line along with the test data"}}