{"cell_type":{"75a53efb":"code","68459b7c":"code","beaaa046":"code","4ec9ca90":"code","0fe52dcb":"code","86e0ea4a":"code","d882834b":"code","2e7c19e5":"code","c8d08592":"code","4561bdad":"code","8065cc19":"code","aba3ab16":"code","ee8d95e5":"code","95eb3dde":"code","8df08c13":"code","c1e9285a":"code","985b1e9f":"code","321d6fb7":"code","c6bc97a0":"code","de5694fe":"code","11dbcbf6":"code","fa81ccee":"code","fe477d9c":"code","a51d3eac":"code","4f0be990":"code","2de48891":"code","1548a072":"code","cdd722af":"code","88ccfc4e":"code","c85c8847":"code","a31701a3":"code","dd41aaf3":"code","84df04d4":"code","6c0b8122":"code","3e7e3921":"code","3ad74ea4":"code","3bf1a0bd":"code","98ee1677":"code","04deb6a0":"code","8dad9596":"code","9744f554":"code","99eb8f96":"code","f90c1ddb":"code","3e02d85b":"code","603ae768":"code","e5db179d":"code","1574e5eb":"code","d2043a7f":"code","cedfcac5":"code","c23622fd":"code","412c0b76":"code","387c5020":"code","87289e07":"code","e7e3d90b":"markdown","51264903":"markdown","d9966aa7":"markdown","cad04e69":"markdown","fc2a3cac":"markdown","acdb9304":"markdown","663520b8":"markdown","c826e0c5":"markdown","d16e1dc7":"markdown","38748b5e":"markdown","a3d318ce":"markdown","206d9b5e":"markdown","4b3daf16":"markdown","9aab9831":"markdown","38454c42":"markdown","2194cd25":"markdown","fd6c3f71":"markdown","a3427212":"markdown","9355ed37":"markdown","752ed4ab":"markdown","9d41b80c":"markdown","f166bd6b":"markdown","bea1775c":"markdown","9b10aed0":"markdown","1926527e":"markdown","39f97cdf":"markdown","9b345709":"markdown","515af115":"markdown","62632080":"markdown","c4ad48ad":"markdown","ee606b1e":"markdown","1e9790bf":"markdown","dd6d65db":"markdown","aaf10de9":"markdown","74f0f389":"markdown","fd47e16d":"markdown","690806c5":"markdown","102bb7c8":"markdown","9714d748":"markdown","66390211":"markdown","1fbe805b":"markdown","9ece3759":"markdown","d437a72a":"markdown","74d6b4e3":"markdown","2fd048ca":"markdown","35344b56":"markdown","a916b3a6":"markdown","01c14bc3":"markdown","fa024569":"markdown","f45c2cad":"markdown","66e72ba0":"markdown","0bddcc48":"markdown","1b08d158":"markdown"},"source":{"75a53efb":"import pandas as pd \n\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","68459b7c":"train_df_na = (train_df.isnull().sum() \/ len(train_df)) * 100\ntrain_df_na = train_df_na.drop(train_df_na[train_df_na == 0].index).sort_values(ascending=False)[:20]\nmissing_data = pd.DataFrame({'Missing Ratio' :train_df_na})\nmissing_data.head(10)","beaaa046":"train_df = train_df.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1)\ntest_df = test_df.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1)","4ec9ca90":"for dataset in [train_df, test_df]:\n    dataset['SF'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF'] \n    dataset['TotalBath'] = dataset['BsmtFullBath'] + (1\/2) * dataset['BsmtHalfBath'] + dataset['FullBath'] + (1\/2) * dataset['HalfBath']\n    dataset['HasPool'] = dataset['PoolArea'].apply(lambda x : 1 if x>0 else 0)\n    dataset['Has2ndFloor'] = dataset['2ndFlrSF'].apply(lambda x : 1 if x>0 else 0)\n    dataset['HasGarage'] = dataset['GarageArea'].apply(lambda x : 1 if x>0 else 0)","0fe52dcb":"train_df = train_df.drop(['Id'], axis=1)","86e0ea4a":"LotShape_mapping = {\"Reg\": 1, \"IR1\": 2, \"IR2\": 3, \"IR3\": 4}\nLandSlope_mapping = {\"Gtl\": 1, \"Mod\": 2, \"Sev\": 3}\nExterQual_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nExterCond_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nBsmtQual_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5, \"NA\": 6}\nBsmtCond_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5, \"NA\": 6}\nBsmtExposure_mapping = {\"Gd\": 1, \"Av\": 2, \"Mn\": 3, \"No\": 4, \"NA\": 5}\nBsmtFinType1_mapping = {\"GLQ\": 1, \"ALQ\": 2, \"BLQ\": 3, \"Rec\": 4, \"LwQ\": 5, \"Unf\": 6, \"NA\": 7}\nBsmtFinType2_mapping = {\"GLQ\": 1, \"ALQ\": 2, \"BLQ\": 3, \"Rec\": 4, \"LwQ\": 5, \"Unf\": 6, \"NA\": 7}\nHeatingQC_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nCentralAir_mapping = {\"N\": 0, \"Y\": 1}\nKitchenQual_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nFireplaceQu_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nGarageQual_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nGarageCond_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nPavedDrive_mapping = {\"Y\": 1, \"P\": 2, \"N\": 3}\n\nfor dataset in [train_df, test_df]:\n    dataset['LotShape'] = dataset['LotShape'].map(LotShape_mapping)\n    dataset['LandSlope'] = dataset['LandSlope'].map(LandSlope_mapping)\n    dataset['ExterQual'] = dataset['ExterQual'].map(ExterQual_mapping)\n    dataset['ExterCond'] = dataset['ExterCond'].map(ExterCond_mapping)\n    dataset['BsmtQual'] = dataset['BsmtQual'].map(BsmtQual_mapping)    \n    dataset['BsmtCond'] = dataset['BsmtCond'].map(BsmtCond_mapping)    \n    dataset['BsmtExposure'] = dataset['BsmtExposure'].map(BsmtExposure_mapping)    \n    dataset['BsmtFinType1'] = dataset['BsmtFinType1'].map(BsmtFinType1_mapping)    \n    dataset['BsmtFinType2'] = dataset['BsmtFinType2'].map(BsmtFinType2_mapping)    \n    dataset['HeatingQC'] = dataset['HeatingQC'].map(HeatingQC_mapping) \n    dataset['CentralAir'] = dataset['CentralAir'].map(CentralAir_mapping)        \n    dataset['KitchenQual'] = dataset['KitchenQual'].map(KitchenQual_mapping)             \n    dataset['GarageQual'] = dataset['GarageQual'].map(GarageQual_mapping)        \n    dataset['GarageCond'] = dataset['GarageCond'].map(GarageCond_mapping)        \n    dataset['PavedDrive'] = dataset['PavedDrive'].map(PavedDrive_mapping)        ","d882834b":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncorr_matrix = train_df.corr()\nf, ax = plt.subplots(figsize=(25, 20))\nsns.heatmap(corr_matrix, vmax=.8, cmap=\"Blues\", square=True)","2e7c19e5":"import numpy as np\n\nn_largest_top = 31\nn_smallest_top = 15\ntop_large_corr = corr_matrix.nlargest(n_largest_top, 'SalePrice')['SalePrice']\ntop_small_corr = corr_matrix.nsmallest(n_smallest_top, 'SalePrice')['SalePrice']\nprint(\"Top Largest Corrolations :\") \nprint(top_large_corr)\nprint(\"____________________________\")\nprint(\"Top Smallest Corrolations :\") \nprint(top_small_corr)\nnum_attrs = np.append(top_large_corr.index.values, top_small_corr.index.values, axis=0)","c8d08592":"cm = train_df[num_attrs].corr()\nf, ax = plt.subplots(figsize=(20, 15))\nhm = sns.heatmap(cm, cmap=\"Blues\", annot=True, square=True, fmt='.2f', yticklabels=num_attrs, annot_kws={'size': 8}, xticklabels=num_attrs)","4561bdad":"train_df.describe(include=['O'])","8065cc19":"for attr in train_df.select_dtypes(include='object'):\n    print(train_df[[attr, \"SalePrice\"]].groupby([attr], as_index=False).mean().sort_values(by='SalePrice', ascending=False))\n    print(\"\\n ________________________ \\n\")","aba3ab16":"cat_attrs =[\"MSZoning\",\"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"HouseStyle\", \"RoofStyle\", \n            \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"Electrical\", \"Functional\", \n            \"GarageType\", \"GarageFinish\", \"SaleType\", \"SaleCondition\"]","ee8d95e5":"for var in ['OverallQual', 'SF', 'GrLivArea']:\n    data = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\n    data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), label= 'Affect of ' + var + ' on SalePrice')","95eb3dde":"drop_indexes = train_df.sort_values(by='SF', ascending=False)[:2].index\ntrain_df = train_df.drop(drop_indexes)","8df08c13":"for var in ['OverallQual', 'SF', 'GrLivArea']:\n    data = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\n    data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), label= 'Affect of ' + var + ' on SalePrice')","c1e9285a":"from scipy import stats\n\nprint(train_df['SalePrice'].describe())\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nsns.distplot(train_df['SalePrice'].dropna() , fit=stats.norm);\nplt.subplot(1,2,2)\nprob=stats.probplot(train_df['SalePrice'].dropna(), plot=plt)","985b1e9f":"import math\n\ntrain_df['SalePrice'] = [np.log(x) for x in train_df['SalePrice']]\n\nprint(train_df['SalePrice'].describe())\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nsns.distplot(train_df['SalePrice'].dropna() , fit=stats.norm);\nplt.subplot(1,2,2)\nprob=stats.probplot(train_df['SalePrice'].dropna(), plot=plt)","321d6fb7":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attr_names):\n        self.attr_names = attr_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attr_names].values","c6bc97a0":"from sklearn.preprocessing import MultiLabelBinarizer\n\nclass LabelBinarizerPipelineFriendly(MultiLabelBinarizer):\n    def fit(self, X, y=None):\n        super(LabelBinarizerPipelineFriendly,self).fit(X)\n    def transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).transform(X)\n    def fit_transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).fit(X).transform(X)","de5694fe":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nnum_data_attrs = [x for x in num_attrs if not x=='SalePrice']\nnum_pipeline = Pipeline([\n    ('selector', DataFrameSelector(num_data_attrs)),\n    ('imputer',  SimpleImputer(strategy=\"median\")),\n    ('std_scaler', StandardScaler()),\n])","11dbcbf6":"cat_pipeline = Pipeline([\n    ('selector', DataFrameSelector(cat_attrs)),\n    ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n    ('binarizer', LabelBinarizerPipelineFriendly()),\n])","fa81ccee":"from sklearn.pipeline import FeatureUnion\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('num_pipeline', num_pipeline),\n    ('cat_pipeline', cat_pipeline),\n])","fe477d9c":"def find_outliers(model, X, y, sigma=3):\n    y_pred = pd.Series(model.predict(X), index=y.index)\n        \n    resid = y - y_pred\n    mean_resid = resid.mean()\n    std_resid = resid.std()\n\n    z = (resid - mean_resid)\/std_resid    \n    outliers = z[abs(z)>sigma].index\n    return outliers","a51d3eac":"from sklearn.linear_model import Ridge\n\ntrain_data = train_df.drop(['SalePrice'], axis=1)\ntrain_labels = train_df['SalePrice']\n\nmodel = Ridge()\ntrain_prepared = full_pipeline.fit_transform(train_data)\nmodel.fit(train_prepared, train_labels)\noutliers = find_outliers(model, train_prepared, train_labels)\ntrain_df = train_df.drop(np.asarray(outliers) )","4f0be990":"train_data = train_df.drop(['SalePrice'], axis=1)\ntrain_labels = train_df['SalePrice']","2de48891":"train_prepared = full_pipeline.fit_transform(train_data)","1548a072":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\n\ndef rms_score_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, train_prepared, train_labels, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse.mean())","cdd722af":"from sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nprint(\"Linear Regression : \", rms_score_cv(linear_reg) )","88ccfc4e":"from sklearn.linear_model import Ridge\n\nridge = Ridge(alpha=0.6, solver='cholesky')\nrms_score_cv(ridge)","c85c8847":"from sklearn.linear_model import Lasso\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n# lasso = Lasso(alpha =0.0005, random_state=1)\nrms_score_cv(lasso)","a31701a3":"from sklearn.linear_model import ElasticNet\n\ne_net = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n# e_net = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\nrms_score_cv(e_net)","dd41aaf3":"from sklearn.linear_model import RANSACRegressor\n\nransac = RANSACRegressor()\nrms_score_cv(ransac)","84df04d4":"from sklearn.svm import LinearSVR\n\nsvm_reg = LinearSVR(epsilon=0.01)\nrms_score_cv(svm_reg)","6c0b8122":"from sklearn.svm import SVR\n\nsvm_poly_reg = SVR(kernel='poly', degree=2, C=100, epsilon=0.1)\nrms_score_cv(svm_poly_reg)","3e7e3921":"from sklearn.tree import DecisionTreeRegressor\n\ndecision_tree = DecisionTreeRegressor(max_depth=7, random_state=42)\nrms_score_cv(decision_tree)","3ad74ea4":"from sklearn.ensemble import RandomForestRegressor\n\nrand_forest = RandomForestRegressor(max_depth=15, random_state=42)\nrms_score_cv(rand_forest)","3bf1a0bd":"from sklearn.ensemble import VotingRegressor\n\nvoting_reg = VotingRegressor(\n        estimators=[('e_net', e_net), ('ridge', ridge), ('svm_poly_reg', svm_poly_reg), ('rand_forest', rand_forest)])\nrms_score_cv(voting_reg)","98ee1677":"from sklearn.ensemble import BaggingRegressor\n\nbagging_reg = BaggingRegressor(e_net, n_estimators=300, bootstrap=True, n_jobs=-1)\n# rms_score_cv(bagging_reg)","04deb6a0":"from sklearn.ensemble import BaggingRegressor\n\nbagging_reg = BaggingRegressor(e_net, n_estimators=500, bootstrap=False, n_jobs=-1)\n# rms_score_cv(bagging_reg)","8dad9596":"from sklearn.ensemble import ExtraTreesRegressor\n\nextra_tree = ExtraTreesRegressor(random_state=42)\nrms_score_cv(extra_tree)","9744f554":"from sklearn.ensemble import AdaBoostRegressor\n\nada_boost = AdaBoostRegressor(e_net, n_estimators=500, learning_rate=0.5)\n# rms_score_cv(ada_boost)","99eb8f96":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr = GradientBoostingRegressor(max_depth=5, n_estimators=15, learning_rate=0.6)\nrms_score_cv(gbr)","f90c1ddb":"estimators = [ e_net, ridge, svm_poly_reg, rand_forest, extra_tree]\nslice = int((0.8)*(len(train_prepared)))\ntrain_d = train_prepared[:slice]\ntrain_l = train_labels[:slice]\nval_d = train_prepared[slice: ]\nval_l = train_labels[slice:]","3e02d85b":"for estimator in estimators:\n  estimator.fit(train_d, train_l)","603ae768":"val_preds = np.empty((len(val_d), len(estimators)), dtype=np.float32)\nfor index, estimator in enumerate(estimators):\n    val_preds[:, index] = model.predict(val_d)\nblender = RandomForestRegressor(n_estimators=200, oob_score=True, random_state=42)\nblender.fit(val_preds, val_l)\nblender.oob_score_","e5db179d":"from sklearn.base import RegressorMixin\nfrom sklearn.base import clone\nfrom sklearn.model_selection import KFold\n\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n    def fit(self, X, y=None):\n        \n        X = X\n        y = y.values\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n    def get_metafeatures(self, X):\n        return np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","1574e5eb":"stacked_averaged_models = StackingAveragedModels(base_models = (e_net, ridge, svm_poly_reg, rand_forest), meta_model = lasso)\n\n# rms_score_cv(stacked_averaged_models)","d2043a7f":"el_net = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n# stacked_averaged_models = StackingAveragedModels(base_models = (e_net, ridge, svm_poly_reg, rand_forest, extra_tree),meta_model = lasso)\n\nel_net.fit(train_prepared, train_labels)","cedfcac5":"test_prepared = full_pipeline.transform(test_df)","c23622fd":"pred = el_net.predict(test_prepared)\npredSalePrice = [np.exp(x) for x in pred]","412c0b76":"\noutput = pd.DataFrame({\n    'Id' : test_df['Id'],\n    'SalePrice' : predSalePrice\n})","387c5020":"output.head() ","87289e07":"output.to_csv('prediction.csv', index=False)","e7e3d90b":"Now it's time to delete outliers in our data as they can degrade our model and prediction. Below we plot the three most corrolated features with 'SalePrice' and if there exist any outlier we will remove them.","51264903":"## Transformation Pipelines","d9966aa7":"Now it's time to select useful categorical features for our model.","cad04e69":"## Random Forest","fc2a3cac":"There are some categorical features which their categories are actually ordinal so it can be a good idea to convert them to numerical features. For instance \"ExerQual\" feature has values below :<br>\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Ex  &emsp; Excellent<br>\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Gd\t &emsp; Good<br>\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; TA\t &emsp; Average\/Typical<br>\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Fa\t &emsp; Fair<br>\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Po  &emsp; Poor<br>\nso they can be mapped to numbers from 1 to 5 to conserve their ordinal quality.","acdb9304":"## Log Transform","663520b8":"Pipeline for numerical features which aplies an imputer to put median of each feature for instances which doesn't have value for that feature and after that standardizing features using standard scaler.","c826e0c5":"Linear SVM Regression","d16e1dc7":"Below you can see the heatmap of corrolation matrix which shows how corrolated each pair of numerical features are. This can asist us in filtering some features for our model.","38748b5e":"## Preparing Train Data for Prediction","a3d318ce":"# 7)Predict Test Data","206d9b5e":"I chose the features which are strongly corrolated with 'SalePrice' as numerical attributed which will be used in future model.","4b3daf16":"We need to know how much each feature is useful for us to predict prices which means it should be corrolated with 'SalePrice'. After calculating corrolations we gain an insight of the most important features which we should use them in our model.","9aab9831":"2)","38454c42":"Another pipeline for categorical features which first use an imputer to replace missing values of features which the most frequent value and then applies a label binarizer.","2194cd25":"As you can see there is a high ratio of missing values in some features. I deleted the features having missing ratio greater that 80 percent. ","fd6c3f71":"You can observe that there is long tail of outlying properties with high sale prices. This causes to biase the mean much higher than the median. For normalizing 'SalePrice' we use log transform.","a3427212":"## Ensemble Learning","9355ed37":"## Cross Validation","752ed4ab":"And the full pipeline for transforming our data which consists of the two pipelines you see above.","9d41b80c":"Now we need some feature engineering. These new features are some new meaningful features which I added to train and test datasets.","f166bd6b":"# 3) Feature Selection","bea1775c":"## Ridge Regression","9b10aed0":"## Ransac","1926527e":"# 2) Data Preparation","39f97cdf":"## Elastic Net Regression","9b345709":"## Selecting Categorical Features","515af115":"Then 'Id' feature can be deleted as it doesn't give us any information and it's not needed.","62632080":"## Deleting more outliers","c4ad48ad":"# 6) Testing Models","ee606b1e":"## SVM Regression","1e9790bf":"## Deleting Outliers","dd6d65db":"## Decision Tree","aaf10de9":"# 1) Load Data","74f0f389":"# 4) More Data Preparation","fd47e16d":"1)","690806c5":"Now we need to know how many missing values we have in each field and if they are considerable we should delete this field as we don't have it for many instances and it will not be helpful.","102bb7c8":"## Adding a New Feature","9714d748":"## Lasso Regression","66390211":"### Pasting","1fbe805b":"### Stacking","9ece3759":"### Bagging","d437a72a":"## Linear Regression","74d6b4e3":"### AdaBoost","2fd048ca":"## Deleteing Missing Data","35344b56":"### Extra Tree","a916b3a6":"### Gradient Boosting","01c14bc3":"Non-linear SVM Regression","fa024569":"# 5) Data Transformation","f45c2cad":"Firstly, we load train and test data into dataframe using pandas library because it makes handling data much easier and efficient.","66e72ba0":"## Converting Categorical Features to Numerical","0bddcc48":"### Voting","1b08d158":"## Selecting Numerical Features Using Corrolation"}}