{"cell_type":{"559fefc0":"code","050f79a9":"code","fac4dd1c":"code","7db804de":"code","f89ea34b":"code","678814f3":"code","7f16ded1":"code","1f6799ac":"code","fda36ea9":"code","ece76c1f":"code","35af6f2a":"markdown"},"source":{"559fefc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","050f79a9":"train_data = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","fac4dd1c":"X = train_data.drop(['label'], axis=1)\ny = train_data['label']\ndel train_data","7db804de":"X = X \/ 255.; test_data = test_data \/ 255.\nX = X.values.reshape(-1, 28, 28, 1)\ntest_data = test_data.values.reshape(-1, 28, 28, 1)","f89ea34b":"import tensorflow as tf","678814f3":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[28, 28, 1]),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n    \n    tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n    \n    tf.keras.layers.Flatten(),\n    \n    tf.keras.layers.Dense(units=256, activation='relu'),\n    tf.keras.layers.Dropout(.15),\n    tf.keras.layers.Dense(units=128, activation='relu'),\n    tf.keras.layers.Dropout(.1),\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    \n    tf.keras.layers.Dense(units=10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nlearning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='acc',\n    patience=5,\n    factor=0.5,\n    min_lr=0.00001\n)","7f16ded1":"y = tf.keras.utils.to_categorical(y, num_classes=10)\n\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False, \n        featurewise_std_normalization=False,    \n        samplewise_std_normalization=False, \n        zca_whitening=False,  \n        rotation_range=10, \n        zoom_range = 0.1,\n        width_shift_range=0.1, \n        height_shift_range=0.1, \n        horizontal_flip=False, \n        vertical_flip=False) \ndatagen.fit(X)\n\nfrom sklearn.model_selection import train_test_split\nXtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=.1, random_state=333)","1f6799ac":"history = model.fit(datagen.flow(Xtrain, ytrain, batch_size=100), validation_data=(Xvalid, yvalid), epochs = 100, callbacks=[learning_rate_reduction])","fda36ea9":"ypred = model.predict(test_data)\nfor i in range(ypred.shape[0]):\n    pred = ypred[i, :]\n    ypred[i, 0] = list(pred).index(pred.max())","ece76c1f":"submission = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\nsubmission.iloc[:, 1] = ypred[:, 0].astype(np.int)\nsubmission.to_csv('submission.csv', index=False)","35af6f2a":"---"}}