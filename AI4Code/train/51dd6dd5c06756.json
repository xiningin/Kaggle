{"cell_type":{"44482b93":"code","ade2e1ea":"code","8fc558e4":"code","36a07520":"code","02cb2aeb":"code","93831fc9":"code","87a758ec":"code","7fb40cfb":"code","87303161":"markdown","4ea76318":"markdown","83afd30b":"markdown","83d789c0":"markdown","be3d4910":"markdown","8aef764e":"markdown","cf5b6ac7":"markdown"},"source":{"44482b93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ade2e1ea":"import pandas as pd\nimport numpy as np\n\nX_train_csv = pd.read_csv(\"..\/input\/used-car-price-dataset-competition-format\/X_train.csv\")\nX_test_csv  = pd.read_csv(\"..\/input\/used-car-price-dataset-competition-format\/X_test.csv\")\ny_train_csv = pd.read_csv(\"..\/input\/used-car-price-dataset-competition-format\/y_train.csv\")\ny_test_csv  = pd.read_csv(\"..\/input\/used-car-price-dataset-competition-format\/test_label\/y_test.csv\")\n\ndef EDA(name, df) :\n    print(\"== ============================================================\")\n    print(\"== EDA : \", name)\n    print(\"== ============================================================\")\n    \n    print(\">>> all column info\")\n    print(df.columns)\n    \n    print(\">>> inlude object column info\")\n    print(df.select_dtypes(include=object).columns)\n    for col in df.select_dtypes(include=object).columns :\n        print(\"-- ------------------------------------------------------------\")\n        print(col, \"-\", df[col].nunique(), \" : \", df[col].unique())\n        print(df[col].value_counts())\n        print(\"null check : \", df[col].isnull().sum())\n        \n    print(\">>> exclude object column info\")\n    print(df.select_dtypes(exclude=object).columns)\n    for col in df.select_dtypes(exclude=object).columns :\n        print(\"-- ------------------------------------------------------------\")\n        print(col, \"-\", df[col].unique()[:10])\n        print(df[col].describe())\n        print(\"null check : \", df[col].isnull().sum())\n    \n    print(\"== ============================================================\")\n    print(df.isnull().sum())\n    print(df.info())\n    print(df.describe())\n    print(\"== ============================================================\\n\\n\")\n\nEDA(\"X_train_csv\", X_train_csv)\nEDA(\"y_train_csv\", y_train_csv)\nEDA(\"X_test_csv\", X_test_csv)\n","8fc558e4":"## User Defined Variable\ntarget_col = \"price\"\ndrop_col = [\"carID\"]\nis_scaled = True\n\n## Strategy\n\"\"\"\nIndex(['carID', 'brand', 'model', 'year', 'transmission', 'mileage',\n       'fuelType', 'tax', 'mpg', 'engineSize'],\n      dtype='object')\n\"\"\"\n\n## Data Preprocessing\ndef data_preprocessing(df_train, df_test) :\n    split_len = df_train.shape[0]\n    \n    print(\" > One-Hot encoding pre count check ---------------------------\")\n    print(df_train.shape)\n    print(df_test.shape)\n    \n    df_dum = pd.get_dummies(pd.concat([df_train, df_test], axis=0))\n    df_train = df_dum[:split_len]\n    df_test  = df_dum[split_len:]\n    \n    print(\" > One-Hot encoding shape check -------------------------------\")\n    print(df_dum.shape)\n    \n    print(\" > One-Hot encoding split count check -------------------------\")\n    print(df_train.shape)\n    print(df_test.shape)\n    \n    print(\" > One-Hot encoding type check --------------------------------\")\n    print(df_train.info())\n    print(df_test.info())\n    \n    return df_train, df_test\n\nX = X_train_csv.drop(drop_col, axis=1)\ny = y_train_csv[target_col]\nX_sub = X_test_csv.drop(drop_col, axis=1)\n\nX, X_sub = data_preprocessing(X, X_sub)","36a07520":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_scaled_train = X_train\nX_scaled_test = X_test\nX_scaled_sub = X_sub\n\nif is_scaled :\n    X_scaled_train = scaler.transform(X_train)\n    X_scaled_test = scaler.transform(X_test)\n    X_scaled_sub = scaler.transform(X_sub)","02cb2aeb":"## model - RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_scaled_train, y_train)\n\n## predict\npred_train = model.predict(X_scaled_train)\npred_test = model.predict(X_scaled_test)\npred_sub = model.predict(X_scaled_sub)\n\n## Evaluation\nfrom sklearn.metrics import r2_score\nprint(\"\\n\\n>>> train -----------------------------------------------\")\nprint(\"score    : \", model.score(X_scaled_train, y_train))\nprint(\"r2_score : \", r2_score(y_train, pred_train))\n\nprint(\"\\n\\n>>> test ------------------------------------------------\")\nprint(\"score    : \", model.score(X_scaled_test, y_test))\nprint(\"r2_score : \", r2_score(y_test, pred_test))\n\nprint(\"\\n\\n>>> submission creation ---------------------------------\")\ndf_sub = pd.DataFrame({\"carID\":X_test_csv[\"carID\"], \"price\":pred_sub})\ndf_sub.to_csv(\"y_submission.csv\", index=False)\n\ndf_subr = pd.read_csv(\".\/y_submission.csv\")\ndisplay(df_subr.head())\n\ny_sub     = y_test_csv[\"price\"]\npred_subr = df_subr[\"price\"]\n\nprint(\"\\n\\n>>> submission ------------------------------------------\")\nprint(\"score    : \", model.score(X_scaled_sub, y_sub))\nprint(\"r2_score : \", r2_score(y_sub, pred_subr))","93831fc9":"## Enclosure.01 Cross_val\n## cross valid \nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold\nkfold = KFold(n_splits=5)\nscore = cross_val_score(RandomForestRegressor(random_state=42), X_scaled_train, y_train, cv=kfold, scoring=\"r2\")\nprint(score.mean())\nprint(score)\n# help(GridSearchCV)","87a758ec":"## Enclosure.02 GridSearchCV\n## GridSearchCV\n\"\"\"\ngrid_param = {\"n_estimators\":range(100, 1000, 100), \"min_samples_split\":range(1, 10, 1), \"min_samples_leaf\":range(1,5,1), \"max_features\":[\"log2\", \"sqrt\", \"auto\"]}\ngrid_search = GridSearchCV(model, grid_param)\ngrid_search.fit(X_scaled_train, y_train)\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.score(X_scaled_test, y_test))\n\"\"\"","7fb40cfb":"## Enclosure.03 FeatureImportances\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nftr_importances_values = model.feature_importances_\nftr_importances = pd.Series(ftr_importances_values, index = X_train.columns)\nftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n\nplt.figure(figsize=(8,6))\nplt.title(\"Top 20 Feature Importances\")\nsns.barplot(x=ftr_top20, y=ftr_top20.index)\nplt.show()","87303161":"## Part.01 Data Load & EDA","4ea76318":"## Enclosure.01 Cross_val","83afd30b":"## Part.02 Data Preprocessing","83d789c0":"## Enclosure.03 FeatureImportances","be3d4910":"## Part.04 Data Modeling & Evaluation","8aef764e":"## Enclosure.02 GridSearchCV","cf5b6ac7":"## Part.03 Data Holdout & Data Normalization"}}