{"cell_type":{"5921a921":"code","7e87fcb5":"code","686f621d":"code","a6809bc0":"code","d21557d7":"code","a19dd5a1":"code","f12fbb09":"code","4e2355af":"code","412618b5":"code","0f876aa2":"code","31aa4486":"code","12221e71":"markdown","b3840725":"markdown"},"source":{"5921a921":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e87fcb5":"text = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\nlabels = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')","686f621d":"new_labels = np.concatenate((np.zeros(len(labels['less_toxic'])), \n                             np.ones(len(labels['more_toxic']))))\nnew_comments = np.concatenate((labels['less_toxic'].values, \n                               labels['more_toxic'].values))\n\ndataset = np.stack((new_comments, new_labels), axis=1)\n\ndataset","a6809bc0":"from torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\n\ndef yield_tokens(data_iter):\n    for text, _ in data_iter:\n        yield tokenizer(text)\n        \nvocab = build_vocab_from_iterator(yield_tokens(dataset))\nvocab.set_default_index(1)\nprint(vocab.get_default_index())","d21557d7":"text_pipeline = lambda x: vocab(tokenizer(x))\nlabel_pipeline = lambda x: float(x)\n\ntext_pipeline('Hello world')","a19dd5a1":"import torch\nfrom torch.utils.data import DataLoader\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef collate_batch(batch):\n    label_list, text_list, offsets = [], [], [0]\n    for (_text, _label) in batch:\n        label_list.append(_label)\n        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n        text_list.append(processed_text)\n        offsets.append(processed_text.size(0))\n    label_list = torch.tensor(label_list, dtype=torch.float)\n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    text_list = torch.cat(text_list)\n    return label_list.unsqueeze(1).to(device), text_list.to(device), offsets.to(device)\n\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_batch)\n\nnext(iter(dataloader))","f12fbb09":"import time\n\ndef train(dataloader):\n    model.train()\n    total_acc, total_count = 0, 0\n    log_interval = 500\n    start_time = time.time()\n\n    for idx, (label, text, offsets) in enumerate(dataloader):\n        optimizer.zero_grad()\n        predicted_label = model(text, offsets)\n        loss = criterion(predicted_label, label)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n        optimizer.step()\n        total_acc += ((predicted_label > 0.5).float() == label).sum().item()\n        total_count += label.size(0)\n        if idx % log_interval == 0 and idx > 0:\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}\/{:5d} batches '\n                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n                                              total_acc\/total_count))\n            total_acc, total_count = 0, 0\n            start_time = time.time()\n\ndef evaluate(dataloader):\n    model.eval()\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (label, text, offsets) in enumerate(dataloader):\n            predicted_label = model(text, offsets)\n            loss = criterion(predicted_label, label)\n            total_acc += ((predicted_label > 0.5).float() == label).sum().item()\n            total_count += label.size(0)\n    return total_acc\/total_count","4e2355af":"from torch import nn\n\nclass CustomModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim):\n        super(CustomModel, self).__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.fc_1 = nn.Linear(embed_dim, 32)\n        self.relu = nn.ReLU()\n        self.fc_2 = nn.Linear(32, 1)\n        self.init_weights()\n        \n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc_1.weight.data.uniform_(-initrange,initrange)\n        self.fc_1.bias.data.zero_()\n        self.fc_2.weight.data.uniform_(-initrange,initrange)\n        self.fc_2.bias.data.zero_()\n        \n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        embedded = self.fc_1(embedded)\n        embedded = self.relu(embedded)\n        return torch.sigmoid(self.fc_2(embedded))\n    \nvocab_size = len(vocab)\nemsize = 128\nmodel = CustomModel(vocab_size, emsize).to(device)","412618b5":"from torch.utils.data.dataset import random_split\nfrom torchtext.data.functional import to_map_style_dataset\nfrom sklearn.model_selection import train_test_split\n\nEPOCHS = 10 \nLR = 1.\nBATCH_SIZE = 64 \n  \ncriterion = torch.nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=LR)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, .01, gamma=0.1)\ntotal_accu = None\ntrain_iter, test_iter = train_test_split(dataset, test_size=0.1, train_size=0.9, random_state=42)\ntrain_dataset = to_map_style_dataset(train_iter)\ntest_dataset = to_map_style_dataset(test_iter)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_batch)\nvalid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_batch)\n\nfor epoch in range(1, EPOCHS + 1):\n    epoch_start_time = time.time()\n    train(train_dataloader)\n    accu_val = evaluate(valid_dataloader)\n    if total_accu is not None and total_accu > accu_val:\n        scheduler.step()\n    else:\n        total_accu = accu_val\n    \n    print('| end of epoch {:3d} | time: {:5.2f}s | valid accuracy {:8.3f} '\n          .format(epoch, time.time() - epoch_start_time, accu_val))","0f876aa2":"def predict(text, text_pipeline):\n    with torch.no_grad():\n        text = torch.tensor(text_pipeline(text), dtype=torch.int64).to(device)\n        offsets = [0]\n        offsets.append(text.size(0))\n        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0).to(device)\n        \n        output = model(text, offsets)\n        return output\n\ntext.text=text.text.astype(str)\n    \nfor index, i in enumerate(text['text']):\n    item = predict(i, text_pipeline).item()\n    text.at[index, 'score'] = item\n    \ntext.head()","31aa4486":"sub_cv = text.drop('text', axis=1)\nsub_cv.to_csv('submission.csv', index=False)","12221e71":"Just a quick demo to test torchtext lib and my first experience with NLP.","b3840725":"If you liked it or found useful - plz UV ;) \n\nPS: I would like to learn more about NLP, so please share some wisdom or guidance."}}