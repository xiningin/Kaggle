{"cell_type":{"d84db5ba":"code","33f3aec1":"code","ea71dd60":"code","e90d2fc8":"code","e94ffbea":"code","805fbaa4":"code","2fc9301e":"code","fd19a27e":"code","6fc72c90":"code","bf65250b":"code","011c7127":"code","8e203b8f":"markdown","6a0318ca":"markdown","16c63ac2":"markdown","064e29eb":"markdown","4fc493d1":"markdown","d6a1fe21":"markdown"},"source":{"d84db5ba":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\nsns.set(style='white', context='notebook', palette='deep')\n\n# import Keras and its layers\nfrom keras import Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Lambda, Flatten, Conv2D, Convolution2D, MaxPool2D, MaxPooling2D, Activation, BatchNormalization, Input\nfrom keras.optimizers import Adam ,RMSprop\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score\nimport itertools\n# import time for checkpoint saving\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load Data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nprint(train.shape)\n\n# Save the 'Label' column\nY_train = train['label']\nX_train = train.drop(labels = [\"label\"],axis = 1)\n\n#Y_train.value_counts()\n\n# To get more control over visualization we'll define our figure instead of simply using plt.\nfig = plt.figure(figsize=(8, 8))  # create figure\nax = fig.add_subplot(111)  # add subplot\nsns.countplot(Y_train);\nax.set_title(f'Labelled Digits Count ({Y_train.size} total)', size=20);\nax.set_xlabel('Digits');\n# writes the counts on each bar\nfor patch in ax.patches:\n        ax.annotate('{:}'.format(patch.get_height()), (patch.get_x()+0.1, patch.get_height()-150), color='w')","33f3aec1":"def check_missing_data(df):\n    flag=df.isna().sum().any()\n    if flag==True:\n        total = df.isnull().sum()\n        percent = (df.isnull().sum())\/(df.isnull().count()*100)\n        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        data_type = []\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n            data_type.append(dtype)\n        output['Types'] = data_type\n        return(np.transpose(output))\n    else:\n        return(False)\n\ncheck_missing_data(train)\ncheck_missing_data(test)","ea71dd60":"# perform a grayscale normalization to reduce the effect of illumination's differences\nX_train = X_train \/ 255.0\ntest = test \/ 255.0\n\nmean_px = X_train.mean().astype(np.float32)\nstd_px = X_train.std().astype(np.float32)\ndef standardize(x): \n    return (x-mean_px)\/std_px\n\n# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\n\nfor i in range(6, 9):\n    plt.subplot(330 + (i+1))\n    plt.imshow(X_train[i,:, :, 0], cmap=plt.get_cmap('gray'))\n    plt.title(f'Label: {Y_train[i]}');\n\n# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_train = to_categorical(Y_train, num_classes = 10)\n\n# Split the train and the validation set for the fitting\ntrain_x, test_x, train_y, test_y = train_test_split(X_train, Y_train, test_size = 0.1)","e90d2fc8":"# Data Augmenting\nvalidation_split = 0.15\nimg_gen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False,  # randomly flip images\n        validation_split=validation_split)\n\nbatch_size = 86\n# used to feed the model augmented training data\ntrain_gen = img_gen.flow(x=train_x, y=train_y, subset='training', batch_size=batch_size)\n# The valid_gen used for validation will also use the training dataset from the split, as we said before that the model must never learn from the test data.\n# used to feed the model augmented validation data\nvalid_gen = img_gen.flow(x=train_x, y=train_y, subset='validation', batch_size=batch_size)\n\n# used only to visualize the augmentations\nvisualizaion_flow = img_gen.flow(train_x, train_y, batch_size=1, shuffle=False)\n# Now, let's visualize how the generator augments the original images\nfig, axs = plt.subplots(2, 4, figsize=(20,10))  # let's see 4 augmentation examples\nfig.suptitle('Augmentation Results', size=32)\n\nfor axs_col in range(axs.shape[1]):\n    idx = np.random.randint(0, train_x.shape[0])  # get a random index\n    img = train_x[idx,:, :, 0]  # the original image\n    aug_img, aug_label = visualizaion_flow[idx]  # the same image after augmentation\n    \n    axs[0, axs_col].imshow(img, cmap='gray');\n    axs[0, axs_col].set_title(f'example #{axs_col} - Label: {np.argmax(train_y[idx])}', size=20)\n    \n    axs[1, axs_col].imshow(aug_img[0, :, :, 0], cmap='gray');\n    axs[1, axs_col].set_title(f'Augmented example #{axs_col}', size=20)","e94ffbea":"# fix random seed for reproducibility\nseed = 43\nnp.random.seed(seed)\n\nlearning_rate=0.00065\n# Define the optimizer\n#optimizer = RMSprop(lr=learning_rate, decay=learning_rate\/2)\noptimizer = Adam(lr=learning_rate)\n\ndef conv_block(x, filters, kernel_size, strides, layer_no, use_pool=False, padding='same'):\n    x = Convolution2D(filters=filters, kernel_size=kernel_size, strides=strides, name=f'conv{layer_no}',\n               padding=padding)(x)\n    \n    x = BatchNormalization(name=f'bn{layer_no}')(x)\n    x = Activation('relu', name=f'activation{layer_no}')(x)\n    if use_pool:\n        x = MaxPooling2D(pool_size=[2, 2], strides=[2, 2], name=f'pool{layer_no}', padding='same')(x)\n    return x\n\ndef build_model(X):\n    h, w, c = X.shape[1:]  # get shape of input (height, width, channels)\n    X = Input(shape=(h, w, c))\n    conv1 = conv_block(X, filters=8, kernel_size=[3, 3], strides=[1, 1], layer_no=1)\n    conv2 = conv_block(conv1, filters=16, kernel_size=[2, 2], strides=[1, 1], layer_no=2)\n    conv3 = conv_block(conv2, filters=32, kernel_size=[2, 2], strides=[1, 1], layer_no=3, use_pool=True)\n    \n    conv4 = conv_block(conv3, filters=64, kernel_size=[3, 3], strides=[2, 2], layer_no=4)\n    conv5 = conv_block(conv4, filters=128, kernel_size=[2, 2], strides=[1, 1], layer_no=5)\n    conv6 = conv_block(conv5, filters=256, kernel_size=[2, 2], strides=[1, 1], layer_no=6, use_pool=True)\n    \n    flat1 = Flatten(name='flatten1')(conv6)\n    drop1 = Dropout(0.35, name='Dopout1')(flat1)\n    \n    dens1 = Dense(128, name='dense1')(drop1)\n    bn7 = BatchNormalization(name='bn7')(dens1)\n    drop2 = Dropout(0.35, name='Dopout2')(bn7)\n    relu1 = Activation('relu', name='activation7')(drop2)\n    \n    dens1 = Dense(256, name='dense01')(relu1)\n    bn7 = BatchNormalization(name='bn07')(dens1)\n    drop2 = Dropout(0.5, name='Dopout02')(bn7)\n    relu1 = Activation('relu', name='activation07')(drop2)\n    \n    dens2 = Dense(10, name='dense2')(relu1)\n    bn8 = BatchNormalization(name='bn8')(dens2)\n    output_layer = Activation('softmax', name='softmax')(bn8)\n    \n    model = Model(inputs=X, outputs=output_layer)\n    # The model needs to be compiled before training can start.\n    model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\ndef build_seq_model(X):\n    h, w, c = X.shape[1:]  # get shape of input (height, width, channels)\n    model = Sequential([\n        Convolution2D(32,(3,3), activation='relu', input_shape = (h, w, c)),\n        BatchNormalization(axis=1),\n        Convolution2D(32,(3,3), activation='relu'),\n        MaxPooling2D(),\n        BatchNormalization(axis=1),\n        Convolution2D(64,(3,3), activation='relu'),\n        BatchNormalization(axis=1),\n        Convolution2D(64,(3,3), activation='relu'),\n        MaxPooling2D(),\n        Dropout(0.25),\n        Flatten(),\n        Dense(512, activation='relu'),\n        Dropout(0.25),\n        Dense(1024, activation='relu'),\n        Dropout(0.5),\n        Dense(10, activation='softmax')\n        ])\n    # The model needs to be compiled before training can start\n    model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n    \n# create the model\nmodel = build_model(train_x)\n\n# Set a learning rate annealer\nlearning_rate_annealer = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n# learning_rate_annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\nepochs = 10 # Turn epochs to achieve higher accuracy\nhist = model.fit_generator(generator=train_gen, steps_per_epoch=train_x.shape[0]*(1-validation_split)\/\/batch_size, epochs=epochs, \n                    validation_data=valid_gen, validation_steps=train_x.shape[0]*validation_split\/\/batch_size, callbacks=[learning_rate_annealer])\n\n# save model\n#keras.models.save_model(model, f'model_{time.time()}.h5')\n\n# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(hist.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(hist.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(hist.history['acc'], color='b', label=\"Training accuracy\")\nax[1].plot(hist.history['val_acc'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","805fbaa4":"def calculate_performance(labels, pred, dataset):\n    pred_cat = np.argmax(pred, axis=1)  # categorical predictions 0-9\n    labels_cat = np.argmax(labels, axis=1)  # categorical labels 0-9\n    \n    # a boolean vector of element-wise comparison between prediction and label\n    corrects = (pred_cat == labels_cat)\n    \n    # get the falses data\n    falses = dataset[~corrects]  # the falses images\n    falses_labels = labels_cat[~corrects]  # true labels of the falsely classified images - categorical\n    falses_preds = pred[~corrects]  # the false predictions of the images - 10-dim prediction\n     \n    examples_num = labels.shape[0]  # total numbers of examples\n    accuracy = np.count_nonzero(corrects) \/ examples_num\n\n    return accuracy, [falses, falses_labels, falses_preds], [labels_cat, pred_cat]\n\ntest_y_pred = model.predict(test_x)\ntest_accuracy, (test_falses, test_falses_labels, test_falses_preds), (true_labels, pred_labels) = calculate_performance(test_y, test_y_pred, test_x)\n\n# calculate the F1 Score of the model on the test dataset\n# micro averaging calculates it over false and correct predictions regardless of the class\ntest_f1 = f1_score(y_pred=pred_labels, y_true=true_labels, average='micro')\n\nprint(f'Test Dataset Accuracy: {np.round(test_accuracy*100, 3)}%')\nprint(f'F1 Score = {test_f1}')\n\nplt.figure(figsize=(10, 10))\n\n# Calculate the confusion matrix and visualize it\ntest_matrix = confusion_matrix(y_pred=pred_labels, y_true=true_labels)\nsns.heatmap(data=test_matrix, annot=True, cmap='Blues', fmt=f'.0f')\n\nplt.title('Confusion Matrix - Test Dataset', size=24)\nplt.xlabel('Predictions', size=20);\nplt.ylabel('Labels', size=20);\n\nfinal_loss, final_acc = model.evaluate(test_x, test_y, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))","2fc9301e":"# Let's investigate for errors.\ntest_falses_preds_classes = np.argmax(test_falses_preds, axis=1)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\nfig.suptitle('False Classifications Plots', size=32)\n\nsns.countplot(test_falses_labels, ax=ax1);\nax1.set_title(f'Falses ({test_falses_labels.size} total)',size=20);\nax1.set_xlabel('Digits', size=15);\nax1.set_ylabel('Count', size=15);\n\nfor patch in ax1.patches:\n    bar_height = patch.get_height()\n    ax1.annotate(f'{bar_height}', (patch.get_x()+0.25, bar_height-0.2), color='w', size=15);\n\nfalses_matrix = confusion_matrix(y_pred=test_falses_preds_classes, y_true=test_falses_labels)\nsns.heatmap(data=falses_matrix, annot=True, cmap='Blues', fmt=f'.0f')\n\ndef display_errors(examples, preds_probs, preds_classes, labels):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    fig, axs = plt.subplots(nrows, ncols, figsize=(30, 12))\n    fig.suptitle('Misclassified Images', size=32)\n    for ax, pred, pred_prob, label, example in zip(axs.ravel(), preds_classes, preds_probs, labels, examples):\n        ax.imshow(example[:, :, 0] ,cmap='gray');\n        ax.set_title(f'Label: {label}\\nPrediction: {pred} with {np.round(np.max(pred_prob)*100, 3)}% Confidence.',\n                size = 15)\n        ax.axis('off')\n\nnrows = 2\nncols = 3\nplt.xlabel('Predictions', size=20);\nplt.ylabel('Labels', size=20);\n# Random display of errors between predicted labels and true labels\nrandom_errors = np.random.choice(range(test_falses_labels.size), size=np.min((nrows * ncols, test_falses_labels.size)), replace=False)\n# Top 6 errors \nY_pred_errors_prob = np.max(test_falses_preds,axis = 1)\ntrue_prob_errors = np.diagonal(np.take(test_falses_preds, test_falses_labels, axis=1))\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\nmost_important_errors = sorted_dela_errors[-6:]\n\nexamples = test_falses[most_important_errors]\npreds_probs = test_falses_preds[most_important_errors]\npreds_classes = np.argmax(preds_probs, axis=1)\nlabels = test_falses_labels[most_important_errors]\ndisplay_errors(examples, preds_probs, preds_classes, labels)","fd19a27e":"# Example Experiment 2: feature maps\nstyles=[':','-.','--','-',':','-.','--','-',':','-.','--','-']\n\nnets = 3\nmodel = [0] *nets\nfor j in range(3):\n    model[j] = Sequential()\n    model[j].add(Conv2D(j*16+16,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n    model[j].add(MaxPool2D())\n    model[j].add(Conv2D(j*32+32,kernel_size=5,activation='relu'))\n    model[j].add(MaxPool2D())\n    model[j].add(Flatten())\n    model[j].add(Dense(256, activation='relu'))\n    model[j].add(Dense(10, activation='softmax'))\n    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# CREATE VALIDATION SET\nX_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.333)\n# TRAIN NETWORKS\nhistory = [0] * nets\nnames = [\"16 maps\",\"32 maps\",\"64 maps\"]\nepochs = 10\nfor j in range(nets):\n    history[j] = model[j].fit(X_train2,Y_train2, batch_size=80, epochs = epochs, \n        validation_data = (X_val2,Y_val2), verbose=0)\n    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))\n\n    # PLOT ACCURACIES\nplt.figure(figsize=(15,5))\nfor i in range(nets):\n    plt.plot(history[i].history['val_acc'],linestyle=styles[i])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(names, loc='upper left')\naxes = plt.gca()\naxes.set_ylim([0.98,1])\nplt.show()","6fc72c90":"model = Sequential()\n\nmodel.add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,kernel_size=3,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(64,kernel_size=3,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,kernel_size=3,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# TRAIN OUR BEST NET MORE\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x+epochs))\nhist = model.fit_generator(img_gen.flow(X_train,Y_train, batch_size=64), epochs = 100, \n    steps_per_epoch = X_train.shape[0]\/\/64, callbacks=[annealer], verbose=0)\nprint(\"Train accuracy={0:.5f}, Train loss={1:.5f}\".format(max(hist.history['acc']),max(hist.history['loss'])))","bf65250b":"# predictions = model.predict_classes(test, verbose=0)\npredictions = model.predict(test)\npredictions = np.argmax(predictions,axis = 1)\nsubmissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"submission.csv\", index=False, header=True)\nprint(submissions.head(5))","011c7127":"# from keras import models\n\n# # Visualize the model\n# model.summary()\n# # Visualize the metrix of first convolutional layer\n# layer1 = model.layers[0]\n# layer1.name\n# conv2d_1w = layer1.get_weights()[0][:,:,0,:]\n# for i in range(1,17):\n#       plt.subplot(4,4,i)\n#       plt.imshow(conv2d_1w[:,:,i-1],interpolation=\"nearest\",cmap=\"gray\")\n# plt.show()\n\n# # Let's see the activation of the first layer\n# test_im = X_train[2]\n# plt.imshow(test_im.reshape(28,28), cmap='viridis', interpolation='none')\n\n# layer_outputs = [layer.output for layer in model.layers[:13]]\n# activation_model = models.Model(input=model.input, output=layer_outputs)\n# activations = activation_model.predict(test_im.reshape(1,28,28,1))\n\n# first_layer_activation = activations[0]\n# plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')\n\n# # Let's plot the activations of the other layers as well.\n# layer_names = []\n# for layer in model.layers[:-1]:\n#     layer_names.append(layer.name) \n# images_per_row = 16\n# for layer_name, layer_activation in zip(layer_names, activations):\n#     if layer_name.startswith('conv'):\n#         n_features = layer_activation.shape[-1]\n#         size = layer_activation.shape[1]\n#         n_cols = n_features \/\/ images_per_row\n#         display_grid = np.zeros((size * n_cols, images_per_row * size))\n#         for col in range(n_cols):\n#             for row in range(images_per_row):\n#                 channel_image = layer_activation[0,:, :, col * images_per_row + row]\n#                 channel_image -= channel_image.mean()\n#                 channel_image \/= channel_image.std()\n#                 channel_image *= 64\n#                 channel_image += 128\n#                 channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n#                 display_grid[col * size : (col + 1) * size,\n#                              row * size : (row + 1) * size] = channel_image\n#         scale = 1. \/ size\n#         plt.figure(figsize=(scale * display_grid.shape[1],\n#                             scale * display_grid.shape[0]))\n#         plt.title(layer_name)\n#         plt.grid(False)\n#         plt.imshow(display_grid, aspect='auto', cmap='viridis')\n              \n# # visualize full conected layer\n# fc_layer = model.layers[-5]\n# activation_model = models.Model(input=model.input, output=fc_layer.output)\n# activations = activation_model.predict(test_im.reshape(1,28,28,1))   \n# activation = activations[0].reshape(32,32)\n# plt.imshow(activation, aspect='auto', cmap='viridis')\n\n# # organize the training images by label\n# #Y_train_value_df = pd.DataFrame(train[\"label\"],columns=['label'])\n# #Y_train_value_df['pos']=Y_train_value_df.index\n# #Y_train_label_pos = Y_train_value_df.groupby('label')['pos'].apply(list)\n# #pos = Y_train_label_pos[1][0]\n\n# # display 3 rows of digit image [0,9], with last full connected layer at bottom\n# plt.figure(figsize=(16,8))\n# x, y = 10, 3\n# for i in range(y):  \n#     for j in range(x):\n#         # digit image\n#         plt.subplot(y*2, x, i*2*x+j+1)\n#         #pos = Y_train_label_pos[j][i] # j is label, i in the index of the list\n#         pos = i * j\n#         plt.imshow(X_train[pos].reshape((28,28)),interpolation='nearest')\n#         plt.axis('off')\n#         plt.subplot(y*2, x, (i*2+1)*x+j+1)\n#         activations = activation_model.predict(X_train[pos].reshape(1,28,28,1))   \n#         activation = activations[0].reshape(32,32)\n#         plt.imshow(activation, aspect='auto', cmap='viridis')\n#         plt.axis('off')\n# plt.subplots_adjust(wspace=0.1, hspace=0.1)\n# plt.show()\n","8e203b8f":"## Data Augmenting\nIn order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. We can make your existing dataset even larger. There are many ways to augment the images like centering the images, normalization, rotation, shifting, and flipping. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n\nThe model augments data randomly each time the training requires a new batch of images, so the model isn't training on the same examples exactly at each epoch which is the case if we have simply fitted the model using the original images.\n\nThe augmentation generator will only use the train portion we got from the split. The model must never learn from the test images or even their augmentations, to have a meaningful performance and error analysis.","6a0318ca":"# Modeling and Evaluation\n## Designing Neural Network Architecture\nA typical Convolution Block would consists of\n1. 2D Convolution - extracts features\n2. BatchNormalization - enhances training efficiency and reduces Vanishing\/Exploding Gradients effect\n3. ReLU Activation Function\n4. 2D Max Pool - optional, reduces overfitting and reduces the shape of the tensor\n\nThe first is the convolutional (Conv2D) layer. It is like a set of learnable filters. I choosed to set 32 filters for the two firsts conv2D layers and 64 filters for the two last ones. Each filter transforms a part of the image (defined by the kernel size) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters can be seen as a transformation of the image.\n\nThe CNN can isolate features that are useful everywhere from these transformed images (feature maps).\n\nThe second important layer in CNN is the pooling (MaxPool2D) layer. This layer simply acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size (i.e the area size pooled each time) more the pooling dimension is high, more the downsampling is important.\n\nCombining convolutional and pooling layers, CNN are able to combine local features and learn more global features of the image.\n\nDropout is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their wieghts to zero) for each training sample. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting.\n\n'relu' is the rectifier (activation function max(0,x). The rectifier activation function is used to add non linearity to the network.\n\nThe Flatten layer is use to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional\/maxpool layers. It combines all the found local features of the previous convolutional layers.\n\nIn the end used the features in two fully-connected (Dense) layers which is just artificial an neural networks (ANN) classifier. In the last layer(Dense(10,activation=\"softmax\")) the net outputs distribution of probability of each class.\n\n**Cost function**\n\nIt is a measure of the overall loss in our network after assigning values to the parameters during the forward phase so it indicates how well the parameters were chosen during the forward probagation phase. We are using categorical_crossentropy as the cost function.\n\n**Optimizer**\n\nIt is the gradiant descent algorithm that is used. We use it to minimize the cost function to approach the minimum point. We are using adam optimizer which is one of the best gradient descent algorithms. You can refere to this paper to know how it works https:\/\/arxiv.org\/abs\/1412.6980v8\n\nYou can use other metrics to measure the performance other than accuracy as precision or recall or F1 score. the choice depends on the problem itself. Where high recall means low number of false negatives , High precision means low number of false positives and F1 score is a trade off between them. You can refere to this article for more about precision and recall http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_precision_recall.html\n \n### Best Practice\n* Getting convergence should not be a problem, unless you use an extremely large learning rate. It's easy, however, to create a net that overfits, with perfect results on the training set and very poor results on the validation data. If this happens, you could try increasing the Dropout parameters, increase augmentation, or perhaps stop training earlier. If you instead wants to increase accuracy, try adding on two more layers, or increase the number of filters.\n* In order to make the optimizer converge faster and closest to the global minimum of the loss function, its better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function.\n*  Advanced techniques include data augmentation, nonlinear convolution layers, learnable pooling layers, ReLU activation, ensembling, bagging, decaying learning rates, dropout, batch normalization, and adam optimization.","16c63ac2":"# Introduction #\nThis is my first computer vision kernel writtern for the [Digit Recognizer Competetion](https:\/\/www.kaggle.com\/c\/digit-recognizer). I am trying to understand CNN Deep Learning models. \n\nThe layers of convolutional neural network consists of two main components :\n1. Convolutional layers : the convolutional layer is responsible for the convolutional operation in which feature maps identifies features in the images.\nand is usually followed by two types of layers which are :\n>*   **Dropout** : Dropout is a regulization technique where you turn off part of the network's layers randomally to increase regulization and hense decrease overfitting. We use when the training set accuracy is muuch higher than the test set accuracy.\n>*   **Max Pooling** : The maximum output in a rectangular neighbourhood. It is used to make the network more flexible to slight changes and decrease the network computationl expenses by extracting the group of pixels that are highly contributing to each feature in the feature maps in the layer.\n2. Dense layers : The dense layer is a fully connected layer that comes after the convolutional layers and they give us the output vector of the Network.\n\nAs a convention in Convolutional Neural Network we decrease the dimensions of the layers as we go deeper and increase the number of feature maps to make it detect more features and decrease the number of computational cost.\n\n![alt text](https:\/\/raw.githubusercontent.com\/MoghazyCoder\/Machine-Learning-Tutorials\/master\/assets\/Untitled.png)\n \nHope it might be useful for someone else here as well. If you Like the notebook and think that it helped you, <font color=\"red\"><b> please upvote<\/b><\/font>.\n\n\n---\n## Table of Content\n1. Data Preprocessing\n    * Data Preparation\n    * Data Augmentation\n2. Modeling and Evaluation\n    * Designing Neural Network Architecture\n    * Hyperparameters Tuning and Model Evaluation\n3. Final Prediction & Submission","064e29eb":"## Hyperparameters Tuning & Model Evaluation\n### Hyperparameters\n* One of the hyperparameters to tune is the learning rate and the decay. \n* As you can see there are quite a few other parameters that could also be tweaked (number of layers, number of filters, Dropout parameters, augmentation settings). This is often done with trial and error, and there is no easy shortcut.\n* Confusion matrix can be very helpfull to see your model drawbacks.\n\n### Model Evaluation\nThere are so many choices for CNN architecture. How do we choose the best one? First we must define what **best** means. The best may be the simplest, or it may be the most efficient at producing accuracy while minimizing computational complexity. In this kernel, we will run experiments to find the most accurate and efficient CNN architecture for classifying MNIST handwritten digits.\n\n#### 1. How many convolution-subsambling pairs?\nFirst question, how many pairs of convolution-subsampling should we use? For example, our network could have 1, 2, or 3:\n * 784 - **[24C5-P2]** - 256 - 10\n * 784 - **[24C5-P2] - [48C5-P2]** - 256 - 10\n * 784 - **[24C5-P2] - [48C5-P2] - [64C5-P2]** - 256 - 10  \n   \nIt's typical to increase the number of feature maps for each subsequent pair as shown here. Let's see whether one, two, or three pairs is best. We are not doing four pairs since the image will be reduced too small before then. The input image is 28x28. After one pair, it's 14x14. After two, it's 7x7. After three it's 4x4 (or 3x3 if we don't use padding='same'). It doesn't make sense to do a fourth convolution.\n#### 2. How many feature maps?\nIn the previous experiement, we decided that two pairs is sufficient. How many feature maps should we include? For example, we could do\n * 784 - [**8**C5-P2] - [**16**C5-P2] - 256 - 10\n * 784 - [**16**C5-P2] - [**32**C5-P2] - 256 - 10\n * 784 - [**24**C5-P2] - [**48**C5-P2] - 256 - 10\n * 784 - [**32**C5-P2] - [**64**C5-P2] - 256 - 10\n * 784 - [**48**C5-P2] - [**96**C5-P2] - 256 - 10  \n * 784 - [**64**C5-P2] - [**128**C5-P2] - 256 - 10  \n\n#### 3. How large a dense layer?\nHow many dense units should we use? For example we could use\n * 784 - [32C5-P2] - [64C5-P2] - **0** - 10\n * 784 - [32C5-P2] - [64C5-P2] - **32** - 10\n * 784 - [32C5-P2] - [64C5-P2] - **64** - 10\n * 784 - [32C5-P2] - [64C5-P2] - **128** -10\n * 784 - [32C5-P2] - [64C5-P2] - **256** - 10\n * 784 - [32C5-P2] - [64C5-P2] - **512** -10\n * 784 - [32C5-P2] - [64C5-P2] - **1024** - 10\n * 784 - [32C5-P2] - [64C5-P2] - **2048** - 10\n\n#### 4. How much dropout?\nDropout will prevent our network from overfitting thus helping our network generalize better. How much dropout should we add after each layer?\n * 0%, 10%, 20%, 30%, 40%, 50%, 60%, or 70%\n\n#### 5. Advanced features\nInstead of using one convolution layer of size 5x5, you can mimic 5x5 by using two consecutive 3x3 layers and it will be more nonlinear. Instead of using a max pooling layer, you can subsample by using a convolution layer with strides=2 and it will be learnable. Lastly, does batch normalization help? And does data augmentation help? Let's test all four of these\n * replace '32C5' with '32C3-32C3'  \n * replace 'P2' with '32C5S2'\n * add batch normalization\n * add data augmentation\n \n**Notations:**\n * **24C5** means a convolution layer with 24 feature maps using a 5x5 filter and stride 1\n * **24C5S2** means a convolution layer with 24 feature maps using a 5x5 filter and stride 2 \n * **P2** means max pooling using 2x2 filter and stride 2\n * **256** means fully connected dense layer with 256 units ","4fc493d1":"# Final Prediction & Submission\nUse full train dataset here to train model and predict on test set. Then submitting predictions to Kaggle.\n\nYou can increase number of epochs on your GPU enabled machine to get better results.","d6a1fe21":"# Data Preprocessing\nThis section will talk about imputation, normalization, reshaping and transform\/encoding of the data. Statistical summaries and visulization plots will be used to help recognizing underlying pattens to exploit in the model.\n### Best Practice\n* If the data is balanced which means all the classes have fair contribution in the dataset regarding its numbers then we can easily use accuracy. But if the data is skewed then we won't be able to use accurace as it's results will be misleading and we may use F-beta score instead."}}