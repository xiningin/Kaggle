{"cell_type":{"60c5e910":"code","422d1bab":"code","f0f29661":"code","8e767ece":"code","3b611eb8":"code","bcebf074":"code","9554f4a6":"code","2c9b04b8":"code","76cf2fcc":"code","c51f105a":"code","3f9be860":"code","b0f26f63":"code","a6c804b4":"code","14c008d9":"code","018ec0d8":"code","7c69bd4f":"code","f2955e20":"code","d20464ea":"code","e96c695a":"code","386c81f0":"code","19311f0c":"code","fdfb135e":"code","452586e2":"code","eff0d89d":"code","c1031be4":"code","574af08a":"code","b197bf5d":"code","301d9b8f":"code","8fefa0c0":"code","4f70869b":"code","8c76d01b":"code","c28b572f":"code","33292f4b":"code","a40a6883":"code","50ea88bd":"code","7c2d7ad2":"code","ed312b25":"code","320b1f56":"code","bde667d4":"code","837fb6d0":"code","0bb6fa7d":"code","04a13dc9":"code","162a6fe5":"code","486f5b60":"code","4b8fb718":"code","7e43d8b2":"code","0f6279cd":"code","bdce1770":"code","edfe1bb2":"code","5077fcbb":"code","ac4614fe":"code","8716f2eb":"markdown","2bad0783":"markdown","e7246719":"markdown","c70698bc":"markdown","3ac10e6b":"markdown","ce124d64":"markdown","21cce78e":"markdown","4af9e16b":"markdown","4b2401a0":"markdown","46a4531c":"markdown","a5d66e65":"markdown","6b888831":"markdown","68febcb9":"markdown","835fe71c":"markdown","ed364456":"markdown","89c3d725":"markdown","6a99bb67":"markdown","ff28df5d":"markdown","044b6b40":"markdown","f624b458":"markdown","0d3eca7a":"markdown","b9f62bd5":"markdown","57634217":"markdown","2d271e2a":"markdown","5b25a575":"markdown","ec9d4e15":"markdown","53e34681":"markdown","a5d676a8":"markdown","d7f3bc1a":"markdown"},"source":{"60c5e910":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nimport torch\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils import data\nfrom torch import nn, optim\nfrom collections import defaultdict\nimport warnings\n%matplotlib inline","422d1bab":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","f0f29661":"pd.set_option(\"display.max_colwidth\", None)","8e767ece":"warnings.filterwarnings(\"ignore\")","3b611eb8":"filename = \"..\/input\/commonlitreadabilityprize\/train.csv\"\ndf_train = pd.read_csv(filename)\ndf_train = df_train.drop([\"url_legal\", \"license\"], axis = 1)","bcebf074":"df_train.head(1)","9554f4a6":"df_train.info()","2c9b04b8":"df_train['target'].describe()","76cf2fcc":"std = df_train['target'].std()\nmean = df_train['target'].mean()\nprint('mean:', mean)\nprint('std: ', std)","c51f105a":"filename = \"..\/input\/commonlitreadabilityprize\/test.csv\"\ndf_test = pd.read_csv(filename)\ndf_test = df_test.drop([\"url_legal\", \"license\"], axis = 1)","3f9be860":"df_test.head(1)","b0f26f63":"def to_string(row_text):\n  lines = row_text.split('\\n')\n  string = \"\"\n  for line in lines:\n    string = string + \" \" + line\n  return string","a6c804b4":"df_train['excerpt'] = df_train['excerpt'].apply(to_string)","14c008d9":"df_train.head(1)","018ec0d8":"df_test['excerpt'] = df_test['excerpt'].apply(to_string)","7c69bd4f":"df_test.head(1)","f2955e20":"sns.set_style(\"darkgrid\")\nrcParams['figure.figsize'] = 9, 6","d20464ea":"sns.kdeplot(df_train.target, shade=True, color=\"r\")\nplt.xlabel('Average ratings')\nplt.show()","e96c695a":"sns.kdeplot(df_train.standard_error, shade=True, color=\"r\")\nplt.xlabel('Standard errors')\nplt.show()","386c81f0":"x=df_train['target']\ny=df_train['standard_error']\nplt.scatter(x=x, y=y)\nplt.annotate(\"remove\", xy=(0, 0), arrowprops=dict(facecolor='orange', shrink=0.05), \n             xytext=(0.6, 0.3), textcoords='axes fraction', fontsize=12, weight='bold',\n             horizontalalignment='right', verticalalignment='top', color='orange')\nplt.xlabel('Targets')\nplt.ylabel('Standard errors')\nplt.show()","19311f0c":"ind = df_train[df_train['target'] == 0].index\ndf_train = df_train.drop(ind)","fdfb135e":"lower_bound = mean - std\nupper_bound = mean + std\nlower_bound, upper_bound","452586e2":"plt.scatter(x=df_train['target'], y=df_train['standard_error'])\n\nplt.axvline(x=lower_bound, ymin=0, ymax=1, linewidth=1.5, linestyle=\"--\", color='darkorchid')\nplt.axvline(x=upper_bound, ymin=0, ymax=1, linewidth=1.5, linestyle=\"--\", color='darkorchid')\n\nplt.xlabel('Targets')\nplt.ylabel('Standard errors')\nplt.show()","eff0d89d":"min_value = df_train[\"target\"].min()\nmax_value = df_train[\"target\"].max()\nprint(\"min: \",  min_value)\nprint(\"max: \",  max_value)","c1031be4":"PRE_TRAINED_MODEL = \"bert-base-uncased\"","574af08a":"tokenizer = transformers.BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)","b197bf5d":"%%time\n\nfor df in [(\"training data\", df_train), (\"test data\", df_test)]:\n  excerpt_tokens = []\n  for excerpt in df[1].excerpt:\n    tokens = tokenizer.tokenize(excerpt)\n    excerpt_tokens.append(len(tokens))\n\n  min_tokens = min(excerpt_tokens)\n  max_tokens = max(excerpt_tokens)\n  print(df[0],\":\")\n  print(\"-\" * 100)\n  print('min ve max tokens:', min_tokens, max_tokens)\n  print('\\n')\n\n  sns.distplot(excerpt_tokens)\n  plt.xlim([min_tokens-50, max_tokens+50]);\n  plt.xlabel('Token count');\n  plt.show()\n\n  print('\\n')\n","301d9b8f":"BS = 4\nMAX_LEN = 320\nEPOCHS = 5\nRANDOM_SEED = 42\nBIAS = False\nSPLIT_RATIO = 0.1\nDROPOUT = 0.3\n\nWD = 0\nLEARNING_RATE = 2e-5\nNUM_WARMUP_PERCENTAGE = 0.1","8fefa0c0":"def define_scheduler(data_loader):\n\n  total_steps = len(data_loader) * EPOCHS\n\n  scheduler = get_linear_schedule_with_warmup(\n      optimizer,\n      num_warmup_steps=int(NUM_WARMUP_PERCENTAGE*total_steps),\n      num_training_steps=total_steps\n  )\n\n  return scheduler","4f70869b":"def split_train_val(full_data, SPLIT_RATIO):\n  df_train, df_val = train_test_split(\n      full_data,\n      test_size=SPLIT_RATIO,\n      random_state=RANDOM_SEED\n      )\n\n  print(\"training data:\", df_train.shape)\n  print(\"validation data:\", df_val.shape)\n\n  return df_train, df_val","8c76d01b":"class ExcerptDataset(data.Dataset):\n  def __init__(self, ids, excerpts, targets, tokenizer, max_len):\n    self.ids = ids\n    self.excerpts = excerpts\n    self.targets = targets\n    self.tokenizer = tokenizer\n    self.max_len = max_len\n\n  def __len__(self):\n    return len(self.excerpts)\n\n  def __getitem__(self, item):\n    id = str(self.ids[item])\n    excerpt = str(self.excerpts[item])\n    target = self.targets[item]\n\n    encoding = tokenizer.encode_plus(\n        excerpt,\n        max_length=self.max_len,\n        truncation=True,\n        add_special_tokens=True,\n        pad_to_max_length=True,\n        return_attention_mask=True, \n        return_token_type_ids=False,\n        return_tensors='pt'\n    )\n\n    return {\n      'id': id,\n      'excerpt_text': excerpt,\n      'input_ids': encoding['input_ids'].flatten(),\n      'attention_mask': encoding['attention_mask'].flatten(),\n      'targets': torch.tensor(target, dtype=torch.float)\n    }","c28b572f":"df_train_set, df_val_set = split_train_val(df_train, SPLIT_RATIO)","33292f4b":"def create_data_loader(df, tokenizer, MAX_LEN, batch_size=4, shuffle=True):\n  dataset = ExcerptDataset(\n    ids = df.id.to_numpy(),\n    excerpts=df.excerpt.to_numpy(),\n    targets=df.target.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=MAX_LEN\n  )\n\n  data_loader = data.DataLoader(\n    dataset,\n    batch_size=batch_size,\n    shuffle=shuffle,\n    num_workers=2\n  )\n\n  data_item = next(iter(data_loader))\n\n  print(data_item.keys())\n  print('\\n')\n  print(data_item['input_ids'].shape)\n  print(data_item['attention_mask'].shape)\n  print(data_item['targets'].shape)\n  print('\\n')\n  print(\"input_ids:\", data_item['input_ids'])\n  print(\"attention_mask:\", data_item['attention_mask'])\n  print(\"targets:\", data_item['targets'])\n\n  return data_loader","a40a6883":"train_data_loader = create_data_loader(df_train_set, tokenizer, MAX_LEN, batch_size=BS, shuffle=True)\nval_data_loader = create_data_loader(df_val_set, tokenizer, MAX_LEN, batch_size=BS, shuffle=True)","50ea88bd":"class ExcerptRegression(nn.Module):\n\n  def __init__(self, DROPOUT):\n    super(ExcerptRegression, self).__init__()\n    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL)\n    self.drop = nn.Dropout(p=DROPOUT)\n    self.linear = nn.Linear(self.bert.config.hidden_size, 1)\n\n  def forward(self, input_ids, attention_mask):\n    _, output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict = False)\n    output = self.drop(output)\n    output = self.linear(output)\n\n    return output","7c2d7ad2":"reg_model = ExcerptRegression(DROPOUT)\nreg_model = reg_model.to(device)","ed312b25":"class RMSELoss(nn.Module):\n    def __init__(self):\n        super(RMSELoss,self).__init__()\n        self.mse = nn.MSELoss()\n\n    def forward(self,pred,y):\n        loss_fn = torch.sqrt(self.mse(pred, y))\n        return loss_fn","320b1f56":"optimizer = AdamW(reg_model.parameters(), lr=LEARNING_RATE, correct_bias=BIAS, weight_decay=WD)\n\nloss_fn = RMSELoss().to(device)","bde667d4":"def train(reg_model, data_loader, loss_fn, optimizer, device):\n\n  scheduler = define_scheduler(data_loader)\n  reg_model = reg_model.train()\n  losses = []\n\n  for dl in data_loader:\n    input_ids = dl[\"input_ids\"].to(device)\n    attention_mask = dl[\"attention_mask\"].to(device)\n    targets = dl[\"targets\"].to(device)\n    \n    outputs = reg_model(\n      input_ids=input_ids,\n      attention_mask=attention_mask,\n    )\n\n    loss = loss_fn(outputs.view(-1), targets.view(-1)) # pred, y\n    losses.append(loss.item())\n\n    loss.backward()\n    nn.utils.clip_grad_norm_(reg_model.parameters(), max_norm=1.0)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n\n  return np.mean(losses)","837fb6d0":"def evaluate(reg_model, data_loader, loss_fn, device):\n\n  reg_model = reg_model.eval()\n  losses = []\n\n  with torch.no_grad():\n\n    for dl in data_loader:\n      input_ids = dl[\"input_ids\"].to(device)\n      attention_mask = dl[\"attention_mask\"].to(device)\n      targets = dl[\"targets\"].to(device)\n\n      outputs = reg_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n      )\n\n      loss = loss_fn(outputs.view(-1), targets.view(-1)) \n      losses.append(loss.item())\n\n  return np.mean(losses)","0bb6fa7d":"%%time\n\nepoch_results = defaultdict(list)\nmin_loss = 1000.0\n\nprint(\"TRAINING RESULTS:\")\nprint('*' * 50)\nprint('\\n')\n\nfor epoch in range(EPOCHS):\n  print(f'Epoch {epoch + 1}\/{EPOCHS}')\n  print('-' * 50)\n\n  train_loss = train(reg_model, train_data_loader, loss_fn, optimizer, device)\n\n  print(f'Training   loss: {train_loss}')\n  print('\\n')\n \n  val_loss = evaluate(reg_model, val_data_loader, loss_fn, device)\n\n  print(f'Validation loss: {val_loss}')\n  print('\\n')\n\n  epoch_results['train_loss'].append(train_loss)\n  epoch_results['validation_loss'].append(val_loss)\n  \n  if val_loss < min_loss:\n    name = \"1set_best_model_1.pt\"\n    torch.save(reg_model.state_dict(), '.\/' + name)\n    min_loss = val_loss","04a13dc9":"train_loss = epoch_results['train_loss']\nvalidation_loss = epoch_results['validation_loss']\n\nmin_train_loss= min(train_loss)\nmin_val_lost = min(validation_loss)\n\nval_index = validation_loss.index(min_val_lost)\n\nprint(\"model: \", PRE_TRAINED_MODEL)\nprint(\"batch size:\", BS)\nprint(\"maximum sequence length:\", MAX_LEN)\nprint(\"number of epochs:\", EPOCHS)\nprint(\"random seed:\", RANDOM_SEED)\nprint(\"learning rate:\", LEARNING_RATE)\nprint(\"weight decay:\", WD)\nprint(\"warmup percentage:\", NUM_WARMUP_PERCENTAGE)\nprint(\"bias correction:\", BIAS)\nprint(\"dropout:\", DROPOUT)\nprint(\"split ratio:\", SPLIT_RATIO)\nprint('\\n')\nprint(\"minimum train loss:\", min_train_loss)\nprint(\"minimum validation loss:\", min_val_lost)","162a6fe5":"x = range(1, EPOCHS+1)\ny1 = train_loss\ny2 = validation_loss\n\nplt.plot(x, y1, label='train loss')\nplt.plot(x, y2, label='validation loss')\n\nplt.title('Loss functions for training and validation data', fontsize=15)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.legend()\nplt.axvline(x=x[val_index], ymin=0, ymax=1, linewidth=1.5, linestyle=\"--\", color='darkorchid')\nplt.axhline(y=min_val_lost, linewidth=1.5, linestyle=\"--\", color='darkorchid')\n\nplt.show()","486f5b60":"DROPOUT = 0","4b8fb718":"reg_model = ExcerptRegression(DROPOUT)\nreg_model = reg_model.to(device)","7e43d8b2":"df_test['target'] = 100\n\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, batch_size=4, shuffle=False)","0f6279cd":"def find_predictions(reg_model, data_loader):\n\n  id_data = []\n  excerpt_data = []\n  output_data = []\n\n  reg_model = reg_model.eval()\n\n  with torch.no_grad():\n  \n    for dl in data_loader:\n\n      id = dl['id']\n      excerpt = dl['excerpt_text']\n      input_ids = dl[\"input_ids\"].to(device)\n      attention_mask = dl[\"attention_mask\"].to(device)\n\n      outputs = reg_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n\n      outputs = outputs.flatten().tolist()\n      output_data.extend(outputs)\n      excerpt_data.extend(excerpt)\n      id_data.extend(id)\n\n  return id_data, excerpt_data, output_data","bdce1770":"id_data, excerpt_data, output_data = find_predictions(reg_model, test_data_loader)","edfe1bb2":"predictions_df = pd.DataFrame(list(zip(id_data, excerpt_data, output_data)), columns =['id', 'excerpt', 'target'])\npredictions_df","5077fcbb":"predictions_df = predictions_df.drop(\"excerpt\", axis = 1)\npredictions_df","ac4614fe":"predictions_df.to_csv('.\/submission.csv', index=False, float_format='%.6f')","8716f2eb":"### **Train-validation split**","2bad0783":"#### Create data loader of the test data","e7246719":"#### Optimizer","c70698bc":"### **Load data**","3ac10e6b":"#### Load the best model","ce124d64":"#### Load and describe the training data","21cce78e":"### **Set parameters**","4af9e16b":"### **Import libraries**","4b2401a0":"### **Encoding**","46a4531c":"#### Load the test data","a5d66e65":"### **Choose sequence length**","6b888831":"#### Remove new lines from the training data","68febcb9":"#### **Save the results**","835fe71c":"### **Training the data**","ed364456":"### **Predict the test data**","89c3d725":"### **Transform data**","6a99bb67":"#### Remove new lines","ff28df5d":"# **CommonLit Readability**","044b6b40":"### **Run the model**","f624b458":"### **Regression**","0d3eca7a":"### **Plot the results**","b9f62bd5":"Predict","57634217":"#### Regression model","2d271e2a":"### **Exploratory Data Analysis**","5b25a575":"#### Encode train excerpts.","ec9d4e15":"#### Define loss function RMSE","53e34681":"### **Create data loaders**","a5d676a8":"#### Remove new lines from the test data","d7f3bc1a":"### **Evaluation**"}}