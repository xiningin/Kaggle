{"cell_type":{"ba5e6822":"code","ae6cf7eb":"code","a75882d0":"code","792532dd":"code","b433679d":"code","e1b596ca":"code","c0d42a1b":"code","372d3a89":"code","a19c1456":"code","93b43730":"code","f3243c52":"code","4755852c":"code","57bdfb9e":"code","52bf578f":"code","169d1ff7":"code","d26f04d0":"code","b426034c":"code","a55ffa8c":"code","682c8d40":"code","68f138fc":"code","752d5fb6":"code","22d57ba1":"code","2a8de25f":"code","c4ebbd4f":"code","e9c58ae1":"code","61351e4e":"code","79f043c9":"code","7471bf41":"code","528e22fe":"code","77476502":"markdown","63c79768":"markdown","4f24617c":"markdown","d689bde0":"markdown","a490aaa6":"markdown","b29a6546":"markdown","a142e542":"markdown","231418f3":"markdown","def4fcca":"markdown","13cdbe68":"markdown","500d53c9":"markdown","50cc7ef5":"markdown","b5104884":"markdown","4281dc05":"markdown","7678a94d":"markdown","83a0558a":"markdown","60a916cc":"markdown","ab920564":"markdown","ca1d06ac":"markdown","70a6ab93":"markdown","0ab882ac":"markdown"},"source":{"ba5e6822":"# Importing the relevant libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer \nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nimport re\nfrom functools import reduce\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook, reset_output\nfrom bokeh.palettes import d3\nimport bokeh.models as bmo\nfrom bokeh.io import save, output_file\n\n# init_notebook_mode(connected = True)\n# color = sns.color_palette(\"Set2\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\npd.options.display.max_rows = 999","ae6cf7eb":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ndisplay(train[train.target == 0].head(3))\ndisplay(train[train.target == 1].head(3))","a75882d0":"train.target.value_counts()","792532dd":"# Full number of insincere data points\nsample_size = 80_810 \n\n# halved the original size as rendering all the data points was causing lag issues\n# sample_size = int(sample_size\/2)\n\n# Rebalancing the training set\ntrain_rebal = train[train.target == 1].sample(sample_size).append(train[train.target == 0].sample(sample_size)).reset_index()","b433679d":"def remove_stopwords(words):\n    \"\"\"\n    Function to remove stopwords from the question text\n    \"\"\"\n    stop_words = set(stopwords.words(\"english\"))\n    return [word for word in words if word not in stop_words]\n\ndef remove_punctuation(text):\n    \"\"\"\n    Function to remove punctuation from the question text\n    \"\"\"\n    return re.sub(r'[^\\w\\s]', '', text)\n\ndef lemmatize_text(words):\n    \"\"\"\n    Function to lemmatize the question text\n    \"\"\"\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef stem_text(words):\n    \"\"\"\n    Function to stem the question text\n    \"\"\"\n    ps = PorterStemmer()\n    return [ps.stem(word) for word in words]","e1b596ca":"puncts=['\u2639', '\u0179', '\u017b', '\u1f30', '\u1f75', '\u0160', '\uff1e', '\u03be','\u0e09', '\u0e31', '\u0e19', '\u0e08', '\u0e30', '\u0e17', '\u0e33', '\u0e43', '\u0e2b', '\u0e49', '\u0e14', '\u0e35', '\u0e48', '\u0e2a', '\u0e38', '\u03a0', '\u092a', '\u090a', '\u00d6', '\u062e', '\u0628', '\u0b9c', '\u0bcb', '\u0b9f', '\u300c', '\u1ebd', '\u00bd', '\u25b3', '\u00c9', '\u0137', '\u00ef', '\u00bf', '\u0142', '\ubd81', '\ud55c', '\u00bc', '\u2206', '\u2265', '\u21d2', '\u00ac', '\u2228', '\u010d', '\u0161', '\u222b', '\u1e25', '\u0101', '\u012b', '\u00d1', '\u00e0', '\u25be', '\u03a9', '\uff3e', '\u00fd', '\u00b5', '?', '!', '.', ',', '\"', '#', '$', '%', '\\\\', \"'\", '(', ')', '*', '+', '-', '\/', ':', ';', '<', '=', '>', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', '\u201c', '\u201d', '\u2019', '\u00e9', '\u00e1', '\u2032', '\u2026', '\u027e', '\u0303', '\u0256', '\u00f6', '\u2013', '\u2018', '\u090b', '\u0960', '\u090c', '\u0961', '\u00f2', '\u00e8', '\u00f9', '\u00e2', '\u011f', '\u092e', '\u093f', '\u0932', '\u0917', '\u0908', '\u0915', '\u0947', '\u091c', '\u094b', '\u0920', '\u0902', '\u0921', '\u017d', '\u017e', '\u00f3', '\u00ae', '\u00ea', '\u1ea1', '\u1ec7', '\u00b0', '\u0635', '\u0648', '\u0631', '\u00fc', '\u00b2', '\u20b9', '\u00fa', '\u221a', '\u03b1', '\u2192', '\u016b', '\u2014', '\u00a3', '\u00e4', '\ufe0f', '\u00f8', '\u00b4', '\u00d7', '\u00ed', '\u014d', '\u03c0', '\u00f7', '\u02bf', '\u20ac', '\u00f1', '\u00e7', '\u3078', '\u306e', '\u3068', '\u3082', '\u2191', '\u221e', '\u02bb', '\u2105''\u03b9', '\u2022', '\u00ec', '\u2212', '\u043b', '\u044f', '\u0434', '\u0644', '\u0643', '\u0645', '\u0642', '\u0627', '\u2208', '\u2229', '\u2286', '\u00e3', '\u0905', '\u0928', '\u0941', '\u0938', '\u094d', '\u0935', '\u093e', '\u0930', '\u0924', '\u00a7', '\u2103', '\u03b8', '\u00b1', '\u2264', '\u0909', '\u0926', '\u092f', '\u092c', '\u091f', '\u0361', '\u035c', '\u0296', '\u2074', '\u2122', '\u0107', '\u00f4', '\u0441', '\u043f', '\u0438', '\u0431', '\u043e', '\u0433', '\u2260', '\u2202', '\u0906', '\u0939', '\u092d', '\u0940', '\u00b3', '\u091a', '...', '\u231a', '\u27e8', '\u27e9', '\u2216', '\u02c2', '\u207f', '\u2154', '\u0c28', '\u0c40', '\u0c15', '\u0c46', '\u0c02', '\u0c26', '\u0c41', '\u0c3e', '\u0c17', '\u0c30', '\u0c3f', '\u0c1a', '\u09b0', '\u09dc', '\u09dd', '\u0ab8', '\u0a82', '\u0a98', '\u0ab0', '\u0abe', '\u0a9c', '\u0acd', '\u0aaf', '\u03b5', '\u03bd', '\u03c4', '\u03c3', '\u015f', '\u015b', '\u0633', '\u062a', '\u0637', '\u064a', '\u0639', '\u0629', '\u062f', '\u00c5', '\u263a', '\u2107', '\u2764', '\u2668', '\u270c', '\ufb01', '\u3066', '\u201e', '\u0100', '\u178f', '\u17be', '\u1794', '\u1784', '\u17d2', '\u17a2', '\u17bc', '\u1793', '\u1798', '\u17b6', '\u1792', '\u1799', '\u179c', '\u17b8', '\u1781', '\u179b', '\u17c7', '\u178a', '\u179a', '\u1780', '\u1783', '\u1789', '\u17af', '\u179f', '\u17c6', '\u1796', '\u17b7', '\u17c3', '\u1791', '\u1782', '\u00a2', '\u3064', '\u3084', '\u0e04', '\u0e13', '\u0e01', '\u0e25', '\u0e07', '\u0e2d', '\u0e44', '\u0e23', '\u012f', '\u06cc', '\u044e', '\u028c', '\u028a', '\u05d9', '\u05d4', '\u05d5', '\u05d3', '\u05ea', '\u1820', '\u1873', '\u1830', '\u1828', '\u1864', '\u1860', '\u1875', '\u1e6d', '\u1ebf', '\u0927', '\u095c', '\u00df', '\u00b8', '\u0447',  '\u1ec5', '\u1ed9', '\u092b', '\u03bc', '\u29fc', '\u29fd', '\u09ae', '\u09b9', '\u09be', '\u09ac', '\u09bf', '\u09b6', '\u09cd', '\u09aa', '\u09a4', '\u09a8', '\u09df', '\u09b8', '\u099a', '\u099b', '\u09c7', '\u09b7', '\u09af', '\u09bc', '\u099f', '\u0989', '\u09a5', '\u0995', '\u1fe5', '\u03b6', '\u1f64', '\u00dc', '\u0394', '\ub0b4', '\uc81c', '\u0283', '\u0278', '\u1ee3', '\u013a', '\u00ba', '\u0937', '\u266d', '\u093c', '\u2705', '\u2713', '\u011b', '\u2218', '\u00a8', '\u2033', '\u0130', '\u20d7', '\u0302', '\u00e6', '\u0254', '\u2211', '\u00be', '\u042f', '\u0445', '\u041e', '\u0437', '\u0641', '\u0646', '\u1e35', '\u010c', '\u041f', '\u044c', '\u0412', '\u03a6', '\u1ef5', '\u0266', '\u028f', '\u0268', '\u025b', '\u0280', '\u010b', '\u0585', '\u028d', '\u057c', '\u0584', '\u028b', '\u5170', '\u03f5', '\u03b4', '\u013d', '\u0252', '\u00ee', '\u1f08', '\u03c7', '\u1fc6', '\u03cd', '\u12a4', '\u120d', '\u122e', '\u12a2', '\u12e8', '\u129d', '\u1295', '\u12a0', '\u1201', '\u2245', '\u03d5', '\u2011', '\u1ea3', '\ufffc', '\u05bf', '\u304b', '\u304f', '\u308c', '\u0151', '\uff0d', '\u0219', '\u05df', '\u0393', '\u222a', '\u03c6', '\u03c8', '\u22a8', '\u03b2', '\u2220', '\u00d3', '\u00ab', '\u00bb', '\u00cd', '\u0b95', '\u0bb5', '\u0bbe', '\u0bae', '\u2248', '\u2070', '\u2077', '\u1ea5', '\u0169', '\ub208', '\uce58', '\u1ee5', '\u00e5', '\u060c', '\uff1d', '\uff08', '\uff09', '\u0259', '\u0a28', '\u0a3e', '\u0a2e', '\u0a41', '\ufe20', '\ufe21', '\u0251', '\u02d0', '\u03bb', '\u2227', '\u2200', '\u014c', '\u315c', '\u039f', '\u03c2', '\u03bf', '\u03b7', '\u03a3', '\u0923']\nodd_chars=[ '\u5927','\u80fd', '\u5316', '\u751f', '\u6c34', '\u8c37', '\u7cbe', '\u5fae', '\u30eb', '\u30fc', '\u30b8', '\u30e5', '\u652f', '\u90a3', '\u00b9', '\u30de', '\u30ea', '\u4ef2', '\u76f4', '\u308a', '\u3057', '\u305f', '\u4e3b', '\u5e2d', '\u8840', '\u2153', '\u6f22', '\u9aea', '\u91d1', '\u8336', '\u8a13', '\u8aad', '\u9ed2', '\u0159', '\u3042', '\u308f', '\u308b', '\u80e1', '\u5357', '\uc218', '\ub2a5', '\u5e7f', '\u7535', '\u603b', '\u03af', '\uc11c', '\ub85c', '\uac00', '\ub97c', '\ud589', '\ubcf5', '\ud558', '\uac8c', '\uae30', '\u4e61', '\u6545', '\u723e', '\u6c5d', '\u8a00', '\u5f97', '\u7406', '\u8ba9', '\u9a82', '\u91ce', '\u6bd4', '\u3073', '\u592a', '\u5f8c', '\u5bae', '\u7504', '\u5b1b', '\u50b3', '\u505a', '\u83ab', '\u4f60', '\u9171', '\u7d2b', '\u7532', '\u9aa8', '\u9673', '\u5b97', '\u9648', '\u4ec0', '\u4e48', '\u8bf4', '\u4f0a', '\u85e4', '\u9577', '\ufdfa', '\u50d5', '\u3060', '\u3051', '\u304c', '\u8857', '\u25e6', '\u706b', '\u56e2', '\u8868',  '\u770b', '\u4ed6', '\u987a', '\u773c', '\u4e2d', '\u83ef', '\u6c11', '\u570b', '\u8a31', '\u81ea', '\u6771', '\u513f', '\u81e3', '\u60f6', '\u6050', '\u3063', '\u6728', '\u30db', '\u062c', '\u6559', '\u5b98', '\uad6d', '\uace0', '\ub4f1', '\ud559', '\uad50', '\ub294', '\uba87', '\uc2dc', '\uac04', '\uc5c5', '\ub2c8', '\u672c', '\u8a9e', '\u4e0a', '\u624b', '\u3067', '\u306d', '\u53f0', '\u6e7e', '\u6700', '\u7f8e', '\u98ce', '\u666f', '\u00ce', '\u2261', '\u768e', '\u6ee2', '\u6768', '\u221b', '\u7c21', '\u8a0a', '\u77ed', '\u9001', '\u767c', '\u304a', '\u65e9', '\u3046', '\u671d', '\u0634', '\u0647', '\u996d', '\u4e71', '\u5403', '\u8bdd', '\u8bb2', '\u7537', '\u5973', '\u6388', '\u53d7', '\u4eb2', '\u597d', '\u5fc3', '\u6ca1', '\u62a5', '\u653b', '\u514b', '\u79ae', '\u5100', '\u7d71', '\u5df2', '\u7d93', '\u5931', '\u5b58', '\u0668', '\u516b', '\u201b', '\u5b57', '\uff1a', '\u522b', '\u9ad8', '\u5174', '\u8fd8', '\u51e0', '\u4e2a', '\u6761', '\u4ef6', '\u5462', '\u89c0', '\u300a', '\u300b', '\u8a18', '\u5b8b', '\u695a', '\u745c', '\u5b6b', '\u701b', '\u679a', '\u65e0', '\u6311', '\u5254', '\u8056', '\u90e8', '\u982d', '\u5408', '\u7d04', '\u03c1', '\u6cb9', '\u817b', '\u908b', '\u9062', '\u064c', '\u00c4', '\u5c04', '\u7c4d', '\u8d2f', '\u8001', '\u5e38', '\u8c08', '\u65cf', '\u4f1f', '\u590d', '\u5e73', '\u5929', '\u4e0b', '\u60a0', '\u5835', '\u963b', '\u611b', '\u8fc7', '\u4f1a', '\u4fc4', '\u7f57', '\u65af', '\u8339', '\u897f', '\u4e9a', '\uc2f1', '\uad00', '\uc5c6', '\uc5b4', '\ub098', '\uc774', '\ud0a4', '\u5922', '\u5f69', '\u86cb', '\u9c39', '\u7bc0', '\u72d0', '\u72f8', '\u9cf3', '\u51f0', '\u9732', '\u738b', '\u6653', '\u83f2', '\u604b', '\u306b', '\u843d', '\u3061', '\u3089', '\u3088', '\u60b2', '\u53cd', '\u6e05', '\u5fa9', '\u660e', '\u8089', '\u5e0c', '\u671b', '\u6c92', '\u516c', '\u75c5', '\u914d', '\u4fe1', '\u958b', '\u59cb', '\u65e5', '\u5546', '\u54c1', '\u767a', '\u58f2', '\u5206', '\u5b50', '\u521b', '\u610f', '\u68a6', '\u5de5', '\u574a', '\u06a9', '\u067e', '\u06a4', '\u862d', '\u82b1', '\u7fa1', '\u6155', '\u548c', '\u5ac9', '\u5992', '\u662f', '\u6837', '\u3054', '\u3081', '\u306a', '\u3055', '\u3044', '\u3059', '\u307f', '\u307e', '\u305b', '\u3093', '\u97f3', '\u7ea2', '\u5b9d', '\u4e66', '\u5c01', '\u67cf', '\u8363', '\u6c5f', '\u9752', '\u9e21', '\u6c64', '\u6587', '\u7cb5', '\u62fc', '\u5be7', '\u53ef', '\u932f', '\u6bba', '\u5343', '\u7d55', '\u653e', '\u904e', '\u300d', '\u4e4b', '\u52e2', '\u8bf7', '\u56fd', '\u77e5', '\u8bc6', '\u4ea7', '\u6743', '\u5c40', '\u6a19', '\u9ede', '\u7b26', '\u865f', '\u65b0', '\u5e74', '\u5feb', '\u4e50', '\u5b66', '\u4e1a', '\u8fdb', '\u6b65', '\u8eab', '\u4f53', '\u5065', '\u5eb7', '\u4eec', '\u8bfb', '\u6211', '\u7684', '\u7ffb', '\u8bd1', '\u7bc7', '\u7ae0', '\u6b22', '\u8fce', '\u5165', '\u5751', '\u6709', '\u6bd2', '\u9ece', '\u6c0f', '\u7389', '\u82f1', '\u5567', '\u60a8', '\u8fd9', '\u53e3', '\u5473', '\u5947', '\u7279', '\u4e5f', '\u5c31', '\u7f62', '\u4e86', '\u975e', '\u8981', '\u4ee5', '\u6b64', '\u4e3a', '\u4f9d', '\u636e', '\u5bf9', '\u4eba', '\u5bb6', '\u6279', '\u5224', '\u4e00', '\u756a', '\u4e0d', '\u5730', '\u9053', '\u554a', '\u8c22', '\u516d', '\u4f6c']\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",  \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } ","c0d42a1b":"def clean_numbers(x):\n    x = re.sub('[0-9]{5,}', ' ##### ', x)\n    x = re.sub('[0-9]{4}', ' #### ', x)\n    x = re.sub('[0-9]{3}', ' ### ', x)\n    x = re.sub('[0-9]{2}', ' ## ', x)\n    return x\n\ndef punct_add_space(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x  \n\ndef odd_add_space(x):\n    x = str(x)\n    for odd in odd_chars:\n        x = x.replace(odd, f' {odd} ')\n    return x \n\ndef clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","372d3a89":"train_rebal[\"question_text\"] = train_rebal[\"question_text\"].apply(lambda x: clean_numbers(x))\ntrain_rebal[\"question_text\"] = train_rebal[\"question_text\"].apply(lambda x: punct_add_space(x))\ntrain_rebal[\"question_text\"] = train_rebal[\"question_text\"].apply(lambda x: odd_add_space(x))\ntrain_rebal[\"question_text\"] = train_rebal[\"question_text\"].apply(lambda x: clean_contractions(x, contraction_mapping))","a19c1456":"# With the new updated processing - let's leave out the removal of stopwords, punctuation and lemmatization functions.\n\n# Tokenizing the text\ntrain_rebal[\"question_text\"] = train_rebal[\"question_text\"].apply(lambda x: word_tokenize(x))\n\n# Removing stopwords - Leaving Stopwords\n# train_rebal[\"question_text\"] = train_rebal[\"question_text\"].apply(lambda x: remove_stopwords(x))\n\n# Lemmatizting\n# train_rebal[\"question_text\"] = train_rebal[\"question_text\"].apply(lambda x: lemmatize_text(x))","93b43730":"train_rebal.head(3)","f3243c52":"tf_idf_vec = TfidfVectorizer(min_df=3,\n                             max_features = 60_000, #100_000,\n                             analyzer=\"word\",\n                             ngram_range=(1,3), # (1,6)\n                             stop_words=\"english\")\ntf_idf = tf_idf_vec.fit_transform(list(train_rebal[\"question_text\"].map(lambda tokens: \" \".join(tokens))))","4755852c":"# Applying the Singular value decomposition\nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=50, random_state=2018)\nsvd_tfidf = svd.fit_transform(tf_idf)\nprint(\"Dimensionality of LSA space: {}\".format(svd_tfidf.shape))","57bdfb9e":"# Showing scatter plots \nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(16,12))\n\n# Plot models:\nax = Axes3D(fig) \nax.scatter(svd_tfidf[:,0],\n           svd_tfidf[:,1],\n           svd_tfidf[:,2],\n           c=train_rebal.target.values,\n           cmap=plt.cm.winter_r,\n           s=2,\n           edgecolor='none',\n           marker='o')\nplt.title(\"Semantic Tf-Idf-SVD reduced plot of Sincere-Insincere data distribution\")\nplt.xlabel(\"First dimension\")\nplt.ylabel(\"Second dimension\")\nplt.legend()\nplt.xlim(0.0, 0.20)\nplt.ylim(-0.2,0.4)\nplt.show()","52bf578f":"# from sklearn.manifold import TSNE\n\n# Importing multicore version of TSNE\nfrom MulticoreTSNE import MulticoreTSNE as TSNE","169d1ff7":"tsne_model = TSNE(n_jobs=4,\n                  early_exaggeration=4, # Trying out exaggeration trick\n                  n_components=2,\n                  verbose=1,\n                  random_state=2018,\n                  n_iter=500)\ntsne_tfidf = tsne_model.fit_transform(svd_tfidf)","d26f04d0":"# Putting the tsne information into a dataframe\ntsne_tfidf_df = pd.DataFrame(data=tsne_tfidf, columns=[\"x\", \"y\"])\ntsne_tfidf_df[\"qid\"] = train_rebal[\"qid\"].values\ntsne_tfidf_df[\"question_text\"] = train_rebal[\"question_text\"].values\ntsne_tfidf_df[\"target\"] = train_rebal[\"target\"].values","b426034c":"output_notebook()\nplot_tfidf = bp.figure(plot_width = 800, plot_height = 700, \n                       title = \"T-SNE applied to Tfidf_SVD space\",\n                       tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                       x_axis_type = None, y_axis_type = None, min_border = 1)\n\n# colormap = np.array([\"#6d8dca\", \"#d07d3c\"])\ncolormap = np.array([\"darkblue\", \"red\"])\n\n# palette = d3[\"Category10\"][len(tsne_tfidf_df[\"asset_name\"].unique())]\nsource = ColumnDataSource(data = dict(x = tsne_tfidf_df[\"x\"], \n                                      y = tsne_tfidf_df[\"y\"],\n                                      color = colormap[tsne_tfidf_df[\"target\"]],\n                                      question_text = tsne_tfidf_df[\"question_text\"],\n                                      qid = tsne_tfidf_df[\"qid\"],\n                                      target = tsne_tfidf_df[\"target\"]))\n\nplot_tfidf.scatter(x = \"x\", \n                   y = \"y\", \n                   color=\"color\",\n                   legend = \"target\",\n                   source = source,\n                   alpha = 0.7)\nhover = plot_tfidf.select(dict(type = HoverTool))\nhover.tooltips = {\"qid\": \"@qid\", \n                  \"question_text\": \"@question_text\", \n                  \"target\":\"@target\"}\n\nshow(plot_tfidf)","a55ffa8c":"# Perplexity = 5\ntsne_model_5 = TSNE(n_jobs=4, \n                    early_exaggeration=4,\n                  perplexity=5,\n                  n_components=2,\n                  verbose=1,\n                  random_state=2018,\n                  n_iter=500)\ntsne_tfidf_5 = tsne_model_5.fit_transform(svd_tfidf[:50_000,:])\n# Creating a Dataframe for Perplexity=5\ntsne_tfidf_df_5 = pd.DataFrame(data=tsne_tfidf_5, columns=[\"x5\", \"y5\"])\ntsne_tfidf_df_5[\"target\"] = train_rebal[\"target\"][:50_000].values","682c8d40":"# Perplexity = 25\ntsne_model_25 = TSNE(n_jobs=4, \n                     early_exaggeration=4,\n                  perplexity=25,\n                  n_components=2,\n                  verbose=1,\n                  random_state=2018,\n                  n_iter=500)\ntsne_tfidf_25 = tsne_model_25.fit_transform(svd_tfidf[:50_000,:])\n# Creating a Dataframe for Perplexity=5\ntsne_tfidf_df_25 = pd.DataFrame(data=tsne_tfidf_25, \n                             columns=[\"x25\", \"y25\"])\ntsne_tfidf_df_25[\"target\"] = train_rebal[\"target\"][:50_000].values","68f138fc":"# Perplexity = 50\ntsne_model_50 = TSNE(n_jobs=4, \n                     early_exaggeration=4,\n                  perplexity=200,\n                  n_components=2,\n                  verbose=1,\n                  random_state=2018,\n                  n_iter=500)\ntsne_tfidf_50 = tsne_model_50.fit_transform(svd_tfidf[:50_000,:])\n# Creating a Dataframe for Perplexity=50\ntsne_tfidf_df_50 = pd.DataFrame(data=tsne_tfidf_50, \n                                columns=[\"x50\", \"y50\"])\ntsne_tfidf_df_50[\"target\"] = train_rebal[\"target\"][:50_000].values","752d5fb6":"# Showing scatter plots \nplt.figure(figsize=(14,8))\nplt.scatter(tsne_tfidf_df_5.x5, \n            tsne_tfidf_df_5.y5, \n            alpha=0.75,\n            c=tsne_tfidf_df_5.target,\n            cmap=plt.cm.coolwarm)\nplt.title(\"T-SNE plot in SVD space (perplexity=5)\")\nplt.legend()\nplt.show()","22d57ba1":"plt.figure(figsize=(14,8))\nplt.scatter(tsne_tfidf_df_25.x25, \n            tsne_tfidf_df_25.y25, \n            c=tsne_tfidf_df_25.target,\n            cmap=plt.cm.coolwarm)\nplt.title(\"T-SNE plot in SVD space (perplexity=25)\")\nplt.legend()\nplt.show()","2a8de25f":"plt.figure(figsize=(14,8))\nplt.scatter(tsne_tfidf_df_50.x50, \n            tsne_tfidf_df_50.y50, \n            c=tsne_tfidf_df_50.target,\n            cmap=plt.cm.coolwarm)\nplt.title(\"T-SNE plot in SVD space (perplexity=50)\")\nplt.legend()\nplt.show()","c4ebbd4f":"from gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument","e9c58ae1":"# Storing the question texts in a list\nquora_texts = list(train_rebal[\"question_text\"])\n\n# Creating a list of terms and a list of labels to go with it\ndocuments = [TaggedDocument(doc, tags=[str(i)]) for i, doc in enumerate(quora_texts)]","61351e4e":"max_epochs = 100\nalpha=0.025\nmodel = Doc2Vec(documents,\n                size=10, \n                min_alpha=0.00025,\n                alpha=alpha,\n                min_count=1,\n#                 window=2, \n                workers=4)","79f043c9":"# model.build_vocab(documents)\n\n# for epoch in range(max_epochs):\n#     print('iteration {0}'.format(epoch))\n#     model.train(documents,\n#                 total_examples=model.corpus_count,\n#                 epochs=model.iter)\n#     # decrease the learning rate\n#     model.alpha -= 0.0002\n#     # fix the learning rate, no decay\n#     model.min_alpha = model.alpha","7471bf41":"# Creating and fitting the tsne model to the document embeddings\ntsne_model = TSNE(n_jobs=4,\n                  early_exaggeration=4,\n                  n_components=2,\n                  verbose=1,\n                  random_state=2018,\n                  n_iter=300)\ntsne_d2v = tsne_model.fit_transform(model.docvecs.vectors_docs)\n\n# Putting the tsne information into sq\ntsne_d2v_df = pd.DataFrame(data=tsne_d2v, columns=[\"x\", \"y\"])\n# tsne_tfidf_df.columns = [\"x\", \"y\"]\ntsne_d2v_df[\"qid\"] = train_rebal[\"qid\"].values\ntsne_d2v_df[\"question_text\"] = train_rebal[\"question_text\"].values\ntsne_d2v_df[\"target\"] = train_rebal[\"target\"].values","528e22fe":"output_notebook()\nplot_d2v = bp.figure(plot_width = 800, plot_height = 700, \n                       title = \"T-SNE applied to Doc2vec document embeddings\",\n                       tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                       x_axis_type = None, y_axis_type = None, min_border = 1)\n\n# colormap = np.array([\"#6d8dca\", \"#d07d3c\"])\ncolormap = np.array([\"darkblue\", \"cyan\"])\n\n# palette = d3[\"Category10\"][len(tsne_tfidf_df[\"asset_name\"].unique())]\nsource = ColumnDataSource(data = dict(x = tsne_d2v_df[\"x\"], \n                                      y = tsne_d2v_df[\"y\"],\n                                      color = colormap[tsne_d2v_df[\"target\"]],\n                                      question_text = tsne_d2v_df[\"question_text\"],\n                                      qid = tsne_d2v_df[\"qid\"],\n                                      target = tsne_d2v_df[\"target\"]))\n\nplot_d2v.scatter(x = \"x\", \n                   y = \"y\", \n                   color=\"color\",\n                   legend = \"target\",\n                   source = source,\n                   alpha = 0.7)\nhover = plot_d2v.select(dict(type = HoverTool))\nhover.tooltips = {\"qid\": \"@qid\", \n                  \"question_text\": \"@question_text\", \n                  \"target\":\"@target\"}\n\nshow(plot_d2v)","77476502":"### Visualization of target variable via Bokeh\n\nTurning to the target variable visualization in T-SNE reduced concept space, we will use the plotting library Bokeh to suit our purposes:","63c79768":"Reading in the train and test datasets and inspecting the first 5 rows of the dataset, we see that each Quora question text comes with a label (the \"target\" column) consisting of the binary values of either 1 - insincere or 0 - sincere questions.","4f24617c":"----\n## 2. T-SNE applied to Latent Semantic (LSA) space\n\nTo start off we look at the sparse representation of text documents via the Term frequency Inverse document frequency method. What this does is create a matrix representation that upweights locally prevalent but globally rare terms - therefore accounting for the occurence bias when using just term frequencies.","d689bde0":"*[CAVEAT: THIS NOTEBOOK TAKES A QUITE SOME TIME TO LOAD DUE TO BOKEH RENDERING THE PLOTS AND IT ALSO TAKES MORE THEN AN HOUR TO RUN - NOTE IF YOU FORK IT]*\n\n----\n# Introduction\n\nThis kernel will be an exploration into the target variable and how it is distributed across the structure of the training data to see if any potential information or patterns can be gleaned going forward. Since classical treatments of text data normally comes with the challenges of high dimensionality (using term frequencies or term frequency inverse document frequencies), the plan therefore in this kernel is to visually explore the target variable in some lower dimensional space. \n\nWe will explore two methods of representing the data in a lower dimensional space, with the first being using the truncated SVD method to linear reduce the dimensions of the term frequency representation of the text data - i.e. a method known as Latent Semantic Analysis (LSA). The second method would be to utilize document embeddings via the Doc2Vec method, which learns a lower dimensional projection for each question. In these lower dimensional spaces, we can finally then utilize the manifold learning method of the t-Distributed Stochastic Neighbor Embedding (t-SNE) technique to further reduce the dimensionality for target variable visualization.\n\nThe kernel is structured as follows:\n\n1. Text preprocessing on question text via standard NLP\n2. T-SNE on LSA feature space: \n3. T-SNE on Doc2Vec space: \n\nLet's go","a490aaa6":"Finally we can implement the Doc2Vec model as follows","b29a6546":"----\n## 3. T-SNE applied on Doc2Vec embedding\n\nPushing forward with our T-SNE visual explorations, we next move away from semantic matrices into the realm of embeddings. Here we will use the Doc2Vec algorithm and much like its very well known counterpart Word2vec involves unsupervised learning of continuous representations for text. Unlike Word2vec which involves finding the representations for words (i.e. word embeddings), Doc2vec modifies the former method and extends it to  sentences and even documents.\n\nFor this notebook, we will be using gensim's Doc2Vec class which inherits from the base Word2Vec class where style of usage and parameters are similar. The only differences lie in the naming terminology of the training method used which are the \u201cdistributed memory\u201d or \u201cdistributed bag of words\u201d methods.\n\nAccording to the Gensim documentation, Doc2Vec requires the input to be an iterable object representing the sentences in the form of two lists, a list of the terms and a list of labels.","a142e542":"Perhaps a non-linear technique (T-SNE) yield more insights? This also helps to collapse the information from all 50 dimensions when we apply the T-SNE technique to this LSA reduced space . Here I've used the multicore implementation of T-SNE to speed things up instead of the plain vanilla sklearn version. ","231418f3":"Peeking into the newly processed training set, we can see that the resulting text in the \"question text\" columns are now nicely tokenized and lemmatized with all stopwords removed.","def4fcca":"For the purposes of speeding up code execution and modeling in the latter stages, I shall re-balance (naively via random sampling) the dataset set by with equal rows of sincere and insincere data points:","13cdbe68":"Having obtained our tf-idf matrix - a sparse matrix object, we now apply the TruncatedSVD method to first reduce the dimensionality of the Tf-idf matrix to a decomposed feature space, referred to in the community as the LSA (Latent Semantic Analysis) method.\n\nLSA has been one of the classical methods in text that have existed for a while allowing \"concept\" searching of words whereby words which are semantically similar to each other (i.e. have more context) are closer to each other in this space and vice-versa.","500d53c9":"**Takeaways from the plot**\n\nThe visual overlap between Sincere and Insincere labelled questions are even greater in the Doc2Vec plots - so much so that there doesn't seem to be any obvious manner to segragate the labels via eye-balling if going down the route of document embeddings.","50cc7ef5":"**Takeaways from the plot**\n\n- It seems that the distribution of the sincere to insincere data points overlap quite substantially in certain regions of the T-SNE plots in concept space, which unfortunately does not allow easy visual discernment between the two classes (even using this non-linear method).\n- This begs the question of how easy therefore, is it in terms of semantic meaning (to a human) to distinguish between an insincere and a sincere question, when we see data from both class labels overlapping quite heavily across each other. Quite a few of the insincere and sincere questions when read aloud do share quite a lot of similarities as well.\n- There is also a popular thread going on in the Discussions\/Forums on how the Sincere and Insincere labels were generated - potentially offering the argument that some of the classes could even have been wrongly applied.\n- There does appear to be one region in the T-SNE plot where the sincere class labels visually seem to be quite separated and exist in its own area of the non-linear semantic space. \n\nOne key point to note in T-SNE plots is that cluster sizes as well as distances from one cluster to another are not meaningful to the overall global geometry of the picture. This can be evinced from the variation in geometry when we for example change the perplexity value (defaulted to 30) to range from 5 all the way to 50 and note how this in turn affects the geometry of the manifold :\n","b5104884":"Quickly plotting a scatter plot of the first 3 dimensions of the latent semantic space just to get an initial feel for how the target variables are distributed:","4281dc05":"Now, inspecting the class distributions we see that the provided training set and labels are rather imbalanced with about 6% of the data points being insincere, while the remainder 94% being sincere questions.","7678a94d":"----\n## 1. Text Processing\n\nIn this section, we arrive at the pre-processing of the question text contained within the training data. The processing applied here are some of the standard NLP steps that one would implement in a text based problem, consisting of:\n\n* Tokenization\n* Stemming or Lemmatization","83a0558a":"Fitting a T-SNE model to the dense embeddings and overlaying that with the target visuals, we get:","60a916cc":"Now applying these functions sequentially to the question text field:","ab920564":"**Takeaways from the plot**\n\nFrom the above scatter plots, It is apparent that sincere and insincere question data points overlap quite significantly in the LSA semantic space. The data points visually appear to be evenly distributed and there does not seem to be any clear or obvious pattern in segregating the class labels. However, do keep in mind that this involves only the first three dimensions of the decomposed space whilst we haven't incorporated information from the other 48 dimensions yet and also the fact that the SVD is a linear decomposition technique.","ca1d06ac":"**Tf-idf space**","70a6ab93":"In the cell below, I've defined a whole bunch of punctuation, odd characters and contractions so feel free to unhide it should you wish to see the terms explicitly.","0ab882ac":"**Updates - 28\/12\/18**\n\nBased on the many effective public LSTM\/GRU-variant kernels which employ text pre-processing that cleans numbers, add spaces to punctuation and odd characters as well as expanding out contractions, let us employ these methods as well and see how that affects the T-SNE and Doc2Vec plots."}}