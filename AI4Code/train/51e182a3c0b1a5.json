{"cell_type":{"7f848baf":"code","6c190e18":"code","a38bcd45":"code","47b17422":"code","e2526be5":"code","c951ede0":"code","62526c2a":"code","5d069ebf":"code","7a7c75f8":"code","e9e7e3aa":"code","95f5dc47":"code","539f74f1":"code","adb02252":"code","b4dc6428":"code","1a506838":"code","a99d8a9d":"code","bd2d6cc7":"code","ae7e346c":"code","87a561a2":"code","e860d935":"code","2ed6d966":"markdown","a0838de3":"markdown","6861fe58":"markdown","8c193588":"markdown","ad043ca7":"markdown","438fcad4":"markdown","122e8159":"markdown","c9c172ff":"markdown","17fd8a3b":"markdown","6bb3b184":"markdown","03b971e7":"markdown"},"source":{"7f848baf":"#Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict \nfrom pathlib import Path \nfrom tqdm import tqdm \nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom xgboost import plot_importance\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder\nimport gc\nimport json\nimport shap\nimport os","6c190e18":"kenpom = pd.read_csv('..\/input\/kenpom-2020\/Mkenpom2021.csv')\n\nkenpom.Team = kenpom.Team.str.lower()\nteams = pd.read_csv('..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2\/MTeamSpellings.csv', encoding='cp1252')\nkenpom = pd.merge(kenpom, teams, left_on=['Team'], right_on=['TeamNameSpelling'], how='left')\nkenpom = kenpom.drop(columns=['TeamNameSpelling', 'Team'])\n\nnumerical_cols = ['AdjustO', 'AdjustD', 'AdjustT', 'Luck']\nkenpom = kenpom[['TeamID', 'Year'] + numerical_cols].dropna(subset=['TeamID', 'Year'])\nkenpom.TeamID = kenpom.TeamID.astype(int)\nkenpom = kenpom.rename(columns={'Year': 'Season'})\nkenpom","a38bcd45":"def get_moving_averages(df, team_col, target_col): \n    totals = defaultdict(int)\n    weight_sums = defaultdict(int)\n    df_ = df.set_index(['Season', team_col])\n    found = 0 \n    not_found = 0\n    year_weights = {2019: 128, 2018: 64, 2017: 32, 2016: 16, 2015: 8, 2014: 4, 2013: 2, 2012: 1, 2011: 0.5, 2010: 0.25}\n    for team in df[team_col].unique(): \n        for year in [2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010]: \n            wt = year_weights[year]\n            \n            df_year = df[df.Season == year]\n            if (year, team) in df_.index: \n                found += 1\n                weight_sums[team] += wt \n                totals[team] += wt * df_.loc[year, team][target_col]\n            else: \n                not_found += 1\n        try: \n            totals[team] = round(totals[team] \/ weight_sums[team], 3)\n        except: \n            continue\n    print('found for: ', found * 100 \/ (found + not_found) )\n    return totals\n\n","47b17422":"import pandas as pd\nfrom sklearn.preprocessing import RobustScaler\n\nf38 = pd.read_csv('..\/input\/ncaa-men-538-team-ratings\/538ratingsMen.csv')\nf38.TeamID = f38.TeamID.astype(int)\nf38 = f38[['Season', 'TeamID', '538rating']]\n\nteam_to_f38_rating = get_moving_averages(f38, 'TeamID', '538rating')\n\ndef get_f38(team_id): \n    return team_to_f38_rating[team_id] if team_id in team_to_f38_rating else 0\nkenpom['f38'] = kenpom.TeamID.apply(get_f38)","e2526be5":"import numpy as np\nimport pandas as pd\n\ninp = '..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2\/'\nseason_df = pd.read_csv(inp+'MRegularSeasonCompactResults.csv')\ntourney_df = pd.read_csv(inp+'MNCAATourneyCompactResults.csv')\nordinals_df = pd.read_csv(inp+'MMasseyOrdinals.csv').rename(columns={'RankingDayNum':'DayNum'})\n\n# Get the last available data from each system previous to the tournament\nordinals_df = ordinals_df.groupby(['SystemName','Season','TeamID']).last().reset_index()\nordinals_df['Rating']= 100-4*np.log(ordinals_df['OrdinalRank']+1)-ordinals_df['OrdinalRank']\/22\nref_system = 'POM'\nordinals_df = ordinals_df[ordinals_df.SystemName==ref_system]\nmassey_df = ordinals_df.set_index(['Season', 'TeamID'])['Rating']\n\nmassey_df","c951ede0":"massey_df.mean()","62526c2a":"seeds = pd.read_csv('..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2\/MNCAATourneySeeds.csv')\nseeds['seed'] = seeds['Seed'].apply(lambda x: int(x[1:3]))\nseeds = seeds.set_index(['Season', 'TeamID'])\nseeds = seeds.drop('Seed', 1)\nseeds","5d069ebf":"def prepare_data(df):\n    dfswap = df[['Season', 'DayNum', 'LTeamID', 'LScore', 'WTeamID', 'WScore', 'WLoc', 'NumOT', \n    'LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', \n    'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF']]\n\n    dfswap.loc[df['WLoc'] == 'H', 'WLoc'] = 'A'\n    dfswap.loc[df['WLoc'] == 'A', 'WLoc'] = 'H'\n    df.columns.values[6] = 'location'\n    dfswap.columns.values[6] = 'location'    \n      \n    df.columns = [x.replace('W','T1_').replace('L','T2_') for x in list(df.columns)]\n    dfswap.columns = [x.replace('L','T1_').replace('W','T2_') for x in list(dfswap.columns)]\n\n    output = pd.concat([df, dfswap]).reset_index(drop=True)\n    output.loc[output.location=='N','location'] = '0'\n    output.loc[output.location=='H','location'] = '1'\n    output.loc[output.location=='A','location'] = '-1'\n    output.location = output.location.astype(int)\n    \n    output['PointDiff'] = output['T1_Score'] - output['T2_Score']\n    \n    return output\nregular_results = pd.read_csv(\"..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2\/MRegularSeasonDetailedResults.csv\")\ntourney_results = pd.read_csv('..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2\/MNCAATourneyDetailedResults.csv')\nregular_data = prepare_data(regular_results)\ntourney_data = prepare_data(tourney_results)\nboxscore_cols = ['T1_FGM','T1_Stl', 'T2_FGM',  'T2_Stl']\nseason_statistics = regular_data.groupby([\"Season\", 'T1_TeamID'])[boxscore_cols].mean().reset_index()\nseason_statistics_T1 = season_statistics.copy()\nseason_statistics_T2 = season_statistics.copy()\n\nseason_statistics_T1.columns = [\"T1_\" + x.replace(\"T1_\",\"\").replace(\"T2_\",\"opponent_\") for x in list(season_statistics_T1.columns)]\nseason_statistics_T2.columns = [\"T2_\" + x.replace(\"T1_\",\"\").replace(\"T2_\",\"opponent_\") for x in list(season_statistics_T2.columns)]\nseason_statistics_T1.columns.values[0] = \"Season\"\nseason_statistics_T2.columns.values[0] = \"Season\"\nseason_statistics_T1","7a7c75f8":"team_to_fgm = get_moving_averages(season_statistics_T1, 'T1_TeamID', 'T1_FGM')\nteam_to_stl = get_moving_averages(season_statistics_T1, 'T1_TeamID', 'T1_Stl')\n\n\ndef get_stl(team_id): \n    return team_to_stl[team_id] if team_id in team_to_stl else 0\n\ndef get_fgm(team_id):\n    return team_to_fgm[team_id] if team_id in team_to_fgm else 0\n\nkenpom['STL'] = kenpom.TeamID.apply(get_stl)\nkenpom['FGM'] = kenpom.TeamID.apply(get_fgm)\nkenpom = kenpom.set_index(['Season', 'TeamID'])\n\nkenpom = kenpom.drop('f38', 1) #f38 has a lot of missing data\nkenpom","e9e7e3aa":"def get_kenpom_data(seasons, team_set1, team_set2):\n    found_in_kenpom = 0\n    found_in_massey = 0\n    found_in_seeds = 0\n    all_features = []\n    for season, team1, team2 in tqdm(zip(seasons, team_set1, team_set2), total=len(team_set1)): \n        \n        team1_kenpom_features = kenpom.mean().values\n        team2_kenpom_features = kenpom.mean().values\n        if (season, team1) in kenpom.index: \n            team1_kenpom_features = kenpom.loc[season, team1].values\n            found_in_kenpom += 1\n        if (season, team2) in kenpom.index: \n            team2_kenpom_features = kenpom.loc[season, team2].values\n            found_in_kenpom += 1\n        \n        team1_massey = team2_massey = 73\n        if (season, team1) in massey_df.index: \n            team1_massey = massey_df.loc[season, team1]\n            found_in_massey += 1\n        if (season, team2) in massey_df.index: \n            team2_massey = massey_df.loc[season, team2]\n            found_in_massey += 1\n        \n        team1_seed = team2_seed = 10\n        if (season, team1) in seeds.index: \n            team1_seed = seeds.loc[season, team1]\n            found_in_seeds += 1\n        if (season, team2) in seeds.index: \n            team2_seed = seeds.loc[season, team2]\n            found_in_seeds += 1 \n        \n        team1_features = np.append(team1_kenpom_features, team1_massey)\n        team1_features = np.append(team1_features, team1_seed)\n        team2_features = np.append(team2_kenpom_features, team2_massey)\n        team2_features = np.append(team2_features, team2_seed)\n        \n        features = np.concatenate((team2_features, team1_features))\n        massey_pred = 1. \/ (1e-6 + 10 ** ((team1_massey - team2_massey) \/ 15)) \n        features = np.append(features, massey_pred)\n        all_features.append(features)\n    \n    return {\n        'X': np.array(all_features), \n        'kenpom_found': found_in_kenpom \/ (2 * len(team_set1) + 1e-6), \n        'massey_found': found_in_massey \/ (2 * len(team_set1) + 1e-6), \n        'seeds_found': found_in_seeds \/ (2 * len(team_set1) + 1e-6)\n    }\n            ","95f5dc47":"FAST_RUN = False","539f74f1":"from sklearn.model_selection import KFold\nimport random \nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm import LGBMRegressor\nimport lightgbm\nimport matplotlib.pyplot as plt \nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss, mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nbase_dir = Path('..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2')\nDEBUG = False\n\nall_seasons = pd.read_csv(base_dir \/ 'MRegularSeasonCompactResults.csv')\nlgbm_parameters= {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n}\n\nfrom_year = 2007\nif FAST_RUN: \n    from_year = 2012\nall_seasons = all_seasons[all_seasons.Season > from_year] \ntest = pd.read_csv('..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2\/MSampleSubmissionStage2.csv')\ntest['Season'] = test['ID'].apply(lambda x: int(x.split('_')[0]))\ntest['TeamID1'] = test['ID'].apply(lambda x: int(x.split('_')[1]))\ntest['TeamID2'] = test['ID'].apply(lambda x: int(x.split('_')[2]))\ntest = test.drop(['Pred','ID'], axis=1)\n\n\ntest_pred = np.zeros(len(test))\ntest_pred = []\ntest_pred_mse = []\n\nSPLITS = 5\nif FAST_RUN: \n    SPLITS = 3\n\nkf = KFold(n_splits=SPLITS, shuffle=True)\ny_preds = []\nfor year in test['Season'].unique():\n    if FAST_RUN and year > 2015: \n        break\n    season_df = all_seasons[all_seasons.Season < year] #training on past data\n    team1_wins = np.random.randint(0, 2, len(season_df))\n    winner_teams = season_df.WTeamID.values \n    loser_teams = season_df.LTeamID.values\n    team_set1 = np.where(team1_wins == 1, winner_teams, loser_teams)\n    team_set2 = np.where(team1_wins == 0, winner_teams, loser_teams)\n    seasons = season_df.Season.values\n    res = get_kenpom_data(seasons, team_set1, team_set2)\n    X_year = res['X']\n    y_year = team1_wins\n    \n    score_diff = (season_df.WScore.values - season_df.LScore.values) \/ (season_df.WScore.values + season_df.LScore.values)\n    y_year_mse = np.where(team1_wins == 1, score_diff, -score_diff)\n    \n    print('Kenpom data found for : ', res['kenpom_found'] * 100)\n    print('Massey data found for: ', res['massey_found'] * 100)\n    print('Seeds found for: ', res['seeds_found'] * 100)\n    \n    test_year = test[test['Season'] == year]\n \n    lgbm_val_pred = np.zeros(len(y_year))\n    lgbm_val_pred_mse = np.zeros(len(y_year))\n    lgbm_test_pred = np.zeros(len(test_year))\n    lgbm_test_pred_mse = np.zeros(len(test_year))\n    logloss = []\n    losses_mse = []\n    \n    for trn_idx, val_idx in kf.split(X_year,y_year):\n        x_train_idx = X_year[trn_idx]\n        x_valid_idx = X_year[val_idx]\n        \n        y_valid_idx = y_year[val_idx]\n        y_train_idx = y_year[trn_idx]\n        y_valid_idx_mse = y_year_mse[val_idx]\n        y_train_idx_mse = y_year_mse[trn_idx]\n        \n        lgbm_model = LGBMRegressor(**lgbm_parameters)\n        lgbm_model_mse = LGBMRegressor(metric = 'mse')\n        \n        lgbm_model.fit(x_train_idx, y_train_idx, eval_set = ((x_valid_idx,y_valid_idx)),verbose = False, early_stopping_rounds = 100)\n        lgbm_model_mse.fit(x_train_idx, y_train_idx_mse, eval_set=((x_valid_idx, y_valid_idx_mse)), verbose=False, early_stopping_rounds=100)\n        \n        seasons, team_set1, team_set2 = test_year.values.transpose()\n        X_test = get_kenpom_data(seasons, team_set1, team_set2)['X']\n        \n        lgbm_test_pred += lgbm_model.predict(X_test) \/ SPLITS\n        lgbm_test_pred_mse = lgbm_model_mse.predict(X_test) \/ SPLITS\n        \n        y_pred = lgbm_model.predict(x_valid_idx)\n        y_preds.append(y_pred)\n        \n        y_pred_mse = lgbm_model_mse.predict(x_valid_idx)\n        y_pred_mse = np.where(y_pred_mse > 0, 1, 0)\n        y_pred = (19 * y_pred + y_pred_mse) \/ 20\n        logloss.append(log_loss(y_valid_idx, y_pred)) \n        losses_mse.append(mean_squared_error(y_valid_idx_mse, y_pred_mse))\n        \n    test_pred += lgbm_test_pred.tolist()\n    test_pred_mse += lgbm_test_pred_mse.tolist()\n    print('Year_Predict:',year,'Log_Loss:',np.mean(logloss))\n","adb02252":"model1_preds = test_pred","b4dc6428":"#Read Data\ntourney_result = pd.read_csv('..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2\/MNCAATourneyCompactResults.csv')\ntourney_seed = pd.read_csv('..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2\/MNCAATourneySeeds.csv')\nseason_result = pd.read_csv('..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2\/MRegularSeasonCompactResults.csv')\ntest_df = pd.read_csv('..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2\/MSampleSubmissionStage2.csv')\nsubmission_df = pd.read_csv('..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2\/MSampleSubmissionStage2.csv')","1a506838":"# deleting unnecessary columns\ntourney_result = tourney_result.drop(['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'], axis=1)\n# Merge Seed\ntourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Seed':'WSeed'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)\ntourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Seed':'LSeed'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)\n\ndef get_seed(x):\n    return int(x[1:3])\n\ntourney_result['WSeed'] = tourney_result['WSeed'].map(lambda x: get_seed(x))\ntourney_result['LSeed'] = tourney_result['LSeed'].map(lambda x: get_seed(x))\n# Merge Score\nseason_win_result = season_result[['Season', 'WTeamID', 'WScore']]\nseason_lose_result = season_result[['Season', 'LTeamID', 'LScore']]\nseason_win_result.rename(columns={'WTeamID':'TeamID', 'WScore':'Score'}, inplace=True)\nseason_lose_result.rename(columns={'LTeamID':'TeamID', 'LScore':'Score'}, inplace=True)\nseason_result = pd.concat((season_win_result, season_lose_result)).reset_index(drop=True)\nseason_score = season_result.groupby(['Season', 'TeamID'])['Score'].sum().reset_index()\ntourney_result = pd.merge(tourney_result, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Score':'WScoreT'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)\ntourney_result = pd.merge(tourney_result, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Score':'LScoreT'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)\ntourney_win_result = tourney_result.drop(['Season', 'WTeamID', 'LTeamID'], axis=1)\ntourney_win_result.rename(columns={'WSeed':'Seed1', 'LSeed':'Seed2', 'WScoreT':'ScoreT1', 'LScoreT':'ScoreT2'}, inplace=True)\ntourney_lose_result = tourney_win_result.copy()\ntourney_lose_result['Seed1'] = tourney_win_result['Seed2']\ntourney_lose_result['Seed2'] = tourney_win_result['Seed1']\ntourney_lose_result['ScoreT1'] = tourney_win_result['ScoreT2']\ntourney_lose_result['ScoreT2'] = tourney_win_result['ScoreT1']\ntourney_win_result['Seed_diff'] = tourney_win_result['Seed1'] - tourney_win_result['Seed2']\ntourney_win_result['ScoreT_diff'] = tourney_win_result['ScoreT1'] - tourney_win_result['ScoreT2']\ntourney_lose_result['Seed_diff'] = tourney_lose_result['Seed1'] - tourney_lose_result['Seed2']\ntourney_lose_result['ScoreT_diff'] = tourney_lose_result['ScoreT1'] - tourney_lose_result['ScoreT2']\ntourney_win_result['result'] = 1\ntourney_lose_result['result'] = 0\ntourney_result = pd.concat((tourney_win_result, tourney_lose_result)).reset_index(drop=True)\ntrain_df = tourney_result\n# Get Test\ntest_df['Season'] = test_df['ID'].map(lambda x: int(x[:4]))\ntest_df['WTeamID'] = test_df['ID'].map(lambda x: int(x[5:9]))\ntest_df['LTeamID'] = test_df['ID'].map(lambda x: int(x[10:14]))\ntest_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Seed':'Seed1'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)\ntest_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Seed':'Seed2'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)\ntest_df = pd.merge(test_df, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Score':'ScoreT1'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)\ntest_df = pd.merge(test_df, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Score':'ScoreT2'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)\ntest_df['Seed1'] = test_df['Seed1'].map(lambda x: get_seed(x))\ntest_df['Seed2'] = test_df['Seed2'].map(lambda x: get_seed(x))\ntest_df['Seed_diff'] = test_df['Seed1'] - test_df['Seed2']\ntest_df['ScoreT_diff'] = test_df['ScoreT1'] - test_df['ScoreT2']\ntest_df = test_df.drop(['ID', 'Pred', 'Season', 'WTeamID', 'LTeamID'], axis=1)","a99d8a9d":"train_df.shape, test_df.shape","bd2d6cc7":"test_df['result']=np.NaN\nclass Base_Model(object):\n    \n    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=False):\n        self.train_df = train_df\n        self.test_df = test_df\n        self.features = features\n        self.n_splits = n_splits\n        self.categoricals = categoricals\n        self.target = 'result'\n        self.cv = self.get_cv()\n        self.verbose = verbose\n        self.params = self.get_params()\n        self.y_pred, self.model = self.fit()\n        \n    def train_model(self, train_set, val_set):\n        raise NotImplementedError\n        \n    def get_cv(self):\n        cv = KFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n        return cv.split(self.train_df, self.train_df[self.target])\n    \n    def get_params(self):\n        raise NotImplementedError\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        raise NotImplementedError\n        \n    def convert_x(self, x):\n        return x\n        \n    def fit(self):\n        oof_pred = np.zeros((len(train_df), ))\n        y_pred = np.zeros((len(test_df), ))\n        for fold, (train_idx, val_idx) in enumerate(self.cv):\n            print('Fold:',fold+1)\n            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            model = self.train_model(train_set, val_set)\n            \n            conv_x_val = self.convert_x(x_val)\n            oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)\n            \n            x_test = self.convert_x(self.test_df[self.features])\n            y_pred += model.predict(x_test).reshape(y_pred.shape) \/ self.n_splits\n        return y_pred, model\n    \n    \nclass Lgb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        return lgb.train(self.params, train_set, 10000, valid_sets=[train_set, val_set])\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {'num_leaves': 400,\n                  'min_child_weight': 0.034,\n                  'feature_fraction': 0.379,\n                  'bagging_fraction': 0.418,\n                  'min_data_in_leaf': 106,\n                  'objective': 'binary',\n                  'max_depth': -1,\n                  'learning_rate': 0.0068,\n                  \"boosting_type\": \"gbdt\",\n                  \"bagging_seed\": 11,\n                  \"metric\": 'logloss',\n                  'reg_alpha': 0.3899,\n                  'reg_lambda': 0.648,\n                  'random_state': 47,\n            }\n        return params\n    \nclass Xgb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        return xgb.train(self.params, train_set, \n                         num_boost_round=5000, evals=[(train_set, 'train'), (val_set, 'val')], \n                         verbose_eval=verbosity, early_stopping_rounds=100)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = xgb.DMatrix(x_train, y_train)\n        val_set = xgb.DMatrix(x_val, y_val)\n        return train_set, val_set\n    \n    def convert_x(self, x):\n        return xgb.DMatrix(x)\n        \n    def get_params(self):\n        params = { 'colsample_bytree': 0.8,                 \n                   'learning_rate': 0.01,\n                   'max_depth': 3,\n                   'subsample': 1,\n                   'objective':'binary:logistic',\n                   'eval_metric':'logloss',\n                   'min_child_weight':3,\n                   'gamma':0.25,\n                   'n_estimators':5000}\n        return params\nclass Catb_Model(Base_Model):\n    \n    def train_model(self, train_df, test_df):\n        verbosity = 100 if self.verbose else 0\n        clf = CatBoostClassifier(**self.params)\n        clf.fit(train_df['X'], \n                train_df['y'], \n                eval_set=(test_df['X'], test_df['y']),\n                verbose=verbosity, \n                cat_features=self.categoricals)\n        return clf\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train, 'y': y_train}\n        val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {'loss_function': 'Logloss',\n                   'task_type': \"CPU\",\n                   'iterations': 5000,\n                   'od_type': \"Iter\",\n                    'depth': 3,\n                  'colsample_bylevel': 0.5, \n                   'early_stopping_rounds': 300,\n                    'l2_leaf_reg': 18,\n                   'random_seed': 42,\n                    'use_best_model': True\n                    }\n        return params\nfeatures = train_df.columns\nfeatures = [x for x in features if x not in ['result']]\nprint(features)\ncategoricals = []\n\n#cat_model = Catb_Model(train_df, test_df, features, categoricals=categoricals)\nlgb_model = Lgb_Model(train_df, test_df, features, categoricals=categoricals)\nxgb_model = Xgb_Model(train_df, test_df, features, categoricals=categoricals)","ae7e346c":"weights = {\n    'model_1': 2, \n    'lgb_model': 24, \n    'xgb_model': 8\n}","87a561a2":"X1, X2, X3 = np.array(model1_preds), lgb_model.y_pred, xgb_model.y_pred\nW1, W2, W3 = weights['model_1'], weights['lgb_model'], weights['xgb_model']\nfinal_preds = (W1 * X1 + W2 * X2 + W3 * X3) \/ (W1 + W2 + W3)","e860d935":"submission = pd.read_csv('..\/input\/ncaam-march-mania-2021\/MDataFiles_Stage2\/MSampleSubmissionStage2.csv')\nsubmission.Pred = final_preds\nsubmission.to_csv('submission.csv', index=False)","2ed6d966":"\nShoot me an email at - sarthak.bhatt314@gmail.com","a0838de3":"## Using Seeds","6861fe58":"# NCAAM Prediction\n___\n# If you fork, do leave an upvote!","8c193588":"## Using tourney data","ad043ca7":"### Merging 538 team ratings data","438fcad4":"### Using Kenpom Data","122e8159":"\n### Experiments\n1. Concating team1, team2 features gives a better score than using (team1_features - team2_features) \/ (team1_features + team2_features)\n2. Adding Seeds improves the score from ~0.50 to 0.49\n3. Using massey ranking boosted the score significantly\n4. Ensembling a lgbm model trained on mean squared error had a small improvement. \nAdding mse model acts as a polarizer and forces the model to make bolder bets in case of nearer certainity.\n5. Adding 538 ratings had a small improvement. It's not very useful because most of the data is missing.\n6. Training on dart had a very small improvement of 0.001\n7. Using moving averages of seasons instead of means gives 0.05 score improvement\n8. Adding FGM, Sqt did not have much effect on the score\n9. Trying to ensemble previous year's best solutions proved to be a disappointment.\n10. Neural networks do not work because we don't have enough data for deeplearning.\n11. Changing the hyperparameters of LGBM models has a huge (negative) effect on the score. \n12. Using (only) all the relevent features from TourneyDetailedResults.csv gave a score of 0.53\n13. Adding a team quality feature from @raddar did not improve the score.\n14. Men's basketball is more unpredictable than women's basketball. Betting on sure outcome is risky.\n15. Massey is as important as all the kenpom features combined\n16. Just using Kenpom + 538 gave 0.53 log loss. On using massey loss went from 0.53 to 0.49\n17. Using more n-estimators and lower learning rate improved the score","c9c172ff":"---\n# Give This Notebook An Upvote!","17fd8a3b":"## Model 2, 3: XGB, LGB Blend","6bb3b184":"### Using Masssey's Ordinals","03b971e7":"# Model 1, 2: LGBM Trained on Kenpom + 538 + Massey's Ordinals\n### Log Loss: 0.49\n---"}}