{"cell_type":{"76fa86b6":"code","484e5ec2":"code","d59040a6":"code","b123ac52":"code","b0acb022":"code","356e01f4":"code","871c2798":"code","c430d01e":"code","9e8fa286":"code","51f6bc48":"code","ae569d7a":"code","5f62a7eb":"code","4c7f306e":"code","d015c9de":"code","d49fe6b1":"code","387e1706":"code","5bcd5f42":"code","ea6f99aa":"code","be579304":"code","d40e6541":"code","af185108":"code","22fd6d92":"code","aa4e6f17":"code","d90f91de":"code","aab78000":"code","56e3197b":"code","f3d40d13":"markdown","e411ce25":"markdown","6eef437b":"markdown","7a1654c2":"markdown","65ca043f":"markdown","e35292df":"markdown","958313d3":"markdown","8cc38bab":"markdown"},"source":{"76fa86b6":"\n!pip install imutils\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.layers import Activation\nfrom keras.utils import plot_model\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.models import Sequential , Model , load_model\nfrom tensorflow.keras.models import load_model \n\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\n\nfrom tensorflow.keras.preprocessing.image import load_img , img_to_array\nfrom tensorflow.keras.utils import to_categorical\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport cv2\nfrom imutils import paths\nimport numpy as np\nimport os\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","484e5ec2":"#Dataset folder path\ndata = \"\/kaggle\/input\/face-mask-dataset\/data\"\n","d59040a6":"Img_Paths = list(paths.list_images(data))\n\n#Displaying sample image from dataset\nsample1 = Image.open(Img_Paths[1])\nplt.imshow(sample1)\nsample1\n","b123ac52":"print(type(Img_Paths[1]))","b0acb022":"sample2 = Image.open(Img_Paths[-1])\nplt.imshow(sample2)\nsample2","356e01f4":"#Initializing learning rate\nINIT_LR = 0.0001\nBATCH_SIZE = 32\nEPOCHS = 20\n\n#Getting all images and their labels in list\nprint(\"Loading images...\")\nImg_Paths = list(paths.list_images(data))\nimgs = []\nlabels = []\n\n#Looping over the image paths\nfor i in Img_Paths:\n    #Extracting the class label\n    label = i.split(os.path.sep)[-2]\n    \n    #Loading input image and processing it\n    img = load_img(i,target_size=(224,224)) #Resizning all images with 224 Width and 224 height\n    img = img_to_array(img) #Converting images to array\n    img = preprocess_input(img)\n    \n    #updating imgs and labels respectively\n    imgs.append(img)\n    labels.append(label)\n    \n    \n#Coverting imgs and labels to numpy array with float type\nimgs = np.array(imgs,dtype=\"float32\")\nlabels = np.array(labels)\nprint(\"...Done\")\n","871c2798":"#Performing one-hot encoding on labels\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)\nlabels = to_categorical(labels)\n\n#Splitting data into train and test\n\n(X_train , X_test , y_train , y_test) = train_test_split(imgs,labels,test_size=0.20,stratify=labels,random_state=42)\n\n\n#Constructing the generator for data augmentation\n\nimg_gen = ImageDataGenerator(rotation_range=40,\n                            zoom_range=0.20,\n                            width_shift_range=0.2,\n                            height_shift_range=0.2,\n                            shear_range=0.15,\n                            horizontal_flip=True,\n                            fill_mode=\"nearest\")\n\nprint(\"Train size: \",len(X_train),\"Test size: \",len(X_test))\n\n","c430d01e":"model = Sequential()\nmodel.add(Conv2D(512,(2,2),padding=\"same\",activation=\"relu\",input_shape=(224,224,3)))\nmodel.add(MaxPooling2D((2,2),strides=2))\n\nmodel.add(Conv2D(256,(2,2),padding=\"same\",activation=\"relu\"))\nmodel.add(MaxPooling2D((2,2),strides=2))\n\nmodel.add(Conv2D(128,(2,2),padding=\"same\",activation=\"relu\"))\nmodel.add(MaxPooling2D((2,2),strides=2))\n\nmodel.add(Conv2D(64,(2,2),padding=\"same\",activation=\"relu\"))\nmodel.add(MaxPooling2D((2,2),strides=2))\n\nmodel.add(Conv2D(32,(2,2),padding=\"same\",activation=\"relu\"))\nmodel.add(MaxPooling2D((2,2),strides=2))\n\n#model.add(BatchNormalization())\n\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(64,activation=\"relu\"))\nmodel.add(Dense(2,activation=\"softmax\"))\n\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()\n","9e8fa286":"start = time.time()\n\nhistory = model.fit(img_gen.flow(X_train,y_train,batch_size=5),\n                    steps_per_epoch=300,\n                    validation_data=(X_test,y_test),\n                    validation_steps=300,\n                    epochs=45)\nend = time.time()\nprint(\"Total train time: \",(end-start)\/60,\" mins\")\n","51f6bc48":"\ndef plot_graph(history,string):\n    plt.figure(figsize=(16,7))\n    plt.plot(history.history[string],label=str(string))\n    plt.plot(history.history[\"val_\"+str(string)],label=str(string))\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string,\"val_\"+string])\n    plt.show()\n\n\nplot_graph(history,\"accuracy\")\nplot_graph(history,\"loss\")","ae569d7a":"#Saving trained model\n#model.save(\"Face_Mask_Net_Cunstom_CNN.h5\")","5f62a7eb":"input_image = Img_Paths[0]\nimage1 = load_img(input_image,target_size=(224,224))\nimage2 = img_to_array(image1)\nimage2 = preprocess_input(image2)\nimage2 = np.array([image2],dtype=\"float32\")\ndetection = model.predict(image2)\nprint(detection)\nlabels_dict={0:'MASK',1:'NO MASK'}\nprint(labels_dict[np.argmax(detection)])\ninput_image = Image.open(input_image)\nplt.imshow(input_image)","4c7f306e":"#Loading MobileNetV2 architecture\nCLASSIFIER_URL =\"https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/classification\/2\"\nIMAGE_RES = 224 #Image size in pixels as MobileNet is trained on same img size\n\nmodel = Sequential()\nmodel.add(hub.KerasLayer(CLASSIFIER_URL,input_shape=(IMAGE_RES,IMAGE_RES,3)))\n\nmodel.add(Dense(2,activation=\"softmax\"))\n\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()\n\n","d015c9de":"start = time.time()\n\nhistory = model.fit(img_gen.flow(X_train,y_train,batch_size=5),\n                    steps_per_epoch=300,\n                    validation_data=(X_test,y_test),\n                    validation_steps=300,\n                    epochs=45)\nend = time.time()\nprint(\"Total train time: \",(end-start)\/60,\" mins\")\n","d49fe6b1":"plot_graph(history,\"accuracy\")\nplot_graph(history,\"loss\")\n","387e1706":"#model.save(\"Transfer_Learning_Model.h5\")\n","5bcd5f42":"input_image = Img_Paths[0]\nimage1 = load_img(input_image,target_size=(224,224))\nimage2 = img_to_array(image1)\nimage2 = preprocess_input(image2)\nimage2 = np.array([image2],dtype=\"float32\")\ndetection = model.predict(image2)\nprint(detection)\nlabels_dict={0:'MASK',1:'NO MASK'}\nprint(labels_dict[np.argmax(detection)])\ninput_image = Image.open(input_image)\nplt.imshow(input_image)","ea6f99aa":"# load the MobileNetV2 network, ensuring the head FC layer sets are\n# left off\nfrom tensorflow.keras.applications import MobileNetV2\nbaseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n\tinput_tensor=Input(shape=(224, 224, 3)))\n# construct the head of the model that will be placed on top of the\n# the base model\nheadModel = baseModel.output\nheadModel = AveragePooling2D(pool_size=(7, 7))(headModel)\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(128, activation=\"relu\")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(2, activation=\"softmax\")(headModel)\n# place the head FC model on top of the base model (this will become\n# the actual model we will train)\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n# loop over all layers in the base model and freeze them so they will\n# *not* be updated during the first training process\nfor layer in baseModel.layers:\n\tlayer.trainable = False\n    \n    ","be579304":"model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()\n","d40e6541":"start = time.time()\n\nhistory = model.fit(img_gen.flow(X_train,y_train,batch_size=5),\n                    steps_per_epoch=300,\n                    validation_data=(X_test,y_test),\n                    validation_steps=300,\n                    epochs=45)\nend = time.time()\nprint(\"Total train time: \",(end-start)\/60,\" mins\")\n","af185108":"#model.save(\"Transfer Learned.h5\")","22fd6d92":"input_image = Img_Paths[0]\nimage1 = load_img(input_image,target_size=(224,224))\nimage2 = img_to_array(image1)\nimage2 = preprocess_input(image2)\nimage2 = np.array([image2],dtype=\"float32\")\ndetection = model.predict(image2)\nprint(detection)\nlabels_dict={0:'MASK',1:'NO MASK'}\nprint(labels_dict[np.argmax(detection)])\ninput_image = Image.open(input_image)\nplt.imshow(input_image)","aa4e6f17":"plot_graph(history,\"accuracy\")\nplot_graph(history,\"loss\")\n","d90f91de":"maskNet = load_model(\"Face_Mask_Net_Cunstom_CNN.h5\")\nface_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\nlabels_dict={0:'MASK',1:'NO MASK'}\ncolor_dict={0:(0,255,0),1:(0,0,255)}\n\n\ndef Detector(image_file):\n    \n    input_image = image_file\n    input_image1 = cv2.imread(input_image)\n    \n    faces=face_clsfr.detectMultiScale(input_image1,1.1,1)  \n    \n    for (x,y,w,h) in faces:\n    \n        face_img=input_image1[y:y+w,x:x+w]\n        #resized=cv2.resize(face_img,(224,224))\n        #image1 = load_img(face_img,target_size=(224,224))\n        image1 = cv2.resize(face_img,(224,224))\n        image2 = img_to_array(image1)\n        image3 = preprocess_input(image2)\n        \n        image4 = np.array([image3],dtype=\"float32\")\n        result=maskNet.predict(image4)\n\n        label=np.argmax(result,axis=1)[0]\n      \n        cv2.rectangle(input_image1,(x,y),(x+w,y+h),color_dict[label],1)\n        cv2.rectangle(input_image1,(x,y-40),(x+w,y),color_dict[label],-1)\n        cv2.putText(input_image1, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1)\n    cv2.imshow(\"Detection\",input_image1)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n","aab78000":"Detector(str(image_file_path))","56e3197b":"maskNet = load_model(\"Face_Mask_Net_Cunstom_CNN.h5\")\nface_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\n\nsource=cv2.VideoCapture(0)\nlabels_dict={0:'MASK',1:'NO MASK'}\ncolor_dict={0:(0,255,0),1:(0,0,255)}\n\n\nwhile(True):\n\n    ret,img=source.read()\n    \n    faces=face_clsfr.detectMultiScale(img,1.2,2)  \n    \n    \n\n    for (x,y,w,h) in faces:\n    \n        face_img=img[y:y+w,x:x+w]\n        #resized=cv2.resize(face_img,(224,224))\n        #image1 = load_img(face_img,target_size=(224,224))\n        image1 = cv2.resize(face_img,(224,224))\n        image2 = img_to_array(image1)\n        image3 = preprocess_input(image2)\n        \n        image4 = np.array([image3],dtype=\"float32\")\n        result=maskNet.predict(image4)\n\n        label=np.argmax(result,axis=1)[0]\n      \n        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],-1)\n        cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n        \n        \n    cv2.imshow('Face Mask Detector',img)\n    key=cv2.waitKey(1)\n    \n    # if(key==27):\n    #     break\n    \n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n        \ncv2.destroyAllWindows()\nsource.release()\n","f3d40d13":"Below code is to load trained model and make predictions on single image or input from live webcam.","e411ce25":"**Face Mask Detection**\n\nIn recent trend in world wide Lockdowns due to COVID19 outbreak, as Face Mask is became mandatory for everyone while roaming outside, approach of Deep Learning for Detecting Faces With and Without mask were a good trendy practice. Here I have created a model that detects face mask trained on 7553 images with 3 color channels (RGB).\nOn Custom CNN architecture Model training accuracy reached 94% and Validation accuracy 96%.\n\nWith transfer learning using MobieNetV2 architecture training accuracy achieved 98% and validation accuracy 99%.\n\nAlgorithm : Convolutional Neural Network\n\nFramework : Tensorflow\n\nAccelerator : GPU\n\nDataset : 7553 RGB Images (With Mask\/Without Mask)\n\nDataset Credit:\n\nI am going to use my own data set of Total 7553 images.\n\nI have taken 1776 images including both With and Without Face Mask images from Prajna Bhandary's Github account\n\nhttps:\/\/github.com\/prajnasb\/observations\n\nRemaining 5777 images are collected and filtered from Google search engine.\n\n3725 Images of Face with Mask\n\n3828 Images of Face without Mask.","6eef437b":"Defining function to plot accuracy and loss of trained model which can be used for further use too.","7a1654c2":"I am going to use my own data set of Total 7553 images.\n\nData set credits:-> I have taken 1776 images including both With and Without Face Mask images from Prajna Bhandary's Github account\n\nhttps:\/\/github.com\/prajnasb\/observations\n\nRemaining 5777 images are collected and filtered from Google search engine.\n\n3725 Images of Face with Mask\n3828 Images of Face without Mask.","65ca043f":"This is another method for Transfer learning with same architecture.\n\nCredits:-> Adrian Rosebrock\n\nhttps:\/\/www.pyimagesearch.com\/2020\/05\/04\/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning\/","e35292df":"To run with webcam use below code.","958313d3":"Conducting experimental training I am first crating Custom CNN architecture.\n\nThen using transfer learning method with MobileNetV2 acrchitecture.","8cc38bab":"Now trying with simple transfer learning method with state of the art MobileNetV2 architecture.\n\nFind more details about MobileNetV2 \n\nhttps:\/\/analyticsindiamag.com\/why-googles-mobilenetv2-is-a-revolutionary-next-gen-on-device-computer-vision-network\/"}}