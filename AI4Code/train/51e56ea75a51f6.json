{"cell_type":{"a2cddda2":"code","6c22193a":"code","109313e9":"code","8cedc895":"code","39ec5792":"code","11673659":"code","cbcc50d0":"code","bcae3f25":"code","bf509192":"code","9e722fbf":"code","eb4fad39":"code","3fc8d049":"code","25e87920":"code","d086b7f5":"code","51fa7002":"code","8e6e2391":"code","9605e878":"code","20a3d657":"code","d952f5ba":"code","42653477":"code","320f658e":"code","268068d7":"code","7d67a715":"code","ed00425b":"code","d1057910":"code","b077f18f":"code","00c1cfc8":"code","99ede196":"code","5e366a1a":"code","be55613f":"code","7a7602a3":"code","b649e021":"code","50c16eb9":"markdown","e93d6bed":"markdown","d7d8f38f":"markdown","c54ba07f":"markdown","e805b9da":"markdown","1fb4098c":"markdown","62e7f12a":"markdown","644876d8":"markdown","400bde80":"markdown","fdb6d00e":"markdown","e7befc43":"markdown","6571ba67":"markdown","ed639ef0":"markdown","fe3446f5":"markdown","77ef789a":"markdown","d256408a":"markdown","67868530":"markdown","35660398":"markdown","bc65b03a":"markdown","c9945203":"markdown","51af65bf":"markdown","7bc2c091":"markdown","c30390cb":"markdown","77bb3d22":"markdown"},"source":{"a2cddda2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport pylab\nimport warnings\nwarnings.filterwarnings('ignore')\n","6c22193a":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","109313e9":"# preview train data\nprint(train.head())\nprint('\\n')\nprint(train.shape)","8cedc895":"def get_missing(df):\n    missing = train.isnull().sum().sort_values(ascending=False).reset_index().rename(columns={'index':'variable',0:'count'})\n    missing = missing[missing['count']>0]\n    missing['missing_ratio'] = np.round(missing['count']\/len(train),4)\n    return missing\nget_missing(train)","39ec5792":"# drop variable with too many missing values(>50%)\ntrain.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu'],axis=1,inplace=True)\nprint(train.shape)","11673659":"# missing data list & percentage of missing\nmissing = get_missing(train)","cbcc50d0":"missing_under10 = missing[missing['missing_ratio']<0.1]\nmissing_under10","bcae3f25":"# check dtypes of variables\ntrain[missing_under10['variable'].unique()].dtypes","bf509192":"train[train['BsmtExposure'].isna()==True][missing_under10['variable'].unique()].head()","9e722fbf":"train[train['GarageYrBlt'].isna()==True][missing_under10['variable'].unique()].head()","eb4fad39":"# deletion of missing data in garage data\ntrain = train.dropna(subset=['GarageYrBlt','BsmtExposure','BsmtFinType2'])","3fc8d049":"get_missing(train)","25e87920":"# mean imputation\ntrain2 = train.copy()\ntrain2['MasVnrArea'].fillna(train2['MasVnrArea'].mean(),inplace=True)","d086b7f5":"# create function that compares imputation result and original data\ndef compare_imputation(original,imputed, variable):\n    plt.subplot(1,2,1)\n    sns.distplot(original[variable])\n    plt.title('Distribution before imputation')\n    plt.subplot(1,2,2)\n    sns.distplot(imputed[variable],color='red')\n    plt.tight_layout()\n    plt.title('Distribution after imputation')\n    plt.show()\n    \n    print('##### Before mean imputation #####')\n    print('mean:' ,original[variable].mean())\n    print('variance: ',original[variable].var())\n    print('\\n')\n    print('##### After mean imputation #####')\n    print('mean: ',imputed[variable].mean())\n    print('variance: ',imputed[variable].var())\n\ncompare_imputation(train,train2,'MasVnrArea')","51fa7002":"train['MasVnrArea'].fillna(train['MasVnrArea'].mean(),inplace=True)","8e6e2391":"# drop missing data in Electrical & MasVnrType\ntrain = train.dropna(subset=['Electrical'])\ntrain = train.dropna(subset=['MasVnrType'])","9605e878":"get_missing(train)","20a3d657":"train_compare1 = train.copy()\ntrain_compare1['LotFrontage'].fillna(train_compare1['LotFrontage'].mean(),inplace=True)","d952f5ba":"compare_imputation(train,train_compare1,'LotFrontage')","42653477":"train_compare2 = train.copy()\ntrain_compare2['LotFrontage'].fillna(method='ffill',inplace=True)","320f658e":"compare_imputation(train,train_compare2,'LotFrontage')","268068d7":"# apply hotdeck imputation to train data\ntrain['LotFrontage'].fillna(method='ffill',inplace=True)\nget_missing(train)","7d67a715":"plt.figure(figsize=(8,6))\nsns.distplot(train['SalePrice'])\nplt.title('Distribution of SalePrice', fontsize=15)\nplt.show()","ed00425b":"stats.probplot(train['SalePrice'],dist='norm',plot=pylab)","d1057910":"corr_mat = train.corr()\nmask = np.triu(np.ones_like(corr_mat,dtype=np.bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_mat, mask=mask, vmin=-1, vmax=1, cmap=cmap, annot=True, fmt='.2f')","b077f18f":"corr_mat['SalePrice'].sort_values(ascending=False)[1:7]","00c1cfc8":"train['OverallQual'].value_counts().sort_index().plot.bar(color='powderblue')\nplt.title('Overall Quality');plt.xticks(rotation=0);plt.ylabel('Count');plt.xlabel('OverallQual')","99ede196":"sns.catplot(data=train,x='OverallQual',y='SalePrice')\nplt.title('OverallQual - SalePrice',fontsize=15)\nplt.show()","5e366a1a":"sns.jointplot(x=\"GrLivArea\", y=\"SalePrice\", data=train, kind='reg',joint_kws={'line_kws':{'color':'red'}},color='skyblue')\nplt.show()","be55613f":"sns.catplot(data=train,x='GarageCars',y='GarageArea')\nplt.title('GarageCars - GarageArea',fontsize=15)\nplt.show()","7a7602a3":"sns.jointplot(x=\"GarageArea\", y=\"SalePrice\", data=train, kind='reg',joint_kws={'line_kws':{'color':'red'}},color='skyblue')\nplt.show()","b649e021":"plt.subplot(1,2,1)\nsns.regplot(x=\"1stFlrSF\", y=\"SalePrice\", data=train,color='skyblue')\nplt.title('1stFlrSF - SalePrice',fontsize=15)\nplt.subplot(1,2,2)\nsns.regplot(x=\"TotalBsmtSF\", y=\"SalePrice\", data=train,color='lightgreen')\nplt.title('TotalBsmtSF - SalePrice',fontsize=15)\nplt.tight_layout()\nplt.show()","50c16eb9":"#### `1stFlrSF` and `TotalBsmtSF`: First Floor square feet \/ Total square feet of basement area","e93d6bed":"Next, conduct mean imputation to `MasVnrArea` variable and compare before and after imputation data.\nI made `train2` data, which is mean imputed data of train data just to compare results. \n<br>\n + Roughly, distribution of `MasVnrArea` before and after imputation shows no difference! To be more specific, mean of before and after mean imputation is same(since this is mean imputation). <br>\n + However, variance of mean imputed data was little bit smaller than that of original train data. This is because if we mean impute missing data, density of mean value increases and therefore leads to underestiamtion of variance. But, we don't need to worry in this case. Distribution plot of before\/after imputation are almost the same and difference in variance is about 20, which is negligable.","d7d8f38f":"#### `OverallQual`(categorical): Overall material and finish quality\n`OverallQual` variable consists of 9 categories. Larger value denotes that overall quality of house is better. To see relationship with `SalePrice`, I drew catplot using seaborn package. \n**Higher overall quality means higher sale price.**","c54ba07f":"## (3) Explanatory variables(\uc124\uba85\ubcc0\uc218)","e805b9da":"Let's preview data first. This house price data consists of 81 columns and 1460 rows. `SalePrice` variabls is our target variable and now in **EDA part**, we are going to get insight from each variables and discover how to deal with those values well.","1fb4098c":"### 2) Top 6 variables highly correlated with Target `SalePrice`\nWe are going to get Top 6 variable that are highly correlated with `SalePrice` and analyze those. Below is the list of those variables.","62e7f12a":"First, check the number of missing values in each variables of house price data. \n + There are 19 variables which have missing data, and we are going to delete variables with too many missing variables. I'll remove variables with missing percentage more than 40%.\n + Therefore, remove `PoolQC`, `MiscFeature`, `Alley`, `Fence`, `FireplaceQu` variables.\n + It's quite unsure wheter to delete `FireplaceQu` variable or not. We're going to decide how to deal with this variable later.","644876d8":"**1st method: Mean imputation** <br>\nIf we impute `LonFrontage` with mean imputation method, the result is quite different from above `MasVnrArea` case. First, distribution of before and after imputation of data is definetly different. After imputation distribution is much denser around mean value. <br>\nTherefore, **underestimation of variance occurs when we conduct mean imputation.**","400bde80":"### 1) Check correlation heatmap between variables\nDraw heatmap of correlation between explanatory variables. These are lists of variables with high correlation(more than 0.7).\n + `1stFlrSF`, `TotalBsmtSF`: 0.89\n + `GarageYrBlt`, `YearBlt`: 0.83\n + `TotRmsAbvGrd`, `GrLivArea`: 0.82\n + `GarageArea`, `GarageCars`: 0.83","fdb6d00e":"#### `GrLivArea`(integer):  Above grade (ground) living area square feet\nThis jointplot shows that `GrLivArea` and `SalePrice` is positvely correated. Therefore, `GrLivArea` is one of the important variables to predict `SalePrice`.","e7befc43":"#### `GarageCars` and `GarageArea`: Size of garage in car capacity\/ Size of garage in square feet\n`GarageCars` means size of garage in car capacity and `GarageArea` means size of garage in square feet. Those two variables both represents size of garage so they are quite similar variables! (Maybe we can delete one of those variables since there can be multicolinearity error.) <br>\nAnd we can check this in graphs belows. `GarageCars` is categorical variable with 4 categories(1-4) and except category 4, the larger category values are, the greater `GarageArea` is.","6571ba67":"`GarageYrBlt` is float type varialbe and it denotes year when garage was built. Therefore it's not adequate to mean impute missing values in `GarageYrBlt` variable, since year cannot be calculated with mean. Then how should we deal with those missing values? <br>\n\nLet's check missing values in `GarageYrBlt` and their relationship with other Garage related variables. It's interesting to check that if `GarageYrBlt` has Nan value, other Garage related variables also have Nan value. Same applis to `Bsmt` related variables. \n<br>\nWe can conclude that **it's more reasonable to delete those missing values in Garage & Bsmt related variables**, for below reasons.\n + missing percentage in garage variables are only 5% and 2% each.\n + it's impossible to impute **year** without any information.","ed639ef0":"First, delete missing data in `GarageYrBlt` variable. Then according to result below, missing values in `Garage` related variables are all deleted. ","fe3446f5":"If you take a loot at QQ-plot of `SalePrice`, you can easily see that values of `SalePrice` deviates from straight red line. Theoretically, if QQ-plot deviates from straight line, we say that this variable does not follow normal distribution.\n<br>\nSo `SalePrice` does not follow normal distribution.","77ef789a":"**2nd method: Hotdeck imputation** <br>\nHotdeck imputation is the imputation method which imputed missing data by random sampling complete data. In python, we can conduct hotdeck imputation using `ffill` or `bfill`. In this case, I used `ffil` method. <br>\nFollowing result is quite interesting. Unlike mean imputation method, there is very small bias in variance and mean bias is also small. Also, distribution of before and after imputation is very similar. <br>\n\nTherefore, it's better to apply **Hotdeck imputation** to `LotFrontage` variable.","d256408a":"#### 2) missing percentage over 10% \n: `LotFrontage`","67868530":"# Kaggle Competition: House Price Prediction\n\nHi! I am beginner of Kaggle and DataAnalysis and this is my first notebook work! Hope you guys enjoy and please leave comments if you have any questions or feedback.\n\n**Contents**\n + Part 1. EDA\n + Part 2. Feature Engineering\n + Part 3. Modeling","35660398":"# Part 1. Exploratory Data Analysis","bc65b03a":"### Missing data Imputation\nWe will try several imputation method to impute missing data. Since missing percentage of each variable is different, I'm going to apply different imputation method to each variable. For variables with very low percentage of missing data(under 10%), I'll apply mean imputation and for other variable, I'll apply regression\/hotdeck\/imputation cell method. <br>\n\n**Imputation method list**\n + mean imputation\n + hotdeck imputation","c9945203":"## (1) Deal with missing data(variables)\nBy previewing our data, we can easily see that there are lots of missing values in house price data. Therefore, we'll start with removing variables with too many missing data and find out how to impute those variables efficiently.","51af65bf":"This joinplot shows that `GarageArea` and `SalePrice` is positively correlated. Since `GarageCars` and `GarageArea` are positively correlated, so do `GarageCars` and `SalePrice`.","7bc2c091":"**Mean imputation & deletion of data**","c30390cb":"## (2) Explore Target variable: `SalePrice`\nDistribution of `SalePrice` is skewed. Therefore, we may need to apply `np.log1p` to conduct linear regression.","77bb3d22":"#### 1) missing percentage under 10% \nCaution) For variable with missing percentage under 10%, there are several object columns so it's hard to apply mean imputation. However, if you take a look at data description, for object columns NA means 'There is no garage\/basement'. So we are not going to impute object type variables. \n + `MasVnrArea`: mean imputation\n + `GarageYrBlt`: ????\n + `Electrical`, `MasVnrType`: there's only one\/eight missing data in this variable, so just delete it!"}}