{"cell_type":{"c101adce":"code","e5a56911":"code","380295ac":"code","ca5254bb":"code","be4e060e":"code","c5bcfbbb":"code","9f201557":"code","65ab93e2":"code","c56facd0":"code","7c614bf7":"code","b40a9480":"code","3883ef66":"code","e9c51643":"code","1681c0ee":"code","96aca2de":"code","4305e467":"code","9ccc4d46":"code","8c7b30ac":"code","4635f6bd":"code","5e02861e":"code","974765d4":"code","a476783a":"code","4557848f":"code","ee220b18":"code","b9eebe3c":"code","73f206d2":"code","dae052e4":"code","3a02ced3":"code","03e94fb9":"code","91b33679":"code","08741071":"code","144dd1ee":"code","56cb3bb3":"code","a9d16c5c":"markdown","5d2dd4a7":"markdown","475c47ad":"markdown","f4bed647":"markdown","7ab68601":"markdown","e00f6c2e":"markdown","534c2a24":"markdown"},"source":{"c101adce":"import torch\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom pylab import rcParams\nfrom matplotlib import rc\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pandas.plotting import register_matplotlib_converters\nfrom torch import nn, optim\nfrom sklearn.model_selection import train_test_split\nimport argparse\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nimport time\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12,8\nregister_matplotlib_converters()\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)","e5a56911":"merged_data = pd.read_csv('..\/input\/bearingdataset\/Averaged_BearingTest_Dataset.csv')","380295ac":"merged_data.index = merged_data[merged_data.columns[0]]\nmerged_data = merged_data.drop('Unnamed: 0', axis=1)\nmerged_data.index = pd.to_datetime(merged_data.index)","ca5254bb":"merged_data","be4e060e":"train = merged_data[:'2004-02-15 12:42:39']\ntest = merged_data['2004-02-15 12:52:39':]\nprint(train.shape, test.shape)","c5bcfbbb":"fig, ax = plt.subplots(figsize=(14,6), dpi = 80)\nax.plot(train['bearing_1'], label = 'bearing_1', color='blue', linewidth=1)\nax.plot(train['bearing_2'], label = 'bearing_2', color='red', linewidth=1)\nax.plot(train['bearing_3'], label = 'bearing_3', color='green', linewidth=1)\nax.plot(train['bearing_4'], label = 'bearing_4', color='black', linewidth=1)\nplt.legend(loc='lower left')\nax.set_title('Bearing Sensor Training Data',fontsize=16)\nplt.show();","9f201557":"fig, ax = plt.subplots(figsize=(14,6), dpi = 80)\nax.plot(test['bearing_1'], label = 'bearing_1', color='blue', linewidth=1)\nax.plot(test['bearing_2'], label = 'bearing_2', color='red', linewidth=1)\nax.plot(test['bearing_3'], label = 'bearing_3', color='green', linewidth=1)\nax.plot(test['bearing_4'], label = 'bearing_4', color='black', linewidth=1)\nplt.legend(loc='lower left')\nax.set_title('Bearing Sensor test Data',fontsize=16)\nplt.show();","65ab93e2":"# transforming data from the time domain to the frequency domain using fast Fourier transform\ntrain_fft = np.fft.fft(train)\ntest_fft = np.fft.fft(test)","c56facd0":"fig, ax = plt.subplots(figsize=(14,6), dpi = 80)\nax.plot(train_fft[:,0].real, label = 'bearing_1', color='blue', linewidth=1)\nax.plot(train_fft[:,1].imag, label = 'bearing_2', color='red', linewidth=1)\nax.plot(train_fft[:,2].real, label = 'bearing_3', color='green', linewidth=1)\nax.plot(train_fft[:,3].real, label = 'bearing_4', color='black',linewidth=1)\nplt.legend(loc='lower left')\nax.set_title('Bearing Sensor Training Data',fontsize=16)\nplt.show();","7c614bf7":"fig, ax = plt.subplots(figsize=(14,6), dpi = 80)\nax.plot(test_fft[:,0].real, label = 'bearing_1', color='blue', linewidth=1)\nax.plot(test_fft[:,1].imag, label = 'bearing_2', color='red', linewidth=1)\nax.plot(test_fft[:,2].real, label = 'bearing_3', color='green', linewidth=1)\nax.plot(test_fft[:,3].real, label = 'bearing_4', color='black', linewidth=1)\nplt.legend(loc='lower left')\nax.set_title('Bearing Sensor Training Data',fontsize=16)\nplt.show();","b40a9480":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(train)\nX_test = scaler.transform(test)","3883ef66":"X_train, X_val = train_test_split(X_train, test_size = 0.15, random_state=RANDOM_SEED)\nprint(X_train.shape, X_val.shape,X_test.shape)","e9c51643":"train_sequence  = X_train.astype(np.float32).tolist()\nval_sequence = X_val.astype(np.float32).tolist()\ntest_sequence = X_test.astype(np.float32).tolist()","1681c0ee":"def create_dataset(sequences):\n    dataset = [torch.tensor(s).unsqueeze(1) for s in sequences] \n \n    n_seq, seq_len, n_features = torch.stack(dataset).shape # n_seq,4,1\n    return dataset, seq_len, n_features ","96aca2de":"train_dataset, seq_len, n_features = create_dataset(train_sequence)\nval_dataset, _, _ = create_dataset(val_sequence)\ntest_dataset, _, _ = create_dataset(test_sequence)","4305e467":"class Encoder(nn.Module):\n    def __init__(self, seq_len, n_features):\n        super(Encoder, self).__init__()\n\n        self.seq_len = seq_len\n        self.n_features = n_features\n        self.hidden_dim = 2 * args.embedding_dim\n\n        self.rnn1 = nn.LSTM(\n          input_size=n_features,\n          hidden_size=self.hidden_dim,\n          num_layers=args.n_layers,\n          batch_first=True  # True = (batch_size, seq_len, n_features)\n                            # False = (seq_len, batch_size, n_features) \n                            #default = false\n        )\n        self.rnn2 = nn.LSTM(\n          input_size=self.hidden_dim,\n          hidden_size=args.embedding_dim,\n          num_layers=args.n_layers,\n          batch_first=True\n        )\n\n    def forward(self, x):\n        #(4,1)\n        x = x.reshape((args.batch_size, self.seq_len, self.n_features)) \n        # (batch, seq, feature)   #(1,4,1) \n        x, (_, _) = self.rnn1(x) #(1,4,256) \n        x, (hidden_n, _) = self.rnn2(x)\n        # x shape (1,4,128)\n        # hidden_n (1,1,128)\n        return hidden_n.reshape((self.n_features, args.embedding_dim)) #(1,128)\n\n","9ccc4d46":"class Decoder(nn.Module):\n    def __init__(self, seq_len, n_features):\n        super(Decoder, self).__init__()\n\n        self.seq_len = seq_len\n        self.hidden_dim = 2 * args.embedding_dim\n        self.n_features = n_features\n\n        self.rnn1 = nn.LSTM(\n        input_size=args.embedding_dim,\n        hidden_size=args.embedding_dim,\n        num_layers=args.n_layers,\n        batch_first=True\n        )\n        self.rnn2 = nn.LSTM(\n        input_size=args.embedding_dim,\n        hidden_size=self.hidden_dim,\n        num_layers=args.n_layers,\n        batch_first=True\n        )\n        self.output_layer = nn.Linear(self.hidden_dim, n_features)\n\n    def forward(self, x):\n        # x shape(1,128)\n        x = x.repeat(self.seq_len, args.batch_size) # (4, 128)\n\n        x = x.reshape((n_features, self.seq_len, args.embedding_dim)) # 1,4,128\n        x, (hidden_n, cell_n) = self.rnn1(x) #(1,4,128)\n        x, (hidden_n, cell_n) = self.rnn2(x) #(1,4,256)\n        x = x.reshape((self.seq_len, self.hidden_dim)) #(4,256)\n\n        return self.output_layer(x) # (4,1)","8c7b30ac":"class RecurrentAutoencoder(nn.Module):\n    def __init__(self, seq_len, n_features):\n        super(RecurrentAutoencoder, self).__init__()\n        self.seq_len = seq_len\n        self.n_features = n_features\n\n        self.encoder = Encoder(seq_len, n_features).to(args.device)\n        self.decoder = Decoder(seq_len, n_features).to(args.device)\n    def forward(self, x):\n        x = self.encoder(x)\n\n        x = self.decoder(x)\n        return x","4635f6bd":"def train_model( train_dataset, val_dataset, n_epochs):\n    model = RecurrentAutoencoder(seq_len, n_features)\n    model = model.to(args.device)\n    \n    model.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\n    criterion = nn.L1Loss(reduction='sum').to(args.device)\n\n    history = dict(train=[], val=[])\n\n    for epoch in range(1 , n_epochs + 1):\n        model = model.train()\n        ts = time.time()\n        train_losses = []\n\n        for seq_true in train_dataset:\n            optimizer.zero_grad()\n\n            seq_true = seq_true.to(args.device)\n            seq_pred = model(seq_true)\n    \n            loss = criterion(seq_pred, seq_true)\n            \n            loss.backward()\n            optimizer.step()\n\n            train_losses.append(loss.item())\n\n        val_losses = []\n        model = model.eval()\n        with torch.no_grad():\n            for seq_true in val_dataset:\n\n                seq_true = seq_true.to(args.device)\n                seq_pred = model(seq_true)\n\n                loss = criterion(seq_pred, seq_true)\n\n                val_losses.append(loss.item())\n        te = time.time()\n        train_loss = np.mean(train_losses)\n        val_loss = np.mean(val_losses)\n        history['train'].append(train_loss)\n        history['val'].append(val_loss)\n        \n        print(f\"Epoch: {epoch}  train loss: {train_loss}  val loss: {val_loss}  time: {te-ts} \")\n\n    return model.eval(), history    ","5e02861e":"seed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\nparser = argparse.ArgumentParser()\nargs = parser.parse_args('')\n\nargs.device = 'cuda' if torch.cuda.is_available else 'cpu'\n\n# ===== data loading ==== #\nargs.batch_size = 1\n\n# ==== model capacity ==== #\nargs.n_layers = 1\nargs.embedding_dim = 128\n\n\n# ==== regularization ==== #\nargs.dropout = 0\nargs.use_bn = False\n\n# ==== optimizer & training  # ====\nargs.lr = 0.001\nargs.epoch = 100\n\n\n\n# ==== Experiment Variable ==== #\nmodel, history = train_model(train_dataset, val_dataset, args.epoch)","974765d4":"plt.plot(history['train'])\nplt.plot(history['val'])\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.title('Loss over training epochs')\nplt.legend(['train','val'])\nplt.show();","a476783a":"MODEL_PATH = 'model.pth'\ntorch.save(model, MODEL_PATH)","4557848f":"model = torch.load('model.pth')\nmodel = model.to(args.device)","ee220b18":"model","b9eebe3c":"def predict(model, dataset):\n    predictions, losses = [], []\n    criterion = nn.L1Loss(reduction='sum').to(args.device)\n\n    with torch.no_grad():\n        model = model.eval()\n        for seq_true in dataset:\n            seq_true = seq_true.to(args.device)\n            seq_pred = model(seq_true)\n\n            loss = criterion(seq_pred, seq_true)\n\n            predictions.append(seq_pred.cpu().numpy().flatten())\n            losses.append(loss.item())\n    return predictions, losses","73f206d2":"_, losses = predict(model, train_dataset)","dae052e4":"sns.distplot(losses, bins=50, kde=True)","3a02ced3":"predictions, val_losses = predict(model, val_dataset)\nsns.distplot(val_losses, bins=50, kde= True)","03e94fb9":"predictions, pred_losses = predict(model, test_dataset)\nsns.distplot(pred_losses,bins=50, kde= True)","91b33679":"total_losses = losses + val_losses + pred_losses\nThreshold = 0.25","08741071":"score = pd.DataFrame(index = merged_data.index)\nscore['Loss'] = total_losses\nscore['Threshold'] = Threshold\nscore['Anomaly'] = score['Loss'] > score['Threshold']","144dd1ee":"score","56cb3bb3":"score.plot(logy=True, figsize=(16,9),ylim=[1e-2, 1e2], color=['blue','red'])","a9d16c5c":"## Threshold = 0.25","5d2dd4a7":"## Visualization train, test data","475c47ad":"## Experiment","f4bed647":"## Normalize the data","7ab68601":"## Modeling(Autoencoder LSTM)","e00f6c2e":"## Load Data","534c2a24":"## Prediction"}}