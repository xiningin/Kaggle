{"cell_type":{"ec7c92eb":"code","0e2bf64a":"code","4db0fe84":"code","581ff371":"code","9d87546d":"code","22b01a18":"code","8e081ba8":"code","8bd4b370":"code","ca8e37fb":"code","d6968169":"code","84d405e7":"code","3cf00fbb":"code","38c1cea7":"code","aa03250a":"code","b58deb80":"code","4eac4f2d":"code","b2043222":"code","23a03e34":"code","35a63dd3":"code","24d18d38":"code","2fd872c0":"code","baa5f523":"code","ef28268f":"code","e246cb9b":"code","f41d357e":"code","cb1622f2":"code","f1766e67":"code","088e8fd2":"code","c63dfcfc":"code","f01e6d47":"markdown","6756498b":"markdown","4189b775":"markdown","cc307f24":"markdown","e9406242":"markdown","956134bc":"markdown","55332615":"markdown","242d0f28":"markdown","9114b599":"markdown","9dd2b20d":"markdown","285e4355":"markdown","de0b84f0":"markdown"},"source":{"ec7c92eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0e2bf64a":"import os\n\nimport pandas as pd\ndf = pd.read_csv('\/kaggle\/input\/massive-stock-news-analysis-db-for-nlpbacktests\/raw_partner_headlines.csv')","4db0fe84":"df.head()","581ff371":"news = []\nfor i, j in df.iterrows():\n    news.append(j['headline'])\n    \nprint(len(news))","9d87546d":"news[:1]","22b01a18":"len(news)","8e081ba8":"news = news[:109233]","8bd4b370":"len(news)","ca8e37fb":"os.path.join('\/kaggle\/working', 'finance_news.txt')","d6968169":"f = open('\/kaggle\/working\/finance_news.txt', 'w')\nf.write('\\n'.join(news))\nf.close()","84d405e7":"import os\nimport pickle\nimport torch\n\n\nSPECIAL_WORDS = {'PADDING': '<PAD>'}\n\n\ndef load_data(path):\n    \"\"\"\n    Load Dataset from File\n    \"\"\"\n    input_file = os.path.join(path)\n    with open(input_file, \"r\") as f:\n        data = f.read()\n\n    return data\n\n\ndef preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n    \"\"\"\n    Preprocess Text Data\n    \"\"\"\n    text = load_data(dataset_path)\n    \n    # Ignore notice, since we don't use it for analysing the data\n    text = text[81:]\n\n    token_dict = token_lookup()\n    for key, token in token_dict.items():\n        text = text.replace(key, ' {} '.format(token))\n\n    text = text.lower()\n    text = text.split()\n\n    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n    int_text = [vocab_to_int[word] for word in text]\n    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n\n\ndef load_preprocess():\n    \"\"\"\n    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n    \"\"\"\n    return pickle.load(open('preprocess.p', mode='rb'))\n\n\ndef save_model(filename, decoder):\n    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n    torch.save(decoder, save_filename)\n\n\ndef load_model(filename):\n    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n    return torch.load(save_filename)","3cf00fbb":"data_dir = '\/kaggle\/working\/finance_news.txt'\ntext = load_data(data_dir)","38c1cea7":"view_line_range = (0, 10)\n\nimport numpy as np\n\nprint('Dataset Stats')\nprint('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n\nlines = text.split('\\n')\nprint('Number of lines: {}'.format(len(lines)))\nword_count_line = [len(line.split()) for line in lines]\nprint('Average number of words in each line: {}'.format(np.average(word_count_line)))\n\nprint()\nprint('The lines {} to {}:'.format(*view_line_range))\nprint('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))","aa03250a":"from collections import Counter\n\ndef create_lookup_tables(text):\n    \"\"\"\n    Create lookup tables for vocabulary\n    :param text: The text of tv scripts split into words\n    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n    \"\"\"\n    # TODO: Implement Function\n    word_count = Counter(text)\n    sorted_vocab = sorted(word_count, key = word_count.get, reverse=True)\n    int_to_vocab = {ii:word for ii, word in enumerate(sorted_vocab)}\n    vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n    \n    # return tuple\n    return (vocab_to_int, int_to_vocab)\n","b58deb80":"def token_lookup():\n    \"\"\"\n    Generate a dict to turn punctuation into a token.\n    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n    \"\"\"\n    # TODO: Implement Function\n    token = dict()\n    token['.'] = '<PERIOD>'\n    token[','] = '<COMMA>'\n    token['\"'] = 'QUOTATION_MARK'\n    token[';'] = 'SEMICOLON'\n    token['!'] = 'EXCLAIMATION_MARK'\n    token['?'] = 'QUESTION_MARK'\n    token['('] = 'LEFT_PAREN'\n    token[')'] = 'RIGHT_PAREN'\n    token['-'] = 'QUESTION_MARK'\n    token['\\n'] = 'NEW_LINE'\n    return token\n","4eac4f2d":"preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)","b2043222":"int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()","23a03e34":"train_on_gpu = torch.cuda.is_available()","35a63dd3":"from torch.utils.data import TensorDataset, DataLoader\nimport torch\nimport numpy as np\n\n\ndef batch_data(words, sequence_length, batch_size):\n    \"\"\"\n    Batch the neural network data using DataLoader\n    :param words: The word ids of the TV scripts\n    :param sequence_length: The sequence length of each batch\n    :param batch_size: The size of each batch; the number of sequences in a batch\n    :return: DataLoader with batched data\n    \"\"\"\n    # TODO: Implement function\n    n_batches = len(words)\/\/batch_size\n    x, y = [], []\n    words = words[:n_batches*batch_size]\n    \n    for ii in range(0, len(words)-sequence_length):\n        i_end = ii+sequence_length        \n        batch_x = words[ii:ii+sequence_length]\n        x.append(batch_x)\n        batch_y = words[i_end]\n        y.append(batch_y)\n    \n    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n    data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n        \n    \n    # return a dataloader\n    return data_loader\n","24d18d38":"# test dataloader\n\ntest_text = range(50)\nt_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n\ndata_iter = iter(t_loader)\nsample_x, sample_y = data_iter.next()\n\nprint(sample_x.shape)\nprint(sample_x)\nprint()\nprint(sample_y.shape)\nprint(sample_y)","2fd872c0":"import torch.nn as nn\n\nclass RNN(nn.Module):\n    \n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n        \"\"\"\n        Initialize the PyTorch RNN Module\n        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n        :param output_size: The number of output dimensions of the neural network\n        :param embedding_dim: The size of embeddings, should you choose to use them        \n        :param hidden_dim: The size of the hidden layer outputs\n        :param dropout: dropout to add in between LSTM\/GRU layers\n        \"\"\"\n        super(RNN, self).__init__()\n        # TODO: Implement function\n        \n        # define embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        # define lstm layer\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n        \n        \n        # set class variables\n        self.vocab_size = vocab_size\n        self.output_size = output_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        \n        # define model layers\n        self.fc = nn.Linear(hidden_dim, output_size)\n    \n    \n    def forward(self, x, hidden):\n        \"\"\"\n        Forward propagation of the neural network\n        :param nn_input: The input to the neural network\n        :param hidden: The hidden state        \n        :return: Two Tensors, the output of the neural network and the latest hidden state\n        \"\"\"\n        # TODO: Implement function   \n        batch_size = x.size(0)\n        x=x.long()\n        \n        # embedding and lstm_out \n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        \n        # stack up lstm layers\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout, fc layer and final sigmoid layer\n        out = self.fc(lstm_out)\n        \n        # reshaping out layer to batch_size * seq_length * output_size\n        out = out.view(batch_size, -1, self.output_size)\n        \n        # return last batch\n        out = out[:, -1]\n\n        # return one batch of output word scores and the hidden state\n        return out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        '''\n        Initialize the hidden state of an LSTM\/GRU\n        :param batch_size: The batch_size of the hidden state\n        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n        '''\n        # create 2 new zero tensors of size n_layers * batch_size * hidden_dim\n        weights = next(self.parameters()).data\n        if(train_on_gpu):\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(), \n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        # initialize hidden state with zero weights, and move to GPU if available\n        \n        return hidden","baa5f523":"def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n    \"\"\"\n    Forward and backward propagation on the neural network\n    :param decoder: The PyTorch Module that holds the neural network\n    :param decoder_optimizer: The PyTorch optimizer for the neural network\n    :param criterion: The PyTorch loss function\n    :param inp: A batch of input to the neural network\n    :param target: The target output for the batch of input\n    :return: The loss and the latest hidden state Tensor\n    \"\"\"\n    \n    # TODO: Implement Function\n    \n    # move data to GPU, if available\n    if(train_on_gpu):\n        rnn.cuda()\n    \n    # creating variables for hidden state to prevent back-propagation\n    # of historical states \n    h = tuple([each.data for each in hidden])\n    \n    rnn.zero_grad()\n    # move inputs, targets to GPU \n    inputs, targets = inp.cuda(), target.cuda()\n    \n    output, h = rnn(inputs, h)\n    \n    loss = criterion(output, targets)\n    \n    # perform backpropagation and optimization\n    loss.backward()\n    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n    optimizer.step()\n\n    # return the loss over a batch and the hidden state produced by our model\n    return loss.item(), h\n","ef28268f":"def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n    batch_losses = []\n    \n    rnn.train()\n\n    print(\"Training for %d epoch(s)...\" % n_epochs)\n    for epoch_i in range(1, n_epochs + 1):\n        \n        # initialize hidden state\n        hidden = rnn.init_hidden(batch_size)\n        \n        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n            \n            # make sure you iterate over completely full batches, only\n            n_batches = len(train_loader.dataset)\/\/batch_size\n            if(batch_i > n_batches):\n                break\n            \n            # forward, back prop\n            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n            # record loss\n            batch_losses.append(loss)\n\n            # printing loss stats\n            if batch_i % show_every_n_batches == 0:\n                print('Epoch: {:>4}\/{:<4}  Loss: {}\\n'.format(\n                    epoch_i, n_epochs, np.average(batch_losses)))\n                batch_losses = []\n\n    # returns a trained rnn\n    return rnn","e246cb9b":"# Data params\n# Sequence Length\nsequence_length = 10  # of words in a sequence\n# Batch Size\nbatch_size = 128\n\n# data loader - do not change\ntrain_loader = batch_data(int_text, sequence_length, batch_size)","f41d357e":"# Training parameters\n# Number of Epochs\nnum_epochs = 10\n# Learning Rate\nlearning_rate = 0.001\n\n# Model parameters\n# Vocab size\nvocab_size = len(vocab_to_int)\n# Output size\noutput_size = vocab_size\n# Embedding Dimension\nembedding_dim = 200\n# Hidden Dimension\nhidden_dim = 250\n# Number of RNN Layers\nn_layers = 2\n\n# Show stats for every n number of batches\nshow_every_n_batches = 500","cb1622f2":"# create model and move to gpu if available\nrnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\nif train_on_gpu:\n    rnn.cuda()\n\n# defining loss and optimization functions for training\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\n# training the model\ntrained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n\n# saving the trained model\nsave_model('.\/save\/trained_rnn', trained_rnn)\nprint('Model Trained and Saved')","f1766e67":"\"\"\"\nDON'T MODIFY ANYTHING IN THIS CELL\n\"\"\"\nimport torch\n\n_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\ntrained_rnn = load_model('.\/save\/trained_rnn')","088e8fd2":"import torch.nn.functional as F\n\ndef generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n    \"\"\"\n    Generate text using the neural network\n    :param decoder: The PyTorch Module that holds the trained neural network\n    :param prime_id: The word id to start the first prediction\n    :param int_to_vocab: Dict of word id keys to word values\n    :param token_dict: Dict of puncuation tokens keys to puncuation values\n    :param pad_value: The value used to pad a sequence\n    :param predict_len: The length of text to generate\n    :return: The generated text\n    \"\"\"\n    rnn.eval()\n    \n    # create a sequence (batch_size=1) with the prime_id\n    current_seq = np.full((1, sequence_length), pad_value)\n    current_seq[-1][-1] = prime_id\n    predicted = [int_to_vocab[prime_id]]\n    \n    for _ in range(predict_len):\n        if train_on_gpu:\n            current_seq = torch.LongTensor(current_seq).cuda()\n        else:\n            current_seq = torch.LongTensor(current_seq)\n        \n        # initialize the hidden state\n        hidden = rnn.init_hidden(current_seq.size(0))\n        \n        # get the output of the rnn\n        output, _ = rnn(current_seq, hidden)\n        \n        # get the next word probabilities\n        p = F.softmax(output, dim=1).data\n        if(train_on_gpu):\n            p = p.cpu() # move to cpu\n         \n        # use top_k sampling to get the index of the next word\n        top_k = 5\n        p, top_i = p.topk(top_k)\n        top_i = top_i.numpy().squeeze()\n        \n        # select the likely next word index with some element of randomness\n        p = p.numpy().squeeze()\n        word_i = np.random.choice(top_i, p=p\/p.sum())\n        \n        # retrieve that word from the dictionary\n        word = int_to_vocab[word_i]\n        predicted.append(word)     \n        \n        # the generated word becomes the next \"current sequence\" and the cycle can continue\n        current_seq = np.roll(current_seq.cpu(), -1, 1)\n        current_seq[-1][-1] = word_i\n    \n    gen_sentences = ' '.join(predicted)\n    \n    # Replace punctuation tokens\n    for key, token in token_dict.items():\n        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n    gen_sentences = gen_sentences.replace('( ', '(')\n    \n    # return all the sentences\n    return gen_sentences\n","c63dfcfc":"gen_length = 50 # modify the length to your preference\nprime_words = ['tesla'] # name for starting the script\n\n\"\"\"\nDON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n\"\"\"\nfor prime_word in prime_words:\n    pad_word = SPECIAL_WORDS['PADDING']\n    generated_script = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n    print(generated_script)","f01e6d47":"# RNNs and LSTM for Text Generation\n\n\n## Drawbacks of one-hot encoding \n\nConsidering an example of an excert from a book containing large collection of dataset and when you use these words as an input to RNN, we can one-hot encode them, but this would mean that we will end up having giant vector with mostly zeros except that one entry as shown below:<br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/One%20hot%20encoded%20vectors.png?raw=1\" width=\"500\"><\/img>\n\nThen we pass this one-hot encoded vector into hidden-layer of RNN and the result is a huge matrix of values most of which are zeros because of the initial one-hot encoding and this is really *computaionally inefficient*.<br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/Computationally%20in-efficient.png?raw=1\" width=\"500\"><\/img>\n\nThis is where *Embeddings* come into picture.\n\n\n## Word Embeddings\n\nWord embeddings is a general technique of reducing the dimensionality of text data, but the embedding models can also learn some interesting traits about words in a vocabulary.<br>\n\nEmbeddings can improve the ability of neural networks to learn from text data by representing them as *lower dimensional vectors.*\n\nThe idea here is when we multiply one-hot encoded vector with weight-matrix, returns only the row of the matrix that corresponds to the 1 or the on input unit.<br><br>\n\nHence, instead of doing matrix multiplication, we use weight-matrix as a look-up table and instead of representing words as one-hot vectors, we encode each word with a unique integer.\n\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/Embedding%20Lookup.png?raw=1\" width=\"500\"><\/img>\n\n","6756498b":"# Back Propogation Through Time (BPTT)\n\nLets look at the timestep t=3, the error associated w.r.t Wx depends on : vector S3 and its predecessor S2 and S1.<br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/BPTT.png?raw=1\" width=\"600\"><\/img><br>\n\nLooking at the pattern above while calculating the *accumulative gradient*, we can generalize the formula for Back Propogation Through Time (BPTT)as follows - <br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/General%20formula%20for%20BPTT.png?raw=1\" width=\"300\"><\/img><br>\n\n","4189b775":"# Long Short Term Memory Cells (LSTM Cells)\n\n## Basics of LSTM\n\nBasic RNN was unable to retain long term memory to make prediction regarding the current picture is that od a wolf or dog. This is where LSTM comes into picture. The LSTM cell allows a recurrent system to learn over many time steps without the fear of losing information due to the vanishing gradient problem. It is fully differentiable, therefore gives us the option of easily using backpropagation when updating the weights. Below is the a sample mathematical model of an LSTM cell - <br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/01.lstm_cell.png?raw=1\" width=\"300\"><\/img><br>\n\n\nIn an LSTM, we would expect the following behaviour -\n\n\n| Expected Behaviour of LSTM                                                                   | Reference Diagram                                                       |\n|----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|\n| 1. Long Term Memory (LTM) and Short Term Memory (STM) to combine and produce correct output. | <img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/05.%20lstm_basics_1.png?raw=1\" width=\"300\"> |\n| 2. LTM and STM and event should update the new LTM.                                          | <\/img>  <img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/06.%20lstm_basics_2.png?raw=1\" width=\"300\"><\/img>  |\n| 3. LTM and STM and event should update the new STM.                                          | <img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/07.%20lstm_basics_3.png?raw=1\" width=\"300\"><\/img>          |\n\n\n\n## How LSTMs work?\n\n| LSTM consists of 4 types of gates -  <br>1. Forget Gate<br>  2. Learn Gate<br> 3. Remember Gate<br> 4. Use Gate<br> | <img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/10.%20lstm_architecture_02.png?raw=1\" width=\"530px\" height=\"250px\"><\/img> |\n|-------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|\n\n### LSTM Explained\nAssume the following - \n1. LTM = Elephant\n2. STM = Fish\n3. Event = Wolf\/Dog\n\n| LSTM Operations                                                                                                                                                                                            | Reference Video                                      |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|\n| **LSTM places LTM, STM and Event as follows -**<br> 1. Forget Gate = LTM<br>  2. Learn Gate = STM + Event<br> 3. Remember Gate = LTM + STM + Event<br> 4. Use Gate = LTM + STM + Event<br> 5. In the end, LTM and STM are updated.<br> | <img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/Animated%20GIF-downsized_large.gif?raw=1\"><\/img> |\n\n\n## General Architecture of LSTM \n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/LSTM%20Architecture.png?raw=1\" width=\"400\"><img>\n\n\n\n\n## Learn Gate\nLearn gate takes into account **short-term memory and event** and then ignores a part of it and retains only a part of information.<br>\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/11.%20learn_gate.png?raw=1\" height=\"200px\" width=\"500px\"><\/img>\n\n### Mathematically Explained\nSTM and Event are combined together through **activation function** (tanh), which we further multiply it by a **ignore factor** as follows -<br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/12.lean_gate_equation.png?raw=1\" height=\"200px\" width=\"500px\"><\/img>\n\n## Forget Gate\nForget gate takes into account the LTM and decides which part of it to keep and which part of LTM is useless and forgets it. LTM gets multiplied by a **forget factor** inroder to forget useless parts of LTM. <br>\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/13.%20forget_gate.png?raw=1\" height=\"200px\" width=\"500px\"><\/img>\n\n## Remember Gate\nRemember gate takes LTM coming from Forget gate and STM coming from Learn gate and combines them together. Mathematically, remember gate adds LTM and STM.<br><br>\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/14.%20remember_gate.png?raw=1\" height=\"200px\" width=\"400px\"><\/img> <img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/15.%20remember_gate_equation.png?raw=1\" height=\"200px\" width=\"450px\"><\/img>\n\n## Use Gate\nUse gate takes what is useful from LTM and what's useful from STM and generates a new LTM.<br><br>\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/16.%20use_gate.png?raw=1\" height=\"200px\" width=\"400px\"><\/img> <img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/17.%20use_gate_equation.png?raw=1\" height=\"200px\" width=\"450px\"><\/img>\n\n\n\n\n\n","cc307f24":"# Drawbacks of RNNs\n\n## Vanishing Gradient Problem\n\nIn RNNs, if we continue to back-propogate further after 8-9 time steps, the contributions of information (graident) keeps on decreading geometrically over time which is known as the *vanishing gradient problem.* Here is where the **LSTM** comes into picture.<br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/LSTM%20Intro.png?raw=1\" width=\"600\"><\/img>\n\n## Exploding Gradient Problem\n\nIn RNNs we can also have the opposite problem, called the *exploding gradient* problem, in which the value of the gradient grows uncontrollably. A simple solution for the exploding gradient problem is **Gradient Clipping.**\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/Gradient%20Clipping.png?raw=1\" width=\"500\"><\/img>\n","e9406242":"# Batching Data\n\n We'll use `TensorDataset` to provide a known format to our dataset; in combination with DataLoader, it will handle batching, shuffling, and other dataset iteration functions.<br>\nWe can create data with TensorDataset by passing in feature and target tensors. Then create a DataLoader as usual.\n\n```python\ndata = TensorDataset(feature_tensors, target_tensors)\ndata_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n```\n\nFor example, say we have these as input:<br>\n```\nwords = [1, 2, 3, 4, 5, 6, 7]\nsequence_length = 4\n```\nOur first feature_tensor should contain the values:<br>\n```\n[1, 2, 3, 4]\n```\nAnd the corresponding target_tensor should just be the next \"word\"\/tokenized word value:<br>\n```\n5\n```\nThis should continue with the second feature_tensor, target_tensor being:<br>\n```\n[2, 3, 4, 5]  # features\n6             # target\n```","956134bc":"# Talking Points Model\n\n## Genral Architecture\n\n### Embedding Layer\n\nThe model should take our word tokens and firstly pass it through our embedding layer. This layer will be responsible for converting out word tokens or integers into embeddings of specific size. These word embeddings are then fed to the next layer of LSTM cells. <br>\n\nThe main purpose of using embedding layer is dimensionality reduction.\n\n### Contiguous LSTM Layer\n\nOur LSTM layer is defined by *hidden state size and number of layers*. At each step, an LSTM cell will produce an output and a new hidden state. The hidden state will be passed to next cell as input (memory representation.)\n\n### Final Fully Connected Linear Layer\n\nThe output generated by LSTM cell will be then fed into a *Sigmoid activated fully-connected linear layer.* This layer is responsible for mapping LSTM output to desired output size.\n\nThe output of the sigmoid function will be the probability distribution of most likely next word.<br><br>\n\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/lstm_rnn_2.png?raw=1\" height=\"500\"><\/img>\n","55332615":"# Pre-processing Stock News \n\nThe following section pre-processes our text file so that -\n1. Any punctuation are converted into tokens, so a period is changed to a bracketed period.\n2. In this data set, there aren't any periods, but it will help in other NLP problems.\n3. It removes all words that show up five or fewer times in the dataset.This will greatly reduce issues due to noise in the data and improve the quality of the vector representations.\n4. It returns a list of words in the text.","242d0f28":"# Look-up Tables\n\nConsidering the example of \"heart\" mentioned above, we see that \"heart\" is encoded as the integer \"958\", we can look-up the embedding vector for this word in the 958th row of the embedding weight matrix. This is called a *look-up table*\n\n## Dimensions of Look-up table\n\nIf we have a vocabulary of 10k words, then we will have a 10k row embedded weight matrix. The width of the table is called *embedding dimensions*.\n\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/embedding_lookup_table.png?raw=1\" width=\"500\"><\/img>\n","9114b599":"## Future Work\n\nThere are few things which I would like to work on to improvise the model's performance-\n\n1. Use of bidirectional LSTM\n2. Pre-trained word embeddings such as GloVe or FastText\n3. Larger dataset that focuses on impact of corona pandemic on stocks.","9dd2b20d":"# Recurrent Neural Network\n\n## References\n\n1. [Udacity's Deep Learning Nanodegree](https:\/\/classroom.udacity.com\/nanodegrees\/nd101-ent\/syllabus\/core-curriculum) \n2. [Machine Talk](https:\/\/machinetalk.org\/2019\/02\/08\/text-generation-with-pytorch\/)\n3. [KD Nuggets tutorial on text generation via LSTM](https:\/\/www.kdnuggets.com\/2020\/07\/pytorch-lstm-text-generation-tutorial.html)\n4. [Pytorch official documentation](https:\/\/pytorch.org\/tutorials\/intermediate\/char_rnn_generation_tutorial.html)\n\nSome applications of deep-learning involve temporal-dependencies i.e. dependencies over time i.e. not just on current input but also on past inputs. RNNs are similar to feed-forward networks but in addition to *memory*.<br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/RNNs%20-%20Temporal%20Dependencies.png?raw=1\" width=\"250\" height=\"40%\"><\/img>\n\nIn RNNs, the current output *y* depends not only on current input *x*, but also on memory element *s*, that takes into account past inputs. \n\nRNNs also attempt to address the need of capturing information in previous inputs by maintaining internal memory elements called *States.*<br><br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/RNNs-%20States.png?raw=1\" width=\"300\"><\/img>\n\n## Applications of RNNs\n\n1. Some of the applications of RNN requires predicting the next word in the sentence which requires looking at *last few words instead of the current one.*\n\n2. Sentiment Analysis\n3. Speech Recognition\n4. Time Series Prediction\n5. NLP\n6. Gesture Recognition\n\n## Structure of RNNs\nBelow are the folded and unfolded sructure of RNNs - <br>\n\n| Folded RNN                                                    | Un-folded RNN                                               |\n|---------------------------------------------------------------|-------------------------------------------------------------|\n| <img  src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/RNN-%20Folded%20Model.png?raw=1\" width=\"300\"><\/img> | <img  src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/RNNs%20-%20Unfolded.png?raw=1\" width=\"300\"><\/img> |\n\n","285e4355":"# Vocab2int & Int2vocab\n\nHere we are creating 2 dictionaries to convert words to integers (`vocab_to_int`) and integers to vocab (`int_to_vocab`). The integers are assigned in descending order of the frequency, so the most frequent word, \"the\",  is given the integer \"0\" and the next most frequent word is given \"1\" and so on.","de0b84f0":"# Word2Vec Models\n\nWord2Vec model provides much efficient representations by finding vectors that represents words.<br>\n\nThere are 2 architectures for implementing Word2Vec -\n1. CBOW (Continous Bag Of Words)\n2. Skip-gram\n\n\n\n<img src=\"https:\/\/github.com\/purvasingh96\/Talking-points-global-hackathon\/blob\/master\/assets\/word2vec_architectures.png?raw=1\" width=\"500\"><\/img>\n\n\nWe have implemened *Talking Points* using the *Skip-gram* model."}}