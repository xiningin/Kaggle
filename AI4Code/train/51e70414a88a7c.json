{"cell_type":{"6304c9bc":"code","0ec6dbdc":"code","a8d637a7":"code","ef946688":"code","ada61e91":"code","01a93d89":"code","c993c3d4":"code","775883d7":"code","0fa53db9":"code","6312a51e":"code","7b6287c8":"code","167c074d":"code","06491288":"code","517c0283":"code","296af502":"code","2239bf40":"code","f6215bd8":"code","067f3bbf":"code","639cf8b7":"code","939a92f9":"code","17d96ce9":"code","fcb5c641":"code","c32778b5":"code","577fe08a":"code","60264c43":"code","4081396f":"code","6120e35e":"code","e3850838":"code","d78be4ac":"code","2970febd":"code","771dfc07":"code","4520eff2":"code","fc6853a2":"code","e50dd36c":"code","87d71e21":"code","31993832":"code","d1a87f93":"code","65400e0d":"code","faafd21b":"code","89fd5560":"code","4d72de28":"code","1b2d0be1":"code","13dd0635":"code","42d88821":"code","5cd644de":"code","f97b7d08":"code","05af3fb2":"code","2eea4e46":"code","f8974176":"code","30139a3a":"markdown","7fb79add":"markdown","917d7e75":"markdown","499f3210":"markdown","28c00bd6":"markdown","f4cad22c":"markdown","8a24617a":"markdown","0350116e":"markdown","7bb4db55":"markdown","ff9be539":"markdown","bbbdecad":"markdown","4bcd3f59":"markdown","08810641":"markdown","27de780d":"markdown","85563ecd":"markdown"},"source":{"6304c9bc":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","0ec6dbdc":"import nltk\nimport spacy\nimport re,string,unicodedata\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem import LancasterStemmer,WordNetLemmatizer","a8d637a7":"from sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom textblob import TextBlob\nfrom textblob import Word\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom bs4 import BeautifulSoup","ef946688":"movie = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv') # dataset from kaggle","ada61e91":"movie.head()","01a93d89":"movie.tail()","c993c3d4":"movie.isnull().any()  # there is no any null values","775883d7":"movie[\"sentiment\"].count()","0fa53db9":"movie.groupby('sentiment').count()","6312a51e":"movie.shape","7b6287c8":"movie.info()","167c074d":"movie.describe()","06491288":"nltk.download('stopwords')","517c0283":"token_ = ToktokTokenizer()","296af502":"stopwords = nltk.corpus.stopwords.words('english')","2239bf40":"stopwords","f6215bd8":"# Noisy text removing\ndef noiseremoval_text(text):\n    soup = BeautifulSoup(text,\"html.parser\")\n    text = soup.get_text()\n    text = re.sub('\\[[^]]*\\]','',text)\n    return text","067f3bbf":"movie['review'] = movie['review'].apply(noiseremoval_text)","639cf8b7":"# text semming \ndef stemmer(text):\n    \n    ps = nltk.porter.PorterStemmer()\n    text = ' '.join([ps.stem(word) for word in text.split()])\n    return text","939a92f9":"# apply function on review column\nmovie['review'] = movie['review'].apply(stemmer)","17d96ce9":"movie.head()","fcb5c641":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize","c32778b5":"# set stopwords to english\nstop_word = set(stopwords.words('english'))\nprint(stop_word)","577fe08a":"# removing the stopwords\ndef removing_stopwords(text, is_lower_case = False):\n    # Tokenization of text\n    tokenizers = ToktokTokenizer()\n    tokens = tokenizers.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stop_word]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stop_word]\n    filtered_text = ' '.join(filtered_tokens)\n    return filtered_text","60264c43":"movie['review'] = movie['review'].apply(removing_stopwords)","4081396f":"movie.head()","6120e35e":"train_reviews_movie = movie.review[:30000] # for train dataset","e3850838":"test_review_movie = movie.review[30000:] # for test dataset","d78be4ac":"## BAG of WORDS\ncv = CountVectorizer(min_df = 0 , max_df = 1, binary = False , ngram_range = (1,3))","2970febd":"cv_train = cv.fit_transform(train_reviews_movie)\ncv_test = cv.transform(test_review_movie)","771dfc07":"cv_train.shape","4520eff2":"cv_test.shape","fc6853a2":"#TF-IDF\ntf = TfidfVectorizer(min_df = 0 , max_df = 1, use_idf = True,ngram_range = (1,3))","e50dd36c":"tf_train = tf.fit_transform(train_reviews_movie)","87d71e21":"tf_test = tf.transform(test_review_movie)","31993832":"tf_train.shape","d1a87f93":"tf_test.shape","65400e0d":"label  = LabelBinarizer()\nsentimentOfmovie = label.fit_transform(movie['sentiment'])","faafd21b":"sentimentOfmovie.shape","89fd5560":"train_movie_sentiment = movie.sentiment[:30000]","4d72de28":"test_movie_sentiment = movie.sentiment[30000:]","1b2d0be1":"logistic=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n#Fitting the model for Bag of words\nlr_bow=logistic.fit(cv_train,train_movie_sentiment)\nprint(lr_bow)","13dd0635":"#Fitting the model for tfidf features\nlr_tfidf=logistic.fit(tf_train,train_movie_sentiment)\nprint(lr_tfidf)","42d88821":"#Predicting the model for bag of words\nlr_bow_predict=logistic.predict(cv_test)\nprint(lr_bow_predict)","5cd644de":"#Predicting the model for tfidf features\nlr_tfidf_predict=logistic.predict(tf_test)\nprint(lr_tfidf_predict)","f97b7d08":"#Accuracy score for bag of words\nlr_bow_score=accuracy_score(test_movie_sentiment,lr_bow_predict)\nprint(\"lr_bow_score :\",lr_bow_score)","05af3fb2":"#Accuracy score for tfidf features\nlr_tfidf_score=accuracy_score(test_movie_sentiment,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)","2eea4e46":"lr_class_score=classification_report(test_movie_sentiment,lr_bow_predict)\nprint(\"lr_bow_score :\",lr_class_score)","f8974176":"lr_tfidf_class_report=classification_report(test_movie_sentiment,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_class_report)","30139a3a":"## Text Tokenization ","7fb79add":"## Kumar Gaurav , *M.sc(Data Science) CHRIST UNIVERSITY","917d7e75":"### This dataset is availble on kaggle. I downloaded from that. I used NLP method and ML technique for predict sentiment analysis purpose","499f3210":"### 1. bag of words\n### 2. TF - IDF\n### 3. Label encoding","28c00bd6":"### Predict _ model","f4cad22c":"# Movie_IMDB_review_sentiment_analysis","8a24617a":"### Model Accuracy ","0350116e":"# Now, we use NLP model technique","7bb4db55":"### Label_encoding","ff9be539":"## Classification_report","bbbdecad":"### Conclusion - I tried many method in background like naive,svm but logistic give me high accuracy , It may give us more high accuracy but will be more work on that.","4bcd3f59":"### now TF-IDF","08810641":"## stemming text","27de780d":"### Train,test,split the datset","85563ecd":"## Logistic model "}}