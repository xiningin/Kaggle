{"cell_type":{"b8318f09":"code","299dd025":"code","9ba4453a":"code","7db33aa5":"code","dcbf0e43":"code","712e57b2":"code","21be088e":"code","06d35f7c":"code","0f7f5727":"code","8d9021da":"code","585f762c":"code","8ee9cc15":"markdown","4374c42c":"markdown","3b644c4e":"markdown","62cfecd1":"markdown","661bc674":"markdown","c06ddc72":"markdown","9e56b671":"markdown","ad4f87d8":"markdown","50938123":"markdown"},"source":{"b8318f09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","299dd025":"import nltk    # NLP toolbox\nfrom os import getcwd\nimport pandas as pd\nfrom nltk.corpus import twitter_samples\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nnltk.download('twitter_samples')","9ba4453a":"# functions for NLP\n\ndef process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks    \n    tweet = re.sub(r'https?:\/\/[^\\s\\n\\r]+', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean","7db33aa5":"# functions for NLP\n\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs","dcbf0e43":"# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\ntweets = all_positive_tweets + all_negative_tweets # concatenate the lists\nlabels = np.append(np.ones((len(all_positive_tweets), 1)), np.zeros((len(all_negative_tweets), 1)), axis=0)\n\n# split the data into two pieces, one for  training and one for testing (validation set)\ntrain_pos = all_positive_tweets[:4000]\ntrain_neg = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg\n\nprint('Number of tweets: ', len(train_x))","712e57b2":"data = pd.read_csv('..\/input\/logistic-featurescsv\/logistic_features.csv')\n\ndata.tail()","21be088e":"## get rid of the data frame to keep only Numpy arrays\n\n# Each feature is labeled as bias, positive and negative\nX = data[['bias', 'positive', 'negative']].values \nY = data['sentiment'].values\n\nprint(X.shape, Y.shape)\nprint(X)","06d35f7c":"theta = [6.03518871e-08, 5.38184972e-04, -5.58300168e-04]","0f7f5727":"# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color based on the sentiment Y\nax.scatter(X[:, 1], X[:, 2], c=[colors[int(k)] for k in Y], s=0.1) # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n","8d9021da":"# Equation for the separation plane \n# It give a value in the negative axe as a function of a positive value\n# f(pos, neg, W) = w0 + w1 * pos + w2 * neg = 0\n# s(pos, W) = (w0 - w1 * pos) \/ w2\ndef neg(theta, pos):\n    return (-theta[0] - pos * theta[1]) \/ theta[2]\n\n# Equation for the direction of the sentiments change\n# We don't care about the magnitude of the change, We are only interested in the direction\n# So This direction is just perpendicular function to the separetion plane\n# df(pos, W) = pos * w2 \/ w1\ndef direction(theta, pos):\n    return pos * theta[2] \/ theta[1]","585f762c":"# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color base on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\n# Now lets represent the logistic regression model in this chart. \nmaxpos = np.max(X[:,1])\n\noffset = 5000 # The pos value for the direction vectors origin\n\n# Plot a gray line that divides the 2 areas.\nax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n\n# Plot a green line pointing to the positive direction\nax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=500, head_length=500, fc='g', ec='g')\n# Plot a red line pointing to the negative direction\nax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=500, head_length=500, fc='r', ec='r')\n\nplt.show()","8ee9cc15":"## Plot the model alngside the data\n\nWe will draw a gray line to show the cutoff betweent the positive and negative regions. In other words, the gray line marks the line where \n$$ z = \\theta * x = 0 $$ \n$$ x = [1, pos, neg] $$\n$$ z(\\theta, x) = \\theta_0+ \\theta_1 * pos + \\theta_2 * neg = 0 $$\n$$ neg = (-\\theta_0 - \\theta_1 * pos) \/ \\theta_2 $$\n\nThe red and green lines that point in the direction of the corresponding sentiment are calculated using a perpendicular line to the separation line calculated in the previous equations (neg function). It must point in the same direction as the derivative of the Logit function, but the magnitude may differ. It is only for a visual representation of the model. \n\n$$direction = pos * \\theta_2 \/ \\theta_1$$\n    ","4374c42c":"From the plot, it is evident that the features that we have chosen to represent tweets as numerical vectors allow an almost perfect separation between positive and negative tweets. So you can ","3b644c4e":"**Note that more critical than the Logistic regression itself, are the features extracted from tweets that allow getting the right results in this exercise**\n","62cfecd1":"## Import the required libraries\n\nWe will be using *NLTK*, and opensource NLP library, for collecting, handling, and processing Twitter data. Inthis lab, we will use the example dataset that comes alongside with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly.\n\nSo, to stat, let's import the required libraries.","661bc674":"## Plot the samples in as scatter plot\n\nThe vector theta represents a plane that split our features space into two parts. Samples located over that plane are considered positive, and samples located under that plane are considered negative. Remember that we have a 3D feature space, i.e., each tweet is represented as a vector comprised of three values: `[bias, positive_sum, negative_sum]`, always having `bias = 1`.\n\nIf we ignore the bias term, we can plot each tweet in a cartesian plane, using `positive_sum` and `negative_sum`. In the cell below, we do precisely this. Addintionally, we color each tweet, depending on its class. Positive tweets will be green and negative tweets will be red.\n","c06ddc72":"The green line in the chart points in the direction where z > 0 and the red line points in the direction where z < 0. The direction of these lines are given by the weights $\\theta_1$ and $\\theta_2$","9e56b671":"## Load the extracted features\n\nPart of this week's assignment is the creation of the numerical features needed for the  Logistic regression model. In order not to interfere with it, we have previously calculated and stored these features in a CSV file for the entire training set.\n\n","ad4f87d8":"# Visualizing tweets and the Logistic Regression model\n\n#### **Objectives:** Visualize and interpret the logistic regression model\n\n#### **Steps:**\n - Plot tweets in a scatter plot using their positive and negative sums.\n - Plot the output of the logistic regression model in the same plot as a solid line\n ","50938123":"## Load the NLTK sample dataset\n\nTo complete this lab, you need the sample dataset of the previous lab. Here, we assume the files are already available, and we only need to load into Python lists."}}