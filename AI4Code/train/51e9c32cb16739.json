{"cell_type":{"c3b8b3a0":"code","0b4cf662":"code","663546e8":"code","74fb5098":"code","a49fed90":"code","56c3beeb":"code","6eb0769f":"code","4d8214fd":"code","fc2a5362":"code","30bde06b":"code","b48167de":"code","66a82578":"code","b8b01de8":"code","0a68c57f":"code","67f7ad39":"code","eb4ab57a":"code","91395c59":"code","71224ff6":"code","437d370f":"code","e06cad1c":"code","096aef65":"code","eb5bcf95":"code","50782ac0":"code","974249fd":"code","1220d949":"code","ed6a66ea":"code","5114bee2":"code","c00ea1a5":"code","11db3ef7":"code","4be185d9":"code","8626605f":"code","b543c92e":"code","237f17f9":"code","0c43db4c":"code","f568a070":"code","815288b7":"code","828ebdd4":"code","a82fa745":"code","66af747c":"code","93e593b1":"code","32822a1e":"code","122472da":"code","b9b3740f":"code","7ccfc68f":"code","a8d02282":"code","3ed486b6":"code","22dda823":"code","7ce49e6f":"code","9cf01cc3":"code","06c8302a":"code","045734e7":"code","258068ed":"markdown","46ba000c":"markdown","63ee2ade":"markdown","2aedb278":"markdown","2c3bddcb":"markdown","85138c8f":"markdown","f7e4f3fb":"markdown","3fc13074":"markdown","ad042d96":"markdown","4b48ecf2":"markdown","576961df":"markdown","5bffecd4":"markdown","0830a397":"markdown","82101b48":"markdown","076a811a":"markdown","896c7865":"markdown","99cd1571":"markdown","0ed28a21":"markdown","42ba815b":"markdown","bfe13d0c":"markdown","0d1aa8fd":"markdown","164079b1":"markdown","bfd51884":"markdown","923d377b":"markdown","4fb17322":"markdown","2416ae15":"markdown","0026ac71":"markdown","ebe4e4c7":"markdown","f8085d9a":"markdown","809f5ec4":"markdown","95947c82":"markdown","d45a70ae":"markdown","cd585cc5":"markdown","555385b0":"markdown","6693d52b":"markdown","57bf27d4":"markdown"},"source":{"c3b8b3a0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","0b4cf662":"items = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitem_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\n\ntrain = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\n\nsample_submission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","663546e8":"print('item_categories')\ndisplay(item_categories.head())\n\nprint('items')\ndisplay(items.head())\n\nprint('shops')\ndisplay(shops.head())\n\nprint('train')\ndisplay(train.head())\n\nprint('test')\ndisplay(test.head())\n\nprint('sample_submission')\ndisplay(sample_submission.head())","74fb5098":"train.info()","a49fed90":"print('train')\ndisplay(train.isnull().sum())\n\nprint('test')\ndisplay(test.isnull().sum())","56c3beeb":"print('train')\ndisplay(train.describe(include='all'))\n\nprint('test')\ndisplay(test.describe(include='all'))","6eb0769f":"#drop duplicates\nsubset = ['date','date_block_num','shop_id','item_id','item_cnt_day']\nprint(train.duplicated(subset=subset).value_counts())\ntrain.drop_duplicates(subset=subset, inplace=True)","4d8214fd":"train[train['item_price'] < 0]","fc2a5362":"#drop negative value in item_price\ntrain = train[train['item_price'] > 0]","30bde06b":"train = train[train['item_cnt_day'] > 0]","b48167de":"sns.boxplot(train['item_price']);","66a82578":"sns.boxplot(train['item_cnt_day']);","b8b01de8":"#define a drop outliers function\ndef drop_outliers(df, feature, percentile_high = .99):\n    '''df (dataframe)           : dataset\n       feature (string)         : column\n       percentile_high (float)  : upper limit\n       .........................................................\n    '''\n    #train size before dropping values\n    shape_init = df.shape[0]\n    \n    #get percentile value\n    max_value = df[feature].quantile(percentile_high)\n    \n    #drop outliers\n    print('dropping outliers...')\n    df = df[df[feature] < max_value]\n    \n    print(str(shape_init - df.shape[0]) + ' ' + feature + ' values over ' + str(max_value) + ' have been removed' )\n    \n    return df","0a68c57f":"#drop outliers in item_price feature\ntrain = drop_outliers(train, 'item_price')","67f7ad39":"#drop outliers in item_cnt_day\ntrain = drop_outliers(train, 'item_cnt_day')","eb4ab57a":"prices_shop_df = train[['shop_id','item_id','item_price']]\nprices_shop_df = prices_shop_df.groupby(['shop_id','item_id']).apply(lambda df: df['item_price'][-2:].mean())\nprices_shop_df = prices_shop_df.to_frame(name = 'item_price')\n\nprices_shop_df","91395c59":"test = pd.merge(test, prices_shop_df, how='left', left_on=['shop_id','item_id'], right_on=['shop_id','item_id'])\n\ntest.head()","71224ff6":"#check for missing values\ntest['item_price'].isnull().sum()","437d370f":"#split content in date into month and year\ntrain['month'] = [date.split('.')[1] for date in train['date']]\ntrain['year'] = [date.split('.')[2] for date in train['date']]\n\n#drop date and date_block_num features\ntrain.drop(['date','date_block_num'], axis=1, inplace=True)\n\n#create month and year features fot test dataset\ntest['month'] = '11'\ntest['year'] = '2015'","e06cad1c":"#change item_cnt_day into item_cnt_month\ntrain_monthly = train.groupby(['year','month','shop_id','item_id'], as_index=False)[['item_cnt_day']].sum()\ntrain_monthly.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=True)\n\ntrain_monthly = pd.merge(train_monthly, prices_shop_df, how='left', left_on=['shop_id','item_id'], right_on=['shop_id','item_id'])\n\ntrain_monthly.head()","096aef65":"train = train_monthly","eb5bcf95":"test = test.reindex(columns=['ID','year','month','shop_id','item_id','item_price'])\n\ntest.head()","50782ac0":"#extract main categories\nitem_categories['main_category'] = [x.split(' - ')[0] for x in item_categories['item_category_name']]\n\n#some items don't have sub-categories. For those, we will use None as a sub-category (consider the main category as a sub)\nsub_categories = []\nfor i in range(len(item_categories)):\n    try:\n        sub_categories.append(item_categories['item_category_name'][i].split(' - ')[1])\n        \n    except IndexError as e:\n        sub_categories.append('None')\n        #sub_categories.append(item_categories['main_category'][i])\n\nitem_categories['sub_category'] = sub_categories\n\n#drop item_category_name\nitem_categories.drop(['item_category_name'], axis=1, inplace=True)\n\nitem_categories.head()","974249fd":"#merge with item_categories\nitems = pd.merge(items, item_categories, how='left')\n\n#drop item_name and item_category_id\nitems.drop(['item_name','item_category_id'], axis=1, inplace=True)\n\nitems.head()","1220d949":"#merge to train and test datasets\ntrain = pd.merge(train, items, how='left')\ntest = pd.merge(test, items, how='left')","ed6a66ea":"from string import punctuation\n\n# replace all the punctuation in the shop_name columns\nshops[\"shop_name_cleaned\"] = shops[\"shop_name\"].apply(lambda s: \"\".join([x for x in s if x not in punctuation]))\n\n# extract the city name\nshops[\"shop_city\"] = shops[\"shop_name_cleaned\"].apply(lambda s: s.split()[0])\n\n#extract the type\nshops[\"shop_type\"] = shops[\"shop_name_cleaned\"].apply(lambda s: s.split()[1])\n\n#extract shop's name\nshops[\"shop_name\"] = shops[\"shop_name_cleaned\"].apply(lambda s: \" \".join(s.split()[2:]))\n\nshops.drop(['shop_name_cleaned'], axis=1, inplace=True)\n\nshops.head()","5114bee2":"#merge to train and test datasets\ntrain = pd.merge(train, shops, how='left')\ntest = pd.merge(test, shops, how='left')","c00ea1a5":"print('train')\ndisplay(train.head())\n\nprint('test')\ndisplay(test.head())","11db3ef7":"#fill missing values with median of each main_category and sub_category\ntest['item_price'] = test.groupby(['main_category','sub_category'])['item_price'].apply(lambda df: df.fillna(df.median()))","4be185d9":"test['item_price'].isnull().sum()","8626605f":"#fill missing values with median of each sub_category\ntest['item_price'] = test.groupby(['sub_category'])['item_price'].apply(lambda df: df.fillna(df.median()))","b543c92e":"test['item_price'].isnull().sum()","237f17f9":"test[test['item_price'].isnull()]","0c43db4c":"#fill missing values with median of main_category and sub_category from train dataset\nfiller = train[(train['main_category'] == 'PC') & (train['sub_category'] == '\u0413\u0430\u0440\u043d\u0438\u0442\u0443\u0440\u044b\/\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438')]['item_price'].median()\n\ntest['item_price'].fillna(filler, inplace=True)","f568a070":"test['item_price'].isnull().sum()","815288b7":"train['item_cnt_month'] = train['item_cnt_month'].clip(0,20)","828ebdd4":"target_array = train['item_cnt_month']\ntrain.drop(['item_cnt_month'], axis=1, inplace=True)\n\ntest_id = test['ID']\ntest.drop(['ID'], axis=1, inplace=True)","a82fa745":"train.drop(['shop_id','item_id'], axis=1, inplace=True)\ntest.drop(['shop_id','item_id'], axis=1, inplace=True)","66af747c":"def downcast_dtypes(df):\n    '''df (dataframe)  : data\n       Changes column types in the dataframe\n           `float64` type to `float32`\n           `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","93e593b1":"#reduce memory\ndowncast_dtypes(train)\ndowncast_dtypes(test)","32822a1e":"train.info()","122472da":"#check for any missing data\nprint('missing data in the train dataset : ', train.isnull().any().sum())\nprint('missing data in the test dataset : ', test.isnull().any().sum())","b9b3740f":"#define a normality test function\ndef normalityTest(data, alpha=0.05):\n    \"\"\"data (array)   : The array containing the sample to be tested.\n\t   alpha (float)  : Significance level.\n\t   return True if data is normal distributed\"\"\"\n    \n    from scipy import stats\n    \n    statistic, p_value = stats.normaltest(data)\n    \n    #null hypothesis: array comes from a normal distribution\n    if p_value < alpha:  \n        #The null hypothesis can be rejected\n        is_normal_dist = False\n    else:\n        #The null hypothesis cannot be rejected\n        is_normal_dist = True\n    \n    return is_normal_dist","7ccfc68f":"#check normality of all numericaal features and transform it if not normal distributed\nfor feature in train.columns:\n    if (train[feature].dtype != 'object'):\n        if normalityTest(train[feature]) == False:\n            train[feature] = np.log1p(train[feature])\n            test[feature] = np.log1p(test[feature])","a8d02282":"#use numpy.log1p in order to target_array follows a normal distribution\ntarget_array = np.log1p(target_array)","3ed486b6":"from sklearn.preprocessing import OrdinalEncoder\n\nenc = OrdinalEncoder()\n\nX = enc.fit_transform(train)\ny = target_array\n\nX_predict = enc.fit_transform(test)","22dda823":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .1, random_state = 0)","7ce49e6f":"from xgboost import XGBRegressor\n\n#create a model\nmodel = XGBRegressor()\n\n#fitting\nmodel.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_test, y_test)], \n    verbose=True, \n    early_stopping_rounds = 20)","9cf01cc3":"#calculate Mean Squared Error\nfrom sklearn.metrics import mean_squared_error\n\nprint('MSE : ', mean_squared_error(y_test, model.predict(X_test)))","06c8302a":"#make a prediction\ny_predict = model.predict(X_predict)\n\n#transform the values back\ny_predict = np.expm1(y_predict)","045734e7":"#sava results to a file\nresults = pd.DataFrame({'ID': test_id, 'item_cnt_month': y_predict})\nresults.to_csv('my_submission.csv', index=False)","258068ed":"- Check for missing values","46ba000c":"# Import The Data","63ee2ade":"### Exploratory Data Analysis: Epilogue\n- From competition's evaluation note, target values are clipped into [0,20] range.","2aedb278":"- Normality test","2c3bddcb":"- Define target_array","85138c8f":"### Reindex test dataset","f7e4f3fb":"- Encoding","3fc13074":"- Drop shop_id & item_id","ad042d96":"### Transform Data in Train Dataset As Monthly","4b48ecf2":"### Price\nMake a dataframe with item_price feature group by shop_id and item_id to get price for each item per shop. We can use this dataframe to create item_price feature for test dataset.","576961df":"Display current train and test datasets","5bffecd4":"# Creating a model","0830a397":"# Import Libraries\nFirst, we import necessary libraries, such as:","82101b48":"- Exploring Shops Dataset","076a811a":"We begin by splitting data into two subsets: for training data and for testing data.","896c7865":"- Exploring Items Dataset","99cd1571":"Since there is only 1 negative value in item_price, we can just drop that because it won't affect the prediction too much.","0ed28a21":"# Exploratory Data Analysis","42ba815b":"- Reduce memory usage","bfe13d0c":"Show remaining missing values","0d1aa8fd":"# Read The Data","164079b1":"### Fill missing values in item_price (by item categories)","bfd51884":"There are still missing values in test's item_price. We will fill this later by creating more features from item_categories.","923d377b":"- Check train info","4fb17322":"All remaining item_price's missing values have same main_category and sub_category. This main and sub categories are not in the test dataset, but in train dataset.","2416ae15":"- Drop outliers","0026ac71":"- Quick look using ```describe()``` function","ebe4e4c7":"We will use XGBRegressor model to predict total sales for every product and store in the next month.","f8085d9a":"### Cleaning item_price and item_cnt_day\n- Check min and max","809f5ec4":"### Exploring other datasets\n- Exploring Item Categories dataset","95947c82":"**Quick observations:**\n- There are no missing values.\n- The train and test datasets did not match in term of features.\n- There is minus value(s) in item_price.\n- There is minus value(s) in item_cnt_day.","d45a70ae":"Now we can merge this dataframe with test dataset to create item_price feature in test dataset.","cd585cc5":"This kernel is going to solve [Predict Future Sales Competition](https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales) on Kaggle.\n\n**Competition Description:**\n\n\nThis challenge serves as final project for the [\"How to win a data science competition\"](https:\/\/www.coursera.org\/learn\/competitive-data-science\/home\/welcome) Coursera course.\n\nIn this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - [1C Company](http:\/\/1c.ru\/eng\/title.htm). \n\nWe are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.","555385b0":"- Check for missing values","6693d52b":"### Removing Duplicates","57bf27d4":"### Check negative values in item_price"}}