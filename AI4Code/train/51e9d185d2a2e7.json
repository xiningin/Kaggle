{"cell_type":{"6670c9ce":"code","9d054b96":"code","24127eb0":"code","e2952be0":"code","5b8b5d48":"code","e3d7a67e":"code","c009b338":"code","ddfe4139":"code","edccd06c":"code","c9c51739":"code","27fbfc39":"code","769ed7b0":"code","fb808c92":"code","e76e7ac4":"code","1ab6568f":"code","3002d203":"code","e2931c8c":"code","3a0dcf14":"code","8537a055":"code","131243d7":"code","7447e367":"code","5594243d":"code","37571486":"code","7909c277":"code","2532ffec":"code","a9020a97":"code","1871581b":"code","0ee9ff35":"code","3fca29ca":"code","36093652":"code","ae0b8009":"code","61484346":"code","4fda0bfc":"code","fe03c4c6":"code","92ff9596":"code","9f7c7a13":"markdown","ebe69c08":"markdown","10128b5a":"markdown","8729708b":"markdown","e75783bc":"markdown","2863de42":"markdown","9b262104":"markdown","ad059be5":"markdown","60253c61":"markdown","1b959d8b":"markdown"},"source":{"6670c9ce":"from fastai.text import *\nfrom tqdm import tqdm_notebook as tqdm","9d054b96":"!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-development.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-test.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-validation.tsv -q","24127eb0":"data_path = Path(\".\")","e2952be0":"train = pd.read_csv(data_path\/\"gap-development.tsv\", sep=\"\\t\")\nval = pd.read_csv(data_path\/\"gap-validation.tsv\", sep=\"\\t\")\ntest = pd.read_csv(data_path\/\"gap-test.tsv\", sep=\"\\t\")","5b8b5d48":"print(len(train), len(val), len(test))","e3d7a67e":"train[\"is_valid\"] = True\ntest[\"is_valid\"] = False\nval[\"is_valid\"] = True\n\ndf_pretrain = pd.concat([train, test, val])","c009b338":"db = (TextList.from_df(df_pretrain, data_path\/\"db\", cols=\"Text\").split_from_df(col=\"is_valid\").label_for_lm().databunch())","ddfe4139":"vocab = db.vocab","edccd06c":"lm = language_model_learner(db, AWD_LSTM, drop_mult=0.5, pretrained=True)","c9c51739":"lm.unfreeze()","27fbfc39":"lm.lr_find()","769ed7b0":"lm.recorder.plot()","fb808c92":"lm.fit_one_cycle(3, 1e-3)","e76e7ac4":"lm.fit_one_cycle(3, 1e-3)","1ab6568f":"spacy_tok = SpacyTokenizer(\"en\")\ntokenizer = Tokenizer(spacy_tok)","3002d203":"df_pretrain.Text.apply(lambda x: len(tokenizer.process_text(x, spacy_tok))).describe()","e2931c8c":"import spacy\nnlp = spacy.blank(\"en\")\n\ndef get_token_num_by_offset(s, offset):\n  s_pre = s[:offset]\n  return len(spacy_tok.tokenizer(s_pre))\n\n# note that 'xxunk' is not special in this sense\nspecial_tokens = ['xxbos','xxfld','xxpad', 'xxmaj','xxup','xxrep','xxwrep']\n\n\ndef adjust_token_num(processed, token_num):\n  \"\"\"\n  As fastai tokenizer introduces additional tokens, we need to adjust for them.\n  \"\"\"\n  counter = -1\n  do_unrep = None\n  for i, token in enumerate(processed):\n    if token not in special_tokens:\n      counter += 1\n    if do_unrep:\n      do_unrep = False\n      if processed[i+1] != \".\":\n        token_num -= (int(token) - 2) # one to account for the num itself\n      else:  # spacy doesn't split full stops\n        token_num += 1\n    if token == \"xxrep\":\n      do_unrep = True\n    if counter == token_num:\n      return i\n  else:\n    counter = -1\n    for i, t in enumerate(processed):\n      if t not in special_tokens:\n        counter += 1\n      print(i, counter, t)\n    raise Exception(f\"{token_num} is out of bounds ({processed})\")","3a0dcf14":"def dataframe_to_tensors(df, max_len=512):\n  # offsets are: pron_tok_offset, a_tok_offset, a_tok_right_offset, b_tok_offset, b_tok_right_offset\n  offsets = list()\n  labels = np.zeros((len(df),), dtype=np.int64)\n  processed = list()\n  for i, row in tqdm(df.iterrows()):\n    try:\n      text = row[\"Text\"]\n      a_offset = row[\"A-offset\"]\n      a_len = len(nlp(row[\"A\"]))\n      \n      b_offset = row[\"B-offset\"]\n      b_len = len(nlp(row[\"B\"]))\n\n      pron_offset = row[\"Pronoun-offset\"]\n      is_a = row[\"A-coref\"]\n      is_b = row[\"B-coref\"]\n      a_tok_offset = get_token_num_by_offset(text, a_offset)\n      b_tok_offset = get_token_num_by_offset(text, b_offset)\n      a_right_offset = a_tok_offset + a_len - 1\n      b_right_offset = b_tok_offset + b_len - 1\n      pron_tok_offset = get_token_num_by_offset(text, pron_offset)\n      tokenized = tokenizer.process_text(text, spacy_tok)[:max_len]\n      tokenized = [\"xxpad\"] * (max_len - len(tokenized)) + tokenized  # add padding\n      a_tok_offset = adjust_token_num(tokenized, a_tok_offset)\n      a_tok_right_offset = adjust_token_num(tokenized, a_right_offset)\n      b_tok_offset = adjust_token_num(tokenized, b_tok_offset)\n      b_tok_right_offset = adjust_token_num(tokenized, b_right_offset)\n      pron_tok_offset = adjust_token_num(tokenized, pron_tok_offset)\n      numericalized = vocab.numericalize(tokenized)\n      processed.append(torch.tensor(numericalized, dtype=torch.long))\n      offsets.append([pron_tok_offset, a_tok_offset, a_tok_right_offset, b_tok_offset, b_tok_right_offset])\n      if is_a:\n        labels[i] = 0\n      elif is_b:\n        labels[i] = 1\n      else:\n        labels[i] = 2\n    except Exception as e:\n      print(i)\n      raise\n  processed = torch.stack(processed)\n  offsets = torch.tensor(offsets, dtype=torch.long)\n  labels = torch.from_numpy(labels)\n  return processed, offsets, labels","8537a055":"train_ds = TensorDataset(*dataframe_to_tensors(test))\nvalid_ds = TensorDataset(*dataframe_to_tensors(val))\ntest_ds = TensorDataset(*dataframe_to_tensors(train))","131243d7":"train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=32, shuffle=False)\ntest_dl = DataLoader(test_ds, batch_size=32, shuffle=False)","7447e367":"lm.freeze()","5594243d":"encoder_hidden_sz = 400\n\ndevice = torch.device(\"cuda\")\n\nclass CorefResolver(nn.Module):\n  def __init__(self, encoder, dropout_p=0.3):\n    super(CorefResolver, self).__init__()\n    self.encoder = encoder\n    self.dropout = nn.Dropout(dropout_p)\n    self.hidden2hidden = nn.Linear(encoder_hidden_sz * 2 + 1, 25)\n    self.hidden2logits = nn.Linear(50, 3)\n    self.relu = nn.ReLU()\n    self.activation = nn.LogSoftmax(dim=1)\n    self.loss = nn.NLLLoss()\n    \n  def forward(self, seqs, offsets, labels=None):\n    encoded = self.dropout(self.encoder(seqs)[0][2])\n    a_q = list()\n    b_q = list()\n    for enc, offs in zip(encoded, offsets):\n      # extract the hidden states that correspond to A, B and the pronoun, and make pairs of those \n      a_repr = enc[offs[2]]\n      b_repr = enc[offs[4]]\n      a_q.append(torch.cat([enc[offs[0]], a_repr, torch.dot(enc[offs[0]], a_repr).unsqueeze(0)]))\n      b_q.append(torch.cat([enc[offs[0]], b_repr, torch.dot(enc[offs[0]], b_repr).unsqueeze(0)]))\n    a_q = torch.stack(a_q)\n    b_q = torch.stack(b_q)\n    # apply the same \"detector\" layer to both batches of pairs\n    is_a = self.relu(self.dropout(self.hidden2hidden(a_q)))\n    is_b = self.relu(self.dropout(self.hidden2hidden(b_q)))\n    # concatenate outputs of the \"detector\" layer to get the final probability distribution\n    is_a_b = torch.cat([is_a, is_b], dim=1)\n    is_logits = self.hidden2logits(self.dropout(self.relu(is_a_b)))\n\n    activation = self.activation(is_logits)\n    if labels is not None:\n      return activation, self.loss(activation, labels)\n    else:\n      return activation","37571486":"enc = lm.model[0]","7909c277":"resolver = CorefResolver(enc)","2532ffec":"resolver.to(device)","a9020a97":"for param in resolver.encoder.parameters():\n  param.requires_grad = False","1871581b":"lr = 0.001\n\nloss_fn = nn.NLLLoss()\noptimizer = torch.optim.Adam(resolver.parameters(), lr=lr)","0ee9ff35":"from sklearn.metrics import classification_report","3fca29ca":"def train_epoch(model, optimizer, train_dl, report_every=10):\n  model.train()\n  step = 0\n  total_loss = 0\n  \n  for texts, offsets, labels in train_dl:\n    texts, offsets, labels = texts.to(device), offsets.to(device), labels.to(device)\n    step += 1\n    optimizer.zero_grad()\n    _, loss = model(texts, offsets, labels)\n    total_loss += loss.item()\n    \n    loss.backward()\n    optimizer.step()\n    \n    if step % report_every == 0:\n      print(f\"Step {step}, loss: {total_loss\/report_every}\")\n      total_loss = 0\n      \ndef evaluate(model, optimizer, valid_dl, probas=False):\n  probas = list()\n  model.eval()\n  predictions = list()\n  total_loss = 0\n  all_labels = list()\n  with torch.no_grad():\n    for texts, offsets, labels in valid_dl:\n      texts, offsets, labels = texts.cuda(), offsets.cuda(), labels.cuda()\n      preds, loss = model(texts, offsets, labels)\n      total_loss += loss.item()\n      probas.append(preds.cpu().detach().numpy())\n      predictions.extend([i.item() for i in preds.max(1)[1]])\n    \n    \n  print(f\"Validation loss: {total_loss\/len(valid_dl)}\")\n  print()\n  print(classification_report(valid_dl.dataset.tensors[2].numpy(), predictions))\n  if probas:\n    return total_loss, np.vstack(probas)\n  return total_loss, predictions","36093652":"total_epoch = 0\nbest_loss = 1e6\n\nfor i in range(3):\n  print(\"Epoch\", i + 1)\n  total_epoch += 1\n  train_epoch(resolver, optimizer, train_dl) \n  loss, labels = evaluate(resolver, optimizer, valid_dl)\n  if loss < best_loss:\n    best_loss = loss\n    print(f\"Loss improved, saving {total_epoch}\")\n    torch.save(resolver.state_dict(), data_path\/\"model_best.pt\")","ae0b8009":"for param in resolver.encoder.parameters():\n  param.requires_grad = True","61484346":"lr = 3e-4\noptimizer = torch.optim.Adam(resolver.parameters(), lr=lr)","4fda0bfc":"for i in range(6):\n  print(\"Epoch\", i + 1)\n  total_epoch += 1\n  train_epoch(resolver, optimizer, train_dl)\n  loss, labels = evaluate(resolver, optimizer, valid_dl)\n  if loss < best_loss:\n    best_loss = loss\n    print(f\"Loss improved, saving {total_epoch}\")\n    torch.save(resolver.state_dict(), data_path\/\"model_best.pt\")","fe03c4c6":"resolver.load_state_dict(torch.load(data_path\/\"model_best.pt\"))","92ff9596":"loss, res = evaluate(resolver, optimizer, test_dl, True)\nres_s = np.exp(res)  # don't forget that we have log-softmax outputs:\nsubmission = pd.DataFrame(res_s, index=train[\"ID\"], columns=[\"A\", \"B\", \"NEITHER\"])\nsubmission.to_csv(\"submission.csv\", index=\"id\")","9f7c7a13":"Note that for simplicity we only use the first token of the entity, this is a point that can be improved.","ebe69c08":"## Preprocess the dataset for classification","10128b5a":"# Coreference resolution with fast.ai\n\nIn this notebook, we will explore ULMFiT approach to solve this task. With proper fine-tuning, you can get decent results in a matter of 20 minutes. Some 15 epochs of fine-tuning will get you up to 20-ish place.\n\nChanges in this version:\n1. More civilized approach to validation.\n2. The model uses the representation of the last token of the entity instead of the first token. With a unidirectional encoder, this might be the right thing to do.\n\nI will be grateful for any suggestions, especially about converting two logits\/probabilities into the three classes without the need for an additional layer.\n\n## Collect the data","8729708b":"## Define the training loop ","e75783bc":"As the language model is already trained on Wikipedia, which is also the source of the excerpts, we can proceed to unfreezing right away:","2863de42":"Unfreeze the encoder and do fine-tuning. We do the finetuning until the model starts to recognize class `2`.","9b262104":"## Train ","ad059be5":"Finetune the language model:","60253c61":"## Classifier architecture \n\nUnfortunately, the magic of fast.ai stops here: we need to create a custom classifier on top. What we do here is:\n1. Extract hidden states corresponding to entities and the pronoun.\n2. For each pair (pronoun, entity) we run it through a hidden layer to retrieve a 25-dimensional vector that describes their similarity.\n3. Concat the vectors.\n4. Use another layer to turn these into probabilities.","1b959d8b":"## Fin: get the predictions and submit!"}}