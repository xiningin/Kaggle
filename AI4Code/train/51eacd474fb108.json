{"cell_type":{"5bbee7d6":"code","5ad51524":"code","56acdc98":"code","eaa401ec":"code","8b5127e3":"code","295dad88":"code","708c4ecd":"code","91b53018":"code","e142e90b":"code","8f7d2bf3":"code","88c7a5d1":"code","fc4f758c":"code","78fd1fd2":"code","b3791d21":"code","67469f2e":"code","42338d18":"code","c6df2000":"code","18108ca8":"code","6c68dea8":"code","285ef3ad":"code","51d25425":"code","7466cc3c":"code","8449aafd":"code","045446ba":"code","2f9d70ac":"code","35c01240":"code","672e342b":"code","4ab83cdd":"code","fbb809bc":"code","4029aabc":"code","695c521e":"code","20f5115e":"code","45077ec3":"code","2b7d27a2":"code","1719a071":"code","4bbc0547":"code","106f966b":"code","2f949b52":"markdown","dbe32c04":"markdown","8894d711":"markdown","1a6e54ee":"markdown","cacb5535":"markdown"},"source":{"5bbee7d6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport math\nfrom scipy.spatial import Voronoi\nimport numba as nb","5ad51524":"import lightgbm as lgb","56acdc98":"import keras\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom keras.utils import plot_model\nimport keras.backend as K\nimport tensorflow as tf","eaa401ec":"from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda,BatchNormalization\nfrom keras.models import Model\nfrom keras.losses import binary_crossentropy\nfrom  keras.callbacks import EarlyStopping, ModelCheckpoint\nimport codecs\n\nfrom keras.utils import to_categorical\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import f1_score","8b5127e3":"pd.set_option(\"display.max_columns\", 1000)\npd.set_option(\"display.max_rows\", 1000)","295dad88":"from kaggle.competitions import nflrush\nenv = nflrush.make_env()","708c4ecd":"# https:\/\/www.kaggle.com\/anandavati\/ngboost-for-nfl\/\n\nimport numpy.random as np_rnd\nimport scipy as sp\nfrom sklearn.base import BaseEstimator\nfrom sklearn.tree import DecisionTreeRegressor\nfrom scipy.stats import norm as dist\n\ndef default_tree_learner(depth=3):\n    return DecisionTreeRegressor(\n        criterion='friedman_mse',\n        min_samples_split=2,\n        min_samples_leaf=1,\n        min_weight_fraction_leaf=0.0,\n        max_depth=depth,\n        splitter='best')\n\nclass MLE:\n    def __init__(self, seed=123):\n        pass\n\n    def loss(self, forecast, Y):\n        return forecast.nll(Y.squeeze()).mean()\n\n    def grad(self, forecast, Y, natural=True):\n        fisher = forecast.fisher_info()\n        grad = forecast.D_nll(Y)\n        if natural:\n            grad = np.linalg.solve(fisher, grad)\n        return grad\n\n\nclass CRPS:\n    def __init__(self, K=32):\n        self.K = K\n\n    def loss(self, forecast, Y):\n        return forecast.crps(Y.squeeze()).mean()\n\n    def grad(self, forecast, Y, natural=True):\n        metric = forecast.crps_metric()\n        grad = forecast.D_crps(Y)\n        if natural:\n            grad = np.linalg.solve(metric, grad)\n        return grad\n\nEPS = 1e-8\nclass Normal(object):\n    n_params = 2\n\n    def __init__(self, params, temp_scale = 1.0):\n        self.loc = params[0]\n        self.scale = np.exp(params[1] \/ temp_scale) + 1e-8\n        self.var = self.scale #** 2  + 1e-8\n        self.shp = self.loc.shape\n\n        self.dist = dist(loc=self.loc, scale=self.scale)\n\n    def __getattr__(self, name):\n        if name in dir(self.dist):\n            return getattr(self.dist, name)\n        return None\n\n    def nll(self, Y):\n        return -self.dist.logpdf(Y)\n\n    def D_nll(self, Y_):\n        Y = Y_.squeeze()\n        D = np.zeros((self.var.shape[0], 2))\n        D[:, 0] = (self.loc - Y) \/ self.var\n        D[:, 1] = 1 - ((self.loc - Y) ** 2) \/ self.var\n        return D\n\n    def crps(self, Y):\n        Z = (Y - self.loc) \/ (self.scale + EPS)\n        return (self.scale * (Z * (2 * sp.stats.norm.cdf(Z) - 1) + \\\n                2 * sp.stats.norm.pdf(Z) - 1 \/ np.sqrt(np.pi)))\n\n    def D_crps(self, Y_):\n        Y = Y_.squeeze()\n        Z = (Y - self.loc) \/ (self.scale + EPS)\n        D = np.zeros((self.var.shape[0], 2))\n        D[:, 0] = -(2 * sp.stats.norm.cdf(Z) - 1)\n        D[:, 1] = self.crps(Y) + (Y - self.loc) * D[:, 0]\n        return D\n\n    def crps_metric(self):\n        I = np.c_[2 * np.ones_like(self.var), np.zeros_like(self.var),\n                  np.zeros_like(self.var), self.var]\n        I = I.reshape((self.var.shape[0], 2, 2))\n        I = 1\/(2*np.sqrt(np.pi)) * I\n        return I #+ 1e-4 * np.eye(2)\n\n    def fisher_info(self):\n        FI = np.zeros((self.var.shape[0], 2, 2))\n        FI[:, 0, 0] = 1\/self.var + 1e-5\n        FI[:, 1, 1] = 2\n        return FI\n\n    def fisher_info_cens(self, T):\n        nabla = np.array([self.pdf(T),\n                          (T - self.loc) \/ self.scale * self.pdf(T)])\n        return np.outer(nabla, nabla) \/ (self.cdf(T) * (1 - self.cdf(T))) + 1e-2 * np.eye(2)\n\n    def fit(Y):\n        m, s = sp.stats.norm.fit(Y)\n        return np.array([m, np.log(s)])\n        #return np.array([m, np.log(1e-5)])\n\n\nclass NGBoost(BaseEstimator):\n\n    def __init__(self, Dist=Normal, Score=MLE(),\n                 Base=default_tree_learner, natural_gradient=True,\n                 n_estimators=500, learning_rate=0.01, minibatch_frac=1.0,\n                 verbose=True, verbose_eval=100, tol=1e-4):\n        self.Dist = Dist\n        self.Score = Score\n        self.Base = Base\n        self.natural_gradient = natural_gradient\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.minibatch_frac = minibatch_frac\n        self.verbose = verbose\n        self.verbose_eval = verbose_eval\n        self.init_params = None\n        self.base_models = []\n        self.scalings = []\n        self.tol = tol\n\n    def pred_param(self, X, max_iter=None):\n        m, n = X.shape\n        params = np.ones((m, self.Dist.n_params)) * self.init_params\n        for i, (models, s) in enumerate(zip(self.base_models, self.scalings)):\n            if max_iter and i == max_iter:\n                break\n            resids = np.array([model.predict(X) for model in models]).T\n            params -= self.learning_rate * resids * s\n        return params\n\n    def sample(self, X, Y, params):\n        if self.minibatch_frac == 1.0:\n            return np.arange(len(Y)), X, Y, params\n        sample_size = int(self.minibatch_frac * len(Y))\n        idxs = np_rnd.choice(np.arange(len(Y)), sample_size, replace=False)\n        return idxs, X[idxs,:], Y[idxs], params[idxs, :]\n\n    def fit_base(self, X, grads):\n        models = [self.Base().fit(X, g) for g in grads.T]\n        fitted = np.array([m.predict(X) for m in models]).T\n        self.base_models.append(models)\n        return fitted\n\n    def line_search(self, resids, start, Y, scale_init=1):\n        S = self.Score\n        D_init = self.Dist(start.T)\n        loss_init = S.loss(D_init, Y)\n        scale = scale_init\n        while True:\n            scaled_resids = resids * scale\n            D = self.Dist((start - scaled_resids).T)\n            loss = S.loss(D, Y)\n            norm = np.mean(np.linalg.norm(scaled_resids, axis=1))\n            if not np.isnan(loss) and (loss < loss_init or norm < self.tol) and\\\n               np.linalg.norm(scaled_resids, axis=1).mean() < 5.0:\n                break\n            scale = scale * 0.5\n        self.scalings.append(scale)\n        return scale\n\n    def fit(self, X, Y, X_val = None, Y_val = None, train_loss_monitor = None, val_loss_monitor = None):\n\n        loss_list = []\n        val_loss_list = []\n        self.fit_init_params_to_marginal(Y)\n\n        params = self.pred_param(X)\n        if X_val is not None and Y_val is not None:\n            val_params = self.pred_param(X_val)\n\n        S = self.Score\n\n        if not train_loss_monitor:\n            train_loss_monitor = S.loss\n\n        if not val_loss_monitor:\n            val_loss_monitor = S.loss\n\n        for itr in range(self.n_estimators):\n            _, X_batch, Y_batch, P_batch = self.sample(X, Y, params)\n\n            D = self.Dist(P_batch.T)\n\n            loss_list += [train_loss_monitor(D, Y_batch)]\n            loss = loss_list[-1]\n            grads = S.grad(D, Y_batch, natural=self.natural_gradient)\n\n            proj_grad = self.fit_base(X_batch, grads)\n            scale = self.line_search(proj_grad, P_batch, Y_batch)\n\n            params -= self.learning_rate * scale * np.array([m.predict(X) for m in self.base_models[-1]]).T\n\n            val_loss = 0\n            if X_val is not None and Y_val is not None:\n                val_params -= self.learning_rate * scale * np.array([m.predict(X_val) for m in self.base_models[-1]]).T\n                val_loss = val_loss_monitor(self.Dist(val_params.T), Y_val)\n                val_loss_list += [val_loss]\n                if len(val_loss_list) > 10 and np.mean(np.array(val_loss_list[-5:])) > \\\n                   np.mean(np.array(val_loss_list[-10:-5])):\n                    if self.verbose:\n                        print(f\"== Quitting at iteration \/ VAL {itr} (val_loss={val_loss:.4f})\")\n                    break\n\n            if self.verbose and int(self.verbose_eval) > 0 and itr % int(self.verbose_eval) == 0:\n                grad_norm = np.linalg.norm(grads, axis=1).mean() * scale\n                print(f\"[iter {itr}] loss={loss:.4f} val_loss={val_loss:.4f} scale={scale:.4f} \"\n                      f\"norm={grad_norm:.4f}\")\n\n            if np.linalg.norm(proj_grad, axis=1).mean() < self.tol:\n                if self.verbose:\n                    print(f\"== Quitting at iteration \/ GRAD {itr}\")\n                break\n\n        return self\n\n    def fit_init_params_to_marginal(self, Y, iters=1000):\n        try:\n            E = Y['Event']\n            T = Y['Time'].reshape((-1, 1))[E == 1]\n        except:\n            T = Y\n        self.init_params = self.Dist.fit(T)\n        return\n\n\n    def pred_dist(self, X, max_iter=None):\n        params = np.asarray(self.pred_param(X, max_iter))\n        dist = self.Dist(params.T)\n        return dist\n\n    def predict(self, X):\n        dist = self.pred_dist(X)\n        return list(dist.loc.flatten())","91b53018":"# https:\/\/www.kaggle.com\/bgmello\/neural-networks-feature-engineering-for-the-win\nclass RAdam(keras.optimizers.Optimizer):\n    \"\"\"RAdam optimizer.\n    # Arguments\n        learning_rate: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n        decay: float >= 0. Learning rate decay over each update.\n        weight_decay: float >= 0. Weight decay for each param.\n        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n            algorithm from the paper \"On the Convergence of Adam and\n            Beyond\".\n        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n        min_lr: float >= 0. Minimum learning rate after warmup.\n    # References\n        - [Adam - A Method for Stochastic Optimization](https:\/\/arxiv.org\/abs\/1412.6980v8)\n        - [On the Convergence of Adam and Beyond](https:\/\/openreview.net\/forum?id=ryQu7f-RZ)\n        - [On The Variance Of The Adaptive Learning Rate And Beyond](https:\/\/arxiv.org\/pdf\/1908.03265v1.pdf)\n    \"\"\"\n\n    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n        learning_rate = kwargs.pop('lr', learning_rate)\n        super(RAdam, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n            self.total_steps = K.variable(total_steps, name='total_steps')\n            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n            self.min_lr = K.variable(min_lr, name='min_lr')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.initial_weight_decay = weight_decay\n        self.initial_total_steps = total_steps\n        self.amsgrad = amsgrad\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n\n        if self.initial_decay > 0:\n            lr = lr * (1. \/ (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n\n        if self.initial_total_steps > 0:\n            warmup_steps = self.total_steps * self.warmup_proportion\n            decay_steps = K.maximum(self.total_steps - warmup_steps, 1)\n            decay_rate = (self.min_lr - lr) \/ decay_steps\n            lr = K.switch(\n                t <= warmup_steps,\n                lr * (t \/ warmup_steps),\n                lr + decay_rate * K.minimum(t - warmup_steps, decay_steps),\n            )\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n        else:\n            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        beta_1_t = K.pow(self.beta_1, t)\n        beta_2_t = K.pow(self.beta_2, t)\n\n        sma_inf = 2.0 \/ (1.0 - self.beta_2) - 1.0\n        sma_t = sma_inf - 2.0 * t * beta_2_t \/ (1.0 - beta_2_t)\n\n        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n\n            m_corr_t = m_t \/ (1.0 - beta_1_t)\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                v_corr_t = K.sqrt(vhat_t \/ (1.0 - beta_2_t))\n                self.updates.append(K.update(vhat, vhat_t))\n            else:\n                v_corr_t = K.sqrt(v_t \/ (1.0 - beta_2_t))\n\n            r_t = K.sqrt((sma_t - 4.0) \/ (sma_inf - 4.0) *\n                         (sma_t - 2.0) \/ (sma_inf - 2.0) *\n                         sma_inf \/ sma_t)\n\n            p_t = K.switch(sma_t >= 5, r_t * m_corr_t \/ (v_corr_t + self.epsilon), m_corr_t)\n\n            if self.initial_weight_decay > 0:\n                p_t += self.weight_decay * p\n\n            p_t = p - lr * p_t\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    @property\n    def lr(self):\n        return self.learning_rate\n\n    @lr.setter\n    def lr(self, learning_rate):\n        self.learning_rate = learning_rate\n\n    def get_config(self):\n        config = {\n            'learning_rate': float(K.get_value(self.learning_rate)),\n            'beta_1': float(K.get_value(self.beta_1)),\n            'beta_2': float(K.get_value(self.beta_2)),\n            'decay': float(K.get_value(self.decay)),\n            'weight_decay': float(K.get_value(self.weight_decay)),\n            'epsilon': self.epsilon,\n            'amsgrad': self.amsgrad,\n            'total_steps': float(K.get_value(self.total_steps)),\n            'warmup_proportion': float(K.get_value(self.warmup_proportion)),\n            'min_lr': float(K.get_value(self.min_lr)),\n        }\n        base_config = super(RAdam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n#from https:\/\/www.kaggle.com\/davidcairuz\/nfl-neural-network-w-softmax\ndef crps_loss(y_true, y_pred):\n    return K.mean(K.square(y_true - K.cumsum(y_pred, axis=1)), axis=1)\ndef get_model(X_train):\n    x = keras.layers.Input(shape=[X_train.shape[1]])\n    fc1 = keras.layers.Dense(units=450, input_shape=[X_train.shape[1]])(x)\n    act1 = keras.layers.PReLU()(fc1)\n    bn1 = keras.layers.BatchNormalization()(act1)\n    dp1 = keras.layers.Dropout(0.55)(bn1)\n    gn1 = keras.layers.GaussianNoise(0.15)(dp1)\n    concat1 = keras.layers.Concatenate()([x, gn1])\n    fc2 = keras.layers.Dense(units=600)(concat1)\n    act2 = keras.layers.PReLU()(fc2)\n    bn2 = keras.layers.BatchNormalization()(act2)\n    dp2 = keras.layers.Dropout(0.55)(bn2)\n    gn2 = keras.layers.GaussianNoise(0.15)(dp2)\n    concat2 = keras.layers.Concatenate()([concat1, gn2])\n    fc3 = keras.layers.Dense(units=400)(concat2)\n    act3 = keras.layers.PReLU()(fc3)\n    bn3 = keras.layers.BatchNormalization()(act3)\n    dp3 = keras.layers.Dropout(0.55)(bn3)\n    gn3 = keras.layers.GaussianNoise(0.15)(dp3)\n    concat3 = keras.layers.Concatenate([concat2, gn3])\n    output = keras.layers.Dense(units=199, activation='softmax')(concat2)\n    model = keras.models.Model(inputs=[x], outputs=[output])\n    return model\ndef train_model(X_train, y_train, X_val, y_val, batch_size=64, epochs=100):\n    model = get_model(X_train)\n    model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-7), loss=crps_loss)\n    er = EarlyStopping(patience=20, min_delta=1e-4, restore_best_weights=True, monitor='val_loss')\n    model.fit(X_train, y_train, epochs=epochs, callbacks=[er], validation_data=[X_val, y_val], batch_size=batch_size)\n    return model","e142e90b":"# https:\/\/www.kaggle.com\/dandrocec\/location-eda-with-rusher-features#Let's-build-NN\n\nclass CRPSCallback(Callback):\n    def __init__(self,validation, predict_batch_size=20, include_on_batch=False):\n        super(CRPSCallback, self).__init__()\n        self.validation = validation\n        self.predict_batch_size = predict_batch_size\n        self.include_on_batch = include_on_batch\n\n    def on_batch_begin(self, batch, logs={}):\n        pass\n\n    def on_train_begin(self, logs={}):\n        if not ('CRPS_score_val' in self.params['metrics']):\n            self.params['metrics'].append('CRPS_score_val')\n\n    def on_batch_end(self, batch, logs={}):\n        if (self.include_on_batch):\n            logs['CRPS_score_val'] = float('-inf')\n\n    def on_epoch_end(self, epoch, logs={}):\n        logs['CRPS_score_val'] = float('-inf')\n            \n        if (self.validation):\n            X_valid, y_valid = self.validation[0], self.validation[1]\n            y_pred = self.model.predict(X_valid)\n            y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n            y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n            val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) \/ (199 * X_valid.shape[0])\n            val_s = np.round(val_s, 6)\n            logs['CRPS_score_val'] = val_s\n\ndef get_model2(x_tr,y_tr,x_val,y_val,epochs=100,batch_size=1024):\n    inp = Input(shape = (x_tr.shape[1],))\n    x = Dense(1024, input_dim=X.shape[1], activation='relu')(inp)\n    x = Dropout(0.5)(x)\n    x = BatchNormalization()(x)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = BatchNormalization()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = BatchNormalization()(x)\n    \n    out = Dense(199, activation='softmax')(x)\n    model = Model(inp,out)\n    model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-7), loss=crps_loss)\n    #add lookahead\n    #lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n    #lookahead.inject(model) # add into model\n    es = EarlyStopping(monitor='CRPS_score_val', \n                       mode='min',\n                       restore_best_weights=True, \n                       verbose=1, \n                       patience=10)\n    mc = ModelCheckpoint('best_model2.h5',monitor='CRPS_score_val',mode='min',\n                                   save_best_only=True, verbose=1, save_weights_only=True)\n    steps = x_tr.shape[0]\/batch_size\n    model.fit(x_tr, y_tr,callbacks=[CRPSCallback(validation = (x_val,y_val)),es,mc], epochs=epochs, batch_size=batch_size)\n    model.load_weights(\"best_model2.h5\")\n    \n    y_pred = model.predict(x_val)\n    y_valid = y_val\n    y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n    val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) \/ (199 * x_val.shape[0])\n    crps = np.round(val_s, 6)\n\n    return model,crps\n\ndef predict2(x_te, models2, batch_size=1024):\n    model_num = len(models2)\n    for k,m in enumerate(models2):\n        if k==0:\n            y_pred = m.predict(x_te,batch_size=batch_size)\n        else:\n            y_pred+=m.predict(x_te,batch_size=batch_size)\n            \n    y_pred = y_pred \/ model_num\n    \n    return y_pred","8f7d2bf3":"%%time\ntrain_df = pd.read_csv('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv', low_memory=False)","88c7a5d1":"train_df.shape","fc4f758c":"train_df.describe(include='all')","78fd1fd2":"train_df_orig = train_df.copy()","b3791d21":"train_df.shape","67469f2e":"stad_dict = {\n    'AT&T Stadium' : \"AT&T Stadium\",'Arrowhead Stadium' : \"Arrowhead Stadium\",'Bank of America Stadium' : \"Bank of America Stadium\",\n    'Broncos Stadium At Mile High' : \"Broncos Stadium At Mile High\",'Broncos Stadium at Mile High' : \"Broncos Stadium At Mile High\",\n    'CenturyField' : \"CenturyField\", 'CenturyLink':\"CenturyField\", 'CenturyLink Field':\"CenturyField\",\n    'Dignity Health Sports Park':\"Dignity Health Sports Park\",'Empower Field at Mile High':\"Empower Field at Mile High\",\n    'Estadio Azteca':\"Estadio Azteca\", 'EverBank Field':\"EverBank Field\",'Everbank Field': \"EverBank Field\", 'FedExField':\"FedExField\",\n    'FedexField':\"FedExField\", 'First Energy Stadium':\"First Energy Stadium\", 'FirstEnergy':\"First Energy Stadium\",\n    'FirstEnergy Stadium':\"First Energy Stadium\", 'FirstEnergyStadium':\"First Energy Stadium\", 'Ford Field':\"Ford Field\",\n    'Gillette Stadium':\"Gillette Stadium\", 'Hard Rock Stadium':\"Hard Rock Stadium\", 'Heinz Field':\"Heinz Field\",\n    'Lambeau Field':\"Lambeau Field\", 'Lambeau field':\"Lambeau Field\", 'Levis Stadium':\"Levis Stadium\",\n    'Lincoln Financial Field':\"Lincoln Financial Field\", 'Los Angeles Memorial Coliesum':\"Los Angeles Memorial Coliesum\",\n    'Los Angeles Memorial Coliseum':\"Los Angeles Memorial Coliesum\", 'Lucas Oil Stadium':\"Lucas Oil Stadium\",\n    'M & T Bank Stadium':\"M & T Bank Stadium\", 'M&T Bank Stadium':\"M & T Bank Stadium\", 'M&T Stadium':\"M & T Bank Stadium\",\n    'Mercedes-Benz Dome':\"Mercedes-Benz Dome\", 'Mercedes-Benz Stadium':\"Mercedes-Benz Stadium\",\n    'Mercedes-Benz Superdome' : \"Mercedes-Benz Dome\", 'MetLife':\"MetLife\", 'MetLife Stadium':\"MetLife\",\n    'Metlife Stadium':\"MetLife\", 'NRG':\"NRG\", 'NRG Stadium':\"NRG\", 'New Era Field':\"New Era Field\",\n    'Nissan Stadium':\"Nissan Stadium\", 'Oakland Alameda-County Coliseum':\"Oakland Alameda-County Coliseum\",\n    'Oakland-Alameda County Coliseum':\"Oakland Alameda-County Coliseum\", 'Paul Brown Stadium':\"Paul Brown Stadium\",\n    'Paul Brown Stdium':\"Paul Brown Stadium\", 'Raymond James Stadium':\"Raymond James Stadium\", 'Soldier Field':\"Soldier Field\",\n    'Sports Authority Field at Mile High':\"Sports Authority Field at Mile High\", 'State Farm Stadium':\"State Farm Stadium\",\n    'StubHub Center':\"StubHub Center\", 'TIAA Bank Field':\"TIAA Bank Field\", 'Tottenham Hotspur Stadium':\"Tottenham Hotspur Stadium\",\n    'Twickenham':\"Twickenham\", 'Twickenham Stadium':\"Twickenham\", 'U.S. Bank Stadium':\"U.S. Bank Stadium\",\n    'University of Phoenix Stadium':\"University of Phoenix Stadium\", 'Wembley Stadium':\"Wembley Stadium\"\n}\nstad_type_dict = {\n    'Bowl' : \"bowl\", 'Closed Dome': \"closed dome\", 'Cloudy' : \"outdoor\",\n    'Dome' : \"dome\",'Dome, closed' : \"closed dome\", 'Domed' : \"dome\",\n    'Domed, Open' : \"dome\",'Domed, closed' : \"closed dome\",\n    'Domed, open' : \"dome\",'Heinz Field' : \"heinz\",'Indoor' : \"indoor\",\n    'Indoor, Open Roof' : \"indoor open\",'Indoor, Roof Closed' : \"indoor\",\n    'Indoor, roof open' : \"indoor open\",'Indoors' : \"indoor\",'Open' : \"outdoor\",\n    'Oudoor' : \"outdoor\",'Ourdoor' : \"outdoor\",'Outddors' : \"outdoor\",\n    'Outdoor' : \"outdoor\",'Outdoor Retr Roof-Open' : \"retr open\",\n    'Outdoors' : \"outdoor\",'Outdor' : \"outdoor\",'Outside' : \"outdoor\",\n    'Retr. Roof - Closed' : \"retr closed\",'Retr. Roof - Open' : \"retr open\",\n    'Retr. Roof Closed' : \"retr closed\",'Retr. Roof-Closed' : \"retr closed\",\n    'Retr. Roof-Open' : \"retr open\",'Retractable Roof' : \"retr closed\",\n    'indoor' : \"indoor\"\n}\nturf_dict = {\n    'A-Turf Titan' : \"titan\", 'Artifical' : \"artificial\", 'Artificial' : \"artificial\",\n    'DD GrassMaster' : \"ddgrand\",'Field Turf' : \"field turf\",'Field turf' : \"field turf\",\n    'FieldTurf' : \"field turf\",'FieldTurf 360' : \"field turf 360\",\n    'FieldTurf360' : \"field turf 360\",'Grass' : \"grass\",'Natural' : \"natural\",\n    'Natural Grass' : \"natural\",'Natural grass' : \"natural\",'Naturall Grass' : \"natural\",\n    'SISGrass' : \"sisgrass\",'Turf' : \"field turf\",'Twenty Four\/Seven Turf' : \"24\/7 turf\",\n    'Twenty-Four\/Seven Turf' : \"24\/7 turf\",'UBU Speed Series-S5-M' : \"ubu speed s5m\",\n    'UBU Sports Speed S5-M' : \"ubu speed s5m\",'UBU-Speed Series-S5-M' : \"ubu speed s5m\",\n    'grass' : \"grass\",'natural grass' : \"natural\"\n}\ngame_weather_dict = {\n    \"Breezy\": \"clear\", \"Light rain\": \"rain-light\", \"Mostly Clear\" : \"clear\",\n    \"N\/A Indoors\": \"indoor\", \"Partly cloudy and mild\": \"sunny\", \"partly cloudy\": \"sunny\",\n    '30% Chance of Rain': \"cloudy\", 'Clear' : \"clear\", 'Clear Skies' : \"clear\",\n    'Clear and Cool' : \"clear-cool\",'Clear and Sunny' : \"sunny\",\n    'Clear and cold' : \"clear-cool\",'Clear and sunny' : \"sunny\",\n    'Clear and warm' : \"sunny\",'Clear skies' : \"clear\",\n    'Cloudy' : \"cloudy\",'Cloudy and Cool' : \"cloudy-cool\",'Cloudy and cold' : \"cloudy-cool\",\n    'Cloudy with periods of rain, thunder possible. Winds shifting to WNW, 10-20 mph.' : \"rainy\",\n    'Cloudy, 50% change of rain' : \"cloudy\",'Cloudy, Rain' : \"rainy\",\n    'Cloudy, chance of rain' : \"cloudy\",'Cloudy, fog started developing in 2nd quarter' : \"cloudy-cool\",\n    'Cloudy, light snow accumulating 1-3\"': \"snow\",'Cold' : \"cool\",'Controlled Climate' : \"controlled\",\n    'Coudy': \"cloudy\",'Fair': \"clear\",'Hazy' : \"haze\",'Heavy lake effect snow' : \"snow\",\n    'Indoor' : \"indoor\",'Indoors' : \"indoor\",'Light Rain' : \"rain-light\",'Mostly Cloudy' : \"cloudy\",\n    'Mostly Coudy' : \"cloudy\",'Mostly Sunny' : \"sunny\",'Mostly Sunny Skies' : \"sunny\",\n    'Mostly cloudy' : \"cloudy\",'Mostly sunny' : \"sunny\",'N\/A (Indoors)' : \"indoor\",\n    'N\/A Indoor' : \"indoor\",'Overcast' : \"cloudy\",'Partly Cloudy' : \"sunny\",'Partly Clouidy' : \"sunny\",\n    'Partly Sunny' : \"cloudy\",'Partly clear' : \"cloudy\",'Partly cloudy' : \"sunny\",\n    'Partly sunny' : \"cloudy\",'Party Cloudy' : \"sunny\",'Rain' : \"rainy\",'Rain Chance 40%' : \"cloudy\",\n    'Rain likely, temps in low 40s.' : \"rainy\",'Rain shower' : \"rain-light\",\n    'Rainy' : \"rainy\",'Scattered Showers' : \"rain-light\",'Showers' : \"rain-light\",'Snow' : \"snow\",\n    'Sun & clouds' : \"cloudy\",'Sunny' : \"sunny\",'Sunny Skies' : \"sunny\",'Sunny and clear' : \"sunny\",\n    'Sunny and cold' : \"sunny\",'Sunny and warm' : \"sunny\",'Sunny, Windy' : \"sunny\",\n    'Sunny, highs to upper 80s' : \"sunny\",'T: 51; H: 55; W: NW 10 mph' : \"sunny\",'cloudy' : \"cloudy\"\n}\nwind_speed_dict = {\n    \"6mph\": \"6\",\"10mph\": \"10\",\"9mph\": \"9\",\"14 Gusting to 24\": \"14-24\",\n    \"6 mph, Gusts to 10\": \"6-10\",\"13 MPH\": \"13\",\"4 MPh\": \"4\",\n    \"15 gusts up to 25\": \"15-25\",\"10MPH\": \"10\",\"10mph\": \"10\",\n    \"7 MPH\": \"7\",\"Calm\": \"0\",\"6 mph\": \"6\",\"12mph\": \"12\"\n}\nwind_direc_dict = { 'E' : \"e\", 'EAST' : \"e\",'ENE' : \"ene\",'ESE' : \"ese\",'East' : \"e\",\n    'East North East' : \"ene\",'East Southeast' : \"ese\",'From ESE' : \"ese\",'From NNE' : \"nne\",\n    'From NNW' : \"nnw\",'From S' : \"s\",'From SSE' : \"sse\",'From SSW' : \"ssw\",\n    'From SW' : \"sw\",'From W' : \"w\",'From WSW' : \"wsw\",'N' : \"n\",'N-NE' : \"nne\",\n    'NE' : \"ne\",'NNE' : \"nne\",'NNW' : \"nnw\",'NW' : \"nw\",'North': \"n\",'North East' : \"ne\",\n    'North\/Northwest' : \"nne\",'NorthEast' : \"ne\",'Northeast' : \"ne\",'Northwest' : \"nw\",\n    'S' : \"s\",'SE' : \"se\",'SSE' : \"sse\",'SSW' : \"ssw\",\"S-SW\": \"ssw\",'SW' : \"sw\",'South' : \"s\",\n    'South Southeast' : \"sse\",'South Southwest' : \"ssw\",'SouthWest' : \"sw\",'Southeast' : \"se\",\n    'Southwest' : \"sw\",'W' : \"w\",'W-NW' : \"wnw\",'W-SW' : \"wsw\",'WNW' : \"wnw\",'WSW' : \"wsw\",\n    'West' : \"w\",'West Northwest' : \"wnw\",'West-Southwest' : \"wsw\",'from W' : \"w\",'s' : \"s\"\n}","42338d18":"map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\nfor abb in train_df['PossessionTeam'].unique():\n    map_abbr[abb] = abb","c6df2000":"from sklearn.preprocessing import LabelEncoder\nfrom functools import partial\n\ndef label_encode(df, col, lb=None, extra_keys=[]):\n    if lb is None: \n        lb = LabelEncoder()\n        lb.fit(df[col].unique().tolist() + extra_keys)\n    df[col] = lb.transform(df[col])\n    return lb\n\ndef _find_parts(x):\n    if x in [np.nan, None]: return {}\n    spl1 = [t.strip() for t in x.split(\",\")]\n    dict_ = {s.split(\" \")[1]:np.int(s.split(\" \")[0]) for s in spl1}\n    return dict_\n\ndef handle_offence_defence(df, col):\n    cols_add = None\n    if col == \"DefensePersonnel\": cols_add = ['DB', 'DL', 'LB', 'OL', 'RB']\n    elif col == \"OffensePersonnel\": cols_add = ['DB', 'DL', 'LB', 'OL', 'QB', 'RB', 'TE', 'WR']\n    else: raise Exception()\n    mapper = {el: 0 for el in cols_add}\n    #print(mapper.keys())\n    for el in mapper.keys():\n        df[col + f\"_{el}\"] = df[col].map(lambda x: _find_parts(x)[el] if el in _find_parts(x).keys() else 0)\n        \ndef preprocess(df, lbs=None, print_=False):\n    #for col in df.columns:\n    #    if col in [\"GameClock\", \"TimeHandoff\", \"TimeSnap\",\n    #               \"PlayerBirthDate\"]: continue\n    #    if df[col].dtype in [\"object\"]:\n    #        print(\":::: \", col, \" ::::\")\n    #        print(df[col].nunique())\n    #        print(df[col].sort_values().unique())\n    #        print(\"\\n\\n\")\n    #yards = df1[\"Yards\"].copy()\n    #df1.drop([\"Yards\"], axis=1, inplace=True)\n    #df = pd.concat([df1, df2], axis=0)\n    get_lbs = False\n    if lbs is None:\n        lbs = {}\n        get_lbs = True\n    \n    df['PossessionTeam'] = df['PossessionTeam'].map(map_abbr)\n    df['HomeTeamAbbr'] = df['HomeTeamAbbr'].map(map_abbr)\n    df['VisitorTeamAbbr'] = df['VisitorTeamAbbr'].map(map_abbr)\n    # https:\/\/www.kaggle.com\/cpmpml\/initial-wrangling-voronoi-areas-in-python\n    df['ToLeft'] = df.PlayDirection == \"left\"\n    df['IsBallCarrier'] = df.NflId == df.NflIdRusher\n    df['TeamOnOffense'] = \"home\"\n    df.loc[df.PossessionTeam != df.HomeTeamAbbr, 'TeamOnOffense'] = \"away\"\n    df['IsOnOffense'] = df.Team == df.TeamOnOffense # Is player on offense?\n    df['TeamOnOffense'] = df['TeamOnOffense'] == \"home\"\n    df['YardLine_std'] = 100 - df.YardLine\n    df.loc[df.FieldPosition.fillna('') == df.PossessionTeam,  \n              'YardLine_std'\n             ] = df.loc[df.FieldPosition.fillna('') == df.PossessionTeam,  \n              'YardLine']\n    df['X_std'] = df.X\n    df.loc[df.ToLeft, 'X_std'] = 120 - df.loc[df.ToLeft, 'X'] \n    df['Y_std'] = df.Y\n    df.loc[df.ToLeft, 'Y_std'] = 160\/3 - df.loc[df.ToLeft, 'Y']\n    df['Dir_rad'] = np.mod(90 - df.Dir, 360) * math.pi\/180.0\n    df['Dir_rad'] = df['Dir_rad'].fillna(df['Dir_rad'].mean())\n    df['Dir_std'] = df.Dir_rad\n    df.loc[df.ToLeft, 'Dir_std'] = np.mod(np.pi + df.loc[df.ToLeft, 'Dir_rad'], 2*np.pi)\n    df['Dir_std'] = df['Dir_std'].fillna(df['Dir_std'].mean())\n    df['Orientation_rad'] = np.mod(df.Orientation, 360) * math.pi\/180.0\n    df.loc[df.Season >= 2018, 'Orientation_rad'\n         ] = np.mod(df.loc[df.Season >= 2018, 'Orientation'] - 90, 360) * math.pi\/180.0\n    df['Orientation_rad'] = df['Orientation_rad'].fillna(df['Orientation_rad'].mean())\n    if print_: print(\"Part I. Done.\")\n    # https:\/\/www.kaggle.com\/anandavati\/ngboost-for-nfl\/\n    feat_names = ['Fmap0_r3q0', 'Emap0_r3q0', 'Fmap0_r3q1', 'Emap0_r3q1', 'Fmap0_r3q2', 'Emap0_r3q2', 'Fmap0_r3q3', 'Emap0_r3q3', 'Fmap0_r3q4', 'Emap0_r3q4', 'Fmap0_r3q5', 'Emap0_r3q5', 'Fmap0_r3q6', 'Emap0_r3q6', 'Fmap0_r3q7', 'Emap0_r3q7', 'Fmap0_r10q0', 'Emap0_r10q0', 'Fmap0_r10q1', 'Emap0_r10q1', 'Fmap0_r10q2', 'Emap0_r10q2', 'Fmap0_r10q3', 'Emap0_r10q3', 'Fmap0_r10q4', 'Emap0_r10q4', 'Fmap0_r10q5', 'Emap0_r10q5', 'Fmap0_r10q6', 'Emap0_r10q6', 'Fmap0_r10q7', 'Emap0_r10q7', 'Fmap0_r30q0', 'Emap0_r30q0', 'Fmap0_r30q1', 'Emap0_r30q1', 'Fmap0_r30q2', 'Emap0_r30q2', 'Fmap0_r30q3', 'Emap0_r30q3', 'Fmap0_r30q4', 'Emap0_r30q4', 'Fmap0_r30q5', 'Emap0_r30q5', 'Fmap0_r30q6', 'Emap0_r30q6', 'Fmap0_r30q7', 'Emap0_r30q7', 'Fmap0_r120q0', 'Emap0_r120q0', 'Fmap0_r120q1', 'Emap0_r120q1', 'Fmap0_r120q2', 'Emap0_r120q2', 'Fmap0_r120q3', 'Emap0_r120q3', 'Fmap0_r120q4', 'Emap0_r120q4', 'Fmap0_r120q5', 'Emap0_r120q5', 'Fmap0_r120q6', 'Emap0_r120q6', 'Fmap0_r120q7', 'Emap0_r120q7']\n    for col in feat_names: df[col] = np.nan\n\n    @nb.njit()\n    def calc(friends_R0, friends_Theta0, enemies_R0, enemies_Theta0, result):\n        itr = 0\n        Rs = [0, 3, 10, 30, 120]\n        for r in range(len(Rs)-1):\n            rl, rh = Rs[r], Rs[r+1]\n            for t in range(8):\n                tl, th = t * np.pi\/8, (t+1) * np.pi\/8\n                result[itr] = (np.sum((rl <= friends_R0) & (friends_R0 < rh) & (tl <= friends_Theta0) & (friends_Theta0 < th)))\n                itr+=1\n                result[itr] = (np.sum((rl <= enemies_R0) & (enemies_R0 < rh) & (tl <= enemies_Theta0) & (enemies_Theta0 < th)))\n                itr+=1\n        return result\n    def func(gp):\n        ball_XY = gp.loc[gp[\"NflId\"]==gp[\"NflIdRusher\"], [\"X\", \"Y\"]].values[0]\n        friends_rel_XY = gp.loc[gp[\"Team\"]==gp[\"TeamOnOffense\"], [\"X\", \"Y\"]].values - ball_XY\n        enemy_rel_XY = gp.loc[gp[\"Team\"]!=gp[\"TeamOnOffense\"], [\"X\", \"Y\"]].values - ball_XY\n\n        friends_R0 = np.sqrt(np.power(friends_rel_XY[:, 0], 2) + np.power(friends_rel_XY[:, 1], 2))\n        enemy_R0 = np.sqrt(np.power(enemy_rel_XY[:, 0], 2) + np.power(enemy_rel_XY[:, 1], 2))\n\n        friends_Theta0 = np.arctan2(friends_rel_XY[:, 0], friends_rel_XY[:, 1])\n        enemy_Theta0 = np.arctan2(enemy_rel_XY[:, 0], enemy_rel_XY[:, 1])\n\n        features = np.empty((len(feat_names), ))\n        features = calc(friends_R0, friends_Theta0, enemy_R0, enemy_Theta0, features)\n\n        return pd.DataFrame([features] * gp.shape[0], index=gp.index)\n    df.loc[:, feat_names] = df.groupby(\"PlayId\").apply(func).values\n    if print_: print(\"Part II. Done.\")\n    # https:\/\/stackoverflow.com\/questions\/24467972\/calculate-area-of-polygon-given-x-y-coordinates\n    from functools import partial\n    def PolyArea(x,y):\n        return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n    def func(mdf, type_=1):\n        xy = mdf[['X_std', 'Y_std']].values\n        n_points = xy.shape[0]\n        offense = mdf.IsOnOffense.values\n        vor = Voronoi(xy)\n        def_area = 0\n        off_area = 0\n        for r in range(n_points):\n            region = vor.regions[vor.point_region[r]]\n            if not -1 in region:\n                polygon = [vor.vertices[i] for i in region]\n                if offense[r]: off_area += PolyArea(*zip(*polygon))\n                else: def_area += PolyArea(*zip(*polygon))\n        if type_ == 1: return def_area\n        elif type_ == 2: return off_area\n    area_def = df.groupby(\"PlayId\").apply(partial(func, type_=1))\n    df[\"DefenceAreaCovered\"] = df[\"PlayId\"].map(area_def)\n    area_off = df.groupby(\"PlayId\").apply(partial(func, type_=2))\n    df[\"OffenceAreaCovered\"] = df[\"PlayId\"].map(area_off)\n    df[\"Area_diff\"] = df[\"DefenceAreaCovered\"] - df[\"OffenceAreaCovered\"]\n    if print_: print(\"Part III. Done.\")\n    # https:\/\/www.kaggle.com\/scirpus\/hybrid-gp-and-nn\n    if \"Yards\" in df.columns: outcomes = df[['GameId','PlayId','Yards']].drop_duplicates()\n    def new_X(x_coordinate, play_direction):\n        if play_direction == 'left':\n            return 120.0 - x_coordinate\n        else:\n            return x_coordinate\n    def new_line(rush_team, field_position, yardline):\n        if rush_team == field_position:\n            # offense starting at X = 0 plus the 10 yard endzone plus the line of scrimmage\n            return 10.0 + yardline\n        else:\n            # half the field plus the yards between midfield and the line of scrimmage\n            return 60.0 + (50 - yardline)\n    def euclidean_distance(x1,y1,x2,y2):\n        x_diff = (x1-x2)**2\n        y_diff = (y1-y2)**2\n\n        return np.sqrt(x_diff + y_diff)\n    def back_direction(orientation):\n        if orientation > 180.0: return 1\n        else: return 0\n    def back_features(df):\n        carriers = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','NflIdRusher','X','Y','Orientation','Dir','YardLine']]\n        carriers['back_from_scrimmage'] = carriers['YardLine'] - carriers['X']\n        carriers['back_oriented_down_field'] = carriers['Orientation'].apply(lambda x: back_direction(x))\n        carriers['back_moving_down_field'] = carriers['Dir'].apply(lambda x: back_direction(x))\n        carriers = carriers.rename(columns={'X':'back_X',\n                                            'Y':'back_Y'})\n        carriers = carriers[['GameId','PlayId','NflIdRusher','back_X','back_Y','back_from_scrimmage',\n                             'back_oriented_down_field','back_moving_down_field']]\n\n        return carriers\n    def features_relative_to_back(df, carriers):\n        player_distance = df[['GameId','PlayId','NflId','X','Y']]\n        player_distance = pd.merge(player_distance, carriers, on=['GameId','PlayId'], how='inner')\n        player_distance = player_distance[player_distance['NflId'] != player_distance['NflIdRusher']]\n        player_distance['dist_to_back'] = player_distance[['X','Y','back_X','back_Y']]\\\n                                                    .apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n\n        player_distance = player_distance.groupby(['GameId','PlayId','back_from_scrimmage','back_oriented_down_field',\n                                                   'back_moving_down_field'])\\\n                                         .agg({'dist_to_back':['min','max','mean','std']})\\\n                                         .reset_index()\n        player_distance.columns = ['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field',\n                                   'min_dist','max_dist','mean_dist','std_dist']\n\n        return player_distance\n    def defense_features(df):\n        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','Y']]\n        rusher.columns = ['GameId','PlayId','RusherTeam','RusherX','RusherY']\n        defense = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n        defense = defense[defense['Team'] != defense['RusherTeam']][['GameId','PlayId','X','Y','RusherX','RusherY']]\n        defense.loc[:, 'def_dist_to_back'] = defense[['X','Y','RusherX','RusherY']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n\n        defense = defense.groupby(['GameId','PlayId'])\\\n                         .agg({'def_dist_to_back':['min','max','mean','std']})\\\n                         .reset_index()\n        defense.columns = ['GameId','PlayId','def_min_dist','def_max_dist','def_mean_dist','def_std_dist']\n\n        return defense\n    def static_features(df):\n        static_features = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','X','Y','S','A','Dis','Orientation','Dir',\n                                                            'YardLine','Quarter','Down','Distance','DefendersInTheBox']].drop_duplicates()\n        static_features['DefendersInTheBox'] = static_features['DefendersInTheBox'].fillna(np.mean(static_features['DefendersInTheBox']))\n\n        return static_features\n    def combine_features(df, relative_to_back, defense, rushing, static, deploy=False):\n        tdf = pd.merge(relative_to_back,defense,on=['GameId','PlayId'],how='inner')\n        tdf = pd.merge(tdf,rushing,on=['GameId','PlayId'],how='inner')\n        tdf = pd.merge(tdf,static,on=['GameId','PlayId'],how='inner')\n\n        if not deploy:\n            tdf = pd.merge(tdf, outcomes, on=['GameId','PlayId'], how='inner')\n\n        return pd.merge(df.loc[:, ['GameId','PlayId'] + [v for v in df.columns if v not in tdf.columns]], tdf, on=['GameId','PlayId'], how='inner')\n    def rusher_features(df):\n        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Dir', 'S', 'A', 'X', 'Y']]\n        rusher.columns = ['GameId','PlayId', 'RusherDir', 'RusherS', 'RusherA', 'RusherX', 'RusherY']\n        \n        radian_angle = (90 - rusher['RusherDir']) * np.pi \/ 180.0\n        v_horizontal = np.abs(rusher['RusherS'] * np.cos(radian_angle))\n        v_vertical = np.abs(rusher['RusherS'] * np.sin(radian_angle)) \n        \n        rusher['v_horizontal'] = v_horizontal\n        rusher['v_vertical'] = v_vertical\n        rusher.columns = ['GameId','PlayId', 'RusherDir', 'RusherS','RusherA','RusherX', 'RusherY','v_horizontal', 'v_vertical']\n        return rusher\n    back_feats = back_features(df)\n    rel_back = features_relative_to_back(df, back_feats)\n    def_feats = defense_features(df)\n    rush_feats = rusher_features(df)\n    static_feats = static_features(df)\n    if \"Yards\" in df.columns:\n        df = combine_features(df, rel_back, def_feats, rush_feats, static_feats, deploy=False)\n    else: df = combine_features(df, rel_back, def_feats, rush_feats, static_feats, deploy=True)\n    if print_: print(\"Part IV. Done.\")\n    #if get_lbs: lbs[\"Team\"] = label_encode(df, \"Team\")\n    #else: label_encode(df, \"Team\", lb=lbs[\"Team\"])\n    df['Team'] = df['Team'].apply(lambda x: x.strip()=='home')\n    # https:\/\/www.kaggle.com\/anandavati\/ngboost-for-nfl\/\n    # df.Team == df.TeamOnOffense\n    df[\"FriendScore\"] = df[[\"HomeScoreBeforePlay\", \"VisitorScoreBeforePlay\", \"Team\", \"TeamOnOffense\"]].apply(lambda x: x[0] if (x[2]==x[3]) else x[1], axis=1)\n    df[\"EnemyScore\"] = df[[\"HomeScoreBeforePlay\", \"VisitorScoreBeforePlay\", \"Team\", \"TeamOnOffense\"]].apply(lambda x: x[1] if (x[2]!=x[3]) else x[0], axis=1)\n    df = df.drop([\"HomeScoreBeforePlay\", \"VisitorScoreBeforePlay\"], axis=1)\n    df['PlayDirection'] = df['PlayDirection'].apply(lambda x: x.strip() == 'right')\n    # length of arr = 250\n    players_not_in_train = ['Darnell Savage', 'David Montgomery', 'Duke Shelley', 'Sebastian Joseph-Day', 'Brian Burns', 'Taylor Rapp', 'Christian Miller', 'Darrell Henderson', 'Travin Howard', 'Dennis Daley', 'Isaiah Mack', 'Jamil Douglas', 'Greedy Williams', 'A.J. Brown', 'Daniel Ekuale', 'Amani Hooker', \"D'Ernest Johnson\", 'Quincy Williams', 'Juan Thornhill', 'Will Richardson', 'Jawaan Taylor', 'Mecole Hardman', 'Byron Pringle', 'Joey Ivie', 'Gardner Minshew', 'Andrew Wingard', 'Dontavius Russell', 'Ryquell Armstead', 'Marquise Brown', 'Miles Boykin', 'Sam Eguavoen', 'Jonathan Ledbetter', 'Christian Wilkins', 'Justice Hill', 'Michael Deiter', 'Preston Williams', 'Jomal Wiltz', 'Chandler Cox', 'Chris Lammons', 'Patrick Mekari', 'Steven Parker', 'DeShon Elliott', 'James Crawford', 'Chris Lindstrom', 'Kaleb McGary', 'Garrett Bradbury', 'Irv Smith', 'Kris Boyd', 'Alexander Mattison', 'Bisi Johnson', 'Brandon Dillon', 'Jaeden Graham', 'Ed Oliver', 'Darryl Johnson', 'Cody Ford', 'Tommy Sweeney', 'Trevon Wesco', 'Dawson Knox', 'Quinnen Williams', 'Devin Singletary', 'Blake Cashman', 'Derrius Guice', 'Terry McLaurin', 'Kelvin Harmon', 'Montez Sweat', 'Jimmy Moreland', 'Cole Holcomb', 'Miles Sanders', 'Andre Dillard', 'J.J. Arcega-Whiteside', 'Rock Ya-Sin', 'Deon Cain', 'Ben Banogu', 'Parris Campbell', 'Jerry Tillery', 'Bobby Okereke', 'Khari Willis', 'D.K. Metcalf', 'Mike Jordan', 'Damion Willis', 'Bryan Mone', 'Germaine Pratt', 'Drew Sample', 'Ugo Amadi', 'Renell Wren', 'T.J. Hockenson', 'Zach Allen', 'Byron Murphy', 'Nick Bawden', 'Jahlani Tavai', 'Kyler Murray', 'KeeSean Johnson', 'Kevin Strong', 'Ty Johnson', 'Nick Gates', 'Deandre Baker', 'Dexter Lawrence', 'Oshane Ximines', 'Devin Smith', 'Tony Pollard', 'R.J. McIntosh', 'Ryan Connelly', 'Joe Jackson', 'Corey Ballentine', 'Dre Greenlaw', 'Deebo Samuel', 'Vernon Hargreaves III', 'Devin White', 'Nick Bosa', 'Dare Ogunbowale', 'Anthony Nelson', 'Emmanuel Moseley', 'Sean Murphy-Bunting', 'Mike Edwards', 'Kam Kelly', 'Devin Bush', 'Ryan Izzo', 'Isaiah Wynn', 'Chase Winovich', 'Jakobi Meyers', 'Gunner Olszewski', 'Shy Tuttle', 'Erik McCoy', 'Chauncey Gardner-Johnson', 'Alec Ingold', 'Josh Jacobs', 'Foster Moreau', 'Troy Fumagalli', 'Noah Fant', 'Dalton Risner', 'Clelin Ferrell', 'Johnathan Abram', 'Hunter Renfrow', 'Maxx Crosby', 'Justin Hollins', 'Malik Reed', 'Mike Purcell', 'Trayvon Mullen', 'Deionte Thompson', 'Miles Brown', 'Justin Skule', 'Azeez Al-Shaair', 'Elgton Jenkins', 'Rashan Gary', 'Will Redmond', 'Lonnie Johnson', 'Roderick Johnson', 'Tytus Howard', 'Charles Omenihu', 'Cullen Gillaspia', 'Byron Cowart', 'Ken Webster', 'Lano Hill', 'L.J. Collier', 'Benny Snell', 'Mason Rudolph', 'Diontae Johnson', 'Steven Sims', 'Robert Davis', 'Ced Wilson', 'Simeon Thomas', 'Keisean Nixon', 'Keelan Doss', 'Darwin Thompson', 'Andrew Beck', 'Diontae Spencer', \"Dre'Mont Jones\", 'Greg Gaines', 'Jamil Demby', 'Kendall Sheffield', 'Craig James', 'John Cominsky', 'Luke Falk', 'Kyle Phillips', 'Sheldrick Redwine', 'Mack Wilson', 'Aaron Stinnie', 'Andrew Brown', 'Trent Harris', 'Trysten Hill', 'Shaq Calhoun', 'Allen Lazard', 'Darrius Shepherd', 'Juwann Winfree', 'Kingsley Keke', 'E.J. Speed', 'Jaylon Ferguson', \"Hercules Mata'afa\", 'Marcus Epps', 'Nathan Meadors', 'Braxton Berrios', 'Will Harris', 'Greg Ward', 'Greg Little', 'Reggie Bonnafon', 'Daniel Jones', 'Scott Miller', 'Darius Slayton', 'Tuzar Skipper', 'Max Scharping', 'Roderic Teamer', 'Daniel Brunskill', 'Zach Gentry', 'Abdullah Anderson', 'J.P. Holtz', 'Nate Davis', 'Daylon Mack', 'Jakob Johnson', 'Tom Kennedy', 'C.J. Moore', 'Ashton Dulin', 'Troymaine Pope', 'Isaiah Prince', 'Andre Patton', 'Nasir Adderley', 'Drue Tranquill', 'Cortez Broughton', 'Trey Pipkins', 'Wes Martin', 'Jon Hilliman', 'Dwayne Haskins', 'Andy Isabella', 'Troy Reeder', 'Jonathan Harris', 'Duke Dawson', 'Fred Brown', 'Deonte Harris', 'Carl Granderson', \"Lil'Jordan Humphrey\", 'Jay Elliott', 'Jamarco Jones', 'Jalen Thompson', 'Michael Dogbe', 'Stanley Morgan', 'Josiah Tauaefa', 'Chuma Edoga', 'T.J. Edwards', 'Devlin Hodges', 'Duke Williams', 'Ryan Bates', 'Jarrett Stidham', 'Joejuan Williams', 'A.J. Johnson', 'Davontae Harris', 'Brandon Knight', 'Shakial Taylor', 'Marvell Tell', 'Ryan Hunter', 'Khalen Saunders', 'Deon Yelder']\n    if get_lbs: lbs[\"DisplayName\"] = label_encode(df, \"DisplayName\", extra_keys=players_not_in_train)\n    else: label_encode(df, \"DisplayName\", lb=lbs[\"DisplayName\"])\n    df.drop([\"DisplayName\"], axis=1, inplace=True)\n    df[\"GameClock\"] = pd.to_datetime(df[\"GameClock\"], format=\"%M:%S:00\")\n    df[\"GameClock_minute\"] = df[\"GameClock\"].dt.minute\n    df[\"GameClock_second\"] = df[\"GameClock\"].dt.second\n    df.drop(\"GameClock\", axis=1, inplace=True)\n    if print_: print(\"Part V. Done.\")\n    # https:\/\/www.kaggle.com\/bgmello\/neural-networks-feature-engineering-for-the-win\n    def new_orientation(angle, play_direction):\n        if play_direction == 0:\n            new_angle = 360.0 - angle\n            if new_angle == 360.0:\n                new_angle = 0.0\n            return new_angle\n        else:\n            return angle\n    df['Orientation'] = df.apply(lambda row: new_orientation(row['Orientation'], row['PlayDirection']), axis=1)\n    df['Dir'] = df.apply(lambda row: new_orientation(row['Dir'], row['PlayDirection']), axis=1)\n    df['IsRusher'] = (df['NflId'] == df['NflIdRusher'])\n    df['HomePossesion'] = df['PossessionTeam'] == df['HomeTeamAbbr']\n    df['Field_eq_Possession'] = df['FieldPosition'] == df['PossessionTeam']\n    df['HomeField'] = df['FieldPosition'] == df['HomeTeamAbbr']\n    df['YardsLeft'] = df.apply(lambda row: 100-row['YardLine'] if row['HomeField'] else row['YardLine'], axis=1)\n    df['YardsLeft'] = df.apply(lambda row: row['YardsLeft'] if row['PlayDirection'] else 100-row['YardsLeft'], axis=1)\n    df['X'] = df[['X', 'PlayDirection']].apply(lambda x: x['X'] if x['PlayDirection'] else 120-x['X'], axis=1)\n    if \"Yards\" in df.columns: df.drop(df.index[(df['YardsLeft']<df['Yards']) | (df['YardsLeft']-100>df['Yards'])], inplace=True)\n    \n    if get_lbs: lbs[\"PossessionTeam\"] = label_encode(df, \"PossessionTeam\")\n    else: label_encode(df, \"PossessionTeam\", lb=lbs[\"PossessionTeam\"])\n    df[\"FieldPosition\"] = df[\"FieldPosition\"].fillna(\"ZZZ\")\n    if get_lbs: lbs[\"FieldPosition\"] = label_encode(df, \"FieldPosition\")\n    else: label_encode(df, \"FieldPosition\", lb=lbs[\"FieldPosition\"])\n    df[\"OffenseFormation\"] = df[\"OffenseFormation\"].fillna(\"ZZZ\")\n    if get_lbs: lbs[\"OffenseFormation\"] = label_encode(df, \"OffenseFormation\")\n    else: label_encode(df, \"OffenseFormation\", lb=lbs[\"OffenseFormation\"])\n    if print_: print(\"Part VI. Done.\")\n    #if get_lbs: lbs[\"OffensePersonnel\"] = label_encode(df, \"OffensePersonnel\", extra_keys=['7 OL, 1 RB, 1 TE, 0 WR,1 LB',\n    #                                                                                                   '2 QB, 6 OL, 1 RB, 1 TE, 1 WR'])\n    #else: label_encode(df, \"OffensePersonnel\", lb=lbs[\"OffensePersonnel\"])\n    #if get_lbs: lbs[\"DefensePersonnel\"] = label_encode(df, \"DefensePersonnel\", extra_keys=['0 DL, 4 LB, 6 DB, 1 RB',\n    #                                                                                                     '1 DL, 4 LB, 5 DB, 1 RB',\n    #                                                                                                     '2 DL, 4 LB, 4 DB, 1 RB',\n    #                                                                                                     '2 DL, 3 LB, 5 DB, 1 RB',\n    #                                                                                                     '1 DL, 3 LB, 6 DB, 1 RB',\n    #                                                                                                     '3 DL, 4 LB, 3 DB, 1 RB'])\n    #else: label_encode(df, \"DefensePersonnel\", lb=lbs[\"DefensePersonnel\"])\n    handle_offence_defence(df, \"OffensePersonnel\")\n    handle_offence_defence(df, \"DefensePersonnel\")\n    df = df.drop([\"OffensePersonnel\", \"DefensePersonnel\"], axis=1)\n    #if get_lbs: lbs[\"PlayDirection\"] = label_encode(df, \"PlayDirection\")\n    #else: label_encode(df, \"PlayDirection\", lb=lbs[\"PlayDirection\"])\n    if print_: print(\"Part VII. Done.\")\n    df[\"TimeHandoff\"] = pd.to_datetime(df[\"TimeHandoff\"], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n    df[\"TimeHandoff_minute\"] = df[\"TimeHandoff\"].dt.minute\n    df[\"TimeHandoff_second\"] = df[\"TimeHandoff\"].dt.second\n    #df[\"TimeHandoff_microsecond\"] = df[\"TimeHandoff\"].dt.microsecond\n    df[\"TimeSnap\"] = pd.to_datetime(df[\"TimeSnap\"], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n    df[\"TimeSnap_minute\"] = df[\"TimeSnap\"].dt.minute\n    df[\"TimeSnap_second\"] = df[\"TimeSnap\"].dt.second\n    df['TimeDelta'] = (df['TimeHandoff'] - df['TimeSnap']).dt.seconds\n    #df[\"TimeSnap_microsecond\"] = df[\"TimeSnap\"].dt.microsecond\n    if print_: print(\"Part VIII. Done.\")\n    df[\"PlayerHeight\"] = df[\"PlayerHeight\"].map(lambda x: np.float(x.replace(\"-\", \".\")))\n    # https:\/\/www.kaggle.com\/bgmello\/neural-networks-feature-engineering-for-the-win\n    df['PlayerBMI'] = 703*(df['PlayerWeight']\/(df['PlayerHeight'])**2)\n    df[\"PlayerBirthDate\"] = pd.to_datetime(df[\"PlayerBirthDate\"], format=\"%m\/%d\/%Y\")\n    df[\"PlayerBirthDate_year\"] = df[\"PlayerBirthDate\"].dt.year\n    df[\"PlayerBirthDate_month\"] = df[\"PlayerBirthDate\"].dt.month\n    df[\"PlayerBirthDate_day\"] = df[\"PlayerBirthDate\"].dt.day\n    seconds_in_year = 60*60*24*365.25\n    df['PlayerAge'] = (df['TimeHandoff']-df['PlayerBirthDate']).dt.seconds \/ seconds_in_year\n    df.drop(\"PlayerBirthDate\", axis=1, inplace=True)\n    df.drop([\"TimeHandoff\", \"TimeSnap\"], axis=1, inplace=True)\n    df[\"PlayerCollegeName\"] = df[\"PlayerCollegeName\"].map(lambda x: re.sub(\"[\\s]+\", \" \", x.lower().replace(\"state\", \"\")).strip())\n    if get_lbs: lbs[\"PlayerCollegeName\"] = label_encode(df, \"PlayerCollegeName\", extra_keys=['murray',\n                                                                                                         'bemidji',\n                                                                                                         'california-davis',\n                                                                                                         'mcneese',\n                                                                                                         'charleston, w. va.',\n                                                                                                         'tarleton',\n                                                                                                         'bowling green',\n                                                                                                         'bryant',\n                                                                                                         'malone',\n                                                                                                         'sioux falls'])\n    else: label_encode(df, \"PlayerCollegeName\", lb=lbs[\"PlayerCollegeName\"])\n    if get_lbs: lbs[\"Position\"] = label_encode(df, \"Position\")\n    else: label_encode(df, \"Position\", lb=lbs[\"Position\"])\n    if get_lbs: lbs[\"HomeTeamAbbr\"] = label_encode(df, \"HomeTeamAbbr\")\n    else: label_encode(df, \"HomeTeamAbbr\", lb=lbs[\"HomeTeamAbbr\"])\n    if get_lbs: lbs[\"VisitorTeamAbbr\"] = label_encode(df, \"VisitorTeamAbbr\")\n    else: label_encode(df, \"VisitorTeamAbbr\", lb=lbs[\"VisitorTeamAbbr\"])\n    df[\"Stadium\"] = df[\"Stadium\"].map(lambda x: stad_dict[x] if x in stad_dict.keys() else x)\n    if get_lbs: lbs[\"Stadium\"] = label_encode(df, \"Stadium\", extra_keys=[stad_dict[v] for v in ['Dignity Health Sports Park',\n                                                                                                             'Empower Field at Mile High',\n                                                                                                             'FedexField',\n                                                                                                             'Tottenham Hotspur Stadium']])\n    else: label_encode(df, \"Stadium\", lb=lbs[\"Stadium\"])\n    if print_: print(\"Part IX. Done.\")\n    def func(x):\n        x = x.replace(\"Texas\", \"TX\").replace(\"Maryland\", \"Md\")\\\n             .replace(\"e North Carolina\", \"e, NC\").replace(\"North Carolina\", \"NC\")\\\n             .replace(\". IL\", \", IL\").replace(\"r CO\", \"r, CO\").replace(\"E. \", \"East \")\\\n             .replace(\"d Ohio\", \"d, Ohio\")\\\n             .replace(\"Ohio\", \"OH\").replace(\"e Florida\", \"e, FL\").replace(\"Florida\", \"FL\")\\\n             .replace(\"Calif.\", \"CA\").replace(\"FLA\", \"FL\").replace(\"Fla.\", \"FL\")\\\n             .replace(\"La.\", \"LA\").replace(\"k NY\", \"k, NY\").replace(\"a, CA\", \"a, CSA\")\\\n             .replace(\"Pa.\", \"PA\")\n        dict_ = {\"Chicago\":\"Chicago, IL\", \"Cleveland\":\"Cleveland, OH\",\"Detroit\":\"Detroit, MI\",\n                 \"London\":\"London, England\", \"New Orleans\":\"New Orleans, LA\", \"Pittsburgh\":\"Pittsburgh, PA\",\n                 \"Seattle\":\"Seattle, WA\", \"Mexico City\":\"Mexico City, Mexico\"}\n        if x in dict_.keys(): x = dict_[x]\n        x = x.replace(\".\", \"\").lower()\n        import re\n        x = re.sub(\"[\\s]+\", \" \", x)\n        x = x.replace(\", \", \",\")\n        return x\n    df[\"Location\"] = df[\"Location\"].map(func)\n    df[\"Location_place\"] = df[\"Location\"].map(lambda x: x.split(\",\")[0])\n    df[\"Location_state\"] = df[\"Location\"].map(lambda x: \"ZZZ\" if len(x.split(\",\"))==1 else x.split(\",\")[1])\n    df.drop(\"Location\", axis=1, inplace=True)\n    if get_lbs: lbs[\"Location_place\"] = label_encode(df, \"Location_place\")\n    else: label_encode(df, \"Location_place\", lb=lbs[\"Location_place\"])\n    if get_lbs: lbs[\"Location_state\"] = label_encode(df, \"Location_state\")\n    else: label_encode(df, \"Location_state\", lb=lbs[\"Location_state\"])\n    df[\"StadiumType\"] = df[\"StadiumType\"].map(lambda x: stad_type_dict[x] if x in stad_type_dict.keys() else x)\n    df[\"StadiumType\"] = df[\"StadiumType\"].fillna(\"ZZZ\")\n    if get_lbs: lbs[\"StadiumType\"] = label_encode(df, \"StadiumType\")\n    else: label_encode(df, \"StadiumType\", lb=lbs[\"StadiumType\"])\n    df[\"Turf\"] = df[\"Turf\"].map(lambda x: turf_dict[x] if x in turf_dict.keys() else x)\n    if get_lbs: lbs[\"Turf\"] = label_encode(df, \"Turf\")\n    else: label_encode(df, \"Turf\", lb=lbs[\"Turf\"])\n    df[\"Temperature\"] = df[\"Temperature\"].map(np.float)\n    df.loc[train_df[\"GameWeather\"] == \"T: 51; H: 55; W: NW 10 mph\",\\\n             [\"Humidity\", \"Temperature\", \"WindSpeed\", \"WindDirection\"]] = np.array([55., 51., 10., \"nw\"])\n    df[\"GameWeather\"] = df[\"GameWeather\"].map(lambda x: game_weather_dict[x] if x in game_weather_dict.keys() else x)\n    df[\"GameWeather\"] = df[\"GameWeather\"].fillna(\"ZZZ\")\n    if get_lbs: lbs[\"GameWeather\"] = label_encode(df, \"GameWeather\")\n    else: label_encode(df, \"GameWeather\", lb=lbs[\"GameWeather\"])\n    if print_: print(\"Part X. Done.\")\n    def func(x):\n        try:\n            np.int(x)\n            return True\n        except: return False\n    locs = df[\"WindDirection\"].map(func)\n    winD = df.loc[locs, \"WindDirection\"].copy()\n    winS = df.loc[locs, \"WindSpeed\"].copy()\n    df.loc[locs, \"WindDirection\"] = winS\n    df.loc[locs, \"WindSpeed\"] = winD\n    \n    def func(x, dict_):\n        if x in dict_.keys(): return dict_[x]\n        else: return x\n    df[\"WindSpeed\"] = df[\"WindSpeed\"].map(lambda x: func(x, wind_speed_dict))\n    df[\"WindDirection\"] = df[\"WindDirection\"].map(lambda x: func(x, wind_direc_dict))\n\n    def func(x, type_=0):\n        if type(x) in [float, np.float, np.float64, np.float32,\n                       int, np.int, np.int32, np.int64]:\n            if type_==0: return 0\n            elif type_==1: return x\n        if x in [None, np.nan]: return np.nan\n        if \"-\" in x:\n            sp = x.split(\"-\")\n            if type_ == 0: return np.float(sp[1]) - np.float(sp[0])\n            elif type_ == 1: return np.float(sp[0])\n        else:\n            if type_==0: return 0\n            elif type_==1: return np.float(x)\n    df[\"WindSpeed_diff\"] = df[\"WindSpeed\"].map(lambda x: func(x, type_=0))\n    df[\"WindSpeed\"] = df[\"WindSpeed\"].map(lambda x: func(x, type_=1))\n    df[\"WindDirection\"] = df[\"WindDirection\"].fillna(\"ZZZ\")\n    if get_lbs: lbs[\"WindDirection\"] = label_encode(df, \"WindDirection\")\n    else: label_encode(df, \"WindDirection\", lb=lbs[\"WindDirection\"])\n        \n    df[\"WindSpeed\"] = df[\"WindSpeed\"].fillna(0)\n    df[\"WindSpeed_diff\"] = df[\"WindSpeed_diff\"].fillna(0)\n    for col in [\"Temperature\", \"Humidity\", \"DefendersInTheBox\", \"Dir\", \"Orientation\"]:\n        #print(df[col].astype(np.float).quantile(.5))\n        df[col] = df[col].fillna(df[col].astype(np.float).quantile(.5)).astype(np.float)\n    if print_: print(\"Completed.\")\n    return df, lbs","18108ca8":"%%time\ntrain_df = train_df_orig.copy()\ntrain_df, label_encoders = preprocess(train_df)","6c68dea8":"train_df.head()","285ef3ad":"for col in train_df.columns:\n    if col in [\"OffensePersonnel\", \"DefensePersonnel\"]: continue\n    if train_df[col].std() == 0: print(\"'\"+col+\"'\", end=\", \")","51d25425":"drop = ['Fmap0_r3q0', 'Fmap0_r3q1', 'Fmap0_r3q2', 'Fmap0_r3q3', 'Fmap0_r3q4', 'Fmap0_r3q5', 'Fmap0_r3q6', 'Fmap0_r3q7', 'Fmap0_r10q0', 'Fmap0_r10q1', 'Fmap0_r10q2', 'Fmap0_r10q3', 'Fmap0_r10q4', 'Fmap0_r10q5', 'Fmap0_r10q6', 'Fmap0_r10q7', 'Fmap0_r30q0', 'Fmap0_r30q1', 'Fmap0_r30q2', 'Fmap0_r30q3', 'Fmap0_r30q4', 'Fmap0_r30q5', 'Fmap0_r30q6', 'Fmap0_r30q7', 'Fmap0_r120q0', 'Fmap0_r120q1', 'Fmap0_r120q2', 'Fmap0_r120q3', 'Fmap0_r120q4', 'Fmap0_r120q5', 'Fmap0_r120q6', 'Fmap0_r120q7', 'Emap0_r120q7', 'DefensePersonnel_RB']\ntrain_df = train_df.drop(drop, axis=1)","7466cc3c":"t = train_df.isnull().sum()\nt[t>0]","8449aafd":"print(train_df.columns.values.tolist())","045446ba":"columns = train_df.drop(\"PlayId\", axis=1).columns.values.tolist()\ntrain_df_new = train_df.groupby('PlayId').mean()\ntrain_df_new.columns = columns\ntrain_df = train_df_new.reset_index(drop=True)","2f9d70ac":"train_df = train_df.drop(['GameId', 'Team', 'NflId', 'JerseyNumber', 'Season', 'NflIdRusher'], axis=1)","35c01240":"columns = train_df.columns.values.tolist()\ndef chk_corr(df, thresh=0.99, threaded=False):\n    \"\"\"\n    Checks for highly correlated features and removes them.\n    ---------------------------------------------------------------------\n    Parameters:\n        df: Dataframe to check for correlation\n    Output:\n        Return list of removed features\/columns.\n    \"\"\"\n    if threaded:\n        import threading.Threads as t\n        import Queue\n        \n        corr_matrix = pd.DataFrame(index=df.columns, columns=df.columns)\n    else:\n        corr_matrix = df.corr()\n        \n        # Taking only the upper triangular part of correlation matrix: (We want to remove only one of corr features)\n        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    \n        # Find index of feature columns with correlation greater than {thresh}\n        to_drop = [column for column in upper.columns if any(abs(upper[column]) > thresh)]\n        \n        print('%d feature are highly correlated.'%len(to_drop))\n        return to_drop\n\nto_drop = chk_corr(train_df, thresh=0.99)","672e342b":"print(to_drop)","4ab83cdd":"train_df = train_df.drop(to_drop + ['Y'], axis=1)","fbb809bc":"X, yards = train_df.drop(\"Yards\", axis=1).copy(), train_df[\"Yards\"].copy()","4029aabc":"Y = np.zeros(shape=(X.shape[0], 199))\nfor i, yard in enumerate(yards):\n    Y[i, np.int(yard+99):] = np.ones(shape=(1, np.int(100-yard)))","695c521e":"print(Y)","20f5115e":"from sklearn.preprocessing import StandardScaler\nssc = StandardScaler()\nX = ssc.fit_transform(X)","45077ec3":"%%time\n# https:\/\/www.kaggle.com\/bgmello\/neural-networks-feature-engineering-for-the-win\nfrom sklearn.model_selection import RepeatedKFold\nrkf = RepeatedKFold(n_splits=5, n_repeats=2)\n\nmodels1 = []\nlosses2, models2, crps_csv2 = [], [], []\nmodels4 = []\nfor tr_idx, vl_idx in rkf.split(X, Y, yards):\n    x_tr, y_tr = X[tr_idx], Y[tr_idx]\n    x_vl, y_vl = X[vl_idx], Y[vl_idx]\n    yards_tr, yards_vl = yards.values[tr_idx], yards.values[vl_idx]\n    \n    # 1st NN\n    # https:\/\/www.kaggle.com\/bgmello\/neural-networks-feature-engineering-for-the-win\n    model = train_model(x_tr, y_tr, x_vl, y_vl, batch_size=128, epochs=50)\n    models1.append(model)\n    \n    # 2nd NN\n    # https:\/\/www.kaggle.com\/dandrocec\/location-eda-with-rusher-features\/data#Let's-build-NN\n    model2, crps2 = get_model2(x_tr, y_tr, x_vl, y_vl, batch_size=128, epochs=50)\n    models2.append(model2)\n    crps_csv2.append(crps2)\n    \n    # 1st Boosting (NGBoost)\n    # https:\/\/www.kaggle.com\/anandavati\/ngboost-for-nfl\/\n    #ngb = NGBoost(Dist=Normal, Score=CRPS(), verbose=True, learning_rate=0.01, n_estimators=500)\n    #ngb.fit(x_tr, yards_tr, X_val=x_vl, Y_val=yards_vl)\n    \n    # 2nd Boosting (LightGBM)\n    # https:\/\/www.kaggle.com\/hukuda222\/nfl-simple-model-using-lightgbm\n    clf = lgb.LGBMRegressor(n_estimators=500, learning_rate=0.01)\n    clf.fit(x_tr, yards_tr, eval_set=[(x_vl, yards_vl)], early_stopping_rounds=20, verbose=False)\n    models4.append(clf)","2b7d27a2":"np.mean(crps_csv2)","1719a071":"import warnings\nwarnings.filterwarnings(\"ignore\")","4bbc0547":"%%time\nfor (test_df, sample_prediction_df) in env.iter_test():\n    prepared_df, _ = preprocess(test_df, lbs=label_encoders)\n    prepared_df = prepared_df.drop(drop, axis=1)\n    columns = prepared_df.drop(\"PlayId\", axis=1).columns.values.tolist()\n    prepared_df = prepared_df.groupby('PlayId').mean()\n    prepared_df.columns = columns\n    prepared_df = prepared_df.reset_index(drop=True)\n    prepared_df = prepared_df.drop(['GameId', 'Team', 'NflId', 'JerseyNumber', 'Season', 'NflIdRusher'], axis=1)\n    prepared_df = prepared_df.drop(to_drop + ['Y'], axis=1)\n    yardsleft = prepared_df['YardsLeft'].values\n    prepared_df = ssc.transform(prepared_df)\n    \n    # 1st NN\n    y_pred1 = np.mean([np.cumsum(model.predict(prepared_df), axis=1) for model in models1], axis=0)\n    for i in range(len(yardsleft)):\n        y_pred1[i, :np.int(yardsleft[i])-1] = 0\n        y_pred1[i, np.int(yardsleft[i])+100:] = 1\n    \n    # 2nd NN\n    y_pred2 = predict2(prepared_df, models2, batch_size=1024)\n    y_pred2 = np.cumsum(y_pred2, axis=1)\n    \n    # 1st Boosting (NGBoost)\n    #Q = list(range(-99, 100))\n    #y_pred3 = ngb.pred_dist(prepared_df)\n    #y_pred3 = y_pred3.cdf(Q)\n    #print(type(y_pred3))\n    #print(y_pred3.shape)\n    #print(y_pred3)\n    \n    # 2nd Boosting (LightGBM)\n    y_pred4 = np.zeros(199)        \n    y_pred4_p = np.sum(np.round([model.predict(prepared_df)[0] for model in models4]))\/len(models4)\n    y_pred4_p += 99\n    for j in range(199):\n        if j>=y_pred4_p+10: y_pred4[j]=1.0\n        elif j>=y_pred4_p-10: y_pred4[j]=(j+10-y_pred4_p)*0.05\n    \n    y_pred_final = (2\/4.*y_pred1+1\/4.*y_pred2+1\/4.*y_pred4) # +1\/12.*y_pred3\n    env.predict(pd.DataFrame(data=y_pred_final.clip(0,1), index=sample_prediction_df.index, columns=sample_prediction_df.columns))","106f966b":"env.write_submission_file()","2f949b52":"## 2. train_df:","dbe32c04":"## Functions:","8894d711":"## 1. Initialize:","1a6e54ee":"## 3. Preprocessing:","cacb5535":"## Predict:"}}