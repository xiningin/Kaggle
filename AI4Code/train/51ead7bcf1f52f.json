{"cell_type":{"6ef07865":"code","8ea213cb":"code","463e7f62":"code","ed51d217":"code","fefe3a9a":"code","0f46d374":"code","3bf28332":"code","6b730db1":"code","4c60e548":"code","8e1456d2":"code","e2631907":"code","94ec1881":"code","5c6f47e8":"code","fb300279":"code","b40dbc4c":"code","d1a71b42":"code","1e25e033":"code","8a4b8a95":"code","a98c7f2d":"code","1f9462e7":"code","5cc6119a":"code","c13d53b7":"code","a00fb2d1":"code","5d5c8dc0":"code","bddfdabe":"markdown","10f07394":"markdown","08e28c5b":"markdown","39f4fc22":"markdown","edd6f5ab":"markdown","67fb494e":"markdown","ecf042f0":"markdown","3c75e8d5":"markdown","24dfa91a":"markdown","f54b75c8":"markdown","a5334e9e":"markdown","7a86b73a":"markdown","37b2fb11":"markdown","a22df7ae":"markdown","ffe7823c":"markdown","efbf1562":"markdown","18a0eea0":"markdown","af6165be":"markdown","5310edf6":"markdown","b7e28b13":"markdown","27f37f63":"markdown","ab0cfa32":"markdown","7aa8fc1c":"markdown"},"source":{"6ef07865":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ea213cb":"import matplotlib.pyplot as plt","463e7f62":"# load data into df\ndf = pd.read_csv(\"\/kaggle\/input\/mushroom-classification\/mushrooms.csv\")","ed51d217":"# check content\ndf.head()","fefe3a9a":"len(df)","0f46d374":"# describe data: simple stats\ndf.describe()","3bf28332":"# data types\ndf.dtypes","6b730db1":"# distributions of features and target (categorical)\ncategorical_features = df.columns # all columns are categorical\nfig, ax = plt.subplots(len(categorical_features), 1, figsize=(6,len(categorical_features)*5))\nfor i, categorical_feature in enumerate(df[categorical_features]):\n    df[categorical_feature].value_counts().plot(kind=\"bar\", ax=ax[i]).set_title(categorical_feature)\nfig.show()","4c60e548":"# correlation matrix\ndf.apply(lambda x : pd.factorize(x)[0]).corr(method='pearson', min_periods=1)","8e1456d2":"df[\"class\"].value_counts()","e2631907":"beta = 0.5 # we give recall half the importance of precision","94ec1881":"from sklearn.metrics import fbeta_score\n\n# fbeta_score example\n\ny_true = [0, 1, 1, 0, 1, 1]\ny_pred = [0, 0, 1, 0, 0, 1]\n\nfbeta_score(y_true, y_pred, beta=0.5)","5c6f47e8":"from sklearn.preprocessing import LabelEncoder","fb300279":"# encode target: we aim to predict poisonous mushrooms => we need high precision\ny = df[\"class\"].map({'p':1, 'e':0})\ny","b40dbc4c":"# encode features\nX = df.drop(columns=[\"class\"]).apply(LabelEncoder().fit_transform)\nX.head()","d1a71b42":"from sklearn.model_selection import train_test_split","1e25e033":"X_train, X_test, Y_train, Y_test = train_test_split(X, y,\n                                                   test_size=0.2,\n                                                   random_state=0)\n\nprint(\"X_train: \", X_train.shape)\nprint(\"X_test: \", X_test.shape)\nprint(\"Y_train: \", Y_train.shape)\nprint(\"Y_test: \", Y_test.shape)","8a4b8a95":"from xgboost import XGBClassifier","a98c7f2d":"model = XGBClassifier()\nmodel.fit(X_train, Y_train, eval_metric=\"auc\", eval_set=[(X_test, Y_test)], verbose=False) # TODO use fbeta_score for eval_metric","1f9462e7":"pred = model.predict(X)\nfbeta_score(y, pred, beta=0.5)","5cc6119a":"pred_baseline = np.ones(y.shape) # class=1\nfbeta_score(y, pred_baseline, beta=beta)","c13d53b7":"pred_baseline = np.zeros(y.shape) # class=0\nfbeta_score(y, pred_baseline, beta=beta)","a00fb2d1":"from xgboost import plot_importance","5d5c8dc0":"plot_importance(model)","bddfdabe":"## 5.1 Encoding","10f07394":"We choose XGBoost, because this is a classification with only categorical features and XGBoost has proven to give a good idea of what is possible on this type of dataset especially with classification tasks.","08e28c5b":"# Task 3 \u2705\nIdentify the problem: is it regression? classification?","39f4fc22":"=> All features are categorical. ","edd6f5ab":"# Task 2 \u2705\nExplore the basic parameters: how many data points do we have? What are the targets and what is their distribution? Any kind of exploratory data analysis is welcome","67fb494e":"=> There are 8124 datapoints.","ecf042f0":"# Task 6 \u2705\nCompare your results with some kind of baseline (simplest possible solution to the problem)","3c75e8d5":"# Requirements\n1. Load the data and check its correctness\n2. Explore the basic parameters: how many data points do we have? What are the targets and what is their distribution? Any kind of exploratory data analysis is welcome\n3. Identify the problem: is it regression? classification?\n4. Identify metric you're going to use\n5. Design and run the experiment: train and validate your model\n6. Compare your results with some kind of baseline (simplest possible solution to the problem)\n7. (Optional) estimate feature importances and select the most important features\n","24dfa91a":"XGBoost allows us to display feature importance for the trained model. More details: https:\/\/machinelearningmastery.com\/feature-importance-and-feature-selection-with-xgboost-in-python\/","f54b75c8":"# Task 4 \u2705\nIdentify metric you're going to use","a5334e9e":"## 5.3 Train Model","7a86b73a":"The baseline of predicting all mushrooms as edible (class=0) leeds to the worst score.","37b2fb11":"=> The three most important features according to the trained XGBoost model are:\n1. spore-print-color\n2. odor\n3. gill-size","a22df7ae":"# Task 5 \u2705\nDesign and run the experiment: train and validate your model","ffe7823c":"# Task 1 \u2705\nLoad the data and check its correctness\n","efbf1562":"=> The target class can be \"e\" or \"p\". Both \"e\" and \"p\" seem to be about equally likely.","18a0eea0":"=> We are able to achieve perfect classification \ud83c\udf89.\nEven when training with auc instead of fbeta_score.","af6165be":"We choose the baseline of predicting all mushrooms as poisonous (class=1).","5310edf6":"We will use the F1 score, since it considers both precision and recall.\n\nComparison of classification metrics:\nhttps:\/\/towardsdatascience.com\/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226\n\nWe can alter the \u03b2 parameter to value precision over recall, because wrongly predicting a poisonous (\"p\") mushroom as edible (\"e\") is worse than predicting a edible (\"e\") mushroom as poisonous (\"p\").\n\n\"The beta parameter determines the weight of recall in the combined score. beta < 1 lends more weight to precision, while beta > 1 favors recall (beta -> 0 considers only precision, beta -> +inf only recall).\" - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.fbeta_score.html","b7e28b13":"=> The target is class. Class is categorical, therefore the task is a classification.","27f37f63":"# Task 7 \u2705\n(Optional) estimate feature importances and select the most important features","ab0cfa32":"## 5.4 Evaluate","7aa8fc1c":"## 5.2 Train\/Test Split"}}