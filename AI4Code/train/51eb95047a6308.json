{"cell_type":{"78ce2c75":"code","2fcbd6bc":"code","d4b8b0d6":"code","e37cccd3":"code","d9f0573e":"code","77ec8b46":"code","3b4e1afe":"code","b0f3094b":"code","b6a2f9e8":"code","6c0cf86f":"code","9220454b":"code","65634091":"code","d7f26ad5":"code","ae491504":"code","8fc2894a":"code","bab303b6":"code","05d7457a":"code","43039da6":"code","743396ad":"code","666dd895":"code","c4cc7f13":"code","9a7c5c76":"code","edc2600f":"code","df6076e5":"code","1e8ed989":"code","201dc3e6":"code","393bdc2e":"code","957f7dd7":"code","a942c8d4":"code","54c4e951":"code","cf0ffbba":"code","c9de8d2d":"code","3bcd436a":"code","cf76225c":"code","00f178b3":"code","d77238c1":"markdown","a6a79370":"markdown","04fd8199":"markdown","aae3b237":"markdown","03348f8b":"markdown","9ef066fc":"markdown","e587262e":"markdown","f9b3ce58":"markdown","34f7477a":"markdown","27266116":"markdown","b523922c":"markdown","b5b1314c":"markdown","bedfb396":"markdown","c51db7b7":"markdown","65bde158":"markdown","7706e859":"markdown","bdf1050c":"markdown","0283946b":"markdown","5afc75ee":"markdown","82355823":"markdown","8ef9ec71":"markdown","979959db":"markdown","a3c54c02":"markdown"},"source":{"78ce2c75":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport tensorflow as tf\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nimport sklearn.model_selection as sk\n\nimport plotly.express as px\n\nimport re\n\nfrom io import StringIO\nfrom IPython.display import display\n\nfrom scipy.stats import randint\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn import __version__ \n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nprint(\"Input files:\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# For neural nets with my GPU, RNN doesn't work without this in TF 2.0\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)\n\nprint()\nprint(\"sklearn Version: \", __version__)\nprint(\"TF Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n\nif tf.test.gpu_device_name():\n    print('GPU found')\nelse:\n    print(\"No GPU found\")","2fcbd6bc":"path_to_file = '\/kaggle\/input\/wine-reviews\/winemag-data-130k-v2.csv'\n\ndfWine = pd.read_csv(path_to_file)","d4b8b0d6":"dfWine.info()\nprint()\nprint(dfWine.shape)\nprint(dfWine.columns)","e37cccd3":"dfWine.head(5)","d9f0573e":"dfWine.describe()","77ec8b46":"# # Removes the unnecessary column of row number\ndfWine = dfWine.drop(['Unnamed: 0'], axis=1)\ndfWine.head(5)","3b4e1afe":"# # Removes the Twitter handles, that doesn't matter here\ndfWine = dfWine.drop(['taster_twitter_handle'], axis=1)\ndfWine.head(5)","b0f3094b":"# Read title and find vintage\nyearSearch = []    \nfor value in dfWine['title']:\n    regexresult = re.search(r'19\\d{2}|20\\d{2}', value)\n    if regexresult:\n        yearSearch.append(regexresult.group())\n    else: yearSearch.append(None)\n\ndfWine['year'] = yearSearch\n\n#Tell me which ones don't have a year listed\nprint(\"We extracted %d years from the wine titles and %d did not have a year.\" %(len(dfWine[dfWine['year'].notna()]), len(dfWine[dfWine['year'].isna()].index)))\ndfWine['year'].describe()","b6a2f9e8":"#If we're missing year values, remove the row\ndfWine_goodyears=dfWine\ndfWine_goodyears=dfWine_goodyears.dropna(subset=['year'])\nprint('Removed ' + str(dfWine.shape[0]-dfWine_goodyears.shape[0]) + ' rows with empty year values.' + \"\\n\")\n\ndfWine_goodyears['year']=dfWine_goodyears['year'].astype(int)\n# dfWine_goodyears['year']=pd.to_numeric(dfWine_goodyears['year'], downcast='integer', errors='coerce')\n\nprint(dfWine_goodyears['year'].describe())\n\ndfWineYear = dfWine_goodyears.groupby(['year']).mean()\ndfWineYear = pd.DataFrame(data=dfWineYear).reset_index()","6c0cf86f":"dfWine = dfWine.replace({'country': r'USA?'}, {'country': 'United States of America'}, regex=True)\n\n# #For ISO-3 codes of countries, for mapping\ndfcountry = pd.read_csv('\/kaggle\/input\/country-names-mapping-to-iso3\/countryMap.txt',sep='\\t')\ndfWine = dfWine.merge(dfcountry, on='country')\n\n# #For ISO-3 codes of states, if needed\nstate_codes = {\n    'District of Columbia' : 'DC','Mississippi': 'MS', 'Oklahoma': 'OK', \n    'Delaware': 'DE', 'Minnesota': 'MN', 'Illinois': 'IL', 'Arkansas': 'AR', \n    'New Mexico': 'NM', 'Indiana': 'IN', 'Maryland': 'MD', 'Louisiana': 'LA', \n    'Idaho': 'ID', 'Wyoming': 'WY', 'Tennessee': 'TN', 'Arizona': 'AZ', \n    'Iowa': 'IA', 'Michigan': 'MI', 'Kansas': 'KS', 'Utah': 'UT', \n    'Virginia': 'VA', 'Oregon': 'OR', 'Connecticut': 'CT', 'Montana': 'MT', \n    'California': 'CA', 'Massachusetts': 'MA', 'West Virginia': 'WV', \n    'South Carolina': 'SC', 'New Hampshire': 'NH', 'Wisconsin': 'WI',\n    'Vermont': 'VT', 'Georgia': 'GA', 'North Dakota': 'ND', \n    'Pennsylvania': 'PA', 'Florida': 'FL', 'Alaska': 'AK', 'Kentucky': 'KY', \n    'Hawaii': 'HI', 'Nebraska': 'NE', 'Missouri': 'MO', 'Ohio': 'OH', \n    'Alabama': 'AL', 'Rhode Island': 'RI', 'South Dakota': 'SD', \n    'Colorado': 'CO', 'New Jersey': 'NJ', 'Washington': 'WA', \n    'North Carolina': 'NC', 'New York': 'NY', 'Texas': 'TX', \n    'Nevada': 'NV', 'Maine': 'ME'}\n\ndfWine['state_code'] = dfWine['province'].apply(lambda x : state_codes[x] if x in state_codes.keys() else None)","9220454b":"# dfWine['country-3let'].value_counts()\n# dfWine[dfWine.country==\"United States of America\"][dfWine['state_code'].isna()]\n\nprint(\"%d did not get a country code and %d US wines did not get a state code.\" \n      %((len(dfWine[dfWine.country==\"\"])), \n        len(dfWine[dfWine.country==\"United States of America\"][dfWine['state_code'].isna()])))","65634091":"# Get label frequencies in descending order\nlabel_freq = dfWine['variety'].apply(lambda s: str(s)).explode().value_counts().sort_values(ascending=False)\n\n# Bar plot\nstyle.use(\"fivethirtyeight\")\nplt.figure(figsize=(12,10))\nsns.barplot(y=label_freq.index.values, x=label_freq, order=label_freq.iloc[:15].index)\nplt.title(\"Grape frequency\", fontsize=14)\nplt.xlabel(\"\")\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","d7f26ad5":"dfWineClassifier = dfWine[[ 'description', 'year', 'variety', 'country-3let', 'province' ]]\n\n# Tell us where we have missing or NaN values (isnull or isna):\nprint(dfWineClassifier.isnull().sum())\nprint()\n\n#Tell me which ones don't have a variety listed\nprint(\"Missing entries: %d\" %(dfWineClassifier[dfWineClassifier['variety'].isna()].index[0]))\nprint(dfWineClassifier[dfWineClassifier['variety'].isna()].head(10))\nprint()\n\n# pd.DataFrame(dfWineClassifier.variety.unique()).values\n\n#If we're missing important values, remove the row\ndfWineClassifier=dfWineClassifier.dropna(subset=['description', 'variety'])\nprint('Removed ' + str(dfWine.shape[0]-dfWineClassifier.shape[0]) + ' rows with empty values.' + \"\\n\")","ae491504":"RARE_CUTOFF = 400 # It must have this many examples of the grape variety, otherwise it's \"other.\"\n\n# Create a list of rare labels\nrare = list(label_freq[label_freq<RARE_CUTOFF].index)\n# print(\"We will be ignoring these rare labels: \\n\", rare)\n\n\n# Transform the rare ones to just \"Other\"\ndfWineClassifier['variety'] = dfWineClassifier['variety'].apply(lambda s: str(s) if s not in rare else 'Other')\n\nlabel_words = list(label_freq[label_freq>=RARE_CUTOFF].index)\nlabel_words.append('Other')\nprint(label_words)\n\nnum_labels = len(label_words)\nprint(\"\\n\"  + str(num_labels) + \" different categories.\")\n\n# pd.DataFrame(dfWineClassifier.variety.unique()).values","8fc2894a":"# Create a new column 'category_id' with encoded categories \ndfWineClassifier['category_id'] = dfWineClassifier['variety'].factorize()[0]\ncategory_id_df = dfWineClassifier[['variety', 'category_id']].drop_duplicates()\n\n# Dictionaries for future use\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'variety']].values)\n\n# Show me the new dataframe\ndfWineClassifier.head()","bab303b6":"num_bars = 19\n\nfig = plt.figure(figsize=(8,6))\ncolors = plt.cm.get_cmap('viridis', num_bars)\ndfWineClassifier.groupby('variety').variety.count().sort_values()[num_bars:].plot.barh(\n    ylim=0, color=colors.colors, title= 'NUMBER IN EACH CATEGORY\\n')\nplt.xlabel('Number of ocurrences', fontsize = 10);","05d7457a":"df2 = dfWineClassifier.sample(10000, random_state=1).copy()","43039da6":"# Uni, bi, and trigrams requested with ngram_range=(1,3)\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n                        ngram_range=(1, 3), \n                        stop_words='english')\n\n# We transform each description into a vector\n# we use this later in models\nfeatures = tfidf.fit_transform(df2.description).toarray()\n\nlabels = df2.category_id\n\nprint(labels)\n\nprint(\"Each of the %d descriptions is represented by %d features (TF-IDF score of unigrams and bigrams and trigrams)\" %(features.shape))","743396ad":"# Finding the three most correlated terms with each of the product categories\nN = 3\nfor variety, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n  print(\"\\n==> %s:\" %(variety))\n  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))\n  print(\"  * Most Correlated Trigrams are: %s\" %(', '.join(trigrams[-N:])))","666dd895":"X = df2['description'] # Collection of reviews\ny = df2['variety'] # Target or the labels we want to predict\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25,\n                                                    random_state = 0)\n\nprint('Test: ' + str(len(X_test)) + ' Train: ' + str(len(X_train)))","c4cc7f13":"models = [\n    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state=0),\n]\n\n# 5 Cross-validation\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\n\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\n    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])","9a7c5c76":"mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\nstd_accuracy = cv_df.groupby('model_name').accuracy.std()\n\nacc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n          ignore_index=True)\nacc.columns = ['Mean Accuracy', 'Standard deviation']\nacc","edc2600f":"plt.figure(figsize=(8,5))\nsns.boxplot(x='model_name', y='accuracy', \n            data=cv_df, \n            color='lightblue', \n            showmeans=True)\nplt.title(\"MEAN ACCURACY (cv = 5)\\n\", size=14);","df6076e5":"X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features, \n                                                               labels, \n                                                               df2.index, test_size=0.25, \n                                                               random_state=1)\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","1e8ed989":"# # Classification report\n# print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\n# print(metrics.classification_report(y_test, y_pred, \n#                                     target_names=df2['variety'].unique(), labels=label_words),\n#                                     zero_division)","201dc3e6":"conf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(conf_mat[1:8, 1:8], annot=True, cmap=\"Blues\", fmt='d',\n            xticklabels=category_id_df.variety.values[1:8], \n            yticklabels=category_id_df.variety.values[1:8])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title(\"CONFUSION MATRIX - LinearSVC\\n\", size=16);","393bdc2e":"for predicted in category_id_df.category_id:\n  for actual in category_id_df.category_id:\n    if predicted != actual and conf_mat[actual, predicted] >= 20:\n      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], \n                                                           id_to_category[predicted], \n                                                           conf_mat[actual, predicted]))\n    \n      display(df2.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['variety', \n                                                                'description']])\n      print('')","957f7dd7":"model.fit(features, labels)\n\nN = 4\nfor variety, category_id in sorted(category_to_id.items()):\n  indices = np.argsort(model.coef_[category_id])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n  trigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 3][:N]\n  print(\"\\n==> '{}':\".format(variety))\n  print(\"  * Top unigrams: %s\" %(', '.join(unigrams)))\n  print(\"  * Top bigrams: %s\" %(', '.join(bigrams)))\n  print(\"  * Top trigrams: %s\" %(', '.join(trigrams)))","a942c8d4":"df3 = dfWineClassifier.sample(5000, random_state=1).copy()\nX = df3['description'] # Collection of reviews\ny = df3['variety'] # Target or the labels we want to predict\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25,\n                                                    random_state = 0)\n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n                        ngram_range=(1, 3), \n                        stop_words='english')\n\nfitted_vectorizer = tfidf.fit(X_train)\ntfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n\nmodel = LinearSVC().fit(tfidf_vectorizer_vectors, y_train)","54c4e951":"new_review = \"\"\"Jammy and bold with hints of thyme and tobacco.\"\"\"\nprint(model.predict(fitted_vectorizer.transform([new_review])))","cf0ffbba":"new_review = \"\"\"Grassy lemon and grapefruit.\"\"\"\nprint(model.predict(fitted_vectorizer.transform([new_review])))","c9de8d2d":"df3 = dfWineClassifier.sample(30000, random_state=1).copy()\nX = df3['description'] # Collection of reviews\ny = df3['variety'] # Target or the labels we want to predict\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25,\n                                                    random_state = 0)\n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n                        ngram_range=(1, 3), \n                        stop_words='english')\n\nfitted_vectorizer = tfidf.fit(X_train)\ntfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n\nmodel2 = SVC(kernel='linear',probability=True).fit(tfidf_vectorizer_vectors, y_train)","3bcd436a":"new_review = \"\"\"Crisp grapefruit and grassy lemon.\"\"\"\nprint(model2.predict(fitted_vectorizer.transform([new_review]))[0])\nprint()\n\nprobas = (model2.predict_proba(fitted_vectorizer.transform([new_review])))\n\nsortedProbs=sorted(zip(probas[0], model2.classes_), reverse=True)\nfor i in sortedProbs[0:4]:\n    print(\"Guess: %s \\n Prob: %f\" %(i[1], 100*i[0]) + '%')","cf76225c":"new_review = \"\"\"Soft tannins and astringent peppery blackberry\"\"\"\nprint(model2.predict(fitted_vectorizer.transform([new_review]))[0])\nprint()\n\nprobas = (model2.predict_proba(fitted_vectorizer.transform([new_review])))\n\nsortedProbs=sorted(zip(probas[0], model2.classes_), reverse=True)\nfor i in sortedProbs[0:4]:\n    print(\"Guess: %s \\n Prob: %f\" %(i[1], 100*i[0]) + '%')","00f178b3":"new_review = \"\"\"Tart cherry and light, with velvety mushroom with lingering tannins.\"\"\"\nprint(model2.predict(fitted_vectorizer.transform([new_review]))[0])\nprint()\n\nprobas = (model2.predict_proba(fitted_vectorizer.transform([new_review])))\n\nsortedProbs=sorted(zip(probas[0], model2.classes_), reverse=True)\nfor i in sortedProbs[0:4]:\n    print(\"Guess: %s \\n Prob: %f\" %(i[1], 100*i[0]) + '%')","d77238c1":"## Evaluate","a6a79370":"## Background Analysis","04fd8199":"# Classifier with more transparency\n## Use a linear SVC to get probs","aae3b237":"Examine a few options.","03348f8b":"## Wines","9ef066fc":"### Clean into a new simpler dataframe for this task","e587262e":"See if we missed any or had missing data.","f9b3ce58":"### Drop missing years, convert year to a number type","34f7477a":"## Preprocess\n### This can almost certainly be done smarter.\n### Take a smaller sample here.","27266116":"### Split into test and train","b523922c":"### What do we have?\n#### Exploratory analysis and descriptors of the raw dataset","b5b1314c":"## Geography\n\n### Country Data\n\nLet's deal with country data now\n\nhttps:\/\/github.com\/gsnaveen\/plotly-worldmap-mapping-2letter-CountryCode-to-3letter-country-code","bedfb396":"Convert classes into numeric values.\nhttps:\/\/www.kaggle.com\/selener\/multi-class-text-classification-tfidf","c51db7b7":"## Feature Extraction and Engineering","65bde158":"### Create features and labels here","7706e859":"### Predictions","bdf1050c":"### Group weird less common grapes into an \"other\" category","0283946b":"## Data Cleaning","5afc75ee":"### Looks at the title of the wine and extracts the vintages out of there.\n#### Creates a new column for the Vintages","82355823":"# Data Visualization","8ef9ec71":"# Classification\n## Term Frequency \u2013 Inverse Document Frequency (TFIDF)","979959db":"### Load the Data","a3c54c02":"# Classifier\n\n## Term Frequency \u2013 Inverse Document Frequency (TFIDF)"}}