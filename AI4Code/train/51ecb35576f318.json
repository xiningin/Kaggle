{"cell_type":{"f9703e10":"code","da2e150b":"code","391c4dfb":"code","d92dd7b9":"code","6a3f6fa7":"code","120455dc":"code","61d66387":"code","26192b37":"code","973f1151":"code","80e4a53a":"code","fa848716":"code","5e250ac3":"code","2f3b3fc4":"code","63579212":"code","6e0aa887":"code","4fbc07b9":"code","89e17aa6":"code","6f52600f":"markdown","e9c58ad6":"markdown","eea2349c":"markdown","0b680417":"markdown","95ca2dd3":"markdown","2b10b68b":"markdown","d99f3949":"markdown","d199e166":"markdown","46937b6c":"markdown","5212b600":"markdown","918d90f3":"markdown","679dd8ba":"markdown","f942070e":"markdown","c84441a5":"markdown","52a3d506":"markdown","b09f6b87":"markdown","04f53865":"markdown","136d7491":"markdown","6c4c0a8f":"markdown","6e5ebf12":"markdown","24e0b428":"markdown","2e8e46d9":"markdown","1b4fc74f":"markdown","44fb6068":"markdown","0622761b":"markdown","41a0a878":"markdown","4ff2617a":"markdown","92ecf52d":"markdown"},"source":{"f9703e10":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# import visualization\nimport matplotlib.pyplot as plt\n\n# import machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.compose import ColumnTransformer","da2e150b":"# load data\nfires = pd.read_csv('..\/input\/forest-fires-data-set\/forestfires.csv')\nfires.head()","391c4dfb":"plt.hist(fires.area, bins=20)\nplt.title('Area Distribution')\nplt.show()","d92dd7b9":"# see https:\/\/www.kaggle.com\/travelcodesleep\/end-to-end-regression-pipeline-using-scikitlearn \n# for explained data preprocessing\nfires = fires.drop(columns=['X', 'Y'])\n\nmonths_to_remove = ['nov','jan','may']\nfires = fires.drop(fires[fires.month.isin(months_to_remove)].index ,axis=0)\n\nfires['temp_bins'] = pd.cut(fires.temp, bins=[0, 15, 20, 25, 40], include_lowest=True, \n                                 labels=['0-15', '15-20', '20-25', '>25'])\n\n# split into train\/test\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in split.split(fires.values, fires.temp_bins.values):\n    st_train_set = fires.iloc[train_index]\n    st_test_set = fires.iloc[test_index]","6a3f6fa7":"X_train = st_train_set.drop(columns=['area', 'temp_bins'])\nX_train = pd.get_dummies(X_train)\ny_train = st_train_set[['area']]\n\nX_test = st_test_set.drop(columns=['area', 'temp_bins'])\nX_test = pd.get_dummies(X_test)\ny_test = st_test_set[['area']]\n\nmodel = RandomForestRegressor(max_depth=2, min_samples_split=2, min_samples_leaf=1, random_state=0, n_estimators=100)\nmodel.fit(X_train, y_train.values.ravel())\n\ny_pred = model.predict(X_test)\n\n# print out the prediction scores\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))\nprint('R-squared: {}'.format(r2_score(y_test, y_pred)))","120455dc":"plt.scatter(y_pred, y_test)\nplt.plot(np.linspace(0,400,400), np.linspace(0,400,400), c = 'orange', linestyle='--')\nplt.xlabel('prediction')\nplt.ylabel('true values')\nplt.xlim(0,400)\nplt.ylim(0,400)\nplt.title('Predicted vs True values')","61d66387":"# define sigmoid function\n# https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\n# plot sigmoid function\nplt.plot(np.linspace(-10, 10, 100), sigmoid(np.linspace(-10, 10, 100)))\nplt.title('Sigmoid Function')\nplt.xlabel('x')\nplt.axhline(y=0, c='black', linestyle=':')\nplt.axvline(x=0, c='black', linestyle=':')\nplt.ylabel('sigmoid(x)')\nplt.show()","26192b37":"# plot the boxplot of area distribution\nplt.boxplot(fires.area, vert=False)\nplt.title('Area Distribution')\nplt.xlabel('area')\nplt.show()","973f1151":"# implement relevance function\n# see paper: https:\/\/www.researchgate.net\/publication\/220699419_Utility-Based_Regression\ndef relevance(x):\n    x = np.array(x)\n    return sigmoid(x - 50)\n\n# plot relevance function\nplt.plot(np.linspace(30, 70, 1000), relevance(np.linspace(30, 70, 1000)))\nplt.title('Relevance Function')\nplt.xlabel('x')\nplt.axhline(y=0, c='gray', linestyle='--')\nplt.axvline(x=50, c='gray', linestyle='--')\nplt.ylabel('relevance(x)')\nplt.show()","80e4a53a":"# implement SMOTER\n# see paper: https:\/\/core.ac.uk\/download\/pdf\/29202178.pdf\n\ndef get_synth_cases(D, target, o=200, k=3, categorical_col = []):\n    '''\n    Function to generate the new cases.\n    INPUT:\n        D - pd.DataFrame with the initial data\n        target - string name of the target column in the dataset\n        o - oversampling rate\n        k - number of nearest neighbors to use for the generation\n        categorical_col - list of categorical column names\n    OUTPUT:\n        new_cases - pd.DataFrame containing new generated cases\n    '''\n    new_cases = pd.DataFrame(columns = D.columns) # initialize the list of new cases \n    ng = o \/\/ 100 # the number of new cases to generate\n    for index, case in D.iterrows():\n        # find k nearest neighbors of the case\n        knn = KNeighborsRegressor(n_neighbors = k+1) # k+1 because the case is the nearest neighbor to itself\n        knn.fit(D.drop(columns = [target]).values, D[[target]])\n        neighbors = knn.kneighbors(case.drop(labels = [target]).values.reshape(1, -1), return_distance=False).reshape(-1)\n        neighbors = np.delete(neighbors, np.where(neighbors == index))\n        for i in range(0, ng):\n            # randomly choose one of the neighbors\n            x = D.iloc[neighbors[np.random.randint(k)]]\n            attr = {}          \n            for a in D.columns:\n                # skip target column\n                if a == target:\n                    continue;\n                if a in categorical_col:\n                    # if categorical then choose randomly one of values\n                    if np.random.randint(2) == 0:\n                        attr[a] = case[a]\n                    else:\n                        attr[a] = x[a]\n                else:\n                    # if continious column\n                    diff = case[a] - x[a]\n                    attr[a] = case[a] + np.random.randint(2) * diff\n            # decide the target column\n            new = np.array(list(attr.values()))\n            d1 = cosine_similarity(new.reshape(1, -1), case.drop(labels = [target]).values.reshape(1, -1))[0][0]\n            d2 = cosine_similarity(new.reshape(1, -1), x.drop(labels = [target]).values.reshape(1, -1))[0][0]\n            attr[target] = (d2 * case[target] + d1 * x[target]) \/ (d1 + d2)\n            \n            # append the result\n            new_cases = new_cases.append(attr,ignore_index = True)\n                    \n    return new_cases\n\ndef SmoteR(D, target, th = 0.999, o = 200, u = 100, k = 3, categorical_col = []):\n    '''\n    The implementation of SmoteR algorithm:\n    https:\/\/core.ac.uk\/download\/pdf\/29202178.pdf\n    INPUT:\n        D - pd.DataFrame - the initial dataset\n        target - the name of the target column in the dataset\n        th - relevance threshold\n        o - oversampling rate\n        u - undersampling rate\n        k - the number of nearest neighbors\n    OUTPUT:\n        new_D - the resulting new dataset\n    '''\n    # median of the target variable\n    y_bar = D[target].median()\n    \n    # find rare cases where target less than median\n    rareL = D[(relevance(D[target]) > th) & (D[target] > y_bar)]  \n    # generate rare cases for rareL\n    new_casesL = get_synth_cases(rareL, target, o, k , categorical_col)\n    \n    # find rare cases where target greater than median\n    rareH = D[(relevance(D[target]) > th) & (D[target] < y_bar)]\n    # generate rare cases for rareH\n    new_casesH = get_synth_cases(rareH, target, o, k , categorical_col)\n    \n    new_cases = pd.concat([new_casesL, new_casesH], axis=0)\n    \n    # undersample norm cases\n    norm_cases = D[relevance(D[target]) <= th]\n    # get the number of norm cases\n    nr_norm = int(len(norm_cases) * u \/ 100)\n    \n    norm_cases = norm_cases.sample(min(len(D[relevance(D[target]) <= th]), nr_norm))\n    \n    # get the resulting dataset\n    new_D = pd.concat([new_cases, norm_cases], axis=0)\n    \n    return new_D","fa848716":"# construct the initial dataset for SmoteR\ncols = X_train.columns.tolist()\ncols.append('area')\nD = pd.DataFrame(np.concatenate([X_train, y_train], axis=1), columns = cols)","5e250ac3":"# apply SmoteR to get the new dataset\nnp.random.seed(43)\nXs = SmoteR(D, target='area', th = 0.999, o = 300, u = 100, k = 10, categorical_col = ['month_apr',\n       'month_aug', 'month_dec', 'month_feb', 'month_jan', 'month_jul',\n       'month_jun', 'month_mar', 'month_may', 'month_nov', 'month_oct',\n       'month_sep', 'day_fri', 'day_mon', 'day_sat', 'day_sun', 'day_thu',\n       'day_tue', 'day_wed'])\n\nX_train = Xs.drop(columns=['area'])\ny_train = Xs[['area']]\n\n\n# model and check the results\nmodel = RandomForestRegressor(max_depth=2, min_samples_split=5, min_samples_leaf=5, random_state=0, n_estimators=100)\n\nmodel.fit(X_train, y_train.values.ravel())\n\ny_pred = model.predict(X_test)\n\n# print out the prediction scores\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))\nprint('R-squared: {}'.format(r2_score(y_test, y_pred)))","2f3b3fc4":"# plot the results\nplt.scatter(y_pred, y_test)\nplt.plot(np.linspace(0,400,400), np.linspace(0,400,400), c = 'orange', linestyle='--')\nplt.xlabel('prediction')\nplt.ylabel('true values')\nplt.xlim(0,400)\nplt.ylim(0,400)\nplt.title('Predicted vs True values')","63579212":"import bisect as bs\nimport numpy.random as rd\n\n## calculate parameters for phi relevance function\ndef phi_ctrl_pts(\n    \n    ## arguments \/ inputs\n    y,                    ## response variable y\n    method = \"auto\",      ## relevance method (\"auto\" or \"manual\")\n    xtrm_type = \"both\",   ## distribution focus (\"high\", \"low\", \"both\")\n    coef = 1.5,           ## coefficient for box plot\n    ctrl_pts = None       ## input for \"manual\" rel method\n    \n    ):\n    \n    \"\"\" \n    generates the parameters required for the 'phi()' function, specifies the \n    regions of interest or 'relevance' in the response variable y, the notion \n    of relevance can be associated with rarity\n    \n    controls how the relevance parameters are calculated by selecting between \n    two methods, either \"auto\" or \"manual\"\n    \n    the \"auto\" method calls the function 'phi_extremes()' and calculates the \n    relevance parameters by the values beyond the interquartile range\n    \n    the \"manual\" method calls the function 'phi_range()' and determines the \n    relevance parameters by user specification (the use of a domain expert \n    is recommended for utilizing this method)\n    \n    returns a dictionary containing 3 items \"method\", \"num_pts\", \"ctrl_pts\": \n    1) the \"method\" item contains a chartacter string simply indicating the \n    method used calculate the relevance parameters (control points) either \n    \"auto\" or \"manual\"\n    \n    2) the \"num_pts\" item contains a positive integer simply indicating the \n    number of relevance parameters returned, typically 3\n    \n    3) the \"ctrl_pts\" item contains an array indicating the regions of \n    interest in the response variable y and their corresponding relevance \n    values mapped to either 0 or 1, expressed as [y, 0, 1]\n    \n    ref:\n    \n    Branco, P., Ribeiro, R., Torgo, L. (2017).\n    Package 'UBL'. The Comprehensive R Archive Network (CRAN).\n    https:\/\/cran.r-project.org\/web\/packages\/UBL\/UBL.pdf.\n    \n    Ribeiro, R. (2011). Utility-Based Regression.\n    (PhD Dissertation, Dept. Computer Science, \n    Faculty of Sciences, University of Porto).\n    \"\"\"\n    \n    ## quality control check for response variable 'y'\n    if any(y == None) or isinstance(y, (int, float, complex)):\n        print(\"response variable 'y' must be specified and numeric\")\n    \n    ## quality control check for user specified method\n    if method in [\"auto\", \"manual\"] == False:\n        print(\"method must be either: 'auto' or 'manual'\")\n    \n    ## conduct 'extremes' method (default)\n    if method == \"auto\":\n        phi_params = phi_extremes(y, xtrm_type, coef, ctrl_pts)\n    \n    ## conduct 'range' method\n    if method == \"manual\":\n        phi_params = phi_range(y, xtrm_type, coef, ctrl_pts)\n    \n    ## return phi relevance parameters dictionary\n    return phi_params\n\n## calculates phi parameters for statistically extreme values\ndef phi_extremes(y, xtrm_type, coef, ctrl_pts):\n  \n    \"\"\" \n    assigns relevance to the most extreme values in the distribution of response \n    variable y according to the box plot stats generated from 'box_plot_stat()'\n    \"\"\"\n    \n    ## create 'ctrl_pts' variable\n    ctrl_pts = []\n    \n    ## calculates statistically extreme values by\n    ## box plot stats in the response variable y\n    ## (see function 'boxplot_stats()' for details)\n    bx_plt_st = box_plot_stats(y, coef)\n    \n    ## calculate range of the response variable y\n    rng = [y.min(), y.max()]\n    \n    ## adjust low\n    if xtrm_type in [\"both\", \"low\"] and any(bx_plt_st[\"xtrms\"]\n    < bx_plt_st[\"stats\"][0]):\n        ctrl_pts.extend([bx_plt_st[\"stats\"][0], 1, 0])\n   \n    ## min\n    else:\n        ctrl_pts.extend([rng[0], 0, 0])\n      \n    ## median\n    if bx_plt_st[\"stats\"][2] != rng[0]:\n        ctrl_pts.extend([bx_plt_st[\"stats\"][2], 0, 0])\n    \n    ## adjust high\n    if xtrm_type in [\"both\", \"high\"] and any(bx_plt_st[\"xtrms\"]\n    > bx_plt_st[\"stats\"][4]):\n        ctrl_pts.extend([bx_plt_st[\"stats\"][4], 1, 0])\n    \n    ## max\n    else:\n        if bx_plt_st[\"stats\"][2] != rng[1]:\n            ctrl_pts.extend([rng[1], 0, 0])\n    \n    ## store phi relevance parameter dictionary\n    phi_params = {}\n    phi_params[\"method\"] = \"extremes\"\n    phi_params[\"num_pts\"] = round(len(ctrl_pts) \/ 3)\n    phi_params[\"ctrl_pts\"] = ctrl_pts\n    \n    ## return dictionary\n    return phi_params\n\n## calculates phi parameters for user specified range\ndef phi_range(y, xtrm_type, coef, ctrl_pts):\n    \n    \"\"\"\n    assigns relevance to values in the response variable y according to user \n    specification, when specifying relevant regions use matrix format [x, y, m]\n    \n    x is an array of relevant values in the response variable y, y is an array \n    of values mapped to 1 or 0, and m is typically an array of zeros\n    \n    m is the phi derivative adjusted afterward by the phi relevance function to \n    interpolate a smooth and continous monotonically increasing function\n    \n    example:\n    [x,  y, m],\n    [15, 1, 0],\n    [30, 0, 0],\n    [55, 1, 0]\n    \"\"\"\n    \n    ## load dependencies\n    # import numpy as np\n    \n    ## convert 'ctrl_pts' to numpy 2d array (matrix)\n    ctrl_pts = np.array(ctrl_pts)\n                   \n    ## quality control checks for user specified phi relevance values\n    if np.isnan(ctrl_pts).any() or np.size(ctrl_pts, axis = 1) > 3 or np.size(\n        ctrl_pts, axis = 1) < 2 or not isinstance(ctrl_pts, (np.ndarray)):\n        print(\"ctrl_pts must be given as a matrix in the form: [x, y, m]\" \n              \"or [x, y]\")\n    \n    if (ctrl_pts[1: ,[1, ]] > 1).any() or (ctrl_pts[1: ,[1, ]] < 0).any():\n        print(\"phi relevance function only maps values: [0, 1]\")\n          \n    ## store number of control points \n    else:\n        num_pts = np.size(ctrl_pts, axis = 0)\n        dx = ctrl_pts[1:,[0,]] - ctrl_pts[0:-1,[0,]]\n    \n    ## quality control check for dx\n    if np.isnan(dx).any() or dx.any() == 0:\n        print(\"x must strictly increase (not na)\")\n    \n    ## sort control points from lowest to highest\n    else:\n        ctrl_pts = ctrl_pts[np.argsort(ctrl_pts[:,0])]\n    \n    ## calculate for two column user specified control points [x, y]\n    if np.size(ctrl_pts, axis = 1) == 2:\n        \n        ## monotone hermite spline method by fritsch & carlson (monoH.FC)\n        dx = ctrl_pts[1:,[0,]] - ctrl_pts[0:-1,[0,]]\n        dy = ctrl_pts[1:,[1,]] - ctrl_pts[0:-1,[1,]]\n        sx = dy \/ dx\n        \n        ## calculate constant extrapolation\n        m = np.divide(sx[1:] + sx[0:-1], 2)\n        m = np.array(ex).ravel().tolist()\n        m.insert(0, 0)\n        m.insert(len(ex), 0)\n        \n        ## add calculated column 'm' to user specified control points \n        ## from [x, y] to [x, y, m] and store in 'ctrl_pts'\n        ctrl_pts = np.insert(ctrl_pts, 2, m, axis = 1)\n    \n    ## store phi relevance parameter dictionary\n    phi_params = {}\n    phi_params[\"method\"] = \"range\"\n    phi_params[\"num_pts\"] = np.size(ctrl_pts, axis = 0)\n    phi_params[\"ctrl_pts\"] = np.array(ctrl_pts).ravel().tolist()\n    \n    ## return dictionary\n    return phi_params\n\n## calculate parameters for phi relevance function\ndef phi_ctrl_pts(\n    \n    ## arguments \/ inputs\n    y,                    ## response variable y\n    method = \"auto\",      ## relevance method (\"auto\" or \"manual\")\n    xtrm_type = \"both\",   ## distribution focus (\"high\", \"low\", \"both\")\n    coef = 1.5,           ## coefficient for box plot\n    ctrl_pts = None       ## input for \"manual\" rel method\n    \n    ):\n    \n    \"\"\" \n    generates the parameters required for the 'phi()' function, specifies the \n    regions of interest or 'relevance' in the response variable y, the notion \n    of relevance can be associated with rarity\n    \n    controls how the relevance parameters are calculated by selecting between \n    two methods, either \"auto\" or \"manual\"\n    \n    the \"auto\" method calls the function 'phi_extremes()' and calculates the \n    relevance parameters by the values beyond the interquartile range\n    \n    the \"manual\" method calls the function 'phi_range()' and determines the \n    relevance parameters by user specification (the use of a domain expert \n    is recommended for utilizing this method)\n    \n    returns a dictionary containing 3 items \"method\", \"num_pts\", \"ctrl_pts\": \n    1) the \"method\" item contains a chartacter string simply indicating the \n    method used calculate the relevance parameters (control points) either \n    \"auto\" or \"manual\"\n    \n    2) the \"num_pts\" item contains a positive integer simply indicating the \n    number of relevance parameters returned, typically 3\n    \n    3) the \"ctrl_pts\" item contains an array indicating the regions of \n    interest in the response variable y and their corresponding relevance \n    values mapped to either 0 or 1, expressed as [y, 0, 1]\n    \n    ref:\n    \n    Branco, P., Ribeiro, R., Torgo, L. (2017).\n    Package 'UBL'. The Comprehensive R Archive Network (CRAN).\n    https:\/\/cran.r-project.org\/web\/packages\/UBL\/UBL.pdf.\n    \n    Ribeiro, R. (2011). Utility-Based Regression.\n    (PhD Dissertation, Dept. Computer Science, \n    Faculty of Sciences, University of Porto).\n    \"\"\"\n    \n    ## quality control check for response variable 'y'\n    if any(y == None) or isinstance(y, (int, float, complex)):\n        print(\"response variable 'y' must be specified and numeric\")\n    \n    ## quality control check for user specified method\n    if method in [\"auto\", \"manual\"] == False:\n        print(\"method must be either: 'auto' or 'manual'\")\n    \n    ## conduct 'extremes' method (default)\n    if method == \"auto\":\n        phi_params = phi_extremes(y, xtrm_type, coef, ctrl_pts)\n    \n    ## conduct 'range' method\n    if method == \"manual\":\n        phi_params = phi_range(y, xtrm_type, coef, ctrl_pts)\n    \n    ## return phi relevance parameters dictionary\n    return phi_params\n\n## calculates phi parameters for statistically extreme values\ndef phi_extremes(y, xtrm_type, coef, ctrl_pts):\n  \n    \"\"\" \n    assigns relevance to the most extreme values in the distribution of response \n    variable y according to the box plot stats generated from 'box_plot_stat()'\n    \"\"\"\n    \n    ## create 'ctrl_pts' variable\n    ctrl_pts = []\n    \n    ## calculates statistically extreme values by\n    ## box plot stats in the response variable y\n    ## (see function 'boxplot_stats()' for details)\n    bx_plt_st = box_plot_stats(y, coef)\n    \n    ## calculate range of the response variable y\n    rng = [y.min(), y.max()]\n    \n    ## adjust low\n    if xtrm_type in [\"both\", \"low\"] and any(bx_plt_st[\"xtrms\"]\n    < bx_plt_st[\"stats\"][0]):\n        ctrl_pts.extend([bx_plt_st[\"stats\"][0], 1, 0])\n   \n    ## min\n    else:\n        ctrl_pts.extend([rng[0], 0, 0])\n      \n    ## median\n    if bx_plt_st[\"stats\"][2] != rng[0]:\n        ctrl_pts.extend([bx_plt_st[\"stats\"][2], 0, 0])\n    \n    ## adjust high\n    if xtrm_type in [\"both\", \"high\"] and any(bx_plt_st[\"xtrms\"]\n    > bx_plt_st[\"stats\"][4]):\n        ctrl_pts.extend([bx_plt_st[\"stats\"][4], 1, 0])\n    \n    ## max\n    else:\n        if bx_plt_st[\"stats\"][2] != rng[1]:\n            ctrl_pts.extend([rng[1], 0, 0])\n    \n    ## store phi relevance parameter dictionary\n    phi_params = {}\n    phi_params[\"method\"] = \"extremes\"\n    phi_params[\"num_pts\"] = round(len(ctrl_pts) \/ 3)\n    phi_params[\"ctrl_pts\"] = ctrl_pts\n    \n    ## return dictionary\n    return phi_params\n\n## calculates phi parameters for user specified range\ndef phi_range(y, xtrm_type, coef, ctrl_pts):\n    \n    \"\"\"\n    assigns relevance to values in the response variable y according to user \n    specification, when specifying relevant regions use matrix format [x, y, m]\n    \n    x is an array of relevant values in the response variable y, y is an array \n    of values mapped to 1 or 0, and m is typically an array of zeros\n    \n    m is the phi derivative adjusted afterward by the phi relevance function to \n    interpolate a smooth and continous monotonically increasing function\n    \n    example:\n    [x,  y, m],\n    [15, 1, 0],\n    [30, 0, 0],\n    [55, 1, 0]\n    \"\"\"\n    \n    ## load dependencies\n    # import numpy as np\n    \n    ## convert 'ctrl_pts' to numpy 2d array (matrix)\n    ctrl_pts = np.array(ctrl_pts)\n                   \n    ## quality control checks for user specified phi relevance values\n    if np.isnan(ctrl_pts).any() or np.size(ctrl_pts, axis = 1) > 3 or np.size(\n        ctrl_pts, axis = 1) < 2 or not isinstance(ctrl_pts, (np.ndarray)):\n        print(\"ctrl_pts must be given as a matrix in the form: [x, y, m]\" \n              \"or [x, y]\")\n    \n    if (ctrl_pts[1: ,[1, ]] > 1).any() or (ctrl_pts[1: ,[1, ]] < 0).any():\n        print(\"phi relevance function only maps values: [0, 1]\")\n          \n    ## store number of control points \n    else:\n        num_pts = np.size(ctrl_pts, axis = 0)\n        dx = ctrl_pts[1:,[0,]] - ctrl_pts[0:-1,[0,]]\n    \n    ## quality control check for dx\n    if np.isnan(dx).any() or dx.any() == 0:\n        print(\"x must strictly increase (not na)\")\n    \n    ## sort control points from lowest to highest\n    else:\n        ctrl_pts = ctrl_pts[np.argsort(ctrl_pts[:,0])]\n    \n    ## calculate for two column user specified control points [x, y]\n    if np.size(ctrl_pts, axis = 1) == 2:\n        \n        ## monotone hermite spline method by fritsch & carlson (monoH.FC)\n        dx = ctrl_pts[1:,[0,]] - ctrl_pts[0:-1,[0,]]\n        dy = ctrl_pts[1:,[1,]] - ctrl_pts[0:-1,[1,]]\n        sx = dy \/ dx\n        \n        ## calculate constant extrapolation\n        m = np.divide(sx[1:] + sx[0:-1], 2)\n        m = np.array(ex).ravel().tolist()\n        m.insert(0, 0)\n        m.insert(len(ex), 0)\n        \n        ## add calculated column 'm' to user specified control points \n        ## from [x, y] to [x, y, m] and store in 'ctrl_pts'\n        ctrl_pts = np.insert(ctrl_pts, 2, m, axis = 1)\n    \n    ## store phi relevance parameter dictionary\n    phi_params = {}\n    phi_params[\"method\"] = \"range\"\n    phi_params[\"num_pts\"] = np.size(ctrl_pts, axis = 0)\n    phi_params[\"ctrl_pts\"] = np.array(ctrl_pts).ravel().tolist()\n    \n    ## return dictionary\n    return phi_params\n\n## calculate box plot statistics\ndef box_plot_stats(\n    \n    ## arguments \/ inputs\n    x,          ## input array of values \n    coef = 1.5  ## positive real number\n                ## (determines how far the whiskers extend from the iqr)\n    ):          \n    \n    \"\"\" \n    calculates box plot five-number summary: the lower whisker extreme, the \n    lower \u2018hinge\u2019 (observed value), the median, the upper \u2018hinge\u2019, and upper \n    whisker extreme (observed value)\n    \n    returns a results dictionary containing 2 items: \"stats\" and \"xtrms\"\n    1) the \"stats\" item contains the box plot five-number summary as an array\n    2) the \"xtrms\" item contains values which lie beyond the box plot extremes\n    \n    functions much the same as R's 'boxplot.stats()' function for which this\n    Python implementation was predicated\n    \n    ref:\n    \n    The R Project for Statistical Computing. (2019). Box Plot Statistics. \n    http:\/\/finzi.psych.upenn.edu\/R\/library\/grDevices\/html\/boxplot.stats.html.\n    \n    Tukey, J. W. (1977). Exploratory Data Analysis. Section 2C.\n    McGill, R., Tukey, J.W. and Larsen, W.A. (1978). Variations of Box Plots. \n    The American Statistician, 32:12-16. http:\/\/dx.doi.org\/10.2307\/2683468.\n    Velleman, P.F. and Hoaglin, D.C. (1981). Applications, Basics and \n    Computing of Exploratory Data Analysis. Duxbury Press.\n    Emerson, J.D. and Strenio, J. (1983). Boxplots and Batch Comparison. \n    Chapter 3 of Understanding Robust and Exploratory Data Analysis, \n    eds. D.C. Hoaglin, F. Mosteller and J.W. Tukey. Wiley.\n    Chambers, J.M., Cleveland, W.S., Kleiner, B. and Tukey, P.A. (1983). \n    Graphical Methods for Data Analysis. Wadsworth & Brooks\/Cole.\n    \"\"\"\n    \n    ## load dependency\n    # import numpy as np\n    \n    ## convert input to numpy array\n    x = np.array(x)\n    \n    ## determine median, lower \u2018hinge\u2019, upper \u2018hinge\u2019\n    median = np.quantile(a = x, q = 0.50, interpolation = \"midpoint\")\n    first_quart = np.quantile(a = x, q = 0.25, interpolation = \"midpoint\")\n    third_quart = np.quantile(a = x, q = 0.75, interpolation = \"midpoint\")\n    \n    ## calculate inter quartile range\n    intr_quart_rng = third_quart - first_quart\n    \n    ## calculate extreme of the lower whisker (observed, not interpolated)\n    lower = first_quart - (coef * intr_quart_rng)\n    lower_whisk = np.compress(x >= lower, x)\n    lower_whisk_obs = np.min(lower_whisk)\n    \n    ## calculate extreme of the upper whisker (observed, not interpolated)\n    upper = third_quart + (coef * intr_quart_rng)\n    upper_whisk = np.compress(x <= upper, x)\n    upper_whisk_obs = np.max(upper_whisk)\n    \n    ## store box plot results dictionary\n    boxplot_stats = {}\n    boxplot_stats[\"stats\"] = np.array([lower_whisk_obs, \n                                       first_quart, \n                                       median, \n                                       third_quart, \n                                       upper_whisk_obs])\n   \n    ## store observations beyond the box plot extremes\n    boxplot_stats[\"xtrms\"] = np.array(x[(x < lower_whisk_obs) | \n                                        (x > upper_whisk_obs)])\n    \n    ## return dictionary        \n    return boxplot_stats\n\n## calculate the phi relevance function\ndef phi(\n    \n    ## arguments \/ inputs\n    y,        ## reponse variable y\n    ctrl_pts  ## params from the 'ctrl_pts()' function\n    \n    ):\n    \n    \"\"\"\n    generates a monotonic piecewise cubic spline from a sorted list (ascending)\n    of the response variable y in order to determine which observations exceed \n    a given threshold ('rel_thres' argument in the main 'smogn()' function)\n    \n    returns an array of length n (number of observations in the training set) of \n    the phi relevance values corresponding to each observation in y to determine\n    whether or not an given observation in y is considered 'normal' or 'rare'\n    \n    the 'normal' observations get placed into a majority class subset or 'bin' \n    (normal bin) and are under-sampled, while the 'rare' observations get placed \n    into seperate minority class subset (rare bin) where they are over-sampled\n    \n    the original implementation was as an R foreign function call to C and later \n    adapted to Fortran 90, but was implemented here in Python for the purposes\n    of consistency and maintainability\n    \n    ref:\n    \n    Branco, P., Ribeiro, R., Torgo, L. (2017). \n    Package 'UBL'. The Comprehensive R Archive Network (CRAN).\n    https:\/\/cran.r-project.org\/web\/packages\/UBL\/UBL.pdf.\n    \n    Fritsch, F., Carlson, R. (1980).\n    Monotone Piecewise Cubic Interpolation.\n    SIAM Journal on Numerical Analysis, 17(2):238-246.\n    https:\/\/doi.org\/10.1137\/0717021.\n    \n    Ribeiro, R. (2011). Utility-Based Regression.\n    (PhD Dissertation, Dept. Computer Science, \n    Faculty of Sciences, University of Porto).\n    \"\"\"\n    \n    ## assign variables\n    y = y                                ## reponse variable y\n    n = len(y)                           ## number of points in y\n    num_pts = ctrl_pts[\"num_pts\"]    ## number of control points\n    ctrl_pts = ctrl_pts[\"ctrl_pts\"]  ## control points\n    \n    ## reindex y\n    y = y.reset_index(drop = True)\n    \n    ## initialize phi relevance function\n    y_phi = phi_init(y, n, num_pts, ctrl_pts)\n    \n    ## return phi values\n    return y_phi\n\n## pre-process control points and calculate phi values\ndef phi_init(y, n, num_pts, ctrl_pts):\n    \n    ## construct control point arrays\n    x = []\n    y_rel = []\n    m = []\n    \n    for i in range(num_pts):\n        x.append(ctrl_pts[3 * i])\n        y_rel.append(ctrl_pts[3 * i + 1])\n        m.append(ctrl_pts[3 * i + 2])\n    \n    ## calculate auxilary coefficients for 'pchip_slope_mono_fc()'\n    h = []\n    delta = []\n    \n    for i in range(num_pts - 1):\n        h.append(x[i + 1] - x[i])\n        delta.append((y_rel[i + 1] - y_rel[i]) \/ h[i])\n    \n    ## conduct monotone piecewise cubic interpolation\n    m_adj = pchip_slope_mono_fc(m, delta, num_pts)\n    \n    ## assign variables for 'pchip_val()'\n    a = y_rel\n    b = m_adj\n    \n    ## calculate auxilary coefficients for 'pchip_val()'\n    c = []\n    d = []\n    \n    for i in range(num_pts - 1):\n        c.append((3 * delta[i] - 2 * m_adj[i] - m_adj[i + 1]) \/ h[i])\n        d.append((m_adj[i] - 2 * delta[i] + m_adj[i + 1]) \/ (h[i] * h[i]))\n    \n    ## calculate phi values\n    y_phi = [None] * n\n    \n    for i in range(n):\n        y_phi[i] = pchip_val(y[i], x, a, b, c, d, num_pts)\n    \n    ## return phi values to the higher function 'phi()'\n    return y_phi\n\n## calculate slopes for shape preserving hermite cubic polynomials\ndef pchip_slope_mono_fc(m, delta, num_pts):\n    \n    for k in range(num_pts - 1):\n        sk = delta[k]\n        k1 = k + 1\n        \n        if abs(sk) == 0:\n            m[k] = m[k1] = 0\n        \n        else:\n            alpha = m[k] \/ sk\n            beta = m[k1] \/ sk\n            \n            if abs(m[k]) != 0 and alpha < 0:\n                m[k] = -m[k]\n                alpha = m[k] \/ sk\n            \n            if abs(m[k1]) != 0 and beta < 0:\n                m[k1] = -m[k1]\n                beta = m[k1] \/ sk\n            \n            ## pre-process for monotoncity check\n            m_2ab3 = 2 * alpha + beta - 3\n            m_a2b3 = alpha + 2 * beta - 3\n            \n            ## check for monotoncity\n            if m_2ab3 > 0 and m_a2b3 > 0 and alpha * (\n                m_2ab3 + m_a2b3) < (m_2ab3 * m_2ab3):\n                \n                ## fix slopes if outside of monotoncity\n                taus = 3 * sk \/ sqrt(alpha * alpha + beta * beta)\n                m[k] = taus * alpha\n                m[k1] = taus * beta\n    \n    ## return adjusted slopes m\n    return m\n\n## calculate phi values based on monotone piecewise cubic interpolation\ndef pchip_val(y, x, a, b, c, d, num_pts):\n    \n    ## load dependency\n    # import bisect as bs\n    \n    ## find interval that contains or is nearest to y\n    i = bs.bisect(\n        \n        a = x,  ## array of relevance values\n        x = y   ## single observation in y\n        ) - 1   ## minus 1 to match index position\n    \n    ## calculate phi values\n    if i == num_pts - 1:\n        y_val = a[i] + b[i] * (y - x[i])\n    \n    elif i < 0:\n        y_val = 1\n    \n    else:\n        s = y - x[i]\n        y_val = a[i] + s * (b[i] + s * (c[i] + s * d[i]))\n    \n    ## return phi values to the higher function 'phi_init()'\n    return y_val\n\n## generate synthetic observations\ndef synth_gen(\n    \n    ## arguments \/ inputs\n    data,       ## training set\n    index,      ## index of input data\n    perc,       ## % over \/ under sampling\n    k           ## num of neighs for over-sampling\n    \n    ):\n    \n    \"\"\"\n    generates synthetic observations and is the primary function underlying the\n    over-sampling technique utilized in the higher main function 'smogn()', the\n    4 step procedure for generating synthetic observations is:\n    \n    1) pre-processing: temporarily removes features without variation, label \n    encodes nominal \/ categorical features, and subsets the training set into \n    two data sets by data type: numeric \/ continuous, and nominal \/ categorical\n    \n    2) distances: calculates the cartesian distances between all observations, \n    distance metric automatically determined by data type (euclidean distance \n    for numeric only data, heom distance for both numeric and nominal data, and \n    hamming distance for nominal only data) and determine k nearest neighbors\n    \n    3) over-sampling: selects between two techniques, either synthetic minority \n    over-sampling technique for regression 'smoter' or 'smoter-gn' which applies\n    a similar interpolation method to 'smoter', but perterbs the interpolated \n    values\n    \n    'smoter' is selected when the distance between a given observation and a \n    selected nearest neighbor is within the maximum threshold (half the median \n    distance of k nearest neighbors) 'smoter-gn' is selected when a given \n    observation and a selected nearest neighbor exceeds that same threshold\n    \n    both 'smoter' and 'smoter-gn' only applies to numeric \/ continuous features, \n    for nominal \/ categorical features, synthetic values are generated at random \n    from sampling observed values found within the same feature\n    \n    4) post processing: restores original values for label encoded features, \n    reintroduces constant features previously removed, converts any interpolated\n    negative values to zero in the case of non-negative features\n    \n    returns a pandas dataframe containing synthetic observations of the training\n    set which are then returned to the higher main function 'smogn()'\n    \n    ref:\n    \n    Branco, P., Torgo, L., Ribeiro, R. (2017).\n    SMOGN: A Pre-Processing Approach for Imbalanced Regression.\n    Proceedings of Machine Learning Research, 74:36-50.\n    http:\/\/proceedings.mlr.press\/v74\/branco17a\/branco17a.pdf.\n    \n    Branco, P., Ribeiro, R., Torgo, L. (2017). \n    Package 'UBL'. The Comprehensive R Archive Network (CRAN).\n    https:\/\/cran.r-project.org\/web\/packages\/UBL\/UBL.pdf.\n    \"\"\"\n    \n    ## load dependencies\n    # import numpy as np\n    # import pandas as pd\n    # import random as rd\n    \n    ## subset original dataframe by bump classification index\n    data = data.iloc[index]\n    \n    ## store dimensions of data subset\n    n = len(data)\n    d = len(data.columns)\n    \n    ## store original data types\n    feat_dtypes_orig = [None] * d\n    \n    for j in range(d):\n        feat_dtypes_orig[j] = data.iloc[:, j].dtype\n    \n    ## find non-negative numeric features\n    feat_non_neg = [] \n    num_dtypes = [\"int64\", \"float64\"]\n    \n    for j in range(d):\n        if data.iloc[:, j].dtype in num_dtypes and any(data.iloc[:, j] > 0):\n            feat_non_neg.append(j)\n    \n    ## find features without variation (constant features)\n    feat_const = data.columns[data.nunique() == 1]\n    \n    ## temporarily remove constant features\n    if len(feat_const) > 0:\n        \n        ## create copy of orignal data and omit constant features\n        data_orig = data.copy()\n        data = data.drop(data.columns[feat_const], axis = 1)\n        \n        ## store list of features with variation\n        feat_var = list(data.columns.values)\n        \n        ## reindex features with variation\n        for i in range(d - len(feat_const)):\n            data.rename(columns = {\n                data.columns[i]: i\n                }, inplace = True)\n        \n        ## store new dimension of feature space\n        d = len(data.columns)\n    \n    ## create copy of data containing variation\n    data_var = data.copy()\n    \n    ## create global feature list by column index\n    feat_list = list(data.columns.values)\n    \n    ## create nominal feature list and\n    ## label encode nominal \/ categorical features\n    ## (strictly label encode, not one hot encode) \n    feat_list_nom = []\n    nom_dtypes = [\"object\", \"bool\", \"datetime64\"]\n    \n    for j in range(d):\n        if data.dtypes[j] in nom_dtypes:\n            feat_list_nom.append(j)\n            data.iloc[:, j] = pd.Categorical(pd.factorize(\n                data.iloc[:, j])[0])\n    \n    data = data.apply(pd.to_numeric)\n    \n    ## create numeric feature list\n    feat_list_num = list(set(feat_list) - set(feat_list_nom))\n    \n    ## calculate ranges for numeric \/ continuous features\n    ## (includes label encoded features)\n    feat_ranges = list(np.repeat(1, d))\n    \n    if len(feat_list_nom) > 0:\n        for j in feat_list_num:\n            feat_ranges[j] = max(data.iloc[:, j]) - min(data.iloc[:, j])\n    else:\n        for j in range(d):\n            feat_ranges[j] = max(data.iloc[:, j]) - min(data.iloc[:, j])\n    \n    ## subset feature ranges to include only numeric features\n    ## (excludes label encoded features)\n    feat_ranges_num = [feat_ranges[i] for i in feat_list_num]\n    \n    ## subset data by either numeric \/ continuous or nominal \/ categorical\n    data_num = data.iloc[:, feat_list_num]\n    data_nom = data.iloc[:, feat_list_nom]\n    \n    ## get number of features for each data type\n    feat_count_num = len(feat_list_num)\n    feat_count_nom = len(feat_list_nom)\n    \n    ## calculate distance between observations based on data types\n    ## store results over null distance matrix of n x n\n    dist_matrix = np.ndarray(shape = (n, n))\n    \n    for i in range(n):\n        for j in range(n):\n            \n            ## utilize euclidean distance given that \n            ## data is all numeric \/ continuous\n            if feat_count_nom == 0:\n                dist_matrix[i][j] = euclidean_dist(\n                    a = data_num.iloc[i],\n                    b = data_num.iloc[j],\n                    d = feat_count_num\n                )\n            \n            ## utilize heom distance given that \n            ## data contains both numeric \/ continuous \n            ## and nominal \/ categorical\n            if feat_count_nom > 0 and feat_count_num > 0:\n                dist_matrix[i][j] = heom_dist(\n                    \n                    ## numeric inputs\n                    a_num = data_num.iloc[i],\n                    b_num = data_num.iloc[j],\n                    d_num = feat_count_num,\n                    ranges_num = feat_ranges_num,\n                    \n                    ## nominal inputs\n                    a_nom = data_nom.iloc[i],\n                    b_nom = data_nom.iloc[j],\n                    d_nom = feat_count_nom\n                )\n            \n            ## utilize hamming distance given that \n            ## data is all nominal \/ categorical\n            if feat_count_num == 0:\n                dist_matrix[i][j] = overlap_dist(\n                    a = data_nom.iloc[i],\n                    b = data_nom.iloc[j],\n                    d = feat_count_nom\n                )\n    \n    ## determine indicies of k nearest neighbors\n    ## and convert knn index list to matrix\n    knn_index = [None] * n\n    \n    for i in range(n):\n        knn_index[i] = np.argsort(dist_matrix[i])[1:k + 1]\n    \n    knn_matrix = np.array(knn_index)\n    \n    ## calculate max distances to determine if gaussian noise is applied\n    ## (half the median of the distances per observation)\n    max_dist = [None] * n\n    \n    for i in range(n):\n        max_dist[i] = box_plot_stats(dist_matrix[i])[\"stats\"][2] \/ 2\n    \n    ## number of new synthetic observations for each rare observation\n    x_synth = int(perc - 1)\n    \n    ## total number of new synthetic observations to generate\n    n_synth = int(n * (perc - 1 - x_synth))\n    \n    ## randomly index data by the number of new synthetic observations\n    r_index = np.random.choice(\n        a = tuple(range(0, n)), \n        size = n_synth, \n        replace = False, \n        p = None\n    )\n    \n    ## create null matrix to store new synthetic observations\n    synth_matrix = np.ndarray(shape = ((x_synth * n + n_synth), d))\n    \n    if x_synth > 0:\n        for i in range(n):\n            \n            ## determine which cases are 'safe' to interpolate\n            safe_list = np.where(\n                dist_matrix[i, knn_matrix[i]] < max_dist[i])[0]\n            \n            for j in range(x_synth):\n                \n                ## randomly select a k nearest neighbor\n                neigh = int(np.random.choice(\n                    a = tuple(range(k)), \n                    size = 1))\n                \n                ## conduct synthetic minority over-sampling\n                ## technique for regression (smoter)\n                if neigh in safe_list:\n                    diffs = data.iloc[\n                        knn_matrix[i, neigh], 0:(d - 1)] - data.iloc[\n                        i, 0:(d - 1)]\n                    synth_matrix[i * x_synth + j, 0:(d - 1)] = data.iloc[\n                        i, 0:(d - 1)] + rd.random() * diffs\n                    \n                    ## randomly assign nominal \/ categorical features from\n                    ## observed cases and selected neighbors\n                    for x in feat_list_nom:\n                        synth_matrix[i * x_synth + j, x] = [data.iloc[\n                            knn_matrix[i, neigh], x], data.iloc[\n                            i, x]][round(rd.random())]\n                    \n                    ## generate synthetic y response variable by\n                    ## inverse distance weighted\n                    for z in feat_list_num:\n                        a = abs(data.iloc[i, z] - synth_matrix[\n                            i * x_synth + j, z]) \/ feat_ranges[z]\n                        b = abs(data.iloc[knn_matrix[\n                            i, neigh], z] - synth_matrix[\n                            i * x_synth + j, z]) \/ feat_ranges[z]\n                    \n                    if len(feat_list_nom) > 0:\n                        a = a + sum(data.iloc[\n                            i, feat_list_nom] != synth_matrix[\n                            i * x_synth + j, feat_list_nom])\n                        b = b + sum(data.iloc[knn_matrix[\n                            i, neigh], feat_list_nom] != synth_matrix[\n                            i * x_synth + j, feat_list_nom])\n                    \n                    if a == b:\n                        synth_matrix[i * x_synth + j, \n                            (d - 1)] = data.iloc[i, (d - 1)] + data.iloc[\n                            knn_matrix[i, neigh], (d - 1)] \/ 2\n                    else:\n                        synth_matrix[i * x_synth + j, \n                            (d - 1)] = (b * data.iloc[\n                            i, (d - 1)] + a * data.iloc[\n                            knn_matrix[i, neigh], (d - 1)]) \/ (a + b)\n                    \n                ## conduct synthetic minority over-sampling technique\n                ## for regression with the introduction of gaussian \n                ## noise (smoter-gn)\n                else:\n                    if max_dist[i] > 0.02:\n                        t_pert = 0.02\n                    else:\n                        t_pert = max_dist[i]\n                    \n                    index_gaus = i * x_synth + j\n                    \n                    for x in range(d):\n                        if pd.isna(data.iloc[i, x]):\n                            synth_matrix[index_gaus, x] = None\n                        else:\n                            synth_matrix[index_gaus, x] = data.iloc[\n                                i, x] + float(np.random.normal(\n                                    loc = 0,\n                                    scale = np.std(data.iloc[:, x]), \n                                    size = 1) * t_pert)\n                            \n                            if x in feat_list_nom:\n                                if len(data.iloc[:, x].unique() == 1):\n                                    synth_matrix[\n                                        index_gaus, x] = data.iloc[0, x]\n                                else:\n                                    probs = [None] * len(\n                                        data.iloc[:, x].unique())\n                                    \n                                    for z in range(len(\n                                        data.iloc[:, x].unique())):\n                                        probs[z] = len(\n                                            np.where(data.iloc[\n                                                :, x] == data.iloc[:, x][z]))\n                                    \n                                    synth_matrix[index_gaus, x] = rd.choices(\n                                        population = data.iloc[:, x].unique(), \n                                        weights = probs, \n                                        k = 1)\n    \n    if n_synth > 0:\n        count = 0\n        \n        for i in r_index:\n            \n            ## determine which cases are 'safe' to interpolate\n            safe_list = np.where(\n                dist_matrix[i, knn_matrix[i]] < max_dist[i])[0]\n            \n            ## randomly select a k nearest neighbor\n            neigh = int(np.random.choice(\n                a = tuple(range(0, k)), \n                size = 1))\n            \n            ## conduct synthetic minority over-sampling \n            ## technique for regression (smoter)\n            if neigh in safe_list:\n                diffs = data.iloc[\n                    knn_matrix[i, neigh], 0:(d - 1)] - data.iloc[i, 0:(d - 1)]\n                synth_matrix[x_synth * n + count, 0:(d - 1)] = data.iloc[\n                    i, 0:(d - 1)] + rd.random() * diffs\n                \n                ## randomly assign nominal \/ categorical features from\n                ## observed cases and selected neighbors\n                for x in feat_list_nom:\n                    synth_matrix[x_synth * n + count, x] = [data.iloc[\n                        knn_matrix[i, neigh], x], data.iloc[\n                        i, x]][round(rd.random())]\n                \n                ## generate synthetic y response variable by\n                ## inverse distance weighted\n                for z in feat_list_num:\n                    a = abs(data.iloc[i, z] - synth_matrix[\n                        x_synth * n + count, z]) \/ feat_ranges[z]\n                    b = abs(data.iloc[knn_matrix[i, neigh], z] - synth_matrix[\n                        x_synth * n + count, z]) \/ feat_ranges[z]\n                \n                if len(feat_list_nom) > 0:\n                    a = a + sum(data.iloc[i, feat_list_nom] != synth_matrix[\n                        x_synth * n + count, feat_list_nom])\n                    b = b + sum(data.iloc[\n                        knn_matrix[i, neigh], feat_list_nom] != synth_matrix[\n                        x_synth * n + count, feat_list_nom])\n                \n                if a == b:\n                    synth_matrix[x_synth * n + count, (d - 1)] = data.iloc[\n                        i, (d - 1)] + data.iloc[\n                        knn_matrix[i, neigh], (d - 1)] \/ 2\n                else:\n                    synth_matrix[x_synth * n + count, (d - 1)] = (b * data.iloc[\n                        i, (d - 1)] + a * data.iloc[\n                        knn_matrix[i, neigh], (d - 1)]) \/ (a + b)\n                \n            ## conduct synthetic minority over-sampling technique\n            ## for regression with the introduction of gaussian \n            ## noise (smoter-gn)\n            else:\n                if max_dist[i] > 0.02:\n                    t_pert = 0.02\n                else:\n                    t_pert = max_dist[i]\n                \n                for x in range(d):\n                    if pd.isna(data.iloc[i, x]):\n                        synth_matrix[x_synth * n + count, x] = None\n                    else:\n                        synth_matrix[x_synth * n + count, x] = data.iloc[\n                            i, x] + float(np.random.normal(\n                                loc = 0,\n                                scale = np.std(data.iloc[:, x]),\n                                size = 1) * t_pert)\n                        \n                        if x in feat_list_nom:\n                            if len(data.iloc[:, x].unique() == 1):\n                                synth_matrix[\n                                    x_synth * n + count, x] = data.iloc[0, x]\n                            else:\n                                probs = [None] * len(data.iloc[:, x].unique())\n                                \n                                for z in range(len(data.iloc[:, x].unique())):\n                                    probs[z] = len(np.where(\n                                        data.iloc[:, x] == data.iloc[:, x][z])\n                                    )\n                                \n                                synth_matrix[\n                                    x_synth * n + count, x] = rd.choices(\n                                        population = data.iloc[:, x].unique(), \n                                        weights = probs, \n                                        k = 1\n                                    )\n            \n            ## close loop counter\n            count = count + 1\n    \n    ## convert synthetic matrix to dataframe\n    data_new = pd.DataFrame(synth_matrix)\n    \n    ## synthetic data quality check\n    if sum(data_new.isnull().sum()) > 0:\n        print(\"oops! synthetic data contains missing values\")\n    \n    ## replace label encoded values with original values\n    for j in feat_list_nom:\n        code_list = data.iloc[:, j].unique()\n        cat_list = data_var.iloc[:, j].unique()\n        \n        for x in code_list:\n            data_new.iloc[:, j] = data_new.iloc[:, j].replace(x, cat_list[x])\n    \n    ## reintroduce constant features previously removed\n    if len(feat_const) > 0:\n        data_new.columns = feat_var\n        \n        for j in range(len(feat_const)):\n            data_new.insert(\n                loc = int(feat_const[j]),\n                column = feat_const[j], \n                value = np.repeat(\n                    data_orig.iloc[0, feat_const[j]], \n                    len(synth_matrix))\n            )\n    \n    ## convert negative values to zero in non-negative features\n    for j in feat_non_neg:\n        data_new.iloc[:, j][data_new.iloc[:, j] < 0] = 0\n    \n    return data_new\n\n## euclidean distance calculation\ndef euclidean_dist(a, b, d):\n    \n    \"\"\" \n    calculates the euclidean distance between observations for data \n    containing only numeric \/ continuous features, returns float value\n    \"\"\"\n    \n    ## load dependency\n    # import numpy as np\n    \n    ## create list to store distances\n    dist = [None] * d\n    \n    ## loop through columns to calculate euclidean \n    ## distance for numeric \/ continuous features\n    for i in range(d):\n        \n        ## the squared difference of values in\n        ## vectors a and b of equal length \n        dist[i] = (a.iloc[i] - b.iloc[i]) ** 2\n        \n    ## sum all the squared differences and take the square root\n    dist = np.sqrt(sum(dist))\n    \n    ## return distance list\n    return dist\n\n## heom distance calculation\ndef heom_dist(a_num, b_num, d_num, ranges_num, a_nom, b_nom, d_nom):\n    \n    \"\"\" \n    calculates the heterogenous euclidean overlap (heom) distance between \n    observations for data containing both numeric \/ continuous and nominal  \n    \/ categorical features, returns float value\n    \n    ref:\n        \n    Wilson, D., Martinez, T. (1997). \n    Improved Heterogeneous Distance Functions.\n    Journal of Artificial Intelligence Research, 6:1-34.\n    https:\/\/arxiv.org\/pdf\/cs\/9701101.pdf.\n    \"\"\"\n    \n    ## load dependency\n    # import numpy as np\n    \n    ## create list to store distances\n    dist = [None] * d_num\n    \n    ## specify epsilon\n    eps = 1e-30\n    \n    ## loop through columns to calculate euclidean \n    ## distance for numeric \/ continuous features\n    for i in range(d_num):\n        \n        ## epsilon utilized to avoid division by zero\n        if ranges_num[i] > eps:\n        \n            ## the absolute value of the differences between values in\n            ## vectors a and b of equal length, divided by their range, squared\n            ## (division by range conducted for normalization)\n            dist[i] = (abs(a_num.iloc[i] - b_num.iloc[i]) \/ ranges_num[i]) ** 2\n    \n    ## loop through columns to calculate hamming\n    ## distance for nominal \/ categorical features\n    for i in range(d_nom):\n        \n        ## distance equals 0 for values that are equal\n        ## in two vectors a and b of equal length\n        if a_nom.iloc[i] == b_nom.iloc[i]:\n            dist[i] = 0\n        \n        ## distance equals 1 for values that are not equal\n        else:\n            dist[i] = 1\n        \n        ## theoretically, hamming differences are squared when utilized\n        ## within heom distance, however, procedurally not required, \n        ## as squaring [0,1] returns same result\n    \n    ## sum all the squared differences and take the square root\n    dist = np.sqrt(sum(dist))\n    \n    ## return distance\n    return dist\n\n## hamming distance calculation\ndef overlap_dist(a, b, d):\n    \n    \"\"\" \n    calculates the hamming (overlap) distance between observations for data \n    containing only nominal \/ categorical features, returns float value\n    \"\"\"\n    \n    ## create list to store distances\n    dist = [None] * d\n    \n    ## loop through columns to calculate hamming\n    ## distance for nominal \/ categorical features\n    for i in range(d):\n        \n        ## distance equals 0 for values that are equal\n        ## in two vectors a and b of equal length\n        if a.iloc[i] == b.iloc[i]:\n            dist[i] = 0\n        \n        ## distance equals 1 for values that are not equal\n        else:\n            dist[i] = 1\n    \n    ## sum all the differences   \n    dist = sum(dist)\n    \n    ## return distance\n    return dist\n\n## synthetic minority over \/ under sampling with gaussian noise for regression\ndef smogn(\n    \n    ## primary arguments \/ inputs\n    data,                     ## training set  (pandas dataframe)\n    y,                        ## response variable y by name  (string)\n    k = 5,                    ## num of neighs for over-sampling  (pos int)\n    samp_method = \"balance\",  ## % over \/ under sample  (\"balance\" or extreme\")\n    drop_na_col = True,       ## auto drop columns with nan's  (bool)\n    drop_na_row = True,       ## auto drop rows with nan's  (bool)\n    replace = False,          ## sampling replacement  (bool)\n    \n    ## phi relevance function arguments \/ inputs\n    rel_thres = 0.5,          ## relevance threshold considered rare  (pos real)\n    rel_method = \"auto\",      ## relevance method  (\"auto\" or \"manual\")\n    rel_xtrm_type = \"both\",   ## distribution focus  (\"high\", \"low\", \"both\")\n    rel_coef = 1.5,           ## coefficient for box plot  (pos real)\n    rel_ctrl_pts_rg = None    ## input for \"manual\" rel method  (2d array)\n    \n    ):\n    \n    \"\"\"\n    the main function, designed to help solve the problem of imbalanced data \n    for regression, much the same as SMOTE for classification, SMOGN applies \n    the combintation of under-sampling the majority class (in the case of \n    regression, values commonly found near the mean of a normal distribution \n    in the response variable y) and over-sampling the minority class (rare \n    values in a normal distribution of y, typically found at the tails)\n    \n    procedure begins with a series of pre-processing steps, and to ensure no \n    missing values (nan's), sorts the values in the response variable y by\n    ascending order, and fits a function 'phi' to y, corresponding phi values \n    (between 0 and 1) are generated for each value in y, the phi values are \n    then used to determine if an observation is either normal or rare by the \n    threshold specified in the argument 'rel_thres' \n    \n    normal observations are placed into a majority class subset (normal bin) \n    and are under-sampled, while rare observations are placed in a seperate \n    minority class subset (rare bin) where they're over-sampled\n    \n    under-sampling is applied by a random sampling from the normal bin based \n    on a calculated percentage control by the argument 'samp_method', if the \n    specified input of 'samp_method' is \"balance\", less under-sampling (and \n    over-sampling) is conducted, and if \"extreme\" is specified more under-\n    sampling (and over-sampling is conducted)\n    \n    over-sampling is applied one of two ways, either synthetic minority over-\n    sampling technique for regression 'smoter' or 'smoter-gn' which applies a \n    similar interpolation method to 'smoter', but takes an additional step to\n    perterb the interpolated values with gaussian noise\n    \n    'smoter' is selected when the distance between a given observation and a \n    selected nearest neighbor is within the maximum threshold (half the median \n    distance of k nearest neighbors) 'smoter-gn' is selected when a given \n    observation and a selected nearest neighbor exceeds that same threshold\n    \n    both 'smoter' and 'smoter-gn' are only applied to numeric \/ continuous \n    features, synthetic values found in nominal \/ categorical features, are \n    generated by randomly selecting observed values found within their \n    respective feature\n    \n    procedure concludes by post-processing and returns a modified pandas data\n    frame containing under-sampled and over-sampled (synthetic) observations, \n    the distribution of the response variable y should more appropriately \n    reflect the minority class areas of interest in y that are under-\n    represented in the original training set\n    \n    ref:\n    \n    Branco, P., Torgo, L., Ribeiro, R. (2017).\n    SMOGN: A Pre-Processing Approach for Imbalanced Regression.\n    Proceedings of Machine Learning Research, 74:36-50.\n    http:\/\/proceedings.mlr.press\/v74\/branco17a\/branco17a.pdf.\n    \"\"\"\n    \n    ## load dependencies\n    import numpy as np\n    import pandas as pd\n    import random as rd\n    import bisect as bs\n    \n    ## pre-process missing values\n    if bool(drop_na_col) == True:\n        data = data.dropna(axis = 1)  ## drop columns with nan's\n    \n    if bool(drop_na_row) == True:\n        data = data.dropna(axis = 0)  ## drop rows with nan's\n    \n    ## data quality check\n    if data.isnull().values.any():\n        print(\"cannot proceed: data cannot contain NaN values\")\n    \n    ## relative threshold parameter quality check\n    if rel_thres == None:\n        print(\"cannot proceed: phi relevance threshold required\")\n    \n    ## input quality check for k number specification\n    if k > len(data):\n        print(\"cannot proceed: k is greater than number of \\\n               observations \/ rows contained in the dataframe\")\n    \n    ## store data dimensions\n    n = len(data)\n    d = len(data.columns)\n    \n    ## store original data types\n    feat_dtypes_orig = [None] * d\n    \n    for j in range(d):\n        feat_dtypes_orig[j] = data.iloc[:, j].dtype\n    \n    ## determine column position for response variable y\n    y_col = data.columns.get_loc(y)\n    \n    ## move response variable y to last column\n    if y_col < d - 1:\n        cols = list(range(d))\n        cols[y_col], cols[d - 1] = cols[d - 1], cols[y_col]\n        data = data[data.columns[cols]]\n    \n    ## store original feature headers and\n    ## encode feature headers to index position\n    feat_names = list(data.columns)\n    data.columns = range(d)\n    \n    ## sort response variable y by ascending order\n    y = pd.DataFrame(data[d - 1])\n    y_sort = y.sort_values(by = d - 1)\n    y_sort = y_sort[d - 1]\n    \n    ## -------------------------------- phi --------------------------------- ##\n    ## calculate parameters for phi relevance function\n    ## (see 'phi_ctrl_pts()' function for details)\n    phi_params = phi_ctrl_pts(\n        \n        y = y_sort,                ## y (ascending)\n        method = rel_method,       ## defaults \"auto\" \n        xtrm_type = rel_xtrm_type, ## defaults \"both\"\n        coef = rel_coef            ## defaults 1.5\n    )\n    \n    ## calculate the phi relevance function\n    ## (see 'phi()' function for details)\n    y_phi = phi(\n        \n        y = y_sort,                ## y (ascending)\n        ctrl_pts = phi_params      ## from 'phi_ctrl_pts()'\n    )\n    \n    ## phi relevance quality check\n    if all(i == 0 for i in y_phi):\n        print(\"redefine phi relevance function: all points are 1\")\n    \n    if all(i == 1 for i in y_phi):\n        print(\"redefine phi relevance function: all points are 0\")\n    ## ---------------------------------------------------------------------- ##\n    \n    ## determine bin (rare or normal) by bump classification\n    bumps = [0]\n    \n    for i in range(0, len(y_sort) - 1):\n        if ((y_phi[i] >= rel_thres and y_phi[i + 1] < rel_thres) or \n            (y_phi[i] < rel_thres and y_phi[i + 1] >= rel_thres)):\n            bumps.append(i + 1)\n    \n    bumps.append(n)\n            \n    ## number of bump classes\n    n_bumps = len(bumps) - 1\n    \n    ## determine indicies for each bump classification\n    b_index = {}\n    \n    for i in range(n_bumps):\n        b_index.update({i: y_sort[bumps[i]:bumps[i + 1]]})\n    \n    ## calculate over \/ under sampling percentage according to\n    ## bump class and user specified method (\"balance\" or \"extreme\")\n    b = round(n \/ n_bumps)\n    s_perc = []\n    scale = []\n    obj = []\n    \n    if samp_method == \"balance\":\n        for i in b_index:\n            s_perc.append(b \/ len(b_index[i]))\n            \n    if samp_method == \"extreme\":\n        for i in b_index:\n            scale.append(b ** 2 \/ len(b_index[i]))\n        scale = n_bumps * b \/ sum(scale)\n        \n        for i in b_index:\n            obj.append(round(b ** 2 \/ len(b_index[i]) * scale, 2))\n            s_perc.append(round(obj[i] \/ len(b_index[i]), 1))\n    \n    ## conduct over \/ under sampling and store modified training set\n    data_new = pd.DataFrame()\n    \n    for i in range(n_bumps):\n        \n        ## no sampling\n        if s_perc[i] == 1:\n            \n            ## simply return no sampling\n            ## results to modified training set\n            data_new = pd.concat([data.iloc[b_index[i].index], data_new])\n        \n        ## over-sampling\n        if s_perc[i] > 1:\n            \n            ## generate synthetic observations\n            ## (see 'synth_gen()' function for details)\n            synth_obs = synth_gen(\n                data = data,\n                index = list(b_index[i].index),\n                perc = s_perc[i],\n                k = k\n            )\n            \n            ## concatenate over-sampling\n            ## results to modified training set\n            data_new = pd.concat([synth_obs, data_new])\n        \n        ## under-sampling\n        if s_perc[i] < 1:\n            \n            ## drop observations in training set\n            ## considered 'normal' (not 'rare')\n            omit_index = np.random.choice(\n                a = list(b_index[i].index), \n                size = int(s_perc[i] * len(b_index[i])),\n                replace = replace\n            )\n            \n            omit_obs = data.drop(data.iloc[omit_index], axis = 0)\n            \n            ## concatenate under-sampling\n            ## results to modified training set\n            data_new = pd.concat([omit_obs, data_new])\n    \n    ## rename feature headers to originals\n    data_new.columns = feat_names\n    \n    ## restore response variable y to original position\n    if y_col < d - 1:\n        cols = list(range(d))\n        cols[y_col], cols[d - 1] = cols[d - 1], cols[y_col]\n        data_new = data_new[data_new.columns[cols]]\n    \n    ## restore original data types\n    for j in range(d):\n        data_new.iloc[:, j] = data_new.iloc[:, j].astype(feat_dtypes_orig[j])\n    \n    ## return modified training set\n    return data_new","6e0aa887":"# use SMOGN\nX_s = smogn(\n    \n    ## primary arguments \/ inputs\n    D,                     ## training set  (pandas dataframe)\n    'area',                        ## response variable y by name  (string)\n    k = 3,                    ## num of neighs for over-sampling  (pos int)\n    samp_method = \"balance\",  ## % over \/ under sample  (\"balance\" or extreme\")\n    drop_na_col = True,       ## auto drop columns with nan's  (bool)\n    drop_na_row = True,       ## auto drop rows with nan's  (bool)\n    replace = False,          ## sampling replacement  (bool)\n    \n    ## phi relevance function arguments \/ inputs\n    rel_thres = 0.5,          ## relevance threshold considered rare  (pos real)\n    rel_method = \"auto\",      ## relevance method  (\"auto\" or \"manual\")\n    rel_xtrm_type = \"both\",   ## distribution focus  (\"high\", \"low\", \"both\")\n    rel_coef = 1.5,           ## coefficient for box plot  (pos real)\n    rel_ctrl_pts_rg = None    ## input for \"manual\" rel method  (2d array)\n    \n    )","4fbc07b9":"X_train = X_s.drop(columns=['area'])\ny_train = X_s[['area']]\n\n\n# model and check the results\nmodel = RandomForestRegressor(max_depth=2, min_samples_split=2, min_samples_leaf=1, random_state=0, n_estimators=100)\n\nmodel.fit(X_train, y_train.values.ravel())\n\ny_pred = model.predict(X_test)\n\n# print out the prediction scores\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))\nprint('R-squared: {}'.format(r2_score(y_test, y_pred)))","89e17aa6":"# plot the results\nplt.scatter(y_pred, y_test)\nplt.plot(np.linspace(0,400,400), np.linspace(0,400,400), c = 'orange', linestyle='--')\nplt.xlabel('prediction')\nplt.ylabel('true values')\nplt.xlim(0,400)\nplt.ylim(0,400)\nplt.title('Predicted vs True values')","6f52600f":"Use the resulting dataset to train the model:","e9c58ad6":"## Conclusions\n\nIn this notebook:\n\n* I described the problem of forecasting of rare values of the target variable and ways to address this problem with resampling techniques;\n* Implemented SmoteR algorithm for oversampling the extreme rare cases;\n* Demonstrated how to use SmoteR and SMOGN using forest fires dataset.","eea2349c":"## Introduction\n\nRegression problems, when we have to forecast a continious variables are very common machine learning tasks. Often, we have to predict extremely rare values of the target variables. Prediction of the forest fire area with [this dataset](https:\/\/www.kaggle.com\/elikplim\/forest-fires-data-set) can serve as a great example of such problem. There are various methods for addressing similar problems for classification tasks, for example [SMOTE](https:\/\/arxiv.org\/pdf\/1106.1813.pdf) (see also the [imbalanced-learn package](https:\/\/github.com\/scikit-learn-contrib\/imbalanced-learn) for Python implementation). Similar approaches also exist for regression.\n\nIn this notebook I will demonstrate [SmoteR](https:\/\/core.ac.uk\/download\/pdf\/29202178.pdf) and [SMOGN](http:\/\/proceedings.mlr.press\/v74\/branco17a\/branco17a.pdf) for forecasting of forest fire areas.","0b680417":"Now we can implement SmoteR itself:","95ca2dd3":"## Add SmoteR","2b10b68b":"### Implement SmoteR","d99f3949":"[SMOGN](http:\/\/proceedings.mlr.press\/v74\/branco17a\/branco17a.pdf) is the more advanced algorithm for resampling the rare cases for regression problems. Fortunately, there is a Python [implementation of SMOGN on GitHub](https:\/\/pypi.org\/project\/smogn\/).","d199e166":"## Implement SmoteR","46937b6c":"Let's preprocess the data and prepare for the simple rigression model. I took some data preprocessing steps from [this kernel](https:\/\/www.kaggle.com\/travelcodesleep\/end-to-end-regression-pipeline-using-scikitlearn).","5212b600":"Let's start with [SmoteR](https:\/\/core.ac.uk\/download\/pdf\/29202178.pdf): a simple algorithm to generate more examples of extreme rare cases, we want to learn to predict. Unfortunately, there is no implementation in Python, but we can implement it ourselves. See the code below.","918d90f3":"This is how the sigmoid function looks like:","679dd8ba":"And this is the distribution of our target variable:","f942070e":"We can use SMOGN in the same way as SmoteR to oversample the rare cases:","c84441a5":"We can see that we are now trying to predict non-zero forest fire areas, and even improved the [r-squared score](https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination).","52a3d506":"## Load the Data","b09f6b87":"We see that the model performs bad. Ideally the dots should be near the line.\n\nThis happens because most of the records in our dataset have zero value of the target variable. Our model is trained to predict only low area values. \n\nWe could remove the datapoints with high values of the target variable to achieve much better metric scores (see [this kernel](https:\/\/www.kaggle.com\/travelcodesleep\/end-to-end-regression-pipeline-using-scikitlearn)). But this doesn't make much sense, because in this case our model won't learn to predict high values of the target variables. This is bad, because __high values of the target variable are actually of interest__. In the end we want to learn to predict fires.\n\nThat is why we need to resample the dataset in the way our model will be able to learn to predict high values of the target variable.","04f53865":"I will use the sigmoid function to create the relevance function, so that relevance function has values close to 1 when the target variable is greater than 50:","136d7491":"Loading the dataset:","6c4c0a8f":"We see a long-tailed distribution with small number of samples for high area values.","6e5ebf12":"Let's have a look at the target variable:","24e0b428":"## Regression","2e8e46d9":"## Add SMOGN","1b4fc74f":"Now let's train a simple regression model:","44fb6068":"## References and Further Reading\n1. [SmoteR paper](https:\/\/core.ac.uk\/download\/pdf\/29202178.pdf)\n2. [Utility-based regression paper](https:\/\/www.researchgate.net\/publication\/220699419_Utility-Based_Regression)\n3. [SMOGN paper](http:\/\/proceedings.mlr.press\/v74\/branco17a\/branco17a.pdf): an enchancement of SmoteR\n4. [SMOGN implementation](https:\/\/pypi.org\/project\/smogn\/) in Python\n5. [Kaggle kernel](https:\/\/www.kaggle.com\/travelcodesleep\/end-to-end-regression-pipeline-using-scikitlearn) I brought some data preprocessing from","0622761b":"We are ready to use SmoteR to predict forest fires area:","41a0a878":"### Create Relevance Function\n\nFirst of all, we need to formalize which cases we will consider as extremely rare. We will implement a `relevance function` which has small (almost zero) values for the common cases and is almost equal to 1 for rare cases. This function can come from the common sense, domain knowledge, or it can be constructed based on the [IQR](https:\/\/en.wikipedia.org\/wiki\/Interquartile_range) of the target variable distribution and a [sigmoid function](https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function) (see [this paper](https:\/\/www.researchgate.net\/publication\/220699419_Utility-Based_Regression) for the approach details).","4ff2617a":"# Addressing Extreme Rare Cases with SmoteR for Regression","92ecf52d":"I can't say that our metrics improved. But now we are actually trying to __predict something rather than zero__."}}