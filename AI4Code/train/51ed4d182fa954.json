{"cell_type":{"6f635739":"code","d79ec6d5":"code","8220a9e0":"code","1b0dd8bd":"code","f6daed06":"code","8453c339":"code","39978d3d":"code","e97119ce":"code","838752da":"code","d9bd493c":"code","a2eefa32":"code","8ec08d0f":"code","b1272032":"code","e2de7e21":"code","4a4ac375":"code","163fbc12":"code","e95a0539":"code","835ec278":"code","85257e98":"code","b3a4e7b5":"code","5c624cfb":"code","722dc830":"code","ab0f4e98":"code","487ead7d":"code","455ab34a":"code","828c95cf":"code","6399fefa":"code","77673e0a":"code","edae7d04":"code","e46a0873":"code","d5f0b718":"code","0d9e3fdd":"code","18e80437":"code","42c069a9":"code","1d1e30ec":"code","e87e3b61":"code","0a301add":"code","3dddcc46":"code","1cfab809":"code","972c703c":"markdown","a7ad3784":"markdown","06a137ae":"markdown","80e66296":"markdown","6bf8e32b":"markdown","9be4694e":"markdown","708982f6":"markdown"},"source":{"6f635739":"# Kaggle internals\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Libraries\nfrom math import sqrt\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\n# Data\ntest=pd.read_csv('..\/input\/competitive-data-science-final-project\/test.csv.gz',compression='gzip')\nsales_train=pd.read_csv(\"..\/input\/coursera-competition-getting-started-eda\/sales_train.csv\")               # From my EDA Kernel","d79ec6d5":"# Data cleaning\n# I need to have clean data that I can use to train \n\n# No NaNs in the data\n# Shops are cleaned up\n# Should I do something with negative item counts? Not for now, I think its ok.\n# Probably cleaning the outliers - the B2B deal values could be a good idea. But lets leave it for now.\n# I have to aggregate the figures to monthly values, right?\n# Construct a feature into monthly aggregated figures with \"working days\"\n\n# The value to be found (y) is \"item_cnt_month\"\n# IDs are categorical features. Do I have to encode item_ids eg one_hot? Huge and sparse!\n\"\"\"\ndate: We dont need\ndate_block_num: keep \nshop_id: keep\nitem_id: keep\nitem_price: keep and construct groups\nday: delete\nmonth: keep (for seasonality)\nyear: delete\nweekday: delete\nitem_category_id: keep\nitems_english: delete\nmeta_category: keep\nshops_english: delete\ntown: keep\nregion: keep\nrevenue: delete\nvalue: delete\n\"\"\"\n\n# Aggregate to monthly counts\ndf=pd.DataFrame()\ndf=sales_train.groupby([\"date_block_num\",\"shop_id\",\"item_id\",\"month\",\"item_price\",\"item_category_id\",\"meta_category\",\"town\",\"region\"],as_index=False).sum()[[\"date_block_num\",\"shop_id\",\"item_id\",\"month\",\"item_price\",\"item_category_id\",\"meta_category\",\"town\",\"region\",\"item_cnt_day\"]]\ndf[\"item_cnt_day\"].clip(0,20,inplace=True)\ndf=df.rename(columns = {'item_cnt_day':'item_cnt_month'})\n# unravel\ndf.head()","8220a9e0":"# Feature selection\n# Features are the input variables to the model\n\"\"\"\ndate_block_num: numerical \nshop_id: categorical\nitem_id: categorical\nitem_price: numerical\nmonth: categorical\nitem_category_id: categorical\nmeta_category: categorical\ntown: categorical\nregion: categorical\n\"\"\"\n\n# Create price categories\ndf[\"price_category\"]=np.nan\ndf[\"price_category\"][(df[\"item_price\"]>=0)&(df[\"item_price\"]<=100)]=0\ndf[\"price_category\"][(df[\"item_price\"]>100)&(df[\"item_price\"]<=200)]=1\ndf[\"price_category\"][(df[\"item_price\"]>200)&(df[\"item_price\"]<=400)]=2\ndf[\"price_category\"][(df[\"item_price\"]>400)&(df[\"item_price\"]<=750)]=3\ndf[\"price_category\"][(df[\"item_price\"]>750)&(df[\"item_price\"]<=1000)]=4\ndf[\"price_category\"][(df[\"item_price\"]>1000)&(df[\"item_price\"]<=2000)]=5\ndf[\"price_category\"][(df[\"item_price\"]>2000)&(df[\"item_price\"]<=3000)]=6\ndf[\"price_category\"][df[\"item_price\"]>3000]=7\n\nsns.countplot(df.price_category)\n\n# Label encode meta_category, town and region\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\nle.fit(df.meta_category)\ndf[\"meta_category\"]=le.transform(df.meta_category)\n\nle.fit(df.town)\ndf[\"town\"]=le.transform(df.town)\n\nle.fit(df.region)\ndf[\"region\"]=le.transform(df.region)\n\ndf.head()","1b0dd8bd":"X_train=df.drop(\"item_cnt_month\", axis=1)\n# Reason for dropping item_price explained below\ny_train=df[\"item_cnt_month\"]\n\nX_train.fillna(0, inplace=True)","f6daed06":"linmodel=LinearRegression()","8453c339":"linmodel.fit(X_train,y_train)\n\npredictions=linmodel.predict(X_train)","39978d3d":"print(sqrt(mean_squared_error(y_train,predictions)))\n# Dropping item_price didnt result in a lower RMSE","e97119ce":"linmodel.score(X_train, y_train)\n# R^2 of 1,6%?! wow.","838752da":"linmodel.coef_","d9bd493c":"X_train.columns","a2eefa32":"# Apply model to test-data\n\n# At first I have to \"blow-up\" the test data to have some features\n\n# test2=test.merge(X_train[[\"item_id\",\"shop_id\",\"item_category_id\",\"meta_category\",\"town\",\"region\",\"price_category\"]])\n# Here several things went bad:\n# Merge ignored when e.g. there was no item 5320 in shop 5 and ignored these lines\n# If shop,item combinations appeared more than once all lines were added, therefore blowing up the test set\n\n# Following carefully instructions from documentation:\n# https:\/\/pandas.pydata.org\/pandas-docs\/stable\/merging.html\n\n# And do it step by step. I found out that nearly 40% of item-shop-id pairs are new, therefore lots of nans\n# after merge for eg price if i merge on these two items.\n# but town, region, category etc depends only on one of them\n\n# Start with adding shop-related values\ntest2 = pd.merge(test,X_train[[\"shop_id\",\"town\",\"region\"]].drop_duplicates(), how=\"left\",on=[\"shop_id\"])\n\n# Continue with item-related values\ntest3 = pd.merge(test2,X_train[[\"item_id\",\"item_category_id\",\"meta_category\"]].drop_duplicates(), how=\"left\",on=[\"item_id\"])\n\n# And now the tricky part: The price\n# For existing shop-item pairs probably smart to use the latest price point\n# As there are different steps to fill the price column I do it in single pieces and later join all methods back together\npart1 = pd.merge(test3,X_train[[\"item_id\",\"shop_id\",\"item_price\"]][X_train[\"date_block_num\"]==33].drop_duplicates().groupby([\"item_id\",\"shop_id\"],as_index=False).max(), how=\"left\",on=[\"shop_id\",\"item_id\"]).dropna()\npart2 = pd.merge(test3,X_train[[\"item_id\",\"shop_id\",\"item_price\"]][X_train[\"date_block_num\"]==32].drop_duplicates(), how=\"left\",on=[\"shop_id\",\"item_id\"]).dropna()\npart3 = pd.merge(test3,X_train[[\"item_id\",\"shop_id\",\"item_price\"]][X_train[\"date_block_num\"]==31].drop_duplicates(), how=\"left\",on=[\"shop_id\",\"item_id\"]).dropna()\npart4 = pd.merge(test3,X_train[[\"item_id\",\"shop_id\",\"item_price\"]][X_train[\"date_block_num\"]==30].drop_duplicates(), how=\"left\",on=[\"shop_id\",\"item_id\"]).dropna()\npart5 = pd.merge(test3,X_train[[\"item_id\",\"shop_id\",\"item_price\"]][X_train[\"date_block_num\"]==29].drop_duplicates(), how=\"left\",on=[\"shop_id\",\"item_id\"]).dropna()\npart6 = pd.merge(test3,X_train[[\"item_id\",\"shop_id\",\"item_price\"]][X_train[\"date_block_num\"]==28].drop_duplicates(), how=\"left\",on=[\"shop_id\",\"item_id\"]).dropna()\n# half a year should be enough\n\n# For non-existing pairs lets use the average price of items\npart7 = pd.merge(test3,X_train[[\"item_id\",\"item_price\"]].drop_duplicates().groupby(\"item_id\").mean(), how=\"left\",on=[\"item_id\"]).dropna()\n\n# Now join together. Tricky. Only one value needed. Strict order. Part 1 better than part2, i.e. if \n# part 1 exists dont add part 2 and so forth\nprices=part1\nprint(\"After adding month -1:\")\nprint(prices.shape)\nids=prices.item_id\nprices=pd.concat([prices,part2[~part2.item_id.isin(ids)]])\nprint(\"After adding month -2:\")\nprint(prices.shape)\nids=prices.item_id\nprices=pd.concat([prices,part3[~part3.item_id.isin(ids)]])\nprint(\"After adding month -3:\")\nprint(prices.shape)\nids=prices.item_id\nprices=pd.concat([prices,part4[~part4.item_id.isin(ids)]])\nprint(\"After adding month -4:\")\nprint(prices.shape)\nids=prices.item_id\nprices=pd.concat([prices,part5[~part5.item_id.isin(ids)]])\nprint(\"After adding month -5:\")\nprint(prices.shape)\nids=prices.item_id\nprices=pd.concat([prices,part6[~part6.item_id.isin(ids)]])\nids=prices.item_id\nprint(\"After adding month -6:\")\nprint(prices.shape)\n\n# Until here it is clear. I build up a list of shop-item-pairs and the according prices that exist\n# Step 7 is different. This is supposed to fill \"the rest\" except 15k rows of unknown item_id\n\n# It is not about item_id, it is the shift from shop-item pairs to only according to item price-setting\n# I need shop-item-pair IDs\nprices[\"shop_item_pair_id\"]=prices.shop_id.astype(str)+\"-\"+prices.item_id.astype(str)\npart7[\"shop_item_pair_id\"]=part7.shop_id.astype(str)+\"-\"+part7.item_id.astype(str)\nprices=pd.concat([prices,part7[~part7.shop_item_pair_id.isin(prices.shop_item_pair_id)]])\nprices.drop(\"shop_item_pair_id\", axis=1,inplace=True)\nprint(\"After adding prices only according to item_id:\")\nprint(prices.shape)\n\n# Merge with original DF to include also the 15k lines with unknown items\nfinal_test=pd.merge(test3, prices[[\"shop_id\",\"item_id\",\"item_price\"]], how=\"left\",on=[\"shop_id\",\"item_id\"])\nfinal_test=final_test.groupby([\"ID\"],as_index=False).max()\n\n# Lets add the month and price_category\nfinal_test[\"date_block_num\"]=34\nfinal_test[\"month\"]=11","8ec08d0f":"# This check helped me to find duplicates\n\ncounts = final_test['ID'].value_counts()\nfinal_test[final_test['ID'].isin(counts.index[counts > 1])]","b1272032":"# Now we have the issue with NaNs for shop-item pairs that did not exists in X_train\nprint(final_test[final_test[\"item_price\"].isnull()].head())\n# 15k lines of new items. Fill with global price median\nfinal_test.item_price.fillna(final_test.item_price.median(),inplace=True)\nfinal_test.item_category_id.fillna(final_test.item_category_id.median(),inplace=True)\nfinal_test.meta_category.fillna(final_test.meta_category.median(),inplace=True)\nprint(final_test[final_test[\"item_price\"].isnull()])\nprint(final_test.shape)\n\nfinal_test.head(20)","e2de7e21":"print(X_train.columns)\nprint(final_test.columns)","4a4ac375":"# Create price categories\nfinal_test[\"price_category\"]=np.nan\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>=0)&(final_test[\"item_price\"]<=100)]=0\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>100)&(final_test[\"item_price\"]<=200)]=1\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>200)&(final_test[\"item_price\"]<=400)]=2\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>400)&(final_test[\"item_price\"]<=750)]=3\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>750)&(final_test[\"item_price\"]<=1000)]=4\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>1000)&(final_test[\"item_price\"]<=2000)]=5\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>2000)&(final_test[\"item_price\"]<=3000)]=6\nfinal_test[\"price_category\"][final_test[\"item_price\"]>3000]=7","163fbc12":"# Right order\nfinal_test=final_test[['ID','date_block_num','shop_id','item_id','month','item_price','item_category_id','meta_category','town','region','price_category']]\n\n# Now predict\npredictions=linmodel.predict(final_test.drop(\"ID\",axis=1))\nprint(predictions)","e95a0539":"output=pd.DataFrame()\noutput[\"ID\"]=final_test[\"ID\"]\noutput[\"item_cnt_month\"]=predictions\noutput.head(20)","835ec278":"output.to_csv('linmodel1.csv',index=False)\n# Score of 1.99739. Not surprisingly very bad","85257e98":"# Checks to do:\n# - Why is amount always between 1 and 2,5? Very low, isnt it?\n# - Is is correct that there are so few fitting shop\/item-id pairs from the past 6 month? only 30k?","b3a4e7b5":"# Still continue with a more meaningful linear model:\n## Pivot by month to wide format\ntrain = df.pivot_table(index=['shop_id','item_id'], columns='date_block_num', values='item_cnt_month',aggfunc='sum').fillna(0.0)\ntrain.head()","5c624cfb":"train=train.reset_index()\ntrain.head()","722dc830":"final_train=train.merge(df[[\"shop_id\",\"item_id\",\"item_price\",\"item_category_id\",\"meta_category\",\"town\",\"region\",\"price_category\"]])\nfinal_train.head()","ab0f4e98":"final_train[final_train.shop_id.isnull()]","487ead7d":"print(final_train.shape)\nfinal_train.dropna(inplace=True)\nprint(final_train.shape)","455ab34a":"linmodel=LinearRegression()\nlinmodel.fit(final_train.drop(33,axis=1),final_train[33])\n\npredictions=linmodel.predict(final_train.drop(33,axis=1))\n\nprint(sqrt(mean_squared_error(final_train[33],predictions)))\n# vs. 2,35 in the naive linear model","828c95cf":"linmodel.score(final_train.drop(33,axis=1),final_train[33])\n# improved from 1,6% to 89%?! wow.","6399fefa":"# The columns that I need in my testset\nprint(final_train.columns)","77673e0a":"print(test.head())","edae7d04":"# Now prepare the test-set\n# After many many many,... many (really many!!!) tries to fill the panda dataframe I figured out my approach:\n# It took me hours of painful tries to pandas-merge the complete df...\n\n# Fill complete set with the roughest estimation (dont use shop_id and item_id at all)\n# Increasingly overwrite data with increasing level of knowledge (use item_id only, then overwrite with shop-item_id pairs)\n# In this way there will be no NaNs after the process as in step 1 everything is filled and only overwritten when a better estimate\/data is available\n\n# Structure:\n# Step 1: Delete \"ID\"\n# Step 2: Fill geographic columns (town, region)\n# Step 3: Fill 0-33 with median values\n# Step 4: Overwrite 0-33 with values where shop, item-pairs are known\n# Step 5: Overwrite 0-33 with values where item is known\n# Step 6: Fill item_price\n# Step 7: Fill price_category\n# Step 8: Fill 'item_category_id' and 'meta_category' with median value\n# Step 9: Overwrite both, if known item\n\n# Step1:\ntest.drop(\"ID\",axis=1,inplace=True)\ntest","e46a0873":"# Step 2: Fill geographic columns (town, region)\ntest.merge(final_train[[\"shop_id\",\"town\",\"region\"]])\ntest","d5f0b718":"\n# Very easily filled are the geographic columns:\nunique_shops=final_train.shop_id.unique()\nprint(unique_shops)\n\n#test.merge(final_train[[\"shop_id\",\"town\",\"region\"]].unique())\n\n# Step 1: Fill with median values for every column\n\n\n#test2=pd.merge(test,final_train, how=\"left\", on=[\"shop_id\",\"item_id\"])\n#test2=test2.groupby(\"ID\",as_index=False).max()\n\nfor i in range(33):\n    test[i]=final_train[i+1].mean()\n\ntest","0d9e3fdd":"test2[test2[0].isnull()].head()","18e80437":"print(final_train.groupby(\"item_id\",as_index=False).median().shape)\nprint(final_train.groupby(\"item_id\",as_index=False).median().head())","42c069a9":"final_train.groupby(\"item_id\",as_index=False).median().set_index('item_id')","1d1e30ec":"test2.set_index(\"item_id\",inplace=True)\ntest2","e87e3b61":"print(test2.set_index(\"item_id\").fillna(final_train.groupby(\"item_id\",as_index=False).median(),df.set_index('item_id')).head())","0a301add":"# Fill based only on item_id median in month\n#print(test2.shape)\n#print(test2[test2[0].isnull()])\ntest2.fillna()\n#test2[test2[0].isnull()]=pd.merge(test2[test2[0].isnull()],final_train.groupby(\"item_id\",as_index=False).median(),how=\"left\",on=[\"item_id\"])\n#print(pd.merge(test2[test2[0].isnull()],final_train.groupby(\"item_id\",as_index=False).median(),how=\"left\",on=[\"item_id\"]))\n#print(test2.shape)\ntest2\n\n# Shift by one month","3dddcc46":"# Already better but of course categories, towns need to be one_hot_encoded","1cfab809":"# Much more logical will be to add the past months as features\nX_train.head()","972c703c":"A 2nd linear model: Include the past sales history per row to estimate todays sales","a7ad3784":"Observations from our very first linear model:\n* The later in time (date_block_num) is higher, the smaller sold amoung (Makes sense as EDA showed declining volumes over time\n* item_id also negative. As item_ids are increasing same reason applies for lower and lower volumes\n* month is positively correlated. makes sense as Year-End \/ Christmas business has high volumes\n* price_category is positively correlated. The higher the category, the higher the volume!? That doesnt seem right\n* Same for price. Higher price indicates higher sales!?","06a137ae":"Tree-Model\n\nLets use this template for XGBoost modelling to learn what it is about:\nhttps:\/\/www.kaggle.com\/alexeyb\/coursera-winning-kaggle-competitions\/notebook","80e66296":"LSTM-Model\nAnd here a possible template for a LSTM model\nhttps:\/\/www.kaggle.com\/shubhammank\/lstm-for-beginners","6bf8e32b":"WORK IN PROGRESS\n\nThis is the continuation of my work on EDA which can be found **[here](https:\/\/www.kaggle.com\/dennise\/coursera-competition-getting-started-eda\/edit ).**","9be4694e":"Let's get a first model, a linear model, to run to better understand what am I actually modelling\nBased on some [former work ]( https:\/\/www.kaggle.com\/dennise\/sklearn-runthrough-everybody-need-a-titanic-kernel)of mine ","708982f6":"From week 3: \nYou can get a rather good score after creating some lag-based features like in advice from previous week and \nfeeding them into gradient boosted trees model.\nApart from item\/shop pair lags you can try adding lagged values of total shop or total item sales \n(which are essentially mean-encodings). All of that is going to add some new information."}}