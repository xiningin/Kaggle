{"cell_type":{"86a16bee":"code","17ea8a3c":"code","6680ee75":"code","2b9badcf":"code","86628e2c":"code","62fffff5":"code","2da65d07":"code","62ad6b51":"code","58b499f9":"code","be2e516c":"code","b6efea03":"code","13f9b079":"code","53fe1d29":"code","41551c0f":"code","cb4fa17f":"code","d6b22666":"code","bd43ce2c":"code","64df9ee2":"code","88677882":"code","e86c6f14":"code","712021c1":"code","f237070f":"code","994c04e5":"code","39f9f332":"code","168c24dc":"code","8a1ac5d4":"code","df247268":"code","2464b01c":"code","3384a360":"code","6936e893":"code","72ffcf5b":"code","639722f7":"code","4cec3611":"code","55d34fa4":"code","2a21e1f8":"code","03c736a6":"code","d076c99a":"markdown","28782e29":"markdown","23d96a96":"markdown","0d32b8b6":"markdown","6278e77d":"markdown","9882dfea":"markdown","29cbb336":"markdown","f77bb894":"markdown","1830aadc":"markdown","360fae9f":"markdown","776e6618":"markdown","7d3aa284":"markdown","c257e398":"markdown","0b15c59a":"markdown","acb9725c":"markdown","c5235a5a":"markdown","cfbfaea9":"markdown","ff2defa4":"markdown","62ae5c00":"markdown"},"source":{"86a16bee":"import os\nprint(os.listdir(\"..\/input\"))","17ea8a3c":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = 10, 5","6680ee75":"train = pd.read_csv('..\/input\/train_data.csv', encoding='utf', engine='python', index_col=0)\ntest = pd.read_csv('..\/input\/test_data.csv', encoding='utf', engine='python', index_col=0)","2b9badcf":"train.shape, test.shape","86628e2c":"targ_distr = pd.DataFrame([train.type.value_counts().rename(\"Abs\"), \n                           (train.type.value_counts()\/train.shape[0]).rename(\"Rel\")])","62fffff5":"display(targ_distr.T)\n_ = train.type.value_counts().plot.bar()\nplt.title('Type distribution')\nplt.xticks(rotation=90)\nplt.show()\n\ndel targ_distr","2da65d07":"train['text_len'] = train.text.str.len()\ntest['text_len'] = test.text.str.len()\n\ntrain['title_len'] = train.title.str.len()\ntest['title_len'] = test.title.str.len()","62ad6b51":"prsntls = [.25, .5, .75, 0.9, 0.95, 0.99]\ntext_length = pd.DataFrame([train['text_len'].describe(percentiles=prsntls).rename(\"train\"), \n              test['text_len'].describe(percentiles=prsntls).rename(\"test\")])\ndisplay(text_length)","58b499f9":"text_length = pd.DataFrame([train['title_len'].describe(percentiles=prsntls).rename(\"train\"), \n              test['title_len'].describe(percentiles=prsntls).rename(\"test\")])\ndisplay(text_length)","be2e516c":"fig, ax = plt.subplots(nrows=1, ncols=2)\nax[0].set_title(\"Lens text\")\ntrain['text_len'].plot.hist(alpha=0.7, ax=ax[0])\ntest['text_len'].plot.hist(alpha=0.7, ax=ax[0])\nax[0].legend([\"Train\", \"Test\"]);\n\nax[1].set_title(\"Lens titles\")\ntrain['title_len'].plot.hist(alpha=0.7, ax=ax[1])\ntest['title_len'].plot.hist(alpha=0.7, ax=ax[1])\nax[1].legend([\"Train\", \"Test\"]);\nplt.tight_layout()","b6efea03":"#display(train.groupby('type')['text_len'].describe(percentiles=prsntls))\n#display(train.groupby('type')['title_len'].describe(percentiles=prsntls))\nfig, ax = plt.subplots(nrows=1, ncols=2, )\n\n_ = sns.catplot(x=\"type\", y=\"text_len\", data=train, kind=\"bar\", ax=ax[0])\nax[0].tick_params(labelrotation=90)\nplt.close(_.fig)\n_ = sns.catplot(x=\"type\", y=\"title_len\", data=train, kind=\"bar\", ax=ax[1])\nax[1].tick_params(labelrotation=90)\nplt.close(_.fig)\nplt.show()","13f9b079":"# https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, \n                   #max_words=200, \n                   max_font_size=100, \n                   title = None, image_color=False, ax = None):\n    stopwords = set(STOPWORDS)\n    # more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    # stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    #max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    ax.imshow(wordcloud);\n    ax.set_title(title)\n    ax.axis('off');\n    plt.tight_layout()","53fe1d29":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 16))\nplot_wordcloud(train[\"text\"], title=\"Word Cloud Train Text\", ax=ax[0][0])\nplot_wordcloud(test[\"text\"], title=\"Word Cloud Test Text\", ax=ax[0][1])\nplot_wordcloud(train[\"title\"], title=\"Word Cloud Train Title\", ax=ax[1][0])\nplot_wordcloud(test[\"title\"], title=\"Word Cloud Test Title\", ax=ax[1][1])\nplt.tight_layout()","41551c0f":"%%time\nfig, ax = plt.subplots(nrows=9, ncols=3, figsize=(20, 40))\nfor number, mask in enumerate(train.type.unique()):\n    plot_wordcloud(train['text'], title=f\"{mask}: Word Cloud Train Text\", ax=ax[number][0])\n    plot_wordcloud(train['text'][train.type == mask], title=f\"{mask}: Word Cloud Score Type Text\", ax=ax[number][1])\n    plot_wordcloud(train['title'][train.type == mask], title=f\"{mask}: Word Cloud Score Type Title\", ax=ax[number][2])\nplt.tight_layout()","cb4fa17f":"del fig, ax, plot_wordcloud","d6b22666":"import gc","bd43ce2c":"gc.collect()","64df9ee2":"# https:\/\/www.kaggle.com\/ogrellier\/lgbm-with-words-and-chars-n-gram\nimport re \n\ndef count_regexp_occ(regexp=\"\", text=None):\n    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n    return len(re.findall(regexp, text))","88677882":"def get_indicators_and_clean_comments(df):\n    \"\"\"\n    Check all sorts of content as it may help find toxic comment\n    Though I'm not sure all of them improve scores\n    \"\"\"\n    # Get length in words and characters\n    df[\"raw_word_len\"] = df[\"text\"].apply(lambda x: len(x.split()))\n    # Check number of upper case, if you're angry you may write in upper case\n    df[\"nb_upper\"] = df[\"text\"].apply(lambda x: count_regexp_occ(r\"[\u0410-\u042f]\", x))\n    # Check for time stamp\n    df[\"has_timestamp\"] = df[\"text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))","e86c6f14":"%%time\ntrain_prepr = train.copy()\ntest_prepr = test.copy()\n\nget_indicators_and_clean_comments(train_prepr)\nget_indicators_and_clean_comments(test_prepr)","712021c1":"tf_idf_word = TfidfVectorizer(ngram_range=(1, 2),\n                         stop_words=stopwords.words('russian'), \n                         token_pattern=('\\w{1,}'),\n                         #preprocessor=None,\n                         analyzer='word',\n                         max_df=0.9, \n                         min_df=10,\n                         max_features=30000,\n                         sublinear_tf=True,\n                        )","f237070f":"%%time\ntf_idf_model = tf_idf_word.fit(np.concatenate([train['text'], test['text']]))","994c04e5":"%%time\ntrain_tf_idf_vec = tf_idf_model.transform(train['text'])\ntest_tf_idf_vec = tf_idf_model.transform(test['text'])","39f9f332":"from scipy.sparse import hstack","168c24dc":"train_tf_idf_vec = hstack([train_tf_idf_vec, \n                           train_prepr[train_prepr.columns.difference(['title', 'text', 'type'])]])\ntest_tf_idf_vec = hstack([test_tf_idf_vec, \n                           test_prepr[test_prepr.columns.difference(['title', 'text', 'type'])]])","8a1ac5d4":"train_tf_idf_vec, test_tf_idf_vec","df247268":"del train_prepr, test_prepr","2464b01c":"lm = LogisticRegression(#solver='newton-cg', \n                        #n_jobs=-1,\n                        penalty='l2',\n                        #tol=0.000000001,\n                        random_state=42,\n                        C=10, \n                        max_iter=1500)","3384a360":"lm_params = {'penalty':['l2'],\n             'C':[0.001, 0.01, 0.1, 1, 2, 5, 10, 20, 100],\n             #'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n             #'tol' : [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.0001]\n    \n    \n}\nlm_search = GridSearchCV(estimator=lm, \n                         param_grid=lm_params, \n                         scoring ='accuracy', \n                         cv=StratifiedKFold(3), \n                         #n_jobs=-1,\n                         verbose=1)","6936e893":"%%time\nlm_search_fitted = lm_search.fit(X=train_tf_idf_vec, \n                                 y=pd.factorize(train.type)[0])","72ffcf5b":"lm_search_fitted.best_score_","639722f7":"lm_search_fitted.best_estimator_","4cec3611":"pred_scores = cross_val_score(estimator=lm_search_fitted.best_estimator_, \n                              X=train_tf_idf_vec, \n                              y=pd.factorize(train.type)[0],\n                scoring='accuracy',  \n                cv=10, #stratified by default\n                n_jobs=-1)\ndisplay(np.mean(pred_scores))","55d34fa4":"predicts = lm_search_fitted.best_estimator_.predict(test_tf_idf_vec)","2a21e1f8":"sub = pd.DataFrame({'index': range(0, len(predicts)),\n                    'type':pd.factorize(train.type)[1][predicts]})\nsub.to_csv('LRandTFIDF_sample_submission.csv', index=False)","03c736a6":"pd.read_csv('LRandTFIDF_sample_submission.csv').head()","d076c99a":"# Model fitting","28782e29":"\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u043e\u0431\u043b\u0430\u043a\u0430 \u0441\u043b\u043e\u0432 \u043f\u043e \u043a\u043b\u0430\u0441\u0441\u0443","23d96a96":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0438 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445.","0d32b8b6":"**\u0414\u043b\u0438\u043d\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 \u0438 \u0434\u043b\u0438\u043d\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0430**","6278e77d":"\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0444\u0430\u0439\u043b \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438.","9882dfea":"# EDA","29cbb336":" **\u0414\u043b\u0438\u043d\u0430 \u043d\u0430 \u043a\u043b\u0430\u0441\u0441**\n \n \u041d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u0434\u043b\u0438\u043d\u043d\u0435\u0435 \u0438 \u043f\u043e \u0442\u0435\u043a\u0441\u0442\u0443, \u0438 \u043f\u043e \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0443.","f77bb894":"\u0414\u043b\u0438\u043d\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u043e\u0432 \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0435.","1830aadc":"\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u044d\u0442\u0443 \u043c\u043e\u0434\u0435\u043b\u044c.","360fae9f":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0443\u0436\u043d\u044b\u0435 \u043f\u0430\u043a\u0435\u0442\u044b","776e6618":"\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \"\u043e\u0431\u0449\u0438\u0435\" \u043e\u0431\u043b\u0430\u043a\u0430 \u0441\u043b\u043e\u0432.","7d3aa284":"## Params optimisation","c257e398":"\u0423 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0431\u043e\u043b\u044c\u0448\u0438\u0439 \u0440\u0430\u0437\u0431\u0440\u043e\u0441,","0b15c59a":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u0430\u0440\u0433\u0435\u0442\u0430","acb9725c":"## Best model local validation","c5235a5a":"\u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c tf_idf \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430","cfbfaea9":"# Some FE","ff2defa4":"# \u0412\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435","62ae5c00":"\u041f\u043e\u0434\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435"}}