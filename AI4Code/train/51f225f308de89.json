{"cell_type":{"97cffded":"code","5e34d2d8":"code","6166d8eb":"code","05ff4e72":"code","ab58f666":"code","fee19bcb":"code","8ff98d9c":"code","b8a70808":"code","471d6f0c":"code","905060a8":"code","72ec8a90":"code","dbe1f45e":"code","8b77f3d0":"code","04f3b648":"code","fef532ea":"code","434b1a2d":"code","e21f8ef0":"code","017a5f3d":"code","e5aaebb9":"code","727e86e9":"code","1275d541":"code","7a33a258":"code","38adce4e":"code","df0dce63":"code","3f81ae2f":"code","45ea7793":"code","6e8f13ea":"code","30dcc9f4":"code","064b53e4":"code","e5a34c87":"code","d6932586":"code","e272a35f":"code","475a893a":"code","f8fb43cf":"code","4055cc38":"code","c29b2e9e":"code","662a8dfc":"code","105ca29f":"code","2d358c08":"code","cb631867":"code","1e317108":"code","0da97b67":"code","2f7bf1f5":"code","5ef4ab32":"code","a7f80247":"code","8aa158f7":"code","20a5a581":"code","940bd168":"code","22ed8f17":"code","8b6710a4":"code","9d15222a":"code","65919a0d":"code","f8c625bf":"code","1fed59b8":"code","d7a04c65":"code","273e1ee9":"code","0af071f0":"code","0231c19f":"code","9e86c0b8":"markdown","796f385a":"markdown","73ec74a7":"markdown","2d9449c2":"markdown","e9df67d8":"markdown","f5bc9f80":"markdown","c7d96e4a":"markdown","f25ec337":"markdown","9fc5ee99":"markdown","e5dc494a":"markdown","0297a564":"markdown","69aa0f53":"markdown","4f0b68d7":"markdown","08df4de3":"markdown","e23586e0":"markdown","93aa870d":"markdown","7b0a4dd4":"markdown","9beed4c8":"markdown","67f1031a":"markdown","66c51481":"markdown","9f76b031":"markdown","3d5efbb8":"markdown","fe44f281":"markdown","80edb8c9":"markdown","7bc57416":"markdown","d17c76b4":"markdown","5e9027f2":"markdown","aeab089f":"markdown","512ca1bb":"markdown","13d1d74f":"markdown","af57df93":"markdown","dda2ce41":"markdown","0ee62998":"markdown","955eff63":"markdown"},"source":{"97cffded":"# necessary libraries\nimport warnings\nwarnings.simplefilter(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.signal import find_peaks","5e34d2d8":"# reading data\ndata = pd.read_csv(\"..\/input\/piworksbus\/municipality_bus_utilization.csv\")\ndata.head(20)","6166d8eb":"# datatypes and null counts\ndata.info()","05ff4e72":"# analysis of sample counts\ncount_dict = dict(data['municipality_id'].value_counts())\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize = (8, 4))\nsns.barplot(x = list(count_dict.keys()), y = list(count_dict.values()))\nplt.xlabel('Municipality')\nplt.ylabel('Data points per municipality')\nplt.title('Number of samples by municipality')\nplt.show()\n\n# Percentage-wise distribution of municipalities\nprint(\"- \"* 50)\nfor i in count_dict.keys():\n  print(\"Number of data points in municipality {0} = {1} ~ {2}%\".format(\n  i, count_dict[i], round((count_dict[i]*100)\/sum(count_dict.values()), 2)))\n\nprint(\"-\"*50)\nprint(\"total datapoints:\", sum(count_dict.values()))","ab58f666":"# Usage analysis\nsns.set_style(\"whitegrid\")\nplt.figure(figsize = (8, 4))\nsns.barplot(x = data[\"municipality_id\"], y = data[\"total_capacity\"])\nplt.xlabel('Municipality')\nplt.ylabel('Total capacity')\nplt.title('Total capacity per municipalities')\nplt.show()\n\n# Percentage-wise distribution of total capacities\nprint(\"- \"* 50)\ncapacities = data[[\"municipality_id\", \"total_capacity\"]].drop_duplicates().sort_values(\"municipality_id\")\nfor i in capacities.iterrows():\n    print(\"Total capacity of the municipality {} = {} ~ {}%\".format(\n        i[1][\"municipality_id\"], i[1][\"total_capacity\"], round((i[1][\"total_capacity\"]*100)\/sum(capacities[\"total_capacity\"]), 2)))\n   # print(i, type(i))\nprint(\"-\"*50)\nprint(\"total capacity:\", sum(capacities[\"total_capacity\"]))","fee19bcb":"# Analysis of usages\nsns.set(style = 'whitegrid')\nsns.FacetGrid(data, hue = 'municipality_id', height=6).map(sns.distplot, 'usage').add_legend()\nplt.title('Distrubiton of Usages')\nplt.show()","8ff98d9c":"# Usage in time-series\nplt.figure(figsize = (16, 4))\nfor i in range(10):\n    plt.plot(data[data['municipality_id'] == i][['usage']].reset_index(drop=True), label=i)\nplt.legend(loc='lower right',bbox_to_anchor=(1.03, 0.25))\nplt.title('Usages in Time-series Format')\nplt.show()","b8a70808":"# Choose municipality 6\nmun_6 = data[data['municipality_id'] == 6].reset_index(drop=True)\nlen(mun_6)","471d6f0c":"# Create columns based on hour and day\nmun_6[\"timestamp_til_hour\"] = mun_6[\"timestamp\"].apply(lambda x: x[:-6])\nmun_6[\"timestamp_til_day\"] = mun_6[\"timestamp\"].apply(lambda x: x[:-9])","905060a8":"# number of days in dataset\nmun_6_daycount = mun_6.groupby(\"timestamp_til_day\").count()\nprint(\"number of days in dataset:\", len(mun_6_daycount))\n\n# hour counts of days\nmun_6_daycount_sorted = mun_6_daycount.sort_values(\"usage\")\nprint(\"Hour counts:\")\nmun_6_daycount_sorted[\"usage\"].value_counts()","72ec8a90":"# find irregular days for municipality 6\nirregular_days = mun_6_daycount_sorted[mun_6_daycount_sorted[\"usage\"]!=18].index.to_list()\nprint(\"irregular days:\")\nprint(irregular_days)\n\n# empty hours for the day with 34 measurements\nmun_6[mun_6[\"timestamp_til_day\"]==irregular_days[5]]","dbe1f45e":"# find hour counts of all municipalities\ndef hour_counts_in_days(mun_id):\n    mun_data = data[data['municipality_id'] == mun_id].reset_index(drop=True)\n    mun_data[\"timestamp_til_hour\"] = mun_data[\"timestamp\"].apply(lambda x: x[:-6])\n    mun_data[\"timestamp_til_day\"] = mun_data[\"timestamp\"].apply(lambda x: x[:-9])\n    mun_daycount = mun_data.groupby(\"timestamp_til_day\").count()\n    # hour counts of days\n    print(\"municipality id: \", mun_id)\n    print(mun_daycount[\"usage\"].value_counts())\n    # print irregular days\n    print(\"irregular days:\")\n    print(mun_daycount[mun_daycount[\"usage\"]!=18].index.to_list())\n    \nfor i in range(10):\n    hour_counts_in_days(i)\n    print(\"----------------\")","8b77f3d0":"# Find irregular hourly measurements\nmun_6_hourcount = mun_6.groupby(\"timestamp_til_hour\").count()\nmun_6_hourcount[mun_6_hourcount[\"usage\"]!=2]","04f3b648":"# 13th hour of 2017-06-30 has 6 measurements\nmun_6[mun_6[\"timestamp_til_hour\"]==\"2017-06-30 13\"]","fef532ea":"# find hourly measurement counts of all municipalities\ndef measurement_counts_in_hours(mun_id):\n    mun_data = data[data['municipality_id'] == mun_id].reset_index(drop=True)\n    mun_data[\"timestamp_til_hour\"] = mun_data[\"timestamp\"].apply(lambda x: x[:-6])\n    mun_data[\"timestamp_til_day\"] = mun_data[\"timestamp\"].apply(lambda x: x[:-9])\n    mun_hourcount = mun_data.groupby(\"timestamp_til_hour\").count()\n    # hour counts of days\n    print(\"municipality id: \", mun_id)\n    print(\"Total number of hours: \", len(mun_hourcount))\n    print(mun_hourcount[\"usage\"].value_counts())\n    # print irregular hours\n    irregular_hours = mun_hourcount[mun_hourcount[\"usage\"]!=2].index.to_list()\n    print(\"irregular hours: \", len(irregular_hours))\n    #print(irregular_hours)\n    \n\nfor i in range(10):\n    measurement_counts_in_hours(i)\n    print(\"----------------\")","434b1a2d":"# group timestamps to see if they have 10 municipalities or not\ndata_grouped = data.groupby(\"timestamp\").count()\ndata_grouped[data_grouped[\"usage\"]!=10]","e21f8ef0":"# lets see these timestamps\ndata[data[\"timestamp\"].isin([\"2017-06-16 14:27:13\",\"2017-08-11 08:19:39\"])]","017a5f3d":"# create a column by taking maximum of usages in a hour\nhourly_min = mun_6.groupby(\"timestamp_til_hour\").max(\"usage\")[\"usage\"]\nmun_6 = mun_6.merge(hourly_min, how=\"left\", left_on=\"timestamp_til_hour\", \n            right_on=\"timestamp_til_hour\", suffixes=[None, \"_max\"])\n\nmun_6.head(5)","e5aaebb9":"# Converting the column to DateTime format\nmun_6[\"timestamp\"] = pd.to_datetime(mun_6[\"timestamp_til_hour\"], format='%Y-%m-%d %H')\n\n# drop unnecesary columns\nmun_6.drop(columns=[\"usage\", \"timestamp_til_day\", \"timestamp_til_hour\",\n                    \"municipality_id\", \"total_capacity\"], inplace=True)\n\n# drop duplicates\nmun_6.drop_duplicates(inplace=True)\n\n# set index\nmun_6 = mun_6.set_index('timestamp')\nmun_6","727e86e9":"# Create new date-related features\ndates = mun_6.index.values\nmun_6['date_month'] = pd.Series(dates).apply(lambda x: x.month).values\nmun_6['date_day'] = pd.Series(dates).apply(lambda x: x.day).values\nmun_6['date_week_of_year'] = pd.Series(dates).apply(lambda x: x.weekofyear).values\nmun_6[\"date_weekday\"]= pd.Series(dates).apply(lambda x: x.isoweekday()).values","1275d541":"# Examine weekly trends\nweekly_counts = mun_6.groupby(\"date_weekday\").count()[\"usage_max\"]\nweekly_counts","7a33a258":"# average measurement in a week\navg_week = weekly_counts.mean()\nprint(\"average measurement count in a week: \", avg_week)","38adce4e":"# hour to hour change\nmun_6[\"Diff_h2h\"] = mun_6[\"usage_max\"].diff()*100\n\n# day to day change\nmun_6[\"Diff_d2d\"] = mun_6[\"usage_max\"].diff(periods=18)*100\n\n# week to week change\nmun_6[\"Diff_w2w\"] = mun_6[\"usage_max\"].diff(periods=96)*100","df0dce63":"# hour to hour percentage change\nmun_6[\"Perc_change_h2h\"] = mun_6[\"usage_max\"].pct_change()*100\n\n# day to day percentage change\nmun_6[\"Perc_change_d2d\"] = mun_6[\"usage_max\"].pct_change(periods=18)*100\n\n# week to week percentage change\nmun_6[\"Perc_change_w2w\"] = mun_6[\"usage_max\"].pct_change(periods=96)*100","3f81ae2f":"# Create 18 hours (1 day) exponential moving average column\nmun_6['18_ewm'] = mun_6['usage_max'].ewm(span = 18, adjust = False).mean()\n\n# Create 96 hours (1 week) exponential moving average column\nmun_6['96_ewm'] = mun_6['usage_max'].ewm(span = 96, adjust = False).mean()\n\n# create a new column 'Signal' such that if 18 EWM is greater   \n# than 96 EWM then set Signal as 1 else 0\nmun_6['Signal'] = 0.0  \nmun_6['Signal'] = np.where(mun_6['18_ewm'] > mun_6['96_ewm'], 1.0, 0.0)\n\n# create a new column 'Position' which is a day-to-day difference of \n# the 'Signal' column\nmun_6['Buy_sell_pts'] = mun_6['Signal'].diff()\nmun_6.head(5)","45ea7793":"# calculate another feature that can be useful for ML models instead of Signal\nmun_6[\"Ewm_diff\"] = mun_6['18_ewm'] - mun_6['96_ewm']\nmun_6.drop(columns=[\"Signal\"], inplace=True)","6e8f13ea":"plt.figure(figsize = (20,12))\n\n# plot close price, short-term and long-term moving averages \nmun_6['usage_max'].plot(color = 'k', lw = 1, label = 'Max Usage')  \nmun_6['18_ewm'].plot(color = 'b', lw = 1, label = '18 EWM') \nmun_6['96_ewm'].plot(color = 'g', lw = 1, label = '96 EWM')\n\n# plot \u2018buy\u2019 and 'sell' signals\nplt.plot(mun_6[mun_6[\"Buy_sell_pts\"] == 1].index, \n         mun_6[\"18_ewm\"][mun_6[\"Buy_sell_pts\"] == 1], \n         \"^\", markersize = 15, color = \"g\", label = 'buy')\nplt.plot(mun_6[mun_6[\"Buy_sell_pts\"] == -1].index, \n         mun_6[\"18_ewm\"][mun_6[\"Buy_sell_pts\"] == -1], \n         \"v\", markersize = 15, color = \"r\", label = 'sell')\nplt.ylabel('Hourly Usages', fontsize = 15 )\nplt.xlabel('Date', fontsize = 15 )\nplt.title('EWM Crossover', fontsize = 20)\nplt.legend()\nplt.show()","30dcc9f4":"# calculate simple moving average (SMA), std, upper (+2*std) and lower bands (-2*std)\nmun_6[\"18_sma\"] = mun_6[\"usage_max\"].rolling(18).mean()\nmun_6[\"18_std\"] = mun_6[\"usage_max\"].rolling(18).std()\nmun_6[\"18_sma_upband\"] = mun_6[\"18_sma\"] + 2*mun_6[\"18_std\"]\nmun_6[\"18_sma_dwnband\"] = mun_6[\"18_sma\"] - 2*mun_6[\"18_std\"]\nmun_6.tail()","064b53e4":"# calculate start and exit points\nmun_6['Enter_exit_pts'] = 0\nmun_6.loc[mun_6[\"usage_max\"] <= mun_6['18_sma_dwnband'], 'Enter_exit_pts'] = 1\nmun_6.loc[mun_6[\"usage_max\"] >= mun_6['18_sma_upband'], 'Enter_exit_pts']= -1","e5a34c87":"# plot bollinger bands\nmun_6[\"usage_max\"].plot(c = \"k\", figsize = (20,10), lw = 2)\nmun_6[\"18_sma\"].plot(c = \"b\", figsize = (20, 10), lw = 1)\nmun_6[\"18_sma_upband\"].plot(c = \"g\", figsize = (20, 10), lw = 1) \nmun_6[\"18_sma_dwnband\"].plot(c = \"r\", figsize = (20, 10), lw = 1)\n\n# plot \u2018enter\u2019 and 'exit' signals\nplt.plot(mun_6[mun_6[\"Enter_exit_pts\"] == 1].index, \n         mun_6[\"usage_max\"][mun_6[\"Enter_exit_pts\"] == 1], \n         \"^\", markersize = 15, color = \"g\", label = 'enter')\nplt.plot(mun_6[mun_6[\"Enter_exit_pts\"] == -1].index, \n         mun_6[\"usage_max\"][mun_6[\"Enter_exit_pts\"] == -1], \n         \"v\", markersize = 15, color = \"r\", label = 'sell')\n\nplt.title(\"Bollinger Bands\", fontsize = 20)\nplt.ylabel(\"Usage\",fontsize = 15 )\nplt.xlabel(\"Date\", fontsize = 15 )\nplt.legend()\nplt.show()","d6932586":"# construct time windows\nx_list = []\nfor i in range(0, mun_6.shape[0] - 18, 1):\n    xs = mun_6['usage_max'].values[i: i + 18]\n    x_list.append(xs)","e272a35f":"# drop first 18 elements\nprint(\"before: \", len(mun_6))\nmun_6 = mun_6[18:]\nprint(\"after: \", len(mun_6))","475a893a":"# mean (\"18_sma\") and std (\"18_std\") are calculated before \n\n# avg absolute diff\nmun_6['18_aad'] = pd.Series(x_list).apply(lambda x: np.mean(np.absolute(x - np.mean(x)))).values\n\n# min\nmun_6['18_min'] = pd.Series(x_list).apply(lambda x: x.min()).values\n\n# max\nmun_6['18_max'] = pd.Series(x_list).apply(lambda x: x.max()).values\n\n# max-min diff\nmun_6['18_maxmin_diff'] = mun_6['18_max'] - mun_6['18_min']\n\n# median\nmun_6['18_median'] = pd.Series(x_list).apply(lambda x: np.median(x)).values\n\n# median abs dev \nmun_6['18_mad'] = pd.Series(x_list).apply(lambda x: np.median(np.absolute(x - np.median(x)))).values\n\n# interquartile range\nmun_6['18_IQR'] = pd.Series(x_list).apply(lambda x: np.percentile(x, 75) - np.percentile(x, 25)).values\n\n# values above mean\nmun_6['18_above_mean'] = pd.Series(x_list).apply(lambda x: np.sum(x > x.mean())).values\n\n# number of peaks\nmun_6['18_peak_count'] = pd.Series(x_list).apply(lambda x: len(find_peaks(x)[0])).values\n\n# skewness\nmun_6['18_skewness'] = pd.Series(x_list).apply(lambda x: stats.skew(x)).values\n\n# kurtosis\nmun_6['18_kurtosis'] = pd.Series(x_list).apply(lambda x: stats.kurtosis(x)).values\n\n# energy\nmun_6['18_energy'] = pd.Series(x_list).apply(lambda x: np.sum(x**2)\/18).values","f8fb43cf":"# an example of transformed time window to frequency domain\nfreq_domain = pd.Series(np.fft.fft(pd.Series(x_list[142])))\nfreq_domain.plot()\nplt.show()","4055cc38":"# trim initial values and symmetric part\nfreq_domain[1:10].plot()\nplt.show()","c29b2e9e":"# converting the signals from time domain to frequency domain using FFT\nx_list_fft = pd.Series(x_list).apply(lambda x: np.abs(np.fft.fft(x))[1:10])\n\n# Statistical Features on raw x, y and z in frequency domain\n# FFT mean\nmun_6['18_sma_fft'] = pd.Series(x_list_fft).apply(lambda x: x.mean()).values\n\n# FFT std dev\nmun_6['18_std_fft'] = pd.Series(x_list_fft).apply(lambda x: x.std()).values\n\n# FFT avg absolute diff\nmun_6['18_aad_fft'] = pd.Series(x_list_fft).apply(lambda x: np.mean(np.absolute(x - np.mean(x)))).values\n\n# FFT min\nmun_6['18_min_fft'] = pd.Series(x_list_fft).apply(lambda x: x.min()).values\n\n# FFT max\nmun_6['18_max_fft'] = pd.Series(x_list_fft).apply(lambda x: x.max()).values\n\n# FFT max-min diff\nmun_6['18_maxmin_diff_fft'] = mun_6['18_max_fft'] - mun_6['18_min_fft']\n\n# FFT median\nmun_6['18_median_fft'] = pd.Series(x_list_fft).apply(lambda x: np.median(x)).values\n\n# FFT median abs dev \nmun_6['18_mad_fft'] = pd.Series(x_list_fft).apply(lambda x: np.median(np.absolute(x - np.median(x)))).values\n\n# FFT Interquartile range\nmun_6['18_IQR_fft'] = pd.Series(x_list_fft).apply(lambda x: np.percentile(x, 75) - np.percentile(x, 25)).values\n\n# FFT values above mean\nmun_6['18_above_mean_fft'] = pd.Series(x_list_fft).apply(lambda x: np.sum(x > x.mean())).values\n\n# FFT number of peaks\nmun_6['18_peak_count_fft'] = pd.Series(x_list_fft).apply(lambda x: len(find_peaks(x)[0])).values\n\n# FFT skewness\nmun_6['18_skewness_fft'] = pd.Series(x_list_fft).apply(lambda x: stats.skew(x)).values\n\n# FFT kurtosis\nmun_6['18_kurtosis_fft'] = pd.Series(x_list_fft).apply(lambda x: stats.kurtosis(x)).values\n\n# FFT energy\nmun_6['18_energy_fft'] = pd.Series(x_list_fft).apply(lambda x: np.sum(x**2)\/9).values","662a8dfc":"# index of max value in time domain\nmun_6['18_argmax'] = pd.Series(x_list).apply(lambda x: np.argmax(x)).values\n\n# index of min value in time domain\nmun_6['18_argmin'] = pd.Series(x_list).apply(lambda x: np.argmin(x)).values\n\n# absolute difference between above indices\nmun_6['18_arg_diff'] = abs(mun_6['18_argmax'] - mun_6['18_argmin'])\n\n# index of max value in frequency domain\nmun_6['18_argmax_fft'] = pd.Series(x_list_fft).apply(lambda x: np.argmax(np.abs(np.fft.fft(x))[1:10])).values\n\n# index of min value in frequency domain\nmun_6['18_argmin_fft'] = pd.Series(x_list_fft).apply(lambda x: np.argmin(np.abs(np.fft.fft(x))[1:10])).values\n\n# absolute difference between above indices\nmun_6['18_arg_diff_fft'] = abs(mun_6['18_argmax_fft'] - mun_6['18_argmin_fft'])","105ca29f":"mun_6.info()","2d358c08":"# fill na's with mean in two columns\nmun_6[\"Diff_w2w\"].fillna(mun_6[\"Diff_w2w\"].mean(), inplace=True)\nmun_6[\"Perc_change_w2w\"].fillna(mun_6[\"Perc_change_w2w\"].mean(), inplace=True)","cb631867":"mun_6.describe()","1e317108":"# train-test split\ntrain = mun_6[:\"2017-08-04\"]\ntest = mun_6[\"2017-08-05\":]\nprint (\"train size: \", len(train))\nprint (\"test size: \", len(test))","0da97b67":"# X-y splits\ny_train = train[\"usage_max\"].values.reshape(-1,1)\nX_train = train.drop(columns=[\"usage_max\"])\n\ny_test = test[\"usage_max\"].values.reshape(-1,1)\nX_test = test.drop(columns=[\"usage_max\"])","2f7bf1f5":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n# standardization\nfeature_scaler = StandardScaler()\nfeature_scaler.fit(X_train)\nX_train_scaled = feature_scaler.transform(X_train)\nX_test_scaled = feature_scaler.transform(X_test)\n\nlabel_scaler = StandardScaler()\nlabel_scaler.fit(y_train)\ny_train_scaled = label_scaler.transform(y_train)\ny_test_scaled = label_scaler.transform(y_test)\n\n# ridge regression model\nbaseline_model = Ridge(random_state = 21)\nbaseline_model.fit(X_train_scaled, y_train_scaled)\ny_pred_scaled = baseline_model.predict(X_test_scaled)\n\n# revert scaling of predictions\ny_pred = np.round(label_scaler.inverse_transform(y_pred_scaled))\nprint(\"R2 score: \", r2_score(y_test, y_pred))\nprint(\"MSE score: \", mean_squared_error(y_test, y_pred))\n\n# save results\nresults=list()\nresults.append({\"model\":\"ridge_default\", \n                \"R2\":r2_score(y_test, y_pred), \n                \"MSE\":mean_squared_error(y_test, y_pred)})\ntest[\"ridge_default_preds\"] = y_pred\n\n# plot predictions along with truth values\nplt.figure(figsize=(14,7))\nplt.plot(train[\"usage_max\"], label=\"Train\")\nplt.plot(test[\"usage_max\"], label=\"Truths\")\nplt.plot(test[\"ridge_default_preds\"], label=\"Predictions\")\nplt.legend(loc=\"best\")\nplt.fill_betweenx(plt.gca().get_ylim(), pd.to_datetime(\"2017-08-05\"), mun_6.index[-1],\n                 alpha=.1, zorder=-1);","5ef4ab32":"# hyperparameter search\nfrom sklearn.model_selection import GridSearchCV\n\n# parameters \nparameters = {\n                \"alpha\": np.logspace(-5, 2, 200),          \n}\n\n# find best model\ngrid_ridge = GridSearchCV(estimator=baseline_model, param_grid=parameters, cv=10, n_jobs=-1)\ngrid_ridge.fit(X_train_scaled, y_train_scaled)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid_ridge.best_params_, grid_ridge.best_score_))\n\n# prediction results\ny_pred_scaled = grid_ridge.predict(X_test_scaled)\n\n# revert scaling of predictions\ny_pred = np.round(label_scaler.inverse_transform(y_pred_scaled))\nprint(\"R2 score: \", r2_score(y_test, y_pred))\nprint(\"MSE score: \", mean_squared_error(y_test, y_pred))\n\n# save results\nresults.append({\"model\":\"ridge_best\", \n                \"R2\":r2_score(y_test, y_pred), \n                \"MSE\":mean_squared_error(y_test, y_pred)})\ntest[\"ridge_best_preds\"] = y_pred\n\n# plot predictions along with truth values\nplt.figure(figsize=(14,7))\nplt.plot(train[\"usage_max\"], label=\"Train\")\nplt.plot(test[\"usage_max\"], label=\"Truths\")\nplt.plot(test[\"ridge_best_preds\"], label=\"Predictions\")\nplt.legend(loc=\"best\")\nplt.fill_betweenx(plt.gca().get_ylim(), pd.to_datetime(\"2017-08-05\"), mun_6.index[-1],\n                 alpha=.1, zorder=-1);","a7f80247":"# Plot feature importances\nimportances = abs(grid_ridge.best_estimator_.coef_.ravel())\nfeatures = X_train.columns.tolist()\nimportance_df = pd.DataFrame(list(zip(features,importances)), \n                             columns=[\"feature\",\"importance\"]).sort_values(by=\"importance\", \n                                                                           ascending=False)\nplt.figure(figsize=(8, 10))\nsns.barplot(\n    x=\"importance\",\n    y=\"feature\",\n    data = importance_df\n)\nplt.title('Ridge Feature Importances')\nplt.tight_layout()","8aa158f7":"#Importing XGBM Classifier \nimport xgboost as xgb\n\n#creating an extreme Gradient boosting instance\nxgb_model = xgb.XGBRegressor(\n                   #early_stopping_rounds=100,\n                   random_state=0, \n                   #tree_method='gpu_hist', \n                   verbosity=0\n                )\n\nxgb_model.fit(X_train_scaled, y_train_scaled)\ny_pred_scaled = xgb_model.predict(X_test_scaled)\n\n# revert scaling of predictions\ny_pred = np.round(label_scaler.inverse_transform(y_pred_scaled))\nprint(\"R2 score: \", r2_score(y_test, y_pred))\nprint(\"MSE score: \", mean_squared_error(y_test, y_pred))\n\n# save results\nresults.append({\"model\":\"xgb_default\", \n                \"R2\":r2_score(y_test, y_pred), \n                \"MSE\":mean_squared_error(y_test, y_pred)})\ntest[\"xgb_default_preds\"] = y_pred","20a5a581":"# hyperparameter search\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# parameters \nparameters = {'max_depth': list(range(4, 10)),\n              'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n              'n_estimators': list(range(50, 1000)),\n              'gamma': list(np.linspace(0, 1)),\n              'min_child_weight': list(range(1, 50)),\n              'max_delta_step': list(range(0, 10)),\n              \"max_leaves\": list(range(10, 90)),\n              'subsample': list(np.linspace(0.4, 1, 10)),\n              'reg_alpha': list(np.linspace(0, 1)),\n              'reg_lambda': list(np.linspace(0, 1)),\n              'colsample_bytree' :list(np.linspace(0.4, 1, 10)),\n              },\n\n# grid search for parameters\ngrid_xgb = RandomizedSearchCV(estimator=xgb_model, param_distributions=parameters, \n                              cv=10, n_iter=100)\ngrid_xgb.fit(X_train_scaled, y_train_scaled)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid_xgb.best_params_, grid_xgb.best_score_))\n\n# prediction results\ny_pred_scaled = grid_xgb.predict(X_test_scaled)\n\n# revert scaling of predictions\ny_pred = np.round(label_scaler.inverse_transform(y_pred_scaled))\nprint(\"R2 score: \", r2_score(y_test, y_pred))\nprint(\"MSE score: \", mean_squared_error(y_test, y_pred))\n\n# save results\nresults.append({\"model\":\"xgb_best\", \n                \"R2\":r2_score(y_test, y_pred), \n                \"MSE\":mean_squared_error(y_test, y_pred)})\ntest[\"xgb_best_preds\"] = y_pred","940bd168":"# extend n_estimators to 10000\nbest_params_dict=grid_xgb.best_estimator_.get_params()\nbest_params_dict[\"n_estimators\"]=10000\nextented_xgb = xgb.XGBRegressor().set_params(**best_params_dict)\nextented_xgb.fit(X_train_scaled, y_train_scaled)\n\n# prediction results\ny_pred_scaled = grid_xgb.predict(X_test_scaled)\n\n# revert scaling of predictions\ny_pred = np.round(label_scaler.inverse_transform(y_pred_scaled))\nprint(\"R2 score: \", r2_score(y_test, y_pred))\nprint(\"MSE score: \", mean_squared_error(y_test, y_pred))\n\n# save results\nresults.append({\"model\":\"xgb_extended\", \n                \"R2\":r2_score(y_test, y_pred), \n                \"MSE\":mean_squared_error(y_test, y_pred)})\ntest[\"xgb_extended_preds\"] = y_pred\n\n# plot predictions along with truth values\nplt.figure(figsize=(14,7))\nplt.plot(train[\"usage_max\"], label=\"Train\")\nplt.plot(test[\"usage_max\"], label=\"Truths\")\nplt.plot(test[\"xgb_extended_preds\"], label=\"Predictions\")\nplt.legend(loc=\"best\")\nplt.fill_betweenx(plt.gca().get_ylim(), pd.to_datetime(\"2017-08-05\"), mun_6.index[-1],\n                 alpha=.1, zorder=-1);","22ed8f17":"# Plot feature importances\nimportances = grid_xgb.best_estimator_.feature_importances_\nfeatures = X_train.columns.tolist()\nimportance_df = pd.DataFrame(list(zip(features,importances)), \n                             columns=[\"feature\",\"importance\"]).sort_values(by=\"importance\", \n                                                                           ascending=False)\nplt.figure(figsize=(8, 10))\nsns.barplot(\n    x=\"importance\",\n    y=\"feature\",\n    data = importance_df\n)\nplt.title('XGBoost Feature Importances')\nplt.tight_layout()","8b6710a4":"import statsmodels.api as sm\nwarnings.filterwarnings(\"ignore\", category = sm.tools.sm_exceptions.ValueWarning)\nimport itertools\n\n# define search spaces as list of tuples\np = d = q = range(0, 2)\nP = D = Q = range(0, 2)\ns = [18, 96]\npdq = list(itertools.product(p, d, q))\nPDQs = list(itertools.product(P, D, Q, s))\naic_min=np.inf\nparam_min=0\nparam_seasonal_min=0\n\n\n#\u201cgrid search\u201d to find the optimal set of parameters that yields \n# the best performance for our model.\nfor param in pdq:\n    for param_seasonal in PDQs:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(mun_6[\"usage_max\"],\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n            \n            sarimax_results = mod.fit(disp=False)\n            aic_current=sarimax_results.aic\n            print('SARIMA{}x{}4 - AIC:{}'.format(param, param_seasonal, aic_current))\n            if(aic_current < aic_min):\n                aic_min=aic_current\n                param_min=param\n                param_seasonal_min=param_seasonal\n        except:\n            continue\n            \nprint(aic_min, \"-->>\", param_min,\"---\",param_seasonal_min)","9d15222a":"# fit model with best parameters\nmodel = sm.tsa.statespace.SARIMAX(mun_6[\"usage_max\"],\n                                order=param_min,\n                                seasonal_order=param_seasonal_min,\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\n\n\nsarimax_results = model.fit(disp=False)\nsarimax_results.summary().tables[1]","65919a0d":"# get prediction with confidence intervals\ny_pred = sarimax_results.get_prediction(start='2017-08-05', dynamic=False)\ny_pred_ci = y_pred.conf_int()\ny_pred = np.round(y_pred.predicted_mean.values.reshape(-1,1))\n\n# calculate scores\nprint(\"R2 score: \", r2_score(y_test, y_pred))\nprint(\"MSE score: \", mean_squared_error(y_test, y_pred))\n\n# save results\nresults.append({\"model\":\"sar\u0131max\", \n                \"R2\":r2_score(y_test, y_pred), \n                \"MSE\":mean_squared_error(y_test, y_pred)})\ntest[\"sarimax_preds\"] = y_pred\n\n# plot predictions along with truth values\nplt.figure(figsize=(14,7))\nplt.plot(train[\"usage_max\"], label=\"Train\")\nplt.plot(test[\"usage_max\"], label=\"Truths\")\nplt.plot(test[\"sarimax_preds\"], label=\"Predictions\")\nplt.fill_between(y_pred_ci.index,\n                y_pred_ci.iloc[:, 0],\n                y_pred_ci.iloc[:, 1], color='k', alpha=.25)\nplt.legend(loc=\"best\")\nplt.fill_betweenx(plt.gca().get_ylim(), pd.to_datetime(\"2017-08-05\"), mun_6.index[-1],\n                 alpha=.1, zorder=-1);","f8c625bf":"from fbprophet import Prophet\n\n# build dataframe with necesarry renaming\ndf_prophet = pd.DataFrame(zip(mun_6.index.values, mun_6[\"usage_max\"].values),\n                         columns=[\"ds\", \"y\"])\n\n# run model\nmodel_prophet = Prophet(interval_width=0.95)\nmodel_prophet.fit(df_prophet)\n\nfuture_df = model_prophet.make_future_dataframe(periods=0)\nresults_prophet = model_prophet.predict(future_df)\ny_pred = results_prophet[-142:].set_index(\"ds\")\ny_pred_ci = y_pred[[\"yhat_lower\", \"yhat_upper\"]]\ny_pred = np.round(y_pred[\"yhat\"].values)\n\n# calculate scores\nprint(\"R2 score: \", r2_score(y_test, y_pred))\nprint(\"MSE score: \", mean_squared_error(y_test, y_pred))\n\n# save results\nresults.append({\"model\":\"prophet\", \n                \"R2\":r2_score(y_test, y_pred), \n                \"MSE\":mean_squared_error(y_test, y_pred)})\ntest[\"prophet_preds\"] = y_pred\n\n# plot predictions along with truth values\nplt.figure(figsize=(14,7))\nplt.plot(train[\"usage_max\"], label=\"Train\")\nplt.plot(test[\"usage_max\"], label=\"Truths\")\nplt.plot(test[\"prophet_preds\"], label=\"Predictions\")\nplt.fill_between(y_pred_ci.index,\n                y_pred_ci.iloc[:, 0],\n                y_pred_ci.iloc[:, 1], color='k', alpha=.25)\nplt.legend(loc=\"best\")\nplt.fill_betweenx(plt.gca().get_ylim(), pd.to_datetime(\"2017-08-05\"), mun_6.index[-1],\n                 alpha=.1, zorder=-1);","1fed59b8":"model_prophet.plot_components(results_prophet);","d7a04c65":"from tensorflow.keras import models, layers, preprocessing as kprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Define LSTM model\nmodel_lstm = models.Sequential()\nmodel_lstm.add(layers.LSTM(input_shape=(18, 1), units=50, activation='relu', \n                           return_sequences=False))\nmodel_lstm.add(layers.Dense(1))\nmodel_lstm.compile(optimizer='adam', loss='mean_squared_error')\nmodel_lstm.summary()","273e1ee9":"# normalize values\nx_list_scaled = []\nfor sequence in x_list:\n    x_list_scaled.append(label_scaler.transform(sequence.reshape(-1,1)))\n\n# train-test split\nX_train_sequences = np.array(x_list_scaled[:-142])\n# y_train_scaled is already defined above\nX_test_seqeunces = np.array(x_list_scaled[-142:])\n# y_test_scaled is already defined above\n","0af071f0":"model_lstm.fit(X_train_sequences, y_train_scaled, epochs=500, batch_size=32, verbose=0)\npred_lstm = model_lstm.predict(X_test_seqeunces)\n\n# revert scaling of predictions\ny_pred = np.round(label_scaler.inverse_transform(pred_lstm))\nprint(\"R2 score: \", r2_score(y_test, y_pred))\nprint(\"MSE score: \", mean_squared_error(y_test, y_pred))\n\n# save results\nresults.append({\"model\":\"lstm\", \n                \"R2\":r2_score(y_test, y_pred), \n                \"MSE\":mean_squared_error(y_test, y_pred)})\ntest[\"lstm_preds\"] = y_pred\n\n# plot predictions along with truth values\nplt.figure(figsize=(14,7))\nplt.plot(train[\"usage_max\"], label=\"Train\")\nplt.plot(test[\"usage_max\"], label=\"Truths\")\nplt.plot(test[\"lstm_preds\"], label=\"Predictions\")\nplt.legend(loc=\"best\")\nplt.fill_betweenx(plt.gca().get_ylim(), pd.to_datetime(\"2017-08-05\"), mun_6.index[-1],\n                 alpha=.1, zorder=-1);","0231c19f":"# print results\nresults_df = pd.DataFrame(results)\nresults_df[\"R2\"] = np.round(results_df[\"R2\"], 4)\nresults_df[\"MSE\"] = np.round(results_df[\"MSE\"], 2)\nresults_df","9e86c0b8":"Pink one (municipality_id=6) is chosen for next steps, due to it's normal distrubiton and irregular seasonality.","796f385a":"Some observations:\n\n- The first value is unusually high. This is called as \u2018DC component\u2019 or \u2018DC offset\u2019 in electrical terminology.\n- The wave signal is symmetric about the centre.","73ec74a7":"# 5. SEQUENTIAL MODELS\n\nSeasonal ARIMA, Prophet and LSTM are implemented here, only on single attribute \"usage_max\" without using the ones extracted in feature engineering section (univariate prediction). This section is based on a previous work by me and my friend :  \nhttps:\/\/www.kaggle.com\/aysenur95\/time-series-analysing-forecasting","2d9449c2":"We can conclude that all measurements had done at exactly the same timestamps except for only two measurements belonging municipality 5 and 3. These exceptions and missing timestamps with 9 counts belong to same hours so we can ignore them when we resample data by hourly basis.\n\nIn conclusion, **it's not necessary to impute missing hours or days since all municipalities have almost the same missing patterns.** We will continue with municipality 6.","e9df67d8":"# 3. FEATURE ENGINEERING","f5bc9f80":"### 5.1. Seasonal model: SARIMAX\nThe (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. **D** must be an integer indicating the integration order of the process, while **P** and **Q** may either be an integers indicating the AR and MA orders (so that all lags up to those orders are included). **s** is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data. In our case we will try **18 as daily** and **96 as weekly** periodicty.","c7d96e4a":"# 2. PREPROCESSING","f25ec337":"### 3.5. Features based on time window: Bollinger Bands\n\nCalculate enter and exit points depending on overbought and oversold conditions (outliers).\nhttps:\/\/medium.datadriveninvestor.com\/bollinger-bands-for-identifying-overbought-and-oversold-conditions-in-market-python-ed26380b91d3","9fc5ee99":"**Static** method uses previous truth values in each steps of prediction while **dynamic** method uses previous predicted values. Normally, dynamic method is the valid way to modeling but we used static method here to allow comparison with classical machine learning models. Because extracted features above are based on truth values.","e5dc494a":"### 2.2. Handling missing measurements in hours","0297a564":"### 3.3. Features based on adjacency: Difference and percentage change \n\nCalculate difference and percentage change on hour-to-hour, day-to-day and week-to-week basis.  \nhttps:\/\/towardsdatascience.com\/data-analysis-visualization-in-finance-technical-analysis-of-stocks-using-python-269d535598e4","69aa0f53":"### 3.6. Features based on time window: Statistical measures\n\nCalculate statistical metrics of short-term (1-day) past window.  \nhttps:\/\/towardsdatascience.com\/feature-engineering-on-time-series-data-transforming-signal-data-of-a-smartphone-accelerometer-for-72cbe34b8a60","4f0b68d7":"### 2.1. Handling missing hours","08df4de3":"### 4.2. Alternative: XGBoost regressor","e23586e0":"### 3.4. Features based on time window: Crossover strategy\n\nCalculate buy and sell points depending on slow and fast reactions (short-term and long-term moving averages) of the market.\nhttps:\/\/towardsdatascience.com\/making-a-trade-call-using-simple-moving-average-sma-crossover-strategy-python-implementation-29963326da7a","93aa870d":"Prophet plots the observed values of our time series (the black dots), the forecasted values (blue line) and the uncertainty intervals of our forecasts (the blue shaded regions).","7b0a4dd4":"### 3.8. Features based on time window: Indices of significant points\n\nFind significant points of short-term (1-day) past window and save them as new features. ","9beed4c8":"One other particularly strong feature of Prophet is its ability to return the components of our forecasts. This can help reveal how daily, weekly and yearly patterns of the time series contribute to the overall forecasted values.","67f1031a":"We create X,y for train, since the data has daily seasnoality length=18. We will use **x_list** we counstructed before since it contains sequences for the first 18 values.","66c51481":"### 5.2. Seasonal model: PROPHET\nGenerally Prophet performs better than ARIMA since Prophet considers non-linear relationship between elements of time series (trend, nested seasonality, residual). It also considers holiday effects and is robust to missing data and outliers. ","9f76b031":"### 3.2. Features based on date\nUtilization may depend on weekday or weekend, month or week so they are given as features.","3d5efbb8":"### 5.3. Deep learning model: LSTM","fe44f281":"# 1. EXPLORATORY DATA ANALYSIS","80edb8c9":"- Best score is the baseline model Ridge Regressor since we did heavy feature engineering and it works very well. It is possible to improve this model with handling overfitting by dimensionality reduction. \n- Ridge classifier is even better than XGBoost and ARIMA. Because it behaves like already autoregressive and moving average model by calculating sum of statistics of lagged values.\n- Generally Prophet is better than SARIMAX because it handles seasonality and outliers automatically but in our case the opposite is true owing to the tuning of SARIMAX by gridsearch. \n- It is possible to improve SARIMAX by using exagenous (features) variables. Also, extending search spaces for parameters and examining ACF and PACF plots may work.\n- LSTM is worst model because it's not tuned and probably sequence length is not enough.\n- Implementing dynamic predictions is the scientifically right way but we choose static one to compare models to baseline model. (Though, it's possible to implement baseline model in a dynamic way.)","7bc57416":"# 6. RESULTS","d17c76b4":"### 3.7. Features based on time window: Statistical measures on fourier transform\n\nCalculate statistical metrics of fourier transformed short-term (1-day) past window. ","5e9027f2":"### 3.9. Creating training testing datasets","aeab089f":"It seems like all municipalities have the same pattern on  hours with missing measurements. \n\nLet's examine if all measurements are done at exactly the same timestamps for all municipalities. ","512ca1bb":"This is not out-of-sample prediction, we first fit on train and then make predictions on test dataset. For each prediction, use truth values, not forecasted values in other words static predictions.","13d1d74f":"# 4. REGRESSION MODELS","af57df93":"## IDEAS\n#### Choosing municipality:\n- Choose longest one\n- Choose less missing one\n\n#### Handling Missing Values:\n- Dropping\n- If there's a single value missing in a particular hour: Copy other\n- If both of values are missing in a particular hour: Spline, find best one on longest uninterrupted sequence.\n\n#### Feature engineering:\n- Statistical features on two value of a particular hour (min, max, differences ...)\n- Statistical features on a time window (min, max, differences, fourier stats, special indexes...)\n- Bolinger bands (outliers)\n- EWMA crossover points\n- diff, pct_change\n- date-related features: weekday, season, month etc.\n\n#### Dimensionality reduction:\n- Feature importance with pearson's correlation\n- Multicollinearity\n\n#### Models:\n- Logistic regression as baseline with all features\n- XGBoost with all features\n- ARIMA or Prophet on a single feature\n- LSTM on a single feature\n\n#### Prediction Strategies:\n- Static-Dynamic","dda2ce41":"### 3.1. Resampling data in hourly basis\nIt's possible to use minimum, average, differences, ratio to total_capacity etc. but it's not a valid way since the maximum usage will be used as target label and it will cause information leak otherwise.","0ee62998":"#### 4.1. Baseline: Ridge regression","955eff63":"They all have same bus utilization hours and it seems like all measurements are done between aproximately **08:00 - 16:30** including irregular days.\n\nTo my point of view, missing hours can be neglected since municipalities have the same pattern. But we may need to handle missing measurements in hours depending on the method we use to choose the signature measurement for that particular hour."}}