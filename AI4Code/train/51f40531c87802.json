{"cell_type":{"af94d0fa":"code","45bed6d4":"code","b063d26d":"code","95b80e89":"code","4dc40df9":"code","a210ec43":"code","479f69a8":"code","cf51dbd8":"code","cf66c1d4":"code","43402f41":"code","43e8113d":"code","59469f32":"code","704bf6bf":"code","39680f14":"code","1e33e949":"code","e98bb574":"code","e5fe3a1c":"code","3eb5d26d":"code","b33096e4":"code","28c70644":"code","9ee82ffb":"code","6a668863":"code","d75cef0a":"code","d8c0fda9":"code","68fbaaa2":"code","63927212":"code","50e23a8e":"code","3403f650":"code","8df5af96":"code","67458915":"code","4764bdb2":"code","92c22bb5":"code","4baed8d0":"code","dcd18fce":"code","45c44b43":"code","7db52c9a":"code","3c39a1d6":"code","57e14988":"markdown","24d51c7d":"markdown","68c864d4":"markdown","d22aa19b":"markdown","f73b070f":"markdown","74d85574":"markdown","8e776ac3":"markdown","926dc5d9":"markdown","c7f496fe":"markdown"},"source":{"af94d0fa":"from sys import stdout\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scipy.signal import savgol_filter\n\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import mean_squared_error, r2_score","45bed6d4":"data = pd.read_csv(\"..\/input\/peach-nir-spectra-brix-values\/peach_spectrabrixvalues.csv\")","b063d26d":"data.head()","95b80e89":"y = data['Brix'].values\nX = data.values[:, 1:]","4dc40df9":"# Plot the data\nwl = np.arange(1100, 2300, 2)\nprint(len(wl))","a210ec43":"def plot_spectrum(X, wl, xLabel, yLabel):\n    with plt.style.context('ggplot'):\n        plt.plot(wl, X.T)\n        plt.xlabel(xLabel)\n        plt.ylabel(yLabel)","479f69a8":"plot_spectrum(X, wl, 'Wavelengths (nm)', 'Absorbance')","cf51dbd8":"# Calculate also the first and the second derrivative\nX1 = savgol_filter(X, 11, polyorder=2, deriv=1)\nX2 = savgol_filter(X, 13, polyorder=2, deriv=2)","cf66c1d4":"plot_spectrum(X1, wl, \"Wavelengths (nm)\", \"SG - first derivative\")","43402f41":"plot_spectrum(X2, wl, \"Wavelengths (nm)\", \"SG - second derivative\")","43e8113d":"def plot_spectra_vs_pls_coefficients(X, wl,num_comp, yLabel):\n    # Define the PLS regression object\n    pls = PLSRegression(n_components=num_comp)\n    # Fit data\n    pls.fit(X, y)\n    # Plot spectra\n    plt.figure(figsize=(8, 9))\n    with plt.style.context('ggplot'):\n        ax1 = plt.subplot(211)\n        plt.plot(wl, X.T)\n        plt.ylabel(yLabel)\n\n        ax2 = plt.subplot(212, sharex=ax1)\n        plt.plot(wl, np.abs(pls.coef_[:, 0]))\n        plt.xlabel(\"Wavelength (nm)\")\n        plt.ylabel(\"Absolute value of PLS coefficients\")\n\n        plt.show()","59469f32":"plot_spectra_vs_pls_coefficients(X, wl, 8, \"Absorbance\")","704bf6bf":"plot_spectra_vs_pls_coefficients(X1, wl, 8, \"SG - first derivative\")","39680f14":"plot_spectra_vs_pls_coefficients(X2, wl, 8, \"SG - second derivative\")","1e33e949":"# define a function to evaluate pls\ndef pls_evaluate_num_comp(X, y, num_comp):\n    pls = PLSRegression(n_components=num_comp)\n    y_cv = cross_val_predict(pls, X, y, cv=5)\n    mse = mean_squared_error(y_cv, y)\n    r2 = r2_score(y_cv, y)\n    rpd = y.std()\/np.sqrt(mse)\n    return (y_cv, mse, r2, rpd)","e98bb574":"# Try optimize the number of components (without variable selection) => we will use X1\ndef pls_evaluate_num_comps(X, y, num_comps):\n    mses = []\n    r2s = []\n    rpds = []\n    for num_comp in num_comps:\n        _, mse, r2, rpd = pls_evaluate_num_comp(X, y, num_comp)\n        mses.append(mse)\n        r2s.append(r2)\n        rpds.append(rpd)\n    return (mses, r2s, rpds)","e5fe3a1c":"def plot_metric(scores, objective, yLabel):\n    with plt.style.context('ggplot'):\n        plt.plot(num_comps, scores, '-o', color='blue')\n        idx = np.argmin(scores) if objective == 'min' else np.argmax(scores)\n        plt.plot(num_comps[idx], scores[idx], 'P', color='red', ms=10)\n        plt.xlabel(\"Number of components\")\n        plt.ylabel(yLabel)\n    plt.show()\n    return (num_comps[idx], scores[idx])","3eb5d26d":"def pls_evaluate_plot_num_comps(X, y, num_comps):\n    mses, r2s, rpds = pls_evaluate_num_comps(X, y, num_comps)\n    # Plot mses\n    num_comp, mse = plot_metric(mses, 'min', 'MSE')\n    print(f'The best mse is {mse} with {num_comp} PLS components')\n    # Plot r2s\n    num_comp, r2  = plot_metric(r2s, 'max', 'R2')\n    print(f'The best r2 is {r2} with {num_comp} PLS components')\n    # Plot rpds\n    num_comp, rpd = plot_metric(rpds, 'max', 'RPD')\n    print(f'The best RPD is {rpd} with {num_comp} PLS components')","b33096e4":"# test with the first 15 components and choose the best for absorbance.\nnum_comps = np.arange(1, 16)\npls_evaluate_plot_num_comps(X, y, num_comps)","28c70644":"# test with the first 15 components and choose the best for Savitzky and Golay first derivative.\nnum_comps = np.arange(1, 16)\npls_evaluate_plot_num_comps(X1, y, num_comps)","9ee82ffb":"# test with the first 15 components and choose the best for Savitzky and Golay second derivative.\nnum_comps = np.arange(1, 16)\npls_evaluate_plot_num_comps(X2, y, num_comps)","6a668863":"def pls_evaluate_and_plot_num_comp(X, y, num_comp):    \n    # Evaluate the result with first three components\n    y_cv, mse, r2, rpd = pls_evaluate_num_comp(X, y, num_comp)\n    # Print the result\n    print('MSE: %0.4f' % (mse))\n    print('R2: %0.4f' % (r2))\n    print('RPD: %0.4f' % (rpd))\n    # plot the regression\n    p = np.polyfit(y, y_cv, deg=1)\n    with plt.style.context('ggplot'):\n        plt.figure(figsize=(6, 6))\n        plt.scatter(y, y_cv, color='red', edgecolors='black')\n        plt.plot(y, y, '-g', label='Expectation')\n        plt.plot(y, np.polyval(p, y),'-b', label='Prediction regression')\n        plt.legend()\n        plt.xlabel('Actual')\n        plt.ylabel('Predicted')\n        plt.plot()\n    return (y_cv, mse, r2, rpd)","d75cef0a":"# Test for absorbance at its best number of components (6)\n_ = pls_evaluate_and_plot_num_comp(X, y, 6)","d8c0fda9":"# Test for the first derivative at its best number of components (3)\n_ = pls_evaluate_and_plot_num_comp(X1, y, 3)","68fbaaa2":"# Test for the second derivative at its best number of components (4)\n_ = pls_evaluate_and_plot_num_comp(X2, y, 4)","63927212":"def sort_variable(X, y, num_comp):\n     # Use PLS using full spectrum (all the wavelengths)\n    pls1 = PLSRegression(n_components=num_comp)\n    pls1.fit(X, y)\n    # Sort the wavelengths by the coefficients\n    sorted_ind = np.argsort(np.abs(pls1.coef_[:, 0]))\n    # Sort the spectra accordingly\n    Xc = X[:, sorted_ind]\n    return Xc","50e23a8e":"def pls_evaluate_variable(X, y, num_comp):\n    # Array of MSE each time we reduce a wavelength\n    mses = np.array([float('inf') for _ in range(X.shape[1])]) # Array of 600 elements (each time we reduce one, but we can't reduce the number of components to smaller than num_comp, thus there will be maximum values for num_comp elements at the end)\n    r2s =  np.array([float('-inf') for _ in range(X.shape[1])]) # R2 objective is to maximize (as close to 1 as possible) so the default value is set to -inf\n    rpds = np.array([float('-inf') for _ in range(X.shape[1])]) # RPD objective is to maximize (as big as possible) so the default value is set to -inf\n    \n    # Sort the spectra accordingly\n    Xc = sort_variable(X, y, num_comp)\n    # Discard wavelength one at a time (but the remained number of wavelengths must be >= num_comp)\n    for num_discarded in range(Xc.shape[1] - num_comp):\n        Xn = Xc[:, num_discarded:]\n        _, mse, r2, rpd = pls_evaluate_num_comp(Xn, y, num_comp)\n        mses[num_discarded] = mse\n        r2s[num_discarded] = r2\n        rpds[num_discarded] = rpd\n    return (mses, r2s, rpds)","3403f650":"# Helper function\ndef find_min_2d_indices(x):\n    '''\n    Find the min index from a 2D array and gives row and column indices for the min element\n    Parameters:\n        x: the 2D array\n    Returns:\n        (iIdx, jIdx): iIdx is the row index and jIdx is the column index\n    Test:\n    >>> find_min_2d_indices(np.array([[1, 2, 3], [1, 0, 1], [2, 2, 4]]))\n    (1, 1)\n    '''\n    idx = np.argmin(x)\n    iIdx = idx\/\/x.shape[1]\n    jIdx = idx - iIdx * x.shape[1]\n    return (iIdx, jIdx)\n# Helper function\ndef find_max_2d_indices(x):\n    '''\n    Find the max index from a 2D array and gives row and column indices for the min element\n    Parameters:\n        x: the 2D array\n    Returns:\n        (iIdx, jIdx): iIdx is the row index and jIdx is the column index\n    Test:\n    >>> find_max_2d_indices(np.array([[1, 2, 3], [1, 0, 1], [2, 2, 4]]))\n    (2, 2)\n    '''\n    idx = np.argmax(x)\n    iIdx = idx\/\/x.shape[1]\n    jIdx = idx - iIdx * x.shape[1]\n    return (iIdx, jIdx)","8df5af96":"# Now we evaluate the different PLS with different number of components and different number of variables and collect all the data back\ndef evaluate_with_different_num_comps(X, y, num_comps):\n    comp_mses = [] # array of array of mses (first dimension is for the number of components, second dimension is for all different )\n    comp_r2s = [] # similar to mse for r2\n    comp_rpds = [] # similar to mse for rpd\n    for num_comp in num_comps:\n        mses, r2s, rpds = pls_evaluate_variable(X, y, num_comp)\n        comp_mses.append(mses)\n        comp_r2s.append(r2s)\n        comp_rpds.append(rpds)\n    comp_mses = np.array(comp_mses)\n    comp_r2s = np.array(comp_r2s)\n    comp_rpds = np.array(comp_rpds)\n    return (comp_mses,comp_r2s, comp_rpds)","67458915":"def print_metric_info(comp_mses, comp_r2s, comp_rpds):\n    # considering comp_mses\n    min_mse_i, min_mse_j = find_min_2d_indices(comp_mses)\n    print(f'Min MSE: component index: {min_mse_i}, variable cut-off index: {min_mse_j}, Components: {num_comps[min_mse_i]}, MSE: {comp_mses[min_mse_i][min_mse_j]}')\n\n    # considering r2s => objective is to maximize r2\n    max_r2_i, max_r2_j = find_max_2d_indices(comp_r2s)\n    print(f'Max R2: component index: {max_r2_i}, variable cut-off index: {max_r2_j}, Components: {num_comps[max_r2_i]}, R2: {comp_r2s[max_r2_i][max_r2_j]}')\n    \n    # considering rpds => objective is to maximize rpds\n    max_rpd_i, max_rpd_j = find_max_2d_indices(comp_rpds)\n    print(f'Max RPD: component index: {max_rpd_i}, variable cut-off index: {max_rpd_j}, Components: {num_comps[max_rpd_i]}, RPD: {comp_rpds[max_rpd_i][max_rpd_j]}')","4764bdb2":"def find_best_components_variables(X, y, num_comps):\n    comp_mses, comp_r2s, comp_rpds = evaluate_with_different_num_comps(X, y, num_comps)\n    print_metric_info(comp_mses, comp_r2s, comp_rpds)","92c22bb5":"# Try with absorbance\nfind_best_components_variables(X, y, num_comps)","4baed8d0":"# Now try with Savitzky first derivative\nfind_best_components_variables(X1, y, num_comps)","dcd18fce":"# Now try with Savitzky second derivative\nfind_best_components_variables(X2, y, num_comps)","45c44b43":"# Now plot results for first derivative\nnum_comp = 15\nnum_discarded = 448\n# Sort according to the num_comp then select (cut from num_discardeed onward)\nXselected = sort_variable(X, y, num_comp)[:, num_discarded:]\n# Now test the performance with the selected num_comp and selected variables\ny_cv, mse, r2, rpd = pls_evaluate_and_plot_num_comp(Xselected, y, num_comp)","7db52c9a":"# Now plot results for first derivative\nnum_comp = 10\nnum_discarded = 417\n# Sort according to the num_comp then select (cut from num_discardeed onward)\nXselected = sort_variable(X1, y, num_comp)[:, num_discarded:]\n# Now test the performance with the selected num_comp and selected variables\ny_cv, mse, r2, rpd = pls_evaluate_and_plot_num_comp(Xselected, y, num_comp)","3c39a1d6":"# Now plot results for first derivative\nnum_comp = 12\nnum_discarded = 503\n# Sort according to the num_comp then select (cut from num_discardeed onward)\nXselected = sort_variable(X2, y, num_comp)[:, num_discarded:]\n# Now test the performance with the selected num_comp and selected variables\ny_cv, mse, r2, rpd = pls_evaluate_and_plot_num_comp(Xselected, y, num_comp)","57e14988":"So if we are considering first derivative, the mse and rpds, the number of components is at the 9th index, and the cut off for variable selection is at 417 variables (cut the first 417)\nNote that the number of components is the index + 1 (since we start with 1 components for the first zero index), but the number of variable is 417 since we start with 0 (discard_index:)\n\nSimilarly for original data or second derivative.\n\nIn this case, the second derivative works best with variable selection.","24d51c7d":"## Commment\nIt is observable second derivatives and with 12 PLS components perform the best","68c864d4":"The second deriviate is not as good as the first one.","d22aa19b":"The result for the first derivative is slightly better","f73b070f":"# Variable selection method for PLS in Python\nLearned from: https:\/\/nirpyresearch.com\/variable-selection-method-pls-python\/\n\nRPD of 1.3495 is low and not very stable. Therefore, we could use PLS for variable selection.\n## Variable selection in chemometrics\nThe idea behind variable selection in chemometrics is that when it comes to spectral measurements not all wavelengths are created equals. In visible and NIR spectroscopy especially, it is often hard topredict in advance which wavelength bands will contain most of the signal related to the analysis we want to measure. So, as a first shot, we measure the whole range allowed by our instrument, and then figure out later which bands are more releavant for our calibration.\n\nTo say the same thing in a bit more quantitatie way, we want to check which wavelength bands lead to a better quality model. Actually, in practice, we check which bands give a worse quality model, so we can get rid of them.\n\nThis seems logical enough, but I've deliberately left a fundamental piece of the puzzle out. How dod we even break our spectrum into bands and searching for the worst performing ones?\n\nFor thsi example we'll take a simple approach: We'll filter out wavelength bands based on the strength of the related regression coefficients.\n\n## Feature selection by filtering\nFeature selection by filtering is one of the simplest ways of performing wavelegnth selection. For an overview of the methods that are currently used, check out this excellent review paper by T. Mehmood et al.: [A review of variable selection methods in Partial Least Square Regression](https:\/\/doi.org\/10.1016\/j.chemolab.2012.07.010)\n\nThe idea behind this method is simple, and can be summarized as the following:\n1. Optimize the PLS regression using the full spectrum, for instance using cross-validation or prediction data to quantify its quality\n2. Extract the regression coefficients form the best model. Each regression coefficient uniquely associates each wavelength with the response. A low absolute value of the regression coefficient means that specific wavelegnth has a low correlation with the quantity of interest.\n3. Discard the lowest correlation wavelengths accordign to some rule. These wavelengths are the one that typically worsen the quality of the calibration model, therefore by discarding them effectively we expect to improve the metrics associated with our prediction or cross-validation\n\nOne way to discard the lowest correlation wavelengths is to set a threshold and get rid of all wavelengths whose regression coefficients (in absolute value) fall below that threshold. However, this method is sensitive to the choice of threshold, which tends to be subjective choice, or requires a trial-and-error approach.\n\nAnother approach, that is much less subjective, is to discard one wavelength at a time (the one with the lowest absolute value of the associated regression coefficient) and rebuild the calibration model. By choosing the MSE of prediction or cross-validation as metric, the procedure is interated until the MSE decreases. At some point, removing wavelengths will produce a worse calibration, and that is the stopping criterion for the optimisation algorithm.\n\nAlternatively, one could simply remove a fixed number of wavelengths iteratively, and then check for which number of removed wavelengths the MSE is minimised. Either way we have a method that does not depend on the subjective choice of the thresold and can be applied without changes regardless of the data set being analysed.\n\nOne caveat of this method, at least in its simple implementaiton is that it may get a bit slow for large datasets.","74d85574":"What we have done is a basic PLS regression with 8 components and used it to fit the absorbance, first derivative, and second derivate data. These are the regression coefficients that quantify the strength of the association between each wavelength and the response.\nWe are interested in the absolute value of these coefficients, as large positive and large negative coefficients are equally important for our regression, denoting large positive or large negative correlation with response respectively. In other words we want to identify and discoard the regression coefficients that are close to zero: they are the ones with poor correlation with the response.\n\nAs we can observe, the number of wavelengths associated with the responses are small.\n\n## NIR variable selection for PLS regression\ne.g.,<br\/>\n`# get the list of indices that sorts the PLS coefficients in ascending order of the absolute value\nsorted_ind = np.argsort(np.abs(pls.coef_[:, 0]))`\n\n`# sort the spectra accoridng to ascendign absolute value of PLS coefficients.\nXc = X1[:, sorted_ind]`","8e776ac3":"If required, data can be easily sorted by PCA and corrected with multiplicative scatter correction, however, another simple yet effective way to get rid of baseline and linear variations is to perform second derivative on the data.","926dc5d9":"## Comments\n### Event with the best number of PLS components (3) for the first derivative (best results)\nRPD of 1.3335 is not acceptable.<br\/>\nR2: -0.0662 is even worse than the simple base-line prediction as mean of y\n\nTherefore we will try to select variables","c7f496fe":"Note also that we might consider to normalize (standardize using z-score, or to make unit variance and zero mean) the data and see if the result is better."}}