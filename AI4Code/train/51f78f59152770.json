{"cell_type":{"178c0cfb":"code","b52580b9":"code","fc0e9eb0":"code","9d82619a":"code","65bbf849":"code","3844a9e6":"code","c25af84c":"code","2985a63b":"code","5597c075":"code","3a1eb24d":"code","db647e11":"code","f95722c8":"markdown","f35aa87b":"markdown","71ed602b":"markdown","7ce761bc":"markdown","031ff627":"markdown","c60261e6":"markdown","e4c80c8f":"markdown","e23f0750":"markdown","d443cd56":"markdown","60b1331a":"markdown","e3d1c573":"markdown","4aaefa44":"markdown","5a19c131":"markdown","1816d919":"markdown","001d06a5":"markdown","0580e7d9":"markdown","af33f304":"markdown","eeaddfad":"markdown","ca8bf22a":"markdown"},"source":{"178c0cfb":"import pandas as pd\nimport numpy as np\nfrom sklearn import ensemble \nfrom sklearn import metrics\nfrom sklearn import model_selection\n","b52580b9":"df = pd.read_csv(\"..\/input\/mobile-price-classification\/train.csv\")\n# features are all columns without price_range\n# note that there is no id column in this dataset\n# here we have training features\nX = df.drop(\"price_range\", axis=1).values\n# and the targets\ny = df.price_range.values\nX.shape, y.shape","fc0e9eb0":"\"\"\"\nRandomForestClassifier( \n    n_estimators=100, \n    criterion='gini', \n    max_depth=None, \n    min_samples_split=2,\n    min_samples_leaf=1, \n    min_weight_fraction_leaf=0.0,\n    max_features='auto',\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0, \n    min_impurity_split=None, \n    bootstrap=True,\n    oob_score=False,\n    n_jobs=None,\n    random_state=None,\n    verbose=0,\n    warm_start=False, \n    class_weight=None,\n    ccp_alpha=0.0, \n    max_samples=None,\n)\n\nThere are nineteen parameters,\nand all the combinations of all \nthese parameters for all the values\nthey can assume are going to be infinite. \nNormally, we don\u2019t have the resource and\ntime to do this. Thus, we specify a\ngrid of parameters. A search over this\ngrid to find the best combination of parameters \nis known as grid search. \nWe can say that n_estimators can be\n100, 200, 250, 300, 400, 500; \nmax_depth can be 1, 2, 5, 7, 11, 15 and \ncriterion can be gini or entropy. \nThese may not look like a lot of parameters, \nbut it would take a lot of time for computation\nif the dataset is too large.\n\"\"\"","9d82619a":"# define the model here\n# i am using random forest with n_jobs=-1\n# n_jobs=-1 => use all cores\nclassifier = ensemble.RandomForestClassifier(n_jobs=-1)\n# define a grid of parameters\n# this can be a dictionary or a list of \n# dictionaries\nparam_grid = {\n    \"n_estimators\" : [100, 200, 300, 400],\n    \"max_depth\" : [1, 4, 6, 9],\n    \"criterion\" : [\"gini\", \"entropy\"],\n}\n\n# initialize grid search\n# estimator is the model that we have defined\n# param_grid is the grid of parameters\n# we use accuracy as our metric. you can define your own\n# higher value of verbose implies a lot of details are printed\n# cv=5 means that we are using 5 fold cv (not stratified)\n\nmodel = model_selection.GridSearchCV(\n    estimator=classifier,\n    param_grid=param_grid,\n    scoring=\"accuracy\",\n    verbose=10,\n    n_jobs=1,\n    cv=5\n)\n\n# fit the model and extract best score\nmodel.fit(X, y)\n","65bbf849":"print(model.best_score_)\nprint(model.best_estimator_.get_params())","3844a9e6":"# define the model here\n# i am using random forest with n_jobs=-1\n# n_jobs=-1 => use all cores\nclassifier = ensemble.RandomForestClassifier(n_jobs=-1)\n\n# define a grid of parameters\n# this can be a dictionary or a list of # dictionaries\nparam_grid = {\n    \"n_estimators\" : np.arange(100, 1500, 100),\n    \"max_depth\" : np.arange(1, 20),\n    \"criterion\" : [\"gini\", \"entropy\"],\n}\n\n# initialize random search\n# estimator is the model that we have defined\n# param_distributions is the grid\/distribution of parameters\n# we use accuracy as our metric. you can define your own\n# higher value of verbose implies a lot of details are printed\n# cv=5 means that we are using 5 fold cv (not stratified)\n# n_iter is the number of iterations we want\n# if param_distributions has all the values as list,\n# random search will be done by sampling without replacement\n# if any of the parameters come from a distribution,\n# random search uses sampling with replacement\nmodel = model_selection.RandomizedSearchCV(\n    estimator=classifier,\n    param_distributions=param_grid,\n    scoring=\"accuracy\",\n    n_iter=20,\n    verbose=10,\n    n_jobs=1,\n    cv=5\n)\nmodel.fit(X, y)\nprint(f\"Best score: {model.best_score_}\")\n\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(f\"\\t{param_name}: {best_parameters[param_name]}\")","c25af84c":"from sklearn import decomposition\nfrom sklearn import preprocessing\nfrom sklearn import pipeline","2985a63b":"rf = ensemble.RandomForestClassifier(n_jobs=-1)\n\n\n# Here I am creating pipeline\nscl = preprocessing.StandardScaler()\npca = decomposition.PCA()\nrf = ensemble.RandomForestClassifier(n_jobs=-1)\n\nclassifier = pipeline.Pipeline(\n    [\n        (\"scaling\", scl),\n        (\"pca\", pca),\n        (\"rf\", rf),\n    ]\n\n)\n\nparam_grid = {\n    # Here we are using our pipeline key \n    # like #pca__, #scaling__, #rf__\n    \"pca__n_components\" : np.arange(5, 10),\n    \"rf__n_estimators\" : np.arange(100, 1500, 100),\n    \"rf__max_depth\" : np.arange(1, 20),\n    \"rf__criterion\" : [\"gini\", \"entropy\"],\n}\n\nmodel = model_selection.RandomizedSearchCV(\n    estimator=classifier,\n    param_distributions=param_grid,\n    scoring=\"accuracy\",\n    n_iter=10,\n    verbose=10,\n    n_jobs=1,\n    cv=5\n)\nmodel.fit(X, y)\nprint(model.best_score_)\nprint(model.best_estimator_.get_params())","5597c075":"from functools import partial\nfrom skopt import space\nfrom skopt import gp_minimize\n# Function to minimize. \n# Should take a single list of parameters\n# and return the objective value.\ndef optimize(params, param_names, x, y):\n    params = dict(zip(param_names, params)) # This you cannot use when you tuning multiple params things  \n    model = ensemble.RandomForestClassifier(**params)\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y=y):\n        train_idx, valid_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        \n        xvalid = x[valid_idx]\n        yvalid = y[valid_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xvalid)\n        fold_acc = metrics.accuracy_score(yvalid, preds)\n        accuracies.append(fold_acc)\n        \n    # We need to return minimize \n    return -1.0 * np.mean(accuracies)\n\nparam_space = [\n    # Order is matter\n    space.Integer(3, 15, name=\"max_depth\"),\n    space.Integer(100, 600, name=\"n_estimators\"),\n    space.Categorical([\"gini\", \"entropy\"], name=\"criterion\"),\n    space.Real(0.01, 1, prior=\"uniform\", name=\"max_features\")\n]\nparam_names = [\n    \"max_depth\",\n    \"n_estimators\",\n    \"criterion\",\n    \"max_features\"\n]\noptimization_function = partial(\n    optimize, \n    param_names=param_names,\n    x=X,\n    y=y\n)\n\nresult = gp_minimize(\n    optimization_function,\n    dimensions=param_space,\n    n_calls=15,\n    n_random_starts=10,\n    verbose=10,\n)\nprint(dict(zip(param_names, result.x)))\n","3a1eb24d":"from hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom functools import partial\nfrom skopt import space\nfrom skopt import gp_minimize\n# Function to minimize. \n# Should take a single list of parameters\n# and return the objective value.\ndef optimize(params, x, y):\n    model = ensemble.RandomForestClassifier(**params)\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y=y):\n        train_idx, valid_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        \n        xvalid = x[valid_idx]\n        yvalid = y[valid_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xvalid)\n        fold_acc = metrics.accuracy_score(yvalid, preds)\n        accuracies.append(fold_acc)\n        \n    # We need to return minimize \n    return -1.0 * np.mean(accuracies)\n\nparam_space = {\n    # Order is matter\n    \"max_depth\" : scope.int(hp.quniform(\"max_depth\", 3, 15, 1)),\n    \"n_estimators\" : scope.int(hp.quniform(\"n_estimators\",100, 600, 1)),\n    \"criterion\" : hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n    \"max_features\" : hp.uniform(\"max_features\", 0.01, 1)\n}\n\noptimization_function = partial(\n    optimize, \n    x=X,\n    y=y\n)\n\ntrials = Trials()\n\nresult = fmin(\n    fn=optimization_function,\n    space=param_space,\n    algo=tpe.suggest,\n    max_evals=15,\n    trials=trials,\n)\nprint(result)","db647e11":"import optuna\nfrom functools import partial\nfrom skopt import space\nfrom skopt import gp_minimize\nfrom hyperopt import Trials\n\n# Function to minimize. \n# Should take a single list of parameters\n# and return the objective value.\ndef optimize(trails, x, y):\n    criterion = trails.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n    n_estimators = trails.suggest_int(\"n_estimators\", 100, 1500)\n    max_depth = trails.suggest_int(\"max_depth\", 3, 15)\n    max_features = trails.suggest_uniform(\"max_features\", 0.01, 1.0)\n    \n    model = ensemble.RandomForestClassifier(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        max_features=max_features,\n        criterion=criterion\n    )\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y=y):\n        train_idx, valid_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        \n        xvalid = x[valid_idx]\n        yvalid = y[valid_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xvalid)\n        fold_acc = metrics.accuracy_score(yvalid, preds)\n        accuracies.append(fold_acc)\n        \n    # We need to return minimize \n    return -1.0 * np.mean(accuracies)\n\ntrails = Trials()\noptimization_function = partial(optimize, x=X, y=y)\n\nstudy = optuna.create_study(direction=\"minimize\")\n\nstudy.optimize(optimization_function, n_trials=15)","f95722c8":"# 3. You can also do same thing with some kind of custom.","f35aa87b":"# 2. Randomized Search CV <\/br>\nRandomizedSearchCV is very useful when we have many parameters to try and the training time is very long. For this example, I use a random-forest classifier, so I suppose you already know how this kind of algorithm works.","71ed602b":"> I show you how you can use different hyperparameter optimization techniques and libraries to tune hyperparameters of almost any kind of model or just to optimize any function! \n\nContents:-\n1. GridSearchCV - score(0.8805)\n2. RandomizedSearchCV - score(0.8825) \n3.  Grid\/Random Search with Pipelines. - score(0.45990\n4. Bayesian optimization using Gaussian Processes. - score(0.9050)\n5. Hyperopt - score(0.9075)\n6. Optuna - score(0.9085) ---Best","7ce761bc":"- WOW...with using hyperopt we improved our score futhure more.","031ff627":"# 4. Bayesian optimization using Gaussian Processes.\n\n<\/br>\nYou can read here : https:\/\/scikit-optimize.github.io\/stable\/modules\/generated\/skopt.gp_minimize.html","c60261e6":"- It must also be noted that if you have k- fold cross-validation, you need even more loops which implies even more time to find the perfect parameters. Grid search is therefore not very popular.","e4c80c8f":"# 5. Hyperopt: Distributed Asynchronous Hyper-parameter Optimization\n","e23f0750":"Wow... again we improved our accuracy.\nSo, for this situation the Optuna scored high.\n0.9050","d443cd56":"We have changed the grid of parameters for random search, and it seems like we even improved the results a little bit.","60b1331a":"WOW, it is amazing to see model itself choose its best values. <\/br>These are the best hyperparameter values for this model. <\/br>\nYou can try with different parameters values and see if the accuracy score goes high or not, play with parameters.","e3d1c573":"WOW...with using gp_minimize we got improved our score.","4aaefa44":"- You can also use some kind of pipelines.","5a19c131":"# 1. Grid_search. <\/br>\nGrid-search is used to find the optimal hyperparameters of a model which results in the most \u2018accurate\u2019 predictions.","1816d919":"#################################################<\/br>\nI like to try to tune my hyper parameters manually  first and then choose a range of values and then \nthrow in some kind of Optimization Algorithm.<\/br>\n##################################################","001d06a5":"In random search, we randomly select a combination of parameters and calculate the cross-validation score. The time consumed here is less than grid search because we do not evaluate over all different combinations of parameters. We choose how many times we want to evaluate our models, and that\u2019s what decides how much time the search takes.","0580e7d9":"we see that our best five fold accuracy score was 0.8805 and we have the best parameters from our grid search. ","af33f304":"Sometimes, you might want to use a pipeline. For example, let\u2019s say that we are dealing with a multiclass classification problem. In this problem, the training data consists of two text columns, and you are required to build a model to predict the class. ","eeaddfad":"The score is not good after using randomizedsearchcv with pipeline.","ca8bf22a":"# 6. Optuna\n<\/br>\nTake a look : https:\/\/optuna.org\/"}}