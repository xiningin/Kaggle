{"cell_type":{"3adcc384":"code","bb6385d6":"code","e8296144":"code","15468a0d":"code","81b84b2d":"code","8322be41":"code","ce0ef4de":"code","92754f93":"code","9c4c6915":"code","3c1cc08c":"code","725d0a28":"code","7fe1b5a7":"code","80318f22":"code","14adbbe5":"code","2779cca3":"code","c3a10f4b":"code","8b4d44b0":"code","244a665c":"code","9e938dad":"code","26c720ee":"code","e3b19f6a":"code","015184f2":"code","a599d439":"code","aa7e4edd":"code","52c7dd21":"code","e1d9f1bb":"code","b3b34b48":"code","c09debb4":"code","afb88839":"code","40b79ff5":"code","19547f56":"code","99af90ed":"code","d1b31f53":"code","37b13d61":"code","cf50ecfb":"code","0fd900a0":"code","08019ec4":"code","99b890ab":"code","62e1507b":"code","f6418715":"code","2eee92cb":"code","67fdc901":"code","dd85ae6e":"code","b44359df":"code","67eb33f1":"code","65e25b47":"code","8f4631a6":"markdown","79ea453a":"markdown","f2ab8a05":"markdown","baaa7034":"markdown","6556c224":"markdown","1a45f459":"markdown","83b5755c":"markdown","f66a0e23":"markdown","02aac0cb":"markdown","b2e9a146":"markdown","3ce47f1e":"markdown","f4fd1ceb":"markdown","67d16c47":"markdown","8d2d2515":"markdown","2e36beab":"markdown","3ee80859":"markdown","b121be4b":"markdown","8d649546":"markdown","a54970f8":"markdown","f038017b":"markdown","67d6f67f":"markdown","7e9ec340":"markdown","7305d050":"markdown","b46054ed":"markdown","001f2269":"markdown","f6e021ee":"markdown","cc8a88f1":"markdown","b0126053":"markdown","c0b65a04":"markdown","7c423392":"markdown","45277fa8":"markdown","bcbdb863":"markdown","3684ec67":"markdown","382806a8":"markdown","37f7f659":"markdown","d24e902f":"markdown","f00480a4":"markdown","4a17a615":"markdown","ccaa2e17":"markdown","e131eff9":"markdown","8a287f44":"markdown","52e6f17d":"markdown","7c40bc52":"markdown","bd9f07d2":"markdown","14ddcdf1":"markdown","e04454f6":"markdown","fd86edfc":"markdown","d83520e1":"markdown","da4022be":"markdown"},"source":{"3adcc384":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport category_encoders as ce\nimport sklearn\nfrom sklearn.model_selection import train_test_split #splitting the data\nfrom sklearn.ensemble import RandomForestClassifier #Classification using randomforest\nfrom sklearn.metrics import confusion_matrix # mapping the true and false points\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom collections import Counter","bb6385d6":"df = pd.read_csv(\"..\/input\/ashrae-global-thermal-comfort-database-ii\/ashrae_db2.01.csv\")\ndf","e8296144":"df.isnull().sum()","15468a0d":"# loc[] is primarily label based filtering method, to access a group of rows and columns by label\ndf = df.loc[:, df.isna().mean() < .67]","81b84b2d":"df.columns","8322be41":"df.isnull().sum()","ce0ef4de":"df.dtypes","92754f93":"# Categories in the \"Country\" feature\n(df['Country'].unique())","9c4c6915":"# Publication\ndf['Publication (Citation)'].unique()","3c1cc08c":"df['Koppen climate classification'].unique()","725d0a28":"df['Climate'].unique()","7fe1b5a7":"df['Building type'].unique()","80318f22":"df['Year'].unique()","14adbbe5":"df['Season'].unique()","2779cca3":"df['City'].unique()","c3a10f4b":"df['Cooling startegy_building level'].unique()","8b4d44b0":"df['Building type'].unique()","244a665c":"df['Thermal preference'].unique()","9e938dad":"df['Clo'].unique()","26c720ee":"df['Met'].unique()","e3b19f6a":"# The target variable\ndf['Thermal comfort'].unique()","015184f2":"# Also check the values per category\ndf['Thermal comfort'].value_counts()","a599d439":"df = df[['Publication (Citation)', 'Data contributor', 'Year', 'Season',\n       'Koppen climate classification', 'Climate', 'City', 'Country',\n       'Building type', 'Cooling startegy_building level', 'Sex',\n       'Thermal sensation',\n       'Thermal preference','Clo', 'Met',\n       'Air temperature (C)', 'Relative humidity (%)',\n       'Air velocity (m\/s)',\n       'Outdoor monthly air temperature (C)', 'Database', 'Thermal comfort']]","aa7e4edd":"df.isnull().sum()","52c7dd21":"df = df.dropna()","e1d9f1bb":"# Checking for outliers in the target variable \"Thermal Comfort\"\ndf = df[df['Thermal comfort']!= 'Na']\ndf = df[df['Thermal comfort']!= 1.3]\n\n# Converting the data to integer so 5.0 and 5 will under one category and outliers would be categorised in nearest integer\ndf['Thermal comfort'] = df['Thermal comfort'].astype('int64')\ndf['Thermal comfort'].value_counts()","b3b34b48":"# Converting the thermal comfort column back to categorical data type so as to make it cleaner and easier for prediction\ndf['Thermal comfort'] = df['Thermal comfort'].astype('category')","c09debb4":"df.dtypes","afb88839":"df = pd.concat([df.select_dtypes([], ['object']),df.select_dtypes(['object']).apply(pd.Series.astype, dtype='category')], axis=1)","40b79ff5":"# Checking the data types again after alteration and conversion\ndf.dtypes","19547f56":"# Checking for NULL values\ndf.isnull().sum()","99af90ed":"y = df['Thermal comfort']\ndf.drop(['Thermal comfort'],axis = 1, inplace = True)","d1b31f53":"# One hot encoding\ndf = pd.get_dummies(data=df,drop_first=True)","37b13d61":"X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.3, random_state = 42)","cf50ecfb":"# fitting randomforestclassifier algorithm to the train set\nclf = RandomForestClassifier(n_estimators = 540) # ideally the number of estimators in a random forest algorithm should be 10 x number of features\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)","0fd900a0":"# Get the Accuracy Score\nprint('Accuracy : %.4f' % (accuracy_score(y_test, pred)*100))","08019ec4":"confusion_matrix(y_test, pred)","99b890ab":"clf = GaussianNB()\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)","62e1507b":"# Get the Accuracy Score\nprint('Accuracy : %.4f' % (accuracy_score(y_test, pred)*100))","f6418715":"confusion_matrix(y_test, pred)","2eee92cb":"clf = sklearn.tree.DecisionTreeClassifier()\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)","67fdc901":"# Get the Accuracy Score\nprint('Accuracy : %.4f' % (accuracy_score(y_test, pred)*100))","dd85ae6e":"confusion_matrix(y_test, pred)","b44359df":"clf = BaggingClassifier(n_estimators=540)\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)","67eb33f1":"# Get the Accuracy Score\nprint('Accuracy : %.4f' % (accuracy_score(y_test, pred)*100))","65e25b47":"confusion_matrix(y_test, pred)","8f4631a6":"> Selecting the required columns","79ea453a":"We always need numeric or categorical data type for predictions, so we need to convert \"object\" data types to categorical. \n","f2ab8a05":"# **Predicting the \"Thermal Comfort\" using Ashrae Database**","baaa7034":"**1. Loading the Data**","6556c224":"**4. Fitting the Machine Learning Algorithms**","1a45f459":"![inbox-23583-1f2b289820901ce42af1df75532e32f7-Screenshot%202020-07-21%20at%209.53.04%20AM.png](attachment:inbox-23583-1f2b289820901ce42af1df75532e32f7-Screenshot%202020-07-21%20at%209.53.04%20AM.png)","83b5755c":"> Selecting the columns which have less than 67 % Null values ","f66a0e23":"![inbox-23583-4d6258c2e9ad19b8e0dd48b5dcf6c34c-Screenshot%202020-07-21%20at%209.52.50%20AM.png](attachment:inbox-23583-4d6258c2e9ad19b8e0dd48b5dcf6c34c-Screenshot%202020-07-21%20at%209.52.50%20AM.png)","02aac0cb":"We have splitted the train and test data in ratio of 70:30. We will train the model on our train dataset and then test it on our test set, as the name says.","b2e9a146":"Thank you for reading my notebook, if you liked it please **_Upvote_** !","3ce47f1e":"Lets try out a few more Machine Learning Algorithms used for Classification","f4fd1ceb":"![inbox-23583-6240821b776b8d6be0cecc9081acb2c7-Screenshot%202020-07-21%20at%209.52.58%20AM.png](attachment:inbox-23583-6240821b776b8d6be0cecc9081acb2c7-Screenshot%202020-07-21%20at%209.52.58%20AM.png)","67d16c47":"The horizontal rows of the matrix are the **Actual Values (1-6)** top to bottom and the vertical columns are the **Predicted Values (1-6)** left to right. And the diagnals are the number of correctly mapped values. The accuracy score might not be too high, but if you look at the the number of values assigned as its prior or succesive category is quite high.","8d2d2515":"> *Cleaning the target variable ~ Thermal Comfort*","2e36beab":"> Converting data type of all columns with \"object\" to \"category\"","3ee80859":"What is [\"One Hot Encoding\"](https:\/\/medium.com\/@michaeldelsole\/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179) ? Here are two simple steps to define it.\n1. Each unique category value is assigned an integer value. \n2. Then the integer encoded variable is removed and a new binary variable is added for each unique integer value.","b121be4b":"> Let us again check the number of empty data points (rows) in the columns we are left with.","8d649546":"Lets check the features which are of the data type object","a54970f8":"> Checking the data types","f038017b":"**2. Cleaning and Analysing the Data**","67d6f67f":"The Ashrae database we have here, is a huge dataset with 1,07,583 rows and wide set features (70).","7e9ec340":"Splitting the data into train and test set","7305d050":"> Ready for the Machine Learning !!","b46054ed":"Plotting the confusion matrix, to describe the performance of our random forest classification model on the set test dataset","001f2269":"We will do one hot encoding so as to get the most out of the feature variables. ","f6e021ee":"As you can see the accuracy of Naive Bayes algorithm was lesser than that of our previously used random forest algorithm. ","cc8a88f1":"Slightly better than Naive Bayes but not as good as random forest algorithm","b0126053":"> Checking the number of empty values in each column","c0b65a04":"This is the description of the dataset, uploaded by the [Data Contributor.](https:\/\/www.kaggle.com\/claytonmiller\/ashrae-global-thermal-comfort-database-ii)","7c423392":"**3. Splitting the Data**","45277fa8":"Point to note, some of the features has \"nan\" as an category, but it should not be eliminated unless we weigh the merits of these categories, lets see if this helps us further or else we can simply remove them.","bcbdb863":"> * _Naive Bayes Classification Algorithm_","3684ec67":"> Dropping the rows with Null values","382806a8":"Dividing the data into target variables and feature variables","37f7f659":"Used the number 67 here because there were many columns which were totally empty, so wanted to remove them where there were some important columns like **\"Thermal Comfort\"** which were crucial for the analysis. Our Analysis is based on \"Thermal Comfort\" as we it is our target variable (The column which we are predicting). ","d24e902f":"> Loading the Ashrae data","f00480a4":"> Loading different modules required","4a17a615":"> We will be using 4 types of classification machine learning algorithms : \n> * Random Forest\n> * Naive Bayes\n> * Decision Tree \n> * Bagging","ccaa2e17":"> * _Bagging Algorithm_","e131eff9":"We will be using the columns mentioned above. Dropped the columns which were same, like some features were given in two formats, celsius and fahreinheit ","8a287f44":"> Thermal Comfort","52e6f17d":"> * _Decision Tree Classifier_","7c40bc52":"Bagging is also ensemble meta algorithm like randomforest. Bagging has produced a result to our best result produced by randomforest.","bd9f07d2":"A point to note here is the feature, year, should not be labelled as numerical data, the data type of year is \"float64\" which should be converted to categorical data as the collection of data is classified based on the year of collection too. ","14ddcdf1":"Thermal Comfort column is not clean enough to work on, let us keep 6 categories only, we will segragate the data on a scale of integers (1 to 6). Will come back to this part later on in the notebook, now let us clean the whole data.","e04454f6":"[Thermal comfort](https:\/\/en.wikipedia.org\/wiki\/Thermal_comfort) is defined as the condition of mind that expresses satisfaction with the thermal environment and is assessed by subjective evaluation. Being a subjective evaluation, in a congregation some people may be satisfied of the climate conditions, others may feel too warm, others too cool. The human body will release excess heat into the environment, so the body can continue to operate. \n\nLet us look analyse our Ashrae database and predict the \"Thermal Comfort\" using various classification algorithms and then compare them on various factors. ","fd86edfc":"> * _Random Forest Classification Algorithm_","d83520e1":"What is interesting in the results of Naive Bayes is that, the model predicted values a lot around the upper half of the categorical set (4-6). It has correctly predicted more number of 6's (Very Comfortable) than our previous model, but has failed to classify anything under 1 (Very Uncomfortable).","da4022be":"> Lets see whats all feature we have for our Analysis"}}