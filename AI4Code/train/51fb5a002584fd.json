{"cell_type":{"575b2c20":"code","1b84dce2":"code","79b0d263":"code","ae34ff10":"code","f4df310d":"code","78f8a577":"code","73df1343":"code","d64be192":"code","8c13a907":"code","27aeb67e":"code","5f0ffe57":"code","857ace1c":"markdown","96cd167f":"markdown","a4834491":"markdown","440a63f6":"markdown","ab73da6e":"markdown"},"source":{"575b2c20":"import numpy as np\nimport pandas as pd\nimport re\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import PorterStemmer\n\nstopwords=set(stopwords.words('english'))\n\nfrom gensim.models import TfidfModel, LdaModel\nfrom gensim.corpora import Dictionary\n\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\npd.set_option('display.max_columns', 20)\npd.set_option('max_colwidth',100)","1b84dce2":"got = pd.read_csv('..\/input\/got-subtitles\/Scripts.csv')\n\n#Remove \".srt\" from Episode\ngot['Episode'] = got['Episode'].map(lambda x: x.split('.srt')[0])\n\ngot.head()","79b0d263":"got_byepline = got.melt(id_vars='Episode').rename(columns={'variable': 'Line', 'value': 'Text'})\n\ngot_byepline['Line'] = got_byepline['Line'].astype(int)\n\ngot_byepline = got_byepline.dropna().groupby(['Episode', 'Line']).first()","ae34ff10":"# Remove bad lines containing no natural language\n\ndef remove_wordlist(series, wordlist):\n    out = series\n    for word in wordlist:\n        out= remove_word(out,word)\n    return out\n\ndef remove_word(series, word):\n    return series[~series.str.contains(word)]","f4df310d":"bad_words = [\"@elder_man\", \"Fidel33\", \"Game of Thrones\", \"Sync & corrections\", \"usic play\", \"GRUNT\", \"GROAN\", \"PANT\", \"PIANO\", \"EXPLO\", \"YELL\", \"CACKLE\"]\n\n\ngot_byepline = got_byepline.loc[remove_wordlist(got_byepline['Text'], bad_words).index]\ngot_byep = got_byepline.groupby('Episode')[['Text']].agg(lambda x : '\\n'.join(x)) ","78f8a577":"# We need to clean our subtitles from over-used words\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_signs(text):\n    return re.sub(r'[^\\w]', ' ', text)\n\nporter=PorterStemmer()\n\ndef clean_text(text):\n    no_html = remove_html(text)\n    no_signs = remove_signs(no_html)\n    word_tokens = word_tokenize(no_signs.lower())  \n    tokens_without_sw = [w for w in word_tokens if not w in stopwords]\n    stemmed_tokens = [porter.stem(w) for w in tokens_without_sw]\n    out = (\" \").join(stemmed_tokens)\n    return out","73df1343":"got_byep['Clean Text'] = got_byep['Text'].map(clean_text)\ngot_byep","d64be192":"# List of episodes. Each episode itself is a list of clean, stemmed words\ndataset = [episode.split() for episode in got_byep['Clean Text'].values]\n\n# Fit tokenizer\ndct = Dictionary(dataset)\n\n# Tokenized BoW dataset\ncorpus = [dct.doc2bow(episode) for episode in dataset]\n\n#Fit TfIdf\nmodel = TfidfModel(corpus)\n\n# Not used - The TfIdf model and dictionary are enough to go!\n#vectors = model[corpus]","8c13a907":"# Chunksize is by default 2000, but we only have 67 episodes (no, season 8 doesn't count)\nlda = LdaModel(corpus, id2word=dct, num_topics=5, iterations = int(1e6), chunksize = 1)\n\n# Not used\n#corpus_lda = lda[vectors]","27aeb67e":"lda.show_topics(5,8)","5f0ffe57":"for t in range(lda.num_topics):\n    plt.figure(figsize=(8, 8))\n    wc = WordCloud(background_color='white', scale=3)\n    plt.imshow(wc.fit_words(dict(lda.show_topic(t, 200))))\n    plt.axis(\"off\")\n    plt.title(\"Topic \" + str(t))\n    plt.show()","857ace1c":"## Fit Gensim's LDA","96cd167f":"## Bag of Words Tf-Idf vectors\nWe need to create several objects to power the human-friendly Gensim library","a4834491":"# Game of Thrones Topic Modeling\n\n## In this notebook we extract and visualize Topics from GOT episodes.\n\n### By using Gensim, we achieve a sound result with few lines of effort!","440a63f6":"### Wrangle data","ab73da6e":"## Visualize with Word Clouds"}}