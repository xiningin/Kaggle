{"cell_type":{"0a8840f2":"code","b5c175b5":"code","cf906a95":"code","5f1bb182":"code","05d55860":"code","4b39c2bc":"code","ce24075d":"code","e6c180c0":"code","311ecf90":"code","7d6b5c96":"code","c8ca04fa":"code","96c3d0d0":"code","80c1a9ed":"code","069ccbaf":"code","4af648c0":"code","641d3602":"code","3b73d783":"code","01cbe310":"code","3b6c219b":"code","6f329449":"code","df06304c":"code","a7b000a4":"code","6fe77877":"code","67a4c4b5":"code","f11e9c73":"code","25d05215":"code","e8604d68":"code","ed40818c":"code","333a2ee2":"code","79b3ad47":"code","a8fd6c92":"code","6ace3795":"code","7bb76382":"code","90928789":"code","b53e6061":"code","7759f5e4":"code","177b1de8":"code","88ea5c65":"code","10306d80":"code","32a1a95f":"code","e8e366da":"code","b4baccf0":"code","9963beda":"code","479498cb":"code","fc873c0c":"code","8e066220":"markdown","d884de76":"markdown","41885ff9":"markdown","65109658":"markdown","9ec27ab7":"markdown","e3241d1f":"markdown","77e90173":"markdown","95f9468d":"markdown","a1e0e158":"markdown","4917777c":"markdown","39528ae1":"markdown","22b18264":"markdown","616666ca":"markdown","5aff0da5":"markdown","03c1f423":"markdown","23dc8189":"markdown","17288de8":"markdown","c54aa1d1":"markdown","3d84368f":"markdown","7273efe5":"markdown","078a802c":"markdown","3768835f":"markdown","f397454e":"markdown"},"source":{"0a8840f2":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport missingno as msno\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV","b5c175b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cf906a95":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\n# train_df = pd.read_csv('train.csv')\n# test_df = pd.read_csv('test.csv')","5f1bb182":"# preview the data\ntrain_df.head()","05d55860":"train_df.tail()","4b39c2bc":"# preview test data\ntest_df.head()","ce24075d":"print(train_df.shape,test_df.shape)","e6c180c0":"# check null values in train dataset\nnull_cols = train_df.isnull().mean()*100\nnull_cols[null_cols>0]","311ecf90":"# check null values in test dataset\nnull_cols = test_df.isnull().mean()*100\nnull_cols[null_cols>0]","7d6b5c96":"# check for data type\ntrain_df.info()\nprint('_'*40)\ntest_df.info()","c8ca04fa":"train_df.describe()","96c3d0d0":"train_df.describe(include='O')","80c1a9ed":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","069ccbaf":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4af648c0":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","641d3602":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","3b73d783":"# check null values in train dataset\nnull_cols = train_df.isnull().mean()*100\nnull_cols[null_cols>0]","01cbe310":"# check null values in train dataset\nnull_cols = test_df.isnull().mean()*100\nnull_cols[null_cols>0]","3b6c219b":"import warnings\nwarnings.filterwarnings('ignore')\n\ncombine = [train_df, test_df]\nfor dataset in combine:\n    dataset['isCabin'] = (dataset['Cabin'].notnull().astype('int'))\n\ntrain_df.head()","6f329449":"test_df.head()","df06304c":"train_df.drop(['Cabin'], axis=1, inplace=True)\ntest_df.drop(['Cabin'], axis=1, inplace=True)","a7b000a4":"train_df['Embarked'].describe()","6fe77877":"combine = [train_df, test_df]\nfor dataset in combine:\n    most_freq = dataset['Embarked'].mode()[0]\n    dataset['Embarked'].fillna(most_freq, inplace = True)\n\ntrain_df['Embarked'].isnull().sum()","67a4c4b5":"# merge train and test data\nmerged_df =  pd.concat([train_df, test_df], ignore_index=True)","f11e9c73":"# extract saluation from name\nmerged_df['Title'] = merged_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\nmerged_df['Title'].value_counts(normalize=True)*100","25d05215":"# most freqent titles\nfrequent_titles = merged_df['Title'].value_counts(normalize=True)*100\nfrequent_titles = frequent_titles[frequent_titles>1].index.tolist()\nfrequent_titles","e8604d68":"# Tagging all minor titles as 'Other'\nmerged_df['Title'] = merged_df['Title'].apply(lambda x: x if x in frequent_titles else 'Other')\n\nmerged_df.Title.value_counts()","ed40818c":"# fill missing age with median age group for each title\nmedian_ages = {}\n# calculate median age for different titles\nfor title in frequent_titles:\n    median_ages[title] = merged_df.loc[merged_df['Title'] == title]['Age'].median()\n\nmedian_ages['Other'] = merged_df['Age'].median()\nmedian_ages","333a2ee2":"# lets map median age to corresponding title\nimpute_age = merged_df[merged_df['Age'].isnull()]['Title'].map(median_ages)\nmerged_df['Age'].fillna(impute_age, inplace = True)","79b3ad47":"# check null values in train dataset\nnull_cols = merged_df.isnull().mean()*100\nnull_cols[null_cols>0]","a8fd6c92":"# #complete missing fare with median\n\nmedian_fare = merged_df['Fare'].median()\nmerged_df['Fare'].fillna(median_fare, inplace = True)    \nmerged_df['Fare'].isnull().sum()","6ace3795":"merged_df['Title'].value_counts()","7bb76382":"from sklearn.preprocessing import LabelEncoder\n\ncol_list = ['Sex', 'Embarked', 'Title']\nfor feature in col_list:\n    label = LabelEncoder()\n    merged_df[feature] = label.fit_transform(merged_df[feature])\n\nmerged_df[col_list].head()","90928789":"merged_df.Title.describe()","b53e6061":"num_Features = ['Age', 'Fare']\nnum_bins = 5\nfor feature in num_Features:\n    bin_feature = feature + '_Bin'\n    merged_df[bin_feature] = pd.qcut(merged_df[feature], num_bins)\n    label = LabelEncoder()\n    merged_df[bin_feature] = label.fit_transform(merged_df[bin_feature])\nmerged_df.head(10)","7759f5e4":"# train_df.Ticket.duplicated().sum()\nmerged_df.Ticket.duplicated().sum()","177b1de8":"# tagging for family\nmerged_df['Surname'] = merged_df.Name.str.extract(r'([A-Za-z]+),', expand=False) # Extracting Surname from name\nmerged_df['Surname_Ticket']  = merged_df['Surname'] +\"_\"+ merged_df['Ticket'] # Joining Surname with Ticket\n\nmerged_df['IsFamily'] = merged_df.Surname_Ticket.duplicated(keep=False).astype(int) # if surname in a ticket are same then they are belong to same family","88ea5c65":"# check family with Child\nmerged_df['Child'] = merged_df.Age.map(lambda x: 1 if x <=16 else 0) # create column based on age\nFamilyWithChild = merged_df[(merged_df.IsFamily==1)&(merged_df.Child==1)]['Surname_Ticket'].unique()","10306d80":"# assigning family ID for family with child\nmerged_df['FamilyId'] = 0\nfor ind, identifier in enumerate(FamilyWithChild):\n    merged_df.loc[merged_df.Surname_Ticket==identifier, ['FamilyId']] = ind + 1","32a1a95f":"merged_df['FamilySurvival'] = 1 \nSurvived_by_FamilyId = merged_df.groupby('FamilyId').Survived.sum()\nfor i in range(1, len(FamilyWithChild)+1):\n    if Survived_by_FamilyId[i] >= 1:\n        merged_df.loc[merged_df.FamilyId==i, ['FamilySurvival']] = 2\n    elif Survived_by_FamilyId[i] == 0:\n        merged_df.loc[merged_df.FamilyId==i, ['FamilySurvival']] = 0\n    \nmerged_df.head(10)","e8e366da":"train = merged_df[:len(train_df)].copy()\ntest = merged_df[len(train_df):].copy()\n\nprint(train_df.shape, test_df.shape)","b4baccf0":"sel_features = ['Sex', 'FamilySurvival', 'Fare_Bin', 'Pclass', 'Title']\n\nX_train = train[sel_features]\ny_train = train['Survived']\nX_test = test[sel_features]","9963beda":"from sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn import svm, neighbors","479498cb":"ensemble = [(\"rf\", RandomForestClassifier()),\n    (\"NuSVC\", svm.NuSVC(probability=True)),\n    (\"knn\", neighbors.KNeighborsClassifier()),]\n\nvoting = VotingClassifier(estimators= ensemble, voting='hard')","fc873c0c":"cv_results = cross_validate(voting, X_train, y_train, cv=5)\nprint(cv_results['test_score'].mean())\n\nvoting.fit(X_train, y_train)\npredictions = voting.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions.astype(int)})\noutput.to_csv('my_submission.csv', index=False)\n\nprint(\"Your submission was successfully saved!\")","8e066220":"## Analyze by pivoting features\n\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n\n- **Pclass** We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\n- **Sex** We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\n- **SibSp and Parch** These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).","d884de76":"We could see that most of the passenger have embarked from `S`\n<br>\nThe % missing value is only 0.22%. We decide to impute these missing values with `Mode`.","41885ff9":"## Wrangle data\n\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.","65109658":"### Creating New Features","9ec27ab7":"- It seems that `77%` passenger did not have cabin. So we will create feature `isCabin` and drop this columns","e3241d1f":"it seems there are multiple passengers in a single ticket. That mean there could be family travelling with same ticket. \n<br>\nSo we shall do family tagging then find familes with children as survival rate of children were high","77e90173":"### Missing value handing:","95f9468d":"#### Numerical\nFor numerical variables we shall create bins then we shall encode bins using LabelEncoder","a1e0e158":"#### Cabin","4917777c":"#### Categorical","39528ae1":"### Model Training","22b18264":"#### Embarked","616666ca":"# <font color=blue> Titanic - Ensemble Approach <font>\n\n\n## Background:\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\n## Data Dictionary:\nThe data has already been splited into two groups:\n- training set (train.csv)\n- test set (test.csv)\n\n**Features details:**\n\n| Variable | Definition | Key |\n| :- | :- | :-: |\n| survival | Survival | 0 = No; 1 = Yes |\n| class | Passenger Class | 1 = 1st; 2 = 2nd; 3 = 3rd\n| name | Name\n| sex | Sex\n| age | Age\n| sibsp | Number of Siblings\/Spouses aboard\n| parch | Number of Parents\/Children aboard\n| ticket | Ticket Number\n| fare | Passenger Fare\n| cabin | Cabin\n| embarked | Port of Embarkation | C = Cherbourg; Q = Queenstown; S = Southampton\n\n## Workflow stages\n\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\n\n1. Question or problem definition.\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Visualize, report, and present the problem solving steps and final solution.\n7. Supply or submit the results.\n\n","5aff0da5":"## Environment setup","03c1f423":"Although Age is continous feature but imputing them only by media or mean shall not be correct.\n<br>\nThe closer gues for age can be of `Title`. e.g master, Mr, Miss, Mrs etc\n<br>\nLest See what are the title we have by extracting from the name. For doing this we shall merge train and test datset. and later we shall separate them","23dc8189":"#### Age","17288de8":"## Reading and Understanding Data\n\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.","c54aa1d1":"### Separate train and test dataset","3d84368f":"### LabelEncoding","7273efe5":"The null is merged data: Servived column is due to test data, and fare we will be treating it next","078a802c":"#### Fare","3768835f":"## References\n\nThis notebook has been created based on the great works done in solving the Titanic competition and other sources.\n\n- [A journey through Titanic](https:\/\/www.kaggle.com\/omarelgabry\/titanic\/a-journey-through-titanic)\n- [Titanic Best Working Classifier](https:\/\/www.kaggle.com\/sinakhorami\/titanic\/titanic-best-working-classifier)\n\n\nYou may refer to my other aproaches whion the same dataset\n- [Titanic - Model Comparisons Top 13% Leaders Board](https:\/\/www.kaggle.com\/santhraul\/titanic-model-comparisons-top-13-leaders-board)\n- [Titanic - Machine Learning from Disaster](https:\/\/www.kaggle.com\/santhraul\/titanic-machine-learning-from-disaster)\n- [Pandas 100 tricks](https:\/\/www.kaggle.com\/santhraul\/pandas-100-tricks)","f397454e":"Missing value columns in train data are : `Age`, `Cabin` and `Emabrked`\n<br>\nMissing value columns in test date are: `Age`, `Cabin` and `Fare`"}}