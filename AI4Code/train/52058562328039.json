{"cell_type":{"d4bee4de":"code","ef0ee2ec":"code","7648de38":"code","2b74fbec":"code","3dc34747":"code","ddb3df4d":"code","14c026fd":"code","5ae57454":"code","9c74aba3":"markdown"},"source":{"d4bee4de":"import pandas as pd\nimport numpy as np\n\nfrom catboost import CatBoostClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score","ef0ee2ec":"train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv', index_col=0)\ntest = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv', index_col=0)\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')","7648de38":"# Inpiration: https:\/\/www.kaggle.com\/bagusbpg\/my-12th-notebook\n\ntrain['AnyMissing'] = np.where(train.isnull().any(axis=1) == True, 1, 0)\ntest['AnyMissing'] = np.where(test.isnull().any(axis=1) == True, 1, 0)\n\nage_mean_input = train['Age'].mean()\ntrain['Age'].fillna(age_mean_input,inplace = True)\ntrain['Age_Pclass'] = train['Age'] * train['Pclass']\ntrain['Age'] = train['Age'].apply(lambda x: '80s' if x >= 80 else '70s' if x>=70 else '60s' if x>=60 else '50s' if x>=50 else '40s' if x>=40 else '30s' if x>=30 else '20s' if x>=20 else '10s' if x>=10 else '0s')\ntest['Age'].fillna(age_mean_input,inplace = True)\ntest['Age'] = test['Age'].apply(lambda x: '80s' if x >= 80 else '70s' if x>=70 else '60s' if x>=60 else '50s' if x>=50 else '40s' if x>=40 else '30s' if x>=30 else '20s' if x>=20 else '10s' if x>=10 else '0s')\n\ntrain['FamName'] = train['Name'].str.extract('([A-Za-z]+)\\,', expand = False)\ntest['FamName'] = test['Name'].str.extract('([A-Za-z]+)\\,', expand = False)\n\nFamName = train['FamName'].append(test['FamName']).value_counts()\nFamName = FamName.apply(lambda x: 'UltraCommon' if x >= 512 else 'VeryCommon' if x >= 256 else 'ModeratelyCommon' if x >= 128 else 'Common' if x >= 64 else 'SlightlyCommon' if x >= 32 else 'SlightlyRare' if x >= 16 else 'Rare' if x >= 8 else 'ModeratelyRare' if x >= 4 else 'VeryRare' if x >= 2 else 'UltraRare')\ntrain['FamName'] = train['FamName'].apply(lambda x: FamName[x])\ntest['FamName'] = test['FamName'].apply(lambda x: FamName[x])\n\ntrain['FamSize'] = train['SibSp'] + train['Parch'] + 1\ntrain['FamSize'] = train['FamSize'].apply(lambda x: 'VeryBig' if x >= 12 else 'Big' if x >= 8 else 'Medium' if x >= 5 else 'Small' if x >= 3 else 'Couple' if x ==2 else 'Alone')\ntest['FamSize'] = test['SibSp'] + test['Parch'] + 1\ntest['FamSize'] = test['FamSize'].apply(lambda x: 'VeryBig' if x >= 12 else 'Big' if x >= 8 else 'Medium' if x >= 5 else 'Small' if x >= 3 else 'Couple' if x ==2 else 'Alone')\n\ntrain['Fare'].fillna(train['Fare'].mean(),inplace = True)\ntrain['Fare'] = train['Fare'].apply(lambda x: 'CrazyRich' if x >= 640 else 'UltraRich' if x >= 320 else 'VeryRich' if x >= 160 else 'Rich' if x >= 80 else 'SlightlyRich' if x >= 40 else 'SlightlyPoor' if x >= 20 else 'Poor' if x >= 10 else 'VeryPoor' if x >= 5 else 'UltraPoor')\ntest['Fare'].fillna(test['Fare'].mean(),inplace = True)\ntest['Fare'] = test['Fare'].apply(lambda x: 'CrazyRich' if x >= 640 else 'UltraRich' if x >= 320 else 'VeryRich' if x >= 160 else 'Rich' if x >= 80 else 'SlightlyRich' if x >= 40 else 'SlightlyPoor' if x >= 20 else 'Poor' if x >= 10 else 'VeryPoor' if x >= 5 else 'UltraPoor')\n\ntrain['FirstName'] = train['Name'].map(lambda x: x.split(',')[0]).str.strip()\ntrain['SecondName'] = train['Name'].map(lambda x: x.split(',')[1])\n\ntest['FirstName'] = test['Name'].map(lambda x: x.split(',')[0]).str.strip()\ntest['SecondName'] = test['Name'].map(lambda x: x.split(',')[1])\n","2b74fbec":"def initial_prep(data):\n    \n    # pipe: \n    # inpute lm AGE~Pclass ???\n    # input Fare~Pclass ???\n    # Embarked need input ???\n    # create NameCount ???\n    \n    #data['CabinNum'] = pd.to_numeric(data['Cabin'].fillna('X').map(lambda x: x[1:].strip()))\n    data['Cabin'] = data['Cabin'].fillna('X').map(lambda x: x[0].strip())\n    \n    #data['TicketNum'] = data.Ticket.str.extract(r'(\\d+)').astype('float64', copy=False)\n    data['Ticket'] = data.Ticket.str.replace('\\.','', regex=True).str.replace('(\\d+)', '', regex=True).str.replace(' ', '', regex=True).replace(r'^\\s*$', 'X', regex=True).fillna('X')\n    \n    data['Embarked'] = data.Embarked.fillna('X')\n    \n    for col in data.columns[data.dtypes==\"object\"].tolist():\n        data.loc[:,col] = data.loc[:,col].astype('category')\n    \n    return data","3dc34747":"lab_cols = ['Age','FirstName', 'Ticket', 'Fare', 'Pclass', 'Sex', 'Cabin', 'Embarked','FamSize', 'FamName', 'AnyMissing']\nnum_cols = ['SibSp', 'Parch']\ntarget = 'Survived'\n\ntrain = initial_prep(train)\n\nX = train[lab_cols + num_cols]\ny = train[target]\n\ntest = test.pipe(initial_prep)[lab_cols + num_cols]","ddb3df4d":"def kfold_prediction(X, y, X_test, K):\n\n    yp = np.zeros(len(X_test))\n    \n    kf = KFold(n_splits=K, shuffle=True, random_state=314)\n    \n    for i, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n        print(f\"\\n FOLD {i} ...\")\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        params = {'loss_function':'Logloss',\n                  'eval_metric':'AUC', \n                  'early_stopping_rounds': 500,\n                  'n_estimators': 10000,\n                  'cat_features': lab_cols,\n                  'verbose': 500,\n                  'random_seed': 314\n         }\n        \n        clf = CatBoostClassifier(**params)\n        \n        model_fit = clf.fit(X_train,y_train,\n                            eval_set=[(X_train, y_train), (X_val, y_val)],\n                            use_best_model=True,\n                            plot=False)\n        \n        yp += model_fit.predict_proba(X_test)[:, 1] \/ K\n        \n        \n        yp_val = np.zeros(len(X_val))\n        yp_val += model_fit.predict_proba(X_val)[:, 1]\n        acc = accuracy_score(y_val, np.where(yp_val>=0.5, 1, 0))\n        print(f\"\\n Accuracy: {acc} !\")\n        \n    \n    return yp","14c026fd":"submission.loc[:, 'Survived'] = kfold_prediction(X, y, test, 8)","5ae57454":"submission.loc[:, 'Survived'] = np.where(submission['Survived']>=0.5, 1, 0)\n\nsubmission.to_csv('submission.csv', index = False)","9c74aba3":"# Problem definition\n\nThe dataset is used for this competition is synthetic but based on a real dataset (in this case, the actual Titanic data!) and generated using a CTGAN.\n\nData description: \n\n| Variable        | Definition           | Key  |\n|---------------|:-------------|------:|\n|survival |\tSurvival | 0 = No, 1 = Yes |\n|pclass |\tTicket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n|sex |\tSex\t ||\n|Age |\tAge in years\t ||\n|sibsp |\t# of siblings \/ spouses aboard the Titanic\t ||\n|parch |\t# of parents \/ children aboard the Titanic\t ||\n|ticket |\tTicket number\t ||\n|fare |\tPassenger fare\t ||\n|cabin |\tCabin number\t| |\n|embarked |\tPort of Embarkation\t| C = Cherbourg, Q = Queenstown, S = Southampton |\n\n<br>\n\nWhere `survival` will be our target variable! \ud83c\udfaf\n\n<br>\n\nCheck out: \n\n  \u279c [Tuning of a Lightgbm with Bayesian Optimization using the `tidymodels` framework in R](https:\/\/www.kaggle.com\/gomes555\/tps-apr2021-r-eda-lightgbm-bayesopt)\n\n  \u279c [AutoML (lgbm + catboost) with mljar](https:\/\/www.kaggle.com\/gomes555\/tps-apr2021-autoboost-mljar)\n<br>\n\n<p align=\"right\"><span style=\"color:firebrick\">Dont forget the upvote if you liked the notebook! \u270c\ufe0f <\/p>"}}