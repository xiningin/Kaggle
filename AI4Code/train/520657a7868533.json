{"cell_type":{"8795315b":"code","1aca352b":"code","5c35fefe":"code","8b6ec79e":"code","b1b38395":"code","6b1af668":"code","88f667f3":"code","001d452c":"code","be61e21f":"code","e4af1b2b":"code","dd121ff9":"code","f7dcdc5f":"markdown"},"source":{"8795315b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1aca352b":"# \u5fc5\u8981\u305d\u3046\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u8aad\u307f\u8fbc\u307f\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import (\n    LinearRegression,\n    Ridge,\n    Lasso\n)\n%matplotlib inline","5c35fefe":"# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\n# \u8a13\u7df4\u30c7\u30fc\u30bf\ntrain_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\ntest_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv') \n\n# \u4e00\u62ec\u3067\u6b20\u640d\u5024\u4fdd\u7ba1\u3059\u308b\u305f\u3081\u4e00\u65e6\u30de\u30fc\u30b8\n# \u305f\u3060\u3057\u3001\u3042\u3068\u3067\u5206\u5272\u3059\u308b\u305f\u3081WhatIsData\u306b\u30bf\u30b0\u6b8b\u3057\ntrain_data['WhatIsData'] = 'Train'\ntest_data['WhatIsData'] = 'Test'\n\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306bSalePrice\u306f\u5b58\u5728\u3057\u306a\u305f\u3081\u3001\u30c0\u30df\u30fc\u30c7\u30fc\u30bf\u633f\u5165\ntest_data['SalePrice'] = 9999999999\n\n# \u30de\u30fc\u30b8\u51e6\u7406\nalldata = pd.concat([train_data,test_data],axis=0).reset_index(drop=True)\n\nprint('The size of train_data is : ' + str(train_data.shape))\nprint('The size of test_data is : ' + str(test_data.shape))\nprint('The size of alldata is : ' + str(alldata.shape))","8b6ec79e":"# \u6b20\u640d\u3092\u542b\u3080\u30ab\u30e9\u30e0\u3092\u30ea\u30b9\u30c8\u5316\nna_col_list = alldata.isnull().sum()[alldata.isnull().sum()>0].index.tolist()","b1b38395":"# \u30c7\u30fc\u30bf\u578b\u306b\u5fdc\u3058\u3066\u6b20\u640d\u5024\u3092\u88dc\u5b8c\u3059\u308b\n\n# float\u306e\u5834\u5408\u306f0\n# object\u306e\u5834\u5408\u306f'NA'\nna_float_cols = alldata[na_col_list].dtypes[alldata[na_col_list].dtypes=='float64'].index.tolist() #float64\nna_obj_cols = alldata[na_col_list].dtypes[alldata[na_col_list].dtypes=='object'].index.tolist() #object\n\n# float64\u578b\u3067\u6b20\u640d\u3057\u3066\u3044\u308b\u5834\u5408\u306f0\u3092\u4ee3\u5165\nfor na_float_col in na_float_cols:\n    alldata.loc[alldata[na_float_col].isnull(),na_float_col] = 0.0\n\n# object\u578b\u3067\u6b20\u640d\u3057\u3066\u3044\u308b\u5834\u5408\u306f'NA'\u3092\u4ee3\u5165\nfor na_obj_col in na_obj_cols:\n    alldata.loc[alldata[na_obj_col].isnull(),na_obj_col] = 'NA'","6b1af668":"# \u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u306e\u7279\u5fb4\u91cf\u3092\u30ea\u30b9\u30c8\u5316\ncat_cols = alldata.dtypes[alldata.dtypes=='object'].index.tolist()\n# \u6570\u5024\u5909\u6570\u306e\u7279\u5fb4\u91cf\u3092\u30ea\u30b9\u30c8\u5316\nnum_cols = alldata.dtypes[alldata.dtypes!='object'].index.tolist()\n# \u30c7\u30fc\u30bf\u5206\u5272\u304a\u3088\u3073\u63d0\u51fa\u6642\u306b\u5fc5\u8981\u306a\u30ab\u30e9\u30e0\u3092\u30ea\u30b9\u30c8\u5316\nother_cols = ['Id','WhatIsData']\n# \u4f59\u8a08\u306a\u8981\u7d20\u3092\u30ea\u30b9\u30c8\u304b\u3089\u524a\u9664\ncat_cols.remove('WhatIsData') #\u5b66\u7fd2\u30c7\u30fc\u30bf\u30fb\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u533a\u5225\u30d5\u30e9\u30b0\u9664\u53bb\nnum_cols.remove('Id') #Id\u524a\u9664\n\n# \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306b\u5bfe\u3057\u3066Label Encoding\u306e\u5b9f\u65bd\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nencoded_data = alldata[cat_cols].apply(le.fit_transform)\n\n# \u30c7\u30fc\u30bf\u7d71\u5408\nall_data = pd.concat([alldata[other_cols],alldata[num_cols],encoded_data],axis=1)\nprint(all_data)","88f667f3":"# \u30de\u30fc\u30b8\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u518d\u5206\u5272\ntrain_ = all_data[alldata['WhatIsData']=='Train'].drop(['WhatIsData','Id'], axis=1).reset_index(drop=True)\ntest_ = all_data[alldata['WhatIsData']=='Test'].drop(['WhatIsData','SalePrice'], axis=1).reset_index(drop=True)\n\n\n# \u5b66\u7fd2\u30c7\u30fc\u30bf\u5185\u306e\u5206\u5272\ntrain_x = train_.drop('SalePrice',axis=1)\ntrain_y_ = train_['SalePrice']\ntrain_y = np.log(train_['SalePrice'])\n\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u5185\u306e\u5206\u5272\ntest_id = test_['Id']\ntest_x= test_.drop('Id',axis=1)","001d452c":"# \u30c7\u30fc\u30bf\u306e\u5206\u5272\ntrain_x, valid_x, train_y, valid_y = train_test_split(\n        train_x,\n        train_y,\n        test_size=0.3,\n        random_state=0,\n        )","be61e21f":"import lightgbm as lgb\n\nlgb_train = lgb.Dataset(train_x, train_y,\n                       categorical_feature=cat_cols)\nlgb_eval = lgb.Dataset(valid_x, valid_y,\n                       reference=lgb_train,\n                       categorical_feature=cat_cols)\n\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'l2'},\n    'num_leaves': 128,\n    'learning_rate': 0.01,\n    'feature_fraction': 0.38,\n    'bagging_fraction': 0.68,\n    'bagging_freq': 5,\n    'verbose': 0\n}\n\nmodel = lgb.train(params,\n                  lgb_train,\n                  valid_sets=[lgb_train, lgb_eval],\n                 verbose_eval=10,\n                 num_boost_round=5000,\n                 early_stopping_rounds=1000,)\n","e4af1b2b":"y_pred = model.predict(test_x, \n                      num_iteration=model.best_iteration)","dd121ff9":"# \u7d50\u679c\u306e\u51fa\u529b\n\nmy_submission = pd.DataFrame()\nmy_submission[\"Id\"] = test_id\nmy_submission[\"SalePrice\"] = np.exp(model.predict(test_x))\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","f7dcdc5f":"![image.png](attachment:image.png)![](http:\/\/)"}}