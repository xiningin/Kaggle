{"cell_type":{"db2ddc78":"code","f1971935":"code","226913d2":"code","9ae437a6":"code","00e762ac":"code","0e6b1965":"code","b45b348e":"code","1c58a264":"code","f8df9870":"code","8cabc8b0":"code","db4bd68f":"code","de2b30f9":"code","2b1cef54":"code","d4d9c6eb":"code","d6d37454":"code","e883868b":"code","db0cbe11":"code","fb01ae3e":"code","9f26e3c6":"code","a472f901":"code","0cf62a45":"code","5639fa78":"code","362693da":"code","bfac7685":"code","0227ed0d":"code","851aee44":"code","cd31567b":"code","6fa2d5e2":"code","52e36caa":"code","bc08da9a":"code","3d291ba9":"code","aa1dbe5e":"code","f739d1a9":"code","e3f84f5d":"code","111a86c1":"code","589e5621":"code","7ec89abb":"code","4e79ce99":"code","fd0c5af5":"code","e4e1b7b0":"code","42164951":"code","b9fb0bf5":"markdown","06d4aac0":"markdown","82f985b5":"markdown","b8f6bbc3":"markdown","f9db25d1":"markdown","c7fb5853":"markdown","9a2204bf":"markdown","d050f767":"markdown","21de7e12":"markdown","6ec3c030":"markdown","c9abaef9":"markdown","fdbb10b6":"markdown","efd2d6bb":"markdown","cece0c1f":"markdown","a5eefe80":"markdown","747e2e55":"markdown","b84b2f32":"markdown","21f635a6":"markdown","4aab6e86":"markdown","caf427a2":"markdown","00c287af":"markdown","e886190b":"markdown","1915895d":"markdown","b7bcbb15":"markdown"},"source":{"db2ddc78":"# Import the required library \n\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport os \nimport zipfile\nimport glob \n\nimport tensorflow as tf \nimport tensorflow_addons as tfa\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers \nfrom tensorflow.keras.models import Model,load_model\nfrom keras.callbacks import Callback\nimport ml_metrics\n\nfrom sklearn import preprocessing \n\n%matplotlib inline \nplt.style.use('fivethirtyeight')\n\nimport warnings\nwarnings.filterwarnings('ignore')","f1971935":"\npath ='\/kaggle\/input\/mercedes-benz-greener-manufacturing'\n\nworking_path='\/kaggle\/working'\n\n\nif (os.getcwd()!=path):\n    os.chdir(path)\n    \n#Uzip the data \n\nfor file in glob.glob('*.zip'):\n    with zipfile.ZipFile(os.path.join(path,file), 'r') as zip_ref:\n        zip_ref.extractall(working_path)\n\n\nos.chdir(working_path)\n\n# Import the dataset \n\n\ndf_train =pd.read_csv('.\/train.csv')\n\ndf_test=pd.read_csv('.\/test.csv')\n\ndf_submission=pd.read_csv('.\/sample_submission.csv')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 150)\n","226913d2":"df_train.head()","9ae437a6":"df_train.shape, df_test.shape","00e762ac":"df_dtypes=pd.DataFrame({'col':df_train.columns,'dtypes':df_train.dtypes}).reset_index(drop=True)\nnp.transpose(df_dtypes[:400])","0e6b1965":"# lets see the distribution of y variable \nplt.figure(figsize=(15,8))\nplt.subplot(1, 2, 1)\nplt.scatter(range(df_train.shape[0]), np.sort(df_train.y.values));\nplt.xlabel('index', fontsize=12);\nplt.ylabel('y', fontsize=12);\n\nplt.subplot(1,2,2)\nplt.hist(df_train.y,bins=15)\nplt.xlim(40,180)\nplt.xlabel('y', fontsize=12);\nplt.ylabel('counts', fontsize=12);","b45b348e":"df_train=df_train[df_train.y<=180]","1c58a264":"df_train.isnull().sum().sum()","f8df9870":"df_test.head()","8cabc8b0":"df_test.isnull().sum().sum()","db4bd68f":"df_train['test']=0\ndf_test['test']=1\n\ndata=pd.concat([df_train,df_test],axis=0)\n\ndata.shape","de2b30f9":"# Lets seperate the int features \n\ninterger_columns=[]\n\nfor col in data.columns:\n    if col not in ['X0','X1','X2','X3','X4','X5','X6','X8']:\n        interger_columns.append(col)\n        \n","2b1cef54":"data=data[interger_columns]\n\ndata.head()","d4d9c6eb":"data.dtypes","d6d37454":"col=[c for c in data.columns if c not in ['y','test','ID']]\n\n# Feature engineering \n\ndata['sum']=data[col].sum(axis=1)\ndata['mean']=data[col].mean(axis=1)\ndata['median']=data[col].median(axis=1)\ndata['skew']=data[col].skew(axis=1)\ndata['kurt']=data[col].kurtosis(axis=1)\ndata['mode']=data[col].mode(axis=1)\n\ndata.head()","e883868b":"# Lets split the data back  into test and train set \n\ntrain =data[data['test']!=1]\ntest =data[data['test']==1]\n\n\ntest.shape, train.shape","db0cbe11":"from sklearn import preprocessing\n\nX=train.drop(columns=['ID','y'],axis=1)\ny=train['y']\n\n\nfrom sklearn import model_selection\nX_train,X_val,y_train,y_val=model_selection.train_test_split(X,y,shuffle=True,test_size=0.1,random_state=101)","fb01ae3e":"# Scale the Continous Variable \n\nnormalize_col=['sum','skew','kurt']\n\nfor col in normalize_col:\n    scaler=preprocessing.MinMaxScaler()\n    scaler.fit(X_train[col].values.reshape(-1,1))\n    X_train.loc[:,col]=scaler.transform(X_train[col].values.reshape(-1,1))\n    X_val.loc[:,col]=scaler.transform(X_val[col].values.reshape(-1,1))\n    test.loc[:,col]=scaler.transform(test[col].values.reshape(-1,1))\n    \n# Scaling Response variable \n\ny_scaler=preprocessing.MinMaxScaler()\ny_scaler.fit(y_train.values.reshape(-1,1))\ny_train=y_scaler.transform(y_train.values.reshape(-1,1))\ny_val=y_scaler.transform(y_val.values.reshape(-1,1))","9f26e3c6":"\nfrom sklearn import linear_model\nreg = linear_model.RidgeCV(alphas=(0.001,0.01,0.1,0.3,0.003,1,5))\n\nreg.fit(X_train,y_train)\npred_reg=reg.predict(X_val)\n\n\nfrom sklearn import metrics \n\n\n#Actually not the right way to score but just wanted to use max data in RidgeCV model for traning . Completely leaving out val set from training gives R2 score ~0.59\nprint('The R2 score for Ridge Regression is {}'.format(metrics.r2_score(y_val,pred_reg)))","a472f901":"lasso=linear_model.Lasso(alpha=0.001,random_state=101)\n\nlasso.fit(X_train,y_train)\n\npred_lasso=lasso.predict(X_val)\n\nprint('The R2 score for Lasso Regression is {}'.format(metrics.r2_score(y_val,pred_lasso)))","0cf62a45":"br=linear_model.BayesianRidge()\n\nbr.fit(X_train,y_train)\n\npred_bayesian=br.predict(X_val)\n\nprint('The R2 score for Bayesian Regression is {}'.format(metrics.r2_score(y_val,pred_bayesian)))","5639fa78":"#from sklearn.model_selection import RandomizedSearchCV\n#from sklearn import ensemble \n\n#params = {\n\n#'n_estimators': [50,100,150,200,250],\n\n## Number of features to consider at every split\n#'max_features' : ['auto', 'sqrt'],\n\n## Maximum number of levels in tree\n#'max_depth' :[5,10,15,20,25],\n\n## Minimum number of samples required to split a node\n#'min_samples_split' : [2, 5, 10],\n\n## Minimum number of samples required at each leaf node\n#'min_samples_leaf': [1, 2, 4],\n\n# Method of selecting samples for training each tree\n#'bootstrap' : [True, False],\n    \n#'criterion':['mse', 'mae']\n    \n#}\n\n#from sklearn.metrics import r2_score, make_scorer\n#r2_scorer = make_scorer(r2_score)\n\n\n#rf = ensemble.RandomForestRegressor()\n\n#folds = 5\n#param_comb = 20\n\n#kfold = model_selection.KFold(n_splits=folds, shuffle = True, random_state = 101)\n\n#random_search = RandomizedSearchCV(rf, param_distributions=params, n_iter=param_comb, scoring=r2_scorer, n_jobs=1, cv=kfold.split(X,y), verbose=5, random_state=101,refit=True )\n\n#random_search.fit(X, y)\n\n#print(\"The best score is {}\".format(random_search.best_score_ ))\n\n#print('\/n')\n\n#print ('The best paramerts are {}'.format(random_search.best_estimator_))","362693da":"# From running the random search CV above  we get the following values \n\nfrom sklearn import ensemble\n\nrf = ensemble.RandomForestRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=4,n_estimators=50,random_state=101)\nrf.fit(X_train,y_train)\n\nrf_predict=rf.predict(X_val)\nmetrics.r2_score(y_val,rf_predict)","bfac7685":"\n# Result Prediction with Lasso and Ridge \n\n\ntest.drop('y',axis=1,inplace=True)\n\ntest_Ridge=reg.predict(test.drop('ID',axis=1))\ntest_Lasso=lasso.predict(test.drop('ID',axis=1))\ntest_Bayesian=br.predict(test.drop('ID',axis=1))\ntest_random_forest=rf.predict(test.drop('ID',axis=1))\n\ntest['Ridge_Prediction']=test_Ridge\ntest['Lasso_Prediction']=test_Lasso\ntest['Bayesian_Prediction']=test_Bayesian\ntest['Random_Prediction']=test_random_forest\n\ntest.loc[:,'y']=test[['Ridge_Prediction', 'Lasso_Prediction','Bayesian_Prediction','Random_Prediction']].mean(axis=1)","0227ed0d":"#Stacking all Validation dataset prediction into a dataframe \ndf_stacking=pd.DataFrame(np.column_stack([y_val,pred_lasso,pred_reg,pred_bayesian,rf_predict]),\n                         columns=['y','lasso','Ridge','Bayesian','rf'])\n\n#All X_Test set predictions using all the above 4 models this will be later multiplied with the weights of the stacking model to get the final model \ndf_stacking_test=pd.DataFrame(np.column_stack([test.ID,test_Lasso,test_Ridge,test_Bayesian,test_random_forest]),\n                              columns=['ID','lasso','Ridge','Bayesian','rf'])\n\nfor col in df_stacking.columns:\n    df_stacking.loc[:,col]=y_scaler.inverse_transform(df_stacking[col].values.reshape(-1,1))\n\nfor col in ['lasso','rf','Ridge','Bayesian']:\n    df_stacking_test.loc[:,col]=y_scaler.inverse_transform(df_stacking_test[col].values.reshape(-1,1))","851aee44":"lr_stack=linear_model.LinearRegression()\nlr_stack.fit(df_stacking[['lasso','rf','Ridge','Bayesian']].values,df_stacking['y'].values)\n\ndf_stacking_test.loc[:,'y']=lr_stack.predict(df_stacking_test[['lasso','rf','Ridge','Bayesian']])\ndf_stacking_test[['ID','y']].to_csv('\/kaggle\/working\/Stacking_Integer.csv',index=False)","cd31567b":"predictions=test.copy()","6fa2d5e2":"df_train['test']=0\ndf_test['test']=1\n\ndata=pd.concat([df_train,df_test],axis=0)\n\n\ncol=[c for c in data.columns if c not in ['y','test','ID','X0','X1','X2','X3','X4','X5','X6','X8']]\ndata['sum']=data[col].sum(axis=1)\ndata['mean']=data[col].mean(axis=1)\ndata['median']=data[col].median(axis=1)\ndata['skew']=data[col].skew(axis=1)\ndata['kurt']=data[col].kurtosis(axis=1)\ndata['mode']=data[col].mode(axis=1)\n\n\nfor col in ['X0','X1','X2','X3','X4','X5','X6','X8']:\n    lbl_XG=preprocessing.LabelEncoder()\n    data.loc[:,col]=lbl_XG.fit_transform(data[col].values.reshape(-1,1))\n    \n","52e36caa":"train=data[data.test!=1]\ntest=data[data.test==1]\n\ntrain.drop(columns='test',axis=1,inplace=True)\ntest.drop(columns='test',axis=1,inplace=True)\n\nX=train.drop(columns=['y','ID'],axis=1)\ny=train.y\n\nX_test=test.drop('y',axis=1)","bc08da9a":"\nfrom sklearn import model_selection\n\nX_train,X_val,y_train,y_val=model_selection.train_test_split(X,y,shuffle=True,random_state=101,test_size=0.1)\n\nnormalize_col=['sum','skew','kurt']\n\nfrom sklearn import preprocessing\n\n\nfor col in normalize_col:\n    scaler=preprocessing.MinMaxScaler()\n    scaler.fit(X_train[col].values.reshape(-1,1))\n    X_train.loc[:,col]=scaler.transform(X_train[col].values.reshape(-1,1))\n    X_val.loc[:,col]=scaler.transform(X_val[col].values.reshape(-1,1))\n    X_test.loc[:,col]=scaler.transform(X_test[col].values.reshape(-1,1))\n    \n    \n# Scaling Response variable \ny_scaler=preprocessing.MinMaxScaler()\ny_scaler.fit(y_train.values.reshape(-1,1))\ny_train=y_scaler.transform(y_train.values.reshape(-1,1))\ny_val=y_scaler.transform(y_val.values.reshape(-1,1))","3d291ba9":"import xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV","aa1dbe5e":"#y_db=np.concatenate((y_train,y_val))\n#X_db=pd.concat([X_train,X_val])\n\n\n\n#params = {\n#        'learning_rate':[0.01,0.1,1],\n#        'n_estimators':[50,100,150,200,250],\n#        'min_child_weight': [1, 5, 10],\n#        'gamma': [0.5, 1, 1.5, 2, 5],\n#        'subsample': [0.6, 0.8, 1.0],\n#        'colsample_bytree': [0.6, 0.8, 1.0],\n#        'max_depth': [5,10,15],\n#        'reg_lambda':[0.5,1]\n#        }\n\n#xgb = xgb.XGBRegressor(objective ='reg:squarederror',\\\n#                    silent=False, nthread=1)\n\n#folds = 5\n#param_comb = 20\n\n#from sklearn.metrics import r2_score, make_scorer\n#r2_scorer = make_scorer(r2_score)\n\n\n#kfold = model_selection.KFold(n_splits=folds, shuffle = True, random_state = 101)\n\n#random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring=r2_scorer, n_jobs=1, cv=kfold.split(X_db,y_db), verbose=5, random_state=101,refit=True )\n\n#random_search.fit(X_db, y_db)\n\n\n#print(\"The best score is {}\".format(random_search.best_score_ ))\n\n#print('\/n')\n\n#print ('The best paramerts are {}'.format(random_search.best_estimator_))","f739d1a9":"#From a previous Random Search CV run \n\nxgb=xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=50,learning_rate=1,min_child_weight=5,gamma=2,\n                     colsample_by_tree=0.6,max_depth=15,reg_lambda=0.75,subsample=1)\n\nxgb.fit(X_train,y_train)\n\n\nX_test['XGB_predict']=xgb.predict(X_test.drop(\"ID\",axis=1))\n\npredictions['XGB_predictions']=X_test['XGB_predict']\n\nX_test['XGB_predict']=y_scaler.inverse_transform(X_test['XGB_predict'].values.reshape(-1,1))\n\nX_test_final=X_test[['ID','XGB_predict']]\n\n\n# Private LB score for just XGboost 0.53881\nX_test_final.to_csv('\/kaggle\/working\/XG_boost_solution.csv',index=False)\n","e3f84f5d":"from sklearn import ensemble\n\nrfc = ensemble.RandomForestRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=4,n_estimators=50)\nrfc.fit(X_train,y_train)\n\nrfc_predict=rfc.predict(X_val)\n\nmetrics.r2_score(y_val,rfc_predict)","111a86c1":"predictions['Random_forest_entire']=rfc.predict(X_test.drop(columns=['ID','XGB_predict'],axis=1))","589e5621":"# Private LB score =0.54222\n\npredictions['y1']=predictions[['Ridge_Prediction','Lasso_Prediction','Random_Prediction','XGB_predictions','Bayesian_Prediction']].mean(axis=1)\npredictions['y1']=y_scaler.inverse_transform(predictions['y1'].values.reshape(-1,1))","7ec89abb":"# Private LB score =0.54324\n\npredictions['y2']=((0.40*predictions['XGB_predictions']+0.25*predictions['Lasso_Prediction']+0.25*predictions['Ridge_Prediction']+0.10*predictions['Random_Prediction']))\npredictions['y2']=y_scaler.inverse_transform(predictions['y2'].values.reshape(-1,1))\n","4e79ce99":"# Private LB score = 0.54453 Best Score of the notebook \npredictions['y3']=(0.4*y_scaler.inverse_transform(predictions['XGB_predictions'].values.reshape(-1,1))+ 0.6*df_stacking_test['y'].values.reshape(-1,1))","fd0c5af5":"# Private LB score =0.54357\n\npredictions['y4']=(0.25*y_scaler.inverse_transform(predictions['XGB_predictions'].values.reshape(-1,1))+ 0.5*df_stacking_test['y'].values.reshape(-1,1)+\\\n                    0.25*y_scaler.inverse_transform(predictions['Random_forest_entire'].values.reshape(-1,1)))","e4e1b7b0":"# Private LB score =0.54091\n\npredictions['y5']=(0.15*predictions['XGB_predictions']+0.30*predictions['Lasso_Prediction']+0.30*predictions['Ridge_Prediction']+0.10*predictions['Random_Prediction']\\\n                    +0.15*predictions['Random_forest_entire'])\n\npredictions['y5']=y_scaler.inverse_transform(predictions['y5'].values.reshape(-1,1))\n","42164951":"#predictions[['ID','y3']].to_csv('\/kaggle\/working\/Stacking_XGboost_Random_forest.csv',index=False)","b9fb0bf5":"# A parameter grid for XGBoost\n\n### uncomment and run the below cell to run the random search CV ","06d4aac0":"# Using a Simple Linear Regression as the Stacking model ","82f985b5":"# Lets correct the outlier point before we begin the model building ","b8f6bbc3":"# XGbost with Entire Dataset ","f9db25d1":"# Ridge Regression on Interger Features ","c7fb5853":"# XGBoost With Random Search CV ","9a2204bf":"# Useful Resource\/References \n\nhttps:\/\/medium.com\/@songxia.sophia\/two-machine-learning-algorithms-to-predict-xgboost-neural-network-with-entity-embedding-caac68717dea <br>\n\nhttps:\/\/towardsdatascience.com\/neural-network-embeddings-explained-4d028e6f0526<br>\n\nhttps:\/\/gdcoder.com\/entity-embeddings-of-categorical-variables-in-neural-networks\/<br>\n\nhttps:\/\/github.com\/WillKoehrsen\/wikipedia-data-science\/blob\/master\/notebooks\/Book%20Recommendation%20System.ipynb<br>\n\nXGboost tutorial :https:\/\/www.datacamp.com\/community\/tutorials\/xgboost-in-python<br>\n\nRandomSearchCV on XGboost : https:\/\/www.kaggle.com\/tilii7\/hyperparameter-grid-search-with-xgboost<br>","d050f767":"# Unzip and Import the dataset ","21de7e12":"So in this dataframe we have : \n\n* 8 object columns \n* y as float \n* x10 to x17 as int64 ","6ec3c030":"# The result of the above parameter grid is below ","c9abaef9":"***As a Mechanical Engineer working in working in an automotive domain I know how time consuming vechile testing can be. The process consists of building the prototype car, instrumenting it and then running the required tests . The major bottle neck in car testing occurs during instrumention phase which requires to de-assemble the car ,fit the required recording instruments and then re-assemble the car.*** <br>\n\n**Another bottle neck during testing is also the avaliablity of testing equipments such as drive cells required to run the test.** <br>\n\n**All this factors results in man-hours wasteage and a increased development time in the vechile development program. This adds and over-head cost to the company.** <br>\n\n**The Mercedes-Benz greeener manufacturing challenge on Kaggle provides one such case .In this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing***<br>","fdbb10b6":"From EDA of various notebooks we know that all int variables are between 0 and 1 <br>\n\nLets combine test and train datasets for feature engineering <br>","efd2d6bb":"# Bayesian Regression on Integer Features ","cece0c1f":"![](https:\/\/images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com\/f\/a5a71474-025c-4ffe-b8c1-373c30b8bd6c\/dc6kiet-017f562b-03e8-4636-97da-44a8df70b589.jpg\/v1\/fill\/w_1024,h_587,q_75,strp\/that_s_all_folks_space_jam_by_toon1990_dc6kiet-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOiIsImlzcyI6InVybjphcHA6Iiwib2JqIjpbW3siaGVpZ2h0IjoiPD01ODciLCJwYXRoIjoiXC9mXC9hNWE3MTQ3NC0wMjVjLTRmZmUtYjhjMS0zNzNjMzBiOGJkNmNcL2RjNmtpZXQtMDE3ZjU2MmItMDNlOC00NjM2LTk3ZGEtNDRhOGRmNzBiNTg5LmpwZyIsIndpZHRoIjoiPD0xMDI0In1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmltYWdlLm9wZXJhdGlvbnMiXX0.Jb5O8VlFxU3vIZOYOsU5ICuht58Igo2Ss1rro97ArYw)","a5eefe80":"# Looks like there are no Null values in the train dataframe ","747e2e55":"Bestparams = {<br>\n        'learning_rate':[1],<br>\n        'n_estimators':[50],<br>\n        'min_child_weight': [5],\n        'gamma': [ 2],<br>\n        'subsample': [ 1.0],\n        'colsample_bytree': [0.6],<br>\n        'max_depth': [15],<br>\n        'reg_lambda':[0.5,1]<br>\n        }<br>","b84b2f32":"# Random Forest with Entire Dataset variables ","21f635a6":"# Random Forest on Interger Features \n\n### Below cell is  RandomGridsearch ran for getting the parameters of random forest ","4aab6e86":"# Lets Explore the testing dataframe ","caf427a2":"# Export the combination you need ","00c287af":"# Looks like the data is quite less just 4K samples and lot of columns ","e886190b":"# Result Prediction Using , Lasso, Ridge , Bayesian and RF on Integer Columns ","1915895d":"# Lasso Regression on Integer Features ","b7bcbb15":"# Using Stacking option "}}