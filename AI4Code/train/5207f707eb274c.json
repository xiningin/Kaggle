{"cell_type":{"602d12e0":"code","d97992e6":"code","9d6e538a":"code","06578c4e":"code","bdf4c076":"code","0dcdfd64":"code","9939edb9":"code","9affe8e9":"code","2ee91dbd":"code","2c187471":"code","ac8e518c":"code","83eb4eab":"code","b74ce9c4":"code","5528cf46":"code","6aaa9d80":"code","570651e9":"code","0edae354":"code","a5fada24":"code","1108d900":"code","c2bfbe3d":"code","75fca5a7":"code","033a55ec":"code","e72052c0":"code","7774e6c5":"code","50f93d44":"code","240236f4":"code","541698b2":"code","9a7911c9":"code","cff415c6":"code","0ba84e74":"code","8159e8f6":"code","69332d9e":"code","bc061718":"code","a699de18":"code","aebbb039":"code","31b6ab82":"code","9074141c":"code","f9c72fff":"code","ffb39b50":"code","1f5d2413":"code","c6a39c11":"code","0ab1c949":"code","a1bd522c":"code","9f43f6e6":"code","daf6911c":"code","2387d5be":"code","8a6fdb11":"code","ef09dda5":"code","468842b7":"markdown","efb42594":"markdown","7e225205":"markdown","967a98e1":"markdown","164af191":"markdown","904990d5":"markdown","48ece498":"markdown","51ca744b":"markdown","6bb3fbe9":"markdown","231d811a":"markdown","c1a19ba8":"markdown","27db125f":"markdown","0dedc247":"markdown","f880f434":"markdown","eddcd67f":"markdown","a5ba87ee":"markdown","ad5b8178":"markdown","928eda55":"markdown","65d09a11":"markdown","b8355c6b":"markdown","fa423bd8":"markdown","51e67a54":"markdown","a369f605":"markdown","fb4050a1":"markdown","5ab0ee79":"markdown","9d4eb5b8":"markdown","200bfb1d":"markdown","eea0c36d":"markdown","d1d69751":"markdown","eabcf074":"markdown","e07eac9b":"markdown"},"source":{"602d12e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d97992e6":"# importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9d6e538a":"# reading the data\ndf = pd.read_csv(\"\/kaggle\/input\/amazon-fine-food-reviews\/Reviews.csv\")","06578c4e":"df.head()","bdf4c076":"import sqlite3","0dcdfd64":"con = sqlite3.connect(\"\/kaggle\/input\/amazon-fine-food-reviews\/database.sqlite\")","9939edb9":"# running the query where the score is not equal to 3\n\ndata = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score != 3\"\"\",con)","9affe8e9":"data.head()","2ee91dbd":"def partit(x):\n    if x<3:\n        return 0\n    return 1\n\ndata['Score'] = data['Score'].map(partit)","2c187471":"data.head()","ac8e518c":"#counting the number of occurence of 1 & 0\ndata.Score.value_counts()","83eb4eab":"data['Score'].value_counts().plot.bar()","b74ce9c4":"#dropping the ID column \ndata.drop('Id',inplace=True,axis=1)","5528cf46":"data[data.duplicated()].shape","6aaa9d80":"#sorting the values with respective to ProductId\ndata = data.sort_values('ProductId')","570651e9":"data.head()","0edae354":"# dropping the duplicate values\ndata1 = data.drop_duplicates(subset= ['UserId', 'ProfileName', 'Time', 'Text', 'Summary'])","a5fada24":"data1.shape","1108d900":"print(\"Retained Data is \",round((len(data1)\/len(data)) * 100,2))\nprint(\"Lost Data is \",round(100 - round((len(data1)\/len(data)) * 100,2),2))\n","c2bfbe3d":"# we know that text data contain http... something .\n#But if we wanna confrim it \n\nimport re\npattern = r\"http\\S+\" # S+ is nothing but take everthing which is after the http\nregex = re.compile(pattern)\n\nfor sent in data1[\"Text\"].values:\n    if regex.search(sent):\n        print(\"Found the http protocol\")\n        text = sent\n        break","75fca5a7":"from bs4 import BeautifulSoup","033a55ec":"soup = BeautifulSoup(text,'html')","e72052c0":"soup.get_text()","7774e6c5":"#replacing the words like(I\\'d,he\\'d,...etc)\n\ndef decontraction(text):\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"can not\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)  \n    text = re.sub(r\"\\re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t'\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m'\", \" am\", text)\n    text = re.sub('[^A-Za-z0-9]+',' ',text)\n    return text","50f93d44":"cleaned_text = soup.get_text()","240236f4":"cleaned_text = decontraction(cleaned_text)","541698b2":"print(\"Raw text : \", text )\nprint(\"-\"*75)\nprint(\"cleaned text : \",cleaned_text)","9a7911c9":"import nltk\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')","cff415c6":"\ncleaned_text = []\n\nfor text in data1[\"Text\"].values:\n    sent = re.sub(\"http\\S+\", \"\", text)\n    sent = BeautifulSoup(sent, 'html').get_text()\n    sent = decontraction(sent)\n    sent = re.sub(r\"\\S*\\d\\S*\", \" \", sent)\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    sent = \" \".join([text.lower() for text in sent.split() if text not in stopwords])\n    cleaned_text.append(sent)","0ba84e74":"# lets see the cleaned text\ncleaned_text[3]","8159e8f6":"from sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n    \"This is the first document.\",\n    \"This document is the secind document.\",\n    \"And this is the third one\",\n    \"Is this the first document?\"\n]\n\n#vectorizer = CountVectorizer(max_features=3)  #max_features  = as we mentioned max_features(3) so it will just take max features 3 ,if we dont mention max_features it will take all the features\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())","69332d9e":"print(vectorizer.get_feature_names())\nprint(X.toarray())","bc061718":"count_vec = CountVectorizer()\ncount_vec.fit(cleaned_text)","a699de18":"#bow= bag of words\nvectorized_bow_data = count_vec.transform(cleaned_text)","aebbb039":"vectorized_bow_data.shape","31b6ab82":"bi_gram_count_vec_without_mfeat = CountVectorizer(ngram_range=(1, 2))\nbigram_count_wmfeat = bi_gram_count_vec_without_mfeat.fit_transform(cleaned_text)\nprint(bigram_count_wmfeat.shape)\nprint(bi_gram_count_vec_without_mfeat.get_feature_names()[:50])","9074141c":"from sklearn.feature_extraction.text import TfidfVectorizer","f9c72fff":"tf_idf = TfidfVectorizer(ngram_range=(1,2),min_df=10)\ntf_idf.fit_transform(cleaned_text)","ffb39b50":"#lets see the top 50 features name\nprint(tf_idf.get_feature_names()[:50])","1f5d2413":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score","c6a39c11":"#separating features and lables\nX = cleaned_text.copy()\ny = data1[\"Score\"].values","0ab1c949":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2),min_df=10)","a1bd522c":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size= 0.25,random_state=123)","9f43f6e6":"final_X_train = tf_idf_vect.fit_transform(X_train)\nfinal_X_test  = tf_idf_vect.transform(X_test)","daf6911c":"# first we will work on 5000 data point\nsample_5000_train = final_X_train[:5000],y_train[:5000]\nsample_5000_test = final_X_test[:5000],y_test[:5000]","2387d5be":"\nauc_score_train = []\nauc_score_test = []\n\nk = []\n\n\nfor i in range(1, 50, 3):\n    print(f\"For K = {i} | Remaining Neighbours {50-3}\")\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(final_X_train[:5000], y_train[:5000])\n    y_pred_train = knn.predict_proba(final_X_train[:5000])\n    y_pred_test = knn.predict_proba(final_X_test[:5000])\n    auc_score_train.append(roc_auc_score(y_train[:5000],y_pred_train[:,1]))\n    auc_score_test.append(roc_auc_score(y_test[:5000],y_pred_test[:,1]))\n    k.append(i) ","8a6fdb11":"# best value of k is 47\nk = 47\n\n# plot auc roc curve (FPR-TPR curve)\nfrom sklearn.metrics import roc_curve, confusion_matrix\n\nknn = KNeighborsClassifier(n_neighbors=k)\nknn.fit(final_X_train[:5000],y_train[:5000])\n\ny_pred_train = knn.predict_proba(final_X_train[:5000])[:,1]\ny_pred_test = knn.predict_proba(final_X_test[:5000])[:,1]\n\nfpr_train , tpr_train , thresholds_train = roc_curve(y_train[:5000],y_pred_train)\nfpr_test , tpr_test , thresholds_test  = roc_curve(y_test[:5000],y_pred_test)\n","ef09dda5":"plt.plot(fpr_train,tpr_train,label = \"FPR-TRP Train\")\nplt.plot(fpr_test,tpr_test,label = \"FPR-TRP Test\")\nplt.title(\"FPR-TRP Train v\/s Test\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.legend()\nplt.show()","468842b7":"\n# Objective\n\nGiven review , determine if the review is positive or negative(Sentiment polarity or text classification)\n","efb42594":"we haven't mentioned max_features in CountVectorizer so that is why we are getting shape 365333,116780 is big.","7e225205":"In the upper code we have done that, we cleaned all the text.","967a98e1":"we have successfully applied SQL query and the result that is score != 3","164af191":"so this 0,1,2 .. shows that word is occured in that line or not,if it occured than how many times it occured","904990d5":"# Bi-grams & n-grams","48ece498":"## Columns\n\n1.Id\n\n2.ProductId\n\n3.UserId\n\n4.ProfileName\n\n5.HelpfulnessNumerator\n\n6.HelpfulnessDenominator\n\n7.Score\n\n8.Time\n\n9.Summary\n\n10.Text\n\n","51ca744b":"subset = its nothing but dropping the duplicate values from the columns.\nwe have wriiten the columns name from which we have to remove duplicate values.","6bb3fbe9":"# Bag of words and Tf-Idf vectorizers","231d811a":"# tf-IDF vectorizer","c1a19ba8":"Data includes:\n\n    1.Reviews from Oct 1999 - Oct 2012\n    2.568,454 reviews\n    3.256,059 users\n    4.74,258 products\n    5.260 users with > 50 reviews","27db125f":"# Decontraction","0dedc247":"From this plot we can see that it is unstable","f880f434":"# Applying KNN with tf-IDF ","eddcd67f":"#### using BeautifulSoup ","a5ba87ee":"As we can see there are more duplicate values so we have to drop it","ad5b8178":"we were having 30% of duplicated data","928eda55":"### so lets now continue this vectorizatin on our cleaned_text","65d09a11":"### Removing stopwords","b8355c6b":"our guess was right.\n\nIn this http... present","fa423bd8":"## How to determine if a review is positive or negative?\n\n\n Approach = if rating is 4\/5 than polarity is 1, if rating is 1\/2 than polarity is 0","51e67a54":"# Text Preprocessing","a369f605":"as we can see the difference between the raw and clean text","fb4050a1":"# Checking for Duplicates & dropping them","5ab0ee79":"#### Converting data into binary classification","9d4eb5b8":"#### OPTIONAL STEP","200bfb1d":"so we reviews (1,2)=0 that is 82037\n\n(4,5) = 1 = 443777 \n\nwe have more positve reviews","eea0c36d":"#### First lets see a sample code for bag of words and what it is","d1d69751":"# Amazon Fine Food","eabcf074":"# How to read data with SQL \n  #### lets see ;)\n  ","e07eac9b":"# in this partit() function we have just given the values 1 and 0 to the score label\n\n# 1,2 = 0\n# 4,5 = 1"}}