{"cell_type":{"ca83d24d":"code","ec521ba5":"code","6e0d7c0d":"code","909358e2":"code","da575daf":"code","b20728d3":"code","372c1630":"code","a4107bf6":"code","b17d369f":"code","29e98f4f":"code","3e96f144":"code","5cef97f7":"code","0421692a":"code","290be8d0":"code","f46651fa":"code","96087023":"code","3903320f":"code","d119bc49":"code","c92dad18":"code","cd7e43c8":"code","692d2c8b":"code","03768477":"code","060a298d":"code","6743452f":"code","77e7faf7":"code","2d33fcc9":"code","36a7e8b5":"code","848f94a1":"code","6ace099d":"code","f0f49814":"code","f95c31a4":"code","f8289892":"code","231c6628":"code","8a798f1f":"code","c1166de9":"code","49503002":"code","1833d5bb":"code","6e5044d2":"code","f3276b2b":"code","dccf5926":"code","4fa62c29":"code","0aa5676c":"code","94255a79":"code","34d2837b":"code","9d13327e":"code","83c25cd4":"code","b6f70930":"code","b1ef181c":"code","df6d0b4f":"code","3a35f04d":"code","007c035c":"code","ed339af4":"code","b94df4c1":"code","59ed84f5":"code","e7fd89ad":"code","d2e7bc5f":"code","d2e08073":"code","48b83e3b":"code","19563366":"code","729e5683":"code","75249173":"code","47d37dbd":"code","64a64fe3":"code","c06686b0":"code","3067efd9":"code","5e3933f7":"code","d3f22903":"code","d86a7ef0":"code","96959a5b":"code","2f08a073":"markdown"},"source":{"ca83d24d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom __future__ import print_function\nimport numpy as np # linear algebra\nimport tensorflow as tf\nimport tensorflow.keras\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n\nfrom scipy import stats\nfrom scipy.stats import norm,skew\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom fancyimpute import KNN\nfrom sklearn.linear_model import SGDRegressor \nfrom sklearn.linear_model import Ridge \nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom functools import reduce\nrng = np.random\n\npd.options.display.max_rows = 10\npd.options.display.float_format = '{:.2f}'.format\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nwarnings.filterwarnings('ignore')","ec521ba5":"\n\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain\n\n\n","6e0d7c0d":"test=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest","909358e2":"train.columns","da575daf":"test.columns","b20728d3":"train['SalePrice'].describe()","372c1630":"sns.distplot(train['SalePrice'])","a4107bf6":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corrmat, vmax=.8, annot=True);","b17d369f":"corrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","29e98f4f":"print(\"Skewness: %f\" % train['SalePrice'].skew())#measure of symmetry\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())#measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution.","3e96f144":"sns.distplot(train.skew(),color='green',axlabel ='Skewness')","5cef97f7":"plt.figure(figsize=(12,7))\nsns.distplot(train.kurt(),color='r',axlabel ='Kurtosis',norm_hist= False, kde = True,rug = False)","0421692a":"train.shape","290be8d0":"sns.set()\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.pairplot(train[columns],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()","f46651fa":"plt.figure(figsize = (12, 6))\nsns.countplot(x = 'Neighborhood', data = train)\nxt = plt.xticks(rotation=90)","96087023":"colnames_cont=train.iloc[:,1:-1].select_dtypes(include=np.number).columns.tolist()\nprint('numerical features')\nprint(colnames_cont)\nprint(\"number of numerics features = \",len(colnames_cont))","3903320f":"#missing data\ntotal=train.isnull().sum().sort_values(ascending=False)\npercent=(train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing=pd.concat([total,percent], axis=1,keys=['total','percent'])\nmissing.head(30)","d119bc49":"train.shape","c92dad18":"#dropping the missing\ntrain_df=train.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage','GarageCond','GarageType','GarageYrBlt','GarageFinish','GarageQual','BsmtExposure','BsmtFinType2','BsmtFinType1','BsmtCond','BsmtQual','MasVnrArea','MasVnrType','Electrical'], axis=1)\ntrain_df.shape","cd7e43c8":"#KNN (K Nearest Neighbors) with number of neighbors = 5 to impute missing values.\ntrain_df.columns\n","692d2c8b":"print(len(train_df.columns))","03768477":"#continous features\ncolnames_cont=train_df.iloc[:,1:-1].select_dtypes(include=np.number).columns.tolist()\nprint('numerical features')\nprint(colnames_cont)\nprint(\"number of numerics features = \",len(colnames_cont))","060a298d":"#imputing\ntrain_df[colnames_cont]=KNN(k=5).fit_transform(train_df[colnames_cont])","6743452f":"#categorical\ncolnames_cat=train_df.iloc[:,1:-1].select_dtypes(include='object').columns.tolist()\nprint('numerical features')\nprint(colnames_cat)\nprint(\"number of numerics features = \",len(colnames_cat))","77e7faf7":"\n\nfor categorical_col in colnames_cat:\n    most_frequent = train_df[categorical_col].value_counts().idxmax()\n    hasCol       = 'Has'+categorical_col\n    \n    #new col \n    train_df[hasCol] = pd.Series(len(train_df[categorical_col]), index=train_df.index)\n    \n    #set new col = 1\n    train_df[hasCol] = 1\n    \n    #set new col = 0 if train_df[categorical_col] not empty\n    train_df.loc[train_df[categorical_col].isnull(),hasCol] = 0\n    \n    #set data_train[categorical_col] = most_frequent if new col = 0\n    #if location of new col = 0 this mean that data_train[categorical_col] in this location is empty\n    train_df.loc[train_df[hasCol] == 0,categorical_col] = most_frequent\n    \n    #drop new col\n    train_df = train_df.drop(hasCol, axis=1)\n    \nprint('missing values of categorical features imputed successfully')    \n\n","2d33fcc9":"#null values\ntrain_df.isnull().sum().max()","36a7e8b5":"#outlier is an observation point that is distant from other observations.\n#plotting outliers\nsns.distplot(train_df['SalePrice'],fit=norm);\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)","848f94a1":"#anomaly or outliers\ncols = ['MSSubClass','LotArea','OverallQual']\nfor col in cols:\n    plt.figure()\n    ax = sns.boxplot(x=train_df[col])","6ace099d":"#interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 \u2212 Q1.\n#IQR for numerical methods\nQ1 = train_df[colnames_cont].quantile(0.25)\nQ3 = train_df[colnames_cont].quantile(0.75)\nIQR = Q3 - Q1\n\nhasOutlier = (train_df[colnames_cont] < (Q1 - 1.5 * IQR)) | (train_df[colnames_cont] > (Q3 + 1.5 * IQR))\nhasOutlier","f0f49814":"#clean the outliers\nnum_data = train_df[colnames_cont]\n\nfor numeric_col in colnames_cont: \n    train_df = train_df.drop(train_df.loc[hasOutlier[numeric_col]].index)","f95c31a4":"# drop  rows which contain outliers data  \ntrain_df.shape[0]","f8289892":"#feature selection\ntrain_df = train_df.drop('Id', axis=1)\nprint('Id column deleted successfully')","231c6628":"#correlation matrix\ncorrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)","8a798f1f":"#scatterplot\nsns.set()\ncols = ['LotArea','OverallQual','YearBuilt','YearRemodAdd','TotalBsmtSF','1stFlrSF',\n        'GrLivArea','FullBath','TotRmsAbvGrd','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF']\n\ngroup_1 = cols[0:8]\ngroup_1.insert(0, \"SalePrice\")\n\n#scatter plot of first group\nsns.pairplot(train_df[group_1], size = 2)\nplt.show();","c1166de9":"group_2 = cols[8:]\ngroup_2.insert(0, \"SalePrice\")\n\n#draw scatter plot of first group\nsns.pairplot(train_df[group_2], size = 2)\nplt.show();","49503002":"# all numerical features in our data\nallNumericalFeatures = colnames_cont\n\n# numerical features which we use it in our model\nselectedNumericalFeatures = cols\n\n# numerical features that we will drop it\ndeletedFeatures =  list(set(allNumericalFeatures) - set(selectedNumericalFeatures))\n\nprint(\"data shape before delete features = \",train_df.shape)\n\n# delete unwanted features\ntrain_df = train_df.drop(deletedFeatures, axis=1)\n\nprint(\"data shape after delete features = \",train_df.shape)\n\nprint(\"unwanted features deleted successfully\")","1833d5bb":"#convert categorical variable into lables\nlabelEncoder = LabelEncoder()\n\nfor categorical_col in colnames_cat:\n    train_df[categorical_col] =  labelEncoder.fit_transform(train_df[categorical_col])\n    \nprint(\"categorical columns converted successfully\")","6e5044d2":"colnames_cat","f3276b2b":"#data scaling usind standardization\nscaler = StandardScaler(copy=True, with_mean=True, with_std=True)\ntrain_df[selectedNumericalFeatures] = scaler.fit_transform(train_df[selectedNumericalFeatures])\n\nprint(\"data scaling successfully\")\ntrain_df.describe()","dccf5926":"#training our data\nX = train_df.drop('SalePrice', axis=1)\ny = train_df['SalePrice']\n\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.40, random_state=55, shuffle =True)\nprint('data splitting successfully')","4fa62c29":"#model bulding\n#\nSGDRRegModel = SGDRegressor(random_state=55,loss = 'squared_loss')\nSelectedParameters = {\n                      'alpha':[0.1,0.5,0.01,0.05,0.001,0.005],\n                      'max_iter':[100,500,1000,5000,10000],\n                      'tol':[0.0001,0.00001,0.000001],\n                      'penalty':['l1','l2','none','elasticnet']\n                      }\n\nGridSearchModel = GridSearchCV(SGDRRegModel,SelectedParameters, cv = 5,return_train_score=True)\nGridSearchModel.fit(X_train,y_train)\n\nSGDRRegModel = GridSearchModel.best_estimator_\nSGDRRegModel.fit(X_train,y_train)\nprint('successfull')","0aa5676c":"#ridge regression model==shrinkage regression==used when the data suffers from multicollinearity ( independent variables are highly correlated).\nRidgeRegModel = Ridge(random_state= 55, copy_X=True)\nSelectedParameters = {\n                      'alpha':[0.1,0.5,0.01,0.05,0.001,0.005],\n                      'normalize':[True,False],\n                      'max_iter':[100,500,1000,5000,10000],\n                      'tol':[0.0001,0.00001,0.000001],\n                      'solver':['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']\n                      }\n\nGridSearchModel = GridSearchCV(RidgeRegModel,SelectedParameters, cv = 5,return_train_score=True)\nGridSearchModel.fit(X_train,y_train)\n\nRidgeRegModel = GridSearchModel.best_estimator_\nRidgeRegModel.fit(X_train,y_train)\n\nprint(\" successfully\")","94255a79":"#lasso model run==Least Absolute Shrinkage and Selection Operator\nLassoRegModel = Lasso(random_state= 55 ,copy_X=True)\nSelectedParameters = {\n                      'alpha':[0.1,0.5,0.01,0.05,0.001,0.005],\n                      'normalize':[True,False],\n                      'tol':[0.0001,0.00001,0.000001],\n                      }\n\nGridSearchModel = GridSearchCV(LassoRegModel,SelectedParameters, cv = 5,return_train_score=True)\nGridSearchModel.fit(X_train,y_train)\n\nLassoRegModel = GridSearchModel.best_estimator_\nLassoRegModel.fit(X_train,y_train)\n\nprint(\" successfully\")","34d2837b":"#Linear regression model run\nlinearRegModel = LinearRegression(copy_X=True)\nlinearRegModel.fit(X_train,y_train)\nprint(\" successfully\")","9d13327e":"#decision Tree Regressor model run\ndecisionTreeModel = DecisionTreeRegressor(random_state=55)\n\nSelectedParameters = {\n                      'criterion': ['mse','friedman_mse','mae'] ,\n                      'max_depth': [None,2,3,4,5,6,7,8,9,10],\n                      'splitter' : ['best','random'],\n                      'min_samples_split':[2,3,4,5,6,7,8,9,10],\n                      }\n\nGridSearchModel = GridSearchCV(decisionTreeModel,SelectedParameters, cv = 5,return_train_score=True)\nGridSearchModel.fit(X_train,y_train)\n\ndecisionTreeModel = GridSearchModel.best_estimator_\ndecisionTreeModel.fit(X_train,y_train)\n\nprint(\" successfully\")","83c25cd4":"#Xgboost Regressor model run==Extreme Gradient Boosting==uses the decisoin tree(parallelization and tree prunning) method and is fast\n#boosting trains models in succession, with each new model being trained to correct the errors made by the previous ones.\nXGBRModel = XGBRegressor(n_jobs = 4)\n\nSelectedParameters = {\n                      'n_estimators': [100,1000,10000] ,\n                      'learning_rate': [0.1,0.5,0.01,0.05],\n                      }\n\nGridSearchModel = GridSearchCV(XGBRModel,SelectedParameters, cv = 5,return_train_score=True)\nGridSearchModel.fit(X_train,y_train)\n\nXGBRModel = GridSearchModel.best_estimator_\nXGBRModel.fit(X_train,y_train)\n\nprint(\" successfully\")","b6f70930":"#evaluating the model\nmodels = [SGDRRegModel, RidgeRegModel, LassoRegModel, linearRegModel, decisionTreeModel,XGBRModel]\n\nfor model in models:\n    print(type(model).__name__,' Train Score is   : ' ,model.score(X_train, y_train))\n    print(type(model).__name__,' Test Score is    : ' ,model.score(X_test, y_test))\n   ","b1ef181c":"#predicting\nfor model in models:\n    print(type(model).__name__,\" error metrics\")\n    print('---------------------------------------------------------')\n    y_pred = model.predict(X_test)\n\n    MAE = mean_absolute_error(y_test,y_pred)\n    print(\"mean absolute error = \",MAE)\n\n    MSE = mean_squared_error(y_test,y_pred)\n    print(\"mean squared error = \",MSE) \n\n    RMSE = np.sqrt(mean_squared_error(y_test,y_pred))\n    print(\"root mean squared error = \",RMSE) \n    print()","df6d0b4f":"#XGBRegressor has the minimal error since  new models are trained to predict the residuals (i.e errors) of prior models.\n#this helps focusing on correcting the mistakes which were caused by other models","3a35f04d":"\nk=11\n\nfeature_cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncmk = train[feature_cols].corr()\nf, ax = plt.subplots(figsize=(20, 15))\ncolormap = plt.cm.RdBu\nsns.heatmap(cmk,linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)\n\n","007c035c":"sns.set()\nsns.pairplot(train[feature_cols], size = 2)\nplt.show();","ed339af4":"#feature engineering\n\ntrain[feature_cols].head(10)\n\n","b94df4c1":"train[feature_cols].describe()","59ed84f5":"#combining related features\ndef combine_feature(train):\n    train['Garage'] = train['GarageCars'] * train['GarageArea']\n    train['SF'] = train['TotalBsmtSF'] + train['1stFlrSF']\n    train['TG'] = train['TotRmsAbvGrd'] * train['GrLivArea']\n\n    \ncombine_feature(train)\ncombine_feature(test)\ntrain = train.fillna(0)\ntest = test.fillna(0)","e7fd89ad":"feature_cols2 = feature_cols.tolist()\nfeature_cols2.extend(['Garage','SF','TG'])\nprint(feature_cols2)\ncmk2 = train[feature_cols2].corr()\nf, ax = plt.subplots(figsize=(20, 15))\ncolormap = plt.cm.RdBu\nsns.heatmap(cmk2,linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","d2e7bc5f":"cols = feature_cols2\nfor e in ['SalePrice','GarageCars', 'GarageArea','TotalBsmtSF', '1stFlrSF','TG']:\n    cols.remove(e)","d2e08073":"cols\n","48b83e3b":"\nvar = 'GrLivArea'\ntrain.plot.scatter(x=var, y='SalePrice');\n\n\n","19563366":"train.plot.scatter(x='GrLivArea', y='SalePrice');","729e5683":"train = train[train['GrLivArea']<4500]\ntrain = train[train['SalePrice']< 700000]","75249173":"var = 'GrLivArea'\ntrain.plot.scatter(x=var, y='SalePrice');","47d37dbd":"#machine learning with tensorflow\ndef normalize(data, col):\n    data[col] = (data[col] - data[col].mean()) \/ (data[col].max() - data[col].mean())\n    \ndef trans(data, feature_columns):\n    trans = []\n    for col in feature_columns:\n        trans.append(list(data[col])) \n    return trans","64a64fe3":"training_example = train.head(1000)\nvalidation_example = train.tail(train.shape[0] - 1000)\ntest_example = test.copy()\n\ntrain.shape[0]","c06686b0":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() ","3067efd9":"learning_rate = 0.1\ntraining_epochs = 100\ndisplay_step = 10\n\nfeature_columns = cols\nfeatures_num = len(feature_columns)\n\ntraining_X = []\nvalidation_X = []\n\nfor col in feature_columns:\n    normalize(training_example, col)\n    normalize(validation_example, col)\n    normalize(test_example, col)\n\ntraining_X = training_example[feature_columns]\nvalidation_X = validation_example[feature_columns]\ntest_X = test_example[feature_columns]\n#print(training_X)\ntraining_Y = training_example['SalePrice']\nvalidation_Y = validation_example['SalePrice']\nn_samples = tf.placeholder(tf.float32)\n\nX = tf.placeholder(tf.float32,[features_num, None])\nY = tf.placeholder(tf.float32)\n\nW =  tf.Variable(tf.zeros([1, features_num],name=\"weight\"))  \nb = tf.Variable(tf.zeros([1]), name=\"bias\")\n\npred = tf.add(tf.matmul(W, X), b)\ncost = tf.reduce_sum(tf.pow(pred - Y, 2)) \/ (n_samples)\n#optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\noptimizer = tf.train.MomentumOptimizer(learning_rate,momentum = 0.9).minimize(cost)\ninit = tf.compat.v1.global_variables_initializer()\n\n","5e3933f7":"tf.compat.v1.disable_eager_execution()","d3f22903":"result = []\nwith tf.compat.v1.Session() as sess:\n    sess.run(init)\n    training_XT = sess.run(tf.transpose(training_X[feature_columns]))\n    validation_XT = sess.run(tf.transpose(validation_X[feature_columns]))\n    for epoch in range(training_epochs):\n        for index, row in training_X.iterrows():\n            x = [ [i] for i in row[feature_columns]]\n            sess.run(optimizer, feed_dict={X: x, Y: training_Y[index], n_samples:training_Y.shape[0]})\n        if (epoch + 1) % display_step == 0:\n            c = sess.run(cost, feed_dict={X: training_XT, Y: training_Y, n_samples:training_Y.shape[0]})            \n            vc = sess.run(cost, feed_dict={X: validation_XT, Y: validation_Y, n_samples:validation_Y.shape[0]})\n            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", c, \"v_cost=\", vc,\"W=\", sess.run(W), \"b=\", sess.run(b))\n            result.append([epoch, c, vc])\n    print(\"Optimization Finished!\")\n    training_cost = sess.run(cost, feed_dict={X: training_XT, Y: training_Y, n_samples:training_Y.shape[0]})\n    validation_cost = sess.run(cost, feed_dict={X: validation_XT, Y: validation_Y, n_samples:validation_Y.shape[0]})\n    print(\"Training cost=\", training_cost,\"Validation cost=\",validation_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n    r = pd.DataFrame(result, columns = ['epoch', 'cost', 'validation cost']) \n    sns.pointplot(x=\"epoch\",y=\"cost\",data=r, markers=\"^\")\n    sns.pointplot(x=\"epoch\",y=\"validation cost\",data=r)\n    \n    p = sess.run(pred, feed_dict={X: trans(test_X,feature_columns)})\n    print('pred: ',p)","d86a7ef0":"tf.__version__","96959a5b":"result = pd.DataFrame()\nresult['Id'] = test.Id\nresult['SalePrice'] = p[0]\nresult.head()","2f08a073":"DEEP LEARNING USING TENSOR FLOW"}}