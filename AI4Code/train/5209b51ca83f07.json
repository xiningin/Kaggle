{"cell_type":{"c0850664":"code","239ff5c8":"code","1043a2b5":"code","8560b48a":"code","e882f7ab":"code","3ac448da":"code","825f2404":"code","f7d47843":"code","b887a841":"code","88bcaaac":"code","a35b0b6f":"code","1e50a2a3":"code","86413e0f":"code","e8c6ba76":"code","8339755d":"code","2c9b6584":"code","a16f7f63":"code","4e901a81":"code","99469b57":"code","3a7eeb04":"code","db58b0e6":"code","ea650536":"code","5d342cf5":"code","c2efc1c0":"code","9835e528":"code","62639418":"code","c2273004":"code","5f71f7d6":"code","1ca225ac":"code","258edc4c":"code","e74bb17b":"code","9598822c":"code","ecf1edf3":"code","0743594c":"code","4d4712a5":"code","cde415e5":"code","c47dda16":"code","3ab4c5ef":"code","de91716a":"markdown","53c9f774":"markdown","5dd3d6d4":"markdown","64578047":"markdown","83518568":"markdown","a90238e7":"markdown","5a0906b9":"markdown","884d8946":"markdown","27b84e82":"markdown","ef3ab961":"markdown","39af9382":"markdown","62a0a897":"markdown","07438e1a":"markdown","0803aa29":"markdown","0c5a6faf":"markdown","b3eceb29":"markdown","a620feaa":"markdown","ca1c804f":"markdown","92efee97":"markdown","93d399c7":"markdown","fe317ee9":"markdown","ed9d6309":"markdown"},"source":{"c0850664":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport squarify\nimport matplotlib.pyplot as plt\nplt.style.use(\"default\")\nfrom IPython.display import Image","239ff5c8":"def convert_from_ms(milliseconds):\n    seconds, milliseconds = divmod(milliseconds, 1000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    days, hours = divmod(hours, 24)\n    seconds = seconds + milliseconds \/ 1000\n    return days, hours, minutes, round(seconds)","1043a2b5":"%%time\n\npath = \"..\/input\/riiid-train-data-multiple-formats\/riiid_train.pkl.gzip\"\ntrain = pd.read_pickle(path)\ntrain = train.astype({'row_id': 'int64',\n                      'timestamp': 'int64',\n                      'user_id': 'int32',\n                      'content_id': 'int16',\n                      'content_type_id': 'int8',\n                      'task_container_id': 'int16',\n                      'user_answer': 'int8',\n                      'answered_correctly': 'int8',\n                      'prior_question_elapsed_time': 'float32',\n                      'prior_question_had_explanation': 'boolean'})\n\ntrain.info()","8560b48a":"train.head()","e882f7ab":"train.isna().sum()","3ac448da":"n_user = train.user_id.nunique()\nprint(f'There are {n_user} users')","825f2404":"%%time\n\nagg = {'row_id' : 'count',\n       'timestamp' : lambda x: convert_from_ms(x.max())[0],\n       'answered_correctly' : lambda x: round(x.mean() * 100)}\n\nuser_info = train.groupby('user_id').agg(agg)\nuser_info = user_info.rename(columns={'row_id' : 'nb_interactions',\n                                   'timestamp' : 'nb_jours',\n                                   'answered_correctly' : 'score'})\nuser_info","f7d47843":"user_info.describe(percentiles=[.05, .25, .5, .75, 0.95])","b887a841":"%%time\nfig = sns.pairplot(user_info, diag_kind=\"kde\", plot_kws={'alpha': 0.01})\nfig.savefig('.\/img_eda_user_pairplot.png', transparent=True)","88bcaaac":"sns.distplot(user_info.nb_interactions, hist=False)\nplt.xscale('log')\nplt.title('Nb interactions with log scale')\nplt.savefig('.\/img_eda_user_nb_interactions_log_scale.png', transparent=True)","a35b0b6f":"sns.distplot(user_info.nb_jours, hist=False)\nplt.xscale('log')\nplt.title('Nb days with log scale')\nplt.savefig('.\/img_eda_user_nb_days_log_scale.png', transparent=True)","1e50a2a3":"fig, (ax0, ax1) = plt.subplots(nrows=1,ncols=2, sharey=False, figsize=(7,4))\nsns.boxplot(y=user_info['nb_interactions'], ax=ax0)\nsns.boxplot(y=user_info['nb_interactions'], ax=ax1)\nax0.set(title=\"Number of interactions per user\")\nax1.set(ylim=(0,500), title=\"Focus on the box\", ylabel=\"\")\nfig.savefig('.\/img_eda_interaction_boxplot.png', transparent=True)","86413e0f":"fig, (ax0, ax1) = plt.subplots(nrows=1,ncols=2, sharey=False, figsize=(7,4))\nsns.boxplot(y=user_info['nb_jours'], ax=ax0)\nsns.boxplot(y=user_info['nb_jours'], ax=ax1)\nax0.set(title=\"Number of days per user\")\nax1.set(ylim=(0,150), title=\"Focus on the box\", ylabel=\"\")\nfig.savefig('.\/img_eda_jour_boxplot.png', transparent=True)","e8c6ba76":"# R\u00e9gler ici les seuils de nettoyage\nQ1 = 0.05\nQ4 = 0.95\n\n# Filtre des interaction exceptionnelles\nq1_int = user_info.nb_interactions.quantile(Q1)\nq4_int = user_info.nb_interactions.quantile(Q4)\ncond_int = (user_info.nb_interactions > q1_int) & (user_info.nb_interactions < q4_int)\n\n# Filtre des d\u00e9lais exceptionnels\nq1_delay = user_info.nb_jours.quantile(Q1)\nq4_delay = user_info.nb_jours.quantile(Q4)\ncond_delay = (user_info.nb_jours > q1_delay) & (user_info.nb_jours < q4_delay)\n\nreduced_user_info = user_info[cond_int & cond_delay]\nreduced_user_info","8339755d":"reduced_n_user = len(reduced_user_info)\n\nreduced_user_rate = round((1 - reduced_n_user \/ n_user) * 100)\nprint(f'Users have been reduced by {reduced_user_rate} %.')","2c9b6584":"%%time\nfig = sns.pairplot(reduced_user_info,  diag_kind=\"kde\", plot_kws={'alpha': 0.01})\nfig.savefig('.\/img_eda_reduced_user_pairplot.png', transparent=True)","a16f7f63":"sns.distplot(reduced_user_info.nb_interactions, hist=False)\nplt.xscale('log')\nplt.title('Nb interactions with log scale')\nplt.savefig('.\/img_eda_reduced_user_nb_interactions_log_scale.png', transparent=True)","4e901a81":"sns.distplot(reduced_user_info.nb_jours, hist=False)\nplt.xscale('log')\nplt.title('Nb days with log scale')\nplt.savefig('.\/img_eda_reduced_user_nb_days_log_scale.png', transparent=True)","99469b57":"reduced_train = pd.merge(train, reduced_user_info, on='user_id')\n\ncol = train.columns\nreduced_train = reduced_train[col]\nreduced_train","3a7eeb04":"reduced_train_rate = round((1 - len(reduced_train) \/ len(train)) * 100)\nprint(f'Interactions were reduced by {reduced_train_rate} %.')","db58b0e6":"reduced_train.to_pickle(\".\/reduced_riiid_train.pkl.gzip\")","ea650536":"help_usage = reduced_train[reduced_train.answered_correctly == -1].groupby('user_id')['content_id'].count()\nhelp_usage = help_usage.reset_index().rename(columns={'content_id': 'help_usage'})\nhelp_usage","5d342cf5":"help_usage.describe()","c2efc1c0":"sns.boxplot(y='help_usage', data=help_usage)\nplt.title('Help usage of users')\nplt.savefig('.\/img_eda_help_usage.png', transparent=True)","9835e528":"help_usage['binned_help_usage'] = pd.cut(help_usage['help_usage'], bins=[-np.inf, 0, 1, 3, np.inf], labels=[0, 1, 2, 3])\nhelp_usage","62639418":"# Export file for submissions\ncol = ['user_id', 'binned_help_usage']\nhelp_usage = help_usage[col]\nhelp_usage[col].to_csv('.\/help.csv', index=False)\n\n# Delete conferences\nreduced_train = reduced_train[reduced_train.content_type_id == 0]\nreduced_train.shape","c2273004":"path = \"..\/input\/riiid-test-answer-prediction\/questions.csv\"\nquestions = pd.read_csv(path)\nquestions.info()","5f71f7d6":"col = ['question_id','part']\nquestions = questions[col]\nquestions = questions.rename(columns={'question_id': 'content_id'})","1ca225ac":"# Listening (0) or reading (1)\nquestions['L | R'] = pd.cut(questions['part'], bins=[-np.inf, 4, np.inf], labels=['Listening', 'Reading'])\nquestions.sample(10)","258edc4c":"questions['Difficulty_level'] = 0\nquestions['Difficulty_level'][questions['L | R'] == 'Listening'] = questions['part'][questions['L | R'] == 'Listening']\nquestions['Difficulty_level'][questions['L | R'] == 'Reading'] = questions['part'][questions['L | R'] == 'Reading'] - 4\n\nquestions.sample(10)","e74bb17b":"questions.to_csv('.\/level_content.csv', index=False)","9598822c":"featured_train = pd.merge(reduced_train, help_usage, on='user_id', how='left')\nfeatured_train = pd.merge(featured_train, questions, on='content_id', how='left')\n# Users with no help usage equals 0\nfeatured_train.binned_help_usage = featured_train.binned_help_usage.fillna(0)","ecf1edf3":"%%time\n\nagg = {'row_id' : 'count',\n       'answered_correctly' : lambda x: round(x.mean() * 100),\n       'prior_question_had_explanation' : lambda x: round(x.mean() * 100)}\n\npart_info = featured_train.groupby('part').agg(agg)\npart_info = part_info.rename(columns={'row_id' : 'nb_interactions',\n                                   'answered_correctly' : 'score'})\npart_info","0743594c":"norm = matplotlib.colors.Normalize(vmin=min(part_info.score), vmax=max(part_info.score))\ncolors = [matplotlib.cm.Blues(norm(value)) for value in part_info.score]\nsquarify.plot(sizes=part_info.nb_interactions, color=colors, label=part_info.index, alpha=0.8)\nplt.title(\"Part of TOEIC test\")\nplt.axis('off')\nplt.savefig('.\/img_part_toeic_test_repartitions.png', transparent=True)\nplt.show()","4d4712a5":"featured_train.groupby(\"L | R\")['row_id'].count().reset_index().plot.bar(x='L | R', y='row_id')\nplt.title('Interactions of Listening or Reading')\nplt.savefig('.\/img_part_LR_repartitions.png',\n            transparent=True,\n            bbox_inches=\"tight\")\n#sns.barplot(y=, data=featured_train)","cde415e5":"featured_train.groupby('binned_help_usage')['row_id'].count().reset_index().plot.bar(x='binned_help_usage', y='row_id')\nplt.title('Users by help usage group ')\nplt.savefig('.\/img_help_usage_repartitions.png', transparent=True)","c47dda16":"col = ['timestamp', 'user_id', 'content_id',\n       'task_container_id', 'binned_help_usage', 'L | R', 'Difficulty_level', 'answered_correctly']\nfeatured_train = featured_train[col]\nfeatured_train.sample(20)","3ab4c5ef":"featured_train.to_pickle(\".\/featured_riiid_train.pkl.gzip\")","de91716a":"# 1. Import of training data and preliminary exploration","53c9f774":"### Content","5dd3d6d4":"I am using the dataset compressed in this [kernel](https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets\/)\n\nThanks to Rohan ! \ud83d\ude09","64578047":"The training data set is substantial; more than 100 million lines \ud83d\ude32\ud83d\ude32 It is necessary to conduct a deep exploration and possibly a deep cleansing. I focus the cleaning on the users.\n\n\ud83d\udc94 Do not remove interactions no matter how \ud83d\udc94","83518568":"# 3. Features engineering\n## 3-1. Help usage\n\nSome of the interactions are considered as conferences, no response is required from the user (answered_correctly = -1). I remove them from the data set.\n\nBefore, it is interesting to extract a feature on the level of use of the help.","a90238e7":"The weaker the colors, the lower the score.\nPart 5 is the most worked on in the data, but is the one with the lowest score.\nThis is also where users require the most explanation.","5a0906b9":"## 2-2. Number of interactions per user","884d8946":"## 2-4. Apply filters","27b84e82":"## Cleaning users \ud83d\udca5 Features engineering\n\nHello Kagglers**** \ud83d\udd90\ud83d\udd90 \n\nHere is my approach \u23ec\u23ec\u23ec","ef3ab961":"Distributions are less compressed \u2757 I apply cleansing to the train data.","39af9382":"## 3-2. Content difficult\n\n\nThe TOEIC exam is classified into 2 parts; listening and reading. Each part has difficulty groups, it is interesting to extract this information.","62a0a897":"### Parts","07438e1a":"# 2. Cleaning user\n## 2-1. Building the user dataframe","0803aa29":"# 4. Reduced train dataset export","0c5a6faf":"Here I see big differences especially from the 95th percentile \ud83d\udca5\ud83d\udca5 It is difficult for me to be able to generalize a model from these data, it must be excluded!","b3eceb29":"If you made it to the point thank you for reading and \ud83c\udd99 vote if it helped you. \nI will read your comments with pleasure.\n\nOooh ! And i'm soory for my perfectible english !\n\nThanks \ud83d\ude37 Stay at home \ud83d\ude37","a620feaa":"I count the number of interactions for a user and i recover the number of days in the targeted interaction and the first. I calculate the score \ud83d\udcaf for informations","ca1c804f":"## 3-3. New features exploration","92efee97":"## 2-3. Number of days between the first interaction and the last","93d399c7":"## Baseline Coming soon","fe317ee9":"### Level usage","ed9d6309":"Here, too, it is advisable to clean \u2705"}}