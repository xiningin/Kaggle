{"cell_type":{"2d3a2284":"code","a34f3cab":"code","c8bd9e86":"code","b8b5f825":"code","436aec51":"code","dc096b0c":"code","732a9017":"code","d57ea478":"code","ffc562e1":"code","7eaf9303":"code","128f858a":"code","4e12640a":"code","d8e7ac62":"code","d758a20a":"code","e15dcfb5":"code","c405f629":"code","6112c6b3":"code","b5859f6b":"code","c7776fed":"code","4cd7196a":"code","ed737b84":"code","acef1f1d":"code","5c046d06":"code","da563a62":"code","3223dcff":"code","b1dea061":"code","a2ee4af4":"code","a0082d3d":"code","419199ef":"code","5cc2416e":"code","a5446389":"code","dbdc1613":"code","24bb711c":"code","c6a381e2":"code","4082ab99":"code","dfaac7e4":"code","619fbdea":"code","38a5be3e":"code","5a85c8e9":"code","e3d42ef2":"code","a506b089":"code","d4af98d3":"code","a3e6653f":"markdown","af7ce0cb":"markdown","a649885e":"markdown","d47da8d4":"markdown","54c5fa19":"markdown","a9ac9070":"markdown","18b117fe":"markdown","adfbea7f":"markdown","393f6e63":"markdown","5980c175":"markdown","fab13ff4":"markdown","20a2df50":"markdown","2f0d6473":"markdown","3a306f51":"markdown","180b1c6e":"markdown","d2cb2414":"markdown","907768b2":"markdown","851f96cc":"markdown","e17765b0":"markdown","10c5a33a":"markdown","e6e36b73":"markdown","dc8b0263":"markdown","beed9db9":"markdown","ccb4c1aa":"markdown","fb828868":"markdown","28dfa53b":"markdown","215231cc":"markdown","7dfeb9f5":"markdown","7d50b96e":"markdown","d4920602":"markdown","d24b1b28":"markdown","3e167ef1":"markdown","9a30f799":"markdown","5d29463f":"markdown","8da096be":"markdown","e1428365":"markdown","d87d5db1":"markdown","af3c4c4b":"markdown","bd92ac57":"markdown","ef8bcfae":"markdown","6492f209":"markdown","970b6aa6":"markdown","067b1c2e":"markdown","44742331":"markdown","dc4a5fd1":"markdown","a6265f1d":"markdown","bb8bebd1":"markdown","e254a51c":"markdown","d91f3d0f":"markdown","09232762":"markdown","19fb46b7":"markdown","cbfbf0a4":"markdown","1a900c20":"markdown","9b0551cb":"markdown","8a43b649":"markdown","e700f77a":"markdown","a301456b":"markdown","f6cf08a0":"markdown","0decfee5":"markdown","eaacf041":"markdown","5fc9b427":"markdown","b6b96161":"markdown","f3d3d244":"markdown","8c177c7e":"markdown","2bc9c1a3":"markdown","44f8fdd1":"markdown","9c923b95":"markdown","fc430325":"markdown","623103c7":"markdown","1ddf8c94":"markdown","b050a282":"markdown","d198bd2e":"markdown","6151bf73":"markdown"},"source":{"2d3a2284":"# Standard libraries\nimport pandas as pd\nimport numpy as np\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\npd.set_option('display.max_columns', 500)\nfrom collections import Counter\nfrom PIL import Image\n\n# Viz libs\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom matplotlib.gridspec import GridSpec\nfrom mpl_toolkits.axes_grid.inset_locator import InsetPosition\nimport folium\nfrom folium.plugins import HeatMap, FastMarkerCluster\nfrom wordcloud import WordCloud\n\n# Geolocation libs\nfrom geopy.geocoders import Nominatim\n\n# Utils modules\nfrom custom_transformers import *\nfrom viz_utils import *\nfrom ml_utils import *\n\n# ML libs\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport shap","a34f3cab":"# Reading restaurants data\ndata_path = r'..\/input\/zomato-bangalore-restaurants\/zomato.csv'\ndf_restaurants = import_data(path=data_path, n_lines=5000)\n\n# Results\nprint(f'Dataset shape: {df_restaurants.shape}')\ndf_restaurants.head()","c8bd9e86":"# An overview from the data\ndf_overview = data_overview(df_restaurants)\ndf_overview","b8b5f825":"# Changing the data type from approx_cost columns\ndf_restaurants['approx_cost'] = df_restaurants['approx_cost(for two people)'].astype(str).apply(lambda x: x.replace(',', ''))\ndf_restaurants['approx_cost'] = df_restaurants['approx_cost'].astype(float)\n\n# Extracting the rate in a float column\ndf_restaurants['rate_num'] = df_restaurants['rate'].astype(str).apply(lambda x: x.split('\/')[0])\nwhile True:\n    try:\n        df_restaurants['rate_num'] = df_restaurants['rate_num'].astype(float)\n        break\n    except ValueError as e1:\n        noise_entry = str(e1).split(\":\")[-1].strip().replace(\"'\", \"\")\n        print(f'Threating noisy entrance on rate: {noise_entry}')\n        df_restaurants['rate_num'] = df_restaurants['rate_num'].apply(lambda x: x.replace(noise_entry, str(np.nan)))\n\n# Dropping old columns\ndf_restaurants.drop(['approx_cost(for two people)', 'rate'], axis=1, inplace=True)\ndf_restaurants.head()","436aec51":"# Building a figure\nfig = plt.figure(constrained_layout=True, figsize=(15, 9))\n\n# Axis definition with GridSpec\ngs = GridSpec(1, 3, figure=fig)\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[0, 1:3])\n\n# Axis 1 - Big Number for total restaurants and total types in the data\ntotal_restaurants = len(df_restaurants)\ntotal_types = len(df_restaurants['rest_type'].value_counts())\nax1.text(0.00, 0.75, 'There are', fontsize=14, ha='center')\nax1.text(0.00, 0.63, f'{total_restaurants}', fontsize=64, color='orange', ha='center')\nax1.text(0, 0.59, 'restaurants in Bengaluru divided into', fontsize=14, ha='center')\nax1.text(0.00, 0.43, total_types, fontsize=44, ha='center', color='orange', style='italic', weight='bold',\n         bbox=dict(facecolor='gold', alpha=0.5, pad=14, boxstyle='round, pad=.7'))\nax1.text(0, 0.39, 'different types', fontsize=14, ha='center')\nax1.axis('off')\n\n# Axis 2 - Total number of restaurants per type (Top N)\ntop = 10\nsingle_countplot(df_restaurants, ax2, x='rest_type', top=top)\nax2.set_title(f'Top {top} Restaurants Type in Bengaluru', color='dimgrey', size=18)\nfor tick in ax2.get_xticklabels():\n    tick.set_rotation(25)\n    \n# Axis 3 - Representative of the top two restaurant type\ndf_restaurants['top_types'] = df_restaurants['rest_type'].apply(lambda x: 'Quick Bites + Casual Dining' if x in ('Quick Bites', 'Casual Dining') else 'Other')\nax3 = plt.axes([0, 0, 1, 1])\nip = InsetPosition(ax2, [0.57, 0.3, 0.6, 0.65])\nax3.set_axes_locator(ip)\ndonut_plot(df_restaurants, col='top_types', ax=ax3, colors=['darkslateblue', 'silver'], title='')","dc096b0c":"popular_franchises = df_restaurants.groupby(by='name', as_index=False).agg({'votes': 'sum',\n                                                                            'url': 'count',\n                                                                            'approx_cost': 'mean',\n                                                                            'rate_num': 'mean'})\npopular_franchises.columns = ['name', 'total_votes', 'total_unities', 'mean_approx_cost', 'mean_rate_num']\npopular_franchises['votes_per_unity'] = popular_franchises['total_votes'] \/ popular_franchises['total_unities']\npopular_franchises = popular_franchises.sort_values(by='total_unities', ascending=False)\npopular_franchises = popular_franchises.loc[:, ['name', 'total_unities', 'total_votes', 'votes_per_unity',\n                                                'mean_approx_cost', 'mean_rate_num']]\n\n# Correcting a restaurant name\nbug_name = 'Sant\u00c3\\x83\u00c2\\x83\u00c3\\x82\u00c2\\x83\u00c3\\x83\u00c2\\x82\u00c3\\x82\u00c2\\x83\u00c3\\x83\u00c2\\x83\u00c3\\x82\u00c2\\x82\u00c3\\x83\u00c2\\x82\u00c3\\x82\u00c2\\x83\u00c3\\x83\u00c2\\x83\u00c3\\x82\u00c2\\x83\u00c3\\x83\u00c2\\x82\u00c3\\x82\u00c2\\x82\u00c3\\x83\u00c2\\x83\u00c3\\x82\u00c2\\x82\u00c3\\x83\u00c2\\x82\u00c3\\x82\u00c2\u00a9 Spa Cuisine'\npopular_franchises['name'] = popular_franchises['name'].apply(lambda x: 'Santa Spa Cusisine' if x == bug_name else x)\n\n\npopular_franchises.head(10)","732a9017":"# Creating a figure por restaurants overview analysis\nfig, axs = plt.subplots(3, 3, figsize=(15, 15))\n\n# Plot Pack 01 - Most popular restaurants (votes)\nsns.barplot(x='total_votes', y='name', data=popular_franchises.sort_values(by='total_votes', ascending=False).head(),\n            ax=axs[1, 0], palette='plasma')\naxs[1, 0].set_title('Top 5 Most Voted Restaurants', size=12)\nsns.barplot(x='total_votes', y='name', \n            data=popular_franchises.sort_values(by='total_votes', ascending=False).query('total_votes > 0').tail(),\n            ax=axs[2, 0], palette='plasma_r')\naxs[2, 0].set_title('Top 5 Less Voted Restaurants\\n(with at least 1 vote)', size=12)\nfor ax in axs[1, 0], axs[2, 0]:\n    ax.set_xlabel('Total Votes')\n    ax.set_xlim(0, popular_franchises['total_votes'].max())\n    format_spines(ax, right_border=False)\n    ax.set_ylabel('')\n\n# Annotations\naxs[0, 0].text(0.50, 0.30, int(popular_franchises.total_votes.mean()), fontsize=45, ha='center')\naxs[0, 0].text(0.50, 0.12, 'is the average of votes', fontsize=12, ha='center')\naxs[0, 0].text(0.50, 0.00, 'received by restaurants', fontsize=12, ha='center')\naxs[0, 0].axis('off')\n\n# Plot Pack 02 - Cost analysis\nsns.barplot(x='mean_approx_cost', y='name', data=popular_franchises.sort_values(by='mean_approx_cost', ascending=False).head(),\n            ax=axs[1, 1], palette='plasma')\naxs[1, 1].set_title('Top 5 Most Expensives Restaurants', size=12)\nsns.barplot(x='mean_approx_cost', y='name', \n            data=popular_franchises.sort_values(by='mean_approx_cost', ascending=False).query('mean_approx_cost > 0').tail(),\n            ax=axs[2, 1], palette='plasma_r')\naxs[2, 1].set_title('Top 5 Less Expensive Restaurants', size=12)\nfor ax in axs[1, 1], axs[2, 1]:\n    ax.set_xlabel('Avg Approx Cost')\n    ax.set_xlim(0, popular_franchises['mean_approx_cost'].max())\n    format_spines(ax, right_border=False)\n    ax.set_ylabel('')\n\n# Annotations\naxs[0, 1].text(0.50, 0.30, round(popular_franchises.mean_approx_cost.mean(), 2), fontsize=45, ha='center')\naxs[0, 1].text(0.50, 0.12, 'is mean approx cost', fontsize=12, ha='center')\naxs[0, 1].text(0.50, 0.00, 'for Bengaluru restaurants', fontsize=12, ha='center')\naxs[0, 1].axis('off')\n\n# Plot Pack 03 - Rate analysis\nsns.barplot(x='mean_rate_num', y='name', data=popular_franchises.sort_values(by='mean_rate_num', ascending=False).head(),\n            ax=axs[1, 2], palette='plasma')\naxs[1, 2].set_title('Top 5 Rstaurants with Highest Rates', size=12)\nsns.barplot(x='mean_rate_num', y='name', \n            data=popular_franchises.sort_values(by='mean_rate_num', ascending=False).query('mean_rate_num > 0').tail(),\n            ax=axs[2, 2], palette='plasma_r')\naxs[2, 2].set_title('Top 5 Restaurants with Lowest Rate', size=12)\nfor ax in axs[1, 2], axs[2, 2]:\n    ax.set_xlabel('Avg Rate')\n    ax.set_xlim(0, popular_franchises['mean_rate_num'].max())\n    format_spines(ax, right_border=False)\n    ax.set_ylabel('')\n\n# Annotations\naxs[0, 2].text(0.50, 0.30, round(popular_franchises.mean_rate_num.mean(), 2), fontsize=45, ha='center')\naxs[0, 2].text(0.50, 0.12, 'is mean rate given by customers', fontsize=12, ha='center')\naxs[0, 2].text(0.50, 0.00, 'for Bengaluru restaurants', fontsize=12, ha='center')\naxs[0, 2].axis('off')\n\nplt.tight_layout()\nplt.suptitle('The Best and the Worst Restaurants to Visit in Bengaluru', size=16)\nplt.show()","d57ea478":"fig, axs = plt.subplots(1, 2, figsize=(15, 10))\ndonut_plot(df_restaurants, col='book_table', colors=['crimson', 'mediumseagreen'], ax=axs[0], \n           title='Book Table Service in Bengaluru')\ndonut_plot(df_restaurants, col='online_order', colors=['darkslateblue', 'lightsalmon'], ax=axs[1], \n           title='Online Order Service in Bengaluru')","ffc562e1":"# Building a figure\nfig = plt.figure(constrained_layout=True, figsize=(15, 12))\n\n# Axis definition with GridSpec\ngs = GridSpec(2, 5, figure=fig)\nax2 = fig.add_subplot(gs[0, :3])\nax3 = fig.add_subplot(gs[0, 3:])\nax4 = fig.add_subplot(gs[1, :3])\nax5 = fig.add_subplot(gs[1, 3:])\n\n# First Line (01) - Rate\nsns.kdeplot(df_restaurants.query('rate_num > 0 & book_table == \"Yes\"')['rate_num'], ax=ax2,\n             color='mediumseagreen', shade=True, label='With Book Table Service')\nsns.kdeplot(df_restaurants.query('rate_num > 0 & book_table == \"No\"')['rate_num'], ax=ax2,\n             color='crimson', shade=True, label='Without Book Table Service')\nax2.set_title('Restaurants Rate Distribution by Book Table Service Offer', color='dimgrey', size=14)\nsns.boxplot(x='book_table', y='rate_num', data=df_restaurants, palette=['mediumseagreen', 'crimson'], ax=ax3)\nax3.set_title('Box Plot for Rate and Book Table Service', color='dimgrey', size=14)\n\n# First Line (01) - Cost\nsns.kdeplot(df_restaurants.query('approx_cost > 0 & book_table == \"Yes\"')['approx_cost'], ax=ax4,\n             color='mediumseagreen', shade=True, label='With Book Table Service')\nsns.kdeplot(df_restaurants.query('approx_cost > 0 & book_table == \"No\"')['approx_cost'], ax=ax4,\n             color='crimson', shade=True, label='Without Book Table Service')\nax4.set_title('Restaurants Approx Cost Distribution by Book Table Service Offer', color='dimgrey', size=14)\nsns.boxplot(x='book_table', y='approx_cost', data=df_restaurants, palette=['mediumseagreen', 'crimson'], ax=ax5)\nax5.set_title('Box Plot for Cost and Book Table Service', color='dimgrey', size=14)\n\n\n# Customizing plots\nfor ax in [ax2, ax3, ax4, ax5]:\n    format_spines(ax, right_border=False)\n    \nplt.tight_layout()","7eaf9303":"# Building a figure\nfig = plt.figure(constrained_layout=True, figsize=(15, 12))\n\n# Axis definition with GridSpec\ngs = GridSpec(2, 5, figure=fig)\nax2 = fig.add_subplot(gs[0, :3])\nax3 = fig.add_subplot(gs[0, 3:])\nax4 = fig.add_subplot(gs[1, :3])\nax5 = fig.add_subplot(gs[1, 3:])\n\n# First Line (01) - Rate\nsns.kdeplot(df_restaurants.query('rate_num > 0 & online_order == \"Yes\"')['rate_num'], ax=ax2,\n             color='darkslateblue', shade=True, label='With Online Order Service')\nsns.kdeplot(df_restaurants.query('rate_num > 0 & online_order == \"No\"')['rate_num'], ax=ax2,\n             color='lightsalmon', shade=True, label='Without Online Order Service')\nax2.set_title('Restaurants Rate Distribution by Online Order Service Offer', color='dimgrey', size=14)\nsns.boxplot(x='online_order', y='rate_num', data=df_restaurants, palette=['darkslateblue', 'lightsalmon'], ax=ax3)\nax3.set_title('Box Plot for Rate and Online Order Service', color='dimgrey', size=14)\n\n# First Line (01) - Cost\nsns.kdeplot(df_restaurants.query('approx_cost > 0 & online_order == \"Yes\"')['approx_cost'], ax=ax4,\n             color='darkslateblue', shade=True, label='With Online Order Service')\nsns.kdeplot(df_restaurants.query('approx_cost > 0 & book_table == \"No\"')['approx_cost'], ax=ax4,\n             color='lightsalmon', shade=True, label='Without Online Order Service')\nax4.set_title('Restaurants Approx Cost Distribution by Online Order Service Offer', color='dimgrey', size=14)\nsns.boxplot(x='online_order', y='approx_cost', data=df_restaurants, palette=['darkslateblue', 'lightsalmon'], ax=ax5)\nax5.set_title('Box Plot for Cost and Online Order Service', color='dimgrey', size=14)\n\n\n# Customizing plots\nfor ax in [ax2, ax3, ax4, ax5]:\n    format_spines(ax, right_border=False)\n    \nplt.tight_layout()","128f858a":"# Grouping data into location\ngood_ones = df_restaurants.groupby(by='location', as_index=False).agg({'votes': 'sum',\n                                                                       'url': 'count',\n                                                                       'approx_cost': 'mean',\n                                                                       'rate_num': 'mean'})\ngood_ones.columns = ['location', 'total_votes', 'total_unities', 'mean_approx_cost', 'mean_rate_num']\ngood_ones['votes_per_unity'] = good_ones['total_votes'] \/ good_ones['total_unities']\ngood_ones = good_ones.sort_values(by='total_unities', ascending=False)\ngood_ones = good_ones.loc[:, ['location', 'total_unities', 'total_votes', 'votes_per_unity',\n                                                'mean_approx_cost', 'mean_rate_num']]\ngood_ones.head(10)","4e12640a":"# Creating a figure por restaurants overview analysis\nfig, axs = plt.subplots(3, 3, figsize=(15, 15))\nlist_cols = ['total_votes', 'mean_approx_cost', 'mean_rate_num']\n\n# PLotting best and worst by grouped data\nanswear_plot(grouped_data=good_ones, grouped_col='location', axs=axs, list_cols=list_cols, top=10, palette='magma')\n\n# Finishing the chart\nplt.suptitle('Where Are the Top Restaurants in Bengaluru?', size=16)\nplt.tight_layout()\nplt.show()","d8e7ac62":"# Grouping data by city\ncity_group = df_restaurants.groupby(by='listed_in(city)', as_index=False).agg({'rate_num': 'mean',\n                                                                               'approx_cost': 'mean'})\ncity_group.sort_values(by='rate_num', ascending=False, inplace=True)\n\n# Ploting\nfig, ax = plt.subplots(figsize=(15, 8))\nsns.barplot(x='listed_in(city)', y='approx_cost', data=city_group, palette='cividis', \n            order=city_group['listed_in(city)'])\nax2 = ax.twinx()\nsns.lineplot(x='listed_in(city)', y='rate_num', data=city_group, color='gray', ax=ax2, sort=False)\n\n# Labeling line chart (rate)\nxs = np.arange(0, len(city_group), 1)\nys = city_group['rate_num']\nfor x,y in zip(xs, ys):\n    label = \"{:.2f}\".format(y)\n    plt.annotate(label, # this is the text\n                 (x,y), # this is the point to label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center', # horizontal alignment can be left, right or center\n                 color='black')\n    \n# Labeling bar chart (cost)\nfor p in ax.patches:\n    x = p.get_bbox().get_points()[:, 0]\n    y = p.get_bbox().get_points()[1, 1]\n    ax.annotate('{}'.format(int(y)), (x.mean(), 15), va='bottom', rotation='vertical', color='white', \n                fontweight='bold')\n\n# Customizing chart\nformat_spines(ax)\nformat_spines(ax2)\nax.tick_params(axis='x', labelrotation=90)\nax.set_title('Bengaluru Cities and all its Restaurants by Approx Cost (bars) and Rate (line)')\nplt.show()","d758a20a":"# Extracting lat and long from the restaurant city using an API service\ngeolocator = Nominatim(user_agent=\"Y_BzShFZceZ_rj_t-cI13w\")\n\n# Creating a auxiliar dataset with cities location (reducing the API calls and time consuming by consequence)\ncities_aux = pd.DataFrame(df_restaurants['listed_in(city)'].value_counts())\ncities_aux.reset_index(inplace=True)\ncities_aux.columns = ['city', 'total_restaurants']\n\n# Extracting cities lat and long features\ncities_aux['lat'] = cities_aux['city'].apply(lambda x: geolocator.geocode(x)[1][0])\ncities_aux['lng'] = cities_aux['city'].apply(lambda x: geolocator.geocode(x)[1][1])\n\n# Adding more features do further analysis\ncity_group = df_restaurants.groupby(by='listed_in(city)', as_index=False).agg({'votes': 'sum',\n                                                                               'approx_cost': 'mean',\n                                                                               'rate_num': 'mean'})\ncity_group.columns = ['city', 'total_votes', 'avg_approx_cost', 'avg_rate_num']\n\n# Creating an unique city data\ncities_aux = cities_aux.merge(city_group, how='left', on='city')\n\n# Merging the original data to the grouped cities lat and long\ndf_restaurants = df_restaurants.merge(cities_aux, how='left', left_on='listed_in(city)', right_on='city')\ndf_restaurants.drop(['city', 'total_restaurants'], axis=1, inplace=True)\n\n# Results on cities grouped data\ncities_aux","e15dcfb5":"# Zipping locations for folium map\nlocations = list(zip(df_restaurants['lat'].values, df_restaurants['lng'].values))\n\n# Creating a map using folium\nmap1 = folium.Map(\n    location=[12.97, 77.63],\n    zoom_start=11.5\n)\n\n# Plugin: FastMarkerCluster\nFastMarkerCluster(data=locations).add_to(map1)\n\nmap1","c405f629":"map1 = folium.Map(\n    location=[12.97, 77.63],\n    zoom_start=11.0,\n    tiles='cartodbdark_matter'\n)\n\nHeatMap(\n    data=cities_aux.loc[:, ['lat', 'lng', 'avg_rate_num']],\n    radius=35\n).add_to(map1)\n\nmap1","6112c6b3":"rest_types = list(df_restaurants['listed_in(type)'].value_counts().index)\ncolors = ['darkslateblue', 'mediumseagreen', 'gray', 'salmon', 'cornflowerblue', 'cadetblue', 'gold']\n\nfig, axs = plt.subplots(2, 1, figsize=(17, 15))\nfor r_type in rest_types:\n    idx = rest_types.index(r_type)\n    kde_data = df_restaurants[(df_restaurants['rate_num'] > 0) & (df_restaurants['listed_in(type)'] == r_type)]\n    sns.kdeplot(kde_data['rate_num'], ax=axs[0], color=colors[idx], shade=True, label=r_type)\n    sns.kdeplot(kde_data['approx_cost'], ax=axs[1], color=colors[idx], shade=True, label=r_type)\n\n# Customizing charts\naxs[0].set_title('Distribution of Rate by Restaurant Type', color='dimgrey', size=18)\naxs[1].set_title('Distribution of Approx Cost by Restaurant Type', color='dimgrey', size=18)\nfor ax in axs:\n    format_spines(ax, right_border=False)\nplt.tight_layout()","b5859f6b":"# Creating a list with all options available\ncuisines = list(df_restaurants['cuisines'].astype(str).values)\ncuisines_word_list = []\nfor lista in [c.split(',') for c in cuisines]:\n    for word in lista:\n        cuisines_word_list.append(word.strip())\n        \n# Creating a Counter for unique options and generating the wordcloud\ncuisines_wc_dict = Counter(cuisines_word_list)\n\nwordcloud = WordCloud(width=1280, height=720, collocations=False, random_state=42, \n                      colormap='magma', background_color='white').generate_from_frequencies(cuisines_wc_dict)\n\n# Visualizing the WC created and the total for each cuisine\nfig, axs = plt.subplots(1, 2, figsize=(20, 12))\nax1 = axs[0]\nax2 = axs[1]\nax1.imshow(wordcloud)\nax1.axis('off')\nax1.set_title('WordCloud for Cuisines Available on Bengaluru Restaurants', size=18, pad=20)\n\n# Total for each cuisine\ndf_cuisines = pd.DataFrame()\ndf_cuisines['cuisines'] = cuisines_wc_dict.keys()\ndf_cuisines['amount'] = cuisines_wc_dict.values()\ndf_cuisines.sort_values(by='amount', ascending=False, inplace=True)\nsns.barplot(x='cuisines', y='amount', data=df_cuisines.head(10), palette='magma', ax=ax2)\nformat_spines(ax2, right_border=False)\nax2.set_title('Top 10 Cuisines in Bengaluru Restaurants', size=18)\n\n# Customizing chart\nncount = df_cuisines['amount'].sum()\nx_ticks = [item.get_text() for item in ax2.get_xticklabels()]\nax2.set_xticklabels(x_ticks, rotation=45, fontsize=14)\nfor p in ax2.patches:\n    x = p.get_bbox().get_points()[:, 0]\n    y = p.get_bbox().get_points()[1, 1]\n    ax2.annotate('{}\\n{:.1f}%'.format(int(y), 100. * y \/ ncount), (x.mean(), y), fontsize=14, ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()","c7776fed":"# Creating a list with all options available\ndishes = list(df_restaurants['dish_liked'].dropna().astype(str).values)\ndishes_word_list = []\nfor lista in [c.split(',') for c in dishes]:\n    for word in lista:\n        dishes_word_list.append(word.strip())\n        \n# Creating a Counter for unique options and generating the wordcloud\ndished_wc_dict = Counter(dishes_word_list)\n\n# Reading and preparing a mask for serving as wordcloud background\nfood_mask = np.array(Image.open(\"..\/input\/img-icons\/delivery_icon.png\"))\nfood_mask = food_mask[:, :, -1]\ntransf_mask = np.ndarray((food_mask.shape[0], food_mask.shape[1]), np.int32)\nfor i in range(len(food_mask)):\n    transf_mask[i] = [255 if px == 0 else 0 for px in food_mask[i]]\n\n# Generating the wordcloud    \nwordcloud = WordCloud(width=1000, height=500, collocations=False, random_state=42, colormap='rainbow', \n                      background_color='black', mask=transf_mask).generate_from_frequencies(dished_wc_dict)\n\n# Visualizing the WC created\nplt.figure(figsize=(20, 17))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('WordCloud for Dish Liked on Bengaluru Restaurants', size=18, pad=20)\nplt.show()","4cd7196a":"# Splitting restaurants\ndf_restaurants['rated'] = df_restaurants['rate_num'].apply(lambda x: 1 if x >= 0 else 0)\nnew_restaurants = df_restaurants.query('rated == 0')\ntrain_val_restaurants = df_restaurants.query('rated == 1')\n\n# PLotting a donut chart for seeing the distribution\nfig, ax = plt.subplots(figsize=(10, 10))\ndonut_plot(df_restaurants, col='rated', ax=ax, label_names=['Rated', 'New or Not Rated'], \n           colors=['darkslateblue', 'silver'], title='Amount of Rated and Non Rated Restaurants',\n           text=f'Total of Restaurants:\\n{len(df_restaurants)}')","ed737b84":"# Defining a custom threshold for splitting restaurants into good and bad\nthreshold = 3.75\ntrain_val_restaurants['target'] = train_val_restaurants['rate_num'].apply(lambda x: 1 if x >= threshold else 0)\n\n# Donut chart\nfig, ax = plt.subplots(figsize=(10, 10))\nlabel_names = ['Bad' if target == 0 else 'Good' for target in train_val_restaurants['target'].value_counts().index]\ncolor_list = ['salmon' if label == 'Bad' else 'cadetblue' for label in label_names]\ndonut_plot(train_val_restaurants, col='target', ax=ax, label_names=label_names, \n           colors=color_list, title='Amount of Good and Bad Restaurants \\n(given the selected threshold)',\n           text=f'Total of Restaurants:\\n{len(train_val_restaurants)}\\n\\nThreshold: \\n{threshold}')","acef1f1d":"# Selecting initial features\ninitial_features = ['online_order', 'book_table', 'location', 'rest_type', 'cuisines', \n                    'listed_in(type)', 'listed_in(city)', 'approx_cost', 'target']\ntrain_val_restaurants = train_val_restaurants.loc[:, initial_features]\n\n# Extracting new features\ntrain_val_restaurants['multiple_types'] = train_val_restaurants['rest_type'].astype(str).apply(lambda x: len(x.split(',')))\ntrain_val_restaurants['total_cuisines'] = train_val_restaurants['cuisines'].astype(str).apply(lambda x: len(x.split(',')))\n\n# Dropping another ones\ntrain_val_restaurants.drop('cuisines', axis=1, inplace=True)\ntrain_val_restaurants.head()","5c046d06":"# Splitting the data\nX = train_val_restaurants.drop('target', axis=1)\ny = train_val_restaurants['target'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42)","da563a62":"# Splitting features by data type\ncat_features= [col for col, dtype in X_train.dtypes.items() if dtype == 'object']\nnum_features = [col for col, dtype in X_train.dtypes.items() if dtype != 'object']\n\n# Apply encoding for categorical features\nX_train_cat = X_train[cat_features]\nfor col in cat_features:\n    col_encoded = pd.get_dummies(X_train_cat[col], prefix=col, dummy_na=True)\n    X_train_cat = X_train_cat.merge(col_encoded, left_index=True, right_index=True)\n    X_train_cat.drop(col, axis=1, inplace=True)\n    \nprint(f'Total categorical features after encoding: {X_train_cat.shape[1]}')","3223dcff":"# Class for applying initial prep on key columns\nclass PrepareCostAndRate(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # Extracting the approx cost feature\n        X['approx_cost'] = X['approx_cost(for two people)'].astype(str).apply(lambda x: x.replace(',', '.'))\n        X['approx_cost'] = X['approx_cost'].astype(float)\n        \n        # Extracting the rate feature\n        X['rate_num'] = X['rate'].astype(str).apply(lambda x: x.split('\/')[0])\n        while True:\n            try:\n                X['rate_num'] = X['rate_num'].astype(float)\n                break\n            except ValueError as e1:\n                noise_entry = str(e1).split(\":\")[-1].strip().replace(\"'\", \"\")\n                #print(f'Threating noisy entrance on rate feature: {noise_entry}')\n                X['rate_num'] = X['rate_num'].apply(lambda x: x.replace(noise_entry, str(np.nan)))              \n        \n        return X\n\n# Class for selection the initial features\nclass InitialFeatureSelection(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, initial_features=['online_order', 'book_table', 'location', 'rest_type', 'cuisines', \n                                         'listed_in(type)', 'listed_in(city)', 'approx_cost', 'rate_num']):\n        self.initial_features = initial_features\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X[self.initial_features]\n                \n# Class for creating some features\nclass RestaurantAdditionalFeatures(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, multiples_types=True, total_cuisines=True, top_locations=10, top_cities=10, top_types=10):\n        self.multiples_types = multiples_types\n        self.total_cuisines = total_cuisines\n        self.top_locations = top_locations\n        self.top_cities = top_cities\n        self.top_types = top_types\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        \n        # Adding features based on counting of restaurant types and cuisines\n        if self.multiples_types:\n            X['multiple_types'] = X['rest_type'].astype(str).apply(lambda x: len(x.split(',')))\n        if self.total_cuisines:\n            X['total_cuisines'] = X['cuisines'].astype(str).apply(lambda x: len(x.split(',')))\n            X.drop('cuisines', axis=1, inplace=True)\n            \n        # Creating for features for reducing granularity on location\n        main_locations = list(X['location'].value_counts().index)[:self.top_locations]\n        X['location_feature'] = X['location'].apply(lambda x: x if x in main_locations else 'Other')\n        X.drop('location', axis=1, inplace=True)\n        \n        # Creating for features for reducing granularity on city\n        main_cities = (X['listed_in(city)'].value_counts().index)[:self.top_cities]\n        X['city_feature'] = X['listed_in(city)'].apply(lambda x: x if x in main_cities else 'Other')\n        X.drop('listed_in(city)', axis=1, inplace=True)\n        \n        # Creating for features for reducing granularity on restaurant type\n        main_rest_type = (X['rest_type'].value_counts().index)[:self.top_types]\n        X['type_feature'] = X['rest_type'].apply(lambda x: x if x in main_rest_type else 'Other')\n        X.drop('rest_type', axis=1, inplace=True)\n        \n        return X\n            \n# Class for creating a target based on a threshold (training only)\nclass CreateTarget(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, threshold=3.75):\n        self.threshold = threshold\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X['target'] = X['rate_num'].apply(lambda x: 1 if x >= self.threshold else 0)\n        \n        return X\n    \n# Class for splitting the data into new (not rated) and old (rated) restaurants\nclass SplitRestaurants(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # Splits the restaurants based on rate column (rated and non rated)\n        rated = X[~X['rate_num'].isnull()]\n        non_rated = X[X['rate_num'].isnull()]\n        \n        # Dropping the rate column\n        rated.drop('rate_num', axis=1, inplace=True)\n        non_rated.drop('rate_num', axis=1, inplace=True)\n        \n        return rated, non_rated","b1dea061":"# Reading raw data\ndata_path = r'..\/input\/zomato-bangalore-restaurants\/zomato.csv'\nraw_data = import_data(path=data_path, n_lines=5000)\n\n# Defining a commoon pipeline to be applied after reading the raw data\ncommon_pipeline = Pipeline([\n    ('initial_preparator', PrepareCostAndRate()),\n    ('selector', InitialFeatureSelection()),\n    ('feature_adder', RestaurantAdditionalFeatures()),\n    ('target_creator', CreateTarget()),\n    ('new_splitter', SplitRestaurants())\n])\n\n# Applying the initial pipeline\ntrain_restaurants, new_restaurants = common_pipeline.fit_transform(raw_data)\nprint(f'Total restaurants to be used on training: {len(train_restaurants)}')\nprint(f'Total restaurants to be used on prediction: {len(new_restaurants)}')","a2ee4af4":"train_restaurants.head()","a0082d3d":"# Splitting into training and testing data\nX = train_restaurants.drop('target', axis=1)\ny = train_restaurants['target'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42)\n\n# Splitting into cat and num data\ncat_features = [col for col, dtype in X_train.dtypes.items() if dtype == 'object']\nnum_features = [col for col, dtype in X_train.dtypes.items() if dtype != 'object']\n\n# Building a numerical processing pipeline\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median'))\n])\n\n# Building a categorical processing pipeline\ncat_pipeline = Pipeline([\n    ('encoder', DummiesEncoding(dummy_na=True))\n])\n\n# Building a complete Pipeline\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_features),\n    ('cat', cat_pipeline, cat_features)\n])\n\n# Applying the full pipeline into the data\nX_train_prep = full_pipeline.fit_transform(X_train)\nX_test_prep = full_pipeline.fit_transform(X_test)\nprint(f'Shape of X_train_prep: {X_train_prep.shape}')\nprint(f'Shape of X_test_prep: {X_test_prep.shape}')\n\n# returning categorical features after encoding and creating a new set of features after the pipeline\nencoded_features = full_pipeline.named_transformers_['cat']['encoder'].features_after_encoding\nmodel_features = num_features + encoded_features\nprint(f'\\nSanity check! Number of features after the pipeline (must be the same as shape[1]): {len(model_features)}')","419199ef":"# Logistic Regression hyperparameters\nlogreg_param_grid = {\n    'C': np.linspace(0.1, 10, 20),\n    'penalty': ['l1', 'l2'],\n    'class_weight': ['balanced', None],\n    'random_state': [42],\n    'solver': ['liblinear']\n}\n\n# Decision Trees hyperparameters\ntree_param_grid = {\n    'criterion': ['entropy', 'gini'],\n    'max_depth': [3, 5, 10, 20],\n    'max_features': np.arange(1, X_train.shape[1]),\n    'class_weight': ['balanced', None],\n    'random_state': [42]\n}\n\n# Random Forest hyperparameters\nforest_param_grid = {\n    'bootstrap': [True, False],\n    'max_depth': [3, 5, 10, 20, 50],\n    'n_estimators': [50, 100, 200, 500],\n    'random_state': [42],\n    'max_features': ['auto', 'sqrt'],\n    'class_weight': ['balanced', None]\n}\n\n# LightGBM hyperparameters\nlgbm_param_grid = {\n    'num_leaves': list(range(8, 92, 4)),\n    'min_data_in_leaf': [10, 20, 40, 60, 100],\n    'max_depth': [3, 4, 5, 6, 8, 12, 16],\n    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n    'bagging_freq': [3, 4, 5, 6, 7],\n    'bagging_fraction': np.linspace(0.6, 0.95, 10),\n    'reg_alpha': np.linspace(0.1, 0.95, 10),\n    'reg_lambda': np.linspace(0.1, 0.95, 10),\n}\n\nlgbm_fixed_params = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0\n}","5cc2416e":"# Setting up classifiers\nset_classifiers = {\n    'LogisticRegression': {\n        'model': LogisticRegression(),\n        'params': logreg_param_grid\n    },\n    'DecisionTrees': {\n        'model': DecisionTreeClassifier(),\n        'params': tree_param_grid\n    },\n    'RandomForest': {\n        'model': RandomForestClassifier(),\n        'params': forest_param_grid\n    },\n    'LightGBM': {\n        'model': lgb.LGBMClassifier(**lgbm_fixed_params),\n        'params': lgbm_param_grid\n    }\n}","a5446389":"# Creating an instance for the homemade class BinaryClassifiersAnalysis\nclf_tool = BinaryClassifiersAnalysis()\nclf_tool.fit(set_classifiers, X_train_prep, y_train, random_search=True, cv=5, verbose=5)","dbdc1613":"# Evaluating metrics\ndf_performances = clf_tool.evaluate_performance(X_train_prep, y_train, X_test_prep, y_test, cv=5)\ndf_performances.reset_index(drop=True).style.background_gradient(cmap='Blues')","24bb711c":"clf_tool.plot_roc_curve()","c6a381e2":"clf_tool.plot_confusion_matrix(classes=['Good', 'Bad'])","4082ab99":"fig, ax = plt.subplots(figsize=(13, 11))\nforest_feature_importance = clf_tool.feature_importance_analysis(model_features, specific_model='LightGBM', ax=ax)\nplt.show()","dfaac7e4":"clf_tool.plot_score_distribution('LightGBM', shade=True)","619fbdea":"clf_tool.plot_score_bins('LightGBM', bin_range=0.1)","38a5be3e":"# Returning the LightGBM model and applying shap value\nmodel = clf_tool.classifiers_info['LightGBM']['estimator']\nexplainer = shap.TreeExplainer(model)\ndf_X_train_prep = pd.DataFrame(X_train_prep, columns=model_features)\nshap_values = explainer.shap_values(df_X_train_prep)\n\n# Plotting a summary plot using shap\nshap.summary_plot(shap_values, df_X_train_prep)","5a85c8e9":"# Applying the full pipeline into new restaurants\nnew_restaurants_prep = full_pipeline.fit_transform(new_restaurants.drop('target', axis=1))\n\n# Returning the best model and predicting the rate for new restaurants\nmodel = clf_tool.classifiers_info['LightGBM']['estimator']\ny_pred = model.predict(new_restaurants_prep)\ny_probas = model.predict_proba(new_restaurants_prep)\ny_scores = y_probas[:, 1]\n\n# Labelling new data\nnew_restaurants['success_class'] = y_pred\nnew_restaurants['success_proba'] = y_scores\nnew_restaurants.head()","e3d42ef2":"# Looking at the score distribution for new restaurants\nfig, ax = plt.subplots(figsize=(16, 9))\nsns.kdeplot(new_restaurants['success_proba'], ax=ax, shade=True, color='mediumseagreen')\nformat_spines(ax, right_border=False)\nax.set_title('Score Distribution of Success for New Restaurants on the Dataset', size=16, color='dimgrey')\nplt.show()","a506b089":"# Ordering new restaurants by proba score\nnew_restaurants_data = new_restaurants.reset_index().merge(raw_data.reset_index()[['name', 'index']], how='left', on='index')\ntop_new = new_restaurants_data.sort_values(by='success_proba', ascending=False).head(10)\ntop_new = top_new.loc[:, ['name', 'success_proba', 'online_order', 'book_table', 'listed_in(type)',\n                          'approx_cost', 'multiple_types', 'total_cuisines', 'location_feature',\n                          'city_feature', 'type_feature']]\ntop_new","d4af98d3":"# Ordering new restaurants by proba score\nbottom_new = new_restaurants_data.sort_values(by='success_proba', ascending=True).head(10)\nbottom_new = bottom_new.loc[:, ['name', 'success_proba', 'online_order', 'book_table', 'listed_in(type)',\n                          'approx_cost', 'multiple_types', 'total_cuisines', 'location_feature',\n                          'city_feature', 'type_feature']]\nbottom_new","a3e6653f":"Let's take a closer look into the type of restaurants in this zomato dataset. The idea is to see if there is some preference in zomato orders.","af7ce0cb":"It's probably time to build a complete Pipeline for receiving the raw data with restaurants information and apply all the steps we select for make the data ready for training or prediction. In a first approach, the pipeline will consider:\n\n* Preparing the cost and rate attribute from raw data;\n* Selecting initial features to be part of data prep;\n* Creating new features based on original data;\n* Creating a target for using on training;\n* Splitting restaurants based on rated and non-rated ones;\n* Encoding the data for categorical features;\n* Filling the null data with meadian for numerical features;","a649885e":"Looking forward to put those numbers into a beautiful chart, let's use a homemade function called `answear_plot()` created to simulate the same analysis made for the best and worst restaurants in Bengaluru. This function can be found at the `viz_utils.py` module.","d47da8d4":"___\n*What are the most promissor restaurants (with the highest proba score)?*\n___","54c5fa19":"Searching for useful insights for maybe predicting the success of a restaurant in Bengaluru, let's dive into locations to answear the question in the session title. By the way, where are the good restaurants?*\n\n_*Good restaurants are relative and we will point that the the good ones have a high rate_","a9ac9070":"By now we've already splitted our original data into `new_restaurants` and `train_val_restaurants` pandas DataFrames. Let's keep the first one aside for now and let's work only with the training and validation set. The next step is to create our target variable to be used in this classification task.\n\nThe main point here is to define a fair threshold for splitting the restaurants into `good` and `bad` ones. It would be a really experimental decision and we must keep in mind that this approach is not the best one. Probably it would let margin for classification errors. Even so, let's try!","18b117fe":"Well, after reading the data and taking a look at its characteristics, we can now do a hands on to apply some preparation steps. As we said right above, there are some columns that could be threated better. Let's point in topics we will do right now:\n\n* **approx_cost(for two people):**\n    - Change the data type from object to float;\n    \n\n* **rate:**\n    - Let's eliminate the \"\/5\" text and change data type from object to float","adfbea7f":"Well, after going trough all the tasks we can conclude that it was possible to predict the success (in terms of rate) for new restaurants put in Zomato service. This is very interesting for business areas to evaluate the main concepts of restaurants and customers preferences in Bengaluru region. With this information, it's clearly possible to take a look at restaurant's features before launching it! The business men and the customers would appreciate it!\n\nSo, thinking of a developer way, it would be nice to code this as a production script that receives data from new restaurants monthly (or daily if you wish) and returns the \"success score probability\". For this to be true, we have to encapsulate a bunch of steps, like building (or reading) a complete pipeline in a pkl format, building (or reading) a classification model also in pkl format and, by the end, applying the `predict` or `predict_proba` methods for scoring the new data. Let's simulate it.","393f6e63":"Well, as I said before, this map vision session have low precision because we get the latitude and longitude information using the cities instead of the address. Again, we did this way because it was difficult to parse all the addresses in this dataset and also because of API's free tier time consuming and total requests limitations.\n\nEven so, it's fair to say that some good information was delivered on the maps above. You can improve it by bringing precision geolocation information to this zomato dataset.","5980c175":"For answearing the question above, let's group our data into some numerical variables that could probably show a good overview from restaurant's indicators like `total votes`, `mean approx cost`, `rate` and others. By the end, we will have in hands a new DataFrame with grouped information about all the restaurant's franchise in the dataset.\n\nIt's important to say that here we are grouping by `restaurant name` and, as long as there are restaurants with the same name in the dataset (same name but different locations, for example), we will also see how many unities each \"franchise\" have.","fab13ff4":"<a id=\"3.3\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>3.3 Where Are the Good Ones?<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","20a2df50":"___\n* _What are the most popular restaurants?_\n___","2f0d6473":"**Well, that's all folks!**\n\nWe finally finished our job here and we can be very excited of what we've done. With this implementation it was possible to delivery useful information for Business areas and also for Zomato customers on choosing the best restaurant for ordering (specially the new ones). This is an example of what predictive models can do in practice.\n\nThank you for going until now with me and, if you liked, **please UPVOTE this kernel** and leave a comment below.","3a306f51":"<a id=\"3.5\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>3.5 Food Options<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","180b1c6e":"<a id=\"2.2\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>2.2 Initial Prep<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","d2cb2414":"<a id=\"4.1\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.1 Target Definition<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","907768b2":"___\n* _How the offering of Book Table service impact on Rate and Approx Cost?_\n___","851f96cc":"<a id=\"4.5\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.5 Training a Model<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","e17765b0":"<a id=\"4.3\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.3 Encoding<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","10c5a33a":"Once read the data, let's now use a homemade implementation to extract useful information about the dataset.","e6e36b73":"<a id=\"4.4\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.4 Pipeline<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","dc8b0263":"<a id=\"3.4\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>3.4 Zomato Customers Preferences<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","beed9db9":"After defining the target and splitting the data into train+val and test sets, let's define the features to be used on training. Here we will take a look at the raw data to select valuable features and apply some steps to create another ones.\n\nThe initial set of selected features inclue:\n    - online_order;\n    - book_table;\n    - location;\n    - rest_type;\n    - cuisines;\n    - listed_in(type);\n    - listed_in(city);\n    - approx_cost","ccb4c1aa":"___\n* _What we can conclude about main options available on Bengaluru restaurants?_\n___","fb828868":"<a id=\"4\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>4. Predicting the Success of a Restaurant<\/b><\/font>","28dfa53b":"In this session, let's present useful charts that could be used to get intuition about the data. We will divide the exploratory data analysis into topics to help the users take their own conclusions. In each topic, we will also try to purpose questions that can be answeared with some graphic or table analysis.","215231cc":"<a id=\"3.2\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>3.2 Restaurants Services<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","7dfeb9f5":"___\n* _Geo Analysis: where are the restaurants located in Bengaluru?_\n___","7d50b96e":"Finally, we come to the main point of our project: training a classification model to predict the probability of success (high rate) of a given restaurant. We have already went trough about 80% of work done on prep and transformations. So, for making this training step easier, let's use a homemade utility script called `ml_utils.py`. It contains a class called `BinaryClassifierAnalysis` with useful methods for training and evaluating selected classifiers.\n\nFor this task, let's train a `LogisticRegression`, `DecisionTrees`, `RandomForest` and `LightGBM` classifiers, each one using `RandomizedGridSearchCV` with 5 k-folds and hiperparameters pre-defined.","d4920602":"For answearing this question, let's do a text mining on `cuisine` column fo extract single options for each restaurant. After that, we will plot a WordCloud to see trends on food options.","d24b1b28":"___\n* _Plotting Confusion Matrix for each classifier (train and test)_\n___","3e167ef1":"By looking at the dataset page on Kaggle, it's possible to see the meaning of each columns of the data. Just to allign the concepts, we have:\n\n* **_url:_** contains the url of the restaurant in the zomato website;\n* **_address:_** contains the address of the restaurant in Bengaluru;\n* **_name:_** contains the name of the restaurant;\n* **_online-order:_** whether online ordering is available in the restaurant or not;\n* **_book-table:_** table book option available or not;\n* **_rate:_** contains the overall rating of the restaurant out of 5;\n* **_votes:_** contains total number of rating for the restaurant as of the above mentioned date;\n* **_phone:_** contains the phone number of the restaurant;\n* **_location:_** contains the neighborhood in which the restaurant is located;\n* **_rest-type:_** restaurant type.","9a30f799":"<a id=\"1\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>1. Libraries<\/b><\/font>","5d29463f":"Let's take a look at our `train_restaurants` data:","8da096be":"___\n* _Training and evaluating classifiers_\n___","e1428365":"Now let's purpose a clearly view for cities with restaurants in Bengaluru and take a look at the average cost and rate of restaurants in each one. The idea is to see how these two variables are related and to present customers the insight of choosing the best city to visit (eating purposes) in Bengaluru.","d87d5db1":"Wow! There are a huge amount of Pasta, Burgers and Cocktails. We can also see Pizza, Biryani, Coffe, Sanwiches and others.","af3c4c4b":"___\n*What are the restaurants with the lowest likely probability? (the ones that maybe customers won't order for Zomato)*\n___","bd92ac57":"Ok, for our first trial it's fair. The meaning of all this is that we marked as `good` restaurants with a rate greather or ewual **3.75**. Correct or not, let's continue to see what we can get from this.\n\nThe next step is to prepare some features for training our classification model.","ef8bcfae":"<a id=\"3.1\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>3.1 Restaurants Overview<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","6492f209":"<a id=\"3\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3. Graphical Exploration<\/b><\/font>","970b6aa6":"For the last act on this topic, let's split the data into training and validation sets once and for all.","067b1c2e":"___\n* _Looking at score (proba) distribution for a specific model_\n___","44742331":"<a id=\"2.1\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>2.1 Overview from the Data<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","dc4a5fd1":"___\n* _Plotting the ROC Curve for the classifiers (train and test)_\n___","a6265f1d":"With the donuts above we can clearly see the proportions of Bengaluru restaurants who offers book table and online order service. In the first case, we can see that just 12.5% of restaurants have book table, while 87.5% don't offer this service.\n\nBy the other hand, we have almost 59% of restaurants who have online order service.","bb8bebd1":"Now if we look for online order service, it's fair to say that the customer don't give importance for this service as much they give for book table one. The rate distribution shows customers usually give highest rates for restaurants who offer online order service (even if the difference is little).\n\nLooking at the cost comparison between restaurants who offer online order service, we can conclude that they are almost the same.","e254a51c":"<a id=\"1\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>2. Reading and Exploring the Data<\/b><\/font>","d91f3d0f":"___\n* _How many restaurants offer Book Table service? And how about Online Order service?_\n___","09232762":"___\n* _Using shap analysis for evaluating features patterns_\n___","19fb46b7":"Let's define and apply a categorical and a numerical pipeline for preparing the data:","cbfbf0a4":"___\n* _How the rate and approx cost are distributed among restaurant types?_\n___","1a900c20":"Perfect! We are now ready to train classification models and select the best one for our task!","9b0551cb":"___\n* _How many types of restaurants we have?_\n___","8a43b649":"<a id=\"5\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>5. Conclusion + Production Script<\/b><\/font>","e700f77a":"In this session we will discuss the customer's food preferences. The goal is to go trough the text presented in the columns like `cuisines` and `dish_liked` to extract valuable information about options available in Bengaluru set of restaurants.","a301456b":"<a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Content<\/h3>\n\n* [1. Libraries](#1)\n* [2. Reading and Exploring the Data](#2)\n    - [2.1 An Overview from the Data](#2.1)\n    - [2.2 Initial Prep](#2.1)\n* [3. Graphical Exploration](#3)\n    - [3.1 Restaurants Overview](#3.1)\n    - [3.2 Restaurants Services](#3.2)\n    - [3.3 Where Are the Good Ones?](#3.3)\n    - [3.4 Zomato Customers Preferences](#3.4)\n    - [3.5 Food Options](#3.5)\n* [4. Predicting the Success of a Restaurant](#4)\n    - [4.1 Target Definition](#4.1)\n    - [4.2 Feature Extraction](#4.2)\n    - [4.3 Encoding](#4.3)\n    - [4.4 Pipeline](#4.4)\n    - [4.5 Training a Model](#4.5)","f6cf08a0":"Well, finally we arrived at our main task on this notebook: predict the success of a restaurant in Bengaluru using the data provided and extracting some additional features. At this point probably you're asking: but how we will do it? That's what a thought for this task:\n\n    1. Use the restaurant rate to classify our data in two classes: good and bad (thresholds to be defined)\n    2. Create a target variable using a pre-defined rate threshold\n    3. Extract features from the data\n    4. Create a classification model using a supervisioned approach\n    5. Predict the \"probability for being a good restaurant\" for the ones marked as NEW or without rate\n    \nFirst of all, let's split the data we will use for train\/validation and the data we want to do a real prediction.","0decfee5":"<a id=\"4.2\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.2 Feature Extraction<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","eaacf041":"___\n* _Looking at the feature importance of a specific model_\n___","5fc9b427":"We can see a lot of North Indian, Chinese, South Indian, Fast Food, Desserts and other patterns related to options available in Bengaluru area. Good! Let's apply the same approach on `dish_liked` column to see if there are more trends.","b6b96161":"In this topic, let's make an initial analysis on our data looking at restaurant's features presented in it.","f3d3d244":"Well, with the chart above we purpose a really good look at the Bengaluru scenario: we chose three numerical features (total votes, mean approx cost and rate). Now let's use other useful features of our dataset to understand how these numerical features can be changed along the services restaurants offers, like `book table` and `online order`.","8c177c7e":"The distplot and the boxplot above show us the importance given by the customers for book table service in restaurants: on the first line we notice that, in general, restaurants with book table service usually receive highest rate notes. Meanwhile, we can conclude by the second line that those same restaurants are usually more expensive.","2bc9c1a3":"In this notebook we will go trough a complete Data Analysis on Zomato Bengalore Restaurants dataset available on [Kaggle](https:\/\/www.kaggle.com\/himanshupoddar\/zomato-bangalore-restaurants). The goal of this project is to provide decision power for decision makers when looking at informations about Bengalore restaurants. For this we can:\n\n- Get intuition about the data;\n- Document an exploratory data analysis;\n- Point out conclusions in every step of analysis;\n- Use graphical modules (matplotlib and seaborn) to answear questions;\n- Apply a predictive point of view for helping people to choose the best restaurant.\n- Using this predictive approach for predicting the success of a new restaurant in Bengaluru.","44f8fdd1":"<font size=\"+1\" color=\"black\"><b>Please visit my other kernels by clicking on the buttons<\/b><\/font><br>\n\n<a href=\"https:\/\/www.kaggle.com\/thiagopanini\/e-commerce-sentiment-analysis-eda-viz-nlp\" class='btn btn-primary' style='color:white;'>E-Commerce Sentiment Analysis<\/a>\n<a href=\"https:\/\/www.kaggle.com\/thiagopanini\/predicting-credit-risk-eda-viz-pipeline#Training-and-Evaluating-a-Model\" class=\"btn btn-primary\" style=\"color:white;\">Credit Risk Detection<\/a>\n<a href=\"https:\/\/www.kaggle.com\/thiagopanini\/credit-fraud-how-to-choose-the-best-classifier\" class=\"btn btn-primary\" style=\"color:white;\">Credit Card Fraud Detection<\/a>\n<a href=\"https:\/\/www.kaggle.com\/thiagopanini\/global-terrorism-eda-nlp\" class=\"btn btn-primary\" style=\"color:white;\">Global Terrorism<\/a>","9c923b95":"It's very interesting to see that the `RandomForest` was the best classification model for our task. Meanwhile it's also the heaviest one in terms of time consuming. Let's use it for further evaluation.","fc430325":"___\n* _How the offering of Online Order service impact on Rate and Approx Cost?_\n___","623103c7":"___\n* _Heat Map: where are the restaurants with high average rate?_\n___","1ddf8c94":"Some considerations:\n\n* We won't use the `votes` feature as long as this is a information we only know after launching a restaurant. As we want to be predictive, the idea is to return the probability of success of a restaurant before launching it.\n* We created the `multiple_types`, `total_dishes` and `total_cuisines` features in a way of counting the food services offered by the restaurant. This is information can be gotten before the launching of the establishment.","b050a282":"In the following steps, the real idea was to extract lat and long features from the address in the dataset, but there are huge differences between address formats that made it difficult. So, maybe the best we can do here is to extract geolocation features based in the restaurant city. \n\nOf course this is not the best approach in terms of location precision, but I think is the best we can do regarding on the problems in the dataset address column construction . Let's use the `Nominatim` API to help us doing this job. I'll let my user_agent open.","d198bd2e":"We don't have explanation for some columns like **_dish-liked_**, **_approx-cost(for two people)_** and others but its meaning can be extracted by looking at the dataset head above. As long as we look at those initial rows, we can see either the need of transformation on some columns like **_rate_** for example (we will talk about this in the future).","6151bf73":"Looking at the data we prepared for training our classification model, it's important to point the need to apply some encoding technique on categorical features. Here we can define the \"global reach\" of your restaurant success predictor: using features like `location` and `listed_in(city)` for training will restrict the use of our model to Bengaluru area only. Let's keep those ones for awhile pointing that, at least on this first moment, our classifcation model could only be used to predict success of restaurants in Bengaluru."}}