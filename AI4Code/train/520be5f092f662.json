{"cell_type":{"7480b56d":"code","8b737e44":"code","e12c8707":"code","1d73af38":"code","d9f395e6":"code","595ff4f7":"code","4a3e610a":"code","a4190cd0":"code","e208cd72":"code","d02b2fd8":"code","7adcf32d":"code","7f891357":"code","0bd50e7a":"code","c67e6bcd":"code","57715bb0":"code","40aab2eb":"code","91f25cde":"code","af9af53d":"code","8f837a81":"code","02321bb3":"code","95e1d495":"code","e0de1408":"code","ba4ceea3":"code","39e42e6b":"code","ecf4c23a":"code","ae7f7992":"code","0daa5ba3":"code","2ac3ce57":"code","f2bfb43f":"code","c9da1c4a":"code","1f8941eb":"code","453f19b3":"code","85f773e4":"code","b354f666":"code","1605d7ce":"code","a05b5eca":"code","5b0e2248":"code","28d72895":"code","526cce2d":"code","3f767f25":"code","90adf918":"code","cc168388":"code","d4001863":"code","54f7a20b":"code","e99a0f27":"code","a1e8bea3":"code","0378b457":"code","8c726631":"code","9cf0d6f5":"code","36f16fb2":"code","c720601b":"code","ce41145e":"code","55842f9b":"code","e4370044":"code","f6173bd6":"code","22496b9c":"code","f7ebd1d4":"code","e486cca7":"markdown","31ebc485":"markdown","9e80d2b1":"markdown","9d58162f":"markdown","a68eb6cd":"markdown","c5dde45b":"markdown","0bb95ead":"markdown","51a0642c":"markdown","871df2ec":"markdown","43ed47ee":"markdown","d019d621":"markdown","90717bb6":"markdown","6c87eb42":"markdown","74e18f3d":"markdown","eb10ccd4":"markdown","4c535f61":"markdown","95a35b3f":"markdown","6ee44768":"markdown","2ea21ad0":"markdown","83826212":"markdown","cc5680e1":"markdown","867ab526":"markdown","0ed7b749":"markdown","a8a87884":"markdown","8231b297":"markdown","81b8798b":"markdown","1724c0e8":"markdown"},"source":{"7480b56d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport warnings\nimport os \nwarnings.filterwarnings(\"ignore\")\nimport datetime\n","8b737e44":"data=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\n","e12c8707":"data.head()      #displaying the head of dataset they gives the 1st to 5 rows of the data","1d73af38":"data.describe()      #description of dataset ","d9f395e6":"data.info()","595ff4f7":"data.shape       #569 rows and 33 columns","4a3e610a":"data.columns     #displaying the columns of dataset","a4190cd0":"data.value_counts","e208cd72":"data.dtypes","d02b2fd8":"data.isnull().sum()","7adcf32d":"data.drop('Unnamed: 32', axis = 1, inplace = True)\n","7f891357":"data","0bd50e7a":"data.corr()","c67e6bcd":"plt.figure(figsize=(18,9))\nsns.heatmap(data.corr(),annot = True, cmap =\"Accent_r\")\n\n\n\n","57715bb0":"sns.barplot(x=\"id\", y=\"diagnosis\",data=data[160:190])\nplt.title(\"Id vs Diagnosis\",fontsize=15)\nplt.xlabel(\"Id\")\nplt.ylabel(\"Diagonis\")\nplt.show()\nplt.style.use(\"ggplot\")\n","40aab2eb":"sns.barplot(x=\"radius_mean\", y=\"texture_mean\", data=data[170:180])\nplt.title(\"Radius Mean vs Texture Mean\",fontsize=15)\nplt.xlabel(\"Radius Mean\")\nplt.ylabel(\"Texture Mean\")\nplt.show()\nplt.style.use(\"ggplot\")\n","91f25cde":" \nmean_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nsns.pairplot(data[mean_col],hue = 'diagnosis', palette='Accent')\n","af9af53d":"sns.violinplot(x=\"smoothness_mean\",y=\"perimeter_mean\",data=data)","8f837a81":"plt.figure(figsize=(14,7))\nsns.lineplot(x = \"concavity_mean\",y = \"concave points_mean\",data = data[0:400], color='green')\nplt.title(\"Concavity Mean vs Concave Mean\")\nplt.xlabel(\"Concavity Mean\")\nplt.ylabel(\"Concave Points\")\nplt.show()\n\n","02321bb3":"worst_col = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\n\nsns.pairplot(data[worst_col],hue = 'diagnosis', palette=\"CMRmap\")","95e1d495":"# Getting Features\n\nx = data.drop(columns = 'diagnosis')\n\n# Getting Predicting Value\ny = data['diagnosis']\n","e0de1408":"\n#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n","ba4ceea3":"print(len(x_train))\n","39e42e6b":"print(len(x_test))","ecf4c23a":"print(len(y_train))","ae7f7992":"print(len(y_test))","0daa5ba3":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)                         \n","2ac3ce57":"y_pred=reg.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",reg.score(x_train,y_train)*100)\n\n\n","f2bfb43f":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n\n\n\n\n","c9da1c4a":"print(accuracy_score(y_test,y_pred)*100)","1f8941eb":"from sklearn.model_selection import GridSearchCV\nparam = {\n         'penalty':['l1','l2'],\n         'C':[0.001, 0.01, 0.1, 1, 10, 20,100, 1000]\n}\nlr= LogisticRegression(penalty='l1')\ncv=GridSearchCV(reg,param,cv=5,n_jobs=-1)\ncv.fit(x_train,y_train)\ncv.predict(x_test)\n","453f19b3":"print(\"Best CV score\", cv.best_score_*100)","85f773e4":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123)\n\ndtree.fit(x_train,y_train)\n\n#y_pred = dtree.predict(x_test)\n","b354f666":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",dtree.score(x_train,y_train)*100)\n\n","1605d7ce":"print(accuracy_score(y_test,y_pred)*100)","a05b5eca":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)\n\n","5b0e2248":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",rfc.score(x_train,y_train)*100)\n","28d72895":"print(accuracy_score(y_test,y_pred)*100)","526cce2d":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train,y_train)\n","3f767f25":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",knn.score(x_train,y_train)*100)\nprint(knn.score(x_test,y_test))\n","90adf918":"print(accuracy_score(y_test,y_pred)*100)\n","cc168388":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\n","d4001863":"y_pred=svc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",svc.score(x_train,y_train)*100)\nprint(svc.score(x_test,y_test))\n","54f7a20b":"print(\"Training Score: \",svc.score(x_train,y_train)*100)","e99a0f27":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nadb.fit(x_train,y_train)\n\n\n\n\n\n","a1e8bea3":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",adb.score(x_train,y_train)*100)","0378b457":"print(accuracy_score(y_test,y_pred)*100)","8c726631":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\n","9cf0d6f5":"y_pred=gbc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gbc.score(x_train,y_train)*100)\nprint(gbc.score(x_test,y_test))\n","36f16fb2":"print(accuracy_score(y_test,y_pred)*100)","c720601b":"from xgboost import XGBClassifier\n\nxgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxgb.fit(x_train, y_train)\n","ce41145e":"y_pred=xgb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",xgb.score(x_train,y_train)*100)\nprint(xgb.score(x_test,y_test))\n","55842f9b":"print(\"Training Score: \",xgb.score(x_train,y_train)*100)","e4370044":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","f6173bd6":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)","22496b9c":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(\"Training Score: \",gnb.score(x_train,y_train)*100)\nprint(gnb.score(x_test,y_test))\n","f7ebd1d4":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","e486cca7":"# 4. KNeighborsClassifier\n\n","31ebc485":"**So we get a accuracy score of 98.24 % using AdaBoostClassifier**","9e80d2b1":"**So we get a accuracy score of 70.17 % using KNeighborsClassifier**","9d58162f":"# IMPORTING THE LIBRARIES","a68eb6cd":"# TRAINING AND TESTING DATA","c5dde45b":"**So now we conclude the accuracy of different models:**\n\n**1. AdaBoost Classifier = 98.24 %**\n\n**2. XGB Classifier= 97.84 %**\n\n**3. Random Forest Classifier =96.57 %**\n\n**4. Gradient Boosting Classifier= 95.66%**\n\n**5. Decision Tree Classifier= 94.78 %**\n\n**6. K Neighbours Classifier= 70.18 %**\n\n**7. SVC = 63.80 %**\n\n**8. Naiye Bayes= 63.30 %**\n\n**9. Logistic Regression = 58.82%**\n","0bb95ead":"# MODELS","51a0642c":"**So we get a accuracy score of 96.49 % using Random Forest Classifier**","871df2ec":"# 2. DECISION TREE CLASSIFIER","43ed47ee":"**So we get a accuracy score of 58.7 % using logistic regression**","d019d621":"# LOADING THE DATASET","90717bb6":"# VISUALIZING THE DATA","6c87eb42":"**Ada Boost Classifier got the highest accuracy**","74e18f3d":"# If you liked this notebook, please UPVOTE it.","eb10ccd4":"**So we get a accuracy score of 63.29 % using Naive Bayes**","4c535f61":"**So we get a accuracy score of 97.80 % using  XGBClassifier**","95a35b3f":"#  7. Gradient Boosting Classifier","6ee44768":"# 9. Naive Bayes","2ea21ad0":"**So we get a accuracy score of 95.61 % using GradientBoostingClassifier**","83826212":"**So we get a accuracy score of 63.7 % using SVC**","cc5680e1":"**So we have to drop the Unnamed: 32 coulumn which contains NaN values**","867ab526":"# 8. XGBClassifier","0ed7b749":"# 6. AdaBoostClassifier","a8a87884":"# 5. SVC","8231b297":"**So we get a accuracy score of 94.73 % using Decision Tree Classifier**","81b8798b":"# 3. Random Forest Classifier","1724c0e8":"# 1. Logistic Regression"}}