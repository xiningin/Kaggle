{"cell_type":{"1965fcf5":"code","50e81c8b":"code","13503998":"code","5596a2f2":"code","3bf0d2a3":"code","d6fac142":"code","b73e406b":"code","1ac5ece5":"code","da53f25d":"code","32c667fb":"code","5c212f11":"code","3ddd2142":"code","5d5b3e96":"code","4c47be4b":"code","29c149d2":"code","6af96c77":"code","93b62ab2":"code","0b3e70c0":"code","8b6df782":"code","76f9c387":"code","2f636210":"code","76a36ca3":"code","5b012ba7":"code","298411c1":"markdown","56b65a18":"markdown","b8d9e227":"markdown","c76da142":"markdown","d0d1b74f":"markdown"},"source":{"1965fcf5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50e81c8b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","13503998":"#Reading and merging 2 files\ndf1 = pd.read_csv(\"\/kaggle\/input\/supply-chain-cel-dataset\/canceled_test.csv\")\ndf2 = pd.read_csv(\"\/kaggle\/input\/supply-chain-cel-dataset\/sales_test.csv\")\nlst = [df2,df1]\ndf = pd.concat(lst)","5596a2f2":"#Checking for null values in the dataframe\nsns.heatmap(df.isnull())","3bf0d2a3":"#Converting the 'DATE' column in the standard date-time format and sorting\ndf['DATE'] = pd.to_datetime(df['DATE'])\ndf.sort_values('DATE')","d6fac142":"#Grouping by date and reseting index to make column name as 'DATE' again\nitem_plot = df.groupby('DATE').sum()\nitem_plot = item_plot.reset_index()","b73e406b":"#Constructing the 'VOLUME' column and correcting for some rows - \n#where orders are cancelled without having a sale (manual\/software error)\nitem_plot['VOLUME'] = item_plot['NS_ORDER'] - item_plot['NC_ORDER']\nVol = np.array(item_plot['VOLUME'])\nVol[Vol<0] = 0\nitem_plot['VOLUME'] = Vol\nitem_plot.head()","1ac5ece5":"#Plotting the daily Volume\nplt.figure(figsize=(12,7))\nplt.bar(item_plot['DATE'],item_plot['VOLUME']) # A bar chart\nplt.xlabel('Date')\nplt.ylabel('Volume')","da53f25d":"#ABC analysis per SKU\n#Reading and merging 2 files\ndf1 = pd.read_csv(\"\/kaggle\/input\/supply-chain-cel-dataset\/canceled_test.csv\")\ndf2 = pd.read_csv(\"\/kaggle\/input\/supply-chain-cel-dataset\/sales_test.csv\")\nlst = [df2,df1]\ndf = pd.concat(lst)\ndf","32c667fb":"#Grouping by ITEM No.\ndf = df.groupby('ITEM').sum()\n\n\n#Constructing the 'VOLUME' column and correcting for some rows where orders are cancelled without having a sale (manual\/software error)\ndf['VOLUME'] = df['NS_ORDER'] - df['NC_ORDER']\nVol = np.array(df['VOLUME'])\nVol[Vol<0] = 0\ndf['VOLUME'] = Vol","5c212f11":"#Sorting and constructing column for cumulative values\ndf = df.sort_values('VOLUME',ascending=False)\ndf['CUMULATIVE'] = df['VOLUME'].cumsum(axis = 0)\ndf = df.reset_index()\ndf","3ddd2142":"#ABC Analysis\nx = df['CUMULATIVE'].iloc[-1]\n\ndef ABC_category(value):\n    if value > 0 and value < 0.8*x:\n        return 'A'\n    elif value >= 0.8*x and value < 0.95*x:\n        return 'B'\n    else:\n        return 'C'\n    \ndf['CATEGORY'] = df['CUMULATIVE'].apply(ABC_category)\ndf","5d5b3e96":"#Values per category\ndf['CATEGORY'].value_counts()","4c47be4b":"#Finding the most Important items which make up 80% of the sales volume\ndf.loc[df['CATEGORY'] == 'A']","29c149d2":"#ABC Analysis per Customer Id\ndf1 = pd.read_csv(\"\/kaggle\/input\/supply-chain-cel-dataset\/canceled_test.csv\")\ndf2 = pd.read_csv(\"\/kaggle\/input\/supply-chain-cel-dataset\/sales_test.csv\")\nlst = [df2,df1]\ndf3 = pd.concat(lst)\nCN=df3.groupby(\"CUSTOMER_NO\").sum()\nCN['VOLUME']=CN['NS_SHIP']-CN['NC_SHIP']","6af96c77":"Vol = np.array(CN['VOLUME'])\nVol[Vol<0] = 0\nCN['VOLUME'] = Vol","93b62ab2":"CN = CN.sort_values('VOLUME',ascending=False).reset_index()\nCN['CUMULATIVE'] = CN['VOLUME'].cumsum(axis = 0)\nCN.head()","0b3e70c0":"x =CN['CUMULATIVE'].iloc[-1]\n\ndef ABC_category(value):\n    if value > 0 and value < 0.8*x:\n        return 'A'\n    elif value >= 0.8*x and value < 0.95*x:\n        return 'B'\n    else:\n        return 'C'\n\nCN['CATEGORY'] = CN['CUMULATIVE'].apply(ABC_category)","8b6df782":"CN['CATEGORY'].value_counts()","76f9c387":"CN.loc[CN['CATEGORY'] == 'A']","2f636210":"#Service Level per sku - determined by items ordered\/items actually shipped\n\ndf.drop(['ORDER_NO','LINE','CUSTOMER_NO','NC_ORDER','NC_SHIP','NS_SHIP'], axis='columns', inplace=True)\ndf['SERVICE LEVEL'] = df['VOLUME']\/df['NS_ORDER']","76a36ca3":"df['SERVICE LEVEL'] = df['SERVICE LEVEL'].fillna(0)\ndf.head()","5b012ba7":"#Service level per customer \nCN['SERVICE LEVEL'] = CN[\"VOLUME\"]\/CN[\"NS_ORDER\"]\nCN.drop(['ORDER_NO','LINE','NS_ORDER','NC_ORDER','NC_SHIP','NS_SHIP','ITEM'], axis='columns', inplace=True)\nCN['SERVICE LEVEL'] = CN['SERVICE LEVEL'].fillna(0)\nCN.head()","298411c1":"# Plotting Daily Volume","56b65a18":"**The 19 items in category 'A' make up for 80% of our sales and we should focus on them to increase profits**","b8d9e227":"# Service Level","c76da142":"**The 62 customers in category 'A' drive 80% of our sales and we should improve service levels for these customers to increase profits.**","d0d1b74f":"# ABC Analysis"}}