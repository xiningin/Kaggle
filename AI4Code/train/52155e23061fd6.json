{"cell_type":{"7d478cf6":"code","c0048b68":"code","e919156f":"code","def46ae8":"code","fbb5ce21":"code","fe7ebf7b":"code","75d339b2":"code","9d37146b":"code","95aab190":"code","6d380b85":"code","eb7f5334":"code","48e7abb0":"code","59bc7afd":"code","1f1273a6":"code","b7d20d62":"code","faed885b":"code","41913ea1":"code","6654f19c":"code","2ffe6155":"code","1d5d7479":"code","3d19370b":"markdown","e391db31":"markdown","61de621d":"markdown","23eb17d7":"markdown","7f7ed1b4":"markdown","3f2a4d95":"markdown","b4f99472":"markdown","601219d7":"markdown","6d1c1c69":"markdown","6036609a":"markdown","5ef4a6e8":"markdown","9ff0e057":"markdown","55327eea":"markdown","e8c2efe4":"markdown","481c1e38":"markdown","f4185d5e":"markdown","07dcdbd5":"markdown","f2934ea2":"markdown","bfe8c575":"markdown","5d2f55bb":"markdown","8c5ca809":"markdown"},"source":{"7d478cf6":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Visualization\n!pip install dataprep | grep -v 'already satisfied'\nfrom dataprep.eda import plot, plot_diff, create_report\n\n#Preprocessing and Modelling\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Fine-tuning\n!pip install -q -U keras-tuner\nimport keras_tuner as kt\n\n# Warning\nimport warnings\nwarnings.filterwarnings('ignore')","c0048b68":"train_full = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_full = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint('Training Set Shape = {}'.format(train_full.shape))\nprint('Training Set Memory Usage = {:.2f}MB'.format(train_full.memory_usage().sum()\/2**20))\n\nprint('Test Set Shape = {}'.format(test_full.shape))\nprint('Test Set Memory Usage = {:.2f}MB'.format(test_full.memory_usage().sum()\/2**20))","e919156f":"plot(train_full)","def46ae8":"create_report(train_full)","fbb5ce21":"df_train = pd.read_csv('\/kaggle\/input\/disastertweet-prepared2\/train_prepared.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/disastertweet-prepared2\/test_prepared.csv')","fe7ebf7b":"# Instantiate the Vectorizer\nvect = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0, max_df=0.9, max_features=100)\ndf_dtm = vect.fit_transform(df_train)\ndf_dtm.toarray()[0]","75d339b2":"tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0, max_df=0.98, max_features=100)\ndf_ifidf= tfidf_vect.fit_transform(df_train)\ndf_ifidf.toarray()[0]","9d37146b":"df_test.text","95aab190":"max_len = 0\n# Find the longest sentence \nfor sentence in pd.concat([df_train.text, df_test.text]):\n    if len(sentence) > max_len: # number of word in a sentence tokenizer is greater max_len\n        max_len = len(sentence)\nmax_len","6d380b85":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\ntrain_x = tokenizer.batch_encode_plus(df_train.text.tolist(), max_length=max_len, padding='max_length',return_tensors='tf')\ntest_x = tokenizer.batch_encode_plus(df_test.text.tolist(), max_length=max_len, padding='max_length', return_tensors='tf')\ntrain_y = df_train.target","eb7f5334":"train_x","48e7abb0":"from transformers import TFAutoModelForSequenceClassification\n\ndef distilBERT_tuner(hp):\n    model = TFAutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')\n    # Using learning_rate values are recommendated from paper BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding\n    hp_learning_rate = hp.Choice('learning_rate', values=[5e-5, 4e-5 , 3e-5, 2e-5])\n    optimizer = keras.optimizers.Adam(learning_rate=hp_learning_rate)\n    model.compile(optimizer=optimizer,\n                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                 metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n    return model\n    ","59bc7afd":"tuner = kt.Hyperband(distilBERT_tuner,\n                    objective='val_accuracy',\n                    max_epochs=4,\n                    factor=3,\n                    directory='my_dir',\n                    project_name='DistilBERT_to_kt')","1f1273a6":"stop_early = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)","b7d20d62":"tuner.search(train_x['input_ids'], \n             train_y, \n             epochs=4, \n             validation_split=0.2, \n             callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"The hyperparameter search is complete. The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\"\"\")","faed885b":"# Build the model with the optimal hyperparameters and train it on the data for 4 epochs\nmodel = tuner.hypermodel.build(best_hps)\nhistory = model.fit(train_x['input_ids'], \n                    train_y, \n                    epochs=4, \n                    validation_split=0.2)\n\nval_acc_per_epoch = history.history['val_accuracy']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))","41913ea1":"hypermodel = tuner.hypermodel.build(best_hps)\n\n# Retrain the model\nhypermodel.fit(train_x['input_ids'], \n               train_y, \n               epochs=best_epoch, \n               validation_split=0.2)","6654f19c":"def submission_transformer(model, test):\n    \"\"\"For Bert\"\"\"\n    sample_sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n    predictions =  model.predict(test['input_ids'])\n    y_preds = [ np.argmax(x) for x in predictions[0]]\n    sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_preds})\n    sub.to_csv('submission.csv', index=False)","2ffe6155":"submission_transformer(hypermodel, test_x)","1d5d7479":"pd.read_csv('submission.csv')","3d19370b":"### Train the model","e391db31":"<a id=4 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">4. Vectorization<\/p>\n\nThree steps using the Bag-of-words (BOW) model:\n1. Term frequency : count occurrences of word in sentence\n2. Inverse document frequency: \n3. L2 Norm\nReference : https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction\n\n[Content](#0)","61de621d":"BERT(*Bi-directional Encoder Representations from Transformers*)\n\n    - GLUE Score to 80.5%\n    - MultiNLI accuracy to 86.7%\n    - SQuAD v1.1 question answering Test F1 to 93.3\n    - SQuAD v2.0 Test F1 to 83.1","23eb17d7":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. EDA \ud83d\udcca<\/p>\n\n\n[Content](#0)","7f7ed1b4":"<a id=7 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">7. References<\/p>\n[Content](#0)","3f2a4d95":"<a id=5 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5 BERT model<\/p>\n\n[Content](#0)","b4f99472":"# Main technics I used in this data\n    * [3.1] Remove 92 duplicated rows\n    * [3.2] Cleaning text\n    * [3.3] Spelling Checker\n    * [3.4] Remove Stemming\n #### Step 3.3 spends a lot time (around 4000s in 4536s in total). \n #### So, I splits Data Preprocessing into [another kernel](http:\/\/https:\/\/www.kaggle.com\/phanttan\/disastertweet-prepareddata). \n #### And the prepared data to save in to [new dataset](http:\/\/https:\/\/www.kaggle.com\/phanttan\/disastertweet-prepared2)\n #### I am so appreciate to you for using\/upvoting it.\n","601219d7":"<a id=5.1 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.1 Preprocessing Data<\/p>\n\n[Content](#0)","6d1c1c69":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">0. What are updated in the last version?<\/p>\n\n## Current Version\n\n   1. Upload syntax for pip install dataprep\n   \n   2. Using *val_accuracy* monitor in EarlyStopping because it make the better result.\n   \n   \n# Older Versions\n\n## Current Version\n\n   1. Update References\n   \n   2. Using again data at [this dataset](https:\/\/www.kaggle.com\/phanttan\/disastertweet-prepared2) \n\n## Version 6\n\n   1. Add 4e-5 into learning_rate for Tunning\n   \n   2. Using data at [this dataset](https:\/\/www.kaggle.com\/phanttan\/disastertweets-prepared) \n   \n## Version 5\n\n   1. Find the maximum length to create smaller data to model (from 256 -> 149)\n\n[Content](#0)","6036609a":"Range from 120 to 140 characters is the most common in tweet.","5ef4a6e8":"<a id=0><\/a>\n## <p style=\"background-color:lightblue; font-family:newtimeroman; font-size:120%; text-align:left; border-radius: 15px 50px;\">Table of Content<\/p>\n* [0. What are updated in the last version?](#0)\n* [1. Loading Data](#1)\n* [2. EDA ](#2)\n* [3. Data Preprocessing](#3)\n* [4. Vectorization](#4)\n    * [4.1 Common Vectorizer Usage](#4.1)\n    * [4.2 If-Idf Term Weightings](#4.2)\n\n* [5. BERT model](#5)\n    * [5.1 Preprocessing Data](#5.1)\n    * [5.2 DistilBERT model with Fine-tuning using Keras](#5.2)\n* [6. Make a Submission](#6)\n* [7. References](#7)","9ff0e057":"# If you like this kernel, please upvote and tell me your thought. Thank you @@","55327eea":"<a id=4.2 ><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">4.2 TF-IDF<\/p>\nReference: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n\n[Content](#0)","e8c2efe4":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Pre-processing <\/p>\n\nNow we are going to engineering the data to make it easier for the model to clasiffy.\n\nThis section is very important to reduce the dimensions of the problem.\n\n\n[Content](#0)","481c1e38":"<a id=5.2 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.2 DistilBERT model with Fine-tuning using Keras <\/p>\n\n[Content](#0)\n\nThe DistilBERT model was proposed in the blog post Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT","f4185d5e":"### Re-instantiate the hypermodel and train it with the optimal number of epochs from above","07dcdbd5":"## Instantiate the tuner and perform hypertuning","f2934ea2":"[Keras Tuner](https:\/\/keras.io\/keras_tuner)\n\n[Distil Bert](https:\/\/huggingface.co\/transformers\/model_doc\/distilbert.html)","bfe8c575":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Loading Data \ud83d\udc8e<\/p>\n\nJust load the dataset and global variables for colors and so on.\n\n[Content](#0)","5d2f55bb":"<a id=6 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">6. Make a Submission<\/p>\n\n[Content](#0)","8c5ca809":"<a id=4.1 ><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">4.1 Common Vectorizer Usage<\/p>\nReference: https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#common-vectorizer-usage\n\n[Content](#0)"}}