{"cell_type":{"9b3f47c9":"code","8d79da3e":"code","263c5d67":"code","73a6ac58":"code","207f532b":"code","bd01e2b9":"code","11863fdc":"code","10623822":"code","bb72bebf":"code","dcbef9ad":"code","0248359b":"code","473a08bc":"code","cb427cd0":"code","bfdba6c3":"code","4e904b4a":"code","4a01cb82":"code","ca89b1b7":"code","d6d239a3":"code","810e577c":"code","bce99cc0":"code","9ee71534":"code","3036673c":"code","1a09d618":"code","e568e978":"code","c4a6b781":"code","f3058cfa":"code","43ab078c":"code","b892ca07":"code","b088e268":"code","2d50dfdc":"code","fc2962c4":"code","e73bae43":"code","f7f35397":"code","998cbc91":"code","c3aa62cb":"code","c9179989":"code","c9aa03ed":"code","d2d7fb86":"code","254ff34a":"code","fd6d322c":"code","63716b86":"code","26a9ac16":"code","a3f28c76":"code","a179f224":"code","43662889":"code","4a2b26cb":"code","d1ab0178":"code","a84e6ac0":"code","b7411317":"code","f3d23b3f":"code","a3154f4f":"code","6fcc77ff":"code","d707bd74":"code","c89ea217":"code","322a4483":"code","803642d7":"code","847408da":"code","b0298a74":"code","33d83d4c":"code","a4b43983":"code","6a95d64e":"code","dab0f6a9":"code","c8e78de4":"code","b6258eac":"markdown","a172eb3f":"markdown","dd73a617":"markdown","9828d818":"markdown","a41b738c":"markdown","7385d055":"markdown","b2eb42c5":"markdown","ea2eaeaa":"markdown","141510a4":"markdown","3969c5ff":"markdown","fab74459":"markdown","0eb3cb8d":"markdown","0bdae872":"markdown","13de0978":"markdown","d5470f93":"markdown","67ff1a5a":"markdown","0312ecbb":"markdown","a7e92847":"markdown","44d2e455":"markdown","ccbfd129":"markdown","650fb25d":"markdown","b5bdc0c1":"markdown","5ed842d5":"markdown","945a1436":"markdown","8e231712":"markdown","e623c36b":"markdown","f06c097f":"markdown","0e3acd10":"markdown","b300deba":"markdown","2e63c21a":"markdown","a9036b94":"markdown","70369608":"markdown","653bbac8":"markdown","94a67dcd":"markdown","54d93cd8":"markdown","454f3c0e":"markdown","cf37b6a9":"markdown","0c3f0ef4":"markdown","563e1b33":"markdown","ebfc962e":"markdown","f5ff8164":"markdown","7b83b859":"markdown","a60bf17d":"markdown","6454e17b":"markdown","feeb16a9":"markdown","d4cc6916":"markdown","69abfabe":"markdown","f041f3c9":"markdown","9202d201":"markdown","79714b28":"markdown","aa803251":"markdown","c0d1f900":"markdown","9f00972f":"markdown","f817991f":"markdown","5d71e2de":"markdown","465cb316":"markdown","cf0a2b0b":"markdown","99e76df8":"markdown","7fae3dad":"markdown","5c26e74a":"markdown","c564fa00":"markdown","90e27a77":"markdown","17a92b79":"markdown","1ee0ede0":"markdown","eabf9548":"markdown","0906a308":"markdown","ba13f2f3":"markdown","a2232d9f":"markdown","551ecffb":"markdown","88efab36":"markdown","1a18a187":"markdown","407a7793":"markdown","24abeb24":"markdown","59b631d3":"markdown","c56e373b":"markdown","345886e8":"markdown","ca385743":"markdown","3e590d7f":"markdown"},"source":{"9b3f47c9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 80)\n\n\n#  Kaggle directories\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n\n#  Load the Datasets\ndf_TRN = pd.read_csv('..\/input\/train.csv')\ndf_TST = pd.read_csv('..\/input\/test.csv')\nprint(\"training set:\\t\", df_TRN.shape, \"\\ntest set:\\t\",df_TST.shape, \"\\t- no \\\"Survived\\\" column\")\nprint(\"\\nColumns:\\n\",df_TRN.columns.values)","8d79da3e":"df_TRN.describe()  # NUMERIC DATA","263c5d67":"df_TRN.describe(include='O')  # CATEGORICAL DATA","73a6ac58":"#  heatmap of null values\nfig = plt.figure(figsize=(12,5))\nfig.add_subplot(121)\nplt.title('df_TRN - \"NULLs\\\"')\nsns.heatmap(df_TRN.isnull(), cmap='gray')\nfig.add_subplot(122)\nplt.title('df_TST - \"NULLs\\\"')\nsns.heatmap(df_TST.isnull(), cmap='gray')\nplt.show()\n\nfor i in [df_TRN,df_TST]:\n    nulls = i.isnull().sum().sort_values(ascending = False)\n    prcet = round(nulls\/len(i)*100,2)\n    i.null = pd.concat([nulls, prcet], axis = 1,keys= ['Total', 'Percent'])\nprint(pd.concat([df_TRN.null,df_TST.null], axis=1,keys=['TRAIN Data - NULL', 'TEST Data - NULL']))\n\n#  check for duplicate values\nprint('\\n\\nDuplicated - TRAIN:  {}'.format(df_TRN.duplicated().sum()))\nprint('Duplicated - TEST:   {}'.format(df_TST.duplicated().sum()))","207f532b":"#  check for null\nprint(df_TRN[['PassengerId','Embarked']][df_TRN['Embarked'].isnull()])\nprint(df_TST[['PassengerId','Fare']][df_TST['Fare'].isnull()])\n\ndf_TRN['Embarked'].fillna(df_TRN.Embarked.mode()[0], inplace=True) # fill with mode\ndf_TST['Fare'].fillna(df_TST['Fare'].mean(), inplace=True)      # fill with mean\n\n#  verify nulls were filled\nprint(df_TRN[['PassengerId','Embarked']][df_TRN['PassengerId'].isin([62,830])])\nprint(df_TST[['PassengerId','Fare']][df_TST['PassengerId'] == 1044])","bd01e2b9":"fig = plt.figure(figsize=(12,5))\nfig.add_subplot(121)\nplt.title('TRAIN - Age\/Sex per Passenger Class')\nsns.barplot(data=df_TRN, x='Pclass',y='Age',hue='Sex')\nfig.add_subplot(122)\nplt.title('TEST - Age\/Sex per Passenger Class')\nsns.barplot(data=df_TST, x='Pclass',y='Age',hue='Sex')\nplt.show()","11863fdc":"#  calculate age per pclass and sex\n#  training - mean Age per Pclass and Sex\nmeanAgeTrnMale = round(df_TRN[(df_TRN['Sex'] == \"male\")]['Age'].groupby(df_TRN['Pclass']).mean(),2)\nmeanAgeTrnFeMale = round(df_TRN[(df_TRN['Sex'] == \"female\")]['Age'].groupby(df_TRN['Pclass']).mean(),2)\n\n#  test - - mean Age per Pclass and Sex\nmeanAgeTstMale = round(df_TST[(df_TST['Sex'] == \"male\")]['Age'].groupby(df_TST['Pclass']).mean(),2)\nmeanAgeTstFeMale = round(df_TST[(df_TST['Sex'] == \"female\")]['Age'].groupby(df_TST['Pclass']).mean(),2)\n\nprint('\\n\\t\\tMEAN AGE PER SEX PER PCLASS')\nprint(pd.concat([meanAgeTrnMale, meanAgeTrnFeMale,meanAgeTstMale, meanAgeTstFeMale], axis = 1,keys= ['TRN-Male','TRN-Female','TST-Male','TST-Female']))","10623822":"#  define function APS to fill Age NaN for training data\ndef age_fillna_TRN(APStrn):\n    Age     = APStrn[0]\n    Pclass  = APStrn[1]\n    Sex     = APStrn[2]\n    \n    if pd.isnull(Age):\n        if Sex == 'male':\n            if Pclass == 1:\n                return 41.28\n            if Pclass == 2:\n                return 30.74\n            if Pclass == 3:\n                return 26.51\n\n        if Sex == 'female':\n            if Pclass == 1:\n                return 34.61\n            if Pclass == 2:\n                return 28.72\n            if Pclass == 3:\n                return 21.75\n    else:\n        return Age\n\n#  define function APS to fill Age NaN for test data\ndef age_fillna_TST(APStst):\n    Age     = APStst[0]\n    Pclass  = APStst[1]\n    Sex     = APStst[2]\n    \n    if pd.isnull(Age):\n        if Sex == 'male':\n            if Pclass == 1:\n                return 40.52\n            if Pclass == 2:\n                return 30.94\n            if Pclass == 3:\n                return 24.53\n\n        if Sex == 'female':\n            if Pclass == 1:\n                return 41.33\n            if Pclass == 2:\n                return 24.38\n            if Pclass == 3:\n                return 23.07\n    else:\n        return Age","bb72bebf":"#  execute Age functions\ndf_TRN['Age'] = df_TRN[['Age','Pclass','Sex']].apply(age_fillna_TRN,axis=1)\ndf_TST['Age'] = df_TST[['Age','Pclass','Sex']].apply(age_fillna_TST,axis=1)\n\n#  Check missing Age values\nprint('Missing values for Age: \\ntraining\\t', df_TRN.Age.isnull().sum(), \"\\ntest\\t\\t\",df_TST.Age.isnull().sum())","dcbef9ad":"#  Steps 1, 2 and 3\nfor i in [df_TRN, df_TST]:\n    i.fillna(\"N\", inplace = True)      #  step 1\n    i.Cabin = [j[0] for j in i.Cabin]  #  step 2\n    i.uniq = i.Cabin.value_counts()\n    i.cost = i.groupby('Cabin')['Fare'].mean()  # step 3\n\nprint('Cabin Data and mean Fare\\n',pd.concat([df_TRN.uniq,df_TST.uniq,df_TRN.cost,df_TST.cost], axis=1,keys=['TRAIN Cabin', 'TEST Cabin', 'Train Fare', 'Test Fare']))","0248359b":"#  create dataframes with Cabin != N\ndf_TRN_noN = df_TRN[df_TRN['Cabin'] != 'N']\ndf_TST_noN = df_TST[df_TST['Cabin'] != 'N']\n\n#  plot TRAIN Cabin and Fare\nfig = plt.figure(figsize=(10,4))\nfig.add_subplot(121)\nplt.title('TRAIN - Cabin')\nsns.countplot(data=df_TRN_noN, x=df_TRN_noN['Cabin'].sort_values())\nfig.add_subplot(122)\nplt.title('TRAIN - Fare vs. Cabin')\nsns.lineplot(data=df_TRN_noN, x='Cabin', y='Fare')\nplt.show()\n\n#  plot TEST Cabin and Fare\nfig = plt.figure(figsize=(10,4))\nfig.add_subplot(121)\nplt.title('TEST - Cabin')\nsns.countplot(data=df_TST_noN, x=df_TST_noN['Cabin'].sort_values())\nfig.add_subplot(122)\nplt.title('TEST - Fare vs. Cabin')\nsns.lineplot(data=df_TST_noN, x='Cabin', y='Fare')\nplt.show()","473a08bc":"#  Step 4 - Assign Cabin letter to all \"N\" based on Fare\ndf_TRN.groupby('Cabin')['Fare'].mean().sort_values()\ndef cabin_fillN_TRN(i):\n    j = 0\n    if i < 16:\n        j = \"G\"\n    elif i >= 16 and i <27:\n        j = \"F\"\n    elif i >= 27 and i <37:\n        j = \"T\"\n    elif i >= 37 and i <43:\n        j = \"A\"\n    elif i >= 43 and i <51:\n        j = \"E\"\n    elif i >= 51 and i <79:\n        j = \"D\"\n    elif i >= 79 and i <107:\n        j = \"C\"\n    else:\n        j = \"B\"\n    return j\n\ndf_TST.groupby('Cabin')['Fare'].mean().sort_values()\ndef cabin_fillN_TST(i):\n    j = 0\n    if i < 17:\n        j = \"G\"\n    elif i >= 17 and i <30:\n        j = \"F\"\n    elif i >= 30 and i <43:\n        j = \"D\"\n    elif i >= 43 and i <64:\n        j = \"A\"\n    elif i >= 64 and i <103:\n        j = \"E\"\n    elif i >= 103 and i <133:\n        j = \"C\"\n    else:\n        j = \"B\"\n    return j\n\n","cb427cd0":"#  Run function - fill out all of Cabin per mean\nprint(\"BEFORE\\ntraining Cabin values:\\t\",df_TRN.Cabin.sort_values().unique())\nprint(\"test Cabin values:\\t\",df_TST.Cabin.sort_values().unique())\n\ndf_TRN.Cabin =  df_TRN.Fare.apply(cabin_fillN_TRN)\ndf_TST.Cabin =  df_TST.Fare.apply(cabin_fillN_TST)\n\nprint(\"\\nAFTER\\ntraining Cabin values:\\t\",df_TRN.Cabin.sort_values().unique())\nprint(\"test Cabin values:\\t\",df_TST.Cabin.sort_values().unique())","bfdba6c3":"#  heatmap of null values\nfig = plt.figure(figsize=(12,5))\nfig.add_subplot(121)\nplt.title('df_TRN - \"NULLs\\\"')\nsns.heatmap(df_TRN.isnull(), cmap='gray')\nfig.add_subplot(122)\nplt.title('df_TST - \"NULLs\\\"')\nsns.heatmap(df_TST.isnull(), cmap='gray')\nplt.show()\n\nfor i in [df_TRN,df_TST]:\n    nulls = i.isnull().sum().sort_values(ascending = False)\n    prcet = round(nulls\/len(i)*100,2)\n    i.null = pd.concat([nulls, prcet], axis = 1,keys= ['Total', 'Percent'])\nprint(pd.concat([df_TRN.null,df_TST.null], axis=1,keys=['TRAIN Data - NULL', 'TEST Data - NULL']))\n\n#  check for duplicate values\nprint('\\n\\nDuplicated - TRAIN:  {}'.format(df_TRN.duplicated().sum()))\nprint('Duplicated - TEST:   {}'.format(df_TST.duplicated().sum()))","4e904b4a":"df_TRN['Title'] = df_TRN['Name'].str.split(',', expand = True)[1].str.split(' ', expand = True)[1]\ndf_TST['Title'] = df_TST['Name'].str.split(',', expand = True)[1].str.split(' ', expand = True)[1]\n\n#  create a list of all the titles\ntitles = sorted(pd.concat([df_TRN['Title'], df_TST['Title']]).unique())\nprint(titles)","4a01cb82":"#  Check Titles that may be male or female\nfor i in [df_TRN,df_TST]:\n    print(i[['PassengerId','Title','Sex']][i['Title'].isin(['Capt.', 'Col.', 'Dr.', 'Major.', 'Rev.', 'the'])],\"\\n\")\n    \n#  \nprint(df_TRN[['PassengerId','Title','Sex']][df_TRN['PassengerId'].isin([760,797])])","ca89b1b7":"def replace_titles(x):\n    title=x['Title']\n    if title in ['Don.', 'Sir.']:\n        return 'Mr.'       # adult male\n    elif title in ['Jonkheer.', 'Master.']:\n        return 'Master.'    # young male\n    elif title in ['Dona.', 'Lady.', 'Mme.']:\n        return 'Mrs.'      # adult female\n    elif title in ['Mlle.', 'Ms.']:\n        return 'Miss.'     #  young female\n    elif title in ['Capt.', 'Col.', 'Dr.', 'Major.', 'Rev.', 'the']:\n        if x['Sex']=='Male':\n            return 'Mr.'\n        else:\n            return 'Mrs.'\n    else:\n        return title\n\n#  Run the function\ndf_TRN['Title']=df_TRN.apply(replace_titles, axis=1)\ndf_TST['Title']=df_TST.apply(replace_titles, axis=1)\n\nprint('df_TRN Titles:\\t',df_TRN.Title.unique())\nprint('df_TST Titles:\\t',df_TST.Title.unique())","d6d239a3":"#  Re-Check Titles that may be male or female\nprint(df_TRN[['PassengerId','Title','Sex']][df_TRN['PassengerId'].isin([760,797])])","810e577c":"fig = plt.figure(figsize=(12,5))\nfig.add_subplot(121)\nplt.title('df_TRN - \"Title\\\"')\nax = sns.countplot(data = df_TRN, x = 'Title')\nfor p in ax.patches:\n    ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nfig.add_subplot(122)\nplt.title('df_TST - \"Title\\\"')\nax = sns.countplot(data = df_TST, x = 'Title')\nfor p in ax.patches:\n    ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()","bce99cc0":"for i in [df_TRN,df_TST]:\n    i['FamilySize'] = i['SibSp'] + i['Parch'] + 1\n    #  IsAlone - create attribute\n    i['IsAlone']  = 0    # set default to '0'\n    i.loc[i['FamilySize'] == 1, 'IsAlone'] = 1\n\n#  check\nprint(sorted(df_TRN.FamilySize.unique()))\nprint(sorted(df_TST.FamilySize.unique()))","9ee71534":"fig = plt.figure(figsize=(12,4))\nfig.add_subplot(121)\nplt.title('df_TRN - \"FamilySize\\\"')\nax = sns.countplot(data = df_TRN, x = 'FamilySize')\nfor p in ax.patches:\n    ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nfig.add_subplot(122)\nplt.title('df_TST - \"FamilySize\\\"')\nax = sns.countplot(data = df_TST, x = 'FamilySize')\nfor p in ax.patches:\n    ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()","3036673c":"df_TRN['FarePerPerson'] = df_TRN['Fare']\/(df_TRN['FamilySize'])\ndf_TST['FarePerPerson'] = df_TST['Fare']\/(df_TST['FamilySize'])\n\nprint(df_TRN[['Fare','FamilySize','FarePerPerson']].groupby(['FamilySize']).mean())","1a09d618":"fig = plt.figure(figsize=(12,4))\nfig.add_subplot(121)\nplt.title('df_TRN - \"FarePerPerson\\\"')\nsns.barplot(x='FamilySize',y='FarePerPerson', data=df_TRN)\nfig.add_subplot(122)\nplt.title('df_TST - \"FarePerPerson\\\"')\nsns.barplot(x='FamilySize',y='FarePerPerson', data=df_TST)\nplt.show()","e568e978":"def ageGroup(i):\n    j = 0\n    if i < 1:\n        j = \"Infant\"\n    elif i >= 1 and i <13:\n        j = \"Child\"\n    elif i >= 13 and i <19:\n        j = \"Teenager\"\n    elif i >= 19 and i <35:\n        j = \"Young Adult\"\n    elif i >= 35 and i <65:\n        j = \"Adult\"\n    else:\n        j = \"Elderly\"\n    return j\n\ndf_TRN['AgeGroup']=df_TRN.Age.apply(ageGroup)\ndf_TST['AgeGroup']=df_TST.Age.apply(ageGroup)\n\nprint(df_TRN.AgeGroup.unique())","c4a6b781":"fig = plt.figure(figsize=(12,4))\nfig.add_subplot(121)\nplt.title('df_TRN - \"AgeGroup\\\"')\nax = sns.countplot(data = df_TRN, x = 'AgeGroup',order=['Infant','Child','Teenager','Young Adult','Adult','Elderly'])\nfor p in ax.patches:\n    ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nfig.add_subplot(122)\nplt.title('df_TST - \"AgeGroup\\\"')\nax = sns.countplot(data = df_TST, x = 'AgeGroup',order=['Infant','Child','Teenager','Young Adult','Adult','Elderly'])\nfor p in ax.patches:\n    ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()","f3058cfa":"#  Funtion survival_rate\n#  input:  attributes\n#  output:  printsurvival rates for all unique values in the attribute\ndef survival_rate(*args):\n    for i in args:\n        print(\"{:12}   ---------------------------------\".format(i.upper()))\n        x = sorted(df_TRN[i].unique())  # values in attribute\n        for j in x:\n            y = len(df_TRN[i][(df_TRN[i] == j) & (df_TRN['Survived'] == 1)])  # survived number\n            z = len(df_TRN[i][df_TRN[i] == j])   # total number\n            print('   {:<12}{:3} out of {:3} survived -  {:3.2%}'.format(j,y,z,y\/z))\n    return\nprint(\"\\tfunction \\'survival_rate\\' created.\")","43ab078c":"fig = plt.figure(figsize=(6,6))\nsns.countplot(x='Survived',data=df_TRN, palette='Set1')\nplt.title('Overall Survival (training dataset)',fontsize= 16)\nplt.xlabel('Passenger Fate',fontsize = 14)\nplt.ylabel('Count\/Percentage of Passengers',fontsize = 14)\nplt.axis('auto')\nplt.xticks(np.arange(2), ['died', 'survived'])\nlabels = df_TRN['Survived'].value_counts()\nfor x, y in enumerate(labels):\n    z = \"{}\\n({:.2%})\".format(y,y\/len(df_TRN))\n    plt.text(x, y-60, str(z), ha = 'center', va='center', size = 18)\nplt.show()","b892ca07":"plotList = ['Sex', 'Title']\n\nfig = plt.figure(figsize=(12,5))\nplotNum  = 1     # initialize plot number\nfor i in plotList:\n    fig.add_subplot(1,2,plotNum)\n    ax = sns.countplot(x=sorted(df_TRN[i]),hue='Survived',data=df_TRN)\n    plt.title('Survival per \\\"{}\\\"'.format(i), fontsize=14)\n    plt.xlabel(i, fontsize=12)\n    plt.ylabel('Survival Rate', fontsize=12)\n    plt.axis('auto')\n    plt.legend(('died', 'survived'), loc='best')\n    for p in ax.patches:\n        ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n    plotNum = plotNum + 1\nplt.show()\n\n#  Survival Rate\nfor i in plotList:\n    survival_rate(i)","b088e268":"plotList = ['Pclass','Cabin','Embarked']\n\nfig = plt.figure(figsize=(12,12))\nplotNum  = 1     # initialize plot number\nfor i in plotList:\n    fig.add_subplot(2,2,plotNum)\n    ax = sns.countplot(x=sorted(df_TRN[i]),hue='Survived',data=df_TRN)\n    plt.title('Survival Rate per \\\"{}\\\"'.format(i), fontsize=14)\n    plt.xlabel(i, fontsize=12)\n    plt.ylabel('Survival Rate', fontsize=12)\n    plt.axis('auto')\n    plt.legend(('died', 'survived'), loc='best')\n    for p in ax.patches:\n        ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n    plotNum = plotNum + 1\n    if i == 'Pclass':  #  add kde plot for Pclass (numeric)\n        fig.add_subplot(2,2,plotNum)\n        df_TRN.Pclass[df_TRN.Survived == 0].plot(kind='kde')\n        df_TRN.Pclass[df_TRN.Survived == 1].plot(kind='kde')\n        plt.title('Survival Rate per \\\"{}\\\"'.format(i), fontsize=14)\n        plt.xlabel(i, fontsize=12)\n        plt.ylabel('Survival Rate', fontsize=12)\n        plt.legend(('died', 'survived'), loc='best')\n        plt.xlim(0,4)\n        plotNum = plotNum + 1\nplt.show()\n\n#  Survival Rate\nfor i in plotList:\n    survival_rate(i)","2d50dfdc":"plotList = ['SibSp', 'Parch', 'FamilySize']\n\nfig = plt.figure(figsize=(12,16))\nplotNum  = 1     # initialize plot number\nfor i in plotList:\n    fig.add_subplot(3,2,plotNum)\n    ax = sns.countplot(x=sorted(df_TRN[i]),hue='Survived',data=df_TRN)\n    plt.title('Survival Rate per \\\"{}\\\"'.format(i), fontsize=14)\n    plt.xlabel(i, fontsize=12)\n    plt.ylabel('Survival Rate', fontsize=12)\n    plt.axis('auto')\n    plt.legend(('died', 'survived'), loc='best')\n    for p in ax.patches:\n        ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n    plotNum = plotNum + 1\n    fig.add_subplot(3,2,plotNum)\n    df_TRN[i][df_TRN.Survived == 0].plot(kind='kde')\n    df_TRN[i][df_TRN.Survived == 1].plot(kind='kde')\n    plt.title('Survival Rate per \\\"{}\\\"'.format(i), fontsize=14)\n    plt.xlabel(i)\n    plt.legend(('died', 'survived'), loc='best')\n    plt.xlim(0,6)\n    plotNum = plotNum + 1\nplt.show()\n\n#  Survival Rate\nfor i in plotList:\n    survival_rate(i)","fc2962c4":"fig = plt.figure(figsize=(12,6))\n#  Survival per Age\nfig.add_subplot(121)\ndf_TRN.Age[df_TRN.Survived == 0].plot(kind='kde')\ndf_TRN.Age[df_TRN.Survived == 1].plot(kind='kde')\nplt.title('Survival Rate per Age', fontsize=14)\nplt.legend(('died', 'survived'), loc='best')\nplt.xlim(0,100)\n\nfig.add_subplot(122)\n#  Survival per AgeGroup\nax = sns.countplot(x='AgeGroup',data=df_TRN,hue='Survived',order=['Infant','Child','Teenager','Young Adult','Adult','Elderly'], palette='hsv')\nplt.title('Survival per Age Group',fontsize= 14)\nplt.xlabel('Age Groups',fontsize = 12)\nplt.ylabel('Count',fontsize = 12)\nplt.legend(['died', 'survived'])\nplt.axis('auto')\nfor p in ax.patches:\n   ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()\n\nsurvival_rate('AgeGroup')\nprint('\\nmean age: {:.2f}.'.format(df_TRN.Age.mean()))\nprint('mode age: {:.2f}.'.format(df_TRN.Age.mode()[0]))","e73bae43":"#  Plot - Survival\/Death for Fare\nplt.figure(figsize=(12,6))\ndf_TRN.Fare[df_TRN.Survived == 0].plot(kind='kde')\ndf_TRN.Fare[df_TRN.Survived == 1].plot(kind='kde')\nplt.title('Survival Rate per Fare', fontsize=14)\nplt.legend(('died', 'survived'), loc='best')\nplt.xlim(0,200)\n\nprint('max fare: ${:.2f}.'.format(df_TRN.Fare.max()))","f7f35397":"#  Plot - Survival\/Death for FarePerPerson\nplt.figure(figsize=(12,6))\ndf_TRN.FarePerPerson[df_TRN.Survived == 0].plot(kind='kde')\ndf_TRN.FarePerPerson[df_TRN.Survived == 1].plot(kind='kde')\nplt.title('Survival Rate per FarePerPerson', fontsize=14)\nplt.legend(('died', 'survived'), loc='best')\nplt.xlim(0,120)","998cbc91":"fig = plt.figure(figsize=(12,5))\nfig.add_subplot(121)\ndf_TRN.Age[df_TRN.Pclass == 1].plot(kind='kde')\ndf_TRN.Age[df_TRN.Pclass == 2].plot(kind='kde')\ndf_TRN.Age[df_TRN.Pclass == 3].plot(kind='kde')\nplt.title('Age Distribution per Class')\nplt.xlabel('Age')\nplt.legend(('1st Class','2nd Class','3rd Class'), loc='best')\nplt.xlim(0,100)\n\nfig.add_subplot(122)\ndf_TRN.Fare[df_TRN.Pclass == 1].plot(kind='kde')\ndf_TRN.Fare[df_TRN.Pclass == 2].plot(kind='kde')\ndf_TRN.Fare[df_TRN.Pclass == 3].plot(kind='kde')\nplt.title('Fare Distribution per Class')\nplt.xlabel('Fare')\nplt.legend(('1st Class','2nd Class','3rd Class'), loc='best')\nplt.xlim(0,120)\nplt.show()\n\n#  Statistical Summary \ndf_class = pd.DataFrame(columns = {'1st-Age','1st-Fare','2nd-Age','2nd-Fare','3rd-Age','3rd-Fare'})\ndf_class[['1st-Age','1st-Fare']] = df_TRN[['Age','Fare']][df_TRN.Pclass == 1].describe()\ndf_class[['2nd-Age','2nd-Fare']] = df_TRN[['Age','Fare']][df_TRN.Pclass == 2].describe()\ndf_class[['3rd-Age','3rd-Fare']] = df_TRN[['Age','Fare']][df_TRN.Pclass == 3].describe()\ndf_class = df_class[['1st-Age','2nd-Age','3rd-Age','1st-Fare','2nd-Fare','3rd-Fare']]\ndf_class","c3aa62cb":"df_TRN['AgeBand'] = pd.cut(df_TRN['Age'],5)\ndf_TST['AgeBand'] = pd.cut(df_TST['Age'],5)\ndf_TRN[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","c9179989":"for i in [df_TRN,df_TST]:\n    #  Age - ordinal values\n    i.loc[i['Age'] <= 16, 'Age'] = 0\n    i.loc[(i['Age'] > 16) & (i['Age'] <= 32), 'Age'] = 1\n    i.loc[(i['Age'] > 32) & (i['Age'] <= 48), 'Age'] = 2\n    i.loc[(i['Age'] > 48) & (i['Age'] <= 64), 'Age'] = 3\n    i.loc[i['Age'] > 64, 'Age'] = 4\n\nax = sns.countplot(x=df_TRN.Age)\nplt.title('df_TRN.Age')\nfor p in ax.patches:\n   ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()","c9aa03ed":"df_TRN['FareBand'] = pd.qcut(df_TRN['Fare'],4)\ndf_TST['FareBand'] = pd.qcut(df_TST['Fare'],4)\ndf_TRN[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","d2d7fb86":"for i in [df_TRN,df_TST]:\n    #  Fare - ordinal values\n    i.loc[i['Fare'] <= 7.91, 'Fare'] = 0\n    i.loc[(i['Fare'] > 7.91) & (i['Fare'] <= 14.454), 'Fare'] = 1\n    i.loc[(i['Fare'] > 14.454) & (i['Fare'] <= 31), 'Fare'] = 2\n    i.loc[i['Fare'] > 31, 'Fare'] = 3\n\nax = sns.countplot(x=df_TRN.Fare)\nplt.title('df_TRN.Fare')\nfor p in ax.patches:\n   ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()","254ff34a":"df_TRN['AgeClass'] = df_TRN['Age'] * df_TRN['Pclass']\ndf_TST['AgeClass'] = df_TST['Age'] * df_TST['Pclass']\n\nax = sns.countplot(x=df_TRN.AgeClass)\nplt.title('df_TRN.AgeClass')\nfor p in ax.patches:\n   ax.annotate(\"%.0f\" % p.get_height(),(p.get_x()+p.get_width()\/2.,p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()","fd6d322c":"for i in [df_TRN,df_TST]:\n    i['Sex'] = i['Sex'].map({'male':0, 'female':1})\n    i['Embarked'] = i['Embarked'].map({'C':0,'Q':1,'S':2})\n    i['Title'] = i['Title'].map({'Mr.':0,'Mrs.':1,'Master.':2,'Miss.':3})\n\ndf_TRN[['Sex','Embarked','Title']].head(5)","63716b86":"#  copy the PassengerId to another dataframe\n#  will need it for submission\npassID = df_TST.filter(['PassengerId'], axis=1)\n\ndrop_columns = ['AgeBand','AgeGroup','Cabin','FareBand','Name','Parch','PassengerId','SibSp','Ticket']\n\nfor i in drop_columns:\n    df_TRN = df_TRN.drop(i, axis=1)\n    df_TST = df_TST.drop(i, axis=1)\n\nprint(df_TRN.columns.values)\nprint(df_TST.columns.values)","26a9ac16":"#  change type to int\nfor i in ['Age','Fare','IsAlone','AgeClass']:\n    df_TRN[i] = df_TRN[i].astype('int64')\n    df_TST[i] = df_TST[i].astype('int64')","a3f28c76":"df_TRN.head()","a179f224":"from sklearn.preprocessing import MinMaxScaler\n\nnormTRN = MinMaxScaler().fit_transform(df_TRN)\nnormTST = MinMaxScaler().fit_transform(df_TST)\n\n#  create dataframe with normalized data\ndf_TRN = pd.DataFrame(normTRN, index=df_TRN.index, columns=df_TRN.columns)\ndf_TST = pd.DataFrame(normTST, index=df_TST.index, columns=df_TST.columns)\n\n#  move Survived columns to first position\ncol = df_TRN['Survived']\ndf_TRN.drop(labels=['Survived'], axis=1, inplace = True)\ndf_TRN.insert(0, 'Survived', col)\ndf_TRN.columns.values\n\ndf_TRN.head()","43662889":"from sklearn.ensemble import ExtraTreesClassifier\n\ny = df_TRN['Survived']\n#  sort by column names\nX = df_TRN.drop(['Survived'], axis=1).sort_index(axis=1)\n\n# Building the model \nextra_tree_forest = ExtraTreesClassifier(n_estimators = 5,criterion ='entropy', max_features = 5) \n# Training the model \nextra_tree_forest.fit(X, y) \n# Computing the importance of each feature \nfeature_importance = extra_tree_forest.feature_importances_ \n# Normalizing the individual importances \nfeature_importance_normalized = np.std([tree.feature_importances_ for tree in extra_tree_forest.estimators_], axis = 0) \n\n# Plot - compare feature importance\nplt.figure(figsize=(8,6))\nsns.barplot(x=feature_importance_normalized,y=X.columns)\nplt.xlabel('Feature Importance',fontsize=12)\nplt.ylabel('Feature',fontsize=12)\nplt.title('Comparison of Feature Importances', fontsize=14)\nplt.show()","4a2b26cb":"#  Correlation TABLE\ncorrALL = df_TRN.corr()['Survived'].sort_values(ascending=False)\ncorrALL = corrALL.drop(['Survived'])\n\n#  heatmap and barplot\nfig = plt.figure(figsize=(16,8))\nfig.add_subplot(121)\nplt.title('Titanic Survival - Correlation-OVERALL', fontsize=14)\nsns.heatmap(df_TRN.corr(), annot=True, fmt='.2f', square=True, cmap = 'Greens')\nfig.add_subplot(122)\nplt.title('Titanic Survival - Correlation-OVERALL', fontsize=14)\nax = sns.barplot(y=corrALL.index,x=corrALL.values)\nfor i in ax.patches: \n    plt.text(i.get_width()\/1.5, i.get_y()+.5,  \n             str(round((i.get_width()), 4)), \n             fontsize = 12, fontweight ='bold', \n             color ='black')\nplt.show()","d1ab0178":"#  Correlation FEMALE - filter dataframe for female\ndataFemale = df_TRN[(df_TRN['Sex'] == 1)]\ndataFemaleCorr = dataFemale.drop([\"Sex\",\"Title\"], axis=1).corr()\ncorrF = dataFemaleCorr['Survived'].sort_values(ascending=False)\ncorrF = corrF.drop(['Survived'])\n\n#  heatmap and barplot\nfig = plt.figure(figsize=(12,6))\nfig.add_subplot(121)\nplt.title('Titanic Survival - Correlation-FEMALE', fontsize=14)\nsns.heatmap(dataFemaleCorr, annot=True, fmt='.2f', square=True, cmap = 'Reds')\nfig.add_subplot(122)\nplt.title('Titanic Survival - Correlation-FEMALE', fontsize=14)\nax = sns.barplot(y=corrF.index,x=corrF.values)\nfor i in ax.patches: \n    plt.text(i.get_width()\/1.5, i.get_y()+.5,  \n             str(round((i.get_width()), 4)), \n             fontsize = 12, fontweight ='bold', \n             color ='black')\nplt.show()","a84e6ac0":"dataMale   = df_TRN[(df_TRN['Sex'] == 0)]\ndataMaleCorr = dataMale.drop([\"Sex\",\"Title\"], axis=1).corr()\ncorrM = dataMaleCorr['Survived'].sort_values(ascending=False)\ncorrM = corrM.drop(['Survived'])\n\n#  heatmap and barplot\nfig = plt.figure(figsize=(12,6))\nfig.add_subplot(121)\nplt.title('Titanic Survival - Correlation-MALE', fontsize=14)\nsns.heatmap(dataMaleCorr, annot=True, fmt='.2f', square=True, cmap = 'Blues')\nfig.add_subplot(122)\nplt.title('Titanic Survival - Correlation-MALE', fontsize=14)\nax = sns.barplot(y=corrM.index,x=corrM.values)\nfor i in ax.patches: \n    plt.text(i.get_width()\/1.5, i.get_y()+.5,  \n             str(round((i.get_width()), 4)), \n             fontsize = 12, fontweight ='bold', \n             color ='black')\nplt.show()","b7411317":"corrALL = pd.DataFrame(columns = ['MALE','correlation-m','FEMALE','correlation-f'])\ncorrALL['MALE']   = corrM.index\ncorrALL['correlation-m'] = corrM.values\ncorrALL['FEMALE'] = corrF.index\ncorrALL['correlation-f'] = corrF.values\ncorrALL","f3d23b3f":"from sklearn.model_selection import train_test_split\n\nX = df_TRN.drop(['Survived'], axis = 1)\ny = df_TRN['Survived']\n\nseed = 7\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\n\nX_train.head()","a3154f4f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n#  Define the Classification Models\nmodels = []\nmodels.append(('DT   ', DecisionTreeClassifier()))\nmodels.append(('KNN  ', KNeighborsClassifier()))\nmodels.append(('LR   ', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('NB   ', GaussianNB()))\nmodels.append(('RF   ', RandomForestClassifier()))\nmodels.append(('SVC  ', SVC(gamma='auto')))\nmodels.append(('lSVC ', LinearSVC()))","6fcc77ff":"from sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\n\n#  Evaluate the Models:\nresults = []\nnames = []\nmodelDF = pd.DataFrame(columns=['model','CV-mean','CV-std','AccuracyScore'])\ncountDF = 0\n\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    model.fit(X_train,y_train)\n    modelPredict = model.predict(X_test)\n    accu = accuracy_score(y_test,modelPredict)\n    print(\"{0:s}:  {1:3.5f}  ({2:3.5f})  {3:.2%}\".format(name,cv_results.mean(),cv_results.std(),accu))\n    modelDF.loc[countDF]=[name,cv_results.mean(),cv_results.std(),accu]\n    countDF = countDF + 1","d707bd74":"#  pick the best model from ModelDF\nmax(modelDF['CV-mean'])\nmaxCV = modelDF[(modelDF['CV-mean'] == max(modelDF['CV-mean']))]\nmaxCV","c89ea217":"best_model = KNeighborsClassifier()\nbest_model.fit(X_train,y_train)\nprint(best_model)\n\n#  predict\ny_predict = best_model.predict(X_test)\ny_predict[0:10]","322a4483":"from sklearn.model_selection import cross_val_score\n\ncross_val = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')\n\nprint(\"Cross Validation Scores:         {}\".format(cross_val))\nprint('Cross Validation Scores - mean:  {:3.4%}'.format(cross_val.mean()))","803642d7":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test,y_predict)\nprint('Accuracy Score:  {:3.4%}'.format(accuracy_score(y_test,y_predict)))","847408da":"from sklearn.metrics import f1_score\n\nf1score = f1_score(y_test, y_predict)\nprint('F1 Score:  {:3.4%}'.format(f1score))","b0298a74":"from sklearn.metrics import confusion_matrix\n\nconf_matrix = confusion_matrix(y_test, y_predict)\n\nsns.heatmap(conf_matrix, annot=True,cmap='Blues',annot_kws={\"size\": 36})\nplt.title(\"Confusion Matrix, F1 Score: {:3.4%}\".format(f1score))\nplt.show()","33d83d4c":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nbest_model.probability = True   # need for predict_proba to work\nbest_model.fit(X_train,y_train)\ny_predita = best_model.predict_proba(X_test)\ny_predita = y_predita[:,1]   # positive values only\n\nROC_AUC = roc_auc_score(y_test, y_predita)\nfpr, tpr, thresholds = roc_curve(y_test, y_predita)\n\nplt.plot([0,1],[0,1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.title(\"ROC Curve, ROC_AUC Score: {:3.4%}\".format(ROC_AUC))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","a4b43983":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test,y_predict))","6a95d64e":"from sklearn.metrics import log_loss\n\ny_predict_prob = best_model.predict_proba(X_test)\nprint(y_predict_prob[0:5])\n\nprint(\"\\nLog Loss:  {:3.4}\".format(log_loss(y_test, y_predict_prob)))","dab0f6a9":"# Check shape of TEST data\nprint('Test data shapes must match order to \\\"fit\\\":')\nprint('shape X_test:\\t{}'.format(X_test.shape))\nprint('shape y_test:\\t{}'.format(y_test.shape))\nprint('shape df_TST:\\t{}'.format(df_TST.shape))","c8e78de4":"print(best_model)\nbest_model.fit(X_test,y_test)          #  fit\nSF = best_model.predict(df_TST)        #  predictions\n\n#  Add PassengerId back into test data\ndf_TST['PassengerId'] = passID['PassengerId']\n\nSF = pd.DataFrame(SF, columns=['Survived'])\nSF_TST = pd.concat([df_TST, SF], axis=1, join='inner')\nSF_final = SF_TST[['PassengerId','Survived']].astype(\"int64\")\n\n#  SF_final.to_csv('<path>\/predictions.csv', index=False)\nprint('all done ...')","b6258eac":"[go to top of document](#top)     \n\n---\n\n#  2.  Data Cleaning<a id=\"prep\"><\/a>\nData cleaning is always required to ensure that the analysis is based on as complete a dataset as possible.  This typically involves taking care of null and duplicate values.\n\n### Data Cleaning Sections:\n[2.1  Check for NULLs\/Duplicates](#prep_null)<br>\n[2.2  Fill NULLs for Embarked and Fare](#prep_fare)<br>\n[2.3  Fill NULLs for Age](#prep_age)<br>\n[2.4  Fill NULLs for Cabin](#prep_cabin)<br>\n[2.5  Final Check of NULLs\/Duplicates](#prep_final)<br>","a172eb3f":"## 4.1  Plot - Overall Survival<a id=\"eda_over\"><\/a>\nWe'll start with looking at the overall survival rate of Titanic passengers before digging deeper into other attributes.","dd73a617":"[go to top of section](#eda)\n\n## 4.6  Plot - Fare and FarePerPerson<a id=\"eda_Fare\"><\/a>\nPlots look at the distribution of Fare and FarePerPerson.","9828d818":"[go to top of section](#eda)\n\n## 4.4  Plot - SibSp, Parch and FamilySize<a id=\"eda_SibParFam\"><\/a>\nPassengers will family members.<br><br>\nSibSp \t    - number of Siblings or Spouses<br>\nParch \t    - number of Parents or Children<br>\nFamilySize  - total number of family members","a41b738c":"## 7.1 Train\/Test Split <a id=\"model_split\"><\/a>\nTrain\/Test Split randomly splits a dataset into training and testing subsets.  The model learns on the training set based on known output, and the test data is used to evaluate the accuracy of the model.","7385d055":"---\n<b>CAUTION:<\/b> Make sure that 'PassengerId' and 'Survived' are saved as **int64** in the submission file.  Otherwise, the submission will be accepted, but scored as '0.000'.\n\n---","b2eb42c5":"[go to top of section](#model)\n\n## 7.2 Classification Models <a id=\"model_class\"><\/a>   \nIn machine learning, **classification** is a *supervised* learning approach which attempts to learn the relationship between a set of feature variables and a target variable. The target attribute in classification is a categorical variable with discrete values such as yes\/no, start\/go, on\/off, survived\/died.","ea2eaeaa":"[go to Correlation section](#corr_corr)\n###  6.3.2 Correlation - FEMALE passengers<a id=\"corr_female\"><\/a>","141510a4":"[go to top of section](#corr)\n\n##  6.2 Feature Importance <a id=\"corr_fi\"><\/a>\nFeature selection is the process of identifying the most significant features from a given dataset.","3969c5ff":"[go to top of document](#top)     \n\n---\n<a id=\"eda\"><\/a>\n![titanic_sinking.jpg](attachment:titanic_sinking.jpg)\n\n#  4. Exploratory Data Analysis\nNow that the data has been cleaned and engineered, we can start visualizing the data and see if we can gain some insight into the survival rates.\n\n### Exploratory Data Analysis Section:\n[4.1  Plot - Overall Survival](#eda_over)<br>\n[4.2  Plot - Sex and Title](#eda_sexTitle)<br>\n[4.3  Plot - Pclass, Cabin and Embarked](#eda_PclassEmb)<br>\n[4.4  Plot - SibSp, Parch and FamilySize](#eda_SibParFam)<br>\n[4.5  Plot - Age and AgeGroup](#eda_Age)<br>\n[4.6  Plot - Fare and FarePerPerson](#eda_Fare)<br>\n[4.7  Plot - Age and Fare distribution per PClass](#eda_AgePclass)<br>\n\n\n###  Function for Survival Rate:\nThe plots will give a good visual representation of the survival rates.  However, we'd like to see the actual numeric survival rates also.  Function **survival_rate** is created to accomplish this and will used throughout the analysis.","fab74459":"[go to top of document](#top)     \n\n---\n\n#  6.  Feature Importance and Correlations<a id=\"corr\"><\/a>\nThis section will encode and normalize the datasets, plot feature importance and show the overall, female and male correlations.\n\n[6.1 Normalize Data ](#corr_norm)<br>\n[6.2 Feature Importance](#corr_fi)<br>\n[6.3 Correlation - Overall, Female and Male](#corr_corr)<br>","0eb3cb8d":"Cabin and Fare information vary considerable between the training and test datasets.  In the next  step, we will populate all the missing Cabin values based on the average Fare for that Cabin.","0bdae872":"We can see that all except PassengerIDs 760 and 797 in training are female.\n\nCreate function **replace_titles** to update the titles, verifying **Sex** of some of the titles before replacement.","13de0978":"[go to top of section](#engr2)\n##  5.6  Change Datatypes to 'int64'<a id=\"engr2_int64\"><\/a>\nVerify that ordinal attribute datatypes to all integer.","d5470f93":"[back to \"Evaluate the Model\"](#model_eval)\n### 7.6.4 Receiver Operating Characteristics (ROC) Curve<a id=\"eval_roc\"><\/a>   \nAUC\u2013ROC curve is the model selection probability curve. AUC area is covered by the curve is the area between the orange line (ROC) and the axis.  The bigger the area covered, the better the machine learning models. Ideal value for AUC is 1.\n\n*  **ROC** - Receiver Operating Characteristics\n*  **AUC** - Area Under the Curve","67ff1a5a":"We'll need to dig a little deeper and determine the mean Age per Pclass and Sex for both the datasets separately.  This will be done in two steps:\n\n  1.  Calculate the mean Age per Pclass and Sex for both datasets\n  2.  Use the values from the above table to fill out NULLs for Age\n\n**Step 1:  Calculate Age per Pclass and Sex for training and test datasets:**","0312ecbb":"**Step 2:  Use function *age_fillna* to fill out NULLs for Age for training and test datasets:**","a7e92847":"We knew that **Sex** and **Title** would be significant factors in surviving the Titanic Disaster.  The correlation section will quantify the survival rates per attribute.","44d2e455":"###  Statistic Summary - CATEGORICAL data:","ccbfd129":"**OBSERVATIONS:**\n\n**FarePerPerson** and **Embarked** were the most important factors for both male and female passengers.  This makes sense since most of the First Class passengers, paying the highest fare and boarding at Cherbourg, were kept informed of the disaster by the senior staff of the Titanic. They were given priority, along with women and children, to the lifeboats.","650fb25d":"## 6.1  Normalize Data <a id=\"corr_norm\"><\/a>\nNormalization is a rescaling of the data from the original range so that all values are within a certain range, typically between 0 and 1.  Normalized data is essential in machine learning.  Correlation and models will not produce good results if the scales are not standardized.","b5bdc0c1":"[go to top of document](#top)     \n\n---\n\n# END\n\nPlease upvote if you found this useful :-)\n\n---\n***PS:  There was room for Jack!***","5ed842d5":"[go to top of section](#model)\n\n## 7.3 Evaluate the Model<a id=\"model_eval\"><\/a>\nEvaluation allows the quality of the model to be assessed.  Following metrics will demonstrate the validity of the model prior to implementation with the test data.\n\n[7.6.1 Cross Validation Score](#eval_cv)<br> \n[7.6.2 Accuracy Score](#eval_acc)<br> \n[7.6.3 F1 Score](#eval_f1)<br> \n[7.6.4 Confusion Matrix](#eval_conf)<br> \n[7.6.5 Receiver Operating Characteristics (ROC) Curve](#eval_roc)<br> \n[7.6.6 Classification Report](#eval_class)<br> \n[7.6.7 Log Loss](#eval_log)<br>","945a1436":"[go to top of section](#engr2)\n##  5.5  Drop Unneeded Columns<a id=\"engr2_drop\"><\/a>\nFollowing columns will not be used for machine learning and will be dropped from training and test datasets:\n\n-  **AgeBand**:  Age is more informative than AgeBand\n-  **AgeGroup**:  Age is more informative than AgeGroup\n-  **Cabin**:  FarePerPerson is more informative than the location of the cabin\n-  **FareBand**:  FarePerPerson is more informative than just the Fare amount\n-  **Name**:  Titles already extracted, Name column no longer needed\n-  **Parch**:  FamilySize created, Parch column no longer needed\n-  **PassengerId**:  not needed for modeling, however, will be saved (*passID*) for the submission file\n-  **SibSp**:  FamilySize created, SibSp column no longer needed\n-  **Ticket**:  No information in this alphanumeric column","8e231712":"[back to \"Evaluate the Model\"](#model_eval)\n### 7.6.4 Confusion Matrix<a id=\"eval_conf\"><\/a>   \nConfusion matrix shows the corrected and wrong predictions, in comparison with the actual labels. It shows the model\u2019s ability to correctly predict or separate the classes.\n\n   - **True Positive [0,0]** \u2013 model predicted positive class correctly to be a positive class\n   - **False Positive [0,1]** \u2013 model predicted negative class incorrectly to be a positive class\n   - **False Negative [1,0]** \u2013 model predicted positive class incorrectly to be the negative class\n   - **True Negative [1,1]** \u2013 model predicted negative class correctly to be the negative class","e623c36b":"-  training dataset has 891 samples or 40% of the actual passengers\n-  test dataset has 418 samples or ~19% of the actual passengers\n-  survival rate of the sample population is 38%, whereas the actual survival rate was 32%\n-  most of the passengers were travelling in the 3rd Class\n-  most of the passengers embarked from Southampton\n-  most of the passengers were under 30s years old","f06c097f":"**Import Python Libraries & Load Titanic Datasets:**","0e3acd10":"###  6.3.1 Correlation - OVERALL passengers<a id=\"corr_over\"><\/a>","b300deba":"-  Training dataset contains 891 unique names of passengers\n-  64% of the passengers were male\n-  Ticket column contains alphanumeric values\n-  Cabin column contains alphanumeric values\n-  Most of the passengers boarded from Southampton","2e63c21a":"[go to top of section](#engr2)\n\n##  5.2  Create New Column - FareBand<a id=\"engr2_fare\"><\/a>\nCreate a placeholder attribute called **FareBand** using **quantile cut (pd.qcut)** and update **Fare** with ordinal values from **FareBand**.","a9036b94":"[go to top of section](#eda)\n\n## 4.5  Plot - Age and AgeGroup<a id=\"eda_Age\"><\/a>\n**Age Group** will show the age ranges of the passengers that survived or died and is defined as:\n\n|  Age Group | age ranges (years) |\n|:----|:----|\n|Infant     | < 1|\n|Child      | >= 1 and < 13|\n|Teenager   | >= 13 and < 19|\n|Young Adult| >= 19 and < 35|\n|Adult      | >= 35 and < 65|\n|Elderly    | >= 65|","70369608":"[go to top of section](#engr)     \n\n## 3.3 Create New Attributes - FamilySize and IsAlone<a id=\"engr_familySize\"><\/a>\n**SibSp** and **Parch** can be combined into a new attribute **FamilySize** to give a better overview of the passengers with families.  New attribute **IsAlone** will show whether a passenger was travelling alone or with family.","653bbac8":"<a id=\"top\"><\/a>  \n# Titanic Disaster - FE, EDA, Correlation, ML\nRMS Titanic was a British passenger liner operated by the White Star Line and hailed as \"unsinkable\".  It sank in the Atlantic Ocean after striking an iceberg on the morning of 15 April 1912 during its maiden voyage from Southampton to New York City.  Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking of the Titanic one of the modern history's deadliest peacetime commercial marine disasters (source: en.wikipedia.org\/wiki\/RMS_Titanic).\n\n---\n![Titanic-Cobh-Harbour-1912.jpg](attachment:Titanic-Cobh-Harbour-1912.jpg)\n\n---\n**Titanic: Machine Learning from Disaster** dataset contains information on 1309 passengers (891 for train & 418 for test) on the Titanic.  The challenge is to predict which of these passengers survived based on their age, sex, cabin, etc.  \n\nThis notebook will cover data cleaning, feature engineering, exploratory data analysis, mapping, feature importance, correlations, machine learning, model selection, evaluation of the model and finally, submitting the prediction file for the competition.\n\n#### Table of Content\n1. [Titanic Dataset](#titanic)   \n2. [Data Cleaning](#prep)  (fill out missing information)<br>\n3. [Feature Engineering #1](#engr)   (extract meaningful information and create attributes)<br>\n4. [Exploratory Data Analysis](#eda)   (insights via plots)\n5. [Feature Engineering #2](#engr2)   (create additional  attributes and map values)<br>\n6. [Feature Importance and Correlations](#corr)   (normalization and correlations)\n7. [Machine Learning](#model)  (model selection and evaluation of model)\n8. [Submission File](#sub_file)  ","94a67dcd":"[go to top of section](#eda)\n\n## 4.2  Plot - Sex and Title<a id=\"eda_sexTitle\"><\/a>\nPlots for **Sex** will show that it's a major factor in the survival of the sinking.  **Title** will further breakdown **Sex** into young (master & miss) and old (mr and mrs) passengers.","54d93cd8":"[back to \"Evaluate the Model\"](#model_eval)\n### 7.6.3 F1 Score<a id=\"eval_f1\"><\/a>   \n**F1 Score** is the weighted average of Precision and Recall.","454f3c0e":"[back to \"Evaluate the Model\"](#model_eval)\n### 7.6.5 Log Loss<a id=\"eval_log\"><\/a> \nLogarithmic loss measures the performance of a classification model where the prediction input is a probability value between 0 and 1. The goal of machine learning models is to minimize this value. A perfect model would have a log loss of 0.","cf37b6a9":"[go to top of document](#top)     \n\n---\n![fe_titanic_combo.jpg](attachment:fe_titanic_combo.jpg)\n\n#  3. Feature Engineering #1<a id=\"engr\"><\/a>\nFeature engineering involves any modifications to the original dataset to facilitate machine learning algorithms.  This typically involves extracting meaningful information from attributes and\/or creating new attributes.\n\n\n###  Feature Engineering Section:\n[3.1  Extract Cabin Information](#prep_cabin) (in previous section)<br>\n[3.2  Extract Title from Name](#engr_title)<br>\n[3.3  Create New Attributes - Family Size and IsAlone](#engr_familySize)<br>\n[3.4  Create New Attribute - Fare per Person](#engr_farePerson)<br>\n\n\nNOTE:  This section is based on *feature engineering* article in www.kaggle.com\/c\/titanic\/data","0c3f0ef4":"---\n![passenger.jpg](attachment:passenger.jpg)\n\n\n#  1.  Titanic Dataset <a id=\"titanic\"><\/a>\nThe Titanic dataset is a subset of the actual number of 2,224 passengers on-board the Titanic.  The dataset contains the following attributes that can be identified as **Categorical**, **Numeric** and **Text** datatypes:\n\n|**ATTRIBUTE**|**DATA TYPE**|**DESCRIPTION**|**KEY**|\n|:-----|:-----|:-----|:-----|\n| PassengerId \t| ``NUMERIC`` \t| Passenger ID\/Ticket Number| |\n| Survived \t| ``CATEGORICAL`` \t| Female Survived? \t| 0 = No, 1 = Yes |\n| Pclass \t| ``CATEGORICAL``\t| Ticket Class \t\t| 1 = 1st, 2 = 2nd, 3 = 3rd |\n| Name\t\t| ``TEXT``\t\t| Passenger Name\t| |\n| Sex \t\t| ``CATEGORICAL``\t| Male or Female \t| |\n| Age\t\t| ``NUMERIC``\t| Passenger Age\t\t| |\n| SibSp\t\t| ``CATEGORICAL``\t| # of Siblings or Spouses | |\n| Parch\t\t| ``CATEGORICAL``\t| # of Parents or Children | |\n| Ticket\t| ``TEXT``\t\t| Ticket Number \t|  |\n| Fare\t\t| ``NUMERIC``\t| Passenger Fare | |\n| Cabin\t\t| ``NUMERIC``\t| Cabin Number  | |\n| Embarked\t| ``CATEGORICAL``\t| Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |\n\n###  Statistic Summary - NUMERIC data:","563e1b33":"[go to top of section](#prep)     \n\n## 2.2  Fill NULLs for Embarked and Fare<a id=\"prep_fare\"><\/a>\nWe will fill the NULLs for **Embarked** with the most common port of embarkation and the **Fare** with the mean fare.","ebfc962e":"The datasets contains the following unique Titles:\n\n|  **Title** |  Description |  notes  |\n|:-----|:-----|:-----|\n| ``Capt.``|Captain | male or female  |   \n| ``Col.``|Colonel  | male or female   |\n| ``Don.``|male title  |    |\n| ``Dona.``|female title  |    |\n| ``Dr.``|doctor  | male or female   |\n| ``Jonkheer.``|young lord  |    |\n| ``Lady.``|lady  |    |\n| ``Major.``|Major  | male or female   |\n| ``Master.``|young male  |    |\n| ``Miss.``|young female  |    |\n| ``Mlle.``|Mademoiselle  |    |\n| ``Mme.``|Madam  |    |\n| ``Mr.``|adult male  |    |\n| ``Mrs.``|adult female  |    |\n| ``Ms.``|young female  |    |\n| ``Rev.``|Reverend   | male or female   |\n| ``Sir.``|adult male  |    |\n| ``the``|  unknown  | male or female   |\n\nFollowing Titles will have to confirmed as male or female, and replaced with either \"Mr.\" or \"Mrs.\"\n\n-  Capt., Col., Dr., Major., Rev., the","f5ff8164":"Yay!!  All the NULLs have been taken care of!!","7b83b859":"[go to top of section](#engr)     \n\n## 3.2 Extract Title from Name<a id=\"engr_title\"><\/a>\nEach name in the dataset contains a salutation which can be extracted.  We can use the split function to achieve this and save it in a new column called \"Title\".","a60bf17d":"Verifying that PassengerIDs 760 and 797 titles were correctly updated to \"Mrs.\"","6454e17b":"**OBSERVATIONS:**\n\n*  **Overall**:  Chances of surviving the sinking were around 40%.  This may sound good (or ok, at least) till we start looking into other factors such as sex, age, family sizes, fare, etc. in the next sections.","feeb16a9":"[go to top of section](#eda)\n\n## 4.7  Plot - Age and Fare distribution per Pclass<a id=\"eda_AgePclass\"><\/a>\nPlots look at the distribution of Age and Fare based on the passenger class (Pclass).","d4cc6916":"[go to top of document](#top)     \n\n---\n#  8. Submission File<a id=\"sub_file\"><\/a>\nUsing model with the best Cross Validation score to run the predictions and submit the file.","69abfabe":"[back to \"Evaluate the Model\"](#model_eval)\n### 7.6.5 Classification Report<a id=\"eval_class\"><\/a>   \n**Precision** is a measure of the accuracy, provided that a class label has been predicted. It is defined by:   \n*  ```precision = True Positive\/(True Positive + False Positive)```   \n    \n**Recall** is the true positive rate:    \n*  ```recall = True Positive\/(True Positive + False Negative)```   \n    \n**F1-Score** is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (which represents perfect precision and recall) and its worst at 0    \n*  ```F1-Score = 2x (precision x recall)\/ (precision + recall)```","f041f3c9":"[go to Correlation section](#corr_corr)\n###  6.3.4 Correlation Table<a id=\"corr_table\"><\/a>","9202d201":"**Step 4.  Use function cabin_fillN to assign Cabin letter based on mean Fare per Cabin**","79714b28":"**OBSERVATIONS:**\n\n*  **1st Class:**  Average age of 1st class passengers is close to 40, with the fare being almost 4 times more than the other two classes\n*  **2nd Class:**  Average age of 2nd class passengers is close to 30, which implies that most of the young adults were 2nd class passengers\n*  **3rd Class:**  Average age of 3rd class passengers is around 25, with the fares being almost half of 2nd class","aa803251":"[back to \"Evaluate the Model\"](#model_eval)\n### 7.6.1 Cross Validation Score<a id=\"eval_cv\"><\/a>   \n**Cross Validation Score** splits the dataset into K equal groups. Each group is referred to as a fold.  Some of the folds are used for training and the remaining for testing the model.  The process is repeated until each partition is used for both training and testing.","c0d1f900":"**OBSERVATIONS:**\n*  **Fare**:  Passengers paying the lower fares were most likely to die.  That is primarily due to the fact that most of the passengers on the Titanic were travelling in the 2nd or 3rd class.\n*  **FarePerPerson**:  Again, we see that younger people paying the lower fares were most like to die.","9f00972f":"[go to top of section](#prep)     \n\n## 2.5  Final Check of NULLs\/Duplicates<a id=\"prep_final\"><\/a>\nFinal check of NULLs\/Duplicates before moving on to feature engineering.","f817991f":"[go to top of section](#corr)\n\n##  6.3 Correlation <a id=\"corr_corr\"><\/a>\nCorrelation is a statistical test of association between variables that is measured on a -1 to 1 scale. The closer the correlation value is to -1 or 1 the stronger the association, the closer to 0, the weaker the association. It measures how change in one variable is associated with change in another variable. \n\nThe following sub-sections will quantify the survival rates per attribute:\n\n-  [OVERALL passengers](#corr_over)\n-  [FEMALE passengers](#corr_female) - survival for female only\n-  [MALE passengers](#corr_male) - survival for male only\n-  [TABLE](#corr_table)","5d71e2de":"[go to top of section](#engr)     \n\n## 3.5 Create New Attribute - AgeGroup<a id=\"engr_farePerson\"><\/a>\n **AgeGroup** is based on the ages of the passenger and will be useful for visualizations.  It will be dropped for correlation and modeling.\n\n|  Age Group | age ranges (years) |\n|:----|:----|\n|Infant     | < 1|\n|Child      | >= 1 and < 13|\n|Teenager   | >= 13 and < 19|\n|Young Adult| >= 19 and < 35|\n|Adult      | >= 35 and < 65|\n|Elderly    | >= 65|","465cb316":"**OBSERVATIONS:**\n\n*  **SibSp**:  Passengers with 1 or 2 siblings or spouse were more like to survive the sinking, but passenger survival rates decreased dramatically if there were more than 3 family members.\n*  **Parch**:  Passengers with children were more likely to survive, however, if there were more than four family members, the survival rate went down significantly.  This could be due larger families wanting to stay together on the ship and hope for a rescue.\n*  **FamilySize**:  Families with three of less members had a better chance of surviving.  Again, family size greater that four significantly impacted the chances of survival.  Passengers that were by themselves were more likely to die.","cf0a2b0b":"[go to top of section](#engr2)\n##  5.4  Mapping - Sex, Embarked and Title<a id=\"engr2_map\"><\/a>\nMap values for Sex, Embarked and Title as:\n\n\n|  attribute | mapping |\n|:----|:----|\n|Sex | male=0, female=1|\n|Embarked | C = 0, Q = 1, S = 2|\n|Title | Mr. = 0,Mrs. = 1,Master. = 2,Miss. = 3|","99e76df8":"## 2.1  Check for NULLs\/Duplicates <a id=\"prep_null\"><\/a>\nCleaning up the NULL and duplicate values in the dataset.","7fae3dad":"Update **Fare** with ordinal values from the **FareBand** table.","5c26e74a":"[go to top of document](#top)     \n\n---\n![titanic_cabin1.jpg](attachment:titanic_cabin1.jpg)\n\n#  7. Machine Learning<a id=\"model\"><\/a>\nMachine Learning can be used to identify relationships and trends in data that might otherwise not be accessible or identified.  It gives computers the ability to learn without being explicitly programmed.  For the Titanic Disaster dataset, the training dataset will use supervised classification machine learning algorithms will be used to predict the survival outcomes of the passengers in the test dataset.\n\n[7.1 Train\/Test Split](#model_split)<br>\n[7.2 Classification Models](#model_class)<br>\n[7.3 Evaluate the Model](#model_eval)<br>","c564fa00":"[back to \"Evaluate the Model\"](#model_eval)\n### 7.6.2 Accuracy Score<a id=\"eval_acc\"><\/a>   \n**Accuracy Score** function computes subset accuracy in a multilabel classification dataset and is equal to the **Jaccard Score** function in binary and multiclass classification.","90e27a77":"Update **Age** with ordinal values from the **AgeBand** table.","17a92b79":"**There are no duplicates, but four attributes have NULL values.  Following actions will be taken:**\n* [Embarked](#prep_fare):  fill NULLs with most common location\n* [Fare](#prep_fare):      fill NULLs with mean value\n* [Age](#prep_age):       fill NULLs with mean age per Pclass per sex\n* [Cabin](#prep_cabin):     fill NULLs based on Fare","1ee0ede0":"[go to top of document](#top)     \n\n---\n\n#  5.  Feature Engineering #2<a id=\"engr2\"><\/a>\n\nSince the EDA section would have been impacted, some of the attributes were not further engineered.  They will be now created\/engineered, along with mapping and dropping unneeded columns.\n\n[5.1  Create New Column - AgeBand](#engr2_age)<br>\n[5.2  Create New Column - FareBand](#engr2_fare)<br>\n[5.3  Create New Column - AgeClass](#engr2_ageClass)<br>\n[5.4  Mapping - Sex, Embarked and Title](#engr2_map)<br>\n[5.5  Drop Unneeded Columns](#engr2_drop)<br>\n[5.6  Change Datatypes to int64](#engr2_int64)<br>","eabf9548":"[go to top of section](#eda)\n\n## 4.3  Plot - Pclass, Cabin and Embarked<a id=\"eda_PclassEmb\"><\/a>\nPassenger class, cabin location and port of embarkation played significant roles in the survival rate of the passengers.","0906a308":"**OBSERVATIONS:**\n\n*  **Age**:  Most of the passengers were around 30 years old and they were most likely to die.  As we've seen on the Sex plots, majority of the 30 year old surviving were adult females, hence the chances of an adult male surviving were very low.\n*  **AgeGroup**:  Elderlies were most likely to die while infants were most likely to survive.  Young Adults, which made up most of the passengers, barely had a 35% change of surviving.","ba13f2f3":"**Picking the algorithm with the best Cross Validation Score.**","a2232d9f":"## 3.1 Extract Cabin Information\nCabin feature engineering was completed as part of [Cabin NULL values](#prep_cabin) in the previous chapter.","551ecffb":"[go to Correlation section](#corr_corr)\n###  6.3.3 Correlation - MALE passengers<a id=\"corr_male\"><\/a>","88efab36":"---\n**Step 1.  Fill missing Cabin data with \"N\"**<br>\n**Step 2.  Strip the letter off of Cabin, discard the numeric**<br>\n**Step 3.  Calculate the mean Fare per Cabin**","1a18a187":"[go to top of section](#prep)     \n\n## 2.3  Fill NULLs for Age<a id=\"prep_age\"><\/a>\nWe cannot fill the NULLs for age with the overall mean age because:\n- first class passengers were much older than the other classes\n- there is a significant difference in Pclass female passenger ages\n- Mean ages for Pclass and Sex are significantly different in both datasets","407a7793":"**OBSERVATIONS:**\n\n*  **Sex**:  There were more male passengers on the Titanic, however, they also constituted a significant portion (over 81.11%) of the fatalities when the ship sank.  Females had a much higher (almost 75%) likelihood of surviving the disaster.\n*  **Title**:  Young males had a 56% likelihood of surviving, whereas, adult males were very likely to die.  Females, young and adult had an over 70% of surviving the sinking.","24abeb24":"###  Distribution Plots for FarePerPerson ","59b631d3":"##  5.1  Create New Column - AgeBand<a id=\"engr2_age\"><\/a>\nCreate a placeholder attribute called **AgeBand** and update **Age** with ordinal values from **AgeBand**.","c56e373b":"[go to top of section](#engr2)\n##  5.3  Create New Column - AgeClass<a id=\"engr2_ageClass\"><\/a>\nCreate attribute **AgeClass** by multiplying the ordinal values of **Age** and **Pclass**.","345886e8":"[go to top of section](#prep)     \n\n## 2.4  Fill NULLs for Cabin<a id=\"prep_cabin\"><\/a>\nAlmost 80% of **Cabin** data is missing.  This can be dropped, however, we can figure out a way to fill the missing Cabin data with a best-guess approach using the fare.\n\n1.  fill missing Cabin data with \"N\"\n2.  strip the letter off of Cabin, discard the numeric\n3.  calculate the mean Fare per Cabin\n4.  assign Cabin letter based on mean Fare per Cabin","ca385743":"[go to top of section](#engr)     \n\n## 3.4 Create New Attribute - FarePerPerson<a id=\"engr_farePerson\"><\/a>\n\n**FarePerPerson**  will give the average fare based on the size of the family.\n","3e590d7f":"**OBSERVATIONS:**\n\n\n*  **Pclass**:  Passengers in 1st class had the best chance of surviving (62.96%), whereas the 3rd class passenger had the highest chance of not surviving.\n*  **Cabin**:  Passengers in B, C and D classes had the best chances of surviving, whereas, passengers in E and G classes had the lowest chance of survival.\n*  **Embarked**:  Most of the first-class passengers boarded from Cherbourg, France, and were more likely to survive the disaster.  Third class passengers primarily boarded from Southampton and were less likely to survive."}}