{"cell_type":{"70aaf1e0":"code","750c6595":"code","89895cdc":"code","369f4074":"code","d87dbb66":"code","2ce80d58":"code","0d118eaf":"code","bffeeb73":"code","df56add8":"code","6bd0c4cf":"markdown","5ce64fd5":"markdown","5556934c":"markdown","de1f325c":"markdown","edccf4f1":"markdown"},"source":{"70aaf1e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import ElasticNet,ElasticNetCV\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LogisticRegression\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","750c6595":"df=pd.read_csv(\"..\/input\/chipotle-locations\/chipotle_stores.csv\")","89895cdc":"plt.figure(figsize=(25,15))\ng = sns.scatterplot(x=df['longitude'], y= df['latitude'], data=df, hue='state')\ng.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\nplt.savefig('map.png', bbox_inches=\"tight\")\nplt.show()","369f4074":"states = df.state.value_counts()\nstates","d87dbb66":"df_new = df.state.value_counts().rename_axis('state1').reset_index(name='count')\nnew_df = pd.merge(df, df_new, left_on='state', right_on='state1', how='left').drop('state1', axis=1)\n\nle = preprocessing.LabelEncoder()\nnew_df['state'] = le.fit_transform(new_df['state'])\nnew_df['location'] = le.fit_transform(new_df['location'])\nnew_df = new_df.drop(columns = ['address'])","2ce80d58":"relation = new_df.corr()\n\nfig = plt.figure(figsize = (10,10)) # Determines the size of the figure that will be displayed\n\nticks=[-1, -0.5 , 0 , +0.5, +1]  # Shows the interval of the colorbar displayed beside the heatmap\n\nsns.heatmap(relation, vmin = -1, vmax = 1, square = True,center=0, cmap='BrBG', annot=True,robust=True, cbar_kws= {'shrink' : 0.8 , \"ticks\" : ticks}, linewidths= 0.2)\n# vmin is the minimum range and vmax is the maximum range till which the heatmap will be displayed.\n# cbar_kws shrinks the colobar to the same size as the heatmap\n# linewidth is used to seperate the rows and coulnms by the given value to make the heatmap more presentable\n\nplt.title(\"Relationship between Inputs and Outputs using Heatmap\", fontsize = 16) ## Sets the title of the heatmap\nplt.savefig('Heatmap.png', bbox_inches=\"tight\")\nplt.show() ## Displaying the heatmap","0d118eaf":"X = new_df.drop(columns = ['count'])\ny = new_df[['count']]\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25)\n\nRandomForestmodel = RandomForestRegressor()\nRandomForestmodel.fit(X_train,y_train)\ny_pred = RandomForestmodel.predict(X_test)\n\nmse = mean_squared_error(y_test,y_pred)\nrmse = np.sqrt(mse)\nr2_score_model = r2_score(y_test, y_pred)\nprint(\"RMSE value of the model is:\", rmse)\nprint(\"R2 Score of the model is:\", r2_score_model)","bffeeb73":"logisticRegr = LogisticRegression()\nlogisticRegr.fit(X_train, y_train)\ny_pred = logisticRegr.predict(X_test)\n\nmse = mean_squared_error(y_test,y_pred)\nrmse = np.sqrt(mse)\nr2_score_model = r2_score(y_test, y_pred)\nprint(\"RMSE value of the model is:\", rmse)\nprint(\"R2 Score of the model is:\", r2_score_model)","df56add8":"alphas = [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 1]\n\nfor a in alphas:\n    model = ElasticNet(alpha=a).fit(X_train,y_train)   \n    pred_y = model.predict(X_test)\n    mse = mean_squared_error(y_test, pred_y)   \n    score = r2_score(y_test,pred_y)\n    print(\"Alpha:{0:.4f}, R2:{1:.2f}, MSE:{2:.2f}, RMSE:{3:.2f}\"\n       .format(a, score, mse, np.sqrt(mse)))","6bd0c4cf":"### Logistic Regression","5ce64fd5":"### Random Forest Model","5556934c":"### ElasticNet Regression","de1f325c":"### So, the best model for this dataset is Random Forest Regressor as the R2 score is 0.98 which is very close to 1.","edccf4f1":"#### So,from this we can deduce that California has the highest number of chipotle restaurants followed by Texas and Ohio respectively"}}