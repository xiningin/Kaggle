{"cell_type":{"88db36f2":"code","9c50cf63":"code","879c3f62":"code","d5cb056c":"code","cfcf2a32":"code","c53054ce":"code","671da945":"code","4dcbcac3":"code","189fffda":"code","5b562c62":"code","de3e8fcd":"code","d0a890df":"code","e1bd67b9":"code","153596fd":"code","8606d31a":"code","5f50283b":"code","b2d01526":"code","35ba2345":"code","2503ae90":"code","2eb652e9":"code","dea36e62":"code","ff2af511":"code","0be2914d":"code","5fa54f46":"code","2b29e6a7":"code","4a810cc5":"code","69ffe6b1":"code","10bce196":"code","1da3b891":"code","0cc9efe6":"code","d6e74583":"code","3f4be8e3":"code","02218a90":"code","80fc54d2":"code","64a58511":"code","d5efe4a0":"code","76454bc2":"code","9577b957":"code","525b2034":"code","a8019a64":"code","de1d61c4":"code","5889b7f0":"code","b167cc46":"code","5ff0b1b4":"code","e0cf6a97":"code","56c47e64":"code","738279eb":"code","1d3b2d26":"code","4aec1831":"code","2f7b7e6b":"code","5da83a1f":"code","f595df2d":"code","cf8f8369":"code","f1f711c3":"code","68cef841":"code","5a4bc6b6":"code","645261c1":"code","c2a08e31":"code","3c9c3df0":"code","5ce0312e":"code","6dadc269":"code","dc64ef63":"code","968ce70a":"code","3b262798":"code","95d871d5":"code","9ccf8c29":"code","5db2e8b1":"code","ccc827b5":"code","c2c96677":"code","e0aec917":"markdown","b39c89df":"markdown","94396e43":"markdown","3f50b536":"markdown","b20fa93b":"markdown","73eeb56d":"markdown","dc15b084":"markdown","18bfc680":"markdown","1d65f9d2":"markdown","503cdb8b":"markdown","b32aa3bc":"markdown","ed65c9c5":"markdown","fd7b6cf8":"markdown","1bb7cfb7":"markdown","5a4b0a50":"markdown","c9bb5563":"markdown","e73851d4":"markdown","208c7493":"markdown","3218f1a1":"markdown","213c0b9e":"markdown","08a51e79":"markdown","df79bdf7":"markdown"},"source":{"88db36f2":"# lets import the required libraries\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings \nwarnings.filterwarnings(\"ignore\") # never prints matching warning.\n","9c50cf63":"df = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')","879c3f62":"df.shape","d5cb056c":"df.info()  # for the detailed information about each column.\n# We can aslo identify if we have missing values.\n","cfcf2a32":"df.describe","c53054ce":"df['Credit_History'] = df['Credit_History'].astype('O')","671da945":"# describe categorical data (\"object\")\n\ndf.describe(include='O')","4dcbcac3":"#Lets drop the 'Loan_ID' attribute as it doesn't make any impact. \ndf.drop('Loan_ID', axis=1, inplace=True)","189fffda":"#To check if there's any duplicate rows.\ndf.duplicated().any()\n","5b562c62":"plt.figure(figsize=(8,6)) #This creates a figure object, which has a width of 8 inches and 6 inches in height.\nsns.countplot(df['Loan_Status']);","de3e8fcd":"print('The percentage of Y class : %.2f' % (df['Loan_Status'].value_counts()[0] \/ len(df)))\nprint('The percentage of N class : %.2f' % (df['Loan_Status'].value_counts()[1] \/ len(df)))","d0a890df":"df.columns","e1bd67b9":"#Credit History\n\ngrid = sns.FacetGrid(df, col = 'Loan_Status', size = 3.2, aspect = 1.6)\ngrid.map(sns.countplot, 'Credit_History');","153596fd":"#Gender\n\ngrid  = sns.FacetGrid(df, col = 'Loan_Status', size = 3.2, aspect  = 1.6)\ngrid.map(sns.countplot, 'Gender');","8606d31a":"#Married\ngrid = sns.FacetGrid(df , col = 'Loan_Status', size = 3.2, aspect = 1.6)\ngrid.map(sns.countplot, 'Married');","5f50283b":"#dependent\ngrid = sns.FacetGrid(df, col = 'Loan_Status', size=3.2, aspect= 1.6)\ngrid.map(sns.countplot, 'Dependents');","b2d01526":"#Education\ngrid = sns.FacetGrid(df, col = 'Loan_Status', size=3.2, aspect= 1.6)\ngrid.map(sns.countplot, 'Education');","35ba2345":"#Self_Employed\ngrid = sns.FacetGrid(df, col = 'Loan_Status', size=3.2, aspect= 1.6)\ngrid.map(sns.countplot, 'Self_Employed');","2503ae90":"#Property_Area\ngrid = sns.FacetGrid(df, col = 'Loan_Status', size=3.2, aspect= 1.6)\ngrid.map(sns.countplot, 'Property_Area');","2eb652e9":"#applicant income\n\nplt.scatter(df['ApplicantIncome'], df['Loan_Status']);","dea36e62":"df.groupby('Loan_Status').median()","ff2af511":"df.isnull().sum().sort_values(ascending = False)","0be2914d":"#Now let's start separating categorical and numerical data.\n\ncat_data = []\nnum_data = []\n\nfor i, c in enumerate(df.dtypes) :\n    if c == object:\n        cat_data.append(df.iloc[:, i])\n    else: \n        num_data.append(df.iloc[:, i])","5fa54f46":"cat_data = pd.DataFrame(cat_data).transpose()\nnum_data = pd.DataFrame(num_data).transpose()","2b29e6a7":"cat_data.head()","4a810cc5":"num_data.head()","69ffe6b1":"#for categorical data\n\ncat_data = cat_data.apply(lambda x:x.fillna(x.value_counts().index[0]))\n\ncat_data.isnull().sum().any()","10bce196":"num_data.fillna(method='bfill', inplace=True)\n\nnum_data.isnull().sum().any()","1da3b891":"from sklearn.preprocessing import LabelEncoder \nle = LabelEncoder()\ncat_data.head()","0cc9efe6":"#transform the target column.\ntarget_values = {'Y': 0 , 'N' : 1}\n\ntarget = cat_data['Loan_Status']\ncat_data.drop('Loan_Status', axis=1, inplace=True)\n\ntarget = target.map(target_values)","d6e74583":"#LETS transform all other columns.\nfor i in cat_data:\n    cat_data[i] = le.fit_transform(cat_data[i])","3f4be8e3":"target.head()","02218a90":"cat_data.head()","80fc54d2":"df = pd.concat([cat_data, num_data, target], axis =1)","64a58511":"df.head()","d5efe4a0":"X = pd.concat([cat_data,num_data], axis= 1)\ny = target","76454bc2":"from sklearn.model_selection import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n\nfor train, test in sss.split(X, y):\n    X_train, X_test = X.iloc[train], X.iloc[test]\n    y_train, y_test = y.iloc[train], y.iloc[test]\n    \nprint('X_train shape ', X_train.shape)\nprint('y_train shape', y_train.shape)\nprint('X_test shape ', X_test.shape)\nprint('y_test shape', y_test.shape)\n\n# almost same ratio\nprint('\\nratio of target in y_train :',y_train.value_counts().values\/ len(y_train))\nprint('ratio of target in y_test :',y_test.value_counts().values\/ len(y_test))\nprint('ratio of target in original_data :',df['Loan_Status'].value_counts().values\/ len(df))\n\n","9577b957":"#we can use 4 different algorithms.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodels = {\n    'LogisticRegression: ' : LogisticRegression(random_state=42), \n    'KNeighborsClassifier :' : KNeighborsClassifier(),\n    'SVC:' : SVC(random_state=42),\n    'DecisionTreeClassifier: ': DecisionTreeClassifier(max_depth=1,random_state=42)\n}","525b2034":"# loss\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score\n\ndef loss(y_true, y_pred, retu=False):\n    pre = precision_score(y_true, y_pred)\n    rec = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    loss = log_loss(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    \n    if retu:\n        return pre, rec, f1, loss, acc\n    else:\n        print('  pre: %.3f\\n  rec: %.3f\\n  f1: %.3f\\n  loss: %.3f\\n  acc: %.3f' % (pre, rec, f1, loss, acc))","a8019a64":"\ndef train_eval_train(models, X, y):\n    for name, model in models.items():\n        print(name,':')\n        model.fit(X, y)\n        loss(y, model.predict(X))\n        print('-'*30)\n        \ntrain_eval_train(models, X_train, y_train)\n","de1d61c4":"X_train.shape","5889b7f0":"#cross_validation\n\nfrom sklearn.model_selection import StratifiedKFold\nskf= StratifiedKFold(n_splits=10, random_state=42, shuffle = True)\n\ndef train_eval_cross(models, X , y, folds):\n    X = pd.DataFrame(X)\n    y = pd.DataFrame(y)\n    idx = ['pre', 'rec', 'f1', 'loss', 'acc'] \n    \n    for name, model in models.items():\n        ls = []\n        print(name, ':')\n        \n        for train, test in folds.split(X, y):\n            model.fit(X.iloc[train], y.iloc[train])\n            y_pred = model.predict(X.iloc[test])\n            ls.append(loss(y.iloc[test], y_pred, retu= True))\n        print(pd.DataFrame(np.array(ls).mean(axis =0),index = idx)[0])\n        print('-'*30)\n        \ntrain_eval_cross(models, X_train, y_train, skf)","b167cc46":"data_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);","5ff0b1b4":"X_train.head()","e0cf6a97":"X_train['new_col'] = X_train['CoapplicantIncome'] \/ X_train['ApplicantIncome']  \nX_train['new_col_2'] = X_train['LoanAmount'] * X_train['Loan_Amount_Term'] ","56c47e64":"data_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);","738279eb":"X_train.drop(['CoapplicantIncome', 'ApplicantIncome', 'Loan_Amount_Term', 'LoanAmount'], axis=1, inplace=True)","1d3b2d26":"train_eval_cross(models, X_train, y_train, skf)\n","4aec1831":"for i in range(X_train.shape[1]):\n    print(X_train.iloc[:,i].value_counts(), end='\\n------------------------------------------------\\n')","2f7b7e6b":"from scipy.stats import norm\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\n\nsns.distplot(X_train['new_col_2'], ax=ax[0], fit=norm)\nax[0].set_title('new_col_2 before log')\n\nX_train['new_col_2'] = np.log(X_train['new_col_2'])  # here we take the log of all these values.\n\nsns.distplot(X_train['new_col_2'], ax=ax[1], fit=norm)\nax[1].set_title('new_col_2 after log');","5da83a1f":"train_eval_cross(models, X_train, y_train, skf)\n# taking log has drastically improved our model.","f595df2d":"print('before:')\nprint(X_train['new_col'].value_counts())\n\nX_train['new_col'] = [x if x==0 else 1 for x in X_train['new_col']]\nprint('-'*50)\nprint('\\nafter:')\nprint(X_train['new_col'].value_counts())","cf8f8369":"train_eval_cross(models, X_train, y_train, skf)","f1f711c3":"for i in range(X_train.shape[1]):\n    print(X_train.iloc[:,i].value_counts(), end='\\n------------------------------------------------\\n')\n","68cef841":"sns.boxplot(X_train['new_col_2']);\nplt.title('new_col_2 outliers', fontsize=15);\nplt.xlabel('');","5a4bc6b6":"threshold= 0.1\n\nnew_col_2_out = X_train['new_col_2']\nq25, q75 = np.percentile(new_col_2_out, 25), np.percentile(new_col_2_out, 75) # Q25, Q75\nprint('Quartile 25: {} , Quartile 75: {}'.format(q25, q75))\n\niqr = q75 - q25\nprint('iqr: {}'.format(iqr))\n\ncut = iqr * threshold\nlower, upper = q25 - cut, q75 + cut\nprint('Cut Off: {}'.format(cut))\nprint('Lower: {}'.format(lower))\nprint('Upper: {}'.format(upper))\n\noutliers = [x for x in new_col_2_out if x < lower or x > upper]\nprint('Nubers of Outliers: {}'.format(len(outliers)))\nprint('outliers:{}'.format(outliers))\n\ndata_outliers = pd.concat([X_train, y_train], axis=1)\nprint('\\nlen X_train before dropping the outliers', len(data_outliers))\ndata_outliers = data_outliers.drop(data_outliers[(data_outliers['new_col_2'] > upper) | (data_outliers['new_col_2'] < lower)].index)\n\nprint('len X_train before dropping the outliers', len(data_outliers))","645261c1":"X_train = data_outliers.drop('Loan_Status', axis=1)\ny_train = data_outliers['Loan_Status']","c2a08e31":"sns.boxplot(X_train['new_col_2']);\nplt.title('new_col_2 without outliers', fontsize=15);\nplt.xlabel('');","3c9c3df0":"train_eval_cross(models, X_train, y_train, skf)","5ce0312e":"# Lets check the correlation.\ndata_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);","6dadc269":"X_train.drop(['Self_Employed'], axis=1, inplace=True)\n\ntrain_eval_cross(models, X_train, y_train, skf)\n","dc64ef63":"X_train.drop(['Dependents', 'new_col_2', 'Education', 'Gender', 'Property_Area','Married', 'new_col'], axis=1, inplace=True)","968ce70a":"data_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);","3b262798":"#X_test1 = pd.read_csv('test.csv')","95d871d5":"X_test.head()","9ccf8c29":"X_test_new = X_test.copy()","5db2e8b1":"x = []\n\nX_test_new['new_col'] = X_test_new['CoapplicantIncome'] \/ X_test_new['ApplicantIncome']  \nX_test_new['new_col_2'] = X_test_new['LoanAmount'] * X_test_new['Loan_Amount_Term']\nX_test_new.drop(['CoapplicantIncome', 'ApplicantIncome', 'Loan_Amount_Term', 'LoanAmount'], axis=1, inplace=True)\n\nX_test_new['new_col_2'] = np.log(X_test_new['new_col_2'])\n\nX_test_new['new_col'] = [x if x==0 else 1 for x in X_test_new['new_col']]\n\nX_test_new.drop(['Self_Employed'], axis=1, inplace=True)\n\n# drop all the features Except for Credit_History\n#X_test_new.drop(['Self_Employed','Dependents', 'new_col_2', 'Education', 'Gender', 'Property_Area','Married', 'new_col'], axis=1, inplace=True)","ccc827b5":"for name,model in models.items():\n    print(name, end=':\\n')\n    loss(y_test, model.predict(X_test_new))\n    print('-'*40)","c2c96677":"#So we can see that the logistic and decision tree performs well with the given data. ","e0aec917":"**TRAINING THE DATA**","b39c89df":"1. From the above chart we can notice that the semi urban area have higher chance of getting the loan.","94396e43":"From the above chart we can notice that the graduates have a better chance of getting a job.","3f50b536":"From the above data we can see that the loan has been approved if the **Co-applicant's income** is really high.","b20fa93b":"    Lets check if the ratio of 'Y' to 'N' is equal.","73eeb56d":"From the above chart we realize that more of people who were not married were given the loan.","dc15b084":"**Lets start working with the LOAN dataset.**","18bfc680":"**NOW its time to handle all the MISSING VALUES in out data.**","1d65f9d2":"Now we see that the data  is having 614 rows and 13 columns.","503cdb8b":"From the above chart we can realize that poeple who are **not self employed** have recieved loan.","b32aa3bc":"from the above chart we can say the applicants with one dependant has more","ed65c9c5":"From the above chart we realize that if the person is mail then the possibility of getting the loan is high.","fd7b6cf8":"here we see the logistic regression has the maximum score.","1bb7cfb7":"#as we can see the decision tree is still doing great :)","5a4b0a50":"**Now lets move on to the NUMERICAL VARIABLES.**","c9bb5563":"Here we can see that DecisionTreeClassifier is giving us more accuracy than logistic regression.","e73851d4":"**From the above chart we realize that people with credit history = 1 gets loan easily. **","208c7493":"#lets do Feature engineering. ","3218f1a1":"lets use StratifiedShuffleSplit","213c0b9e":"From the above chart we see we can't find any particular pattern.","08a51e79":"# Lets select the final features required.","df79bdf7":"# lets test our model on the test data."}}