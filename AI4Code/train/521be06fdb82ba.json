{"cell_type":{"42ab6a36":"code","fc44c099":"code","f216b6b6":"code","60a6a541":"code","e3d61df0":"code","0b862565":"code","2321523a":"code","537d3d53":"code","ddc71188":"code","a3e52afe":"markdown","e11af081":"markdown","cdab900b":"markdown","90351282":"markdown","d09dface":"markdown"},"source":{"42ab6a36":"import numpy as np # Linear algebra\nimport pandas as pd # Data processing.\nimport matplotlib.pyplot as plt # Visualize","fc44c099":"data = pd.read_csv(\"..\/input\/column_2C_weka.csv\") # Import Data\nprint(data[\"class\"].unique())\ndata.head()","f216b6b6":"abnormal = data[data[\"class\"] == \"Abnormal\"]\nnormal = data[data[\"class\"] == \"Normal\"]\n\nplt.scatter(abnormal.pelvic_radius, abnormal.sacral_slope,color = \"red\",label = \"abnormal\")\nplt.scatter(normal.pelvic_radius, normal.sacral_slope,color = \"green\",label = \"normal\")\nplt.legend()\nplt.xlabel(\"pelvic_radius\")\nplt.ylabel(\"sacral_slope\")\nplt.show()","60a6a541":"data[\"class\"] = [1 if each == \"Abnormal\" else 0 for each in data[\"class\"]] # We need 1 and 0\ndata.head()","e3d61df0":"y = data[\"class\"].values\nx_data = data.drop([\"class\"],axis = 1)\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)) # Normalize\nx.head()","0b862565":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.1, random_state = 15) # 85% train, 15% test","2321523a":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 2) # Optimal n_neighbors value is 2\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test) # We predicted x_test values\nprint(\"KNN score:\", knn.score(x_test,y_test))","537d3d53":"scores = []\nfor each in range(1,len(x_train)):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score = knn2.score(x_test,y_test) # Calculate R-Square\n    scores.append(score)\n\nplt.plot(range(1,len(x_train)), scores)\nplt.xlabel(\"k values\")\nplt.ylabel(\"scores\")\nplt.show()\nprint(\"Maximum Score :\", max(scores))\nprint(\"K value :\", scores.index(max(scores))+1)\n# Here, We write max(scores)+1 because normally counting starts from 0 in software but scores list is starting with 1\n# As you can see, optimal k value is 2","ddc71188":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ny_predict = knn.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_predict) # We use confusion matrix for comparison\n\nf, ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True, linewidths = 0.5, linecolor = \"red\", fmt = \".0f\",ax = ax)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"real\")\nplt.show()","a3e52afe":"### Visualize with Scatter Plot","e11af081":"### KNN","cdab900b":"### Find K Value","90351282":"### Train - Test Split","d09dface":"## Compare Predicted Values with Real Values"}}