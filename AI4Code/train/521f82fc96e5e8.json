{"cell_type":{"1a6fce0f":"code","76da47e3":"code","36d099ec":"code","9afc955d":"code","b03ae9b1":"code","71fc3ffb":"code","94867a97":"code","5df7ee58":"code","464e1855":"code","ab3fba43":"code","bb814ffa":"code","d8aca400":"code","99da4079":"code","cb26696d":"code","52456740":"code","f36aceb3":"code","4a6ff326":"code","fb5accd7":"code","66a51c6a":"code","6a13e83b":"code","856dcd56":"code","0e955b6f":"code","4062c661":"code","a5d1890b":"code","2817b5de":"code","c5df1d58":"code","b67f7310":"code","a72e6b3a":"code","121a1692":"code","2e9921e6":"markdown","d3c3ce64":"markdown","d9d56f4f":"markdown"},"source":{"1a6fce0f":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import image\nimport pandas as pd\nimport glob\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers import (Dense, Dropout, Conv2D, MaxPool2D, Flatten, \n                          BatchNormalization, MaxPooling2D, GlobalAveragePooling2D)\nfrom keras.utils import to_categorical\nfrom keras.regularizers import l2\nfrom keras.callbacks import ReduceLROnPlateau\n\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras import layers as L\n\n# visualization\nimport seaborn as sns\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\nsns.set_context(\"talk\")\nsns.set_palette('husl')\nsns.set()\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\npath = '\/kaggle\/input\/plant-pathology-2020-fgvc7\/'\nimage_size = 300\n\nimport tensorflow as tf\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","76da47e3":"test = pd.read_csv(path + 'test.csv')\ntrain = pd.read_csv(path + 'train.csv')\ntrain['image_id']=train['image_id']+'.jpg'  # this is used in the image data generator below\ntest['image_id']=test['image_id']+'.jpg'\n\ntrain.shape, test.shape","36d099ec":"image = cv2.imread(path + 'images\/Train_0.jpg')\n# summarize shape of the pixel array\nprint(image.dtype)\nprint(image.shape)\nplt.imshow(image)\nplt.show()","9afc955d":"'''train_files = glob.glob(path + 'images\/Train*')\ntrain_batch = [cv2.resize(cv2.imread(tr_file),(image_size, image_size)) for tr_file in train_files]\ntrain_batch = np.array(train_batch)'''","b03ae9b1":"'''test_files = glob.glob(path + 'images\/Test*')\ntest_batch = [cv2.resize(cv2.imread(ts_file),(image_size, image_size)) for ts_file in test_files]\ntest_batch = np.array(test_batch)'''","71fc3ffb":"train, val = train_test_split(train, test_size = 0.3)","94867a97":"datagen = ImageDataGenerator(horizontal_flip=True,\n                                vertical_flip=True,\n                                rotation_range=45,\n                                width_shift_range=0.1,\n                                height_shift_range=0.1,\n                                zoom_range=.1,\n                                fill_mode='nearest',\n                                shear_range=0.1,\n                                rescale=1\/255,\n                                brightness_range=[0.5, 1.5])\n\n# prepare an iterators to scale images\ntrain_gen = datagen.flow_from_dataframe(train,\n                                        directory=path+'images\/',\n                                        target_size=(image_size, image_size),\n                                        x_col=\"image_id\",\n                                        y_col=['healthy','multiple_diseases','rust','scab'],\n                                        class_mode='raw',\n                                        shuffle=False,\n                                        subset='training',\n                                        batch_size=50)\n\nval_gen = datagen.flow_from_dataframe(val,\n                                        directory=path+'images\/',\n                                        target_size=(image_size,image_size),\n                                        x_col=\"image_id\",\n                                        y_col=['healthy','multiple_diseases','rust','scab'],\n                                        class_mode='raw',\n                                        shuffle=False,\n                                        batch_size=50)\n\ntest_gen = datagen.flow_from_dataframe(test,\n                                        directory=path+'images\/',\n                                        target_size=(image_size,image_size),\n                                        x_col=\"image_id\",\n                                        y_col=None,\n                                        class_mode=None,\n                                        shuffle=False,\n                                        batch_size=50)","5df7ee58":"# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    input_shape = (image_size, image_size, 3)  # 3 for RGB images and 1 for gray scale images\n\n    model = Sequential()\n    model.add(Conv2D(64, kernel_size=(3, 3),  # size of the filter to use\n                     strides=(1, 1),  # steps the filter is moved\n                     padding='same',\n                     activation='relu', \n                     kernel_regularizer=l2(0.001),\n                     input_shape=input_shape)) \n    model.add(MaxPool2D(pool_size=(3, 3),  strides=(1, 1)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', kernel_regularizer=l2(0.001),))\n    model.add(MaxPool2D(pool_size=(3, 3)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())  # so the data can proceed to the fully-connected layer\n    model.add(Dense(50, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dense(4, activation='softmax'))\n    \n    model.compile(optimizer='adam', \n              loss='categorical_crossentropy',\n              metrics=['categorical_accuracy'])","464e1855":"train_step = train_gen.n\/\/train_gen.batch_size\nval_step = val_gen.n\/\/val_gen.batch_size\nhistory1 = model.fit_generator(generator=train_gen,\n                                steps_per_epoch=train_step,\n                                validation_data=val_gen,\n                                validation_steps=val_step,\n                                epochs=20,\n                                callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.3,patience=3, min_lr=0.000001)],\n                             )","ab3fba43":"history = history1\n\n# set up figure\nf = plt.figure(figsize=(12,5))\nf.add_subplot(1,2, 1)\n\n# plot accuracy as a function of epoch\nplt.plot(history.history['categorical_accuracy'])\nplt.plot(history.history['val_categorical_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('categorical_accuracy')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\n\n# plot loss as a function of epoch\nf.add_subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\nplt.show(block=True)","bb814ffa":"SUB_PATH = \"..\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv\"\n\nsub = pd.read_csv(SUB_PATH)\npredictions = model.predict(test_gen, verbose=1)\nsub.loc[:, 'healthy':] = predictions\nsub.to_csv('submission1.csv', index=False)\nsub.head()","d8aca400":"test = pd.read_csv(path + 'test.csv')\ntrain = pd.read_csv(path + 'train.csv')\n\ntrain_paths = np.array(glob.glob(path + 'images\/Train*'))\ntest_paths = np.array(glob.glob(path + 'images\/Test*'))\ntrain_labels = np.float32(train.loc[:, 'healthy':'scab'].values.copy())\n\ntrain_paths, valid_paths, train_labels, valid_labels =\\\ntrain_test_split(train_paths, train_labels, test_size=0.15, random_state=2020)","99da4079":"from kaggle_datasets import KaggleDatasets\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\ndef format_path(st):\n    return GCS_DS_PATH + '\/images\/' + st + '.jpg'\n\ntest = pd.read_csv(path + 'test.csv')\ntrain = pd.read_csv(path + 'train.csv')\n\ntrain_paths = train.image_id.apply(format_path).values\ntest_paths = test.image_id.apply(format_path).values\ntrain_labels = np.float32(train.loc[:, 'healthy':'scab'].values.copy())\n\ntrain_paths, valid_paths, train_labels, valid_labels =\\\ntrain_test_split(train_paths, train_labels, test_size=0.15, random_state=2020)","cb26696d":"def decode_image(filename, label=None, image_size=(512, 512)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","52456740":"BATCH_SIZE = 32\nAUTO = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","f36aceb3":"def build_lrfn(lr_start=0.00001, lr_max=0.00005, \n               lr_min=0.00001, lr_rampup_epochs=5, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * tpu_strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n                                - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn","4a6ff326":"lrfn = build_lrfn()\nSTEPS_PER_EPOCH = train_labels.shape[0] \/\/ BATCH_SIZE\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)","fb5accd7":"with tpu_strategy.scope():\n    model = tf.keras.Sequential([DenseNet121(input_shape=(512, 512, 3),\n                                             weights='imagenet',\n                                             include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(train_labels.shape[1],\n                                         activation='softmax')])\n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])","66a51c6a":"history2 = model.fit(train_dataset,\n                    epochs=10,\n                    callbacks=[lr_schedule],\n                    steps_per_epoch=STEPS_PER_EPOCH,\n                    validation_data=valid_dataset)","6a13e83b":"history = history2\n\n# set up figure\nf = plt.figure(figsize=(12,5))\nf.add_subplot(1,2, 1)\n\n# plot accuracy as a function of epoch\nplt.plot(history.history['categorical_accuracy'])\nplt.plot(history.history['val_categorical_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('categorical_accuracy')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\n\n# plot loss as a function of epoch\nf.add_subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\nplt.show(block=True)","856dcd56":"SUB_PATH = \"..\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv\"\n\nsub = pd.read_csv(SUB_PATH)\npredictions = model.predict(test_dataset, verbose=1)\nsub.loc[:, 'healthy':] = predictions\nsub.to_csv('submission2.csv', index=False)\nsub.head()","0e955b6f":"test = pd.read_csv(path + 'test.csv')\ntrain = pd.read_csv(path + 'train.csv')\ntrain['image_id']=train['image_id']+'.jpg'  # this is used in the image data generator below\ntest['image_id']=test['image_id']+'.jpg'","4062c661":"train, val = train_test_split(train, test_size = 0.3)","a5d1890b":"datagen = ImageDataGenerator(horizontal_flip=True,\n                                vertical_flip=True,\n                                rotation_range=45,\n                                width_shift_range=0.1,\n                                height_shift_range=0.1,\n                                zoom_range=.1,\n                                fill_mode='nearest',\n                                shear_range=0.1,\n                                rescale=1\/255,\n                                brightness_range=[0.5, 1.5])\n\n# prepare an iterators to scale images\ntrain_gen = datagen.flow_from_dataframe(train,\n                                        directory=path+'images\/',\n                                        target_size=(image_size, image_size),\n                                        x_col=\"image_id\",\n                                        y_col=['healthy','multiple_diseases','rust','scab'],\n                                        class_mode='raw',\n                                        shuffle=False,\n                                        subset='training',\n                                        batch_size=50)\n\nval_gen = datagen.flow_from_dataframe(val,\n                                        directory=path+'images\/',\n                                        target_size=(image_size,image_size),\n                                        x_col=\"image_id\",\n                                        y_col=['healthy','multiple_diseases','rust','scab'],\n                                        class_mode='raw',\n                                        shuffle=False,\n                                        batch_size=50)\n\ntest_gen = datagen.flow_from_dataframe(test,\n                                        directory=path+'images\/',\n                                        target_size=(image_size,image_size),\n                                        x_col=\"image_id\",\n                                        y_col=None,\n                                        class_mode=None,\n                                        shuffle=False,\n                                        batch_size=50)","2817b5de":"from keras import regularizers\nfrom keras.models import Model\nfrom keras.applications import densenet\nfrom keras.layers import Activation, Dropout, Flatten, Dense, LeakyReLU\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n\n# Option 1\nwith tpu_strategy.scope():\n    model = densenet.DenseNet121(input_shape=(image_size, image_size, 3),\n                                     weights='imagenet',\n                                     include_top=False,\n                                     pooling='avg')\n    for layer in model.layers:\n        layer.trainable = True\n\n    x = model.output\n    predictions = Dense(4, activation='softmax')(x)\n    model = Model(inputs=model.input, outputs=predictions)\n    \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])","c5df1d58":"early_stop = EarlyStopping(monitor='val_loss', patience=8, verbose=1, min_delta=1e-4)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_delta=1e-4)\ncallbacks_list = [early_stop, reduce_lr]","b67f7310":"# Option 1\ntrain_step = train_gen.n\/\/train_gen.batch_size\nval_step = val_gen.n\/\/val_gen.batch_size\nhistory3 = model.fit_generator(generator=train_gen,\n                                steps_per_epoch=train_step,\n                                validation_data=val_gen,\n                                validation_steps=val_step,\n                                epochs=20,\n                                callbacks=callbacks_list,\n                             )","a72e6b3a":"history = history3\n\n# set up figure\nf = plt.figure(figsize=(12,5))\nf.add_subplot(1,2, 1)\n\n# plot accuracy as a function of epoch\nplt.plot(history.history['categorical_accuracy'])\nplt.plot(history.history['val_categorical_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('categorical_accuracy')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\n\n# plot loss as a function of epoch\nf.add_subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\nplt.show(block=True)","121a1692":"SUB_PATH = \"..\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv\"\n\nsub = pd.read_csv(SUB_PATH)\npredictions = model.predict(test_gen, verbose=1)\nsub.loc[:, 'healthy':] = predictions\nsub.to_csv('submission3.csv', index=False)\nsub.head()","2e9921e6":"___","d3c3ce64":"___","d9d56f4f":"Part of what is in here was taken from [here](https:\/\/www.kaggle.com\/tarunpaparaju\/plant-pathology-2020-eda-models) and other notebooks from this competition."}}