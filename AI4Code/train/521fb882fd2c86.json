{"cell_type":{"59f7197a":"code","e18ed976":"code","3e47c162":"code","6f9e357e":"code","c85069be":"code","cda4c3f7":"code","e5bb8a5a":"code","e76504ab":"code","00ccb237":"code","4514b91e":"code","b9805335":"code","fb957d41":"code","09530ecf":"code","a9517d6e":"code","79d6d0b5":"code","a25f3b8c":"code","b9ea3ce3":"code","4039a5dd":"code","06eb0357":"code","e3507e20":"code","bc5bf914":"code","074e632e":"code","2e27b299":"code","7119f164":"code","1ce28ab8":"code","cbe74536":"code","5c759986":"code","8f44f947":"code","8df10723":"code","4e674d0b":"code","799ae324":"code","a6ead618":"code","bc94d177":"code","cabf365a":"code","0c1b57ac":"code","8b1c34f0":"code","5666ed27":"code","44823ab4":"code","5c245f4d":"code","199504ef":"code","bfdfc986":"code","56a1ec31":"code","1147e0f9":"markdown","99e22db3":"markdown","ff36f1ad":"markdown","245560d7":"markdown","40b20b34":"markdown","4139c655":"markdown","224e0c7b":"markdown","f78aaa51":"markdown","e482b3ef":"markdown","6f55ab3d":"markdown","fe7e7b87":"markdown","2ebfe8b5":"markdown","a7f8c059":"markdown","b53aad9b":"markdown","8341162d":"markdown","bd5a0e88":"markdown","4c704e0f":"markdown","be0e05c6":"markdown","cc8b2f67":"markdown","e7e00414":"markdown","f4cf011c":"markdown","341eb982":"markdown","8852fdce":"markdown","ee061671":"markdown","32811edb":"markdown","d26a30de":"markdown","0d9c38b9":"markdown","2808098c":"markdown","1dc0e10f":"markdown","9dd38bbd":"markdown","19cebb6a":"markdown","6e8f506c":"markdown"},"source":{"59f7197a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e18ed976":"import warnings \nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import RobustScaler\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\npd.pandas.set_option(\"display.max_columns\",None)\nprint(\"all necessary libraries are imported\")","3e47c162":"train=pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ntrain.head()","6f9e357e":"train.shape","c85069be":"train.isnull().values.any()","cda4c3f7":"train.nunique()","e5bb8a5a":"plt.figure(figsize=(10,6))\ng = sns.countplot(train['Class'])\ng.set_xticklabels(['Not Fraud','Fraud'])\nplt.show()","e76504ab":"sns.histplot(train['Amount'],bins=25,color=\"green\")","00ccb237":"train['log_Amount'] = train['Amount'].apply(lambda x: np.log(x + 0.01))\nsns.histplot(train['log_Amount'],bins=25,color=\"green\")","4514b91e":"plt.figure(figsize=(15,15))\nplt.subplot(221)\nax1=sns.boxplot(x=train['Class'],y=train['Amount'],data=train)\nax1.set_title(\"class vs amount\")\nax1.set_xlabel(\"Class\")\nax1.set_ylabel(\"Amount\")\n\nplt.subplot(222)\nax1=sns.boxplot(x=train['Class'],y=train['log_Amount'],data=train)\nax1.set_title(\"class vs log_amount\")\nax1.set_xlabel(\"Class\")\nax1.set_ylabel(\"log_Amount\")\n","b9805335":"# robust scaler\nscaler=RobustScaler()\ntrain['scaled_Amount']=scaler.fit_transform(train['log_Amount'].values.reshape(-1, 1))\ntrain['scaled_time']=scaler.fit_transform(train['Time'].values.reshape(-1, 1))","fb957d41":"sns.histplot(train['scaled_Amount'],bins=25,color=\"lightgreen\")","09530ecf":"sns.histplot(train['scaled_time'],bins=25,color=\"lightcoral\")","a9517d6e":"class_count_0, class_count_1 = train['Class'].value_counts()\nclass_0 = train[train['Class'] == 0]\nclass_1 = train[train['Class'] == 1]\nprint('class 0:', class_0.shape)\nprint('class 1:', class_1.shape)","79d6d0b5":"fraud_df=train[train['Class']==1]\nnon_fraud_df=train[train['Class']==0]\nfraud_df.shape, non_fraud_df.shape","a25f3b8c":"train.drop(['Time','Amount','log_Amount'], axis=1, inplace=True)","b9ea3ce3":"scaled_amount = train['scaled_Amount']\nscaled_time = train['scaled_time']\n\ntrain.drop(['scaled_Amount', 'scaled_time'], axis=1, inplace=True)\ntrain.insert(0, 'scaled_Amount', scaled_amount)\ntrain.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ntrain.head()","4039a5dd":"colormap = 'coolwarm_r'\n\nplt.figure(figsize=(21,21))\n\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap = colormap, linecolor='white', annot=True)\nplt.show()\n","06eb0357":"y=train['Class']\ntrain.drop(['Class'],axis=1,inplace=True)","e3507e20":"# train test split of the dataset\nx_train,x_test,y_train,y_test=train_test_split(train,y,test_size=0.2,random_state=42)","bc5bf914":"# random over sampler\nfrom imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=42)\n\n# fit predictor and target variable\nx_ros, y_ros = ros.fit_resample(x_train, y_train)\n\nprint('Original dataset shape', Counter(y))\nprint('Resample dataset shape', Counter(y_ros))","074e632e":"sns.countplot(y_ros)","2e27b299":"# Random under sampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\nros = RandomUnderSampler(random_state=42)\n\n# fit predictor and target variable\nx_rus, y_rus = ros.fit_resample(x_train, y_train)\n\nprint('Original dataset shape', Counter(y))\nprint('Resample dataset shape', Counter(y_rus))","7119f164":"sns.countplot(y_rus)","1ce28ab8":"pip install cluster-over-sampling","cbe74536":"import imblearn","5c759986":"from imblearn.over_sampling import SMOTE\nfrom sklearn.cluster import KMeans\nfrom clover.over_sampling import ClusterOverSampler\nsmote = SMOTE(random_state=42)\nkmeans = KMeans(n_clusters=50, random_state=42)\nkmeans_smote = ClusterOverSampler(oversampler=smote, clusterer=kmeans)\n\n# Fit and resample imbalanced data\nx_cos, y_cos = kmeans_smote.fit_resample(x_train, y_train)\nprint('original dataset shape:', Counter(y))\nprint('Resample dataset shape', Counter(y_cos))","8f44f947":"sns.countplot(y_cos)","8df10723":"# smote\noversample = SMOTE()\nx_smote, y_smote = oversample.fit_resample(x_train, y_train)\nprint('original dataset shape:', Counter(y))\nprint('Resample dataset shape', Counter(y_smote))","4e674d0b":"sns.countplot(y_smote)","799ae324":"from imblearn.under_sampling import NearMiss\n\nnm = NearMiss()\n\nx_nm, y_nm = nm.fit_resample(x_train, y_train)\n\nprint('Original dataset shape:', Counter(y))\nprint('Resample dataset shape:', Counter(y_nm))","a6ead618":"sns.countplot(y_nm)","bc94d177":"# with smote\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report,roc_auc_score\nmodel_1=LogisticRegression()\nmodel_1.fit(x_smote,y_smote)\npred_1=model_1.predict(x_test)\nacc_1 = accuracy_score(y_test, pred_1)\ncon_mat = confusion_matrix(y_test, pred_1)\nclf_report = classification_report(y_test, pred_1)\nprint(f\"Accuracy Score of Logistic Regression : {acc_1}\")\nprint(f\"Confusion Matrix : \\n{con_mat}\")\nprint(f\"Classification Report : \\n{clf_report}\")","cabf365a":"# cross validation\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report,roc_auc_score\nfrom sklearn.linear_model import LogisticRegressionCV\nmodel_1=LogisticRegressionCV(cv=10, random_state=0)\nmodel_1.fit(x_smote,y_smote)\npred_1=model_1.predict(x_test)\nacc_1 = accuracy_score(y_test, pred_1)\ncon_mat = confusion_matrix(y_test, pred_1)\nclf_report = classification_report(y_test, pred_1)\nprint(f\"Accuracy Score of Logistic Regression : {acc_1}\")\nprint(f\"Confusion Matrix : \\n{con_mat}\")\nprint(f\"Classification Report : \\n{clf_report}\")","0c1b57ac":"# near miss\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report,roc_auc_score\nmodel_2=LogisticRegression()\nmodel_2.fit(x_nm,y_nm)\npred_2=model_2.predict(x_test)\nacc_2 = accuracy_score(y_test, pred_2)\ncon_mat = confusion_matrix(y_test, pred_2)\nclf_report = classification_report(y_test, pred_2)\nprint(f\"Accuracy Score of Logistic Regression : {acc_2}\")\nprint(f\"Confusion Matrix : \\n{con_mat}\")\nprint(f\"Classification Report : \\n{clf_report}\")","8b1c34f0":"# cross validation\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report,roc_auc_score\nfrom sklearn.linear_model import LogisticRegressionCV\nmodel_2=LogisticRegressionCV(cv=5, random_state=1,max_iter=1000)\nmodel_2.fit(x_nm,y_nm)\npred_2=model_1.predict(x_test)\nacc_2 = accuracy_score(y_test, pred_2)\ncon_mat = confusion_matrix(y_test, pred_2)\nclf_report = classification_report(y_test, pred_2)\nprint(f\"Accuracy Score of Logistic Regression : {acc_2}\")\nprint(f\"Confusion Matrix : \\n{con_mat}\")\nprint(f\"Classification Report : \\n{clf_report}\")","5666ed27":"# cluster based sampling\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report,roc_auc_score\nmodel_3=LogisticRegression()\nmodel_3.fit(x_cos,y_cos)\npred_3=model_3.predict(x_test)\nacc_3 = accuracy_score(y_test, pred_3)\ncon_mat = confusion_matrix(y_test, pred_3)\nclf_report = classification_report(y_test, pred_3)\nprint(f\"Accuracy Score of Logistic Regression : {acc_3}\")\nprint(f\"Confusion Matrix : \\n{con_mat}\")\nprint(f\"Classification Report : \\n{clf_report}\")","44823ab4":"# cross validation\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report,roc_auc_score\nmodel_3=LogisticRegressionCV(cv=5,random_state=0)\nmodel_3.fit(x_cos,y_cos)\npred_3=model_3.predict(x_test)\nacc_3 = accuracy_score(y_test, pred_3)\ncon_mat = confusion_matrix(y_test, pred_3)\nclf_report = classification_report(y_test, pred_3)\nprint(f\"Accuracy Score of Logistic Regression : {acc_3}\")\nprint(f\"Confusion Matrix : \\n{con_mat}\")\nprint(f\"Classification Report : \\n{clf_report}\")","5c245f4d":"# decision Tree Classifier\nmodel_4=DecisionTreeClassifier()\nmodel_4.fit(x_smote,y_smote)\npred_4=model_4.predict(x_test)\nacc_4 = accuracy_score(y_test, pred_4)\ncon_mat = confusion_matrix(y_test, pred_4)\nclf_report = classification_report(y_test, pred_4)\nprint(f\"Accuracy Score of Decision Tree : {acc_4}\")\nprint(f\"Confusion Matrix : \\n{con_mat}\")\nprint(f\"Classification Report : \\n{clf_report}\")","199504ef":"# decision Tree Classifier\nmodel_5=DecisionTreeClassifier()\nmodel_5.fit(x_nm,y_nm)\npred_5=model_5.predict(x_test)\nacc_5 = accuracy_score(y_test, pred_5)\ncon_mat = confusion_matrix(y_test, pred_5)\nclf_report = classification_report(y_test, pred_5)\nprint(f\"Accuracy Score of Decision Tree : {acc_5}\")\nprint(f\"Confusion Matrix : \\n{con_mat}\")\nprint(f\"Classification Report : \\n{clf_report}\")","bfdfc986":"# decision Tree Classifier\nmodel_6=DecisionTreeClassifier()\nmodel_6.fit(x_cos,y_cos)\npred_6=model_6.predict(x_test)\nacc_6 = accuracy_score(y_test, pred_6)\ncon_mat = confusion_matrix(y_test, pred_6)\nclf_report = classification_report(y_test, pred_6)\nprint(f\"Accuracy Score of Decision Tree : {acc_6}\")\nprint(f\"Confusion Matrix : \\n{con_mat}\")\nprint(f\"Classification Report : \\n{clf_report}\")","56a1ec31":"d = {'Technique': ['Logistic SMOTE', 'Logistic Near miss','Logistic Clover','Decision Tree SMOTE', 'Decision Tree Near miss',\n                   'Decision Tree Clover'], 'Score': [acc_1,acc_2,acc_3,acc_4,acc_5,acc_6]}\nfinal_df = pd.DataFrame(data=d)\n\n# Move column\nscore = final_df['Score']\nfinal_df.drop('Score', axis=1, inplace=True)\nfinal_df.insert(1, 'Score', score)\n \nfinal_df.sort_values(by=\"Score\",ascending=False)","1147e0f9":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >1. Visualizing The Target feature<\/h2> ","99e22db3":"<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>Any suggestions or comments are most welcome !\n<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>Thanks For Reading :)","ff36f1ad":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color:lightgreen ; color : #36609A; border-radius: 3px 3px; padding:2px;text-align:center; font-weight: bold\" >Visualize The change<\/h2> ","245560d7":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >9. SMOTE<\/h2> ","40b20b34":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color:lightgreen ; color : #36609A; border-radius: 3px 3px; padding:2px;text-align:center; font-weight: bold\" >Visualize The change<\/h2> ","4139c655":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color:lightgreen ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >Logistic Regression With Near Miss<\/h2> ","224e0c7b":"<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>As Robust scalers are less prone to outliers Hence i used it here for scaling the features<\/strong><\/p>","f78aaa51":"<br>\n<br>\n<h2 style = \"font-family: garamond; font-size: 50px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:10px;text-align:center; font-weight: bold\" >Credit Card Fraud Detection<\/h2> \n<br> \n<br>\n<div class = 'image'> <img style=\"float:center; border:10px solid #fed049; width:90%\" align=center src = https:\/\/res.cloudinary.com\/dofgyxd4y\/image\/upload\/v1623419916\/screen-shot-2018-05-03-at-3-51-26-pm_psz4a9.png> \n<\/div>\n<br>\n<br>\n<a href =\"https:\/\/res.cloudinary.com\/dofgyxd4y\/image\/upload\/v1623419916\/screen-shot-2018-05-03-at-3-51-26-pm_psz4a9.png\" style = \"font-size:20px,color: dimgrey, text-align:left,font-family:serif\"><\/a>\n<br>\n","e482b3ef":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color:lightgreen ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >Logistic Regression With SMOTE<\/h2> ","6f55ab3d":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >2. Visualizing The Amount feature<\/h2> ","fe7e7b87":"<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>model building\n<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>best models used for binary clssification\n<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>1.logistic regression Logistic Regression\n<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>2.k-Nearest Neighbors\n<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>3.Decision Trees\n<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>4.Support Vector Machine\n<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>5.Naive Bayes\n<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>6.Bagging Decision Tree (Ensemble Learning I)\n<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>7.Boosted Decision Tree (Ensemble Learning II)\n<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>8. Random Forest (Ensemble Learning III)\n","2ebfe8b5":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >8. Cluster Based Over Sampling<\/h2> \nThis approach to addressing imbalanced data uses K-mean clustering. The clustering algorithm is applied to both the majority class and the minority class in which each class is oversampled, such that each class has the same number of data elements. Though this is an efficient method, it suffers from the issue of overfitting.","a7f8c059":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color:lightgreen ; color : #36609A; border-radius: 3px 3px; padding:2px;text-align:center; font-weight: bold\" >Visualize The change<\/h2> ","b53aad9b":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color:lightgreen ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >Decision Tree Classifier with SMOTE<\/h2> ","8341162d":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >3. Visualizing The Outliers in amount feature<\/h2> ","bd5a0e88":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >5. Heatmap<\/h2> ","4c704e0f":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >5.Ways Of handling Unbalanced Datset<\/h2> ","be0e05c6":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >12.Model Comparison<\/h2> ","cc8b2f67":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >1. Load The Dataset<\/h2> ","e7e00414":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color:lightgreen ; \ncolor : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >Decision Tree With Near Miss<\/h2> ","f4cf011c":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >7. Random Under sampler<\/h2> ","341eb982":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >5. Preparing the Dataset for analysis<\/h2> ","8852fdce":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >10.Near Miss<\/h2> ","ee061671":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color:lightgreen ; color : #36609A; border-radius: 3px 3px; padding:2px;text-align:center; font-weight: bold\" >Visualize The change<\/h2> ","32811edb":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >6. Random Over sampler<\/h2> ","d26a30de":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color:lightgreen ; color : #36609A; border-radius: 3px 3px; padding:2px;text-align:center; font-weight: bold\" >Visualize The change<\/h2> ","0d9c38b9":"<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>This shows:\nNegatively Correlated= V3,V7,V10,V12,V14,V17 and Positively Correlated= V2,V4,V11,V19 Features<\/strong>\n  ","2808098c":"<p style = \"font-size: 20px; font-style: normal;color : #36609A;font-weight: bold\" ><strong>This plot shows it is highly unbalanced as most of the features are non_fraud <\/strong><\/p>","1dc0e10f":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >4. Scaling the data<\/h2> ","9dd38bbd":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color:lightgreen ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >Decision Tree With ClusterOverSampler<\/h2> ","19cebb6a":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color:#FFCE30 ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >11.Model Building<\/h2> ","6e8f506c":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color:lightgreen ; color : #36609A; border-radius: 5px 5px; padding:3px;text-align:center; font-weight: bold\" >Logistic Regression With ClusterOverSampler<\/h2> "}}